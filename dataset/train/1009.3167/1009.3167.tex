\documentclass[journal,draftcls,onecolumn,11pt]{IEEEtran}
\usepackage{amsmath,amssymb,graphicx,amsfonts,epsfig,cite,subfigure,times,latexsym,array,verbatim,url}

\newcommand{\emb}[1]{\text{\boldmath{}}}
\newcommand{\Prob}{\textsf{P}}
\newcommand{\Expect}{\textsf{E}}
\newcommand{\indic}[1]{\mbox{}{\{#1\}}}

\newcommand{\QMDP}{}
\newcommand{\term}{\mathcal{T}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\graphicspath{{figures/}}

\title{Sensor Management for Tracking in Sensor Networks\thanks{This work was funded in part by a grant from the Motorola corporation, a U.S. Army Research Office MURI grant W911NF-06-1-0094 through a subcontract from Brown University at the University of Illinois, a NSF Graduate Research Fellowship, and by a Vodafone Fellowship.}}
\author{Jason~A.~Fuemmeler,~\IEEEmembership{Member,~IEEE}, George~K.~Atia,~\IEEEmembership{Member,~IEEE}, and Venugopal~V.~Veeravalli,~\IEEEmembership{Fellow,~IEEE}
\thanks{This work was done at the Coordinated Science Laboratory (CSL), University of Illinois at Urbana-Champaign, Urbana IL 61801, Emails: \{fuemmele,atia1,vvv\}@illinois.edu}}
\date{}

\begin{document}
\maketitle
\begin{abstract}
We study the problem of tracking an object moving through a network of wireless sensors. In order to conserve energy, the sensors may be put into a sleep mode with a timer that determines their sleep duration. It is assumed that an asleep sensor cannot be communicated with or woken up, and hence the sleep duration needs to be determined at the time the sensor goes to sleep based on all the information available to the sensor. Having sleeping sensors in the network could result in degraded tracking performance, therefore, there is a tradeoff between energy usage and tracking performance. We design sleeping policies that attempt to optimize this tradeoff and characterize their performance.  As an extension to our previous work in this area \cite{fuemmeler08}, we consider generalized models for object movement, object sensing, and tracking cost. For discrete state spaces and continuous Gaussian observations, we derive a lower bound on the optimal energy-tracking tradeoff. It is shown that in the low tracking error regime, the generated policies approach the derived lower bound. \end{abstract}

\section{Introduction}
Large sensor networks collecting data in dynamic environments are typically composed of a distributed collection of cheap nodes with limited energy and processing capabilities. Hence, it is imperative to efficiently manage the sensors' resources to prolong the lifetime of such networks without sacrificing performance. Our focus in this paper is on sensor resource management for tracking and surveillance applications.

Previous work on sensor resource management considered the design of sensor sleeping protocols for sensor sleeping via wakeup mechanisms \cite{brooks03,balasubramanian04,gupta03,yang03,xu04,yang06} or by modifying power-save functions in MAC protocols for wireless ad hoc networks \cite{gui04,gui05,vasanthi06}. In the context of target classification, Castanon \cite{castanon} developed an approximate dynamic programming approach for dynamic scheduling of multi-mode sensors subject to sensors resource constraints. In \cite{asilomar_scheduling,tsp_scheduling} we studied a single object tracking problem where the sensors can be turned on or off at consecutive time steps to conserve energy (sensor scheduling). A controller selects the subset of sensors to activate at each time step. Also in \cite{fuemmeler08}, we studied a tracking problem where each sensor could enter a sleep mode with a sleep timer (sensor sleeping). While in sleep mode, the sensor could not assist in tracking the object by making observations. In contrast to \cite{tsp_scheduling}, in \cite{fuemmeler08} we assumed that sleeping sensors could not be woken up externally but instead had to set internal timers to determine the next time to come awake, wherefore, the control actions correspond to the sleep durations of awake sensors. In turn, this did not only entail a different control space, but also led to a significantly different policy design problem since a decision to put a sensor to sleep implies that this sensor cannot be scheduled at future time steps until it comes awake. The consequences of the current action on the tracking performance could be more dramatic rendering future planning more crucial. This led to a design problem that sought to optimize a tradeoff between energy efficiency and tracking performance. While optimal solutions to this problem could not be found, suboptimal solutions were devised that were demonstrated to be near optimal. To aid analysis, we assumed particularly simple models for object movement, object sensing, and tracking cost.  In particular, we assumed that the network could be divided into cells, each of which contained a single sensor.  The object moved among the cells and could only be observed by the sensor in the currently occupied cell.  Tracking performance was a binary quantity; either the object was observed in a particular time slot or it was not observed depending on whether the right sensor was awake.

In this paper, we continue to examine the fundamental theory of sleeping in sensor networks for tracking but we extend our analysis to more generalized models for object movement, object sensing, and tracking cost. We allow the number of possible object locations to be different from the number of sensors. The number of possible object locations can even be infinite to model the movement of an object on a continuum. Moreover, the object sensing model allows for an arbitrary distribution for the observations given the current object location, and the tracking cost is modeled via an arbitrary distance measure between the actual and estimated object location.

Not surprisingly, this generalization results in a problem that is much more difficult to analyze. Our approach is to build on the policies designed in \cite{fuemmeler08}. The design of those policies relied on the separation of the problem into a set of simpler subproblems. In \cite{fuemmeler08}, we have shown that under an observable-after-control assumption, the design problem lends itself to a natural decomposition into simpler per-sensor subproblems due to the simplified nature of the tracking cost structure. Unfortunately, this does not extend to the generalized cases we consider herein. However, based on the intuition gained from the structure of the solution in the simplified case, in this work we artificially separate our problem into a set of simpler per-sensor subproblems. The parameters of these subproblems are not known {\em a priori} due to the difficulties in analysis. However, we use Monte Carlo simulation and learning algorithms to compute these parameters. We characterize the performance of the resulting sleeping policies through simulation. For the special case of a discrete state space with continuous Gaussian observations, we derive a lower bound on the optimal energy-tracking tradeoff which is shown to be loose at the high tracking error regime, but is reasonably tight for the low tracking error region.

The remainder of this paper is organized as follows. In Section~\ref{sec:probform}, we describe the tracking problem in mathematical terms and define the optimization problem.  In Section~\ref{sec:subopt_sol} we derive our suboptimal solutions and the aforementioned lower bound. In Section~\ref{sec:num_res}, we provide numerical results that illustrate the efficacy of the proposed sleeping policies. We summarize and conclude in Section~\ref{sec:concl}.


\section{Problem Formulation} \label{sec:probform}
\subsection{POMDP Formulation}
Consider a network with  sensors. Each sensor can be in one of two states: awake or asleep. A sensor in the awake state consumes more energy than one in the asleep state. However, object sensing can be performed only in the awake state.
We denote the set of possible object locations as  such that  where the -th state represents an absorbing terminal state that occurs when the object leaves the network.  We also refer to this terminal state as .  If  is not a finite set then  is .  We define a {\em kernel}  such that  is the probability that the next object location is in the set  given that the current object location is .  We can predict  time steps into the future by defining  and  inductively as


Suppose  is a probability measure on  such that  for  is the probability that the state is in  at the current time step.  Then the probability that the state will be in  after  time steps in the future is given by

This defines the measure  which depends on both the prior  and the transition Kernel .  Let  denote the state for the object at time .  Also, let  denote a probability measure such that  if , and  otherwise. Conditioned on the object state , the future state  has a distribution . This defines the evolution of the object location. For a discrete state space this is simply the probability mass function defined by the -th row of a transition matrix .
We assume that it is always possible to determine if the object has left the network, i.e., if .  To this end, we define a virtual sensor  that detects without error whether the object has left the network. In other words, sensor  is always awake but consumes no energy.

To provide a means for centralized control, we assume the presence of an extra node called the central controller.  The central controller keeps track of the state of the network and assigns sleep times to sensors that are awake.  In particular, each sensor that wakes up remains awake for one time unit during which the following actions are taken: (i) the sensor sends its observation of the object to the central unit, and (ii) the sensor receives a new sleep time (which may equal zero) from the central controller.  The sleep time input is used to initialize a timer at the sensor that is decremented by one time unit each time step.  When this timer expires, the sensor wakes up.  Since we assume that wakeup signals are impractical, this timer expiration is the only mechanism for waking a sensor.

Let  denote the value of the sleep timer of sensor  at time .  We call the -vector  the residual sleep times of the sensors at time .  Also, let  denote the sleep time input supplied to sensor  at time .  We add the constraints  and  due to the nature of the virtual sensor .  We can describe the evolution of the residual sleep times as

for all  and .  The first term on the right hand side of this equation expresses that if the sensor is currently asleep (the sleep timer for the sensor is not zero), the sleep timer is decremented by 1.  The second term expresses that if the sensor is currently awake (the sleep timer is zero), the sleep timer is reset to the current sleep time input for that sensor.

Based on the probabilistic evolution of the object location and \eqref{eq:rklh}, we see that we have a discrete-time dynamical model that describes our system with a well-defined state evolution.  The {\em state} of the system at time  is described by . Unfortunately, not all of  is known to the central unit at time  since  is known only if the object location is being tracked precisely.  Thus we have a dynamical system with incomplete (or partially observed) state information.

We write the observations for our problem as

where  is an -vector of observations.  These observations are drawn from a probability measure  that depends on .
However, we add two restrictions.  The first is that if a sensor is not awake at time , its observation is an erasure.  Mathematically, we say that  implies .  The second restriction is that  is a binary observation that indicates whether the object has left the network.

The total information available to the control unit at time  is given by

with  denoting the initial (known) state of the system.
The control input for sensor  at time  is allowed to be a function of the information state , i.e.,

The vector-valued function  is the sleeping policy at time  which defines a mapping from the information state  to the set of admissible actions .

We now identify the costs present in our tracking problem.  The first is an {\em energy cost} of  for each sensor that is awake.  The energy cost can be written mathematically as

The second cost is a {\em tracking cost}.  To define the tracking cost, we first define the estimated object location at time  to be .  We can think of  as an additional control input that is a function of , i.e.,

Since  does not affect the state evolution, we do not need past values of this control input in .  The tracking cost is a distance measure that is a function of the actual and estimated object locations and is written as

We assume that  is a bounded function on .  Two examples of distance measures we might employ are the Hamming cost (if the space  is finite), i.e.,

and the squared Euclidean distance (if the space  is a subset of an appropriate vector space), i.e.,

The parameter  is used to trade off energy consumption and tracking errors.

Recall that the input  does not affect the state evolution; it only affects the cost. Therefore, we can compute the optimal choice of , given by , using an optimization minimizing the tracking error over a single time step.  We can thus write


Remembering that once the terminal state is reached no further cost is incurred, we can write the total cost for time step  as

The infinite horizon cost for the system is given by

Since  is bounded (since the function  is bounded) and the expected time till the object leaves the network is finite, the cost function  is well defined.  The goal is to compute the solution to

The solution to this optimization problem for each value of  yields an optimal sleeping policy.  The optimization problem falls under the framework of a partially observable Markov decision process (POMDP) \cite{aberdeen03,littman95,monahan,Hauskrecht2000}.

\subsection{Dealing With Partial Observability}
Partial observability presents a problem since the information for decision-making at time  given in \eqref{eq:Ikh} is unbounded in memory.  To remedy this, we seek a sufficient statistic for optimization that is bounded in memory.  The observation  depends only on , which in turn depends only on , , and some random disturbance .  It is a standard argument (e.g., see \cite{bertsekas07}) that for such an observation model, a sufficient statistic is given by the probability distribution of the state  given .  Such a sufficient statistic is referred to as a {\em belief state} in the POMDP literature (e.g., see \cite{aberdeen03,littman95}).  Since the residual sleep times portion of our state is observable, the sufficient statistic can be written as , where  is a probability measure on .  Mathematically, we have

The task of recursively computing  for each  is a problem in nonlinear filtering (e.g., see \cite{doucet01}).  In other words,  can be computed using standard Bayesian techniques as the posterior measure resulting from prior measure  and observations .

The function  that determines  can now be written in terms of  and  instead of .  We can rewrite it as

Note that due to the stationarity of the state evolution,  has the same form for every  and is independent of .  Thus, we can drop the subscript and refer to  as , a function of  alone.

Now we write our dynamic programming problem in terms of the sufficient statistic.  We first rewrite the cost at time step .  Since only expected values of the cost function  appear in \eqref{eq:tot_costh}, we can take our cost function to be the expected value of  (defined in \eqref{eq:gh}) conditioned on  being distributed according to . With a slight abuse of notation, we call this redefined cost . The cost can then be written as

The selection of sleep times, originally presented in \eqref{eq:mukh}, can now be rewritten as

The total cost defined in \eqref{eq:tot_costh} becomes

and the optimal cost defined in \eqref{eq:opt_probh} becomes











\section{Suboptimal Solutions} \label{sec:subopt_sol}

Similar to the problem in \cite{fuemmeler08}, an optimal policy could be found by solving the Bellman equation

However, since an optimal solution could not be found for the simpler problem considered in \cite{fuemmeler08}, we immediately turn our attention to finding suboptimal solutions to our problem.

Note that in \cite{fuemmeler08}, simpler sensing models and cost structures were employed. Under a simplifying observable-after-control assumption, the simplicity of the sensing models allowed for the decoupling of the contributions of the individual sensors.  The simplicity of the cost structures allowed the cost to be written as a sum of per-sensor costs.  The result was a problem that could be written as a number of simpler subproblems. The present case is more complicated.  In general, the cooperation among the sensors may be difficult to analyze and understand.  Furthermore, the tracking cost may not be easily written as a sum across the sensors.

Based on the intuition gained from \cite{fuemmeler08}, our approach to generating suboptimal solutions is to artificially write the problem as a set of subproblems that can be solved using the techniques of \cite{fuemmeler08}. The tracking cost expressions (which are a function of the sleeping actions of the sensors) in these subproblems will be left as unknowns. To determine appropriate values for these tracking costs, we either perform Monte Carlo simulations before tracking begins or use data gathered during tracking.  The intuition is that if the resultant tracking cost expressions capture the ``typical'' behavior of the actual tracking cost, then our sleeping policies should perform well.

\subsection{General approach}
The complexity of the sleeping problem stems from:
\begin{enumerate}
\item The complicated evolution of the belief state  (non-linear filtering).
\item The complexity of the model including the dimensionality of the state space, the control space and the observation space.
\end{enumerate}
To address the aforementioned difficulties, our approach has two main ingredients. First, we make assumptions about the observations that will be available to the controller at future time steps. To generate sleeping policies, we assume that the system is either perfectly observable or totally unobservable after control. Hence, we define approximate recursions with special structure as surrogates for the optimal value function. Second, we devise different methodologies to evaluate suitable tracking costs in Sections \ref{sec:non_learning} and \ref{sec:learning} whereby we capture the effect of each sensor on the overall tracking cost. Writing the combined tracking cost as the sum of independent contributions of different sensors (with respect to some baseline) allows us to write the Bellman equation as the sum of per-sensor recursions. Instead of solving the Bellman equation in (\ref{eq:bellmanh}), we alternatively solve  simpler Bellman equations to find per-sensor policies and cost functions. The overall policy is then the per-sensor policies applied in parallel.

We denote by  the cost function of the -th sensor approximate subproblem. We define  to be the increase in tracking cost due to not waking up sensor  at time  given that . This is meant to capture the contribution of the -th sensor to the total tracking cost. Next we define our approximations.


\subsubsection{\QMDP{}}
First introduced in the artificial intelligence literature \cite{cassandra97,QMDP}, the \QMDP{} solution for POMDPs assumes that the system will be perfectly observable after control, i.e., the partially observable state becomes fully observable after taking a control action. In other words, under a \QMDP{} assumption the belief state simply evolves as

Noting that the future cost is not only affected by the current control action through belief evolution, but also by the fact that no future decisions can be made for a sleeping sensor until it wakes up, the observable-after-control policy is by no means a myopic policy. Note that (\ref{eq:QMDP_belief_evolution}) does not imply zero tracking errors; it is merely an assumption simplifying the state evolution in order to generate a sleeping policy. Now we can readily define a {\em \QMDP{} per-sensor Bellman equation} analogous to the one in \cite{fuemmeler08} as

To clarify, the first summation in the R.H.S. of (\ref{eq:qmdpbellh}) corresponds to the expected tracking cost incurred by the sleep duration  of sensor . The second term consists of: (i) the energy cost incurred as the sensor comes awake after its sleep timer expires (after  time slots); and (ii) the cost to go under an observable-after-control assumption (hence the belief state is ).

We cannot find an analytical solution for (\ref{eq:qmdpbellh}). However, note that if we can solve (\ref{eq:qmdpbellh}) for  for all , then it is straightforward to find the solution for all values of . Thus, given a function , \eqref{eq:qmdpbellh} can be solved through standard policy iteration \cite{bertsekas07}, but only if  is finite.

\subsubsection{FCR}
Similarly, we define a First Cost Reduction {\em (FCR) Bellman equation} analogous to the one in \cite{fuemmeler08} as

In this case, it is assumed that we will have no future observations. In other words, we define the belief evolution as . Again, it is worth mentioning that this does not mean that it would be impossible to track the object; we are simply making a simplifying assumption about the future state evolution in order to generate a sleeping policy. Given a function , it is easy to verify that the solution to \eqref{eq:fcrbellh} is

and the associated policy is to choose the first value of  such that

In other words, the policy is to come awake at the first time the expected tracking cost exceeds the expected energy cost where the tracking cost is defined based on  (to be determined) hence the name First Cost Reduction.

The solutions to the per-sensor Bellman equations in (\ref{eq:qmdpbellh}) and (\ref{eq:fcrbellh}) define the \QMDP{} and FCR policies for each sensor, respectively. Note that, unlike \cite{fuemmeler08,asilomar_scheduling,tsp_scheduling}, the solution to the \QMDP{} recursion does not necessarily provide a lower bound on the optimal value function since the employed tracking cost is not a lower bound on the actual tracking cost. In Sec \ref{sec:lower_bound} we derive a lower bound on the optimal energy-tracking tradeoff for discrete state spaces with Gaussian Observations. The remaining task is to identify appropriate values of  for all  and for all .  This is the subject of the next two sections.

\subsection{Nonlearning approach}
\label{sec:non_learning}
For now, suppose that  is a finite space.  Suppose .  To generate  for a particular , we first assume a ``baseline'' behavior for the sensors, i.e., we make an assumption about the set of sensors that are awake at time  given that .  We consider two possibilities:
\begin{enumerate}
   \item That all sensors are asleep.
   \item That the set of sensors awake is selected through a greedy algorithm.  In other words, the sensor that causes the largest decrease in expected tracking cost is added to the awake set until any further reduction due to a single sensor is less than .  The expected tracking cost can be evaluated through the use of Monte Carlo simulation (repeatedly simulating our system from time  to time ) to avoid the need for numerical integration.
\end{enumerate}
Starting with this set of awake sensors, the value of  is then computed as the absolute difference in expected tracking cost incurred by changing the state of sensor .  Again, Monte Carlo simulation can be used to evaluate the change in expected tracking cost.  We can think of this procedure as linearizing the tracking cost about some baseline behavior.

If  is not finite, then a parameterized version of  can be computed instead.  We choose  elements of  and evaluate  at these points.  The value of  at all other values of  can be computed via an interpolation algorithm.  Recall that only an FCR policy is appropriate in the infinite state case, since solving the \QMDP{} Bellman equation for an infinite number of point mass distributions is infeasible.

\subsection{Learning approach}  \label{sec:learning}
In this section, we describe an alternative learning-based approach. For ease of exposition, suppose that  is a finite space.  Then our probability measure  can be characterized by a probability mass function.  We refer to this probability mass function as  (a row vector).  Define  to be the approximated expected increase in tracking cost due to sensor  sleeping at time  as

Ideally, we would like this approximation to be equal to the actual expected increase in tracking cost due to sensor  sleeping.  Unfortunately, we do not have access to actual tracking costs at time  since  is not known exactly.  However, we do have access to , , and .  It is therefore possible to estimate the tracking cost as

For example, if Hamming cost is being used, then we can estimate the tracking cost as

and if squared Euclidean distance is being used we can estimate the tracking cost using the variance of the measure . Next we describe how we learn  by solving a least squares problem.

Determining an estimate of the {\em increase} in the tracking cost due to the sleeping of sensor  at time , denoted , depends on the value of .  If , we ignore the observation from sensor  and generate a new version of  called .  We can compute  as

If on the other hand , we we first generate an object location  according to  and then generate an observation according to the probability measure .  This observation is used to generate a new distribution  from .  Then we compute  as


We now have an approximation sequence  and an observation sequence . At time , our goal is to choose  to minimize

We apply the Robbins-Monro algorithm, a form of stochastic gradient descent, to this problem in order to recursively compute a sequence of  that will hopefully solve this minimization problem for large .  The update equation is

where  is a step size.  Note that  is the gradient of  with respect to .

Using a constant step size in our simulations, we could only observe small oscillations in the values of . It is unclear whether there are conditions under which the local or global convergence of this learning algorithm is guaranteed.  The difficulty is that the observations we are trying to model depend on the model itself.  The problem is reminiscent of optimistic policy iteration (see \cite{bertsekas07}), the convergence properties of which are little understood.  We have left a proof of convergence for future work.  It should be pointed out that the algorithm will likely converge more slowly for a two-dimensional network than a one-dimensional network.  The reason is that in two dimensions it is easier for an object to avoid visiting an object location state and causing an update to that particular value of .

If  is not finite, then we can again parameterize  as in the previous section.  The Robbins-Monro algorithm can be applied in this context as well, although the gradient expressions will depend on the type of interpolation used.

\subsection{A Lower Bound}
\label{sec:lower_bound}
Unfortunately, deriving a lower bound is generally difficult for the considered problem. However, in this section we derive a lower bound for the special case of a discrete state space with Gaussian observations. Our approach is similar to \cite{tsp_scheduling} in which we considered a related scheduling problem. The idea is to combine the observable-after-control assumption with a separable lower bound on the tracking cost as we demonstrate in what follows.

Given the current belief , an action vector , and the current residual sleep times vector , the expected tracking cost can be written as:


When awake, the sensors observations are Gaussian, i.e.,

where  is the location of sensor .

Defining,

which is a conditional error probability for a multiple hypothesis testing problem with  hypotheses, each corresponding to a different mean vector contaminated with white Gaussian noise. Conditioned on , the observation model is:

where  is the -th entry of an  vector  denoting the received signal strength at the  sensors,  is the mean received signal strength when the target is at state  (-th hypothesis) and  is a zero mean white Gaussian Noise, i.e. . According to (\ref{eq,underHj}), if awake at the next time step, sensor  gets a Gaussian observation that depends on the future target location, and an erasure, otherwise. Since the current belief is , the prior for the -th hypothesis is .

The error event  can be written as the union of pairwise error regions as

where 
is the region of observations for which the -th hypothesis  is more likely than the -th hypothesis , and where

denotes the likelihood ratio for  and .

Using standard analysis for likelihood ratio tests ~\cite{poor,levy}, it is not hard to show that:

where , , and  is the normal distribution -function. The quantity  plays the role of distance between the two hypothesis and hence depends on the difference of their corresponding mean vectors and the noise variance . Hence,  is a function of the next step residual sleep vector . To highlight this dependence, we will sometimes use the notation  when needed. Note that, for different values of  and ,  are not generally disjoint but allow us to lower bound the error probability in terms of pairwise error probabilities, namely, a lower bound can be written as:

And we can readily lower bound the expected tracking error:


Next we separate out the effect of each sensor on the tracking error:

where  is the all zero vector designating that all sensors will be awake at the next time slot. The inequality in
(a) follows from the fact that if we separate out the effect of the -th sensor we get a better tracking performance when all the remaining sensors are awake. Since this holds for every , a lower bound on the expected tracking error can be written as a convex combination of all sensors contributions:

where .

Let  denote a vector of length  with all entries equal to zero except for the -th entry which can be anything greater than . Then replacing from (\ref{eq:lb_tracking}),


To simplify notation, we introduce the following  quantities:



Intuitively,  represents the contribution of sensor  to the total expected tracking cost when the underlying state is , the belief is  and when all sensors are awake. On the other hand  is the -th sensor contribution when it is asleep and all the other sensors are awake.

Now if we assume that the target will be perfectly observable after taking the sleeping action, a lower bound on the total cost can be obtained from the solution of the following Bellman equation:

where,

Note that if we can solve the equation above for  for all , then it is straightforward to find the solution for all other values of . We therefore focus on specifying the value function at those points. Since this is the case, we further simplify our notation and use  and  as shorthand for  and , respectively. Also since an action only needs to be made when the sensor wakes up, we only need to define actions at . Observing that

and

we recursively substitute from (\ref{eq,recursive_any_u}) and (\ref{eq,recursive_u_equals_1}) in (\ref{eq:lb_valfunc_per_sensor}) until the system reaches . We can see that a lower bound on the value function of sensor  can be obtained as a solution of the following minimization problem over , where  is the control action for sensor  given a belief state 


Equation (\ref{eq,final_lb_per_sensor}) together with (\ref{eq,decomposed_valfunc}) define a lower bound on the total expected cost. To further tighten the bound we can now optimize over a matrix  for every value of , where  is an  matrix with the  entry equal to , i.e., . Hence,


where  is a column vector of all ones of length . A closed form solution for (\ref{eq:final_lb}) cannot be obtained, and hence, we solve for  numerically. First, we fix  and use policy iteration \cite{bertsekas07} to solve for the control of each sensor at each state. Then, we change  and repeat the process. The envelope of the generated value functions (corresponding to different instants of ) is hence a lower bound on the optimal value function.

\section{Numerical Results} \label{sec:num_res}
In this section, we show some simulation results illustrating the performance of the policies we derived in previous sections.  These results will be for one-dimensional sensor networks, but the general behavior should extend to two-dimensional networks.  In each simulation run, the object was initially placed at the center of the network and the location of the object was made known to each sensor.  A simulation run concluded when the object left the network.  The results of many simulation runs were then averaged to compute an average tracking cost and an average energy cost.  To allow for easier interpretation of our results, we then normalized our costs by dividing by the {\em expected} time the object spends in the network.  We refer to these normalized costs as costs per unit time, even though the true costs per unit time would use the {\em actual} times the object spent in the network (the difference between the two was found to be small).

For the non-learning policies, the value of  for each  and  was generated using 200 Monte Carlo simulations.  The results of 50 simulation runs were averaged when plotting the curves.  For the learning policies, the values for  were initialized to those obtained from the non-learning approach using greedy sensor selection as a baseline.  A constant step size of 0.01 was used in the learning algorithm.  First, 100 simulation runs were performed but the results were not recorded while the values for   stabilized.  Then an additional 50 simulation runs were performed ( continued to be updated) and these results were averaged when plotting curves.  In the case of \QMDP{} learning policies, computation time was saved by performing policy iteration only after every fifth simulation run.

We first consider a simple network that we term Network~A.  This is a one-dimensional network with 41 possible object locations where the object moves with equal probability either one to the left or one to the right in each time step.  There is a sensor at each of the 41 object locations that makes (when awake) a binary observation that determines without error whether the object is at that location.  Hamming cost is used for the tracking cost.

For Network~A, we illustrate the performance of the \QMDP{} versions of our policies in Figure~\ref{fig:hard41}(a) and the FCR versions of our policies in Figure~\ref{fig:hard41}(b).

\begin{figure}
\centering
\begin{tabular}{cc}
\epsfig{file=hard41q_new.eps,width=0.45\linewidth} &
\epsfig{file=hard41f_new.eps,width=0.45\linewidth} \\
\mbox{(a)} & \mbox{(b)}
\end{tabular}
\caption{Tradeoff curves for Network~A: (a) \QMDP{} policies; (b) FCR policies}
\label{fig:hard41}
\end{figure}
The curves labeled ``Asleep'' are for the nonlearning approach for computing  where we assume that all sensors are asleep as a baseline.  The curves labeled ``Greedy'' are for the nonlearning approach for computing  where we use a greedy algorithm to determine our baseline.  The curves labeled ``Learning'' employ our learning algorithm for computing .


From the tradeoff curves, it is apparent that using the learning algorithm to compute  results in improved performance.  A close inspection of Figures~\ref{fig:hard41}(a) and \ref{fig:hard41}(b) will reveal that the \QMDP{} policies perform somewhat better than their FCR counterparts.  This is consistent with what was observed in \cite{fuemmeler08}.

It is instructive to consider the final matrix of values for  that was obtained at the end of all learning algorithm simulations.  In Figures~\ref{fig:T41c0} and \ref{fig:T41c1} we plot this matrix for the \QMDP{} learning policy simulations for the smallest  and for the largest  used in simulation, respectively.
\begin{figure}
   \begin{center}
      \includegraphics[height=3.25in]{T41c0.eps}
      \caption{The final matrix for  for the \QMDP{} learning policy and small  for Network~A. \label{fig:T41c0}}
   \end{center}
\end{figure}
\begin{figure}
   \begin{center}
      \includegraphics[height=3.25in]{T41c1.eps}
      \caption{The final matrix for  for the \QMDP{} learning policy and large  for Network~A. \label{fig:T41c1}}
   \end{center}
\end{figure}
In Figure~\ref{fig:T41c0}, it is evident that only a single sensor has an impact for each value of .  Due to the way our simulations worked, it is the sensor to the left that has the impact, but it could just as easily be the sensor to the right of the current object position. The fact that most of the nonzero values of the matrix are less than 0.5 reflects the fact that the sensor to the right of the current object location might wake up due to a sleep time selected at a previous time step.  In Figure~\ref{fig:T41c1}, it is evident that the sensors on either side of the current object location (which is actually not known since Figure~\ref{fig:T41c1} corresponds to the case where no sensors are awake) appear to have a major impact on the tracking cost.  There are nonzero values off the two main diagonals due to probabilistic nature of the learning process when the actual object location is not known.

\begin{table}
   \begin{center}
      \caption{Object movement for Network~B. \label{tab:moves}}
      \begin{tabular}{|l|r|r|r|r|}
         \hline
         {\em Change in Position} & 0 & 1 & 2 & 3  \\ \hline
         {\em Probability} & 0.3125 & 0.2344 & 0.0938 & 0.0156 \\ \hline
      \end{tabular}
   \end{center}
\end{table}


We now consider a new one-dimensional network termed Network~B.  The possible object locations are located on the integers from 1 to 21.  The object moves according to a random walk anywhere from three steps to the left to three steps to the right in each time step. The distribution of these movements is given in table \ref{tab:moves}. The change in position indicate movement by a corresponding number of steps to the right or to the left. There are 10 sensors in this network so that . The locations of the sensors are given in Table~\ref{tab:sensloc} and awake sensors make Gaussian observations as in (\ref{eq:gaussian_obs}).
\begin{table}
   \begin{center}
      \caption{Sensor locations for Network~B. \label{tab:sensloc}}
      \begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|}
         \hline
         {\em Sensor} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
         {\em Location} & 1.36 & 1.61 & 3.91 & 8.09 & 11.96 & 13.39 & 13.52 & 13.66 & 16.60 & 18.68 \\ \hline
      \end{tabular}
   \end{center}
\end{table}

Results for the \QMDP{} and FCR versions of our policies are shown in Figures~\ref{fig:hard21}(a) and \ref{fig:hard21}(b), respectively.
\begin{figure}
\centering
\begin{tabular}{cc}
\epsfig{file=hard21q_newest.eps,width=0.45\linewidth} &
\epsfig{file=hard21f_newest.eps,width=0.45\linewidth} \\
\mbox{(a)} & \mbox{(b)}
\end{tabular}
\caption{Tradeoff curves for Network~B and a lower bound: (a) \QMDP{} policies; (b) FCR policies}
\label{fig:hard21}
\end{figure}
The results confirm the same general trends observed for Network~A. The figures also show our derived lower bound on the energy-tracking tradeoff using the approach described in Sec. \ref{sec:lower_bound}. Not surprisingly, the lower bound is particularly loose at the high tracking cost regime, yet the gap is reasonably small for the low tracking error region. This is expected since the lower bound uses an all-awake assumption to lower bound the contribution of each sensor to the tracking error. However, it is worth mentioning that we can exactly compute the saturation point for the optimal scheduling policy, which matches the saturation limit of the shown curves, since every policy has to eventually meet the all-asleep performance curve when the energy cost per sensor is high. At that point, all sensors are put to sleep and hence the target estimate can only be based on prior information. The small gap at the low tracking error regime combined with the aforementioned saturation effect highlight good performance for our sleeping policies. For illustration, we plot the matrix for  for the \QMDP{} learning policy simulations for the smallest  and for the largest  when the object moves according to a symmetric random walk in Figures~\ref{fig:T21c0} and \ref{fig:T21c1}, respectively.
\begin{figure}
   \begin{center}
      \includegraphics[height=3.25in]{T21c0.eps}
      \caption{The final matrix for  for the \QMDP{} learning policy and small  for Network~B. \label{fig:T21c0}}
   \end{center}
\end{figure}
\begin{figure}
   \begin{center}
      \includegraphics[height=3.25in]{T21c1.eps}
      \caption{The final matrix for  for the \QMDP{} learning policy and large  for Network~B. \label{fig:T21c1}}
   \end{center}
\end{figure}
Note the difference between the rows corresponding to object locations 7 and 8 in Figure~\ref{fig:T21c0}.  Examining the sensor locations, we see that sensor 4 is located at 8.09.  This sensor is useful for distinguishing between object locations 6 and 8 (for an initial object position of 7) but is of less value for distinguishing between object locations 7 and 9 (for an initial object position of 8).  This is evidenced in the figure as a large value for  and a small value for .

To demonstrate that our techniques can be applied to an object that moves on a continuum, we define a new network, Network~C.  This network is identical to Network~B except for two changes.  First, the object can take locations anywhere on the interval .  Second, the object moves according to Brownian motion with the change in position between time steps having a Gaussian distribution with mean zero and variance 1.  As mentioned earlier, only FCR policies can be generated for this type of network.  Values of  were computed for each integer-valued object location on  and linear interpolation used to compute values of  for other object locations.  Since continuous distributions cannot be easily stored, particle filtering techniques were employed (e.g., see \cite{doucet01}).  The number of particles used was 512 and resampling was performed at each time step.  As is consistent with particle filtering, in generating the sleep times the computation of future probability distributions was approximated through Monte Carlo movement of the particles.  The number of simulation runs that were averaged for each data point was increased to 200 for these simulations.

Tradeoff curves for Network~C are shown in Figure~\ref{fig:vhard21}.
\begin{figure}
   \begin{center}
      \includegraphics[height=3.25in]{vhard21.eps}
      \caption{Tradeoff curves for FCR policies for Network~C. \label{fig:vhard21}}
   \end{center}
\end{figure}
Although the tradeoff curves are less smooth than before, this figure illustrates performance trends similar to those already seen.  The reason the curves are not as smooth is that occasionally the particle filter would fail to keep track of the distribution with sufficient accuracy.  This would cause the network to lose track of the object and cause abnormally bad tracking for that simulation run.  These outliers were not removed when generating the tradeoff curves.  A recovery mechanism would need to be added to the sleeping policies to overcome this limitation of particle filters.

\section{Conclusion} \label{sec:concl}
In this paper, we considered energy-efficient tracking of an object moving through a network of wireless sensors. While an optimal solution could not be found, it was possible to design suboptimal, yet efficient, sleeping solutions for general motion, sensing, and cost models. We proposed \QMDP{} and FCR approximate policies, where in the former, the system is assumed to be perfectly observable after control, and in the latter, to be  totally unobservable. We combined these approximations with a decomposition of the optimization problem into simpler per-sensor subproblems, and developed learning and non-learning based approaches to compute the parameters of each subproblem. The learning-based \QMDP{} policies were shown to provide the best energy-tracking tradeoff. In the low tracking error regime, our sleeping policies approach a derived lower bound on the optimal energy-tracking tradeoff. 

Avenues for future research include developing distributed sleeping strategies in the absence of central control and solving the tracking problem for unknown or partially known object movement statistics.

\bibliographystyle{IEEEtran}
\bibliography{phdthesis}

\end{document}
