 \def\year{2020}\relax
\documentclass[letterpaper]{article} \usepackage{aaai20}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{graphicx}  \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{tabu}
\usepackage{array,booktabs,makecell}
\usepackage{subfig}
\usepackage[reqno]{amsmath}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{textcomp}
\allowdisplaybreaks[1]
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\dashrule}[1][black]{\color{#1}\rule[\dimexpr.5ex-.2pt]{4pt}{.4pt}\xleaders\hbox{\rule{4pt}{0pt}\rule[\dimexpr.5ex-.2pt]{4pt}{.4pt}}\hfill\kern0pt}
\newcommand{\newcite}[1]{\citeauthor{#1} \shortcite{#1}}
\pdfinfo{
/Title (Syntactically Look-Ahead Attention Network for Sentence Compression)
/Author (Hidetaka Kamigaito, Manabu Okumura)
}
\setlength\titlebox{2.5in} 
\title{Syntactically Look-Ahead Attention Network for Sentence Compression}
\author{
    Hidetaka Kamigaito, Manabu Okumura\\
    Institute of Innovative Research, Tokyo Institute of Technology\\
 kamigaito@lr.pi.titech.ac.jp, oku@pi.titech.ac.jp
}

\begin{document}

\maketitle

\begin{abstract}
Sentence compression is the task of compressing a long sentence into a short one by deleting redundant words.
In sequence-to-sequence (Seq2Seq) based models, the decoder unidirectionally decides to retain or delete words.
Thus, it cannot usually explicitly capture the relationships between decoded words and unseen words that will be decoded in the future time steps.
Therefore, to avoid generating ungrammatical sentences, the decoder sometimes drops important words in compressing sentences. 
To solve this problem, we propose a novel Seq2Seq model, \textit{syntactically look-ahead attention network} (SLAHAN), that can generate informative summaries by explicitly tracking both dependency parent and child words during decoding and capturing important words that will be decoded in the future.
The results of the automatic evaluation on the Google sentence compression
dataset showed that SLAHAN achieved the best kept-token-based-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores of 85.5, 79.3, 71.3 and 79.1, respectively.
SLAHAN also improved the summarization performance on longer sentences.
Furthermore, in the human evaluation, SLAHAN improved informativeness without losing readability.
\end{abstract}

\section{Introduction}

Sentence compression is the task of producing a shorter sentence by deleting words in the input sentence while preserving its grammaticality and important information. To compress a sentence so that it is still grammatical, tree trimming methods \cite{jing:2000:ANLP,knight2000statistics,bergkirkpatrick-gillick-klein:2011:ACL-HLT2011,filippova-altun:2013:EMNLP} have been utilized. However, these methods often suffer from parsing errors.
As an alternative, \newcite{filippova-EtAl:2015:EMNLP} proposed a method based on sequence-to-sequence (Seq2Seq) models that do not rely on parse trees but produce fluent compression. However, the vanilla Seq2Seq model has a problem that it is not so good for compressing longer sentences.

To solve the problem, \newcite{kamigaito-etal-2018-higher} expanded Seq2Seq models to capture the relationships between long-distance
words through recursively tracking dependency parents from a word with their recursive attention module.
Their model learns dependency trees and compresses sentences jointly to avoid the effect of parsing errors.
This improvement enables their model to compress a sentence while preserving the important words and its fluency.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{dependency.pdf}
\caption{An example sentence and its dependency tree during the decoding process.
The gray words represent deleted words, and the words in black frames are currently decoded words.
Already decoded words are underlined.
The tracking of parent nodes is represented as blue edges, and that of child nodes is represented as red edges.
The bold words represent the important words in this sentence.\label{fig:exp}}
\end{figure*}

However, since their method focuses only on parent words, important child words of the currently decoded word would be lost in compressed sentences. That is, in Seq2Seq models, because the decoder unidirectionally compresses sentences,  it cannot usually explicitly capture the relationships between decoded words and unseen words which will be decoded in the future time steps.
As the result, to avoid producing ungrammatical sentences, the decoder sometimes drops important words in compressing sentences.
To solve the problem, we need to track both parent and child words to capture unseen important words that will be decoded in the future time steps.

Fig.\ref{fig:exp} shows an example of sentence compression\footnote{This sentence actually belongs to the test set of the Google sentence compression dataset (\url{https://github.com/google-research-datasets/sentence-compression}).} that needs to track both parent and child words.
Since the input sentence mentions the export of the plane between two countries, we have to retain the name of the plane, import country and export country in the compressed sentence.

When the decoder reads ``Japan'', it should recursively track both the parent and child words of ``Japan''. Then, it can decide to retain ``hold'' that is the parent of ``Japan'' and the syntactic head of the sentence. By retaining ``hold'' in the compressed sentence, it can also retain ``Japan'', ``and'' and ``India'' because these are the child and grandchild of ``holdâ€™' (the top case in Fig.\ref{fig:exp}).

When the decoder reads ``hold'', it should find the important phrase ``Japan's
export'' by recursively tracking child words from ``hold''. The tracking also supports the decoder for retaining ``talks'' and ``on'' to produce grammatical compression (the middle case).

When the decoder reads ``export'', it should track child words to find the important phrase ``US2 rescue plane'' and retain ``of'' for producing grammatical compression (the bottom case). 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{freq.pdf}
    \caption{The proportion of words retained later that are linked from right to the retained words in the summary as a parent or a child word in the left-to-right decoding.\label{fig:length}}
\end{figure}

Note that a decoder that tracks only parent words cannot
find the important phrases or produce grammatical compression in this example.
Furthermore, Fig.\ref{fig:length} shows that tracking only parent words is not sufficient for Seq2Seq models to cover explicitly important words which will be decoded in the future time steps, especially in long sentences\footnote{This statistic is calculated on the gold compression and its dependency parse result, which are contained in the training set of the Google sentence compression dataset.}.

To incorporate this idea into Seq2Seq models, we propose \textit{syntactically look-ahead attention network} (SLAHAN), which can generate informative summaries by considering important words that will be decoded in the future time steps by explicitly tracking both parent and child words during decoding. 
The recursive tracking of dependency trees in SLAHAN is represented as attention distributions and is jointly learned with generating summaries to alleviate the effect of parse errors.
Furthermore, to avoid the bias of parent and child words, the importance of the information from recursively tracked parent and child words is automatically decided with our gate module.

The evaluation results on the Google sentence compression
dataset showed that SLAHAN achieved the best kept-token-based-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores of 85.5, 79.3, 71.3 and 79.1, respectively.
SLAHAN also improved the summarization performance on longer sentences.
In addition, the human evaluation results showed that SLAHAN improved informativeness without losing readability.

\section{Our Base Seq2Seq Model\label{sec:base}}

Sentence compression is a kind of text generation task.
However, it can also be considered as a sequential tagging task, where given a sequence of input tokens , a sentence summarizer predicts an output label  from specific labels (``keep'', ``delete'' or ``end of a sentence'') for each corresponding input token ~(). Note that  is the start symbol of a sentence.

To generate a grammatically correct summary, we choose Seq2Seq models as our base model.
For constructing a robust baseline model, we introduce recently proposed contextualized word embeddings such as ELMo \cite{peters-etal-2018-deep} and BERT \cite{devlin2018bert} into the sentence compression task.
As described later in our evaluation results, this baseline exceeds the state-of-the-art  scores reported by \newcite{zhao-etal-2018-language}.

Our base model consists of embedding, encoder, decoder, and output layers.
In the embedding layer, the model extracts features from an input token  as a vector  as follows:

where  represents the vector concatenation,  is
a vector of the -th feature for token , and  is the number of features (at most 3). We choose features from GloVe \cite{pennington2014glove}, ELMo or BERT vectors.
Because ELMo and BERT have many layers, we treat their weighted sum as  as follows:

where  represents the
-th layer of the -th feature for the token , and  is the weight vector for
the -th layer of the -th feature. In BERT, to align the input token and the output label, we treat the average of sub-word vectors as a single word vector.

The encoder layer first converts  into a hidden state  by using forward-LSTM, and  is calculated similarly by using backward-LSTM.
Secondly,  and  are concatenated as .
Through this process, the encoder layer converts the embedding  into a sequence of hidden states:

The final state of the backward LSTM  is inherited by the decoder as its initial state.

At time step , the decoder layer encodes the concatenation of a 3-bit one-hot vector determined by the predicted label , the final hidden state  (which we will explain later), 
and the token embedding  into the decoder hidden state , by using a forward-LSTM.

The output layer predicts an output label probability as follows:

where  is the weight matrix,  is the bias term,  is the weight matrix of the softmax layer, and  is the
binary vector where the -th element is set to  and the other elements are set to .

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{network.pdf}
    \caption{The entire network structure of our Syntactically Look-Ahead Attention Network (SLAHAN).}
    \label{fig:my_label}
\end{figure*}

\section{Syntactically Look-Ahead Attention Network}
\label{sec:slahan}
In this section, we first explain the graph representation for a dependency tree that is used in SLAHAN and then introduce its entire network structure and the modules inside it.
Both the graph representation and network parameters are jointly updated, as described in the later section.

\subsection{Graph Representation of Dependency Relationships}
We explain the details of our representation for tracking parent and child words from a word in a dependency tree.
As described in \newcite{hashimoto-tsuruoka:2017:EMNLP2017}, a dependency relationship can be represented as a weighted graph.
Given sentence , the parent of each word  is selected from . We treat  as a root node.
We represent the probability of  being the parent of  in  as .
By using , \newcite{kamigaito-etal-2018-higher} show that , a probability of  being the -th order parent of , is calculated as follows:

Because the 1st line of Eq.(\ref{eqn:recatt}) is a definition of matrix multiplication,
by using a matrix , which satisfies ,
Eq.(\ref{eqn:recatt}) is reformulated as follows:

We call  the \textit{-th parent graph} hereafter.

We expand Eq.(\ref{eq:recatt:matmul}) to capture -th child words of a word .
At first, we define , the probability of  being a child of  in , , the probability of  being a parent word in , , the probability of  being a child word in , and , the  probability of  and  having a link in .
Assuming the probability of words having a link is independent of each other, the following equations are satisfied: 

This can be reformulated as follows:

Here,  is always  because of the dependency tree definition, and in this  formulation, we treat  as a parent;
thus,  is a constant value.
Therefore, we can obtain the following relationship:

Based on Eq.(\ref{eq:prop}), we can define , the strength of  being the -th order child of , as follows:

Similar to Eq.(\ref{eqn:recatt}), by using a matrix , which satisfies ,
Eq.(\ref{eqn:recatt:trans}) is reformulated as follows:

We call  the \textit{-th child graph} hereafter.
Note that from the definition of the 2nd lines in Eq.(\ref{eqn:recatt}) and Eq.(\ref{eqn:recatt:trans}),  and  always satisfy . This can be reformulated as .
Furthermore, from the definition of the transpose of a matrix, we can obtain the following formulation:

Thus, once we calculate Eq.(\ref{eq:recatt:matmul}), we do not need to compute Eq.(\ref{eqn:recatt:trans:matmul}) explicitly.
Therefore, letting  be a dimension size of hidden vectors, the computational cost of SLAHAN is , similar to \newcite{kamigaito-etal-2018-higher}.
This is based on the assumption that  is larger than  in many cases.
Note that the computational cost of the base model is . 

\subsection{Network Structure}
Fig.\ref{fig:my_label} shows the entire structure of SLAHAN.
It is constructed on our base model, as described in the previous section.
After encoding the input sentence, the hidden states are passed to our network modules.
The functions of each module are as follows:\\
{\bf Head Attention} module makes a dependency graph of a sentence by calculating the probability of  being the parent of  based on  and  in Eq.(\ref{eq:enc_hidden}) for each . \\
{\bf Parent Recursive Attention} module calculates -th parent graph  and extracts a weighted sum of important hidden states  from  in Eq.(\ref{eq:enc_hidden}) based on  () for each decoder time step . \\
{\bf Child Recursive Attention} module uses -th child graph  to extract , a weighted sum of important hidden states from  in Eq.(\ref{eq:enc_hidden}) based on  () for each decoder time step . \\
{\bf Selective Gate} module supports the decoder to capture important words that will be decoded in the future by calculating , the weighted sum of  and , based on the current context.  is inherited to the decoder for deciding the output label . 

The details of each module are described in the following subsections.

\subsubsection{Head Attention}
Similar to \newcite{zhang-cheng-lapata:2017:EACLlong}, we calculate  as follows:

where ,  and  are weight matrices of .
In a dependency tree, the {\it root} has no parent, and a token does not depend on itself.
In order to satisfy these rules, we impose the following constraints on :

The 1st and 2nd lines of Eq.(\ref{eqn:recatt:const}) represent the case where the parent of {\it root} is also {\it root}.
These imply that {\it root} does not have a parent.
The 3rd line of Eq.(\ref{eqn:recatt:const}) prevents a token from depending on itself.
In the training phase,  is jointly learned with output label probability , as described in the objective function section.

\subsubsection{Parent Recursive Attention}

The parent recursive attention module recursively calculates  by using  based on Eq.(\ref{eqn:recatt}).
The calculated  is used to weight the bi-LSTM hidden layer  as follows:


To select suitable dependency order  for the input sentence,  is  further weighted and summed to  by using weighting parameter , according to the current context as follows:

where  is the weight matrix,  is the group of dependency orders, and  is the vector representing the current context.

\subsubsection{Child Recursive Attention}

The child recursive attention module weights the bi-LSTM hidden layer  based on -th child graph .
Unlike the parent recursive attention module,  is not a probability, and a word sometimes has more than two children.
For that reason, we use max-pooling rather than the attention distribution in Eq.(\ref{eqn:rec:par:softmax}).
In the child recursive attention module, the bi-LSTM hidden layer  is weighted by  and then pooled as follows:


To select suitable dependency order  for the input sentence,  is further weighted and summed to  by using weighting parameter , according to the current context as follows:

where  is the weight matrix.

\subsubsection{Selective Gate}
This module calculates , a weighted sum of parent information  and child information . The weight is decided by a gate  by considering whether  or  is more important in the current context.
Specifically,  is calculated as follows:

where  is the element-wise product,  is the sigmoid function, and  is the weight matrix.
Then,  in Eq.(\ref{eqn:lasth}) is replaced by a concatenated vector ; furthermore, instead of ,  is also fed to the decoder input at . 

\subsection{Objective Function}
\label{sec:objfunc}
To alleviate the effect of parse errors, we jointly update dependency parent probability  and label probability  \cite{kamigaito-etal-2017-supervised}.
We denote the existence of an edge between parent word  and child word  on a dependency tree as .
In contrast, we denote the absence of an edge as . By using these notations, our objective function is defined as follows:

where  is a hyper-parameter balancing the importance of output labels and parse trees in training steps.
To investigate the importance of the syntactic information, we used  for the \textit{with syntax (w/ syn)} setting and  for the \textit{without syntax (w/o syn)} setting.

\section{Experiments}
For comparing our proposed models with the baselines, we conducted both automatic and human evaluations. The following subsections describe the evaluation details.
\subsection{Settings}
\subsubsection{Datasets}

We used the Google sentence compression dataset (Google dataset) \cite{filippova-altun:2013:EMNLP} for our evaluations. To evaluate the performances on an out-of-domain dataset, we also used the Broadcast News Compression Corpus (BNC Corpus)\footnote{\url{https://www.jamesclarke.net/research/resources}}.
The setting for these datasets is as follows:\\
{\bf Google dataset:} Similar to the previous researches \cite{filippova-EtAl:2015:EMNLP,Tran:2016:EAN:3011077.3011111,wang-EtAl:2017:Long4,kamigaito-etal-2018-higher,zhao-etal-2018-language}, we used the first 1,000 sentences of {\sl comp-data.eval.json} as the test set.
We used the last 1,000 sentences of {\sl comp-data.eval.json} as our development set.
Following recent researches \cite{kamigaito-etal-2018-higher,zhao-etal-2018-language}, we used all 200,000 sentences in {\sl sent-comp.train*.json} as our training set.
We also used the dependency trees contained in this dataset.

To investigate the summarization performances on long sentences, we additionally performed evaluations on 417 sentences that are longer than the average sentence length () in the test set.\\
{\bf BNC Corpus:} This dataset contains spoken sentences and their summaries created by three annotators. 
To evaluate the compression performances on long sentences in the out-of-domain setting, we treated sentences longer than the average sentence length, , as the test set (595 sentences), and training was conducted with the Google dataset.
Because this dataset does not contain any dependency parsing results, we parsed all sentences in this dataset by using the Stanford dependency parser\footnote{\url{https://nlp.stanford.edu/software/}}.
In all evaluations, we report the average scores for three annotators.

\subsubsection{Compared Models}
The baseline models are as follows. We used ELMo, BERT and GloVe vectors for all models in our experiments.\\
{\bf Tagger:} This is a bi-LSTM tagger which is used in various sentence summarization researches \cite{klerke-goldberg-sogaard:2016:N16-1,wang-EtAl:2017:Long4}.\\
{\bf LSTM:} This is an LSTM-based sentence summarizer, which was proposed by \newcite{filippova-EtAl:2015:EMNLP}.\\
{\bf LSTM-Dep:} This is an LSTM-based sentence summarizer with dependency features, called LSTM-Par-Pres in \newcite{filippova-EtAl:2015:EMNLP}.\\
{\bf Base:} Our base model explained in the 2nd section.\\
{\bf Attn:} This is an improved attention-based Seq2Seq model with ConCat attention, described in \newcite{luong-pham-manning:2015:EMNLP}. To capture the context of long sentences, we also feed input embedding into the decoder, similar to the study of \newcite{filippova-EtAl:2015:EMNLP}.\\
{\bf Parent:} This is a variant of SLAHAN that does not have the child recursive attention module. This model captures only parent words, similar to HiSAN in the study of \newcite{kamigaito-etal-2018-higher}. For the fair comparisons, we left the gate layer in Eq.(\ref{eqn:gate}).

Our proposed models are as follows:\\
{\bf SLAHAN:} This is our proposed model which is described in the 3rd section.\\
{\bf Child:} This is a variant of SLAHAN that does not have the parent recursive attention module. Similar to {\bf Parent}, we left the gate layer in Eq.(\ref{eqn:gate}).

\begin{table}[t]
    \centering
\resizebox {0.8\columnwidth} {!} {
\small
    \begin{tabular}{|l|c|c|c|c|c|c|c|}
         \hline
         Glove              &  &  &              &  &  &              &              \\
         ELMo               &  &  &  &              &              &  &              \\
         BERT               &  &              &  &  &              &              &  \\
         \hline
            &  & 86.0 & 85.9 & 85.4 & 85.5 & 85.9 & 84.8 \\
         \hline
    \end{tabular}
}
    \caption{ scores for \textbf{Base} with various features in the development data. The bold score represents the highest score. \label{tb:eval:dev}}
\end{table}
\begin{table*}[t]
\centering
\resizebox {0.8\textwidth} {!} {
\small
\begin{tabular}{llcccccccccccc}
\toprule
& & \multicolumn{5}{c}{\textbf{ALL}} & \multicolumn{5}{c}{\textbf{LONG}}
\\
\cmidrule(lr){3-7} \cmidrule(lr){8-12}
 & &  & \textbf{R-1} & \textbf{R-2} & \textbf{R-L} &  &  & \textbf{R-1} & \textbf{R-2} & \textbf{R-L} &  \\
\midrule
\multicolumn{2}{l}{Evaluator-LM \cite{zhao-etal-2018-language}}    & 85.0 & - & - & - & -2.7 & - & - & - & - & -\\
\multicolumn{2}{l}{Evaluator-SLM \cite{zhao-etal-2018-language}}    & 85.1 & - & - & - & -4.7 & - & - & - & - & -\\
\midrule
\multicolumn{2}{l}{Tagger}  & 85.0 & 78.1 & 69.9 & 77.9 & -3.1 & 83.0 & 75.4 & 66.8 & 74.9 & -3.1\\
\multicolumn{2}{l}{LSTM}    & 84.8 & 77.7 & 69.6 & 77.4 & -3.4 & 82.7 & 74.8 & 66.3 & 74.4 & -3.5\\
\multicolumn{2}{l}{LSTM-Dep}& 84.7 & 77.8 & 69.7 & 77.5 & -3.3 & 82.6 & 74.9 & 66.5 & 74.4 & -3.3\\
\multicolumn{2}{l}{Attn}    & 84.5 & 77.3 & 69.3 & 77.1 & -3.8 & 82.3 & 74.7 & 66.4 & 74.3 & -3.6\\
\multicolumn{2}{l}{Base}    & 85.4 & 78.5 & 70.4 & 78.2 & -2.9 & 83.4 & 75.8 & 67.4 & 75.3 & -3.0\\
Parent & w/ syn             & 85.0 & 78.3 & 70.3 & 78.1 & -2.5 & 82.8 & 75.3 & 67.0 & 74.9 & -2.9\\
Parent & w/o syn            & 85.3 & 78.3 & 70.4 & 78.1 & -3.4 & 83.3 & 75.6 & 67.3 & 75.2 & -3.4\\
\midrule
Child & w/ syn              & 85.4 & 78.8 & 70.7 & 78.5 & -2.9 & 83.0 & 75.8 & 67.3 & 75.4 & -3.0\\
Child & w/o syn             & 85.2 & 78.6 & 70.8 & 78.4 & -3.1 & 83.2 & 76.3 & 68.2 & 75.8 & -2.8\\
SLAHAN & w/ syn             &  &  &  &  &  & 83.3 &  & 68.3 &  & \\
SLAHAN & w/o syn            & 85.4 &  &  &  & -3.0 &  &  &  &  & -2.9\\
\bottomrule
\end{tabular}
}
    \caption{Results on the Google dataset.  and  represent, respectively, the results for all sentences and only for long sentences (longer than average length 27.04) in the test dataset. The bold values indicate the best scores.  indicates that the difference of the score from the best baseline (mostly Base) is statistically significant.\footnotemark[8]} \label{tb:eval:google}
\end{table*}
\begin{table}[t]
\centering
\small
\resizebox {0.825\columnwidth} {!} {
\begin{tabular}{llccccc}
\toprule
 & &  & \textbf{R-1} & \textbf{R-2} & \textbf{R-L} &  \\
\midrule
\multicolumn{2}{l}{Tagger}  & 54.6 & 36.8 & 27.7 & 36.4 & -39.1\\
\multicolumn{2}{l}{LSTM}    & 54.8 & 36.6 & 28.0 & 36.2 & -39.2\\
\multicolumn{2}{l}{LSTM-Dep}& 55.1 & 36.9 & 28.2 & 36.5 & -38.8\\
\multicolumn{2}{l}{Attn}    & 54.1 & 36.1 & 27.4 & 35.6 & -39.6\\
\multicolumn{2}{l}{Base}    & 55.4 & 37.4 & 28.5 & 36.9 & -38.6\\
Parent & w/ syn             & 54.2 & 36.3 & 27.7 & 35.9 & -39.1\\
Parent & w/o syn            & 54.0 & 35.8 & 27.2 & 35.4 & -40.1\\
\midrule
Child & w/ syn              & 55.6 & 37.8 & 28.5 & 37.3 & -38.2\\
Child & w/o syn             & 54.8 & 36.7 & 28.1 & 36.3 & -39.2\\
SLAHAN & w/ syn             &  &  &  &  & \\
SLAHAN & w/o syn            & 54.6 & 36.4 & 27.8 & 36.0 & -39.5\\
\bottomrule
\end{tabular}}
    \caption{Results on the BNC Corpus.  indicates the same as in Table \ref{tb:eval:google}.\label{tb:eval:bnc}}
\end{table}

\subsubsection{Model Parameters}

We used GloVe ({\sl glove.840B.300d}), 3-layers of ELMo and 12-layers of BERT ({\sl cased\_L-12\_H-768\_A-12}) as our features.
We first investigated the best combination of GloVe, ELMo, and BERT vectors as shown in Table \ref{tb:eval:dev}.
Following this result, we used the combination of all of GloVe, ELMo and BERT for all models.

The dimensions of the LSTM layer and the attention layer were set to 200.
The depth of the LSTM layer was set to 2.
These sizes were based on the setting of the LSTM NER tagger with ELMo in the study of \newcite{peters-etal-2018-deep}.
All parameters were initialized with \newcite{glorot2010understanding}'s method.
For all methods, we applied Dropout \cite{srivastava2014dropout} to the input of the LSTM layers.
All dropout rates were set to 0.3.
We used Adam \cite{DBLP:journals/corr/KingmaB14} with an initial learning rate of 0.001 as our optimizer.
All gradients were averaged by the number of sentences in each mini-batch.
The clipping threshold value for the gradients was set to 5.0.
The maximum training epoch was set to 20.
We used  as  in Eq.(\ref{eqn:rec:parent:order:softmax}) and  Eq.(\ref{eqn:rec:child:order:softmax}). 
The maximum mini-batch size was set to 16, and the order of mini-batches was shuffled at the end of each training epoch.
We adopted early stopping to the models based on maximizing per-sentence accuracy (i.e., how many summaries are fully reproduced) of the development data set.

To obtain a compressed sentence, we used greedy decoding, following the previous research \cite{kamigaito-etal-2018-higher}.
We used Dynet \cite{neubig2017dynet} to implement our neural networks\footnote{Our implementation is publicly available on GitHub at \newline\url{https://github.com/kamigaito/slahan}.}.

\subsection{Automatic Evaluation}

\subsubsection{Evaluation Metrics}

In the evaluation, we used kept-token-based-F measures () for comparing to the previously reported scores.
In this metric, precision is defined as the ratio of kept tokens that overlap with the gold summary, and recall is defined as the ratio of tokens in the gold summary that overlap with the system output summary.
For more concrete evaluations, we additionally used ROUGE-1 (\textbf{R-1}), ROUGE-2 (\textbf{R-2}), and ROUGE-L (\textbf{R-L}) \cite{lin-och:2004:ACL}\footnote{We used the ROUGE-1.5.5 script with option ``-n 2 -m -d -a''.} with limitation by reference byte lengths\footnote{If a system output exceeds the reference summary byte length, we truncated the exceeding tokens.} as evaluation metrics.
We used  \cite{kamigaito-etal-2018-higher} to evaluate how close the compression ratio of system outputs was to that of gold compressed sentences.
Note that the gold compression ratios of all the sentences and the long sentences in the Google test set are respectively  and . Those of all the sentences and the long sentences in the BNC corpus are respectively  and .
We used the macro-average for all reported scores.
All scores are reported as the average scores of three randomly initialized trials.

\subsubsection{Results}

Table \ref{tb:eval:google} shows the evaluation results on the Google dataset. 
\textbf{SLAHAN} achieved the best scores on both all the sentences and the long sentences. Through these gains, we can understand that \textbf{SLAHAN} successfully captures important words by tracking both parent and child words.
\textbf{Child} achieved better scores than \textbf{Parent}. This result coincides with our investigation that tracking child words is important especially for long sentences, as shown in Fig.\ref{fig:length}. 
We can also observe the score of \textbf{SLAHAN w/o syn} is comparable to that of \textbf{SLAHAN w/ syn}.
This result indicates that dependency graphs can work on the in-domain dataset without relying on given dependency parse trees.
\begin{table}[!t]
\centering
\small
\resizebox {0.6\columnwidth} {!} {
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{} & \multicolumn{2}{c}{} \\
\midrule
Tagger       & 3.90 & \hspace{-3mm}(73.4) & \hspace{-1mm}3.79 & \hspace{-3mm}(72.9) \\
Base        & 3.86 & \hspace{-3mm}(72.4) & \hspace{-1mm}3.80 & \hspace{-3mm}(73.6) \\
Parent w/ syn & 3.82 & \hspace{-3mm}(70.5) & \hspace{-1mm}3.77 & \hspace{-3mm}(71.5) \\
\midrule
Child w/ syn &  & \hspace{-3mm}() & \hspace{-1mm} & \hspace{-3mm}(74.9) \\
SLAHAN w/ syn & 3.91 & \hspace{-3mm}(74.8) & \hspace{-1mm} & \hspace{-3mm}() \\
\bottomrule
\end{tabular}
}
    \caption{Results of the human evaluation.
    The numbers in parentheses are the percentages of over four ratings.  indicates the same as in Table \ref{tb:eval:google}. \label{tb:eval:human}}
\end{table}

We also show the evaluation results on the BNC corpus, the out-of-domain dataset, in Table \ref{tb:eval:bnc}.
We can clearly observe that \textbf{SLAHAN w/ syn} outperforms other models for all metrics.
Comparing between \textbf{Base}, \textbf{Parent}, \textbf{Child} and \textbf{SLAHAN}, we can understand that \textbf{SLAHAN w/ syn} captured important words during the decoding step even in the BNC corpus.
The remarkable performance of \textbf{SLAHAN w/ syn} supports the effectiveness of explicit syntactic information.
That is, in the out-of-domain dataset, the dependency graph learned with
implicit syntactic information obtained lower scores than
that learned with explicit syntactic information.
The result agrees with the findings of the previous research \cite{wang-EtAl:2017:Long4}.
From these results, we can conclude that SLAHAN is effective even for both long and out-of-domain sentences.
\begin{table}[t]
    \small
    \centering
    \resizebox {0.9\columnwidth} {!} {
    \begin{tabular}{p{1.1\columnwidth}}
        \toprule
         {\bf Input}: British mobile phone giant Vodafone said Tuesday it was seeking regulatory approval to take full control of its Indian unit for \p<0.05F_{1}\mathbf{F_1}\mathbf{\Delta C}\mathbf{69.4}^{\dagger}\mathbf{57.6}^{\dagger}\mathbf{45.2}^{\dagger}\mathbf{57.3}^{\dagger}\mathbf{-23.7}^{\dagger}\daggerp<0.05\mathbf{\Delta C}\mathbf{\Delta C}\mathbf{\Delta C}\mathbf{-1.6}^{\dagger}\mathbf{-2.5}^{\dagger}\mathbf{\Delta C}\mathbf{\Delta C}\mathbf{-24.8}^{\dagger}\mathbf{-37.2}^{\dagger}$ \\
SLAHAN & w/o syn            & 49.3 & -27.2 & 30.7 & -40.9 \\
\bottomrule
\end{tabular}\caption{Compression ratios in characters on the BNC Corpus. The notations are the same as in Table \ref{tb:eval:bnc:all}. \label{tb:eval:bnc:cr}}
\end{table}

\end{document}