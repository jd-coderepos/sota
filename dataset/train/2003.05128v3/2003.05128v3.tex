
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{latex/cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{subfig}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{dblfloatfix}    \usepackage{array}
\usepackage[font=small,labelfont=bf]{caption}

\newcommand{\drule}{\specialrule{0.2pt}{1pt}{1pt}\specialrule{0.2pt}{0pt}{\belowrulesep}}
\newenvironment{myindentpar}[1]{\begin{list}{}{\setlength{\leftmargin}{#1}}\item[]}
  {\end{list}}
\newcommand*{\affaddr}[1]{#1} \newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}
\usepackage{kotex}
\newcommand{\todoc}[2]{{\textcolor{#1} {\textbf{[[#2]]}}}}
\newcommand{\todoblue}[1]{\todoc{blue}{\textbf{[[#1]]}}}
\newcommand{\modblue}[1]{\todoc{blue}{[[#1]]}}
\newcommand{\todored}[1]{\todoc{red}{\textbf{[[#1]]}}}
\newcommand{\jg}[1]{\todored{JG: #1}}
\newcommand{\sh}[1]{\todoblue{SH: #1}}
\newcommand{\tr}[1]{\todoblue{TR: #1}}
\newcommand{\shmod}[1]{\modblue{#1}}
\newcommand{\az}[1]{\todoblue{AZ: #1}}
\newcommand{\todo}[1]{\textcolor{blue}{#1}}
\newcommand{\todogreen}[1]{\textcolor{cyan}{#1}}





\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\cvprfinalcopy 

\def\cvprPaperID{6529} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}
\title{Cars Can't Fly up in the Sky: Improving Urban-Scene Segmentation via\\Height-driven Attention Networks}

\author{
\textbf{Sungha Choi}\affmark[1,3]\quad\quad\textbf{Joanne T. Kim}\affmark[2,3]\quad\quad \textbf{Jaegul Choo}\affmark[4]
\vspace*{0.1cm}
\\
\affaddr{\affmark[1]A\&B Center, LG Electronics,\, Seoul, South Korea\\
\affmark[2]Lawrence Livermore National Laboratory,\, Livermore, CA, USA\\
\affmark[3]Korea University,\, Seoul, South Korea\quad\quad\affmark[4]KAIST,\, Daejeon, South Korea}\\
\small{\email{\{shachoi,tengyee\}@korea.ac.kr}\quad\email{jchoo@kaist.ac.kr}}\\
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
\vspace*{-0.2cm}
  This paper exploits the intrinsic features of urban-scene images and proposes a general add-on module, called \emph{height-driven attention networks (HANet)}, for improving semantic segmentation for urban-scene images. It 
emphasizes informative features or classes selectively according to the vertical position of a pixel. The pixel-wise class distributions are significantly different from each other among horizontally segmented sections in the urban-scene images. Likewise, urban-scene images have their own distinct characteristics, but most semantic segmentation networks do not reflect such unique attributes in the architecture. The proposed network architecture incorporates the capability exploiting the attributes to handle the urban-scene dataset effectively. We validate the consistent performance (mIoU) increase of various semantic segmentation models on two datasets when HANet is adopted.
This extensive quantitative analysis demonstrates that adding our module to existing models is easy and 
cost-effective.
  Our method achieves a new state-of-the-art performance on the Cityscapes benchmark with a large margin among ResNet-101 based segmentation models.
Also, we show that the proposed model is coherent with the facts observed in the urban scene by visualizing and interpreting the attention map. 
  Our code and trained models are publicly available\footnote{\vspace*{-0.2cm}\url{https://github.com/shachoi/HANet}}.











\end{abstract}
\vspace{-0.5cm}
\begin{figure*}[ht]
  \centering\includegraphics[width=0.95\linewidth]{latex/figures/fig_cvpr_intro_n.pdf}

\vspace*{-0.4cm}
 \caption{Motivation of our approach, the pixel-wise class distributions. All numbers are average values obtained from the entire training set of the Cityscapes dataset with its pixel-level class labels~\cite{cordts2016cityscapes}. Note that there exists a total of 2048K pixels per image, and the y-axis is in log-scale.
(a) Each bar
represents the average number of pixels assigned to each class contained in a single image. 
For example, on average, about 685K pixels per image are assigned to the road class.
(b) Each part of an image divided into three horizontal sections has a significantly different class distribution from each other. For example, the upper region has just 38 pixels of the road class, while the lower region has 480K pixels of it. }
\label{fig:intro}
\vspace*{-0.7cm}
\end{figure*}

\section{Introduction} \label{sec:Intro}
\vspace{-0.1cm}
Semantic image segmentation, a fundamental task in computer vision, is employed for basic urban-scene understanding in autonomous driving. Fully convolutional networks (FCNs)~\cite{long2015fully} are 
seminal work that adopts deep convolutional neural networks (CNNs)
in semantic segmentation, by replacing fully connected layers with convolutional ones at the last stage in typical CNN architectures. Other advanced techniques, such as skip-connections in an encoder-decoder architecture~\cite{badrinarayanan2017segnet,ronneberger2015u,chen2018encoder}, an atrous convolution~\cite{chen2018deeplab, yu2015multi}, an atrous spatial pyramid pooling (ASPP)~\cite{chen2017rethinking}, and a pyramid pooling module~\cite{zhao2017pyramid}, have further improved the FCN-based architecture in terms of semantic segmentation performance. They have proven to be successful in diverse semantic segmentation benchmarks~\cite{everingham2010pascal,pascal-voc-2012,mottaghi2014role,cordts2016cityscapes,Alhaija2018IJCV,lin2014microsoft,neuhold2017mapillary} including urban-scene datasets.


Yet, urban-scene images have their own distinct nature related to perspective geometry~\cite{Li2017FoveaNetPU} and positional patterns~\cite{zou2018unsupervised,chen2018road}. Due to the fact that the urban-scene images are captured by the cameras mounted 
on the front side of a car, the urban-scene datasets consist only of road-driving pictures. This leads to the possibility of incorporating common structural priors depending on a spatial position, markedly 
in a vertical position. To verify this characteristic, we present the class distribution of an urban-scene dataset across vertical positions in Fig.~\ref{fig:intro}. Although the pixels of the few classes
are dominant in an entire region of an image (Fig.~\ref{fig:intro}(a)), the class distribution has significant dependency on a vertical position. That is, a lower part of an image is mainly composed of road, while 
the middle part contains various kinds of relatively small objects. In 
the upper part, buildings, vegetation, and sky are principal objects as shown in Fig.~\ref{fig:intro}(b).


Table~\ref{tab_intro} shows the probabilities with respect to the dominant top-5 classes: road, building, vegetation, car, and sidewalk. The class distribution is extremely imbalanced that the dominant classes take over 88\% of the entire dataset. As mentioned above, the class distribution is completely different if the image is divided into 
three regions: upper, middle, and lower parts. For instance, the probability of the road class  is 36.9\% on average given an entire image, but this chance drops dramatically to 0.006\% for an upper region, while jumps to 87.9\% if a lower region is considered. 

Also, we analyzed this observation using entropy, a measure of uncertainty. The entropy of the probability distribution  of a pixel over 19 classes in the Cityscapes dataset~\cite{cordts2016cityscapes} is computed as\\
\vspace*{-0.5cm}

where  denotes the probability that an arbitrary pixel is assigned to the -th class.
the conditional entropy \big|image of  given an image, computed by Eq.~\eqref{eq_entropy}, is 1.84. On the other hand, the average conditional entropy of  given each of the three regions as \big|upper, \big|middle, and \big|lower, is 1.26 as shown in Table~\ref{tab_intro}. As a result, one can see the uncertainty is reduced if we divide an image into several parts horizontally. Based on this analysis, if we can identify the part of an image to which a given arbitrary pixel belongs, it will be helpful for pixel-level classification in semantic segmentation.
\begin{table}[!b]
\vspace*{-0.4cm}
\begin{center}
\footnotesize
\begin{tabular}{c|ccccc|c|c}
\toprule
\multirow{2}{*}{Given} & \multicolumn{5}{c|}{Probabilities of top-5 classes} & \multicolumn{2}{c}{Entire class} \\\cline{2-6}
&  &   &  &  &  & \multicolumn{2}{c}{entropy}\\
\drule
Image & 36.9 & 22.8 & 15.9 & 7.0 & 6.1 & \multicolumn{2}{c}{1.84} \\
\midrule
Upper & 0.006 & 47.8 & 35.4 & 0.6 & 0.009 & 1.24 & \multirow{3}{*}{\shortstack{1.26\0.3em]
 \,By adding HANet to the baseline, with negligible computational and memory overhead, we achieve a new state-of-the-art performance on the Cityscapes benchmark with a large margin among ResNet-101 based models.
\
    \mathbf{\Tilde{X}}_h=\text{F}_{\text{HANet}}\left(\mathbf{X}_{\ell}\right)\odot\mathbf{X}_h=\mathbf{A}\odot\mathbf{X}_h.

    \mathbf{Z}=\text{G}_{\text{pool}}\left(\mathbf{X}_\ell\right).
\vspace*{-0.1cm}

    \mathbf{Z}_{:,h}=[\frac{1}{W}\sum_{i=1}^W\mathbf{X}_{1,h,i};\dots;\frac{1}{W}\sum_{i=1}^W\mathbf{X}_{C,h,i}].

\mathbf{A}=\text{G}_{\text{up}}\left(\sigma\left(\text{G}_{\text{Conv}}^N\left(\dotsm\delta\left(\text{G}_{\text{Conv}}^1\big(\mathbf{\hat{Z}}\big)\right)\right)\right)\right),

    PE_{(p, 2i)}&=sin\big(p/100^{2i/C}\big)\\
    PE_{(p, 2i+1)}&=cos\big(p/100^{2i/C}\big),

    \Tilde{Q}=Q\oplus PE,
^\ddagger^\dagger^\ddaggerrr\times\times\times\timespKr+$HANet & 82.05 & 98.6 & 87.7 & 93.7 & 66.7 & \textbf{66.2} & 68.7 & 74.4 & 81.9 & 93.3 & \textbf{67.7} & 95.3 & 84.5 & 66.9 & 96.1 & \textbf{87.9} & 92.7 & \textbf{86.0} & \textbf{70.7} & 80.1 \\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.6cm}
\caption{Performance comparison of our methods against the baseline in terms of per-class IoU and mIoU measures. Inference techniques such as sliding, multi-scale, and flipping are applied. ResNet-101, output stride 8 on the Cityscapes validation set.
}
\label{tab_experiment}
\vspace*{-0.2cm}
\end{table*}


\subsection{Height- and width-wise class distribution} \label{app:height_wdith}
\vspace*{-0.1cm}
As shown in Fig.~\ref{fig:row_vs_col}, the width-wise class distributions are relatively similar across columns than the height-wise ones are, so it would be relatively difficult to extract distinct information with respect to the horizontal position of an image. 
Also, empirically, no meaningful performance increase has been observed when using the attention networks exploiting a width-wise class distribution.

This clear pattern corroborates the rationale behind the idea of HANet that extracts and incorporates height-wise contextual information rather than the width-wise one.











\begin{figure}[!t]
\centering
  \includegraphics[width=\linewidth]{latex/figures/fig_class_distibution_n.pdf}
  \caption{Comparison of a height-wise and a width-wise class distributions. A darker color indicates a higher probability (more pixels) assigned to a particular class (from 0 to 18). The height-wise class distributions show distinct patterns across vertical positions while it is not the case for width-wise ones. Normalized distributions of each class are presented on the right column.
}
\label{fig:row_vs_col}
\vspace*{-0.3cm}
\end{figure}

\subsection{Per-class IoU and segmentation maps} \label{app:segmentation_maps}
\vspace*{-0.1cm}
We present per-class IoU and segmentation maps to analyze HANet qualitatively and quantitatively. 
\vspace*{-0.4cm}
\paragraph{Comparison to baseline.}
Table~\ref{tab_experiment} shows the per-class IoU and mIoU results to compare the baseline and our methods in detail. 
Compared to the baseline, all the classes show similar or improved results; up to 7.3\% IoU increase is observed.
Qualitatively, ours can properly distinguish individual objects, even between the classes much alike to each other (Figs.~\ref{fig:vis_result_1} and \ref{fig:vis_result_2}). 
From our result in Fig.~\ref{fig:vis_result_1}(a) and Fig.~\ref{fig:vis_result_2}(a)-(c), one can see that the train, trucks, or cars in a far, crowded region are properly predicted by ours, even if similar vehicles are found nearby. 
Also, vegetation is accurately distinguished from terrain in ours, compared to the baseline (Fig.~\ref{fig:vis_result_1}(b)). 
Another interesting examples are found in  Fig.~\ref{fig:vis_result_2}(e)-(f);
the poles are connected fully in ours but dotted or missed in the baseline. 
We conjecture that HANet helps to distinguish confusing classes by properly gating the activation maps using height-wise contextual information based on their vertical positions. To summarize, compared to the baseline, our method generally forms a clear boundary of a object while avoiding its unnecessary fragmentation into multiple pieces. 









\begin{figure*}[!t]
\centering
  \includegraphics[width=\linewidth]{latex/figures/fig_seg_results1_n.pdf}
  \caption{Comparison of predicted segmentation maps: (a) truck, bus, and car. (b) vegetation and terrain. (c) motorcycle and bicycle. (d) fence and vegetation.}
\label{fig:vis_result_1}
\vspace*{+1.0cm}
\end{figure*}

\begin{figure*}[!t]
\centering
  \includegraphics[width=\linewidth]{latex/figures/fig_seg_results2_n.pdf}
  \caption{Comparison of predicted segmentation maps: (a) truck and car. (b) car and truck. (c) bus and car. (d) building and fence. (e) sky and building; pole and building. (f) pole and building.}
\label{fig:vis_result_2}
\vspace*{+1.0cm}
\end{figure*}


\end{document}
