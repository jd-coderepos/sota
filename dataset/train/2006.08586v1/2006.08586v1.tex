In this Section, we present the empirical evaluation of our approach. First, we describe the datasets used for training and evaluation (Subsection~\ref{sec:datasets}). Then, we focus on the quantitative evaluation (Subsections~\ref{sec:sota} and~\ref{sec:ablatives}), and finally we present more qualitative results (Subsection~\ref{sec:qualitative}).

\subsection{Datasets} \label{sec:datasets}

\noindent
\textbf{Human3.6M}~\cite{ionescu2013human3}:
It is an indoor dataset where a single person is visible in each frame. It provides 3D ground truth for training and evaluation. We use Protocol 2 of~\cite{kanazawa2018end}, where Subjects S1,S5,S6,S7 and S8 are used for training, while Subjects S9 and S11 are used for evaluation.

\noindent
\textbf{MuPoTS-3D}~\cite{mehta2018single}:
It is a multi-person dataset providing 3D ground truth for all the people in the scene. We use this dataset for evaluation using the same protocol as~\cite{mehta2018single}.

\noindent
\textbf{Panoptic}~\cite{joo2015panoptic}:
It is a dataset with multiple people captured in the Panoptic studio. We use this dataset for evaluation, following the protocol of~\cite{zanfir2018monocular}.

\noindent
\textbf{MPI-INF-3DHP}~\cite{mehta2017monocular}:
It is a single person dataset with 3D pose ground truth. We use subjects S1 to S8 for training.

\noindent
\textbf{PoseTrack}~\cite{andriluka2018posetrack}:
In-the-wild dataset with 2D pose annotations. Includes multiple frames for each sequence. We use this dataset for training and evaluation.

\noindent
\textbf{LSP}~\cite{johnson2010clustered}, \textbf{LSP Extended}~\cite{johnson2011learning}, \textbf{MPII}~\cite{andriluka20142d}:
In-the-wild datasets with annotations for 2D joints. We use the training sets of these datasets for training.

\noindent
\textbf{COCO}~\cite{lin2014microsoft}:
In-the-wild dataset with 2D pose and instance segmentation annotations. We use the 2D joints for training as we do with the other in the-wild datasets, while the instance segmentation masks are employed for the computation of the depth ordering-aware loss.


\subsection{Comparison with the state-of-the-art} \label{sec:sota}
For the comparison with the state-of-the-art, as a sanity check, we first evaluate the performance of our approach on a typical single person baseline. Our goal is always multi-person 3D pose and shape, but we expect our approach to achieve competitive results, even in easier settings, i.e., when only one person is in the image. More specifically, we evaluate the performance of our network on the popular Human3.6M dataset~\cite{ionescu2013human3}. The most relevant approach here is HMR by Kanazawa~\etal~\cite{kanazawa2018end}, since we share similar architectural choices (iterative regressor, regression target), training practices (adversarial prior) and training data. The results are presented in Table~\ref{table:h36m}. Our approach outperforms HMR, as well as the approach of Arnab~\etal~\cite{arnab2019exploiting}, that uses the same network with HMR, but is trained with more data.

\begin{table}\centering
	\begin{tabular}{c|c|c|c}
		\toprule
		Method & HMR~\cite{kanazawa2018end} & Arnab~\etal~\cite{arnab2019exploiting} & Ours  \\
		\midrule
		Reconst. Error & 56.8 & 54.3 & \textbf{52.7}\\
		\bottomrule
	\end{tabular}
	\caption{\textbf{Results on Human3.6M}. The numbers are mean 3D joint errors in mm after Procrustes alignment (Protocol 2). The results of all approaches are obtained from the original papers.}
	\label{table:h36m}
\end{table}

Having established that our approach is competitive in the single person setting, we continue the evaluation with multi-person baselines. In this case, we consider approaches that also estimate pose and shape for multiple people. The most relevant baselines are the works of Zanfir~\etal~\cite{zanfir2018monocular,zanfir2018deep}. We compare with these approaches in the Panoptic dataset~\cite{joo2015panoptic,joo2017panoptic}, using their evaluation protocol (assuming no data from the Panoptic studio are used for training). The full results are reported in Table~\ref{table:panoptic}. Our initial network (baseline), trained without our proposed losses, achieves performance comparable with the results reported by the previous works of Zanfir~\etal. More importantly though, adding the two proposed losses (full), improves performance across all subsequences and overall, while we also outperform the previous baselines. These results demonstrate both the strong performance of our approach in the multi-person setting, as well as the benefit we get from the losses we propose in this work.

\begin{table}\centering
\footnotesize
	\begin{tabular}{c|c|c|c|c|c}
	\toprule
	Method & Haggling & Mafia & Ultim. & Pizza & Mean\\
	\midrule
	Zanfir~\etal~\cite{zanfir2018monocular} & 140.0 & 165.9 & 150.7 & \textbf{156.0} & 153.4\\ 
	Zanfir~\etal~\cite{zanfir2018deep} & 141.4 & 152.3 & \textbf{145.0} & 162.5 & 150.3\\ 	
	Ours (baseline) & 141.2 & 140.3 & 160.7 & 156.8 & 149.8\\	
	Ours (full) & \textbf{129.6} & \textbf{133.5} & 153.0 & 156.7 & \textbf{143.2}\\
	\bottomrule
	\end{tabular}
	\caption{\textbf{Results on the Panoptic dataset}. The numbers are mean per joint position errors after centering the root joint. The results of all approaches are obtained from the original papers.}
	\vspace{-2mm}
	\label{table:panoptic}
\end{table}

Another popular benchmark for multi-person 3D pose estimation is the MuPoTS-3D dataset~\cite{mehta2017monocular}. Since no multi-person 3D pose and shape approach reports results on this benchmark, we implement two strong top-down baselines, based on state-of-the-art approaches for single-person 3D pose and shape. Specifically, we select a regression approach, HMR~\cite{kanazawa2018end}, and an optimization approach, SMPLify-X~\cite{pavlakos2019expressive}, and we apply them on detections provided by OpenPose~\cite{cao2019openpose} (as is suggested by their public repositories), or by Mask-RCNN~\cite{he2017mask} (for the case of HMR). The full results are reported in Table~\ref{table:mupots}. As we can see, our baseline model performs comparably to the other approaches, while our full model trained with the proposed losses improves significantly over the baseline. Similarly with the previous results, this experiment further justifies the use of our coherency losses. Besides this, we also demonstrate that na\"ive baselines trained with a single person in mind are suboptimal for the multi-person setting of 3D pose. This is different from the 2D case, where a single-person network can perform particularly well in multi-person top-down pipelines as well, e.g.,~\cite{chen2018cascaded,sun2019deep,xiao2018simple}. For the 3D case though, when multiple people are involved, making the network aware of occlusions and interpenetrations during training, can actually be beneficial at test-time too.

\begin{table}\centering
	\begin{tabular}{c|c|c}
	\toprule
	Method & All & Matched\\
	\midrule	
	OpenPose + SMPLify-X~\cite{pavlakos2019expressive} & 62.84 & 68.04\\
	OpenPose + HMR~\cite{kanazawa2018end} & 66.09 & 70.90\\	
	Mask-RCNN + HMR~\cite{kanazawa2018end} & 65.57 & 68.57\\			
	Ours (baseline) & 66.95 & 68.96\\	
	Ours (full) & \textbf{69.12} & \textbf{72.22}\\
	\bottomrule
	\end{tabular}
	\caption{\textbf{Results on MuPoTS-3D}. The numbers are 3DPCK. We report the overall accuracy (All), and the accuracy only for person annotations matched to a prediction (Matched).}
	\label{table:mupots}
\end{table}

\subsection{Ablative studies} \label{sec:ablatives}
For this work, our interest in multi-person 3D pose estimation extends beyond just estimating poses that are accurate under the typical 3D pose metrics. Our goal is also to recover a coherent reconstruction of the scene. This is important, because in many cases we can improve the 3D pose metrics, e.g., get a better 3D pose for each detected person, but return incoherent results holistically. For example, the depth ordering of the people might be incorrect, or the reconstructed meshes might be positioned such that they overlap each other. To demonstrate how our proposed losses improve the network predictions under these coherency metrics even if they are only applied during training, we perform two ablative studies for more detailed evaluation.

First, we expect our interpenetration loss to naturally eliminate most of the overlapping people in our predictions. We evaluate this on MuPoTS-3D and PoseTrack, reporting the number of collisions with and without the interpenetration loss. The results are reported in Table~\ref{table:interpenetration}. As we expected, we observe significant decrease in the number of collisions when we train the network with the $L_{\mathcal{P}}$ loss.

\begin{table}\centering
	\begin{tabular}{c|c|c}
		\toprule
		Method & MuPoTS-3D & PoseTrack \\
		\midrule
	    Our baseline & 114 & 653 \\
	    Our baseline + $L_{\mathcal{P}}$  & 34 & 202 \\
		\bottomrule
	\end{tabular}
	\caption{\textbf{Ablative for interpenetration loss}. The results indicate the number of collisions on MuPoTS-3D and PoseTrack.}
	\vspace{-2mm}
	\label{table:interpenetration}
\end{table}

Moreover, our depth ordering-aware loss should improve the translation estimates for the people in the scene. Since for monocular methods it is not meaningful to evaluate metric translation estimates, we propose to evaluate only the returned depth ordering. More specifically, we consider all pairs of people in the scene, and we evaluate whether our method predicted the ordinal depth relation for this pair correctly. In the end, we report the percentage of correctly estimated ordinal relations in Table~\ref{table:ordering}. As expected, the depth ordering-aware loss improves upon our baseline. In the same Table, we also report the results of the approach of Moon~\etal~\cite{moon2019camera} which is the state-of-the-art for 3D skeleton regression. Although~\cite{moon2019camera} is skeleton-based and thus, not directly comparable to us, we want to highlight that even a state-of-the-art approach (under 3D pose metric evaluation) can still suffer from incoherency in the results. This provides evidence that we often might overlook the coherency of the holistic reconstruction, and we should also consider this aspect when we evaluate the quality of our results.

Finally, we underline that we do not apply these coherency losses at test time. Instead, during training, our losses act as constraints to the reconstruction and ultimately provide better supervision to the network, for images that no explicit 3D annotations are available. The improved supervision leads to more coherent results {\it at test time too}. 


\begin{table}\centering
\footnotesize
	\begin{tabular}{c|c|c|c}
	\toprule
	Method & Moon~\etal~\cite{moon2019camera} & Our baseline & Our baseline + $L_{\mathcal{D}}$ \\
	\midrule
	Accuracy & 90.85\% & 92.17\% & 93.68\%\\
	\bottomrule
	\end{tabular}
	\caption{\textbf{Ablative for depth-ordering-aware loss.} Depth ordering results on MuPoTS-3D. We consider all pairs of people in the image, and we evaluate whether the approaches recovered the ordinal depth relation between the two people correctly. The numbers are percentages of correctly estimated ordinal depth relations. }
	\vspace{-2mm}
	\label{table:ordering}
\end{table}

\begin{figure*}[!t]
	\centering
    \includegraphics[width=0.24\textwidth]{figures/final/ablative/abl_images__val__020880_mpii_test__000080.jpg/img_cropped.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/ablative/abl_images__val__020880_mpii_test__000080.jpg/output_cropped_color.jpg}
    \includegraphics[width=0.12\textwidth]{figures/final/ablative/abl_images__val__020880_mpii_test__000080.jpg/output_merged.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/ablative/images__val__020880_mpii_test__000080.jpg/output_cropped_color.jpg}
    \includegraphics[width=0.12\textwidth]{figures/final/ablative/images__val__020880_mpii_test__000080.jpg/output_merged.jpg}\\
    \includegraphics[width=0.24\textwidth]{figures/final/ablative/abl_images__val__005290_mpii_test__000074.jpg/img_cropped.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/ablative/abl_images__val__005290_mpii_test__000074.jpg/output_cropped_color.jpg}
    \includegraphics[width=0.12\textwidth]{figures/final/ablative/abl_images__val__005290_mpii_test__000074.jpg/output_merged.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/ablative/images__val__005290_mpii_test__000074.jpg/output_cropped_color.jpg}
    \includegraphics[width=0.12\textwidth]{figures/final/ablative/images__val__005290_mpii_test__000074.jpg/output_merged.jpg}\\
  \vspace{-1mm}
   \text{\scriptsize Input image } \hspace{40mm} \text{\scriptsize Baseline} \hspace{56mm} \text{\scriptsize Ours}  \hspace{12mm} \text{\color{white} \scriptsize}
	\vspace{-3mm}   
	\caption{{\bf Qualitative effect of proposed losses.}
Results of our baseline model (center) and our full model trained with our proposed losses (right). As expected, we improve over our baseline in terms of coherency in the results (i.e., fewer interpenetrations, more consistent depth ordering for the reconstructed meshes).
}
\label{fig:ablative}
\vspace{-2mm}
\end{figure*}
 
\begin{figure*}[!t]
	\centering
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__018713_mpii_test__000059.jpg/img_cropped.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__018713_mpii_test__000059.jpg/output_cropped_color.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__018713_mpii_test__000059.jpg/output_top_color.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__018713_mpii_test__000059.jpg/output_left_color.jpg}\\
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__000901_mpii_test__000060.jpg/img_cropped.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__000901_mpii_test__000060.jpg/output_cropped_color.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__000901_mpii_test__000060.jpg/output_top_color.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__000901_mpii_test__000060.jpg/output_left_color.jpg}\\
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__009607_mpii_test__000060.jpg/img_cropped.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__009607_mpii_test__000060.jpg/output_cropped_color.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__009607_mpii_test__000060.jpg/output_top_color.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/images__val__009607_mpii_test__000060.jpg/output_left_color.jpg}\\
    \includegraphics[width=0.24\textwidth]{figures/final/single/COCO_val2014_000000025393.jpg/img_cropped.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/COCO_val2014_000000025393.jpg/output_cropped_color.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/COCO_val2014_000000025393.jpg/output_top_color.jpg}
    \includegraphics[width=0.24\textwidth]{figures/final/single/COCO_val2014_000000025393.jpg/output_left_color.jpg}\\ 
	\vspace{-2mm}
	\caption{{\bf Qualitative evaluation.}
We visualize the reconstructions of our approach from different viewpoints; front (green background), top (blue background) and side (red background). More qualitative results can be found in the Sup.Mat.
}
\label{fig:qualitative}
\vspace{-2mm}
\end{figure*}
 
\subsection{Qualitative evaluation} \label{sec:qualitative}
In this Subsection, we present more qualitative results of our approach. In Figure~\ref{fig:ablative} we compare our baseline with our full model trained with the proposed losses. As expected, our full model generates more coherent reconstructions, improving over the baseline as far as interpenetration and depth ordering mistakes are concerned. Errors can happen when there is significant scale difference among the people and there is no overlap on the image plane (last row of Figure~\ref{fig:qualitative}). More results can be found in the Sup.Mat.