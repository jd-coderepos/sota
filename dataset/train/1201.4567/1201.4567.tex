\documentclass[envcountsame]{llncs}



\usepackage{microtype}
\usepackage{xspace,paralist}  
\usepackage{latexsym,amsmath,amssymb,amstext}



\spnewtheorem{requirement}[theorem]{Requirement}{\bfseries}{\itshape}

\newenvironment{notes}[1][1em]{\begin{compactitem}[\decoone]\sl\small}{\end{compactitem}}
\newcommand{\sfv}{\ensuremath{\mathbf{sfv}}}\newcommand{\ssize}[1]{\Vert #1\Vert}

\newcommand{\co}[1]{\hat{#1}}
\newcommand{\safe}[1]{#1^{\mathsf{S}}}
\newcommand{\key}[1]{\ensuremath{\mathop{\mathsf{#1}}\nolimits}\xspace}

\newcommand{\Case}{\key{case}}
\newcommand{\Of}{\key{of}}
\newcommand{\In}{\key{in}}
\newcommand{\Let}{\key{let}}
\newcommand{\Letstar}{\key{let*}}
\newcommand{\fold}{\key{fold}}
\newcommand{\unfold}{\key{unfold}}
\newcommand{\folds}[1]{\mathop{\safe{\mathsf{fold}}_{#1}}}
\newcommand{\unfolds}[1]{\mathop{\safe{\mathsf{unfold}}_{#1}}}
\newcommand{\hc}{\widehat{c}}
\newcommand{\hd}{\widehat{d}}
\newcommand{\ms}{\mathit{ms}}
\newcommand{\ns}{\mathit{ns}}


\newcommand{\type}[1]{\ensuremath{\mathbf{#1}}\xspace}
\newcommand{\Nat}{\type{nat}}
\newcommand{\Bool}{\type{bool}}
\newcommand{\Bit}{\type{bit}}
\newcommand{\Tree}{\type{tree}}
\newcommand{\Stream}{\type{stream}}
\newcommand{\Nats}{\type{nats}}
\newcommand{\String}{\type{string}}

\newcommand{\size}{\ensuremath{\mathrm{size}}\xspace}
\newcommand{\Unit}{\ensuremath{\mathbf{unit}}\xspace}
\newcommand{\up}{\ensuremath{\mathord{\mathrm{up}}}\xspace}
\newcommand{\alt}{\mathrel{[\!]}}
\newcommand{\Ty}{\mathit{Ty}}

\newcommand{\constr}[1]{\ensuremath{\mathop{\mathsf{#1}}\nolimits}\xspace}
\newcommand{\Nought}{\constr{Nought}}
\newcommand{\One}{\constr{One}}
\newcommand{\Zero}{\constr{Zero}}
\newcommand{\Succ}{\constr{Succ}}
\newcommand{\Leaf}{\constr{Leaf}}
\newcommand{\Fork}{\constr{Fork}}
\newcommand{\Empty}{\constr{Empty}}
\newcommand{\declare}{\constr{declare}}
\newcommand{\data}{\constr{data}}
\newcommand{\codata}{\constr{codata}}
\newcommand{\Cons}{\constr{Cons}}

\newcommand{\id}{\mathrm{id}}
\newcommand{\des}{\mathit{des}}
\newcommand{\asize}[1]{\ensuremath{\mathopen{|}#1\mathclose{|}}\xspace}
\newcommand{\osize}[1]{\ensuremath{\mathopen{\Vert}#1\mathclose{\Vert}}}
\newcommand{\tail}{\textit{tail}}
\newcommand{\tails}{\textit{tails}}

\newcommand{\gquote}[1]{\guillemotleft#1\guillemotright}

\newcommand{\Danger}{\colorbox{red}{\color{white}\!\danger\!}}
\newcommand{\fork}{\mathbin{\bigtriangleup}}
\newcommand{\join}{\mathbin{\bigtriangledown}}
\newcommand{\Id}{\mathbf{Id}}


\newcommand{\lfp}[1]{\mu #1\,\sqdot\,}
\newcommand{\gfp}[1]{\nu #1\,\sqdot\,}
\newcommand{\Dcl}{\mathit{Dcl}}
\newcommand{\SetCat}{\ensuremath{\mathcal{S}\mkern-2.5mu\mathit{et}}\xspace}

\newcommand{\ustrut}{\rule[-.18\baselineskip]{0pt}{0pt}}


\newcommand{\inj}{\mathop{\iota}\nolimits}
\newcommand{\proj}{\mathop{\pi}\nolimits}
\newcommand{\upair}[2]{\underline{(}#1\underline{\ustrut,}#2\underline{)}}
\newcommand{\uiota}{\mathop{\underline{\iota}}\nolimits}
\newcommand{\uep}{\underline{()}}
\newcommand{\uc}{\underline{c}}

\newcommand{\SB}{\mathcal{B}}
\newcommand{\SE}{\mathcal{E}}
\newcommand{\RS}{\mathit{RS}}
\newcommand{\RSi}{\mathit{RS_1}}
\newcommand{\RSmi}{\mathit{RS^-_1}}

\newcommand{\topic}[1]{\smallskip\noindent{\textit{#1{.}}}\enspace}
\newcommand{\Topic}[1]{\smallskip\noindent{\textbf{#1{.}}}\enspace}


\newcommand{\stub}[1]{\typeout{*** Stub! ***} 
	{\textbf{Stub: \emph{#1}}}}

	
\newcommand{\newfontobj}[2]{
  \newcommand{#1}[1]{
    \expandafter\def\csname##1\endcsname{{#2 ##1}}}}

\newcommand{\newthingie}[2]{
  \newcommand{#1}[1]{
    \expandafter\def\csname##1\endcsname{{#2 ##1}}}}

\newfontobj{\class}{\rm}
\newfontobj{\fnc}{\it}

\newfontobj{\category}{\bf}

\newcommand{\Strut}[1]{\rule{0pt}{#1}}
\newcommand{\restrict}{\mathclose{\mid}}
\newcommand{\restricted}[1]{\restrict_{#1}}
\newcommand{\confined}{\mathclose{\upharpoonright}} 
\renewcommand{\gets}{\ensuremath{\mathrel{\colon=}}\xspace}

\newcommand{\restr}[1]{\restrict_{#1}}

\newcommand{\ran}{\mathrm{range}}
\newcommand{\corange}{\mathrm{corange}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\preimg}{\mathrm{preimage}}
\newcommand{\image}{\mathrm{image}}
\newcommand{\set}[1]{\{\,#1\,\}}
\newcommand{\Set}[1]{\left\{\,#1\,\right\}}
\newcommand{\pair}[1]{\mathopen{\langle}#1\mathclose{\rangle}}

\newcommand{\forallae}{\mathord{\stackrel{\kern 0.1em\infty}{\forall}}}
\newcommand{\existio}{\mathord{\stackrel{\infty}{\exists}}}
\newcommand{\converges}{\mathclose{\hbox{}}}
\newcommand{\diverges}{\mathclose{\hbox{}}}
\newcommand{\Implies}{\Longrightarrow}
\newcommand{\entails}{\vdash}
\renewcommand{\phi}{\varphi}
\newcommand{\floor}[1]{\lfloor#1\rfloor}
\newcommand{\ceiling}[1]{\lceil#1\rceil}
\newcommand{\monus}{\mathop{\dot-}}

\newcommand{\suchthat}{\mathrel{\,\stackrel{\rule{0.03em}{0.5ex}}{\rule[-.1ex]{0.03em}{0.5ex}}\,}}

\newcommand{\bo}{\mathbf{0}}
\newcommand{\bl}{\mathbf{1}}

\newcommand{\defiff}{\iff_{\!\rm def}}
\newcommand{\Iff}{\mathbin{\Leftrightarrow}}

\newcommand{\sqdot}{\rule{0.5mm}{0.5mm}}
\newcommand{\lam}[1]{\lambda #1\,\sqdot\,}
\newcommand{\llambda}{{\lambda\hskip-.45em\lambda}}	
\newcommand{\llam}[1]{\llambda #1\,\sqdot\,}

\newcommand{\Card}[1]{\left\Vert #1 \right\Vert}
\newcommand{\symdiff}{\bigtriangleup}
\newcommand{\defeq}{\mathrel{\stackrel{\text{def}}{=}}}
\newcommand{\fv}{\ensuremath{\mathbf{fv}}}

\newcommand{\is}{\ensuremath{\mathrel{\mathrel{:}\mathrel{:}=}}}
\newcommand{\synsep}{\;\;|\;\;}
\newcommand{\yields}{\mathbin{\downarrow}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\SC}{\mathcal{C}}
\newcommand{\of}{\colon}
\renewcommand{\colon}{\mathpunct{:}}
\newcommand{\Quad}[1]{\hspace*{#1em}}

\newcommand{\partto}{\rightharpoonup}

\renewcommand{\today}{\number\day \space \ifcase\month\or
	January\or February\or March\or April\or May\or June\or
	July\or August\or September\or October\or November\or
	December\fi \space \number\year}
	
\newcommand{\Comment}[1]{\texttt{(*} \textsl{#1} \texttt{*)}}
\newcommand{\Norm}[1]{\Vert #1 \Vert}

\newcommand{\irule}[2]{\ensuremath{{\frac{\strut\textstyle #1}{\strut\textstyle #2}}}} 
\newcommand{\rulelabel}[1]{\hbox{\it #1:}\Quad{0.5}}
\newcommand{\sidecond}[1]{\Quad{0.5} \left(#1\right)}
  
\newcommand{\depth}{\textit{depth}}
\newcommand{\semantics}[1]{\ensuremath{[\![ #1 ]\!]}}

\newcommand{\figrule}{\par\vspace{1ex}\hrule\par}

\newcommand{\level}{\textit{level}}
\newcommand{\down}{\mathsf{down}}

 

\newcommand{\FLAG}{\marginpar[\raggedleft\Huge]{\Huge}}
\makeatletter 
\def\rightmark{Danner and Royer: 
               Ramified Structural Recursion and Corecursion}
\def\leftmark{\rightmark}
\def\ps@myheadings{\let\@mkboth\@gobbletwo
  \def\@oddhead{\hfill\small\rm\rightmark\qquad\thepage}\def\@oddfoot{}
  \def\@evenhead{\small\thepage\qquad\rightmark\hfill}\def\@evenfoot{}
  }
\ps@myheadings
\makeatother


\title{Ramified Structural Recursion and Corecursion \normalsize
 Extended Abstract}
\titlerunning{Ramified Structural Recursion and Corecursion}
\toctitle{Ramified Structural Recursion and Corecursion}
\author{Norman Danner \inst{1}
	    \and
       James S.~Royer \inst{2}
     }
          
\institute{Department of Mathematics and Computer Science, 
  Wesleyan University, 
  Middletown, CT 06459, USA; 
  \email{ndanner@wesleyan.edu}
 \and
  Department of Electrical Engineering and Computer Science, 
  Syracuse University, 
  Syracuse, NY 13210, USA; 
  \email{jsroyer@syr.edu}
}
        
          
          
\begin{document}
\bibliographystyle{splncs03}


\maketitle

\begin{abstract} 
  We investigate feasible computation over a fairly general notion
  of data and codata. 
  Specifically, we present a direct Bellantoni-Cook-style
  normal/safe typed programming formalism, , that expresses
  feasible structural recursions and corecursions over data and
  codata specified by polynomial functors. (Lists, streams, finite
  trees, infinite trees, etc.~are all directly definable.)  A novel
  aspect of  is that it embraces structure-sharing as in
  standard functional-programming implementations.  As our data
  representations use sharing, our implementation of structural
  recursions are memoized to avoid the possibly exponentially-many
  repeated subcomputations a na\"{\i}ve implementation might
  perform.  We introduce notions of size for representations of data
  (accounting for sharing) and codata (using ideas from type-2
  computational complexity) and establish that type-level~1
  -functions have polynomial-bounded runtimes and satisfy a
  polynomial-time completeness condition.  Also, restricting 
   terms to particular types produces characterizations of
  some standard complexity classes (e.g., -regular
  languages, linear-space functions) and some less-standard classes
  (e.g., log-space streams).
\end{abstract}
  
\section{Introduction}

What counts as polynomial-time (much less ``feasible'') computation
over general forms of data is not a settled matter.  The
complexity-theoretic literature of higher-type computability is
still thin, it is  spotty on computation over codata (infinite
lists and trees) with some notable exceptions,\footnote{Hartmanis and Stearns' paper \cite{HartmanisStearns65} that
  founded computational complexity largely focuses on the
  time-complexity of infinite streams as the authors directly
  adapted Turing's original machine model \cite{Turing36}
  which, recall, concerns stream-computation.}  
and even in the case of inductively defined data there are there
remain issues that are not that well explored (see the end of \S2
below).  We develop a notion of polynomial-time computation over
data and codata using a fairly simple implicit complexity formalism,
, that satisfies poly-time soundness and completeness
properties.   is constructed in stages.  We first introduce 
, a formalism for computing over inductively defined data by
classical structural (aka primitive) recursion.   has roughly 
the computational power of G\"{o}del's primitive recursive
functionals \cite{Longley:notions:1}.  To tame this power, we impose
a form of Bellantoni and Cook normal/safe ramification on 's
structural recursions and obtain , a system that satisfies
appropriate poly-time soundness and completeness properties.  We
next introduce , an extension of 
to include codata definitions and
classical structural (aka primitive) corecursions.  We extend the
safe/normal ramification to corecursions and obtain  that
also satisfies  poly-time soundness and completeness
properties.  The subscript on  and  is a reminder
that these formalisms focus on type-level 1 computation, eventhough
 and  allow higher-type terms.  It turns out that by
restricting types in -terms, one can characterize other
complexity classes, e.g., -regular languages, log-space
streams of characters, linear-space streams of strings, etc.  These
seem to be related to the \emph{two-sorted} complexity class
characterizations studied by Cook and Nguyen \cite{CookNguyen:book}.

\emph{Related Work.}
The Pola project of Burrell, Cockett, and Redmond
\cite{Burrell:2009,cockett:redmond:low:cat} has  aims similar to ours,
but Pola forbids any structure-sharing of safe-data or safe-codata.
 and , in contrast, embrace structure-sharing and
adjust the implementation of structural recursions to accomodate it.
As a result  and Pola describe different notions of
polynomial-time over data and codata.  
How deep these differences go is an intriguing question.
Pola also has a well-developed categorical
semantics that,  at present,  notably lacks.
Ramyaa and Leivant \cite{Ramyaa:2010,Ramyaa:Leivant:2011} explore
feasible first-order stream programming formalisms.
In~\cite{Ramyaa:2010}, they use infinite binary trees with
string-labels to give a partial proof-theoretic characterization of
the type-2 basic feasible functionals () of
Mehlhorn~\cite{Mehlhorn76} and Cook and
Urquhart~\cite{CookUrquhart:feasConstrArith}.  In
\cite{Ramyaa:Leivant:2011}, they give a definition of logspace
stream computation and a schema of ramified co-recurrence which
parallels Leivant's ramified recurrence of \cite{Leivant:FM2}, and
characterize logspace streams as those definable using -tier
co-recurrences.  F\'{e}r\'{e}e \emph{et al.}~\cite{Feree:2010} also
consider stream computation, but primarily as a technical tool in
characterizing  as the functions computed by a
rewrite system over streams that has a second-order polynomial
interpretation.












\Topic{Background}
\emph{Pointer Machines.} 
We assume that the underlying model of computation is along the
lines of Kolmogorov and Uspenskii's 
``pointer machines'' or Sch\"{o}nhage's \emph{storage modification 
machines}
\cite{vanEmdeBoas90}.


\emph{Types.} The \emph{simple types} over a  set of base types  are given by: 
    
    , where  
(which counts as a base type) is the type of the 
empty product .  
Let a base type, 
   
 ,  
  , 
and .
We call level-0 types 
\emph{ground types}.  
A type judgment  asserts
that  can be assigned type  under type context ,
where a \emph{type context} is a finite function from variables to types. 



\emph{Algebraic Notions.} 
 denotes the category of sets and total functions.  
Below we are mainly concerned with total functions and lower
type-levels, so  suffices as the setting for the semantics of 
our programming formalisms.
Types are thus interpreted as sets where
coproduct (), product (), and exponentiation () have
their standard -interpretations.  Let  () be the canonical coproduct injections and
 () be the canonical
product projections.
A \emph{polynomial functor} is a functor inductively built from 
identity and functors and coproducts and products,
e.g.,
 with 
, where 
 is the type of natural numbers introduced below in 
Example~\ref{ex:nattree}.\footnote{Other authors (e.g., \cite{rutten:coalg}) use broader 
  notions of polynomial functor.} 
The constant-objects in our polynomial functors will always be
types.  \emph{Convention:} For , a polynomial function given by
, and , a type, read  as
the type .
E.g.,  = .


\emph{The Base Formalism.}
This paper's programming formalism are built atop , a standard,
simply-typed, call-by-value lambda calculus.  The -types are
.  Figs.~\ref{fig:core:syn}
and~\ref{fig:core:typing} give 's syntax and typing rules.  We
use the standard syntactic sugar:
\begin{inparaenum}[\it (i)]
\item 
   
  
   and 
\item 
  
  
  .
\end{inparaenum}

\emph{Semantics.} 
The denotational semantics of  is standard.  As  is the
sole base type of , for each
, \  is a finite set.
's operational semantics is also fairly standard as specified by
the evaluation relation, , described in
Fig.~\ref{fig:core:eval}.  \emph{Terminology:} An \emph{evaluation
  relation} relates closures to values.  A
\emph{closure}~ consists of a term
 and an environment~ for
.  (We write  for  when 's typing is understood.)  An
\emph{environment}~ for  is a
finite map from variables to values with
 and, for each
,  is a type- value.
A \emph{value}~ is a closure in which~ (the \emph{value
  term}) is either an abstraction or else an internal representation
of  or  or , where  and
 are value terms.  By \emph{internal representation} we mean
the ``machine'' representation of value terms, the details of which
are not important for , but vital for the  and 
formalisms below.  

\section{Structural Recursions} 	

\textbf{The Classical Case.} \enspace
We extend  to , a formalism that computes,
roughly, G\"{o}del's primitive recursive functionals
\cite{Longley:notions:1} over inductively-defined data
types.  Later we  introduce , a ramified, ``feasible''
version of .  Fig.~\ref{fig:S-} gives the revised raw syntax
\eqref{e:Ssyn}, typing rules (-\emph{I},
-\emph{I}, -\emph{I}) and evaluation rules
(\emph{Const}, \emph{Destr},  \emph{Fold})
for .  A declaration, , introduces
a data-type .
The polynomial functor  is called 's
\emph{signature functor}.  The declaration also implicitly
introduces: 's \emph{constructor function} 
        ,  
's \emph{destructor  function}  
        ,
        and
's \emph{recursor} .
We require that the  in  be a
ground type with constituent base types are drawn from , ,
and previously declared types.  Semantically, the data type 
is the \emph{least fixed point} of : it is a
smallest set  isomorphic to , where  and
 witness this isomorphism.
It is standard that polynomial functors have 
such least fixed points.
In examples we use syntactically-sugared versions of  of the form:
,
where  and, for each , 
if , 
   then    and 
if , 
   then  
   .\footnote{For , define: 
   and
  .
  Also, define: .   
}   
Type- data can then be identified with the elements of the
free algebra over the sugared constructors  and the
other constituent data-types' constructors.

\begin{figure}[t]
\begin{minipage}{\textwidth}\small
0ex]
\notag
  \rulelabel{-I}
  \irule{\Gamma\entails e\of F\tau}{\Gamma\entails (c_\tau\,e)\of\tau}
  \Quad2
  \rulelabel{-I}
  \irule{\Gamma\entails e\of\tau}{\Gamma\entails (d_\tau\,e) \of F\tau}
  \Quad2
  \rulelabel{-I}
  \irule{\Gamma \entails f\of F\sigma\to\sigma
  \Quad{1} 
  \Gamma\entails e\of\tau}{\Gamma\entails \fold_\tau f \,e\of \sigma} 
  \0.5ex]
\notag
  \rulelabel{Fold}
  \irule{e\theta\yields (\underline{c}_\tau\,v)\theta_1
  \Quad1 
  f(F\,(\lam{x}(\fold_\tau f\,x))\,y) 
      \theta[y\mapsto v\theta_1]\yields v'\theta'}{(\fold_\tau f\,e)\theta \yields v'\theta'}
  \sidecond{\hbox{ and  are fresh}}
\label{e:tn}
  \Letstar \, t_0 = \Leaf;\;
           t_1=\Fork(t_0,t_0);\,
	         \dots\,;\;
           t_n=\Fork(t_{n-1},t_{n-1}) 
           \,\In\, t_n

  \rulelabel{-I}
  \irule{\Gamma \entails f\of F\sigma\to\sigma
  \Quad{1} 
  \Gamma\entails e\of\tau}{\Gamma\entails \folds\tau f \,e\of \sigma} 
  \sidecond{\dagger}
  \Quad2
  \rulelabel{lower}
  \irule{\Gamma\entails e\of\safe\tau}{
  \Gamma\entails e\of\tau}
  \sidecond{\ddagger}

\label{e:S}
Dcl \;\;\is\;\; \data\; T = \lfp{X}\Ty_0
  \synsep \codata\; T = \gfp{X}\Ty_0
\
\caption{Key Additions for .}
\label{fig:S}
\end{minipage}
\end{figure}


\begin{example}\label{ex:nats}
  The declaration,   ,
  introduces the type  with signature functor 
   
  and  constructor .
  Each element of  corresponds to an infinite 
  sequence of 's.
  Given an , let 
  ;
  ,
  so  the sequence , , .
  Given an , let 
   
  , 
   so  the th  in  's sequence. 
\end{example}

As the above shows, codata are really higher-type objects.  To help
analyze this, define
a rank-0 type is at type with no constituent codata types, and
a rank- type is a type with constituent codata types of maximum rank .
E.g., , , and  are rank 0 and a stream of
 is rank 1.  Let  be the restriction of  to types of
levels  and 
ranks .
Not surprisingly, the -functions of types
 correspond to P\'eter's
-primitive recursive functions \cite{Longley:notions:1}.
We shall show how normal/safe ramification can rein in the power of
these corecursions.  First, we consider codata representations and
their size.














\Topic{Representation and Size}
A type- codatum  is represented via lazy -
and/or -expressions; if we probe  with ever-longer
series of destructor applications, a possibly infinite structure
unfurls.  A codatum is thus a function-like object that must be
queried (via destructor applications) to be computed over.  To
measure codata-size we adapt Kapron and Cook's notion of the length
of a type-1 function \cite{KapronCook:mach}.  Measuring just rank-0
codata suffices for this paper.

\begin{definition}  \label{d:ob:size} \
Suppose  is of type , a rank-0 codata-type.
  
  \begin{asparaenum}[(a)]
  \item 
The \emph{apparent size} of  (written:
    ) is 1.


  \item 
The \emph{observed size} of  (written:
    ) is the function over natural numbers:
    
    varies over sequences of compositions of destructors with (i)
     type correct and (ii) at most  occurrences of
    .



  \end{asparaenum}
\end{definition}

Roughly,  is the maximum apparent-size of the
data in -cons-cells along any path from the head of 
that includes at most  type- links.  \emph{Example:} For
 of Example~\ref{ex:nats},
(the th element of 's
sequence).



\Topic{The Ramified Case}
, our ramification of , extends the normal/safe distinction
to codata.  \emph{Key Points:} As the value of
 is the result of a (co)recursion, it should be
safe,  as  gives the computation step, we should have
, and as 's are lazy, 
destructs drive the computation.
 \emph{Normal and Safe
  Types:} First, we bring in all the
 conventions to this setting to ramify data.
Second, a declaration
 introduces the normal type  with
constructor  and destructor  as before, a
safe type  with constructor  and destructor
, and
 where  has
the same operational semantics as .  \emph{Example:}
Replace , ,  , and 
with , ,  , and
 in
Example~\ref{ex:nats}'s definition of , then  can be
assigned assigned type .  
\textbf{N.B.}
Given an -computable , there may 
\emph{not} be an -definable analogue of  from
Example~\ref{ex:nats}.

 
\emph{Poly-Heap Size Bounds.} 
To adapt poly-heap bounds to take account of observed sizes we use
Kapron and Cook's notion of second-order polynomials
\cite{KapronCook:mach}; these are roughly ordinary polynomials with
applied type-1 function symbols included (e.g.,
).  Now  
is a \emph{poly-heap bound on apparent size} when  is a 
\emph{normal second-order polynomial} 
(i.e., over  is normal 
and  is a normal codata type)
and 
 
is a \emph{poly-heap bound on observed size} where now 
can have 
as a type-0 variable.

\begin{theorem}[ Poly-Heap Size-Boundness] \label{t:RS:size}
  For an -judgment  where
   and each  is a ground type, one
  can effectively find a normal second-order polynomial  such
  that, if  is a data-type, then  and, if  is a codata-type, then
  .
\end{theorem}

 satisfies appropriate \emph{poly-cost boundness}
and \emph{poly-completeness} properties with proofs similar to the
analogous (type-2) results in \cite{DR:ATS:LMCS,DannerRoyer:2algs};
but, as with , we have not the space to describe, much less
prove, these results.

\emph{Type Restricted  .} 
Let  the functions of type
 computable by -terms with types from
 where  the normal and safe versions of the base 
types occurring in , , and .
For
 and 
,
one can show:
\begin{inparaenum}[\it (i)]
  \item 
 =
  	-regular languages,
  \item 
 =
  	finite-state stream maps,
  \item \label{i:logstr}
 =
    logspace streams, and
  \item \label{i:logtrans}
 =
    logspace stream-functions.
\end{inparaenum}
(In \eqref{i:logstr} and \eqref{i:logtrans},  plays the
r\^{o}le of counter/pointer type as it does in the two-sorted
characterizations considered in \cite{CookNguyen:book}.)




\section{Conclusions}\label{S:fini}

 characterizes a notion of poly-time computation over data 
and codata. As a formalism,  is not much more complicated  
than  the original ones of Bellantoni and Cook
\cite{BellantoniCook} and Leivant \cite{Leivant:FM2}, although
a few of  's additions involve  subtleties.  
The above work suggests many paths for exploration.
Here, briefly, are a few.



\emph{Pola vs.~.}
Pola restricts sharing for its notion of poly-time
over data and codata.  essentially \emph{forces} sharing 
to obtain its notion of poly-time over data and codata. 
How different are these two notions?  Can one notion
``simulate'' the other in some reasonable sense?  Is there
a good notion of poly-time over data and codata that sits above
both the Pola and  notions?


\emph{Higher-types.} Higher-type functions over data-realm and higher-rank streams and
trees in the codata-realm are roughly two different perspectives 
on the same thing.  In investigating true higher-type
extensions of , having these two views may help puzzling out
sensible approaches to higher-type feasibility. 



\emph{Programming in  is clumsy.}  
One problem is that  -recursions carry out their computations 
using  functions, but there are very few
of these that have \emph{closed} definitions in . E.g.,
there is no closed -function that gives the
-maximum of two -values, even though adding
such a function would be a complexity-theoretic conservative 
extension.
Based on an insight first pointed out and studied by
Hofmann \cite{Hofmann03}, any polynomial-time computable
 with  for all , would be a similarly conservative extension to
.  Finding a simple scheme to add to  that allows the definition
of more such functions over data (and the dual notion,
, for functions over codata) is a nice
problem.



                
\bibliography{ops}



\newpage
\appendix
\section*{Technical Appendix}

\setcounter{figure}{0}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{theorem}{0}
\renewcommand{\thetheorem}{A.\arabic{theorem}}
\setcounter{lemma}{0}
\renewcommand{\thelemma}{A.\arabic{lemma}}



\subsubsection{Notes}

\begin{figure}[t]
\begin{minipage}{\textwidth}\small

\caption{ raw syntax, where  identifiers.}\label{fig:core:syn}
\end{minipage}
\begin{minipage}{\textwidth}\small
0ex] \rulelabel{-I}
   \irule{}{\Gamma\entails () \of \Unit}
	\Quad{2}
    \rulelabel{-I}
    \irule{ 
	            \Gamma\entails e_1\of\sigma_1
	            \Quad{1.5}
	            \Gamma\entails e_2\of\sigma_2}{
                \Gamma\entails (e_1,e_2)
                \of\sigma_1\times\sigma_2} 
\Quad{2}
    \rulelabel{-E}
    \irule{ 
	            \Gamma\entails e\of\sigma_{1}\times\sigma_2}{
                \Gamma\entails (\proj_i e)\of\sigma_i} 
					\sidecond{\dagger}
\
\caption{ typing rules.\quad  () .}
  \label{fig:core:typing}
\end{minipage}
\begin{minipage}{\textwidth}\small

\caption{ evaluation rules. 
\   () .
\  () , but  avoids
clashes with .
}
\label{fig:core:eval}
\end{minipage}
\end{figure}
\begin{enumerate}
 \item 
    The ``S'' in  and  stands for \emph{structure} and 
    the ``R'' in  and  stands for \emph{ramified}.
 \item 
    Internal 
    representations of constructors are {underlined}
    as in Figs.~\ref{fig:S-} and~\ref{fig:core:eval}.
 \item 
   \emph{The side-condition of \emph{Pair}-rule in 
   Fig.~\ref{fig:core:eval}.}
   If  and 
   , then  and  
   may  be inconsistent. Hence in \emph{Pair},  is alpha-reduced
   to  so that the - and -evaluations introduce 
   distinct
   variables into their value's environments.
\item 
    The unsugared version of  is
	.


 \item 
   \emph{Call-by-value and growth.}
Note that for  of ground-type,    
     
   because, by the call-by-value semantics,  is evaluated to a 
   value  (i.e., a reference to a data-representation) which 
   becomes the value of  used in . This is explicit in 
   our closure-based evaluation semantics, since this expression 
   evaluates to .
 \item  
   \emph{Dodging exponential growth.}
   \emph{If} one could define a function  such that
    and , then
    could be exponentially larger than . 
   Theorem~\ref{t:RS-:size} implies that no such  is -definable, but intuitively the reason is  that 
a  definition provides \emph{one} reference to the
   result of the recursive call since the -constructor is
   unary.  This one reference can be used multiple times, but always
   representing links to the same result, and hence not increasing the
   size.  The function that \emph{is} -definable is (in effect)
    and .   
   
 \item 
    \emph{Bellantoni and Cook's Raising Rule.}  It amounts to a (sound!) 
    specialization of  Whitehead and Russell's \emph{Axiom of 
    Reducibility}.  Compare the end of the first paragraph of 
    \cite[\S5]{BellantoniCook} and 12.1 of 
    \emph{Principia Mathematica}, Vol.~1, 1/e, Cambridge University Press, 
    1910,  available from 
    \texttt{http://name.umdl.umich.\allowbreak{}edu/AAT3201.0001.001}.

\item 
  Consider  where 
   the number of leaves of .
   cannot compute this because 
  can   exponentially-larger than .
  In contrast, Pola can compute this as Pola
  allows some forms of change-of-parameter in recursions and,
  under Pola,  is always a strict tree.
\item 
  \emph{Codata, Memoization, and Sharing.}
  Corecursions (s) are not memoized, but structure
  sharing is allowed in codata. 


\item

  \emph{Codata and poly-completeness.}
  Since type-level 1  functions can have codata inputs 
  and outputs, we can translate some standard examples from
  type-2 complexity to show that  is missing some 
  functions over codata, where these functions' runtime
  complexity is comparable to that of -computable 
  functions. 
  The cure to this problem is to introduce an analogue of 
  Bellantoni's  function 
  \cite[Chapter 8]{bellantoni:thesis}  
  () or the authors' 
  function \cite[\S4]{DR:ATS:LMCS}
  (, if ; 
  , otherwise) both of which are 
   functions. 
  As to the motivations for such functions and their odd typing
  we refer the reader to \cite{DR:ATS:LMCS}.
  Adding such a function to  is \emph{not} a major change.
  

\end{enumerate}




The next lemma is a key property of terms with normal types.
Its proof is a simple induction on type derivations.

\begin{lemma}
\label{l:L1:sclosed} 
  If 
 where  is normal,
  then .
\end{lemma} 


\begin{lemma}[Basic Poly-Heap Bounds Arithmetic] \label{l:ph}
Suppose , 
, 
, 
and , 
where  and  are  polynomials over 
 is normal.
Also suppose  with .
Then:
\begin{asparaenum}[(a)]
 \item \label{i:norm:sub}
,
     if  is normal.
  \item \label{i:safe:sub}
,
    if  is safe.
  \item  \label{i:pair}
.
\end{asparaenum}
\end{lemma}

\begin{proof}[Sketch]
\emph{Part~\eqref{i:norm:sub}:} By . 
Hence, by the monotonicity of our polynomials, (a) follows.


\emph{Part~\eqref{i:safe:sub}:}
By monotonicity again 
(and some  abuse of notation):
.

\emph{Part~\eqref{i:pair}:}
A na\"{\i}ve upper bound on  is 
, but this double counts the
structure shared by  and . 
So by eliminating the double counting, 
we have the required bound. 
\qed
\end{proof}

\emph{Poly-Heap vs.~Poly-Max Bounds.}
The analogue of parts \eqref{i:norm:sub} and \eqref{i:safe:sub} 
of Lemma \ref{l:ph} hold for poly-max bounds. 
Bounds of the form of part \eqref{i:safe:sub} are key in 
poly-boundedness arguments for forms of ``safe'' recursions. 
The analogue of Lemma \ref{l:ph}\eqref{i:pair} \emph{fails}
for poly-max bounds.  However, if one requires (\emph{\`a la} Pola)
that  and  have no safe variables in common, then 
the poly-max-analogue of Lemma \ref{l:ph}\eqref{i:pair} 
does hold.
These two alternative ways of counting are at the heart of 
the /Pola  split.  Note that what is a stake in how one
bounds a pair is how, in general, one bounds the size of 
branching structures.








\begin{theorem}[Theorem~\ref{t:RS-:size} Restated] 
Given an  judgment   in which
   and each  is a ground type, one can
  effectively find a normal polynomial  with
  .
\end{theorem}




\begin{proof}[Partial sketch]
Our first problem in exhibiting the upper bound is that  may well 
contain higher-type subterms.  
Let  be the normalized version of .  Note
that , where  can be much larger
than .  But a poly-heap bound on  serves as a
bound on .  Thus, we assume without loss of generality that
 is normalized.
Since  is normalized, the only place a -expression can 
occur in  is
as the first argument of a -construct, moreover, these
-expressions have level-1 types.
Also note that each variable occurring in  must be of ground type.

The proof is a structural induction on the derivation of 
.  We consider the last rule
used in this derivation.  

All of the cases, save one, are standard, straightforward arguments---adjusting for the change from poly-max to poly-heap 
bounds.  So we omit these. The interesting case is the one for 
.  We treat this case which, for simplicity 
and concreteness, we further narrow to the case for
, which 
touchs on the key issues in the general -case.
Recall that
, \
; 
,
and . 


\emph{Some conventions:}  To cut down on clutter, 
when  is of ground type  and  is a type-
value  (i.e., a pointer to an internal representation of 
a type- object), we shall rewrite 
 to , 
\emph{provided} the value named by  is a function
of .  The substitution of the (pointer)  for
the variable  in  is, in essence, just cutting out
one level of indirection and thus simplifies reasoning about
the value of .
Similarly, in ``heap'' expressions 
we allow value terms (i.e., pointers to representations) 
among the 's with the obvious meaning of 
, again we are simply cutting out a
level of indirection.  Finally, if  a set of -many
expressions , then 
.





\textsc{Case:} -\emph{I}.  
Thus, , where 
, 
, 
, and  is a safe base type.
By the induction hypothesis, there are normal polynomials 
and  that , 
and .  Fix an environment 
and suppose .  Recall that
 is a pointer to the dag-representation of 's value.
(Since  is a data-constant, it suffices to take .) 
Let  be pointers to the other -cons-cells
in the representation, ordered so that, for all  and ,
if  is an dag-ancestor of , then . 
Suppose, for , , where  is a pointer
to the dag-representation of the result of the -recursion. \textbf{N.B.}  The 's and 's are functions
of .  So, as a reminder of this, 
 in our bounds calculations, we shall make explicit
the usually suppressed .


\smallskip\noindent\emph{Claim 1:} Suppose the 
hypotheses of Lemma~\ref{l:ph} and suppose  is safe.
Then 
,
  for all .

\smallskip\noindent\emph{Proof:}
This is just an extension of the proof of Lemma~\ref{l:ph}(b).


\smallskip\noindent\emph{Claim 2:} For each :
\begin{asparaenum}[(a)]
  \item If , 
  	then .
  \item If , 
  	then 
	      where .
  \item .
\end{asparaenum}

\smallskip\noindent\emph{Proof:}
Part (a) is a  straightforward calculation.  

Part (b) is another straightforward calculation, taking into 
account that the implementation of  is memoizing.

Part (c). \emph{Case:}  is a leaf. 
Then

\emph{Case:}  is a fork. 
Then


Thus by Claim 2(c), .
Recall that .
Therefore,  suffices for this case.

The effectiveness part of the theorem follows from the fact
that the induction argument essentially describes a recursive 
algorithm for constructing .  \qed
\end{proof}





\begin{theorem}[Theorem~\ref{t:RS:size} Restated]
  For an -judgment  where
   and each  is a ground type, one
  can effectively find a normal second-order polynomial  such
  that, if  is a data-type, then  and, if  is a codata-type, then
  .
\end{theorem}  



\begin{proof}[Partial sketch]
As in the proof of Theorem~\ref{t:RS-:size}, we may without loss of
generality assume that  is normalized.  Thus 
the only place a -expression can occur in  is 
as the first argument of a - or an -construct, 
and moreover, these -expressions have level-1 types.
Also note that each variable occurring in  must be of ground type. 
Our proof is a structural induction on the derivation of 
.  
We consider the last rule used in this derivation.


Now, as in our sketch of the proof of Theorem~\ref{t:RS-:size}, 
here we shall present just one key case (-\emph{I}), 
and in fact, a specialization of that (-\emph{I}).
Unlike the situation for the proof of Theorem~\ref{t:RS-:size},
the omitted cases here are less standard and a few 
involve some fine points.  However, almost all of these omitted 
cases parallel problems we dealt with our work on feasible
type-level 2 programming formalisms 
\cite{DR:ATS:LMCS,DannerRoyer:2algs}.




\textsc{Case:} -\emph{I}.
We consider the case where  is a \emph{data} type.
Thus, , where 
 is a safe ground data-type,
, and .  
Recall 
,
, and
.
Let  and , then
for all :

Now, by the induction hypothesis, there are normal polynomials
 and  such that
 and
.  
By \eqref{e:nth}, to bound  for , 
it suffices to bound  for 

and .
For , 
.
Iterating this, we have for , 
.
Hence,  suffices.


In the case where  is a \emph{codata} type, the basic 
structure of the argument stays the same but the (second-order
polynomial) algebra becomes more involved. 





The induction above essentially describes a recursive algorithm 
for constructing .  Hence, the effectiveness part of the 
theorem follows. 
\qed
\end{proof}

\end{document}
