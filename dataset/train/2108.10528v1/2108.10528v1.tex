\vspace{-0.3cm}
\section{Experiments}
\vspace{-0.2cm}
\paragraph{Datasets and metrics.}
Among the existing RGB-D segmentation problems, the indoor semantic segmentation is rather challenging, as the objects are often complex and with severe occlusions~\cite{chen2021spatial}. Thus, in order to validate the effectiveness of the proposed method, we conducted experiments on three indoor RGB-D benchmarks: NYU-Depth-V2 (NYUDv2-13 and -40)~\cite{silberman2012indoor}, SUN-RGBD~\cite{song2015sun} and Stanford Indoor Dataset (SID)~\cite{armeni2017joint}. NYUDv2 contains 1,449 RGB-D scene images, where 795 images are split for training and 654 images for testing. We adopted two popular settings for this dataset, i.e., 13-class~\cite{silberman2012indoor} and 40-class~\cite{gupta2013perceptual}, where all pixels are labeled with 13 and 40 classes, respectively. SUN-RGBD is composed of 10,355 RGB-D indoor images with 37 categories for each pixel label. We followed the widely used setting in ~\cite{song2015sun} to split the dataset into a training set of 5285 images and a testing set of 5050 images. SID contains 70, 496 RGB-D images with 13 object categories. In particular, areas 1, 2, 3, 4, and 6 used for the training and Area 5 is for testing following~\cite{wang2018depth}. 


We reported the results using the same evaluation protocol and metrics as FCN~\cite{long2015fully}, i.e., Pixel Accuracy (\emph{Pixel Acc.}), Mean Accuracy (\emph{Mean Acc.}), Mean Region Intersection Over Union (\emph{Mean IoU}), and Frequency Weighted Intersection Over Union (\emph{f.w. IoU}).

\vspace{-0.6cm}
\paragraph{Comparison protocol.} We adopted several popular architectures with different backbones as our baseline methods to demonstrate the effectiveness and generalization capability of ShapeConv. For all the baseline methods, we only replaced the vanilla convolutional layers with our ShapeConv, without any change to other settings. This guarantees that the obtained performance improvements is due to the application of ShapeConv, but not other factors. 

\begin{table}[h!]
\centering
\caption{
\label{tab:nyu-13}
Performance comparison with baselines on NYUDv2-13 dataset. Deeplabv3+ is the adopted architecture. }
\scalebox{0.8}{
\begin{tabular}{c|c|c|c|c|c}
\hline
Back  &\multirow{2}{*}{Setting} & Pixel  & Mean   & Mean  & f.w. \\
 bone                          &         & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
                        \hline
                                   & Baseline  & 80.0 & 72.5      & 60.8   & 67.6      \\ \cline{2-6} 
                                   & Baseline        & 80.6 & 72.7      & 61.6   & 68.5     \\ \cline{2-6} 
  ResNet                           & Ours & 80.4 & 73.0      & 61.8   & 68.1      \\ \cline{2-6} 
  50~\cite{he2016deep}             & Ours         & 81.1 & 73.4      & 62.7    & 69.1      \\ \cline{2-6} 
                                   & +  & 0.4  & 0.5       & 1.0     & 0.5      \\ \cline{2-6} 
                                   & +    & 0.5  & 0.7  & 1.1     & 0.6       \\ \hline
                                   & Baseline  & 80.0 & 73.4      & 61.3    & 67.6      \\ \cline{2-6} 
                                   & Baseline        & 81.0 & 74.3      & 63.1    & 68.9     \\ \cline{2-6} 
   ResNet                          & Ours & 81.2 & 74.9      & 62.9   & 69.1     \\ \cline{2-6} 
    101~\cite{he2016deep}         & Ours        & 81.9 & 75.7      & 64.0    & 70.1      \\ \cline{2-6} 
                                   & +   & 1.2  & 1.5      & 1.6     & 1.5      \\ \cline{2-6} 
                                   & +   & 0.9  & 1.4      & 0.9      & 1.2       \\ \hline
                                   & Baseline  & 81.8& 73.9     & 63.2   & 70.1      \\ \cline{2-6} 
                                   & Baseline       & 82.2 & 74.4      & 63.7   & 70.6     \\ \cline{2-6} 
  ResNext                       & Ours & 82.6 & 75.7      & 65.1    & 71.2      \\ \cline{2-6} 
 101\_32x8d                     & Ours        & 82.9 & 76.0      & 65.6    & 71.6      \\ \cline{2-6} 
  ~\cite{xie2017aggregated}     & +         & 0.8  & 1.8       & 1.9   & 1.1       \\ \cline{2-6} 
                                   & +  & 0.7  & 1.6      & 1.9   & 1.0       \\ \hline
\end{tabular}
}

\end{table}  


\vspace{-0.6cm}
\paragraph{Implementation Details.} We used the ResNet~\cite{he2016deep} and ResNeXt~\cite{xie2017aggregated} initialized with the pre-trained model on ImageNet~\cite{russakovsky2015imagenet} in the backbone stage. If not otherwise noted, the inputs of both the baseline and ours are the concatenation of RGB and HHA images. We adopted both single-scale and multi-scale testing strategies during inference. For the latter one, left-right flipped images and five scales are exploited: [0.5, 0.75, 1.0, 1.25, 1.5, 1.75].  in tables of this section denotes the multi-scale strategy. Note that, no post-processing tricks like CRF~\cite{chen2017deeplab} is used in our experiments.

\begin{figure*}[t!]
	\centering
	\centering
	\includegraphics[width=\textwidth]{seg-vis.pdf}
\caption{Visualization results from NYUDv2 dataset. Input column denotes RGB, Depth, HHA images from top to bottom; the black regions in the GT, Baseline and Ours indicate the ignored category. The upper and lower cases are from NYUDv2-40 and NYUDv2-13, respectively.}
	\label{fig:vis-seg}
\end{figure*}


\begin{table}[h!]
\centering

\caption{
\label{tab:nyu-40}
Performance comparison with baselines on NYUDv2-40 dataset. Deeplabv3+ is the adopted architecture.}
\scalebox{0.8}{
\begin{tabular}{c|c|c|c|c|c}
\hline
Back  &\multirow{2}{*}{Setting} & Pixel  & Mean   & Mean  & f.w. \\
bone                           &         & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
                        \hline
                                   & Baseline  & 73.1 & 57.7      & 45.6   & 59.2     \\ \cline{2-6} 
                                   & Baseline        & 74.2 & 59.0    & 47.1   & 60.2    \\ \cline{2-6} 
  ResNet                         & Ours & 74.1 & 59.1     & 47.3  & 60.5     \\ \cline{2-6} 
  50~\cite{he2016deep}           & Ours    & 75.0 & 60.4     & 48.8  & 61.4     \\ \cline{2-6} 
                                   & +       & 1.0  & 1.4     & 1.7   & 1.3      \\ \cline{2-6} 
                                   & +    & 0.8 & 1.4      & 1.7    & 1.2    \\ \hline
                                   & Baseline  & 73.4 & 58.9      & 45.9    & 59.7      \\ \cline{2-6} 
                                   & Baseline        & 74.4 & 60.2      & 47.6   & 60.7     \\ \cline{2-6} 
      ResNet                        & Ours & 74.5 & 59.5      & 47.4    & 60.8      \\ \cline{2-6} 
       101~\cite{he2016deep}        & Ours     & 75.5 & 60.7      & 49.0   & 61.7     \\ \cline{2-6} 
                                   & +         & 1.1  & 0.6       & 1.59    & 1.1       \\ \cline{2-6} 
                                   & +   & 1.1  & 0.5       & 1.4     & 1.0      \\ \hline

                                   & Baseline  & 74.7 & 61.5      & 48.9    & 61.5      \\ \cline{2-6} 
                                   & Baseline        & 75.4 & 62.6      & 50.3    & 62.2      \\ \cline{2-6} 
  ResNext                       & Ours & 75.8 & 62.8      & 50.2    & 62.6     \\ \cline{2-6} 
   101\_32x8d                         & Ours       & 76.4 & 63.5      & 51.3    & 63.0      \\ \cline{2-6} 
  ~\cite{xie2017aggregated}     & +         & 1.1  & 1.3       & 1.3     & 1.1       \\ \cline{2-6} 
                                   & +   & 1.0  & 0.9       & 1.0    & 0.8       \\ \hline
\end{tabular}
}
\end{table}  

\subsection{Experiments on Different Datasets}
\vspace{-0.1cm}
\paragraph{NYUDv2 Dataset.} We adopted two popular settings for this dataset, i.e., 13-class~\cite{silberman2012indoor} and 40-class~\cite{gupta2013perceptual}, and show the results of baseline and our method with different backbones on NYUDv2-13 and NYUDv2-40 in Table~\ref{tab:nyu-13} and Table~\ref{tab:nyu-40}, respectively. 
It can be seen that architectures with ShapeConv outperform the baselines with a large margin under all settings.


We also compare the performance of our ShapeConv with several recently developed methods in Table~\ref{tab:NYUDv2-13 performance} and Table~\ref{tab:NYUDv2-40 performance}. As illustrated in Table~\ref{tab:NYUDv2-13 performance}, ShapeConv achieves the best over all the four metrics on NYUDv2-13. Compared to the recently proposed  method~\cite{zhao2018dense}, our approach yields around 6.3\% improvements on Mean IOU which is the most commonly used metric for semantic segmentation. In addition, our method also achieves a competitive performance on NYUDv2-40 in Table~\ref{tab:NYUDv2-40 performance}. 

\vspace{-0.2cm}
\begin{table}[h!]
 
   \caption{
        \label{tab:NYUDv2-13 performance}
        Performance comparison with other methods on NYUDv2-13 dataset.}
     \centering
  \scalebox{0.85}{
      \begin{tabular}{c | c|c | c|c  }
       \hline 
         \multirow{2}{*}{Method}  & Pixel  & Mean   & Mean  & f.w. \\
                                 & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
        \hline
        Eigen~\cite{eigen2015predicting} &75.4 &66.9 &- &- \\
        \hline
        MVCNet~\cite{ma2017multi}       & 77.8    &69.5     &   57.3   &- \\
        \hline 
        Ours  & \textbf{82.6} & \textbf{75.7}      & \textbf{65.1}    & \textbf{71.2}\\
        \hline
        \hline
        MVCNet~\cite{ma2017multi}       & 79.1    &70.6     &   59.1   &- \\
        \hline 
        PVNet~\cite{zhao2018dense}  &82.5 &74.4 &59.3 &- \\
        \hline
        Ours        &\textbf{ 82.9} & \textbf{76.0 }     & \textbf{65.6 }   & \textbf{71.6 }    \\
        \hline
        
      \end{tabular}
  }


\end{table} 

\vspace{-0.4cm}
\begin{table}[h!]
 
   \caption{
        \label{tab:NYUDv2-40 performance}
        Performance comparison with other methods on NYUDv2-40 dataset.}
     \centering
  \scalebox{0.85}{
      \begin{tabular}{c | c|c | c|c  }
       \hline 
         \multirow{2}{*}{Method}  & Pixel  & Mean   & Mean  & f.w. \\
                                 & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
        \hline
        FCN~\cite{long2015fully}     & 65.4   &  46.1      &  34.0   &49.5\\
        \hline 
        LSD-GF~\cite{cheng2017locality} &71.9 &60.7 &45.9 &59.3\\
        \hline
        D-CNN~\cite{wang2018depth} & -   &61.1  & 48.4 & -\\
        \hline 
        MMAF-Net~\cite{fooladgar2019multi}  &72.2     & 59.2        & 44.8  &-    \\
        \hline
        ACNet~\cite{hu2019acnet}  &- & - &48.3 &- \\
        \hline
        Ours  & \textbf{75.8} & \textbf{62.8}      & \textbf{50.2}    & \textbf{62.6}  \\
        \hline
        \hline
        CFN~\cite{lin2017cascaded}     &  -   &  -       & 47.7 &- \\
        \hline 
        3DGNN~\cite{qi20173d}       & -    & 55.7        & 43.1  &- \\
        \hline 
        RDF~\cite{park2017rdfnet}   &76.0     &62.8   &50.1  &-\\
        \hline 
        M2.5D~\cite{xing2020malleable}  &\textbf{76.9} &- &50.9 &-  \\
        \hline
        SGNet~\cite{chen2021spatial}  & 76.8 &63.3 &51.1 & -\\
        \hline
        Ours       & 76.4 & \textbf{63.5}      & \textbf{51.3}    & \textbf{63.0}\\
        \hline
        
      \end{tabular}
  }


\end{table} 


\paragraph{SUN-RGBD Dataset.}
The comparison results between baseline and ours with SUN-RGBD dataset are reported in Table~\ref{tab:sun}. It can be observed that our ShapeConv also produces a positive effect under all settings. We also compared the performance of ours with several recently developed methods in Table~\ref{tab:SUN-RGBD performance}. It is worth noting that the performance of the ShapeConv-enhanced Network with backbone of ResNet-50 in Table~\ref{tab:sun} has already achieved better results than several methods in Table~\ref{tab:SUN-RGBD performance}, such as 3DGNN-101~\cite{qi20173d} and RDF-152~\cite{park2017rdfnet} which take the ResNet-101 and -152 as backbone, respectively.


\begin{table}[]
\centering
\caption{
\label{tab:sun}
Performance comparison with baselines on SUN-RGBD dataset. The architectures adopted in this table is deeplabv3+ with different backbones.}
\scalebox{0.8}{
\begin{tabular}{c|c|c|c|c|c}
\hline
\multirow{2}{*}{Backbone}  &\multirow{2}{*}{Setting} & Pixel  & Mean   & Mean  & f.w. \\
                           &         & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
                        \hline
                     & Baseline  & 81.1 & 56.5      & 45.5    & 69.7      \\ \cline{2-6} 
                        & Baseline         & 81.4 & 57.5      & 46.6    & 70.0      \\ \cline{2-6} 
     ResNet                   & Ours & 81.6 & 56.8      & 46.3    & 70.3      \\ \cline{2-6} 
     50~\cite{he2016deep}     & Ours       & 81.9 & 57.9      & 47.7   & 70.6     \\ \cline{2-6} 
                        & +         & 0.5  & 0.3       & 0.8    & 0.6       \\ \cline{2-6} 
                        & +    & 0.5  & 0.4      & 1.1     & 0.6      \\ \hline
                        & Baseline  & 81.6 & 57.8      & 46.9    & 70.4      \\ \cline{2-6} 
                        & Baseline         & 81.6 & 58.4      & 47.6   & 70.5      \\ \cline{2-6} 
   ResNet                     & Ours & 82.0 & 58.5      & 47.6    & 71.2      \\ \cline{2-6} 
     101~\cite{he2016deep}    & Ours        & 82.2 & 59.2      & 48.6    & 71.3      \\ \cline{2-6} 
                        & +         & 0.4  & 0.7       & 0.7     & 0.8      \\ \cline{2-6} 
                        & +    & 0.6  & 0.8       & 1.0    & 0.8       \\ \hline
   
\end{tabular}
}

\end{table}  

\begin{table}[h!]
\centering


\vspace{-0.2cm}
\caption{
\label{tab:SUN-RGBD performance}
Performance comparison on SUN-RGBD dataset.}
\scalebox{0.85}{
\begin{tabular}{c | c|c | c|c  }
       \hline 
         \multirow{2}{*}{Method}  & Pixel  & Mean   & Mean  & f.w. \\
                                 & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
        \hline
        3DGNN-101~\cite{qi20173d}       & -    & 55.7        & 44.1   &-\\
        \hline 
        D-CNN-50~\cite{wang2018depth} & -   &53.5  & 42.0 & -\\
        \hline 
        MMAF-Net-152~\cite{fooladgar2019multi}  &81.0     & 58.2        & 47.0  &-    \\
        \hline
        SGNet-101~\cite{chen2021spatial}  & 81.0 &\textbf{59.8} &47.5 & -\\
        \hline
        Ours-101  & \textbf{82.0}    & 58.5     & \textbf{47.6}  & \textbf{71.2} \\
        \hline
        \hline
        CFN-101~\cite{lin2017cascaded}     &  -   &  -       & 48.1 &- \\
        \hline 
        3DGNN-101~\cite{qi20173d}       & -    & 57.0        & 45.9  &- \\
        \hline 
        RDF-152~\cite{park2017rdfnet}   &81.5     &60.1   &47.7  &-\\
        \hline 
        SGNet-101~\cite{chen2021spatial}  & 82.0 &\textbf{60.7} &48.6 & -\\
        \hline
        Ours-101  & \textbf{82.2}    &59.2      &\textbf{48.6}   &\textbf{71.3}  \\
        \hline
        
      \end{tabular}
}
\end{table} 


\vspace{-0.7cm}
\paragraph{SID Dataset.}
Note that SID dataset is much larger than the other two datasets, contributing to a better testbed for evaluating RGB-D semantic segmentation model capabilities. The results on SID dataset between the baseline with ours and the state-of-the-art methods are reported in Table~\ref{tab:SID-performance}. We can observe that our ShapeConv surpasses these methods with a large margin. Note that even though we utilized a strong baseline (ResNet-101 backbone) which surpasses MMAF-Net-152 (ResNet-152 backbone) with 1.7\% Mean IoU, our ShapeConv can still achieves a 6\% Mean IoU improvement. This highlights the effectiveness of our method.


\begin{table}[h!]
  \centering
\caption{
\label{tab:SID-performance}
Performance comparison on SID dataset. The architectures of baseline and ours adopted in this table is deeplabv3+ with ResNet-101 backbone and the ``+'' denote the deltas relative to the baseline method.}
  \scalebox{0.85}{
      \begin{tabular}{c | c | c | c | c }
       \hline 
        \multirow{2}{*}{Method}  & Pixel  & Mean   & Mean  & f.w. \\
                                 & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
        \hline 
        D-CNN~\cite{wang2018depth}  & 65.4  &55.5 & 39.5  &49.9\\
        \hline 
        MMAF-Net-152~\cite{fooladgar2019multi}  &76.5     & 62.3        & 52.9  &-    \\
        \hline
        \hline
        Baseline-101  &78.7     &63.2   & 54.6        & 65.6   \\ \hline
        Ours-101  &\textbf{82.7} &\textbf{70.0} &\textbf{60.6} &\textbf{71.2}\\
        \hline 
        +         &  4.0   &    6.8        &   6.0      &   5.6 \\ \hline
      \end{tabular}
  }


\end{table}  


\subsection{Experiments on Different Architectures}
\label{sec:diff ar}
\vspace{-0.2cm}
Our proposed ShapeConv is a general layer for RGB-D semantic segmentation which can be easily plugged into most CNNs as a replacement for the vanilla convolution in semantic segmentation. To verify its generalization properties, we also evaluated the effectiveness of our method in several representative semantic segmentation architectures: Deeplabv3+~\cite{chen2018encoder}, Deeplabv3~\cite{chen2017rethinking}, UNet~\cite{ronneberger2015u}, PSPNet~\cite{zhao2017pyramid} and FPN~\cite{lin2017feature} with different backbones (ResNet-50~\cite{he2016deep}, ResNet-101~\cite{he2016deep}) on NYUDv2-40 dataset, and reported the performance in Table~\ref{tab:baseline-backbone}. We can see that ShapeConv brings significant performance improvements under all settings, demonstrating the generalization capability of our method.

\begin{figure*}[t!]
	\centering
	\centering
	\includegraphics[width=\textwidth]{trimap.pdf}
	\vspace{-0.035\linewidth}
	\caption{Segmentation accuracy around object boundaries. In this figure, the left is the visualization of the ``trimap'' measure; The right is the percent of misclassified pixels within trimaps of different widths.}
	\label{fig:trimap}
\end{figure*}

\begin{table}[h!]
\centering
\caption{
\label{tab:baseline-backbone}
Performance comparison with different baseline methods on NYUDv2-40 dataset.}
\scalebox{0.70}{
\begin{tabular}{c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Architecture} &Back &\multirow{2}{*}{Setting} & Pixel  & Mean   & Mean  & f.w. \\
                  &  bone    &         & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
\hline
                    &Res  & Baseline      & 73.4 & 58.9      & 45.9    & 59.7      \\ \cline{3-7} 
                    &Net  & Ours     & 74.5 & 59.5     & 47.4   & 60.8      \\ \cline{3-7} 
 Deeplabv3+         & 101 & +             & 1.1  & 0.6       & 1.5     & 1.1       \\ \cline{2-7} 
 ~\cite{chen2018encoder} &Res   & Baseline      & 73.1 & 57.7      & 45.6   & 59.2     \\ \cline{3-7} 
                         &Net   & Ours     & 74.1 & 59.1     & 47.3  & 60.5     \\ \cline{3-7} 
                         &50    & +            & 1.0  & 1.4     & 1.7   & 1.3       \\ \hline
                         &Res   & Baseline      & 73.3 & 57.3      & 45.1    & 59.2     \\ \cline{3-7} 
                         &Net   & Ours     & 73.6 & 58.5      & 46.4    & 59.7      \\ \cline{3-7} 
 Deeplabv3               &101   & +             & 0.3 & 1.2       & 1.3    & 0.5       \\ \cline{2-7} 
 ~\cite{chen2017rethinking} & Res  & Baseline      & 71.6 & 55.5      & 43.2    & 57.2      \\ \cline{3-7} 
                            &Net   & Ours     & 72.8 & 56.6     & 44.9    & 58.5     \\ \cline{3-7} 
                            & 50   & +       & 1.2  & 1.1      & 1.7    & 1.3       \\ \hline
                          &Res & Baseline    & 70.9 & 54.7      & 42.1    & 57.7     \\ \cline{3-7} 
                          &Net & Ours   & 72.3 & 56.5     & 43.9   & 58.8      \\ \cline{3-7} 
UNet                      & 101     & +      & 1.4  & 1.8        & 1.8     & 1.1      \\ \cline{2-7} 
 ~\cite{ronneberger2015u} &Res  & Baseline      & 70.0 & 51.7      & 39.7   & 55.5     \\ \cline{3-7} 
                          &Net  & Ours     & 70.8 & 54.1     & 42.0    & 56.9      \\ \cline{3-7} 
                          &50   & +      & 0.8 & 2.4        & 2.3     & 1.4      \\ \hline
                          &Res   & Baseline      & 72.8 & 56.8      & 44.2    & 58.9      \\ \cline{3-7} 
                          &Net  & Ours     & 73.3 & 59.2      & 46.3    & 59.6      \\ \cline{3-7} 
PSPNet                    &101  & +             & 0.5  & 2.4      & 2.1    & 0.7     \\ \cline{2-7} 
 ~\cite{zhao2017pyramid}  &Res  & Baseline      & 71.1 & 53.6      & 42.0    & 56.7      \\ \cline{3-7} 
                          &Net  & Ours     & 72.0 & 56.2      & 44.0    & 57.7     \\ \cline{3-7} 
                          &50   & +             & 0.9 & 2.6       & 2.0      & 1.0      \\ \hline
                          &Res  & Baseline & 72.8 & 57.3      & 44.7    & 59.1     \\ \cline{3-7} 
                          &Net  & Ours     & 73.6 & 58.4      & 45.9   & 60.0     \\ \cline{3-7} 
 FPN                      & 101 & +             & 0.8  & 1.1      & 1.2   & 0.9      \\ \cline{2-7} 
 ~\cite{lin2017feature}   & Res & Baseline      & 70.3 & 52.8      & 40.9    & 56.0      \\ \cline{3-7} 
                          & Net & Ours     & 71.5 & 54.9     & 42.8    & 57.5     \\ \cline{3-7} 
                          & 50  & +             & 1.2  & 2.1      & 1.9    & 1.5      \\ \hline
\end{tabular}
}
\end{table}  


\subsection{Visualization}
\vspace{-0.2cm}
Figure~\ref{fig:vis-seg} illustrates the qualitative results on NYUDv2-13 and -40, more results can be found in the Supp. As shown in this figure, the depth information, especially the detailed one, can be well utilized by ShapeConv to extract the object features. For instance, the chair and table regions in the top example of Figure~\ref{fig:vis-seg}(a) are with gradually changed colors, making it hard to predict accurate segmentation boundaries of the baseline method. The shape features learned by ShapeConv makes the accurate cut following the geometric hints compare with the conventional convolutional layer. For other two cases, i.e., the chair in the bottom example of Figure~\ref{fig:vis-seg}(a) and the desk in the top example of Figure~\ref{fig:vis-seg}(b), the ShapeConv can also significantly improve the segmentation results in edge areas compared with the baseline. It is worth noting that for the multiple bookshelves in the bottom example of Figure~\ref{fig:vis-seg}(b), ShapeConv achieves more consistent predictions. This is because our ShapeConv yields a positive tendency for smoothing neighborhood regions within same classes.


To validate the effectiveness of our method on modeling the depth information, we adopted the comparison strategy proposed by Kohli \etal~\cite{kohli2009robust}. Specifically, we counted the relative number of misclassified pixels within a narrow band (``trimap'') surrounding ground-truth object boundaries. As shown in Figure~\ref{fig:trimap}, our method outperforms the baseline across all trimap widths. This further demonstrates the segmentation effectiveness of our method on edge areas, where the shape information matters.

\vspace{-0.1cm}
\subsection{Ablation Study}
\vspace{-0.2cm}
We conducted ablation experiments to validate the indispensability of the two introduced weights in Equation~\ref{eq:shapeconv-kernel}. As can be observed in Table~\ref{tab:wb-ws-ab}, the model performance degrades when removing either  or , or both. This proves that both the base-kernel and shape-kernel are essential for the final performance improvement, and combing these two achieves the best results.

\vspace{-0.2cm}
\begin{table}[h]
	\centering
		\caption{\label{tab:wb-ws-ab}
	Performance comparison with and without  and  in ShapeConv on NYUDv2-40. The architecture adopted in this table is deeplabv3+ with ResNet-101 as backbone.}
	\scalebox{0.8}{
	\begin{tabular}{ c c | c c c c}
	\hline 
        \multirow{2}{*}{} &\multirow{2}{*}{}  & Pixel  & Mean   & Mean  & f.w. \\
        &      & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
        \hline
         &  & 73.4 & 58.9      & 45.9    & 59.7      \\ 
       \hline
         & & 73.9 & 59.4 &47.0  &60.1   \\
        \hline
        & &74.1 &59.2  &46.3 & 60.1  \\
        \hline
         & & 74.5 & 59.5      & 47.4    & 60.8  \\
        \hline
	\end{tabular}
	}
\end{table}
\vspace{-0.4cm}

To provide a more in-depth analysis of ShapeConv, we conducted detailed ablation studies on the NYUDv2-40 dataset with deeplabv3+ and ResNet-101 as baseline and backbone, respectively. Results on more datasets can be found at the Supp. Table~\ref{tab:ab-ex-nyu-40} illustrates the results and the key observations from this table are as follows: 1) The input features with HHA outperform the Depth images for the baseline and ours; 2) Replacing the vanilla convolution with ShapeConv leads to considerable performance improvements on both Depth and HHA; 3) The multi-scale setting in testing phase brings more performance gains; 4) Cascading the ShapeConv with HHA and multi-scale testing can achieve the best result.


\begin{table}[]
\centering
\caption{
\label{tab:ab-ex-nyu-40}
Ablation study of the proposed ShapeConv on the NYUDv2-40 dataset. RGB, Detph and HHA denote the inputs consisting of RGB images, depth images and HHA images.}
\scalebox{0.75}{
\begin{tabular}{l|l|l|l|l}
\hline
\multirow{2}{*}{Setting}  & Pixel  & Mean   & Mean  & f.w. \\
                                 & Acc.(\%) &Acc.(\%)  &IoU.(\%)  &IoU.(\%) \\
\hline
.RGB                                 & 71.8 & 56.9     & 43.9    & 57.3                                                    \\ 
\hline
.RGB+Depth                       & 72.8 &58.9 &44.9&57.7 \\ 
\hline
.RGB+Depth &  73.9 &59.1 &46.8 &60.0\\
\hline

.RGB+HHA                             & 73.4 & 58.9     & 45.9    & 59.7                                                   \\ 
\hline
.RGB+HHA           & 74.4 & 60.2     & 47.6    & 60.7                                                    \\ 
\hline
.RGB+Depth+ShapeConv             &73.9 &58.2 &46.2 &60.0 \\ 
\hline
.RGB+Depth+ShapeConv   &74.8 &59.2 &47.5 &60.8\\ 
\hline
.RGB+HHA+ShapeConv                   & 74.5 & 59.5      & 47.4    & 60.8                                                   \\ 
\hline
.RGB+HHA+ShapeConv                 & 75.5 & 60.7      & 49.0    & 61.7                                                   \\ 
\hline
\end{tabular}
}
\end{table}  