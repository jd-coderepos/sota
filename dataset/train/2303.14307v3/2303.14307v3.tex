
\begin{table}[!t]
\small
\centering
\renewcommand\arraystretch{0.8}
\begin{tabularx}{.99\linewidth}{v  y  y | y y}
\toprule
    \multirow{2}{*}[-0.1em]{\textbf{Method}} & \multicolumn{4}{c}{\textbf{WER [\%]}} \\
\cmidrule(lr){2-5}
 & A & A & V  & A \\
\midrule\midrule
CM-Transducer~\cite{DBLP:journals/corr/abs-1909-09577} & 1.62 &3.31 & 19.1 &0.99 \\
\midrule
HuBERT~\cite{hsu2021hubert} & 1.90 &6.87 & 19.8 &1.12 \\
\midrule
Wav2vec\,2.0~\cite{baevski2020wav2vec} & 3.40 &11.22 & 19.1 &1.06\\
\midrule
Whisper~\cite{radford2018improving} & 4.10 &1.81 & 19.0 &1.04 \\
\bottomrule
\end{tabularx}
\caption{
Impact of the pre-trained ASR models used to generate automatic transcriptions from unlabelled data on the performance of VSR/ASR models on the LRS3 dataset.  and  denote the word error rate (WER) reported on Librispeech test-clean set~\cite{panayotov2015librispeech} and LRS3 test set~\cite{afouras2018lrs3}, respectively. ``CM'' denotes Conformer. ``V'' and ``A'' denote the visual-only and audio-only models trained on LRW, LRS2, LRS3, VoxCeleb2 and AVSpeech (using the automatically-generated transcriptions from the corresponding pre-trained ASR model), with a total of~3\,448 hours.}
\label{tab:different_asr_models_lrs3}
\vspace{-5mm}
\end{table} \vspace{-1mm}
\subsection{Do better Librispeech ASR models provide better transcriptions for VSR?}
Given that several publicly-available ASR models are available, we use performance on Librispeech as a criterion for model selection. We use models that have achieved state-of-the-art performance on the test-clean set of Librispeech, i.e., Conformer-Transducer~\cite{DBLP:journals/corr/abs-1909-09577} and HuBERT~\cite{hsu2021hubert}. We also use  ASR models that are widely used in the speech community, wav2vec\,2.0~\cite{baevski2020wav2vec} and Whisper~\cite{radford2018improving}.
Performance of ASR models on Librispeech clean-test set is shown in the first column of Table~\ref{tab:different_asr_models_lrs3}. 
Results of the ASR and VSR models trained with the automatically-generated transcriptions on the LRS3 dataset are shown in the third and fourth columns, respectively, of Table~\ref{tab:different_asr_models_lrs3}.
We observe that overall the WER on Librispeech is not highly correlated with the performance of the ASR and VSR models trained with the automatically-generated transcriptions from the corresponding pre-trained ASR models. The same conclusion is also true when we measure the WER on the LRS3 test. We show that using the transcriptions from most ASR models (i.e., wav2vec\,2.0~\cite{baevski2020wav2vec}, Whisper~\cite{radford2018improving}, and Conformer-Transducer~\cite{DBLP:journals/corr/abs-1909-09577}) results in very similar WER for both audio-only and visual-only models. The only exception is the use of automatically-generated transcriptions from HuBERT~\cite{hsu2021hubert} which results in slightly worse performance despite being one of the best performing models on Librispeech. 
In this work, we rely on the automatically-generated transcriptions from the Conformer-Transducer~\cite{DBLP:journals/corr/abs-1909-09577} since on average it leads to the best performance for both ASR and VSR models.

\vspace{-1mm}
\subsection{Impact of the number of hours of unlabelled data}
\begin{table}
\small
\centering
\renewcommand\arraystretch{0.8}
\begin{tabularx}{.99\linewidth}{t | c c c c c c }
\toprule
\textbf{P}& 0\,\% & 20\,\% & 40\,\% & 60\,\% & 80\,\% & 100\,\%
    \\
\textbf{U}& \,h &\,h &\,h &\,h &\,h &\,h
    \\
\textbf{T}& \,h &\,h &\,h &\,h &\,h &\,h
    \\
\midrule\midrule
A & 1.5 & 1.3 & 1.3 & 1.1 & 1.0 &1.0 \\
\midrule
V & 33.0 & 26.6 & 23.6 & 21.9 & 20.0 &19.1 \\
\bottomrule
\end{tabularx}
\caption{
Impact of the size of additional training data (from AVSpeech and VoxCeleb2) on the WER (\%) of audio-only and visual-only models evaluated on LRS3. All models are initialised from a model pre-trained on LRW and trained on LRS2, LRS3 plus X\,\% hours of VoxCeleb2 and AVSpeech. ``\textbf{P}'' and ``\textbf{U}'' denote the amount of additional data in percentages and in hours, respectively. ``\textbf{T}'' denotes the total amount of training data (hours).}
\label{tab:percentage_experiments_on_lrs3}
\vspace{-5mm}
\end{table} Table~\ref{tab:percentage_experiments_on_lrs3} shows the impact of varying the numbers of hours of unlabelled data on the performance of ASR and VSR models on LRS3. An absolute improvement of~1.7\,\% in WER is observed for VSR by using only labelled data from LRS2 and LRS3~(818 hours) compared to~\cite{ma2022visual}. This gain is likely due to the increase in model capacity. When including~20\,\%~(526 hours) of AVSpeech~\cite{DBLP:journals/tog/EphratMLDWHFR18} and VoxCeleb2~\cite{DBLP:conf/interspeech/ChungNZ18}, the performance of audio- and visual-only models can be further improved to 1.3\,\% and 26.6\,\% WER, respectively. Increasing further the number of training hours leads to a further reduction of the WER especially for the VSR model. This is in line with the recent trend observed in the literature~\cite{ma2022visual}, where using larger training sets substantially improves performance. In this experiment, we also show that the WER can be improved even by adding data that have been automatically transcribed and inevitably have noisy labels. We also notice that the improvement for the ASR model is marginal when using more than 1\,578 hours of unlabelled training data, indicating that the ASR performance may have saturated.

\begin{table}[!t]
\centering
\small
\renewcommand\arraystretch{0.9}
\begin{tabularx}{1.05\linewidth}{l y y y y}
\toprule
\textbf{Method} &\textbf{Type} &\textbf{Extra Data} &\textbf{Total Hours} & \textbf{WER} (\%) \\ \midrule\midrule
MV-WAS~\cite{chung2017lip} &\multirow{13}{*}[-0.4em]{V} &\multirow{4}{*}[-0.1em]{\xmark} &\multirow{4}{*}[-0.1em]{223} &70.4 \\
TDNN~\cite{yu2020audio} & & & & 48.9 \\
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657} & & & &39.1 \\
CM-aux~\cite{ma2022visual} & & &  &32.9 \\
\cmidrule(lr){1-1} \cmidrule(lr){3-5}
CTC/Attention \cite{petridis2018audio} & &\multirow{9}{*}[-0.4em]{\cmark}  &380 &63.5 \\
KD\,+\,CTC  \cite{afouras2020asr} & & &995 &51.3 \\
KD-seq2seq~\cite{DBLP:conf/cvpr/RenDLHH21} & & &818 &49.2 \\
TM-seq2seq~\cite{afouras2018deep} & & &1\,391 &48.3 \\
CTC/Attention~\cite{pan2022leveraging} & & &60\,000 &43.2 \\
CM-aux~\cite{ma2022visual} & & &1\,459  &25.5 \\
VTP~\cite{prajwal2022sub} & & &2\,676 &22.6 \\
Ours& & &818 &\textbf{27.9} \\
Ours& & &3\,448 &\textbf{14.6} \\
\midrule
TDNN~\cite{yu2020audio}  &\multirow{5}{*}[-0.4em]{A} &\multirow{2}{*}[-0.1em]{\xmark} &\multirow{2}{*}[-0.1em]{223} &6.7 \\
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657} & & & &4.3 \\
\cmidrule(lr){1-1} \cmidrule(lr){3-5}
CTC/Attention~\cite{pan2022leveraging} & &\multirow{3}{*}[-0.1em]{\cmark} &60\,000 &2.7 \\
Ours& & &818 &\textbf{2.6} \\
Ours& & &3\,448 &\textbf{1.5} \\
\midrule
TDNN~\cite{yu2020audio}  &\multirow{6}{*}[-0.1em]{A+V} &\multirow{2}{*}[-0.1em]{\xmark} &\multirow{2}{*}[-0.1em]{223} &5.9 \\
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657} & & & &4.2 \\
\cmidrule(lr){1-1} \cmidrule(lr){3-5}
TM-seq2seq~\cite{afouras2018deep} & &\multirow{4}{*}[-0.1em]{\cmark} &1\,391 &8.3 \\
CTC/Attention \cite{petridis2018audio} & &  &380 &7.0 \\
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657}  & & &380 &3.9 \\
Ours& & &3\,448 &\textbf{1.5} \\
\bottomrule
\end{tabularx}
\caption{WER (\%) of our audio-only, visual-only and audio-visual models on the LRS2 dataset.  The total hours are counted by including the datasets used for both pre-training and training. Our model trained on 818 hours uses LRW, LRS2 and LRS3. Our model trained on 3\,448 hours uses LRW, LRS2, LRS3, VoxCeleb2 and AVSpeech.}
\vspace{-4mm}
\label{tab:sota lrs2}
\end{table} \begin{table}[!t]
\centering
\small
\renewcommand\arraystretch{0.9}
\begin{tabularx}{1.05\linewidth}{l y y y y}
\toprule
\textbf{Method} &\textbf{Type} &\textbf{Extra Data} &\textbf{Total Hours} & \textbf{WER} (\%) \\ \midrule\midrule
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657} &\multirow{13}{*}[-0.4em]{V} &\multirow{3}{*}[-0.2em]{\xmark} &\multirow{3}{*}[-0.1em]{438} &46.9 \\
CM-aux~\cite{ma2022visual} & & &  &37.9 \\
Ours & & &  &\textbf{36.3} \\
\cmidrule(lr){1-1}\cmidrule(lr){3-5}
KD\,+\,CTC~\cite{afouras2020asr} & &\multirow{10}{*}[-0.4em]{\cmark} &772 &59.8 \\
KD-seq2seq~\cite{DBLP:conf/cvpr/RenDLHH21} & & &818 &59.0 \\
TM-seq2seq~\cite{afouras2018deep} & & &1\,362 &58.9 \\
AVHuBERT~\cite{DBLP:journals/corr/abs-2201-02184} & & &1\,759 & 26.9 \\
RNN-T~\cite{makino2019recurrent} & & &31\,000 &33.6 \\
VTP~\cite{prajwal2022sub} & & &2\,676 &30.7 \\
ViT3D-CM~\cite{serdyuk2022transformer} & & &90\,000 &17.0 \\
Ours & & &818 &\textbf{33.0} \\
Ours& & &1\,902 &\textbf{23.5} \\
Ours & & &3\,448 &\textbf{19.1} \\
\midrule
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657} &\multirow{6}{*}[-0.4em]{A} &\multirow{1}{*}[-0.1em]{\xmark} &438 &2.3 \\
\cmidrule(lr){1-1}\cmidrule(lr){3-5}
RNN-T~\cite{makino2019recurrent} & &\multirow{5}{*}[-0.1em]{\cmark} &31\,000 &4.5 \\
AV-HuBERT~\cite{DBLP:journals/corr/abs-2201-02184}  & & &1\,759 &1.3 \\
Ours& & &818 &\textbf{1.5} \\
Ours& & &1\,902 &\textbf{1.0} \\
Ours& & &3\,448 &\textbf{1.0} \\
\midrule
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657} &\multirow{6}{*}[-0.4em]{A+V} &\multirow{1}{*}[-0.1em]{\xmark} &438 &2.3 \\
\cmidrule(lr){1-1}\cmidrule(lr){3-5}
RNN-T~\cite{makino2019recurrent} & &\multirow{5}{*}[-0.1em]{\cmark} &31\,000 &4.8 \\
AV-HuBERT~\cite{DBLP:journals/corr/abs-2201-02184} & & &1\,759 &1.4 \\
ViT3D-CM~\cite{serdyuk2022transformer} & & &90\,000 &1.6 \\
Ours& & &1\,902 &\textbf{1.0} \\
Ours& & &3\,448 &\textbf{0.9} \\
\bottomrule
\end{tabularx}
\caption{WER (\%) of our audio-only, visual-only and audio-visual models on the LRS3 dataset.  The total hours are counted by including the datasets used for both pre-training and training. Our model trained on 818 hours uses LRW, LRS2 and LRS3. Our model trained on 1\,902 hours uses LRW, LRS3 and VoxCeleb2. Our model trained on 3\,448 hours uses LRW, LRS2, LRS3, VoxCeleb2 and AVSpeech.}
\label{tab:sota lrs3}
\vspace{-1mm}
\end{table} \vspace{-1mm}
\subsection{Comparison with the state-of-the-art}
Results on LRS2 and LRS3 are presented in Tables~\ref{tab:sota lrs2} and~\ref{tab:sota lrs3}, respectively. For LRS2, it is clear that our visual-only, audio-only and audio-visual models further push the state-of-the-art performance to a WER of~14.6\,\%, 1.5\,\% and 1.5\,\% respectively. For LRS3, the best visual-only model has a WER of 19.1\,\%, which is outperformed only by~\cite{serdyuk2022transformer} (17.0\,\% WER) which uses 26 more training data.
Similarly, our audio-only model establishes a new state-of-the-art~\cite{DBLP:journals/corr/abs-2201-02184} by achieving a WER of 1.0\,\% when using 1\,921 hours of training data from LRW, LRS3 and VoxCeleb2 datasets. However, when further introducing AVSpeech for training, no further improvement is observed, suggesting that the ASR performance may have reached saturation. State-of-the-art performance is also achieved for AV-ASR with a WER of 0.9\,\%.


\vspace{-1mm}
\begin{table}[!bt]
\centering
\small
\renewcommand\arraystretch{0.8}
\begin{tabularx}{.99\linewidth}{o o | c c c c c  }
\toprule
\multirow{2}{*}{\textbf{Type}}&\multirow{2}{*}{\textbf{Noise}} &\multicolumn{5}{c}{\textbf{SNR levels [dB]}}
\\
& & 12.5 & 7.5 & 2.5 & -2.5 & -7.5
\\
\midrule\midrule
A &\multirow{2}{*}{Babble} &1.1 & 1.2 & 1.6 & 2.7 & 8.3
\\
\cmidrule(lr){1-1}\cmidrule(lr){3-7}
A+V & &1.0 &1.0 &1.5 &2.2 &5.6
\\
\midrule
A &\multirow{2}{*}{Pink} &1.4 & 1.9 & 4.3 & 13.1 & 56.8
\\
\cmidrule(lr){1-1}\cmidrule(lr){3-7}
A+V & &1.2 &1.4 &2.3 &6.0 &16.2
\\
\midrule
A &\multirow{2}{*}{White} &2.1 &4.0 &10.4 &30.2 &88.9
\\
\cmidrule(lr){1-1}\cmidrule(lr){3-7}
A+V & &1.4 &2.3 &4.3 &9.5 &24.2
\\
\bottomrule
\end{tabularx}
\caption{WER (\%) of our audio-only and audio-visual models as a function of the noise levels on the LRS3 dataset. The babble noise from the NOISEX dataset~\cite{DBLP:journals/speech/VargaS93} is used for training while one of SNR levels from [-5\,dB, 0\,dB, 5\,dB, 10\,dB, 15\,dB, 20\,dB,  dB] is selected with a uniform distribution. For testing, the pink and white noise from the Speech Commands dataset~\cite{DBLP:journals/corr/abs-1804-03209} is added to the raw audio waveforms with a specific SNR level.  denotes the noise type used in both training and test set.}
\label{tab: noise experiments}
\vspace{-6 mm}
\end{table}
 \subsection{Noise experiments}
Results of ASR and AV-ASR models, when tested with different acoustic noise levels, are shown in Table.~\ref{tab: noise experiments}.
During training we use the babble noise from the NOISEX dataset~\cite{DBLP:journals/speech/VargaS93}, while the SNR level is selected from [-5\,dB, 0\,dB, 5\,dB, 10\,dB, 15\,dB, 20\,dB,  dB] with a uniform distribution. For evaluation, we test three types of noise:  babble noise~\cite{DBLP:journals/speech/VargaS93}, pink and white noise from the Speech Commands dataset~\cite{DBLP:journals/corr/abs-1804-03209}.
We show that, overall, the results are consistent with those presented in~\cite{afouras2018deep, DBLP:journals/corr/abs-2102-06657, makino2019recurrent, DBLP:conf/interspeech/ShiHM22}, i.e. the performance of audio-only models is closer to the audio-visual counterpart in the presence of low levels of noise, whereas the performance gap becomes larger as the noise levels increase. We notice that when using babble noise for evaluation, the performance of either audio-only or audio-visual models has a WER lower than 10\,\% at -7.5\,dB. This is likely mainly a consequence of the overlapping noise type in the training and testing phases (despite mismatched levels of noise).