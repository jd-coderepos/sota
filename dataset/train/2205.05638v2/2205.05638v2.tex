\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[preprint]{neurips_2022}









\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[colorlinks=true,allcolors=blue]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{cleveref}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{array}

\newcolumntype{R}[2]{>{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}l<{\egroup}}
\newcommand*\rot{\multicolumn{1}{R{90}{1em}}}

\usepackage{parskip}

\renewcommand{\arraystretch}{1.1} 



\newcommand{\ia}{{\fontfamily{lmtt}\selectfont (IA)\textsuperscript{3}}\xspace}
\newcommand{\tfew}{{\fontfamily{lmtt}\selectfont T-Few}\xspace}
\newcommand{\e}{{\text{e}}} 

\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother


\title{Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning}





\author{Haokun Liu\thanks{Equal contribution.} \quad Derek Tam \quad Mohammed Muqeeth 
  \And Jay Mohta \quad Tenghao Huang \quad Mohit Bansal \quad Colin Raffel \vspace{0.5em} \\ 
  Department of Computer Science\\
  University of North Carolina at Chapel Hill\\
  \texttt{\{haokunl,dtredsox,muqeeth,craffel\}@cs.unc.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input.
ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made.
Parameter-efficient fine-tuning (PEFT) (e.g.\ adapter modules, prompt tuning, sparse update methods, etc.)\ offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task.
In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs.
Along the way, we introduce a new PEFT method called \ia that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters.
We also propose a simple recipe based on the T0 model \cite{sanh2021multitask} called \tfew that can be applied to new tasks without task-specific tuning or modifications.
We validate the effectiveness of \tfew on completely unseen tasks by applying it to the RAFT benchmark \cite{alex2021raft}, attaining super-human performance for the first time and outperforming the state-of-the-art by 6\% absolute.
All of the code used in our experiments is publicly available.\footnote[1]{\label{note:code} \url{https://github.com/r-three/t-few}}
\end{abstract}

\section{Introduction}

Pre-trained language models have become a cornerstone of natural language processing, thanks to the fact that they can dramatically improve \textit{data efficiency} on tasks of interest -- i.e., using a pre-trained language model for initialization often produces better results with less labeled data.
A historically common approach has been to use the pre-trained model's parameters for initialization before performing gradient-based fine-tuning on a downstream task of interest.
While fine-tuning has produced many state-of-the-art results \cite{sanh2021multitask}, it results in a model that is specialized for a single task with an entirely new set of parameter values, which can become impractical when fine-tuning a model on many downstream tasks.

An alternative approach popularized by \cite{radford2019language,brown2020language} is \textit{in-context learning} (ICL), which induces a model to perform a downstream task by inputting \textit{prompted} examples.
Few-shot prompting converts a small collection of input-target pairs into (typically) human-understandable instructions and examples \cite{radford2019language,brown2020language}, along with a single unlabeled example for which a prediction is desired.
Notably, ICL requires no gradient-based training and therefore allows a single model to immediately perform a wide variety of tasks.
Performing ICL therefore solely relies on the capabilities that a model learned during pre-training.
These characteristics have led to a great deal of recent interest in ICL methods \citep{chen2021meta,min2021metaicl,lampinen2022can,lazaridou2022internet,min2022rethinking,wang2022benchmarking}.

Despite the practical benefits of ICL, it has several major drawbacks.
First, processing all prompted input-target pairs every time the model makes a prediction incurs significant compute costs.
Second, ICL typically produces inferior performance compared to fine-tuning \cite{brown2020language}.
Finally, the exact formatting of the prompt (including the wording \cite{webson2021prompt} and ordering of examples \cite{zhao2021calibrate}) can have significant and unpredictable impact on the model's performance, far beyond inter-run variation of fine-tuning.
Recent work has also demonstrated that ICL can perform well even when provided with incorrect labels, raising questions as to how much learning is taking place at all \citep{min2022rethinking}.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{diagram.pdf}
\captionof{figure}{Diagram of \ia and the loss terms used in the \tfew recipe. \textit{Left:} \ia introduces the learned vectors , and  which respectively rescale (via element-wise multiplication, visualized as ) the keys and values in attention mechanisms and the inner activations in position-wise feed-forward networks. \textit{Right:} In addition to a standard cross-entropy loss , we introduce an unlikelihood loss  that lowers the probability of incorrect outputs and a length-normalized loss  that applies a standard  cross-entropy loss to length-normalized log-probabilities of all output choices.}
\label{fig:diagram}
\end{figure}

An additional paradigm for enabling a model to perform a new task with minimal updates is \textit{parameter-efficient fine-tuning} (PEFT), where a pre-trained model is fine-tuned by only updating a small number of added or selected parameters.
Recent methods have matched the performance of fine-tuning the full model while only updating or adding a small fraction (e.g.\ 0.01\%) of the full model's parameters \cite{hu2021lora,lester2021power}.
Furthermore, certain PEFT methods allow \textit{mixed-task batches} where different examples in a batch are processed differently \citep{lester2021power}, making both PEFT and ICL viable for multitask models. 

\looseness=-1
While the benefits of PEFT address some shortcomings of fine-tuning (when compared to ICL), there has been relatively little focus on whether PEFT methods work well when very little labeled data is available.
Our primary goal in this paper is to close this gap by proposing a recipe -- i.e., a model, a PEFT method, and a fixed set of hyperparameters -- that attains strong performance on novel, unseen tasks while only updating a tiny fraction of the model's parameters.
Specifically, we base our approach on the T0 model \cite{sanh2021multitask}, a variant of T5 \cite{raffel2019exploring} fine-tuned on a multitask mixture of prompted datasets.
To improve performance on classification and multiple-choice tasks, we add unlikelihood \cite{tam2021improving,welleck2019neural} and length normalization-based \citep{brown2020language} loss terms.
In addition, we develop \ia, a PEFT method that multiplies intermediate activations by learned vectors.
\ia attains stronger performance than full-model fine-tuning while updating up to 10,000 fewer parameters.
Finally, we demonstrate the benefits of pre-training the \ia parameters before fine-tuning \citep{gu2021ppt, vu2021spot}.
Our overall recipe, which we dub ``\tfew'', performs significantly better than ICL (even against  larger models) and outperforms humans for the first time on the real-world few-shot learning benchmark RAFT \citep{alex2021raft} while requiring dramatically less compute and allowing for mixed-task batches during inference.
To facilitate the use of \tfew on new problems and future research on PEFT, we release our code.

After providing background on ICL and PEFT in the following section, we discuss the design of \tfew in \cref{sec:tfew}.
In \cref{sec:experiments}, we present experiments comparing \tfew to strong ICL baselines.
Finally, we discuss related work in \cref{sec:related} and conclude in \cref{sec:conclusion}.

\section{Background}
In this section, we provide am verview of ICL and PEFT with a focus on characterizing the computation, memory, and on-disk storage costs of making a prediction.
Real-world costs depend on implementation and hardware, so we report costs in terms of FLOPs for computation and bytes for memory and storage, respectively.
Additional related work is discussed in \cref{sec:related}.

\subsection{Few-shot in-context learning (ICL)}
\label{sec:icl}

ICL \citep{radford2019language,brown2020language} aims to induce a model to perform a task by feeding in concatenated and prompted input-target examples (called ``shots'') along with an unlabeled query example.
Taking the cycled letter task from \citet{brown2020language} as an example, a 4-shot input or \textit{context} would be ``\texttt{Please unscramble the letters into a word, and write that word: asinoc = casino, yfrogg = froggy, plesim = simple, iggestb = biggest, astedro =}'', for which the desired output would be ``\texttt{roasted}''.
ICL induces an autoregressive language model to perform this task by feeding in the context and sampling from the model.
For classification tasks, each label is associated with a string (e.g.\ ``\texttt{positive}'' and ``\texttt{negative}'' for sentiment analysis) and a label is assigned by choosing the label string that the model assigns the highest probability to.
For multiple-choice tasks (e.g.\ choosing between  possible answers to a question), the model's prediction is similarly determined by determining which choice is assigned the highest probability.

The primary advantage of ICL is that it enables a single model to perform many tasks immediately without fine-tuning.
This also enables \textit{mixed-task batches}, where different examples in a batch of data correspond to different tasks by using different contexts in the input.
ICL is also typically performed with only a limited number of labeled examples -- called few-shot learning -- making it data-efficient.

Despite these advantages, ICL comes with significant practical drawbacks:
First, making a prediction is dramatically more expensive because the model needs to process all of the in-context labeled examples.
Specifically, ignoring the quadratic complexity of self-attention operations in Transformer language models (which are typically small compared to the costs of the rest of the model \citep{kaplan2020scaling}), processing the  training examples for -shot ICL increases the computational cost by approximately  times compared to processing the unlabeled example alone.
Memory costs similarly scale approximately linearly with , though during inference the memory costs are typically dominated by storing the model's parameters.
Separately, there is a small amount of on-disk storage required for storing the in-context examples for a given task.
For example, storing  examples for a task where the prompted input and target for each example is  tokens long would require about  kilobytes of storage on disk ( examples  tokens  bits).

Beyond the aforementioned costs, ICL also exhibits unintuitive behavior.
\citet{zhao2021calibrate} showed that the \textit{ordering} of examples in the context heavily influences the model's predictions. 
\citet{min2022rethinking} showed that ICL can still perform well even if the labels of the in-context examples are swapped (i.e.\ made incorrect), which raises questions about whether ICL is really ``learning'' from the labeled examples. 

Various approaches have been proposed to mitigate these issues.
One way to decrease computational costs is to cache the key and value vectors for in-context examples.
This is possible because decoder-only Transformer language models have a causal masking pattern, so the model's activations for the context do not do not depend on the unlabeled example.
In an extreme case, -shot ICL with  tokens per in-context example would result in over 144 gigabytes of cached key and value vectors for the GPT-3 model ( examples  tokens  layers  bits \textit{each} for the key and value vectors).
Separately, \citet{min2021noisy} proposed \textit{ensemble ICL}, where instead of using the output probability from concatenating the  training examples, the output probabilities of the model on each training example (i.e.\ -shot ICL for each of the  examples) are multiplied together.
This lowers the non-parameter memory cost by a factor of  but increases the computational cost by a factor of .
In terms of task performance, \citet{min2021noisy} find that ensemble ICL outperforms the standard concatenative variant.

\subsection{Parameter-efficient fine-tuning}

While standard fine-tuning updates all parameters of the pre-trained model, it has been demonstrated that it is possible to instead update or add a relatively small number of parameters.
Early methods proposed adding \textit{adapters} \cite{rebuffi2017learning,Houlsby2019ParameterEfficientTL,bapna2019simple}, which are small trainable feed-forward networks inserted between the layers in the fixed pre-trained model.
Since then, various sophisticated PEFT methods have been proposed, including methods that choose a sparse subset of parameters to train \cite{guo2020parameter,sung2021training}, produce low-rank updates \cite{hu2021lora}, perform optimization in a lower-dimensional subspace \cite{aghajanyan2020intrinsic}, add low-rank adapters using hypercomplex multiplication \cite{mahabadi2021compacter}, and more.
Relatedly, \textit{prompt tuning} \cite{lester2021power} and \textit{prefix tuning} \cite{li2021prefix} concatenate learned continuous embeddings to the model's input or activations to induce it to perform a task; this can be seen as a PEFT method \cite{he2021towards}.
State-of-the-art PEFT methods can match the performance of fine-tuning all of the model's parameters while updating only a tiny fraction (e.g.\ 0.01\%) of the model's parameters.

PEFT drastically reduces the memory and storage requirements for training and saving the model.
In addition, certain PEFT methods straightforwardly allow mixed-task batches -- for example, prompt tuning enables a single model to perform many tasks simply by concatenating different prompt embeddings to each example in the batch \cite{lester2021power}.
On the other hand, PEFT methods that re-parameterize the model (e.g.\ \citep{aghajanyan2020intrinsic,hu2021lora}) are costly or onerous for mixed-task batches.
Separately, different PEFT methods increase the computation and memory required to perform inference by different amounts.
For example, adapters effectively add additional (small) layers to the model, resulting in small but non-negligible increases in computational costs and memory.
An additional cost incurred by PEFT is the cost of fine-tuning itself, which must be performed once and is then amortized as the model is used for inference.
However, we will show that PEFT can be dramatically more computationally efficient when considering both fine-tuning and inference while achieving better accuracy than ICL. 

\section{Designing the \tfew Recipe}
\label{sec:tfew}

Given that PEFT allows a model to be adapted to a new task with relatively small storage requirements and computational cost, we argue that PEFT presents a promising alternative to ICL.
Our goal is therefore to develop a recipe that allows a model to attain high accuracy on new tasks with limited labeled examples while allowing mixed-task batches during inference and incurring minimal computational and storage costs.
By \textit{recipe}, we mean a specific model and hyperparameter setting that provides strong performance on any new task without manual tuning or per-task adjustments.
In this way, we can ensure that our approach is a realistic option in few-shot settings where limited labeled data is available for evaluation \cite{perez2021true,oliver2018realistic}.

\subsection{Model and Datasets}
\label{sec:model_datasets}

As a first step, we must choose a pre-trained model.
Ideally, the model should attain high performance on new tasks after fine-tuning on a limited number of labeled examples.
In preliminary experiments applying PEFT methods to different pre-trained models, we attained the best performance with T0 \cite{sanh2021multitask}.
T0 is based on T5 \citep{raffel2019exploring}, an encoder-decoder Transformer model \citep{vaswani2017attention} that was pre-trained via a masked language modeling objective \citep{devlin2018bert} on a large corpus of unlabeled text data.
T0 was created by fine-tuning T5 on a multitask mixture of datasets in order to enable zero-shot generalization, i.e.\ the ability to perform tasks without any additional gradient-based training.
Examples in the datasets used to train T0 were prompted by applying the prompt templates from the Public Pool of Prompts (P3 \citep{bach2022promptsource}), which convert each example in each dataset to a prompted text-to-text format where each label corresponds to a different string.
For brevity, we omit a detailed description of T0 and T5; interested readers can refer to \citet{sanh2021multitask} and \citet{raffel2019exploring}.
T0 was released in three billion and eleven billion parameter variants, referred to as ``T0-3B'' and simply ``T0'' respectively.
In this section (where our goal is to design the \tfew recipe through extensive experimentation), we use T0-3B to reduce computational costs.
For all models and experiments, we use Hugging Face Transformers \citep{wolf2020transformers}.

While T0 was designed for zero-shot generalization, we will demonstrate that it also attains strong performance after fine-tuning with only a few labeled examples.
To test T0's generalization, \citet{sanh2021multitask} chose a set of tasks (and corresponding datasets) to hold out from the multitask training mixture -- specifically, sentence completion (COPA \citep{copa}, H-SWAG \citep{zellers2019hellaswag}, and Story Cloze \citep{sharma2018tackling} datasets), natural language inference (ANLI \citep{nie2019adversarial}, CB \citep{cb}, and RTE \citep{dagan2005pascal}), coreference resolution (WSC \citep{wsc} and Winogrande \citep{sakaguchi2020winogrande}), and word sense disambiguation (WiC \citep{pilehvar2018wic}).
Evaluation of generalization capabilities can then be straightforwardly done by measuring performance on these held-out datasets.
We also will later test \tfew's abilities in the RAFT benchmark \citep{alex2021raft} in \cref{sec:raft}, a collection of unseen ``real-world'' few-shot tasks with no validation set and a held-out test set. 
ANLI, WiC, WSC is licensed under a Creative Commons License. Winogrande is licnsed under an Apache license.  COPA is under a BSD-2 Clause license. We could not find the license of RTE and CB but they are part of SuperGLUE which mentions the datasets are allowed for use in research context.

To ease comparison, we use the same number of few-shot training examples for each dataset as \citet{brown2020language}, which varies from 20 to 70.
Unfortunately, the few-shot dataset subsets used by \citet{brown2020language} have not been publicly disclosed.
To allow for a more robust comparison, we therefore constructed five few-shot datasets by sampling subsets with different seeds and report the median and interquartile range.
We prompt examples from each dataset using the prompt templates from P3 \citet{bach2022promptsource}, using a randomly-sampled prompt template for each example at each step.
Unless otherwise stated, we train our model for 1K steps with a batch size of 8 and report performance at the end of training.

For evaluation, we use ``rank classification'', where the model's log-probabilities for all possible label strings are ranked and the model's prediction is considered correct if the highest-ranked choice is the correct answer.
Rank classification evaluation is compatible with both classification and multiple-choice tasks.
Since model performance can vary significantly depending on the prompt template used, we report the median accuracy across all prompt templates from P3 and across few-shot data subsets for each dataset.
For all datasets, we report the accuracy on the test set or validation set when the test labels are not public (e.g.\ SuperGLUE datasets).
In the main text, we report median accuracy across the nine datasets mentioned above.
Detailed results on each dataset are provided in the appendices.


\subsection{Unlikelihood Training and Length Normalization}
\label{sec:ul_ln}

Before investigating PEFT methods, we first explore two additional loss terms to improve the performance of few-shot fine-tuning of language models.
Language models are normally trained with cross-entropy loss  where the model is trained to increase the probability of the correct target sequence  given the input sequence . 

For evaluation, we use rank classification (described in \cref{sec:model_datasets}) which depends on both the probability that the model assigns to the correct choice as well as the probabilities assigned by the model to the incorrect choices.
To account for this during training, we consider adding an unlikelihood loss \cite{tam2021improving,welleck2019neural}:

which discourages the model from predicting tokens from incorrect target sequences, where   is the -th of  incorrect target sequences.
We hypothesize that adding  will improve results on rank classification because the model will be trained to assign lower probabilities to incorrect choices, thereby improving the chance that the correct choice is ranked highest.

The possible target sequences for a given training example can have significantly different lengths, especially in multiple-choice tasks.
Ranking each choice based on probability can therefore ``favor'' shorter choices because the model's assigned probability to each token is .
To rectify this, we consider using length normalization when performing rank classification, which divides the model's score on each possible answer choice by the number of tokens in the choice (as used in GPT-3 \citep{brown2020language}). 
When using length normalization during evaluation, we introduce an additional loss term during training that more closely reflects length-normalized evaluation.
First, we compute the length-normalized log probability of a given output sequence .
Then, we maximize the length-normalized log probability of the correct answer choice by minimizing the  cross-entropy loss:

When training a model with , , and , we simply sum them.
This avoids introducing any hyperparameters that would be problematic to tune in the few-shot setting (where realistically-sized validation sets are tiny by necessity \cite{perez2021true,oliver2018realistic}).

We report the results of fine-tuning all of T0-3B's parameters with and without length normalization on all datasets in \cref{sec:full_ul_and_ln}.
We find that adding  improves the accuracy from 60.7\% to 62.71\% and including both  and  provides a further improvement to 63.3\%.
Since these loss terms improve performance without introducing any additional hyperparameters, we include them in our recipe and use them in all following experiments.

\subsection{Parameter-efficient fine-tuning with \ia}
\label{sec:ia}

In order to compare favorably to few-shot ICL, we need a PEFT method that has the following properties:
First, it must add or update as few parameters as possible to avoid incurring storage and memory costs.
Second, it should achieve strong accuracy after few-shot training on new tasks.
Finally, it must allow for mixed-task batches, since that is a capability of ICL.
In order to easily enable mixed-task batches, a PEFT method should ideally not modify the model itself.
Otherwise, each example in a batch would effectively need to be processed by a different model or computational graph.
A more convenient alternative is provided by methods that directly modify the \textit{activations} of the model since this can be done independently and cheaply to each example in the batch according to which task the example corresponds to.
Prompt tuning and prefix tuning methods \citep{lester2021power,li2021prefix} work by concatenating learned vectors to activation or embedding sequences and are therefore examples of activation-modifying PEFT methods that allow for mixed-task batches.
However, as we will discuss later, we were unable to attain reasonable accuracy with prompt tuning and found that the more performant PEFT methods did not allow for mixed-task batches.
We therefore developed a new PEFT method that meets our desiderata.

As an alternative, we explored element-wise multiplication (i.e.\ rescaling) of the model's activations against a learned vector.
Specifically, we consider adaptation of the form  where  is a learned task-specific vector,  represents element-wise multiplication, and  is a length- sequence of activations.
We use ``broadcasting notation'' \cite{van2011numpy} so that the  entry of  is .
In preliminary experiments, we found it was not necessary to introduce a learned rescaling vector for each set of activations in the Transformer model.
Instead, we found it was sufficient to introduce rescaling vectors on the keys and values in self-attention and encoder-decoder attention mechanisms and on the intermediate activation of the position-wise feed-forward networks.
Specifically, using the notation from \citet{vaswani2017attention}, we introduce three learned vectors , and , which are introduced into the attention mechanisms as:

and in the position-wise feed-forward networks as , where  is the feed-forward network nonlinearity.
We introduce a separate set of , and  vectors in each Transformer layer block.
This adds a total of  new parameters for a -layer-block Transformer encoder and  (with factors of 2 accounting for the presence of both self-attention and encoder-decoder attention) for a -layer-block decoder.
, and  are all initialized with ones so that the overall function computed by the model does not change when they are added.
We call our method \ia, which stands for ``Infused Adapter by Inhibiting and Amplifying Inner Activations''.

\ia makes mixed-task batches possible because each sequence of activations in the batch can be separately and cheaply multiplied by its associated learned task vector.
We also note that, in the event that a model will only be used on a single task, the modifications introduced by \ia can also be applied to weight matrices permanently so that no elementwise multiplication is required and the model's architecture remains unchanged.
This possible because element-wise multiplications performed in \ia always co-occur with a matrix multiplication, and .
In this case, our method incurs no additional computational cost compared to the original model.

To validate \ia, we compare it to a large variety of existing adaptation methods in our setting of fine-tuning T0-3B on few-shot datasets from held-out tasks.
Specifically, we compare with 9 strong PEFT methods: BitFit \cite{zaken2021bitfit} which updates only the bias parameters; Adapters \cite{Houlsby2019ParameterEfficientTL} which introduce task-specific layers after the self-attention and position-wise feed-forward networks; Compacter and Compacter++ \cite{mahabadi2021compacter} which improve upon adapters by using low-rank matrices and hypercomplex multiplication; prompt tuning \cite{lester2021power} which learns task-specific prompt embeddings that are concatenated to the model's input; FISH Mask \cite{sung2021training} which chooses a subset of parameters to update based on their approximate Fisher information; Intrinsic SAID \cite{aghajanyan2020intrinsic} which performs optimization in a low-dimensional subspace; prefix-tuning \cite{li2021prefix} which learns task-specific vectors that are concatenated to the model's activations; and LoRA \cite{hu2021lora} which assigns low-rank updates to parameter matrices.
Additionally, we include the baselines of full-model fine-tuning and updating only the layer normalization parameters.
For certain methods that allow changing the parameter efficiency, we report results for different budgets: 0.2\% and 0.02\% sparsity for FISH Mask, 10 and 100 learned prompt vectors for prompt tuning, and 20,000- or 500,000-dimensional subspaces for Intrinsic SAID.

The results are shown in \cref{fig:peft}, with detailed per-dataset results in \cref{sec:full_peft}.
We find that \ia is the only method that attains higher accuracy than the full-model-fine-tuning baseline.
While other PEFT methods (e.g.\ Intrinsic SAID and prompt tuning) update or introduce fewer parameters, \ia performs considerably better.
Our results and setting differ with some past work on the PEFT methods we compare against.
\citet{mahabadi2021compacter} report that Compacter and Compacter++ outperform full-model fine-tuning, including in the few-shot setting.
\citet{lester2021power} found that prompt tuning could match full-model fine-tuning, and in subsequent work \citet{wei2021finetuned} found that prompt tuning performed well when applied to a multitask fine-tuned model in the few-shot setting.
In both cases, we experimented with various hyperparameter choices to try to match past results.
We hypothesize the disagreement comes from us using a different model and different datasets.
For prompt tuning specifically, we noticed that the validation set performance could fluctuate wildly over the course of training, hinting at possible optimization issues.

\begin{minipage}[r]{0.48\textwidth}
\vspace{1em}
\centering
\includegraphics[width=0.93\textwidth]{peft.pdf}
\captionof{figure}{Accuracy of PEFT methods with  and   when applied to T0-3B. Methods that with variable parameter budgets are represented with larger and smaller markers for more or less parameters.}
\label{fig:peft}
\end{minipage}
\hfill
\begin{minipage}[r]{0.48\textwidth}
\vspace{0.8em}
\centering
\includegraphics[width=0.93\textwidth]{main_results.pdf}
\captionof{figure}{Accuracy of different few-shot learning methods. \tfew uses \ia for PEFT methods of T0, T0 uses zero-shot learning, and T5+LM and the GPT-3 variants use few-shot ICL. The x-axis corresponds to inference costs; details are provided in \cref{sec:costs}.}
\vspace{1em}
\label{fig:main}
\end{minipage}

\subsection{Pre-training \ia}

In recent work, \citet{gu2021ppt,vu2021spot} showed that \textit{pre-training} the prompt embeddings in prompt tuning can improve performance when fine-tuning on downstream few-shot tasks.
For pre-training, \citet{gu2021ppt} use a suite of self-supervised tasks applied to unlabeled text data, and \citet{vu2021spot} consider using embeddings from a separate task or multitask mixture.
We follow \citet{vu2021spot} and simply pre-train the new parameters introduced by \ia on the same multitask mixture used to train T0.
We pre-train for 100,000 steps with a batch size of 16 before fine-tuning the \ia parameters on each individual downstream dataset.
A full comparison of accuracy with and without pre-training \ia is detailed in \cref{sec:full_pretraining}.
We find that pre-training improves fine-tuned accuracy from 64.6 to 65.8 and therefore add it to our recipe.

\subsection{Combining the ingredients}

In summary, the \tfew recipe is defined as follows:
We use the T0 model as a backbone.
We add \ia for downstream task adaptation and use parameters initialized from pre-training \ia on the same multitask mixture for T0.
As an objective, we use the sum of a standard language modeling loss , an unlikelihood loss  for incorrect choices, and a length-normalized loss .
We train for 1,000 steps with a batch size of 8 sequences using the Adafactor optimizer \cite{shazeer2018adafactor} with a learning rate of  and a linear decay schedule with a 60-step warmup.
We apply prompt templates to downstream datasets during training and inference to convert each example into an instructive text-to-text format.
Importantly, \textit{we apply this recipe to every downstream dataset in exactly the same way} without per-dataset hyperparameter tuning or modifications.
This makes the recipe a realistic option for few-shot learning settings where validation sets are tiny by definition \cite{perez2021true,oliver2018realistic}.

\section{Outperforming ICL with \tfew}
\label{sec:experiments}

Having designed and established the \tfew recipe on T0-3B, we now apply it to T0 (with 11 billion parameters) and compare performance to strong few-shot ICL baselines.
From this point onwards, we use exactly the same recipe and hyperparameters across all tasks.



\subsection{Performance on T0 tasks}

First, we evaluate \tfew on the datasets that were held out from T0's training mixture. We compare against zero-shot learning with T0 \cite{sanh2021multitask} (since we found few-shot ICL to performed worse than zero-shot for T0, see \cref{sec:full_main}); few-shot ICL with T5+LM \cite{lester2021power} (the next-step-prediction language model upon which T0 is based); and few-shot ICL with the 6.7, 13, and 175 billion parameter variants of GPT-3. See \cref{sec:full_main} for more details on these baselines.
The accuracy on the held-out T0 datasets (described in \cref{sec:model_datasets}) is shown in \cref{tab:main} and \cref{fig:main}, with per-dataset results reported in \cref{sec:full_main}.
We find that \tfew outperforms all other methods by a substantial margin.
Notably, \tfew achieves a 6\% higher accuracy than few-shot ICL with GPT-3 175B despite being about  smaller and outperforms the smaller GPT-3 variants by an even larger margin.
\tfew also attains significantly higher accuracy than both zero-shot learning with T0 and few-shot ICL with T5+LM.

\begin{minipage}[l]{0.64\textwidth}
    \centering
    \begin{tabular}{p{2.2cm} p{1.25cm} p{1.25cm} p{1.1cm} p{0.9cm}}
        \toprule
       \phantom{placeholder} Method & Inference FLOPs & Training FLOPs & Disk space & \phantom{plac} Acc. \\
       \midrule
       \tfew & 1.1e12 & 2.7e16 & 4.2 MB & 72.4\% \\
       T0 \cite{sanh2021multitask} & 1.1e12 & 0 & 0 B & 66.9\% \\
       T5+LM \cite{lester2021power} & 4.5e13 & 0 & 16 kB & 49.6\% \\
       GPT-3 6.7B \cite{brown2020language} & 5.4e13 & 0 & 16 kB & 57.2\% \\
       GPT-3 13B \cite{brown2020language} & 1.0e14 & 0 & 16 kB & 60.3\% \\
       GPT-3 175B \cite{brown2020language} & 1.4e15 & 0 & 16 kB & 66.6\% \\
    \bottomrule
    \end{tabular}
    \captionof{table}{Accuracy on held-out T0 tasks and computational costs for different few-shot learning methods and models. \tfew attains the highest accuracy with 1,000 lower computational cost than ICL with GPT-3 175B. Fine-tuning with \tfew costs about as much as ICL on 20 examples with GPT-3 175B.}
    \label{tab:main}
\end{minipage}
\hfill
\begin{minipage}[r]{0.33\textwidth}
    \centering
    \begin{tabular}{l c}
        \toprule
       Method & Acc. \\
       \midrule
       \tfew  & 75.8\% \\
       Human baseline \citep{alex2021raft} & 73.5\% \\
       PET \citep{schick2021true} & 69.6\% \\
       SetFit \citep{wasserblat2021sentence} & 66.9\% \\
       GPT-3 \citep{brown2020language} & 62.7\% \\
    \bottomrule
    \end{tabular}
    \captionof{table}{Top-5 best methods on RAFT as of writing. \tfew is the first method to outperform the human baseline and achieves over 6\% higher accuracy than the next-best method.}
    \label{tab:raft}
\vspace{1em}
\end{minipage}

\subsection{Comparing computational costs}
\label{sec:costs}

Having established that \tfew significantly outperforms ICL-based models, we now compare the relative costs of each few-shot learning approach.
For simplicity, we use the FLOPs-per-token estimates for Transformer-based language models introduced by \citet{kaplan2020scaling}.
Specifically, we estimate that a decoder-only Transformer (e.g.\ the GPT series) with  parameters uses  FLOPs per token for inference and  FLOPs per token for training.
Encoder-decoder models like T0 and T5 (where the encoder and decoder have the same number of layers and layer sizes) only process each token with either the encoder or decoder (each having roughly half the parameters of the full model), so the FLOPs per token estimates are halved to  and  FLOPs per token for inference and training.
We note that FLOPs are not a direct measurement of real-world computational cost because latency, power usage, and other costs can vary significantly depending on hardware and other factors \cite{dehgani2021efficiency}.
However, we focus on FLOPs because it is a hardware-independent metric that closely with real-world costs the hardware setup used for running the different methods we consider would likely vary significantly across methods.
We summarize the costs in \cref{tab:main} and discuss them below.
For all estimates, we use the median number of shots (41) across the datasets we consider.
Rank evaluation and our unlikelihood loss both require processing every possible output choice to attain a prediction for an unlabeled example.
The median combined tokenized sequence length for the input and all possible targets is 103 for the datasets we consider.
For in-context examples processed for few-shot ICL, only the correct target is required, producing a median sequence length of 98.
Assuming that key and value vectors are cached, processing a single example with ICL therefore involves processing  tokens.
A summary of our cost estimates is provided in \cref{tab:main}.

\paragraph{Inference cost.}

Beyond improved accuracy, the primary advantage of avoiding few-shot ICL is dramatically lower inference costs.
Processing a single input and all target choices with \tfew requires  FLOPs, whereas few-shot ICL with GPT-3 175B requires  FLOPs -- more than 3 orders of magnitude more.
Inference costs with ICL using the smaller GPT-3 variants are also dramatically higher than the inference cost of \tfew.
As discussed in \cref{sec:icl}, caching the key and value vectors when the same set of in-context examples is to be reused can reduce the computational cost of ICL.
However, this would only result in an approximately  reduction, which is not nearly enough to make any of the GPT-3 ICL costs as low as \tfew.

\paragraph{Training cost.}

Since \tfew is the only method that involves updating parameters, it is the only method that incurs a training cost.
Training an eleven billion parameter encoder-decoder model for 1,000 steps with a batch size of 8 length-103 sequences requires approximately  FLOPs.
While not insignificant, this is only about 20 times larger than the FLOPs required to process a \textit{single} example with few-shot ICL using GPT-3 175B.
In other words, training \tfew costs as much as using GPT-3 175B to process 20 examples with few-shot ICL.
We also found that fine-tuning T0 with \tfew on a single dataset only takes about a half an hour on a single NVIDIA A100 GPU.
As of writing, this would cost about \41 \times 98 \times 32\text{ bits} = 16\text{ kB}\times78.0_{2.0}39.2_{0.2}91.5_{1.0}54.5_{0.9}66.4_{1.0}53.8_{1.7}81.0_{3.0}46.1_{4.8}93.6_{2.5}56.5_{2.2}61.5_{8.7}56.4_{4.1}86.0_{4.0}47.1_{22.4}94.0_{0.6}56.9_{3.8}65.4_{3.9}53.9_{2.0}81.0_{11.0}46.4_{8.8}93.8_{2.7}56.5_{1.5}65.4_{7.7}57.7_{3.9}75.8_{5.4}82.1_{5.4}47.8_{1.5}40.6_{0.8}37.8_{1.8}77.6_{1.4}89.3_{1.8}47.9_{1.9}40.9_{1.9}38.8_{5.0}75.8_{4.3}89.3_{7.1}48.2_{0.6}40.9_{0.9}38.3_{1.6}79.8_{3.6}87.5_{5.4}46.6_{2.5}41.3_{0.9}40.2_{5.3}0.06L_{\mathrm{LN}}L_{\mathrm{UL}}3e^{-4}3e^{-4}3e^{-4}323e^{-3}3e^{-3}(n=4)3e^{-3}(n=4)3e^{-1}3e^{-3}0.2\%0.02\%3e^{-4}3e^{-2}40.013e^{-3}81.0_{11.0}46.4_{8.8}93.8_{2.7}56.5_{1.5}75.0_{2.0}29.5_{3.6}88.6_{0.7}49.6_{1.3}76.0_{2.0}29.6_{3.4}88.7_{0.9}49.4_{1.4}84.0_{3.0}41.9_{3.8}91.7_{3.7}54.7_{3.6}84.0_{5.0}46.4_{2.5}93.5_{2.2}55.5_{2.9}86.0_{3.0}46.3_{3.0}93.5_{1.2}55.1_{1.1}67.0_{5.0}29.9_{0.6}84.2_{0.8}51.9_{1.6}60.0_{19.0}26.8_{0.6}74.0_{3.4}51.1_{0.8}71.0_{8.0}42.1_{4.0}90.2_{3.1}52.0_{1.3}82.0_{5.0}44.1_{4.2}94.2_{1.8}54.5_{2.1}84.0_{6.0}38.2_{3.6}93.6_{0.7}53.9_{2.8}77.0_{4.0}36.7_{4.5}89.3_{2.3}52.7_{2.1}76.0_{4.0}38.3_{6.4}89.7_{2.7}50.9_{1.0}88.0_{5.0}47.1_{3.2}93.6_{2.1}56.8_{3.3}87.0_{3.0}49.4_{4.6}94.7_{2.7}59.8_{0.6}65.4_{7.7}57.7_{3.9}79.8_{3.6}87.5_{5.4}61.5_{11.5}51.7_{2.2}72.2_{1.1}57.1_{1.8}63.5_{12.5}52.2_{1.6}71.8_{0.4}57.1_{1.8}65.4_{1.0}55.5_{2.7}76.2_{3.6}87.5_{3.6}(n = 4)64.4_{6.7}55.2_{3.8}75.8_{6.1}82.1_{3.6}(n = 4)65.4_{3.9}54.1_{2.2}76.9_{0.4}82.1_{3.6}54.8_{10.6}51.6_{2.0}52.7_{5.4}66.1_{1.8}60.6_{4.8}50.0_{1.1}48.0_{2.9}53.6_{17.9}56.7_{3.3}54.2_{3.3}68.6_{3.3}84.0_{1.8}63.5_{4.8}52.5_{3.3}76.9_{4.7}83.9_{3.6}61.5_{1.0}53.5_{1.3}75.5_{5.4}76.8_{3.6}61.5_{8.7}55.0_{2.7}69.0_{7.6}80.4_{0.0}55.8_{6.7}55.3_{0.5}66.1_{5.4}83.9_{1.8}60.6_{5.8}55.2_{5.0}78.3_{7.6}85.7_{1.8}68.3_{6.7}56.0_{4.6}78.0_{2.5}87.5_{1.8}46.6_{2.5}41.3_{0.9}40.2_{5.3}36.5_{0.8}35.3_{2.2}36.6_{0.8}36.5_{0.7}35.1_{2.6}36.3_{1.0}45.1_{2.6}40.4_{1.2}35.3_{1.3}40.8_{3.3}37.4_{0.2}35.8_{3.3}41.7_{0.4}38.3_{1.8}36.9_{1.5}34.2_{1.9}33.5_{1.1}33.5_{1.3}33.4_{1.2}33.8_{0.5}33.3_{0.8}43.3_{4.1}37.5_{1.2}36.5_{1.5}43.7_{0.3}39.7_{1.4}37.2_{1.1}39.9_{0.9}38.1_{2.0}36.2_{1.8}40.4_{3.3}35.4_{4.1}35.5_{1.6}41.3_{1.3}38.5_{1.8}35.8_{2.0}45.1_{2.5}41.0_{1.4}39.5_{4.8}48.6_{2.0}40.8_{1.5}40.8_{2.3}L_{\mathrm{UL}}L_{\mathrm{LN}}86.00_{4.00}47.12_{22.44}93.96_{0.59}56.91_{3.79}80.00_{6.00}31.33_{0.16}92.89_{0.27}51.38_{0.71}82.00_{2.00}31.25_{0.64}92.84_{0.48}51.14_{0.39}84.00_{5.00}44.05_{3.22}92.89_{2.35}52.64_{0.55}(n = 4)85.00_{3.00}47.20_{5.34}94.33_{1.23}53.91_{1.34}(n = 4)85.00_{2.00}47.86_{1.65}94.55_{0.69}54.38_{2.92}72.00_{5.00}30.43_{1.07}90.38_{1.23}50.51_{0.95}65.00_{1.00}27.93_{4.69}87.01_{3.05}51.93_{0.39}79.00_{6.00}34.40_{9.71}90.33_{3.15}51.10_{1.72}85.00_{4.00}26.65_{0.14}93.80_{0.90}54.38_{0.16}82.00_{2.00}26.65_{0.14}93.64_{1.12}53.91_{1.97}86.00_{1.00}48.68_{2.62}94.44_{1.66}56.12_{1.03}90.00_{2.00}50.03_{3.02}95.40_{1.12}58.25_{0.55}65.38_{3.85}53.92_{2.04}75.81_{4.33}89.29_{7.14}63.46_{2.88}54.23_{3.13}75.45_{1.81}67.86_{0.00}60.58_{2.88}55.33_{1.88}76.17_{1.44}67.86_{1.79}63.46_{3.85}55.49_{3.61}77.26_{3.97}80.36_{3.57}(n = 4)64.42_{3.85}53.29_{5.49}75.45_{2.89}82.14_{5.36}(n = 4)65.38_{3.85}54.86_{3.45}77.26_{5.78}76.79_{7.14}53.85_{4.81}52.04_{1.72}55.23_{2.53}66.07_{3.57}50.96_{6.73}51.88_{1.57}48.38_{3.69}62.50_{12.50}60.58_{3.85}68.95_{0.72}80.36_{12.50}75.00_{8.93}66.35_{2.88}54.23_{1.10}75.81_{3.61}83.93_{7.14}60.58_{1.92}52.82_{1.10}75.09_{3.61}76.79_{3.57}61.54_{1.92}55.02_{4.70}74.73_{4.69}85.71_{1.79}66.35_{3.85}53.76_{0.63}76.90_{2.89}83.93_{0.00}48.20_{0.60}40.90_{0.90}38.25_{1.58}63.2536.10_{1.40}35.60_{1.40}35.42_{2.00}56.737.30_{0.50}37.10_{0.70}36.25_{1.08}57.0742.40_{3.20}38.80_{0.60}36.50_{3.83}60.71(n = 4)42.90_{3.90}38.00_{0.80}37.33_{2.33}61.27(n = 4)41.90_{0.50}38.50_{2.40}36.00_{0.58}61.1334.20_{1.10}34.20_{1.30}34.42_{0.83}52.1234.10_{1.10}34.20_{0.20}34.08_{1.25}49.8237.50_{3.60}34.17_{4.50}34.40_{9.71}58.7143.40_{0.60}40.00_{0.90}36.75_{2.83}60.0340.10_{0.90}38.00_{2.00}35.50_{0.75}57.7346.20_{1.70}41.40_{0.90}38.42_{2.67}62.5749.20_{2.80}40.30_{2.30}40.42_{3.17}64.05L_{\mathrm{LN}}81.00_{3.00}46.12_{4.82}93.64_{2.51}56.51_{2.21}81.00_{4.00}35.51_{2.34}92.78_{0.86}50.91_{0.08}82.00_{1.00}34.60_{2.31}92.68_{0.75}51.78_{1.26}83.00_{1.00}42.53_{5.35}90.49_{3.15}53.67_{3.63}(n = 4)88.00_{3.00}42.95_{4.06}92.89_{1.87}54.62_{1.50}(n = 4)85.00_{2.00}48.26_{2.95}93.85_{1.60}54.85_{2.84}74.00_{5.00}29.24_{2.48}88.88_{1.12}51.38_{0.47}68.00_{7.00}28.51_{2.43}86.91_{4.33}50.59_{0.16}69.00_{2.00}29.04_{10.83}86.44_{2.35}50.63_{1.41}85.00_{5.00}27.78_{0.51}94.01_{1.55}53.67_{2.60}84.00_{4.00}27.78_{0.51}93.16_{1.23}53.59_{2.21}87.00_{3.00}46.97_{1.98}93.11_{2.03}57.93_{3.63}86.00_{4.00}48.78_{4.12}94.01_{2.83}58.72_{1.34}61.54_{8.65}56.43_{4.08}77.62_{1.44}89.29_{1.79}64.42_{3.85}53.61_{2.51}76.17_{3.61}60.71_{1.79}60.58_{8.65}53.92_{2.35}75.09_{1.81}57.14_{3.57}65.38_{6.73}54.39_{3.13}79.06_{5.42}85.71_{3.57}(n = 4)65.38_{4.81}54.55_{3.61}75.45_{5.05}82.14_{0.00}(n = 4)64.42_{3.85}55.64_{3.61}77.62_{4.69}80.36_{7.14}54.81_{6.73}52.82_{3.29}52.71_{1.08}69.64_{5.36}50.00_{3.85}50.16_{0.94}52.71_{4.33}58.93_{12.50}55.77_{1.92}71.12_{6.14}82.14_{5.36}83.93_{8.93}62.50_{3.85}53.61_{1.41}76.17_{2.17}83.93_{8.93}59.62_{1.92}53.61_{0.47}74.37_{5.05}75.00_{1.79}59.62_{12.50}55.49_{4.86}79.06_{1.81}87.50_{1.79}65.38_{4.81}56.74_{4.39}77.26_{2.53}87.50_{1.79}47.90_{1.90}40.90_{1.90}38.83_{5.00}62.7136.40_{1.10}34.00_{0.70}35.25_{2.42}56.4337.00_{1.90}36.00_{2.10}35.58_{2.17}56.0343.90_{1.10}38.60_{1.10}36.17_{2.17}61.17(n = 4)41.80_{1.30}37.60_{3.00}37.17_{1.92}61.14(n = 4)41.70_{0.60}38.20_{2.50}35.58_{0.33}61.4135.00_{2.10}33.80_{0.60}33.67_{2.75}52.3635.70_{0.90}33.80_{1.50}33.00_{2.17}49.8534.60_{1.60}36.83_{4.67}38.52_{3.00}5844.10_{1.00}38.70_{1.50}38.25_{0.83}59.7940.50_{2.60}37.00_{1.20}35.58_{0.75}57.6645.90_{2.20}41.10_{1.70}38.83_{1.08}62.9649.80_{2.10}40.30_{0.30}40.17_{3.33}64.06L_{\mathrm{UL}}78.00_{2.00}39.16_{0.24}91.45_{0.96}54.46_{0.87}77.00_{7.00}33.76_{0.38}90.49_{0.27}51.54_{0.16}77.00_{7.00}33.58_{0.65}90.43_{0.21}51.38_{0.32}76.00_{5.00}36.41_{2.27}90.59_{1.71}52.01_{0.47}(n = 4)81.00_{5.00}37.53_{0.67}91.50_{0.21}52.57_{0.87}(n = 4)78.00_{2.00}37.00_{1.02}91.98_{0.91}53.12_{0.87}73.00_{4.00}30.09_{1.67}88.88_{1.12}52.25_{0.32}66.00_{4.00}26.31_{4.46}87.44_{0.21}51.14_{0.55}70.00_{3.00}27.98_{6.62}86.75_{2.24}51.07_{1.10}77.00_{3.00}35.45_{0.87}90.54_{1.07}52.96_{0.87}74.00_{2.00}31.15_{1.30}89.52_{1.28}52.57_{0.47}80.00_{5.00}39.14_{1.26}92.04_{1.07}53.75_{0.47}82.00_{1.00}40.59_{0.56}92.57_{0.48}56.91_{2.53}66.35_{0.96}53.76_{1.72}75.81_{5.42}82.14_{5.36}61.54_{3.85}53.13_{1.72}76.53_{1.08}64.29_{8.93}61.54_{3.85}53.29_{1.72}76.17_{2.17}62.50_{8.93}65.38_{7.69}54.70_{1.72}77.26_{2.89}83.93_{1.79}(n = 4)61.54_{2.88}55.33_{3.61}76.17_{2.17}83.93_{0.00}(n = 4)61.54_{1.92}54.70_{4.23}73.65_{1.81}78.57_{5.36}53.85_{7.69}52.51_{1.88}57.40_{4.33}69.64_{10.71}56.73_{6.73}52.35_{0.63}54.15_{3.97}53.57_{19.64}52.88_{7.69}52.51_{0.31}72.56_{11.91}75.00_{17.86}62.50_{4.81}54.23_{2.04}77.26 _{5.42}82.14_{1.79}58.65_{2.88}54.39_{1.10}76.17_{5.05}75.00_{3.57}64.42_{12.50}54.86_{3.45}77.26_{4.33}87.50_{3.57}64.42_{3.85}54.23_{1.57}77.98_{1.81}82.14_{5.36}47.80_{1.50}40.60_{0.80}37.75_{1.83}60.6637.30_{1.80}36.10_{2.60}35.17_{3.67}56.0837.50_{1.50}36.00_{2.80}35.08_{3.42}55.8640.70_{3.70}39.20_{1.10}35.83_{1.92}59.27(n = 4)41.80_{2.70}38.00_{0.80}36.00_{2.75}59.58(n = 4)41.10_{1.50}38.90_{2.50}36.92_{1.42}58.6833.60_{0.70}33.80_{1.10}34.83_{1.00}52.7135.60_{1.70}34.50_{0.70}34.75_{1.42}50.2337.60_{2.30}34.10_{3.50}35.08_{0.67}54.1443.50_{0.30}40.30_{0.40}36.42_{2.25}59.340.40_{2.20}37.50_{1.00}36.42_{1.08}56.8944.20_{2.60}40.40_{1.20}37.58_{0.58}61.0148.50_{0.90}40.20_{1.80}39.42_{1.67}61.72L_{\mathrm{UL}}L_{\mathrm{LN}}87.0_{3.0}49.4_{4.6}94.7_{2.7}59.8_{0.6}68.3_{6.7}56.0_{4.6}89.0_{5.0}51.2_{4.6}95.1_{2.5}62.6_{1.1}70.2_{8.7}57.2_{2.5}78.0_{2.5}87.5_{1.8}48.6_{2.0}40.8_{1.5}40.83_{2.3}80.9_{1.4}87.5_{1.8}49.3_{1.1}41.1_{0.5}39.8_{4.8}93.0_{2.0}67.1_{6.0}97.9_{0.3}74.3_{1.5}75.0_{5.5}62.2_{7.8}90.833.794.760.564.457.268.060.962.856.963.550.092.079.387.777.775.055.386.071.383.070.075.051.183.067.381.267.467.353.185.6_{2.9}87.5_{3.6}59.3_{3.6}49.8_{2.6}44.8_{8.0}81.278.644.739.442.453.432.133.332.734.172.982.136.834.040.260.666.133.332.634.549.560.733.133.133.993.0_{2.0}67.1_{6.0}97.9_{0.3}74.3_{1.5}75.0_{5.5}62.15_{7.8}92.0_{2.0}64.5_{6.6}97.8_{0.8}72.7_{1.0}73.1_{6.3}60.8_{6.4}L_{\mathrm{UL}}L_{\mathrm{LN}}91.0_{2.0}52.1_{2.7}97.4_{0.5}71.9_{1.1}71.2_{1.0}62.2_{2.4}L_{\mathrm{UL}}L_{\mathrm{LN}}94.0_{2.3}52.7_{4.9}98.0_{0.3}74.0_{1.1}72.6_{4.8}62.6_{5.0}85.6_{2.9}87.5_{3.6}59.3_{3.6}49.8_{2.6}44.8_{8.0}84.5_{2.8}83.9_{5.4}57.9_{3.2}48.6_{3.0}43.1_{5.7}L_{\mathrm{UL}}L_{\mathrm{LN}}82.0_{0.7}82.1_{3.6}54.8_{0.4}46.1_{0.6}40.8_{5.2}L_{\mathrm{UL}}L_{\mathrm{LN}}84.5_{2.9}80.4_{3.6}57.1_{3.1}47.1_{2.4}43.8_{5.9}L_{\mathrm{UL}}L_{\mathrm{LN}}80.469.583.367.695.091.550.873.675.058.687.983.060.785.764.691.790.846.860.962.772.289.782.259.385.764.690.881.649.363.857.648.382.472.653.887.252.190.768.249.362.862.053.283.768.629.967.943.193.776.951.665.657.452.682.1$ \\
        \bottomrule
    \end{tabular}
    \vspace{1em}
    \captionof{table}{Detailed per-dataset results for \tfew and the other top-5 methods on RAFT.}
    \label{tab:full_raft}
    \end{table}

\end{document}