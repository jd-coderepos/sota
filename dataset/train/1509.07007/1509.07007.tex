\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{caption}

\usepackage{mdframed}




\def\showkeys{1}
\def\showdraftbox{0}
\def\showcolorlinks{1}




\usepackage{etex}



\usepackage[l2tabu, orthodox]{nag}




\usepackage[utf8]{inputenc}

\usepackage{xspace,enumerate}

\usepackage[dvipsnames]{xcolor}

\usepackage[T1]{fontenc}
\usepackage[full]{textcomp}



\usepackage[american]{babel}




\usepackage{mathtools}




\newcommand\hmmax{0} \usepackage{bm}


\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{itheorem}{Theorem}

\newtheorem{subclaim}{Claim}[theorem]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{proposition*}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{conjecture*}{Conjecture}
\newtheorem{fact}[theorem]{Fact}
\newtheorem*{fact*}{Fact}
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem*{hypothesis*}{Hypothesis}
\newtheorem{conjecture}[theorem]{Conjecture}


\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}[theorem]{Question}
\newtheorem{openquestion}[theorem]{Open Question}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{protocol}[theorem]{Protocol}
\newtheorem{assumption}[theorem]{Assumption}

\theoremstyle{remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{claim*}{Claim}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{observation}[theorem]{Observation}
\newtheorem*{observation*}{Observation}





















\ifnum\showcolorlinks=1
\usepackage[
pagebackref,
letterpaper=true,
colorlinks=true,
urlcolor=blue,
linkcolor=blue,
citecolor=OliveGreen,
]{hyperref}
\fi

\ifnum\showcolorlinks=0
\usepackage[
pagebackref,
letterpaper=true,
colorlinks=false,
pdfborder={0 0 0}
]{hyperref}
\fi


\usepackage{prettyref}



\newcommand{\savehyperref}[2]{\texorpdfstring{\hyperref[#1]{#2}}{#2}}

\newrefformat{eq}{\savehyperref{#1}{\textup{(\ref*{#1})}}}
\newrefformat{lem}{\savehyperref{#1}{Lemma~\ref*{#1}}}
\newrefformat{def}{\savehyperref{#1}{Definition~\ref*{#1}}}
\newrefformat{thm}{\savehyperref{#1}{Theorem~\ref*{#1}}}
\newrefformat{cor}{\savehyperref{#1}{Corollary~\ref*{#1}}}
\newrefformat{cha}{\savehyperref{#1}{Chapter~\ref*{#1}}}
\newrefformat{sec}{\savehyperref{#1}{Section~\ref*{#1}}}
\newrefformat{app}{\savehyperref{#1}{Appendix~\ref*{#1}}}
\newrefformat{tab}{\savehyperref{#1}{Table~\ref*{#1}}}
\newrefformat{fig}{\savehyperref{#1}{Figure~\ref*{#1}}}
\newrefformat{hyp}{\savehyperref{#1}{Hypothesis~\ref*{#1}}}
\newrefformat{alg}{\savehyperref{#1}{Algorithm~\ref*{#1}}}
\newrefformat{rem}{\savehyperref{#1}{Remark~\ref*{#1}}}
\newrefformat{item}{\savehyperref{#1}{Item~\ref*{#1}}}
\newrefformat{step}{\savehyperref{#1}{step~\ref*{#1}}}
\newrefformat{conj}{\savehyperref{#1}{Conjecture~\ref*{#1}}}
\newrefformat{fact}{\savehyperref{#1}{Fact~\ref*{#1}}}
\newrefformat{prop}{\savehyperref{#1}{Proposition~\ref*{#1}}}
\newrefformat{prob}{\savehyperref{#1}{Problem~\ref*{#1}}}
\newrefformat{claim}{\savehyperref{#1}{Claim~\ref*{#1}}}
\newrefformat{relax}{\savehyperref{#1}{Relaxation~\ref*{#1}}}
\newrefformat{red}{\savehyperref{#1}{Reduction~\ref*{#1}}}
\newrefformat{part}{\savehyperref{#1}{Part~\ref*{#1}}}




































































































































































































\newcommand{\problemmacro}[1]{\texorpdfstring{\textsc{#1}}{#1}\xspace}
















\ifnum\showdraftbox=1
\newcommand{\draftbox}{\begin{center}
  \fbox{\begin{minipage}{2in}\begin{center}\Large\textsc{Working Draft}\\Please do not distribute\end{center}\end{minipage}}\end{center}
\vspace{0.2cm}}
\else
\newcommand{\draftbox}{}
\fi
































































 








\setcounter{page}{0}

\title{Finding Perfect Matchings in Bipartite Hypergraphs}

\author{Chidambaram Annamalai\thanks{Department of Computer Science,
    ETH Zurich. Email:
\href{mailto:cannamalai@inf.ethz.ch}{cannamalai@inf.ethz.ch}. \newline Work performed while the author was at the School of Basic Sciences, EPFL.}}

\date{\today}

\begin{document}

\maketitle
\draftbox
\thispagestyle{empty}

\begin{abstract}
  Haxell's condition~\cite{haxell1995condition} is a natural \emph{hypergraph} analog of Hall's condition, which is a well-known necessary and sufficient condition for a bipartite graph to admit a perfect matching. That is, when Haxell's condition holds it forces the existence of a perfect matching in the bipartite hypergraph. Unlike in graphs, however, there is no known polynomial time algorithm to find the hypergraph perfect matching that is guaranteed to exist when Haxell's condition is satisfied.

We prove the existence of an efficient algorithm to find perfect matchings in bipartite hypergraphs whenever a stronger version of Haxell's condition holds. Our algorithm can be seen as a generalization of the classical \emph{Hungarian algorithm} for finding perfect matchings in bipartite graphs. The techniques we use to achieve this result could be of use more generally in other combinatorial problems on hypergraphs where disjointness structure is crucial, e.g. \problemmacro{Set Packing}.

 \end{abstract}

\medskip
\noindent
{\small \textbf{Keywords:}
bipartite hypergraphs, matchings, local search algorithms.
}

\newpage
\section{Introduction}
Bipartite matchings are a ubiquitous quantity across science and
engineering. The task of finding a maximum matching (or, specifically,
a perfect matching) in a bipartite graph captures a fundamental notion
of assignment that has turned out to have wide applicability. One
reason their influence has been felt so deeply is essentially
\emph{computational}. The basic fact that maximum matchings in
bipartite graphs can be found efficiently is arguably the most
important factor in determining their widespread use. Confirming the
importance of this problem, decades of research in theoretical
computer science has contributed to increasingly faster and more
sophisticated algorithms for finding maximum matchings in bipartite
graphs. These include connections to other fundamental problems like
matrix
multiplication~\cite{lovasz1979determinants,mucha2004maximum}. Most
recently, the exciting work of M\c{a}dry~\cite{madry2013navigating} breaks
the decades-old 
Hopcroft-Karp-Karzanov~\cite{hopcroft1973n,karzanov1973nakhozhdenii}
barrier for finding maximum matchings in bipartite graphs.

In this paper we look at the analogous problem in the hypergraph
setting. We address the following question: Do there exist
efficient---in the sense of \emph{polynomial} running time---algorithms
to find perfect matchings in bipartite \emph{hypergraphs}?

In an -uniform bipartite hypergraph the vertex set is partitioned
into two sets  and  such that each edge contains exactly one
vertex from  and  vertices from . A perfect matching is a
collection of disjoint edges such that each vertex in  is covered
by exactly one edge in the collection.

In order for the question to make any sense at all we need to impose
some additional restrictions on the input as any such algorithm that
works unconditionally even for the case  is tantamount to PNP,
as the trivial reduction from \problemmacro{-Dimensional
  Matching}\footnote{For a definition of this NP-complete problem see, for example,~\cite{cygan2013improved}.} shows. In this sense it is not surprising that the question of
\emph{finding} perfect matchings in bipartite hypergraphs was not
considered before. However, as we will see, under certain conditions
the problem becomes interesting algorithmically. The starting point
for such investigations is to ask ourselves if there is a condition
similar to Hall's~\cite{hall1935representatives} condition that
guarantees the existence of perfect matchings in bipartite
hypergraphs. This question was solved in a very satisfying way by
Haxell~\cite{haxell1995condition} in the mid-90s leading to a striking
generalization of Hall's theorem. The condition is that for every
subset  of , the size of the hitting set of hyperedges incident
to  must be proportional to the size of . This condition
is sufficient to force the existence of a perfect matching in the
hypergraph. More formally, given a bipartite hypergraph ,
for a set  let
 be the set of hyperedges of
 incident to . For a given collection of edges ,
define  to be the smallest cardinality subset of 
that hits\footnote{A subset  \emph{hits} all the edges
  in  if for each , .} all the
edges in .

\begin{theorem}[Haxell~\cite{haxell1995condition}]\label{thm:haxell}
  Let  be an -uniform bipartite hypergraph. If
   then  admits a perfect matching.
\end{theorem}

There are several interesting aspects to
Theorem~\ref{thm:haxell}. First, it reduces to Hall's condition when
. Second, the statement is ``tight'' in the sense that it is
\emph{not} true when the strict inequality is replaced by a non-strict
one, i.e., for every  there is an -uniform bipartite hypergraph
that satisfies Haxell's condition with non-strict inequalities and yet
contains no perfect matching. Finally, in constrast to the graph case,
the proof is not constructive and does not lead to an efficient
algorithm that also finds the perfect matching.

Besides being interesting objects in their own right, perfect
matchings in bipartite hypergraphs have become a crucial concept in
recent
work~\cite{bansal2006santa,feige2008allocations,haeupler2011new,asadpour2012santa,svensson2012santa,polacek2012quasi,DBLP:conf/soda/AnnamalaiKS15}
on a particular allocation problem, called \problemmacro{Restricted
  Max-Min Fair Allocation}, where the goal is to partition
\emph{indivisible} resources among players in a balanced manner. The
latest work that exploits this
connection~\cite{DBLP:conf/soda/AnnamalaiKS15} has further shown that
local search algorithms based on alternating trees for this problem
can be made to run in polynomial time. These developments brought to
light the question posed towards the beginning of this section. Is it
possible to efficiently find perfect matchings in bipartite
hypergraphs by assuming a stronger version of Haxell's condition?

The question was also raised implicitly by Haeupler, Saha and
Srinivasan~\cite{haeupler2011new} who write that ``Haxell's theorems
are again highly non-constructive \ldots.'' As a direction for future
research it was therefore asked in~\cite{DBLP:conf/soda/AnnamalaiKS15} if
ensuring the much stronger condition

was sufficient to also find the perfect matching efficiently. The
large constant before  reflected the belief that some loss is to be
expected, at least by using their techniques. The allocation algorithm
underlying their work turned out to yield a weaker guarantee under
strengthenings similar to \eqref{eqn:stronger}. In particular
their techniques yield the following theorem.

\begin{theorem}[\cite{DBLP:conf/soda/AnnamalaiKS15}]\label{thm:weak}
  Let  be some absolute constant. Choose any
  ,  and consider -uniform bipartite
  hypergraphs  that satisfy
   For such a family of
  hypergraphs there is a polynomial time algorithm that assigns one
  hyperedge  for every vertex  such that it is
  possible to choose disjoint subsets
   of cardinality at least
  .
\end{theorem}

Whereas in the allocation setting sufficiently large subsets
 also correspond to a good allocation, in a
hypergraph setting Theorem~\ref{thm:weak} does not guarantee a
collection of valid and disjoint hyperedges unless .
However, for a choice of  in latter range, the assumption
 is much stronger than the
one in Theorem~\ref{thm:haxell}.

\paragraph{Our results} Our main result is that a suitable
constructivization of Theorem~\ref{thm:haxell} is indeed possible. We
prove the following.

\begin{theorem}\label{thm:main}
  For every fixed choice of  and , there exists
  an algorithm  that finds, in time
  polynomial in the size of the input, a perfect matching in
  -uniform biparite hypergraphs  satisfying
  
\end{theorem}



Notice that such an algorithm with a polynomial running time
dependence on  would be able to efficiently find perfect
matchings in bipartite graphs only assuming Haxell's
condition. Currently we see no way of achieving such a result. In
particular, our techniques make essential use of the 
strengthening of Haxell's condition as assumed in
Theorem~\ref{thm:main}. We emphasize that the running time of
 depends exponentially on  and , which
explains the particular order of the quantifiers in the statement. See
also Theorem~\ref{thm:maincorollary} for a slightly stronger corollary of
our main result.

An outline of the ideas behind Theorem~\ref{thm:main} requires setting
up some context involving previous work, which we do presently.

\paragraph{Context} It helps to start with the graph case. Here the
basic augmenting algorithm (often called the \emph{Hungarian
  algorithm} after K\H onig and
Egerv\'ary~\cite{west2001introduction}) takes a partial matching and
constructs an alternating tree of unmatched and matched edges with an
unmatched vertex at the root. For convenience we can imagine this tree
partitioned into ``layers'', where the th layer contains all vertices
at distance  and  from the root (along with their associated
edges in the tree). Matched edges appearing in the tree can also be
called ``blocking'' since they prevent us from augmenting the partial
matching immediately. If a leaf of the alternating tree happens to be
an unmatched vertex (i.e., the leaf edge is not blocked by some edge
in the partial matching) then the corresponding root to leaf path is
an augmenting path for the considered matching and thus the augmenting
algorithm terminates. The fact that such a leaf always exists is
guaranteed by Hall's condition~\cite{hall1935representatives}. The
proof of Haxell's theorem (Theorem~\ref{thm:haxell}) involves a
similar alternating tree, the key difference being that a single
hyperedge in the alternating tree may now be blocked by \emph{several}
hyperedges (up to ) from the partial matching. Therefore, even if
a leaf hyperedge in the tree does not intersect any hyperedges from
the partial matching, we may not be able to immediately augment the
partial matching like in the graph case. What we can do is only swap
the corresponding blocking edge (the unique ancestor of the unblocked
leaf edge) with the leaf edge in the partial matching and
continue. When there are no longer leaf edges in the alternating tree,
the existence of a vertex disjoint hyperedge for one of the 
vertices in the tree is implied by Haxell's condition. This
alternating tree algorithm for hypergraph matchings by
Haxell~\cite{haxell1995condition}, which underlies
Theorem~\ref{thm:haxell}, is not known to make fewer than
exponentially many modifications (swapping operations) to the partial
matching before termination (at which point the root is matched).

To make such a local search algorithm
efficient~\cite{DBLP:conf/soda/AnnamalaiKS15} devised a similar but
different algorithm that ensures a (constant factor) multiplicative
increase in the number of blocking edges from layer to layer. This
guarantees that the height of the alternating tree is always
logarithmic, leading to ``short'' augmenting paths. They also avoid
making changes to the partial matching unless sufficiently many
changes can be made at once. In other words, the partial matching is
updated lazily. Coupled with other ideas, this leads to a polynomial
time combinatorial allocation algorithm that achieves their main
result. For the hypergraph setting, however, this only yields
Theorem~\ref{thm:weak}.

\paragraph{Our Techniques} One obstacle with the algorithm
of~\cite{DBLP:conf/soda/AnnamalaiKS15}, is that a single blocking edge
can block up to  hyperedges in the same layer. This effect can
accumulate across consecutive layers preventing the desired growth in
the number of blocking edges across the layers of the alternating
tree, which we require in order to guarantee a logarithmic bound on
the height of the alternating tree. This makes it important to view
the structure of the blocking edges when the layers of the alternating
tree are constructed. On the other hand the problem with the regular
alternating tree algorithm for hypergraph
matchings~\cite{haxell1995condition} is that a single  vertex in a
layer can be part of an unbounded number of hyperedges in the next
layer in the alternating tree. This skews any subsequent progress made
by the alternating tree algorithm vastly in favor of a few 
vertices in the previous layers. To avoid this we impose a degree
bound on the  vertices in the alternating tree, making the progress
more balanced among  vertices in the same layer. In
Section~\ref{section:analysis} we show, despite imposing this upper
bound, a multiplicative growth in the number of blocking edges from
layer to layer. Next, since the structure of the blocking edges was
considered when constructing a layer, this creates complications when
we modify the partial matching and some layer in the tree. For
example, when some blocking edges are removed by swapping operations
in a layer it may be possible to have additional hyperedges for some
of the  vertices in the same layer. At this point our algorithm
performs a so-called ``superposed-build'' operation (see
Section~\ref{section:augmenting2}) on the layer to check if
sufficiently many new hyperedges can be included. If so, it commits
the changes, otherwise it ignores the newly available hyperedges.

\subsection{Related work}
Most relevant to the result of this paper is the line of work
concerning \emph{alternating tree} algorithms for hypergraph matchings
starting with the work of Haxell~\cite{haxell1995condition}. The
algorithmic question of whether the underlying local search algorithm
can be made efficient was not considered until the work of Asadpour,
Feige and Saberi~\cite{asadpour2012santa}. They uncovered a beautiful
connection to strong integrality gaps for configuration linear
programs for allocation problems. This direction was subsequently also
pursued by Svensson~\cite{svensson2012santa} leading to a
breakthrough in the context of scheduling. Both
results~\cite{asadpour2012santa,svensson2012santa} were
non-constructive and only proved integrality gap upper
bounds. Following these results it became an important question if
such approaches based on alternating trees can be turned into
efficient algorithms with similar guarantees. Pol\'a\v{c}ek and
Svensson~\cite{polacek2012quasi} obtained significant savings leading
to a quasipolynomial time alternating tree algorithm for
\problemmacro{Restricted Max-Min Fair Allocation}, but it is still not
clear if their approach can be made truly polynomial. Building on
these ideas, a polynomial time alternating tree algorithm for the same
problem was obtained by Annamalai, Kalaitzis and
Svensson~\cite{DBLP:conf/soda/AnnamalaiKS15}.

The success of local search for combinatorial problems on hypergraphs
where disjointness structure is crucial has been a recurring theme in
the literature on \problemmacro{-Set
  Packing}~\cite{karp1972reducibility}. Hurkens and
Schrijver~\cite{hurkens1989size} showed a 
approximation algorithm using an intuitive local search
algorithm. Halld\'orsson~\cite{halldorsson1995approximating} then
obtained a quasipolynomial -approximation. Using a different
approach this was improved by Cygan, Grandoni, and
Mastrolilli~\cite{cygan2013sell} to a quasipolynomial time
-approximation. A polynomial time
-approximation was obtained by Sviridenko and
Ward~\cite{sviridenko2013large} using color-coding techniques. The
best known result for \problemmacro{-Set Packing} to date is a
-approximation due to
Cygan~\cite{cygan2013improved}, and also by Furer and
Yu~\cite{furer2014approximating}. It is interesting to note that all
of these results are based on local search. We believe our techniques
to be a useful addition to this repertoire.

In an important direction of research Chan and
Lau~\cite{chan2012linear} consider the power of linear and
semidefinite relaxations for the \problemmacro{-Set Packing
  problem}. They show that a particular LP relaxation has integrality
gap at most . A different LP relaxation arrived at by
applying  rounds of Chv\'atal-Gomory cuts to the standard LP
relaxation was also shown to have no worse integrality gap by Singh
and Talwar~\cite{singh2010improving}. It remains interesting to
consider the applicability of ``alternating tree'' style analyses, as
presented in this paper, to better understand the integrality gaps of
such strong LP and SDP relaxations.

For a different notion of bipartiteness in hypergraphs, Conforti
et al.~\cite{conforti1996perfect} study sufficient conditions for the
existence of perfect matchings. We also mention that for the case of
general hypergraphs, sufficient conditions in the spirit of Dirac's
theorem for graphs~\cite{dirac1952some} are known (see Alon
et al.~\cite{alon2012large} and references therein).

\section{Preliminaries}\label{section:preliminaries}
\begin{definition}[Bipartite hypergraph]
  An -uniform bipartite hypergraph  is a hypergraph
  on a vertex set partitioned into two sets  and  such that for
  every edge ,  and .
\end{definition}

Let  be a -uniform bipartite hypergraph. It is important
to note that we assume that the underlying bipartition of the vertex set is given
to the algorithm. We will
use  and  to refer to  and  respectively in . A
subset of edges  is called a \textit{partial matching}
if any pair of edges in the set are disjoint. A partial matching whose
edges contain every vertex of  is a \textit{perfect matching}.

We need some notation for referring to the collection of  vertices
and  vertices in a set of edges . For a subset of
edges  we use  to denote the set
  is defined similarly as
.

We say that a vertex  is \textit{matched} by a partial
matching  if . Recall that a perfect matching is a
partial matching that matches all the vertices of .

For the definitions that follow consider a fixed partial matching
 in . From the context it will always be clear what the
considered partial matching is.

\begin{definition}[Blocking edges]\label{def:blocking}
The set of
edges \textit{blocking} a given edge  is the set

i.e., it contains edges in  that prevent us from adding
 to it.
\end{definition}
Note that  may contain an edge  such that
 and it matches  in , where , in
which case we may want to also add  to the set of blocking edges
of  but we do \emph{not} do so according to Definition~\ref{def:blocking}.

An edge  is called \textit{immediately addable} if it has no
blocking edges. The name reflects the property that  is
also a partial matching for such an edge , unless the  vertex
contained in  is already matched by . We refer to an edge  as an edge \emph{for}
 if .



The definitions are made with the following simple operation in mind.

\paragraph{Swapping operation} Suppose that  is matched by
 through some edge  and that there is
an immediately addable edge  for . Then the set
 is also a partial matching
that matches exactly the same set of  vertices as
.

A final piece of notation is the following. For a collection of
indexed sets  we write  to denote


\begin{definition}[Layer]\label{def:layer}
  A layer  for a bipartite hypergraph  with respect to
  a partial matching  is a tuple  where
  \begin{itemize}
  \item ,
  \item for each pair of distinct edges   , ,
  \item  is precisely the set of blocking edges of , and
  \item every  intersects exactly one edge from .
  \end{itemize}
\end{definition}

\begin{definition}[Alternating tree]\label{def:alttree}
  An alternating tree  for a bipartite hypergraph 
  with respect to a partial matching  is a tuple 
   such that:
  \begin{itemize}
  \item  is defined to be  for some 
   not matched by ,
  \item  are layers,
\item  for all , and
\item .
  \end{itemize}
   is called the \emph{root} of the alternating tree . The
  \emph{degree} of an  vertex  is
  defined to be the number of edges from  that contain .
\end{definition}

\paragraph{Intuition} Our goal will be to obtain an augmenting
algorithm that takes some partial matching  that does not match
some  and turns it into a different partial matching 
that matches all the vertices of . To accomplish
this consider some edge  for . If it is immediately
addable then we are done. Otherwise there are some blocking edges of
, call them , that prevent us from adding  to . To make
progress we will try to perform a swapping operation on some of the
vertices from  thereby reducing the number of blocking edges of
. To do so we need to find edges for  which may themselves
turn out to be blocked and so on. This alternating structure is
captured in our definition of a layer and the tree structure that
follows is the reason behind Definition~\ref{def:alttree}. See
Figure~\ref{figure:alttree} for an example of an alternating tree.

\begin{figure}
  \captionsetup{width=0.90\textwidth}
  \centering
  \includegraphics[width=0.75\textwidth, trim=0 3cm 0 1cm]{alttree.pdf}
  \caption{Alternating tree arising from some -uniform bipartite
    hypergraph and partial matching, depicted here with a root  and layers  and . The
    edges in  and  appear with a solid border, whereas
    (blocking) edges in  and  appear with a dotted border. The second edge from
the left with a solid border in  is an example of an immedidately addable edge.}
  \label{figure:alttree}
\end{figure}





\paragraph{Degree bound} Our augmenting algorithm
depends on a single parameter
 We also define  As
 will turn out to be an upper bound on the degree of any 
vertex in the alternating tree maintained by the augmenting algorithm,
we refer to  as the degree bound. Note that every  vertex in an
alternating tree , except for the root, is part of exactly one
blocking edge, which follows from Definition~\ref{def:alttree} and the
fact that  is a partial matching. Therefore, the degree bound implies that each non-root
 vertex can be part of at most  other (non-blocking) edges in
the alternating tree.

\begin{remark}
  Without loss of generality we will assume . The
  parameters  of our augmenting algorithm are set keeping in
  mind this range of values that  can assume. If we knew
  stronger guarantees about the hypergraph , for example, for
   as large as , then the values of these parameters
  can be set less aggressively and the running time bounds we obtain
  in later sections can also be improved drastically. Our goal here,
  however, is to show the existence of polynomial time algorithms even
  for a tiny advantage .
\end{remark}

\section{The Augmenting Algorithm}\label{section:augmenting}
\subsection{The BuildLayer Subroutine}\label{section:augmenting1}
We first describe a subroutine  that is used by the
augmenting algorithm. It takes as input an alternating tree , and a
pair of sets  that serve as the initial values for
the layer that the subroutine constructs. The subroutine augments 
and  and returns them at the end.

\paragraph{}
\begin{enumerate}[(a)]
\item We now describe what we mean by an ``addable edge'' for some
  given  and alternating tree . Suppose . For an  vertex
   we say that  has an \emph{addable
    edge} if i)  has fewer than  edges in , and ii)
   edge  for  disjoint from
  .
\item
    \textbf{While} there is an  having an
    addable edge , add  to  and its blocking
    edges to  as follows:
    
    \textbf{EndWhile}.
\item Return 
\end{enumerate}

\subsection{Main Algorithm}\label{section:augmenting2}

We now describe the augmenting algorithm. The input to the algorithm
is a partial matching  along with an  vertex  that is not
matched by .

\paragraph{Initialization} Initialize layer  in an alternating
tree  by setting . The
variable  will be updated to always point to the last layer in
the tree . Set it to .

\paragraph{Main Loop} Repeat the following two phases in order until
 is matched by .

\begin{enumerate}[(I)]
\item \textbf{Building phase}

  \begin{enumerate}
  \item Set .
  \item 
  \item Add the new layer  to
    .
  \item Increment  to .
  \end{enumerate}

\item \textbf{Collapse phase} Recall that  is
  \textit{immediately addable} if no edges from  are blocking it,
  i.e., .

    \textbf{While}  contains more than 
    immediately addable edges, perform the following steps:

    For convenience, we call this set of steps in this iteration, the
    \emph{collapse} operation of layer .

    \begin{enumerate}
    \item\label{step:lazy1} For each  such that there is an immediately
      addable edge  for ,
      
    \item Discard layer  from .
    \item\label{step:simulate} In this step we perform a
      \emph{superposed-build} operation on layer
       in . Note that this
      layer is modified in this step iff the condition in
      Step~\ref{step:cond} is satisfied.
      \begin{enumerate}
      \item
        
      \item\label{step:cond} If
         then,
        
      \end{enumerate}
    \item .
    \end{enumerate}
    \textbf{EndWhile}.
\end{enumerate}

After the initialization, the main loop of the algorithm consists of
repeating the build and collapse phases in order. The state of the
algorithm at any moment is described by the alternating tree
 and the partial matching  maintained by the
algorithm, both of which are dynamically modified. It is not difficult
to verify that the addition of an extra layer  in the
build phase and the collapse operations in the collapse phase modify
 and  in legal ways so that the resulting objects are consistent
with the definitions of an alternating tree and a partial matching,
respectively. We use these
facts without mention in the rest of the analayis.

Also note that set of vertices matched by  always remains the same
throughout the execution of the algorithm until a collapse operation
on layer  is performed, after which  additionally matches
, and the algorithm terminates.

 


\section{Analysis}\label{section:analysis}
We call a layer  \emph{collapsible} if more than  many edges in  are immediately addable with respect to . This is precisely the condition of the while loop in the collapse phase of the augmenting algorithm from Section~\ref{section:augmenting2}.



\begin{proposition}\label{lem:notcollapsible}
  Suppose that the alternating tree  and the
  partial matching  describe the state at the beginning of some
  iteration of the main loop of the augmenting algorithm. Then none of
  the layers  are collapsible. As a corollary it
  follows that  for each .
\end{proposition}

\begin{proof}
  Suppose that the statement is true at the beginning of the current
  iteration. During the build phase a new layer  is
  constructed. If  is not collapsible then the claim
  follows for the beginning of the next iteration since none of the
  previous layers were modified in the current iteration. If
   turns out to be collapsible, then by the definition of
  the collapse phase the layers  are left at the end of
  the collapse phase for some  (note that 
  unless  was matched and the algorithm terminates in the current
  iteration). The state of each of the layers  is
  unchanged from the beginning of the current iteration. Layer 
  on the other hand could have possibly been modified in
  Step~\ref{step:simulate} of the collapse phase. However, since it
  remains part of the alternating tree after the collapse phase it
  implies that , subsequent to any modifications, is not
  collapsible. Therefore none of the layers in the alternating tree
  are collapsible at the end of the iteration (unless the algorithm
  terminates after the current iteration). Since the claim is true for
  the first iteration, the claim follows by induction on the number of
  iterations of the main loop of the augmenting algorithm.

  The corollary follows since  is a layer, for each
  , and, by Definition~\ref{def:layer},  contains
  all the blocking edges of edges in  and each edge in 
  intersects (at most) one edge of .
\end{proof}

Before we state the next proposition some clarification is necessary
concerning the description of the augmenting algorithm in
Section~\ref{section:augmenting}. For instance, in the building phase,
there could be many vertices  that have an
\emph{addable edge} (as defined in the Section~\ref{section:augmenting1}), and
even a a given vertex could take many addable edges from which one is
eventually chosen. The final state of layer , at the
conclusion of the build phase, depends on the sum total of such
choices. The situation is similar in the collapse phase as well. In
order to properly specify the algorithm and refer to the quantities
maintained by it without ambiguity, we assume that there is a total
ordering on the vertices in  and edges in , and that
these orderings are used to choose a unique vertex and edge in any
event that many are admissible according to the algorithm description
in Section~\ref{section:augmenting}. This allows us, for example, to
refer precisely to \emph{the} layer  after performing a
build operation, or to \emph{the} layer  after performing a
superposed-build operation on layer , etc.

\begin{proposition}\label{lem:noaugment}
  Suppose that the alternating tree  and the
  partial matching  describe the state at the beginning of some
  iteration of the main loop of the augmenting algorithm.  Then the
  superposed-build operation on  (while ignoring layers
  )
  
  where  satisfies  for each
  .
\end{proposition}

\begin{proof}
  Consider some layer  for  present in the
  alternating tree at the beginning of the current iteration. At the
  iteration when layer  was built a superposed-build operation
  could not have increased the size of  even by one. If no
  collapse operations of some layer occurred until the current
  iteration then the situation remains identical, because layer
   was not collapsed in particular. If however, some layer
  was collapsed then it must have an index strictly greater than 
  (since otherwise, the algorithm would have discarded layer  in
  that case). As every time layer  is collapsed, and some
  edges from  are removed, the algorithm tries to augment 
  by a  fraction when possible (in Step~\ref{step:simulate} of
  the collapse phase), it follows that the number of edges in 
  cannot increase by more than a  fraction on performing
  superposed-build operation on .
\end{proof}



To ensure that the algorithm does not get stuck we need to show that, for some state  and  reached at the beginning of an iteration of the main loop, the build phase creates a new layer  with at least one edge. We prove the following stronger statement.

\begin{theorem}\label{lem:addable}
  Suppose  is the alternating tree at the beginning of some iteration of the main loop of the augmenting algorithm and let  be the newly constructed layer in the build phase of the iteration. Then,
  
  for each .
\end{theorem}

The proof of Theorem~\ref{lem:addable} uses Lemma~\ref{lem:addable0} and Lemma~\ref{lem:addable1}, which we prove below.

\begin{lemma}\label{lem:addable0}
  Suppose that  at the beginning of some iteration of the main loop. Then when  is built in the build phase of the iteration, .
\end{lemma}
\begin{proof}
  By Proposition~\ref{lem:notcollapsible},
  
  where we use that . By the choice of
   we then have . Therefore, by the invariants from
  Proposition~\ref{lem:notcollapsible} and
  Proposition~\ref{lem:noaugment}, every edge in  has
  at least one blocking edge in the tree, and no  vertex with less
  than  edges in  has an edge that is disjoint from
  . Next, no  vertex in the
  tree can have  edges in the tree, as in
  that case the number of blocking edges for that vertex would be at
  least  which is greater than 
  contradicting our hypothesis. Taking  to be the set of all the
   vertices in the tree, so that , these
  arguments show that the number of  vertices in the tree is an
  upper bound on .

  We now show that the number of  vertices in the tree is at most
  . To see this, note that every non-root  vertex in
  the tree is included in a unique edge in  (also in the tree),
  which in turn intersects some unique edge in . Therefore for each non-root  vertex in the tree, we
  have  corresponding  vertices from the matching edge in
   and an additional set of at most  many  vertices from the
  unique edge in the tree that intersects this matching edge. Further,
  this accounts for all the  vertices in the tree. We may over
  count some  vertices in edges that were added as addable edges in
  the alternating tree but this is fine since we are only aiming for
  an upper bound. So each non-root  vertex can be throught to
  contribute at most  many  vertices to the tree.

  However, the guarantee is that  must be larger than
  , which is a contradiction.
\end{proof}

We can say something stronger than Lemma~\ref{lem:addable0} when the
number of edges from  in the alternating tree becomes . Notice that this condition is satisfied when the number of layers in the alternating tree is .

\begin{lemma}\label{lem:addable1}
  If , at the beginning of some iteration then when layer  is built

\end{lemma}
\begin{proof}
  Suppose that after the build phase constructing layer  is complete,  where . 

  Let  be the set of  vertices from  that would
  take an addable edge if we were to perform a superposed-build
  operation on layer  while ignoring layers
  . Formally,  where
  

  Now define  algorithmically (in the sense of performing steps in
  order) as follows:
  \begin{itemize}
  \item set  to be the set of all  vertices in ,
  \item remove all  vertices from  that have  edges in the alternating tree (i.e., appear  times in the edges in ),
  \item remove all  vertices in  from .
  \end{itemize}
The number of  vertices that have  edges in the alternating tree is at most . The number of  vertices in  is upper bounded by  using Proposition~\ref{lem:noaugment}. Therefore, 
By our hypothesis towards contradiction  Also, by Proposition~\ref{lem:notcollapsible},  Putting these together,


As , we have 

Recall that . So, after upper bounding the inner sum by

we have 

Next we obtain an upper bound on . We start by proving the following claim.

\begin{claim}\label{claim:local}
   is an upper bound on the cardinality of the
  smallest size hitting set for  that is also a subset of , i.e., an upper bound on .
\end{claim}
\begin{proof}
  From the definition of , every vertex  appears in one of
  the layers  and has strictly less than  edges
  in the tree. Further, since each  is not part of
   this means that there is no edge in  for
  the vertex  that is disjoint from the  vertices in the tree
  and the  vertices introduced in the superposed-build operations
  in each of the layers . We now bound the total
  number of such  vertices, to prove the claim.

The number of  vertices present in the alternating tree
 is simply . Next, we know by Proposition~\ref{lem:noaugment} that a
superposed-build operation on a layer  for 
produces a layer  such that . Further,
the set of  vertices introduced in  (as part of an addable
edge and their associated blocking edges) not already present in layer
 (which was counted previously), is at most
---each addable edge along with their blocking edges
contains at most  many  vertices.
\end{proof}

We now bound the total number of  vertices in layers
 in the alternating tree. The contribution from
layer  is at most  since each addable
edge and its associated set of blocking edges can introduce at most
 many  vertices. Next, every edge in  is
either immediately addable or not. The  vertices in immediately
addable edges from layers  is at most  using Proposition~\ref{lem:notcollapsible}. The 
vertices from layers  that are \emph{not} present in
immediately addable edges can be upper bounded simply by  using the same argument as in Lemma~\ref{lem:addable0}.

Therefore, from Claim~\ref{claim:local} the following upper bound is then obtained for :


We now explain the terms in the bound. The first three terms bound the number of  vertices in the alternating tree as we saw above. The final term upper bounds contributions from edges not present in the alternating tree but those that could be added during the superposed-build operations on each of the layers . Using the known bounds on  (from Proposition~\ref{lem:notcollapsible}) and  (by hypothesis),



For the chosen parameters  and , we get,



From \eqref{eqn:largeHS2} and \eqref{eqn:smallHS}, we have a contradiction when,




which is true for .
\end{proof}

We now complete the proof of Theorem~\ref{lem:addable}.

\begin{proof}[Proof of Theorem~\ref{lem:addable}]
  First notice that after a layer  is built, the number of
  edges in  is non-decreasing until it is collapsed in some
  future iteration. Also, any collapse operation on a layer leads to
  discarding that layer. Then the claim follows by combining
  Lemma~\ref{lem:addable0} and Lemma~\ref{lem:addable1} to note that
  at the moment when layer  is created, for some , the inequality  holds. This also remains true in future iterations until a collapse operation occurs in layer , in which case it will no longer be part of the alternating tree maintained by the augmenting algorithm. 
\end{proof}

We are now in a position to bound the number of layers in the alternating tree at any point in the execution of the augmenting algorithm.

\begin{lemma}\label{lem:layers}
  The number of layers in the alternating tree  maintained during the execution of the augmenting algorithm is always bounded by .
\end{lemma}

\begin{proof}
Suppose there are  layers  at the beginning of some iteration of the main loop of the augmenting algorithm. Consider some layer  for . By Proposition~\ref{lem:notcollapsible} less than  fraction of  are immediately addable, and hence . Then by Theorem~\ref{lem:addable} we have  for each . This quickly yields , so that , where . Altogether this implies that the number of layers  at any moment in the algorithm is bounded by .
\end{proof}

\section{Signature Vectors}

To keep track of the progress made by the augmenting algorithm we design a potential function. For a given state of the alternating tree with layers  in total we define the signature of layer  (for ) as:

where . The potential function associated with
the alternating tree at any state is the sequence obtained by
concatenating the signatures of the individual layers in order, and
finally appending the symbol  at the end. We refer to this
potential function as the signature vector. In total there are
 coordinates in the signature vector which we write as
.



\begin{lemma}\label{lem:decrease}
  The lexicographic value of the signature vector reduces across each iteration of the main loop in the augmenting algorithm unless the algorithm terminates during that iteration.
\end{lemma}
\begin{proof}
  Suppose the alternating tree  and the partial matching  define the state of the algorithm at the beginning of the iteration. Let the signature of the corresponding alternating tree be . We consider two cases depending on whether a collapse operation occurred during the collapse phase of the current iteration.
  
  \begin{itemize}
  \item \textbf{No collapse operation occurred.}  In this case only the build phase of the iteration modified the state of the algorithm by adding a new layer . Thus, the new signature of the alternating tree is  where  for all  and  are defined as in \eqref{eqn:signature} for layer  at the beginning of the next iteration. Clearly the lexicographic value of the signature of the alternating tree has reduced.

  \item \textbf{At least one collapse operation occurred.}  This means that during the iteration a new layer  was built, and one or more collapse operations occurred in the collapse phase. Let primed quantities denote the variables after the end of the collapse phase in the iteration. Suppose that  () is the index of the earliest layer that was collapsed among all the collapse operations in the collapse phase in the iteration. If  then  was matched and the algorithm terminates. Otherwise  and by the description of the algorithm, the only layers left in the alternating tree after the collapse phase are  where  is identical to  for all . Thus the new signature after the collapse phase is  where  for all  and,

When layer  was collapsed Step~\ref{step:simulate} of the collapse phase could have possibly modified layer . Accordingly there are two subcases.

\begin{itemize}
\item  Since there was no modification to  we look at how  has changed. As we collapsed layer  in the alternating tree, there must have been at least  immediately addable edges in . These must have caused the removal of at least  many matching edges in . Further, by Theorem~\ref{lem:addable}, . Together this means that 
 By our choice of the base of the logarithm it holds that . Therefore,   whereas .

\item  In this subcase the fact that  implies that the lexicographic value of the signature vector has reduced since .
\end{itemize}
  \end{itemize}
\end{proof}

To show that the augmenting algorithm terminates in polynomial time we need one more fact.

\begin{proposition}\label{lem:nondecreasing}
  The coordinates of the signature vector are non-decreasing in absolute value at the beginning of each iteration of the main loop of the augmenting algorithm.
\end{proposition}
\begin{proof}
  Consider some layer  for . Clearly the corresponding pair of coordinates  in the signature vector are non-decreasing in absolute value since  using Proposition~\ref{lem:notcollapsible}. Between any two layers, by Theorem~\ref{lem:addable} we have  and so . Thus the coordinates are non-decreasing in absolute value in the signature vector.
\end{proof}

\begin{lemma}\label{lem:sigs}
  The number of signature vectors is bounded by a polynomial in .
\end{lemma}
\begin{proof}
  By Lemma~\ref{lem:layers} we know that the signature vector has at
  most  coordinates. By Proposition~\ref{lem:nondecreasing}
  the coordinates are also integers that are non-decreasing in
  absolute value. At this point one can obtain a trivial bound of
   on the absolute value of each coordinate of the
  signature vector using the definition in
  \eqref{eqn:signature} and Lemma~\ref{lem:layers}. Since the sign
  pattern of the signature vector is always fixed, each signature
  vector can be thought to describe a unique partition of some
  positive integer of size at most . Recall that a
  \emph{partition} of a positive integer  is a way of writing 
  as the sum of positive integers without regard to order. Since the
  number of partitions of an integer of size  is (asymptotically)
  at most  for some absolute constant
  ~\cite{HardyRam18}, the claim then follows.
\end{proof}

As noticed by one of the reviewers, the dependence of
Lemma~\ref{lem:sigs} on the asymptotics of the partition function can
be avoided by modifying the signature vector to ensure that its
entries are strictly increasing in absolute value (instead of simply
being non-decreasing as in Lemma~\ref{lem:nondecreasing}), thereby
allowing a signature vector to be inferred by specifying a subset of a
set of size at most . One way to get this property is by
adding/subtracting  to the -th coordinate of the signature
vector, consistent with its sign pattern.

We are now in a position to use the potential function defined in this
section to wrap up the proof of our main result.

\begin{proof}[Proof of Theorem~\ref{thm:main}]
  From Lemma~\ref{lem:decrease} we have that every iteration of the main loop of the augmenting algorithm described in Section~\ref{section:augmenting} reduces the lexicographic value of the signature vector. Lemma~\ref{lem:sigs} further tells us that the number of such signature vectors is bounded by a polynomial in . Thus, the augmenting algorithm terminates in polynomially many iterations. It can also be verified that each iteration of the augmenting algorithm can be implemented to run in time polynomial in  and . Finally, running the augmenting algorithm  times, starting with an empty partial matching, yields the desired perfect matching in .  \end{proof}

 
From the proof of Lemma~\ref{lem:addable1} we also note that the
algorithm in Section~\ref{section:augmenting} can be suitably modified
to yield the following
slightly stronger version of Theorem~\ref{thm:main} as a corollary.
\begin{theorem}\label{thm:maincorollary}
  For every fixed choice of  and , there exists
  an algorithm  that takes as input an
  -uniform bipartite hypergraph , runs in polynomial
  time, and terminates after finding either:
  \begin{itemize}
  \item a perfect matching in , or
  \item a set  such that 
  \end{itemize}
\end{theorem}

\section{Conclusion and Open Problems}
In this paper we presented a polynomial time algorithm for finding
perfect matchings in bipartite hypergraphs satisfying a slightly
stronger version of Haxell's condition. The algorithm is essentially
the natural generalization of the well known Hungarian algorithm for
finding perfect matchings in graphs with two essential modifications:
i) restricting the degree of vertices in the constructed alternating
tree, and ii) performing updates on the alternating tree lazily. The
two ideas in tandem give us a polynomial running time bound on the
procedure.

One subtlety here is that the algorithm performs lazy updates in two
places, in Steps~\ref{step:lazy1} and~\ref{step:simulate}, in the
collapse phase. While the former is crucial for the running time
bound, the latter seems to be an artifact of the analysis driven by
the specific choice of the signature vector in
Section~\ref{section:analysis}. In particular, this can likely be
avoided by choosing a different signature vector to measure progress.

Finally, we point out the obvious open problem in this line of work.

\begin{question}
  Does there exist such an algorithm with a polynomial running time
  dependence on at least one of the parameters  and ?
\end{question}

\section*{Acknowledgements}
We thank Yuri Faenza and Ola Svensson for providing helpful comments
on an earlier draft of this paper. We also thank anonymous SODA
reviewers for their valuable comments that helped improved the
presentation.

\bibliographystyle{alpha}
\bibliography{refs}











\end{document}
