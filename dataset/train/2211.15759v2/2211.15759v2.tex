\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[accsupp]{axessibility}  


\usepackage{enumitem}


\wacvalgorithmstrack
\wacvfinalcopy



\ifwacvfinal
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\else
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\fi

\pagestyle{empty}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\usepackage{multirow}

\begin{document}

\title{
\textit{Supplementary Material for}

PIDS: Joint Point Interaction-Dimension Search for 3D Point Cloud

}


\maketitle
\thispagestyle{empty}

\setcounter{section}{9}
\setcounter{table}{8}
\setcounter{figure}{4}

 
In this appendix, we first evaluate PIDS on ModelNet40 to study its effect on classification benchmarks.
Then, we elaborate the detailed structure of hand-crafted PIDS, built upon efficient 3D point operators within the PIDS search space.
Finally, we discuss the detailed configuration of Dense-Sparse predictor. Our code is publicly available \href{https://github.com/lordzth666/WACV23_PIDS-Joint-Point-Interaction-Dimension-Search-for-3D-Point-Cloud}{here}.

\subsection{Evaluation on ModelNet40 Classification}
We run PIDS on ModelNet40 classification benchmark to search for highly representative classification models.
On ModelNet40, a single training batch in ModelNet40 contains 16 sub-sampled point clouds.
We adopt a similar training pipeline following the original KPConv~\cite{thomas2019kpconv} paper yet incorporates SimpleView~\cite{goyal2021revisiting} RSCNN training protocol for fair comparison.
Specifically, the SimpleView-RSCNN protocol adopts random scaling \& translation for data augmentation, employ cross-entropy loss to optimize, and utilize a voting scheme on the best test model to best exploit the potential of searched model.
\begin{table}[h]
    \begin{center}
    \caption{Overall Accuracy (OA) on ModelNet40. : Use native protocol instead of SimpleView RSCNN evaluation protocol.}
    \scalebox{1.0}{
    \begin{tabular}{|c|c|c|}
        \hline
         \multirow{2}{*}{\textbf{Architecture}} &  \textbf{Params} &  \textbf{OA} \\
         &  \textbf{(M)} & \textbf{(\%)}  \\
         \hline
PointCNN~\cite{li2018pointcnn} & 0.60 & 92.2 \\
         PointConv~\cite{wu2019pointconv} & - & 92.5 \\
SPH3D-GCN~\cite{lei2020spherical} & 0.8  & 92.1 \\
         FPConv~\cite{lin2020fpconv} & 2.1 & 92.5 \\
         RSCNN~\cite{liu2019relation} & 1.3 & 92.5 \\
         DGCNN~\cite{wang2019dynamic} & 1.8 & 92.8 \\
         KPConv~\cite{thomas2019kpconv} & 14.9 & 92.9 \\
SimpleView~\cite{goyal2021revisiting} & 0.80 & 93.2 \\
         PointNet++~\cite{qi2017pointnet++} & 1.48 & 93.3 \\
         \hline
PIDS (second-order) & 1.25 & 92.6 \\
         \hline
         PIDS (NAS) & \textbf{0.56} & 93.1 \\
         PIDS (NAS, 2) & 1.21 & \textbf{93.4} \\
         \hline
    \end{tabular}
    }    
    \label{tab:result_modelnet40}
    \end{center}
    \vspace{-2em}
\end{table}

We demonstrate the evaluation results on the testing dataset of ModelNet40 in Table \ref{tab:result_modelnet40}.
PIDS explores more efficient model designs with higher performance, out-performing KPConv and other prior arts~\cite{goyal2021revisiting} with up to 1.3\% OA while being up to 12 smaller in size.


\subsection{Structure of First-order PIDS}
\begin{table}[h]
\caption{Structure of hand-crafted PIDS (first-order). "1/2" strides means up-sampling by 2, and "O" denotes "Octahedron" kernel with 7 kernel points. All point operators employ a first-order point interaction.}
\begin{center}
\scalebox{0.95}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        \multirow{2}{*}{\textbf{Point-Op}} & \multirow{2}{*}{\textbf{Depth}} & \multirow{2}{*}{\textbf{Stride}} & \textbf{In} & \textbf{Out} & \multirow{2}{*}{\textbf{E}} & \multirow{2}{*}{\textbf{K}} \\
        & & & \textbf{Width} & \textbf{Width} & & \\
        \hline
         \textbf{1} & 1 & 1 & 16 & 16 & 1 & O \\
         \textbf{2} & 2 & 2 & 16 & 24 & 3 & O \\
         \textbf{3} & 3 & 2 & 24 & 32 & 3 & O \\
         \textbf{4} & 4 & 2 & 32 & 64 & 3 & O \\
         \textbf{5} & 3 & 1 & 64 & 96 & 3 & O \\
         \textbf{6} & 3 & 2 & 96 & 160 & 3 & O \\
         \textbf{7} & 1 & 1 & 160 & 320 & 3 & O \\
         \textbf{8} & 1 & 1/2 & 416 & 160 & 3 & O \\
         \textbf{9} & 1 & 1/2 & 160 & 96 & 3 & O \\
         \textbf{10} & 1 & 1/2 & 96 & 64 & 3 & O \\
         \textbf{11} & 1 & 1/2 & 64 & 32 & 3 & O \\
         \hline
    \end{tabular}
    }
\end{center}
\label{tab:pids_1st_order}
\vspace{-2em}
\end{table}
We follow the layer organization to manually craft first-order PIDS (Table 2), and demonstrate the detailed architecture in Table \ref{tab:pids_1st_order}, echoing the design of search components in Section 3.3.
The hand-crafted architecture contains a total of 11 point operators (Point-Op). 
Among them, 7 point operators serve as a classification backbone, and 4 additional point operators serve as a semantic segmentation head. 
The positional setting (i.e., depth, block width) of hand-crafted PIDS strictly follows MobileNet-V2, which is considered as a good hand-crafted model design with remarkable efficiency.
For structural settings, we employ an expansion factor (E) of 3 for all point operators except for the first one.
We leverage the 7-point Octahedron layout for each point operator for design efficiency. 
As a result, first-order PIDS serves as a strong baseline to compare with, when we evaluate the performance of NAS-crafted PIDS models.


\subsection{Details of PIDS Search Space}
We present the detailed settings of the PIDS interaction-dimension search space in Table \ref{tab:search_cfg}. 
We jointly search the point interaction (kernel size and interaction type) and point dimension (block depth, block width, expansion factor) to find the best architecture.

\begin{table*}[t]
\begin{center}
    \caption{Configuration of the PIDS Search Space. "1/2" strides means up-sampling by 2.}
    \scalebox{0.825}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
        \textbf{Hierarchy} & \textbf{Stage}  & \textbf{Strides} & \textbf{Order Type}  & \textbf{Kernel Type} & \textbf{Depth} & \textbf{Expansion Factor} & \textbf{Width} \\
        \hline
         & \textbf{1} & 1 & first-order & Tetrahedron, Octahedron, Icosahedron & 1  & 1 & 16 \\
         & \textbf{2} & 2 & first-order & Tetrahedron, Octahedron, Icosahedron & 2, 3 & 2, 3, 4 & 16, 24 \\
         & \textbf{3} & 2 & first-order/second-order  & Tetrahedron, Octahedron, Icosahedron & 2, 3, 4 & 2, 3, 4 & 24, 32 \\
        \textbf{Backbone} & \textbf{4} & 2 & first-order/second-order &  Tetrahedron, Octahedron, Icosahedron & 3, 4, 5 & 2, 3, 4 & 24, 32, 40 \\
         & \textbf{5} & 1 & first-order/second-order & Tetrahedron, Octahedron, Icosahedron  & 2, 3, 4 & 2, 3, 4 & 40, 56, 72 \\
         & \textbf{6} & 2 & first-order/second-order  & Tetrahedron, Octahedron, Icosahedron & 3, 4, 5 & 2, 3, 4 & 64, 80, 96 \\
         & \textbf{7} & 1 & first-order/second-order  & Tetrahedron, Octahedron, Icosahedron & 1 & 2, 3, 4 & 160 \\
         \hline
         & \textbf{8} & 1/2 & first-order/second-order &  Tetrahedron, Octahedron, Icosahedron & 1  &  2, 3 & 64, 80, 96 \\
         \textbf{Segmentation} & \textbf{9} & 1/2 & first-order/second-order & Tetrahedron, Octahedron, Icosahedron & 1  & 2, 3 & 40, 56, 72 \\
         \textbf{Head} & \textbf{10} & 1/2 & first-order/second-order  & Tetrahedron, Octahedron, Icosahedron & 1  & 2, 3 & 24, 32, 40 \\
         & \textbf{11} & 1/2 & first-order & Tetrahedron, Octahedron, Icosahedron & 1 & 2, 3 & 16, 24 \\
         \hline
    \end{tabular}
    }    
    \label{tab:search_cfg}
    \end{center}
    \vspace{-2em}
\end{table*}


\subsection{Design of Dense-Sparse Predictor}
We manually configure the architectural hyperparameters of our Dense-Sparse predictor without delicate architecture engineering.
Specifically, we employ a 3-layer MLP with 64-128-256 layers as the dense architecture to extract dense neural architecture representations for both positional organizations and structural settings.
The overall architecture that is responsible for processing fused features after dense-sparse interaction is a 2-layer MLP with 256-256 layers. We utilize ReLU as the activation function and apply Dropout of 0.5 before the final regression head to mitigate overfitting. To ensure fair comparison in Table 3, both the Dense predictor and the Sparse predictor also adopt a 2-layer MLP with 256-128 units to keep the same maximum projection dimension as the Dense-Sparse predictor.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage
\appendix

\end{document}
