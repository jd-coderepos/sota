

\typeout{IJCAI--21 Instructions for Authors}



\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{ijcai21}

\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfigure}
\usepackage{multirow}
\urlstyle{same}
\usepackage{bm}
\usepackage{color}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}


\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}







\pdfinfo{
/TemplateVersion (IJCAI.2021.0)
}

\title{HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping} 





\author{
Yuhan Wang\footnotemark[1] \and
Xu Chen\footnotemark[1] \and
Junwei Zhu\and
Wenqing Chu \and
Ying Tai\footnotemark[2] \\
Chengjie Wang \and
Jilin Li \and
Yongjian Wu \and
Feiyue Huang \And
Rongrong Ji \\
\affiliations
{ {Youtu Lab, Tencent}} \\
{ {Zhejiang University}} \\
{ {Media Analytics and Computing Lab, Department of Artificial Intelligence, School of Informatics,
Xiamen University}} \\
{ {Institute of Artiﬁcial Intelligence, Xiamen University}} \\
\emails
{
{~\color{magenta} ~\url{https://johann.wang/HifiFace}}
}
}


\begin{document}



\twocolumn[{\renewcommand\twocolumn[1][]{#1}\maketitle 

\begin{figure}[H] 
\hsize=\textwidth
\centering
\includegraphics[width=0.95\textwidth]{figures/front_final.pdf}
\caption{Face swapping results generated by our HifiFace. 
The face in the target image is replaced by the face in the source image.} 
\label{fig:title} 
\end{figure}
}]




\footnotetext[1]{Equal Contribution. Work done when Yuhan and Xu are both interns at Youtu Lab, Tencent.} 
\footnotetext[2]{Corresponding Author.}


\begin{abstract}
In this work, we propose a high fidelity face swapping method, called HifiFace, which can well preserve the face shape of the source face and generate photo-realistic results. Unlike other existing face swapping works that only use face recognition model to keep the identity similarity, we propose 3D shape-aware identity to control the face shape with the geometric supervision from DMM and D face reconstruction method. Meanwhile, we introduce the Semantic Facial Fusion module to optimize the combination of encoder and decoder features and make adaptive blending, which makes the results more photo-realistic. Extensive experiments on faces in the wild demonstrate that our method can preserve better identity, especially on the face shape, and can generate more photo-realistic results than previous state-of-the-art methods. 
\end{abstract}




\section{Introduction}

Face swapping is a task of generating images with the identity from a source face and the attributes (\textit{e.g.}, pose, expression, lighting, background, \textit{etc.}) from a target image (as shown in Figure~\ref{fig:title}), which has attracted much interest with great potential usage in film industry~\cite{alexander2009creating} and computer games.


In order to generate high-fidelity face swapping results, there are several critical issues: () The identity of the result face including the \textit{face shape} should be close to the source face.   
() The results should be \textit{photo-realistic} which are faithful to the expression and posture of the target face and consistent with the details of the target image like lighting, background, and occlusion.


To preserve the identity of the generated face, previous works ~\cite{nirkin2018face,Nirkin2019fsgan,jiang2020deeperforensics} generated inner face region via DMM fitting or face landmark guided reenactment and blend it into the target image, as shown in Figure~\ref{fig:previous_work}(a). These methods are weak in identity similarity because DMM can not imitate the identity details and the target landmarks contain the identity of target image. Also, the blending stage restricts the change of face shape. As shown in Figure~\ref{fig:previous_work}(b),~\cite{liu2019identity,chen2020simswap} draw support from a face recognition network to improve the identity similarity.
However, face recognition network focuses more on texture and is insensitive to the geometric structure. Thus, these methods can not preserve the exact face shape robustly.

As for generating photo-realistic results,~\cite{nirkin2018face,Nirkin2019fsgan} used Poisson blending to fix the lighting, but it tended to cause ghosting and could not deal with complex appearance conditions.~\cite{jiang2020deeperforensics,zhu2020aot,li2019faceshifter} designed an extra learning-based stage to optimize the lighting or occlusion problem, but they are fussy and can not solve all problems in one model. 

To overcome the above defects, we propose a novel and elegant end-to-end learning framework, named HifiFace, to generate high fidelity swapped faces via D shape and semantic prior. Specifically, we first regress the coefficients of source and target face by a D face reconstruction model and recombine them as shape information. Then we concatenate it with the identity vector from a face recognition network. We explicitly use the D geometric structure information and use the recombined 3D face model with source's identity, target's expression, and target's posture as auxiliary supervision to enforce precise face shape transfer. With this dedicated design, our framework can achieve more similar identity performance, especially on face shape.

Furthermore, we introduce a Semantic Facial Fusion (SFF) module to make our results more photo-realistic. 
The attributes like lighting and background require spatial information and the high image quality results need detailed texture information. The low-level feature in the encoder contains spatial and texture information, but also contains rich identity from the target image. 
Hence, to better preserve the attributes without the harm of identity, our SFF module integrates the low-level encoder features and the decoder features by the learned adaptive face masks.
Finally, in order to overcome the occlusion problem and achieve perfect background, we blend the output to the target by the learned face mask as well.
Unlike~\cite{Nirkin2019fsgan} that used the face masks of the target image for directly blending, HifiFace learns face masks at the same time under the guidance of dilated face semantic segmentation, which helps the model focus more on the facial area and make adaptive fusion around the edge.  
HifiFace handles image quality, occlusion, and lighting problems in \textit{one} model, making the results more photo-realistic. Extensive experiments demonstrate that our results surpass other State-of-the-Art (SOTA) methods on wild face images with large facial variations. 

Our contributions can be summarized as follows:
\begin{enumerate}
    \item We propose a novel and elegant end-to-end learning framework, named HifiFace, which can well preserve the face shape of the source face and generate high fidelity face swapping results.
    \item We propose a 3D shape-aware identity extractor, which can generate identity vector with exact shape information to help preserve the face shape of the source face.
    \item We propose a semantic facial fusion module, which can solve occlusion and lighting problems and generate results with high image quality.

    

\end{enumerate}




\section{Related Work}


\paragraph{D-based Methods.} 
D Morphable Models (DMM) transformed the shape and texture of the examples into a vector space representation ~\cite{blanz1999morphable}.   
~\cite{thies2016face2face} transferred expressions from source to target face by fitting a 3D morphable face model to both faces. 
 ~\cite{nirkin2018face} transferred the expression and posture by DMM and trained a face segmentation network to preserve the target facial occlusions. 
These D-based methods follow a source-oriented pipeline like Figure~\ref{fig:previous_work}(a) which only generates the face region by D fitting and blends it into the target image by the mask of the target face. They suffer from unrealistic texture and lighting because the DMM and the renderer can not simulate complex lighting conditions. Also, the blending stage limits the face shape. In contrast, our HifiFace accurately preserves the face shape via geometric information of DMM and achieves realistic texture and attributes via semantic prior guided recombination of both encoder and decoder feature.
\begin{figure*}[t] 
\begin{center} 
\includegraphics[width=0.85\linewidth]{figures/previous_work_crop2.pdf} 
\end{center} 
\vspace{-2mm}
\caption{The pipelines of previous works and our HifiFace. 
(a) Source-oriented pipeline uses D fitting or reenactment to generate inner face region and blend it into the target image, in which  means the face region of the result. 
(b) Target-oriented pipeline uses a face recognition network to exact identity and combines encoder feature with identity in the decoder. 
(c) Our pipeline consists of four parts: the Encoder part, Decoder part, D shape-aware identity extractor, and SFF module. 
The encoder extracts features from , and the decoder fuses the encoder feature and the D shape-aware identity feature. Finally, the SFF module helps further improve the image quality.} 
\label{fig:previous_work} 
\end{figure*}


\paragraph{GAN-based Methods.}
GAN has shown great ability in generating fake images since it was proposed by ~\cite{goodfellow2014generative}.~\cite{isola2017image} proposed a general image-to-image translation method, which proves the potential of conditional GAN architecture in swapping face, although it requires paired data. 

The GAN-based face swapping methods mainly follow source-oriented pipeline or target-oriented pipeline. ~\cite{Nirkin2019fsgan,jiang2020deeperforensics} followed source-oriented pipeline in Figure~\ref{fig:previous_work}(a) which used face landmarks to compose face reenactment. But it may bring weak identity similarity, and the blending stage limited the change of face shape. ~\cite{liu2019identity,chen2020simswap,li2019faceshifter} followed target-oriented pipeline in Figure~\ref{fig:previous_work}(b) which used a face recognition network to extract the identity and use decoder to fuse the encoder feature with identity, but they could not robustly preserve exact face shape and is weak in image quality.
Instead, HifiFace in Figure~\ref{fig:previous_work}(c) replaces the face recognition network with a D shape-aware identity extractor to better preserve identity including the face shape and introduces an SFF module after the decoder to further improve the realism.


Among these, FaceShifter~\cite{li2019faceshifter} and SimSwap~\cite{chen2020simswap} follow target-oriented pipeline and can generate high fidelity results. FaceShifter~\cite{li2019faceshifter} leveraged a two-stage framework and achieved state-of-the-art identity performance. But it could not perfectly preserve the lighting despite using an extra fixing stage. However, HifiFace can well preserve lighting and identity in \textit{one} stage. Meanwhile, HifiFace can generate photo-realistic results with higher quality than FaceShifter.  
~\cite{chen2020simswap} proposed weak feature matching loss to better preserve the attributes, but it harms the identity similarity. While HifiFace can better preserve attributes and do not harm the identity.  






\section{Approach}
Let  be the source images and  the target images, respectively. We aim to generate result image  with the identity of the  and the attributes of . As illustrated in Figure~\ref{fig:previous_work}(c), our pipeline consists of four parts: the Encoder part, Decoder part, D shape-aware identity extractor (Sec.~\ref{sec:sid}), and SFF module (Sec.~\ref{sec:sff}). 
First, we set  as the input of the encoder and use several res-blocks~\cite{he2016deep} to get the attribute feature. Then, we use the D shape-aware identity extractor to get D shape-aware identity. 
After that, we use res-block with adaptive instance normalization~\cite{karras2019style} in decoder to fuse the D shape-aware identity and attribute feature. 
Finally, we use the SFF module to get higher resolution and make the results more photo-realistic.  



\subsection{3D Shape-Aware Identity Extractor}\label{sec:sid}
Most GAN-based methods only use a face recognition model to obtain identity information in the face swapping task. However, the face recognition network focuses more on texture and is insensitive to the geometric structure.
To get more exact face shape features, we introduce DMM and use a pre-trained state-of-the-art D face reconstruction model~\cite{deng2019accurate} as a shape feature encoder, which represents the face shape  by an affine model:
where  is the average face shape; ,  are the PCA bases of identity and expression; {} and {} are the corresponding coefficient vectors for generating a 3D face.

As illustrated in Figure~\ref{fig:pipline}(a), we regress DMM coefficients  and , containing identity, expression, and posture of the source and target face by the D face reconstruction model . 
Then, we generate a new D face model by  with the source's identity, target's expression and posture.
Note that posture coefficients do not decide face shape but may affect the D landmarks locations when computing the loss. 
We do not use the texture and lighting coefficients because the texture reconstruction still remains unsatisfactory. 
Finally, we concatenate the  with the identity feature  extracted by , a pre-trained state-of-the-art face recognition model~\cite{huang2020curricularface}, and get the final vector , called 3D shape-aware identity. 
Thus, HifiFace achieves well identity information including geometric structure, which helps preserve the face shape of the source image.


\begin{figure*}[t] 
\begin{center} 
\includegraphics[width=0.9\linewidth]{figures/3D_SFF_final_0530.pdf} 
\end{center} 
\vspace{-3mm}
\caption{Details of D shape-aware identity extractor and SFF module. 
(a) D shape-aware identity extractor uses  (D face reconstruction network) and  (face recognition network) to generate shape-aware identity. 
(b) SFF module recombines the encoder and decoder feature by  and makes the final blending by . The  means the upsample Module.} 
\label{fig:pipline} 
\end{figure*}


\subsection{Semantic Facial Fusion Module}\label{sec:sff}

\paragraph{Feature-Level.} 
The low-level feature contains rich spatial information and texture details, which may significantly help generate more photo-realistic results.
Here, we propose the SFF module to not only make full use of the low-level encoder and decoder features, but also overcome the contradiction in avoiding harming the identity because of the target's identity information in low-level encoder feature.

As shown in Figure~\ref{fig:pipline}(b), we first predict a face mask  when the decoder features  are of size  of the target. Then, we blend  by  and get , formulated as:
where  means the low-level encoder feature with  size of the original and  means a res-block~\cite{he2016deep}.

The key design of SFF is to adjust the attention of the encoder and decoder, which helps disentangle identity and attributes.
Specifically, the decoder feature in non-facial area can be damaged by the inserted source's identity information, thus we replace it with the clean low-level encoder feature to avoid potential harm.
While the facial area decoder feature, which contains rich identity information of the source face, should not be disturbed by the target, therefore we preserve the decoder feature in the facial area. 

After the feature-level fusion, we generate  to compute auxiliary loss for better disentangling the identity and attributes. Then we use a  Upsample Module  which contains several res-blocks to better fuse the feature maps. Based on , it's convenient for our HifiFace to generate even higher resolution results (\textit{e.g.}, ).



\paragraph{Image-Level.}  In order to solve the occlusion problem and better preserve the background, previous works~\cite{Nirkin2019fsgan,natsume2018fsnet} directly used the mask of the target face. 
However, it brings artifacts because the face shape may change.
Instead, we use SFF to learn a sightly dilated mask and embrace the change of the face shape.  
Specifically, we predict a -channel  and -channel , and blend  to the target image by , formulated as:
In summary, HifiFace can generate photo-realistic results with high image quality and well preserve lighting and occlusion with the help of the SFF module. 
Note that these abilities still work despite the change of face shape, because the masks have been dilated and our SFF benefits from inpainting around the contour of predicted face. 


\subsection{Loss Function}


\paragraph{3D Shape-Aware Identity (SID) Loss.} 
SID loss contains shape loss and ID loss. 
We use D landmark keypoints as geometric supervision to constrain the face shape, which is widely used in D face reconstruction~\cite{deng2019accurate}. 
First we use a mesh renderer to generate D face model by coefficients of source image identity and target image expression and posture. 
Then, we generate D face model of  and  by regressing DMM coefficients. 
Finally we project the D facial landmark vertices of reconstructed face shapes
onto the image obtaining landmarks \{{}\}, \{{}\} and \{{}\}:Also, we use identity loss to preserve source image's identity:
where  means the identity vectors generated by , and  means the cosine similarity of two vectors. 
Finally, our SID loss is formulated as:
where  =  and  = .


\paragraph{Realism Loss.} 
Realism loss contains segmentation loss, reconstruction loss, cycle loss, perceptual loss, and adversarial loss. 
Specifically,  and  in the SFF module are both under the guidance of a SOTA face segmentation network HRNet~\cite{sun2019high}. We dilated the masks of the target image to eliminate the limitation in face shape change and get . 
The segmentation loss is formulated as:
where  means the resize operation.

If  and  share the same identity, the predicted image should be the same as . So we use reconstruction loss to give pixel-wise supervision:




The cycle process can be conducted in the face swapping task too. Let  as the re-target image and the original target image as the re-source image. In the cycle process, we hope to generate results with re-source image's identity and re-target image's attributes, which means it should be the same as the original target image. The cycle loss is a supplement of pixel supervision and can help generate high-fidelity results:
where  means the whole generator of HifiFace.


To capture fine details and further improve the realism, we follow the Learned Perceptual Image Patch Similarity (LPIPS) loss in~\cite{zhang2018unreasonable} and adversarial objective in~\cite{choi2020stargan}.
Thus, our realism loss is formulated as:
where  = ,  = ,  =  and  = .




\begin{figure}[t] 
\begin{center} 
\includegraphics[width=0.95\linewidth]{figures/figure4_final_0530.pdf} 
\end{center} 
\vspace{-2mm}
\caption{Comparison with FSGAN, SimSwap and FaceShifter. Our results can well preserve the source face shape, target attributes and have higher image quality, even when handling occlusion cases.} 
\label{fig:compare_big} 
\end{figure}



\paragraph{Overall Loss.}Our full loss is summarized as follows:





\section{Experiments}


\begin{table}[t]
\centering
\small
\begin{tabular}{cccc|cc}
\toprule
Method & ID & Pose & Shape &MAC~ & FPS      \\
\midrule
FaceSwap    &   &  &  & - & -      \\
FaceShifter     &   &  &  &  &      \\
SimSwap   &   &  &  &  &     \\
\midrule 
Ours-     &   &  &  &  &    \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{Quantitative Experiments on FaceForensics++. FPS is tested under GPU V.}
\label{tab:qua}
\end{table}


\begin{figure}[t] 
\begin{center} 
\includegraphics[width=0.92\linewidth]{figures/compare_aot_df_crop_0530.pdf} 
\end{center} 
\vspace{-2mm}
\caption{(a) Comparison with AOT. (b) Comparison with DF.} 
\label{fig:compare_aot_df} 
\end{figure}




\paragraph{Implementation Details.} 
We choose VGGFace~\cite{cao2018vggface2} and Asian-Celeb~\cite{DeepGlint} as the training set. 
For our model with resolution  (\textit{i.e.}, Ours-), we remove images with either size smaller than  for better image quality. 
For each image, we align the face using  landmarks and crop to ~\cite{li2019faceshifter}, which contains the whole face and some background regions. 
For our more precise model (\textit{i.e.}, Ours-), we adopt a portrait enhancement network~\cite{li2020blind} to improve the resolution of the training images to  as supervision, and also correspondingly add another res-block in  of SFF compared to Ours-. 
The ratio of training pairs with the same identity is \%. ADAM~\cite{kingma2014adam} is used with  = ;  =  and learning rate = . 
The model is trained with K steps, using  V GPUs and  batch size. 



\subsection{Qualitative Comparisons}
First, we compare our method with FSGAN~\cite{Nirkin2019fsgan}, SimSwap~\cite{chen2020simswap} and FaceShifter~\cite{li2019faceshifter} in Figure~\ref{fig:compare_big},  AOT~\cite{zhu2020aot} and DeeperForensics (DF)~\cite{jiang2020deeperforensics} in Figure~\ref{fig:compare_aot_df}. 


As shown in Figure~\ref{fig:compare_big}, FSGAN shares the same face shape with target faces and it can not well transfer the lighting of the target image either.
SimSwap can not well preserve the identity of the source image especially for the face shape because it uses a feature matching loss and focuses more on the attributes. 
FaceShifter exhibits a strong identity preservation ability,
but it has two limitations: (1) Attribute recovery, while our HifiFace can well preserve all the attributes like face color, expression, and occlusion. (2) Complex framework with two stages, while HifiFace presents a more elegant end-to-end framework with even better recovered images. 
As shown in Figure~\ref{fig:compare_aot_df}(a), AOT is specially designed to overcome the lighting problem but is weak in identity similarity and fidelity.  As shown in Figure~\ref{fig:compare_aot_df}(b), DF has reduced the bad cases of style mismatch, but is weak in identity similarity too.
In contrast, our HifiFace not only perfectly preserves the lighting and face style, but also well captures the face shape of the source image and generates high quality swapped faces.
More results can be found in our .




\begin{table}[t]
\centering
\small
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}[-0.2em]{Method} & \multicolumn{2}{c}{FF++} & \multicolumn{2}{c}{DFDC}\\
\cmidrule(l{2pt}r{2pt}){2-3}
\cmidrule(l{2pt}r{2pt}){4-5}
& AUC↓ & AP↓ & AUC↓ & AP↓   \\
\midrule
FaceSwap      &   &  &  &       \\
FaceShifter   &   &  &  &      \\
SimSwap   &   &  &  &     \\
\midrule
Ours-     &   &  &  &      \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{Results in terms of AUC and AP on FF++ and DFDC.}
\label{tab:auc}
\end{table}

\begin{figure}[t] 
\begin{center} 
\includegraphics[width=0.9\linewidth]{figures/compare_shape_final_crop_0530.pdf} 
\end{center} 
\vspace{-2mm}
\caption{Face shape error between  and  in  FF++ pairs with large shape difference. Samples are sorted by shape error of HifiFace. Same column index indicates the same source/target pair.}
\label{compare_shape} 
\end{figure}

\subsection{Quantitative Comparisons}

Next, we conduct quantitative comparison on FaceForensics (FF)++ ~\cite{rossler2019faceforensics++} dataset with respect to the following metrics: ID retrieval, pose error, face shape error, and performance on face forgery detection algorithms to again demonstrate the effectiveness of our HifiFace. For FaceSwap~\cite{FaceSwap} and FaceShifter, we evenly sample  frames from each video and compose a K test set. For SimSwap and our HifiFace, we generate face swapping results with the same source and target pairs above. 

For ID retrieval and pose error, we follow the same setting in ~\cite{li2019faceshifter,chen2020simswap}. 
As shown in Table~\ref{tab:qua}, HifiFace achieves the best ID retrieval score and is comparable with others in pose preservation.
For face shape error, we use another D face reconstruction model~\cite{sanyal2019learning} to regress coefficients of each test face. The error is computed by L distances of identity coefficients between the swapped face and its source face, and our HifiFace achieves the lowest face shape error. 
The parameter and speed comparisons are also shown in Table~\ref{tab:qua}, and our HifiFace is faster than FaceShifter, along with higher generation quality. 

To further illustrate the ability of HifiFace in controlling the face shape, we visualize the samplewise shape differences between HifiFace and FaceShifter~\cite{li2019faceshifter} in Figure~\ref{compare_shape}. 
The results show that, when the source and target differs much in face shape, HifiFace significantly outperforms Faceshifter with \% samples having smaller shape errors.





Besides, we apply the models from
FF++~\cite{rossler2019faceforensics++} and DeepFake Detection Challenge (DFDC)~\cite{dolhansky2019deepfake,dfdc} to examine the realism performance of HifiFace. 
The test set contains K swapped faces and K real faces from FF++ for each method. 
As shown in Table~\ref{tab:auc}, HifiFace achieves the best score, indicating higher fidelity to further help improve face forgery detection.




\begin{figure}[t] 
\begin{center} 
\includegraphics[width=0.9\linewidth]{figures/3d_cropped_0530.pdf} 
\end{center} 
\vspace{-2mm}
\caption{Ablation study for 3D shape-aware identity extractor.} 
\label{ab_3D}
\end{figure}

\begin{figure}[t] 
\begin{center} 
\includegraphics[width=0.9\linewidth]{figures/sff_cropped_0530.pdf} 
\end{center} 
\vspace{-2mm}
\caption{Ablation study for SFF module.} 
\label{ab_SFF} 
\end{figure}

\begin{figure}[t] 
\begin{center} 
\includegraphics[width=0.96\linewidth]{figures/ab_face_crop_0530.pdf} 
\end{center} 
\vspace{-2mm}
\caption{Comparison with results using directly mask blending. `Blend-T', `Blend-DT', and `Blend-R' mean blending bare results to the target image by the mask of target, the dilated mask of target and the mask of bare results, respectively. } 
\label{ab_face} 
\end{figure}

\begin{figure}[t] 
\begin{center} 
\includegraphics[width=0.96\linewidth]{figures/ab_heat_crop_0530.pdf} 
\end{center} 
\vspace{-2mm}
\caption{Difference feature maps of SFF.}\label{ab_heatmap} 
\end{figure}


\subsection{Analysis of HifiFace}

\paragraph{3D Shape-Aware Identity.}
To verify the effectiveness of shape supervision  on face shape, we train another model Ours-nd, which replaces the shape-aware identity vector with the normal identity vector from . 
As shown in Figure~\ref{ab_3D}, the results of Ours-nd can hardly change the face shape or have obvious artifacts, while the results of Ours- can generate results with much more similar face shape.


\paragraph{Semantic Facial Fusion.}
To verify the necessity of the SFF module, we compare with  baseline models: 
() `Bare' that removes both the feature-level and image-level fusion. 
() `Blend' that removes the feature-level fusion. 
() `Concat' that replaces the feature-level fusion with a concatenate. As shown in Figure~\ref{ab_SFF}, `Bare' can not well preserve the background and occlusion, `Blend' lacks legibility, and 'Concat' is weak in identity similarity, which proves that the SFF module can help preserve the attribute and improve the image quality without harming the identity. 


\paragraph{Face Shape Preservation in Face Swapping.} 
Face shape preservation is quite difficult for face swapping, which is not just because of the difficulty in getting shape information, but also the challenge of inpainting when the face shape has changed. Blending is a valid way to preserve occlusion and background, but it is hard to be applied when the face shape changes. 
As shown in Figure~\ref{ab_face}, when the source face is fatter than the target face (row ), it may limit the change of face shape in Blend-T. If we use Blend-DT or Blend-R, it can not well handle the occlusion. 
When the source face is thinner than the target (row ), it is easy to bring artifacts around the face in Blend-T and Blend-DT and may cause a double face in Blend-R. 
In contrast, our HifiFace can apply the blending without the above issue, because our SFF module has the ability to inpaint the edge of the predicted mask. 

To further illustrate how SFF addresses the problem, we show difference feature maps of every stage in the SFF module, named SFF-, between the input of (,) and (,), where (,) obtains Ours- and (,) achieves target itself. 
In Figure~\ref{ab_heatmap}, the bright area means where the face shape changes or contains artifacts. SFF module recombines the feature between the face region and non-face area and focuses more on the contour of the predicted mask, which brings great benefits for inpainting areas where the shape changes.



\section{Conclusions}
In this work, we propose a high fidelity face swapping method, named HifiFace, which can well preserve the face shape of the source face and generate photo-realistic results. A 3D shape-aware identity extractor is proposed to help preserve identity including face shape. An SFF module is proposed to achieve a better combination in feature-level and image-level for realistic image generation. Extensive experiments demonstrate that our method can generate higher fidelity results than previous SOTA face swapping methods both quantitatively and qualitatively. Last but not least, HifiFace can also be served as a sharp spear, which contributes to the development of the face forgery detection community.







\small 
\bibliographystyle{named}
\bibliography{ijcai21}



\normalsize


\begin{figure*}[t] 
\begin{center} 
\includegraphics[width=0.95\linewidth]{figures/supp_sid_crop.pdf} 
\end{center} 
\caption{Interpolated results with different compositions of SID.} 
\label{supp_sid} 
\end{figure*}



\section*{Network Structures} 
Detailed structures of our HifiFace are given in Figure~\ref{arch}. For all residual units, we use the Leaky ReLU (LReLU) as the activation function. Resample means the Average Pooling or the Upsampling, which is used to change the size of feature maps. Res-Blocks with the Instance
Normalization (IN) are used in encoder, while  Res-Blocks with the Adaptive Instance Normalization (AdaIN) are used in decoder.


\section*{More Results} 

To analyse the specific impacts of the shape information from 3D face reconstruction model and the identity information from face recognition model, we adjust the composition of SID to generate interpolated results. It is formulated as:
where ,  and  means the D identity coefficients of source, target and interpolated image, ,  and  means source, target and interpolated image's identity vector from recognition model. 

As we can see in Figure~\ref{supp_sid} rows , we first fix  and , the face shape can still change but lake of identity detail. Then in rows , we fix  and , and the identity becomes more similar. The results prove that the shape information control the basic of shape and identity, while the identity vector is helpful to the identity texture.



In the end, we download lots of wild face images from Internet
and generate more face swapping results in Figure~\ref{wild_1} and Figure~\ref{wild_2} to demonstrate the strong capability of our methods. And more results can be found at{~\color{magenta} ~\url{https://johann.wang/HifiFace}}. 



\begin{figure}[t] 
\begin{center} 
\includegraphics[width=0.96\linewidth]{figures/structure_crop.pdf} 
\end{center} 
\caption{Architectural details of HifiFace.}
\label{arch} 
\end{figure}



\newpage

\begin{figure*}[t] 
\begin{center} 
\includegraphics[width=0.95\linewidth]{figures/supp_crop.pdf} 
\end{center} 
\caption{More results on high resolution wild faces. 
The face in the target image is replaced by the face in the source image.}
\label{wild_1} 
\end{figure*}


\newpage
\begin{figure*}[t] 
\hsize=\textwidth
\centering
\subfigure{
    \includegraphics[width=0.6\textwidth]{figures/wild_2_01.pdf}    
}

\subfigure{
    \includegraphics[width=0.6\textwidth]{figures/wild_2_02.pdf}    
}

\subfigure{
    \includegraphics[width=0.6\textwidth]{figures/wild_2_03.pdf}    
}

\caption{Some results on high quality real world photos. 
The face in the target image is replaced by the face in the source image. This is to show that the faces generated by our method can be very naturally integrated into high-resolution shot real world scenes.} 
\label{wild_2}
\end{figure*}



\end{document}
