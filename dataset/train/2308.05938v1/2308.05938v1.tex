\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{multirow}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}


\usepackage{hyperref}	

\begin{document}

\title{FoodSAM: Any Food Segmentation}
\author{Xing Lan, Jiayi Lyu, Hanyu Jiang, Kun Dong, Zehai Niu, Yi Zhang, Jian Xue

\textit{School of Engineering Science, University of Chinese Academy of Sciences}

{\tt \small \{lanxing19, lyujiayi21, jianghanyu23, dongkun22, niuzehai18, zhangyi214\}@mails.ucas.ac.cn}

{\tt \small xuejian@ucas.ac.cn}



\thanks{
This work was supported by the National Natural Science Foundation of China (62032022, 61929104, 62027827, 61972375) and Scientific Research Program of Beijing Municipal Education Commission (KZ201911417048).
\emph{(Corresponding authors: Jian Xue.)}}}



\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\M_aM_sM_dI\in R^{H\times W}M_sm_s = M_s(I)IM_aIm_a = M_a(I), m_a\in R^{K\times H \times W}Km_a^i D_i =  m_s[m_a^i]s_iD_im_a^id_i = \text{Count}(S_i, D_i) / \text{Len}(D_i)d_is_is_iD_iD_id_i\taum_s^eKm_a^is_i\text{Len}(D_i)m_a^ts_tm_i^ttm_iTm_i^tM_dM_dIB_dC_dKm_aB_dm_pm_iM_dK\mathrm{TP}_i\mathrm{FP}_i\mathrm{FN}_i\bullet\mathrm{TP}_ii\bullet\mathrm{FP}_ii\bullet\mathrm{FN}_ii\rightarrow\rightarrow\rightarrow\rightarrow\tau$ equals zero, it means not using this practice.
The experiments on FoodSeg103 in shown in Tab.\ref{tab:ablation_thres} and UECFoodPix Complete shownn in \ref{tab:ablation_uec_thres}. 
Using the area of the original semantic mask to represent the confused mask can make a minor improvement in FoodSeg103, and a significant increase in UECFoodPix Complete. 
The reason behind this is that FoodSeg103 is the dataset for fine-grained ingredients, while UECFoodPix Complete only contains the food label, its label is coarse-grained. 
Therefore, when the number of confused labels is larger, our method achieves higher improvement. 


\begin{table}
\centering
\caption{Ablation Study on the Confused threshold of mask category on Uecfoodpix Complete.}
\begin{tabular}{c|ccc}
\hline
\textbf{Confused threshold}             & \textbf{mIoU(\%)} & \textbf{mAcc(\%)} & \textbf{aAcc(\%)} \\ \hline
0                        & 65.61      & 77.56       & 88.20                             \\
0.3                      & 65.78      & 78.20       & 87.81                             \\ 
0.5                      & 65.81      &  78.21      & 87.85                              \\
0.7                      & 66.06      & 78.15       & 88.27                             \\
0.8                      & 66.14      & 78.01       & 88.47                             \\
0.9                      & 66.08      & 77.96       & 88.45                             \\ \hline
\end{tabular}
\captionsetup{justification=centering}
\label{tab:ablation_uec_thres}
\end{table}
 
\begin{figure*}[tbh]
\centering
\newpage
\includegraphics[width=\linewidth]{pdf/prompt_v2_crop.pdf}
\caption{Visualization results on promptable segmentation. From left to right: input, double point prompts, double box prompts}
\label{fig: prompt vis}
\end{figure*}
     
\section{Conclusion}
This paper investigates the zero-shot capability of SAM for food image segmentation, a challenging task in the domain of food computing. 
The vanilla mask generation method of SAM alone falls short in capturing class-specific information, hindering accurate food item categorization. 
To address this limitation, we propose FoodSAM, a novel zero-shot framework that combines original semantic masks with SAM-generated category-agnostic masks to enhance semantic segmentation quality. 
Additionally, we leverage SAM's inherent instance-based masks to perform instance segmentation on food images. 
FoodSAM also incorporates object detection methodologies to detect non-food objects, allowing for panoptic segmentation. 
Furthermore, we extend our investigation to promptable segmentation, supporting various prompt variants. 
Our comprehensive evaluation on benchmark datasets demonstrates FoodSAM's state-of-the-art performance, 
affirming SAM's potential as an influential tool for food image segmentation. 
This work encompasses the first exploration of SAM in food segmentation, accomplishing instance, panoptic, and promotable segmentation on food images, and surpassing existing methods in performance.
 
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, foodsam}



\end{document}
