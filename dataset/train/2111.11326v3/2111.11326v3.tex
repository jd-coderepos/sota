\documentclass[rebuttal]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{color}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{2527} 

\def\cvprPaperID{2527} \def\confYear{CVPR 2022}

\newcommand{\rone}{\textcolor{red}{\textbf{R\#1}}\xspace}
\newcommand{\rtwo}{\textcolor{blue}{\textbf{R\#2}}\xspace}
\newcommand{\rthree}{\textcolor{green}{\textbf{R\#3}}\xspace}
\newcommand{\alex}[1]{\textcolor{red}{[#1]}}
\newcommand{\alexd}[2]{\textcolor{blue}{[#1]}\textcolor{red}{[#2]}}
\newcommand{\matt}[1]{\textcolor{magenta}{[#1]}}

 \setlength{\parindent}{0pt} 

\begin{document}

\title{DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion}  

\maketitle
\thispagestyle{empty}


\noindent We wish to thank the reviewers for their feedback. They all acknowledged the novelty, effectiveness, and soundness of our DyTox strategy for Continual Learning.




\textcolor{brown}{\rone\textbf{Q1}: Discussion of the related work}. We thank \rone for the 3 references  pointed out. We will add them to our paper with a discussion on conditioning mechanism. 
In particular, we will detail how DyTox differs from the Perez et al.'s FiLM paper [R2]. Indeed, FiLM proposes a VQA architecture with an affine modulation of the visual backbone features using the question encoding. DyTox is somehow conditioning the other way around, modifying a task token query using the visual features, and of course our strategy departs from affine modulation by using an Attention Block (TAB). If accepted, we will discuss how this transformer-based attention is positioned in relation to FiLM and follow-up papers like [Murel CVPR19] proposing VQA advanced modulation mechanisms.  





\rone\textcolor{brown}{\textbf{Q2}: DyTox with a convolution backbone?} During initial experiments we also tried using a ResNet in place of our transformer-based encoder. It performed better than all baselines but DER, and lower than our DyTox.
We'll add this existing experiment in the paper, that we have initially omitted to focus on our end-to-end transformer framework; we could also compare other ConvNet-based backbones. Other distantly related conditioners --- in particular a FiLM-like strategy --- could also be compared, but we stress that our token-based conditioning is the most suitable and simple to include in our transformer framework.





\rone\textcolor{brown}{\textbf{Q3}: more information about the inference time?} In L776-L784, we display the time overhead (roughly the same between training and testing) for DyTox: +2.24\% slower per task. In contrast, DER's speed without pruning grows linearly w.r.t. the number of tasks: \textit{e.g.}, with 50 tasks, 50 forwards are done. We couldn't compare to DER's speed with pruning because their pruning code isn't released, and there is no information available about it.
Moreover, our transformer architecture has a base speed comparable to a ResNet18 (the backbone used by all baselines): 50.3ms \textit{v.s.} 51.4ms for a forward of 32 RGB images of .

\rone\textcolor{brown}{\textbf{Q4}: boosting performance with MixUp?} Our main experiments are done with DyTox without MixUp but still reach SotA peformance on ImageNet (Table 2) and very close to DER on CIFAR100 (Table 3). DyTox+, with MixUp, was added as an additional experiment to showcase a new continual training procedure, particularly efficient with transformers. We tried MixUp with ResNet: it also reduced forgetting, albeit to a less degree than in DyTox and more generally in transformers. We'll this detailled comparison in the final paper.


\rone {\textcolor{brown}{'s discussion about how the method performs in more realistic scenarios?}} As \rone remarked, the evaluated benchmarks of this paper (ImageNet1000 and CIFAR100, standard in the literature) can be distant from more realistic contexts. With non-mutually exclusive tasks, new data from old classes would appear: in that case, this data would be treated as if they were rehearsal data from previous tasks. We thank \rone for suggesting more realistic applications: unbalanced distribution handling could be integrated in DyTox by drawing inspiration from [Learning to Segment the Tail CVPR20], and overlapped tasks using real data with the CORE50 scenarios [CoRL 2017]. We will be pleased to extend our limitation subsection (L851-855) if accepted.








\rtwo \textcolor{brown}{\textbf{Q1}: which rehearsal is used?}
We thank \rtwo for this question about our rehearsal protocol, that was shortly described in L533-535 but will be clarified/lengthened in the final version. We use the rehearsal protocol of iCaRL [47] which is the standard method used by all baselines in this paper, including DER [62].
Namely, we gather rehearsal samples from current classes using iCaRL's herding (see Algo 4 of [47]). Rehearsal samples are then added to the new task data without any oversampling.




\rthree raised 3 "minor issues":\\
- \rthree \textcolor{brown}{\textbf{i1}: including more discussion on our motivation} We will further expand our current motivation (L108-115) in the final paper: practically, we aimed to have a model with limited memory (\#1) and time overhead (\#2) in order to run easily in production even when faced to dozen of consecutive tasks. Moreover, it should perform well without hyperparameter tuning to be robust and easily applicable in a wide variety of applications (\#3). We choose to develop a dynamic extendable network in order to minimize forgetting by using task-specific components. This type of method recently showed the best continual performance [40, 62]. We adapted it to then satisfy our goals: our decoder has sparse attention with linear complexity w.r.t. the number of patches (\#2). The expansion of our dynamic framework is limited by the addition of small learned vectors (\#1). This small memory overhead eliminates the need of hyperparameter-sensitive pruning [62] (\#3).






- \rthree \textcolor{brown}{\textbf{i2}: balancing the performance among different step settings?} Our model performs always close or better than DER and significantly better than all other baselines. Moreover, DER uses different sets of hyperparameters for their careful pruning while our hyperparameters are exactly the same regardless of the number of steps: our hyperparameter selection was done on CIFAR100 with 50 steps (L81-L91 and Table 2 in the Appendix). Better performance on 10 steps could be reached if we had tuned explicitly for this benchmark, but it'd defeat the purpose of having robust hyperparameters (motivation \#3). 

- \rthree \textcolor{brown}{\textbf{i3}: typos and grammar issues} We'll do our best to fix the typos and grammar issues in the final version.


\end{document}
