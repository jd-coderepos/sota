\documentclass{LMCS}

\def\dOi{10(3:21)2014}
\lmcsheading {\dOi}
{1--43}
{}
{}
{Dec.~\phantom05, 2013}
{Sep.~12, 2014}
{}

\ACMCCS{[{\bf Theory of computation}]: Semantics and
  reasoning---Program reasoning---Program analysis}

\usepackage{mathtools,amssymb}
\usepackage[
  pdfauthor={Matija Pretnar},
  pdftitle={Inferring Algebraic Effects}
]{hyperref}
\usepackage{listings}
\usepackage{mathpartir}
\usepackage{booktabs}
\usepackage[only, llbracket, rrbracket]{stmaryrd}
\usepackage{xspace,hyperref}
\makeatletter
\providecommand*{\cupdot}{\mathbin{\mathpalette\@cupdot{}}}
\newcommand*{\@cupdot}[2]{\ooalign{\cr
    \sbox0{}\dimen@=\ht0 \sbox0{}\advance\dimen@ by -\ht0 \dimen@=.5\dimen@
    \hidewidth\raise\dimen@\box0\hidewidth
  }}

\DeclareFontFamily{U}{mathb}{\hyphenchar\font45}
\DeclareFontShape{U}{mathb}{m}{n}{
      <5> <6> <7> <8> <9> <10> gen * mathb
      <10.95> mathb10 <12> <14.4> <17.28> <20.74> <24.88> mathb12
      }{}
\DeclareSymbolFont{mathb}{U}{mathb}{m}{n}
\DeclareMathSymbol{\dotminus}{2}{mathb}{"01}

\providecommand*{\bigcupdot}{\mathop{\vphantom{\bigcup}\mathpalette\@bigcupdot{}}}
\newcommand*{\@bigcupdot}[2]{\ooalign{\cr
    \sbox0{}\dimen@=\ht0 \advance\dimen@ by -\dp0 \sbox0{\scalebox{2}{}}\advance\dimen@ by -\ht0 \dimen@=.5\dimen@
    \hidewidth\raise\dimen@\box0\hidewidth
  }}
\makeatother


\newcommand{\bnfis}{\mathrel{\;{:}{:}\!=}\;}
\newcommand{\bnfor}{\mathrel{\;\big|\;}}
\newcommand{\defeq}{\mathrel{\;\stackrel{\text{def}}{=}\;}}
\newcommand{\fra}[1]{\forall #1 .\ }
\newcommand{\exs}[1]{\exists #1 .\ }
\newcommand{\rulename}[1]{{\mdseries \small \textsc{#1}}}
\newcommand{\set}[1]{\{ #1 \}}
\newcommand{\step}{\leadsto}

\newcommand{\type}[1]{\mathtt{#1}}
\newcommand{\boolty}{\type{bool}}
\newcommand{\natty}{\type{nat}}
\newcommand{\unitty}{\type{unit}}
\newcommand{\emptyty}{\type{empty}}
\newcommand{\listty}{\;\type{list}}
\newcommand{\stringty}{\type{string}}
\renewcommand{\to}[1][]{
  \def\arg{#1}\ifx\arg\emptyarg\rightarrow\else\xrightarrow{#1}\fi }
\newcommand{\hto}[1][]{
  \def\arg{#1}\ifx\arg\emptyarg\Rightarrow\else\xRightarrow{#1}\fi }
\newcommand{\C}{\underline{C}}
\newcommand{\D}{\underline{D}}
\newcommand{\Drt}{\Delta}
\newcommand{\drt}{\delta}
\newcommand{\Rgn}{R}
\newcommand{\wild}{\top}
\makeatletter
\newcommand{\rgn}{\@ifstar{\dot{\rho}}{\rho}}
\makeatother
\newcommand{\prs}{\psi}
\newcommand{\uniq}[2]{\bigcupdot\nolimits_{#1} #2}
\newcommand{\varuniq}[2]{\bigcupdot_{#1} #2}

\newcommand{\kord}[1]{\mathtt{#1}}
\newcommand{\kop}[1]{\;\mathtt{#1}\;}
\newcommand{\kpre}[1]{\mathtt{#1}\;}
\newcommand{\kpost}[1]{\;\mathtt{#1}}

\newcommand{\absurd}{\kpre{absurd}}
\newcommand{\case}{\mathop{\text{\texttt{|}}}}
\newcommand{\cont}[2]{(#1.\,#2)}
\newcommand{\fls}{\kord{false}}
\newcommand{\fun}[1]{\kpre{fun} #1 \mapsto}
\newcommand{\handler}{\kpre{handler}}
\newcommand{\hash}[2]{#1\text{\texttt{\char35}}#2}
\newcommand{\iszero}{\kpre{iszero}}
\newcommand{\letin}[1]{\kpre{let} #1 \kop{in}}
\newcommand{\letrecin}[1]{\kpre{let} \kpre{rec} #1 \kop{in}}
\newcommand{\letvalin}[1]{\kpre{let} \kpre{val} #1 \kop{in}}
\newcommand{\call}[4]{{\hash{#1}{#2}\,{#3}\, #4}}
\newcommand{\op}{\mathtt{op}}
\newcommand{\inst}{\mathtt{ins}}
\newcommand{\pred}{\kpre{pred}}
\newcommand{\tru}{\kord{true}}
\newcommand{\unt}{\kord{\text{\texttt{()}}}}
\newcommand{\val}{\kpre{val}}
\newcommand{\withhandle}[2]{\kpre{with} #1 \kop{handle} #2}
\renewcommand{\ifthenelse}[3]{\kpre{if} #1 \kop{then} #2 \kop{else} #3}
\renewcommand{\succ}{\kpre{succ}}

\newcommand{\goesto}{\leadsto}
\newcommand{\ctx}{\Gamma}
\newcommand{\pctx}{\Xi}
\newcommand{\ent}[1][]{\vdash_{#1}}
\newcommand{\T}{\mathrel{:}}
\newcommand{\E}{\mathrel{!}}
\newcommand{\while}{\mid}
\renewcommand{\le}{\leqslant}

\newcommand{\cstr}{\mathcal{C}}
\newcommand{\queue}{\mathcal{Q}}
\newcommand{\prms}{\Psi}
\newcommand{\ops}{\mathcal{O}}
\newcommand{\insts}{\mathcal{I}}
\newcommand{\types}[2][A]{\llbracket #1 \mid #2 \rrbracket}
\newcommand{\sol}{\sigma}
\newcommand{\sig}{\Sigma}
\newcommand{\extend}[2]{#1 \cup \set{#2}}

\newcommand{\Eff}{\emph{Eff}\xspace}
\newcommand{\unify}{\kord{unify}}
\newcommand{\pos}{\kord{pos}}
\renewcommand{\neg}{\kord{neg}}
\newcommand{\gc}[1][P, N]{\kord{gc}_{#1}}
 \lstset{language=Caml,
moredelim=*[is][\itshape]{/@}{@/},
numbers=none,mathescape=true,showstringspaces=false,
morekeywords={handle,handler,with,new,effect,operation,type,end,val,finally,match,true,false},
keywordstyle=\relax,
xleftmargin=1em,basicstyle=\ttfamily}
\lstnewenvironment{source}{\lstset{
basicstyle=\ttfamily\small,
}}{}
\let\inline\lstinline

\begin{document}

\title{Inferring Algebraic Effects}

\author{Matija Pretnar}
\address{Faculty of Mathematics and Physics, University of Ljubljana, Slovenia}
\email{matija@pretnar.info}

\keywords{algebraic effects, effect handlers, effect inference, effect system}
\subjclass{F.3.2}

\begin{abstract}
  \noindent
  We present a complete polymorphic effect inference algorithm
  for an ML-style language with handlers of not only exceptions,
  but of any other algebraic effect such as
  input \& output, mutable references and many others.

  Our main aim is to offer the programmer a useful insight into the effectful behaviour of programs.
  Handlers help here by cutting down possible effects
  and the resulting lengthy output that often plagues precise effect systems.
  Additionally, we present a set of methods
  that further simplify the displayed types,
  some even by deliberately hiding inferred information from the programmer.
\end{abstract}

\maketitle

\noindent
Though Haskell~\cite{jones2003haskell} fans may not think it is better to write impure programs in ML~\cite{milner1997definition},
they do agree it is easier.
You can insert a harmless printout without rewriting the rest of the program,
and you can combine multiple effects without a monad transformer.
This flexibility comes at a cost, though ---
ML types offer no insight into what effects may happen.
The suggested solution is to use an effect system~\cite{lucassen1988polymorphic,talpin1992type,benton1998compiling,tolmach1998optimizing,wadler1999marriage,benton2002monads,rytz2012lightweight},
which enriches existing types with information about effects.

An effect system can play two roles:
it can be \emph{descriptive} and inform about potential effects,
and it can be \emph{prescriptive} and limit the allowed ones.
In this paper, we focus on the former.
It turns out that striking a balance between expressiveness and simplicity of a descriptive effect system is hard.
One of the bigger problems is that effects tend to pile up,
and if the effect system takes them all into account,
we are often left with a lengthy output listing every single effect there is.

In this paper, we present a complete inference algorithm
for an expressive and simple descriptive polymorphic effect system of \Eff~\cite{bauer2012programming}
(freely available at \url{http://eff-lang.org}),
an ML-style language with handlers of not only exceptions, but of any other \emph{algebraic effect}~\cite{plotkin2003algebraic}
such as input \& output, non-determinism, mutable references and many others~\cite{plotkin2009handlers, bauer2012programming}.
Handlers prove to be extremely versatile and can express
  stream redirection,
  transactional memory,
  backtracking,
  cooperative multi-threading,
  delimited continuations,
and, like monads, give programmers a way to define their own.
And as handlers eliminate effects, they make the effect system \emph{non-monotone},
which helps with the above issue of a snowballing output.

We start in Section~\ref{sec:eff} with an tour of \Eff in which we informally explain handlers and show the main features of the proposed effect system.
Afterwards, we switch to formal development and in Section~\ref{sec:core-eff},
we recap the effect system for \emph{core Eff}~\cite{bauer2013effect},
a minimal formalization of \Eff.
Our contributions are:
\begin{itemize}
\item
  A set of syntax-directed rules for inferring types and constraints they must satisfy (Section~\ref{sec:inferring}).
  We show that these rules are sound and complete with regard to the effect system.
\item
  A unification algorithm that decomposes a set of constraints to a more basic form
  and decides if it admits a solution (Section~\ref{sec:unifying}).
  Unification fails only in the case of type mismatch,
  which fits our goal of a descriptive effect system
  that just refines existing ML types with details about effects.
\item
  A number of techniques that reduce the number of constraints without changing the set of solutions (Section~\ref{sec:simplifying}).
  The heavy lifting is done by \emph{garbage collection} of constraints~\cite{pottier2001simplifying, simonet2003type, trifonov1996subtyping},
  which we borrow and adapt slightly to our purpose.
  We also introduce a few more techniques particular to the algebraic setting.
\item
  A further collection of tactics for simplifying the display of inferred types (Section~\ref{sec:displaying}).
  To fully achieve their purpose,
  some of the presented tactics deliberately hide information from the programmer,
  though entire information is always used in the background.
\end{itemize}
We conclude by showing a couple of full runs of the algorithm (Section~\ref{sec:examples}) and by discussing related and future work.
To preserve the flow of the paper, we gather the (mostly routine) proofs in Appendix~\ref{app:proofs}.

\section{\Eff}
\label{sec:eff}

Effects aside, \Eff should be familiar to anyone that has worked with OCaml~\cite{ocaml}.
For example, the \inline{map} function,
which applies a function \inline{f} to each element of the list \inline{xs}
is defined as:
\begin{source}
  let rec map f xs =
    match xs with
    | [] -> []
    | x :: xs -> f x :: map f xs
\end{source}
The first important feature that distinguishes \Eff from OCaml is its effect system.
For example, the inferred type of \inline{map} is 

Here, the annotation on the arrow, called the \emph{dirt},
describes any effects that the function may trigger.
This additional information is very easy to understand:
the function \inline{map f} causes exactly the same effects~ as \inline{f}.
The lack of dirt on the second arrow signifies that the application of \inline{map} to \inline{f} is pure.
Inferred types of some other typical higher-order functions are:

The dirt annotations are similar to ones usually given in existing polymorphic effect systems such as~\cite{leroy2000type}.
Also note that if we disable the display of annotations, \Eff shows the programmer exactly the same types as OCaml.

The second distinguishing feature is that \Eff is based on algebraic effects~\cite{plotkin2001adequacy, plotkin2003algebraic}.
This means that effects are accessed exclusively through a set of \emph{operations},
which are all of the form ,
  where an \emph{operation symbol}~ describes the action,
  and an \emph{instance}~ describes where it should happen.
This means that we print to the standard output channel
using an operation \inline{std#print} instead of a primitive function \inline{print_string},
we access a reference \inline{r}
through operations \inline{r#lookup} and \inline{r#update} instead of through \inline{!} and \inline{:=},
and we raise an exception \inline{exc}
by calling \inline{exc#raise} instead of using a special keyword \inline{raise}.
The latter constructs are, of course, all definable in terms of the former.

This uniform representation allows effects to be seamlessly combined~\cite{hyland2006combining},
forms a natural basis for an effect system ---
possible effects of a given computation are captured accurately by the set of operations it calls ---
and paves the way for the third distinguishing feature of \Eff: handlers of arbitrary algebraic effects~\cite{plotkin2009handlers}.

Let us take a look at a few concrete examples.
What follows is by no means an exhaustive list of what can be accomplished with handlers.
For more examples, please see~\cite{plotkin2009handlers,bauer2012programming,kammar2013handlers}.


\subsection{Exceptions}

We start with exceptions as they are the simplest algebraic effect and
as exception handlers are already a well established concept.


\subsubsection{Effect types}

Instances are first-class values in \Eff and are given an \emph{effect type}.
For exception instances, called simply \emph{exceptions},
the effect type is of the form ,
where  is the type of any additional information that the exception may carry.

We declare each effect type together with an \emph{effect signature} that lists
available operation symbols together with the types of parameters they accept
and of results they yield to the waiting continuation.
The signature corresponding to  is

where the result type of \inline{raise} is  (a sum type with zero constructors)
because raising an exception terminates the computation and yields nothing to the continuation.

Each term of an effect type is also annotated with a \emph{region},
which describes (an over-approximation of) the set of instances it may occupy.
For example, an exception \inline{exc1} is given
the type ,
while the conditional \inline{if cond then exc1 else exc2} is given
the type 
(any bigger sets would also be fine).


\subsubsection{Raising exceptions}
\label{ssub:raising-exceptions}

Since there is little we can do with a result of the empty type,
we rarely raise exceptions directly with \inline{exc#raise}.
Instead, we define a convenience function, also named \inline{raise},
which places the exception call inside the empty type eliminator:
\begin{source}
  let raise exc arg =
    match (exc#raise arg) with
\end{source}
with the inferred type

So, \inline{raise} takes an exception from a region captured by a region parameter~
and a matching argument of type .
The second application results in a computation that can raise any exception from ,
while its return type~ can be any arbitrary.
This allows us to use \inline{raise} at any point in the computation,
for example in computing the tail of a list:
\begin{source}
  let tail xs =
    match xs with
    | [] -> raise emptyListTail
    | _ :: xs -> xs
\end{source}
where \inline{emptyListTail} is the exception stating that the empty list has no tail.
The inferred type of \inline{tail} is

where the dirt shows that \inline{emptyListTail} may be raised during execution.

In addition to inferring types of values, we can also infer \emph{dirty types~} of computations.
These consist of a value type~ describing the result
and of a dirt~ describing the operations that may be called while computing it.
For example, the inferred dirty type of the computation \inline{tail [1; 2; 3]} is .
Note that the effect system is conservative and signals a possible exception even though it will not be raised during runtime.
This also means that a computation is guaranteed to be pure whenever its inferred dirt is empty.

For a final example of the effect system before we turn to handlers,
let us combine \inline{map} and \inline{tail} as
\begin{source}
  let map_tail f xs = map f (tail xs)
\end{source}
with the inferred type

Unlike the argument to \inline{map},
the argument of \inline{map_tail} has its dirt split into two parts:
the region parameter  captures the region of all the exceptions that \inline{f} may raise,
while the dirt parameter  captures all other operations.
The split allows us to express the fact that \inline{map_tail f} may raise either \inline{emptyListTail} or an exception from ,
and that it may call any operation from  that \inline{f} can.
This is similar to \emph{row typing}~\cite{remy1993type},
where we use a parameter to capture all the fields of a record that are not explicitly mentioned.


\subsubsection{Handling exceptions}
\label{ssub:handling-exceptions}

As exceptions are the simplest example of effects,
exception handlers are the simplest example of handlers.
A \inline{try with} handling construct known from OCaml, like the one in
\begin{source}
  let print_ratio x y =
    try
      print_string ("Ratio is " ^ string_of_float (x /. y))
    with
    | Division_by_zero -> print_string "Ratio does not exist!"
\end{source}
would be used in \Eff as
\begin{source}
  let print_ratio x y =
    handle
      std#print ("Ratio is " ^ string_of_float (x /. y))
    with
    | divisionByZero#raise _ _ -> std#print "Ratio does not exist!"
\end{source}
Each handler case takes two arguments which we ignore for now.

Like instances, handlers are first-class values in \Eff,
and the above is just special syntax for
\begin{source}
  let print_ratio x y =
    with h handle
      std#print ("Ratio is " ^ string_of_float (x /. y))
\end{source}
where \inline{h} is given by
\begin{source}
  handler
  | divisionByZero#raise _ _ -> std#print "Ratio does not exist!"
\end{source}
Handlers are given \emph{handler types~},
meaning that a handler takes a computation with \emph{incoming} dirty type 
and transforms it into a computation with \emph{outgoing} dirty type .
The inferred type for \inline{h} is

We see that \inline{h} leaves the type of results to be .
Its incoming dirt is split into three parts:
exceptions  that we may raise,
channels  that we may print to,
and any other operations~ different from \inline{raise} or \inline{print} that we may call.
The outgoing dirt is similarly split:
the handled computation may still raise exceptions from ,
except that now \inline{divisionByZero} will be handled.
Next, the handled computation may print to \inline{std} in addition to any channel in .
Finally, any operation in  will be neither caught nor called by \inline{h},
so this part remains as it is.

Handler types usually (but not always) all have the same shape:
they remove certain operations,
possibly add some of their own,
and pass through any unhandled dirt.
This results in a repetitive type, which can be written in a more compact form
that emphasises only the differences.
For \inline{h}, this is:

So, \inline{h} removes \inline{divisionByZero#raise}, adds \inline{std#print}, and leaves the rest as it is.

In practice, exception handlers are rarely reused,
because treatment of exceptions varies greatly
depending on the context in which they are handled.
An example of a more general exception handler is \inline{optionalize},
which transforms a computation into one that yields an optional result,
depending on if a given exception~\inline{exc} was raised or not.
We define \inline{optionalize} as:
\begin{source}
  let optionalize exc =
    handler
    | exc#raise _ _ -> None
    | val x -> Some x
\end{source}
The first thing to notice is the special \inline{val} case,
which determines what to do when the handled computation returns a value.
In our case, we wrap it with the \inline{Some} constructor of the  type.
Then, if we define
\begin{source}
  let tail_opt xs =
    with (optionalize emptyListTail) handle (tail xs)
\end{source}
the call \inline{tail_opt [1; 2; 3]} evaluates to \inline{Some [2; 3]},
while \inline{tail_opt []} evaluates to \inline{None}.
The inferred type of \inline{tail_opt} is the pure ,
while the type of \inline{optionalize} is

So, we change the result type from  to ,
and remove any call of \inline{exc#raise} for the exception \inline{exc} determined by the region~.

\subsubsection{Handled regions}
\label{ssub:handled-regions}

You may have observed that we wrote  instead of  in the type of \inline{optionalize}.
This is meant to point out that we may remove  from the dirt only if it denotes a singleton.
The reason for this is as follows.

Take exceptions \inline{exc1} and \inline{exc2} and define a handler \inline{h} as:
\begin{source}
  let exc = if cond then exc1 else exc2 in
  let h = 
    handler
    | exc#raise _ _ -> ...
\end{source}
So, does the \inline{exc#raise} case of \inline{h} handle \inline{exc1#raise} or \inline{exc2#raise} at runtime?
Unfortunately, just by looking at the type
,
we cannot say anything,
so we must assume that both are unhandled.
The only way we can be sure during type-checking that a handling case removes a given operation is
when the region of its instance is a singleton.

For this reason, we write  whenever the region of the handled instance is still some parameter~.
If this eventually turns out to denote some singleton , we may safely replace it by .
But if it turns to be a bigger set, we need to drop it from the handler type.


\subsection{Input \& output}

Interactive input \& output is also a very simple algebraic effect,
yet its handlers expose almost all the important aspects of general ones.
For input \& output, the effect instances are called \emph{channels}
and have the effect type  with the signature

A simple output handler is one that reverses the order of printouts:
\begin{source}
  handler
  | std#print msg k -> k (); std#print msg
\end{source}
The \inline{std#print} case takes two parameters:
  the string \inline{msg} given to \inline{std#print} and
  the continuation \inline{k} waiting for its result.
We first resume the continuation by passing it the unit value \inline{()},
and only after that finishes, we print out \inline{msg}.
The handler recursively handles any other \inline{std#print} that the continuation may call,
so the order of printouts in the continuation is reversed as well.
Note, however, that \inline{std#print} on the right-hand side is outside the scope of the handler
and remains unhandled (unless there are more handlers nested outside).

A more interesting example that can be useful in unit testing is a handler that
collects all printouts to a list of strings and returns it together with the result:
\begin{source}
  handler
  | val x -> (x, [])
  | std#print msg k ->
      let (x, msgs) = k () in
      (x, msg :: msgs)
\end{source}
So, a computation that just returns the value \inline{x} prints nothing,
and we return an empty list \inline{[]} together with \inline{x}.
If, however, the computation prints the string \inline{msg},
we resume the continuation \inline{k}.
This is also handled with the same handler, so it returns some value \inline{x}
and a list of its messages \inline{msgs}.
Now, we only need to prepend \inline{msg} to this list and return it together with \inline{x}.
The fact that the handler changes the type of the handled computation is reflected in its inferred type
.

A matching handler that is also useful in unit testing is one that feeds a list of strings to \inline{std#read}:
\begin{source}
  handler
  | val x -> (fun strs -> x)
  | std#read () k -> (fun strs ->
      match strs with
      | str :: strs' -> (k str) strs'
      | [] -> (k "") []
    )
\end{source}
We accomplish this by transforming a computation into a function that accepts a list of strings \inline{strs}.
If the computation returns some value \inline{x},
the function ignores its argument and returns \inline{x}.
But if the computation calls \inline{std#read}, we take a look at the list \inline{strs}.
If it is non-empty, we pass \inline{k} its first element \inline{str}.
And since the continuation is further handled,
it is also a function that accepts a list of strings,
so we pass it the remainder \inline{strs'}.
If it is empty, we pass \inline{k} the empty string and again the empty list.

The inferred type of the handler is

where .
The reason  appears in two places is
because operations other than \inline{std#read} may occur before or after we handle the first \inline{std#read} and so obtain a function.
Since  appears twice, we unfortunately cannot use the compact form.

\label{page:finally}
However, \Eff extends handlers with an additional \inline{finally} case,
which first transforms the computation with the handler,
and then applies the finally case computation to the resulting value.
In particular, if \inline{h} is some handler and \inline{h_fin} is the same as \inline{h}
except that it also contains a case \inline{finally x -> c_fin},
the computation \inline{with h_fin handle c} behaves exactly as
\begin{source}
  let x = (with h handle c) in c_fin
\end{source}
Using this extension, we can define
\begin{source}
  let supply_input strs0 =
    handler
    | val x -> (* ...as before... *)
    | std#read () k -> (* ...as before... *)
    | finally f -> f strs0
\end{source}
So, once we get back a function \inline{f} accepting a list of inputs, we apply it to the given list \inline{strs0}.
We use the handler as
\begin{source}
  with
    supply_input ["Alpha"; "Bravo"; "Charlie"]
  handle
    ...
\end{source}
The inferred type then takes the simpler form



\subsection{References}
\label{sub:references}\enlargethispage{\baselineskip}

Similar to OCaml, mutable references in \Eff are given the effect type 
with the signature

We can define the OCaml accessors by:
\begin{source}
  let (!) r = r#lookup ()
  let (:=) r v = r#update v
\end{source}
with the types

With reference handlers, we can temporarily alter the value stored in the reference,
make it read-only, log all changes, and more. 
However, handlers are not meant only for overriding but also for defining effects.
In particular, we can use handlers to implement references with a state monad:
\begin{source}
  let state r s0 =
    handler
    | val x -> (fun s -> x)
    | r#lookup () k -> (fun s -> k s s)
    | r#update s' k -> (fun s -> k () s')
    | finally f -> f s0
\end{source}
The function \inline{state} gives a handler that
handles a stateful computation using reference \inline{r} into a pure function that accepts the current state~\inline{s}
and passes it around.
So, we handle \inline{lookup} by a function
that takes the state \inline{s} and passes it to the continuation \inline{k},
which expects the current state as the outcome of \inline{lookup}.
Since this continuation is further handled, it is again a function accepting current state.
Looking up a reference does not change the state, so we pass \inline{s} again,
thus \inline{k} is applied to \inline{s} twice.
In the case for \inline{update},
the expected outcome of the call is the unit value,
while the current state is overwritten by the parameter \inline{s'}.
And the \inline{finally} case says that once we get back a function accepting current state,
we apply it to a given initial state~\inline{s0}.

The inferred type of \inline{state} is

If we want to access the final state, we define \inline{state'} to be exactly the same as \inline{state},
except that its value case is \inline{val x -> (fun s -> (x, s))}.
In this case the inferred type is


We can use multiple references without a hitch.
For example, given two references \inline{r1} and \inline{r2}, the computation
\begin{source}
  with (state r1 6) handle
    with (state r2 0) handle
      let x = !r1 in
      r2 := x + 1;
      !r2 * x
\end{source}
returns \inline{42} and its inferred type is the pure .

If we replace \inline{state} by \inline{state'},
the handled computation returns \inline{(6, (7, 42))} and the inferred type is .
This shows how easy it is to change the effectful behaviour by just switching the handlers and keeping the imperative code as it is.

\section{Core Eff}
\label{sec:core-eff}

For formal development,
we restrict our attention to \emph{core \Eff}, a minimal fragment of \Eff.
Core \Eff differs from \Eff in the following aspects:
\begin{itemize}
  \item
    Core \Eff is a fine-grain call-by-value calculus~\cite{levy03modelling},
    which means that its terms are split into
    inert \emph{expressions} and possibly effectful \emph{computations}.
    The separation makes the formalization much simpler,
    but makes programming that much harder.
    For this reason, \Eff allows the programmer to freely mix the terms,
    and performs the routine separation automatically.
    For example, a program such as
\begin{source}
  let transform f m n = (f m 42, n)
    \end{source}
gets translated into\enlargethispage{\baselineskip}
    \begin{source}
  let transform = fun f -> val (fun m -> val (fun n ->
    let tmp1 =
      let tmp2 = f m in
      tmp2 42
    in
    val (tmp1, n)
  ))
    \end{source}
    where \inline{val} constructs a computation that immediately returns
    a value represented by a given expression.
  \item
    \Eff allows programmers to define their own parametric inductive datatypes and effect types,
    while in core \Eff, we fix the signature of effect types and drop inductive datatypes entirely.
  \item
    \Eff provides a \inline{new} construct
    that allows a programmer to create fresh instances at runtime~\cite{bauer2012programming},
    and can be used to model both exception declarations and reference allocations in ML.
    Since the formalization of this feature is still under investigation, we omit it from our development
    and only briefly discuss it in the Conclusion.
  \item
    Handlers in \Eff allow the additional \inline{finally} case in handlers,
    but as already discussed on page~\pageref{page:finally},
    this is merely a convenience that can be expressed with existing constructs.
\end{itemize}

\subsection{Terms}

We start with a given set of \emph{effects}~,
which are just labels such as ,  or 
for all possible effects we want to use in our programs.
Next, for each effect~, we have a fixed set  of operation symbols~
and a fixed set  of instances~.
We assume that in each operation ,
both  and  belong to the same effect.

The \emph{expressions}~ and \emph{computations}~ of core \Eff are given by:

Here and everywhere, we write  or just  to denote a finite repetition of~.
The language constructs are standard except for:
the empty type eliminator ,
the computation  that immediately evaluates to a value,
polymorphic let-binding ,
and the already discussed \emph{instance constants}, \emph{handlers}, \emph{operation calls} and the \emph{handling construct}.
Note that both instance constants and handlers are first-class values in core \Eff.

The operation calls in core \Eff are slightly different from the ones in \Eff.
The call~ represents an application
of an operation~ to a parameter 
with a continuation  waiting for the result of the call to be bound to~.
Explicit continuations are convenient for operational semantics,
but we do not expect the programmer to use them.
Instead, \Eff uses \emph{generic effects}~\cite{plotkin2003algebraic}, defined as

which take a parameter and perform the operation call with the trivial continuation.
Then, the programmer can write the more familiar 
instead of ,
and we shall see that the two exhibit equivalent behaviour.  

The rough idea is that each non-divergent computation either evaluates to a value or calls an operation.
We use a handler~ on a computation~ as follows:
\begin{itemize}
\item
  If  evaluates to a value , we use the value case and evaluate .
\item
  If  performs an operation call~
  and the handler contains a matching case ,
  we evaluate .
  We wrap the handler  around the continuation so that it may continue handling future operation calls,
  though any operations called by  escape its scope.
\item
  If the handler has no matching operation cases for the called operation,
  then just like in exception handlers,
  we propagate the call outwards for other handlers to catch,
  though we still wrap  around the continuation as it may handle some other operations.
\end{itemize}

\noindent Let-binding  works as follows:
if  evaluates to , we continue with ,
but if  calls an operation,
we propagate the call outwards just like when a handler has no matching cases.
In fact, let-binding  works exactly as

Though this makes let binding redundant,
we keep it in the language for convenient notation and
to serve as a stepping stone to the less familiar handling construct.

To make the above intuition more precise and to motivate the effect system,
we now give a small-step operational semantics,
determined by a relation  defined in Figure~\ref{fig:small-step},
stating that a computation~ takes a single step to .
Note that the relation is given for computations only and that expressions are inert.

\begin{figure}[h]
\hrulefill
  \small
  \begin{mathpar}
  \inferrule{
  }{
    \ifthenelse{\tru}{c_1}{c_2} \step c_1
  }

  \inferrule{
  }{
    \ifthenelse{\fls}{c_1}{c_2} \step c_2
  }

  \inferrule{
  }{
    \iszero 0 \step \val \tru
  }

  \inferrule{
  }{
    \iszero (\succ e) \step \val \fls
  }

  \inferrule{
  }{
    \pred 0 \step \val 0
  }

  \inferrule{
  }{
    \pred (\succ e) \step \val e
  }

  \inferrule{
  }{
    (\fun{x} c) \, e \step c[e / x]
  }

  \inferrule{
    c_1 \step c_1'
  }{
    \letin{x = c_1} c_2 \step \letin{x = c_1'} c_2
  }

  \inferrule{
  }{
    \letin{x = \val e} c \step c[e / x]
  }

  \inferrule{
  }{
    \letin{x = \call{\inst}{\op}{e}{\cont{y}{c_1}}} c_2
      \step \call{\inst}{\op}{e}{\cont{y}{\letin{x = c_1} c_2}}      
  }

  \inferrule{
  }{
    \letvalin{x = e} c \step c[e / x]
  }

  \inferrule{
  }{
    \letrecin{f \, x = c_1} c_2
      \step c_2[(\fun{x} \letrecin{f \, x = c_1} c_1) / f]
  }

  \inferrule{
    c \step c'
  }{
    \withhandle{e}{c} \step \withhandle{e}{c'}
  }

  \inferrule{
  }{
    \withhandle{h}{(\val e)} \step c_v[e / x]
  }

  \inferrule{
  }{
    \withhandle{h}{(\call{\inst_j}{\op_j}{e}{\cont{y}{c}})}
      \step c_j[e / x, (\fun{y} \withhandle{h}{c}) / k]
  }

  \inferrule{
    \hash{\inst}{\op} \not\in \set{\hash{\inst_i}{\op_i}}_i
  }{
    \withhandle{h}{(\call{\inst}{\op}{e}{\cont{y}{c}})}
      \step \call{\inst}{\op}{e}{\cont{y}{\withhandle{h}{c}}}
  }
\end{mathpar}
\caption{
  The inductive definition of the relation .
  In the last three rules, we set .}
\label{fig:small-step}
\hrulefill
\end{figure}

\begin{exa}
\label{exa:reduction}
Take a reference handler

which temporarily treats a reference  as if it always contains ,
and afterwards updates it with the final result of the handled computation.
This update is not handled by~ because it escapes its scope.
If we apply  on the computation

the outcome of the first lookup is , which is then bound to ,
while the handler continues handling the continuation.
Then, the update is ignored and finally,
the handler applies the value case on  and terminates with a call of .
Note that \Eff provides \emph{resources}~\cite{bauer2012programming},
which at this point trigger real-world effects and resume the continuation.
The exact reduction sequence is given in Figure~\ref{fig:reduction}.
\end{exa}

\begin{figure}[h]
\hrulefill
  \small
\newcommand{\hilite}{\underline}

\caption{
  The evaluation of , as described in Example~\ref{exa:reduction}.
  We underline the active parts of each step and shorten  to .
  We can see how the operation call to  in the first line propagates
  outwards to the matching handler while its continuation builds up.
  Once the call reaches a handler, it is replaced with the handling term
  in which  is replaced by the further handled continuation.}
\label{fig:reduction}
\hrulefill
\end{figure}

\subsection{Types}
\label{sub:types}

The types, which are also split into pure and potentially effectful (here called \emph{dirty}) ones, are given by

We have the usual ground types and the function type~
of functions that take expressions of type~ and perform computations of type~.
Next, we have the \emph{effect type}~ of instances of effect~ from a \emph{region}~,
which is just a \emph{non-empty} finite set of instances .
Finally, we have the \emph{handler type}~ of handlers that take a computation of type~ and transform it into a computation of type~.
We call  the \emph{incoming} and  the \emph{outgoing} type.
Finally, dirty type~ contains computations that
return values of type  and may cause effects described by a \emph{dirt}~,
which is a set of operations

To lighten the syntax, we write  as 
where we also omit the outer braces around .

Note that for simplicity, the types of core \Eff are monomorphic.
However, we are going to shift to polymorphic types with type, region and dirt parameters
when we start with type inference in Section~\ref{sec:inferring}.


\subsection{Subtyping}
\label{sub:subtyping}

As in most effect systems,
we need to take care of the \emph{poisoning problem}~\cite{wansbrough1999once}.
For example, what type should we give to~ in

for some suitable boolean ?
If we give it the type~
(for this example, we allow ourselves the type  of strings),
we cannot type the conditional statement as the two branches cannot have the same type,
but if we give it the type~,
we lose information that the final result is a pure function.

The simplest antidote for the poisoning problem is to allow subtyping,
so that we may give  the type with an empty dirt,
and use subsumption to suitably enlarge this dirt in the conditional statement.

Subtyping also solves a similar problem with regions of handled instances.
Consider the computation

Without subtyping we are forced to give both  and  the type .
Therefore, as discussed in Section~\ref{ssub:handled-regions},
the type of  does not tell us whether  handles
 or , and so we must assume that both may remain unhandled.
With subtyping we may give  the type , which makes it clear that  handles .

For our purposes, it is enough to use \emph{structural} subtyping~\cite{fuh1990type},
where we relate only types of the same shape.
The subtyping relations between types and between dirty types are defined in Figure~\ref{fig:subtyping}.
\begin{figure}
\hrulefill
  \small
  \begin{mathpar}
  \inferrule[Sub-]{
  }{
    \boolty \le \boolty
  }

  \inferrule[Sub-]{
  }{
    \natty \le \natty
  }

  \inferrule[Sub-]{
  }{
    \unitty \le \unitty
  }

  \inferrule[Sub-]{
  }{
    \emptyty \le \emptyty
  }

  \inferrule[Sub-]{
    A' \le A \\
    \C \le \C'
  }{
    A \to \C \le A' \to \C'
  }

  \inferrule[Sub-]{
    \Rgn \subseteq \Rgn'
  }{
    E^\Rgn \le E^{\Rgn'}
  }

  \inferrule[Sub-]{
    \C' \le \C \\
    \D \le \D'
  }{
    \C \hto \D \le \C' \hto \D'
  }

  \inferrule[Sub-]{
    A \le A' \\
    \Drt \subseteq \Drt'
  }{
    A \E \Drt \le A' \E \Drt'
  }
\end{mathpar}
\caption{
  The inductive definition of the subtyping relations  and .}
\label{fig:subtyping}
\hrulefill
\end{figure}
Sometimes, we shall be interested in types that have the same shape.
So, we define  as the equivalence relation on types, generated by .
Equivalence classes of  are called \emph{skeletons}~\cite{simonet2003type}.


\subsection{Effect system}
\label{sub:effect-system}

Our effect system is built on two typing judgements, defined in Figure~\ref{fig:typing}.
The judgement  states that in context~ and \emph{signature}~, an expression~ has a type~.
The judgement  states a similar thing for a computation~ and a dirty type~.
In both cases, the context  is a unique assignment of (pure) types to variables,
while the signature  consists of \emph{effect signatures}~ for each effect .
These are of the form

and assign a \emph{parameter type}~ and a \emph{result type}~ to each listed operation~.
For example, the effect signatures for references is:

For technical reasons, we assume that both the parameter and the result type for each operation do not contain any regions or dirt,
which limits them to the basic ground types such as  or .
We further discuss this restriction in Remark~\ref{rem:glitch}.


The purpose of the presented effect system is to offer guarantees on the behaviour of programs,
not (yet) to lead to an efficient inference algorithm.
One sign of that is rules like \rulename{Val}, \rulename{Inst} or \rulename{Pred},
where we assign types that are safe, but much coarser than needed.
A more obvious sign is the rule \rulename{LetVal},
where we employ a very naive form of let-polymorphism that performs an explicit substitution.
This is, of course, extremely inefficient, but lets us postpone the use of parameters to
the inference rules, which use the more efficient variant with universally quantified types.

\begin{figure}
\hrulefill
  \small
  \begin{mathpar}
  \inferrule[Var]{
    (x \T A) \in \ctx
  }{
    \ctx \ent x \T A
  }

  \inferrule[True]{
  }{
    \ctx \ent \tru \T \boolty
  }

  \inferrule[False]{
  }{
    \ctx \ent \fls \T \boolty
  }

  \inferrule[Zero]{
  }{
    \ctx \ent 0 \T \natty
  }

  \inferrule[Succ]{
    \ctx \ent e \T \natty
  }{
    \ctx \ent \succ e \T \natty
  }

  \inferrule[Unit]{
  }{
    \ctx \ent \unt \T \unitty
  }

  \inferrule[Fun]{
    \ctx, x \T A \ent c \T \C
  }{
    \ctx \ent \fun{x} c \T A \to \C
  }

  \inferrule[Inst]{
    \inst \in \Rgn \subseteq \insts_E
  }{
    \ctx \ent \inst \T E^\Rgn
  }

  \text{\rulename{Hand} --- in the text}

  \inferrule[SubExpr]{
    \ctx \ent e \T A \\
    A \le A'
  }{
    \ctx \ent e \T A'
  }

  \medskip

  \inferrule[IfThenElse]{
    \ctx \ent e \T \boolty \\
    \ctx \ent c_1 \T \C \\
    \ctx \ent c_2 \T \C
  }{
    \ctx \ent \ifthenelse{e}{c_1}{c_2} \T \C
  }

  \inferrule[IsZero]{
    \ctx \ent e \T \natty
  }{
    \ctx \ent \iszero e \T \boolty \E \Drt
  }

  \inferrule[Pred]{
    \ctx \ent e \T \natty
  }{
    \ctx \ent \pred e \T \natty \E \Drt
  }

  \inferrule[Absurd]{
    \ctx \ent e \T \emptyty
  }{
    \ctx \ent \absurd e \T \C
  }
  
  \inferrule[App]{
    \ctx \ent e_1 \T A \to \C \\
    \ctx \ent e_2 \T A
  }{
    \ctx \ent e_1 \, e_2 \T \C
  }

  \inferrule[Val]{
    \ctx \ent e \T A
  }{
    \ctx \ent \val e \T A \E \Drt
  }

  \inferrule[Op]{
    \ctx \ent e_1 \T E^\Rgn \\
    \op \T A^\op \to B^\op \in \sig(E) \\\\
    \ctx \ent e_2 \T A^\op \\
    \ctx, y \T B^\op \ent c \T A \E \Drt \\
    \fra{\inst \in \Rgn} \hash{\inst}{\op} \in \Drt
  }{
    \ctx \ent \call{e_1}{\op}{e_2}{\cont{y}{c}} \T A \E \Drt
  }

  \inferrule[Let]{
    \ctx \ent c_1 \T A \E \Drt \\
    \ctx, x \T A \ent c_2 \T B \E \Drt
  }{
    \ctx \ent \letin{x = c_1} c_2 \T B \E \Drt
  }

  \inferrule[LetVal]{
    \ctx \ent e \T A \\
    \ctx \ent c[e / x] \T \C
  }{
    \ctx \ent \letvalin{x = e} c \T \C
  }

 \inferrule[LetRec]{
    \ctx, f \T A \to \C, x \T A \ent c_1 \T \C \\
    \ctx, f \T A \to \C \ent c_2 \T \D
  }{
    \ctx \ent \letrecin{f \, x = c_1} c_2 \T \D
  }

  \inferrule[With]{
    \ctx \ent e \T \C \hto \D \\
    \ctx \ent c \T \C
  }{
    \ctx \ent \withhandle{e}{c} \T \D
  }

  \inferrule[SubComp]{
    \ctx \ent c \T \C \\
    \C \le \C'
  }{
    \ctx \ent c \T \C'
  }
\end{mathpar}
\caption{
  The inductive definition of the typing judgements  and .
  As the signature~ does not change, we omit its display from all the judgements.
  The rule for handlers is given in the main text.}
\label{fig:typing}
\hrulefill
\end{figure}

All the typing rules are standard except for:
\begin{description}
\item[\rulename{Inst}]
  in which we check that  is contained in the region  that belongs to an effect .
\item[\rulename{Op}]
  in which we first check that  and  belong to the same effect.
  Then, we need to check that the dirt  covers not just all possible operations that the operation call may cause
  (recall that  may contain more than one instance), but also any operations in the continuation~.
  We may assume that  has the same dirt, as we can use \rulename{SubComp} otherwise.
  We use the same reasoning in rules \rulename{IfThenElse} and \rulename{Let}.
\item[\rulename{With}]
  where the handling construct is typed like an application,
  except that it is applied to a computation rather than an expression.
\item[\rulename{Hand}]
  which is a bit more daunting, so we write it out separately:
  
  For a handler to be of type ,
  we first check that it takes values of type  to computations of type .
  Then, for each operation case , we check the premises~,
  comprising:
  
  Like in \rulename{Op}, we check that  and  belong to the same effect .
  Then, the handling computation~ needs to have the same type ,
  assuming that parameter  is of type ,
  and the continuation~ of type~.
  Observe that since the continuation is further handled, it already has the outgoing type.

  Finally, in  we check that any operation in the incoming dirt~
  that is not guaranteed to be caught by the handler
  must appear in the outgoing dirt~ as well.
  As discussed in~Section~\ref{ssub:handled-regions},
  an operation  will be (if not sooner)
  surely caught by the case for 
  whenever  and the region~ is the singleton~.
  Thus, we define  to be
  
  where the \emph{singleton union}  behaves like a union, except that it considers only singletons.
  For example, .
  More precisely, we define:
  
\end{description}\medskip

\noindent The given effect system is then safe with respect to the operational semantics:
a computation  can only call operations from .
In particular, if  is empty, then  is guaranteed to be pure, though it may diverge.

\begin{thm}[Safety]
\label{thm:safety}
If for a computation~, the typing judgement  holds, then either:
\begin{itemize}
\item
   is of the form  for some expression , or
\item
   is of the form  for some , or
\item
  there exists a computation  such that  and .
\end{itemize}
\end{thm}

We do not give a proof of Theorem~\ref{thm:safety} in this paper.
Instead, a full formalization of core \Eff in Twelf is available at~\url{https://github.com/matijapretnar/twelf-eff/}

\begin{exa}
To illustrate the type system, let us revisit Example~\ref{exa:reduction}.
First, assume that the reference  is given the type .
Then, the stateful computation~ has the dirty type
,
while the handler~ has the type

as it handles both  and , but then triggers  in the value case.
This  also changes the type of computation from  to .

If the reference  has a less precise type ,
the dirt of  is 

while the best type we can give to  is .
Since the region of  is not a singleton,
we unfortunately cannot give any guarantees on the handled operations.
\end{exa}

\section{Inferring constraints}
\label{sec:inferring}

Turning to our inference algorithm, we first describe
a collection of syntax-directed inference rules
that are readily transcribed into a recursive function that
infers a type and a set of constraints from a given term.


\subsection{Parametric types}
\label{sub:parametric-types}

As indicated in Section~\ref{sub:types}, we switch to a language that is more suited for inference:

From now on, we refer to types and dirt from Section~\ref{sub:types},
which contain no parameters, as \emph{closed} ones.

To enable polymorphism, types are extended with the \emph{type parameters}~.
Then, regions are not just extended,
but completely replaced with \emph{region parameters}~,
as this greatly simplifies the inference.
We are going to capture the information about instances using constraints instead.
Recall that regions~ describing the possible instances in an effect type  are always inhabited.
This information will prove useful in Section~\ref{sec:simplifying} as it allows further simplification.
For this reason, we designate a special subset of region parameters, called \emph{inhabited} and marked by~.

Finally, we adopt a row-like~\cite{remy1993type} representation of dirt as described in Section~\ref{ssub:raising-exceptions}.
The first part is a set of operation symbols together with a region parameter
that captures the (possibly empty) region of all the instances on which these symbols are used.
This is similar to before, except that operations are grouped by their operation symbols.
The reason for this grouping is that we are always able to precisely determine the operation symbols,
but not the instances of called or handled operations.

The second part consists of a single \emph{dirt parameter}~,
intended to capture the rest of operations.
If the first part is empty, we write the dirt simply as .
To keep track of the operation symbols captured by 
and ensure that it does not capture any symbols from the first part,
we sometimes write the parameter as 
to emphasise the set~ of symbols not captured by 
(though  can always be reconstructed by looking
at the first part of any dirt in which  appears
because our algorithm ensures that all such dirts consistently list the same operation symbols).

Any additional information is captured with \emph{constraints}.
For example, a conditional can call any operation that one of its branches does.
If these two branches cause dirt captured by  and ,
we can represent the dirt of the whole conditional with a fresh parameter 
together with constraints  and .
Constraints can be of one of the following five kinds:
\begin{itemize}
\item
   states that the type  needs to be smaller than ,
\item
   states the same for dirty types,
\item
   is a generalisation of the inequality  due to handlers.
  It states that all instances from  are either in  or in some  that is a singleton,
\item
   similarly states that  is either in  or in some  that is a singleton,
\item
   states that the dirt  is smaller than .
\end{itemize}
In the right-hand side  of two region constraints,
we refer to  as the \emph{covering},
and to  as the \emph{handled} region parameters.

Unlike the subtyping relation, constraints do not have any inherent reasoning principles,
but are just a way of writing down the relationship between parameters.
Instead, we give constraints a meaning by specifying their solutions.

\begin{defi}
\label{defi:closed-substitution}
A \emph{closed substitution}~ is a partial mapping that maps:
  each type parameter~ to a closed type~,
  each region parameter~ to a closed region~,
  each inhabited region parameter~ to a non-empty closed region~,
  and each dirt parameter~ to a closed dirt~
  that contains no operations with operation symbols in~.
\end{defi}

We write substitutions by specifying a set of mappings of parameters, for example

We can extend a closed substitution to other constructs by:


\begin{defi}
A closed substitution~ is a \emph{solution} of a set of constraints~,
which we write as ,
if it satisfies all the constraints in .
This is defined by:

\end{defi}

A parametric type  together with a set of constraints  between its parameters
then describes a family of closed types, obtained by taking all instances  of 
for all solutions ,
and all their supertypes.
More precisely, we define

Our aim now is to take an expression 
and compute a type  and a set of constraints 
such that the set of all possible types  we can assign to  is captured exactly by .

\subsection{Inference rules}
\label{sub:inference-rules}

We infer types and constraints using syntax-directed inference rules of the form
 for expressions and
 for computations,
defined in Figure~\ref{fig:constraints}.
Here,  is a set of constraints,
 is a set of all fresh parameters introduced in the derivation,
and  is the \emph{polymorphic context}, which is collection
of unique assignments  of
\emph{type schemes} to variables (assumed to be different from the ones in ).
As in typing judgements, we assume (though never write) a fixed signature~.

The type schemes are similar to polymorphic types of ML, which are types,
universally quantified over a given set of type parameters.
In our case, we may also quantify over region and dirt parameters,
but we need to keep information about the constraints these parameters need to satisfy.
Even though the ordinary context  can be seen as a particular instance of ,
we keep the two separate in order to relate the inference judgements to typing judgements,
as the latter employ only .

Though  and  are sets, we sometimes write them as sequences to save space.
For example, we write  instead of .
We also assume that all parameters listed in  are distinct
and this implies the usual freshness conditions~\cite[p.~321]{pierce2002types}.
For example, the above sequence implies that sets  and 
are disjoint and do not contain  or .
In particular, in the rule \rulename{Cstr-PolyVar},
we implicitly rename any bound parameters  so that a fresh copy is obtained at each use.

\begin{figure}
\hrulefill
  \small
  \begin{mathpar}
  \inferrule[Cstr-Var]{
    (x \T A) \in \ctx
  }{
    \ctx; \pctx \ent[\emptyset] x \T A \while \emptyset
  }

  \inferrule[Cstr-PolyVar]{
    \big(x \T \fra{F} A \while \cstr\big) \in \pctx
  }{
    \ctx; \pctx \ent[F] x \T A \while \cstr
  }

  \inferrule[Cstr-True]{
  }{
    \ctx; \pctx \ent[\emptyset] \tru \T \boolty \while \emptyset
  }

  \inferrule[Cstr-False]{
  }{
    \ctx; \pctx \ent[\emptyset] \fls \T \boolty \while \emptyset
  }

  \inferrule[Cstr-Zero]{
  }{
    \ctx; \pctx \ent[\emptyset] 0 \T \natty \while \emptyset
  }

  \inferrule[Cstr-Succ]{
    \ctx; \pctx \ent[F] e \T A \while \cstr
  }{
    \ctx; \pctx \ent[F] \succ e \T \natty \while \cstr, A \le \natty
  }

  \inferrule[Cstr-Unit]{
  }{
    \ctx; \pctx \ent[\emptyset] \unt \T \unitty \while \emptyset
  }

  \inferrule[Cstr-Fun]{
    \ctx, x \T \alpha; \pctx \ent[F] c \T \C \while \cstr
  }{
    \ctx; \pctx \ent[F, \alpha] \fun{x} c \T \alpha \to \C \while \cstr
  }

  \inferrule[Cstr-Inst]{
  }{
    \ctx; \pctx \ent[\rgn*] \inst \T E^{\rgn*} \while \inst \in \rgn*
  }

  \text{\rulename{Cstr-Hand} --- in the text}

  \inferrule[Cstr-IfThenElse]{
    \ctx; \pctx \ent[F] e \T A \while \cstr \\
    \ctx; \pctx \ent[F_1] c_1 \T \C_1 \while \cstr_1 \\
    \ctx; \pctx \ent[F_2] c_2 \T \C_2 \while \cstr_2
  }{
    \ctx; \pctx \ent[F, F_1, F_2, \alpha, \drt] \ifthenelse{e}{c_1}{c_2} \T \alpha \E \drt \while
      \cstr, \cstr_1, \cstr_2, A \le \boolty, \C_1 \le (\alpha \E \drt), \C_2 \le (\alpha \E \drt)
  }

  \inferrule[Cstr-IsZero]{
    \ctx; \pctx \ent[F] e \T A \while \cstr
  }{
    \ctx; \pctx \ent[F, \drt] \iszero e \T \boolty \E \drt \while \cstr, A \le \natty
  }

  \inferrule[Cstr-Pred]{
    \ctx; \pctx \ent[F] e \T A \while \cstr
  }{
    \ctx; \pctx \ent[F, \drt] \pred e \T \natty \E \drt \while \cstr, A \le \natty
  }

  \inferrule[Cstr-Absurd]{
    \ctx; \pctx \ent[F] e \T A \while \cstr
  }{
    \ctx; \pctx \ent[F, \alpha, \drt] \absurd e \T \alpha \E \drt \while \cstr, A \le \emptyty
  }
  
  \inferrule[Cstr-App]{
    \ctx; \pctx \ent[F_1] e_1 \T A_1 \while \cstr_1 \\
    \ctx; \pctx \ent[F_2] e_2 \T A_2 \while \cstr_2
  }{
    \ctx; \pctx \ent[F_1, F_2, \alpha, \drt] e_1 \, e_2 \T \alpha \E \drt
      \while \cstr_1, \cstr_2, A_1 \le (A_2 \to[\drt] \alpha)
  }

  \inferrule[Cstr-Val]{
    \ctx; \pctx \ent[F] e \T A \while \cstr
  }{
    \ctx; \pctx \ent[F, \drt] \val e \T A \E \drt \while \cstr
  }

  \inferrule[Cstr-Op]{
    \ctx; \pctx \ent[F_1] e_1 \T A_1 \while \cstr_1 \\
    \op \T A^\op \to B^\op \in \sig(E) \\
    \ctx; \pctx \ent[F_2] e_2 \T A_2 \while \cstr_2 \\
    \ctx, y \T B^\op; \pctx \ent[F] c \T A \E \Drt \while \cstr
  }{
    \ctx; \pctx \ent[F_1, F_2, F, \rgn, \rgn*, \drt] \call{e_1}{\op}{e_2}{\cont{y}{c}} \T A \E \set{\op \T \rgn \mid \drt}
      \while \cstr_1, \cstr_2, \cstr, A_1 \le E^{\rgn*}, A_2 \le A^\op, \rgn* \le \rgn, \Drt \le \set{\op \T \rgn \mid \drt}
  }

  \inferrule[Cstr-Let]{
    \ctx; \pctx \ent[F_1] c_1 \T A \E \Drt_1 \while \cstr_1 \\
    \ctx, x \T \alpha; \pctx \ent[F_2] c_2 \T B \E \Drt_2 \while \cstr_2
  }{
    \ctx; \pctx \ent[F_1, F_2, \alpha, \drt] \letin{x = c_1} c_2 \T B \E \drt
      \while \cstr_1, \cstr_2, A \le \alpha, \Drt_1 \le \drt, \Drt_2 \le \drt
  }

  \inferrule[Cstr-LetVal]{
    \ctx; \pctx \ent[F_1] e \T A \while \cstr_1 \\
    \ctx; \pctx, (x \T \fra{F_1} A \mid \cstr_1) \ent[F_2] c \T \C \while \cstr_2
  }{
    \ctx; \pctx \ent[F_2] \letvalin{x = e} c \T \C \while \cstr_2
  }

 \inferrule[Cstr-LetRec]{
    \ctx, f \T \alpha_1 \to[\drt] \alpha_2, x \T \alpha_1; \pctx \ent[F_1] c_1 \T \C \while \cstr_1 \\
    \ctx, f \T \alpha_1 \to[\drt] \alpha_2; \pctx \ent[F_2] c_2 \T \D \while \cstr_2
  }{
    \ctx; \pctx \ent[F_1, F_2, \alpha_1, \alpha_2, \drt] \letrecin{f \, x = c_1} c_2 \T \D \while \cstr_1, \cstr_2, \C \le \alpha_2 \E \drt
  }

  \inferrule[Cstr-With]{
    \ctx; \pctx \ent[F_1] e \T A \while \cstr_1 \\
    \ctx; \pctx \ent[F_2] c \T \C \while \cstr_2
  }{
    \ctx; \pctx \ent[F_1, F_2, \alpha, \drt] \withhandle{e}{c} \T \alpha \E \drt
      \while \cstr_1, \cstr_2, A \le (\C \hto \alpha \E \drt)
  }
\end{mathpar}
\caption{
  The inductive definition of the inference judgements
   and .
  The rule for handlers is given in the main text.}
\label{fig:constraints}
\hrulefill
\end{figure}

As announced at the beginning of Section~\ref{sub:parametric-types},
regions and dirts have a fixed representation with parameters.
Thus in \rulename{Cstr-Inst}, we assign each instance a fresh region parameter and add a suitable constraint.
Similarly, we cannot simply state that the dirt of  is empty.
Instead, in \rulename{Cstr-Val}, we assign it a fresh dirt parameter~
that needs to satisfy no constraints.
That means that we may replace  by anything, including the empty set.

Though we get an equivalent set of constraints in the rule \rulename{Cstr-Op}
if we use a single region parameter,
we introduce two for technical reasons, discussed in Section~\ref{sub:simplifying-region-constraints}.
The rule~\rulename{Cstr-With} is analogous to~\rulename{Cstr-App}.

Otherwise, the rules for the standard constructs are
similar to ones in the Hindley-Milner algorithm~\cite[p.~322]{pierce2002types},
except that we need to use (correctly oriented) inequalities instead of equalities in the constraints.
In \rulename{Cst-LetVal} we can safely generalize over all the fresh parameters  generated
while inferring the type of  because they are guaranteed to be distinct from any parameters appearing in .

This leaves us with

To start, we take type parameters  and  to represent the incoming and outgoing type of the handler.
Next, we take  to be the set of all distinct operation symbols listed in operation cases.
For each , we take fresh parameters  and 
that represent the region assigned to  in the incoming and outgoing dirt of the handler.
Finally, we take fresh parameters  and  to represent the rest of incoming and outgoing dirt.
The incoming and outgoing types are then

After introducing the necessary parameters, we infer the type and constraints of the value case.
Next, for each operation case, in the premises~, consisting of:

we check the suitability of operation,
and infer the types and constraints of the handled instance~ and of the operation case~.

We end up with the a set of constraints~ consisting of the following five parts:
\begin{itemize}
\label{pag:hand-rules}
\item
  constraints~ inherited from the value case and
  constraints~ and  inherited from each operation case;
\item
  constraint  stating that the outgoing type~ subsumes the type of the value case
  and constraints  stating the same for all the operation cases;
\item
  constraints  stating that the type~ of the instance expression~ is subsumed by the effect type~ for some fresh ;
\item
  constraints  for each  ---
  the outgoing dirt must be big enough to cover all operations in the incoming dirt
  that are not surely handled by one of the operation cases; and
\item
  a constraint  --- any operation that is not listed in a handler cannot be handled,
  so must appear in the outgoing dirt as well.
\end{itemize}
The set~ of all fresh parameters gathers all fresh parameters mentioned above and equals


Looking at the presented inference rules,
we see that there is exactly one rule that applies to each language construct,
so we can assign a unique type and set of constraints to each term (up to a renaming of fresh parameters).
This allows us to turn the rules into a recursive function,
which computes exactly the information about all the types we can assign to a given term:

\begin{thm}[Soundness \& completeness]
\label{thm:completeness}
Let  be a closed context.
\begin{itemize}
\item
  If we have  for some ,
  then  holds and
  
\item
  We have 
  if and only if  holds for some .
  In this case
  
\end{itemize}
\end{thm}

\begin{rem}
\label{rem:glitch}
We limit the parameter and result types in the signature~ to basic types
because  is shared between typing judgements, which feature concrete regions and dirt,
and inference judgements, which represent regions and dirt exclusively with parameters.
There are three ways of reconciling this conflict:
\begin{itemize}
\item
  Extend the language of constraints with concrete upper bounds of the form  and .
  Then we may, say, replace any occurrence of  in inference judgements
  with  for some fresh ,
  and add constraints ,
  or replace 
  with a suitably fresh  and
  constraints .
\item
  Extend monomorphic types with \emph{wild card} regions and dirt.
  For example, the type  would capture any exception,
  no matter which concrete region it comes from.
  Similarly, the dirt  would mean that a computation raises some exception,
  though we do not know which one,
  while the dirt  would mean that any operation may get called.

  This solution agrees with practice~\cite{bauer2012programming},
  where most of types that appear in the signature are already basic,
  and the only two deviations so far are cooperative multithreading and delimited continuations,
  which both take functions as parameters.
  However, in both cases, we are not interested in imposing any limits on this dirt,
  so a wild card dirt would fit our goal.

  To add wild card regions and dirt to core \Eff, we need to:
  (1) add subtyping rules such as ,
  (2) extend \rulename{Op} with a condition that if  then ,
  and (3) adapt the rule \rulename{Hand} to ensure that any wild card dirt cannot be handled.
\item
  Each of the above approaches has its advantages,
  and in addition, the two are compatible,
  so one may consider both in a practical implementation.
  However,
  the first approach leads a more powerful \emph{prescriptive} effect system
  which is well beyond the scope of this paper
  (discussed more in the Conclusion),
  while the second one is routine but messy.
  Thus, we opt for the simplest option:
  prohibit any types that include regions and dirt from appearing in .
\end{itemize}
\end{rem}

\section{Unifying constraints}
\label{sec:unifying}

Unfortunately, unlike in ML,
subtyping prevents us from computing a principal type from a given set of constraints~\cite{pottier1998type}.
For example (ignoring dirt for a moment), if we just drop the constraint in the type

we get a type that captures too many closed types.
On the other hand, the more restricted parametric type  is
too strict because it fails to capture the type

or any of its subtypes (otherwise, the subtyping rules would imply ).

Instead, the best we can do is to simplify the constraints as much as possible.
First, we are going to reduce the constraints down to a more convenient and basic form.
Constraints in this form always admit a solution,
so the reduction also detects any unsolvable constraints.

\begin{defi}
A set of constraints~ is \emph{unified},
if all constraints are \emph{decomposed} down to ones between parameters
and the set is \emph{closed} under logical implication.
In particular,  may contain only constraints of the form

and the following closure properties must hold:
\begin{itemize}
\item
  if  and ,
  then ;
\item
  if 
  and ,
  then ;
\item
  if 
  and ,
  then ;
\item
  if  and ,
  then .
\end{itemize}
To avoid circular types, we need to track all type parameters of the same shape.
So, we assume that  is equipped with an equivalence relation~
on type parameters, such that  implies .
For solutions of a unified set of constraints~, we consider only such ,
for which we also have  for any .
\end{defi}

\begin{lem}
\label{lem:unify}
If a set of constraints~ is unified, there exists a solution~.
\end{lem}

In Figure~\ref{fig:unify}, we define an algorithm ,
which is similar to Robinson's unification algorithm~\cite[p.~327]{pierce2002types},
except that it returns a set of unified constraints in addition to the unifying substitution.
The algorithm is defined recursively, passing around a triple , where
 is a unifying substitution of replaced parameters (initially taken to be the identity),
 is a set of already unified constraints (initially  is empty while  is the identity relation on all type parameters in ), and
the queue  is a set of constraints yet to be processed.

The unifying substitution is not a closed substitution (Definition~\ref{defi:closed-substitution}),
which maps type parameters to closed types, etc., but one that maps them to parametric ones. 
\begin{defi}
A \emph{substitution}~ is a mapping that maps:
  each type parameter~ to a type~,
  each region parameter~ to a region parameter~,
  each inhabited region parameter~ to an inhabited region parameter~,
  and each dirt parameter~ to a dirt~
  of the form ,
  where  is disjoint from .
This ensures that the dirt 
does not capture any operations from .
\end{defi}
We write substitutions by listing a set of all the non-idempotent rules.
We can extend a substitution from parameters to other constructs just like we extended closed substitutions.
This allows us to compose substitutions and additionally,
to compose a closed substitution~ with a substitution~
and obtain a closed substitution~.

\begin{figure}
\small
\hrulefill

\caption{Definition of the constraint unification algorithm~.}
\label{fig:unify}
\hrulefill
\end{figure}

The unification works as follows.
Take, say, a constraint .
Since the rules of structural subtyping admit only comparison between types of the same shape,
the only way to satisfy this constraint is to set  to be some function type into .
So, we decompose the constraint by replacing  with some fresh 
and adding constraints  and 

We add these constraints using the closure operator , defined in Figure~\ref{fig:closure},
which extends  with a given constraint and all constraints it implies.
This is done with a simplified version of an algorithm for computing the transitive closure of a graph~\cite{pottier1998type},
except that for region parameters, we also have to take instances and handled regions into account.

\begin{figure}[h]
\small
\hrulefill

\caption{Definition of the closure operator~.
In all cases, we assume that  implicitly contains all reflexive constraints,
so that, for example in the first case,
the added set also includes  and all constraints of the form  and .}
\label{fig:closure}
\hrulefill
\end{figure}

Before decomposition, we need to perform an \emph{occur check}
in order to prevent ill-formed types and ensure termination.
This check is slightly more involved than usually~\cite[p.~327]{pierce2002types}.
Say that we want to unify the set of constraints (let us again ignore dirt):

We decompose  as some  and end up with constraints

Now, we need to decompose , then , then  and the whole thing repeats.
So we need to check not only that  is not in ,
the set of all parameters that occur in ,
but also that no other parameters in its skeleton are.

When decomposing ,
we also replace each  with a (distinct) fresh copy of .
We do this by repeatedly calling a function  that on each call returns a type of the same form as ,
except with all its type, region, and dirt parameters replaced with fresh ones.
Because of this expansion, any constraints~
that mention parameters from the skeleton of  are no longer decomposed.
So we need to take them out of the otherwise unified  and put them back into the queue~.

On the remaining unified set of constraints ,
we define  to be as before,
except that we remove the whole skeleton of ,
and add the freshly generated parameters into the skeletons of matching parameters in .
For example, if the skeletons of  were

and we decompose the constraint ,
we replace  by  and  by ,
and the skeletons of  are

When we unify a constraint , we merge the skeletons of  and .

For dirt constraints,
we similarly expand both sides so to list the same operations.
For example, if we have a constraint
,
we replace  with some fresh 
and  with 
and add constraints , , and .
We define  to be the equivalence relation on dirt parameters generated by ,
so that we can capture all related dirt parameters and expand them at the same time.

An example of a full run of the unification algorithm is given in Figure~\ref{fig:unification}.

\begin{figure}[h]
\small
\hrulefill
\newcommand{\bigset}[1]{\Bigl\{#1\Big\}}
\newcommand{\bigunify}[4][\quad]{\unify\Big(\bigset{#2};#1\bigset{#3};\quad\bigset{#4}\Big)}

    {&\alpha_2 \mapsto (\alpha_3 \to[\op \T \rgn*_3 \mid \drt_5] \natty),\ 
      \drt_1 \mapsto \set{\op \T \rgn*_2 \mid \drt_4},\ 
      \drt_3 \mapsto \set{\op \T \rgn*_3 \mid \drt_5}}
    {&\alpha_3 \le \alpha_1}
    {\set{\op \T \rgn*_1 \mid \drt_2} \le \set{\op \T \rgn*_2 \mid \drt_4},\ 
      \set{\op \T \rgn*_2 \mid \drt_4} \le \set{\op \T \rgn*_3 \mid \drt_5}}
  = \\
  \bigunify[\
    {&\alpha_2 \mapsto (\alpha_3 \to[\op \T \rgn*_3 \mid \drt_5] \natty),\ 
      \drt_1 \mapsto \set{\op \T \rgn*_2 \mid \drt_4},\ 
      \drt_3 \mapsto \set{\op \T \rgn*_3 \mid \drt_5}}
    {&\alpha_3 \le \alpha_1,\ \rgn*_1 \le \rgn*_2 \le \rgn*_3,\ \drt_2 \le \drt_4 \le \drt_5}
    {}
  = \\
  \Bigl(\bigset{
    &\alpha_2 \mapsto (\alpha_3 \to[\op \T \rgn*_3 \mid \drt_5] \natty),\ 
    \drt_1 \mapsto \set{\op \T \rgn*_2 \mid \drt_4},\ 
    \drt_3 \mapsto \set{\op \T \rgn*_3 \mid \drt_5}}, \\
   \bigset{
    &\alpha_3 \le \alpha_1,\ \rgn*_1 \le \rgn*_2 \le \rgn*_3,\ \drt_2 \le \drt_4 \le \drt_5
    }\Bigr)

  \pos(\alpha) &= \set{\alpha} &
  \neg(\alpha) &= \emptyset \\
  \pos(A \to \C) &= \neg(A) \cup \pos(\C) &
  \neg(A \to \C) &= \pos(A) \cup \neg(\C) \\
  \pos(E^{\rgn}) &= \set{\rgn} &
  \neg(E^{\rgn}) &= \emptyset \\
  \pos(\C \hto \D) &= \neg(\C) \cup \pos(\D) &
  \neg(\C \hto \D) &= \pos(\C) \cup \neg(\D)

  \pos(A \E \set{\op_1 \T \rgn_1, \dots, \op_n \T \rgn_n \mid \drt}) &= \pos(A) \cup \set{\rgn_1, \dots, \rgn_n, \drt} \\
  \neg(A \E \Drt) &= \neg(A)

  \gc(\cstr) = {}
    &\set{(\alpha^- \le \alpha^+) \in \cstr \mid \alpha^- \in N, \alpha^+ \in P} \cup {} \\
    &\set{(\rgn^- \le \rgn^+ \cup \uniq{i}{\rgn*_i}) \in \cstr \mid \rgn^- \in N, \rgn^+ \in P} \cup {} \\
    &\set{(\inst \in \rgn^+ \cup \uniq{i}{\rgn*_i}) \in \cstr \mid \rgn^+ \in P} \cup {} \\
    &\set{(\drt^- \le \drt^+) \in \cstr \mid \drt^- \in N, \drt^+ \in P}

  P_0 = {} &\pos(A) \cup \neg(\ctx) \\
  P = {}
      &P_0 \cup \set{\rgn*_i \mid (\rgn^- \le \rgn^+ \cup \uniq{i}{\rgn*_i}) \in \cstr, \rgn^- \in N, \rgn^+ \in P_0} \cup {} \\
      &\hphantom{P_0 \cup {}}\set{\rgn*_i \mid (\inst \in \rgn^+ \cup \uniq{i}{\rgn*_i}) \in \cstr, \rgn^+ \in P_0} \\
  N = {} &\neg(A) \cup \pos(\ctx)  

  \drt_1^- \le \drt_2^+, \drt_1^- \le \drt_3^+, \drt_4^- \le \drt_2^+

  (\alpha_1 \to[\drt] \alpha_2) \to \alpha_3 \to[\drt] \alpha_4
  \while \alpha_3 \le \alpha_1, \alpha_2 \le \alpha_4

  (\alpha \to[\drt] \beta) \to \alpha \to[\drt] \beta

  \inst_1 \in \rgn*, \inst_2 \in \rgn*, \rgn_1 \le \rgn*, \rgn_2 \le \rgn*

  (\rgn \dotminus \rgn*_1 \dotminus \rgn*_2 \dotminus \rgn*_3) \cup
  (\rgn \dotminus \rgn*_2 \dotminus \rgn*_3 \dotminus \rgn*_4) \cup
  (\rgn \dotminus \rgn*_2 \dotminus \rgn*_3 \dotminus \rgn*_5)

  \alpha \E \set{\kord{lookup} \T \rgn_1, \kord{raise} \T \rgn_2 \mid \drt} \hto
  \beta \E \set{\kord{lookup} \T \rgn_1 \dotminus \rgn, \kord{raise} \T (\rgn_2 \dotminus \rgn') \cup \rgn'' \mid \drt}

  \alpha \hto[\kord{lookup} \T \dotminus\rgn,\ \kord{raise} \T \dotminus\rgn', +\rgn''] \beta

  \kord{compose} \defeq \fun{f} \val (\fun{g} \val (\fun{x}
          \letin{y = f \, x}
          g \, y))

  \inferrule{
    \inferrule{
      \ctx \ent f \T \alpha_f \while \emptyset \\
      \ctx \ent x \T \alpha_x \while \emptyset
    }{
      \ctx \ent f \, x \T \alpha_1 \E \drt_1 \while \alpha_f \le (\alpha_x \to[\drt_1] \alpha_1)
    }
    \and
    \inferrule{
      \ctx, y\!:\!\alpha_y \ent g \T \alpha_g \\
      \ctx, y\!:\!\alpha_y \ent y \T \alpha_y
    }{
      \ctx, y\!:\!\alpha_y \ent g \, y \T \alpha_2 \E \drt_2 \while \alpha_g \le (\alpha_y \to[\drt_2] \alpha_2)
    }
  }{
    \inferrule{
      \inferrule{\ctx \ent \letin{y = f \, x} g \, y \T \alpha_2 \E \drt_3 \while \cstr}{\vdots}
    }{
      \emptyset \ent \kord{compose} \T \alpha_f \to[\drt_5] \alpha_g \to[\drt_4] \alpha_x \to[\drt_3] \alpha_2 \while \cstr
    }
  }

  \cstr = \set{
    \alpha_f \le (\alpha_x \to[\drt_1] \alpha_1),
    \alpha_g \le (\alpha_y \to[\drt_2] \alpha_2),
    \alpha_1 \le \alpha_y, \drt_1 \le \drt_3, \drt_2 \le \drt_3
  }

  \sol = \set{
    \alpha_f \mapsto (\alpha_3 \to[\drt_6] \alpha_4),
    \alpha_g \mapsto (\alpha_5 \to[\drt_7] \alpha_6)  
  }

  \cstr' = \set{
    \alpha_x \le \alpha_3,
    \alpha_4 \le \alpha_1 \le \alpha_y \le \alpha_5,
    \alpha_6 \le \alpha_2,
    \drt_6 \le \drt_1 \le \drt_3,
    \drt_7 \le \drt_2 \le \drt_3  
  }

  \kord{compose} \T (\alpha_3 \to[\drt_6] \alpha_4) \to[\drt_5] (\alpha_5 \to[\drt_7] \alpha_6) \to[\drt_4] (\alpha_x \to[\drt_3] \alpha_2) 
  N = \set{\alpha_x, \alpha_4, \alpha_6, \drt_6, \drt_7}
  \qquad \text{and} \qquad
  P = \set{\alpha_2, \alpha_3, \alpha_5, \drt_3, \drt_4, \drt_5}

  \gc(\cstr') = \set{
    \alpha_x \le \alpha_3,
    \alpha_4 \le \alpha_5,
    \alpha_6 \le \alpha_2,
    \drt_6 \le \drt_3,
    \drt_7 \le \drt_3  
  }

  \kord{compose} \T
    (\alpha \to[\drt] \beta)
    \to (\beta \to[\drt'] \gamma)
    \to (\alpha \to[\drt \cup \drt'] \gamma)

  \kord{count\_print} \defeq \fun{c} \val (&\handler \\
    &\case \val x \mapsto \val 0 \\
    &\case \call{c}{\kord{print}}{y}{k} \mapsto \letin{n = k \, \unt} \val (\succ n)\\
  &)

  \C \defeq \alpha_\text{in} \E \set{\kord{print} \T \rgn_\text{in} \mid \drt_\text{in}}
  \qquad\text{and}\qquad
  \D \defeq \alpha_\text{out} \E \set{\kord{print} \T \rgn_\text{out} \mid \drt_\text{out}}

  \big\{
    (\unitty \to \D) \le (\unitty \to \alpha_2 \E \drt_2),
    \alpha_3 \le \natty,
    \alpha_2 \le \alpha_3,
    \drt_2 \le \drt_3,
    \drt_4 \le \drt_3, \\
    \natty \E \drt_1 \le \D,
    \natty \E \drt_3 \le \D,
    \alpha_c \le \kord{channel}^{\rgn*},
    \rgn_\text{in} \le \rgn_\text{out} \cupdot \rgn*,
    \drt_\text{in} \le \drt_\text{out}
  \big\}

  \kord{channel}^{\rgn} \to[\drt_0]
  (\alpha_\text{in} \E \set{\kord{print} \T \rgn_\text{in} \mid \drt_\text{in}}
  \hto \natty \E \set{\kord{print} \T \rgn_\text{out} \mid \drt_\text{out}})

  \kord{channel}^{\rgn_1} \to (\alpha \E \set{\kord{print} \T \rgn_2 \mid \drt}
  \hto \natty \E \set{\kord{print} \T \rgn_2 \dotminus \rgn_1 \mid \drt})

  \letin{h = (\kord{count\_print} \, \kord{std})} (\withhandle{h}{c})

        \sol = \sol_v \cup \smash{\bigcup_i \sol_i} \cup \smash{\bigcup_i \sol_i'} \cup \Big\{
        &\alpha_\text{in} \mapsto A,
        \alpha_\text{out} \mapsto B,
        \drt_\text{in} \mapsto \set{\hash{\inst}{\op} \in \Drt \mid \op \not\in \ops}, \\
        &\drt_\text{out} \mapsto \set{\hash{\inst}{\op} \in \Drt' \mid \op \not\in \ops},
        (\rgn*_i \mapsto \Rgn_i)_i, \\
        &(\rgn_\text{in}^\op \mapsto \set{\inst \mid \hash{\inst}{\op} \in \Drt})_{\op \in \ops}, \\
        &(\rgn_\text{out}^\op \mapsto \set{\inst \mid \hash{\inst}{\op} \in \Drt'})_{\op \in \ops}
        \Big\}
      
        \ctx &\ent[F_1, F_2, F, \rgn, \rgn*, \drt] \call{e_1}{\op}{e_2}{\cont{y}{c}} \T A \E \set{\op \T \rgn \mid \drt} \\
          &\while \cstr_1, \cstr_2, \cstr, A_1 \le E^{\rgn*}, A_2 \le A^\op, \rgn* \le \rgn, \Drt \le \set{\op \T \rgn \mid \drt}
      
        \sol'' = \extend{\sol_1 \cup \sol_2 \cup \sol'}{\rgn \mapsto \Rgn, \rgn* \mapsto \Rgn, \drt \mapsto \set{\hash{\inst}{\op'} \in \Drt \mid \op' \ne \op}}
      
        \ctx \ent[F_1, F_2, \alpha, \drt] \withhandle{e}{c} \T \alpha \E \drt
          \while \cstr_1, \cstr_2, A' \le (\C' \hto \alpha \E \drt)
      
        \sol' = \extend{\sol_1 \cup \sol_2}{\alpha \mapsto A, \drt \mapsto \Drt}
      
        \sol'(A')
        = \sol_1(A')
        \le \C \hto A \E \Drt
        \le \sol_2(\C') \hto A \E \Drt
        = \sol'(\C' \hto \alpha \E \drt) 
      
  \sol(\drt) \defeq \bigcup_{\ministack{(\drt^- \le \drt) \in \cstr \\ \drt^- \in N}} \sol'(\drt^-)

  \sol(\drt^+) = \bigcup_{\drt^- \le \drt^+} \sol'(\drt^-) \subseteq \bigcup_{\drt^- \le \drt^+} \sol'(\drt^+) = \sol'(\drt^+)

  \sol(\alpha) \defeq \bigcup_{\ministack{(\alpha^- \le \alpha) \in \cstr \\ \alpha^- \in N}} \sol'(\alpha^-)

  \boolty \cup \boolty &= \boolty \qquad&
  (A_1 \to \C_1) \cup (A_2 \to \C_2) &= (A_1 \cap A_2) \to (\C_1 \cup \C_2) \\
  \natty \cup \natty &= \natty &
  E^{\Rgn_1} \cup E^{\Rgn_2} &= E^{\Rgn_1 \cup \Rgn_2} \\
  \unitty \cup \unitty &= \unitty &
  (\C_1 \hto \D_1) \cup (\C_2 \hto \D_2) &= (\C_1 \cap \C_2) \hto (\D_1 \cup \D_2) \\
  \emptyty \cup \emptyty &= \emptyty &
  (A_1 \E \Drt_1) \cup (A_2 \E \Drt_2) &= (A_1 \cup A_2) \E (\Drt_1 \cup \Drt_2)

  \sol(\rgn) \defeq \Big(\bigcup_{\ministack[0.6]{(\rgn^- \le \rgn \cup \uniq{i}{\rgn*_i}) \in \cstr \\ \rgn^- \in N}} \big(\sol'(\rgn^-) - \uniq{i}{\sol'(\rgn*_i)}\big)\Big) \cup \Big(\bigcup_{{(\inst \in \rgn \cup \uniq{i}{\rgn*_i}) \in \cstr}} \big(\set{\inst} - \uniq{i}{\sol'(\rgn*_i)}\big)\Big)

  \big(\sol'(\rgn^-) - \uniq{i \in J}{\sol'(\rgn*_i)}\big) \subseteq \sol(\rgn') \cup \uniq{i \in I}{\sol'(\rgn*_i)}

  \big(\set{\inst} - \uniq{i \in J}{\sol'(\rgn*_i)}\big) \subseteq \sol(\rgn') \cup \uniq{i \in I}{\sol'(\rgn*_i)}

  \sol(\rgn) \subseteq \sol(\rgn') \cup \uniq{i \in I}{\sol'(\rgn*_i)}

  \sol(\rgn) \subseteq \sol(\rgn') \cup \uniq{i \in I}{\sol(\rgn*_i)}

and .

A careful reader might have observed that not all parameters  occur in ,
only those that appear in constraints 
where  and  (and similar ones for instances).
However, this is easy to fix.

First, take  to be  and the set of all parameters  that appear in \emph{any} singleton union in \emph{any} constraint in .
In this case, the above reasoning is valid and the set of constraints  is equivalent to .
Now, repeat the whole process and take  to be  and all parameters that appear in any singleton union in any constraint in .
Again,  is equivalent to  and so also to .
However, the only region constraints left in  are ones of the form 
with  and , thus .

In the end, let us show that  is unified if  is.
We can consider only the case for type constraints as for other cases the proof is almost exactly the same.
Take constraints  and  in .
Since , these two constraints must also be in ,
which is unified, hence  is in  as well.
From our assumption we have  and ,
therefore .
We can similarly show that if we have ,
then .
\end{proof}


\end{document}
