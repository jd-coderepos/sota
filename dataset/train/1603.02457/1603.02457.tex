\documentclass[journal]{acm_proc_article-sp}

\usepackage{graphicx,algorithm,algpseudocode,algorithmicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{xspace}
\usepackage{xcolor}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{thmprime}{Contrapositive}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{claim}{Claim}
\def\S{\ensuremath{\mathcal{S}}\xspace}
\def\Sp{\ensuremath{\mathcal{S}^\prime}\xspace}
\def\reopt1{Reopt-CSP\ensuremath{_{\mathcal{M}_+}}\xspace}
\def\reoptk{Reopt-CSP\ensuremath{_{\mathcal{M}_{k^+}}}\xspace}
       
\title{On Self-Reducibility and Reoptimization of Closest Substring Problem}

\numberofauthors{1}
\author{ Jeffrey A. Aborot,   Henry Adorna,  Jhoirene B. Clemente		\\
\affaddr{Algorithms and Complexity Laboratory}\\
		\affaddr{Department of Computer Science}\\
		\affaddr{College of Engineering}\\
		\affaddr{University of the Philippines Diliman}\\
		\email{jeffrey.aborot@up.edu.ph, ha@dcs.upd.edu.ph, jbclemente@up.edu.ph}
}

\begin{document}
\maketitle

\pagestyle{empty}
\thispagestyle{empty}

\begin{abstract}
In this paper, we define  the  reoptimization variant of the closest substring problem (CSP) under sequence addition. We show that, even with the additional information we have about the problem instance, the problem of finding a closest substring is still NP-hard. We investigate the combinatorial property of optimization problems called self-reducibility. We show that problems that are polynomial-time reducible to  self-reducible problems also exhibits the same property. We illustrate this in the context of CSP. We used the property to show that although we cannot improve the approximability of the problem, we can improve the running time of the existing PTAS for CSP.

\end{abstract}

\begin{keywords}
reoptimization, closest substring problem, approximation
\end{keywords}



\section{Introduction}
The \textit{consensus pattern problem} is a combinatorial problem applied to a wide range of applications from string matching problems in genomic sequences \cite{Bodlaender1995} to finding repeated patterns in graphs \cite{Dondi2013} and time-series databases \cite{Li2010}. Due to its utmost importance in the field of genomics, several efforts have been made to characterize the computational requirements needed to solve the problem. If the set of input instances is constrained only to a set of input strings, the problem is known as the \textit{closest substring problem} (CSP). The problem seeks to identify a pattern that occurs approximately in each of the given set of sequences. 

It has been shown that the CSP is NP-hard \cite{Garey1979}, i.e., unless P=NP, we cannot obtain a polynomial-time algorithm solving the problem. Moreover, it was also shown that  fixing  parameters such as pattern length or alphabet size, does not address the intractability of the problem \cite{Evans2003}. Aside from parameterization attempts, other studies tried to relax the condition of always finding the optimal solution by providing approximation algorithms for the problem. The first constant-factor approximation algorithm is presented in \cite{Lanctot1998}, then subsequently improved in  \cite{Li1999}. These results include CSP to the class of problems that are constant-factor approximable (APX). In addition to this, several studies  \cite{Li1999,Ma2000,Ma2008} even presented a \textit{polynomial-time approximation scheme} (PTAS) for the problem.

To address the intractability and to improve the solution quality of approximation algorithms, another approach is to use additional information about the problem instance whenever possible. A method called \textit{reoptimization} has already been applied to a variety of hard problems in the literature. The idea of reoptimization is to make use of a solution to a locally modified version of the input instance. It was shown that reoptimization can help to either improve the approximability and even provide a PTAS for some problems that are APX-hard. These results include improvements for the metric-traveling salesman problem \cite{Bockenhauer2008}, the Steiner tree problem \cite{Bilo2012}, the common superstring problem  \cite{Bilo2011}, and hereditary graph problems \cite{Boria2012}.

In this paper, we investigate whether reoptimization can help in approximating the closest substring problem. The additional information given in advance is a solution to a smaller instance of the problem. The algorithm for the reoptimization variant of the problem aims to make use of the given solution to become feasible for the larger instance. We show that, using the given solution as a greedy partial solution to the larger instance, we can achieve an additive error (with respect to the optimal solution) that grows linearly as we increase the number of sequences added to the original instance, which can be worse than the existing -approximation algorithms for the original problem. 

The self-reducibility property of some hard combinatorial problem has been showed to improve the approximability of the problem. However, existing general approaches incur a much longer time for providing a solution with improved quality. Providing an improved ratio is already possible for CSP, due to the existence of  a PTAS.   However, \cite{Wang2008} deemed the PTASs in the literature as impractical for small error bounds. In this paper, we showed that it is possible to improve the running time of the existing PTAS  for CSP while maintaining the same approximation ratio through reoptimization.

\section{Closest Substring Problem}
Given a set of sequences  defined over some alphabet , where each  is of length , and for some , find a string  of length  and a set of substrings  each from , where   such that the total Hamming distance  is minimized \cite{Garey1979}.

The closest substring of a given set of input sequences, may not be unique given the closest substring  occurrences , for .  So instead of focusing on the closest substring , we will use the collection of 's to represent a feasible solution for the problem. We can represent a feasible solution  using a sequence of substrings obtained from each of the given sequences to \S. We may refer to SOL as a feasible solution and OPT as the optimal solution for input instance \S.

We can easily obtain a substring  from the collection of occurrences SOL. This is done by aligning all substrings in SOL and taking the majority symbol for each column of the alignment. Let us call this string the \textit{consensus} of SOL. Moreover, the consensus of  is called the closest substring of \S. It is easy to see that the consensus of OPT will minimize the total Hamming distance for all  . On the other hand, given a string , we can also obtain a set of occurrences SOL by searching the best aligning substrings from each of the sequences in \S. Given the closest substring, we can obtain OPT in . 

Naively, we can search for the optimal solution by considering all possible locations of the closest substring in each sequence  from . For smaller alphabet sizes, it is easier to search over all  as compared to  possible occurrences. However, since we are considering the general CSP, we do not restrict the alphabet size of the given sequences. 


\subsection{Related Works}
Algorithms for the CSP can be categorized into three classes: \textit{exact}, \textit{heuristics}, and \textit{approximate}. Exact algorithms always obtain the optimal solution, but are exhaustive and impractical due to the NP-hardness of the problem. Among the earliest exact algorithms, some use graphs to model the problem. This includes WINNOWER \cite{Pevzner2000} which involves finding cliques in graphs obtained from the set of sequences. An integer linear programming (ILP) formulation of the problem was also presented in \cite{Zaslavsky2006}.

Some algorithms make use of some strategy in searching through the set of all feasible solutions. Algorithms following this approach  are called \textit{heuristic algorithms}. A majority of these results use probabilistic models to represent solutions. When it is possible to guarantee the quality of the solution by means of identifying the bounds of its solution's cost, it is called an \textit{approximation algorithm}. Since we do not know the cost of an optimal solution, certain properties of the input instances and the problem itself are needed to design the algorithm. For the CSP, the first constant-factor -approximation algorithm was presented in \cite{Lanctot1998}, which was subsequently  improved in \cite{Li1999}.

Due to the hardness results presented in \cite{Bodlaender1995}, several other efforts were made to identify  for which types of input instances the problem becomes easier to solve. A result in \cite{Brejova2006} shows that even if the set of input instances is defined over the binary alphabet, we still cannot obtain a practical polynomial-time algorithm for small error bounds.  Aside from characterizing input instances, one line of research focused on the \textit{parameterization} of the problem  \cite{Evans2003}. Based on these studies, it is shown that the problem is \textit{fixed-parameter intractable}, i.e., fixing a parameter such as the pattern length or alphabet size will not make the problem easier to solve.

A PTAS for a hard problem is set of polynomial-time algorithms  for which one can specify a certain guaranteed quality. However, since there is a trade-off between the quality of solution and the  running time of the algorithm, we can expect a longer running time for smaller error bounds. 

The summary of the approximation ratio and running time of the existing PTASs for CSP is shown in Table \ref{table:summary}. The second on the list is a randomized PTAS for CSP, while the third one assumes a general alphabet . We can see that the approximation ratio of the third PTAS is only dependent on the sampling size , because the alphabet size is compensated in the running time of the algorithm. In contrast, we have the alphabet size as a parameter in the approximation ratio of the first PTAS. 

\begin{table}
\centering
\resizebox{0.495\textwidth}{!}{  
\def\arraystretch{2}
\begin{tabular}{| c | c | c |}
\hline
\textbf{PTAS} & \textbf{Approximation Ratio} & \textbf{Running Time} \\ \hline
\textbf{[Li1999]} &  & \\ \hline
\textbf{[Li1999]} &  &  \\ \hline
\textbf{[Ma2000]} &  & \footnote{where } \\ \hline 

\end{tabular} }
\caption{Summary of approximation ratio and running time of the existing PTASs for CSP in the literature.}
\label{table:summary}
\end{table}

We consider the first PTAS in Table \ref{table:summary} for our study, since we will  only focus on deterministic algorithms for reoptimization. Moreover, we assume that the set of input instances are obtained from  a general alphabet  and so choosing the third PTAS may not be ideal in our case since it will not just have  as a parameter but also the alphabet size.

The first PTAS from \cite{Li1999} is shown in Algorithm \ref{alg:ptas_csp}. For each parameter , it describes an approximation algorithm for CSP that outputs a solution SOL with    in  time.

An -\textit{sample} from a given instance , i.e., 
 is a collection of  -length substrings  from .  Repetition of substrings are  allowed for as long as no two substrings are obtained from the same sequence.  Let  denote the set of all possible  -\textit{sample} from .  The total number of 
samples in  is   which is bounded above by .  From each sample, the algorithm obtains a consensus pattern. Solution SOL is then derived by aligning the  closest substrings from the given consensus. The Algorithm \ref{alg:ptas_csp} minimizes  through all possible -\textit{sample} in \S  to provide a feasible solution with a guaranteed quality.

\begin{algorithm}
\textbf{Input:} {Set of sequences , {pattern length} , {sampling size }} \\
\textbf{Output:} {SOL and consensus string } 
\begin{algorithmic}[1]

\State{}
\For{\textbf{each} -  from }
\State{ consensus pattern from }
\State{ }
\For{\textbf{each} }
\State{    }
\State{ merge  to SOL }
\EndFor
\State{\textbf{return} SOL with }
\EndFor
\end{algorithmic}
\caption{PTAS for the CSP [Li1999]}
\label{alg:ptas_csp}
\end{algorithm}

\section{Self-reducibility}  \label{sec:selfreducibility}
A solution to a combinatorial optimization problem is composed of a set of discrete elements called \textit{atoms} \cite{Zych2012}. For certain graph problems, it can be the set of all vertices or set of all edges, e.g., a clique in the maximum clique problem is a collection of vertices.

A problem  is said to be \textit{self-reducible} if there is a polynomial-time algorithm, , satisfying the following conditions \cite{Zych2012}.
\begin{enumerate}
\item Given an instance  and an atom  of a solution to ,  outputs an instance . We require that the size of  is smaller than the size of , i.e., . Let  represent the set of feasible solutions to  containing  the atom . We require that every solution SOL of , i.e., , has a corresponding  and that this correspondence is one-to-one.
\item For any set  it holds that the 

\end{enumerate}

Given the properties of a self-reducible problem, we prove that the following lemma is true. 

\begin{lemma}
CSP is \textit{self-reducible}.
\end{lemma}

\begin{proof}
With the assumption that the pattern length  is constant, a valid input instance  to CSP is a set of  sequences . A feasible solution  is an ordered set  . The set of atoms in CSP are all possible -length substrings in .
Let us define a reduction function , which returns a reduced instance .
The reduced instance is derived by removing one sequence   where  is obtained. We argue next that  has the following properties.

\begin{enumerate}
\item For a  there is a corresponding  . Note that,   guarantees at least one occurrence of the closest substring per sequence. For any atom , a solution  corresponds to , i.e.,
 
Instead of using the operation 
`' for sets, we used  to denote the merge of an element to a sequence.
\item For any feasible solution  to the reduced instance , we can obtain a feasible solution  for  with   equal to the sum of  and , since\\

\resizebox{0.46\textwidth}{!}{
\begin{tabular}{l l}
 &  \\
& ,\\
\end{tabular}
}
where  is the consensus of .
\end{enumerate}
\end{proof}

Let us use the concept of polynomial-time reduction for combinatorial problems. We say that a problem  is polynomial-time reducible to , denoted by , if  a polynomial-time transformation , which for every input 

In other words, in order to solve problem , we must at least solve problem . The problem of finding the closest substring is reduced to a graph problem in \cite{Pevzner2000}. The transformation is as follows. Given a set of sequences  and a pattern length , an edge weighted  -partite graph  is obtained, where the problem is reduced to finding  a minimum weighted clique on a -partite graph (MWCP). Each substring in  represents a vertex in . A part  represents the set of vertices obtained from a single sequence . A vertex  is connected to all other vertices in  except those belonging to . The cost defined by function  between two vertices can be interpreted as the Hamming distance between two substrings in . The cost of a clique in , which is computed by getting the sum of all the edges is equal to sum of all pairwise Hamming distances of substrings in SOL in . Moreover, we can create an instance  from  in polynomial-time. Thus, showing a polynomial-time reduction from CSP to MWCP. 

It is also shown that MWCP has an exact reduction to  Minimum Weighted Independent Set Problem (MWISP), since a clique in a graph is an independent set in the corresponding complement of the graph \cite{Garey1979}. In line with this we would like to cite the following Lemma from \cite{Zych2012}. 

\begin{lemma}
Maximum Weighted Independent Set Problem (MWISP) is \textit{self-reducible}.
\end{lemma}

Since we proved earlier that CSP is self-reducible and we know that there is a polynomial-time reduction from CSP to MWCP, which is equivalent to a self-reducible problem MWISP, we are interested to know if all problems that are polynomial-time reducible to self-reducible problems also exhibit the same property.


\begin{theorem} \label{thm:gen_selfreduc}
If problem  is polynomial-time reducible to problem  (), and  is self-reducible, then   is self-reducible. 
\end{theorem}

\begin{proof}
Let  and \\
 be two NP optimization problems. Let , , where  is composed of atoms . Similarly, let , , where  is composed of atoms .

Given that , then by definition, there exists a polynomial-time computable function  such that  for every instance  for problem ,  is an instance of problem  and for every solution  to , . Equivalently, as a decision problem, the polynomial-time reducibility implies
  
By definition of self-reducibility, if  is self-reducible then we can obtain a self-reduction function , such that  and the following conditions hold.
\begin{enumerate}

\item For  there is a corresponding  , where .

\item For a subset of atoms ,   .  
\end{enumerate}

Given that  is self-reducible, we need to show that problem  is also self-reducible.
If so, we must construct a self-reduction function  that follows the two properties.
 
Given the polynomial-time function  and the self-reduction function , we realize  using  through the following,
  
The self-reduction function for  clearly inherits the two conditions because of our premise that  is self-reducible. Moreover, the reduction function runs in polynomial-time since  is polynomial-time computable.
 \end{proof}
 
Theorem \ref{thm:gen_selfreduc} is presented in the hope that we can use the current approaches for the reoptimization variants of  clique and independent set problem for providing improvements for CSP. It is shown that for some defined reoptimization variant of clique and independent set, a general method is shown to improve the approximability with trade-off on the running time of the approximation algorithm.

\section{Reoptimization}

For real-world applications, additional information about the problems we are solving often is available and so we may not have to solve them from scratch. One of the approaches is to make use of a priori information, which can be a solution to a smaller input instance of a problem to solve a larger instance of it. This approach is called \textit{reoptimization}. The idea was first mentioned in \cite{Schaffter1997}. For some problems, we can transform the given optimal solution so that it may become feasible for the modified instance in polynomial-time. Furthermore, this approach can help to improve the approximability of the problem or the running time of the algorithms solving it. In fact, we can obtain a PTAS for a reoptimization variant of some problem given that the unmodified problem has a constant-factor approximation algorithm \cite{Bockenhauer2008}. 

In the reoptimization variant of any problem, it is important to define precisely the modification relation  over the set of input instances.  For graph problems, common modifications involve addition/deletion of edges and vertices. Other types of modification involve changes in the edge/vertex weights. A simple definition of reoptimization is as follows.\\


\noindent INPUT: Original instance , its optimal solution OPT, and a modified instance , where  \\
\noindent OUTPUT: Solution  to .\\

For the CSP, we consider the basic type of modification where one  or several sequences are added to . Since the length of the pattern remains unchanged, we will only define the modification relation over the given set of sequences. When a single sequence is added to , we have the modification relation , where , if  and . As a generalization, we define the modification relation  to denote addition of  sequences to , i.e., , if  and , for .

Let us define the reoptimization variant of the CSP under single sequence addition. In the following definition, we can see that we already have the optimal solution with closest substring pattern , of the original instance . Note that we can easily compute occurrences of  in , i.e., the set of substrings , each from  and their positions where  is obtained.
\begin{definition}{-} \ \\ INPUT: Pattern length , original instance , the optimal closest substring  of , and a modified instance  where \\
\noindent OUTPUT: Solution  to the modified instance .
\end{definition}
Even with the additional information we have, the reoptimization variant of the problem is still NP-hard, i.e., no polynomial-time algorithm exists to obtain the optimal solution for , unless . 
\begin{theorem} 
- is NP-hard.
\label{thm:hard}
\end{theorem}
\begin{proof}
Towards contradiction, assume that - is polynomial-time solvable. We will make use of the polynomial-time algorithm for - to solve the CSP. 

We start by showing that for two sequences, both of length , in , we can obtain the optimal closest substring  in polynomial-time. We can obtain  in  by exhausting all possible -length patterns in both sequences.

By making use of the optimal solution  and a polynomial-time algorithm for -, we can solve the CSP for any number of sequences in  in polynomial-time by adding one sequence at a time. But then, we know that closest substring is NP-hard. Therefore, - is also NP-hard.

\end{proof}

\subsection{Approximation Algorithms for CSP}

For the purpose of our discussion, we may refer to OPT and  to be the optimal solution of the smaller instance and the larger instance of the problem, respectively. The output of the presented approximation algorithms is denoted by , unless otherwise stated. 

First, we show how we can use a  simple algorithm to give an approximate solution for -. The algorithm  makes use of the given solution as a greedy partial solution to the larger instance. 

\begin{algorithm} 
\caption{Given a sequence  and solution SOL of input instance , procedure BEST-ALIGN produces a feasible solution  by aligning the closest substring from  to }
\label{alg:best_align}
\begin{algorithmic}[1]
\Procedure{BEST-ALIGN}{}
\State{ Consensus pattern from SOL}
\State{ }
\For{\textbf{each} -length substring  of } 
\If{ }
\State{}
\State{}
\EndIf
\EndFor
\State{}
\State{\textbf{return} }
\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:best_align} searches for the best aligning substring  to the given solution. In adding one sequence to , the computed solution may or may not be the optimal solution for the larger instance. 

For the first case, if OPT is subset of  for , then the result of Algorithm \ref{alg:best_align} will yield the optimal solution for . Otherwise, there exists a non optimal solution for  that is part of the optimal solution for , i.e., . The solution  of Algorithm \ref{alg:best_align} is obtained by merging the given optimal solution OPT with the best possible aligning substring in the new sequence. Algorithm \ref{alg:best_align} obviously runs in linear time with respect to the length of the additional sequence . 
To illustrate that we cannot always get the optimal solution using Algorithm \ref{alg:best_align}, let us consider the following example.

\begin{example} \label{ex:1}
Let  with  as the additional sequence for the modified input . If we are looking for a closest substring of length , the optimal solution for  is  .  

\begin{center}
\begin{tabular}{ r  c c c c c c c c}
: &  {\bf A} & {\bf A} & {\bf A} & {\bf A} & B & B & B & B \\
 & B & B & B & B &  {\bf A} & {\bf A} & {\bf A} & {\bf A}  \\
 & {\bf A} & {\bf A} & {\bf A} & {\bf A} & B & B & B & A \\
 & B & B & B & B &  {\bf A} & {\bf A} & {\bf A} & {\bf A}  \\ \\

 &  {\bf B} & {\bf B} & {\bf B} & {\bf B} & B & B & B & B \\
\end{tabular}
\end{center}

Algorithm \ref{alg:best_align} will return a solution by aligning the most similar substring  from the new sequence . The solution produced by Algorithm \ref{alg:best_align} will have . On the other hand,  the optimal solution for  is  with . 
\end{example}

In this case, the subset of the optimal solution for the larger instance is not the optimal solution for the smaller instance.  Algorithm \ref{alg:best_align} obviously runs in linear time with respect to the length of the additional sequence , multiplied by getting the cost of each substring . 





To get the quality of the approximation algorithm, we need to compare  and . Using Algorithm \ref{alg:best_align} and the second property of self-reducibility, we have
 where  is the best aligning substring to  from the new sequence. We know that the cost of the optimal solution for the smaller instance is less than or equal to the cost of the optimal solution for the larger instance, i.e., . Therefore,

In the worst case scenario, we can get  
Since we only added a single sequence, the quality of the solution depends solely on how long the pattern is.  Using Algorithm \ref{alg:best_align}, we can get  a feasible solution for  in . Compared to the -approximation algorithm for CSP with running time  from \cite{Lanctot1998}, our approach can benefit an improved approximation ratio and running time for instances  where .
Note that, the first property of the self-reducibility of CSP allows us to provide a feasible solution for the modified instance by extending the additional information that we have. We illustrate in the following generalization that it is possible to produce a feasible solution even if we add -sequences to . 
 




\subsection{Generalization}
In this section, we consider the extension of the reoptimization variant where, instead of adding one sequence, the modification relation, denoted by , is characterized by adding  sequences to the original instance. The definition of the generalized reoptimization version under sequence addition is as follows.
\begin{definition}{Reopt-CSP} \ \\ INPUT: Pattern length , original instance , the optimal closest substring  of , and a modified instance  where \\
\noindent OUTPUT: Solution  to the modified instance .
\end{definition}
Since - is a generalization of -, where , we no longer need to show that this variant is also NP-hard. To give an approximate solution, we  can generalize Algorithm \ref{alg:best_align} for -. 

\begin{algorithm} 
\caption{Given  additional sequences , where each  and solution SOL for input instance , procedure K-BEST-ALIGN produces a feasible solution  by aligning the closest substrings from each of the additional sequences to }\label{alg:k_best_align}
\begin{algorithmic}[1]
\Procedure{K-BEST-ALIGN}{}
\State{ Consensus pattern from SOL}
\For{\textbf{each}    in  to }
\State{ }
\For{\textbf{each} -length substring  of } 
\If{ }
\State{}
\State{}
\EndIf
\EndFor
\EndFor
\State{}
\State{\textbf{return} }
\EndProcedure
\end{algorithmic}
\end{algorithm}

Given , we can give a feasible solution for  by getting the best aligning substring  from each of the newly added sequences. Let  be the set of substrings  and let 

be the contribution of the set  to . If  is the same as , then we can always guarantee optimality, otherwise

In the worst case scenario, we can have

The extension of Algorithm \ref{alg:k_best_align} for - can produce a feasible solution in . However, the quality of the solution degrades as we increase the number of sequences added to the original instance. A general method for reoptimization is applied to several self-reducible hard problems to further improve the approximability of the problem but with tradeoff on the running time of the algorithm. The PTAS for the CSP already provides the option of improving the approximation ratio by increasing  the sampling size  in Algorithm \ref{alg:ptas_csp}. As  approaches ,  we can have a solution with error that converges to  but with running time that is comparable to the naive exhaustive search that is exponential in . In the following section, we illustrate how reoptimization can help in improving the running time of the PTAS in \cite{Li1999}.

\subsection{Improving the PTAS for Reopt-CSP}

Let us consider an input instance  for CSP with a given optimal solution for a subset of its sequences. Without loss of generality, suppose we have an optimal solution OPT for the first  sequences in , i.e., . Let us also assume that .

Algorithm \ref{alg:ptas_csp} can provide a feasible solution for \Sp with an approximation ratio of   in . We argue in this section that we can achieve the same approximation ratio in  time using the given OPT for , as presented in Theorem \ref{thm:imptas}

 
In the following approximation algorithm, we implement the PTAS in Algorithm \ref{alg:ptas_csp} by using the given optimal solution OPT.

\begin{algorithm}
\caption{Approximation algorithm for Reopt- using the K-BEST-ALIGN procedure from Algorithm \ref{alg:k_best_align}.}
\label{alg:imptas}
\noindent \textbf{Input: } Set of sequences , pattern length , and optimal closest substring OPT of  \\
\textbf{Output:}  Solution  for .\\

\begin{algorithmic}[1]
\State{} \\

\State{}
\State{}
\For{\textbf{each} - }
\State{ K-BEST-ALIGN}

\If{ }
\State{}
\State{}
\EndIf
\EndFor \\

\If{}
\State{\textbf{return} }
\Else{}
\State{\textbf{return} }
\EndIf
\end{algorithmic} 
\end{algorithm}

\begin{theorem}
\label{thm:imptas}
Algorithm \ref{alg:imptas} is a  PTAS for  Reopt-CSP  which runs in .
\end{theorem} \vspace{3pt}

\begin{proof}
To prove that Algorithm \ref{alg:imptas} provides the same approximation ratio as Algorithm \ref{alg:ptas_csp}  from \cite{Li1999}, we need to show that the set of all samples in  is considered in Algorithm \ref{alg:imptas}.  is obtained by aligning   to  other sequences in \Sp using Algorithm \ref{alg:best_align}. Since we have the optimal solution for \S,   is equal to the minimum over all feasible solutions sampled over . On the other hand,  solution  is obtained by getting the minimum over all feasible solutions sampled from . The output  is the minimum between  and . Thus, all samples from  are considered   in Algorithm \ref{alg:imptas}.

For the running time of Algorithm \ref{alg:imptas}, getting  will require  steps. Line 5 will iterate in .  Getting the best alignment for each sample will require  and getting the distance between two -length substrings will take . Thus, Algorithm \ref{alg:imptas} runs in .
\end{proof}

\section{Conclusion and Future Work}

In this paper, we presented reoptimization variant of the CSP under sequence addition. The additional information, i.e., optimal solution to a smaller instance, does not help to address the intractability of the problem, as shown in Theorem \ref{thm:hard}.

We considered the basic type of modification , where the new instance contains an additional sequence. The general approach in reoptimization is to transform the given optimal solution for it to become feasible to the new instance. For some cases, i.e., , our simple transformation can lead to , otherwise we can have the worst case scenario where OPT cannot help to obtain . In fact, even for a small modification such as ,   can be totally different from .

We generalized this approach by considering the modification , characterized by adding  new sequences to the original instance. By using the same approach for Reopt-CSP, we can obtain an error that grows  as the number of additional sequences is increasing. However, we showed that by using the given optimal  solution for reoptimization we can improve the running of the existing PTAS for CSP. With the same approximation ratio, we can improve the running time from  to . 

It is natural to consider the opposite scenario, where an optimal solution to a larger instance is given and we are looking for the solution to a smaller subset of the instance. We argue that we are not also guaranteed  to get the optimal solution by transforming the larger solution to provide a feasible solution to the smaller instance. Example \ref{ex:1} can also illustrate the idea. Exploring other techniques to make use of such kind of information is still part of our future work. Moreover, we would like to study other types of modification in the input instance such as changes in pattern length . 

\section{Acknowledgements}
The authors would like to give their utmost gratitude to Dr. Hans-Joachim B\"{o}ckenhauer and Dr. Dennis Komm for patiently reading and proof-reading this paper, to Prof. Juraj Hromkovic for his supervision and valuable inputs.

Ms. Jhoirene Clemente would like to thank the  Engineering Research and Development for Technology (ERDT) for funding her PhD program in University of the Philippines Diliman and the Bases Conversion and Development Authority (BCDA) project for funding her Sandwich program.

\bibliography{library}
\bibliographystyle{plain}
\end{document}