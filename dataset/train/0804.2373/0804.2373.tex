\documentclass{elsart}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{amsmath,amssymb}
\usepackage{theorem}
\usepackage{url}
\usepackage{version}
\usepackage{alltt}



\def\C {\ensuremath{\mathbb{C}}}
\def\Q {\ensuremath{\mathbb{Q}}}
\def\N {\ensuremath{\mathbb{N}}}
\def\R {\ensuremath{\mathbb{R}}}
\def\F {\ensuremath{\mathbb{F}}}
\def\Z {\ensuremath{\mathbb{Z}}}
\def\H {\ensuremath{\mathbb{H}}}
\def\K {\ensuremath{\mathbb{K}}}
\def\S {\ensuremath{\mathbb{S}}}
\def\M {\ensuremath{\mathsf{M}}}

\def\E{\textsf{Expand}}
\def\D{\textsf{Decomp}}

\def\mul{\mathrm{mul}}
\def\rev{\mathrm{rev}}
\def\gathen#1{{#1}}
\def\hoeven#1{{#1}}

\newtheorem{Definition}{Definition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Corollary}{Corollary}
\newtheorem{Proposition}{Proposition}
\newtheorem{Lemma}{Lemma}
\newtheorem{Remark}{Remark}

\def\proof{\textsc{Proof.} }
\def\foorp{\hfill}



\begin{document}

\begin{frontmatter}
\title{Fast Conversion Algorithms \\ for Orthogonal Polynomials}
\author{Alin Bostan} 
\ead{Alin.Bostan@inria.fr} \qquad
\author{Bruno Salvy} 
\ead{Bruno.Salvy@inria.fr} 
\address{Algorithms Project, INRIA Rocquencourt \\ 78153 Le Chesnay Cedex France} \medskip
and
\author{{\'E}ric Schost} 
\ead{eschost@uwo.ca}
\address{ORCCA and Computer Science Department, Middlesex College, \\
University of Western Ontario, London, Canada}


\begin{abstract}
We discuss efficient conversion algorithms for orthogonal polynomials. We describe a known 
conversion algorithm from an arbitrary orthogonal basis to the monomial basis,
and deduce a new algorithm of the same complexity for the converse operation.
\end{abstract}


\begin{keyword}
Fast algorithms, transposed algorithms, basis conversion, orthogonal polynomials.
\end{keyword}


\maketitle 
\end{frontmatter}



\section{Introduction}

Let ,  and  be
sequences with entries in a field~. We can then define the
sequence  of orthogonal polynomials in  by
,  and for  by the second order
recurrence

Following standard conventions, we require that  is non-zero
for all ; in particular,  has degree  for all  and  forms a basis of the -vector space
.

Basic algorithmic questions are then to perform efficiently the base
changes between the basis  and the monomial basis
. More precisely, for , we study the following problems.

\medskip
\begin{description}
\item[Expansion Problem ().] Given , compute the coefficients on the monomial basis of the
polynomial  defined by the map
 
\item[Decomposition Problem ().] Conversely, given the
coefficients of  on the monomial basis, recover the coefficients
 in the decomposition~\eqref{eq:exp} of
 as a linear combination of the 's.
\end{description}

\medskip
For , let  be the coefficient of  in ,
and let  be the  matrix with entries
. Problem  amounts to multiplying the
matrix  by the vector ;
hence, the inverse map  is well-defined, since  is an
upper-triangular matrix whose -th diagonal entry  is non-zero. As we will see, the {\it dual problem}
(multiplying the matrix  by a vector), denoted
, plays an important role as well.

Naive algorithms work in complexity  for both problems 
and . Faster algorithms are already known, see details below on
prior work. The only new result in this article is the second part of
Theorem~\ref{Theo:1} below; it concerns fast computation of the map . 

As usual, we denote by  a \emph{multiplication time} function,
such that polynomials of degree less than  in  can be
multiplied in  operations in~, when written in the monomial
basis. Besides, we impose the usual super-linearity conditions
of~\cite[Chap.~8]{GaGe99}.  Using Fast Fourier Transform algorithms,
 can be taken in  over fields with suitable roots
of unity, and in  over any
field~\cite{ScSt71,CaKa91}.

\begin{Theorem}\label{Theo:1}
  Problems \emph{} and \emph{} can be solved in
   arithmetic operations in .
\end{Theorem}

The asymptotic estimates of Theorem~\ref{Theo:1} also hold for
conversions between any arbitrary orthogonal bases, using the monomial
basis in an intermediate step. In conjunction with FFT algorithms for
polynomial multiplication, Theorem~\ref{Theo:1} shows that all such
base changes can be performed in \emph{nearly linear time.}




\paragraph*{Previous work.} 
Fast algorithms are known for problems closely related to Problem
. {}From these, one could readily infer fast algorithms for
Problem  itself.

In~\cite{PoStTa98}, the question is the computation of the values
 where the  are the
Chebyshev points . This is done by expanding
 on the Chebyshev basis and applying a
discrete cosine transform. The article~\cite{DrHeRo97} studies the
transposed problem: computing the map
 The algorithm in~\cite{DrHeRo97} is
(roughly, see~\cite{PoStTa98} for details) the transpose of the one
in~\cite{PoStTa98}: it applies a transposed multipoint evaluation, then 
a transposed conversion, to either the monomial or the Chebyshev basis.

Regarding problem  to the best of our knowledge, no  algorithm has appeared before, except for particular
families of polynomials, like Legendre~\cite{Frumkin95},
Chebyshev~\cite{Pan98} and Hermite~\cite{LeRoCh07}. In the case of
arbitrary orthogonal polynomials, the best complexity result we are
aware of is due to Heinig~\cite{Heinig01}, who gives a
 algorithm for solving inhomogeneous linear systems
with matrix . {}From this, it is possible to
deduce an algorithm of the same cost for Problem .

In~\cite{PoStTa98}, one sees mentions of left and right inverses for
the related problem
 In~\cite{LeRoCh07}, the
inverse of the map~\eqref{eq:DrHeRo97} is discussed: when  are
the roots of~, Gauss' quadrature formula shows that this map is
orthogonal, so that inversion reduces to transposition. In other
cases, approximate solutions are given.

The various algorithms mentioned up to now have costs
 or .  In~\cite{BoSaSc08b}, we
give algorithms of lower cost  for many classical orthogonal
polynomials (Jacobi, Hermite, Laguerre, \dots), for both Problems
 and .



\paragraph*{Main ideas.}
Here is a brief description of the strategy used to obtain the
complexity estimate of Theorem~\ref{Theo:1}. The complete treatment
with detailed algorithms is given in Sections~\ref{sec:expand}
and~\ref{sec:decomp}. Three main ingredients are used:  a
 algorithm for Problem ;  the
transposition principle;  the Favard-Shohat theorem.

We first recast \eqref{eq:1} into the matrix recurrence
, where  is a  polynomial matrix. Problem  then
amounts to computing .  This is done by using a divide-and-conquer algorithm
similar to the one in~\cite[Th.~2.4]{Gerhard00} for the conversions
between Newton and monomial bases. Assuming for simplicity that  is
even, we rely on the decomposition , with
1mm] T_1 &=& \alpha_{ \frac{n}{2}}{\bf
M}^{( \frac{n}{2})} + \cdots + \alpha_{n-1} {\bf
M}^{(n-1)} \cdots {\bf M}^{( \frac{n}{2}
)}.A=\alpha_0 F_0 + \cdots + \alpha_{n-1} F_{n-1}.{\bf M}^{(i,i+1)} = \left [ \begin{matrix} 0 & 1 \\ c_{i+1} & a_{i+1} x + b_{i+1} \end{matrix} \right ],\left [ \begin{matrix} F_{i} \\ F_{i+1} \end{matrix} \right ]
 = {\bf M}^{(i,i+1)} \left [ \begin{matrix} F_{i-1} \\ F_{i} \end{matrix} \right ].\left [ \begin{matrix} F_{j-1} \\ F_{j} \end{matrix} \right ]
 = {\bf M}^{(i,j)} \left [ \begin{matrix} F_{i-1} \\ F_{i} \end{matrix} \right ];\left [ \begin{matrix} A \end{matrix} \right ]
= \left [ \begin{matrix} \alpha_0 & \alpha_1 \end{matrix} \right ]
            \left [ \begin{matrix} F_0 \\ F_1 \end{matrix} \right ]  + \cdots + 
            \left [ \begin{matrix} \alpha_{n-2} & \alpha_{n-1} \end{matrix} \right ]
            \left [ \begin{matrix} F_{n-2} \\ F_{n-1} \end{matrix} \right ],
\left [ \begin{matrix} A \end{matrix} \right ] =  
\left [ \begin{matrix} \alpha_0 & \alpha_1 \end{matrix} \right ] {\bf M}^{(1,1)}
            \left [ \begin{matrix} F_0 \\ F_1 \end{matrix} \right ]  + \cdots + 
            \left [ \begin{matrix} \alpha_{n-2} & \alpha_{n-1} \end{matrix} \right ] {\bf M}^{(1,n-1)}
            \left [ \begin{matrix} F_0 \\ F_1 \end{matrix} \right ] \\
= {\bf B} \left [ \begin{matrix} F_0 \\ F_1 \end{matrix} \right ],{\bf L}^{(j,i)} = {\bf M}^{(2^{d-j}i+1,\, 2^{d-j}(i+1)+1)}.\label{eq:linearcomb}
 {\bf
v}^{(d-1,i)}=[\alpha_{2i}~\alpha_{2i+1}] \quad\text{and}\quad {\bf
v}^{(j,i)} = {\bf v}^{(j+1,2i)} + {\bf v}^{(j+1,2i+1)} {\bf
L}^{(j+1,2i)}.
1mm]
{\bf for}  {\bf do}\\
\> {\bf for}  {\bf do}\\
\> \>  \>  \> \1mm]
\> \>  \>  \> \1mm]
\> \>  \>  \> \1mm]
{\bf return} 
\end{tabbing}
\end{minipage}
}\end{center}
\caption{Algorithm solving Problem }
\label{algo:texpand}
\end{figure}

The transpose of this map is denoted by ; by
identifying  with its dual, one sees that 
maps  to . In~\cite{BoLeSc03,HaQuZi04},
details of the transposed versions of plain, Karatsuba and FFT
multiplications are given, with a cost matching that of the direct
product. Without using such techniques, writing down the
multiplication matrix shows that  is  Using standard multiplication algorithms, this formulation
leads to slower algorithms than those
of~\cite{BoLeSc03,HaQuZi04}. However, here  and  are of the same
order of magnitude, and only a constant factor is lost.

Using this tool, the transposed expansion algorithm in
Figure~\ref{algo:texpand} is obtained by reversing the flow of the
direct one in Figure~\ref{algo:expand}. The loops are traversed in
opposite order. Then, the operation  in the inner
loop is replaced by a truncated copy of  into  and a transposed matrix-vector product, where
polynomial multiplications are replaced by transposed
multiplications. To perform truncations and transposed
multiplications, we need information on the degrees of the polynomials
involved. By induction, we get the following inequalities, for 
and ,  This information enables us to write the
transposed algorithm in Figure~\ref{algo:texpand}. Using either the
transposition principle or a direct analysis, one sees that the cost
of this algorithm is .






\section{Decomposition Problem}\label{sec:decomp}

The Favard-Shohat theorem~\cite{Favard35,Shohat36}, see
also~\cite[Theorem 4.4]{Chihara78}, asserts that for  as
in~\eqref{eq:1}, there exists a linear form  for which
 is {\it formally orthogonal}, in the sense that, for ,
 
The linear form  is specified by its {\it moments} , for , or equivalently by the generating series 

For completeness, we give in the following theorem a self-contained,
constructive, proof of this classical result, showing how to compute
truncations of . The proof is inspired by the presentation
in~\cite[Section~3]{Flajolet80}.

\begin{Theorem}
Let  be the sequence satisfying  and recurrence~\eqref{eq:1}.
 Define the sequence  by  ,  and, for 
  
Then, there exists a -linear form  such that
	
Moreover, for any , the following equality holds between truncated series in
:
	
\end{Theorem}

\proof For , write  and
. Let also define . These polynomials satisfy the recurrences

for , which 
can be recast into the matrix form

Taking determinants, we deduce that for  the following identity holds

Applying it to  and denoting , we get  that for ,

A separate check shows that Equation~\eqref{eq:takeDet} also holds for . 

For ,  has constant coefficient
, which is non-zero, and is thus invertible in
. Since the  are non-zero as well,
Equation~\eqref{eq:takeDet} shows that the sequence
 is Cauchy and thus convergent in
. Besides, if we let  be its limit, summing up
Equation~\eqref{eq:takeDet} for  yields

 Write  and define the linear form  on
   by . Then Equation~\eqref{eq:momentSeries} is
  a direct consequence of~\eqref{eq:S}.
  
  For , equating coefficients of  and
 in Equation~\eqref{eq:S} multiplied by 
implies  for  and .  By linearity, this shows that 
also satisfies Equality~\eqref{eq:L}.  \foorp


\begin{figure}[t]
\begin{center} \fbox{
\begin{minipage}{9 cm}
\begin{center}  \end{center}
\vspace{0.5cm}

\textbf{Input:} 
 and \\
\textbf{Output:}  such that  \\
 
\begin{tabbing}
 \\ [1mm]
 \\ [1mm]
\\ [1mm]
 \\ [1mm]
 \\ [1mm]
\\ [1mm]
 \\  [1mm]
 \\  [1mm]
, for \\ [1mm]
 for  , where \ {\bf F}_n^t \, {\bf H}_n\, {\bf F}_n \,=\, {\bf D}_n,
\quad\text{or}\quad {\bf F}_n^{-1} \, = \, {\bf D}_n^{-1}\, {\bf
F}_n^t \, {\bf H}_n.
Equation~\eqref{eq:L} shows that one can compute the entries of
 in  operations. 

At this stage, all elements of  and  are
known. Right-multiplication of  by the coefficient vector
of a polynomial  amounts to the transposed
multiplication of the polynomial  by
, that can be performed in time . Using the transposed expansion algorithm
 of the previous section, multiplication by  costs . Finally, multiplying by  takes linear time. This concludes the proof of Theorem~\ref{Theo:1}.

\smallskip\noindent{\bf Acknowledgments.} This work was supported in part by the French National Agency for Research (ANR Project ``Gecko'') and the Microsoft Research-INRIA Joint Centre.


\bibliographystyle{plain}
\bibliography{BoSaSc08b}

\end{document}
