\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[final]{neurips_2021}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{wrapfig,lipsum}
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathtools}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}

\title{R-Drop: Regularized Dropout for Neural Networks}








\author{Xiaobo Liang\textsuperscript{\normalfont 1} \quad
    Lijun Wu\textsuperscript{\normalfont 2}\thanks{Equal contribution (listed in alphabetical order).} \quad
    Juntao Li\textsuperscript{\normalfont 1} \quad
    Yue Wang\textsuperscript{\normalfont 1} \quad
    Qi Meng\textsuperscript{\normalfont 2} \\
    {\bf Tao Qin}\textsuperscript{\normalfont 2} \quad
    {\bf Wei Chen}\textsuperscript{\normalfont 2} \quad
    {\bf Min Zhang}\textsuperscript{\normalfont 1} \quad
    {\bf Tie-Yan Liu}\textsuperscript{\normalfont 2} \quad
    \\ 
    \textsuperscript{1}Soochow University, \textsuperscript{2}Microsoft Research Asia \\
    \texttt{xbliang3@stu.suda.edu.cn, \{ljt,minzhang\}@suda.edu.cn, wangyuenlp@gmail.com} \\
    \texttt{\{lijuwu,meq,taoqin,wche,tyliu\}@microsoft.com} \\
}




\begin{document}

\maketitle

\begin{abstract}
Dropout is a powerful and widely used technique to regularize the training of deep neural networks.
Though effective and performing well, the randomness introduced by dropout causes unnegligible inconsistency between training and inference. 
In this paper, we introduce a simple consistency training strategy to regularize dropout, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other.
Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout.
Theoretical analysis reveals that R-Drop reduces the above inconsistency.
Experiments on  widely used deep learning tasks ( datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 EnglishGerman translation ( BLEU) and WMT14 EnglishFrench translation ( BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models.
Our code is available at GitHub\footnote{\url{https://github.com/dropreg/R-Drop}}.

\end{abstract}

\section{Introduction}
In recent years, deep learning has achieved remarkable success in various areas, e.g., natural language processing, computer vision, speech/audio processing, etc. 
When training a deep neural network, regularization techniques ~\cite{srivastava2014dropout,wan2013regularization,ioffe2015batch,ba2016layer,wu2018group,szegedy2016rethinking,hinton2015distilling,zhang2019your} are indispensable to prevent over-fitting and improve the generalization ability of deep models.
Among them, the dropout technique~\cite{hinton2012improving}, the most widely used one, aims to prevent co-adaptation and performs implicit ensemble by simply dropping a certain proportion of hidden units from the neural network during training.
Existing literature~\citep{ma2016dropout,zolna2017fraternal} has revealed the possible side effect of dropout that there is an unnegligible inconsistency between training and inference stage of dropout models, i.e., the randomly sampled sub model (caused by dropout) during training is inconsistent with the full model (without dropout) during inference.
Through imposing  regularization on the inconsistent hidden states~\citep{ma2016dropout,zolna2017fraternal}, current methods can mitigate the inconsistency problem to some extent but are far from being widely used.


In this paper, we introduce a simple yet more effective alternative to regularize the training inconsistency induced by dropout, named as R-Drop.
Concretely, in each mini-batch training, each data sample goes through the forward pass twice, and each pass is processed by a different sub model by randomly dropping out some hidden units. 
R-Drop forces the two distributions for the same data sample outputted by the two sub models to be consistent with each other, through minimizing the bidirectional Kullback-Leibler (KL) divergence between the two distributions. 
That is, R-Drop regularizes the outputs of two sub models randomly sampled from dropout for each data sample in training. 
In this way, the inconsistency between the training and inference stage can be alleviated.
Compared with the dropout strategy in conventional neural network training, R-Drop only adds a KL-divergence loss without any structural modifications. 

From the perspective of deep neural network regularization, our proposed R-Drop can be treated as a new variation of dropout.
Different from most of the previous methods that merely work on the hidden units of each layer (e.g., the standard dropout~\cite{hinton2012improving}) or model parameters (e.g., dropconnect~\cite{wan2013regularization}), R-Drop works on both the hidden units and the output of sub models sampled by dropout, which is much more effective.
We theoretically analyze the regularization effect of R-Drop, where the result shows that R-Drop can reduce the inconsistency existed in the training and inference.


Though R-Drop regularization is simple, we find it is surprisingly effective through extensive experiments on  tasks with  datasets, spanning from natural language processing, including language modeling, neural machine translation, abstractive summarization, and language understanding, to computer vision, i.e.,  image classification. It creates new records on multiple datasets, such as  BLEU score on WMT14 EnglishGerman and  on WMT14 EnglishFrench translation tasks while only be simply applied to the training of the vanilla Transformer, and also achieves SOTA results on the CNN/DailyMail summarization dataset. 
These universal improvements clearly demonstrate the effectiveness of R-Drop.

Our main contributions are summarized as follows: 
\begin{itemize}
    \item We propose R-Drop, a simple yet effective regularization method built upon dropout, which can be universally applied to train different kinds of deep models. 
    \item We theoretically show that our R-Drop can reduce the inconsistency between training and inference of the dropout based models.
    \item Through extensive experiments on  NLP and  CV tasks with a total of  datasets, we show that R-Drop achieves extremely strong performances, including multiple SOTA results. 
\end{itemize}



\section{Approach}
\label{sec:approach}

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{pic/dropreg_sep.pdf}
    \caption{The overall framework of our proposed R-Drop. We take Transformer~\cite{vaswani2017attention} structure for illustration. The left picture shows that one input  will go through the model twice and obtain two distributions  and , while the right one shows two different sub models produced by dropout.}
    \label{fig:R-Drop}
\end{figure}

The overall framework of our R-Drop regularization method is shown in Figure~\ref{fig:R-Drop}. 
Before elaborating on the details, we first present some necessary notations. 
Given the training dataset , the goal of the training is to learn a model , where  is the number of the training samples,  is the labeled data pair.  is input data and  is the label. For example, in NLP,  can be the source language sentence in machine translation, and  is the corresponding target language sentence. In CV,  can be one image, and  is the categorical class label. 
The probability distribution of the mapping function is also denoted as , and the Kullback-Leibler (KL) divergence between two distributions  and  is represented by . 
In the following, we will explain our proposed R-Drop, training algorithm, and theoretical analysis, respectively.

\subsection{R-Drop Regularization}
We introduce our simple regularization method in this part. Given the training data , the main learning objective for a deep learning model is to minimize the negative log-likelihood loss function, which is as follow:


Since the deep neural networks are prone to over-fitting, regularization methods such as dropout~\cite{srivastava2014dropout} are usually adopted during training to reduce the generalization error of the model. 
Specifically, dropout randomly drops part of units in each layer of the neural network to avoid co-adapting and over-fitting. 
Besides, dropout also approximately performs to combine exponentially many different neural network architectures efficiently~\cite{srivastava2014dropout}, while model combination can always improve the model performance. 
Though simple and effective, there is a huge inconsistency between training and inference that hinders the model performance. That is, the training stage takes the sub model with randomly dropped units, while the inference phase adopts the full model without dropout. Also, the sub models caused by randomly sampled dropout units are also different without any constraints. 
Based on above observations and the randomness of the structure brought by dropout, we propose our R-Drop to regularize the output predictions of sub models from dropout.

Concretely, given the input data  at each training step, we feed  to go through the forward pass of the network twice. 
Therefore, we can obtain two distributions of the model predictions, denoted as  and . 
As discussed above, since the dropout operator randomly drops units in a model, the two forward passes are indeed based on two different sub models (though in the same model). 
As shown in the right part of Figure~\ref{fig:R-Drop}, the dropped units in each layer of the left path for the output prediction  are different from that of the right path for output distribution . 
Thus the distributions of  and  are different for the same input data pair . 
Then, at this training step, our R-Drop method tries to regularize on the model predictions by minimizing the bidirectional Kullback-Leibler (KL) divergence between these two output distributions for the same sample, which is:

With the basic negative log-likelihood learning objective  of the two forward passes:

the final training objective is to minimize  for data :

where  is the coefficient weight to control .
In this way, our R-Drop further regularizes the model space beyond dropout and improves the generalization ability of a model. 
Compared Equation~(\ref{eqn:nll_one}) with Equation~(\ref{eqn:rdrop}), our R-Drop only adds a KL-divergence loss  based on two forward passes in training. 
Note that our regularization methodology can be universally applied on different model structures if there exists randomness in a model (e.g., dropout) that can produce different sub models or outputs. We leave further explorations as future work.

\subsection{Training Algorithm}
\label{sec:train_algorithm}
The overall training algorithm based on our R-Drop is presented in Algorithm~\ref{al:R-Drop}.
As introduced before, at each training step, Line - show that we go forward the model and obtain output distributions  and , then Line - calculate the negative log-likelihood and the KL-divergence between the two distributions.
It is worth nothing that we do not forward the input data twice, instead, we repeat the input data  and concatenate them () in batch-size dimension, which can make forward procedure happen in the same mini-batch to save the training cost.
Finally, the model parameters are updated (Line ) according to the loss of Equation~(\ref{eqn:rdrop}).  The training will continue over the data epochs till convergence.
Compared to the conventional training, our implementation is similar to enlarge the batch size to be double, and one potential limitation is that the computational cost of R-Drop increases at each step. As we show in Section~\ref{sec:cost}, similar to other regularization methods (e.g., training w/ or w/o dropout), though R-Drop needs more training to converge, the final optimum is much better with a superior performance. We also show another study of baseline with doubled batch size in Appendix C.1. 


\begin{algorithm}[!t]
\caption{R-Drop Training Algorithm}
\label{al:R-Drop}
\textbf{Input}: Training data . \\
\textbf{Output}: model parameter .

\begin{algorithmic}[1]
\STATE Initialize model with parameters .
\WHILE{not converged}
\STATE randomly sample data pair ,
\STATE repeat input data twice as  and obtain the output distribution ,
\STATE calculate the negative log-likelihood loss  by Equation~(\ref{eqn:nll}),
\STATE calculate the KL-divergence loss  by Equation~(\ref{eqn:kl}),
\STATE update the model parameters by minimizing loss  of Equation~(\ref{eqn:rdrop}).
\ENDWHILE

\end{algorithmic}
\end{algorithm}


\subsection{Theoretical Analysis}
\label{sec:theoretical}

We analyze the regularization effect of R-Drop in this subsection. 
Let  denote the output of the -th layer of a neural network with input vector , and let  denote a random vector, each dimension of which is independently sampled from a Bernoulli distribution :
{\small}Then the dropout operation on  can be represented by {\small} where  denotes the element-wised product. Hence, the output distribution of the neural network with parameter  after applying dropout is , where .
The objective for R-Drop enhanced training can be formulated as solving the following constrained optimization problem: 
More precisely, R-Drop optimizes the constrained optimization problem in Equation~(\ref{eq:6}) and Equation~(\ref{eq:7}) in a stochastic manner, i.e., it samples two random vectors  and  (corresponding to two dropout instantiations) from Bernoulli distribution and one training instance , and updates the parameter  according to the stochastic gradient  from Equation~(\ref{eqn:rdrop}).


As we presented, one problem for dropout is the inconsistency between the training and inference models. Specifically, the training objective for dropout is the average loss of the sub models, i.e., , while the full model (denoted as ) is used for inference. Our proposed R-Drop enhanced training reduces this inconsistency by forcing the sub structures to be similar.
The following proposition uses a linear model to demonstrate that with the constraint in Equation~(\ref{eq:7}), the inconsistency gap between the average loss of sub structures and the loss of the full model can be bounded (detailed proof can be found in Appendix B).
\begin{proposition}
For a linear model  where  denotes the normalization layer and , with the constraint in Equation (6) in the main paper, we have , where  are the empirical loss calculated by the full model and a random sub model respectively,  is a constant related to the Liptschtz constant of the softmax operator.
\end{proposition}


\subsection{Discussion}
\label{sec:discuss}
The most related works with our R-Drop are ELD~\citep{ma2016dropout} and FD~\citep{zolna2017fraternal}, which also study the consistency training with dropout.
However, R-Drop has key differences with them.
(1) The gap control is from different views. ELD works on directly reducing the gap between the sub model with dropout (train) and the expected full model without dropout (inference), while R-Drop and FD are both working on penalizing the discrepancy between the sub models, the superiority of regularizing the sub models has been proved in FD.
(2) The regularization efficiency is different. ELD only back-propagates the gradients through sub model without the full model, which is less efficient than R-Drop that updates both sub models.
(3) The regularization effect is different. Both ELD and FD use the  distance on hidden states as the regularization loss function. However, this is far away from the main training objective that minimizes the negative log-likelihood over model output distribution.
The distance of hidden states is not in the same space as the probability distribution since log-softmax hugely affects the optimization.
In comparison, R-Drop utilizes the KL-divergence between the output probability distributions as the consistency regularization, which is in the same space as the training objective.
More analysis and experimental comparisons are shown in Appendix C.4.


\section{Experiments}
\label{sec:experiments}
To evaluate our approach and show its universal impact, we conduct experiments on  different tasks, including  natural language processing (NLP) and  computer vision (CV) tasks, which are neural machine translation (NMT) ( datasets), abstractive summarization ( dataset), language understanding ( datasets), language modeling ( dataset), and image classification ( datasets). 
For convenience, we utilize `RD' to represent R-Drop in the tables of experimental results hereinafter. More details of experimental settings for each dataset can be found in Appendix A.


\subsection{Application to Neural Machine Translation}
\label{sec:nmt}

\begin{table}[ht]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l | cc cc cc cc | l}
\toprule
\textbf{Model} & \textbf{EnDe} & \textbf{DeEn} & \textbf{EnFr} & \textbf{FrEn} & \textbf{EnZh} & \textbf{ZhEn} & \textbf{EnEs} & \textbf{EsEn} &
\textbf{Avg}\\
\midrule
Transformer~\cite{vaswani2017attention} & 28.57  &  34.64  & 35.9 & 36.1 & 26.3 & 18.4 & 39.0 & 40.6 & 32.44 \\
\midrule
\textbf{Transformer + RD} & \bf{30.72} & \bf{37.25}  & \bf{38.0}  & \bf{38.9}  & \bf{28.1}  & \bf{19.5}  & \bf{41.8} & \bf{43.2} & \bf{34.68} \\
\bottomrule
\end{tabular}
}
\caption{
BLEU scores on  IWSLT machine translation tasks.
}
\label{tab:iwslt_all}
\vspace{-0.3cm}
\end{table}

We first evaluate the NMT tasks, which is very important in NLP. 
To best show the effectiveness of our method, experiments are conducted on both low-resource and rich-resource translation tasks.

\begin{wraptable}{r}{7cm}
\vspace{-0.2cm}
\centering
\scalebox{0.92}{
\begin{tabular}{l c c}
\toprule
\textbf{Method} & \textbf{EnDe} & \textbf{EnFr}  \\
\midrule
Transformer~\cite{vaswani2017attention} & 29.12  & 42.69 \\
\midrule
MUSE~\cite{zhao2019muse} & 29.90 & 43.50 \\
Depth Growing~\cite{wu2019depth} & 30.07 & 43.27 \\
Transformer-Admin~\cite{liu2020very} & 30.10 & 43.80 \\
Data Diversification~\cite{nguyen2020data} & 30.70 & 43.70 \\
BERT-fused NMT~\cite{zhu2019incorporating} & 30.75 & 43.78 \\
\midrule
\textbf{Transformer + RD} & \bf{30.91}  & \bf{43.95} \\
\bottomrule
\end{tabular}}
\caption{
BLEU scores on WMT14 EnDe and EnFr machine translation tasks.
}
\label{tab:wmt_all}
\vspace{-0.3cm}
\end{wraptable}



\noindent{\bf Datasets} The datasets of low-resource scenario are from IWSLT competitions, which include IWSLT14 EnglishGerman (EnDe), EnglishSpanish (EnEs), and IWSLT17 EnglishFrench (EnFr), EnglishChinese (EnZh) translations. 
The rich-resource datasets come from the widely acknowledged WMT translation tasks, and we take the WMT14 EnglishGerman and EnglishFrench tasks.
The IWSLT datasets contain about  training sentence pairs,  valid pairs, and  test pairs. The WMT data sizes are ,  for EnDe and EnFr respectively, valid and test data are from the corresponding newstest data. 



\noindent{\bf Model \& Training} We take the most popular Transformer~\cite{vaswani2017attention} network as our model structure. 
The \texttt{transformer\_iwslt\_de\_en} and \texttt{transformer\_vaswani\_wmt\_en\_de\_big} are the configurations for IWSLT and WMT translations respectively.
The weight  is set as  for all translation tasks.
Implementation is developed on Fairseq~\cite{ott2019fairseq}. 


\noindent{\bf Results}
We calculate the BLEU scores on these tasks for evaluation, following~\cite{zhu2019incorporating}. 
The IWSLT performances are shown in Table~\ref{tab:iwslt_all} and the rich-resource WMT results are in Table~\ref{tab:wmt_all}. 
First, we can see that our R-Drop achieves more than  BLEU score improvements on  IWSLT translation tasks, which clearly shows the effectiveness of our method. 
The results on WMT translations are more impressive. 
After applying our simple method on the basic Transformer network, we achieve the state-of-the-art ({\bf SOTA}) BLEU score on WMT14 EnDe () and EnFr () translation tasks, which surpass current SOTA models, such as the BERT-fused NMT~\cite{zhu2019incorporating} model that leverages large-scale monolingual data, and the Data Diversification~\cite{nguyen2020data} method trained with many translation models. Note that R-Drop is complementary to the above methods, and we believe stronger results can be achieved if we apply R-Drop on their methods and better backbone models beyond Transformer. 




\subsection{Application to Language Understanding}

\noindent{\bf Dataset} 
We further evaluate our proposed approach on the language understanding tasks by fine-tuning the pre-trained models\footnote{We apply our R-Drop on the fine-tuning stage only in this work. R-Drop can also be applied during pre-training. Due to the computational cost, we leave this as future work.}, which are the standard development sets of GLUE~\cite{wang2018glue} benchmark. 
The GLUE benchmark includes  different text classification or regression tasks, which are MNLI, MRPC, QNLI, QQP, RTE, SST-2, STS-B (regression), CoLA. The detailed statistics are in Appendix.


\noindent{\bf Model \& Training}
We take the BERT-base~\cite{devlin2019bert} and strong RoBERTa-large~\cite{liu2019roberta} pre-trained models as our backbones to perform fine-tuning, which are publicly available.
For each task, different random seeds and parameter settings are required, thus we dynamically adjust the coefficient  among  for each setting.
Other configurations are following the previous works~\cite{devlin2019bert,liu2019roberta}.
For the regression task STS-B, we use MSE instead of KL-divergence to regularize the outputs (see Appendix for MSE regularization details).

\noindent{\bf Results}
The evaluation metrics for above  tasks are as follows: The result for STS-B is the Pearson correlation; Matthewâ€™s correlation is used for CoLA; Other tasks are measured by Accuracy.
The results are presented in Table~\ref{tab:glue}.
We can see that R-Drop achieves  points and  points (on average) improvement over the two baselines BERT-base and RoBERTa-large, respectively, which clearly demonstrate the effectiveness of R-Drop. 
Specifically, our RoBERTa-large + RD also surpasses the other two strong models: XLNet-large~\cite{yang2019xlnet} and ELECTRA-large~\cite{clark2019electra}, which are specially designed with different model architecture and pre-training task.

\begin{table*}[ht]
\centering
\renewcommand\arraystretch{1.0}
\resizebox{\textwidth}{!}{
\begin{tabular}{l | cc cc cc cc | c }
\toprule
\textbf{Model} & \textbf{MNLI} & \textbf{MRPC} & \textbf{QNLI} & \textbf{QQP} & \textbf{RTE} & \textbf{SST-2} & \textbf{STS-B} & \textbf{CoLA} &
\textbf{Avg}\\
\midrule
BERT-base~\cite{devlin2019bert} & 83.8 & 85.3 & 90.8 & 91.0 & 68.2 & 92.4 & 89.3 & 62.3 & 82.85 \\
\midrule
\textbf{BERT-base + RD}  & 85.5  & 87.3 & 92.0 & 91.4  & 71.1  & 93.0  &  89.6 & 62.6 & \bf{84.06} \\
\midrule
\midrule
RoBERTa-large~\cite{liu2019roberta} & 90.2 & 90.9 & 94.7 & 92.2 & 86.6 & 96.4 & 92.4 & 68.0 & 88.93 \\
XLNet-large~\cite{yang2019xlnet} & 90.8 & 90.8 & 94.9 & 92.3 & 85.9 & 97.0 & 92.5 & 69.0 & 89.15 \\
ELECRTA-large~\cite{clark2019electra} & 90.9 & 90.8 & 95.0 & 92.4 & 88.0 & 96.9 & 92.6 & 69.1 & 89.46 \\
\midrule
\textbf{RoBERTa-large + RD}  & 90.9  & 91.4  & 95.2  & 92.5  & 88.4  & 96.9  & 92.5 & 70.0 & \bf{89.73} \\
\bottomrule
\end{tabular}
}
\caption{
Fine-tuned model performances on GLUE language understanding benchmark.
}
\label{tab:glue}
\vspace{-0.3cm}
\end{table*}



\subsection{Application to Summarization}

\noindent{\bf Dataset}
Abstractive summarization task is to summarize the long sentence/document into a short sequence/sentence (through generation) with the main content remained. 
For this generation task, we use the CNN/Daily Mail dataset originally introduced by \citet{hermann2015teaching} to evaluate our method.
This dataset contains news documents (source), and their corresponding highlights (target) crawled from CNN and Daily Mail website.
It contains 287,226 documents for training, 13,368 documents for validation and 11,490 documents for test. We follow~\cite{lewis2020bart} to preprocess the dataset.

\noindent{\bf Model \& Training}
To mostly show the effectiveness, we take the super strong pre-trained sequence-to-sequence BART~\cite{lewis2020bart} model as our backbone and fine-tune it using our method. 
In this task, the coefficient weight  is set as  to control the KL-divergence.
For other hyper-parameters, we follow the setting of the original paper~\cite{lewis2020bart} without modification. 

\begin{wraptable}{r}{6cm}
\centering
\scalebox{0.92}{
\begin{tabular}{l c c c}
\toprule
\textbf{Method} & \textbf{RG-1} & \textbf{RG-2} & \textbf{RG-L}  \\
\midrule
Transformer~\cite{vaswani2017attention} & 39.50 & 16.06 & 36.63   \\
ProphetNet~\cite{qi2020prophetnet} & 44.02 & 21.17 & \bf{41.30}   \\
BART~\cite{lewis2020bart} & 44.16 & 21.28 & 40.90   \\
PEGASUS~\cite{zhang2020pegasus} & 44.17 &  21.47 & 41.11  \\
BART + R3F~\cite{aghajanyan2020better} & 44.38 & 21.53 & 41.17   \\
\midrule
\textbf{BART + RD} & \bf{44.51} & \bf{21.58} & 41.24 \\
\bottomrule
\end{tabular}}
\caption{
ROUGE results on CNN/Daily Mail summarization dataset. RG-1, RG-2, RG-L stand for ROUGE-1, ROUGE-2, and ROUGE-L scores. 
}
\label{tab:summarization}
\vspace{-0.3cm}
\end{wraptable}



\noindent{\bf Results}
The performance is evaluated by ROUGE F1 score~\cite{lin2002manual}.
Specifically, we report the unigram ROUGE-1 (RG-1) and bigram ROUGE-2 (RG-2) overlap to assess the informativeness, and the longest common subsequence ROUGE-L (RG-L) score to assess the fluency. 
The results are shown in Table~\ref{tab:summarization}. We can see that R-Drop based training outperforms the fine-tuned BART model by  points on RG-1 and RG-2 score and achieves the {\bf SOTA} performance. 
Specifically, our result also surpasses the PEGASUS method~\cite{zhang2020pegasus}, which brings a novel self-supervised paradigm carefully designed for summarization, and the previous best work BART+R3F~\cite{aghajanyan2020better}, which introduces a parametric noise sampled from normal or uniform distributions. Instead, our R-Drop does not introduce any extra parameters or model structure changes during training.




\subsection{Application to Language Modeling}

\noindent{\bf Dataset} We also evaluate our approach on another widely acknowledged NLP task: language modeling. 
The dataset we choose for this task is the commonly adopted Wikitext-103 dataset~\cite{merity2016pointer}, which is the largest available word-level language modeling benchmark with long-term dependency. 
WikiText-103 contains about  training tokens from
 articles on Wikipedia, and the average length of tokens per article is about . 
The data is preprocessed by following~\cite{ott2019fairseq}.

\noindent{\bf Model \& Training}
We take two models to conduct the language modeling task. One is the basic Transformer decoder~\cite{vaswani2017attention}, another is the more advanced one: Adaptive Input Transformer~\cite{baevski2018adaptive}, which introduces adaptive input embeddings into the Transformer model. 
We use the open-source Fairseq~\cite{ott2019fairseq} toolkit, and the corresponding model configurations are \texttt{transformer\_lm\_gpt} and \texttt{transformer\_lm\_wiki103} for Transformer and Adaptive Input Transformer. 
We simply set the weight  to be  without tuning during training. Other configurations are same as~\cite{ott2019fairseq} and~\cite{baevski2018adaptive}.

\begin{wraptable}{r}{6cm}
\centering
\scalebox{0.92}{
\begin{tabular}{l c c}
\toprule
\textbf{Method} & \textbf{Valid} & \textbf{Test} \\
\midrule
Transformer~\cite{vaswani2017attention} & 25.76 & 26.62  \\
\textbf{Transformer + RD} & \bf{23.97} & \bf{24.94} \\
\midrule
Adaptive~\cite{baevski2018adaptive} & 18.94 &  18.87 \\
\textbf{Adaptive + RD} & \bf{18.18} &  \bf{18.07} \\
\bottomrule
\end{tabular}}
\caption{
Perplexity results on Wikitext-103 language modeling task. Adaptive refers to Adaptive Input Transformer~\cite{baevski2018adaptive}. 
}
\label{tab:lm}
\end{wraptable}



\noindent{\bf Results}
The evaluation metric for language modeling is perplexity, which can well measure the probability of a sentence. Same as~\cite{baevski2018adaptive}, we report the perplexity on both valid and test sets. 
The results are shown in Table~\ref{tab:lm}. 
From the table, we can see that our R-Drop based training improves the perplexity on both two different model structures, e.g.,  perplexity improvement on test set over Adaptive Input Transformer. 
Besides, more improvement can be achieved when the baseline model is not so strong, e.g.,  perplexity gain on valid set and  on test set above the Transformer baseline. 



\subsection{Application to Image Classification}


\noindent{\bf Dataset}
For image classification, we conduct experiments on two widely acknowledged benchmark datasets, i.e., CIFAR-100~\cite{krizhevsky2009learning} and the ILSVRC-2012 ImageNet dataset~\cite{deng2009imagenet} (denoted as ImageNet for short). CIFAR-100 dataset consists of  images of  classes, and there are  images per class with  for training and  for testing.

\begin{wraptable}{r}{6cm}
\vspace{-0.3cm}
\centering
\scalebox{0.92}{
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{CIFAR-100} & \textbf{ImageNet} \\
\midrule
ViT-B/16~\cite{dosovitskiy2021an} & 92.64 & 83.97  \\
\textbf{ViT-B/16 + RD} & \bf{93.29} &\bf{84.38} \\
\midrule
ViT-L/16~\cite{dosovitskiy2021an} & 93.44 & 85.15  \\
\textbf{ViT-L/16 + RD} & \bf{93.85} & \bf{85.57} \\
\bottomrule
\end{tabular}}
\caption{
Accuracy on CIFAR-100 and ImageNet classification tasks.
}
\label{tab:image}
\vspace{-0.5cm}
\end{wraptable}


The ImageNet dataset consists of  image samples of  categorical classes. 
We utilize the same data preprocessing strategies with~\cite{dosovitskiy2021an}, where the details are given in~\cite{kolesnikov2019big}.



\noindent{\bf Model \& Training}
We choose the recent strong and popular Vision Transformer (ViT)~\cite{dosovitskiy2021an} model as our backbone. 
More specifically, we take the two publicly released pre-trained models, ViT-B/16 and ViT-L/16, with  and  parameters respectively, and we conduct model fine-tuning on the CIFAR-100 and ImageNet datasets. 
During fine-tuning, the weight  is set as  for both models, and we set other hyper-parameters/training details to be same as~\cite{dosovitskiy2021an}.



\noindent{\bf Results}
The classification performance is measured by Accuracy, and the results are presented in Table~\ref{tab:image}. 
For CIFAR-100, we achieve about  accuracy improvement over ViT-B/16 baseline, and  points over ViT-L/16 model. 
Similarly, on the large-scale ImageNet dataset, consistent improvements are also obtained. 
These observations demonstrate that our R-Drop can still benefit the model performance even the baseline is powerful. 
In a word, through the above NLP tasks and this image classification task, we clearly show R-Drop is effective and can be universally applied. 



\section{Study}
\label{sec:study}
Beyond the superior experimental results, in this section, we conduct extensive studies on different perspectives to better understand our R-Drop method. 
The analysis experiments are performed on the IWSLT14 DeEn translation task. More studies can be found in Appendix C. 


\subsection{Regularization and Cost Analysis}
\label{sec:cost}
We first show the regularization effect of our R-Drop and study the potential limitation of training cost (as discussed in Section~\ref{sec:train_algorithm}).
Hence, we plot the curves of training/valid loss and valid BLEU along the training update number for Transformer and Transformer + RD models. Besides, we also plot the corresponding curves along the training time (minutes).
The curves are shown in Figure~\ref{fig:cost}. We can observe: 1) Along with the training, Transformer quickly becomes over-fitting, and the gap between train and valid loss of Transformer is large, while R-Drop has a lower valid loss. This well proves that R-Drop can provide persistent regularization during training. 2) At the early training stage, Transformer improves the BLEU score quickly but converges to bad local optima soon. In comparison, R-Drop gradually improves the BLEU score and achieves a much superior performance. Though it needs more training to converge, the final optimum is better. This is same as other regularization methods (e.g., training w/ or w/o dropout). R-Drop indeed increases the training cost at each step since it requires repeating input  for another computation in a mini-batch. Note that this is similar to batch size doubled training without KL-divergence. In Appendix C.1, we conduct this training and show that R-Drop increases negligible cost but with a much stronger performance.

\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.55\textwidth}
\centering
\includegraphics[width=8.4cm]{pic/cost_train.pdf}
\caption{Loss/BLEU curves along with model training.}
\label{fig:cost}
\end{minipage}
\begin{minipage}[t]{0.44\textwidth}
\centering
\includegraphics[width=5.8cm]{pic/step.pdf}
\caption{R-Drop with different step.}
\label{fig:diff_step}
\end{minipage}
\vspace{-0.3cm}
\end{figure}


\subsection{-step R-Drop}
The above study shows that R-Drop can achieve much stronger performance, but with a lower convergence, thus we study another training strategy that is to perform R-Drop every  steps to improve the training efficiency, instead of applying at each step. 
We vary  in  to see the difference, where  is the current training strategy. 
The valid BLEU curves along with training update number and training time are presented in Figure~\ref{fig:diff_step}. 
From the curves, we can conclude that though the convergence is faster with larger , the training fails to fall into good optima, which quickly over-fits, and the BLEU scores become worse and worse when we increase . 
This proves that our R-Drop at each step can well regularize the training and obtain superior performances. 




\subsection{-time R-Drop}

Our method regularizes the model output between two distributions  and , and it is also interesting to see whether more improvements can be achieved if we regularize  distributions for the same input data, where  is the current setting. Therefore, we extend our R-Drop to be:
{\small}, and we take  for a feasible implementation.
The BLEU score for IWSLT14 DeEn test set is  when , which is similar to that when  ( BLEU score). This reflects that R-Drop already has a strong regularization effect between two distributions, without the necessity of stronger regularization. 


\subsection{Two Dropout Rates}

\begin{wrapfigure}{r}{7cm}
\vspace{-0.6cm}
    \centering
    \includegraphics[scale=0.23]{pic/2dropout_signal.pdf}
    \caption{R-Drop with two different dropout rate combinations. Among the  numbers,  are different since the table is symmetric and triangular.} 
    \label{fig:dropout2}
    \vspace{-0.4cm}
\end{wrapfigure}


Besides the above studies, we investigate R-Drop from another perspective, i.e., the dropout values. In current training, the two distributions are based on the same dropout value (e.g.,  for IWSLT translations). In this study, we utilize two different dropout values for the two output distributions during training (e.g.,  for ,  for ) to see the difference. We choose the two dropout rates from  with total  ( for two different rates +  for two same rates) combinations. The results are shown in Figure~\ref{fig:dropout2}. Among these different results, we can see that: 1) Dropout rates with the same value  is the best choice (current setting), 2) R-Drop can stably achieve strong results when the two dropout rates are in a reasonable range () without a big performance difference. One interesting point is that even the two dropout values are both , which means half of the units are expected to be dropped, R-Drop can still obtain a satisfied result ( BLEU) compared with the baseline Transformer ( BLEU). These results all confirm the advantage of our R-Drop, and we are interested in studying more in the future. 



\subsection{Effect of Weight }

\begin{wraptable}{r}{4cm}
\vspace{-0.5cm}
\centering
\scalebox{0.92}{
\begin{tabular}{l c}
\toprule
 & \textbf{TF + RD} \\
\midrule
 & 36.05    \\
 & 36.85    \\
 & \textbf{37.25}    \\
 & 37.20    \\
& 36.95    \\
\bottomrule
\end{tabular}}
\caption{
BLEU scores with different .
}
\label{tab:effect_alpha}
\vspace{-0.3cm}
\end{wraptable}


Further, we investigate the impact of the KL-divergence loss weight . As mentioned in Section~\ref{sec:nmt}, we set  for NMT experiments. Here we vary the  in  and conduct experiments. As shown in Table~\ref{tab:effect_alpha}, small  (e.g., ) can not perform as good as large  (e.g., ), which means we should pay more attention to the KL-divergence regularization. However, too much regularization () is also not good, and the best balanced choice is . Note that the choice of  is distinct for different tasks (e.g., NMT, language understanding), which depends on how easy the over-fitting happens caused by the specific data size and model size of each task. 



\section{Related Work}


\noindent{\bf Regularization Methods.} 
Bigger models always tend to have better performance, 
especially for various large-scale pre-trained models, e.g., Vision Transformer~\cite{dosovitskiy2021an}, Swin Transformer~\cite{liu2021swin}, GPT families~\cite{radford2018improving,radford2019language,brown2020language}, BERT~\cite{devlin2019bert}, BART~\cite{lewis2020bart}, Switch Transformers~\cite{fedus2021switch}, etc.
With millions and even billions of parameters, these deep models are prone to over-fitting, thus requiring regularization strategies to improve their generalization ability~\cite{labach2019survey}.
To tackle with over-fitting, many regularization techniques have been proposed, e.g., weight decay~\cite{krogh1992simple,krizhevsky2012imagenet,kang2016shakeout,wen2016learning}, dropout~\cite{hinton2012improving,wan2013regularization,ba2013adaptive,wang2013fast,srivastava2014dropout}, normalization~\cite{ioffe2015batch,salimans2016weight,ba2016layer,huang2018orthogonal,wu2018group}, adding noise~\cite{hochreiter1995simplifying,poole2014analyzing}, layer-wise pre-training and initialization~\cite{erhan2009difficulty,he2015delving}, label-smoothing~\cite{szegedy2016rethinking}, and so on.
Among which, dropout and its variants are most popular owing to its effectiveness and moderate cost as well as good compatibility with other regularization methods~\cite{moradi2020survey}, which has been successfully applied to regularize a wide range of neural network architectures~\cite{pham2021autodropout}, e.g., convolutional neural network layers~\cite{wu2015towards,devries2017improved}, recurrent neural networks~\cite{gal2016theoretically,semeniuta2016recurrent,merity2018regularizing}, Transformer~\cite{zehui2019dropattention,zhou2020scheduled,wu2021not}.
The success of dropout methods can be interpreted by preventing co-adaptation of neurons and performing an implicit ensemble of sub models from dropout.
Owing to the effect in promoting sparsity of weights and stochastic nature, dropout methods are also adapted to other applications, e.g., contrastive learning for sentence representation learning~\cite{gao2021simcse}, neural network compression~\cite{molchanov2017variational,neklyudov2017structured} and model uncertainty estimation~\cite{gal2016dropout}.

Unlike previous researches of designing specific dropout variants or adapting dropout to different applications, we consider to further regularize the model on the success of dropout.
Specifically, any two sub models sampled from dropout are encouraged to produce consistent model prediction for an input data by utilizing KL-divergence in the training stage.
That is, we conduct regularization on the model output level.
In doing so, the sub model outputs produced by the randomness of dropout are regularized to reduce the parameter freedom, which will enhance generalization in inference.


\noindent{\bf Consistency Training.} 
Besides regularization methods, our work also relates to a few works of consistency training on dropout models or data augmentation.
Among them, the most representative methods are ELD~\citep{ma2016dropout}, FD~\citep{zolna2017fraternal}, and Cutoff~\citep{shen2020simple}.
As discussed in Section~\ref{sec:discuss}, ELD only focuses on the inconsistency between the sub model with dropout (train) and the expected full-model without dropout (inference), while FD works between the sub models only (consistence between two sub models).
Both ELD and FD utilize  to regularize the hidden space.
Instead, our R-Drop performs consistency training on dropout from the output space with a more effective bidirectional KL loss.
Unlike the above consistency training method on sub models, Cutoff resembles launching consistency training from a data perspective by regularizing the inconsistency between the original data the augmented samples with part of the information within an input sentence being erased.


\noindent{\bf Self-distillation.} 
Minimizing the KL-divergence between the output distributions of two different models correlates with knowledge distillation \cite{hinton2015distilling,furlanello2018born,allen2020towards,liang2021mixkd,fang2021seed,zhou2021rethinking}, where the two models refer to teacher and student, respectively.
In our setting, the teacher and student are the dropout instantiations of the same model, thus it resembles self-knowledge distillation~\cite{mobahi2020self} scenario.
Different from existing method that exploits dark knowledge from the model itself~\cite{hahn2019self,gotmare2018closer} or distills knowledge between different layers~\cite{zhang2019your}, our strategy can be regarded as an instance-wise self-knowledge distillation, i.e., each pair of sampled sub models perform distillation between each other for the same input, which also relates to mutual learning~\cite{zhang2018deep} but ours is much more efficient without extra parameters. 



\section{Conclusions and Future Work}

In this paper, we proposed a simple yet very effective consistency training method built upon dropout, namely R-Drop, which minimizes the bidirectional KL-divergence of the output distributions of any pair of sub models sampled from dropout in model training. Experimental results on  popular deep learning datasets show that not only can our R-Drop effectively enhance strong models, e.g., ViT, BART, Roberta-large, but also work well on large-scale datasets and even achieve SOTA performances when combined with vanilla Transformer on WMT14 EnglishGerman and EnglishFrench translations.
Due to the limitation of computational resources, for pre-training related tasks, we only tested R-Drop on downstream task fine-tuning in this work. We will test it on pre-training in the future. In this work, we focused on Transformer based models. We will apply R-Drop to other network architectures such as convolutional neural networks.  

\begin{ack}
We would like to thank the reviewers for their constructive comments.
Juntao Li is the corresponding author.
This work was supported by the National Science Foundation of China (NSFC No. 62036004).
\end{ack}



\bibliographystyle{mybst}
\bibliography{neurips_2021}

\clearpage
\appendix

\section{Detailed Experimental Settings}
\label{sec:experiments_appendix}
We provide more detailed settings for the experiments of each task in this part. 

\subsection{Neural Machine Translation}
For all the NMT tasks, we use the public datasets from IWSLT competitions\footnote{\url{https://iwslt.org/}} and WMT competitions\footnote{\url{https://www.statmt.org/wmt14/translation-task.html}}. We tokenize all the datasets with byte-pair-encoding (BPE) \cite{sennrich2016neural} approach with the dictionary built jointly upon the source and target sentence pairs except the IWSLT17 EnZh translation dataset that is built separately. After tokenization, the resulted vocabularies for IWSLT datasets are near , while for WMT datasets, the vocabulary size is about .

To train the Transformer based NMT models, we use \texttt{transformer\_iwslt\_de\_en} configuration for IWSLT translations, which has  layers in both encoder and decoder, embedding size , feed-forward size , attention heads , dropout value , weight decay . For the WMT experiments, the \texttt{transformer\_vaswani\_wmt\_en\_de\_big} setting has  layers in encoder and decoder, embedding size , feed-forward size , attention heads , dropout value , attention dropout  and relu dropout . The training is optimized with Adam~\cite{kingma2014adam} with . The learning rate scheduler is \texttt{inverse\_sqrt} with default learning rate  and warmup steps . Label smoothing~\cite{szegedy2016rethinking} is adopted with value . Our code implementation is based on open-source Fairseq\footnote{\url{https://github.com/pytorch/fairseq/tree/master/examples/translation}}. We train the IWSLT translations on  GEFORCE RTX 3090 card and the WMT translations on  GEFORCE RTX 3090 cards.

To evaluate the performance, we use \texttt{multi-bleu.perl}\footnote{\url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl}} to evaluate IWSLT14 EnDe and all WMT tasks for a fair comparison with previous works~\cite{zhu2019incorporating,ott2018scaling}. For other NMT tasks, we use \texttt{sacre-bleu}\footnote{\url{https://github.com/mjpost/sacrebleu}}~\cite{post2018call} for evaluation. When inference, we follow~\cite{vaswani2017attention} to use beam size  and length penalty  for WMT14 EnDe, beam size  and penalty  for other tasks. We further report the \texttt{sacre-bleu} evaluated BLEU score on WMT14 EnDe and EnFr tasks to show advanced comparisons, where the corresponded results are  and , respectively. 

\subsection{Abstrative Summarization}
For summarization, we take the pre-trained BART~\cite{lewis2020bart} model as backbone and fine-tune on the CNN/DailyMail dataset\footnote{\url{https://github.com/abisee/cnn-dailymail}}. BART is a pre-trained sequence-to-sequence model based on the masked source input and autoregressive target output, which contains  layers of Transformer encoder and  layers of Transformer decoder, the embedding size is , and the feed-forward size is . Dropout value is . During fine-tuning, we follow the hyper-parameters used in~\cite{lewis2020bart}. The pre-trained model and the backbone implementations are all from Fairseq\footnote{\url{https://github.com/pytorch/fairseq/tree/master/examples/bart}}. The training is conducted on  GEFORCE RTX 3090 GPU cards.

\subsection{Language Modeling}
For language modeling, we train on the Transformer decoder~\cite{vaswani2017attention} and Adaptive Input Transformer~\cite{baevski2018adaptive} models. The configuration for Transformer is \texttt{transformer\_lm\_gpt}, which contains  layers with embedding size  and feed-forward size , attention heads . Dropout and attention dropout are . For Adaptive Input Transformer, the configuration is \texttt{transformer\_lm\_wiki103} with  layers, embedding size , feed-forward size , attention heads , dropout , attention dropout , gelu dropout  and adaptive softmax dropout . We train Transformer model for  steps and Adaptive Input Transformer for  steps. 
The development is based on the code base Fairseq\footnote{\url{https://github.com/pytorch/fairseq/tree/master/examples/language_model}}. The training is on  Tesla V100 GPU cards.


\begin{table}
	\centering
	\scalebox{0.92}{
		\begin{tabular}{l c c c c c c c c}
			\toprule
			\textbf{Hyper-parameter} & \textbf{CoLA} & \textbf{MRPC} & \textbf{RTE} & \textbf{SST-2} & \textbf{MNLI} & \textbf{QNLI} & \textbf{QQP} &\textbf{STS-B} \\
			\midrule
			Learning Rate & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-5 \\
			\midrule
			Max Update & 5336 & 2296 & 3120 & 20935 & 123873 & 33112 & 113272 & 3598 \\
			\midrule
			Max Sentence (Batch) & 16 & 16 & 8 & 32 & 32 & 32 & 32 & 16 \\
			\midrule
			Dropout & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
			\midrule
			Coefficient  & 0.5 & 1.0 & 1.0 & 1.0 & 0.5 & 1.0 & 0.5 & 1.0 \\
			\bottomrule
	\end{tabular}}
	\caption{
		Hyper-parameters when fine-tuning our models on GLUE benchmark.
	}
	\label{tab:nlu_parameter}
\end{table}

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{pic/bert_dr.pdf}
	\caption{Results on  GLUE tasks with different random seeds.} 
	\label{fig:bert_dr}
\end{figure}

\subsection{Language Understanding}
For language understanding tasks, we follow the popular pre-training and fine-tuning methodology, and the fine-tuned sets are the GLUE~\cite{wang2018glue} benchmark. We follow previous works~\cite{clark2019electra,liu2019roberta} to work on the  tasks, including singe-sentence classification tasks (CoLA, SST-2), sentence-pair classification tasks (MNLI, QNLI, RTE, QQP, MRPC), and the sentence-pair regression task (STS-B). The detailed data statistics can be found from the original paper~\cite{wang2018glue}. 

The pre-trained BERT-base model is the Transformer~\cite{vaswani2017attention} encoder network, which contains  layers with embedding size , feed-forward size  and attention heads . Correspondingly, the Roberta-large model contains  layers with embedding size , feed-forward size  and attention heads . During fine-tuning, we use Adam~\cite{kingma2014adam} as our optimizer with , , , and  weight decay of . We select the learning rate in range  and batch size in . Other hyper-parameter settings are mostly same as previous works~\cite{liu2019roberta}. The pre-trained model and the backbone implementations are all from Huggingface Transformers\footnote{\url{https://github.com/huggingface/transformers}}. We report the specific settings of several important hyper-parameters in Table~\ref{tab:nlu_parameter}, including the dropout value. The fine-tuning experiments are conducted on  GEFORCE RTX 3090 GPU card.

Further, to give a clear comparison of our R-Drop based fine-tuning and vanilla fine-tuning, we plot the performance changes from different random seeds over the pre-trained BERT model on each GLUE task. The curves are shown in Figure~\ref{fig:bert_dr}. We can see that consistent improvements are achieved on different random seeds, which means our R-Drop can robustly help improve the model generalization and model performance.
Besides, we also provide some task performances of different  values, shown in Table~\ref{tab:diff_alpha}.
This result demonstrated that  indeed is a sensitive hyper-parameter for each GLUE task, while  is a good choice for most tasks.

\begin{table}
	\centering
	\begin{tabular}{l c c c c}
		\toprule
		\textbf{} & \textbf{0.1} & \textbf{0.5} & \textbf{1.0} & \textbf{1.5} \\
		\midrule
		\textbf{MRPC} & 84.30 & 86.03 & \textbf{86.51} & 85.78 \\
		\textbf{SST-2} & 92.54 & 92.77 & \textbf{93.02} & 92.43 \\
		\textbf{MNLI} & 84.20 & \textbf{84.48} & 83.44 & 82.27 \\
		\textbf{QNLI} & 91.21 & 91.92 & \textbf{92.01} & 91.12 \\
		\bottomrule
	\end{tabular}
	\caption{
		Comparison of the effect of different  for some GLUE tasks.
	}
	\label{tab:diff_alpha}
\end{table}

\paragraph{MSE Regularization}
Our R-Drop is presented under the KL-divergence between two distributions. To extend our method into the regression task, such as STS-B in GLUE, we introduce the MSE-based regularization. For input data , we forward the  two times similarly as in classification and obtain the two predicted values  and . Then we regularize these two predicted values with MSE as follow:

and we add  with conventional MSE loss: . The final optimization objective is:


\subsection{Image Classification}
The image classification task is evaluated with the recent popular Vision Transformer (ViT)~\cite{dosovitskiy2021an} model, which is the same as Transformer but with the image patch data as input. We take the two publicly released models\footnote{\url{https://github.com/jeonsworld/ViT-pytorch}}, ViT-B/16 and ViT-L/16, which are pre-trained on ImageNet-21k~\cite{deng2009imagenet} dataset with  classes and  images in total. ViT-B/16 is a Transformer model with  Transformer encoder layers, embedding size , feed-forward size  and attention heads , while ViT-L/16 with  layers,  embedding size,  feed-forward size and  attention heads. We only conduct the fine-tuning stage experiments on CIFAR-100 and ImageNet. Note that the ImageNet results are computed without additional techniques (Polyak averaging and 512 resolution images) used to achieve results in~\cite{dosovitskiy2021an}. During fine-tuning, the dropout values are  for both models. Fine-tuning is on  GEFORCE RTX 3090 GPU cards.

\section{Theoretical Discussion of R-Drop}
\label{sec:theory_appendix}
In this section, we provide the proof for Proposition 2.1 in the main paper and make some discussions.
\begin{proposition}
	For a linear model  where  denotes the normalization layer and , with the constraint in Equation (6) in the main paper, we have , where  are the empirical loss calculated by the full model and a random sub model respectively,  is a constant related to the Liptschtz constant of the softmax operator.
\end{proposition}
\textit{Proof:} Here, the normalization operator normalize the row of the weight matrix  to be .  
According to the Lipschitz continuity of the loss, we have
where  is the Lipshitz constant.\\
According to the condition  , we have
because the relation between the KL-divergence and the total variation distance. Since we constrain the norm of , for fixed , the softmax operator is a bijection from  to . Suppose  is the Lipschitz constant of the inverse function from  to , we have
For the left term in Eq.(5), we have
, because  independently follow Bernoulli distribution.  Then we have . Combined with Eq.(4), we have 
Let , we can get the result.


\section{More Studies}

\subsection{Batch Size Doubled Training}
\begin{figure}
	\centering
	\includegraphics[scale=0.4]{pic/double_batch.pdf}
	\caption{Results of R-Drop and Transformer with a doubled batch size.} 
	\label{fig:double_batch}
\end{figure}

\label{sec:double_appendix}
As discussed in Section 2.2, we implement the algorithm by repeating input data  once and concatenating the  with repeated one in the same mini-batch to forward once. This is similar to enlarging the batch size to be double at each step. The difference is that half of the data are the same as the other half, while directly doubling the batch size, the data in the same mini-batch are all different. Therefore, we are still interested in the result of directly doubling the batch size to see the performance. We conduct experiments on IWSLT14 DeEn translation with Transformer, and the batch size is enlarged from  to be . The result is  BLEU score. We can see that though slight improvement is achieved (compared to baseline ), it falls far behind our strong performance . For the detailed training cost for each step, we present the number here: Transformer + Double Batch costs near ms per step, while Transformer + DR costs about ms per step. The additional cost is from the KL-divergence loss backward computation. We can see the cost is  times, which is a negligible cost. We also plot the valid BLEU curves along with the training for this study. The curves are shown in Figure~\ref{fig:double_batch}. Compared to this batch size doubled training and our R-Drop, we can clearly see the advantage of R-Drop. With similar training costs, R-Drop gradually improves the performance to a much stronger one. In the figures, we also plot the curve for Transformer with the original batch size training (e.g., ) for a better comparison. 


\subsection{Importance of KL-Divergence}
Our method introduces a KL-divergence between the two distributions from the same sample. In this study, we specifically investigate the importance of KL-divergence loss. Thus, this ablation removes the  loss between the two distributions and only keeps the  loss for training. Similar to other studies, we work on IWSLT14 DeEn translation, and the model is Transformer. The result is also  BLEU score (same as above enlarged batch size), which is slightly better than the Transformer baseline (), but far worse from our R-Drop based training result  BLEU score. This result well demonstrates the importance and effectiveness of our introduced KL-divergence loss. 

\subsection{Training Time and Efficiency}

\begin{table}
	\centering
	\scalebox{0.92}{
		\begin{tabular}{l cc cc cc c}
			\toprule
			\textbf{Minutes} & 10min & 30min & 60min & 90min & 150min & 200min & 300min\\
			\midrule
			\textbf{Baseline} & 27.14 & 33.62 & 34.18 & 34.54 & - & -  & - \\
			\textbf{R-Drop} & 16.06 & 31.56 & 33.61 & 34.91 & 35.74 & 36.13 & 36.39 \\
			\bottomrule
	\end{tabular}}
	\caption{
		Comparison Baseline and R-Drop (reduced half-batch training) BLEU score along with training time on IWSLT14 DeEn translation. 
	}
	\label{tab:half_batch}
\end{table}

To fairly compare the training time between baseline and R-Drop model for the same batch size, We reduce the batch size for the R-Drop model and provide the BLEU score along with training time on IWSLT14 DeEn translation tasks, which is shown in Table~\ref{tab:half_batch}.
From the table, we can see that though R-Drop needs more training to converge, the final model is much better than the baseline.
We can see that the time cost is comparable for reduced half-batch training when reaching the BLEU score equal to the baseline.


\subsection{Experiments for ELD and FD}

\begin{table}
	\centering
	\scalebox{0.92}{
		\begin{tabular}{l cc}
			\toprule
			\textbf{Model} & Acc (CIFAR-100) & BLEU (IWSLT14 DeEn)\\
			\midrule
			Baseline & 77.1  & 34.78 \\
			FD~\cite{zolna2017fraternal} & 77.6 & 35.04 \\
			\textbf{R-Drop} & 78.13 & 37.25 \\
			\bottomrule
	\end{tabular}}
	\caption{
		Comparison of Baseline, FD and R-Drop on IWSLT14 DeEn and CIFAR-100 tasks.
	}
	\label{tab:eld_fd}
\end{table}

To give a better understanding of the advantages of R-Drop over hidden space regularization, we also conduct experimental studies. Since FD~\citep{zolna2017fraternal} has shown its advantage over ELD~\citep{ma2016dropout}, we mainly present experimental comparisons between FD and R-Drop. The experiments are image classification on the cifar-10 dataset (used in FD work) and IWSLT14 DeEn Translation (used in R-Drop work).
The cifar10 experiments are conducted on the released code\footnote{\url{https://github.com/akamaster/pytorch\_resnet\_cifar10}}, and we add R-Drop regularization as our implementation.
For the image classification task, the R-Drop model hyper-parameter  needs to reduce linearly with the learning rate implemented by \texttt{torch.optim.lr\_scheduler.MultiStepLR}.
The IWSLT experiments are on our released code, and we implement the FD same as its released code. From Table~\ref{tab:eld_fd}, we can clearly see that R-Drop is superior to FD on both tasks, which can prove the advantages of the KL-divergence consistency regularization.


\subsection{Ensemble and Weight Averaging}

\begin{table}[ht]
	\centering
	\scalebox{0.9}{
		\begin{tabular}{l | cc cc cc}
			\toprule
			\diagbox{\textbf{Model Settings}}{\textbf{Model Number}} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} \\
			\midrule
			Transformer full-model deep ensembling & 34.78 & 36.06 & 36.37 & 36.59 & 36.80 & 36.92 \\
			Transformer full-model weight averaging (model) & 34.78 & 0 & 0 & 0 & 0 & 0 \\
			Transformer full-model weight averaging (checkpoint) & 34.78 & 35.00 & 35.21 & 35.26 & 35.29 & 35.44 \\
			R-Drop full-model weight averaging (checkpoint) & 37.25 & 37.22 & 37.27 & 37.30 & 37.22 & 37.31 \\
			\bottomrule
	\end{tabular}}
	\caption{
		Comparison of BLEU scores achieved by model deep ensembling or weight averaging with different model (trained with different random seed) and epoch checkpoint (trained with same seed).
	}
	\label{tab:ensemble_wa}
\end{table}


We also compare our R-Drop based training with deep ensembling and weight average methods. 
(1) Deep ensembling usually utilizes multiple models with different parameters, which incurs additional inference costs, including both numbers of model parameters and inference time. 
(2) Parameter/weight averaging directly averages the multiple model parameters, and it can only be useful when the parameters are not far away from each other. 
In contrast, R-Drop aims to make sub models consistent within dropout-based training. It is simple yet effective by adding a consistent regularization loss without any other modifications. The model parameters are not increased, and the inference cost is the same as a single model.
We train several independent models with different parameters (each with dropout) and then do deep ensembling, weight averaging on these full models. The results are show in Table~\ref{tab:ensemble_wa} .
Obviously, R-Drop achieves great performance with a single full model (e.g., 37.25), much better than weight averaging and deep ensembling.
Ensemble methods improve the independently trained full models, but the best result is still from R-Drop (at least with 6 models ensembling here). 
Weight averaging can improve the performance of the base dropout model by averaging nearby checkpoints with the same random seed, but the improvement is relatively small compared with R-Drop. 
Besides, the weights average obtains an extremely low result for different random seeds trained models (BLEU=0) due to the far difference of model parameters.



\end{document}