\subsection{Learnable Feature Augmentation} 
\label{sec:reconstruction_loss}
As the proposed framework implicitly learns the distribution of each identity using disentangled feature embeddings through the generative module,  it can generate a new set of hard samples to improve discriminability. This is achieved by two types of augmentations using a subset of learned features from a query and a subset of learned features from either a positive or negative sample.  
\subsubsection{Positive Feature Augmentation} For positive samples, two kinds of augmentations are considered to increase the diversity of positives. Given a subset  of triplet, the encoder module produces feature embeddings: id-relevant embeddings , and id-irrelevant embeddings  . 
With positive samples, although embeddings  and  can generate images of different identities assigned, we want the model to produce embeddings  and  that can reconstruct images containing distinct id-relevant features.
To deal with this, two augmentation are considered, where the same identity of a query image is assigned to the augmented samples, allowing intra-class variations. We feed augmented input sets and the reconstructed input to the generative module : two augmentations , , and the reconstruction , which all correspond to a person of the query image. The reconstruction loss of these positive feature augmentations measures the similarity between the query samples and the augmented samples: 

where  represents the distribution of samples belonging to the query person, and  is the grayscale image of . In practice, we use the grayscale image as an alternative to  to reduce the impact of color on the similarity measure. 

\begin{figure*}[!t]
    \centering
    \renewcommand{\tabcolsep}{0.77mm}
    \begin{tabular}{cccccc|cccccc}
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/pseudo_query0.png} &  
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/pseudo_query_feature_map0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/cam_query0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_I_q0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_A_q0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/x_n_q0.png} \hspace{0.05cm} & \hspace{0.05cm}
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/pseudo_query1.png}  &  
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/pseudo_query_feature_map1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/cam_query1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_I_q1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_A_q1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/x_n_q1.png} \\
     & {\small } & {\small } &  &  &  & 
     & {\small } & {\small } &  &  &  \vspace{0.1cm} \\
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/pseudo_negative0.png} &  
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/pseudo_negative_feature_map0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/cam_negative0.png} & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_I_n0.png}
    & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_a_n0.png} & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/x_q_n0.png} \hspace{0.05cm} & \hspace{0.05cm} 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/pseudo_negative1.png} &  
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/pseudo_negative_feature_map1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/cam_negative1.png} & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_I_n1.png}
    & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_a_n1.png} & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/x_q_n1.png} \\
     & {\small } & {\small } &  &  &  &
 & {\small } & {\small } &  &  &  \vspace{0.1cm} \\
    \end{tabular}
    \caption{The examples of pseudo ground truths. The pseudo ground truths are generated by mingling features with an id-relevant indicator and features with an id-irrelevant indicator. The gradient does not flow to the connections where both  and  or both  and  are zero. \vspace{-0.2cm}} \label{fig:pseudo-ground-truth}
\end{figure*}
 
\subsubsection{Negative Feature Augmentation}
Given a subset  of triplet, we want the model to learn a feature embedding space, where people of different identities but similar appearances and a person of various appearances are well separable. To create such hard samples, two kinds of augmentation with negative samples are considered by swapping id-relevant embeddings with id-irrelevant embeddings. 
As a result, the generative module generates two augmentations  and  in the feature embedding space. However, there are no comparative references for the augmentations to measure reconstruction capability. We deal with this absence of a ground-truth problem by creating pseudo-ground-truths originating from class activation maps~\cite{zhou2016learning}. To obtain class activation maps, we instantiate a fully-connected layer followed by global average pooling to the output of the backbone network . The fully-connected layer is trained using the conventional cross-entropy function :

Note that we distinguish the loss  denoting classification loss on pseudo-label generation and reconstruction from the loss  in \eref{eq:cls_loss_feature} denoting the classification loss on the feature extraction module~(See \fref{fig:overall_architecture}). 

A class activation map for a particular category indicates the discriminative regions used for identifying that category~\cite{zhou2016learning}. Based on this attention mechanism, we can identify the most represented id-relevant features by projecting back the weights of the fully-connected layer onto the feature embedding and localizing the regions having high-intensity values. Opposite to id-relevant features, id-irrelevant features are presumably selected as the features that less contribute to classifying identities. Two indicators are then defined as  for identifying id-relevant features and  for id-irrelevant features, where  produces the class activation map,  is the average value of the elements of the class activation map, and  is the unit step function. Using the indicators, we create pseudo-ground-truths by re-entangling the decomposed id-relevant features of queries and id-irrelevant features of negatives, and vice versa:

where operation  is the element-wise product, and operation  denotes the intersection of two indicators. The feature re-entanglements in \eref{eq:pseudo_ground_truth} are designed to create feature maps that maintain the id-relevant features of a single person without any contamination by id-irrelevant features from other identities. That is, we exclude the regions relevant to either query or negative identities~(\ie ) when localizing id-irrelevant features, where operations  and  denote the union of two indicators and the complement of a set. The examples of the generated ground-truth feature maps are shown in \fref{fig:pseudo-ground-truth}. 

Like \eref{eq:recon_loss_pos}, the reconstruction loss measures the similarity between the pseudo-ground-truth feature map  and the augmented features:  

where  and  represents the feature distribution of samples belonging to the query person and the negative person, respectively. 

Finally, the total reconstruction loss  is the weighted sum of \eref{eq:recon_loss_pos},  \eref{eq:recon_loss_neg}, and \eref{eq:cam_loss},  as follows:

where ,  and  are the weighting factors. The reconstruction loss in \eref{eq:rec_loss} encourages that the proposed framework consistently forms a single cluster of the embeddings of a person from both hard positive and hard negative samples. The examples of re-entangled features are shown in \fref{fig:mix}, and the examples of activation maps are in \fref{fig:activation}. 
\vspace{0.2cm}