\subsection{Learnable Feature Augmentation} 
\label{sec:reconstruction_loss}
As the proposed framework implicitly learns the distribution of each identity using disentangled feature embeddings through the generative module,  it can generate a new set of hard samples to improve discriminability. This is achieved by two types of augmentations using a subset of learned features from a query and a subset of learned features from either a positive or negative sample.  
\subsubsection{Positive Feature Augmentation} For positive samples, two kinds of augmentations are considered to increase the diversity of positives. Given a subset $\left( x_{\scriptscriptstyle q}, \: x_{\scriptscriptstyle p}\right)$ of triplet, the encoder module produces feature embeddings: id-relevant embeddings $e_{\scriptscriptstyle I, q} \:, \: e_{\scriptscriptstyle I, p}$, and id-irrelevant embeddings  $e_{\scriptscriptstyle A, q} \: , \: e_{\scriptscriptstyle A, p}$. 
With positive samples, although embeddings $e_{\scriptscriptstyle A, q}$ and $e_{\scriptscriptstyle A, p}$ can generate images of different identities assigned, we want the model to produce embeddings $e_{\scriptscriptstyle I, q}$ and $e_{\scriptscriptstyle I, p}$ that can reconstruct images containing distinct id-relevant features.
To deal with this, two augmentation are considered, where the same identity of a query image is assigned to the augmented samples, allowing intra-class variations. We feed augmented input sets and the reconstructed input to the generative module $G$: two augmentations $\left\{ e_{\scriptscriptstyle I, p}, \:e_{\scriptscriptstyle A, q} \right\}$, $\left\{ e_{\scriptscriptstyle I, q}, \:e_{\scriptscriptstyle A, p} \right\}$, and the reconstruction $\left\{ e_{\scriptscriptstyle I, q}, \:e_{\scriptscriptstyle A, q} \right\}$, which all correspond to a person of the query image. The reconstruction loss of these positive feature augmentations measures the similarity between the query samples and the augmented samples: 
\begin{align} \label{eq:recon_loss_pos}
\begin{array}{l}
    \mathcal{L}_{aug}^{p} = 
    \mathbb{E}_{\scriptscriptstyle \{G, x_{\scriptscriptstyle q}\}\sim X_{\scriptscriptstyle q}} \left[\:\left\Vert G\left( e_{\scriptscriptstyle I, p}\:e_{\scriptscriptstyle A, q} \right) 
    - x_{\scriptscriptstyle q^*}\right\Vert_{1}\: \right] \vspace{0.3cm}\\
    \hspace{1cm}\:+\:  \mathbb{E}_{\scriptscriptstyle \{G, x_{\scriptscriptstyle p}\}\sim X_{\scriptscriptstyle q}} \left[\:\left\Vert G\left(
    e_{\scriptscriptstyle I, q}\:e_{\scriptscriptstyle A, p}
    \right) 
    - x_{\scriptscriptstyle p^*}\right\Vert_{1}\: \right] \vspace{0.3cm}\\
    \hspace{1cm}\:+\:
    \mathbb{E}_{\scriptscriptstyle \{G, x_{\scriptscriptstyle q}\}\sim X_{\scriptscriptstyle q}} \left[\:\left\Vert G \left( e_{\scriptscriptstyle I, q}\:e_{\scriptscriptstyle A, q} \right) 
    - x_{\scriptscriptstyle q^*}\right\Vert_{1}\: \right],
\end{array}\end{align}
where $X_q$ represents the distribution of samples belonging to the query person, and $x_{\scriptscriptstyle q^*}$ is the grayscale image of $x_{\scriptscriptstyle q}$. In practice, we use the grayscale image as an alternative to $x_{\scriptscriptstyle q}$ to reduce the impact of color on the similarity measure. 

\begin{figure*}[!t]
    \centering
    \renewcommand{\tabcolsep}{0.77mm}
    \begin{tabular}{cccccc|cccccc}
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/pseudo_query0.png} &  
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/pseudo_query_feature_map0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/cam_query0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_I_q0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_A_q0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/x_n_q0.png} \hspace{0.05cm} & \hspace{0.05cm}
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/pseudo_query1.png}  &  
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/pseudo_query_feature_map1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/cam_query1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_I_q1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_A_q1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/x_n_q1.png} \\
    $x_{\scriptscriptstyle q}$ & {\small $E_b(x_{\scriptscriptstyle q})$} & {\small $E_g(x_{\scriptscriptstyle q})$} & $m_{\scriptscriptstyle I}^{\scriptscriptstyle q}$ & $m_{\scriptscriptstyle A}^{\scriptscriptstyle q}$ & $\tilde{x}_{\scriptscriptstyle n}^{\scriptscriptstyle q}$ & 
    $x_{\scriptscriptstyle q}$ & {\small $E_b(x_{\scriptscriptstyle q})$} & {\small $E_g(x_{\scriptscriptstyle q})$} & $m_{\scriptscriptstyle I}^{\scriptscriptstyle q}$ & $m_{\scriptscriptstyle A}^{\scriptscriptstyle q}$ & $\tilde{x}_{\scriptscriptstyle n}^{\scriptscriptstyle q}$ \vspace{0.1cm} \\
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/pseudo_negative0.png} &  
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/pseudo_negative_feature_map0.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/cam_negative0.png} & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_I_n0.png}
    & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_a_n0.png} & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/x_q_n0.png} \hspace{0.05cm} & \hspace{0.05cm} 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/pseudo_negative1.png} &  
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/pseudo_negative_feature_map1.png} & 
    \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/cam_negative1.png} & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_I_n1.png}
    & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/m_a_n1.png} & \includegraphics[width=0.05\textwidth]{figs/pseudo_ground_truth/viridis/x_q_n1.png} \\
    $x_{\scriptscriptstyle n}$ & {\small $E_b(x_{\scriptscriptstyle n})$} & {\small $E_g(x_{\scriptscriptstyle n})$} & $m_{\scriptscriptstyle I}^{\scriptscriptstyle n}$ & $m_{\scriptscriptstyle A}^{\scriptscriptstyle n}$ & $\tilde{x}_{\scriptscriptstyle q}^{\scriptscriptstyle n}$ &
$x_{\scriptscriptstyle n}$ & {\small $E_b(x_{\scriptscriptstyle n})$} & {\small $E_g(x_{\scriptscriptstyle n})$} & $m_{\scriptscriptstyle I}^{\scriptscriptstyle n}$ & $m_{\scriptscriptstyle A}^{\scriptscriptstyle n}$ & $\tilde{x}_{\scriptscriptstyle q}^{\scriptscriptstyle n}$ \vspace{0.1cm} \\
    \end{tabular}
    \caption{The examples of pseudo ground truths. The pseudo ground truths are generated by mingling features with an id-relevant indicator and features with an id-irrelevant indicator. The gradient does not flow to the connections where both $m_I^q$ and $m_A^n$ or both $m_I^n$ and $m_A^q$ are zero. \vspace{-0.2cm}} \label{fig:pseudo-ground-truth}
\end{figure*}
 
\subsubsection{Negative Feature Augmentation}
Given a subset $\left( x_{\scriptscriptstyle q}, \: x_{\scriptscriptstyle n}\right)$ of triplet, we want the model to learn a feature embedding space, where people of different identities but similar appearances and a person of various appearances are well separable. To create such hard samples, two kinds of augmentation with negative samples are considered by swapping id-relevant embeddings with id-irrelevant embeddings. 
As a result, the generative module generates two augmentations $G\left( e_{\scriptscriptstyle I, q},\: e_{\scriptscriptstyle A, n} \right)$ and $G\left( e_{\scriptscriptstyle I, n},\: e_{\scriptscriptstyle A, q} \right)$ in the feature embedding space. However, there are no comparative references for the augmentations to measure reconstruction capability. We deal with this absence of a ground-truth problem by creating pseudo-ground-truths originating from class activation maps~\cite{zhou2016learning}. To obtain class activation maps, we instantiate a fully-connected layer followed by global average pooling to the output of the backbone network $E_{b}$. The fully-connected layer is trained using the conventional cross-entropy function $\mathcal{L}_{cam}$:
\begin{align}\label{eq:cam_loss}
    \mathcal{L}_{cam} = -\frac{1}{N_b}\sum\limits_{j}^{N_b} y_{\scriptscriptstyle q, j} \log\left( p\left( y_{\scriptscriptstyle q, j} | E(x_{\scriptscriptstyle q, j}) \right)\right), 
\end{align}
Note that we distinguish the loss $\mathcal{L}_{cam}$ denoting classification loss on pseudo-label generation and reconstruction from the loss $\mathcal{L}_{cls}$ in \eref{eq:cls_loss_feature} denoting the classification loss on the feature extraction module~(See \fref{fig:overall_architecture}). 

A class activation map for a particular category indicates the discriminative regions used for identifying that category~\cite{zhou2016learning}. Based on this attention mechanism, we can identify the most represented id-relevant features by projecting back the weights of the fully-connected layer onto the feature embedding and localizing the regions having high-intensity values. Opposite to id-relevant features, id-irrelevant features are presumably selected as the features that less contribute to classifying identities. Two indicators are then defined as $m_{\scriptscriptstyle I}^{*} = u\left( E_{g}( x_{\scriptscriptstyle *} )- \tau \right)$ for identifying id-relevant features and $m_{\scriptscriptstyle A}^{*} = u\left( \tau - E_{g}( x_{\scriptscriptstyle *} ) \right)$ for id-irrelevant features, where $E_{g}(\cdot)$ produces the class activation map, $\tau$ is the average value of the elements of the class activation map, and $u(\cdot)$ is the unit step function. Using the indicators, we create pseudo-ground-truths by re-entangling the decomposed id-relevant features of queries and id-irrelevant features of negatives, and vice versa:
\begin{align} \label{eq:pseudo_ground_truth}
\begin{array}{l}
    \widetilde{x}_{\scriptscriptstyle n}^{\scriptscriptstyle q} = m_{\scriptscriptstyle I}^{q} \otimes E_{b}( x_{\scriptscriptstyle q}) + \left(m_{\scriptscriptstyle A}^{q} \cap m_{\scriptscriptstyle A}^{n}\right) \otimes E_{b}( x_{\scriptscriptstyle n}) \\
    \widetilde{x}_{\scriptscriptstyle q}^{\scriptscriptstyle n} = m_{\scriptscriptstyle I}^{n} \otimes E_{b}( x_{\scriptscriptstyle n}) + \left(m_{\scriptscriptstyle A}^{n} \cap m_{\scriptscriptstyle A}^{q}\right) \otimes E_{b}( x_{\scriptscriptstyle q}),
\end{array}\end{align}
where operation $\otimes$ is the element-wise product, and operation $\cap$ denotes the intersection of two indicators. The feature re-entanglements in \eref{eq:pseudo_ground_truth} are designed to create feature maps that maintain the id-relevant features of a single person without any contamination by id-irrelevant features from other identities. That is, we exclude the regions relevant to either query or negative identities~(\ie $\left(m_{\scriptscriptstyle I}^{q} \cup m_{\scriptscriptstyle I}^{n}\right)^{c} = m_{\scriptscriptstyle A}^{q} \cap m_{\scriptscriptstyle A}^{n}$) when localizing id-irrelevant features, where operations $\cup$ and $c$ denote the union of two indicators and the complement of a set. The examples of the generated ground-truth feature maps are shown in \fref{fig:pseudo-ground-truth}. 

Like \eref{eq:recon_loss_pos}, the reconstruction loss measures the similarity between the pseudo-ground-truth feature map $\widetilde{x}$ and the augmented features:  
\begin{align} \label{eq:recon_loss_neg}
\begin{array}{l}
    \mathcal{L}_{aug}^{n} = 
    \mathbb{E}_{\scriptscriptstyle \{ \widetilde{G}, \widetilde{x}_{\scriptscriptstyle q}\}\sim \widetilde{X}_{\scriptscriptstyle q}} \left[\:\left\Vert \widetilde{G}\left(e_{\scriptscriptstyle I, q},\:e_{\scriptscriptstyle A, n}
    \right)
    - \widetilde{x}_{\scriptscriptstyle n}^{\scriptscriptstyle q} \right\Vert_{1} \right] \vspace{0.3cm}\\
     \hspace{1cm} + \:\mathbb{E}_{\scriptscriptstyle \{ \widetilde{G}, \widetilde{x}_{\scriptscriptstyle n}\}\sim \widetilde{X}_{\scriptscriptstyle n}} \left[\: \left\Vert \widetilde{G}\left( e_{\scriptscriptstyle I, n},\: e_{\scriptscriptstyle A, p}
    \right)
    - \widetilde{x}_{\scriptscriptstyle q}^{\scriptscriptstyle n} \right\Vert_{1} \right],
\end{array}\end{align}
where $\widetilde{X}_q$ and $\widetilde{X}_n$ represents the feature distribution of samples belonging to the query person and the negative person, respectively. 

Finally, the total reconstruction loss $\mathcal{L}_{rec}$ is the weighted sum of \eref{eq:recon_loss_pos},  \eref{eq:recon_loss_neg}, and \eref{eq:cam_loss},  as follows:
\begin{align}\label{eq:rec_loss}
    \mathcal{L}_{rec} = \lambda_{aug}^{p}\mathcal{L}_{aug}^{p} + \lambda_{aug}^{n}\mathcal{L}_{aug}^{n} \:+\: \lambda_{cam}\mathcal{L}_{cam},
\end{align}
where $\lambda_{aug}^{p}$, $\lambda_{aug}^{n}$ and $\lambda_{cam}$ are the weighting factors. The reconstruction loss in \eref{eq:rec_loss} encourages that the proposed framework consistently forms a single cluster of the embeddings of a person from both hard positive and hard negative samples. The examples of re-entangled features are shown in \fref{fig:mix}, and the examples of activation maps are in \fref{fig:activation}. 
\vspace{0.2cm}