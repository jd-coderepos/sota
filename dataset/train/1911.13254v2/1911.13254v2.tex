\section{Experimental setup}
\label{sec:exp}

\subsection{Evaluation framework}

\paragraph{MusDB and additional data}
\label{sec:data}
We use the MusDB dataset~\citep{musdb} , which is composed of 150 songs
with full supervision in stereo and sampled at
44100Hz. For each song, we have the exact waveform of the \source{drums}, \source{bass}, \source{other} and \source{vocals} parts, i.e. each of the sources. The actual song, the mixture,
is the sum of those four parts. The first 84 songs form the \emph{train set}, the next 16 songs form the \emph{valid set} (the exact split is defined in the \texttt{musdb} python package) while the remaining 50 are
kept for the \emph{test set}.
We collected raw stems for 150 tracks, i.e., individual
instrument recordings used in music production software to make a song.
We manually assigned each instrument to one of the sources in MusDB.
We call this extra supervised data the \emph{stem set}.
We also report the performances of Tasnet and Demucs trained
using these 150 songs in addition to the 84 from MusDB, to anaylze the effect of adding more training data.



\paragraph{Source separation metrics}
\label{sec:metrics}
Measurements of the performance of source separation models was
developed by Vincent et al.~for blind source separation~\citep{measures}
and reused for supervised source separation in the SiSec Mus evaluation campaign~\citep{sisec}.
Similarly to previous work~\citep{wavunet,sony_densenet,sony_denselstm}, we focus on the SDR (Signal to Distortion Ratio) which measures the log ratio between the volume
of the estimated source projection onto the ground truth, and the volume of what is left out of this projection, typically contamination by other sources or artifacts. Other metrics can be defined ( and ) and we present them in the supplementary material.
We used the python package
\texttt{museval}
 which provide a reference implementation for the SiSec Mus 2018 evaluation campaign.
As done in the SiSec Mus competition, we report the median over all tracks of the median
of the metric over each track computed using the \texttt{museval} package.

\subsection{Baselines}

We compare to several state-of-the-art baselines, both in the spectral and temporal domain.
Open Unmix~\citep{openunmix}, a 3-layer BiLSTM model with encoding and decoding fully connected layers
on spectrogram frames. It was released by the organizers of the SiSec 2018~\citep{sisec} to act as a strong reproducible baseline and matches the
performances of the best candidates trained only on MusDB.
We also selected MMDenseLSTM~\citep{sony_denselstm}, a multi-band dense net with LSTMs at different scales of the encoder and decoder.
This model was submitted as \textsc{TAK2} and trained with 804 extra labeled songs\footnote{Source: \url{https://sisec18.unmix.app/\#/methods/TAK2}}.

We also compare to the more recent options like the D3Net~\citep{takahashi2020d3net} model, which used dilated convolutions with dense connections, as well as the Spleeter model~\citep{spleeter2020}, a U-Net model trained on a very diverse dataset.
MMDenseLSTM, D3Net, Spleeter, and Open Unmix use Wiener filtering~\citep{multichannel_deep} as a last post processing step.
We provide the metrics for the Ideal Ratio Mask oracle (IRM), which computes the best possible mask using the ground truth sources
and is the topline of spectrogram based method~\citep{sisec}.

The only waveform based method submitted to the SiSec 2018 evaluation campaign is Wave-U-Net~\citep{wavunet} with the identifier \textsc{STL2}.
We also compare to the more recent variant of DPRNN by \citet{nachmani2020voice}, Meta-Tasnet~\citep{samuel2020meta}, and
Tasnet trained on extra data~\citep{lancaster2020frugal}.




\subsection{Training procedure}
\label{sec:training}

\paragraph{Epoch definition and augmentations}
One epoch over the dataset is defined as a pass over all 11-second extracts with a stride of 1 second. We use a random time shift between
0 and 1 second and keep 10 seconds of audio from there as a training example. We perform the following data augmentation~\citep{uhlich2017improving}, also used by Open Unmix and MMDenseLSTM: shuffling sources within one batch to generate a new mix, randomly swapping channels, random scaling by a uniform factor between 0.25 and 1.25. We additionally
multiply each source by ~\citep{nachmani2019unsupervised}. 
Following~\citep{cohen2019improving}, we also experiment with pitch/tempo shift. 20\% of the time, we randomly change the pitch by
-2, -1, 0, , +1, or +2 semitones, and the tempo by a factor taken uniformly between 0.88 and 1.12, using the Soundstretch utility\footnote{\url{https://www.surina.net/soundtouch/soundstretch.html}}.
When trained with extra data, we do not use pitch/tempo augmentation for Conv-Tasnet as it deteriorates its performance.

All Demucs models were trained over 360 epochs. Conv-Tasnet was trained for 360 epochs when trained only on MusDB and 240 when trained with extra data and using only 2-seconds audio extracts, due to its high memory usage.


\paragraph{Training setup and hyper-parameters}
The Demucs models are trained with 8 V100 GPUs with 16GB of RAM, while the Conv-Tasnet
models were trained with 16 V100 with 32GB of RAM. We use a batch size of 64, the Adam~\citep{adam} optimizer with a learning rate of 3e-4. 
For the Demucs model, we experimented with an initial number of channels in [32, 48, 64], with the best performance achieved for 64 channels.
Based on early experiments, we set the initial weight rescaling reference level described in Section~\ref{sec:weight_init}, to 0.1.
We computed confidence intervals using 3 random seeds in Table~\ref{table:comparison}.
For the ablation study on Table~\ref{table:ablation}, we provide metrics for a single run. 

\paragraph{Quantization}

For quantization, we use the DiffQ method~\citep{defossez2021differentiable}. DiffQ uses additive
noise on the weights as a proxy for the true quantization error.
The scale of the noise depends on learnable parameters that controls the number of bits used per group of 8 weight entries.
Because this transformation is completely differentiable, one can simply use a model size penalty (expressed in MegaBytes), with a weight of 0.0003, added to the main reconstruction loss.


\section{Experimental results}
\label{sec:results}


\begin{table}
\caption{Comparison of Conv-Tasnet and Demucs to state-of-the-art models that operate on the waveform (Wave-U-Net, Meta-Tasnet, DPRNN-like, Tasnet trained with extra data) and on spectrograms (Open-Unmix without extra data, D3Net, MMDenseLSTM with extra data) as well as the IRM oracle on the MusDB test set.
The \emph{Extra?} indicates the number of extra training songs used. We report the median over all tracks
of the median  over each track, as done in the SiSec Mus evaluation campaign~\citep{sisec}.
The \source{All} column reports the average over all sources. Demucs metrics are averaged over 3 runs, the confidence interval is the standard deviation over .
In bold are the values that are statistically state-of-the-art either with or without extra training data.}
\label{table:comparison}
\begin{center}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{l c c l l l l l}
  \toprule
     &&& \multicolumn{5}{c}{Test  in dB}\\
     \cmidrule{4-8}
  \textbf{Architecture} & \textbf{Wav?} & \textbf{Extra?} &
  \source{All} & \source{Drums} &  \source{Bass} &\source{Other} & \source{Vocals}\\
  \midrule
  IRM oracle& \crmark & N/A & 8.22 & 8.45 & 7.12 & 7.85 & 9.43\\
  \midrule
  Wave-U-Net & \chmark & \crmark & 3.23 & 4.22 & 3.21 & 2.25 & 3.25\\
  Open-Unmix & \crmark & \crmark & 5.33 & 5.73 & 5.23 & 4.02 & 6.32\\
  Meta-Tasnet & \chmark & \crmark & 5.52 & 5.91 & 5.58 &  4.19 & 6.40\\
  Conv-Tasnet & \chmark & \crmark & 5.73  & 6.02  & 6.20  & 4.27  & 6.43  \\
  DPRNN & \chmark & \crmark & 5.82 &6.15 & 5.88 & 4.32 & 6.92 \\
  D3Net & \crmark & \crmark & 6.01 &\textbf{7.01} & 5.25 & \textbf{4.53} & \textbf{7.24} \\
  Demucs & \chmark & \crmark & 6.28  & 6.86  & \textbf{7.01}  & 4.42  & 6.84  \\
  \midrule
  Spleeter & \crmark &  & 5.91 & 6.71 & 5.51 & 4.55 & 6.86\\
  TasNet & \chmark &  & 6.01 & 7.01 & 5.25 & 4.53 & 7.24\\
  MMDenseLSTM & \crmark & 804 & 6.04 & 6.81 & 5.40 & 4.80 & 7.16\\
  Conv-Tasnet & \chmark & 150 & 6.32  & 7.11  & 7.00  & 4.44 & 6.74  \\
  D3Net & \crmark & 1.5k & 6.68 & 7.36  & 6.20 & \textbf{5.37} & \textbf{7.80} \\
  Demucs & \chmark & 150 & \textbf{6.79}  & \textbf{7.58}  & \textbf{7.60}  & 4.69  & 7.29  \\
  \bottomrule
\end{tabular}}
\end{center}
*: each track is only 30 seconds,
: from current work, : trained without pitch/tempo augmentation, as it deteriorates performance.
\end{table}

\begin{table}
\caption{Mean Opinion Scores (MOS) evaluating the quality and absence of artifacts of the separated audio. 38 people rated 20 samples each, 
randomly sample from one of the 3 models or the ground truth. There is one sample per track in the MusDB
test set and each is 8 seconds long. Ratings of 5 means that the quality is perfect (no artifacts).}
\label{table:mos_quality}
\begin{center}
\begin{tabular}{l l l l l l}
  \toprule
     & \multicolumn{5}{c}{Quality Mean Opinion Score}\\
     \cmidrule{2-6}
  \textbf{Architecture} & 
  \source{All} & \source{Drums} &  \source{Bass} &\source{Other} & \source{Vocals}\\
  \midrule
  Ground truth & 4.46  & 4.56  & 4.25  & 4.45  & 4.64  \\
  \midrule
  Open-Unmix & 3.03  & 3.10  & 2.93  & 3.09  & 3.00  \\
  Demucs & 3.22  & 3.77  & 3.26  & 3.32  &  2.55  \\
 Conv-Tasnet & 2.85  & 3.39  & 2.29  & 3.18  & 2.45 \\
  \bottomrule
\end{tabular}
\end{center}
\end{table}
\begin{table}
\caption{Mean Opinion Scores (MOS) evaluating contamination by other sources. 38 people rated 20 samples each, 
randomly sampled from one of the 3 models or the ground truth. There is one sample per track in the MusDB
test set and each is 8 seconds long. Ratings of 5 means no contamination by other sources.}
\label{table:mos_contamination}
\begin{center}
\begin{tabular}{l l l l l l}
  \toprule
     & \multicolumn{5}{c}{Contamination Mean Opinion Score}\\
     \cmidrule{2-6}
  \textbf{Architecture} & 
  \source{All} & \source{Drums} &  \source{Bass} &\source{Other} & \source{Vocals}\\
  \midrule
  Ground truth & 4.59  & 4.44  & 4.69  & 4.46  & 4.81  \\
  \midrule
  Open-Unmix & 3.27  & 3.02  & 4.00  & 3.11  & 2.91  \\
  Demucs & 3.30  & 3.08  & 3.93  & 3.15  &  3.02  \\
 Conv-Tasnet & 3.42  & 3.37  & 3.73  & 3.46  & 3.10 \\
  \bottomrule
\end{tabular}
\end{center}
\end{table}

In this section, we first provide experimental results on the MusDB dataset for Conv-Tasnet and Demucs compared with state-of-the-art baselines. We then dive into the ablation study of Demucs. 

\subsection{Comparison with baselines}

We provide a comparison with the state-of-the-art baselines on Table~\ref{table:comparison}. The models on the top half were trained without any extra data
while the lower half used unreleased training songs. As no previous work included confidence intervals, we considered the single metric provided by for the baselines as the exact estimate of their mean performance.


\paragraph{Quality of the separation}

We first observe that Demucs and Conv-Tasnet outperform previous waveform domain methods such as Wave-U-Net. Demucs surpasses the best spectrogram domain method, D3Net~\citep{takahashi2020d3net}, when averaging over all sources, by 0.3 dB of SDR.
When looking into more details, we see a clear advantage for spectrogram domain methods for the \source{other} and \source{vocals} sources. Without extra training data, D3Net also beats Demucs for the \source{drums}. However, we will see in the next section that human evaluations show that spectrogram domain methods tend to deteriorate the attack of an instrument, so that it is still possible that this high SDR hides significant degradation of the attack of percussive sounds. As no audio samples were released for D3Net, this is impossible to verify.

When trained with extra training data, we still see a small advantage overall from Demucs over D3Net (0.1 point of SDR). Again we notice that the \source{bass} and \source{drums} sources are better handled in the temporal domain, while \source{other} and \source{vocals} are better handled in the spectrogram domain.
Note that for the first time for music source separation, a temporal domain model beats the IRM oracle for the \source{bass} source, by 0.5 points of SDR.

An interesting experiment would be retrain D3Net with pitch/tempo augmentation and see if it benefits from it. Interestingly, the Conv-Tasnet model achieved the same SDR whether this augmentation was used or not, and the SDR even deteriorated when trained with extra tracks. For Demucs, which comparatively has more tunable weights (see Table~\ref{table:quantization}), the augmentation always improves performance, most likely by preventing some overfitting. We conjecture that pitch/tempo shift lead to deteriorated audio, but reduce overfitting. For Demucs, this is a clear win, while for smaller models, it might not be as beneficial.

\paragraph{Human evaluations}

We noticed strong artifacts on the audio separated by Conv-Tasnet, especially for the \source{drums} and \source{bass} sources: static noise between 1 and 2 kHz, 
hollow instrument attacks or missing notes as illustrated on Figure~\ref{fig:mel}. In order to confirm this observation, we organized a mean opinion score survey. 
We separated 8 seconds extracts from all of the 50 test set tracks for Conv-Tasnet, Demucs and Open-Unmix. We asked 38 participants to rate 20 samples each, randomly taken from one
of the 3 models or the ground truth. For each one, they were required to provide 2 ratings on a scale of 1 to 5. The first one evaluated the quality and absence of artifacts (1: many artifacts and distortion, content is hardly recognizable, 5: perfect quality, no artifacts) and the second one evaluated contamination by other sources (1: contamination if frequent and loud, 5: no contamination). We show the results on Tables~\ref{table:mos_quality} and \ref{table:mos_contamination}.

Note that for this experiment, with used an early version of Demucs that was trained with 100 channels, and without pitch/tempo augmentation. We had trouble differentiating between the latest version of Demucs and this early version in an informal blind test performed by the authors, so that we believe the results presented here stay mostly valid for the best performing Demucs model presented on Table~\ref{table:comparison}.

We confirmed that the presence of artifacts in the output of Conv-Tasnet degrades the user experience, with a rating of 2.85 against  for Demucs.
On the other hand, Conv-Tasnet samples had less contamination by other sources than Open-Unmix or Demucs, although by a small margin, with a rating of 
against  for Demucs and  for Open-Unmix.

\paragraph{Training speed} We measured the time taken to process a single batch of size 16 with 10 seconds of audio at 44.1kHz (the original Wave-U-Net being only trained on 22 kHz audio, we double the time for fairness), on a single GPU, ignoring data loading and using \verb|torch.cuda.synchronize|
to wait on all kernels to be completed. MMDenseLSTM does not provide a reference implementation. Wave-U-Net takes 1.2 seconds per batch, Open Unmix 0.2 seconds per batch
and Demucs 1.4 seconds per batch. Conv-Tasnet cannot be trained with such a large sample size, however a single iteration over 2 seconds of audio with a batch size of 4
takes 0.7 seconds.


\subsection{Ablation study for Demucs}
\label{sec:ablation}
\begin{table}
\caption{Ablation study for the novel elements in our architecture described in Section~\ref{sec:model}.
We use only the train set from MusDB and report best L1 loss over the valid set throughout training
as well the SDR on the test set for the epoch that achieved this loss.
}
\label{table:ablation}
\begin{center}
  \setlength\extrarowheight{1pt}
\begin{tabular}{l r r}
  \toprule
  & \textbf{Valid set} & \textbf{Test set}\\
  \textbf{Difference}& L1 loss  & \\
\midrule
  no BiLSTM & 0.171 & 5.40\\
  no pitch/tempo aug. & 0.156 & 5.80 \\
  no initial weight rescaling & 0.156 & 5.88\\
  no 1x1 convolutions in encoder &  0.154 & 5.89 \\
  ReLU instead of GLU &  0.157 & 5.92 \\
  no BiLSTM, depth=7  & 0.160 & 5.94\\
  MSE loss  & N/A & 5.99\\
  no resampling & 0.153 & 6.03 \\
  no shift trick &  N/A & 6.05 \\
  kernel size of 1 in decoder convolutions & 0.153 & 6.11 \\
  \midrule
  Reference & 0.150 & 6.28\\
  \bottomrule
\end{tabular}
\end{center}
\vskip -3mm
\end{table}

\begin{table}
\caption{
Impact of the initial number of channels on the model size, and on the performance. We also report
the results obtained from quantizing the model with DiffQ.
The bottom part consist in the base Demucs and Conv-Tasnet models presented on Table~\ref{table:comparison}.
}
\label{table:quantization}
\begin{center}
  \setlength\extrarowheight{1pt}
\begin{tabular}{l r r r}
  \toprule
  & & \textbf{Valid set} & \textbf{Test set}\\
  \textbf{Model}& \textbf{Model size (MB)} & L1 loss  & \\
\midrule
  Demucs, 32 channels & 254 & 0.154 & 6.08 \\
  Demucs, 48 channels &  570 & 0.151 & 6.20\\
  \midrule
  Demucs, 64 channels, DiffQ quantized & 120 & \textbf{0.150} & \textbf{6.28} \\ 
  \midrule
  Demucs, 64 channels & 1014 & \textbf{0.150} & \textbf{6.28}\\
  Conv-Tasnet & 42 & 0.152 & 5.73\\
  \bottomrule
\end{tabular}
\end{center}
\vskip -3mm
\end{table}

We provide an ablation study of the main design decisions for Demucs in Table~\ref{table:ablation}. Given the cost of training a single model, we did not
compute confidence intervals for each variation.


We notice strong contributions for different design decisions we made. The L1 loss seems to be more robust than the MSE. The LSTM part plays a crucial role,
and replacing it by an extra layer of convolution is not sufficient to recover the best possible SDR.
The initial weight rescaling described in Section~\ref{sec:weight_init} provides significant gain, with an extra 0.4 dB for the SDR.

We introduced extra convolutions in the encoder and decoder, as described in Sections~\ref{sec:auto}. The two proved useful, 
improving the expressivity of the model, especially when combined with GLU activation.
Using a kernel size of 3 instead of 1 in the decoder
further improves performance. We conjecture that the context from adjacent time steps helps the output of the transposed convolutions to be consistent through time
and reduces potential artifacts arising from using a stride of 4.

The pitch/tempo augmentation gives one of the largest gain, with almost 0.5 dB extra for the SDR, which highlights the importance of strong data augmentation when training data is limited.


We observe that using the shift trick as described in Section~\ref{sec:model} gives a gain of almost 0.3 SDR.
We did not report the validation loss as we only use the stabilization when computing the SDR over the test set.
We applied the randomized stabilization to Open-Unmix and Conv-Tasnet with no gain, since, as explained in Section~\ref{sec:equi}, both are naturally equivariant with respect to initial time shifts.







\subsection{Quantization of Demucs}

One of the main drawbacks of the Demucs model when compared to other architecture is its large model size, more than 1014MB, against 42MB for Conv-TasNet. On Table~\ref{table:quantization}, we propose 
two ways to reduce this issue, either by reducing the initial number of channels (32 or 48 channels), which will improve both the model size, as well as reduce the computational complexity of the model, or using the DiffQ quantization technique~\citep{defossez2021differentiable}.

While it is possible to use 48 channels without suffering from a large loss in SDR, it seems getting to 32 channels will lead to a decrease of 0.2 dB in performance.
On the other hand, quantization will reduce the model size down to 120MB without any loss of SDR. This is still more than the 42MB of Conv-Tasnet, but close to 10x improvement over the uncompressed baseline.

Note that the 120MB is achieved if each integer is exactly
coded on the right number of bits. As this is currently
not implemented in Python, we rely on \texttt{gzip} to
approximately reach this ideal size. This leads to a small overhead, so that the download size for our quantized model is
150MB.