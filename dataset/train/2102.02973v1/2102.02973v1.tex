\begin{table*}
\tabcolsep=0.17cm
\centering
\begin{tabular}{c|ccccc|cc}
\toprule
        & \multicolumn{5}{c|}{\textbf{Same style}}                                   & \multicolumn{2}{c}{\textbf{Different style}}           \\ \midrule
Teacher & \small{ResNet56}        & \small{ResNet110}      & \small{ResNet110}       & \small{WRN-40-2}        & \small{WRN-40-2}     & \small{WRN-40-2}            & \small{ResNet34}                   \\
Student & \small{ResNet20}        & \small{ResNet20}        & \small{ResNet56}        & \small{WRN-16-2}        & \small{WRN-40-2}     & \small{ResNet56}            & \small{WRN-28-2}                   \\ \midrule
Teacher & 0.7254          & 0.7409          & 0.7409          & 0.7620          & 0.7620           & 0.7620              & 0.7860                       \\
Student & 0.6940          & 0.6940          & 0.7254          & 0.7289          & 0.7620           & 0.7254              & 0.7532                       \\ \midrule
KD      & 0.7098          & 0.7081          & 0.7483          & 0.7499          & 0.7763           & 0.7497              & 0.7648                       \\
FitNet  & 0.7005          & 0.7002          & 0.7411          & 0.7522          & 0.7766           & 0.7506              & 0.7644                       \\
ATT     & 0.7054          & 0.7081          & 0.7488          & 0.7520          & 0.7778           & 0.7516              & 0.7720                       \\
RKD     & 0.7043          & 0.7076          & 0.7477          & 0.7459          & 0.7762           & 0.7439              & 0.7632                       \\
CRD     & 0.7095          & 0.7091          & 0.7512          & 0.7515          & 0.7780           & 0.7525              & 0.7697                       \\
L2T     & 0.7037          & 0.7001          & 0.7457          & 0.7486          & 0.7678           & 0.7463              & 0.7640                       \\ \midrule
Ours    & \textbf{0.7153} & \textbf{0.7138} & \textbf{0.7539} & \textbf{0.7547} & \textbf{0.7813} & \textbf{0.7540}     & \textbf{0.7747}     \\ \bottomrule
\end{tabular}
\vspace{0.3em}
\caption{Performance comparison on CIFAR-100. The teachers and the students have same or different architectural style. All experiments are repeated 5 times.}
\label{table:cifar}
\end{table*}

We evaluate our proposed method on model compression tasks that train a smaller, or better model under a limitation of the model capacity and transfer learning tasks that obtain a better model in a specific domain by utilizing a pre-trained model. Following by the quantitative evaluations, we qualitatively analyze how our method works. Finally we provide ablation studies to provide further properties of our methods. 

In our experiment, our baseline methods are a traditional knowledge distillation method (KD) firstly introduced by Hinton \textit{et al.}~\cite{hinton2015distilling}, three popular feature-level distillation methods (FitNet~\cite{fitnets}, ATT~\cite{atts}, CRD~\cite{crd}) that require a manual feature matching, and one feature-level distillation method (L2T) automatically linking the teacher and student features. For CRD, we set the number of negative samples same as the batch size of each experiment. Note that KD is applied to all baselines to reveal additional gains from the feature distillation methods. 

\subsection{Model Compression}

We demonstrate the effectiveness of the proposed distillation method on model compression tasks. The experiments are conducted on three popular benchmark datasets such as \textit{CIFAR-100} \cite{cifar100}, \textit{tinyImageNet}, and \textit{ImageNet}~\cite{imagenet}. We utilize Residual Network (ResNet)~\cite{resnet} and Wide Residual Network (WRN)~\cite{wrn} architectural styles. First, we conduct an experiment on CIFAR-100 that consists of  sized color images for 100 object classes and has 50K training and 10K validation images. For data augmentation, the horizontal flipping and random cropping are applied. We set the batch size as 64 and the maximum iteration as 240 epochs. All models are trained with stochastic gradient descent with  of momentum, weight decay as , initial learning rate as 0.05, and divide it by 10 at 150, 180, 210 epochs. For the baseline methods, we use their official code and their hyper-parameters. For the distillation loss of our model, we apply \{30, 50, 100, 200\} of beta, , and choose the best performer. We provide how the performance changes depending on the value of  in \textit{Sensitivity Analysis} section. For the feature candidates of our method, we use all output features of the teacher and student residual blocks as candidate for all experiments, except ResNet110. 
For ResNet110, we skip one for every two residual blocks and used only half of the entire residual blocks.
In detail, the number of the candidates become 9 for ResNet20, 27 for ResNet56, 27 for ResNet110, 8 for ResNet18, 16 for ResNet34, 6 for WRN-16-2, 12 for WRN-28-2, and 18 for WRN-40-2.

Table~\ref{table:cifar} shows our experiment settings and results on the CIFAR-100 dataset with various network architecture. We pre-train large teacher networks and utilize them to train smaller or same-scaled student networks. The experiments are divided into two groups according to the architectural style of the teacher and student. When considering the student without knowledge distillation, all students shows the worst performance over all experiment settings. With manually linked feature pairs, the baseline models including KD, FitNet, ATT, RKD and CRD show better performances than the vanilla students. 
When applying L2T that identifies feature links with individual gates, we observed worse performance than other baseline methods even though it identifies beneficial feature pairs to meet their objective. 
The reason of the degradation is that L2T tends to propagate high-level knowledge of the teacher to the low-level feature of the student (See \textit{Qualitative Studies on Feature Attention} section).
Intuitively, the low-level of a small network cannot mimic the high-level of a large network, which adversely affects performance.
Our method that utilizes an attention mechanism to identify similar features between the teacher and student shows the best performance over all experiment settings.
In particular, our method shows an improvement over ATT which uses the same feature distance for distillation. In other words, the proposed linking method significantly contributes to distillation performance.

In order to validate our method on more real world environment, we compare our method from other baseline methods on tinyImageNet and ImageNet~\cite{imagenet}. The tinyImageNet dataset consists of  sized 100K training and 10K validation images for 200 object classes and the ImageNet dataset includes 1.2M training and 50K validation large-scale images for 1K object classes. 
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.9\columnwidth]{figure/Fig_3_v3.png}
    \caption{
    Manual and attention-based feature links for knowledge distillation. Rows and columns of matrices indicate the student and teacher features, respectively. Each matrix is the average overall  at the corresponding training epoch. The pairs are compared on multiple distillation settings; ResNet56  ResNet20 (\textbf{Same architectural style}), ResNet34  WRN-28-2 (\textbf{Different architectural style}), and WRN-40-2  WRN-40-2 (\textbf{Self}). The manual feature links are not changed once they are selected, but AFD are adaptively selected during the distillation.}
    \label{fig:attention_epoch}
\end{figure*}

\begin{table}
\centering
\begin{tabular}{c|cc|c}
\toprule
        & \multicolumn{2}{c|}{Tiny ImageNet}         & ImageNet \\ \midrule
Teacher & ResNet34        & ResNet34        & ResNet34 \\
Student & ResNet18        & ResNet34        & ResNet18 \\ \midrule
Teacher & 0.6750          & 0.6750          & 0.7355   \\
Student & 0.6530          & 0.6750          & 0.7028   \\ \midrule
KD      & 0.6818          & 0.6971          & 0.7066   \\
FitNet  & 0.6779          & 0.6892          & 0.7085   \\
ATT     & 0.6782          & 0.6961          & 0.7093   \\
RKD     & 0.6772          & 0.6846          & 0.7137   \\
CRD     & 0.6819          & 0.6968          & 0.7135   \\ \midrule
Ours    & \textbf{0.6880} & \textbf{0.6981} & \textbf{0.7138}       \\ \bottomrule 
\end{tabular}
\vspace{0.3em}
\caption{Performance comparison on large-scale datasets; Tiny ImageNet and ImageNet.}
\label{table:imagenet}
\end{table}

For tinyImageNet, we pad the images to  and then randomly cropped to  and flipped for data augmentation. We set the batch size as 128 and the maximum iteration as 200 epochs. All models are trained with stochastic gradient descent with momentum 0.9. We set weight decay as , initial learning rate as 0.1, and we divide the learning rate by 5 at 60, 120, 150 and 180 epochs. We adopt ResNet34 for the teacher and utilize ResNet34 and ResNet18 both for the student. Unlike the ResNet34 and ResNet18 architecture for large image classification, we resize the first convolutional filter size from 7 to 3. For ImageNet, we randomly crop size of  of each of images and flipped for data augmentation. We optimize the student with initial learning rate as 0.1, divide it by 10 at 30, 60, 90 epochs, and set the maximum iteration as 100 epochs. We set weight decay as  and the batch size as 256. We utilize ResNet34 for the teacher and ResNet18 for the student. For tinyImageNet and ImageNet, we set the hyperparameter, , as 50.

Table \ref{table:imagenet} shows the experiment results on tinyImageNet and ImageNet. The proposed method achieve better performance on large-scale datasets than other baseline knowledge distillation methods. It should be noted that we do not conduct L2T on these large-scaled datasets due to the heavy time complexity to update their meta-network. In addition, our method shows constantly better performances than ATT that holds the same distillation loss for the manually selected feature pairs.

\subsection{Transfer Learning}

Transfer learning with knowledge distillation utilizes the teacher pre-trained on a source domain task to train the student for a target domain task. We investigate the effectiveness of our method on transfer learning tasks. We adopt a ResNet34 pre-trained on ImageNet as the teacher network and transfer its knowledge into the students for four tasks, such as Caltech-UCSD Bird (CUB 200) \cite{cub}, MIT Indoor Scene Recognition (MIT67) \cite{mit67}, Stanford 40 Actions (Stanford40) \cite{stan40} and Stanford Dogs \cite{stan_dog}. CUB 200 consists of 5k training and 6k validation images with 200 bird species. MIT67 contains 5k training and 1k validation images with 67 type of indoor scenes. Stanford40 has 4k training and 5k validation images with 40 human actions. Stanford Dogs consists of 12k training and 8k validation images with 120 dog species. The images of transfer learning datasets consists of large scale images. The student architecture for the specific target tasks are set as ResNet18. All experiment settings are followed by those of L2T~\cite{l2t} and our hyper-parameter, , is set as 1,000.

\begin{table}[h]
\centering
\begin{tabular}{c|cccc}
\toprule
        & \small{CUB200} & \small{MIT67}  & \small{Stanford40} & \small{Stanford Dogs} \\ \midrule
Scratch & 0.4215 & 0.4891 & 0.3693     & 0.5808        \\ 
FitNet  & 0.4893 & 0.5488 & 0.4450     & 0.6725        \\
ATT     & 0.5774 & 0.5918 & 0.5929     & 0.6970        \\
L2T     & 0.6505 & 0.6485 & 0.6308     & \textbf{0.7808}        \\ \midrule
Ours    & \textbf{0.6829} & \textbf{0.6647} & \textbf{0.6792}     & 0.7606             \\ \bottomrule 
\end{tabular}
\vspace{0.3em}
\caption{Performance comparison on multi-domain transfer learning tasks; from a ResNet34 model pre-trained with ImageNet (\textbf{source domain}) to a ResNet18 models for CUB200, MIT67, Stanford40, and Stanford Dogs (\textbf{target domains}). Scratch, ATT and L2T are referred from \cite{l2t}. All experiments are repeated 3 times.}
\label{table:multi-domain}
\end{table}

Table \ref{table:multi-domain} summarizes the experiment results for the multi-domain transfer learning tasks. For most datasets, our method shows better performance than L2T and ATT. In transfer learning, dataset from target tasks give limited information. Therefore, training without any knowledge transfer (scratch) shows the worst performance with large margin. Comparing ATT with L2T and our method, it can be seen that identifying the feature links is effective in transfer learning tasks. 


\subsection{Qualitative Studies on Feature Attention}

\subsubsection{Feature Links.}
Here, we analyze the attention values, , learned from our proposed method in order to provide its further properties. First, we observe the attention maps of various architecture pairs, such as the same architectural style, different architectural style and identical architecture (self).

Figure \ref{fig:attention_epoch} shows the feature links  during the training phases. As can be seen, the traditional knowledge distillation methods manually link the teacher and student features and transfer teacher's knowledge only through the pre-defined links. In contrast to the manual feature links, our method identifies feature links in a data-driven way and thus the feature links  are changed over training steps and converged at the end. In the case of the same architectural style, low and mid-levels of the teacher features are linked with a low-level student feature. This indicates our method affects the student to use more layers to learn high-level of the teacher features. 
In the case of the different architectural style, the lowest-level and the highest-level features are linked among themselves and mid-level features are smoothly connected.
These results show that it is difficult to manually create feature links in different architecture styles.
In the case of the identical architectures, the features are linked in the order of the levels but the student tends to use more layers to extract high-level features. 
Based on observations, we can infer that AFD tends to link features in the same level.
However, when the teacher and student have different architectures, the connection is extended to other levels.
It is an advantage of our method that the feature links can be changed and extended according to the difference between the teacher and the student's architecture, and improves the distillation performance in various architecture settings.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\columnwidth]{figure/Fig_4_2.png}
    \caption{Activation map corresponding to each distillation methods and the teacher. The teacher is ResNet56 and the students are ResNet20. The colored boxes indicate the links from the teacher to the student.}
    \label{fig:activation_map}
\end{figure}

\subsubsection{Activation Map}
Figure~\ref{fig:activation_map} shows activation maps of a teacher and a students trained with knowledge distillation methods such as ATT with manual feature pairs, L2T, and ours. When comparing links between the teacher and student features, ATT has manually set ordered links through the levels of the features. In the L2T, the identified links switch the order of the levels; the last feature of the teacher is connected with the mid-level feature of the student (green box), and the mid-level feature of the teacher is linked to the last feature of the student (blue box). 
The identified links of the L2T may hinder the student training by transferring different order of features learned by the teacher, see green and blue boxes.
In contrast to both, our proposed method spreads the high-level features of the teacher to various feature levels of the student (blue, red, and green boxes) and identify links for the student to train while keeping the order of the features from the teacher.
More interestingly, compared with ATT, the low-level and the mid-level features of the AFD student tends to mimic activated regions of the high-level features of the teacher. 

\subsection{Sensitivity Analysis}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\columnwidth]{figure/rebuttal_fig1.png}
    \caption{Sensitivity analysis for . Red line indicates accuracy of model compression task (WRN-40-2  WRN-16-2). Blue line indicates accuracy of transfer learning task (ResNet34  ResNet18)}
    \label{fig:sensitivity}
\end{figure}

To investigate the impact of the hyperparameter of AFD, , we evaluate our model by varying the value of . , is used to train the attention map, , which determines the links of the AFD network, and to decide the degree of how much the student mimic the teacher features. We perform the sensitivity analysis for  with the model compression and the transfer learning tasks. Figure~\ref{fig:sensitivity} shows the accuracy of each task.

For the model compression task, we use WRN-40-2 as the teacher network and WRN-16-2 as the student network with CIFAR-100 dataset. As can be seen from the red line in Figure~\ref{fig:sensitivity}, the accuracy decreases when  is more than 1,000 compared to the interval between 30 to 200. For the transfer learning task, we use ResNet34 as the teacher network and ResNet18 as the student network with MIT67 dataset. In contrast with model compression task, the transfer learning task shows better performance when the  value is relatively large. However, we observe that the different result from the model compression task using the same network architecture of transfer learning setting with ImageNet dataset (we use  as 50 for ImageNet). The rationale behind the gap of the hyperparameter lies in the degree of reliance on teachers' knowledge cased by the size of the dataset and the relevance between source and target tasks \cite{sp,vid,rkds}

\subsection{Ablation Studies}

\begin{table}[h]
\tabcolsep=0.17cm
\centering
\begin{tabular}{cc|cc}
\toprule
 \small{Teacher candi.} & \small{Linking method} & \small{ResNet56  ResNet20}  \\ \midrule
 \multirow{3}{*}{\small{Random}}  & \small{Random link}       & 0.6945  0.002             \\ 
   & \small{Ordered} & 0.6999  0.003                     \\  
   & \small{AFD (co-train)}       &  \textbf{0.7105}  0.002                 \\
\midrule
\multirow{4}{*}{\small{Equal interval}} & \small{Random link}         & 0.6941  0.005               \\
  & \small{Ordered}          & 0.7025  0.002               \\
  & \small{AFD (pre-trained)}      & 0.7121  0.001      \\ 
  & \small{AFD (co-train)}          & \textbf{0.7135}  0.002          \\ \bottomrule
\end{tabular}
\vspace{0.3em}
\caption{Ablation studies on selecting the teacher candidates and linking them to the student features. All experiments are repeated 5 times.}
\label{table:2+3}
\end{table}

\subsubsection{Linking methods.}
In order to reveal the benefits of the similarity-based links, we compare ours from other linking methods in Table~\ref{table:2+3}. In this experiment, we set the numbers of the teacher and student candidates same as 9 for the same architectural style (ResNet56ReNet20) to use all possible student features. We choose the teacher candidates in two ways; ``Random'' that randomly selects the candidates from all features upon all residual blocks and ``Equal interval'' that selects the features that are sequentially equidistant between themselves. For the linking method, we evaluate three linking methods; ``Random link'', ``Ordered'', ``AFD''.  

As shown in Table~\ref{table:2+3}, ``Ordered'' shows better performance than ``Random link''. It should be noted that the combination of ``Equal interval'' and ``Ordered'' is usually used when manually selecting links between the teacher and student features. Interestingly, when applying the links identified from the pre-trained AFD, we observe the performance improvement although we only change links from the usually link setting. The result proves that there is more effective way to set the links than manually decided links. 
In addition, when training AFD together, AFD shows the best performances. 
This experiments prove the superiority of AFD on identifying links of the feature pairs and transferring teacher's knowledge to the student.

\begin{table}[h]
\tabcolsep=0.2cm
\begin{subtable}{.48\textwidth}
\centering
\begin{tabular}{c|ccc}
\toprule
                          \# of candi.             & \multicolumn{3}{c}{\# of candi.(\textbf{student})} \\ \cmidrule{2-4} 
(\textbf{teacher}) & 3           & 6          & 9          \\ \midrule
3                                     & \textbf{0.7140}      & 0.7097     & 0.7115     \\
9                                     & 0.7109      & 0.7149     & 0.7135     \\
27                                    & 0.7121      & \textbf{0.7152}     & \textbf{0.7153}   \\ \bottomrule  
\end{tabular}
\caption{ResNet56  ResNet20}
\end{subtable}
\hspace{1em}
\begin{subtable}{.48\textwidth}
\centering
\begin{tabular}{c|ccc}
\toprule
                          \# of candi.             & \multicolumn{3}{c}{\# of candi.(\textbf{student})} \\ \cmidrule{2-4} 
 (\textbf{teacher})  & 3           & 6          & 12         \\ \midrule
4                                     & 0.7698      & 0.7704     & 0.7730     \\
8                                     & \textbf{0.7714}      & 0.7717     & 0.7724     \\
16                                    & 0.7706      & \textbf{0.7733}     & \textbf{0.7747}    \\ \bottomrule  
\end{tabular}
\caption{ResNet34  WRN-28-2}
\end{subtable}
\caption{Ablation studies on the numbers of the candidates for both the teacher and the student. The candidates are set as the output features of the residual blocks that are sequentially equidistant between themselves.}
\label{table:ablation_layer}
\end{table}

\subsubsection{Number of candidates.}
Here, we analyze the impact of the numbers of the teacher and student candidates. For this experiment, the candidates are set as the output features of the residual blocks that are sequentially equidistant between themselves. Table~\ref{table:ablation_layer} shows distillation performances over varying numbers of the candidates. When the student candidates are more than half of the total features, and using all teacher features (27 for ResNet56 and 16 for ResNet34) provides better student performances. In the other hand, with the small number of the student candidates, using all teacher features causes a information bottleneck and degrades the performances. This experiments provide a guidance to choose the number of the candidates.

\subsubsection{Distance Metrics and Pooling Methods}
We use L2 distance for distilling the teacher's feature to the student according to the trained link , see equation~\ref{eq:sad_loss}. However, other distance metric can used for distillation \cite{nst,vid,atts} and it may affect the behavior of the student. Therfore, we explore four distance metrics, L1, L2, KL divergence, and cosine similarity, on model compression task with WRN-40-2 as the teacher network and WRN-16-2 as the student network. Table \ref{table:distance_metric} shows that L2 distance is the optimal metric for our experiments, so we use L2 distance for the whole experiments.

Also, channel-wise pooling method, , applied to feature in equation~\ref{eq:sad_loss} may affect the performance of distillation. Therefore, we compare three channel-wise pooling methods including max-pooling () and average-pooling (). We denote A1 and A2 as average pooling methods with  and , respectively. As can be seen in the table \ref{table:pooling}, A2 shows the best performance. Therefore, we use A2 for all experiment in this paper.

\begin{table}[h]
\centering
\begin{tabular}{cccc}
\toprule
L1     & L2              & KL     & Cosine \\ \midrule
0.7513 & \textbf{0.7547} & 0.7523 & 0.7541 \\ \bottomrule
\end{tabular}
\caption{Accuracy according to distance metrics.}
\label{table:distance_metric}
\end{table}
\vspace{-1em}

\begin{table}[h]
\centering
\begin{tabular}{ccc}
\toprule
A1     & A2              & Max      \\ \midrule
0.7520 & \textbf{0.7547} & 0.7528  \\ \bottomrule
\end{tabular}
\caption{Accuracy according to pooling methods.}
\label{table:pooling}
\end{table}
