\documentclass[11pt]{article}
\usepackage{acl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{color}
\usepackage{graphicx}
\usepackage{tabulary}
\usepackage{enumitem}
\usepackage{url}
\usepackage{amsfonts}
\newcommand{\E}{\mathbb{E}}
\newenvironment{tightitemize}{\begin{itemize}[topsep=0pt, partopsep=0pt] \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}{\end{itemize}}
  
  \usepackage{color}
  \newenvironment{tightenumerate}{\begin{enumerate}â‰ˆ \footnotesize \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}{\end{enumerate}}

\makeatletter
\newcommand{\@emptybiblabel}[1]{}
\makeatother

\usepackage{microtype}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{array}
\usepackage[scaled=0.86]{helvet}
\usepackage{ifthen}
\usepackage{courier}
\usepackage[linesnumbered,vlined,ruled]{algorithm2e}
\aclfinalcopy


\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}
\abovedisplayskip 1.0pt plus2pt minus2pt

\belowdisplayskip \abovedisplayskip

\renewcommand{\baselinestretch}{0.99}

\def\sbmath#1{\mbox{\scriptsize\boldmath}} 
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\transpose}[1]{\ensuremath{\sp{\text{#1}}}}
\newcommand{\tsup}[1]{{#1}^{\intercal}}
\newcommand{\nlambda}{\text{{\bf w}}}

\newcommand{\reduceVerticalSpace}{
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
\setlength{\itemindent}{0pt}
\setlength{\listparindent}{0pt}
}
\newcommand\itm[1]{\begin{itemize} \reduceVerticalSpace #1 \end{itemize}}

\newcommand{\bleu}{{{\sc Bleu}}\xspace}
\newcommand{\meteor}{{{\sc Meteor}}\xspace}
\newcommand{\camready}[1]{{{#1}}}


\newcommand{\eos}{{\it EOS}\xspace}
\newcommand{\sts}{{{\textsc{Seq2Seq}}}\xspace}
\newcommand{\dbleu}{{{\sc Bleu}}\xspace}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\todoJiwei}[1]{\textcolor{blue}{#1}}

\title{Adversarial Learning for Neural Dialogue Generation}
\author{
Jiwei Li\hspace{1pt}, Will Monroe\hspace{1pt}, Tianlin Shi\hspace{1pt},
S\'ebastien Jean,  Alan Ritter\hspace{1pt} and Dan Jurafsky\hspace{1pt} \
\begin{aligned}
&\pi(p_{i}|p_{i-1},q_{i-1})=\\
&~~~~~~~~~\prod_{w_t\in p_{i}} \pi(w_t|w_1,...,w_{t-1},p_{i-1},q_{i-1})
\end{aligned}
\label{eq2}

 J(\theta)=\mathbb{E}_{y\sim p(y|x)}(Q_+(\{x,y\})|\theta)
 \label{lb1}
 
\nabla J(\theta)\approx [Q_+(\{x,y\})-b(\{x,y\})] \\ \nabla\log \pi(y|x)
\\ = [Q_+(\{x,y\})-b(\{x,y\})] \\ \nabla\sum_t \log p(y_t|x,y_{1:t-1})
\label{partial}

\nabla J(\theta)\approx \sum_t  (Q_+(x,Y_t)-b(x,Y_t))  \\
\nabla\log p(y_t|x,Y_{1:t-1})
\label{action}

L= ||\theta-\frac{1}{t}\sum_{t}\theta_t ||^2
 
where  denotes the value of the parameters at past time . The historical average of the parameters is updated after each parameter update. 
\end{comment}

For 
data processing, 
model training and decoding  (both the proposed adversarial training model and the standard \sts models), we employ a few  strategies that  improve response quality, including: 
(2) Remove training examples with length of responses shorter than a threshold (set to 5).
We find that this significantly improves the general response quality.\footnote{To compensate for the loss of short responses, one can train a separate model using short sequences.}
(2) Instead of using the same learning rate for all examples, using a weighted learning rate that considers the average tf-idf score for tokens within the response.
Such a strategy
 decreases the influence from dull and generic utterances.\footnote{We treat each sentence as a document. Stop words are removed.
 Learning rates are normalized within one batch. 
 For example, suppose , , ..., , ... , denote the tf-idf scores for  sentences within current batch and  denotes the original learning rate. The learning rate for sentence with index  is . 
 To avoid exploding learning rates for sequences with extremely rare words, the tf-idf score of a sentence 
 is capped at  times the minimum tf-idf score in the current batch.  is empirically chosen and is set to 3.
  } (3) Penalizing intra-sibling ranking when doing beam search decoding to promote N-best list diversity as described in \newcite{li2016simple}.  (4) Penalizing word types (stop words excluded) that have already been generated. Such a strategy dramatically decreases the rate of repetitive responses such as {\it no. no. no. no. no.} or contradictory responses such as {\it I don't like oranges but i like oranges}. 

\section{Adversarial Evaluation}
In this section, we discuss strategies for successful adversarial evaluation. 
Note that the proposed adversarial training and adversarial evaluation 
are separate procedures. 
They are independent of each other and share no common parameters. 


 The idea of adversarial evaluation, first proposed by 
\newcite{bowman2015generating}, is to 
train a discriminant
function
 to separate  generated and
true sentences, in an attempt to evaluate the model's sentence generation capability.
The idea has been preliminarily studied by \newcite{kannan} in the context of dialogue generation. 
Adversarial evaluation  also resembles the idea of the Turing test,
which requires a human evaluator 
to distinguish  machine-generated texts from human-generated ones. 
Since it is time-consuming and costly to 
 ask a human to talk to a model 
and give judgements,
we  train a machine evaluator
in place of the  human evaluator to distinguish the human dialogues and machine dialogues, and we use it to measure 
 the general quality of the generated responses. 

Adversarial evaluation involves both  training and testing. 
At training time, the evaluator is trained to
label dialogues as machine-generated (negative) or  human-generated (positive). 
At test time, the trained evaluator is evaluated on a held-out dataset.
If the human-generated dialogues and machine-generated ones are indistinguishable, the model  will achieve 50 percent accuracy at test time.  
\subsection{Adversarial Success}
We define Adversarial Success ({\it AdverSuc} for short) to be the fraction of instances in which a model is capable of fooling the evaluator. {\it AdverSuc} is the difference between 1 and the accuracy 
achieved by the  evaluator. Higher values of {\it AdverSuc} for a dialogue generation model are better. 
\begin{comment}
Existing evaluation metrics---e.g., word-overlap metrics such as BLEU and METEOR, or the recently proposed dialogue evaluation metric ADEM \cite{hey}, which learns to predict human evaluation score to input responses---all focus on single turn dialogue quality evaluation. We argue that a dialogue generation model can be more properly evaluated on multi-turn dialogues. 
This is comparable to  the Turing test, in which the human evaluator is asked to chat with the machine for five minutes rather than just a single turn. 
We therefore propose evaluating the model
using multi-turn dialogues generated 
 by having  two bots  talk with each other. 
 \end{comment}
\begin{comment}
We  report {\it AdverSuc} scores
 on dialogues with different numbers of simulated turns (length), specifically, {\it AdverSuc-1}, {\it AdverSuc-2} and {\it AdverSuc-3}, where {\it AdverSuc-N} denotes
the 
{\it AdverSuc} score
 of an evaluator trained on dialogues 
 between the two bots
 simulated for  turns. Theoretically, the larger  is, the easier it is for the discriminator to recognize a machine-generated dialogue and thus the lower the {\it AdverSuc} score should be. 
 \end{comment}
\subsection{Testing the Evaluator's Ability}
One caveat  with the adversarial evaluation methods is that 
they are model-dependent. 
We approximate the human evaluator in the Turing test with an automatic evaluator and assume that the evaluator is perfect: low accuracy of the discriminator should indicate high quality of the responses, since we interpret this to mean the generated responses are indistinguishable from the human ones. Unfortunately, there is another factor that can lead to low discriminative accuracy: a poor discriminative model. 
 Consider a  discriminator that always gives random labels or always gives the same label.
 Such an evaluator 
   always yields a high {\it AdverSuc} value of 0.5. 
   \newcite{bowman2015generating} propose  two different discriminator models 
separately using {\it unigram} features and {\it neural} features. It is hard to tell which feature set is more reliable. 
The standard strategy of testing the model on a held-out development set is not suited to this case, since a model that overfits the development set is necessarily superior. 


To deal with this issue, we propose setting up a few manually-invented situations to test the ability of the automatic evaluator.
 This is akin to setting up  examinations to test the ability of the human evaluator in the Turing test. 
We  report not only the {\it AdverSuc} values,  but also the scores that the evaluator
achieves
 in these manually-designed test cases, indicating how much we can trust the reported {\it AdverSuc}. 
 We develop scenarios in which we know  in advance how a perfect evaluator should behave, and then compare   {\it AdverSuc}  from a discriminative model with the gold-standard  {\it AdverSuc}. Scenarios we design include:
 \begin{tightitemize}
 \item We use human-generated dialogues as both positive examples and negative examples. A perfect evaluator should give an {\it AdverSuc} of  (accuracy ), which is the gold-standard result.
  \item We use machine-generated dialogues as both positive examples and negative examples. A perfect evaluator should give an {\it AdverSuc} of  (accuracy ).
 \item We use original human-generated dialogues as positive examples and dialogues 
 consisting of
  random utterances 
  as negative examples. A perfect evaluator should give an {\it AdverSuc} of  (accuracy ).
 \item We use original human-generated dialogues as positive examples and dialogues with 
 responses replaced by the utterances immediately following the true responses. 
 A perfect evaluation should give an {\it AdverSuc} of  (accuracy ).
\end{tightitemize}
The {\it evaluator reliability error} (ERE) is the average deviation of an evaluator's adversarial error
 from the gold-standard error in the above tasks, with equal weight for each task. 
The smaller the error, the more reliable the evaluator is. 

\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{p{4cm}p{11cm}}\hline
{\bf Input}&tell me ... how long have you had this falling sickness ?\\
{\bf ~~Vanilla-MLE}&i 'm not a doctor .\\
{\bf ~~Vanilla-Sample}&well everything you did was totally untrue .\\
{\bf ~~REINFORCE}&i don 't know how long it 's been .\\
{\bf ~~REGS Monte Carlo}& A few months, I guess .\\\hline
{\bf Input}&so i had the doctors test sammy 's response to conditioning .\\
{\bf ~~Vanilla-MLE}&sammy wrote the test sammy wrote the test .\\
{\bf ~~Vanilla-Sample}&objects pick objects up objects objects objects objects objects \\
{\bf ~~REINFORCE}&i 'm not sure that 's relevant .\\
{\bf ~~REGS Monte Carlo}&so he took the pills .\\\hline
{\bf Input}&what are you going to do ?\\
{\bf ~~Vanilla-MLE}&i 'm going to the movies .\\
{\bf ~~Vanilla-Sample}&get him outta here first !\\
{\bf ~~REINFORCE}&i 'm going to get you .\\
{\bf ~~REGS Monte Carlo}&i 'm going to get you a new car .\\\hline
{\bf Input}&they fear your power your intellect .\\
{\bf ~~Vanilla-MLE}&you 're the only one who knows what 's going on .\\
{\bf ~~Vanilla-Sample}&when they are conquered and you surrender they will control all of us .\\
{\bf ~~REINFORCE}&i 'm afraid i 'm not ready yet .\\
{\bf ~~REGS Monte Carlo}&i 'm not afraid of your power .\\\hline
\end{tabular}
\caption{Sampled responses from different models. More in Appendix Tables \ref{example-appendix1} and \ref{example-appendix2}.}
\end{table*}


\subsection{Machine-vs-Random Accuracy}
Evaluator reliability error uses scenarios constructed from human-generated
dialogues to assess feature or hyper-parameter choice for the evaluator. Unfortunately, no machine-generated responses are involved in the ERE metric. 
The following example illustrates the serious weakness resulting from this strategy:
as will be shown in the experiment section, 
when inputs are decoded using greedy or beam search models,  
most generation systems to date yield an adversarial success less than 10 percent (evaluator accuracy 90 percent). 
But when using sampling for decoding, the adversarial success skyrockets to around 40 percent,\footnote{Similar results are also reported in \newcite{kannan}.} only 10 percent less than what's needed to pass the Turing test. 
A close look at the decoded sequences using sampling tells a different story: the responses from sampling are sometimes incoherent, irrelevant or even ungrammatical. 

We thus propose an additional sanity check, in which we report the accuracy of distinguishing between machine-generated responses and randomly sampled responses ({\it machine-vs-random} for short). 
This resembles the N-choose-1 metric described in \newcite{shao15}. 
Higher accuracy indicates that the generated responses are distinguishable from randomly sampled human responses, indicating that the generative model is not fooling the generator simply by introducing randomness. 
As we will show in Sec.~\ref{sec:experiments}, using sampling results in high {\it AdverSuc} values but low {\it machine-vs-random} accuracy. 
\begin{comment}
\subsection{A Special Issue: Sequence Length}
It is widely accepted that
\sts models have a extremely strong bias towards generating short responses \cite{sountsov2016length}, and it is usually hard to generate coherent and meaningful long responses \cite{shao15}. 
The length of the sequence is thus a feature that might dominate the evaluator.\footnote{Only using this single feature, a evaluator can achieve an accuracy of roughly 75 percent.} 
On one hand, we acknowledge the inferiority of algorithms to date and the incapability of generating long, coherent responses;
on the other hand, we also want to compare different models
and evaluate 
 different aspects of responses such as coherence, meaningfulness, etc. 
Excluding the effect of this predominant feature is thus favorable in comparing different models. 
In our experimental section, we will report the discriminative success both including and excluding the effect of sequence length. 
\end{comment}

\section{Experimental Results} \label{sec:experiments}
In this section, 
we detail experimental results on adversarial success and human evaluation.  

\begin{table}[htb]
\centering
\small
\begin{tabular}{ccc}
Setting&ERE\\\hline
SVM+Unigram&0.232\\
Concat Neural &0.209 \\
Hierarchical Neural &0.193 \\
SVM+Neural+multil-features&0.152 \\
\hline
\end{tabular}
\caption{ERE scores obtained by different models.}
\label{ERE}
\end{table} 


\subsection{Adversarial Evaluation}
\paragraph{ERE} We first test 
 adversarial evaluation models with different 
 feature sets and 
 model architectures for reliability, as measured by evaluator reliability error (ERE).
 We explore the following models:
   (1) {\it SVM+Unigram}:  SVM using unigram features.\footnote{Trained using the SVM-Light package \cite{joachims2002learning}.}
 A multi-utterance dialogue (i.e., input messages and responses) is transformed to a unigram representation; (2) 
{\it Concat Neural}: 
a neural classification model with 
a softmax function that takes as input the concatenation of representations of constituent dialogues sentences;
 (3) {\it Hierarchical Neural}: 
  a hierarchical encoder    
  with a  structure similar to the discriminator used in the reinforcement; and
  (4) 
  {\it SVM+Neural+multi-lex-features}: 
  a SVM model that uses the following features: unigrams,  neural representations of dialogues obtained by the neural model trained using strategy (3),\footnote{The representation before the softmax layer.} the forward likelihood  and backward likelihood .

ERE scores obtained by different models are reported in Table \ref{ERE}. 
As can be seen, the {\it hierarchical neural} evaluator (model 3) is more reliable than simply concatenating the sentence-level representations (model 2).
Using the combination of neural features and lexicalized features yields the most reliable evaluator. 
For the rest of this section, we report results obtained 
by the 
{\it Hierarchical Neural} setting due to its end-to-end nature, despite its inferiority to {\it SVM+Neural+multil-features}. 

Table \ref{adv} presents
 {\it AdverSuc} values for different models, along with {\it machine-vs-random} accuracy described in Section 4.3. 
Higher values of  {\it AdverSuc}  and  {\it machine-vs-random} are better. 

Baselines we consider include standard \sts models using greedy decoding ({\it MLE-greedy}), beam-search ({\it MLE+BS}) and sampling, as well as the 
mutual information reranking model of \newcite{li2015diversity} with two algorithmic variations: (1) MMI+, in which a large N-best list is first generated using a pre-trained \sts model and then reranked by the backward probability  and (2) MMI, in which language model probability is penalized during decoding. 

Results are shown in Table \ref{adv}. What first stands out  is decoding using sampling (as discussed in Section 4.3), achieving a significantly higher {\it AdverSuc} number than all the rest models. 
However, this does not indicate the superiority of the sampling decoding model, since the {\it machine-vs-random} accuracy is at the same time significantly lower. This means that sampled responses based on \sts models are not only hard for an evaluator to distinguish from real human responses, but also from randomly sampled responses.
A similar, though much less extreme, effect is observed for MMI, which has an {\it AdverSuc} value slightly higher than {\it Adver-Reinforce}, but a significantly lower {\it machine-vs-random} score. 

By comparing different baselines, we find that MMI+ is better than {\it MLE-greedy}, which is in turn better than {\it MLE+BS}. This result is in line with human-evaluation results from \newcite{li2015diversity}. 
The two proposed adversarial algorithms achieve better performance than the baselines. We expect this to be the case, since the adversarial algorithms are trained on an objective function 
more similar to the
evaluation metric (i.e., adversarial success). 
{\it REGS} performs slightly better than the vanilla REINFORCE algorithm. 

\begin{table}
\small
\centering
\begin{tabular}{ccc}
\hline
Model&{\it AdverSuc}&{\it machine-vs-random} \\\hline
MLE-BS&0.037&0.942 \\
MLE-Greedy&0.049&0.945 \\ 
MMI+&0.073&0.953\\
MMI-&0.090& 0.880\\
Sampling&0.372&0.679\\\hline
Adver-Reinforce&0.080&0.945 \\
Adver-REGS&0.098&0.952\\
\hline
\end{tabular}
\caption{{\it AdverSuc} and {\it machine-vs-random} scores achieved by different training/decoding strategies.}
\label{adv}
\end{table}

\begin{comment}
According to the {\it evaluator reliability error} shown in the first row of Table \ref{adver}, the neural evaluator is generally more reliable than the unigram evaluator. Also, the 
evaluator  becomes more reliable as the number of simulated turns increases. 

The proposed adversarial  model yields better adversarial error scores than the  baselines. 
Interestingly, we find that {\it sampling} generally performs  better than beam search. 
This is similar to phenomenon reported in \newcite{li2015diversity} that greedy decoding yields better results than beam-search. 
All  models perform poorly on {\it adver-3} evaluation metric, with the best adversarial error being 0.422 (the trained evaluator is able to  distinguish between human-generated and machine-generated dialogues with greater than 90 percent accuracy for all models), indicating that none of the models we have to date is capable of simulating conversations longer than three turns.

We observe a performance boost from the {\it two-turn} adversarial model over the {\it single-turn} model. 
The explanation is as follows: dull, generic responses are commonly found in the training set. If we are doing single-turn simulation, it might confuse the discriminator in terms of whether to label the dull responses as positive or negative. 
 However, it is very rare that the two consecutive turns in a normal conversation are both dull and generic. The  discriminator in the two-turn simulation can thus more confidently assign negative rewards to the commonplace simulated instances. As the reward is back-propagated to the generative model, the probability of generating dull responses will be pushed down for both turns.

We do not see a significant performance boost yielded by the {\it three-turn} training strategy 
over the 
 {\it two-turn} strategy. The performance even deteriorates as the number of the simulated turns surpasses three. We suspect that this is because: (1) The number of actions   grows linearly with the number the simulated turns. As the sequence of actions becomes longer, the system becomes increasingly hard to train.
(2) The discriminator is too much more powerful than the generator as the number of the simulated turns becomes larger. The results for {\it four-turn} simulation are thus omitted for brevity.  \todo{(WM: maybe list 4-turn numbers anyway?)}
\end{comment}
\begin{comment}
\subsection{N-choose-1 Metric}
We evaluate the model using an N-choose-1 ranking metric, as described in \newcite{shao15}. 
 In
the N-choose-1 evaluation, given dialogue history, we ask the generator to score N candidate responses, where one of the N candidate responses is the ground truth and the rest are random responses.
N-choose-1 accuracy is defined to be the fraction of test cases in which the trained model assigns the highest score to the ground truth.
We report 50-, 10-, and 2-choose-1 accuracy. Results are shown in Table \ref{choose}. 
Again we see a clear performance boost from the adversarial model over the
standard \sts variants.

\begin{table}
\small
\centering
\begin{tabular}{cccc}
\hline
Model&50&10&2\\\hline
\sts&0.087&0.28&0.73 \\
MMI&0.43&0.69&0.88\\
Adversarial&0.53&0.75&0.95\\\hline
\end{tabular}
\caption{N-choose-1 evaluation on different models.}
\label{choose}
\end{table}
\end{comment}

\subsection{Human Evaluation}
For human evaluation, we follow protocols defined in \newcite{li2016deep}, employing
crowdsourced judges to evaluate a random sample of
200 items. We present both an input message and the
generated outputs to 3 judges and ask them to decide
which of the two outputs is better ({\it single-turn}
general quality). Ties are permitted. Identical
strings are assigned the same score. 
We also
 present the judges with {\it multi-turn}
conversations simulated between the two agents. Each conversation
consists of 3 turns.
Results are presented in Table \ref{human}.
We observe a significant quality improvement on both 
single-turn quality and multi-turn quality from the proposed adversarial model.
It is worth noting that the reinforcement learning system described in \newcite{li2016deep}, which 
simulates conversations between two bots and 
is trained based on manually designed reward functions, only improves multi-turn dialogue quality, while the model described in this paper improves both single-turn and multi-turn dialogue generation quality. 
This confirms that the reward adopted in adversarial training is more general, natural and effective in training dialogue systems.


\begin{table}
\small
\centering
\begin{tabular}{cccc}\\\hline
Setting &adver-win &adver-lose &tie\\\hline
single-turn& 0.62& 0.18 &0.20 \\
multi-turn& 0.72 &0.10& 0.18\\\hline
\end{tabular}
\caption{The gain
from the proposed adversarial model
 over the mutual information system
based on pairwise human judgments.}
\label{human}
\end{table}

\begin{comment}
\begin{table}
\centering
\small
\begin{tabular}{l}\hline
{\bf Input}: {Do you love me?} (1) \\\hline
{\it Mutual information reranking} \\ 
{Yes, of course}. (2)\\
 {I love you too.} (3)\\
{Goodbye, honey}. (4) \\\hline
{\it Adversarial training} \\
More than anything in this world. (2) \\
Then we can stay together, you and me. (3) \\
Yes, we can. (4) \\\hline
{\bf Input}: {Where are you going?} (1) \\\hline
{\it Mutual information reranking} \\ 
I am going to the restroom. (2) \\
See you later. (3) \\
See you later. (4)\\\hline
{\it Adversarial training} \\
I'm going to join the Army. (2)\\
 You're joking! (3)\\
 No I'm not. (4)\\\hline
\end{tabular}	
\caption{
Multi-turn dialogue simulation. The first turn (index 1) is input by the authors. Then the two agents take turns conversing, taking as input the other agent's prior generated turn.}
\label{multi}
\end{table}
\end{comment}






\section{Conclusion and Future Work}
In this paper, drawing  intuitions from the Turing test, we propose using an adversarial training approach for response generation. We cast the model in the framework of reinforcement learning and train a generator based on the signal from a discriminator to generate response sequences indistinguishable from human-generated dialogues.
We observe clear performance improvements on multiple metrics from the adversarial training strategy.

The adversarial training model should theoretically benefit a variety of generation tasks in NLP. Unfortunately, in preliminary experiments applying the same training paradigm to machine translation, we did not observe a clear performance boost. We conjecture that this is because the adversarial training strategy 
is more beneficial to
 tasks in which there is 
 a big discrepancy between the distributions of the generated sequences and the  reference target sequences. In other words, the adversarial approach is more beneficial on tasks in which entropy of the targets is  high. Exploring this relationship further is a focus of our future work. 
 ~~\\~~\\
 \paragraph{Acknowledgements}
 The authors thank Michel Galley, Bill Dolan, Chris Brockett, Jianfeng Gao 
and other members of the NLP group at Microsoft Research, as well as
 Sumit Chopra and Marc'Aurelio Ranzato from Facebook AI Research for helpful discussions and comments. 
Jiwei Li is supported by a Facebook Fellowship,  which we gratefully acknowledge. 
This work is also partially supported by the NSF under award
IIS-1514268, and the DARPA Communicating
with Computers (CwC) program under ARO prime
contract no. W911NF- 15-1-0462, IIS-1464128. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the authors and
do not necessarily reflect the views of DARPA, the NSF,
or Facebook.
 

\bibliographystyle{acl_natbib}
\bibliography{emnlp2016}


\begin{table*}[!htb]
\centering
\footnotesize
\begin{tabular}{p{4cm}p{8cm}}\hline
{\bf Input}&maybe his eyes were too big for his stomach .\\
{\bf Vanilla-MLE}&he 's got a big mouth .\\
{\bf Vanilla-Sample}&i 'm sure he 's unhappy with the fact .\\
{\bf Reinforce}&he 's a little bit of a pain .\\
{\bf MC}&i don 't think so .\\\hline
{\bf Input}&they didn 't have much success with this problem commander .\\
{\bf Vanilla-MLE}&they 're not the only ones who have been in the system .\\
{\bf Vanilla-Sample}&it 's the new priority only .\\
{\bf Reinforce}&they 're not the only ones who can help us .\\
{\bf MC}&can we find someone else ?\\\hline
{\bf Input}&and he carried them all the time wherever he went .\\
{\bf Vanilla-MLE}&i don 't know who he is .\\
{\bf Vanilla-Sample}&the church ... suffered the winds of 900 meters with horse aid .\\
{\bf Reinforce}&he was a good man .\\
{\bf MC}&he was a very good man .\\\hline
{\bf Input}&somebody probably flushed their pet snake down the toilet .\\
{\bf Vanilla-MLE}&i 'm sure the dog will be here soon .\\
{\bf Vanilla-Sample}&i could at least think .\\
{\bf Reinforce}&i don 't think so .\\
{\bf MC}&i don 't think so .\\\hline
{\bf Input}&excuse me but how did we pay for these suits ?\\
{\bf Vanilla-MLE}&i 'm not a beggar .\\
{\bf Vanilla-Sample}&is this the money eugene ?\\
{\bf Reinforce}&i don 't know what to do .\\
{\bf MC}& how much money you owe me ?\\\hline
{\bf Input}&... while the beautiful and happy world was madly destroyed ;\\
{\bf Vanilla-MLE}&i was born in the world of the world\\
{\bf Vanilla-Sample}&as in childhood i took you to one tenth of a postcard\\
{\bf Reinforce}&and the world was full of joy .\\
{\bf MC}&and the world was full of joy .\\\hline
{\bf Input}&i was kinda hoping that you were my dad .\\
{\bf Vanilla-MLE}&i 'm just a little bit of a little bit of a little bit of a little bit of a\\
{\bf Vanilla-Sample}&shiny gold gold and silver in the mountains .\\
{\bf Reinforce}&i 'm not gonna be a baby .\\
{\bf MC}&i 'm not going to be a father .\\\hline
{\bf Input}&the man who does that is not afraid to die .\\
{\bf Vanilla-MLE}&he 's not afraid of the dark .\\
{\bf Vanilla-Sample}&even the program is the only truth\\
{\bf Reinforce}&he 's also not afraid of me .\\
{\bf MC}&he 's such a brave man !\\\hline
{\bf Input}&do you have any idea what caused the explosion ?\\
{\bf Vanilla-MLE}&i think it 's the same thing .\\
{\bf Vanilla-Sample}&i 'm still very nervous .\\
{\bf Reinforce}&maybe it 's just a coincidence .\\
{\bf MC}&i don 't know if it 's a coincidence .\\\hline
{\bf Input}&don 't bother the citizens in hong kong island anymore\\
{\bf Vanilla-MLE}&i 'll be back in a few days\\
{\bf Vanilla-Sample}&you wiii be transferred to the target court\\
{\bf Reinforce}&i 'm not sure if you can handle it without bothering them .\\
{\bf MC}&i 'm not going to be a nuisance\\\hline
{\bf Input}&you know you ought to find yourself a girl .\\
{\bf Vanilla-MLE}&you know what i mean ?\\
{\bf Vanilla-Sample}&it 's full of crime ...\\
{\bf Reinforce}&i don 't know what you 're talking about .\\
{\bf MC}&i don 't want to be a girl .\\\hline
{\bf Input}&i got you the big screen tv deluxe UNknown machine\\
{\bf Vanilla-MLE}&i 'm gonna be a little late for the movie .\\
{\bf Vanilla-Sample}&you mean you 're living ?\\
{\bf Reinforce}&i 'm gonna get you a new one .\\
{\bf MC}&i 'm gonna get you a new tv set .\\\hline
{\bf Input}&we won 't give her the chance to surprise us .\\
{\bf Vanilla-MLE}&she 's not going to be a party member .\\
{\bf Vanilla-Sample}&the timing 's good and bad .\\
{\bf Reinforce}&we 'll have to get her back .\\
{\bf MC}&i don 't think we have to worry about her .\\\hline
{\bf Input}&does it live in this matter we call dna ?\\
{\bf Vanilla-MLE}&i don 't think so .\\
{\bf Vanilla-Sample}&and it requires time to be wrong and no one believes it .\\
{\bf Reinforce}&it 's a genetic disorder .\\
{\bf MC}&i don 't think so .\\\hline
\end{tabular}
\caption{Appendix: Responses sampled from different models.}
\label{example-appendix1}
\end{table*}


\begin{table*}[!htb]
\centering
\footnotesize
\begin{tabular}{p{4cm}p{8cm}}\hline

{\bf Input}&i 'll have to drive home .\\
{\bf Vanilla-MLE}&i 'll be back in a minute .\\
{\bf Vanilla-Sample}&let 's go for some fun .\\
{\bf Reinforce}&i 'll drive you home .\\
{\bf MC}&i don 't have a car .\\\hline
{\bf Input}&i had to work late and then i went for a walk .\\
{\bf Vanilla-MLE}&i was in the middle of the road .\\
{\bf Vanilla-Sample}&my body was just pounding .\\
{\bf Reinforce}&i was just trying to help you .\\
{\bf MC}&i was just trying to get a little sleep .\\\hline
{\bf Input}&lots of cream lots of sugar .\\
{\bf Vanilla-MLE}&i 'm gonna be in the kitchen .\\
{\bf Vanilla-Sample}&was this gay or red ?\\
{\bf Reinforce}&i don 't want to be a vegetable .\\
{\bf MC}&i 'm not a very good cook .\\\hline
{\bf Input}&so they just let him go ?\\
{\bf Vanilla-MLE}&he 's not gonna let them go through the motions .\\
{\bf Vanilla-Sample}&they asked him whether we could go see ernesto ?\\
{\bf Reinforce}&i don 't know what to do .\\
{\bf MC}&he 's not going to be here any longer .\\\hline
{\bf Input}&you must not bomb this town .\\
{\bf Vanilla-MLE}&you have to go back to the city .\\
{\bf Vanilla-Sample}&the one is a multiple choice .\\
{\bf Reinforce}&I think I have to .\\
{\bf MC}&I think I have to .\\\hline
{\bf Input}&communism recently failed in eastern europe .\\
{\bf Vanilla-MLE}&the war is not over .\\
{\bf Vanilla-Sample}&there are people who hope that in tokyo they won 't risk losing their departing .\\
{\bf Reinforce}&and the people who are not the same .\\
{\bf MC}&the war is still going on .\\\hline
{\bf Input}&you should stop in more often .\\
{\bf Vanilla-MLE}&I don 't think so .\\
{\bf Vanilla-Sample}&is it possible as old as you ?\\
{\bf Reinforce}&i 'm not going to be a burden on you .\\
{\bf MC}&i don 't want to be a burden .\\\hline{\bf Input}&he couldn 't have heard us .\\
{\bf Vanilla-MLE}&he was in the middle of the night  .\\
{\bf Vanilla-Sample}&he should have been somewhere out here .\\
{\bf Reinforce}&he 's not a good guy .\\
{\bf MC}&he 's got a lot of nerve.\\\hline
{\bf Input}&that 's our wake up call .\\
{\bf Vanilla-MLE}&we 're gonna be late for the meeting .\\
{\bf Vanilla-Sample}&c 'mon c 'mon let 's get outta here .\\
{\bf Reinforce}&i 'll be right back .\\
{\bf MC}&we 're gonna have to get to the station .\\\hline
{\bf Input}&i had 'em when i started .\\
{\bf Vanilla-MLE}&i 'm gonna get 'em .\\
{\bf Vanilla-Sample}&those weak rotten guys had me in a rush on a broken swoop .\\
{\bf Reinforce}&i don 't know what to do .\\
{\bf MC}&i was just a little bit nervous .\\\hline
{\bf Input}&oatmeal delicious start into a new day\\
{\bf Vanilla-MLE}& i 'll be right back\\
{\bf Vanilla-Sample}&sure if you don 't put it into the water\\
{\bf Reinforce}&i 'm gonna be a little busy with the dishes .\\
{\bf MC}&i 'm gonna make you a little dinner .\\\hline
\end{tabular}
\caption{Appendix: More responses sampled from different models.}
\label{example-appendix2}
\end{table*}


\end{document}
