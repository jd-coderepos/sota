

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{microtype}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{dsfont}
\usepackage{arydshln}


\aclfinalcopy 



\setcounter{dbltopnumber}{8}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}

\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Neural Belief Tracker: Data-Driven Dialogue State Tracking}
  
\author{Nikola Mrk\v{s}i\'c, ~ Diarmuid {\'O S\'eaghdha} \\ 
\textbf{Tsung-Hsien Wen, ~ {Blaise Thomson, ~ Steve Young}}  \\
   University of Cambridge  \\
   Apple Inc.  \\
  { \tt \{nm480, thw28, sjy\}@cam.ac.uk} \\ { \tt\{doseaghdha, blaisethom\}@apple.com}}

\date{}

\begin{document}
\maketitle


\begin{abstract}

One of the core components of modern spoken dialogue systems is the \textit{belief tracker}, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: \textbf{a)} Spoken Language Understanding models that require large amounts of annotated training data; or \textbf{b)} hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.



\end{abstract}

\section{Introduction}

Spoken dialogue systems (SDS) allow users to interact with computer applications through conversation. Task-based systems help users achieve goals such as finding restaurants or booking flights. The \emph{dialogue state tracking} (DST) component of an SDS serves to interpret user input and update the \emph{belief state}, which is the system's internal representation of the state of the conversation \cite{young:10c}. This is a probability distribution over dialogue states used by the downstream \emph{dialogue manager} to decide which action the system should perform next \cite{su:2016:nnpolicy,Su:16}; the system action is then verbalised by the {natural language generator} \cite{wen:15a,wen:15b,Dusek:15}.   

\begin{figure} 
{
\begin{tabular}{p{9cm}}
  \textbf{User:} I'm looking for a \underline{cheaper} restaurant\\
  \texttt{inform(price=\underline{cheap})}\0.4ex]
  \textbf{System:} The House serves cheap Thai food\\
  \textbf{User:} Where is it?\\
  \texttt{inform(price=cheap, food=Thai, area=centre); request(address)}\0.8ex]
   \textsc{\textbf{Rating=High:}} [best, high-rated, highly rated, \\ top-rated, cool, chic, popular, trendy, ...]   \
\mathbf{v}_{i}^{n} = \mathbf{u}_{i} \oplus \ldots \oplus \mathbf{u}_{i+n-1}

\mathbf{r}_{n} = \sum_{i=1}^{k_u-n+1}{\mathbf{v}_{i}^{n}} 
\mathbf{r}_{n}' = \sigma (W_{n}^{s}\mathbf{r}_{n} + b_{n}^{s})  

\mathbf{r} ~ = ~ \mathbf{r}_{1}' + \mathbf{r}_{2}' +\mathbf{r}_{3}' \label{eqn:r} \\

{R}_{n} = F_n^s ~ \mathbf{m}_n

\mathbf{r}_{n}' = \mathtt{maxpool} \left( \mathtt{ReLU} \left( {R}_{n} + b_{n}^{s} \right)   \right)  
\mathbf{c} ~ &= \sigma \big( W_{c}^{s} (\mathbf{c_{s}} + \mathbf{c_{v}}) + b_{c}^{s} \big) \\
\mathbf{d} &=  \mathbf{r} \otimes \mathbf{c}   
 
\mathbf{m_{r}} &= ~ (\mathbf{c_{s}} \cdot \mathbf{t_{q}}) \mathbf{r} \\
\mathbf{m_{c}} &= ~ ( \mathbf{c_{s}} \cdot \mathbf{t_{s}} )  ( \mathbf{c_{v}} \cdot \mathbf{t_{v}} ) \mathbf{r}

\mathbf{y} &=  \phi_{2} \big( \phi_{100}(\mathbf{d}) + \phi_{100}({\mathbf{m_r}}) + \phi_{100}({\mathbf{m_c}}) \big)

\mathbb{P}(s, v \mid h^{t}, sys^{t-1}) = \sum_{i=1}^{N} p_{i}^{t} ~ \mathbb{P}\left( s,v \mid h_{i}^{t}, sys^{t}\right)  

\mathbb{P}(s, v \mid h^{1:t}, sys^{1:t-1}) ~=~ \lambda ~ \mathbb{P}\left(s, v \mid h^{t}, sys^{t-1}\right)   \\
                             + ~ (1 - \lambda) ~ \mathbb{P}\left(s, v \mid h^{1:t-1}, sys^{1:t-2}\right)

V_{s}^{t} = \lbrace {v \in V_{s}} ~ \mid ~ {\mathbb{P}\left( s,v \mid h^{1:t}, sys^{1:t-1} \right) \geq 0.5} \rbrace

For informable (i.e.~goal-tracking) slots, the value in   with the highest probability is chosen as the current goal (if ). For requests, all slots in  are deemed to have been requested. As requestable slots serve to model single-turn user queries, they require no belief tracking across turns.


\section{Experiments}

\subsection{Datasets}

Two datasets were used for training and evaluation. Both consist of user conversations with task-oriented dialogue systems designed to help users find suitable restaurants around Cambridge, UK.  The two corpora share the same domain ontology, which contains three \emph{informable} (i.e.~goal-tracking) slots: \textsc{food}, \textsc{area} and \textsc{price}. The users can specify {values} for these slots in order to find restaurants which best meet their criteria. Once the system suggests a restaurant, the users can ask about the values of up to eight \emph{requestable} slots (\textsc{phone number, address}, etc.). The two datasets are: 

\begin{enumerate}

\item \textbf{DSTC2}: We use the transcriptions, ASR hypotheses and turn-level semantic labels provided for the Dialogue State Tracking Challenge 2 \cite{Henderson:14a}. The official transcriptions contain various spelling errors which we corrected manually; the cleaned version of the dataset is available at \url{mi.eng.cam.ac.uk/~nm480/dstc2-clean.zip}. The training data contains 2207 dialogues \iffalse (15,611 dialogue turns) \fi and the test set consists of 1117 dialogues. We train NBT models on transcriptions but report belief tracking performance on test set ASR hypotheses provided in the original challenge. 

\item \textbf{WOZ 2.0}: Wen et al.~\shortcite{Wen:16} performed a Wizard of Oz style experiment in which Amazon Mechanical Turk users assumed the role of the system or the user of a task-oriented dialogue system based on the DSTC2 ontology. Users typed instead of using speech, which means performance in the WOZ experiments is more indicative of the model's capacity for semantic understanding than its robustness to ASR errors. Whereas in the DSTC2 dialogues users would quickly adapt to the system's (lack of) language understanding capability, the WOZ experimental design gave them freedom to use more sophisticated language. We expanded the original WOZ dataset from Wen et al.~\shortcite{Wen:16} using the same data collection procedure, yielding a total of 1200 dialogues. \iffalse (5,012 turns). \fi We divided these into 600 training, 200 validation and 400 test set dialogues. The WOZ 2.0 dataset is available at \url{mi.eng.cam.ac.uk/~nm480/woz_2.0.zip}.

\end{enumerate}

\paragraph{Training Examples} The two corpora are used to create training data for two separate experiments. For each dataset, we iterate over all train set utterances, generating one example for \emph{each} of the slot-value pairs in the ontology. An example consists of a transcription, its context (i.e.~list of preceding system acts) and a candidate slot-value pair. The binary label for each example indicates whether or not its utterance and context express the example's candidate pair. For instance, `\emph{I would like Irish food}' would generate a positive example for candidate pair \textsc{food={Irish}}, and a negative example for every other slot-value pair in the ontology.   


\paragraph{Evaluation} We focus on two key evaluation metrics introduced in \cite{Henderson:14a}: \vspace{-0mm}
\begin{enumerate}
\item \textbf{Goals} (`joint goal accuracy'): the proportion of dialogue turns where all the user's search goal constraints were correctly identified; \vspace{-0mm}
\item \textbf{Requests}: similarly, the proportion of dialogue turns where user's requests for information were identified correctly. \vspace{-0mm}
\end{enumerate}


\subsection{Models}

We evaluate two NBT model variants: \textsc{NBT-DNN} and \textsc{NBT-CNN}. To train the models, we use the Adam optimizer \cite{Adam:15} with cross-entropy loss, backpropagating through all the NBT subcomponents while keeping the pre-trained word vectors fixed (in order to allow the model to deal with unseen words at test time). The model is trained separately for each slot. Due to the high class bias (most of the constructed examples are negative), we incorporate a fixed number of positive examples in each mini-batch.\footnote{Model hyperparameters were tuned on the respective validation sets.  For both datasets, the initial Adam learning rate was set to , and th of positive examples were included in each mini-batch. The batch size did not affect performance: it was set to 256 in all experiments. Gradient clipping (to ) was used to handle exploding gradients. Dropout \cite{Srivastava:2014} was used for regularisation (with 50\% dropout rate on all intermediate representations). Both \textsc{NBT} models were implemented in TensorFlow \cite{tf:15}. } 



\paragraph{Baseline Models} For each of the two datasets, we compare the NBT models to: 
\begin{enumerate}
\item A baseline system that implements a well-known competitive delexicalisation-based model for that dataset. For DSTC2, the model is that of Henderson et al.~\shortcite{Henderson:14d,Henderson:14b}. This model is an -gram based neural network model with recurrent connections between turns (but not inside utterances) which replaces occurrences of slot names and values with generic delexicalised features. For WOZ 2.0, we compare the NBT models to a more sophisticated belief tracking model presented in \cite{Wen:16}. This model uses an RNN for belief state updates and a CNN for turn-level feature extraction. Unlike \textsc{NBT-CNN}, their CNN operates not over vectors, but over delexicalised features akin to those used by \newcite{Henderson:14d}. 

\item The same baseline model supplemented with a task-specific semantic dictionary (produced by the baseline system creators). The two dictionaries are available at \url{mi.eng.cam.ac.uk/\~nm480/sem-dict.zip}. The DSTC2 dictionary contains only three rephrasings. Nonetheless, the use of these rephrasings translates to substantial gains in DST performance (see Sect.~6.1). We believe this result supports our claim that the vocabulary used by Mechanical Turkers in DSTC2 was constrained by the system's inability to cope with lexical variation and ASR noise. The WOZ dictionary includes 38 rephrasings, showing that the unconstrained language used by Mechanical Turkers in the Wizard-of-Oz setup requires more elaborate lexicons.  

\end{enumerate}

Both baseline models map exact matches of ontology-defined intents (and their lexicon-specified rephrasings) to one-hot delexicalised -gram features. This means that pre-trained vectors cannot be incorporated directly into these models.  

\section{Results}

\subsection{Belief Tracking Performance}

Table \ref{tab:dstc2_performance} shows the performance of NBT models trained and evaluated on DSTC2 and WOZ 2.0 datasets. The NBT models outperformed the baseline models in terms of both {joint goal} and request accuracies. For goals, the gains are \emph{always} statistically significant (paired -test, ). Moreover, there was no statistically significant variation between the NBT and the lexicon-supplemented models, showing that the NBT can handle semantic relations which otherwise had to be explicitly encoded in semantic dictionaries.

\begin{table*} [!t]
\centering

\begin{tabular}{l|cc|cc}
\multirow{2}{*}{\bf DST Model } & \multicolumn{2}{|c|}{\bf DSTC2} & \multicolumn{2}{|c}{\bf WOZ 2.0}  \\ 
                 & \bf Goals &  \bf Requests & \bf Goals & \bf Requests  \\ \hline
\bf Delexicalisation-Based Model &  69.1~    & 95.7 &  70.8~     &  87.1~    \\ 
\bf Delexicalisation-Based Model + Semantic Dictionary &  72.9* & 95.7 &   83.7* &  87.6~    \\ \hline
\bf \textsc{Neural Belief Tracker:} \textsc{NBT-DNN}      &  72.6* & 96.4 &  \bf 84.4* &  91.2* \\
\bf \textsc{Neural Belief Tracker:} \textsc{NBT-CNN}      &  \bf 73.4* & \bf 96.5 &  84.2* & \bf 91.6* \\ 
\end{tabular}\caption{DSTC2 and WOZ 2.0 test set accuracies for: \textbf{a)} joint goals; and \textbf{b)} turn-level requests. The asterisk indicates statistically significant improvement over the baseline trackers (paired -test; ).  \vspace{-1mm}   \label{tab:dstc2_performance}} 
\end{table*}

While the NBT performs well across the board, we can compare its performance on the two datasets to understand its strengths. The improvement over the baseline is greater on WOZ 2.0, which corroborates our intuition that the NBT's ability to learn linguistic variation is vital for this dataset containing longer sentences, richer vocabulary and no ASR errors. By comparison, the language of the subjects in the DSTC2 dataset is less rich, and compensating for ASR errors is the main hurdle: given access to the DSTC2 test set transcriptions, the NBT models' goal accuracy rises to 0.96. This indicates that future work should focus on better ASR compensation if the model is to be deployed in environments with challenging acoustics.


\subsection{The Importance of Word Vector Spaces}

The NBT models use the semantic relations embedded in the pre-trained word vectors to handle semantic variation and produce high-quality intermediate representations. Table \ref{tab:wv_comparison} shows the performance of \textsc{NBT-CNN}\footnote{The \textsc{NBT-DNN} model showed the same trends. For brevity, Table \ref{tab:wv_comparison} presents only the \textsc{NBT-CNN} figures. } models making use of three different word vector collections: \textbf{1)} `random' word vectors initialised using the \textsc{xavier} initialisation \cite{Glorot:2010aistats}; \textbf{2)} distributional GloVe vectors \cite{Pennington:14}, trained using co-occurrence information in large textual corpora; and \textbf{3)} \emph{semantically specialised} Paragram-SL999 vectors \cite{Wieting:15}, which are obtained by injecting \emph{semantic similarity constraints} from the Paraphrase Database \cite{ppdb:13} into the distributional GloVe vectors in order to improve their semantic content.  

The results in Table \ref{tab:wv_comparison} show that the use of semantically specialised word vectors leads to considerable performance gains: Paragram-SL999 vectors (significantly) outperformed GloVe and \textsc{xavier} vectors for goal tracking on both datasets. The gains are particularly robust for noisy DSTC2 data, where both collections of pre-trained vectors consistently outperformed random initialisation. The gains are weaker for the noise-free WOZ 2.0 dataset, which seems to be large (and clean) enough for the NBT model to learn task-specific rephrasings and compensate for the lack of semantic content in the word vectors. For this dataset, GloVe vectors do not improve over the randomly initialised ones. We believe this happens because distributional models keep related, yet antonymous words close together (e.g.~\emph{north} and \emph{south}, \emph{expensive} and \emph{inexpensive}), offsetting the useful semantic content embedded in this vector spaces. 


\begin{table} \centering
\resizebox{1.0\columnwidth}{!}{\begin{tabular}{c|cc|cc}
\multirow{2}{*}{\bf Word Vectors} &  \multicolumn{2}{|c|}{\bf DSTC2} & \multicolumn{2}{|c}{\bf WOZ 2.0}  \\ 

 & \bf  Goals &  \bf Requests & \bf Goals & \bf Requests  \\ \hline
\bf \textsc{xavier} (No Info.)   & 64.2~ & 81.2~  & 81.2~ & 90.7~  \\
\bf GloVe  &  69.0* &  96.4*  & 80.1~ & 91.4~ \\  
\bf Paragram-SL999  & \bf 73.4* & \bf 96.5* & \bf 84.2* & \bf 91.6 \\ 
\end{tabular}}
\caption{DSTC2 and WOZ 2.0 test set performance (\emph{joint goals} and \emph{requests}) of the \textsc{NBT-CNN} model making use of three different word vector collections. The asterisk indicates statistically significant improvement over the baseline \textsc{xavier} (random) word vectors (paired -test; ). \vspace{-0mm} \label{tab:wv_comparison}} 
\end{table}



\section{Conclusion}

In this paper, we have proposed a novel neural belief tracking (NBT) framework designed to overcome current obstacles to deploying dialogue systems in real-world dialogue domains. The NBT models offer the known advantages of coupling Spoken Language Understanding and Dialogue State Tracking, without relying on hand-crafted semantic lexicons to achieve state-of-the-art performance. Our evaluation demonstrated these benefits: the NBT models match the performance of models which make use of such lexicons and vastly outperform them when these are not available. Finally, we have shown that the performance of NBT models improves with the semantic quality of the underlying word vectors. To the best of our knowledge, we are the first to move past intrinsic evaluation and show that \emph{semantic specialisation} boosts performance in downstream tasks. 

In future work, we intend to explore applications of the NBT for multi-domain dialogue systems, as well as in languages other than English that require handling of complex morphological variation. 

\section*{Acknowledgements}

The authors would like to thank Ivan Vuli\'{c}, Ulrich Paquet, the Cambridge Dialogue Systems Group and the anonymous ACL reviewers for their constructive feedback  and helpful discussions.

\clearpage


\bibliography{acl2017}
\bibliographystyle{acl2017}


\clearpage


\end{document}
