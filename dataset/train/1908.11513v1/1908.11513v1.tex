

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{array}
\usepackage{bm}

\aclfinalcopy 



\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

\title{Adapting Meta Knowledge Graph Information for \\ Multi-Hop Reasoning over Few-Shot Relations}

\newcommand*{\affaddr}[1]{#1}
  \newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
  \newcommand*{\email}[1]{\texttt{#1}}

  \author{
    \textbf{Xin Lv}, \textbf{Yuxian Gu}, \textbf{Xu Han}, \textbf{Lei Hou}\thanks{\quad Corresponding Author}\hspace{0.5em}, \textbf{Juanzi Li}, \textbf{Zhiyuan Liu}\\
    Department of Computer Science and Technology, Tsinghua University \\
    KIRC, Institute for Artificial Intelligence, Tsinghua University \\
     Beijing National Research Center for Information Science and Technology \\
    \email{\{lv-x18,gu-yx17,hanxu17\}@mails.tsinghua.edu.cn}\\
    \email{\{houlei,lijuanzi,liuzy\}@tsinghua.edu.cn}
    }

\date{}

\begin{document}
\maketitle
\begin{abstract}
Multi-hop knowledge graph (KG) reasoning is an effective and explainable method for predicting the target entity via reasoning paths in query answering (QA) task. Most previous methods assume that every relation in KGs has enough training triples, regardless of those few-shot relations which cannot provide sufficient triples for training robust reasoning models. In fact, the performance of existing multi-hop reasoning methods drops significantly on few-shot relations. In this paper, we propose a meta-based multi-hop reasoning method (Meta-KGR), which adopts meta-learning to learn effective meta parameters from high-frequency relations that could quickly adapt to few-shot relations. We evaluate Meta-KGR on two public datasets sampled from Freebase and NELL, and the experimental results show that Meta-KGR outperforms the current state-of-the-art methods in few-shot scenarios. Our code and datasets can be obtained from  \url{https://github.com/THU-KEG/MetaKGR}.

\end{abstract}

\section{Introduction}

Recently, large-scale knowledge graphs (KGs) have been demonstrated to be beneficial for many NLP tasks like query answering (QA). A triple query for QA is generally formalized as , where  is the source entity and  is the query relation. 
For example, given a language query ``\textit{What is the nationality of Mark Twain?}", we can transform it into  and then search the target entity \emph{America} from KGs as the answer.
However, as many KGs are constructed automatically and face serious incompleteness problems~\cite{TransE}, it is often hard to directly get target entities for queries.

To alleviate this issue, some knowledge graph embedding methods~\cite{TransE,ConvE} have been proposed to embed entities and relations into semantic spaces to capture inner connections, and then use the learned embeddings for final predictions. Although these embedding-based methods have shown strong abilities in predicting target entities for queries, they only give answers and lack interpretability for their decisions~\cite{MultiHop}. In order to make models more intuitive, ~\citet{MINERVA} and~\citet{MultiHop} propose multi-hop reasoning methods, which leverage the symbolic compositionality of relations in KGs to achieve explainable reasoning results. For example, when queried with , multi-hop reasoning models can give not only the target entity \textit{America} but also multi-hop explainable paths  as well.

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{2pt}
\setlength{\belowcaptionskip}{0pt}
\includegraphics[width=75mm]{pic/relation_frequency.pdf}
\caption{The histogram of relation frequency in the real-world knowledge graph Wikidata.}
\label{relation_frequency}
\end{figure}

Most previous work assumes that there are enough triples to train an effective and robust reasoning models for each relation in KGs. However, as shown in Figure \ref{relation_frequency}, a large portion of KG relations are actually long-tail~\cite{one-shot,Fewrel} and only contain few triples, which can be called few-shot relations. Some pilot experiments show that the performance of previous multi-hop reasoning models, e.g., MINERVA~\cite{MINERVA} and MultiHop~\cite{MultiHop}, on these few-shot relations will drop significantly. Note that, there are some knowledge graph embedding models~\cite{one-shot, shi2018open, DKRL} that can deal with zero-shot or one-shot relations, but they still have two main drawbacks: (1) they are embedding-based models and lack interpretability; (2) they focus on zero-shot or one-shot relations, which is a little far away from real-world scenes. In fact, even for people, it is hard to grasp new knowledge with almost no examples. Therefore, few-shot multi-hop reasoning is a quite important and practical problem that has not yet been fully resolved.

In this paper, we propose a meta-based algorithm for multi-hop reasoning (Meta-KGR) to address the above problems, which is explainable and effective for few-shot relations. Specifically, in Meta-KGR, we regard triple queries with the same relation  in KGs as a task. For each task, we adopt reinforcement learning (RL) to train an agent to search target entities and reasoning paths. Similar to previous meta-learning method MAML~\cite{MAML}, we use tasks of high-frequency relations to capture meta information, which includes common features among different tasks. Then, the meta information can be rapidly adapted to the tasks of few-shot relations, by providing a good starting point to train their specific reasoning agents. In experiments, we evaluate Meta-KGR on two datasets sampled from Freebase and NELL, and the experimental results show that Meta-KGR outperforms  the current state-of-the-art multi-hop reasoning methods in few-shot scenarios.  










\section{Problem Formulation}
\label{problem}

We formally define a KG , where  and  denote the entity and relation sets respectively.  is the triple set, where  are entities and  is a relation between them. In this paper, if the number of triples mentioning a relation  is smaller than the specific threshold ,  is considered as a few-shot relation, otherwise, it is a normal relation.

Given a query , where  is the source entity and  is the few-shot relation, the goal of few-shot multi-hop reasoning is to predict the right entity  for this query. Different from the previous knowledge graph embedding work, multi-hop reasoning also gives a reasoning path from  to  over  to illustrate the whole reasoning process.

\section{Related Work}

\textbf{Knowledge Graph Embedding}~\cite{TransE,DistMult,ConvE} aims to embed entities and relations into low-dimensional spaces, and then uses embeddings to define a score function  to evaluate the probability that a triple is true. Recently, several models~\cite{DKRL,shi2018open} incorporate additional entity descriptions to learn embeddings for unseen entities, which can be seen as zero-shot scenarios. \citet{one-shot} predict new facts under a challenging setting where only one training triple for a given relation  is available, which can be seen as a one-shot scenario. Although these models are effective, they lack interpretability for their decisions.

\textbf{Multi-Hop Reasoning} over KGs aims to learn symbolic inference rules from relational paths in  and has been formulated as sequential decision problems in recent years. DeepPath \cite{DeepPath} first applies RL to search reasoning paths in KGs for a given query, which inspires much later work (e.g., MINERVA~\cite{MINERVA} and DIVA \cite{chen2018variational}). Because it is hard to train an RL model, Reinforce-Walk~\cite{shen2018reinforcewalk} proposes to solve the reward sparsity problem using off-policy learning. MultiHop~\cite{MultiHop} further extends MINERVA with reward shaping and action dropout, 
achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. \citet{yang2018hotpotqa} proposes a high quality dataset, which greatly promotes the development of this field. After that, many methods like CogQA \cite{CogQA} and DFGN \cite{DFGN} are also proposed.

\textbf{Meta-Learning} tries to solve the problem of ``fast adaptation on a new training task''. It has been proved to be very successful on few-shot task \cite{lake2015human, meta-translate}. Previous meta-learning models mainly focus on computer vision and imitation learning domains. In this paper, we propose a new model (Meta-KGR) using the meta-learning algorithm MAML \cite{MAML} for few-shot multi-hop reasoning. To the best of our knowledge, this work is the first research on few-shot learning for multi-hop reasoning.







\section{Model}

The main idea of MAML is to use a set of tasks to learn well-initialized parameters  for a new task with few training examples. When applied to few-shot multi-hop reasoning, triple queries with the same relation  are considered as a task . We use triples with normal relations to find well-initialized parameters  and train new models on triples with few-shot relations from the found initial parameters. As shown in Figure \ref{meta}, we can easily fast adapt to new models with parameters ,  or  for few-shot relation ,  or . 

The learning framework of Meta-KGR can be divided into two parts: (1) relation-specific learning; (2) meta-learning. Relation-specific learning aims to learn a REINFORCE model with parameters  for a specific relation  (task) to search target entities and reasoning paths. Meta-learning is based on relation-specific learning and is used to learn a meta-model with parameters .
We will introduce these two parts in the following sections.

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{2pt}
\setlength{\belowcaptionskip}{0pt}
\includegraphics[width=0.7\linewidth]{pic/meta.pdf}
\caption{Meta-learning.}
\label{meta}
\end{figure}


\subsection{Relation-Specific Learning}

For each query relation , we learn a relation-specific multi-hop reasoning agent to search for reasoning paths and target entities over , which is based on the on-policy RL approach proposed by~\citet{MultiHop}. 

\subsubsection{Reinforcement Learning Formulation}

The search process over  can be seen as a Markov Decision Process (MDP): We express  as a directed graph, with entities and relations as nodes and edges respectively. When given a query and its answer , we expect the agent for  can start from the source entity  and walk through several edges to reach the target entity . More specifically, the MDP is defined as follows.

\textbf{States} \quad The state of the -th time step is defined as a tuple , where  is the query relation,  is the source entity and  is the current entity over . 

\textbf{Actions} \quad The action space  for a given state  includes all outgoing edges and next entities of . Formally,  . We add a self-loop edge to every , which is similar to a ``STOP" action.


\textbf{Transition} \quad For the state , if the agent selects an action , the state will be changed to . The transition function is defined as . 
In this paper, we unroll the search over  up to a fixed number of time steps , and finally achieve the state . 

\textbf{Rewards} \quad The reward  will be  if the agent finally stops at the right entity, i.e., , otherwise it will get an embedding-based reward , where  is a score function from knowledge graph embedding methods to measure the probability over .

\begin{algorithm}[t] 
\small
\caption{Meta-Learning for multi-hop reasoning over knowledge graphs} 
\label{algorithm-kgr}
\begin{algorithmic}[1] 
\Require 
: distribution over relations
\Require 
: learning rate hyperparameters
\State Randomly initialize  for meta policy network
\While{not stop training}
\State Sample batch of relations 
\ForAll{relation }
\State Sample supporting set  and query set  for task 
\State Evaluate  in Eq.~(\ref{loss_func})
\State Compute adapted parameters: 
\EndFor
\State Update 
\EndWhile
\end{algorithmic} 
\end{algorithm}

\begin{table*}[t]
\small
\centering
\setlength{\belowcaptionskip}{-1pt}
    \begin{tabular}{c|ccc|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{FB15K-237}} & \multicolumn{3}{c}{\textbf{NELL-995}} \\
    & MRR & Hits@1 & Hits@10 & MRR & Hits@1 & Hits@10  \\
    \midrule
    NeuralLP  & 10.2  & 7.0  & 14.8   & 17.9  & 4.8 & \textbf{35.1}  \\
    NTP-  & 21.0  & 17.4 & 30.8   & 15.5  & 10.2 & 33.4  \\
    MINERVA  & 30.5  & 28.4  & 34.1   & 20.1  & 16.2 & 28.3  \\
    MultiHop(DistMult)  & 38.1  & 38.1  & 50.3   & 20.0  & 14.5 & 30.6  \\
    MultiHop(ConvE)  & 42.7  & 36.7  & 53.3   & 23.1  & 17.8 & 32.9  \\
    Meta-KGR(DistMult)  & 45.8  & 40.3  & 58.0   & 24.8  & \textbf{19.7} & 34.5  \\
    Meta-KGR(ConvE)  & \textbf{46.9}  & \textbf{41.2} & \textbf{58.8}   & \textbf{25.3}  & \textbf{19.7} & 34.7  \\
    \bottomrule
    \end{tabular}
    \caption{\label{table} Experimental results for link prediction. The MRR, Hits@1 and Hits@10 metrics are multiplied by 100.}
\end{table*}


\subsubsection{Policy Network}

To solve the above MDP problem, we need a model which has policy to choose actions at each state. Specifically, different from normal RL problems, we apply a policy network considering the search history over . Formally, after embedding all entities and relations in  as  and  respectively, each action  is represented as . We use an LSTM to encode the search path,


Then, we represent the action space by stacking all actions in  as . The parameterized policy network is defined as,

where  is the probability distribution over all actions in .

\subsubsection{Loss Function}

Given a query relation  and a batch of triples set  with relation , the overall loss for this relation-specific policy network is defined as:







\subsection{Meta-Learning}





In meta-learning, we aim to learn well-initialized parameters , such that small changes in the parameters will produce significant improvements on the loss function of any task \cite{MAML}.

Formally, we consider a meta policy network with parameters . When adapting to a new task , the parameters of the model become . Following MAML, the updated parameter  is computed using one or more gradient descent updates on task . For example, assuming a single gradient step is taken with the learning rate 

where  is a supporting set randomly sampled from the triples belonging to . After the relation-specific parameters  is learned, we evaluate  on the query set  belonging to , which is sampled like . The gradient computed from this evaluation can be used to update the meta policy network with parameters . Usually, we will go over many tasks in a batch and update  as follows:

where  is the meta-learning rate. We detail the meta-learning algorithm in Algorithm~\ref{algorithm-kgr}.

After previous meta-learning steps, Meta-KGR can fast adapt to a relation-specific policy network for every few-shot relation by using  as well-initialized parameters .

\begin{table}[t]
\small
\centering
\setlength{\belowcaptionskip}{-1pt}
    \begin{tabular}{lrrr}
    \toprule
    \textbf{Dataset} & \textbf{\#Ent} & \textbf{\#Rel} & \textbf{\#Triples} \\
    \midrule
    FB15k-237 (normal) & 14,448 & 200 & 268,039 \\
    FB15k-237 (few-shot) & 3,078 & 37 & 4,076 \\
    NELL-995 (normal) & 63,524 & 170 & 115,454 \\
    NELL-995 (few-shot) & 2,951 & 30 & 2,680 \\
    \bottomrule
    \end{tabular} 
    \caption{\label{table2} Statistics of datasets.}
    \vspace{-0.8em}
\end{table}

\section{Experiments}

\subsection{Datasets}

We use two typical datasets FB15K-237 \cite{FB15K-237} and NELL-995 \cite{DeepPath} for training and evaluation. Specifically, we set  and  to select few-shot relations from FB15K-237 and NELL-995 respectively. Besides, we rebuild NELL-995 to generate few-shot relations in valid and test set. Statistics are given separately for normal relations and few-shot relations in Table \ref{table2}.

\subsection{Baselines}

We compare with four multi-hop reasoning models in experiments: (1) Neural Logical Programming (NerualLP) \cite{neuralLP}; (2) NTP- \cite{NTP}; (3) MINERVA \cite{MINERVA} and (4) MultiHop \cite{MultiHop}. For MultiHop and our model, we use both DistMult \cite{DistMult} and ConvE \cite{ConvE} as the reward function to create two different model variations, which are labeled in parentheses in Table \ref{table}.

\subsection{Link Prediction}

Given a query , link prediction for QA aims to give a ranking list of candidate entities from the KG. Following previous work \cite{ConvE, MultiHop}, we use two evaluation metrics in this task: (1) the mean reciprocal rank of all correct entities (MRR) and (2) the proportion of correct entities that rank no larger than N (Hits@N). 

Evaluation results on two datasets are shown in Table~\ref{table}. From the table, we can conclude that: (1) Our models outperform previous work in most cases, which means meta parameters learned from high-frequency relations can adapt to few-shot relations well. (2) ConvE is better than DistMult when used as the reward function in our models. This indicates that more effective knowledge graph embedding methods may provide fine-grained rewards for training multi-hop reasoning models. (3) Compared with NELL-995, FB15K-237 is denser. Our models perform well on both datasets, which demonstrates Meta-KGR can accommodate different types of KGs.

\begin{table}[t]
\small
\centering
\setlength{\belowcaptionskip}{-1pt}
    \begin{tabular}{l|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Threshold}} & \multicolumn{2}{c|}{\textbf{MultiHop}} & \multicolumn{2}{c}{\textbf{Meta-KGR}} \\
    & MRR & Hits@1  & MRR & Hits@1  \\
    \midrule
      & 20.8  & 16.9  & \textbf{22.3}   & \textbf{19.3}   \\
      & 25.7  & 20.8  & \textbf{29.6}   & \textbf{26.6}   \\
      & 29.1  & 25.0  & \textbf{31.3}   & \textbf{27.2}  \\
      & 42.7  & 36.7  & \textbf{46.9}   & \textbf{41.2}    \\
    \bottomrule
    \end{tabular}
    \caption{\label{table3} Experimental results for robustness analysis.}
\end{table}

\subsection{Robustness Analysis}

We can use different frequency thresholds  to select few-shot relations. In this section, we will study the impact of  on the performance of our model. In our experiments, some triples will be removed until every few-shot relation has only  triples. We do link prediction experiments on FB15K-237 and use ConvE as our reward function. The final results are shown in Table \ref{table3}.  means we use the whole datasets in Table \ref{table2} and do not remove any triples. From Table \ref{table3} we can see our model is robust to  and outperforms MultiHop in every case.

\section{Conclusion}

In this paper, we propose a meta-learning based model named Meta-KGR for multi-hop reasoning over few-shot relations of knowledge graphs. Meta-KGR uses training triples with high-frequency relations to find well-initialized parameters and fast adapt to few-shot relations. The meta information learned from high-frequency relations is helpful for few-shot relations. In experiments, our models achieve good performance on few-shot relations and outperform previous work in most cases. Some empirical analysis also demonstrates that our models are robust and generalized to different types of knowledge graphs.


\section*{Acknowledgments}
  
The work is supported by NSFC key projects (U1736204, 61533018, 61661146007), Ministry of Education and China Mobile Joint Fund (MCM20170301), and THUNUS NExT Co-Lab.

\bibliography{emnlp-ijcnlp-2019}
\bibliographystyle{acl_natbib}

\end{document}
