\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}  
\usepackage{algorithmicx}  
\usepackage{algpseudocode}
\usepackage{amsmath} 
\usepackage{algorithm}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{float}

\usepackage{etoolbox}
\makeatletter
\AfterEndEnvironment{algorithm}{\let\@algcomment\relax}
\AtEndEnvironment{algorithm}{\kern2pt\hrule\relax\vskip3pt\@algcomment}
\let\@algcomment\relax
\newcommand\algcomment[1]{\def\@algcomment{\footnotesize#1}}
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
  \def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}\def\@fs@post{}\def\@fs@mid{\kern2pt\hrule\kern2pt}\let\@fs@iftopcapt\iftrue}
\makeatother




\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{1647} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{TransFG: A Transformer Architecture for Fine-grained Recognition}


\author{
	Ju He
	\;\; Jie-Neng Chen
	\;\; Shuai Liu
	\;\; Adam Kortylewski
	\;\; Cheng Yang
	\;\; Yutong Bai\\
	\;\; Changhu Wang
	\;\; Alan Yuille\\
	Johns Hopkins University \;\; ByteDance Inc.\\	
}

\maketitle

\begin{abstract}
    Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Recent works mainly tackle this problem by focusing on how to locate the most discriminative image regions and rely on them to improve the capability of networks to capture subtle variances. Most of these works achieve this by re-using the backbone network to extract features of selected regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The self-attention mechanism of the transformer links every patch token to the classification token. The strength of the attention link can be intuitively considered as an indicator of the importance of tokens. In this work, we propose a novel transformer-based framework TransFG where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to further enlarge the distance between feature representations of similar sub-classes. We demonstrate the value of TransFG by conducting experiments on five popular fine-grained benchmarks: CUB-200-2011, Stanford Cars, Stanford Dogs, NABirds and iNat2017 where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model. Code is available at \href{https://github.com/TACJu/TransFG}{this https URL}.
\end{abstract} \section{Introduction}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth,height=6cm]{intro.jpg}
    \caption{An overview of performance comparison of ViT and TransFG with state-of-the-art methods with CNN backbones on five datasets.}
    \label{fig:intro}
    \vspace{-4mm}
\end{figure}

Fine-grained visual classification aims at classifying sub-classes of a given object category, \eg, subcategories of birds \cite{WahCUB_200_2011, van2015building}, cars \cite{KrauseStarkDengFei-Fei_3DRR2013}, aircrafts \cite{maji13fine-grained}. It has long been considered as a very challenging task due to the small inter-class variations and large intra-class variations along with the deficiency of annotated data, especially for the long-tailed classes. Benefiting from the progress of deep neural networks \cite{NIPS2012_c399862d, simonyan2015deep, he2015deep}, the performance of FGVC obtain a steady progress in recent years. To avoid labor-intensive parts annotation, the community currently focuses on weakly-supervised FGVC with only image-level labels. Methods now can be roughly classified into two categories, i.e., localization methods and feature-encoding methods. Compared to feature-encoding methods, the localization methods have the advantages that they explicitly capture the subtle differences among different sub-classes which is more interpretable and usually yields better results.

Early works in localization methods rely on the annotations of parts to locate discriminative regions while recent works \cite{ge2019weakly, liu2020filtration, ding2019selective, zheng2019looking, yang2018learning} mainly adopt region proposal networks (RPN) to propose bounding boxes which contain the discriminative regions. After obtaining the selected image regions, they are resized into predefined size and forwarded through the backbone network again to acquire informative local features. A typical strategy is to use these local features for classification individually and adopt a rank loss \cite{chen2009ranking} to maintain the consistency between the quality of bounding boxes and their final probability output. However, this mechanism ignores the relation between selected regions thus inevitably encourages the RPN to propose large bounding boxes that contain most parts of the objects in order to obtain correct classification results. Sometimes these bounding boxes can even contain large areas of backgrounds and lead to confusion. Besides, the RPN module with different optimizing goals compared to the backbone network makes the network harder to train and the re-use of backbone complicates the overall algorithm pipeline.

Recently, vision transformer \cite{dosovitskiy2020image} achieved huge success in traditional classification task which shows that applying a pure transformer directly to a sequence of image patches with its innate attention mechanism can capture the important regions in images thus facilitate the classification. A series of extended work on downstream tasks such as object detection \cite{carion2020end}, semantic segmentation \cite{zheng2020rethinking, xie2021trans2seg, chen2021transunet} confirmed the strong ability for vision transformer to capture both global and local features.

In this paper, we present the first study which explores the potential of vision transformers in the context of fine-grained visual classification. We find that directly applying ViT on FGVC already produces satisfactory results while a lot of adaptations according to the characteristics of FGVC can be applied to further boost the performance. To this end, we propose TransFG, a simple yet effective framework based on ViT. To be specific, by leveraging the innate multi-head self-attention mechanism, we propose a part selection module to compute the discriminative regions and remove redundant information. We then concatenate the selected part tokens along with the global classification token as input sequence to the last transformer layer. To further enlarge the distance between feature representations of samples from different categories and decrease that of samples from the same categories and decrease that of samples from same categories, we introduce a contrastive loss that further boosts performance.

We evaluate our model extensively on five popular fine-grained visual classification benchmarks (CUB-200-2011, Stanford Cars, Stanford Dogs, NABirds, iNat2017). An overview of the performance comparison can be seen in Fig \ref{fig:intro} where our TransFG outperforms existing SOTA CNN methods with different backbones on most datasets. In summary, we make several important contributions in this work:

\begin{enumerate}
    \item To the best of our knowledge, we are the first to verify the effectiveness of vision transformer on fine-grained visual classification which offers an alternative to the dominating CNN backbone with RPN model design.
    \item We introduce TransFG, a novel neural architecture for fine-grained visual classification that naturally focuses on the most discriminative regions of the objects and achieve SOTA performance on several standard benchmarks.
    \item Visualization results are presented which illustrate the ability of our TransFG to accurately capture discriminative image regions and help us to better understand how it makes correct predictions.
\end{enumerate} \begin{figure*}
    \centering
    \includegraphics[width=\linewidth,height=10cm]{pipeline.pdf}
    \caption{The framework of our proposed TransFG. Images are split into small patches (a non-overlapping split is shown here) and projected into the embedding space. The input to the Transformer Encoder consists of patch embeddings along with learnable position embeddings. Before the last Transformer Layer, a Part Selection Module (PSM) is applied to select tokens that corresponds to the discriminative image patches and only use these selected tokens as input. Cross-entropy loss and contrastive loss on the final classification token contribute to the training of TransFG. Best viewed in color.}
    \label{fig:model}
\end{figure*}


\section{Related Work}

In this section, we briefly review existing works on fine-grained visual classification and transformer.

\subsection{Fine-Grained Visual Classification}
Many works have been done to tackle the problem of fine-grained visual classification and they can roughly be classified into two categories: localization methods \cite{ge2019weakly, liu2020filtration, ding2019selective, zheng2019looking, yang2018learning, yang2021rerank} and feature-encoding methods \cite{yu2018hierarchical, zheng2019learning, gao2020channel, zhuang2020learning, luo2019cross}. The former focuses on training a detection network to localize discriminative part regions and reuse them to perform classification. The latter targets at learning more informative features by either computing higher-order information or finding the relationships among contrastive pairs.

\subsubsection{Localization FGVC methods}
Previously, some works \cite{branson2014bird, wei2016maskcnn} tried to exploit the part annotations to supervise the learning procedure of the localization process. However, since such annotations are expensive and usually unavailable, weakly-supervised parts proposal with only image-level labels draw more attentions nowadays. He et al. \cite{article} proposed a sophisticated reinforcement learning procedure to estimate how to select the discriminative image regions and the number of them. Ge et al. \cite{ge2019weakly} exploited Mask R-CNN and CRF-based segmentation alternatively to extract object instances and discriminative regions. Yang \cite{yang2021rerank} proposed a re-ranking strategy to re-rank the global classification results based on the database constructed with region features. However, these methods all need a special designed module to propose potential regions and these selected regions need to be forwarded through the backbone again for final classification. Besides, some of the proposed regions often contain the whole object which are not discriminative enough.

\subsubsection{Feature-encoding methods}
The other branch of methods focus on enriching the feature representations to obtain better classification results. Yu et al. \cite{yu2018hierarchical} proposed a hierarchical framework to do cross-layer bilinear pooling. Zheng et al. \cite{zheng2019learning} adopted the idea of group convolution to first split channels into different groups by their semantic meanings and then do the bilinear pooling within each group without changing the dimension thus it can be integrated into any existed backbones directly. Zhuang et al. \cite{zhuang2020learning} proposed to construct contrastive input batches and compute the cues between them to force the features contain such discriminative information. However, these methods are usually not interpretable such one does not know what makes the model distinguish sub-categories with subtle differences.

\subsection{Transformer} 
Transformer and self-attention models have greatly facilitated research in natural language processing and machine translation \cite{dai2019transformer, devlin2018bert, vaswani2017attention, yang2019xlnet}. Inspired by this, many recent studies try to apply transformers in computer vision area. Initially, transformer is used to handle sequential features extracted by CNN backbone for the videos. Girdhar et al. \cite{girdhar2019video} exploited a variant of transformer to aggregate contextual cues related to a specific person in a video. Later, transformer models are further extended to other popular computer vision tasks such as object detection \cite{carion2020end}, segmentation \cite{xie2021trans2seg, chen2021transunet, yun2021spectr}, object tracking \cite{sun2020transtrack}.
Most recently, pure transformer models are becoming more and more popular. ViT \cite{dosovitskiy2020image} is the first work to show that applying a pure transformer directly to a sequence of image patches can yield state-of-the-art performance on image classification. Based on that, Zheng et al. \cite{zheng2020rethinking} proposed SETR to exploit ViT as the encoder for segmentation. He et al. \cite{he2021transreid} proposed TransReID which embedded side information into transformer along with the JPM to boost the performance on object re-identification. In this work, we extend ViT to fine-grained visual classification and show its effectiveness.  \section{Method}

We briefly review the framework of vision transformer and show how to do some preprocessing steps to extend it into fine-grained recognition in Section \ref{sec:ViT}. Then, the overall framework of TransFG will be elaborated in Section \ref{sec:TransFG}

\subsection{Vision transformer as feature extractor}
\label{sec:ViT}

\textbf{Image Sequentialization.} Following ViT, we first preprocess the input image into a sequence of flattened patches . However, the original split method cut the images into non-overlapping patches, which harms the local neighboring structures especially when discriminative regions are split. To alleviate this problem, we propose to generate overlapping patches with sliding window. To be specific, we denote the input image with resolution , the size of image patch as  and the step size of sliding window as . Thus the input images will be split into N patches where

In this way, two adjacent patches share an overlapping area of size  which helps to preserve better local region information. Typically speaking, the smaller the step  is, the better the performance will be. But decreasing S will at the same time enlarge the computational cost, so a trade-off needs to be made here.

\textbf{Patch Embedding.} We map the vectorized patches  into a latent D-dimensional embedding space using a trainable linear projection. A learnable position embedding is added to the patch embeddings to retain positional information as follows:

where  is the number of image patches,  is the patch embedding projection, and  denotes the position embedding.

The Transformer encoder \cite{vaswani2017attention} contains  layers of multihead self-attention (MSA) and multi-layer perceptron (MLP) blocks. Thus the output of the -th layer can be written as follows:

where  denotes the layer normalization operation and  is the encoded image representation. ViT exploits the first token of the last encoder layer  as the representation of the global feature and forward it to a classifier head to obtain the final classification results without considering the potential information stored in the rest tokens.

\subsection{TransFG Architecture}
\label{sec:TransFG}

While our experiments in Section \ref{sec:exp} show that the pure Vision Transformer can be directly applied into fine-grained visual classification and achieve impressive results. It does not well capture the local information required for FGVC. To this end, we propose the Part Selection Module (PSM) and apply contrastive feature learning to enlarge the distance of representations between similar sub-categories. The framework of our proposed TransFG is illustrated in Fig \ref{fig:model}.

\subsubsection{Part Selection Module}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth,height=6cm]{dataset.jpg}
    \caption{A confusing pair of instances from the CUB-200-2011 dataset. Model needs to has the ability to capture the subtle differences in order to classify them correctly. The second column shows the overall attention maps and two selected tokens of our TransFG method. Best viewed in color.}
    \label{fig:dataset}
\end{figure}

One of the most important problems in fine-grained visual classification is to accurately locate the discriminative regions that account for subtle differences between similar sub-categories. Take a confusing pair of images from the CUB-200-2011 dataset as shown in Fig \ref{fig:dataset} for example. The model needs to have the ability to capture the very small differences, i.e., the color of eyes and throat in order to distinguish these two bird species. Region proposal networks and weakly-supervised segmentation strategies are widely introduced to tackle this problem in the traditional CNN-based methods.

Vision Transformer model is perfectly suited here with its innate multi-head attention mechanism. To fully exploit the attention information, we change the input to the last Transformer Layer. Suppose the model has K self-attention heads and the hidden features input to the last layer are denoted as . The attention weights of the previous layers can be written as follows:

Previous works \cite{serrano2019attention, abnar2020quantifying} suggested that the raw attention weights do not necessarily correspond to the relative importance of input tokens especially for higher layers of a model, due to lack of token identifiability of the embeddings. To this end, we propose to integrate attention weights of all previous layers. To be specific, we recursively apply a matrix multiplication to the raw attention weights in all the layers as

As  captures how information propagates from the input layer to the embeddings in higher layers, it serves as a better choice for selecting discriminative regions compared to the single layer raw attention weights . We then choose the index of the maximum value  with respect to the K different attention heads in . These positions are used as index for our model to extract the corresponding tokens in . Finally, we concatenate the selected tokens along with the classification token as the input sequence which is denoted as:

By replacing the original entire input sequence with tokens corresponding to informative regions and concatenate the classification token as input to the last Transformer Layer, we not only keep the global information but also force the last Transformer Layer to focus on the subtle differences between different sub-categories while abandoning less discriminative regions such as background or common features among a super class.















\subsubsection{Contrastive feature learning}
Following ViT, we still adopt the first token  of the PSM module for classification. A simple cross-entropy loss is not enough to fully supervise the learning of features since the differences between sub-categories might be very small. To this end, we adopt contrastive loss  which minimizes the similarity of classification tokens corresponding to different labels and maximizes the similarity of classification tokens of samples with the same label . To prevent the loss being dominated by easy negatives (different class samples with little similarity), a constant margin  is introduced that only negative pairs with similarity larger than  contribute to the loss . Formally, the contrastive loss over a batch of size  is denoted as:

where  and  are pre-processed with  normalization and  is the cosine similarity of  and .

In summary, our model is trained with the sum of cross-entropy loss  and contrastive  together which can be expressed as:

where  is the cross-entropy loss between the predicted label  and the ground-truth label .
 \section{Experiments}
\label{sec:exp}

In this section, we first introduce the detailed setup including datasets and training hyper-parameters in Section \ref{sec:setup}. Quantitative analysis is given in Section \ref{sec:quan} follow by ablation studies in Section \ref{sec:ablation}. We further give qualitative analysis and visualization results in Section \ref{sec:qual}.

\subsection{Experiments Setup}
\label{sec:setup}

\textbf{Datasets.} We evaluate our proposed TransFG on five widely used fine-grained benchmarks, i.e., CUB-200-2011 \cite{WahCUB_200_2011}, Stanford Cars \cite{KrauseStarkDengFei-Fei_3DRR2013}, Stanford Dogs \cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}, NABirds \cite{van2015building} and iNat2017 \cite{vanhorn2018inaturalist}. The detailed statistics such as category numbers and data splits are summarized below

\begin{table}[!h]
    \small
    \centering
    \caption{Statistics of fine-grained datasets used in this paper.}
    \label{tab:stat}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Datasets & Category & Training & Testing \\ \hline
    CUB-Birds & 200 & 5994 & 5794 \\ \hline
    Stanford Dogs & 120 & 12000 & 8580 \\ \hline
    Stanford Cars & 196 & 8144 & 8041 \\ \hline
    NABirds & 555 & 23929 & 24633 \\ \hline
    iNat2017 & 5089 & 579184 & 95986 \\ \hline
    \end{tabular}
\end{table}

\textbf{Implementation details.}
Unless stated otherwise, we implement TransFG as follows. First, we resize input images to  except  on iNat2017 for fair comparison (random cropping for training and center cropping for testing). We split image to patches of size 16 and the step size of sliding window is set to be 12. Thus the  in Eq \ref{equ:split} is 448, 448, 16, 12 respectively. The margin  in Eq \ref{equ:con} is set to be 0.4. Random horizontal flipping and AutoAugment \cite{cubuk2019autoaugment} are adopted for data augmentation. We load intermediate weights from official ViT-B\_16 model pretrained on ImageNet21k. The batch size is set to 16. SGD optimizer is employed with a momentum of 0.9. The learning rate is initialized as 0.03 except 0.003 for Stanford Dogs dataset and 0.01 for iNat2017 dataset. We adopt cosine annealing as the scheduler of optimizer.

All the experiments are performed with four Nvidia Tesla V100 GPUs using the PyTorch toolbox and APEX with FP16 training.

\subsection{Quantitative Analysis}
\label{sec:quan}

We compare our proposed method TransFG with state-of-the-art works on above mentioned fine-grained datasets. The experiment results on CUB-200-2011 and Stanford Cars are shown in Table \ref{tab:cub}. From the results, we find that our method outperforms all previous methods on CUB dataset and achieve competitive performance on Stanford Cars.

\begin{table}[]
    \small
    \centering
    \caption{Comparison of different methods on CUB-200-2011, Stanford Cars.}
    \label{tab:cub}
    \begin{tabular}{c|c|c|c}
    \hline
    Method & Backbone & CUB & Cars \\ \hline
    ResNet-50 \cite{he2015deep} & ResNet-50 & 84.5 & - \\
    RA-CNN \cite{fu2017look} & VGG-19 & 85.3 & 92.5 \\
    GP-256 \cite{wei2018grassmann} & VGG-16 & 85.8 & 92.8 \\
    MaxEnt \cite{NEURIPS2018_0c74b7f7} & DenseNet-161 & 86.6 & 93.0 \\
    DFL-CNN \cite{wang2018learning} & ResNet-50 & 87.4 & 93.1 \\
    NTS-Net \cite{yang2018learning} & ResNet-50 & 87.5 & 93.9 \\ 
    Cross-X \cite{luo2019cross} & ResNet-50 & 87.7 & 94.6 \\
    DCL \cite{chen2019destruction} & ResNet-50 & 87.8 & 94.5 \\
    CIN \cite{gao2020channel} & ResNet-101 & 88.1 & 94.5 \\
    DBTNet \cite{zheng2019learning} & ResNet-101 & 88.1 & 94.5 \\
    ACNet \cite{ji2020attention} & ResNet-50 & 88.1 & 94.6 \\
    S3N \cite{ding2019selective} & ResNet-50 & 88.5 & 94.7 \\
    FDL \cite{liu2020filtration} & DenseNet-161 & 89.1 & 94.2 \\
    PMG \cite{du2020fine} & ResNet-50 & 89.6 & 95.1 \\ 
    API-Net \cite{zhuang2020learning} & DenseNet-161 & 90.0 & \textbf{95.3} \\
    StackedLSTM \cite{Ge_2019_CVPR} & GoogleNet & 90.4 & - \\ \hline
    DeiT-B \cite{touvron2021training} & DeiT-B & - & 93.9 \\
    ViT \cite{dosovitskiy2020image} & ViT-B\_16 & 90.3 & 93.7 \\ 
    TransFG & ViT-B\_16 & \textbf{91.7} & 94.8 \\ \hline
    \end{tabular}
\end{table}

To be specific, the third column of Table \ref{tab:cub} shows the comparison results on CUB-200-2011. Compared to the best result StackedLSTM \cite{Ge_2019_CVPR} up to now, our TransFG achieves a \textbf{1.3\%} improvement on Top-1 Accuracy metric and 1.4\% improvement compared to our base framework ViT \cite{dosovitskiy2020image}. NTS-Net \cite{yang2018learning} exploits ranking loss to maintain a consistency for learning region features which ignored the global relationship. Multiple ResNet-50 are adopted as multiple branches in \cite{ding2019selective} which greatly increases the complexity. It is also worth noting that StackLSTM is a very messy multi-stage training model which hampers the availability in practical use, while our TransFG maintains the simplicity of both framework and training strategy.

The fourth column of Table \ref{tab:cub} shows the results on Stanford Cars. Our method outperforms most existing methods while performs worse than PMG \cite{du2020fine} and API-Net \cite{zhuang2020learning} with small margin. We argue that the reason might be the images of Stanford Cars dataset have much simpler and clearer backgrounds than others thus it requires less work on locating discriminative regions for classifying sub-categories. We can observe that most recent methods achieve quite similar results on this dataset. However, even with this property, our TransFG consistently gets \textbf{1.1\%} improvement compared to the standard ViT model.

\begin{table}[]
    \small
    \centering
    \caption{Comparison of different methods on Stanford Dogs.}
    \label{tab:dog}
    \begin{tabular}{c|c|c}
    \hline
    Method & Backbone & Dogs \\ \hline
    MaxEnt \cite{NEURIPS2018_0c74b7f7} & DenseNet-161 & 83.6 \\ 
    FDL \cite{liu2020filtration} & DenseNet-161 & 84.9 \\
    RA-CNN \cite{fu2017look} & VGG-19 & 87.3 \\
    SEF \cite{luo2020learning} & ResNet-50 & 88.8 \\
    Cross-X \cite{luo2019cross} & ResNet-50 & 88.9 \\
    API-Net \cite{zhuang2020learning} & ResNet-101 & 90.3 \\ \hline
    ViT \cite{dosovitskiy2020image} & ViT-B\_16 & 91.7 \\ 
    TransFG & ViT-B\_16 & \textbf{92.3} \\ \hline
    \end{tabular}
\end{table}

The results of experiments on Stanford Dogs are shown in Table \ref{tab:dog}. Stanford Dogs is a more challenging dataset compared to Stanford Cars with its the more subtle differences between certain species and the large variances of samples from the same category. Only a few methods have tested on this dataset and our TransFG outperforms all of them. API-Net \cite{zhuang2020learning} learns to capture the subtle differences by elaborately constructing batches of data and learning the mutual feature vectors and residuals of them. While ViT \cite{dosovitskiy2020image} outperforms other methods by a large margin, our TransFG achieves 92.3\% accuracy which outperforms SOTA by \textbf{2.0\%} with its discriminative part selection and contrastive loss supervision.

\begin{table}[]
    \small
    \centering
    \caption{Comparison of different methods on NABirds.}
    \label{tab:na}
    \begin{tabular}{c|c|c}
    \hline
    Method & Backbone & NABirds \\ \hline
    MaxEnt \cite{NEURIPS2018_0c74b7f7} & DenseNet-161 & 83.0 \\ 
    Cross-X \cite{luo2019cross} & ResNet-50 & 86.4 \\ 
    API-Net \cite{zhuang2020learning} & DenseNet-161 & 88.1 \\ 
    CS-Parts \cite{korsch2019classification} & ResNet-50 & 88.5 \\ 
    MGE-CNN \cite{zhang2019learning} & ResNet-50 & 88.6 \\ 
    FixSENet-154 \cite{touvron2019fixing} & SENet-154 & 89.2 \\ \hline
    ViT \cite{dosovitskiy2020image} & ViT-B\_16 & 89.9 \\
    TransFG & ViT-B\_16 & \textbf{90.8} \\ \hline
    \end{tabular}
\end{table}

NABirds is a much larger birds dataset not only from the side of images numbers but also with 355 more categories which significantly makes the fine-grained visual classification task more challenging. We show our results on it in Table \ref{tab:na}. 
We observe that most methods achieve good results by either exploiting multiple backbones for different branches or adopting quite deep CNN structures to extract better features. 
While the pure ViT \cite{dosovitskiy2020image} can directly achieve 89.9\% accuracy, our TransFG constantly gets 0.9\% performance gain compared to ViT and reaches 90.8\% accuracy which outperforms SOTA by \textbf{1.6\%}.

\begin{table}[]
    \small
    \centering
    \caption{Comparison of different methods on iNat2017.}
    \label{tab:inat}
    \begin{tabular}{c|c|c}
    \hline
    Method & Backbone & iNat2017 \\ \hline
    ResNet152 \cite{he2015deep} & ResNet152 & 59.0 \\
    SSN \cite{recasens2018learning} & ResNet101 & 65.2 \\
    Huang et al. \cite{huang2020interpretable} & ResNet101 & 66.8 \\
    IncResNetV2 \cite{szegedy2017inception} & IncResNetV2 & 67.3 \\ 
    TASN \cite{zheng2019looking} & ResNet101 & 68.2 \\ \hline
    ViT \cite{dosovitskiy2020image} & ViT-B\_16 & 68.7 \\ 
    TransFG & ViT-B\_16 & \textbf{71.7} \\ \hline
    \end{tabular}
\end{table}

iNat2017 is a large-scale dataset for fine-grained species recognition. Most previous methods do not report results on iNat2017 because of the computational complexity of the multi-crop, multi-scale and multi-stage optimization. With the simplicity of our model pipeline, we are able to scale TransFG well to big datasets and evaluate the performance which is shown in Table \ref{tab:inat}. This dataset is very challenging for mining meaningful object parts and the background is very complicated as well. We find that Vision Transformer structure outperforms ResNet structure a lot in these large challenging datasets. ViT outperformes ResNet152 by nearly 10\% and similar phenomenon can also be observed in iNat2018 and iNat2019. Our TransFG is the only method to achieve above 70\% accuracy with input size of 304 and outperforms SOTA with a large margin of \textbf{3.5\%}.

\subsection{Ablation Study}
\label{sec:ablation}

We conduct ablation studies on our TransFG pipeline to analyze how its variants affect the fine-grained visual classification result. All ablation studies are done on CUB-200-2011 dataset while the same phenomenon can be observed on other datasets as well. We evaluate the influence of the following designs: overlap patches, part selection module, contrastive loss and the results are analyzed in details below.

\begin{table}[]
    \small
    \centering
    \caption{Ablation study on split way of image patches on CUB-200-2011 dataset.}
    \label{tab:absplit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Method & Patch Split & Accuracy (\%) & Training Time (h)\\ \hline
    ViT & Non-Overlap & 90.3 & 1.30 \\ 
    ViT & Overlap & \textbf{90.5} & 3.38  \\ \hline
    TransFG & Non-Overlap & 91.5 & 1.98 \\
    TransFG & Overlap & \textbf{91.7} & 5.38 \\ \hline
    \end{tabular}
\end{table}

\textbf{Influence of image patch split method.} We investigate the influence of our overlapping patch split method through experiments with standard non-overlapping patch split. As shown in Table \ref{tab:absplit}, both on the pure Vision Transformer and our improved TransFG framework, the overlapping split method bring consistently improvement, i.e., 0.2\% for both frameworks. The additional computational cost introduced by this is also affordable as shown in the fourth column. 

\begin{table}[]
    \small
    \centering
    \caption{Ablation study on Part Selection Module (PSM) on CUB-200-2011 dataset.}
    \label{tab:abpsm}
    \begin{tabular}{|c|c|}
    \hline
    Method & Accuracy (\%) \\ \hline
    ViT & 90.3 \\ 
    TransFG & \textbf{91.0} \\ \hline
    \end{tabular}
\end{table}

\textbf{Influence of Part Selection Module.} As shown in Table \ref{tab:abpsm}, by applying the Part Selection Module (PSM) to select discriminative part tokens as the input for the last Transformer layer, the performance of the model improves from 90.3\% to 91.0\%. We argue that this is because in this way, we sample the most discriminative tokens as input which explicitly throws away some useless tokens and force the network to learn from the important parts.

\begin{table}[]
    \small
    \centering
    \caption{Ablation study on contrastive loss on CUB-200-2011 dataset.}
    \label{tab:abdup}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Method & Contrastive Loss & Acc (\%) \\ \hline
    ViT & & 90.3 \\
    ViT & \checkmark & \textbf{90.7} \\ \hline
    TransFG & & 91.0 \\ 
    TransFG & \checkmark & \textbf{91.5} \\ \hline
    \end{tabular}
\end{table}

\textbf{Influence of contrastive loss.} The comparisons of the performance with and without contrastive loss for both ViT and TransFG frameworks are shown in Table \ref{tab:abdup} to verify the effectiveness of it. We observe that with contrastive loss, the model obtains a big performance gain. Quantitatively, it increases the accuracy from 90.3\% to 90.7\% for ViT and 91.0\% to 91.5\% for TransFG. We argue that this is because contrastive loss can effectively enlarge the distance of representations between similar sub-categories and decrease that between the same categories which can be clearly seen in the comparison of confusion matrix in Fig \ref{fig:confusion}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth,height=4cm]{contrast.jpg}
    \caption{Illustration of contrastive loss. Confusion matrices without and with contrastive loss of a batch with four classes where each contains four samples are shown. The metric of confusion matrix is cosine similarity. Best viewed in color.}
    \label{fig:confusion}
\end{figure}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_02.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_12.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_22.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_02.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_12.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_22.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_01.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_11.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_21.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_01.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_11.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_21.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_02.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_12.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_22.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_02.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_12.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_22.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_01.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_11.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_21.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_01.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_11.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_21.jpg}
    \end{subfigure}
    \caption{Visualization results of TransFG on CUB-200-2011, Stanford Dogs, Stanford Cars and NABirds datasets. Two kinds of visualization are given, where the first and the third row show the selected Top-4 token positions while the second and fourth rows show the overall global attention maps. Best viewed in color.}
    \label{fig:vis}
\end{figure*}

\begin{table}[]
    \small
    \centering
    \caption{Ablation study on value of margin  on CUB-200-2011 dataset.}
    \label{tab:abalpha}
    \begin{tabular}{|c|c|c|}
    \hline
    Method & Value of  & Accuracy (\%) \\ \hline
    TransFG & 0 & 91.1 \\ 
    TransFG & 0.2 & 91.4 \\ 
    TransFG & 0.4 & \textbf{91.7} \\ 
    TransFG & 0.6 & 91.5 \\ \hline
    \end{tabular}
\end{table}

\textbf{Influence of margin .} The results of different setting of the margin  in Eq \ref{equ:con} is shown in Table \ref{tab:abalpha}. We find that a small value of  will lead the training signals dominated by easy negatives thus decrease the performance while a high value of  hinder the model to learn sufficient information for increasing the distances of hard negatives. Empirically, we find 0.4 to be the best value of  in our experiments. 

\subsection{Qualitative Analysis}
\label{sec:qual}

We show the visualization results of proposed TransFG on the four benchmarks in Fig \ref{fig:vis}. We randomly sample three images from each dataset. Two kinds of visualizations are presented. The first and the third row of Fig \ref{fig:vis} illustrated the selected tokens positions. For better visualization results, we only draw the Top-4 image patches (ranked by the activation value) and enlarge the square of the patches by two times while keep the center positions unchanged. The second and fourth rows show the overall attention map of the whole image where we average the weights of all attention heads to obtain a single attention map. The lighter a region is, the more important it is. From the figure, we can clearly see that our TransFG successfully captures the most important regions for an object, i.e., head, wings, tail for birds; ears, eyes, legs for dogs; lights, doors for cars. At the same time, our overall attention map maps the entire object precisely even in complex backgrounds. See examples from NABirds dataset where birds are sitting on twigs. The bird parts are lighted while the occluder twigs are ignored. \section{Conclusion}

In this work, we propose a novel fine-grained visual classification framework TransFG and achieve state-of-the-art results on four common fine-grained benchmarks. We exploit self-attention mechanism to capture the most discriminative regions. Compared to bounding boxes produced by other methods, our selected image patches are much smaller thus becoming more meaningful by showing what regions really contribute to the fine-grained classification. The effectiveness of such small image patches also comes from the Transformer Layer to handle the inner relationships between these regions instead of relying on each of them to produce results separately. Contrastive feature learning is introduced to increase the discriminative ability of the classification tokens. Qualitative visualizations further prove the effectiveness and the interpretability of our method. 

With the promising results achieved by TransFG, we believe that the transformer-based models have great potential on fine-grained tasks and our TransFG could be a starting point for future works.
%
 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}