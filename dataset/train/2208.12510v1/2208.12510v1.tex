


\documentclass[sigconf]{acmart}

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\let\Bbbk\relax
\usepackage{amssymb}

\usepackage{verbatim}
\usepackage{changepage}
\usepackage{url}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{amsmath,dsfont}
\usepackage{array}
\usepackage{enumitem}

\usepackage{pifont}
\usepackage{threeparttable}

\usepackage{verbatim}

\usepackage{colortbl}
\definecolor{mygray}{gray}{.75}

\usepackage{xcolor}
\usepackage{bbding}


\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}

\newcommand{\todo}[1]{\textcolor{red}{[\textbf{TODO}: #1]}}
\newcommand{\jf}[1]{\textcolor{blue}{[#1]}}
\newcommand{\xr}[1]{\textcolor{purple}{[\textbf{XR}: #1]}}
\newcommand{\xre}[1]{\textcolor{purple}{#1}}


\usepackage[normalem]{ulem}


\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\sep}{,}

\newcommand{\eg}{\emph{e.g.,}~}
\newcommand{\etal}{\emph{et al.}~}
\newcommand{\ie}{\emph{i.e.,}~}







\copyrightyear{2022} 
\acmYear{2022} 
\setcopyright{acmcopyright}\acmConference[MM '22]{Proceedings of the 30th ACM International Conference on Multimedia}{October 10--14, 2022}{Lisboa, Portugal}
\acmBooktitle{Proceedings of the 30th ACM International Conference on Multimedia (MM '22), October 10--14, 2022, Lisboa, Portugal}
\acmPrice{15.00}
\acmDOI{10.1145/3503161.3547976}
\acmISBN{978-1-4503-9203-7/22/10}






\begin{document}

\title{Partially Relevant Video Retrieval}




\author{Jianfeng Dong}
\author{Xianke Chen}
\affiliation{
\institution{Zhejiang Gongshang University}
\country{}
}

\author{Minsong Zhang}
\affiliation{
\institution{Zhejiang Gongshang University}
\country{}
}

\author{Xun Yang}
\affiliation{
\institution{University of Science and Technology of China}
\country{}
}

\author{Shujie Chen}
\affiliation{
\institution{Zhejiang Gongshang University}
\country{}
}

\author{Xirong Li}
\authornote{Corresponding authors: Xirong Li and Xun Wang}
\affiliation{
\institution{Key Lab of DEKE, Renmin University of China}
\country{}
}

\author{Xun Wang}
\authornotemark[1]
\affiliation{
\institution{Zhejiang Gongshang University}
\country{}
}



















\renewcommand{\shortauthors}{Jianfeng Dong et al.}

\begin{abstract}

Current methods for text-to-video  retrieval (T2VR) are trained and tested on video-captioning oriented datasets such as MSVD, MSR-VTT and VATEX. A key property of these datasets is that videos are assumed to be temporally pre-trimmed with short duration, whilst the provided captions well describe the gist of the video content. Consequently, for a given paired video and caption, the video is supposed to be fully relevant to the caption. In reality, however, as queries are not known a priori, pre-trimmed video clips may not contain sufficient content to fully meet the query. This suggests a gap between the literature and the real world. To fill the gap, we propose in this paper a novel T2VR subtask termed \emph{Partially Relevant Video Retrieval} (PRVR). An untrimmed video is considered to be partially relevant w.r.t. a given textual query if it contains a moment relevant to the query. PRVR aims to retrieve such partially relevant videos from a large collection of untrimmed videos. PRVR differs from single video moment retrieval and video corpus moment retrieval, as the latter two are to retrieve moments rather than untrimmed videos. We formulate PRVR as a multiple instance learning (MIL) problem, where a video is simultaneously viewed as a bag of video clips and a bag of video frames. Clips and frames represent video content at different time scales. We propose a Multi-Scale Similarity Learning (MS-SL) network that jointly learns clip-scale and frame-scale similarities for PRVR. Extensive experiments on three datasets (TVR, ActivityNet Captions, and Charades-STA) demonstrate the viability of the proposed method. We also show that our method can be used for improving video corpus moment retrieval. 
\end{abstract}







\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003371.10003386.10003388</concept_id>
       <concept_desc>Information systems~Video search</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Video search}



\keywords{Video-Text Retrieval, Partially Relevant, Multiple Instance Learning, Video Representation Learning}

\begin{teaserfigure}
  \includegraphics[width=\textwidth]{examples_new.pdf}
  \vspace{-6mm}
  \caption{Two textual queries partially relevant to a given video. Only a specific moment in the video is relevant to the corresponding query, while the other frames are irrelevant. We formulate the task of partially relevant video retrieval (PRVR) as a multiple instance learning problem, and propose a MS-SL network.
  MS-SL first detects a key clip that is most likely to be relevant to the query. Then, the importance of each frame is measured in a fine-grained temporal scale under the guidance of the key clip. The final similarity is computed by jointly considering the query's similarities with the key clip and the frames.
  }
\label{fig:teaser}
\end{teaserfigure}
\maketitle


\section{Introduction}\label{sec:introduction}


With the advent of the big data era, millions of videos are uploaded to the Internet every day. There is an increasing need of retrieving videos from the big data. 
As common users prefer to express their information need by natural-language queries, research on text-to-video retrieval (T2VR) is important \cite{li2020sea,sigir2020tree,croitoru2021teachtext}.
Given a query in the form of a natural language sentence, T2VR asks to retrieve videos that are semantically relevant to the given query from a gallery of videos.
Current methods \cite{liu2019use,jin2021hierarchical,chen2020fine,gabeur2020multi,han2021fine,luo2021clip4clip} for T2VR are trained and tested on video-captioning oriented datasets such as MSVD \cite{chen2011collecting}, MSRVTT \cite{xu2016msr} and VATEX \cite{wang2019vatex}. A key property of these datasets is that videos are assumed to be temporally pre-trimmed with short duration, whilst the provided captions well describe the gist of the video content. Consequently, for a given paired video and caption, the video is supposed to be fully relevant to the caption. In reality, however,  as queries are not known a priori, pre-trimmed video clips may not contain sufficient content to fully meet the query. This suggests a gap between the literature and the real world. 

To fill the above gap, we propose in this paper a novel T2VR subtask termed \textit{Partially Relevant Video Retrieval (PRVR)}.
An untrimmed video is considered to be partially relevant w.r.t. a given textual query as long as the video contains a (short) moment relevant w.r.t. the query, see Fig. \ref{fig:teaser}.
PRVR aims to retrieve such partially relevant videos from a large collection of untrimmed videos.
Because where the relevant moment is localized and how long it lasts are both unknown (see Fig. \ref{fig:ratio_distribution}), PRVR is more challenging than the conventional T2VR task.

Observing the connection between PRVR and Multiple Instance Learning (MIL) \cite{dietterich1997solving,maron1997framework} at a high level, we tackle the new task by a multi-scale MIL approach.
In the current context, a video is simultaneously viewed as a bag of video clips and a bag of video frames. 
Clips and frames represent video content at different temporal scales, which is helpful for handling moments of varying temporal lengths.
Besides, based on the multi-scale video representation, we propose Multi-Scale Similarity Learning (MS-SL) network. In MS-SL, we regard the clip-scale as the coarse temporal granularity as it is typically of longer duration. Besides, the frame-scale is regraded as the fine-grained temporal granularity, as frames usually reflect more detailed content of videos.
The multi-scale similarity learning consists of a clip-scale SL branch built on clip-scale video representation and a frame-scale SL branch built on frame-scale video representation.
They are jointly learned in a coarse-to-fine manner.
Note that the two similarity learning branches are not independent. In clip-scale SL, a key clip that is most likely to be
relevant to the query will be detected.
Then the clip-scale similarity is computed as the similarity between the key clip and the query. Additionally, the key clip is regarded as a guide for frame-scale SL to measure the importance of each frame in a fine-grained temporal scale. The frame-scale similarity is computed as the similarity between the weighted frames with the query. Finally, the clip-scale similarity and the frame-scale similarity are jointly used to measure the final video-text similarity.


It is worth noting that PRVR differs from \emph{Single Video Moment Retrieval (SVMR)} \cite{yuan2019find,zhang2020learning,xiao2021boundary,yang2021deconfounded} and \emph{Video Corpus Moment Retrieval (VCMR)} \cite{lei2020tvr,zhang2020hierarchical,zhang2021video,hou2021conquer}, as the latter two are to retrieve moments rather than untrimmed videos.
Additionally, although our model is proposed for PRVR, it can also be used for improving VCMR.
In sum, our main contributions are as follows: \\
     We propose a new T2VR subtask named PRVR, where an untrimmed video is considered to be partially relevant with respect to a given textual query if it contains a moment relevant to the query. PRVR aims to retrieve such partially relevant videos from a large collection of untrimmed videos. \\
     We formulate the PRVR subtask as a MIL problem, simultaneously viewing a video as a bag of video clips and a bag of video frames. Clips and frames represent video content at different temporal scales. Based on multi-scale video representation, we propose MS-SL to compute the  relevance between videos and queries in a coarse-to-fine manner. \\
     Extensive experiments on three datasets (TVR \cite{lei2020tvr}, ActivityNet Captions \cite{krishna2017dense}, and Charades-STA \cite{gao2017tall}) demonstrate the viability of the proposed method for PRVR. We also show that our method can be used for improving video corpus moment retrieval. Source code and datasets are available at \url{http://danieljf24.github.io/prvr}



\begin{figure}[tb!]
\subfigure[TVR]{
\includegraphics[width=0.47\columnwidth]{TVR_moment-to-video_ratio_data.pdf}
}
\subfigure[ActivityNet Captions]{
\includegraphics[width=0.47\columnwidth]{ac_moment-to-video_ratio_data.pdf}
}
\vspace{-4mm}
\caption{Distribution of moment-to-video ratio on (a) TVR and (b) ActivityNet Captions. Moment-to-video ratio indicates the moment's length ratio in the entire video. Moments show a large variance in their temporal lengths. }\label{fig:ratio_distribution}
\end{figure}


\section{Related Work} \label{sec:rel-work}


\textbf{T2VR}. The T2VR task has gained much attention in recent years \cite{dong2018predicting,song2019Polysemous,chen2020fine,ging2020coot,sigir2020tree,han2021fine,liu2021progressive,wang2020learning,eccv2022-laff}, aiming to retrieve relevant videos by a given query from a set of pre-trimmed  video clips. The retrieved clips are supposed to be fully relevant to the given query.
A common solution for T2VR is to first encode videos and textual queries and then map them into common embedding spaces where the cross-modal similarity is measured.
Therefore, current works mainly focus on video encoding \cite{liu2019use,jin2021hierarchical,feng2021exploiting,song2021spatial}, sentence encoding \cite{chen2020fine,li2020sea,croitoru2021teachtext}, and their cross-modal similarity learning \cite{yu2018joint,dong2021dual,gabeur2020multi,wu2021hanet}.
Different from the above works, we consider a more realistic scenario, 
where videos are supposed to be partially relevant to a specific query. We thus focus more on how to measure the partial relevance between textual queries and videos.



\textbf{VMR}. The VMR task is to retrieve moments semantically relevant to the given query from a given single untrimmed video or a large collection of untrimmed videos.
The former is known as SVMR~\cite{anne2017localizing,liu2021context,zheng2022progressive,qu2020fine,liu2020jointly,yang2022video,wang2021visual}, and the latter is known as VCMR~\cite{escorcia2019temporal,paul2021text,zhang2020hierarchical,wang2022siamese}.
In SVMR, existing methods mainly concentrate on how to precisely localize temporal boundings of target moments, and could be typically classified as proposal-based methods~\cite{chen2018temporally,yuan2019semantic,zhang2019man,wang2020temporally,gao2021fast} and proposal-free methods~\cite{yuan2019find,qu2020fine,chen2020rethinking}.
Proposal-based methods first generate multiple moment proposals, then match them with a query to determine the most relevant one from the proposals. Without generating moment proposals, proposal-free methods predict the start and end time points of the target moment based on the fused video-query feature.
As the extension of SVMR, the VCMR task is to retrieve moments (or video segments) that are semantically relevant w.r.t. a given query from a collection of untrimmed videos. The state-of-the-art methods (\eg ReLoCLNet \cite{zhang2021video} and XML \cite{lei2020tvr}) for VCMR have a two-stage workflow. The first stage is to retrieve a number of candidate videos which may contain the target moment, while the second stage is to retrieve moments from the candidate videos. 

Different from video moment retrieval aiming to retrieve moments, our proposed PRVR task aims to retrieve untrimmed videos.
 Besides, while PRVR is similar to VCMR's first stage yet requires no moment-level annotations as commonly needed for VCMR. Therefore, a method for PRVR can in principle be used to improve a two-stage method for VCMR, and our proposed model is designed for PRVR, it can also be used for improving VCMR. 


\textbf{MIL}. 
MIL \cite{dietterich1997solving,maron1997framework} is a classical framework for learning from weakly annotated data, and widely used for classification tasks \cite{li2021multi,li2021dual}. In MIL, a sample is defined as a bag of multiple instances, and there is only a label associated with the bag instead of the instance. 
Besides, a bag is positive if the bag contains at least one positive instance and negative if it contains no such positive instance. 
Existing MIL methods could be roughly grouped into instance-based methods \cite{pinheiro2015image,oquab2015object,feng2017deep} and embedding-based methods \cite{ilse2018attention,tu2019multiple,li2021dual}.
The former typically predicts a score of each instance in the bag and aggregates them to generate a bag score. The latter usually aggregates embedding of all instances into a bag embedding, then outputs a bag score based on the bag embedding.
In this work, we formulate the PRVR task as a MIL problem.
Different from the previous MIL works that usually regrade a sample as a specific bag of instances, in this work a video is simultaneously viewed as a bag of video clips and a bag of video frames. Moreover, we employ MIL for the retrieval task instead of the classification task.



\begin{figure}[tb!]
\centering\includegraphics[width=\columnwidth]{pvr_model_1.pdf}
\vspace{-8mm}
\caption{The framework of our proposed model for partially relevant video retrieval.
\# denotes a temporal sliding window of size  with a stride of 1.
}\label{fig:framework}
\vspace{-4mm}
\end{figure}

\section{Our Method}
We formulate PRVR as a MIL problem. 
As moments relevant to queries typically show large variations in their temporal lengths, we devise multi-scale video representation to represent videos at multiple temporal scales, obtaining a bag of video clips of varying lengths and a bag of video frames. Based on the two bags, we further propose multi-scale similarity learning to measure the partial query-video relevance, see Fig. \ref{fig:framework}.

\subsection{Formulation of PRVR}

Given a natural language query, the task of \textit{PRVR} aims to retrieve videos containing a moment that is semantically relevant to the given query, from a large corpus of untrimmed videos.
As the moment referred to by the query is typically a small part of a video, we argue that the query is partially relevant to the video.
It is worth pointing out that PRVR is different from conventional T2V retrieval~\cite{dong2019dual,chen2020fine,han2021fine}, where videos are pre-trimmed and much shorter, and queries are usually fully relevant to the whole video.

To build a PRVR model, a set of untrimmed videos are given for training, where each video is associated with multiple natural language sentences. Each sentence describes the content of a specific moment in the corresponding video. Note that we do not have access to the start/end time points of the moments (moment annotations) referred to by the sentences.

\subsection{Sentence Representation} \label{ssec:sent-rep}


For sentence representation, we adopt the method by Lei \etal \cite{lei2020tvr}, considering its good performance on VCMR.
Specifically, given a sentence consisting of  words, a pre-trained RoBERTa~\cite{liu2019roberta} is firstly employed to extract word features.
Then a fully connected (FC) layer with a ReLU activation is utilized to map the word features into a lower-dimensional space.
After adding the learned positional embedding to the mapped features, a standard Transformer layer~\cite{vaswani2017attention} is further employed to obtain a sequence of -dimensional contextualized word feature vectors . 
In the Transformer, the features are fed to a multi-head attention layer followed by a feed-forward layer, and both layers are connected with residual connection \cite{he2016deep} and layer normalization \cite{ba2016layer}.
Finally, a sentence-level representation  is obtained by employing a simple attention on :

where  denotes softmax layer,  is trainable vector, and  indicates the attention vector.


\subsection{Multi-Scale Video Representation}

Given an untrimmed video, we first represent it by a sequence of -dimensional feature vectors , where  denotes the number of the vectors.
The feature sequence is obtained by extracting frame-level features using a pre-trained 2D CNN, or extracting segment-level features using a pre-trained 3D CNN.
For the ease of description, we regard  as a sequence of frame-level features in the following.
Based on , we construct multi-scale video representation, jointly using a clip-scale feature learning branch and a frame-scale feature learning branch.



\subsubsection{Clip-scale video representation}
Before constructing video clips, we first downsample the input in the temporal domain to reduce the length of the feature sequence, which helps reduce the computational complexity of the model.
Specifically, given a sequence of frame feature vectors  as the input, we downsample them into a fixed number of feature vectors, where each feature vector is obtained by mean pooling over the corresponding multiple consecutive frame features.
Then, the video is described by a sequence of new feature vectors , where  indicates the number of the corresponding feature vectors.
In order to make the features more compact, we employ an FC layer with a ReLU activation.
Moreover, we also use standard Transformer~\cite{vaswani2017attention} with a learned positional embedding to improve the temporal dependency of the features.
Formally, through an FC layer and a one-layer Transformer, we obtain :

where  denotes the output of the positional embedding. The reduced feature array  is further used for clip-scale  video representation learning. 


For clip construction, we employ a multi-scale sliding window strategy to generate video clips, as illustrated in Fig. \ref{fig:framework}.
Note that different from previous work~\cite{zhang2020hierarchical} where video clips are of equal length and non-overlapping, our video clips are of varied lengths and overlapping.
Concretely, we apply sliding windows of different sizes over   along its temporal dimension with a stride of 1. 
Given a sliding window of size , a clip feature is obtained by mean pooling over the features within the given window. The resultant feature sequence is denoted as .
Consequently, by jointly employing sliding windows of varied sizes as , we are able to obtain . 
Putting them together, a video can be represented a sequence of video clips :

where  denotes the feature representation of -th clip,  is the number of all generated clips which meets .




\subsubsection{Frame-scale video representation}
As the initial frame features are extracted independently, they naturally lack temporal dependency. To bring such dependency back,  we again utilize Transformers. 
Specifically, given the frame feature sequence , we first utilize an FC layer with ReLU activation to reduce the dimensionality of the input, followed by a standard Transformer with a positional embedding layer. The re-encoded frame features, denoted , are computed as:  

Note that the network structures of Transformer, FC and PE are the same as that in the clip-scale branch, but their trainable parameters are not shared. This allows each branch to learn parameters suitable for their own scale.



\begin{figure}[tb!]
\centering\includegraphics[width=\columnwidth]{pvr_model_2_new.pdf}
\vspace{-8mm}
\caption{The illustration of multi-scale similarity learning. }\label{fig:ms-sl}
\end{figure}



\subsection{Multi-Scale Similarity Learning}\label{ssec:mssl}
As we have no priori about where the relevant content is localized in PRVR, it is challenging to directly compute video-text similarity on a fine-grained scale.
Here, we propose multi-scale similarity learning, which computes the similarity in a coarse-to-fine manner.
It first detects a key clip that is most likely to be relevant to the query. 
Then, the importance of each frame is measured in a fine-grained temporal scale under the guidance of the key clip.
The final similarity is computed by jointly considering the query's similarities with the key clip and the frames.
The hypothesis here is that if one model briefly knows coarse relevant content with respect to the query, it will help the model to find more relevant content on a more fine-grained scale accurately.
The framework of the multi-scale similarity learning is illustrated in Fig. \ref{fig:ms-sl}.

\subsubsection{Clip-scale Similarity}\label{ssec:clip}
Given a video as a sequence of video clips, we first measure the cross-modal similarity of each video clip with the query, and then aggregate the instance similarities to obtain the clip-scale similarity.
Specifically, given a sequence of video clips , 
we use cosine similarity between each instance representation and the query representation, followed by a max-pooling operator on the similarities. More formally, the clip-scale similarity is obtained as:

where  denotes the cosine similarity function.
The max-pooling determines the clip with the highest similarity, and we then utilize its similarity with the query as the whole video's similarity with the query. Besides, we select this clip as \emph{key clip}, which is used for the latter frame-scale similarity learning.


\subsubsection{Frame-scale Similarity}
To obtain the frame-scale similarity, we first aggregate a sequence of frame feature vectors to a feature vector under the guidance of the key clip obtained in Section \ref{ssec:clip}, and then compute its similarity with the query as the frame-scale similarity. 
Specifically, given a sequence of video frames , we devise a Key Clip Guided Attention (KCGA) to aggregate frame features.
The implementation of KCGA borrows the idea of multi-head self-attention (MHSA) mechanism in Transformer \cite{vaswani2017attention}.
MHSA first projects the input into queries, keys, values, and then computes the output as a weighted sum of the values. The weight assigned to each value is computed by a compatibility function of the query with the corresponding key.
Different from MHSA utilizes the same input to construct queries, keys, and values, here we take the feature vector of the key clip as the query, and the video frame features as keys and values. Formally, the aggregated frame feature vector is obtained as:

where  indicates the feature vector of the key clip,  and  are two trainable projection matrices.
The dot product measures the similarity between frames and the key clip, resulting in larger values for frames that are more similar to the key clip. Therefore, frames that are more similar to the key clip will have greater attention weights.

Finally, the frame-scale similarity is measured as the cosine similarity between the aggregated frame feature vector  and query feature vector , namely:




\subsubsection{Similarity Learning}
In this section, we first introduce the definition of positive and negative pairs for similarity learning.
Inspired by MIL \cite{dietterich1997solving,maron1997framework}, we define that a query and video pair is positive if the video contains certain content that is relevant to the query, and negative if no relevant content in the video. 

Based on the above definition, we jointly use the triplet ranking loss \cite{faghri2017vse++,dong2021dual} and InfoNCE loss \cite{miech2020end,zhang2021video} that are widely used in retrieve related tasks, and found them complementary. 
Given a positive video-query pair , the triplet ranking loss over the mini-batch  is defined as:

where  is the margin constant,  denotes the similarity function which we can use the clip-scale similarity  or the frame-scale similarity . Besides,  and  respectively indicate a negative sentence sample for  and a negative video sample for . The negative samples are randomly sampled from the mini-batch at the beginning of the training, while being the hardest negative samples after 20 epochs.

Given a positive video-query pair , the infoNCE loss over the mini-batch  is computed as:

where  denotes all negative queries of the video  in the mini-batch, while  denotes all negative videos of the query  in the mini-batch.

As the previous work~\cite{li2020sea} has concluded that using one loss per similarity function performs better than using one loss on the summation of multiple similarities, we employ the above two losses on both clip-scale similarity and frame-scale similarity, instead of their sum.
Finally, our model is trained by minimizing the following overall training loss:

where  and  denote the triplet ranking loss using the clip-scale similarity  and frame-scale similarity  respectively, and accordingly for  and .  and  are hyper-parameters to balance the contribution of infoNCE loss.




\subsection{Model Inference}
After the model has been trained, the similarity between a video and a sentence query is computed as the sum of their clip-level similarity and frame-level similarity, namely:

where  is a hyper-parameter to balance the importance of two similarities, ranging within [0, 1].
Given a query, we sort all videos from the video gallery in descending order according to their similarity with respect to the given video.




\section{Experiments} \label{sec:eval}


\subsection{Experimental Setup} \label{ssec:exp-set}


\subsubsection{Datasets}
In order to verify the viability of our proposed model for PRVR, queries that are partially relevant to videos are required. As videos in popular T2VR datasets such as MSR-VTT \cite{xu2016msr}, MSVD \cite{chen2011collecting} and VATEX \cite{wang2019vatex} are supposed to be fully relevant to the queries, they are not suited for our experiments.
Here, we re-purpose three datasets commonly used for VCMR, \ie TVR \cite{lei2020tvr}, Activitynet Captions \cite{krishna2017dense}, and Charades-STA \cite{gao2017tall}, considering their natural language queries partially relevant with the corresponding videos (a query is typically associated with a specific moment in a video).
Table \ref{tab:dataset_stat} summarizes the brief statistics of these datasets, including average lengths of moments and videos, and the average moment length proportion in the whole video (moment-to-video ratio). Note that as we focus on retrieving videos, moment annotations provided by these datasets are not used in our proposed new PRVR task.


\begin{table} [tb!]
\renewcommand{\arraystretch}{1.2}
\caption{Brief statistics of three public datasets used in our experiments.
The length is measured in seconds.}
\vspace{-4mm}
\label{tab:dataset_stat}
\centering 
\scalebox{0.85}{
\begin{tabular}{lrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{Datasets}} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{Average length}} & \multicolumn{1}{l}{} & \multicolumn{3}{l}{\textbf{Moment-to-video ratio}} \\ \cline{3-4} \cline{6-8} 
 &  & moments & videos & \multicolumn{1}{l}{} & \multicolumn{1}{r}{\emph{min}} & \multicolumn{1}{r}{\emph{max}} & \multicolumn{1}{r}{\emph{mean}} \\ \hline
TVR &    & 9.1  & 76.2  & & 0.48\% & 100\%  & 11.9\% \\
\multicolumn{1}{c}{Activitynet Captions} &   & 36.2  & 117.6 &  & 0.48\% & 100\% & 30.8\% \\
Charades-STA &   & 8.1   & 30.0 & & 4.3\% & 100\%  & 26.3\% \\
\bottomrule
\end{tabular}
 }\end{table}




\textbf{TV show Retrieval (TVR)} \cite{lei2020tvr} is a multimodal dataset originally for video corpus moment retrieval, where videos are paired with subtitles that are generated by automatic speech recognition.
It contains 21.8K videos collected from 6 TV shows, and each video is associated with 5 natural language sentences that describe a specific moment in the video. As a moment is typically a part of a video, we assume that sentences are partially relevant to videos, and use them to evaluate our model.
Following \cite{zhang2020hierarchical,zhang2021video}, we utilize 17,435 videos with 87,175 moments for training and 2,179 videos with 10,895 moments for testing. 



\textbf{ActivityNet Captions} \cite{krishna2017dense} is originally developed for dense video captioning task, and is now a popular dataset for single video moment retrieval.
It contains around 20K videos from Youtube, and the average length of videos is the largest among the three datasets we used. On average, each video has around 3.7 moments with corresponding sentence descriptions.We use the popular data partition used in \cite{zhang2020hierarchical, zhang2021video}. 



 \textbf{Charades-STA} \cite{gao2017tall} is a dataset for single video moment retrieval. It contains 6,670 videos with 16,128 sentence descriptions. Each video has around 2.4 moments with corresponding sentence descriptions on average. We utilize the official data partition for model training and evaluation.



\subsubsection{Evaluation Metrics}
To evaluate PRVR models, we utilize the rank-based metrics, namely  (), which are commonly used for the conventional text-to-video retrieval~\cite{wang2020learning,dong2021dual}. 
 is the fraction of queries that correctly retrieve desired items in the top  of the ranking list. 
The performance is reported in percentage (\%).
Higher  means better performance. For overall comparison, we also report the Sum of all Recalls (SumR).


\subsubsection{Implementation Details}
We use PyTorch as our deep learning environment, and we will release our source code.
For video feature, on TVR, we utilize the feature provided by \cite{lei2020tvr}, that is, 3,072-D  visual feature obtained by the concatenation of frame-level ResNet152 \cite{he2016deep} feature and segment-level I3D \cite{carreira2017quo} feature.
For ease of reference, we refer to it as ResNet152-I3D.
On ActivityNet-Captions and  Charades-STA, we only utilize the same I3D feature, which are respectively provided by \cite{zhang2020hierarchical} and \cite{mun2020local}.
For sentence feature, we use the 768-D RoBERTa feature provided by \cite{lei2020tvr} on TVR, where RoBERTa is finetuned on the queries and subtitle sentences of TVR.
On ActivityNet-Captions and  Charades-STA, we use the 1,024-D RoBERTa feature extracted by ourselves using the open RoBERTa toolkit\footnote{\url{https://pytorch.org/hub/pytorch_fairseq_roberta/}}.
Due to the limited space of the paper, we present more detailed implementation details in the supplementary material.



\subsection{Comparison with Baseline Methods} \label{ssec:exp-sota}


\begin{table} [tb!]
\renewcommand{\arraystretch}{1.2}
\caption{Performance of PRVR on the TVR dataset. 
Models are sorted in ascending order in terms of their overall performance. Visual feature: ResNet152-I3D.
}\vspace{-4mm}
\label{tab:sota-tvr}
\centering 
\scalebox{0.9}{
\begin{tabular}{l*{6}{r}c @{}}
\toprule
\textbf{Model} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R100} &&  \textbf{SumR}\\
\cmidrule{1-7}
\emph{T2VR models:} \\ 
W2VV, TMM18 \cite{dong2018predicting} & 2.6 & 5.6 & 7.5 & 20.6 && 36.3\\
HGR, CVPR20 \cite{chen2020fine} & 1.7 & 4.9 & 8.3 & 35.2 && 50.1  \\
HTM, ICCV19 \cite{miech2019howto100m}  & 3.8 & 12.0 & 19.1 & 63.2 && 98.2\\
CE, BMVC19 \cite{liu2019use} & 3.7 & 12.8 & 20.1 & 64.5 && 101.1  \\
W2VV++, MM19 \cite{li2019w2vv++}& 5.0 & 14.7 & 21.7 & 61.8 && 103.2 \\
VSE++, BMVC19 \cite{faghri2017vse++} & 7.5 & 19.9 & 27.7 & 66.0  && 121.1  \\
DE, CVPR19 \cite{dong2019dual}  & 7.6 & 20.1 & 28.1 & 67.6  && 123.4 \\
DE++, TPAMI21 \cite{dong2021dual} & 8.8 & 21.9 & 30.2 & 67.4  && 128.3 \\
RIVRL, TCSVT22 \cite{dong2022reading} & 9.4 & 23.4 & 32.2 & 70.6 && 135.6\\
\cmidrule{1-7}
\multicolumn{3}{l}{\emph{VCMR models w/o moment localization:}} \\
XML, ECCV20 \cite{lei2020tvr}& 10.0 & 26.5 & 37.3 & 81.3  && 155.1\\
ReLoCLNet, SIGIR21\cite{zhang2021video} & 10.7 & 28.1 & 38.1 & 80.3  && 157.1 \\ 
\cmidrule{1-7}
Ours & \textbf{13.5} & \textbf{32.1} & \textbf{43.4} & \textbf{83.4} && \textbf{172.4}\\
\bottomrule
\end{tabular}
 }\end{table}



\subsubsection{Baseline selection}
As models specifically designed for PRVR are non-existing, we compare with models targeted at conventional T2VR and models developed for VCMR.
Given the rich literature, we have to be selective, choosing \emph{open-source} models for fair and reproducible comparison. In particular, we choose the following nine T2VR models, \ie 
VSE++~\cite{faghri2017vse++}, W2VV~\cite{dong2018predicting}, CE~\cite{liu2019use}, W2VV++~\cite{li2019w2vv++}, DE~\cite{dong2019dual}, HTM \cite{miech2019howto100m}, HGR~\cite{chen2020fine}, DE++~\cite{dong2021dual} and RIVRL \cite{dong2022reading}, and the following two VCMR models, \ie XML \cite{lei2020tvr} and ReLoCLNet\cite{zhang2021video}.
Both XML and ReLoCLNet are two-stage, where a first-stage module is used to retrieve candidate videos followed by a second-stage module to localize specific moments in the candidate videos. As moment annotations are unavailable for PRVR, we have re-trained XML and ReLoCLNet (with their moment localization modules removed) using the same video features as ours.


\begin{figure*}[tb!]
\centering\includegraphics[width=1.9\columnwidth]{group_recalls.pdf}
\vspace{-4mm}
\caption{Performance of different models on different types of queries. Queries are grouped according to their M/V.
}\label{fig:group_recalls}
\end{figure*}


\subsubsection{Results on the TVR dataset}

Table \ref{tab:sota-tvr} summarizes the performance comparison on TVR.
Our proposed model consistently outperforms all conventional T2VR models with a clear margin. Even the best performing model RIVRL among the T2VR models, our model outperforms it by 36.8 in terms of SumR.
As these models focus on the whole similarity between videos and queries, the results allow us to conclude that such similarity modeling is sub-optimal for PRVR.
For the models of the second group, \ie ReLoCLNet and XML, they perform better than the conventional T2VR models, but they are still worse than ours. 
ReLoCLNet and XML focus on retrieving moment, which to some extent model the partial relevance, but they compute the similarity only in terms of a specific scale. By contrast, we compute the similarity in terms of both clip scale and frame scale. The results demonstrate the effectiveness of our proposed multi-scale similarity learning for PRVR.
Note that when using the extra subtitle feature provided by \cite{lei2020tvr}, our model obtains better performance (R@1 of 24.0 and SumR of 220.8).

\begin{table} [tb!]
\renewcommand{\arraystretch}{1.0}
\caption{Performance of PRVR on the ActivityNet Captions dataset. Visual feature: I3D.
}\vspace{-4mm}
\label{tab:sota-activitynet}
\centering 
\scalebox{0.9}{
\begin{tabular}{l*{6}{r}c @{}}
\toprule
\textbf{Model} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R100} &&  \textbf{SumR}\\
\cmidrule{1-7}
\emph{T2VR models:} \\ 
W2VV \cite{dong2018predicting} & 2.2 & 9.5 & 16.6 & 45.5 && 73.8\\
HTM \cite{miech2019howto100m}  & 3.7 & 13.7 & 22.3 & 66.2 && 105.9\\
HGR \cite{chen2020fine} & 4.0 & 15.0 & 24.8 & 63.2 && 107.0  \\
RIVRL \cite{dong2022reading} & 5.2 & 18.0 & 28.2 & 66.4 && 117.8\\
VSE++ \cite{faghri2017vse++} & 4.9 & 17.7 &28.2 & 67.1 && 117.9  \\
DE++ \cite{dong2021dual} & 5.3 & 18.4 & 29.2 & 68.0 && 121.0 \\
DE \cite{dong2019dual}  & 5.6 & 18.8 & 29.4 & 67.8  && 121.7 \\
W2VV++ \cite{li2019w2vv++}& 5.4 & 18.7 & 29.7 & 68.8 && 122.6 \\
CE \cite{liu2019use} & 5.5 & 19.1 & 29.9 & 71.1 && 125.6  \\
\cmidrule{1-7}
\multicolumn{7}{l}{\emph{VCMR models w/o moment localization:}} \\
ReLoCLNet \cite{zhang2021video} & 5.7 & 18.9 & 30.0 & 72.0 && 126.6 \\ 
XML \cite{lei2020tvr}& 5.3 & 19.4 & 30.6 & 73.1 && 128.4\\
\cmidrule{1-7}
Ours & \textbf{7.1} & \textbf{22.5} & \textbf{34.7} & \textbf{75.8}  && \textbf{140.1} \\
\bottomrule
\end{tabular}
 }\end{table}


\begin{table} [tb!]
\renewcommand{\arraystretch}{1.0}
\caption{Performance of PRVR on the Charades-STA dataset. Visual feature: I3D.
}\vspace{-4mm}
\label{tab:sota-charades}
\centering 
\scalebox{0.9}{
\begin{tabular}{l*{6}{r}c @{}}
\toprule
\textbf{Model} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R100} &&  \textbf{SumR}\\
\cmidrule{1-7}
\emph{T2VR models:} \\ 
W2VV \cite{dong2018predicting} & 0.5 & 2.9 &4.7 & 24.5 && 32.6\\
VSE++ \cite{faghri2017vse++} & 0.8 & 3.9 &7.2 & 31.7 && 43.6  \\
W2VV++ \cite{li2019w2vv++} & 0.9 & 3.5 &6.6 & 34.3 && 45.3\\
HGR \cite{chen2020fine} & 1.2 & 3.8 & 7.3 & 33.4 && 45.7  \\
CE \cite{liu2019use} & 1.3 & 4.5 & 7.3 & 36.0 && 49.1  \\
DE \cite{dong2019dual}  & 1.5 & 5.7 & 9.5 & 36.9  && 53.7 \\
DE++ \cite{dong2021dual}& 1.7 & 5.6 & 9.6 & 37.1  && 54.1 \\
RIVRL\cite{dong2022reading}  & 1.6 & 5.6 & 9.4 & 37.7 && 54.3\\ 
HTM \cite{miech2019howto100m}  & 1.2 & 5.4 & 9.2 & 44.2 && 60.0\\
\cmidrule{1-7}
\multicolumn{7}{l}{\emph{VCMR models w/o moment localization:}}\\
ReLoCLNet \cite{zhang2021video} & 1.2 & 5.4 & 10.0 & 45.6 && 62.3\\
XML \cite{lei2020tvr} & 1.6 & 6.0 & 10.1 & 46.9 && 64.6\\
\cmidrule{1-7}
Ours   & \textbf{1.8} & \textbf{7.1} & \textbf{11.8} & \textbf{47.7}  && \textbf{68.4} \\
\bottomrule
\end{tabular}
 }\end{table}

To gain a further understanding of the individual models, we define \textit{moment-to-video ratio} (M/V) for query, which is measured by its corresponding moment's length ratio in the entire video.
The smaller M/V indicates less relevant content while more irrelevant content with respect to the query.
Besides, the smaller M/V to some extent means a lower relevance of a query to its corresponding video, while the larger one indicates a higher relevance.
According to M/V, queries can be automatically classified into different groups, which enables a fine-grained analysis of how a specific model responds to the different types of queries.
On TVR, the 10,895 test queries are split according to their M/V into six groups, with the performance of each group shown in Fig. \ref{fig:group_recalls}.

Unsurprisingly, our model consistently performs the best in all groups.
Observing the figure from left to right, the average performance of the twelve compared models increases along with the M/V, from 106.8, 114.2 114.3, 118.6, 125.8 and 127.7.
The performance in the group with the lowest M/V is the smallest, while the group with the highest M/V is the largest.
The result allows us to conclude that the current video retrieval baseline models better address queries of larger relevance to the corresponding video. 
By contrast, the performance we achieved is more balanced in all groups. This result shows that our proposed model is less sensitive to irrelevant content in videos.


\subsubsection{Results on Activitynet Captions and Charades-STA}

The performance of different models on Activitynet Captions and Charades-STA are summarized in Table \ref{tab:sota-activitynet} and Table \ref{tab:sota-charades}, respectively.
On both datasets, our model is still at the leading position.The results again verify the effectiveness of our model for measuring partial relevance between videos and queries.
Interestingly, we observe that HTM performs badly on TVR and Activitynet Captions, while on Charades-STA it achieves the best SumR score among the T2VR models. We speculate it is due to the fact that Charades-STA has the least training data among the three datasets. Besides, the model structure of HTM is very simple, respectively using an FC layer with gating mechanism to embed videos and sentences into a common space, showing an advantage of training on small-scale data.
For our proposed model, it consistently performs the best on the three datasets of the varying number of training samples, which to some extent shows that our model is not sensitive to the scale of training data.




\begin{table*}[tb!]
\caption{Model comparison in terms of FLOPs and memory consumption.}
\vspace{-3mm}
\label{consumption}
\begin{tabular}{llcccccccccccc}
\toprule
\multicolumn{2}{l}{} & W2VV & HGR & HTM & CE & W2VV++ & VSE++ & DE & DE++ & RIVRL & XML & ReLoCLNet & Ours \\
\hline
\multicolumn{2}{l}{\textbf{FLOPs (G)}} & 0.42  & 2.96  & 0.06  & 0.06  & 0.4  & 0.20  & 5.24  & 5.30  & 8.64  & 0.80  & 0.96   & 1.22  \\
\hline
\multicolumn{2}{l}{\textbf{Memory (MiB)}} & 1231 & 8555  & 1225  & 1435 & 1281 & 1299 & 5837 & 3515  & 4809  & 2451  & 2673  & 5349\\
\bottomrule
\end{tabular}

\end{table*}



\subsection{Comparison on Model Complexity}\label{sec:consumption}
Table \ref{consumption} summarizes the model complexity comparison in terms of the time complexity and memory consumption.
For a specific method, its time complexity is measured as FLOPs it takes to encode a given video-text pair. In terms of FLOPs, our model is at the mid-level, slightly slower than XML and ReLoCLNet, yet faster than RIVRL, DE and HGR. In terms of memory consumption, our model requires more memory than the majority of compared models, which is mainly due to the usage of transformer and multi-scale video representations. 
However, we found that our model takes about 0.2 seconds to retrieve videos from 20,000 candidate untrimmed videos, given that the video embeddings are pre-computed. The retrieval speed is adequate for instant response.




\begin{figure}[tb!]
\subfigure[IoU=0.5]{
\includegraphics[width=0.47\columnwidth]{replace_iou_0.5_bigger.pdf}
}
\subfigure[IoU=0.7]{
\includegraphics[width=0.47\columnwidth]{replace_iou_0.7_bigger.pdf}
}
\vspace{-4mm}
\caption{Performance of XML and ReLoCLNet without/with our model as the first stage for VCMR. 
}\label{fig:replace_experiment}
\end{figure}


\subsection{PRVR for VCMR}\label{ssec:prvr4vcmr}

Our PRVR model can also be used in the first stage of VCMR.To that end, we replace the first stage of two VCMR models, \ie XML \cite{lei2020tvr} and ReLoCLNet \cite{zhang2021video}, with our model. 
Both visual and subtitle features are used for video representation.

Fig. \ref{fig:replace_experiment} shows the performance of the original models and the replaced ones on the TVR dataset.
Here, we report \textit{SumR}, the sum of R1/R5/R10/R100. Replacing the first stage with our model improves both XML and  ReLoCLNet.





\subsection{Ablation Studies} \label{ssec:ablation}

\begin{table} [tb!]
\renewcommand{\arraystretch}{1.0}
\caption{Ablation study on the TVR dataset. 
}\vspace{-3mm}
\label{tab:ablation}
\centering 
\scalebox{0.95}{
\begin{tabular}{lrrrrr}
\toprule
\textbf{Model} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R100} &  \textbf{SumR}\\
\cmidrule{1-6}
Full setup & \textbf{13.5} & \textbf{32.1} & \textbf{43.4} & \textbf{83.4} & \textbf{172.4}\\
\hline
w/o frame-scale branch & 12.3 & 30.5 & 41.5 & 82.3 & 166.6  \\
w/o clip-scale branch & 8.0 & 21.0 & 30.0 & 74.0 & 133.0 \\
\hline 
w/o key clip guide & 12.2 &	30.6 &	41.0 &	82.4 &	166.3 \\
\hline
w/o InfoNCE &11.3  & 29.1 & 40.1 & 81.3 & 161.8  \\
w/o Triplet loss & 11.2 & 29.2 & 40.4 & 81.9 & 162.6 \\
\bottomrule
\end{tabular}
}\end{table}

\subsubsection{The effectiveness of multi-scale branches}
To examine the usefulness of the multi-scale branches, we compare the counterpart without the clip-scale branch or the frame-scale branch.
As shown in Table \ref{tab:ablation}, removing any branch results in clear performance degeneration. The result not only demonstrates the effectiveness of the multi-scale solution, but also shows the complementary of the clip-scale and the frame-scale branches.

\subsubsection{The effectiveness of key clip guided attention}
Additionally, we also compare the model w/o key clip guide, which is implemented by replacing key clip guided attention with a simple attention. The simple attention is implemented as Eq. \ref{eq:att} without any guide. 
As Table \ref{tab:ablation} shows, our model with the full setup still performs better, which shows the importance of key clip guided attention for PRVR.

\subsubsection{The effectiveness of the combination of triplet ranking loss and InfoNCE loss}
To validate the choice of joint use of the two losses, we compare the results of using either triplet ranking loss or infoNCE loss. As shown in Table \ref{tab:ablation}, triplet ranking loss and InfoNCE give comparable results when used alone, but they are much worse than the model with the full setup of jointly using both. 
The result demonstrates the benefit of using these two losses jointly.




\begin{figure}[tb!]
\centering\includegraphics[width=0.6\columnwidth]{alpha.pdf}
\vspace{-4mm}
\caption{The influence of the hyper-parameter  in Eq. \ref{eq:alpha}.
}\label{fig:parameter_alpha}
\vspace{-4mm}
\end{figure}

\subsubsection{The effect of  on the retrieval performance.}
The influence of the hyper-parameter  in Eq. \ref{eq:alpha} is studied as follows. We try  with its value ranging from 0.1 to 0.9 with an interval of 0.1. As shown in Fig. \ref{fig:parameter_alpha}, when the  is larger than 0.3, the performance of using multi-scale similarity are all over 170, which consistently outperform the counterparts using the frame-scale or the clip-scale similarity alone. 


\section{Conclusions} \label{sec:conc}
In this paper, we have proposed a novel T2VR subtask termed PRVR.
Different from the conventional T2VR where a query is usually fully relevant to the corresponding video, it is typically partially relevant in PRVR. 
Besides, videos in the conventional T2VR are temporally pre-trimmed with short durations, while videos are untrimmed in PRVR and a video is typically partially relevant to multiple sentences of different semantics. 
Additionally, PRVR differs from SVMR and VCMR, as the latter two are to retrieve moments rather than untrimmed videos.
Towards PRVR, we have formulated it as a MIL problem, and propose MS-SL which computes the similarity on both clip scale and frame scale in a coarse-to-fine manner. Extensive experiments on three datasets have verified the effectiveness of MS-SL for PRVR, and have shown that it can also be used for improving VCMR.

\medskip

\textbf{Acknowledgements}.
This work was supported by the National Key R\&D Program of China (2018YFB1404102), NSFC (62172420, 61902347, 61976188, 62002323), the Public Welfare Technology Research Project of Zhejiang Province (LGF21F020010), the Open Projects Program of the National Laboratory of Pattern Recognition, the Fundamental Research Funds for the Provincial Universities of Zhejiang, and Public Computing Cloud of RUC.



























































































































































\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{mm2022}




















\clearpage

\appendix
\section{Appendix}

We report more experimental results and more technical details which are not included in the paper due to space limit:
\begin{itemize}
    \item More comparisons on the TVR dataset, including extra ablation studies to explore the significance of the transformer module, comparing with models using extra subtitles  and comparing with conventional T2VR models using clips (Section \ref{sec:tvr}).
    \item Performance comparison on pre-trimmed video datasets (Section \ref{sec:pre-trimmed}).
    \item Distribution of moment-to-video ratios on Charades-STA (Section \ref{sec:mvr}).
    \item More technical details of our method (Section \ref{sec:details}).
\end{itemize}

\begin{table}[b]
\caption{Ablations on the usage of the Transformer.}
\label{extra ablation}
\centering
\scalebox{0.9}{
\begin{tabular}{lllccccc}
\toprule
\multicolumn{2}{l}{} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R@100} & \textbf{SumR} \\
\hline
\multicolumn{2}{l}{ReLoCLNet (best baseline)} & 10.7 & 28.1 & 38.1 & 80.3 & 157.1 \\
\multicolumn{2}{l}{Ours (1D-CNN)} & 10.6 & 29.3 & 39.9 & 81.9 & 161.6 \\
\multicolumn{2}{l}{Ours (bi-LSTM)}& 11.2 & 29.0 & 40.2 & 81.9 & 162.3 \\
\multicolumn{2}{l}{Ours (bi-GRU)}& 12.4 & 31.6 & 42.9 & 83.3 & 170.2 \\
\multicolumn{2}{l}{Ours (Transformer)}& 13.5 & 32.1 & 43.4 & 83.4 & 172.4\\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[b]
\renewcommand{\arraystretch}{1.2}
\caption{Performance of PRVR with using subtitle features. Visual feature:ResNet152-I3D, Subtitle feature:RoBERTa.
}
\label{tab:tvr_sub}
\centering 
\scalebox{0.99}{
\begin{tabular}{lrrrrr}
\toprule
\textbf{Method} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R100} &  \textbf{SumR}\\
\cmidrule{1-6}
XML\cite{lei2020tvr} & 17.4 & 39.3 & 51.5 & 89.1 &197.3\\
ReLoCLNet\cite{zhang2021video}  & 19.1 & 40.3 & 51.5 & 87.0 & 197.9\\
Ours & \textbf{24.0} & \textbf{47.8} & \textbf{58.8} & \textbf{90.2} & \textbf{220.8}\\
\hline
\end{tabular}
}\end{table}

\subsection{More Experiments on TVR}\label{sec:tvr}

\subsubsection{Ablations on the usage of the transformer} \label{sec:extra ablation}
Among the compared methods on the TVR dataset, the top three ranked methods (ReLoCLNet, XML and RIVRL) use Transformer modules: ReLoCLNet and XML utilize Transformers for text and video representation, while RIVRL uses a Transformer for video representation.
So we conduct extra ablation study to explore whether is the usage of the  Transformer module mainly contributes the significant performance improvement of our method.
We replace all the three Transformer modules in our model with 1D-CNN, bi-GRU, bi-LSTM, respectively. As shown in the Table \ref{extra ablation}, our model in the Transformer-free setup obtains SumR of 161.6, 170.2 and 162.3, respectively \cite{lei2020tvr} dataset. While their performance is worse than the counterpart using Transformer, they are still better than the best baseline, \ie, ReLoCLNet with SumR of 157.1.


\subsubsection{Comparison with models using extra subtitles}\label{tvr-sub}
As the TVR dataset is a multimodal dataset where each video is additionally associated with subtitle (dialogue) texts, in this experiment we compare with models using extra subtitles. Here, we do not compare with conventional T2VR models, as they do not support using extra subtitles.
We compare with XML \cite{lei2020tvr} and ReLoCLNet \cite{zhang2021video}, and use the same 768-D subtitles features provided by \cite{lei2020tvr} as extra video features. The results are shown in Table \ref{tab:tvr_sub}, and our proposed model again performs the best.





\subsubsection{Comparison with conventional T2VR models using clips}\label{tvr-clip}

As the conventional T2VR models are typically designed for retrieving video clips, in this experiment we employ conventional T2VR  models using clips to explore their potential for PRVR.
Specifically, during the inference, we first split videos into multiple clips, and then compute the similarity of each clip with the query. The maximum similarity is regarded as the final similarity between the video and the query.

\begin{figure}[htb]
\subfigure[Content-agnostic Strategy]{
\centering\includegraphics[width=0.75\columnwidth]{split_by_handcraft2.pdf}
}
\subfigure[Content-aware Strategy]{\centering\includegraphics[width=0.75\columnwidth]{split_by_shots2.pdf}
}
\caption{Performance of four conventional T2VR models using clips generated by (a) content-agnostic strategy and (b) content-aware strategy on the TVR dataset. Their performance is still much worse than our proposed model which achieves a SumR score of 172.4.}\label{fig:split_experiment}
\end{figure}

Besides, we adopt two strategies to generate clips from video, \ie content-agnostic strategy and content-aware strategy.
The content-agnostic strategy first splits the video into  video units evenly, then constructs video clips by using a specific video unit or concatenating adjacent video units. The larger  means generating more video clips, and  indicates using the whole video for inference.
The content-aware strategy generates video clips by a scene detector toolkit\footnote{https://github.com/Breakthrough/PySceneDetect/} with an own provided threshold , which automatically splits the video into individual clips according to their content change.
The smaller threshold means generating more video clips.






We conduct experiments with top-4 performing T2VR models on TVR, \ie VSE++, DE, DE++, RIVRL, and the results are shown in Fig. \ref{fig:split_experiment}.
As shown in \ref{fig:split_experiment} (a), all the models achieve performance gains when  is large than 1, and obtain the best performance when . Recall that  indicates using the whole video for inference. The results allow us to conclude that the T2VR  models could be improved by splitting the video into multiple clips with the content-agnostic strategy. However, their performance is still much worse than our proposed model which achieves a SumR score of 172.4 on TVR.
Fig. \ref{fig:split_experiment} (b) shows the results when the content-aware strategy is used. Note that  indicates using the whole video without splitting. We found that splitting the video into multiple clips by the content-aware strategy result in relative performance degeneration, which shows the scene detector is not suitable for PRVR. Besides, we speculate it is due to the content in a moment may has scene changes, and the scene detector is likely to split a moment into multiple parts.



\begin{table}[htb]
\caption{Performance comparison on the MSR-VTT and MSVD dataset. Visual feature:ResNeXt101+ResNet-152.}
\label{tab:msrvtt}
\begin{tabular}{llcccc}
\toprule
\multicolumn{2}{l}{} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{SumR} \\
\hline
\multicolumn{2}{l}{\textbf{On MSR-VTT:}} &  &  &  &  \\
\multicolumn{2}{l}{CE, BMVC19\cite{liu2019use}} & 7.9 & 23.6 & 34.6 & 66.1 \\
\multicolumn{2}{l}{VSE++, BMVC19\cite{faghri2017vse++}} & 8.7 & 24.3 & 34.1 & 67.1 \\
\multicolumn{2}{l}{DE, CVPR19\cite{dong2019dual}} & 11.1 & 29.4 & 40.3 & 80.8 \\
\multicolumn{2}{l}{W2VV++, MM19\cite{li2019w2vv++}} & 11.1 & 29.6 & 40.5 & 81.2 \\
\multicolumn{2}{l}{DE++, TPAMI21\cite{dong2021dual}} & 11.6 & 30.3 & 41.3 & 83.2 \\
\multicolumn{2}{l}{HGR, CVPR20\cite{chen2020fine}} & 11.1 & 30.5 & 42.1 & 83.7 \\
\multicolumn{2}{l}{SEA, TMM21\cite{li2020sea}} & 12.4 & 32.1 & 43.3 & 87.8 \\
\multicolumn{2}{l}{RIVRL, TCSVT22\cite{dong2022reading}} & 13.0 & 33.4 & 44.8 & 91.2 \\
\multicolumn{2}{l}{Ours} & 11.3 & 30.4 & 42.2 & 83.9 \\
\hline
\multicolumn{2}{l}{\textbf{On MSVD:}} &  &  &  &  \\
\multicolumn{2}{l}{DE, CVPR19\cite{dong2019dual}} & 20.3 & 46.8 & 59.7 & 126.8 \\
\multicolumn{2}{l}{CF-GNN, TMM21\cite{wang2020learning}} & 22.8 & 50.9 & 63.6 & 137.3 \\
\multicolumn{2}{l}{W2VV++, MM19\cite{li2019w2vv++}} & 22.4 & 51.6 & 64.8 & 138.8 \\
\multicolumn{2}{l}{SEA, TMM21\cite{li2020sea}} & 24.6 & 55.0 & 67.9 & 147.5 \\
\multicolumn{2}{l}{Ours} & 22.0 & 52.6 & 67.2 & 141.8 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Results on Pre-trimmed Datasets}\label{sec:pre-trimmed}
Although our proposed model is designed for untrimmed video, it can also be utilized for retrieving pre-trimmed by text. Therefore, we conduct experiments on MSR-VTT \cite{xu2016msr} and MSVD \cite{chen2011collecting}, two commonly used pre-trimmed datasets for T2VR.
For MSR-VTT, we follow the official partition, where 6513 video clips for training, 497 video clips for validation and the remaining 2,990 video clips for testing. For MSVD, we also follow the official partition, 1200 video clips are used for training, 100 video clips for validation and 670 video clips for testing
Following the previous works \cite{dong2021dual,dong2022reading,li2019w2vv++}, we use the concatenation of 2048-dim ResNeXt-101 and 2048-dim ResNet-152 features as the video feature. For text representation, we use the open RoBERTa toolkit to extract 1,024-D sentence feature.  


The results are shown in the Table \ref{tab:msrvtt}, where all the methods use the same video feature. Note that not all the compared methods report their performance on both datasets. As expected, our model is not on par with the state-of-the-art models on the two pre-trimmed datasets. Recall that the rationale for our proposed model is to first detect a key clip that is most likely to be relevant to the query and then measure the importance of other frames in a fine-grained temporal scale under the guidance of the key clip. As for pre-trimmed videos in MSR-VTT and MSVD, the majority of their frames are actually relevant w.r.t. the associated descriptions, making key clip detection unnecessary. Our method is thus suboptimal for text-to-video retrieval on MSR-VTT and MSVD.





\subsection{Others}
\subsubsection{Distribution of Moment-to-Video Ratios}\label{sec:mvr}
Fig. \ref{fig:ratio_distribution_appendix} shows the distribution of moment-to-video ratio on Charades-STA. Moment-to-video ratio indicates the moment’s length ratio in the entire video.
Moments on Charades-STA show a large variance in their temporal lengths.
\begin{figure}[htb]
\includegraphics[width=0.7\columnwidth]{ch_moment-to-video_ratio_data.pdf}
\vspace{-3mm}
\caption{Distribution of moment-to-video ratio on Charades-STA. Moment-to-video ratio indicates the moment’s length ratio in the entire video.}\label{fig:ratio_distribution_appendix}
\vspace{-3mm}
\end{figure}




\subsubsection{More Implementation Details}\label{sec:details}
For the video representation module, we set the fixed number  to 32 in the downsampling strategy.
Besides, we set the maxmium frame number  to 128. Once the number of frame is over , it will be downsampled to .
For sentences, we set the maximum length of query  to 30 on TVR and Charades-STA, 64 on ActivityNet Captions, and the words outside the maximum length are simply discarded.
For the Transformer module used in our model, we set its hidden size , and 4 attention heads are employed.
For hyper-parameters in the loss functions, we empirically set =0.02 and =0.04 to make all loss elements have a similar loss value at the beginning of the model training.
For model training, we utilize an Adam optimizer with a mini-batch size of 128. 
The initial learning rate is set to 0.00025, and we take a learning rate adjustment schedule similar to \cite{lei2020tvr}. 
Early stop occurs if the validation performance does not improve in ten consecutive epochs. The maximal number of epochs is set to 100. Note that we will release our source code and data.



\end{document}
