\pdfoutput=1


\documentclass[11pt]{article}


\usepackage[final]{emnlp2021}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{multirow}



\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax} 






\usepackage{array}

\title{One Agent To Rule Them All: Towards Multi-agent Conversational AI}





\author{
  Christopher Clarke\hspace{10pt} Joseph J. Peper\hspace{10pt} Karthik Krishnamurthy\hspace{10pt} Walter Talamonti \\ \textbf{Kevin Leach\thanks{\hspace{1mm} Work was done while at University of Michigan}\hspace{10pt} Walter Lasecki\hspace{10pt} Yiping Kang\hspace{10pt} Lingjia Tang\hspace{10pt} Jason Mars}   \vspace{0.3cm}\\
    \text{University of Michigan, Ann Arbor, MI}\\
    \text{Ford Motor Company, Dearborn, MI}\\
    \text{Vanderbilt University, Nashville, TN}\\
    \text{\{csclarke, jpeper, ypkang, lingjia, profmars\}@umich.edu} \\
    \text{\{kkrish65, wtalamo1\}@ford.com},
    \text{kevin.leach@vanderbilt.edu, wslasecki@gmail.com}
}

\begin{document}
\maketitle
\begin{abstract}
The increasing volume of commercially available conversational agents (CAs) on the market has resulted in users being burdened with learning and adopting multiple agents to accomplish their tasks.
Though prior work has explored supporting a multitude of domains within the design of a single agent, the interaction experience suffers due to the large action space of desired capabilities.
To address these problems, we introduce a new task BBAI: \textbf{B}lack-\textbf{B}ox \textbf{A}gent \textbf{I}ntegration, focusing on combining the capabilities of multiple black-box CAs at scale.
We explore two techniques: \emph{question agent pairing} and \emph{question response pairing} aimed at resolving this task.
Leveraging these techniques, we design One For All (OFA), a scalable system that provides a unified interface to interact with multiple CAs.
Additionally, we introduce MARS: \textbf{M}ulti \textbf{A}gent \textbf{R}esponse \textbf{S}election, a new encoder model for question response pairing that jointly encodes user question and agent response pairs.
We demonstrate that OFA is able to automatically and accurately integrate an ensemble of commercially available CAs spanning disparate domains.
Specifically, using the MARS encoder we achieve the highest accuracy on our BBAI task, outperforming strong baselines.
\end{abstract}

\section{Introduction}

Influenced by the popularity of intelligent conversational agents (CAs), such as Apple Siri and Amazon Alexa, the conversational AI market is growing at an increasingly rapid pace and is projected to reach a valuation of US \QA = \{a_{1}, a_{2}, \ldots, a_{n}\}R = \{r_{1}, r_{2}, ..., r_{n}\}Q(Q, A_{i}, R_{i})QQA = \{a_{1}, a_{2}, \ldots, a_{n}\}(Q, A_{i})QQR = \{r_{1}, r_{2}, ..., r_{n}\}(Q, R_{i})R_iQQAAAQQAD_{i}QD_{i}QS_{i}A\max_i{SemSim(Q, S_{i})}Q(Q, R_{i})QQS(Q, R_{i})Q0.12 for 5 utterances. These submitted utterances were then vetted by hand to ensure quality. Using the curated utterances, we then generated question responses by querying each agent to gather its response to the utterance. 

In order to generate ground truth samples on which of the question-response pairs  correctly resolves the query  we launched a crowdsourcing task asking workers to indicate the candidate responses that best answer the question shown. Five workers were assigned to each response selection task and majority voting (>2) was used to label the gold responses. As such for each query  and the set of responses  we were able to gather the necessary question-agent pairs  and question-response pairs  needed to evaluate our approaches.

\begin{table*}[]
\small
\centering
\resizebox{12cm}{!}{\begin{tabular}{|clc|rrrr|}
\hline
\multicolumn{3}{|l|}{} & \multicolumn{4}{c|}{\textbf{Agent Breakdown}} \\ \hline
\multicolumn{1}{|l}{} & \textbf{Method} & \multicolumn{1}{l|}{\textbf{Accuracy (n=4)}} & \multicolumn{1}{c}{\textbf{Alexa}} & \multicolumn{1}{c}{\textbf{Google}} & \multicolumn{1}{c}{\textbf{Houndify}} & \multicolumn{1}{c|}{\textbf{Adasa}} \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Question Agent Pairing \\ (QA Labels)\end{tabular}} & Bert & 68.31 & 37.98 & \textbf{40.93} & 18.49 & 2.6 \\
 & Electra & 67.86 & 35.28 & \textbf{42.01} & 20.11 & 2.6 \\
 & Roberta & \textbf{69.03} & 34.92 & \textbf{41.56} & 20.65 & 2.87 \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Question Agent Pairing \\ (Descriptions)\end{tabular}} & BM25 & 27.91 & 13.91 & 10.95 & 17.33 & \textbf{57.81} \\
 & USE & \textbf{47.84} & 13.20 & 28.82 & \textbf{52.42} & 5.56 \\
 & Roberta+STS & 39.40 & 18.94 & 22.35 & \textbf{51.35} & 7.36 \\ \hline
\multirow{5}{*}{Response Selection} & BM25 & 51.07 & 28.64 & 24.69 & 14.81 & \textbf{31.86} \\
 & USE & 72.89 & \textbf{34.20} & 27.65 & 22.98 & 15.17 \\
 & USE QA & 75.49 & \textbf{41.65} & 36.45 & 17.95 & 3.95 \\
 & Roberta+STS & 69.83 & 18.94 & 22.35 & \textbf{51.35} & 7.36 \\
 & MARS & \textbf{79.70} & 37.34 & \textbf{43.9} & 15.71 & 3.05 \\ \hline
\multirow{4}{*}{Individual Agents} & Alexa & 49.37 & \multicolumn{1}{c}{\textbf{-}} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c|}{-} \\
 & Google & \textbf{51.79} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c|}{-} \\
 & Houndify & 34.82 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c|}{-} \\
 & Adasa & 4.12 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c|}{-} \\ \hline
\end{tabular}
}
\caption{Performance breakdown of QA and QR approaches on our BBAI task when using our 4 largest agents Alexa, Google, Houndify and Adasa. \textbf{Note:} n = number of agents.}
\label{tab:results-4}
\end{table*}

\paragraph{Agent Descriptions}
We gather our agent descriptions by scraping the contents of each of the agent's public product pages and their built-in feature documentation web pages. We then manually clean, reformat and merge this data into a single document per agent. For our experiment, we focus only on extracting descriptions related to the built-in features of our agents.

Overall our dataset contains 5550 utterances with 19 question-response pairs per question (one from each of the 19 agents), 105,450 in total. The utterances are split into 3700 utterances (100 per domain) for the training set and 1850 (50 per domain) for the test set. The train and test sets respectively contain 2399 and 1186 utterances with at least one positive question-response pair. In the remaining examples, none of the agents were able to achieve annotator agreement (>= 3). A sample dataset example is shown in table \ref{tab:data} with responses from 4 of the 19 agents.

\begin{table}[]
\small
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{|clr|c|}
\hline
\multicolumn{2}{|c}{\textbf{Method}} & \multicolumn{1}{l|}{\textbf{Accuracy (n=19)}} & \textbf{Agents} \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Question Agent Pairing\\  (QA Labels)\end{tabular}} & Bert & 59.10 & \multirow{15}{*}{\begin{tabular}[c]{@{}c@{}}Alexa, Google\\ Houndify, Adasa\\ Recipe agent\\ Dictionary agent\\ Task Manager\\ Hotel agent, Stock agent\\ Math agent, Sports agent\\ Wikipedia agent\\ Mobile Account agent\\ Banking agent\\ Coffee shop agent\\ Event Search agent\\ Jokes agent\\ Reminders agent\\ Covid-19 agent\end{tabular}} \\
 & Electra & 52.86 &  \\
 & Roberta & \textbf{61.88} &  \\ \cline{1-3}
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Question Agent Pairing\\  (Descriptions)\end{tabular}} & BM25 & 23.69 &  \\
 & USE & \textbf{43.59} &  \\
 & Roberta+STS & 36.67 &  \\ \cline{1-3}
\multirow{5}{*}{Response Selection} & BM25 & 59.94 &  \\
 & USE & 64.42 &  \\
 & USE QA & 71.66 &  \\
 & Roberta+STS & 56.82 &  \\
 & MARS & \textbf{83.55} &  \\ \cline{1-3}
\multirow{4}{*}{Individual Agents} & Alexa & 44.09 &  \\
 & Google & \textbf{48.06} &  \\
 & Houndify & 32.04 &  \\
 & Adasa & 3.45 &  \\ \hline
\end{tabular}
}
\caption{Performance breakdown of QA and QR approaches on our BBAI task on all 19 commercial agents we show that the MARS encoder is able to scale and leverage the capabilities of new agents added to the ensemble without diminishing performance compared to other approaches.}
\label{tab:results-19}
\vspace{-.6cm}
\end{table}





\section{Results and Discussion}

In this section we present and analyze the results of our experiments, detailing our insights and discussing the implications of each of our techniques. 

\paragraph{Evaluation task:} Similar to standard information retrieval evaluation measures, we denote accuracy as the metric \emph{precision@1} and use it to evaluate both our question agent and question response pairing approaches. For question agent pairing this metric denotes: Given a set of  agents to the given query, whether the agent selected ultimately resolves the query successfully. For question response pairing it denotes: Given a set of  responses to the given query, whether the top-scoring response resolves the query successfully. For this evaluation, we test on examples with at least one valid agent response.

\subsection{Question agent pairing} The results are summarized in tables \ref{tab:results-4} and \ref{tab:results-19}. We find that for the QA pairing Roberta yields the best result with an accuracy of 69\% in selecting the correct agent and 61.8\% when scaled to 19 agents. Similarly, we see that we can achieve fair performance in extreme data-scarce environments when using simple agent descriptions compared to that of query agent examples, with USE achieving 47.8\% accuracy. Using agent descriptions offers greater flexibility in facilitating the improvement of agents over time compared to query examples since it only requires an update to the agent description. However, it still falls short when compared to using a single agent like Google or Alexa. Also, while consistent in learning to recognize the domain a given agent may be performant in, QA approaches fall short in a few cases: 

\textbf{(1) Agent overlap} - This is when a given domains' coverage is split between various agents. e.g The model learns that both Alexa \& Google have proficiency in handling some weather queries but it remains unclear about which one is best suited for the current query at hand. 

\textbf{(2) Query variation} - While an agent's examples or descriptions may allude to proficiency in a given domain, it may still fail when asked certain query variations. e.g Figure \ref{fig:ofa} shows a case where Alexa is capable of handling weather queries but fails when a condition like humidity is asked for. Another example is when a similar question is asked in a different or more complex way. Both Houndify \& Alexa are known to be proficient at answering age-related questions but for questions like \textit{"How old I will be on September 28, 1995, if I was born on March 29, 1967?"}, Alexa is unable to answer as opposed to Houndify.

These cases are further highlighted when inspecting QA pairing performance at the domain level in table \ref{tab:domain}. We find that the QA approaches struggle with domains such as \emph{"travel suggestion"} and \emph{"Directions"} which are heavily split in coverage and more diverse in their variation.

\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{|lrrr|}
\hline
  \multicolumn{4}{|c|}{\textbf{Evaluation Performance per Domain (n=19)}} \\ \hline
\textbf{Domain} & \multicolumn{1}{l|}{\textbf{MARS (QR)}} & \multicolumn{1}{l|}{\textbf{USE (QA)}} & \multicolumn{1}{l|}{\textbf{Roberta (QA)}} \\ \hline
Weather & \multicolumn{1}{r|}{0.88} & \multicolumn{1}{r|}{0.45} & 0.67 \\ \hline
Directions & \multicolumn{1}{r|}{0.78} & \multicolumn{1}{r|}{0.29} & 0.44 \\ \hline
Auto & \multicolumn{1}{r|}{1.00} & \multicolumn{1}{r|}{0.79} & 0.82 \\ \hline
Restaurant Suggestion & \multicolumn{1}{r|}{0.79} & \multicolumn{1}{r|}{0.5} & 0.68 \\ \hline
Travel Suggestion & \multicolumn{1}{r|}{0.97} & \multicolumn{1}{r|}{0.33} & 0.57 \\ \hline
Time & \multicolumn{1}{r|}{0.81} & \multicolumn{1}{r|}{0.54} & 0.76 \\ \hline
Flight Info & \multicolumn{1}{r|}{0.83} & \multicolumn{1}{r|}{0.61} & 0.7 \\ \hline
Date & \multicolumn{1}{r|}{0.82} & \multicolumn{1}{r|}{0.47} & 0.56 \\ \hline
\end{tabular}
}
\caption{Further breakdown of the best-performing approaches per technique on a subset of 8 out of the 37 domains. We find that our MARS encoder generalizes well across the various agent domains.}
\label{tab:domain}
\vspace{-.5cm}
\end{table}

\subsection{Question response pairing}
In overall performance we find that our MARS encoder outperforms strong baselines, achieving 83.55\% accuracy on the BBAI task. We note that our MARS encoder outperforms the best single performing agent (Google Assistant) by 32\%. This shows the utility and power of OFA in not only alleviating the need for users to learn and adopt multiple agents but also validating that multiple agents working collectively can achieve significantly more than single agents working in isolation.

When inspecting the performance of MARS at the domain level we see in Table \ref{tab:domain} that it is able to maintain its high performance across the varying domains unlike the QA approaches. This advantage comes from the ability to select an agent at the response level allowing the system to catch cases in which an agent once deemed proficient fails or another agent improves.


\subsection{Agent pairing vs Response pairing}
We now describe the trade-offs between agent pairing and response pairing. Question response pairing greatly outperforms agent pairing in terms of accuracy, given that it is privy to the final responses from each of the agents. However, in practice, this comes with additional networking, compute, and latency costs from having to send the query to each of the agents and await their response. Given that the querying of agents is done in parallel, the latency cost is equal to that of the slowest agent. 
Question response pairing also better supports agent adaptation. With response pairing, a system can seamlessly add or remove an agent without diminishing the experience as shown by MARS in table \ref{tab:results-19}. In addition, as conversational agents are upgraded to offer a more diverse feature-set such as new domain support or improved responses, they can instantly be integrated into a response pairing approach.

\subsection{Scalability}
We evaluate our approaches on a suite of 19 commercially deployed agents spanning 37 broad domain categories. As shown in table \ref{tab:results-4} we examine performance when using the 4 largest agents in terms of domain support and popularity (Alexa, Google Assistant, Houndify and Ford Adasa) showing improvement upon single-agent use in both QA and QR approaches. When scaled up to 19 agents, MARS encoder improves even further by leveraging the new capabilities of the additional agents and is the only approach that does not decrease in performance as the number of agents and domains scale. This improvement is due to the input sensitive representations that the MARS encoder is able to learn by encoding both the question and response in a single transformer.

\textbf{Cross-encoding vs Bi-encoding}
For pairwise sentence scoring tasks such as response selection which compare question response pairs, it is important to be mindful of the trade-offs between cross encoder based models such as MARS in figure \ref{fig:system} (b) and bi-encoder models such as USE in \ref{fig:system} (a). Cross-encoders perform full self-attention over the pairwise input of the question and response, thus, producing an encoding representative of the combined input. This typically leads to much more performative models, especially in pairwise scoring tasks such as ours. However, given that this encoding isn't independent of the question for each question response pair, it is necessary to produce an encoding for each question label pair. Bi-encoders on the other hand perform self-attention over the question response pairs separately, map them to a dense vector space, and score them using an appropriate distance metric. With this separation, bi-encoders are able to index the question and compare these representations for each response resulting in faster prediction times when the numbers of candidate responses to a given question scales. Given the nature of our BBAI task which focuses on the scoring of responses to a singular question as opposed to a clustering task which requires an encoding for every pairwise combination across a set of sentences, cross-encoder based architectures remain a viable option even at the production scale for our use case.


\section{Related Work}

Ensemble approaches to solving complex tasks in the context of NLP are widely used \cite{deng2014ensemble, ARAQUE2017236}.
In dialogue systems, recent attempts at ensemble approaches and multi-agent architectures include \citet{alanav2} and \citet{Subramaniam:2018:CCM:3237383.3237472}.
AlanaV2 \cite{alanav2} demonstrated an ensemble architecture of multiple bots using a combination of rule-based machine learning systems built to support topic-based conversations across domains. It was built to be an open domain bot supporting topic-based conversations. Specifically, AlanaV2's architecture utilizes a variety of ontologies and NLU pipelines that draw information from a variety of web sources such as Reddit. However, its agent selection approach is guided by a simple priority bot list.
Subramaniam et al.~\cite{Subramaniam:2018:CCM:3237383.3237472} describe their conversational framework that employs an \textit{Orchestrator Bot} to understand the user query and direct them to a domain-specific bot that handles subsequent dialogue. In our work, we expand up the multi-agent goal by focusing on the integration of black-box conversational agents at scale.

\subsection{Response Selection}
This is the task of selecting the most appropriate response given context from a pool of candidates. It is a central component of information retrieval applications and has become a focal point in the evaluation of dialogue systems. \cite{sato-etal-2020-evaluating, henderson2019convert, DBLP:journals/corr/abs-2010-07785}. Prior work has shown strong performance on sentence pairing tasks through the use of sentence encoders and language model fine-tuning \cite{henderson2019convert, Humeau2020Poly-encoders:, reimers-2019-sentence-bert}. In our work, we explore the task of response selection using it as one of the bases for integrating black-box conversation agents.

\section{Conclusion}
The rapid proliferation of conversational agents calls for a unified approach to interacting with multiple CAs. The key challenge of building such an interface lie in that most commercial CAs are black-boxes with hidden internals. This paper introduces BBAI a new task of agent integration that focuses on unifying black-boxes CAs across varying domains. We explore two task techniques, question agent pairing and question response pairing and present One For All, a scalable system that unifies multiple black-box CAs with a centralized user interface. Using a combination of commercially available conversational agents, we evaluate a variety of approaches to multi-agent integration through One For All.
Our MARS encoder achieves 83.5\% accuracy on BBAI and outperforms the best single agent configuration by over 32\%. These results demonstrate the power of One For All which can leverage state-of-the-art NLU approaches to enable multiple agents to collectively achieve more than any single conversational agent in isolation eliminating the need for users to learn and adopt multiple agents. 

This work opens up a wide range of potential future work involving the design of systems geared towards facilitating more advanced multi-agent interaction. We foresee a system with even greater response selection performance as the NLP community continues to produce more state-of-the-art language models with even greater contextual knowledge of the world. Extensions of this work can include examining not only the integration of agents but the interoperability by facilitating the passing of shared conversation knowledge across agents especially in multi-turn conversational scenarios across multiple agents.


\section*{Acknowledgements}
We thank our anonymous reviewers for their feedback and suggestions. We also thank Yi-Chun Chen who assisted in designing the diagrams and figures shown in this work. This work was sponsored by Ford Motor Company.


































































\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}







\end{document}
