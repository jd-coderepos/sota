\documentclass{eptcs}
\providecommand{\event}{GANDALF 2015} 

\usepackage{microtype}
\usepackage{amsmath,amssymb,stmaryrd,xspace,tikz,wasysym}
\usepackage{algorithm, algorithmicx}
\usepackage{algpseudocode}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{thm-restate}

\usetikzlibrary{arrows,automata,shapes,calc,fit}





\newtheorem{defi}{Definition}
\newtheorem{theo}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemm}{Lemma}
\newtheorem{coro}{Corollary}
\newtheorem{exam}{Example}

\newenvironment{definition}{\begin{defi} \rm }{\end{defi}}
\newenvironment{theorem}{\begin{theo} \rm }{\end{theo}}
\newenvironment{proposition}{\begin{prop} \rm }{\end{prop}}
\newenvironment{lemma}{\begin{lemm} \rm }{\end{lemm}}
\newenvironment{corollary}{\begin{coro} \rm }{\end{coro}}
\newenvironment{example}{\begin{exam} \rm }{\end{exam}}

\newenvironment{proof}{\begin{trivlist} \item[\hspace{\labelsep}\bf Proof:]}{\hfill\end{trivlist}}


\newcommand{\tuple}[1]{\langle #1 \rangle}
\newcommand{\nontop}[1]{\textsc{NonTop}(#1)}


\def\nat{\mathbb{N}}
\def\eqdef{\stackrel{\triangle}{=}}
\def\vis{\textit{vis}}
\def\vislen{vislen}
\def\oddresponse{\textsc{OddResponse}}
\newcommand{\plays}[2]{\Pi(#1,#2)}

\renewcommand{\i}{\ensuremath{\ocircle}\xspace}
\newcommand{\odd}{\ensuremath{\square}\xspace}
\newcommand{\even}{\ensuremath{\Diamond}\xspace}
\newcommand{\player}{\ensuremath{\ocircle}\xspace}
\newcommand{\notplayer}{\ensuremath{\overline{\ocircle}}\xspace}

\newcommand{\comment}[1]{\textcolor{blue}{[#1]}}

\newcommand{\dull}[1][]{\ensuremath{\mc{D}^{#1}}\xspace}
\newcommand{\solitaire}[1][]{\ensuremath{\mc{G}^{#1}}\xspace}
\newcommand{\whitegame}[1][]{\ensuremath{\mc{M}^{#1}}\xspace}
\newcommand{\weak}[1][]{\ensuremath{\mc{W}^{#1}}\xspace}

\newcommand{\jur}{\textsc{\small Jurdzi\'nski}\xspace}
\newcommand{\ziel}{\textsc{\small Zielonka}\xspace}

\newcommand{\post}[1]{\ensuremath{#1^{\bullet}}}

\usepackage{todonotes}
\newcommand\todoin[2][]{\todo[inline, caption={2do}, #1]{
\begin{minipage}{\textwidth-4pt}#2\end{minipage}}}


\newcommand{\pnot}[1]{\bar{#1}}
\def\edge{\rightarrow}
\def\false{\bot}
\def\true{\top}

\newcommand{\domain}[1]{\textbf{dom}(#1)}

\newcommand{\attrsym}{\ensuremath{\textit{Attr}}}
\newcommand{\attr}[3][]{\ensuremath{{#2}{\text{-}}\attrsym^{#1}(#3)}}
\newcommand{\myattr}[3]{\ensuremath{{#2}{\text{-}}\attrsym^{#1}(#3)}}

\newcommand{\attrW}[3]{\ensuremath{\mathop{{#1}{\text{-}}\attrsym_{#3}(#2)}}}
\newcommand{\myattrW}[4]{\ensuremath{\mathop{{#2}{\text{-}}\attrsym^{#1}_{#4}(#3)}}}

\newcommand{\winsubodd}[1]{\textsf{Win}_{\odd}(#1)}
\newcommand{\winsubeven}[1]{\textsf{Win}_{\even}(#1)}
\newcommand{\minprio}[1]{\textsf{min}_{\priosym}(#1)}


\newcommand{\mc}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\ie}{\emph{i.e.}\xspace}
\newcommand{\eg}{\emph{e.g.}\xspace}
\newcommand{\viz}{\emph{viz.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\etc}{\emph{etc.}\xspace}
\newcommand{\oftype}{{:}}
\newcommand{\isdef}{=}

\newcommand{\priosym}{\mathcal{P}}
\newcommand{\prio}[1]{\priosym(#1)}

\newcommand{\strategy}[1]{\mathbb{S}_{#1}}
\newcommand{\memstrategy}[1]{\mathbb{S}_{#1}^*}
\newcommand{\allows}{\Vdash}


\newcommand{\nlifts}[1]{\#lifts(#1)}
\newcommand{\oddreach}[1]{\textsf{odd-reach}(#1)}

\pagestyle{plain}



\title{Improvement in Small Progress Measures}
\def\titlerunning{Improvement in Small Progress Measures}
\author{Maciej Gazda and Tim A.C. Willemse
\institute{Eindhoven University of Technology, The Netherlands}
}
\def\authorrunning{M. Gazda and T.A.C. Willemse}



\def\runtimeceil{O(dm \cdot (n/\lceil d / 2 \rceil)^{\lceil d/2 \rceil})}
\def\runtimefloor{O(dm \cdot (n/\lfloor d / 2 \rfloor)^{\lfloor d/2 \rfloor})}
\def\floord2{{\lfloor d/2 \rfloor}}
\def\ceild2{{\lceil d/2 \rceil}}


\begin{document}
\maketitle 


\begin{abstract}
Small Progress Measures is one of the classical parity game solving
algorithms. For games with  vertices,  edges and  different
priorities, the original algorithm computes the winning regions and
a winning strategy for one of the players in  time.
Computing a winning strategy for the other player requires a re-run
of the algorithm on that player's winning region, thus increasing
the runtime complexity to  for computing the winning
regions and winning strategies for both players. We modify the
algorithm so that it derives the winning strategy for both players
in one pass. This reduces the upper bound on strategy derivation
for SPM to . At the basis of our modification is a
novel operational interpretation of the least progress measure that
we provide.  \end{abstract}

\section{Introduction}


A parity game \cite{EJ:91,McN:93,Zie:98} is an infinite duration
game played on a directed graph by two players called \emph{even}
and \emph{odd}. Each vertex in the graph is owned by one of the
players, and labelled with a natural number, called a priority.
The game is played by pushing a token along the edges in the graph;
the choice where to move next is made by the owner of the vertex
on which the token currently resides.  The winner of the thus
constructed play is determined by the parity of the minimal (or
maximal, depending on the convention) priority that occurs infinitely
often, and the winner of a vertex is the player who has a \emph{strategy}
to force every play originating from that vertex to be winning for
her.  Parity games are positionally determined; that is, each vertex is won by
some player~\cite{McN:93}, and  each player has a positional winning strategy
on her winning region. \emph{Solving} a game essentially means
deciding which player wins which vertices in the game.

Parity games play an important role in several foundational results;
for instance, they allow for an elegant simplification of the hard
part of Rabin's proof of the decidability of a monadic second-order
theory, and a number of decision problems of importance can be
reduced to deciding the winner in parity games. For instance, the
model checking problem for the modal -calculus is equivalent,
via a polynomial-time reduction, to the problem of solving parity
games~\cite{EJS:93,SS:98}; this is of importance in computer-aided
verification.  Winning strategies for the players play a crucial
role in supervisory control of discrete event systems, in which
such strategies are instrumental in constructing a supervisor that
controls a plant such that it reaches its control objectives and
avoids bad situations; see \eg~\cite{AVW:03} and the references
therein. In model checking, winning strategies are essential in
reporting witnesses and counterexamples, see~\cite{SS:98}.\medskip

A major impetus driving research in parity games is their computational
status: the solution problem lies in UP and coUP and is, despite
the continued research effort, not known to be in PTIME.  
Deterministic algorithms for solving parity games include the
classical \emph{recursive algorithm}~\cite{Zie:98} and
\emph{small progress measures} (SPM) algorithm~\cite{Jur:00}, the
\emph{bigstep} algorithm~\cite{Sch:07}, the deterministic subexponential
algorithm~\cite{JPZ:06} and a class of strategy improvement
algorithms~\cite{VJ:00,Sch:08,Fea:10}.

Strategy improvement algorithms were long perceived as likely
candidates for solving parity games in polynomial time, but, save
a recently introduced symmetric variant~\cite{STV:15}, they were
ultimately proven to be exponential in the worst-case~\cite{Fri:11}.
The deterministic subexponential algorithm, running in 
where  is the number of vertices in the game, is a modification
of the recursive algorithm which
itself runs in , where  is the number of edges
and  is the number of different priorities in the game. Bigstep,
which runs in , where 
is a small constant and , is a combination
of the recursive algorithm and 
the SPM algorithm.
This latter algorithm solves a game in .

Somewhat surprisingly, our knowledge of classical algorithms such
as SPM and the recursive algorithm is still far from complete. For
instance, the recursive algorithm is regarded as one of the best
algorithms in practice, which is corroborated by experiments~\cite{FL:09}.
However, until our recent work~\cite{GW:13} where we showed the
algorithm is well-behaved on several important classes of parity
games, there was no satisfactory explanation why this would be the
case. In a similar vein, in \emph{ibid.} we provided tighter bounds
on the worst-case running time, but so far, no tight bounds for
this seemingly simple algorithm have been established.  We expect
that if improvements on the upper bound on the parity game solving
problem can be made, such improvements will come from improvements
in, or through a better understanding of the classical algorithms;
this expectation is fuelled by the fact that these classical
algorithms and the ideas behind them have been at the basis of the
currently optimal algorithms.\medskip

In this paper, we focus on Jurdzi\'nski's
small progress measures algorithm. Using a fixpoint computation,
it computes a \emph{progress measure}, a labelling of vertices,
that witnesses the existence of winning strategies.   In general, no clear,
intuitive interpretation of the information contained in the progress
measures has been given, and the mechanics of the algorithm remain rather
technical.  This is in contrast to the self-explanatory
recursive algorithm, and the strategy improvement algorithm, where,
thanks to the ordering of plays according to profiles, at every step,
one has a clear picture of the currently known best strategy. Apart
from Jurdzi{\'n}ski's original article, some additional insight was
offered by Klauck and Schewe.  In~\cite{2001automata},
Klauck defines a specific parity progress measure for a solitaire game with
only even simple cycles and explains that it has a particular interpretation,
but his parity progress measure is not generally related to the measure
computed by the SPM algorithm (not even in the setting of solitaire games).
Schewe, in his paper on
\emph{bigstep} \cite{Sch:07}, analyses progress measures with restricted
codomains and shows how they can be utilised to efficiently detect small dominions.
Our \emph{first} contribution  is to provide a better understanding of
these progress measures and the intermediate values in the fixpoint
computation, see Section~\ref{sec:interpretation}.  By doing so,
a better understanding of the algorithm itself is obtained.

Progress measures come in two flavours, \viz even-and odd-biased,
and their computation time is bounded by either  or
, depending on the parity of the extreme priorities that
occur in the game.
From an even-biased progress measure, one immediately obtains winning
regions for \emph{both} players, but only a winning strategy for
player even on its winning region and not for her opponent. Likewise
for odd-biased progress measures. Obtaining the winning strategy
for an opponent thus requires re-running the algorithm on the
opponent's winning region.  Note that the effort that needs to be
taken to obtain a strategy in the same time bound as the winning
region stems from a more general feature of parity games: a winning
partition in itself does not allow one to efficiently compute a
winning strategy (unless there is an efficient algorithm for solving
parity games). This basic result, which we nevertheless were unable
to find in the literature, is formalised in Section \ref{sec:strategy}.

An essential consequence of this is that the algorithm solves a
parity game in , as one can always compute one of
the two types of progress measures in the shorter time bound. Contrary
to what is stated in~\cite{Jur:00},
the same reasoning does not apply to computing the winning strategy for
a fixed player; this still requires  in the worst
case,  as also observed by Schewe in~\cite{Sch:07}.
An intriguing open problem is
whether it is at all possible to derive the winning strategies for
both players while computing one type of measure only, as this would
lower the exponent in the time bound to .  Our \emph{second}
contribution is to
give an affirmative answer to the above problem.  We modify the
generic SPM by picking a partial strategy when a vertex won by
player  is discovered, and subsequently adjust the lifting
policy so that it prioritises the area which contains an -dominion.
Both steps are inspired by the interpretation of the progress measures
that we discuss in Section~\ref{sec:interpretation}.
Like the original algorithm, our solution, which we present in
Section~\ref{sec:strategy}, still works in polynomial
space. 
Formal proofs of all results can be found in our technical report~\cite{GW:14}.
\section{Parity Games}

\label{sec:parity_games}

A parity game is an infinite duration game, played by players \emph{odd},
denoted by  and \emph{even}, denoted by , on a directed,
finite graph. The game is formally defined as follows.

\begin{definition}
A parity game is a tuple , where
\begin{itemize}
\item  is a finite set of vertices, partitioned in a set  of
vertices owned by player , and a set of vertices  
owned by player ,
\item  is a total edge relation, \ie all vertices
have at least one outgoing edge,
\item  is a priority function that assigns
priorities to vertices.
\end{itemize}
\end{definition}
Henceforth, we assume that  denotes an arbitrary
player; that is . We write  for
's opponent:   and .
Parity games are depicted as graphs; diamond-shaped nodes
represent vertices owned by player , box-shaped nodes
represent vertices owned by player  and the priority assigned
to a vertex is written inside the node, see the game depicted in
Figure~\ref{fig:example}.
\begin{figure}[h!]
\centering
\begin{tikzpicture}[>=stealth']
\tikzstyle{every node}=[draw, inner sep=2pt, outer sep=0pt, node distance=40pt];
  \node[label=above:{\scriptsize },shape=rectangle,minimum size=15] (y4)                    {\scriptsize 1};
  \node[label=above:{\scriptsize },shape=rectangle,minimum size=15] (y5) [left of=y4,xshift=-20pt]                    {\scriptsize 0};
  \node[label=above:{\scriptsize },shape=rectangle,minimum size=15]   (y6) [left of=y5,xshift=-20pt]                   {\scriptsize 2};
  \node[label=above:{\scriptsize },shape=diamond,minimum size=19] (y7) [left of=y6,xshift=-20pt]                   {\scriptsize 3};
  \node[label=above:{\scriptsize },shape=diamond,minimum size=19]   (y8) [left of=y7,xshift=-20pt]                   {\scriptsize 3};
  \node[label=above:{\scriptsize },shape=rectangle,minimum size=15] (y9) [left of=y8,xshift=-20pt]                   {\scriptsize 0};

\path[->]
  (y4.south) edge[bend left] (y6.south)
  (y5) edge (y4)
  (y6) edge (y5)
  (y6.north) edge[bend left] (y4.north) 
  (y7) edge (y6) edge[bend right] (y8) 
  (y8) edge[bend right] (y7) edge[bend right] (y9)
  (y9) edge[loop left] (y9) edge[bend right] (y8) 
;

\end{tikzpicture}
\caption{A simple parity game with 4 different priorities, in which 4 vertices are 
owned by player odd and 2 vertices are owned by player even.}
\label{fig:example}
\end{figure}

Throughout this section, assume that  is an arbitrary parity game.  We write  iff
, and we write  to denote the set of successors of
, \ie\ . For a set of vertices , we will denote
the minimal priority occurring in  with ; by 
 we denote the set of vertices with priority ; likewise
for .   For
a set , we define  as the structure ;
the structure  is defined as . The structures  and  are again 
parity games if their edge relations are total. 

\paragraph*{Plays and Strategies}
A sequence of vertices  is a \emph{path} if  for all .  Infinite paths are defined in a similar
manner. We write  to denote the  vertex in a path .

A game starts by placing a token on some vertex .  Players
 and  move the token indefinitely according to a single
simple rule: if the token is on some vertex that belongs to player
, that player moves the token to an adjacent vertex.
An infinite sequence of vertices created this way is called a
\emph{play}.  The \emph{parity} of the \emph{least} priority that occurs
infinitely often on a play defines the \emph{winner} of the play:
player  wins if, and only if this priority is even. 

A \emph{strategy} for player  is a partial function  satisfying that whenever it is defined for a
finite path , both  and
. We denote the set of
strategies of player  by .  
An infinite play  is \emph{consistent} with a strategy  if
for all prefixes  of the play for which  is defined, . The
set of all plays, consistent with some strategy , and starting
in  is denoted . Some
strategy  is winning for player  from vertex  iff
all plays consistent with  are won by player .  Vertex
 is won by player  whenever she has a winning strategy for
all plays starting in vertex .  Parity games are
\emph{determined}~\cite{EJ:91}, meaning that every vertex is won
by one of the players; they are even \emph{positionally determined},
meaning that if  wins a vertex then she has a winning
\emph{positional strategy}: a strategy that determines where to
move the token next based solely on the vertex on which the token
currently resides. Such strategies can be represented by a function
, and the set of all such strategies for
player  is denoted .  \emph{Solving} a parity game 
essentially means computing the partition 
of  into vertices won by player  and player ,
respectively.

\begin{example}
In the parity game depicted in Figure~\ref{fig:example}, vertices
 and  are won by player  while vertices  and  are
won by player . A winning positional strategy for player 
is to play from  to  and from  to . A winning
strategy for  is to move from  to  and
from  to .
\end{example}

\paragraph*{Attractors and Dominions}
An -\emph{attractor} into a
set  contains all those vertices from which player  can
force any play to ; it is formally defined as follows.
\begin{definition} The -\emph{attractor} into a set ,
denoted , is the least set  satisfying:
\begin{enumerate}
 \item 
 \item
 \begin{enumerate}
 \item if  and , then 
 \item if  and , then 
 \end{enumerate}
\end{enumerate}

\end{definition}
Observe that the complement of an attractor set of any subset of a
parity game induces a parity game, \ie 
for any  and  is a well-defined parity game.  Whenever we
refer to an \emph{attractor strategy} associated with ,
we mean the positional strategy that player  can employ to force
play to ; such a strategy can be computed in 
using a straightforward fixpoint iteration.

A set of vertices  is an -dominion whenever there is a
strategy  for player  such that every play starting in
 and conforming to  is winning for  and stays within
. A game is a \emph{paradise} for player  if the entire game
is an -dominion.

\begin{example}
Reconsider the parity game of Figure~\ref{fig:example}. We have
 and .
The winning region  is an -dominion, but the
subset  is not; the set  is an -dominion.
\end{example}


\newcommand{\Nat}{\ensuremath{\mathbb{N}}}
\newcommand{\prog}[3]{\ensuremath{\textsf{Prog}(#1,#2,#3)}}
\newcommand{\lift}[2]{\ensuremath{\textsf{Lift}(#1,#2)}}
\newcommand{\liftodd}[2]{\ensuremath{\textsf{Lift}_\odd(#1,#2)}}
\def\progname{\textsf{Prog}}
\def\liftname{\textsf{Lift}}


\section{Jurdzi\'nski's Small Progress Measures Algorithm}
\label{sec:spm}

The SPM algorithm works by computing a \emph{measure} associated with
each vertex. This measure is such
that it decreases along the play with each ``bad'' odd priority
encountered, and only increases upon reaching a beneficial even
priority. In what follows, we recapitulate the essentials for defining
and studying the SPM algorithm; our presentation is---as in
the original paper by Jurdzi\'nski---from the perspective of player
. \medskip

Let  be a parity game.
Let  be a positive number and let  be a \emph{-tuple} of natural numbers.  We number
its components from  to , \ie , and let  on such -tuples be given by
the lexicographic ordering. We define a derived ordering  on -tuples and
-tuples of natural numbers as follows:

where, if  or , the tuples are suffixed with s.
Analogously, we write  to denote that  and
 are identical up-to and including position .
The derived ordering provides enough information to
reason about the interesting bits of plays: when encountering a
priority  in a play, we are only interested in how often we can
or must visit vertices of a more significant (\ie smaller) priority
than ,  whereas we no longer care about the less significant priorities that we shall encounter; a more precise interpretation of the information that will be encoded will be given in Section~\ref{sec:interpretation}. 
\medskip

Now, assume from hereon that  is the largest priority occurring in
; \ie,  is one larger than the largest priority
in .  For , let . 
Define , as the largest
set containing 
() and
only those -tuples with  on \emph{even} positions and
natural numbers   on \emph{odd} positions .

The lexicographical ordering  and the family of orderings
 on -tuples is extended to an ordering on 
by setting  and . 
Let  and suppose . Then
 is the least , such that
\begin{itemize}
  \item  if  is even,
  \item , or  if  is odd.
\end{itemize}

\begin{definition} Function
 is a \emph{game parity progress measure}
if for all :
\begin{itemize}
  \item if , then for some , 
  \item if , then for all , 
\end{itemize}
\end{definition}
The following proposition is due to Jurdzi\'nski~\cite{Jur:00}; it shows that
the least game parity progress measure characterises the winning regions of a
parity game.

\begin{proposition}\label{prop:jurdzinski}
If  is the \emph{least} game parity progress measure for , 
then for all :
 iff .
\end{proposition}
The least game parity progress measure can be described as the least fixpoint
of a monotone transformer on the complete lattice we define next. Let
 and define
 as  for all . We write
 if  and . Then
the set of all functions  with  is a complete
lattice. The monotone transformer defined on this set is given as follows:

As a consequence of Tarski's fixpoint theorem, we know the least fixpoint of the
above monotone transformer exists and can be computed using a standard fixpoint iteration
scheme. This leads to the original SPM algorithm, see Algorithm~\ref{alg:spm}.
\begin{algorithm}[h!t]
\begin{algorithmic}[1]
\Function{SPM}{}
\State \emph{\textbf{Input} }
\State \emph{\textbf{Output} Winning partition }
\State 
\While{ for some }
  \State  for some  such that 
\EndWhile
\State \Return 
\EndFunction
\end{algorithmic}
\caption{The original Small Progress Measures Algorithm}
\label{alg:spm}
\end{algorithm}
Upon termination of the iteration within the SPM algorithm, the
computed game parity progress measure  is used to compute the
sets  and  of vertices won by player  and
, respectively.
\begin{theorem} Algorithm~\ref{alg:spm} solves a parity
game in , see~\cite{Jur:00}.
\end{theorem}

The runtime complexity of SPM is obtained by considering the more optimal
runtime of solving a game , or 's `dual'; the latter is obtained by shifting
all priorities by one and swapping ownership of all vertices (alternatively,
a `dual' algorithm can be used, computing with a domain ). 
The runtime complexity for computing winning strategies for both players using
the SPM algorithm is worse than the runtime complexity of solving the game.  
A winning strategy
 for player  can be extracted
from  by setting  for  and  such that 
for all .  A winning strategy for player 
cannot be extracted from  \emph{a posteriori}, so, as also
observed in~\cite{Sch:07}, the runtime
complexity of computing a winning strategy cannot be improved by
considering the dual of a game (contrary to what is stated in~\cite{Jur:00}). 
\begin{theorem}[See also~\cite{Sch:07}]
Algorithm~\ref{alg:spm} can compute
winning strategies for both players in~. 
\end{theorem}
As an illustration of the above observations, consider the family of games
depicted in Figure~\ref{fig:tops_faster}. The more optimal runtime
of  will be achieved by solving the games themselves
(using ) and not their dual. As all games in the family are
-paradises, we cannot extract a winning strategy for player
 from the computed progress measure and the only option we have is to 
solve the dual games with the less favourable runtime of .
In fact, all instances of the family of games depicted in
Figure~\ref{fig:tops_faster} are solved exponentially faster than
their dual, underlining the potential practical implications of re-running
the algorithm.
\begin{figure}[h]
\centering
\begin{tikzpicture}[>=stealth']
\tikzstyle{every node}=[draw, inner sep=1pt, outer sep=0pt, node distance=30pt];
  \node[shape=rectangle,minimum size=24] (y0)                            {\scriptsize };
  \node[shape=rectangle,minimum size=24] (y1) [right of=y0,xshift=20pt]  {\scriptsize };
  \node[shape=rectangle,minimum size=24] (y2) [right of=y1,xshift=20pt]  {\scriptsize };
  \node[shape=rectangle,minimum size=24] (y21) [right of=y2,xshift=20pt]  {\scriptsize };
  \node[draw=none] (y3) [right of=y21,xshift=20pt] {\Huge };
  \node[shape=rectangle,minimum size=24] (y4) [right of=y3,xshift=20pt]  {\scriptsize };
  \node[shape=rectangle,minimum size=24] (y5) [right of=y4,xshift=20pt]  {\scriptsize };
  \node[shape=rectangle,minimum size=24] (y6) [right of=y5,xshift=20pt]  {\scriptsize };

\path[->]
  (y0) edge (y1)
  (y1) edge (y2)
  (y2) edge (y21)
  (y21) edge (y3)
  (y3) edge (y4)
  (y4) edge (y5)
  (y5) edge (y6)
  (y6.south west) edge[bend left=7] (y1.south east) 
  (y6) edge[loop right] (y6)
;
\end{tikzpicture}
\caption{A parity game won by player . Solving the game using
, the first  value is reached after the first full pass
of the cycle containing priority  ( using a fair lifting
strategy), and it will then propagate through the game.
Solving the dual game, or using  takes exponential
time to lift the node with priority .}

\label{fig:tops_faster}
\end{figure}


To facilitate the analysis of SPM, we will use the following  terms and notions. The term \emph{measure} refers to the intermediate values of  in the lifting process as well. Given a tuple , we say that the position  in  is \emph{saturated}, if .


\section{An operational interpretation of progress measures}
\label{sec:interpretation}

\def\globmaxprio{max \priosym}

\def\eventups{\mathbb{M}^{\even}}
\def\exteventups{\mathbb{M}^{\even}_{ext}}
\def\oddtups{\mathbb{M}^{\odd}}

\def\allplays{\Pi}
\def\profilesym{\theta}
\newcommand{\profilefun}[1]{\profilesym_{#1}}
\newcommand{\profile}[2]{\profilesym_{#1}(#2)}

\newcommand{\maxval}[2]{\varphi^{*}_{#1}(#2)}

\newcommand{\succtup}[2]{\textsf{succ}_{#1}(#2)}

While, from a technical perspective, SPM is a relatively simple
algorithm in the sense that it is
concise and its individual steps are elementary operations, it lacks
a clear and appealing explanation of the devices used. It is therefore
difficult to understand, and possibly enhance. Apart from the formal
definition of progress measures, little explanation
of what is hidden behind the values in tuples is offered.
Notable exceptions are \cite{Klau:01}, which explains that for
-solitaire games with only even simple cycles, one can use the 
maximal degrees of `odd
stretches' (a concept we make precise below) in order to define a
parity progress measure, and Schewe's bigstep paper \cite{Sch:07},
where it is shown that dominions of a bounded size can be detected
using measures with a restricted codomain. Klauck's construction
for a specific parity progress measure breaks down for arbitrary parity
games and the constructed parity progress measure is not related to
the measure that is computed by the SPM algorithm, nor to any of the
intermediate measures. In general, the  high-level
intuition is that the larger progress measure values indicate more
capabilities of player , and a value at a given position in the
tuple is somehow related to the number of priorities that 
can enforce to visit.\medskip

In what follows, we present a precise and operational interpretation
of measures. Our interpretation is similar in spirit to the one used in \cite{Klau:01},
but applicable to all parity games, and uses a concept known
from the realm of strategy improvement algorithms -- a value (or
profile) of a play. Here, values are defined in terms of maximal odd-dominated
stretches occurring in a prefix of a play. With this notion at hand,
we can consider an optimal valuation of vertices, being the best
lower bound on play values that player  can enforce, or the
worst upper bound that  can achieve, \ie it is an \emph{equilibrium}.
The optimal valuations range over the same codomain as progress
measures, and the main result of this section states that the least
game parity progress measure is equal to the optimal valuation.\medskip


Let  denote all tuples in 
such that for all  and even positions , ; \ie compared to , we drop the requirement that
values on odd positions  are bounded by . Elements in  are ordered in the same fashion as those in . Given a 
play , a \emph{stretch} is a subsequence
 of . For a priority , a
\emph{-dominated stretch} is a stretch in which the minimal
priority among all vertices in the stretch is . The \emph{degree}
of a -dominated stretch is the number of vertices with priority
 occurring in the stretch.
\begin{definition}
Let us denote all plays in the parity game by . An \emph{value} (or simply value) of a play is a function  defined as follows:
\begin{itemize}
 \item if  is winning for , then 
 \item if  is winning for , then , where 
 , and for every odd position ,  is the \emph{degree} of the maximal -dominated stretch that is a prefix of  
\end{itemize}
\end{definition}
Observe that a play value is well-defined, as an
infinite -dominated stretch for an odd  implies that a game
is won by , and its value is  in such case. 

\begin{example}
Suppose that the sequence of priorities corresponding to a certain
play  is . Then .

\end{example}
 
 
 
The theorem below links the progress measure values to players' capabilities to enforce beneficial plays or avoid harmful ones, where the benefit from a play is measured by a specially devised play value, as it is done in strategy improvement algorithms. 
This offers a more operational view on progress measure values, which, combined with a more fine-grained analysis of the mechanics of SPM allows us to extract winning strategies for both players in the next section.
 

\begin{theorem}
\label{thm:progint}
If  is the least progress measure of a parity game , then, for all :
\begin{enumerate}
 \item there is a strategy  such that for every , 
 \item there is a strategy  such that for every , 
\end{enumerate}
\end{theorem}



A notable difference between strategy improvement algorithms and SPM is
that SPM does not work with explicit strategies, and the intermediate
measure values do not represent any proper valuation induced by
strategies -- only the final least progress measure does. Still,
these intermediate values give an underapproximation of the
capabilities of player  in terms of odd-dominated stretches
that she can enforce.

Note that a consequence of Theorem \ref{thm:progint} is that the least (resp. greatest) play values that player  (resp. ) can enforce are equal, and coincide with the least game parity progress measure  computed by SPM. 
Observe also that player  can always achieve the strategy guaranteeing the optimal even-biased play value with a memoryless strategy, whereas player  may require to that end a strategy that depends on a play's history.

\section{Strategy construction for player \odd}
\label{sec:strategy}

Computing winning strategies is typically part of a practical
solution to a complex verification or a controller synthesis problem.
In such use cases, these strategies are employed to
construct witnesses and counterexamples for the verification problems,
or for constructing control strategies for the controller~\cite{AVW:03}.
As we explained in Section \ref{sec:spm}, the SPM algorithm can
easily be extended to construct a winning strategy for player
. The problem of deriving a winning strategy for player \odd
in SPM (other than by running the algorithm on the `dual' game, or
by using a `dual' domain ) has, however, never
been addressed. Note that the problem of computing strategies is
at least as hard as solving a game. Indeed, even if we are equipped
with a valid winning partition  for
a game , then deriving the strategies witnessing these partitions
involves the same computational overhead as the one required to
compute  in the first place.
\begin{restatable}{proposition}{propstrategiesdifficult} 
\label{prop:strategies_difficult}
The problem of finding the winning partition
 of a given game  can be reduced
in polynomial time to the problem of computing a winning strategy
for player  in a given -dominion.
\end{restatable}

Deriving a strategy for both players by using the SPM to compute
 measures and  measures
consecutively, or even simultaneously, affects, as we already
discussed in Section~\ref{sec:spm}, SPM's runtime.    Being able
to compute \odd strategies without resorting to the aforementioned
methods would also allow us to potentially significantly improve
efficiency on such instances as given by Figure~\ref{fig:tops_faster}.
It may be clear, though, that extracting a winning strategy from
the small progress measures algorithm for vertices with measure
 will require modifying the algorithm (storing additional
data, augmenting the lifting process).  For instance, simply following
the vertex that caused the update to top, fails, as the example
below shows.

\begin{example} \label{ex:greedytop_wrong} Reconsider the game
depicted in Figure~\ref{fig:example}. Recall that 
vertices  and  are won by player , and in all
possible lifting schemes, the first vertex whose measure becomes
 is . After that, a possible sequence of liftings is
that first  is set to  (due to ), followed
by  (due to ). If we set the strategy
based on the vertex that caused the given vertex to be lifted to top,
we obtain , which is not winning for
.

\end{example}
The key problem is that for vertices won by player \odd, from some
point onward, the lifting process discards significant information.
This is best seen in case of lifting to  -- a partial
characterisation of reachable odd priorities contained in a tuple
(see also our previous section) is ultimately replaced with a mere
indication that player \odd can win.


\subsection{A Bounded -Dominion}

Consider a game  on which a standard SPM algorithm with an arbitrary lifting policy has been applied. Suppose that at some point a vertex  is the first one to be lifted to , and after lifting of  the algorithm is suspended, resulting in some temporary measure . Let  be the priority of . 

We will start with two straightforward observations. The first one
is that  must be an odd number; this is because a vertex with an
even priority obtains, after lifting, a -value equal to the
-value of one of its successors, and therefore it cannot be
the first vertex lifted to . Another immediate conclusion is
that at least one (or all, if ) successor(s) of
 has (have)  a -value saturated up to the -th position,
\ie it is of the form ;
were it not the case, then a non-top value  such that  would exist, which would be inconsistent with the definition of
\progname.


\begin{figure}[h!]
\centering
\begin{tikzpicture}[>=stealth']
\tikzstyle{every node}=[draw, inner sep=2pt, outer sep=0pt, node distance=40pt];

\node[draw=none] (x) {};
\node[draw=none] (w) [below of=x,xshift=-10pt,yshift=37pt] {};
\path[draw,fill=gray!8] (w) ellipse (2.4cm and 1.7cm) ;
\node[shape=rectangle,minimum size=12pt,label=left:{\scriptsize },label=right:{\scriptsize }] (y) [above of=x,xshift=-28pt,yshift=-15pt] {\scriptsize };

\node[shape=rectangle,minimum size=12pt,label=left:{\scriptsize }] (z1) [below of= y,xshift=-20pt] {};

\node[draw=none] (z2) [below of=y,xshift=0pt] {};
\node[draw=none] (z3) [below of=y,xshift=20pt] {};


\node[draw=none] (z5) [below of=y,xshift=92pt] {};
\node[draw=none] (z6) [below of=y,xshift=104pt] {};
\node[draw=none] (u) [below of=z2,yshift=20pt] {\scriptsize \begin{tabular}{l}-dominion \\  \end{tabular} };


\path[draw] (x) node[draw=none,xshift=63pt,yshift=10pt] {\Large } ellipse (2.8cm and 2.0 cm);

\draw[->] (y) edge[color=red] (z1) edge  (z3) edge (z5) edge (z6)
;

\end{tikzpicture}
\qquad\quad
\begin{tikzpicture}[>=stealth']
\tikzstyle{every node}=[draw, inner sep=2pt, outer sep=0pt, node distance=40pt];

\node[draw=none] (x) {};
\node[draw=none] (w) [below of=x,xshift=-10pt,yshift=37pt] {};
\path[draw,fill=gray!8] (w) ellipse (2.4cm and 1.7cm) ;
\node[shape=diamond,minimum size=17pt,label=left:{\scriptsize },label=right:{\scriptsize }] (y) [above of=x,xshift=-28pt,yshift=-15pt] {\scriptsize };
\node[draw=none] (z1) [below of=y,xshift=-20pt] {};
\node[draw=none] (z2) [below of=y,xshift=0pt] {};
\node[draw=none] (z3) [below of=y,xshift=20pt] {};
\node[draw=none] (u) [below of=z2,yshift=20pt] {\scriptsize \begin{tabular}{l}-dominion \\  \end{tabular} };


\path[draw] (x) node[draw=none,xshift=63pt,yshift=10pt] {\Large } ellipse (2.8cm and 2.0 cm);

\draw[->] (y) edge (z1) edge  (z3)
;

\end{tikzpicture}
\caption{Snapshot of the SPM algorithm after the first vertex  is lifted to top.}
\label{fig:egg}
\end{figure}

There are two more complex properties, which we can utilise to modify the SPM algorithm and compute the winning strategy for player \odd (see Figure \ref{fig:egg}). 
\begin{enumerate}
 \item Vertex  belongs to an \odd-dominion  within  such that the minimal priority in  is .
 \item If , then picking the successor  of  with the maximal current -value among  is a part of a (positional) winning strategy for \odd that stays within such a dominion  as described above. 
 \end{enumerate}
The intuition concerning the above facts is as follows. Vertices with
a measure value  saturated up to but possibly excluding a certain
position  have a neat interpretation of the measure value at
position : \medskip

\noindent\emph{Player \odd can force the following outcome of a play:
\begin{enumerate}
 \item priority  appears  times without any lower priority in between
 \item the play will reach a -labelled vertex via priorities not more significant than 
 \item the play enters a cycle with an odd dominating priority larger (less significant) than .
\end{enumerate}
}\medskip

\noindent
Therefore, in the situation as described above, \odd can force a
play starting at  to first go in one step to the successor
 of  with a measure of the form , and then to play further and either force
a less significant odd-dominated cycle (cases 2 and 3, since 
is the only -labelled vertex), or to visit vertices with
priority   times without any lower priority in between.
But in the latter case, since  has priority , we have in fact
 vertices with priority  not ``cancelled'' by a lower
priority. Hence player \odd has forced an odd-dominated cycle with
the lowest (most significant) priority .  Note that this does
not imply we can simply construct a winning strategy for  by
always picking a successor with the maximal measure to further
vertices that can be visited along the play; such a method may lead
to an erroneous strategy, as illustrated by Figure \ref{fig:greedywrong}.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[>=stealth']
\tikzstyle{every node}=[draw, inner sep=1pt, outer sep=1pt, node distance=40pt];
  \node[shape=diamond,minimum size=26,label=above:{},label=below:{\scriptsize }] (y1)                            {\scriptsize 1};
  \node[shape=rectangle,minimum size=22,label=above:{},label=below:{\scriptsize ~~~}] (y2) [right of=y1,xshift=20pt]  {\scriptsize 2};
  \node[shape=diamond,minimum size=26,label=below:{},label=right:{\scriptsize }] (y3) [above right of=y2,xshift=35pt]  {\scriptsize 1};
  \node[shape=diamond,minimum size=26,label=above:{},label=right:{\scriptsize }] (y4) [below right of=y2,xshift=35pt]  {\scriptsize 3};

\path[->]
  (y1) edge[color=red] (y2) 
  (y2) edge (y3) edge[bend left,color=red] (y4)
  (y3) edge[bend right] (y1)
  (y4) edge[bend left] (y2)
;
\end{tikzpicture}
\caption{A game, won entirely by player , and demonstrating
that a strategy defined by a greedy choice of vertex with the maximal 
tuple does not work. After lifting the vertices in order , we obtain the measure values as above. Player \odd would then choose , which leads to a losing play, whereas the choice of the other successor  yields a winning play for \odd.}
\label{fig:greedywrong}
\end{figure}


Propagating a top value only to vertices with less significant
priorities is, however, safe. This can be achieved efficiently by
a slightly modified attractor that works within a given context of vertices , 
which we call a \emph{guarded} attractor.
\begin{definition}
Let  be some priority and  some sets for which
. 
Then  is the least set  satisfying:
\begin{enumerate}
 \item 
 
 \item 
 \begin{enumerate}
  \item if  and , then 
  \item if  and , then 
 \end{enumerate}
\end{enumerate}
If , we drop the subscript  from the guarded attractor.
\end{definition}


\newcommand{\exttuples}[1]{\mathbb{M}^{#1,\textsf{ext}}}
\def\extprog{newProg}

 
\noindent
The theorem below forms the basis of our algorithm; it describes
the relevant information about an \odd-dominion that can be extracted
once the first vertex in the game is lifted to top.

\begin{restatable}{theorem}{thmcentraal}
 \label{thm:centraal}
  Let  be a parity game on which an arbitrary lifting sequence is applied, such that at some point a vertex  with  is the first vertex whose measure value becomes top. Let  be the temporary measure at that point. The following holds:
  
  \begin{itemize}
   \item if , then for every successor  of  with a maximal measure among  there is an \odd-dominion  containing  such that for all , . Moreover,  has a winning strategy that is closed on , and which is defined on  as , and on  as the strategy attracting towards ,
   \item if , then there is an \odd-dominion  containing  such that for all , . Moreover,  \odd has a winning strategy  that is closed on , and defined on  as the strategy attracting towards .  Note that in this case . 
   \end{itemize}     
     
 \end{restatable} 

\subsection{The Extended SPM Algorithm}

\def\res{\textsf{RES}}
\def\rem{\textsf{REM}}
\def\irr{\textsf{IRR}}
\def\dom{\textsf{DOM}}
\def\newspm{\textsf{SPM-Within}}


\begin{algorithm}[h!t]
\caption{Modified SPM with strategy derivation for player Odd}
\label{alg:newspm}
\begin{algorithmic}[1]
\Function{Solve}{}
\State \emph{\textbf{Input} }
\State \emph{\textbf{Output} Winning partition and strategies }
\State initialise  to an empty assignment
\State 
\State 
\State compute strategy  for player Even by picking min. successor w.r.t. 
\State \Return 

\State

\Procedure{\newspm}{}  


\While{} \label{line:outerwhile}
\While{w \in Ww \in W} \label{line:liftloop_start}
\State  for  such that 
\EndWhile \label{line:liftloop_end}
\State \textbf{if} {w \in W} \textbf{break} \textbf{end if}

\State \label{line:firsttop_v}  the (unique) vertex in  such that 
\State 
\State \label{line:firsttop_strat}  where  for which  for all 
\State 
\For{ \textbf{all} }
\State 
\State \textbf{if}  \textbf{then} assign  the strategy \emph{attracting to } \textbf{end if} 
\EndFor \label{line:defsigma_res_end}
\State 
\State \label{line:compute_irr} 
  \State \label{line:rem} 
  \State \label{line:reccall}   
  \State  \label{line:dominion_resolved}
\State 

  \For{ \textbf{all} }
  \State 
    \If{} 
    assign  to be the strategy  \emph{attracting} to  
    \EndIf
  \EndFor
\State \label{line:smallerW} 
\EndWhile
\EndProcedure
\EndFunction

\end{algorithmic}
\end{algorithm}

\newcommand{\genlift}[3]{\ensuremath{\text{lift}_{#1}(#2,#3)}}
\def\genliftname{\text{lift}_{W}}

Theorem \ref{thm:centraal} captures the core idea behind our algorithm.
It provides us with the means to locally resolve (\ie define a local
strategy for) at least one vertex in , once a top value
is found while lifting. Moreover, it indicates in which part of
the game the remainder of the \odd-dominion resides, implying
that one can temporarily restrict the lifting to that area until
the dominion is fully resolved. 
We will give a description of our solution (Algorithm \ref{alg:newspm}), and
informally argue the correctness of our approach.  For a (intricate and rather involved) formal proof, we
refer to~\cite{GW:14}.

The algorithm proceeds as follows. First, a standard SPM is run until the first vertex reaches top [l. \ref{line:liftloop_start}--\ref{line:liftloop_end} in Alg. \ref{alg:newspm}\,]. Whenever  is the first vertex lifted to top, then the issue of the winning strategy for  can be resolved immediately [l. \ref{line:firsttop_strat}\,], as well as for vertices in the guarded attractor of  (if there are any). We will denote this set of `resolved' vertices with . Moreover, we can restrict our search for the remainder of the \odd-dominion  only to vertices with priorities not more significant than , in fact only those from which player \even cannot attract a play to visit a priority more significant than . Hence we can remove from the current computation context the set , vertices that may be considered at the moment irrelevant [l. \ref{line:compute_irr}--\ref{line:rem}\,].

After discarding the resolved and currently irrelevant vertices, the algorithm proceeds in the remaining set of vertices that constitutes a proper subgame (i.e. without dead ends) induced by the set . After the subroutine returns [l. \ref{line:reccall}\,], all vertices labelled with top are won by player \odd in the subgame . In other words, those vertices are won by \odd provided that the play does not leave . Since the only way for player \even to escape from  is to visit , where every vertex is won by player \odd, the top-labelled vertices from  are in fact won by \odd in the context of the larger game . Therefore the set  computed in line \ref{line:dominion_resolved} constitutes an \odd-dominion within the game , in which we have moreover fully defined a winning strategy  for player \odd. Finally, every vertex from  that can be attracted by player \odd to the dominion  is certainly won by \odd, and for those vertices we assign the standard strategy attracting to . The \odd-dominion  is then removed, and the computation continues in the remaining subgame. 
\medskip

The algorithm may at first sight appear to deviate much from the
standard SPM algorithm. However, the additional overlay, apart from
defining the strategy, is no more than a special lifting policy that
temporarily restricts the lifting to parts where an \odd dominion
resides. \medskip

Every attractor computation takes 
time, and whenever it occurs, at least one new vertex is `resolved'.
Hence the total extra time introduced by the attractor computations
is bounded by . As with the standard SPM, the lifting
operations dominate the running time, and their total number for
every vertex is bounded by the size of .
\begin{theorem}
For a game  with  vertices,  edges, and
 priorities,  solves  and computes winning
strategies for player  and  in worst-case .
\end{theorem}

\section{Illustrating the new algorithm}
We illustrate the various aspects of Algorithm~\ref{alg:newspm} on
the game  depicted in Figure~\ref{fig:illustrating_example}, with two (overlapping) subgames
 and .
Note that the entire game is an -paradise: every vertex
eventually is assigned measure  by Algorithm~\ref{alg:newspm}
(and Algorithm~\ref{alg:spm}, for that matter). Suppose we use a
lifting strategy prioritising  and ; then vertex
's measure is the first to reach , and the successor
with maximal measure is . Therefore, 's strategy is to
move from  to .  The set , computed next consists of
vertices  and ; the strategy for  is set to 
and its measure is set to . The -attractor into those
vertices with priorities , \ie, vertices
 and , is exactly those vertices, so, next, the algorithm
zooms in on solving the subgame .
\begin{figure}[h!tbp]
\centering
\begin{tikzpicture}[>=stealth']
\tikzstyle{every node}=[draw, inner sep=0pt, outer sep=0pt, node distance=40pt];
  \node[draw=none,shape=diamond,minimum size=26]   (s1)                                 {};
  \node[draw=none,shape=rectangle,minimum size=22] (s2) [right of=s1,xshift=20pt]                   {};
  \node[draw=none,shape=rectangle,minimum size=22] (s3) [right of=s2,xshift=20pt]                   {};
  \node[draw=none,shape=diamond,minimum size=26]   (s4) [right of=s3,xshift=80pt]                   {};
  \node[draw=none,shape=rectangle,minimum size=22] (s5) [below of=s4]                   {};
  \node[draw=none,shape=diamond,minimum size=26]   (s6) [left of=s5,xshift=-20pt]                   {\scriptsize 5};
  \node[draw=none,shape=rectangle,minimum size=22] (s7) [left of=s6,xshift=-20pt]                   {};
  \node[draw=none,shape=diamond,minimum size=26]   (s8) [left of=s7,xshift=-20pt]                   {};
  \node[draw=none,shape=rectangle,minimum size=22] (s9) [left of=s8,xshift=-20pt]                   {};


\draw [fill=gray!15, thick,rounded corners=55pt, fill opacity=0.5] 
    () -- 
    () -- 
    () -- cycle
;
\draw 
    [fill=gray!10, thick,rounded corners=10pt, fill opacity=0.3] 
    () -- 
    () -- 
    () -- 
    () -- cycle;

\draw () node[draw=none] {};
\draw () node[draw=none] {};


  \node[label=above:{},shape=diamond,minimum size=26]   (y1)                                 {\scriptsize 0};
  \node[label=above:{},shape=rectangle,minimum size=22] (y2) [right of=y1,xshift=20pt]                   {\scriptsize 4};
  \node[label=above:{},shape=rectangle,minimum size=22] (y3) [right of=y2,xshift=20pt]                   {\scriptsize 3};
  \node[label=above:{},shape=diamond,minimum size=26]   (y4) [right of=y3,xshift=80pt]                   {\scriptsize 1};
  \node[label=below:{},shape=rectangle,minimum size=22] (y5) [below of=y4]                   {\scriptsize 4};
  \node[label=below:{},shape=diamond,minimum size=26]   (y6) [left of=y5,xshift=-20pt]                   {\scriptsize 5};
  \node[label=below:{},shape=rectangle,minimum size=22] (y7) [left of=y6,xshift=-20pt]                   {\scriptsize 5};
  \node[label=below:{},shape=diamond,minimum size=26]   (y8) [left of=y7,xshift=-20pt]                   {\scriptsize 6};
  \node[label=below:{},shape=rectangle,minimum size=22] (y9) [left of=y8,xshift=-20pt]                   {\scriptsize 4};

\path[->]
  (y1) edge[bend left] (y3)
  (y2) edge (y3) edge[bend left] (y8)
  (y3) edge (y7)
  (y4) edge (y3) edge[loop right] (y4)
  (y5) edge (y4) edge[bend left] (y6)
  (y6) edge[bend left] (y5) edge[bend left] (y7)
  (y7) edge [bend left] (y6) edge[bend left] (y8) edge[bend left] (y9)
  (y8) edge [bend left] (y2) edge[bend left] (y7)
  (y9) edge (y1) edge[loop left] (y9)
;

\end{tikzpicture}
\caption{An example game  with two (overlapping) subgames  and .}
\label{fig:illustrating_example}
\end{figure}

Suppose that within the latter subgame, we prioritise the lifting of
vertex  and ; then vertex 's measure is set to 
first, and 's successor with the largest measure is ;
therefore 's strategy is to move from  to . At this
point in the algorithm,  is assigned the set of vertices 
and , and the measure of  is set to . Note that in this
case, in this subgame, the winning strategy for  on  is to remain within
the set . Since all
remaining vertices have more signficant priorities than , or
are forced by  to move there, the next recursion is run on
an empty subgame and immediately returns without changing the
measures. Upon return, the -attractor to all -won
vertices (within the subgame , so these are only the 
vertices  and )
is computed, and the algorithm continues solving the remaining
subgame (\ie the game restricted to vertices  and ),
concluding that no vertex in this entire game will be assigned
measure .

At this point, the algorithm returns to the global game again and
computes the -attractor to the vertices won by player  at that stage
(\ie vertices  and ), adding vertices  and ,
setting their measure to  
and setting 's strategy for  to move to . 

As a final step, the algorithm next homes in on the subgame ,
again within the larger game.  The only vertex assigned measure
 in the above subgame is vertex ; at this point  is
assigned all vertices in , the measure of  and  is set to
 and the  strategy for vertex  is set to .
This effectively solves the entire game.



\section{Conclusions and Future Work}
In this paper, we studied the classical Small Progress Measures algorithm for solving
parity games.  The two key contributions of our work are as follows:
\begin{enumerate}
 \item We have proposed a more operational interpretation of progress measures by characterising the types of plays that players can enforce. 
 \item We have provided a modification of the SPM algorithm that allows to compute the winning strategies for both players in one pass, thus improving the worst-case running time of strategy derivation.
 \end{enumerate}
The second enhancement has been made possible due to a thorough study of the contents of progress measures, and their underapproximations in the intermediate stages of the algorithm (building on the proposed operational interpretation). \medskip

As for the future work, we would like to perform an analysis of
SPM's behaviour on special classes of games, along the same lines
as we have done in case of the recursive algorithm \cite{GW:13}.
More specifically, we would like to identify the games for which
SPM runs in polynomial time, and study enhancements that allow to
solve more types of games efficiently. It would also be interesting
to see whether the ideas behind our modification of the SPM algorithm
carry over to the algorithm for small energy progress
measures~\cite{BCDGR:11} for mean payoff games.

\bibliographystyle{eptcs}


\begin{thebibliography}{10}
\providecommand{\bibitemdeclare}[2]{}
\providecommand{\surnamestart}{}
\providecommand{\surnameend}{}
\providecommand{\urlprefix}{Available at }
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\href}[2]{\texttt{#2}}
\providecommand{\urlalt}[2]{\href{#1}{#2}}
\providecommand{\doi}[1]{doi:\urlalt{http://dx.doi.org/#1}{#1}}
\providecommand{\bibinfo}[2]{#2}

\bibitemdeclare{article}{AVW:03}
\bibitem{AVW:03}
\bibinfo{author}{A.~\surnamestart Arnold\surnameend},
  \bibinfo{author}{A.~\surnamestart Vincent\surnameend} \&
  \bibinfo{author}{I.~\surnamestart Walukiewicz\surnameend}
  (\bibinfo{year}{2003}): \emph{\bibinfo{title}{{Games for synthesis of
  controllers with partial observation}}}.
\newblock {\sl \bibinfo{journal}{TCS}}
  \bibinfo{volume}{303}(\bibinfo{number}{1}), pp. \bibinfo{pages}{7--34},
  \doi{10.1016/S0304-3975(02)00442-5}.

\bibitemdeclare{article}{BCDGR:11}
\bibitem{BCDGR:11}
\bibinfo{author}{L.~\surnamestart Brim\surnameend},
  \bibinfo{author}{J.~\surnamestart Chaloupka\surnameend},
  \bibinfo{author}{L.~\surnamestart Doyen\surnameend},
  \bibinfo{author}{R.~\surnamestart Gentilini\surnameend} \&
  \bibinfo{author}{J.{-}F. \surnamestart Raskin\surnameend}
  (\bibinfo{year}{2011}): \emph{\bibinfo{title}{Faster algorithms for
  mean-payoff games}}.
\newblock {\sl \bibinfo{journal}{Formal Methods in System Design}}
  \bibinfo{volume}{38}(\bibinfo{number}{2}), pp. \bibinfo{pages}{97--118},
  \doi{10.1007/s10703-010-0105-x}.

\bibitemdeclare{inproceedings}{EJ:91}
\bibitem{EJ:91}
\bibinfo{author}{E.A. \surnamestart Emerson\surnameend} \&
  \bibinfo{author}{C.S. \surnamestart Jutla\surnameend} (\bibinfo{year}{1991}):
  \emph{\bibinfo{title}{Tree automata, Mu-Calculus and determinacy}}.
\newblock In: {\sl \bibinfo{booktitle}{FOCS'91}}, \bibinfo{publisher}{IEEE
  Computer Society}, \bibinfo{address}{Washington, DC, USA}, pp.
  \bibinfo{pages}{368--377}, \doi{10.1109/SFCS.1991.185392}.

\bibitemdeclare{inproceedings}{EJS:93}
\bibitem{EJS:93}
\bibinfo{author}{E.A. \surnamestart Emerson\surnameend}, \bibinfo{author}{C.S.
  \surnamestart Jutla\surnameend} \& \bibinfo{author}{A.P. \surnamestart
  Sistla\surnameend} (\bibinfo{year}{1993}): \emph{\bibinfo{title}{On
  Model-Checking for Fragments of -Calculus}}.
\newblock In: {\sl \bibinfo{booktitle}{CAV}}, {\sl \bibinfo{series}{Lecture
  Notes in Computer Science}} \bibinfo{volume}{697},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{385--396},
  \doi{10.1007/3-540-56922-7\_32}.

\bibitemdeclare{inproceedings}{Fea:10}
\bibitem{Fea:10}
\bibinfo{author}{J.~\surnamestart Fearnley\surnameend} (\bibinfo{year}{2010}):
  \emph{\bibinfo{title}{Non-oblivious Strategy Improvement}}.
\newblock In: {\sl \bibinfo{booktitle}{LPAR-16}}, {\sl \bibinfo{series}{Lecture
  Notes in Computer Science}} \bibinfo{volume}{6355},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{212--230},
  \doi{10.1007/978-3-642-17511-4\_13}.

\bibitemdeclare{article}{Fri:11}
\bibitem{Fri:11}
\bibinfo{author}{O.~\surnamestart Friedmann\surnameend} (\bibinfo{year}{2011}):
  \emph{\bibinfo{title}{Recursive algorithm for parity games requires
  exponential time}}.
\newblock {\sl \bibinfo{journal}{RAIRO -- Theor. Inf. and Applic.}}
  \bibinfo{volume}{45}(\bibinfo{number}{4}), pp. \bibinfo{pages}{449--457},
  \doi{10.1051/ita/2011124}.

\bibitemdeclare{inproceedings}{FL:09}
\bibitem{FL:09}
\bibinfo{author}{O.~\surnamestart Friedmann\surnameend} \&
  \bibinfo{author}{M.~\surnamestart Lange\surnameend} (\bibinfo{year}{2009}):
  \emph{\bibinfo{title}{Solving Parity Games in Practice}}.
\newblock In: {\sl \bibinfo{booktitle}{ATVA}}, {\sl \bibinfo{series}{Lecture
  Notes in Computer Science}} \bibinfo{volume}{5799},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{182--196},
  \doi{10.1007/978-3-642-04761-9\_15}.

\bibitemdeclare{inproceedings}{GW:13}
\bibitem{GW:13}
\bibinfo{author}{M.~\surnamestart Gazda\surnameend} \& \bibinfo{author}{T.A.C.
  \surnamestart Willemse\surnameend} (\bibinfo{year}{2013}):
  \emph{\bibinfo{title}{Zielonka's Recursive Algorithm: dull, weak and
  solitaire games and tighter bounds}}.
\newblock In: {\sl \bibinfo{booktitle}{GandALF}}, {\sl
  \bibinfo{series}{{EPTCS}}} \bibinfo{volume}{119}, pp. \bibinfo{pages}{7--20},
  \doi{10.4204/EPTCS.119.4}.

\bibitemdeclare{misc}{GW:14}
\bibitem{GW:14}
\bibinfo{author}{M.~\surnamestart Gazda\surnameend} \& \bibinfo{author}{T.A.C.
  \surnamestart Willemse\surnameend} (\bibinfo{year}{2014}):
  \emph{\bibinfo{title}{Strategy Derivation for Small Progress Measures}}.
\newblock \bibinfo{note}{\url{http://arxiv.org/abs/1407.2149}}.

\bibitemdeclare{proceedings}{2001automata}
\bibitem{2001automata}
\bibinfo{editor}{E.~\surnamestart Gr{\"a}del\surnameend},
  \bibinfo{editor}{W.~\surnamestart Thomas\surnameend} \&
  \bibinfo{editor}{T.~\surnamestart Wilke\surnameend}, editors
  (\bibinfo{year}{2002}): \emph{\bibinfo{title}{Automata, Logics, and Infinite
  Games: A Guide to Current Research}}. {\sl \bibinfo{series}{Lecture Notes in
  Computer Science}} \bibinfo{volume}{2500}, \bibinfo{publisher}{Springer}.

\bibitemdeclare{incollection}{Jur:00}
\bibitem{Jur:00}
\bibinfo{author}{M.~\surnamestart Jurdzi{\'n}ski\surnameend}
  (\bibinfo{year}{2000}): \emph{\bibinfo{title}{Small Progress Measures for
  Solving Parity Games}}.
\newblock In: {\sl \bibinfo{booktitle}{STACS'00}}, {\sl
  \bibinfo{series}{Lecture Notes in Computer Science}} \bibinfo{volume}{1770},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{290--301},
  \doi{10.1007/3-540-46541-3\_24}.

\bibitemdeclare{inproceedings}{JPZ:06}
\bibitem{JPZ:06}
\bibinfo{author}{M.~\surnamestart Jurdzi\'nski\surnameend},
  \bibinfo{author}{M.~\surnamestart Paterson\surnameend} \&
  \bibinfo{author}{U.~\surnamestart Zwick\surnameend} (\bibinfo{year}{2006}):
  \emph{\bibinfo{title}{{A Deterministic Subexponential Algorithm for Solving
  Parity Games}}}.
\newblock In: {\sl \bibinfo{booktitle}{SODA'06}},
  \bibinfo{publisher}{ACM/SIAM}, pp. \bibinfo{pages}{117--123},
  \doi{10.1145/1109557.1109571}.

\bibitemdeclare{incollection}{Klau:01}
\bibitem{Klau:01}
\bibinfo{author}{H.~\surnamestart Klauck\surnameend} (\bibinfo{year}{2001}):
  \emph{\bibinfo{title}{Algorithms for Parity Games}}.
\newblock In: {\sl \bibinfo{booktitle}{Automata, Logics, and Infinite Games:
  {A} Guide to Current Research}}, chapter~\bibinfo{chapter}{7}, {\sl
  \bibinfo{series}{Lecture Notes in Computer Science}} \bibinfo{volume}{2500},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{107--129},
  \doi{10.1007/3-540-36387-4\_7}.

\bibitemdeclare{article}{McN:93}
\bibitem{McN:93}
\bibinfo{author}{R.~\surnamestart McNaughton\surnameend}
  (\bibinfo{year}{1993}): \emph{\bibinfo{title}{Infinite games played on finite
  graphs}}.
\newblock {\sl \bibinfo{journal}{APAL}}
  \bibinfo{volume}{65}(\bibinfo{number}{2}), pp. \bibinfo{pages}{149--184},
  \doi{10.1016/0168-0072(93)90036-D}.

\bibitemdeclare{inproceedings}{Sch:07}
\bibitem{Sch:07}
\bibinfo{author}{S.~\surnamestart Schewe\surnameend} (\bibinfo{year}{2007}):
  \emph{\bibinfo{title}{Solving Parity Games in Big Steps}}.
\newblock In: {\sl \bibinfo{booktitle}{FSTTCS'07}}, {\sl
  \bibinfo{series}{Lecture Notes in Computer Science}} \bibinfo{volume}{4855},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{449--460},
  \doi{10.1007/978-3-540-77050-3\_37}.

\bibitemdeclare{inproceedings}{Sch:08}
\bibitem{Sch:08}
\bibinfo{author}{S.~\surnamestart Schewe\surnameend} (\bibinfo{year}{2008}):
  \emph{\bibinfo{title}{An Optimal Strategy Improvement Algorithm for Solving
  Parity and Payoff Games}}.
\newblock In: {\sl \bibinfo{booktitle}{CSL}}, {\sl \bibinfo{series}{Lecture
  Notes in Computer Science}} \bibinfo{volume}{5213},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{369--384},
  \doi{10.1007/978-3-540-87531-4\_27}.

\bibitemdeclare{inproceedings}{STV:15}
\bibitem{STV:15}
\bibinfo{author}{S.~\surnamestart Schewe\surnameend},
  \bibinfo{author}{A.~\surnamestart Trivedi\surnameend} \&
  \bibinfo{author}{T.~\surnamestart Varghese\surnameend}
  (\bibinfo{year}{2015}): \emph{\bibinfo{title}{Symmetric Strategy
  Improvement}}.
\newblock In: {\sl \bibinfo{booktitle}{ICALP}}, {\sl \bibinfo{series}{Lecture
  Notes in Computer Science}} \bibinfo{volume}{9135},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{388--400},
  \doi{10.1007/978-3-662-47666-6\_31}.

\bibitemdeclare{inproceedings}{SS:98}
\bibitem{SS:98}
\bibinfo{author}{P.~\surnamestart Stevens\surnameend} \&
  \bibinfo{author}{C.~\surnamestart Stirling\surnameend}
  (\bibinfo{year}{1998}): \emph{\bibinfo{title}{Practical Model Checking Using
  Games}}.
\newblock In: {\sl \bibinfo{booktitle}{TACAS'98}}, {\sl
  \bibinfo{series}{Lecture Notes in Computer Science}} \bibinfo{volume}{1384},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{85--101},
  \doi{10.1007/BFb0054166}.

\bibitemdeclare{inproceedings}{VJ:00}
\bibitem{VJ:00}
\bibinfo{author}{J.~\surnamestart V{\"{o}}ge\surnameend} \&
  \bibinfo{author}{M.~\surnamestart Jurdzi\'{n}ski\surnameend}
  (\bibinfo{year}{2000}): \emph{\bibinfo{title}{A Discrete Strategy Improvement
  Algorithm for Solving Parity Games}}.
\newblock In: {\sl \bibinfo{booktitle}{CAV}}, {\sl \bibinfo{series}{Lecture
  Notes in Computer Science}} \bibinfo{volume}{1855},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{202--215},
  \doi{10.1007/10722167\_18}.

\bibitemdeclare{article}{Zie:98}
\bibitem{Zie:98}
\bibinfo{author}{W.~\surnamestart Zielonka\surnameend} (\bibinfo{year}{1998}):
  \emph{\bibinfo{title}{Infinite games on finitely coloured graphs with
  applications to automata on infinite trees}}.
\newblock {\sl \bibinfo{journal}{TCS}}
  \bibinfo{volume}{200}(\bibinfo{number}{1-2}), pp. \bibinfo{pages}{135 --
  183}, \doi{10.1016/S0304-3975(98)00009-7}.

\end{thebibliography}

\newpage

\appendix

 
\end{document}
