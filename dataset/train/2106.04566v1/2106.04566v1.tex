\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[preprint]{neurips_2021}


\usepackage[utf8]{inputenc}                               \usepackage[T1]{fontenc}                                  \usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref} \usepackage{booktabs,multirow,adjustbox}                  \usepackage{amsmath,amssymb,amsfonts,dsfont,bm,bbm,mathrsfs,pifont}  \usepackage{algorithm,algorithmic}                        \usepackage{nicefrac}                                     \usepackage{microtype}                                    \usepackage{xcolor}                                       \hyphenpenalty=1200


\newcommand{\E}{\mathbb{E}}      \newcommand{\z}{{\rm\bf z}}      \newcommand{\Z}{\mathcal{Z}}     \newcommand{\x}{{\rm\bf x}}      \newcommand{\X}{\mathcal{X}}     \renewcommand{\v}{{\rm\bf v}}    \newcommand{\T}{{\mathcal{T}}}   \renewcommand{\L}{{\mathcal{L}}} \newcommand{\C}{{\mathcal{C}}}   


\title{Data-Efficient Instance Generation \\ from Instance Discrimination}


\author{
  Ceyuan Yang$^\dagger$ \quad
  Yujun Shen$^\ddagger$ \quad
  Yinghao Xu$^\dagger$ \quad
  Bolei Zhou$^\dagger$ \\
  $^\dagger$The Chinese University of Hong Kong \quad
  $^\ddagger$ByteDance Inc. \\
}


\begin{document}


\maketitle


\begin{abstract}
Generative Adversarial Networks (GANs) have significantly advanced image synthesis, however, the synthesis quality drops significantly given a limited amount of training data.
To improve the data efficiency of GAN training, prior work typically employs data augmentation to mitigate the overfitting of the discriminator yet still learn the discriminator with a bi-classification (\textit{i.e.}, real \textit{vs.} fake) task.
In this work, we propose a data-efficient Instance Generation (\textit{InsGen}) method based on instance discrimination.
Concretely, besides differentiating the real domain from the fake domain, the discriminator is required to distinguish every individual image, no matter it comes from the training set or from the generator.
In this way, the discriminator can benefit from the infinite synthesized samples for training, alleviating the overfitting problem caused by insufficient training data.
A noise perturbation strategy is further introduced to improve its discriminative power.
Meanwhile, the learned instance discrimination capability from the discriminator is in turn exploited to encourage the generator for diverse generation.
Extensive experiments demonstrate the effectiveness of our method on a variety of datasets and training settings.
Noticeably, on the setting of $2K$ training images from the FFHQ dataset, we outperform the state-of-the-art approach with 23.5\% FID improvement.\footnote{Code and models are available at \url{https://genforce.github.io/insgen}.}
\end{abstract}


\section{Introduction}\label{sec:intro}


Generative Adversarial Network (GAN)~\cite{goodfellow2014generative} has become a popular paradigm to learn the distribution of the observed data.
It is formulated as a two-player game, where a generator synthesizes realistic data, while a discriminator distinguishes synthesized samples from real ones.
To reach equilibrium in this minimax game, it requires both the generator and the discriminator to be sufficiently trained.
In other words, the synthesis quality of the generator will be largely determined by the discriminator as an adaptive loss function~\cite{karras2020training, tran2021data, zhao2020differentiable, zhao2020image}.


Recent success of GANs~\cite{karras2017progressive, karras2019style, Karras2019stylegan2, brock2018large} relies on big data to assure the sufficient training of the discriminator.
Prior work~\cite{zhao2020differentiable, karras2020training} has found that reducing the amount of training data leads to the overfitting of the discriminator, which tends to memorize the entire training set.
In turn, the back-propagation from the discriminator to the generator deteriorates the synthesis quality of the generator and potentially causes the mode collapse problem~\cite{arjovsky2017towards, zhang2018pa}.
Data augmentation is one of the most widely used methods to alleviate the overfitting issue in training deep neural networks~\cite{zhang2017mixup,cubuk2020randaugment,cubuk2018autoaugment}.
Some recent attempts~\cite{karras2020training, tran2021data, zhao2020differentiable, zhao2020image, zhang2018pa} have been made to apply data augmentation to GAN training.
It is found that the discriminator can be improved by augmenting not only the real images from the dataset but also the synthesized images by the generator~\cite{zhao2020differentiable, karras2020training}.
However, the learning objective of the discriminator remains as categorizing real and fake domains and a substantial performance drop can be observed given limited training data.


The real-fake domain classification task could be too simple for the discriminator to gain sufficient discriminative power as an adaptive loss to train the generator, especially when the size of training set is small.
In this work, we propose to improve the data efficiency in GAN training by assigning a more challenging task to the discriminator, which is to distinguish every image instance as an independent category.
In this way, the discriminator is forced to improve its discriminative capability to accomplish the instance discrimination task.
Notably, besides distinguishing real samples, we also demand the discriminator to differentiate fake samples synthesized by the generator.
Thus the discriminator can be considered to train with infinite data, preventing it from memorizing the training samples.
When distinguishing synthesized data, we design a noise perturbation strategy to increase the difficulty of the task and hence make the discriminator more capable.
Meanwhile, we also alter the training objectives from the generator side.
Concretely, besides making the generator to fool the discriminator, we expect all the samples produced by the generator to be well identified as different instances with our instance-induced discriminator.
This matches the goal of diverse generation, which requires every synthesis to be as different as possible.
We evaluate our method on a range of datasets and it achieves appealing generation result in terms of image quality, diversity, and data efficiency.
Experiments show that our method significantly improves the baselines and outperform previous data-augmentation methods.
To be specific, our method improves the FID from 15.60 to 11.92, 7.29 to 4.90, and 3.88 to 3.31 with $2K$, $10K$, and $70K$ training images from FFHQ~\cite{karras2019style} respectively.
We can even learn a large-scale GAN with only 100 in-the-wild images to produce satisfying synthesis.


Our main \textbf{contributions} are summarized as follows:
1) We propose a data-efficient instance generation (\textit{InsGen}) method which incorporates instance discrimination as an auxiliary task in GAN training.
2) The synthesized data is used as infinite samples for improving the discriminative power of the discriminator, which in turn substantially improves the synthesis quality and diversity of the generator.
3) Under various data-regime settings, our method consistently surpasses existing alternatives by a substantial margin.


\section{Related Work}\label{sec:related}


\textbf{Data Augmentation in GANs.}
Data augmentation makes the maximum use of available data to alleviate the overfitting of deep models that have millions of parameters.
It plays an essential role in training discriminative models~\cite{zhang2017mixup,cubuk2020randaugment,cubuk2018autoaugment}.
Some recent work explores how data augmentation can help the training of GANs~\cite{zhao2020image, tran2021data, zhao2020differentiable, karras2020training}.
\citet{zhao2020image} conduct empirical studies on the effects of different types of augmentations for GAN training.
\citet{tran2021data} make a theoretical analysis of several data augmentations.
\citet{zhao2020differentiable} propose a differentiable augmentation method such that the augmenting operations can be applied to both real and synthesized data.
Similarly, \citet{karras2020training} design augmentations that do not leak and introduce a probability-based adaptive strategy to stabilize the training process.
Different from prior work, we focus on introducing the unsupervised representation learning which also requires augmentations into GAN training.
Our work shows that the recent instance discrimination task~\cite{wu2018unsupervised} can be used as an auxiliary task for the discriminator to learn more discriminative representations with limited training data, which in turn substantially improves the synthesis quality of the generator.


\textbf{Self-supervised Learning in GANs.}
The rationale behind self-supervised learning is to set up various pretext tasks with supervisory-free labels~\cite{donahue2019large, chen2020generative, xu2021generative, zhang2016colorful,doersch2015unsupervised, oord2018representation, pathak2016context, yang2021insloc, pathak2016context, gidaris2018unsupervised, noroozi2016unsupervised, pathak2017learning}.
Similar idea is recently introduced in GAN training as an auxiliary loss to improve the synthesis performance.
For example, \citet{chen2019self} assign the rotation prediction task to the discriminator to prevent it from catastrophic forgetting, and \citet{tran2019self} propose a multi-class minimax game to encourage the generator to produce diverse samples.
Among all self-supervised learning approaches, contrastive learning~\cite{wu2018unsupervised, he2019momentum, chen2020simple, henaff2020data, bachman2019learning}, which aims at distinguishing instances, shows great potential in large-scale representation learning.
Many attempts have been made to improve generative models by drawing lessons from contrastive learning, like the consistency regularization for GANs~\cite{zhang2019consistency, zhao2020improved}, the patch-level contrastive learning for image-to-image translation~\cite{park2020contrastive}, and the latent-augmented contrastive loss for conditional image synthesis~\cite{liu2021divco}.
Akin to supervised contrastive loss~\cite{khosla2020supervised}, some concurrent work~\cite{jeong2021training, kang2020contragan, yu2021dual} reformulates the conventional bi-classification task (\textit{i.e.}, real domain \textit{vs.} fake domain) with contrastive loss.
By contrast, we assign the discriminator a simple auxiliary task, which is to \textit{recognize every image instance}, no matter it is real or synthesized by the generator.
Such instance discrimination task helps sustain the discriminative power of the discriminator under a low-data regime, which in turn improves the synthesis performance substantially.


\section{Methodology}\label{sec:method}


\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/framework1.pdf}
	\vspace{-15pt}
	\caption{
	    Illustration of the \textit{InsGen} method.
Besides the bi-classification task to differentiate real and fake domains, the discriminator is assigned an auxiliary task, which aims at maximally distinguishing each image instance as illustrated on the right.
$\C$ denotes the training objective for such instance discrimination task.
(a) The discriminator is asked to recognize not only every real sample $\x_i$ but also every synthesized sample $G(\z_i)$ by a frozen generator.
(b) With the instance-induced discriminator, the generator is encouraged to make all synthesis recognizable from each other, leading to more diverse generation.
	}
    \label{fig:framework}
    \vspace{0pt}
\end{figure}


In this section, we introduce the proposed InsGen method.
Recall that our method is built based on GAN, which is commonly formulated as a two-player game between a generator and a discriminator.
They compete with each other in that the generator tries to produce as realistic data as possible while the discriminator works on recognizing synthesized data from real data.
Besides the conventional bi-classification task (\textit{i.e.}, differentiating real and fake domains), we also require the discriminator to \textit{distinguish every image instance}.
With such a challenging task, the discriminator can mitigate the overfitting problem even with limited training data.
We will briefly introduce the image synthesis and instance discrimination mechanisms in Sec.~\ref{subsec:preliminary}, followed by our improved training pipeline in Sec.~\ref{subsec:insgen} and the practical usage of InsGen on the state-of-the-art StyleGAN2-ADA model~\cite{karras2020training} in Sec.~\ref{subsec:usage}.


\subsection{Preliminaries}\label{subsec:preliminary}


\textbf{Synthesizing Images with GANs.}
GAN is a popular paradigm for image generation~\cite{goodfellow2014generative}.
It typically consists of two networks: a generator $G(\cdot)$ that learns to map a latent variable $\z$ to a photo-realistic image, and a discriminator $D(\cdot)$ that aims at separating real images $\x$ from synthesized ones $G(\z)$.
These two networks compete with each other and are jointly optimized with
\begin{align}
    \L_D & = - \E_{\x\in\X}[\log(D(\x))] - \E_{\z\in\Z}[\log(1-D(G(\z)))], \label{eqa:loss_d} \\
    \L_G & = - \E_{\z\in\Z}[\log(D(G(\z)))], \label{eqa:loss_g}
\end{align}
where $\Z$ and $\X$ denote the pre-defined latent distribution and real data distribution respectively.
After the training converges, the synthesized images should be as realistic as real ones to fool the discriminator.
From this perspective, the synthesis quality highly depends on the discriminative power of the discriminator.
Prior literature~\cite{karras2020training, tran2021data, zhao2020differentiable, zhao2020image} has affirmed that GANs will suffer from the insufficient training of the discriminator and proposed to apply a series of data augmentations $\T(\cdot)$ to alleviate the overfitting problem.
But they do not change the learning objectives of GAN and observe drastic performance drop given limited training data.


\textbf{Distinguishing Images with Contrastive Learning.}
It is well-known that image classification tasks usually benefit from more discriminative representations~\cite{deng2009imagenet}.
Unlike supervised training algorithms that optimize the model parameters based on annotated data, contrastive learning~\cite{wu2018unsupervised, he2019momentum, chen2020simple, henaff2020data, bachman2019learning} is able to extract representative features from images in an unsupervised manner.
As shown in Fig.~\ref{fig:framework}a, the rationale behind is to ``label'' every sample as an individual class, \textit{i.e.}, instance discrimination.
Concretely, given an image $\x$, two random ``views'' (\textit{e.g.}, through different augmentations) are created as the query $\x_q$ and the key $\x_{k_+}$.
This query-key pair is regarded as the positive pair while all ``views'' from other images, $\{\x_{k_i}\}_{i=1}^{N}$, are treated as negative pairs with respect to the query.
Here, $N$ is the total number of images in addition to the query image.
Contrastive learning aims at maximizing the agreement across augmentations (\textit{i.e.}, $\x_q$ and $\x_{k_+}$) and make the query as much dissimilar to a number of negative samples as possible.
Accordingly, we can design a pretext task of $(N+1)$-way classification and learn the model with the contrastive loss $\C$, \textit{i.e.}, InfoNCE loss~\cite{oord2018representation}
\begin{gather}
    \v_q = F(\x_q), \quad \v_{k_+} = F(\x_{k_+}), \quad \v_{k_i} = F(\x_{k_i}),\ i = 1 \dots N, \label{eqd:representation} \\
    \C_{F(\cdot),\phi(\cdot)}(\x_q, \x_{k_+}, \{\x_{k_i}\}_{i=1}^{N}) = -\log \frac{\exp(\phi(\v_q)^T\phi(\v_{k_+}) / \tau)}{\sum_{i=0}^{N}\exp(\phi(\v_q)^T\phi(\v_{k_i}) / \tau)}, \label{eqa:cl}
\end{gather}
where $F(\cdot)$ is the backbone network to extract the representation $\v$ from a given image $\x$, and $\phi(\cdot)$ is the head network (\textit{e.g.}, usually implemented with several fully-connected layers) to project the extracted feature onto a unit sphere.
$\tau$ stands for the temperature, which is a hyper-parameter.
Recall that the primitive goal of the discriminator in GANs can also be viewed as a bi-classification task, which is to recognize real and fake domains.
In this work, we demonstrate that introducing the instance discrimination task can help enhance the discriminative power of the discriminator and in turn improve the synthesis quality of the generator significantly.


\subsection{Generating Diverse Instances from Distinguishing Instances}\label{subsec:insgen}


In this part, we will introduce how instance discrimination~\cite{wu2018unsupervised, he2019momentum} is incorporated into the GAN training for data-efficient and diverse image generation. There are four essential components of the InsGen method: 1) distinguishing real images, 2) distinguishing fake images that can be sampled infinitely, 3) a noise perturbation strategy, and 4) a loop-back mechanism to encourage the generator for the diverse generation.


\textbf{Distinguishing Real Images.}
As discussed above, the synthesis quality of GAN models not only depends on the training scheme~\cite{arjovsky2017wasserstein, miyato2018spectral, karras2017progressive, brock2018large} and the architecture design of the generator~\cite{zhang2019self, karras2019style, Karras2019stylegan2}, but more importantly it relies on the discriminative capability of the discriminator.
That is because the discriminator is the one that sees how real data looks like and further guides the training of generator accordingly, while the generator doesn't have the direct access to the real data.
To make the maximum use of the limited training data and avoid the discriminator from memorizing the entire dataset, we assign it with a more challenging task beyond domain classification, which is to recognize every independent instance from the dataset, as shown in Fig.~\ref{fig:framework}a.
For this purpose, we introduce a new task head $\phi^r(\cdot)$ beyond the original domain classification head $\phi^{domain}(\cdot)$ on top of its backbone $d(\cdot)$\footnote{The conventional discriminator is a composition of $d(\cdot)$ and $\phi^{domain}(\cdot)$ to perform real/fake classification, \textit{i.e.}, $D(\cdot) = \phi^{domain}(\cdot) \circ d(\cdot)$.}
and train the discriminator with an extra training objective
\begin{align}
    \C^r_D = \C_{d(\cdot), \phi^r(\cdot)}(\T_q(\x_q), \T_{k_+}(\x_q), \{\T_{k_i}(\x_{k_i})\}_{i=1}^{N}). \label{eqa:real_cl}
\end{align}
Here, $\x_q, \{\x_{k_i}\}_{i=1}^{N}$ are all sampled from the real data distribution $\X$ and transformed with various differentiable augmentations $\T(\cdot)$.


\textbf{Distinguishing Fake Images.}
However, the amount of training data could be extremely few (like thousands or even hundreds) in practice.
In such a case, the improvement of the discriminator gained by differentiating real instances will be also limited.
On the other hand, we notice that the number of synthesized samples can be sufficiently large due to the sampling mechanism of GANs.
Ideally, different latent codes $\z\in\Z$ should lead to different synthesis $G(\z)$.
Hence, we propose to also ask the discriminator to recognize every individual fake images, as shown in Fig.~\ref{fig:framework}a.
Similarly, we introduce another task head $\phi^f(\cdot)$ into the discriminator.
It is worth mentioning that we use separate task heads (\textit{i.e.}, $\phi^r(\cdot)$ and $\phi^f(\cdot)$) for real and fake data.
That is because even though the synthesized images can be with high-quality, they still lie in a different distribution from the real ones, especially when the generator starts training from scratch.
Meanwhile, the task of discriminating a real instance from a fake instance can be achieved by the native domain classification head $\phi^{domain}(\cdot)$.


\textbf{Noise Perturbation.}
Prior work has observed the continuity of the latent space~\cite{radford2015unsupervised} such that images synthesized from the latent codes within a neighbourhood are very close to each other.
Accordingly, they are more suitable to be treated as positive pairs than negative pairs.
From this perspective, we introduce a noise perturbation strategy into fake image discrimination.
The objective becomes
\begin{gather}
    \x'_q = \T_q(G(\z_q)), \quad \x'_{k_+} = \T_{k_+}(G(\z_q+\epsilon)), \quad x'_{k_i} = \T_{k_i}(G(\z_{k_i})), \\
    \C^f_D = \C_{d(\cdot), \phi^f(\cdot)}(\x'_q, \x'_{k_+}, \{\x'_{k_i}\}_{i=1}^{N}). \label{eqa:fake_cl}
\end{gather}
Concretely, given a query image $\x'_q$, the key image $\x'_{k_+}$ is created with $\T_{k_+}(G(\z_q+\epsilon))$ instead of $\\T_{k_+}(G(\z_q))$.
Here, $\epsilon$ stands for the perturbation term, which is sampled from a Gaussian distribution whose variance is sufficiently smaller than that of $\Z$,
and $\T_q(\cdot)$ and $\T_{k_+}(\cdot)$ denote two different augmentations.
Such design aims to enforce the discriminator invariant to the small perturbation, which makes the instance discrimination task more challenging.


\textbf{Toward Diverse Generation.}
Besides utilizing the instance discrimination task to improve the discriminative power of the discriminator, we further design a loop-back mechanism to in turn use the learned instance discrimination to guide the generator.
Recall that image diversity, in addition to image quality, is also an important metric to evaluate generative models.
Diverse generation, which requires all generated samples to be distinguishable from each other, exactly matches our goal of instance discrimination.
In other words, given a discriminator with the ability to distinguish different instances, we would like all the samples produced by the generator to be recognized as different ones.
This idea is illustrated in Fig.~\ref{fig:framework}b.
By comparing Fig.~\ref{fig:framework}a and Fig.~\ref{fig:framework}b, we can see that the generator shares the same target as the discriminator yet is trained separately.
Hence, the same objective function is added into the generator loss
\begin{gather}
     \x''_{k_+} = \T_{k_+}(G(\z_q)), \\
    \C^f_G=\C_{d(\cdot), \phi^f(\cdot)}(\x'_q, \x''_{k_+}, \{\x'_{k_i}\}_{i=1}^{N}), \label{eqa:fake_cl_on_g}
\end{gather}
where the only difference is that noise perturbation is not applied during the training of the generator.


\textbf{Complete Objective Function.}
To summarize, with the purposes of both image synthesis and instance discrimination, the discriminator and the generator in InsGen are optimized with
\begin{align}
    \L^{\prime}_D & = \L_D + \lambda^r_D \C^r_D + \lambda^f_D \C^f_D, \label{eqa:new_loss_d} \\
    \L^{\prime}_G & = \L_G + \lambda_G \C^f_G, \label{eqa:new_loss_g}
\end{align}
where $\lambda_G$, $\lambda^r_D$, and $\lambda^f_D$ denote the weights for different terms.


\subsection{Implementation}\label{subsec:usage}


On top of the adversarial training pipeline in GANs, our InsGen method only inserts an extra loss output on the discriminator network for instance discrimination.
Therefore, it can be easily implemented on any GAN framework.
In this part, we take the state-of-the-art GAN model, StyleGAN2-ADA~\cite{karras2020training}, as an example to demonstrate how InsGen is implemented in practice.


\textbf{Generative Model.}
StyleGAN2-ADA~\cite{karras2020training} adopts the architecture of StyleGAN2~\cite{Karras2019stylegan2} and proposes the adaptive discriminator augmentation strategy for training with limited data.
In particular, it designs a differentiable augmentation pipeline, consisting of 18 transformations, as well as an adaptive hyper-parameter to control the strength of these augmentations.
For a fair comparison, in this work, we exactly reuse the network structure, the augmentation pipeline, the adaptive strategy of the augmenting strength, and other hyper-parameters like batch size and learning rate.


\textbf{Instance Discrimination.}
We reuse the backbone of the discriminator to perform instance discrimination, so that the extra computing load is extremely small and the training efficiency is barely affected.
We treat the last fully-connected layer in the StyleGAN2-ADA discriminator as the domain-classification head $\phi^{domain}(\cdot)$, while all remaining layers serve as the backbone network $d(\cdot)$.
The real instance discrimination head $\phi^r(\cdot)$ and the fake head $\phi^f(\cdot)$ are both implemented with 2 fully-connected layers, followed by $\ell_2$ normalization.
Strictly following MoCo-v2~\cite{chen2020mocov2}, an extra queue is employed for each task head to store the sample features to save computational cost.
The number of samples in $\L^{r}_{D}$ and $\L^{f}_{D}$ is thus equal to the queue size, which usually contains around 5\% data of the whole set.
We also introduce the momentum encoder $D^{\prime}$, whose parameters are updated with moving average scheme: $\Theta_{D^\prime} \gets \alpha \Theta_{D^\prime} + (1 - \alpha) \Theta_D$.
Here, $\alpha=0.999$ follows the same setting in MoCo-v2~\cite{chen2020mocov2}.
The temperature $\tau$ in Eq.~\eqref{eqa:cl} is set as 2.


\section{Experiments}\label{sec:exp}


We evaluate the proposed InsGen method on multiple benchmarks.  Sec.~\ref{subsec:sota} presents the comparison to prior literature on both FFHQ~\cite{karras2019style} and AFHQ~\cite{choi2020starganv2} datasets. Our InsGen substantially improves the baselines under multiple data-regime settings and outperforms previous data-augmentation approaches by a significant margin. Moreover, Sec.~\ref{subsec:ablation} provides a detailed ablation study to show the importance of each component. Lastly Sec.~\ref{subsec:limitation} discusses about the limitation of data-efficiency.


\definecolor{azure}{rgb}{0.0, 0.5, 1.0}
\setlength{\tabcolsep}{15pt}
\begin{table}[t]
    \centering
    \caption{
        \textbf{Performance on FFHQ.}
FID (lower is better) is reported as the evaluation metric.
``$2K$'', ``$10K$'', and ``$140K$'' stand for the number of samples used for training, where ``$140K$'' horizontally flips the     original FFHQ dataset (with $70K$ samples) to double the size of data.
Results with $*$ are also achieved with horizontally flipped data, which are slightly better than those reported     in~\cite{karras2020training}.
Numbers in \textbf{\textcolor{azure}{blue}} color indicate our improvements over the baseline~\cite{karras2020training}.
    }
    \label{table:sota-ffhq}
    \vspace{0pt}
    \begin{tabular}{llll}
        \toprule
        256$\times$256 Resolution                     & $2K$    &  $10K$ & $140K$ \\
        \midrule
        PA-GAN~\cite{zhang2018pa}                     & 56.49  & 27.71  & 3.78 \\
        zCR~\cite{zhao2020improved}                   & 71.61  & 23.02  & 3.45 \\
        Auxiliary rotation~\cite{chen2019self}        & 66.64  & 25.37  & 4.16 \\
        \midrule
        StyleGAN2~\cite{karras2019style}              & 78.80  & 30.73  & 3.66 \\
        w/ Shallow mapping~\cite{karras2020training}  & 71.35  & 27.71  & 3.59 \\
        w/ Adaptive dropout~\cite{karras2020training} & 67.23  & 23.33  & 4.16 \\
        w/ DiffAugment~\cite{zhao2020differentiable}  & 24.32  & 7.86   & -    \\
        w/ ADA~\cite{karras2020training}              & 15.60$^*$  & 7.29$^*$   & 3.88 \\
        \midrule
        \textbf{InsGen} (Ours) & \textbf{11.92 \textcolor{azure}{($-$3.68)}}  & \textbf{4.90 \textcolor{azure}{($-$2.39)}}  & \textbf{3.31 \textcolor{azure}{($-$0.57)}} \\
        \bottomrule
    \end{tabular}
    \vspace{-10pt}
\end{table}


\subsection{Main Results}\label{subsec:sota}


\noindent\textbf{Datasets.}
We evaluate our InsGen with a number of other approaches on FFHQ~\cite{karras2019style} and AFHQ~\cite{choi2020starganv2} datasets. FFHQ contains unique 70,000 high-resolution images (1024$\times$1024), with large variation regarding age, ethnicity, and background. All images of FFHQ are well aligned~\cite{kazemi2014one} and cropped. In order to conduct a fair comparison, we resize images to 256$\times$256. For the experiments of limited data, we follow ADA~\cite{karras2020training} to collect a subset of training data by randomly sampling. Moreover, AFHQ consists of around 5000 images per category for dogs, cats, and wild life at 512$\times$512 resolution. Each category is regarded as a dataset and thus we train a different network on each dataset.


\noindent\textbf{Training.}
We implement our InsGen on the official implementation of \href{https://github.com/NVlabs/stylegan2-ada-pytorch}{StyleGAN2-ADA}. The training regularization is preserved, including path length regularization, lazy regularization, and style mixing regularization. Moreover, all parameters share the same learning rate and the minibatch standard deviation layer is adopted at the end of the discriminator. Exponential moving average of generator weights, non-saturating logistic loss with $R_1$ regularization, and Adam optimizer~\cite{kingma2014adam} is also adopted. In particular, the coefficient of gradient penalty would be decreased correspondingly, according to the official implementation of ADA~\cite{karras2020training}. All the experiments are conducted on a server with 8 GPUs. Mixed-precision training is also used for faster training.


\noindent\textbf{Evaluation Metric.}
We use Fr√©chet Inception Distance (FID)~\cite{heusel2017gans} as the metric for quantitative comparison metric since FID tends to reflect the human perception of synthesis quality. As mentioned in \cite{heusel2017gans}, we always calculate the FID between 50,000 fake images and all training images, no matter how much data the training set contains. The \href{http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz}{official pre-trained Inception network} is used to compute the FID.


\noindent\textbf{Results on FFHQ.}
Tab.~\ref{table:sota-ffhq} presents the comparison on FFHQ. Akin to ADA~\cite{karras2020training}, we compare against PA-GAN~\cite{zhang2018pa}, zCR~\cite{zhao2020improved} and auxiliary rotation~\cite{chen2019self}. Also, StyleGAN2 together with its variants is also introduced as the baseline methods. For instance, less data is usually required when a shallower mapping network is applied. Besides, dropout~\cite{srivastava2014dropout} is also well-studied to be replaced with the augmentations as the regularization. Note that $^*$ means the dataset is amplified by 2$\times$ via the horizontal flip, which is recommended in the official implementation of ADA~\cite{karras2020training}. Such that, ``$2K$'' denotes 2,000 unique images and the dataset is enlarged to 4,000 via the flip operation, leading to a better baseline.


Although ADA~\cite{karras2020training} has already improved the performance significantly under various low-data regimes, our
InsGen continues to improve the low-data image generation by a clear margin, establishing a new state-of-the-art synthesis quality with limited training images. To be specific, our method improves the FID from 15.60 to 11.92, 7.29 to 4.90, and 3.88 to 3.31 with $2K$, $10K$ and $70K$ training images from FFHQ~\cite{karras2019style} respectively. Fig.~\ref{fig:main_results} presents several generated examples under various data regimes. All images on FFHQ are generated with truncation. It is also worth noting that our approach further improves the synthesis quality when the full dataset is given, even outperforming previous best one, \textit{i.e.}, zCR~\cite{zhao2020improved}. Namely, the data can be further exploited when it is not the bottleneck for training.


\setlength{\tabcolsep}{15pt}
\begin{table}[t]
    \centering
    \caption{
        \textbf{Performance on AFHQ.}
FID (lower is better) is reported as the evaluation metric.
Numbers in \textbf{\textcolor{azure}{blue}} color indicate our improvements over the baseline~\cite{karras2020training}.
    }
    \label{table:sota-afhq}
    \vspace{0pt}
        \begin{tabular}{llll}
        \toprule
        512$\times$512 Resolution                     & Cat    &  Dog   & Wild life  \\
        \midrule
        StyleGAN2~\cite{karras2019style}              & 5.13   & 19.4   & 3.48 \\
        ContraD~\cite{jeong2021training}              & 3.82   & 7.16   & 2.54 \\
        ADA~\cite{karras2020training}                 & 3.55   & 7.40   & 3.05 \\
        \midrule
        \textbf{InsGen} (Ours) & \textbf{2.60 \textcolor{azure}{($-$0.95)}}  & \textbf{5.44 \textcolor{azure}{($-$1.96)}}  & \textbf{1.77 \textcolor{azure}{($-$1.28)}} \\
        \bottomrule
    \end{tabular}
    \vspace{-5pt}
\end{table}


\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/main_results2.pdf}
	\vspace{-10pt}
	\caption{
	    \textbf{Generated images under various data regimes.}
The number of training images and the corresponding FID are reported.
All images on FFHQ are synthesized with truncation following \cite{karras2020training} while those on AFHQ are not.
	}
    \label{fig:main_results}
    \vspace{-8pt}
\end{figure}


\noindent\textbf{Results on AFHQ.}
We also evaluate our approach on AFHQ dataset~\cite{choi2020starganv2} which is divided into cat, dog and wild life, with the number of 5153, 4739 and 4738 images respectively. Therefore, three models are trained on them individually. Note that all models on AFHQ are trained on 512$\times$512 images while the generated samples are resized to present. We involve StyleGAN2~\cite{Karras2019stylegan2}, ContraD~\cite{jeong2021training} and ADA~\cite{karras2020training} as the baseline approaches, compared to our InsGen.  Quantitative and qualitative results are shown in Tab.~\ref{table:sota-afhq} and Fig.~\ref{fig:main_results} respectively.


The synthesis quality on those datasets is substantially improved by our method, which also outperforms previous data-augmentation methods. To be specific, our method improves the FID from 3.55 to 2.60, 7.40 to 5.44, and 3.05 to 1.77 on cat, dog and wild life images  respectively. In particular, ContraD~\cite{jeong2021training} introduced stronger augmentations to train a better discriminator via contrastive learning. One term in this method shares the similar motivation that real images could result in powerful representations. In terms of the use of synthesized samples, ContraD turned to focus on the binary classification (\textit{i.e.}, real \textit{vs.} fake) with some specific designs like the stop-gradient operation. Differently, our method leverages the generated images as a kind of data complement to produce a stronger representation and guide the learning of the generator. Accordingly, InsGen achieves the new state-of-the-art performances on AFHQ~\cite{choi2020starganv2}.


\subsection{Ablation Study}\label{subsec:ablation}


In order to investigate the importance of each component in our InsGen, we conduct an ablation study on FFHQ~\cite{karras2019style} with the image resolution of 256$\times$256. FID serves as the main metric for the comparison, and the results on $2K$, $10K$ and $70K$ unique images are reported. During training each unique image go through random flip operation to obtain a stronger baseline. Tab.~\ref{table:ablation} presents the collection of various experiments in the ablation study. We choose the ADA~\cite{karras2020training} as the baseline.


\noindent\textbf{How important is the instance discrimination?}
After performing the real image discrimination, the synthesis quality is improved, with the FID consistently decreased by \textbf{-1.45}, \textbf{-1.31} and \textbf{-0.20} in Tab.~\ref{table:ablation}, no matter how many unique images the training set includes. To some extent, the discriminator would benefit from the powerful representations derived from the challenging pretext task. Accordingly, the generator is required to produce more photo-realistic images in order to confuse the discriminator.


When adding instance discrimination with fake images, performances could be further boosted. For instance, FID obtains an improvement of  \textbf{-0.69} and \textbf{-0.30} with $2K$ and $10K$ images respectively. In particular, the gains rise as the number of real images goes down, verifying one of our motivations that the fake samples can be also regarded as data source for unsupervised representation learning.


\noindent\textbf{How important is the noise perturbation?}
In Sec.~\ref{subsec:insgen}, a noise perturbation strategy is proposed as a type of latent space augmentation for fake image discrimination. In particular, this latent space augmentation, \textit{i.e.}, the small movement in the latent space always leads to an obvious but semantically consistent change of the original image, which could not easily be implemented by some geometric and color transformations. Meanwhile, the discriminator is required to be invariant to such noise perturbation due to the goal of instance discrimination. Accordingly, the fake images are made best use of to result in stronger representations for the discrimination. As shown in Tab.~\ref{table:ablation}, such strategy further brings consistent gains of \textbf{-1.27}, \textbf{-0.38} and \textbf{-0.18} on $2K$, $10K$ and $70K$ datasets respectively.


\setlength{\tabcolsep}{15pt}
\begin{table}[t]
    \centering
    \caption{
        \textbf{Ablation Study.}
FID (lower is better) is reported as the evaluation metric.
Here, vanilla $\C^f_D$ means that the noise perturbation is not applied in the fake instance discrimination.
    }
    \label{table:ablation}
    \vspace{-5pt}
    \begin{tabular}{cccc|ccc}
        \toprule
        $\C^r_D$ & vanilla $\C^f_D$  & $\C^f_D$ & $\C^f_G$ & $2K$ & $10K$ & $70K$  \\
        \midrule
                  &           &           &           & 15.60 & 7.29 & 3.76      \\
        \ding{51} &           &           &           & 14.15 & 5.98 & 3.56      \\
        \ding{51} & \ding{51} &           &           & 13.46 & 5.68 & 3.67      \\
        \ding{51} & \ding{51} & \ding{51} &           & 12.19 & 5.30 & 3.49      \\
        \ding{51} & \ding{51} & \ding{51} & \ding{51} & 11.92 & 4.90 & 3.31      \\
        \bottomrule
    \end{tabular}
    \vspace{-15pt}
\end{table}


\noindent\textbf{How important is the supervision signal for the generator?}
The last row of Tab.~\ref{table:ablation} shows the performances with the gradients which are back-propagated to the generator. Even if we have already obtained quite strong results, such a supervision signal on the generator could also introduce improvements under various data regimes.


The goal of instance discrimination is to distinguish every individual image according to its appearance cues~\cite{wu2018unsupervised}. Assuming this pretext task is well-performed on a fixed dataset, the semantic representation would be derived from this learning process. However, when distinguishing fake images, the fake dataset actually varies dynamically. Namely, we could accomplish this pretext task from the perspective of data, if the engine of this dynamical fake dataset, \textit{i.e.}, the generator could produce as many different images as possible. In general, this pretext task is exploited to encourage the diverse generation directly on the generator.


\noindent\textbf{How important is the number of negative samples?}
We follow the MoCo-v2~\cite{chen2020mocov2} to store multiple features in a queue, in order to reduce the computational complexity. Empirically, the length of the feature queue tends to be the 5\% number of the dataset. Therefore, it is 200 when we have $2K$ unique images and enlarge them via the flip operation. However, there is no any reference number for the synthesized data. Accordingly, we collect as the same amount of fake data as that of the real.


As mentioned in Sec.~\ref{subsec:insgen}, there could be much more synthesized samples than the real samples. Namely, we could leverage infinite samples for the synthesized instance discrimination. Therefore, we investigate the effect of the different number of synthesized samples \textit{i.e.,} the length of the feature queue, shown in Fig.~\ref{fig:curve}a.
Obviously, FID gradually decreases with the increasing number of synthesized samples, suggesting that involving more fake images is of great benefit to the synthesis, especially with the limited training data.


\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/curve.pdf}
	\vspace{-20pt}
	\caption{
	    \textbf{Effect of the number of synthesized and real images used for instance discrimination.}
FID (lower is better) in log-scale is reported as the evaluation metric.
We can see the consistent performance gain along with the increasing number of instances for discrimination.
	}
    \label{fig:curve}
    \vspace{-5pt}
\end{figure}


\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/training_curve.pdf}
	\vspace{-20pt}
 	\caption{
        Training progress on FFHQ-2$K$.
Larger value means that the image is more realistic under the view of the discriminator.
Our discriminator can \textit{better and more stably} differentiate real and fake data compared to ADA~\cite{karras2020training}.
 	}
    \label{fig:prob}
    \vspace{-5pt}
\end{figure}


\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/limitations.pdf}
	\vspace{-20pt}
	\caption{
	    \textbf{Results with different number of training images.}
The number of training images and the corresponding FID are reported above the synthesis samples.
All images are generated with truncation following \cite{karras2020training}.
	}
    \label{fig:limitations}
    \vspace{-5pt}
\end{figure}


\noindent\textbf{Whether the discriminative ability of the discriminator is really enhanced.}
As is mentioned in our work, it is challenging to gain sufficient discriminative power for the discriminator to train the generator when the size of training set is small. However, introducing instance discrimination is able to improve its discriminative capability, achieving new state-of-the-art synthesis quality. In order to investigate whether the discriminative ability is improved, we plot the logits (derived from the discriminator) of any input image during the training in Fig.~\ref{fig:prob}. To be specific, the logit denotes how much the input image is identified as the real. And the number of training images are 2000.


Obviously, our method produces higher real and lower fake scores throughout the whole training progress, compared to the baseline approach ADA~\cite{karras2020training}. It indicates that the discriminator of our method performs the domain bi-classification (\textit{i.e.}, real \textit{vs.} fake) better than that of baseline, showing stronger discriminative ability. It also verifies our motivation that a challenging pretext task which is to distinguish every individual image could indeed enhance the discriminator. Besides, the training progress is much more stable when equipped with our approach.


\subsection{Towards the Limit of Data-efficiency}\label{subsec:limitation}


Although we have obtained the new state-of-the-art synthesis performances under the standard settings, we also wonder how much data-efficiency our InsGen could achieve. Therefore, the number of real data in the training set is further reduced to 1000, 500, 250 and 100. In order to conduct the apple-to-apple comparison, we remain to train the same model of StyleGAN2 without decreasing its generative capacity by using fewer channels or shallower mapping networks since such designs require less data. Meanwhile, the generated resolution remains 256$\times$256 and the datasets are amplified via the horizontal flip operation as well.


The quantitative and qualitative results are shown in Fig.~\ref{fig:curve}b and Fig.~\ref{fig:limitations} respectively.
Obviously, FID significantly increases with the decreasing number of training images from 70K to 100. Nevertheless, our InsGen trained with only 100 unique images remains to outperform many approaches like PA-GAN in Fig.~\ref{fig:curve}b with 2K images.
Besides, with 500 training samples, our method is able to obtain the competitive performance to those using 10k images. Namely, our InsGen could improve the data-efficiency by more than 20$\times$. Qualitative results suggest that our approach still produces meaningful images without incurring the model collapse no matter how many training images exist in the data collection.


\section{Conclusion}\label{sec:conclusion}


In this work, we develop a novel data-efficient Instance Generation (\textit{InsGen}) method for training GANs with limited data.
With the instance discrimination as an auxiliary task, our method makes the best use of both real and fake images to train the discriminator.
In turn the discriminator is exploited to train the generator to synthesize as many diverse images as possible.
Experiments under different data regimes show that InsGen brings a substantial improvement over the baseline in terms of both image quality and image diversity, and outperforms previous data augmentation algorithms by a large margin.


{\small
\bibliographystyle{abbrvnat}
\bibliography{references}
}


\end{document}
