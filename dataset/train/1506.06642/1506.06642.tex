\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{courier}
\usepackage{subfigure}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{graphicx} 
\usepackage{mathtools}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{cases}
\graphicspath{{charts/}} 

\def\baselinestretch{0.98}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}
\pdfpagewidth=8.5in
\pdfpageheight=11in


\newcommand{\N}{\mathbb{N}}
\newcommand{\MatN}{n}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\RTT}{\mbox{T}}
\newcommand{\VR}{\mbox{VRTT}}
\newcommand{\eg}{{e.g.,}}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]

\begin{document}

\title{LAC: Introducing Latency-Aware Caching in Information-Centric Networks}

\author{
\IEEEauthorblockN{
Giovanna Carofiglio\IEEEauthorrefmark{1},
Leonce Mekinda\IEEEauthorrefmark{2},
Luca Muscariello\IEEEauthorrefmark{2}
}
\IEEEauthorblockA{
\IEEEauthorrefmark{1} Cisco Systems,
\IEEEauthorrefmark{2} Orange Labs Networks,
\\
gcarofig@cisco.com, 
firstname.lastname@orange.com}
}


\maketitle

\begin{abstract} 
Latency-minimization is recognized as one of the pillars of
5G network architecture design. Information-Centric 
Networking (ICN) appears a promising candidate technology for
building an agile communication model that reduces latency
through in-network caching. However, no proposal has
developed so far latency-aware cache management mechanisms 
for ICN.
In the paper, we investigate the role of latency awareness on
data delivery performance in ICN and introduce LAC, a new
simple, yet very effective, Latency-Aware Cache management
policy. The designed mechanism leverages in a distributed
fashion local latency observations to decide whether to store
an object in a network cache. The farther the object,
latency-wise, the more favorable the caching decision.
By means of simulations, show that LAC outperforms state of 
the art proposals and  results in a reduction of the content 
mean delivery time and standard deviation by up to 50\%, 
along with a very fast convergence to these figures.
\end{abstract}

\section{Introduction}
Latency minimization, or building for virtual zero latency as 
commonly referred to, is one of the pillars of 5G network 
architecture design and is currently fostering important 
research work in this space.
Inserting cache memories across the communication data path 
between different processing elements has been already 
demonstrated to be a reliable way of improving performance 
by localizing - especially popular - content at network edge 
and so reducing retrieval latency.

Besides other advantageous architectural choices, the 
introduction of in-network caching as a native building block 
of the network design makes Information Centric Networking 
(ICN)\cite{Jacobson:2009:NNC:1658939.1658941}, a promising 5G 
network technology.
In a nutshell, every ICN router potentially manages a cache 
of previously requested objects in order to improve object 
delivery by reducing retrieval path length for frequently 
requested content. In fact, if content is locally available 
in the cache, the router sends it back directly to the 
requester, otherwise it forwards the request (or Interest) 
for the object to the next hop according to name-based 
routing criteria. When the requested object comes back, it is 
stored in the local cache before sending it back to the 
requester. Given cache size limitations, a replacement policy 
is put in place to evict previously stored objects for 
accommodating the newly available ones. To this aim, various 
classical cache replacement policies, not specifically 
ICN-based exist: to cite a few, Least-Recently-Used (LRU), 
Least-Frequently-Used (LFU), First-In-First-Out (FIFO) and 
Random (RND) \cite{DBLP:journals/corr/abs-1202-4880}.
Within the panoply of cache management policies proposed in 
the literature, very few exploit object retrieval latency to 
orchestrate cache decisions, while requiring transport 
protocol modifications \cite{Prob-Cache} or involving 
additional computational 
complexity\cite{Tong:2013:LSS:2505515.2507857}, without 
significant caching performance increase.

Clearly, the constraints imposed by ICN in terms of high 
speed packet processing exclude every complex cache 
management policy. Therefore, we focus in this paper on a 
simple, hence feasible, cache management policy leveraging
not only the objects replacement, but the cache insertion 
criterion,  that we define based on monitored object latency.

The cache management mechanism we propose in this paper, LAC, 
lies upon the following principle: every time an object is 
received from the network, it is stored into the cache with a 
probability proportional to its recently observed retrieval 
latency. As such, it is an add-on laying on top of any cache 
replacement policy and feeding it at a regulated pace. 
In this way, LAC implicitly prioritizes long-to-retrieve 
objects, instead of caching every object regardless. 
The underlying trade off such caching mechanism tackles is 
between a limited cache size and delivery time minimization.
As caching intrinsically aims to relieve the fallouts of 
network distance or traffic congestion, it must be aware of 
both delay factors to efficiently handle the cache size / 
delivery time tradeoff.  Data retrieval latency is a simple, 
locally measurable and consistent metric for revealing either 
haul distance or traffic congestion.
More precisely, the contribution of this paper is threefold :

\begin{itemize}
\item We design LAC, a randomized dynamic cache management 
policy leveraging in-network retrieval latency for cache 
insertion. The locally monitored metric is the time elapsing 
at a given node between request forwarding and corresponding 
packet reception.

\item We provide a preliminary analysis of LAC
to prove its superior performance over a symmetric p-LRU 
(probabilistic LRU) policy using 
the same probability  for Move-To-Front (MTF) operation in 
case of hit and miss events. 

\item We evaluate LAC performance
by means of packet level simulations carried out with our ICN simulator CCNPL-Sim (\url{http://systemx.enst.fr/ccnpl-sim}). 
\end{itemize}

The rest of the paper is structured in the following way. We 
review the state of the art and perceived limitations in 
\S\ref{sec:related_work}. The problem formulation of 
latency-aware caching is reported in 
\S\ref{sec:problem_formulation_design_choices}.
Sec. \S\ref{sec:system_model} gathers analytical results, 
while performance evaluation of our proposal is in 
\S\ref{sec:performance_evaluation}.
Finally, Sec.\ref{sec:conclusion} concludes the paper, by 
giving a glimpse on future activities.

\section{Related Work} \label{sec:related_work}
In the context of ICN research, previous work have considered 
the enhancement of cache mechanisms with the aim of reducing 
caching redundancy over a delivery path. We can distinguish 
two categories of related work: those leveraging content 
placement (e.g. \cite{Congestion-Aware-Gerla}, 
\cite{Time-Shifted-Simon}) as opposed to
those proposing caching mechanisms based on selective 
insertion/replacement in cache (e.g. \cite{Prob-Cache}, 
\cite{ICN2014-Kurose}, \cite{LCN2014}, \cite{Age-Based}).
The first class of approaches has a limited applicability to 
controlled environments like a CDN (Content Delivery 
Network), where topology and content catalog are know a 
priori. Either \cite{Congestion-Aware-Gerla} and 
\cite{Time-Shifted-Simon} deals with video streaming 
in ICN and orchestrate caching and scheduling of requests to 
caches in order to create a cluster of caches with a certain 
number of guaranteed replicas (\cite{Time-Shifted-Simon}).
Unlike these approaches, our work belongs to the second class 
of caching solutions and aims at defining a decentralized 
caching solution that automatically adapts to changes in 
content popularity, network variations etc. by leveraging 
content insertion/replacement operations in cache. 
We share the same objective as \cite{ICN2014-Kurose}, where 
authors propose a congestion-aware caching mechanism
for ICN based on estimation of local congestion,  
of popularity and of position w.r.t. the bottleneck. 
The congestion estimate in this work does not allow to 
differentiate content items in terms of latency like in our 
work. A similar consideration holds for other related 
approaches: the ProbCache work in \cite{Prob-Cache}, which 
utilizes the same cache probability for every content item at 
a given node and the cooperative caching mechanism in 
\cite{LCN2014}-\cite{Age-Based} exploiting overall popularity 
and distance-to-server. Clearly, the rationale behind is the 
same, but the distance-to-server metric does not reflects the 
differences in terms of latency, distance to bottleneck on a 
per-flow basis that our approach takes into account. 
Beyond ICN, caching literature is vast 
\cite{Podlipnig:2003:SWC:954339.954341} and our review 
here does not attempt to be exhaustive, while rather to 
position our contribution w.r.t. closest classical caching 
approaches. Starobinski 
\textit{et al.}
\cite{Starobinski:2001:PMW:570289.570293} and later
Jelenkovi\'{c} 
\textit{et al.}\cite{Jelenkovic:2004:OLC:1024662.1024670} 
describe a cache management mechanisms to optimize the 
storage of variable size documents. In their work, the whole 
Move-To-Front rule is \textit{symmetric}, i.e. applied in 
both hit and miss events (as for LRU,LFU etc.)
while our approach, instead, may be denoted as 
\textit{asymmetric}, since it restricts the stochastic 
decision of MTF to cache miss events, leaving object 
replacement subject to deterministic LRU. 

Furthermore, Starobinski \textit{et al.} model only focuses 
on a single cache, while assuming that every object is 
associated with a fixed retrieval cost, just 
like an intrinsic property. However, in a network of caches, 
an object's retrieval cost 
may vary considerably, depending on its current location and 
on network congestion.

\section{Problem formulation and design choices}
\label{sec:problem_formulation_design_choices}
The problem of improving end-user delivery performance can be 
formulated as the minimization of the overall average 
delivery time  for all users in the network and 
over all requested objects.

\begin{numcases}{}
\min \sum_{u\in \mathcal{U}} \sum_{k\in\mathcal{K} } 
\sum_{r \in \mathcal{R}_{u,k}} q_{k,u} p_{k,r,u} \E[\RTT_{k,r,u}]  \nonumber \\
\sum_{k} q_{k,u} =1, \qquad \forall u  \\
\sum_{r} p_{k,r,u} =1, \qquad \forall k, r   \label{eq:const2}\\
0 \leq q_{k,u} \leq 1 \qquad \forall k, u \\
0 \leq p_{k,r,u} \leq 1 \qquad \forall k, r, u
\end{numcases}
where  is the normalized request rate of object  from user  (namely, the popularity function at user ),
and  is the probability to download object  
from route  and  is the average latency 
to retrieve object  on route . The set of routes 
available at user  is identified by .

Using the Lagrangian of the problem and imposing the 
Karush-Kuhn-Tucker (KKT) optimality conditions it is easy to 
show that for some constants , .
 
In this paper we look for a distributed algorithm that tries 
to minimize this objective by obtaining  without 
any coordination among the nodes and no signaling.
The optimal objective expressed in (\ref{eq:const2}]) can be 
heuristically generalized to every node  in the network by 
substituting  with the local residual popularity 
 at node  and with  
the local virtual residual round trip time for object  on 
route , denoted by . Hence we set the 
probability to store an object  at a given node , 
proportional to the popularity and latency locally observed 
at noted . It is left to future work to prove that this 
distributed heuristic is actually optimal.
The intuition behind eq.(\ref{eq:objective}) is that user  
downloads an object  from a remote path  inversely 
proportional to its popularity and retrieval latency.
A globally optimal strategy performed in each node would 
heuristically prefer to cache locally popular content and 
with high retrieval latency.

In this paper we design an heuristic based on the following 
criterion. Thus our general formulation of the probability 
that the caching decision  is true i.e. the probability 
to cache the  requested object considering all encountered object retrieval latencies, is



where   is the retrieval latency of the  
requested object,  might be, for example, either a mean, 
the median or a maximum function,  and  are 
intensity parameters,  means  ``is proportional 
to''. The object retrieval latency and the probability of 
caching it are, hereby, made proportional.  Note that the 
caching decision may cumulatively depend on another fixed or 
dynamic factors (such as the outcome of another random 
experiment).
In the following section we analyze the performance of the 
proposed caching system and compare it to mechanisms existing 
in the literature under a dynamic workload.

\section{Model}
\label{sec:system_model}
The dynamics of the system are complex to capture in a simple 
model due to the tight coupling between delivery performance 
and caching functions: delivery performance is certainly 
affected by network conditions, while clearly network load is 
a result of caching performance and vice-versa. 

In this section, we first introduce modeling assumptions 
(Sec.\ref{sec:ass}), then proceed in two steps: (i) we tackle 
the single cache case, developing analytically some performance bounds, (ii) we leverage such analysis to 
provide an insight on the network of caches case.

\subsection{Assumptions}\label{sec:ass}
The purpose of this model is to identify the added value of the latency-aware stochastic 
decision in outperforming existing alternatives.
In this context, we consider the smallest set of assumptions to have a simple and feasible 
analytical representation.

\begin{itemize}
\item{Zipf-like popularity:}
We assume that object popularity follows a generalized Zipf 
law. Thus let  be the popularity object with rank :

with   and 
. This assumption is widely accepted in the 
literature \cite{breslau:web} 
\cite{Mitra:2011:CWV:1961659.1961662}.

\item{Poisson requests:}
We assume that clients request objects according to a Poisson 
distribution with rate , similarly to 
\cite{Carofiglio:2013:PBS:2542828.2542992}

\item{Independent Reference Model:}
Temporal correlation between object requests, though 
neglected here like in 
\cite{Starobinski:2001:PMW:570289.570293} and 
\cite{Fricker:2012:VAA:2414276.2414286}, is foreseen in 
future extensions of this work.

\item{LRU replacement policy:} We focus on the widely adopted 
LRU replacement policy whose common implementation consists 
in moving the most recently served object to 
the front of a list. This allows to study Move-To-Front 
algorithm as an LRU scheme 
\cite{Jelenkovic:2004:OLC:1024662.1024670}.

\item{Same object size:} For the sake of simplicity, we 
assume that, like in \cite{Che:2006:HWC:2312147.2313846}, all 
retrieved objects have the same size. The model will later be 
improved to encompass more fine-grained features such as 
variable object size. We aim to calculate two metrics that, 
we think, give an insight  of a caching system asymptotic 
behavior: the steady-state miss probability  and mean 
delivery time per popularity rank.
\end{itemize}

Refer to Table \ref{tab:symbols} the notation used throughout 
the paper.

\begin{table}[htb!]
\begin{footnotesize}
\centering
\begin{tabular}{|l||p{7cm}|}
\hline
 & Local cache size in number of objects\\
\hline
 & Request rate of rank- objects. Under Poisson arrivals, \\
\hline
 & Probability of receiving at least one request for a rank- object during  seconds\\
\hline
 & Local cache miss probability for rank- objects at time \\
\hline
 & Probability of a positive caching decision for object rank  at time \\
\hline
 & Characteristic Time threshold for filling a cache of size \\
\hline
\end{tabular}
\caption{Notation.}\label{tab:symbols}
\end{footnotesize}
\end{table}

\subsection{LAC in the single cache model} \label{sec:single_cache}
In this section, we analyze the latency-aware mechanism proposed in this paper
by computing its performance, expressed in terms of the cache miss probability.
The analysis starts from the computation of LAC steady-state per object 
miss probability, , . LAC is referred to as -LRU 
as opposed to systems where either the insertion is determined by a constant 
probability  or insertion/replacement operations are symmetrically driven 
by the same probability. Indeed, recall that LAC asymmetry stems from the fact 
that the insertion is probabilistically determined on a per-object basis by the 
monitored residual latency, while using LRU replacement.

\begin{proposition} \label{prop:MTF_arrival_process}
In a LRU cache with insertion probability , 
the move to front probability at time , during the time window , 
for object  is given by

being   be the probability of receiving at least one request for a 
rank- object during  seconds.
\end{proposition}

The characteristic time (``Che'') approximation 
\cite{Che:2006:HWC:2312147.2313846} states that for a 
sufficiently large cache, the object eviction time is well 
approximated by a unique constant , being  the 
cache size.  Under this approximation, hence, the miss 
process for a cache under stochastic caching decision, 
.
 denotes the number of distinct objects moved to 
the cache front. Upon the assumption that every object gets 
eventually cached at least once over time.
Under this approximation  
which implies 
and generalizes what obtained in 
\cite{DBLP:journals/corr/MartinaGL13} and 
\cite{Bianchi:2013:CBS:2500098.2500106}.
for any inter-arrival time distributions of the request 
process. If we assume that  and   
are both ergodic 


If we restrict to a discrete set of positive caching decision 
probabilities,





That holds from the Che approximation. Note that 
 
under Poisson object arrivals. Note that 
Eq.\eqref{eq:pi_asym_mean} might not be computationally 
tractable. However the following theorem shows that values of 
 can be replaced by its mean.

\begin{theorem}
\label{theo:pi_asym_LRU}
If positive caching decision probabilities  and 
popularity ranks are deemed independent, and assuming Poisson 
object arrivals,


\label{jensen_asym}
\end{theorem}

\begin{proof}
The miss probabilities are a convex function of the caching 
decision probabilities as

By Jensen's inequality,

A proof that also  is convex  follows by 
computing 
 from the first derivative obtained 
in \cite{DBLP:journals/corr/MartinaGL13} Appendix A, which applies the implicit function 
theorem over  Eq.\eqref{eq:che}. 
 is composed of positive terms expect one 
 that is negative  and 
surely positive for any sufficiently large number of 
popularity ranks. Hence, by invoking Jensen's inequality once more,  .
As ,  tends to counterbalance inequality 
\eqref{first_jensen}, giving


\end{proof}

This result is important because it states that caching based on a random  with values  ends up in a steady-state miss probability similar to the one obtained directly using a constant positive decision probability .
Fig.\ref{fig:asym} depicts this miss probability over popularity rank as a function of the decision probability. It gives a first intuition that keeping  very small decreases drastically the miss probability of high popularity ranks. The number of beneficiary ranks being limited by the cache size (set to 8 files in this instance).
\begin{figure}[!ht]
\centering
\includegraphics[width=0.4\textwidth]{asym.pdf}
\caption{ increases with  for the most popular objects.} 
\label{fig:asym} 
\end{figure}
However, the drawback of a constant and small  
is that it postpones considerably the time popular objects 
are first stored in the cache. LRU+LCP suffers from this 
phenomenon because the expected time to enter the cache is
. Consequently, the overall 
object delivery time converges slowly.

\subsection{-LRU, an analytical lower bound to -LRU}
\label{subsec:lower_bound}
Providing a closed-form expression for -LRU's 
miss probability and its Characteristic Time  
is hard. Instead, we demonstrate its superiority over the 
analytically tractable -LRU mechanism.
With some loss of generality,  is assumed greater 
than one. Let us consider the symmetric mechanism 
-LRU where the MTF probabilities are conditioned  
by the same probability . By contrast in 
-LRU the MTF decision is taken in case of miss 
only.
\begin{theorem} \label{theo:pi_sym_LRU}
-LRU steady-state miss probability

under the assumption that the mean positive caching decision 
probability, , is the same for all popularity 
ranks i.e. .
\end{theorem}

\begin{proof} 
Let  denote the number of times a rank- object 
is moved to the cache front. The mean number of distinct objects moved 
to the front of the LRU cache over time is

In virtue of Lemma 5 of \cite{Jelenkovic:2004:OLC:1024662.1024670}.
Hence, the power of alpha-magnified mean number of distinct objects moved to the front of the LRU cache over time 

The rest follow by using the exponential inter-arrival distribution
for an object with rank .
\end{proof}
The closed-form expression of Theorem \ref{theo:pi_sym_LRU} is intrinsically the same as LRU's in \cite{Carofiglio:2013:PBS:2542828.2542992}. This observation yields the next corollary.

\begin{corollary}
If decision probabilities and popularity ranks are deemed independent,

i.e. -LRU behaves in first-order mean like LRU.
\end{corollary}
-LRU consequently outperforms -LRU thanks to its convergence to the Least Frequently Used replacement policy\cite{DBLP:journals/corr/MartinaGL13}. This leads to the next theorem.

\begin{theorem}
\label{theorem_asym_gt_sym}
Let  be the number of most popular objects ``permanently'' accommodated thanks to a caching mechanism,

\end{theorem}
i.e. -LRU allows to accommodate ``permanently'' -times more of the most popular objects than . 
\begin{proof}\textit{(optional)}
Let the miss probabilities of all ``permanently'' stored objects admit a sufficiently small value  as upper bound.
Then, 

and 
.
Since a first-order Taylor series expansion of  for -LRU, when , yields
,



\end{proof}
Let  LRU equipped for latency-aware stochastic caching decision (presented in this paper) 
and   LRU modified for latency-aware stochastic MTF decision (Starobinski-Tse-Jelenkovi\'c-Radovanovi\'c's).
\begin{corollary}
\label{corollary_asym_gt_sym}
As a mere special case of Theorem \ref{theorem_asym_gt_sym}, 

.
\end{corollary}

This typically means that the performance of LRU caches equipped with latency-aware stochastic caching decision can exceed beyond a given factor  that of -LRU, then LRU studied analytically and extensively in previous works \cite{Carofiglio:2013:PBS:2542828.2542992}.


\subsection{Network of caches}
\label{subsec:network_of_caches}

The analytical characterization of the dynamics of a network of caches, even in a broader scope than ICN, is an active research topic \cite{DBLP:journals/icl/Blefari-MelazziBCD14} \cite{Rosensweig:2010:AMG:1833515.1833684}\cite{journals/corr/JinYKSYHL13} and some closed-form results have been presented, but only for networks of LRU caches\cite{Carofiglio:2013:PBS:2542828.2542992}. 


Leaving for future work a thorough analytical characterization of network dynamics under LAC, we explain here the entanglement between latency-aware caching and network performance we need to take into account.

Let focus on a single path, where in-network caching is enabled at each node.
As in \cite{ITC,Carofiglio:2013:PBS:2542828.2542992}, we may denote with , the Virtual Round Trip Time (VRTT) for any packet of object  and at a given user which we assume to be the first node of this path towards the repository.  is defined as the weighted sum of user-to-node  round trip time,  times the probability for node  to be the first hitting cache for the request sent by the user, 

 being the miss probability for packets of object  at node .
Now, at every intermediate node  along the path, the measured residual latency can be defined as:

Over time, the expectation of the Residual Virtual Round Trip Time for a rank- 
object at node ,  represent the mean cost of all routes 
to a permanent copy of the object departing from . 

Hence, for the caching node , the probability of a positive LAC decision for rank  at each discrete decision instant  results to be proportional to the monitored average Residual Round Trip Time, \\
.
Beyond the normalization of the probability to , the specific function we have selected accounts for a normalization of the monitored metric over a function,   which is meant to indicate the overall latency cache  welcomes. 
We have had successful experience with the instance , namely the average
.


\section{Performance Evaluation}
\label{sec:performance_evaluation}

We implement and test LAC by means of simulations carried out with the packet-level NDN simulator CCNPL-Sim (\url{https://code.google.com/p/ccnpl-sim/}). We evaluate (i) single cache topologies, then (ii) networks of caches topologies with a single content server on the top and three intermediate layers of caches and a client layer at the bottom.
LAC, our latency-aware LRU denoted as  is tested against two other fully distributed caching management mechanisms: LRU+Leave-Copy-Probabilistically and LRU \cite{1395054}\cite{643}. By fully distributed, we mean mechanisms that do not require the exchange of any specific signalling between caches.



\subsection{Single cache topology}
The following results are achieved in a simulated ICN with a single caching node between the object consumers and the publishing server and with the following parameters. 
\begin{itemize}
	\item Cache sizes are equal to 80KBytes. 
	\item The Poisson process for generating content requests is characterized by a rate of 1 object/s
	\item Objects are requested over a catalog of 20,000 items, according to a  Zipf-like popularity distribution of
	parameter . This value of  has been demonstrated realistic \cite{Mitra:2011:CWV:1961659.1961662}. Each file is 10KBytes size.
	\item The two FIFO links from the consumers up to the content publisher have a capacity of 200Kbps and of 30Kbps, respectively. 
	\item Each object conveyed through these links has an average size of 10KBytes, that we also take as fixed packet size.
\end{itemize}
About LAC parameters, caches are equipped for latency-aware stochastic caching decision, with  to stress the rejection of quickly delivered objects. The function  is the mean latency of all ever-cached objects.
We report the related charts in Fig.\ref{single_topology_results}.
The load  of the 30Kbps downlink equals 0.56 when the cache is ruled by LRU, 0.58 under , 0.41 under LRU+LCP and 0.37 under LAC ().

\begin{figure*}[htbp] 
\centering
\subfigure[Miss probability curves validate models.]{\includegraphics[width=0.32\textwidth]{single_miss_prob.pdf}
\label{fig:single_miss_prob}}
\subfigure[Delivery time w.r.t content rank overview (vs LRU)]{\includegraphics[width=0.32\textwidth]{single_delivery_time_LRU.pdf}
\label{fig:single_delivery_time_LRU}}
\subfigure[Delivery time w.r.t content rank overview (vs LRU+LCP)]{\includegraphics[width=0.32\textwidth]{single_delivery_time_LCP.pdf}
\label{fig:single_delivery_time_LCP}}
\subfigure[Evolution of the mean delivery time]{\includegraphics[width=0.32\textwidth]{single_delivery_time_mean_vs_seqnumber.pdf}
\label{fig:single_delivery_time_mean_vs_seqnumber}} 
\subfigure[Evolution of the delivery time standard deviation]{\includegraphics[width=0.32\textwidth]{single_delivery_time_stddev_vs_seqnumber.pdf}
\label{fig:single_delivery_time_stddev_vs_seqnumber}}
\caption{Single cache topology simulation: LAasym decreases LRU delivery time by 30\% and outperforms LRU+LCP on convergence.}
\label{single_topology_results}
\end{figure*}
A first observation we draw from the plots in Fig.\ref{single_topology_results} is that our LAC proposal,  converges to the same steady state as LRU+LCP, which approximates the optimal LFU behavior.
Note that it this is true in static and hierarchical network of caches with no regeneration (no user requests from intermediate nodes), in general LAC is based on temporal measurements of residual latency, so adapting over time based on the sensed variations in terms of experienced latency. 
Secondly, we observe how much  latency-aware technique reduces both delivery time mean and standard deviation. It is striking to see how quickly they converge, compared to classical LRU+LCP.
The constant decision probability used in LRU+LCP is, indeed, the average of all latency-aware decision probabilities () 
and this impacts negatively either the convergence either the system reactivity to temporal variations of latency, as opposed to our LAC proposal. 
Finally, we observe that  and LRU miss probability curves coincide in steady state as predicted in \cite{Jelenkovic:2004:OLC:1024662.1024670}. A symmetric filtering of objects to put in and to remove from the cache has the only effect of slowing down convergence while not modifying the dynamics of the underlying Markov chain. 

\subsection{Network of caches}
\subsubsection{Line topology network}
\begin{figure}[htbp] 
\centering
\includegraphics[width=0.45\textwidth, height=2.5cm]{line_topology.pdf}
\vspace{-1cm}
\caption{Simulated line topology.} 
\label{fig:line_topology} 
\end{figure}
We now consider the setting in Fig.\ref{fig:line_topology}, with three caching nodes in-line
between the users and the publishing server. 
The set of parameters we consider is the same as before for what concerns cache sizes, request process and popularity. 
The four links from the consumers up to the
publisher have capacities equal to 300Kbps, 200Kbps, 200Kbps and 30Kbps respectively. 
Under LRU+LCP,   and corresponds to the lowest mean latency-aware caching decision probability, Cache 3's. 
Related results are reported in Fig.\ref{line_topology_results}.
The resulting link load  on downlinks from the repository to the users is respectively : (0.5, 0.01, 0.03, 0.27) under LRU,
(0.27, 0.02, 0.02, 0.27) under LRU+LCP and (0.22, 0.04, 0.06, 0.27) under our LAC proposal, . 
\begin{figure*}[ht] 
\centering
\subfigure[LRU and LRU+LCP miss probability]{\includegraphics[width=0.32\textwidth]{line_miss_prob_LRU.pdf}
\label{fig:line_miss_prob_LRU}}
\subfigure[LAasym miss probability]{\includegraphics[width=0.32\textwidth]{line_miss_prob_LAasym.pdf}
\label{fig:line_miss_prob_LAasym}}
\subfigure[Delivery time w.r.t content rank overview]{\includegraphics[width=0.32\textwidth]{line_delivery_time.pdf}
\label{fig:line_delivery_time}}
\subfigure[Evolution of the mean delivery time]{\includegraphics[width=0.32\textwidth]{line_delivery_time_mean_vs_seqnumber.pdf}
\label{fig:line_delivery_time_mean_vs_seqnumber}} 
\subfigure[Evolution of the delivery time standard deviation]{\includegraphics[width=0.32\textwidth]{line_delivery_time_stddev_vs_seqnumber.pdf}
\label{fig:line_delivery_time_stddev_vs_seqnumber}}
\caption{Line topology simulation: LAasym decreases  LRU delivery time by 50\% and outperforms LRU+LCP on convergence.}
\label{line_topology_results}
\end{figure*} 
Clearly, the expensive traffic to the publisher decreases significantly with , while very little increase can be oberserved on the other links. The tremendous gain in delivery time (50\% of LRU's) can be appreciated in both its first and second moments. Such a delivery time standard deviation decrease plays a central role in stabilizing customers quality of experience.

\subsubsection{Tree topology network}
The next results are those achieved in the ICN setting in Fig.\ref{fig:tree_topology}, spanning a binary tree topology whose
seven caching nodes are spread over three network levels, between the users and
the repository (publishing server). In such configuration, 
\begin{itemize}
	\item Cache sizes are 8MBytes.
	\item Poisson object request rate at the user is 1 object/s.
	\item Object popularity follow a Zipf(1.7) distribution.
	\item Object size is taken equal to 1 MB.
	\item Downlink capacities from the users up to the repository are 
	30Mbps-capable, except the last one toward the repository, which is 9Mbps.
	\item Each packet has an average size of 10KBytes, making every object equal to 100 packets in size.
\end{itemize}

Caches are equipped for LAC decision, with , with  the function  remaining equal to the mean latency of all ever-cached objects.
Cache 4 is on the first layer (the closest to the consumers), Cache 8 on the second layer and
Cache 10 on the third (the farthest to the users). 
LRU+LCP's . That corresponds to 's mean latency-aware caching decision probability. 
\begin{figure}[htbp] 
\centering
\includegraphics[width=0.35\textwidth]{tree_topology.pdf}
\caption{Simulated tree topology } 
\label{fig:tree_topology} 
\end{figure}

We report the related charts in Fig.\ref{tree_topology_results}. 
The observed  link load  on downlinks from the repository to the users is respectively: (0.7, 0.31, 0.18, 0.6) under LRU, 
(0.7, 0.07, 0.33, 0.6) under LRU+LCP and (0.7, 0.12, 0.23, 0.6) under . Again, our LAC mechanisms allows to lower maximum and average link load over the network. 


 \begin{figure*}[htbp] 
 \centering
 \subfigure[LRU and LRU+LCP miss probability]{\includegraphics[width=0.32\textwidth]{tree_miss_prob_LRU.pdf}
 \label{fig:tree_miss_prob_LRU}}
 \subfigure[LAasym miss probability]{\includegraphics[width=0.32\textwidth]{tree_miss_prob_LAasym.pdf}
 \label{fig:tree_miss_prob_LAasym}}
 \subfigure[Delivery time w.r.t content rank overview (vs LRU)]{\includegraphics[width=0.32\textwidth]{tree_delivery_time_LRU.pdf}
 \label{fig:tree_delivery_time_LRU}}
 \subfigure[Delivery time w.r.t content rank overview (vs LRU+LCP)]{\includegraphics[width=0.32\textwidth]{tree_delivery_time_LCP.pdf}
 \label{fig:tree_delivery_time_LCP}}
 \subfigure[Evolution of the mean delivery time]{\includegraphics[width=0.32\textwidth]{tree_delivery_time_mean_vs_seqnumber.pdf}
 \label{fig:tree_delivery_time_mean_vs_seqnumber}} 
 \subfigure[Evolution of the delivery time standard deviation]{\includegraphics[width=0.32\textwidth]{tree_delivery_time_stddev_vs_seqnumber.pdf}
 \label{fig:tree_delivery_time_stddev_vs_seqnumber}}
 \caption{Tree topology simulation:  LAasym decreases LRU delivery time by 30\% and outperforms LRU+LCP on convergence.}
 \label{tree_topology_results}
 \end{figure*}
 
Finally, we observe as a general rule that implementing  decreases the
overall cache miss probability i.e. the probability that all solicited caches fail to serve the
requested object. It also decreases and stabilizes the overall object delivery time. Indeed, the mean delivery time and the 95\% confidence interval around the average, both decrease by up to 50\%. 

Note also that this overall improvement is not achieved to the detriment of the convergence speed, unlike LRU+LCP. The latter, indeed, exhibits tremendously slow convergence and extremely high delivery time standard deviation.

\section{Conclusion and future work}
\label{sec:conclusion}
In the paper, we showed the benefits of leveraging latency for caching decisions in ICN and proposed LAC, a latency-aware cache management policy that bases cache insertion decisions on measurements of residual latency over time on a per-object basis. 
While keeping the same low complexity as standard LRU with probabilistic cache insertion, it provides a finer-granular differentiation of content in terms of expected residual latency. 
Two main advantages have been demonstrated: (i) superior performance in terms of realized delivery time at the end-user plus maximum and average link load reduction, when compared to classical LRU and probabilistic caching approaches; (ii) faster convergence w.r.t. probabilistic caching approaches along with reduced standard deviation. 

We leave for future work a thorough characterization of LAC dynamics, especially in a network of caches, where the coupling with hop-by-hop forwarding may be addressed through a joint optimization. 

The sensitivity to variations in network conditions and routing will also be investigated to highlight the benefit in terms of self-adaptiveness of a measurement-based approach w.r.t. classical latency-insensitive approaches.


\bibliographystyle{abbrv}
\bibliography{references}

\end{document}