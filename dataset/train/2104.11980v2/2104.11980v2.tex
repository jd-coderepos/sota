\subsection{Multi-agent trajectory modelling}

Our problem description closely follows \citet{alcorn2021baller2vec}, whom we quote here:

\begin{quote}
Let $A = \{1, 2, \dots, B\}$ be a set indexing $B$ agents and $P = \{p_{1}, p_{2}, \dots, p_{K}\} \subset A$ be the $K$ agents involved in a particular sequence.
[Let] $C_{t} = \{(x_{t,1}, y_{t,1}), (x_{t,2}, y_{t,2}), \dots, (x_{t,K}, y_{t,K})\}$ [be] an unordered set of $K$ coordinate pairs such that $(x_{t,k}, y_{t,k})$ are the coordinates for agent $p_{k}$ at time step $t$.
The ordered sequence of sets of coordinates $\mathcal{C} = (C_{1}, C_{2}, \dots, C_{T})$, together with $P$, thus defines the trajectories for the $K$ agents over $T$ time steps.
\end{quote}

In multi-agent trajectory modeling, the goal is to model a joint probability of the form:

\[
     p(\Delta x_{t,1}, \Delta x_{t, 2}, \dots, \Delta x_{t, K} | x_{1:t,1}, x_{1:t,2}, \dots, x_{1:t,K})
\]

\noindent
i.e., the joint probability of the $K$ agents' trajectories $\Delta x_{t,k}$ at time step $t$ given the agents' location histories $x_{1:t,k}$.
We note here that the common practice of \textit{simultaneously} predicting the trajectories for all of the agents at a specific time step is not required by theory.
Using the chain rule of probability, the joint probability of the agents' trajectories can be factorized as, e.g.:

\[
    p(\Delta x_{t,1}, \Delta x_{t, 2}, \dots, \Delta x_{t, K}) = p(\Delta x_{t,1}) p(\Delta x_{t, 2} | \Delta x_{t,1}) \dots p(\Delta x_{t, K} | \Delta x_{t,1}, \Delta x_{t, 2}, \dots, \Delta x_{t, K - 1})
\]

where we omit the conditional historical trajectories for brevity.
As a result, it is perfectly acceptable to generate trajectories agent-wise, using the previously generated trajectories as additional conditioning information when generating the trajectories for later agents (see Figure \ref{fig:legal_dep}).

\begin{figure}[ht]
\centering
\subfigure{\includegraphics[width=\columnwidth]{legal}}
\caption{
At inference time, a model is not \textit{required} to \textit{simultaneously} generate the trajectories for all of the agents at a specific time step.
An alternative strategy is to allow the model to generate the agents' trajectories one at a time, and let the model use the previously generated trajectories to inform the trajectories it generates for the remaining agents.
}
\label{fig:legal_dep}
\end{figure}

\subsection{\btv{} is a (conditional) generative model.}\label{sec:baller2vec_generative}

\btv{} is a recently described \textit{multi-entity} Transformer that can model sequences of \textit{sets} (the underlying data structure for multi-agent spatiotemporal systems), as opposed to sequences of individual inputs (like words in a sentence).
When used to model the game of basketball, the input at each time step for \btv{} is a set of feature vectors where each feature vector contains information about the identity and location of a player on the court.
\btv{} maps each input feature vector to an output feature vector, which is then used to ``classify'' the binned trajectory for that specific player at that specific time step.

Here, we provide a probabilistic interpretation of \btv{}, which establishes the theoretical grounds for using the chain rule to generate trajectories agent-wise at each time step in \btvpp{}.
Without loss of generality, we only consider one-dimensional trajectories for a single agent here.
To briefly summarize, the outputs of the softmax function over the $n$ binned trajectories in \btv{} can be interpreted as mixture proportions for a mixture of uniform distributions with predetermined bounds that partition the Euclidean trajectory space.
Further, because $x_{t + 1} = x_{t} + \Delta x_{t}$, \btv{} is in fact a conditional generative model that assigns a probability to a sequence of trajectories given the initial position of the agent, i.e., $p(\Delta x_{1}, \Delta x_{2}, \dots, \Delta x_{T} | x_{1})$.
Using the chain rule, we decompose the joint probability of the trajectories as:

\[
    p(\Delta x_{1}, \Delta x_{2}, \dots, \Delta x_{T}) = p(\Delta x_{1}) p(\Delta x_{2} | \Delta x_{1}) \dots p(\Delta x_{T} | \Delta x_{1}, \Delta x_{2}, \dots, \Delta x_{T - 1})
\]

\noindent
to reflect their temporal structure (we omit the conditional initial position term for brevity).
Therefore, new trajectories can be generated from \btv{} with the following procedure (see Figure \ref{fig:graphical_model}):

\begin{enumerate}
    \item First, sample one of the $n$ different mixture components using the mixture proportions output from the classifier $f$ (i.e., \btv{}) conditioned on the agent's current position, i.e., $i \sim \text{Categorical}(\pi_{1}, \pi_{2}, \dots, \pi_{n})$ where $[\pi_{1}, \pi_{2}, \dots, \pi_{n}] = f(x_{t})$.
    \item Next, sample a trajectory from the uniform distribution associated with the sampled component, i.e., $\Delta x_{t} \sim \mathcal{U}(a_{i}, b_{i})$.
    \item Finally, add the sampled trajectory to the agent's input position to generate the agent's position at the start of the next time step, i.e., $x_{t+1} = x_{t} + \Delta x_{t}$.
\end{enumerate}

\begin{wrapfigure}{r}{0.5\linewidth}
\vskip -0.2in
\includegraphics[width=\linewidth]{graphical_model_tight}
\caption{
\btv{} can be viewed as a conditional generative model that assigns a probability to a sequence of trajectories given the initial positions of the agents.
Here, we show a graphical model depiction of a \btv{} model that generates a sequence of one-dimensional trajectories for a single agent.
Given the initial position of the agent (the circle containing $x_{1}$), one of $n$ different uniform distributions (the square containing $i_{1}$) is sampled using the mixture proportions ($\pi_{i}$) output by \btv{} ($f$).
The agent's trajectory (the diamond containing $\Delta x_{1}$) is then sampled from the selected uniform distribution, which has bounds $-\infty < a_{i} < b_{i} < \infty$.
At the start of the next time step, the agent's position is $x_{2} = x_{1} + \Delta x_{1}$.
Maximizing the likelihood of \btv{} as a classifier over the binned trajectories is thus equivalent to maximizing its likelihood when assuming the trajectories are generated from a mixture of uniform distributions that partition the Euclidean trajectory space (see Section \ref{sec:baller2vec_generative} for details).
}
\label{fig:graphical_model}
\end{wrapfigure}
Let $[\Delta x_{min}, \Delta x_{max})$ be an interval on the real line such that any trajectory $\Delta x < \Delta x_{min}$ or $\Delta x \geq \Delta x_{max}$ has zero density (i.e., such trajectories are humanly impossible).
Let $\{[a_{i}, b_{i})\}_{i=1}^{n}$ be a set of $n$ intervals that partition the interval $[\Delta x_{min}, \Delta x_{max})$ into $n$ bins, i.e., $\cup_{i=1}^{n} [a_{i}, b_{i}) = [\Delta x_{min}, \Delta x_{max})$ and $i \neq j \implies [a_{i}, b_{i}) \cap [a_{j}, b_{j}) = \emptyset$.
Recall that the probability density function (PDF) for a uniform distribution with bounds $-\infty < a < b < \infty$ is:

\[
    p(\Delta x) = 
    \begin{cases}
        \frac{1}{b - a} & \text{for } \Delta x \in [a,b)  \\
        0               & \text{otherwise}
    \end{cases}
\]

\noindent
Letting $c_{i} = \frac{1}{b_{i} - a_{i}}$, the PDF for a mixture of uniforms with these bounds is thus:

\begin{equation}\label{eq:mix_uniforms}
    p(\Delta x) = \sum_{i=1}^{n} \pi_{i} \mathcal{U}(\Delta x; a_{i}, b_{i}) = \sum_{i=1}^{n} \pi_{i} c_{i}
\end{equation}

\noindent
where $p(\Delta x)$ is the density assigned to $\Delta x$ by the mixture, $\pi_{i}$ is the mixture proportion for the mixture component indexed by $i$ (i.e., $0 \leq \pi_{i} \leq 1$ and $\sum \pi_{i} = 1$), and $\mathcal{U}(\Delta x; a_{i}, b_{i})$ is the density assigned to $\Delta x$ by the uniform distribution with bounds $-\infty < a_{i} < b_{i} < \infty$.
Because the bounds of the uniform distributions partition $[\Delta x_{min}, \Delta x_{max})$, Equation \eqref{eq:mix_uniforms} reduces to:

\[
    p(\Delta x) = \pi_{i'} c_{i'}
\]

\noindent
where $\Delta x \in [a_{i'}, b_{i'})$ (because the other uniform distributions will assign a density of zero to $\Delta x$).
The likelihood for data $D$ (with $|D| = N$) is then:

\[
    \mathcal{L}(D) = \prod_{j=1}^{N} p(\Delta x_{j}) = \prod_{j=1}^{N} \pi_{j,i'} c_{j,i'}
\]

\noindent
where $\pi_{j,i'}$ is the mixture proportion assigned to the component with  $\Delta x_{j} \in [a_{i'}, b_{i'})$ and $c_{j,i'}$ is the associated density.
Taking the negative logarithm of the likelihood gives:

\begin{equation}\label{eq:mixture_nll}
    -\ln(\mathcal{L}(D)) = -\sum_{j=1}^{N} \ln(\pi_{j,i'}) -\sum_{j=1}^{N} \ln(c_{j,i'})
\end{equation}

\noindent
Because the bounds are fixed, the second summation is a constant, and Equation \eqref{eq:mixture_nll} becomes:

\[
    -\ln(\mathcal{L}(D)) = -\sum_{j=1}^{N} \ln(\pi_{j,i'}) + C
\]

\noindent
where $C = -\sum_{j=1}^{N} \ln(c_{j,i'})$.
Therefore, minimizing the loss of \btv{} as a classifier of binned trajectories is equivalent to minimizing the loss of the model when assuming the trajectories are generated from a mixture of uniform distributions as specified in Equation \eqref{eq:mix_uniforms}.