

\documentclass[11pt]{article}

\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}







\usepackage[utf8]{inputenc}

\usepackage{tabularx}
\usepackage{tabu}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{graphicx} 
\usepackage{float} 
\usepackage{subfigure} 
\usepackage{amssymb}
\usepackage{fdsymbol}

\usepackage{hyperref}
\usepackage{tablefootnote}
\usepackage{xspace}


\usepackage{float}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}




\definecolor{amber}{rgb}{1.0, 0.75, 0.0}
\newcommand{\draftonly}[1]{#1} 
\newcommand{\draftcomment}[3]{\draftonly{{\textcolor{#3}{[\textbf{#1--\textsc{#2}}]}}}}
\iffalse
    \newcommand{\yilun}[1]{\textcolor{blue}{}}
    \newcommand{\rui}[1]{}
\else
    \newcommand{\yilun}[1]{\textcolor{blue}{\bf\small [Yilun: #1]}}
    \newcommand{\rui}[1]{\draftcomment{#1}{rui}{amber}}
    \newcommand{\linyong}[1]{\draftcomment{#1}{linyong}{violet}}
\fi
\newcommand{\ours}{\textsc{ReasTAP}\xspace}
\newcommand{\wikisql}{\textsc{WikiSQL-Weak}\xspace}
\newcommand{\tabfact}{\textsc{TabFact}\xspace}
\newcommand{\wtq}{\textsc{WikiTQ}\xspace}
\newcommand{\sqa}{\textsc{SQA}\xspace}
\newcommand{\logicnlg}{\textsc{LogicNLG}\xspace}
\newcommand{\nreason}{7\xspace}

\newcommand{\tapas}{\textsc{TaPas}\xspace}
\newcommand{\tabert}{\textsc{TaBERT}\xspace}
\newcommand{\tapex}{\textsc{TaPEx}\xspace}

\title{\ours: Injecting Table Reasoning Skills During Pre-training \\ via Synthetic Reasoning Examples}

\author{Yilun Zhao \quad Linyong Nan \quad Zhenting Qi \quad Rui Zhang \quad Dragomir Radev\\
Yale University \quad  Zhejiang University \quad Penn State University \\
\texttt{\{yilun.zhao, linyong.nan\}@yale.edu}
}

\begin{document}
\maketitle
\begin{abstract}
Reasoning over tabular data requires both table structure understanding and a broad set of table reasoning skills. Current models with table-specific architectures and pre-training methods perform well on understanding table structures, but they still struggle with tasks that require various table reasoning skills.
In this work, we develop \ours to show that high-level table reasoning skills can be injected into models during pre-training without a complex table-specific architecture design.
We define \nreason table reasoning skills, such as numerical operation, temporal comparison, and conjunction.
Each reasoning skill is associated with one example generator, which synthesizes questions over semi-structured tables according to the sampled templates.
We model the table pre-training task as a sequence generation task and pre-train \ours to generate precise answers to the synthetic examples.
\ours is evaluated on four benchmarks covering three downstream tasks including: 1) \wikisql and \wtq for Table Question Answering; 2) \tabfact for Table Fact Verification; and 3) \logicnlg for Faithful Table-to-Text Generation.
Experimental results demonstrate that \ours achieves new state-of-the-art performance on all benchmarks and delivers a significant improvement on low-resource setting.
Our code is publicly available at \url{https://github.com/Yale-LILY/ReasTAP}. \end{abstract}


\section{Introduction}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{figure/model.pdf}
    \caption{The illustration of \ours pre-training. The tables are crawled from Wikipedia. During pre-processing, we perturb the table row order to alleviate unwanted bias brought by table encoding. The colored cells are relevant facts necessary to answer the given question.  Each color corresponds to a different table reasoning skill. And each reasoning skill corresponds to an example generator, which synthesizes QA pairs over tables according to the sampled templates. We model the pre-training task as a sequence generation task and pre-train \ours to generate correct answers given the flatten table and synthetic question.}
    \label{example}
\end{figure}

Inspired by the massive success of pre-trained language models (LM) on free-form natural language (NL) tasks~\cite{devlin-etal-2019-bert, unilm, 2020t5, lewis-etal-2020-bart}, researchers have attempted to extend the pre-training to table data. Tables are a valuable form of data that organize information in a structured way. They often contain data that is organized in a more accessible manner than in unstructured texts. To adapt the pre-training paradigm on structured tabular data, previous works mainly focus on designing models with table-specific architectures and pre-training methods. This includes introducing a structure-aware attention mechanism~\cite{yin2020tabert, turl, zayats-etal-2021-representations}, adding auxiliary structure indicative embeddings~\cite{tapas1, tapas2, tuta}, and designing table-specific pre-training objectives~\cite{yin2020tabert, yu2021grappa,tuta, liu2022tapex, liu2022plog}.
While these methods are effective in understanding table structures, they increase the modeling complexity and lack interpretability on why models learns table reasoning skills during pre-training.


This paper presents a new table pre-training approach, named \ours, which enables a model to efficiently learn table structure understanding and table reasoning skills during pre-training. 
We first defined \nreason table reasoning skills, such as numerical operation and temporal comparison. As shown in Figure~\ref{example}, for each reasoning skill, a corresponding example generator was applied to synthesize Question Answering (QA) examples over tables.
We modeled the pre-training task as a sequence generation task and pre-trained a sequence-to-sequence (seq2seq) LM to generate the answer to the synthetic questions. \ours is theoretically applicable to any seq2seq LM without a table-specific architecture design. Our key insight is that if a language model can be pre-trained to generate the answers to synthetic questions, which require various table reasoning skills, it should have a great table structure understanding and table reasoning capacity, thereby conferring benefits to downstream tasks. 
The main contributions of our work can be summarized as follows: 
\begin{itemize}
    \item We develop a new table reasoning example generation pipeline, which produces a large-scale table QA corpus that requires various reasoning skills over semi-structured tables.
    \item We propose a new table pre-training method, \ours, which helps the model to learn table structure understanding and various table reasoning skills during pre-training without any table-specific architecture design.
    \item \ours is evaluated on four downstream benchmarks. Experimental results demonstrate that \ours achieves new state-of-the-art results on all of them, and delivers a great improvement on low-resource setting. 
\end{itemize}
     
 \section{Pre-training Corpus}
\newcommand{\qm}{\text{\usefont{OT1}{iwona}{m}{n}?}}

\begin{table*}[!t]
\centering
\small
\begin{tabular}{p{1.4cm}p{5cm}p{6.3cm}r}
\toprule
\textbf{Reasoning}  & 
\textbf{Example Templates} & 
\textbf{Example Questions \& Answers} &
\textbf{\%Data} \\
\midrule
    Conjunction &
    What was the \texttt{col:1} when the \texttt{col:2} was \texttt{CONDITION:2} and the \texttt{col:3} was \texttt{CONDITION:3}? &
    \textbf{Q:} What was the \textbf{Television Service} when the \textbf{Country} was \textit{Italy} and the \textbf{Content} was \textit{Sport}? \newline
    \textbf{A:} Sky OMC Sports, ESPN, Gazzetta TV, ... &
21.6\%
    \\
    \midrule
    
    Quantifiers Only/Every &
    Does \texttt{OPERATOR} \texttt{col:1}, with \texttt{col:2} was \texttt{CONDTION:2}, have \texttt{col:3} \texttt{CONDITION:3}? &
    \textbf{Q:} Does \textit{every} \textbf{Company}, with \textbf{Headquarter} was \textit{Paris}, have \textbf{Industry} \textit{Financials}? \newline
    \textbf{A:} Yes \newline 
    \textbf{Q:} Does \textit{only} \textbf{Company Name}, with \textbf{Founded Year} was \textit{later than 1964}, have \textbf{Employee Number} \textit{greater than 30,000}? \newline
    \textbf{A:} No &
    10.3\%
    \\
    \midrule
    
    Temporal \newline Comparison &
    Which \texttt{col:1}, with \texttt{col:2} was \texttt{CONDITION:2}, happened the \texttt{ORDINAL} according to \texttt{col:3}? 
&
    \textbf{Q:} Which \textbf{Romaji}, with \textbf{Sales} was \textit{greater than 203,471}, happened the \textit{4th} according to \textbf{Date}? \newline 
    \textbf{A:} Hepburn
    &
    14.5\%
    \\
    \midrule
    
    Date \newline Difference & 
    how much time had passed between when the \texttt{col:1} was \texttt{val:1} and when the \texttt{col:2} was \texttt{val:2}? 
    &
    \textbf{Q:} how much time had passed between when the \textbf{Candidate} was \textit{John Kufuor} and when the \textbf{Candidate} was \textit{Paul McCartney}? \newline
    \textbf{A:} 16 years 
    &
    5.7\%
    \\
    \midrule
    
    Counting &
    How many \texttt{col:1} have \texttt{col:2} \texttt{CONDITION:2}? &
    \textbf{Q:} How many \textbf{Event Location} have \textbf{Attendance} \textit{greater than 10,235}? \newline
    \textbf{A:} 7 
&
    18.0\%
    \\
    \midrule
    
    Numerical \newline Operation &
    What was the {OPERATOR} of \texttt{col:1} when the \texttt{col:2} was \texttt{CONDITION:2}? &
    \textbf{Q:} What was the \textit{sum} of \textbf{GDP Estimate (\ US Million)} was \textit{greater than 841,969}? \newline
\textbf{A:}  1,574,013 &
    15.9\%
    \\
    \midrule
    
    Numerical \newline Comparison & 
    Which \texttt{col:1}, with \texttt{col:2} was \texttt{CONDITION:2}, has the \texttt{ORDINAL} \texttt{col:3}? &
\textbf{Q:} Which \textbf{Franchise}, with \textbf{Owner(s)} was \textit{Nintendo}, has the \textit{5th} \textbf{Total revenue(\TT\bm{q}\bm{a}\bm{q}\bm{a}\bm{q}\bm{a}\bm{q}T\bm{a}=(a_1, a_2, \dots, a_n)\bm{q}T\thetaTL \in \{0, 1\}\bm{s}=(s_1,s_2,{\cdots},s_n)TT\boldsymbol{y}=(y_1, y_2, \dots, y_n)||||||||||||\dots||||||||||||\dots||||||||||||\dots\dots$ & John Duren played for Utah Jazz for 2 years.\\
\bottomrule
\end{tabular}
\caption{
The example inputs and outputs for our model on experimental datasets.
} 
\label{table: downstream_example}
\end{table*}
  \end{document}
