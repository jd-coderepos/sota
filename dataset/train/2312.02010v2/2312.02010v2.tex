





\section{Experimental Setup}
In this section, we first introduce the implementation details (\S \ref{sec:implementation}). Then we describe the evaluation datasets, metrics, and baseline methods for VLN (\S \ref{sec:exp_vln}), 3D-QA (\S \ref{sec:exp_3dqa}) and EQA (\S \ref{sec:exp_eqa}), respectively.

\subsection{Implementation Details}
\label{sec:implementation}

\para{Model Details.} We fine-tune the multi-view fusion module and LLM, where the former consists of a 2-layer transformer encoder with a hidden size of 1024 and the LLM is built upon Vicuna-7B-v0 \cite{peng2023instruction}. The ViT in the scene encoder is EVA-CLIP-02-Large (428M) \cite{EVA-CLIP} and is kept frozen during the training phase.
In addition, we leverage the object features extracted from ViT-B16 by Chen \textit{et al.} \cite{chen2022learning}.


\para{Training Details.} We follow the previous works \cite{Chen_2022_CVPR, Gao_2023_CVPR, Chen_2022_HM3D_AutoVLN} to employ a two-stage training strategy.
Throughout both stages, we utilize the Adam optimizer with a learning rate of 3e-5. The model is trained for 10,000 steps in the pre-training stage and 5,000 steps in the multi-task fine-tuning stage with a batch size of 64.
It takes approximately 80 hours with 8 Nvidia A100 GPUs.
During the testing phase, we employ a sampling strategy with a temperature of 0.01 for action generation in the SOON and REVERIE tasks, to encourage more exploration. For other tasks, we opt for a greedy strategy in generating actions.

\para{Training Data.} In the pre-training stage, we perform teacher-forcing training on the combined dataset from CVDN, SOON, R2R, REVERIE, ScanQA, and augmented data from R2R and REVERIE. 
In the multi-task fine-tuning stage, we alternate between teacher forcing and student forcing on the combined dataset from CVDN, SOON, R2R, REVERIE, ScanQA, and LLaVA-23k~\cite{liu2023visual}.

For object localization, we utilize the corresponding annotations from REVERIE and SOON.
For trajectory summarization, we convert the instruction-trajectory pairs of VLN datasets into trajectory-instruction pairs, where the trajectory serves as input and the instruction as output. 
As for 3D-QA, in addition to ScanQA, we also construct question-answer pairs from the fine-grained annotations on R2R~\cite{hong2020sub}.
In concrete, the constructed questions ask the model to predict the corresponding sub-instruction for a selected viewpoint.
Additionally, we held out the task of EQA to verify the generalization capability on out-of-domain tasks.


\subsection{Setup for VLN} 
\label{sec:exp_vln}

\para{Datasets.} We adopt four datasets, each addressing distinct challenges posed by VLN. These datasets are split into train, val-seen, val-unseen, and test sets according to environments.

\begin{itemize}[leftmargin=*]
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item{\textbf{CVDN}~\cite{thomason2020vision}} requires the agent to navigate towards the target based on a dialog history, thereby requiring the ability to comprehend the dialog and interpret it as actions.

\item{\textbf{SOON}~\cite{zhu2021soon}} asks the agent to locate a thoroughly described object, which necessitates intricate alignment between rich semantic descriptions and the corresponding visual cues.

\item{\textbf{R2R}~\cite{anderson2018vision}} demands the agent to navigate following a step-by-step instruction.
To make effective decisions, it requires the agent to dynamically track the progress, demanding fine-grained alignment between history and instructions.

\item{\textbf{REVERIE}~\cite{qi2020reverie}} requires the agent to localize a distant target object according to a concise high-level instruction.

\end{itemize}

\para{Metrics.} 
We follow \cite{anderson2018vision} to evaluate our method on the following metrics: 
1) Sucess Rate (\textbf{SR}), whether the agent successfully reaches the target location within a predefined distance threshold. 
2) Success Rate Weighted by Path Length (\textbf{SPL}), calculated as the SR weighted by the ratio of the ground truth length and actual path length. 
3) Oracle Success Rate (\textbf{OSR}), SR given the oracle stop strategy.
4) Trajectory Length (\textbf{TL}), the overall distance covered by the agent during navigation. 
5) Goal Process (\textbf{GP}), the progress in meters towards the goal.
GP is adopted for the CVDN dataset, while SPL is employed as the primary evaluation metric for other datasets.

\para{Baseline Methods.} We compare our method with the latest SoTA methods on the CVDN, SOON, R2R, and REVERIE datasets. We do not consider methods with pre-exploration (e.g., AuxRN \cite{zhu2020vision}, RREx-BoT \cite{sigurdsson2023rrex}), and models augmented by newly environments (e.g., HM3D-AutoVLN \cite{Chen_2022_HM3D_AutoVLN}).

\subsection{Setup for 3D-QA} 
\label{sec:exp_3dqa}
\para{Dataset.}
ScanQA dataset \cite{azuma2022scanqa} is a widely used dataset for 3D-QA, which is divided into train, val, and test sets. Here, we use val and `test w/ objects' sets for comparison. 
For the results on `test w/o objects' set, please refer to the appendix.

\para{Metrics.} We follow \cite{hong20233dllm} to evaluate our method with Exact Match (\textbf{EM}), METEOR, ROUGE-L, CIDER, and BLEU-4. 

\para{Baseline Methods.} We include some representative methods for comparison, including VoteNet+MCAN \cite{yu2019mcan}, ScanRefer+MCAN \cite{yu2019mcan}, and 3D-LLM \cite{hong20233dllm}. 3D-LLM is the current SoTA method on the ScanQA benchmark.

\subsection{Setup for EQA} 
\label{sec:exp_eqa}

\para{Dataset.} 
We test the zero-shot inference capability on the val split of the MP3D-EQA~\cite{wijmans2019embodied} dataset.
Since MP3D-EQA is generated from functional programs, there are some data with inaccurate endpoints. Therefore, we manually check the dataset and filter out those invalid data in our experiments.



\para{Metrics.} We report SR and SPL for the navigation phase, and Accuracy (\textbf{ACC}) for the question-answering phase.

\para{Baseline Methods.} 
We compare our \modelname~with the fully-supervised VQA model~\cite{embodiedqa} and zero-shot DUET models~\cite{Chen_2022_CVPR} separately trained on R2R, REVERIE, and SOON.











\begin{table*}[]
\centering
\renewcommand\arraystretch{1}
\setlength{\tabcolsep}{3.2mm}{
\begin{tabular}{l|cc|cc|cc|cc|cc} \toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{CVDN}} & \multicolumn{2}{c}{\textbf{SOON}} & \multicolumn{2}{c}{\textbf{R2R}} & \multicolumn{2}{c}{\textbf{REVERIE}} & \multicolumn{2}{c}{\textbf{ScanQA}} \\ 
\multicolumn{1}{c}{} & Val-U & \multicolumn{1}{c}{Test} & Val-U & \multicolumn{1}{c}{Test}  & Val-U & \multicolumn{1}{c}{Test} & Val-U & Test & \multicolumn{1}{c}{Val} & Test \\ \midrule
\multicolumn{11}{c}{\textit{\textbf{Separate Model For Each Task}}} \\ \hline
PREVALENT \cite{hao2020towards} & 3.15 & 2.44 & - & -  & 53 & 51 & - & -  & - & - \\
HOP \cite{Qiao2022HOP} & 4.41 & 3.24 & - & & 57 & 59 & 26.11 & 24.34 & - & - \\
HAMT \cite{NEURIPS2021_2e5c2cb8} & 5.13 & 5.58  & - & -  & 61 & \underline{60} & 30.20 & 26.67 & - & - \\
VLN-BERT \cite{Hong_2021_CVPR}& - & - & - & -  & 57 & 57 & 24.90 & 23.99 & - & -\\
GBE \cite{zhu2021soon} & - & - & 13.34 & 9.23 & - & - & - & - & - & - \\
DUET \cite{Chen_2022_CVPR}& - & - & 22.58 & \underline{21.42}  & 60 & 58 & 33.73 & \underline{36.06} & - & - \\
Meta-Explore \cite{Hwang_2023_CVPR} & - & - & - & \underline{25.80}  & 62 & \textbf{61} & 34.03 & - & - & - \\
AZHP \cite{Gao_2023_CVPR} & - & - & - & - & 61 & \underline{60} & \textbf{36.63} & 35.85 & - & -\\
VLN-SIG \cite{li2023vlnsig} & 5.52 & 5.83 &- & - & \underline{62} & \underline{60} & - & - & - & - \\
VLN-PETL \cite{qiao2023vln} & \underline{5.69} & \underline{6.13} & - & - & 60 & 58 & 27.67 & 26.73 & - & -\\
BEV-BERT \cite{an2023bevbert} & & & & & \textbf{64} & \underline{60} & \underline{36.37} & \textbf{36.41} & - & - \\
3D-LLM \cite{hong20233dllm} & - & - & - & - & - & - & - & - & \underline{20.5} & \underline{19.1} \\ \hline
\multicolumn{11}{c}{\textit{\textbf{Unified Model For All Tasks}}} \\ \hline
MT-RCM+Env \cite{wang2020environment} & 4.65 & \multicolumn{1}{c}{3.91} & - & \multicolumn{1}{c}{-}  & 49 & \multicolumn{1}{c}{40} & - & \multicolumn{1}{c}{-} & - & - \\
\modelname\ & \textbf{6.16} & \multicolumn{1}{c}{\textbf{7.90}} & \textbf{29.24} & \multicolumn{1}{c}{\textbf{26.26}} & 59 & \multicolumn{1}{c}{\underline{60}} & 35.68 & \multicolumn{1}{c}{32.33} & \textbf{23.0} & \textbf{26.3} \\
\bottomrule
\end{tabular}}
\caption{Overall comparison with state-of-the-art methods on all tasks. `Val-U' denotes val-unseen split.}
\label{table:performance}
\end{table*}



\begin{table*}[t]
\centering
\renewcommand\arraystretch{1}
\resizebox{0.99\textwidth}{!}{
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{l|ccccc|ccccc} \toprule
& \multicolumn{5}{c|}{ScanQA-Val} & \multicolumn{5}{c}{ScanQA-Test} \\
& EM & ROUGE-L & METEOR & CIDER & BLEU-4 &  EM & ROUGE-L & METEOR & CIDER & BLEU-4 \\ \midrule
VoteNet+MCAN \cite{yu2019mcan} & 17.3 & 29.8 & 11.4 & 54.7 & 6.2 & 19.7 & 30.9  & 12.0& 58.2 & 6.0 \\
ScanRefer+MCAN \cite{yu2019mcan} & 18.6 & 30  & 11.5& 55.4 & 7.9 & 20.6  & 30.7 & 11.9 & 57.4 & 7.5 \\
ScanQA \cite{azuma2022scanqa} & 21.0 & 33.3  & 13.1& 64.9 & 10.1 & 23.5  & 34.3 & 13.5& 67.3 & 12.0 \\ 
3D-LLM (flamingo) \cite{hong20233dllm} & 20.4 & 32.3  & 12.2& 59.2 & 7.2 & 23.2& 34.8  & 13.5 & 65.6 & 8.4 \\
3D-LLM (BLIP2-flant5) \cite{hong20233dllm} & 20.5 & 35.7 & 14.5& 69.4 & 12.0 & 19.1 & 35.3  & 14.9 & 69.6 & 11.6 \\

\textbf{\modelname} (Ours) & \textbf{23.0} & \textbf{38.4} & \textbf{15.4} & \textbf{75.9} & \textbf{12.5}  & \textbf{26.3} & \textbf{40.2} & \textbf{16.6} & \textbf{80.8} & \textbf{13.9} \\
\bottomrule
\end{tabular}}
}
\caption{Detail comparison with state-of-the-art methods on ScanQA.}
\label{table:scanqa_sota}
\end{table*}








\section{Experimental Results}
\label{sec:experiments}
We conduct a series of experiments to answer three critical questions about \modelname: 
(1) Can \modelname, when trained with diverse tasks, demonstrate superior performance compared to existing SoTA methods (\S \ref{sec:performance})?
(2) How well does \modelname~generalize to unseen tasks, compared to previous task-specific models (\S \ref{sec:generalization})?
(3) What is the impact of each component in our method (\S \ref{sec:ablation})?
Lastly, we also provide visualization for \modelname~on unseen scenes and tasks (\S \ref{sec:visualization}).







\subsection{Comparision with SoTA Methods}
\label{sec:performance}

\para{Delivering SoTA Results with a Single Model.}
We present the comparison across all tasks in Table \ref{table:performance}.
Our single model achieves SoTA performance on the test sets of the CVDN, SOON, and ScanQA datasets, and demonstrates comparable results to the latest SoTA methods on R2R and REVERIE.



\para{Significant Improvement on CVDN Can Be Credited to Our Innovative Design.} 
Compared to the 6.13 GP of VLN-PETL \cite{qiao2023vln}, our method shows a significant increase in the GP at 7.90, winning the first place on the leaderboard of CVDN.
Compared to other datasets, the improvement on CVDN is the most pronounced. 
We attribute this significant improvement primarily to two factors: 1) The dialog structure in CVDN is relatively complex, and the knowledge inherited from LLM in our model can better help comprehend the dialog. 2) Given that the size of the CVDN dataset is smaller compared to other datasets, the unification of these datasets effectively mitigates the issue of data scarcity.


\para{Our Method Also Excels in 3D Tasks.} As shown in Table \ref{table:scanqa_sota}, our method achieves SoTA results on the val and test sets in all metrics.
It obtains a 26.3\% EM, with a 7.2\% improvement over 3D-LLM, which is specially designed for 3D tasks.

\para{Better Performance on Tasks with Complex Instructions.}
We count the average length of instructions across different datasets, with CVDN averaging 81.6 words per instruction, SOON averaging 38.6 words, R2R averaging 29 words, and REVERIE averaging 18 words. 
This reflects the complexity of the instructions to some extent.
We notice that our method exhibits superior performance on datasets with complex instructions, such as CVDN, SOON, and R2R, achieving performance better than or comparable to SoTA methods.
However, there is still a slight gap with DUET on datasets with relatively simple instructions, such as REVERIE.
This may indicate that our method possesses excellent instruction comprehension capabilities, which helps improve the performance on tasks with complex instructions.

\para{\textit{\modelname} Demonstrates An Excellent Object Localization Capability.} 
In the object localization task of REVERIE, our method achieves 19.83\% RGS and 16.04\% RGSPL, and outperforms the 14.88\% and 13.08\% achieved by HAMT \cite{NEURIPS2021_2e5c2cb8}, demonstrating an excellent object localization capability. 
Given that existing methods typically integrate object features with image features, we believe that our method could be further enhanced by combining these features.

\begin{table*}[]
\centering
\renewcommand\arraystretch{1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{l|cc|cccc|cccc} \toprule
& \multicolumn{2}{c|}{CVDN}  & \multicolumn{4}{c|}{SOON}  & \multicolumn{4}{c}{REVERIE} \\ 
& TL & \textbf{GP$\uparrow$} & TL & \textbf{OSR$\uparrow$} & \textbf{SR$\uparrow$} & \textbf{SPL$\uparrow$} & TL & \textbf{OSR$\uparrow$} & \textbf{SR$\uparrow$} & \textbf{SPL$\uparrow$} \\ \midrule
DUET (R2R)  & 21.12 & 3.38 & 26.83 & 7.64 & 4.66 & 2.84 & 7.88 & 29.11 & 24.91 & 20.00  \\
DUET (REVERIE) & 76.13 & 3.30 & 33.72 & 20.86 & 10.24 & 6.06 & - & - & - & -  \\
DUET (SOON) & 48.61 & 2.40 & - & - & - & - & 38.10 & 43.45 & 10.91 & 3.64 \\
\textbf{\modelname} & 26.37  &\textbf{4.46} &28.66 & \textbf{33.11} & \textbf{19.81} & \textbf{14.29} & 18.96 & \textbf{51.47} & \textbf{28.10} & \textbf{21.04}  \\
\bottomrule
\end{tabular}}
\caption{Held-out results on val-unseen splits of CVDN, SOON and REVERIE. We only perform the multi-task fine-tuning for held-out experiments.
Trajectory Length (TL) serves as a statistical indicator rather than an evaluation metric.
}
\label{table:held_out}
\end{table*}

\begin{table}[]
\centering
\renewcommand\arraystretch{1.}
\setlength{\tabcolsep}{0.8mm}{
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|lc|cccc} \toprule
\multirow{2}{*}{\#}& & \multirow{2}{*}{GT Path} & \multicolumn{3}{c}{Navigation} & \multicolumn{1}{c}{QA} \\
& & &TL & \textbf{SR$\uparrow$} & \textbf{SPL$\uparrow$} & \textbf{ACC$\uparrow$}  \\ \midrule
1 & DUET (R2R)~\cite{Chen_2022_CVPR} &  & 16.47 & 47.00 & 30.51 & - \\
2 & DUET (REVERIE)~\cite{Chen_2022_CVPR} & & 12.59 & 39.22 & 11.47 & -\\
3 & DUET (SOON)~\cite{Chen_2022_CVPR} & & 47.01 & 17.43 & 3.91 & -\\
4 & \textbf{\modelname} (Ours) & & 14.15 & 47.78 & 35.60 & 44.5 \\  \hline
5 & EQA (habitat-lab)$\dagger$~\cite{embodiedqa} & \checkmark & - & - & - & 46.0 \\ 
6 & \textbf{\modelname} (Ours) & \checkmark & - & - & - & 47.4 \\ 
\bottomrule
\end{tabular}
}
}
\caption{Zero-shot inference results on MP3D-EQA. `GT Path' means using the ground truth trajectory for question answering. $\dagger$ indicates the method is finetuned on the training set of MP3D-EQA. 
Trajectory Length (TL) serves as a statistical indicator rather than an evaluation metric.
}\label{table:eqa}
\end{table}



\begin{table*}[]
\centering
\renewcommand\arraystretch{1}
\setlength{\tabcolsep}{1.9mm}{
\begin{tabular}{l|ccc|c|cc|cc|cc|cc} \toprule
\multirow{2}{*}{\textbf{\#}} & \multirow{2}{*}{\textbf{LLM}} & \multirow{2}{*}{\textbf{Multi-Task}} & \multirow{2}{*}{\textbf{Pretrain}}  & \multicolumn{1}{c|}{CVDN} & \multicolumn{2}{c|}{SOON}& \multicolumn{2}{c|}{R2R}  & \multicolumn{2}{c|}{REVERIE} & \multicolumn{2}{c}{ScanQA} \\
& & &  & GP & SR & SPL & SR & SPL& SR & SPL & EM & ROUGE-L \\ \midrule
1& \checkmark & & & 5.54 & 28.37 & 21.37 & 64 & 57  & 30.95 & 24.10 & 21.8 & 37.0 \\
2& & \checkmark & & 3.64 & 20.73 & 17.39  & 49 & 40 & 31.49 & 26.87 & 12.4 & 22.8 \\
3& \checkmark & \checkmark &  & 5.91 & 35.44 & 28.09 & \textbf{67} & 58  & \textbf{44.56} & \textbf{36.63} & \textbf{23.3} & \textbf{38.2}  \\
4& \checkmark & \checkmark & \checkmark & \textbf{6.16} & \textbf{38.33} & \textbf{29.24}  & \textbf{67} & \textbf{59} & 42.15 & 35.68 & 22.1 & 37.6 \\
\bottomrule
\end{tabular}}
\caption{Ablation study of \modelname\ across all tasks.
`LLM', `Multi-Task', and `Pretrain' denote the utilization of pretrained LLM weights for initialization, the execution of multi-task learning, and the performance of pre-training, respectively.
The results reported are from the val-unseen splits for VLN tasks and the val split for ScanQA.}
\label{table:ablation}
\end{table*}










\subsection{Generalization Ability on Unseen Tasks}
\label{sec:generalization}
We evaluate the zero-shot inference performance of our method on unseen tasks, and compare it with zero-shot DUET models (DUET (R2R), DUET (REVERIE), and DUET (SOON)), with each model being separately trained on its corresponding dataset.

\para{Generalize to Out-of-Domain VLN Tasks.}
We conduct held-out experiments to verify the generalization ability to out-of-domain VLN tasks. 
Specifically, we individually exclude CVDN, SOON, and REVERIE from the training set, train three separate models, and then test their zero-shot performance on the respective excluded datasets.
DUET specially designs different hyper-parameters and pre-training schemes for each VLN task, giving the model strong in-domain navigation capabilities. However, such learned task-specific agents lack out-of-domain generation abilities. As illustrated in Table \ref{table:held_out}, our method significantly outperforms DUET on CVDN and SOON, improving 32\% of GP on CVDN and 136\% SPL on SOON, respectively. Since instructions in REVERIE are relatively simpler and similar to R2R, DUET (R2R) delivers an SR of 24.91\% on REVERIE, but we still achieve a better SR of 28.10\%. 
This demonstrates that our schema-based instruction and multi-task learning empower the out-of-domain generation abilities.


\para{Skill Combination for EQA.}
We perform a zero-shot evaluation on MP3D-EQA to show that \modelname~can combine the learned navigation and question-answering ability to solve more complex tasks. 
We ask our agent to first execute the navigation process and then answer the question after reaching the goal.
As illustrated in Table \ref{table:eqa}, our model achieves 47.78\% SR and 35.60\% SPL, surpassing DUET (R2R) by 5.1\% in SPL (row 1 vs. 4).
At the same time, it can also answer questions at a decent accuracy of 44.5\%, while the DUET models are incapable of performing question answering.
Moreover, when the ground truth trajectories are provided, our zero-shot model presents superior performance over the fully-supervised EQA model (rows 5 vs. 6).


\begin{figure*}[t] \centering \includegraphics[width=0.95\textwidth]{figs/visualization.pdf} \caption{The visualization for our method on unseen scenes and unseen tasks.
In Figure (a), lines and text of the same color represent sub-trajectories and their corresponding sub-instructions.
In Figures (b) and (c), the text in gray is the description of the actions of the agent during navigation, while the red arrow indicates the direction that the agent moves towards.}
\label{fig:visulaiaztion}
\end{figure*}


\subsection{Ablation}
\label{sec:ablation}

\para{Multi-task Learning Enhances the Performance On All Tasks.}
Table \ref{table:ablation} illustrates that multi-task learning improves performance on all tasks (row 1 vs. 3).
This demonstrates that expanding the volume and diversity of training data is crucial for learning a generalist model for embodied navigation.

\para{LLM plays a Key Role in Our Method.}
Comparing rows 2 and 3, we can observe a significant performance drop when the LLM is randomly initialized, underscoring the substantial role that the LLM plays.


\para{Limited Benefits of Pre-Training on Augmented Data.}
Previous works \cite{Chen_2022_CVPR, Gao_2023_CVPR} have consistently shown notable improvements after pre-training on augmented data from R2R and REVERIE. 
However, comparing rows 3 and 4, we find only a slight enhancement on R2R, CVDN, and SOON after pre-training. 
We speculate that the quality of the data may play a more crucial role than its quantity for our method.

\subsection{Visualization}
\label{sec:visualization}
Figure \ref{fig:visulaiaztion} (a) and (b) are examples of trajectory summarization and object navigation on unseen scenes, respectively.
The first example illustrates that our model can generate accurate step-by-step instructions given trajectories, which could be further used for data augmentation.
Figure \ref{fig:visulaiaztion} (c) and (d) respectively present examples of EQA and 3D captioning, which are not encountered in training data, demonstrating the generalizability of \modelname.
Specifically, as shown in Figure \ref{fig:visulaiaztion} (d), our model is capable of producing captions of varying granularity according to the instructions.






