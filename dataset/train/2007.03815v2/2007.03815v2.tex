\section{Experiments}
\subsection{Datasets and Evaluation}
\textit{Cityscapes}\cite{cordts2016cityscapes} is a large benchmark containing 19 semantic classes for urban scene understanding with 2975/500/1525 scenes for train/validation/test respectively. \textit{CamVid}\cite{brostow2008segmentation} is another street-view dataset with 11 classes. The annotated frames are divided into 367/101/233 for training/validation/testing. \textit{COCO-Stuff}\cite{caesar2018coco} contains both diverse indoor and outdoor scenes for semantic segmentation. This dataset has 9,000 densely annotated images for training and 1,000 for testing.  Following previous work\cite{zhao2018icnet}, we adopt the resolution 640640 and evaluate on 182 classes including 91 for things and 91 for stuff. 
We evaluate our method on image semantic segmentation for all four datasets, and additionally evaluate on Cityscapes for video semantic segmentation. The mIoU (mean Intersection over Union) is reported for evaluation.




\subsection{Implementation Details}


We use ResNet-18/34~\cite{he2016deep} pretrained on Imagenet as the encoder in FANet, and randomly initialize parameters in fast attention modules as well as the decoder network.
We train using mini-batch stochastic gradient descent (SGD) with batchsize 16, weight decay 5e, and momentum 0.9. The learning rate is initialized as 1e, and multiplied with  after each iteration.
We apply data augmentation including random horizontal flipping, random scaling (from 0.75 to 2), random cropping and color jittering in the training process.
During testing, we input images at full resolution, and resize the output to the original size for calculating the accuracy.
All the evaluation experiments are conducted with batchsize 1 on a single Titan X GPU. 





\begin{table}[]
\renewcommand\arraystretch{0.85}
\small
\begin{tabular}{p{1.7cm}p{0.6cm}p{0.6cm}p{0.6cm}p{0.6cm}p{0.6cm}p{0.6cm}} 
\toprule
\textit{C=}    &~~32   &~~64  &~128  &~256  &~512 &~1024\\
\midrule 
Self-Att.\cite{vaswani17att}  &~~68   &~103  &~173  &~313  &~602 &~1203\\
\textbf{Ours}         &~~0.2  &~~0.6 &~1.7   &~~5   &~~19 &~~~73\\
\bottomrule
\end{tabular}
\vspace{-0.1cm}
\caption{GFLOPs for non-local  module~\cite{wang2018non} and our fast attention module with C128256 features as input.}
\label{tab:cmp_flops}
\vspace{-0.6cm}
\end{table}


\iffalse
\begin{table}[t]
\renewcommand\arraystretch{0.85}
\small
\begin{tabular}{p{1.7cm}p{0.6cm}p{0.6cm}p{0.6cm}p{0.6cm}p{0.6cm}p{0.6cm}} 
\toprule
   \scriptsize{\textit{C=}}   &\scriptsize{~~32}        &\scriptsize{~~64} &\scriptsize{~128} &\scriptsize{~256} &\scriptsize{~512}&\scriptsize{~1024}\\
 \midrule 
\scriptsize{Self-Att.\cite{vaswani17att}}  &\scriptsize{~~35}     &\scriptsize{~~43}    &\scriptsize{~~64}  &\scriptsize{~~83}  &\scriptsize{~149}&\scriptsize{~~297}\\
\scriptsize{\textbf{Ours}}         &\scriptsize{~~0.8}    &\scriptsize{~~1.4}    &\scriptsize{~~~3}  &\scriptsize{~~~5}  &\scriptsize{~~12}&\scriptsize{~~~30}\\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{\footnotesize{Runtime (in ms) for non-local module~\cite{wang2018non} and our fast attention module with C128256 feature as input.}}
\label{tab:cmp_speed}
\vspace{-0.2cm}
\end{table}
\fi

\subsection{Method Analysis}
\label{sec:ablations}
\noindent\textbf{Fast Attention.} We first show the advantage in efficiency due to our fast attention. 
In Table~\ref{tab:cmp_flops}, we compare GFLOPs between a single original self-attention module and our fast attention module. 
Note that our fast attention runs significantly more efficiently for different size input features with more than 94 less computation.

We also compare our fast attention to the original self-attention module~\cite{vaswani17att} in our FANet. 
As shown in Table~\ref{tab:atn}, compared to the model without attention (denoted as ``w/o Att.''), applying the original self-attention module to the network increases mIoU by 2.4 while decreasing the speed from 83 fps to 8 fps.
In contrast to the original self-attention module, our fast attention (denoted as ``FA with L2-norm'') can achieve only slightly worse quality performance while greatly saving the computation cost.
To further analyze our cosine-similarity based fast attention, we also train without the L2-normalization for both \textit{Query} and \textit{Key} features (denoted as ``FA w/o L2-norm'') and achieve 74.1 mIoU on the Cityscapes , which is lower than 75.0  mIoU of our full model. 
This validates the necessity of cosine similarity to ensure bounded values for affinity computation.

 \begin{table}[t!]
\centering
\small
\begin{tabular}{cccccc} 
\toprule
  Channel (')    &~8  &~16  &~32  &~64  &~128\\ 
 \midrule 
 mIoU () &73.5 &74.6 &75.0 &75.0 &75.0\\ 
 Speed (fps) &~74 &~74 &~72 &~69 &~65\\ 
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\caption{\footnotesize{Performance on Cityscapes \textit{val} for different channel numbers (') in fast attention in FANet-18.}}
\vspace{-0.4cm}
\label{tab:chn}
\end{table}

In Table~\ref{tab:chn}, we analyze the influence of channel numbers for \textit{Key} and \textit{Query} maps in our fast attention module. 
As we can see, too few channels such as '=8 or '=16 saves computation, but limits the representing capacity of the feature and leading to lower accuracy. On the other hand, when increasing the channel number from 32 to 128, the accuracy becomes stable, yet the speed drops. 
As a result, we adopt '=32 in our experiments. 

\begin{table}
\centering
\begin{tabular}{cccc} 
\toprule
             &mIoU(\%)  &Speed(fps) &GFLOPs\\ 
 \midrule 
 w/o Att.    &~~72.7  &~~~83   &~~48\\ 
 Self-Att.\cite{vaswani17att}  &~~75.1  &~~~8    &~~121\\ 
 Channel-Att.\cite{fu2019dual} &~~74.6  &~~70    &~~51\\
 \midrule 
 FA w/o L2-norm &~~74.1  &~~~72   &~~49\\ 
 FA with L2-norm &~~75.0  &~~~72   &~~49\\ 
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\caption{\small{Performance on Cityscapes for different attention mechanisms for FANet-18. ``FA'' denotes our fast attention.}}
\vspace{-0.8cm}
\label{tab:atn}
\end{table}




\noindent\textbf{Spatial Reduction} Next, we analyze the effect of applying the extra spatial reduction at different feature stages of FANet. 
The effects of additionally down-sampling different blocks are presented in Fig.~\ref{fig:DS_loc}. 
As we can see, down-sampling before ``Conv-0'' (down-scaling the input image), reduces the computation of all the subsequent layers, but loses critical spatial details which reduces the result quality. 
``Res-1'' indicates that we reduce the spatial size at the stage of the first Res-block in FANet. 
Extra spatial reduction at higher stages like ``Res-2'', ``Res-3'', and ``Res-4'' do not increase speed significantly.
Interestingly enough, we observe that applying down-sampling to ``Res-4'' actually performs \emph{better} than ``None'', no additional downsampling. 
We hypothesize that this may be because that the block ``Res-4'' process high-level features, and adding extra down-sampling helps to enlarge the receptive field thus benefiting with rich contextual information.
Based on these observations and with an aim of real-time semantic segmentation, we choose to apply extra down-sampling to ``Res-1'' and denote the model as FANet-18/34 based on the ResNet encoder used.


\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{./Figure/DS_loc_all.pdf}
\vspace{-0.6cm}
\caption{\small{Accuracy and speed analysis on Cityscapes \textit{val} for adding an additional down-sampling operation (rate=2) to different stages of the encoder in FANet. ``Conv-0'' means to directly down-sample the input image. ``Res-'' indicates double the stride of the first Conv layer in the -th Res-block. ``None'' means no additional down-sampling operation is applied.}}
\label{fig:DS_loc} 
\vspace{-0.4cm}
\end{figure}
 
 
 

In additional to doubling the stride of convolutional layers to achieve 75.0 mIoU, we also experiment with other forms of down-sampling including average pooling (72.9 mIoU) and max pooling (74.2 mIoU). Enlarging stride for Conv layers performs the best.
This may be because that stride convolution helps to capture more spatial details while keeping sizable receptive fields.

 
 


\subsection{Image Semantic Segmentation}
We compare our final method to the recent state-of-the-art efficient approaches for real-time semantic segmentation. For fair comparisons, we evaluate the speed for different methods with PyTorch on the same Titan X GPU. 
Please check our supplementary material for details.
On benchmarks including Cityscapes~\cite{cordts2016cityscapes}, CamVid~\cite{brostow2008segmentation}, and COCO-Stuff~\cite{caesar2018coco}, our FANet achieves accuracy comparable to the state-of-the-art with the highest efficiency. 




\begin{table*}[!t]
\centering
\renewcommand\arraystretch{1}
\resizebox{.95\linewidth}{!}{
\begin{tabular}{p{2cm}p{0.9cm}p{0.9cm}p{1.8cm}p{1.4cm}p{2.5cm}p{1.6cm}}
\toprule
\multirow{2}{*}{~~~\footnotesize{Methods}}                                 & \multicolumn{2}{p{0.9cm}}{~~~~~~\footnotesize{mIoU()}} & \multirow{2}{*}{~~\footnotesize{Speed(fps)}} & \multirow{2}{*}{\footnotesize{GFLOPs}}& \multirow{2}{*}{GFLOPs\scriptsize{@1Mpx}} & ~~~\footnotesize{Input}  \\
\cline{2-3}
                                                            & ~~val                & ~~test          &                        &                    &                 & Resolution\\
\midrule 
~~\footnotesize{SegNet}~\cite{badrinarayanan2017segnet}     & ~~~~--               &~~56.1           &~~~~~~~36               &~~~~143             &~~~~~~{650}        &360640\\
~~\footnotesize{ICNet}~\cite{zhao2018icnet}                 & ~~67.7               &~~69.5           &~~~~~~~38               &~~~~~\textbf{30}    &~~~~~~~\textbf{15}   &10242048\\
~~\footnotesize{ERFNet}~\cite{romera2017efficient}          & ~~71.5               &~~69.7           &~~~~~~~48               &~~~~103             &~~~~~~{206}         &5121024\\
~~\footnotesize{BiseNet}~\cite{yu2018bisenet}               & ~~74.8               &~~74.7           &~~~~~~~47               &~~~~~67             &~~~~~~{59.5}        &7681536\\
~~\footnotesize{ShelfNet}~\cite{Zhuang_2019_ICCV_Workshops} & ~~~~--               &~~\underline{74.8}&~~~~~~~39              &~~~~~95             &~~~~~~{47.5}        &10242048\\
~~\footnotesize{SwiftNet}~\cite{orsic2019defense}           & ~~\underline{75.4}   &~~\textbf{75.5}   &~~~~~~~40              &~~~~106             &~~~~~~~{53}          &10242048\\
\midrule 
~~\textbf{\footnotesize{FANet-34}}                          & ~~\textbf{76.3}      &~~\textbf{75.5}    &~~~~~~~\underline{58} &~~~~~65             &~~~~~~{32.5}  &10242048\\
~~\textbf{\footnotesize{FANet-18}}                          & ~~75.0               &~~74.4             &~~~~~~~\textbf{72}    &~~~~~\underline{49} &~~~~~~\underline{24.5}  &10242048\\
\bottomrule 
\end{tabular}}
\vspace{-0.2cm}
\caption{\small{ Image semantic segmentation performance comparison with recent state-of-the-art real-time methods on Cityscapes dataset. ``GFLOPs@1Mpx'' shows the GFLOPs for input with resolution 1M pixels.}}
\label{tab:cityscapes}
\vspace{-0.4cm}
\end{table*}

\noindent\textbf{Cityscapes.} In Table~\ref{tab:cityscapes}, we present the speed-accuracy comparison. 
FANet-34 achieves mIoU 76.3 for validation and  75.5 for testing at a speed of 58 fps with full-resolution (10242048) inputs. 
To our best knowledge, FANet-34 outperforms existing approaches for real-time semantic segmentation with better speed and state-of-the-art accuracy. 
By adopting a lighter-weight encoder ResNet-18, our FANet-18 further accelerates the speed to 72 fps, which is nearly two times faster than the recent methods like ShelfNet~\cite{Zhuang_2019_ICCV_Workshops} and SwiftNet~\cite{orsic2019defense}. 
Although the accuracy drops to mIoU 75.0 for validation and  74.4 for testing, it is still much better than many previous methods like SegNet~\cite{badrinarayanan2017segnet} and ICNet~\cite{zhao2018icnet}, and comparable to the most recent methods like BiseNet~\cite{yu2018bisenet} and ShelfNet~\cite{Zhuang_2019_ICCV_Workshops}. 
The performance achieved by our models demonstrates the superior ability to better balance the accuracy and speed for real-time semantic segmentation. Some visual results of our method are shown in Fig.~\ref{fig:visual_res}.

\noindent\textbf{CamVid.} Results for this dataset are reported in Table~\ref{tab:coco}. As we can see, our FANet outperforms previous methods with better accuracy and much faster speed. Comparing to BiseNet~\cite{yu2018bisenet}, our FANet-18 runs  efficient, and our FANet-34 outperforms with  mIoU and a faster speed.

\noindent\textbf{COCO-Stuff.} To be consistent with previous methods~\cite{zhao2018icnet}, we evaluate at resolution 640640 for segmenting the 182 categories. As shown in Table~\ref{tab:coco}, for the general scene understanding task with this dataset, our FANet is also able to achieve satisfying accuracy with much faster speed than previous methods. Compared to the state-of-the-art real-time model ICNet~\cite{zhao2018icnet}, our FANet-34 achieves both better accuracy and speed, and FANet-18 can further accelerate the speed with a comparable mIoU.





\begin{figure}
\centering
\includegraphics[width=0.96\linewidth]{./Figure/Visual_res.pdf}
\vspace{-0.2cm}
\caption{\small{Image semantic segmentation results on Cityscapes.}}
\label{fig:visual_res} 
\vspace{-0.2cm}
\end{figure}


\begin{table}[!t]
\begin{minipage}[!t]{0.46\columnwidth}
\centering
\renewcommand\arraystretch{0.95}
\begin{tabular}{p{1.5cm}p{0.6cm}p{0.6cm}} 
\toprule
 \multirow{2}{*}{{Method}} & {mIoU} &{Speed}\\ 
   & {(\%)} &{(fps)}\\ 
 \midrule 
 {SegNet~\cite{badrinarayanan2017segnet}}    &~{55.6}              &{12}\\ 
 {ENet~\cite{paszke2016enet}}                &~{51.3}              &{46}\\ 
 {ICNet~\cite{zhao2018icnet}}                &~{67.1}              &{82}\\ 
 {BiseNet~\cite{yu2018bisenet}}              &~{68.7}              &{75}\\ 
\midrule 
 {\textbf{FANet-34}}         &\textbf{~{70.1}}             &\underline{{121}}\\ 
 {\textbf{FANet-18}}         &~\underline{{69.0}}          &\textbf{{154}}\\ 
\bottomrule
\end{tabular}
\vspace{0mm}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[!t]{0.46\columnwidth}
\centering
\renewcommand\arraystretch{0.95}
\begin{tabular}{p{1.5cm}p{0.6cm}p{0.6cm}} 
\toprule
 \multirow{2}{*}{{Method}} & {mIoU} &{Speed}\\ 
   &{(\%)} &{(fps)}\\ 
 \midrule 
 {FCN~\cite{long2015fully}}               &~{22.7}              &{~~9}\\ 
 {DeepLab~\cite{chen2017deeplab}}         &~{26.9}              &{~14}\\ 
 {ICNet~\cite{zhao2018icnet}}             &~\underline{{29.1}}              &{110}\\
 {BiseNet~\cite{yu2018bisenet}}           &~{25.6}              &{113}\\ 
\midrule 
 {\textbf{FANet-34}}                     &\textbf{~{29.5}}     &\underline{{142}}\\
 {\textbf{FANet-18}}                     &~{27.8}              &\textbf{{191}}\\ 
\bottomrule
\end{tabular}
\vspace{0mm}
\end{minipage}
\vspace{-0.1cm}
\caption{\small{Image semantic segmentation performance on Camvid (left) and COCO-Stuff (right).}}
\vspace{-0.7cm}
\label{tab:coco}
\end{table}


\begin{table}[h]
\renewcommand\arraystretch{1.1}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{ccccc}
\toprule
{Method} & {mIoU} & {Speed} & {Avg RT} & {MaxLatency}\\ 
& () & (fps) & (ms) & (ms) \\
 \midrule 
 {DVSNet-fast\cite{xu2018dynamic}}        &{63.2}              &{30.4}            &{33}              &- \\
 {Clockwork\cite{shelhamer2016clockwork}} &{64.4}              &{5.6}             &{177}             &{221} \\
 {DFF\cite{zhu17dff}}                     &{69.2}              &{5.7}             &{175}             &{644} \\
  {Accel\cite{jain2019accel}}  & 72.1 & 2.9 & 340 & 575 \\
{Low-Latency\cite{li2018low}}            &{75.9}              &{7.5}             &{133}             &{133}  \\
{Netwarp\cite{gadde2017semantic}}& \textbf{80.6} &0.33  & 3004 & 3004\\
 \midrule 
 {FANet34}                                   &{76.3}              &\underline{58}             &\underline{17}             &\underline{17} \\
 {FANet34+Temp}                     &{\underline{76.7}}     &{\underline{58}} &{\underline{17}} &{\underline{17}} \\
 \midrule 
 {FANet18}                                   &{75.0}              &\textbf{72}             &\textbf{14}             &\textbf{14}\\
 {FANet18+Temp}                     &{75.5}              &{\textbf{72}}    &{\textbf{14}}    &{\textbf{14}}\\
\bottomrule
\end{tabular}
\vspace{-0.1cm}
\caption{\small{Video semantic segmentation on Cityscapes. ``+Temp'' indicates FANet with spatial-temporal attention (t=2). Avg RT is the average per-frame running time, and MaxLatency is the maximum per-frame running time.}}
\label{tab:vid}
\vspace{-0.6cm}
\end{table}

\subsection{Video Semantic Segmentation}
In this part, we evaluate our method for video semantic segmentation on the challenging dataset Cityscapes~\cite{cordts2016cityscapes}. Without significantly increasing the computational cost, our method can effectively capture both spatial and temporal contextual information to achieve better accuracy, and outperforms previous methods with much lower latency.  In Table~\ref{tab:vid}, we compare our method with recent state-of-the-art approaches for video semantic segmentation. 
Compared to the image segmentation baseline models FANet18 and FANet34, both our spatial-temporal version FANet18+Temp and FANet34+Temp help to improve the accuracy at the same computational costs.
We also see that most of the existing methods fail to achieve real-time speed ( 30fps), apart from DVSNet which has much lower accuracy than ours.
Methods like Clockwork~\cite{shelhamer2016clockwork} and DFF~\cite{zhu17dff} save the overall computation while suffering from high latency due to the heavy computation at keyframes. PEARL~\cite{jin2017video} and Networp~\cite{gadde2017semantic} achieves state-of-the-art accuracy at the cost of very low speed and high latency. 
In contrast, FANet18+Temp and FANet34+Temp achieve state-of-the-art accuracy with a much faster speed. FANet18+Temp achieves more than 200 better efficiency than Netwarp~\cite{gadde2017semantic}. FANet34+Temp outperforms PEARL~\cite{jin2017video} with 40 faster speed. 






