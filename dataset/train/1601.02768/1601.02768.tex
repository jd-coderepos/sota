\makeatletter
\newcommand{\dontusepackage}[2][]{\@namedef{ver@#2.sty}{9999/12/31}\@namedef{opt@#2.sty}{#1}}
\makeatother
\dontusepackage{subfigure}


\documentclass[]{sigchi}

\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage[usenames,dvipsnames]{color}
\usepackage{fixltx2e} \ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 \usepackage[utf8]{inputenc}
\else \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} }{}
\bibliographystyle{SIGCHI-Reference-Format}
\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=lines,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color[gray]{0.4}\bfseries,
    commentstyle=\color[gray]{0.65}\itshape,
    numbers=left,
    captionpos=b,
}
\usepackage{longtable,booktabs}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\usepackage[font={small,bf},skip=2pt]{caption}
\usepackage{float}
\renewcommand{\topfraction}{0.85}	\renewcommand{\bottomfraction}{0.75}	\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\setcounter{dbltopnumber}{2}    \renewcommand{\dbltopfraction}{0.85}	\renewcommand{\textfraction}{0.10}	\renewcommand{\floatpagefraction}{0.85}	\renewcommand{\dblfloatpagefraction}{0.85}	\floatplacement{figure}{htbp}
\floatplacement{scholmdAlgorithm}{htbp}
\floatplacement{table}{htbp}
\usepackage{subfig}
\captionsetup[subfloat]{margin=1em}

\def\plainauthor{Jérémy Frey, Maxime Daniel, Julien Castet, Martin Hachet, Fabien Lotte}

\ifxetex
  \usepackage[setpagesize=false, unicode=false, xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={\plainauthor},
            pdftitle={Framework for Electroencephalography-based Evaluation of User Experience},
            colorlinks=true,
            citecolor=black,
            urlcolor=blue,
            linkcolor=black,
            pdfborder={0 0 0}}
\urlstyle{same}  \setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  \setcounter{secnumdepth}{-2}

\usepackage{balance}


\title{Framework for Electroencephalography-based Evaluation of User Experience}


\numberofauthors{5}
\author{
  \alignauthor Jérémy Frey\\
    \affaddr{Univ. Bordeaux, France}\\
    \email{jeremy.frey@inria.fr}\\
  \alignauthor Maxime Daniel\\
    \affaddr{Immersion SAS, France}\\
    \email{maxime.daniel@u-bordeaux.fr}\\ 
  \alignauthor Julien Castet\\
    \affaddr{Immersion SAS, France}\\
    \email{julien.castet@immersion.fr}\\ 
  \alignauthor Martin Hachet\\
    \affaddr{Inria, France}\\
    \email{martin.hachet@inria.fr}\\
  \alignauthor Fabien Lotte\\
    \affaddr{Inria, France}\\
    \email{fabien.lotte@inria.fr}\\
}

\date{}


\usepackage{times}

\usepackage{suffix}

\usepackage{subfig}

\toappear{Publication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only \\
 \emph{CHI'16}, May 07 - 12, 2016, San Jose, CA, USA \\
Copyright is held by the owner/author(s). Publication rights licensed to ACM. \\
ACM 978-1-4503-3362-7/16/05...\\times\times\times\timesp=0.05$, until no more outlier was detected \citep{Grubbs1969}. We then
normalized the outlier-free scores between -1 and +1. As such, for all
participants a workload index close to +1 represents the highest mental
workload they had to endure while they were playing. It should come
close to the 2-back condition of the calibration phase. On the opposite,
a workload index close to -1 denotes the lowest workload, similar to the
0-back condition.

The process was similar for attention, but we only extracted epochs that
corresponded to the target stimuli onset, i.e.~when the high pitch sound
was played. Note that contrary to \citep{Burns2015}, that studied the
amplitudes of ERPs and did not use the data gathered during the
calibration phase, here we kept the machine learning approach. As such,
the resulting scores can be seen as a confidence index of the LDA
classifier about whether participants noticed odd events while they were
playing.

As for the classifier dedicated to error recognition, the processing
differs. Indeed, we could not assume which interaction yielded or not an
interaction error, i.e.~if and when participants perceived a discrepancy
between what they intended to do and what occurred. Consequently, we
simply counted over an entire game session the number of times the
classifier labelled an interaction as being erroneous in the eye of the
participants.

\subsection{Results}\label{results-1}

Unless otherwise noted, we tested for significance using repeated
measures ANOVA. For significant main effects, we used post-hoc pairwise
Student's t-test with FDR correction.

\begin{table*}
\centering
\begin{tabular}{llllllllllllll}
\toprule\addlinespace
Construct & P1 & P2 & P3 & P4 & P5 & P6 & P7 & P8 & P9 & P10 & P11 & P12
& Average\tabularnewline
\midrule
Workload & 0.85 & 0.93 & 0.98 & 0.95 & 0.97 & 0.97 & 0.79 & 0.87 & 0.87
& 0.98 & 0.95 & 0.94 & 0.92\tabularnewline
Attention & 0.83 & 0.82 & 0.96 & 0.81 & 0.85 & 0.90 & 0.82 & 0.82 & 0.86
& 0.92 & 0.88 & 0.83 & 0.86\tabularnewline
Error recognition & 0.88 & 0.57 & 0.90 & 0.90 & 0.86 & 0.90 & 0.78 &
0.80 & 0.88 & 0.78 & 0.85 & 0.74 & 0.82\tabularnewline
\bottomrule
\end{tabular}
\caption{Classification accuracy during the calibration tasks for the 3
measured constructs (AUROCC scores).}\label{tab:classification}
\end{table*}

\begin{figure}
\centering
\subfloat[\label{fig:workload-stats}]{\includegraphics[width=1.000\hsize]{./img/workload_ezPlot_cust.pdf}}
\\
\subfloat[\label{fig:attention-stats}]{\includegraphics[width=0.660\hsize]{./img/attention_ezPlot_cust.pdf}}
\subfloat[\label{fig:errp-stats}]{\includegraphics[width=0.330\hsize]{./img/errp_boxplot_thin.pdf}}
\caption{EEG measures. \emph{a}: The workload index significantly
differs across difficulties and between interaction techniques.
\emph{b}: The attention index significantly differs across difficulties.
\emph{c}: The number of interaction errors differs by a tendency between
KEYBOARD and TOUCH.}\label{fig:EEG-stats}
\end{figure}

\subsubsection{Workload}\label{workload}

On average, the classifier AUROCC score during the training task was
0.92 (SD: 0.06) -- see Table~\ref{tab:classification}. Over the test set
there were on average 2171 data points per subject across all condition
(time windows). The statistical analysis of the classifier output during
the game session showed a significant effect of the difficulty factor (p
\textless{} 0.01); the workload index increasing along the difficulty of
the levels (Figure~\ref{fig:workload-stats}). The post-hoc analysis
showed that all difficulty levels significantly differs one from the
other with p \textless{} 0.01; except for the MEDIUM level, which
differs from EASY with p \textless{} 0.05 and with HARD only by a margin
(p = 0.11). There was a significant effect of the interaction factor as
well (p \textless{} 0.01), the workload being higher on average during
the TOUCH condition. There was no interaction between difficulty and
interaction factors.

\subsubsection{Attention}\label{attention}

On average, the classifier AUROCC score during the training task was
0.86 (SD: 0.05) -- see Table~\ref{tab:classification}. Over the test set
there were on average 497 data points per subject across all conditions
(odd events). The statistical analysis of the classifier output during
the game session showed a significant effect of the difficulty factor (p
\textless{} 0.01) but not of the interaction factor. The attention index
decreases as the difficulty increases
(Figure~\ref{fig:attention-stats}). The post-hoc analysis showed that
the ULTRA level significantly differs from the others (p \textless{}
0.05).

\subsubsection{Error recognition}\label{error-recognition}

On average, the classifier AUROCC score during the training task was
0.82 (SD: 0.10) -- see Table~\ref{tab:classification}. Over the test set
there were on average 388 data points per subject across all conditions
(interactions). Due to the nature of the data (numbers of interaction
errors across entire game sessions), we used a one-tailed Wilcoxon
Signed Rank Test to stress our hypothesis. The number of interaction
errors differs by a tendency (p = 0.08) between the KEYBOARD and the
TOUCH conditions. 19\% of the interactions (SD: 9\%) were labelled as
interaction errors by the classifier for KEYBOARD \emph{vs} 22\% (SD:
9\%) for TOUCH (Figure~\ref{fig:errp-stats}).

\subsection{Behavioral measures}\label{behavioral-measures}

Besides EEG metrics, we had the opportunity to study participants'
reaction time and performance so as to get a clearer picture of their
user experience.

\begin{figure}
\centering
\subfloat[\label{fig:time-stats}]{\includegraphics[width=0.480\hsize]{./img/reactionTimeCust.pdf}}
\subfloat[\label{fig:perf-stats}]{\includegraphics[width=0.480\hsize]{./img/perfsCust.pdf}}
\caption{Behavioral measures: reaction time in seconds (left) and
performance (proportion of correctly selected directions -- right)
significantly differs between difficulty levels and interactions. E:
EASY, M: MEDIUM, H: HARD, U: ULTRA.}\label{fig:behav-stats}
\end{figure}

\subsubsection{Reaction time}\label{reaction-time}

There was a significant effect of both the difficulty and interaction
factors, as well as an interaction effect between them (p \textless{}
0.01). Post-hoc tests showed that all difficulty levels differ from one
another (p \textless{} 0.01), except for MEDIUM and HARD, which do not
differ significantly (p = 0.91). The mean reaction times were
respectively for EASY, MEDIUM, HARD and ULTRA: 0.78s (SD: 0.14), 0.97s
(0.18), 0.98s (0.15) and 0.69s (0.06). The mean reaction time was 0.78
(0.12) for KEYBOARD and 0.93 (0.13) for TOUCH. See
Figure~\ref{fig:time-stats}. Note that users had less time to respond
during higher difficulty levels.

\subsubsection{Performance}\label{performance}

The performance was computed as the ratio between the number of correct
selections and the total number of interactions. There was a significant
effect of both the difficulty and interaction factors, as well as an
interaction effect between them (p \textless{} 0.01). Post-hoc tests
showed that all difficulty levels differ from one another (p \textless{}
0.01). The mean performance was respectively for EASY, MEDIUM, HARD and
ULTRA: 98\% (SD: 3), 89\% (12), 83\% (17) and 55\% (21). The Mean
performance was 85\% (13) for KEYBOARD and 77\% (13) for TOUCH. See
Figure~\ref{fig:perf-stats}.

\subsection{Discussion}\label{discussion-1}

Most of the main hypotheses are verified. The workload index as computed
with EEG showed significant differences that match the intended design
of the difficulty levels. It was also shown that in the highest
difficulty the attention level of participants toward external stimuli
was significantly lower -- i.e.~inattentional blindness increased.
Concerning the interaction techniques, the number of interaction errors
as measured by EEG was higher with the TOUCH condition, but this is a
tendency and not a significant effect. The workload index, on the other
hand, was significantly higher in the TOUCH condition compared to the
KEYBOARD condition.

Thanks to the ground truth obtained during the pilot study with the
NASA-TLX questionnaire, these results validate the use of a workload
index measured by EEG for HCI evaluation and set the path for two other
constructs: attention and error recognition. Beside the evaluation of
the content (i.e.~difficulty levels) we were able to compare two
interaction techniques. These are promising results for those who seek
to assess how intuitive a UI is with exocentric measures
\citep{Frey2014a}.

In this study, we chose to use the particularity of the touch screen to
make the task \emph{more} difficult. Indeed, while we used a touch
screen for its possibility of direct manipulation, we kept the character
as a frame of reference, resulting in input commands that were
(patently) not co-localized with output directions. Besides results
denoting the differences between the conditions, participants also
spontaneously reported how non intuitive this condition was. We wanted
to investigate our evaluation method on a salient difference at first.
Then our framework could well be employed to go further; for example
seeking physiological differences between direct and indirect
manipulation interfaces in more traditional tasks.

It is interesting to note how those EEG measures could be combined with
existing methods to broaden the overall comprehension of the user
experience. For instance, while we did show significant differences
across difficulty levels and between interaction techniques with
behavioral measures (reaction time and performance index), EEG measures
could help to understand the underlying mechanisms. Because we have a
more direct access to brain activity, we can make assumptions about the
cause of observed behaviors. For example participants' worse performance
with TOUCH than with KEYBOARD could be due to the fact that they
anticipate less the outcomes of their actions (more interaction errors);
the higher reaction time may not only be caused by the interaction
technique \emph{per se}, but by a higher workload. And while
participants manage to cope with the fast pace of the ULTRA level (the
smallest reaction times), the increase in perceptual load lower their
awareness to task-irrelevant stimuli.

Additionally, we can observe that the performances obtained at the EASY,
MEDIUM and HARD levels are very similar with the keyboard and the touch
screen (see Figure~\ref{fig:perf-stats}). However, EEG analyses revealed
that the workload was significantly higher in the TOUCH condition,
meaning that users had to allocate significantly more cognitive
resources to reach the same performance. This further highlights that
EEG-based measures do bring additional information that can complement
traditional evaluations such a behavioral measures.

Measuring users' cognitive processes such as workload and attention may
prove particularly useful to assess 3D user interfaces (3DUI), since
they are known to be more cognitively demanding. They require users to
perform 3D mental rotation tasks to successfully manipulate objects or
to orientate themselves in the 3D environment. Moreover, the usual need
for a mapping between the user inputs (with limited degrees-of-freedom
-- e.g., only 2 for a mouse) and the corresponding actions on 3D objects
(with typically 6 degrees-of-freedom), makes 3DUI usually difficult to
assess and design. We reproduced part of this problematic with our game
environment and obtained coherent results from EEG measures.

\begin{figure}
\centering
\includegraphics[]{./img/workload_over_time_s3_split.pdf}
\caption{Workload index over time for participant 3 -- 60s smoothing
window. \emph{Left}: KEYBOARD condition, \emph{right}: TOUCH condition.
Background color represents the corresponding difficulty
level.}\label{fig:workload-continuous}
\end{figure}

Above all, an evaluation method based on EEG enables a continuous
monitoring of users. The intended use case of our framework is to enroll
dedicated testers that would wear the EEG equipment and perform well
during the calibration tasks. As a matter of fact, the best performer
during workload calibration (participant 3 in
Table~\ref{tab:classification}) shows patterns that clearly meet the
expectations concerning both difficulty levels and interactions, as
pictured in Figure~\ref{fig:workload-continuous}.

\section{Limitations and future
challenges}\label{limitations-and-future-challenges}

Although using EEG measures as an evaluation method for HCI was proven
conclusive regarding workload -- we obtained a continuous index on par
with a ground truth based on traditional questionnaires -- the two other
constructs we studied could benefit from further improvements.

Despite the direct interaction (TOUCH) being more disorienting for users
than the indirect one (KEYBOARD), the recognition of interaction errors
differed only by a tendency. This could be explained by the fact that
the calibration task was too dissimilar to the virtual environment.
Notably, while there was few and slow paced events during the
calibration, users were confronted to many stimuli while they were
playing, hence overlapping ERPs must have appeared within EEG, which may
have disturbed the classifier. A calibration task closer to real-life
scenarios than the one described in \citep{Ferrez2008} should be
envisioned. Such task should remain generic in order to facilitate the
dissemination of EEG as an evaluation method for HCI.

Signal processing could also facilitate the transfer of the
classification between a standard task and the evaluated HCI. Indeed, if
our results demonstrate that the EEG classification of workload could be
transferred from the N-back tasks to a dissimilar virtual environment
and user interface, we benefited from spatial filters that specifically
take into account the variance between calibration contexts and use
contexts -- the stationary subspace CSP \citep{Wobrock2015}. Since ERPs
may also slightly differ in amplitudes and delays between calibration
and use contexts, in the future, it would be worth designing similar
approaches to optimize temporal or spatial filters for ERPs as well.

The reliability of mental states measures is strongly correlated to the
quality of EEG signals. Interestingly enough, participants' mindset
during the recordings is one of the factors influencing EEG signals.
Their awareness and involvement toward the tasks improve system
accuracy. The form of the calibration tasks could be enhanced to engage
more users, for example through gamification \citep{Flatla2011} -- and
our virtual environment proved to be suitable to do so. Whereas our
participants were volunteers enrolled among students, in the end the
outcome of an evaluation method based on EEG should be strengthened by
recruiting dedicated testers, using as selection criteria how reliably
the different constructs could be estimated from their EEG signals
during calibration tasks.

Finally, one should acknowledge that when it comes to recordings as
sensitive as EEG, artifacts such as the ones induced by muscular
activity are of major concern. The way we prevented the appearance of
such bias in the present study is threefold. 1) The hardware we used --
active electrodes with Ag/AgCl coating -- is robust to cable movements,
see e.g., \citep{Wilson2012a}. 2) The classifiers were trained on
features not related to motion artifacts or motor cortex activation. 3)
The position of the screen during the ``touch'' condition minimized
participants' motion, and gestures occurred mostly before the time
window used for detecting interaction errors. These precautions are
important for the technology to be correctly apprehended.

To further control for any bias in our protocol, we ran a batch of
simulations where the labels of the calibration tasks had been randomly
shuffled, similarly to the verification process described in
\citep{Wobrock2015}. Should artifacts bias our classifiers, differences
would have appeared between the KEYBOARD and TOUCH conditions even with
such random training. Among the 20 simulations that ran for each of the
3 constructs (workload, attention, error recognition), none yielded
significant differences.

\section{Conclusion}\label{conclusion}

In this paper, we demonstrated how brain signals -- recorded by means of
electroencephalography -- could be put into practice in order to obtain
a continuous evaluation of different interaction techniques, for
assessing their ergonomic pros and cons. In particular, we validated an
EEG-based workload estimator that does not necessitate to modify the
existing software. Furthermore, we showed how users' attention level
could be evaluated using background stimuli, such as sounds. Finally, we
investigated how the recognition of interaction errors could help to
determine the best user interface.

Being able to estimate these three constructs -- workload, attention and
error recognition -- continuously during realistic and complex
interaction tasks opened new possibilities. Notably, it enabled us to
obtain additional and more objective metrics of user experience, based
on the users' cognitive processes. It also provided us with additional
insights that traditional measures (e.g., behavioral measures) could not
reveal. To sum up, this suggests that combined with existing evaluation
methods, EEG-based evaluation tools such as the ones proposed here can
help to understand better the overall user experience. We hope that the
increasing availability of EEG devices will foster such approaches and
benefit the HCI field.

\balance{}

\bibliography{biblio}


\end{document}
