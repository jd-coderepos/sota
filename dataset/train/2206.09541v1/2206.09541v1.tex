\section{Experiments}\label{sec:experiments}

\subsection{Multi-Label Recognition with Partial Labels}\label{sec:mlc_pl}
\begin{table}
    \begin{center}
     \caption{\small \textbf{Multi-label Recognition on MS-COCO and VOC2007 with partial labels.} \ours achieves the best performance over all SOTA methods.  indicates previous models using weights pretrained by CLIP~\cite{radford2021learning}}~\label{table:partial_label}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c| c| c c c c c c c c c | c }
            \Xhline{3\arrayrulewidth} 
            Methods & \cellcolor{yellow!15} \#P & \cellcolor{yellow!15}  &   &  &  &  &  &    &   &    & \cellcolor{yellow!15}Avg. \\
            \Xhline{3\arrayrulewidth} 
            \multicolumn{12}{c}{MS-COCO~\cite{lin2014microsoft}} \\
            \Xhline{3\arrayrulewidth}
            SSGRL~\cite{chen2019learning} & \cellcolor{yellow!15} 64.7M & \cellcolor{yellow!15} 62.5 & 70.5 & 73.2 & 74.5 & 76.3 & 76.5 &  77.1 & 77.9 & 78.4 & \cellcolor{yellow!15} 74.1  \\
            GCN-ML~\cite{chen2019multi} & \cellcolor{yellow!15} 44.9M & \cellcolor{yellow!15} 63.8 & 70.9 & 72.8 & 74.0 & 76.7 & 77.1 & 77.3 & 78.3 & 78.6 & \cellcolor{yellow!15} 74.4 \\
            KGGR~\cite{chen2020knowledge} & \cellcolor{yellow!15}  25M  & \cellcolor{yellow!15} 66.6 & 71.4 & 73.8 & 76.7 & 77.5 & 77.9 & 78.4 & 78.7 & 79.1 & \cellcolor{yellow!15} 75.6 \\
            Curriculum labeling~\cite{durand2019learning} & \cellcolor{yellow!15}  38M & \cellcolor{yellow!15} 26.7 & 31.8 & 51.5 & 65.4 & 70.0 & 71.9 & 74.0 & 77.4 & 78.0 & \cellcolor{yellow!15} 60.7 \\
            Patial BCE~\cite{durand2019learning} & \cellcolor{yellow!15}  38M & \cellcolor{yellow!15} 61.6 & 70.5 & 74.1 &  76.3 & 77.2 &  77.7 & 78.2 &  78.4 & 78.5 &\cellcolor{yellow!15} 74.7\\
            SST~\cite{chen2022structured} & \cellcolor{yellow!15} 33.5M  & \cellcolor{yellow!15} 68.1 &  73.5 & 75.9 & 77.3 & 78.1 & 78.9 & 79.2 & 79.6 & 79.9 & \cellcolor{yellow!15} 76.7 \\
            SST &  \cellcolor{yellow!15} 33.5M  & \cellcolor{yellow!15} 69.1 & \underline{78.5} & \underline{79.3} & \underline{79.9} & 80.1 & \underline{80.5} & \underline{81.1} & \underline{80.7} & 80.7 & \cellcolor{yellow!15} 78.9 \\
            SARB~\cite{pu2022semantic} & \cellcolor{yellow!15} 29.6M & \cellcolor{yellow!15} 71.2 & 75.0 & 77.1 & 78.3 & 78.9 & 79.6 & 79.8 &  80.5 & 80.5 & \cellcolor{yellow!15} 77.9 \\
            SARB & \cellcolor{yellow!15} 29.6M & \cellcolor{yellow!15} \underline{75.5} & \underline{78.5} & 79.0 & 79.5 & \underline{80.4} & 80.2 & 80.8 & 80.6 & \underline{80.8} & \cellcolor{yellow!15} \underline{79.4} \\
            \ours (ours) & \cellcolor{yellow!15} \textbf{1.3M} & \cellcolor{yellow!15} \textbf{78.7} & \textbf{80.9} & \textbf{81.7} & \textbf{82.0} & \textbf{82.5} & \textbf{82.7} & \textbf{82.8} & \textbf{83.0} & \textbf{83.1} & \cellcolor{yellow!15} \textbf{81.9} \\            
            \Xhline{3\arrayrulewidth} 
              \multicolumn{12}{c}{PASCAL VOC 2007 ~\cite{everingham2010pascal}}\\
            \Xhline{3\arrayrulewidth}
             SSGRL~\cite{chen2019learning} & \cellcolor{yellow!15}66.6M  & \cellcolor{yellow!15} 77.7 & 87.6 & 89.9 & 90.7 & 91.4 & 91.8 & 91.9 & 92.2 & 92.2 & \cellcolor{yellow!15} 89.5\\
            GCN-ML~\cite{chen2019multi} & \cellcolor{yellow!15}44.9M & \cellcolor{yellow!15} 74.5 & 87.4 & 89.7 & 90.7 & 91.0 & 91.3 & 91.5 & 91.8 & 92.0 & \cellcolor{yellow!15} 88.9\\
            KGGR~\cite{chen2020knowledge} & \cellcolor{yellow!15} 25M  & \cellcolor{yellow!15} 81.3 & 88.1 & 89.9 & 90.4 & 91.2 & 91.3 & 91.5 & 91.6 & 91.8 & \cellcolor{yellow!15} 89.7 \\
            Curriculum labeling~\cite{durand2019learning} & \cellcolor{yellow!15}  38M  & \cellcolor{yellow!15} 44.7 & 76.8 & 88.6 & 90.2 & 90.7 & 91.1 & 91.6 & 91.7 & 91.9 & \cellcolor{yellow!15} 84.1\\
            Patial BCE~\cite{durand2019learning} & \cellcolor{yellow!15}  38M & \cellcolor{yellow!15} 80.7 & 88.4 & 89.9 &  90.7 & 91.2 & 91.8 & 92.3 & 92.4 & 92.5  & \cellcolor{yellow!15} 90.0 \\
            SST~\cite{chen2022structured} & \cellcolor{yellow!15} 32.4M & \cellcolor{yellow!15} 81.5 & 89.0 & 90.3 & 91.0 & 91.6 &  92.0 & 92.5 & 92.6 & 92.7 & \cellcolor{yellow!15} 90.4 \\
            SARB~\cite{pu2022semantic} & \cellcolor{yellow!15} 29.6M & \cellcolor{yellow!15} \underline{83.5} & \underline{88.6} & \underline{90.7} & \underline{91.4} & \underline{91.9} & \underline{92.2} & \underline{92.6} & \underline{92.8} & \underline{92.9} & \cellcolor{yellow!15} \underline{90.7} \\
            \ours (ours) &\cellcolor{yellow!15} \textbf{0.3M} & \cellcolor{yellow!15} \textbf{90.3} & \textbf{92.2} & \textbf{92.8} & \textbf{93.3} & \textbf{93.6} & \textbf{93.9} & \textbf{94.0} & \textbf{94.1} & \textbf{94.2} & \cellcolor{yellow!15} \textbf{93.2} \\ 
             \Xhline{3\arrayrulewidth} 
        \end{tabular}
        } 
    \end{center}
\vspace{-10pt}
\end{table} 
\textbf{Datasets.} 
We conduct experiments on MS-COCO~\cite{lin2014microsoft} and VOC2007~\cite{everingham2010pascal} to evaluate multi-label recognition with partial labels. MS-COCO~\cite{lin2014microsoft} contains 80 common object categories and we use the official \texttt{train2014} (82K images) and \texttt{val2014} (40K images) splits for training and test. VOC2007~\cite{everingham2010pascal} contains 20 object categories and we use the official \texttt{trainval} (5K images) and \texttt{test} (5K images) splits for training and test. To create the training set with partial labels, we randomly mask out labels from the fully annotated training set\footnote{The difference in performance is within  of independent runs.} and use the remaining labels for training by following standard practice~\cite{chen2022structured,durand2019learning,pu2022semantic}. In this work, we vary the proportion of kept labels from  to ~\cite{chen2022structured,pu2022semantic}.

\textbf{Evaluation.} 
On MS-COCO and VOC2007 datasets, we follow \cite{chen2022structured,durand2019learning, pu2022semantic} to report the mean average precision (mAP) for each proportion of labels available for optimization (from  to ) and its average value for all proportions.
We count the learnable parameters (\#P) of each baseline and \ours to measure the complexity of optimization\footnote{For baselines without public released implementation, we only measure the major part of the learnable parameters based on description in their papers. (indicated as \#P  [a value] in Table~\ref{table:partial_label}-\ref{table:zsl_nus_wide})}. 
We also report the per-class and the average overall precision (CP and OP), recall (CR and OR), and F1 (CF1 and OF1) of \ours under different proportions of labels for training in the supplementary material due to the page limit.


\textbf{Implementation.} We adopt ResNet-101~\cite{he2016deep} as the visual encoder in all baselines and \ours for input resolution 448448,  and use the same Transformer~\cite{radford2019language,vaswani2017attention} in CLIP~\cite{radford2021learning} as the text encoder. The visual and text encoders are initialized from the CLIP pretrained model and kept frozen during optimization. For each class/label, we learn two independent context vectors with 16 context tokens (N = 16) following \cite{zhou2021learning}, which is the only learnable part in \ours. We use the SGD optimizer with an initial rate of 0.002 which is decayed by the cosine annealing rule. We train context vectors for 50 epochs with a batch-size 32/8 for MS-COCO/VOC2007, respectively. For ASL loss, we choose ,  and  via validation. The training is done with one RTX A6000.

\textbf{Baselines.} To evaluate the effectiveness of \ours, we compare with the following baselines: (1).SSGRL~\cite{chen2019learning}, GCN-ML~\cite{chen2019multi} and KGGR~\cite{chen2020knowledge} adopt graph neural networks to model label dependencies. We follow \cite{chen2022structured} to report their performance in the partial-label setting. (2). Curriculum labeling~\cite{durand2019learning} and  SST~\cite{chen2022structured} generate pseudo labels for unknown labels. (3). Partial BCE~\cite{durand2019learning} uses a normalized BCE loss to better exploit partial labels. (4).SARB~\cite{pu2022semantic} blends category-specific representation across different images to transfer information of known labels to complement unknown labels.


\textbf{Results.} 
Table~\ref{table:partial_label} shows the comparison of mAP between \ours and all baselines optimized with  to  of labels. For the two most recent works (SST~\cite{chen2022structured} and SARB~\cite{pu2022semantic}), we further substitute the ImageNet pretrained weights~\cite{he2016deep} with the CLIP pretrained weights~\cite{radford2021learning} when initializing of their visual encoders, which results in  and  in Table~\ref{table:partial_label}. Since we learn class-specific prompts, \ours on MS-COCO adopts more learnable parameters than VOC2007. Our proposed \ours achieves the best performance across all  proportions of labels available during the training with the smallest learnable overhead (1.3M vs. 29.6M in  on MS-COCO and 0.3M vs. 29.6M in  on VOC2007). Notably, \ours yields a great improvement over the second-best method,  on MS-COCO and  on VOC2007,  especially when only providing  of labels during the training. This indicates that \ours can quickly adapt to the multi-label recognition task with a few labels by taking advantage of the powerful vision-language pretraining.

\subsection{Zero-shot Multi-Label Recognition}
\textbf{Datasets.} 
Following \cite{ben2021semantic, huynh2020shared}, we conduct experiments on MS-COCO~\cite{lin2014microsoft} and NUS-WIDE~\cite{chua2009nus} to perform zero-shot multi-label recognition. On MS-COCO, we follow \cite{bansal2018zero, ben2021semantic} to split the dataset into 48 seen classes and 17 unseen classes. NUS-WIDE~\cite{chua2009nus} dataset includes 270K images. Following \cite{ben2021semantic, huynh2020shared} we use 81 human-annotated categories as unseen classes and an additional set of 925 labels obtained from Flickr tags as seen classes. 

\textbf{Evaluation.}
We follow \cite{ben2021semantic} and report precision, recall, and F1 score at Top-3 predictions in each image on MS-COCO. We also follow \cite{ben2021semantic, huynh2020shared} to report mAP over all categories as well as precision, recall, and F1 score at Top-3 and Top-5 predictions in each image on NUS-WIDE. We evaluate all methods with both zero-shot setting (test only on unseen classes) and generalized zero-shot setting (test on both seen and unseen classes).

\textbf{Implementation.} We adopt ResNet-50~\cite{he2016deep} similar to \cite{ben2021semantic} as the visual encoder in \ours for input resolution 224. Instead of learning class-specific prompts, we learn the class-agnostic context vectors with 64 context tokens (N = 64) for all classes, which is the only learnable part in \ours. We optimize context vectors for 50 epochs with a batch-size 32/192 for MS-COCO/NUS-WIDE, respectively. Other implementation details are the same with Sec.~\ref{sec:mlc_pl}

\textbf{Baselines.}
To evaluate the effectiveness of \ours in the zero-shot setting, we compare with the following baselines: (1). CONSE~\cite{norouzi2013zero} adopts an ensemble of classifiers for unseen classes. (2). LabelEM~\cite{akata2015label} learns a joint image-label embedding. (3). Fast0Tag~\cite{zhang2016fast} and SDL~\cite{ben2021semantic} estimate one or multiple diverse principal directions of the input images. (4). Deep0Tag~\cite{rahman2018deep} and LESA~\cite{huynh2020shared} estimate the relevant regions via region proposals and attention techniques respectively. (5). BiAM~\cite{narayan2021discriminative} enhances the region-based features to minimize inter-class feature entanglement.








\begin{table}
    \begin{center}
     \caption{\small{\textbf{Zero-Shot Multi-Label Recognition on MS-COCO\cite{lin2014microsoft}}. \ours achieves the best F1 score in both ZSL and GZSL settings.}}~\label{tab:zero_shot_mscoco}
    
        \resizebox{0.68\linewidth}{!}{
        \begin{tabular}{c c c c c c c c}
        \Xhline{3\arrayrulewidth} 
        \multirow{2}{*}{Methods} & \cellcolor{yellow!15}  & \multicolumn{3}{c}{ZSL} & \multicolumn{3}{c}{GZSL} \\
        &  \cellcolor{yellow!15}\multirow{-2}{*}{\#P}  & \textbf{P} & \textbf{R} & \cellcolor{yellow!15} \textbf{F1} & \textbf{P} & \textbf{R} &\cellcolor{yellow!15} \textbf{F1} \\ 
           \Xhline{3\arrayrulewidth} 
           CONSE~\cite{norouzi2013zero} &  \cellcolor{yellow!15}  -   & 11.4 & 28.3 &  \cellcolor{yellow!15}  16.2  & 23.8 & 28.8 &  \cellcolor{yellow!15} 26.1 \\
           Fast0Tag~\cite{zhang2016fast} &  \cellcolor{yellow!15}  0.61M    & 24.7 & 61.4 &  \cellcolor{yellow!15}  25.3 & 38.5 & 46.5 &  \cellcolor{yellow!15}  42.1 \\
           Deep0Tag~\cite{rahman2018deep}  &  \cellcolor{yellow!15}   23M   & \underline{26.5} & \underline{65.9} &  \cellcolor{yellow!15} \underline{37.8} &  43.2 &  52.2 &  \cellcolor{yellow!15}  47.3 \\
           SDL (M=2)~\cite{ben2021semantic}  &  \cellcolor{yellow!15}  30.6M   &  26.3 & 65.3 &  \cellcolor{yellow!15}  37.5 &  \textbf{59.0} & \underline{60.8} &  \cellcolor{yellow!15}  \underline{59.9} \\
           \ours (ours) &   \cellcolor{yellow!15}  \textbf{0.02M} & \textbf{35.3} & \textbf{87.6} & \cellcolor{yellow!15} \textbf{50.3}  &	\underline{58.4} & \textbf{68.1} & \cellcolor{yellow!15}  \textbf{62.9}	 \\
            \Xhline{3\arrayrulewidth} 
        \end{tabular}
        } 
    \end{center}
\vspace{-20pt}
\end{table} 

\begin{table}
    \begin{center}
     \caption{\small \textbf{Zero-Shot Multi-label Recognition on NUS-WIDE~\cite{chua2009nus}.} \ours achieves the best F1 score over all SOTA methods at Top-3/Top-5 predictions in both ZSL and GZSL settings.  }~\label{table:zsl_nus_wide}
        \resizebox{0.83\linewidth}{!}{
        \begin{tabular}{c c c c c c c c c}
        \Xhline{3\arrayrulewidth} 
        \multirow{2}{*}{Methods} & \cellcolor{yellow!15} & \multicolumn{3}{c}{Top-3} & \multicolumn{3}{c}{Top-5} & \cellcolor{yellow!15}  \\
        & \cellcolor{yellow!15} \multirow{-2}{*}{\#P} & \textbf{P} & \textbf{R} & \cellcolor{yellow!15} \textbf{F1} & \textbf{P} & \textbf{R} & \cellcolor{yellow!15}\textbf{F1} & \cellcolor{yellow!15} \multirow{-2}{*}{mAP}\\
        \Xhline{3\arrayrulewidth} 
         \multicolumn{9}{c}{Zero-Shot Learning (ZSL)}  \\
 \Xhline{3\arrayrulewidth} 
CONSE~\cite{norouzi2013zero} & \cellcolor{yellow!15} - & 17.5 & 28.0 &  \cellcolor{yellow!15} 21.6 & 13.9 & 37.0 & \cellcolor{yellow!15} 20.2 &\cellcolor{yellow!15} 9.4 \\ 
LabelEM~\cite{akata2015label} & \cellcolor{yellow!15} - & 15.6 & 25.0 & \cellcolor{yellow!15} 19.2 & 13.4 & 35.7 & \cellcolor{yellow!15} 19.5 & \cellcolor{yellow!15} 7.1 \\ 
Fast0Tag~\cite{zhang2016fast} & \cellcolor{yellow!15} 0.61M  &   22.6 &  36.2 & \cellcolor{yellow!15} 27.8 &  18.2 & 48.4 & \cellcolor{yellow!15} 26.4 & \cellcolor{yellow!15} 15.1 \\
One Attention per Label~\cite{Kim2018}  &  12.8M &  20.9 & 33.5 & \cellcolor{yellow!15} 25.8 & 16.2 & 43.2 & \cellcolor{yellow!15} 23.6 & \cellcolor{yellow!15} 10.4 \\
LESA (M=10)~\cite{huynh2020shared} & \cellcolor{yellow!15}  0.45M &  \underline{25.7} &  41.1 & \cellcolor{yellow!15} 31.6 & \underline{19.7} & 52.5 & \cellcolor{yellow!15} 28.7 & \cellcolor{yellow!15} 19.4  \\
BiAM~\cite{narayan2021discriminative} & \cellcolor{yellow!15} 3.8M & -- & -- & \cellcolor{yellow!15} \underline{33.1} & -- & -- & \cellcolor{yellow!15} \underline{30.7} & \cellcolor{yellow!15} \underline{26.3} \\
SDL (M=7)~\cite{ben2021semantic} & \cellcolor{yellow!15} 33.6M  & 24.2 & \underline{41.3} & \cellcolor{yellow!15} 30.5 & 18.8 & \underline{53.4} & \cellcolor{yellow!15} 27.8 &\cellcolor{yellow!15} 25.9  \\
\ours (ours) & \cellcolor{yellow!15} \textbf{0.02M}  &  \textbf{37.3} &	\textbf{46.2} & \cellcolor{yellow!15} \textbf{41.3} & \textbf{28.7} & \textbf{59.3} & \cellcolor{yellow!15} \textbf{38.7} & \cellcolor{yellow!15} \textbf{43.6}  \\
             \Xhline{3\arrayrulewidth} 
             \multicolumn{9}{c}{Generalized Zero-Shot Learning (GZSL)}  \\
             \Xhline{3\arrayrulewidth} 
CONSE~\cite{norouzi2013zero} & \cellcolor{yellow!15} -  & 11.5 & 5.1 & \cellcolor{yellow!15}7.0 & 9.6 & 7.1 &  \cellcolor{yellow!15} 8.1 &  \cellcolor{yellow!15}  2.1  \\
LabelEM~\cite{akata2015label} & \cellcolor{yellow!15} - & 15.5 & 6.8 & \cellcolor{yellow!15}9.5 & 13.4 & 9.8 & \cellcolor{yellow!15}  11.3 &\cellcolor{yellow!15}  2.2  \\
Fast0Tag~\cite{zhang2016fast} & \cellcolor{yellow!15} 0.61M & 18.8 & 8.3 & \cellcolor{yellow!15}11.5 & 15.9 & 11.7 &  \cellcolor{yellow!15}  13.5 & \cellcolor{yellow!15}  3.7  \\
One Attention per Label~\cite{Kim2018}  & \cellcolor{yellow!15}  12.8M & 17.9 & 7.9 & \cellcolor{yellow!15}10.9 & 15.6 & 11.5 & \cellcolor{yellow!15}  13.2 & \cellcolor{yellow!15}  3.7 \\
LESA (M=10)~\cite{huynh2020shared} &  0.45M & 23.6 & 10.4 & \cellcolor{yellow!15}14.4 & 19.8 & 14.6 & \cellcolor{yellow!15} 16.8 &\cellcolor{yellow!15}   5.6 \\
BiAM~\cite{narayan2021discriminative} & \cellcolor{yellow!15} 3.8M & -- & -- & \cellcolor{yellow!15} 16.1 & -- & -- & \cellcolor{yellow!15} 19.0 & \cellcolor{yellow!15} 9.3 \\
SDL (M=7)~\cite{ben2021semantic} & \cellcolor{yellow!15}33.6M   & \underline{27.7} & \textbf{13.9} &\cellcolor{yellow!15} \underline{18.5} & \underline{23.0} & \textbf{19.3} & \cellcolor{yellow!15}  \underline{21.0} & \cellcolor{yellow!15} \textbf{ 12.1} \\
\ours (ours) & \cellcolor{yellow!15}\textbf{ 0.02M  } & \textbf{31.9} & \textbf{13.9} & \cellcolor{yellow!15} \textbf{19.4} & \textbf{26.2} & 19.1 & \cellcolor{yellow!15} \textbf{ 22.1} & \cellcolor{yellow!15} \underline{12.0} \\
           \Xhline{3\arrayrulewidth} 
        \end{tabular}
        } 
    \end{center}
\vspace{-15pt}
\end{table} 
\textbf{Results.} 
Table~\ref{tab:zero_shot_mscoco}-\ref{table:zsl_nus_wide} shows the comparison between \ours and all SOTA methods of zero-shot learning and generalized zero-shot learning on MS-COCO and NUS-WIDE datasets. \ours achieves the best F1 score in all cases with a very light learnable overhead (0.02M) and improves the performance of zero-shot learning (unseen labels) with a significant margin: F1 score improves by 12.5 @Top-3 on MS-COCO, and by 10.8 @Top-3 and 10.9 @Top-5 on NUS-WIDE. This shows the power of exploiting the pretrained alignment of textual and visual spaces in CLIP via \ours to solve multi-label recognition.


\subsection{Ablation Studies}



\begin{table}
    \begin{center}
     \caption{\small \textbf{Comparison among methods on MS-COCO using partial labels with the same initialization. All methods use parameters pretrained by CLIP~\cite{radford2021learning}.}}~\label{table:semantic_guide}
        \resizebox{0.65\linewidth}{!}{
        \begin{tabular}{c | c |c c c c c  }
            \Xhline{3\arrayrulewidth} 
            Method &  Text Supervision &   &  &  &  &  \\
            \Xhline{3\arrayrulewidth}  
            Discrete Label & \xmark & 70.6 & 75.1 &  76.5 & 77.3 & 78.0 \\
            SST & \cmark &  69.1 & 79.3 & 80.1 & 81.1 & 80.7\\
            SARB & \cmark &  75.5 & 79.0 & 80.4 & 80.8 & 80.8\\
            \ours & \cmark & \textbf{ 78.4} &\textbf{ 81.0} & \textbf{82.0} & \textbf{82.5}  & \textbf{82.8 } \\
             \Xhline{3\arrayrulewidth} 
        \end{tabular}
        } 
    \end{center}
\vspace{-10pt}
\end{table} \textbf{Effectiveness of Text Supervision.} To show the effectiveness of text supervision from label space, we compare the model learned with discrete label space (``Discrete Label'') with three methods (SST~\cite{chen2022structured}, SARB~\cite{pu2022semantic}] and  \ours) which introduce the textual space to utilize the contextual correlation of labels in Table~\ref{table:semantic_guide}. We find that methods with text supervision usually perform better than the method only using discrete labels. However, when the semantic annotations are limited, text supervision sometimes yields worse performance (e.g. mAP of SST is  lower than Discrete Labels with only  of labels). By adopting the well-pretrained visual-textual alignment, \ours achieves a great performance (\textit{e.g.}  higher than Discrete Labels with  of labels) and quickly adapts to the dataset even with limited labels.

\begin{table}
    \begin{center}
    \vspace{-10pt}
     \caption{\small \textbf{Ablation on Linguistic Inputs for Zero-Shot Learning of MS-COCO.}}~\label{table:ablation_prompt}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c c c c c c  c c c  }
            \Xhline{3\arrayrulewidth} 
            & \multirow{2}{*}{Linguistic Input} & \multirow{2}{*}{\#P}  & \multicolumn{3}{c}{ZSL} & \multicolumn{3}{c}{GZSL}  \\
            & & & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} \\
            \Xhline{3\arrayrulewidth} 
            \scriptsize{M0} &Contextless Classname & 0 & 5.2 & 12.9 & 7.4 & 3.5 & 4.1 & 3.8 \\
            \scriptsize{M1} &Hand-crafted Pos./Neg. Templates + Classname & 0 & 25.6 & 63.6 & 36.5 &  31.0 & 36.2 & 33.4  \\
            \scriptsize{M2} &Pos. Learnable Prompt + Classname (=64) & 0.01M & 31.2 & 77.5 & 44.5 & 55.7 & 65.0 & 60.0 \\
            \scriptsize{M3} &Neg. Learnable Prompt + Classname (=64) & 0.01M & 9.3 & 23.0 & 13.2 & 2.6 & 3.0 & 2.8 \\
            \hline
            \scriptsize{M4} &Dual Learnable Prompts + Classname (=64) & 0.02M & 35.3 & 87.6 & 50.3  &	\textbf{58.4} & \textbf{68.1} & \textbf{62.9}  \\
            \scriptsize{M5} &Dual Learnable Prompts + Classname (=32)  & 0.01M & \textbf{35.8} & \textbf{88.9} &	\textbf{51.0} & 57.4 &  67.0 &  61.9  \\
             \Xhline{3\arrayrulewidth} 
        \end{tabular}
        } 
    \end{center}
\end{table}




 \textbf{Ablation of Prompt Design.}  We compare our proposed dual learnable prompts with two hand-crafted prompts and one prompt learning method on the MS-COCO dataset with the zero-shot setting (see Table~\ref{table:ablation_prompt}). Hand-crafted prompts can use either contextless class names~\cite{li2017learning} or manually designed prompt templates. In our experiments, we carefully choose the positive and negative prompt templates as ``a photo of a [classname]'' and ``a photo without a [classname]''. In contrast with performing the binary classification for each class with dual learnable prompts as the input, we also experiment with learning a single prompt of positive or negative contexts and use a chosen threshold (0.5 in our experiment) to make the prediction for each class. 
As we can see, the single positive prompt learning method (M2) performs better than non-learnable methods (M0 and M1), and a single negative learnable prompt (M3) achieves much worse accuracy than its positive counterpart (M2).  
However, when we include both positive and negative prompts, dual prompts (M4) performs even better than a single prompt, which indicates that \ours learns complementary and beneficial information in the dual prompt pair. 
To keep the same amount of learnable parameters as in single prompt settings, we also halve the token size (M5), and find that \ours still outperforms two single prompts in M2 and M3 by large gaps, demonstrating the effectiveness of our dual-prompt design.





\begin{table}
    \begin{center}
     \caption{\small \textbf{Comparison between multi-headed attention and class-specific feature aggregation on MS-COCO} }~\label{table:mha_vs_conv_proj}
        \resizebox{0.9\linewidth}{!}{
        \begin{tabular}{c| c c c | c c c c c  }
            \Xhline{3\arrayrulewidth} 
            Visual Aggregation & Finetune. & Train Res. & Test Res. &   &  &  &  &   \\
            \Xhline{3\arrayrulewidth} 
              \multirow{4}{*}{\makecell{Multi-Headed \\ Attention}}&  \xmark & 224 & 224 & 70.4 & 74.1 & 74.8 & 75.4 & 75.7 \\
            &  \xmark  & 224 & 448 & 65.9 & 70.2 & 71.2 & 72.0 & 72.1 \\
            &  \xmark  & 448 & 448 & 72.1 & 75.5 & 76.5 & 77.1 & 77.3 \\
            &  \cmark  & 448 & 448 & 74.1 & 77.6 & 78.2 & 78.5 & 78.4 \\
            \hline
              \multirow{3}{*}{\makecell{Class-Specific \\ Feature Aggregation \\ (\ours)}} & \xmark  & 224 & 224 & 73.1 & 76.4 & 77.7 & 78.2 & 78.4 \\
            & \xmark  & 224 & 448 & 76.0 & 78.1 & 79.5 & 80.3 & 80.5 \\
            & \xmark  & 448 & 448 &  \textbf{78.4} & \textbf{81.1} & \textbf{82.0} & \textbf{82.5} & \textbf{82.8} \\
             \Xhline{3\arrayrulewidth} 
        \end{tabular}
        } 
    \end{center}
\vspace{-10pt}
\end{table} \textbf{Multi-Headed Attention vs. Class-Specific Region Aggregation.} 
In Table~\ref{table:mha_vs_conv_proj}, we compare the adaptive ability of these two visual aggregation methods when training/testing with a larger resolution (see Table~\ref{table:mha_vs_conv_proj}), which is crucial in multi-label recognition as spatial details matter. For a fair comparison, we only replace the class-specific region aggregation in \ours with the original multi-headed attention layer in CLIP~\cite{radford2019language} at the end of the visual encoder. We adaptively resize the input feature map to match the input dimension of the multi-headed attention layer.  

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\vspace{-10pt}
\includegraphics[width=0.99\linewidth]{figures/aggre.pdf}
    \caption{\small{Comparison among different aggregations on MS-COCO using partial labels.}}
\label{fig:aggre}
\vspace{-0.3cm}
\end{wrapfigure}

As shown in Table~\ref{table:mha_vs_conv_proj}, multi-headed attention is bonded to the pre-training image resolution (224 in CLIP), while our class-specific region aggregation benefits from the increased input resolution either during training or in inference. Our class-specific feature aggregation uses original weights, but actually performs better than finetuning the original multi-headed attention layer.

\textbf{Ablation of Aggregation Function.} We experiment with different functions to aggregate the regional logits for each class in Fig.~\ref{fig:aggre}. We compute final logits in three ways: (1) taking the average of logits at all spatial locations (``Ave''), (2) taking the region with the largest positive logit (``Max''), and (3) generating aggregating weights for all spatial locations via a softmax function over the positive logits (``Ours'').  ``Max'' performs better than ``Ave'', which indicates the regional feature is more informative than the global feature in multi-label recognition. Furthermore, by taking account of both the regional and the global features, ``Ours'' gives the best performance.



