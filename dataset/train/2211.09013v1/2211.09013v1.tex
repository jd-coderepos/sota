

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor,color,colortbl}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newtheorem{myDef}{\textit{Definition}}
\newtheorem{myTheo}{Theorem}

\def\cvprPaperID{*****} \def\confName{CVPR}
\def\confYear{2023}



\def\cvprPaperID{***} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{Masked Reconstruction Contrastive Learning with \\ Information Bottleneck Principle}

\author{Ziwen Liu\\
University of Chinese Academy of Sciences\\
{\tt\small liuziwen18@mails.ucas.ac.cn}
\and
Bonan Li\\
University of Chinese Academy of Sciences\\
{\tt\small libonan@ucas.ac.cn}
\and
Congying Han \thanks{Corresponding author}\\
University of Chinese Academy of Sciences\\
{\tt\small hancy@ucas.ac.cn}
\and
Tiande Guo\\
University of Chinese Academy of Sciences\\
{\tt\small tdguo@ucas.ac.cn}
\and 
Xuecheng Nie\\
MT Lab, Meitu Inc., China\\
{\tt\small nxc@meitu.com}
}

\maketitle

\begin{abstract}
Contrastive learning (CL) has shown great power in self-supervised learning due to its ability to capture insight correlations among large-scale data. Current CL models are biased to learn only the ability to discriminate positive and negative pairs due to the discriminative task setting. However, this bias would lead to ignoring its sufficiency for other downstream tasks, which we call the discriminative information overfitting problem. In this paper, we propose to tackle the above problems from the aspect of the Information Bottleneck (IB) principle, further pushing forward the frontier of CL. Specifically, we present a new perspective that CL is an instantiation of the IB principle, including information compression and expression. We theoretically analyze the optimal information situation and demonstrate that minimum sufficient augmentation and information-generalized representation are the optimal requirements for achieving maximum compression and generalizability to downstream tasks. Therefore, we propose the Masked Reconstruction Contrastive Learning~(MRCL) model to improve CL models. For implementation in practice, MRCL utilizes the masking operation for stronger augmentation, further eliminating redundant and noisy information. In order to alleviate the discriminative information overfitting problem effectively, we employ the reconstruction task to regularize the discriminative task. We conduct comprehensive experiments and show the superiority of the proposed model on multiple tasks, including image classification, semantic segmentation and objective detection.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Contrastive Learning (CL) is a fundamental and important problem in the self-supervised learning field. It aims to maximize the similarity of positive pairs and dissimilarity of negative ones without requirements of explicit data labeling. Due to the strong capability of scaling up training set and extracting representative features, it has shown promising results in a variety of tasks, such as image classification~\cite{yang2022unified,he2020momentum}, semantic segmentation~\cite{wang2021exploring,zhao2021contrastive} and object detection~\cite{xie2021detco,yang2021instance}. 
\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/motivation.pdf}
    \caption{Motivation of our MRCL model. (a) We propose to exhibit an minimum sufficient augmentation that only compresses redundancy and remains essential information; (b) Only discriminative pretext task leads to the discriminative information overfitting problem, which degrades the performance on the various downstream tasks.}
    \label{fig:motivation}
\end{figure}



Existing works~\cite{chen2020simple,bachman2019learning,tian2020makes} on CL explore data augmentation to enhance generalizability and verify that a strong augmentation method can benefit the performance. However, due to the low information density of image data~\cite{he2022masked,xie2022simmim}, there is still much noisy and redundant information after their proposed augmentation, which would affect the extraction of desired information and thus limit the performance of CL in practice. Therefore, we need stronger information compression methods to yield~\textit{minimum sufficient augmentation} that can remove all redundant information.

In addition, for the pretext task setting, current CL models choose the discriminative task, which aims to measure and separate the distances of positive and negative samples to distinguish different samples. To keep the simplicity and efficacy, they focus on the discrimination of samples at the abstract semantic level of the feature space rather than complicated details.
However, the isolated discriminative task leads the model to fall into the bias that only extracts the information to help discriminate samples, i.e.,~\textit{discriminative information overfitting} problem, thus degrading the performance on different downstream tasks. 



Motivated by this, we propose to tackle the above problems from the perspective of the Information Bottleneck~(IB) principle~\cite{tishby2015deep,tishby99information,shwartz2017opening}, further pushing forward the frontier of CL. In particular, the IB principle aims to analyze information changes in the learning process of deep neural networks, relying on the idea of the simultaneous existence of information compression and information expression.
It has been widely applied to interpretability and engineering deep learning~\cite{alemi2017vib,voloshynovskiy2020variational,wu2020graph,wan2021multi,gao2021information}. Given this inspiration, we utilize IB principle to make theoretical analysis on the information variation and demonstrate that CL is actually an instantiation of the IB principle. We apply this principle at each processing stage for extracting the optimal information, thus leading to improved performance of CL.

Specifically, this paper presents a novel Masked Reconstruction Contrastive Learning (MRCL) model. To compress the redundant information, existing works exploit data augmentation transformer to generate views and then further derive compact feature information shared by all views with multi-view architectures. However, they suffer from excessive redundancy problems when meeting image data with low information density.
Differently, our MRCL model conducts redundant information removal at the pixel level to attain the minimum sufficient augmentation. As shown in Fig.\ref{fig:motivation}(a), minimum sufficient augmentation can generate multiple views that share exactly the required downstream task information content. In addition, information expression of existing CL models is determined by the pretext task, \emph{i.e.,} discrimination task. This setting makes the model biased to extract the discriminative information, resulting in the overfitting problem and poor generalizability to different downstream tasks, as shown in Fig.\ref{fig:motivation}(b). To solve this problem, our MRCL model rectifies the information content expressed by representation with more generalized and critical information. In this way, MRCL is enhanced in generalizability. By combining the designs in terms of both redundant information compression and better information expression, our MRCL model can achieve a better information state. 

For implementation, MRCL combines the mask operation with the current data augmentation mechanism in CL to enhance pixel level compression. Especially this masking operation has been verified to be effective in Masked Image Modeling~(MIM) and Masked Language Modeling~(MLM)~\cite{he2022masked,xie2022simmim,devlin2018bert,brown2020language}. The proposed new module can produce stronger augmentation results, thus improving the results\cite{chen2020simple,bachman2019learning}. In addition, MRCL introduces a reconstruction task as a regularization to make corrections on the information expression, avoiding the information bias and alleviating the overfitting problem. The backbone design of the encoder is also a point of concern. Following prior works, MRCL adopts a siamese structure encoder~\cite{bromley1993signature,chen2020simple,grill2020bootstrap,he2020momentum,chen2020improved,chen2021empirical,zbontar2021barlow,chen2021exploring} to learn the representations. In addition, MRCL applies Vision Transformer (ViT)~\cite{dosovitskiy2020image,liu2021swin} as the backbone, in light of its excellent representation learning capability for image data.

We conduct comprehensive experiments on multiple tasks to verify the efficacy of the proposed model, including image classification, semantic segmentation, and object detection. Results show that our MRCL model achieves superior performance over the prior state-of-the-art on multiple benchmarks (ImageNet1K~\cite{deng2009imagenet}, ADE20K~\cite{zhou2019semantic} and MSCOCO~\cite{lin2014microsoft}). In summary, our contributions are fourfold: 1) We present a new perspective on CL using the information bottleneck principle and theoretically analyze the information compression and expression therein. To our best knowledge, we are the first to point out problems of inadequate compression and discriminative information overfitting in CL. Given this, we define the minimum sufficient augmentation and information-generalized representation.
2) We present a novel combination of masking operation and basic data augmentation for deriving strong augmentation results, improving the performance of contrastive learning. 
3) We present an effective reconstruction task to regularize the discriminative task, leading to improved generalizability.
4) With the proposed model, called MRCL, we set new state-of-the-art for multiple tasks on multiple benchmarks. We also provide extensive analytical experiments to understand further our theoretical analysis and verify the rationality and effectiveness of our proposed components.



\section{Related Work}
\label{sec:rela}
\subsection{Contrastive Learning (CL)}
Contrastive learning aims to learn representations that can discriminate between different samples, which is achieved by bringing the positive pairs closer together and the negative pairs farther apart. CL first utilizes data augmentation to obtain multi-views, after which Siamese network~\cite{chen2021exploring,grill2020bootstrap} is used to encode the views further.
Prior symbolic works, like MoCo~\cite{he2020momentum,chen2020improved,chen2021empirical}, SimCLR~\cite{chen2020simple}, BYOL~\cite{grill2020bootstrap} and SimSiam~\cite{chen2021exploring}, have made significant model improvements, such as projector, predictor, momentum encoder, memory bank, to address some critical problems in contrastive learning. In the meantime, other efforts have been made to introduce new insights and improvements from an information perspective, which are more relevant to our work. InfoMin~\cite{tian2020makes} has given an insight that good views share the minimum necessary information to perform the downstream tasks well. Therefore, it gives a learned perspective generator and minimizes the mutual information between the two views. Nevertheless, this work only reduces the gap between shared and optimal information while ignoring demands for minimum information.
In addition, H. Wang~\textit{et al.}~\cite{wang2022rethinking} argues that non-shared task-related information can not be ignored and thus retains non-shared usable information by maximizing the mutual information of view and representation.

The relevant works mentioned above only consider the information problem in the model in a one-sided way, without considering it as a whole. In contrast, our work analyzes the holistic aspects of CL and thus proposes new improvements at each stage, with a balance between information compression and expression.

\subsection{Information Bottleneck~(IB) Principle}
Information bottleneck~(IB) principle~\cite{tishby99information,shwartz2017opening,alemi2017vib,voloshynovskiy2020variational} is an approach based on information theory, which formally describes meaningful and relevant information in the data. IB principle states that the model will become more robust to downstream tasks if the obtained representations discard information in the input that is not helpful for the given task. Specifically, given the original data  with the label , the IB principle proposes to learn a compact and informative representation  of data . The objective of the IB principle can be described as follows:

where  denotes the mutual information between two random variables.  denotes the parameters in encoder network.  is the upper bound of mutual information , indicating the level of information compression. And in the Lagrangian formulation as:

where  is the hyper-parameter balancing the compression term  and expression term .

Following IB, Alemi~\textit{et al.}~\cite{alemi2017vib} presented a variational approximation form for practical computation and optimization. Voloshynovskiy~\textit{et al.}~\cite{voloshynovskiy2020variational} also gave a new perspective that existing unsupervised, supervised methods based on VAE, GAN are products of the IB framework. Zbontar~\textit{et al.}~\cite{zbontar2021barlow} presented the cross-correlation matrix as the similarity metric, resulting in a new CL method, Barlow Twins. They also found that their method is an instantiation of the IB principle. Following Wang~\textit{et al.}~\cite{wang2019deep}, CMIB~\cite{wan2021multi} extended IB to multi-view representation learning by maximizing both the mutual information between learned representation and shared representation as well as view-specific representation while reducing the unnecessary information by minimizing the mutual information between representation and original representation.

Our paper also explores the understanding of CL under the framework of IB principle, pointing out how information compression and expression are implemented and what problems exist in the CL framework. To improve these two information processing stages, we respectively use masking operation and reconstruction task to achieve better performance.

\subsection{Masked Image Modeling~(MIM)}
Self-supervised Learning~(SSL) seeks to alleviate the strong dependence of deep learning on data labels and has done many promising works, but more improvements in performance are needed. The masked autoencoder method has been widely applied to Natural Language Processing (NLP)~\cite{devlin2018bert,brown2020language} and has shown great performance. In the CV field, BEiT~\cite{bao2021beit} quantized image patches as discrete tokens using an off-the-shelf discrete VAE(dVAE) tokenizer~\cite{ramesh2021zero}, then proposes to predict the masked tokens. Recently, masked autoencoder~(MAE)~\cite{he2022masked} and SimMIM~\cite{xie2022simmim} developed the MIM methods~\cite{wei2022masked,bao2021beit,zhou2021ibot}, using vision transformers~(ViT)~\cite{dosovitskiy2020image} backbone to narrow the data distinction between computer vision and natural language. 

These works have found that computer vision data are natural signals with heavy spatial redundancy. A simple masking operation can directly remove the redundant information by discarding pixels with excellent performance, encouraging us to further enhance the CL data augmentation at the pixel level, i.e., masking out patches.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.8\columnwidth]{figures/architecture.pdf}
    \caption{The diagram of our model. We build an encoder-decoder network to learn representation and reconstruct the image. Besides the data augmentation, we further randomly mask out the image patches to reduce the redundant information. Only the unmasked patches are encoded, and then the mask tokens are introduced after the encoder. Both unmasked patches and mask tokens are fed to the decoder to reconstruct the original image.} 
    \label{fig:architecture}
\end{figure*}
\section{Information Analysis and MRCL}
\subsection{Information Analysis on Contrastive Learning}
\label{sec:IBCL}
As illustrated in Section~\ref{sec:intro}, we make a theoretical analysis of CL from an information perspective and find that it is an instantiation of the IB principle. Therefore, this section discusses how information compression and expression work in CL in detail.

For the information compression, both data augmentation (DA)~ and encoder~ play an essential role, where DA achieves it by different transformations to remove the variance of data, and encoder by siamese network structures and powerful feature extraction capabilities.

Regarding information expression, the discriminative task  for positive and negative samples enables the representation to express the discriminative information. Thus we can describe CL as follow:

where  denotes the compression level. 
In the following, we make a definition of optimal augmentation method and representation with generalized information.

\begin{myDef}
(Sufficient Augmentation) Given an augmentation transformation , we get an augmentation view  of a sample . The augmentation  is called sufficient augmentation for task , if and only if .
\end{myDef}

Intuitively, sufficient augmentation ensures that information about desired downstream tasks  is not removed, thus ensuring that the information in the obtained multi-views works well in the downstream tasks. However, it also remains redundant information that degrades the performance.

\begin{myDef}\label{min_suff_aug}
(Minimum Sufficient Augmentation) The sufficient augmentation is called minimum sufficient augmentation, , if and only if its two augmented views satisfy  ,  that is sufficient augmentation.
\end{myDef}

Based on the sufficient augmentation, the minimum sufficient augmentation further removes redundant information (non-task information) to avoid degrading the performance of the downstream tasks. In conclusion, previous works on augmentation are about finding augmentations that satisfy the minimum sufficient augmentation requirement.

\begin{myDef}\label{representation}
(Information-generalized representation) We denote all tasks set as . Thus, the discriminative information over-fitting representation  can be expressed as: ; The representation is called information-generalized representation  if and only if , and when the equality holds, it is called the maximal information-generalized representation.
\end{myDef}

Since CL models only seek to discriminate the samples, the \textit{discriminative information overfitting} problem arises, which causes the learned representations to express only discriminative information and thus perform poorly in other downstream tasks. Therefore, to perform well in various downstream tasks, we want the learned information-generalized representation to be enriched with information about more general downstream tasks.
\subsection{A Revisit of Vision Transformers~(ViT)}
Due to the extraordinary performance of vision transformer~(ViT)~\cite{dosovitskiy2020image}, we adopt the ViT backbone in our method. Following the vanilla ViT, we split the image  with patch size , yielding a sequence of flattened 2D patches , where  is the resolution of the original image,  is the number of channels,  denotes the number of resulting patches. Then we transfer it to  dimensions with the linear projection, as formulated in Eq~\ref{eq:lp}. We refer to the output of this projection as the patch embeddings. Also, we prepend a learnable embedding () to the obtained patches embedding. To retain the positional information, positional embeddings~ are added to the patch embeddings. The resulting sequences are severed to the transformer encoder~\cite{vaswani2017attention}, which consists of  layers of multi-head self-attention~(MSA) and MLP blocks. LayerNorm~(LN) is applied before every block, and residual connections after every block~\cite{baevski2018adaptive,wang2019learning}. The equation is as follow:


\subsection{Our Method}
The overall diagram of our model is illustrated in Fig.~\ref{fig:architecture}, which includes three main components. Data augmentation transfers the input image to multi-views, which keeps the image in-variance and removes variance. Unlike previous CL works, we further conduct a masking operation to compress information at the pixel level. The encoder transforms the obtained view into a hidden representation that expresses the extracted data information. Here we adopt ViT, which shows stronger feature extraction capability, as the encoder backbone. The decoder is used to get the image reconstruction, which does not exist in CL models. In this way, we prevent the model from being biased to extract only the discriminative information, thus achieving better downstream tasks performance. Details are given below.


Assuming that original data augmentation transfer the input image  into the multi augmented view , we then get the masked views  with a mask ratio . Then we can get the representation  of masked view .
\paragraph{Masking and Encoder}
Inspired by MIM methods, the masking operation is an efficient way to remove redundant information. As we have analyzed, compressing redundant information is exactly what data augmentation requires. Therefore, after the regular augmentation transformation, we give a masking operation  into the masked augmented view~:


where  denotes the mask ratio portion. There exist many masking strategies like 'block-wise'~\cite{bao2021beit}, 'grid-wise' and 'random sampling', where 'random sampling' has shown better performance in many works. Therefore, our paper adopts 'random sampling' as the default masking strategy. The higher the ratio of random sampling is, the more blocks are masked out, i.e., the more redundancy is eliminated. In practice, we first patchify the augmented views  and then generate a token for every input patch by linear projection with an added positional embedding. Next, we shuffle the tokens list randomly and drop the last part of the list according to the masking ratio .
For better adaptation to patches masking and better feature information extraction ability, our encoder is ViT~\cite{dosovitskiy2020image} but only applied to the unmasked patches of . Following vanilla ViT, our encoder embeds the patches by linear projection, adds positional embedding, and then feeds the resulting patch set into a series of Transformer blocks, obtaining representation .

\begin{table*}[th!]
    \centering
    \small
    \begin{tabular}[width=\textwidth]{l|ccccc}
    \toprule
        Method & Arch &Params.(M) & Pretraining-epochs & Supervision& Top-1 accuracy(\%)   \\
        \midrule
         BYOL\cite{grill2020bootstrap} & ResNet-50 & 23&1000 &RGB&74.3\\
         SimCLR\cite{chen2020simple} & ResNet-50 &23 & 1000&RGB&69.3\\
         BarTwins\cite{zbontar2021barlow} & ResNet-50&23&1000 &RGB&73.2\\
         InfoMin\cite{tian2020makes} & ResNet-50 & 23& 800 &RGB&73.0\\
         MoCo-v2\cite{chen2020improved} & ResNet-50&23&800&RGB&71.1\\
         InfoCL\cite{wang2022rethinking}& ResNet-50&23&200&RGB&61.6\\
         \midrule
         CAE\cite{chen2022context} & ViT-B & 88 & 600 & DALLE+RGB & 68.3\\
         MoCo-v3\cite{chen2021empirical} & ViT-B&88&600&RGB&76.5\\
         DINO\cite{caron2021emerging} & ViT-B & 88 & 300 & RGB & \textbf{\textcolor{blue}{80.1}}  \\
         BEiT\cite{bao2021beit} & ViT-B & 86 & 800  & RGB & 56.7  \\
         SimMIM\cite{xie2022simmim} & ViT-B & 88 & 800 & RGB & 56.7    \\
         iBOT\cite{zhou2021ibot}& ViT-B & 88 & 1600 & RGB & 79.5\\
         MaskFeat\cite{wei2022masked} & ViT-L & 218 & 1600 &HOG& 67.7\\
         MAE\cite{he2022masked} & ViT-L & 218 & 1600 & RGB & 75.1\\
         \midrule[1pt]
         SimCLR(w/ ViT) & ViT-B & 87 & 600 &RGB& 73.1 \\
         \rowcolor{gray!20} MR SimCLR() & ViT-B & 113 & 600 &RGB&  \\
         \midrule
         BarTwins(w/ ViT) & ViT-B & 87 & 600 &RGB& 74.1\\
         \rowcolor{gray!20} MR BarTwins() & ViT-B & 113 & 600 &RGB& \\
         \bottomrule
    \end{tabular}
    \caption{Results in ImageNet1K~\cite{deng2009imagenet}. We evaluate our masked reconstruction method with other models, including ResNet and ViT backbones. The \colorbox{gray!20}{Gray} lines denote the results of our MRCL methods, including MR BarTwins and MR SimCLR. The \textbf{Bold} and \textbf{\textcolor{blue}{Blue}} denote the best and second results, respectively.}
    \label{table:Imagenet}
\end{table*}

\paragraph{Reconstruction Decoder}
To alleviate the \textit{discriminative information overfitting} problem, we conduct a decoder to reconstruct views. Although the encoder only encodes the unmasked blocks, the input to the decoder includes encoded visible patches and mask tokens, which are shared, learned vectors indicating the presence of missing patches to be predicted. Moreover, we also add the positional embeddings to the full tokens list to encode the location information. Following MAE, we adopt the ViT as the backbone of the decoder . Due to the independence of the encoder and decoder, it is flexible to design the decoder architecture. In our experiments, we conduct a small and narrow decoder, which forms an asymmetric structure together with the encoder.
Thus, we can get the reconstruction image  with loss:

Since  and the entropy term  is is not related to , maximizing  is equivalent to decreasing the conditional entropy , i.e, encouraging  to express more information about . However the distribution  is practically intractable, therefore a hypothetical prior distribution  is often used to approximate , such as Bernoulli distribution, Gaussian distribution or
Laplace distribution. Here we utilize the Gaussian distribution as the prior, and we therefore get the derivation of Eq.\eqref{con_loss}:

where  is the decoder network to compute the mean of Gaussian prior distribution.  is a constant which can be ignored for optimization.

As we described, the patches of reconstruction image  include masked and unmasked, which correspond to the prediction and reconstruction task, respectively. Therefore, we seek to explore the respective significance of these two underlying tasks to the model, and Eq.\eqref{eq:rec} can be decomposed as two terms:


\paragraph{Training Objective}
As our analysis in Section.\ref{sec:IBCL}, CL can be written as the form IB framework, Eq.\eqref{eq:con_ib}. Then together with our practical designs of the masking operation  and reconstruction task, we can further derive our MRCL model as:



Specifically, after expanding the comparison learning loss term  and the reconstruction loss term , the model can be derived as:

where  and  are hyper-parameters, weighting the corresponding tasks losses.
The term  denotes the reconstruction loss on masked patches, which is essentially considered a prediction task. The term  indicates the loss of reconstructing unmasked patches.

\section{Experiments}
In this section, we first verify the effectiveness of our models on ImageNet1K~\cite{deng2009imagenet}. Our method is a refinement of the contrastive learning framework. Therefore, we choose three dominant models: SimCLR~\cite{chen2020simple}, BYOL~\cite{grill2020bootstrap} and Barlow Twins~\cite{zbontar2021barlow} to make improvements for experimental comparison.

\paragraph{Pretraining} Following MoCo~v3~\cite{chen2021empirical}, we adopt the AdamW optimizer as default, and the momentum is set to . Besides, the weight decay is set to  and the linear scaling rule~\cite{goyal2017accurate}:  is introduced to set the learning rate. The base learning rate  is  with the batch size of . Note that cosine learning rate schedule~\cite{loshchilov2016sgdr} with a warmup~\cite{goyal2017accurate} is adopted for early 30 epochs. All pre-training experiments are conducted on 8 NVIDIA RTX3090 GPUs.

\begin{table*}[t!]
    \centering
    \small
    \renewcommand\arraystretch{1.1}
    \begin{subtable}[h]{0.42\textwidth}
        \setlength{\tabcolsep}{6pt}
        \centering
        \begin{tabular}{l|cccc}
    \toprule
        Method & Pre-Epoch & mIoU  \\
        \midrule
        MoCo-v3\cite{chen2021empirical} & 300 & 47.3\\
        BEiT\cite{bao2021beit} & 800 & 47.1\\
        iBOT\cite{zhou2021ibot} & 1600 & \textbf{\textcolor{blue}{50.0}}\\
        CAE\cite{chen2022context} & 1600 & 44.0\\
        DINO\cite{caron2021emerging} & 400 & 47.2\\
        MAE\cite{he2022masked} & 1600 & 48.1\\
        \midrule
        MR BarTwins & 600 & \textbf{51.5}()\\
        MR SimCLR & 600 & \textbf{51.3}()\\
        \bottomrule
    \end{tabular}
      \caption{Semantic segmentation results on ADE20K. Upernet~\cite{xiao2018unified} is used as the default segmentation framework.}
      \label{tab:ADE20K}
    \end{subtable}
    \hfill
    \begin{subtable}[h]{0.54\textwidth}
        \centering
        \setlength{\tabcolsep}{8pt}
        \begin{tabular}{l|ccccc}
    \toprule
         Method & Epochs &  &   \\
         \midrule
         MoCo-v3\cite{chen2021empirical} & 300 & 47.9 & 42.7\\
         BEiT\cite{bao2021beit} & 800 & 49.8 & 44.4\\
         iBOT\cite{zhou2021ibot} & 1600 & 51.2 & 44.2 \\
         CAE\cite{chen2022context} & 1600 & 50.1 & 44.0\\
         SIM\cite{tao2022siamese} & 1600 & 49.1 & 43,8\\
         MAE\cite{he2022masked} & 1600 & \textbf{\textcolor{blue}{52.4}} & \textbf{\textcolor{blue}{46.5}}\\
         \midrule
         MR BarTwins & 600 & \textbf{53.3}& \textbf{46.6}\\
         MR SimCLR & 600 & \textbf{53.7} & \textbf{46.9} \\
         \bottomrule
    \end{tabular}
        \caption{COCO object detection and segmentation. We use the Mask R-CNN model~\cite{he2017mask} as our framework.}
        \label{tab:COCO}
     \end{subtable}
     \caption{Results on the transfer experiments including semantic segmentation
    and object detection. MR denotes the model that merges our method. The \textbf{Bold} indicates the best result and \textbf{\textcolor{blue}{Blue}} denotes the second place.}
     \label{tab:transfer_exp}
\end{table*}

\subsection{Results on ImageNet1K}
Following previous works~\cite{caron2021emerging,chen2021empirical,chen2020simple,bao2021beit}, we evaluate MRCL in ImageNet1K~\cite{li2021image} linear-probing. After pre-training, we follow~\cite{chen2021empirical} to use SGD optimizer for training 100 epochs. The learning rate is 0.2 with the batch size of  and  of 0. We only use random resized cropping and flipping augmentation. The classification top-1 accuracy results are shown in Table.~\ref{table:Imagenet}. Many typical and widely applied works, including works with the ResNet and ViT backbones, are in the comparison. We utilize two classical models as our baseline, BarTwins, and SimCLR (both with ViT-B backbone), and they are denoted as MR BarTwins and MR SimCLR.

As shown in Table.\ref{table:Imagenet}, MR BarTwins model achieves the state-of-the-art result , which is  higher than DINO~\cite{caron2021emerging}, indicating that improvements in information compression and expression can extract better information content to serve downstream classification tasks.

When , our MR SimCLR achieves  top-1 linear probing accuracy(\colorbox{gray!20}{Gray} in Table.\ref{table:Imagenet}), with  better than vanilla SimCLR (with ViT-B backbone) . When , MR BarTwins achieve the  accuracy (\colorbox{gray!20}{Gray} in Table.\ref{table:Imagenet}), which makes  improvement for original BarTwins model (with ViT-B backbone). These remarkable improvements are due to adding our masking and reconstruction tasks to the underlying models, allowing it to learn more valuable information. In addition,  means that the pixel prediction task contributes more than reconstruction, i.e., prediction can provide more information, which is also demonstrated in MAE~\cite{he2022masked} and SimMIM~\cite{xie2022simmim}.



\subsection{Transfer Results}
To further validate the transferability of our model, we follow previous methods to evaluate pretrained models on ADE20K~\cite{zhou2019semantic} semantic segmentation and MSCOCO~\cite{lin2014microsoft} object detection and segmentation. All models in the comparison adopt ViT-B backbone.

\paragraph{Semantic segmentation} We evaluate our MR BarTwins and MR SimCLR models on ADE20K dataset~\cite{zhou2019semantic}, which includes 25,562 images with 150 semantic categories. By default, Upernet~\cite{xiao2018unified} is used as the segmentation framework. Following the standard setting, the Mean Intersection over Union (mIoU) results are shown in Table.\ref{tab:ADE20K}. MR BarTwins and MR SimCLR achieve  and  mIoU, respectively, both surpassing the previous best result iBOT~\cite{zhou2021ibot}~(). This excellent performance on ADE20K demonstrates that our method effectively alleviates the \textit{discriminative information overfitting} problem, allowing the model to learn more generalized information that can be well applied to the semantic segmentation task with good performance.
\paragraph{Objective detection and segmentation} Our MRCL methods conduct the objective detection and segmentation experiments on MSCOCO dataset~\cite{lin2014microsoft}, which consists of  training,  validation, and  test-dev images. MRCL adopts the Mask-RCNN~\cite{he2017mask} as our default objection detection framework. As shown in Table.\ref{tab:COCO}, MRCL methods significantly improve the performance on MSCOCO object detection and segmentation. In the objective detection, MR SimCLR achieves the best results with  improvement on  than MAE~\cite{he2022masked} (). MR BarTwins also get competitive results of . In the objective segmentation, MR SimCLR significantly improves  over MAE by 0.4 points (). These promising results again verify the effectiveness of our method in various downstream tasks, which benefited from our regularization reconstruction task.
\subsection{Analytical Experiments}
Here, we provide some analytical experiments to understand our model design further. The analytical experiments are performed on STL-10 dataset~\cite{coates2011analysis} and the base models are BarTwins~\cite{zbontar2021barlow} and BYOL~\cite{grill2020bootstrap}. We random crop and resize the STL-10 data to . We also adopt the AdamW optimizer, and the base learning rate is  with a batch size of . The default training epochs are set to 300. Following experiments on ImageNet, we adopt a learning rate warmup for 30 epochs. After the warmup, the learning rate follows a cosine decay schedule~\cite{loshchilov2016sgdr}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/ratios_ablation.pdf}
    \caption{The effect of mask ratio. The red dotted line denotes the accuracy of baseline model.}
    \label{fig:ratio_abl}
\end{figure}
\paragraph{Masking Ratio} According to our analysis based on the information bottleneck principle, mask ratio indicates the degree of information compression: the larger the ratio, the more compression is obtained, and vice versa the smaller. We fix the reconstruction hyperparameter  and  in MR BarTwins and MR BYOL, respectively. The results are present in Fig.~\ref{fig:ratio_abl}, where  denotes that we only add the reconstruction task to the base model without masking. When , both models show an accuracy improvement, indicating that reconstruction regularization provides an excellent alleviation for the \textit{discriminative information overfitting} problem. The MR BarTwins and MR BYOL models show the best result when  and the whole accuracy curve shown in Fig.~\ref{fig:ratio_abl} shows a reversed U-shape. This indicates that there is still redundancy information after the basic augmentation (), thus reducing the performance. Moreover, overly strong compression (higher mask ratio) leads to loss of valuable information and thus degrades performance too. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/recon_hyper_ablation.pdf}
    \caption{Ablation on the hyperparameter of reconstruction loss. The red dotted line denotes the accuracy of baseline model.}
    \label{fig:abl2}
\end{figure}

\paragraph{Reconstruction Weight}
Our MRCL method introduces a reconstruction task to lead the model to express more essential information. Here we ablate the weight of the reconstruction loss to explore the importance of the task. The mask ratio is fixed at  and , respectively, in MR BarTwins and MR BYOL. The results in Fig.~\ref{fig:abl2} show that when , MR BarTwins get the best performance, which was  higher than the baseline model~(w/o reconstruction task). MR BYOL achieves the best performance when , with  higher than the base model. Both models have minimum and maximum thresholds beyond which performance degradation can occur. A small reconstruction hyperparameter force the model to learn based on the discriminative task, which still suffers from the discriminative information overfitting problem. In the meantime, a larger reconstruction parameter makes the model entirely biased to reconstruct pixels, ignoring the extraction of semantic information from the image and degrading the performance.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/patches_hyper_ablation.pdf}
    \caption{Exploratory experiments on the importance of masked and unmasked patches. The red dotted line denotes the accuracy of baseline model.}
    \label{fig:abl3}
\end{figure}
\paragraph{Reconstruction vs. Prediction}
As we described, the reconstruction loss can be decomposed as Eq.~\eqref{eq:rec_de}, including unmasked patches reconstruction and masked patches prediction. Here we ablate the weight of these two different tasks to improve the model, with fix the mask ratio  and masked patches weight  in MR BarTwins (MR SimCLR). We get different  ratios by adjusting the unmasked patches weight .  indicates more focus on prediction tasks, while  indicates more attention on reconstruction.
Results in Fig.~\ref{fig:abl3} show that more mask weighting (i.e.,  in MR BarTwins and  in MR BYOL, respectively) gives the best results. However, an upper limit exists beyond which the model learning becomes completely biased towards prediction, leading to greater difficulty and performance degradation. Also, this mask ratio has a lower bound, namely the relatively excessive weight of unmasked patches. In this case, the model collapses to a pure reconstruction task, which loses information from the unmasked patches, leading to poor results. These result curves also show a reversed U-shape, demonstrating our minimum sufficient information statement.
\section{Discussion and Conclusion}
As a critical problem of self-supervised learning, contrastive learning has become significant in deep learning due to its non-dependence on labels and good performance in downstream tasks. However, its discriminative task setting leads to the \textit{discriminative information overfitting} problem in the learned representation, degrading performance in various downstream tasks. Here we first make a theoretic analysis of the contrastive learning framework via the IB principle, pointing out the information compression and expression process. We then make the following improvements in the respective process: for better information compression, we utilize the masking operation to achieve stronger data augmentation, thus removing unnecessary and redundant information; for resolving the \textit{discriminative information over-fitting} problem to achieve better expression, we propose to add a regularization reconstruction task to learn information-generalized representation. In practice, the state-of-the-art results in ImageNet1K image classification, ADE20K semantic segmentation, and COCO objective detection and segmentation illustrate that the MR approach practically implements our analysis based on the IB principle.

\begin{thebibliography}{10}\itemsep=-1pt
	
	\bibitem{alemi2017vib}
	Alex Alemi, Ian Fischer, Josh Dillon, and Kevin Murphy.
	\newblock Deep variational information bottleneck.
	\newblock In {\em ICLR}, 2017.
	
	\bibitem{bachman2019learning}
	Philip Bachman, R~Devon Hjelm, and William Buchwalter.
	\newblock Learning representations by maximizing mutual information across
	views.
	\newblock {\em Advances in neural information processing systems}, 32, 2019.
	
	\bibitem{baevski2018adaptive}
	Alexei Baevski and Michael Auli.
	\newblock Adaptive input representations for neural language modeling.
	\newblock {\em arXiv preprint arXiv:1809.10853}, 2018.
	
	\bibitem{bao2021beit}
	Hangbo Bao, Li Dong, and Furu Wei.
	\newblock Beit: Bert pre-training of image transformers.
	\newblock {\em arXiv preprint arXiv:2106.08254}, 2021.
	
	\bibitem{bromley1993signature}
	Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S{\"a}ckinger, and Roopak
	Shah.
	\newblock Signature verification using a" siamese" time delay neural network.
	\newblock {\em Advances in neural information processing systems}, 6, 1993.
	
	\bibitem{brown2020language}
	Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
	Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
	et~al.
	\newblock Language models are few-shot learners.
	\newblock {\em Advances in neural information processing systems},
	33:1877--1901, 2020.
	
	\bibitem{caron2021emerging}
	Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal,
	Piotr Bojanowski, and Armand Joulin.
	\newblock Emerging properties in self-supervised vision transformers.
	\newblock In {\em Proceedings of the IEEE/CVF International Conference on
		Computer Vision}, pages 9650--9660, 2021.
	
	\bibitem{chen2020simple}
	Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
	\newblock A simple framework for contrastive learning of visual
	representations.
	\newblock In {\em International conference on machine learning}, pages
	1597--1607. PMLR, 2020.
	
	\bibitem{chen2022context}
	Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang,
	Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang.
	\newblock Context autoencoder for self-supervised representation learning.
	\newblock {\em arXiv preprint arXiv:2202.03026}, 2022.
	
	\bibitem{chen2020improved}
	Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
	\newblock Improved baselines with momentum contrastive learning.
	\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.
	
	\bibitem{chen2021exploring}
	Xinlei Chen and Kaiming He.
	\newblock Exploring simple siamese representation learning.
	\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition}, pages 15750--15758, 2021.
	
	\bibitem{chen2021empirical}
	Xinlei Chen, Saining Xie, and Kaiming He.
	\newblock An empirical study of training self-supervised vision transformers.
	\newblock In {\em Proceedings of the IEEE/CVF International Conference on
		Computer Vision}, pages 9640--9649, 2021.
	
	\bibitem{coates2011analysis}
	Adam Coates, Andrew Ng, and Honglak Lee.
	\newblock An analysis of single-layer networks in unsupervised feature
	learning.
	\newblock In {\em Proceedings of the fourteenth international conference on
		artificial intelligence and statistics}, pages 215--223. JMLR Workshop and
	Conference Proceedings, 2011.
	
	\bibitem{deng2009imagenet}
	Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
	\newblock Imagenet: A large-scale hierarchical image database.
	\newblock In {\em 2009 IEEE conference on computer vision and pattern
		recognition}, pages 248--255. Ieee, 2009.
	
	\bibitem{devlin2018bert}
	Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
	\newblock Bert: Pre-training of deep bidirectional transformers for language
	understanding.
	\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.
	
	\bibitem{dosovitskiy2020image}
	Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
	Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
	Heigold, Sylvain Gelly, et~al.
	\newblock An image is worth 16x16 words: Transformers for image recognition at
	scale.
	\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.
	
	\bibitem{gao2021information}
	Gege Gao, Huaibo Huang, Chaoyou Fu, Zhaoyang Li, and Ran He.
	\newblock Information bottleneck disentanglement for identity swapping.
	\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition}, pages 3404--3413, 2021.
	
	\bibitem{goyal2017accurate}
	Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
	Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
	\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
	\newblock {\em arXiv preprint arXiv:1706.02677}, 2017.
	
	\bibitem{grill2020bootstrap}
	Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre
	Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan
	Guo, Mohammad Gheshlaghi~Azar, et~al.
	\newblock Bootstrap your own latent-a new approach to self-supervised learning.
	\newblock {\em Advances in neural information processing systems},
	33:21271--21284, 2020.
	
	\bibitem{he2022masked}
	Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
	Girshick.
	\newblock Masked autoencoders are scalable vision learners.
	\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition}, pages 16000--16009, 2022.
	
	\bibitem{he2020momentum}
	Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
	\newblock Momentum contrast for unsupervised visual representation learning.
	\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
		pattern recognition}, pages 9729--9738, 2020.
	
	\bibitem{he2017mask}
	Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
	\newblock Mask r-cnn.
	\newblock In {\em Proceedings of the IEEE international conference on computer
		vision}, pages 2961--2969, 2017.
	
	\bibitem{li2021image}
	Xinyang Li, Shengchuan Zhang, Jie Hu, Liujuan Cao, Xiaopeng Hong, Xudong Mao,
	Feiyue Huang, Yongjian Wu, and Rongrong Ji.
	\newblock Image-to-image translation via hierarchical style disentanglement.
	\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition}, pages 8639--8648, 2021.
	
	\bibitem{lin2014microsoft}
	Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
	Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
	\newblock Microsoft coco: Common objects in context.
	\newblock In {\em European conference on computer vision}, pages 740--755.
	Springer, 2014.
	
	\bibitem{liu2021swin}
	Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
	Baining Guo.
	\newblock Swin transformer: Hierarchical vision transformer using shifted
	windows.
	\newblock In {\em Proceedings of the IEEE/CVF International Conference on
		Computer Vision}, pages 10012--10022, 2021.
	
	\bibitem{loshchilov2016sgdr}
	Ilya Loshchilov and Frank Hutter.
	\newblock Sgdr: Stochastic gradient descent with warm restarts.
	\newblock {\em arXiv preprint arXiv:1608.03983}, 2016.
	
	\bibitem{ramesh2021zero}
	Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
	Radford, Mark Chen, and Ilya Sutskever.
	\newblock Zero-shot text-to-image generation.
	\newblock In {\em International Conference on Machine Learning}, pages
	8821--8831. PMLR, 2021.
	
	\bibitem{shwartz2017opening}
	Ravid Shwartz-Ziv and Naftali Tishby.
	\newblock Opening the black box of deep neural networks via information.
	\newblock {\em arXiv preprint arXiv:1703.00810}, 2017.
	
	\bibitem{tao2022siamese}
	Chenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai.
	\newblock Siamese image modeling for self-supervised vision representation
	learning.
	\newblock {\em arXiv preprint arXiv:2206.01204}, 2022.
	
	\bibitem{tian2020makes}
	Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and
	Phillip Isola.
	\newblock What makes for good views for contrastive learning?
	\newblock {\em Advances in Neural Information Processing Systems},
	33:6827--6839, 2020.
	
	\bibitem{tishby99information}
	Naftali Tishby, Fernando~C. Pereira, and William Bialek.
	\newblock The information bottleneck method.
	\newblock In {\em Proc. of the 37-th Annual Allerton Conference on
		Communication, Control and Computing}, pages 368--377, 1999.
	
	\bibitem{tishby2015deep}
	Naftali Tishby and Noga Zaslavsky.
	\newblock Deep learning and the information bottleneck principle.
	\newblock In {\em 2015 IEEE Information Theory Workshop (ITW)}, pages 1--5.
	IEEE, 2015.
	
	\bibitem{vaswani2017attention}
	Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
	Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
	\newblock Attention is all you need.
	\newblock {\em Advances in neural information processing systems}, 30, 2017.
	
	\bibitem{voloshynovskiy2020variational}
	Slava Voloshynovskiy, Olga Taran, Mouad Kondah, Taras Holotyak, and Danilo
	Rezende.
	\newblock Variational information bottleneck for semi-supervised
	classification.
	\newblock {\em Entropy}, 22(9):943, 2020.
	
	\bibitem{wan2021multi}
	Zhibin Wan, Changqing Zhang, Pengfei Zhu, and Qinghua Hu.
	\newblock Multi-view information-bottleneck representation learning.
	\newblock In {\em Proceedings of the AAAI Conference on Artificial
		Intelligence}, volume~35, pages 10085--10092, 2021.
	
	\bibitem{wang2022rethinking}
	Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu.
	\newblock Rethinking minimal sufficient representation in contrastive learning.
	\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition}, pages 16041--16050, 2022.
	
	\bibitem{wang2019deep}
	Qi Wang, Claire Boudreau, Qixing Luo, Pang-Ning Tan, and Jiayu Zhou.
	\newblock Deep multi-view information bottleneck.
	\newblock In {\em Proceedings of the 2019 SIAM International Conference on Data
		Mining}, pages 37--45. SIAM, 2019.
	
	\bibitem{wang2019learning}
	Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek~F Wong, and
	Lidia~S Chao.
	\newblock Learning deep transformer models for machine translation.
	\newblock {\em arXiv preprint arXiv:1906.01787}, 2019.
	
	\bibitem{wang2021exploring}
	Wenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Ender Konukoglu, and Luc
	Van~Gool.
	\newblock Exploring cross-image pixel contrast for semantic segmentation.
	\newblock In {\em Proceedings of the IEEE/CVF International Conference on
		Computer Vision}, pages 7303--7313, 2021.
	
	\bibitem{wei2022masked}
	Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph
	Feichtenhofer.
	\newblock Masked feature prediction for self-supervised visual pre-training.
	\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition}, pages 14668--14678, 2022.
	
	\bibitem{wu2020graph}
	Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec.
	\newblock Graph information bottleneck.
	\newblock {\em arXiv preprint arXiv:2010.12811}, 2020.
	
	\bibitem{xiao2018unified}
	Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
	\newblock Unified perceptual parsing for scene understanding.
	\newblock In {\em Proceedings of the European conference on computer vision
		(ECCV)}, pages 418--434, 2018.
	
	\bibitem{xie2021detco}
	Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Peize Sun, Zhenguo
	Li, and Ping Luo.
	\newblock Detco: Unsupervised contrastive learning for object detection.
	\newblock In {\em Proceedings of the IEEE/CVF International Conference on
		Computer Vision}, pages 8392--8401, 2021.
	
	\bibitem{xie2022simmim}
	Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi
	Dai, and Han Hu.
	\newblock Simmim: A simple framework for masked image modeling.
	\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition}, pages 9653--9663, 2022.
	
	\bibitem{yang2021instance}
	Ceyuan Yang, Zhirong Wu, Bolei Zhou, and Stephen Lin.
	\newblock Instance localization for self-supervised detection pretraining.
	\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition}, pages 3987--3996, 2021.
	
	\bibitem{yang2022unified}
	Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and
	Jianfeng Gao.
	\newblock Unified contrastive learning in image-text-label space.
	\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition}, pages 19163--19173, 2022.
	
	\bibitem{zbontar2021barlow}
	Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
	\newblock Barlow twins: Self-supervised learning via redundancy reduction.
	\newblock In {\em International Conference on Machine Learning}, pages
	12310--12320. PMLR, 2021.
	
	\bibitem{zhao2021contrastive}
	Xiangyun Zhao, Raviteja Vemulapalli, Philip~Andrew Mansfield, Boqing Gong,
	Bradley Green, Lior Shapira, and Ying Wu.
	\newblock Contrastive learning for label efficient semantic segmentation.
	\newblock In {\em Proceedings of the IEEE/CVF International Conference on
		Computer Vision}, pages 10623--10633, 2021.
	
	\bibitem{zhou2019semantic}
	Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
	and Antonio Torralba.
	\newblock Semantic understanding of scenes through the ade20k dataset.
	\newblock {\em International Journal of Computer Vision}, 127(3):302--321,
	2019.
	
	\bibitem{zhou2021ibot}
	Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao
	Kong.
	\newblock ibot: Image bert pre-training with online tokenizer.
	\newblock {\em arXiv preprint arXiv:2111.07832}, 2021.
	
\end{thebibliography}

\end{document}
