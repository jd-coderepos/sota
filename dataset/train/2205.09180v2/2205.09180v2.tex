\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}


\title{\vspace{-0.7cm}LeRaC: Learning Rate Curriculum}

\author{Florinel-Alin Croitoru, Nicolae-C\u{a}t\u{a}lin Ristea, Radu Tudor Ionescu\thanks{Corresponding author: raducu.ionescu@gmail.com} , Nicu Sebe\\
University of Bucharest, Romania, University Politehnica of Bucharest, Romania\\ 
University of Trento, Italy\vspace*{-0.3cm}
}
\def\cvprPaperID{8267} \def\confName{CVPR}
\def\confYear{2023}

\begin{document}

\maketitle

\begin{abstract}
\vspace{-0.3cm}
Most curriculum learning methods require an approach to sort the data samples by difficulty, which is often cumbersome to perform. In this work, we propose a novel curriculum learning approach termed \textbf{Le}arning \textbf{Ra}te \textbf{C}urriculum (LeRaC), which leverages the use of a different learning rate for each layer of a neural network to create a data-free curriculum during the initial training epochs. More specifically, LeRaC assigns higher learning rates to neural layers closer to the input, gradually decreasing the learning rates as the layers are placed farther away from the input. The learning rates increase at various paces during the first training iterations, until they all reach the same value. From this point on, the neural model is trained as usual. This creates a model-level curriculum learning strategy that does not require sorting the examples by difficulty and is compatible with any neural network, generating higher performance levels regardless of the architecture. We conduct comprehensive experiments on nine data sets from the computer vision (CIFAR-10, CIFAR-100, Tiny ImageNet, ImageNet-200), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D) domains, considering various convolutional (ResNet-18, Wide-ResNet-50, DenseNet-121), recurrent (LSTM) and transformer (CvT, BERT, SepTr) architectures, comparing our approach with the conventional training regime. Moreover, we also compare with Curriculum by Smoothing (CBS), a state-of-the-art data-free curriculum learning approach. Unlike CBS, our performance improvements over the standard training regime are consistent across all data sets and models. Furthermore, we significantly surpass CBS in terms of training time (there is no additional cost over the standard training regime for LeRaC). Our code is freely available at: \url{http//github.com/link.hidden.for.review}.
\vspace{-0.6cm}
\end{abstract}

\setlength{\abovedisplayskip}{2.5pt}
\setlength{\belowdisplayskip}{2.5pt}

\section{Introduction}


\begin{figure}[!t]
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{fig_LeRaC.pdf}}
\vspace{-0.25cm}
\caption{Training based on Learning Rate Curriculum.}
\label{fig_lerac}
\vspace{-1.1cm}
\end{center}
\end{figure}



Curriculum learning \cite{Bengio-ICML-2009} refers to efficiently training effective neural networks by mimicking how humans learn, from easy to hard. As originally introduced by Bengio \etal~\cite{Bengio-ICML-2009}, curriculum learning is a training procedure that first organizes the examples in their increasing order of difficulty, then starts the training of the neural network on the easiest examples, gradually adding increasingly more difficult examples along the way, until all training examples are fed to the network. The success of the approach relies in avoiding to force the learning of very difficult examples right from the beginning, instead guiding the model on the right path through the imposed curriculum. This type of curriculum is later referred to as data-level curriculum learning \cite{Soviany-IJCV-2022}. Indeed, Soviany \etal~\cite{Soviany-IJCV-2022} identified several types of curriculum learning approaches in the literature, dividing them into four categories based on the components involved in the definition of machine learning given by Mitchell \cite{Mitchell-MH-1997}. The four categories are: data-level curriculum (examples are presented from easy to hard), model-level curriculum (the modeling capacity of the network is gradually increased), task-level curriculum (the complexity of the learning task is increased during training), objective-level curriculum (the model optimizes towards an increasingly more complex objective). While data-level curriculum is the most natural and direct way to employ curriculum learning, its main disadvantage is that it requires a way to determine the difficulty of data samples. Despite having many successful applications \cite{Soviany-IJCV-2022,Wang-PAMI-2021}, there is no universal way to determine the difficulty of the data samples, making the data-level curriculum less applicable to scenarios where the difficulty is hard to estimate, \eg~classification of radar signals. The task-level and objective-level curriculum learning strategies suffer from similar issues, \eg~it is hard to create a curriculum when the model has to learn an easy task (binary classification) or the objective function is already convex.

Considering the above observations, we recognize the potential of model-level curriculum learning strategies of being applicable across a wider range of domains and tasks. To date, there are only a few works \cite{Burduja-ICIP-2021,Karras-ICLR-2018,Sinha-NIPS-2020} in the category of pure model-level curriculum learning methods. However, these methods have some drawbacks caused by their domain-dependent or architecture-specific design. To benefit from the full potential of the model-level curriculum learning category, we propose LeRaC (\textbf{Le}arning \textbf{Ra}te \textbf{C}urriculum), a novel and simple curriculum learning approach which leverages the use of a different learning rate for each layer of a neural network to create a data-free curriculum during the initial training epochs. More specifically, LeRaC assigns higher learning rates to neural layers closer to the input, gradually decreasing the learning rates as the layers are placed farther away from the input. This prevents the propagation of noise caused by the random initialization of the network's weights. The learning rates increase at various paces during the first training iterations, until they all reach the same value, as illustrated in Figure~\ref{fig_lerac}. From this point on, the neural model is trained as usual. This creates a model-level curriculum learning strategy that is applicable to any domain and compatible with any neural network, generating higher performance levels regardless of the architecture, without adding any extra training time. To the best of our knowledge, we are the first to employ a different learning rate per layer to achieve the same effect as conventional (data-level) curriculum learning.

We conduct comprehensive experiments on nine data sets from the computer vision (CIFAR-10 \cite{Krizhevsky-TECHREP-2009}, CIFAR-100 \cite{Krizhevsky-TECHREP-2009}, Tiny ImageNet \cite{Russakovsky-IJCV-2015}, ImageNet-200 \cite{Russakovsky-IJCV-2015}), language (BoolQ \cite{Clark-NAACL-2019}, QNLI \cite{Wang-ICLR-2019}, RTE \cite{Wang-ICLR-2019}) and audio (ESC-50 \cite{Piczak-ACMMM-2015}, CREMA-D \cite{Cao-TAC-2014}) domains, considering various convolutional (ResNet-18 \cite{He-CVPR-2016}, Wide-ResNet-50 \cite{Zagoruyko-ArXiv-2016wide}, DenseNet-121 \cite{Gao-CVPR-2017}), recurrent (LSTM \cite{Hochreiter-NC-1997}) and transformer (CvT \cite{Wu-ICCV-2021}, BERT \cite{Devlin-NAACL-2019}, SepTr \cite{Ristea-ARXIV-2022}) architectures, comparing our approach with the conventional training regime and Curriculum by Smoothing (CBS) \cite{Sinha-NIPS-2020}, our closest competitor. Unlike CBS, our performance improvements over the standard training regime are consistent across all data sets and models. Furthermore, we significantly surpass CBS in terms of training time, since there is no additional cost over the conventional training regime for LeRaC, whereas CBS adds Gaussian kernel smoothing layers.

In summary, our contributions are twofold:
\begin{itemize}
    \item \vspace{-0.25cm} We propose a novel and simple model-level curriculum learning strategy that creates a curriculum by updating the weights of each neural layer with a different learning rate, considering higher learning rates for the low-level feature layers and lower learning rates for the high-level feature layers.
    \item \vspace{-0.25cm} We empirically demonstrate the applicability to multiple domains (image, audio and text), the compatibility to several neural network architectures (convolutional neural networks, recurrent neural networks and transformers), and the time efficiency (no extra training time added) of LeRaC through a comprehensive set of experiments.
\end{itemize}

\vspace{-0.2cm}
\section{Related Work}
\vspace{-0.1cm}

\noindent {\bf Curriculum learning.}
Curriculum learning was initially introduced by Bengio \etal~\cite{Bengio-ICML-2009} as a training strategy that helps machine learning models to generalize better when the training examples are presented in the ascending order of their difficulty. Extensive surveys on curriculum learning methods, including the most recent advancements on the topic, were conducted by Soviany \etal~\cite{Soviany-IJCV-2022} and Wang \etal~\cite{Wang-PAMI-2021}. In the former survey, Soviany \etal~\cite{Soviany-IJCV-2022} emphasized that curriculum learning is not only applied at the data level, but also with respect to the other components involved in a machine learning approach, namely at the model level, the task level and the objective (loss) level. Regardless of the component on which curriculum learning is applied, the technique has demonstrated its effectiveness on a broad range of machine learning tasks, from computer vision \cite{Bengio-ICML-2009, Gui-FG-2017, Jiang-ICML-2018, Shi-ECCV-2016, Soviany-CVIU-2021, Chen-ICCV-2015, Sinha-NIPS-2020} to natural language processing \cite{Platanios-NAACL-2019, Kocmi-RANLP-2017, Spitkovsky-NIPS-2009, Liu-IJCAI-2018,Bengio-ICML-2009} and audio processing \cite{Ranjan-ACM-2018, Amodei-ICML-2016}.

The main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input \cite{Pentina-CVPR-2015, Sanchez-MICCAI-2019,Wei-WACV-2021} or metrics based on domain-specific heuristics. For instance, the text length \cite{Kocmi-RANLP-2017, Cirik-Arxiv-2016,Tay-ACL-2019,Zhang-ISPASS-2021} and the word frequency \cite{Bengio-ICML-2009, Liu-IJCAI-2018} have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works \cite{Soviany-CVIU-2021, Shi-ECCV-2016}. Other solutions employed difficulty estimators \cite{Ionescu-CVPR-2016} or even the confidence level of the predictions made by the neural network \cite{Gong-TIP-2016, Hacohen-Arxiv-2019} to approximate the complexity of the data samples.

The solutions listed above have shown their utility in specific application domains. Nonetheless, measuring the difficulty remains problematic when implementing standard (data-level) curriculum learning strategies, at least in some application domains. Therefore, several alternatives have emerged over time, handling the drawback and improving the conventional curriculum learning approach. In \cite{Kumar-ANIPS-2010}, the authors introduced self-paced learning to evaluate the learning progress when selecting the easy samples. The method was successfully employed in multiple settings \cite{Kumar-ANIPS-2010, Gong-TEVC-2019, Fan-AAAI-2017, Li-AAAI-2016, Jhou-PR-2018, Jiang-AAAI-2015, Ristea-INTERSPEECH-2021}. Furthermore, some studies combined self-paced learning with the traditional pre-computed difficulty metrics \cite{Jiang-AAAI-2015, Ma-ICML-2017}. An additional advancement related to self-paced learning is the approach called self-paced learning with diversity \cite{Jiang-NIPS-2014}. The authors demonstrated that enforcing a certain level of variety among the selected examples can improve the final performance. Another set of methods that bypass the need for predefined difficulty metrics is known as teacher-student curriculum learning \cite{Zhang-AS-2019, Wu-NIPS-2018}. In this setting, a teacher network learns a curriculum to supervise a student neural network.

Closer to our work, a few methods \cite{Karras-ICLR-2018,Sinha-NIPS-2020,Burduja-ICIP-2021} proposed to apply curriculum learning at the model level, by gradually increasing the learning capacity (complexity) of the neural architecture. Such curriculum learning strategies do not need to know the difficulty of the data samples, thus having a great potential to be useful in a broad range of tasks. For example, Karras \etal~\cite{Karras-ICLR-2018} proposed to gradually add layers to generative adversarial networks during training, while increasing the resolution of the input images at the same time. They are thus able to generate realistic high-resolution images. However, their approach is not applicable to every domain, since there is no notion of resolution for some input data types, \eg~text. 
Sinha \etal~\cite{Sinha-NIPS-2020} presented a strategy that blurs the activation maps of the convolutional layers using Gaussian kernel layers, reducing the noisy information caused by the network initialization. The blur level is progressively reduced to zero by decreasing the standard deviation of the Gaussian kernels. With this mechanism, they obtain a training procedure that allows the neural network to see simple information at the start of the process and more intricate details towards the end. Curriculum by Smoothing (CBS)~\cite{Sinha-NIPS-2020} was only shown to be useful for convolutional architectures applied in the image domain. Although we found that CBS is applicable to transformers by blurring the tokens, it is not necessarily applicable to any neural architecture, \eg~standard feed-forward neural networks. As an alternative to CBS, Burduja \etal~\cite{Burduja-ICIP-2021} proposed to apply the same smoothing process on the input image instead of the activation maps. The method was applied with success in medical image alignment. However, this approach is not applicable to natural language input, as it it not clear how to apply the blurring operation on the input text. 

Different from Burduja \etal~\cite{Burduja-ICIP-2021} and Karras \etal~\cite{Karras-ICLR-2018}, our approach is applicable to various domains, including but not limited to natural language processing, as demonstrated throughout our experiments. To the best of our knowledge, the only competing model-level curriculum method which is applicable to various domains is CBS \cite{Sinha-NIPS-2020}. Unlike CBS, LeRaC does not introduce new operations, such as smoothing with Gaussian kernels, during training. As such, our approach does not increase the training time with respect to the conventional training regime, as later shown in the experiments. In summary, we consider that the simplicity of our approach comes with many important advantages: applicability to any domain and task, compatibility with any neural network architecture, time efficiency (adds no extra training time). We support all these claims through the comprehensive experiments presented in Section~\ref{sec_experiments}.

\noindent {\bf Relation to learning rate schedulers.}
The are other contributions \cite{Singh-ICMLA-2015,You-arXiv-2017} showing that using adaptive learning rates can lead to improved results. We explain how our method is different below. In \cite{Singh-ICMLA-2015}, the main goal is increasing the learning rate of certain layers as necessary, to escape saddle points. Different from \cite{Singh-ICMLA-2015}, our strategy reduces the learning rates of deeper layers, introducing soft optimization restrictions in the initial training epochs. You \etal~\cite{You-arXiv-2017} proposed to train models with very large batches using a learning rate for each layer, by scaling the learning rate with respect to the norms of the gradients. The goal of You \etal~\cite{You-arXiv-2017} is to specifically learn models with large batch sizes, \eg formed of 8K samples. Unlike \cite{You-arXiv-2017}, we propose a more generic approach that can be applied to multiple architectures (convolutional, recurrent, transformer) under unrestricted training settings. 

Gotmare \etal~\cite{Gotmare-ICLR-2019} point out that learning rate with warmup restarts is an effective strategy to improve stability of training neural models using large batches. Different from LeRaC, this approach does not employ a different learning rate for each layer. Moreover, the strategy restarts the learning rate at different moments during the entire training process, while LeRaC is applied only during the first few training epochs. Aside from these technical differences, our experiments already include a direct comparison of the two strategies for the CvT architecture. The results show that introducing LeRaC brings consistent improvements. We thus conclude that our strategy is a viable and distinct alternative to learning rate warmup with restarts. 

\noindent {\bf Relation to optimizers.}
We consider Adam \cite{Kingma-ICLR-1015} and related optimizers as orthogonal approaches that perform the optimization rather than setting the learning rate. Our approach, LeRaC, only aims to guide the optimization during the initial training iterations by reducing the relevance of deeper network layers. Most of the baseline architectures used in our experiments are already based on Adam or some of its variations, \eg AdaMax, AdamW \cite{Loshchilov-ICLR-2019}. LeRaC is applied in conjunction with these optimizers, showing improved performance over various architectures and application domains. This supports our claim that LeRaC is an orthogonal contribution to the family of Adam optimizers. 

\vspace{-0.15cm}
\section{Method}
\vspace{-0.1cm}

Deep neural networks are commonly trained on a set of labeled data samples denoted as:

where  is the number of examples,  is a data sample and  is the associated label. The training process of a neural network  with parameters  consists of minimizing some objective (loss) function  that quantifies the differences between the ground-truth labels and the predictions of the model :


The optimization is generally performed by some variant of Stochastic Gradient Descent (SGD), where the gradients are back-propagated from the neural layers closer to the output towards the neural layers closer to input through the chain rule. Let , , ....,  and , , ...,  denote the neural layers and the corresponding weights of the model , such that the weights  belong to the layer , . The output of the neural network for some training data sample  is formally computed as follows:


To optimize the model via SGD, the weights are updated as follows:

where  is the index of the current training iteration,  is the learning rate at iteration , and the gradient of  with respect to  is computed via the chain rule. Before starting the training process, the weights  are commonly initialized with random values. 

Due to the random initialization of the weights, the information propagated through the neural model during the early training iterations can contain a large amount of noise, which can negatively impact the learning process, as discussed by Sinha \etal~\cite{Sinha-NIPS-2020}. Due to the feed-forward processing, we conjecture that the noise level tends to grow with each neural layer, from  to  (the empirical proof is provided in the supplementary). The same issue can occur if the weights are pre-trained on a distinct task, where the misalignment of the weights with a new task is likely higher for the high-level (specialized) feature layers. To alleviate this problem, we propose to introduce a curriculum learning strategy that assigns a different learning rate  to each layer , as follows:

such that:


where  are the initial learning rates and  are the updated learning rates at iteration . The condition formulated in Eq.~\eqref{eq_init} indicates that the initial learning rate  of a neural layer  gets lower as the level of the respective neural layer becomes higher (farther away from the input). With each training iteration , the learning rates are gradually increased, until they become equal, according to Eq.~\eqref{eq_iter_k}. Thus, our curriculum learning strategy is only applied during the early training iterations, where the noise caused by the random weight initialization is most prevalent. Hence,  is a hyperparameter of LeRaC that is usually adjusted such that , where  is the total number of training iterations. In practice, we obtain optimal results by running LeRaC up to any epoch between  and .

We increase each learning rate  from iteration  to iteration  using an exponential scheduler that is based on the following rule:


We set  in Eq.~\eqref{eq_update_exp} across all our experiments.
In practice, we obtain optimal results by initializing the lowest learning rate  with a value that is around five or six orders of magnitude lower than , while the highest learning rate  is usually equal to . Apart from these general practical notes, the exact LeRaC configuration for each neural architecture is established by tuning the hyperparameters on the available validation sets.

We underline that the output feature maps of a layer  are affected  by the initial random weights (noise)  of the respective layer, and  by the input feature maps, which are in turn affected by the random weights of the previous layers . Hence, the noise affecting the feature maps increases with each layer processing the feature maps, being multiplied with the weights from each layer along the way. Our curriculum learning strategy imposes the training of the earlier layers at a faster pace, transforming the noisy weights into discriminative patterns. As noise from the earlier layer weights is eliminated, we train the later layers at faster and faster paces, until all learning rates become equal at epoch .

From a technical point of view, we note that our approach can also be regarded as a way to guide the optimization, which we see as an alternative to loss function smoothing. The link between curriculum learning and loss smoothing is mentioned in \cite{Soviany-IJCV-2022}, where the authors suggest that curriculum learning strategies induce a smoothing of the loss function, where the smoothing is higher during the early training iterations (simplifying the optimization) and lower to non-existent during the late training iterations (restoring the complexity of the loss function). LeRaC is aimed at producing a similar effect, but in a softer manner by dampening the importance of optimizing the weights of high-level layers in the early training iterations. Additionally, we empirically observe (see results in the supplementary) that LeRaC tends to balance the training pace of low-level and high-level features, while the conventional regime seems to update the high-level layers at a faster pace. This could provide an additional intuitive explanation of why our method works.



\vspace{-0.1cm}
\section{Experiments}
\label{sec_experiments}
\vspace{-0.1cm}
\subsection{Data Sets}
\vspace{-0.1cm}

In general, we adopt the official data splits for the nine benchmarks considered in our experiments. When a validation set is not available, we keep  of the training data for validation.

\noindent{\bf CIFAR-10.}
CIFAR-10 \cite{Krizhevsky-TECHREP-2009} is a popular data set for object recognition in images. It consists of 60,000 color images with a resolution of  pixels. An images depicts one of 10 object classes, each class having 6,000 examples. We use the official data split with a training set of 50,000 images and a test set of 10,000 images. 

\noindent{\bf CIFAR-100.}
The CIFAR-100 \cite{Krizhevsky-TECHREP-2009} data set is similar to CIFAR-10, except that it has 100 classes with 600 images per class. There are 50,000 training images and 10,000 test images.

\noindent{\bf Tiny ImageNet.}
Tiny ImageNet is a subset of ImageNet-1K \cite{Russakovsky-IJCV-2015} which provides 100,000 training images, 25,000 validation images and 25,000 test images representing objects from 200 different classes. The size of each image is  pixels.

\noindent{\bf ImageNet-200.}
ImageNet-200 is a part of ImageNet-1K \cite{Russakovsky-IJCV-2015} with images from a subset of 200 classes, where the original resolution of the images is preserved.

\noindent{\bf BoolQ.}
BoolQ \cite{Clark-NAACL-2019} is a question answering data set for yes/no questions containing 15,942 examples. The questions are naturally occurring, being generated in unprompted and unconstrained settings. Each example is a triplet of the form: \{question, passage, answer\}. We use the data split provided in the SuperGLUE benchmark \cite{Wang-NIPS-2019}, containing 9,427 examples for training, 3,270 for validation and 3,245 for testing.

\noindent{\bf QNLI.}
The QNLI (Question-answering NLI) data set \cite{Wang-ICLR-2019} is a natural language inference benchmark automatically derived from SQuAD \cite{Rajpurkar-EMNLP-2016}. The data set contains \{question, sentence\} pairs and the task is to determine whether the context sentence contains the answer to the question. The data set is constructed on top of Wikipedia documents, each document being accompanied, on average, by 4 questions. We consider the data split provided in the GLUE benchmark \cite{Wang-ICLR-2019}, which comprises 104,743 examples for training, 5,463 for validation and 5,463 for testing.

\noindent{\bf RTE.} 
Recognizing Textual Entailment (RTE) \cite{Wang-ICLR-2019} is a natural language inference data set containing pairs of sentences with the target label indicating if the meaning of one sentence can be inferred from the other. The training subset includes 2,490 samples, the validation set 277, and the test set 3,000 examples.

\noindent{\bf CREMA-D.}
The CREMA-D multi-modal database \cite{Cao-TAC-2014} is formed of 7,442 videos of 91 actors (48 male and 43 female) of different ethnic groups. The actors perform various emotions while uttering 12 particular sentences that evoke one of the 6 emotion categories: anger, disgust, fear, happy, neutral, and sad. Following \cite{Ristea-INTERSPEECH-2021}, we conduct experiments only on the audio modality, dividing the set of audio samples into  for training,  for validation and  for testing.

\noindent{\bf ESC-50.}
The ESC-50 \cite{Piczak-ACMMM-2015} data set is a collection of 2,000 samples of 5 seconds each, comprising 50 classes of various common sound events. Samples are recorded at a 44.1 kHz sampling frequency, with a single channel. In our evaluation, we employ the 5-fold cross-validation procedure, as described in related works \cite{Piczak-ACMMM-2015,Ristea-ARXIV-2022}.

\vspace{-0.1cm}
\subsection{Experimental Setup}
\vspace{-0.1cm}

\noindent{\bf Architectures.} To demonstrate the compatibility of LeRaC with multiple neural architectures, we select several convolutional, recurrent and transformer models. As representative convolutional neural networks (CNNs), we opt for ResNet-18 \cite{He-CVPR-2016}, Wide-ResNet-50 \cite{Zagoruyko-ArXiv-2016wide} and DenseNet-121 \cite{Gao-CVPR-2017}. As representative transformers, we consider CvT-13 \cite{Wu-ICCV-2021}, BERT \cite{Devlin-NAACL-2019} and SepTr \cite{Ristea-ARXIV-2022}. For CvT, we consider both pre-trained and randomly initialized versions. We use an uncased large pre-trained version of BERT. As Ristea \etal~\cite{Ristea-ARXIV-2022}, we train SepTr from scratch.
In addition, we employ a long short-term memory (LSTM) network \cite{Hochreiter-NC-1997} to represent recurrent neural networks (RNNs). The recurrent neural network contains two LSTM layers, each having a hidden dimension of 256 components. These layers are preceded by one embedding layer with the embedding size set to 128 elements. The output of the last recurrent layer is passed to a classifier comprised of two fully connected layers. The LSTM is activated by rectified linear units (ReLU). 
We apply the aforementioned models on distinct input data types, considering the intended application domain of each model\footnote{The only exception is DenseNet-121, which is applied on audio instead of image data.}. Hence, ResNet-18, Wide-ResNet-50 and CvT are applied on images, BERT and LSTM are applied on text, and SepTr and DenseNet-121 are applied on audio.

\noindent{\bf Baselines.} 
We compare LeRaC with two baselines: the conventional training regime (which uses early stopping and reduces the learning rate on plateau) and the state-of-the-art Curriculum by Smoothing~\cite{Sinha-NIPS-2020}. For CBS, we use the official code released by Sinha \etal~\cite{Sinha-NIPS-2020} at \url{https://github.com/pairlab/CBS}, to ensure the replicability of their method in our experimental settings, which include a more diverse selection of input data types and neural architectures.

\begin{table*}
\small{
  \begin{center}
\begin{tabular}{llccccccccc}
    \toprule
    \multirow{2}{*}{Architecture}    & \multirow{2}{*}{Optimizer}     & \multirow{2}{*}{Mini-batch}          &  \multirow{2}{*}{\#Epochs}      &  \multirow{2}{*}{}  & \multicolumn{3}{c}{CBS} & & \multicolumn{2}{c}{LeRaC}          \\
    \cline{6-8}
    \cline{10-11}
    & & & & &  &  &  &&  &  - \\
    \midrule
    ResNet-18       & SGD          &  64 &  100-200 &  & 1 & 0.9 & 2-5 && 5-7 &  - \\
    
    Wide-ResNet-50  & SGD          & 64 &   100-200 &  & 1 & 0.9 & 2-5  && 5-7 &  -  \\
    CvT-13          & AdaMax       & 64-128 & 150-200 &  & 1 & 0.9 & 2-5 && 2-5 &  -  \\
    CvT-13 & AdaMax    & 64-128 & 25 &  & 1 & 0.9 & 2-5 && 3-6 &  - \\
    \midrule
    BERT           & AdaMax        & 10     & 7-25 &  & 1 & 0.9 & 1 && 3 &  - \\
    LSTM           & AdamW         & 256-512 & 25-70 &  & 1 & 0.9 & 2 && 3-4 &  -  \\
    \midrule    
    SepTR           & Adam         &  2  &  50  &   & 0.8 & 0.9 & 1-3 && 2-5 &  - \\

    DenseNet-121   & Adam         &  64 &  50  &   &  0.8 & 0.9 & 1-3  && 2-5 &  - \\

    \bottomrule
  \end{tabular}
  \end{center}
  }
  \vspace{-0.6cm}
    \caption{Optimal hyperparameter settings for the various neural architectures used in our experiments.\vspace{-0.3cm}}
    \label{tab_parameters}
\end{table*}

\noindent{\bf Hyperparameter tuning.} 
We tune all hyperparameters on the validation set of each benchmark. In Table \ref{tab_parameters}, we present the optimal hyperparameters chosen for each architecture. In addition to the standard parameters of the training process, we report the parameters that are specific for the CBS and LeRaC strategies. In the case of CBS,  denotes the standard deviation of the Gaussian kernel,  is the decay rate for , and  is the decay step. Regarding the parameters of LeRaC,  represents the number of iterations used in Eq.~\eqref{eq_update_exp}, and  and  are the initial learning rates for the first and last layers of the architecture, respectively. We underline that  and , in all experiments. Moreover, , \ie~the initial learning rates of LeRaC converge to the original learning rate set for the conventional training regime. All models are trained with early stopping and the learning rate is reduced by a factor of  when the loss reaches a plateau.

\noindent{\bf Evaluation.} We evaluate all models in terms of the classification accuracy. We repeat the training process of each model for 5 times and report the average accuracy and the standard deviation.

\noindent{\bf Image preprocessing.}
For the image classification experiments, we apply the same data preprocessing approach as Sinha \etal~\cite{Sinha-NIPS-2020}. Hence, we normalize the images and maintain their original resolution,  pixels for CIFAR-10 and CIFAR-100, and  pixels for Tiny ImageNet. Similar to Sinha \etal~\cite{Sinha-NIPS-2020}, we do not employ data augmentation.

\noindent{\bf Text preprocessing.}
For the text classification experiments with BERT, we lowercase all words and add the classification token ([CLS]) at the start of the input sequence. We add the separator token ([SEP]) to delimit sentences. For the LSTM network, we lowercase all words and replace them with indexes from vocabularies constructed from the training set. The input sequence length is limited to  tokens for BERT and  tokens for LSTM.

\noindent{\bf Speech preprocessing.}
We transform each audio sample into a time-frequency matrix by computing the discrete Short Time Fourier Transform (STFT) with  FFT points, using a Hamming window of length  and a hop size .
For CREMA-D, we first standardize all audio clips to a fixed dimension of  seconds by padding or clipping the samples. Then, we apply the STFT with ,  and a window size of . For ESC-50, we keep the same values for  and ,
but we increase the hop size to . Next, for each STFT, we compute the square root of the magnitude and map the values to  Mel bins. The result is converted to a logarithmic scale and normalized to the interval . Furthermore, in all our speech classification experiments, we use the following data augmentation methods: noise perturbation, time shifting, speed perturbation, mix-up and SpecAugment \cite{Park-INTERSPEECH-2019}. The speech preprocessing steps are carried out following Ristea \etal~\cite{Ristea-ARXIV-2022}.

\begin{table*}[!th]
\small{
  \begin{center}
  \begin{tabular}{llcccc}
    \toprule
    Architecture       & Training Regime     & CIFAR-10  & CIFAR-100 & Tiny ImageNet & ImageNet-200\\
    \midrule      
             & conventional               &     &     &  &  \\
    ResNet-18          & CBS \cite{Sinha-NIPS-2020}             &     &     &  &  \\
             & LeRaC (ours)    &     &     &  & \\
    \midrule   
         & conventional               &     &     &  & \\
    Wide-ResNet-50     & CBS \cite{Sinha-NIPS-2020}             &     &     &  & \\
         & LeRaC (ours)    &     &     &  & \\
    \midrule    
                 & conventional               &     &     &  & \\
    CvT-13             & CBS \cite{Sinha-NIPS-2020}             &     &     &  & \\
                 & LeRaC (ours)    &     &     &  & \\
    \midrule
      & conventional               &     &     &  & - \\
    CvT-13  & CBS \cite{Sinha-NIPS-2020}             &     &     &  & - \\
     & LeRaC (ours)    &     &     &  & - \\
    \bottomrule
  \end{tabular}
  \end{center}
}
    \vspace{-0.6cm}
  \caption{Average accuracy rates (in \%) over 5 runs on CIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet-200 for various neural models based on different training regimes: conventional, CBS \cite{Sinha-NIPS-2020} and LeRaC. The accuracy of the best training regime in each experiment is highlighted in bold.\vspace{-0.4cm}}\label{tab_vision}
\end{table*}

\begin{table*}[!th]
\small{
  \begin{center}
  \begin{tabular}{llccc}
    \toprule
    Architecture    & Training Regime     & BoolQ          & RTE                 &  QNLI             \\
    \midrule    
      & conventional              &     &    &  \\
    BERT  & CBS \cite{Sinha-NIPS-2020}           &     &    &  \\
      & LeRaC (ours)   &     &    &  \\
    \midrule  
                & conventional              &    &    & \\
    LSTM            & CBS \cite{Sinha-NIPS-2020}           &    &    & \\
                & LeRaC (ours)   &    &    & \\
    \bottomrule
  \end{tabular}
  \end{center}
}
    \vspace{-0.6cm}
    \caption{Average accuracy rates (in \%) over 5 runs on BoolQ, RTE and QNLI for BERT and LSTM based on different training regimes: conventional, CBS \cite{Sinha-NIPS-2020} and LeRaC. The accuracy of the best training regime in each experiment is highlighted in bold.\vspace{-0.4cm}}
  \label{tab_text}
\end{table*}

\vspace{-0.1cm}
\subsection{Results}
\vspace{-0.1cm}

\noindent{\bf Image classification.} In Table~\ref{tab_vision}, we present the image classification results on CIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet-200. Since CvT-13 is pre-trained on ImageNet, it does not make sense to fine-tune it on ImageNet-200. Thus, the respective results are not reported. On the one hand, there are three scenarios (ResNet-18 on CIFAR-100, Wide-ResNet-50 on ImageNet-200, and CvT-13 on CIFAR-100) in which CBS provides the largest improvements over the conventional regime, surpassing LeRaC in the respective cases. On the other hand, there are eight scenarios where CBS degrades the accuracy with respect to the standard training regime. This shows that the improvements attained by CBS are inconsistent across models and data sets. Unlike CBS, our strategy surpasses the baseline regime in all fifteen cases, thus being more consistent. In six of these cases, the accuracy gains of LeRaC are higher than . Moreover, LeRaC outperforms CBS in twelve out of fifteen cases. We thus consider that LeRaC can be regarded as a better choice than CBS, bringing consistent performance gains.

\noindent{\bf Text classification.}
In Table~\ref{tab_text}, we report the text classification results on BoolQ, RTE and QNLI. Here, there are only two cases (BERT on QNLI and LSTM on RTE) where CBS leads to performance drops compared to the conventional training regime. In all other cases, the improvements of CBS are below . Just as in the image classification experiments, LeRaC brings accuracy gains for each and every model and data set. In four out of six scenarios, the accuracy gains yielded by LeRaC are higher than . Once again, LeRaC proves to be the best and most consistent regime, generally outperforming CBS by significant margins.

\begin{table}
  \small{
  \setlength\tabcolsep{3.3pt}
  \begin{center}
  \begin{tabular}{llcc}
    \toprule
    Architecture  & Training Regime     & CREMA-D     & ESC-50 \\
    \midrule
         & conventional                   &       &    \\
    SepTr     & CBS \cite{Sinha-NIPS-2020}                &       &    \\
         & LeRaC (ours)        &       &    \\
    \midrule
         & conventional             &      &  \\
    DenseNet-121     & CBS \cite{Sinha-NIPS-2020}          &      & \\
         & LeRaC (ours)  &      & \\
    \bottomrule
  \end{tabular}
    \end{center}
    }
      \vspace{-0.6cm}
    \caption{Average accuracy rates (in \%) over 5 runs on CREMA-D and ESC-50 for SepTr and DenseNet-121 based on different training regimes: conventional, CBS \cite{Sinha-NIPS-2020} and LeRaC. The accuracy of the best training regime in each experiment is highlighted in bold.\vspace{-0.4cm}}
  \label{tab_audio}
\end{table}

\begin{table*}
\small{
  \begin{center}
  \begin{tabular}{llccc}
    \toprule
    Architecture       & Training Regime     & CIFAR-10  & CIFAR-100 & Tiny ImageNet \\
    \midrule      
                 & conventional               &     &     &  \\
                 & CBS \cite{Sinha-NIPS-2020}            &     &     &  \\
    CvT-13             & LeRac (linear update) &  &  & \\
                 & LeRaC (exponential update)   &     &     &  \\
                 & CBS \cite{Sinha-NIPS-2020} + LeRaC    &     &     &  \\
    \bottomrule
  \end{tabular}
  \end{center}
  }
  \vspace{-0.6cm}
  \caption{Average accuracy rates (in \%) over 5 runs on CIFAR-10, CIFAR-100 and Tiny ImageNet for CvT-13 based on different training regimes: conventional, CBS \cite{Sinha-NIPS-2020}, LeRaC with linear update, LeRaC with exponential update (proposed), and a combination of CBS and LeRaC.\vspace{-0.45cm}}
  \label{tab_extra}
\end{table*}

\noindent{\bf Speech classification.}
In Table~\ref{tab_audio}, we present the results obtained on the audio data sets, namely CREMA-D and ESC-50. We observe that the CBS strategy obtains lower results compared with the baseline in two cases (SepTr on CREMA-D and DenseNet-121 on ESC-50), while our method provides superior results for each and every case. By applying LeRaC on SepTr, we set a new state-of-the-art accuracy level () on the CREMA-D audio modality, surpassing the previous state-of-the-art value attained by Ristea \etal~\cite{Ristea-ARXIV-2022} with SepTr alone. When applied on DenseNet-121, LeRaC brings performance improvements higher than , the highest improvement () over the baseline being attained on CREMA-D.

\noindent{\bf Additional results.} 
An interesting aspect worth studying is to determine if putting the CBS and LeRaC regimes together could bring further performance gains. We study the effect of combining CBS and LeRaC for CvT-13, since there are three data sets for which both CBS and LeRaC improve CvT-13 (see Table~\ref{tab_vision}). The corresponding results are shown in Table~\ref{tab_extra}. The reported results show that the combination brings accuracy gains across all three data sets (CIFAR-10, CIFAR-100, Tiny ImageNet). We thus conclude that the combination of curriculum learning regimes is worth a try, whenever the two independent regimes boost performance.

Another important aspect is to establish if the exponential learning rate update proposed in Eq.~\eqref{eq_update_exp} is a good choice. To test this out, we keep the CvT-13 model and change the LeRaC regime to use a linear update of the learning rate. We observe performance gains with both types of update rules, but our exponential learning rate update seems to bring higher gains on all three data sets. We thus conclude that the update rule defined in Eq.~\eqref{eq_update_exp} is a sound option.

\begin{figure*}
\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=.8\linewidth]{r18.pdf}  
  \caption{ResNet-18 on Tiny ImageNet.}
  \label{fig:sub-third}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=.8\linewidth]{wr.pdf}  
  \caption{Wide-ResNet-50 on Tiny ImageNet.}
  \label{fig:sub-fourth}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=.8\linewidth]{bert.pdf}  
  \caption{BERT on BoolQ.}
  \label{fig:sub-second}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{septr.pdf}  
  \caption{SepTr on CREMA-D.}
  \label{fig:sub-first}
\end{subfigure}
\vspace{-0.3cm}
\caption{Validation accuracy (on the y-axis) versus training time (on the x-axis) for four distinct architectures. The number of training epochs is the same for both LeRaC and CBS, the observable time difference being caused by the overhead of CBS due to the Gaussian kernel layers.\vspace{-0.5cm}}
\label{fig_train_time}
\end{figure*}

\noindent{\bf Training time comparison.}
For a particular model and data set, all training regimes are executed for the same number of epochs, for a fair comparison. However, the CBS strategy adds the smoothing operation at multiple levels inside the architecture, which increases the training time. To this end, we compare the training time (in hours) versus the validation error of CBS and LeRaC. For this experiment, we selected four neural models and illustrate the evolution of the validation accuracy over time in Figure~\ref{fig_train_time}. We underline that LeRaC introduces faster convergence times, being around 7-12\% faster than CBS. It is trivial to note that LeRaC requires the same time as the conventional regime.

\noindent{\bf Ablation results.} We present several ablation results in the supplementary.

\vspace{-0.15cm}
\section{Conclusion}
\vspace{-0.1cm}

In this paper, we introduced a novel model-level curriculum learning approach that is based on starting the training process with increasingly lower learning rates per layer, as the layers get closer to the output. We conducted comprehensive experiments on nine data sets from three domains (image, text and audio), considering multiple neural architectures (CNNs, RNNs and transformers), to compare our novel training regime (LeRaC) with a state-of-the-art regime (CBS~\cite{Sinha-NIPS-2020}) as well as the conventional training regime (based on early stopping and reduce on plateau). The empirical results demonstrate that LeRaC is significantly more consistent than CBS, perhaps being the most versatile curriculum learning strategy to date, due to its compatibility with multiple neural models and its usefulness across different domains. Remarkably, all these benefits come for free, \ie~LeRaC does not add any extra time over the conventional approach.

\noindent{\bf Acknowledgements.}
Work funded by UEFISCDI through project number PN-III-P1-1.1-TE-2019-0235.

\bibliographystyle{IEEEbib}
\bibliography{references}

\clearpage

\section{Supplementary}
\vspace{-0.1cm}
In the supplementary, we present a series of ablation and extra experiments to validate several choices and statements in our paper. Additionally, we clarify some important aspects and discuss the limitations of our work.

\subsection{Additional and Ablation Results}
\vspace{-0.1cm}

\noindent{\bf Significance testing.} 
To determine if the reported accuracy gains observed for LeRaC with respect to the baseline are significant, we apply McNemar significance testing \cite{Dietterich-NC-1998} to the results reported in the main article on all nine data sets. In 19 of 25 cases, we found that our results are significantly better than the corresponding baseline, at a confidence threshold of . This confirms that our gains are statistically significant in the majority of cases.

\noindent{\bf Noise quantification of early and later layers.}
The application of LeRaC is justified by the fact that the level of noise gradually grows with each layer during a forward pass through a neural network with randomly initialized weights. To empirically confirm this conjecture, we have computed the distances for the low-level (first conv) and high-level (last conv) layers between the activation maps at iteration  (based on random weights) and the last iteration (based on weights optimized until convergence) for ResNet-18 on CIFAR-10, while using the conventional training regime. The computed distances shown in Table \ref{tab_noise_quant} confirm our conjecture, namely that shallow layers contain less noise than deep layer when applying the conventional training regime.

\noindent{\bf Entropy of low-level versus high-level features.}
We showed a few examples of training dynamics in Fig. 2 from the main article. All four graphs exhibit a higher gap between standard training and LeRaC in the first half of the training process, suggesting that LeRaC has an important role towards faster convergence. To assess the comparative quality of low-level versus high-level feature maps obtained either with conventional or LeRaC training, we compute the entropy of the first and last conv layers of ResNet-18 on CIFAR-10, after  iterations. We report the computed entropy levels in Table~\ref{tab_entropy}. Conventional training seems to update deeper layers faster, observing a higher difference between the entropy levels of low-level and high-level features obtained with conventional training than with LeRac. This shows that LeRaC balances the training pace of low-level and high-level features. We conjecture that updating the deeper layers too soon could lead to overfitting to the noise still present in the early layers. This statement is supported by our empirical results on nine data sets, showing that giving a chance to the early layers to converge before introducing large updates to the later layers leads to superior performance. 

\begin{table}[t]
  \small{
\begin{center}
  \begin{tabular}{lcc}
    \toprule
    Training Regime     & \multicolumn{2}{c}{Distance} \\
    & First Conv Layer & Last Conv Layer \\
    \midrule
     conventional             &  & \\
    \bottomrule
  \end{tabular}
    \end{center}
    }
      \vspace{-0.6cm}
    \caption{Distances between feature maps at epoch  and feature maps after the final epoch for ResNet-18 on CIFAR-10, while using the conventional training regime. Distances are independently computed for the first and last convolutional layers.}
  \label{tab_noise_quant}
\end{table}

\begin{table}[t]
  \small{
\begin{center}
  \begin{tabular}{lcc}
    \toprule
    Training Regime     & \multicolumn{2}{c}{Entropy} \\
    & First Conv Layer & Last Conv Layer \\
    \midrule
     conventional             &  & \\
      LeRaC (ours)              &  &  \\
    \bottomrule
  \end{tabular}
    \end{center}
    }
      \vspace{-0.6cm}
    \caption{Entropy after  epochs for ResNet-18 on CIFAR-10, while alternating between the conventional and LeRaC training regimes.}
  \label{tab_entropy}
\end{table}

\begin{table}[t]
  \small{
\begin{center}
  \begin{tabular}{lcc}
    \toprule
    Training Regime     & \multicolumn{2}{c}{Distance} \\
    & First Conv Layer & Last Conv Layer \\
    \midrule
     conventional             &  & \\
      LeRaC (ours)              &  &  \\
    \bottomrule
  \end{tabular}
    \end{center}
    }
      \vspace{-0.6cm}
    \caption{Distances between feature maps at epoch  and feature maps after the final epoch for ResNet-18 on CIFAR-10, while alternating between the conventional and LeRaC training regimes. Distances are independently computed for the first and last convolutional layers.}
  \label{tab_distance}
\end{table}

\begin{table*}[t]
  \small{
\begin{center}
  \begin{tabular}{lllcc}
    \toprule
    Data Set & Architecture  & Training Regime & -     & Accuracy \\
    \midrule
    \multirow{16}{*}{\vspace{-0.52cm}CIFAR-100}  & \multirow{8}{*}{ResNet-18}              & conventional    & - &  \\
    \cmidrule{3-5}
                                &      & \multirow{7}{*}{LeRaC (ours)} & -             & \\
                                &               &  & -             & \\
                                &               &  & -             & \\
                                &               &  & -             & \\
                                &               & & -             & \\
                                &               &  & -             & \\
                                &               &  & -             & \\
    \cmidrule{2-5}
                                & \multirow{8}{*}{Wide-ResNet-50}              & conventional & -            &  \\
    \cmidrule{3-5}
                       &      & \multirow{7}{*}{LeRaC (ours)} & -            & \\
                                 &               &  & -             & \\
                                &               &  & -             & \\
                                &               &  & -             & \\
                                &               &  & -             & \\
                                &               &  & -             & \\
                                &               &  & -             & \\
    \midrule
   \multirow{7}{*}{CREMA-D} &   \multirow{7}{*}{SepTr}   & conventional                   & - &        \\
    \cmidrule{3-5}
    &      & \multirow{6}{*}{LeRaC (ours)}                    & - &  \\
     &     &                     & - &    \\
    &     &                     & - &    \\
     &     &                     & - &    \\
      &     &                     & - &    \\
       &     &                     & - &    \\
    \bottomrule
  \end{tabular}
    \end{center}
    }
      \vspace{-0.6cm}
    \caption{Average accuracy rates (in \%) over 5 runs for ResNet-18 and Wide-ResNet-50 on CIFAR-100, as well as SepTr on CREMA-D, based on different ranges for the initial learning rates. The accuracy rates surpassing the baseline training regime are highlighted in bold.}
  \label{tab_various_ranges}
\end{table*}

\begin{figure*}[!t]
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{all_activations.pdf}}
\vspace{-0.25cm}
\caption{Activation maps with low and high entropy from the first and last conv layers of ResNet-18 trained on CIFAR-10 for  epochs with the conventional (baseline) and LeRaC (ours) regimes. The input images are taken from ImageNet. Best viewed in color.}
\label{fig_activations}
\vspace{-0.9cm}
\end{center}
\end{figure*}

Aside from computing the global entropy over all training samples, in Figure \ref{fig_activations}, we illustrate some activation maps with the highest and lowest entropy from the first and last conv layers for three randomly chosen examples from ImageNet. The activation maps are extracted at epoch  from the ResNet-18 model trained on CIFAR-10 either with the conventional regime or the LeRaC regime. In general, we observe that the low-level activation maps corresponding to LeRaC exhibit a higher degree of variability (being more distinct from each other), regardless of the entropy level (low or high). We believe the higher degree of variability comes from the fact that, having lower learning rates for the deeper layers, the model based on LeRaC is likely focused on finding a higher variety of patterns within the first layers to minimize the loss. For the third example (the image of an airplane), we observe that the activation maps with the highest entropy from the last conv layer produced by LeRaC have a higher entropy than the activation maps with the highest entropy produced by the conventional regime. This observation is in line with the results reported in Table~\ref{tab_entropy}, confirming that LeRaC is able to better balance the entropy of low-level and high-level features by preventing the faster convergence of the deeper layers. 

\noindent{\bf Distances at epoch  versus final epoch.}
In Table~\ref{tab_entropy}, we report the entropy of the low-level and high-level layers after  epochs, before and after using LeRaC to train ResNet-18 on CIFAR-10. We consider that using the distance to the final feature maps provides additional useful insights about how LeRaC works. To this end, we compute the Euclidean distances of both low-level and high-level features between epoch  and the final epoch, before and after using LeRaC. We report the distances in Table~\ref{tab_distance}. The computed distances confirm our previous observations, namely that LeRaC is capable of balancing the training pace of low-level and high-level layers.

\noindent{\bf Varying value ranges for initial learning rates.}
In general, we note that our hyperparameters are tuned on the validation data. In our first ablation study, we present results with LeRaC using multiple ranges for  and  to demonstrate that LeRaC is sufficiently stable with respect to suboptimal hyperparameter choices. We carry out experiments with ResNet-18 and Wide-ResNet-50 on CIFAR-100, as well as SepTr on CREMA-D. We report the corresponding results in Table~\ref{tab_various_ranges}. We observe that there are multiple hyperparameter configurations that lead to surpassing the baseline regime. This indicates that LeRaC can bring performance gains even outside the tuned initial learning rate bounds, demonstrating low sensitivity to suboptimal hyperparameter tuning.

\begin{table}[t]
  \small{
\begin{center}
  \begin{tabular}{llcc}
    \toprule
    Architecture  & Training Regime   &   & Accuracy \\
    \midrule
     \multirow{6}{*}{\vspace{-0.15cm}ResNet-18}              & conventional           & - &  \\
     \cmidrule{2-4}
                                &                          & 5  & \\
                                &                           & 6  & \\
                                &                LeRaC (ours)           & 7  & \\
                                &                          & 8  & \\
                                &                         & 9  & \\
    \midrule
    \multirow{6}{*}{\vspace{-0.15cm}Wide-ResNet-50}              & conventional         & - &  \\
    \cmidrule{2-4}
                                 &                           & 5  & \\
                                &                           & 6  & \\   
                                               & LeRaC (ours)           & 7  & \\
                                &                          & 8  & \\
                                &                          & 9  & \\
    \bottomrule
  \end{tabular}
    \end{center}
    }
      \vspace{-0.6cm}
    \caption{Average accuracy rates (in \%) over 5 runs for ResNet-18 and Wide-ResNet-50 on CIFAR-100 using the LeRaC regime until iteration , while varying . The results of the conventional regime are added for reference. The accuracy rates surpassing the baseline training regime are highlighted in bold.}
  \label{tab_var_k}
\end{table}

\noindent{\bf Varying .}
In Table \ref{tab_var_k}, we present additional results with ResNet-18 and Wide-ResNet-50 on CIFAR-100, considering various values for  (the last iteration for our training regime). We observe that all configurations surpass the baselines on CIFAR-100. Moreover, we observe that the optimal values for  ( for ResNet-18 and  for Wide-ResNet-50) obtained on the validation set are not the values producing the best results on the test set. 

\begin{table}[t]
  \small{
  \setlength\tabcolsep{3.3pt}
  \begin{center}
  \begin{tabular}{lllc}
    \toprule
    Data Set & Architecture  & Training Regime     & Accuracy \\
    \midrule
    \multirow{6}{*}{\vspace{-0.15cm}CIFAR-100}  &               & conventional             &  \\
                                & ResNet-18     & anti-LeRaC             & \\
                                &               & LeRaC (ours)              & \\
    \cmidrule{2-4}
    &               & conventional             &  \\
                       & Wide-ResNet-50     & anti-LeRaC             & \\
                                &               & LeRaC (ours)              & \\
    \midrule
    &      & conventional                   &        \\
    CREMA-D & SepTr     & anti-LeRaC                &       \\
    &     & LeRaC (ours)                    &    \\
    \bottomrule
  \end{tabular}
    \end{center}
    }
      \vspace{-0.6cm}
    \caption{Average accuracy rates (in \%) over 5 runs for ResNet-18 and Wide-ResNet-50 on CIFAR-100, as well as SepTr on CREMA-D, based on different training regimes: conventional, anti-LeRaC and LeRaC. The accuracy of the best training regime in each experiment is highlighted in bold.}
  \label{tab_anti}
\end{table}

\noindent{\bf Anti-curriculum.}
Since our goal is to perform curriculum learning (from easy to hard), we restrict the settings for , , such that deeper layers start with lower learning rates. However, another strategy is to consider the opposite setting, where we use higher learning rates for deeper layers. We tested this approach, which belongs to the category of anti-curriculum learning strategies, in a set of new experiments with ResNet-18 and Wide-ResNet-50 on CIFAR-100, as well as SepTr on CREMA-D. We report the corresponding results with LeRaC and anti-LeRaC in Table~\ref{tab_anti}. Although anti-curriculum, \eg hard negative sample mining, was shown to be useful in other tasks \cite{Soviany-IJCV-2022}, our results indicate that learning rate anti-curriculum attains inferior performance compared to our approach. 

\begin{table}[t]
  \small{
  \setlength\tabcolsep{3.3pt}
  \begin{center}
  \begin{tabular}{lllc}
    \toprule
    Architecture  & Optimizer& Training Regime     & Accuracy \\
    \midrule
     \multirow{3}{*}{ResNet-18} & Adam & conventional             &  \\
                                & SGD & conventional             &  \\
                                & SGD & LeRaC (ours)              & \\
    \midrule
    \multirow{3}{*}{Wide-ResNet-50} & Adam  & conventional             &  \\
                                    & SGD   & conventional             &  \\
                                    & SGD   & LeRaC (ours)              & \\
    \bottomrule
  \end{tabular}
    \end{center}
    }
      \vspace{-0.6cm}
    \caption{Average accuracy rates (in \%) over 5 runs for ResNet-18 and Wide-ResNet-50 on CIFAR-100 using different optimizers and training regimes (conventional versus LeRaC). The accuracy of the best training regime in each experiment is highlighted in bold.}
  \label{tab_optimizer}
\end{table}

\noindent{\bf SGD+LeRaC versus Adam.}
In Table \ref{tab_optimizer}, we present results showing that SGD and SGD+LeRaC obtain better accuracy rates than Adam \cite{Kingma-ICLR-1015} for the ResNet18 and Wide-ResNet-50 models on CIFAR-100. This indicates that a simple optimizer combined with LeRaC can obtain better results than a state-of-the-art optimizer such as Adam. This justifies our decision to use a different optimizer for each neural model (see Table 1 in the main article).

\begin{table}[t]
  \small{
\begin{center}
  \begin{tabular}{llc}
    \toprule
    Architecture  & Training Regime     & Accuracy \\
    \midrule
     \multirow{2}{*}{ResNet-18}              & conventional             &  \\
                                &                LeRaC (ours)              & \\
    \midrule
    \multirow{2}{*}{Wide-ResNet-50}              & conventional             &  \\
                                               & LeRaC (ours)              & \\
    \bottomrule
  \end{tabular}
    \end{center}
    }
      \vspace{-0.6cm}
    \caption{Average accuracy rates (in \%) over 5 runs for ResNet-18 and Wide-ResNet-50 on CIFAR-100 using data augmentation and different training regimes (conventional versus LeRaC). The accuracy of the best training regime in each experiment is highlighted in bold.}
  \label{tab_vision_aug}
\end{table}

\noindent{\bf Data augmentation on vision data sets.}
Following Sinha \etal~\cite{Sinha-NIPS-2020}, we did not use data augmentation for the vision data sets. We consider training data augmentation as an orthogonal method for improving results, expecting improvements for both baseline and LeRaC models. Nevertheless, since we extended the experimental settings considered in \cite{Sinha-NIPS-2020} to other domains, we took the liberty to use data augmentation in the audio domain (see the results in Table 4 from the main paper). The same augmentations (noise perturbation, time shifting, speed perturbation, mix-up and SpecAugment) are used for all audio models, ensuring a fair comparison. Moreover, we next present additional results with ResNet-18 and Wide-ResNet-50 on CIFAR-100 using the following augmentations: horizontal flip, rotation, solarization, blur, sharpening and auto-contrast. The results reported in Table \ref{tab_vision_aug} confirm that the performance gaps in the vision domain are in the same range after introducing data augmentation. In addition, we note that data augmentation seems to be rather harmful for the Wide-ResNet-50 model, which attains better results without data augmentation.

\subsection{Discussion}

\noindent{\bf Another relation to curriculum learning.}
Our method can be seen as a curriculum learning strategy that simplifies the optimization in the early training stages by restricting the model updates (in a soft manner) to certain directions (corresponding to the weights of the earlier layers). Due to the imposed soft restrictions (lower learning rates for deeper layers), the optimization is easier at the beginning. As the training progresses, all directions become equally important, and the loss function is permitted to be optimized in any direction. As the number of directions grows, the optimization task becomes more complex (it is harder to find the optimum). Another relationship to curriculum learning can be discovered by noting that the complexity of the optimization increases over time, just as in curriculum learning.

\noindent{\bf Interaction with other curriculum learning strategies.}
Our simple and generic curriculum learning scheme can be integrated into any model for any task, not relying on domain or task dependent information, \eg the data samples. We already showed that combining LeRaC and CBS can boost performance (see the results presented in Table 5 from the main paper). In a similar fashion, LeRaC can be combined with data-level curriculum strategies for improved performance. We leave this exploration for future work.

\noindent{\bf Interaction with optimization algorithms.}
Throughout our experiments, we always keep using the same optimizer for a certain neural model, for all training regimes (conventional, CBS, LeRaC). The best optimizer for each neural model is established for the conventional training regime. We underline that our initial learning rates and scheduler are used independently of the optimizers. Although our learning rate scheduler updates the learning rates at the beginning of every iteration, we did not observe any stability or interaction issues with any of the optimizers (SGD, Adam, AdaMax, AdamW).

\noindent{\bf Interaction with other learning rate schedulers.}
Whenever a learning rate scheduler is used for training a model in our experiments, we simply replace the scheduler with LeRaC until epoch . For example, all the baseline CvT results are based on Linear Warmup with Cosine Annealing, this being the recommended scheduler for CvT \cite{Wu-ICCV-2021}. When we introduce LeRaC, we simply deactivate Linear Warmup with Cosine Annealing between epochs  and . In general, we recommend deactivating other schedulers while using LeRaC for simplicity in avoiding stability issues.

\noindent{\bf Setting the initial learning rates.}
We should emphasize that the different learning rates , , are not optimized nor tuned during training. Instead, we set the initial learning rates  through validation, such that  is around five or six orders of magnitude lower than  and . After initialization, we apply our exponential scheduler, until all learning rates become equal at iteration . 
In addition, we would like to underline that the difference  between the initial learning rates of consecutive layers is automatically set based on the range given by  and . For example, let us consider a network with 5 layers. If we choose  and , than the intermediate initial learning rates are automatically set to , , , \ie  is used in the exponent and is equal to  in this case. To obtain the intermediate learning rates according to this example, we actually apply an exponential scheduler (similar to the one defined in Eq.~(8)). This reduces the number of tunable hyperparameters from  (the number layers) to , namely  and . However, tuning all , , might lead to even better results. We leave this exploration for future work.

\noindent{\bf Setting  without tuning.}
Learning rates are usually expressed as a power of , \eg . If we start with a learning rate of  for some layer  and we want to increase it to  during the first  epochs, the intermediate learning rates are ,  and . We thus believe it is more intuitive to understand what happens when setting  in Eq.~(8), as opposed to using some tuned value for . To this end, we refrained from tuning  and fix it to .

\noindent{\bf Number of hyperparameters.}
LeRaC adds three additional hyperparameters compared to the conventional training regime. These are the initial highest and lowest learning rates,  and , and the number of iterations  to employ LeRaC. We reduce the number of hyperparameters that require tuning by using a fixed rule to adjust the intermediate learning rates, \eg by employing an exponential scheduler, or by fixing less important hyperparameters, \eg . We emphasize that CBS \cite{Sinha-NIPS-2020} has an identical number of additional hyperparameters to LeRaC. Furthermore, we note that data-level curriculum methods also introduce additional hyperparameters. Even a simple method that splits the examples into easy-to-hard batches that are gradually added to the training set requires at least two hyperparameters: the number of batches, and the number of iterations before introducing a new training batch. We thus believe that, in terms of the number of additional hyperparameters, LeRaC is comparable to CBS and other curriculum learning strategies. We emphasize that the same happens if we look at new optimizers, \eg Adam \cite{Kingma-ICLR-1015} adds three additional hyperparameters compared to SGD.

\noindent{\bf Limitations of our work.}
One limitation is the need to disable other learning rate schedulers while using LeRaC. We already tested this scenario with CvT (the baseline CvT uses Linear Warmup with Cosine Annealing, which is removed when using LeRaC), observing consistent performance gains (see Table 2 from the main paper). However, disabling alternative learning rate schedulers might bring performance drops in other cases. Hence, this has to be decided on a case by case basis. Another limitation is the possibility of encountering longer training times or poor convergence when the hyperparameters are not properly configured. We recommend hyperparameter tuning on the validation set to avoid this outcome. An additional limitation is that we tested our approach on mainstream classification tasks involving mainstream classification losses (multi-class or binary cross-entropy). We leave the integration with additional losses for future work.


\end{document}