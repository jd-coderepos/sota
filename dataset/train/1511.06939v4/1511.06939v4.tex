\section{Experiments}\label{sec:experiments}
We evaluate the proposed recursive neural network against popular baselines on two datasets.

The first dataset is that of RecSys Challenge 2015\footnote{http://2015.recsyschallenge.com/}. This dataset contains click-streams of an e-commerce site that sometimes end in purchase events. We work with the training set of the challenge and keep only the click events. We filter out sessions of length 1. The network is trained on  months of data, containing 7,966,257 sessions of 31,637,239 clicks on 37,483 items. We use the sessions of the subsequent day for testing. Each session is assigned to either the training or the test set, we do not split the data mid-session. Because of the nature of collaborative filtering methods, we filter out clicks from the test set where the item clicked is not in the train set. Sessions of length one are also removed from the test set. After the preprocessing we are left with 15,324 sessions of 71,222 events for the test set. This dataset will be referred to as RSC15.

The second dataset is collected from a Youtube-like OTT video service platform. Events of watching a video for at least a certain amount of time were collected. Only certain regions were subject to this collection that lasted for somewhat shorter than 2 months. During this time item-to-item recommendations were provided after each video at the left side of the screen. These were provided by a selection of different algorithms and influenced the behavior of the users. Preprocessing steps are similar to that of the other dataset with the addition of filtering out very long sessions as they were probably generated by bots. The training data consists of all but the last day of the aforementioned period and has  million sessions of  million watch events on  thousand videos. The test set contains the sessions of the last day of the collection period and has  thousand sessions with  thousand watch events. This dataset will be referred to as VIDEO.

The evaluation is done by providing the events of a session one-by-one and checking the rank of the item of the next event. The hidden state of the GRU is reset to zero after a session finishes. Items are ordered in descending order by their score and their position in this list is their rank. With RSC15, all of the 37,483 items of the train set were ranked. However, this would have been impractical with VIDEO, due to the large number of items. There we ranked the desired item against the most popular 30,000 items. This has negligible effect on the evaluations as rarely visited items often get low scores. Also, popularity based pre-filtering is common in practical recommender systems.

As recommender systems can only recommend a few items at once, the actual item a user might pick should be amongst the first few items of the list. Therefore, our primary evaluation metric is recall@20 that is the proportion of cases having the desired item amongst the top-20 items in all test cases. Recall does not consider the actual rank of the item as long as it is amongst the top-N. This models certain practical scenarios well where there is no highlighting of recommendations and the absolute order does not matter. Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)\citep{Liu:2012EBR,itals_ecml}. The second metric used in the experiments is MRR@20 (Mean Reciprocal Rank). That is the average of reciprocal ranks of the desired items. The reciprocal rank is set to zero if the rank is above 20. MRR takes into account the rank of the item, which is important in cases where the order of recommendations matter (e.g. the lower ranked items are only visible after scrolling).

\subsection{Baselines}
We compare the proposed network to a set of commonly used baselines.
\begin{itemize}[noitemsep,nolistsep]
\item \textbf{POP}: Popularity predictor that always recommends the most popular items of the training set. Despite its simplicity it is often a strong baseline in certain domains.
\item \textbf{S-POP}: This baseline recommends the most popular items of the current session. The recommendation list changes during the session as items gain more events. Ties are broken up using global popularity values. This baseline is strong in domains with high repetitiveness.
\item \textbf{Item-KNN}: Items similar to the actual item are recommended by this baseline and similarity is defined as the cosine similarity between the vector of their sessions, i.e. it is the number of co-occurrences of two items in sessions divided by the square root of the product of the numbers of sessions in which the individual items are occurred. Regularization is also included to avoid coincidental high similarities of rarely visited items. This baseline is one of the most common item-to-item solutions in practical systems, that provides recommendations in the ``others who viewed this item also viewed these ones'' setting. Despite of its simplicity it is usually a strong baseline \citep{linden2003amazon,JamesRecsys10}.
\item \textbf{BPR-MF}: BPR-MF \citep{bpr} is one of the commonly used matrix factorization methods. It optimizes for a pairwise ranking objective function (see Section~\ref{sec:rnn4rec}) via SGD. Matrix factorization cannot be applied directly to session-based recommendations, because the new sessions do not have feature vectors precomputed. However we can overcome this by using the average of item feature vectors of the items that had occurred in the session so far as the user feature vector. In other words we average the similarities of the feature vectors between a recommendable item and the items of the session so far.
\end{itemize}

\begin{table}
\centering
\caption{Recall@20 and MRR@20 using the baseline methods}\label{tab:baselines}
\medskip
{\small
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Baseline}} & \multicolumn{2}{c}{\textbf{RSC15}} & \multicolumn{2}{c}{\textbf{VIDEO}} \\
& Recall@20 & MRR@20 & Recall@20 & MRR@20 \\
\midrule
POP & 0.0050 & 0.0012 & 0.0499 & 0.0117 \\
S-POP & 0.2672 & 0.1775 & 0.1301 & 0.0863 \\
Item-KNN & 0.5065 & 0.2048 & 0.5508 & 0.3381 \\
BPR-MF & 0.2574 & 0.0618 & 0.0692 & 0.0374 \\
\bottomrule
\end{tabular}}
\end{table}

Table~\ref{tab:baselines} shows the results for the baselines. The item-KNN approach clearly dominates the other methods.

\subsection{Parameter \& structure optimization}
We optimized the hyperparameters by running 100 experiments at randomly selected points of the parameter space for each dataset and loss function. The best parametrization was further tuned by individually optimizing each parameter. The number of hidden units was set to 100 in all cases. The best performing parameters were then used with hidden layers of different sizes. The optimization was done on a separate validation set. Then the networks were retrained on the training plus the validation set and evaluated on the final test set.

The best performing parametrizations are summarized in table~\ref{tab:params}. Weight matrices were initialized by random numbers drawn uniformly from  where  depends on the number of rows and columns of the matrix. We experimented with both rmsprop \citep{rmsprop} and adagrad \citep{adagrad}. We found adagrad to give better results.

\begin{table}
\centering
\caption{Best parametrizations for datasets/loss functions}\label{tab:params}
\medskip
{\small
\begin{tabular}{llcccc}
\toprule
\textbf{Dataset} & \textbf{Loss} & \textbf{Mini-batch} & \textbf{Dropout} & \textbf{Learning rate} & \textbf{Momentum} \\
\midrule
RSC15 & TOP1 & 50 & 0.5 & 0.01 & 0 \\
RSC15 & BPR & 50 & 0.2 & 0.05 & 0.2 \\
RSC15 & Cross-entropy & 500 & 0 & 0.01 & 0 \\
VIDEO & TOP1 & 50 & 0.4 & 0.05 & 0 \\
VIDEO & BPR & 50 & 0.3 & 0.1 & 0 \\
VIDEO & Cross-entropy & 200 & 0.1 & 0.05 & 0.3 \\
\bottomrule
\end{tabular}}
\end{table}

We briefly experimented with other units than GRU. We found both the classic RNN unit and LSTM to perform worse.

We tried out several loss functions. Pointwise ranking based losses, such as cross-entropy and MRR optimization (as in \citet{gaussian_ranking}) were usually unstable, even with regularization. For example cross-entropy yielded only 10 and 6 numerically stable networks of the 100 random runs for RSC15 and VIDEO respectively. We assume that this is due to independently trying to achieve high scores for the desired items and the negative push is small for the negative samples. On the other hand pairwise ranking-based losses performed well. We found the ones introduced in Section~\ref{sec:rnn4rec} (BPR and TOP1) to perform the best.

Several architectures were examined and a single layer of GRU units was found to be the best performer. Adding addition layers always resulted in worst performance w.r.t. both training loss and recall and MRR measured on the test set. We assume that this is due to the generally short lifespan of the sessions not requiring multiple time scales of different resolutions to be properly represented. However the exact reason of this is unknown as of yet and requires further research. Using embedding of the items gave slightly worse results, therefore we kept the 1-of-N encoding. Also, putting all previous events of the session on the input instead of the preceding one did not result in additional accuracy gain; which is not surprising as GRU -- like LSTM -- has both long and short term memory. Adding additional feed-forward layers after the GRU layer did not help either. However increasing the size of the GRU layer improved the performance. We also found that it is beneficial to use tanh as the activation function of the output layer.

\subsection{Results}
Table~\ref{tab:results} shows the results of the best performing networks. Cross-entropy for the VIDEO data with 1000 hidden units was numerically unstable and thus we present no results for that scenario. The results are compared to the best baseline (item-KNN). We show results with 100 and 1000 hidden units. The running time depends on the parameters and the dataset. Generally speaking the difference in runtime between the smaller and the larger variant is not too high on a GeForce GTX Titan X GPU and the training of the network can be done in a few hours\footnote{Using Theano with fixes for the subtensor operators on GPU.}. On CPU, the smaller network can be trained in a practically acceptable timeframe. Frequent retraining is often desirable for recommender systems, because new users and items are introduced frequently.

The GRU-based approach has substantial gain over the item-KNN in both evaluation metrics on both datasets, even if the number of units is 100\footnote{Except for using the BPR loss on the VIDEO data and evaluating for MRR.}. Increasing the number of units further improves the results for pairwise losses, but the accuracy decreases for cross-entropy. Even though cross-entropy gives better results with 100 hidden units, the pairwise loss variants surpass these results as the number of units increase. Although, increasing the number of units increases the training times, we found that it was not too expensive to move from 100 units to 1000 on GPU. Also, the cross-entropy based loss was found to be numerically unstable as the result of the network individually trying to increase the score for the target items, while the negative push is relatively small for the other items. Therefore we suggest using any of the two pairwise losses. The TOP1 loss performs slightly better on these two datasets, resulting in  accuracy gain over the best performing baseline.

\begin{table}
\centering
\caption{Recall@20 and MRR@20 for different types of a single layer of GRU, compared to the best baseline (item-KNN). Best results per dataset are highlighted.}\label{tab:results}
\medskip
{\small
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Loss / \#Units}} & \multicolumn{2}{c}{\textbf{RSC15}} & \multicolumn{2}{c}{\textbf{VIDEO}} \\
& Recall@20 & MRR@20 & Recall@20 & MRR@20 \\
\midrule
TOP1 \ 100 & 0.5853 (+15.55\%) & 0.2305 (+12.58\%) & 0.6141 (+11.50\%) & 0.3511 (+3.84\%)\\
BPR \ 100 & 0.6069 (+19.82\%) & 0.2407 (+17.54\%) & 0.5999 (+8.92\%) & 0.3260 (-3.56\%)\\
Cross-entropy \ 100 & 0.6074 (+19.91\%) & 0.2430 (+18.65\%) & 0.6372 (+15.69\%) & 0.3720 (+10.04\%)\\
TOP1 \ 1000 & 0.6206 (+22.53\%) & \textbf{0.2693 (+31.49\%)} & \textbf{0.6624 (+20.27\%)} & \textbf{0.3891 (+15.08\%)}\\
BPR \ 1000 & \textbf{0.6322 (+24.82\%)} & 0.2467 (+20.47\%) & 0.6311 (+14.58\%) & 0.3136 (-7.23\%)\\
Cross-entropy \ 1000 & 0.5777 (+14.06\%) & 0.2153 (+5.16\%) & -- & -- \\
\bottomrule
\end{tabular}}
\end{table}

