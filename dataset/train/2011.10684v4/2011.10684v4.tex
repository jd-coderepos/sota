\def\year{2021}\relax
\documentclass[letterpaper]{article} 

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{aaai21}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{graphicx}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicensure}{ \textbf{Output:}} 
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[switch]{lineno} 
\hyphenpenalty=2000
\tolerance=1000

 \pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen)
} 



\setcounter{secnumdepth}{0} 

\setlength\titlebox{2.5in} \begin{document}
\title{SHOT-VAE: Semi-supervised Deep Generative Models \\ With Label-aware ELBO Approximations}
\author {
Hao-Zhe Feng,\textsuperscript{\rm 1}
        Kezhi Kong,\textsuperscript{\rm 2} 
        Minghao Chen\textsuperscript{\rm 1}\\
        Tianye Zhang,\textsuperscript{\rm 1}
        Minfeng Zhu,\textsuperscript{\rm 1}
        Wei Chen\textsuperscript{\rm 1} \\
}
\affiliations {
\textsuperscript{\rm 1} Zhejiang University \\
    \textsuperscript{\rm 2} University of Maryland, College Park \\
    \{fenghz, zhangtianye1026, chenvis\}@zju.edu.cn, kong@cs.umd.edu,\\
    minghaochen01@gmail.com,  minfeng.zhu@outlook.com
}

\maketitle
\begin{abstract}
Semi-supervised variational autoencoders (VAEs) have obtained strong results, but have also encountered the challenge that \textbf{good} \textbf{ELBO values} \textbf{do not} \textbf{always imply} \textbf{accurate} \textbf{inference} \textbf{results}. In this paper, we investigate and propose two causes of this problem: (1) The ELBO objective cannot utilize the label information directly. (2) A bottleneck value exists, and continuing to optimize ELBO after this value will not improve inference accuracy. On the basis of the experiment results, we propose SHOT-VAE to address these problems without introducing additional prior knowledge. The SHOT-VAE offers two contributions: (1) A new ELBO approximation named \textit{smooth-ELBO} that integrates the label predictive loss into ELBO. (2) An approximation based on \textit{optimal interpolation} that breaks the ELBO value bottleneck by reducing the margin between ELBO and the data likelihood. The SHOT-VAE achieves good performance with 25.30\% error rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11\% on CIFAR-10 with 4k labels. \end{abstract}
\section{Introduction}
Most deep learning models are trained with large labeled datasets via supervised learning. However, in many scenarios, although acquiring a large amount of original data is easy, obtaining corresponding labels is often costly or even infeasible. Thus, semi-supervised variational autoencoder (VAE) \citep{DBLP:conf/nips/KingmaMRW14} is proposed to address this problem by training classifiers with multiple unlabeled data and a small fraction of labeled data. 

Based on the latent variable assumption \citep{DBLP:journals/corr/Doersch16}, semi-supervised VAE models combine the evidence lower bound (ELBO) and the classification loss as objective, so that it can not only learn the required classification representations from labeled data, but also capture the disentangled factors which could be used for data generation. Although semi-supervised VAE models have obtained strong empirical results on many benchmark datasets (e.g. MNIST, SVHN, Yale B) \citep{DBLP:conf/nips/NarayanaswamyPM17}, it still encounters one common problem that \textbf{good ELBO values do not always imply accurate inference results} \citep{DBLP:journals/corr/ZhaoSE17b}. To address this problem, existing works introduce prior knowledge that needs to be set manually, e.g., the stacked VAE structure (M1+M2, \citealt{DBLP:conf/nips/KingmaMRW14,DBLP:conf/uai/DavidsonFCKT18}), the prior domain knowledge (\citealt{DBLP:journals/corr/LouizosSLWZ15, DBLP:conf/iclr/IlseTLW19}) and mutual information bounds \citep{DBLP:conf/nips/Dupont18}. 

In this study, we investigate the training process of semi-supervised VAE with extensive experiments and propose two possible causes of the problem. (1) First, the ELBO cannot utilize label information directly. In the semi-supervised VAE framework \citep{DBLP:conf/nips/KingmaMRW14}, the classification loss and ELBO learn from the labels and unlabeled data separately, making it difficult to improve the inference accuracy with ELBO. (2) Second, an \textit{``ELBO bottleneck"} exists, and continuing to optimize the ELBO after a certain bottleneck value will not improve inference accuracy. Thus, we propose \textbf{S}moot\textbf{H}-ELBO \textbf{O}ptimal 
In\textbf{T}erpolation VAE (SHOT-VAE) to solve the \textit{``good ELBO, bad performance"} problem without requiring additional prior knowledge, which offers the following contributions:
\begin{itemize}
    \item \textbf{The smooth-ELBO objective that integrates the classification loss into ELBO}. 
    
        We derive a new ELBO approximation named \textit{smooth-ELBO} with the label-smoothing technique \citep{DBLP:conf/nips/MullerKH19}. Theoretically, we prove that the \textit{smooth-ELBO} integrates the classification loss into ELBO. Then, we empirically show that a better inference accuracy can be achieved with \textit{smooth-ELBO}.
    
    \item \textbf{The margin approximation that breaks the ELBO bottleneck.}
    
    We propose an approximation of the margin between the real data distribution and the one from ELBO. The approximation is based on the \textit{optimal interpolation} in data space and latent space. In practice, we show this optimal interpolation approximation (\textit{OT-approximation}) can break the \textit{"ELBO bottleneck"} and achieve a better inference accuracy.
    
    \item \textbf{Good semi-supervised performance.}
    
    We evaluate SHOT-VAE on 4 benchmark datasets and the results show that our model achieves good performance with 25.30\% error rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11\% on CIFAR-10 with 4k labels. Moreover, we find it can get strong results even with fewer labels and smaller models, for example obtaining a 14.27\% error rate on CIFAR-10 with 500 labels and 1.5M parameters.
\end{itemize} \section{Background}
\label{sec:background}
\subsection{Semi-supervised VAE}
In supervised learning, we are facing with training data that appears as input-label pairs  sampled from the labeled dataset . While in semi-supervised learning, we can obtain an extra collection of unlabeled data  denoted by . We hope to leverage the data from both  and  to achieve a more accurate model than only using . 

Semi-supervised VAEs \cite{DBLP:conf/nips/KingmaMRW14} solve the problem by constructing a probabilistic model to disentangle the data into continuous variables  and label variables . It consists of a generation process and an inference process parameterized by  and  respectively. The generation process assumes the posterior distribution of  given the latent variables  and  as 

The inference process assumes the posterior distribution of  and  given  as

where  is the multinomial distribution of the label,  is a probability vector, and the functions , ,  and  are represented as deep neural networks. 

To make the model learn disentangled representations, the independent assumptions (\citealt{DBLP:conf/nips/KingmaMRW14, DBLP:conf/nips/Dupont18}) are also widely used as 


For the unlabeled dataset , VAE models want to learn the disentangled representation of  and  by maximizing the evidence lower bound of  as


For the labeled dataset , the labels  are treated as latent variables and the related ELBO becomes


Considering the label prediction  contributes only to the unlabeled data in (\ref{eq:ELBOU}), which is an undesirable property as we wish the semi-supervised model can also learn from the given labels, \citet{DBLP:conf/nips/KingmaMRW14} proposes to add a \textit{cross-entropy (CE)} loss as a solution and the extended target is as follows:

where  is a hyper-parameter controlling the loss weight. 

\subsection{Good ELBO, Bad Inference}
However, a frequent phenomenon is that good ELBO values do not always imply accurate inference results \citep{DBLP:journals/corr/ZhaoSE17b}, which often occurs on realistic datasets with high variance, such as CIFAR-10 and CIFAR-100. In this paper, we investigate the training process of semi-supervised VAE models on the above two datasets and propose two possible causes of the \textit{``good ELBO, bad inference"} problem.

\textbf{The ELBO cannot utilize the label information.} As mentioned in equation (\ref{eq:SSLVAETarget}), the label prediction  only contributes to the unlabeled loss , which indicates that the labeled loss  can not utilize the label information directly. We assume this problem will make the ELBO value irrelevant to the final inference accuracy. To evaluate our assumption, we compare the semi-supervised VAE (M2) models \citep{DBLP:conf/nips/KingmaMRW14} with the same model but removing the  in (\ref{eq:SSLVAETarget}). As shown in Figure~\ref{fig:frozen-ELBO}, the results indicate that  can accelerate the learning process of , but it fails to achieve a better inference accuracy than the one removing . 

\begin{figure}[h]
\centering
\subfigure[CIFAR-10]{
    \includegraphics[width=1.35in]{content/image/Cifar10-with-ELBO-remove.PNG}
}
\subfigure[CIFAR-100]{
\includegraphics[width=1.35in]{content/image/Cifar100-with-ELBO-remove.PNG}
}
\vspace{-0.15cm}
\caption{Test accuracy of semi-supervised VAE (M2) model with and w/o . Results indicates that the  fails to achieve a better inference accuracy. }
\vspace{-0.15cm}
\label{fig:frozen-ELBO}
\end{figure}

\textbf{The} \textbf{\textit{``ELBO bottleneck"} effect.} Another possible cause is the \textit{``ELBO bottleneck"}, that is, continuing to optimize ELBO after a certain bottleneck value will not improve the inference accuracy. Figure~\ref{fig:ELBO-Accuracy-Comparison} shows that the inference accuracy raises rapidly and peaks at the bottleneck value. After that, the optimization of ELBO value does not affect the inference accuracy. 
\begin{figure}[h]
\centering
\subfigure[CIFAR-10]{
    \includegraphics[width=1.35in]{content/image/CIFAR10-ELBO-top1.png}
}
\subfigure[CIFAR-100]{
\includegraphics[width=1.35in]{content/image/CIFAR100-ELBO-top5.png}
}
\vspace{-0.15cm}
\caption{Comparison between the negative ELBO value and accuracy for semi-supervised VAE. Results indicate that a ELBO bottleneck exists, and continuing to optimize ELBO after this bottleneck will not improve the inference accuracy.}
\vspace{-0.15cm}
\label{fig:ELBO-Accuracy-Comparison}
\end{figure}

Existing works introduce prior knowledge and specific structures to address these problems. \citet{DBLP:conf/nips/KingmaMRW14} and \citet{DBLP:conf/uai/DavidsonFCKT18} propose the stacked VAE structure (M1+M2), which forces the model to utilize the representations learned from  to inference the . \citet{DBLP:journals/corr/LouizosSLWZ15} and \citet{DBLP:conf/iclr/IlseTLW19} incorporate domain knowledge into models, making the ELBO representations relevant to the label prediction. \citet{DBLP:journals/corr/ZhaoSE17b} and \citet{DBLP:conf/nips/Dupont18} utilize the ELBO decomposition technique \citep{hoffman2016elbo}, setting the mutual information bounds to perform feature selection. These methods have achieved great success on many benchmark datasets (e.g. MNIST, SVHN, Yale B). However, the related prior knowledge and structures need to be selected manually. Moreover, for some standard datasets with high variance such as CIFAR-10 and CIFAR-100, the semi-supervised performance of VAE is not satisfactory.

Instead of introducing additional prior knowledge, we propose a novel solution based on the ELBO approximations, \textbf{S}moot\textbf{H}-ELBO \textbf{O}ptimal in\textbf{T}erpolation VAE. 

 \section{SHOT-VAE}
In this section, we derive the SHOT-VAE model by introducing its two improvements. First, for the labeled dataset , we derive a new ELBO approximation named \textit{smooth-ELBO} that unifies the ELBO and the label predictive loss. Then, for the unlabeled dataset , we create the differentiable \textit{OT-approximation} to break the ELBO value bottleneck.
\label{sec:SHOT-VAE}
\subsection{Smooth-ELBO: integrating the classification loss into ELBO.}
\label{subsec:smooth-ELBO-experiment}
To overcome the problem that the  cannot utilize the label information directly, we first perform an \textit{``ELBO surgery"}. Following previous works \citep{DBLP:journals/corr/Doersch16}, the  can be derived with \textit{Jensen-Inequality} as:


Utilizing the independent assumptions in equation , we have . In addition, the labels  are treated as latent variables directly in , which equals to obey the empirical degenerate distribution, i.e. . Substituting the above two conditions into (\ref{eq:elbo-surgery-1}), we have

In equation (\ref{eq:elbo-surgery-2}), the last objective  is irrelevant to the label prediction , which causes the \textit{“good ELBO, bad inference”} problem. Inspired by this, we derive a new ELBO approximation named \textit{smooth-ELBO}. 

The \textit{smooth-ELBO} provides two improvements. First, we propose a more flexible assumption of the empirical distribution . Instead of treating  as the degenerate distribution, we use the label smoothing technique \citep{DBLP:conf/nips/MullerKH19} and view the one-hot label  as the parameters of the empirical distribution , that is, 

where  is the number of classes and  controls the smooth level. We use  in all experiments. 

Then, we derive the following convergent approximation with the smoothed :


The proof can be found in Appendix A and we also point out the approximation (\ref{eq:smooth-elbo-1}) does not converge under the empirical degenerate distribution, which explains the importance of label smoothing. Combining equations (\ref{eq:elbo-surgery-2}), (\ref{eq:smooth-label-1}) and (\ref{eq:smooth-elbo-1}), we propose the \textit{smooth-ELBO} objective for :

Theoretically, we demonstrate the following properties.

\textbf{Smooth-ELBO integrates the classification loss into ELBO.} Compared with the original , \textit{smooth-ELBO} derives two extra components,  and . Utilizing the decomposition in \citep{hoffman2016elbo}, we can rewrite the first component into 

where  is the constant of mutual information between  and ,  is the true marginal distribution for  which can be estimated with  in  and  is the estimation of marginal distribution. By optimizing , the first component can learn the marginal distribution  from labels. 

For the second component, with Pinsker's inequality, it's easy to prove that for all  

The proof can be found in Appendix B, which indicates that  converges to  in training process.

\textbf{Convergence analysis.} As mentioned above,  converge to the smoothed  in training. Based on this property, we can assert that the \textit{smooth-ELBO} converges to  with the following equation:

The proof can be found in Appendix C.  are the constants related to class number  and  is the distance between  and .

To summarize the above, \textbf{smooth-ELBO can utilize the label information directly}. Compared with the original  loss in (\ref{eq:SSLVAETarget}), \textit{smooth-ELBO} has three advantages. First, it not only learns from single labels, but also learns from the marginal distribution . Second, we do not need to manually set the loss weight . Moreover, it also takes advantages of the , such as disentangled representations and convergence assurance. The extensive experiments will also show that a better model performance can be achieved with smooth-ELBO.

\subsection{OT-approximation: breaking the ELBO bottleneck}
\label{subsec:optimal-interpolation-approximation}
To overcome the \textit{ELBO bottleneck} problem, we first analyze what the semi-supervised VAE model does after reaching the bottleneck, then we create the differentiable \textit{OT-approximation} to break it, which is based on the \textit{optimal interpolation} in latent space. 

As mentioned in equation (\ref{eq:ELBOU}), VAE aims to learn disentangled representations  and  by maximizing the lower bound of the likelihood of data as , while the margin between  and  has the following closed form:


Ideally, the optimization process of ELBO will make the representation  and  converge to their ground truth  and . However, the unimproved inference accuracy of  indicates that \textbf{optimizing ELBO after the bottleneck will only contribute to the continuous representation , while the  seems to get stuck in the local minimum}. Since the ground truth  is not available for the unlabeled dataset , it is hard for the model to jump out by itself. Inspired by this, we create a differentiable approximation of   for  to break the bottleneck. 

Following previous works \citep{lee2013pseudo}, the approximation is usually constructed with two steps: creating the pseudo input  with data augmentations and creating the pseudo distribution  of . Recent advanced works use autoaugment \citep{Cubuk_2019_CVPR} and random mixmatch \citep{DBLP:journals/corr/abs-1905-02249} to perform data augmentations. However, these strategies will greatly change the representation of continuous variable , e.g., changing the image style and background. To overcome this, we propose the \textit{optimal interpolation} based approximation.  

The optimal interpolation consists of two steps. First, for each input  in , we find the optimal match  with the most similar continuous variable , that is,  . Then, on purpose of jumping out the stuck point , we take the widely-used mixup strategy \citep{DBLP:conf/iclr/ZhangCDL18} to create pseudo input  as follows:

where  is sampled from the uniform distribution . 

The mixup strategy can be understood as calculating the \text{optimal interpolation} between two points  in input space with the maximum likelihood:

where  is the latent variables for the data points , and the proof can be found in Appendix D.
\begin{algorithm}[t] 
\caption{SHOT-VAE training process with epoch .} 
\label{alg:SHOT-VAE} 
\begin{algorithmic}[1] 
\REQUIRE ~~\\ Batch of labeled data ;\\ Batch of unlabeled data ;\\
Mixup ;\\
Loss weight ;\\
Model parameters: ;\\ 
Model optimizer: 
\ENSURE ~~\\ Updated parameters: 
\STATE 
\STATE 
\STATE 
\STATE 
\STATE 
\end{algorithmic}
\end{algorithm}

To create the pseudo distribution  of , it is a natural thought that \textbf{the optimal interpolation in data space could associate with the same in latent space} with  distance used in ELBO. Inspired by this, we propose the optimal interpolation method to calculate  as

\textbf{Proposition 1} \textit{The optimal interpolation derived from  distance between  and   with  can be written as}

\textit{and the solution  satisfying}

The proof can be found in Appendix E. 

Combining the optimal interpolation in data space and latent space, we derive the \textbf{o}ptimal in\textbf{t}erpolation approximation (OT-approximation) for  as


Notice that the \textit{OT-approximation} does not require additional prior knowledge and is easy to implement. Moreover, although \textit{OT-approximation} utilizes the mixup strategy to create pseudo input , our work has two main advantages over mixup-based methods (\citealt{DBLP:conf/iclr/ZhangCDL18,DBLP:conf/icml/VermaLBNMLB19}). First, mixup methods directly assume the pseudo label  behaves linear in latent space without explanations. Instead, we derive the  from ELBO as the metric and utilize the optimal interpolation  to construct . Second, mixup methods use  loss between  and , while we use the  loss and achieve better semi-supervised learning performance.
\begin{figure*}[t]
\centering
\subfigure{
    \includegraphics[width=3.2in]{content/image/CIFAR10-semi-perform.pdf}
}   \subfigure{
\includegraphics[width=3.2 in]{content/image/CIFAR100-semi-perform.pdf}
}
\vspace{-0.2in}
\caption{Error rate comparison of SHOT-VAE to baseline methods on CIFAR-10 (left) and CIFAR-100 (right) for a varying number of labels. ``Supervised" refers to training with all 50000 training samples and no unlabeled data. Results show that (1) SHOT-VAE surpasses other models with a large margin in all cases. (2) both \textit{smooth-ELBO} and \textit{OT-approximation} contribute to the inference accuracy, reducing the error rate on  labels from  to  and from  to .}
\label{fig:vary-ratios-label}
\end{figure*}
\subsection{The implementation details of SHOT-VAE}
The complete algorithm of SHOT-VAE can be obtained by combining the \textit{smooth-ELBO} and the \textit{OT-approximation}, as shown in Algorithm \ref{alg:SHOT-VAE}. In this section, we discuss some details in training process.

First, the working condition for \textit{OT-approximation} is that the ELBO has reached the bottleneck value. However, quantifying the ELBO bottleneck value is difficult. Therefore, we extend the \textit{\textit{warm-up strategy}} in  \citep{DBLP:conf/iclr/HigginsMPBGBML17} to achieve the working condition. The main idea of \textit{\textit{warm-up}} is to make the weight  for OT-approximation increase slowly at the beginning and most rapidly in the middle of the training, i.e. exponential schedule. The function of the exponential schedule is

where  is the hyper-parameter controlling the increasing speed, and we use  in all experiments.

Second, the optimal match operation in equation  requires to find the most similar  for each  in , which consumes a lot of computation resources. To overcome this, we set a large batch-size (e.g., 512) and use the most similar  in one mini-batch to perform optimal interpolation.

Moreover, calculating the gradient of the expected log-likelihood  is difficult. Therefore, we apply the reparameterization tricks  \cite{DBLP:conf/icml/RezendeMW14,DBLP:conf/iclr/JangGP17} to obtain the gradients as follows:

and

Following previous works \citep{DBLP:conf/nips/Dupont18}, we used  and  in all experiments.

To make the VAE model learn the disentangled representations, we also take the widely-used -VAE strategy \citep{DBLP:journals/corr/abs-1804-03599} in training process and chose  in all experiments. 


 \section{Experiments}
\label{sec:experiments}
In this section, we evaluate the SHOT-VAE model with sufficient experiments on four benchmark datasets, i.e. MNIST, SVHN, CIFAR-10, and CIFAR-100. In all experiments, we apply stochastic gradient descent (SGD) as optimizer with momentum  and multiply the learning rate by  at regularly scheduled epochs. For each experiment, we create five - splits with different random seeds and the error rates are reported by the mean and variance across splits. Due to space limitations, we mainly show results on CIFAR-10 and CIFAR-100; more results on MNIST and SVHN as well as the robustness analysis of hyper-parameters are provided in Appendix F. The code, with which the most important results can be reproduced, is available at Github\footnote{\url{https://github.com/PaperCodeSubmission/AAAI2021-260}}. 

\begin{table*}[t]
\vskip -0.2cm
\centering
\begin{tabular}{ccccc}
\toprule
\makecell[c]{Parameter\\Amount}   & Method & 
\makecell[c]{CIFAR10\4k)} & \makecell[c]{CIFAR100\
    \begin{split}
      \KL(\hat{p}(\rvy\vert \rmX) \Vert& q_{\bm{\phi}}(\rvy\vert \rmX))
    +\KL ( q_{\bm{\phi}}(\rvy\vert \rmX) \Vert p(\rvy) )\\
    &\rightarrow \KL(\hat{p}(\rvy\vert \rmX) \Vert p(\rvy)) \\
    \end{split}
    \label{eq:smooth-elbo-1}

\begin{split}
        &\KL(\hat{p}(\rvy\vert \rmX) \Vert p(\rvy)) =\E_{\hat{p}(\rvy\vert \rmX)}\log\frac{\hat{p}(\rvy\vert \rmX)}{p(\rvy)}\\
        &=\E_{\hat{p}(\rvy\vert \rmX)}\log\frac{\hat{p}(\rvy\vert \rmX)}{q_{\bm{\phi}}(\rvy\vert \rmX)}+\E_{\hat{p}(\rvy\vert \rmX)}\log\frac{q_{\bm{\phi}}(\rvy\vert \rmX)}{p(\rvy)}\\
        &=\KL(\hat{p}(\rvy\vert \rmX) \Vert q_{\bm{\phi}}(\rvy\vert \rmX))+\E_{\hat{p}(\rvy\vert \rmX)}\log\frac{q_{\bm{\phi}}(\rvy\vert \rmX)}{p(\rvy)}
\end{split}
\label{eq:decomposition-1}

\lim_{q_{\bm{\phi}}(\rvy\vert \rmX) \rightarrow \hat{p}(\rvy\vert \rmX)  }\E_{q_{\bm{\phi}}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)}=\E_{\hat{p}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)}.

\begin{split}
    \E_{\hat{p}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)} -\E_{q_{\bm{\phi}}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)}&\leq \zeta\\
       \text{when } \sup_{i}  \vert \bm{\pi}_{\bm{\phi}}(\rmX)_{i}-\text{smooth}(\1_{\rvy})_{i}\vert &\leq \delta
\end{split}
    \label{eq:limitation}

    \begin{split}
        \vert \E_{\hat{p}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)} &-\E_{q_{\bm{\phi}}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)}\vert\\
        =\vert \sum_{i=1}^{K}(\text{smooth}(\1_{\rvy})_{i}-&\bm{\pi}_{\bm{\phi}}(\rmX)_{i})(\log p(\rvy)_i-\log \bm{\pi}_{\bm{\phi}}(\rmX)_{i})\vert\\
        \leq K\cdot \delta\cdot M+ &K\cdot \delta\cdot \sup_{i} \vert\log \bm{\pi}_{\bm{\phi}}(\rmX)_{i}\vert
    \end{split}
    \label{eq:margin}
\sup_{i}\vert\log p(\rvy)_{i}\vert\leq M.
    \sup_{i} \vert \frac{\bm{\pi}_{\bm{\phi}}(\rmX)_{i}}{\text{smooth}(\1_{\rvy})_{i}}-1\vert \leq \delta\cdot \frac{K-1}{\epsilon}
    \label{eq:limitation2}

    \sup_{i}\vert\log \bm{\pi}_{\bm{\phi}}(\rmX)_{i}\vert \leq \delta\cdot \frac{K-1}{\epsilon}  +\log \frac{1}{1-\epsilon}+\Delta(\delta)
    \label{eq:limitation3}

    \begin{split}
        &\vert \E_{\hat{p}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)} -\E_{q_{\bm{\phi}}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)}\vert\\
        &\leq KM\delta+\frac{K(K-1)}{\epsilon}\delta^2+K\log \frac{1}{1-\epsilon}\delta+\Delta(\delta)
    \end{split}

\frac{K(K-1)}{\epsilon}\delta^2+K\log \frac{1}{1-\epsilon}\delta\rightarrow \infty

    \begin{split}
        \vert \bm{\pi}_{\bm{\phi}}(\rmX)-\text{smooth}(\1_{\rvy})\vert_{i}
        \leq \sqrt{\frac{1}{2} \KL(\hat{p}(\rvy\vert \rmX) \Vert q_{\bm{\phi}}(\rvy\vert \rmX))}
    \end{split}

\delta(P,Q)\leq \sqrt{\frac{1}{2}\KL(P\Vert Q)}

\delta(P,Q) =\sup\{\vert P(\rmA)-Q(\rmA)\vert\vert \rmA \in \Sigma \text{ is a measurable event.} \}

\vert P(\rmA)-Q(\rmA)\vert =\vert \sum_{i\in \rmA} p(\rvy=i)-q(\rvy=i)\vert 

\vert P({i})-Q({i})\vert  \leq \delta(P,Q)

\vert P({i})-Q({i}) \vert = \vert \bm{\pi}_{\bm{\phi}}(\rmX)-\text{smooth}(\1_{\rvy})\vert_{i}

 \vert \text{smooth-ELBO}_{\sD_{L}}(\rmX,\rvy)-\text{ELBO}_{\sD_{L}}(\rmX,\rvy)\vert \leq C_1\delta+C_2\frac{\delta^2}{\epsilon}+\Delta(\delta)

\begin{split}
         &\vert \text{smooth-ELBO}_{\sD_{L}}(\rmX,\rvy)-\text{ELBO}_{\sD_{L}}(\rmX,\rvy)\vert \\
         &= \vert \E_{\hat{p}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)} -\E_{q_{\bm{\phi}}(\rvy\vert \rmX)}\log \frac{p(\rvy)}{q_{\bm{\phi}}(\rvy\vert \rmX)}\vert\\
        &\leq KM\delta+\frac{K(K-1)}{\epsilon}\delta^2+K\log \frac{1}{1-\epsilon}\delta+\Delta(\delta)
\end{split}

    \begin{split}
          & \log p(\rmX) = \log \E_{q_{\bm{\phi}}(\rvz\vert \rmX),\hat{p}(\rvy\vert \rmX)}\frac{p(\rmX,\rvz,\rvy)}{q_{\bm{\phi}}(\rvz\vert \rmX)\hat{p}(\rvy\vert \rmX)}\\
          &\geq \E_{q_{\bm{\phi}}(\rvz\vert \rmX),\hat{p}(\rvy\vert \rmX)} \log \frac{p(\rmX,\rvz,\rvy)}{q_{\bm{\phi}}(\rvz\vert \rmX)\hat{p}(\rvy\vert \rmX)} \\
        &= \E_{q_{\bm{\phi}},\hat{p}}\log p(\rmX\vert \rvz,\rvy) -\KL(q_{\bm{\phi}}(\rvz\vert \rmX)\Vert p(\rvz))\\
        &-\KL(\hat{p}(\rvy\vert \rmX) \Vert p(\rvy)) =\text{ELBO}_{\sD_{L}}(\rmX,\rvy).
    \end{split}
    \label{eq:smooth-elbo-2}

\begin{split}
        \max_{\Tilde{X}} (1-\lambda)\cdot \log(p_{\bm{\theta}}(\Tilde{\rmX}\vert \rvz_0,\rvy_0))
        +\lambda\cdot \log(p_{\bm{\theta}}(\Tilde{\rmX}\vert \rvz_1,\rvy_1)).
\end{split}

   \min_{\Tilde{X}} (1-\lambda)\cdot \Vert\Tilde{\rmX}-\rmX_0\Vert_2^2+\lambda\cdot \Vert\Tilde{\rmX}-\rmX_1\Vert_2^2.

    \rmX_0 = f_{\bm{\theta}}(\rvz_0,\rvy_0);\quad \rmX_1 = f_{\bm{\theta}}(\rvz_1,\rvy_1)

    p_{\bm{\theta}}(\Tilde{\rmX}\vert \rvz,\rvy)=C_{0}(\bm{\sigma})\cdot\exp{-\frac{\Vert \Tilde{\rmX}- f_{\bm{\theta}}(\rvz,\rvy)\Vert }{2\cdot C_{1}(\bm{\sigma})}}

  \log p(\rmX)-\text{ELBO} = \KL(q_{\bm{\phi}}(\rvz\vert \rmX) q_{\bm{\phi}}(\rvy\vert \rmX)\Vert p(\rvz,\rvy \vert \rmX)

\begin{split}
    &\log p(\rmX)=\int_{\rvz,\rvy} q_{\bm{\phi}}(\rvz\vert \rmX) q_{\bm{\phi}}(\rvy\vert \rmX) \log p(\rmX)d \rvz d\rvy\\ &=\int_{\rvz,\rvy} q_{\bm{\phi}}(\rvz\vert \rmX) q_{\bm{\phi}}(\rvy\vert \rmX) \log \frac{p(\rmX,\rvz,\rvy)}{p(\rvz,\rvy\vert \rmX)}d \rvz d\rvy\\
    &=\E_{q_{\bm{\phi}}(\rvz\vert \rmX) q_{\bm{\phi}}(\rvy\vert \rmX)}\log \frac{p(\rmX,\rvz,\rvy)}{q_{\bm{\phi}}(\rvz\vert \rmX)q_{\bm{\phi}}(\rvy\vert \rmX)}\\
    &+\int_{\rvz,\rvy} q_{\bm{\phi}}(\rvz\vert \rmX) q_{\bm{\phi}}(\rvy\vert \rmX)\log \frac{q_{\bm{\phi}}(\rvz\vert \rmX) q_{\bm{\phi}}(\rvy\vert \rmX)}{p(\rvz,\rvy\vert \rmX)}\\
    &=\text{ELBO} + \KL(q_{\bm{\phi}}(\rvz\vert \rmX) q_{\bm{\phi}}(\rvy\vert \rmX)\Vert p(\rvz,\rvy \vert \rmX)
\end{split}

\begin{split}
        \min_{\Tilde{\bm{\pi}}} (1-\lambda)\cdot&\KL(\bm{\pi}_{\bm{\phi}}(\rmX_0)\Vert \Tilde{\bm{\pi}})+\lambda\cdot \KL(\bm{\pi}_{\bm{\phi}}(\rmX_1)\Vert \Tilde{\bm{\pi}})\\
        &\textbf{s.t. }\sum_{i=1}^{K}\Tilde{\bm{\pi}}_{i}=1 ;\Tilde{\bm{\pi}}_{i}\geq 0,
\end{split}

    \Tilde{\bm{\pi}} = (1-\lambda) \bm{\pi}_{\bm{\phi}}(\rmX_0) + \lambda\bm{\pi}_{\bm{\phi}}(\rmX_1).

\mathcal{L}(\Tilde{\bm{\pi}},t) = (1-\lambda)\cdot\KL(\bm{\pi}_0\Vert \Tilde{\bm{\pi}})+\lambda\cdot \KL(\bm{\pi}_1\Vert \Tilde{\bm{\pi}})+t*(\sum_{i=1}^{K}\Tilde{\bm{\pi}}_{i}-1)

\begin{split} 
    \frac{\partial \mathcal{L}(\Tilde{\bm{\pi}},t)}{\partial \Tilde{\bm{\pi}}}&=t-\frac{(1-\lambda)\cdot \bm{\pi}_0 + \lambda\cdot \bm{\pi}_1}{\Tilde{\bm{\pi}}}=0\\
    &t*(\sum_{i=1}^{K}\Tilde{\bm{\pi}}_{i}-1)=0
\end{split}

\Tilde{\bm{\pi}} = (1-\lambda)\cdot \bm{\pi}_0 + \lambda\cdot \bm{\pi}_1

\subsection{Appendix F}
\subsubsection{F.1: The semi-supervised performance on MNIST and SVHN.}
We evaluate the inference accuracy of \textit{SHOT-VAE} with experiments on two benchmark datasets, MNIST and SVHN. In experiments, we consider five advanced VAE models as baselines, i.e. the standard VAE (M2)\citep{DBLP:conf/nips/KingmaMRW14}, stacked-VAE (M1+M2) \citep{DBLP:conf/nips/KingmaMRW14}, disentangled-VAE \citep{DBLP:conf/nips/NarayanaswamyPM17}, hyperspherical-VAE \citep{DBLP:conf/uai/DavidsonFCKT18}, and domain-VAE \citep{DBLP:conf/iclr/IlseTLW19}. For fairness, the backbones are all 4-layer MLPs with the same amount of parameters (approximately 1M) and the latent dimensions of  are 10 for MNIST and 32 for SVHN. The results presented in Table \ref{table:smooth-ELBO} show that our SHOT-VAE achieves competitive results to other VAE models without introducing additional domain knowledge or multi-stage structures.
\begin{table}[t]
\caption{Error rates for \textit{SHOT-VAE} and other advanced VAE models on MNIST with 100 labels and SVHN with 1000 labels.}
\label{table:smooth-ELBO}
\begin{center}
\begin{tabular}{ccc}
\toprule
Method & MNIST & SVHN \\
\midrule
M2  &  &  \\
M1+M2  &   &    \\
Disentangled-VAE  &   &  \\
Hyperspherical-VAE &  & /\ \\
Domain-VAE &  &  \\
\textbf{smooth-ELBO}   &  &      \\
\textbf{SHOT-VAE}   &  &      \\
\bottomrule
\end{tabular}
\end{center}
\end{table}
\begin{table}[t]
\centering
\caption{p-value of ANOVA.}
\label{table:anova}
\begin{tabular}{ccc}
\toprule
 & CIFAR-10 & CIFAR-100 \\
\midrule
        &          &            \\
          &          &        \\
\bottomrule
\end{tabular}
\end{table}
\subsubsection{F.2: Robustness analysis of hyper-parameters.}
We have introduced some hyper-parameters in training SHOT-VAE, which can be grouped into 2 categories: (1) Parameters to train a deep generative model, i.e.  for the reparameterization tricks \cite{DBLP:conf/icml/RezendeMW14,DBLP:conf/iclr/JangGP17} and  for the beta-VAE model \citep{DBLP:journals/corr/abs-1804-03599}. (2) Parameters to improve the semi-supervised learning performance, i.e.  in smooth-label and  in the warm-up strategy. Following the previous works \cite{DBLP:conf/iclr/JangGP17,DBLP:conf/nips/Dupont18,DBLP:journals/corr/abs-1804-03599}, we set  and  to make generative models work.

For parameters related to semi-supervised performance, we also simply use the default value in previous works \cite{DBLP:journals/corr/abs-1804-03599,DBLP:conf/nips/MullerKH19}, setting  and . Here we use the statistic hypothesis testing method \textit{one-way ANOVA} \citep{tabachnick2007experimental} to test the null hypothesis of the above three hyper-parameters, that is, the semi-supervised learning performance is the same for different settings of parameters. For , as stated in equation , it should not be too large or too small. If  is too large, the smoothed  is over flexible and becomes too far from the basic degenerated distribution. If too small, then the convergence speed  may be too slow. Therefore, we set 5 value scales, i.e. . For , we also use 5 value scales, i.e. . For each value, we performe 5 experiments with different random seeds to conduct ANOVA. The results in Table \ref{table:anova} do not reject the null hypothesis (all p-values ), which proves that \textbf{the semi-supervised performance is robust to the selection of hyper-parameters}.
\begin{table}[t]
\centering
\caption{The results of combined SHOT-VAE.}
\label{table:combined-VAE}
\begin{tabular}{ccc}
\toprule
Method & CIFAR-10 & CIFAR-100 \\
\midrule
EnAET        &          &            \\
FixMatch          &          &        \\
ReMixMatch          &          &      \\
UDA & & \\
SWSA & &  \\
Mean Teacher & & \\
SHOT-VAE + DA & & \\
SHOT-VAE + MT & & \\
\bottomrule
\end{tabular}
\end{table}
\subsection{Appendix G: Comparison with advanced SOTA methods in leaderboard.}
Recent works on semi-supervised learning can be grouped into 3 categories: (1) data augmentation based models (e.g., VAT). (2) consistency learning (e.g., Mean Teacher and model). (3) generative models (e.g., GAN and VAE). As we know, combining different useful methods will improve the performance, and there are 5 combined methods in the leaderboard\footnote{\url{https://paperswithcode.com/sota/semi-supervised-image-classification-on-cifar}} that report better results than SHOT-VAE, i.e. EnAET \citep{DBLP:journals/corr/abs-1911-09265} combining generative models and data augmentation, FixMatch \citep{DBLP:journals/corr/abs-2001-07685}, ReMixMatch \citep{DBLP:journals/corr/abs-1911-09785} and UDA \citep{xie2019unsupervised} combining data augmentation and consistency learning, and SWSA \citep{DBLP:conf/iclr/AthiwaratkunFIW19} combining model and fast-SWA. 

In our experiments, we choose the latest single models as baselines and claim that our model outperformed others. Moreover, our SHOT-VAE model can also easily combine other semi-supervised models and raise much better results. As shown in Table \ref{table:combined-VAE}, we combine SHOT-VAE with data augmentations (DA) and Mean Teacher (MT) separately and achieved competitive results in the leaderboard.  \end{document}