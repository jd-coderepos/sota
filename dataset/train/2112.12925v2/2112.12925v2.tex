\subsection{Overview of the Proposed Method}



The overall architecture of the proposed method is illustrated in Figure~\ref{fig:network}. 
Our model consists of a point stream and a voxel stream.
To reduce the computation redundancy, we convert the TSDF volume to a point cloud by removing the visible empty voxels, which serves as the input of the point stream. 
The point stream adopts a PointNet++~\cite{qi2017pointnet++}-like encoder-decoder architecture.
The encoder extracts the semantic features in a hierarchical way and the decoder encourages feature propagation in points of the same class. 
Meanwhile, a light-weight voxel stream that only contains two 3D dense convolutional layers is applied to the TSDF volume.
An AVA module is proposed to aggregate the local voxel features to the point features. 
Finally, the predicted point labels are converted back to the voxel representation to calculate the evaluation metrics. 

\subsection{Point Clouds Generation}
\label{sec:point-encoding}
Voxel-based methods always encode the depth map into a 3D TSDF volume~\cite{song2017semantic-sscnet}, and carry out later procedures in this voxel space. 
However, we argue that not all voxels in this volume are of equal importance for the SSC task.
In fact, there are three kinds of voxels inside this volume:
1) the \textbf{observed surface} voxels which are directly projected from the given depth image,
2) the \textbf{occluded} voxels behind the observed surface which we need to complete and recognize,
and 3) the \textbf{visible empty} voxels (such as the atmosphere) between camera and the observed surface. 
The last kind of voxel is useless for our task since we already know it's empty. 
Thus, these voxels are removed during our point clouds generation process.
As shown in Figure~\ref{fig:Point_gen}, we only keep the observed surface and the occluded regions in our point clouds.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{figs/point_generation.pdf}
  \caption{
 \textbf{Points generation from the voxel volumes}. 
 Only the observed surface (yellow) and occluded regions (dark gray) are kept, while the visible empty voxels (light gray) are discarded.
 For example, the average number of the kept points is 16313 for the 60  36  60 input voxels from the NYUCAD dataset, which means about 87\% of the input voxels are redundant.
 }
  \label{fig:Point_gen}
\end{figure}


Each input point  has a -dim feature vector . 
Suppose  is generated from voxel  in the  3D volume, then  are normalized -- indexes of  in the volume. 
 is the TSDF value of , and  is the normalized height value of . 
Please note that  are normalized according to the mass center of the points in the scene, while  is normalized by , the maximum height of the voxelized scene. 
We think the normalized height serves as a prior that describes the positions of objects in the room. 
This could help to distinguish some categories with significantly different height values, such as the floor and the ceiling.

\subsection{Anisotropic Voxel Aggregation}
Due to the sparsity of the point clouds, it is hard for the point stream to model the detailed structure information which is important for the scene completion and recognition. 
Since we have the denser volume data as well, we design a voxel stream to extract the structure features and propose the Anisotropic Voxel Aggregation (AVA) module to fuse the point-voxel features.

We first extract the local features of the TSDF volume through two simple convolution layers. 
This requires little computational cost and takes about only 15.0\% of the overall memory cost, but enables each voxel in the volume to have a suitable receptive field to encode local geometry information. 
As shown in the top part of Figure~\ref{fig:network}, for each center point  in the point cloud, we define three ellipsoidal receptive fields with  axis as the major axis respectively. 
Taking the -axis as an example, the receptive field  of  in the volume could be defined as:

where  is the -th voxel and  is its position,  is the radius for minor axes, and  is a scale factor for the major axis. 
Unless mentioned specifically, we use  as the default value for . 
The receptive field along the -axis and -axis could be defined in a similar way. 
From the perspective of pattern recognition, the anisotropic receptive field ensures us to activate feature patterns in three directions, which is more flexible and effective than the isotropic spherical receptive field.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figs/swfp.pdf}
  \caption{ \textbf{Different Feature Propagation strategies.} (a)  Feature Propagation (FP) in PointNet++~\protect\cite{qi2017pointnet++}. (b) The proposed Semantic-aware Propagation (SP). Different colors mean different semantic classes. Larger points are from a deeper layer, while smaller points are in the current layer. Dashed lines mean smaller weight. The boundary color of the center point means the interpolated features are dominated by which semantic class. In (a), the center point is dominated by the wrong class due to the unsuitable FP strategy, while the proposed method (b) avoids this problem.}
  \label{fig:SP}
\end{figure}

Then we could aggregate the structure features of voxels around  with: 

where  is the fused point-voxel feature, 
 is the feature of , 
 is the feature of -th voxel in the volume,
 is a MLP layer for non-linear feature extraction in the -axis,
 denotes max-pooling operation that keeps the maximum activation in the neighborhood,
and  represents the set of neighbor voxels of  inside the ellipsoidal receptive field with the -axis as the major axis.
This AVA module enables the sparse center points to aggregate local structure information from nearby dense voxels.
Therefore, the information from voxels could positively affect the completion and recognition of point clouds through back propagation.

\subsection{Semantic-aware Propagation}
\label{sec:SP}
During the encoding process, Set Abstract (SA)~\cite{qi2017pointnet++} layers will down-sample the input points. 
Suppose we have  SA layers in the network, named , ..., . 
We denote  as the raw input point set, where  is the feature of the point . 
Then the corresponding output point sets of  SA layers are , ..., , respectively. 
Please note that if a point belongs to  (), then it must belong to  as well, because each SA layer only down-samples points from the former layer. 

To obtain the features of all the raw input points, 
we propose the Semantic-aware Propagation (SP) module, which is a hierarchical propagation strategy as shown in Figure~\ref{fig:SP}.
For a target point  in , we find its -neighbors (, ..., ) in  according to the  coordinates. 
To interpolate the feature of , the general feature propagation can be represented by:

where  is the set of  nearest neighbors of  in , and  is the weight factor for  with respect to the point .

An intuitive idea is to measure the similarity between the point  and  for  and use the similarity as the weight factor. 
However, since  and  belong to different levels and thus are embedded to different feature spaces, 
it is not suitable to directly compare their feature vectors. 
We notice that the point  also exists in  since the points in  is a subset of . 
Then we could measure the similarity between  and  in a learnable manner:

where  is the sigmoid function,  is a MLP and  means channel-wise concatenation. 

In this way, we could interpolate point features with semantic information, which is helpful in SSC task.
We explicitly supervise the learned weights during training, 
by setting the ground truth of  to  if the two points belong to the same semantic class, and  if they belong to different classes.
We think this could encourage the network to only propagate semantically similar features, which weakens the effect of neighbor points from different classes during interpolation.



\subsection{Training Loss}


The training loss involves two terms: SSC loss  and SP loss .
The SSC loss is a weighted voxel-wise cross-entropy loss:

where  is set to  if the voxel at index  is not visible empty (\ie, can be converted to a point) or  otherwise. 
 is the ground truth label,
 is the prediction of the voxel mapped back from the corresponding point,
 is the number of valid voxels in this volume, 
and  is the cross-entropy loss.


The SP loss is designed to supervise the pairwise similarity introduced in Equation \ref{eq:pair-sim}. The SP loss could be formulated as:


where  represents the number of point pairs involved,  means the number of points in the -th level, 
 is defined in Section \ref{sec:SP}. 
 is the ground truth of the pair-wise similarity. If two points  and  belong to the same category, it is 1, otherwise it is 0.

We optimize the entire network by the balanced combination of the two terms:



