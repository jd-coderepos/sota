\section{Experiments}
\label{exp}
\subsection{Experimental Settings}
We evaluate the proposed PIoU loss with anchor-based and anchor-free OBB-detectors (RefineDet, CenterNet) under different parameters, backbones. We also compare the proposed method with other state-of-the-art OBB-detection methods in different benchmark datasets (\textit{i.e.} DOTA~\cite{Xia2018DAS}, HRSC2016~\cite{Liu2016SRB}, PASCAL VOC~\cite{Everingham2015TPV}) and the proposed Retail50K dataset. The training and testing tasks are accomplished on a desktop machine with Intel(R) Core(TM) i7-6850K CPU @ 3.60GHzs, 64 GB installed memory, a GeForce GTX 1080TI GPU (11 GB global memory), and Ubuntu 16.04 LTS. With this machine, the batch size is set to 8 and 1 for training and testing, respectively.

\noindent\textbf{Anchor-based OBB Detector:}
\label{approach:anchorbased}
For anchor-based object detection, we train RefineDet~\cite{refinedet} by updating its loss using the proposed PIoU method. Since the detector is optimized by classification and regression losses, we can easily replace the regression one with PIoU loss  while keeping the original Softmax Loss  for classification. We use ResNet~\cite{He2016DRL} and VGG ~\cite{Simonyan2014VDC} as the backbone models. The oriented anchors are generated by rotating the horizontal anchors by  for . We adopt the data augmentation strategies introduced in \cite{Liu2016SSS} except cropping, while including rotation (i.e. rotate the image by a random angle sampled in . In training phase, the input image is resized to 512512. We adopt the mini-batch training on 2 GPUs with 8 images per GPU. SGD is adopted to optimize the models with momentum set to 0.9 and weight decay set to 0.0005. All evaluated models are trained for 120 epochs with an initial learning rate of 0.001 which is then divided by 10 at 60 epochs and again at 90 epochs. Other experimental settings are the same as those in \cite{refinedet}.

\noindent\textbf{Anchor-free OBB Detector:}
\label{approach:anchorfree}
To extend anchor-free frameworks for detecting OBB, we modify CenterNet~\cite{Zhou2019OAP} by adding an angle dimension regressed by L1-Loss in its overall training objective as our baseline. To evaluate the proposed loss function, in similar fashion as anchor-based approach, we can replace the regression one with PIoU loss  while keeping the other classification loss  the same. Be noted that CenterNet uses a heatmap to locate the center of objects. Thus, we do not back-propagate the gradient of the object's center when computing the PIoU loss. We use DLA~\cite{Yu2018Deep} and ResNet~\cite{He2016DRL} as the backbone models. The data augmentation strategies is the same as those for RefineDet-OBB (shown before). In training phase, the input image is resized to 512512. We adopt the mini-batch training on 2 GPUs with 16 images per GPU. ADAM is adopted to optimize the models. All evaluated models are trained for 120 epochs with an initial learning rate of 0.0005 which is then divided by 10 at 60 epochs and again at 90 epochs. Other settings are the same as those in \cite{Zhou2019OAP}.

\subsection{Ablation Study}
\label{exp:evaluation:anchorbased}
In this section, we investigate the impact of our design settings of the proposed method, and conduct several controlled experiments on DOTA~\cite{Xia2018DAS} and PASCAL VOC~\cite{Everingham2015TPV} datasets.

\noindent{\bf Comparison on different parameters:}
In Eq.~\ref{equ:model:kernel},  is an adjustable factor in our kernel function to control the sensitivity of each pixel. In order to evaluate its influence as well as to find a proper value for the remaining experiments, we conduct a set of experiments by varying  values based on DOTA~\cite{Xia2018DAS} dataset with the proposed anchor-based framework. To simplify discussions, results of  are detailed in Table~\ref{tab:Different_hyperparameter} while their distributions can be visualized in Fig.~\ref{fig:pixel_au}. We finally select  for the rest of the experiments since it achieves the best accuracy.

\noindent{\bf Comparison for oriented bounding box:} Based on DOTA~\cite{Xia2018DAS} dataset, we compare the proposed PIoU loss with the commonly used L1 loss, SmoothL1 loss as well as L2 loss. For fair comparisons, we fix the backbone to VGGNet~\cite{Simonyan2014VDC} and build the network based on FPN~\cite{Lin2017FPN}. Table~\ref{tab:losses:dota} details the comparisons and we can clearly see that the proposed PIoU Loss improves the detection performance by around 3.5\%. HPIoU (Hard PIoU) loss is the simplified PIoU loss using Eq.~\ref{equ:model:optmize}. Its performance is slightly reduced but still comparable to PIoU loss. Thus, HPIoU loss can be a viable option in practise as it has lower computational complexity. We also observe that the proposed PIoU costs 15-20\% more time than other three loss functions, which shows that it is still acceptable in practice. We also observed that HPIoU costs less training time than PIoU. Such observation verifies the theoretical analysis and usability of Eq.~\ref{equ:model:optmize}.

\begin{table}[t!]
\small
\centering
\setlength{\tabcolsep}{20pt}
\caption{Comparison between different sensitivity factor  in Eq.~\ref{equ:model:kernel} for PIoU loss on DOTA dataset. RefineDet~\cite{refinedet} is used as the detection model.}
\vspace{-1mm}
\begin{tabular}{cccc}
\hline
    & AP   & AP & AP  \\ \hline
5  & 46.88 & 59.03 &  34.73 \\
10 & 54.24 & 67.89 & 40.59  \\
15  & 53.41& 65.97 & 40.84  \\ \hline
\end{tabular}
\vspace{-0.2em}
\label{tab:Different_hyperparameter}
\end{table}
\begin{table}[t!]
\small
\centering
\setlength{\tabcolsep}{12pt}
\caption{Comparison between different losses for oriented bounding box on DOTA dataset. RefineDet~\cite{refinedet} is used as the detection model. HPIoU (Hard PIoU) loss refers to the PIoU loss simplified by Eq.~\ref{equ:model:optmize}. Training time is estimated in hours.}
\vspace{-1mm}
\begin{tabular}{lcccc}
\hline
Loss    & AP   & AP & AP & Training Time  \\ \hline
L1 Loss     & 50.66 & 64.14 &  37.18 & 20 \\
L2 Loss     & 49.70 & 62.74 &  36.65 & 20 \\
SmoothL1 Loss & 51.46 & 65.68 & 37.25 & 21.5 \\
\textbf{PIoU Loss} & \textbf{54.24} & \textbf{67.89} & \textbf{40.59} & \textbf{25.7}  \\
\textbf{HPIoU Loss} & \textbf{53.37} & \textbf{66.38} & \textbf{40.36} & \textbf{24.8} \\ \hline
\end{tabular}
\vspace{-0.2em}
\label{tab:losses:dota}
\end{table}
\begin{table}[t!]
\small
\centering
\setlength{\tabcolsep}{7pt}
\caption{Comparison between different losses for horizontal bounding box on PASCAL VOC2007 dataset. SSD~\cite{Liu2016SSS} is used as the detection model.}
\vspace{-1mm}
\begin{tabular}{lccccccccccc}
\hline
Loss    & AP   & AP & AP & AP &AP & AP \\
\hline
SmoothL1 Loss & 48.8 & 79.8 & 72.9 & 60.6 & 40.3 & 10.2    \\
GIoU Loss~\cite{Rezatofighi2019GIO} & 49.9 & 79.8 & 74.1 & 63.2 & 41.9 & 12.4 \\
\textbf{PIoU Loss} & \textbf{50.3} & \textbf{80.1} & \textbf{74.9} & \textbf{63.0} & \textbf{42.5} & \textbf{12.2}  \\
\hline
\end{tabular}
\vspace{-3mm}
\label{tab:losses:voc}
\end{table}

\noindent{\bf Comparison for horizontal bounding box:} Besides, we also compare the PIoU loss with SmoothL1 loss and GIoU loss~\cite{Rezatofighi2019GIO} for horizontal bounding box on PASCAL VOC dataset~\cite{Everingham2015TPV}. In Table \ref{tab:losses:voc}, we observe that the proposed PIoU loss is still better than SmoothL1 loss and GIoU loss for horizontal bounding box regression, particularly at those AP metrics with high IoU threshold. Note that the GIoU loss is designed only for horizontal bounding box while the proposed PIoU loss is more robust and well suited for both horizontal and oriented bounding box. Together with the results in Table \ref{tab:losses:dota}, we observe the strong generalization ability and effectiveness of the proposed PIoU loss.

\subsection{Benchmark Results}
\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{3pt}
\caption{Detection results on Retail50K dataset. The PIoU loss is evaluated on RefineDet~\cite{refinedet} and CenterNet~\cite{Zhou2019OAP} with different backbone models.}
\vspace{-2mm}
\begin{tabular}{lcccccc}
\hline
Method               & Backbone    & AP    & AP & AP & Time (ms) & FPS  \\ \hline
RefineDet-OBB~\cite{refinedet}     & ResNet-50 & 53.96 & 74.15 & 33.77 & 142 & 7  \\
\textbf{RefineDet-OBB+PIoU} & ResNet-50 & \textbf{61.78} & \textbf{80.17} & \textbf{43.39} & \textbf{142} & \textbf{7}   \\
RefineDet-OBB~\cite{refinedet} & ResNet-101      & 55.46 & 77.05 & 33.87 & 167 & 6   \\
\textbf{RefineDet-OBB+PIoU} & ResNet-101 & \textbf{63.00} & \textbf{79.08} & \textbf{46.01} & \textbf{167} & \textbf{6}   \\ \hline
CenterNet-OBB~\cite{Zhou2019OAP} & ResNet18      & 54.44 & 76.58  & 32.29  & 7 & 140 \\
\textbf{CenterNet-OBB+PIoU} & ResNet18 & \textbf{61.02} & \textbf{87.19}  & \textbf{34.85}  & \textbf{7} & \textbf{140} \\
CenterNet-OBB~\cite{Zhou2019OAP} & DLA-34      & 56.13 & 78.29  & 33.97  & 18.18 & 55 \\
\textbf{CenterNet-OBB+PIoU} & DLA-34 & \textbf{61.64} & \textbf{88.47}  & \textbf{34.80}  & \textbf{18.18} & \textbf{55} \\\hline
\end{tabular}
\label{tab:exp:eval:potential}
\end{table}
\begin{table}[t!]
\footnotesize
\centering
\setlength{\tabcolsep}{5pt}
\caption{Detection results on HRSC2016 dataset. \emph{Aug.} indicates data augmentation. \emph{Size} means the image size that used for training and testing.}
\vspace{-2mm}
\begin{tabular}{llcccc}
\hline
Method                 & Backbone    & Size & Aug. & mAP &FPS  \\ \hline
RCNN~\cite{Jiang2017RRR}               & ResNet101   & 800  800    &          & 73.03 & 2\\
RC1 \& RC2~\cite{liu2017high} & VGG-16 & - & - & 75.7 & 1fps \\
RRPN~\cite{Ma2018AST}                   & ResNet101   & 800  800    &          & 79.08 & 3.5\\
RPN~\cite{zhang2018toward} & VGG-16 & - &  & 79.6 & 1fps \\
RetinaNet-H~\cite{Yang2019RRS}            & ResNet101   & 800  800    &          & 82.89 & 14\\
RetinaNet-R~\cite{Yang2019RRS}            & ResNet101   & 800  800    &          & 89.18 & 10\\
RoI-Transformer~\cite{Jian2019LRT}        & ResNet101   & 512  800    &          & 86.20 & - \\ \hline
\multirow{3}{*}{RDet~\cite{Yang2019RRS}}
        & ResNet101   & 300  300    &   & 87.14 & 18\\
        & ResNet101   & 600  600    &   & 88.97 & 15\\
        & ResNet101   & 800  800  &  & 89.26 & 12\\
        \hline \hline
CenterNet-OBB~\cite{Zhou2019OAP}       & ResNet18   & 512  512    &          & 67.73 & 140\\
\textbf{CenterNet-OBB+PIoU}   & \textbf{ResNet18}   & \textbf{512  512}    &          & \textbf{78.54} & \textbf{140}\\
CenterNet-OBB~\cite{Zhou2019OAP}       & ResNet101   & 512  512    &          & 77.43 & 45\\
\textbf{CenterNet-OBB+PIoU}   & \textbf{ResNet101}   & \textbf{512  512}    &          & \textbf{80.32} & \textbf{45}\\
CenterNet-OBB~\cite{Zhou2019OAP}       & DLA-34   & 512  512    &          & 87.98 & 55\\
\textbf{CenterNet-OBB+PIoU}             & \textbf{DLA-34}   & \textbf{512  512}    &          & \textbf{89.20} & \textbf{55}\\ \hline
\end{tabular}
\vspace{-1mm}
\label{tab:state-of-the-art:hrsc2016}
\end{table}

\noindent\textbf{Retail50K:}
We evaluate our PIoU loss with two OBB-detectors (\textit{i.e.} the OBB versions of RefineDet~\cite{refinedet} and CenterNet~\cite{Zhou2019OAP}) on Retail50K dataset. The experimental results are shown in Table \ref{tab:exp:eval:potential}. We observe that, both detectors achieve significant improvements with the proposed PIoU loss ( 7\% improvement for RefineDet-OBB and  6\% improvement for CenterNet-OBB). One reason for obtaining such notable improvements is that the proposed PIoU loss is much better suited for oriented objects than the traditional regression loss. Moreover, the improvements from PIoU loss in Retail50K are more obvious than those in DOTA (\textit{c.f.} Table \ref{tab:losses:dota}), which could mean that the proposed PIoU loss is extremely useful for objects with high aspect ratios and complex environments. This verifies the effectiveness of the proposed method.

\noindent\textbf{HRSC2016:}
The HRSC2016 dataset~\cite{Liu2016SRB} contains 1070 images from two scenarios including ships on sea and ships close inshore. We evaluate the proposed PIoU with CenterNet~\cite{Zhou2019OAP} on different backbones, and compare them with several state-of-the-art detectors. The experimental results are shown in Table \ref{tab:state-of-the-art:hrsc2016}. It can be seen that the CenterNet-OBB+PIoU outperforms all other methods except RDet-800. This is because we use a smaller image size (512512) than RDet-800 (800800). Thus, our detector preserves a reasonably competitive detection performance, but with far better efficiency (55 fps \textit{v.s} 12 fps). This exemplifies the strength of the proposed PIoU loss on OBB detectors.

\noindent\textbf{DOTA:}
The DOTA dataset~\cite{Xia2018DAS} contains 2806 aerial images from different sensors and platforms with crowd-sourcing. Each image is of size about 40004000 pixels and contains objects of different scales, orientations and shapes. Note that image in DOTA is too large to be directly sent to CNN-based detectors. Thus, similar to the strategy in \cite{Xia2018DAS}, we crop a series of 512512 patches from the original image with the stride set to 256. For testing, the detection results are obtained from the DOTA evaluation server. The detailed performances for each category are reported so that deeper observations could be made. We use the same short names, benchmarks and forms as those existing methods in~\cite{Yang2019RRS} to evaluate the effectiveness of PIoU loss on this dataset. The final results are shown in Table~\ref{tab:state-of-the-art:dota}.
We find that the performance improvements vary among different categories. However, it is interesting to find that the improvement is more plausible for some categories with high aspect ratios. For example, harbour (HA), ground track field (GTF), soccer-ball field (SBF) and basketball court (BC) all naturally have large aspect ratios, and they appear to benefit from the inclusion of PIoU. Such observations confirm that the PIoU can effectively improve the performance of OBB detectors, particularly on objects with high-aspect ratios. These verify again the effectiveness of the proposed PIoU loss on OBB detectors. We also find that our baselines are relatively low than some state-of-the-art performances. We conjecture the main reason is that we use much smaller input size than other methods (512 vs 1024 on DOTA). However, note that the existing result (89.2 mAP) for HRSC2016 in Table~\ref{tab:state-of-the-art:hrsc2016} already achieves the state-of-the-art level performance with only  image size. Thus, the proposed loss function can bring gain in this strong baseline.

\begin{table*}[t!]
\tiny
\centering
\setlength{\tabcolsep}{0.7pt}
\caption{Detection results on DOTA dataset. We report the detection results for each category to better demonstrate where the performance gains come from.}
\vspace{-1mm}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
Method & Backbone & Size & PL    & BD    & BR    & GTF   & SV    & LV    & SH    & TC    & BC    & ST    & SBF   & RA    & HA    & SP    & HC  &mAP   \\ \hline
SSD~\cite{Liu2016SSS}      & VGG16 & 512      & 39.8 & 9.1  & 0.6  & 13.2 & 0.3  & 0.4  & 1.1  & 16.2 & 27.6 & 9.2  & 27.2 & 9.1  & 3.0  & 1.1  & 1.0  & 10.6 \\
YOLOV2~\cite{Redmon2016YBF} & DarkNet19 & 416 & 39.6 & 20.3 & 36.6 & 23.4 & 8.9  & 2.1  & 4.8  & 44.3 & 38.4 & 34.7 & 16.0 & 37.6 & 47.2 & 25.5 & 7.5  & 21.4 \\
R-FCN~\cite{Dai2016ROD} & ResNet101 & 800 & 37.8 & 38.2 & 3.6 & 37.3 & 6.7 & 2.6 & 5.6 & 22.9 & 46.9 & 66.0 & 33.4 & 47.2 & 10.6 & 25.2 & 18.0 & 26.8 \\
FR-H~\cite{Ren2015FRT} & ResNet101 & 800 & 47.2 & 61.0 & 9.8 & 51.7 & 14.9 & 12.8 & 6.9 & 56.3 & 60.0 & 57.3 & 47.8 & 48.7 & 8.2 & 37.3 & 23.1 & 32.3 \\
FR-O~\cite{Xia2018DAS} & ResNet101 & 800 & 79.1 & 69.1 & 17.2 & 63.5 & 34.2 & 37.2 & 36.2 & 89.2 & 69.6 & 59.0 & 49. & 52.5 & 46.7 & 44.8 & 46.3 & 52.9 \\
R-DFPN~\cite{yang2018automatic} & ResNet101 & 800 & 80.9 & 65.8 & 33.8 & 58.9 & 55.8 & 50.9 & 54.8 & 90.3 & 66.3 & 68.7 & 48.7 & 51.8 & 55.1 & 51.3 & 35.9 & 57.9 \\
RCNN~\cite{Jiang2017RRR} & ResNet101 & 800 & 80.9 & 65.7 & 35.3 & 67.4 & 59.9 & 50.9 & 55.8 & 90.7 & 66.9 & 72.4 & 55.1 & 52.2 & 55.1 & 53.4 & 48.2 & 60.7 \\
RRPN~\cite{Ma2018AST} & ResNet101 & 800 & 88.5 & 71.2 & 31.7 & 59.3 & 51.9 & 56.2 & 57.3 & 90.8 & 72.8 & 67.4 & 56.7 & 52.8 & 53.1 & 51.9 & 53.6 & 61.0 \\
\hline \hline
RefineDet~\cite{refinedet} & VGG16  & 512 & 80.5 & 26.3 & 33.2 & 28.5 & 63.5 & 75.1 & 78.8 & 90.8 & 61.1 & 65.9 & 12.1 & 23.0 & 50.9 & 50.9 & 22.6 &  50.9 \\
\textbf{RefineDet+PIoU}  & VGG16 & 512 & \textbf{80.5} & \textbf{33.3} & \textbf{34.9} & \textbf{28.1} & \textbf{64.9} & \textbf{74.3} & \textbf{78.7} & \textbf{90.9} & \textbf{65.8} & \textbf{66.6} & \textbf{19.5} & \textbf{24.6} & \textbf{51.1} & \textbf{50.8} & \textbf{23.6}& \textbf{52.5} \\
RefineDet~\cite{refinedet}& ResNet101 & 512 & 80.7 & 44.2 & 27.5 & 32.8 & 61.2 & 76.1 & 78.8 & 90.7 & 69.9 & 73.9 & 24.9 & 31.9 & 55.8 & 51.4 & 26.8 & 55.1 \\
\textbf{RefineDet+PIoU} & ResNet101 & 512 & \textbf{80.7} & \textbf{48.8} & \textbf{26.1} & \textbf{38.7} & \textbf{65.2} & \textbf{75.5} & \textbf{78.6} & \textbf{90.8} & \textbf{70.4} & \textbf{75.0} & \textbf{32.0} & \textbf{28.0} & \textbf{54.3} & \textbf{53.7} & \textbf{29.6} & \textbf{56.5}\\
\hline
CenterNet~\cite{Zhou2019OAP} & DLA-34 & 512 & 81.0 & 64.0 & 22.6 & 56.6 & 38.6 & 64.0 & 64.9 & 90.8 & 78.0 & 72.5 & 44.0 & 41.1 & 55.5 & 55.0 & 57.4 & 59.1 \\
\textbf{CenterNet+PIoU} & DLA-34 & 512 & \textbf{80.9} & \textbf{69.7} & \textbf{24.1} & \textbf{60.2} & \textbf{38.3} & \textbf{64.4} & \textbf{64.8} & \textbf{90.9} & \textbf{77.2} & \textbf{70.4} & \textbf{46.5} & \textbf{37.1}  & \textbf{57.1} & \textbf{61.9} & \textbf{64.0} & \textbf{60.5} \\
\hline
\end{tabular}
\label{tab:state-of-the-art:dota}
\vspace{-2mm}
\end{table*}
\begin{figure*}[t]
  \centering
    \includegraphics[width=1\linewidth]{images/results.pdf}
  \vspace{-2mm}
  \caption{Samples results using PIoU (red boxes) and SmoothL1 (yellow boxes) losses on Retail50K (first row), HRSC2016 (second row) and DOTA (last row) datasets.}
\label{fig:exp:baseline:ourmethod:vis}
\vspace{-4mm}
\end{figure*}

In order to visually verify these performance improvements, we employ the anchor-based model RefineDet~\cite{refinedet} and conduct two independent experiments using PIoU and SmoothL1 losses. The experiments are applied on all three datasets (\textit{i.e.} Retail50K, DOTA~\cite{Xia2018DAS}, HRSC2016~\cite{Liu2016SRB}) and selected visual results are presented in Figure~\ref{fig:exp:baseline:ourmethod:vis}. We can observe that the OBB detector with PIoU loss (in red boxes) has more robust and accurate detection results than the one with SmoothL1 loss (in yellow boxes) on all three datasets, particularly on Retail50K, which demonstrates its strength in improving the performance for high aspect ratio oriented objects. Here, we also evaluate the proposed HPIoU loss with the same configuration of PIoU. In our experiments, the performances of HPIoU loss are slightly lower than those of PIoU loss (0.87, 1.41 and 0.18 mAP on DOTA, Retail50K and HRSC2016 respectively), but still better than smooth-L1 loss while having higher training speed than PIoU loss. Overall, the performances of HPIoU are consistent on all three datasets.