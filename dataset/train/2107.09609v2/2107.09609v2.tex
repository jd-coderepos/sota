\section{Experiments and Results}

\subsection{Experimental Setup}\label{sec:experiment_setup}
\textbf{Data and evaluation metrics.}
We split \DatasetName~into 70\% \textit{train}, 15\% \textit{val}, and 15\% \textit{test} portions.
To evaluate moment retrieval with multiple moments, we use mean average precision (mAP) with IoU thresholds 0.5 and 0.75, as well as the average mAP over multiple IoU thresholds [0.5: 0.05: 0.95], similar to action detection in~\cite{caba2015activitynet}.
We also report standard metric Recall@1 (R@1) used in single moment retrieval, where we define a prediction to be positive if it has a high IoU (>= 0.7) with one of the ground truth moments. 
For highlight detection, we use mAP as the main metric. 
We also follow~\cite{liu2015multi} to use HIT@1 to compute the hit ratio for the highest scored clip.
Similar to~\cite{liu2015multi}, we define a clip as positive if it has a score of `Very Good'.
Since we have ground truth saliency scores from 3 users, we evaluate performance against each then take the average. 

\textbf{Implementation details.}
Our model is implemented in PyTorch~\cite{paszke2019pytorch}. We set the hidden size =256, \#layers in encoder/decoder =2, \#moment queries =10.
We use dropout of 0.1 for transformer layers and 0.5 for input projection layers.
We set the loss hyperparameters as =, =, =, =, =. 
The model weights are initialized with Xavier init~\cite{glorot2010understanding}.
We use AdamW~\cite{loshchilov2017decoupled} with an initial learning rate of 1e-4, weight decay 1e-4 to optimize the model parameters.
The model is trained for 200 epochs with batch size 32.
For pretraining, we use the same setup except that we train the model for 100 epochs with batch size 256. 
Both training/finetuning and pretraining are conducted on an RTX 2080Ti GPU, with training/finetuning taking ~12 hours and pretraining ~2 days.



\subsection{Results and Analysis}\label{sec:results}
\begin{table*}[t!]
    \setlength{\tabcolsep}{0.4em}
    \small
    \centering
    \caption{Baseline Comparison on \DatasetName~\textit{test} split. We highlight the best score in each column in \textbf{bold}, and the second best score with \underline{underline}. XML+ denotes our improved XML~\cite{lei2020tvr} model. 
    \textit{PT} denotes weakly supervised pretraining with ASR captions. For Moment-DETR variants, we also report standard deviation of 5 runs with different random seeds.}
    \scalebox{0.96}{
\begin{tabular}{lccccccc}
\toprule
\multirow{3}{*}{ Method } & \multicolumn{5}{c}{Moment Retrieval} & \multicolumn{2}{c}{Highlight Detection} \\
& \multicolumn{2}{c}{R1} & \multicolumn{3}{c}{mAP} & \multicolumn{2}{c}{>= Very Good} \\
\cmidrule(l){2-3} \cmidrule(l){4-6}  \cmidrule(l){7-8}
& @0.5 & @0.7 & @0.5 & @0.75 & avg & mAP & HIT@1 \\
\midrule
BeautyThumb~\cite{song2016click} & - & - & - & - & - & 14.36 & 20.88 \\
DVSE~\cite{liu2015multi} & - & - & - & - & - & 18.75 & 21.79 \\
MCN~\cite{anne2017localizing} & 11.41 & 2.72 & 24.94 & 8.22 & 10.67 & - & - \\
CAL~\cite{escorcia2019temporal} & 25.49 & 11.54 & 23.40 & 7.65 & 9.89 & - & - \\
CLIP~\cite{radford2021learning} & 16.88 & 5.19 & 18.11 & 7.00 & 7.67 & 31.30 & \bf 61.04 \\
XML~\cite{lei2020tvr} & 41.83 & 30.35 & 44.63 & 31.73 & 32.14 & 34.49 & 55.25 \\
XML+ & 46.69 & \underline{33.46} & 47.89 & \underline{34.67} & \underline{34.90} & 35.38 & 55.06 \\
\midrule
Moment-DETR & \underline{52.89 \std{2.3}} & 33.02\std{1.7} & \underline{54.82\std{1.7}} & 29.40\std{1.7} & 30.73\std{1.4} & \underline{35.69\std{0.5}} & 55.60\std{1.6} \\
Moment-DETR w/ PT & \bf 59.78\std{0.3} & \bf 40.33\std{0.5} & \bf 60.51\std{0.2} & \bf 35.36\std{0.4} & \bf 36.14\std{0.25} & \bf 37.43\std{0.2} & \underline{60.17\std{0.7}} \\
\bottomrule
\end{tabular}
    }
    \label{tab:baseline_comparison}
    \vspace{-5pt}
    \end{table*}


\textbf{Comparison with baselines.}
We compare Moment-DETR with various moment retrieval and highlight detection methods on the \DatasetName~test split; results are shown in Table~\ref{tab:baseline_comparison}.
For moment retrieval, we provide three baselines, two proposal-based methods MCN~\cite{anne2017localizing} and CAL~\cite{escorcia2019temporal}, and a span prediction method XML~\cite{lei2020tvr}.
For highlight detection, we provide two baselines, BeautyThumb~\cite{song2016click} based solely on frame quality, and DVSE~\cite{liu2015multi} based on clip-query similarity.
Since XML also outputs clip-wise similarity scores to the user query, we provide highlight detection results for this model as well.
The original XML model has a smaller capacity than Moment-DETR, hence for a fair comparison, we increased its capacity by adding more layers and train it for the same number of epochs as Moment-DETR. Moreover, to leverage the saliency annotations in \DatasetName, we further added an auxiliary saliency loss to it (referred to as `XML+'). 
These enhancements improve the original XML model by 2.76 average mAP.
In addition, for both tasks, we also add CLIP~\cite{radford2021learning} as a baseline. Specifically, we compute clip-wise similarity scores by computing image-query scores where the image is the center frame of the clip. For highlight detection, we directly use these scores as prediction; for moment retrieval, we use TAG~\cite{zhao2017temporal} to progressively groups top-scored clips with the classical watershed algorithm~\cite{roerdink2000watershed}. 


Compared to the best baseline XML+, Moment-DETR performs competitively on moment retrieval, where it achieves significantly higher scores on a lower IoU threshold, i.e., R1@0.5 and mAP@0.5 (>7\% absolute improvement), but obtains lower scores on higher IoU threshold, i.e., R1@0.7 and mAP@0.75.
We hypothesize that this is because the L1 and generalized IoU losses give large penalties only to large mismatches (i.e., small IoU) between the predicted and ground truth moments.
This property encourages Moment-DETR to focus more on predictions with small IoUs with the ground truth, while less on those with a reasonably large IoU (e.g., 0.5).
This observation is the same as DETR~\cite{carion2020end} for object detection, where it shows a notable improvement over the baseline in AP, but lags behind in AP.
For highlight detection, Moment-DETR performs similarily to XML+.
As discussed in Section~\ref{sec:weakly_asr}, Moment-DETR is designed without human priors or hand-crafted components, thus may require more training data to learn these priors from data.
Therefore, we also use ASR captions for weakly supervised pretraining.
With pretraining, Moment-DETR greatly outperforms the baselines on both tasks, showing the effectiveness of our approach.\footnote{We also tried pretraining with XML+, and under careful tuning, the results are still worse than without pretraining, which might because XML+'s cross-entropy loss gives strong penalties to small mismatches, preventing it from learning effectively from noisy (thus many small mismatches) data.} 
One surprising finding is that CLIP~\cite{radford2021learning} gives the best highlight detection performance in terms of HIT@1, though its overall performance is much lower than Moment-DETR.



\begin{table}[t!]
\setlength{\tabcolsep}{0.2em}
\small
\centering
\caption{Loss ablations on \DatasetName~\textit{val} split. 
All models are trained from scratch.}
\scalebox{0.96}{
    \begin{tabular}{P{0.8cm}
        P{1cm}
        P{1cm}
        P{1cm}
        P{1cm}P{1.2cm}P{1.7cm}P{2cm}P{2cm}}
        \toprule
        \multirow{2}{*}{L1} & \multirow{2}{*}{gIoU} & \multirow{2}{*}{Saliency} & \multirow{2}{*}{CLS} & \multicolumn{3}{c}{Moment Retrieval} & \multicolumn{2}{c}{Highlight Detection (>=Very Good)}
        \\ \cmidrule(l){5-7} \cmidrule(l){8-9}
& & & & R1@0.5 & R1@0.7 & mAP avg & mAP & Hit@1 \\
\midrule
\cmark & \cmark & & \cmark & 44.84 & 25.87 & 25.05 & 17.84 & 20.19 \\
\cmark & & \cmark & \cmark & \underline{51.10} & \underline{31.16} & 27.61 & 35.28 & 54.32 \\
& \cmark & \cmark & \cmark & 50.90 & 30.97 & \underline{28.84} & \bf 36.61 & \bf 56.71 \\
\cmark & \cmark & \cmark & \cmark & \bf 53.94 & \bf  34.84 & \bf 32.20 & \underline{35.65} & \underline{55.55} \\
        \bottomrule
\end{tabular}
}
\label{tab:loss_ablation}
\end{table}




\begin{figure*}[t]
    \centering
    \includegraphics[width=0.96\linewidth]{res/moment_query_dist.pdf}
    \caption{Visualization of all moment span predictions for all the 1550 videos on \DatasetName~val split, for all the 10 moment query slots in Moment-DETR decoder. x-axis denotes the normalized moment span center coordinates w.r.t. the videos, y-axis denotes the normalized moment span width (also indicated by color). We observe that each slot learns to predict moments in different temporal locations and different widths. For example, the first slot mostly predicts short moments near the beginning of the videos, while the second slot mostly predicts short moments near the end.}
    \label{fig:moment_query_dist}
    \vspace{-5pt}
\end{figure*}
  

\textbf{Loss ablations.}
In Table~\ref{tab:loss_ablation}, we show the impact of the losses by turning off one loss at a time. 
When turning off the saliency loss, we observe a significant performance drop for highlight detection, and surprisingly moment retrieval as well. 
We hypothesize that the moment span prediction losses (L1 and IoU) and classification (CLS) loss do not provide strong supervision for learning the similarity between the input text queries and their relevant clips, while the saliency loss gives a direct signal to learn such similarity.
Under our framework, this also suggests that \emph{jointly detecting saliency is beneficial to retrieve moments}. 
When turning off one of the span predictions losses (L1 or IoU), we see a notable drop in moment retrieval performance while the highlight detection performance stays similar, showing that both losses are important for moment retrieval. 


\textbf{Moment query analysis.}
In Figure~\ref{fig:moment_query_dist}, we visualize moment span predictions for all the 1550 \DatasetName~\textit{val} videos, for the 10 moment query slots in Moment-DETR decoder. As shown in the figure, each slot learns to predict moments of different patterns, i.e., different temporal locations and different widths. 
For example, some slots learn to predict short moments near the beginning or end of the videos (e.g., the first two slots), while some slots learn to predict both short and long moments near the center (e.g., the third slot).
Overall, most of the slots learn to predict short moments while only a handful of them learn to predict long moments, possibly because there are more short moments in \DatasetName~than long moments (see our data analysis in Section~\ref{sec:data_analysis}). 



\begin{table}[t!]
\setlength{\tabcolsep}{0.8em}
\small
\centering
\caption{Effect of pretraining data domain and size, results are on \DatasetName~\textit{val} split. 
}
\scalebox{1.0}{
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{ Pretraining Videos } & \multicolumn{3}{c}{Moment Retrieval} & Highlight Detection & \\ \cmidrule(l){2-4} \cmidrule(l){5-6}
& R1@0.5 & R1@0.7 & mAP avg & mAP & HIT@1 \\
\midrule
None & 53.94 & 34.84 & 32.20 & 35.65 & 55.55 \\
2.5K HowTo100M & 54.58 & 36.45 & 32.82 & 36.27 & 56.45 \\
5K HowTo100M & 55.23 & 35.74 & 32.84 & \underline{36.82} & \underline{58.39} \\
10k HowTo100M & \underline{56.06} & \underline{38.39} & \underline{34.16} & 35.97 & 56.84 \\
\midrule
5.4K \DatasetName & \bf 59.68 & \bf 40.84 & \bf 36.30 & \bf  37.70 & \bf  60.32 \\
\bottomrule
\end{tabular}
}
\label{tab:pt_data_ablation}
\end{table}


\textbf{Pretraining data domain and size.} To better understand the role of pretraining, we examine the effect of using data of different domain and sizes for pretraining. Specifically, we use HowTo100M~\cite{miech2019howto100m} instructional videos of different sizes for pretraining Moment-DETR and then evaluate its finetuning performance on \DatasetName~dataset. The results are shown in Table~\ref{tab:pt_data_ablation}. 
We notice that out-of-domain pretraining on HowTo100M videos also improves the model performance, even when only trained on 2.5K videos. 
When increasing the number videos, we see there is a steady performance gain for the moment retrieval task. While for highlight detection, the performance fluctuates, probably because of the pretraining task is not well aligned with the goal of detecting highlights.
Compared to in-domain and out-of-domain videos, we notice that 5.4K in-domain \DatasetName~videos greatly outperforms 10K HowTo100M videos, demonstrating the importance of aligning the pretraining and the downstream task domain.


\textbf{Generalization to other datasets.} In Table~\ref{tab:charades_sta_results}, we test Moment-DETR's performance on moment retrieval dataset CharadesSTA~\cite{gao2017tall}. 
Similar to our observations in Table~\ref{tab:baseline_comparison}, when comparing to SOTA methods, we notice that Moment-DETR shows a significant performance gain on R1@0.5, while performs slightly worse on a tighter metric, i.e., R1@0.7. 
Meanwhile, after pretrained on 10K HowTo100M videos, Moment-DETR's performance is greatly improved, setting new state-of-the-art for the dataset. This shows Moment-DETR's potential to adapt to different data and tasks.



\begin{table}[t!]
\setlength{\tabcolsep}{0.8em}
\small
\centering
\caption{Results on CharadesSTA~\cite{gao2017tall} \textit{test} split.
}
\scalebox{1.0}{
\begin{tabular}{lcc}
\toprule
Method & R1@0.5 & R1@0.7 \\
\midrule
CAL~\cite{escorcia2019temporal} & 44.90 & 24.37 \\
2D TAN~\cite{zhang2020learning} & 39.70 & 23.31 \\
VSLNet~\cite{zhang2020span} & 47.31 & 30.19 \\
IVG-DCL~\cite{nan2021interventional} & 50.24 & \underline{32.88} \\
\midrule
Moment-DETR & \underline{53.63} & 31.37 \\
Moment-DETR w/ PT (on 10K HowTo100M videos) & \bf 55.65 & \bf 34.17 \\
\bottomrule
\end{tabular}
}
\label{tab:charades_sta_results}
\vspace{-5pt}
\end{table}



\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{res/predictions.pdf}
  \caption{Prediction visualization. Predictions are shown in \textcolor{red}{solid red boxes or lines}, ground-truth are indicated by \textcolor{ForestGreen}{dashed green lines}. \textit{Top} row shows a correct prediction, \textit{bottom} row shows a failure.}
  \label{fig:prediction_examples}
  \vspace{-5pt}
\end{figure*}



\textbf{Prediction visualization.} 
Figure~\ref{fig:prediction_examples} (\textit{top}) shows a correct prediction from Moment-DETR. 
We can see that the model is able to correctly localize two disjoint moments relevant to the user query. 
Meanwhile, the saliency scores also align very well with the ground truth score curve (obtained by averaging the scores from 3 annotators).
And not surprisingly, this saliency score curve also matches the moment predictions -- where we see higher scores for localized regions than those outside.
Figure~\ref{fig:prediction_examples} (\textit{bottom}) shows a failure case, where it incorrectly localized a partially true moment (2nd frame) where the family is playing on a court but not playing basketball. 
More examples in Appendix. 


