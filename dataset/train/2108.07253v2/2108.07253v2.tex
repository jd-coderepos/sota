\section{Results and Evaluation}\label{sec:results}

We compare our model
to other visual grounding methods trained on a variety of datasets. We study four key questions: How well do previous methods for visual grounding perform on our proposed task? To what extent is our model reasoning over complex multimodal signals? What is the impact of our design choices? And, what has our model learned?  
We also present qualitative results (Figure~\ref{fig:results_paper} and supplemental material), which  highlight the complexity and unique challenges of our proposed task.

\begin{table}[t]
  \centering
  \setlength{\tabcolsep}{3.2pt}
  \def\arraystretch{0.95}
  \begin{tabularx}{1.0\columnwidth}{lllcccccc}
    \toprule
    Method                                   & Training Data      &   & Accuracy    \\
    \midrule
    \textbf{Full Names} \\
    Gupta et al.~\cite{gupta2020contrastive} & COCO               &   & 36.9 $\pm$ 1.04          \\
    Gupta et al.~\cite{gupta2020contrastive} & Flickr30K Entities &   & 39.3 $\pm$ 1.05          \\
    SL-CCRF~\cite{liu2020phrase}             & Flickr30K Entities &   &  43.5 $\pm$ 1.06         \\
    MAttNet~\cite{yu2018mattnet}             & RefCOCOg           &   &  43.6 $\pm$ 1.06         \\
    UNITER~\cite{chen2020uniter}             & Multiple~\cite{lin2014microsoft,krishna2017visual,ordonez2011im2text,sharma2018conceptual}  &   &  36.3 $\pm$ 1.03            \\
    \midrule
    \textbf{Random} \\
    Gupta et al.~\cite{gupta2020contrastive} & COCO               &   &  39.3 $\pm$ 1.05         \\
    Gupta et al.~\cite{gupta2020contrastive} & Flickr30K Entities &   &  41.1 $\pm$ 1.06          \\
    SL-CCRF~\cite{liu2020phrase}             & Flickr30K Entities &   & 44.1 $\pm$ 1.07          \\
    MAttNet~\cite{yu2018mattnet}             & RefCOCOg           &   &  44.0 $\pm$ 1.07          \\
    UNITER~\cite{chen2020uniter}             & Multiple~\cite{lin2014microsoft,krishna2017visual,ordonez2011im2text,sharma2018conceptual}  &   &  38.4 $\pm$ 1.04          \\
    \midrule
    \textbf{Constant} \\
    Gupta et al.~\cite{gupta2020contrastive} & COCO               &   &  35.6 $\pm$ 1.03          \\
    Gupta et al.~\cite{gupta2020contrastive} & Flickr30K Entities &   &  38.2 $\pm$ 1.04          \\
    SL-CCRF~\cite{liu2020phrase}             & Flickr30K Entities &   &  46.4 $\pm$ 1.07          \\
    MAttNet~\cite{yu2018mattnet}             & RefCOCOg           &   &  24.1 $\pm$ 0.92           \\
    UNITER~\cite{chen2020uniter}             & Multiple~\cite{lin2014microsoft,krishna2017visual,ordonez2011im2text,sharma2018conceptual}  &   & 34.2 $\pm$ 1.02          \\
    \midrule
    Random                             & --                 &   & 30.9 $\pm$ 0.99          \\
    Big$\rightarrow$Small                       & --                 &   &  48.2 $\pm$ 1.07           \\
    L$\rightarrow$R (All)                            & --                 &   &  38.4 $\pm$ 1.04            \\
    L$\rightarrow$R (Largest)                           & --                 &   &  57.7 $\pm$ 1.06            \\
    \midrule
    Ours                                     & \dataset              && \textbf{63.5} $\boldsymbol{\pm}$ \textbf{1.03} \\
    \bottomrule
  \end{tabularx}
  \vspace{3pt}
  \caption{Evaluation on the \dataset test set. We compare against prior grounding methods using multiple configurations, varying according to how names are processed. We also compare to several simple baselines, detailed in the text.}
\label{tab:evaluation}
\end{table}

 
\begin{figure*} \centering

\jsubfig{\includegraphics[height=4.2cm,trim={0 1.0cm 0 2.0cm},clip]{figures/examples/fig6_1.jpg}}{\vspace{-8pt}\begin{flushleft}
U.S. Air Force Colonel \colorbox{gold}{\textbf{Clay Garrison}} goes over some final instructions with U.S. Congressman \colorbox{tomato}{\textbf{David Valadao}} prior to take-off from the Fresno Air National Guard Base May 27, 2015.\end{flushleft}} \hfill
\jsubfig{\includegraphics[height=4.2cm]{figures/examples/fig6_2.jpg}}{\vspace{-8pt}{\begin{flushleft} \colorbox{tomato}{\textbf{Daniel Sch√ºtz}} (\#20) behind goalkeeper \colorbox{gold}{\textbf{Domenik Schierl}}.\end{flushleft}}} \hfill
\jsubfig{\includegraphics[height=4.2cm]{figures/examples/fig6_3.jpg}}{\vspace{-8pt}\begin{flushleft} \textbf{Kathryn Hire}, an astronaut and Navy reserve component Sailor assigned to the Office of Naval Research, presents items she took to space to Rear Adm. \colorbox{tomato}{\textbf{Nevin Carr}}.\end{flushleft}}

\vspace{-7pt}
\caption{\textbf{Box--name correspondences predicted by our model.} We show predicted entities on top of the their associated box (in white). Ground truth links are denoted by matching colors. Please refer to the supplemental material for additional qualitative results.}
\label{fig:results_paper}
\end{figure*}



 
\subsection{Comparison to Prior Work}

We evaluate several recent visual grounding models on the \dataset test set: a weakly-supervised framework by Gupta \etal~\cite{gupta2020contrastive}, a supervised neural chain conditional random field that captures entity dependencies (SL-CCRF)~\cite{liu2020phrase}, and a supervised network that combines attention from separate modules (MAttNet)~\cite{yu2018mattnet}.
We also evaluate UNITER~\cite{chen2020uniter}, a pretrained multi-task vision-and-language framework, which our model is based on. 

Table~\ref{tab:evaluation} shows test set accuracies for our approach and for existing methods trained on different datasets. We report 95\% binomial proportion confidence intervals (Wilson score intervals) with these accuracies.
For existing models, we vary how names are provided during inference because these models are not automatically compatible with our placeholder \NAME token: (a) unmodified \textbf{full names}, (b) \textbf{random} popular names, or (c) a \textbf{constant} ``person'' string---\eg, ``Harry met Sally'' is modified to ``person met person''. 

We also evaluate several heuristics that illustrate the challenges and biases in our data (Table~\ref{tab:evaluation}), such as a potential left-to-right bias for named individuals. In particular, we order the names in the caption from left to right, and pair them with detections sorted by (a) decreasing area (Big$\rightarrow$Small), (b) left-to-right upper-left coordinates (L$\rightarrow$R (All)), or (c) left-to-right upper-left coordinates with only the largest $d$ detections (L$\rightarrow$R (Largest)).
We set $d = max(m, n)$ for $m$ detections and $n$ names. We also compare to random guessing.
We observe that these heuristics yield non-trivial, and even strong, performances. This could be because realistic captions tend to follow a left-to-right ordering (especially for posed people---but see Figure~\ref{fig:results_paper} for counterexamples) and filtering by detection size can remove unreferred people.
However, even the strongest heuristic leaves much room for improvement. 
These heuristics are also useful to frame the performance of pretrained visual grounding models. Supervised models (SL-CCRF and MAttNet) perform similarly to Big$\rightarrow$Small, illustrating that these models may be utilizing size-related cues---especially MAttNet, which \emph{only} processes names and not full sentences. We show qualitative results for all baselines in the supplemental material.

\subsection{Ablation study}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{3.1pt}
\def\arraystretch{0.95}
\begin{tabularx}{0.655\columnwidth}{lccc}
\toprule
  Method  && Accuracy        \\ \midrule
  \textbf{Input features} \\
 \quad w/o visual features && 55.4 $\pm$ 1.07  \\
 \quad w/o spatial features &&  58.0 $\pm$ 1.06 \\
 \quad w/o textual features && 51.3 $\pm$ 1.07 \\
  \quad spatial features only && 31.2 $\pm$ 0.99 \\
 \midrule
 \textbf{Learning} \\
 \quad w/o $\mathcal{L}_\mathsf{intra}$ && 31.4 $\pm$ 1.00 \\
  \quad  w/o $\mathcal{L}_\mathsf{inter}$ &&  61.9 $\pm$ 1.04 \\
    \quad  w/o $\mathcal{L}_{\varnothing}$ && 61.7 $\pm$ 1.04 \\
 \quad w/o pretraining &&  50.2 $\pm$ 1.07 \\
\bottomrule
\end{tabularx}
\vspace{3pt}
\caption{Ablation study, evaluating the effect of using different input features, loss terms and the impact using a pretrained model. }\label{tab:ablations}
\end{table}
 
Table~\ref{tab:ablations} shows ablation results. 
We train models using only a subset of features by ablating (i) visual features: set instead to a fixed representation, averaged over $1000$ random detections; (ii) spatial features: fixed at image center coordinates; (iii) textual embeddings: with all words masked out, retaining only position features and special \NAME tokens; and (iv) textual and visual embeddings: retaining only spatial features.
The impact of each input modality is significant, with performance dropping by $5.5\%$ for (ii) and $12.2\%$ for (iii). While these ablations significantly limit the information available for this task, our model performs much better than random guessing in all cases, suggesting that it learns some data biases. Both (i) and (iii) are capable of learning a left-to-right association. Indeed, their correct matches significantly overlap with those of the ``L$\rightarrow$R (Largest)'' heuristics, by $81.7\%$ for (i) and $82.4\%$ for (iii). Finally, from (iv) we infer that spatial features alone are not enough for learning such similarities.

We also quantify the importance of each proposed objective. Training without estimated correspondences (\ie, $\mathcal{L}_\mathsf{intra}$) yields the most significant drop in performance, resulting in nearly random guessing. This illustrates the importance of supervised data for our task. Ablating the other losses ($\mathcal{L}_\mathsf{inter}$ and $\mathcal{L}_\mathsf{\varnothing}$) degrades performance by only $1.7\%$. The relatively small impact of $\mathcal{L}_\mathsf{inter}$ highlights the importance of having many samples that capture interactions between multiple people, rather than samples with just one detection and referred person.

We also report the performance obtained by training our full model 
from scratch, without UNITER's pretrained weights. This leads to a large drop in performance ($>13\%$). 


\subsection{Analysis of results}

We analyze the performance of our model over different test subsets to better understand what the model is learning. 
We observe that our model is more robust to a larger number of faces compared to L$\rightarrow$R (Largest). For instance, in the case of only one referred person in an image, our model retains high performance over an increasing number of faces, while the heuristic drops by almost $20\%$ (from an accuracy of $84.5\%$ for two detected people down to an accuracy of $67.6\%$ for four or more detected people). We further demonstrate this breakdown in the supplemental material.
Another subset we consider is an \emph{interactive} subset of test samples (\ie those with at least two detections and referred people and a verb in their caption). 
This potentially more challenging subset constitutes nearly one-third of our test set.
Our model's performance drops to $52.1\%$, while the baseline performance drops to $45.0\%$.

We also analyzed whether having multiple mentions of a person's name affects performance. Approximately $3\%$ of referred people in the test set are mentioned multiple times in the caption. For these identities, our model has a modest improvement of $2.1\%$ if provided additional mentions. This illustrates that our model can leverage additional information from co-occurrences in a caption to some extent.
Finally, we also analyze the performance of our method over several categories of identity occupations in the supplemental material, as we observe that these correlate well with different situations captured by our dataset.

\subsection{Limitations}

The complexity of certain interactions, such as in sports games where players compete closely together, poses challenges, not only to our model, but also to the person detector and our method for estimating ground truth links. Figure~\ref{fig:limitations} (left) demonstrates an example of a basketball game where the bodies of players overlap, thus some are not detected. The example on the right illustrates a failure of our model, where the interaction ``blocks'' is not correctly interpreted.

Further, some captions are insufficient to produce meaningful links. For example, in Figure~\ref{fig:limitations} (center), after replacing the names ``Joe Jonas'' and ``Demi Lovato'' with \NAME, it is impossible to tell which performer each corresponds to. Hence our model resorts to a simple left-to-right heuristic.

\begin{figure}
  \jsubfig{\includegraphics[height=2.75cm]{figures/examples/fig7_1.jpg}}{} 
  \hfill
\jsubfig{\includegraphics[height=2.75cm,trim={3cm 0 3.5cm 0},clip]{figures/examples/fig7_2.jpg}}{} 
\hfill
\jsubfig{\includegraphics[height=2.75cm,,trim={0.5cm 0 1cm 0},clip]{figures/examples/fig7_3.jpg}}{} 
\newline
 {\leftskip=0.1pt   \footnotesize{
  \textbf{\underline{Left}}: ``Butler's \colorbox{mediumaquamarine}{\textbf{Andrew Smith}} and Siena's \textbf{Ryan Rossiter} both try to anticipate the rebound, as Butler's \textbf{Shawn Vanzant} closes in from behind.''
  \newline
  \textbf{\underline{Center}}: ``\colorbox{gold}{\textbf{Joe Jonas}} and \colorbox{tomato}{\textbf{Demi Lovato}} performing in the Jonas Brothers Live In Concert.''}
  \newline
  \textbf{\underline{Right}}:``\colorbox{tomato}{\textbf{Markus Heikkinen}} blocks \colorbox{mediumaquamarine}{\textbf{Freddy Guar√≠n}}.''}
  \vspace{3pt}
  \caption{
  \textbf{Examples our model predicted incorrectly}, showing detected named entities in bold and entities linked with image regions in unique colors, corresponding to the boxes on the images. }\label{fig:limitations}
\end{figure}

 