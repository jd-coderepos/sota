

\typeout{IJCAI-18 Instructions for Authors}



\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{ijcai18}

\usepackage{times}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{CJKutf8}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{bigstrut}
\usepackage{url}
\usepackage{multirow}
\usepackage{graphicx}  \usepackage{tabularx}  \urlstyle{same}









\title{Translations as Additional Contexts for Sentence Classification}

\author{Reinald Kim Amplayo$^\dag${,}
  Kyungjae Lee$^\dag${,}
  Jinyeong Yeo$^\ddag$ \and
  Seung-won Hwang$^\dag$ \\
  $^\dag$Yonsei University, Seoul, South Korea \\
  $^\ddag$Pohang University of Science and Technology, Pohang, South Korea \\
  {\{rktamplayo, lkj0509, seungwonh\}@yonsei.ac.kr~	
  jinyeo@postech.edu} \\
}

\begin{document}

\maketitle
\begin{abstract}
In sentence classification tasks, additional contexts, such as the neighboring sentences, may improve the accuracy of the classifier.
However, such contexts are {\bf domain-dependent} and thus cannot be used for another classification task with an inappropriate domain.
In contrast, we propose the use of translated sentences
as {\bf domain-free} context 
that is always available regardless of the domain.
We find that naive feature expansion of translations gains only marginal improvements and may decrease the performance of the classifier, due to possible inaccurate translations thus producing noisy sentence vectors.
To this end, we present 
multiple context fixing attachment (MCFA), a series of modules attached to multiple sentence vectors to fix the noise in the vectors using the other sentence vectors as context. 
We show that our method performs competitively compared to previous
models, achieving best classification performance on multiple data sets.
We are the first to use translations as domain-free contexts for sentence classification.
\end{abstract}

\setlength{\parskip}{0pt}
\setlength{\abovedisplayskip}{0pt}\setlength{\belowdisplayskip}{0pt}\setlength{\abovedisplayshortskip}{0pt}\setlength{\belowdisplayshortskip}{0pt}\setlength{\jot}{0pt}
\setlength{\textfloatsep}{10pt}

\renewcommand{\footnotesize}{\normalsize}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}{\parskip}{-1em}{\normalfont\normalsize\bfseries}}

\section{Introduction}\label{sec:intro}

One of the primary tasks in natural language processing (NLP) is sentence classification, where given a sentence (e.g. a sentence of a review) as input, we are tasked to classify it into one of multiple classes (e.g. into positive or negative). This task is important as it is widely used in almost all subareas of NLP such as sentiment classification for sentiment analysis \cite{Pang2007OpinionMA} and question type classification for question answering \cite{li2002learning}, to name a few. While past methods require feature engineering, recent methods enjoy neural-based methods to automatically encode the sentences into low-dimensional dense vectors \cite{Kim2014ConvolutionalNN,Joulin2017BagOT}. 
Despite the success of these methods, the major challenge in this task is that extracting features from a single sentence limits the performance.



To overcome this limitation, recent works attempted to augment different kinds of features to the sentence, such as the neighboring sentences \cite{lin2015hierarchical} 
and the topics of the sentences \cite{zhao2017topic}.
However, these methods used {\bf domain-dependent} contexts that are only effective when the domain of the task is appropriate.
For one thing,
neighboring sentences 
may not be available in some tasks such as question type classification. Moreover, topics 
inferred using topic models 
may produce less useful topics when the data set is domain-specific such as movie review sentiment classification \cite{mimno2011optimizing}.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{example_trec}
	\caption{PCA visualizations of unaltered sentence vectors on TREC data set, where each language is effective for a specific class,  highlighted using a yellow circle.}
	\label{fig:trecvecs}
\end{figure}


In this paper, 
we propose the usage of translations as compelling and effective \textbf{domain-free} contexts, or contexts that are always available no matter what the task domain is. We observe two opportunities when using translations.
\begin{CJK}{UTF8}{}
	\CJKfamily{mj}


First, each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class.
Figure \ref{fig:trecvecs} contrasts the sentence vectors of
the original English sentences and their Arabic-translated sentences in the question type classification task.
A yellow circle signifies a clear separation of a class.
For example, 
the green class, or the numeric question type, is circled in the Arabic space as it is clearly separated from other classes, while such separation
cannot be observed
in English. Meanwhile, location type questions (in orange)
are better classified in English.


Second, the original 
sentences may include language-specific ambiguity, which may be resolved when presented with its translations.
Consider the example English sentence ``\textit{The movie is terribly amazing}'' for the sentiment classification task. In this case, 
\textit{terribly}
can be used in both 
positive and negative sense, 
thus introduces ambiguity in the sentence. When translated to Korean,
it becomes ``\textit{영화는 대단히 훌륭합니다}'' which means ``\textit{The movie is greatly magnificent}'', removing the ambiguity.
\end{CJK}

The above two observations hold only when translations are supported for (nearly) arbitrary language pairs with sufficiently high quality.
Thankfully, translation services (e.g. Google Translate)
Moreover, recent research on neural machine translation (NMT) \cite{bahdanau2014neural} improved the efficiency
and even enabled 
zero-shot translation \cite{johnson2016google} 
of models
for languages with no parallel data.
This provides an opportunity to leverage on as many languages as possible to any domain, providing a much wider context compared to the limited contexts provided by past studies.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{mrvecs4}
	\caption{PCA visualizations of unaltered sentence vectors (left) and the corresponding MCFA-altered vectors (right)  on the MR data set.
	$d$ is the Mahalanobis distance between the two class clusters.
}
	\label{fig:mrvecs}
\end{figure}

However, despite the maturity of translation,
naively concatenating their vectors to the original sentence vector
may introduce more noise 
than signals.
The unaltered translation space on the left of Figure \ref{fig:mrvecs} shows an example
where translation noises make the two classes indistinguishable.




In this paper, we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations. Suppose there are two translated sentences $a$ and $b$ with slight errors. We posit that $a$ can be used to fix $b$ when $a$ is used as a context of $b$, and vice versa\footnote{Hereon, we mean to ``fix'' as to ``correct, repair, or alter.''}. Revisiting the example above, to fix the vector of the English sentence ``\textit{The movie is terribly amazing}'', we use the Korean translation to move the vector towards the location where the vector ``\textit{The movie is greatly magnificent}'' is.






Based on these observations, we present a neural attention-based multiple context fixing attachment (MCFA). MCFA is a series of modules that uses all the sentence vectors (e.g. Arabic, English, Korean, etc.) as context to fix a sentence vector (e.g. Korean). 
Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class, as shown in Figure~\ref{fig:mrvecs}.
Noises from translation may cause adverse effects to the vector
\textbf{itself} (e.g. when a noisy vector is directly used for the task) and \textbf{relatively} to other vectors (e.g. when a noisy vector is used to fix another noisy vector).
MCFA computes two sentence
usability metrics to control the noise when fixing vectors:
(a) \textbf{self usability} $\rho_i(a)$ weighs the confidence of using sentence $a$ in solving the task.
(b) \textbf{relative usability} $\rho_r(a, b)$ weighs the confidence of using sentence $a$ in fixing sentence $b$.

Listed below are the three main strengths of the MCFA attachment.
(1) 
MCFA is attached
after encoding the sentence, 
which makes it widely adaptable to other models.
(2) 
MCFA is extensible and improves the accuracy as the number of translated sentences increases.
(3) 
MCFA 
moves the vectors inside the same space, thus preserves the meaning of vector dimensions.
Results show that a convolutional neural network (CNN) attached with MCFA significantly improves the classification performance of CNN, achieving state of the art performance over multiple data sets. 


\section{Preliminaries}



\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{attention}
        \caption{Self and relative usability modules}
        \label{fig:sga}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fixer_2}
        \caption{Vector fixing module}
        \label{fig:vf}
    \end{subfigure}
    \caption{Full architecture of the MCFA attachment. An arrow marked with a variable is a matrix multiplication of the vector and the variable. An arrow without a variable simply carries the previous element to the next element.}
    \label{fig:arch}
\end{figure*}

\subsection{Problem: Translated Sentences as Context}

In this paper, the ultimate task that we solve is the sentence classification task where given a sentence and a list of classes, one is task to classify which class (e.g. positive or negative sentiment) among the list of classes does the sentence belong. However, the main challenge that we tackle is the task on how to utilize translated sentences as additional context in order to improve the performance of the classifier.
Specifically, the problem states: given the original sentence $s$, the goal is to use $t_1, t_2, ..., t_n$, or sentences in other languages which are translated from $s$, as additional context.

\paragraph{Base Model: Convolutional Neural Network.}

The base model used is the convolutional neural network (CNN) for sentences \cite{Kim2014ConvolutionalNN}. It is a simple variation of the original CNN for texts \cite{collobert2011natural} to be used on sentences. Let $\mathbf{x}_i \in \mathbb{R}^d$ be the $d$-dimensional word vector of the $i$-th word in a sentence of length $n$. A convolution operation involves applying a filter matrix $\mathbf{W} \in \mathbb{R}^{h \times d}$ to a window of $h$ words and producing a new feature vector $c_i$ using the equation $c_i = f([\mathbf{x}_i; ...; \mathbf{x}_{i+h-1}]^\top \mathbf{W}+b)$, where $b$ is a bias vector and $f(.)$ is a non-linear function.
By doing this on all possible windows of words we produce a feature map $\mathbf{c} = [c_1, c_2, ...]$. We then apply a max-over-time pooling operation \cite{collobert2011natural} over the feature map and take the maximum value as the feature vector of the filter. We do this on all feature vectors and concatenate all the feature vectors to obtain the final feature vector $\mathbf{v}$. We can then use this vector as input features to train a classifier such as logistic regression.
We use CNN to create sentence vectors for all sentences $s, t_1, t_2, ..., t_n$. From here on, we refer to these vectors as $\mathbf{v}_s, \mathbf{v}_{t_1}, \mathbf{v}_{t_2}, ..., \mathbf{v}_{t_n}$, respectively. We refer to them collectively as $\mathbb{V}$.

\paragraph{Baseline 1: Naive Concatenation.}

A simple method in order to use the translated sentences as additional context is to naively concatenate their vectors with the vector of the original sentence. That is, we create a wide vector $\mathbf{\hat{v}} = [\mathbf{v}_s; \mathbf{v}_{t_1}; ...; \mathbf{v}_{t_n}]$, and use this as the input feature vector of the sentence to the 
classifier.
This method works fine if the translated sentences are translated properly. However, sentences translated using machine translation models usually contain incorrect translation. In effect, this method will have adverse effects on the overall performance of the classifier. This will especially be very evident if the number of additional sentences increases.

\paragraph{Baseline 2: L2 Regularization.}

In order to alleviate the problems above, we can use L2 regularization to automatically select useful features by weakening the appropriate weights. The main problem of this method occurs when almost all of the weights coming from the vectors of the translated sentence are weakened. This leads to making the additional context vectors useless and to having a similar performance when there are no additional context. Ultimately, this method does not make use of the full potential of the additional context.

\section{Model}

To solve the problems of the baselines discussed above, we introduce an attention-based neural multiple context fixing attachment (MCFA)\footnote{The code we use in this paper is publicly shared: \url{https://github.com/rktamplayo/MCFA}}, a series of modules attached to the sentence vectors $\mathbb{V}$. MCFA attachment is used to fix the sentence vectors, by slightly modifying the per-dimension values of the vector, before concatenating them into the final feature vector. The sentence vectors are altered using other sentence vectors as context (e.g. $\mathbf{v}_{t_1}$ is altered using $\mathbf{v}_s, \mathbf{v}_{t_2}, ..., \mathbf{v}_{t_n}$). This results to moving the vectors in the same vector space.
The full architecture is 
shown in Figure \ref{fig:arch}.


\subsection{Self Usability Module}

To fix a source sentence vector\footnote{Hereon, we say that $\mathbf{v}_k$ is a \textit{source sentence vector} if $\mathbf{v}_k$ is the current vector to be altered.}, we use the other sentence vectors as guide to know which dimensions to fix and to what extent do we need to fix them. However, other vectors might also contain errors which may reflect to the fixing of the source sentence vector. In order to cope with this, we introduce self usability modules. A self usability module contains the \textbf{self usability} of the vector
$\rho_i(a)$, which measures how confident sentence $a$ is for the task at hand. For example, an ambiguous sentence (e.g. ``\textit{The movie is terribly amazing}'') may receive a low self usability, while a clear
and definite 
sentence (e.g. ``\textit{The movie is very good}'') may receive a high self usability.


Mathematically, we calculate the self usability of the vector $\mathbf{v}_i$ of sentence $i$, denoted as $\rho_{i}(\mathbf{v}_i)$, using the equation $\rho_i(\mathbf{v}_i) = \sigma(\mathbf{v}_i^\top \mathbf{T}_i)$, where $\mathbf{T}_i \in \mathbb{R}^{d \times 1}$ is a matrix to be learned. The produced value is a single real number from 0 to 1.
We pre-calculate the self usability of all sentence vectors $\mathbf{v}_i \in \mathbb{V}$. These are used in the next module, the relative usability module.

\subsection{Relative Usability Module}

\textbf{Relative usability} $\rho_r(a, b)$ measures how useful $a$ can be when fixing $b$, relative to other sentences. There are two main differences between $\rho_i(a)$ and $\rho_r(a, b)$. First, $\rho_i(a)$ is calculated before $a$ knows about $b$ while $\rho_r(a, b)$ is calculated when $a$ knows about $b$. Second, $\rho_r(a, b)$ can be low even though $\rho_i(a)$ is not. This means that $a$ is not able to help in fixing the wrong information in $b$.
Here, we extend the additive attention module \cite{bahdanau2014neural} and use it as a method to calculate the relative usability of two sentences of different languages. To better visualize the original attention mechanism, we present the equations below.
\begin{align}
    \label{eq:addattscore}
    e_i &= u^\top tanh( s^\top \mathbf{W}+t_i^\top \mathbf{U} ) \\
    \alpha_i &= \frac{exp(e_i)}{\sum_{j \in T} exp(e_j)}
\end{align}
One major challenge in using the attention mechanism in our problem is that the sentence vectors do not belong to the same vector space. Moreover, one characteristic of our problem is that the sentence vectors can be both a source and a context vector (e.g. $\mathbf{v}_s$ can be both $s$ and $t_i$ in Equation \ref{eq:addattscore}). Because of these, we cannot directly use the additive attention module. We extend the module such that (1) each sentence vector $\mathbf{v}_k$ has its own projection matrix $\mathbf{X}_k \in \mathbb{R}^{d \times d}$, and (2) each projection matrix $\mathbf{X}_k$ can be used as projection matrix of both the source (e.g. when sentence $k$ is the current source) and the context vectors. Finally, we incorporate the self usability function $\rho_i(\mathbf{v}_k)$ to reflect the self usability of a sentence. Ultimately, the relative usability denoted as $\rho_r(\mathbf{v}_i, \mathbf{v}_j)$ is calculated using the equations below, where $\times$ is the multiplication of a vector and a scalar through broadcasting.
\begin{align}
    e(\mathbf{v}_i, \mathbf{v}_j) &= x^\top tanh( \mathbf{v}_i^\top \mathbf{X}_i+\mathbf{v}_j^\top \mathbf{X}_j \times \rho_i(\mathbf{v}_j) ) \\
    \rho_r(\mathbf{v}_i, \mathbf{v}_j) &= \frac{exp(e(\mathbf{v}_i, \mathbf{v}_j))}{\sum_{\mathbf{v}_k \in \mathbb{V}} exp(e(\mathbf{v}_i, \mathbf{v}_k))}
\end{align}
\subsection{Vector Fixing Module}

The vector fixing module applies the attention weights to the sentence vectors and creates an integrated context vector. We then use this vector alongside with the source sentence vector to create a weighted gate vector. The weighted gate vector is used to determine to what extent should a dimension of the source sentence vector be altered.

The common way to apply the attention weights to the context vectors and create an integrated context vector $c_i$ is to directly do weighted sum of all the context vectors.
However, this is not possible because the context vectors are not on the same space. Thus, we use a projection matrix $\mathbf{U}_k \in \mathbb{R}^{d \times d}$ to linearly project the sentence vector $\mathbf{v}_k$ to transform the sentence vectors into a common vector space. The integrated context vector $c_i$ is then calculated as
$c_i = \sum_{\mathbf{v}_k \in \mathbb{V}} \rho_r(\mathbf{v}_i, \mathbf{v}_k) \mathbf{v}_k^\top \mathbf{U}_k$.

Finally, we construct a weighted gate vector $w_k$ and use it to fix the source sentence vectors using the equations below, where $\mathbf{V}_k \in \mathbb{R}^{2d \times d}$ is a trainable parameter and $\otimes$ is the element-wise multiplication procedure. The weighted gate vector is a vector of real numbers between 0 and 1 to modify the intensity of per-dimension values of the sentence vector.
This causes the vector to move in the same vector space towards the correct direction.
\begin{equation}
    w_k = \sigma([\mathbf{v_k}; c_k]^\top \mathbf{V}_k)
\end{equation}
\begin{equation}
    \hat{\mathbf{v}}_k = \mathbf{v}_k \otimes w_k
\end{equation}
An alternative approach to do vector correction is using a residual-style correction, where instead of multiplying a gate vector, a residual vector \cite{he2016deep} is added to the original vector. However, this approach makes the correction not interpretable; it is hard to explain what does adding a value to a specific dimension mean.
One major advantage of MCFA is that the corrections in the vectors are interpretable; the weights in the gate vector correspond to the importance of the per-dimension features of the vector.
The altered vectors $\hat{\mathbf{v_s}}, ..., \hat{\mathbf{v_{t_n}}}$ are then concatenated and fed directly as an input vector to the logistic regression classifier for training.

\section{Experiments}



\subsection{Experimental Setting}

We test our model on four different data sets as listed below and summarized
in Table \ref{tab:data}.
(a) \textbf{MR}\footnote{\url{https://www.cs.cornell.edu/people/pabo/movie-review-data/}} \cite{pang2005seeing}: 
Movie reviews data where the task is to classify whether the review sentence has positive or negative sentiment.
(b) \textbf{SUBJ} \cite{pang2004sentimental}: Subjectivity data where the task is to classify whether the sentence is subjective or objective.
(c) \textbf{CR}\footnote{\url{http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html}} \cite{hu2004mining}: Customer reviews where
The task is to classify whether the review sentence is positive or negative.
(d) \textbf{TREC}\footnote{\url{http://cogcomp.cs.illinois.edu/Data/QA/QC/}} \cite{li2002learning}: TREC question data set 
the task is to classify the type of question.


\begin{table}[!t]
	\scriptsize
  \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Data set} & $c$ & $|w|$ & $M$  & \textit{Test} \\
    \hline
    MR    & 2     & 20    & 10662 & CV \\
    SUBJ  & 2     & 19    & 10000 & CV \\
    CR    & 2     & 23    & 3775  & CV \\
    TREC  & 6     & 10    & 5952  & 500 \\
    \hline
    \end{tabular}\caption{Statistics of the four data sets used in this paper. $c$: number of target classes. $|w|$: average number of words. $M$: number of data instances. \textit{Test}: size of the test data, if available. If not, we use 10-fold cross validation (marked as CV) with random split.}
  \label{tab:data}\end{table}

All our data sets are in English. For the additional contexts, we use ten other languages, selected based on their diversity and their performance on prior experiments: Arabic, Finnish, French, Italian, Korean, Mongolian, Norwegian, Polish, Russian, and Ukranian. We translate the data sets using Google Translate. Tokenization is done using the polyglot library\footnote{\url{https://pypi.python.org/pypi/polyglot}}. We experiment on using only one additional context ($N=1$) and using all ten languages at once ($N=10$). For $N=1$, we only show the accuracy of the best classifier for conciseness.

For our CNN, we use rectified linear units and three filters with different window sizes $h=3,4,5$ with $100$ feature maps each, following \cite{Kim2014ConvolutionalNN}. For the final sentence vector, we concatenate the feature maps to get a 300-dimension vector. We use dropout \cite{srivastava2014dropout} on all non-linear connections with a dropout rate of 0.5. We also use an $l_2$ constraint of 3, following
\cite{Kim2014ConvolutionalNN} for accurate comparisons. We use FastText pre-trained vectors\footnote{\url{https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md}} \cite{bojanowski2016enriching} for all our data sets and their corresponding additional context.
During training, we use mini-batch size of 50. Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule. We perform early stopping using a random $10\%$ of the training set as the development set.

We present several competing models, listed below to compare the performance of our model. (A) Aside from the base model (\textbf{CNN}) \cite{Kim2014ConvolutionalNN}, we use Dependency-based CNN (\textbf{Dep-CNN}) \cite{ma2015dependency}, which parses the sentences first and does convolution on ancestor paths and Dependency-sensitivity CNN (\textbf{DSCNN}) \cite{zhang2016dependency}, which uses LSTM to capture dependency information within each sentence; (B) \textbf{AdaSent} \cite{zhao2015self} adopts a hierarchical structure, where consecutive levels are connected through gated recursive composition of adjacent segments, and feeds the hierarchy as a multi-scale representation through a gating network;
(C) Topic-aware Convolutional Neural Network (\textbf{TopCNN}) \cite{zhao2017topic} uses topics as additional contexts and changes the CNN architecture. TopCNN uses two types of topics: word-specific topic and sentence-specific topic; and
(D) \textbf{CNN+B1} and \textbf{CNN+B2} are the two baselines presented in this paper. 

We do not show results from RNN models because they were shown to be less effective in sentence classification in our prior experiments.
For models with additional context, we further use an ensemble classification model using a commonly used method by averaging the class probability scores generated by the multiple variants (in our model's case, $N=1$ and $N=10$ models), following \cite{zhao2017topic}.

\begin{table*}[htbp]
	\scriptsize
	\centering
	\begin{tabular}{|c|ccc|ccc|ccc|ccc|}
		\hline
		\textbf{Model} & \multicolumn{3}{c|}{\textbf{MR}} & \multicolumn{3}{c|}{\textbf{SUBJ}} & \multicolumn{3}{c|}{\textbf{CR}} & \multicolumn{3}{c|}{\textbf{TREC}} \\
		\hline
		CNN   & \multicolumn{3}{c|}{81.5} & \multicolumn{3}{c|}{93.4} & \multicolumn{3}{c|}{85.0} & \multicolumn{3}{c|}{93.6} \\
		Dep-CNN & \multicolumn{3}{c|}{81.9} & \multicolumn{3}{c|}{-} & \multicolumn{3}{c|}{-} & \multicolumn{3}{c|}{95.4} \\
		DSCNN & \multicolumn{3}{c|}{82.2} & \multicolumn{3}{c|}{93.2} & \multicolumn{3}{c|}{-} & \multicolumn{3}{c|}{\textbf{95.6}} \\
		\hline
AdaSent & \multicolumn{3}{c|}{\textbf{83.1}} & \multicolumn{3}{c|}{\underline{\textbf{95.5}}} & \multicolumn{3}{c|}{86.3} & \multicolumn{3}{c|}{92.4} \\
		\hline
		\hline
		\textbf{C = Topic} & \multicolumn{1}{c}{word} & \multicolumn{1}{c}{sent} & \multicolumn{1}{c|}{ens} & \multicolumn{1}{c}{word} & \multicolumn{1}{c}{sent} & \multicolumn{1}{c|}{ens} & \multicolumn{1}{c}{word} & \multicolumn{1}{c}{sent} & \multicolumn{1}{c|}{ens} & \multicolumn{1}{c}{word} & \multicolumn{1}{c}{sent} & \multicolumn{1}{c|}{ens} \\
		\hline
		TopCNN & \makecell{81.7\\(+0.2)} & \textcolor[rgb]{ 1,  0,  0}{\makecell{81.3\\(-0.2)}} & \makecell{83.0\\(+1.5)} & \textcolor[rgb]{ 1,  0,  0}{\makecell{93.4\\(+0.0)}} & \textcolor[rgb]{ 1,  0,  0}{\makecell{93.4\\(+0.0)}} & \makecell{95.0\\(+1.6)} & \textcolor[rgb]{ 1,  0,  0}{\makecell{84.9\\(-0.1)}} & \textcolor[rgb]{ 1,  0,  0}{\makecell{84.8\\(-0.2)}} & \textbf{\makecell{86.4\\(+1.4)}} & \textcolor[rgb]{ 1,  0,  0}{\makecell{92.5\\(-1.1)}} & \textcolor[rgb]{ 1,  0,  0}{\makecell{92.0\\(-1.6)}} & \makecell{94.0\\(+0.4)} \\
		\hline
		\hline
		\textbf{C = Trans} & \multicolumn{1}{c}{N=1} & \multicolumn{1}{c}{N=10} & \multicolumn{1}{c|}{ens} & \multicolumn{1}{c}{N=1} & \multicolumn{1}{c}{N=10} & \multicolumn{1}{c|}{ens} & \multicolumn{1}{c}{N=1} & \multicolumn{1}{c}{N=10} & \multicolumn{1}{c|}{ens} & \multicolumn{1}{c}{N=1} & \multicolumn{1}{c}{N=10} & \multicolumn{1}{c|}{ens} \\
		\hline
		CNN+B1 & \makecell{81.9\\(+0.4)} & \textcolor[rgb]{ 1,  0,  0}{\makecell{81.4\\(-0.1)}} & \makecell{82.6\\(+1.1)} & \makecell{94.6\\(+1.2)} & \makecell{93.8\\(+0.4)} & \makecell{94.9\\(+1.5)} & \makecell{86.2\\(+1.2)} & \makecell{85.9\\(+0.9)} & \makecell{86.7\\(+1.7)} & \makecell{95.4\\(+1.8)} & \makecell{95.0\\(+1.4)} & \makecell{96.4\\(+3.0)} \\
		CNN+B2 & \makecell{82.1\\(+0.6)} & \makecell{82.1\\(+0.6)} & \makecell{82.2\\(+0.7)} & \makecell{94.6\\(+1.2)} & \makecell{94.0\\(+0.6)} & \makecell{94.8\\(+1.4)} & \makecell{86.1\\(+1.1)} & \makecell{86.3\\(+1.3)} & \makecell{86.6\\(+1.6)} & \makecell{95.4\\(+1.8)} & \makecell{95.2\\(+1.6)} & \makecell{96.4\\(+3.0)} \\
		CNN+MCFA & \makecell{82.3\\(+0.8)} & \makecell{82.7\\(+1.2)} & \textbf{\makecell{\underline{83.2}\\\underline{(+1.7)}}} & \makecell{94.7\\(+1.3)} & \makecell{94.8\\(+1.4)} & \textbf{\makecell{95.2\\(+1.8)}} & \makecell{87.6\\(+2.6)} & \makecell{88.6\\(+3.6)} & \textbf{\makecell{\underline{89.4}\\\underline{(+4.4)}}} & \makecell{95.4\\(+1.8)} & \makecell{96.0\\(+2.4)} & \textbf{\makecell{\underline{96.8}\\\underline{(+3.4)}}} \\
		\hline
	\end{tabular}\caption{Classification accuracies of competing models. \textbf{C} refers to the additional context, $N$ refers to the number of translations. In TopCNN, word refers to using word-specific topic while sentence refers to using sentence-specific topic. 
Accuracies colored \textcolor[rgb]{1,0,0}{red} are accuracies that perform worse than CNN. Previous state of the art results and the results of our best model are \textbf{bold-faced}. The winning result is \underline{underlined}. The number inside the parenthesis indicates the increase from the base model, CNN.}
	\label{tab:result}\end{table*}

\subsection{Results and Discussion}

We report the classification accuracy of the competing models in Table \ref{tab:result}. We show that CNN+MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set. When $N=1$, MCFA increases the performance of a normal CNN from $85.0$ to $87.6$, beating the current state of the art on the CR data set. When $N=10$, MCFA additionally beats the state of the art on the TREC data set. Finally, our ensemble classifier additionally outperforms all competing models on the MR data set. We emphasize that we only use the basic CNN as our sentence encoder for our experiments, yet still achieve state of the art performance on most data sets. Hence, MCFA is successful in effectively using translations as additional context to improve the performance of the classifier.

We compare our model (CNN+MCFA) and the baselines discussed above (CNN+B1, CNN+B2). On all settings, our model outperforms the baselines. When $N=10$, the performance of our model increases over the performance when $N=1$, however the performance of CNN+B1 decreases when compared to the performance when $N=1$. We also show the accuracies of the worst classifiers when $N=1$ in Table \ref{tab:minresult}. On all data sets except SUBJ, the accuracy of CNN+B1 decreases from the base CNN accuracy, while the accuracy of our model always improves from the base CNN accuracy.
This is resolved by CNN+B2 by applying L2 regularization, however the increase in performance is marginal. 

\begin{table}[t]
	\scriptsize
  \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Model} & {\textbf{MR}} & \textbf{SUBJ} & \textbf{CR} & {\textbf{TREC}} \\
    \hline
    CNN   & 81.5  & 93.4  & 85.0    & 93.6 \\
    \hline
    CNN+B1 & \textcolor[rgb]{ 1,  0,  0}{81.4} & 94.2  & \textcolor[rgb]{ 1,  0,  0}{83.8} & \textcolor[rgb]{ 1,  0,  0}{93.0} \\
    CNN+B2 & 81.7  & 94.2  & \textcolor[rgb]{ 1,  0,  0}{84.0} & \textcolor[rgb]{ 1,  0,  0}{93.2} \\
    CNN+MCFA & 81.8  & 94.4  & 85.8  & 94.2 \\
    \hline
    \end{tabular}\caption{Accuracies of the worst CNN+translation classifiers when $N=1$. Accuracies less than CNN accuracies are highlighted in \textcolor[rgb]{1,0,0}{red}.}
  \label{tab:minresult}\end{table}

We also compare two different kinds of additional context: topics (TopCNN) and translations (CNN+B1, CNN+B2, CNN+MCFA). Overall, we conclude that translations are better additional contexts than topics. When using a single context (i.e. TopCNN$_\text{word}$, TopCNN$_\text{sent}$, and our models when $N=1$), translations always outperform topics even when using the baseline methods. Using topics as additional context also decreases the performance of the CNN classifier on most data sets, giving an adverse effect to the CNN classifier.



\section{Model Interpretation}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.4\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{attention1}
		\caption{Example where English attention weight is larger}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{attention2}
		\caption{Example where Korean attention weight is larger}
	\end{subfigure}
	\caption{Attention weights of example Korean sentences from the MR data set. The red color fill represents the attention weights given to each sentence. The darker the fill, the larger the attention weight.}
	\label{fig:attention}
\end{figure}

\begin{CJK}{UTF8}{}
 \CJKfamily{mj}
\begin{table}[!ht]
	\scriptsize
    \centering
    \begin{subtable}{0.47\textwidth}
        \centering
        \begin{tabularx}{\textwidth}{X}
            Original sentence: \\
            \textit{skip this turd and pick your nose instead because you're sure to get more out of the latter experience .} \\ \hline
            Korean translation: \\
            \textit{\textcolor[rgb]{1,0,0}{후자의 경험에서 더 많은 것을 얻으려면} 이 웅덩이를 건너 뛰고 \textcolor[rgb]{1,0,0}{코를 골라야합니다} .} \\ \hline
            Human re-translation: \\ 
            \textit{In order to get more from the latter experience , you need to skip this puddle and choose your nose .} \\ \hline
            \textbf{Self Usability: 0.3958} \\
        \end{tabularx}
        \caption{Low self usability example}
    \end{subtable}
    \begin{subtable}{0.47\textwidth}
        \centering
        \begin{tabularx}{\textwidth}{X}
            Original sentence: \\
            \textit{michael moore's latest documentary about america's thirst for violence is his best film yet . . .} \\ \hline
            Korean translation: \\
            \textit{마이클 무어 ( Michael Moore ) 의 최근 미국 다큐멘터리 \textcolor[rgb]{1,0,0}{`` 폭력 장면 ''} 은 그의 최고의 영화 다 . . .} \\ \hline
            Human re-translation: \\
            \textit{Michael Moore's latest American documentary `` Violent Scene '' is his best film yet . . .} \\ \hline
            \textbf{Self Usability: 1.0000} \\
        \end{tabularx}
        \caption{High self usability example}
    \end{subtable}
    \caption{Two examples of self usability of Korean sentences from the MR data set. Texts colored in \textcolor[rgb]{1,0,0}{red} are mistranslated texts.}
    \label{tab:usability}
\end{table}
\end{CJK}





We first provide examples shown in Table \ref{tab:usability} on how the self usability module determines the score of sentences. In the first example, it is hard to classify whether the translated sentence is positive or negative, thus it is given a low self usability score. In the second example, although the sentence contains mistranslations, these are minimal and may actually help the classifier by telling it that \textit{thirst for violence} is not a negative phrase. Thus, it is given a high self usability score.



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{comb_mr4}
\caption{PCA visualization of unaltered (left) and altered (right) vectors of the MR data set. $d$ is the Mahalanobis distance between two class clusters.}
    \label{fig:mrvecsbefaf}
\end{figure}

\begin{table}[t]
	\scriptsize
  \centering
  \def\arraystretch{0.7}
    \begin{tabular}{|m{0.06\textwidth}|m{0.36\textwidth}|} \hline
    \textbf{Sentence} & may take its sweet time to get wherever it's going, but if you have the patience for it, you won't feel like it's wasted yours. \\\hline
    \textbf{NN \newline (Unaltered)} & you know that ten bucks you'd spend on a ticket? just send it to cranky. we don't get paid enough to sit through crap like this. \\\hline
    \textbf{NN \newline (altered)} & what might have been readily dismissed as the tiresome rant of an aging filmmaker still thumbing his nose at convention takes a surprising, subtle turn at the midway point. \\\hline
    \end{tabular}\vspace*{0.1cm}
    \begin{tabular}{|m{0.06\textwidth}|m{0.36\textwidth}|} \hline
    \textbf{Sentence} & every nanosecond of the new guy reminds 
that you could be doing something else 
more pleasurable. 
like scrubbing the toilet. 
emptying rat traps. or doing last year's taxes with your ex-wife. \\\hline
    \textbf{NN \newline (Unaltered)} & in the new release of cinema paradiso, the tale has turned from sweet to bittersweet, and when the tears come during that final, beautiful scene, they finally feel absolutely earned. \\\hline
    \textbf{NN \newline (altered)} & after 
scenes of 
nonsense, you'll be wistful for the testosterone-charged wizardry of jerry bruckheimer productions, especially because half past dead is like the rock on 
walmart budget. \\\hline
    \end{tabular}\caption{Two example sentences, from English (first) and Korean (second) vector spaces, and their nearest neighbors (NN) on both the unaltered and altered vector spaces. We only show the original English sentences for the Korean example for conciseness.}
  \label{tab:vecfixexample}\end{table}

Figure \ref{fig:attention} shows two data instance examples where we show the attention weights given to the other contexts when fixing a Korean sentence. The larger the attention weight is, the more the context is used to fix the Korean sentence. In the first example, the Korean sentence contains translation errors; especially, the words \textit{bore} and \textit{climactic setpiece} were not translated and were only spelled using the Korean alphabet. In this example, the English attention weight is larger than the Korean attention weight. In the second example, the Korean sentence correctly translates all parts of the English sentence, except for the phrase \textit{as it does in trouble}. However, this phrase is not necessary to classify the sentence correctly, and may induce possible vagueness because of the word \textit{trouble}. Thus, the Korean attention weight is larger.



Figure \ref{fig:mrvecsbefaf} shows the PCA visualization of the unaltered and the altered vectors of four different languages. In the first 
example, the unaltered 
sentence vectors are mostly in the middle of the vector space, making it hard to draw a boundary between
the two
examples. After the fixing, the boundary is much clearer.
\begin{comment}
In the second example, it is impossible to distinguish the classes as
the vectors 
are overlapping with each other. After the fix,
the blue-class and red-class vectors moved to the opposite sides,
making it easier to tell and differentiate the two classes. 
\end{comment}
We also show the English sentence vectors in the second example. Even without fixing the unaltered English sentence vectors, it is easy to distinguish both classes. After the fix, the sentence vectors in the middle of the space are moved,
making the distinction more obvious and clearer. We also provide quantitative evidence by showing that the Mahalanobis distance between the two classes in the altered vectors are significantly farther than that of the unaltered vectors.

We also show two examples sentences from English and Korean vector spaces and their corresponding nearest neighbors on both the unaltered and altered vector spaces in Table \ref{tab:vecfixexample}. In the first example, the unaltered vector focuses on the meaning of \textit{``wasted yours''} in the sentence, which puts it near sentences regarding wasted time or money. After fixing, the sentence vector focuses its meaning on the slow yet worth-the-wait pace of the movie, thus moving it closer to the correct vectors. In the second example, all three sentences have highly descriptive tones, however, the nearest neighbor on the altered space is hyperbolically negative, comparing the movie to a description unrelated to the movie itself.



\begin{comment}
\begin{table}[t]
	\scriptsize
	\centering
	\begin{tabular}{|c|cccc|}
		\hline
		Rank  & MR    & CR    & SUBJ  & TREC \\
		\hline
		1     & \textcolor[rgb]{1,0,0}{\textbf{FI}} & UK & IT    & \textcolor[rgb]{0,0,1}{\textbf{FR}} \\
		2     & UK & \textcolor[rgb]{1,0,0}{\textbf{FI}} & PL    & RU \\
		3     & IT    & KO    & AR    & MN \\
		4     & \textcolor[rgb]{0,0.5,0}{\textbf{NO}}    & PL    & \textcolor[rgb]{1,0,0}{\textbf{FI}} & PL \\
		5     & AR    & \textcolor[rgb]{0,0.5,0}{\textbf{NO}}    & \textcolor[rgb]{0,0,1}{\textbf{FR}} & \textcolor[rgb]{0,0.5,0}{\textbf{NO}} \\
		6     & RU & IT    & MN    & AR \\
		7     & KO    & RU & \textcolor[rgb]{0,0.5,0}{\textbf{NO}}    & UK \\
		8     & \textcolor[rgb]{0,0,1}{\textbf{FR}} & \textcolor[rgb]{0,0,1}{\textbf{FR}} & RU & KO \\
		9     & MN    & AR    & UK & \textcolor[rgb]{1,0,0}{\textbf{FI}} \\
		10    & PL    & MN    & KO    & IT \\
		\hline
	\end{tabular}\caption{Ranking of the accuracies of a language when used as an additional context for CNN+MCFA when $N=1$ on multiple datasets.}
	\label{tab:ranking}\end{table}\end{comment}

\section{Related Work}



One way to improve the performance of a sentence classifier is to introduce new context. Common and obvious kinds of context are the neighboring sentences of the sentence \cite{lin2015hierarchical}, and the document where the sentence belongs \cite{huang2012improving}. 
Topics of the words in the sentence induced by a topic model
were also used as contexts \cite{zhao2017topic}.
In this paper, we introduce yet another type of additional context, sentence translations, which to the best of our knowledge have not been used previously.



Sentence encoders trained from neural machine translation (NMT) systems were also used for transfer learning \cite{hill2016learning}. 
\cite{hill2017representational} 
demonstrated that altered-length sentence vectors from NMT encoders outperform sentence vectors from monolingual encoders on semantic similarity tasks. Recent work used representation of each word in the sentence to create a sentence representation suitable for multiple NLP tasks \cite{mccann2017learned}.
Our work shares the commonality of using NMT for another task, but instead of using NMT to encode our sentences, we 
use it to translate the sentences into new contexts.




Increasing the number of data instances of the training set has also been explored to improve the performance of a classifier. 
Recent methods include the usage of thesaurus \cite{zhang2015character}, paraphrases \cite{fu2014improving}, among others.
These simple variation techniques are preferred because they are found to be very effective despite their simplicity.
Our work similarly augments training data, not by adding data instances (vertical augmentation), but rather by adding more context (horizontal augmentation). Though the paraphrase of $p$ can be alternatively used as an augmented context, this could not leverage the added semantics coming from another language, as discussed in Section~\ref{sec:intro}. 



\section{Conclusion}

This paper investigates the use of translations as better additional contexts for sentence classification. To answer the problem on mistranslations, we propose 
multiple context fixing attachment (MCFA) to fix the context vectors using other context vectors. We show that our method improves the classification performance and achieves state-of-the-art performance on multiple data sets. In our future work, we plan to use and extend our model to other complex NLP tasks.


\section*{Acknowledgements}

This work was supported by Microsoft Research Asia and the ICT R\&D program of MSIT/IITP.
[2017-0-01778, Development of Explainable Human-level Deep Machine Learning Inference Framework]

\bibliographystyle{named}
\bibliography{ijcai18}

\end{document}
