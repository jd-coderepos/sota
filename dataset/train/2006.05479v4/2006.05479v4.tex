\documentclass[10pt]{article} \usepackage[accepted]{tmlr}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{aas_macros}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{ bbold }
\usepackage{booktabs}
\usepackage{ dsfont }
\graphicspath{ {./figures/} }
\usepackage{wrapfig}
\DeclareMathOperator{\EE}{\mathbb{E}}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator*{\argmaxx}{arg\,max}
\DeclareMathOperator*{\argminn}{arg\,min}
\DeclareMathOperator{\tr}{tr}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\def\bi#1{\hbox{\boldmath{$#1$}}}
\usepackage{xcolor}
\newcommand{\rev}[1]{{\color{black}#1}}
\newcommand{\VB}[1]{{\color{black}VB: #1}}


\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}







\title{Probabilistic Autoencoder}

\author{\name Vanessa B\"ohm \email vboehm@berkeley.edu\\
        \addr Berkeley Center for Cosmological Physics\\
        Department of Physics\\
        University of California\\
        Berkeley, CA, USA\\
        Lawrence Berkeley National Laboratory\\
        \AND
        \name Uro\v s Seljak \email useljak@berkley.edu\\
        \addr Berkeley Center for Cosmological Physics\\
        Department of Physics\\
University of California\\
        Berkeley, California, USA\\
        Lawrence Berkeley National Laboratory}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{09}
\def\year{2022}
\def\openreview{\url{https://openreview.net/forum?id=AEoYjvjKVA}}


\begin{document}



\maketitle

\begin{abstract}Principal Component Analysis (PCA) minimizes the reconstruction error given a class of linear models of fixed component dimensionality. Probabilistic PCA adds a probabilistic structure by learning the probability distribution of the PCA latent space weights, thus creating a generative model. Autoencoders (AE) minimize the reconstruction error in a class of nonlinear models of fixed latent space dimensionality and outperform PCA at fixed dimensionality. Here, we introduce the Probabilistic Autoencoder (PAE) that learns the probability distribution of the AE latent space weights using a normalizing flow (NF). The PAE is fast and easy to train and achieves small reconstruction errors, high sample quality, and good performance in downstream tasks. We compare the PAE to Variational AE (VAE), showing that the PAE trains faster, reaches a lower reconstruction error, and produces good sample quality without requiring special tuning parameters or training procedures. 
We further demonstrate that the PAE is a powerful model for performing the downstream tasks of probabilistic image reconstruction in the context of Bayesian inference of inverse problems for inpainting and denoising applications. Finally, we identify latent space density from NF as a promising outlier detection metric.
\end{abstract}

\section{Introduction}
Deep generative models are powerful machine learning models that can learn complex, high-dimensional data likelihoods and generate samples from them. Because of their probabilistic formulation, generative models are becoming an indispensable tool for scientific data analysis in a range of domains including particle physics~\citep{Paganini2018,LHC_competition} and cosmology~\citep{Thorne2021,Reiman2020}. 

Variational Autoencoders (VAEs)~\citep{ KingmaWelling13,RezendeMW14} are among the most popular generative models. VAEs project the data to a lower dimensional latent space and reformulate the data likelihood estimation as a variational inference problem. Their training objective is the Evidence Lower BOund (ELBO), which approximates the true data likelihood with a variational ansatz from below. VAEs can be built with expressive architectures, enjoy the benefits of regularization through data compression and have a firm theoretical foundation. Different to generative adversarial networks~\citep{GANs}, another popular class of generative models, VAEs provide an estimator for the data likelihood and a posterior distribution for the latent variables. 

Despite their popularity, variational autoencoders have well known practical limitations. Successful VAE training requires to find a delicate balance between the two contributing terms to the ELBO: The distortion term, which encourages high quality reconstructions, and the rate term, which controls the sample quality by matching the aggregate posterior with a chosen prior distribution~\citep{FixElbo}. Whether the VAE training process succeeds in striking this balance depends on a number of factors, including the network architectures, the chosen prior and the class of allowed posterior distributions~\citep{Hoffman2016ELBO}. In some cases, too powerful decoders can decouple the latent space from the input~\citep{BowmanVVDJB16, KSDDSSA17} and lead to posterior collapse~\citep{OordVK17}.

A long list of works have dissected and studied the training behavior of VAEs~\citep{FixElbo, Hoffman2016ELBO} and suggested modifications to remedy common issues. Many fixes add complexity to the VAE model, e.g. by modifying or annealing the ELBO objective~\citep{BowmanVVDJB16, Alemi2016, beta-VAE, Makhzani2015}, choosing more expressive posterior distributions~\citep{KingmaSW16,RezendeM15, SalimansKW15, TranRB15}, or using more flexible priors~\citep{BauerM19,KSDDSSA17, TomczakW17Vamp}.

In this work we take a different approach. We give up on the variational ansatz that lies at the heart of VAEs and instead suggest a conceptually very simple model with very stable training properties. The Probabilistic Autoencoder (PAE) is motivated by probabilistic principal component analysis~\citep{TippingBishop1999} and consists of an Autoencoder (AE), which is interpreted probabilistically after training by means of a Normalizing Flow (NF). Both of these components are comparably easy to set up and train and this two-stage set up allows the practitioner to optimize their hyper-parameters (model architecture, training procedure, etc.) independently. We claim that the PAE is a viable alternative to VAEs despite its conceptual simplicity. We back this claim empirically through ablation studies. Specifically, we compare the performance of the PAE to that of equivalent VAEs in a number of tasks which we think are specifically relevant for practical applications: data compression (reconstruction quality), data generation, anomaly detection and probabilistic data denoising and imputation. 

Our primary contributions are: 1) a simple generative model designed with ease-of-use and training in mind 2) a quantitative comparison of this model to variational autoencoders, showing that it performs relevant tasks at comparable quality and accuracy without variational
inference 3) a new anomaly detection metric through NF density estimation in latent space, which is a byproduct of the PAE, but can also be used within the VAE framework. We make all of our code publicly available.\footnote{\url{https://github.com/VMBoehm/PAE-ablation}}

\section{Motivation: Probabilistic PCA}
\label{sec:PPCA}
The probabilistic autoencoder is motivated by linear Principal Component Analysis (PCA) and its probabilistic interpretation, probabilistic principal component analysis~\citep{TippingBishop1999}, which provides a PCA-based data likelihood estimate. 

A principal component analysis of data $\bi{x}\in \mathds{R}^N$ at fixed latent space dimensionality $K$ ($K{<}N$) finds the orthogonal linear transformation, $\bi{O}$,
\begin{equation}
    \bi{O}: \mathbb{R}^K \to \mathbb{R}^N, \bi{z} \mapsto \bi{O}\bi{z},\, \bi{O}\bi{O}^T{=}\mathds{1}_N
\end{equation}
that maximizes the data variance in the latent space. Maximizing the variance of the transformed data is equivalent to minimizing the average reconstruction error (the residual variance in data space). The PCA problem can be solved analytically and the principal components are given by the eigenvectors of the data covariance matrix.

A suitable latent space dimensionality, $K$, is chosen by inspecting the eigenvalues, $\lambda_i$, of the data covariance and keeping only the eigenvectors that correspond to the largest eigenvalues. The average reconstruction error that originates from the discarded eigenvalues is $\sigma_{\mathrm{recon}}^2{=}\sum_{i=K+1}^N \lambda_i$.

The data model under a PCA is
\begin{equation}
    \bi{x}=\bi{O}\bi{z}+\bi{\epsilon},
\end{equation} where $\bi{O}$ is constructed from the eigenvectors that correspond to the largest eigenvectors and $\bi{\epsilon}$ is the residual not captured by the PCA transformation.


In probabilistic PCA (PPCA) the residuals are assumed to follow a Gaussian distribution. The implicit likelihood is then,
\begin{equation}
\label{eq:GaussLikelihood}
    \ln \tilde{p}(\bi{x}|\bi{z}) = -\frac{1}{2}\left[N \ln(2\pi)+\ln \det{\bi{\Sigma}} + (\bi{x}-\bi{O}\bi{z})^T \bi{\Sigma}^{-1} (\bi{x}-\bi{O}\bi{z})\right].
\end{equation}
Under the approximation that the reconstruction error is uncorrelated and isotropic, $\bi{\Sigma}$ is a diagonal matrix with $\sigma_{\mathrm{recon}}^2$ along its diagonal, $\bi{\Sigma} = \sigma_{\mathrm{recon}}^2 {\mathds{1}}_N$.  


The implicit likelihood in equation~\ref{eq:GaussLikelihood} alone is not yet a probabilistic model for the data. 
A fully probabilistic structure requires a prior over the latent space. 
PPCA~\citep{TippingBishop1999} assumes that the latent variables follow a Gaussian distribution with mean zero and covariance $\bi \Lambda$,  where $\bi \Lambda$ is a diagonal matrix with the rank-ordered eigenvalues $\lambda_i$ along its diagonal. 

The Gaussian prior allows us to analytically compute the marginal,
\begin{equation}
\label{eq:PPCA}
    \ln \tilde p(\bi{x}) = -\frac{1}{2}\left[N \ln(2\pi)+\ln \det{{\bi C}} + \bi x^T {\bi C}^{-1} \bi x\right],
\end{equation}
with ${\bi C} = \bi O \bi \Lambda \bi O^T + \bi \Sigma$.

In summary, probabilistic PCA constructs a probabilistic model by first finding a basis which minimizes 
the reconstruction error in a class of models, followed by using the probability 
distribution of the latent variables as the prior. With the probabilistic autoencoder we generalize this 
approach to non-linear models. The PCA is replaced by an autoencoder trained to minimize the reconstruction error, and the Gaussian ansatz for the prior is replaced by a normalizing flow.

\section{The probabilistic autoencoder}

\subsection{PAE training}
In analogy to PPCA the PAE is constructed in two stages. Stage 1 is an autoencoder with encoder $\bi{f}$ and decoder $\bi{g}$, both deep neural networks with respective trainable parameters $\phi$ and $\theta$,
\begin{align}
\bi{f}_\phi:  \mathbb{R}^N \to \mathbb{R}^K, \bi{x} \mapsto \bi{f}_\phi(\bi{x}),\;
\bi{g}_\theta: \mathbb{R}^K \to \mathbb{R}^N, \bi{z} \mapsto \bi{g}_\theta(\bi{z}).
\end{align}
The training objective of the AE is the reconstruction error or $L_2$-distance,
\begin{equation}
\mathcal{L}_{\mathrm{AE}} = \EE_{p({\bi{x}})}||\bi{x}-\bi g_{\theta}(\bi{f}_{\phi}(\bi{x}))||_2^2.
\label{loss_AE}
\end{equation}
The autoencoder is not a probabilistic model. To construct the PAE, we interpret it probabilistically with a second stage.
We approximate the latent space prior, $p(\bi{z})$, by performing a density estimation on the AE-encoded training data. In PPCA the latent space density is modeled with a Gaussian. The PAE employs a more flexible density estimator for modeling the prior, a normalizing flow.

Normalizing flows~\citep{RippelAdams13,DinhKB14,DinhSB16,glow,ffjord18} address the task of modeling the density distribution $p(\bi z)$ of input data $\bi z$ by introducing a bijective mapping, $\bi{b}_{\gamma}(\bi{z})= \bi{u}$, from the data $\bi{z}$ 
to an underlying latent representation $\bi u$,
\begin{equation}
\label{nf}
\bi{b}_\gamma: \mathbb{R}^K \to \mathbb{R}^K, \bi{z} \mapsto \bi{u}{=}\bi b_{\gamma}(\bi z).
\end{equation}
Requiring the latent variables to follow a given prior distribution $q(\bi{u})$, one can write the modeled data 
probability density using 
conservation of probability,
\begin{equation}
p_{\gamma}(\bi z) =q(\bi u)|\nabla_{\bi{z}}\bi{b}_{\gamma}(\bi{z})|.
\label{nll2}
\end{equation}
Here, $q(\bi u)$ is some simple normalized latent space 
probability density, usually a Gaussian $\mathcal{N}(\bi{0},\bi{I})$,  and $|\nabla_{\bi{z}}\bi{b}_{\gamma}(\bi{z})|$ is the Jacobian determinant of the transformation $\bi{b}_{\gamma}(\bi{z})$.  The NF is parametrized by parameters $\gamma$ and training takes the form of 
maximizing the data likelihood $p_{\gamma}(\bi z)$
with respect to $\gamma$. Equation~\ref{nll2} requires an evaluation of the Jacobian. NF architectures have forms for which this is simple and fast. The architectural constraints originating from this requirement also introduce beneficial regularizing properties and prevent overfitting. A schematic diagram of the PAE is shown in figure~\ref{fig:illu}.

In the PAE, an NF is trained as a mapping from the latent space of the AE, $\bi z$, to the Gaussian latent space of the normalizing flow, $\bi u$.
The training objective of the NF is the negative log likelihood of the encoded samples, $\bi{z}=f_\phi(\bi{x})$,
\begin{equation}
\mathcal{L}_{\mathrm{NF}}=  \EE_{\tilde{p}({\bi{z}})}[-\ln p_\gamma(\bi z)]{=} \EE_{\tilde{p}({\bi{z}})}\left[-\ln p(\bi u)- 
\ln \left|\det \frac{\partial \bi{b}^{-1}_{\gamma}(\bi{u})}{\partial \bi u}\right| \right]_{\bi u = \bi b_\gamma(\bi z)}.
\label{loss_NF}
\end{equation}
The normalizing flow maps the potentially very irregular latent space distribution of the AE to a Gaussian distribution. 

To sample from the PAE we draw a sample, $\bi{u}{\sim} \mathcal{N}(\bi 0, \bi{1})$, from the NF latent distribution and pass it through both the NF and AE generators (left panel in figure~\ref{fig:illu}), 
\begin{equation}
    \bi{x}=\bi{g}_\theta(\bi b_{\gamma}^{-1}(\bi u)).
\end{equation}

Just as in PPCA, density estimation on the encoded data only provides an approximate prior.
Formally, for a fully probabilistic model, a prior would be given by the aggregate posterior,
\begin{equation}
\label{prior1}
p_{\mathrm{model}}(\bi{z}) = \int \mathrm{d}\bi{x}\, p(\bi{x})\, p_{\mathrm{model}}(\bi{z}|\bi{x}) =  \EE_{p(\bi{x})}\left[ p_{\mathrm{model}}(\bi{z}|\bi{x})\right],
\end{equation}
meaning that the density estimation should be conducted on samples from the posteriors.
Since our first stage is a non-probabilistic autoencoder, there is no notion of a posterior. Our choice to fit an approximate prior on the encoded data, however, is not completely unjustified: we know that small reconstruction errors (which can be easily achieved with an AE) generally result in very narrow posteriors centered on latent space points which are well determined through the projection of the data into the latent space. The position of these points are hardly influenced by the prior. 
By replacing samples from the posterior with the encoded samples, we replace the narrow posteriors by Dirac delta distributions. By fitting on the AE encoded data we approximate the maximum a posteriori (MAP) solution with the AE encoded position. This is not a formal mathematical derivation, but can serve as a reasoning for why the PAE is able to compete with fully probabilistic models even in probabilistic tasks.

The generalization and regularization properties of NFs are another important ingredient for enabling the use of approximate MAP positions instead of samples from the posterior. NFs are unlikely to fit delta functions to their training points, but smoothly interpolate between them. The success of NF-based density estimation is proof of this: on many datasets NFs achieve the highest validation log data likelihoods \citep{durkan2019neural}. If the NF does not provide sufficient regularization, it will become apparent as overfitting (lower density estimates on validation data), which can be controlled by simplifying the architecture or early stopping. 

The AE latent space is usually of relatively low dimensionality, $K{\ll}N$, which allows for computationally tractable density estimation: the NF models do not require complex deep architectures and are fast to train, which enables efficient hyper-parameter optimization.

\begin{figure}
\centering{
\includegraphics[width=0.8\linewidth,keepaspectratio]{figures/model_illustration_new.pdf}}
\caption{\label{fig:illu} Schematic diagram of the PAE (left panel) and an illustration of the sampling procedure from the PAE (right panel). The autoencoder networks are depicted as gray trapezia, the normalizing flow is represented by black arrows and the latent spaces of the autoencoder and normalizing flow are shown in red and blue, respectively.}
\end{figure}






\subsection{Comparison to VAE}
Different to the PAE a variational autoencoder is trained on a fully probabilistic objective, the Evidence Lower BOund (ELBO),
\begin{equation}
\label{eq:ELBO}
\mathrm{ELBO} = -\mathcal{L}_{\mathrm{VAE}}= \EE_{p({\bi{x}})}\left[\EE_{q_{\phi}(\bi{z}|\bi x)}\left[\ln p_\theta(\bi x|\bi z) \right]- \mathrm{D}_\mathrm{KL}\left[q_\phi(\bi z|\bi x)||p(\bi z)\right]\right],
\end{equation}
where $q_\phi$ is an approximate, parametrized variational posterior, usually a Gaussian with diagonal covariance. Typical choices for the parametrized implicit likelihood $p_\theta(\bi x|\bi z)$ are a Bernoulli distribution for binary valued data or a diagonal Gaussian distribution for continuous data. The ELBO is guaranteed to bound the true evidence $p(\bi x)$ from below.
During the VAE training equation~\ref{eq:ELBO} is evaluated stochastically on samples from the approximate posterior.
Equation~\ref{eq:ELBO} shows that the VAE objective balances the average reconstruction error (the likelihood term or distortion) with the sample quality (the KL term or rate). If the former dominates the loss during training, the encoded distribution and prior do not match well. If this is the case, samples from the prior can land outside of the encoded domain resulting in low sample quality. If the KL term dominates, some latent dimensions will solely be used to satisfy the second term and not encode any information about the input data, a problem known as posterior collapse~\citep{FixElbo}. 
Balancing the two terms can be controlled by an additional parameters $\beta$,
\begin{equation}
    \label{eq:beta_VAE}
    \mathcal{L}_{\beta-\mathrm{VAE}}=- \EE_{p({\bi{x}})}\left[\EE_{q_{\phi}(\bi{z}|\bi x)}\left[\ln p_\theta(\bi x|\bi z) \right]-\beta\, \mathrm{D}_\mathrm{KL}\left[q_\phi(\bi z|\bi x)||p(\bi z)\right]\right].
\end{equation}
This and related modification are known as $\beta$-VAEs~\citep{BowmanVVDJB16, Alemi2016, beta-VAE, Makhzani2015}. Training on equation~\ref{eq:beta_VAE} usually involves a grid search in order to find an optimal value for $\beta$ and annealing schedules. 

The PAE optimizes the reconstruction and sample quality individually. Training of stage 1 reaches an optimal reconstruction error. The latter is then left unchanged in the training of stage 2, which can focus entirely on matching the latent space distribution. We test in our experiments whether this procedure results in an advantage in reconstruction error and sample quality. A practical advantage of this procedure is that it facilitates the hyper-parameter search over model architecture and training schedule. Instead of having to iterate over encoder/decoder and flow architecture and the balance between rate and distortion term, each step can be optimized individually and towards a single objective. 

For our comparisons between VAE and PAE to be fair, we allow the VAE prior, which is typically a standard normal distribution, to be more flexible. In analogy to the PAE, we model it with a normalizing flow,
\begin{equation}
\label{eq:flow-VAE}
\mathcal{L}_{\mathrm{flow-VAE}} = - \EE_{p({\bi{x}})}\left[\EE_{q_{\phi}(\bi{z}|\bi x)}\left[\ln p_\theta(\bi x|\bi z) \right]- \mathrm{D}_\mathrm{KL}\left[q_\phi(\bi z|\bi x)||p_\gamma(\bi z)\right]\right].
\end{equation}


\section{Downstream Tasks}
In our experiments, we test the PAE performance not only in terms of sample and reconstruction quality, but also in terms of anomaly detection, a highly relevant downstream task of generative models. In appendix~\ref{app:recon}, we further show how the PAE can be used for posterior-based probabilistic image inputation.  

\subsection{Anomaly detection}
\label{sec:ood-detector}
One application of generative models is anomaly or out-of-distribution (OoD) detection. This is often based on the assumption that a density estimator should return smaller probability densities for out-of-distribution data than in-distribution (iD) data. However, this is assumption is not always satisfied and generative model based density estimators have been reported to perform poorly in some OoD detection problems. OoD detection with VAEs, NFs~\citep{glow} and PixelCNNs~\citep{OordKEKVG16} can exhibit catastrophic outlier detection failures~\citep{Nalisnick2019}.

The PAE model is not trained to maximize the data likelihood, nor does it provide an estimate of it. While we could attempt to perform a marginalization over the latent space (after introducing an approximate implicit likelihood) in order to obtain such an estimate, we suggest a much simpler outlier detection metric: the estimated density in latent space. We find in our experiments that the NF estimated latent space density is an excellent OoD detection metric for outlier detection problems that have been identified as problematic in the literature.

Our OoD metric is again motivated by PPCA:
The PPCA model is a useful toy model to understand how dimensionality reduction prior to density estimation can be beneficial. The PPCA density estimate in latent space is given by
\begin{equation}
\label{eq:pz_PPCA}
    \ln \tilde{p}(\bi z) = -\frac{1}{2}\left[K \ln(2\pi)+\sum^K_i \ln \lambda_i + \sum_i^K z_i \lambda_i^{-1} z_i \right].
\end{equation}
Equation~\ref{eq:pz_PPCA} diverges 
for $\lambda_i \rightarrow 0$. This suggests that $\tilde{p}(\bi z)$ and henceforth $\tilde p(\bi x)$ (Equation~\ref{eq:PPCA}) can be dominated by small eigenvalues. \rev{These small eigenvalue components are well known to be difficult to estimate from a limited amount of data, i.e. they are prone to overfitting. This has led to the development of special covariance estimation regularization techniques known as shrinkage methods in the statistics literature~\citep{ChenOAS,LedoitWolf2004}.}
Dimensionality reduction keeping the largest eigenvalues removes dimensions with vanishing variance and hence cures the estimator's sensitivity to small \rev{and likely mis-estimated eigenvalues}.  In \rev{PPCA} $\tilde p(\bi{x})$, the data covariance $\bi C = \bi{O}^T \bi \Lambda \bi O + \bi \Sigma$ is regularized by the noise covariance, $\bi \Sigma$, \rev{which reinterprets the discarded  and mis-estimated eigenvalues as noise}. \rev{Noise is not informative for anomaly detection, which suggests that PPCA data space density estimation has no advantage over density estimation in latent space for this task.}

We illustrate this on an outlier detection problem between the FashionMNIST~\citep{f-mnist} and MNIST~\citep{LecunMNIST} data sets in figure~\ref{fig:PPCA}. Both of these data sets have a data covariance matrix with a high condition number. Only 86 PCA components are required to capture 90\% of the data variance in MNIST (compared to $N{=}784$) and the smallest eigenvalues are evidently singular. Similar applies to F-MNIST, where a PCA captures 90\% of the data variance with 83 components. 
These data are known to produce catastrophic failures in OoD detection, specifically when presenting samples from the MNIST data set to models trained on F-MNIST~\citep{Nalisnick2019}.
We construct a probabilistic PCA model for F-MNIST (from training data) and use equation~\ref{eq:PPCA} and equation~\ref{eq:pz_PPCA} as outlier detection metrics to separate F-MNIST in-Distribution (iD) test data from MNIST Out-of-Distribution (OoD) data. In figure~\ref{fig:PPCA} we show the Area Under Receiver Operator Curve for this outlier detection task as a function of the number of PCA components. The highest outlier detection accuracy based on latent space density estimation (equation~\ref{eq:pz_PPCA}) is $\mathrm{AUROC}=0.980$ and it is reached at a relatively low number of PCA components of 127. The highest accuracy for OoD detection based on data space density estimation (equation~\ref{eq:PPCA}) is $\mathrm{AUROC}=0.974$ at 37 components.
\rev{We argue analogously that the PAE latent space density is better for OoD detection than using a full dimensionality NF.}
In our experiments, we find that the PAE latent space density is a superior anomaly detector than the ELBO \rev{ or full dimensionality NF}. 

\begin{figure}
\centering{
\includegraphics[width=0.32\linewidth,keepaspectratio]{figures/eigenvalueAnalysis.pdf}
\includegraphics[width=0.32\linewidth,keepaspectratio]{figures/eigenvalueAnalysis_cumsum.pdf}
\includegraphics[width=0.32\linewidth,keepaspectratio]{figures/PPCA_OoD.pdf}}
\caption{\label{fig:PPCA} Principal component analysis of FashionMNIST and MNIST data sets and outlier detection accuracy (in-distribution: Fashion MNIST, out-of-distribution: MNIST) with equation~\ref{eq:PPCA} and equation~\ref{eq:pz_PPCA} as a function of included number of PCA components. A higher AUROC value corresponds to a better separation between in- and out-of-distribution data.}
\end{figure}

\section{Related Work}
\label{sec:rel_work}
 Generative moment matching networks~\citep{GMMN} have been proposed to be used in a 2-stage PAE-like set up, consisting of an autoencoder and a mapping of a Gaussian to the encoded distribution. The second stage is non-invertible and trained with a moment-matching objective. Wasserstein autoencoders~\citep{WAE} employ a training objective based on the Wasserstein distance to match the encoded distribution to a given prior. WAEs achieve high sample quality, but do not provide a density estimate. Generative latent flows~\citep{GLF} are a similar conjunction of AE and NF and achieve high sample quality, but the authors do not perform controlled ablation studies to compare their approach with ELBO-based training objectives, nor do they explore the probabilistic interpretation for downstream tasks.

Giving more flexibility to the prior distribution has also been suggested in the context of VAEs, e.g. by modeling it with an NF~\citep{BauerM19,KSDDSSA17, TomczakW17Vamp}. This has sometimes been deemed prone to overfitting~\citep{TomczakW17Vamp} and many works suggest using more expressive variational distributions instead~\citep{KingmaSW16,RezendeM15, SalimansKW15, TranRB15}. In our experiments, we compare our PAE with a VAE with an NF prior because it allows us to compare models with the same architecture. We pay special attention to overfitting, but do not observe it to be a problem.

Other approaches that improve the VAE sample quality include $\beta$-VAEs~\citep{tishby2000,Alemi2016,Tkacik2018,beta-VAE} and
2-Stage-VAEs~\citep{2StageVAE}.
$\beta$-VAEs balance rate and distortion by means of an additional scalar parameters ($\beta$). 2-stage-VAEs combine two consecutive VAE's, one for the purpose of data compression, where the KL term in the ELBO is suppressed and a second one for latent space density estimation. 
This two-stage approach achieves high quality samples. With the PAE we demonstrate that this success does not rely on the ELBO objective in the first stage and that the first stage can be replaced by an AE and the second stage by a powerful NF density estimator. High sample quality is also achieved by VQ-VAE~\citep{OordVK17,RazaviOV19}, another model that requires 2-stage training. It combines a more complicated first stage which includes hyper-parameter tuning and discretization with a second stage in which an autoregressive model is trained to learn the prior. The successes of 2-stage models indicate that separating the tasks of learning a lower-dimensional representation and learning its distribution is beneficial for generative model performance.

Other non-ELBO approaches that address density estimation for data that is confined to a lower dimensional manifold include $\mathcal{M}_{(e)}$-flows~\citep{Brehmer2020} and relaxed injective probability flows~\citep{Kumar2020}. Instead of separating the tasks of compression and density estimation in the lower dimensional manifold, these models regularize a flow itself. The authors of $\mathcal{M}_{(e)}$-flows compare their model with the PAE and find that the PAE reaches the same low reconstruction error and a higher accuracy in anomaly detection. The recently introduced regularized autoencoder is another deterministic VAE alternative based on an autoencoder, that produces comparable or even better sample quality than VAEs~\citep{Gosh2020}.

Downstream tasks: Out-of-distribution detection with generative models has recently attracted a lot of attention, triggered by the finding that state-of-the art generative models such as VAE, GLOW~\citep{glow} and MAF~\citep{MAF} fail in this task on a number of standard data sets~\citep{Nalisnick2019}. \rev{Our finding that dimensionality reduction combined with latent space density estimation results in reliable anomaly detection is in line with other works: e.g. \cite{AbatiPCC19} use a single stage training with a free parameter that 
controls the relative contribution of 
reconstruction error and NF latent space density and requires tuning of the free 
parameter. They apply it to OoD, but not 
to other PAE tasks such as generating samples, denoising and inpainting.} Other proposed solutions include the use likelihood ratios as an OoD metric instead of the likelihood itself~\citep{Gosh2020}. More recently, a reliable OoD detection was reported with density-of-states, a method which leverages another density estimator on top of the density estimation~\citep{DensityofStates}. 
\section{Experiments}
The aim of our experiments is to test whether training on the ELBO offers any measurable advantage over the PPCA-inspired PAE training approach. For this test, we construct and train equivalent PAE and VAE models and compare them in terms of reconstruction error, sample quality, outlier detection accuracy and probabilistic inpainting and denoising ability. We conduct our detailed comparisons between PAE and VAE models on the FashionMNIST data set. For anomaly detection we perform an additional comparison on MNIST and CIFAR10 with the anomaly detection method by~\citet{AbatiPCC19} (Appendix~\ref{app:AbatiComparison}). We further train a PAE model on the higher dimensional Celeb-A~\citep{celeba} data set (Appendix~\ref{app:celba}).

\subsection{Ablation studies}
\label{sec:overview}
We compare the PAE to two ELBO-based alternatives:
\begin{enumerate}
\item A VAE with a normalizing flow prior, where the encoder/decoder pair is trained on equation~\ref{eq:beta_VAE} with $\beta{=}0$, i.e. without the KL-Divergence term and the variance of $q(z)$ is kept constant. The normalizing flow is trained in a second stage on the encoded distribution. The ELBO with $\beta{=}1$ and the normalizing flow prior is used as a density estimator. We call this model \textbf{$\bi \beta_0$-VAE}. The difference to the PAE lies in the noisy estimation of the likelihood during training. Comparing the PAE to the $\bi \beta_0$-VAE tests whether training on the reconstruction error instead of the distortion term offers any advantage.

\item A VAE with a  normalizing flow prior that is trained on the ELBO with $\beta{=}1$. We call this model \textbf{flow-VAE}. The difference to the PAE lies in the training procedure. In the flow-VAE, the normalizing flow, encoder and decoder are trained jointly under the ELBO training objective. This means that the normalizing flow is trained on samples from the approximate posterior instead of the encoded samples, which are used in the PAE. 
\end{enumerate}

Table~\ref{tab:models} summarizes the different models and the parameters in which they differ. To allow for a fair comparison, we use the same model architecture and training parameters for all experiments, except in cases where fixing them might disadvantage the VAE. Encoder and decoder networks are loosely based on the infoGAN architecture~\citep{infoGAN}. Normalizing flows are constructed from RealNVP transformations~\citep{DinhSB16}, Neural Spline Flow (NSF) transformations~\citep{durkan2019neural} and trainable permutations~\citep{glow} (GLOW). The exact model architectures are listed in table~\ref{app:architectures} and the choice of parameters is detailed below in table~\ref{sec:param_choice}. Because we use non-binarized data, we use a Gaussian implicit likelihood in our VAE models (instead of a Bernoulli likelihood which would be the suitable choice for binarized data). The value of the scale parameter, $\sigma$, of this implicit likelihood was set to $\sigma=0.1$. We found this to be to be the optimal value in a small ablation study (appendix~\ref{app:sigma}). We also found that the model performance is not overly sensitive to this choice. \rev{In addition to the experiments presented here, we trained a vanilla $\beta$-VAE and show results in appendix~\ref{sec:VanillaVAE}. We did not include this model in main text because it does not use a flow prior and requires parameter fine-tuning. We also find that it performs worse than all alternatives studied in the main text.}
\begin{table}[h]
\begin{center}
\begin{tabular}{llll}
\multicolumn{4}{c}{\textbf{model parameters}}                                                                                                                                                    \\ \hline
\multicolumn{1}{l|}{\textbf{model name}}   & \multicolumn{1}{c|}{$\beta_0$-VAE } & \multicolumn{1}{c|}{PAE}         & \multicolumn{1}{l}{flow-VAE}\\ \hline
\multicolumn{1}{l|}{training objective(s)} & \multicolumn{1}{l|}{$\mathcal{L}_{\mathrm{\beta-VAE}}$, $\mathcal{L}_{\mathrm{NF}}$}       & \multicolumn{1}{l|}{$\mathcal{L}_{\mathrm{AE}}$,$\mathcal{L}_{\mathrm{NF}}$}    & \multicolumn{1}{l}{$\mathcal{L}_{\mathrm{flow-VAE}}$}          \\ 
\multicolumn{1}{l|}{flow prior}            & \multicolumn{1}{l|}{TRUE}                 & \multicolumn{1}{l|}{TRUE}                     & \multicolumn{1}{l}{TRUE }              \\ 
\multicolumn{1}{l|}{2 stage training}      & \multicolumn{1}{l|}{TRUE}                & \multicolumn{1}{l|}{TRUE}                      & \multicolumn{1}{l}{FALSE}                  \\
\multicolumn{1}{l|}{$\beta$}               & \multicolumn{1}{l|}{0}                    & \multicolumn{1}{l|}{N/A}                      & \multicolumn{1}{l}{1}                     \\
\multicolumn{1}{l|}{dropout rate}          & \multicolumn{1}{l|}{0.15}                 & \multicolumn{1}{l|}{0.15}                     & \multicolumn{1}{l}{N/A}                       \\
\multicolumn{1}{l|}{OoD metric}            & \multicolumn{1}{l|}{ELBO}                 & \multicolumn{1}{l|}{$\log p(z)$}                 & \multicolumn{1}{l}{ELBO}                     \\
\end{tabular}
\end{center}
\caption{\label{tab:models}Overview of the different models used in the ablation studies.}
\end{table}
\subsection{Data sets and preprocessing}
We perform our ablation studies on the Fashion-MNIST~\citep{f-mnist} data set, which we split into 50,000 training examples, 10,000 validation and 10,000 test samples. As outlier data sets we use MNIST~\citep{LecunMNIST} and Omniglot~\citep{OMNIGLOT}, as well as horizontal and vertical flips of F-MNIST test data. We preprocess the data by dequantizing (adding uniform noise $\in[-1/256,1/256]$) before rescaling pixel values to the interval [-0.5,0.5].

\subsection{Parameter choice and training procedure}
\label{sec:param_choice}
\hfill \break
\textbf{Training of the encoder/decoder pair:} We used the same encoder and decoder architecture for all of our experiments. We further fixed the latent space dimensionality to 40, the number of training steps to 300,000 and used a learning rate schedule in which we keep the learning rate constant at the initial value up to training step 100,000, then reduce it linearly down to 1/10 of the initial rate over 50,000 steps. For the remaining 150,000 steps we restart the learning rate and repeat the annealing scheme. We did not find our final loss to depend on the details of this annealing scheme, but found that annealing and restarting was beneficial. We used the ADAM optimizer~\citep{KingmaB14} with parameters $\beta_1=0.9, \beta_2=0.999, \epsilon=10^{-7}$ in all of our trainings (including for the normalizing flow).

The batch size, initial learning rate, sample size in the stochastic evaluation of the ELBO as well as the drop out rate were optimized to yield the best possible reconstruction error on validation data on the $\beta_0$-VAE model: we ran around 30 shorter trainings (100,000 steps) with different combinations of these parameters and used a Gaussian Process surrogate to determine the combination of parameter values that optimized the reconstruction error on the validation data. The values we obtained through this procedure are outlined in table~\ref{tab:com_params} and were used in all experiments. The only parameter that we adapted in some of our experiments is the dropout rate. The dropout layer is a necessary regularization to prevent overfitting in autoencoder models and models with $\beta=0$. This regularization is not necessary in VAE models with a flow prior and was not used in these models. We chose the $\beta_0$-VAE for parameter optimization, because we think that it is a fair middle ground between PAE and flow-VAE sharing some properties with both. It was also chosen for convenience, because it allowed us to optimize the encoder/decoder pair and and normalizing flow separately, without having to worry about the feedbacks of changing one on the other. We point out that optimizing on the flow-VAE would have suffered from this complication. To not disadvantage the flow-VAE, we also train a flow-VAE with a different (simpler) NF architecture. \begin{table}[h]
\begin{center}
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{common model parameters}} \\ \hline
batch size                       & 256               \\
initial learning rate            & 1.23E-03          \\
learning rate annealing          & TRUE              \\
sample size                      & 16                \\
training steps                   & 300000            \\
latent size                      & 40               
\end{tabular}
\end{center}
\caption{\label{tab:com_params} Training parameters that were kept constant in all encoder/decoder pair trainings. The parameters were optimized to achieve the lowest reconstruction error on validation data for the $\beta_0$-VAE model.}
\end{table}
\newline
\textbf{Training of normalizing flows:}
We construct normalizing flows from realNVP, NSF and GLOW building blocks. We optimized the number of building blocks of each kind to achieve maximal $\log p_\gamma(z)$ on the validation data encoded with the $\beta_0$-VAE model. In models where the NF was trained separately, we used a learning schedule consisting of 120 epochs during which both learning rate and batch size were annealed and restarted. We found that a restarting learning rate was especially helpful when using NSF transformation layers, but that the final loss did not strongly depend on the details of the annealing and restarting scheme. 
\newline
\textbf{Training of VAE with normalizing flow prior:}
In the flow-VAEs, encoder, decoder and NF are trained jointly on the ELBO objective with the same training parameters and architectures that were used for $\beta_0$-VAE and PAE. The only parameter that we adapted was the dropout rate, which we set to zero. To test the dependence on the flow architecture, we ran two experiments. One in which we adapted the flow architecture we had found to work best on the $\beta_0$-VAE encoded data, and another experiment, in which we simplified the flow architecture (flow-VAE (s)). We found the deeper architecture to be superior. All NF architectures are detailed in appendix~\ref{app:architectures}.
\newline
\textbf{Robustness of results:}
For every model (PAE, $\beta_0$-VAE, flow-VAE) we repeated every training run three times starting from different initial network parameter values. We report results for the trained models that achieved the lowest training loss on validation data out of these three runs. We found very little scatter between different trainings of the same model.

\subsection{Results}
\label{sec:results}
\textbf{Reconstruction Quality:} We measure the reconstruction quality in all models by means of the average reconstruction error on the test data set,
\begin{equation}
    \overline{\sigma_\mathrm{recon}^2} = \frac{1}{M} \frac{1}{N} \sum_j^M \sum_i^N \left[x_{ji} - \bi{g}_{\theta}(\bi{f}_{\phi}(\bi{x}_j))_i\right]^2 = \frac{1}{M} \sum_j^M \sigma_{\mathrm{recon},j}^2,
\end{equation}
where $j$ labels the image and $i$ the pixel. \rev{In the VAE models we evaluate the reconstruction error using the mean of the variational posterior}. We also report the 95 percentile of the image-wise reconstruction errors $P_{95\%}(\sigma_{\mathrm{recon}}^2)$, which better characterizes the large error tail of the distribution. The results are listed in table~\ref{tab:recons}, the reported errors were obtained through bootstrapping on the test data.
\begin{table}[h]
\begin{center}
\begin{tabular}{lllll}
\multicolumn{5}{c}{\textbf{reconstruction quality}}                                                                                   \\ \hline
\multicolumn{1}{c|}{\textbf{model name}}   & \multicolumn{1}{c|}{$\beta_0$-VAE} & \multicolumn{1}{c|}{PAE}         & \multicolumn{1}{c|}{flow-VAE} & \multicolumn{1}{c}{flow-VAE(s)} \\ \hline
\multicolumn{1}{l|}{$\overline{\sigma_\mathrm{recon}^2}$ $[\times10^{-3}]$ ($\downarrow$)} & \multicolumn{1}{l|}{$6.24\pm0.06$}       & \multicolumn{1}{l|}{\textbf{5.87}$\bi \pm\bi{0.06}$}& \multicolumn{1}{l|}{$6.22\pm0.07$} &       $6.66\pm0.06$      \\
\multicolumn{1}{l|}{$P_{95\%}(\sigma_{\mathrm{recon}}^2)$ $[\times10^{-3}]$ ($\downarrow$)} & \multicolumn{1}{l|}{$29.6\pm0.3$}       & \multicolumn{1}{l|}{$\bi{27.6\pm0.3}$} &  \multicolumn{1}{l|}{$35.7 \pm 0.4$} &  $31.9\pm0.3$ \\
\end{tabular}
\end{center}
\caption{\label{tab:recons} Comparison of F-MNIST models in terms of reconstruction quality. The PAE model achieves the lowest reconstruction errors.}
\end{table}

We find that the PAE model has a consistently and significantly lower mean and 95 percentile reconstruction error than ELBO-based models. In figure~\ref{fig:reconstructions}, we show reconstructions of test data points for each model. \begin{figure}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/input.pdf}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/rec_beta0VAE.pdf}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/rec_PAE.pdf}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/rec_flowVAE2.pdf}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/rec_flowVAE1.pdf}
\caption{Visual comparison of F-MNIST reconstructions with different models. From left to right: original data, $\beta_0$-VAE, PAE, flow-VAE, flow-VAE(s).}
\label{fig:reconstructions}
\end{figure}




\textbf{Sample Quality:} How to best measure image generation quality in a way that quantifies both image quality (are the samples visually compelling?) and diversity (is the model sampling from the full distribution?), is still a question of active research. Here, we measure the sample quality in terms of the well established Frechet-Inception Distance (FID) score~\citep{FID_Heusel}, which is known to correlate well with human perception of image quality. The FID score takes a sample of generated images and a sample of real images, passes each of them through the pre-trained Inception network~\citep{SzegedyVISW16} and extracts as features the outputs of one of the last layers. Each set of features is then fitted with a multivariate Gaussian distribution, $\mathcal{N}(x|\mu,\Sigma)$. The FID score is defined as the Fréchet distance between these two Gaussians
\begin{equation}
\label{eq:FID}
\mathrm{FID}(x_\mathrm{orig},x_\mathrm{gen}) =||\mu_{\mathrm{orig}}-\mu_{\mathrm{gen}}||^2_2+ \mathrm{Tr}\left(\Sigma_{\mathrm{orig}}+\Sigma_{\mathrm{gen}}-2(\Sigma_{\mathrm{orig}}\Sigma_{\mathrm{gen}})^\frac{1}{2}\right).
\end{equation}
In figure~\ref{fig:samples} we show generated images from each model. While visually comparable, we get the lowest FID scores for the flow-VAE model (table~\ref{tab:sample_qual}).
\begin{table}[h]
\begin{center}
\begin{tabular}{lllll}
\multicolumn{5}{c}{\textbf{sample quality}}                                                                      \\ \hline
\multicolumn{1}{c|}{\textbf{model name}}   & \multicolumn{1}{c|}{$\beta_0$-VAE} & \multicolumn{1}{c|}{PAE}         & \multicolumn{1}{c|}{flow-VAE} & \multicolumn{1}{c}{flow-VAE (s)} \\ \hline
\multicolumn{1}{l|}{FID score ($\downarrow$)} & \multicolumn{1}{l|}{$32.8\pm0.4$}       & \multicolumn{1}{l|}{$28.4\pm0.3$} &      \multicolumn{1}{l|}{$\bi{22.8\pm0.3}$}         & $33.9\pm0.4$             \\
\end{tabular}
\end{center}
\caption{\label{tab:sample_qual} Sample quality, measured in terms of the FID score. Lower values are better. Error estimates obtained through bootstrapping. }
\end{table}
\begin{figure}
\includegraphics[width=\columnwidth/100*24,keepaspectratio]{figures/betaVAE_samples.pdf}
\includegraphics[width=\columnwidth/100*24,keepaspectratio]{figures/PAE_samples.pdf}
\includegraphics[width=\columnwidth/100*24,keepaspectratio]{figures/flowVAE1_samples.pdf}
\includegraphics[width=\columnwidth/100*24,keepaspectratio]{figures/flowVAE2_samples.pdf}
\caption{Visual comparison of random samples. From left to right: $\beta_0$-VAE, PAE, flow-VAE, flow-VAE (s).}
\label{fig:samples}
\end{figure}
A concern sometimes raised with models that use flow priors is that the added expressiveness of the model results in overfitting or memorization of training data. We pay special attention to overfitting during training by monitoring the loss on validation data.
\newline
\textbf{Outlier Detection Accuracy:}
For outlier detection, we use the outlier detection metrics listed in table~\ref{tab:models} and report the outlier detection accuracy for each model in terms of the Area Under Receiver Operator Curve (AUROC). An AUROC of 1 corresponds to a perfect separation between the out-of-distribution and in-distribution data. For the latter we use the F-MNIST test data set. \rev{The ELBO of the VAE is evaluated using 10 samples from the variational posterior and we verified that using more samples does not achieve better AUROC values.} The results are listed in~\ref{tab:auroc}. The latent space density estimation of the PAE model outperforms the ELBO of our VAE models in all but one of our experiments. However, we do not observe a catastrophic failure of OoD detection with the ELBO as it has been reported for this data set~\citep{Nalisnick2019}. 
\begin{table}[h]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{lllll}
\multicolumn{5}{c}{\textbf{OoD detection accuracy measured in terms of AUROC} $\uparrow$} \\ \hline
\multicolumn{1}{c|}{\textbf{OoD data}}   & \multicolumn{1}{c|}{$\beta_0$-VAE} & \multicolumn{1}{c|}{PAE (AE+flow)}         & \multicolumn{1}{c|}{flow-VAE} & \multicolumn{1}{c}{flow-VAE (s)} \\ \hline
\multicolumn{1}{l|}{MNIST} & \multicolumn{1}{l|}{$0.9086\pm0.0023$}       & \multicolumn{1}{l|}{$\bi{0.9970\pm 0.0003}$} &       \multicolumn{1}{l|} {$0.9633\pm0.0014$}      &         $0.9230\pm0.0022$     \\
\multicolumn{1}{l|}{Omniglot} & \multicolumn{1}{l|}{$0.8668\pm 0.0025$}       & \multicolumn{1}{l|}{$\bi{0.9736\pm 0.0011}$} &         \multicolumn{1}{l|}{$0.9221\pm 0.0020$}             &    $0.8753\pm 0.0021$    \\
\multicolumn{1}{l|}{F-MNIST hor.} & \multicolumn{1}{l|}{$0.6790\pm0.0036$}       & \multicolumn{1}{l|}{$\bi{0.6883\pm 0.0038}$} &        \multicolumn{1}{l|}{$0.6758\pm0.0036$}  & $0.6773\pm0.0035$                     \\
\multicolumn{1}{l|}{F-MNIST vert.} & \multicolumn{1}{l|}{$0.8891 \pm 0.0022$}       & \multicolumn{1}{l|}{$0.8789\pm 0.0024$} &      \multicolumn{1}{l|}{$\bi{0.8994\pm0.0022}$}  & $0.8964\pm0.0019$    \\
\end{tabular}}
\end{center}
\caption{\label{tab:auroc} Out of distribution detection with different models. The outlier detection accuracy is measured in terms of the AUROC$\in [0,1]$. Higher values are better. Error estimates obtained through bootstrapping.}
\end{table}
Following our discussion in~\ref{sec:ood-detector}, which suggests that latent space density estimation is superior to data space density estimation in many cases, we disseminate the ELBO further and take a look at the different contributing terms of the ELBO. Specifically, we  split the ELBO into three terms, which we identify as distortion (first term), entropy (second term) and cross entropy (last term),
\begin{equation}
\label{eq:ELBO_split}
\mathrm{ELBO} =  \EE_{p({\bi{x}})}\left[\EE_{q_{\phi}(\bi{z}|\bi x)}\left[\ln p_\theta(\bi x|\bi z)- q_{\phi}(\bi{z}|\bi x)+p_\gamma(\bi z)\right]\right],
\end{equation}
and  measure the outlier detection accuracy with each of these terms individually. The results are listed in table~\ref{tab:ELBO_auroc}. 

We find that the cross entropy term, which measures a stochastic mean over the latent space density and is therefore closely related to the latent space density, is a very reliable outlier detection metric for models trained on the ELBO objective.
\begin{table}[h]
\resizebox{\textwidth}{!}{
\begin{tabular}{lllll}
\multicolumn{5}{c}{\textbf{OoD detection with flow-VAE, AUROC}($\uparrow$)}                                                                      \\ \hline
\multicolumn{1}{c|}{\textbf{OoD data}}   & \multicolumn{1}{c|}{distortion} & \multicolumn{1}{c|}{rate}         & \multicolumn{1}{c|}{entropy} & \multicolumn{1}{c}{cross entropy} \\ \hline
\multicolumn{1}{l|}{MNIST} & \multicolumn{1}{l|}{$0.9620\pm0.0014$}       & \multicolumn{1}{l|}{$\bi{0.9965 \pm 0.0003}$} &       \multicolumn{1}{l|} {$0.9659\pm0.0012$}      &          $ \bi{0.9963\pm0.0004} $       \\
\multicolumn{1}{l|}{Omniglot} & \multicolumn{1}{l|}{$0.9176\pm0.0023$}       & \multicolumn{1}{l|}{$\bi{0.9786 \pm 0.0009}$} &         \multicolumn{1}{l|}{$0.8965\pm0.0023$}             &     $\bi{0.9795\pm0.0009}$     \\
\multicolumn{1}{l|}{F-MNIST hor.} & \multicolumn{1}{l|}{$\bi{0.6751\pm0.0042}$}       & \multicolumn{1}{l|}{$\bi{0.6728\pm0.0038}$} &        \multicolumn{1}{l|}{$0.6274\pm0.0041$} &  $0.6619\pm0.0038$                    \\
\multicolumn{1}{l|}{F-MNIST vert.} & \multicolumn{1}{l|}{$\bi{0.8977\pm0.0023}$}       & \multicolumn{1}{l|}{$\bi{0.8981\pm0.0022}$} &      \multicolumn{1}{l|}{$0.7415\pm0.0041$}  & $0.8921\pm0.0024$    \\
\end{tabular}}
\caption{\label{tab:ELBO_auroc}  Dissecting out-of-distribution detection with the ELBO. Error estimates obtained through bootstrapping.}
\end{table}



\section{Discussion and conclusion}
We have introduced the probabilistic autoencoder, a simple generative model with a lower dimensional latent space that is motivated by probabilistic PCA. Different to variational autoencoders, it builds 
the probabilistic structure after the 
first stage of training, but 
has the advantage of being simple and straightforward to set up and train. Because it is first trained to achieve optimal reconstruction error and then, in a second stage, to produce optimal samples, it performs both tasks reliably and well.  We further test its performance in two additional downstream tasks, which we think are particularly relevant for practical applications: anomaly detection and probabilistic image denoising and inputation. 
We find that the PAE performs all considered tasks at comparable quality as equivalent VAE models, suggesting that 
ELBO-based variational optimization is not 
an essential component of this class of models. We find that our proposed OoD metric of NF density in latent space, 
while a natural byproduct of PAE, can also 
be used in VAEs if they are complemented with an NF prior.
\bibliography{cosmo,ML,vae_sample_improvements}  
\bibliographystyle{tmlr}
\appendix

\section{PAE on Celeb-A}
\label{app:celba}
To demonstrate the feasibility of the PAE approach on higher dimensional, more complex data, we train a PAE on Celeb-A. The celeb-A samples are cropped to the central 128x128 pixels and then downsampled to 64x64 pixels. We used the same preprocessing, architecture and training procedure as for the F-MNIST experiments and a latent space dimensionality of $K=64$. Samples from this model are shown in figure~\ref{fig:celeba_samples} on the left. On the right we show interpolations between images, produced by projecting two images from the test data set into the latent space of the NF, connecting them in the NF latent space by \rev{linear interpolation and sampling along the connecting line} in equally spaced intervals. The samples then get forward modeled into data space to produce the images shown. The smooth transitions indicates that the PAE produces a continuous latent space without any holes.  

\begin{figure}
\begin{subfigure}{0.4\textwidth}
\centering{
\includegraphics[width=\columnwidth/100*85,keepaspectratio]{figures/nvp_samples_64_celeba.pdf}}
\end{subfigure}
\begin{subfigure}{0.59\textwidth}
\centering{
\includegraphics[width=\columnwidth/100*98,keepaspectratio,trim=0 7 0 7, clip]{figures/interp11.pdf}
\includegraphics[width=\columnwidth/100*98,keepaspectratio, trim=0 7 0 7, clip]{figures/interp10.pdf}
\includegraphics[width=\columnwidth/100*98,keepaspectratio, trim=0 7 0 7, clip]{figures/interp0.pdf}
\includegraphics[width=\columnwidth/100*98,keepaspectratio, trim=0 7 0 7, clip]{figures/interp6.pdf}
\includegraphics[width=\columnwidth/100*98,keepaspectratio, trim=0 7 0 7, clip]{figures/interp8.pdf}}
\end{subfigure}
\caption{PAE performance on Celeb-A at $K{=}64$. Samples (left) reach $\mathrm{FID}{=}49.2$ (reconstructions $\mathrm{FID}{=}44.0$). Right: Interpolations between samples from the test set.}
\label{fig:celeba_samples}
\end{figure}


\section{Comparison to vanilla VAE}
\label{sec:VanillaVAE}
\rev{For completeness, we include here results for a conventional VAE, which shares the same architecture and training parameters as the models used in the other experiments, but uses a standard normal distribution as a prior. We trained the VAE on equation~\ref{eq:beta_VAE} with initial value of $\beta=100$, which was then linearly annealed during the first 100,000 steps. We show samples and reconstructions of this model in Fig~\ref{fig:betaVAE} and list reconstruction error and FID scores in Table~\ref{tab:betaVAE}. In all cases the results and visual images are inferior to flow-VAE and PAE described in the main text. 
\begin{table}[h]
\begin{center}
\begin{tabular}{lll}
\multicolumn{1}{c|}{$\sigma_{\mathrm{recon}}^2$ $[\times10^{-3}]$ ($\downarrow$)}   &  \multicolumn{1}{c|}{$P_{95\%}(\sigma_{\mathrm{recon}}^2)$ $[\times10^{-3}]$ ($\downarrow$)}         & \multicolumn{1}{c}{FID Score ($\downarrow$)}\\ \hline
\multicolumn{1}{l|}{$12.90\pm0.10$} & \multicolumn{1}{l|}{$68.00\pm0.60$}       & \multicolumn{1}{l}{$32.3\pm0.3$}
\end{tabular}
\end{center}
\caption{\label{tab:betaVAE} Reconstruction errors and FID scores of vanilla $\beta$-VAE.}
\end{table}}
\begin{figure}
\begin{center}
\includegraphics[width=\columnwidth/100*24,keepaspectratio]{figures/input.pdf}
\includegraphics[width=\columnwidth/100*24,keepaspectratio]{figures/rec_valid_set_VAE40.pdf}
\includegraphics[width=\columnwidth/100*24,keepaspectratio]{figures/raw_samples_VAE40.pdf}
\caption{From left to right: Original test images, their reconstructions and samples generated with the vanilla $\beta$-VAE described in Appendix~\ref{sec:VanillaVAE}.}
\label{fig:betaVAE}
\end{center}
\end{figure}

\section{Comparison of anomaly detection with Abati et al.}
\label{app:AbatiComparison}
We compare anomaly detection with the PAE to the results presented in~\citet{AbatiPCC19}. Their set up is similar, but instead of a two-stage training, they propose a joint training of autoencoder and density estimator. Their loss function is a combination of reconstruction error and estimated density of encoded data. The contribution of the density term to the loss function is controlled by a tunable scalar parameter $\lambda$.

Using the same autoencoder architecture\, latent space dimension, and batch size\footnote{obtained from \url{https://github.com/aimagelab/novelty-detection}}, we perform anomaly detection experiments as~\citet{AbatiPCC19} but with a PAE-style two-stage training: We train models on each class of the CIFAR10 and MNIST datasets separately and then evaluate the outlier detection accuracy when each of these models is applied to the full test dataset containing all classes (considering all other classes outliers). Results of these experiments and the results from~\citet{AbatiPCC19} are shown in tables~\ref{tab:MNIST_results} and~\ref{tab:CIFAR_results}.

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
    \hline
        Class & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & mean \\ \hline
        Abati et al. & 0.993 & {0.999} & 0.959 & {0.966} & 0.956 & {0.964} & {0.994} & {0.98} & 0.953 & {0.981} & {0.975} \\ 
        PAE & 0.988 & {0.999} & {0.972} & 0.964 & {0.97} & 0.955 & 0.991 & 0.974 & {0.955} & 0.978 & {0.975}
    \end{tabular}
    \caption{\label{tab:MNIST_results} Comparison of MNIST anomaly detection results.}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
    \hline
        Class & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & mean \\ \hline
        Abati et al. & 0.735 & 0.58 & 0.69 & 0.542 & 0.761 & 0.546 & 0.751 & 0.535 & 0.717 & 0.548 & 0.608\\ 
        PAE & {0.737} & 0.472 & 0.684 & 0.52 & 0.749 & 0.506 & 0.758 & 0.529 & 0.682 & 0.446 & 0.604 \\ 
    \end{tabular}
    \caption{\label{tab:CIFAR_results} Comparison of CIFAR anomaly detection results.}
\end{table}

\section{Interpolation studies}

\rev{We expand on the image interpolation results shown in Appendix~\ref{app:celba} by comparing PAE image interpolation with AE image interpolations and adding pixel-level interpolation (linear interpolation between pixel values) as a baseline. Results are shown in figure~\ref{fig:interp_study}. While both AE and PAE produce fairly well interpolated images of high quality, PAE tends to produce more natural looking interpolations with fewer artifacts. 
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth/100*70]{figures/interp11_u_vers_z_bugfixed4.pdf}
\includegraphics[width=\textwidth/100*70,keepaspectratio]{figures/interp12_u_vers_z_bugfixed4.pdf}
\includegraphics[width=\textwidth/100*70,keepaspectratio]{figures/interp9_u_vers_z_bugfixed4.pdf}
\includegraphics[width=\textwidth/100*70,keepaspectratio]{figures/interp6_u_vers_z_bugfixed3.pdf}

\caption{Linear interpolations between images. in each panel we show in the top row the reconstructions from linear interpolations in PAE latent space, in the middle row from AE latent space, and in the bottom row linear interpolations in data space (linear interpolation between pixel values).}
\label{fig:interp_study}
\end{center}
\end{figure}}

\section{PAE posterior and application to data inputation}
\label{app:recon}
Large-scale data acquisition often results in noisy and incomplete data. In most applications, e.g. when one is interested in finding certain rare features in an image, the aim of data restoration is not only to obtain the most probable uncorrupted image, but also to obtain an estimate of its fidelity. 
A plethora of generative model based approaches for image reconstruction have been suggested in the literature~\citep{RezendeMW14, Mattei2018, DongLHT15, JinMcCann2016, RIMPutzky, DeepImagePrior, AmbientGAN, MIWAE}, but few of them enable uncertainty quantification~\citep{DeepUQ}. With the PAE, we can perform sound posterior-based data restoration. This does not only enable uncertainty quantification, it also provides a framework for consistently including analytical data models, such as physics models.

A latent space posterior for a corrupted data point $\tilde{\bi{x}}=\bi M \bi x + \bi n$, where $\bi{M}$ is a pixel-wise mask and $\bi n$ denotes the noise, consists of an implicit likelihood and a prior. The form of the implicit likelihood is determined by the noise properties. For Gaussian noise, $\bi n$, with noise 
covariance $\bi{\sigma}_{\mathrm{noise}}$
(typically a diagonal matrix) and a generative model $\bi{g}_\theta$, trained on uncorrupted data, the implicit likelihood is given by
\begin{equation}
     p_{\theta}(\tilde{\bi {x}}|\bi{z},\bi{M},\bi{\sigma}_{\mathrm{noise}}) = \mathcal{N}\left(\bi{\tilde x} |\bi M \bi{g}_\theta(\bi z) , \bi{\sigma}_{\mathrm{recon}}^2+\bi{\sigma}^2_{\mathrm{noise}}\right).
     \label{eq:recon_likelihood}
\end{equation}
Note that the covariance of this Gaussian likelihood is composed of the generative model's reconstruction error, $\bi \sigma_{\mathrm{recon}}^2$, and the noise level in the corrupted data, $\bi \sigma^2_{\mathrm{noise}}$. For sufficiently high latent space dimensionalities the latter dominates, $\sigma_{\mathrm{recon},i}{\ll}\sigma_{\mathrm{noise},i}$, ensuring that the likelihood is well approximated by a Gaussian. By replacing $\bi x$ by its generative process, $ \bi{g}_\theta(\bi z)$, we bring the inference problem to the low dimensional latent space of the generative model. Posterior analysis of high-dimensional data becomes computationally tractable in this lower dimensional space.

The prior is modeled by the generative models' latent space distribution. Combining the two, we obtain the log posterior
\begin{equation}
    \ln p_{\theta,\gamma}(\bi z|\tilde{\bi{x}},\bi{M},\bi{\sigma}_{\mathrm{noise}}) = \ln p_{\theta}(\tilde{\bi {x}}|\bi{z},\bi{M},\bi{\sigma}_{\mathrm{noise}}) +\ln p_{\gamma}(\bi{z}) - \mathrm{const}.
    \label{eq:recon_posterior}
\end{equation}

To denoise and inpaint a corrupted image one performs latent space posterior analysis. A point estimate is given by the MAP, $\bi z'$, the maximum of equation~\ref{eq:recon_posterior}, which forward modeled into data space, $\bi x_\mathrm{recon}=\bi{g}_\theta(\bi z')$, yields the most likely underlying image. A full posterior analysis can be performed with many techniques, including Laplace approximation, VI or MCMC sampling. Given the multi-modal posterior of some of our examples we fit a full rank Gaussian mixture model to equation~\ref{eq:recon_posterior} following~\citep{SeljakYu19} in our experiments. We can then sample from this model to obtain other solutions that are compatible with the data. 

We examine probabilistic reconstruction with both PAE and flow-VAE on three examples created by corrupting test data with uncorrelated Gaussian noise ($\mu_n{=}0$, $\sigma_n{=}0.1$) and masks. The true underlying images are shown in the first column of figure~\ref{fig:inpaint_denois} and the corrupted input data to the reconstructions in the second column. The masked areas were chosen to allow for several plausible inpainting solutions. 
To obtain reconstructions we minimize the negative of the log posterior in equation~\ref{eq:recon_posterior}. Since we expect multimodal posteriors we run 20 minimizations starting from random points which we draw from the prior. We keep only those minimization results that are associated with a positive definite Hessian (true minima and no saddle points). In the third and fourth column we show the forward modeled deepest minimum found by this procedure for the PAE (third column) and flow-VAE model (fourth column). The reconstructed examples are well denoised, but flow-VAE and PAE find slightly different inpainting solutions for the masked areas. This is most visible for the first example, where the flow-VAE prefers a clasp on the bag, while the VAE reconstructs a dint at the top. While this suggests that the models have learned slightly different priors and forward models, we find that all reconstructions are very plausible and that the PAE does not seem to perform worse at this task than the flow-VAE. In fact, in the first example, the PAE seems to better reconstruct the area outside of the masked area (e.g. the ribbon on the left), a consequence of the lower reconstruction error of the PAE model.

To obtain uncertainty estimates we fit a Gaussian mixture model using the local minima associated with the largest posterior mass. We use the thus constructed posterior approximation for uncertainty estimation by generating samples from this posterior. The samples are shown in figure~\ref{fig:post_samples}, with samples from the PAE posterior in the top row and samples from the flow-VAE in the bottom row. We observe some variety in these samples, which we encouraged by posing problems that should have multiple plausible solutions. For example the PAE finds that handbags with different dint depths are compatible with the data. The flow-VAE mostly prefers handbags with clasps, but it also produces two samples without a clasp. It seems that neither of the two models explores the full variety of potential solutions (the flow-VAE missing the dints and the PAE missing the clasps). In the second and third example the PAE model seems to provide some greater variety in the samples. This could be an indicator that the flow-VAE posterior is more complex and wasn't explored properly by our poterior analysis. It is not in scope of this work to explore these possibilities in great detail, instead, we note that flow-VAE and PAE perform comparably well at this task. 
\begin{figure}
\begin{center}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/truth_VAE_solidmask_1470.pdf}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/input_data_PAE_solidmask_1470.pdf}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/best_reconstruction_PAE_1470.pdf}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/best_reconstruction_flowVAE_1470.pdf}

\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/truth_VAE_solidmask_3055.pdf}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/input_data_PAE_solidmask_3055.pdf}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/best_reconstruction_PAE_3055.pdf}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/best_reconstruction_flowVAE_3055.pdf}


\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/truth_VAE_solidmask_5463.pdf}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/input_data_PAE_solidmask_5463.pdf}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/best_reconstruction_PAE_5463.pdf}
\includegraphics[width=\columnwidth/100*12,keepaspectratio]{figures/best_reconstruction_flowVAE_5463.pdf}

\end{center}
\caption{Underlying true data (left column), corrupted data (second column) and most likely reconstructions with the PAE (third column) and flow-VAE (fourth column).
\label{fig:inpaint_denois}}
\end{figure}
\begin{figure}
\includegraphics[width=\columnwidth/100*31,keepaspectratio]{figures/posterior_samples_PAE_1470.pdf}
\includegraphics[width=\columnwidth/100*31,keepaspectratio]{figures/posterior_samples_PAE_3055.pdf}
\includegraphics[width=\columnwidth/100*30,keepaspectratio]{figures/posterior_samples_PAE_5463.pdf}

\includegraphics[width=\columnwidth/100*31,keepaspectratio]{figures/posterior_samples_flowVAE_1470.pdf}
\includegraphics[width=\columnwidth/100*31,keepaspectratio]{figures/posterior_samples_flowVAE_3055.pdf}
\includegraphics[width=\columnwidth/100*31,keepaspectratio]{figures/posterior_samples_flowVAE_5463.pdf}

\caption{Samples from the reconstruction posteriors (PAE: top row, flow-VAE: bottom row).  
\label{fig:post_samples}}
\end{figure}


\section{Model architectures and training procedures}
\label{app:architectures}
We use the same encoder and decoder architecture in all experiments. Details are given in table~\ref{tab:enc_archi} and table~\ref{tab:dec_archi}. The only model parameter that is varied is the dropout rate, which was set to 0.15 in models where it was necessary to avoid overfitting and to zero otherwise. The choice of hyperparameters is described in detail in section~\ref{sec:param_choice}.
\begin{table}[h]
\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{encoder}}                                                                                        \\ \hline
\multicolumn{1}{c|}{\textbf{layer}} & \multicolumn{1}{c}{\textbf{details}}                                                  \\ \hline
\multicolumn{1}{l|}{input}          & normalized to {[}-0.5,0.5{]}, de-quantized with uniform noise $\in$ {[}-1/256, 1/256{]} \\
\multicolumn{1}{l|}{convolutional}  & kernel size={[}4,4{]} , strides={[}2,2{]},  filters=64, padding=`SAME'              \\
\multicolumn{1}{l|}{leakyReLU}      & $\alpha$=0.2                                                                            \\
\multicolumn{1}{l|}{convolutional}  & kernel size={[}4,4{]}, strides={[}2,2{]}, filters=128, padding=`SAME'              \\
\multicolumn{1}{l|}{batch norm}      & momentum=0.999, $\epsilon$=1e-5                                                        \\
\multicolumn{1}{l|}{dropout}        & dropout rate dependent on model, see~\ref{tab:com_params}                                                                   \\
\multicolumn{1}{l|}{leakyReLU}      & $\alpha$=0.2                                                                            \\
\multicolumn{1}{l|}{reshape}        & flatten                                                                               \\
\multicolumn{1}{l|}{linear}         & output size=1024                                                                    \\
\multicolumn{1}{l|}{batch norm}      & momentum=0.999, $\epsilon$=1e-5                                                        \\
\multicolumn{1}{l|}{dropout}        & dropout rate dependent on model, see~\ref{tab:com_params}                                                          \\
\multicolumn{1}{l|}{leakyReLU}      & $\alpha$=0.2                                                                            \\
\multicolumn{1}{l|}{linear}         & 2x latent size                                                                       
\end{tabular}}
\caption{\label{tab:enc_archi} The layout of the encoder network used in all experiments.}
\end{table}
\begin{table}[h]
\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{decoder}}                                                                                          \\ \hline

\multicolumn{1}{c|}{\textbf{layer}}        & \multicolumn{1}{c}{\textbf{details}}                                             \\ \hline
\multicolumn{1}{l|}{input}                 & encoded data, latent size=40 \\
\multicolumn{1}{l|}{linear}                & output size=1024                                                               \\
\multicolumn{1}{l|}{batch norm}             & momentum=0.999, $\epsilon$=1e-5                                                   \\
\multicolumn{1}{l|}{leakyReLU}             & $\alpha$= 0.2                                                                       \\
\multicolumn{1}{l|}{linear}                & output size=128x28/4x28/4                                                    \\
\multicolumn{1}{l|}{batch norm}             & momentum=0.999, $\epsilon$=1e-5                                                   \\
\multicolumn{1}{l|}{leakyReLU}             & $\alpha$=0.2                                                                       \\
\multicolumn{1}{l|}{dropout}               & dropout rate dependent on model, see~\ref{tab:com_params}                                                               \\
\multicolumn{1}{l|}{reshape}               & output size={[}28/4,28/4,128{]}                                              \\
\multicolumn{1}{l|}{transpose convolution} & output shape={[}28/2,28/2,64{]}, kernel size ={[}4,4{]} , strides={[}2,2{]} \\
\multicolumn{1}{l|}{batch norm}             & momentum=0.999, $\epsilon$=1e-5                                                   \\
\multicolumn{1}{l|}{leakyReLU}             & $\alpha$=0.2                                                                       \\
\multicolumn{1}{l|}{dropout}               & dropout rate dependent on model, see~\ref{tab:com_params}                                                   \\
\multicolumn{1}{l|}{transpose convolution} & output shape={[}28,28,1{]}, kernel size ={[}4,4{]} , strides = {[}2,2{]}        \\
\multicolumn{1}{l|}{sigmoid}               & subtract 0.5 to match input data                                                
\end{tabular}}
\caption{\label{tab:dec_archi} The layout of the decoder network used in all experiments.}
\end{table}
\begin{table}[h]
\resizebox{\textwidth}{!}{
\begin{tabular}{cll}
\multicolumn{3}{c}{\textbf{normalizing flow}}                                                                                                                   \\ \hline
\multicolumn{1}{c|}{\textbf{blocks}}    & \multicolumn{1}{c|}{\textbf{layer}}          & \multicolumn{1}{c}{\textbf{details}}                                  \\ \hline
\multicolumn{1}{l|}{}                   & \multicolumn{1}{l|}{rescale}                 & \multicolumn{1}{l}{scale dependent on model, see~\ref{tab:flows}}                                           \\ \hline
\multicolumn{1}{c|}{1} & \multicolumn{1}{l|}{neural spline transform} & \multicolumn{1}{l}{bins=36, bins: bin network, slopes: slope network} \\
\multicolumn{1}{c|}{}                   & \multicolumn{1}{l|}{swap permutation}        & \multicolumn{1}{l}{swap first half of dimensions with second half}    \\ 
\multicolumn{1}{c|}{}                   & \multicolumn{1}{l|}{neural spline transform} & \multicolumn{1}{l}{bins=36, bins: bin network, slopes: slope network} \\
\multicolumn{1}{c|}{}                   & \multicolumn{1}{l|}{trainable permutation}   & \multicolumn{1}{l}{GLOW-style LU decomposition of orthogonal matrix}  \\ \hline
\multicolumn{1}{c|}{2} & \multicolumn{1}{l|}{real NVP transform}      & \multicolumn{1}{l}{trainable shift and rescale}                                 \\
\multicolumn{1}{c|}{}                   & \multicolumn{1}{l|}{swap permutation}        & \multicolumn{1}{l}{swap first half of dimensions with second half}    \\ 
\multicolumn{1}{c|}{}                   & \multicolumn{1}{l|}{real NVP transform}      & \multicolumn{1}{l}{trainable shift and rescale}                       \\
\multicolumn{1}{c|}{}                   & \multicolumn{1}{l|}{trainable permutation}   & \multicolumn{1}{l}{GLOW-style LU decomposition of orthogonal matrix}  \\ \hline
\multicolumn{1}{c|}{3} & \multicolumn{1}{l|}{real NVP transform}      & \multicolumn{1}{l|}{trainable shift}                                        \\
\multicolumn{1}{c|}{}                   & \multicolumn{1}{l|}{swap permutation}        & \multicolumn{1}{l}{swap first half of dimensions with second half}    \\ 
\multicolumn{1}{c|}{}                   & \multicolumn{1}{l|}{real NVP transform}      & \multicolumn{1}{l}{trainable shift}                                        \\
\multicolumn{1}{c|}{}                   & \multicolumn{1}{l|}{trainable permutation}   & \multicolumn{1}{l}{GLOW-style LU decomposition of orthogonal matrix}  \\ \hline
\multicolumn{1}{l|}{}                   & \multicolumn{1}{l|}{rescale}                 & \multicolumn{1}{l}{1/scale dependent on model, see table~\ref{tab:flows}}                                          
\end{tabular}}
\caption{\label{tab:flow_archi} Building blocks of the normalizing flows used in all experiments. }
\end{table}

The normalizing flow(s) we use are composed of the same building blocks in all experiments but vary in how often the building blocks are repeated. All blocks have the same structure and only differ in the type of transformation that is performed. A block is made up of a transformation of the first half of data variables, a swapping of the first half of data variables with the second half, a transformation on the other half of variables and finally a trainable permutation of the variables. Block 1 applies a neural spline flow transformation, Block 2 a realNVP transformation with shifting and rescaling, Block 3 a realNVP transformation with only shifting. The architectural details are given in table~\ref{tab:flow_archi}. We use two different layouts in our experiments, a deeper flow with [2x block 1, 4x block 2 and 4x block 3] and a simpler flow with [1x block 1, 2x block 2 and 1x block 3]. For our Celeb-A experiments, we use a normalizing flow consisting of [1x block 1, 2x block 2 and 2x block 3]. Every flow also features a re-scale operation that ensures that the encoded training samples lie within the range $z_i \in [-1,1]$. 

The neural spline flow uses two networks to determine the bin widths and slopes of the rational quadratic splines. The layouts of these networks are listed in table~\ref{tab:bin_archi}. 
\begin{table}[h]
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{slope network}}                                        \\ \hline
\multicolumn{1}{c|}{\textbf{layers}} & \multicolumn{1}{c}{\textbf{details}}       \\ \hline
\multicolumn{1}{l|}{dense}           & output size=latent size//2               \\
\multicolumn{1}{l|}{leakyReLU}       & $\alpha$=0.2                                  \\
\multicolumn{1}{l|}{dense}           & output size=bins-1                      \\
\multicolumn{1}{l|}{reshape}         & {[}latent size/2, bins-1{]} \\
\multicolumn{1}{l|}{softplus}        & softplus(x)+1e-2  \\     

\end{tabular}
\hfill
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{bin network}}                                       \\ \hline
\multicolumn{1}{c|}{\textbf{layers}} & \multicolumn{1}{c}{\textbf{details}}     \\ \hline
\multicolumn{1}{l|}{dense}          & output size=latent size/2             \\
\multicolumn{1}{l|}{leakyReLU}      & $\alpha$=0.2                                \\
\multicolumn{1}{l|}{dense}          & output size=latent size/2            \\
\multicolumn{1}{l|}{leakyReLU}      & $\alpha$=0.2                                \\
\multicolumn{1}{l|}{dense}          & output size=bins                       \\
\multicolumn{1}{l|}{reshape}        & {[}latent size/2, bins{]} \\
\multicolumn{1}{l|}{softmax}        & softmax(x)(2-1e-2 bins)+1e-2   
\end{tabular}
\caption{\label{tab:bin_archi} Networks used to determine bin widths and slopes in the rational quadratic spline of the neural spline flow.}
\end{table}
\begin{table}[h]
\begin{center}
\begin{tabular}{llll}
\multicolumn{4}{c}{\textbf{normalizing flow layouts for each model}}           \\ \hline
\multicolumn{1}{c|}{\textbf{model name}}   & \multicolumn{1}{c|}{$\beta_0$-VAE} & \multicolumn{1}{c|}{PAE \& flow-VAE}                   & \multicolumn{1}{c}{flow-VAE(s)} \\ \hline
\multicolumn{1}{l|}{repeats flow block 1}  & \multicolumn{1}{l|}{2}                    & \multicolumn{1}{l|}{2}                     & 1                                  \\
\multicolumn{1}{l|}{repeats flow block 2}  & \multicolumn{1}{l|}{4}                    & \multicolumn{1}{l|}{4}                     & 2                                  \\
\multicolumn{1}{l|}{repeats flow block 3}  & \multicolumn{1}{l|}{4}                    & \multicolumn{1}{l|}{4}                     & 1                                  \\
\multicolumn{1}{l|}{flow rescale scale}    & \multicolumn{1}{l|}{344}                 & \multicolumn{1}{l|}{3.44}                  & 1                                  \\
\end{tabular}
\caption{\label{tab:flows} Normalizing flow layouts of different models. Blocks are described in table~\ref{tab:flow_archi}.}
\end{center}
\end{table}


\section{Flow-VAE ablation studies}
\label{app:sigma}
We ran several flow-VAE models with different values of $\sigma$, with $\sigma$ being the scale parameter in the implicit ELBO likelihood, a multivariate Gaussian with $\bi{\mu}{=}\bi{f}_\theta(\bi{z})$ and $\bi{\Sigma}{=}\sigma \mathds{1}$, 
\begin{equation}
    p_\theta(\bi{x}|\bi{z}) = \mathcal{G}(\bi{x}|f_\theta(\bi{z}),\sigma \mathds{1}).
\end{equation}
The objective of this ablation study is to test whether the flow-VAE performance depends on this parameter. Similar to the $\beta$-parameter, $\sigma$ could have an influence on the relative contribution of distortion and rate term to satisfying the ELBO objective during training. The results of this ablation study (in terms of reconstruction error and sample quality) are listed in table~\ref{tab:flow-ab}. We do not find a strong dependence of neither reconstruction error nor sample quality on $\sigma$. Somewhat counter-intuitively we get slightly higher reconstruction errors for lower values of $\sigma$, an indication that the model is starting to overfit. A test run with $\sigma=0.05$ (not listed in the table) resulted in catastrophic overfitting. 
A slightly higher value, 
$\sigma=0.1$, seems to be a sweet spot, with the lowest reconstruction error, no signs of overfitting and good FID score. This is the model we use in the result section. We note that the other values of $\sigma$ we tried would not change our results in a way that invalidates our comparison or conclusions. 
We also note that such an ablation study/ parameter optimization is not needed for the PAE training, which highlights an advantage of the PAE approach.

\begin{table}[h]
\begin{center}
\begin{tabular}{llll}
\multicolumn{4}{c}{$\bf{\sigma}$-\textbf{ablation study}}                                                                                   \\ \hline
\multicolumn{1}{c|}{$\sigma$}   & \multicolumn{1}{c|}{0.08} & \multicolumn{1}{c|}{0.1}         & \multicolumn{1}{c}{0.12} \\ \hline
\multicolumn{1}{l|}{$\overline{\sigma_\mathrm{recon}^2}$ $[\times10^{-3}]$ ($\downarrow$)} & \multicolumn{1}{l|}{$7.85\pm0.09$}    & \multicolumn{1}{l|}{$\bi{6.22\pm0.07}$}    & \multicolumn{1}{l}{$8.35\pm0.09$}  \\
\multicolumn{1}{l|}{$P_{95\%}(\sigma_{\mathrm{recon}}^2)$ $[\times10^{-3}]$ ($\downarrow$)} & \multicolumn{1}{l|}{$36.5\pm0.3$}       &  \multicolumn{1}{l|}{$\bi{35.7 \pm 0.4}$}  & \multicolumn{1}{l}{$39.9\pm 0.5$}\\
\multicolumn{1}{l|}{sample FID score ($\downarrow$)} & \multicolumn{1}{l|}{$\bi{22.5\pm0.2}$}       &  \multicolumn{1}{l|}{$\bi{22.8 \pm 0.3}$} & \multicolumn{1}{l}{$\bi{22.8\pm0.3}$} \\
\end{tabular}
\end{center}
\caption{\label{tab:flow-ab}Flow-VAE ablation study: reconstruction error and sample quality as a function of $\sigma$, the width of the Gaussian likelihood entering the distortion term in the ELBO.}
\end{table}
\end{document}
