\documentclass[preprint]{elsarticle}

\journal{arXiv}


\usepackage{float}
\floatstyle{boxed}
\restylefloat{table}
\restylefloat{figure}
\usepackage[section,above,below]{placeins}
\usepackage{microtype}
\usepackage{amsmath,amsfonts,amssymb}
\allowdisplaybreaks
\usepackage{stmaryrd}
\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n}
\usepackage[braket]{qcircuit}
\usepackage{proof}
\usepackage{tikz}
\usepackage{multicol,multirow}
\usepackage{graphicx}
\usepackage{array,longtable}
\usepackage{wrapfig}

\newcolumntype{C}{>{}}  \newcolumntype{R}{>{}}  \newcolumntype{L}{>{}}  


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newdefinition{definition}[theorem]{Definition}
\newproof{proof}{Proof}

\newcommand\Span{\ensuremath{\mathsf{Span}}}
\newcommand\interp[1]{\llparenthesis #1\rrparenthesis}
\newcommand\size[1]{||#1||}
\newcommand\Red[1]{\mathsf{Red}(#1)}
\newcommand\SN{\mathsf{SN}}
\newcommand\lpl[1]{|#1|}
\newcommand\titulo[3][\scriptsize]{\rotatebox[origin=c]{90}{\parbox[t]{#2}{\centering #1{#3}}}}
\newcommand\ti[1]{\textrm{\scriptsize\bf #1}}
\newcommand\recap[3]{\noindent {\bf #1 \ref{#2}.} \emph{#3}}
\newcommand\xrecap[4]{\noindent {\bf #1 \ref{#3} (#2).} \emph{#4}}
\newcommand\braket[2]{\langle{#1}|{#2}\rangle}
\newcommand\lra[1][1]{\longrightarrow_{\left(#1\right)}}
\newcommand\lrap{\lra[p]}
\newcommand\B{\ensuremath{\mathbb B}}
\newcommand\Bs{\ensuremath{\mathfrak B}}
\newcommand\gB{\ensuremath{\Psi}}
\newcommand\ite[3]{{#1}?{#2}\mathord{\cdot}{#3}}
\newcommand\pair[2]{({#1}+{#2})}
\newcommand\npair[2]{({#1}-{#2})}
\newcommand\bqtypes{\ensuremath{\mathbf B}}
\newcommand\qtypes{\ensuremath{\mathbf Q}}
\newcommand\types{\ensuremath{\mathbf T}}
\newcommand\basis{\ensuremath{\mathcal B}}
\newcommand\values{\ensuremath{\mathcal V}}
\newcommand\s[1]{\ensuremath{\mathsf{#1}}}
\newcommand\head{\text{\sl head}}
\newcommand\tail{\text{\sl tail}}
\newcommand\rulestitlehalf[1]{\omit\rlap{\parbox{0.3\linewidth}{\centering\textbf{#1}}}}
\newcommand\red[2][1]{\overset{\scriptscriptstyle\smash{#2}\vphantom{x}}{\lra[#1]}\ }
\newcommand\z[1][A]{\vec 0_{S(#1)}}
\newcommand\nullvec[1]{\z[#1]}
\newcommand\may[1][\alpha]{[{#1}.]}
\newcommand\rbetab{(\s{\beta_b})}
\newcommand\rbetan{(\s{\beta_n})}
\newcommand\riftrue{(\s{if_{1}})}
\newcommand\riffalse{(\s{if_{0}})}
\newcommand\rlinr{(\s{lin^+_r})}
\newcommand\rlinscalr{(\s{lin^\alpha_r})}
\newcommand\rlinzr{(\s{lin^0_r})}
\newcommand\rlinl{(\s{lin^+_l})}
\newcommand\rlinscall{(\s{lin^\alpha_l})}
\newcommand\rlinzl{(\s{lin^0_l})}
\newcommand\rneut{(\s{neutral})}
\newcommand\runit{(\s{unit})}
\newcommand\rzeros{(\s{zero_\alpha})}
\newcommand\rzero{(\s{zero})}
\newcommand\rzeroS{(\s{zero_S})}
\newcommand\rprod{(\s{prod})}
\newcommand\rdists{(\s{\alpha dist})}
\newcommand\rdistcasum{(\s{dist^+_\Uparrow})}
\newcommand\rdistcascal{(\s{dist^\alpha_\Uparrow})}
\newcommand\rcaneutl{(\s{neut^\Uparrow_\ell})}
\newcommand\rcaneutr{(\s{neut^\Uparrow_r})}
\newcommand\rfact{(\s{fact})}
\newcommand\rfacto{(\s{fact^1})}
\newcommand\rfactt{(\s{fact^2})}
\newcommand\rproj{(\s{proj})}
\newcommand\rhead{(\s{head})}
\newcommand\rtail{(\s{tail})}
\newcommand\rdistzr{(\s{dist^0_r})}
\newcommand\rdistzl{(\s{dist^0_l})}
\newcommand\rdistscalr{(\s{dist^\alpha_r})}
\newcommand\rdistscall{(\s{dist^\alpha_l})}
\newcommand\rdistsumr{(\s{dist^+_r})}
\newcommand\rdistsuml{(\s{dist^+_l})}
\newcommand\rcomm{(\s{comm})}
\newcommand\rassoc{(\s{assoc})}
\newcommand\tax{\textsl{Ax}}
\newcommand\teq{\textsl{Eq}}
\newcommand\tif{\textsl{If}}
\newcommand\gen{\mathcal S} \newcommand\den[1]{\llbracket #1 \rrbracket}
\newcommand\vect[2]{\left(\begin{smallmatrix} #1 \\ #2\end{smallmatrix}\right)}

\newcommand{\app}[2]{#1 #2}
\newcommand{\reducesto}{\rightarrow}
\newcommand{\proj}[2]{\pi_{#1} #2}
\newcommand{\vrbl}[2]{#1^{#2}}
\newcommand{\abstr}[2]{\lambda#1\ldotp#2}
\newcommand{\cast}[2]{\Uparrow_{#1}^{#2}}

\begin{document}

\begin{frontmatter}

  \title{Two linearities for quantum computing\\ in the lambda
    calculus\tnoteref{previous,projects}} \tnotetext[previous]{This paper
    extends a paper presented at TPNC'17 by the first two authors
    \cite{DiazcaroDowekTPNC17} and the third author \emph{Licenciatura}'s thesis
    \cite{RinaldiUNR18}.} \tnotetext[projects]{Partially supported by the ECOS
    Project A17C03 QuCa, PICT-PRH 2015-1208, and the Laboratoire International
    Associ\'e SINFIN.}

  \author[CONICET,UNQ]{Alejandro D\'iaz-Caro}
  \ead{adiazcaro@icc.fcen.uba.ar}

  \author[ENS]{Gilles Dowek} \ead{gilles.dowek@ens-paris-saclay.fr}

  \author[UNR]{Juan Pablo Rinaldi} \ead{juampi.rinaldi@gmail.com}

  \address[CONICET]{CONICET-UBA, ICC,
    Pabell\'on 1, Ciudad Universitaria, Buenos Aires, Argentina.}
  \address[UNQ]{Universidad Nacional de Quilmes, R.~S\'aenz Pe\~na
    352, Bernal, BA, Argentina}
  \address[ENS]{Inria, LSV, ENS Paris-Saclay, 61, avenue du Pr\'esident Wilson,
    Cachan, France}
  \address[UNR]{Universidad Nacional de Rosario, Pellegrini 250, Rosario, SF, Argentina}

\begin{abstract}
  We propose a way to unify two approaches of non-cloning in quantum
  lambda-calculi: logical and algebraic linearities. The first approach is to forbid duplicating variables, while
  the second is to consider all lambda-terms as algebraic-linear functions. We
  illustrate this idea by defining a quantum extension of first-order
  simply-typed lambda-calculus, where the type is linear on superposition, while
  allows cloning base vectors. In addition, we provide an interpretation of the
  calculus where superposed types are interpreted as vector spaces and
  non-superposed types as their basis.
\end{abstract}

\begin{keyword}
  quantum computing, lambda-calculus, algebraic linearity, linear logic,
  measurement
\end{keyword}

\end{frontmatter}
\section{Introduction}


Extending -calculus into a programming language for quantum computing
requires to add, besides the usual abstraction and application symbols, a sum
and a product to build linear combinations of terms, and a tensor-product like
symbol to include datatypes composed of several qubits. Yet, mixing all the
constructs in a naive way leads to a too powerful calculus where non linear,
that is non physical functions, can be defined. For instance, in -calculus, applying the term , that
expresses a non-linear function for some convenient definition of , to
a term  yields the term , that reduces to . But ``cloning'' this vector  is forbidden in quantum computing.
Various quantum -calculi address this problem in different ways.

One way is to forbid the construction of the term 
using a typing system inspired from linear logic
\cite{GirardTCS87,AbramskyTCS93}, leading to logic-linear calculi
\cite{AltenkirchGrattageLICS05,SelingerValironSTQC09,GreenLeFanulumsdaineRossSelingerValironPLDI13,PaganiSelingerValironPOPL14,ZorziMSCS16}.
Another is to define the operational semantics in such a way that every -term represents a linear function. The term
, for instance, expresses the linear function that maps
 to  and  to \footnote{Where  is the Dirac notation for vectors, with  and
  ,
  so  is an orthonormal basis of , called here
  the ``computational basis''.}. This leads to restrict beta-reduction to the
case where  is a base vector (in the computational basis) and to add the
linearity rule , leading to
algebraic-linear calculi \cite{ArrighiDowekRTA08, ArrighiDiazcaroLMCS12,
  DiazcaroPetitWoLLIC12,
  ArrighiDiazcaroValironIC17,AssafDiazcaroPerdrixTassonValironLMCS14}.

Each solution has its advantages and drawbacks. For example, let  be
the conditional statement on  and . Interpreting -terms
as algebraic-linear functions permits to reduce the term  to
 then to , instead of reducing it to the term  that would be blocked. This explains that
this linearity rule, that is systematic in the algebraic-linear languages cited
above, is also present for the condition in \cite{AltenkirchGrattageLICS05} (the
so-called  operator).

However, interpreting all -terms as linear functions forbids to extend
the calculus with non-linear operators, such as measurement. For instance, the
term , where  represents a
measurement in the computational basis, would reduce to , while it should reduce to  with
probability  and to  with probability .

In this paper, we propose a way to unify the two approaches, distinguishing
duplicable and non-duplicable data by their type, like in the logic-linear
calculi; and interpreting -terms as linear functions, like in the
algebraic-linear calculi, when they expect duplicable data. We illustrate this
idea with an example of such a calculus.

In this calculus, a qubit has type  when it is in the computational basis,
hence duplicable (a non-linear term in the sense of linear logic), and
\footnote{ for \emph{superposition} and also for the \emph{Span} of
  .} when it is a superposition, hence non-duplicable (a linear term in the
sense of linear logic). Hence, we can distinguish a basis term, from a term in
the span of such a basis. We could also state that the term  has type . However, giving this
type to this term and the type  to the term  would jeopardize the subject
reduction property as, using the bilinearity of the tensor product, the former
should develop to the latter. This dilemma is not specific to quantum computing
as computing is often a non-reversible process where some information is lost.
For instance, if we express, in its type, that the term  is a
product of two polynomials, developing it to  does not preserve
this type. Therefore, instead of a bilinear tensor product, we will use -ary
Cartesian products, so the term  has type
, and to move from this type to  we use an
explicit cast. Notice that, if  is a set of vectors and  is the span
of the set , then  is isomorphic to .
Hence, the term  has type 
and it cannot be reduced. But the term , where  is used as a mark to allow casting, has type  and can be developed
to .

This language permits expressing quantum algorithms with a very precise
information about the nature of the data processed by these algorithms.

\paragraph*{Outline of the paper}

Section~\ref{sec:QC} introduces some basic notations and concepts of quantum computing.
In Section~\ref{sec:calculus} we introduce the calculus, without product. In
Section~\ref{sec:tensor} we extend the language with a -ary Cartesian product
for multiple-qubits systems. In Section~\ref{sec:SR} we state and prove the
Subject Reduction property. In Section~\ref{sec:SN} we state and prove the
Strong Normalization property. Then, in Section~\ref{sec:denSem} we provide a
straightforward interpretation of the calculus considering base types as sets of
vectors, and types  as vector spaces. In Section~\ref{sec:examples} we
express two non-trivial example in our calculus: the Deutsch algorithm and the
Teleportation algorithm, demonstrating the expressivity of the proposed
language. Finally, in Section~\ref{sec:conclusion}, we conclude. There are also
two appendices, \ref{ap:Deutsch} and \ref{ap:telep}, which have more details of the
examples given in Section~\ref{sec:examples}.

\section{Basics notions of quantum computing}\label{sec:QC}
This section does not intend to introduce a full description of quantum
computing, the interested reader can find actual introductions to this area in
many textbooks, e.g.~\cite{NielsenChuang00,Jaeger07}. This section only intends
to introduce some basic notations and concepts.

In quantum computation, data is expressed by normalised vectors in Hilbert
spaces. For our purpose, this means that the vector spaces are defined over
complex numbers and come with a norm and a notion of orthogonality. The smallest
space usually considered is the space of {\em qubits}. This space is the
two-dimensional vector space , and it comes with a chosen
orthonormal basis denoted by . A qubit (or quantum bit) is a
normalised vector , where .
To denote an unknown qubit  it is common to write . A
two-qubits vector is a normalised vector in ,
that is, a normalised vector generated by the orthonormal basis
, where  stands for
. In the same way, a -qubits vector is a normalised
vector in  (or  with ). Also common is
the notation  for the transposed, conjugate of , e.g.~if
, then
 where for any
,  denotes its conjugate.

The operators on qubits that are considered in this paper are the {\em quantum
gates}, that is, unitary operators. An {\em unitary operator} is an invertible linear
function preserving the norm and the orthogonality of vectors. The {\em adjoint}
of a given operator  is denoted by , and the unitary condition
imposes that . These functions are linear, and so it is enough
to describe their action on the base vectors. Another way to describe these
functions would be by matrices, and then the adjoint is just its conjugate
transpose. A set of universal quantum gates is the set ,
 and , which can be defined as follows:
\begin{description}
\item[The  gate.] The {\em controlled-not} is a two-qubits gate which only changes the second qubit if the first one is :
  
  where  and .
\item[The  gate.] The  gate is a
  single-qubit gate that modifies the {\em phase} of the qubit:
  
  where  is the phase shift.
\item[The  gate.] The {\em Hadamard} gate is a single-qubit gate which
  produces a 45 degree rotation of the basis:
  
\end{description}

To make these gates act in higher-dimension qubits, they can be put together
with the bilinear symbol . For example, to make the Hadamard gate act
only in the first qubit of a two-qubits register, it can be taken to , and to apply a Hadamard gate to both qubits, just .

An important restriction, which has to be taken into account if a calculus
pretends to encode quantum computing, is the so called {\em no-cloning
  theorem}~\cite{WoottersZurekNATURE82}:

\begin{theorem}[No cloning]\label{thm:nocloningINT}
  There is no linear operator such that, given any qubit
  , can clone it. That is, it does not exist any
  unitary operator  and fixed  such that
  .
\end{theorem}
\begin{proof}
  Assume there exists such an operator , so given any  and
   one has  and also
  . Then
  
  where  is the conjugate transpose of .
  However, notice that the left side of equation \eqref{eq:proofNCINT} can be
  rewritten as
  
  While the right side of equation \eqref{eq:proofNCINT} can be rewritten as
  

  So , which implies either
   or , none of which can be
  assumed in the general case, since  and  were
  picked as random qubits. \qed
\end{proof}

The implication of this theorem in the design choices of a calculus is that it
must be forbidden to allow functions duplicating arbitrary arguments. However,
notice that this does not forbid cloning some specific qubit states. Indeed, for
example the qubits  and  can be cloned without much effort by
using the  gate:  and . In
this sense, the imposed restriction is not a resources-aware restriction {\em
  \`a la} linear logic~\cite{GirardTCS87}. It is a restriction that forbids us
from creating a `universal cloning machine', but still allows us to clone any
given known term.

Another operation considered on qubits is the measurement. A projector is an
operator of the form . For example, in the canonical base
 of ,  is a projector and
 is another projector, with respect to such a base. Indeed,

With these projectors we can define the measurement operators  and  as

For example,

\footnotetext{The scalar  is known as a {\em phase} and
  can be ignored, so only  remains.}

The quantum measurement is defined in terms of sets of measurements operators.
For example, in the canonical base , the set  is
a quantum measurement. When it acts on a qubit , it will apply the
operator , with probability .
\section{No cloning, superpositions and measurement}\label{sec:calculus}
\label{sec:fstGram}\label{sec:fstTS}\label{sec:fstTRS}


The grammar of types is defined in Table~\ref{tab:typesNoX} and the grammar of
terms in Table~\ref{tab:termsNoX}, where .

\begin{table}
  \centering
  
  \caption{First grammar of types, without product.}
  \label{tab:typesNoX}
\end{table}

\begin{table}
  \centering
  
  \caption{First grammar of terms, without product.}
  \label{tab:termsNoX}
\end{table}


Terms are variables, abstractions, applications, two constants for base qubits
( and ), linear combinations of terms (built with addition and
product by a scalar, addition being commutative and associative), a family of
constants for the null vectors, one for each type of the form , (),
and an if-then-else construction () deciding on base vectors. We use
the notation  for , so making  a function, which applied to a term  produces ``if  then  else '', when  is a basis term. We also include a symbol  for
measurement in the computational basis.

The grammar is split into base terms (non-superposed values), general values,
and general terms. Types are also split into qubit types and general types.

The set of free variables of a term  is defined as usual in the
-calculus and denoted by . We use  as a notation to
refer indistinctly to  and to . We use  as a shorthand notation
for , and  as a shorthand notation for . The term
 will have type , and reduce to , which is not a base term.

An important property of this calculus is that types  are linear
types. Indeed, those correspond to superpositions, and so no duplication is
allowed on them. Instead, at this tensor-free stage, a type without an
 on head position is a non-linear type, such as , which correspond
to base terms, i.e.~terms that can be cloned. A non-linear function is allowed
to be applied to a linear argument, for example,  can be
applied to , however,
it distributes in the following way: .

Hence, the beta reduction occurs only when the type of the argument is the same
as the type expected by the abstraction. Thus, the rewrite system depends on
types. For this reason, we describe first the type system, and only then the
rewrite system.

A type  will be interpreted as a set of vectors and  as the vector
space generated by the span of such a set (cf.~Section~\ref{sec:denSem}). Hence,
we naturally have  and . Therefore, we also
define a subtyping relation on types (cf.~Table~\ref{tab:SubtypingNoX}). The
type system is given in Table~\ref{tab:TS}, where contexts  and 
have a disjoint support.

\begin{table}
  \centering
  
  
  \caption{First subtyping relation, without product.}
  \label{tab:SubtypingNoX}
\end{table}

\begin{table}
  
  \caption{First type system, without product.}
  \label{tab:TS}
\end{table}

Remarks: Rule  allows typing variables only with qubit types. Hence, the
system is first-order and only qubits can be passed as arguments (more when the
rewrite system is presented). Rule  types the null vector as a
non-base term, because the null vector cannot belong to the base of any vector
space. Rules  and  type the base qubits with the
base type .

Thanks to rule  the term  has type  and also the more
general type . Note that  has type
 and reduces to  which has the same type . Reducing this
term to  of type  would not preserve its type. Moreover, this type
would contain information impossible to compute, because the value  is
not the result of a measurement, but of an interference.

Rule  states that a term multiplied by a scalar is not a base term.
Even if the scalar is just a phase, we must type the term with an 
type, because our measurement operator will remove any scalars, so having the
scalar means that it has not been measured yet. Rule  is the analog for
sums to the previous rule. Rule  is the elimination of the superposition,
which is achieved by measuring (using the  operator).

Notice that  is typed as a non-linear function by rule , and so,
the if-then-else linearly distributes over superpositions, e.g.


Rule  is the elimination for superpositions, corresponding to
the linear distribution. Notice that the type of the argument is a superposition
of the argument expected by the abstraction ( vs.~). Also, the
abstraction is allowed to be a superposition. If, for example, we want to apply
the sum of functions  to the base argument , we would obtain
the superposition . The typing is as follows:


which reduces to


Similarly, a linear function () applied to a
superposition  reduces to a superposition . The typing is as follows:

which reduces to



Finally, Rules  and  correspond to weakening and contraction on variables
with base types. The rationale is that base terms can be cloned.

The null vectors  need to be interpreted as the null vector of the vector
space . Therefore, since the vector space  is the same as ,
their null vectors should coincide. Then, we define a function 
which gives us the smallest type in terms of the amounts of  it includes, that generates the vector space, so the null vector can be
taken from such a space.

Therefore, we will identify, through reduction, the term  with .

The rewrite system is given in Table~\ref{tab:RSNoX}.
\begin{table}
  \vspace{-3mm}
  
  \centering
  All the terms are considered to be closed (i.e. reduction is weak).
  \caption{First rewrite system, without product.}
  \label{tab:RSNoX}
\end{table}
The relation  is a probabilistic relation where  is the probability of
occurrence. Every rewrite rule has a probability  of occurrence, except for
the projection rule . The rewrite system depends on the typing, in
particular an abstraction can either expect a base term as argument (that is, a
non-linear term) or a superposition, which has to be treated linearly. However,
an abstraction expecting a non-linear argument can be given a superposition
(which is linear), and it is typable, only that the reduction distributes before
beta-reduction.

There are two beta rules. Rule  acts only when the argument is a base
term, and the type expected by the abstraction is a base type. Hence, rule
 is ``call-by-base'' (base terms coincides with values of
-calculus, while values on this calculus also includes superpositions
of base terms and the null vector). Instead,  is the usual call-by-name
beta rule. They are distinguished by the type of the argument. Rule \rbetab\
acts on non-linear functions while \rbetan\ is for linear functions. The test on
the type of the argument is due to the type system that allows an argument with
a type not matching with the type expected by the abstraction (in such a case,
one of the linear distribution rules applies).

Since there are two beta reductions, the contextual rule admitting reducing the
argument on an application is valid only when the abstraction expects an
argument of type . If the argument is typed with a base type, then it
reduces to a term that can be cloned, and we must reduce it first to ensure that
we are cloning a term that can be cloned indeed. For example, a measure over a
superposition (e.g.  has a base type , but it
cannot be cloned until it is reduced. Indeed,  can reduce either to  or
, but never to  or , which would be
possible only if the measure happens after the cloning machine. A more physical
way to state it is that cloning after measurement is not a problem, since we
already know the state to be cloned: It would be enough to prepare a second
system in the same state.

The group If-then-else contains the tests over the base qubits  and
.

The first three of the linear distribution rules (those marked with subindex
), are the rules that are used when a non-linear abstraction is applied to a
linear argument (that is, when an abstraction expecting a base term is given a
superposition). In these cases the beta reductions cannot be used since the side
conditions on types are not met. Hence, these distributivity rules apply
instead.

For example, let us give more details in the reduction sequence on the example
given at the beginning of this Section.


Notice that in Rule \rlinzr, the term needs to be reduced to
. Indeed, if we just reduce  to , there is a
problem of subject reduction:  having type  do not implies
it has no other type, for example, , and so, reducing to 
would break subject reduction since  does not have necesarilly type .
On the contrary, we can prove (cf.~Lemmas~\ref{lem:minlqA} and \ref{lem:min}) that if  has types  and
, then , and so subject reduction is preserved.

The remaining rules in this group deal with a superposition of functions. For
example, rule \rlinl\ is the sum of functions: A superposition is a sum,
therefore, if an argument is given to a sum of functions, it needs to be given
to each function in the sum. We use a weak reduction strategy (i.e.~reduction
occurs only on closed terms), hence the argument  on this rule is closed,
otherwise, it could not be typed. For example  is
derivable, but  is not.

The vector space axioms rules are the directed axioms of vector spaces
\cite{ArrighiDowekRTA08,AssafDiazcaroPerdrixTassonValironLMCS14}. The rule
 ensures that each vector space have only one null vector. The Modulo
AC rules are not proper rewrite rules, but express that we consider the symbol
 to be associative and commutative, and hence our rewrite system is {\em
  rewrite modulo AC}~\cite{PetersonStickelJACM81}. As a consequence, the
parenthesis are not needed and we may use the notation .
(for example, in rule \rproj).

Rule \rproj\ is the projection over weighted associative pairs, that
is, the projection over a generalization of multisets where the multiplicities
are given by complex numbers. This reduction rule is the only one with a
probability different from , and it is given by the square of the modulus of
the weights\footnote{We speak about weights and not amplitudes, since the vector
  may not have norm . The projection rule normalizes the vector while
  reducing.}, implementing this way the quantum measurement over the
computational basis.

Remark, to conclude, that this calculus can represent only pure states, and not
mixed states. For example, let  be an encoding for the quantum  gate
(cf.~Section~\ref{sec:examples}), , and . The terms  and  may be considered equivalent if taking into account the
density matrix representation of mixed states. Indeed, the first reduces either
to  or , with probability  each, while the second
reduces to  or to , with probability  each. The sets
of pure states  and  have both density matrix , and hence are
indistinguishable. However, once the result of the measure is known, the pure
states can be distinguished. A different approach, using density matrices, can
be seen in~\cite{DiazcaroAPLAS17}, however such a calculus has a linear type
system, and no algebraic reduction occurs.

\section{Multi-qubit systems: Tensor products}\label{sec:tensor}
\label{sec:TRS}

One postulate of quantum mechanics determines how to compose several quantum
systems. This way, the Hilbert space of a multi-qubit system is the tensor
product between single-qubit Hilbert spaces. If  and
 represent the states of two quantum systems, the state
of the full system composed by those two is . In particular, if we chose bases  and
 of  and  respectively, we can write
 and
, and so
. The last equality can be
seen as a matter of notation, but also it is clear that , and so . Therefore, since we already introduced a
symbol for the span of a type, and basis types, we only need to introduce an associative
Cartesian product to our calculus in order to recover the tensor product. For
example, the term  have type , while  have type .
Therefore, the distributivity of linear combinations over tensor products is not
trivially tracked in the type system, and so an explicit cast between types is
also added: The term  does not rewrite to , but the term  does, where  casts the type  into the type .


The grammar of types is given in Table~\ref{tab:types}, where the Cartesian
product is added to each level. The new level ``base qubit types'' (\bqtypes) is needed
since the abstractions with variables in \bqtypes\ are the non-linear ones.
We will use the following notation:  (-times).
Hence, . Also, we may use the notation .
\begin{table}
  \centering
  
  \caption{Grammar of types.}
  \label{tab:types}
\end{table}

The grammar of terms is given in Table~\ref{tab:terms}.
\begin{table}
  \centering
  
  \caption{Grammar of terms.}
  \label{tab:terms}
\end{table}

Each level in the term grammar (base terms, values and general terms) is
extended with the tensor of the terms in such a level. The primitives 
and  are added to the general terms. The projector  is generalized
to , where the subindex  stands for the number of qubits to be
measured, which are those in the first  positions.
Notice that it is always possible to do a swap between qubits and so place the
qubits to be measured at the beginning. For instance, .
Finally, an explicit type cast of a term  ( and ) is included in the
general terms. We may use just  to refer to any of  or
. As the product is associative, we also may use the notation  and  for associative Cartesian products.

The subtyping relation is also updated to include Cartesian products, and it is
given in Table~\ref{tab:Subtyping}.
\begin{table}[!h]
  \centering
  
  
  
  \caption{Subtyping relation.}
  \label{tab:Subtyping}
\end{table}

The updated type system, given in Table~\ref{tab:UTS}, includes all the typing
rules given in the previous section, plus the rules for tensor, for cast, and an
updated rule .

\begin{table}
  \centering
  
  \caption{Type system.}
  \label{tab:UTS}
\end{table}

Rules , , , , ,
, , , ,  and
 remain unchanged. Rule  types the generalized
projection: we force the term to be measured to be typed with a type of the form
, and then, after measuring the first  qubits, the new type becomes
, that is, we remove the superposition mark 
from the first  types in the tensor product. Rules  and  are updated
only to act on types  instead of just .

Rules ,  and  are the standard
introduction and eliminations for lists, however, the elimination is only
allowed on terms with type  (basis qubits). Rules  and
 type the castings. We only need to allow to cast a superposed type
into a superposed tensor product, thanks to the subtyping relation. Indeed, for
example, to cast  from type  to type , we can use the subtyping first to assign the type  to
.

To update the rewrite system, we need to update the function  to
include products, as follows.


The updated rewrite system is given in Table~\ref{tab:URS}. It includes all the
rules from Table~\ref{tab:RSNoX} plus the rules for lists:  and 
and the typing casts rules, which normalize superpositions to sums of base
terms, while update the types.

\begin{table}\centering
  \scalebox{.94}{
    \hspace{-3mm}\parbox{\textwidth}{\vspace{-4mm}
      
  \centering
  All the terms are considered to be closed (i.e. reduction is weak).
    }
  }
  \caption{Rewrite system.}
  \label{tab:URS}
\end{table}

The rule  has been updated to account for multiple qubits systems. It
normalizes (as in norm ) the scalars on the obtained term. The call-by-base
beta rule \rbetab, and the contextual rule admitting reducing the argument on an
application for the call-by-base abstraction are updated to allow for
abstractions expecting arguments of type  instead of just  (that is, any
base qubit type).


The first six rules in the group typing casts---\rdistsumr, \rdistscalr, and
\rdistzr, and their analogous \rdistsuml, \rdistscall, and \rdistzl---deal with
the distributivity of sums, scalar product and null vector respectively. If we
ignore the type cast  on each rule, these rules are just
distributivity rules. For example, rule \rdistsumr\ acts on the term , distributing the sum with respect to the tensor product, producing
 (distribution to the right). However, the term
 may have type ,  or
, while, among those, the term 
can only have type . Hence, we cannot reduce the first term to
the second without losing subject reduction. Instead, we need to cast the term
explicitly to the valid type in order to reduce.

The next two rules, \rdistcasum\ and \rdistcascal, distribute the cast over sums
and scalars. For example  reduces by rule 
to , and hence,
the distributivity rule can act.
The last two rules in the group, \rcaneutr\ and \rcaneutl, remove the cast when
it is not needed anymore. For example



The measurement rule \rproj\ is updated to measure the first  qubits. Hence,
a -qubits in normal form (that is, a sum of products of qubits with or without
a scalar in front), for example, the term

can be measured and will produce a -qubits where the
first  qubits are the same and the remaining are untouched, with its scalars
changed to have norm . In this 3-qubits example, measuring the first two can
produce either

or

The
probability of producing the first is
 and the probability of producing the second is
.


Remark, to conclude, that since the calculus presented in this paper is
call-by-base for the functions expecting a non-linear argument, it avoids a
well-known problem in others -calculi with a linear logic type system
including modalities. To illustrate this problem, consider the following typing
judgment:

If we allow to -reduce this term, we would obtain  which is not typable in the context . A standard
solution to this problem is illustrated in~\cite{Barber96}, where the terms that
can be cloned are distinguished by a mark, and used in a 
construction, while non-clonable terms are used in  abstractions. Since
this term will not beta reduce in our calculus, but project first, the problem
is not present neither in our case.


\section{Subject reduction}\label{sec:SR}
Thanks to the explicit casts, the resulting system has the Subject Reduction
property (Theorem~\ref{thm:SR}), that is, the typing is preserved by
weak-reduction (i.e.~reduction on closed terms). The proof of this theorem is
not trivial, specially due to the complexity of the system itself.

The two main lemmas in the proof, the generation lemma (Lemma~\ref{lem:generation}) and the substitution
lemma (Lemma~\ref{lem:substitution}), are stated below, together with a few paradigmatic cases of the proof.

We denote by  to the multiset of types in . For example,



\begin{lemma}
  \label{lem:lqBn}
  If , then 
\end{lemma}
\begin{proof}
  By rule inspection.
\end{proof}

\begin{lemma}\label{lem:noBasisPrec}
  If , then there exists  such that 
\end{lemma}
\begin{proof}
  Straightforward induction on the definition of .
\end{proof}

\begin{lemma}\label{lem:SAxBmE}
  If , then there exist  such that
   with  and .
\end{lemma}
\begin{proof}
  By induction on the derivation of .
\end{proof}
\begin{lemma}\label{lem:minlqA}
          For any type , we have .
        \end{lemma}
        \begin{proof}
          By cases over .
        \end{proof}
        \begin{lemma}\label{lem:minS}
          If , then .
        \end{lemma}
        \begin{proof}
          By induction on the derivation of .
        \end{proof}

        \begin{lemma}\label{lem:min}
          If  and , then .
        \end{lemma}
        \begin{proof}
          Let  be the derivation tree of  and  the
          derivation tree of , we proceed by induction on
          , where  is the size of the derivation tree.

          We only give three case.
          
          \begin{itemize}
          \item If  and both derivations end with rule
            , then , , 
            ,
            ,
            ,
            and  where  is defined only on  and 
            on .
            By the induction hypothesis, ,
            hence, .
          \item If ,  ends with  and  with
            , then 
            , ,
            , and , with .
            By the induction hypothesis,
            ,
            hence .
          \item If  ends with rule , then  with
            . By the induction hypothesis, we have
            , and by Lemma~\ref{lem:minS},
            .
            Hence, .
            \qed
          \end{itemize}
        \end{proof}
\begin{lemma}
  [Generation lemmas]~
  \label{lem:generation}
  \begin{itemize}
  \item If , then ,
    , and .
  \item If , then  and
    .
  \item If , then  and
    .
  \item If , then  and
    .
  \item If , then , with
    ,  and
    .
  \item If , then  and
    , with  and
    ,
    .
  \item If , then , with
    ,  and
    .
  \item If , then , , with  and .
    Moreover, the derivation trees of  and 
    are strictly smaller than the derivation tree of .
  \item If , then ,
    with ,  and
    . Moreover, the derivation tree of
     is strictly smaller than the derivation tree of
    .
  \item If , then one of the following possibilities happens:
    \begin{itemize}
    \item  and , with
      , or
    \item  and ,
      with .
    \end{itemize}
    In both cases,  and
    .
  \item If , then  and
    , with ,
     and .
  \item If , then , with
    ,  and
    .
  \item If , then , with
    ,  and
    .
  \item If , then , with
    ,  and
    .
  \item If , then , with
    ,  and
    .
  \end{itemize}
\end{lemma}
\begin{proof}
  First notice that if  is derivable, then 
  is derivable, with  and
   (because of rule ) and , (because of rule ). Notice also that those are the only typing
  rules changing the sequent without changing the term on the sequent. Rules
   and  and are straightforward to check. All
  the other rules are syntax directed: one
  rule for each term. Therefore, the lemma is proven by a straightforward rule
  by rule analysis.

  With an analogous reasoning, the condition on the derivation trees stated in cases
   and  are also straightforward.
  \qed
\end{proof}

\begin{corollary}[Simplification]\label{cor:simplification}~
  \begin{enumerate}
  \item If , then  and .
  \item If , then .
  \item If , then .
  \item If , then .
  \item If , then .
  \end{enumerate}
\end{corollary}
\begin{proof}
  Cf.~\ref{ap:SR}.\qed
\end{proof}

\begin{corollary}
  \label{cor:basisTermNoSup}
  If  and , then .
\end{corollary}
\begin{proof}
  By cases analysis on . Cf.~\ref{ap:SR}. \qed
\end{proof}

\begin{lemma}
  \label{lem:linearContext}
  If  and , then .
\end{lemma}
\begin{proof}
  If  then . If , the only way
  to derive  is by using rule  to form , hence
  .
  \qed
\end{proof}

\begin{lemma}[Substitution lemma]\label{lem:substitution}
  Let , then if , ,
  where if  then , we have .
\end{lemma}
\begin{proof}
  Notice that due to Lemma~\ref{lem:linearContext}, ,
  hence, it suffices to consider . By structural
  induction on . Cf.~\ref{ap:SR}.
  \qed
\end{proof}

Since the strategy is weak, subject reduction is proven for closed terms.

\begin{theorem}
  [Subject reduction on closed terms]
  \label{thm:SR}
  For any closed terms  and  and type , if  and , then .
\end{theorem}
\begin{proof}
  By induction on the rewrite relation. Cf.~\ref{ap:SR}.
  \qed
\end{proof}

\section{Strong normalization}\label{sec:SN}
In this section we adapt Tait's proof of strong normalization of the simply
typed lambda calculus to show the same property in our calculus.

Let  be the size of the longest reduction sequence started in  and
. Also, let  of type , then . Notice that Theorem~\ref{thm:SR} proves the Subject Reduction only
for closed terms, that is why the definition of  requires a condition on types.

\begin{definition}
  We define the following measure  on terms:
  
\end{definition}

\begin{lemma}\label{lem:size}
  If  by any of the rules in the groups linear distribution,
  vector space axioms or lists, or their contextual closure, then . Moreover,  if and only if the rule is .
\end{lemma}
\begin{proof}
  By induction and rule by rule analysis. Cf.~\ref{ap:SN}.
  \qed
\end{proof}

\begin{lemma}\label{lem:ri_in_snset_implies_sum_ri_in_snset}
  If for every  we have ,
  then .
\end{lemma}
\begin{proof}
  By induction on the lexicographic order of . Cf.~\ref{ap:SN}. \qed
\end{proof}

\begin{lemma}\label{lem:t_implies_proj_t}
  If , then .
\end{lemma}

\begin{proof}
  By induction on . Cf.~\ref{ap:SN}. \qed
\end{proof}

From now on,  where  can be determined by the context.

As usual, we associate to each type  a set of strongly normalising terms
. However, since reduction depends on types, these sets must be
sets of typed terms, otherwise we would need to consider ill-typed reductions,
which would make the proof more complex.
\begin{definition}
  For each type  we define a set of strongly normalising terms as follows:
  
\end{definition}

We define a set of neutral terms (Definition~\ref{def:neut}), in order to prove
that for every type, its interpretation have the so-called CR3 property
(Lemma~\ref{lem:cr}), that is, the closure by anti-reduction of neutral terms.
Such a property will be useful to prove the adequacy lemma
(Lemma~\ref{lem:adequacy}).
\begin{definition}\label{def:neut}
  The set of neutral terms ( is defined by the following
  grammar:
  
  where  is any term produced by the grammar from Table~\ref{tab:terms}.
\end{definition}

\begin{lemma}\label{lem:cr}
  For all , the following properties hold:
  \begin{description}
    \item[(CR1)] If , then .
    \item[(CR2)] If , then .
    \item[(CR3)] If ,  has the same type as all the
      terms in , and  then .
    \item[(HAB)] For all , .
    \item[(LIN1)] If  and , then .
    \item[(LIN2)] If  then .
    \item[(NULL)] 
  \end{description}
\end{lemma}
\begin{proof}
  By induction over . Cf.~\ref{ap:SN}. \qed
\end{proof}

\begin{lemma}\label{lem:a_subset_b}
  If  then .
\end{lemma}
\begin{proof}
  By induction on the relation . Cf.~\ref{ap:SN}. \qed
\end{proof}

Let  be a substitution of variables by terms. We write
 if for every , .

\begin{lemma}[Adequacy]\label{lem:adequacy}
  If  and  then .
\end{lemma}
\begin{proof}
  By induction in the derivation of . Cf.~\ref{ap:SN}. \qed
\end{proof}

\begin{theorem}
  [Strong normalization]\label{thm:SN}
  If  then .
\end{theorem}
\begin{proof}
  By Lemma~\ref{lem:adequacy}, if , then
  . By Lemma~\ref{lem:cr} (CR1), .
  Finally, by Lemma~\ref{lem:cr} (HAB), , hence .
  \qed
\end{proof}

\section{Interpretation}\label{sec:denSem}

We consider vector spaces equipped with a canonical base, and subsets of such
spaces.

Let  and  be two vector spaces with canonical bases  and . The tensor product  of
 and  is the vector space of canonical base , where 
is the ordered pair formed with the vector  and the vector
.
The operation  is extended to the vectors of  and  by making
pairs bilinear:
.

Let  and  be two vector spaces equipped with bases  and , and 
and  be two subsets of  and  respectively, we define the set , subset of the vector space , as follows: .
Remark that  differs from . For instance, if  and
 are  equipped with the base , then  contains  and  but not
, that is not a Cartesian product of
two vectors of .
Let  be a vector space equipped with a base , and  a subset of . We
write  for the vector space over  generated by the span of
, that is, containing all the linear combinations of elements of .
Hence, if  and  are two vector spaces of bases  and  then .

Let  and  be two sets. We write  for the vector space of formal
linear combination of functions from  to . The set
 the set of
the functions from  to  is a subset---and even a basis---of this vector
space.
Note that if  and  are two sets , then .

To each type we associate the subset of some vector space

Remark that
.

If  is a context, then a -valuation
is a function mapping each  to .

We now would associate to each term  of type  an element  of . But as our calculus is probabilistic, due to the presence of a measurement
operator, we must associate to each term a set of elements of .


Let  be a term of type  in  and  a -valuation. We
define the interpretation of ,  as follows.



\begin{lemma}
  \label{lem:inc}
  If , then .
\end{lemma}
\begin{proof}
  By induction on the relation . Cf.~\ref{ap:denSem}. \qed
\end{proof}

\begin{lemma}
  \label{lem:subsDen}
  If  and  is a
  -valuation, then .
\end{lemma}
\begin{proof}
  By induction on . Cf.~\ref{ap:denSem}. \qed
\end{proof}

\begin{theorem}\label{thm:soundness}
  If , and  is a -valuation. Then .
\end{theorem}
\begin{proof}
  By induction on the typing derivation. Cf.~\ref{ap:denSem}. \qed
\end{proof}

\begin{theorem}\label{thm:denRed}
  If ,  is a -valuation, and ,
  with , then .
\end{theorem}
\begin{proof}
  We proceed by induction on the rewrite relation.
  \begin{description}
  \item[\rbetab\ and \rbetan] Let , with , where, if , then . Then by
    Lemma~\ref{lem:generation}, one of the following possibilities happens:
    \begin{enumerate}
    \item  and , with
      . Thus, .
    \item  and ,
      with . Thus, .
    \end{enumerate}
    In any case, by Lemma~\ref{lem:subsDen}, .
  \item[Other cases] All the remaining cases are straightforward by the
    algebraic nature of the interpretation.
    \qed
  \end{description}
\end{proof}

\section{Examples}\label{sec:examples}
In this section we show that our language is expressive enough to express the
Deutsch algorithm (Section~\ref{ex:deut}) and the Teleportation algorithm.
(Section~\ref{ex:telep}).

\subsection{Deutsch algorithm}\label{ex:deut}
The Deutsch algorithm tests whether the binary function  implemented by the
oracle  is constant () or balanced (). The
algorithm is as follows: it starts with a qubit in state  and another in
state , and applies Hadamard gates to both. Then it applies the 
operator, followed by a Hadamard and a measurement to the first qubit. When the
function is constant, the first qubit ends in , when it is balanced, it
ends in .


The Hadamard gate () produces  when
applied to  and  when applied
to . Hence, it can be implemented with the if-then-else construction:
. Notice that the abstracted variable has a base type
(i.e.~non-linear). Hence, if  is applied to a superposition, say
, it reduces, as expected, in the following
way:

and then is applied to the base terms. We define  as the function taking
a two-qubits system and applying  to the first. . Similarly,  applies  to both qubits.
 The gate
 is called {\em oracle}, and it is defined by  where  is the addition modulo . In order to implement it, we
need the  gate, which can be implemented similarly to the Hadamard
gate:

Then, the  gate is implemented by:

where  is a given term of type .

Finally, the Deutsch algorithm combines all the previous definitions:


The casts after the Hadamards are needed to fully develop the qubits and then be
able to use it as an argument of a non-linear abstraction (i.e.~an abstraction
expecting for base terms and so linear-distributing over superpositions). The
 term is typed, as expected, by .

This term, on the identity function, reduces as follows:


The trace on this reduction and the type derivation are given in~\ref{ap:Deutsch}.

\subsection{Teleportation algorithm}\label{ex:telep}

The circuit for this algorithm is given in Figure~\ref{fig:telep}.
\begin{figure}[t]
  \begin{center}
    \begin{tikzpicture}
      \node[anchor=west] at (0,0) { };
      \node[anchor=east] at (0,.9) {}; \node[anchor=west] at (5,-.85)
      {}; \node[anchor=east] at (0,-.4) {}; \draw[dashed]
      (.3,-.3) -- (4.2,-.3) -- (4.2,1.2) -- (.3,1.2) -- (.3,-.3); \node at
      (4.7,1.1) {Alice}; \draw[dashed] (.3,-.5) -- (4.7,-.5) -- (4.7,-1.2) --
      (.3,-1.2) -- (.3,-.5); \node at (4.8,-.3) {Bob};
    \end{tikzpicture}
  \end{center}
  \caption{Teleportation circuit}
  \label{fig:telep}
\end{figure}

The  gate, which applies  to the second qubit only
when the first qubit is , can be implemented with an if-then-else
construction as follows:


We define  to apply  to the first qubit of a three-qubit system.


Remark that the only difference with  is the type of the abstracted
variable. In addition, we need to apply  to the two first qubits, so we
define  as


The  gate returns  when it receives , and  when it
receives . Hence, it can be implemented by:


The Bob side of the algorithm will apply  and/or  according to
the bits it receives from Alice. Hence, for any  or , we define  to be the
function which depending on the value of a base qubit  applies the 
gate or not:


Alice and Bob parts of the algorithm are defined separately:


Notice that before passing to  the parameter of type
, we need to fully develop the term using the two
casts, and again, after the Hadamard gate. Bob side is implemented by


The teleportation is applied to an arbitrary qubit and to the following Bell
state
 and it is defined by:


This term is typed, as expected, by:  and applying the teleportation to any superposition  will reduce, as expected, to . The trace on this reduction and the type derivation are
given in~\ref{ap:telep}.

\section{Conclusion}\label{sec:conclusion}

In this paper we have proposed a way to unify logic-linear and algebraic-linear
quantum -calculi, by interpreting -terms as linear functions
when they expect duplicable data and as non-linear ones when they do not, and
illustrated this idea with the definition of a calculus.

This calculus is first-order in the sense that variables do not have functional
types. In a higher-order version we should expect abstractions to be clonable.
But, allowing cloning abstractions allows cloning superpositions, by hiding them
inside. For example, . It has been argued
\cite{ArrighiDowekRTA08,ArrighiDiazcaroValironIC17} that what is cloned is not
the superposition but a function that creates the superposition, because we had
no way there to create such an abstraction from an arbitrary superposition. The
situation is different in the calculus presented in this paper as the term
 precisely takes any term  of type
 and returns the term . So, a cloning machine could be
constructed by encapsulating any superposition  under a lambda, which
transform it into a basis term, so a clonable term. Extending this calculus to
the higher-order will require characterizing precisely the abstractions that can
be taken as arguments, not allowing to duplicate functions creating
superpositions.

\subsubsection*{Acknowledgements}
We would like to thank Eduardo Bonelli, Octavio Malherbe, Luca Paolini, Simona
Ronchi della Rocca and Luca Roversi for interesting comments and suggestions in
an early draft of this work.


\bibliographystyle{elsarticle-num} \bibliography{biblio}

\newpage
\appendix


\section{Detailed proofs of Section~\ref{sec:SR} (Subject reduction)}\label{ap:SR}
\xrecap{Corollary}{Simplification}{cor:simplification}{
  \begin{enumerate}
  \item If , then  and .
  \item If , then .
  \item If , then .
  \item If , then .
  \item If , then .
  \end{enumerate}
}
\begin{proof}
  ~
  \begin{enumerate}
  \item By Lemma~\ref{lem:generation},  and , with
    , then, we conclude by rule .
  \item By Lemma~\ref{lem:generation},  and , with
    , but then, by Lemma~\ref{lem:noBasisPrec}, 
    for some type .
  \item By Lemma~\ref{lem:generation}, , with , then,
    we conclude by rule .
  \item By Lemma~\ref{lem:generation}, , with , then
    we conclude by rules  and .
  \item By Lemma~\ref{lem:generation},  with , but
    then, by Lemma~\ref{lem:noBasisPrec},  for some type .
    \qed
  \end{enumerate}
\end{proof}

\recap{Corollary}{cor:basisTermNoSup}{If  and , then .}
\begin{proof}
  We proceed by cases on .
  \begin{itemize}
  \item Let . Then, by Lemma~\ref{lem:generation},
    , with , and so
    , and we conclude by rule .
  \item Let . Then, by Lemma~\ref{lem:generation}, ,
    hence  and we conclude by rule .
  \item Let . Analogous to previous case.
  \item Let . Then, by Lemma~\ref{lem:generation}, , , and . Hence,
     and we conclude by rule .
    \qed
  \end{itemize}
\end{proof}

\xrecap{Lemma}{Substitution lemma}{lem:substitution}{
  Let , then if , ,
  where if  then , we have .
}
\begin{proof}
  Notice that due to Lemma~\ref{lem:linearContext}, ,
  hence, it suffices to consider . We proceed by structural
  induction on .

  The set of terms be divided in the following groups:
  
  Hence, we can consider the terms by groups:
  \begin{description}
  \item[\s{unclassified} terms] ~
    \begin{description}
    \item[.] By Lemma~\ref{lem:generation}, 
       and . Since , we have
      . Hence, since , by rule ,
      . Finally, since , by rule ,
      we have .
    \item[.] By Lemma~\ref{lem:generation}, ,
       and . Hence, by rule , . Since
      , by rule , we have .
      Finally, since , we have .
    \item[.] Without loss of generality, assume  is
      does not appear free in . By Lemma~\ref{lem:generation},
      , with ,
       and
      . By the induction
      hypothesis, , with
      . Notice that if ,
      the induction hypothesis applies directly, in other case, 
      and so by rule  the context can be enlarged to include , hence
      the induction hypothesis applies in any case. Therefore, by rule
      , . Since , by rule ,
      . Hence, since
      , by rule ,
      . Since  does not appear free in
      , . Therefore,
      .
    \end{description}
  \item[ terms] All of these terms are typed by an axiom with a
    type  which, by Lemma~\ref{lem:generation}, . Also, by the
    same Lemma, . So, we can type with the axiom,
    and empty context, , and so, by rule ,
    . Notice that . We
    conclude by rule .
  \item[ terms] By Lemma~\ref{lem:generation}, , such that by a derivation tree , ,
    where ,
     and . If
    , then  and so we can extend  with
    . Hence, in any case, by the induction hypothesis,
    . Then, using the derivation tree
    , . Notice that
    . We conclude by rules  and
    .

  \item[ terms] By Lemma~\ref{lem:generation},
     and , such that by a typing rule R,
    , with , and where
     and
    .
    Therefore, if , , we can extend  with
     using rule . Hence, by the induction hypothesis,
     and
    . So, by rule R,
    . Notice that
    . We conclude by rules
     and .
    \qed
  \end{description}
\end{proof}

\xrecap{Theorem}{Subject reduction on closed terms}{thm:SR}{
  For any closed terms  and  and type , if  and , then .
}
\begin{proof}
  We proceed by induction on the rewrite relation.
  \begin{description}
  \item[\rbetab\ and \rbetan] Let , with , where, if , then . Then by
    Lemma~\ref{lem:generation}, one of the following possibilities happens:
    \begin{enumerate}
    \item\label{it:caseEbeta}  and
      , with , or
    \item\label{it:caseESbeta} 
      and , with .
    \end{enumerate}
    Thus, in any case, by Lemma~\ref{lem:generation} again, ,
    with, in case \ref{it:caseEbeta},  and in case \ref{it:caseESbeta}, . Hence,  and in the first case , while in the second, , so, in
    general . Since , where if , then
    , by Lemma~\ref{lem:substitution}, , and by
    rule , .

  \item[\riftrue] Let . Then, by
    Lemma~\ref{lem:generation}, one of the following possibilities happens:
    \begin{itemize}
    \item  and , with
      .
      Then, by
      Lemma~\ref{lem:generation} again,
      ,  and . Hence,  and .
    \item  and ,
      with . Then, by Lemma~\ref{lem:generation} again,
      ,  and . Hence,  and .
    \end{itemize}
    So, by rule , .
  \item[\riffalse] Analogous to case \riftrue.
  \item[\rlinr] Let , with . Then,
    by Lemma~\ref{lem:generation}, one of the following cases happens:
    \begin{enumerate}
    \item  and , with . However, since , we have ,
      which is impossible due to Corollary~\ref{cor:simplification}.
    \item  and , with
      . Then, by Corollary~\ref{cor:simplification},  and . Hence,
      
    \end{enumerate}
  \item[\rlinscalr] Let , with .
    Then, by Lemma~\ref{lem:generation}, one of the following cases happens:
    \begin{enumerate}
    \item  and , with . However, since , we have ,
      which is impossible due to Corollary~\ref{cor:simplification}.
    \item  and , with
      . Then, by Corollary~\ref{cor:simplification}, . Hence,
      
    \end{enumerate}
  \item[\rlinzr] Let , with . Then, by
    Lemma~\ref{lem:generation}, one of the following cases happens:
    \begin{enumerate}
    \item  and , with .
      Then, by Lemma~\ref{lem:generation} again, . However,
      since , , which is impossible by
      Lemma~\ref{lem:noBasisPrec}.
    \item  and , with
      . By rule , .
      Since  and , by
      Lemma~\ref{lem:min}, we have , so
      . Then, by Lemma~\ref{lem:minlqA}, , then , so we conclude by rule .
    \end{enumerate}
  \item[\rlinl] Let . Then by Lemma~\ref{lem:generation}, one
    of the following cases happens:
    \begin{enumerate}
    \item , which is impossible by
      Corollary~\ref{cor:simplification}.
    \item  and , with
      . Then, by Corollary~\ref{cor:simplification},  and . Hence,
      
    \end{enumerate}
  \item[\rlinscall] Let . Then, by
    Lemma~\ref{lem:generation}, one of the following cases happens:
    \begin{enumerate}
    \item , which is impossible by
      Corollary~\ref{cor:simplification}.
    \item  and , with
      . Then, by Corollary~\ref{cor:simplification}, . Hence,
      
    \end{enumerate}
  \item[\rlinzl] Let . Then, by
    Lemma~\ref{lem:generation}, one of the following cases happens:
    \begin{enumerate}
    \item  and , with
      . Then, by Lemma~\ref{lem:generation} again, , which is impossible by
      Lemma~\ref{lem:noBasisPrec}.
    \item  and ,
      with . By Lemma~\ref{lem:generation} again, .
      By Lemma~\ref{lem:min}, , so ,
      and by Lemma~\ref{lem:minlqA}, , hence, 
      , and then . By rule , , hence we
      conclude by rule .
    \end{enumerate}
  \item[\rneut] Let . Then, by
    Corollary~\ref{cor:simplification}, .
  \item[\runit] Let . Then, by Corollary~\ref{cor:simplification},
    .
  \item[\rzeros] Let , with . Then, we must show that .
    
    By Lemma~\ref{lem:generation},  and .
    By Lemma~\ref{lem:min}, . By
    Lemma~\ref{lem:minlqA}, . Therefore,
    .
    By rule , ,
    hence we conclude by rule .
  \item[\rzero] Let .
    By Lemma~\ref{lem:generation},  with .
    Then, by Lemma~\ref{lem:generation} again, .
    In addition, by Lemma~\ref{lem:minlqA}, .
    Therefore, .
    Since, by rule , , we
    conclude by rule  that .
  \item[\rprod] Let . By
    Corollary~\ref{cor:simplification}, . Then, by
    Corollary~\ref{cor:simplification} again, .
  \item[\rdists] Let . By Lemma~\ref{lem:generation},
    , with . Then, by
    Corollary~\ref{cor:simplification},  and . Hence, by
    rule ,  and . We
    conclude by rules  and .
  \item[\rfact] Let . By
    Corollary~\ref{cor:simplification}, . Then, by
    Corollary~\ref{cor:simplification} again, .
  \item[\rfacto] Let . By
    Corollary~\ref{cor:simplification}, . Then, by
    Corollary~\ref{cor:simplification} again, .
  \item[\rfactt] Let . By Lemma~\ref{lem:generation},
    , with . Then, by rule , . We conclude by rule .
  \item[\rzeroS] Let . Then, by Lemma~\ref{lem:generation},
    .
    By Lemma~\ref{lem:minlqA}, , hence
    .
    By rule , , and
    since , we conclude by by rule
    .
  \item[\rcomm] Let . By Lemma~\ref{lem:generation},  and , with . So,
    
  \item[\rassoc] Let . By Lemma~\ref{lem:generation},
     and , with . Then, by
    Corollary~\ref{cor:simplification},  and . Hence,
    
  \item[\rhead] Let , with . Hence, by
    Lemma~\ref{lem:generation}, , with . Then, by Lemma~\ref{lem:generation} again,  and , with .
    Lemma~\ref{lem:lqBn}   
    , so .
    Since , by Lemma~\ref{lem:generation},  is not a product,
    and so . Therefore, we conclude by rule .
  \item[\rtail] Analogous to case \rhead.
  \item[\rdistsumr] Let . By Lemma~\ref{lem:generation},  and
    . Then, by the same Lemma,
     and , with , so by Lemma~\ref{lem:SAxBmE}, there exists  such that
     and  and .
    Therefore,  and .
    Since , by Lemma~\ref{lem:generation}, there exists 
    such that , so by transitivity . Then, by
    Lemma~\ref{lem:noBasisPrec},  has the form .
    Therefore, neither  nor  are products, and hence  and
    .
    Hence,  and , and hence, 
    and . Then, by Corollary~\ref{cor:simplification},  and . Therefore,
    
  \item[\rdistsuml] Analogous to case \rdistsumr.
  \item[\rdistscalr] Let . By Lemma~\ref{lem:generation}, , and . Then, by
    the same Lemma,  and , with , so by Lemma~\ref{lem:SAxBmE}, there exists  such that
     and  and .
    Therefore,  and .
    Since , by Lemma~\ref{lem:generation}, there exists 
    such that , so by transitivity . Then, by
    Lemma~\ref{lem:noBasisPrec},  has the form .
    Therefore, neither  nor  are products, and hence  and
    .
    Hence,  and , so by rule
    ,  and . By
    Corollary~\ref{cor:simplification}, . Therefore,
    
  \item[\rdistscall] Analogous to case \rdistscalr.
  \item[\rdistzr] Let .
    By Lemma~\ref{lem:generation}, . 
    By lemma~\ref{lem:minlqA}, ,
    hence, .
    By rule , .
    Hence, since , we conclude by rule
    .
  \item[\rdistzl] Analogous to case \rdistzr.
  \item[\rdistcasum] Let . Then, by
    Lemma~\ref{lem:generation}, ,  and . We
    conclude by rules  and .
  \item[\rdistcascal] Let . Then, by
    Lemma~\ref{lem:generation}, , and . We conclude by rules  and
    .
  \item[\rcaneutr] Let ,
    with . Then, by Lemma~\ref{lem:generation},  and . Then, by
    Lemma~\ref{lem:generation} again,  and , with
    .
    Without lost of generality, let  where each  is not
    a product. Then, by Lemma~\ref{lem:generation},  with
    , and, by the same lemma,  are not
    products.
    Therefore, , so by Lemma~\ref{lem:SAxBmE}, there exists  such that
     and  and .
    Therefore,  and .
    Since  is not a product,  is not a product. Hence, since 
    neither  nor  are products, we have  and .
     and ,
    hence,  and . Therefore, by
    Corollary~\ref{cor:basisTermNoSup}, , and so, by rule
    , , and by rule , .
  \item[\rcaneutl] Analogous to case .
  \item[\rproj] Let .
    Then, by Lemma~\ref{lem:generation}, we have that
    . Hence, we have the derivation from Figure~\ref{fig:deriv}.
    \begin{figure}[t]
      \centering
      
      \caption{Derivation from case  on Theorem~\ref{thm:SR}.}
      \label{fig:deriv}
    \end{figure}

  \item[Contextual rules]~ Let . Then,
    \begin{description}
    \item[()] Let . By Lemma~\ref{lem:generation}, one
      of the following cases happens:
      \begin{itemize}
      \item  and , with .
        Then, by the induction hypothesis, . We
        conclude by rules  and .
      \item  and , with
        . Then, by the induction hypothesis, . We conclude by rules  and
        .
      \end{itemize}
    \item[()] Let . By Lemma~\ref{lem:generation}, one of the following cases
      happens:
      \begin{itemize}
      \item  and , with
        . Then, by the induction hypothesis, . We
        conclude by rules  and .
      \item  and , with . Then, by the induction hypothesis,
        . We conclude by rules  and
        .
      \end{itemize}
    \item[()] Let . By
      Lemma~\ref{lem:generation},  and , with
      . Then, by the induction hypothesis, . We
      conclude by rules  and .
    \item[()] Let . By
      Lemma~\ref{lem:generation}, , with . Then,
      by the induction hypothesis, . We conclude by rules
       and .
    \item[()] Let . By
      Lemma~\ref{lem:generation}, , and  Then, by the induction hypothesis, . We conclude by rules  and .
    \item[()] Let . By
      Lemma~\ref{lem:generation},  and , with . Then, by the induction hypothesis, . We conclude
      by rules  and .
    \item[()] Analogous to previous case.
    \item[()] Let . By
      Lemma~\ref{lem:generation}, , and . Then, by the induction hypothesis, , and so, by rule , . We
      conclude by rule .
    \item[()] Let . By
      Lemma~\ref{lem:generation}, , and . Then, by the induction hypothesis, , and so, by rule , . We
      conclude by rule .
    \item[()] Let . By
      Lemma~\ref{lem:generation}, , with .
      Then, by the induction hypothesis, . We conclude by
      rules  and .
    \item[()] Let . By
      Lemma~\ref{lem:generation}, , with .
      Then, by the induction hypothesis, . We conclude by
      rules  and .
      \qed
    \end{description}
  \end{description}
\end{proof}

\section{Detailed proofs of Section~\ref{sec:SN} (Strong normalisation)}\label{ap:SN}
\recap{Lemma}{lem:size}{
  If  by any of the rules in the groups linear distribution,
  vector space axioms or lists, or their contextual closure, then . Moreover,  if and only if the rule is .
}
\begin{proof}
  By induction and rule by rule analysis:
 \begin{description}
 \item[\rlinr:] .
   
 \item[\rlinscalr:] 
   
 \item[\rlinzr:] 
   
 \item[\rlinl:] 
   
 \item[\rlinscall:] 
   
 \item[\rlinzl:] 
   
 \item[\rneut:] 
   
 \item[\runit:] 
   
 \item[\rzeros:] 
   
 \item[\rzero:] 
   
 \item[\rprod:] 
   
 \item[\rdists:] 
   
 \item[\rfact:] 
   
 \item[\rfacto:] 
   
 \item[\rfactt:] 
   
 \item[\rzeroS:] 
   

 \item[\rhead:] 
   
 \item[\rtail:] 
   
 \item[Contextual rules:] If , then, by the induction hypothesis,
   , and hence, , where  is
   a context with one hole.
    \qed
  \end{description}
\end{proof}

\recap{Lemma}{lem:ri_in_snset_implies_sum_ri_in_snset}{
  If for every  we have ,
  then .
}
\begin{proof}
  Induction on the lexicographic order of  to show that . Let .
  The possibilities are:
  \begin{itemize}
  \item  where for all ,  and . Since , we conclude by the induction
    hypothesis.
  \item  where for all ,  and .
    Then, the reduction , is by one of the following
    rules: \runit, \rzeros, \rzero, \rprod, or \rdists.
    In any of these cases  and, by Lemma~\ref{lem:size}, . Hence, we conclude by the induction
    hypothesis.
  \item , where  (rule
    \rfact, \rfacto, or \rfactt). In this case  and by Lemma~\ref{lem:size},
    . Hence, we conclude by the
    induction hypothesis. \qed
  \end{itemize}
\end{proof}

\recap{Lemma}{lem:t_implies_proj_t}{
  If , then .
}
\begin{proof}
  We show by induction on  that .
  Let . The possibilities are:
  \begin{itemize}
  \item  where . Since 
    and , we conclude by the induction hypothesis.
  \item , .
    Any sequence starting on  will only use vector space axioms rules, which,
    by Lemma~\ref{lem:size} reduce the size of the term, except for
    \rzeroS, which anyway can be used only a finite number of times. Therefore, .
    \qed
  \end{itemize}
\end{proof}

\recap{Lemma}{lem:cr}{
  For all , the following properties hold:
  \begin{description}
    \item[(CR1)] If , then .
    \item[(CR2)] If , then .
    \item[(CR3)] If ,  has the same type as all the
      terms in , and  then .
    \item[(HAB)] For all , .
    \item[(LIN1)] If  and , then .
    \item[(LIN2)] If  then .
    \item[(NULL)] 
  \end{description}
}
\begin{proof}
  We proceed by induction over .
  \begin{itemize}
    \item Let .
    \begin{description}
      \item[CR1]
       Let . By definition, , so .
      \item[CR2]
       Let . By definition, , so  and . Furthermore,
      since , we have . Let , then . Therefore, by definition, .
      \item[CR3]
       Let  and  where .
      Since , we have .
      Therefore, by definition, .
      \item[HAB]
       Since  and , we have, by definition, .
      \item[LIN1]
       Since  and , we have, by definition of , that , ,  and . Then,  and, by Lemma~\ref{lem:ri_in_snset_implies_sum_ri_in_snset}, . Therefore, by definition, .
      \item[LIN2]
       Since , we have, by definition of  that  and . Then,  and, by Lemma~\ref{lem:ri_in_snset_implies_sum_ri_in_snset}, . Therefore, by definition, .
      \item[NULL]
       Since  and , we have by definition of  that .
    \end{description}
    \item Let 
    \begin{description}
      \item[CR1]
         Since , we have by definition that .
      \item[CR2]
         Since , we have . Let . Since ,
          we have that . On the other hand, since , we have that . Then, .
         Therefore, by definition, , which means 
      \item[CR3]
         Let  and  where .
        Since , we have .
        Therefore, by definition, .
      \item[HAB]
         Since  and , we have by definition that .
      \item[LIN1]
         Since , we have that  and . Similarly, we have that  and . Then,  and, by Lemma~\ref{lem:ri_in_snset_implies_sum_ri_in_snset}, .
         Therefore, by definition, .
      \item[LIN2]
         Since , we have that  and . Then,  and, by Lemma~\ref{lem:ri_in_snset_implies_sum_ri_in_snset}, .
         Therefore, by definition, .
      \item[NULL]
         Since  and , we have by definition that .
    \end{description}
    \item Let 
    \begin{description}
      \item[CR1]
         Given , we want to show that . Let  (note that by induction hypothesis (HAB), such  exists). By definition, we have that .
        And by induction hypothesis, we have that , which means,  is finite.
        And since , we have that  is finite, and therefore, .
      \item[CR2]
         Given , we want to show that , which means that given , . By definition of , this is the same as showing that  and, for all , .
         Since , we have by definition
        that, for all , . And by induction hypothesis, this implies that, for all , . In
        particular, given , we have that, for all , . And since , we
        have by definition that . Since
        , we have that .
      \item[CR3]
         Given  and  where , we want to show that
        . By definition, this is the same
        than showing that for all , . By induction hypothesis, it suffices to show that for all
        , . Notice
        that if , then , and since
        , we have .
        Let . By induction hypothesis (CR1), we have that , which means  exists. Therefore, we can proceed by induction (2) over .
        We analyze the reducts of :
        \begin{itemize}
          \item  where 
             Since , we have that . And since , we have by definition that .
          \item  where 
             Since , we have by induction hypothesis (CR2) that . And since , we have by induction hypothesis (2) that .
          \item 
             Since  and, by Lemma~\ref{lem:size}, , we have by induction hypothesis (2) that . Similarly, we have that . Therefore, by Lemma~\ref{lem:cr} (LIN1), we have that .
          \item 
             Since  and, by Lemma~\ref{lem:size}, , we have by induction hypothesis (2) that . Therefore, by Lemma~\ref{lem:cr} (LIN2), we have that .
          \item 
             By Lemma~\ref{lem:cr} (NULL), we have that .
             By Lemma~\ref{lem:minlqA},
             , and , hence,
             by definition, .
        \end{itemize}
      \item[HAB]
         By definition of , and since , it suffices to show that, for all , we have that .
         Let . Since , it suffices to show that . Since , we have by induction hypothesis (CR1) that . Therefore, we can proceed by induction (2) over . We analyze the possible reducts of :
        \begin{itemize}
          \item 
             Since  and, by Lemma~\ref{lem:size}, , we have by induction hypothesis (2) that . Similarly, we have that . Therefore, by induction hypothesis (LIN1), we have that .
          \item 
             Since  and, by Lemma~\ref{lem:size}, , we have by induction hypothesis (2) that . Therefore, by induction hypothesis (LIN2), we have that .
	   \item .
             By induction hypothesis (NULL), we have that .
             By Lemma~\ref{lem:minlqA},
             , and , hence,
             by definition, .
        \end{itemize}
      \item[LIN1]
         By definition of , it suffices to show that  and, for all , .
         Since  and , we have that ,  and, for all ,  and . Therefore,  and .
         It remains to show that, for all , . Since  and, by type derivation, , we have by induction hypothesis (CR3) that it is sufficient to show that, for all , . Since , , y , we have by induction hypothesis (CR1) that ,  y . Therefore, we can proceed by induction (2) over . We analyze the possible reducts of :
        \begin{itemize}
          \item  where 
           Since , we have by induction hypothesis that .
          \item  where 
           Analogous to the previous case.
          \item 
           Since  and , we have by induction hypothesis (2) that . Similarly, we have that . Therefore, by induction hypothesis, we have that .
          \item 
           Since  and , we have by induction hypothesis (2) that . Therefore, by induction hypothesis (LIN2), we have that .
          \item 
             By induction hypothesis (NULL), we have that .
             By Lemma~\ref{lem:minlqA},
             , and , hence,
             by definition, .
          \item 
           Since  and , we have by induction hypothesis that .
        \end{itemize}
      \item[LIN2]
         By definition of , it suffices to show that  and, for all , .
         Since , we have that  and, for all , . Therefore,  and .
         It remains to show that, for all , . Since , we have by induction hypothesis (CR3) that it is sufficient to show that, for all , . Since  and , we have by induction hypothesis (CR1) that  and . Therefore, we can proceed by induction (2) over . We analyze the possible reducts of :
        \begin{itemize}
          \item  where 
           Since , we have by induction hypothesis (2) that .
          \item 
           Since  and , we have by induction hypothesis that . Similarly, . Therefore, by induction hypothesis (LIN1), .
          \item 
           Since  and , we have by induction hypothesis (2) that . Therefore, by induction hypothesis, .
          \item 
             By induction hypothesis (NULL), we have that .
             By Lemma~\ref{lem:minlqA},
             , and , hence,
             by definition, .
          \item 
           Since , we have by induction hypothesis that .
        \end{itemize}
      \item[NULL]
         We want to show that . By definition of , this is
equivalent to showing that, for all , . Since  and , we have by induction hypothesis
(CR3) that this is equivalent to showing that . Since the only possible reduct of
 is , it suffices to
show that .
             By induction hypothesis, we have that .
             By Lemma~\ref{lem:minlqA},
             , and , hence,
             by definition, .
    \end{description}
    \item Let 
    \begin{description}
      \item[CR1]
         Since , we have by definition that .
      \item[CR2]
         Given , we want to show that .
        By definition, , and so, . Therefore, .
      \item[CR3]
         Given  where  and , we want to show that .
        By definition, , and so, . Therefore, by definition, .
      \item[HAB]
       Since  and , we have by definition that .
      \item[LIN1]
         Since  and , we have by definition of  that , ,  and . Therefore,  and, by Lemma~\ref{lem:ri_in_snset_implies_sum_ri_in_snset}, . Therefore, by definition, .
      \item[LIN2]
         Since , we have by definition of  that  and . Then,  and . Therefore, by definition, .
      \item[NULL]
         We want to show . Since  and , we have by definition that .
         \qed
    \end{description}
  \end{itemize}
\end{proof}

\recap{Lemma}{lem:a_subset_b}{
  If  then .
}
\begin{proof}
  We proceed by induction on the relation .
  \begin{itemize}
  \item . Trivial by the reflexivity of set inclusion.
  \item . Trivial by the transitivity of set inclusion.
  \item . Let . By definition, . And by Lemma~\ref{lem:cr} (CR1), . Therefore, by definition, .
  \item .
    Let . By definition,  and . Therefore, by definition, .
  \item .
    Let . By definition,  and, for all , . By induction hypothesis, . And since , . Therefore, by definition, .
  \item . Let . By definition,  and . And since , we have . Therefore, by definition, .
  \item . Let . By definition,  and . And since , we have that , and so, . Therefore, by definition, .
  \item . Analogous to the previous case.
  \qed
  \end{itemize}
\end{proof}

\xrecap{Lemma}{Adequacy}{lem:adequacy}{
  If  and  then .
}
\begin{proof}
  By induction in the derivation of .
  We proceed by cases.
  \begin{itemize}
  \item . Since
    , we have .

  \item . By Lemma~\ref{lem:cr} (NULL) and Lemma~\ref{lem:a_subset_b}, .

  \item . By definition, . And since , we have by definition that .

  \item . Analogous to the previous case.

  \item . By the induction hypothesis, , and by Lemma~\ref{lem:cr} (LIN2), . Finally, by Lemma~\ref{lem:a_subset_b}, .

  \item .
    By the induction hypothesis,  and
    , with  and
    . Then, since  and  are disjoint, . And by Lemma~\ref{lem:cr} (LIN1), . Therefore, by Lemma~\ref{lem:a_subset_b}, .

  \item .
    We want to show that if , then . By definition, it suffices to show that  and .
    By induction hypothesis, , which implies that . Therefore, .
    On the other hand, since , we have by Lemma~\ref{lem:t_implies_proj_t} that .
    Therefore, .

  \item .
    By the induction hypothesis , and by
    Lemma~\ref{lem:a_subset_b}, .

  \item .
    We want to show that if , then . By definition, this is equivalent to showing that  and, for all .
    \\ By induction hypothesis, we have that  and , which implies that  and . Therefore,  and .
    \\ And since  (because such a term is actually an application), we have by Lemma~\ref{lem:cr} (CR3) that it suffices to show that .
    \\ We proceed by induction (2) over . We analyze each of the reducts of :
    \begin{itemize}
    \item  where 
      \\ Since , we have by induction hypothesis (2) that .
    \item  where 
      \\ By induction hypothesis, .
    \item  where 
      \\ By induction hypothesis, .
    \item 
      \\ Since  and , we have by induction hypothesis (2) that . Similarly, . Therefore, by Lemma~\ref{lem:cr} (LIN1), .
    \item 
      \\ Since  and , we have by induction hypothesis (2) that . Therefore, by Lemma~\ref{lem:cr} (LIN2), .
    \item 
      \\ By Lemma~\ref{lem:cr} (NULL), .
    \end{itemize}

  \item .
    We want to show that if , then , which is equivalent to showing that  and, for all .
    \\ By induction hypothesis, , which implies that . Therefore,  and . And since , Lemma~\ref{lem:cr} (CR3) tells us that if , then .
    We are going to show that in fact .
    \\ Since , we have by Lemma~\ref{lem:cr} (CR2) that . Therefore, we can proceed by induction (2) over . We analyze each of the reducts of :
    \begin{itemize}
    \item 
      \\ We want to show that . By definition, , and by induction hypothesis, . Therefore, .
    \item  where 
      \\ By induction hypothesis (2), .
    \item 
      \\ Since  y , we have by induction hypothesis (2) that . Similarly, we have that . Therefore, by Lemma~\ref{lem:cr} (LIN1), we have that .
    \item 
      \\ Since  y , we have by induction hypothesis (2) that . Therefore, by Lemma~\ref{lem:cr} (LIN2), we have that .
    \item 
      \\ By Lemma~\ref{lem:cr} (NULL), we have that .
    \end{itemize}

  \item  We must show that if , then .
    Since  and  are disjoint, we have that , where  and .
    Therefore, it suffices to show that .
    By induction hypothesis and definition of , we have that .

  \item . We want to show that if , then .
    Since  y  are disjoint, we have that , where  y .
    Therefore, it suffices to show that .
    \\ Since  and , we have by definition that  y . Therefore, .
    \\ On the other hand, we need to show that . To do that, it is sufficient to show that . Since  and , we can proceed by induction (2) over . We analyze the possible reducts of :
    \begin{itemize}
    \item  where 
      \\ Since , we have by induction hypothesis (2) that .
    \item  where 
      \\ Analogous to the previous case.
    \item 
      \\ Since , we have by Lemma~\ref{lem:generation}  that  with a smaller derivation tree. Then, by induction hypothesis (Adequacy), we have that , and therefore, by Lemma~\ref{lem:cr} (CR1), we have that .
    \item 
      \\ Analogous to the previous case.
    \item 
      \\ Since , we have by Lemma~\ref{lem:generation}  that  with a smaller derivation tree. Then, by induction hypothesis (Adequacy), we have that , and therefore, by Lemma~\ref{lem:cr} (CR1), we have that .
    \item 
      \\ Analogous to the previous case.
    \item 
      \\ Since  and, by Lemma~\ref{lem:size}, , we have by induction hypothesis (2) that . Similarly, we have that . Therefore, by Lemma~\ref{lem:ri_in_snset_implies_sum_ri_in_snset}, we have that .
    \item 
      \\ Analogous to the previous case.
    \item 
      \\ Since  and, by Lemma~\ref{lem:size}, , we have by induction hypothesis (2) that . Therefore, by Lemma~\ref{lem:ri_in_snset_implies_sum_ri_in_snset}, we have that .
    \item 
      \\ Analogous to the previous case..
    \item 
      \\ By definition, .
    \item 
      \\ Analogous to the previous case.
    \end{itemize}


  \item . By definition of , we have that if , then . And by induction hypothesis, .

  \item . By definition, . And by induction hypothesis, . Therefore, .

  \item .
    Since  and  are disjoint, , where  and .
    Therefore, it suffices to show that .
    \\ Since , we have by definition that  and, by Lemma~\ref{lem:cr} (CR1), . Similarly,  and .
    \\ Then,  and . Therefore, by definition of , .

  \item .
    \\ Since , we have by Lemma~\ref{lem:substitution} that . And by induction hypothesis, . Then, . Therefore, by definition, .

  \item .
    \\ Since , we have by Lemma~\ref{lem:substitution} that . Furthermore, by induction hypothesis, , and by Lemma~\ref{lem:cr} (CR1), . Then, . Therefore, by definition, .

  \item .
    \\ Por definicin de , tenemos que  y . Entonces,  y . Por lo tanto, , que es lo que queramos mostrar.

  \item .
    Analogous to the previous case. \qed
  \end{itemize}
\end{proof}

\section{Detailed proofs of Section~\ref{sec:denSem} (Interpretation)}\label{ap:denSem}
\recap{Lemma}{lem:inc}{If , then .}
\begin{proof}
  We proceed by induction on the relation .
  \begin{itemize}
  \item  by the reflexivity of set inclusion.
  \item Let  and . By the induction hypothesis,  and . Then  by the transitivity of set inclusion.
  \item .
  \item .
  \item Let  and . Then,
    \begin{itemize}
    \item 
    \item 
    \item  by an analogous reasoning
      to the previous one.
    \item .
      \qed
    \end{itemize}
  \end{itemize}
\end{proof}

\recap{Lemma}{lem:subsDen}{
  If  and  is a
  -valuation, then .
}
\begin{proof}
  We proceed by induction on .
  \begin{description}
  \item[Independent cases.] The cases where  does not includes  nor 
    and the denotation does not depends on the valuation, are trivial. Those
    cases are: , ,  and .
  \item[Let .] Then, .
  \item[Let .] Then, .
  \item[Let .] Then, .
  \item[Let .] Then,
    
  \item[Let .] Then,
    
  \item[Let .] Then,
    
  \item[Let .] Then,
    
  \item[Let .] Then,
    
  \item[Let .] Then,
    
    with  where
     or .

    By the induction hypothesis, , hence,
    
  \item[Let .] Then,
    
  \item[Let .] Then,
    
  \item[Let ] Then,
     \qed
  \end{description}
\end{proof}

\recap{Theorem}{thm:soundness}{
  If , and  is a -valuation. Then .
}
\begin{proof}
  We proceed by induction on the typing derivation.
  \begin{itemize}
  \item .
    Then, .
  \item .
    Then, .
  \item .
    Then, .
  \item .
    Then, .
  \item .

    Then, by the induction hypothesis,  and by
    Lemma~\ref{lem:inc}, , hence, .
  \item .

    Then, by the induction hypothesis, .
    Hence, .
  \item .

    Then, since by the induction hypothesis,  and , we have
    
  \item .

    Then, by the induction hypothesis, . Hence,
    
  \item .

    Then, by the induction hypothesis  and , where .
    Then,
    

    Since  is a set of functions (and not a linear
    combination of them),  is a singleton and so this set is equal to
    .
  \item .

    Then, by the induction hypothesis  and . Then,

    
  \item .

    Then, by the induction hypothesis  and
    , with .
    Then,
    .
  \item .

    Then, by the induction hypothesis, . By definition,
    
    where , with
    , and . Then,
    .
  \item .

    Then, by the induction hypothesis, , where
     is a -valuation. Notice that any  that is a
    -valuation is also a -valuation. Then, .
  \item .

    Then, by the induction hypothesis, , where
     is a -valuation. Let  be a
    -valuation, then, by Lemma~\ref{lem:subsDen},
    .
  \item .


    Then, by the induction hypothesis,  and
    . Then,
    .
  \item .

    Then, by the induction hypothesis, . So,
    .
  \item .

    Then, by the induction
    hypothesis, . So
    .
  \item .

    Then, by the induction hypothesis, .
  \item .
    This case is analogous to the previous one.
    \qed
  \end{itemize}
\end{proof}

\section{Trace and typing of the Deutsch algorithm}\label{ap:Deutsch}

We may use  as a shorthand notation for .

The full trace of  is given below.



The typing of , for any , is given
below:














\section{Trace and typing of the Teleportation algorithm}\label{ap:telep}
The full trace of  is
given below.




The next rewrite step following rule , may produce one of the following
four results probability  each:
\begin{description}
\item[(00)] 
\item[(01)] 
\item[(10)] 
\item[(11)] 
\end{description}

So, in general, . Then,

Cases:
\begin{description}
\item[]  \pair
        {\alpha.\s Z^{\ket 0}(\ite{\ket 0}{\s{not}\ket 0}{\ket 0})} {\beta.\s
          Z^{\ket 0}(\ite{\ket 0}{\s{not}\ket 1}{\ket 1})}
        

\item[]  \pair
        {\alpha.\s Z^{\ket 0}(\ite{\ket 1}{\s{not}\ket 1}{\ket 1})} {\beta.\s
          Z^{\ket 0}(\ite{\ket 1}{\s{not}\ket 0}{\ket 0})}
        

\item[]  \npair
        {\alpha.\s Z^{\ket 1}(\ite{\ket 0}{\s{not}\ket 0}{\ket 0})} {\beta.\s
          Z^{\ket 1}(\ite{\ket 0}{\s{not}\ket 1}{\ket 1})}
        

\item[]  \npair
        {\alpha.\s Z^{\ket 1}(\ite{\ket 1}{\s{not}\ket 1}{\ket 1})} {\beta.\s
          Z^{\ket 1}(\ite{\ket 1}{\s{not}\ket 0}{\ket 0})}
        
\end{description}
Hence, in every case,  as expected. \medskip


The typing of  is given below:























\end{document}
