\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{boxedminipage}
\usepackage{xy}
\xyoption{all}

\newtheorem{theorem}{\bf Theorem}[section]
\newtheorem{corollary}[theorem]{\bf Corollary}
\newtheorem{lemma}[theorem]{\bf Lemma}
\newtheorem{fact}[theorem]{\bf Fact}
\newtheorem{definition}[theorem]{\bf Definition}
\newtheorem{proposition}[theorem]{\bf Proposition}
\newtheorem{claim}[theorem]{\bf Claim}
\newtheorem{observation}[theorem]{\bf Observation}
\newtheorem{property}[theorem]{\bf Property}

\newcommand {\myproof} {{\it Proof: }}
\newcommand {\myproofsketch} {{\it Proof Sketch: }}
\newcommand {\qed} {\hfill}

\newcommand{\eat}[1] {}
\newcommand{\calA} {{\cal A}}
\newcommand{\calB} {{\cal B}}
\newcommand{\calC} {{\cal C}}
\newcommand{\calP} {{\cal P}}
\newcommand{\calQ} {{\cal Q}}
\newcommand{\calI} {{\cal I}}
\newcommand{\calS} {{\cal S}}
\newcommand{\calZ} {{\cal Z}}
\newcommand{\calY} {{\cal Y}}
\newcommand{\calX} {{\cal X}}
\newcommand{\calR} {{\cal R}}
\newcommand{\wh}[1] {\widehat{#1}}
\newcommand{\wt}[1] {\widetilde{#1}}
\newcommand{\pair}[2] {\langle #1, #2\rangle}
\newcommand{\TMIS} {{\rm Time(MIS)}}

\title{Distributed and Parallel Algorithms for Set Cover Problems with Small Neighborhood Covers}
\author{Archita Agarwal \and Venkatesan T. Chakaravarthy \and Anamitra R. Choudhury \and Sambuddha Roy \and Yogish Sabharwal}
\date{IBM Research Lab, New Delhi, India\\
  \texttt{\{archiaga,vechakra,anamchou,sambuddha,ysabharwal\}@in.ibm.com}
}
\begin{document}

\maketitle

\begin{abstract}
In this paper, we study a class of set cover problems that satisfy a special property which 
we call the {\em small neighborhood cover} property.
This class encompasses several well-studied problems including vertex cover, interval cover, 
bag interval cover and tree cover. We design unified distributed and parallel algorithms that can handle 
any set cover problem falling under the above framework and yield constant factor approximations.
These algorithms run in polylogarithmic communication rounds in the distributed setting
and are in NC, in the parallel setting.
\end{abstract}

\section{Introduction}
In the classical set cover problem, we are given a set system ,
where  is a {\em universe} consisting of  {\em elements}
and  is a collection of  subsets of .
Each set  has cost  associated with it.
The goal is to select a collection of sets 
having the minimum aggregate cost such that every element is included in at least one
of the sets found in .

There are two well-known classes of approximation algorithms for the set cover problem \cite{ShmoysBook}.
The first class of algorithms have an approximation ratio of ,
where  is the maximum cardinality of the sets in .
The second class of algorithms have an approximation ratio of ,
where  is the {\em frequency parameter} which is the maximum number of sets of 
that any element belongs to. 
The above approximation ratios are nearly optimal \cite{Hypergraph-Hardness, RazS97, Feige}. 
In general the parameters  and  can be arbitrary and so the above algorithms do not yield
constant factor approximations.
The goal of this paper is to develop parallel/distributed constant factor approximation algorithms
for certain special cases of the problem.

In the parallel setting, we shall use the NC model of computation and its randomized version RNC.
Under this model, Rajagopalan and Vazirani \cite{RV} presented a randomized 
parallel -approximation algorithm for the general set cover problem.
Under the same model, Khuller et al. \cite{KVY} presented a -approximation algorithm
for any constant frequency parameter  and .

In the distributed setting, we shall adopt a natural communication model which has also been used in prior work.
In this model, there is a processor for every element 
and there is a communication link between any two elements  and ,
if and only if both  and  belong to some common set .
We shall view the element itself as the processor.
Each element has a unique ID and knows all the sets to which it belongs.
We shall assume the standard synchronous, message passing model.
The algorithm proceeds in multiple communication rounds, where in each round
an element can send a message to each of its neighbors in the communication network.
We allow each element to perform a polynomial amount of processing in each round
and the messages to be of polynomial size.
We are interested in two performance measures: 
(i) the approximation ratio achieved by the algorithm; and
(ii) the number of communication rounds. 
Ideally a distributed algorithm should have polylogarithmic communication rounds.
Under the above distributed model, Kuhn et al. \cite{near-sighted} and 
Koufogiannakis and Young \cite{Kouf-Young} presented distributed algorithms for the general set cover problem
with approximation ratios of  and , respectively;
both the algorithms run in polylogarithmic communication rounds.

There are special cases of the set cover problem wherein both  and  are arbitrary,
which nevertheless admit constant factor approximation algorithms.
In this paper, we study one such class of problems satisfying a criteria that we call the  
{\em small neighborhood cover} property (SNC-property).
This class encompasses several well-studied problems such as 
vertex cover, interval cover and tree cover.
Furthermore, the class subsumes set cover problems with a constant frequency parameter .
Our results generalize the known constant factor approximation algorithm for the latter class.

Our goal is to design unified distributed and parallel algorithms
that can handle any set cover problem falling under the above framework.
In order to provide an intuition of the SNC-property,
we next present an informal (and slightly imprecise) description of the property.
We then illustrate the concept using some example problems and intuitively show why these
problems fall under the framework.
The body of the paper will present the precise definition of SNC set systems. 

{\bf SNC Property.}
Fix an integer constant . We say that two elements are neighbors, if some 
contains both of them. The neighborhood of an element is defined to be the set of all its neighbors (including itself).
We say that an element  is a -SNC element,
if there exist at most  sets that cover the neighborhood of .
The given set system is said to have the -SNC property, 
if for any subset , the set system restricted\footnote{the restricted set system is , 
where } to  contains a -SNC element.
The requirement that every restriction has a -SNC element will be useful in solving the problem iteratively.

{\bf Example Problems.}
We next present some example -SNC set cover problems.

{\em Vertex Cover: }Given a graph , we can construct a set system
by taking the edges as the elements and the vertices as sets.
In this setup, an element belongs to only two sets and hence, the set systems defined by the
vertex cover problem satisfy the -SNC property.
In general, set cover problems having a constant frequency parameter  would induce
-SNC set systems with .

\begin{figure}
\begin{center}
\includegraphics[width=5in]{fstt1.pdf}
\caption{Illustration for interval cover problems}
\label{fig:interval}
\end{center}
\end{figure}


{\em Interval Cover: }
In this problem, we are given a timeline divided into some  discrete timeslots
. The input includes a set of intervals , where each interval 
is specified by a range , where  and  are the starting and ending points of .
Each interval  also has an associated cost . We say that an interval  covers a timeslot
, if . The goal is to find a collection of intervals
having minimum aggregate cost such that every timeslot  is covered by at least one interval in the collection.
We can view the problem as a set cover instance by taking the timeslots to be the elements and
taking each interval  as a set consisting of the timeslots covered by . 
See the picture on the left in Figure \ref{fig:interval} for an illustration (ignore the Roman numerals).
Consider any timeslot  and let  be the set of intervals covering .
Among the intervals in , the interval  with the minimum starting point
and the interval  having the maximum ending point can cover the neighborhood of  (resolving ties arbitrarily).
For example, for timeslot ,  and .
Hence, the set systems defined by the interval cover problem satisfy the -SNC property.

{\em Tree Cover Problem: }
In the tree cover problem, we are given a {\em rooted} tree .
The input includes a set of intervals , where each interval is specified as a pair of nodes 
such that  is an ancestor of . The interval  can be visualized as the path from  to .
The interval is said to cover an edge , if  is found along the above path.
Each interval  has a cost  associated with it.
The goal is to find a collection of intervals of minimum cost covering all the edges.
We can view the problem as a set cover instance by taking the edges to the elements
and taking the intervals as sets.
It is not difficult to see that the tree cover problem generalizes the interval cover problem. 
See the picture on the right in Figure \ref{fig:tree} for an illustration.
Consider any leaf edge . Let  be a set of intervals covering the edge .
Among the intervals in , let  be the interval extending the most
towards to the root. Note that  covers the neighborhood of .
For example, in the figure, for the leaf edge , the interval  will serve as .
Thus, any leaf edge satisfies the -SNC property.
It is not difficult to see that any restriction will also contain an element satisfying the -SNC property. 
Hence, the set systems defined by the tree cover problem satisfy the -SNC property.

{\em Bag Interval Cover Problem: }
This problem generalizes both vertex cover and interval cover problems.
The input consists of a timeline divided into discrete timeslots .
We have a set of  intervals . 
Each interval  has a starting timeslot , an ending timeslot  and a weight . 
Timeslots are grouped into  {\em bags} ; a timeslot may belong to more than one bag.
The interval  is said to cover a bag , if it spans at least one timeslot from the bag .
The goal is to find a collection of intervals having minimum aggregate cost such that
each bag is covered by some interval in the collection. 
The {\em girth} of the system is defined to be the maximum cardinality of any bag and it is denoted ; 
Viewed as a set cover problem, each bag will correspond to an element and 
each interval will correspond to a set. 
See the picture on the left in Figure \ref{fig:interval} for an illustration. The bag number are shown in Roman numerals.
For instance, Bag I consists of timeslots . The girth of the system is .

Consider any element (bag)  containing timeslots  (with ).
For each timeslot , among the intervals spanning  select the intervals
having the minimum staring point and the maximum ending point.
This set of  intervals can cover the neighborhood of .
Thus any element satisfies the -SNC property.
Hence, the set systems defined by the bag interval cover problem satisfies the -SNC property.

{\em Priority Interval Cover: }
As in the case of interval covering,
we are given a discrete timeline  and a set of intervals .
In addition, each timeslot  has a priority  (a positive integer)
and similarly, each interval  is also associated with a priority .
An interval  can cover a timeslot , if  and .
The basic interval covering problem corresponds to the case where there is only one priority.
Let the number of priorities used be . See Figure \ref{fig:interval} for an illustration;
the numbers within boxes show the priorities of intervals and timeslots.
The interval  cannot cover timeslot , even though the interval spans the timeslot.

Consider any timeslot  having the highest priority. As in the interval cover problem,
among the set  of intervals covering , the intervals having the minimum starting point
and the maximum ending point put together can cover the neighborhood of .
Thus all timeslots  having the highest priority would be -SNC elements. 
It is not difficult to argue that any restriction will also contain an element satisfying the -SNC property. 
Hence, the set systems defined by the priority intercal cover problem satisfy the -SNC property.

\begin{figure}
\begin{center}
\includegraphics[width=3in]{fstt2.pdf}
\caption{Illustration for tree cover}
\label{fig:tree}
\end{center}
\end{figure}


{\bf Layer Decomposition.}
An important concept that will determine the running time of our algorithms is that of layer decomposition.
We present an intuitive description of layer decomposition. 
The formal definition will be presented in the body of the paper.

Consider a set system  satisfying the -SNC property for some constant .
Let  be the set of all -SNC elements in the given set system.
Let  be the set of -SNC elements in the set system obtained by restricting to .
Proceeding this way, for , let  be the set of -SNC elements in the set system 
obtained by restricting to .
We continue the process until no more elements are left.
Let  be the number of iterations taken by this process.
The sequence  is called the {\em layer decomposition} of the set system .
Each set  is called a {\em layer}. The number of layers  is called the {\em decomposition length}.
The decomposition length of the input set system is of importance, since the running time of our parallel/distributed
algorithms depend on this quantity.

We next study the decomposition length for our example problems.
In the case of vertex cover, interval cover and bag interval cover problems,
we saw that all the elements satisfy the -SNC property in the given system  itself.
Hence, the decomposition length of these set systems is one.
In the tree cover problem, recall that all the leaf edges in the given tree  are -SNC elements.
Thus, all the leaf edges will belong to the first layer . Once these leaf edges are removed,
the leaf edges in the remaining tree will belong to the second layer .
Proceeding this way, we will get a layer decomposition in which the number of layers will be
the same as the depth of the tree; later, we describe how to reduce the decomposition length to be .

In this paper, we will only focus on set cover problems having logarithmic decomposition length 
and derive distributed/parallel algorithms with polylogarithmic rounds/running-time for such problems.
We note that there are set cover problems that induce -SNC systems with a constant , 
but having arbitrary decomposition length. An example for the phenomenon is provided by the priority interval cover problem. 
In this case, all the timeslots having the highest priority would belong to layer .
In general, the timeslots having priority  will belong to layer of index at most , where  is the total number of priorities.
Therefore the number of layers would be the could be as high as the the number of priorities.

{\bf Our Results. }In this paper, we introduce the concept of -SNC property.
We note that all the example problems considered earlier can be solved optimally or within constant factors
using the primal-dual paradigm. All these algorithms have certain common ingredients;
these are abstracted by -SNC framework.
We present three algorithms for the set cover problem on -SNC set systems.
\begin{itemize}
\item
A simple sequential -approximation algorithm.
\item 
A distributed -approximation algorithm for -SNC set systems of logarithmic decomposition length.
The algorithm is randomized and uses  communication rounds.
\item
A parallel -approximation algorithm for -SNC set systems of logarithmic decomposition length.
The algorithm can be implemented in NC.
\end{itemize}

Our algorithms have the following salient features:
\begin{itemize}
\item
They provide unified constant factor approximations for set cover problems falling under the -SNC 
framework with logarithmic decomposition length, in both distributed and parallel settings.
\item
A surprising and interesting characteristic of these algorithms is that they are model independent. 
Meaning, they only require the set system as input and do not need the underlying model defining the set system.
For instance, in the tree cover problem, the algorithms do no need the structure of the tree as input.
At a technical level, we show that the layer decomposition can be constructed by considering only the local 
neighborhood information; this fact is crucial in a distributed setting.
\end{itemize}

Regarding the example problems, we saw that in case of the vertex cover, interval cover and bag-interval cover problems,
the decomposition length is one. Thus our parallel and distributed algorithms will apply to these problems.
The case of tree cover problem is more interesting.
As we observed earlier, the set systems arising from the tree cover problem are -SNC set systems,
however the the decomposition length is the same as the depth of the tree, 
which could be as large as  (where  is the number of edges). 
Hence our parallel and distributed algorithms cannot be applied to this case.
However, we shall show that it is possible to reduce the decomposition length to , 
if we settle for a slightly higher SNC parameter of :
\begin{itemize}
\item
We prove that the set systems
defined by the tree cover problem satisfy the -SNC property with decomposition length .
\end{itemize}
In other words, the tree cover problem instances induce a -SNC set systems
of arbitrary decomposition length, as well as -SNC set systems of decomposition length .
Using the above fact, we can apply our parallel and distributed algorithms and obtain constant factor apporoximations.

It is easy to see that for any constant , set systems with frequency parameter 
satisfy the -SNC property, with .
Dinur et al. \cite{Hypergraph-Hardness} proved that for any , 
it is NP-hard to approximate the set cover problem within a factor of , for any .
Thus, the approximation ratio of the sequential and distributed algorithms are nearly optimal.
In the parallel setting, we present an algorithm with an approximation ratio of .
Improving the approximation ratio is an interesting open problem.

While this is the first paper to consider the general -SNC framework, the specific example
problems have been studied in the sequential, parallel and distributed settings.
Improved algorithms are known in specific cases.
We next present a brief survey of such prior work and provide a comparison to our results.

{\bf Comparison to Prior Work on Example Problems. }
For the vertex cover problem, sequential -approximation algorithms are well known \cite{ShmoysBook}.
In the parallel setting, Khuller et al. \cite{KVY} presented a parallel NC algorithm
having approximation ratio of , for any  (see also \cite{Gandhi}).
Koufogiannakis and Young \cite{Kouf-Young} presented the first parallel algorithm with approximation ratio of .
Their algorithm is randomized and runs in RNC.
The above algorithms can also be implemented in the distributed setting
(see also \cite{GKP}).


The interval cover problem can be solved optimally in the sequential setting via dynamic programming.
Bertossi \cite{Bertossi} presented an optimal parallel (NC) algorithm,
which can also handle the more general case of circular arc covering.
However, their algorithm requires the underlying model (i.e., the timeline and intervals) explicitly as input.
We are not familiar with prior work on the problem in the distributed setting.

Chakrabarty et al. \cite{CGK} study the tree cover problem and its generalizations under the sequential setting.
In this setting, the problem can be solved optimally via dynamic programming or the primal-dual paradigm.
Furthermore, the constraint matrices defined by the problem are totally unimodular (see \cite{CGK}).
We are not familiar with any prior work on parallel/distributed algorithms for this problem.
For this problem  and so, our sequential/distributed algorithms provide an approximation ratio of .
The parallel algorithm has an approximation factor of . However, we note that one of the reasons for the high
ratio is that the algorithm is oblivious to the underlying model.

The priority interval cover problem is studied by Chakrabarty et al. \cite{CGK} and Chakaravarthy et al. \cite{our-esa}. 
They provide polynomial time optimal algorithms based on the dynamic programming.
To the best of our knowledge, the bag interval cover problem has not been considered before.
However, the notion of bag constraints has been considered in the related context
of interval packing problems (see \cite{Bar-Noy-Jacm, Berman-Dasgupta}).
Covering integer programs (CIP) generalize the set cover problem.
These are well studied in both sequential and distributed settings (see \cite{Kouf-Young,CGK}, and references therein).

{\bf Proof Techniques. }
All the algorithms in the paper utilize the primal-dual paradigm. 
The sequential algorithm is fairly straightforward and it is similar to that of
the primal-dual algorithm -approximation algorithm for the set cover problem.
The latter algorithm works by constructing a maximal feasible solution to the dual
which would automatically yield an -approximate integral primal solution.
Our problem requires two additional ingredients. The first is that an arbitraty maximal dual
solution would not suffice. Instead, the solution needs be constructed in accordance
with the layered decomposition. Secondly, a maximal dual solution would not automatically
yield a -approximate integral primal solution. A reverse delete phase is also needed.
In this context, we present a polynomial time algorithm for computing the layer decomposition of the given set system,
which can also be implemented in both parallel and distributed settings.

In the distributed setting, the only issue is that the above steps need to be performed within 
polylogarithmic number of rounds. We address the issue by grouping the elements
based on the Linial-Saks decomposition \cite{LS} of the communication network.

The parallel algorithm is more involved and forms the main technical component of the paper.
For a general set system, Khuller et al. \cite{KVY} (see also \cite{Gandhi}) 
present a parallel procedure for computing nearly maximal dual solution with maximality parameter of ,
using the idea of raising several dual variables simultaneously.
However, the parallel running of the procedure is , where  is the frequency parameter.
In our problems, the parameter  could be arbitrary and the above running time is not satisfactory.
We present a procedure that produces a near maximal solution with maximality parameter .
While the maximality parameter is worse compared to prior work, the running time of our procedure is independent of .
This procedure could be of independent interest. The procedure is similar in spirit to that of Khuller et al.,
but the analysis for bounding the number of iteration takes a different approach.

As mentioned earlier, our setting requires an additional reverse delete phase,
whose parallelization poses interesting technical issues.
Our procedure executes the phase by processing the layer decomposition in a zig-zag manner.
In iteration , the procedure processes layer  and performs the reverse delete for the particular layer.
However, this involves revisiting the older layers .
Each step involves computing the maximal independent set of a suitable graph, for which we utilize
the parallel algorithm due to Luby \cite{Luby}.
The overall number of steps would be  (where  is the decomposition length)
and the approximation ratio is  (as against the ratio  achieved by the sequential/distributed algorithms).

Our algorithm raises two interesting technical problems.
The first is that whether we can construct a near maximal solution to the dual with parameter ,
while keeping the parallel running time independent of the frequency parameter .
Secondly, whether the reverse delete can be performed in parallel while achieving a primal complementary
slackness parameter of . An affirmative answer to either question would result in improved approximation algorithms.

\section{Preliminaries}
In this section, we present the formal definition of the -SNC property and related concepts.
We also present algorithms for computing the layer decomposition for a given -SNC set system.

{\bf -SNC Element: }Fix an integer constant . Consider a subset of elements  and an element .
Let  be the collection of all sets that contain .
The element  is said to be a {\em -SNC element within }, 
if for any ,
there exist at most  sets  (with )
such that every element in  covered by  is also covered by one of the  sets:

Note that the  sets must be selected from the collection .
The property is trivially true if , but it becomes interesting if .

{\bf -SNC Set System: }The given set system  is said to be a {\em -SNC set system}
if for every subset of elements , there exists an element 
which is a -SNC element within . 
The set system is said to be a {\em total -SNC set system},
if for every subset , every  is a -SNC element within .
The following property is easy to verify.

\begin{proposition}
\label{prop:AAA}
If an element  is a -SNC element within , then for any  such that ,
 is also a -SNC element within .
\end{proposition}

However, the converse of the above statement may not be true.
Namely, an element  may be a -SNC element within a set , but it may not be a -SNC element
within a superset . To see this, suppose  is a collection of sets 
such that every  contains .
The collection  may cover an element ,
which may not be covered by some  sets of  that cover the neighborhood of  within .

{\bf Layer Decomposition: }
Consider a -SNC set system .
The notion of layer decomposition is defined via an iterative process, as described in the introduction.
Let  be the set of -SNC elements within .
For , let  be the set of -SNC elements within 
We terminate the process when there are no elements left. Let  be the number of iterations taken by the process.
The sequence  is called the {\em layer decomposition} of the given set system.
Each set  is called a layer and  is called the {\em decomposition length}
We consider  to be the left-most layer and  as the right-most layer.

{\bf Computing Layer Decompositions: }
As part of our algorithms, we will need a procedure for computing the layer decomposition of a 
given -SNC set system. The following lemma provides such a procedure. 
The proof is given in Section \ref{sec:layer-compute}

\begin{lemma}
\label{lem:TTT}
There exists a procedure for computing the layer decomposition of a given -SNC set system.
In the sequential setting, it can be implemented in polynomial time.
In the distributed setting, it can be implemented in  communication rounds.
In the parallel setting, the algorithm takes  iterations each of which can be implemented in NC.
\end{lemma}

{\it Remark: }Notice that any -SNC set system is also a -SNC set system
for any . The decomposition length of the system will depend on the choice of .
The procedure stated in the lemma will produce the layer decomposition corresponding to the value of  provided 
as input to the procedure.

\section{Sequential Algorithm}
In this section, we present a sequential -approximation algorithm for solving the set cover problem
restricted to -SNC set systems, for a constant .
The parallel and distributed algorithms build on the sequential algorithm.
As mentioned in the introduction, our example problems
can be solved optimally or approximately using the primal-dual paradigm. 
All these algorithm have certain common ingredients in the design and analysis,
which are captured by the notion of -SNC property.
Our algorithm for the general -SNC set systems 
also goes via the primal-dual paradigm and utilizes ideas from the algorithms for the example problems.
The pseudocode for the algorithm is given in Figure \ref{fig:seq-pseudo}.

The primal and the dual for the input set system  are given below.

\begin{tabular}{p{3in}p{3in}}

&

\end{tabular}

The primal LP includes a variable  for each set 
and a constraint for each element .
The dual includes a variable  for each element  (corresponding to the primal constraint)
and a constraint for each set  (corresponding to the primal variable).
The primal and the dual would also include the non-negativity constraints  and .

Let the input set system be  having  elements and  sets.
Using the procedure given in Lemma \ref{lem:TTT}, compute the layer decomposition .
Obtain an ordering  of the elements by placing the elements in  first,
then those in  next and so on; the elements in  will appear at the end of the ordering
(within a layer, the elements can be arranged arbitrarily).
Let  be the ordering produced by this process.
Notice that for , the element  is a -SNC element within .
The -approximation algorithm would exploit the above ordering.

The algorithm works in two phases: a forward phase and a reverse-delete phase.
The forward phase would produce a dual feasible solution  and a cover  for the system.
In the reverse-delete phase, some sets in  would be deleted to get the final solution .

The forward phase is an iterative procedure which will scan the ordering  from left to right. 
We start by initializing 
and , for all . 
In iteration , we pick an element next element  from the ordering 
which is uncovered by the collection . We raise the dual variable  until
some dual constraint becomes tight (i.e., LHS becomes equal to the RHS). 
Let the corresponding set be . We include the set  in  and proceed to the next iteration.
The process is terminated when all the elements are covered. 
Let  be the set of elements whose dual variables were raised.

In the second phase (called reverse-delete phase), 
we shall delete some sets from  and construct a new solution  such that the following 
complementary slackness properties are satisfied:
\begin{itemize}
\item 
{\em Dual-slackness:} For any set , the corresponding dual constraint is tight.
\item 
{\em Primal slackness:} For any element , the corresponding primal constraint is approximately tight:

\end{itemize}
Once we ensure these properties, standard weak-duality arguments can be applied
to argue that  is a -approximate solution.

The reverse-delete procedure is described next.
Initialize .
For any element , the corresponding is primal constraint is approximately tight:
Let the number of elements in  be . Arrange these elements
in the order in which they were raised, say .
Let  be the sets picked by the forward phase
when these variables were raised, respectively.
Consider the sequence  in the reverse order, starting with .
The iteration  works as follows. 
Let  be the set of elements that were uncovered by  in the beginning of the iteration 
in which  was picked. Notice that  is a -SNC element within .
Let  be the collection of sets from  which cover . 
The -SNC property ensures that we can collapse  into at most  sets.
Meaning, we can find sets  (with ) such that

Delete all the sets found in  from  and retain only the sets .
In doing so, we have not lost feasibility of . To see this, first notice that the elements in  still remain
covered. Regarding the elements in , 
the sets  covers all these elements.
One potential issue is that some of these set could be part of the sets we deleted;
however, this is not possible, since  was selected to be an uncovered element in the corresponding iteration
of the forward phase..
We have ensure that Equation \ref{eqn:seq-tau} holds for the element .
Proceeding this way, at the end of the reverse-delete phase we will obtain our output solution .

All the elements in  satisfy primal slackness property (Equation \ref{eqn:seq-tau}). 
Regarding the dual-slackness property, we included a set  in the forward phase,
only when the corresponding dual constraint is tight. 
Furthermore, the dual variables were not modified in the reverse-delete phase
and no new set was introduced in .
Thus, the solution  also satisfies the primal-slackness properties.

\begin{figure}[t]
\begin{center}
\begin{boxedminipage}{\hsize}
\begin{small}
\begin{tabbing}
xx\=xx\=xx\=xx\=xxx\=xxx\=\kill
\textbf{Begin}  \\
\> // Forward Phase:\\
\> Let  be the ordering of the elements according to -SNC property.\\
\> Initialize. . For all , .\\
\> For \\
\> \> Among the elements uncovered by , let  be the element appearing earliest in the ordering \\
\> \> Raise the dual variable  until some dual constraint becomes tight:\\
\> \> \> \\
\> \> \> Include the corresponding set  in :\\
\> \> \\
\> // Reverse Delete Phase:\\
\> \\
\> Let  be the sequence of elements whose dual variables were raised.\\
\> For  to \\
\> \> Let  be the elements uncovered by  in the beginning of the th iteration.\\
\> \> Let  be the collection of sets covering \\
\> \> Find sets  (with ) such that\\
\> \> \> all the elements in  covered by  are also covered by \\
\> \> Delete all the sets found in  from , except \\
\> \> Output .\\
\textbf{End}
\end{tabbing}
\end{small}
\end{boxedminipage}
\end{center}
\caption{Sequential -approximation algorithm}
\label{fig:seq-pseudo}
\end{figure}

\section{Parallel Algorithm for -SNC Set Systems}
\label{sec:main}
In this section, we present a parallel algorithm for the set cover problem on -SNC set systems with logarithmic decomposition length.
The approximation ratio of the algorithm is .
Similar to the sequnatial algorithm, the parallel algorithm also proceeds in two phases, a forward phase and a reverse-delete phase.
A pseudocode for the algorithm can be found in Figure \ref{fig:par}

\subsection{Forward Phase}
Consider a pair of solutions ,
where  is a feasible cover and  is a dual feasible solution.
For a constant , we say that the above pair is {\em -maximal},
if for any , the corresponding dual constraint is approximately tight:

In the forward phase, we shall construct a -maximal solution.
The procedure runs in  iterations,
where each iteration can be implemented in NC, where  is the decomposition length.
As we shall see, via a standard preprocessing trick, we can ensure that  is 
bounded by . The process would increase the approximation ratio by an additive factor of one.
Thus when  is logarithmic, the procedure runs in NC.
Furthermore, our procedure would satisfy certain additional properties to be specified later. 

{\it Remark: }
While we shall describe our algorithm for the specific scenario of -SNC set systems,
it can handle arbitrary set systems and produce -maximal solutions
in  iterations.
The problem of finding such approximately maximal solutions in parallel for general set systems is of independent interest.
Khuller et al.\cite{KVY} (see also \cite{Gandhi}) presented procedure for computing -maximal solutions, 
for any .  Their algorithm takes  iterations, where  is the frequency parameter.
For the specific case of  (the vertex cover scenario),
a parallel procedure for producing -maximal solutions is implicit in the work of 
Koufogiannakis and Young \cite{Kouf-Young}.
Their procedure runs in  iterations.
While our procedure has inferior value on the parameter , the number of iteration is independent of the frequency parameter .
The procedure could be independent interest.
The procedure is similar to that of Khuller et al. \cite{KVY}, but the goes via a different analysis for bounding the number of iterations.

We now discuss the forward phase.
Using the procedure given in Lemma \ref{lem:TTT}, compute the layer decomposition , where  is the decomposition length.
Initialize  and set , for all elements .
The forward phase works in  epochs processing the layers from left to right.
For , the goal of epoch  is to ensure that 
 covers all the elements in .

Consider an epoch . While the goal of the previous  epochs would have been to ensure coverage 
for , the collection  might already be covering some elements from  
(unintentionally). Let  be the set of elements found in  which are not covered by .
The purpose of epoch  is to ensure coverage for all the elements in .
The epoch  works in multiple iterations.
Consider an iteration .
A set  is said to {\em participate} in iteration , if it is not already included in .
Similarly, an element  is said to {\em participate} in iteration ,
if it is not all already covered by .
For each participating set , compute: 
(i) Current degree , which is the number of participating elements found in ; 
(ii) Current LHS value of dual constraint of : ;
(iii) Current difference between LHS and RHS of the dual constraint of :
;
(iv) Current {\em penalty} for :  
(intuitively, if  is included in ,  elements will be newly covered and this is the cost/penalty
each such element pays).
For each participating element , compute the minimum penalty offered by each 
set covering : .
Increase (or raise) the dual variable  by .
This would raise the value of the LHS of the dual constraints.
For every participating set , check if its dual constraint is approximately tight:

If the above condition is true, then add  to .
This completes the description of the iteration . 
The above process is continued until all the elements in  are covered by .
This completes epoch  and we proceed to epoch .

Notice that any dual variable  is raised only to an extent of its minimum penalty .
This ensures that all the dual constraints will remain satisfied at the end of each iteration.
The above procedure can be implemented in both distributed and parallel settings.
In the distributed setting, each participating element (or the corresponding node in the network)
can raise its dual variable  independently using information obtained from its neighbors.
Thus, each iteration can be implemented in a single round.
In the parallel setting, in each iteration, the dual variables can be raised in parallel.

The above procedure returns a pair of solutions  and .
It is easy to see that  is a feasible solution  for the given
set cover instance. Furthermore, only sets satisfying the bound (\ref{eqn:DDD})
are added to the collection . Hence, the pair satisfies the
desired approximate primal slackness property.

Let us next analyze the number of iterations taken by the algorithm.
The number of epochs is . Fix any epoch .
For any iteration , define the minimum penalty value  
(where the minimum is taken over all sets participating in iteration ). 
We now establish a bound on the number of iterations taken by the any epoch ,
by tracking minimum penalty value.
For a set  participating in successive iterations  and ,
its penalty may decrease (because both the values  and  may decrease across iterations).
Nevertheless, the lemma below shows that the minimum penalty will increase by a factor of at 
least  across successive iterations. 

\begin{lemma}
\label{lem:RRR}
For any iteration , .
\end{lemma}
\myproof
Let  be any set participating in the th iteration.
In th iteration, when the dual variables are raised for the participating elements,
the LHS value of the dual constraint of  will increase by some amount; let this amount be .
Consider the  elements contained in  that participate in the th iteration.
There are  elements that are uncovered by  in the beginning of the th iteration.
Of these elements, an element  said to be {\em good} to , if .
Intuitively, when we raise  by , the LHS of the dual constraint of  would raise by
at least . We say that an element  is {\em successful} in iteration ,
if at least  elements are good for .
As we observed earlier, the penalty of a set may decrease across iterations.
But, we next show that the penalty of an unsuccessful set cannot decrease by much.

\begin{claim}
\label{claim:EEE}
Any set  successful in the th iteration would be added to  in that iteration.
\end{claim}
\myproof
Since  is successful,  elements are good for  and each would raise the LHS
value by at least . Thus, 

So, after the raise in the dual variables, the LHS value will be at least .

Therefore,  will be added to  in the th iteration.
\qed

\begin{claim}
\label{claim:FFF}
Any set  satisfying  would be added to  in that iteration.
\end{claim}
\myproof 
For such a set , all the  elements will be good. Therefore, it will be successful.
\qed

\begin{claim}
\label{claim:GGG}
For any unsuccessful set  that participates in the iteration ,
.
\end{claim}
\myproof
Consider the increase in LHS .
Since  is unsuccessful, there are at most  good elements,
each of which may contribute  towards .
On the other hand, the bad elements can contribute at most .
Therefore,

It follows that

Since , we get that

\qed

Consider any set  that participates in iteration .
By Claim \ref{claim:EEE}, it must be unsuccessful. Therefore, by Claim \ref{claim:GGG},
. Moreover, by Claim \ref{claim:FFF}, . 
It follows that .
We conclude that .
This completes the proof of the lemma.
\qed

We shall derive a bound on the number of iteration by making some observation on the maximum and minimum values possible
for  and .
The  values can vary between  and .
The maximum value possible for  is ; 
the minimum value possible is  
(because sets with smaller  would have got added to ).
Therefore,  epoch  will take at most 
 iterations.
Hence, the overall forward phase algorithm runs in 
 iterations.

We next record some useful properties satisfied by the pair of solution 
output by the forward phase. These properties will be useful during the reverse-delete phase.
Partition the collection  into , where  is the
collection of sets added to  in the epoch  of the forward phase.
For , let  be the set of elements freshly covered by  (meaning, the elements
covered by  which are not covered by ).
We say that  is {\em responsible} for the elements in .
Intuitively, in epoch , the main task of the algorithm was to ensure coverage for 
and the sets in  were selected for this purpose. But some elements
belonging to  might also be covered by .
The set  consists of  and the above elements.

\begin{proposition}
\label{prop:XXX}
(i) For ,  consists of elements only from layers .
(ii) For , the collection  does not cover any element from .
(iii) The elements found in  are the only elements whose dual variables could 
potentially have been raised in the forward phase.
\end{proposition}

\subsection{Reverse Delete Phase}
The forward phase produces a pair of solutions .
In the reverse delete phase, we prune the collection  and obtain a solution 
such that the solution  satisfies the approximate complementary slackness property: for any , if  then

Furthermore, we will not alter the dual variables during the reverse-delete phase.
Hence, the final pair of solutions  and  satisfy 
both the primal and dual approximate complementary slackness properties, namely bounds (\ref{eqn:DDD}) and
(\ref{eqn:MMM}). The weak duality theorem implies that the solution  is an -approximate solution.

We now describe the reverse-delete phase that would satisfy the bound (\ref{eqn:MMM}).
By the third part of Proposition \ref{prop:XXX},
it suffices if we consider elements in .
The reverse delete procedure is also iterative and works in  epochs, 
but it will consider the layers in the reverse direction,
namely, the iterations are from  to .
Initialize . 
At the end of epoch , we will ensure two properties: 
(i) all the elements in  are covered by ;
(ii) all the elements in  obey  the slackness property (\ref{eqn:MMM}).

Assume by induction that we have satisfied the above two properties in iteration 
and consider epoch . Our plan is to ensure coverage of  by adding sets from  to 
(recall that  is responsible for ).
An important issue here is that the sets added to  in the previous iterations 
 will be from , 
which are not responsible for covering the elements in ; nevertheless, some of these sets might still be 
covering the elements in  (this is an unintended side-effect of the forward phase).
While ensuring slackness property (\ref{eqn:MMM}) for the elements in ,
we have to take the above phenomenon into account and may have to delete sets from .
In doing so, we should not affect the coverage of the elements in .
The procedure given by the lemma below helps us in achieving the above objectives;
the lemma is proved in Section \ref{sec:XY-lemma}.

\begin{lemma}
\label{lem:HHH}
Let  be a set of elements belonging to layers , for some given .
Let  be a cover for .
There exists a parallel procedure that takes  and  as input, and outputs a collection 
such that: (i)  is a cover for ; (ii) for any element in  belonging to layer ,
at most  sets from  cover . 
The algorithm takes at most  iterations, where the dominant operation
in each iteration is computing a maximal independent set (MIS) in an arbitrary graph.
\end{lemma}

We are now ready to discuss epoch . Let . 
Let .
Notice that the requirements of the Lemma \ref{lem:HHH} are satisfied by  and 
(because by induction,  covers  and 
 covers ).
Invoke the procedure given by the lemma and obtain a set . 

We claim that  satisfies two properties: 
(i)  is a cover for ;
(ii) for any element  in  at most  sets from  cover .
The first property is ensured by the lemma itself.
Moreover, the lemma guarantees that the second property is true for any element . 
So, consider an element  belonging to one of the sets .
The lemma ensures that 
and hence, the sets  must come from  or .
Proposition \ref{prop:XXX} implies that  does not contain any set covering .
Therefore, all the sets covering  must come from ; by the induction hypothesis,
there are at most  such sets.
We have shown that  satisfies the induction hypothesis.
We set  and proceed to the next epoch .

We see that the overall algorithm produces a -approximate solution.
Let us now analyze the running time.
We can preprocess the sets so that  is bounded by , while incurring an increase approximation ratio by an additive factor of one (see \cite{RV}).
Computing the layer decomposition will take  iterations and 
the forward phase will take  iterations, where each iteration can be implemented in NC.
The reverse delete phase consists of  iteration, where each iteration mainly involves
computing MIS, which can be computed in NC \cite{Luby}.
Thus, when  is logarithmic in , the overall algorithm runs in NC and produces an -approximate solution.

\begin{figure}[htbp]
\begin{center}
\fbox{\begin{minipage}  {\textwidth} \small
\begin{tabbing}
xx\=xx\=xx\=xx\=xx\=xx\= \kill
\textbf{Begin}  \\
\> // Forward Phase: \\
\> Compute the layer decomposition  (see Lemma \ref{lem:TTT}) \\
\> For all  \\
\> \> let  and let  \\
\> For  to  \\
\> \> let  be the set of elements not covered by  \\
\> \> let  \\
\> \> initialize  (sets selected in this epoch) \\
\> \> While  \\
\> \> \> For each  \\
\> \> \> \> let \\
\> \> \> \> let  \\
\> \> \> \> let  \\
\> \> \> \> let  \\
\> \> \> For each  \\
\> \> \> \>  \\
\> \> \> \> Raise  by  \\
\> \> \> For each  \\
\> \> \> \> If (  ) \\
\> \> \> \> \> Add  to  \\
\> \> \> \> \> Add  to  \\
\> \> \> \> \> Recompute  (i.e., the set of elements not covered by ) \\
\ \\
\> // Reverse-delete Phase: \\
\> Initialize  \\
\> For  down to  \\
\> \> let  \\
\> \> let  \\
\> \> let  \\
\> \> initialize  \\
\> \> partition the set  according to the layers: for , let  \\
\> \> For  to  \\
\> \> \> Let  be the elements of  not covered by  \\
\> \> \> Construct a graph  with  as the vertex set; \\
\> \> \> \> add an edge between two vertices  if  for some  \\
\> \> \> Find an MIS  within the graph  \\
\> \> \> For each  add its petals to the collection  \\
\> \> update  \\
\> Output  \\
\textbf{End}
\end{tabbing}
\end{minipage}
} \end{center}
\caption{Parallel Algorithm}
\label{fig:par}
\end{figure}

\subsection{Proof of Lemma \ref{lem:HHH}}
\label{sec:XY-lemma}
We initialize . Partition the set  according to the layers: for , let . 
We process the sequence  iteratively -- in each iteration ,
we will add some appropriate sets from  to  so as to ensure coverage for all elements in .

Consider any element . Let  be the partition to which  belongs.
Let  be the collection of all sets found in  which contain .
By the properties of layered decompositions,  is a -SNC element within .
Hence, there exist sets 
 (with ) such that
any element  covered by 
is also covered by one of .
We call these  sets as the {\em petals} of .

For  to , iteration  is described next.
Of the elements in , some of the elements would already be covered by .
Let the set of remaining uncovered elements be .
Construct a graph  with  as the vertex set; add an edge between two vertices ,
if some set  includes both of them. Find an MIS  within the graph . 
We call the elements in  as {\em anchors}.
For each anchor  add its petals to the collection .
Proceed to the next iteration.

We now prove that the collection  constructed by the above process satisfies the
properties stated in the lemma. First, consider the coverage property.
For , let us argue that  covers .
In the beginning of iteration ,  would have already covered some elements
from . So, we need to bother only about the remaining elements .
Consider any element . If  was selected as part of the MIS ,
then  is covered by its petals. Otherwise, there must exist some element 
such that  and  share an edge in . This means that some set 
contains both  and . Therefore one of the petals of  would cover .
Since we added all the petals of  to ,  would cover .

Consider the second part of the lemma. 
We shall first argue that any two anchors are independent: namely, for any two anchors,
 and , no set  contains both of them.
By contradiction, suppose some set  contains both  and .
Consider two cases: (i) the two elements belong to the same layer;
(ii) they belong to different layers.
The first case will contradict the fact that  is an MIS, where  is the layer to which both the anchors belong.
For the second case, suppose  and  with .
Our assumption is that the set  contains both  and .
This would mean that  will belong to one of the petals of .
Hence, in the beginning of the iteration , the collection  would have already covered .
This contradicts the fact that  is an anchor.

We return to the second part of the lemma.
Consider any element . We analyze two cases: (i)  is an anchor;
(ii)  is not an anchor. In the first case, since the anchors are independent,
the petals of no other anchor can include . So, the only sets in 
which include  are the petals of  itself; the number of such petals is at most .
Now, consider the second case. Let  be the set of all anchors  such that
at least one petal of  includes . We claim that .
By contradiction, suppose .
Take any  anchors  found in .
The element  belongs to the layer . So, it will be a -SNC element within
. 
Hence, the petals of  will cover all the anchors .
But, the number of petals of  is at most .
Hence, by the pigeon hole principle, two of these anchors must be covered by the 
same petal of . This contradicts our previous claim that the anchors are independent.
Therefore, . The element may belong to more than one petal of an anchor.
Each anchor  has at most  petals. 
It follows that at most  petals of the anchors can cover .
This proves the second part of the claim.

\section{A Distributed Algorithm for the -SNC Set Systems}
In this section, we describe a distributed algorithm for the set cover problem on -SNC set systems
having an approximation ratio of . It runs in 
communication rounds, where  is the decomposition length.
Thus when  is logarithmic in , the number of rounds in bounded by .
The algorithm is obtained by implementing the sequential algorithm in a distributed fashion by appealing to the Linial-Saks decomposition \cite{LS}.

The Linial-Saks decomposition goes via the notion of color class decompositions, described next.
Let  be a graph. A {\em color class decomposition}
of the graph  is a partitioning the vertex set  into {\em clusters}
.
The decomposition also specifies a set of {\em color classes}

and places each cluster  in exactly one of the color classes.
The decomposition must satisfy the following property:
any two clusters  and  placed in the same color class
must be independent; meaning, there should not be an edge in 
connecting some vertex  with some vertex .
We shall measure the efficacy of the decomposition using two parameters:
\begin{itemize}
\item
{\em Diameter}: For a cluster , let  be the maximum distance (number of hops in the
shortest path) between any pair of vertices in . Then, the diameter of the decomposition
is the maximum of  over all the clusters.
\item
{\em Depth}: The depth of the decomposition is the number of color classes .
\end{itemize}

Linial and Saks \cite{LS} showed that any graph has
a decomposition with  diameter and  depth, where  is the number of vertices in the graph.
They also presented a randomized distributed algorithm for finding such a decomposition
running in  communication rounds.

We now describe the distributed algorithm. 
Let  be the given set system.
The first step is to compute the the Linial-Saks decomposition of the graph determined by the communication network
of the set system.
Let  be the clusters and 
 be the color classes, where the depth .
For each cluster , we select a leader (say the element having the least ID).
Since the diameter of the cluster is , the leader can collect all the input data
known to the elements in the cluster in a single communication round.
The leader of the cluster will do all the processing for a cluster.

Compute the layer decomposition of the given set system  (see Lemma \ref{lem:TTT}).
The algorithm consists of a forward phase and reverse-delete phase.
We first describe the forward phase procedure which will process the layers from left to right.
It runs in  epochs, where epoch  will process the layer , as follows.
We take a pass over the color classes  in  steps,
where step  will handle the color class  and process each cluster in the color class .
For a cluster , the leader will consider all the elements in the
belonging to the layer  raise their dual variables using the same mechanism used in the
sequential algorithm. For each element adjacent to the some element in the cluster, 
the leader will then communicate the new values of the relevant dual variables and newly selected sets.
Since the clusters in any color class are independent,
the clusters of a color class can be processed simultaneously.
Each step can be implemented in  communication rounds.
The reverse-delete phase is similar, but processes the elements in the reverse order
and simulates the sequential algorithm.
The pseudo-code is presented in Figure \ref{fig:dist}.
The algorithm will run in  communication rounds.
Since the construction of the Linial-Saks decomposition takes  rounds,
the overall algorithm runs in  communication rounds.

\begin{figure}
\begin{center}
\begin{boxedminipage}{\hsize}
\begin{small}
\begin{tabbing}
xx\=xx\=xx\=xx\=xx\=xx\=xx\=xx\=xx\=xx\kill
\textbf{Begin}  \\
\> // Forward Phase:\\
\> Initialize. . For all , .\\
\> For  to \\
\> \> For  to \\
\> \> \> For each cluster  in the color class \\
\> \> \> \> Let  be the elements in  belonging to layer .\\
\> \> \> \> Arrange the elements in  in some arbitrary order .\\
\> \> \> \> For each element  in \\
\> \> \> \> \> If  is not covered by \\
\> \> \> \> \> \> Raise the dual variable  until some dual constraint becomes tight:\\
\> \> \> \> \> \> \> \\
\> \> \> \> \> \> \> Include the corresponding set  in :\\
\>\\
\> // Reverse-delete phase:\\
\> \\
\> For  to \\
\> \> For  to \\
\> \> \> For each cluster  in the color class \\
\> \> \> \> Scan the ordering  in the reverse order.\\
\> \> \> \> For each element , if  was raised in the forward phase do:\\
\> \> \> \> \> Let  be the neighbors of  not covered by  when  was raised.\\
\> \> \> \> \> Let  be the collection of sets covering \\
\> \> \> \> \> Find sets  (with ) such that\\
\> \> \> \> \> \> all the elements in  covered by  are also covered by \\
\> \> \> \> \> Delete all the sets found in  from , except \\
\> Output .\\
\textbf{End}
\end{tabbing}
\end{small}
\end{boxedminipage}
\end{center}
\caption{Distributed Algorithm}
\label{fig:dist}
\end{figure}

\section{Computing Layer Decomposition : Proof of Lemma \ref{lem:TTT}}
\label{sec:layer-compute}
We first present a polynomial time procedure
that take as input subset of elements  and an element ,
and tests whether  is a -SNC element within .

The following notation is useful in this context.
Let  be the input set system.
Let  be the collection of all sets that include .
We say that a subset  is -collapsible, if
there exist  sets  such that
every element in  covered by  is also covered by one of the above  sets
and ; the  sets are called the {\em base sets} of .
Testing whether  is a -SNC element within  is the same as testing
whether every collection  is -collapsible.
A naive algorithm would enumerate all the possible subsets of  and test whether each one of them
is -collapsible. However, such an approach may take exponential time.
The following combinatorial lemma helps in obtaining a polynomial time procedure.

\begin{lemma}
\label{lem:SSS}
Suppose every collection  of cardinality  is -collapsible.
Then, every collection  is -collapsible.
\end{lemma}
\myproof
Consider any collection  having cardinality at least  
(the claim is trivially true for smaller collections). 
Let the sets contained in the collection be  (for some ), arranged
in an arbitrary manner.
Via induction, we shall argue that for any , the collection 
 is -collapsible.
For the base case, the collection 
; this collection is -collapsible by the hypothesis of the lemma.
By induction, suppose the claim is true for the collection .
Now, consider the collection .
If , the we can simply add  to the sequence 
and get the base sets for the above collection. So, assume that .
By our hypothesis, the collection  must be -collapsible.
Let the collection of base sets of for the above collection be .
Observe that  form base sets for the collection .
Thus, we have proved the claim. The lemma follows by taking .
\qed

Based on the above lemma, it suffices if we consider collections  of cardinality .
The number of such collections is at most , where . 
For each such collection, we can test -collapsibility 
in time polynomial in .
Since  is assumed to be a constant, 
this yields a polynomial time procedure for testing an element  is a -SNC element within a set .

It is now easy to compute the layer decomposition of the given set system .
We consider every element  and test whether  is a -SNC element within .
All the elements passing the test are placed in .
We remove these elements and apply the same procedure on the remaining set of elements.
After  iterations, we would have computed the layer decomposition.

The above procedure runs in polynomial time in the sequential setting.
In the distributed setting, the algorithm can be implemented in  communication rounds.
In the parallel setting, each of the  iterations can be implemented in NC.

\section{Bound on the Decomposition Length for the Tree Cover Problem}
\label{sec:tree}
Recall that the set systems induced by the tree cover problem satisfy the -SNC property 
with decomposition length bounded by the depth of the tree. 
Such a layer decomposition would not be sufficient for obtaining polylogarithmic time bounds.
In this section we show that the set systems induced by the tree cover problem are -SNC set systems
having decomposition length only .

In the given tree , we say that a node  is a {\em junction}, if it has more than one children nodes.
It will be convenient to consider the root also as a junction, even if it has only one child.
Consider any leaf node . Let  be the path connecting the root and . 
Starting from the node  traverse up the path 
until we hit a junction  (or the root node itself). Consider the path  connecting  and ;
we call  as the {\em chain} defined by the leaf node  in the tree .
Let  be any edge on the path . We claim that  is a -SNC element.
Consider any set of interval  covering the edge .
Among these intervals, let  be the interval extending the most towards the leaf node 
and let  be the interval extending the most towards the root node.
Notice that for any interval , the intervals  and  put together cover
all the edges covered by . This shows that all the edges found on the chain  are -SNC elements.
In general, let  be the set of all leaf nodes in .
Let  be the chains defined by the above leaf nodes.
Then, all the edges found along these chains will be -SNC elements.

We shall apply the above procedure iteratively to decompose the set of all edges into chains.
Let  be the given tree. Consider iteration .
Find all the leaf nodes in the tree . Compute the chains defined by these leaf nodes.
Create a group  and put all the edges found on these chains in the group .
Delete all these edges along with their vertices, except for the junctions.
Let the remaining tree be . We then proceed to the iteration ,
and process the tree . We terminate the process when there are no more edges left.
The iterative procedure will terminate after some  iterations,
yielding groups . We call 
as the {\em chain decomposition} of the given tree . The quantity  is called the
{\em length} of the above decomposition. 

Let  be the -SNC layer decomposition of the set system.
We next prove that . We argued that all the edges in  are -SNC elements 
within the entire universe .
Extending this argument, we can show that for , the edges in  will be -SNC elements within ,
where  is the set of edges in the tree .
(Intuitively, this means that the edges in  will belong to layer . 
However, it is possible that some edges from  may belong to a lower layer;
this depends on how the input intervals are constructed). 
Using the above fact, we can formally show that for any ,
any edge  is found in some layer  (i.e., ).
It follows that .

Our next task is to prove a bound on .
Consider the sequence of trees .
Let  be the number of leaf nodes in these trees, respectively.
We claim that for , .
To see this, first notice that the leaf nodes of  are exactly the junctions in .
Thus, , where  are the number of junctions in .
Each junction in , by definition, would have at least  leaf nodes in the sub-tree beneath it.
Hence, . Thus the claim is proved. It follows that the number of leaf nodes
reduces by a factor of at least two in each iteration. Hence,  is at most 
and therefore, .

\section{Conclusions and Open Problems}
In this paper, we introduced the concept of -SNC set systems
and presented a sequential -approximation algorithm for the set cover problems on such systems.
For the case where the decomposition length is logarithmic, we presented distributed 
and parallel algorithms with approximation ratios of  and , respectively.
The parallel algorithm raises the following interesting open questions:
(i) In the forwards phase, can a -maximal dual solution
be produced in number of iterations independent of ? 
(ii) The reverse delete phase, the algorithm prodcues a primal intergal solution satisfying the primal slackness property
with parameter . Can this be improved to ? (iii) The zig-zag nature of the reverse delete phase
leads to  iterations. Can this be improved to ?
Both the distributed and parallel algorithms take number of rounds dependant on .
If this dependence can be removed, then we can hope to construct constant factor approximation
algorithms for -SNC set cover problems of arbitrary decomposition length (rather than logarithmic decomposition length 
addressed in the current paper). 


\bibliographystyle{plain}
\bibliography{main}

\end{document}
