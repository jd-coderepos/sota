\begin{table*}[h]
\centering
\caption{\textbf{PCK comparison} }
\label{pck-table}
\begin{tabular}{l | c | c | c | c | c | c | c | c }
\hline
& back & beak & belly & breast & crown & forehead & left-eye & left-leg\\
\hline
\hline
\cite{HuangXTZ_CVPR2016} & 80.7 & 89.4 & 79.4 & 79.9 & 89.4 & 88.5 & 85.0 & 75.0 \\
\hline
\cite{ZhangSGD_ICLRWorkshop2016} & 85.6 & 94.9 &81.9 & 84.5 & 94.8 & 96.0 & 95.7 & 64.6 \\
\hline
Ours  & \bf{91.3} & \bf{96.8} & \bf{89.0} & \bf{91.5} & \bf{96.9} & \bf{97.6} & \bf{96.9} & \bf{80.2}\\
\hline
& left-wing & nape & right-eye & right-leg & right-wing & tail & throat & \textbf{Overall} \\
\hline
\cite{HuangXTZ_CVPR2016} & 67.0 & 85.7 & 86.1 & 77.5 & 67.8 & 76.0 & 90.8 & 86.6\\
\hline
\cite{ZhangSGD_ICLRWorkshop2016} &  67.8 & 90.7 & 93.8 & 64.9 & 69.3 & 74.7 & 94.5 & N/A\\
\hline
Ours & \bf{76.8} & \bf{94.6} & \bf{97.4} & \bf{80.3} & \bf{75.3} & \bf{83.6} & \bf{97.4} & \bf{90.5}\\
\hline
\hline
\end{tabular}
\end{table*}











We test our algorithm on two datasets, the CUB-200-2011 dataset and the NABirds dataset.
The CUB-200-2011 contains 200 species of birds with 5994 training images and 5794 testing images. 
The NABirds dataset has 555 common species of birds in North America with a total number of 48,562 images.
Class labels and keypoint locations are provided in both datasets.



\subsection{Keypoint Prediction Performance}

We use PCK (Percentage of Correct Keypoints) score to measure the accuracy of keypoint prediction results.
A predicted keypoint ($p$) is ``correct'' if its within a small neighborhood of the ground truth location ($g$), or equally speaking,  
$$||p - g||_2 \le c * \max(h, w)$$ 
$c$ is a constant factor and $\max(h,w)$ is the longer side of the bounding box.

We evaluate our pose estimation network on CUB-200 and compare our PCK score with the others in Table ~\ref{pck-table}. 
We achieve highest score on all 15 keypoints with considerable leading margin.
We do especially well on legs and wings where other models struggle to make precise prediction.
Some visualization results are shown in~\ref{fig:kp_examples}

Although we localize wings and legs better than baselines, they still have the lowest PCK in our model.
This is caused by dramatic pose change as well as the appearance similarity between symmetric parts.
We note that using keypoints to denote the wings isn't always appropriate.
Because wings are two dimensional planar parts that spread over a relatively large area.
Designating a keypoint to the wing can be obscure,
because it is not easy to decide which point represents the wing location better. 
In fact, the ground truth keypoint location of the CUB dataset is the average of five annotators' results and 
it's even hard for them to reach a consensus. 


\begin{figure}[h]
        \centering
\includegraphics[width=0.8\linewidth]{figures/pose_estimation}
        \label{fig:kp_examples}
        \caption{ {\textbf{Keypoint Localization Results}}
        The top two rows show the keypoint detection results of our pose estimation network. Red dots represent the predicted location and black dots are the ground truth locations. Three failure patterns are shown in the third row. The first one confuses left and right because of the visual similarity between symmetric parts. Dramatic and rare pose causes degradation for keypoint localization as seen in the middle one. The last one shows an interesting case where the nose of a seal is mistakenly predicted as the gull's beak.
         }
\end{figure}



\subsection{Patch Classification Network} \label{sec:patch_network}
We adopt the ResNet-50 architecture as the patch classification network due to its high performance and compact GPU footprint.
Alternate architectures like VGG and Inception can easily be adapted. 
We now discuss two considerations which facilitate training the patch classification network.

\noindent\textbf{Symmetry.} 
For a given object with $n$ keypoints, the total number of patches to be classified is:
$$ \begin{pmatrix} n \\ 2  \end{pmatrix} = \frac{n(n-1)}{2} = \mathcal{O}(n^2)$$
which increases quadratically with $n$.
Most real world objects show some kind of symmetry. Due to the visual
similarity inherent in symmetric pairs keypoints (for example, right and left eyes, wings and feet), we treat
each pair as a hybrid keypoint in the patch generation process.  Many real-world objects, however, like birds, cats, cars, \etc~are symmetric in appearance.  Based on this observation, we propose to merge the patches for a symmetric pair of keypoints into a hybrid patch, \eg~\patchpair{left-eye}{tail} and \patchpair{right-eye}{tail} can be merged into the hybrid \patchpair{eye}{tail} pair.
\begin{comment}
We then train a separate CNN model for each hybrid patch.
We observe consistent patch accuracy increase after applying symmetry to the dataset, we believe this is caused by larger training set size.
Additionally, we also reduce the patch classifier number.
For example, in the CUB dataset, total classifiers are reduced from from 105 to 69.  For the NABirds dataset, total classifiers are reduced from 55 yo 37.
At inference stage, we apply hybrid patch model to each original patch, so we still have the full set of patch scores.
\end{comment}

As a result, the total number of patch classification networks is reduced from 105 to 69 for the CUB dataset; on the NABirds dataset, the number is reduced from 55 to 37.

\noindent\textbf{Visibility.} 
\begin{comment}
Due to self-occlusion and foreground-occlusion, some keypoints may be missing from the image. 
One way to deal with keypoint invisibility is to eliminate patches with invisible keypoints. 
The advantage is it keeps a purified dataset with less noisy patches. 
The disadvantage, however, is the reduction of training set size.
Take the CUB dataset as an example, the total effective patch number after elimination is ???., ???\% decrease from the full patch dataset. 
The second strategy is to treat all keypoints as visible and take the maximum activation point as the keypoint location.
As can be seen from Graph [???] this dataset shrinkage affects the patch classifier accuracies negatively compared to full patches. 
Reasons for this include: (1) the unreasonable effectiveness of noisy data and (2) the pose estimation network will make a reasonable guess even if certain keypoints are missing. 
\end{comment}
Due to self-occlusion or foreground-occlusion, not all keypoints are visible in the image. Previous works~\cite{HuangXTZ_CVPR2016} would eliminate patches with invisible keypoints to purify the input data. 
Contrarily, we find that this would hurt the performance of the patch classifiers. Details for comparison can be found in Figure~\ref{fig:many_graph}.
We believe this degradation is caused by the shrinkage of effective training set size. 
This is a similar finding with~\cite{KrauseSHZTDPF_ECCV2016} that noisy but abundant data consistently outperforms clean but limited-sized data. 
Additionally, the pose estimation network would make a reasonable guess even if the keypoint is invisible. 
So during patch classifier training, all keypoints are considered visible by taking the maximally activated location.


\subsection{Classification Network}

Based on the assumption that image patches should contribute differently to classification. Four different strategies are explored and we describe details of them in this section.

\noindent\textbf{Fixed patch selection.}
We assume only few patches contains useful information and others may merely act as noise.
We propose a fixed patch selection strategy to keep the best $k$ patches.A greedy search algorithm would evaluate each $n$ choose $k$  combinations for $k$  from 1 to $n$. 
The number of evaluations needed for this algorithm is 
The complexity grows in the order of $n!$ and quickly becomes intractable. 
We thus employ the \emph{beam search} algorithm.
Instead of greedily searching the whole parameter space, we only keep a fixed $k$ combinations each iteration and build our search path based only on previously learned $k$ patch combinations. 
Out of curiosity,
we also do beam search on the testing set alone. 
This operation, although invalid, provides some insights in the potential of our pose aligned patch representation. 
The results are shown in Figure~\ref{fig:many_graph}.
Our observations is that without overfitting, the potential of fixed patch selection should be well above 89\%, compared to the current state-of-the-art~\cite{LamMT_CVPR2017} 87.5\%. Notably, a simple average over all strategy can achieve 87.6\%.


\noindent\textbf{Dynamic patch selection.} As an alternative attempt 
we experiment with is the sparsely gated network~\cite{ShazeerMMDLHD_ArXiv2017} for dynamic patch selection. 
Different from the beam search algorithm which selects fixed patches for each input,
the gated network would select different combinations depending on the input.
A tiny network is trained to predict weights for each patch and an explicit sparsity constraint is exposed on the weight to only allow $k$ non-zero elements. 
A Sigmoid layer is added to normalize the weight.
The network architecture can be described as,
$$G(x)=\text{softmax}(\text{top\_k}(H(x)))$$
$H(x)$ represents the mapping function from the input to patch weight. $G(x)$ is the patch selection function.
Different architectures for the tiny network are tried and we find a simple linear layer would work decently most of the time.
Best accuracy is achieved when $k$ = 105. 
Interestingly when $k$=1, our dynamic patch selection performs worse than the fixed patch selection, implying the gated network's inability to learn useful information for decision making.

\noindent\textbf{Sequential patch weighting.} Recurrent neural networks (RNN) is specialized at processing sequential data like text and speech.
 RNN has been widely adopted as an attention mechanism to focus on different parts sequentially. We instead employ RNN for sequential patch weighting, aiming to discover different patches for decision making.
We employ a one-layer Long Short Term Memory (LSTM) network with 512 nodes. Each node has a hidden layer of size 1024. 
The last output of sequence is selected as the final output.
We get 82.7\% in this experiment and this confirms the effectiveness of the LSTM network.

\noindent\textbf{Static patch weighting} The final and most effective method we tried is the MLP network.
The MLP network contains one hidden layer with 1024 parameters,
followed by the batch normalization layer, ReLU layer, and then the output layer.
On CUB our final accuracy is 88.7\% , 1.2\% higher than the current state-of-the-art result.
We combine pairs patch with single keypoint patch and achieves a new state-of-the-art 89.2\% accuracy.
We compare our result with several other strong baselines in Table-~\ref{class-table}.

We test our algorithm also on the NAbirds dataset and the result is shown in~\ref{nabirds_table}. Our algorithm attains an accuracy of 87.9\%, more than 8\% better than a strong baseline. 

\begin{table}
\centering
\caption{ \textbf{Classification score on CUB.} 
 Annotation key as follows: GT = class labels; BB = bounding box annotation; KP = keypoint annotations; WEB = images downloaded from the Internet.}  
\label{class-table}
\begin{tabular}{|l | c | c |}

\hline
& Annotations & Accuracy \\
\hline
\hline
Huang \etal~\cite{HuangXTZ_CVPR2016} & GT+BB+KP & 76.2 \\
Zhang \etal~\cite{ZhangDGD_ECCV2014} & GT + BB & 76.4 \\
Krause \etal~\cite{KrauseJYF_CVPR2015} & GT+BB & 82.8 \\
Jaderberg \etal~\cite{JaderbergSZK_NIPS2015}  & GT & 84.1 \\
Shu \etal~\cite{KongF_CVPR2017} & GT & 84.2 \\
Zhang \etal~\cite{ZhangXZLT_CVPR2016} & GT & 84.5 \\
Xu \etal~\cite{XuHZT_ICCV2015} & GT+BB+KP+WEB & 84.6 \\
Lin \etal~\cite{LinRM_ICCV2015} & GT+BB & 85.1 \\
Cui \etal~\cite{CuiZWLLB_CVPR2017} & GT & 86.2 \\
Lam \etal~\cite{LamMT_CVPR2017}& GT+KP & 87.5 \\
\hline
\hline
PAIRS Only & GT + KP & 88.7 \\
PAIRS+Single & GT + KP & \textbf{89.2} \\
\hline

\end{tabular}

\end{table}

\begin{table}[h]
    \begin{center}
     \caption{Performance Comparison on the NABirds dataset.}
\begin{tabular}{|l|c|}
	\hline
	Algorithm & Accuracy\\
	\hline
    ResNet-50 Baseline & 79.2\% \\
	\hline
    Bilinear CNN (PAMI 2017)~\cite{LinRM_PAMI2017} & 79.4\% \\
    \hline
    {\bf PAIRS} & \bf{87.9\%} \\
	\hline
\end{tabular}

\label{nabirds_table}
\end{center}
\end{table}

\begin{figure}[h]
        \centering
\includegraphics[width=0.8\linewidth]{figures/scatter}
        
        \caption{ {\textbf{Patch Accuracy: Axis-Aligned Patches v.s. PAIRS Patches}}
        We compare the patch accuracy of single keypoint patches of keypoint \emph{k} and PAIRS patches containing \emph{k}. The x axis is the keypoint id and the y axis is the patch accuracy. Most single keypoint patch are inferior to PAIRS patches in terms of patch accuracy.}
        \label{fig:scatter}
\end{figure}






\begin{figure*}[h]
        \centering
       \includegraphics[width=0.45\linewidth]{figures/full_miss_bw}
        \includegraphics[width=0.45\linewidth]{figures/hard_case_bw}
        \includegraphics[width=0.45\linewidth]{figures/hist_bw}
        \includegraphics[width=0.45\linewidth]{figures/beam-search-2}
       
        \caption{\textbf{Result visualization.} Top left: we show patch classification network accuracies using two strategies, visible keypoint patch and full keypoint patch separately. This verifies that treating all keypoints as visible will help improve the patch classifier's accuracy. Top right: hard case mining by correct prediction patch number. In the bottom we show sample images from hard to easy. Bottom left: distributions of patch classifier performance. Some examples are shown in the text box. Bottom right: Beam search results using two strategies, patch finding on training set (blue) and testing set (orange). The later one is purely for the estimation of the potential of the pair representation.
}
\label{fig:many_graph} 
\end{figure*}

\subsection{Ablation Study}

\textbf{Axis-Aligned v.s. Pose-Aligned}
We compare and show patch classification accuracy using pose-aligned patches v.s. single keypoint based axis-aligned patches in Figure~\ref{fig:scatter}. Single keypoint based patches performs consistently poorly compared to the pairs patches, confirming the effectiveness of disentangled feature representation.

\textbf{Patch Size Study}
One hyper-parameter in our algorithm is the pose-aligned patch size. We tries several size options on the best performing patch. We see that the larger-size patches generally yield better accuracy. We adopt $256 \times 512$ empirically because our base model is pretrained for such size.

\textbf{Choice of Pose Estimation Network} To test the influence of pose estimation network on the proposed algorithm, we train a separate Stacked Hourglass Network~\cite{newell2016stacked} for comparison. Stacked Hourglass Network is about ~2\% better than the FCN on PCK score, but the final classification accuracy numbers are comparable.

\subsection{Results Visualization}
\begin{comment}
Given 105 patches on one image, we are interested to know how prediction accuracy is distributed across the patches. 
We show the patch effectiveness histogram in Figure~\ref{fig:patch_classification}. We notice there is one patch does extremely poorly, only achieving 14.3\% accuracy.
This patch corresponds to the \patchpair{left-leg}{right-leg} pair
\end{comment}

We show the patch classification accuracy for each patch in the CUB dataset in Figure~\ref{fig:many_graph}. The best performing patch corresponds to \patchpair{belly}{crown}, achieving 79.6\% accuracy. The worst performing patch is the \patchpair{left-leg}{right-leg} pair which achieves only  15.7\% accuracy. Empirically, global patches perform better than local patches, however local patches are also important for localizing discriminative object parts. Patches found by beam search, as shown in Figure~\ref{fig:many_graph}, can provide insight -- a combination of global and local patches are selected to achieve an optimal result.




\begin{comment}
We also identify hard cases by patch score.
If an image can be classified correctly by most of its patches,
then it belongs to the easy category.
Otherwise, it belongs to hard case category. 
The image difficulty distribution is shown in Figure ~\ref{fig:image_difficult}. This figure also provides sample images ranging from hard to easy. Note the increasingly cluttered background towards the difficult end.
\end{comment}

As hard cases often can only be classified by a few highly localized discriminative parts, the number of patches with correct predictions reflects the difficulty of the image. We propose to use the correctly predicted patch number as the indicator of the image difficulty. This is a histogram reflecting the count of many images have a given number of patches correctly predicted the class (top right plot in Figure~\ref{fig:many_graph}). Example images, ranging from hard on the left, to easy on the right, are shown below; the hard cases can be due either to very similar/easily confused classes or to pose-estimation failure.

\begin{comment}
\subsection{Patch Size Study} \label{sec:patch-size}
\pei{Patch size is an important hyper parameter in our experiment. 
To study its affect on the patch classifier accuracy, 
we choose the \patchpair{crown}{left-leg} patch and run ResNet-50 on 4 different sizes of input. 
Results are shown in Table~\ref{patch-size-table}
We find generally larger patch will get better accuracy score. 
We choose 256 $\times$ 512 also because our ResNet-50 models are pretrained on such resolution on ImageNet dataset.}
\end{comment}




