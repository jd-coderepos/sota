\section{Experiments}



\bd{We implement the proposed network in PyTorch~\cite{paszke2019pytorch}. In the training phase, random initialization is used for all components except the FFE (which is a ResNet18 pre-trained on ImageNet). The initial learning rate for the classifier, the bbox regressor, and the CDFI are set to 1e-3, 1e-3, and 1e-4, respectively. The learning rate is adjusted by a decay scheduler, which is scaled by 0.2 for every 15 epochs. We use Adam optimizer to train the network for 50 epochs. The batch size is set to 26. It takes about 20 hours on a 20-core i9-10900K 3.7 GHz CPU, 64 GB RAM, and an NVIDIA RTX3090 GPU.}


\setlength{\tabcolsep}{1.2pt}
\begin{table*}[t]
\centering
	\small
	
\scalebox{0.9}{
	\begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
		\hline
		\hline
		\multirow{2}{*}{methods} & \multicolumn{4}{c|}{HDR} & \multicolumn{4}{c|}{LL} & \multicolumn{4}{c|}{FWB}   & \multicolumn{4}{c|}{FNB}     & \multicolumn{4}{c}{ALL} \\
		\cline{2-21} 
		& RSR & OP$_{0.50}$ &OP$_{0.75}$ & RPR     & RSR &OP$_{0.50}$ &OP$_{0.75}$    & RPR       & RSR &OP$_{0.50}$ &OP$_{0.75}$  & RPR   & RSR &OP$_{0.50}$ &OP$_{0.75}$    & RPR   & RSR &OP$_{0.50}$ &OP$_{0.75}$ & RPR  \\ 
		\hline
		SiamRPN~\cite{li2018high} &15.3   &16.9   &6.1    &21.6 &10.1   & 8.3  & 1.4   &14.5 &26.2  & 32.1  &6.1    &44.1 &33.2  &42.9   &11.5    &51.9 &21.8  & 26.1  &7.0    &33.5 \\ 
		ATOM~\cite{danelljan2019atom}  &36.6   &41.8   &14.4    &56.0 &28.6  &29.1   &5.8    &45.0 &66.8  &89.6   &32.6    &96.7 &57.1  &71.0   &28.0    &88.6 &46.5  &56.4   &20.1    &71.3 \\
		DiMP~\cite{bhat2019learning}   &41.8   &50.0  &17.9    &62.7 &45.6  &52.8  &11.2    &69.5 &69.4  & \textbf{94.7}  & 37.1   &99.7 &60.5  &75.6   &29.3    &93.2 &52.6  &65.4   &23.4    &79.1  \\
		SiamFC++~\cite{xu2020siamfc++}  &15.3   &15.0   &1.3    &25.2 &13.4  & 8.7  &0.8    &15.3 &28.6  & 36.3  &6.0    &48.2 &36.8  & 42.7  &7.4    &63.1 &23.8   &26.0   &3.9   &39.1 \\
		SiamBAN~\cite{chen2020siamese}  &16.3   &16.4   &3.9    &26.6 &15.5  & 14.8  &2.3    &26.5 &25.2  &26.3   &5.8    &46.7 &32.0  & 39.6  & 9.1   &51.4 &22.5  &25.0   &5.6    &37.4  \\
		KYS~\cite{bhat2020know}  &15.7   & 14.5  &5.2    &23.0 &12.0  & 8.0  &1.1    &18.0 &47.0  &63.9   &14.8    &73.3 &36.9  &44.5   &15.2    &57.9 &26.6  & 30.6  &9.2    &41.0  \\
		CLNet~\cite{dongclnet}  &30.0   &33.5   &9.6    &48.3 &13.7  & 6.0  &0.9    &23.6 &52.9  & 71.2  &23.3    &80.3 &40.8  & 46.3  &14.2    &67.7 &34.4  & 39.1  &11.8    &55.5  \\
		PrDiMP~\cite{danelljan2020probabilistic}   &44.3   &52.8   &19.6    &66.3 &44.6  &48.2   &8.9    &69.5 &67.0  & 89.9  &33.6    &99.7 &60.6  & 75.8  &\textcolor{blue}{29.7}    &93.3 &53.0   &65.0   &23.3   &80.5 \\
		
		\hline
		ATOM~\cite{danelljan2019atom} + Event &49.0  &59.2   &21.0    &68.8 &50.8   & 67.8  & 27.7   &72.6 &68.5  &90.4  &42.0    &97.2 &57.4  &71.1   &28.3    &90.2 &55.5  & 70.0  &27.4    &81.8 \\ 
		DiMP~\cite{bhat2019learning} + Event &50.1   &60.2   &23.7    &74.8 &57.0   &70.4  &28.2   &82.8 &\textcolor{blue}{70.1}  &\textcolor{blue}{94.2}  &44.2    &\textcolor{blue}{99.9} &60.8  &75.9   &29.1    &\textcolor{blue}{93.6} &57.1  & 71.2  &28.6    &85.1 \\ 
		PrDiMP~\cite{danelljan2020probabilistic} + Event &\textcolor{blue}{53.1}   &\textcolor{blue}{65.3}   &\textcolor{blue}{24.9}    &\textcolor{blue}{79.1} &\textcolor{blue}{60.3}   & \textcolor{blue}{79.6}  & \textcolor{blue}{29.8}   &\textcolor{blue}{90.5} &70.0  & 93.8  &\textcolor{blue}{44.8} &99.8   &\textcolor{blue}{61.8} &\textcolor{blue}{76.3}  &29.4   &\textcolor{blue}{93.6}   &\textcolor{blue}{59.0}  & \textcolor{blue}{74.4}  &\textcolor{blue}{29.8}    &\textcolor{blue}{87.7} \\ 
		
		\hline
		Ours  &\textbf{59.9}   & \textbf{74.4}  &\textbf{33.0} &\textbf{86.0} &\textbf{65.6}  & \textbf{86.0}  & \textbf{30.8}  &\textbf{95.7} &\textbf{71.2}  & \textbf{94.7}  &\textbf{45.9} &\textbf{100.0} &\textbf{62.8}  & \textbf{80.5}  & \textbf{32.0}  &\textbf{94.5} &\textbf{63.4}   & \textbf{81.3} &\textbf{34.4} &\textbf{92.4}  \\
		\hline
		\hline
	\end{tabular}}
	\caption{\bd{State-of-the-art comparison on FE108 in terms of representative success rate (RSR), representative precision rate (RPR), and overlap precision (OP).}}
	\label{tab:attribute}
	\vspace{-0.35cm}
\end{table*}


\subsection{Comparison with State-of-the-art Trackers}
\bd{To validate the effectiveness of our method, we compare the proposed approach with the following eight state-of-the-art frame-based trackers: SiamRPN~\cite{li2018high}, ATOM~\cite{danelljan2019atom}, DiMP~\cite{bhat2019learning}, SiamFC++~\cite{xu2020siamfc++}, SiamBAN~\cite{chen2020siamese}, KYS~\cite{bhat2020know}, CLNet~\cite{dongclnet}, and PrDiMP~\cite{danelljan2020probabilistic}. To show the quantitative performance of each tracker, we utilize three widely used metrics: success rate (SR), precision rate (PR), and overlap
precision ($\text{OP}_\text{T}$). These metrics represent the percentage of three particular types of frames. SR cares the frame of that
overlap between ground truth and predicted bounding box is larger than a threshold; PR focuses on the frame of that the center distance
between ground truth and predicted bounding box within a given threshold; $\text{OP}_\text{T}$ represents SR with $T$ as the threshold. For SR, we employ the area under curve (AUC) of an SR plot as representative SR (RSR). For PR, we use the PR score associated with a 20-pixel threshold as representative PR (RPR). }

\def\wdenoising{1.0\linewidth}
\def\hdenoising{1.2in}
\begin{figure}[tbp]
	\setlength{\tabcolsep}{1.0pt}
	\centering
	\small
	\begin{tabular}{c}
		
\includegraphics[width=\wdenoising]{figure/prsr/fesr.pdf}  \\


	\end{tabular}
	\caption{\bd{Precision (left) and Success (right) plot on FE108. In terms of both metric, our approach outperforms the state-of-the-art by a large margin.}}
	\label{fig:PRSR}
	\vspace{-0.5cm}
\end{figure}

\bd{Illustrated as the solid curves in Figure~\ref{fig:PRSR}, on FE108 dataset, our method outperforms other compared approaches by a large margin in terms of both precision and success rate. Specifically, the proposed approach achieves a 92.4\% overall RPR and 63.4\% RSR, and it outperforms the runner-up by 11.9\% and 10.4\%, respectively. To get more insights into the effectiveness of the proposed approach, we also show the performances under four different challenging conditions provided by FE108. As shown in Table~\ref{tab:attribute}, our method offers the best results under all four conditions, especially in LL and HDR conditions. Eight visual examples under different degraded conditions are shown in Figure~\ref{visual}, where we can see our approach can accurately track the target under all conditions.}









		


\def\wdenoising{1.0\linewidth}
\def\hdenoising{1.2in}
\begin{figure}[b]
    \vspace{-0.35cm}
	\setlength{\tabcolsep}{1.0pt}
	\centering
	\small
	\begin{tabular}{c}
\includegraphics[width=\wdenoising]{figure/prsr/eedsr.pdf}  \\ 
	\end{tabular}
	\caption{\bd{Precision (left) and Success (right) plot on EED~\cite{mitrokhin2018event}.}}
	\label{fig:PRSR_eed}
\end{figure}

\setlength{\tabcolsep}{0.85pt}
\begin{table}[t]
\small


	
	    \scalebox{0.9}{
	\begin{tabular}{l|cc|cc|cc|cc|cc|cc}


		\hline   
		\hline
		\multirow{2}{*}{Methods} &  \multicolumn{2}{c|}{FD} &  \multicolumn{2}{c|}{LV}  & \multicolumn{2}{c|}{Occ} & \multicolumn{2}{c|}{WiB}
		& \multicolumn{2}{c|}{MO} & \multicolumn{2}{c}{ALL}\\
		\cline{2-13} 
		
		& \footnotesize{RSR} &\footnotesize{RPR} &\footnotesize{RSR} &\footnotesize{RPR} &\footnotesize{RSR} &\footnotesize{RPR} &\footnotesize{RSR} &\footnotesize{RPR} &\footnotesize{RSR} &\footnotesize{RPR} &\footnotesize{RSR} &\footnotesize{RPR} \\
		\hline                  
		SiamRPN~\cite{li2018high} &23  &43 &11 &10 &38 &40 &43 &63 &53 &100  &33  &51 \\
		ATOM~\cite{danelljan2019atom} &12   &19  &7 &12  &47 &60  &74 &100  &47  &100  &37  &58  \\
		DiMP~\cite{bhat2019learning} & 9  &19  &2 &4  &48 &60  &\textbf{79} &100  &50  &100  &37  &57 \\
		SiamFC++~\cite{xu2020siamfc++} & 17 &52 &10 &26 &45 &60 &58 &63 &50 &100 &36 &60 \\ SiamBAN~\cite{chen2020siamese} & 22  &43 &8  &6 &36  &40  &69 &100  &54 &100  &38 &58 \\ KYS~\cite{bhat2020know} &19  &38 &6 &19 &46 &60 &46 &63 &54 &100 &34 &56 \\
		CLNet~\cite{dongclnet} &10  &19 &2 &6 &19 &20 &13 &25 &4 &13 &9 &17 \\
		PrDiMP~\cite{danelljan2020probabilistic} &9  &14 &4 &22 &19 &20 &78 &100 &31 &70 & 28 & 45\\
	    Ours &\textbf{32}  &\textbf{81} &\textbf{35} &\textbf{98} & \textbf{48}  &\textbf{60} &69 &\textbf{100} &\textbf{55} & \textbf{100} & \textbf{48} & \textbf{88} \\	
		\hline
		\hline
	\end{tabular}}
	\caption{\bd{State-of-the-art comparison on EED~\cite{mitrokhin2018event} in terms of RSR and RPR.}} \label{tab:eed}
	\vspace{-0.5cm}
\end{table}


\bd{
Even though EED~\cite{mitrokhin2018event} has very limited frames and associated events, it provides five challenging sequences: fast drone (FD), light variations (LV), occlusions (Occ), what is background (WiB), and multiple objects (MO). The first two sequences both record a fast moving drone under low illumination. The third and the fourth sequences record a moving ball with another object and a net as foreground, respectively.
The fifth sequence consists of multiple moving objects under normal lighting conditions. Therefore, we also compare our approach against other methods on EED~\cite{mitrokhin2018event}. The experimental results are shown in Figure~\ref{fig:PRSR_eed} and Table~\ref{tab:eed}. Our method significantly outperforms other approaches in all conditions except WiB. But with limited frames, the experimental result is less convincing and meaningful compared to the ones obtained from FE108.  
}







\bd{One question in our mind is whether combining the frame and event information can make other frame-based approaches outperform our approach. To answer this question, we combine APS and event aggregated frame by concatenation manner to train and test the top three frame-based performers (\ie, PrDiMP~\cite{danelljan2020probabilistic}, DiMP~\cite{bhat2019learning}, and ATOM~\cite{danelljan2019atom}). We report their RSR and RPR in Table~\ref{tab:attribute} and show the corresponding results as the dashed curve in Figure~\ref{fig:PRSR}. As we can see, our approach still outperforms all others by a considerable margin. It reflects the effectiveness of our specially designed cross-domain feature integrator. We also witness that the performance of the three chosen approaches can be improved significantly only by naively combining the frame and event domains. It means event information definitely plays an important role in dealing with degraded conditions.}





























\def\wdenoising{0.9\linewidth}
\def\hdenoising{2.5in}
\begin{figure*}[]
	\setlength{\tabcolsep}{1.0pt}
	\centering
	\begin{tabular}{c}
		\includegraphics[width=\wdenoising,height=\hdenoising]{figure/single_event_redo.pdf}\\
		
	\end{tabular}
	\caption{\bd{Visual outputs of state-of-the-art algorithms on FE108 dataset. The lower-right dashed boxes show accumulated event frame of the dashed boxes inside the frames.}}
	\label{visual}
	\vspace{-0.4cm}
\end{figure*}



	


\subsection{Ablation Study}
\label{section:ablation}
\noindent
\tb{Multi-modal Input.}
\bd{We design the following experiments to show the effectiveness of multi-modal input.  1. Frame only: only using frames and \textbf{FFE}; 2. Event only: only using events and \textbf{EFE}; 3. Event to Frame: combining frames and events by concatenation as input to \textbf{FFE}; 4. Frame to Event: the same as 3, but input to \textbf{EFE}. For each setup, we train a dedicated model and test with it. As shown in the row \textit{A-D} of Table~\ref{tab:ablation}, the models with multi-modal inputs perform better than the ones with unimodal input. It 
shows the effectiveness of multi-modal fusion and our CDFI.}


\noindent


\setlength{\tabcolsep}{4.0pt}
\begin{table}[t]
\centering

	
    \scalebox{0.9}{
	\begin{tabular}{cl|cccc}
		\hline
		\hline
		& Models & RSR$~\uparrow$ &OP$_{0.50}$$~\uparrow$ &OP$_{0.75}$$~\uparrow$  &RPR$~\uparrow$   \\
		
		\hline
	\textit{A.}	& Frame Only  & 45.6  &54.6   & 21.0  &73.1  \\
	\textit{B.}	& Event Only  & 52.0  & 63.2  & 20.3  &82.0   \\
	\textit{C.}	& Event to Frame &55.5   &70.0 &27.4   &82.8     \\
	\textit{D.}	& Frame to Event &53.6    &66.5  &25.9    &80.4     \\
		\hline
	
	\textit{E.}	& w/o EAB & 60.7  & 77.9 & 31.7  & 88.6    \\
	\textit{F.}	& w/o CDMS &  59.8 & 75.8   & 31.0  &88.1  \\
	\textit{G.}	& CDMS w/o SA  & 62.6  &79.8 &33.8   &91.5   \\
	\textit{H.}	& CDMS w/o CA  & 61.9  &78.8  &33.0   &90.7  \\
	\textit{I.}	& CDMS w/o AW  & 60.9  &77.2 &32.0   &89.9   \\
		
	\hline
	\textit{J.}	& TSLTD~\cite{chen2020end} &60.4  & 77.0 &31.2  &89.2   \\
	
	\textit{K.}	& Time Surfaces~\cite{lagorce2016hots}  & 61.4 &78.5   &32.9 &90.1   \\
	\textit{L.}	& Event Count~\cite{maqueda2018event}  &59.6  & 76.4   &27.4  & 88.6   \\
	\textit{M.}	& Event Frame~\cite{rebecq2017real}  &59.0  &74.5  &29.9  &87.7   \\
	\textit{N.}	& Zhu \textit{et al}.~\cite{zhu2019unsupervised} & 61.9 &79.2& 32.3 &  91.2   \\
	
	\hline
	\textit{O.}	& All $w=1$ & 61.3  &78.1 &31.6   &90.1     \\
	\hline
	\textit{P.}	& Ours  &\textbf{63.4}   &\textbf{81.3} &\textbf{34.4}   &\textbf{92.4}    \\
		\hline
		\hline
	\end{tabular}
	}
	\caption{\bd{Ablation study results.}}
	\label{tab:ablation}
	\vspace{-0.6cm}
\end{table}







\noindent
\bd{
\tb{Effectiveness of the proposed key components.} There are two key components in our approach: EAB and CDMS. Inside the CDMS, there are three primary schemes: self-attention (Eq.~\ref{eq:CDMS_SA}), cross-attention (Eq.~\ref{eq:CDMS_CA}), and adaptive weighting (Eq.~\ref{eq:CDMS_aw}). To verify their effectiveness, we modify the original model by dropping each of the components and retrain the modified models. Correspondingly, we obtain five retrained models: (i) without EAB; (ii) without CDMS; Inside CDMS, (iii) without self-attention (CDMS w/o SA); (iv) without cross-attention (CDMS w/o CA); (v) without adaptive weighting (CDMS w/o AW). The results of the five modified models are shown in the row \textit{E-I} of Table~\ref{tab:ablation}, respectively. Compared to the original model, removing CDMS has the most considerable impact on the performance, whereas removing the self-attention influences the least. It confirms the proposed CDMS is the key to our outstanding performance. Moreover, removing EAB also influences performance significantly. It shows that the EAB indeed enhances the extracted edge features.}

\bd{Inside CDMS, removing adaptive weighting scheme degrades performance the most. To get more insights into it, we report the estimated two weights (\ie, $w_f$ for the frame domain; $w_e$ for the event domain) of all eight visual examples in Figure~\ref{visual}. Except for the second one, the frame domain cannot provide reliable visual cues. Correspondingly, we can see the $w_e$ in these seven examples are significantly higher than $w_f$, whereas $w_e$ is much lower than $w_f$ in the second scene. The fourth one provides an interesting observation. We can see the object clearly in the frame domain, but $w_e$ is still higher than $w_f$. We think it is because the model is trained to focus on texture cues in the frame domain, but no texture cues can be extracted in this case. 
It is worthwhile to mention that only our method can successfully track the target in all examples.}

\noindent
\tb{Event Aggregation.}
\bd{For the events captured between two adjacent frames, we slice them into $n$ chunks in the time domain and then aggregate them as EFE's inputs. Here, we study the impacts of hyperparameter $n$. As shown in Table~\ref{tb:n_test}, both RSR and RPR scores increase with a larger $n$ value. However, with a larger $n$ value, it slows down the inference time. We can see $n=3$ offers the best trade-off between accuracy and efficiency. The way of aggregating events is another factor that has an impact on the performance. We conducted experiments with five commonly used event aggregation methods~\cite{chen2020end,lagorce2016hots,maqueda2018event,rebecq2017real,zhu2019unsupervised}. The results are shown in the row \textit{J-N} of Table~\ref{tab:ablation}, and our method still delivers the best performance. It suggests that discretizing the time dimension and leveraging the recent timpstamp information are effective for tracking. Another component associated with event aggregation is the weights in Eq.~\ref{equ:weight}, which are learned during the training process. 
We manually set the weights to 1 with $n=3$. The result is shown in row \textit{O} of Table~\ref{tab:ablation}, and we can see the corresponding performance is worse than the original model. 
}

\setlength{\tabcolsep}{10.0pt}
\begin{table}[t]
\small
	\centering
	
	    \scalebox{0.9}{
	\begin{tabular}{c|cccccc}
		\hline
		\hline
		$n$ & 1  & 2 &3 &4 &5  & 6    \\
		\hline
		RPR$~\uparrow$  & 89.3  &90.1 &92.4 &92.6 &92.6   &92.7      \\
		RSR$~\uparrow$  & 60.2  & 61.7 & 63.4  &63.8  &63.4 &63.9      \\
		FPS$~\downarrow$ &35.1   &32.7  &30.1 &27.9   &25.2 &22.7      \\
		\hline
		\hline
	\end{tabular}}
	\caption{\bd{Trade-off between accuracy and efficiency introduced by the number of slices of event aggregation (\ie, $n$).}}
	\vspace{-0.46cm}
	\label{tb:n_test}
\end{table}
















