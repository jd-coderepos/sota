\documentclass[preprint]{sigplanconf}


\usepackage[T1]{fontenc}
\usepackage[safe]{tipa} \usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{etex}
\usepackage{stmaryrd}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[amsmath,thmmarks]{ntheorem}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage[numbers]{natbib}
\usepackage{mathpazo}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{mathpartir}
\usepackage{upgreek}
\usepackage{enumerate}

\newcommand\pfun{\mathrel{\ooalign{\hfil\hfil\cr\cr}}}

\usepackage{hyperref}

\usetikzlibrary{shapes.geometric}
\usetikzlibrary{calc}
\usetikzlibrary{shapes.multipart}
\usetikzlibrary{arrows}

\urlstyle{sf}
\makeatletter
\let\UrlSpecialsOld\UrlSpecials
\def\UrlSpecials{\UrlSpecialsOld\do\/{\Url@slash}\do\_{\Url@underscore}}\def\Url@slash{\@ifnextchar/{\kern-.11em\mathchar47\kern-.2em}{\kern-.0em\mathchar47\kern-.08em\penalty\UrlBigBreakPenalty}}
\def\Url@underscore{\nfss@text{\leavevmode \kern.06em\vbox{\hrule\@width.3em}}}
\makeatother

\theorembodyfont{}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{nonumberplain}
\theoremheaderfont{\scshape}
\theoremsymbol{\ensuremath{\blacksquare}}
\theoremseparator{.}
\newtheorem{proof}{Proof}

\usepackage{listings}
\newcommand{\li}{\lstinline[style=Haskell]}
\lstnewenvironment{haskell}{\lstset{style=Haskell}}{}
\lstdefinestyle{Haskell}{language=Haskell
        ,columns=flexible
	,basewidth={.365em}
	,keepspaces=True
        ,texcl=true
,basicstyle=\sffamily
        ,stringstyle=\itshape
        ,showstringspaces=false
        ,literate={->}{}2
                  {<-}{}2
                  {=>}{}2
                  {→}{}2
{>>}{{>>}\hspace{-1pt}}2
{[]}{[\,]\ }1
{++}{{\ }}1
{\ .\ }{{}}2
	,keywords={type,data,where,let,in,case,of}
        }

\newcommand{\ci}{\lstinline[style=Cmm]}
\lstnewenvironment{cmm}{\lstset{style=Cmm}}{}
\lstdefinestyle{Cmm}{language=C
        ,columns=fullflexible
        ,texcl=true
,commentstyle=\sffamily\itshape
        ,basicstyle=\small\ttfamily
}



\conferenceinfo{Haskell Implementors Workshop}{Sep 14 2012, Copenhagen} 
\copyrightyear{2012} 
\copyrightdata{Joachim Breitner}

\titlebanner{---preprint---}
\preprintfooter{Joachim Breitner: dup -- Explicit un-sharing in Haskell}
\title{dup -- Explicit un-sharing in Haskell}


\authorinfo{Joachim Breitner}{Karlsruhe Institute of Technology}{breitner@kit.edu}


\begin{document}
\maketitle


\begin{abstract}
We propose two operations to prevent sharing in Haskell that do not require modifying the data generating code, demonstrate their use and usefulness, and compare them to other approaches to preventing sharing. Our claims are supported by a formal semantics and a prototype implementation.
\end{abstract}



\category{D.1.1}{Programming Techniques}{Applicative (Functional) Programming}
\category{D.3.3}{Programming Languages}{Language Constructs and Features}[Data types and structures]
\category{E.2}{Data storage representation}{}

\keywords space leak, lazy evaluation, sharing, functional programming, natural semantics


\section{Introduction}

Thanks to the immutable nature of data in a pure functional programming language such as Haskell, there are many possibilities for sharing, i.e.\ one object in memory can used in multiple places in the program. In general, this is a good thing, as it can save both execution time (by not calculating the data again) and memory space (by not copying the data).

But there are cases where sharing can hurt, and sometimes hurt badly. A famous example (\citep{wikibook,lazy-evaluators}) is the following function:
\begin{haskell}
let l = [1..100000000]
    f :: [Int] -> Int
    f xs = last xs + head xs
in  f l
\end{haskell}
This program is space-leaky and will quickly run out of memory. If we substitute the term for \li-xs- in the body of \li-f- and evaluate that expression, it runs quickly and in constant memory. We have avoided the sharing of \li-xs- between the calls to \li-last- and \li-head- and the list elements can be garbage collected as soon as they have been consumed by \li-last-. This came at the expense of evaluating the list twice, which is fine, as the list is \emph{large} but \emph{cheap to calculate}.

But this source transformation, as well as other source transformations to avoid sharing (see Section \ref{sec:unit} and \ref{sec:church}), is not always possible or desirable, e.g.\  when the parameter passed to \li-f- comes from library code not under the control of the programmer. Therefore, we propose a new primitive operation \li-dup- which copies a (possibly unevaluated) value on the heap.
\begin{haskell}
data Box a = Box a
dup :: a -> Box a
\end{haskell}
Its value semantics are that of \li!(\x -> Box x)!; the wrapping in \li-Box- just serves the purpose of controlling the exact point of execution of \li-dup- by case-analyzing the \li-Box-. Using \li-dup- allows us to modify in the above example only the code of \li-f- to prevent sharing and achieve constant memory usage:
\begin{haskell}
let f xs = case dup xs of
    	Box xs' -> last xs' + head xs
    l = [1..100000000]
in  f l
\end{haskell}
In Section \ref{sec:unsharing}, we demonstrate the use of \li-dup- and other approaches on the more elaborate example introduced in Section \ref{sec:example}, taking on the programmer’s point of view.

An sharp-witted reader with knowledge of a typical implementation of a Haskell runtime might already have noticed that just copying the object on the heap representing the parameter \li-xs- might not be enough: If, for example, the first cons-cell of \li-xs- is already evaluated, then \li-dup xs- will copy that cell, but the thunk representing the tail of the list will still be shared between \li-xs'- and \li-xs-, and \li-f- will again devour memory. Such things may occur without the programmer’s knowledge, e.g.\ during a compiler optimization pass.

To that end, we propose a variant of \li-dup-, called \li-deepDup-, which effectively copies the complete heap referenced by its argument. This happens -- as one would expect for anything related to Haskell -- lazily: The objects referenced  by the parameter are copied if and when they are needed. In other words: After having evaluated a function which only works on \li-deepDup-’ed copies of its parameters, nothing this evaluation has created on the heap is referenced anymore, unless it is referenced by the function's return value (this is formalized in Theorem \ref{thm:deepdup}).

Our specific contributions are:
\begin{itemize}
\item We introduce primitives that give the programmer the possibility to explicitly prevent sharing.
\item In contrast to approaches based on source transformations, using \li-dup- and \li-deepDup- does \emph{not} require changes to the generating code.
\item We provide precise semantics in the context of Launchbury’s natural semantics for Lazy Evaluation (Section \ref{sec:semantics}) and prove that the recursive variant \li-deepDup- is effective.
\item We show the feasibility of our approach using a proof-of-concept implementation targeting code compiled by an unmodified GHC. (Section \ref{sec:prototype})
\end{itemize}

\section{The running example}
\label{sec:example}

For the remainder of the paper, we will use one running example to demonstrate and discuss the use of \li-dup-. The task at hand, inspired by the minimax algorithm that searches for an optimal strategy in a two-player turn-based game, is to find a path through a (possibly infinite) tree that maximizes some valuation of the nodes. So abstractly, we have a type \li-S- of states, a valuation function \li-value-, an initial state \li-init- and for every state \li-s-, a list of successor states \li-succs s-. For the sake of simplicity of the presentation, we assume this \li-succs s- to be always non-empty (see also Figure~\ref{fig:ex}).

\begin{figure}
\begin{haskell}
-- The problem specification
type S = ...
init :: S
succs :: S -> [S]
value :: S -> Integer

-- The search tree code
data Tree = Node S [Tree]
fstChild :: Tree -> Tree
fstChild (Tree _ (x:xs)) = x

tree :: S -> Tree
tree s = Node s (map tree (succs s))

solve :: Tree -> [S]
solve (Node n ts) = n : solve picked
  where
  rated = [ (t, rate depth t) | t <- ts ]
  picked = fst (maximumBy (comparing snd) rated)
  depth = ...

rate :: Int -> Tree -> Integer
rate 0 (Node s _) = value s
rate d (Node _ ts) = maximum (map (rate (d-1)) ts)

main = do
  let t = tree init
  print bb>0d10\,000\cdot (b-1)\cdot b^{d-1}d=1b=2d=4b=4b=4d=4=b=2d=1b=2d=1b=4d=4 solveDup (fstChild t) !! 1000
\end{haskell}
then \li-dup- will only copy this unevaluated expression, but both copies will reference the same unevaluated expression for~\li-t-, and we are back at the original performance (\stats{stats:SolveDup:SharedThunk:mem}~MB, \stats{stats:SolveDup:SharedThunk:time} seconds).

The same effect occurs if the tree is already partly evaluated. This may even be caused by a compiler transformation, e.g.\ the wrapper/worker transformation, assuming that \li-doSomethingElseWith- is strict  in its argument \citep{unboxed}. Then, the parameter \li-t- is the \li-Node- constructor referencing other nodes or unevaluated trees, and copying the constructor does not help to prevent sharing the referenced data, as shown in the first row of Figure \ref{fig:deepevaluation}.

This is where \li-deepDup- comes in:
Intuitively, \li-deepDup- takes a complete and private copy of the entire heap reachable from its argument, hence preventing any unwanted evaluation outside this copy. In fact this is done lazily: It will just copy the object specified by its parameter, and change all references therein so that before they are evaluated, \li-deepDup- copies them.

So by wrapping \li-solve- in a call to \li-deepDup-:
\begin{haskell}
solveDeepDup t = case deepDup t of Box t' -> solve t'
\end{haskell}
we achieve the performance of a successful run with \li-dup- (\stats{stats:SolveDeepDup:SharedThunk:mem}~MB and \stats{stats:SolveDeepDup:SharedThunk:time} seconds), but also in the cases where \li-t- has already been partly evaluated or is wrapped in another unevaluated expression. The second row of Figure \ref{fig:deepevaluation} shows \li-deepDup- at work.

Using \li-deepDup- is therefore more reliable and easier to handle: The programmer need not have an exact idea of the evaluation state of the arguments when \li-deepDup- is called. And the recursive copying is surprisingly cheap: Even when the tree is already fully evaluated, e.g. by an earlier call to \li-solve t !! 10000-, the runtime stays the same within the precision of the benchmark.





\subsection{The unit type argument pattern}

The problem at hand is, of course, not new, and Haskell programmers have solved it one way or the other before, by rewriting the code to allow more control over sharing.

\label{sec:unit}

A common approach is to replace values that you do not want to be shared by functions, e.g.\ by turning a bound expression \li-let x = e- into a lambda expression \li!let x = {2}
x,y &\in \sVar
\displaybreak[1]
\\
e &\in
\sExp &&\Coloneqq
\begin{aligned}[t]&
\sLam x e
\mid \sApp e x
\mid x \mid
\\&
\sLet {x_1 = e_1,\ldots,x_n = e_n} e \mid
\\&
\sDup x \mid \sDeepDup x
\end{aligned}
\displaybreak[1]\\
\Gamma, \Delta, \Theta &\in \sHeap &&= \sVar \pfun \sExp
\displaybreak[1]\\
z &\in \sVal &&\Coloneqq \sLam x e

\rho \in \sEnv = \sVar \to \sValue

\esem{ x_1\mapsto e_1,\ldots,x_n\mapsto e_n}\rho \\
= \mu \rho'. \rho \sqcup (x_1 \mapsto \dsem{e_1}{\rho'}) \sqcup \cdots \sqcup (x_n \mapsto \dsem{e_n}{\rho'})

\dsem{\sDup x}\rho &\coloneqq \dsem{x}\rho \\
\dsem{\sDeepDup x}\rho &\coloneqq \dsem{x}\rho.

\dsem e {\esem \Gamma \rho} = \dsem z {\esem \Delta \rho}
\text{ and }
\esem \Gamma \rho \le \esem \Delta \rho.

&\phantom{{}={}}\dsem{\sDup x}{\esem{\Gamma, x\mapsto e}\rho}\\
&= \dsem{x}{\esem{\Gamma, x\mapsto e}\rho}\\
&= \dsem{e}{\esem{\Gamma, x\mapsto e}\rho}\\
&= \dsem{\hat e}{\esem{\Gamma, x\mapsto e}\rho}\\
&= \dsem{\hat e}{\esem{\Gamma, x\mapsto e, \fresh x \mapsto \hat e}\rho} && \text{ fresh}\\
&= \dsem{\fresh x}{\esem{\Gamma, x\mapsto e, \fresh x \mapsto \hat e}\rho} \\
&= \dsem{z}{\esem \Delta \rho} &&\text{by (i)}

\esem{\Gamma, x\mapsto e}\rho \le \esem{\Gamma, x\mapsto e, \fresh x\mapsto \hat e} \rho \le \esem{\Delta}\rho

\dsem{\fresh y_i}{\esem{\Gamma'}\rho}
= \dsem{\sDeepDup {y_i}}{\esem{\Gamma'}\rho}
= \dsem{y_i}{\esem{\Gamma'}\rho}
= \dsem{y_i}{\esem{\Gamma, x\mapsto e}\rho}.

&\phantom{{}={}}\dsem{\sDeepDup x}{\esem{\Gamma, x\mapsto e}\rho}\\
&= \dsem{x}{\esem{\Gamma, x\mapsto e}\rho}\\
&= \dsem{e}{\esem{\Gamma, x\mapsto e}\rho}\\
&= \dsem{\hat e[\fresh y_1/y_1,\ldots,\fresh y_n/y_n]}{\esem{\Gamma'}\rho} &&\text{by (iii)}\\
&= \dsem{\fresh x}{\esem{\Gamma'}\rho}\\
&= \dsem{z}{\esem \Delta \rho} &&\text{by (i)}

\esem{\Gamma, x\mapsto e}\rho \le \esem{\Gamma'} \rho \le \esem{\Delta}\rho.

e=\sLet{x_1' = \sDeepDup x_1,\ldots,x_n'= \sDeepDup x_n} e'

\ur{\Gamma}{e} = \ufv e \cup \textstyle\bigcup_{x\in \ufv e}\ur{\Gamma}{\Gamma\ x}.

\ur{\Gamma'}e
&= \ufv e \cup \textstyle\bigcup_{x\in \ufv e}\ur{\Gamma'}{\Gamma'\ x} \\
&\subseteq 
\begin{aligned}[t]
\ufv {e_l} &\cup \{x_1,\ldots,x_n\}\\
&\cup \textstyle\bigcup_{x\in \ufv {e_l}}\ur{\Gamma'}{\Gamma'\ x} \\
&\cup \ur{\Gamma'}{\Gamma'\ x_1} \cup \cdots \cup \ur{\Gamma'}{\Gamma'\ x_n}
\end{aligned}\\
&=
\begin{aligned}[t]
\ufv {e_l} &\cup \{x_1,\ldots,x_n\}\\
&\cup \textstyle\bigcup_{x\in \ufv {e_l}}\ur{\Gamma'}{\Gamma'\ x}\\
&\cup \ur{\Gamma'}{e_1} \cup \cdots \cup \ur{\Gamma'}{e_n}
\end{aligned}\\
&= 
\begin{aligned}[t]
\ufv {e_l} &\cup \{x_1,\ldots,x_n\} \\
&\cup \textstyle\bigcup_{x\in \ufv {e_l}}\ur{\Gamma'}{\Gamma'\ x}
\end{aligned}\\
&= \ur{\Gamma'} {e_l} \cup \{x_1,\ldots,x_n\}\\
&= \ur{\Gamma} {e_l} \cup \{x_1,\ldots,x_n\}.

\ur{\Gamma,x\mapsto e, \fresh x\mapsto \hat e}{\fresh x}
&= \ur{\Gamma, x\mapsto e, \fresh x\mapsto \hat e}{\hat e} \cup \{\fresh x\}\\
&= \ur{\Gamma, x\mapsto e}{e} \cup \{\fresh x\}\\
&\subseteq \ur{\Gamma, x\mapsto e}{\sDup x} \cup \{\fresh x\}.

\ur{\Gamma'}{\fresh x}
&= \{\fresh x\} \cup \ur{\Gamma'}{\hat e[\fresh y_1/y_1,\ldots,\fresh y_n/y_n]} \\
&= \{\fresh x\}
\begin{aligned}[t]
&\cup \ufv{\hat e[\fresh y_1/y_1,\ldots,\fresh y_n/y_n]}\\
&\cup \textstyle\bigcup_{z \in \ufv{\hat e[\fresh y_1/y_1,\ldots,\fresh y_n/y_n]}} \ur{\Gamma'}{\Gamma' \ z}
\end{aligned}\\
&= \{\fresh x\}
\begin{aligned}[t]
&\cup \{\fresh y_1,\ldots, \fresh y_n\}\\
&\cup \textstyle\bigcup_{i=1,\ldots,n} \ur{\Gamma'}{\Gamma'\ \fresh y_i}
\end{aligned}\\
&= \{\fresh x\} 
\begin{aligned}[t]
&\cup \{\fresh y_1,\ldots, \fresh y_n\}\\
&\cup \textstyle\bigcup_{i=1,\ldots,n} \ur{\Gamma'}{\sDeepDup {y_i}}
\end{aligned}\\
&= \{\fresh x\} \cup \{\fresh y_1,\ldots, \fresh y_n\}

}
and, as these are all fresh variables, . Clearly, , so the  first statement follows from the induction hypothesis.

Statement (c) follows immediately as the additional variables are fresh.
\end{proof}

Having cast our intuition of \li-dup- and \li-deepDup- into a precise form using a formal semantics, we now explain how we have implemented this semantics, or rather a pragmatic approximation, in a real environment.

\section{The prototype implementation}
\label{sec:prototype}

Our implementation\footnote{Available at \url{http://darcs.nomeata.de/ghc-dup}} works with the Glasgow Haskell Compiler (GHC), version 7.4.1, and requires no modifications to the compiler or its runtime: The code is compiled to a usual object file, linked into the resulting binary and called via the foreign function interface.

GHC compiles Haskell code first to a polymorphic, explicitly typed lambda-calculus called \emph{Core} \citep{core,system-fc}, then to the \emph{Spineless Tagless G-machine} (STG) \citep{stg}. From there, it generates \emph{Cmm} code, an implementation of the portable assembly language C-{}- which is then compiled to machine code, either directly or via LLVM.

Our work looks at objects in the sense of the STG, so we only need to worry about data representation on the heap \citep{stg}. Design decisions regarding the earlier transformations, such as the evaluation model \cite{evalapply}, are thus not important here.

\def\ux{2.2cm}\def\uy{0.6cm}
\begin{figure}
\begin{center}
\begin{tikzpicture}[x=\ux, y=\uy,word/.style={shape=rectangle, draw, minimum width=\ux, minimum height=\uy},>=latex]
\draw (0,0) rectangle +(1,1) node[midway] (ip) {Info pointer};
\draw (1,0) rectangle +(1,1) node[midway] {Payload};
  (0,0) node[word] (ip) {Info pointer}
++(0,-1) node[word, minimum width=2*\ux] {Payload};

\begin{scope}[yshift=-0.7cm, xshift=2.5cm]
\draw
  (0,0) node[word] (tbl) {Code pointer}
++(0,-1) node[word] {Layout info}
++(0,-1) node[word] {Other fields};
\end{scope}
\draw[*->] (ip.south) |- (tbl.west);
\draw[*->] (tbl.east) -- ++(.5cm,0) node[right] {Entry code};
\end{tikzpicture}
\end{center}
\caption{The common layout of heap objects}
\label{fig:heap}
\end{figure}

The common layout of all objects, or closures,  on the heap is a pointer to a statically allocated \emph{info table}, followed by the payload (Figure \ref{fig:heap}). The info table indicates the type of the object (not to be confused with the type from the type system – these are completely irrelevant at this stage), contains layout information about the payload required by the garbage collector, namely what words are pointers to other objects and what words are not, and the code to be run when the object is evaluated.

There are various types of objects on the heap, most important are:
\begin{itemize}
\item \emph{Data constructors}, representing fully evaluated values. The payload are pointers to the parameters of the constructor.
\item \emph{Function closures}, representing functions. Locally defined functions capture their free variables, these are stored in the payload.
\item \emph{Thunks}, which are unevaluated expressions. Again, the payload contains references to their free variables.
\item \emph{Applications} of a function to a number of arguments. This closure type is usually only used by the GHC interpreter, but we use it in the implementation of \li-deepDup-.
\item \emph{Indirections}, which point to another object on the heap in their payload. These are created during evaluation and removed by the garbage collector.
\end{itemize}


When a thunk is evaluated, it is replaced by an indirection which points to the result of the evaluation, which can be a data constructor or a function closure. This way, when another reference to the thunk is evaluated, the computation is not repeated but the calculated result is used directly, hence the result is \emph{shared}. The indirections do not stay around forever: The next garbage collector run, which copies all live data, will replace references to indirections by whatever the indirection points to.

As we want to avoid this sharing, we need to prevent the original reference to be replaced by the indirection. We cannot change the code of the thunk, but we can copy the thunk, thus creating a new copy that is not referenced by other code, and then evaluate that.
The essence of the surprisingly simple code is listed in Figure \ref{fig:dupcode}; the closure to duplicate is passed in the register \ci-R1- and \ci-Hp- is the heap pointer which is increased by \ci-ALLOC_PRIM-.

\begin{figure}
\begin{cmm}
dupClosure {
    clos = UNTAG(R1);
    // Allocate space for the new closure
    (len) = foreign "C" closure_sizeW(clos "ptr") [];
    ALLOC_PRIM(WDS(len), R1_PTR, dupClosure);
    copy = Hp - WDS(len) + WDS(1);
    p = 0;
    for: // Copy the info pointer and payload
    if(p < len) {
        W_[copy + WDS(p)] = W_[clos + WDS(p)];
        p = p + 1;
        goto for;
    }
    RET_P(copy);
}
\end{cmm}
\caption{The Cmm code for \li-dup-}
\label{fig:dupcode}
\end{figure}

As discussed in Section \ref{sec:deepdup}, this simple approach is not always sufficient, and we want a recursive variant, \li-deepDup-. This function, shown in Figure \ref{fig:deepdupcode}, needs to access the info table of the closure to figure out what part of the payload is a pointer to another heap object. For every referenced object, an application thunk is created which applies \li-deepDup- (or rather the variant \li!deepDupFun! with the better suited type \li!a -> a!), unless we are about to \li-deepDup- a \li-deepDup- thunk. In that case, we just copy it, but leave the argument alone, reflecting the use of  instead of  in the Rule \sRule{Deep} in the formal semantics. The code listing does not include a few shortcuts, e.g.\ data constructors without pointer arguments such as integer values are not copied.

\begin{figure}
\begin{cmm}
deepDupClosure {
    clos = UNTAG(R1);
    // Allocate space for the new closure
    (len) = foreign "C" closure_sizeW(clos "ptr") [];
    ptrs  = TO_W_(bytes = WDS(len) + ptrs * SIZEOF_StgAP + WDS(ptrs);
    ALLOC_PRIM(bytes, R1_PTR, dupClosure);
    copy = Hp - WDS(len) + WDS(1);
    p = 0;
    for1: // Copy the info pointer and payload
    if(p < len) {
        W_[copy + WDS(p)] = W_[clos + WDS(p)];
        p = p + 1;
	goto for1;
    }
    // Do not wrap \textup{deepDup} thunks again
    if (W_[copy] == stg_ap_2_upd_info &&
        W_[copy + WDS(1)] == Dup_deepDupFun_closure) {
       goto done;
    }
    if 
    p = 0;
    for2: // Wrap all referenced closures in \textup{deepDup} thunks
    if(p < ptrs) {
	ap = Hp - bytes + WDS(1)
	     + p * SIZEOF_StgAP + WDS(p);
        W_[ap] = stg_ap_2_upd_info;
        W_[ap + WDS(1)] = Dup_deepDupFun_closure;
	W_[ap + WDS(2)] = W_[clos + WDS(p)];
	W_[copy + WDS(p)] = ap;
	p = p + 1;
	goto for2;
    }
    done:
    RET_P(copy);
}
\end{cmm}
\caption{The Cmm code for \li-deepDup-}
\label{fig:deepdupcode}
\end{figure}


\subsection{Limitations of the implementation}
\label{sec:shortcomings}

Our implementation is but a prototype; it does not yet work in all situations. One large problem is posed by statically allocated thunks: A value, say \li-nats = [0..]-, defined at the module level is compiled to a thunk with closure type \ci-THUNK_STATIC-, also called a constant applicative form (CAF), and receives special treatment by the garbage collector. Copying such a closure to the heap using the code above would make the garbage collector abort, as it does not expect a static thunk to be found on the heap. But it is not possible to change the type of the closure, as the info table containing the type lies directly next to the code. And in order to create a modified info table somewhere else, the code needs to be copied as well. Therefore, \li-dup- and \li-deepDup- currently does not work for static thunks. When it is passed such a thunk, it prints a warning and returns the original reference, retaining sharing.

It should be possible for \li-dup- to support static thunks with some additional information in the compiled code.  Currently, when execution enters a static thunk and the stack and heap checks have been passed, the thunk is replaced by an indirection into the heap and an update frame is pushed on the stack. If there was a way to jump over the code that sets up the indirection and update frame, e.g.\ via an alternative entry point included in the info table, \li-dup- could create a thunk on the heap that calls the static thunk via this route, effectively kicking off evaluation without affecting the original static thunk.  For \li-deepDup- things are more complicated, as references to static objects are not part of the heap object, but are scattered throughout the machine code. Moving these references to the heap would solve the issue here at hand, but is clearly too expensive.

Also, the prototype does not take multithreaded programs into account and will likely produce bad results when used in such an environment, e.g.\ when another thread replaces a thunk by an indirection during the thunk copy loop in \ci-dupClosure-. Similarly, there are several specialized closure type (arrays, mutable references, weak pointers\citep{weakpointers} and others \citep[page HeapObjects]{commentary}). For each of them, we would need to determine whether they can be safely duplicated and if so, whether this is actually useful.

In the presence of Lazy IO, duplicating thunks can be outright dangerous: Not only can the original and the duplicated thunk evaluate to different values but this can make the program crash, e.g.\ when one copy is done evaluating and causes a file to be closed, while the second copy continues to read from it. Generally everything implemented with \li-unsafePerformIO- is prone to behave badly when combined with \li-dup- or \li-deepDup-.

Function closures need special treatment as there are cases where code assumes a certain reference to always be a function closure and never a thunk that will evaluate to a function. But this is what \li-deepDup- wants to create. Currently, \li-deepDup- will in this case leave the reference as it is. A solution would be to copy the function closure eagerly, so that the reference in the copy again points to a function closure. This would require more sophisticated code to detect cycles. 

\section{Conclusions and further work}

While Haskell gives the programmer great devices to get their programs to do the right thing, such as referential transparency and the type system, she has less means to analyze and control their runtime behavior. Several commercial users have mentioned this as one of the main drawbacks of Haskell \citep{sampson,wehr,hesselink}. This problem deserves more attention and we hope that this work is one step towards a Haskell with better controllable and understandable time and space behavior.

We have shown the feasibility of an explicit sharing-preventing operator in a lazy functional language. We provided two variants, \li-dup- and \li-deepDup-, the former is simpler, but possibly more subtle to put to use effectively, the latter works more predictably, but may impose a larger performance penalty. This is, on a prototypical level, possible with an unmodified Haskell compiler.

As described in Section \ref{sec:shortcomings}, there is work to be done on the implementation before it can be used in production code. Some of that might require changes to the compiler code. Given how sensitive the code is to changes in the runtime representation of Haskell values, a productive version of \li-dup- would probably have to be shipped along with the compiler.




\acks

I would like to thank Andreas Lochbihler for fruitful discussions and proof-reading and the anonymous referees for being supportive of the idea and constructive about the presentation. This work was supported by the Deutsche Telekom Stiftung.

\bibliographystyle{abbrvnat}
\bibliography{bib}

\end{document}
