









\documentclass[10pt,journal,cspaper,compsoc]{IEEEtran}
















\ifCLASSOPTIONcompsoc
\usepackage[nocompress]{cite}
\else
\usepackage{cite}
\fi






\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\else
\usepackage[dvips]{graphicx}
\fi






\usepackage[cmex10]{amsmath}
\usepackage{amssymb}






\usepackage{algorithmic}





\usepackage{array}



\usepackage{mdwmath}
\usepackage{mdwtab}






\usepackage{eqparbox}








\usepackage{graphicx,wrapfig,epsfig}
\usepackage{subcaption}


\usepackage{fixltx2e}




\usepackage{stfloats}





\ifCLASSOPTIONcaptionsoff
  \usepackage[nomarkers]{endfloat}
 \let\MYoriglatexcaption\caption
 \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
\fi





\usepackage{url}






\usepackage{slashbox}
\usepackage{algorithm2e}
\usepackage{framed}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[export]{adjustbox}

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\newcommand{\subjectto}{\mathop{\textrm{subject to}}}
\newcommand{\argmax}{\mathop{\textrm{argmax}}}
\newcommand{\argmin}{\mathop{\textrm{argmin}}}
\newcommand{\shortestpath}{\mathop{\textrm{shortest path}}}
\newtheorem{myDefinition}{Definition}
\newtheorem{myTheorem}{Theorem}

\title{Nonlinear Dimensionality Reduction via Path-Based Isometric Mapping}


\author{Amir~Najafi,
        Amir~Joudaki,
        and Emad~Fatemizadeh\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Authors are affiliated with the Biomedical Signal and Image Processing Laboratory (BiSIPL), Department
of Electrical Engineering, Sharif University of Technology, Tehran,
Iran.\protect\\
E-mails: najafi@ee.sharif.edu, amir.judaki@gmail.com, fatemizadeh@sharif.edu}
\thanks{}}





\markboth{IEEE Transactions on Pattern Analysis and Machine Intelligence}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}









\IEEEcompsoctitleabstractindextext{\begin{abstract}
Nonlinear dimensionality reduction methods have demonstrated top-notch performance in many pattern recognition and image classification tasks. Despite their popularity, they suffer from highly expensive time and memory requirements, which render them inapplicable to large-scale datasets. To leverage such cases we propose a new method called ``Path-Based Isomap". Similar to Isomap, we exploit geodesic paths to find the low-dimensional embedding. However, instead of preserving pairwise geodesic distances, the low-dimensional embedding is computed via a path-mapping algorithm. Due to the much fewer number of paths compared to number of data points, a significant improvement in time and memory complexity without any decline in performance is achieved. The method demonstrates state-of-the-art performance on well-known synthetic and real-world datasets, as well as in the presence of noise.
\end{abstract}


\begin{keywords}
Nonlinear dimensionality reduction, manifold learning, geodesic path, optimization criteria.
\end{keywords}}


\maketitle


\IEEEdisplaynotcompsoctitleabstractindextext



\IEEEpeerreviewmaketitle



\section{Introduction}




\IEEEPARstart{O}{ne} of the fundamental problems in machine learning and pattern recognition is to discover compact representations of high-dimensional data. The need to analyze and visualize multivariate data has yielded a surge of interest in dimensionality reduction research \cite{demartines1997curvilinear}, \cite{hinton2002stochastic}, \cite{donoho2003hessian}, \cite{lespinats2009rankvisu}. In particular, manifold learning techniques such as Isomap \cite{tenenbaum2000global}, Locally Linear Embedding (LLE) \cite{roweis2000nonlinear}, and Laplacian Eigenmaps \cite{belkin2003laplacian} have outperformed classical methods like Principal Component Analysis (PCA) and Multi-Dimensional Scaling (MDS) \cite{borg2005modern} in harnessing non-linear data structures \cite{timofte2012iterative}, \cite{guo2011simultaneous}. However, their high time and memory complexity impose severe limitations on their scalabality \cite{kohonen2001self}. To overcome this drawback, we set out to develop a method with lower computational costs yet the same performance. 

Throughout the paper it is assumed that data samples lie on a smooth low-dimensional manifold \cite{baraniuk2009random}, \cite{hegde2012learning}. In the first stage, these data samples are covered by a set of geodesic paths, resulting in a network of intersecting routes. The main point is that data samples that belong to a geodesic path approximately lie on a straight line in the compact representation \cite{bernstein2000graph}. Thus a mapping scheme is developed to compute the lines in the destination space. The scheme is formulated as an optimization problem that attempts to preserve topology of the network of paths instead of pairwise geodesic distances. This is a crucial difference between our approach and Isomap that yields remarkable cost savings.â€¬

Experiments on commonly used synthetic and real-world datasets substantiates superiority of our method in terms of efficiency. They also demonstrate that this achievement do not come at the cost of performance, stability, or robustness of the algorithm. Another advantage of this algorithm is that the aforementioned optimization problem has an analytical solution, that avoids local minima and has deterministic time bounds.

Rest of the paper is organized as follows: Section 2 overviews the Isomap algorithm. Section 3 highlights the main idea of the paper. Section 4 gives a stochastic algorithm for covering the data samples with a set of geodesic paths. Section 5 details the path-mapping scheme. Section 6 is dedicated to complexity analysis of the algorithm. Section 7 discusses experimental results, and finally the conclusions are made in section 8.




\section{Isometric Mapping (Isomap)}
This section briefly explains the Isomap algorithm. Assume a cloud of high dimensional data points  lie on a smooth -dimensional manifold. In most cases of practical interest  is  much smaller than the data dimension  (). Isomap builds upon MDS but attempts to compute the low-dimensional representation by estimating pairwise geodesic distances.

For sufficiently close pairs, referred to as neighboring points, the euclidean distance provides a good approximation of geodesic distance \cite{bernstein2000graph}, \cite{balasubramanian2002isomap}. For faraway points, one needs to walk through these neighboring pairs in the shortest way possible to evaluate the geodesic distance. That can be achieved efficiently by applying a shortest path algorithm on a graph comprising edges that connect neighboring points.

Here we introduce notations for these concepts. The graph is represented as  in which  denotes the set of nodes, and  is the set of edges connecting neighboring samples. Two ways determining the neighbors of a point are K-nearest neighbors \cite{seidl1998optimal}, or all points within a fixed range . In this paper we utilize the former method. For neighboring nodes  and  the weight is taken to be . If we take  to be the shortest route between  and , we could compute geodesic distances as  in which  denotes weight of the path.

Finally, we seek a set of low-dimensional points denoted by  in  that preserves pairwise geodesic distances. This can be accomplished via a classical MDS approach.



\section{A Path-Based Approach}
Isomap discards the fact that a shortest path  will be approximately mapped to a straight line in the representational space. In this regard, if we enforce each  to lie exactly on a straight line, degrees of freedom will be reduced dramatically. Here is the explanation behind this fact: assume  to lie on the shortest path between  and  denoted as . It can be concluded that  equals to . Since MDS tries to preserves these geodesic distances in the representational space, it attempts to satisfy the equation . This, in turn, implies that the three points must lie on a straight line. Not to mention this is the ideal case, without any noise and in the limit of infinite samples. Regarding the fact that points were chosen arbitrarily, it can be concluded that all points on a shortest path must lie on a straight line. 

Thus, assuming  to be the number of points in , the number of degrees of freedom drops from , describing  points in , to , describing the starting point and direction of a line in . Suppose starting point  is mapped to  and the direction of the straight line, , is discovered. Any other point on the path  can be mapped automatically: 

which incorporates the fact that . This approach is sketched out in Fig.\ref{fig_1_1}. As can be seen two geodesic paths on a manifold in  are approximately mapped to straight lines in .

\begin{figure}[t]
\centering
        \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[trim=0.7in 0.4in 0.6in 0.3in,clip,width=\textwidth]{1_1.eps}
\end{subfigure}~
\begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[trim=0.6in 0.2in 0.4in 0.3in,clip,width=\textwidth]{1_2.eps}
\end{subfigure}\caption{{\small Graphical illustration of the scheme that maps geodesic paths on the manifold to straight lines. Direction and starting point of each line are tuned in a way that global geometry of data points is preserved.}}\label{fig_1_1}
\end{figure}

There still remains two issues to be addressed. First is to find a set of shortest paths  that will cover all nodes of the graph, and second is to develop a scheme that maps shortest paths in  to straight lines in . These issues are discussed respectively in sections 4 and 5. 

\section{Stochastic Shortest Path Covering}

\begin{algorithm}[b] 
 \TitleOfAlgo{Stochastic Shortest Path Covering (SSPC)}
 \begin{framed}
 \SetAlgoLined 
 \KwData{\\
 		 \hspace{9.5mm}}  
 \While{}
 {\vspace{1mm}
Choose a random node in , denoted as .\vspace{1mm}\\
Compute all the shortest paths that start from  and end to other members of .\vspace{1mm}\\
Find the path that overlaps the most with , denoted as .\vspace{1mm}\\
\vspace{1mm}\\
\vspace{1mm}
}  
  \KwResult{ = A sufficient set of shortest paths.  }
 \end{framed}
 \vspace{-3mm}
\end{algorithm}


This section presents an stochastic algorithm for covering the graph with a set of shortest paths called "Stochastic Shortest Path Covering (SSPC)".

The general problem of graph covering via shortest paths is known to be NP-hard \cite{boothe2007graph}. Thus we set out to find a sub-optimal solution with a stochastic approach. In practice our sub-optimal algorithm yields substantial time and space savings.
 
The general idea is to iteratively cover as many nodes as possible. First we initialize the set of uncovered points, denoted by , to include all vertices of . In each step, a source node  is selected randomly. Among shortest paths starting from , the path , which overlaps the most with  is chosen. The nodes of  are deducted from . We repeat this procedure until there is no point uncovered, i.e. . This results in a stable network of intersecting paths. A pseudo-code of the method is presented. Fig.\ref{fig_4_1} shows an example of applying the SSPC on Swiss-Roll dataset.

\begin{figure*}[t]
        \begin{subfigure}[b]{0.29\textwidth}
                \includegraphics[trim=1.75in 1.3in 1.75in 1.35in,clip,width=\textwidth]{SSPC_1.eps}
                \caption{}
        \end{subfigure}~
        \begin{subfigure}[b]{0.05\textwidth}
		        \includegraphics[trim=-4.1in 0.05in 0.3in 0.5in,clip,angle=90,width=\textwidth]{arrow.eps}	
        \end{subfigure}~
        \begin{subfigure}[b]{0.29\textwidth}
                \includegraphics[trim=1.75in 1.3in 1.75in 1.35in,clip,width=\textwidth]{SSPC_2.eps}
                \caption{}
        \end{subfigure}~
        \begin{subfigure}[b]{0.05\textwidth}
		        \includegraphics[trim=-4.1in 0.05in 0.3in 0.5in,clip,angle=90,width=\textwidth]{arrow.eps}
        \end{subfigure}~
        \begin{subfigure}[b]{0.29\textwidth}
                \includegraphics[trim=0.6in 0.2in 0.5in 0.3in,clip,width=\textwidth]{SSPC_3.eps}
                \caption{}
        \end{subfigure}
        \vspace{1mm}
        \caption{{\small SSPC sample run on a Swiss-Roll dataset consisting of  data points. The number of paths obtained by the method in this example is . (a) The high-dimensional data in . (b) Result of the SSPC in . (c) The results shown for the unfolded manifold.}}\label{fig_4_1}
\end{figure*}

As it is evident in Fig.\ref{fig_4_1} the number of paths collected by SSPC is significantly fewer than the number of samples. We have empirically investigated the extent of complexity reduction by SSPC algorithm on a number of datasets. It turns out that its contribution to complexity reduction largely depends on the inherent manifold dimensionality. Fig.\ref{fig_4_2} shows number of optimization variables (degrees of freedom) versus number of data points for three manifold dimensionalities:  and . For each  the number of optimization variables are averaged over various synthetic datasets. For the sake of comparison the non-reduced number of variables is shown with a slope of 1. The linear dependence in logarithmic scale implies that there is a power-law relation between the two variables:

where  and  are the number of data points and the number of paths respectively. 

The values that fit the data are given in TABLE 1. Experiments demonstrated that for a specific  variations in the resulting  and  were negligible over a variety of datasets. The numbers in the table suggest that for low-dimensional manifolds and large scale datasets complexity reduction is remarkable, whereas for high-dimensional small-scale ones, the improvement gradually fades out. Surprisingly the same exponent  is obtained for  and . However, it is possible that this observation is due to the curse of dimesniaolity and might not hold for much larger values of .

\begin{table}[b]
\caption{Experimental Parameters}
\begin{center}
\begin{tabular}{| c | c | c | c |}
\hline
\backslashbox[20mm]{Parameters}{} &  &  &  \\ 
\hline\hline 
 &  &  & \\ \hline
 &  &  & \\ \hline
\end{tabular}
\end{center}
\label{tableExp}
\end{table}

\begin{figure}[t]
\centering
    \includegraphics[trim=0.5in 0in 0in 0.3in,clip,width=0.45\textwidth]{Path_vs_Num.eps}
    \caption{{\small Number of optimization variables depicted as a function of number of data points in a log-log plot. Reduction in complexity is shown for  manifold dimensionalities. The original number of variables used in Isomap is also plotted for comparison.}}
    \label{fig_4_2}
\end{figure}

\begin{figure}[b]
\centering
    \includegraphics[trim=0.4in 0in 0.3in 0.28in,clip,width=0.45\textwidth]{Covering_Rate.eps}
    \caption{{\small Number of the uncovered data samples shown during execution of SSPC algorithm.}}
    \label{fig_5_2}
\end{figure}

\subsection{Covering Rate Analysis of SSPC}
It turns out that number of data samples covered by each iteration of SSPC gradually decreases during the execution. Moreover, the rate of decay follows an exponential trend. We have seen that the decay exponent is heavily dependent on the innate dimensionality of dataset. Fig.\ref{fig_5_2} shows the number of uncovered samples during the execution of the algorithm for a number of datasets. The linear decline in semi-log plot reveals an exponential decline rate:

where  is the number of uncovered samples after the th iteration\footnote{The only exception is that (\ref{eq9}) does not necessarily hold for small values of , such as when .}. It is also elucidated by the figure that as long as dimensinality is fixed, the rate of decay is neither sensitive to the embedding geometry nor the initial number of points. In other words, we could express  only in terms of  (manifold dimensionality):


In TABLE 2 the exponent of decay () has been estimated for three values of  and . In this regard, SSPC could shed some light on the manifold dimensioality. One could estimate  for an arbitrary dataset and compare it to these values to get a rough estimate of the underlying embedding dimensionality.

The only remaining issue here is the uniqueness of solution for a network of paths computed via SSPC. In other words we should investigate that under which circumstances the path sequences obtained from a graph covering algorithm will uniquely represent a low-dimensional embedding. This issue is addressed in Appendix A, where sufficient conditions are derived and a compensation strategy is proposed.














\section{Parameter Optimization}
What remains to explain is the path-mapping scheme that finds parameters of straight lines in  . One could attempt to estimate initial points and directions of these lines. We approach this problem from an optimization perspective. The cost function will arise from the inherent constraints of the problem. 

Fig.\ref{fig_4_1} clearly shows that the paths resulting from the SSPC algorithm would have numerous intersections. Two paths that cross each other, share a common data sample in a known position according to their starting points and direction vectors. Hence the estimations provided by each path should be close to each other. This lays out the main idea for defining an optimization criteria. This approach is depicted in Fig.\ref{fig_1_1} for a data sample shared between two paths.

\begin{table}[t]
\label{table2}
\caption{Experimental values for }
\begin{center}
\begin{tabular}{| c | c | c | c |}
\hline
 &  &  & \\ \hline
 &  &  & \\ \hline
\end{tabular}
\end{center}
\end{table}

In technical terms, assume that a particular data sample  is shared by  separate lines . Each line provides an estimation of the point denoted by  via line equation (\ref{eq1}). We have:

where  and  are the starting point and the direction vector of the th line respectively, and  is the geodesic distance between the starting sample and . Ideally, these parameters are tuned in a way that all the estimations become equal. In practice, we attempt to minimize a difference measure among them. A reasonable difference measure can be calculated as:

 indicates the average of s. It is clear that a good candidate for cost function will be the sum of difference measures of all shared data points. 

Here we present definitions and terms that will be used later in this section for formulation of cost function:

where  and  are the starting point and the direction of the th line respectively, and  denotes the total number of paths in . We also designate the th component of the vectors  and  by  and  respectively. The data samples that are shared among more than one path are numbered by the index  where  is the total number of shared samples in .  is the number of paths that contain the th shared data sample .  is the index of the th line  that contains the th shared sample. Finally,  is the geodesic distance of this data sample from the starting point of its line.

The optimization problem is formulated as:


where  and  are  matrices defined as:


The constraints in (\ref{eq12}) assure that the direction vectors (rows of ) have unit norms, keeping the optimization procedure away from finding trivial solutions.

Both the objective function and the constraints in (\ref{eq12}) are quadratic and convex with respect to the line parameters, that means there exists an analytical solution for this problem. By forming the Lagrangian of (\ref{eq12}) and calculating the derivatives with respect to all the variables, it is shown in Appendix D that the optimal direction matrix  can be obtained from the eigenvectors of the following matrix :

where  and  are  matrices defined in (\ref{eq14}),(\ref{eq15}),(\ref{eq16}) and (\ref{eq17}) respectively.  denotes the pseudo-inverse of the matrix  (since  is singular). Eigenvalues of  are non-negative since the matrix is positive semi-definite. 



And for  and  matrices:



where  is the Kronecker-delta operator.

Eigenvectors of the matrix  which correspond to  smallest positive eigenvalues represent the  direction matrix . It should be noted that each row must be normalized so the direction vectors would have unit norms. Interestingly, the starting positions matrix  is linearly related to :


After solving for the direction vectors  and corresponding starting points , we may obtain the low-dimensional representations using an averaging strategy:

where similar to the notation used before,  is total number of the lines that contain the th data sample,  for all .  is the index of the th line that contains the th data sample and  is the length (distance) associated to the mentioned line and data sample.

\section{Computational Complexity Analysis}
In this section we lay out time and memory complexity analysis of the Path-based Isomap and compare it to a number of existing methods. Efficient methods have been proposed for construction of the neighborhood graph  \cite{indyk2004nearest}, \cite{seidl1998optimal}. However, since the procedure is shared among all state-of-the-art methods it is not taken into consideration \cite{van2009dimensionality}. Moreover, it is assumed that number of neighbors, denoted by , in K-nearest algorithm is . Since in practice it is not relevant to the number of samples \cite{tenenbaum2000global}, \cite{roweis2000nonlinear}, \cite{belkin2003laplacian}. 

Computation of shortest paths in the SSPC algorithm requires  computations for applying Dijkstra's algorithm \cite{even2011graph}  times. The Singular Value Decomposition (SVD) used in optimization problem requires  multiplications \cite{henry2010singular}. So the total time complexity of the algorithm is . For memory analysis, there are three major components. First, SVD requires  \cite{henry2010singular}. Second, the  neighborhoods for each sample should be saved which, regarding the assumption about , needs . Third and the most important factor is the memory needed to save the shortest paths. In the limit that  goes to infinity, based on isoperimetric inequality \cite{osserman1978isoperimetric}, the average length of these paths would be lower than . Given the number of paths  the memory complexity of this component is . So the total memory complexity is . 

Isomap requires  for computing shortest paths \cite{balasubramanian2002isomap}, \cite{pallottino1984shortest}. Isomap, LLE and Laplacian-Eigenmaps need the SVD of an  matrix in the final stage \cite{tenenbaum2000global}, \cite{roweis2000nonlinear}, \cite{belkin2003laplacian}. Due to the sparsity of this matrix for LLE and Laplacian-Eigenmaps the complexity will be reduced. Hence computations in the latter stage will be  for Isomap, and  for LLE and Laplacian-Eigenmaps \cite{berry1992large}. For memory the only important component, that is the memory required by SVD, is  \cite{berry1992large}.
\section{Experimental Results}
In this section the performance of the proposed algorithm on both synthetic and real-world datasets has been simulated.

\begin{figure}[t]
\centering
        \begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[trim=1.3in 0.9in 1.3in 1in,clip,width=\textwidth]{Swiss_Roll.eps}
\end{subfigure}~
		\begin{subfigure}[b]{0.23\textwidth}
    		\includegraphics[trim=1.4in 0.9in 1.3in 1in,clip,width=\textwidth]{Swiss_Hole.eps}
\end{subfigure}\\
        \begin{subfigure}[b]{0.23\textwidth}
        \begin{center}
                \includegraphics[trim=0.3in 0.1in 0.3in 0.5in,clip,width=0.07\textwidth]{arrow.eps}
\end{center}
        \end{subfigure}~
        \begin{subfigure}[b]{0.23\textwidth}
        \begin{center}
                \includegraphics[trim=0.3in 0.1in 0.3in 0.5in,clip,width=0.07\textwidth]{arrow.eps}
\end{center}
	    \end{subfigure}\\
\begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[trim=0.8in 0.8in 0.8in 0.7in,clip,width=\textwidth]{Swiss_Roll_unfolded.eps}
                \caption{}
        \end{subfigure}~
        \begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[trim=0.7in 0.8in 0.7in 0.4in,clip,width=\textwidth]{Swiss_Hole_unfolded.eps}
                \caption{}
	    \end{subfigure}
        \vspace{2mm}
        \caption{{\small Applying Path-Based Isomap on (a) Swiss-Roll dataset with  and , and (b) Swiss-Hole dataset with  and .}}
        \label{fig_8_1}
\end{figure}

\subsection{Synthetic Datasets}
Swiss-Role is a typical dataset for testing manifold learning methods. Fig.\ref{fig_8_1}(a) shows the that Path-Based Isomap successfully unfolds a Swiss-Roll with  data points. It is notable that via the path-mapping scheme, degrees of freedom is dropped . 

\begin{figure}[b]
\centering
        \begin{subfigure}[b]{0.35\textwidth}
                \includegraphics[trim=1in 0.6in 0.9in 0.4in,clip,width=\textwidth]{Sshape.eps}
\end{subfigure}\\ \vspace{1mm}
        \begin{subfigure}[b]{0.2\textwidth}
        \begin{center}
                \includegraphics[trim=0.3in 0.1in 0.3in 0.5in,clip,width=0.07\textwidth]{arrow.eps}
\end{center}
        \end{subfigure}\\\begin{subfigure}[b]{0.35\textwidth}
                \includegraphics[trim=1.5in 0.2in 1.1in 0.2in,clip,width=\textwidth]{Sshape_unfolded.eps}
        \end{subfigure}\label{fig_Sshape}
        \vspace{5mm}
        \caption{{\small Investigating the effect of noise on Path-Based Isomap via a noisy S-shape one-dimensional manifold in . Number of data samples is  and the number of obtained paths is .}}
\end{figure}

\begin{figure*}[t]    
\centering
\begin{tabular}{cc|ccc}
	\multicolumn{2}{c|}{\multirow{2}{*}{	
\begin{minipage}[r]{0.3\textwidth}
			\vspace{-4mm}
			\includegraphics[trim=1.4in 1.1in 1.25in 1.2in,clip,width=0.8\textwidth]{HighDimData.eps}
		\end{minipage}    	    	    	
}} & 
	\begin{subfigure}[c]{0.2\textwidth}
        \includegraphics[trim=0.7in 0.7in 0.7in 0.5in,clip,width=\textwidth]{Original.eps}
    \end{subfigure} & 
	\begin{subfigure}[c]{0.2\textwidth}
    	\includegraphics[trim=0.7in 0.7in 0.4in 0.5in,clip,width=\textwidth]{PBIsomap_performance.eps}                
    \end{subfigure} &
	\begin{subfigure}[c]{0.2\textwidth}
        \includegraphics[trim=0.7in 0.7in 0.6in 0.5in,clip,width=\textwidth]{HessianLLE_performance.eps}
    \end{subfigure}    
    \\ \multicolumn{2}{c|}{} & 
	\begin{subfigure}[c]{0.2\textwidth}
        \includegraphics[trim=0.7in 0.7in 0.7in 0.5in,clip,width=\textwidth]{LLE_performance.eps}
    \end{subfigure} &
    \begin{subfigure}[c]{0.2\textwidth}
    	\includegraphics[trim=0.7in 0.7in 0.7in 0.5in,clip,width=\textwidth]{Isomap_performance.eps}
    \end{subfigure} &
    \begin{subfigure}[c]{0.2\textwidth}
    	\includegraphics[trim=0.7in 0.7in 0.7in 0.5in,clip,width=\textwidth]{Laplacian_performance.eps}
    \end{subfigure}
\end{tabular}
	\vspace{3mm}
    \caption{{\small Comparison of performance for the proposed path-based method and four state-of-the-art algorithms on a Swiss-Hole dataset with . The result are obtained by Path-Based Isomap (A), Hessian LLE (B), LLE (C), Isomap (D) and Laplacian-Eigenmaps (E). Hessian LLE and Path-Based Isomap have outperformed other methods.}}
    \label{fig_perf_comp}
\end{figure*}

Another commonplace task for examining performance of an algorithm on non-convex geometrical structures is the Swiss-Hole dataset.
Fig.\ref{fig_8_1}(b) shows the performance of the Path-Based Isomap on a Swiss-Hole. The challenge, especially for Isomap, arises from the fact that for pairs on opposite sides of the hole, the shortest path on  will no longer serve as the Euclidean distance in low-dimensional space. This might lead to the failure of the whole algorithm \cite{wang2008manifold}. However, Path-Based Isomap demonstrates acceptable resiliency to such non-normality. This effect can be understood as a consequence of (\ref{eq19}), in that good estimations in (\ref{eq19}) will correct poor ones to some extent. However, the correction causes the hole to be shrunk.

One of the main drawbacks of manifold learning methods involving shortest path calculation is their sensitivity to noise. Even one short-circuit may lead to miscalculation of many geodesic distances, and cause a drastic decline in performance. Thus the robustness of Path-Based Isomap was tested on a noisy S-shape dataset. The experiment demonstrates that the method is to an acceptable degree resistant to outliers. The averaging strategy might be again the reason behind this observation, since good estimations make for the poor ones.

In Fig.\ref{fig_perf_comp} and Fig.\ref{fig_runTime_comp} we have illustrated a comparison among our proposed method and a number of rival methods for manifold learning. Fig.\ref{fig_perf_comp} demonstrates the performance of the proposed Path-Based Isomap method, compared to  well-known rival methods. Experiments are done on a Swiss-Hole dataset consisting of  data samples which is known as a controversial dataset for most manifold learning techniques. We have utilized the DRtoolbox to implement the  rival algorithms, which is known as an efficient and effective toolbox for manifold learning. In order to find the parameters for each algorithm in DRtoolbox, a precise grid-search is formed and the most appropriate parameters are chosen. As illustrated in Fig.\ref{fig_perf_comp}, Path-Based Isomap and Hessian LLE have been successful in unfolding the embedded manifold. In both methods, the hole has a small shift toward one of the sides. Hessian LLE has preserved the size of the hole, while in the path-based approach the hole is shrunk. Other methods such as Isomap, LLE and Laplacian-Eigenmaps have shown poor performances on this dataset. However, as can be seen, both Isomap and Path-Based Isomap have preserved the overal structure of data since both methods are considered as global approaches. Methods such as Laplacian-Eigenmaps and LLE are local methods and thus do not necessarily preserve the geometrical structure among far samples.

Fig.\ref{fig_runTime_comp} in this section demonstrates the time-complexity analysis of the proposed path-based approach compared to  existing rival methods. Running-time of the methods on an S-shaped one-dimensional manifold are depicted as a function of number of data sample. Rival methods including Isomap, Kernel PCA, Diffusion Maps, LLE, Hessian LLE and Laplacian-Eigenmaps are again implemented via DRtoolbox. The methods are chosen so they have been claimed to have high efficiency or appropriate accuracy. From Fig.\ref{fig_runTime_comp} it is evident that the proposed path-based method have a considerable lower slope in a log-log plot, meaning that the method will outperform all the rival algorithms for sufficiently large . Up to  the proposed method has already surpassed  rival algorithms. Observations agree to theoretical analysis in Section .

\begin{figure}[t]
\centering
	\includegraphics[trim=0.2in 0.2in 0.2in 0.2in,clip,width=0.45\textwidth]{RunTimeComparison.eps}
	\caption{{\small Execution-time vs. number of data samples for  manifold learning techniques including the proposed Path-Based approach. Experiments are done on a noisy S-shape manifold.}}
	\label{fig_runTime_comp}
\end{figure}

\subsection{Real-World Datasets}
A canonical problem in dimensionality reduction is pose estimation. Fig.\ref{fig_8_1} illustrates statue-face database, consisting of   pixel images, rendered with different camera angles and random light directions \cite{statueFace}.  
Data samples are believed to lie on a smooth manifold in  \cite{tenenbaum2000global}. We apply Path-Based Isomap to discover the compact representation for the dataset. Interestingly, the algorithm unfolds the -dimensional manifold of the original -dimensional data samples in . The horizontal and vertical axis are tightly related to horizontal and vertical angles of the camera. 
To plot this figure we have extracted the first  linear components via PCA prior to applying Path-Based Isomap. This preprocessing improved the results, meaning that there is also a significant linear redundancy among data points. 

Fig.\ref{fig_8_4} and Fig.\ref{fig_8_5} illustrate the performance of Path-Based Isomap on MNIST \cite{MNIST}. MNIST is a well known image classification database of handwritten numbers. Despite the fact that these images do not necessarily lie on a manifold, experiments reveal that the proposed method achieves a good performance on them. In Fig.\ref{fig_8_4} the method is applied on handwritten `' images to discover their compact description in . As expected, the samples are placed according to articulation of the bottom loop and horizontal skewness of the structure. In Fig.\ref{fig_8_5} the method is applied to the combined datasets of handwritten images of `'s and `'s. It is evident that different digits are largely separated along the Y-axis. Moreover, `'s and `'s are ordered according to horizontal skewness along the X-axis. It is also notable that `'s on the left side of the plane have stronger bottom loop articulations.

\begin{figure}[t]
\centering
    \includegraphics[trim=0.5in 0.3in 0.5in 0.3in,clip,width=0.45\textwidth]{Statue_Face.eps}
    \caption{{\small Applying Path-Based Isomap on  gray-scale images of the statue-face database. Some images are shown to illustrate performance of algorithm.}}
    \label{fig_8_3}
\end{figure}

\begin{figure}[b]
\centering
    \includegraphics[trim=0.5in 0.3in 0.5in 0.3in,clip,width=0.45\textwidth]{MNIST_2s.eps}
    \caption{{\small The result of applying Path-Based Isomap on MNIST database. There are  images of handwritten `'s images in the database. There is clearly a meaningful relation between place of data points and geometrical features of their corresponding images.}}
    \label{fig_8_4}
\end{figure}

\begin{figure}[t]
\centering
    \includegraphics[trim=0.45in 0.3in 0.5in 0.3in,clip,width=0.45\textwidth]{MNIST_2s_8s.eps}
    \caption{{\small Applying Path-Based Isomap on  images of `'s and `'s in MNIST database to obtain -dimensional representations. Two clusters are formed that mainly contain one type of digit. Besides, there is a link between horizontal placing and left/right skewness in the images.}}
    \label{fig_8_5}
\end{figure}

\section{Conclusion and Future Work}
In this paper we proposed Path-Based Isomap, a new efficient method for dimensionality reduction. The method exploits shortest paths instead of samples to compute the compact representation faster. Path mapping and path selection schemes were also developed to preserve geodesic distances in the representation space. The fewer number of paths leads to significant cost savings in our approach.

It was shown throughout the paper that most virtues of LLE, Isomap and Laplacian Eigenmaps are shared by Path-Based Isomap. Experiments showed the method works surprisingly well on non-convex manifolds, as well as real-world databases like MNIST and face-statue. Moreover, it was shown that the method is to some extent resilient against noise. 

The most encouraging achievement of this paper is that the method works remarkably faster than most of rival methods, especially on large scale datasets. This improvement was confirmed by our theoretical analysis of its memory and time complexity. Since dimensionality reduction methods are now very popular in image classification and pattern recognition tasks, Path-Based Isomap should find widespread use in applications. 

Since the number of found paths has a direct effect on complexity of the method, and regarding suboptimal performance of SSPC, there is considerable space for improvement. Besides , there are still two shortcomings shared between rival approaches and ours. First, the method is sensitive to outliers causing short-circuits, because they could mislead Dijkstra's algorithm in the first stage. Second, we assume the sampling density to be nearly uniform. Further empirical and theoretical research could shed some light on these issues.









\appendices
\section{Network Rigidity}
The method discussed before for the shortest path covering of the data samples guarantees that each sample lies on at least one path (line). However, the uniqueness of low-dimensional representation under these circumstances must be further investigated. In other words many radically different representations of a particular set of data samples may result in one set of shortest paths, which implies that loss of information is possible. 

Fig. \ref{fig_6_1_a} shows a set of  data samples spread in a two-dimensional space which are covered with  separated paths. It can be easily observed that the obtained network of paths is not rigid since each path is free to move independently with respect to others. The constraints of placing all the data samples in their corresponding lines with their corresponding orders and distances are not enough to obtain a unique description of all the data sample positions. It is clear that without any intersection there is no objective function and hence no unique solution.

As shown in Fig. \ref{fig_6_1_b} adding three more paths to the network results into a rigid structure that uniquely describes the proximity information of the data samples. For a rigid structure, there is no degree of freedom except rotation and translation of the whole object in  that do not affect neighborhood information. The translation can be mathematically modeled as a constant -dimensional vector added to all s. The rotation is also modeled with a  unitary matrix affecting all  and  vectors. Therefore we can express the following definition of a rigid network:

\begin{figure}[t]
\centering
        \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[trim=0.8in 0.3in 0.8in 0.3in,width=\textwidth]{NetRigid_1.eps}
                \caption{}
                \label{fig_6_1_a}
        \end{subfigure}\begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[trim=0.8in 0.3in 0.8in 0.3in,width=\textwidth]{NetRigid_2.eps}
                \caption{}
                \label{fig_6_1_b}
        \end{subfigure}
        \caption{{\small An example of a non-rigid and a rigiditized network of intersecting lines. (a)  two-dimensional data samples are covered using  isolated lines. Although all the data samples are covered, the resulting network in not rigid. (b) Adding  intersecting lines as shown (red lines) to the network results into a rigid structure.}}
 \label{fig_6_1}
\end{figure}

\begin{framed}
\begin{myDefinition}
A rigid network of intersecting paths is defined as a set of lines in  whose data samples can be uniquely and unambiguously mapped into the representation space except for a total translation by a constant vector and a rotation by any unitary matrix.
\end{myDefinition}
\end{framed}

In order to reach a rigid network a compensation strategy must be utilized after the shortest path covering stage. So sufficient paths to stabilize the current structure could be found and added to the set . It has been experimentally observed that the stochastic shortest path covering method introduced in section 4 of the submitted paper, usually results directly in a rigid network and does not need this step. The reason behind this phenomenon is the numerous intersections obtained by the stochastic path covering algorithm. However, few stabilizing lines should be added in some cases specially when the number of data samples is not large enough. The proposed strategy for the stabilization of a network is explained as follows.

Assume a set of data samples in high-dimensional space are covered by a set of  shortest paths denoted by . Also assume that the lines in  are intersected in a set of nodes denoted by  where  denotes the common node between the th and the th lines. If lines  and  do not cross each other then  does not exist.

Many sub-networks of a rigid network are also rigid. Formally speaking, a rigid sub-network is a set of lines, denoted as ,  which are shown to be rigid with respect to each other. Here if  is the number of lines in ,  for  are the indices of  lines (paths) in .

\begin{framed}
\begin{myTheorem}
The smallest possible rigid sub-network with non-zero volume in  is a geometrical structure consisting of  lines called a hyper-pyramid. A hyper-pyramid  can be identifies by the following properties.

\vspace{3mm}
For each feasible , we should look for  subsets of , denoted by s (), so that each subset represents  different lines of the object. Each two distinct s must share exactly one line:
\begin{center}
\\
\vspace{3mm}

\end{center}
Also the lines in each subset, , must cross each other in at least one common data sample:
\begin{center}
\\
\\ \vspace{1mm}

\end{center}
\vspace{3mm}
The only exception is the special case of , where the above conditions should be replaced by simply existence of a single line.
\end{myTheorem}
\end{framed}

The  operator in the above expressions denotes the number of members in a set. For the simple cases of  and  the corresponding hyper-pyramid is a single line, two-dimensional triangle and three-dimensional pyramid respectively.

The proof for the Theorem 1 is given in Appendix B. The importance of the Theorem 1 is providing a systematic procedure to find many initial rigid sub-networks (hyper-pyramids) within a network of intersecting lines. It is observed experimentally that running the shortest path covering algorithm on a densely sampled manifold results in several hyper-pyramids.
	
\begin{framed}
\begin{myTheorem}
If a single line  has two intersections with a rigid sub-network , adding  to the  results in a new rigid sub-network. This can be expressed as:
\begin{center}
\\
\vspace{1mm}

\end{center}
\end{myTheorem}
\end{framed}

Proof of Theorem 2 can be found in Appendix C. Theorem 2 enables us to gradually build a rigid network by offering an approach to merge a single line and a rigid network, and obtain a larger rigid structure if they have at least two intersections. These rigidity checking and merging procedures can be iterated until the achieved rigid network cannot grow larger anymore. We can simply start the procedure with a hyper-pyramid. It is not possible to start by single lines since any attached line will be in almost the same direction, leaving no option to grow in other dimensions. In fact, as discussed in Theorem 1, the initial rigid sub-networks should have non-zero volume in .

If adding lines ended up in a sub-network that can not grow larger and does not include all the samples, we should add further paths to insure rigidity. Hereby, we could determine that low-dimensional representation will be unique. However, this usually does not happen, that means usually no further paths are needed in practice.

\section{On Rigidity of Hyper-pyramids}
In this section a proof for the Theorem 1 is derived. Theorem 1 states that a hyper-pyramid is always a rigid sub-network.

According to the definition of a hyper-pyramid in Theorem 1, such sub-networks contain  corner points in  space. Each corner point is the intersection of  different lines (for example a triangle has  corner points while a pyramids contains ). From now on, for a -dimensional hyper-pyramid we denote the mentioned corner points as . In order to cancel the effect of total translation in the representation, we assume that a constant vector  is subtracted from all the corner points to place the center of gravity on the origin. Therefore we would have the following condition:


Corner points of a hyper-pyramid are completely interconnected, i.e. there is a line (path) between each two corners. This indicates that the distances among all pairs of corner points are assumed to be known, which leads to the following system of quadratic equations:

As discussed before, the geodesic distances, s (), are known as a result of the shortest path covering stage.

In order to prove the rigidity of hyper-pyramid structures, we will show that the set of solutions satisfying the above system of equations, differ only in an arbitrary rotation and translation.

Let us define the vector  as follows:

where the parameterized indices  are solely used to facilitate the construction of , and can be defined in any arbitrary order. Left sides of expressions in the equations of (\ref{eq21}) can be rewritten using the components of , since there is the following relation:


Again,  indices are solely defined to facilitate the ordering of equations.

In addition, inner product of each corner point, , by both sides of (\ref{eq20}) gives an extra  linear equations. The collection of all the mentioned linear dependencies results in the following  determined system of linear equations:


The matrix formulation of above equations can be written as follows:

where  is a known  matrix. It can be shown that the matrix  in (\ref{eq25}) is invertible and the components of  which represent all possible inner products of corner points, can be uniquely determined. In (\ref{eq26}) the  matrix for the case of  has been shown.


Invertibility of  states that the inner products among all pairs of the corner points are uniquely determined by the set of non-linear equations in (\ref{eq21}). In order to finalize the proof we must show that any two structures that have the same inner products among their corresponding corner points would differ only in a rigid transformation. For this purpose, assume that there are two different sets of solutions for the equations in (\ref{eq21}), denoted by  and , . Based on the previous discussions, we have:


Let us decompose each vector  into a non-negative value , and a unit length vector , which represent the length and the direction of  respectively.


According to (\ref{eq27}),  for all , so (\ref{eq27}) reduces to the following set of equalities:


Both  and  are unit length vectors in , and thus are related through a particular rotation transform:

where  is a  unitary matrix mapping  onto . Therefore we have:


Since (\ref{eq31}) holds for all the unitary vectors associated with any arbitrary hyper-pyramid in , we may conclude that  for . And finally:


It should be reminded that the translation has been already canceled by shifting the center of gravity for each set of solutions to the origin. Therefore all possible sets of solution for a hyper-pyramid in  only differ in a translation and rotation which do not affect the proximity information of data samples.


\section{Merging Rigid Subnetworks}
This section provides the proof for Theorem 2. Based on Theorem 2, any line that has at least two intersections with a rigid sub-network, is totally rigid with respect to the sub-network. 

Assume a line  with starting point  and direction vector  has two data samples, namely  and , shared with a rigid sub-network . The associated distances from the starting point of the line to the shared samples are denoted by  and  respectively. Hence we have:

	
Using simple algebra, following relations are obtained for the parameters of  in terms of shared data samples  and :


Since the sub-network  is assumed to be rigid, all of its possible representations would differ only in a constant translation  and an arbitrary rotation by a unitary matrix . Assume that another possible representation of  is denoted by , which is related to the primary representation through the following equation:


Therefore, new positions for the shared samples can be computed as  and  since both data samples are shared with .

In the new representation, orientation and position of  would also confront some transformations. Assume that the new line properties are denoted  by  and . Based on the relations in (\ref{eq34}) we can compute the new parameters of the line  through the following equations:


Relations in (\ref{eq36}) declare that the starting point of  has been translated by the constant vector , and both the starting point and the direction vector have been affected by the unitary matrix . This implies that all the samples in the line  will confront the same transformation as the data samples in . Therefor the connected line is rigid with respect to the sub-network and the two objects can be merged to form a larger rigid sub-network.

\section{Analytical Solution of Path Mapping Optimization Problem}
In this section an analytical solution for the optimization problem introduced in section 5 of the submitted paper is derived. The optimization problem is formulated as follows:


where:


In order to solve the optimization problem in (\ref{eq_opt}), the Lagrangian function corresponding to the objective function  and the set of quadratic constraints in (\ref{eq_opt}) should be formulated:


s represent the Lagrange multipliers of the optimization. Finding the minimizers of (\ref{eq_J}), , requires the KKT conditions to be satisfied. First, derivatives of  with respect to all the variables in (\ref{eq_J}) should become zero. Second, the equality constraints in (\ref{eq_opt}) must hold. Derivatives with respect to s and s may be computed as follows:




Derivations with respect to s result in the same constraints in (\ref{eq_opt}). Based on the previous discussions, the preferred low-dimensional representation may be obtained by solving the following set of equations:


And also satisfying following equality constraints:


In order to simplify the notations, the linear equations in (\ref{eq40}) can be rewritten in the following matrix form:

where  and  are  matrices whose entries can be calculated by the formulations given in (\ref{eq14}),(\ref{eq15}),(\ref{eq16}) and (\ref{eq17}) respectively.  is a  diagonal matrix consisting of s on its main diagonal ().



Equations in (\ref{eq42}) will lead to a non-linear matrix equation as follows:


 is the pseudo-inverse of the matrix . The matrix  is singular and thus non-invertible since each one of its rows sum to zero. This property can be investigated from (\ref{eq_A_mat}). Therefore  has an all-one  eigenvector with a zero eigenvalue:
 

The all-one eigenvector in  models a constant translation in the position of all the data samples in low-dimensional representation (Note that  is associated with the optimal starting points  in (\ref{eq42})). As will be discussed in section 2 of this document this translation does not affect the proximity information. Placement of  instead of the true inverse  in (\ref{eq43}) forces the whole low-dimensional representation to be centered on the origin. The same story holds for  (since it is singular too), so that its null eigenvector yields the ``rotation" invariance property of parameters in mathematical terms.

Before going any further with (\ref{eq43}), let us investigate if there could be any alternative way to reach a straight forward solution for the original problem in section V of the submitted paper. The main reason for the constraints in (\ref{eq41}) is to avoid any trivial solution in which all the variables become zero. This goal could be gained by imposing different constraints in the optimization problem in (\ref{eq_opt}). The new set of constraint inequalities can be formulated as follows:


The above set of inequalities although represents a whole different mathematical meaning, however, results into a small difference in the formulation given in (\ref{eq43}):


Also, inequality constraints in the new formulations impose extra KKT conditions, which are:

where s are called KKT multipliers in this formalism.

Unlike (\ref{eq43}), matrix equation of (\ref{eq45}) represents an eigenvector problem that can be efficiently solved by a polynomial-time algorithm. Therefore the problem of finding appropriate low-dimensional representations for a high-dimensional dataset results into eigen-decomposition of a  matrix.

We have denoted the square  matrix  in (\ref{eq45}) with  in section V. Again it can be shown that  is a symmetric semi-definite matrix with real and non-negative eigenvalues. This will satisfy the constraints in (\ref{eq_new_con}). Also the eigenvector computation of  can be accomplished by a singular value decomposition (SVD) problem with up to  operations.
 
Substitution of (\ref{eq45}) into (\ref{eq_J}) reveals that the appropriate columns of  matrix which minimize the objective function are  eigenvectors of  that correspond to the  smallest positive eigenvalues. Note that the  smallest eigenvalues of  are the minimum KKT multipliers of the optimization problem. 

Finally we should apply an appropriate normalization step to match the results with the set of constraint in (\ref{eq_opt}). The final solution of the optimization problem can be formulated as follows:

where s are columns of the unitary matrix  obtained from a singular value decomposition step discussed before.  is the th component of the  vector. It has been assumed that the eigenvalues and corresponding eigenvectors of the  matrix are in descending order, so the last eigenvector corresponds to the smallest eigenvalue.  is the number of zero eigenvalues. Based on the normalization performed in (\ref{eq46}) the direction vectors (rows of ) will be normal vectors with unit lengths. Optimal starting points can be obtained using (\ref{eq45}).

\ifCLASSOPTIONcompsoc
\section*{Acknowledgments}
\else
\section*{Acknowledgment}
\fi


The authors would like to thank M. F. Azampour and A. Ghafari for their helpful comments throughout the paper.


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi







{\small
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{demartines1997curvilinear}
P.~Demartines and J.~H{\'e}rault, ``Curvilinear component analysis: A
  self-organizing neural network for nonlinear mapping of data sets,''
  \emph{Neural Networks, IEEE Transactions on}, vol.~8, no.~1, pp. 148--154,
  1997.

\bibitem{hinton2002stochastic}
G.~E. Hinton and S.~T. Roweis, ``Stochastic neighbor embedding,'' in
  \emph{Advances in neural information processing systems}, 2002, pp. 833--840.

\bibitem{donoho2003hessian}
D.~L. Donoho and C.~Grimes, ``Hessian eigenmaps: Locally linear embedding
  techniques for high-dimensional data,'' \emph{Proceedings of the National
  Academy of Sciences}, vol. 100, no.~10, pp. 5591--5596, 2003.

\bibitem{lespinats2009rankvisu}
S.~Lespinats, B.~Fertil, P.~Villemain, and J.~H{\'e}rault, ``Rankvisu: Mapping
  from the neighborhood network,'' \emph{Neurocomputing}, vol.~72, no.~13, pp.
  2964--2978, 2009.

\bibitem{tenenbaum2000global}
J.~B. Tenenbaum, V.~De~Silva, and J.~C. Langford, ``A global geometric
  framework for nonlinear dimensionality reduction,'' \emph{Science}, vol. 290,
  no. 5500, pp. 2319--2323, 2000.

\bibitem{roweis2000nonlinear}
S.~T. Roweis and L.~K. Saul, ``Nonlinear dimensionality reduction by locally
  linear embedding,'' \emph{Science}, vol. 290, no. 5500, pp. 2323--2326, 2000.

\bibitem{belkin2003laplacian}
M.~Belkin and P.~Niyogi, ``Laplacian eigenmaps for dimensionality reduction and
  data representation,'' \emph{Neural computation}, vol.~15, no.~6, pp.
  1373--1396, 2003.

\bibitem{borg2005modern}
I.~Borg, \emph{Modern multidimensional scaling: Theory and applications}.\hskip
  1em plus 0.5em minus 0.4em\relax Springer, 2005.

\bibitem{timofte2012iterative}
R.~Timofte and L.~Van~Gool, ``Iterative nearest neighbors for classification
  and dimensionality reduction,'' in \emph{Computer Vision and Pattern
  Recognition (CVPR), 2012 IEEE Conference on}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2012, pp. 2456--2463.

\bibitem{guo2011simultaneous}
G.~Guo and G.~Mu, ``Simultaneous dimensionality reduction and human age
  estimation via kernel partial least squares regression,'' in \emph{Computer
  Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 657--664.

\bibitem{kohonen2001self}
T.~Kohonen, \emph{Self-organizing maps}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2001, vol.~30.

\bibitem{baraniuk2009random}
R.~G. Baraniuk and M.~B. Wakin, ``Random projections of smooth manifolds,''
  \emph{Foundations of computational mathematics}, vol.~9, no.~1, pp. 51--77,
  2009.

\bibitem{hegde2012learning}
C.~Hegde, A.~C. Sankaranarayanan, and R.~G. Baraniuk, ``Learning manifolds in
  the wild,'' \emph{Preprint, July}, 2012.

\bibitem{bernstein2000graph}
M.~Bernstein, V.~De~Silva, J.~C. Langford, and J.~B. Tenenbaum, ``Graph
  approximations to geodesics on embedded manifolds,'' Technical report,
  Department of Psychology, Stanford University, Tech. Rep., 2000.

\bibitem{balasubramanian2002isomap}
M.~Balasubramanian and E.~L. Schwartz, ``The isomap algorithm and topological
  stability,'' \emph{Science}, vol. 295, no. 5552, pp. 7--7, 2002.

\bibitem{seidl1998optimal}
T.~Seidl and H.-P. Kriegel, ``Optimal multi-step k-nearest neighbor search,''
  in \emph{ACM SIGMOD Record}, vol.~27, no.~2.\hskip 1em plus 0.5em minus
  0.4em\relax ACM, 1998, pp. 154--165.

\bibitem{boothe2007graph}
P.~Boothe, Z.~Dvor{\'a}k, A.~M. Farley, and A.~Proskurowski, ``Graph covering
  via shortest paths,'' \emph{Congressus Numerantium}, vol. 187, p. 145, 2007.

\bibitem{indyk2004nearest}
P.~Indyk, ``Nearest neighbors in high-dimensional spaces,'' 2004.

\bibitem{van2009dimensionality}
L.~Van~der Maaten, E.~Postma, and H.~Van Den~Herik, ``Dimensionality reduction:
  A comparative review,'' \emph{Journal of Machine Learning Research}, vol.~10,
  pp. 1--41, 2009.

\bibitem{even2011graph}
S.~Even, \emph{Graph algorithms}.\hskip 1em plus 0.5em minus 0.4em\relax
  Cambridge University Press, 2011.

\bibitem{henry2010singular}
E.~Henry and J.~Hofrichter, ``Singular value decomposition: application to
  analysis of experimental data,'' \emph{Essential Numerical Computer Methods},
  vol. 210, pp. 81--138, 2010.

\bibitem{osserman1978isoperimetric}
R.~Osserman, ``The isoperimetric inequality,'' \emph{Bulletin of the American
  Mathematical Society}, vol.~84, no.~6, pp. 1182--1238, 1978.

\bibitem{pallottino1984shortest}
S.~Pallottino, ``Shortest-path methods: Complexity, interrelations and new
  propositions,'' \emph{Networks}, vol.~14, no.~2, pp. 257--267, 1984.

\bibitem{berry1992large}
M.~W. Berry, ``Large-scale sparse singular value computations,''
  \emph{International Journal of Supercomputer Applications}, vol.~6, no.~1,
  pp. 13--49, 1992.

\bibitem{wang2008manifold}
C.~Wang and S.~Mahadevan, ``Manifold alignment using procrustes analysis,'' in
  \emph{Proceedings of the 25th international conference on Machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2008, pp. 1120--1127.

\bibitem{statueFace}
\url{http://isomap.stanford.edu/datasets.html}.

\bibitem{MNIST}
\url{http://yann.lecun.com/exdb/mnist/â€Ž}.

\end{thebibliography}
 }















\begin{IEEEbiography}{Amir Najafi}
\end{IEEEbiography}

\begin{IEEEbiography}{Amir Joudaki}
\end{IEEEbiography}



\begin{IEEEbiography}{Emad Fatemizadeh}
\end{IEEEbiography}









\end{document}
