In this section, we conduct experiments to demonstrate the effectiveness of our proposed model. We first briefly introduce datasets used for training and testing, then we describe our implementation details. Next, we analyze the effect of focal loss on our model. Finally, we compare our model against state-of-the-art techniques on seven public benchmark datasets, including regular and irregular text. 
\subsection{Datasets} \label{sec:datasets}
The training datasets contains two datasets: \textbf{Synth90k} and \textbf{SynthText}. Synth90k is a synthetic dataset introduced in \cite{jaderberg2014synthetic}. This dataset contains 9 million images created by combining  90.000 common English words and random variations and effects. SynthText is a synthetic dataset introduced in \cite{gupta2016synthetic}, which contains 7 million samples by the same generation process as Synth90k \cite{jaderberg2014synthetic}. 
However, SynthText is targeted for text detection so that an image may contain several words. 
All experiments are evaluated on seven well-known public benchmarks described, which can be divided into two categories: regular text and irregular text. 
Regular text datasets include IIIT5K, SVT, ICDAR03, ICDAR13.
\begin{itemize}
    \item IIIT5K \cite{mishra2012top} contains 3000 test images collected from Google image searches. 
    \item ICDAR03 \cite{lucas2005icdar} contains 860 word-box cropped images.
    \item ICDAR13 \cite{karatzas2013icdar} contains 1015 word-box cropped images.
    \item SVT contains 647 testing word-box collected from Google Street View. 
\end{itemize}
{Irregular text} datasets include ICDAR15, SVT-P, CUTE.
\begin{itemize}
    \item ICDAR15 \cite{karatzas2015icdar} contains 1811 testing word-box cropped images collected from Google Glass without careful positioning and focusing. 
    \item SVT-P \cite{quy2013recognizing} contains 645 testing word-box cropped images collected from Google Street View. Most of them are heavily distorted by the non-frontal view angle. 
    \item CUTE \cite{risnumawan2014robust} contains 288 word-box cropped images, which are curved text images.
\end{itemize}
\subsection{Configurations}
\subsubsection{Implementation Detail}
We implement the proposed model by Pytorch library and Python programming language. The model is trained and tested on an NDIVIA RTX 2080 Ti GPU with  GB memory. We train the model from scratch using Adam optimizer with the learning rate of . To evaluate the trained model, we use dataset III5K. The pretrained model and code are available at \cite{sourcecode}

\subsubsection{Rectification Network} 
All input images are resized to  before applying the rectification network. The rectification network consists of three components: a localization network, a thin plate spline (TPS) transformation,  and a sampler. The localization network consists of 6 convolutional layers with the kernel size of  and two fully-connected (FCN) layers. Each FCN is followed by a  max-pooling layer. The number of the output filters is 32, 64, 128, 256, and 256. The number of output units of FCN is 512 and 2K, respectively, where  is the number of the control points. In all experiments, we set  to 20, as suggested by \cite{shi2018aster}. The sampler generates the rectified image with a size of . The size of the rectified image is also the input size of the feature extraction module. 

\subsubsection{Feature Extraction} We construct the feature extraction module based on Resnet architecture \cite{he2016deep}. The configurations of the feature extraction network are listed in Table \ref{tab:FEarchitecture}. Our feature extraction network contains five blocks of 45 residual layers. Each residual unit consists of a  convolutional layer, followed by a  convolution layer. In the first two blocks, we use  stride to reduce the feature map dimension. In the next blocks, we use  stride to downsampled feature maps. The  stride also allows us to retain more information horizontally to distinguish neighbor characters effectively.

\begin{table}[h]
\centering
\caption{Feature extraction network configurations.Each block is a residual network block. "s" stands for stride of the first convolutional layer in a block.}
\label{tab:FEarchitecture}
\resizebox{.5 \textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
 & Layer & Feature map size & Configuration \\ \hline
\multirow{6}{*}{Encoder} & 
Block 0 &  &  conv, s()  \\ \cline{2-4} 
& Block 1 &  & , s()  \\ \cline{2-4} 
& Block 2 &  & , s()   \\ \cline{2-4} 
& Block 3 &  & , s()   \\ \cline{2-4} 
& Block 4 &  & , s()   \\ \cline{2-4}
& Block 5 &  & , s()   \\ \hline
\end{tabular}
}
\end{table}

\subsubsection{Recognition}
The number of blocks in the encoder and the decoder are set both to .
In each block of the encoder and the decoder, the dimension of the feed forward vector and the ouput vector are set to  and , respectively.
The number of head attention layers is set to .
The decoder recognizes 94 different characters, including numbers, alphabet characters, uppercase, lowercase, and 32 punctuation in ASCII.

\subsection{Result and Discussion}
\subsubsection{Impact of focal loss}
To analyze the effect of focal loss, we study two variants of the proposed model. The first variant uses negative log-likelihood, and the second one leverages focal loss. 
\begin{table}[h]
\centering
\caption{Recognition accuracies with Negative log-likelihood and Focal Loss}
\label{tab:loss}
\begin{tabular}{|l|cc|}
\hline
\textbf{Variant} & \textbf{Negative log-likelihood} & \textbf{Focal Loss} \\ \hline
IIIT5K  & 92.6     & \textbf{93.9}\\
SVT  & 85.8     & \textbf{88.6}\\
ICDAR03 & 94.1     & \textbf{95}\\
ICDAR13 & 92& \textbf{92.8}\\
ICDAR15 & 76.1     & \textbf{77.5}\\
SVT-P& 79.4     & \textbf{81.7}\\
CUTE & 80.6     & \textbf{85.4}\\ \hline
Avarage & 86.9 & \textbf{88.2}\\ \hline

\end{tabular}
\end{table}
\par As shown in Table \ref{tab:loss}, the model with focal loss outperforms the one with log-likelihood on all datasets. Notably, on average, focal loss improves the accuracy by 2.3 \% compared to log-likelihood. For the best case, i.e., CUTE, the performance gap between the two variants is 4.8 \%
\subsubsection{Impact of rectification network}
In this section, we study the effect of text rectification by comparing SAFL and a variant which does not include the rectification module. 
\begin{table}[h]
\centering
\caption{Recognition accuracies with and without rectification}
\label{tab:withoutrec}
\begin{tabular}{|l|cc|}
\hline
\textbf{Variant} & \textbf{SAFL w/o text rectification} & \textbf{SAFL} \\ \hline
IIIT5K  & 90.7   & \textbf{93.9}\\
SVT  & 83.3    & \textbf{88.6}\\
ICDAR03 & 93     & \textbf{95}\\
ICDAR13 & 90.7& \textbf{92.8}\\
ICDAR15 & 72.9     & \textbf{77.5}\\
SVT-P& 71.6    & \textbf{81.7}\\
CUTE & 77.4     & \textbf{85.4}\\ \hline
Avarage & 84.1 & \textbf{88.2}\\ \hline

\end{tabular}
\end{table}

Table \ref{tab:withoutrec} depicts the recognition accuracies of the two models over seven datasets. It can be observed that the rectification module increases the accuracy significantly. Specifically, the performance gap between SAFL and the one without the rectification module is  on average. 
In the best cases, SAFL improves the accuracy by  and  compared to the other on the datasets SVT-P and CUTE, respectively.
The reason is that both SVT-P and CUTE contains many both irregular texts such as perspective texts or curved texts. 

\subsubsection{Comparison with State-of-the-art}
In this section, we compare the performance of SAFL with the latest approaches in scene text recognition. 
The evaluation results are shown in Table \ref{tab:CompareSOTA}.
In each column, the best value is bolded. 
the "Avarage" column is the weighted average over all the data sets. 
Concerning the irregular text, it can be observed that SAFL achieves the best performance on  data sets. 
Particularly, SAFL outperforms the current state-of-the-art, ESIR \cite{zhan2019esir}, by a margin of  on average, particulary on CUTE () and SVT-P ().
Concerning the regular datasets, SAFL outperforms the other methods on two datasets IIIT5K and ICDAR03. Moreover, SAFL also shows the highest average accuracy over all the regular text datasets. To summarize, SAFL achieves the best performance on 5 of 7 datasets and the highest average accuracy on both irregular and regular texts. 
\begin{table*}[t]
  \centering
\caption{Scene text accurancies (\%) over seven public benchmark test datasets.}
\label{tab:CompareSOTA}
  \begin{tabular}{|l|cccc||c|ccc||c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{5}{c|}{\textbf{Regular test dataset}}& \multicolumn{4}{c|}{\textbf{Irregular test dataset}} \\ \cline{2-10} 
\multicolumn{1}{|c|}{} & IIIT5k & SVT & ICDAR03 & ICDAR13 &Average & ICDAR15 & SVT-P & CUTE & Average  \\ \hline
Jaderberg et al. \cite{jaderberg2014deep} & - & 80.7 & 93.1  & 90.8  & - & - & - & - & - \\
CRNN \cite{shi2016end} & 78.2 & 80.8 & 89.4 & 86.7 & 81.8 & - & - & - & - \\ 
RARE \cite{shi2016robust} & 81.9 & 81.9 & 90.1 & 88.6 & 85.3 & - & 71.8 & 59.2 & - \\ 
Lee et al. & 78.4 & 80.7& 88.7 & 90.0 & 82.4 & - & - & - & - \\ 
Yang et al. \cite{yang2017learning} & - & 75.8 & - & - & - & - & 75.8 & 69.3 & - \\ 
FAN \cite{cheng2017focusing} & 87.4 & 85.9 & 94.2 & 93.3 & 89.4 & 70.6 & - & - & - \\
Shi et al. \cite{shi2016robust} & 81.2 & 82.7 & 91.9 & 89.6 & 84.6 & - & - & - & - \\
Yang et al. \cite{yang2017learning} & - & - & - & - & - & - & 75.8 & 69.3 & - \\ 
Char-Net \cite{liu2018char}  & 83.6 & 84.4 & 91.5 & 90.8 & 86.2 & 60.0 & 73.5 & - & - \\ 
AON \cite{cheng2018aon}& 87.0 & 82.8 & 91.5 & - & - & 68.2 & 73.0 & 76.8 & 70.0 \\
EP \cite{bai2018edit}& 88.3 & 87.5 & 94.6 &\textbf{ 94.4} & 90.3 & 73.9 & - & - & - \\
Liao et al. \cite{liao2019scene} & 91.9 & 86.4 & - & 86.4 & - & - & - & 79.9 & - \\
Baek et al. \cite{baek2019wrong} & 87.9 & 87.5 & 94.9 & 92.3 & 89.8 & 71.8 & 79.2 & 74.0 & 73.6 \\ 
ASTER \cite{shi2018aster}& 93.4 & 89.5 & 94.5 & 91.8 & 92.8 & 76.1 & 78.5 & 79.5 & 76.9 \\ 
SAR \cite{li2019show}& 91.5 & 84.5 & - & 91.0 & - & 69.2 & 76.4 & 83.3 & 72.1 \\
ESIR \cite{zhan2019esir} & 93.3 & \textbf{90.2} & - & 91.3 & - & 76.9 & 79.6 & 83.3 & 78.1 \\
\hline
\textbf{SAFL}   & \textbf{93.9} & 88.6 & \textbf{95}& 92.8 & \textbf{93.3} & \textbf{77.5} & \textbf{81.7} & \textbf{85.4} & \textbf{79.3} \\ \hline
\end{tabular}
\end{table*}
