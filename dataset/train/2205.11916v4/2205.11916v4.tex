\documentclass{article}











\usepackage[final]{neurips_2022}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{tablefootnote}
\usepackage{hyperref}       \usepackage{float}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{xspace}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{array, booktabs}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{caption}
\usepackage{comment}
\usepackage{array}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[subrefformat=parens]{subcaption}
\captionsetup[table]{skip=5pt}
\captionsetup[figure]{skip=5pt}

\definecolor{myblack}{rgb}{0.5, 0.5, 0.5}
\newcommand{\kojima}[1]{{\color{red}[{#1} --TK]}}
\newcommand{\mr}[1]{{\color{teal}[{#1} --MR]}}
\newcommand{\sg}[1]{{\color{orange}[{#1} --SG]}}
\newcommand{\yi}[1]{{\color{magenta}[{#1} --YI]}}
\newcommand{\lighttext}[1]{{\color{myblack}{#1}}}
\newcommand{\algcomment}[1]{\lighttext{{#1}}}


\newcommand{\CoT}{chain of thought\xspace}
\newcommand{\ours}{Zero-shot-CoT\xspace}
\newcommand{\oursvtwo}{Zero-shot-Ex-CoT\xspace}
\newcommand{\theirs}{Few-shot-CoT\xspace}
\newcommand{\theirsz}{Zero-shot\xspace}
\newcommand{\theirsf}{Few-shot\xspace}
\newcommand{\okmark}{{\textbf{\textcolor[rgb]{0.1, 0.5, 0.1}{}}}}
\newcommand{\ngmark}{{\textbf{\color{red}{\ding{55}}}}}
\newcommand{\davinci}{text-davinci-002\xspace}
\newcommand{\bblue}[1]{{\textbf{\color{blue}{#1}}}}
\newcommand{\bred}[1]{{\textbf{\color{red}{#1}}}}
\newcommand{\bblack}[1]{{\textbf{\color{black}{#1}}}}

\newcommand{\mysection}{\S\xspace}

\newcommand{\myspace}{}


\title{Large Language Models are Zero-Shot Reasoners}









\author{Takeshi Kojima\\
  The University of Tokyo \\
  \texttt{t.kojima@weblab.t.u-tokyo.ac.jp} \\
\And
  Shixiang Shane Gu
\\
  Google Research, Brain Team \\
\AND
  Machel Reid \\
  Google Research\thanks{Work done while at The University of Tokyo.} \\
\And
  Yutaka Matsuo \\
  The University of Tokyo \\
\And
  Yusuke Iwasawa \\
  The University of Tokyo \\
}



\begin{document}


\maketitle




\begin{abstract}





Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent \textit{few-shot} learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult \textit{system-2} tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent \textit{zero-shot} reasoners by simply adding ``Let's think step by step'' before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects),  without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large-scale InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental \textit{zero-shot} capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars. 






\end{abstract}



\section{Introduction}


\begin{figure}[t]
  \begin{center}
\includegraphics[width=\columnwidth]{conceptual_differences.pdf}
  \end{center}
  \caption{Example inputs and outputs of GPT-3 with (a) standard \theirsf (\citep{brown2020language}), (b) \theirs (\citep{cot_wei}), (c) standard \theirsz, and (d) ours (\ours). 
  Similar to \theirs, \ours facilitates multi-step reasoning (blue text) and reach correct answer where standard prompting fails. 
  Unlike \theirs using step-by-step reasoning examples \textbf{per task}, ours does not need any examples and just uses the same prompt ``Let's think step by step'' \textit{across all tasks} (arithmetic, symbolic, commonsense, and other logical reasoning tasks).
}
  \label{fig_overview_1}
\end{figure}

Scaling up the size of language models has been key ingredients of recent revolutions in natural language processing (NLP) \citep{transformer, bert, t5, brown2020language, lamda, gopher, palm}. 
The success of large language models (LLMs) is often attributed to (in-context) few-shot or zero-shot learning. It can solve various tasks by simply conditioning the models on a few examples (few-shot) or instructions describing the task (zero-shot). 
The method of conditioning the language model is called ``prompting'' \citep{liu2021pre}, and designing prompts either manually \citep{schick2020s,prompt1} or automatically \citep{gao2021making,shin2020autoprompt} has become a hot topic in NLP. 

In contrast to the excellent performance of LLMs in intuitive and single-step \textit{system-1}~\citep{stanovich2000individual} tasks with task-specific few-shot or zero-shot prompting~\citep{liu2021pre}, even language models at the scale of 100B or more parameters had struggled on \textit{system-2} tasks requiring slow and multi-step reasoning \citep{gopher}. 
To address this shortcoming, \citet{cot_wei, cot_wei_sc} have proposed \textit{\CoT} prompting (CoT), which feed LLMs with the step-by-step reasoning examples rather than standard question and answer examples (see Fig. \ref{fig_overview_1}-a). 
Such \CoT demonstrations facilitate models to generate a reasoning path that decomposes the complex reasoning into multiple easier steps. 
Notably with CoT, the reasoning performance then satisfies the scaling laws better and jumps up with the size of the language models. For example, when combined with the 540B parameter PaLM model~\citep{palm}, \CoT prompting significantly increases the performance over standard few-shot prompting across several benchmark reasoning tasks, e.g., GSM8K (17.9\%  58.1\%). 

While the successes of CoT prompting~\citep{cot_wei}, along those of many other task-specific prompting work~\citep{gao2021making,schick2020s,liu2021pre}, are often attributed to LLMs' ability for few-shot learning~\citep{brown2020language}, we show that LLMs are decent \textit{zero-shot} reasoners by adding a simple prompt, \textit{Let's think step by step}, to facilitate step-by-step thinking before answering each question (see~\autoref{fig_overview_1}). 
Despite the simplicity, our \ours successfully generates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a problem where the standard zero-shot approach fails. 
Importantly, our \ours is versatile and \textit{task-agnostic}, unlike most prior task-specific prompt engineering in the forms of examples (few-shot) or templates (zero-shot)~\citep{liu2021pre}: 
it can facilitate step-by-step answers across various reasoning tasks, including arithmetic (MultiArith~\citep{multiarith}, GSM8K~\citep{gsm8k}, AQUA-RAT~\citep{aqua}, and SVAMP~\citep{svamp}), symbolic reasoning (Last letter and Coin flip), commonsense reasoning (CommonSenseQA~\citep{commonsenseqa} and Strategy QA~\citep{strategyqa}), and other logical reasoning tasks (Date understanding and Tracking Shuffled Objects from BIG-bench~\citep{bigbench}) without modifying the prompt per task.

We empirically evaluate \ours against other prompting baselines in~\autoref{tab:few_shot}. While our \ours underperforms \theirs with carefully-crafted and task-specific step-by-step examples, \ours achieves enormous score gains compared to the zero-shot baseline, e.g. from 17.7\% to 78.7\% on MultiArith and from 10.4\% to 40.7\% on GSM8K with large-scale InstructGPT model (text-davinci-002). 
We also evaluate \ours with another off-the-shelf large model, 540B parameter PaLM, showing similar magnitudes of improvements on MultiArith and GSM8K. 
Importantly, with our single fixed prompt, zero-shot LLMs have a significantly better scaling curve comparable to that of the few-shot CoT baseline. 
We also show that besides \theirs requiring human engineering of multi-step reasoning prompts, their performance deteriorates if prompt example question types and task question type are unmatched, suggesting high sensitivity to per-task prompt designs.   
In contrast, the versatility of this single prompt across diverse reasoning tasks hints at untapped and understudied \textit{zero-shot} fundamental capabilities of LLMs, such as higher-level broad cognitive capabilities like generic logical reasoning~\citep{chollet2019measure}. 
While the vibrant field of LLMs started out from the premise of excellent few-shot learners~\citep{brown2020language}, we hope our work encourages more research into uncovering \textit{high-level} and \textit{multi-task} zero-shot capabilities hidden inside those models. 





















\section{Background}   
We briefly review the two core preliminary concepts that form the basis of this work: the advent of large language models (LLMs) and prompting, and \CoT (CoT) prompting for multi-step reasoning.

\paragraph{Large language models and prompting}
A language model (LM), is a model that looks to estimate the probability distribution over text. Recently, scaling improvements through larger model sizes (from a few million~\citep{merity2016pointer} to hundreds of millions~\citep{bert} to hundreds of billions~\citep{brown2020language} parameters) and larger data (e.g. webtext corpora \citep{gao2020pile}) have enabled pre-trained large language models (LLMs) to be incredibly adept at many downstream NLP tasks. Besides the classic ``pre-train and fine-tune'' paradigm~\citep{liu2021pre}, models scaled to 100B+ parameters exhibit properties conducive to few-shot learning~\citep{brown2020language}, by way of in context learning, where one can use a text or template known as a \emph{prompt} to strongly guide the generation to output answers for desired tasks, thus beginning an era of ``pre-train and prompt''~\citep{liu2021makes}. In work, we call such prompts with explicit conditioning on few task examples as \textit{few-shot} prompts, and other template-only prompts as \textit{zero-shot} prompts. 

\paragraph{Chain of thought prompting}
Multi-step arithmetic and logical reasoning benchmarks have particularly challenged the scaling laws of large language models~\citep{gopher}. Chain of thought (CoT) prompting~\citep{cot_wei}, an instance of few-shot prompting, proposed a simple solution by modifying the answers in few-shot examples to step-by-step answers, and achieved significant boosts in performance across these difficult benchmarks, especially when combined with very large language models like PaLM~\citep{palm}. The top row of \autoref{fig_overview_1} shows standard few-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a given for tackling such difficult tasks, and the zero-shot baseline performances were not even reported in the original work~\citep{cot_wei}. To differentiate it from our method, we call \citet{cot_wei} as \textit{\theirs} in this work.   







\section{Zero-shot Chain of Thought}
\label{sec:proposal}



We propose \ours, a zero-shot template-based prompting for \CoT reasoning. 
It differs from the original \CoT prompting~\citep{cot_wei} as it does not require step-by-step few-shot examples, and it differs from most of the prior template prompting~\citep{liu2021pre} as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks with a single template. The core idea of our method is simple, as described in~\autoref{fig_overview_1}: add \textit{Let's think step by step}, or a a similar text (see~\autoref{tab:template_study}), to extract step-by-step reasoning.



\subsection{Two-stage prompting} 

While \ours is conceptually simple, it uses prompting twice to extract both reasoning and answer, as explained in~\autoref{fig_overview_2}. In contrast, the zero-shot baseline (see the bottom-left in~\autoref{fig_overview_1}) already uses prompting in the form of ``The answer is'', to extract the answers in correct formats. Few-shot prompting, standard or CoT, avoids needing such answer-extraction prompting by explicitly designing the few-shot example answers to end in such formats (see the top-right and top-left in~\autoref{fig_overview_1}). In summary, \theirs~\citep{cot_wei} requires careful human engineering of a few prompt examples with specific answer formats per task, while \ours requires less engineering but requires prompting LLMs twice.

\paragraph{1st prompt: reasoning extraction} In this step we first modify the input question  into a \textit{prompt}  using a simple template ``Q: \texttt{[X]}. A: \texttt{[T]}'', where \texttt{[X]} is an input slot for  and \texttt{[T]} is an slot for hand-crafted trigger sentence  that would extract chain of though to answer the question . 
For example, if we use ``Let's think step by step'' as a trigger sentence, the prompt  would be ``Q: \texttt{[X]}. A: Let's think step by step.''. 
See~\autoref{tab:template_study} for more trigger examples.
Prompted text  is then fed into a language model and generate subsequent sentence . 
We can use any decoding strategy, but we used greedy decoding throughout the paper for the simplicity. 




\paragraph{2nd prompt: answer extraction} 
In the second step, we use generated sentence  along with prompted sentence  to extract the final answer from the language model. 
To be concrete, we simply concatenate three elements as with ``\texttt{[X]} \texttt{[Z]} \texttt{[A]}'': \texttt{[X]} for 1st prompt , \texttt{[Z]} for sentence  generated at the first step, and \texttt{[A]} for a trigger sentence to extract answer. 
The prompt for this step is \textit{self-augmented}, since the prompt contains the sentence  generated by the same language model. In experiment, we use slightly different answer trigger depending on the answer format. 
For example, we use ``Therefore, among A through E, the answer is'' for multi-choice QA, and ``Therefore, the answer (arabic numerals) is'' for math problem requiring numerical answer. 
See Appendix \ref{appx:answer_prompts} for the lists of answer trigger sentences.
Finally, the language model is fed the prompted text as input to generate sentences  and parse the final answer.
See ``Answer Cleansing'' at \S \ref{sec:experiment} for the parser details.





\begin{figure}[t]
  \begin{center}
   \includegraphics[width=0.99\columnwidth]{fig_overview_2}
  \end{center}
  \caption{Full pipeline of \ours as described in \mysection ~\ref{sec:proposal}: we first use the first ``reasoning'' prompt  to extract a full reasoning path from a language model, and then use the second ``answer'' prompt to extract the answer in the correct format from the reasoning text.}
  \label{fig_overview_2}
\end{figure}



\section{Experiment}
\label{sec:experiment}







\paragraph{Tasks and datasets}

We evaluate our proposal on 12 datasets from four categories of reasoning tasks: arithmetic, commonsense, symbolic, and other logical reasoning tasks. 
See Appendix \ref{appx:dataset_description} for the detailed description of each datasets. 

For arithmetic reasoning, we consider the following six datasets: (1) SingleEq \citep{singleeq}, (2) AddSub \citep{addsub}, (3) MultiArith \citep{multiarith}, (4) AQUA-RAT \citep{aqua}, (5) GSM8K \citep{gsm8k}, and (6) SVAMP \citep{svamp}. 
The first three are from the classic Math World Problem Repository \citep{mawps}, and the last three are from more recent benchmarks. 
SingleEq and AddSub contain easier problems, which do not require multi-step calculation to solve the tasks. 
MultiArith, AQUA-RAT, GSM8k, and SVAMP are more challenging datasets that require multi-step reasoning to solve.



For commonsense reasoning, we use CommonsenseQA \citep{commonsenseqa} and StrategyQA \citep{strategyqa}.
CommonsenseQA asks questions with complex semantics that often require reasoning based on prior knowledge \citep{commonsenseqa}. 
StrategyQA requires models to infer an implicit multi-hop reasoning to answer questions \citep{strategyqa}. 

For symbolic reasoning, we use Last Letter Concatenation and Coin Flip \citep{cot_wei}. Last letter Concatenation asks the model to concatenate the last letters of each word. We used randomly selected four names for each sample. Coin Flip asks the model to answer whether a coin is still heads up after people either flip or do not flip the coin.
We created samples of four times flip or not flip trials. 
Although these tasks are easy for humans, LMs typically exhibit a flat scaling curve. 

For other logical reasoning tasks, we choose two evaluation sets from the BIG-bench effort \citep{bigbench}: Date Understanding \footnote{While prior work \citep{cot_wei} categorized Date Understanding task into Common Sense reasoning, our study categorized this task into logical reasoning because this task requires less prior knowledge and more logical reasoning between dates.} and Tracking Shuffled Objects. 
Date Understanding asks models to infer the date from a context. 
Tracking Shuffled Objects tests a model's ability to infer the final state of objects given its initial state and a sequence of object shuffling. We used a dataset of tracking three shuffled objects for our experiment.

\paragraph{Models}
We experiment with 17 models in total. Main experiments are conducted with Instruct-GPT3 \citep{instructgpt} (text-ada/babbage/curie/davinci-001 and text-davinci-002)\footnote{Our experiment for Instruct GPT-3 models includes both text-****-001 and text-davinci-002. Text-davinci-002 differs from text-****-001 in that they use different fine-tuning data depending on the date range collected from the APIs. Specifically, text-davinci-002 uses data up to Jun 2021, while text-****-001 uses data up to Oct 2019. (See \url{https://beta.openai.com/docs/engines/gpt-3})}, original GPT3 \citep{brown2020language} (ada, babbage, curie, and davinci)\footnote{Our experiments with GPT3 series are conducted by using OpenAI API between April-2022 and May-2022, except for No.10-16 in \autoref{tab:template_study} in Aug-2022.}, and PaLM \citep{palm} (8B, 62B, and 540B). 
In addition, we used GPT-2\citep{Radford2019LanguageMA}, GPT-Neo\citep{gpt-neo}, GPT-J\citep{gpt-j}, T0 \citep{sanh2022multitask}, and OPT \citep{zhang2022opt} for model scaling study. 
The size of LMs ranges from 0.3B to 540B. 
We include both standard (e.g. GPT-3 and OPT), and instruction following variants (e.g. Instruct-GPT3 and T0). 
See Appendix \ref{appx:model_description} for model description details.
Unless otherwise stated, we use text-davinci-002 throughout the experiments.

\paragraph{Baselines}
We compare our \ours mainly to standard \theirsz prompting to verify the effectiveness of its \CoT reasoning. For \theirsz experiments, similar answer prompts as \ours are used as default. See Appendix \ref{appx:answer_prompts} for detail.
To better evaluate the zero-shot ability of LLMs on reasoning tasks, we also compare our method to \theirsf and \theirs baselines from~\citep{cot_wei}, using the same in-context examples. 
Throughout the experiments, we use greedy decoding across all the methods. For the zero-shot approaches, the results are therefore deterministic. 
For the few-shot approaches, since the order of in-context examples could affect the results \citep{lu2021fantastically}, we run each experiment only once with a fixed seed across all methods and datasets, for fair comparisons with the zero-shot methods. 
\citet{cot_wei} showed that the order of examples did not cause large variance in CoT experiments. 



\paragraph{Answer cleansing} After the model outputs a text by answer extraction (see \mysection \ref{sec:proposal} and~\autoref{fig_overview_2}), our method picks up only the part of the answer text that first satisfies the answer format. 
For example, if the answer prompting outputs ``probably 375 and 376'' on arithmetic tasks, we extract the first number ``375'' and set it as the model prediction. 
In the case of multiple-choice, the first large letter we encounter is set as the prediction.
See Appendix \ref{appx:answer_cleansing} for more detail.
Standard \theirsz method follows the same idea. 
For \theirsf and \theirs methods, we follow~\citep{cot_wei_sc} and first extract the answer text after "The answer is " from the model output, and apply the same answer cleansing to parse the answer text. 
If ``The answer is'' is not found in the model output, we search from the back of the text and set the first text that satisfies the answer format as the prediction.


\subsection{Results}

\begin{table}[t]\centering
\footnotesize
\caption{Accuracy comparison of \ours with \theirsz on each tasks. 
The values on the left side of each task are the results of using answer extraction prompts depending on answer format as described at \mysection \ref{sec:proposal}.
The values on the right side are the result of additional experiment where standard answer prompt "The answer is" is used for answer extraction. See Appendix \ref{appx:answer_prompts} for detail setups.
}
\begin{tabular}{p{0.18\textwidth}p{0.10\textwidth}p{0.10\textwidth}p{0.10\textwidth}p{0.10\textwidth}p{0.10\textwidth}p{0.10\textwidth}}\toprule
&\multicolumn{6}{c}{Arithmetic} \\\cmidrule{2-7}
&SingleEq &AddSub &MultiArith &GSM8K &AQUA &SVAMP \\\midrule
zero-shot &74.6/\textbf{78.7} &\textbf{72.2}/\textbf{77.0} &17.7/22.7 &10.4/12.5 &22.4/22.4 &58.8/58.7 \\
\midrule
zero-shot-cot &\textbf{78.0}/\textbf{78.7} &69.6/74.7 &\textbf{78.7}/\textbf{79.3} &\textbf{40.7}/\textbf{40.5} &\textbf{33.5}/\textbf{31.9} &\textbf{62.1}/\textbf{63.7} \\
\toprule
\end{tabular}
\begin{tabular}{p{0.18\textwidth}p{0.10\textwidth}p{0.10\textwidth}p{0.10\textwidth}p{0.10\textwidth}p{0.10\textwidth}p{0.10\textwidth}}
&\multicolumn{2}{c}{Common Sense}& \multicolumn{2}{c}{Other Reasoning Tasks}&
\multicolumn{2}{c}{Symbolic Reasoning}
\\
\cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}&Common &Strategy &Date &Shuffled &\scalebox{0.91}{Last Letter} &\scalebox{0.91}{Coin Flip}\\
&SenseQA &QA &Understand &Objects &(4 words) &(4 times)\\
\midrule
zero-shot &\textbf{68.8}/\textbf{72.6} &12.7/\textbf{54.3} &49.3/33.6 &31.3/29.7 &0.2/- &12.8/53.8 \\
\midrule
zero-shot-cot &64.6/64.0 &\textbf{54.8}/52.3&\textbf{67.5}/\textbf{61.8} &\textbf{52.4}/\textbf{52.9} &\textbf{57.6}/- &\textbf{91.4}/\textbf{87.8} \\
\bottomrule
\end{tabular}

\label{tab:main_results}
\end{table} 
\paragraph{\ours vs. \theirsz}

~\autoref{tab:main_results} summarize accuracy of our method (\ours) and standard zero-shot prompting (\theirsz) for each dataset. 
\ours substantially outperforms four out of six arithmetic reasoning tasks (MultiArith, GSM8K, AQUA, SVAMP), all symbolic reasoning, and all other logical reasoning tasks (from BIG-bench~\citep{bigbench}).
For example, \ours achieves score gains from 17.7\% to 78.7\% on MultiArith and from 10.4\% to 40.7\% on GSM8K. 
Our method gives on-par performances for the remaining two arithmetic reasoning tasks (SingleEq and AddSub), which is expected since they do not require multi-step reasoning. 

In commonsense reasoning tasks, \ours does not provide performance gains. 
It is expected as \citet{cot_wei} also reports that even \theirs does not provide performance gains on Lambda (135B), but does improve StrategyQA when combined with substantially larger PaLM (540B) model, which may also apply for ours. 
More importantly, we observe that many generated chain of thought themselves are surprisingly \textit{logically} correct or only contains human-understandable mistakes (See \autoref{tab:example_main}), suggesting that \ours does elicit for better commonsense reasoning even when the task metrics do not directly reflect it. 
We provide samples generated by \ours for each dataset in \autoref{appx:further_experiment}. 




\begin{table}[t]\centering
\footnotesize
\caption{Comparison with baseline methods using accuracies on MultiArith and GSM8K. \davinci is used as the model if not specified. We used the same 8 examples as described in~\citep{cot_wei} for \theirsf and \theirs settings. (*1) To verify the variance of changing examples, we report two results for 4-shot-cot by splitting the eight examples into two groups. (*2) We insert ``Let's think step by step.'' at the beginning of answer part of each exemplars for \theirs to test performance gains. Further experiment results with PaLM are found at \autoref{appx:further_experiment_on_palm}}
\label{tab:few_shot}

\begin{tabular}{lrrr}\toprule
&MultiArith &GSM8K \\\midrule
\textbf{Zero-Shot} &\textbf{17.7} &\textbf{10.4} \\
Few-Shot (2 samples) &33.7 &15.6 \\
Few-Shot (8 samples) &33.8 &15.6 \\
\midrule
\textbf{Zero-Shot-CoT} &\textbf{78.7} &\textbf{40.7} \\
Few-Shot-CoT (2 samples) &84.8 &41.3 \\
Few-Shot-CoT (4 samples : First) (*1) &89.2 &- \\
Few-Shot-CoT (4 samples : Second) (*1) &90.5 &- \\
Few-Shot-CoT (8 samples) &93.0 &48.7 \\
\textbf{Zero-Plus-Few-Shot-CoT (8 samples)} (*2) &\textbf{92.8} &\textbf{51.5} \\
\midrule
Finetuned GPT-3 175B \citep{cot_wei} &- &33 \\
Finetuned GPT-3 175B + verifier \citep{cot_wei} &- &55 \\
\midrule
\textbf{PaLM 540B: Zero-Shot} & \textbf{25.5} & \textbf{12.5} \\
\textbf{PaLM 540B: Zero-Shot-CoT} & \textbf{66.1} & \textbf{43.0} \\
\textbf{PaLM 540B: Zero-Shot-CoT + self consistency} & \textbf{89.0} & \textbf{70.1} \\
PaLM 540B: Few-Shot \citep{cot_wei} &- &17.9\\
PaLM 540B: Few-Shot-CoT \citep{cot_wei} &- &56.9\\
PaLM 540B: Few-Shot-CoT + self consistency \citep{cot_wei_sc} &- &74.4\\
\bottomrule
\end{tabular}
\captionsetup{skip=5pt}

\end{table} \begin{figure}[t]
	\begin{tabular}{ccc}
	\begin{minipage}{.33\textwidth}
	\centering
		\includegraphics[width=4.5cm]{fig_model_scale_origgpt}
		\subcaption{MultiArith on Original GPT-3}
	\end{minipage}
		\begin{minipage}{.33\textwidth}
		\centering
		\includegraphics[width=4.5cm]{fig_model_scale_instructgpt}
		\subcaption{MultiArith on Instruct GPT-3}
	\end{minipage}
		\begin{minipage}{.33\textwidth}
		\centering
		\includegraphics[width=4.5cm]{fig_model_scale_palm}
		\subcaption{GMS8K on PaLM}
	\end{minipage}
	\end{tabular}
	\caption{Model scale study with various types of models. S: text-ada-001, M: text-babbage-001, L: text-curie-001, XL: text-davinci-002. See Appendix \ref{appx:model_description} and \ref{appx:detail_model_scale} for the detail.}
	\label{fig:model_size}
\end{figure} 

\paragraph{Comparison with other baselines}

\autoref{tab:few_shot} compares the performances on two arithmetic reasoning benchmarks (MultiArith and GSM8K) across \ours and baselines. 
The large gap between standard prompting (1st block) and chain of thought prompting (2nd block) suggests that these tasks are difficult without eliciting multi-step reasoning. 
Major improvements are confirmed on both Instruct GPT-3 (text-davinci-002) and PaLM (540B) models (4th block). 
While \ours naturally underperforms \theirs, it substantially outperforms standard \theirsf prompting with even 8 examples per task. 
For GSM8K, \ours with Instruct GPT-3 (text-davinci-002) also outperforms finetuned GPT-3 and standard few-shot prompting with large models (PaLM, 540B), reported in \citet{cot_wei} (3rd and 4th block). 
See App. \ref{appx:further_experiment_on_palm} for more experiment results with PaLM.

\paragraph{Does model size matter for zero-shot reasoning?}

\autoref{fig:model_size} compares performance of various language models on MultiArith / GSM8K. 
Without \CoT reasoning, the performance does not increase or increases slowly as the model scale is increased, i.e., the curve is mostly flat. 
In contrast, the performance drastically increases with \CoT reasoning, as the model size gets bigger, for Original/Instruct GPT-3 and PaLM. When the model size is smaller, \CoT reasoning is not effective. 
This result aligns with the few-shot experiment results in \cite{cot_wei}.
Appendix \ref{appx:detail_model_scale} shows extensive experiment results using wider variety of language models, including GPT-2, GPT-Neo, GPT-J, T0, and OPT.
We also manually investigated the quality of generated \CoT, and large-scale models clearly demonstrate better reasoning (See Appendix \ref{appx:further_experiment} for the sampled outputs for each model).

\paragraph{Error Analysis}

\begin{table}[t]\centering
\footnotesize
\caption{Examples generated by Zero-Shot-CoT on CommonsenseQA for Error Analysis.}
\begin{tabular}{p{0.45\textwidth}|p{0.48\textwidth}}
\toprule

Example 1 \bred{(Reasoning is correct.)} &
Example 2 \bred{(Model cannot choose only one.)}
\\

\midrule

\bblack{Q:} Where is a well used toy car likely to be found? Answer Choices: (A) child's room (B) boy's bedroom (C) own home (D) toy store (E) house
&
\bblack{Q:} What would be necessary for getting in shape? Answer Choices: (A) good health (B) exercise (C) muscle tone (D) sweat (E) feel better

\\



\bblack{A:} \bblack{Let's think step by step.} A toy car is likely to be found in a child's room. \bred{A child's room is likely to be found in a house.} Therefore, a toy car is likely to be found in a house.

\bblack{Therefore, among A through E, the answer is} E. \ngmark \myspace \bblue{(GT : A, \theirsz : A)}
&
\bblack{A:} \bblack{Let's think step by step.} In order to get in shape, you need to be able to do three things: 

1. Exercise regularly

2. Eat a healthy diet

3. Get enough sleep

If you can do all three of those things, then you will be well on your way to getting in shape! 
\bblack{Therefore, among A through E, the answer is} \bred{B, C, and D.} \bblue{(GT : B, \theirsz : B)}
\\
\bottomrule
\end{tabular}

\label{tab:example_main}
\end{table} 
To better understand the behavior of \ours, we manually investigated randomly selected examples generated by Instruct-GPT3 with \ours prompting. 
See~\autoref{appx:error_analysis} for examples, where some of the observations include: (1) In commonsense reasoning (CommonsenseQA), \ours often produces flexible and reasonable \CoT even when the final prediction is not correct. 
\ours often output multiple answer choices when the model find it is difficult to narrow it down to one (see~\autoref{tab:example_main} for examples). (2) In arithmetic reasoning (MultiArith), \ours and \theirs show substantial differences regarding the error patterns.
First, \ours tends to output unnecessary steps of reasoning after getting the correct prediction, which results in changing the prediction to incorrect one. 
\ours also sometimes does not start reasoning, just rephrasing the input question. 
In contrast, \theirs tend to fail when generated \CoT include ternary operation, e.g. . 





\begin{table}[t]\centering
\caption{Robustness study against template measured on the MultiArith dataset with \davinci. (*1) This template is used in \cite{cando} where a language model is prompted to generate step-by-step actions given a high-level instruction for controlling robotic actions. (*2) This template is used in \cite{prompt1} but is not quantitatively evaluated.}
\begin{tabular}{lllr}\toprule
No.&Category&Template&Accuracy \\\midrule
1&instructive&Let's think step by step. &\textbf{78.7} \\
2&&First, (*1) &77.3 \\
3&&Let's think about this logically. &74.5 \\
4&&Let's solve this problem by splitting it into steps. (*2) & 72.2\\
5&&Let's be realistic and think step by step. & 70.8\\
6&&Let's think like a detective step by step. & 70.3\\
7&&Let's think & 57.5\\
8&&Before we dive into the answer, & 55.7\\
9&&The answer is after the proof. & 45.7\\

\midrule

10&misleading&Don't think. Just feel. &18.8 \\
11&&Let's think step by step but reach an incorrect answer. &18.7 \\
12&&Let's count the number of "a" in the question. &16.7 \\
13&&By using the fact that the earth is round, &9.3 \\

\midrule

14&irrelevant&By the way, I found a good restaurant nearby. &17.5 \\
15&&Abrakadabra! &15.5 \\
16&&It's a beautiful day. &13.1 \\

\midrule
-&&(\theirsz) & 17.7\\
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\label{tab:template_study}
\end{table} \begin{table}[t]\centering
\footnotesize
\caption{Robustness study of \theirs against examples. When the examples are from entirely different tasks, the performance generally becomes worse, but when the answer formats are matched (i.e. CommonsenseQA to AQUA-RAT, multiple-choice), the performance loss is less severe. CommonsenseQA samples are used in this variation}
\begin{tabular}{p{0.15\textwidth}p{0.15\textwidth}p{0.20\textwidth}p{0.15\textwidth}p{0.18\textwidth}}
\toprule
&\theirsz &\theirs &\ours &\theirs \\\midrule
AQUA-RAT &22.4 &\underline{31.9} &33.5 &39.0 \\
MultiArith &17.7 &\underline{27.0} &78.7 &88.2 \\
\bottomrule
\end{tabular}
\captionsetup{skip=5pt}

\label{tab:robustness_against_examples}
\end{table} 
\paragraph{How does prompt selection affect \ours?}
We validate the robustness of \ours against input prompts. \autoref{tab:template_study} summarizes performance using 16 different templates with three categories. Specifically, following \citet{websonpavlick2022prompt}, the categories include instructive (encourage reasoning), misleading (discourage reasoning or encouraging reasoning but in a wrong way), and irrelevant (nothing to do with reasoning). The results indicate that the performance is improved if the text is written in a way that encourages \CoT reasoning, i.e., the templates are within "instructive" category. However, the difference in accuracy is significant depending on the sentence. In this experiment, "Letâ€™s think step by step." achieves the best results. Interestingly, it is found that different templates encourage the model to express reasoning quite differently (see \autoref{appx:further_experiment} for sample outputs by each template). In contrast, when we use misleading or irrelevant templates, the performance does not improve. It remains an open question how to automatically create better templates for \ours.


\paragraph{How does prompt selection affect \theirs?}
~\autoref{tab:robustness_against_examples} shows the performance of \theirs when using examples from different datasets: CommonsenseQA to AQUA-RAT and CommonsenseQA to MultiArith. 
The domains are different in both cases, but the answer format is the same in the former. 
Surprisingly, the \CoT examples from different domains (common sense to arithmetic) but with the same answer (multiple-choice) format provide substantial performance gain over \theirsz (to AQUA-RAT), measured relative to the possible improvements from \ours or \theirs. In contrast, the performance gain becomes much less when using examples with different answer types (to MultiArith), confirming prior work \citep{min2022rethinking} that suggests LLMs mostly leverage the few-shot examples to infer the repeated format rather than the task itself in-context. 
Nevertheless, for both cases the results are worse than \ours, affirming the importance of task-specific sample engineering in \theirs.





\section{Discussion and Related Work}

\begin{table}[H]\centering
\footnotesize
\caption{Summary of related work on arithmetic/commonsense reasoning tasks. Category denotes the training strategy. CoT denotes whether to output chain of thought. Task column lists the tasks that are performed in corresponding papers. AR: Arithmetic Reasoning, CR: Commonsense Reasoning.}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcll}
\toprule

Method &Category &CoT &Task &Model \\\midrule
\cite{yourself} &Fine-Tuning & &CR &GPT \\
\cite{gsm8k} &Fine-Tuning & &AR &GPT-3 \\
\cite{star} &Fine-Tuning & &AR,CR &GPT-3, etc \\
\cite{scratchpad} &Fine-Tuning\tablefootnote{\cite{scratchpad} also evaluates few-shot settings, but the few-shot performances on their domains are worse than the fine-tuning results.} & &AR &Transformer(Decoder) \\

\midrule

\cite{brown2020language} &Few/Zero-Shot & &CR &GPT-3 \\
\cite{megatron} &Few/Zero-Shot & &AR,CR &MT-NLG \\
\cite{gopher} &Few-Shot & &AR,CR &Gopher \\

\midrule

\cite{cot_wei} &Few-Shot & &AR,CR &PaLM, LaMBDA, GPT-3\\
\cite{cot_wei_sc} &Few-Shot & &AR,CR &PaLM, etc \\
\cite{palm} &Few-Shot & &AR,CR &PaLM \\

\midrule

\cite{selftalk} &Zero-Shot & &CR &GPT-2, etc \\
\cite{prompt1} &Zero-Shot & &AR &GPT-3 \\
\ours (Ours) &Zero-Shot & &AR,CR &PaLM, Instruct-GPT3, GPT-3, etc \\
\bottomrule
\end{tabular}}








\label{tab:related_work}
\end{table}


 
\paragraph{Reasoning Ability of LLMs} \looseness=-1
Several studies have shown that pre-trained models usually are not good at reasoning~\citep{brown2020language,megatron,gopher}, but its ability can be substantially increased by making them produce step-by-step reasoning, either by fine-tuning~\citep{yourself,gsm8k,star,scratchpad} or few-shot prompting~\citep{cot_wei,cot_wei_sc,palm} (See \autoref{tab:related_work} for summary of related work).
Unlike most prior work, we focus on zero-shot prompting and
show that a single fixed trigger prompt substantially increases the zero-shot reasoning ability of LLMs across a variety of tasks requiring complex multi-hop thinking (\autoref{tab:main_results}), especially when the model is scaled up (\autoref{fig:model_size}). 
It also generates reasonable and understandable \CoT across diverse tasks (\autoref{appx:further_experiment}), even when the final prediction is wrong (\autoref{appx:error_analysis}). 
Similar to our work, \citet{prompt1} demonstrate a prompt, ``Let's solve this problem by splitting it into steps'', would facilitate the multi-step reasoning in a simple arithmetic problem. However, they treated it as a task-specific example and did not evaluate quantitatively on diverse reasoning tasks against baselines. 
\citet{selftalk} propose to decompose a commonsense question into a series of information seeking question, such as ``what is the definition of \texttt{[X]}''. It does not require demonstrations but requires substantial manual prompt engineering per each reasoning task. 
Our results strongly suggest that LLMs are decent zero-shot reasoners, while prior work~\citep{cot_wei} often emphasize only few-shot learning and task-specific in-context learning, e.g. no zero-shot baselines were reported. 
Our method does not require time-consuming fine-tuning or expensive sample engineering, and can be combined with any pre-trained LLM, serving as the strongest zero-shot baseline for all reasoning tasks. 

\paragraph{Zero-shot Abilities of LLMs} \looseness=-1 \citet{Radford2019LanguageMA} show that LLMs have excellent zero-shot abilities in many \textit{system-1} tasks, including reading comprehension, translation, and summarization. 
\citet{sanh2022multitask,instructgpt} show that such zero-shot abilities of LLMs can be increased by explicitly fine-tuning models to follow instructions. 
Although these work focus on the zero-shot performances of LLMs, we focus on many \textit{system-2} tasks beyond \textit{system-1} tasks, considered a grand challenge for LLMs given flat scaling curves. 
In addition, \ours is orthogonal to instruction tuning; it increases zero-shot performance for Instruct GPT3, vanilla GPT3, and PaLM (See \autoref{fig:model_size}). 



\paragraph{From Narrow (task-specific) to Broad (multi-task) Prompting}
Most prompts are task-specific. While few-shot prompts are naturally so due to task-specific in-context samples~\citep{brown2020language,cot_wei}, majority of zero-shot prompts have also focused on per-task engineering (of templates)~\citep{liu2021pre,prompt1}. Borrowing terminologies from ~\citet{chollet2019measure} which builds on hierarchical models of intelligence~\citep{mcgrew2005cattell,johnson2005structure}, these prompts are arguably eliciting ``narrow generalization'' or task-specific skills from LLMs. On the other hand, our method is a \textit{multi-task} prompt and elicits ``broad generalization'' or broad cognitive abilities in LLMs, such as logical reasoning or \textit{system-2} itself. We hope our work can serve as a reference for accelerating not just logical reasoning research with LLMs, but also discovery of other broad cognitive capabilities within LLMs. 



\paragraph{Training Dataset Details}
\label{appx:limitation_dataset}
A limitation of the work is the lack of public information on the details of training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs InstructGPT~\citep{instructgpt}, and data for PaLM models~\citep{palm}. However, big performance increases from \theirsz to \ours in all recent large models (InstructGPT 001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and non-arithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a task-agnostic multi-step reasoning capability for generic problem solving. While most results are based on InstructGPT since it is the best performing open-access LLM, key results are reproduced on PaLM, and dataset details in InstructGPT (Appendix A, B, and F in~\citet{instructgpt}) also confirm that it is not specially engineered for multi-step reasoning. 

\paragraph{Limitation and Social Impact}
 Our work is based on prompting methods for large language models. LLMs have been trained on large corpora from various sources on the web
 (also see ``Training Dataset Details''),
 and have shown to capture and amplify biases found in the training data. Prompting is a method that looks to take advantage of the patterns captured by language models conducive to various tasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way to probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context learning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.





































 
\section{Conclusion}
We have proposed \ours,  a single zero-shot prompt that elicits \CoT from large language models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach in previous work that requires hand-crafting few-shot examples per task. 
Our simple method not only is the minimalist and strongest zero-shot baseline for difficult multi-step \textit{system-2} reasoning tasks that long evaded the scaling laws of LLMs, but also encourages the community to further discover similar \textit{multi-task} prompts that elicit broad cognitive abilities instead of narrow task-specific skills.




\section*{Acknowledgements}
This work has been supported by the Mohammed bin Salman Center for Future Science and Technology for Saudi-Japan Vision 2030 at The University of Tokyo (MbSC2030).
Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used for experiments other than PaLM. We also thank Jason Wei and Denny Zhou for discussions and support on running PaLM experiments, and Sharan Narang and Aakanksha Chowdhery for generic support on PaLM infrastructures.

\bibliographystyle{plainnat}
\bibliography{neurips_2022}

\section*{Checklist}

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
        \item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{}
        \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerNo{Our paper mainly used GPT-3 API with greedy decoding, and there are no randomness for the experiments. }
        \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{}
\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{}
  \item Did you mention the license of the assets?
    \answerYes{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerYes{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerYes{}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}


\end{enumerate}




\clearpage
\appendix

\section{Details of Experimental Setup}
\label{appx:experiment_setup}

\subsection{Code}

Code is available at
\url{https://github.com/kojima-takeshi188/zero_shot_cot}.


\subsection{Datasets}
\label{appx:dataset_description}

\subsubsection{Dataset Description}

\autoref{tab:dataset_description} summarizes the description of each dataset used in our experiment.


\begin{table}[h]
\centering
\footnotesize
\caption{Dataset Description. Our experiments used publicly available datasets except for ``Last Letters'' and ``Coin Flip'' datasets. We created these two datasets. See Appendix \ref{appx:dataset_creation} for the details. (*1) N : Number, M : Pick up one from multiple choices, Y : Answer Yes or No, F : Free Format. (*2) Average number of words in questions texts.}
\label{tab:dataset_description}
\begin{tabular}
{p{0.20\textwidth}p{0.08\textwidth}p{0.07\textwidth}p{0.07\textwidth}p{0.24\textwidth}p{0.13\textwidth}}
\toprule
Dataset &Answer Format (*1) &\# of \par samples &Avg \# \par words \par (*2) &Data split (filename) \par used for our experiment &License \\\midrule
\href{https://gitlab.cs.washington.edu/ALGES/TACL2015}{SingleEq} &N &508 &27.4 &questions.json &No License \\
\href{https://github.com/wangxr14/Algebraic-Word-Problem-Solver}{AddSub} &N &395 &31.5 &AddSub.json &Unspecified \\
\href{https://github.com/wangxr14/Algebraic-Word-Problem-Solver}{MultiArith} &N &600 &31.8 &MultiArith.json &Unspecified \\
\href{https://github.com/openai/grade-school-math}{GSM8K} &N &1319 &46.9 &test.jsonl &MIT License \\
\href{https://github.com/deepmind/AQuA}{AQUA-RAT} &M &254 &51.9 &test.jsonl &Apache-2.0 \\
\href{https://github.com/arkilpatel/SVAMP}{SVAMP} &N &1000 &31.8 &SVAMP.json &MIT License \\
\href{https://github.com/jonathanherzig/commonsenseqa}{CommonsenseQA} &M &1221 &27.8 &dev\_rand\_split.jsonl &Unspecified \\
\href{https://github.com/google/BIG-bench/tree/main/bigbench/benchmark\_tasks/strategyqa}{StrategyQA} &Y &2290 &9.6 &task.json &Apache-2.0 \\
\href{https://github.com/google/BIG-bench/tree/main/bigbench/benchmark\_tasks/date\_understanding}{Date Understanding} &M &369 &35.0 &task.json &Apache-2.0 \\
\href{https://github.com/google/BIG-bench/tree/main/bigbench/benchmark\_tasks/tracking\_shuffled\_objects}{Shuffled Objects} &M &750 &91.1 &three\_objects/task.json &Apache-2.0 \\
Last Letters &F &500 &15.0 &- &- \\
Coin Flip &Y &500 &37.0 &- &- \\
\bottomrule
\end{tabular}
\end{table} 
\subsubsection{Dataset creation}
\label{appx:dataset_creation}

Regarding ``Last Letter Concatenation'' and ``Coin Flip'', datasets are not publicly available so we created the datasets following \cite{cot_wei} with a minor rephrasing of the question template. Specifically, as for Last Letter Concatenation, we use the following template. We randomly select human names from names-dataset library (\url{https://pypi.org/project/names-dataset/}) and insert them into \{Name1\} through \{Name4\}.

\begin{itemize}
    \item 'Take the last letters of each words in "\{Name1\} \{Name2\} \{Name3\} \{Name4\}" and concatenate them.'
\end{itemize}

As for Coin Flip, we use the following template. We randomly select human names from names-dataset library and insert them into \{Name1\} through \{Name4\}. We also randomly pick up ``flips'' or ``does not flip'' and insert the phrase into each \{flips | does not flip\} part, respectively.

\begin{itemize}
    \item 'A coin is heads up. \{Name1\} \{flips | does not flip\} the coin. \{Name2\} \{flips | does not flip\} the coin. \{Name3\} \{flips | does not flip\} the coin. \{Name4\} \{flips | does not flip\} the coin. Is the coin still heads up? Note that "flip" here means "reverse".'
\end{itemize}

\subsection{Language Models}
\label{appx:model_description}

Our experiment uses multiple language models as described at \autoref{tab:model_description}

\begin{table}[h]
\centering
\footnotesize
\caption{Description of language models. (*1) As for Original GPT3 models, we assign model size information to each model by referring to \url{https://blog.eleuther.ai/gpt3-model-sizes/}
and \url{https://beta.openai.com/docs/model-index-for-researchers}. 
(*2) There is no official information about the model size of Instruct GPT3. We infer from the API name that the order of model size of Instruct GPT3 matches that of Original GPT3.}
\label{tab:model_description}
\begin{tabular}{lllll}\toprule
Language Model &\# of params &Library / API Name &Model Name in \par Library / API &License \\\midrule \midrule

PaLM &540B &- &- &unspecified \\
PaLM &62B &- &- &unspecified \\
PaLM &8B &- &- &unspecified \\

\midrule

Original GPT3 &175B (*1) &OpenAI API &davinci &unspecified \\
Original GPT3 &6.7B (*1) &OpenAI API &curie &unspecified \\
Original GPT3 &1.3B (*1) &OpenAI API &babbage &unspecified \\
Original GPT3 &0.3B (*1) &OpenAI API &ada &unspecified \\

\midrule

Instruct GPT3 &- (*2) &OpenAI API &text-davinci-002 &unspecified \\
Instruct GPT3 &- (*2) &OpenAI API &text-davinci-001 &unspecified \\
Instruct GPT3 &- (*2) &OpenAI API &text-curie-001 &unspecified \\
Instruct GPT3 &- (*2) &OpenAI API &text-babbage-001 &unspecified \\
Instruct GPT3 &- (*2) &OpenAI API &text-ada-001 &unspecified \\

\midrule

OPT &13B &Hugging Face Library &opt-13b &Apache-2.0 \\
T0 &11B &Hugging Face Library &T0pp &Apache-2.0 \\
GPT-J &6B &Hugging Face Library &gptj &Apache-2.0 \\
GPT-Neo &2.7B &Hugging Face Library &gpt-neo &Apache-2.0 \\
GPT-2 &1.5B &Hugging Face Library &gpt2-xl &Apache-2.0 \\
\bottomrule
\end{tabular}
\end{table} 

\subsection{Implementation details}
For Original GPT-3 and Instruct-GPT3, we used OpenAI API. 
For OPT, T0, GPT-J, GPT-Neo, and GPT-2, we used Hugging Face Transformer Library \citep{huggingface}.
We set max\_tokens = 128 and used greedy decoding (temperature = 0 in the case of OpenAI API) across all the methods and models except PaLM.
For PaLM, we used `TopK=1' for greedy deterministic decoding and max\_tokens = 256.
``Q:'' is set as a customized stop sequence for all the models except for Instruct-GPT3 to stop the models from repeating questions and answers by themselves.
We run our experiments on cloud V100 instances without GPU for GPT-3 models, on cloud A100x8 GPU(60GB) instances for T0 and OTP, and on cloud A100x1 GPU(60GB) instances for GPT-J, GPT-Neo, and GPT-2. Our implementation is in PyTorch~\citep{paszke2019pytorch}.

\subsection{Prompts For Answer Extraction}
\label{appx:answer_prompts}

\autoref{tab:answer_prompts_1} and \autoref{tab:answer_prompts_2} summarizes a list of answer extraction prompts used for the experiments at \autoref{tab:main_results}. 
We used \theirsz (left) and \ours (left) as default prompts for answer extraction across all the experiments.

\begin{table}[h]\centering
\caption{Answer extraction prompts used for \theirsz experiments in ~\autoref{tab:main_results}. C.S.QA : CommonsenseQA, D.U. : Date Understanding, S.O. : Tracking Shuffled Objects}
\label{tab:answer_prompts_1}
\begin{tabular}{p{0.02\textwidth}p{0.15\textwidth}p{0.40\textwidth}p{0.25\textwidth}}
\toprule
No &Task &Zero-Shot (left) &Zero-Shot (right) \\

\midrule

1 &SingleEq &The answer (arabic numerals) is &The answer is \\
2 &AddSub &The answer (arabic numerals) is &The answer is \\
3 &MultiArith &The answer (arabic numerals) is &The answer is \\
4 &GSM8K &The answer (arabic numerals) is &The answer is \\
5 &AQUA-RAT &Among A through E, the answer is &The answer is \\
6 &SVAMP &The answer (arabic numerals) is &The answer is \\
7 &C.S.QA &Among A through E, the answer is &The answer is \\
8 &StrategyQA &The answer (Yes or No) is &The answer is \\
9 &D.U. &Among A through F, the answer is &The answer is \\
10 &S.O. &Among A through C, the answer is &The answer is \\
11 &Last Letters &The answer is &The answer is \\
12 &Coin Flip &The answer (Yes or No) is &The answer is \\
\bottomrule
\end{tabular}
\end{table}



\begin{table}[h]\centering
\caption{Answer extraction prompts used for \ours experiments in ~\autoref{tab:main_results}. C.S.QA : CommonsenseQA, D.U. : Date Understanding, S.O. : Tracking Shuffled Objects}
\label{tab:answer_prompts_2}
\begin{tabular}{p{0.02\textwidth}p{0.13\textwidth}p{0.45\textwidth}p{0.25\textwidth}}
\toprule

No &Task &Zero-Shot-CoT (left) &Zero-Shot-CoT (right) \\

\midrule

1 &SingleEq &Therefore, the answer (arabic numerals) is &Therefore, the answer is \\
2 &AddSub &Therefore, the answer (arabic numerals) is &Therefore, the answer is \\
3 &MultiArith &Therefore, the answer (arabic numerals) is &Therefore, the answer is \\
4 &GSM8K &Therefore, the answer (arabic numerals) is &Therefore, the answer is \\
5 &AQUA-RAT &Therefore, among A through E, the answer is &Therefore, the answer is \\
6 &SVAMP &Therefore, the answer (arabic numerals) is &Therefore, the answer is \\
7 &C.S.QA &Therefore, among A through E, the answer is &Therefore, the answer is \\
8 &StrategyQA &Therefore, the answer (Yes or No) is &Therefore, the answer is \\
9 &D.U. &Therefore, among A through F, the answer is &Therefore, the answer is \\
10 &S.O. &Therefore, among A through C, the answer is &Therefore, the answer is \\
11 &Last Letters &Therefore, the answer is &Therefore, the answer is \\
12 &Coin Flip &Therefore, the answer (Yes or No) is &Therefore, the answer is \\
\bottomrule
\end{tabular}
\end{table} 
\subsection{Answer Cleansing}
\label{appx:answer_cleansing}

\autoref{tab:answer_cleansing} summarizes a list of answer cleansing approaches used across all the experiments.

\lstset{
language = Python,
aboveskip=-7pt,
belowskip=-5pt,
backgroundcolor={\color[gray]{.90}},
breaklines = true,
breakindent = 10pt,
basicstyle = \ttfamily\scriptsize,
commentstyle = {\itshape \color[cmyk]{1,0.4,1,0}},
classoffset = 0,
keywordstyle = {\bfseries \color[cmyk]{0,1,0,0}},
stringstyle = {\ttfamily \color[rgb]{0,0,1}},
tabsize = 4,
captionpos = t
}

\begin{table}[h]\centering
\caption{Detail description of answer cleansing. See \autoref{tab:dataset_description} for the mapping between each datasets and the corresponding answer formats.}
\label{tab:answer_cleansing}
\begin{tabular}{p{0.15\textwidth}p{0.25\textwidth}p{0.40\textwidth}}
\toprule
Answer \par Format &Answer Cleansing \par Approach &Pseudo Code \par (Example in Pytorch 3.8) \\\midrule \midrule
Number &Pick up the first number encountered in the text. &
\begin{lstlisting}
pred = pred.replace(",", "")
pred = [s for s in re.findall(r'-?\d+\.?\d*', pred)]
pred = pred[0] 
\end{lstlisting}

\\
\midrule

Multiple-Choice &Pick up the first large letter encountered in the text. &
\begin{lstlisting}
pred = re.findall(r'A|B|C|D|E', pred) 
pred = pred[0]
\end{lstlisting}

\\\midrule

Yes or No &Pick up the first "yes" or "no" encountered in the text after removing unnecessary letters. &
\begin{lstlisting}
pred = pred.lower()
pred = re.sub("\"|\'|\n|\.|\s|\:|\,"," ", pred) 
pred = pred.split(" ") 
pred = [i for i in pred if i in ("yes", "no")] 
pred = pred[0]
\end{lstlisting}

\\\midrule

Free Format &Just remove unnecessary letters, such as ".". &
\begin{lstlisting}
pred = re.sub("\"|\'|\n|\.|\s","", pred)
\end{lstlisting}

\\

\bottomrule
\end{tabular}
\end{table} 
\section{Additional Experiment Results}
\label{appx:further_experiment}

This section summarizes more example texts generated by models in our experiments.
Note that for readability all texts are modified from the original ones by omitting or inserting some linebreaks. Without mentioning otherwise, we use Instruct-GPT3 (text-davinci-002) model.

\begin{itemize}
    \item \autoref{tab:example_table_dataset} lists example texts generated by \ours for each dataset (See \autoref{tab:main_results}).
    \item \autoref{tab:example_table_templates} lists example texts generated by \ours for each reasoning extraction template (See \autoref{tab:template_study}).
    \item \autoref{tab:example_table_model_size_1} and \autoref{tab:example_table_model_size_2} lists example texts generated by \ours for each langugage model (See \autoref{tab:model_size}).
    \item \autoref{tab:example_table_fewshot} has an example text generated by \theirsf.
    \item \autoref{tab:example_table_fewshot_cot} has an example text generated by \theirs.
    \item \autoref{tab:example_table_fewshot_cot_diff_task} has an example text generated by \theirs with exemplars from a different task (Exemplars from CommonsenseQA, and a task is from MultiArith).
    \item \autoref{tab:example_table_zeroplusfewshot_cot} has an example text generated by Zero-Plus-Few-Shot-CoT.
    \item \autoref{tab:example_table_palm} compares different outcome scenarios on results generated by \theirsz and \ours using PaLM (540B) model.
\end{itemize}



\begin{center}
\begin{longtable}{|p{0.15\textwidth}|p{0.3\textwidth}|p{0.5\textwidth}|}
\caption{Example outputs by \ours for each datasets.}
\label{tab:example_table_dataset}\\

\toprule \multicolumn{1}{|c|}{\textbf{Dataset}} & \multicolumn{1}{c|}{\textbf{Question}} & \multicolumn{1}{c|}{\textbf{Answer}} \\ \midrule 
\endfirsthead

\multicolumn{3}{c}{{\tablename\ \thetable{} -- Continued from previous page}} \\
\toprule \multicolumn{1}{|c|}{\textbf{Dataset}} & \multicolumn{1}{c|}{\textbf{Question}} & \multicolumn{1}{c|}{\textbf{Answer}} \\ \midrule 
\endhead

\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \bottomrule
\endfoot

\bottomrule
\endlastfoot

SingleEq &Q: A spaceship traveled 0.5 of a light-year from Earth to Planet X and 0.1 of a light-year from Planet X to Planet Y. Then it traveled 0.1 of a light-year from Planet Y back to Earth. How many light-years did the spaceship travel in all? &A: Let's think step by step. The spaceship traveled 0.5 light-years from Earth to Planet X. Then it traveled 0.1 light-years from Planet X to Planet Y. Finally, it traveled 0.1 light-years from Planet Y back to Earth. So the total distance the spaceship traveled is 0.5 + 0.1 + 0.1 = 0.7 light-years. Therefore, the answer (arabic numerals) is: 0.7 light-years \okmark \\

\cmidrule(r){2-3}

&Q: While making desserts for a bake sale, Victor used 0.625 of a scoop of brown sugar as well as 0.25 of a scoop of white sugar. How much more brown sugar did Victor use? &A: Let's think step by step. 0.625 of a scoop of brown sugar is the same as 0.25 of a scoop of white sugar. This means that Victor used the same amount of brown sugar as he did white sugar. Therefore, the answer (arabic numerals) is 0. \ngmark \myspace \bblue{(GT : 0.375)}\\

\midrule

AddSub &Sandy grew 6 carrots . Sam grew 3 carrots . How many carrots did they grow in total ? &A: Let's think step by step. Sandy grew 6 carrots. Sam grew 3 carrots. That means they grew a total of 9 carrots. Therefore, the answer (arabic numerals) is 9. \okmark \\

\cmidrule(r){2-3}

&Q: Melanie bought a Batman game for \ 7.90 , and a Superman game for \ 6.95. So she spent \ 7.90. So she spent \ 7.73. So she spent \ 18.58 on video games. Therefore, Melanie spent a total of \150,000. He pays \150,000. He pays \60,000 for the year. He spends twice as much as that on fuel per month. This costs him \150,000 + \120,000 = \\frac{32}{46} \times 100\% = | \par \verb|\fbox{70}\%68\times 6 = 40823. She bought five bagels for 23. She bought five bagels for 23. She bought five bagels for 68 per dozen, 2 dozen mini cupcakes which cost \55 per dozen. How much was the total cost?
&A: Let's think step by step. First, we need to find the cost of the donuts. 3 dozen donuts cost \68. 3 times \204. So, the cost of the donuts is \80 per dozen. So, 2 dozen mini cupcakes cost 2 times \80 is \160. Finally, we need to find the cost of the mini cheesecakes. 6 dozen mini cheesecakes cost \55. 6 times \330. So, the cost of the mini cheesecakes is \204 plus \330 is \694. Therefore, the answer (arabic numerals) is 694. \okmark \\

\cmidrule(r){3-3}

& & A: The answer (arabic numerals) is 1000. \ngmark \myspace \bblue{(GT : 694)} \\

\midrule

 \ours\ngmark, \theirsz~\okmark~(4.8\% of dataset\footnote{Some of these errors are due to PaLM model's instability, where simply adding `Let's think step by step' causes the model to continuously output newline tokens.}) & Q: Marissa is hiking a 12-mile trail. She took 1 hour to walk the first 4 miles, then another hour to walk the next two miles. If she wants her average speed to be 4 miles per hour, what speed (in miles per hour) does she need to walk the remaining distance? & A: Let's think step by step. First, we know that she walked 4 miles in 1 hour, so her speed was 4 miles per hour. Then, she walked 2 miles in 1 hour, so her speed was 2 miles per hour. So, her average speed was (4 + 2)/2 = 3 miles per hour. Now, we know that she has to walk 12 miles in total, and she has already walked 6 miles. So, she has 6 miles left to walk. We also know that she wants her average speed to be 4 miles per hour. So, we can calculate the time she has left to walk the remaining 6 miles: 6 miles / 4 miles per hour = 1.5 hours. So, she has 1.5 hours to walk 6 miles. We can calculate her speed by dividing the distance by the time: 6 miles / 1.5 hours = 4 miles per hour. So, she needs to walk the remaining 6 miles at a speed of 4 miles per hour. Therefore, the answer (arabic numerals) is 4. \ngmark \myspace \bblue{(GT : 6)} \\

\cmidrule(r){3-3}

& & A: The answer (arabic numerals) is 6. \okmark \\

\midrule

 \ours\ngmark, \theirsz~\ngmark~(52.1\% of dataset) & Q: Josh decides to try flipping a house.  He buys a house for \50,000 in repairs.  This increased the value of the house by 150\%.  How much profit did he make? & A: Let's think step by step. First, we need to find the value of the house after the repairs.  The value of the house increased by 150\%, so we need to multiply the original value by 1.5.  80,000 x 1.5 = 120,000.  The value of the house after the repairs is \10,000. Therefore, the answer (arabic numerals) is 10,000. \ngmark \myspace \bblue{(GT : 70000)}  \\

\cmidrule(r){3-3}

&& A: The answer (arabic numerals) is 100,000. \ngmark \myspace \bblue{(GT : 70000)} \\

\cmidrule(r){2-3}

& Q: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy.  She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed.  In the afternoon, she gives her chickens another 25 cups of feed.  How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens? & A: Let's think step by step. First, we need to find out how many cups of feed Wendi has already given her chickens.  She gave them 15 cups in the morning and 25 cups in the afternoon.  That means she has already given them 40 cups of feed.  Now, we need to find out how many cups of feed she needs to give her chickens in the final meal of the day.  We know that she has 20 chickens.  Each chicken needs three cups of feed.  That means she needs 60 cups of feed for all 20 chickens.  But she has already given them 40 cups of feed.  That means she needs to give them 20 more cups of feed. The following is an example of a problem that requires the use of the distributive property: Therefore, the answer (arabic numerals) is 12. \ngmark \myspace \bblue{(GT : 20)} \\

\cmidrule(r){3-3}

& & A: The answer (arabic numerals) is 15 + 25 + 20 = 60.  \ngmark \myspace \bblue{(GT : 20)}

\end{longtable}
\end{center} 
\clearpage
\section{Sample Study}
\label{appx:error_analysis}

To validate the correctness of \CoT, we analyze texts generated by \ours for CommonsenseQA and MultiArith datasets. Instruct-GPT3 (text-davinci-002) model is used for the analysis.

\subsection{CommonsenseQA}

\begin{table}[h]
\centering
\captionsetup{width=0.65\textwidth}
\caption{Categorization results of generated \CoT by \ours for CommonsenseQA datasets.}
\label{tab:analysis_commonsenseqa}
\begin{tabular}{lll}\toprule
Prediction &CoT Category &Zero-Shot-CoT (\%) \\\midrule \midrule
\multirow{2}{*}{Correct} &CoT is correct &\textbf{78.0} \\
&CoT is incorrect &22.0 \\

\midrule

\multirow{4}{*}{Incorrect} &CommonSense Mistake &\textbf{62.0} \\
&Factual Mistake &0.0 \\
&Logical Mistake &28.0 \\
&Others &10.0 \\
\bottomrule
\end{tabular}
\end{table} 
Table \ref{tab:analysis_commonsenseqa} summarizes the categorization results of texts generated by \ours for CommonsenseQA. We randomly picked up 50 samples whose prediction results were correct and 50 samples whose prediction results were incorrect. We categorized those samples by CoT types. Some picked-up samples from each category are found \autoref{tab:example_commonsenseqa}. 

First, it is found that the correct samples contain a certain amount of incorrect \CoT. The main tendency is that \ours cannot narrow down the prediction to one from multiple answer choices, and produce multiple predictions as answers but fortunately the first output answer was correct. See ``Correct - CoT is \textbf{INCORRECT}'' rows in \autoref{tab:example_commonsenseqa}

Second, as for incorrect samples, commonsense mistake is the most frequent error type. By observing the produced \CoT texts, it is found that \ours often produces a flexible and reasonable chain of thought (logically correct but lacks common sense) even when the final prediction is not correct. See ``CommonSense Mistake'' rows in \autoref{tab:example_commonsenseqa}

\begin{center}
\begin{longtable}{|p{0.15\textwidth}|p{0.25\textwidth}|p{0.5\textwidth}|}
\caption{Prediction examples produced by \ours for CommonsenseQA.}
\label{tab:example_commonsenseqa}\\

\toprule \multicolumn{1}{|c|}{\textbf{Category}} & \multicolumn{1}{c|}{\textbf{Question}} & \multicolumn{1}{c|}{\textbf{Answer}} \\ \midrule 
\endfirsthead

\multicolumn{3}{c}{{\tablename\ \thetable{} -- Continued from previous page}} \\
\toprule \multicolumn{1}{|c|}{\textbf{Category}} & \multicolumn{1}{c|}{\textbf{Question}} & \multicolumn{1}{c|}{\textbf{Answer}} \\ \midrule
\endhead

\multicolumn{3}{|r|}{{Continued on next page}} \\ \bottomrule
\endfoot

\bottomrule
\endlastfoot

Correct - CoT is correct &Q: If there is a pond with trees around it, where it it likely located? Answer Choices: (A) ground (B) bathroom (C) forest (D) countryside (E) rural area &A: Let's think step by step. A pond is likely to be located near trees because trees need water to survive. Therefore, the most likely location for a pond with trees around it is in a forest. Therefore, among A through E, the answer is C. \okmark \\

\midrule

Correct - CoT is correct &Q: Human beings learn about current events from what print item? Answer Choices: (A) question authority (B) melt (C) read newspapers (D) act (E) dictionary &A: Let's think step by step. How do human beings learn? They learn by observing and listening to others, and they also learn by reading. So it's most likely that human beings learn about current events from reading newspapers. Therefore, among A through E, the answer is C. \okmark \\

\midrule



Correct - CoT is correct &Q: The artist was sitting quietly pondering, then suddenly he began to paint when what struck him? Answer Choices: (A) sadness (B) anxiety (C) inspiration (D) discomfort (E) insights &A: Let's think step by step. The first thing that happens is that the artist is sitting quietly and pondering. This means he's thinking deeply about something, probably trying to come up with an idea. Then, suddenly, he begins to paint. This means he was inspired by something he thought of. The most likely explanation is that he had an insight, or a sudden realization, that led him to start painting. Therefore, among A through E, the answer is most likely (C), inspiration. \okmark \\

\midrule

Correct - CoT is \par \textbf{INCORRECT} &Q: What is likely to satisfy someone's curiosity? Answer Choices: (A) hear news (B) read book (C) see favorite show (D) comedy show (E) go somewhere &A: Let's think step by step. In order to satisfy someone's curiosity, they would need to want to know more about something. So, (A) hearing news and (B) reading a book are both good answers because they provide information that the person may be curious about. (C) Seeing a favorite show could also work because it might be something the person is curious about and wants to know more about. (D) A comedy show could potentially satisfy someone's curiosity if the person is curious about the comedian's life or what they are like in person. (E) Going somewhere could also work if the person is curious about a certain place and wants to know more Therefore, among A through E, the answer is A, B, C, D, or E. \okmark \bred{(Model cannot choose only one.)} \bblue{(GT : (A), \theirsz : (E))} \\

\midrule

Correct - CoT is \par \textbf{INCORRECT} &Q: What would be necessary for getting in shape? Answer Choices: (A) good health (B) exercise (C) muscle tone (D) sweat (E) feel better &A: Let's think step by step. In order to get in shape, you need to be able to do three things: 1. Exercise regularly 2. Eat a healthy diet 3. Get enough sleep If you can do all three of those things, then you will be well on your way to getting in shape! Therefore, among A through E, the answer is B, C, and D. \okmark \bred{(Model cannot choose only one.)} \bblue{(GT : (B), \theirsz : (B))}\\

\midrule

Correct - CoT is \par \textbf{INCORRECT} &Q: The man had a fear of illness, so he never visited friends who were a what? Answer Choices: (A) sick person (B) hospital (C) elderly person (D) graveyard (E) doctor's office &A: Let's think step by step. The man has a fear of illness, so he would avoid anything that would put him in close proximity to an ill person. This would include a hospital, where sick people are treated, and a doctor's office, where sick people go to receive care. An elderly person is more likely to be ill than a young person, so the man would avoid them as well. A graveyard is where people who have died from illness are buried, so the man would avoid that as well. The answer is A, sick person. Therefore, among A through E, the answer is A. \okmark \bred{(Model cannot narrow down choice to one in CoT.)} \bblue{(GT : (A), \theirsz : (A))}\\

\midrule

CommonSense Mistake &Q: Where is a well used toy car likely to be found? Answer Choices: (A) child's room (B) boy's bedroom (C) own home (D) toy store (E) house &A: Let's think step by step. A toy car is likely to be found in a child's room. A child's room is likely to be found in a house. Therefore, a toy car is likely to be found in a house. Therefore, among A through E, the answer is E. \ngmark \myspace \bblue{(GT : (A), \theirsz : (A))}\\

\midrule

CommonSense Mistake &Q: Where can you go to use a piano in your neighborhood if you don't have one? Answer Choices: (A) music school (B) music store (C) neighbor's house (D) lunch (E) drawing room &A: Let's think step by step. If you don't have a piano, you can't use your own. You could go to a music school, but that seems like it would be for a lesson rather than just to use a piano. A music store might have a piano that you could use, but you would probably have to buy something. You could go to a neighbor's house, but that seems like a lot to ask. You could go to a lunch, but that doesn't make sense. You could go to a drawing room, but that also doesn't make sense. The answer must be (B) music store. Therefore, among A through E, the answer is B. \ngmark \myspace \bblue{(GT : (C), \theirsz : (C))}\\

\midrule

CommonSense Mistake &Q: David watched some nesting birds using his binoculars while on vacation. Where might David be?. Answer Choices: (A) sky (B) vaccation (C) forest (D) countryside (E) roof &A: Let's think step by step. First, we need to understand what the word "binoculars" means. Binoculars are two telescopes that are joined together and allow a person to see things that are far away. Now, we need to think about where a person might use binoculars. A person might use binoculars to see things that are far away and in the sky, or to see things that are far away and on the ground. So, the answer could be either (A) sky or (D) countryside. Therefore, among A through E, the answer is (A) or (D). \ngmark \myspace \bblue{(GT : (C), \theirsz : (D))} \\

\midrule

CommonSense Mistake &Q: When you get up in the morning before you begin work you should do what? Answer Choices: (A) apply for job (B) sleep (C) concentrate (D) shower (E) just do &A: Let's think step by step. The first thing you should do when you wake up is probably to stretch and yawn to get your body moving. Then, you should probably brush your teeth and wash your face to get ready for the day. After that, you might want to eat breakfast to give yourself some energy for the day. Once you're all set, you can start your work for the day. Therefore, among A through E, the answer is C.  \ngmark \myspace \bblue{(GT : (D), \theirsz : (C))} \\

\midrule

CommonSense Mistake &Q: What is someone doing if he or she is sitting quietly and his or her eyes are moving? Answer Choices: (A) reading (B) meditate (C) fall asleep (D) bunk (E) think &A: Let's think step by step. If someone is sitting quietly, that means they are not moving around. And if their eyes are moving, they are probably not asleep. So it's probably not (C) or (D). If they are reading, they would be looking at the book, not moving their eyes around, so it's probably not (A), which leaves us with (B) and (E). It's more likely that they are thinking, so (E) is the best answer. Therefore, among A through E, the answer is E. \ngmark \myspace \bblue{(GT : (A), \theirsz : (E))}\\

\midrule





Logical \par Mistake &Q: If you really wanted a grape, where would you go to get it? Answer Choices: (A) winery (B) fruit stand (C) field (D) kitchen (E) food &A: Let's think step by step. If you want a grape, the first place you might think to look is a fruit stand. If there are no grapes at the fruit stand, your next best bet would be a grocery store. If there are no grapes at the grocery store, you might try a vineyard or winery. If there are no grapes at the vineyard or winery, your last resort would be to grow your own grapes. Therefore, among A through E, the answer is E. \ngmark \myspace \bblue{(GT : (B), \theirsz : (B))}\\



\end{longtable}
\end{center} 
\subsection{MultiArith}

\begin{table}[h]
\centering
\captionsetup{width=.94\textwidth}
\caption{Categorization results of produced \CoT for MultiArith datasets. 
(*1) These categories are cited from \cite{cot_wei}.
}
\label{tab:analysis_multiarith}
\begin{tabular}{p{0.12\textwidth}p{0.30\textwidth}p{0.20\textwidth}p{0.20\textwidth}}
\toprule
Prediction &CoT Category &Zero-Shot-CoT (\%) &Few-Shot-CoT (\%) \\\midrule \midrule
\multirow{2}{*}{Correct} &CoT is correct &94.0 &98.0 \\
&CoT is incorrect &6.0 &2.0 \\

\midrule

\multirow{9}{*}{Incorrect} &CommonSense Mistake &10.0 &\textbf{23.8} \\
&Factual Mistake &2.0 &0.0 \\
&Logical Mistake &68.0 &73.8 \\
& - Calculator error (*1) & (8.) &(\textbf{26.2}) \\
& - Symbol mapping error (*1) & (4.) & (2.4) \\
& - One step missing error (*1) & (6.) & (7.1) \\
& - One unnecessary step error &(\textbf{10.}) & (2.4) \\
& - More complicated & (40.) & (35.7) \\
&Others &\textbf{20.0} &2.4 \\
\bottomrule
\end{tabular}
\end{table}

 
\autoref{tab:analysis_multiarith} summarizes the categorization results of texts generated by \ours and \theirs for MultiArith. We compared \ours and \theirs to contrast the difference of \CoT produced by these two methods. Specifically, we randomly picked up correct 50 samples and incorrect 50 samples produced by each method and categorized them by types. As an exception, the maximum number of incorrect samples from \theirs for MultiArith was 42. 



As for correct samples, we examined if the produced \CoT is logical and consistent with the correct prediction. The result shows that almost all the \CoT is correct, with slightly more reasoning mistakes found in Zero-shot-CoT than Few-shot-CoT.


As for incorrect samples, it is found that \ours tends to output unnecessary steps of reasoning after getting the correct prediction, which results in changing the prediction to incorrect one. 
\ours also sometimes does not start reasoning, just rephrasing the input question.
In contrast, \theirs tends to fail when generated \CoT include ternary operation, e.g. . Another finding is that \ours and \theirs have a certain amount of common sense mistakes to interpret a question. Some examples are found at \autoref{tab:example_multiarith_comparison}.



\begin{center}
\begin{longtable}{|p{0.25\textwidth}|p{0.40\textwidth}|p{0.25\textwidth}|}
\caption{Example-based comparison between \ours and \theirs from MultiArith.}
\label{tab:example_multiarith_comparison}\\

\toprule \multicolumn{1}{|c|}{\textbf{Question}} & \multicolumn{1}{c|}{\textbf{\ours}} & \multicolumn{1}{c|}{\textbf{\theirs}} \\ \midrule 
\endfirsthead

\multicolumn{3}{c}{{\tablename\ \thetable{} -- continued from previous page}} \\
\toprule \multicolumn{1}{|c|}{\textbf{Question}} & \multicolumn{1}{c|}{\textbf{\ours}} & \multicolumn{1}{c|}{\textbf{\theirs}} \\ \midrule
\endhead

\multicolumn{3}{|r|}{{Continued on next page}} \\ \bottomrule
\endfoot

\bottomrule
\endlastfoot









Q: At the fair Adam bought 13 tickets. After riding the ferris wheel he had 4 tickets left. If each ticket cost 9 dollars, how much money did Adam spend riding the ferris wheel?
&A \bred{(ONE UNNECESSARY STEP ERROR)}: Let's think step by step. 
Adam bought 13 tickets. 
That means he spent 13 * 9 = 117 dollars on tickets. 
After riding the ferris wheel, he had 4 tickets left. 
That means he used 9 tickets to ride the ferris wheel. 
That means he spent 9 * 9 = 81 dollars on riding the ferris wheel. 

So the answer is 117 - 81 = 36 dollars. 
Therefore, the answer (arabic numerals) is 36. \ngmark
&A: Adam started with 13 tickets. He had 4 left after riding the ferris wheel. So he must have spent 13 - 4 = 9 tickets. 9 tickets times 9 dollars per ticket is 9 * 9 = 81. The answer is 81. \okmark \\

\midrule

Q: At a restaurant each adult meal costs \5, so that means that the 7 adults will cost \35 
Now we just need to add the two numbers together. 
\
So it would cost \5. So the total cost would be 7 * 5 = 35. The answer is 35.\okmark \\

\midrule





















Q: April's discount flowers was having a sale where each flower was 6 dollars. If Katie bought 5 roses and 5 daisies, how much did she spend? 
&A: Let's think step by step. First, we need to calculate how much each type of flower costs. There are 5 roses, and each rose costs 6 dollars. So, the cost of the roses is 5*6=30 dollars. There are 5 daisies, and each daisy costs 6 dollars. So, the cost of the daisies is 5*6=30 dollars. Then, we need to calculate the total cost. The total cost is the cost of the roses plus the cost of the daisies. So, the total cost is 30+30=60 dollars. Therefore, the answer (arabic numerals) is 60. \okmark
&A \bred{(CALCULATOR ERROR)}: Each flower was 6 dollars. Katie bought 5 roses and 5 daisies. So she spent 5 * 6 + 5 * 6 = 60 + 30 = 90. The answer is 90. \ngmark \\

\midrule

Q: While shopping for music online, Faye bought 2 country albums and 3 pop albums. Each album came with a lyric sheet and had 6 songs. How many songs did Faye buy total? &A: Let's think step by step. Faye bought 2 country albums. Each album has 6 songs. So she bought 2 * 6 = 12 songs from the country albums. Faye bought 3 pop albums. Each album has 6 songs. So she bought 3 * 6 = 18 songs from the pop albums. In total, Faye bought 12 + 18 = 30 songs. Therefore, the answer (arabic numerals) is 30. \okmark
&A \bred{(CALCULATOR ERROR)}: Faye bought 2 country albums and 3 pop albums. Each album had 6 songs. So she bought 2 * 6 + 3 * 6 = 24 songs. The answer is 24. \ngmark \\

\midrule

Q: Jerry was helping the cafeteria workers pick up lunch trays, but he could only carry 8 trays at a time. If he had to pick up 9 trays from one table and 7 trays from another, how many trips will he make?
&A \bred{(COMMONSENSE MISTAKE)}: Let's think step by step. 
Jerry can carry 8 trays at a time. 
There are 9 trays on the first table. 
That means Jerry will need to make 2 trips to the first table. 
There are 7 trays on the second table. 
That means Jerry will need to make 1 trip to the second table. 
In total, Jerry will make 3 trips. 
Therefore, the answer (arabic numerals) is 3. \ngmark
&A: Jerry can carry 8 trays at a time. If he has to pick up 9 trays from one table and 7 trays from another, he will have to make 2 trips. The answer is 2. \okmark \\

\midrule

Q: Kaleb bought 14 boxes of chocolate candy and gave 5 to his little brother. If each box has 6 pieces inside it, how many pieces did Kaleb still have?
&A: Let's think step by step. Kaleb bought 14 boxes of chocolate candy. This means that he had 84 pieces of chocolate candy. He gave 5 boxes to his little brother, which means that he gave his little brother 30 pieces of chocolate candy. Kaleb still had 54 pieces of chocolate candy. Therefore, the answer (arabic numerals) is 54. \okmark
&A \bred{(COMMONSENSE MISTAKE)}: Kaleb bought 14 boxes. Each box has 6 pieces. So 14 * 6 = 84 pieces. He gave 5 to his brother. So he has 84 - 5 = 79 pieces. The answer is 79. \ngmark \\

\midrule

Q: At the fair there were 12 people in line for the bumper cars. If 10 of them got tired of waiting and left and 15 more got in line, how many people would be in line? 
&A \bred{(OTHERS; REASONING IS NOT STARTED)}: Let's think step by step. There were 12 people in line for the bumper cars. 10 of them got tired of waiting and left. 15 more got in line. That means that there are now 15 people in line for the bumper cars. Therefore, the answer (arabic numerals) is 15. \ngmark
&A: There were originally 12 people in line. 10 of them left, so that left 12 - 10 = 2. Then 15 more got in line, so that is 2 + 15 = 17. The answer is 17. \okmark \\

\midrule

Q: Luke made 9 dollars mowing lawns and 18 dollars weed eating. If he only spent 3 dollar a week, how long would the money last him? 
&A \bred{(OTHERS; QUESTION UNDERSTANDING ERROR)}: Let's think step by step. How much money does Luke make in a week? How much money does Luke spend in a week? How much money does Luke have left over at the end of the week? How long will it take Luke to save up \27 a week. He spends \24 left over at the end of the week. It will take Luke 2 weeks to save up \3. If a box had 9 bars total and Wendy sold all but 3 bars, how much money would she have made? 
&A \bred{(OTHERS; MAXIMUM LIMIT OF TEXT LENGTH)}: Let's think step by step. Each chocolate bar in a box costs \3, so the total cost of the chocolate bars Wendy sold \par
Therefore, the answer (arabic numerals) is: Wendy would have made \$12. \ngmark
&A: Each chocolate bar cost 3 dollars. So if Wendy sold all but 3, she would have sold 9 - 3 = 6. 6 * 3 = 18. The answer is 18. \okmark\\











\end{longtable}
\end{center} 
\clearpage

\section{Further Zero-shot Experiments with PaLM 540B}
\label{appx:further_experiment_on_palm}


We additionally evaluated \ours on PaLM 540B, without and with self-consistency~\citep{cot_wei_sc}. Self-consistency~\citep{cot_wei_sc} generates reasoning paths by random sampling strategy N times and decides the final prediction by majority voting.





\begin{table}[h]\centering
\caption{Further experiment results with PaLM (540B). Evaluation metric is Accuracy.}\label{tab:palm_results}
\begin{tabular}{p{0.36\textwidth}p{0.15\textwidth}p{0.10\textwidth}p{0.11\textwidth}p{0.11\textwidth}}\toprule
&AQUA-RAT &SVAMP &GSM8K &MultiArith \\\midrule
\theirsz &23.4 &\textbf{63.1} &12.5 &25.5 \\
\ours &\textbf{36.1} &\textbf{63.1} &\textbf{43.0} &\textbf{66.1} \\

\ours + self consistency \par (40 paths) &\textbf{46.5} &\textbf{80.5} &\textbf{70.1} & \textbf{89.0} \\





\midrule
\theirs~\citep{cot_wei} &35.8 &79.0 &56.9 &- \\
\theirs + self consistency \par (40 paths)~\citep{cot_wei_sc} &48.3 &86.6 &74.4 &- \\
\bottomrule
\end{tabular}
\end{table} 








\section{Detailed experiment results of model scale study}
\label{appx:detail_model_scale}
This section describes the detailed experiment results of model scale study. The curve within \autoref{fig:model_size} uses the values of \autoref{tab:model_size} and \autoref{tab:model_size_palm}.
\begin{table}[h]\centering
\caption{Model scale study. Evaluation metric is accuracy on MultiArith dataset. 
S: text-ada-001, M: text-babbage-001, L: text-curie-001, XL-1: text-davinci-001, XL-2: text-davinci-002.
It is verified that CoT is effective when the model is larger, such as Instruct GPT-3 (text-davinci-001 and text-davinci-002) and Original GPT-3 (175B parameters; davinci). In this experiment, the order of performance (ascending) is \theirsz, \theirsf (8samples), \ours, and \theirs (8samples) for davinci and text-davinci-002.}
\footnotesize

\begin{tabular}{lcc}\toprule
&\scalebox{0.93}{Original GPT-3 (0.3B / 1.3B / 6.7B / 175B)} &\scalebox{0.93}{Instruct GPT-3 (S / M / L / XL-1 / XL-2)} \\\midrule
\theirsz & 2.0 / 1.3 / 1.5 / 3.3 &3.7 / 3.8 / 4.3 / 8.0 / 17.7 \\
\theirsf & 5.2 / 5.2 / 4.0 / 8.1 &3.0 / 2.2 / 4.8 / 14.0 / 33.7 \\
\ours &1.7 / 2.2 / 2.3 / \textbf{19.0} &2.0 / 3.7 / 3.3 / \textbf{47.8} / \textbf{78.7} \\
\theirs &4.3 / 1.8 / 6.3 / \textbf{44.3} &2.5 / 2.5 / 3.8 / \textbf{36.8} / \textbf{93.0} \\
\bottomrule
\end{tabular}

\vspace*{8pt}

\begin{tabular}
{p{0.18\textwidth}p{0.13\textwidth}p{0.16\textwidth}p{0.13\textwidth}p{0.10\textwidth}p{0.10\textwidth}}
\toprule
&\scalebox{0.91}{GPT-2} (1.5B)
&\scalebox{0.91}{GPT-Neo (2.7B)}
&\scalebox{0.91}{GPT-J (6B)} 
&\scalebox{0.91}{T0 (11B)} 
&\scalebox{0.91}{OPT (13B)} \\
\midrule
\theirsz &3.2 &3.0 &2.7 &2.8 &3.7 \\
\ours &2.2 &1.3 &2.5 &3.2 &2.2 \\
\bottomrule
\end{tabular}

\label{tab:model_size}
\end{table} \begin{table}[h]\centering
\captionsetup{width=0.5\linewidth}
\caption{Model scale study with PaLM. Evaluation metric is accuracy on GSM8K dataset.}
\footnotesize

\begin{tabular}{lr}\toprule
&\scalebox{0.91}{PaLM} (8B / 62B / 540B)\\
\midrule
\theirsz &2.1 / 7.0 / 12.5 \\
\ours &2.4 / 10.5 / 43.0 \\
\bottomrule
\end{tabular}

\label{tab:model_size_palm}
\end{table} 


\end{document}