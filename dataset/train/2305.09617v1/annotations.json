[{'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PubMedQA', 'Metric': 'Accuracy', 'Score': '79.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PubMedQA', 'Metric': 'Accuracy', 'Score': '75.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PubMedQA', 'Metric': 'Accuracy', 'Score': '74.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MedQA-USMLE', 'Metric': 'Accuracy', 'Score': '85.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MedQA-USMLE', 'Metric': 'Accuracy', 'Score': '83.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MedQA-USMLE', 'Metric': 'Accuracy', 'Score': '79.7'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MMLU (Medical Genetics)', 'Metric': 'Accuracy', 'Score': '92'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MMLU (Medical Genetics)', 'Metric': 'Accuracy', 'Score': '90'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MMLU (Medical Genetics)', 'Metric': 'Accuracy', 'Score': '89'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MedMCQA', 'Metric': 'Test Set (Acc-%)', 'Score': '0.723'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MedMCQA', 'Metric': 'Test Set (Acc-%)', 'Score': '0.715'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MedMCQA', 'Metric': 'Test Set (Acc-%)', 'Score': '0.713'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MMLU (College Biology)', 'Metric': 'Accuracy', 'Score': '95.8'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MMLU (College Biology)', 'Metric': 'Accuracy', 'Score': '95.1'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MMLU (College Biology)', 'Metric': 'Accuracy', 'Score': '94.4'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MMLU (Professional medicine)', 'Metric': 'Accuracy', 'Score': '95.2'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MMLU (Professional medicine)', 'Metric': 'Accuracy', 'Score': '93.4'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'MMLU (Professional medicine)', 'Metric': 'Accuracy', 'Score': '92.3'}}]
