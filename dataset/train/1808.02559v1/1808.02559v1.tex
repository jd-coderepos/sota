\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{ulem}
\usepackage{lmodern}
\usepackage{hyperref}


\clearpage{}\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow, varwidth}
\usepackage{verbatim}
\makeatletter
\newif\if@restonecol
\makeatother
\let\algorithm\relax
\let\endalgorithm\relax
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{xr}
\usepackage[amsmath,thmmarks]{ntheorem}

\theorembodyfont{\normalfont}
\theoremseparator{.}


\def\mathbi#1{\textbf{\em #1}}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\onedot{.\@\xspace}

\def\eg{\textit{e.g}\onedot} \def\Eg{\textit{E.g}\onedot}
\def\ie{\textit{i.e}\onedot} \def\Ie{\textit{I.e}\onedot}
\def\cf{\textit{c.f}\onedot} \def\Cf{\textit{C.f}\onedot}
\def\etc{\textit{etc}\onedot} \def\vs{\textit{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\textit{et al}\onedot}

\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}

\newcommand{\R}{\mathbb{R}}

\newcommand{\bmalpha}{\bm\alpha}
\clearpage{}
\usepackage{color}
\newcommand{\gunhee}[1]{{\color{blue}{\small\bf\sf [Gunhee: #1]}}}
\newcommand{\yj}[1]{{\color{red}{\small\bf\sf [#1]}}}
\newcommand{\js}[1]{{\color{magenta}{\small\bf\sf [Js: #1]}}}

\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\begin{document}
\pagestyle{headings}
\mainmatter

\title{A Joint Sequence Fusion Model for Video Question Answering and Retrieval} 


\titlerunning{A Joint Sequence Fusion Model for Video VQA and Retrieval}


\authorrunning{Y. Yu , J. Kim and G. Kim}


\author{Youngjae Yu \hspace{9pt} Jongseok Kim \hspace{9pt} Gunhee Kim}
 


\institute{Department of Computer Science and Engineering,\\
	Seoul National University, Seoul, Korea\\
	\email{ \{yj.yu,js.kim\}@vision.snu.ac.kr, gunhee@snu.ac.kr} \\
\url{http://vision.snu.ac.kr/projects/jsfusion/}
}

\maketitle
\begin{abstract}
We present an approach named JSFusion (Joint Sequence Fusion) that can measure semantic similarity between any pairs of multimodal sequence data (\eg a video clip and a language sentence).
Our multimodal matching network consists of two key components. First, the \textit{Joint Semantic Tensor} composes a dense pairwise representation of two sequence data into a 3D tensor.
Then, the \textit{Convolutional Hierarchical Decoder} computes their similarity score by discovering hidden hierarchical matches between the two sequence modalities.  Both modules leverage hierarchical attention mechanisms that learn to promote well-matched representation patterns while prune out misaligned ones in a bottom-up manner.
Although the JSFusion is a universal model to be applicable to any multimodal sequence data, this work focuses on video-language tasks including multimodal retrieval and video QA.  
We evaluate the JSFusion model in three retrieval and VQA tasks in LSMDC, for which our model achieves the best performance reported so far. 
We also perform multiple-choice and movie retrieval tasks for the MSR-VTT dataset, on which our approach outperforms many state-of-the-art methods. 
\keywords{Multimodal Retrieval; Video Question and Answering}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\begin{figure*}[t]
\centering
\includegraphics[trim=0.0cm 0.2cm 0cm 0.0cm,clip,width=0.95\textwidth]{pictures/keyidea_twocolumn.pdf}
\caption{The intuition of the Joint Sequence Fusion (JSFusion) model. 
Given a pair of a video clip and a language query, 
Joint Semantic Tensor (in purple) encodes a pairwise joint embedding between the two sequence data, 
and Convolutional Hierarchical Decoder (in blue) discovers hierarchical matching relations from JST. 
Our model is easily adaptable to many video QA and retrieval tasks.
}

\label{fig:keyidea}
\end{figure*}


Recently, various video-language tasks have drawn a lot of interests in computer vision research \cite{rohrbach-arxiv-2016,xu-CVPR-2016,chen-acl-2011}, including video captioning~\cite{jeff-cvpr-2015,guadarrama-iccv-2013,rohrbach-gcpr-2015,venugopalan-iccv-2015,xu-aaai-2015,yu-cvpr-2017}, video question answering (QA)~\cite{tapaswi-cvpr-2016,jang-CVPR-2017}, and video retrieval for a natural language query~\cite{xu-aaai-2015,torabi-arxiv-2016,mayu-arxiv-2016}. 
To solve such challenging tasks, it is important to learn a hidden join representation between word and frame sequences, for correctly measuring their semantic similarity. 
Video classification \cite{laptev-iccv-2003,laptev-cvpr-2008,soomro-crcv-2012,karpathy-CVPR-2014,caba-cvpr-2015} can be  a candidate solution, 
but tagging only a few labels to a video may be insufficient to fully relate multiple latent events in the video to a language description. Thanks to recent advance of deep representation learning, many methods for multimodal semantic embedding (\eg \cite{kiros-tacl-2014,frome-nips-2013,socher-tacl-2014}) have been proposed.
However, most of existing methods embed each of visual and language information into a single vector,
which is often insufficient especially for a video and a natural sentence. 
With single vectors for the two sequence modalities, it is hard to directly compare multiple relations between subsets of sequence data (\ie matchings between subevents in a video and short phrases in a sentence),
for which hierarchical matching is more adequate. There have been some attempts to learn representation of hierarchical structure of natural sentences and visual scenes (\eg \cite{socher-icml-2011,socher-acl-2013} using recursive neural networks), but they require groundtruth parse trees or segmentation labels.


In this paper, we propose an approach that can measure semantic similarity between any pairs of multimodal sequence data, by learning bottom-up recursive matches via attention mechanisms.
We apply our method to tackle several video question answering and retrieval tasks. Our approach, named as Joint Sequence Fusion (JSFusion) model, consists of two key components. First, the Joint Semantic Tensor (JST) performs dense Hadamard products between frames and words and encodes all pairwise embeddings between the two sequence data into a 3D tensor. 
JST further takes advantage of learned attentions to  refine the 3D matching tensor. 
Second, the Convolutional Hierarchical Decoder (CHD) discovers local alignments on the tensor, by using a series of attention-based decoding modules, consisting of convolutional layers and gates. 
These two attention mechanisms promote well-matched representation patterns and prune out misaligned ones in a bottom-up manner.
Finally, CHD obtains hierarchical composible representations of the two modalities, and computes a semantic matching score of the sequence pair. 





We evaluate the performance of our JSFusion model on multiple video question answering and retrieval tasks on LSMDC~\cite{rohrbach-arxiv-2016} and MSR-VTT~\cite{xu-CVPR-2016} datasets.  
First, we participate in three challenges of LSMDC: multiple-choice test, movie retrieval, and fill-in-the-blank,  
which require the model to correctly measure a semantic matching score between a descriptive sentence and a video clip,  
or to predict the most suitable word for a blank in a sentence for a query video.  
Our JSFusion model achieves the best accuracies reported so far with significant margins for the lsmdc tasks.  
Second, we newly create multiple-choice and movie retrieval annotations for the MSR-VTT dataset,  
on which our approach also outperforms many state-of-the-art methods in diverse video topics (\eg \textit{TV shows}, \textit{web videos}, and \textit{cartoons}).  

\begin{comment}
The LSMDC (\textit{Large Scale Movie Description Challenge}) has been one of the most active and successful challenge series to boost up the progress of video and language research.
The challenge defines four interesting tasks on the LSMDC dataset that combines two previous datasets: MPII Movie description dataset (MPII-MD)~\cite{rohrbach-cvpr-2015} and Montreal Video Annotation Dataset (M-VAD)~\cite{torabi-mvad-2015}.
We design a different base model for each of LSMDC tasks, based on JSFusion module Figure~\ref{fig:keyidea} given video and natural language sequence pair.

(i) \textit{multiple-choice test}: given a video query and five descriptive sentences, choosing the most correct one out of them,
(ii) \textit{Movie retrieval}: ranking 1,000 movie clips for a given natural language query,
and (iii) \textit{Fill-in-the-blank}: given a video and a sentence with a single blank, filling in the blank by finding a suitable word from the whole vocabulary set.
Pair of video and natural language sequence are given to solve each task. 
\end{comment}


We summarize the contributions of this work as follows.

\begin{enumerate}
\item We propose the Joint Sequence Fusion (JSFusion) model, consisting of two key components: JST and CHD. 
To the best of our knowledge, it is a first attempt to leverage recursively learnable attention modules for measuring semantic matching scores between multimodal sequence data.   
Specifically, we propose two different attention models, including soft attention in JST and Conv-layers and Conv-gates in CHD. 
\item To validate the applicability of our JSFusion model, especially on video question answering and retrieval, 
we participate in three tasks of LSMDC~\cite{rohrbach-arxiv-2016}, and achieve the best performance reported so far. 
We newly create video retrieval and QA benchmarks based on MSR-VTT~\cite{xu-CVPR-2016} dataset, on which our JSFusion outperforms many state-of-the-art VQA models.  Our source code and benchmark annotations are publicly available in our project page. \end{enumerate}





\section{Related Work}
\label{sec:related_work}

Our work can be uniquely positioned in the context of two recent research directions: video retrieval and video question answering.

\textbf{Video retrieval with natural language sentences}.
Visual information retrieval with natural language queries has long been tackled via  joint visual-language embedding models~\cite{torabi-arxiv-2016,kiros-tacl-2014,hodosh-JAIR-2013,lin-CVPR-2014,vendrov-arxiv-2015,hu-cvpr-2016,mao-cvpr-2016}.
In the video domain, it is more difficult to learn latent relations between a sequence of frames and a sequence of descriptive words, given that a video is not simply a multiple of images.
Recently, there has been much progress in this line of research. Several deep video-language embedding methods \cite{xu-aaai-2015,torabi-arxiv-2016,mayu-arxiv-2016} has been developed by extending image-language embeddings~\cite{frome-nips-2013,socher-tacl-2014}. 
Other recent successful methods benefit from incorporating concept words as semantic priors~\cite{yu-cvpr-2017,yu-arxiv-2016}, or relying on strong representation of videos like RNN-FV~\cite{kaufman-iccv-2017}. 
Another dominant approach may be leveraging RNNs or their variants like LSTM to encode the whole multimodal sequences (\eg~\cite{yu-cvpr-2017,torabi-arxiv-2016,yu-arxiv-2016,kaufman-iccv-2017}).  

Compared to these existing methods, our model first finds dense pairwise embeddings between the two sequences, and then composes higher-level similarity matches from fine-grained ones in a bottom-up manner, leveraging hierarchical attention mechanisms. This idea improves our model's robustness especially for local subset matching (\eg at the activity-phrase level), which places our work in a unique position with respect to previous works. 

\textbf{Video question answering}.
VQA is a relatively new problem at the intersection of computer vision and natural language research~\cite{malinowski-nips-2014,antol-iccv-2015,goyal-cvpr-2017}.
Video-based VQA is often recognized as a more difficult challenge than image-based one, because video VQA models must learn spatio-temporal reasoning to answer problems, which requires large-scale annotated data.
Fortunately, large-scale video QA datasets have been recently emerged from the community using crowdsourcing on various sources of data (\eg movies for MovieQA~\cite{tapaswi-cvpr-2016} and animated GIFs for TGIF-QA~\cite{jang-CVPR-2017}). 
Rohrbach \etal~\cite{rohrbach-arxiv-2016} extend the LSMDC movie description dataset to the VQA domain, introducing several new tasks such as multiple-choice~\cite{torabi-arxiv-2016} and fill-in-the-blank~\cite{Tegan-arxiv-2016}. 

The multiple-choice problem is, given a video query and five descriptive sentences, to choose a single best answer in the candidates. 
To tackle this problem, ranking losses on deep representation~\cite{yu-cvpr-2017,jang-CVPR-2017,torabi-arxiv-2016} or nearest neighbor search on the joint space~\cite{kaufman-iccv-2017} are exploited. 
Torabi \etal \cite{torabi-arxiv-2016} use the temporal attention on the joint representation between the query videos and answer choice sentences. 
Yu \etal~\cite{yu-cvpr-2017} use LSTMs to sequentially feed the query and the answer embedding conditioned on detected concept words. 
The fill-in-the-blank task is, given a video and a sentence with a single blank, to select a suitable word for the blank.
To encode the sentential query sentence on the video context, MergingLSTMs~\cite{mazaheri-arxiv-2016} and LR/RL LSTMs~\cite{mazaheri-iccv-2017} are proposed. 
Yu \etal\cite{yu-cvpr-2017,yu-arxiv-2016} attempt to detect semantic concept words from videos and integrate them with Bidirectional LSTM that encodes the language query.
However, most previous approaches tend to focus too much on the sentence information and easily ignore visual cues. 
On the other hand, our model focuses on learning multi-level semantic similarity between videos and sentences, and consequently achieves the best results reported so far in these two QA tasks, as will be presented in section \ref{sec:experiments}.






\section{The Joint Sequence Fusion Model}
\label{sec:jsfusion}

We first explain the preprocessing steps for describing videos and sentences in section \ref{sec:preproc}, and then discuss the two key components of our JSFusion model in section \ref{subsec:jst}--\ref{subsec:illustrative_example}, respectively. We present the training procedure of our model in section \ref{subsec:training}, and its applications to three video-language tasks in section \ref{sec:vlmodels}. 

\subsection{Preprocessing}
\label{sec:preproc}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth,trim=0cm 1.1cm 0.8cm 0cm]{pictures/architecture.pdf}
\caption{
    The architecture of Joint Sequence Fusion (JSFusion) model. {\bf\color{MidnightBlue}{Blue paths}} indicate the information flows for multimodal similarity matching tasks, while {\bf\color{OliveGreen}{green paths}} for the fill-in-the-blank task.
    (a) JST composes pairwise joint representation of language and video sequences into a 3D tensor, using a soft-attention mechanism. (b) CHD learns hierarchical relation patterns between the sequences, using a series of convolutional decoding module which shares parameters for each stage.  is Hadamard product,  is addition, and  is multiplication between representation and attentions described in Eq.(\ref{eq:jst})--(\ref{eq:convatt}). We omit some fully-connected layers for visualization purpose.  
}
\label{fig:model_jsf}


\end{figure*}


\textbf{Sentence representation}.
We encode each sentence in a word level. We first define a vocabulary dictionary  by collecting the words that occur more than three times in the dataset.
(\eg the dictionary size is  for LSMDC). We ignore the words that are not in the dictionary.
Next we use the pretrained glove.42B.300d \cite{Pennington-emnlp-2014} to obtain the word embedding matrix
 where  is the word embedding dimension. We denote the description of each sentence by  where  is the number of words in the sentence. 
We limit the maximum number of words per sentence to be . If a sentence is too long, we discard the remaining excess words, because only 0.07 of training sentences excess this limit, and no performance gain is observed for larger . 
Throughout this paper, we use  for denoting the word index.

\textbf{Video representation}.
We sample a video at five fps, to reduce the frame redundancy while minimizing information loss.
We employ CNNs to encode both visual and audial information in videos.
For visual description, we extract the feature map of each frame from the pool5 layer
()
of ResNet-152 \cite{he-arxiv-2015} pretrained on ImageNet. For audial information, we extract the feature map using the VGGish~\cite{Hershey-icassp-2017} followed by PCA for dimensionality reduction ().
We then concatenate both features as the video descriptor  where  is the number of frames in the video.
We limit the maximum number of frames to be .
If a video is too long, we select  equidistant frames. We observe no performance gain for larger . We use  for denoting the video frame index.


\subsection{The Joint Semantic Tensor}
\label{subsec:jst}

The Joint Semantic Tensor (JST) first composes pairwise representations between two multimodal sequences into a 3D tensor. 
Next, JST applies a self-gating mechanism to the 3D tensor to refine it as an attention map that discovers fine-grained matching between all pairwise embeddings of the two sequences while pruning out unmatched joint representations

\textbf{Sequence encoders}.
Give a pair of multimodal sequences, we first represent them using encoders.
We use \textit{bidirectional LSTM networks} (BLSTM) encoder~\cite{Schuster-ieee-1997,hochreiter-ieee-1997} for word sequence and CNN encoder for video frames.
It is often advantageous to consider both future and past contexts to represent each element in a sequence, which motivates the use of BLSTM encoders. 
 and  are the forward and backward hidden states of the BLSTM, respectively:
where we set , with initializing them as zeros: .
Finally, we obtain the representation of each modality at each step by 
concatenating the forward/backward hidden states and the input features: 
 for words.
For visual domain, we use 1-d CNN encoder representation for ,  instead, .




\begin{table}[t]
\setlength\tabcolsep{8.5pt} \centering
\begin{tabular}{|c|c|}
	\hline
	FC layers   &  size  \\ \hline    D,D   & 512    \\ 
	D2   & 512    \\ 
	D3, D4   & 512   \\ \hline
	D5   & 256   \\ 
	D6   & 256   \\ 
	D7   & 128   \\ 
	D8   & 1     \\ \hline
\end{tabular}
\begin{tabular}{|c|c|c|}
    \hline
    Conv layer       &  kernel/stride   & channel \\ \hline
    Conv1     &  3  3 / 1& 256    \\ 
    ConvG1     &  3  3 / 1& 1    \\ \hline
    Conv2     &  3  3 / 1& 256    \\ 
    ConvG2     &  3  3 / 1& 1    \\ \hline
    Conv3     &  3  3 / 2& 256    \\ 
    ConvG3     &  3  3 / 2& 1    \\ \hline
    MeanPool        &  17  17 / 17 & 256    \\ \hline
  \end{tabular} 
\medskip

  \caption{
    The detailed setting of layers in the JSFusion model. No padding is used for each layer.  means a fully-connected dense layer, and  and  indicate convolutional and convolutional-gating layer, respectively. }
\label{tbl:layer_detail}
\end{table}


\textbf{Attention-based joint embedding}. 
We then feed the output of the sequence encoder into fully-connected (dense) layer  for each modality separately, which results in , where  is a hidden dimension of . 
We summarize the details of all the layers in our JSFusion model in Table~\ref{tbl:layer_detail}. 
Throughout the paper, we denote fully-connected layers as  and convolutional layers as \textit{Conv}.

Next, we compute attention weights  and representation , from which we obtain the  JST as a joint embedding between every pair of sequential features:

\noindent  is a hadamard product,  is a sigmoid function, and  is a learnable parameter. 
Since the output of the sequence encoder represents each frame conditioned on the neighboring video (or each word conditioned on the whole sentence), 
the attention  is expected to figure out which pairs should be more weighted for the joint embedding among all possible pairs.
For example of Fig.\ref{fig:model_jsf}, expectedly, , if  is \textit{truck}, 
and the third video frame contains the \textit{truck} while the eighth frame does not. 

From Eq.(\ref{eq:jst})--(\ref{eq:att}), we obtain JST in a form of 3D tensor:  and  . 




\subsection{The Convolutional Hierarchical Decoder}
\label{subsec:hierarchical_decoder}
\begin{figure*}[t]
\centering
\includegraphics[trim=0.0cm 0.2cm 0cm 0.0cm,clip,width=0.95\textwidth]{pictures/JST_twocolumn.pdf}
\caption{Attention examples for (a) Joint Semantic Tensor (JST) and (b) Convolutional Hierarchical Decoder (CHD). Higher values are shown in darker.
(a) JST assigns high weights on positively aligned joint semantics in the two sequence data. Attentions are highlighted darker where words coincide well with frames. (b) Each layer in CHD assigns high weights to where structure patterns are well matched between the two sequence data. 
For a wrong pair of sequences, a series of Conv-gating () prune out misaligned patterns with low weights. 
}


\label{fig:chd_att}
\end{figure*}
The Convolutional Hierarchical Decoder (CHD) computes a compatibility score for a pair of multimodal sequences by exploiting the compositionality in the joint vector space of JST.
We pass the JST tensor through a series of a convolutional (Conv) layer and a Conv-gating block, whose learnable kernels progressively find matched embeddings from those of each previous layer.
That is, starting from the JST tensor, the CHD recursively activates the weights of positively aligned pairs than negatively aligned ones. 

Specifically, we apply three sets of Conv layer and Conv-gating to the JST: 
for . We initialize  from the JST, and  [] is the -th Conv layer for joint representation, [] is the -th Conv-gating layer for matching filters, 
whose details are summarized in Table \ref{tbl:layer_detail}.

We apply mean pooling to  to obtain a single  video-sentence vector representation  (\eg ). 
Finally, we compute similarity matching score by feeding  into four dense layers :

We use the tanh activation for all dense layers except . 


\subsection{An Illustrative Example of How the JSFusion Model Works}
\label{subsec:illustrative_example}

Fig.\ref{fig:chd_att} illustrates with actual examples how the attentions of JST and CHD work.

Fig.\ref{fig:chd_att}(a) visualizes the learned attention weights  in Eq.(\ref{eq:jst}) of all pairs between frames in a video and words in a positive and a negative sentence.
The attentions are highlighted with higher values (shown in darker) when the words coincide better with the content in the frames, dominantly in a positive pair.

Fig.\ref{fig:chd_att}(b) shows the output  of each Conv layer and Conv-gating block in Eq.(\ref{eq:convatt}) for the same example.
During training, each Conv layer learns to compose joint embedding from the ones in the lower layer, 
while the Conv-gating layer learns frequent matching patterns in the training pairs of videos and sentences.
At test time, when it comes to compute a similarity score, the Conv-gating layers prune out misaligned patterns;
if the pair is negative where there is no common aligned structure in the two sequences, as shown in the right of Fig.\ref{fig:chd_att}(b), 
most elements of  have very low values. 
As a result, the CHD can selectively filter lower-layer information that needs to be propagated to the final-layer representation,
and the final layer of CHD assigns a high score only if the jointly aligned patterns are significant between the sequence data.

The motivation behind the JSFusion model is that long sequence data like videos and sentences are too complicated to compare them in a single vector space,
although most previous approaches depend on single LSTM embedding such as neural visual semantic embedding~\cite{kiros-tacl-2014} and previous LSMDC winners~\cite{kaufman-iccv-2017,yu-cvpr-2017}.
Instead, in our approach, JST first composes a dense pairwise 3D tensor representation between multimodal sequence data, from which CHD then
exploits convolutional gated layers to learn multi-stage similarity matching. Therefore, our JST model can be more robust for detecting partial matching between short phrases and subhots.




\subsection{Training}
\label{subsec:training}

We train our JSFusion model using the ranking loss.
Each training batch consists of  video-sentence pairs, including a single positive pair and  randomly sampled negative pairs.
We use batch shuffling in every training epoch.
Finally, we train the model using a max-margin structured loss objective as follows:

where  is the answer pair among  candidates,  is a hyperparameter and  denotes weight parameters.
This objective encourages a positive video-sentence pair to have a higher score than a misaligned negative pair by a margin .
We use  in our experiments.
We train all of our models using the Adam optimizer \cite{kingma-iclr-2015}, with an initial learning rate in the range of .
For regularization, we apply batch normalization~\cite{Sergey-icml-2015} to every dense layer. 


\subsection{Implementation of Video-Language Models}
\label{sec:vlmodels}

We below discuss how the JSFusion model is implemented for three video-language tasks, video retrieval, multiple-choice test, and fill-in-the-blank.
We apply the same JSFusion model to both video retrieval and multiple-choice test with slightly different hyperparameter settings. 
For the fill-in-the-blank, we make a minor modification in our model to predict a word for a blank in the middle of the sentence. 

\begin{comment}

For multiple-choice test and movie retrieval tasks, we use similarity matching score between two multimodal sequences. 
For fill-in-the-blank task, we compute matching embedding in every possible (\ie noun, verb) blank positions and predict the correct word. 
For movie description task, we use similarity matching score from retrieval model and retrieve  best plausible sentences. Among retrieved sentences, we select best one based on consensus score.  

For better understanding of our models, we outline the four LSMDC tasks as follows:
(i) \textit{Movie description}: generating a single descriptive sentence for a given movie clip,
(ii) \textit{Fill-in-the-blank}: given a video and a sentence with a single blank, finding a suitable word for the blank from the whole vocabulary set,
(iii) \textit{Multiple-choice test}: given a video query and five descriptive sentences, choosing the correct one out of them, and
(iv) \textit{Movie retrieval}: ranking 1,000 movie clips for a given natural language query.
\end{comment}

\textbf{For retrieval}.
The retrieval model takes a query sentence and ranks 1,000 test videos according to the relevance between the query and videos.
For training, we set  as the size of each training batch. At test, for each query sentence , we compute scores  for all videos  in the test set.
From the score matrix, we can rank the videos for the query.
As will be presented in section \ref{sec:quant_results} and \ref{sec:qual_results}, our method successfully finds hierarchical matching patterns between complex natural language query and video frames with sounds. 


\textbf{For multiple-choice test}.
The multiple-choice model takes a video and five choice sentences among which only one is the correct answer.
Since our model can calculate the compatibility score between the query video and each sentence choice, we use the same model as the retrieval task.
We simply select the choice with the highest score as an answer.
For training, we set  so that each training batch contains 10 pairs of videos and sentences, which include only a single correct sentence, four wrong choices, and 5 randomly selected sentences from other training data. 


\textbf{For fill-in-the-blank}.
The fill-in-the-blank model takes a video and a sentence with one blank, and predict a correct word for the blank. 
Since this task requires more difficult inference (\ie selecting a word out of vocabulary , instead of computing a similarity score),
we make two modifications as follows. 
First, we use deeper dimensions for layers: , ,  
, , instead of the numbers in Table \ref{tbl:layer_detail}.

Second, we add a skip-connection part to our model, which is illustrated as the green paths of Figure~\ref{fig:model_jsf}. 
Letting   as the blank position in the query sentence, we use the BLSTM output from the blank word token \texttt{BLANK} as a sentential context of the blank position: . 
We make a summation between the output of [D7]  and the sentential context , 
and then feed it into [D8] to predict a word.

For training, we set the batch size as . 
We use the different objective, the cross-entropy loss, because this task is classification rather than ranking:
where  denotes weight parameters and .
We use dropout with a rate of 0.2. 


\section{Experiments}
\label{sec:experiments}

We report the experimental results of JSFusion models for the three tasks of LSMDC~\cite{rohrbach-arxiv-2016} and two tasks of MSR-VTT \cite{xu-CVPR-2016}. 



\begin{table*}[tb]
\setlength\tabcolsep{6pt} \centering
\small
\newcommand{\ranked}[1]{\xspace\scriptsize\sf{(#1)}}
\begin{tabular}{|l|cc|cc|cc|cc|}
\hline
\multicolumn{1}{|c|}{Tasks }           & \multicolumn{8}{c|}{\footnotesize Movie Retrieval} \\ \hline
\multicolumn{1}{|c|}{Metrics}           & \multicolumn{2}{c|}{\footnotesize R@1}      & \multicolumn{2}{c|}{\footnotesize R@5 } &  \multicolumn{2}{c|}{\footnotesize R@10 } & \multicolumn{2}{c|}{\footnotesize MedR } \\ \hline
\multicolumn{1}{|c|}{Dataset }           & L     & M         & L      & M        & L      & M        & L     & M         \\ \hline
LSTM-fusion                                 & 3.0   & 3.0       & 8.9    & 9.6      & 15.9   & 17.1     & 95    & 67       \\ 
SA-G+SA-FC7 \cite{torabi-arxiv-2016}        & 3.0   & 3.1       & 8.8    & 9.0      & 13.2   & 13.4     & 114   & 91       \\
LSTM+SA-FC7 \cite{torabi-arxiv-2016}    & 3.3   & 3.2       & 10.2   & 11.1     & 15.6   & 15.7     & 88    & 69        \\
C+LSTM+SA-FC7 \cite{torabi-arxiv-2016} & 4.3   & 4.2       & 12.6   & 12.9     & 18.9   & 19.9     & 98    & 55        \\
VSE-LSTM \cite{kiros-tacl-2014}        & 3.1   & 3.8        & 10.4    & 12.7    & 16.5   & 17.1      & 79    & 66         \\
EITanque \cite{kaufman-iccv-2017}      & 4.7   & 4.7       & 15.9   & 16.6     & 23.4   & 24.1     & 64    & 41        \\
SNUVL  \cite{yu-arxiv-2016}            & 3.6   & 3.5       & 14.7   & 15.9     & 23.9   & 23.8     & 50    & 44        \\ 
CT-SAN           \cite{yu-cvpr-2017}   & 4.5   & 4.4       & 14.1   & 16.6     & 20.9   & 22.3     & 67    & 35        \\ Miech \etal \cite{miech-iccv-2017}     & 7.3   & --        & 19.2   & --       & 27.1   & --       & 52    & --        \\  \hline
JSTfc					   & 4.7   & 5.1       & 17.2   & 21.1     & 25.2   & 29.1     & 52    & 30        \\  
JSTlstm						   & 7.6   & 9.2       & 19.2    & 28.2    & 27.1   & 41.1     & 36    & 18        \\  
JSTmax                                 & 6.7   & 8.8       & 18.0     & 29.8   & 27.2   & 41.0     & 39    & 17        \\ 
JSTmean                                & 7.5   & 9.0       & 20.9     & 27.2   & 28.2   & 40.9     & 36    & 18        \\ \hline
JSFusion-noattention                   & 6.4   & 8.7       & 18.4     & 27.4   & 28.4   & 39.5     & 41    & 19        \\ 
JSFusion-noaudio                       & 9.0   & 9.2       & 20.9     & 28.3   & 32.1   & 41.3     & 39    & 17        \\ JSFusion                               & \textbf{9.1} & \textbf{10.2} & \textbf{21.2} & \textbf{31.2} & \textbf{34.1} & \textbf{43.2} & \textbf{36} & \textbf{13}\\ \hline
\end{tabular}
\medskip
\caption{
Performance comparison for the movie retrieval task using Recall@k (R@k, higher is better) and Median Rank (MedR, lower is better).
We report the results on the two datasets of LSMDC~\cite{rohrbach-arxiv-2016} (L) and MSR-VTT~\cite{xu-CVPR-2016} (M).
}
\vspace{-5pt}

\label{tbl:results_ret}
\end{table*}

\begin{table*}[tb]
\setlength\tabcolsep{4pt} \centering
\small
\newcommand{\ranked}[1]{\xspace\scriptsize\sf{(#1)}}
\begin{tabular}{|l|cc|}
\hline
\multicolumn{1}{|c|}{Multiple-Choice }           & \multicolumn{2}{c|}{\footnotesize Accuracy} \\ \hline
\multicolumn{1}{|c|}{Dataset }         & L       & M           \\ \hline
LSTM-fusion                            & 52.8    & 38.3        \\ 
SA-G+SA-FC7 \cite{torabi-arxiv-2016}   & 55.1    & 55.8         \\
LSTM+SA-FC7 \cite{torabi-arxiv-2016}   & 56.3    & 59.1         \\
C+LSTM+SA-FC7 \cite{torabi-arxiv-2016} & 58.1    & 60.2      \\
VSE-LSTM \cite{kiros-tacl-2014}        & 63.0    & 67.3       \\ 
SNUVL  \cite{yu-arxiv-2016}            & 63.1    & 65.4      \\ 
ST-VQA-Sp.Tp \cite{jang-CVPR-2017}     & 63.5    & 66.1     \\ EITanque \cite{kaufman-iccv-2017}      & 63.7    & 65.5       \\
CT-SAN           \cite{yu-cvpr-2017}   & 63.8    & 66.4     \\ MLB \cite{Kim-iclr-2017}          & 69.0    & 76.1       \\

\hline
JSTfc					   & 64.7    & 68.7       \\  
JSTlstm						   & 72.1    & 79.7       \\  
JSTmax                                 & 68.3    & 74.4       \\ 
JSTmean                                & 70.2    & 80.0        \\ \hline
JSFusion-noattention                   & 69.4    & 79.2        \\ 
JSFusion-VGG-noaudio                   & 68.7    & 75.6     \\
JSFusion-noaudio                       & 72.5    & 82.9     \\ JSFusion                               & \textbf{73.5} & \textbf{83.4}  \\ 

\hline
\end{tabular}
\hfill
\begin{tabular}{|l|c|}
\hline
Fill-in-the-Blank  & {\footnotesize Accuracy}   \\ \hline
Text-only BLSTM \cite{Tegan-arxiv-2016} & 32.0   \\
Text-only Human \cite{Tegan-arxiv-2016} & 30.2  \\
GoogleNet-2D + C3D \cite{Tegan-arxiv-2016}  & 35.7                     \\
Ask Your Neurons \cite{tzeng-iccv-2015}   & 33.2 \\
Merging-LSTM \cite{mazaheri-arxiv-2016}   & 34.2                     \\
SNUVL               \cite{yu-arxiv-2016}  & 38.0                     \\ 
CT-SAN            \cite{yu-cvpr-2017}     & 41.9                     \\
LR/RL LSTMs            \cite{mazaheri-iccv-2017} & 40.9       \\ 
LR/RL LSTMs (Ensemble) \cite{mazaheri-iccv-2017} & 43.5       \\ 
MLB \cite{Kim-iclr-2017} & 41.6                           \\   \hline
JSTfc						             & 42.9       \\  
JSTlstm						             & 43.7       \\  
JSTmax                   & 41.3                               \\ 
JSTmean                  & 44.2                                 \\ \hline
JSFusion-noattention      & 44.5                                 \\ 
JSFusion-VGG-noaudio     & 44.2                     \\
JSFusion-noaudio         & 45.26                    \\
JSFusion                 & \textbf{45.52}                    \\
\hline
Human  \cite{Tegan-arxiv-2016}              & 68.7              \\
\hline
\end{tabular}
\medskip
\caption{
    \textbf{Left}:
Performance comparison for the multiple-choice test using the accuracy in percentage.
We report the results on the two datasets of LSMDC (L) and MSR-VTT (M). \textbf{Right}:
    Accuracy comparison (in percentage) for the movie fill-in-the-blank task.
}
\vspace{-5pt}
\label{tbl:results_mcfib}
\end{table*}


\subsection{LSMDC Dataset and Tasks}
\label{sec:lsmdc_intro}



The LSMDC 2017 consists of four video-language tasks for movie understanding and captioning, 
among which we focus on the three tasks in our experiments: movie retrieval, multiple-choice test, and fill-in-the-blank.
The challenge provides a subset of the LSMDC dataset, which contains a parallel corpus of 118,114 sentences and 118,081 video clips of about 4--5 seconds long sampled from 202 movies.
We strictly follow the evaluation protocols of the challenge.
We defer more details of the dataset and challenge rules to \cite{rohrbach-arxiv-2016} and the homepage\footnote{\url{https://sites.google.com/site/describingmovies/lsmdc-2017}.}.


\textbf{Multiple-choice test}. Given a video query and five candidate captions, the goal is to find the correct one for the query out of five possible choices.
The correct answer is the groundtruth (GT) caption and four other distractors are randomly chosen from other captions that have different activity-phrase labels from the correct answer.
The evaluation metric is the percentage of correctly answered test questions out of 10,053 public-test data.

\textbf{Movie retrieval}. 
The test set consists of 1,000 video/activity phrase pairs sampled from the LSMDC17 public-test data.
Then, the objective is, given a short query activity-phrase (\eg \textit{answering phone}), to find its corresponding video out of 1,000 test videos.
The evaluation metrics include Recall@1, Recall@5, Recall@10, and Median Rank (MedR).
The Recall@ means the percentage of GT videos in the first  retrieved videos,
and the MedR indicates the median rank of GT videos.
The challenge winner is determined by the metric of Recall@10.




\textbf{Movie fill-in-the-blank}. 
This track is related to visual question answering.
The task is, given a video clip and a sentence with a blank in it, to predict a single correct word for the blank.
The test set includes 30,000 examples from 10,000 clips (\ie about 3 blanks per sentence).
The evaluation metric is the prediction accuracy (\ie the percentage of predicted words that match with GTs).

\subsection{MSR-VTT-(RET/MC) Dataset and Tasks}
\label{sec:msrvtt_intro}

The MSR-VTT~\cite{xu-CVPR-2016} is a large-scale video description dataset. 
It collects 118 videos per query of 257 popular queries, and filters manually to 7,180 videos. From the videos, it selects 10K video clips with 41.2 hours and 200K clip-sentence pairs.

Based on the MSR-VTT dataset, we newly create two video-text matching tasks: (i) multiple-choice test and (ii) video retrieval. 
The task objectives for these tasks are identical to those of corresponding tasks in the LSMDC benchmark.
To collect annotations for the two tasks, we exactly follow the protocols that are used in the LSMDC dataset, as described in \cite{torabi-arxiv-2016}.

\textbf{Multiple-choice test}: 
We generate 2,990 questions in total for the multiple-choice test, using all the test video clips of MSR-VTT. For each test video, we use the associated GT caption for the correct answer, while randomly sampled descriptions from other test data for four negative choices.


\textbf{Video retrieval}:
For retrieval, we first sample 1,000 pairs of video clips and description queries from the test set of MSR-VTT
We use 1,000 as the size of the test set, following the LSMDC benchmark. 
As a result, the retrieval task is to find out the video that corresponds to the query caption out of 1000 candidates.



\subsection{Quantitative Results}
\label{sec:quant_results}


Table \ref{tbl:results_ret}--\ref{tbl:results_mcfib} summarize the results of our experiments for the three video-language tasks. 
For LSMDC experiments, we report the results in the published papers and the official leaderboard of LSMDC 2017\footnote{FIB : \url{https://competitions.codalab.org/competitions/11691\#results}. \\Multichoice : \url{https://competitions.codalab.org/competitions/11491\#results}.}. 
For MSR-VTT experiments, we run some participants of LSMDC, including SNUVL, EITanque, VSE-LSTM, ST-VQA-Sp.Tp and CT-SAN, using the source codes provided by the original authors. 
We implement the other baselines by ourselves, only except Miech \etal that require an additional person tracker, which is unavailable to use. Other variants of our method will be discussed in details below in the ablation study. 

Table \ref{tbl:results_ret}--\ref{tbl:results_mcfib} clearly show that  our JSFusion achieves the best performance with significant margins from all the baselines over the three tasks on both datasets.
That is, the two components of our approach, JST and CHD, indeed helps measure better the semantic similarity between multimodal sequences than a wide range of state-of-the-art models, such as a multimodal embedding method  (VSE-LSTM), a spatio-temporal attention-based QA model (ST-VQA-Sp.Tp), and a language model based QA inference (Text-only BLSTM).   Encouragingly, the JSFusion single model outperforms even the ensemble method of runner-up (LR/RL LSTMs) in the fill-in-the-blank task.

Among baselines, multimodal low-rank bilinear attention network (MLB) \cite{Kim-iclr-2017} is competitive. 
The main differences of our model from (MLB) are two-fold.
First, JSFusion embeds both a video and a sentence to feature sequences, 
whereas (MLB) represents the sentence as a single feature.
Second, JSFusion uses the self-gating to generate fine-grained matching between all pairwise embeddings of  the two sequences, 
while (MLB) uses the attention to find a position in the visual feature space that best fits for the sentence vector.  Moreover, JSFusion consistently shows better performance than (MLB) in all experiments.




\textbf{Ablation study}.
We conduct ablation experiments on different variants of our JSFusion model and present the results in Table \ref{tbl:results_ret}--\ref{tbl:results_mcfib}.
As one naive variant of our model, we test a simple LSTM baseline (LSTM-fusion) that only carries out the Hadamard product on a pair of final states of video and language LSTM encoders. That is, (LSTM-fusion) is our JSFusion model that has neither JST nor CHD, which are the two main contributions of our model.
We train (LSTM-fusion) in the same way as done for the JSFusion model in section~\ref{subsec:training}.
As easily expected, the performance of (LSTM-fusion) is significantly worse than our JSFusion in all the tasks.

To further validate the contribution of each component, we remove or replace key components of our model with simpler ones.
To understand the effectiveness of BLSTM encoding, we test two baselines: (JSTfc) that replaces BLSTM with fully-connected layers and (JSTlstm) that replaces BLSTM with LSTM.
(JSTmax) and (JSTmean) denote our variants that use max pooling and mean pooling, instead of the  convolutional layers in CHD.
That is, they use fixed max/mean pooling operations instead of convolutions with learnable kernels.
These comparisons reveal that the proposed CHD is critical to improve the performance of JSFusion nontrivially on all the tasks on both datasets. 
We also compare our model with (JSFusion-noattention) that discards Conv-gating operations of CHD. 
(JSFusion-noattention) shows nontrivial performance drops as MC (acc): , RET (R@10):  for LSMDC and MSR-VTT, respectively.
Finally, we test our model with using no audio information denoted by (JSFusion-noaudio), 
which is also much better than other baselines but only slightly worse than our original model. 




\begin{figure*}[!t]
\centering
\includegraphics[trim=0.0cm 0.2cm 0cm 0.2cm,clip,width=0.9\textwidth]{pictures/retrieval_samples_cvpr.pdf}
\caption{Qualitative examples of the three video-language tasks:
    movie retrieval on LSMDC (a)-(b) and MSR-VTT-RET (c)-(d), multiple-choice on LSMDC (e)-(f) and MSR-VTT-MC (g)-(h), and (i)-(j) fill-in-the-blank on LSMDC.
    The left column shows correct examples, while the right column shows near-miss examples.
    In (b),(d), we show our retrieval ranks of the GT clips (in the red box).
}


\label{fig:examples}
\end{figure*}




\subsection{Qualitative Results}
\label{sec:qual_results}


Fig.\ref{fig:examples} illustrates qualitative results of our JSFusion algorithm with correct (left) and near-miss (right) examples for each task.
In each set, we show natural language query and sampled frames of a video.
We present both groundtruth (GT), our prediction (Ours).


\textbf{Movie retrieval}.
Fig.\ref{fig:examples}(a) is an example that our model can understand human behaviors like \textit{gaze}. Fig.\ref{fig:examples}(b) shows the model's failure to distinguish a small motion (\eg facial expression), and simply retrieve the videos containing the face of a \textit{woman}. 
Fig.\ref{fig:examples}(c) shows that our model successfully catches the features of \textit{horses} in both web videos and 3D animation, and correctly select the highest ranking video by focusing on the word \textit{stall}.
In Fig.\ref{fig:examples}(d), although the model can retrieve relevant videos of  \textit{cooking with bowl}, it fails to find out the answer video that contains the query description of \textit{baking mix}.

\textbf{Movie multiple-choice test}.
Fig.\ref{fig:examples}(e) delivers an evidence that our model uses the whole sentence for computing matching scores, because the model successfully chooses \textcircled{\raisebox{-0.9pt}{5}} instead of \textcircled{\raisebox{-0.9pt}{1}} that shares the same phrases (\eg \textit{shakes his head}).
Fig.\ref{fig:examples}(f) is an example of focusing on a wrong video subsequence, where our model chooses the word \textit{club} by looking at a subsequence with crowded people, 
but the answer is related to another subsequence with \textit{grandmother}. 
Fig.\ref{fig:examples}(g) is an example that the model learns words in a phrase. Choice \textcircled{\raisebox{-0.9pt}{4}} can be very tempting, since it contains the word \textit{kids}, \textit{tv} and \textit{show}. But our model successfully choose the right answer by identifying that \textit{kids tv show} and \textit{kids in tv show} mean differently.
Fig.\ref{fig:examples}(h) shows that our model fails to distinguish the details.


\textbf{Movie fill-in-the-blank}.
In Fig.\ref{fig:examples}(i), the model successfully finds the answer by using both structural information of a sentence and a video (\eg \textit{door} is a likely word after \textit{shuts the}).
Fig.\ref{fig:examples}(j) is an example that the model focuses too much on the word \textit{picture} that follows the blank, instead of visual information, and  thus choose a wrong answer \textit{framed picture} rather than \textit{flash picture}.

\begin{comment}
Figure.\ref{fig:attention} illustrates visualization of attention map on Joint Semantic Tensor and subsequent layers of Convolutional Hierarchical Decoder.
We observe our model locate valid video-language joint region in attention map. 
\end{comment}


\section{Conclusion}
\label{sec:conclusion}

We proposed the Joint Sequence Fusion (JSFusion) model for measuring hierarchical semantic similarity between two multimodal sequence data.
The two key components of the model, Joint Semantic Tensor (JST) and Convolutional Hierarchical Decoder (CHD), are easily adaptable in many video-and-language tasks, including multimodal matching or video question answering. 
We demonstrated that our method significantly improved the performance of video understanding through natural language description.
Our method achieved the best performance in challenge tracks of LSMDC, 
and outperformed many state-of-the-art models for VQA and retrieval tasks on the MSR-VTT dataset. 

Moving forward, we plan to expand the applicability of JSFusion;
since our model is usable to any multimodal sequence data, we can explore other retrieval tasks of different modalities, such as videos-to-voices or text-to-human motions.
\\

\textbf{Acknowledgements.} We thank Jisung Kim and Antoine Miech for helpful comments about the model. This research was supported by Brain Research Program by National Research Foundation of Korea (NRF) (2017M3C7A1047860).  Gunhee Kim is the corresponding author. 

\clearpage

\bibliographystyle{splncs}
\bibliography{egbib}
\end{document}
