\renewcommand{\thefigure}{A.\arabic{figure}} \setcounter{figure}{0} 
\renewcommand{\thetable}{A.\arabic{table}}
\setcounter{table}{0} 

\appendix


This document provides 
dataset statistics (Section~\ref{app:sec:dataset-statistics}),
implementation details (Section~\ref{app:sec:implementation-details}),
additional experiments (Section~\ref{app:sec:experiments}),
and qualitative examples (Section~\ref{app:sec:qualitative-examples}).
We also provide the code, dataset, and an illustrative video on our project page at \href{https://imagine.enpc.fr/~ventural/covr}{imagine.enpc.fr/\textasciitilde ventural/covr}.


\startcontents[sections]
{
	\hypersetup{linkcolor=black}
	\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}
}

    


\section{Dataset statistics and analysis}
\label{app:sec:dataset-statistics}

In this section, we provide analysis on our \ourDS.
A detailed datasheet can be found at the project webpage.

\noindent\textbf{Filtering inappropriate content and vulgar language.}
We take several measures to detect semi-automatically any inappropriate
content, and remove such instances from our dataset. To achieve this, we 
use a combination of tools (such as negative sentiment and profanity detectors)
and apply them on modification texts and video captions.

We conduct a sentiment analysis on the modification texts using the \texttt{TextBlob} library~\cite{textblob}
to identify instances of negative sentiment. We find that less than 0.5\% of the dataset (about 2K instances) exhibits negative sentiment. 
Upon manual review, we identify false positives in this categorization,
including examples such as ``make it an evil pumpkin'' or
``Change him into a frustrated businessman''.
The instances detected as negative sentiment are reviewed and
260 of them are removed from the dataset. 
We ensure that the dataset does not
include any videos marked for mature content, by checking the metadata of
WebVid~\cite{bain21_frozen} provided by \cite{cleanvid}.
Finally, using the \texttt{better-profanity} library~\cite{better_profanity},
we identify approximately 2K video captions that are marked for profanity.
Upon manual inspection, we find that
there were a large number of videos displaying computer-generated visuals with those words.
We also notice false positives (e.g., misinterpretation due to context),
such as the animal cock being incorrectly identified as profanity.
The videos detected to contain profanity in their captions
are reviewed and excluded from the dataset. 

\noindent\textbf{Distribution of caption and video embedding similarities.}
As explained in 
\if\sepappendix1{Section~3.1} \else{Section~\ref{subsec:gen} }\fi
of the main paper,
we filter caption pairs with CLIP text embedding similarity  0.96 
and caption pairs with CLIP text embedding similarity  0.6, and for each caption pair, we choose the 10 video pairs with the highest CLIP visual similarity computed at the middle frame of the videos.
We also note that our cosine similarities are normalized between [0, 1].
Here, we further show the distribution of text embedding similarity in caption pairs and visual embedding similarity in video pairs in Figure~\ref{app:fig:plot-similarity}.
The distribution of video similarity scores exhibits two distinct peaks. 
The first peak corresponds to a score of approximately 0.7 and includes video pairs that are significantly dissimilar.
The second peak corresponds to a score close to 1.0 and represents video pairs 
with highly similar visual content.

\noindent\textbf{Number of words in modification texts.}
Figure~\ref{app:fig:nwords-histogram} further provides the histogram of the
number of words in the generated modification text. We observe that the majority
of texts contain 3-8 words.

\noindent\textbf{Number of triplets per target video.}
In
\if\sepappendix1{Section~3.2} \else{Section~\ref{subsec:data} }\fi
of the main paper,
we provided several statistics about our WebVid-CoVR dataset, e.g., 
on average, a target video is associated with 12.7 triplets.
However, in Figure~\ref{app:fig:ntriplets-pth2}, when visualizing 
the distribution of triplets associated with each target video, we see that
the histogram reveals that the majority of target videos are associated to only 1 or 2 triplets. 
The histogram exhibits a long tail, i.e., a small subset of target videos have a considerably larger number of triplets associated. 
These videos have captions such as ``Mountain landscape'', ``Water stream'', and ``Water river'', leading to numerous one-word difference captions associated with them.
  
\noindent\textbf{Video categories.}
We plot the distribution of video categories in Figure~\ref{app:fig:video-categories}. These categories are found using the WebVid metadata provided by \cite{cleanvid}.
We find 50\% of WebVid-CoVR videos in this metadata collection. Note more than one category can be associated with a single video (e.g., Nature and Animals/Wildlife for a video of a fish in the ocean).


\noindent\textbf{Distribution of part-of-speech (POS) tags.}
We conducted POS tagging on the modification texts within the \ourDS dataset to analyze their distribution. 
The resulting analysis reveals the average counts of different parts of speech per modification text, including Nouns, Verbs, Pronouns, Adjectives, and Adverbs. 
We plot the distribution in Figure~\ref{app:fig:video-categories},
and see that, on average, a modification text contains 1.6 nouns and 1.1 verbs, 
emphasizing the prevalent use of nouns and verbs in the dataset's modifications. 
The most frequently encountered words within each category's top 3 are as follows:
    Noun: \textit{symbol, water, forest.}
    Verb: \textit{make, turn, change.}
    Pronoun: \textit{it, them, her.}
    Adjective: \textit{green, more, black.}
    Adverb: \textit{instead, more, then.}
We also include a visualization of the verb-noun frequency heatmap in Figure~\ref{app:fig:verb-noun-heatmap}, which provides insights into the distribution of verb-noun count combinations across modification texts in our dataset. From the heatmap, we observe that over 60\% of the sentences exhibit a pattern of having one verb paired with one or two nouns.

We also conducted an analysis using POS tagging on the video {\em captions}.
Figure~\ref{app:fig:pos-transitions} visually illustrates the transition of POS tags across
the difference words in Caption 1 and Caption 2. We observe a predominant pattern of noun-to-noun
changes in our caption pairs.


\noindent\textbf{Source of noise.}
As mentioned in 
\if\sepappendix1{Section~3.2} \else{Section~\ref{subsec:data} }\fi
of the main paper,
about 22\% of the automatic collection can be considered as noisy, 
because this was the percentage of discarded triplets when manually curating the \ourDS test set. 
We expect a similar noise ratio in the training set. To inspect the noise in detail, we manually went over the triplet examples that were marked as unsuitable (therefore discarded) 
when annotating the test set. 
We marked whether the reason for discarding falls within any of the following categories, 
and computed the following percentages (normalized by the number of discarded triplets).
\begin{itemize}
    \item 35\%: The generated modification text does not describe the visual difference. Primarily attributed to either the quality of the video captions or the output generated by the MTG-LLM.
    \item 28\%: Paired videos are visually too similar.
    \item 15\%: Paired videos are visually too different.
    \item 13\%: At least one of the videos is difficult to understand/low quality.
    \item 9\%: Captions are too similar (e.g., one-word difference does not change the meaning: ``On the chairlift'' and ``Ride the chairlift'').
\end{itemize}
While the first category of errors is the largest, it is important to also note that our strict standards for the test set necessitated the discarding of many triplets that could potentially be useful for training.

\begin{figure}
  \centering
  \includegraphics[width=0.48\textwidth]{figures/supmat/text-similarity.png}\hfill
  \includegraphics[width=0.48\textwidth]{figures/supmat/video-similarity.png}
    \caption{\textbf{Text/video similarity of the caption/video pairs:} 
    Distribution of text similarity scores between caption pairs  (left) 
    and video similarity scores between video pairs  (right),
    using CLIP embeddings and cosine similarity.}
  \label{app:fig:plot-similarity}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=.5\linewidth]{figures/supmat/nwords-histogram.png}
  \caption{\textbf{Histogram of the number of words in the generated modification text:} Most modification texts have between 3 and 8 words.
  }
  \label{app:fig:nwords-histogram}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=.99\linewidth]{figures/supmat/ntriplets-pth2.png}
    \caption{\textbf{Distribution of number of triplets per target video:} 
    We display the histogram depicting the number of triplets associated with each target video in the WebVid-CoVR dataset. 
    Most target videos have 1 or 2 triplets and certain videos exhibit 
    a high number of triplets (zoomed in to the tail on the right plot), e.g., some target videos are present in over 300 triplets, highlighting the variability in modification texts.
    }
  \label{app:fig:ntriplets-pth2}
\end{figure}



\begin{figure}\centering
  \includegraphics[width=.8\linewidth]{figures/supmat/distribution_video-categories.png}
  \vspace{-0.2cm}
  \caption{\textbf{Distribution of video categories:}  
  {We plot the distribution of categories for videos in WebVid-CoVR, as provided by \cite{cleanvid} as WebVid metadata. Note that 50\% of our WebVid-CoVR videos are present in this metadata collection.
  Looking at the distribution, we observe that around 40\% and 20\% of \ourDS are videos of Nature and People, respectively.}
  }
  \label{app:fig:video-categories}
\end{figure}

\begin{figure} \centering
  \vspace{0.1cm}
  \includegraphics[width=.8\linewidth]{figures/supmat/pos-distribution.png}
  \vspace{-0.2cm}
  \caption{\textbf{Distribution of parts of speech in modification texts:}  
  Distribution of nouns, verbs, pronouns, adjectives, and adverbs in the modification text using part-of-speech (POS) tagging.
  On average, there are more than one noun and one verb per modification text.
  }
  \label{app:fig:pos-distribution}
\end{figure}

\begin{figure} \centering
  \includegraphics[width=.6\linewidth]{figures/supmat/verb-noun_frequency-heatmap.png}
  \vspace{-0.2cm}
  \caption{\textbf{Verb-noun heatmap:} 
  {This heatmap illustrates the percentage of modification texts containing specific combinations of verbs and nouns. 
    Each cell represents the frequency of a particular verb-noun combination, and the values are presented as percentages. The color intensity indicates the relative frequency of occurrence.
    We observe that over 60\% of the sentences exhibit a pattern of having one verb paired with one or two nouns.}
  }
  \label{app:fig:verb-noun-heatmap}
\end{figure}

\begin{figure} \centering
  \includegraphics[width=.8\linewidth]{figures/supmat/pos-tags-transition.png}
  \vspace{-0.2cm}
  \caption{\textbf{Transition of POS tags across the difference words between the two captions:} 
  {The visualization primarily focuses on nouns, adjectives, and verbs, which constitute a significant proportion of modifications at 87\% (comprising 65\% nouns, 13\% adjectives, and 9\% verbs). The remaining words fall into categories where the POS tagger was unable to classify the word (12\%) or adverbs (\textless 1\%).
}
  }
  \label{app:fig:pos-transitions}
\end{figure}

\section{Implementation details}
\label{app:sec:implementation-details}
We describe the dataset generation computation time (Section~\ref{app:subsec:dataset-computation-time}),
further training details (Section~\ref{app:subsec:training}),
provide the templates we use for our rule-based baseline (Section~\ref{app:subsec:rule-based}),
and details about our MTG-LLM finetuning and inference (Section~\ref{app:subsec:mtg-llm}).

\subsection{Dataset generation computation time}
\label{app:subsec:dataset-computation-time}
We outline the detailed computation time for each step of the dataset generation. The computation times below are obtained using a \textbf{single} NVIDIA RTX A6000, but it is important to note that most of the processes can be parallelized, which would significantly reduce the wallclock time required. In practice, we used 2 GPUs. 
\begin{itemize}
    \item \textbf{Text embedding extraction:} We extracted text embeddings from 2 million distinct captions out of a total of 2.4 million video-caption pairs. This process completed in less than 2 hours.
    \item \textbf{Caption similarity search:} To identify captions with one-word differences, we employed the \textit{faiss} library~\cite{faiss_johnson2019billion} to select the 100 closest captions, avoiding the need to compare each caption against the entire set of 2 million captions. This optimization significantly reduced the search time, resulting in 2.5 hours.
    \item \textbf{Text similarity filtering:} Thanks to the precomputed text embeddings, the text similarity filtering step incurred no additional time overhead. All the text filtering processes were completed in less than 5 minutes, even on a large pool of 1.2 million captions.
    \item \textbf{Video similarity computation:} To filter by video similarity, we extracted the middle frame from approximately 135,000 videos and computed CLIP embeddings. This step takes approximately 3 hours.
    \item \textbf{MTG-LLM model finetuning:} Finetuning for 715 examples takes less than 10 minutes. Note that the time required to finetune the MTG-LLM model is independent of the number of CoVR triplets we generate.
    \item \textbf{Modification text generation:} This is the most time-consuming stage of the pipeline. It takes around 24 hours to process the 1.6 million caption pairs.
\end{itemize}



\subsection{Training details}
\label{app:subsec:training}
Here, we provide implementation details
in addition to
\if\sepappendix1{Section~4.1} \else{Section~\ref{subsec:setup} }\fi
of the main paper. 
In terms of the optimization algorithm, 
we utilize AdamW~\cite{loshchilov2017decoupled}.
For our MTG-LLM, we finetune for one epoch with a batch size of 128 and a learning of  that is warmed up linearly for the first  steps and then kept constant.
For our CoVR model, keeping the visual backbone frozen largely improves the efficiency of the training process:
an epoch on the CIRR dataset takes 4 minutes with a frozen backbone and 25 minutes with a finetuned backbone, while leading to similar performance.
During the training process, 
we employ several image data augmentations. 
These transformations include a random resized crop, 
where the input image is resized to a resolution of . 
Additionally, we apply a random horizontal flip and random adjustments to contrast, brightness, sharpness, translation, and rotation.
We use a weight decay of 0.05 and an initial learning rate of  that is decayed to 0 following a cosine schedule over 10 epochs.

\subsection{List of rule-based templates}
\label{app:subsec:rule-based}

In the ablation studies 
\if\sepappendix1{(Section~4.4} \else{(Section~\ref{subsec:ablations} }\fi
of the main paper),
we introduced a rule-based MTG baseline.
Here, in Table~\ref{tab:rule-based-templates}, we show the templates used for the rules.
We refer to Section~\ref{app:subsec:qualitative_mtg} (Table~\ref{tab:rule-based-comparision})
for qualitative comparison with our finetuned MTG-LLM.

\begin{table}\centering
\caption{\textbf{Rule-based templates:}
For our rule-based MTG baseline, we randomly
choose one of the below templates during training.
}
    \begin{tabular}{l}
    \toprule
    \textit{Remove  }\\
    \textit{Take out  and add  } \\
    \textit{Change  for  } \\
    \textit{Replace  with  } \\
    \textit{Replace  by  } \\
    \textit{Replace  with  } \\
    \textit{Make the  into  } \\
    \textit{Add  } \\
    \textit{Change it to  } \\
    \bottomrule
\end{tabular}
\label{tab:rule-based-templates}
\end{table} 
\subsection{Generating a modification text from paired captions with MTG-LLM}
\label{app:subsec:mtg-llm}
As described in
\if\sepappendix1{Section~3.1} \else{Section~\ref{subsec:gen} }\fi
of the main paper,
we use top-k sampling at inference for the MTG-LLM. 
Specifically, we use  and .
We further give details about the text input-output format for the MTG-LLM.
At training, we form the input prompt by concatenating captions and target and adding delimiters and stop sequences similar to InstructPix2Pix~\cite{brooks2022instructpix2pix}.
In detail, given a caption pair  and a corresponding target , we concatenate them and add a separator in the following way: ,
where  is \texttt{\textbackslash n\&\&\textbackslash n}.

For instance, the model takes as input:
\begin{lstlisting}[breaklines, backgroundcolor = \color{backcolour}]
    Clouds in the sky\n&&\nAirplane in the sky \n\n### Response:
\end{lstlisting}
and is trained to generate the response:
\begin{lstlisting}[breaklines, backgroundcolor = \color{backcolour}]
    Clouds in the sky\n&&\nAirplane in the sky \n\n### Response: Add an airplane
\end{lstlisting}
At inference, we simply leave the response empty, and let the model autoregressively generate a modification text.

As mentioned in
\if\sepappendix1{Section~3.1} \else{Section~\ref{subsec:gen} }\fi
of the main paper, we add 15 manually prepared text triplets to the existing 
700 text triplets from \cite{brooks2022instructpix2pix} used for training.
The motivation is to address specific
CoVR cases not present in the original set of triplets, such as \textit{``remove clouds and reveal only sky''} 
given input captions \textit{``Clouds timelapse''} and \textit{``Sky timelapse''}.
We show these 15 samples in Table~\ref{tab:added-examplges-mtg-llm}.


\begin{table}\caption{\textbf{Added examples to the MTG-LLM training:}
We add the below 15 examples to the set of 700 text triplets from \cite{brooks2022instructpix2pix}.
}
    \begin{tabular}{ll}
    \toprule
    Caption & Clouds in the sky \\
Caption & Airplane in the sky \\
Target output & Add an airplane \\
\midrule
Caption & Woman with the tablet computer sitting in the city. \\
Caption & Woman with tablet computer sitting in the park. \\
Target output & In the park \\
\midrule
Caption & Walking swan \\
Caption & White swan \\
Target output & Change color to white \\
\midrule
Caption & Child playing on beach, sea waves view, girl spinning on coastline in summer 4k \\
Caption & Child playing on beach, sea waves view, girl running on coastline in summer 4k \\
Target output & Make her spin \\
\midrule
Caption & Aerial view of forest \\
Caption & Aerial view autumn forest \\
Target output & Change season to autumn \\
\midrule
Caption & Palm tree in the wind \\
Caption & Palm trees in the wind \\
Target output & Add more palm trees \\
\midrule
Caption & Schoolgirl talking on the phone \\
Caption & Girl talking on the phone \\
Target output & Make her older \\
\midrule
Caption & Clouds timelapse \\
Caption & Sky timelapse \\
Target output & remove clouds and reveal only sky \\
\midrule
Caption & Aerial view of a sailboat anchored in the mediterranean sea, vathi, greece. \\
Caption & Aerial view of two sailboat anchored in the mediterranean sea, vathi, greece. \\
Target output & Add one sailboat \\
\midrule
Caption & France flag waving in the wind. realistic flag background. looped animation background. \\
Caption & Italian flag waving in the wind. realistic flag background. looped animation background. \\
Target output & Swap the flag for an italian one \\
\midrule
Caption & Woman jogging with her dog in the park \\
Caption & Woman playing with her dog in the park. \\
Target output & Stop jogging and make them play \\
\midrule
Caption & Oil Painting Reproductions of by humans william-glackens \\
Caption & Oil Painting Reproductions of zombies by william-glackens \\
Target output & Replace the humans with zombies \\
\midrule
Caption & The girl who loved the sea by banafria \\
Caption & The girl, wearing a hat, who loved the sea by banafria \\
Target output & Put a hat on her \\
\midrule
Caption & famous painting Paris, a Rainy Day of Gustave Caillebotte \\
Caption & famous painting Paris, a Sunny Day of Gustave Caillebotte \\
Target output & Change it to more pleasant weather \\
\midrule
Caption & Bee on purple flower \\
Caption & Bee on a flower \\
Target output & Change color of the flower \\
    \bottomrule
\end{tabular}
\label{tab:added-examplges-mtg-llm}
\end{table}


 
\section{Additional experiments}
\label{app:sec:experiments}
We provide additional experiments, reporting CoVR results obtained by training on data 
generated with prompting (i.e., without finetuning) the LLM (Section~\ref{app:subsec:llm}),
results when changing the visual query from an image to a video (Section~\ref{app:subsec:video-query}),
and varying the pretrained BLIP model (Section~\ref{app:subsec:pretrained-blip-models}).

\subsection{Prompting versus finetuning the MTG-LLM}
\label{app:subsec:llm}
Here, we justify why we finetuned Llama as opposed to simply prompting it
without any training. 
For prompting, we prepend few-shot examples of pairs of captions and desired generated texts, before 
adding the two captions in question. 
In particular, we use the following sentence:
\begin{lstlisting}[breaklines, backgroundcolor = \color{backcolour}]
    Clouds in the sky&&Airplane in the sky-> Add an airplane\n
    Aerial view of forest&&Aerial view autumn forest-> Change season to autumn\n
    Clouds timelapse&&Sky timelapse-> remove clouds and reveal only sky\n
    Aerial view of a sailboat anchored in the mediterranean sea.&&Aerial view of two sailboat anchored in the mediterranean sea.-> Add one sailboat\n
\end{lstlisting}
Then, we concatenate our two captions for which we wish to generate a modification text.
Table~\ref{tab:prompting-mtg-llm} shows that finetuning the MTG-LLM for generating the training data is much more effective than prompting it without finetuning, as measured by CoVR performance on \ourDSm and CoIR performance on CIRR. 
This is also consistent with our qualitative observations: we found that the LLM struggles to perform the modification text generation without finetuning (see Table~\ref{tab:rule-based-comparision} in the next section).


\begin{table}\caption{\textbf{Prompting versus finetuning LLM:} 
    \label{tab:prompting-mtg-llm}
    We compare our finetuned model (MTG-LLM) to a prompting baseline (see Section~\ref{app:subsec:llm}) and observe important gains in the downstream performance of the model trained on the generated data.
    }
    \centering
    \resizebox{.8\linewidth}{!}{
    \begin{tabular}{l|cccc|cccc}
        \toprule
        & \multicolumn{4}{c|}{\ourDSm}  & \multicolumn{4}{c}{CIRR}\\
        Model & R@1 & R@5 & R@10 & R@50 & R@1 & R@5 & R@10 & R@50 \\ 
        \midrule
        Prompting & 51.33 & 76.68 & 85.13 & 96.71 & 34.94 & 63.04 & 74.02 & 89.83 \\
        \rowcolor{ourcolor!64}
        Finetuning & \textbf{53.13} & \textbf{79.93} & \textbf{86.85} & \textbf{97.69} & \textbf{38.48} & \textbf{66.70} & \textbf{77.25} & \textbf{91.47}  \\
        \bottomrule
    \end{tabular}
    }
\end{table}

 

\subsection{Video query for CoVR}
\label{app:subsec:video-query}
As noted in
\if\sepappendix1{Section~3 } \else{Section~\ref{sec:method} }\fi
of the main paper,
we focus on image queries in this paper. This was because querying with an image
has arguably more applications for realistic search scenarios.
Here, we explore the setup of using a \textit{video} as the visual query instead of an image query.
We can do this since our dataset consists of video-text-video triplets.
To encode a query video, we sample 5 equally-spaced frames and compute visual
embeddings for each frame using the BLIP image encoder.
We then average the per-frame
embeddings and forward it through the BLIP cross-attention layers to obtain a multimodal query embedding .
Note that we keep the target video representation fixed to 15 frames with weighted embedding averaging
as described in
\if\sepappendix1{Section~3.3 } \else{Section~\ref{subsec:training} }\fi
of the main paper.
As seen in Table~\ref{tab:video-query}, using 5 query frames leads to
similar performance to using the middle frame.

\begin{table}
    \caption{\textbf{Querying with a video:}
        We report results on \ourDSm by using multiple frames from the query {\em video}.
        Recall that the rest of the paper investigates the setup where the middle video frame is used as an {\em image} query.
        To keep the computational complexity low, we only use 5 query video frames (uniformly sampled
        throughout the video). The number of target video frames remains unchanged as 15.
        The performance is similar to the image query setup, with marginal increase.
    }
    \label{tab:video-query} \centering
    \resizebox{.65\linewidth}{!}{
    \begin{tabular}{l|cccc}
        \toprule
        Visual query & R@1 & R@5 & R@10 & R@50 \\ 
        \midrule
        Image (middle frame)     & 53.17 & 79.93 & 86.85 & 97.69 \\
        Video (5 uniform frames) & 53.91 & 79.85 & 87.09 & 97.42 \\
    \bottomrule
    \end{tabular}
    }
\end{table} 

\subsection{Variants of pretrained BLIP backbones}
\label{app:subsec:pretrained-blip-models}
All experiments in this paper
are performed with the BLIP model~\cite{BLIP} finetuned on COCO~\cite{coco}. Here, we include
experiments when changing this backbone with other pretrained BLIP variants.
Specifically, we use the BLIP model without COCO finetuning (BLIP base),
and the BLIP model finetuned on Flickr30k~\cite{Plummer2015Flickr30k}. For this experiment
(as in the last row of
\if\sepappendix1{Table~2 } \else{Table~\ref{tab:video} }\fi
of the main paper),
we use pretrained cross-attention layers of BLIP as our multimodal combined representation,
and finetune them on \ourDS with 15-frame target video embeddings.
In Table~\ref{tab:pretrained-blip-models}, we observe that the BLIP model finetuned on COCO has the highest performance.

\begin{table} \setlength\tabcolsep{5pt}
    \caption{\textbf{Variants of pretrained BLIP backbones:} 
    We compare the BLIP model without finetuning (base), BLIP finetuned on Flickr30k, and BLIP finetuned on COCO (the one used in the rest of the paper)~\cite{BLIP}.
    For this experiment, we finetune the models on \ourDS using the cross-attention layers of
    BLIP as the fusion method, and 15 frames for the target video as in the last row
    of 
    \if\sepappendix1{Table~2.} \else{Table~\ref{tab:video}.}\fi
    }
    \centering
    \resizebox{0.54\linewidth}{!}{
    \begin{tabular}{l|cccc}
        \toprule
        Backbone  & R@1 & R@5 & R@10 & R@50 \\ 
        \midrule
        BLIP Base & 50.74 & 78.91 & 86.23 & 97.34 \\
        BLIP Flickr30k & 52.50 & 79.46 & 86.70 & \textbf{97.77} \\
        \rowcolor{ourcolor!64}
        BLIP COCO  & \textbf{53.13} & \textbf{79.93} & \textbf{86.85} & {97.69} \\
    \bottomrule
    \end{tabular}
}
\label{tab:pretrained-blip-models}
\end{table} 


\section{Qualitative analysis}
\label{app:sec:qualitative-examples}
In this section, we provide
examples of caption filtering (Section~\ref{app:subsec:qualitative_filtering}),
qualitative comparison between different MTG approaches (Section~\ref{app:subsec:qualitative_mtg}),
qualitative examples of our \ourDS triplets (Section~\ref{app:subsec:qualitative_triplets}),
samples from our manual test set annotation process (Section~\ref{app:subsec:manual}),
qualitative CoVR results on \ourDSm (Section~\ref{app:subsec:recall-webvid})
and CoIR results on CIRR (Section~\ref{app:subsec:recall-cirr}).

\subsection{Examples of filtered captions}
\label{app:subsec:qualitative_filtering}
As described in 
\if\sepappendix1{Section~3.1} \else{Section~\ref{subsec:gen} }\fi
of the main paper,
we employ a filtering process to select paired captions 
that facilitate the generation of meaningful training data.
In this section, we provide examples of the filtered captions.

\noindent\textbf{Filtering template captions.} 
Upon analyzing the paired captions, 
we observed that a significant portion of the pairs originated 
from a small set of template captions. 
Out of 1.2M distinct caption pairs, 
approximately 719k () were generated from these template captions. 
The following examples showcase some of these template captions:

\begin{itemize}
    \item \textbf{Abstract:} \textit{Abstract color movement tunnel, Abstract color nature background, Abstract color smoke flowing on white background, Abstract colorful paint ink spread explode, Abstract colorful pattern background, Abstract colorful red cement wall background or texture. the camera moves up, Abstract colorful satin background animation, Abstract colorful shiny bokeh background., Abstract colorful smoke on black background,} etc
    \item \textbf{Background:} \textit{Abstract background, Animated backgrounds, Animation, background., Aquarium background, Artistic background, Aurora background, Balloons background, Basketballs background, Beach background, Bluebell background, Bright background, Brush background, Bubbles background, Bubbly background, Celebrate background, Celebratory background, Cg background, Christmas background, Christmas background, Circles background, Color background, Colored background, Colorful background, Colorfull background,}, etc.
    \item \textbf{Concept:} \textit{Brazil high resolution default concept, Brazil high resolution dollars concept, Businessman with advertising hologram concept, Businessman with algorithm hologram concept, Businessman with automation hologram concept, Businessman with bitcoin hologram concept, Businessman with branding hologram concept, Businessman with public relations hologram concept, Close up of an eye focusing on a freelance concept on a futuristic screen., Coins fall into piggy bank painted with flag of ghana. national banking system or savings related conceptual 3d animation, Communication concept, Communication network concept., Communication team concept, Concept of connection, Concept of dancing at disco party. having fun with friends., Concept of education, Concept of geography, Cyber monday concept}, etc 
    \item \textbf{Flag:} \textit{Flag of america, Flag of andorra, Flag of aruba, Flag of austria, Flag of azerbaijan, Flag of bahrain, Flag of belarus, Flag of belize, Flag of black, Flag of bolivia, Flag of brazil, Flag of bulgaria, Flag of cameroon, Flag of canada,} etc.
\end{itemize}

\noindent\textbf{Filtering caption pairs with high or low similarity.} 
To ensure the generation of meaningful modifications, 
we further refine the selection of caption pairs by filtering out 
those with excessively high or low similarity. 
Caption pairs with highly similar meanings may result 
in trivial or unnoticeable modifications. 
Conversely, pairs with significant dissimilarity can lead to large visual 
differences that are difficult to describe accurately.
We show below some of the filtered captions based on the CLIP text embedding cosine similarity.
\begin{itemize}
    \item \textbf{High similarity:}  of the pairs have CLIP text similarity above 0.96.
    \begin{itemize}
        \item Close-up of a tree with green leaves and \underline{sunlight}
        \item Close-up of a tree with green leaves and \underline{sunshine}
    \end{itemize}
    \begin{itemize}
        \item Businessman \underline{speaking} on the phone
        \item Businessman \underline{talking} on the phone
    \end{itemize}
    \begin{itemize}
        \item Boat on \underline{a} sea
        \item Boat on \underline{the} sea
    \end{itemize}
    \item \textbf{Low similarity:}  of the pairs have CLIP text similarity below 0.60.
    \begin{itemize}
        \item \underline{Leaves} close-up
        \item \underline{Peacock}, close-up
    \end{itemize}
    \begin{itemize}
        \item Moon \underline{jellyfish}
        \item Moon \underline{night}
    \end{itemize}
    \begin{itemize}
        \item Close up of a \underline{lynx}
        \item Close up of a \underline{milkshake}
    \end{itemize}
\end{itemize}

\noindent\textbf{Exclusion of digit differences and out-of-vocabulary words.}
In order to maintain the high quality and coherence of the generated modification text, 
we apply additional filtering criteria. 
Specifically, we exclude caption pairs where the differences between captions 
are numerical digits (often representing dates) 
or involve out-of-vocabulary words (using the python libraries wordfreq and enchant) that may hinder the generation process.

\begin{itemize}
    \item \textbf{Difference between the captions is a digit:} Approximately  of the pairs. 
    \begin{itemize}
        \item \underline{23.09.2015} navigation on the moscow river
        \item \underline{07.08.2015} navigation on the moscow river.	
    \end{itemize}
    \begin{itemize}
        \item Light leaks element \underline{190}
        \item Light leaks element \underline{215}
    \end{itemize}
    \begin{itemize}
        \item Pure silver, shape of granules of pure silver each one is unique \underline{44} (2)
        \item Pure silver, shape of granules of pure silver each one is unique \underline{95} (2)
    \end{itemize}
   \item \textbf{Difference in one of the captions has an out-of-vocabulary word:} Approximately  of the pairs. 
    \begin{itemize}
        \item Businessman writing on hologram desk tech word- bitcoin
        \item Businessman writing on hologram desk tech word- crm
    \end{itemize}
    \begin{itemize}
        \item Mitomycin-c - male doctor with mobile phone opens and touches hologram active ingrident of medicine
        \item Oxazepam - male doctor with mobile phone opens and touches hologram active ingrident of medicine
    \end{itemize}
    \begin{itemize}
        \item Blue forget-me-nots
        \item Blue galaxy
    \end{itemize}
\end{itemize}


\begin{table} \caption{\textbf{Comparison between modification text generation approaches:}
    We provide qualitative examples for a pair of captions, and three methods to generate
    modification text: (i) rule-based, (ii) prompting-based, (iii) our MTG-LLM finetuning.
    Rule-based method is limited, for example in the case where the difference text is a preposition (last row),
    whereas the prompting-based method is prone to hallucinating (e.g., `remove iceberg', `change the pose of the runner').
    Our approach tends to be the most robust across cases.}
    \label{tab:rule-based-comparision}
    \begin{tabular}{ll}
    \toprule
    Caption & \textit{Happy} girl dancing \\
    Caption & \textit{Beautiful} girl dancing \\
    Rule-based & Make the \textit{Happy} into \textit{Beautiful} \\
    Prompting LLM & Change girl \\
    MTG-LLM & Make her more \textit{beautiful} \\
    \midrule
    Caption & Black \textit{bird} \\
    Caption &  black \textit{bear} \\
    Rule-based & Add \textit{bear} \\
    Prompting LLM & Remove animal \\
    MTG-LLM & Turn it into a \textit{bear} \\
    \midrule
    Caption & \textit{Autumn} landscape in the mountains. \\
    Caption & \textit{Winter} landscape in the mountains \\
    Rule-based & Remove \textit{Autumn} \\
    Prompting LLM & Change scene to winter. \\
    MTG-LLM & Change to \textit{winter} \\
    \midrule
    Caption & Flying over the \textit{ice} \\
    Caption & Flying over the \textit{mountains} \\
    Rule-based & Take out \textit{ice} and add \textit{mountains} \\
    Prompting LLM & Remove iceberg \\
    MTG-LLM & Move the \textit{ice} to the \textit{mountains} \\
    \midrule
    Caption & \textit{Dandelion} field \\
    Caption & \textit{Rice} field  \\
    Rule-based & Change \textit{dandelion} for \textit{rice} \\
    Prompting LLM & Change the season \\
    MTG-LLM & Turn it into a \textit{rice} field. \\
    \midrule
    Caption & \textit{Happy} woman \\
    Caption & \textit{Running} woman \\
    Rule-based & Change it to \textit{running} \\
    Prompting LLM & Remove the girl and change the pose of the runner \\
    MTG-LLM & Make her a runner \\
    \midrule
    Caption & Woman working on cellphone at \textit{night}  \\
    Caption & Woman working on cellphone at \textit{outdoor} \\
    Rule-based & Replace \textit{night} with \textit{outdoor} \\
    Prompting LLM & Change the woman and add one cellphone \\
    MTG-LLM & Make it an \textit{outdoor} \\
    \midrule
    Caption & Barber cuts the hair of the client with \textit{clipper}. \\
    Caption & Barber cuts the hair of the client with \textit{trimmer} \\
    Rule-based & Take out \textit{clipper} and add \textit{trimmer} \\
    Prompting LLM & Add or change the hairdresser's tool \\
    MTG-LLM & Change to a \textit{trimmer} \\
    \midrule
    Caption & Aerial shot \textit{of} a lake. \\
    Caption & Aerial shot \textit{above} a lake  \\
    Rule-based & Remove \textit{of} \\
    Prompting LLM & Add a lake to the sequence \\
    MTG-LLM & make it an aerial shot \textit{of} a lake \\
    \bottomrule
\end{tabular}
\end{table}


 
\subsection{Qualitative comparison of MTG approaches}
\label{app:subsec:qualitative_mtg}
In 
\if\sepappendix1{Section~4.4} \else{Section~\ref{subsec:ablations} }\fi
of the main paper and Section~\ref{app:subsec:llm}, we show that finetuning our MTG-LLM works better
than a rule-based approach and than few-shot prompting of the LLM.
In this section, we provide a qualitative comparison of three different methods 
for generating modification text: (i) rule-based, (ii) prompting-based, and (iii) our MTG-LLM finetuning. 
We present examples of paired captions and the corresponding modification texts generated by each method in Table~\ref{tab:rule-based-comparision}.

\textbf{Rule-based method.} 
The rule-based method relies on predefined rules to generate modification text. 
We illustrate an example limitation in the last row of Table~\ref{tab:rule-based-comparision},
where the difference text is simply a preposition (i.e., `of' vs `above'), and the modification
text becomes `Remove of'.
The rule-based method performs well when the modifications follow a specific pattern, 
but it may struggle with more complex modifications (e.g., `tree' vs `trees' should generate `add more trees' for plurality).

\textbf{Prompting LLM.} 
The prompting-based method involves using a pretrained language model without finetuning.
However, this method is prone to hallucinations and may generate modification 
text that does not accurately represent the intended difference. 
For example, in the second example, 
the prompting LLM suggests removing the term `animal' instead of replacing `bird' with `bear'.

\textbf{MTG-LLM (Our approach).} 
Our MTG-LLM approach utilizes a large language model finetuned on a manually annotated dataset 
specifically for modification text generation. 
It tends to be the most robust across different cases.

\subsection{Training triplet examples}
\label{app:subsec:qualitative_triplets}
Figures~\ref{app:fig:triplet-12},~\ref{app:fig:triplet-34}, and~\ref{app:fig:triplet-5} 
all show examples of triplets generated using our automatic dataset creation.
These examples demonstrate the effectiveness of our approach 
in generating coherent modification texts for paired videos.
This capability serves as a form of data augmentation and increasing the diversity in the training set.
In Figure~\ref{app:fig:triplet-multi}, we show that the dataset is not composed by pairs only,
as there are many captions that have many relations between them.
Furthermore, in Figure~\ref{app:fig:triplet-multi-12} we show cases where 
a single caption is associated with multiple videos. 
This scenario allows us to generate multiple triplets by leveraging the 
diverse visual content captured in different videos.
The triplets shown in the aforemention figures exhibit a wide range of variations, 
encompassing different themes such as
emotions, food, actions, camera edits, gender changes, and time of the day. 

\subsection{Manual test set annotation}
\label{app:subsec:manual}
In this section, we further describe the process of manually annotating the test set
for our \ourDSm CoVR benchmark, previously discussed in
\if\sepappendix1{Section~3.2} \else{Section~\ref{subsec:data} }\fi
of the main paper.
The annotation process involves presenting the annotator 
with generated modification texts from three different runs of MTG-LLM, 
along with three frames each from the query and target videos. The annotator's task is to evaluate the quality of the modification texts and the suitability of the videos for the CoVR task.

A total of 3.1K triplets were shown for annotation.
In Figure~\ref{app:fig:manual-test-correct} and Figure~\ref{app:fig:manual-test-incorrect}, 
we present 10 examples that were considered correct during the annotation, along with the chosen modification texts (marked with a checkmark). 
These examples demonstrate successful modification texts and appropriate video content for the CoVR task.

On the other hand, in Figure~\ref{app:fig:manual-test-incorrect}, 
we show 8 examples that were discarded during the annotation. 
These examples were rejected either because the modification texts were incorrect 
or because the videos were deemed unsuitable for the CoVR task due to being either 
too similar ({e.g., bottom left, both videos are showing the same coffe with almost no modification}) or too incoherent ({e.g., top right example ``Make the water a river''}).

\subsection{Qualitative CoVR results on \ourDSm}
\label{app:subsec:recall-webvid}
In Figure~\ref{app:fig:recall-webvid-covr}, we show qualitative CoVR results
on our manually verified \ourDSm set. We observe that top ranked
video frames have high visual and semantic similarity with the queries even
when not corresponding to the ground truth (marked with a green border).

\subsection{Qualitative CoIR results on the CIRR benchmark}
\label{app:subsec:recall-cirr}
In Figure~\ref{app:fig:recall-cirrr}, we demonstrate qualitative CoIR results
of our models trained only on \ourDS (ZS) and the one further finetuned on CIRR training set (Sup.),
tested on the CIRR {test set. We observe promising retrieval quality for both models.}


\begin{figure}\centering
  \includegraphics[width=.98\linewidth]{figures/supmat/triplets/triplet-example-3.pdf}
  \includegraphics[width=.98\linewidth]{figures/supmat/triplets/triplet-example-4.pdf}
  \caption{\textbf{Examples of generated triplets:} 
  {We illustrate triplet samples (one per row) generated using our automatic dataset creation methodology. 
  Each sample consists of two videos with their corresponding captions (at the bottom of each video)
  and the generated modification text using our MTG-LLM (in purple).}
  }
  \label{app:fig:triplet-12}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=.98\linewidth]{figures/supmat/triplets/triplet-example-5.pdf}
  \includegraphics[width=.98\linewidth]{figures/supmat/triplets/triplet-example-6.pdf}
  \caption{\textbf{Examples of generated triplets (ctd)} 
  }
  \label{app:fig:triplet-34}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=.98\linewidth]{figures/supmat/triplets/triplet-example-7.pdf}
  \caption{\textbf{Examples of generated triplets (ctd)} 
  }
  \label{app:fig:triplet-5}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=.98\linewidth]{figures/supmat/triplets/multi-example.pdf}
  \caption{\textbf{Generated triplets from multiple similar captions:} We can train with as many triplets 
  as pairs of captions with one word difference by generating modification texts using our
  trained MTG-LLM:
  \colorbox{bluecolor}{\textit{she is thinking}}, 
  \colorbox{bluecolor}{\textit{Have her look happy}}, 
  \colorbox{yellowcolor}{\textit{Make the businesswoman pregnant}}, 
  \colorbox{yellowcolor}{\textit{make her blonde}}, 
  \colorbox{yellowcolor}{\textit{make her multi-ethnic}}, 
  \colorbox{yellowcolor}{\textit{Make the woman pregnant}}, etc.
  }
  \label{app:fig:triplet-multi}
\end{figure}


\begin{figure}\centering
  \includegraphics[width=.98\linewidth]{figures/supmat/triplets/triplet-multi-example-1.pdf}
  \includegraphics[width=.98\linewidth]{figures/supmat/triplets/triplet-multi-example-2.pdf}
  \caption{\textbf{Generated triplets with multiple videos:} 
  In cases where there are several videos with the same caption,
  we can generate multiple triplets by leveraging the multiple videos. 
  It can be seen as a way of data augmentation.}
  \label{app:fig:triplet-multi-12}
\end{figure}

\begin{figure}
    \centering
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-1.pdf}}\hfill 
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-12.pdf}}\\\vspace{0.2cm}
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-3.pdf}}\hfill
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-4.pdf}}\\\vspace{0.2cm}
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-5.pdf}}\hfill
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-6.pdf}}\\\vspace{0.2cm}
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-7.pdf}}\hfill
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-8.pdf}}\\\vspace{0.2cm}
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-9.pdf}}\hfill
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-10.pdf}}
    \caption{\textbf{Manual annotation examples (kept):} We show samples from \ourDSm which are automatically mined triplets that are marked as correct during the annotation process. Each sample consists of two videos and a set of modification text options (in between each video pair). 
    The chosen modification text is indicated by a checkmark.}
    \label{app:fig:manual-test-correct}
\end{figure}

\begin{figure}
    \centering
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-13.pdf}}\hfill
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-14.pdf}}\\\vspace{0.2cm}
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-15.pdf}}\hfill
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-16.pdf}}\\\vspace{0.2cm}
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-17.pdf}}\hfill
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-18.pdf}}\\\vspace{0.2cm}
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-19.pdf}}\hfill
    {\includegraphics[width=0.48\textwidth]{figures/supmat/manual-test-set/supmat-manual-test-set-20.pdf}}
    \caption{\textbf{Manual annotation examples (discarded):} We show automatically mined triplets that are discarded during the annotation process. Discarded texts include 
    videos that are too similar (bottom left), too dissimilar (bottom right), or have bad modification texts (top left).
    }
    \label{app:fig:manual-test-incorrect}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=.98\linewidth]{figures/supmat/recalls-webvid/recall-webvid-covr-1.pdf}
  \includegraphics[width=.98\linewidth]{figures/supmat/recalls-webvid/recall-webvid-covr-2.pdf}
  \includegraphics[width=.98\linewidth]{figures/supmat/recalls-webvid/recall-webvid-covr-3.pdf}
  \caption{\textbf{Qualitative CoVR results on \ourDSm:} We display the input image and modification text queries on the left, along with the top 3 retrieved videos by our model on the right. 
  Ground-truth is denoted with a green border.  
  }
  \label{app:fig:recall-webvid-covr}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=.98\linewidth, trim=0 18cm 0 0, clip]{figures/supmat/recalls-cirr/recall-cirr-1.pdf}
  \includegraphics[width=.98\linewidth]{figures/supmat/recalls-cirr/recall-cirr-2.pdf}
  \includegraphics[width=.98\linewidth]{figures/supmat/recalls-cirr/recall-cirr-3.pdf}
  \caption{\textbf{Qualitative CoIR results on CIRR:} Given a query image and a modification text, we show our top retrieved videos of our zero-shot (ZS) model trained with \ourDS and the model finetuned on CIRR ground-truth supervision (Sup.).
  }
  \label{app:fig:recall-cirrr}
\end{figure}