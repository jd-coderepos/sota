\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}








\usepackage[final]{neurips_2020}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage[caption=false]{subfig}
\usepackage{todonotes}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{amsmath,amsthm}
\usepackage{wrapfig}

\definecolor{orange}{rgb}{1.0, 0.6, 0.2}
\definecolor{darkblue}{rgb}{0.0, 0.0, 1.0}
\definecolor{darkgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{darkred}{rgb}{0.8, 0.0, 0.0}

\newcommand{\lss}{\mathcal{L}}
\newcommand{\lssh}{\mathcal{H}}
\newcommand{\acc}{\textit{Acc }}

\newcommand{\ind}[1]{\nu_{#1}}
\newcommand{\indr}[1]{\rho_{#1}}


\newcommand{\sacc}{\textit{SAcc}}
\newcommand{\mlacc}{\textit{MLAcc}}
\newcommand{\hloss}{\textit{HLoss}}
\newcommand{\fone}{\textit{F1}}
\newcommand{\module}{\text{MCM}}
\newcommand{\loss}{\text{MCLoss}}
\newcommand{\system}[1]{C-HMCNN(#1)}
\newcommand{\lmlp}{{HMC-LMLP}}
\newcommand{\ens}{{Clus-Ens}}
\newcommand{\hmcnf}{{HMCN-F}}
\newcommand{\hmcnr}{{HMCN-R}}
\newcommand{\awx}{AWX}
\newcommand{\auprc}{}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{rem}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\title{Coherent Hierarchical Multi-Label\\ Classification Networks}





\author{Eleonora Giunchiglia  \\
  Department of Computer Science\\
  University of Oxford, UK\\
  \texttt{eleonora.giunchiglia@cs.ox.ac.uk} \\
   \And
   Thomas Lukasiewicz \\
   Department of Computer Science \\
   University of Oxford, UK \\
   \texttt{thomas.lukasiewicz@cs.ox.ac.uk} \\
}

\begin{document}


\maketitle


\begin{abstract}
Hierarchical multi-label classification (HMC) is a challenging classification task extending standard multi-label classification problems by imposing a hierarchy constraint on the classes. 
In this paper, we propose \system{}, a novel approach for HMC problems, which, given a network  for the underlying multi-label classification problem, exploits the hierarchy information in order to produce  predictions coherent with the constraint and improve performance. 
We conduct an extensive experimental analysis 
showing the superior performance of \system{} when compared to state-of-the-art models. 

\end{abstract}



\section{Introduction}

Multi-label classification is a standard machine learning problem in which an object can be associated with multiple labels. 
A hierarchical multi-label classification (HMC) problem is defined as a multi-label classification problem in which  classes are hierarchically organized as a tree or as a directed acyclic graph (DAG), and in which every prediction must be coherent, i.e.,
respect the hierarchy constraint.
The {\sl hierarchy constraint} states that a datapoint belonging to a given class must also belong to all its ancestors in the hierarchy.  HMC problems naturally arise in many domains, such as image classification~\cite{imagenet,dimitrovski2008,dimitrovski2011}, text categorization~\citep{klimt2004,lewis04,rouso2006}, and functional genomics~\citep{barutcuoglu2006,clare2003,vens2008}. They are very challenging for two main reasons: (i) they are normally characterized by a great class imbalance, because the number of datapoints per class is usually much smaller at deeper levels of the hierarchy, and (ii) the predictions must be coherent. Consider, e.g., the task proposed in~\cite{dimitrovski2008}, where a radiological image has to be annotated with an IRMA code, which specifies, among others, the biological system examined. In this setting, we expect to have many more ``abdomen'' images  
 than ``lung'' images, making the label ``lung'' harder to predict. Furthermore, the prediction ``respiratory system, stomach'' should not be possible given the hierarchy constraint stating that ``stomach'' belongs to ``gastrointestinal system''.
 While most of the proposed methods
 directly output predictions that are coherent with the hierarchy constraint (see, e.g., \citep{kwok2011,masera2018}), there are models that allow incoherent predictions and, at inference time,  require an additional post-processing step to ensure its satisfaction (see, e.g., \citep{cerri2014,obozinski2008,valentini2011}). Most of the state-of-the-art models based on neural networks belong to the second category (see, e.g.,~\citep{cerri2014,cerri2016,cerri2018}). 




In this paper, we propose \system{}, a novel approach for HMC problems, which, given a network  for the underlying multi-label classification problem, exploits the hierarchy information to produce  predictions coherent with the hierarchy constraint and improve performance. 
\system{} is
based on two basic elements: 
(i) a constraint layer built on top of ,  ensuring that the predictions are coherent by construction,
 and (ii) a loss function teaching \system{}
 when to exploit the prediction on the lower classes in the hierarchy to make predictions on the upper ones.  
\system{} has the following four features: (i) its predictions are coherent without any post-processing, (ii) differently from other state-of-the-art models (see, e.g.,~\cite{cerri2018}), its number of parameters is independent from the number of hierarchical levels, (iii)  it can be easily implemented on GPUs using standard libraries, and (iv) it outperforms the state-of-the-art models {\ens}~\citep{schietgat2010}, {\lmlp}~\citep{cerri2016}, {\hmcnf}, and {\hmcnr} \citep{cerri2018} on 20 commonly used real-world HMC benchmarks.
 








The rest of this paper is organized as follows. 
In Section~\ref{sec:terminology}, we introduce the notation and terminology used. 
Then, in Section~\ref{sec:basic_case}, we present the core ideas behind \system{} on a simple HMC problem 
with just two classes, followed by the presentation of the general solution in Section~\ref{sec:main}. Experimental results are presented in Section~\ref{sec:experiments}, while the related work is discussed in Section~\ref{sec:relwork}. The last section gives some concluding remarks. 

\section{Notation and terminology}\label{sec:terminology}

Consider an arbitrary HMC problem with a given set of classes, which are hierarchically organized as a DAG. If there is a path of length  from a class  to a class  in the DAG, then we say that  is a {\sl subclass} of  (every class is thus a subclass of itself).  Consider an arbitrary datapoint , . 
For each class  and model , we assume to have a mapping  such that  is predicted to belong to  whenever 
 is bigger than or equal to a user-defined threshold. To guarantee that the hierarchy constraint is always satisfied independently from the threshold, the model  should guarantee that , for all  , whenever  is a subclass of : if , for some , then we have a {\sl hierarchy violation} (see, e.g., \citep{cerri2018}). 
For ease of readability,  
in the rest of the paper, we always leave implicit the dependence on the considered datapoint ,
and write, e.g.,  for .

\begin{figure*}[t]
\centering
\begin{tabular}{c@{\ \ \,}c@{\ \ \ \ }|@{\ \ \ \ }c@{\ \ \,}c@{\ \ \ \ }|@{\ \ \ \ }c@{\ \ \,}c}
\multicolumn{2}{c}{Neural Network } & \multicolumn{2}{c}{Neural Network } &
\multicolumn{2}{c}{Neural Network } \\
Class  & Class  & Class  & Class  & Class  & Class  \\
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/nested/f_classA_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/nested/f_classB_withoutdots_before.eps} 
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/nested/g_classA_withoutdots_before.eps} 
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/nested/g_classB_withoutdots_before.eps} 
    \end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\linewidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/nested/h_classA_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.165\textwidth} 
    \includegraphics[width=\linewidth,trim={1.1cm 0 1cm 1cm},clip]{img/nested/h_classB_withoutdots_before.eps}
\end{minipage} \\
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/disjuncted/f_classA_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/disjuncted/f_classB_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/disjuncted/g_classA_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/disjuncted/g_classB_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\linewidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/disjuncted/h_classA_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.165\textwidth}
    \includegraphics[width=\linewidth,trim={1.1cm 0 1cm 1cm},clip]{img/disjuncted/h_classB_withoutdots_before.eps}
\end{minipage} \\
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/intersected/f_classA_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/intersected/f_classB_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/intersected/g_classA_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\textwidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/intersected/g_classB_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.135\textwidth}
    \includegraphics[width=\linewidth,trim={1.1cm 0 3.7cm 1cm},clip]{img/intersected/h_classA_withoutdots_before.eps}
\end{minipage} &
\begin{minipage}{.17\textwidth}
    \includegraphics[width=\linewidth,trim={1.1cm 0 1cm 1cm},clip]{img/intersected/h_classB_withoutdots_before.eps}
\end{minipage} \vspace*{-1ex}
\end{tabular}
\caption{In all figures, the smaller yellow rectangle corresponds to , while the bigger yellow one corresponds to~. The first row of figures corresponds to , the second corresponds to , and the third corresponds to . 
 First 4 columns: decision boundaries of  (resp., ) for classes  and  (resp.,  and ). Last 2 columns: decision boundaries of  for classes  and . In each figure, the darker the blue (resp., red), the more confident a model is that the datapoints in the region belong (do not belong) to the class (see the scale at the end of each row).
\vspace*{-1.5ex}
}
\label{fig:dec_bound_figs}
\end{figure*}

\section{Basic case}\label{sec:basic_case}







Our goal is to leverage standard neural network approaches for multi-label classification problems and then exploit the hierarchy constraint in order to produce coherent predictions and improve performance.
Given our goal,
we first present two basic approaches, 
exemplifying their respective strengths and weaknesses. These are useful to then introduce our solution, which
is shown to present their advantages without exhibiting their weaknesses. In this section, we assume to have just two classes~ and  and the constraint stating that  is a subclass of . 

\subsection{Basic approaches}\label{subsec:approaches}

In the first approach, we treat the problem as a standard multi-label classification problem and simply set up a neural network  with one output per class to be learnt: to ensure that no hierarchy violation happens, we need an additional post-processing step. In this simple case, the post-processing could set the output for  to be  or the output for  to be . In this way, all predictions are always coherent with the hierarchy constraint.
Another approach for this case is to build a network  with two outputs, one for  and one for . To meaningfully ensure that no hierarchy violation happens, we need an additional post-processing step in which the predictions for the class  are given by .
Considering the two above approaches, depending on the specific distribution of the points in  and in , one solution may be significantly better than the other, and a priori we may not know which one it is.

To visualize the problem, assume that , and consider two rectangles  and  with ~smal\-ler than , like the two yellow rectangles in the subfigures of Figure~\ref{fig:dec_bound_figs}. Assume  and .
Let  be the model obtained by adding a post-processing step to  setting  and , as in~\cite{cerri2014,cerri2016,feng2018} (analogous considerations hold, if we set  and  instead). Intuitively, we expect  to perform well even with a very limited number of neurons when , as in the first row of Figure~\ref{fig:dec_bound_figs}.
However, if , as in the second row of Figure~\ref{fig:dec_bound_figs}, we expect  to need more neurons to obtain the same performance.
Consider the alternative network , and
let  be the system obtained by setting  and .
Then, we expect  to perform well when .
However, if , we expect  to need  more neurons to obtain the same performance. We do not consider the model with one output for  and one for , since it performs poorly in both~cases.



To test our hypothesis, we implemented  and  as feedforward neural networks with one hidden layer with 4 neurons and {\sl tanh} nonlinearity. We used the {\sl sigmoid} non-linearity for the output layer (from here on, we always assume that the last layer of each neural network presents {\sl sigmoid} non-linearity). 
 and  were trained with binary cross-entropy loss using Adam optimization~\citep{kingma2014} for 20k epochs with learning rate  (). The datasets consisted of  (50/50 train test split) datapoints sampled from a uniform distribution over . 
The first four columns of Figure~\ref{fig:dec_bound_figs} show the decision boundaries of  and . Those of  and , reported in Appendix~\ref{app:decbound}, can be derived from the plotted ones, while the converse does not hold. These figures highlight that  (resp., ) approximates the two rectangles better than  (resp., )  when  (resp., ).
In general, when , we expect that the behavior  of  and  depends on the relative position of  and . 

\subsection{Our solution}\label{subsec:solution}

Ideally, we would like to build a neural network that is able to have roughly the same performance of  when , of  when , and better than both in any other case. We can achieve this behavior in two steps. In the first step, we build a new neural network consisting of two modules: (i) a bottom module  with two outputs in  for  and , and (ii) an upper module, called {\sl max constraint module ()},  consisting of a single layer that takes as input the output of the bottom module and imposes the hierarchy constraint. 
We call the obtained neural network {\sl coherent hierarchical multi-label classification neural network} (\system{}). 

Consider a datapoint . Let  and  be the outputs of  for the classes  and , respectively, and let  and  be the ground truth for the classes  and , respectively. 

The outputs of {\module} (which are also the output of \system{}) are: 

Notice that the output of \system{} ensures that no hierarchy violation happens, i.e., that for any threshold, it cannot be the case that  predicts that a datapoint belongs to  but  not to~. 
In the second step, to exploit the hierarchy constraint during training, 
\system{} is trained with a novel loss function, called {\sl max constraint loss ()},
defined as , where:

{\loss} differs from the standard binary cross-entropy loss

iff  (),  (), and . 



The following example highlights the different behavior of {\loss} compared to .

\begin{example}{\rm 
Assume , , , and . 
Then, we obtain: 

Given the above, we get:

Hence, if \system{} is trained with , then it wrongly learns that it needs to increase  and keep . On the other hand, for \system{} (with ), we obtain:

In this way, \system{} rightly learns that it needs to decrease  and increase .
}\end{example}








Consider the example in Figure~\ref{fig:dec_bound_figs}. To check that our model behaves as expected, we implemented  as , and trained \system{} with  on the same datasets and in the same way as  and~. 
The last two columns of Figure~\ref{fig:dec_bound_figs} show the decision boundaries of  (those of \system{} can be derived from the plotted ones and are in Appendix~\ref{app:decbound}). 
's decision boundaries mirror those of  (resp., ) when 
 (resp., ). Intuitively,
\system{} is able to decide whether to learn : (i) as a whole (top figure), (ii) as the union of  and  (middle figure), and (iii) as the union of a subset of  and a subset of  (bottom~figure). \system{} has learnt when to exploit the prediction on the lower class  to make predictions on the upper class .












\section{General case}\label{sec:main}
















Consider a generic HMC problem with a set  of  hierarchically structured classes, a datapoint , and a generic neural network  with one output for each class in . Given a class ,  is the set of subclasses of  in ,\footnote{By definition, .}  is the ground truth label for class  and  is the prediction made by  for . 
The output  of \system{} for a class  is: 

For each class , the number of operations performed by  is independent from the depth of the hierarchy, making \system{} a scalable model. Thanks to , \system{} is guaranteed to always output predictions satisfying the hierarchical constraint, as stated by the following theorem, which follows immediately from Eq.~(\ref{eq:module_class}). 

\begin{theorem}\label{theorem}
Let  be a set of hierarchically structured classes. Let  be a neural network with outputs . Let 
 be defined as in Eq.~(\ref{eq:module_class}).
Then, \system{} does not admit hierarchy violations.
\end{theorem}

For each class ,   is defined as:

The final  is given by:


 The importance of using \loss{} instead of the standard binary cross-entropy loss  becomes even more apparent in the general case. Indeed, as highlighted by the following example, the more ancestors a class has, the more likely it is that \system{} trained with  will remain stuck in bad local optima.
\begin{example}{\rm
Consider a generic HMC problem with  classes, and a class  being a subclass of . Suppose , , and . Then, if we use the standard binary cross-entropy loss, we obtain:

Since , we would like to get  . However, that is possible only if . Let , then we need , while if , we need . On the contrary, if we use \loss{}, we obtain:

Thus, no matter the value of , we get .
}
\end{example}

Finally, thanks to both  and , \system{} has the ability of delegating the prediction on a class~ to one of its subclasses.
\begin{definition}[Delegate]{\rm 
Let  be a set of hierarchically structured classes. Let  be a datapoint. Let  be the outputs of a neural network  given the input . Let 
 be defined as in Eq.~(\ref{eq:module_class}).
 Consider a class  and a class  with . Then, \system{} \emph{delegates} the prediction on   to  for , if~ and .} 
\end{definition}
Consider the basic case in Section~\ref{sec:basic_case} and the figures in the last column of  Figure~\ref{fig:dec_bound_figs}. Thanks to \module{} and \loss{}, \system{} behaves as expected: it delegates the prediction on  to  for (i) 0\% of the points in  when  (top figure), (ii) 100\% of the points in  when  (middle figure), and (iii)~85\% of the points in  when  and  are as in the bottom figure.





























\section{Experimental analysis}\label{sec:experiments}

In this section, we first discuss how to effectively implement \system{}, leveraging GPU architectures. Then, we present the experimental results of \system{}, first considering two synthetic experiments, and then on 20 real-world datasets for which we compare with current state-of-the-art models for HMC problems. Finally, ablation studies highlight the positive impact of both \module{} and \loss{} on \system{}'s performance.\footnote{Link:  {https://github.com/EGiunchiglia/C-HMCNN/}} 


The metric that we use to evaluate models is the area under the average precision and recall curve \auprc. The \auprc{} is computed as the area under the average precision recall curve, whose points  are computed as:

where TP, FP, and FN are the number of true positives, false positives, and false negatives for class~, respectively.
\auprc{}  has the advantage of being independent from the threshold used to predict when a datapoint belongs to a particular class (which is often heavily application-dependent) and is the most used in the HMC literature \citep{kwok2011,vens2008,cerri2018}.



\subsection{GPU implementation}

For readability,  and  have been defined for a specific class . However, it is possible to compute  \module{} and \loss{} for all classes in parallel, 
leveraging GPU architectures.


 Let  be an  matrix obtained by stacking  times the  outputs of the bottom module  of \system{}. Let  be an  matrix such that, for ,  if  is a subclass of  (), and , otherwise. Then, 

where  represents the Hadamard product, and given an arbitrary  matrix ,  returns a vector of length  whose -th element is equal to . For \loss{}, we can use the same mask  to modify the standard binary cross-entropy loss (BCELoss) that can be found in any available library (e.g., PyTorch). In detail, let  be the ground-truth vector,  be the output vector of , ,  be the  matrix obtained by stacking  times the vector . Then, 










\subsection{Synthetic experiment 1}
Consider the generalization of the experiment  in Section~\ref{sec:main} in which we started with  outside  (as in the second row of Figure~\ref{fig:dec_bound_figs}), and then moved  towards the centre of  (as in the first row of  Figure~\ref{fig:dec_bound_figs}) in 9 uniform steps.
The last row of  Figure~\ref{fig:dec_bound_figs} corresponds to the fifth step, i.e.,  was~halfway.
\begin{wrapfigure}{r}{0.5\textwidth}
     \vspace{-.5cm}
    \centering
    \includegraphics[width=0.5\textwidth]{img/auc_micro.eps}
    \caption{Mean {\auprc} with standard deviation of \system{}, , and  for each step.\label{fig:moving_rect}}
    \vspace{-.45cm}
\end{wrapfigure}
This experiment is meant to show how the performance of \system{},  , and  as in Section~\ref{sec:basic_case}
vary depending on the relative positions of  and . Here, 
, , and  were implemented and trained as 
in Section~\ref{sec:basic_case}. For each step, we run the experiment 10 times,\footnote{All subfigures in Figure~\ref{fig:dec_bound_figs} correspond to the decision boundaries of , , and  in the first of the 10 runs. } and we plot the mean \auprc{} together with the standard deviation for \system{}, , and  in Figure~\ref{fig:moving_rect}.


As expected, Figure~\ref{fig:moving_rect} shows that  performed poorly in the first three steps when , it then started to perform better at step 4 when , and it performed well from step 6 when  overlaps significantly with  (at least 65\% of its area). Conversely,  performed well on the first five steps, and its performance started decaying from step 6. \system{} performed well at all steps, as expected, showing robustness with respect to the relative positions of  and . 



\subsection{Synthetic experiment 2}
In order to prove the importance of using \loss{} instead of , in this experiment we compare two models: (i) our model \system{}, and (ii) , i.e.,  with  built on top and trained with the standard binary cross-entropy loss .
Consider 
the nine rectangles arranged as in Figure~\ref{fig:9rects}
named . Assume (i) that we have classes , (ii) that a datapoint belongs to  if it belongs to the -th rectangle, and (iii) that  (resp., ) is an ancestor (resp., descendant) of every class. Thus, all points in  belong to all classes, and if a datapoint belongs to a rectangle, then it also belongs to class . The datasets consisted of 5000 (50/50 train test split) datapoints sampled from a uniform distribution over .  
\begin{wrapfigure}{r}{0.5\linewidth}
    \centering
     \vspace{-.15cm}
    \begin{tabular}{c c c}
    \begin{tikzpicture}[scale=0.18]
        \draw [fill=red,opacity=.1,very thick] (0,0) rectangle (10,10);
\draw [fill=darkblue,opacity=0.4] (1,1) rectangle (4,4)
        node[pos=.5,text=black, opacity=1] { \scriptsize };
        \draw [fill=blue,opacity=0.4] (1,6) rectangle (4,9)
        node[pos=.5,text=black, opacity=1] { \scriptsize };
        \draw [fill=blue,opacity=0.4] (6,1) rectangle (9,4)
        node[pos=.5,text=black, opacity=1] { \scriptsize };
        \draw [fill=blue,opacity=0.4] (6,6) rectangle (9,9)
        node[pos=.5,text=black, opacity=1] { \scriptsize };
\draw [fill=blue,opacity=0.4] (4.5,4.5) rectangle (5.5,5.5)
        node[pos=.5,text=black, opacity=1] { \scriptsize };
\draw [fill=blue,opacity=0.4] (1,4.5) rectangle (4,5.5)
        node[pos=.5,text=black, opacity=1] { \scriptsize };
        \draw [fill=blue,opacity=0.4] (4.5,1) rectangle (5.5,4)
        node[pos=.5,text=black, opacity=1] { \scriptsize };
        \draw [fill=blue,opacity=0.4] (4.5,6) rectangle (5.5,9)
        node[pos=.5,text=black, opacity=1] { \scriptsize };
        \draw [fill=blue,opacity=0.4] (6,4.5) rectangle (9,5.5)
        node[pos=.5,text=black, opacity=1] { \scriptsize };
        \end{tikzpicture}&
         \includegraphics[trim=30 20 110 38,clip, scale=0.18]{img/synt_exp2/hol_class4_withoutdots_after.eps}&
\includegraphics[trim=30 20 110 38,clip,scale=0.18]{img/synt_exp2/h_class4_withoutdots_after.eps}
    \end{tabular}
    \caption{From left to right: (i) rectangles disposition, (ii) decision boundaries for  of  trained with , and (iii) decision boundaries for  of \system{}. \label{fig:9rects}}
    \vspace{-.25cm}
\end{wrapfigure}
Let  be a feedforward neural network with a single hidden layer with 7 neurons.  We train both  and \system{} for 20k epochs using Adam optimization with learning rate  (). 
As expected , the average {\auprc} (and standard deviation) over 10 runs for  trained with  is 0.938 (0.038), while  trained with {\loss} (\system{}) is 0.974 (0.007). Notice that not only  performs worse, but also, due to the convergence to bad local optima, the standard deviation obtained with  is 5 times higher than the one of \system{}: the (min, median, max) {\auprc} for  are , while for \system{} are .
Since  presents a high standard deviation, the figure shows the decision boundaries of the 6th best performing networks for class . 












\begin{table}[t]
    \centering
    \footnotesize
        \caption{Summary of the 20 real-world datasets. Number of features (), number of classes (), and number of datapoints for each dataset split.}
    \begin{tabular}{l l c c c c c c}
    \toprule
         {\sc Taxonomy} & {\sc Dataset} &  &  & {\sc Training} & {\sc Validation} & {\sc Test}  \\
    \midrule
         {\sc FunCat (FUN)} & {\sc Cellcycle} & 77 & 499 & 1625 & 848 & 1281 \\
         {\sc FunCat (FUN)} & {\sc Derisi} & 63 & 499 &  1605 & 842 & 1272 \\
         {\sc FunCat (FUN)} & {\sc Eisen} & 79 & 461 &  1055 & 529 & 835 \\
         {\sc FunCat (FUN)} & {\sc Expr} & 551 & 499 &  1636 & 849 & 1288 \\
         {\sc FunCat (FUN)} & {\sc Gasch1} & 173 & 499 &  1631 & 846 & 1281 \\
         {\sc FunCat (FUN)} & {\sc Gasch2} & 52 & 499 &  1636 & 849 & 1288 \\
         {\sc FunCat (FUN)} & {\sc Seq} & 478 & 499 & 1692 & 876 & 1332 \\
         {\sc FunCat(FUN)} & {\sc Spo} & 80 & 499 & 1597 & 837 & 1263 \\
    \midrule   
    {\sc Gene Ontology (GO)} & {\sc Cellcycle} & 77 & 4122 & 1625 & 848 & 1281 \\
     {\sc Gene Ontology (GO)} & {\sc Derisi} & 63 & 4116 & 1605 & 842 & 1272 \\
     {\sc Gene Ontology (GO)} & {\sc Eisen} & 79 & 3570 & 1055 & 528 & 835 \\
     {\sc Gene Ontology (GO)} & {\sc Expr} & 551 & 4128 & 1636 & 849 & 1288 \\
     {\sc Gene Ontology (GO)} & {\sc Gasch1} & 173 & 4122 & 1631 & 846 & 1281 \\
     {\sc Gene Ontology (GO)} & {\sc Gasch2} & 52 & 4128 & 1636 & 849 & 1288 \\
     {\sc Gene Ontology (GO)} & {\sc Seq} & 478 & 4130 & 1692 & 876 & 1332 \\
     {\sc Gene Ontology (GO)} & {\sc Spo} & 80 & 4166 & 1597 & 837 & 1263 \\
     \midrule
     {\sc Tree} & {\sc Diatoms} & 371 & 398 & 1085 & 464 & 1054 \\
    {\sc Tree} & {\sc Enron} & 1000 & 56 & 692 & 296 & 660 \\
     {\sc Tree} & {\sc Imclef07a} & 80 & 96 & 7000 & 3000 & 1006 \\
     {\sc Tree} & {\sc Imclef07d} & 80 & 46 & 7000 & 3000 & 1006 \\
    \bottomrule
    \end{tabular}
    \label{tab:datasets}
\end{table}

\begin{table}[t]
    \centering
        \caption{Comparison of \system{} with the other state-of-the-art models. The performance of each system is measured as the \auprc{} obtained on the test set. The best results are in bold.    \label{tab:comparison}}
    \begin{tabular}{l c c c | c c }
    \toprule
         Dataset & \system{}  & \sc{\lmlp} & \sc{\ens} & \sc{\hmcnr} & \sc{\hmcnr} \\
         \midrule
         \sc{Cellcycle FUN} & {\textbf{0.255}} & 0.207 & 0.227 &0.247& 0.252  \\ 
         \sc{Derisi FUN} & {\textbf{0.195}}  & 0.182 & 0.187 &0.189 & 0.193 \\
         \sc{Eisen FUN} & {\textbf{0.306}}  & 0.245 & 0.286 &0.298 & 0.298\\
         \sc{Expr FUN} & {\textbf{0.302}} & 0.242 & 0.271 &0.300 & 0.301 \\
         \sc{Gasch1 FUN} & {\textbf{0.286}} & 0.235 & 0.267 & 0.283 & 0.284 \\
         \sc{Gasch2 FUN} & {\textbf{0.258}}  & 0.211 & 0.231 & 0.249 & 0.254\\
         \sc{Seq FUN} & {\textbf{0.292}} & 0.236 & 0.284 & 0.290 & 0.291 \\ 
         \sc{Spo FUN} & {\textbf{0.215}} & 0.186 & 0.211 &0.210 & 0.211 \\
         \midrule
         \sc{Cellcycle GO} &{\textbf{0.413}} & 0.361 & 0.387 &0.395& 0.400  \\ 
         \sc{Derisi GO} & {\textbf{0.370}}  & 0.343 & 0.361 &0.368& 0.369 \\
         \sc{Eisen GO} & {\textbf{0.455}} & 0.406 & 0.433 &0.435& 0.440\\
         \sc{Expr GO} & 0.447 & 0.373 & 0.422 &0.450& \textbf{0.452} \\
         \sc{Gasch1} GO & {\textbf{0.436}} & 0.380 & 0.415 &0.416& 0.428 \\
         \sc{Gasch2} GO & 0.414 & 0.371 & 0.395 &0.463& \textbf{0.465}\\
         \sc{Seq GO} & 0.446  & 0.370 & 0.438 &0.443& \textbf{0.447} \\ 
         \sc{Spo GO} & {\textbf{0.382}}  & 0.342  & 0.371 &0.375& 0.376 \\
         \midrule
         \sc{Diatoms} & {\textbf{0.758}} & - & 0.501 & 0.514 & 0.530 \\
         \sc{Enron} & {\textbf{0.756}} & - & 0.696 & 0.710 & 0.724 \\
         \sc{Imclef07a} &{\textbf{0.956}} & - & 0.803 & 0.904 & 0.950 \\
         \sc{Imclef07d} & {\textbf{0.927}} & - & 0.881 & 0.897 & 0.920 \\
         \midrule
         \sc{Average Ranking} & 1.25 & 5.00 & 3.93 & 2.93 & 1.90 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Comparison with the state of the art}



We tested our model on 20 real-world datasets commonly used to compare HMC systems (see, e.g.,~\cite{kwok2011,nakano2019,vens2008,cerri2018}): 16 are functional genomics datasets \citep{clare2003}, 2 contain medical images \citep{dimitrovski2008}, 1 contains images of microalgae \citep{dimitrovski2011}, and 1 is a text categorization dataset  \citep{klimt2004}.\footnote{Links: https://dtai.cs.kuleuven.be/clus/hmcdatasets and http://kt.ijs.si/DragiKocev/PhD/resources}
The characteristics of these datasets are summarized in Table~\ref{tab:datasets}. These datasets are particularly challenging,  because their number of training samples is rather limited, and they have a large variation, both in the number of features (from 52 to 1000) and in the number of classes (from 56 to 4130). We applied the same preprocessing  to all the datasets. All the categorical features were transformed using one-hot encoding. The missing values were replaced by their mean in the case of numeric features and by a vector of all zeros in the case of categorical ones. All the features were standardized. 


We built  as a feedforward neural network with two 
hidden layers and ReLU non-linearity.
 To prove the robustness of \system{},
 we kept all the hyperparameters fixed except the hidden dimension and the learning rate used for each dataset, which are given in Appendix~\ref{app:hidden_dim} and were optimized over the validation sets. In all experiments, the loss was minimized using Adam optimizer with weight decay , and patience 20 (, ). The dropout rate was set to 70\% and the batch size to 4. 
As in \citep{cerri2018}, we retrained \system{} on both training and validation data for the same number of epochs, as the early stopping procedure determined was optimal in the first pass.





For each dataset, we run \system{}, {\ens} \citep{schietgat2010}, and {\lmlp} \citep{cerri2016} 10 times, and the average \auprc~is reported in Table~\ref{tab:comparison}. For simplicity, we omit the standard deviations, which for \system{} are in the range ,  proving that it is  a very stable model. As reported in \citep{nakano2019}, {\ens} and {\lmlp} are the current state-of-the-art models with publicly available code. These models were run with the suggested configuration settings on each dataset.\footnote{We also ran the code from \citep{masera2018}. However, we obtained very different results from the ones reported in the paper. 
Similar negative results are also reported in \citep{nakano2019}.}
The results are shown in Table \ref{tab:comparison}, left side. On the right side, we show the results of {\hmcnr} and {\hmcnf} directly taken from \citep{cerri2018}, since the code is not publicly available. We report the results of both systems, because, while {\hmcnr} has worse results than {\hmcnf}, the amount of parameters of the latter grows with the number of hierarchical levels. As a consequence,   {\hmcnr} is much lighter in terms of total amount of parameters, and the authors advise that for very large hierarchies, {\hmcnr} is probably a better choice than {\hmcnf} considering the trade-off performance vs. computational cost~\cite{cerri2018}. Note that the number of parameters of \system{} is independent from the number of hierarchical levels.

  As reported in Table~\ref{tab:comparison}, \system{} has the greatest number of wins (it has the best performance on all datasets but 3) and best average ranking (1.25). We also verified the statistical significance of the
  \begin{wrapfigure}{r}{0.42\textwidth}
    \centering
    \vspace{-.12cm}
    \includegraphics[trim=30 0 30 0,clip, width=0.42\textwidth]{img/nemenyi.eps}
    \caption{Critical diagram for the Nemenyi's statistical test.\label{fig:nemenyi}}
    \vspace{-.2cm}
\end{wrapfigure}
   results following \cite{demsar2006}. We first executed the Friedman test, obtaining p-value .
  We then performed the post-hoc Nemenyi
   test, and the resulting critical diagram is shown in Figure~\ref{fig:nemenyi}, where the group of methods that do not differ significantly (significance level 0.05) are connected through a horizontal line. The Nemenyi test is powerful enough to conclude that there is a statistical significant difference between the performance of \system{} and all the other models but \hmcnf. Hence, following \citep{demsar2006,benavoli2016}, we compared \system{} and {\hmcnf} using the Wilcoxon test. This test, contrarily to the Friedman test and the Nemenyi test, takes into account not only the ranking, but also the differences in performance of the two algorithms.  
 The Wilcoxon test allows us to conclude that there is a statistical significant difference between the  performance of \system{} and {\hmcnf} with p-value of . 




\begin{table}[t]
    \centering
        \caption{Impact of  and + on the performance measured as {\auprc} and on the total number of epochs for the validation set of the Funcat datasets.}
    \begin{tabular}{l c c c c c c}
    \toprule
           & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{\system{}} \\
         Dataset & \auprc & Epochs & \auprc & Epochs & \auprc & Epochs \\
         \midrule
         {\sc Cellcycle} & 0.220 & 74 & 0.229 & 108 & {\textbf{0.232}} & 106 \\
         {\sc Derisi} & 0.179 & 58 & 0.179 & 66  &  {\textbf{0.182}} & 67\\
         {\sc Eisen} & 0.262 & 76 & 0.271 & 107 & {\textbf{0.285}} & 110 \\
{\sc Expr} & 0.246 & 14 &  0.265 & 19 & {\textbf{0.270}} & 20 \\
         {\sc Gasch1} & 0.239 & 28 & 0.258 & 42 & {\textbf{0.261}} & 38\\
         {\sc Gasch2} & 0.221 & 103 & 0.234 & 132 & {\textbf{0.235}} & 131 \\
         {\sc Seq} & 0.245 & 8 & 0.269 & 13 & {\textbf{0.274}} & 13 \\
         {\sc Spo} & 0.186 & 103 & 0.189 & 117 & {\textbf{0.190}} & 115 \\
         \midrule
         {\sc Average Ranking} & 2.94 & & 2.06 & & 1.00 & \\
    \bottomrule
    \end{tabular}
    \label{tab:ablation}
\end{table}



\subsection{Ablation studies}


To analyze the impact of both  and , we compared the performance of \system{} on the validation set of the FunCat datasets against the performance of  , i.e.,  with the post-processing as in \cite{cerri2016} and \cite{feng2018} and , i.e.,  with  built on top. Both these models were trained using the standard binary cross-entropy loss. 
As it can be seen in Table~\ref{tab:ablation}, \module{} by itself already helps to improve the performances on all datasets but Derisi, where  and  have the same performance. However, \system{}, by exploiting both \module{} and \loss, always outperforms  and . In Table~\ref{tab:ablation}, we also report after how many epochs the algorithm stopped training in average. As it can be seen, even though \system{} and  need more epochs than , the numbers are still comparable. 


\section{Related work}
\label{sec:relwork}
HMC problems are a generalization of hierarchical classification problems, where the labels are hierarchically organized, and each datapoint can be assigned to one path in the hierarchy (e.g.,~\cite{dekel2004,ramaswamy2015,sun2001}). Indeed, in HMC problems, each datapoint can be assigned multiple paths in the hierarchy. 

In the literature, HMC methods are traditionally divided into local and global approaches~\citep{silla2011}. 
Local approaches decompose the problem into smaller classification ones, and then the solutions are combined to solve the main task. Local approaches  can be further divided based on the strategy that they deploy to decompose the main task. If a method trains a different classifier for each level of the hierarchy, then we have a {\sl local classifier per level} as in \citep{cerri2011,cerri2014,cerri2016,li2018,zou2019}. The works \citep{cerri2011,cerri2014,cerri2016} are extended by \citep{cerri2018}, where \hmcnr{} and \hmcnf{} are presented. Since  \hmcnr{} and \hmcnf{} are trained with both a local loss and a global loss, they are considered hybrid local-global approaches. If a method trains a classifier for each node of the hierarchy, then we have a {\sl local classifier per node}. In \cite{cesabianchi2006}, a linear classifier is trained for each node with a loss function that captures the hierarchy structure. On the other hand, in~\cite{feng2018}, one multi-layer perceptron for each node is deployed. A different approach is proposed in~\citep{kwok2011}, where kernel dependency estimation is employed to project each label to a low-dimensional vector. To preserve the hierarchy structure, a generalized condensing sort and select algorithm is developed, and each vector is then learned singularly using ridge regression. Finally, if a method trains a different classifier per parent node in the hierarchy, then we have a {\sl local classifier per parent node}. For example, \cite{kulmanov2018} proposes to train a model for each sub-ontology of the Gene Ontology, combining features automatically learned from the sequences and features based on protein interactions. In~\cite{xu2019aaai}, instead, the authors try to solve the overfitting problem typical of local models by representing the correlation among the labels by the label distribution, and then training each local model to map datapoints to label distributions.
Global methods consist of single models able to map objects with their corresponding classes in the hierarchy as a whole. A well-known global method is {\sc Clus-HMC} \citep{vens2008}, consisting of a single predictive clustering tree for the entire hierarchy. This work is extended in \citep{schietgat2010}, where \ens{}, an ensemble of {\sc Clus-HMC}, is proposed. In \cite{masera2018}, a neural network incorporating the structure of the hierarchy in its architecture is proposed. While this network makes predictions that are coherent with the hierarchy, it also makes the assumption that each parent class is the union of the children. In \cite{borges2012}, the authors propose a ``competitive neural network'', whose architecture replicates the hierarchy.





\section{Summary and outlook}\label{sec:conclusions}


In this paper, we proposed a new model for HMC problems, called \system{}, which is able to (i) leverage the hierarchical information to learn when to delegate the prediction on a superclass to one of its subclasses, (ii) produce predictions coherent by construction, and (iii) outperfom current state-of-the-art models on 20 commonly used real-world HMC benchmarks. Further, its number of parameters does not depend on the number of hierarchical levels, and it can be easily implemented on GPUs using standard libraries. In the future, we will use as  an interpretable model (see, e.g.,~\cite{lei2016}), and study how \module{} and \loss{} can be modified to improve the interpretability of \system{}.







\section*{Broader Impact}



In this paper, we proposed a novel model that is shown to outperform the current state-of-the-art models on commonly used HMC benchmarks. We expect our approach to have a large impact on the research community not only because of its positive results but also because it is relatively easy to implement and test using standard libraries, and the code is publicly available. From the application perspective, given the generality of the approach, it is impossible to foresee all the possible impacts in all the different application domains where HMC problems arise.  We thus focus on functional genomics, which is the application domain most benchmarks come from. 

The goal in functional genomics is to describe the functions and interactions of genes and their products, RNA and proteins. As stated in~\cite{nakano2019,radivojac2013}, in  recent years, the generation of proteomic data has increased substantially, and annotating all sequences is costly and time-consuming, making it often unfeasible. It is thus necessary to develop methods (like ours) that are able to automatize this process. Having better models for such a task may unlock many possibilities. Indeed, it may 
 (i)  allow to better understand the role of proteins in disease pathobiology,  (ii)   help determine the function of metagenomes offering possibilities to discover novel genes and novel biomolecules, and (iii)   facilitate finding drug targets, which is crucial to the success of mechanism-based drug discovery.











\begin{ack}
We would like to thank Francesco Giannini and Marco Gori for useful discussions, and Maria  Kiourlappou for her feedback on the broader impact section.
Eleonora Giunchiglia is supported by the EPSRC under the grant EP/N509711/1 and by an Oxford-DeepMind Graduate Scholarship.
This work was also supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1 and by the AXA Research Fund. We also acknowledge the use of the EPSRC-funded Tier 2 facility JADE~(EP/P020275/1) and GPU computing support by Scan Computers International Ltd.
\end{ack}

\bibliographystyle{plainnat}
\bibliography{bibliography}



\newpage

\appendix\label{sec:appendix}

\section{Decision Boundaries }\label{app:decbound}

\begin{figure}[h]
    \centering
     \subfloat[]{
         \includegraphics[width=0.32\textwidth]{img/disjuncted/f_classB_withoutdots_after.eps}
         \label{fig:f_disjuncted}
         }
     \subfloat[]{
         \includegraphics[width=0.32\textwidth]{img/nested/f_classB_withoutdots_after.eps}
        \label{fig:f_nested}
        }
     \subfloat[]{
         \includegraphics[width=.32\textwidth]{img/intersected/f_classB_withoutdots_after.eps}
         \label{fig:f_intersected}
         }
     \caption{Figure~\ref{fig:f_disjuncted}: Decision boundaries for class  of  when . Figure~\ref{fig:f_nested}: Decision boundaries for class  of  when . Figure~\ref{fig:f_intersected}: Decision boundaries for class  of  when .}
\end{figure}

\begin{figure}[h]
    \centering
     \subfloat[]{
         \includegraphics[width=.32\textwidth]{img/disjuncted/g_classB_withoutdots_after.eps}
         \label{fig:g_disjuncted}}
     \subfloat[]{
         \includegraphics[width=.32\textwidth]{img/nested/g_classB_withoutdots_after.eps}
        \label{fig:g_nested}}
     \subfloat[]{
         \includegraphics[width=.32\textwidth]{img/intersected/g_classB_withoutdots_after.eps}
        \label{fig:g_intersected}}
     \caption{Figure~\ref{fig:g_disjuncted}: Decision boundaries for class  of  when . Figure~\ref{fig:g_nested}: Decision boundaries for class  of  when . Figure~\ref{fig:g_intersected}: Decision boundaries for class  of  when .}
\end{figure}

\begin{figure}[h]
    \centering
     \subfloat[]{
         \includegraphics[width=.32\textwidth]{img/disjuncted/h_classB_withoutdots_after.eps}
         \label{fig:h_disjuncted}}
     \subfloat[]{
         \includegraphics[width=.32\textwidth]{img/nested/h_classB_withoutdots_after.eps}
        \label{fig:h_nested}}
     \subfloat[]{
         \includegraphics[width=.32\textwidth]{img/intersected/h_classB_withoutdots_after.eps}
         \label{fig:h_intersected}}
     \caption{Figure~\ref{fig:h_disjuncted}: Decision boundaries for class  of \system{} when . Figure~\ref{fig:h_nested}: Decision boundaries for class  of \system{} when . Figure~\ref{fig:h_intersected}: Decision boundaries for class  of \system{} when .}
\end{figure}



\section{Experimental Analysis Details}\label{app:hidden_dim}
\begin{table}[h]
    \centering
    \caption{Hidden dimension used for each dataset, learning rate used for each dataset, and average inference time per batch in milliseconds (ms). Average computed over 500 batches for each dataset.}
    \begin{tabular}{l c c c}
        \toprule
         {\sc Dataset} & Hidden Dimension &  Learning Rate & Time per batch (ms) \\
         \midrule
         {\sc Cellcycle FUN} & 500 &  & 2.0\\
         {\sc Derisi FUN} & 500 &  & 2.0\\
         {\sc Eisen FUN} & 500 &  & 1.7\\
         {\sc Expr FUN} & 1000 &  & 1.9\\
         {\sc Gasch1 FUN} & 1000 &  & 2.0\\
         {\sc Gasch2 FUN} & 500 &  & 2.8\\
         {\sc Seq FUN} & 2000 &  & 2.0\\
         {\sc Spo FUN} & 250&  & 1.6\\
         \midrule
         {\sc Cellcycle GO} & 1000 &  & 2.4 \\
         {\sc Derisi GO} & 500 &  & 2.5\\
         {\sc Eisen GO} & 500 &  &3.4\\
         {\sc Expr GO} & 4000 &   & 3.9\\
         {\sc Gasch1 GO} & 500 &  & 2.5\\
         {\sc Gasch2 GO} & 500 &  & 2.8\\
         {\sc Seq GO} & 9000 &  & 2.6 \\
         {\sc Spo GO} & 500 &  & 3.3\\
         \midrule
         {\sc Diatoms} & 2000 &  & 2.0\\
         {\sc Enron} & 1000 &  & 3.6\\
         {\sc Imclef07a} & 1000 &  & 3.4\\
         {\sc Imclef07d} & 1000 &  & 2.9\\
         \bottomrule
    \end{tabular}
    \label{tab:hidden_dim}
\end{table}
In this section, we provide more details about the conducted experimental analysis. 
As stated in the paper, across the different experiments, we kept all hyperparameters fixed with the exception of the hidden dimension and the learning rate, which are reported in the first two columns of Table~\ref{tab:hidden_dim}. The other hyperparameters were determined by searching the best hyperparameters configuration on the Funcat datasets; we then took the configuration that led to the best results on the highest number of datasets. The hyperparameter values taken in consideration were: 
(i) learning rate: , (ii) batch size: , (iii) dropout: , and (iv) weight decay: . Concerning the hidden dimension, we took into account all possible dimensions from 250 to 2000 with step equal to 250, and from 2000 to 10000 with step 1000. 
The last column of Table~\ref{tab:hidden_dim} shows the average inference time per batch in milliseconds. The average is computed over 500 batches for each dataset.
All experiments were run on an Nvidia Titan Xp with 12 GB  memory. 


\end{document}