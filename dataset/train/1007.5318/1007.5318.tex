
\documentclass[12pt]{article}
\usepackage{times} 
\usepackage{fullpage}
\usepackage{amsmath,amscd,amssymb,graphics}
\usepackage{mathrsfs} 
\usepackage{color}

\newcommand{\abs}[1]{\lvert#1\rvert} 
\def\E{{\mathbb{E}}}
\def\N{{\mathbb N}}
\def\R{{\mathbb R}}
\def\e{\varepsilon}


\begin{document}
\title{Intrinsic Dimensionality}

\author{Vladimir Pestov}
\date{\small Department of Mathematics and Statistics, University of Ottawa, Ottawa, Ontario, Canada}


\maketitle

\noindent{\bf Nature of the concept.}
Intrinsic dimensionality of data does not refer to a single well-defined parameter, but rather a diverse family of parameters associated to a given similarity workload  and allowing to gauge the performance of various indexing schemes for similarity-based information retrieval.

Perhaps the best known single feature of a high-dimensional dataset  is ``the empty space paradox'': the average distance, , from a query point to the nearest neighbour in  grows along with the dimension of . Asymptotically,  approaches the average distance  between two points of  (the {\em characteristic size} of dataset). 
This is a consequence of the fact that the distance functions , , concentrate near their median/mean values. The boxplot diagram (a) shows (normalized) pairwise distance distributions of  uniformly sampled points in the cube  for various values of . The lines in the middle of the boxes are median values, the lower and upper sides denote the first and third quartiles respectively, and the small circles mark outliers. For high , even the outlier distance values approach the median.

\begin{figure}[ht]
\begin{center}
\scalebox{0.55}[0.55]{\includegraphics{boxplot_dist.eps}} 
\scalebox{0.55}[0.55]{\includegraphics{chavez_dim.eps}} 
\end{center}
\end{figure}  

Generally, the intrinsic dimensionality is 
inversely proportional to some dispersion parameter of data, usually normalized with regard to the characteristic size of . The possible choices of a dispersion parameter are many, and they reflect different aspects of the dataset and its indexability.

\smallskip

\noindent{\bf Illustration: the intrinsic dimensionality of Ch\'avez et al.}
The authors of \cite{CNBYM} have proposed and studied a parameter which we denote by the acronym of their last names: . Here the distance  is treated as a random variable on . The corresponding statistical parameter of a dataset  has a number of advantages: easy to compute, it makes a good statistical estimator for the dimension of the domain . The robustness of ``CNBYM dimension'' can be seen from diag. (b) where the parameter is calculated for independent random samples of  points drawn from the gaussian distribution  on , .
\smallskip 

\noindent{\bf The curse of dimensionality.} 
When applied to high-dimensional datasets, many similarity-based information retrieval algorithms slow down to a point where they cannot outperform a simple sequential scan. Even though the phenomenon is well-known to data practitioners, curiously enough there is still no mathematical validation of it, and one of the major theoretical challenges in the field is the so-called {\em Curse of Dimensionality Conjecture} in the bare-bone setting where  is a Hamming cube \cite{indyk:04}. Rigorous lower bounds are only obtained in a small number of cases, some of which we will describe below.

\smallskip
\noindent{\bf The concentration phenomenon.}
The curse of dimensionality is closely linked to the {\em phenomenon of concentration of measure} (cf. e.g. \cite{gromov:99}, Ch. 3 ): on a typical geometric object  of high dimension not only the distance functions, but all the -Lipschitz functions  concentrate near their median. (A function is {\em -Lipschitz} if it does not increase distances: .)
The {\em concentration function}  of  gives an upper bound on the probability that the value of a -Lipschitz function  at an  deviates from its median by more than . For instance, the concentration function of the -dimensional Hamming cube  with the normalized Hamming metric and uniform measure satisfies a Chernoff bound , cf. diagram (c).
\begin{figure}[ht]
\begin{center}
\scalebox{0.55}[0.55]{\includegraphics{conc_hamming200.eps}} 
\hskip 0.8cm
\raisebox{8ex}{\scalebox{0.28}[0.28]{\includegraphics{discarding.eps}}}
\end{center}
\end{figure}  Similar gaussian bounds exist for Euclidean spheres, cubes, and a variety of other objects. The higher the dimension of , the more sharply  drops off near zero. 

\smallskip
\noindent{\bf Degrading performance of indexing schemes.}
Many known access methods for {\em exact} similarity search (including pivot-based schemes, metric trees, etc.) have the following structure. An indexing scheme consists of a collection of  -Lipschitz functions  on the domain  (possibly partially defined). Given a query point  and the range value , calculations of the values ,  are performed in a recursive way, where the function  is chosen depending on the previous values . If for some  one has , the datapoint  cannot possibly belong to the -ball around  and so is irrelevant. Datapoints  whose irrelevance cannot be certified in this way are returned, and each of them is checked against the condition . 

If the domain  has high concentration dimension, then most values of each function  concentrate near the median. Now denote   the class of all Lipschitz functions used to construct indexing schemes of a given type (e.g. the class of all distance functions for pivot schemes). Assume that  has a small capacity in the sense of statistical learning theory. (This condition is very natural and verified, e.g., by distance functions on the Euclidean spaces or Hamming cubes.)
Assume further that the points of  are sampled from  in an i.i.d. fashion. Then the values of each  at datapoints can be shown to concentrate as well, and only a small proportion of points can be discarded. cf. diag. (d). This argument leads to lower bounds on the performance of pivot indexing schemes which are linear in the size of the dataset \cite{VolPest09}. Similar results can be deduced for metric trees. 

One is naturally led to {\em concentration dimension} of , definied by  and investigated in \cite{pestov0708}. Notice however that while , the relationship between the two is not understood: the latter value (dimension of the empirical measure) need not be a good statistical estimator for the former.

\smallskip
\noindent
{\bf Concentration versus complexity.}
Heuristically, indexability of a workload  amounts to the existence of sufficiently many -Lipschitz functions  on  which at the same time {\em dissipate} (as opposed to {\em concentrate}) and have a low computational complexity. Thus, indexing means finding the right balance between concentration and complexity.

Here is an example of a ``success story'' where smallness of values of an intrinsic dimension guarantees the existence of an efficient access method. 

\smallskip
\noindent{\bf Assouad dimension.}
The {\em Assouad} ({\em doubling}) {\em dimension} of a metric space  is the minimum value  such that every set  in  can be covered by  balls of half the diameter. (The {\em diameter}  of a set  is the supremum of , .) To appreciate that  is indeed an inverse quantity to some parameter of dispersion of data, notice that if  is equipped with a probability measure and  is high, then the size of balls as a function of radius grows fast in the vicinity of , and so  has to be large.

As shown in \cite{KL}, a low Assouad dimension of a workload allows for the construction of a simple and efficient indexing scheme which is essentially a metric tree. The quantity  bounds from above the number of leaf nodes (degree) of such a tree.

\smallskip
\noindent{\bf Concluding remarks and problems.} 
A thorough survey of intrinsic dimensionality in the context of similarity search is \cite{clarkson}, though even this authoritative source is not comprehensive. 

\noindent
 It is safe to predict that the mainstream research direction in the field will be a discovery of further ``dimensionality parameters of positive type'', such as Assouad dimension. More and more real datasets that currently appear high-dimensional will turn out to have low, tractable intrinsic dimensions.

\noindent

(Ciaccia, Pestov). Gromov's reconstruction theorem (\cite{gromov:99}, p. 120) says that a metric space with measure can be recovered from distribution laws of all -point metric subspaces, for all . (Compare with the CNBYM dimension, determined by the distribution law of -point subspaces.) How much can one infer about indexability from the distribution law of -point subspaces?

\noindent
 Notice that fractal-type dimensions (cf. \cite{ttf}) fit into a general paradigm of intrinsic dimensionality outlined by us at the beginning: they reflect the rate of growth of balls/boxes. So does a version of intrinsic dimension  proposed in \cite{lifshits}, the {\em disorder dimension,} whose aim is to quantify the situation where  is close in value to .

\noindent

We assumed above that queries follow the same underlying distribution as datapoints. Now suppose we have two underlying measures,  modelling data and  modelling query centers. Can one build a theory of concentration and intrisic dimension for such metric bi-measure spaces?

\begin{thebibliography}{1}

\bibitem{CNBYM}
E. Ch\'avez, G. Navarro, R. Baeza-Yates, and J. L. Marroqu\'\i n,
{\em Searching in metric spaces,} ACM Computing Surveys \textbf{33} (2001), 273--321.

\bibitem{clarkson}
K.L. Clarkson, {\em Nearest-neighbor searching and metric space dimensions,}
In: Nearest-Neighbor Methods for Learning and Vision: Theory and Practice, MIT Press, 2006, pp. 15--59.

\bibitem 
{gromov:99} 
M. Gromov, {\em Metric Structures for Riemannian and
Non-Riemannian Spaces,} Progress in Mathematics \textbf{152}. Birkhauser
Verlag, 1999.

\bibitem{indyk:04}
P. Indyk, {\em Nearest neighbours in high-dimensional spaces,}
In: J.E. Goodman, J. O'Rourke, Eds.,
Handbook of Discrete and Computational Geometry, Chapman and Hall/CRC, Boca Raton--London--New York--Washington, D.C.,
2004, pp. 877--892.

\bibitem{KL} R. Krauthgamer and J.R. Lee, {\em Navigating nets: simple algorithms for proximity search,} Proc. 15th ACM-SIAM symp. on Discrete algorithms (SODA'04), New Orleans, 2004, pp. 798--807.

\bibitem{lifshits}
Y. Lifshits, {\em Combinatorial framework for similarity search,} Proc. 2nd Int. Workshop on Similarity Search and Applications (SISAP 2009), Prague, Czech Republic, 2009, pp. 39-46. 

\bibitem{pestov0708} V. Pestov, {\em Intrinsic dimension of a dataset: what properties does one expect?} - In: Proc. 20th Int. Joint Conf. on Neural Networks, Orlando, FL, 2007, pp. 1775--1780; {\em An axiomatic approach to intrinsic dimension of a dataset.} - Neural Networks \textbf{21}, 2-3 (2008), 204-213.

\bibitem{ttf} 
C. Traina, Jr., A.J.M. Traina, and C. Faloutsos,
{\em Distance exponent: A new concept for selectivity estimation in metric
trees,} 
Technical Report CMU-CS-99-110, Computer Science Department, 
Carnegie Mellon University, 1999.

\bibitem{VolPest09} I. Volnyansky and V. Pestov, {\em Curse of dimensionality in pivot-based indexes.} - Proc. 2nd Int. Workshop on Similarity Search and Applications (SISAP 2009), Prague, Czech Republic, 2009,
pp. 39-46. 

\end{thebibliography}

\end{document}
