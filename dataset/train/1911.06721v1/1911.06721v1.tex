\section{Introduction}
Remote sensing via computer vision and transfer learning is an important domain to address climate change as outlined by \citet{rolnick2019:climate_change_ml}. Among others, research in remote sensing promises to help in solving challenges in food security (precision farming), water sustainability, disaster prevention (floods/landslides/earthquake forecasting), deforestation or wild fire detection, urban planning, and monitoring of carbon stocks and fluxes or the air quality.

The number of Earth observing satellites is constantly increasing, with currently over 700 satellites monitoring many aspects of the Earth's surface and atmosphere from space, generating terabytes of imagery data every day. However the ground truth data acquisition is costly, usually requiring extensive campaign preparation, people and equipment transportation, and in-field gathering of the characteristics under question.

While there are remote sensing communities working on applying general deep learning methods to remote sensing problems, this domain has received relatively little attention from the representation learning community.
Given its importance, it is still in an early development stage in comparison to the progress made in representation learning on natural and medical images (eg. by \cite{raghu2019:transfusion}).

Some reasons for this are the diversity of data sources (satellite types, data acquisition modes, resolutions), the need of domain knowledge and special data processing, and the wide and scattered field of applications. The scarcity of standard recognized benchmark datasets and evaluation frameworks is another.

For a long time there were only small labeled remote sensing datasets available. Only recently new large-scale datasets have been generated in this domain (eg. by \citet{zhu2018:so2sat, sumbul2019:bigearthnet}). However, a consistent evaluation framework is still missing and the performance is usually reported on non-standard splits and with varying metrics, making reproduction and quick research iteration difficult.



To address this, we provide five representative and diverse remote sensing datasets in a standardized form for easy reuse. In particular, we explore the importance of in-domain representation learning for remote sensing at various data sizes and establish new state-of-the-art baseline results. The main goal of this work is to develop general remote sensing representations that can be applied by researchers to other unseen remote sensing tasks.

By providing these standardized datasets, common problem definition and baselines, we hope this work will simplify and enable faster iteration of research on remote sensing and inspire general representation learning experts to test their newest methods in this critical domain.

In summary, the main contributions of this work are as follows:
\begin{enumerate}
    \item Exploring in-domain supervised fine-tuning to train generic remote sensing representations.
    \item Generating 5 existing remote sensing datasets in a standardized format and establishing a common evaluation
    \iffinal
    protocol\footnote{Published at \url{https://www.tensorflow.org/datasets}}.
    Publishing the best trained representation in TensorFlow Hub for easy reuse in transfer learning applications\footnote{Published at \url{https://tfhub.dev/google/collections/remote_sensing/1}}.
    \else
    protocol. Publishing the best trained representations for easy reuse in transfer learning  applications\footnote{To be released upon publication.}.
    \fi
    \item Establishing state-of-the-art baselines for the \ben, \eur, \res, \sos, and \ucm\ datasets.
\end{enumerate}




\section{Related Work}
\subsubsection*{Representation and transfer learning}
Shortly after AlexNet \citep{krizhevsky2012:imagenet} was trained on ImageNet \citep{deng09:imagenet},
representations obtained from it were used as off-the-shelf feature extractor networks \citep{cnn-off-the-shelf}. Fine-tuning approaches were also explored and led to better results than random-initialization even when the tasks or domains differ \citep{how-transferable, kornblith2019:imagenet}. These approaches extend beyond natural images and have been used the in medical domain, leading \citet{raghu2019:transfusion} to question on how they work.
Simultaneously there have been many improvements on training representations. \citet{mahajan2018:weakly-sup} explored training bigger models on larger datasets with weaker labels and \citep{ngiam2018:jft-specialist, cui2018:inaturalist} showed gains by closer matching the domains. Very promising are the more sample-efficient approaches using semi- or self-supervision \citep{zhai2019:S4L}.


\subsubsection*{Deep Learning in Remote sensing}
Common remote sensing problems include land-use and land cover (LULC) classification, physical and bio-geo-chemical parameter estimation, target detection, time-series analysis, pan-sharpening and change detection. Many of these tasks can be solved or helped by deep learning approaches related to classification, object detection, semantic segmentation, or super-resolution. Reviews of some of these approaches can be found for instance in \citep{ball2017:dl-rs-review, zhu2017:dl_rs_review, zhu2019:dl-rs}. 

Remote sensing data is acquired in different modes: optical, multi- and hyper-spectral, synthetic aperture radar (SAR), lidar, spectrometer \citep{elachi2006:rs-introduction}. Each of these modes has its own acquisition geometry and specific characteristics providing unique and complimentary information about the Earth's surface. In dependence of the instrument, the remote sensing imagery is available from very high resolutions at centimeter scale (aerial optical or radar, eg.\ for urban monitoring) to very low resolution at kilometer scale (eg.\ for atmosphere and ocean surface monitoring). Another important satellite data characteristic is the revisit time (how fast does a satellite revisit the same location, and constructs a time series), which can range from daily to multiple months.

The majority of deep learning approaches in remote sensing are currently based on optical imagery at high (0.3-1 m) to medium (10-30 m) resolution, obtained from aerial imagery (such as seen on Google Earth) or publicly available satellite data (eg.\ NASA's Landsat or ESA's Sentinel-2 satellites), respectively. Though, multi-spectral, hyper-spectral, and radar imagery is increasingly being used as well.



\subsubsection*{Representation/Transfer learning in remote sensing}
Since labeling remote sensing data is expensive, for a long time there was no equivalent to ImageNet and most benchmark datasets were small. The probably most used remote sensing benchmarking dataset, UC Merced \citep{yang2010:ucmerced}, has only 2100 images with 100 images per class. 
Because it was not possible to train large state-of-the-art network on a small dataset from scratch, remote sensing researchers started to build smaller networks, as for instance in \cite{luus2015:cnn-multiview}.



Another direction was to use models pre-trained on natural images. For instance, \citet{marmanis2016} used the extracted CNN features from the pre-trained Overfeat model \citep{sermanet2013:overfeat} to feed to another CNN model. \citet{castelluccio2015} used GoogLeNet \citep{szegedy15:googlenet} pre-trained on ImageNet to fine-tune UC Merced.
Similarly, \citet{nogueira2016:cnn} used pre-trained models on natural imagery to fine-tune five models based of cross--validation splits. Afterwards, they trained an additional SVM on the fine-tuned features to achieve best performance.


Although there are works in using pre-trained representations, there is hardly any work on training representations specific for solving tasks on this domain. An example is the multi-layer generative adversarial network (GAN) \citep{goodfellow2014:gan} approach for unsupervised representation learning presented by \citet{lin2017:rs-gan-rl}. \citet{xie2015:tl-poverty-mapping} developed a transfer learning pipeline for mapping poverty distribution based on transfering the nighttime lights prediction task. Not specifically addressing representation learning, but a still related application using conditional GANs \citep{mirza2014:cgan} was demonstrated for cloud removal from RGB images by fusing RGB and near infrared bands in \citet{enomoto2017:rs-cgan}, or by fusing multi-spectral and synthetic aperture radar data \citep{grohnfeldt2018:rs-cgan}.



\section{Datasets}
\label{sec:datasets}
For this work, five diverse datasets were selected. We prioritized newer and larger datasets that are quite diverse from each other, address scene classification tasks, and include at least optical imagery. 

Image data comes either from aerial high-resolution imagery or from satellites. Three datasets include imagery from the European Space Agency's (ESA) Sentinel-2 satellite constellation, that provides medium resolution imagery of the Earth's surface every three days. The multi-spectral imager on Sentinel-2 delivers next to the 3 RGB channels additional channels at various frequencies (see \cref{sec:sen2}).
One dataset includes co-registered imagery from a dual-polarimetric synthetic aperture radar (SAR) instrument of ESA's Sentinel-1.



Besides the differences in data sources, number of training samples, number of classes, image sizes and pixel resolutions (summarized in \cref{tab:dataset_overview}), the datasets are also quite diverse across:
\begin{itemize}
    \item Intra- and inter-class visual diversity: some datasets have high in-class and low between-classes diversity and vice versa.
    \item Label imbalance: some datatasets are perfectly balanced, while others are highly unbalanced.
    \item Label domain: land-use land cover (LULC), urban structures, fine ecological labels.
    \item Label quality: from fine human selection to weak labels from auxiliary datasets\footnote{Some datasets and images are affected by not filtered-out cloud and snow coverage that makes the correct classification of the samples difficult.}.
\end{itemize}




While having only 5 datasets might not allow us to completely disentangle the confounding factors of remote sensing representation learning, it should still help us in understanding the important factors for good remote sensing representation learning datasets.



\begin{table}[tb]
    \centering
    \caption{Overview of considered remote sensing datasets.
}
    \label{tab:dataset_overview}
    \begin{tabular}{lllrrrrl}
    \toprule
    Name & year & Source & Size & Classes & Image size & Resolution & Problem \\
    \midrule
    \ben & 2019 & Sentinel-2 & ~590k & 43 & 120x120* & 10--60 m  & multi-label \\
    \eur & 2019 & Sentinel-2 & 27k & 10 & 64x64 & 10 m & multi-class\\
    \res & 2017 & aerial & 31.5k & 45 & 256x256 & 0.2--60+ m & multi-class\\
    \sos & 2019 & Sentinel-1/2 & ~376k & 17 & 32x32 & 10 m & multi-class\\
    \ucm & 2010 & aerial & 2.1k & 21 & 256x256 & 0.3 m & multi-class\\
    \bottomrule
    \end{tabular}
    *Image size varies in dependence of resolution from 120x120 to 60x60 to 20x20.
\end{table}

 

\subsection{Individual datasets}
In addition to the descriptions of the individual datasets in this section, \cref{sec:datasets-extra} shows example images for all datasets and the distribution of classes.



{\bf \ben{}} \citep{sumbul2019:bigearthnet} is a challenging large-scale multi-spectral dataset consisting of 590,326 image patches from the Sentinel-2 satellite. 12 frequency channels (including RGB) are provided, each covering an area of 1.2 $\times$ 1.2 km with resolutions of 10 m, 20 m, and 60 m per pixel. This is a multi-label dataset where each image is annotated by multiple land-cover classes. The label distribution is highly unbalanced ranging from 217k images of ``mixed forest" label to only 328 images with label ``burnt areas". About 12\% of the patches are fully covered by seasonal snow, clouds or cloud shadows. The only available public baseline metrics include precision and recall values of 69.93\% and 77.1\%, respectively, for using a shallow CNN on a reduced dataset, after removing the snow and cloud affected samples.



{\bf \eur{}} \citep{helber2019:eurosat} is another recently published dataset containing 27,000 images from Sentinel-2 satellites. All 13 frequency bands of the satellite are included. Each image covers an area of 640 $\times$ 640 meters and is assigned to one of 10 LULC classes, with 2000 to 3000 images per class. Because the classes are quite distinctive, very high accuracies can be achieved when using the entire dataset for training.



{\bf NWPU \res{}} \citep{cheng2017:resisc45} dataset is an aerial dataset consisting of 31,500 RGB images divided into 45 scene classes. Each class includes 700 images with a size of 256 $\times$ 256 pixels. This is the only dataset with varying spatial resolution ranging from 20 cm to more than 30 meters. The data covers a wide range of countries and biomes. During the construction, the authors paid special attention to have classes with high same-class diversity and between-class similarity to make it more challenging.



{\bf \sos\ LCZ-42} \citep{zhu2018:so2sat} is a dataset consisting of co-registered SAR and multi-spectral 320 $\times$ 320 m image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, and the corresponding local climate zones (LCZ) \citep{stewart12:lcz} labels. The dataset is distributed over 42 cities across different continents and cultural regions of the world. This is another challenging dataset and it is intended for learning features to distinguish various urban zones. The challenge of this dataset is the relatively small image size (32$\times$32) and the relatively high inter-class visual similarity.



{\bf \ucm{}} Land-Use Dataset \citep{yang2010:ucmerced} is a high-resolution (30 cm per pixel) dataset that was extracted from aerial imagery from the United States Geological Survey (USGS) National Map over multiple regions in the United States.
The 256 $\times$ 256 RGB images cover 21 land-use classes, with  100 images per class. 
This is a relatively small datasets that has been widely benchmarked for remote sensing scene classification task since 2010 and for which nearly perfect accuracy can be achieved with modern convolutional neural networks \citep{castelluccio2015, marmanis2016, nogueira2016:cnn}.





\subsection{Release of standardized datasets}




\iffinal
To simplify access to the data and its usage, we imported and published these datasets in 
Tensorflow Datasets (TFDS)\footnote{Published at \url{https://www.tensorflow.org/datasets}}.
\else
To simplify access to the data and its usage, we imported and published these datasets\footnote{To be updated/released upon publication.}.
\fi



For reproducability and a common evaluation framework, standard \emph{train, validation}, and \emph{test} splits using the 60\%, 20\%, and 20\% ratios, respectively, were generated for all datasets except \sos.

For the \sos\ dataset, the source already provides train and validation splits. To generate the test split, the original upstream validation is separated into validation and test splits with the 25\% and 75\% ratios, respectively.














\section{Remote Sensing Data Processing}
The remote sensing domain is quite distinctive from natural image domain and requires special attention during pre-processing and model construction. Some characteristics are:
\begin{itemize}
    \item Remote sensing input data usually comes at higher precision (16 or 32 bits).
    \item The number of channels is variable, depending on the satellite instrument. RGB channels are only a subset of a multi- or hyper-spectral imagery dataset. Other data sources might have no optical channels (eg.\ radar or lidar) and the channels distribution can be determined by polarimetric, interferometric or frequency diversity.
    \item The range of values varies largely from dataset to dataset and between channels. The values distribution can be highly skewed.
    \item Many quantitative remote sensing tasks rely on the absolute values of the pixels.
    \item The images acquired from space are usually rotation invariant. 
    \item Source data can be delivered at different product levels (for instance w/ or w/o atmospheric correction, co-registration, orthorectification, radiometric calibration, etc.). 
    \item Especially lower resolution data aggregates a lot of information about the illuminated surface in a single pixel since it covers a large area.
    \item Image axes might be non-standard, eg.\ representing range and azimuth dimensions.
\end{itemize}

This sets some requirements on pre-processing and encourages to adjust data augmentation of the input pipeline for remote sensing data.

Specifically for the problems discussed in this paper, it is recommended to rescale and clip the range of values per channel (accounting for outliers). Data augmentation that affects the intensity of the values should be discarded. On the other hand, one can reuse the rotation invariance and extend the augmentation to perform all rotations and flipping (providing 7 additional images per sample). Given multi-spectral data, such as Sentinel-2 based \ben{}, \eur{}\ and \sos{}, one can use other subsets of channels instead of RGB including all available ones.



\section{Experimental Results}




The main goal is to develop representations that can be used across a wide field of remote sensing applications on unseen tasks. The training and evaluation protocol follows two main stages: (1) \emph{upstream} training of the representations model based on some out- or in-domain data, and (2) \emph{downstream} evaluation of the representations by transferring the trained representation features to the new downstream tasks. For the upstream training on in-domain proxy datasets the entire data is used, that cannot include any data from the downstream tasks. The quality of the trained representations and their generalizability is often evaluated on reduced downstream training sizes to assess efficiency of the representations.

\subsection{Experimental setup}
In order to simplify the training procedure and to draw more general conclusions, all experiments use the same ResNet50 V2 architecture \citep{he2016:identity} and configuration. However, due to the varied number of classes and training samples in the various datasets, we perform sweeps over a small set of hyper-parameters and augmentations
as described in detail in \cref{sec:hparams}.




\subsection{Evaluation metrics}
We report performance results using accuracy metrics commonly used in computer vision tasks: for multi-class problems we use the Top-1 global accuracy metric, which denotes the percentage of correctly labeled samples.
For multi-label problems we use the mean average precision (mAP) metric, which denotes the mean over the average precision (AV) values (AV is the integral over the precision-recall curve) of the individual labels. 

To measure and aggregate relative performance improvement over datasets performing at quite different accuracy levels, the logit transformation of the accuracy is preferred \citep{kornblith2019:imagenet}:
\[ \Delta = \text{logit}(\rho) = \log\left(\frac{\rho}{1-\rho}\right) = \frac{1}{\text{sigmoid}(\rho)} \]
where $\rho$ is the accuracy rate (Top-1 or mAP) and $\Delta$ is the corresponding logit accuracy. It captures the importance of accuracy change at different accuracy levels to provide a more fair evaluation of the achievement.







\subsection{Comparing in-domain representations}
\label{training-in-domain-repr}
To obtain in-domain representations, first we train models either \emph{from scratch} or by \emph{fine-tuning} ImageNet on each full dataset. The best of these models are then used as in-domain representations to train models on other remote sensing tasks (excluding the one used to train the in-domain representation).

For an initial evaluation of the different in-domain representation source data, Table \cref{tab:cross-in-domain} shows a cross-table of evaluating each trained in-domain and ImageNet representation on each of the downstream tasks. The representations were trained using full datasets upstream, while the down-stream tasks used only 1000 training examples to better emphasize the differences.














\begin{table}[tb]
    \caption{Performance of trained In-Domain and ImageNet representations (rows) when using 1000 training examples for downstream tasks (columns). Emphasized (bold font) are the best accuracies per downstream task (column).}
    \label{tab:cross-in-domain}
    \centering

\begin{tabular}{lrrrrr}
\toprule
Source \textbackslash Target & \bigearthnet{} &    \eurosat{} &   \resisc{} &     \sosat{} &  \ucmerced{} \\
\midrule
\imagenet{}    &       25.10 &      96.84 &      84.89 &      53.69 &      99.02 \\
\bigearthnet{} &           - &      96.45 &      78.43 &      50.91 &  \bf 99.61 \\
\eurosat{}     &       27.10 &          - &      79.59 &      52.99 &      98.05 \\
\resisc{}    &   \bf 27.59 &  \bf 97.14 &          - &  \bf 54.43 &  \bf 99.61 \\
\sosat{}      &       26.30 &      96.30 &      77.70 &          - &      97.27 \\
\ucmerced{}   &       26.86 &      96.73 &  \bf 85.73 &      53.52 &          - \\
\bottomrule
\end{tabular}


\end{table}

 
It can be observed that with 1000 training examples, the best results all come from fine-tuning the in-domain representations. 

Additionally, despite having 2 distinctive groups of high-resolution aerial (\res{}, \ucm{}) and medium-resolution satellite datasets (\ben, \eur\ and \sos), the representations trained on \res\ were able to outperform the others in all tasks (\ben\ representations tied for the \ucm\ dataset) and it was the only representation to consistently outperform ImageNet-based representations.

That \res\ would perform so good on both aerial and satellite data was unexpected. The reason is most likely related to the fact that \res\ is the only dataset that has images with various resolutions. 
Combined with the large number of classes that have high within class diversity and high between-class similarity it seems to be able to train good representations for a wide range of remote sensing tasks, despite not being a very big dataset. This learning can be reused to adjust augmentation schemes for remote sensing representation learning by stronger varying the scale of images with randomized zooms and potentially other affine transformations.



The best representations for the \res\ itself come from the other aerial dataset, \ucm{}. The relatively small training size of it was counter-balanced by an aggressive augmentation (\rcropPP{} as described in \cref{sec:preprocessing-appendix}). 

Counter to the expectation that bigger datasets should train better representations, the two biggest datasets, \ben{} and \sos{}, didn't provide the best representations (except of \ben\ representations for \ucm). We hypothesize that this might be due to the weak labeling and the low training accuracy obtained in these datasets. It is possible that the full potential of these large-scale datasets was not yet fully utilized and other self- or semi-supervised representation learning approaches could improve the performance.



\subsection{Large-scale comparison}
Having trained in-domain representations, we can now evaluate and compare the transfer quality of fine-tuning the best in-domain representations with fine-tuning ImageNet and training from scratch at various training data sizes. The results using the default experimental setup are shown in \cref{tab:overall}.

In all cases, fine-tuning from ImageNet is better than training from scratch. And in all but one case, fine-tuning from an in-domain representation for transfer is even better.

The only exception is the \ben\ dataset at its full size. It is expected that having a large dataset should reduce the need for pre-training, but the gap between in-domain and ImageNet pre-training is quite big. We don't have an explanation for this yet and this needs to be further investigated.







\begin{table}[b]
    \centering
    \caption{Accuracy over different training methods and number of used training samples.}
    \label{tab:overall}

\setlength{\tabcolsep}{3pt}

\begin{tabular}{l|rrr|rrr|rrr|rrr|rrr}
\toprule
{} & \multicolumn{3}{c|}{\bigearthnet} & \multicolumn{3}{c|}{\eurosat} & \multicolumn{3}{c|}{\resisc} & \multicolumn{3}{c|}{\sosat} & \multicolumn{3}{c}{\ucmerced} \\ 
{} & 100 & 1k & Full & 100 & 1k & Full & 100 & 1k & Full & 100 & 1k & Full & 100 & 1k & Full \\

\midrule
Scratch  &            14.5 &             21.4 &             72.4 &        63.9 &         91.7 &         98.5 &         21.4 &          56.1 &          95.6 &       33.9 &        47.0 &        62.1 &          50.8 &           91.2 &           95.7 \\
ImageNet &            17.8 &             25.1 &         \bf 75.4 &        87.3 &         96.8 &         99.1 &         44.9 &          84.9 &          96.6 &       44.9 &        53.7 &        63.1 &          79.9 &           99.0 &           99.2 \\
InDomain &        \bf 18.8 &         \bf 27.6 &             69.7 &    \bf 91.3 &     \bf 97.1 &     \bf 99.2 &     \bf 49.0 &      \bf 85.7 &      \bf 96.8 &   \bf 46.4 &    \bf 54.4 &    \bf 63.2 &      \bf 91.0 &       \bf 99.6 &       \bf 99.6 \\
\bottomrule
\end{tabular}
\end{table}












 

Overall, these results establish new state-of-the-art baselines for these datasets, as summarized in \cref{tab:benchmarks}. Note that some results are not comparable: \res\ has been previously evaluated only on 20\% of data, \sos\ has no public benchmarking result to our knowledge, and the only published result of \ben{} is based on a cleaner version of the dataset (after removing the noisy images containing clouds and snow) and only precision and recall metrics were reported.








\cref{fig:relative-improvement} summarizes the results of \cref{tab:overall} across all datasets and training sizes. Logit accuracy metric is used, which fits better for aggregation of accuracies across wide ranges. \cref{fig:lacc_datasets} emphasizes that the smaller datasets (\eur, \res, \ucm) profit more from \emph{in-domain} knowledge than the larger datasets (\ben, \sos). As is re-iterated again in \cref{fig:lacc_sizes}, using in-domain representations leads to higher accuracy gain when small number of training examples are available, which is of most interest for remote sensing applications to reduce the ground truth data acquisition. 


\begin{table}[tb]
    \centering
    \caption{Results on selected remote sensing datasets. All results use Top-1 accuracy, except for \bigearthnet{} which uses precision/recall and mean average precision.
    \textit{Our} best results were obtained by fine-tuning in-domain representations except for \ben\ which was obtained by fine-tuning ImageNet.}
    \label{tab:benchmarks}
    \begin{tabular}{llr}
    \toprule
    Dataset & Reference & Result \\
    
    \midrule
    \multirow{2}{*}{\bigearthnet{}} & \cite{sumbul2019:bigearthnet} & 69.93\% / 77.1\% (P/R) \\
    & \textit{Ours} & \textit{75.36}\% (mAP) \\
    
    \midrule
    \multirow{2}{*}{\eurosat{}} & \cite{helber2019:eurosat} & 98.57\%\\
    & \textit{Ours} & \textit{99.20}\% \\

    \midrule
    \multirow{2}{*}{\resisc{}} & \cite{cheng2017:resisc45} & 90.36\% \\
    & \textit{Ours} & \textit{96.83}\% \\
    
    \midrule
    \multirow{1}{*}{\sosat{}} & \textit{Ours} & \textit{63.25}\% \\
    
    \midrule
    \multirow{4}{*}{\ucmerced{}} &
    \cite{marmanis2016} & 92.4\%  \\
    & \cite{castelluccio2015} & 97.1\%  \\
    & \cite{nogueira2016:cnn} & 99.41\% \\
    & \textit{Ours} & \textit{99.61}\% \\
    
    \bottomrule
    \end{tabular}
\end{table}

 

\begin{figure}[tb]


     \centering
     \begin{subfigure}[b]{0.3\textwidth}
        \centering
\includegraphics[width=1.0\textwidth]{figures/lacc/lacc_ds2}
         \caption{Over datasets}
         \label{fig:lacc_datasets}
     \end{subfigure}~\qquad
     \begin{subfigure}[b]{0.3\textwidth}
        \centering
\includegraphics[width=1.0\textwidth]{figures/lacc/lacc_sizes2.pdf}
         \vspace*{0.5em}
         \caption{Over sizes}
         \label{fig:lacc_sizes}
     \end{subfigure}
     \qquad
     \begin{subfigure}[b]{0.2\textwidth}
        \centering
\includegraphics[width=0.7\textwidth]{figures/lacc/labels_lacc.pdf}
         \vspace*{6em}
     \end{subfigure}
    \caption{Aggregated mean relative improvement in logit accuracy of fine-tuning from ImageNet and in-domain representations in comparison to training from scratch.
}
    \label{fig:relative-improvement}
\end{figure}





\subsection{Limited numbers of training examples}
To look closer into in-domain representation learning for small number of training examples, we trained models with small training sizes ranging from 25 to 2500 (samples were randomly drawn disregarding class distributions). We used a simplified set of hyper-parameters that might not deliver the most optimal performance, but still allows to observe the general trends.
As shown in \cref{fig:dense}, the improvement of using in-domain representations is clearly visible for the \eur, \res\ and \ucm\ datasets. These are the 3 smaller datasets with higher quality labels. The results are less conclusive for the \ben\ and \sos\ datasets that have more noisy labels.





\begin{figure}[tb]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dense/bigearthnet_dense.pdf}
         \caption{\bigearthnet{}}
         \label{fig:dense bigearthnet}
     \end{subfigure}
     \quad
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dense/eurosat_dense.pdf}
         \caption{\eurosat{}}
         \label{fig:dense eurosat}
     \end{subfigure}
     \quad
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dense/resisc45_dense.pdf}
         \caption{\resisc{}}
         \label{fig:dense resisc45}
     \end{subfigure}

    \medskip
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dense/so2sat_dense.pdf}
         \caption{\sosat{}}
         \label{fig:dense so2sat}
     \end{subfigure}
     \quad
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dense/uc_merced_dense.pdf}
         \caption{\ucmerced{}}
         \label{fig:dense uc_merced}
     \end{subfigure}
     \quad
     \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \raisebox{1.1cm}{
        \includegraphics[width=0.7\textwidth]{figures/dense/labels_dense.pdf}
        }
     \end{subfigure}

    \caption{Top-1 accuracy rate or mean average precision (mAP) on validation set after training with a given method over a limited number of training examples on each dataset.}
\label{fig:dense}
\end{figure}
 
















\section{Conclusion}
We present a common evaluation benchmark for remote sensing representation learning based on five diverse datasets. The results demonstrate the enhanced performance of \emph{in-domain} representations, especially for tasks with limited number of training samples, and achieve state-of-the-art performance.
The five analyzed datasets and the best trained in-domain representations are published for easy reuse by the public.

We investigate dataset characteristics to be a good source for remote sensing representation learning. 
As the experimental results indicate, having a multi-resolution dataset helps to train more generalizable representations. Other factors seem to be label quality, number of classes, visual similarity across the classes and visual diversity within the classes. Surprisingly, we observed that representations trained on the large weakly-supervised datasets were not as successful as that of a smaller and more diverse human-curated dataset.

However, some results were inconclusive and require more investigation. Understanding the main factors of a good remote sensing dataset for representation learning is a major challenge, solving which could improve performance  across a wide range of remote sensing tasks and applications.





