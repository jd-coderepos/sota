\documentclass[letterpaper,11pt]{article}



\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[usenames]{color}
\usepackage{fullpage}
\usepackage{xspace}
\usepackage{url,ifthen}
\usepackage{srcltx}
\usepackage{multirow}
\usepackage{boxedminipage}
\usepackage[margin=1.2in]{geometry}
\usepackage{nicefrac}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{color}
\definecolor{DarkGreen}{rgb}{0.1,0.5,0.1}
\definecolor{DarkRed}{rgb}{0.5,0.1,0.1}
\definecolor{DarkBlue}{rgb}{0.1,0.1,0.5}

\usepackage[small]{caption}

\usepackage[pdftex]{hyperref}
\hypersetup{
    unicode=false,          pdftoolbar=true,        pdfmenubar=true,        pdffitwindow=false,      pdfnewwindow=true,      colorlinks=true,       linkcolor=DarkRed,          citecolor=DarkGreen,        filecolor=DarkRed,      urlcolor=DarkBlue,          pdftitle={},
    pdfauthor={},
}


\def\draft{0}

\def\submit{0}

\ifnum\draft=1 \def\ShowAuthNotes{1}
\else
    \def\ShowAuthNotes{0}
\fi

\ifnum\submit=1
\newcommand{\forsubmit}[1]{#1}
\newcommand{\forreals}[1]{}
\else
\newcommand{\forreals}[1]{#1}
\newcommand{\forsubmit}[1]{}
\fi

\ifnum\ShowAuthNotes=1
\newcommand{\authnote}[2]{{ \footnotesize \bf{\color{DarkRed}[#1's Note:
{\color{DarkBlue}#2}]}}}
\else
\newcommand{\authnote}[2]{}
\fi

\newcommand{\mnote}[1]{{\authnote{Moritz} {#1}}}
\newcommand{\Mnote}[1]{{\authnote{Moritz} {#1}}}
\newcommand{\knote}[1]{{\authnote{Katrina} {#1}}}
\newcommand{\Knote}[1]{{\authnote{Kunal} {#1}}}
\newcommand{\Fnote}[1]{{\authnote{Frank} {#1}}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{itheorem}{Informal Theorem}[section]
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{conjecture}{Conjecture}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\newcommand{\chapterlabel}[1]{\label{ch:#1}}
\newcommand{\chapterref}[1]{\hyperref[ch:#1]{Chapter~\ref{ch:#1}}}
\newcommand{\claimlabel}[1]{\label{claim:#1}}
\newcommand{\claimref}[1]{\hyperref[claim:#1]{Claim~\ref{claim:#1}}}
\newcommand{\corollarylabel}[1]{\label{cor:#1}}
\newcommand{\corollaryref}[1]{\hyperref[cor:#1]{Corollary~\ref{cor:#1}}}
\newcommand{\definitionlabel}[1]{\label{def:#1}}
\newcommand{\definitionref}[1]{\hyperref[def:#1]{Definition~\ref{def:#1}}}
\newcommand{\equationlabel}[1]{\label{eq:#1}}
\newcommand{\equationref}[1]{\hyperref[eq:#1]{Equation~\ref{eq:#1}}}
\newcommand{\factlabel}[1]{\label{fact:#1}}
\newcommand{\factref}[1]{\hyperref[fact:#1]{Fact~\ref{fact:#1}}}
\newcommand{\figurelabel}[1]{\label{fig:#1}}
\newcommand{\figureref}[1]{\hyperref[fig:#1]{Figure~\ref{fig:#1}}}
\newcommand{\itemlabel}[1]{\label{item:#1}}
\newcommand{\itemref}[1]{\hyperref[item:#1]{Item~\ref{item:#1}}}
\newcommand{\lemmalabel}[1]{\label{lem:#1}}
\newcommand{\lemmaref}[1]{\hyperref[lem:#1]{Lemma~\ref{lem:#1}}}
\newcommand{\proplabel}[1]{\label{prop:#1}}
\newcommand{\propref}[1]{\hyperref[prop:#1]{Proposition~\ref{prop:#1}}}
\newcommand{\propositionlabel}[1]{\label{prop:#1}}
\newcommand{\propositionref}[1]{\hyperref[prop:#1]{Proposition~\ref{prop:#1}}}
\newcommand{\remarklabel}[1]{\label{rem:#1}}
\newcommand{\remarkref}[1]{\hyperref[rem:#1]{Remark~\ref{rem:#1}}}
\newcommand{\sectionlabel}[1]{\label{sec:#1}}
\newcommand{\sectionref}[1]{\hyperref[sec:#1]{Section~\ref{sec:#1}}}
\newcommand{\theoremlabel}[1]{\label{thm:#1}}
\newcommand{\theoremref}[1]{\hyperref[thm:#1]{Theorem~\ref{thm:#1}}}






\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[varg]{txfonts} \renewcommand{\mathbb}{\varmathbb}
\renewcommand{\Bbbk}{\varBbbk}
\usepackage{microtype}

\newcommand{\Esymb}{\mathbb{E}}
\newcommand{\Psymb}{\mathbb{P}}
\newcommand{\Vsymb}{\mathbb{V}}
\DeclareMathOperator*{\E}{\Esymb}
\DeclareMathOperator*{\Var}{\Vsymb}
\DeclareMathOperator*{\ProbOp}{\Psymb r}
\renewcommand{\Pr}{\ProbOp}
\newcommand{\prob}[1]{\Pr\big\{ #1 \big\}}
\newcommand{\Prob}[1]{\Pr\left\{ #1 \right\}}
\newcommand{\varprob}[1]{\Pr\big( #1 \big)}
\newcommand{\varProb}[1]{\Pr\left( #1 \right)}
\newcommand{\ex}[1]{\E\big[#1\big]}
\newcommand{\Ex}[1]{\E\left[#1\right]}
\newcommand{\varex}[1]{\E\paren{#1}}
\newcommand{\varEx}[1]{\E\Paren{#1}}
\newcommand{\widebar}[1]{\overline{#1}}

\newcommand{\flatfrac}[2]{#1/#2}
\newcommand{\nfrac}{\nicefrac}
\newcommand{\half}{\nfrac12}
\newcommand{\quarter}{\nfrac14}

\newcommand{\mper}{\,.}
\newcommand{\mcom}{\,,}

\renewcommand{\hat}{\widehat}

\newcommand{\cA}{{\cal A}}
\newcommand{\cB}{{\cal B}}
\newcommand{\cC}{{\cal C}}
\newcommand{\cD}{{\cal D}}
\newcommand{\cE}{{\cal E}}
\newcommand{\cF}{{\cal F}}
\newcommand{\cG}{{\cal G}}
\newcommand{\cH}{{\cal H}}
\newcommand{\cI}{{\cal I}}
\newcommand{\cJ}{{\cal J}}
\newcommand{\cK}{{\cal K}}
\newcommand{\cL}{{\cal L}}
\newcommand{\cM}{{\cal M}}
\newcommand{\cN}{{\cal N}}
\newcommand{\cO}{{\cal O}}
\newcommand{\cP}{{\cal P}}
\newcommand{\cQ}{{\cal Q}}
\newcommand{\cR}{{\cal R}}
\newcommand{\cS}{{\cal S}}
\newcommand{\cT}{{\cal T}}
\newcommand{\cU}{{\cal U}}
\newcommand{\cV}{{\cal V}}
\newcommand{\cW}{{\cal W}}
\newcommand{\cX}{{\cal X}}
\newcommand{\cY}{{\cal Y}}
\newcommand{\cZ}{{\cal Z}}

\newcommand{\defeq}{\stackrel{\small \mathrm{def}}{=}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\ge}{\geqslant}
\newcommand{\paren}[1]{(#1 )}
\newcommand{\Paren}[1]{\left(#1 \right )}
\newcommand{\brac}[1]{[#1 ]}
\newcommand{\Brac}[1]{\left[#1 \right]}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\bigset}[1]{\bigl\{#1\bigr\}}
\newcommand{\Set}[1]{\left\{#1\right\}}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\Abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\Bigabs}[1]{\Bigl\lvert#1\Bigr\rvert}
\newcommand{\card}[1]{\lvert#1\rvert}
\newcommand{\Card}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\Norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\from}{\colon}
\newcommand\rd{\,\mathrm{d}}
\newcommand{\bits}{\{0,1\}}
\newcommand{\signs}{\{-1,1\}}
\renewcommand{\Re}{\mathbb{R}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\newcommand{\N}{\mathbb N}
\newcommand{\varR}{\Re}
\newcommand{\Rnn}{\R_+}
\newcommand{\varRnn}{\varR_+}
\renewcommand{\qedsymbol}{{}}
\usepackage{bm}
\renewcommand{\vec}[1]{{\bm{#1}}}


\newcommand{\iprod}[1]{\langle #1\rangle}
\newcommand{\isqur}[1]{\langle #1,#1\rangle}
\newcommand{\trsp}[1]{#1^\dagger}
\newcommand{\vartrsp}[1]{#1^T}
\newcommand{\varvartsp}[1]{#1^{\mathsf{T}}}
\newcommand{\smallrmtrsp}[1]{#1^{\scriptscriptstyle \mathrm{T}}}
\newcommand{\oprod}[2]{\trsp{#1} #2}
\newcommand{\osqur}[1]{\oprod{#1}{#1}}
\newcommand{\trspvec}[1]{{\trsp{\vec{#1}}}}
\newcommand{\snorm}[1]{\norm{#1}^2}
\newcommand{\normt}[1]{\norm{#1}_{\scriptstyle 2}}
\newcommand{\varnormt}[1]{\norm{#1}_{\scriptscriptstyle 2}}
\newcommand{\snormt}[1]{\normt{#1}^2}
\newcommand{\normo}[1]{\norm{#1}_{\scriptstyle 1}}
\newcommand{\Normo}[1]{\Norm{#1}_{\scriptstyle 1}}
\newcommand{\varnormo}[1]{\norm{#1}_{\scriptscriptstyle 1}}
\newcommand{\normi}[1]{\norm{#1}_{\scriptstyle \infty}}
\newcommand{\varnormi}[1]{\norm{#1}_{\scriptscriptstyle \infty}}

\newcommand{\super}[2]{#1^{\paren{#2}}}
\newcommand{\varsuper}[2]{#1^{\scriptscriptstyle\paren{#2}}}

\newcommand{\Ind}{\mathbb I}



\newcommand{\maxcut}{\textsc{Max Cut}\xspace}
\newcommand{\vertexcover}{\textsc{Vertex Cover}\xspace}
\newcommand{\uniquegames}{\textsc{Unique Games}\xspace}
\newcommand{\Erdos}{Erd\H{o}s\xspace}
\newcommand{\Renyi}{R\'enyi\xspace}
\newcommand{\Lovasz}{Lov\'asz\xspace}
\newcommand{\Juhasz}{Juh\'asz\xspace}
\newcommand{\Bollobas}{Bollob\'as\xspace}
\newcommand{\Furedi}{F\"uredi\xspace}
\newcommand{\Komlos}{Koml\'os\xspace}
\newcommand{\Luczak}{\L uczak\xspace}
\newcommand{\Kucera}{Ku\v{c}era\xspace}
\newcommand{\Szemeredi}{Szemer\'edi\xspace}
\newcommand{\Koetter}{Koetter\xspace}

\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\opt}{\mathrm{opt}}
\newcommand{\KL}{\mathrm{RE}}
\newcommand{\sign}{\mathit{sign}}
\newcommand{\polylog}{{\rm polylog}}
\newcommand{\poly}{{\rm poly}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\supp}{\mathrm{supp}}


\renewcommand{\epsilon}{\varepsilon}
\newcommand{\e}{\epsilon}
\newcommand{\eps}{\epsilon}

\newcommand{\Laplace}{\mathit{Lap}}

\newcommand{\remove}[1]{}

\newcommand{\draftbox}{\begin{center}
  \fbox{\begin{minipage}{2in}\begin{center}\begin{Large}\textsc{Working Draft}\end{Large}\\
        Please do not distribute\end{center}\end{minipage}}\end{center}
\vspace{0.2cm}}



\newcommand{\listoptions}{\labelsep0mm\topsep-0mm\itemindent-6mm\itemsep0mm}
\newcommand{\displayoptions}[1]{\abovedisplayshortskip#1mm\belowdisplayshortskip#1mm\abovedisplayskip#1mm\belowdisplayskip#1mm}

\newcommand{\ti}{\tilde}
\newcommand{\range}{\mathrm{range}}
\newcommand{\PFP}{\textrm{\small PFP}\xspace}




\title{Beating Randomized Response on Incoherent Matrices}
\author{Moritz Hardt\thanks{IBM Research Almaden. Email: {\tt
mhardt@us.ibm.com}}
\and Aaron Roth\thanks{Department of Computer and Information Sciences, University of Pennsylvania. Email: {\tt aaroth@cis.upenn.edu}}}

\begin{document}

\maketitle

\begin{abstract}
Computing accurate low rank approximations of large matrices is a fundamental
data mining task. In many applications however the matrix contains sensitive
information about individuals. In such case we would like to release a low
rank approximation that satisfies a strong privacy guarantee such as
differential privacy. Unfortunately, to date the best known algorithm for this
task that satisfies differential privacy is based on naive \emph{input
perturbation} or \emph{randomized response}: Each entry of the matrix is
perturbed independently by a sufficiently large random noise variable, a low
rank approximation is then computed on the resulting matrix.

We give (the first) significant improvements in accuracy over randomized
response under the natural and necessary assumption that the matrix has
\emph{low coherence}.  Our algorithm is also very efficient and finds a
constant rank approximation of an  matrix in time  Note
that even generating the noise matrix required for randomized response already
requires time 
\end{abstract}

\vfill
\thispagestyle{empty}
\pagebreak

\section{Introduction}
Consider a large  matrix  in which rows correspond to
individuals, columns correspond to movies, and the non-zero entry in 
represent the rating that individual  has given to movie .  Such a data
set shares two important characteristics with many other data sets:
\begin{enumerate}
\item It can be represented as a \emph{matrix} with very different dimensions. There are many more people than movies, so 
\item It is composed of \emph{sensitive information}: the rating that an individual gives to a particular movie (and the very fact that he watched said movie) can be possibly compromising information.
\end{enumerate}
Nevertheless, although we want to reveal little about the existence of
individual ratings in this data set, it might be extremely useful to
be able to allow data analysts to mine such a matrix for statistical
information. Even while protecting the privacy of individual entries, it might
still be possible to release another matrix that encodes a great deal of
information about the original data set. For example, we might hope to be able
to recover the cut structure of the corresponding rating graph, perform
principal component analysis (PCA), or apply some other data mining technique.

Indeed, this example is not merely theoretical. Data of exactly this form was
released by Netflix as part of their competition to design improved
recommender systems. Spectral methods such as PCA were commonly used on this
dataset, and privacy concerns were acknowledged: Netflix attempted to
``anonymize'' the dataset in an ad-hoc way. Following this supposedly
anonymized release, Naranyanan and Shmatikov \cite{NarayananS08} were able to
re-identify many individuals in the dataset by cross-referencing the reviews
with publicly available reviews in the internet movie database. As a result of
their work, a planned second Netflix challenge was canceled. The story need
not have ended this way however -- the formal privacy guarantee known as
\emph{differential privacy} could have prevented the attack of
\cite{NarayananS08}, and indeed, McSherry and Mironov \cite{McSherryM09}
demonstrated that many of the recommender systems proposed in the competition
could have been implemented in a differentially private way.
\cite{McSherryM09} make use of private low-rank matrix approximations using
input perturbation methods. In fact, it is not possible to generically improve
on input perturbation methods for all matrices without violating \emph{blatant
non privacy} \cite{DinurN03}. Nevertheless,  in this paper, we give the first
algorithms for low rank matrix approximation with performance guarantees that
are significantly better than input perturbation, under certain commonly
satisfied conditions \emph{which are already assumed} in prior work on
non-private low-rank matrix approximation.

In this paper, we consider the problem of privately releasing accurate
low-rank approximations to datasets that can be represented as matrices. Such
matrix approximations are one of the most fundamental building blocks for
statistical analysis and data mining, with key applications including latent
semantic indexing and principle component analysis. We provide theorems
bounding the accuracy of our approximations as compared to the optimal low
rank approximations in the Frobenius norm. The classical Eckart-Young theorem
asserts that the
optimal rank- approximation of a matrix  (in either the Frobenius or
Spectral norms) is obtained by computing the singular value decomposition , and releasing the \emph{truncated} SVD ,
where in , all but the top  singular values have been zeroed out.
Computing the SVD of a matrix takes time . In addition to offering
privacy guarantees, our algorithm is also extremely efficient: it requires
only elementary matrix operations and simple noisy perturbations, and for
constant  takes time only . This represents a happy confluence of
the two goals of privacy and efficiency. Normally, the two are at odds, and
differentially private algorithms tend to be (much) less efficient than their
non-private counterparts. In this case, however, we will see that some algorithms
for fast approximate low-rank matrix approximation are much more amenable
to a private implementation than their slower counterparts.

Computing low rank matrix approximations privately has been considered at
least since \cite{BlumDMN05}, and to date, no algorithm has improved over simple
input  perturbation, which achieves an error (when compared with the best rank
 approximation ) in Frobenius norm of . Although this error is optimal without making any assumptions on the matrix, this
error can be prohibitive when the best rank  approximation is actually very
good: when  That is, exactly in the case when a
low rank approximation to the matrix would be most useful.  We give an
algorithm which improves over input perturbation under the conditions that  and that the \emph{coherence} of the matrix is small: roughly, that no
single row of the matrix is too significantly correlated with any of the right singular
vectors of the matrix. Equivalently, no left singular vector has large
correlation with one of the standard basis vectors. Low coherence is a
commonly studied and satisfied condition. For example, Candes and Tao,
motivated by the same Netflix Prize dataset re-identified by
\cite{NarayananS08}, consider the problem of matrix completion under low
coherence conditions \cite{CandesT10}. They show that matrix completion is
possible under low coherence assumptions, and that several reasonable random
matrix models exhibit a strong incoherence property.  Notably, \cite{CandesT10}
were not concerned with privacy at all: they viewed low coherence as a natural
assumption satisfied for datasets resembling the Netflix prize data that could
be leveraged to obtain stronger utility guarantees. This represents a second
happy confluence of the goals of data privacy and utility: low coherence is an
assumption that others \emph{already} make free of privacy concerns
in order to improve the state of the art in data analysis.  We show that the
same assumption can simultaneously be leveraged for data privacy. In
retrospect, low coherence is also an extremely natural condition in the
context of privacy, although one that has not previously been considered in
the literature. If a matrix fails to have low coherence, then intuitively the
data of individual rows of the matrix is encoded closely in individual
singular vectors. If it does have low coherence, no small set of singular
vectors can be used to encode any row of the matrix with high accuracy, and
intuitively, low rank approximations reveal less local information about
particular entries of the matrix.

The problem we solve is the following: Given a matrix  and a target rank
 we privately compute and release a rank  matrix  such that
 is not much larger than , where  is the
\emph{optimal} rank  approximation to , and  is the
\emph{Frobenius norm}. The quality of the approximation depends on several
factors, including , , the desired rank , and the coherence of the
matrix. Our approach improves over input perturbation when the matrix
coherence is small.

Our algorithm promises -\emph{differential privacy}
\cite{DworkMNS06} with respect to changes of any single row of magnitude  in
the -norm.  This is only stronger than the standard notion of changing
any single entry in the matrix by a unit amount. In the very special case of
the matrix representing a (possibly unbalanced) graph, this captures (for
example) the addition or removal of a single edge. Therefore in this case our
algorithm is promising \emph{edge privacy} rather than \emph{vertex privacy}.
From a privacy point of view, this is less desirable than vertex privacy, but
is still a strong guarantee which is appropriate in many settings. We note
that edge privacy is well studied with respect to graph problems (see, e.g.
\cite{NissimRS07,GuptaLMRT10, GuptaRU11}), and we do not know of any
algorithms with non-trivial guarantees on graphs that promise vertex privacy,
nor any algorithms in the more general case of matrices that promise privacy
with respect to entire rows.

\subsection{Our results}

We start with our first algorithm that improves over randomized response on
matrices of small \emph{-coherence}. We say that an 
matrix  has \emph{coherence } if no row has Euclidean norm more than
 i.e., more than  times the the typical row
norm.This parameter varies between  and  since no row can have
Euclidean norm more than  Intuitively the condition says that no
single row contributes too significantly to the Frobenius norm of the matrix.

\begin{theorem}[Informal version of \theoremref{C-approx}]
\theoremlabel{informal1}
There is an -differentially private algorithm which given a
matrix  of coherence  such that 
computes a rank  matrix  such that with probability 

Moreover, the algorithm runs in time 
\end{theorem}
Hidden in the -notation is a factor of
 that depends on the privacy parameters.
Usually,  so that

To understand the error bound note that the first term is proportional to the
best possible approximation error  of any rank 
approximation. In particular, this term is optimal up to constant factors.
The second term expresses a more interesting phenomenon. Recall that we assume
 so that  would usually dominate  except that the
the  term is multiplied by a factor which can be very
small if the matrix has low coherence and is not too dense. For example, when
  and  the error is roughly
 which can be as small as  depending
on the magnitude of  However, already in a much wider range of parameters we
observe an error of  In fact, in \sectionref{netflix} we
illustrate why the Netflix data satisfies the assumptions made
here and why they are likely to hold in other recommender systems.

When  the previous theorem cannot improve on randomized
response by more than a factor of  Our next theorem uses a
stronger but standard notion of coherence known as -coherence. We
defer a formal definition of -coherence to \sectionref{incoherent}, but
we remark that this parameter varies between  and  Using this notion we
are able to obtain improvements roughly of order 

\begin{theorem}[Informal version of \theoremref{mu-approx}]
\theoremlabel{informal2}
There is an -differentially private algorithm which given a
matrix  with  and of -coherence  and
rank  computes a rank  matrix  such that with
probability 

Moreover, the algorithm runs in time 
\end{theorem}

The hidden factor here is the same as before.   Note that
when  the theorem can lead to an error bound  depending on the magnitude of  Note that this is
roughly the square root of what randomized response would give. But again under
much milder assumptions on the coherence, the error remains
 Notably, Candes and Tao~\cite{CandesT10} work
with a stronger incoherence assumption than what is needed here.
Nevertheless they show that even their stronger assumption is
satisfied in a number of reasonable random matrix models. A slight
disadvantage of the error bound in \theoremref{informal2} is that the actual
rank~ of the matrix enters the picture. \theoremref{informal2} hence cannot
improve over \theoremref{informal1} when the matrix has very large rank. We do
not know if the dependence on  in the above bound is inherent or rather an
artifact of our analysis.

Finally, we remark that while our result depends on the -coherence of the input
matrix, our algorithm does not require knowledge or estimation of the
-coherence of the input matrix. The only parameters provided to the algorithm
are the target rank and the privacy parameters.

\paragraph{Reconstruction attacks and tightness of our results.}
As it turns out, existing work on ``blatant non-privacy'' and
reconstruction attacks~\cite{DinurN03} demonstrates that our results are
essentially tight under the given assumptions. To draw this connection, let us
first observe why \emph{input perturbation} cannot be improved without any
assumption on the matrix. To be more precise, by input perturbation we refer
to the method which simply perturbs each entry of the matrix with independent
Gaussian noise of magnitude
, which is sufficient to
achieve -differential privacy with respect to unit 
perturbations of the entire matrix. To obtain a rank  approximation to the
original matrix, one can then simply compute the exactly optimal rank 
approximation to the perturbed matrix using the singular value decomposition,
which as one can show introduces error 
compared to the optimal rank  approximation to the original matrix in the
Frobenius norm. First, let us observe that it is not possible in general to
have an algorithm which guarantees error in the Frobenius norm of
 for \emph{every} matrix , without violating \emph{blatant
non-privacy}\footnote{An algorithm  is blatantly non-private if for every
database  it is possible to reconstruct a  fraction of
the entries of  exactly, given only the output of the mechanism .},
as defined by \cite{DinurN03}.  This is because there is a simple
reduction which starts with an -differentially private
algorithm for computing rank  approximations to matrices  and gives an -differentially
private algorithm which can be used to reconstruct almost every entry in any
database  for . It is known that
-private mechanisms do not admit such reconstruction
attacks, 
and so the result is a lower
bound. The reduction follows from the fact that we can always encode a
bit-valued database  for  as  rows of an
 matrix for any , simply by zeroing out all additional
 rows. Note that the resulting matrix only has rank , and so the
optimal rank  approximation to this matrix has \emph{zero}
error. If we could recover a  matrix  such that , this would mean that for a typical nonzero row  of the
matrix with , we would have , and . Then, by simply rounding the entries, we could
reconstruct the original database  in almost all of its entries, giving
blatant non-privacy as defined by \cite{DinurN03}.

What is happening in the above example? Intuitively, the problem is that in
the rank  matrix we construct from , the  nonzero rows of the matrix
are encoded accurately by only ~right singular vectors. On the other hand, low
coherence implies that any  right singular vectors poorly represent a set
of only  rows. Hence, there is hope to circumvent the above impediment using a
low coherence assumption on the matrix. Indeed, this is precisely what
\theoremref{informal1} and \theoremref{informal2} demonstrate.
Nevertheless, reconstruction attacks still lead to lower bounds even under low
coherence assumptions. Indeed, using the above ideas, the next proposition shows
that \theoremref{informal1} is essentially tight up to a factor
of~ Since in many applications  this discrepancy
between our upper bound and the lower bound is often insignificant.
\begin{proposition}
Any algorithm which given an  matrix  of coherence  outputs a
rank  matrix  such that with high probability

cannot satisfy -differential privacy for
sufficiently small constants 
\end{proposition}
\begin{proof}[Informal proof]
For the sake of contradiction, suppose there exists such an algorithm 
that satisfies -differential privacy. Then consider a
randomized algorithm  which takes a data set
 containing a sensitive bit for  individuals and
encodes it as the  matrix  which contains  in its first 
rows and is  everywhere else.  then computes  and
outputs the projection of  onto the first  rows (thought of as a
vector of length ).

We claim that  is
-differentially privacy. This is because the map from 
to  is sensitivity preserving and the post-processing computed on 
preserves -differential privacy of 

On the other hand, we claim that  is blatantly non-private. To see this
note that the matrix  has coherence  and 
so that one can check that  with high probability.
This implies that  with high probability. We therefore also have . But in
this case we can compute a data set  from the output of  such that
 by rounding. This is the definition of a reconstruction attack showing
that  is blatantly non-private. Since -differential
privacy is known to prevent blatant non-privacy\footnote{See, e.g., the proof
of Theorem 4.1 in~\cite{De11}.} for sufficiently small
 this presents the contradiction we sought.
\end{proof}
A similar proof shows that error  (where 
is the -coherence of the matrix) cannot be achieved with
-differential privacy. This shows that also
\theoremref{informal2} is tight up to the exact dependence on~ and~ We
leave it as an intriguing open problem to determine the exact interplay
between coherence and the other parameters.





\subsection{Techniques and proof overview}
Our algorithm is based on a random-projection algorithm of Halko, Martinsson
and Tropp~\cite{HalkoMT11},
which involves two steps: \emph{range finding} and \emph{projection}. The
range finding algorithm first computes  Gaussian measurements of  which
we denote by  Here,  is  and  is  These measurements can be thought of as a random projection of the matrix
into a lower dimensional representation, i.e.,  is  The crux of
the analysis in~\cite{HalkoMT11} is in arguing that  already captures most
of the range of~ Hence, all that remains to be done is to compute the
orthonormal projection operator~ into the span of  and to compute the
projection  Note that  is now a -dimensional approximation of
 and since  closely approximated the range of  it must be a good
approximation, say, in the Frobenius norm.

The motivation of~\cite{HalkoMT11} was to obtain a fast low rank approximation
algorithm. Indeed, \cite{HalkoMT11} give a detailed theoretical analysis and
empirical evaluation of the algorithm's performance.

\paragraph{Step 1: Privacy preserving range finder and projection.}
We will leverage the algorithm of~\cite{HalkoMT11} to obtain improved accuracy
bounds in the privacy setting. As a first step, we need to be able to carry out
the range finding and projection step in a privacy preserving manner.
Our analysis proceeds by observing that the projection of
 to  approximately preserves all of the  row-norms of , and
so we can apply a Gaussian perturbation to , rather than to . (An
 standard Gaussian matrix has Frobenius norm ,
which is now independent of ). The formal presentation of this part of the
argument appears in \sectionref{range-projection}. This step provides an
approximation to the range of  which might already be useful for some applications,
but has not yet achieved our goal of computing a low rank approximation to 
itself. For this, we need the projection step discussed next.

\paragraph{Step 2: Controlling the projection matrix using low coherence.}
We then show that under our low-coherence
assumption on , the entries of the projection matrix into the range of ,
, must be small in magnitude.
Finally, when  has small entries, the final
projection step, of computing  has low sensitivity, and although we must
now again add a Gaussian perturbation of dimension , the magnitude
of the perturbation in each entry can be smaller than would have been
necessary under naive input perturbation.

In order to obtain bounds on the -norm of the projection operator
we make crucial use of the low-coherence assumption. Here we describe the
proof strategy that leads to \theoremref{informal2}. \theoremref{informal1} is
somewhat easier to show and follows along similar lines.
The first observation is that the Gaussian measurements
taken by the range finding algorithm are mostly linear combinations of the top
left singular vectors of the matrix. But when the matrix  has low coherence,
then its top left singular vectors must have very small correlation with the
standard basis. This means that the top singular vectors must have small
coordinates. As a result each of the Gaussian measurements we take must have
small -norm relative to the magnitude of the measurement. Some
complications arise as we must add noise to the matrix  for privacy reasons and
then orthonormalize it using the Gram-Schmidt orthonormalization algorithm.
A key observation is that the noise matrix is generated independently of .
As a result, it must be the case that all columns of the noise matrix have
very small inner product with the columns of  A careful technical argument
uses this observation in order to show that the effect of noise can be
controlled throughout the Gram-Schmidt orthonormalization. The result is a
projection matrix in which the magnitude of each entry is small whenever the
coherence of~ was small to begin with.

The exact proof strategy depends on the notion of coherence that we work with.
Both notions we consider in this paper are presented and analyzed in
\sectionref{incoherent}. We then also show that small -coherence is indeed a
stronger assumption than small -coherence.

\subsection{Related Work}
\subsubsection{Differential Privacy}
We use as our privacy solution concept the by now standard notion of
\emph{differential privacy}, developed in a series of papers \cite{BlumDMN05,
ChawlaDMSW05,DworkMNS06}, and first defined by Dwork, McSherry, Nissim, and Smith
\cite{DworkMNS06}. The problem of privately computing low-rank approximations to
matrix valued data was one of the first problems studied in the differential
privacy literature, first considered by Blum, Dwork, McSherry, and Nissim
\cite{BlumDMN05}, who give an input perturbation based algorithm for computing
the singular value decomposition by directly computing the eigenvector
decomposition of a perturbed covariance matrix. Computing low rank
approximations is an extremely useful primitive for differentially private
algorithms, and indeed, McSherry and Mironov \cite{McSherryM09} used the algorithm
given in \cite{BlumDMN05} in order to implement and evaluate differentially
private versions of recommendation algorithms from the Netflix prize
competition.

Finding differentially private low-rank approximation algorithms with superior
theoretical performance guarantees to input perturbation methods has remained
an open problem. Beating input perturbation methods for arbitrary symmetric
matrices was recently explicitly proposed as an open problem in
\cite{GuptaRU11}, who showed that such algorithms would lead to the first
efficient algorithm for privately releasing synthetic data useful for
\emph{graph cuts} which improves over simple randomized response. Our work
does not resolve this open question because our results only improve over
input perturbation methods for matrices with unbalanced dimensions which
satisfy a low-coherence assumption, but is the first algorithm to improve over
\cite{BlumDMN05} under any condition.

\paragraph{Comparison to recent results of Kapralov, McSherry and Talwar.}

In a recent independent and simultaneous work, Kapralov, McSherry, and
Talwar~\cite{KapralovMT11} give a new polynomial-time algorithm for computing
privacy-preserving rank 1 approximations to symmetric, positive-semidefinite matrices. Their algorithm achieves
-differential privacy under unit spectral norm perturbations to
the matrix. Their algorithm outputs a vector  such that for all ,  (where  denotes the spectral norm) and they show that this is nearly tight for -differential privacy guarantees. Our results are therefore
strictly incomparable. In this work, the goal is to achieve error
 (i.e.  for rank-1 approximations) assuming low coherence, (a stronger error bound) under
-differential privacy (a weaker privacy guarantee) and without making any assumptions about symmetry or positive-semidefiniteness.

\subsubsection{Fast Computation of Low Rank Matrix Approximations}
There is also an extensive literature on randomized algorithms for computing
approximately optimal low rank matrix approximations, motivated by improving
the running time of the exact singular value decompositions. This literature
originated with the work of Papadimitriou et al \cite{PapadimitriouTRV98} and Frieze,
Kannan, and Vempala \cite{FriezeKV04}, who gave algorithms based on random
projections and column sampling (in both cases with the goal of decreasing the
dimension of the matrix). Achlioptas and McSherry \cite{AchlioptasM01} give fast
algorithms for computing low rank approximations based on randomly perturbing
the original matrix (which can be done to induce sparsity). Although
\cite{AchlioptasM01} pre-dated the privacy literature, some of the algorithms presented
in it can be viewed as privacy preserving, because perturbing the actual
matrix with appropriately scaled Gaussian noise is a privacy preserving
operation sometimes referred to as \emph{randomized response}. When
appropriately scaled (for privacy) Gaussian noise is added to an 
matrix, it results in an algorithm for approximating the best rank 
approximation up to an additive error of  in the Frobenius
norm.


Our algorithms are most closely related to  the very recent work of Halko,
Martinsson, and Tropp \cite{HalkoMT11}, who give fast algorithms for computing
low rank approximations based on two steps: range finding, and projection. As
already discussed, in the first step, these algorithms project the matrix 
into an  matrix  which approximately captures the \emph{range}
of . Then  is projected into the range of , which yields a rank 
matrix which gives a good approximation to  if a good rank-
approximation exists. We will further discuss the algorithm of
\cite{HalkoMT11} and our modifications in the course of the paper.



\subsubsection{Low Coherence Conditions}

Low coherence conditions have been recently studied in a number of papers for
a number of matrix problems, and is a commonly satisfied condition on
matrices. Recently, Candes and Recht \cite{CandesR09} and Candes and Tao
\cite{CandesT10} considered the problem of \emph{matrix completion}. Matrix
completion is the problem of recovering all entries of a matrix from which
only a subset of the entries which have been randomly sampled. This problem is
inspired by the Netflix prize recommendation problem, in which a matrix is
given, with individuals on the rows, movies on the columns, and in which the
matrix entries correspond to individual movie ratings. The matrix provides
only a small number of movie ratings per individual, and the challenge is to
predict the missing entries in the matrix.  Clearly accurate matrix completion
is impossible for arbitrary matrices, but \cite{CandesR09,CandesT10} show the
remarkable result that it is possible under low coherence assumptions. Candes
and Tao \cite{CandesT10} also show that almost every matrix satisfies a low
coherence condition, in the sense that randomly generated matrices will be low
coherence with extremely high probability.

Talwalkar and Rostamizadeh recently used low-coherence assumptions for the
problem of (non-private) low-rank matrix approximation \cite{TalwalkarR10}. A common
heuristic for speeding the computation of low-rank matrix approximations is to
compute on only a small randomly chosen subset of the columns, rather than on
the entire matrix. \cite{TalwalkarR10} showed that under low-coherence assumptions
similar to those of \cite{CandesR09,CandesT10}, the spectrum of a matrix is in fact
well approximated by a small number of randomly sampled columns, and give
formal guarantees on the approximation quality of the sampling based
Nystr\"{o}m method of low-rank matrix approximation.

\subsection*{Acknowledgments}

We thank Boaz Barak, Anupam Gupta, and Jon Ullman for many helpful
discussions. We thank Sham Kakade for pointing us to the paper of
\cite{HalkoMT11}. We thank Michael Kapralov, Frank McSherry and Kunal Talwar
for communicating their result to us, and for useful
conversations about low rank matrix approximations. We wish to thank
Microsoft Research New England where part of this research was done.

\section{Preliminaries}
We view our dataset as a real valued \emph{matrix}  We sometimes denote the  -th of a matrix by 
Let

denote the set of matrices
that take  at all values, except possibly in a single row, which has
Euclidean norm at most .
\begin{definition} We say that two matrices  are \emph{neighboring} if .
\end{definition}
We use the by now standard privacy solution concept of differential privacy:
\begin{definition}
An algorithm  (where  is some
arbitrary abstract range) is \emph{-differentially private}
if for all pairs of neighboring databases , and for
all subsets of the range :

\end{definition}

We make use of the following useful facts about differential privacy.
\begin{fact}
If  is -differentially private, and  is an arbitrary randomized algorithm mapping  to , then  is -differentially private.
\end{fact}

The following useful theorem of Dwork, Rothblum, and Vadhan tells us how differential privacy guarantees compose.
\begin{theorem}[Composition \cite{DworkRV10}]
\label{thm:composition}
Let 
If  are each -differentially private
algorithms, then the algorithm  releasing the concatenation of the results of
each algorithm is -differentially private. It is also -differentially private for:

\end{theorem}


We denote the -dimensional Gaussian distribution of mean  and
variance  by  We use  to denote
the distribution over -dimensional vectors with i.i.d. coordinates sampled from
 We write  to indicate that a
variable  is distributed according to a distribution~ We note
the following useful fact about the Gaussian distribution.
\begin{fact}\factlabel{gaussian-sum}
If  then

\end{fact}
The following theorem is well known folklore. We include a proof in the appendix for completeness.

\begin{theorem}[Gaussian Mechanism]
\label{thm:Gaussian}
Let  be any two vectors such that .
Let  be an independent random draw from ,
where . Then for any  
\end{theorem}

\paragraph{Vector and matrix norms.} We denote by  the
-norm of a vector and sometimes use  as a shorthand for the
Euclidean norm. Given a real  matrix  we will work with the
\emph{spectral norm}  and the Frobenius norm~ defined as

For any  matrix  of rank  we have

For a matrix  we denote by~
the orthonormal projection operator onto the range of 
\begin{fact}\label{fac:projection}

\end{fact}

\begin{fact}[Submultiplicativity]\factlabel{sub}
For any  matrix  and  matrix  we have

\end{fact}

\begin{theorem}[Weyl]\label{thm:weyl}
For any  matrices  we have
 where 
denotes the -th singular value of a matrix  where  denotes
the -th singular value of a matrix~
\end{theorem}

\section{Low-rank approximation via Gaussian measurements}

We will begin by presenting an algorithm of Halko, Martinsson and
Tropp~\cite{HalkoMT11} as described in \figureref{HMT}.  The algorithm
produces a rank  approximation that already for  closely matches
the best rank~ approximaton of the matrix in Frobenius norm. The guarantees
of the algorithm are detailed in \theoremref{HMT}.

\begin{figure}[h]
\begin{boxedminipage}{\textwidth}
{\bf Input:} Matrix  target rank  oversampling
parameter 
\begin{enumerate}
\item {\sc Range finder:}
Let  be an  standard Gaussian matrix where 
Compute the  measurement matrix  Compute the
orthonormal projection operator 
\item {\sc Projection:} Compute the projection 
\end{enumerate}
{\bf Output:} Matrix  of rank 
\end{boxedminipage}
\caption{Base algorithm for computing a low-rank approximation}
\figurelabel{HMT}
\end{figure}

\begin{theorem}[\cite{HalkoMT11}]
\label{thm:HMT}
Suppose that  is a real  matrix with singular values
 Choose a target rank  and an
oversampling parameter  where  Draw an
 standard Gaussian matrix  construct the sample
matrix  and let 
Then the expected approximation error in Frobenius norm satisfies

In particular, for  we have

\end{theorem}
When applying the the theorem we will use Markov's inequality to argue that
the error bounds hold with sufficiently high probability up to a constant
factor loss. As shown in~\cite{HalkoMT11}, much better bounds on the failure
probability are possible. We will omit the precise bounds here for the sake of
simplicity.

\section{Privacy-preserving sub-routines: Range finder and projection}
\sectionlabel{range-projection}

In order to give a privacy preserving variant of the above algorithm, we will
first need to carefully bound the sensitivity of the range finder and of the
projection step, and bound the effect of the necessary perturbations. We do
this for each step in this section.


\subsection{Privacy-preserving range finder}
\sectionlabel{range-finder}

In this section we present a privacy-preserving algorithm which finds a set of
vectors  whose span contains most of the spectrum of a given
matrix~

\begin{figure}[ht]
\begin{boxedminipage}{\textwidth}
{\bf Input:} Matrix  target rank  oversampling
parameter  such that  privacy parameters 
\begin{enumerate}
\item Let  be an  standard Gaussian matrix where 
\item Compute the  measurement matrix 
\item Let  where

\item Let 
\item Orthonormalize the columns of  and let the result be 
\end{enumerate}
{\bf Output:} Orthonormal  matrix 
\end{boxedminipage}
\caption{Privacy-preserving range finder}
\figurelabel{range-finder}
\end{figure}

\begin{lemma}
\label{lem:range-privacy}
The algorithm in~\figureref{range-finder} satisfies -differential
privacy.
\end{lemma}

\begin{proof}
We argue that outputting   preserves -differential privacy. That outputting  preserves -differential privacy follows from the fact that differential privacy holds under arbitrary post-processing.

Consider any two neighboring matrices 
differing in their 'th row, and let  and .
Define  to be . Note that by the
definition of neighboring, we must have . Observe that for
each row , we have , and define  to be . First, we will
give a high-probability bound on . Observe that for each   is distributed like a standard Gaussian:

where we used \factref{gaussian-sum}.
Therefore, we have for any 
by standard Gaussian tail bounds,

Taking a union bound
over all  coordinates we have:

In particular, we have except with probability 

Note that we have set  such that
conditioned on \equationref{hate} (which holds with probability at
least ) we have the following by \theoremref{Gaussian}:
For every set 
.
Hence, without any conditioning we can say:

which completes the proof of privacy.
\end{proof}

\begin{theorem}
Let  be an  matrix with singular values

Then, given  and valid parameters 
the algorithm in \figureref{range-finder} returns a matrix  such that
 satisfies -differential privacy,
and moreover we have the error bound

\end{theorem}

\begin{proof}
Privacy follows from \lemmaref{range-privacy}. Let us therefore argue the
second part of the theorem. Consider the  matrix

where  is the  identity matrix. Let  denote
a random  Gaussian matrix.
Note that

That is,  is distributed the same way as  Here, we're
using the fact that 

On the other hand, by \theoremref{HMT}, we know that  is a good range for
 in the sense that

Here,  denotes the -th largest singular value of 

\begin{claim}\claimlabel{one}

\end{claim}
\begin{proof}
The claim is immediate, because we can obtain  from  by truncating the
last  columns. Hence, the approximation error can only decrease.
\end{proof}

\begin{claim}\claimlabel{two}
For all  we have 
\end{claim}

\begin{proof}
Consider the matrix  where  is
the all zeros matrix. Note that

Also,  since we just appended an all zeros matrix.
On the other hand,

Hence, by Weyl's perturbation bound (\theoremref{weyl})

\end{proof}

Combining the previous claims with (\ref{eq:range-error}), we have

Since  and  are identically distributed, the same claim is true
when replacing  by  Furthermore, 
and so the claim follows.
\end{proof}

\begin{corollary}\corollarylabel{range-finder}
Let  be as in the previous theorem. Assume that 
and run the algorithm with  Then, with probability ,

\end{corollary}
\begin{proof}
By Markov's inequality and the previous theorem, we have with probability


But note that  This is because either  and thus
 or else  in which case
 The claim follows by using that
 for non-negative 
\end{proof}

\subsection{Privacy-preserving projections}
\sectionlabel{projection}

In the previous section we showed a privacy-preserving algorithm that finds a small number of orthonormal vectors  such that  is well-approximated by
 To obtain a privacy preserving low-rank approximation algorithm we
still need to show how to carry out the projection step in a
privacy-preserving fashion. We analyze the error of the projection
step in terms of the magnitude of the maximum entry of each column of  This serves to bound the sensitivity of the matrix multiplication operation. The smaller the entries of  the smaller the over all error that we incur.

\begin{figure}[h]
\begin{boxedminipage}{\textwidth}
{\bf Input:} Matrix  matrix
 whose columns have norm at most~
privacy parameters 
\begin{enumerate}
\item Let  and for each 
let  denote the maximum magnitude entry in 
\item Let  be a random  matrix where 
for  and 
\item Compute the matrix 
\end{enumerate}
{\bf Output:} Matrix  of rank 
\end{boxedminipage}
\caption{Privacy-preserving projection}
\figurelabel{projection}
\end{figure}

\begin{lemma}
The output  of the algorithm satisfies -differential
privacy.
\end{lemma}

\begin{proof}
We will argue that releasing  preserves
-differential privacy. That releasing  preserves
differential privacy follows from the fact that differential privacy does not
degrade under arbitrary post-processing.  Fix any two neighboring matrices  differing in their 'th row. Let  and
let 
Recall by the definition of neighboring, , and for all
other , . For any , consider the 'th
row of :

Hence, by \theoremref{Gaussian}, releasing  where  preserves -differential privacy.  Finally, we apply
\theoremref{composition} to see that releasing each of the  rows of 
preserves -differential privacy for: 
as desired.
\end{proof}

\begin{theorem}\theoremlabel{projection}
The algorithm above returns a matrix  such that  satisfies
-differential privacy and moreover with probability


In particular if  we have with the same probability,

\end{theorem}

\begin{proof}

But  so that, by \factref{sub},

\Mnote{the previous step doesn't seem tight. I'd like to get rid of the extra
 so that our algorithm is never worse than randomized response.}
On the other hand, by Jensen's inequality and linearity of expectation,

The claim now follows from Markov's inequality.
\end{proof}

Note that the quantities  are always bounded by~ since all 's are unit
vectors. In the next section, we will show that under certain incoherence assumptions, we will have (or will be able to enforce) the condition that the  values are bounded significantly below .



\section{Incoherent matrices}
\sectionlabel{incoherent}

Intuitively speaking, a matrix is \emph{incoherent} if its left singular vectors
have low correlation with the standard basis vectors. There are multiple ways to
formalize this intuition. Here, we will work with two natural notions of
coherence. In both cases we will be able to show that we can find---in a
privacy-preserving way---projection operators that have small entries. As
demonstrated in the previous section, this directly leads to improvements over
randomized response.

\subsection{-coherent matrices}

In this section we work with matrices~ in which row norms do not deviate by
too much from the typical row norm.
Another way to look at this condition is that coordinate projections
provide little spectral information about the matrix  From this angle the
condition we need can be interpreted as \emph{low coherence} in the sense that
the singular vectors of  that correspond to large singular values
must be far from the standard basis.

\begin{definition}[-coherence]
\definitionlabel{C-coherence}
We say that a matrix  is \emph{-coherent} if

Note that we have 
\end{definition}

The next lemma shows that sparse vectors have poor correlation with the matrix
in the above sense. We say a vector  is \emph{-sparse} if it has at
most  nonzero coordinates.

\begin{lemma}
\lemmalabel{sparse-vector}
Let  be a -coherent matrix.
Let  be an -sparse unit vector in 
Then,

\end{lemma}

\begin{proof}
Since  is -sparse we can write it as 
where  are  distinct standard basis vectors
and 
Hence,

In the last step we used the Cauchy-Schwarz inequality and the fact that  is
-coherent.
\end{proof}

\begin{lemma}
Let 
Let  be a -coherent matrix.
Let  be a unit vector and suppose  is the vector obtained
from  by zeroing all coordinates greater than  Then,

where  is a vector of norm

\end{lemma}

\begin{proof}
Note that  is an -sparse vector with 
Here we used that  is a unit vector and hence there can be at most
 coordinates larger than  The lemma now follows directly
from \lemmaref{sparse-vector}.
\end{proof}

The next lemma is a straightforward extension of the previous one for the case
where we multiply  by a matrix  rather than a single vector.
\begin{lemma}\lemmalabel{truncation-error}
Let 
Let  be a -coherent matrix.
Let  be a matrix whose columns have unit length.
Suppose  is the matrix obtained
from  by zeroing all entries greater than  Then,

where  is a matrix of Frobenius norm

\end{lemma}
\begin{proof}
By the previous lemma,
we have

where every row of  has Euclidean norm
  Hence,

But then

Put  and note that  The
lemma follows.
\end{proof}

The previous lemma quantifies what happens if we replace  by
 Working with  for small  instead of 
will decrease the sensitivity of the computation of   On the
other hand, by the previous lemma we have an expression for the error
resulting from the truncation step.

\subsection{Strong coherence}

Here we introduce and work with the notion of \emph{-coherence} which
is a standard notion of coherence. As we will see in \sectionref{relation}, it
is a stronger notion than -coherence. Consequently, the results we will be
able to obtain using -coherence are stronger than our previous results
on -coherence in certain aspects.

\begin{definition}[-coherence]
\definitionlabel{mu0}
Let  be an  matrix with orthonormal columns and 
Recall, that  The \emph{-coherence} of  is defined as

Here,  denotes the -th -dimensional standard basis vector and
 denotes the -th row of 

The \emph{-coherence} of an  matrix  of rank  given in
its singular value decomposition  where  is
defined as 
\end{definition}

\begin{fact}

\end{fact}
\begin{proof}
Since  is orthonormal, there must always exists a row of square norm 
On the other hand, no row of  has squared norm larger than 
\end{proof}

The above notion is used extensively throughout the literature in the context
of matrix completion and low rank approximation, e.g., in Candes and Recht
\cite{CandesR09}, Keshavan et al. \cite{KeshavanMO10}, Talwalkar and
Rostamizadeh \cite{TalwalkarR10}, Mohri and Talwalkar \cite{MohriT10}.
Motivated by the Netflix problem, Candes and Tao \cite{CandesT10} study matrix
completion for matrices satisfying a stronger incoherence assumption than
small -coherence.

Our goal from here on is to show that if we run our range finding algorithm from
\sectionref{range-finder} on a low-coherence matrix it will produce a
projection matrix with small entries. This result (presented in
\lemmaref{mu0-projection}) requires several technical lemmas.

The first technical step is a lemma showing that vectors that lie in the range
of an incoherent matrix must have small -norm.

\begin{lemma}\lemmalabel{mu0-infty}
Let  be an orthonormal  matrix. Suppose  and
 Then,

\end{lemma}

\begin{proof}
Let  denote the columns of  By our set of assumptions, there
exist  such that

Therefore, denoting the
-th entry of  by  and the -th entry of  by  we have

In particular 
On the other hand,
using \definitionref{mu0},

The lemma follows.
\end{proof}

We will need the following geometric lemma: If we start with a small
orthonormal set of vectors of low coherence and we append few random unit
vectors, then the span of the resulting set of vectors has a low coherence
basis.

\begin{lemma}\lemmalabel{mu0-perturb}
Let  be orthonormal vectors. Pick unit vectors
 uniformly at random. Assume that

where  is a sufficiently large constant.
Then, there exists a
set of orthonormal vectors  such that
 and
furthermore, with probability 

\end{lemma}

\begin{proof}
We will construct the basis iteratively using the Gram-Schmidt orthonormalization
algorithm starting with the partial orthonormal basis  The
algorithm works as follows: At
iteration  we have obtained a partial orthonormal basis 
where  We then pick a random unit vector  and
let  Put

Let  and 
Our goal is to bound  as this will directly lead to a bound
on  in terms of  Summing up this bound over  will
lead to a bound on  which is what the lemma is asking for.

Let us start with a two simple claims that follow from measure concentration
on the sphere. The first one bounds the -norm of a random unit
vector.
\begin{claim}\claimlabel{one}
 with probability

\end{claim}
\begin{proof}
It is not hard to show that for every  the coordinate projection
 is a Lipschitz function on the sphere. Moreover, the median of
 is  by spherical symmetry. By measure
concentration (\theoremref{levy}),  Setting 
and taking a union bound over all  coordinates completes the proof.
\end{proof}

The second claim we need bounds the Euclidean norm of 

\begin{claim}\claimlabel{two}
 with probability 
\end{claim}
\begin{proof}
Proceeding as in proof of the previous claim, we note that for each 
 is a Lipschitz function on the sphere with
median~ Applying \theoremref{levy} with  it follows that with probability 

Taking a union bound over all
 we have with probability 

where we used that 
\end{proof}

On the one hand, note
that  is in the span of  by definition. Hence,
\lemmaref{mu0-infty} directly implies
that

Hence, combining \equationref{three} with \claimref{two},
we have with probability 

On the other hand, we can bound  as follows:

By \claimref{two} we have that with probability 

In the first inequality above we used that 
We then applied \claimref{two} in the second inequality.
By \equationref{mlarge},  is sufficiently large so that


Combining \equationref{v'infty} with \equationref{inverse} and applying
\claimref{one}, we conclude that with
with probability at least 

But when the above bound on  holds, then we must
have

Taking a union bound over all  steps, we find that with probability
 \equationref{plusone} is true at all steps of the Gram-Schmidt
algorithm. Assuming that this event occurs, we have:

This finishes our proof of the lemma since  by
definition.
\end{proof}

The choice of failure probability in the previous lemma was rather arbitrary
and stronger bounds can be achieved.
We finally arrive at the main lemma in this section.

\begin{lemma}\lemmalabel{mu0-projection}
Let  be an  matrix of rank 
Let  with 
denote a random standard Gaussian matrix and define 
Assume that  for sufficiently large constant 
Further, let  and
 denote a random Gaussian matrix with i.i.d.
entries sampled from  Put 
and let
 be an orthonormal basis for the range of 
Then, with probability 

\end{lemma}

\begin{proof}
Let  denote the left singular factor of  Let  denote the
columns of  We have,

since  is an orthonormal basis for the range of  by
construction. On the other hand,

Since  this implies that

where  are the columns of  normalized such that 
By assumption  is large enough so that we can apply \lemmaref{mu0-perturb}.
Thus we obtain orthonormal vectors  satisfying

and the matrix  whose columns are  has coherence

with probability  In particular,
 for all  Therefore,
by \lemmaref{mu0-infty}, we have that

Since  and  we conclude that

The lemma follows.
\end{proof}

\begin{remark}
We remark that the previous lemma is essentially tight. Indeed, under the
given assumption on  there could be a left singular vector of
-norm  The above lemma implies that
we are never more than a -factor away from this bound.
\end{remark}

\subsection{Relation between -coherence and -coherence}
\sectionlabel{relation}

Here we show that the assumption of small -coherence is strictly
stronger than that of small -coherence assuming the rank of the matrix is
not too large.

\begin{lemma}
Let  be an  matrix of rank  Then,  is -coherent where

\end{lemma}
\begin{proof}
Let the SVD of  be  and denote the right singular vectors by
 Extend them arbitrarily to an orthonormal basis of 
denoted  We then have for every 

where we used that the -norm of a vector is bounded by the
-norm. On the other hand,

where we used Cauchy-Schwarz in the inequality. It follows that

Taking square roots on both sides and rearranging, we find

Note that the left hand side is exactly the smallest  for which  is
-coherent. This proves the lemma.
\end{proof}

Recall that \lemmaref{sparse-vector} showed that the singular vectors
corresponding to large singular values of a -coherent matrix  cannot be
too sparse. In particular, the top singular vectors must have
small -coherence as a result. However, we cannot rule out that there
are singular vectors corresponding to small singular values that do have
large coordinates.



\section{Privacy-preserving low rank approximations}

In this section we compose the range finder, projection and truncation step to
get a private low rank approximation algorithm suitable for matrices of low
coherence.

\begin{figure}[ht]
\begin{boxedminipage}{\textwidth}
{\bf Input:} Matrix  target rank  oversampling
parameter  pruning parameter 
privacy parameters 
\begin{enumerate}
\item {\sc Range finder:}
\itemlabel{range-finder}
Run the range finder (\figureref{range-finder})
on  with sampling parameter  and privacy
parameters  Let the output be denoted by~
\item {\sc Pruning:}
\itemlabel{pruning}
Let  be the matrix obtained from  by zeroing out all entries larger
than 
\item {\sc Projection:}
\itemlabel{projection}
Run the projection algorithm (\figureref{projection}) on input  and
privacy parameters 
Let  denote the output of the projection algorithm.
\end{enumerate}
{\bf Output:} Matrix  of rank 
\end{boxedminipage}
\caption{The private find and project algorithm (\PFP) for computing
privacy-preserving low-rank approximations}
\figurelabel{find-project}
\end{figure}

\begin{lemma}
The \PFP algorithm satisfies -differential privacy.
\end{lemma}

\begin{proof}
This follows directly from composition and the privacy guarantee achieved by
the subroutines.
\end{proof}

The next theorem details the performance of \PFP on -coherent matrices. In
particular, it shows that in a natural range of parameters it improves
significantly over randomized response (input perturbation).

\begin{theorem}[Approximation for -coherent matrices]
\theoremlabel{C-approx}
There is an -differentially private algorithm that
given a -coherent matrix  and parameters
 produces a rank  matrix  such that with
probability 

In particular, the second error term is  whenever

\end{theorem}
We generally think of  as small compared to both  and 
\equationref{range-m} states that the algorithm outperforms randomized
response whenever  is not too large compared to  and not too small
compared to the rank  the Frobenius norm of  divided by 
and the coherence parameter  These two conditions are
naturally satisfied for a wide range of parameters. For example, when
 (so that randomized response no longer provides
non-trivial error) and  (i.e., the matrix is very incoherent), then
the requirement on  is just that

The proof of
\theoremref{C-approx} is a straightforward combination of our previous
error bounds for range finding, pruning and projection.
\begin{proof}[Proof of \theoremref{C-approx}]
We run \PFP with the given set of parameters 
and a suitable choice of the pruning parameter 
Before fixing  we claim that the error of the algorithm satisfies,
with probability 

Here, the first term follows from \theoremref{HMT} and an application of
Markov's inequality to argue that the bound holds except with sufficiently
small constant probability. The other terms
follow from \theoremref{projection} (error bound of the
projection algorithm), \corollaryref{range-finder} (error bound of the range
finder), and, \lemmaref{truncation-error} (error bound for the pruning step
with parameter ). We can now optimize  so as to achieve the
geometric mean between the two terms that it appears in (as  and
). Running \PFP with this choice of  directly results
in the error bound stated in
\equationref{C-precise}. \equationref{range-m} is now easily verified by
equating the -term in \equationref{C-precise} with
 and rearranging.

Since all sub-routines fail with probability at most  we can take a
union bound to conclude that the algorithm fails to satisfy the error bound
with probability at most 
\end{proof}

We will next analyze the performance of \PFP on -incoherent matrices. In this
case no truncation is necessary, since we argued that the projection matrix
with high probability already has very small entries. The error bound here is
stronger in certain aspects as we will discuss in a moment.

\begin{theorem}[Approximation for -coherent matrices]
\theoremlabel{mu-approx}
There is an -differentially private algorithm that
given a rank  matrix  and parameters
 such that  and 
produces a rank  matrix  such that with probability 

In particular, the error is  whenever

\end{theorem}

Just as in the previous theorem we get a range for  in which the algorithm
improves over randomized response. Here, we need the coherence of  to be
small compared to~ We also observe a dependence on the rank of the matrix.
This means the algorithm presents no improvement if the matrix is close to
being full rank.  Recall that  can be as small as  In
particular, in the natural case where  all are small compared
to  e.g.,  the requirement in \equationref{range-m-2} reduces to


Note that \theoremref{mu-approx} is quantitatively stronger than
\theoremref{C-approx} in the following regime: When  are all
small (e.g., ),  and 
then \theoremref{mu-approx}
improves over randomized response by a factor of roughly  whereas
\theoremref{C-approx} achieves an -factor improvement.

\begin{proof}[Proof of \theoremref{mu-approx}]
We run \PFP with the given set of parameters 
and  Note that this choice of  implies that we never modify
the matrix returned by the range finder.
We claim that the error of the algorithm is with probability 

which is what we stated in the theorem. The first error term follows as before
from \theoremref{HMT} and Markov's inequality so that it holds with
probability 
The term of 
follows from \corollaryref{range-finder}. To understand
the remaining terms that by \lemmaref{mu0-projection} we have that the matrix
 returned by the range finder satisfies with
probability 

In applying \lemmaref{mu0-projection} we needed that  for
sufficiently large constant which is satisfied by our assumption.
Hence, \theoremref{projection}
ensures that the error resulting from the projection
operation is at most  Expanding
 in the latter bound gives the stated error term.
\equationref{range-m} is now easily verified by
equating the -term in \equationref{mu-precise} with
 and rearranging.

Again, we can take a union bound over the failure probabilities of the
sub-routines to bound the probability that our algorithm fails to satisfy the
stated bound by 
\end{proof}

\bibliographystyle{alpha}
\bibliography{moritz}


\appendix
\section{Privacy of the Gaussian Mechanism}

\begin{theorem}[Gaussian Mechanism]
Let  be any two vectors such that . Let  be an independent random draw from , where . Then for any :

\end{theorem}
\begin{proof}
For a set , write  to denote the set  and  to denote . Write  to denote the projection of the set onto the 'th coordinate of its elements.

First we consider the one dimensional case, where  and . Without loss of generality, we may take  and . Let  be the set  First, we argue that . This follows directly from the tail bound:

Observing that:

and plugging in our choice of   completes the claim. Next we show that conditioned on the event that , we have:  . Conditioned on this event we have:

where here  denotes the probability density function of  at .
This quantity is bounded by  whenever:

i.e. whenever .
Therefore:

which completes the proof in the 1-dimensional case.



For the multi-dimensional case, we will take advantage of the rotational invariance of the Gaussian distribution to rotate any Euclidean length -perturbation into a length  standard basis vector, reducing it to the -dimensional case.

Consider any two vectors  such that .  Let  be the orthonormal (rotation) matrix such that  where  is the 1st standard basis vector , and . We will use the fact that for any orthonormal matrix , and for any , : i.e. spherically symmetric Gaussian distributions are invariant under rotation. We have:

We want to bound:

 Now note that we have chosen  such that  for all  (Because  for all ). Therefore, we have:
 

Note that by rotational invariance, we have:  for any vector , and so we are now again in the -dimensional case, in which the theorem is already proven.

\end{proof}


\section{Measure concentration on the sphere}
In \sectionref{incoherent} we used the following classical result regarding
concentration of Lipschitz functions on the sphere. A proof can be found for
example in Matousek's text book~\cite{Matousek02}.
\begin{theorem}[L\'{e}vy's lemma]\theoremlabel{levy}
Let  be a Lipschitz function in the sense that

and define the median of  as

Then,

where probability probability and expectation are computed with respect to the
uniform measure on the sphere.
\end{theorem}

\section{The Netflix Data}
\sectionlabel{netflix}
In this section we illustrate why the data set released by Netflix satisfies the
assumptions underlying \theoremref{informal1}. That is, the matrix is
unbalanced, sparse and -coherent (\definitionref{C-coherence}) for very
small  Indeed, according to information released by Netflix, the data set
has the following properties:
\begin{enumerate}
\item There are  movie ratings,
 movies and  users. In particular, the data set is very sparse
in that only a  fraction of the matrix is nonzero. Also note
that 
\item The most rated movie in the data set is \emph{Miss Congeniality} with 
ratings (followed by \emph{Independence Day} with ). Hence, the maximum
number of entries in one row is only a  fraction of the
total number of \emph{nonzero} entries. 
Moreover, all entries of the matrix are in  and thus very small
numbers.
\end{enumerate}
We conclude that, indeed, the Netflix matrix is \emph{sparse} and the maximum
norm of any row takes up only a tiny fraction of the total norm of the matrix.
We further believe that these properties are likely to hold in other
recommender systems.  Indeed, the average number of ratings per user should be
small (thus resulting in a sparse matrix), and no item should be rated almost
as often as all other items taken together (thus resulting in low coherence).
\end{document}
