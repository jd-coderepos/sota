\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{indentfirst}
\usepackage{bbm}
\usepackage[table]{xcolor}


\usepackage{lipsum}
\newcommand\blfootnote[1]{\begingroup
\renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}




\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{3562} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning}


\author{
Gongjie Zhang \qquad Zhipeng Luo \qquad Kaiwen Cui \qquad Shijian Lu \smallskip\\
{Nanyang Technological University, Singapore}\\
{\tt\footnotesize gongjiezhang@ntu.edu.sg \quad zhipeng001@e.ntu.edu.sg \quad kaiwen001@e.ntu.edu.sg \quad shijian.lu@ntu.edu.sg}
}


\maketitle
\ificcvfinal\thispagestyle{empty}\fi


\begin{abstract}
Few-shot object detection aims at detecting novel objects with only a few annotated examples. Prior works have proved meta-learning a promising solution, and most of them essentially address detection by meta-learning over regions for their classification and location fine-tuning. However, these methods substantially rely on initially well-located region proposals, which are usually hard to obtain under the few-shot settings. This paper presents a novel meta-detector framework, namely Meta-DETR, which eliminates region-wise prediction and instead meta-learns object localization and classification at image level in a unified and complementary manner. Specifically, it first encodes both support and query images into category-specific features and then feeds them into a category-agnostic decoder to directly generate predictions for specific categories. To facilitate meta-learning with deep networks, we design a simple but effective Semantic Alignment Mechanism (SAM), which aligns high-level and low-level feature semantics to improve the generalization of meta-learned representations. Experiments over multiple few-shot object detection benchmarks show that Meta-DETR outperforms state-of-the-art methods by large margins.
\end{abstract}



\section{Introduction} \label{sec:introduction}
\vspace*{-1.8mm}

\begin{figure}[t!] 
\begin{center}
   \includegraphics[width=0.865\linewidth]{Figures/Fig1.pdf}
\end{center}
\vspace*{-0.8mm}
   \caption{\textbf{Upper:} Most existing meta-detectors essentially perform \textit{region-wise} predictions, which heavily rely on the quality of initial region proposals that cannot be guaranteed under the few-shot settings. \textbf{Lower:} The proposed Meta-DETR meta-learns object localization and classification at \textit{image level} in a unified and complementary manner (without region-wise prediction), leading to superior few-shot object detection performance.}
\label{fig:fig1}
\vspace*{-0.0mm}
\end{figure}


\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Figures/Fig2.pdf}
\end{center}
\vspace*{-0.6mm}
   \caption{\textbf{Comparison of existing few-shot detectors with our Meta-DETR.} Dashed blue boxes indicate meta-learning components.  indicates feature aggregation. Unlike prior works that rely on region-wise predictions, Meta-DETR unifies the meta-learning of object localization and classification at image level with a single meta-learning module.}
\label{fig:fig2_methods_comparison}
\vspace*{-0.0mm}
\end{figure*}


\blfootnote{\, denotes equal contribution.}
\blfootnote{\, denotes corresponding author.}


Computer vision has witnessed significant progress in recent years. However, there still exists a huge gap between current computer vision techniques and human visual systems in learning new concepts from very few examples: most existing methods require large amounts of annotated samples, while humans can effortlessly recognize a new concept even with very little instruction~\cite{Smith2002ObjectNL,background2}. Such a human-like capability of generalizing from limited examples is highly desirable for machine vision systems, especially when sufficient training samples are not available or their annotations are hard to obtain~\cite{DLMedical,lvis,FSPillRecognition,huang2020contextual,DefectGAN,huang2021cross}.

In this work, we explore the challenging \textit{few-shot object detection} task, which requires both recognition and localization of novel objects within an image. Prior works\,\cite{FewshotReweighting,incrementalfsdet,metadet,metarcnn,fsod,FSDetView} have proved meta-learning a promising solution. As illustrated in the upper part of Fig.\,\ref{fig:fig1}, they essentially address object detection by performing meta-learning over regions, including region proposals\,\cite{metarcnn,FSDetView}, anchors\,\cite{FewshotReweighting}, and window centers\,\cite{incrementalfsdet}, for their classification and location fine-tuning. However, as identified in\;\cite{fsod} and\;\cite{CoRPN}, these methods rely heavily on the quality of initial region proposals, which cannot be guaranteed in the few-shot setups with scarce training samples, thus producing inaccurate or missed detection.
Though FSOD\,\cite{fsod} proposes to meta-learn the generation of region proposals, this issue remains as the framework is still inherently region-based.

\newpage
Based on the analysis above, a key limitation rooted in existing meta-detectors is the region-wise prediction approach.
Besides, under the challenging settings of few-shot object detection where supervision from annotated examples is minimal, the complementary effect between classification and localization (as demonstrated in \cite{CAM,RethinkLocAndCls,ren-cvpr020}) should be maximally exploited. 
Therefore, an ideal meta-detector should discard such region-based prediction and effectively leverage the synergistic relationship between classification and localization by meta-learning both sub-tasks in a fully end-to-end manner. However, such a framework is still absent to the best of our knowledge.

Recently, the emergence of fully end-to-end detection frameworks\;\cite{DETR,DeformableDETR} clears the way to such a framework. This paper presents \textit{Meta-DETR}, a novel region-free framework for few-shot object detection that meta-learns image-level localization and classification in a unified and complementary manner. Concretely, it incorporates meta-learning into the DETR frameworks\,\cite{DETR,DeformableDETR} by first encoding support and query images into category-specific features and then feeding them into a category-agnostic decoder to directly generate detection results for the target categories. To facilitate meta-learning with deep networks, we design a simple but effective Semantic Alignment Mechanism (SAM) that aligns high-level and lower-level feature semantics and prevents reliance on category-specific representations with low generalization capability.

The contributions of this work are threefold.
\textit{First}, we propose Meta-DETR, a novel few-shot object detection framework that unifies image-level meta-learning of object localization and classification into a single module without requiring region-wise prediction. Such a design can effectively leverage the synergistic relationship between the two sub-tasks and avoid constraints caused by region-wise prediction.
\textit{Second}, we design a simple but effective Semantic Alignment Mechanism (SAM) that enhances the generalization capacity of meta-learning by aligning high-level and low-level semantics to avoid reliance on category-specific representations.
\textit{Third}, extensive experiments show that our method achieves state-of-the-art performance on multiple benchmarks for few-shot object detection.




\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Figures/Fig3.pdf}
\end{center}
\vspace*{-1.0mm}
   \caption{\textbf{The architecture of our proposed Meta-DETR.} It consists of a Query Encoding Branch (QEB), a Support Encoding Branch (SEB), and a Decoding Branch (DB). QEB receives a query image and generates its query features through a feature extractor and a transformer encoder. SEB, which shares all learnable parameters with QEB, extracts support category codes from the support images. Given the query features with a support category code, DB first aggregates them into category-specific features and then applies a category-agnostic transformer decoder to predict the detection results over the corresponding support category.}
\label{fig:fig3_architecture}
\vspace*{+0.5mm}
\end{figure*}



\section{Related Work}
\smallskip
\noindent {\bf Object Detection.}
Generic object detection\,\cite{Liu2019DeepLF} is a joint task on object localization and classification. Modern object detectors can be broadly classified into two categories including two-stage detectors and single-stage detectors. The dominant two-stage detectors are Faster R-CNN\,\cite{FasterRCNN} and its variants\,\cite{IoUNet,CascadeRCNN,masktextspotter,SNIP,SNIPER,ROITransformer,CADNet,klloss,CircularSmoothLabel,rahman2018zeroshot}, which first adopt a Region Proposal Network (RPN) to generate region proposals as coarse localization and then perform per-region classification and location fine-tuning. Differently, single-stage detectors\;\cite{SSD,YOLO9000,RON,focalloss,ScaleTransferrableOD,LearningRF,RefineDet,RFBNet} employ densely placed anchors as region proposals and directly make predictions on them. 
These aforementioned methods still rely on many heuristics like anchor generation.
Recently, DETR\,\cite{DETR} and its variants\,\cite{DeformableDETR,up-detr,TSP,DETR_Pedestrian,ACT} have received vast attention thanks to their merits of no heuristic design, fully end-to-end pipeline, and comparable or even better performance.
However, these detectors still heavily rely on human supervision in the form of large amounts of annotated training samples, thus will suffer from huge performance drop in the context of few-shot learning.

\medskip
\noindent {\bf Few-Shot Learning.}
Few-shot learning aims at bridging the gap between existing models and human intelligence in learning novel concepts from very few samples. One promising solution is meta-learning\,\cite{metalearning_survey1,metalearning_survey2}, which aims to extract meta-level knowledge that can generalize across various tasks via `learning to learn'. Extensive researches\,\cite{MAML,MatchNet,ProtoNet,RelationNetwork,DynamicFewshotWOForgetting,SNAIL,CloserFewshotClassification,FSLGlobalClassRep,MetaOptNet,MTL,tafe,RepMet,Ravichandran2019FewShotLW,PARN,tewam,Dvornik_2019_ICCV,ye2020fewshot,NewMetaBaseline,FewShotAdaMarginLoss,FSOpenSet,IFSMLIDA} have proved the effectiveness of the meta-learning paradigm for the few-shot classification task. However, other more complex few-shot learning tasks\;\cite{MetaView,Gui2018FewShotHM,Nguyen_2019_ICCV,Wertheimer_2019_CVPR,Wang_2020_CVPR,FSAdaptiveFRCNN} are still relatively under-explored.

\smallskip
\noindent {\bf Few-Shot Object Detection.}
Prior works on few-shot object detection can be formulated in two paradigms: transfer-learning-based and meta-learning-based. Methods using transfer-learning include LSTD\,\cite{LSTD}, PNPDet\,\cite{PNPDet}, TFA\,\cite{fsdet}, and MPSR\,\cite{MPSR}, where novel concepts are learned via fine-tuning. Differently, methods using meta-learning extract meta-level knowledge that can efficiently adapt to novel categories by constructing and learning on various auxiliary tasks, in which target categories are dynamically conditioned on support images. Of them, Meta-YOLO\,\cite{FewshotReweighting} and ONCE\,\cite{incrementalfsdet} are based on single-stage detectors, and Meta R-CNN\,\cite{metarcnn} and its variants\,\cite{metarcnn_acmmm,metadet,FSOD-KT,mmfsod,FSDetView,AFDNet} are built upon Faster R-CNN\,\cite{FasterRCNN}.
As shown in Fig.\,\ref{fig:fig2_methods_comparison}, existing meta-detectors essentially perform region-wise meta-learning, thus requiring initially well-located regions. However, such well-located regions for novel objects are usually hard to obtain with non-learnable shape priors and fine-tuned RPN when training samples are scarce.
FSOD\,\cite{fsod} attempts to mitigate this issue by meta-learning an Attention-RPN, but the issue remains as this framework and Attention-RPN are still innately region-based.

Our Meta-DETR follows the track of meta-learning. Unlike previous works, it discards region-wise prediction and instead unifies the meta-learning of localization and classification at image level with a category-agnostic decoder, thus leveraging global contexts and the synergistic relationship of the two sub-tasks to achieve superior performance.



\section{Method}

\subsection{Problem Definition}
\vspace*{-0.5mm}
Given two sets of categories  and , where , a few-shot object detector aims at detecting objects of  by learning from a base dataset  with abundant annotated instances of  and a novel dataset  with very few annotated instances of .
In the task of -shot object detection, there are exactly  annotated object instances for each novel category in .


\subsection{Meta-DETR}

\subsubsection{Revisiting DETR Frameworks}
\vspace*{-0.5mm}
Modern detectors like Faster R-CNN\,\cite{FasterRCNN} address object detection by performing the surrogate task of classification and location fine-tuning on a number of regions. Such detectors require many heuristics and are not fully end-to-end. Recently, DETR\,\cite{DETR} eliminates the need for such heuristic designs and achieves the first fully end-to-end detection framework. It is built upon the Transformer encoder-decoder architecture\;\cite{transformer}, combined with a set-based Hungarian loss that forces unique predictions for each object via bipartite matching. Besides, Deformable DETR\,\cite{DeformableDETR} further extends DETR by mitigating its high complexity and slow convergence issue.

Meta-DETR extends the DETR frameworks\,\cite{DETR,DeformableDETR} by incorporating meta-learning into such fully end-to-end detection frameworks. Its innovative designs can help evade various issues such as the constraint of region-wise prediction under the context of few-shot object detection.



\subsubsection{Network Description}

Aiming at performing unified meta-learning for localization and classification at image level, our Meta-DETR is conceptually simple. As shown in Fig.\;\ref{fig:fig3_architecture}, it consists of a \textit{Query Encoding Branch (QEB)}, a \textit{Support Encoding Branch (SEB)}, and a \textit{Decoding Branch (DB)}. Given a \textit{Query Image} and several \textit{Support Images} with instance annotations, QEB and SEB first encode them into \textit{Query Features} and \textit{Category Codes}, respectively. DB then takes the query features and category codes as input and predicts \textit{Detection Results} over the corresponding support categories. As target categories to detect are dynamically conditioned on the provided support images, Meta-DETR is able to extract category-agnostic meta-level knowledge that can easily adapt to novel categories.

\medskip
\noindent
\textbf{Query Encoding Branch (QEB).}
The design of QEB follows Deformable DETR\,\cite{DeformableDETR} except for a residual connection that will be introduced later. As illustrated in Fig.\,\ref{fig:fig3_architecture}, it mainly consists of a feature extractor and a transformer encoder. Given a query image, the feature extractor (a CNN backbone such as ResNet\,\cite{resnet}) generates its feature maps and then adopts  to make the feature maps' channel dimension compatible with the downstream modules. Since the transformer encoder expects a sequence as input, we first inject positional encoding into the feature maps, collapse the feature maps' spatial dimensions into one dimension, and then feed them into the transformer encoder to produce the query features.

\medskip
\noindent
\textbf{Support Encoding Branch (SEB).}
SEB shares all learnable parameters with QEB following the philosophy of Siamese Networks\,\cite{SiameseOneshotImageRecognition}. Unlike QEB that preserves image-level information within the query features, SEB aims at extracting category codes that mostly relate to certain object instances within the support images. We, therefore, introduce a Category Code Extractor (CCE) to filter out irrelevant information within the support images. CCE has no learnable parameters. It derives support category codes via three sequential operations: 1) restoring the features' spatial dimension from the transformer encoder, 2) locating support object instances with RoIAlign\,\cite{MaskRCNN}, and 3) global average pooling followed by a sigmoid function. When there are multiple support images for a category, it averages all category codes as the final category code.

\medskip
\noindent
\textbf{Decoding Branch (DB).}
DB receives the outputs of QEB and SEB and produces object detection results, and its target categories are dynamically determined by the category codes. Concretely, it aggregates the query features and category codes into a set of category-specific features. The design of aggregator follows previous work\,\cite{FSDetView}. A transformer decoder with a feed-forward network (FFN, omitted in Fig.\,\ref{fig:fig3_architecture} for simplicity) then takes the category-specific features and a small fixed number of object queries as input and produces detection results over the corresponding categories. Similar to the decoder in DETR frameworks, DB eliminates region-wise prediction and addresses object detection at image level. However, DB is category-agnostic with no intention to detect objects of specific categories. Such unique design enables joint meta-learning of object localization and classification at image level, which can avoid potential issues with region-wise prediction and achieve superior few-shot detection performance.



\begin{figure}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Figures/Fig4.pdf}
\end{center}
\vspace*{-1.5mm}
   \caption{\textbf{Semantic Alignment Mechanism.} A simple residual connection acts as self-regularization to prevent the transformer encoder from relying on undesirable category-specific features by aligning the feature semantics of its input and output.}
\label{fig:fig4_semantic_alignment}
\vspace*{-1.0mm}
\end{figure}

\medskip
\noindent
\textbf{Semantic Alignment Mechanism (SAM).}
Meta-learning has been proved promising for few-shot learning. Its major motivation is to obtain meta-level knowledge that can generalize to various categories instead of focusing on specific categories. However, most works\;\cite{MetaOptNet,CloserFewshotClassification,Li2019FindingTF,ye2020fewshot,DeepEMD} perform meta-learning on relatively shallow networks, such as ResNet-12 and ResNet-18. There is also evidence\;\cite{DeepMetaLearning,DynamicFewshotWOForgetting,CloserFewshotClassification,Act2Param} that meta-learning a deeper network from scratch performs comparable or even worse than without meta-learning. One possible reason is that, even with meta-learning, deeper networks still tend to learn and rely on category-specific semantics with poor generalization undesirably. To mitigate this issue, we propose to incorporate a simple but effective Semantic Alignment Mechanism (SAM), which is essentially a residual connection as illustrated in Fig.\,\ref{fig:fig4_semantic_alignment}, into the proposed Meta-DETR.

The motivation behind SAM is simple and straightforward. As observed in the feature visualization literature\;\cite{Zeiler2014VisualizingAU,understanding-neural-networks}, features from bottom layers relate to low-level cues such as colors and shapes that have better generalization; while features from top layers relate to more complex and specific concepts such as categories. To avoid reliance on such high-level category-specific features, SAM incorporates a shortcut connection to bypass the transformer encoder, which works as self-regularization to guide the feature semantics from the transformer encoder to align with its input feature semantics with better generalization.

It is worth mentioning that the motivation behind SAM is very different from the residual connections that have been widely used in various neural network architectures. The residual connections in ResNet\,\cite{resnet} only bypass several convolutional layers and aim at improving the gradient flow and solving the gradient vanishing issue when training very deep neural networks. Meta-DETR does not suffer from gradient vanishing as its transformer\,\cite{transformer} building blocks already incorporate such residual connections. In contrast, the residual connection used in SAM bypasses the entire transformer encoder, aiming to align its outputs' feature semantics with its inputs', thus acting as self-regularization to prevent reliance on category-specific semantics.


\subsubsection{Training Objective} \label{sec:TrainingObjective}

\noindent
\textbf{Detection Target Generation.}
Assume the fixed number of object queries is , which means Meta-DETR infers  predictions over each category in a single pass through the decoder. Let us denote by  the query image, and  the ground truth set of objects within the query image, which is a set of size . When  indicates an object, , where  denotes the target category label and  denotes the bounding box of the object. When  indicates no object, .

\smallskip
\smallskip
Meta-DETR dynamically conditions its detection targets on support images. Given a support image  along with its object annotation , the detection targets are defined as:

where  acts to filter irrelevant object annotations, which can be formulated as:

Note that  can completely consist of . In this case we call  a negative target category.

\medskip
\smallskip
\noindent
\textbf{Loss Function.}
Assume the  predictions for target category made by Meta-DETR are . We adopt a pair-wise matching loss  to search for a bipartite matching between  and  with the lowest cost:

where  denotes a permutation of  elements, and  denotes the optimal assignment between predictions and targets. Since the matching should consider both classification and localization, the matching loss is defined as:



With the optimal assignment  obtained with Eq.\,\ref{eq:optimalassignment} and Eq.\,\ref{eq:Lmatch}, we optimize the network using the following loss function:

where we adopt sigmoid focal loss\,\cite{focalloss} for  and a linear combination of  loss and GIoU loss\,\cite{giouloss} for . Similar to \cite{DETR} and \cite{DeformableDETR},  is applied to each layer of the transformer decoder.

\smallskip
Following \cite{metarcnn}, we also adopt a conventional cross-entropy loss, denoted as , to classify the category codes produced by SEB. This encourages category codes that belong to different categories to be distinguished from each other.


\begin{table*}[t]
\begin{center}
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}[t]{cc|ccccc|ccccc|ccccc}
\toprule[1pt]
&&\multicolumn{5}{c|}{Category Split 1} & \multicolumn{5}{c|}{Category Split 2} & \multicolumn{5}{c}{Category Split 3} \\\midrule[0.5pt]
Method & multi-scale & 1 & 2 & 3 & 5 & 10 & 1 & 2 & 3 & 5 & 10 & 1 & 2 & 3 & 5 & 10\\\midrule[0.68pt]

FRCN-ft-full\;\cite{FasterRCNN,fsdet} &  & 15.2 & 20.3 & 29.0 & 40.1 & 45.5 & 13.4 & 20.6 & 28.6 & 32.4 & 38.8 & 19.6 & 20.8 & 28.7 & 42.2 & 42.1 \\

D-DETR-ft-full\;\cite{DeformableDETR}  &  & 5.6 & 13.3 & 21.7 & 34.2 & 45.0 & 10.9 & 13.0 & 18.4 & 27.3 & 39.4 & 7.3 & 16.6 & 20.8 & 32.2 & 41.8 \\

LSTD\;\cite{LSTD} &  & 8.2 & 1.0 & 12.4 & 29.1 & 38.5 & 11.4 & 3.8 & 5.0 & 15.7 & 31.0 & 12.6 & 8.5 & 15.0 & 27.3 & 36.3 \\

RepMet\;\cite{RepMet} &  & 26.1 & 32.9 & 34.4 & 38.6 & 41.3 & 17.2 & 22.1 & 23.4 & 28.3 & 35.8 & \textbf{27.5} & 31.1 & 31.5 & 34.4 & 37.2 \\



TFA w/ fc\;\cite{fsdet}  &  & 22.9 & 34.5 & 40.4 & 46.7 & 52.0 & 16.9 & 26.4 & 30.5 & 34.6 & 39.7 & 15.7 & 27.2 & 34.7 & 40.8 & 44.6 \\

TFA w/ cos\;\cite{fsdet}  &  & 25.3 & 36.4 & 42.1 & 47.9 & 52.8 & 18.3 & 27.5 & 30.9 & 34.1 & 39.5 & 17.9 & 27.2 & 34.3 & 40.8 & 45.6 \\

MPSR\;\cite{MPSR}  &  & \textbf{34.7} & \textbf{42.6} & 46.1 & 49.4 & 56.7 & \textbf{22.6} & 30.5 & 31.0 & 36.7 & 43.3 & \textbf{27.5} & 32.5 & 38.2 & 44.6 & 50.0 \\

Meta-YOLO\;\cite{FewshotReweighting} & & 14.8 & 15.5 & 26.7 & 33.9 & 47.2 & 15.7 & 15.3 & 22.7 & 30.1 & 40.5 & 21.3 & 25.6 & 28.4 & 42.8 & 45.9 \\

Meta\,Det\;\cite{metadet} && 18.9 & 20.6 & 30.2 & 36.8 & 49.6 & 21.8 & 23.1 & 27.8 & 31.7 & 43.0 & 20.6 & 23.9 & 29.4 & 43.9 & 44.1\\

Meta R-CNN\;\cite{metarcnn} & & 19.9 & 25.5 & 35.0 & 45.7 & 51.5 & 10.4 & 19.4 & 29.6 & 34.8 & 45.4 & 14.3 & 18.2 & 27.5 & 41.2 & 48.1 \\

FsDetView\;\cite{FSDetView} & & 24.2 & 35.3 & 42.2 & 49.1 & 57.4 & 21.6 & 24.6 & 31.9 & 37.0 & 45.7 & 21.2 & 30.0 & 37.2 & 43.8 & 49.6 \\

\rowcolor{black!6} Meta-DETR (Ours) & & 17.5 & 36.0 & 45.1 & 51.2 & 57.1 & 18.5 & 27.5 & 34.7 & 41.1 & 49.8 & 15.4 & 32.6 & 39.4 & 49.0 & 54.3 \\

\rowcolor{black!6} Meta-DETR (Ours) &  & 20.4 & 35.0 & \textbf{46.3} & \textbf{52.2} & \textbf{57.8} & 20.2 & \textbf{30.9} & \textbf{38.2} & \textbf{44.0} & \textbf{52.6} & 22.8 & \textbf{34.9} & \textbf{43.0} & \textbf{50.2} & \textbf{54.9} \\

\bottomrule[1pt]
\end{tabular}}
\end{center}
\vspace*{-1.25mm}
\caption{Few-shot detection performance (mAP@0.5) on Pascal VOC \textit{test\,07} set for novel categories. Results are averaged over multiple repeated runs with different randomly sampled support datasets.  indicates results are re-evaluated using official codes for multiple runs since original results are evaluated with a single run.}
\label{tab:Performance_VOC_novel}
\vspace*{-1.25mm}
\end{table*}



\subsubsection{Training and Inference Scheme}

The training procedure consists of two stages.
The first stage is \textit{base training stage}. During this stage, the model is trained on the base dataset  with abundant training samples for each base category.
The second stage is \textit{few-shot fine-tuning stage}. In this stage, we train the model on both base and novel categories with limited training samples. Only  object instances are available for each novel category in -shot object detection. Following \cite{fsdet,metarcnn,FSDetView}, we also include several object instances for each base category to prevent performance drop for base categories.
In both stages, we optimize the network in an end-to-end manner using the loss functions described in Section\;\ref{sec:TrainingObjective}. 

In both training stages, multiple auxiliary tasks, also known as episodes, are formed to train the proposed Meta-DETR. Specifically, each episode contains one query image and 10 support images representing different target categories to detect. Target categories include both positive categories and negative categories. Support images are randomly sampled from the training dataset.

Before inference, we first use SEB to obtain the category codes for all categories once and for all. For each category, if there are multiple support images, we average all corresponding category codes as the final category code. After acquiring the category codes, SEB can be detached. During inference, Meta-DETR does not need to repeatedly compute category codes as in the training stage, which promises the efficient inference of Meta-DETR.


\section{Experiments}

\subsection{Datasets}

We follow the data setups of prior works for few-shot object detection~\cite{FewshotReweighting,metadet,metarcnn,fsdet,FSDetView,MPSR}. Concretely, two widely used few-shot object detection benchmarks are evaluated in our experiments.

\smallskip
\textbf{Pascal VOC\,\cite{PascalVOC}}
consists of images with object annotations of 20 categories. We use \textit{trainval\,07+12} for training and perform evaluations on \textit{test\,07}. Following\,\cite{FewshotReweighting,metarcnn,fsdet,FSDetView}, we use 3 novel\,/\,base category splits, \ie, (“bird”, “bus”, “cow”, “motorbike”, “sofa”\;/\;others); (“aeroplane”, “bottle”,“cow”,“horse”,“sofa”\;/\;others) and (“boat”, “cat”, “motorbike”,“sheep”, “sofa”\;/\;others). The number of shots is set to 1, 2, 3, 5 and 10.  Mean average precision (mAP) at IoU threshold 0.5 is used as the evaluation metric. Results are averaged over 10 randomly sampled support datasets. 

\smallskip
\textbf{MS COCO\,\cite{MSCOCO}}
is a more challenging object detection dataset, which contains 80 categories including those 20 categories in Pascal VOC. We adopt the 20 shared categories as novel categories, and adopt the remaining 60 categories in MS COCO dataset as base categories. The number of shots is 10 and 30. We use \textit{train\,2017} for training, and perform evaluations on \textit{val\,2017}. Standard evaluation metrics for MS COCO are adopted. Results are averaged over 5 randomly sampled support datasets.




\begin{table}
\begin{center}
\resizebox{0.43\textwidth}{!}{
\begin{tabular}{ c | c c | c | c  }
\toprule

Shot & Method & multi-scale & Base & Novel 

\\\midrule

\multirow{7}{*}{3} & LSTD\;\cite{LSTD} &  & 66.3 & 12.4  \\

& TFA w/ cos\;\cite{fsdet}  &  & \textbf{77.3} & 42.1 \\

& MPSR\;\cite{MPSR}  &  & 65.9 & 46.1 \\

& Meta-YOLO\;\cite{FewshotReweighting} & & 64.8 & 26.7 \\

& Meta R-CNN\;\cite{metarcnn} & & 64.8 & 35.0 \\

 & \cellcolor{black!6} Meta-DETR (Ours) & \cellcolor{black!6} & \cellcolor{black!6} 65.2 & \cellcolor{black!6} 45.1 \\

 & \cellcolor{black!6} Meta-DETR (Ours) & \cellcolor{black!6} & \cellcolor{black!6}66.5 &  \cellcolor{black!6} \textbf{46.3}

\\\midrule

\multirow{7}{*}{10} & LSTD\;\cite{LSTD} &  & 66.3 & 38.5  \\

& TFA w/ cos\;\cite{fsdet}  &  & \textbf{77.5} & 52.8 \\

& MPSR\;\cite{MPSR}  &  & 69.8 & 56.7 \\

& Meta-YOLO\;\cite{FewshotReweighting} & & 63.6 & 47.2 \\

& Meta R-CNN\;\cite{metarcnn} & & 67.9 & 51.5 \\

& \cellcolor{black!6} Meta-DETR (Ours) & \cellcolor{black!6} & \cellcolor{black!6}67.1 & \cellcolor{black!6}57.1 \\

& \cellcolor{black!6} Meta-DETR (Ours) & \cellcolor{black!6} & \cellcolor{black!6}67.4 & \cellcolor{black!6}\textbf{57.8}

\\\bottomrule
\end{tabular}
}
\end{center}
\vspace*{-1.2mm}
\caption{Few-shot detection performance (mAP@0.5) for base and novel categories on category split 1 of Pascal VOC. Results are averaged over multiple runs.  indicates re-evaluated results.}
\label{tab:Performance_VOC1_basenovel}
\vspace*{-1.0mm}
\end{table} 




\begin{table*}[t]
\begin{center}
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}[t]{c|cc|cccccc|cccccc}
\toprule[1.1pt]

& & &\multicolumn{6}{c|}{Average Precision} & \multicolumn{6}{c}{Average Recall}\\

Shot & Method & multi-scale & AP & AP & AP & AP & AP & AP & AR & AR & AR & AR & AR & AR \\

\midrule[0.68pt]

\multirow{11}{*}{10} & LSTD\;\cite{LSTD} &  & 3.2 & 8.1 & 2.1 & 0.9 & 2.0 & 6.5 & 7.8 & 10.4 & 10.4 & 1.1 & 5.6 & 19.6 \\

& TFA w/ fc\;\cite{fsdet}  &  & 9.1 & 17.3 & 8.5 & - & - & - & - & - & - & - & - & - \\

& TFA w/ cos\;\cite{fsdet}  &  & 9.1 & 17.1 & 8.8 & - & - & - & - & - & - & - & - & - \\

& MPSR\;\cite{MPSR} &  & 9.8 & 17.9 & 9.7 & \textbf{3.3} & 9.2 & 16.1 & 15.7 & 21.2 & 21.2 & 4.6 & 19.6 & 34.3 \\

& Meta-YOLO\;\cite{FewshotReweighting} &  & 5.6 & 12.3 & 4.6 & 0.9 & 3.5 & 10.5 & 10.1 & 14.3 & 14.4 & 1.5 & 8.4 & 28.2 \\

& Meta\,Det\;\cite{metadet} & & 7.1 & 14.6 & 6.1 & 1.0 & 4.1 & 12.2 & 11.9 & 15.1 & 15.5 & 1.7 & 9.7 & 30.1 \\

& Meta R-CNN\;\cite{metarcnn} & & 8.7 & 19.1 & 6.6 & 2.3 & 7.7 & 14.0 & 12.6 & 17.8 & 17.9 & 7.8 & 15.6 & 27.2 \\

& FSOD\;\cite{fsod}  & & 12.0 & 22.4 & 11.8 & 2.9 & 12.2 & 20.7 & 18.8 & 26.4 & 26.4 & 3.6 & 23.6 & 45.6 \\

& FsDetView\;\cite{FSDetView} & & 12.5 & 27.3 & 9.8 & 2.5 & 13.8 & 19.9 & 20.0 & 25.5 & 25.7 & 7.5 & 27.6 & 38.9 \\

&  \cellcolor{black!6} Meta-DETR (Ours)& \cellcolor{black!6} & \cellcolor{black!6}16.7 & \cellcolor{black!6}\textbf{29.0} & \cellcolor{black!6}17.1 & \cellcolor{black!6}2.7 & \cellcolor{black!6}13.7 & \cellcolor{black!6}27.0 & \cellcolor{black!6}19.6 & \cellcolor{black!6}30.4 & \cellcolor{black!6}32.7 & \cellcolor{black!6}7.7 & \cellcolor{black!6}29.3 & \cellcolor{black!6}52.8 \\

&  \cellcolor{black!6} Meta-DETR (Ours)& \cellcolor{black!6} & \cellcolor{black!6}\textbf{17.8} & \cellcolor{black!6}28.8 & \cellcolor{black!6}\textbf{18.5} & \cellcolor{black!6}\textbf{3.3} & \cellcolor{black!6}\textbf{14.0} & \cellcolor{black!6}\textbf{29.3} & \cellcolor{black!6}\textbf{21.0} & \cellcolor{black!6}\textbf{32.2} & \cellcolor{black!6}\textbf{34.1} & \cellcolor{black!6}\textbf{7.9} & \cellcolor{black!6}\textbf{29.9} & \cellcolor{black!6}\textbf{56.0} \\

\midrule[0.68pt]

\multirow{10}{*}{30} & LSTD\;\cite{LSTD} &  & 6.7 & 15.8 & 5.1 & 0.4 & 2.9 & 12.3 & 10.9 & 14.3 & 14.3 & 0.9 & 7.1 & 27.0 \\

& TFA w/ fc\;\cite{fsdet}  &  & 12.0 & 22.2 & 11.8 & - & - & - & - & - & - & - & - & - \\

& TFA w/ cos\;\cite{fsdet}  &  & 12.1 & 22.0 & 12.0 & - & - & - & - & - & - & - & - & - \\

& MPSR\;\cite{MPSR} &  & 14.1 & 25.4 & 14.2 & 4.0 & 12.9 & 23.0 & 17.7 & 24.2 & 24.3 & 5.5 & 21.0 & 39.3 \\

& Meta-YOLO\;\cite{FewshotReweighting} & & 9.1 & 19.0 & 7.6 & 0.8 & 4.9 & 16.8 & 13.2 & 17.7 & 17.8 & 1.5 & 10.4 & 33.5 \\

& Meta\,Det\;\cite{metadet} & & 11.3 & 21.7 & 8.1 & 1.1 & 6.2 & 17.3 & 14.5 & 18.9 & 19.2 & 1.8 & 11.1 & 34.4 \\

& Meta R-CNN\;\cite{metarcnn} & & 12.4 & 25.3 & 10.8 & 2.8 & 11.6 & 19.0 & 15.0 & 21.4 & 21.7 & 8.6 & 20.0 & 32.1 \\

& FsDetView\;\cite{FSDetView} & & 14.7 & 30.6 & 12.2 & 3.2 & 15.2 & 23.8 & 22.0 & 28.2 & 28.4 & 8.3 & 30.3 & 42.1 \\

&  \cellcolor{black!6} Meta-DETR (Ours) & \cellcolor{black!6} & \cellcolor{black!6}21.3 & \cellcolor{black!6}\textbf{36.0} & \cellcolor{black!6}22.0 & \cellcolor{black!6}3.8 & \cellcolor{black!6}17.8 & \cellcolor{black!6}35.5 & \cellcolor{black!6}22.2 & \cellcolor{black!6}33.8 & \cellcolor{black!6}36.3 & \cellcolor{black!6}9.1 & \cellcolor{black!6}34.0 & \cellcolor{black!6}59.0 \\

& \cellcolor{black!6} Meta-DETR (Ours) & \cellcolor{black!6} & \cellcolor{black!6}\textbf{22.9} & \cellcolor{black!6}35.8 & \cellcolor{black!6}\textbf{23.8} & \cellcolor{black!6}\textbf{4.7} & \cellcolor{black!6}\textbf{20.9} & \cellcolor{black!6}\textbf{36.5} & \cellcolor{black!6}\textbf{23.3} & \cellcolor{black!6}\textbf{36.0} & \cellcolor{black!6}\textbf{38.4} & \cellcolor{black!6}\textbf{12.5} & \cellcolor{black!6}\textbf{36.0} & \cellcolor{black!6}\textbf{59.9} \\

\bottomrule[1.1pt]

\end{tabular}
}
\end{center}
\vspace*{-1.25mm}
\caption{Few-shot detection performance on MS COCO \textit{val\,2017} set for novel categories. Results are averaged over multiple repeated runs with different randomly sampled support datasets.  indicates results are re-evaluated using official codes for multiple runs since original results are evaluated with a single run.}
\label{tab:Performance_COCO_novel}
\vspace*{-1.5mm}
\end{table*}







\subsection{Implementation Details} \label{sec:imple_details}

We adopt commonly used ResNet-101\,\cite{resnet} as the feature extractor in both QEB and SEB. The network architectures and hyper-parameters of transformer encoder and decoder remain the same as Deformable DETR\,\cite{DeformableDETR}. The feed-forward network (FFN) after the transformer decoder is a -layer MLP for box prediction and a -layer MLP for object confidence prediction.
Thanks to the multi-scale attention module introduced in Deformable DETR\,\cite{DeformableDETR}, Meta-DETR supports multi-scale features as input by nature without any modification. For a comprehensive comparison, we present results of Meta-DETR with both single-scale and multi-scale features in benchmarking. For ablation study, we only adopt the single-scale setting for Meta-DETR.

We train our model using the AdamW\,\cite{Adam,AdamW} optimizer with an initial learning rate of  and a weight decay of . We adopt a batch size of  and each query image is associated with 10 support images to form an episode. Conventional data augmentation as used in \cite{DETR,DeformableDETR} is adopted during training. In the base training stage, we train the model for  epochs for Pascal VOC and  epochs for MS COCO. Learning rate is decayed at the  and  epoch by a factor of , respectively. In the few-shot fine-tuning stage, the same settings (excluding the total number of epochs and the learning rate decay epochs) are applied to train the model until full convergence.


\subsection{Comparison with State-of-the-Art Methods}

\noindent
\textbf{Pascal VOC.} Table\;\ref{tab:Performance_VOC_novel} shows the few-shot detection performance for novel categories of Pascal VOC. It can be seen that Meta-DETR outperforms existing methods for most cases except when training samples are extremely scarce. We conjecture that the unsatisfactory performance for extremely low-shot settings is largely attributed to the large search space that comes with Meta-DETR's image-level prediction, which may lead to overfitting when training samples are extremely insufficient. However, when there are slightly more training samples for novel categories, \eg, 3-shot, 5-shot, and 10-shot, Meta-DETR performs significantly better across all category splits. Such experimental results demonstrate the superior robustness and generalization capability of our method.

Table\;\ref{tab:Performance_VOC1_basenovel} shows experimental results while taking base categories into consideration. While achieving good performance for novel categories with limited training samples, Meta-DETR can still detect objects of base categories with competitive performance. TFA\,\cite{fsdet} produces outstanding performance for base categories since it works more like conventional detectors with fine-tuning, thus having constrained capacity in generalizing on novel categories.

\medskip
\noindent
\textbf{MS COCO.}
Table\;\ref{tab:Performance_COCO_novel} shows experimental results on MS COCO. It can be seen that, although MS COCO is more challenging with higher complexity like occlusions and large scale variations, Meta-DETR still outperforms all existing methods for all setups by even larger margins.
Specifically, on the primary metric , Meta-DETR outperforms state-of-the-art methods by  for 10-shot and  for 30-shot. On the strict metric , Meta-DETR almost doubles the state-of-the-art method's performance from  to  for 10-shot and from  to  for 30-shot. This demonstrates Meta-DETR's precise localization, which is largely attributed to the unified image-level meta-learning that exploits the synergistic effects of localization and classification. Besides, Meta-DETR achieves the best performance for objects of all scales, especially for large objects, largely because Meta-DETR exploits global contexts via image-level predictions effectively.

Except for Average Precision (AP) that directly measures the performance of a detector, Average Recall (AR) is also an important metric. Higher AR indicates less missed detection. As shown in Table\;\ref{tab:Performance_COCO_novel}, Meta-DETR also outperforms the state-of-the-art by large margins regarding  ( for 10-shot and  for 30-shot). It is noteworthy that FSOD\,\cite{fsod} achieves the highest  among the region-based counterparts, thanks to its meta-learning-based AttentionRPN that generates more accurate region proposals. However, FSOD still suffers from inaccurate or missed detection as it is fundamentally region-based, relying on high-quality region proposals that are hard to obtain under the few-shot scenarios. In contrast, Meta-DETR fully eliminates region-wise prediction and makes predictions at image level, thus avoiding this constraint and achieving superior performance.



\begin{table}
\begin{center}
\resizebox{0.325\textwidth}{!}{
\begin{tabular}{ c c c | c c c }
\toprule
 \multicolumn{3}{c|}{Design Choice} & \multicolumn{3}{c}{Shot} \\
\midrule
CCE & SAM &  & 1 & 3 & 10 \\
\midrule
&  &  & 11.6 & 37.6 & 54.2 \\

&  &  & 15.1 & 39.6 & 53.2 \\
 
&  &  & 15.8 & 40.4 & 53.4 \\
 
&  & & 17.2 & 43.0 & 56.7 \\
  
\rowcolor{black!6} &  &  & \textbf{17.5} & \textbf{45.1} & \textbf{57.1}  \\

\bottomrule
\end{tabular}
}
\end{center}
\vspace*{-1.8mm}
\caption{Ablation studies over several design choices of Meta-DETR. Results for novel categories are averaged over multiple runs on the category split 1 of Pascal VOC.}
\label{tab:ablation1}
\end{table} 


\begin{figure}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Figures/Fig5.pdf}
\end{center}
\vspace*{-1.8mm}
   \caption{Visualization of correlations between query features and category codes. With semantic alignment mechanism (SAM) introduced, clear responses for both base category (cat) and novel category (bird) are observed, demonstrating SAM's effectiveness in enhancing generalization of meta-learned representations.}
\label{fig:fig5}
\vspace*{-1.0mm}
\end{figure}






\subsection{Ablation Study}  \label{sec:ablation}
\vspace*{-1.0mm}
We design extensive ablation experiments to study how our designed technical components contribute to the overall few-shot object detection performance.

\smallskip
\noindent
\textbf{Effect of Category Code Extractor (CCE).}
We introduce CCE into SEB to extract object-level instead of image-level information for generating category codes, thus solving the task mismatch issue between the two encoding branches. Another strategy adopted by prior works\,\cite{FewshotReweighting,metarcnn,FSDetView} is to directly use support images with an extra channel representing objects' locations as input. As shown in Table~\ref{tab:ablation1}, CCE achieves better performance, which shows CCE can effectively filter out redundant information and generate more accurate category codes compared with previous strategy.




\smallskip
\noindent
\textbf{Effect of Semantic Alignment Mechanism (SAM).}
Meta-learning does not aim to learn specific categories. However, with a limited number of base categories, it still inevitably learns category-specific features that only perform well on certain categories and fail to generalize to novel categories. As shown in Table~\ref{tab:ablation1}, SAM consistently boosts few-shot detection performance for novel categories, which demonstrates its effectiveness in preventing reliance on category-specific features. In Fig.\;\ref{fig:fig5}, we further visualize the attention maps of correlations between query features and category codes. Without SAM, our method produces strong responses for the base category (cat) with the learned category-specific features, while failing to produce clear responses for the novel category (bird). With SAM included, clear responses are produced for both base and novel categories, which implies that more generalizable representations are learned effectively.


\smallskip
\noindent
\textbf{Effect of \boldmath .}
We introduce , which is essentially a conventional cross-entropy loss, to classify the category codes of different categories for better discrimination. As shown in Table~\ref{tab:ablation1},  slightly but consistently boosts the performance. When there are relatively more training samples for novel categories (10-shot), the performance gain brought by  is marginal, which means Meta-DETR already can discriminate novel categories even without .

\smallskip
\noindent
\textbf{Effect of Unified Meta-Learning.}
We also study the effect of unified meta-learning in Table~\ref{tab:ablation2}. Specifically, we make modifications to Meta-DETR to perform separated learning for localization and classification, the two sub-tasks of object detection. Detailed architectures for this study are presented in appendices. As shown in Table~\ref{tab:ablation2}, unified meta-learning significantly outperforms other design choices, which proves the synergistic effect of the two sub-tasks. Interestingly, separate meta-learning for both sub-tasks performs slightly worse than meta-learning classification alone. This can be attributed to the intrinsic difficulty in meta-learning image-level localization with no support from the classification task.




\begin{table}
\begin{center}
\resizebox{0.452\textwidth}{!}{
\begin{tabular}{ p{1cm} c p{1cm} c | c c c }
\toprule
\multicolumn{2}{c|}{Transfer-Learning} & \multicolumn{2}{c|}{Meta-Learning} & \multicolumn{3}{c}{Shot} \\
\midrule

\multicolumn{1}{c}{cls} & \multicolumn{1}{c|}{loc} & \multicolumn{1}{c}{cls} & \multicolumn{1}{c|}{loc} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{10} \\

\midrule

\multicolumn{1}{c}{} & \multicolumn{1}{c|}{} &  &  & 5.4 & 21.0 & 44.8 \\

 & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{} &  & 11.0 & 33.9 & 53.9 \\

 & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & 9.8 & 32.5 & 52.7 \\
 
\midrule

\rowcolor{black!6} \multicolumn{4}{c|}{Unified Meta-Learning for cls \& loc} & \textbf{17.5} & \textbf{45.1} & \textbf{57.1} \\


\bottomrule
\end{tabular}
}
\end{center}
\vspace*{-1.8mm}
\caption{Ablation studies over the effect of unified meta-learning. Results for novel categories are averaged over multiple runs on the category split 1 of Pascal VOC.}
\label{tab:ablation2}
\vspace*{-0.8mm}
\end{table} 

\vspace*{+0.8mm}
\section{Conclusion}
\vspace*{+0.50mm}
This paper presents Meta-DETR, a novel few-shot object detection framework that unifies the meta-learning of object localization and classification at image level. By eliminating the region-wise prediction that is problematic in the few-shot scenarios and effectively leveraging the synergistic relationship between localization and classification, it overcomes the common weaknesses rooted in existing methods. Extensive experiments validate that Meta-DETR establishes new state-of-the-art and outperforms prior works by large margins without bells and whistles.


\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{bib}
}

\clearpage

\section{Appendix}

This section provides more details of our proposed method and experimental setups, which are omitted in the main paper due to space limitation. 

\subsection{Detailed Architecture of Meta-DETR}

The transformer encoder and decoder in the proposed Meta-DETR have similar setups as Deformable\;DETR\,\cite{DeformableDETR}. Concretely, both transformer encoder and decoder have 6 layers and adopt the multi-scale deformable attention module\,\cite{DeformableDETR} as their attention mechanism. The channel dimension is , and the intermediate dimension of fully-connected layers (FC) inside the transformer is . The dropout probability, number of attention heads, and number of object queries are set at , , and , respectively.

Fig.\;\ref{fig:supp_aggr} shows the architecture of the Aggregator inside the Decoding Branch (DB). The architecture has similar design as FsDetView\,\cite{FSDetView}, except that the query features represent whole-image rather than region-level information. Aggregation is conducted between category codes and each position of query features. Fig.\;\ref{fig:supp_ffn} illustrates the feed-forward network (FFN) in Decoding Branch (DB) that produces final predictions (omitted for simplicity in Fig.\,3 in the manuscript). It consists of a 1-layer MLP for confidence prediction and a 3-layer MLP for box prediction. FFN is shared for all the embeddings that are generated from the transformer decoder.


\subsection{Modified Meta-DETR for Ablation Study}

In Section\;\ref{sec:ablation}, we modified the proposed Meta-DETR to study the effect of unified meta-learning. In Table\;\ref{tab:ablation2}, \textit{transfer-learning} means that the specific sub-tasks (classification or localization, or both) are learned via naive fine-tuning strategy. Separated \textit{meta-learning} means that the specific sub-tasks are learned via a \textit{standalone} meta-learning-based component. To achieve this, we move the Aggregator after the transformer decoder and perform feature aggregation between category codes and the embeddings generated from the transformer decoder. Therefore, FFN becomes meta-learning-based components for specific sub-tasks, which manages to disentangle the meta-learning for the two sub-tasks. This design enables us to explore the effect of unified meta-learning.



\subsection{Detailed Training and Inference Setups}

\noindent
\textbf{Base Training Stage.}
All essential setups are provided in Section\;\ref{sec:imple_details}. For further details, please refer to our codes.

\smallskip
\noindent
\textbf{Few-Shot Fine-Tuning Stage.}
The few-shot fine-tuning stage shares the same setups as the base training stage, except for the total number of epochs and the learning rate decay epochs. Such differences are due to the significantly smaller number of training samples under the few-shot scenarios, so that more training epochs are required to reach full convergence. Detailed setups are presented in Table\;\ref{tab:appendix_ftepochs}. These numbers are empirically set solely based on the training loss trajectory, so we expect further performance gain if comprehensive hyper-parameter search is conducted.

\smallskip
\noindent
\textbf{Inference.}
Given a query image, Meta-DETR produces  predictions for each category when performing inference. However, both Pascal VOC and MS COCO accept only  predictions per image. We choose the top-scored  predictions across all categories as the final predictions.





\begin{figure}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/Aggr.pdf}
\end{center}
\vspace*{-1.8mm}
   \caption{Illustration of the detailed architecture of Aggregator in Decoding Branch (DB). Aggregation is performed between category codes and each position of query features.}
\label{fig:supp_aggr}
\vspace*{+3.5mm}
\end{figure}


\begin{figure}[t] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/FFN.pdf}
\end{center}
\vspace*{-1.8mm}
   \caption{Illustration of the feed-forward network (FFN) in Decoding Branch (DB) to produce final predictions. FFN is shared for all the embeddings generated from the transformer decoder.}
\label{fig:supp_ffn}
\vspace*{+3.5mm}
\end{figure}




\begin{table}
\begin{center}
\resizebox{0.476\textwidth}{!}{
\begin{tabular}{ c | c c c c c | c c  }
\toprule
\multirow{2}{*}{Setups} & \multicolumn{5}{c|}{Pascal VOC} & \multicolumn{2}{c}{MS COCO}  \\\cmidrule{2-8}
& 1 & 2 & 3 & 5 & 10 & 10 & 30 \\\midrule
Total Epochs & 700 & 600 & 600 & 500 & 500 & 500 & 500 \\
Decay Epochs & 600 & 500 & 500 & 425 & 425 & 425 & 425 \\\bottomrule
\end{tabular}
}
\end{center}
\vspace*{-1.2mm}
\caption{Setups of total number of epochs and learning rate decay epochs for the few-shot fine-tuning stage.}
\label{tab:appendix_ftepochs}
\vspace*{+3.5mm}
\end{table} 


\subsection{Evaluation Metrics}

\smallskip
\noindent
\textbf{Pascal VOC.}
For Pascal VOC, mean average precision (mAP) at IoU threshold 0.5 is used as the evaluation metric. In the context of few-shot object detection, mAP is averaged over all novel categories.

\smallskip
\noindent
\textbf{MS COCO.}
MS COCO's standard metrics are used for evaluation. Specifically,  is the primary metric that directly measures detectors' performance, which adopts 10 different IoU thresholds to reward detectors with better localization. Standard metrics also include  and , which correspond to the Pascal VOC metric and a more strict metric, respectively. In addition to average precision (AP), average recall (AR) also serves as an important evaluation metric, which measures the percentage of detected objects among all ground truth objects. Higher AR indicates less missed detection. Concretely, , , and  correspond to AR given 1 detection per image, 10 detections per image, and 100 detections per image, respectively. The MS COCO metrics also evaluate the performance for objects of different sizes (small, medium, and large), including , , , , , and . Similar to Pascal VOC, all these metrics are averaged over all novel categories in our experiments.



\begin{table}
\begin{center}
\resizebox{0.418\textwidth}{!}{
\begin{tabular}{ c | c c }
\toprule
Method & single-scale & multi-scale \\\midrule

Deformable\;DETR\,\cite{DeformableDETR} & 17.8 FPS  & 11.3 FPS  \\

Meta-DETR\,(Ours) & 11.0 FPS &  5.3 FPS \\\bottomrule

\end{tabular}
}
\end{center}
\vspace*{-1.0mm}
\caption{Inference speed comparison. Results are obtained using NVIDIA GeForce RTX 2080Ti GPU with single batch size on Pascal VOC.}
\label{tab:appendix_inferencespeed}
\end{table} 



\smallskip
\noindent
\textbf{Evaluation with Multiple Repeated Runs.}
More and more researchers have realized that few-shot object detection performance often comes with a large variance. The lower the number of shots, the more unstable the results are. This is because few-shot detection performance relies heavily on the quality of the training samples for novel categories. Therefore, with results from a single run, it is not easy to draw convincing conclusions. To address this issue, following \cite{fsdet} and \cite{FSDetView}, our results, as reported in Table\;\ref{tab:Performance_VOC_novel}-\ref{tab:ablation2}, are averaged over multiple repeated runs with different randomly sampled support datasets. Specifically, as we observe large performance variances in Pascal VOC, especially for 1-shot, 2-shot, and 3-shot, all our results on Pascal VOC are averaged over 10 randomly sampled support datasets. For MS COCO, we observe smaller variances with repeated runs, which can be attributed to the larger number of categories and shots. Therefore, we average our results on MS COCO over 5 randomly sampled support datasets.





\subsection{Inference Speed of Meta-DETR}

During inference, the category codes for all base and novel categories can be computed once and for all. This enables efficient inference of Meta-DETR. Table\;\ref{tab:appendix_inferencespeed} presents the inference speed of Meta-DETR and Deformable DETR\;\cite{DeformableDETR}. We can see that Meta-DETR only introduces moderate extra computational costs as compared with the naive fine-tuning approach.


\subsection{Qualitative Results}

We provide multiple qualitative visualizations of Meta-DETR's few-shot detection results in Figs.\;\ref{fig:supp_results_voc1_10shot_1}-\ref{fig:supp_results_coco_30shot_2}, which give a straightforward illustration of the performance of our method. Note that only detection results of novel categories are presented, as the major focus is to detect objects of novel categories. In addition, we only show results with confidence scores higher than . White boxes indicate correct detections, red solid boxes indicate false positives, and red dashed boxes indicate false negatives. It can be observed that the proposed Meta-DETR is able to detect novel objects even with scarce training samples. In addition, Meta-DETR performs exceptionally well on large objects and we will investigate how to handle small objects and cluttered objects in our future research.

\clearpage

\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/VOC_split1_10shot_1.pdf}
\end{center}
\vspace*{-1.5mm}
   \caption{Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 1. Novel categories include bird, bus, cow, motorcycle, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.}
\label{fig:supp_results_voc1_10shot_1}
\end{figure*}


\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/VOC_split1_10shot_2.pdf}
\end{center}
\vspace*{-1.5mm}
   \caption{Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 1. Novel categories include bird, bus, cow, motorcycle, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.}
\label{fig:supp_results_voc1_10shot_2}
\end{figure*}

\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/VOC_split2_10shot_1.pdf}
\end{center}
\vspace*{-1.5mm}
   \caption{Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 2. Novel categories include airplane, bottle, cow, horse, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.}
\label{fig:supp_results_voc2_10shot_2}
\end{figure*}

\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/VOC_split2_10shot_2.pdf}
\end{center}
\vspace*{-1.5mm}
   \caption{Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 2. Novel categories include airplane, bottle, cow, horse, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.}
\label{fig:supp_results_voc2_10shot_2}
\end{figure*}

\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/VOC_split3_10shot_1.pdf}
\end{center}
\vspace*{-1.5mm}
   \caption{Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 3. Novel categories include boat, cat, motorcycle, sheep, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.}
\label{fig:supp_results_voc3_10shot_1}
\end{figure*}

\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/VOC_split3_10shot_2.pdf}
\end{center}
\vspace*{-1.5mm}
   \caption{Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 3. Novel categories include boat, cat, motorcycle, sheep, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.}
\label{fig:supp_results_voc3_10shot_2}
\end{figure*}

\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/COCO_30shot_1.pdf}
\end{center}
\vspace*{-1.5mm}
   \caption{Visualization of multi-scale Meta-DETR's 30-shot object detection results on MS COCO. Novel categories include person, bicycle, car, motorcycle, airplane, bus, train, boat, bird, cat, dog, horse, sheep, cow, bottle, chair, couch, potted plant, dining table, and tv. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.}
\label{fig:supp_results_coco_30shot_1}
\end{figure*}

\begin{figure*}[t!] 
\begin{center}
   \includegraphics[width=1.0\linewidth]{Supp_Figures/COCO_30shot_2.pdf}
\end{center}
\vspace*{-1.5mm}
   \caption{Visualization of multi-scale Meta-DETR's 30-shot object detection results on MS COCO. Novel categories include person, bicycle, car, motorcycle, airplane, bus, train, boat, bird, cat, dog, horse, sheep, cow, bottle, chair, couch, potted plant, dining table, and tv. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.}
\label{fig:supp_results_coco_30shot_2}
\end{figure*}


\end{document}
