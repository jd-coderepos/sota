\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{soul}
\usepackage[accsupp]{axessibility}  

\usepackage{float}
\usepackage{pifont}

\newcommand{\xmark}{\ding{55}}\newcommand{\cmark}{\ding{51}}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{BiMaL: Bijective Maximum Likelihood Approach to \\Domain Adaptation in Semantic Scene Segmentation}

\author{Thanh-Dat Truong, Chi Nhan Duong, Ngan Le, Son Lam Phung, Chase Rainwater, Khoa Luu\\
CVIU Lab, University of Arkansas \quad 
Concordia University \\
Dep. of Industrial Engineering, University of Arkansas \quad
University of Wollongong\\
{\tt\small \{tt032, thile, cer, khoaluu\}@uark.edu, dcnhan@ieee.org, phung@uow.edu.au}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Semantic segmentation aims to predict pixel-level labels. It has become a popular task in various computer vision applications. While fully supervised segmentation methods have achieved high accuracy on large-scale vision datasets, they are unable to generalize on a new test environment or a new domain well. In this work, we first introduce a new Unaligned Domain Score to measure the efficiency of a learned model on a new target domain in unsupervised manner. Then, we present the new Bijective Maximum Likelihood\footnote{\url{https://github.com/uark-cviu/BiMaL}} (BiMaL) loss that is a generalized form of the Adversarial Entropy Minimization without any assumption about pixel independence. We have evaluated the proposed BiMaL on two domains. The proposed BiMaL approach consistently outperforms the SOTA methods on empirical experiments on ``SYNTHIA to Cityscapes'', ``GTA5 to Cityscapes'', and ``SYNTHIA to Vistas''.

\end{abstract}

\section{Introduction}

Semantic segmentation is one of the most popular  computer vision topics, which aims to to assign each pixel in an image to a 
predefined class.
It has various practical applications, especially in autonomous driving where a segmentation model is needed to recognize roads, sidewalks, pedestrians or vehicles in a large variety of urban conditions. A typical supervised segmentation model is usually trained on datasets with labels. However, annotating images for the semantic segmentation task is costly and time-consuming. 
Alternatively, a powerful and cost-effective way to acquire a large-scale training set is to use a simulation, e.g. game engines, to create a synthetic dataset \cite{Richter_2016_ECCV, Ros_2016_CVPR}. However, fully supervised models \cite{chen2018deeplab, le2018segmentation} trained on the synthetic datasets are often unable to perform well on real images due to the pixel appearance gap between synthetic and real images.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/same_entropy_correct_incorrect_new.png}
    \caption{\textbf{Two images have the same entropy but one has a poor prediction (a top image) and one has an better prediction (a bottom image).} Columns 1 and 2 are an input image and a ground truth. Columns 3 and 4 are an entropy map and a prediction of AdvEnt \cite{vu2019advent}. Column 5 is the results of our proposed method. The two predictions produced by AdvEnt have similar entropy scores ( and ). Meanwhile, the BiMaL value of the bottom prediction () is smaller than the top prediction (). Our results in the last column, which have better BiMaL values than AdvEnt, can well model the structure of an image. In particular, our results have sharper results of a barrier and a rider (white dash box), and a clear boundary between road and sidewalk.}
\label{fig:same_entropy_1}
    \vspace{-4mm}
\end{figure*}

Unsupervised Domain Adaptation (UDA) aims to train a machine learning model on an annotated dataset, i.e. the source, and guarantee its high performance on a new unlabeled dataset, i.e. the target.
The UDA approaches have been applied to various computer vision tasks such as Semantic Segmentation \cite{chen2018deeplab, le2018segmentation, li2020content, vu2019advent, vu2019dada, zhang2019category}, Face Recognition \cite{duong2019shrinkteanet, Luu_BTAS2009, Luu_FG2011, Luu_ROBUST2008, Luu_IJCB2011}. 
The recent UDA methods aim to reduce the cross-domain discrepancy, along with the supervised training on the source domain \cite{chen2018road, ganin2015unsupervised, long2015learning,pan2020unsupervised, tzeng2017adversarial, vu2019advent}.
In particular, these methods aim to minimize the distribution discrepancy of the 
deep representations extracted from the source and the target domains. This process can be performed at single or multiple levels of deep features using maximum mean discrepancies \cite{ganin2015unsupervised, long2015learning, tzeng2017adversarial}, or adversarial training \cite{chen2018road, chen2017no, hoffman18a, hoffman2016fcns, hong2018CVPR, tsai2018learning}. The approaches in this group have shown their potential in aligning the predicted outputs of images from the two domains. However, 
the binary cross-entropy 
label predicted by the learned discriminator is usually a weak indication of structural learning for the segmentation task. Another approach named self-training utilizes the pseudo-labels or generative networks conditioned on target images \cite{murez2018CVPR, zhu2017unpaired}. Semi-supervised learning is an approach related to UDA where the training set consists of both labeled and unlabeled samples. Thus, it has motivated several UDA approaches such as Class-balanced self-training (CBST) \cite{zou2018unsupervised},  and entropy minimization \cite{chen2019domain, grandvalet2005semi, pan2020unsupervised, springenberg2015unsupervised, vu2019advent}.
Although metrics such as entropy can be efficiently computed and adopted for training, they tend to rely on easy predictions, i.e. high confident scores, as references for the label transfer from source to target domains. This issue is alleviated in a later approach \cite{chen2019domain} by preventing learned models from over-focusing on high confident areas. However, this type of metrics is formulated in pixel-wised manner, and, therefore, neglects the structural information presented in the image (see Figure \ref{fig:same_entropy_1}). 



















\noindent
\textbf{Contributions of this Work. }
This work presents a new unsupervised domain adaptation approach to tackle the semantic segmentation problem. Table \ref{tab:summary} summarizes the difference between our proposed approach and the prior ones. Our contributions can be summarized as follows.

Firstly, a new Unaligned Domain Score (UDS) is introduced to measure the efficiency of the learned model on a target domain in an unsupervised manner.
Secondly, the presented UDS is further extended as a new loss function, named Bijective Maximum Likelihood (BiMaL) loss, that can be used with an unsupervised deep neural network to generalize on target domains. Indeed, we further demonstrate BiMaL loss is a generalized form of the Adversarial Entropy Minimization (AdvEnt) \cite{vu2019advent} without pixel independence assumption. 
Far apart from AdvEnt that assumes pixel independence, BiMaL loss is formed using a Maximum-likelihood formulation to model the global structure of a segmentation input and a Bijective function to map that segmentation structure to a deep latent space.
Finally, the proposed BiMaL method is evaluated on three popular large-scale semantic segmentation benchmarks, including GTA5 \cite{Richter_2016_ECCV}  CityScapes \cite{cordts2016cityscapes}, SYNTHIA \cite{Ros_2016_CVPR}  Cityscapes, and SYNTHIA  Vista \cite{MVD2017}. The experimental results demonstrate our proposed BiMaL approach consistently outperforms the State-of-the-Art (SOTA) methods \cite{chen2018road, pan2020unsupervised, tsai2018learning, vu2019advent, vu2019dada} in all these benchmark databases.
To the best of our knowledge, this is one of the first works that introduces a novel bijective maximum likelihood approach with flow-based metric to unsupervised domain adaptation in semantic segmentation.







\section{Related Work}

Unsupervised Domain Adaptation has recently become one of the most active research topics. The common UDA approaches are domain discrepancy minimization \cite{ganin2015unsupervised, long2015learning, tzeng2017adversarial}, adversarial learning \cite{chen2018road,chen2017no, hoffman18a, hoffman2016fcns, hong2018CVPR, tsai2018learning}, entropy minimization \cite{murez2018CVPR, pan2020unsupervised, vu2019advent, zhu2017unpaired}, self-training \cite{zou2018unsupervised}. In the scope of this work, UDA is focused on semantic segmentation.


Adversarial training is the most common approach employed to UDA for semantic segmentation. Similar to generative adversarial networks (GANs), the adversarial training paradigm aims at training a discriminator to predict the domain of inputs while the segmentation network tries to fool the discriminator. 
This adversarial step is trained simultaneously with the supervised segmentation task on the source domain.
Hoffman \etal \cite{hoffman2016fcns} first introduced GAN-based UDA approach to semantic segmentation. Later, Chen \etal \cite{chen2017no} presented global and class-wise adaptation learned by adversarial learning on pseudo labels. Considering the difference in spatial distribution, \cite{chen2018road} proposed a spatial-aware adaptation method to align two domains along with a target guided distillation loss. Hong \etal \cite{hong2018CVPR} learned a conditional generator to transform the feature maps of source domain to be similar to target domain. 
Tasi \etal \cite{tsai2018learning} used adversarial learning to learn a consistency of scene layout and local context between source and target domains. There are some prior methods that utilize the generative networks to synthesize target images conditioned on source images \cite{zhu2017unpaired, murez2018CVPR}. Hoffman \etal \cite{hoffman18a} presented Cycle-Consistent Adversarial Domain Adaptation that aligns at both pixel-level and feature-level representations. Zhu~\etal \cite{zhu2018ECCV} introduced a conservative loss in an adversarial framework that penalizes the easy and hard source examples. We \etal \cite{wu2018dcan} proposed a DCAN framework that uses the channel-wise feature alignment in the segmentation networks.
Sakaridis \etal \cite{SDHV18} proposed an UDA framework on scene understanding that gradually adapts a segmentation model 
from non-foggy to heavy-foggy images.

\begin{table*}[t]
\centering
\caption{ \textbf{Comparison in the properties between our proposed approach and other methods}. Convolutional Neural Network (CNN), Generative Adversarial Net (GAN), Bijective Network (BiN), Entropy Minimization (EntMin), Curriculum Training (CT), Image-wise Weighting (IW), Segmentation Map (Seg), Depth Map (Depth); : Cross-entropy Loss, : Adversarial Loss, : Huber Loss.}
\small
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Methods} & \textbf{Architecture}              & \textbf{Source Label} & \textbf{Learning Mechanism} & \textbf{Loss Function}                       & \textbf{Structural Learning} \\ \hline
AdaptSeg \cite{tsai2018learning}        & CNN + GAN                          & Seg  & Domain Adaptation &                                  & Weak (binary label)                                     \\ \hline
AdaptPatch \cite{tsai2019domain}     & CNN + GAN                          & Seg & Domain Adaptation &                                   & Weak (binary label)                                     \\ \hline
CBST \cite{zou2018unsupervised}             & CNN                                & Seg &           Self-Training                    &                                 & Not Applicable                                    \\ \hline
ADVENT \cite{vu2019advent}          & CNN + GAN                          & Seg   & Domain Adaptation & EntMin                         & Weak (binary label)                                     \\ \hline
MaxSquare \cite{chen2019domain}         & CNN + GAN                          & Seg  & Domain Adaptation & Squares loss + IW & Weak (binary label)                                     \\ \hline
IntraDA \cite{pan2020unsupervised}         & CNN + GAN                          & Seg  & Curriculum Learning & EntMin & Weak (binary label)                                     \\ \hline \hline
SPIGAN \cite{lee2018spigan}         & CNN + GAN                          & Seg + Depth & Domain Adaptation &  +                           & Weak (binary label)                                    \\ \hline
DADA \cite{vu2019dada}            & CNN + GAN                          & Seg + Depth  & Domain Adaptation &  +                           & Depth-aware Label                                   \\ 
\hline \hline
\textbf{BiMaL}  & \textbf{CNN + BiN} & \textbf{Seg}  & \textbf{Domain Adaptation} & \textbf{Maximum   Likelihood}      & \begin{tabular}{@{}c@{}} \textbf{Segmentation Density} \\ \textbf{(Unsupervised)}\end{tabular}                           \\ \hline
\end{tabular}
\label{tab:summary}
\vspace{-4mm}
\end{table*}

To enhance the performance of domain adaptation, several methods explore the use of privileged information available on source data \cite{chen2014recognizing, li2014exploiting, sarafianos2017adaptive}. Vapnik \etal \cite{vapnik2009new} first introduced the idea of privileged information, i.e. additional information only available at the training process. Later, many methods \cite{hoffman2016learning, lopez2015unifying, mordan2018revisiting, Sharmanska_2013_ICCV} take advantage of privileged information for various tasks. In semantic segmentation, SPIGAN \cite{lee2018spigan} proposed an UDA approach that utilizes the depth information during the training phase. Following SPIGAN,  Vu \etal\cite{vu2019dada} presented an adversarial approach that utilizes the depth-aware of source and target images.



Entropy minimization has been used for semi-supervised learning \cite{grandvalet2005semi, springenberg2015unsupervised}. Vu et al. \cite{vu2019advent} first introduced the entropy minimization approach for domain adaptation in semantic segmentation. The minimization process is solved by adversarial learning. Later, \cite{pan2020unsupervised} introduced an intra-domain adaptation approach based on the entropy level of predictions. The learning process involves two phases. The first phase performs adaptation from the source domain to the target domain, whereas the second phase aligns the hard split and easy split within the target domain.
Another recent UDA approach is self-training, where 
the predictions of the trained model are used as pseudo-labels for the unlabeled data to train the new model. Self-training has been widely used in classification \cite{NEURIPS2019_bf25356f}
and segmentation tasks \cite{zou2018unsupervised}. 




















\section{Unaligned Domain Scores (UDS)}





\begin{figure*}[!t]
    \centering
\includegraphics[width=0.8\textwidth]{figures/framework_new.png}
    \caption{\textbf{The Proposed Framework.} The RGB image input is firstly forwarded to a deep semantic segmentation network to produce a segmentation map. The supervised loss is employed on the source training samples. Meanwhile, the predicted segmentation on target training samples will be mapped to the latent space to compute the Bijective Maximum Likelihood loss. The bijective mapping network is trained on the ground-truth images of the source domain.}
    \label{fig:proposed_framework}
\end{figure*}

Let  be an input image of the source domain ( and  are the height and width of an image),   be an input image of the target domain,  where  be a semantic segmentation function 
that maps an input image to its corresponding segmentation map , i.e.  ( is the number of semantic classes). 
In general, given  labeled training samples from a source domain 
 
and  unlabeled samples from a target domain , the 
unsupervised domain adaptation for semantic segmentation can be formulated as: 

where  is the parameters of ,  is the probability density function. As the labels for  are available,  can be efficiently formulated as a supervised cross-entropy loss:

where  and  represent the predicted and ground-truth probabilities of the pixel at the location of   taking the label of  , respectively. 
Meanwhile,  handles unlabeled data from
the target domain where the ground-truth labels are not available. To alleviate this label lacking issue, several forms of  have been exploited such as cross-entropy loss with pseudo-labels \cite{zou2018unsupervised}, Probability Distribution Divergence (i.e. Adversarial loss defined via an additional Discriminator) \cite{tsai2018learning, tsai2019domain}, or entropy formulation \cite{vu2019advent, pan2020unsupervised}.  

\noindent
\textbf{Entropy minimization revisited.}
By adopting the \mbox{Shannon} entropy formulation to the target prediction and constraining function  to produce a high-confident prediction,
 can be formulated as 
Although this form of  can give a direct assessment of the predicted segmentation maps, 
it tends to be dominated 
by the high probability areas (since the high probability areas 
produce a higher value 
updated gradient due to   and ),  i.e. easy classes, rather than difficult classes \cite{vu2019advent}. 
More importantly, this is essentially a pixel-wise formation, where pixels are treated independently of each other.
Consequently, the structural information is usually neglected in this form. This issue could lead to a confusion point during training process where two predicted segmentation maps have similar entropy but different segmentation accuracy, one correct and other incorrect as shown in Fig \ref{fig:same_entropy_1}.

























\subsection{The Proposed UDS Metric}










In the entropy formulation, the pixel independent constraints are employed to convert the image-level metric to pixel-level metric. In contrast,
we propose an image-level UDS metric that can directly evaluate the structural quality of . 
Particularly, let  and  be the probability mass functions of the predicted distribution and the real (actual) distribution of the predicted segmentation map , respectively.
UDS metric measuring the efficiency of function  on the target dataset can be expressed as follows:

where 
 defines the distance between two distributions  and . Since there is no label for sample in the target domain, the direct access to  is not available. 
Note that although  and  could vary significantly in image space (e.g. difference in pixel appearance due to lighting, scenes, weather), their segmentation maps  and  share similar distributions in terms of both class distributions as well as global and local structural constraints (sky has to be above roads, trees should be on sidewalks, vehicles should be on roads, etc.). 
Therefore, one can practically adopted the prior knowledge learned from segmentation labels of the source domains for  as

where the distribution  
is the probability mass functions of the real distribution learned from ground-truth segmentation maps of . As a result, the proposed USD metric can be computed without the requirement of labeled target data for learning the density of segmentation maps in target domain. 
There are several choices for  to estimate the divergence between the two distributions  and . In this paper, we adopt the common metric such as  Kullback–Leibler (KL) formula for . 
Note that other metrics are also applicable in the proposed UDS formulation.
Moreover, to enhance the smoothness of the predicted semantic segmentation, a regularization term  is imposed into  as

By computing UDS, one can measure the quality of the predicted segmentation maps  on the target data. 

In the next sections, we firstly discuss in details the learning process of , and then derivations of the UDS metric for the novel Bijective Maximum Likelihood loss. 

\subsection{Learning Distribution with Bijective Mapping on the Source Domain} \label{sec:bijective}

Let 
 
be the bijective mapping function that maps a segmentation  to the latent space , i.e. , where  is the latent variable, and  is the prior distribution.
Then, the probability distribution  can be formulated via the change of variable formula:

where  is the parameters of ,  denotes the Jacobian determinant of function  with respect to . 
To learn the mapping function, the negative log-likelihood
will be minimized as follows:

In general, there are various choices for the prior distribution . However, the ideal distribution should satisfy two 
criteria: (1) simplicity in the density estimation, and (2) easy in sampling. 
Considering the two criteria, we choose Normal distribution as the prior distribution . 
Note that any other distribution is also feasible as long as it satisfies the mentioned criteria.



To enforce the information flow from a segmentation domain to a latent space with different abstraction levels, the bijective function  can be further formulated as a composition of several sub-bijective functions  as , 
where  is the number of sub-functions. The Jacobian  can be derived by . With this structure, the properties of each  will define the properties for the whole bijective mapping function . 
Interestingly, with this form,  becomes a DNN structure when  is a non-linear function built from a composition of convolutional layers. Several DNN structures \cite{dinh2015nice, dinh2017density,Duong_2017_ICCV, duong2020vec2face, glow, duong2019learning, truong2021fastflow} can be adopted for sub-functions.








\section{Bijective Maximum Likelihood Loss} 

In this section, we present the proposed Bijective Maximum Likelihood (BiMaL) which can be used as the loss of target domain . 
From Eqns. \eqref{eqn:DDS_score_label_extend} and \eqref{eqn:LYUpdated}, UDS metric can be rewritten as follows: 

It should be noticed that with any form of the distribution , the above inequality still holds as  and . Now, we define our Bijective Maximum Likelihood Loss as

where  defines the log-likelihood of  with respect to the density function .
Then, by adopting the bijectve function  learned from Eqn. \eqref{eqn:BijectiveLearning} using samples from source domain and the prior distribution , the first term of  in Eqn. \eqref{eqn:BiMaL} can be efficiently computed via log-likelihood formulation:

where . Thanks to the bijective property of the mapping function , the minimum negative log-likelihood loss 
can be effectively computed via 
the density of the prior distribution  and its associated Jacobian determinant . For the second term of , we further enhance the smoothness of the predicted semantic segmentation with the pair-wised formulation to encourage similar predictions for neighbourhood pixels with similar color:

where  denotes the neighbourhood pixels of ,  represents the color at pixel ; and  are the hyper parameters controlling the scale of Gaussian kernels. 
It should be noted that any regularizers \cite{chen2018deeplab, duong2029cvpr_automatic} enhancing the smoothness of the segmentation results can also be adopted for .   
Putting Eqns. \eqref{eqn:BiMaL}, \eqref{eqn:define_llk}, \eqref{eqn:define_tau} to Eqn \eqref{eqn:objective}, the objective function can be rewritten as: 
Figure \ref{fig:proposed_framework} illustrates our proposed BiMaL framework to learn the deep segmentation network . Also, we can prove that direct entropy minimization as Eqn. \eqref{eqn:entropy_loss} is just a particular case of our log likelihood maximization. We will further discuss how our maximum likelihood can cover the case of pixel-independent entropy minimization in Section \ref{sec:MLE_Entropy}.
















\noindent
\subsection{BiMaL properties}  \label{sec:MLE_Entropy}


\textit{\textbf{Global Structure Learning.}} Sharing similar property with \cite{duong2016dam_cvpr, duong2019dam_ijcv, duong2020vec2face, Duong_2017_ICCV, 9108692}, from Eqn. \eqref{eqn:bijective_mapping}, as the learned density function is adopted for the entire segmentation map , the global structure in  can be efficiently captured and modeled.





\textit{\textbf{Tractability and Invertibility.}} Thanks to the designed bijection F, the complex distribution of segmentation maps can be efficiently captured. Moreover, the mapping function is bijective, and, therefore, both inference and generation  are exact and tractable.
\subsection{Relation to Entropy Minimization} \label{sec:MLE_Entropy}
The first term of UDS in Eqn. \eqref{eqn:KL_to_llk} can be derived as

where  is the random variable with possible values , and  denotes the entropy of the random variable .
It can be seen that the proposed negative log-likelihood  is an upper bound of the entropy of . Therefore, minimizing our proposed BiMaL loss will also enforce the entropy minimization process. 
Moreover, by not assuming pixel independence, our proposed BiMaL can model and evaluate structural information at the image-level better than previous pixel-level approaches \cite{chen2019domain, pan2020unsupervised, vu2019advent}.

































































\section{Experimental Results}

This section will present our experimental results on three different benchmarks, i.e. SYNTHIA  Cityscapes, GTA  Cityscapes, and SYNTHIA  Vistas. First, we overview datasets and network architectures used in our experiments. Second, we present the ablation study to analyze the effectiveness of our proposed BiMaL and the capability of the bijective network. Finally, we present the quantitative and qualitative results of our method compared to prior methods on the three benchmarks.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/databases_small.png}
    \caption{Examples of four semantic segmentation datasets including RGB images (top row) and corresponding ground-truth images (bottom row). (a) GTA and (b) SYNTHIA are synthetic datasets, (c) Cityscapes and (d) Vistas are real collected datasets.} \label{fig:database}
    \vspace{-4mm}
\end{figure}

\subsection{Datasets}

\textbf{GTA5} \cite{Richter_2016_ECCV} is a synthetic dataset containing  densely labelled images at the resolution of . This dataset was collected from the game Grand Theft Auto V. The ground-truth annotations were automatically generated with 33 categories. In our experiments, we consider 19 categories that are compatible with the Cityscapes \cite{cordts2016cityscapes}.



\textbf{SYNTHIA (SYNHIA-RAND-CITYSCAPES) }\cite{Ros_2016_CVPR} is also synthetic dataset that contains  pixel-level labelled RGB images. In our experiments, we use the 16 common categories that overlap with the Cityscapes dataset.

\textbf{Cityscapes} \cite{cordts2016cityscapes} is a real-world dataset including  images with fine semantic, dense pixel annotations of 30 classes.
In our experiments,  images are used for training and  images are used for testing.






\textbf{Vistas (Mapillary Vistas Dataset)} \cite{MVD2017} is diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world. Vistas consists of  high-resolution images and  semantic object categories. In our experiments, we consider 7 classes that are common to SYNTHIA, Cityscapes and Vistas as shown in Fig. \ref{fig:database}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/ablation_loss_new.png}
    \caption{Ablative semantic segmentation performance mIoU (\%) on the effectiveness of the proposed BiMaL loss.}
\label{fig:ablation_loss}
\end{figure}


\begin{figure}[t]
    \centering
\includegraphics[width=0.45\textwidth]{figures/sample_sec_rec.png}
    \caption{\textbf{Reconstructed Images and Synthetic Images From The Bijective Mapping Function }. (a) Reconstructed images (bottom row) from the corresponding input images (top row). (b) Synthetic images sampled from the latent space.} \label{fig:rec_images}
    \vspace{-4mm}
\end{figure}



\begin{table*}[t]
\centering
        \caption{\textbf{Semantic segmentation performance mIoU (\%) on Cityscapes validation set of different models trained on SYNTHIA}. We also show the mIoU (\%) of the  classes (mIoU*) excluding classes with *.}
            \resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
				\multicolumn{19}{c}{ SYNTHIA  Cityscapes (16 classes)}\\
				\hline
				Models  & \rotatebox{90}{\textbf{road}} & \rotatebox{90}{\textbf{sidewalk}} & \rotatebox{90}{\textbf{building}} & \rotatebox{90}{\textbf{wall*}} & \rotatebox{90}{\textbf{fence*}} & \rotatebox{90}{\textbf{pole*}} & \rotatebox{90}{\textbf{light}} & \rotatebox{90}{\textbf{sign}} & \rotatebox{90}{\textbf{veg}} & \rotatebox{90}{\textbf{sky}} & \rotatebox{90}{\textbf{person}} & \rotatebox{90}{\textbf{rider}} & \rotatebox{90}{\textbf{car}} & \rotatebox{90}{\textbf{bus}} & \rotatebox{90}{\textbf{mbike}} & \rotatebox{90}{\textbf{bike}} & \rotatebox{90}{\textbf{mIoU}} & \rotatebox{90}{\textbf{mIoU*}}\\
				\hline
				Without Adaptation & 64.9 & 26.1 & 71.5 & 3.0 & 0.2 & 21.7 & 0.1 & 0.2 & 73.1 & 71.0 & 48.4 & 20.7 & 62.9 & 27.9 & 12.0 & 35.6 & 33.7 & 39.6 \\
				SPIGAN-no-PI~\cite{lee2018spigan}&69.5&29.4&68.7&4.4&0.3&32.4&5.8&15.0&81.0&78.7&52.2&13.1&72.8&23.6&7.9&18.7&35.8&41.2\\
				SPIGAN~\cite{lee2018spigan}&71.1&29.8&71.4&3.7&0.3&\textbf{33.2}&6.4&{15.6}&81.2&78.9&52.7&13.1&75.9&25.5&10.0&20.5&36.8&42.4\\
AdaptSegnet~\cite{tsai2018learning}&79.2&37.2&78.8&-&-&-&9.9&10.5&78.2&80.5&53.5&19.6&67.0&29.5&21.6&31.3&-&45.9\\
				AdaptPatch~\cite{tsai2019domain}&82.2&39.4&79.4&-&-&-&6.5&10.8&77.8&82.0&54.9&21.1&67.7&30.7&17.8&32.2&-&46.3\\
				CLAN~\cite{luo2018taking}&81.3&37.0&80.1&-&-&-&{16.1}&13.7&78.2&81.5&53.4&21.2&73.0&32.9&{22.6}&30.7&-&47.8\\
				AdvEnt~\cite{vu2019advent}&87.0&44.1&79.7&{9.6}&{0.6}&24.3&4.8&7.2&80.1&83.6&{56.4}&\textbf{23.7}&72.7&32.6&12.8&33.7&40.8&47.6\\
IntraDA \cite{pan2020unsupervised} & 84.3 & 37.7 & 79.5 & 5.3 & 0.4 & 24.9 & 9.2 & 8.4 & 80.0 & 84.1 & \textbf{57.2} & 23.0 & 78.0 & 38.1 & 20.3 & 36.5 & 41.7 & 48.9 \\
				DADA\cite{vu2019dada} &{89.2}&{44.8}&{81.4}&6.8&0.3&26.2&8.6&11.1&{81.8}&{84.0}&54.7&19.3&{79.7}&{40.7}&14.0&{38.8}&{42.6}&{49.8} \\
\textbf{Our BiMaL} & \textbf{92.8} & \textbf{51.5} & \textbf{81.5} & \textbf{10.2} & \textbf{1.0} & 30.4 & \textbf{17.6} & \textbf{15.9} & \textbf{82.4} & \textbf{84.6} & 55.9 & 22.3 & \textbf{85.7} & \textbf{44.5} & \textbf{24.6} & \textbf{38.8} & \textbf{46.2} & \textbf{53.7} \\
				\hline
			\end{tabular}
			}
			\label{tab:synthia2city} 
\end{table*}

\begin{table*}[t]
    \centering
     \caption{\textbf{Semantic segmentation performance mIoU (\%) on Cityscapes validation set of different models trained on GTA5}}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
        \multicolumn{21}{c}{GTA5  Cityscapes (19 classes)}\\
\hline 
        Models & \rotatebox{90}{\textbf{road}} & \rotatebox{90}{\textbf{sidewalk}} & \rotatebox{90}{\textbf{building}} & \rotatebox{90}{\textbf{wall}} & \rotatebox{90}{\textbf{fence}} & \rotatebox{90}{\textbf{pole}} & \rotatebox{90}{\textbf{light}} & \rotatebox{90}{\textbf{sign}} & \rotatebox{90}{\textbf{\textbf{veg}}} & \rotatebox{90}{\textbf{terrain}} & \rotatebox{90}{\textbf{sky}} & \rotatebox{90}{\textbf{\textbf{person}}} & \rotatebox{90}{\textbf{rider}} & \rotatebox{90}{\textbf{car}}& \rotatebox{90}{\textbf{truck}} & \rotatebox{90}{\textbf{bus}} & \rotatebox{90}{\textbf{train}} & \rotatebox{90}{\textbf{mbike}} & \rotatebox{90}{\textbf{bike}} & \rotatebox{90}{\textbf{mIoU}} \\
        \hline
        Without Adaptation~\cite{tsai2018learning}  & 75.8 & 16.8 & 77.2 & 12.5 & 21.0 & 25.5 & 30.1 & 20.1 & 81.3 & 24.6 & 70.3 & 53.8 & 26.4 & 49.9 & 17.2 & 25.9 & \textbf{6.5} & 25.3 & 36.0 & 36.6 \\
        ROAD~\cite{chen2018road}                    & 76.3 & 36.1 & 69.6 & 28.6 & 22.4 & {28.6} & 29.3 & 14.8 & 82.3 & 35.3 & 72.9 & 54.4 & 17.8 & 78.9 & 27.7 & 30.3 & 4.0 & 24.9 & 12.6 & 39.4 \\
        AdaptSegNet~\cite{tsai2018learning}         & 86.5 & 36.0 & 79.9 & 23.4 & 23.3 & 23.9 & {35.2} & 14.8 & 83.4 & 33.3 & 75.6 & 58.5 & 27.6 & 73.7 & 32.5 & 35.4 & 3.9 & 30.1 & 28.1 & 42.4 \\
        MinEnt~\cite{vu2019advent}                  & 84.2 & 25.2 & 77.0 & 17.0 & 23.3 & 24.2 & 33.3 & \textbf{26.4} & 80.7 & 32.1 & 78.7 & 57.5 & {30.0} & 77.0 & {37.9} & 44.3 & 1.8 & 31.4 & \textbf{36.9} & 43.1 \\
        AdvEnt~\cite{vu2019advent}                  & {89.9} & {36.5} & {81.6} & {29.2} & {25.2} & {28.5} & 32.3 & 22.4 & 83.9 & 34.0 & 77.1 & 57.4 & 27.9 & {83.7} & 29.4 & 39.1 & 1.5 & 28.4 & 23.3 & 43.8 \\


        \textbf{Our BiMaL} & \textbf{91.2} & \textbf{39.6} & \textbf{82.7} & \textbf{29.4} & \textbf{25.2} & \textbf{29.6} & \textbf{34.3} & 25.5 & \textbf{85.4} & \textbf{44.0} & \textbf{80.8} & \textbf{59.7} & \textbf{30.4 }& \textbf{86.6 }& \textbf{38.5} & \textbf{47.6} & 1.2 & \textbf{34.0} & 36.8 & \textbf{47.3} \\ 
\hline 
    \end{tabular}
    }
    \label{tab:gta52city}
    \vspace{-4mm}
\end{table*}


\subsection{Network Architectures}

In our experiments, we adopt the DeepLab-V2 \cite{chen2018deeplab} with ResNet-101 \cite{He2015} backbone for the segmentation network . Also, we utilize the Atrous Spatial Pyramid Pooling with sampling rate . We only use the output of layer \textit{conv5} to predict the segmentation.
In the Bijective network , we use the multi-scale architecture as \cite{dinh2015nice, dinh2017density, duong2019learning, glow, Duong_2017_ICCV}. For each scale, we have multiple steps of flow, each of which consists of ActNorm, Invertible  Convolution, and Affine Coupling Layer \cite{glow, 9108692}. In our experiments, the number of scales and number of flow steps are set to  and , respectively. 

The entire framework is implemented in PyTorch \cite{paszke2019pytorch}. Training and validating models are conducted on 4 GPUs of NVIDIA Quadpro P8000 with 48GB each GPU. Segmentation and bijective networks are trained by a Stochastic Gradient Descent optimizer \cite{bottou2010large} with learning rate , momentum , and weight decay . The batch size per GPU is set to  for segmentation network, and  for learning bijective network. 
The image size is set to  pixels in all experiments.




\subsection{Ablation Study}

\textbf{Effectiveness of Losses.} Figure \ref{fig:ablation_loss} reports the semantic performance (mIoU) of BiMaL on the 16 classes of the Cityscape validation set when the model is trained on \mbox{SYNTHIA} dataset. We consider three cases: (1) without adaptation (train with source only), (2) BiMaL without regularization term ( only), and (3) BiMaL with regularization term (). Overall, the proposed BiMaL improve the performance of the method. In particular, the mIoU accuracy of the baseline (without adaptation) is . 
In comparison, BiMaL without regularization and BiMaL with regularization achieve the mIoU accuracy of  and , respectively. 
In terms of per-class accuracy, using BiMaL significantly improves the performance on classes of \textit{`road'}, \textit{`sidewalk'}, \textit{`bus'}, and \textit{`motocycle'}.

\textbf{Bijective Network Ability. } 
We conduct a pilot experiment of the bijective network on ground-truth semantic segmentation images of the GTA dataset.
This experiment aims to analyze the ability of the bijective network in modeling the image and structure information.
The number of scales and number of flow steps are set to , and , respectively. 
As shown in Figure \ref{fig:rec_images}(a), our bijective network can successfully reconstruct good-quality images. 
It also can synthesize images sampled from the latent space as shown in Figure \ref{fig:rec_images}(b). These experimental results have shown that the bijective network can model images even with complex structures as scene segmentation.














\begin{figure*}[t]
    \centering
\includegraphics[width=0.72\textwidth]{figures/qualitative_results_10_new_2.png}
    \caption{\textbf{Qualitative results of the SYNTHIA  Cityscapes experiment.} Columns 1 and 5 are the input and corresponding ground truth. Columns 2, 3 and 4 are the results of the model without adaptation, AdvEnt \cite{vu2019advent}, and our method.} \label{fig:qual_res_synthia2cityscape}
    \vspace{-5mm}
\end{figure*}

\subsection{Comparisons with SOTA Methods}

We present the experimental results of the proposed approach in comparison to other strong baselines.
Comparative experiments are conducted on three benchmarks: 
i.e. SYNTHIA  Cityscapes, GTA5  Cityscapes, and SYNTHIA  Vistas. In all three benchmarks, our method consistently achieves the SOTA semantic segmentation performance in term of ``mean Intersection over Union" (mIoU).

\textbf{SYNTHIA  Cityscapes.} Table \ref{tab:synthia2city} presents the semantic performance (mIoU) on the 16 classes of the Cityscape validation set. Our proposed method achieves better accuracy than the prior methods, i.e.  higher than DADA \cite{vu2019dada} by . Considering per-class results, our method significantly improves the results on classes of \textit{`sidewalk'} (), \textit{`car'} (), and \textit{`bus'} (). 
We also report the results on a 13-class subset where our proposed method also achieves the State-of-the-Art performance.

\textbf{GTA5  Cityscapes.} Table \ref{tab:gta52city} shows the mIoU of 19 classes of Cityscapes on the validation set. Our approach gains mIoU of  that is state-of-the-art performance compared to the prior methods. Analysing per-class results,
our method gains the improvement on most classes. In particular, the results on classes of \textit{`terrain'} (+), \textit{`truck'} (), \textit{`bus'} (), \textit{`motorbike'} () demonstrate significant improvements compared to AdvEnt. 
For other classes, the proposed method gains moderate improvements, compared to prior SOTA methods.





\begin{table}[b]
    \vspace{-6mm}
    \centering
    \caption{\textbf{Semantic segmentation performance mIoU (\%) on Vistas testing set of different models trained on SYNTHIA.} (const. denotes for construction)}
\resizebox{8.5cm}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \multicolumn{9}{c}{SYNTHIA  Vistas (7 classes)}\\
    \hline
    Models & \rotatebox{90}{\textbf{flat}} & \rotatebox{90}{\textbf{const.}} & \rotatebox{90}{\textbf{object}} & \rotatebox{90}{\textbf{nature}} & \rotatebox{90}{\textbf{sky}} & \rotatebox{90}{\textbf{human}} & \rotatebox{90}{\textbf{vehicle}} & \rotatebox{90}{\textbf{mIoU}} \\
    \hline
     SPIGAN-no-PI~\cite{lee2018spigan}  &  53.0 & 30.8 & 3.6 & 14.6 & 53.0 & 5.8 & 26.9 & 26.8       \\
     SPIGAN~\cite{lee2018spigan}  &   74.1 & 47.1 & 6.8 & 43.3 & 83.7 & 11.2 & 42.2 & 44.1      \\
AdvEnt \cite{vu2019advent}  & 86.9 & 58.8 & 30.5 & 74.1 & 85.1 & 48.3 & 72.5 & 65.2 \\
    DADA \cite{vu2019dada} & 86.7 & \textbf{62.1} & 34.9 & 75.9 & \textbf{88.6} & 51.1 & 73.8 & 67.6 \\
\textbf{Our BiMaL} & \textbf{87.6} & 61.6 &  \textbf{35.3} & \textbf{77.5} &  87.8 &  \textbf{53.3} & \textbf{75.6} & \textbf{68.4} \\
     \hline 
    \end{tabular}
    }
    \label{tab:synthia2vistas}
\end{table}


\textbf{SYNTHIA  Vistas.} Table \ref{tab:synthia2vistas} reports the mIoU on 7 classes of the Vistas testing set. Our approach gains an mIoU of  which is the SOTA performance compared to prior methods. 
Moreover, our method also gains moderate improvements in per-class accuracy.

\textbf{Qualitative Results.} Figure \ref{fig:qual_res_synthia2cityscape} illustrates the qualitative results of the SYNTHIA  Cityscapces experiment. Our method gives the better qualitative results compared to a model trained on the source domain and AdvEnt \cite{vu2019advent}. 
Our method can model well the structure of an image. In particular, our results have a clear border between \textit{`road'} and \textit{`sidewalk'}. Meanwhile, the results of model trained on source only and AdvEnt have an unclear border between \textit{`road'} and \textit{`sidewalk'}. Overall, our qualitative semantic segmentation results are sharper than the results of AdvEnt.




\vspace{-2mm}
\section{Conclusions}

This paper has presented a new Bijective Maximum Likelihood approach to domain adaptation in semantic scene segmentation. Compared to Adversarial Entropy Minimization loss, it is a more generalized form and can work without any assumption about pixel independence. 
In addition, a new Unaligned Domain Score metric has been also introduced to measure the efficiency of a segmentation model on a new target domain in the unsupervised manner. 
Through intensive experiments on three different datasets, i.e. SYNTHIA  Cityscapes, GTA  Cityscapes, and SYNTHIA  Vistas, we achieve SOTA performance compared to prior methods. Specifically, our semantic segmentation accuracy on these three benchmarks are , , and , respectively.
The future direction of this work is to solve challenging cases coming from the differences in ``segmentation structures'' between source and target domains such as left- and right-hand traffic.



\vspace{-4mm}
\paragraph{Acknowledgement} This work is supported by NSF Data Science, Data Analytics that are Robust and Trusted (DART), Chancellor's Innovation Fund, UAF and NSF Small Business Grant.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egpaper_final}
}

\end{document}
