\documentclass[letterpaper]{article} \usepackage[submission]{aaai24}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in} \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/TemplateVersion (2024.1)
}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}



\setcounter{secnumdepth}{0} 





\title{From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models}
\author{
Changming Xiao\textsuperscript{\rm 1}\equalcontrib,
    Qi Yang\textsuperscript{\rm 1}\equalcontrib, 
    Feng Zhou\textsuperscript{\rm 2},
    Changshui Zhang\textsuperscript{\rm 1}\thanks{Corresponding author.}
}
\affiliations{
\textsuperscript{\rm 1}Department of Automation, Tsinghua University\\




    \textsuperscript{\rm 2}Algorithm Research, Aibee Inc.\\
\{xcm18, yangq18\}@mails.tsinghua.edu.cn, 
zcs@mail.tsinghua.edu.cn
}

\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
\title{My Publication Title --- Multiple Authors}
\author {
First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
\textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


\usepackage{bibentry}


\begin{document}

\maketitle

\begin{abstract}
Diffusion models have revolted the field of text-to-image generation recently.
The unique way of fusing text and image information contributes to their remarkable capability of generating highly text-related images. 
From another perspective, these generative models imply clues about the precise correlation between words and pixels.
In this work, a simple but effective method is proposed to utilize the attention mechanism in the denoising network of text-to-image diffusion models.
Without re-training nor inference-time optimization, the semantic grounding of phrases can be attained directly.
We evaluate our method on Pascal VOC 2012 and Microsoft COCO 2014 under weakly-supervised semantic segmentation setting and our method achieves superior performance to prior methods.
In addition, the acquired word-pixel correlation is found to be generalizable for the learned text embedding of customized generation methods, requiring only a few modifications.
To validate our discovery, we introduce a new practical task called "personalized referring image segmentation" with a new dataset.
Experiments in various situations demonstrate the advantages of our method compared to strong baselines on this task.
In summary, our work reveals a novel way to extract the rich multi-modal knowledge hidden in diffusion models for segmentation.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Dense image prediction is a long-established research field that aims at producing pixel-level labels for given images~\cite{FCN}. 
It is the foundation of many applications in fields such as biomedicine~\cite{unet}, robotics~\cite{affordancenet}, and surveillance~\cite{anomalydetect}.
To attain precise masks, the dense image prediction task usually requires expensive dense annotations for training. 
Although recent works~\cite{groupvit, clims} have shown impressive results when training without pixel-wise labels, deliberate designs of the model architecture or the optimization objective are required.
With the development of powerful foundation models trained with internet-scale data, studies have emerged on how to mine valuable information from these off-the-shelf models for diverse tasks.
Regarding segmentation, some methods~\cite{woanyseg, denseclip} have utilized the learned localization information from text-image discriminative models~\cite{clip} to reduce reliance on segmentation-specific schemes.
Inspired by a renowned quote from Feynman: \emph{What I cannot create, I do not understand}, we believe that generative models, the counterpart of discriminative models, should also have a thorough comprehension of images.


Diffusion models~\cite{score, ddpm} have opened a new era of generative models, and their multi-modal variants~\cite{imagen, stablediffusion} trained on billions of image-caption pairs have revolutionized the field of text-to-image synthesis.
When diffusion models generate images from texts, the association between words and different spatial regions can be leveraged to indicate localization information.
We propose a simple but effective way to distill this information and use it for dense image prediction as shown in Figure~\ref{fig:outline}.
Building off a recently open-sourced text-to-image diffusion model~\cite{stablediffusion}, our method exhibits remarkable semantic segmentation capability.
Compared to previous works, our proposed method doesn't need segmentation-specific re-training~\cite{ddpmseg}, module addition~\cite{ODISE}, or inference time optimization~\cite{peekaboo}, and the whole pipeline is accomplished with no exposure to pixel-wise labels~\cite{SAM}.

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{pics/outline_complex.pdf}
  \caption{
  An overview of our proposed framework.
  We first add noise to the image and then input it into the denoising U-net with specially designed text queries.
  Next, we combine cross-attention and self-attention in the model to obtain the correlation map between words and pixels.
  After comparing different correlation maps and post-processing with dense CRF~\cite{dCRF}, we attain pseudo masks at last.
  Best viewed in color.
  }
  \label{fig:outline}
\end{figure*}
Leveraging pre-trained text-to-image diffusion models brings more benefits.
It has been shown that generative models have a better spatial and relational understanding than discriminative models~\cite{ODISE}, which is crucial for segmentation.
Besides, internet-scale training data endows the model with the ability to handle open-vocabulary scenarios.
Furthermore, customized generation with text-to-image diffusion models has been proven feasible~\cite{dreambooth, customdiffusion}.
These methods typically map the subject to an identifier word, and novel renditions of it can be obtained by inserting the word into various descriptions.
We integrate this technique with our method and locate user-specific items by exploiting the correspondence between learned word embeddings of personalized concepts and image segments.
It is worth noting that we can compose customized instances and textual contexts into multi-modal segmentation queries, which was hard to achieve with previous methods~\cite{PALAVRA}.

To investigate the performance of different methods in this customized case, we introduce a new task called "personalized referring image segmentation" with a new dataset named Mug19.
This task aims to locate user-specific entities and has many practical application scenarios, such as those for household robots.
The dataset is collected in a laboratory scenario and is conscientiously devised to ensure that in most cases, uni-modal information is insufficient to locate the proper object.
This setup makes it a more pragmatic and user-friendly task and allows this new benchmark to assess the multi-modal comprehension ability of different models.

We conduct experiments from different aspects to validate the effectiveness of our method.
We first evaluate the weakly-supervised semantic segmentation ability of our approach on classic datasets: Pascal VOC 2012~\cite{pascalvoc} and MS COCO 2014~\cite{mscoco}.
Then we demonstrate that we can locate personalized embeddings just like locating category embeddings, showcasing the generality of our framework.
We further reveal that traditional approaches using uni-modal information have difficulties in dealing with our proposed task.
Finally, ablation studies are conducted to validate the intuition of our framework designs.

In summary, the contributions of this work are as follows:
\begin{itemize}
    \item A novel method for open-vocabulary segmentation is proposed, which utilizes the attention mechanism in off-the-shelf text-to-image diffusion models.
    \item A new benchmark named "personalized referring image segmentation" is introduced, which is valuable for both industrial application and academic research.
    \item Experiment results on diverse segmentation tasks verify that our method can achieve SOTA performance.
\end{itemize}

\section{Related Work}
\label{sec:related}

\subsection{Open-Vocabulary Segmentation}
\label{sec:openvoc}

Researchers have developed different technologies to make segmentation systems more practical.
One popular avenue is considering the weakly supervised setting and using image-level annotations which are easier to obtain~\cite{affinitynet, clims}.
But these CAM-based~\cite{cam} solutions are limited to a predefined taxonomy.
Other researchers consider zero-shot learning for segmentation, which aims to transfer the knowledge extracted from seen categories to unseen ones.
With the development of word embedding~\cite{word2vec, glove}, language generalization ability has been exploited to depict semantic associations between different classes~\cite{openparse, zs3}.

Recently, foundation models have made game-changing progress, and zero-shot abilities have been found to automatically emerge from them~\cite{gpt3, instructgpt, clip}.
Thus open-vocabulary paradigm\footnote{Also known as open-set paradigm or open-world paradigm.}, a more general zero-shot formulation, has gradually replaced the classic paradigm~\cite{ovd, UVO}.
CLIP~\cite{clip}, a discriminative foundation model which has learned vast image-text knowledge from the internet, has become a common component of open-vocabulary recognition methods~\cite{vild, openseg, lseg, denseclip}.
Nevertheless, it has been discovered that the representation of CLIP is sub-optimal for segmentation tasks~\cite{ODISE}.
Therefore, we leverage a generative diffusion model instead~\cite{stablediffusion}, which is believed to have a thorough perception of scene-level structure~\cite{ODISE, daam}.


\subsection{Diffusion Model for Segmentation}
\label{sec:diffseg}

With the development of diffusion models, researchers begin to pay attention to their potential in perception tasks.
In addition to global prediction tasks like classification~\cite{diffusionclass}, dense prediction tasks like segmentation are also gradually attracting attention~\cite{diffusionimplicitseg, ddpmseg, VPD, grounddiff, diffumask}.
These methods can be roughly divided into  groups.

One group regards the segmentation task as a generative process~\cite{diffusionimplicitseg}.
They learn the conditional denoising process from noise distribution to dense masks.
The input image serves as a condition to guide the sampling process, and dense annotations are required for training.
Instead of learning a different denoising process, another group exploits the knowledge of pre-trained diffusion models~\cite{ddpmseg, VPD}.
They leverage diffusion features to train additional networks that provide dense predictions.
As pre-trained diffusion models possess powerful generative capability, their internal representation captures rich semantic information.
Thus labels used for training and parameters to be optimized can be greatly reduced, making their frameworks very efficient.
In order to further diminish dependence on pixel-wise labels, the last group operates on synthetic data~\cite{grounddiff, diffumask}.
They synthesize realistic images and segmentation masks simultaneously and use this automatically constructed dataset to train segmentation models.
The trained model exhibits competitive performance on real images to models trained on real data.
Nevertheless, due to the domain gap between synthetic data and real data, deliberate designs of data processing are required for good performance.

Compared to these segmentation methods that employ diffusion models, our method is more flexible and does not require dense annotations, segmentation-specific re-training, or complicated data processing designs.


\subsection{Composed Image Retrieval}
\label{sec:CIR}

Composed image retrieval is a multi-modal task aimed at retrieving images through image and text queries~\cite{CIR}.
Earlier studies have focused on synthesis data~\cite{CIR} and fashion products~\cite{fashioniq}, while recent works have taken open-domain data into consideration~\cite{CIRR}.
From a methodological perspective, researchers have explored different ways to fuse multi-modal information.
They have proposed various mechanisms ranging from affine transformation~\cite{film} to cross-attention~\cite{CIRlocal}.

Currently, with the development of vision-and-language pre-trained models~\cite{vilbert, clip}, researchers have contemplated applying them to composed image retrieval~\cite{CIRR, FashionVLP, pic2word, PALAVRA}.
Among these approaches,~\cite{pic2word, PALAVRA} took up early-fusion and had freer reasoning capabilities than late-fusion methods~\cite{CIRR, FashionVLP}.
We adopt early-fusion in our proposed method, but we care more about localization ability, which is valuable for many practical applications.
Although PALAVRA~\cite{PALAVRA} also considered the segmentation task, its experimental results indicated its failure to distinguish between context and subject.
On the contrary, we design the localization task to rely on the comprehension of context and subject, and experimental results demonstrate the effectiveness of our method.







\section{Method}
\label{sec:method}

\subsection{Preliminary}
\label{sec:preliminary}

Diffusion models~\cite{ddpm, score} are a class of generative models which learn the data distribution by progressively denoising from a tractable noise distribution.
They can be interpreted as a sequence of time-conditional denoising auto-encoders.
To reduce resource consumption, Latent Diffusion Model~\cite{stablediffusion} is proposed which conducts the diffusion process in the latent space obtained by a perceptual compression model~\cite{vqgan}.
Additionally, in order to achieve flexible conditional generation, the denoising UNet backbone~\cite{unet} is usually augmented with the cross-attention and self-attention mechanism~\cite{transformer}.
With paired image-condition training data , the optimization objective of denoising model  is ordinarily simplified as:

where  is the encoder of the compression model,  is the total number of denoising steps, and  is the noisy version of .
During inference, the model samples latent-shaped Gaussian noise and gradually denoises it in accordance with conditions.
At last, the image is yielded by decoding the final latent.

We adopt an open-sourced text-conditional latent diffusion model named Stable Diffusion~\cite{stablediffusion} for this work, which is affordable to infer on consumer GPUs.
 represents text in this case, and the model is trained on  billion image-caption pairs~\cite{LAION5B}.
The language prompt is encoded by CLIP first and is then injected into the model through cross-attention layers.

\subsection{Attention Mechanism}
\label{sec:attention}

Previous works have considered exploiting cross-attention and self-attention layers of text-to-image diffusion models for localization tasks~\cite{p2p, Mix-and-Match}.
However, they often apply these two attention layers separately without fully leveraging their internal correlations.
We instead treat self-attention as the affinity matrix of different patches~\cite{affinitynet}, which conforms more to its essence than regarding it as the clustering feature~\cite{Mix-and-Match}.


Specifically, the text condition  is first projected to the word embedding space as , where  is the length of tokens and  is its dimension.
The spatial visual feature at the intermediate layer  of the backbone is denoted by , where  represents the resolution and  is its dimension.
 interacts with  via a cross-attention mechanism: , with

Here , , and  are learned projection matrices and  is the projection dimension.
Then our patch-token correlation map is built as:


Following a similar calculation process, we build the self-attention map as , with

Here , and  are also learned projection matrices.
Inspired by~\cite{affinitynet, irnet}, we regard the self-attention map as a semantic affinity matrix and propagate the cross-attention scores accordingly.
Thereby we take both unary and pairwise potentials into consideration.
We select a particular layer with a fine resolution  to extract the self-attention map as  and accumulate different  via interpolation and averaging to obtain the aggregated cross-attention map .

We then conduct simple random walk propagation to update the belief of each node in the random field, which can be formulated as:

where  is the number of iterations. 
The obtained  can serve as a substitute for  to complete localization tasks~\cite{p2p}.
As shown in Figure~\ref{fig:outline}, the correlation map attained after propagation preserves the structure better and possesses finer details.


To attain the pseudo mask, we leverage the image-level label to identify the categories contained in the image and use  to represent the set of them.
Regarding class , we extract correlation maps of its relevant tokens from corresponding positions of  and average along the token dimension to obtain the object attribution map .
We reshape it to  and further normalize it so that values at each position are between  and :

We further estimate the attribution map of background given by .
By concatenating these attribution maps, we acquire the final attention map .
After upsampling and refining , we assign the label for each pixel to the class (including the background) with the maximum attention score here.


\subsection{Text Query}
\label{sec:query}

When we want to locate specific objects in the image, we should provide a text query to distill the attention value.
Regarding the weakly-supervised semantic segmentation setting, we merge the category names contained in the image into one sentence.
For instance, if one image involves bottles, chairs, and a sofa, the text prompt will be "a photo including bottle, chair, and sofa.".
This approach has several advantages.
First, it is more time-efficient compared to querying each object separately, as only one forward pass is required.
Second, the attention value of different objects will be comparable due to the normalization operation along the token dimension.
Last, we can conveniently append background prompts at the end of the sentence, which can alleviate the false-activation issue~\cite{clims}.
Moreover, class names in classic datasets may not fully represent the semantics of the category.
Therefore, we conduct prompt engineering to find synonyms with richer semantics and replace original category names with them.

\section{Mug19 Dataset}
\label{sec:dataset}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{pics/dataset_example.pdf}
  \caption{
  Examples in our proposed dataset.
  The first  columns display multi-view photos of personalized items and the -rd column presents the image of different scenes.
  The last column shows the highlighted segmentation result along with the text query.
}
\label{fig:dataset}
\end{figure}

\paragraph{Motivation}

To investigate the problem of personalized segmentation, we create a dataset in a laboratory scenario.
As mentioned in Section Introduction, this new task aims to locate the user-specific instance corresponding to the textual query.
It requires a more refined multi-modal comprehension capability than composed image retrieval~\cite{CIR} and shows potential application perspectives in products like home robotics.
Most existing datasets for similar tasks are designed for retrieval only~\cite{CIR, CIRR, fashioniq}, thus are not suitable for us.
\cite{PALAVRA} created a personalized segmentation benchmark repurposing from a video dataset~\cite{YouTube-VOS}.
But the temporal continuity in the video may have information leakage for localization.
Besides, the depictions were not always useful for segmentation as we found it hard to tell the difference between similar entities in most images of this dataset.

\paragraph{Distractors}

To mitigate these issues, we build a new dataset on which models with good image-text reasoning ability would perform better.
We consider various ambiguities and ensure that the multi-modal references are necessary and sufficient to discriminate different objects, which requires a careful arrangement of scenes.
As shown in the -st row of Figure~\ref{fig:dataset}, personalized knowledge is needed to tell the difference between  mugs filled with Cola, while textual context is required to distinguish between  white mugs.
These two situations are termed "semantic distractors" and "visual distractors" in~\cite{PALAVRA}, respectively.

\paragraph{Statistics}

We choose a typical daily essential "mug" to construct the dataset.
The dataset includes  mugs, and they compose  diverse scenes. 
We provide  to   RGB images for each object. 
Each scene is consisted of  to  mugs and contains   RGB images captured from different angles of view.
We create  segmentation triplets (instances in the scene, descriptions, scene image) based on our data, and we pick out  special groups regarding the degree of ambiguity: semantic distractor and visual distractor.
Scenes in the semantic distractor split include different items with similar contexts and it consists of  triplets.
Scenes in the visual distractor split contain various items with analogous appearances and this group comprises  triplets.
The triplets are carefully annotated by experts and the instance mask labels are acquired by the pre-trained Mask R-CNN~\cite{maskrcnn} model in the Detectron2 library~\cite{detectron2}.
This dataset will be made available online upon acceptance.

\section{Experiments}
\label{sec:experiment}

\begin{table}
  \centering
  \begin{tabular}{lclcc}
    \toprule
    Method  & Backbone   & Ver.    & Val   & Test \\


\midrule
    \multicolumn{5}{l}{\textbf{Image-level supervision only.}} \\
    
    PSA~\shortcite{affinitynet}       & WR38       & V1               & 61.7  & 63.7 \\
    IRN~\shortcite{irnet}       & R50        & V2               & 63.5  & 64.8 \\
    ICD~\shortcite{ICD}       & R101       & V1    & 64.1  & 64.3 \\
    SEAM~\shortcite{SEAM}      & WR38       & V1               & 64.5  & 65.7 \\
    SC-CAM~\shortcite{SC-CAM}    & R101       & V2    & 66.1  & 65.9 \\
    BES~\shortcite{BES}       & R101       & V2    & 65.7  & 66.6 \\
    AdvCAM~\shortcite{AdvCAM}    & R101       & V2               & 68.1  & 68.0 \\
    SIPE~\shortcite{SIPE}      & R101       & V2    & 68.8  & 69.7 \\
    RIB~\shortcite{RIB}       & R101       & V2               & 68.3  & 68.6 \\
    ReCAM~\shortcite{ReCAM}     & R101       & V2               & 68.5  & 68.4 \\
    AMN~\shortcite{AMN}       & R101       & V2    & 70.7  & 70.6 \\
\midrule
    \multicolumn{5}{l}{\textbf{Image-level supervision + Language supervision.}} \\
    
    CLIMS~\shortcite{clims}     & R101       & V2               & 69.3  & 68.7 \\
    CLIMS~\shortcite{clims}     & R101       & V2    & 70.4  & 70.0 \\
    CLIP-ES~\shortcite{CLIP-ES}   & R101  & V2               & 71.1 & 71.4 \\
    CLIP-ES~\shortcite{CLIP-ES}   & R101  & V2    & \textbf{73.8} & 73.9 \\
Ours   & R101  & V2               & \textbf{71.2} & \textbf{71.5} \\
    Ours   & R101  & V2    & 73.3 & \textbf{74.2} \\
    \bottomrule
  \end{tabular}
  \caption{
  DeepLab results on PASCAL VOC 2012 \emph{val} and \emph{test} sets. 
  The best results are in \textbf{bold}. 
  Ver. denotes the version. 
 represents adopting COCO pre-trained models.}
  \label{tab:deeplab}
\end{table}
We evaluate our framework quantitatively and qualitatively on various segmentation tasks.
We further compare with strong baselines to showcase our strengths.
Lastly, ablation studies are conducted to analyze our different designs.

\subsection{Weakly-Supervised Semantic Segmentation}
\label{sec:wsss}

\paragraph{Implementation Details}

We adopt Stable Diffusion for our experiment.
When processing one test image, we first rescale it to meet the model requirements and then encode it through .
We add noise to the latent and input it into the denoising network .
The conditional language prompt  is constructed as described in Section Text Query.
We choose the time step  of the input noisy latent  to be  with the total denoising length .
With one feed-forward computation, we aggregate  from the -th layers of  and distill  from the -th layer of .
We set  in Equaion~\ref{eq:randomwalk} to be .
Following~\cite{clims, CLIP-ES}, the obtained segmentation maps are further refined by dense CRF~\cite{dCRF} to generate pseudo masks, which are then used to train a standard segmentation network based on DeepLab~\cite{DeepLab}.






\paragraph{Evaluation}

We select Pascal VOC 2012~\cite{pascalvoc} and Microsoft COCO 2014~\cite{mscoco} for evaluation. 
Pascal VOC 2012 is a semantic segmentation dataset with  object categories.
It consists of  training images,  augmented training images,  validation images, and  test images.
MS COCO 2014 dataset contains  object categories and  background class.
It has  training images and  validation images.
In our experiment, only image-level labels are used for training.
The mean Intersection over Union (mIoU) value is reported as the evaluation metric.

\paragraph{Quantitative Results}



\begin{table}
  \centering
  \begin{tabular}{lclc}
    \toprule
    Method  & Backbone   & Sup.    & Val \\
    \midrule
    IRN~\shortcite{irnet}       & R50        & I    & 32.6 \\
    IRN~\shortcite{irnet}       & R101       & I    & 41.4 \\
    SIPE~\shortcite{SIPE}      & R101       & I    & 40.6 \\
    RIB~\shortcite{RIB}       & R101       & I    & 43.8 \\
    AMN~\shortcite{AMN}       & R101       & I    & 44.7 \\
    CLIP-ES~\shortcite{CLIP-ES} & R101       & I+L    & 45.4 \\
    Ours                    & R101       & I+L    & \textbf{45.7} \\
    \bottomrule
  \end{tabular}
  \caption{
  DeepLab results on MS COCO 2014 \emph{val} set. 
  The best results are in \textbf{bold}. 
I and L represents image-level supervision and language supervision, respectively.
  }
  \label{tab:COCO_val}
\end{table}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{pics/compare.pdf}
  \caption{
  Visualizations of the pseudo masks generated by various methods.
  The -st column shows the input image and the last column shows the ground truth mask.
  Uncertain pixels are set to white.
  }
  \label{fig:compare}
\end{figure}
We first evaluate the quality of our pseudo masks on Pascal VOC 2012.
To train the segmentation model, we generate pseudo masks on the \emph{trainaug} set.
Using these masks, we train segmentation models based on DeepLabV2 and assess them on the \emph{val} and \emph{test} sets.
As shown in Table~\ref{tab:deeplab}, our framework outperforms most previous methods and we achieve a new state-of-the-art with the Imagenet pre-trained model.
As for MS COCO 2014, after training a DeepLabV2 segmentation model with our pseudo masks of \emph{train} images, we can achieve  mIoU on the \emph{val} set as shown in Table~\ref{tab:COCO_val}, which is also a new SOTA.








\paragraph{Qualitative Results}

We visualize the results of our approach and other multi-modal related methods~\cite{clims, CLIP-ES} in Figure~\ref{fig:compare}.
We produce pseudo masks with finer structures, such as the feather of the bird and the legs of the horse as shown in the image.
Furthermore, we can directly generate the confidence map from attention scores.
If the value of a position is within the  interval of the interface, we set it as uncertain.
We find that ambiguous pixels are mainly concentrated on object boundaries.


\subsection{Ablation Study}
\label{sec:ablation}





\paragraph{Attention}
We verify the effectiveness of our attention mechanism based on the quality of pseudo masks.
As shown in Table~\ref{tab:attention},  achieves remarkably better mIoU than , and a qualitative comparison has been displayed in Figure~\ref{fig:outline}.
A further ensemble of  and  shows no benefits.
We also compare to Controllable Background Preservation (CBP) from~\cite{Mix-and-Match}, which clusters the self-attention map and assigns labels to each segment based on the cross-attention value.
We achieve better results as our approach conforms more to the essence of these two attention maps.
\begin{table}
  \centering
\setlength\tabcolsep{2pt}
  \begin{tabular}{ccccc}
    \toprule
    Attention  & Cross & CBP~\shortcite{Mix-and-Match} & SelfCross & Cross+SelfCross \\
    \midrule
    Initial & 61.7 & 64.1 & \textbf{72.7} & 69.9 \\
    dCRF & 67.1 & 69.9 & \textbf{74.4} & 73.0 \\
    \bottomrule
  \end{tabular}
  \caption{Attention mechanism analysis on PASCAL VOC 2012 \emph{train} set. The best results are in \textbf{bold}.}
  \label{tab:attention}
\end{table}
\begin{table}
  \centering
  \begin{tabular}{ccc|ccc|c}
    \hline
    Comp. & Syn. & BG. & bird & person & train & mIoU\\
    \hline
      &   &   & 85.6 & 38.4 & 74.1 & 72.1 \\
      & \checkmark &   & \textbf{87.1} & 59.3 & 73.5 & 73.1 \\
      &   & \checkmark & 80.3 & 36.1 & \textbf{81.9} & 72.7 \\
      & \checkmark & \checkmark & 83.9 & \textbf{63.5} & 81.5 & 74.3 \\
    \cline{1-6}
    \checkmark &   &   & 85.3 & 45.2 & 75.2 & 73.2 \\
    \checkmark & \checkmark &   & \textbf{87.0} & 56.9 & 74.7 & 73.8 \\
    \checkmark &   & \checkmark & 79.4 & 41.0 & 82.2 & 73.1 \\
    \checkmark & \checkmark & \checkmark & 85.0 & \textbf{59.7} & \textbf{82.5} & \textbf{74.4} \\
    \hline
  \end{tabular}
  \caption{
  Prompt strategy comparison on PASCAL VOC 2012 \emph{train} set.
  Comp. denotes using one composed sentence, Syn. denotes adopting category synonyms, and BG. denotes appending background prompts.
  The best results are in \textbf{bold}.}
  \label{tab:prompt}
\end{table}


\paragraph{Prompt}
We analyze different strategies for selecting the text query and depict the IoU of some categories along with the overall average in Table~\ref{tab:prompt}.
Compared to using one composed sentence as the prompt, applying separate texts to query multi-label scenes will decline segmentation precision and increase inference time (the number of pseudo labels generated per second decreases from  to  in practice).
Then following~\cite{CLIP-ES}, we substitute the original class name with category synonyms that can reduce ambiguity.
For instance, the high-attention region corresponding to "person" tends to overlook the body, and adding "clothes" to the query prompt of "person" can enhance the performance.
Furthermore, we append some background prompts at the end of text queries.
With the existing  operation in Equation~\ref{eq:softmax}, category scores in co-occurring background regions~\cite{clims} will be naturally suppressed.
For example, when "railway" and "track" are used as the background prompts for "train", the segmentation results will be improved due to the exclusion of these background areas.
\begin{table*}
  \centering
  \begin{tabular}{l|ccc|ccc|ccc}
    \hline
    Split  & \multicolumn{3}{c|}{All} & \multicolumn{3}{c|}{Semantic Distractor} & \multicolumn{3}{c}{Visual Distractor} \\
    \hline
    Metric  & mIoU & bf\_acc & af\_acc & mIoU & bf\_acc & af\_acc & mIoU & bf\_acc & af\_acc  \\
    \hline
    Mask R-CNN~\cite{maskrcnn}       & \textbf{75.1} & 58.9 & 75.1 & \underline{78.2} & 63.3 & 78.2 & 42.9 & 38.0 & 42.9 \\
    DINO-ViT~\cite{dino}       & 52.7 & 42.3 & 72.8 & 70.7 & 43.3 & 93.2 & 39.9 & 33.7 & 52.0 \\
    Subject Only       & 62.3 & \underline{60.1} & \underline{79.5} & 76.2 & \textbf{79.3} & \underline{95.0} & 31.9 & 42.0 & 40.0 \\
Context Only      & 54.4 & 51.2 & 70.9 & 35.3 & 40.5 & 45.1 & 50.3 & 42.4 & 64.3 \\
    Arithmetic & 28.5 & 35.1 & 37.2 & 27.9 & 37.7 & 35.2 & \underline{55.2} & \underline{46.7} & \underline{70.4} \\
    Ours & \underline{64.9} & \textbf{60.2} & \textbf{83.3} & \textbf{78.8} & \underline{73.5} & \textbf{98.3} & \textbf{56.8} & \textbf{49.4} & \textbf{71.8} \\
    \hline
  \end{tabular}
  \caption{Evaluation results on different splits of Mug19 dataset. The best results are in \textbf{bold} while the second best are \underline{underlined}.}
  \label{tab:personalized}
\end{table*}
\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{pics/case_study.pdf}
  \caption{
  Localization results of different methods on Mug19 dataset samples.
  The -st column shows the object, the last column shows the scene and the rest columns display highlighted segmentation masks with the text reference.
  }
  \label{fig:case}
\end{figure*}

\subsection{Personalized Referring Image Segmentation}
\label{sec:personalized}

\paragraph{Implementation Details}

We first leverage Custom Diffusion~\cite{customdiffusion} to acquire the text embedding of the personalized item.
Given target object images, we optimize the token embedding of the identifier word "new1" along with the diffusion model.
Next, we replace "class" in the textual context with "new1 class" as the condition text prompt, and then attain the object attribution map from  using the same hyper-parameters as in weakly-supervised semantic segmentation (WSSS) experiments.
To focus on the foreground region, we query the diffusion model with "a photo including class." first to get the object area in the scene.
Then we compare the attribution map of different instances to determine the segmentation results in the object area.
We have found it hard to gain a complete silhouette with the above process.
Therefore, we add a clustering process in advance, so that we only have to focus on assigning regions instead of pixels.
We employ a simple version of spectral clustering~\cite{spectral} which performs surprisingly well in separating each instance.
Later, we average the attention value in different segments and finally assign the instance to the area with the highest correlation.


\paragraph{Evaluation}

We use standard mIoU along with two kinds of accuracy for evaluation.
Accuracy represents the proportion of correctly predicted instances to all instances.
When we consider each object solely, the segment most relevant to each object is predicted as its location.
 denotes the accuracy under this protocol.
When we consider all the items contained in the scene together, we treat the correlation between items and segments as the cost matrix of an assignment problem.
Then Hungarian algorithm~\cite{hungarian} is adopted to attain the assignment between instances and segments. 
 denotes the accuracy under this protocol.


\paragraph{Baselines}

We compare our method with several strong baselines. 
First, we use the feature of Mask R-CNN~\cite{maskrcnn} to calculate the similarity between queries and instances in the scene.
It is worth noting that we have used the same model to provide instance masks and we aim to evaluate the ability of this localization model to discriminate similar objects.
We also adopt a self-supervised ViT model (DINO-ViT) as dense visual descriptors following~\cite{dino}.
Next, we simply query the image with "a new1 class" and we call this baseline "Subject Only".
On the contrary, the "Context Only" baseline stands for not incorporating the identifier embedding in the textual prompt.
These approaches only use uni-modal information.
Then the "Arithmetic" baseline combines subject and context by replacing the category embedding in the description with the average between CLIP image embeddings of the instance and the CLIP text embedding of the class.

\paragraph{Quantitative Results}

As shown in Table~\ref{tab:personalized}, our method achieves the best results on the entire data.
Mask R-CNN obtains good mIoU because it knows the exact position of all mugs in advance, but it performs poorly in distinguishing between various mugs.
It is also found that the subject information is beneficial for excluding semantic distractors while the context information is useful for dealing with visual distractors, which is in line with our definition of these two groups.
Besides, the average operation can not fully leverage the information from different modalities, as the "Arithmetic" baseline behaves similarly to "Context Only" on various splits.
We can also conclude that multi-modal knowledge is vital for this task, as uni-modal methods have difficulties in handling distinct distractors.












\paragraph{Case Study}

We select several samples to showcase the deviation of different approaches.
As shown in Figure~\ref{fig:case}, the "Subject Only" baseline has difficulties in distinguishing visual distractors while the "Context Only" baseline is hard to discriminate semantic distractors.
More specifically, the former one finds another mug with a similar appearance in the -st row, and the latter baseline detects another "green" mug in the -nd row.
We also compare our method with the recent SEEM~\cite{SEEM}.
We use its official demo and simultaneously choose example and text interactive mode.
The result of the -nd case indicates that it sometimes neglects textual information.
Compared to these methods, our approach leverages multi-modal information more efficiently.

\section{Limitation}
\label{sec:limitation}

We have found that the segmentation results will drop drastically when an image contains semantically similar objects as shown in Figure~\ref{fig:cohyponym}.
The same phenomenon has also been found in~\cite{daam} and was termed "Cohyponym Entanglement".
Besides, when we query the model with affordance-related texts, the obtained attention value is often unsatisfactory.
One possible solution is to annotate images with functional descriptions and use these paired data to fine-tune the diffusion model.
We expect that this will lead to a diffusion model equipped with affordance ability.
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{pics/cohyponym.pdf}
  \caption{
Failure case: when "cat" and "dog" are in the same image, it is difficult for our method to distinguish them.
}
  \label{fig:cohyponym}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}

In this work, we introduce a simple but effective approach to distill the attention value in text-to-image diffusion models for segmentation.
Without re-training nor inference-time optimization, text-related regions can be located precisely.
We first validate the effectiveness of our framework on weakly-supervised semantic segmentation tasks.
To this end, we propose a novel way to generate query prompts in accordance with image-level labels.
Our framework achieves state-of-the-art performance on PASCAL VOC 2012 and MS COCO 2014 and the validity of our designs is confirmed.
To further verify the role of word-pixel correlation, we introduce a new practical task named "personalized referring image segmentation" with a new real-world dataset.
Experiments on this task demonstrate that our method possesses a better multi-modal comprehension ability than several strong baselines.

\appendix

\section{Experiment Details}

\paragraph{Datasets and Pre-trained Models}

The licenses of the datasets and the pre-trained models are listed here.
The PASCAL VOC 2012 dataset~\cite{pascalvoc} is from~\url{http://host.robots.ox.ac.uk/pascal/VOC/index.html}.
The MS COCO 2014 dataset~\cite{mscoco} is from~\url{https://cocodataset.org/#home}.
The pre-trained CLIP~\cite{clip} is from~\url{https://github.com/openai/CLIP}.
The Stable Diffusion v model~\cite{stablediffusion} is from~\url{https://huggingface.co/CompVis/stable-diffusion}.
The DeepLab pre-trained model~\cite{DeepLab} is from~\url{https://github.com/kazuto1011/deeplab-pytorch}.
The Detectron2 pre-trained model is from~\url{https://github.com/facebookresearch/detectron2}, and we select mask\_rcnn\_R\_50\_FPN\_3x from official baseline models for its good performance.

\paragraph{Implementation Details}

We use Pytorch~\cite{pytorch} for our experiments. 
Our codebase builds heavily on~\url{https://github.com/CompVis/stable-diffusion}.
For Pascal VOC 2012, MS COCO 2014, and Mug19 datasets, we sample , , and  times respectively to generate pseudo masks.

\paragraph{Prompts}

We adopt the category synonyms in~\cite{CLIP-ES} except for "person", as we find "person with clothes" works better than "person with clothes, people, human".
We use "tree, river, sea, lake, water, railway, railroad, track, stone, rocks" as the background prompts following~\cite{clims}.
We find the background prompts in~\cite{CLIP-ES} work slightly worse.
We attribute it to more background classes, which leads to more complex query sentences as we append background prompts to the end of texts.

\paragraph{Training Details of DeepLab}

We refer to the config file in \url{https://github.com/kazuto1011/deeplab-pytorch} and \url{https://github.com/CVI-SZU/CLIMS}.
We adopt DeepLabV2 and use ResNet-101 as the backbone.

\section{More Experiment Results}

\subsection{Weakly-Supervised Semantic Segmentation}

\paragraph{Computation Consumption}

Our method requires no re-training and has M frozen parameters in total.
It takes an average of  seconds to infer an image with a single NVIDIA GeForce RTX 3090 GPU and  GB of memory.

\paragraph{Quantitative Results}

We supplement the evaluation results of the pseudo masks here.
As for Pascal VOC 2012 \emph{train} set, our method outperforms previous methods by a considerable margin on initial seeds as shown in Table~\ref{tab:pseudo_label}.
Without training the extra affinity network, we achieve  mIoU after dCRF post-processing.
We obtain the best quality on the \emph{trainaug} set too, and even our initial masks can exceed the results of previous methods post-processing with dCRF or trained affinity networks as shown in Table~\ref{tab:train_aug}.
Regarding MS COCO 2014 \emph{train} set, our method can also produce more precise masks as shown in Table~\ref{tab:COCO_train}.
\begin{table}[t]
  \centering
  \begin{tabular}{lccc}
    \toprule
    Method  & Initial   &  dCRF & RW\\
    \midrule
    IRN~\shortcite{irnet}       & 48.8        & 54.3  &  66.3\\
    SC-CAM~\shortcite{SC-CAM}  & 50.9        & 55.3  & 63.4\\
    SEAM~\shortcite{SEAM}  & 55.4       & 56.8  &  63.6\\
    AdvCAM~\shortcite{AdvCAM}  & 55.6       & 62.1 & 68.0 \\
    CLIMS~\shortcite{clims} & 56.6 & 62.4 & 70.5 \\
    RIB~\shortcite{RIB} & 56.5 & 62.9 & 70.6 \\
    OoD~\shortcite{OoD} & 59.1  &  65.5 & 72.1 \\
    MCTfomer~\shortcite{MCTfomer} & 61.7 & 64.5 & 69.1 \\
    CLIP-ES~\shortcite{CLIP-ES} & 70.8   &  75.0 & - \\
    Ours (sample 1 time)         & 72.7 &  74.4  & - \\
    Ours (sample 10 times)       & \textbf{74.2}   &  \textbf{76.1} & - \\
    \bottomrule
  \end{tabular}
  \caption{mIoU of pseudo masks on PASCAL VOC 2012 \emph{train} set. The best results are in \textbf{bold}. dCRF represents post-processing with dense CRF. RW denotes refining with trained affinity networks.}
  \label{tab:pseudo_label}
\end{table}
\begin{table}
  \centering
  \begin{tabular}{lcc}
    \toprule
    Method  & Initial   &  Pseudo\\
    \midrule
    CLIMS~\cite{clims} & 55.8 & 67.8 \\
    CLIP-ES~\cite{CLIP-ES} & 65.9   &  68.7 \\
Ours    & \textbf{70.3}   &  \textbf{71.7}  \\
    \bottomrule
  \end{tabular}
  \caption{mIoU of pseudo masks on PASCAL VOC 2012 \emph{trainaug} set. The best results are in \textbf{bold}.}
  \label{tab:train_aug}
\end{table}
\begin{table}
  \centering
  \begin{tabular}{lcc}
    \toprule
    Method  & Initial   &  Pseudo\\
    \midrule
    CLIP-ES~\cite{CLIP-ES} & 39.7   &  41.5 \\
Ours    & \textbf{43.7}   &  \textbf{45.3}  \\
    \bottomrule
  \end{tabular}
  \caption{mIoU of pseudo masks on MS COCO 2014 \emph{train} set. The best results are in \textbf{bold}.}
  \label{tab:COCO_train}
\end{table}

\paragraph{Qualitative Results}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{pics/qualitative.pdf}
  \caption{
  More visualization results on PASCAL VOC 2012 and MS COCO 2014 datasets.
}
  \label{fig:qualitative}
\end{figure*}
We show more qualitative comparisons of our generated pseudo masks in Figure~\ref{fig:qualitative}.
We can observe that our framework accurately locates objects of different sizes in both simple and complex scenarios.

\paragraph{Hyper-parameters}
\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{pics/ablation.pdf}
  \caption{
  Analyses of different hyper-parameters.
  The mIoU values are evaluated on PASCAL VOC 2012 \emph{train} set.
  The default setting is  time step, --th cross layer, -th self layer,  background threshold with power , and sampling  time.
  }
  \label{fig:ablation}
\end{figure*}
\begin{table*}
  \centering
  \begin{tabular}{l|ccc|ccc|ccc}
    \hline
    Split  & \multicolumn{3}{c|}{All} & \multicolumn{3}{c|}{Semantic Distractor} & \multicolumn{3}{c}{Visual Distractor} \\
    \hline
    Metric  & mIoU & bf\_acc & af\_acc & mIoU & bf\_acc & af\_acc & mIoU & bf\_acc & af\_acc  \\
    \hline
    PALAVRA~\shortcite{PALAVRA} + CLIP-ES~\shortcite{CLIP-ES}      & 26.0 & 44.5 & 31.5 & 2.5 & 33.0 & 3.0 & 11.5 & 38.8 & 14.1 \\
Custom~\shortcite{customdiffusion} + CLIP-ES~\shortcite{CLIP-ES}      & 36.1 & 37.6 & 43.0 & 33.4 & 37.1 & 39.6 & 53.4 & 42.0 & 63.5 \\
PALAVRA~\shortcite{PALAVRA} + SelfCross      & \underline{53.1} & \underline{48.2} & \underline{69.4} & \underline{53.6} & \underline{46.3} & \underline{68.3} & \textbf{58.2} & \textbf{50.8} & \textbf{74.5} \\
Custom~\shortcite{customdiffusion} + SelfCross & \textbf{64.9} & \textbf{60.2} & \textbf{83.3} & \textbf{78.8} & \textbf{73.5} & \textbf{98.3} & \underline{56.8} & \underline{49.4} & \underline{71.8} \\
    \hline
  \end{tabular}
  \caption{
  Evaluation results of various combinations on Mug19 dataset. 
  The best results are in \textbf{bold} while the second best results are \underline{underlined}.
  Custom + SelfCross is the combination we ultimately adopt.
  }
  \label{tab:compose}
\end{table*}
We analyze the impact of different hyper-parameters and compare the quality of generated pseudo masks in Figure~\ref{fig:ablation}.
The time step  of the input noisy latent  has little effect on the result.
But the performance is better when employing coarse inner layers for  layers, where the text condition is more relevant to the layout according to a recent study~\cite{differentlayer}.
On the contrary, higher  layers boost the performance more, since deeper layers tend to have a better understanding of appearance similarity.
As for determining background regions, we have found a constant threshold less effective.
We follow~\cite{CLIP-ES} with a minus-max-power mechanism where the background score per pixel is determined by threshold  and the imum category attention value here.
We compare different power exponents and square is the best.
Lastly, due to the stochastic sampling process in the diffusion framework, we can easily ensemble segmentation masks generated from different noises.
But the trade-off between time and performance should be considered.
We sample once for ablation studies to save time, but for better performance, we sample more times to generate pseudo masks.

\paragraph{Attention}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{pics/propagation.pdf}
  \caption{
  Visualization of correlation maps. 
  The texts on the left are the corresponding categories. 
  The -nd column shows the cross-attention map, the -rd column displays the attention score attained after employing the clustering technique in CBP~\shortcite{Mix-and-Match}, and the last column shows our final correlation map after propagation. 
Best viewed in color.}
\label{fig:prop}
\end{figure}
More qualitative comparisons of different attention mechanisms have been shown in Figure~\ref{fig:prop}.
 conserves the structure better and possesses finer details.

\subsection{Personalized Referring Image Segmentation}

\paragraph{Computation Consumption}

The training process of Custom Diffusion takes around  minutes on  A100 GPUs for each instance, and the inference cost is similar to that in Weakly-Supervised Semantic Segmentation.

\paragraph{Combinations of Personalized and Localized Methods}

We conduct a study concerning distinct combinations of personalized and localized approaches.
We adopt discriminative model-based methods: PALAVRA~\cite{PALAVRA} for personalization, CLIP-ES~\cite{CLIP-ES} for segmentation, and generative model-based methods: Custom Diffusion~\cite{customdiffusion} for personalization, our framework for segmentation.
As shown in Table~\ref{tab:compose}, CLIP-ES has difficulties dealing with personalized items.
We attribute it to the instability of Grad-CAM~\cite{gradcam} as we occasionally get all zero CAMs for specific objects.
In contrast, our mechanism is robust to various personalized embeddings and we ultimately choose Custom Diffusion because it performs better and is more consistent with our method.

\paragraph{Objects in the Same Series}

We further construct a new split from Mug19 dataset regarding series.
We build a scenario where we want to locate the object with only access to images of its variants within the same series. 
For instance, we aim to find the red transparent plastic mug in the scene, but we only possess photos of its blue variant.
We handle it by using the identifier embedding of the blue variant along with the context "a red mug" as the segmentation query.
We pick out  triplets based on this notion and name them "variant split" as a whole.
Quantitative comparisons of various baselines are conducted on this split in Table~\ref{tab:variant}.
Our method fully utilizes both subject and context information and addresses the localization issue of this situation more effectively than all the baselines.
\begin{table}
  \centering
  \setlength\tabcolsep{4.5pt}
  \begin{tabular}{l|ccc}
    \hline
    Split  & \multicolumn{3}{c}{Variant} \\
    \hline
    Metric  & mIoU & bf\_acc & af\_acc \\
    \hline
    Mask R-CNN~\cite{maskrcnn}       & \textbf{71.9} & \underline{59.5} & 71.9 \\
    DINO-ViT~\cite{dino}       & 48.5 & 44.7 & 66.7 \\
    Subject Only       & 61.6 & 54.9 & \underline{78.8} \\
Context Only      & 54.7 & 52.6 & 71.1 \\
    Arithmetic & 36.5 & 37.3 & 47.1 \\
    Ours & \underline{69.2} & \textbf{60.7} & \textbf{88.3} \\
\hline
  \end{tabular}
  \caption{Evaluation results on the variant split of Mug19 dataset. The best results are in \textbf{bold} while the second best results are \underline{underlined}.}
  \label{tab:variant}
\end{table}

\section{Comparisons with SAM}
\label{sec:sam}

Drawing inspiration from Large Language Models, foundation models for segmentation have been developed recently~\cite{SAM, SEEM}.
They can solve universal segmentation tasks with prompts of various modalities.
However, this series of methods need dense annotations while our framework only requires image-level labels.

\section{More Discussion on Limitations}
\label{sec:dataset limitation}

We discuss the limitations of the proposed task here.
We care more about the appearance of different personalized entities, and our descriptions currently focus more on what they look like.
As CLIP, the text encoder of Stable Diffusion, struggles with understanding spatial relations~\cite{composerelation}, we doubt our framework's ability on them and have not yet considered them in our dataset.
In future work, we will add this type of description to our dataset and consider how to enhance our method's ability to comprehend them.

\section{Broader Impacts}
\label{sec:impacts}

The booming generative models have caused many concerns.
The unauthentic content they create can be misused.
We instead pay attention to the discriminative task and show the potential of these generative models from another aspect.
Our method can further help researchers to understand the generation process in large models, which will also benefit the detection of generated content.
On the other hand, as we propose a more flexible segmentation technology, surveillance of specific identities becomes easier, which may be abused to infringe on personal privacy.





\bibliography{aaai24}

\end{document}
