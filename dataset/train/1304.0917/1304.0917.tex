\documentclass[10pt]{llncs}
\usepackage[dvips]{graphicx}
\usepackage{latexsym,tabularx}
\usepackage{amsmath,amssymb}
\usepackage{url}


\newtheorem{fact}{{\em Fact}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{document}

\title{
A Succinct Grammar Compression\thanks{
This study was supported by KAKENHI(23680016,24700140) and JST PRESTO program.
}
}

\author{
Yasuo Tabei\inst{1}, 
Yoshimasa Takabatake\inst{2}, and
Hiroshi Sakamoto\inst{2,3}
}
\institute{
ERATO Minato Project, JST, Japan
\and
Kyushu Institute of Technology, Japan
\and PRESTO, JST, Japan\\
\email{
tabei.y.aa@m.titech.ac.jp,
\{takabatake, hiroshi\}@donald.ai.kyutech.ac.jp}
}

\maketitle

\begin{abstract}
We solve an open problem related to an optimal encoding of 
a {\em straight line program} (SLP), a canonical form of grammar compression
deriving a single string deterministically.
We show that an information-theoretic lower bound for representing an SLP with  symbols 
requires at least  bits. 
We then present a succinct representation of an SLP; this representation is asymptotically equivalent to the lower bound.
The space is at most  bits for , while supporting random access to any production rule of 
an SLP in  time.
In addition, we present a novel dynamic data structure associating a digram with a unique symbol.
Such a data structure is called a {\em naming function} and has been implemented using a hash table that has a space-time tradeoff.
Thus, the memory space is mainly occupied by the hash table during the development of production rules.
Alternatively, we build a dynamic data structure for the naming function by leveraging the idea behind the {\em wavelet tree}.
The space is strictly bounded by  bits, while supporting  query and update time.
\end{abstract}

\section{Introduction}
Grammar compression has been an active research area since at least the seventies.
The problem consists of two phases: 
(i) building the smallest\footnote{This is almost equal to minimizing the number of variables in .} 
context-free grammar (CFG) generating an input string uniquely 
and (ii) encoding an obtained CFG as compactly as possible. 

The phase (i) is known as an NP-hard problem which can not be approximated 
within a constant factor~\cite{Lehman-Shelat02}.
Therefore, many researchers have made considerable efforts to design grammar compressions 
achieving better approximation results in the last decade.
Charikar et al.~\cite{Charikar05} and Rytter~\cite{Rytter03} independently 
proposed the first -approximation algorithms based on balanced 
grammar construction for the length  of a string and the size  of the smallest CFG. 
Later, Sakamoto~\cite{Sakamoto05} also developed an -approximation algorithm 
based on an idea called pairwise comparison. 
In particular, Lehman~\cite{Lehman-Shelat02} proved that LZ77~\cite{LZ77} 
achieved the best approximation of  under the condition of an unlimited window size.
Since the minimum {\em addition chain} problem is a special case of the problem 
of finding the smallest CFG~\cite{LehmanPhD}, modifying the approximation algorithms proposed so far is a difficult problem. 
Thus, the problem of grammar compression is pressing in the phase (ii). 

A straight line program (SLP) is a canonical form of a CFG, 
and has been used in many grammar compression algorithms~\cite{LZ78,Larsson00,LZ77,Apostolico00,ESP}.
The production rules in SLPs are in Chomsky normal form 
where the right hand side of a production rule in CFGs is a {\em digram}: a pair of symbols.
Thus, if  symbols are stored in an array called a {\em phrase dictionary} 
consisting of  fixed-length codes each of which is represented by  bits, 
the memory of the dictionary is  bits, resulting in the memory for storing an input string usually being exceeded.
Although directly addressable codes achieving entropy bounds on strings whose memory consumption is the same as that of the fixed-length codes in the worst case have been presented~\cite{ferragina2007simple,sadakane2006squeezing,gonzalez2006statistical}, 
there are no codes that achieve an information-theoretic lower bound of storing an SLP in a phrase dictionary.
Since a nontrivial information-theoretic lower bound of directly addressable codes for a phrase dictionary remains unknown, 
establishing the lower bound and developing novel codes for optimally representing an SLP are challenges.


We present an optimal and directly addressable SLP within a strictly bounded memory close to 
the amount of a plain representation of the phrase dictionary.
We first give an information-theoretic lower bound on the problem of encoding an SLP, 
which has been unknown thus far. 
Let  be a class of objects.
Representing an object  requires at least  bits.
A representation of  is succinct if it requires at most  bits.
Considering the facts and the characteristics of SLPs indicated in \cite{ESP}, 
one can predict that the lower bound for the class of SLPs with  symbols 
would be between  and . 
By leveraging this prediction, we derive that a lower bound of bits to represent SLPs is .

We then present an almost optimal encoding of SLPs based on
{\em monotonic subsequence decomposition} of a sequence.
Any permutation of  is decomposable into at most 
monotonic subsequences in  time~\cite{Yehuda1998} and
there is a -approximation\footnote{Minimizing  is NP-hard.} 
algorithm in  time~\cite{Fomin2002}. 
While the previous encoding method for SLPs presented in~\cite{Takabatake2012} 
is also based on the decomposition, the size is not asymptotically equal
to the lower bound when .
We improve the data structure by using the {\em wavelet tree} (WT)~\cite{Grossi03} and its improved results~\cite{Jeremy2010,Golynski2006} such that
our novel data structure achieves the smaller bound of  
bits for any SLP with  symbols while supporting  access time.
Our method is applicable to any types of algorithm generating SLPs
including Re-Pair~\cite{Larsson00} and an online algorithm called LCA~\cite{Maruyama2012}.
Barbay et al.~\cite{Barbay2009} presented a succinct representation of a sequence using the monotonic subsequence decomposition. 
Their method uses the representation of an integer function built on a succinct representation of integer ranges. 
Its size is estimated to be the {\em degree entropy} of an ordered tree~\cite{Jansson2012-JCSS}.

Another contribution of this paper is to present a dynamic data structure for checking whether or not a production rule in a CFG has been generated in execution. 
Such a data structure is called a {\em naming function}, and is also necessary for practical grammar compressions. 
When the set of symbols is static, we can construct a perfect hash as a naming function in linear time, which achieves 
an amount of space within around a factor of 2 from the information-theoretical minimum~\cite{Botelho2007}.
However, variables of SLPs are generated step by step in grammar compression.
While the function can be dynamically computed by a randomization~\cite{Karp87} or
a deterministic solution~\cite{Karp72} in  time and linear space,
a hidden constant in the required space was not clear.
We present a dynamic data structure to compute function values in  query time
and update time. The space is strictly bounded by  bits.

\section{Preliminaries}

\subsection{Grammar compression}
For a finite set ,  denotes its cardinality.
{\em Alphabet}  is a finite set of letters and  is a constant. 
 is a recursively enumerable set 
of {\em variables} with .
A sequence of symbols from  is called a string.
The set of all possible strings from  is denoted by .
For a string , the expressions , , and  
denote the length of , the -th symbol of ,
and the substring of  from  to , respectively.
Let  be the set of symbols composing .
A string of length two is called a {\em digram}.

A CFG is represented by  
where  is a finite subset of ,  is a finite subset of 
, and .
A member of  is called a production rule and  is called the start symbol.
The set of strings in  derived from  by  is 
denoted by .

A CFG  is called {\em admissible} if exactly one  exists and .
An admissible  deriving  is called a grammar compression of  for any .

We consider only the case  for any production rule 
because any grammar compression with  variables can be transformed into 
such a restricted CFG with at most  variables.
Moreover, this restriction is useful for practical applications of compression algorithms, e.g.,
LZ78~\cite{LZ78}, REPAIR~\cite{Larsson00}, and LCA~\cite{Maruyama2012},
and indices, e.g., SLP~\cite{Claude09} and ESP~\cite{Maruyama2011}.


The derivation tree of  is represented by a rooted ordered binary tree
such that internal nodes are labeled by variables in  and
the {\em yields}, i.e., the sequence of labels of leaves is equal to .
In this tree, any internal node  has 
a left child labeled  and a right child labeled , which 
corresponds to the production rule .

If a CFG is obtained from any other CFG by a permutation 
, they are identical to each other
because the string derived from one is transformed to 
that from the other by the renaming.
For example,  and
 are identical each other.
On the other hand, they are clearly different from 
 because their depths are different.
Thus, we assume the following canonical form of CFG called 
{\em straight line program} (SLP).

\begin{definition}\label{SLP}(Karpinsk-Rytter-Shinohara~\cite{SLP})
An SLP is a grammar compression over  
whose production rules are formed by either 
 or , where  and .
\end{definition}


\subsection{Phrase/reverse dictionary}
For a set  of production rules,
a {\em phrase dictionary}  is a data structure for directly accessing
 the phrase  for any  if .
Regarding a triple  of positive integers as , 
we can store the phrase dictionary consisting of  variables in an integer array , 
where  if  belongs to an alphabet i.e., . 
 and  are accessible as  and  by indices  and  for , 
respectively. 
A plain representation of  using fixed-length codes requires  bits of space to store 
 production rules.

Reverse dictionary  is a data structure for directly accessing the variable  
given  for a production rule . 
Thus,  returns  if .
A hash table is a representative data structure for  enabling  time access 
and achieving  bits of space. 

\subsection{Rank/select dictionary}
We present a phrase dictionary based on the {\em rank/select dictionary}, 
a data structure for a bit string ~\cite{Jacobson89} supporting the following queries:
 returns the number of occurrences of  in  and 
 returns the position of the -th occurrence of  in .
For example, if  is given, then  because the number of s in 
 is ,
and  because the position of the fifth  in  is .
Although naive approaches require the  time to compute a rank, 
several data structures with only the  bit storage to 
achieve  time~\cite{Navarro12,Okanohara07} have been presented. 
Most methods compute a select query by a binary search on a bit string  in  time. 
A data structure for computing the select query in  time has also been presented~\cite{Ram02}.

\subsection{Wavelet tree}
A WT is a data structure for a string , and 
it can be used to compute the rank and select queries on a string  over an ordinal alphabet 
in  time and  bits~\cite{Grossi03}.
Data structures supporting the rank and select queries in  time 
with the same space have been proposed~\cite{Golynski2006,Jeremy2010}. 
WT also supports  which returns  in  time.
Recently, WT has been extended to support various operations on strings~\cite{Navarro2012-cpm}.

A WT for a sequence  over  is a binary tree that 
can be, recursively, presented over a sub-alphabet range . 
Let  be a sequence represented in a node , and let 
 and  be left and right children of node , respectively. 
The root  represents  over the alphabet range .
At each node ,  is split into two subsequences  consisting of 
the sub-alphabet range  for  
and  consisting of the sub-alphabet range  
for  where  and  keep the order of elements in . 
The splitting process repeats until . 
Each node  in the binary tree contains a rank/select dictionary on a bit string . 
Bit  indicates whether  should be moved to  or . 
If ,  contains . 
If ,  inherits . 
Formally,  with an alphabet range  is defined as:


An example of a WT is shown in Figure~\ref{fig:wavelet_tree_ex}.
In this example, since  belongs to the higher half  of an alphabet range  represented in the root; therefore, it is the second element of  that must go to the right child of the root,  and .

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.4\textwidth]{./fig/wavelet_ex}
\end{center}
\vspace{-0.5cm}
\caption{Example of wavelet tree for a sequence  over an alphabet .}
\label{fig:wavelet_tree_ex}
\end{figure*}





\section{Succinct SLP}

\subsection{Information-theoretic lower bound}
In this section, we present a tight lower bound to represent SLPs
having a set of production rules  consisting of  symbols.
Each production rule  is considered as two directed edges  and , 
the SLP can be seen as a directed acyclic graph (DAG) with a single source and  sinks.
Here, we consider  as the left edge and  as the right edge.
In addition,  can be considered as a DAG with the single source and 
with a single sink 
by introducing a super-sink  and drawing directed left and right edges from any sink to ~(Figure~\ref{fig:dag}). 
Let  be the set of all possible s with  nodes and 
. 
Since two SLPs are identical if an SLP can be converted to the other SLP by 
a permutation ,
the number of different SLPs is .
Any internal node of  has exactly two (left/right) edges.
Thus, the following fact remarked in~\cite{Maruyama2011} is true.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.9\textwidth]{./fig/dag2}
\end{center}
\vspace{-0.7cm}
\caption{Example of DAG representation of an SLP and its spanning tree decomposition. 
An SLP is represented by a DAG .  is decomposed into the left tree  and right tree .}
\label{fig:dag}
\end{figure*}

\begin{fact}
An in-branching spanning tree is an ordered tree such that the out-degree of any node 
except the root is exactly one. 
For any in-branching spanning tree of , the graph consisting of the
remaining edges and their adjacent vertices is also an in-branching spanning tree of .
\end{fact}

The in-branching spanning tree consisting of the left edges (respectively the right edges) and their adjacent vertices is called the
{\em left tree}  (respectively {\em right tree} ) of .
Note that the source in  is a leaf of both  and , and
the super-sink of  is the root of both  and .
We shall call the operation of decomposing a DAG  into two spanning trees  and  
{\em spanning tree decomposition}. 
In Figure~\ref{fig:dag}, the source  in  is a leaf of both  and , and 
the super-sink  in  is the root of both  and .

Any ordered tree is an elements in 
where  is the set of all possible ordered trees with  nodes.
As shown in~\cite{Asai2002,Zaki2002}, 
there exists an enumeration tree for  such that
any  appears exactly once. 
The enumeration tree is defined by the {\em rightmost expansion}, i.e.,
in this enumeration tree, a node , 
which is a child of , is obtained by adding a rightmost node to .
In our problem, an ordered tree  is 
identical to a left tree  with  nodes for  symbols.

Let  be the DAG obtained by adding the edge  to a DAG .
If necessary, we write  to indicate that
 is added as a left edge.
For a set  of edges, the DAG  is defined analogously.
The DAG  is defined as adding all the edges  to .
The DAG  is also defined as deleting all the edges  from .

\begin{theorem}\label{th1}
The information-theoretic lower bound on the minimum number of bits needed to
represent an SLP with  symbols is .
\end{theorem}
\begin{proof}
Let  be the set of all possible DAGs with  nodes
and a single source/sink such that any internal node has exactly two children.
This  is a super set of  because
the in-degree of the sink of any DAG in  must be
exactly , whereas  does not have such a restriction.
By the definition, 
 holds.

Let .
We show  for each 
by induction on .
Since the base case  is clear, we assume that the induction hypothesis 
is true for some .

Let  be the rightmost expansion of  such that
the rightmost node  is added as the rightmost child of node  in ,
and let  with a left tree . 
By the induction hypothesis,
the number of  is 
and  is embedded into  as the left tree.
Then,  is constructed by adding the left edge 
and a right edge  for a node  in .

Let  be the source of .
For , each  is admissible,
and the number of them is clearly .
For , if , 
 is admissible.

Otherwise, there exists the lowest common ancestor  of  and 
on  with .
Let  be the unique child of  and let  be the path of  
from  to , where possibly .
If the in-dgree of any node in  is at most one in , 
we generate .
If  contains a cycle, it must contain the edge .
However, there is no such a path because of the condition of .
Thus,  is an admissible DAG in .
Conversely, if some node in  is more than two in , let  be the nearest one from .
Analogously, 
is an admissible DAG in .

In all the cases, the number of such s is also  because 
no edge is changed in  and the pair  
containing the edge  is unique for any fixed .
Thus,  is true for each .

This result derives 
where 
is the number of ordered trees with  nodes.
Combining this with 
 as well,
we get the result that the information-theoretic minimum bits needed to represent
 is at least .
\hspace{\fill}
\end{proof}

\subsection{An optimal SLP representation}
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.7\textwidth]{./fig/phasedictionary2}
\end{center}
\vspace{-.5cm}
\caption{
{\bf Encoded phrase dictionary:}
 indicates the remaining sequence .
 is encoded by 
based on a monotonic decomposition  of , i.e.,
each  indicates a weakly monotonic subsequence in ;
 is the sequence of  indicating the membership for some ,
 is a permutation of  with respect to 
the corresponding value in ,
 is a binary encoding of the sorted  in increasing order.
We show only the case that  is a member of an increasing , but 
the other case is similarly computed by .
}
\label{dic_enc}
\end{figure}

We present an optimal reresentation of an SLP as an improvement of the data structure recently presented in \cite{Takabatake2012}.
We apply the spanning tree decomposition to the DAG  of a given SLP, and obtain the DAG .
We rename the variables in  by breadth-first order 
and also rename variables in  according to the .
Let  be the resulting DAG from . 
Then, for the array representation  of , we obtain
the condition .
Since this monotonic sequence is encoded by  bits,
 is represented by  bits supporting
  in  time.
We focus on the remaining sequence of length , i.e., .
For simplicity, we write  instead of . 

Let  be a disjoint set of subsequences of  such that 
any  is contained in some  and any   are disjoint.
Such an  is called a decomposition of .
A sequence  is weakly monotonic 
if it is {\em increasing}, i.e.,  or {\em decreasing}, i.e., .
In addition,  is called {\em monotonic} if the sequence  is weakly monotonic 
for any .

\begin{theorem}\label{th2}
Any SLP with  symbols can be represented using  bits
for , while supporting  access time. 
\end{theorem}
\begin{proof}
It is sufficient to prove that any  of length  
can be represented using  bits for some .
By the result in~\cite{Yehuda1998},
we can construct a monotonic decomposition  of  such that
.

We represent the sequence  as a four-tuple  using .
For each ,  iff 
 is a member of  for some .
Let  be the sequence of pairs  .
We sort these pairs with respect to the keys   and 
obtain the sorted sequence . 
We define  as the permutation . 

 is defined as the bit string


Finally,  if  is increasing and
 otherwise for .
 and  are represented by WTs, respectively, and  is a rank/select dictionary.

We recover  using .
When  and , i.e.,  is included in the 
-th monotonic subsequence  that is increasing, 
we obtain 
 
by .
When  and , we can similarly obtain  
replacing  by .

The total size of the data structure formed by 
is at most  bits. 
The rank/select/access operations of the WT for a static sequence over  symbols 
can be improved to achieve  time for each query~\cite{Jeremy2010,Golynski2006}.
\hspace{\fill}
\end{proof}

In Figure~\ref{dic_enc}, for the sequence  of pairs  , 
the sorted sequence is .
Thus,  is . 
.
 because  is increasing, and  because  is decreasing.

\section{Data Structure for Reverse Dictionary}

In this section, we present a data structure for simulating 
the naming function  defined as follows.
For a phrase dictionary  with  symbols,


For a sufficiently large , 
we set a total order on , i.e., 
the lexicographical order of the  digrams.
This order is represented by the range .
Then, we recursively define WT  for a phrase dictionary  
partitioning .
On the root node, the initial range  is partitioned into two parts: a left range
 and a right range .
The root is the bit string  such that
 if  and  if .
By this, the sequence of digrams, , is decomposed into two subsequences  and ;
they are projected on the roots of the left and right subtrees, respectively.
Each sub-range is recursively partitioned and 
the subsequence of  on a node is further decomposed
with respect to the partitioning on the node.
This process is repeated until the length of any sub-range is one.
Let  be the bit string assigned to the -th node
of  in the breadth-first traversal.
In Figure~\ref{wavelet_dic}, we show an example
of such a data structure for a phrase dictionary .

\begin{figure}[tb]
\begin{center}
\includegraphics[width=0.8\textwidth]{./fig/wt}
\end{center}
\caption{
{\bf WT for reverse dictionary:}
The bit string  is assigned to 
the -th node in breadth-first order.
For each internal node ,
we can move to the left child by 
and to the right child by  on .
The upward traversal is simulated by 
and  as shown.
The leaf for an existing digram is represented by 
and  is represented by , whereas
these bits are omitted in this figure.
}
\label{wavelet_dic}
\end{figure}


\begin{theorem}\label{wavelet-th}
The naming function for phrase dictionary  over  
symbols can be computed by the proposed data structure 
in  time for any digram.
Moreover, when a digram does not exist in the current ,
 can be updated in the same time and 
the space is at most  bits.
\end{theorem}
\begin{proof}
 is regarded as a WT for 
a string  of length  such that any symbol is represented 
in  bits.
Thus,  is obtained by .
The query time is bounded by the number of
 and  operations for bit strings
performed until the operation flow returns to the root.
Since the total range is , i.e., the height of  is at most ,
the query time and the size are derived.
When  does not exist in ,
let  be the sequence of traversed nodes from the root  to
a leaf  and let  be the bit string on . 
Given an access/rank/select dictionary for ,
we can update it for  and  in  time.
Therefore, the update time of  for any digram is .
\hspace{\fill}
\end{proof}

\section{Discussion}

We have investigated three problems related to the construction of an SLP:
the information-theoretic lower bound for representing the phrase dictionary ,
an optimal representation of a directly addressable ,
and a dynamic data structure for .
Here, we consider the results of this study from the viewpoint of open questions.

For the first problem, we approximately estimated 
the size of a set of SLPs with  symbols, which is
almost equal to the exact set.
This problem, however, has several variants, e.g.,
the set of SLPs with  symbols deriving the same string,
which is quite difficult to estimate owing to the NP-hardness of the smallest CFG problem.
There is another variant obtained by a restriction:
Any two different variables do not derive the same digram, i.e.,
 and  do not exist simultaneously for .
Although such variables are not prohibited in the definition of SLP,
they should be removed for space efficiency.
On the other hand, even if we assume this restriction,
the information-theoretic lower bound is never smaller than  bits because, 
given a directed chain of length  as ,
we can easily construct  admissible DAGs.

For the second problem, we proposed almost optimal encoding of SLPs.
From the standpoint of massive data compression,
one drawback of the proposed encoding is that
the whole phrase dictionary must be stored in memory beforehand.
Since symbols must be sorted, we need a dynamic data structure
to allow the insertion of symbols in an array, e.g.,~\cite{Jansson2012}.
Such data structures, however, require  bits of space.

For the last problem, the query time and update time of proposed 
data structure are both .
This cost is considerable and it is difficult to improve it
to  because  is not static.
When focusing on the characteristics of SLPs, 
we can improve the query time probabilistically;
since any symbol  appears in  at least once and ,
the average of frequency of  is at most two.
Thus, using an additional array of size  bits,
we can check  in  time with probability at least .
However, improving this probability is not easy.
For this problem, achieving  amortized query time is also an interesting challenge.




\bibliographystyle{plain}

{\small
\bibliography{main.bib}
}
\end{document}
