\documentclass{article} \usepackage{iclr2020_conference,times}



\usepackage{amsmath,amsfonts,bm}
\allowdisplaybreaks

\newcommand{\loss}{\mathcal{L}}
\newcommand{\tree}{\mathcal{T}}
\newcommand{\graph}{\mathcal{G}}
\newcommand{\LSTM}{\mathrm{LSTM}}
\newcommand{\ReLU}{\mathrm{ReLU}}
\newcommand{\MLP}{\mathrm{MLP}}
\newcommand{\MPN}{\mathrm{MPN}}
\newcommand{\attention}{\mathrm{attention}}
\newcommand{\len}[1]{\vert #1 \vert}
\newcommand{\set}[1]{\{ #1 \}}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vnu{{\bm{\nu}}}
\def\valpha{{\bm{\alpha}}}
\def\vbeta{{\bm{\beta}}}
\def\vtheta{{\bm{\theta}}}
\def\vdelta{{\bm{\delta}}}
\def\vsigma{{\bm{\sigma}}}
\def\vSigma{{\bm{\Sigma}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{enumitem}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{subcaption}

\title{Hierarchical Graph-to-Graph Translation for Molecules}



\author{Wengong Jin,\;
  Regina Barzilay,\;
  Tommi Jaakkola \\
  CSAIL, Massachusetts Institute of Technology \\
}
\iclrfinalcopy
\begin{document}
\maketitle
\begin{abstract}
The problem of accelerating drug discovery relies heavily on automatic tools to optimize precursor molecules to afford them with better biochemical properties. Our work in this paper substantially extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization. In particular, we realize coherent multi-resolution representations by interweaving the encoding of substructure components with the atom-level encoding of the original molecular graph. Moreover, our graph decoder is fully autoregressive, and interleaves each step of adding a new substructure with the process of resolving its attachment to the emerging molecule. We evaluate our model on multiple molecular optimization tasks and show that our model significantly outperforms previous state-of-the-art baselines.
\end{abstract}
 \section{Introduction}

Molecular optimization seeks to modify compounds in order to improve their biochemical properties. 
This task can be formulated as a graph-to-graph translation problem analogous to machine translation. Given a corpus of molecular pairs , where  is a paraphrase of  with better chemical properties, the model is trained to translate an input molecular graph into its better form.
The task is difficult since the space of potential candidates is vast, and molecular properties can be complex functions of structural features.
Moreover, graph generation is computationally challenging due to complex dependencies involved in the joint distribution over nodes and edges. 
Similar to machine translation, success in this task is predicated on the inductive biases built into the encoder-decoder architecture, in particular the process of generating molecular graphs.

Prior work~\citep{jin2018learning} proposed a junction tree encoder-decoder that utilized valid chemical substructures (e.g., aromatic rings) as building blocks to generate graphs. Each molecule was represented as a junction tree over chemical substructures in addition to the original atom-level graph. 
While successful, the approach remains limited in several ways. 
The tree and graph encoding were carried out separately, and decoding proceeded in strictly successive steps: first generating the junction tree for the new molecule, and then attaching its substructures together. This means the predicted attachments do not impact the subsequent substructure choices.
Moreover, the attachment prediction process involved complex combinatorial enumeration which made the decoding process slow and hard to parallelize.


We propose a multi-resolution, hierarchically coupled encoder-decoder for graph generation. 
Our auto-regressive decoder interleaves the prediction of substructure components with their attachments to the molecule being generated. 
In particular, a target graph is unraveled as a sequence of triplet predictions (where to expand the graph, new substructure type, its attachment).
This enables us to model strong dependencies between successive attachments and substructure choices.
The encoder is designed to represent molecules at different resolutions in order to match the proposed decoding process.
Specifically, the encoding of each molecule proceeds across three levels, with each layer capturing essential information for its corresponding decoding step.
The graph convolution of atoms at the lowest level supports the prediction of attachments and the convolution over substructures at the highest level supports the prediction of successive substructures. 
Compared to prior work, our decoding process is much more efficient because it decomposes each generation step into a hierarchy of smaller steps in order to avoid combinatorial explosion.
We also extend the method to handle conditional translation where desired criteria are fed as input to the translation process. 
This enables our method to handle different combinations of criteria at test time.


We evaluate our new model on multiple molecular optimization tasks. Our baselines include previous state-of-the-art graph generation methods~\citep{you2018graph,liu2018constrained,jin2018learning} and an atom-based translation model we implemented for a more comprehensive comparison. 
Our model significantly outperforms these methods in discovering molecules with desired properties, yielding 3.3\% and 8.1\% improvement on QED and DRD2 optimization tasks. 
During decoding, our model runs 6.3 times faster than previous substructure-based  generation methods. We further conduct ablation studies to validate the advantage of our hierarchical decoding and multi-resolution encoding.
Finally, we show that conditional translation can succeed (generalize) even when trained on molecular pairs with only 1.6\% of them having desired target property combination. 

 \section{Related Work}
\textbf{Molecular Graph Generation }
Previous work have adopted various approaches for generating molecular graphs.
Methods~\citep{gomez2016automatic,segler2017generating,kusner2017grammar,dai2018syntax-directed,guimaraes2017objective,olivecrona2017molecular,popova2018deep,kang2018conditional} generate molecules based on their SMILES strings~\citep{weininger1988smiles}.
\citet{simonovsky2018graphvae,de2018molgan,ma2018constrained} developed generative models which output the adjacency matrices and node labels of the graphs at once. 
\citet{you2018graphrnn,li2018learning,samanta2018nevae,liu2018constrained} proposed generative models decoding molecules sequentially node by node.  
\citet{you2018graph,zhou2018optimization} adopted similar node-by-node approaches in the context of reinforcement learning. 
\citet{kajino2018molecular} developed a hypergraph grammar based method for molecule generation. 

Our work is most closely related to \citet{jin2018junction,jin2018learning} that generate molecules based on substructures. They adopted a two-stage procedure for realizing graphs. The first step generates a junction tree with substructures as nodes, capturing their coarse relative arrangements. The second step resolves the full graph by specifying how the substructures should be attached to each other.
Their major drawbacks are 
1) The second step introduced local independence assumptions and therefore the decoder is not autoregressive.
2) These two steps are applied stage-wise during decoding -- first realizing the junction tree and then reconciling attachments without feedback. 
In contrast, our method jointly predicts the substructures and their attachments with an autoregressive decoder.

\textbf{Graph Encoders } Graph neural networks have been extensively studied for graph encoding~\citep{scarselli2009graph,bruna2013spectral,li2015gated,niepert2016learning,kipf2016semi,hamilton2017inductive,lei2017deriving,velickovic2017graph,xu2018powerful}.
Our method is related to graph encoders for molecules \citep{duvenaud2015convolutional,kearnes2016molecular,dai2016discriminative,gilmer2017neural,schutt2017schnet}. Different to these approaches, our method represents molecules as hierarchical graphs spanning from atom-level graphs to substructure-level trees. 

Our work is most closely related to \citep{defferrard2016convolutional,ying2018hierarchical,gao2019graph} that learn to represent graphs in a hierarchical manner. In particular, \citet{defferrard2016convolutional} utilized graph coarsening algorithms to construct multiple layers of graph hierarchy and
\citet{ying2018hierarchical,gao2019graph} proposed to learn the graph hierarchy jointly with the encoding process.
Despite some differences, all of these methods seek to represent graphs as a single vector for regression or classification tasks. In contrast, our focus is graph generation and a molecule is encoded into multiple sets of vectors, each representing the input at different resolutions. Those vectors are dynamically aggregated by decoder attention modules in each graph generation step.
%
 \section{Hierarchical Generation of Molecular Graphs}

The graph translation task seeks to learn a function  that maps a molecule  into another molecule  with better chemical properties.  is parameterized as an encoder-decoder with neural attention.
Both our encoder and decoder are illustrated in Figure~\ref{fig:paradigm}.
In each generation step, our decoder adds a new substructure (\emph{substructure prediction}) and decides how it should be attached to the current graph. The attachment prediction proceeds in two steps: predicting attaching points in the new substructure and their corresponding attaching points in the current graph (\emph{attachment prediction 1-2}). 

To support the above hierarchical generation, we need to design a matching encoder representing molecules at multiple resolutions in order to provide necessary information for each decoding step. 
Therefore, we propose to represent a molecule  by a hierarchical graph  with three components: 1) \emph{substructure layer} representing how substructures are coarsely connected; 2) \emph{attachment layer} showing the attachment configuration of each substructure; 3) \emph{atom layer} showing how atoms are connected in the graph. Our model encodes nodes in  into substructure vectors , attachment vectors  and atom vectors , which are fed to the decoder for corresponding prediction steps.
As our encoder is tailored for the decoder, we first describe our decoder to clarify relevant concepts.





\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./paradigm.png}
    \caption{Overview of our approach. Each substructure  is a subgraph of a molecule (e.g., rings). In each step, our decoder adds a new substructure and predicts its attachment to current graph. Our encoder represents each molecule across three levels (atom layer, attachment layer and substructure layer), with each layer capturing relevant information for the corresponding decoding step.}
    \label{fig:paradigm}
\end{figure}

\subsection{Hierarchical Graph Decoder}
\label{sec:decoder}

\textbf{Notations } We denote the sigmoid function as .  represents a multi-layer neural network whose input is the concatenation of  and .  stands for a bi-linear attention over vectors  with query vector . 

\textbf{Substructures } 
We define a substructure  as subgraph of molecule  induced by atoms in  and bonds in . Given a molecule, we extract its substructures  such that their union covers the entire molecular graph:  and . In this paper, we consider two types of substructures: rings and bonds. We denote the vocabulary of substructures as , which is constructed from the training set. In our experiments,  and it has over 99.5\% coverage on test sets.

\textbf{Substructure Tree }
To characterize how substructures are connected in the molecule , we construct its corresponding substructure tree , whose nodes are substructures . Specifically, we construct the tree by first drawing edges between  and  if they share common atoms, and then applying tree decomposition over  to ensure it is tree-structured. This allows us to significantly simplify the graph generation process.

\textbf{Generation } 
Our graph decoder generates a molecule  by incrementally expanding its substructure tree in its depth-first order. 
Suppose the model is currently visiting substructure node . It makes the following predictions conditioned on encoding of input  (see Figure~\ref{fig:decoder}):
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt]
    \item \textbf{Topological Prediction}: It first predicts whether there will be a new substructure attached to . If not, the model backtracks to its parent node  in the tree. Let  be the hidden representation of  learned by the decoder (which will be elaborated in \S\ref{sec:encoder}). This probability is predicted by a MLP with attention over substructure vectors  of : 
    
    
    \item \textbf{Substructure Prediction}: If , the model decides to create a new substructure  from  and sets its parent . It then predicts the substructure type of  using another MLP that outputs a distribution over the vocabulary :
    
    
    \item \textbf{Attachment Prediction}: Now the model needs to decide how  should be attached to . The attachment between  and  is defined as atom pairs  where atom  and  are attached together. We predict those atom pairs in two steps:
    
    \begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt] 
        \item[1)] We first predict the atoms  that will be attached to . Since the graph  is always fixed and the number of attaching atoms between two substructures is usually small, we can enumerate all possible configurations  to form a vocabulary  for each substructure . This allows us to formulate the prediction of  as a classification task --  predicting the correct configuration  from the vocabulary :
        
        \item[2)] Given the predicted attaching points , we need to find the corresponding atoms  in the substructure . As the attaching points are always consecutive, there exist at most  different attachments . The probability of a candidate attachment  is computed based on the atom representations  and  learned by the decoder:
        
    \end{enumerate}
\end{enumerate}
The above three predictions together give an autoregressive factorization of the distribution over the next substructure and its attachment. Each of the three decoding steps depends on the outcome of previous step, and predicted attachments will in turn affect the prediction of subsequent substructures.
During training, we apply teacher forcing to the above generation process, where the generation order is determined by a depth-first traversal over the ground truth substructure tree.
The attachment enumeration is tractable because most of the substructures are small. In our experiments, the average size of attachment vocabulary  and the number of candidate attachments is less than 20.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{./decoder.png}
    \caption{Illustration of hierarchical graph decoding. Suppose the decoder is visiting the substructure . 1) It decides to add a new substructure (\emph{topological prediction}). 2) It predicts that new substructure  should be a ring (\emph{substructure prediction}) 3) It predicts how this new ring should be attached to the graph (\emph{attachment prediction}). Finally, the decoder moves to  and repeats the process.}
    \label{fig:decoder}
\end{figure}

\subsection{Hierarchical Graph Encoder}
\label{sec:encoder}
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{./encoder.png}
    \caption{\textbf{Left}: Hierarchical graph encoder. Solid arrows illustrate message passing in each layer. Dashed arrows connect each atom to the substructures it belongs. In the attachment layer, each node  is a particular attachment configuration of substructure . \textbf{Right}: Attachment vocabulary for a ring. The attaching points in each configuration (highlighted in red) must be consecutive.} \label{fig:encoder}
    \vspace{-5pt}
\end{figure}

Our encoder represents a molecule  by a hierarchical graph  in order to support the above decoding process. The hierarchical graph has three components (see Figure~\ref{fig:encoder}):


\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt] 
    \item \textbf{Atom layer}: The atom layer is the molecular graph of  representing how its atoms are connected. Each atom node  is associated with a label  indicating its atom type and charge. Each edge  in the atom layer is labeled with  indicating its bond type.
    
    \item \textbf{Attachment layer}: This layer is derived from the substructure tree of molecule . Each node  in this layer represents a particular attachment configuration of substructure  in the vocabulary . Specifically,  where  are the attaching atoms between  and its parent  in the tree. This layer provides necessary information for the attachment prediction (step 1). Figure~\ref{fig:encoder} illustrates how  and the vocabulary  look like.
    
    \item \textbf{Substructure layer}: This layer is the same as the substructure tree. This layer provides essential information for the substructure prediction in the decoding process.
\end{enumerate}

We further introduce edges that connect the atoms and substructures between different layers in order to propagate information in between. In particular, we draw a directed edge from atom  in the atom layer to node  in the attachment layer if . We also draw edges from node  to node  in the substructure layer. 
This gives us the hierarchical graph  for molecule , which will be encoded by a hierarchical message passing network (MPN) (see Figure~\ref{fig:encoder}). 
The encoder contains three MPNs that encode each of the three layer. We use the MPN architecture from \citet{jin2018learning}.\footnote{We slightly modified their architecture by using LSTM instead of GRU for message propagation due to its better empirical performance. The details are shown in the appendix.} For simplicity, we denote the MPN encoding process as  with parameter .

\textbf{Atom Layer MPN } 
We first encode the atom layer of  (denoted as ). The inputs to this MPN are the embedding vectors  of all the atoms and bonds in . During encoding, the network propagates the message vectors between different atoms for  iterations and then outputs the atom representation  for each atom :


\textbf{Attachment Layer MPN } 
The input feature of each node  in the attachment layer  is an concatenation of the embedding  and the sum of its atom vectors : 

The input feature for each edge  in this layer is an embedding vector , where  describes the relative ordering between node  and  during decoding. Specifically, we set  if node  is the -th child of node  and  if  is the parent. We then run  iterations of message passing over  to compute the substructure representations:


\textbf{Substructure Layer MPN } Similarly, the input feature of node  in this layer is computed as the concatenation of embedding  and the node vector  from the previous layer. Finally, we run message passing over the substructure layer  to obtain the substructure representations:

In summary, the output of our hierarchical encoder is a set of vectors  that represent a molecule  at multiple resolutions. These vectors are input to the decoder attention.

\textbf{Decoder MPN } During decoding, we use the same hierarchical MPN architecture to encode the hierarchical graph  at each step . This gives us the substructure vectors  and atom vectors  in \S\ref{sec:decoder}. All future nodes and edges are masked to ensure the prediction of current substructure and attachment only depends on previously generated outputs.

\subsection{Training}
\label{sec:training}

Our training set contains molecular pairs  where each compound  can be associated with multiple outputs  since there are many ways to modify  to improve its properties. In order to generate diverse outputs, we follow \citet{jin2018learning} and extend our method to a variational translation model  with an additional input . The latent vector  indicates the intended mode of translation which is sampled from a Gaussian prior  during testing.

We train our model using variational inference~\citep{kingma2013auto}. Given a training example , we sample  from the posterior . To compute , we first encode  and  into their representations  and  and then compute vector  that summarizes the structural changes from molecule  to  at both atom and substructure level:

Finally, we compute  and sample  using reparameterization trick. The latent code  is passed to the decoder along with the input representation  to reconstruct output . The overall training objective follows a standard conditional VAE:



\textbf{Conditional Translation } In the above formulation, the model does not know what properties are being optimized during translation. During testing, users cannot change the behavior of a trained model (i.e., what properties should be changed). This may become a limitation of our method in a multi-property optimization setting. Therefore, we extend our method to handle conditional translation where the desired criteria are also fed as input to the translation process. In particular, let  be a translation criteria indicating what properties should be changed. During variational inference, we compute  and   with an additional input :

We then augment the latent code as  and pass it to the decoder.
During testing, the user can specify their criteria in  to control the outcome (e.g.,  should be drug-like and bioactive).

\begin{figure}
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{Variational Translation (unconditional setting, without target criteria )}
\FOR{ in the training set}
\STATE Encode molecule  into vectors 
\STATE Compute  from 
\STATE Sample latent code 
\STATE Generate molecule  given  and 
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill 
\begin{minipage}{0.48\textwidth}
\vspace{10pt}
\centering
\includegraphics[width=\textwidth]{./conditional.png}
\captionof{figure}{Conditional translation.}
\end{minipage}
\end{figure}

 \section{Experiments}
\label{sec:experiment}
\newcommand\Tstrut{\rule{0pt}{2.3ex}}
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}

We follow the experimental design by \citet{jin2018learning} and evaluate our translation model on their single-property optimization tasks.
As molecular optimization in the real-world often involves different property criteria, we further construct a novel conditional optimization task where the desired criteria is fed as input to the translation process.
To prevent the model from ignoring input  and translating it into arbitrary compound, we require the molecular similarity between  and output  to be above certain threshold  at test time. The molecular similarity is defined as the Tanimoto similarity over Morgan fingerprints~\citep{rogers2010extended} of two molecules.

\textbf{Single-property Optimization } This dataset consists of four different tasks. For each task, we train and evaluate our model on their provided training and test sets. For these tasks, our model is trained under an unconditional setting (without  as input).  

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt] 
\item \textbf{LogP Optimization}: The penalized logP score~\citep{kusner2017grammar} measures the solubility and synthetic accessibility of a compound. In this task, the model needs to translate input  into output  such that . We experiment with two similarity thresholds .

\item \textbf{QED Optimization}: The QED score~\citep{bickerton2012quantifying} quantifies a compound's drug-likeness. In this task, the model is required to translate molecules with QED scores from the lower range  into the higher range . The similarity constraint is .

\item \textbf{DRD2 Optimization}: This task involves the optimization of a compound's biological activity against dopamine type 2 receptor (DRD2). The model needs to translate inactive compounds () into active compounds (), where the bioactivity is assessed by a property prediction model from \citet{olivecrona2017molecular}. The similarity constraint is .
\end{itemize}

\textbf{Conditional Optimization } This new task requires the model to translate input  into output  to satisfy different combination of constraints over its QED and DRD2 scores. We define a molecule  as drug-like if  and as DRD2-active if its predicted bioactivity . At test time, our model needs to handle the following two criteria over output molecule :
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt] 
    \item  is both drug-like and DRD2-active. Here both properties need to be improved after translation.
    \item  is drug-like but DRD2-inactive. In this case, DRD2 is an off-target that may cause side effects. Therefore only the drug-likeness should be improved after translation.
\end{enumerate}
As different users may be interested in different settings, we encode the desired criteria as vector  and train our model under the conditional translation setup in \S\ref{sec:training}. 
Like single-property tasks, we impose a similarity constraint  for both settings.

Our training set contains 120K molecular pairs and the test set has 780 compounds. For each pair , we set .
During testing, we translate each compound with  for each setting.
We note that the first criteria () is the most challenging because there are only 1.6\% of the training pairs with target  being both drug-like and DRD2-active.
To achieve good performance, the model must learn to transfer the knowledge from other pairs with ) that partially satisfy the criteria.

\textbf{Evaluation Metrics } Our evaluation metrics include translation accuracy and diversity. Each test molecule  is translated  times with different latent codes sampled from the prior distribution. On the logP optimization, we select compound  as the final translation of  that gives the highest property improvement and satisfies . We then report the average property improvement  over test set . For other tasks, we report the translation success rate. A compound is successfully translated if one of its  translation candidates satisfies all the similarity and property constraints of the task.
To measure the diversity, for each molecule we compute the average pairwise Tanimoto distance between all its successfully translated compounds. Here the Tanimoto distance is defined as .

\textbf{Baselines } We compare our method (HierG2G) against the baselines including GCPN~\citep{you2018graph}, MMPA~\citep{dalke2018mmpdb} and translation based methods Seq2Seq and JTNN~\citep{jin2018learning}.
Seq2Seq is a sequence-to-sequence model that generates molecules by their SMILES strings. 
JTNN is a graph-to-graph architecture that generates molecules structure by structure, but its decoder is not fully autoregressive. 
We also compare with CG-VAE~\citep{liu2018constrained}, a generative model that decodes molecules atom by atom and optimizes properties in the latent space using gradient ascent.

To make a direct comparison possible between our method and atom-based generation, we further developed an atom-based translation model (AtomG2G) as baseline. 
It makes three predictions in each generation step. First, it predicts whether the decoding process has completed (no more new atoms). If not, it creates a new atom  and predicts its atom type. Lastly, it predicts the bond type between  and other atoms autoregressively to fully capture edge dependencies~\citep{you2018graphrnn}.
The encoder of AtomG2G encodes only the atom-layer graph and the decoder attention only sees the atom vectors . All translation models (Seq2Seq, JTNN, AtomG2G and HierG2G) are trained under the same variational objective (\S\ref{sec:training}). Details of baseline architectures are in the appendix. 

\begin{table}[t]
\centering
\caption{Results on single-property translation tasks. ``Div.'' stands for diversity. ``Succ.'' stands for success rate. ``Improve.'' stands for average property improvement.}
\vspace{-4pt}
\begin{tabular}{lcccccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{ logP () } & \multicolumn{2}{c}{ logP () } & \multicolumn{2}{c}{ QED } & \multicolumn{2}{c}{ DRD2 } \Tstrut\Bstrut \\
\cline{2-9}
& Improve. & Div. & Improve. & Div. & Succ. & Div. & Succ. & Div. \Tstrut\Bstrut \\
\hline
JT-VAE &  & - &  & - & 8.8\% & - & 3.4\% & - \Tstrut\Bstrut \\
CG-VAE &  & - &  & - & 4.8\% & - & 2.3\% & - \Tstrut\Bstrut \\
GCPN &  & - &  & - & 9.4\% & 0.216 & 4.4\% & 0.152 \Tstrut\Bstrut \\
MMPA &  & 0.329 &  & 0.496 & 32.9\% & 0.236 & 46.4\% & \textbf{0.275} \Tstrut\Bstrut \\
Seq2Seq &  & 0.331 &  & 0.471 & 58.5\% & 0.331 & 75.9\% & 0.176 \Tstrut\Bstrut \\
JTNN &  & 0.333 &  & 0.480 & 59.9\% & 0.373 & 77.8\% & 0.156 \Tstrut\Bstrut \\
AtomG2G &  & 0.379 & \textbf{3.98  1.54} & 0.563 & 73.6\% & 0.421 & 75.8\% & 0.128 \Tstrut\Bstrut \\
\hline
HierG2G & \textbf{2.49  1.09} & \textbf{0.381} & \textbf{3.98  1.46} & \textbf{0.564} & \textbf{76.9\%} & \textbf{0.477} & \textbf{85.9\%} & 0.192 \Tstrut\Bstrut \\
\hline
\end{tabular}
\label{tab:prop1}
\vspace{-6pt}
\end{table}

\begin{table}[t]
\centering
\caption{Results on conditional optimization tasks and ablation studies over architecture choices.}
\vspace{-5pt}
\begin{subtable}{0.52\textwidth}
\centering
\caption{Conditional optimization results:  means the output  needs to be drug-like and  means it needs to be DRD2-active.}
\begin{tabular}{lcccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{  } & \multicolumn{2}{c}{  }  \Tstrut\Bstrut \\
\cline{2-5}
& Succ. & Div. & Succ. & Div.  \Tstrut\Bstrut \\
\hline
Seq2Seq & 5.0\% & 0.078 & 67.8\% & 0.380  \Tstrut\Bstrut \\
JTNN & 11.1\% & 0.064 & 71.4\% & 0.405  \Tstrut\Bstrut \\
AtomG2G & 12.5\% & 0.031 & 74.5\% & 0.443  \Tstrut\Bstrut \\
\hline
HierG2G & \textbf{13.0\%} & \textbf{0.094} & \textbf{78.5\%} & \textbf{0.480} \Tstrut\Bstrut \\
\hline
\end{tabular}
\label{tab:prop2}
\end{subtable}
~
\begin{subtable}{0.46\textwidth}
\centering
\caption{Ablation study: the importance of hierarchical graph encoding, LSTM MPN architecture and structure-based decoding.}
\begin{tabular}{lcc}
\hline
Method & QED & DRD2  \Tstrut\Bstrut \\
\hline
HierG2G & \textbf{76.9\%} & \textbf{85.9\%}   \Tstrut\Bstrut \\
 atom-based decoder & 76.1\% & 75.0\% \Tstrut\Bstrut \\
 two-layer encoder & 75.8\% & 83.5\%  \Tstrut\Bstrut \\
 one-layer encoder & 67.8\% & 74.1\% \Tstrut\Bstrut \\
 \small{GRU MPN}  & 72.6\% & 83.7\%  \Tstrut\Bstrut \\
\hline
\end{tabular}
\label{tab:ablation}
\end{subtable}
\end{table}

\subsection{Results}

\textbf{Single-property Optimization } As shown in Table~\ref{tab:prop1}, our model achieves the new state-of-the-art on the four translation tasks. In particular, our model significantly outperforms JTNN in both translation accuracy (e.g., 76.9\% versus 59.9\% on the QED task) and output diversity (e.g., 0.564 versus 0.480 on the logP task). While both methods generate molecules by structures, our decoder is autoregressive which can learn more expressive mappings. 
More importantly, our model runs 6.3 times faster than JTNN during decoding. 
Our model also outperforms AtomG2G on three datasets, with over 10\% improvement on the DRD2 task. This shows the advantage of our hierarchical encoding and decoding.

\textbf{Conditional Optimization } For this task, we compare our method with other translation methods: Seq2Seq, JTNN and AtomG2G. All these models are trained under the conditional translation setup where feed the desired criteria  as input.
As shown in Table~\ref{tab:prop2}, our model outperforms other models in both translation accuracy and output diversity. Notably, all models achieved very low success rate on  because it has the strongest constraints and only 1.6K of the training pairs satisfy this criteria. In fact, training our model on the 1.6K examples only gives 4.2\% success rate as compared to 13.0\% when trained with other pairs. This shows our conditional translation setup can transfer the knowledge from other pairs with .
Figure~\ref{fig:multi-translate} illustrates how the input criteria  affects the generated output.

\textbf{Ablation Study } To understand the importance of different architecture choices, we report ablation studies over the QED and DRD2 tasks in Table~\ref{tab:ablation}. We first replace our hierarchical decoder with atom-based decoder of AtomG2G to see how much the structure-based decoding benefits us. We keep the same hierarchical encoder but modified the input of the decoder attention to include both atom and substructure vectors. Using this setup, the model performance decreases by 0.8\% and 10.9\% on the two tasks. We suspect the DRD2 task benefits more from structure-based decoding because biological target binding often depends on the presence of specific functional groups.

Our second experiment reduces the number of hierarchies in our encoder and decoder MPN, while keeping the same hierarchical decoding process. When the top substructure layer is removed, the translation accuracy drops slightly by 0.8\% and 2.4\%. When we further remove the attachment layer, the performance degrades significantly on both datasets. This is because all the substructure information is lost and the model needs to infer what substructures are and how substructure layers are constructed for each molecule.
Implementation details of those ablations are shown in the appendix.

Lastly, we replaced our LSTM MPN with the original GRU MPN used in JTNN. While the translation performance decreased by 4\% and 2.2\%, our method still outperforms JTNN by a wide margin. Therefore we use the LSTM MPN architecture for both HierG2G and AtomG2G baseline. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{./multi_translate.png}
    \caption{Illustration of conditional translation. Our model generates different molecules when the translation criteria changes. When , the model indeed generates a compound with high QED and DRD2 scores. When , the model predicts another compound inactive to DRD2.}
    \label{fig:multi-translate}
    \vspace{-10pt}
\end{figure} \section{Conclusion}
In this paper, we developed a hierarchical graph-to-graph translation model that generates molecular graphs using chemical substructures as building blocks. In contrast to previous work, our model is fully autoregressive and learns coherent multi-resolution representations. The experimental results show that our method outperforms previous models under various settings. 
\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\newpage
\appendix
\section{Network Architecture}

\textbf{LSTM MPN Architecture } The LSTM MPN is a slight modification from the MPN architecture used in \citet{jin2018learning}. Let  be the neighbors of node ,  the node feature of  and  be the feature of edge . During encoding, each edge  is associated with two messages  and , representing the message from  to  and vice versa. The messages are updated by an LSTM cell with parameters   defined as follows: 
\begin{algorithm}
\caption{LSTM Message Passing}
\begin{algorithmic}
\FUNCTION{  }
\STATE \vspace{-8pt}
\vspace{-3pt}
\STATE \textbf{Return } 
\vspace{3pt}
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}

The message passing network  over graph  is defined as:
\begin{algorithm}
\caption{LSTM MPN with  message passing iterations}
\begin{algorithmic}
\FUNCTION{}
\STATE Initialize messages: 
\FOR{ \textbf{to} }
\STATE Compute messages  for all edges  simultaneously.
\ENDFOR
\STATE \textbf{Return } node representations 
\vspace{3pt}
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}

\textbf{Attention Layer } Our attention layer is a bilinear attention function with parameter :


\textbf{AtomG2G Architecture } AtomG2G is an atom-based translation method that is directly comparable to HierG2G. Here molecules are represented solely as molecular graphs rather than a hierarchical graph with substructures. The encoder of AtomG2G is the same LSTM MPN over molecular graph. This gives us a set of atom vectors  representing molecule  only at the atom level.

The decoder of AtomG2G is illustrated in Figure~\ref{fig:atomg2g}. Following \citet{you2018graphrnn,liu2018constrained}, the model generates molecule  atom by atom following their breath-first order. During generation, it maintains a FIFO queue  that contains the frontier nodes in the graph (i.e., nodes who still have neighbors to be generated). 
Let  be the first node in  and  be the current graph at step . In each step, the model makes three predictions to expand the graph :
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item It predicts whether there will be new atoms attached to . If not, the model discards  and move on to the next node in . The generation stops if  is empty.
\item Otherwise, it creates a new atom  and predicts its atom type.
\item Lastly, it predicts the bond type between  and other frontier nodes in  autoregressively to fully capture edge dependencies~\citep{you2018graphrnn}. Since nodes are generated in breath-first order, there will be no edges between  and nodes outside of .
\end{enumerate}

To make those predictions, we use the same LSTM MPN to encode the current graph . Let  be the atom representation of . We represent  as the sum of all its atom vectors . In the first step, we model the probability of expanding a new node from  as:

In the second step, the atom type of the new node  is predicted using another MLP:

In the last step, we predict the bonds between  and nodes in  sequentially starting with . Specifically, for each atom pair , we predict their bond type (single, double, triple or none) as the following:

where  is the atom representation of node  and  is the representation of node  at the  bond prediction. Let  be node 's current neighbor predicted in the first  steps.  is computed as follows to reflect its local graph structure after  bond prediction:

where  is the atom feature of  (i.e., predicted atom type) and  is the bond feature between  and  (i.e., predicted bond type). Intuitively, this can be viewed as running one-step message passing at each bond prediction step (i.e., passing the message  from  to ).

AtomG2G is trained under the same variational objective as HierG2G, with the latent code  sampled from the posterior  and . 

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./atomg2g.png}
    \caption{Illustration of AtomG2G decoding process. Atoms marked with red circles are frontier nodes in the queue . In each step, the model picks the first node  from  and predict whether there will be new atoms attached to . If so, it predicts the atom type of new node  (atom prediction). Then the model predicts the bond type between  and other nodes in  sequentially for  steps (bond prediction, ). Finally, it adds the new atom to the queue .}
    \label{fig:atomg2g}
\end{figure}

\section{Experimental Details}

\textbf{Data } The single-property optimization datasets are directly downloaded from the link provided in \citet{jin2018learning}. The training set and substructure vocabulary size for each dataset is listed in Table~\ref{tab:data}.
\begin{table}[ht]
    \centering
    \begin{tabular}{rcccc}
        \hline
        & logP () & logP () & QED & DRD2 \Tstrut\Bstrut \\
        \hline
        Training set size & 75K & 99K & 88K & 34K \Tstrut\Bstrut \\
        Test set size & 800 & 800 & 800 & 1000 \Tstrut\Bstrut \\
        Substructure vocabulary  & 478 & 462 & 307 & 307 \Tstrut\Bstrut \\
        Average attachment vocabulary  & 3.68 & 3.50 & 3.62 & 3.30 \Tstrut\Bstrut \\ \hline
    \end{tabular}
    \caption{Training set size and substructure vocabulary size for each dataset.}
    \label{tab:data}
\end{table}
We constructed the multi-property optimization by combining the training set of QED and DRD2 optimization task. The test set contains 780 compounds that are not drug-like and DRD2-inactive. The training and test set is attached as part of the supplementary material.

\textbf{Hyperparameters } For HierG2G, we set the hidden layer dimension to be 270 and the embedding layer dimension 200. We set the latent code dimension  and KL regularization weight . We run  iterations of message passing in each layer of the encoder. 
For AtomG2G, we set the hidden layer and embedding layer dimension to be 400 so that both models have roughly the same number of parameters. We also set  and number of message passing iterations to be . We train both models with Adam optimizer with default parameters.

For CG-VAE~\citep{liu2018constrained}, we used their official implementation for our experiments. Specifically, for each dataset, we trained a CG-VAE to generate molecules and predict property from the latent space. This gives us three CG-VAE models for logP, QED and DRD2 optimization tasks, respectively. 
At test time, each compound  is translated following the same procedure as in \citet{jin2018junction}. First, we embed  into its latent representation  and perform gradient ascent over  to maximize the predicted property score. This gives us  vectors for  gradient steps. Then we decode  molecules from  and select the one with the best property improvement within similarity constraint.
We found that it is necessary to keep the KL regularization weight low () to achieve meaningful results. When , the above gradient ascent procedure always generate molecules very dissimilar to the input .

\textbf{Ablation Study } Our ablation studies are illustrated in Figure~\ref{fig:ablation}. In our first experiment, we changed our decoder to the atom-based decoder of AtomG2G. As the encoder is still hierarchical, we modified the input of the decoder attention to include both atom and substructure vectors. We set the hidden layer and embedding layer dimension to be 300 to match the original model size. 

Our next two experiments reduces the number of hierarchies in both our encoder and decoder MPN. In the two-layer model, molecules are represented by . We make topological and substructure predictions based on hidden vector  instead of  because the substructure layer is removed. 
In the one-layer model, molecules are represented by  and we make topological and substructure predictions based on atom vectors . The hidden layer dimension is adjusted accordingly to match the original model size.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./ablation.png}
    \caption{Illustration of three different ablation studies. \textbf{Left}: Atom-based decoder; \textbf{Middle}: Two-layer encoder; \textbf{Right}: One-layer encoder. }
    \label{fig:ablation}
\end{figure} \end{document}
