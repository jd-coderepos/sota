\documentclass{article} \usepackage{iclr2020_conference,times}



\usepackage{amsmath,amsfonts,bm}
\allowdisplaybreaks

\newcommand{\loss}{\mathcal{L}}
\newcommand{\tree}{\mathcal{T}}
\newcommand{\graph}{\mathcal{G}}
\newcommand{\LSTM}{\mathrm{LSTM}}
\newcommand{\ReLU}{\mathrm{ReLU}}
\newcommand{\MLP}{\mathrm{MLP}}
\newcommand{\MPN}{\mathrm{MPN}}
\newcommand{\attention}{\mathrm{attention}}
\newcommand{\len}[1]{\vert #1 \vert}
\newcommand{\set}[1]{\{ #1 \}}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vnu{{\bm{\nu}}}
\def\valpha{{\bm{\alpha}}}
\def\vbeta{{\bm{\beta}}}
\def\vtheta{{\bm{\theta}}}
\def\vdelta{{\bm{\delta}}}
\def\vsigma{{\bm{\sigma}}}
\def\vSigma{{\bm{\Sigma}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{enumitem}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{subcaption}

\title{Hierarchical Graph-to-Graph Translation for Molecules}



\author{Wengong Jin,\;
  Regina Barzilay,\;
  Tommi Jaakkola \\
  CSAIL, Massachusetts Institute of Technology \\
}
\iclrfinalcopy
\begin{document}
\maketitle
\begin{abstract}
The problem of accelerating drug discovery relies heavily on automatic tools to optimize precursor molecules to afford them with better biochemical properties. Our work in this paper substantially extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization. In particular, we realize coherent multi-resolution representations by interweaving the encoding of substructure components with the atom-level encoding of the original molecular graph. Moreover, our graph decoder is fully autoregressive, and interleaves each step of adding a new substructure with the process of resolving its attachment to the emerging molecule. We evaluate our model on multiple molecular optimization tasks and show that our model significantly outperforms previous state-of-the-art baselines.
\end{abstract}
 \section{Introduction}

Molecular optimization seeks to modify compounds in order to improve their biochemical properties. 
This task can be formulated as a graph-to-graph translation problem analogous to machine translation. Given a corpus of molecular pairs $\set{(X,Y)}$, where $Y$ is a paraphrase of $X$ with better chemical properties, the model is trained to translate an input molecular graph into its better form.
The task is difficult since the space of potential candidates is vast, and molecular properties can be complex functions of structural features.
Moreover, graph generation is computationally challenging due to complex dependencies involved in the joint distribution over nodes and edges. 
Similar to machine translation, success in this task is predicated on the inductive biases built into the encoder-decoder architecture, in particular the process of generating molecular graphs.

Prior work~\citep{jin2018learning} proposed a junction tree encoder-decoder that utilized valid chemical substructures (e.g., aromatic rings) as building blocks to generate graphs. Each molecule was represented as a junction tree over chemical substructures in addition to the original atom-level graph. 
While successful, the approach remains limited in several ways. 
The tree and graph encoding were carried out separately, and decoding proceeded in strictly successive steps: first generating the junction tree for the new molecule, and then attaching its substructures together. This means the predicted attachments do not impact the subsequent substructure choices.
Moreover, the attachment prediction process involved complex combinatorial enumeration which made the decoding process slow and hard to parallelize.


We propose a multi-resolution, hierarchically coupled encoder-decoder for graph generation. 
Our auto-regressive decoder interleaves the prediction of substructure components with their attachments to the molecule being generated. 
In particular, a target graph is unraveled as a sequence of triplet predictions (where to expand the graph, new substructure type, its attachment).
This enables us to model strong dependencies between successive attachments and substructure choices.
The encoder is designed to represent molecules at different resolutions in order to match the proposed decoding process.
Specifically, the encoding of each molecule proceeds across three levels, with each layer capturing essential information for its corresponding decoding step.
The graph convolution of atoms at the lowest level supports the prediction of attachments and the convolution over substructures at the highest level supports the prediction of successive substructures. 
Compared to prior work, our decoding process is much more efficient because it decomposes each generation step into a hierarchy of smaller steps in order to avoid combinatorial explosion.
We also extend the method to handle conditional translation where desired criteria are fed as input to the translation process. 
This enables our method to handle different combinations of criteria at test time.


We evaluate our new model on multiple molecular optimization tasks. Our baselines include previous state-of-the-art graph generation methods~\citep{you2018graph,liu2018constrained,jin2018learning} and an atom-based translation model we implemented for a more comprehensive comparison. 
Our model significantly outperforms these methods in discovering molecules with desired properties, yielding 3.3\% and 8.1\% improvement on QED and DRD2 optimization tasks. 
During decoding, our model runs 6.3 times faster than previous substructure-based  generation methods. We further conduct ablation studies to validate the advantage of our hierarchical decoding and multi-resolution encoding.
Finally, we show that conditional translation can succeed (generalize) even when trained on molecular pairs with only 1.6\% of them having desired target property combination. 

 \section{Related Work}
\textbf{Molecular Graph Generation }
Previous work have adopted various approaches for generating molecular graphs.
Methods~\citep{gomez2016automatic,segler2017generating,kusner2017grammar,dai2018syntax-directed,guimaraes2017objective,olivecrona2017molecular,popova2018deep,kang2018conditional} generate molecules based on their SMILES strings~\citep{weininger1988smiles}.
\citet{simonovsky2018graphvae,de2018molgan,ma2018constrained} developed generative models which output the adjacency matrices and node labels of the graphs at once. 
\citet{you2018graphrnn,li2018learning,samanta2018nevae,liu2018constrained} proposed generative models decoding molecules sequentially node by node.  
\citet{you2018graph,zhou2018optimization} adopted similar node-by-node approaches in the context of reinforcement learning. 
\citet{kajino2018molecular} developed a hypergraph grammar based method for molecule generation. 

Our work is most closely related to \citet{jin2018junction,jin2018learning} that generate molecules based on substructures. They adopted a two-stage procedure for realizing graphs. The first step generates a junction tree with substructures as nodes, capturing their coarse relative arrangements. The second step resolves the full graph by specifying how the substructures should be attached to each other.
Their major drawbacks are 
1) The second step introduced local independence assumptions and therefore the decoder is not autoregressive.
2) These two steps are applied stage-wise during decoding -- first realizing the junction tree and then reconciling attachments without feedback. 
In contrast, our method jointly predicts the substructures and their attachments with an autoregressive decoder.

\textbf{Graph Encoders } Graph neural networks have been extensively studied for graph encoding~\citep{scarselli2009graph,bruna2013spectral,li2015gated,niepert2016learning,kipf2016semi,hamilton2017inductive,lei2017deriving,velickovic2017graph,xu2018powerful}.
Our method is related to graph encoders for molecules \citep{duvenaud2015convolutional,kearnes2016molecular,dai2016discriminative,gilmer2017neural,schutt2017schnet}. Different to these approaches, our method represents molecules as hierarchical graphs spanning from atom-level graphs to substructure-level trees. 

Our work is most closely related to \citep{defferrard2016convolutional,ying2018hierarchical,gao2019graph} that learn to represent graphs in a hierarchical manner. In particular, \citet{defferrard2016convolutional} utilized graph coarsening algorithms to construct multiple layers of graph hierarchy and
\citet{ying2018hierarchical,gao2019graph} proposed to learn the graph hierarchy jointly with the encoding process.
Despite some differences, all of these methods seek to represent graphs as a single vector for regression or classification tasks. In contrast, our focus is graph generation and a molecule is encoded into multiple sets of vectors, each representing the input at different resolutions. Those vectors are dynamically aggregated by decoder attention modules in each graph generation step.
%
 \section{Hierarchical Generation of Molecular Graphs}

The graph translation task seeks to learn a function $\gF$ that maps a molecule $X$ into another molecule $\graph$ with better chemical properties. $\gF$ is parameterized as an encoder-decoder with neural attention.
Both our encoder and decoder are illustrated in Figure~\ref{fig:paradigm}.
In each generation step, our decoder adds a new substructure (\emph{substructure prediction}) and decides how it should be attached to the current graph. The attachment prediction proceeds in two steps: predicting attaching points in the new substructure and their corresponding attaching points in the current graph (\emph{attachment prediction 1-2}). 

To support the above hierarchical generation, we need to design a matching encoder representing molecules at multiple resolutions in order to provide necessary information for each decoding step. 
Therefore, we propose to represent a molecule $X$ by a hierarchical graph $\gH_X$ with three components: 1) \emph{substructure layer} representing how substructures are coarsely connected; 2) \emph{attachment layer} showing the attachment configuration of each substructure; 3) \emph{atom layer} showing how atoms are connected in the graph. Our model encodes nodes in $\gH_X$ into substructure vectors $\vc_X^\gS$, attachment vectors $\vc_X^\gA$ and atom vectors $\vc_X^\graph$, which are fed to the decoder for corresponding prediction steps.
As our encoder is tailored for the decoder, we first describe our decoder to clarify relevant concepts.





\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./paradigm.png}
    \caption{Overview of our approach. Each substructure $\gS_i$ is a subgraph of a molecule (e.g., rings). In each step, our decoder adds a new substructure and predicts its attachment to current graph. Our encoder represents each molecule across three levels (atom layer, attachment layer and substructure layer), with each layer capturing relevant information for the corresponding decoding step.}
    \label{fig:paradigm}
\end{figure}

\subsection{Hierarchical Graph Decoder}
\label{sec:decoder}

\textbf{Notations } We denote the sigmoid function as $\sigmoid(\cdot)$. $\MLP(\va, \vb)$ represents a multi-layer neural network whose input is the concatenation of $\va$ and $\vb$. $\attention_\theta(\vh_*, \vc_X)$ stands for a bi-linear attention over vectors $\vc_X$ with query vector $\vh_*$. 

\textbf{Substructures } 
We define a substructure $\gS_i=(\gV_i,\gE_i)$ as subgraph of molecule $\graph$ induced by atoms in $\gV_i$ and bonds in $\gE_i$. Given a molecule, we extract its substructures $\gS_1,\cdots,\gS_n$ such that their union covers the entire molecular graph: $\gV = \bigcup_i \gV_i$ and $\gE = \bigcup_i \gE_i$. In this paper, we consider two types of substructures: rings and bonds. We denote the vocabulary of substructures as $\gS$, which is constructed from the training set. In our experiments, $|\gS|<500$ and it has over 99.5\% coverage on test sets.

\textbf{Substructure Tree }
To characterize how substructures are connected in the molecule $\graph$, we construct its corresponding substructure tree $\tree$, whose nodes are substructures $\gS_1,\cdots,\gS_n$. Specifically, we construct the tree by first drawing edges between $\gS_i$ and $\gS_j$ if they share common atoms, and then applying tree decomposition over $\tree$ to ensure it is tree-structured. This allows us to significantly simplify the graph generation process.

\textbf{Generation } 
Our graph decoder generates a molecule $\graph$ by incrementally expanding its substructure tree in its depth-first order. 
Suppose the model is currently visiting substructure node $\gS_k$. It makes the following predictions conditioned on encoding of input $X$ (see Figure~\ref{fig:decoder}):
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt]
    \item \textbf{Topological Prediction}: It first predicts whether there will be a new substructure attached to $\gS_k$. If not, the model backtracks to its parent node $\gS_{d_k}$ in the tree. Let $\vh_{\gS_k}$ be the hidden representation of $\gS_k$ learned by the decoder (which will be elaborated in \S\ref{sec:encoder}). This probability is predicted by a MLP with attention over substructure vectors $\vc_X^\gS$ of $X$: 
    \begin{equation}
        \vp_k = \sigmoid(\MLP(\vh_{\gS_k}, \valpha_k^d)) \qquad \valpha_k^d = \attention_d\left( \vh_{\gS_k}, \vc_X^\gS \right)
    \end{equation}
    
    \item \textbf{Substructure Prediction}: If $\vp_k > 0.5$, the model decides to create a new substructure $\gS_t$ from $\gS_k$ and sets its parent $d_t=k$. It then predicts the substructure type of $\gS_t$ using another MLP that outputs a distribution over the vocabulary $\gS$:
    \begin{equation}
        \vp_{\gS_t} = \softmax(\MLP(\vh_{\gS_k}, \valpha_k^s)) \qquad \valpha_k^s = \attention_s\left( \vh_{\gS_k}, \vc_X^\gS \right)
    \end{equation}
    
    \item \textbf{Attachment Prediction}: Now the model needs to decide how $\gS_t$ should be attached to $\gS_k$. The attachment between $\gS_t$ and $\gS_k$ is defined as atom pairs $\gM_t = \set{(u_j,v_j) | u_j \in \gS_t, v_j \in \gS_k}$ where atom $u_j$ and $v_j$ are attached together. We predict those atom pairs in two steps:
    
    \begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt] 
        \item[1)] We first predict the atoms $\set{v_j} \subset \gS_t$ that will be attached to $\gS_k$. Since the graph $\gS_t$ is always fixed and the number of attaching atoms between two substructures is usually small, we can enumerate all possible configurations $\set{v_j}$ to form a vocabulary $\gA(\gS_t)$ for each substructure $\gS_t$. This allows us to formulate the prediction of $\set{v_j}$ as a classification task --  predicting the correct configuration $\gA_t=(\gS_t,\set{v_j})$ from the vocabulary $\gA(\gS_t)$:
        \begin{equation}
            \vp_{\gA_t} = \softmax(\MLP(\vh_{\gS_k}, \valpha_k^a)) \qquad \valpha_k^a = \attention_a\left( \vh_{\gS_k}, \vc_X^\gA \right)
        \end{equation}
        \item[2)] Given the predicted attaching points $\set{v_j}$, we need to find the corresponding atoms $\set{u_j}$ in the substructure $\gS_k$. As the attaching points are always consecutive, there exist at most $2|\gS_k|$ different attachments $M=\set{(u_j,v_j)}$. The probability of a candidate attachment $M$ is computed based on the atom representations $\vh_{u_j}$ and $\vh_{v_j}$ learned by the decoder:
        \begin{equation}
            \vp_M = \softmax\left(\vh_M \cdot \attention_m(\vh_M, \vc_X^\graph)\right) \quad
            \vh_M = \sum_j\nolimits \MLP(\vh_{u_j},\vh_{v_j})
        \end{equation}
    \end{enumerate}
\end{enumerate}
The above three predictions together give an autoregressive factorization of the distribution over the next substructure and its attachment. Each of the three decoding steps depends on the outcome of previous step, and predicted attachments will in turn affect the prediction of subsequent substructures.
During training, we apply teacher forcing to the above generation process, where the generation order is determined by a depth-first traversal over the ground truth substructure tree.
The attachment enumeration is tractable because most of the substructures are small. In our experiments, the average size of attachment vocabulary $|\gA(\gS_t)| < 5$ and the number of candidate attachments is less than 20.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{./decoder.png}
    \caption{Illustration of hierarchical graph decoding. Suppose the decoder is visiting the substructure $\gS_k$. 1) It decides to add a new substructure (\emph{topological prediction}). 2) It predicts that new substructure $\gS_t$ should be a ring (\emph{substructure prediction}) 3) It predicts how this new ring should be attached to the graph (\emph{attachment prediction}). Finally, the decoder moves to $\gS_t$ and repeats the process.}
    \label{fig:decoder}
\end{figure}

\subsection{Hierarchical Graph Encoder}
\label{sec:encoder}
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{./encoder.png}
    \caption{\textbf{Left}: Hierarchical graph encoder. Solid arrows illustrate message passing in each layer. Dashed arrows connect each atom to the substructures it belongs. In the attachment layer, each node $\gA_i$ is a particular attachment configuration of substructure $\gS_i$. \textbf{Right}: Attachment vocabulary for a ring. The attaching points in each configuration (highlighted in red) must be consecutive.} \label{fig:encoder}
    \vspace{-5pt}
\end{figure}

Our encoder represents a molecule $X$ by a hierarchical graph $\gH_X$ in order to support the above decoding process. The hierarchical graph has three components (see Figure~\ref{fig:encoder}):


\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt] 
    \item \textbf{Atom layer}: The atom layer is the molecular graph of $X$ representing how its atoms are connected. Each atom node $v$ is associated with a label $a_v$ indicating its atom type and charge. Each edge $(u,v)$ in the atom layer is labeled with $b_{uv}$ indicating its bond type.
    
    \item \textbf{Attachment layer}: This layer is derived from the substructure tree of molecule $X$. Each node $\gA_i$ in this layer represents a particular attachment configuration of substructure $\gS_i$ in the vocabulary $\gA(S_i)$. Specifically, $\gA_i=(\gS_i,\set{v_j})$ where $\set{v_j}$ are the attaching atoms between $\gS_i$ and its parent $\gS_{d_i}$ in the tree. This layer provides necessary information for the attachment prediction (step 1). Figure~\ref{fig:encoder} illustrates how $\gA_i$ and the vocabulary $\gA(\gS_i)$ look like.
    
    \item \textbf{Substructure layer}: This layer is the same as the substructure tree. This layer provides essential information for the substructure prediction in the decoding process.
\end{enumerate}

We further introduce edges that connect the atoms and substructures between different layers in order to propagate information in between. In particular, we draw a directed edge from atom $v$ in the atom layer to node $\gA_i$ in the attachment layer if $v \in \gS_i$. We also draw edges from node $\gA_i$ to node $\gS_i$ in the substructure layer. 
This gives us the hierarchical graph $\gH_X$ for molecule $X$, which will be encoded by a hierarchical message passing network (MPN) (see Figure~\ref{fig:encoder}). 
The encoder contains three MPNs that encode each of the three layer. We use the MPN architecture from \citet{jin2018learning}.\footnote{We slightly modified their architecture by using LSTM instead of GRU for message propagation due to its better empirical performance. The details are shown in the appendix.} For simplicity, we denote the MPN encoding process as $\MPN_\psi(\cdot)$ with parameter $\psi$.

\textbf{Atom Layer MPN } 
We first encode the atom layer of $\gH_X$ (denoted as $\gH_X^g$). The inputs to this MPN are the embedding vectors $\set{\ve(a_u)}, \set{\ve(b_{uv})}$ of all the atoms and bonds in $X$. During encoding, the network propagates the message vectors between different atoms for $T$ iterations and then outputs the atom representation $\vh_v$ for each atom $v$:
\begin{equation}
\vc_X^\graph = \set{\vh_v} = \MPN_{\psi_1}\left(\gH_X^g, \set{\ve(a_u)}, \set{\ve(b_{uv})} \right)
\end{equation}

\textbf{Attachment Layer MPN } 
The input feature of each node $\gA_i$ in the attachment layer $\gH_X^a$ is an concatenation of the embedding $\ve(\gA_i)$ and the sum of its atom vectors $\set{\vh_v \;|\; v \in \gS_i}$: 
\begin{equation}
\vf_{\gA_i} = \MLP\left( \ve(\gA_i), \sum\nolimits_{v \in \gS_i} \vh_v \right) 
\end{equation}
The input feature for each edge $(\gA_i, \gA_j)$ in this layer is an embedding vector $\ve(d_{ij})$, where $d_{ij}$ describes the relative ordering between node $\gA_i$ and $\gA_j$ during decoding. Specifically, we set $d_{ij}=k$ if node $\gA_i$ is the $k$-th child of node $\gA_j$ and $d_{ij}=0$ if $\gA_i$ is the parent. We then run $T$ iterations of message passing over $\gH_X^a$ to compute the substructure representations:
\begin{equation}
\vc_X^\gA = \set{\vh_{\gA_i}} = \MPN_{\psi_2}\left(\gH_X^a, \set{\vf_{\gA_i}}, \set{\ve(d_{ij})} \right)
\label{eq:mpn}
\end{equation}

\textbf{Substructure Layer MPN } Similarly, the input feature of node $\gS_i$ in this layer is computed as the concatenation of embedding $\ve(\gS_i)$ and the node vector $\vh_{\gA_i}$ from the previous layer. Finally, we run message passing over the substructure layer $\gH_X^s$ to obtain the substructure representations:
\begin{equation}
\vf_{\gS_i} = \MLP\left( \ve(\gS_i), \vh_{\gA_i} \right) \qquad 
\vc_X^\gS = \set{\vh_{\gS_i}} = \MPN_{\psi_3}\left(\gH_X^s, \set{\vf_{\gS_i}}, \set{\ve(d_{ij})} \right)
\end{equation}
In summary, the output of our hierarchical encoder is a set of vectors $\vc_X = \vc_X^\gS \cup \vc_X^\gA \cup \vc_X^\graph$ that represent a molecule $X$ at multiple resolutions. These vectors are input to the decoder attention.

\textbf{Decoder MPN } During decoding, we use the same hierarchical MPN architecture to encode the hierarchical graph $\gH_\graph$ at each step $t$. This gives us the substructure vectors $\vh_{\gS_k}$ and atom vectors $\vh_{v_j}$ in \S\ref{sec:decoder}. All future nodes and edges are masked to ensure the prediction of current substructure and attachment only depends on previously generated outputs.

\subsection{Training}
\label{sec:training}

Our training set contains molecular pairs $(X,Y)$ where each compound $X$ can be associated with multiple outputs $Y$ since there are many ways to modify $X$ to improve its properties. In order to generate diverse outputs, we follow \citet{jin2018learning} and extend our method to a variational translation model $\gF: (X, \vz) \rightarrow Y$ with an additional input $\vz$. The latent vector $\vz$ indicates the intended mode of translation which is sampled from a Gaussian prior $P(\vz)$ during testing.

We train our model using variational inference~\citep{kingma2013auto}. Given a training example $(X,Y)$, we sample $\vz$ from the posterior $Q(\vz | X,Y) = \gN(\vmu_{X,Y}, \vsigma_{X,Y})$. To compute $Q(\vz | X,Y)$, we first encode $X$ and $Y$ into their representations $\vc_X$ and $\vc_Y$ and then compute vector $\vdelta_{X,Y}$ that summarizes the structural changes from molecule $X$ to $Y$ at both atom and substructure level:
\begin{equation}
    \vdelta_{X,Y}^\gS = \sum \vc_Y^\gS - \sum \vc_X^\gS \qquad 
    \vdelta_{X,Y}^\graph = \sum \vc_Y^\graph - \sum \vc_X^\graph
    \label{eq:vae_diff}
\end{equation}
Finally, we compute $[\vmu_{X,Y}, \vsigma_{X,Y}] = \MLP(\vdelta_{X,Y}^\gS, \vdelta_{X,Y}^\graph)$ and sample $\vz$ using reparameterization trick. The latent code $\vz$ is passed to the decoder along with the input representation $\vc_X$ to reconstruct output $Y$. The overall training objective follows a standard conditional VAE:
\begin{equation}
    \loss(X,Y) = - \mathbb{E}_{\vz \sim Q}[\log P(Y | \vz,X)] + \lambda_{\text{KL}}\gD_{\text{KL}}[Q(\vz|X,Y) || P(\vz)]
\end{equation}


\textbf{Conditional Translation } In the above formulation, the model does not know what properties are being optimized during translation. During testing, users cannot change the behavior of a trained model (i.e., what properties should be changed). This may become a limitation of our method in a multi-property optimization setting. Therefore, we extend our method to handle conditional translation where the desired criteria are also fed as input to the translation process. In particular, let $\vg_{X,Y}$ be a translation criteria indicating what properties should be changed. During variational inference, we compute $\vmu_{X,Y}$ and  $\vsigma_{X,Y}$ with an additional input $\vg_{X,Y}$:
\begin{equation}
   [\vmu_{X,Y}, \vsigma_{X,Y}] = \MLP(\vdelta_{X,Y}^\gS, \vdelta_{X,Y}^\graph, \vg_{X,Y})
\end{equation}
We then augment the latent code as $[\vz, \vg_{X,Y}]$ and pass it to the decoder.
During testing, the user can specify their criteria in $\vg_{X,Y}$ to control the outcome (e.g., $Y$ should be drug-like and bioactive).

\begin{figure}
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{Variational Translation (unconditional setting, without target criteria $\vg$)}
\FOR{$(X,Y)$ in the training set}
\STATE Encode molecule $X,Y$ into vectors $\vc_X, \vc_Y$
\STATE Compute $\vmu_{X,Y}, \vsigma_{X,Y}$ from $\vdelta_{X,Y}$
\STATE Sample latent code $\vz \sim Q(\vz | X, Y)$
\STATE Generate molecule $Y$ given $\vc_X$ and $\vz$
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill 
\begin{minipage}{0.48\textwidth}
\vspace{10pt}
\centering
\includegraphics[width=\textwidth]{./conditional.png}
\captionof{figure}{Conditional translation.}
\end{minipage}
\end{figure}

 \section{Experiments}
\label{sec:experiment}
\newcommand\Tstrut{\rule{0pt}{2.3ex}}
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}

We follow the experimental design by \citet{jin2018learning} and evaluate our translation model on their single-property optimization tasks.
As molecular optimization in the real-world often involves different property criteria, we further construct a novel conditional optimization task where the desired criteria is fed as input to the translation process.
To prevent the model from ignoring input $X$ and translating it into arbitrary compound, we require the molecular similarity between $X$ and output $Y$ to be above certain threshold $\mathrm{sim}(X,Y) \geq \delta$ at test time. The molecular similarity is defined as the Tanimoto similarity over Morgan fingerprints~\citep{rogers2010extended} of two molecules.

\textbf{Single-property Optimization } This dataset consists of four different tasks. For each task, we train and evaluate our model on their provided training and test sets. For these tasks, our model is trained under an unconditional setting (without $\vg_{X,Y}$ as input).  

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt] 
\item \textbf{LogP Optimization}: The penalized logP score~\citep{kusner2017grammar} measures the solubility and synthetic accessibility of a compound. In this task, the model needs to translate input $X$ into output $Y$ such that $\mathrm{logP}(Y) > \mathrm{logP}(X)$. We experiment with two similarity thresholds $\delta=\set{0.4, 0.6}$.

\item \textbf{QED Optimization}: The QED score~\citep{bickerton2012quantifying} quantifies a compound's drug-likeness. In this task, the model is required to translate molecules with QED scores from the lower range $[0.7, 0.8]$ into the higher range $[0.9,1.0]$. The similarity constraint is $\mathrm{sim}(X,Y) \geq 0.4$.

\item \textbf{DRD2 Optimization}: This task involves the optimization of a compound's biological activity against dopamine type 2 receptor (DRD2). The model needs to translate inactive compounds ($p < 0.05$) into active compounds ($p \geq 0.5$), where the bioactivity is assessed by a property prediction model from \citet{olivecrona2017molecular}. The similarity constraint is $\mathrm{sim}(X,Y) \geq 0.4$.
\end{itemize}

\textbf{Conditional Optimization } This new task requires the model to translate input $X$ into output $Y$ to satisfy different combination of constraints over its QED and DRD2 scores. We define a molecule $Y$ as drug-like if $\mathrm{QED}(Y) \geq 0.9$ and as DRD2-active if its predicted bioactivity $\mathrm{DRD2}(Y) \geq 0.5$. At test time, our model needs to handle the following two criteria over output molecule $Y$:
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt] 
    \item $Y$ is both drug-like and DRD2-active. Here both properties need to be improved after translation.
    \item $Y$ is drug-like but DRD2-inactive. In this case, DRD2 is an off-target that may cause side effects. Therefore only the drug-likeness should be improved after translation.
\end{enumerate}
As different users may be interested in different settings, we encode the desired criteria as vector $\vg$ and train our model under the conditional translation setup in \S\ref{sec:training}. 
Like single-property tasks, we impose a similarity constraint $\mathrm{sim}(X,Y) \geq 0.4$ for both settings.

Our training set contains 120K molecular pairs and the test set has 780 compounds. For each pair $(X,Y)$, we set $\vg_{X,Y}=\left(\mathbb{I}[Y \text{ is drug-like}], \mathbb{I}[Y \text{ is DRD2-active}]\right)$.
During testing, we translate each compound with $\vg=[1,1],[1,0]$ for each setting.
We note that the first criteria ($\vg=[1,1]$) is the most challenging because there are only 1.6\% of the training pairs with target $Y$ being both drug-like and DRD2-active.
To achieve good performance, the model must learn to transfer the knowledge from other pairs with $\vg_{X,Y}=[1,0],[0,1]$) that partially satisfy the criteria.

\textbf{Evaluation Metrics } Our evaluation metrics include translation accuracy and diversity. Each test molecule $X_i$ is translated $K=20$ times with different latent codes sampled from the prior distribution. On the logP optimization, we select compound $Y_i$ as the final translation of $X_i$ that gives the highest property improvement and satisfies $\mathrm{sim}(X_i,Y_i)\geq\delta$. We then report the average property improvement $\frac{1}{\gD}\sum_i\mathrm{logP}(Y_i)-\mathrm{logP}(X_i)$ over test set $\gD$. For other tasks, we report the translation success rate. A compound is successfully translated if one of its $K$ translation candidates satisfies all the similarity and property constraints of the task.
To measure the diversity, for each molecule we compute the average pairwise Tanimoto distance between all its successfully translated compounds. Here the Tanimoto distance is defined as $\mathrm{dist}(X,Y)=1-\mathrm{sim}(X,Y)$.

\textbf{Baselines } We compare our method (HierG2G) against the baselines including GCPN~\citep{you2018graph}, MMPA~\citep{dalke2018mmpdb} and translation based methods Seq2Seq and JTNN~\citep{jin2018learning}.
Seq2Seq is a sequence-to-sequence model that generates molecules by their SMILES strings. 
JTNN is a graph-to-graph architecture that generates molecules structure by structure, but its decoder is not fully autoregressive. 
We also compare with CG-VAE~\citep{liu2018constrained}, a generative model that decodes molecules atom by atom and optimizes properties in the latent space using gradient ascent.

To make a direct comparison possible between our method and atom-based generation, we further developed an atom-based translation model (AtomG2G) as baseline. 
It makes three predictions in each generation step. First, it predicts whether the decoding process has completed (no more new atoms). If not, it creates a new atom $a_t$ and predicts its atom type. Lastly, it predicts the bond type between $a_t$ and other atoms autoregressively to fully capture edge dependencies~\citep{you2018graphrnn}.
The encoder of AtomG2G encodes only the atom-layer graph and the decoder attention only sees the atom vectors $\vc_X^\graph$. All translation models (Seq2Seq, JTNN, AtomG2G and HierG2G) are trained under the same variational objective (\S\ref{sec:training}). Details of baseline architectures are in the appendix. 

\begin{table}[t]
\centering
\caption{Results on single-property translation tasks. ``Div.'' stands for diversity. ``Succ.'' stands for success rate. ``Improve.'' stands for average property improvement.}
\vspace{-4pt}
\begin{tabular}{lcccccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{ logP ($\mathrm{sim} \geq 0.6$) } & \multicolumn{2}{c}{ logP ($\mathrm{sim} \geq 0.4$) } & \multicolumn{2}{c}{ QED } & \multicolumn{2}{c}{ DRD2 } \Tstrut\Bstrut \\
\cline{2-9}
& Improve. & Div. & Improve. & Div. & Succ. & Div. & Succ. & Div. \Tstrut\Bstrut \\
\hline
JT-VAE & $0.28 \pm 0.79$ & - & $1.03 \pm 1.39$ & - & 8.8\% & - & 3.4\% & - \Tstrut\Bstrut \\
CG-VAE & $0.25 \pm 0.74$ & - & $0.61 \pm 1.09$ & - & 4.8\% & - & 2.3\% & - \Tstrut\Bstrut \\
GCPN & $0.79 \pm 0.63$ & - & $2.49 \pm 1.30$ & - & 9.4\% & 0.216 & 4.4\% & 0.152 \Tstrut\Bstrut \\
MMPA & $1.65 \pm 1.44$ & 0.329 & $3.29 \pm 1.12$ & 0.496 & 32.9\% & 0.236 & 46.4\% & \textbf{0.275} \Tstrut\Bstrut \\
Seq2Seq & $2.33 \pm 1.17$ & 0.331 & $3.37 \pm 1.75$ & 0.471 & 58.5\% & 0.331 & 75.9\% & 0.176 \Tstrut\Bstrut \\
JTNN & $2.33 \pm 1.24$ & 0.333 & $3.55 \pm 1.67$ & 0.480 & 59.9\% & 0.373 & 77.8\% & 0.156 \Tstrut\Bstrut \\
AtomG2G & $2.41 \pm 1.19$ & 0.379 & \textbf{3.98 $\pm$ 1.54} & 0.563 & 73.6\% & 0.421 & 75.8\% & 0.128 \Tstrut\Bstrut \\
\hline
HierG2G & \textbf{2.49 $\pm$ 1.09} & \textbf{0.381} & \textbf{3.98 $\pm$ 1.46} & \textbf{0.564} & \textbf{76.9\%} & \textbf{0.477} & \textbf{85.9\%} & 0.192 \Tstrut\Bstrut \\
\hline
\end{tabular}
\label{tab:prop1}
\vspace{-6pt}
\end{table}

\begin{table}[t]
\centering
\caption{Results on conditional optimization tasks and ablation studies over architecture choices.}
\vspace{-5pt}
\begin{subtable}{0.52\textwidth}
\centering
\caption{Conditional optimization results: $\vg=[1,*]$ means the output $Y$ needs to be drug-like and $\vg=[*,1]$ means it needs to be DRD2-active.}
\begin{tabular}{lcccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{ $\vg=[1,1]$ } & \multicolumn{2}{c}{ $\vg=[1,0]$ }  \Tstrut\Bstrut \\
\cline{2-5}
& Succ. & Div. & Succ. & Div.  \Tstrut\Bstrut \\
\hline
Seq2Seq & 5.0\% & 0.078 & 67.8\% & 0.380  \Tstrut\Bstrut \\
JTNN & 11.1\% & 0.064 & 71.4\% & 0.405  \Tstrut\Bstrut \\
AtomG2G & 12.5\% & 0.031 & 74.5\% & 0.443  \Tstrut\Bstrut \\
\hline
HierG2G & \textbf{13.0\%} & \textbf{0.094} & \textbf{78.5\%} & \textbf{0.480} \Tstrut\Bstrut \\
\hline
\end{tabular}
\label{tab:prop2}
\end{subtable}
~
\begin{subtable}{0.46\textwidth}
\centering
\caption{Ablation study: the importance of hierarchical graph encoding, LSTM MPN architecture and structure-based decoding.}
\begin{tabular}{lcc}
\hline
Method & QED & DRD2  \Tstrut\Bstrut \\
\hline
HierG2G & \textbf{76.9\%} & \textbf{85.9\%}   \Tstrut\Bstrut \\
$\boldsymbol{\cdot}$ atom-based decoder & 76.1\% & 75.0\% \Tstrut\Bstrut \\
$\boldsymbol{\cdot}$ two-layer encoder & 75.8\% & 83.5\%  \Tstrut\Bstrut \\
$\boldsymbol{\cdot}$ one-layer encoder & 67.8\% & 74.1\% \Tstrut\Bstrut \\
$\boldsymbol{\cdot}$ \small{GRU MPN}  & 72.6\% & 83.7\%  \Tstrut\Bstrut \\
\hline
\end{tabular}
\label{tab:ablation}
\end{subtable}
\end{table}

\subsection{Results}

\textbf{Single-property Optimization } As shown in Table~\ref{tab:prop1}, our model achieves the new state-of-the-art on the four translation tasks. In particular, our model significantly outperforms JTNN in both translation accuracy (e.g., 76.9\% versus 59.9\% on the QED task) and output diversity (e.g., 0.564 versus 0.480 on the logP task). While both methods generate molecules by structures, our decoder is autoregressive which can learn more expressive mappings. 
More importantly, our model runs 6.3 times faster than JTNN during decoding. 
Our model also outperforms AtomG2G on three datasets, with over 10\% improvement on the DRD2 task. This shows the advantage of our hierarchical encoding and decoding.

\textbf{Conditional Optimization } For this task, we compare our method with other translation methods: Seq2Seq, JTNN and AtomG2G. All these models are trained under the conditional translation setup where feed the desired criteria $\vg_{X,Y}$ as input.
As shown in Table~\ref{tab:prop2}, our model outperforms other models in both translation accuracy and output diversity. Notably, all models achieved very low success rate on $\vc=[1,1]$ because it has the strongest constraints and only 1.6K of the training pairs satisfy this criteria. In fact, training our model on the 1.6K examples only gives 4.2\% success rate as compared to 13.0\% when trained with other pairs. This shows our conditional translation setup can transfer the knowledge from other pairs with $\vg_{X,Y}=[1,0],[0,1]$.
Figure~\ref{fig:multi-translate} illustrates how the input criteria $\vg$ affects the generated output.

\textbf{Ablation Study } To understand the importance of different architecture choices, we report ablation studies over the QED and DRD2 tasks in Table~\ref{tab:ablation}. We first replace our hierarchical decoder with atom-based decoder of AtomG2G to see how much the structure-based decoding benefits us. We keep the same hierarchical encoder but modified the input of the decoder attention to include both atom and substructure vectors. Using this setup, the model performance decreases by 0.8\% and 10.9\% on the two tasks. We suspect the DRD2 task benefits more from structure-based decoding because biological target binding often depends on the presence of specific functional groups.

Our second experiment reduces the number of hierarchies in our encoder and decoder MPN, while keeping the same hierarchical decoding process. When the top substructure layer is removed, the translation accuracy drops slightly by 0.8\% and 2.4\%. When we further remove the attachment layer, the performance degrades significantly on both datasets. This is because all the substructure information is lost and the model needs to infer what substructures are and how substructure layers are constructed for each molecule.
Implementation details of those ablations are shown in the appendix.

Lastly, we replaced our LSTM MPN with the original GRU MPN used in JTNN. While the translation performance decreased by 4\% and 2.2\%, our method still outperforms JTNN by a wide margin. Therefore we use the LSTM MPN architecture for both HierG2G and AtomG2G baseline. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{./multi_translate.png}
    \caption{Illustration of conditional translation. Our model generates different molecules when the translation criteria changes. When $\vg=[1,1]$, the model indeed generates a compound with high QED and DRD2 scores. When $\vg=[1,0]$, the model predicts another compound inactive to DRD2.}
    \label{fig:multi-translate}
    \vspace{-10pt}
\end{figure} \section{Conclusion}
In this paper, we developed a hierarchical graph-to-graph translation model that generates molecular graphs using chemical substructures as building blocks. In contrast to previous work, our model is fully autoregressive and learns coherent multi-resolution representations. The experimental results show that our method outperforms previous models under various settings. 
\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\newpage
\appendix
\section{Network Architecture}

\textbf{LSTM MPN Architecture } The LSTM MPN is a slight modification from the MPN architecture used in \citet{jin2018learning}. Let $N(v)$ be the neighbors of node $v$, $\vx_v$ the node feature of $v$ and $\vx_{uv}$ be the feature of edge $(u,v)$. During encoding, each edge $(u,v)$ is associated with two messages $\vnu_{uv}$ and $\vnu_{vu}$, representing the message from $u$ to $v$ and vice versa. The messages are updated by an LSTM cell with parameters  $\psi=\set{\mW_\psi^z,\mW_\psi^o,\mW_\psi^r,\mW_\psi}$ defined as follows: 
\begin{algorithm}
\caption{LSTM Message Passing}
\begin{algorithmic}
\FUNCTION{$\LSTM_\psi \left(\vx_u, \vx_{uv}, \set{\vnu_{wu}^{(t)}, \vc_{wu}^{(t)}}_{w \in N(u)\backslash v}\right)$  }
\STATE \vspace{-8pt}\begin{flalign}
\begin{aligned}
\vi_{uv} &= \sigma\left(\mW_\psi^z \left[\vx_u, \vx_{uv}, \sum\nolimits_w \vnu_{wu}^{(t)} \right] + \vb^z\right) \\
\vo_{uv} &= \sigma\left(\mW_\psi^o \left[\vx_u, \vx_{uv}, \sum\nolimits_w \vnu_{wu}^{(t)} \right] + \vb^o\right) \\
\vf_{wu} &= \sigma\left(\mW_\psi^r \left[\vx_u, \vx_{uv}, \vnu_{wu}^{(t)} \right] + \vb^r\right) \\
\vc_{uv}^{(t+1)} &= \vi_{uv} \odot \tanh \left(\mW_\psi \left[\vx_u, \vx_{uv}, \sum\nolimits_w \vnu_{wu}^{(t)} \right] + \vb \right) + \sum\nolimits_w \vf_{wu} \odot \vc_{wu}^{(t)}  \\
\vnu_{uv}^{(t+1)} &= \vo_{uv} \odot \tanh\left( \vc_{uv}^{(t+1)}\right)
\end{aligned} && \nonumber
\end{flalign}
\vspace{-3pt}
\STATE \textbf{Return } $\vnu_{uv}^{(t+1)},\vc_{uv}^{(t+1)}$
\vspace{3pt}
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}

The message passing network $\MPN_{\psi}\left(\gH, \set{\vx_u}, \set{\vx_{uv}} \right)$ over graph $\gH$ is defined as:
\begin{algorithm}
\caption{LSTM MPN with $T$ message passing iterations}
\begin{algorithmic}
\FUNCTION{$\MPN_\psi \left(\gH, \set{\vx_v}, \set{\vx_{uv}}\right)$}
\STATE Initialize messages: $\vnu_{uv}^{0} = \mathbf{0}, \vc_{uv}^{0} = \mathbf{0}$
\FOR{$t=0$ \textbf{to} $T-1$}
\STATE Compute messages $\vnu_{uv}^{(t+1)}, \vc_{uv}^{(t+1)} = \LSTM_{\psi}\left(\vx_u, \vx_{uv}, \set{ \vnu_{wu}^{(t)}, \vc_{wu}^{(t)}}_{w \in N(u) \backslash v}\right)$ for all edges $(u,v)\in\gH$ simultaneously.
\ENDFOR
\STATE \textbf{Return } node representations $\vh_v = \MLP\left(\vx_v, \sum_{u \in N(v)} \vnu_{uv}^{(T)}\right)$
\vspace{3pt}
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}

\textbf{Attention Layer } Our attention layer is a bilinear attention function with parameter $\theta=\set{\mA_\theta}$:
\begin{equation}
    \attention_{\theta}(\vv, \set{\vh_i}) = \sum_i \beta_i \vh_i  \qquad \beta_i = \frac{\exp(\vv^T \mA_\theta \vh_i)}{\sum_j \exp(\vv^T \mA_\theta \vh_j)}
\end{equation}

\textbf{AtomG2G Architecture } AtomG2G is an atom-based translation method that is directly comparable to HierG2G. Here molecules are represented solely as molecular graphs rather than a hierarchical graph with substructures. The encoder of AtomG2G is the same LSTM MPN over molecular graph. This gives us a set of atom vectors $\vc_X^\graph$ representing molecule $X$ only at the atom level.

The decoder of AtomG2G is illustrated in Figure~\ref{fig:atomg2g}. Following \citet{you2018graphrnn,liu2018constrained}, the model generates molecule $\graph$ atom by atom following their breath-first order. During generation, it maintains a FIFO queue $\gQ$ that contains the frontier nodes in the graph (i.e., nodes who still have neighbors to be generated). 
Let $v_t$ be the first node in $\gQ$ and $\graph_t$ be the current graph at step $t$. In each step, the model makes three predictions to expand the graph $\graph_t$:
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt]
\item It predicts whether there will be new atoms attached to $v_t$. If not, the model discards $v$ and move on to the next node in $\gQ$. The generation stops if $\gQ$ is empty.
\item Otherwise, it creates a new atom $u_t$ and predicts its atom type.
\item Lastly, it predicts the bond type between $u_t$ and other frontier nodes in $\gQ$ autoregressively to fully capture edge dependencies~\citep{you2018graphrnn}. Since nodes are generated in breath-first order, there will be no edges between $u_t$ and nodes outside of $\gQ$.
\end{enumerate}

To make those predictions, we use the same LSTM MPN to encode the current graph $\graph_t$. Let $\vh_{v_t}$ be the atom representation of $v_t$. We represent $\graph_t$ as the sum of all its atom vectors $\vh_{\graph_t} = \sum_{v \in \graph_t} \vh_v$. In the first step, we model the probability of expanding a new node from $v_t$ as:
\begin{equation}
    \vp_t = \sigmoid(\MLP(\vh_{v_t}, \vh_{\graph_t}, \valpha_t^d)) \qquad \valpha_t^d = \attention_d\left( [\vh_{v_t},\vh_{\graph_t}], \vc_X^\graph \right)
\end{equation}
In the second step, the atom type of the new node $u_t$ is predicted using another MLP:
\begin{equation}
    \vq_t = \softmax(\MLP(\vh_{v_t}, \vh_{\graph_t}, \valpha_t^s)) \qquad \valpha_t^s = \attention_s\left( [\vh_{v_t},\vh_{\graph_t}], \vc_X^\graph \right)
\end{equation}
In the last step, we predict the bonds between $u_t$ and nodes in $\gQ={a_1,\cdots,a_n}$ sequentially starting with $a_1=v_t$. Specifically, for each atom pair $(u_t,a_k)$, we predict their bond type (single, double, triple or none) as the following:
\begin{eqnarray}
    \vb_{u_t, a_k} &=& \softmax( \MLP(\vh_{\graph_t}, \vh_{u_t}^k, \vh_{a_k}, \valpha_t^b) ) \\ 
    \valpha_t^b &=& \attention_b \left( [\vh_{\graph_t}, \vh_{u_t}^k, \vh_{a_k}], \vc_X^\graph \right)
\end{eqnarray}
where $\vh_{a_k}$ is the atom representation of node $a_k$ and $\vh_{u_t}^k$ is the representation of node $u_t$ at the $k^{\textrm{th}}$ bond prediction. Let $N_k(u_t)$ be node $u_t$'s current neighbor predicted in the first $k$ steps. $\vh_{u_t}^k$ is computed as follows to reflect its local graph structure after $k^{\textrm{th}}$ bond prediction:
\begin{equation}
    \vh_{u_t}^k = \MLP\left(\vx_{u_t}, \sum\nolimits_{w \in N_k(u_t)} \vnu_{w,u_t} \right) \qquad \vnu_{w,u_t} = \MLP(\vh_w, \vx_{w,u_t})
\end{equation}
where $\vx_{u_t}$ is the atom feature of $u_t$ (i.e., predicted atom type) and $\vx_{w,u_t}$ is the bond feature between $w$ and $u_t$ (i.e., predicted bond type). Intuitively, this can be viewed as running one-step message passing at each bond prediction step (i.e., passing the message $\vnu_{w,u_t}$ from $w$ to $u_t$).

AtomG2G is trained under the same variational objective as HierG2G, with the latent code $\vz$ sampled from the posterior $Q(\vz | X, Y)=\gN(\vmu_{X,Y}, \vsigma_{X,Y})$ and $[\vmu_{X,Y}, \vsigma_{X,Y}] = \MLP(\sum \vc_Y^\graph - \sum \vc_X^\graph)$. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./atomg2g.png}
    \caption{Illustration of AtomG2G decoding process. Atoms marked with red circles are frontier nodes in the queue $\gQ$. In each step, the model picks the first node $v_t$ from $\gQ$ and predict whether there will be new atoms attached to $v_t$. If so, it predicts the atom type of new node $u_t$ (atom prediction). Then the model predicts the bond type between $u_t$ and other nodes in $\gQ$ sequentially for $|\gQ|$ steps (bond prediction, $|\gQ|=2$). Finally, it adds the new atom to the queue $\gQ$.}
    \label{fig:atomg2g}
\end{figure}

\section{Experimental Details}

\textbf{Data } The single-property optimization datasets are directly downloaded from the link provided in \citet{jin2018learning}. The training set and substructure vocabulary size for each dataset is listed in Table~\ref{tab:data}.
\begin{table}[ht]
    \centering
    \begin{tabular}{rcccc}
        \hline
        & logP ($\delta=0.6$) & logP ($\delta=0.4$) & QED & DRD2 \Tstrut\Bstrut \\
        \hline
        Training set size & 75K & 99K & 88K & 34K \Tstrut\Bstrut \\
        Test set size & 800 & 800 & 800 & 1000 \Tstrut\Bstrut \\
        Substructure vocabulary $|\gS|$ & 478 & 462 & 307 & 307 \Tstrut\Bstrut \\
        Average attachment vocabulary $|\gA(\gS_t)|$ & 3.68 & 3.50 & 3.62 & 3.30 \Tstrut\Bstrut \\ \hline
    \end{tabular}
    \caption{Training set size and substructure vocabulary size for each dataset.}
    \label{tab:data}
\end{table}
We constructed the multi-property optimization by combining the training set of QED and DRD2 optimization task. The test set contains 780 compounds that are not drug-like and DRD2-inactive. The training and test set is attached as part of the supplementary material.

\textbf{Hyperparameters } For HierG2G, we set the hidden layer dimension to be 270 and the embedding layer dimension 200. We set the latent code dimension $|z|=8$ and KL regularization weight $\lambda_{\textrm{KL}}=0.3$. We run $T=20$ iterations of message passing in each layer of the encoder. 
For AtomG2G, we set the hidden layer and embedding layer dimension to be 400 so that both models have roughly the same number of parameters. We also set $\lambda_{\textrm{KL}}=0.3$ and number of message passing iterations to be $T=20$. We train both models with Adam optimizer with default parameters.

For CG-VAE~\citep{liu2018constrained}, we used their official implementation for our experiments. Specifically, for each dataset, we trained a CG-VAE to generate molecules and predict property from the latent space. This gives us three CG-VAE models for logP, QED and DRD2 optimization tasks, respectively. 
At test time, each compound $X$ is translated following the same procedure as in \citet{jin2018junction}. First, we embed $X$ into its latent representation $\vz$ and perform gradient ascent over $\vz$ to maximize the predicted property score. This gives us $\vz_1,\cdots,\vz_K$ vectors for $K$ gradient steps. Then we decode $K$ molecules from $\vz_1,\cdots,\vz_K$ and select the one with the best property improvement within similarity constraint.
We found that it is necessary to keep the KL regularization weight low ($\lambda_{\mathrm{KL}}=0.005$) to achieve meaningful results. When $\lambda_{\mathrm{KL}}=1.0$, the above gradient ascent procedure always generate molecules very dissimilar to the input $X$.

\textbf{Ablation Study } Our ablation studies are illustrated in Figure~\ref{fig:ablation}. In our first experiment, we changed our decoder to the atom-based decoder of AtomG2G. As the encoder is still hierarchical, we modified the input of the decoder attention to include both atom and substructure vectors. We set the hidden layer and embedding layer dimension to be 300 to match the original model size. 

Our next two experiments reduces the number of hierarchies in both our encoder and decoder MPN. In the two-layer model, molecules are represented by $\vc_X = \vc_X^\graph \cup \vc_X^\gA$. We make topological and substructure predictions based on hidden vector $\vh_{\gA_k}$ instead of $\vh_{\gS_k}$ because the substructure layer is removed. 
In the one-layer model, molecules are represented by $\vc_X = \vc_X^\graph$ and we make topological and substructure predictions based on atom vectors $\sum_{v\in \gS_k} \vh_v$. The hidden layer dimension is adjusted accordingly to match the original model size.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./ablation.png}
    \caption{Illustration of three different ablation studies. \textbf{Left}: Atom-based decoder; \textbf{Middle}: Two-layer encoder; \textbf{Right}: One-layer encoder. }
    \label{fig:ablation}
\end{figure} \end{document}
