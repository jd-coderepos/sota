
\documentclass{article} \usepackage{iclr2022_conference,times}

\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}       \usepackage{amsmath,amsthm,amsfonts,thmtools}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{euscript}



\newcommand{\by}{{\bf y}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bM}{{\bf M}}
\newcommand{\bP}{{\bf P}}
\newcommand{\bS}{{\bf S}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bW}{{\bf W}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bV}{{\bf V}}
\newcommand{\bx}{{\bf x}}
\newcommand{\bA}{{\bf A}}
\newcommand{\bX}{{\bf X}}
\newcommand{\bY}{{\bf Y}}
\newcommand{\bB}{{\bf B}}
\newcommand{\bC}{{\bf C}}
\newcommand{\bc}{{\boldsymbol{\psi}}}
\newcommand{\bh}{{\boldsymbol{\phi}}}
\newcommand{\bo}{{\bf o}}
\newcommand{\ep}{\epsilon}
\newcommand{\ord}{{\mathcal O}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Dt}{{\Delta t}}
\newcommand{\ind}{\mathrm{I}}
\newcommand{\fref}[1] {Fig.~\ref{#1}}
\newcommand{\Tref}[1]{Table~\ref{#1}}
\newcommand{\E}{\EuScript{E}}
\newcommand{\bdel}{\overline{\Delta}}

\newcommand{\bF}{{\bf F}}
\newcommand{\cN}{\EuScript{N}}
\newcommand{\cR}{\EuScript{R}}


\newcommand{\bG}{{\bf G}}

\newcommand{\byli}{{\bf y}^{\ell,i}}
\newcommand{\bzli}{{\bf z}^{\ell,i}}
\newcommand{\cli}{\hat{\sigma}({\bf c}^{\ell,i})}
\newcommand{\Ali}{{\bf A}^{\ell,i}}
\newcommand{\sli}{\sigma({\bf A}^{\ell,i}_{n-1})}
\newcommand{\bwli}{{\bf w}^{\ell,i}}
\newcommand{\cLi}{\hat{\sigma}({\bf c}^{L,i})}
\newcommand{\ALi}{{\bf A}^{L,i}}
\newcommand{\bwLi}{{\bf w}^{L,i}}

\newcommand{\bif}{{\bf f}}

\newcommand{\bD}{{\bf D}}
\newcommand{\bE}{{\bf E}}
\newcommand{\bg}{{\bf g}}

\newcommand{\cW}{\EuScript{W}}
\newcommand{\bom}{{\bf \omega}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{maintheorem}[theorem]{Main Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator{\diag}{diag}

\title{Long Expressive Memory for Sequence \\Modeling}



\author{T. Konstantin Rusch \\
ETH Z\"urich\\
\texttt{trusch@ethz.ch}\\
\And 
Siddhartha Mishra\\
ETH Z\"urich \\
\texttt{smishra@ethz.ch}\\
\And
N. Benjamin Erichson \\
University of Pittsburgh \\
\texttt{erichson@pitt.edu}\\
\And
Michael W. Mahoney \\
ICSI and UC Berkeley\\
\texttt{mmahoney@stat.berkeley.edu}\\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
We propose a novel method called \emph{Long Expressive Memory} (LEM) for learning long-term sequential dependencies.  
LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps.
To derive LEM, we consider a system of \emph{multiscale ordinary differential equations}, as well as a \emph{suitable time-discretization} of this system.
For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a well-known challenge for gradient-based recurrent sequential learning methods. 
We also prove that LEM can approximate a large class of dynamical systems to high accuracy. 
Our empirical results, ranging from image and time-series classification through dynamical systems prediction to keyword spotting and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models.
\end{abstract}



\section{Introduction}
Learning tasks with sequential data as inputs (and possibly outputs) arise in a wide variety of contexts, including computer vision, text and speech recognition, natural language processing, and time series analysis in the sciences and engineering. 
While recurrent gradient-based models have been successfully used in processing sequential data sets, it is well-known that training these models to process (very) long sequential inputs is extremely challenging on account of the so-called \emph{exploding and vanishing gradients problem} \citep{vanish_grad}.
This arises as calculating hidden state gradients entails the computation of an iterative product of gradients over a large number of steps. 
Consequently, this (long) product can easily grow or decay exponentially in the number of recurrent~interactions. 

Mitigation of the exploding and vanishing gradients problem has received considerable attention in the literature. 
A classical approach, used in Long Short-Term Memory (LSTM) \citep{lstm} and Gated Recurrent Units (GRUs) \citep{gru}, relies on \emph{gating mechanisms} and leverages the resulting additive structure to ensure that gradients do not vanish. 
However, gradients might still explode, and learning very long-term dependencies remains a challenge for these architectures \citep{indrnn}. 
An alternative approach imposes constraints on the structure of the hidden weight matrices of the underlying recurrent neural networks (RNNs), for instance by requiring these matrices to be unitary or orthogonal \citep{orthornn,urnn,eurnn,nnRNN}. However, constraining the structure of these matrices might lead to significantly reduced expressivity, i.e., the ability of the model to learn complicated input-output maps. 
Yet another approach relies on enforcing the hidden weights to lie within pre-specified bounds, leading to control on gradient norms. 
Examples include \citet{indrnn}, based on \emph{independent neurons} in each layer, and \citet{coRNN}, based on a network of coupled oscillators. 
Imposing such restrictions on weights might be difficult to enforce, and weight clipping could reduce expressivity significantly. 

This brief survey highlights the challenge of \emph{designing recurrent gradient-based methods for sequence modeling which can mitigate the exploding and vanishing gradients problem, while at the same time being sufficiently expressive and possessing the ability to learn complicated input-output maps efficiently.} 
We seek to address this challenge by proposing a novel gradient-based~method. 

The starting point for our method is the observation that realistic sequential data sets often contain information arranged according to multiple (time, length, etc., depending on the data and task) scales. 
Indeed, if there were only one or two scales over which information correlated, then a simple model with a parameter chosen to correspond to that scale (or, e.g., scale difference) should be able to model the data well.
Thus, it is reasonable to expect that a \emph{multiscale model} should be considered to process efficiently such \emph{multiscale data}. 
To this end, we propose a novel gradient-based architecture, \emph{Long Expressive Memory} (LEM), that is based on a suitable time-discretization of a set of multiscale ordinary differential equations (ODEs). 
For this novel gradient-based method (proposed in Section~\ref{sxn:lem}): 
\begin{itemize}
    \item 
    we derive bounds on the hidden state gradients to prove that LEM mitigates the exploding and vanishing gradients problem (Section~\ref{sec:rig});
    \item 
    we rigorously prove that LEM can approximate a very large class of (multiscale) dynamical systems to arbitrary accuracy (Section~\ref{sec:rig}); and
    \item 
    we provide an extensive empirical evaluation of LEM on a wide variey of data sets, including image and sequence classification, dynamical systems prediction, keyword spotting, and language modeling, thereby demonstrating that LEM outperforms or is comparable to state-of-the-art RNNs, GRUs and LSTMs in each task (Section~\ref{sxn:empirical}).  
\end{itemize}
We also discuss a small portion of the large body of related work (Section~\ref{sxn:related}), and we provide a brief discussion of our results in a broader context (Section~\ref{sxn:discussion}). 
Much of the technical portion of our work is deferred to Supplementary Materials.


\section{Long Expressive Memory}
\label{sxn:lem}

We start with the simplest example of a system of \emph{two-scale ODEs},

Here,  is the continuous time,  are the two time scales,  are the vectors of \emph{slow} and \emph{fast} variables and  is the \emph{input signal}. For simplicity, we set . The dynamic interactions between the neurons are modulated by weight matrices , bias vectors  and a \emph{nonlinear} tanh activation function . Note that  refers to the componentwise product of vectors.

However, two scales (one fast and one slow), may not suffice in representing a large number of scales that could be present in realistic sequential data sets. Hence, we need to generalize \eqref{eq:ode2} to a \emph{multiscale} version. One such generalization is provided by the following set of ODEs,

In addition to previously defined quantities, we need additional weight matrices , bias vectors  and sigmoid activation function . 
As  is monotone, we can set  and , for all , with  to observe that the two-scale system \eqref{eq:ode2} is a special case of \eqref{eq:ode}. 
One can readily generalize this construction to obtain many different scales in \eqref{eq:ode}. 
Thus, we can interpret  in \eqref{eq:ode} as input and state dependent gating functions, which endow ODE \eqref{eq:ode} with \emph{multiple time scales}.
These scales can be learned adaptively (with respect to states) and dynamically (in time). Moreover, it turns out that the multiscale ODE system \eqref{eq:ode} is of the same general form (see {\bf SM}\S\ref{app:HH}) as the well-known Hodgkin-Huxley equations modeling the dynamics of the action potential for voltage-gated ion-channels in biological neurons \citep{HH}. 



Next, we propose a time-discretization of the multiscale ODE system \eqref{eq:ode}, providing a circuit to our sequential model architecture. As is common with numerical discretizations of ODEs, doing so properly is important to preserve desirable properties. 
To this end, we fix , and we discretize \eqref{eq:ode} with the following implicit-explicit (IMEX) time-stepping scheme to arrive at LEM, written in compact form as, 

For steps , the hidden states in LEM \eqref{eq:lem} are , with input state . The weight matrices are  and  and the bias vectors are . We also augment LEM \eqref{eq:lem} with a linear \emph{output state}  with , and .


\section{Related Work} 
\label{sxn:related}

We start by comparing our proposed model, LEM \eqref{eq:lem}, to the widely used LSTM of \citet{lstm}.
Observe that  in \eqref{eq:lem} are similar in form to the \emph{input, forget} and \emph{output} gates in an LSTM (see {\bf SM}\S\ref{app:lstm}), and that LEM \eqref{eq:lem} has exactly the same number of parameters (weights and biases) as an LSTM, for the same number of hidden units. 
Moreover, as detailed in {\bf SM}\S\ref{app:lstm}, we show that by choosing very specific values of the LSTM gates and the  terms in LEM \eqref{eq:lem}, the two models are equivalent. 
However, this analysis also reveals key differences between LEM \eqref{eq:lem} and LSTMs, as they are equivalent only under very stringent assumptions. 
In general, as the different gates in both LSTM and LEM \eqref{eq:lem} are \emph{learned} from data, one can expect them to behave differently. 
Moreover in contrast to LSTM, LEM stems from a discretized ODE system \eqref{eq:ode}, which endows it with (gradient) stable dynamics.  

The use of \emph{multiscale} neural network architectures in machine learning has a long history. 
An early example was provided in \citet{HinPla}, who proposed a neural network with each connection having a fast changing weight for temporary memory and a slow changing weight for long-term learning. 
More recently, one can view convolutional neural networks as multiscale architectures for processing multiple \emph{spatial} scales in data \citep{Kolter}. 

The use of ODE-based learning architectures has also received considerable attention in recent years with examples such as \emph{continuous-time} neural ODEs \citep{neuralODE,continuousnet_TR,queiruga2021compressing} and their recurrent extensions ODE-RNNs \citep{ode_rnn}, as well as RNNs based on discretizations of ODEs \citep{anti,lip_rnn,srnn,lim2021noisy,coRNN,unicornn}. In addition to the specific details of our archiecture, we differ from other discretized ODE-based RNNs in the explicit use of multiple (learned) scales in LEM. 





\section{Rigorous Analysis of LEM}
\label{sec:rig}
\paragraph{Bounds on hidden states.} 
The structure of LEM \eqref{eq:lem} allows us to prove (in {\bf SM}\S\ref{app:hsbdpf}) that its hidden states satisfy the following \emph{pointwise bound}.
\begin{proposition}
\label{prop:1}
Denote  and assume that . Further assume that the initial hidden states are .
Then, the hidden states  of LEM \eqref{eq:lem} are bounded pointwise as,

\end{proposition}

\paragraph{On the exploding and vanishing gradient problem.} 
For any , let , denoted the \emph{combined hidden state}, given by . 
For simplicity of the exposition, we 
consider a \emph{loss function}: , with  being the underlying \emph{ground truth}. The training of our proposed model \eqref{eq:lem} entails computing \emph{gradients} of the above loss function with respect to its underlying weights and biases ,
at every step of the gradient descent procedure. Following \cite{vanish_grad}, one uses chain rule to show,


In general, for recurrent models, the partial gradient , which measures the contribution to the hidden state gradient at step  arising from step  of the model, can behave as , for some  \cite{vanish_grad}. If , then the partial gradient grows \emph{exponentially} in sequence length, for long-term dependencies , leading to the exploding gradient problem. On the other hand, if , then partial gradients decays \emph{exponentially} for , leading to the vanishing gradient problem. Thus, mitigation of the exploding and vanishing gradient problem entails deriving bounds on the gradients. We start with the following upper bound (proved in {\bf SM}\S\ref{app:hsgubpf}), 
\begin{proposition}
\label{prop:2}
Let  be the hidden states generated by LEM \eqref{eq:lem}. We assume that  is chosen to be sufficiently small. Then, the gradient of the loss function  with respect to any parameter  is bounded as 

\end{proposition}
If we choose the hyperparameter  (see SM \eqref{eq:ord} for the order-notation), then one readily observes from \eqref{eq:gbd} that the gradient  is \emph{uniformly bounded} for any sequence length  and the exploding gradient problem is clearly mitigated for LEM \eqref{eq:lem}. Even if one chooses , for some , we show in {\bf SM} Remark \ref{rem:gbd} that the gradient can only grow polynomially (e.g. as  for ), still mitigating the exploding gradient problem.

Following \cite{vanish_grad}, one needs a more precise characterization of the partial gradient 
, for long-term dependencies, i.e., , to show mitigation of the vanishing gradient problem. In {\bf SM}\S\ref{app:hsglb}, we state and prove proposition \ref{prop:3}, which provides a precise formula for the asymptotics of the partial gradient. Here, we illustrate this formula in a special case as a corollary,
\begin{proposition}
\label{prop:3cor}
Let  be the hidden states generated by LEM \eqref{eq:lem} and the ground truth satisfy .
Then, for any  (long-term dependencies) we have,

Here, constants in  depend on only on  \eqref{eq:gbd} and  and are independent of .
\end{proposition}
This formula \eqref{eq:glbo} shows that although the partial gradient can be small, i.e.,
, it is in fact \emph{independent of }, ensuring that long-term dependencies contribute to gradients at much later steps and mitigating the vanishing gradient problem.

\paragraph{Universal approximation of general dynamical systems.} 
The above bounds on hidden state gradients show that the proposed model LEM \eqref{eq:lem} mitigates the exploding and vanishing gradients problem. However, this by itself, does not guarantee that it can learn complicated and realistic input-output maps between sequences. To investigate the \emph{expressivity} of the proposed LEM, we will show in the following proposition that it can approximate \emph{any} dynamical system, mapping an input sequence  to an output sequence , of the (very) general form,

 with  denoting the \emph{hidden} and \emph{output} states, respectively. The input signal is  and maps  and  are Lipschitz continuous. For simplicity, we set the initial state .
 \begin{proposition}
\label{prop:exp1}
For all , let  be given by the dynamical system \eqref{eq:ods} with input signal . Under the assumption that there exists a  such that , for all , then for any given  there exists a LEM of the form \eqref{eq:lem}, with hidden states  and output state , for some  such that 
the following holds,

\end{proposition}
From this proposition, proved in {\bf SM}\S\ref{app:exp1pf}, we conclude that, in principle, the proposed LEM \eqref{eq:lem} can approximate a very large class of dynamical systems. 

\paragraph{Universal approximation of multiscale dynamical systems.} 
While expressing a general form of input-output maps between sequences, the dynamical system \eqref{eq:ods} does not explicitly model dynamics at multiple scales. Instead, here we consider the following two-scale \emph{fast-slow} dynamical system of the general form, 

Here,  and  are the slow and fast time scales, respectively. The underlying maps  are Lipschitz continuous. In the following proposition, proved in {\bf SM}\S\ref{app:exp2pf}, we show that LEM \eqref{eq:lem} can approximate \eqref{eq:2sds} to desired accuracy. 
\begin{proposition}
\label{prop:exp2}
For any , and for all , let  be given by the two-scale dynamical system \eqref{eq:2sds} with input signal . Under the assumption that there exists a  such that , for all , then for any given , there exists a LEM of the form \eqref{eq:lem}, with hidden states  and output state  with  such that 
the following holds,

Moreover, the weights, biases and size (number of neurons) of the underlying LEM \eqref{eq:lem} are \emph{independent} of the time-scale .
\end{proposition}
This argument can be readily generalized to more than two time scales (see {\bf SM} Proposition \ref{prop:mts}). Hence, we show that, in principle, the proposed model LEM \eqref{eq:lem} can approximate multiscale dynamical systems, with model size being \emph{independent} of the underlying timescales. These theoretical results for LEM \eqref{eq:lem} point to the ability of this architecture to learn complicated multiscale input-output maps between sequences, while mitigating the exploding and vanishing gradients problem. Although useful prerequisities, these theoretical properties are certainly not sufficient to demonstrate that LEM \eqref{eq:lem} is efficient in practice. To do this, we perform several benchmark evaluations, and we report the results below. 
\section{Empirical results}
\label{sxn:empirical}

We present a variety of experiments ranging from long-term dependency tasks to real-world applications as well as tasks which require high expressivity of the model.  
Details of the training procedure for each experiment can be found in {\bf SM}\S\ref{app:training_details}. 
As competing models to LEM, we choose two different types of architectures---LSTMs and GRUs---as they are known to excel at expressive tasks such as language modeling and speech recognition, while not performing well on long-term dependency tasks, possibly due to the exploding and vanishing gradients problem.  
On the other hand, we choose state-of-the-art RNNs which are tailor-made to learn tasks with long-term dependencies. Our objective is to evaluate the performance of LEM and compare it with competing models.
All code to reproduce our results can be found at \href{https://github.com/tk-rusch/LEM}{\textbf{https://github.com/tk-rusch/LEM}}.
\paragraph{Very long adding problem.}
We start with the well-known adding problem \citep{lstm}, proposed to test the ability of a model to learn (very) long-term dependencies. The input is a two-dimensional sequence of length , with the first dimension consisting of random numbers drawn from  and with two non-zero entries (both set to ) in the second dimension, chosen at random locations, but one each in both halves of the sequence. The output is the sum of two numbers of the first dimension at positions, corresponding to the two 1 entries in the second dimension. We consider three very challenging cases, namely input sequences with length  and . The results of LEM together with competing models including state-of-the-art RNNs, which are explicitly designed to solve long-term dependencies, are presented in \fref{fig:adding_results}. We observe in this figure that while baseline LSTM is not able to beat the baseline mean-square error of  (the variance of the baseline output ) in any of the three cases, a proper weight initialization for LSTM, the so-called \emph{chrono}-initialization of \cite{warp} leads to much better performance in all cases. For , all other architectures (except baseline LSTM) beat the baseline convincingly. However for , only LEM, chrono-LSTM and coRNN are able to beat the baseline. In the extreme case of , only LEM and chrono-LSTM are able to beat the baseline. Nevertheless, LEM outperforms chrono-LSTM by converging faster (in terms of number of training steps) and attaining a lower test MSE than chrono-LSTM in all three cases. 
\begin{figure}[ht!]
\begin{minipage}{.33\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_2000_new.pdf}
\end{minipage}\begin{minipage}{.33\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_5000_new.pdf}
\end{minipage}\begin{minipage}{.33\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_10000_new.pdf}
\end{minipage}
\caption{Results on the very long adding problem for LEM, coRNN, DTRIV \citep{dtriv}, FastGRNN \citep{fastrnn}, LSTM and LSTM with chrono initialization \citep{warp} based on three very long sequence lengths , i.e., ,  and .}
\label{fig:adding_results}
\end{figure}

\paragraph{Sequential image recognition.}
We consider three experiments based on two widely-used image recognition data sets, i.e., MNIST \citep{mnist} and CIFAR-10 \citep{cifar}, where the goal is to predict the correct label after reading in the whole sequence. The first two tasks are based on MNIST images, which are flattened along the rows to obtain sequences of length . In sequential MNIST (sMNIST), the sequences are fed to the model one pixel at a time in streamline order, while in permuted sequential MNIST (psMNIST), a fixed random permutation is applied to the sequences, resulting in much longer dependency than for sMNIST. We also consider the more challenging noisy CIFAR-10 (nCIFAR-10) experiment \citep{anti}, where CIFAR-10 images are fed to the model row-wise and flattened along RGB channels, resulting in -dimensional sequences, each of length . Moreover, a random noise padding is applied after the first  inputs to produce sequences of length . Hence, in addition to classifying the underlying image, a model has to store this result for a long time. In \Tref{tab:image}, we present the results for LEM on the three tasks together with other SOTA RNNs, which were explicitly designed to solve long-term dependency tasks, as well as LSTM and GRU baselines. We observe that LEM outperforms all other methods on sMNIST and nCIFAR-10. Additionally on psMNIST, LEM performs as well as coRNN, which has been SOTA among single-layer RNNs on this task.

\begin{table}[ht!]
\caption{Test accuracies on sMNIST, psMNIST and nCIFAR-10, where  denotes the total number of parameters of the corresponding model. Results of other models are taken from the respective original paper referenced in the main text, except that the results for LSTM are taken from \citet{scornn}, for GRU from \citet{GRU_results} and the results indicated by  are added by us.}
\label{tab:image}
\centering
\begin{tabular}{llllll}
\toprule
\cmidrule(r){1-6}
\multirow{2}{*}{Model} &  \multicolumn{3}{c}{MNIST} & \multicolumn{2}{c}{CIFAR-10} \\
\cmidrule(r){2-4}\cmidrule(r){5-6}  
& sMNIST & psMNIST &\# units /  & nCIFAR-10 & \# units /  \\
\midrule
GRU & 99.1\% & 94.1\% & 256 / 201k & 43.8\%& 128 / 88k\\
LSTM & 98.9\% & 92.9\% & 256 / 267k & 11.6\% & 128 / 116k \\
chrono-LSTM & 98.9\% & 94.6\% & 128 / 68k & 55.9\% & 128 / 116k \\
anti.sym. RNN &  98.0\% & 95.8\% & 128 / 10k & 48.3\% & 256 / 36k\\
Lipschitz RNN & 99.4\% & 96.3\% & 128 / 34k & 57.4\% & 128 / 46k \\
expRNN &  98.4\% &  96.2\%& 360 / 69k & 52.9\%& 360 / 103k \\
coRNN & 99.3\% & \textbf{96.6}\% & 128/ 34k & 59.0\% & 128 / 46k\\
\textbf{LEM} & \textbf{99.5}\% & \textbf{96.6}\% & 128 / 68k & \textbf{60.5}\% & 128 / 116k\\
    \bottomrule
  \end{tabular}
\end{table}
\paragraph{EigenWorms: Very long sequences for genomics classification.}
The goal of this task \citep{eigenworms} is to classify worms as belonging to either the wild-type or four different mutants, based on  very long sequences (length ) measuring the motion of a worm. In addition to the nominal length, it was empirically shown in \citet{unicornn} that the EigenWorms sequences exhibit actual very long-term dependencies (i.e., longer than 10k). 

Following \citet{log_ode} and \cite{unicornn}, we divide the data into a train, validation and test set according to a  ratio. In \Tref{tab:worms}, we present results for LEM together with other models. As the validation and test sets, each consist of only  sequences, we report the mean (and standard deviation of) accuracy over  random initializations to rule out lucky outliers. We observe from this table that LEM outperforms all other methods, even the -layer UnICORNN architecture, which has been SOTA on this task. 
\begin{table}[t!]
\caption{Test accuracies on EigenWorms using  re-trainings of each best performing network (based on the validation set), where all other results are taken from \citet{unicornn} except that the NRDE result is taken from \cite{log_ode} and the results indicated by  are added by us.}
\label{tab:worms}
\centering
\begin{tabular}{llll}
\toprule
\cmidrule(r){1-4}
Model &  test accuracy & \# units & \# params \\
\midrule
NRDE & 83.8\%  3.0\% & 32 & 35k \\
expRNN & 40.0\%  10.1\% & 64 & 2.8k \\
IndRNN (2 layers) & 49.7\%  4.8\% & 32 & 1.6k \\
LSTM & 38.5\%  10.1\% & 32 & 5.3k \\
BiLSTM+1d-conv & 40.5\%  7.3\% & 22 & 5.8k \\
chrono-LSTM & 82.6 \%  6.4\% & 32 & 5.3k \\
coRNN & 86.7\%  3.0\%&32 & 2.4k \\
UnICORNN (2 layers) & 90.3\%  3.0\% & 32 & 1.5k\\
\textbf{LEM} & \textbf{92.3}\%  \textbf{1.8}\% & 32 & 5.3k \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Healthcare application: Heart-rate prediction.}
In this experiment, one predicts the heart rate from a time-series of measured PPG data, which is part of the TSR archive \citep{ai_healthcare} and has been collected at the Beth Isreal Deaconess medical center. The data set, consisting of  sequences, each of length , is divided into a train, validation and test set according to a \%,\%,\% ratio, \citep{log_ode,unicornn}. The results, presented in \Tref{tab:medical}, show that LEM outperforms the other competing models, including having a test  error of  less than the SOTA UnICORNN. 
\begin{table}[ht!]
  \caption{Test  error on heart-rate prediction using PPG data. All results are obtained by running the same code and using the same fine-tuning protocol.}
  \label{tab:medical}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-4}
    Model & test  error & \# units & \# params \\
    \midrule
LSTM  & 9.93 &  & k \\
chrono-LSTM  & 3.31 &  & k \\
expRNN &1.63 &  & k \\
IndRNN (3 layers) & 1.94 &  & k \\
coRNN & 1.61 &  & k \\
UnICORNN (3 layers) & 1.31 &  & k \\
\textbf{LEM} & \textbf{0.85} &  & k \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Multiscale dynamical system prediction.}
The FitzHugh-Nagumo system \citep{Fitznag} 
is a prototypical model for a two-scale fast-slow nonlinear dynamical system, with fast variable  and slow variable  and  determining the slow-time scale. This \emph{relaxation-oscillator} is an approximation to the Hodgkin-Huxley model \citep{HH} of neuronal action-potentials under an external signal . With , , ,  and initial data , with  randomly drawn from , we numerically approximate \eqref{eq:FHN} with the explicit Runge-Kutta method of order  in the interval  and generate  training and validation and  test sequences, each of length , to complete the data set. The results, presented in \Tref{tab:FHN}, show that LEM not only outperforms LSTM by a factor of  but also all other methods including coRNN, which is tailormade for oscillatory time-series. This reinforces our theory by demonstrating efficient approximation of multiscale dynamical systems with LEM.     
\begin{table}[t!]
  \caption{Test  error on FitzHugh-Nagumo system prediction. All results are obtained by running the same code and using the same fine-tuning protocol.}
  \label{tab:FHN}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-4}
    Model &  error () & \# units & \# params \\
    \midrule
LSTM  & 1.2 &  & k\\
expRNN  & 2.3 &  & k  \\
LipschitzRNN  & 1.8 &  & k   \\
FastGRNN  & 2.2 &  & k \\
coRNN  & 0.4 &  & k \\
\textbf{LEM}  & {\bf 0.2} &  & k \\
\bottomrule
\end{tabular}
\end{table}
\paragraph{Google12 (V2) keyword spotting.}
The Google Speech Commands data set V2 \citep{google12} is a widely used benchmark for keyword spotting, consisting of  words, sampled at a rate of  kHz from  second utterances of  speakers. We focus on the -label task (Google12) and follow the pre-defined splitting of the data set into train/validation/test sets and test different sequential models. In order to ensure comparability of different architectures, we do not use performance-enhancing tools such as convolutional filtering or multi-head attention. From \Tref{tab:google12}, we observe that both LSTM and GRU, widely used models in this context, perform very well with a test accuracy of around . Nevertheless, LEM is able to outperform both on this task and provides the best performance. 
\begin{table}[ht!]
  \caption{Test accuracies on Google12. All results are obtained by running the same code and using the same fine-tuning protocol.}
  \label{tab:google12}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-4}
    { Model} &  test accuracy & \# units & \# params \\
    \midrule
tanh-RNN & 73.4\% &  & k\\
anti.sym. RNN & 90.2\% &  & k\\
LSTM  & 94.9\% &  & k\\
GRU  & 95.2\% &  & k  \\
FastGRNN  & 94.8\%&  & k \\
expRNN & 92.3\% &  & k \\
coRNN  &94.7\% &  & k \\
\textbf{LEM}  & \textbf{95.7}\% &  & k \\
\bottomrule
\end{tabular}
\end{table}
\paragraph{Language modeling: Penn Tree Bank corpus.}
Language modeling with the widely used small scale Penn Treebank (PTB) corpus \citep{ptb_corpus}, preprocessed by \citet{ptb_prepro}, has been identified as an excellent task for testing the expressivity of recurrent models \citep{nnRNN}. To this end, in \Tref{tab:ptb_char}, we report the results of different architectures, with a similar number of hidden units, on the PTB char-level task and observe that RNNs, designed explicitly for learning long-term dependencies, perform significantly worse than LSTM and GRU. On the other hand, LEM is able to outperform both LSTM and GRU on this task by some margin (a test bpc of  in contrast with approximately a bpc of ). In fact, LEM provides the smallest test bpc among all reported single-layer recurrent models on this task, to the best of our knowledge. 
This superior performance is further illustrated in \Tref{tab:ptb_word}, where the test perplexity for different models on the PTB word-level task is presented. 
We observe that not only does LEM significantly outperform (by around ) LSTM, but it also provides again the best performance among all single layer recurrent models, including the recently proposed TARNN \citep{tarnn}. 
Moreover, the single-layer results for LEM are better than reported results for multi-layer LSTM models, such as in \citet{var_drop} (-layer LSTM,  units each:  test perplexity) or \citet{tcn} (-layer LSTM,  units each:  test perplexity).


\begin{table}[ht!]
  \caption{Test bits-per-character (bpc) on PTB character-level for single layer LEM and other single layer RNN architectures. Other results are taken from the papers cited accordingly in the table, while the results for coRNN are added by us.}
  \label{tab:ptb_char}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-2}
    { Model} &  test bpc & \# units & \# params \\
    \midrule
anti.sym RNN \citep{lip_rnn}  & 1.60 & 1437 & 1.3M\\
Lipschitz RNN \citep{lip_rnn} & 1.42 & 764 & 1.3M \\
expRNN \citep{nnRNN} &  1.51 & 1437 & 1.3M \\
coRNN  & 1.46 & 1024 & 2.3M\\
nnRNN \citep{nnRNN}  &  1.47 & 1437 & 1.3M \\
LSTM \citep{zoneout} & 1.36  & 1000 & 5M \\
GRU \citep{tcn} & 1.37  & 1024 & 3M \\
\textbf{LEM} & \textbf{1.25} & 1024 & 5M \\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[t!]
  \caption{Test perplexity on PTB word-level for single layer LEM and other single layer RNN architectures.}
  \label{tab:ptb_word}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-2}
    { Model} &  test perplexity & \# units & \# params \\
    \midrule
Lipschitz RNN \citep{lip_rnn} & 115.4 & 160 & 76k \\
FastRNN \citep{tarnn}  &  115.9 & 256 & 131k \\
LSTM \citep{tarnn} & 116.9 & 256 & 524k\\
SkipLSTM \citep{tarnn} &  114.2 & 256 & 524k \\
TARNN \citep{tarnn} & 94.6 & 256 & 524k \\
\textbf{LEM} & \textbf{72.8} & 256 & 524k \\
\bottomrule
\end{tabular}
\end{table}



\section{Discussion}
\label{sxn:discussion}

The design of a gradient-based model for processing sequential data that can learn tasks with long-term dependencies while retaining the ability to learn complicated sequential input-output maps is very challenging. 
In this paper, we have proposed \emph{Long Expressive Memory} (LEM), a novel recurrent architecture, with a suitable time-discretization of a specific multiscale system of ODEs \eqref{eq:ode} serving as the circuit to the model. By a combination of theoretical arguments and extensive empirical evaluations on a diverse set of learning tasks, we demonstrate that LEM is able to learn long-term dependencies while retaining sufficient expressivity for efficiently solving realistic learning tasks. 

It is natural to ask why LEM performs so well. A part of the answer lies in the mitigation of the exploding and vanishing gradients problem. Proofs for gradient bounds \eqref{eq:gbd},\eqref{eq:glbo} reveal a key role played by the hyperparameter . We observe from {\bf SM} \Tref{tab:hyperparameters_rounded} that small values of  might be needed for problems with very long-term dependencies, such as the EigenWorms dataset. On the other hand, no tuning of the hyperparameter  is necessary for several tasks such as language modeling, keyword spotting and dynamical systems prediction and a default value of  yielded very good performance. The role and choice of the hyperparameter  is investigated extensively in {\bf SM}\S\ref{app:dt}. However, mitigation of exploding and vanishing gradients problem alone does not explain high expressivity of LEM. In this context, we proved that LEMs can approximate a very large class of multiscale dynamical systems. Moreover, we provide experimental evidence in {\bf SM}\S\ref{app:ms} to observe that LEM not only expresses a range of scales, as it is designed to do, but also these scales contribute proportionately to the resulting multiscale dynamics. Furthermore, empirical results presented in {\bf SM}\S\ref{app:ms} show that this ability to represent multiple scales correlates with the high accuracy of LEM. We believe that this combination of gradient stable dynamics, specific model structure, and its multiscale resolution can explain the observed performance of LEM. 

We conclude with a comparison of LEM and the widely-used gradient-based LSTM model. 
In addition to having exactly the same number of parameters for the same number of hidden units, our experiments show that LEMs are better than LSTMs on expressive tasks such as keyword spotting and language modeling, while also providing significantly better performance on long-term dependencies. This robustness of the performance of LEM with respect to sequence length paves the way for its application to learning many different sequential data sets where competing models might not perform satisfactorily.

\clearpage

\section*{Acknowledgements.} The research of TKR and SM was performed under a project that has received funding from the European Research Council (ERC) under the European Unionâ€™s Horizon 2020 research and innovation programme (grant agreement No. 770880). NBE and MWM would like to
acknowledge IARPA (contract W911NF20C0035), NSF, and ONR for providing partial support of this work. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. 

The authors thank Dr. Ivo Danihelka (DeepMind) for pointing out that the hidden states for LEM satisfy the maximum principles \eqref{eq:mp2}, \eqref{eq:mp3}.





\bibliography{refs}
\bibliographystyle{iclr2022_conference}

\appendix
\newpage
\begin{center}
{\bf Supplementary Material for:}\\
Long Expressive Memory for Sequence Modeling
\end{center}
\section{Training details}
\label{app:training_details}
All experiments were run on CPU, namely Intel Xeon Gold 5118 and AMD EPYC 7H12, except for Google12, PTB character-level and PTB word-level, which were run on a GeForce RTX 2080 Ti GPU. All weights and biases of LEM \eqref{eq:lem} are initialized according to , where  is the number of hidden units. 

\begin{table}[ht!]
  \caption{Rounded hyperparameters of the best performing LEM architecture for each experiment. If no value is given for , it means that  is fixed to  and no fine-tuning is performed on this hyperparameter.}
  \label{tab:hyperparameters_rounded}
  \centering
  \begin{tabular}{lccc}
    \toprule
    \cmidrule(r){1-4}
    experiment & learning rate & batch size  &  \\
    \midrule
Adding () &  &  & \\
sMNIST &  &  &  \\
psMNIST &  &  &  \\
nCIFAR-10  &  &  &  \\
EigenWorms  &  &  &  \\
Healthcare &  &  &  \\
FitzHugh-Nagumo & &  & / \\
Google12 &  &  & / \\
PTB character-level &  &  & / \\
PTB word-level &  &  & / \\

    \bottomrule
  \end{tabular}
\end{table}

The hyperparameters are selected based on a random search algorithm, where we present the rounded hyperparameters for the best performing LEM model (\emph{based on a validation set}) on each task in \Tref{tab:hyperparameters_rounded}.

We base the training for the PTB experiments on the following language modelling code: \href{https://github.com/deepmind/lamb}{https://github.com/deepmind/lamb}, where we fine-tune, based on a random search algorithm, only the learning rate, input-, output- and state-dropout, -penalty term and the maximum gradient~norm.

We train LEM for  epochs on sMNIST, psMNIST and nCIFAR-10, after which we decrease the learning rate by a factor of  and proceed training for  epochs. Moreover, we train LEM for ,  as well as  epochs on EigenWorms, Google12 and FitzHugh-Nagumo. We decrease the learning rate by a factor of 10 after  epochs on Google12. On the Healthcare task, we train LEM for 250 epochs, after which we decrease the learning rate by a factor of  and proceed training for  epochs.
\section{Further Experimental Results}
\label{app:fer}
\subsection{On the choice of the hyperparameter .}
\label{app:dt}
The hyperparameter  in LEM \eqref{eq:lem} measures the maximum allowed (time) step in the discretization of the multi-scale ODE system \eqref{eq:ode}. In propositions \ref{prop:1}, \ref{prop:2} and \ref{prop:3cor}, this hyperparameter  plays a key role in the bounds on the hidden states \eqref{eq:hsb} and their gradients \eqref{eq:gbd}. In particular, setting  will lead to hidden states and gradients, that are bounded uniformly with respect to the underlying sequence length . However, these upper bounds on the hidden states and gradients account for \emph{worst-case} scenarios and can be very pessimistic for the problem at hand.

\begin{figure}[ht!]
\centering
\begin{minipage}[t]{.48\textwidth}
\includegraphics[width=1.\textwidth]{figures/eigenworms_ablation.pdf}
\caption{Sensitivity study on hyperparameter  in \eqref{eq:lem} using the EigenWorms experiment.}
\label{fig:eworms_abl}
\end{minipage}
\hspace{0.0025\textwidth}
\begin{minipage}[t]{.48\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_fixed_dt.pdf}
\caption{Average (over ten different initializations each) test mean-square error on the adding problem of LEM for different sequence lengths , where the hyperparameter  of LEM \eqref{eq:lem} is fixed to .}
\label{fig:adding_fixed_dt}
\end{minipage}
\end{figure}

Thus, in practice, we determine  through a hyperparameter tuning procedure as described in section \ref{app:training_details}. To this end, we perform a random search within  and present the resulting optimal values of  for each of the considered data sets in \Tref{tab:hyperparameters_rounded}. From this table, we observe that for data sets such as PTB, FitzHugh-Nagumo and Google 12
we do not need any tuning of  and a default value of  resulted in very good empiricial performance. On the other data sets such as sMNIST, nCIFAR-10 and the healthcare example, where the sequence length () is larger, we observe that values of  yielded the best performance. The notable exception to this was for the EigenWorms data set, with a very long sequence length of  as well as demonstrated very long range dependencies in the data, see \cite{unicornn}. Here, a value of  resulted in the best observed performance. To further investigate the role of the hyperparameter  in the EigenWorms experiment, we perform a sensitivity study where the value of  is varied and the corresponding accuracy of the trained LEM is observed. The results of this sensitivity study are presented in \fref{fig:eworms_abl}, where we plot the test accuracy (Y-axis) vs. the value of  (X-axis). From this figure, we observe that the accuracy is rather poor for  but improves monotonically as  is reduced till a value of approximately , after which it saturates. Thus, in this case, a value of  (for sequence length ) suffices to yield the best empirical performance. 

Given this observation, we further test whether  suffices for other problems with long-term dependencies. To this end, we consider the adding problem and vary the input sequence length by an order of magnitude, i.e., from  to . The value of  is now fixed at  and the resulting test loss (Y-axis) vs the number of training steps (X-axis) is plotted in \fref{fig:adding_fixed_dt}. We see from this figure that this value of  sufficed to yield very small average test errors for this problem for all considered sequence lengths . Thus, empirically a value of  in the range  yields very good performance. 

Even if we set , it can happen for very long sequences  that the gradient can be quite small from the gradient asymptotic formula \eqref{eq:glbo}. This might lead to saturation in training, resulting in long training times. However, we do not observe such long training times for very long sequence lengths in our experiment. To demonstrate this, we again consider \fref{fig:adding_fixed_dt} where the number of training steps (X-axis) is plotted for sequence lengths that vary an order of magnitude. The figure clearly shows that the approximately the same number of training steps are needed to attain a low test error, irrespective of the sequence length. This is further buttressed in \fref{fig:adding_results}, where similar number of training steps where needed for obtaining the same very low test error, even for long sequence lengths, with  up to . Moreover, from section \ref{app:training_details}, we see that the number of epochs for different data sets is independent of the sequence length. For instance, only  epochs were necessary for EigenWorms with a sequence length of  and  whereas  epochs were required for the FitzHugh-Nagumo system with a . 

\subsection{Multiscale Behavor of LEM.}
\label{app:ms}
LEM \eqref{eq:lem} is designed to represent multiple scales, with terms  being explicitly designed to learn possible multiple scales. In the following, we will investigate if in practice, LEM learns multiple scales and uses them to yield the observed superior empirical performance with respect to competing models. 

To this end, we start by recalling the proposition \ref{prop:exp2} where we showed that in principle, LEM can learn the two underlying timescales of a \emph{fast-slow} dynamical system (see proposition \ref{prop:mts} for a similar result for the universal approximation of a -time scale (with ) dynamical system with LEM). Does this hold in practice ? To further investigate this issue, we consider the FitzHugh-Nagumo dynamical system \eqref{eq:FHN} which serves as a prototype for a two-scale dynamical system. We consider this system \eqref{eq:FHN} with the two time-scales being  and  and train LEM for this system. In \fref{fig:FHN_ms_hist}, we plot the empirical histogram that bins the ranges of learned scales  (for all  and ) and counts the number of occurrences of  in each bin. From this figure, we observe that there is a clear concentration of learned scales around the values  and , which exactly correspond to the underlying fast and slow time scales. Thus, for this model problem, LEM is exactly learning what it is designed to do and is able to learn the underlying time scales for this particular problem.
\begin{figure}[ht!]
\centering
\includegraphics[width=7cm]{figures/FHN_ms_hist.pdf}
\caption{Histogram of  and  for all  and  of LEM \eqref{eq:lem} after training on the FitzHugh-Nagumo fast-slow system \eqref{eq:FHN} using .}
\label{fig:FHN_ms_hist}
\end{figure}

Nevertheless, one might argue that these learnable mutliple scales  are not necessary and a single scale would suffice to provide good empirical performance. We check this possibility on the FitzHugh-Nagumo data set by simply setting  (with  being the vector with all entries set to ), for all  and tuning the hyperparameter . The comparative results are presented in \Tref{tab:FHN_wo_ms}. We see from this table by not allowing for learnable  and simply setting them to a single scale parameter  and tuning this parameter only leads to results that are comparable to the baseline LSTM model. On the other hand, learning  resulted in an error that is a factor of  less than the baseline LSTM test error. Thus, we demonstrate the importance of the ability of the proposed LEM model to learn multiple scales in this example.  
\begin{table}[h!]
\caption{Test  error on FitzHugh-Nagumo system prediction.}
\label{tab:FHN_wo_ms}
\centering
\begin{tabular}{llll}
\toprule
\cmidrule(r){1-4}
Model &  error  & \# units & \# params \\
\midrule
LSTM & 1.2 & 16 & 1k  \\
LEM w/o multi-scale & 1.1 & 16 & 1k \\
LEM & 0.2 & 16 & 1k  \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.48\textwidth]{figures/google12_hist_dt1.pdf}
\includegraphics[width=0.48\textwidth]{figures/google12_hist_dt2.pdf}
\caption{Histogram of  and  for all  and  of LEM \eqref{eq:lem} after training on the Google12 data set}
\label{fig:g12_hist}
\end{figure}

Hence, the multiscale resolution of LEM seems essential for the fast-slow dynamical system. Does this multiscale resolution also appear for other datasets and can it explain aspects of the observed empirical performance ? To this end, we consider the Google12 Keyword spotting data set and start by pointing out that given the spatial (with respect to input dimension ) and temporal (with respect to sequence length ) heterogeneities, a priori, it is unclear if the underlying data has a multiscale structure. We plot the empirical histograms of  in \fref{fig:g12_hist} to observe that even for this problem, the terms  are expressed over a range of scales, amounting to  orders of magnitude. Thus, a range of scales are present in the trained LEM even for this example, but do they affect the empirical performance of LEM ? We investigate this question by performing an ablation study and reporting the results in \fref{fig:clip_ms_g12}. In this study, we clip the values of  to lie within the range , for  and plot the statistics of the observed test accuracy of LEM. We observe from \fref{fig:clip_ms_g12} that by clipping   to lie near the default (single scale) value of  results in very poor empirical performance of an accuracy of . Then the accuracy jumps to around  when an order of magnitude range for   is considered, before monotonically and slowly increasing to yield the best empirical performance for the largest range of values of  , considered in this study. A closer look at the empirical histograms plotted in \fref{fig:g12_hist} reveal that the proportion of occurrences of  decays as a \emph{power law}, and not exponentially, with respect to the scale amplitude. This, together with results presented in \fref{fig:clip_ms_g12} suggest that not only do a range of scales occur in learned , the small scales also contribute proportionately to the dynamics and enable the increase in performance shown in \fref{fig:clip_ms_g12}.  
\begin{figure}[ht!]
\centering
\includegraphics[width=7cm]{figures/clip_ms_g12.pdf}
\caption{Average (and standard deviation of) test accuracies of 5 runs each for LEM on Google12, where  and  in \eqref{eq:lem} are clipped to the ranges  for  during training.}
\label{fig:clip_ms_g12}
\end{figure}

Finally, in \fref{fig:hist1}, we plot the empirical histograms of  and  for the learned LEM on the sMNIST data set to observe that again a range of scales are observed and the observed occurrences of  and  at each scale decays as a power law with respect to scale amplitude. Hence, we have sufficient empirical evidence to claim that the multi-scale resolution of LEM seems essential to its observed performance. However, further investigation is required to elucidate the precise mechanisms through this multiscale resolution enables superior performance, particularly on problems where the multiscale structure of the underlying data may not be explicit. 
\begin{figure}[ht!]
\centering
\includegraphics[width=0.48\textwidth]{figures/smnist_hist_dt1.pdf}
\includegraphics[width=0.48\textwidth]{figures/smnist_hist_dt2.pdf}
\caption{Histogram of  and  for all  and  of LEM \eqref{eq:lem} after training on the sMNIST data set}
\label{fig:hist1}
\end{figure}
\subsection{On gradient-stable initialization.}
\label{sec:chrono}
Specialized weight initialization is a popular tool to increase the performance of RNNs on long-term dependencies tasks. One particular approach is the so-called chrono initialization \citep{warp} for LSTMs, where all biases are set to zero except for the bias of the forget gate as well as the input gate ( and  in the LSTM \eqref{eq:lstm}), which are sampled from

where  denotes the maximal temporal dependency of the underlying sequential data.
We can see in \Tref{tab:worms} that the chrono initialization significantly improves the performance of LSTM on the EigenWorms task. Hence, we are interested in extending the chrono initialization to LEMs. One possible manner for doing this is as follows: Initialize all biases of LEM to zero except for  in \eqref{eq:lem}, which is sampled from



\begin{table}[h!]
\caption{Test accuracies on EigenWorms using  re-trainings of each best performing network (based on the validation set), where we train LSTM and LEM with and without chrono intialization, as well as LEM without chrono initialization but with tuned .}
\label{tab:worms_chrono}
\centering
\begin{tabular}{llllll}
\toprule
\cmidrule(r){1-6}
Model &  test accuracy & \# units & \# params & chrono & tuning \\
\midrule
LSTM & 38.5\%  10.1\% & 32 & 5.3k & NO & / \\
LSTM & 82.6 \%  6.4\% & 32 & 5.3k & YES & / \\
LEM & 57.9\%  7.7\% & 32 & 5.3k & NO & NO \\
LEM & 88.2\%  6.9\% & 32 & 5.3k & YES & NO \\
LEM & 92.3\%  1.8\% & 32 & 5.3k & NO & YES \\
    \bottomrule
  \end{tabular}
\end{table}

We test the chrono initialization for LEM on the EigenWorms dataset, where we train LEM (without tuning , i.e., setting ), with and without chrono initialization. We provide the results in \Tref{tab:worms_chrono}, where we show again the results of LSTM with and without chrono initialization as well as the LEM result with tuned  and without chrono initialization from \Tref{tab:worms} for comparison. 
We see from \Tref{tab:worms_chrono} that when  is fixed to , the chrono initialization significantly improves the result of LEM. However, if we tune , but do not use the chrono initialization, we significantly improve the performance of LEM again. We further remark that tuning  as well as using chrono initialization for LEM does not improve the results obtained with simply tuning  in LEM. Thus, we conclude that chrono initialization can successfully be adapted to LEM. However, tuning  (which controls the gradients) is still advisable in order to obtain the best possible results.






\section{Relation between LEM and The Hodgkin-Huxley equations}
\label{app:HH}
We observe that the multiscale ODEs \eqref{eq:ode}, on which LEM is based, are a special case of the following ODE system,

As remarked in the main text, it turns out the well-known Hodgkin-Huxley equations \citet{HH}, modeling the the dynamics of the action potential of a biological neuron can also be written down in the abstract form \eqref{eq:ode1}, with ,  and the variables  modeling the voltage and  modeling the concentration of Potassium activation, Sodium activation and Sodium inactivation channels. 

The exact form of the different functions in \eqref{eq:ode1} for the Hodgkin-Huxley equations is given by,

with input current  and constants , whose exact values can be read from \citep{HH}. 

Thus, the multiscale ODEs \eqref{eq:ode} and the Hodgkin-Huxley equations are special case of the same general family \eqref{eq:ode1} of ODEs. Moreover, the \emph{gating functions} , that model voltage-gated ion channels in the Hodgkin-Huxley equations, are similar in form to  in \eqref{eq:ode}.  

It is also worth highlighting the differences between our proposed model LEM (and the underlying ODE system \eqref{eq:ode}) and the Hodgkin-Huxley ODEs modeling the dynamics of the neuronal action potential. Given the complicated form of the nonlinearites  in the Hodgkin-Huxley equations \eqref{eq:HH}, we cannot use them in designing any learning model. Instead, building on the abstract form of \eqref{eq:ode1}, we propose \emph{bespoke} non-linearities in the ODE \eqref{eq:ode} to yield a tractable learning model, such as LEM \eqref{eq:lem}. Moreover, it should be emphasized that the Hodgkin-Huxley equations only model the dynamics of a single neuron (with a scalar voltage and 3 ion channels), whereas the hidden state dimension  of \eqref{eq:ode} can be arbitrary. 

\section{Relation between LEM and LSTM}
\label{app:lstm}
The well-known LSTM \citep{lstm} (in its mainly-used version using a forget gate \citep{lstm_w_forget}) is given by, 

Here, for any ,  is the hidden state and  is the so-called \emph{cell state}. The vectors  are the \emph{input, forget} and \emph{output} gates, respectively.  is the input signal and the weight matrices and bias vectors are given by  and , respectively. 

It is straightforward to relate LSTM \eqref{eq:lstm} and LEM \eqref{eq:lem} by first setting the cell state , for all  and the hidden state .

We further need to assume that the input state  and the forget state has to be . Finally, the output state of the LSTM \eqref{eq:lstm} has to be

Under these assumptions and by setting , we can readily observe that the LEM \eqref{eq:lem} and LSTM \eqref{eq:lstm} are equivalent.

A different interpretation of LEM, in relation to LSTM, is as follows; LEM can be thought of a variant of LSTM but with two cell states  per unit and no output gate. The input gates are  and  and the forget gates are coupled to the input gates. Given that the state  is fed into the update for the state , one can think of one of the cell states sitting above the other, leading to a more sophisticated recursive update for LEM \eqref{eq:lem}, when compared to LSTM \eqref{eq:lstm}. 


\begin{table}[h!]
\caption{Test accuracies on EigenWorms using  re-trainings of each best performing network (based on the validation set) for LSTMs with -scaled input and forget gates, as well as LSTMs with sub-sampling routines, baseline LSTM and LEM.}
\label{tab:dt_scaled_lstms}
\centering
\begin{tabular}{llll}
\toprule
\cmidrule(r){1-4}
Model & test accuracy & \# units & \# params \\
\midrule
t-BPTT LSTM & 57.9\%  7.0\% &32 & 5.3k\\
sub-samp. LSTM &  69.2\%  8.3\% &32 & 5.3k\\
LSTM & 38.5\%  10.1\% & 32 & 5.3k  \\
-LSTM v & 53.3\%  8.2\% & 32 & 5.3k  \\
-LSTM v & 56.9\%  6.7\% & 32 & 5.3k  \\
LEM & 92.3\%  1.8\% & 32 & 5.3k \\
    \bottomrule
  \end{tabular}
\end{table}

Another key difference between LEM and LSTM is the scaling of the learnable gates in LEM by the small hyperparameter . It is natural to examine whether scaling LSTM with such a small hyperparameter  will improve its performance on sequential tasks with long-term dependencies. To this end, we propose to \emph{scale} the input and forget gate of an LSTM with a small hyperparameter in two different ways, where we denote the new forget gate as  and the new input gate as . The first version is (-LSTM v)

while the second version is (-LSTM v)


We can see in \Tref{tab:dt_scaled_lstms} that both -scaled versions of the LSTM lead to some improvements over the baseline LSTM for very long-sequence Eigenworms dataset, while still performing very poorly when compared to LEM. Moreover, we can see that standard sub-sampling routines, such as a truncation of the BPTT algorithm or random sub-sampling, applied to LSTMs lead to better improvements than -scaling the forget and input gate.


\section{Supplement to the rigorous analysis of LEM}
\label{sec:rigan}
In this section, we will provide detailed proofs of the propositions in Section \ref{sec:rig} of the main article. We start with the following simplifying notation for various terms in LEM \eqref{eq:lem},

Note that for all , . 
With the above notation, LEM \eqref{eq:lem} can be written componentwise, for each component  as,

Moreover, we will use the following \emph{order}-notation,



Note that the techniques of proof in this following three sub-sections burrow heavily from those introduced in \citet{coRNN}.
\subsection{Proof of Proposition \ref{prop:1} of main text.}
\label{app:hsbdpf}


First, we prove Proposition \ref{prop:1}, which yields the bound \eqref{eq:hsb} for the hidden states of LEM.
\begin{proof}
The proof of the bound \eqref{eq:hsb} is split into 2 parts. We start with the first equation in \eqref{eq:lemc} and rewrite it as,

Noting that the activation functions are such that , for all  and , for all  and using the fact that , we have from the above expression that,

By a symmetric argument, one can readily show that,

Combining the above inequalities yields,

Iterating \eqref{eq:mp1} over  and using  for all  leads to,


An argument, identical to the derivation of \eqref{eq:mp2}, but for the hidden state  yields,

Thus, we have shown that the hidden states remain in the interval , irrespective of the sequence length. 

Next, we will use the following elementary identities in the proof,

for any , and also,


We fix  and multiply the first equation in \eqref{eq:lemc} with  and apply \eqref{eq:id} to obtain,

    
We fix  in the elementary identity \eqref{eq:id1} to yield, 

Applying this to the inequality for  leads to,    
    
    
Using the fact that  for all ,  and that , we obtain from the last line of the previous equation that,

Iterating the above estimate over , for any  and setting  yields,

Taking a square root in the above inequality yields,

with  defined in the expression \eqref{eq:hsb}. 

We can repeat the above argument with the hidden state  to obtain,

Combining \eqref{eq:en100} and \eqref{eq:en101} with the pointwise bounds \eqref{eq:mp2} and \eqref{eq:mp3} yields the desired bound \eqref{eq:hsb}. 
\end{proof}
\subsection{Proof of Proposition \ref{prop:2} of main text.}
\label{app:hsgubpf}
\begin{proof}


We can apply the chain rule repeatedly (for instance as in \cite{vanish_grad}) to obtain,

Here, the notation  refers to taking the partial derivative of  with respect to the parameter , while keeping the other arguments constant.

A straightforward application of the product rule yields,

For any , a tedious yet straightforward computation yields the following representation formula,

Here  is a matrix whose entries are given below. For any , we have,

Similarly,  is a matrix whose entries are given below. For any , we have,

Using the fact that,

the pointwise bounds \eqref{eq:hsb}, the notation  for all , the definition of  \eqref{eq:gbd} and the definition of matrix norms, we obtain that,

By similar calculations, we obtain,

Applying \eqref{eq:grad9} and \eqref{eq:grad10} in the representation formula \eqref{eq:grad6} and observing that  and , we obtain.

With 


Using the expression \eqref{eq:grad5} with the above inequality yields,


Next, we choose  small enough such that the following holds, 

for any .

Hence applying \eqref{eq:grad12} in \eqref{eq:grad11}, we obtain,


For the sake of definiteness, we fix any  and set  in the following. The following bounds for any other choice of  can be derived analogously. Given this, it is straightforward to calculate from the structure of LEM \eqref{eq:lem} that entries of the vector  are given by,

Hence, by the pointwise bounds \eqref{eq:hsb}, we obtain from \eqref{eq:grd2} that


Finally, it is straightforward to calculate from the loss function  that

Therefore, using the pointwise bounds \eqref{eq:hsb} and the notation , we obtain


Applying \eqref{eq:grd1}, \eqref{eq:grd3} and \eqref{eq:grd5} in the definition \eqref{eq:grad2} yields,

Observing that , we see that  and . Therefore, \eqref{eq:grd6} can be estimated for any  by, 


Applying the bound \eqref{eq:grd6} in \eqref{eq:grad2} leads to the following bound on the total gradient,

which is the desired bound \eqref{eq:gbd} for .



Moreover, for \emph{long-term dependencies} i.e., , we can set , with  independent of sequence length , in \eqref{eq:grd61} to obtain the following bound on the partial gradient, 

\end{proof}
\begin{remark}
\label{rem:gbd}
The bound \eqref{eq:gbd} on the total gradient depends on , with  being the sequence length and , a hyperparameter which can either be chosen a priori or determined through a hyperparameter tuning procedure. The proof of the bound \eqref{eq:grd8} relies on  being sufficiently small. It would be natural to choose , for some . Substituting this expression in \eqref{eq:gbd} leads to a bound of the form, 


If , then clearly  i.e., the total gradient is bounded. Clearly, the exploding gradient problem is mitigated in this case.

On the other hand, if  takes another value, for instance  which is empirically observed during the hyperparameter training (see Section \ref{app:dt}, then we can readily observe from \eqref{eq:gbdord1} that . Thus in this case, the gradient can grow with sequence length  but only linearly and not exponentially. Thus, the exploding gradient problem is also mitigated in this case. 

\end{remark}
\subsection{Proof of Proposition \ref{prop:3cor} of main text.}
\label{app:hsglb}
To mitigate the vanishing gradient problem, we need to obtain a more precise characterization of the gradient  defined in \eqref{eq:grad2}. For the sake of definiteness, we fix any  and set  in the following. The following formulas for any other choice of  can be derived analogously. Moreover, for simplicity of notation, we set the target function . 

Proposition \ref{prop:3cor} is a straightforward corollary of the following, 
\begin{proposition}
Let  be the hidden states generated by LEM \eqref{eq:lem}, then we have the following representation formula for the hidden state gradient,
\label{prop:3}

Here, the constants in  could depend on  defined in \eqref{eq:gbd} (main text).
\end{proposition}
\begin{proof}
The starting point for deriving an asymptotic formula for the hidden state gradient  is to observe from the representation formula \eqref{eq:grad6}, the bound \eqref{eq:grad10} on matrices  and the order notation \eqref{eq:ord} that,

as long as  is independent of .

By using induction and the bounds \eqref{eq:grad9},\eqref{eq:grad10}, it is straightforward to calculate the following representation formula for the product, 

Recall that we have set . Hence, by the expressions \eqref{eq:grd4} and \eqref{eq:grd2}, a direct but tedious calculation leads to,

Therefore, by substituting the above expression into the representation formula \eqref{eq:glb2} yields the desired formula \eqref{eq:glb}.

In order to prove the formula \eqref{eq:glbo} (see Proposition \ref{prop:3cor} of main text), we focus our interest on long-term dependencies i.e., . Then, a closer perusal of the expression in \eqref{eq:glb3}, together with the pointwise bounds \eqref{eq:hsb} which implies that , results in the following,

Similarly, we also obtain,

Combining \eqref{eq:glb4} and \eqref{eq:glb5} results in the desired asymptotic bound \eqref{eq:glbo}.
\end{proof}
\begin{remark}
\label{rem:dtval}
The upper bound on the gradient \eqref{eq:gbd} and the gradient asymptotic formula \eqref{eq:glbo} impact the choice of the timestep hyperparameter . For sequence length , if we choose , with , we see from Remark \ref{rem:gbd} that the upper bound on the total gradient scales like . On the other hand, from \eqref{eq:glbo}, the gradient contribution from long-term dependencies will scale like . Hence, a small value of , will ensure that the gradient with respect to long-term dependencies will be . However, the total gradient will behave like  and possibly blow up fast. Similarly, setting  leads to a bounded gradient, while the contributions from long-term dependencies decay as fast as . Hence, one has to find a value of  that balances both these requirements. Equilibrating them leads to , ensuring that the total gradient grows sub-linearly while long-term dependencies still contribute with a sub-linear decay. This value is very close to the empirically observed value of  which also ensures that the total gradient grows linearly and the contribution of long-term dependencies decays sub-linearly in the sequence length .
\end{remark}
\subsection{Proof of Proposition \ref{prop:exp1}}
\label{app:exp1pf}
\begin{proof}
To prove this proposition, we have to construct hidden states , output state , weight matrices  and bias vectors  such that LEM \eqref{eq:lem} with output state  approximates the dynamical system \eqref{eq:ods}. 



Let  and , be parameters to be defined later. By the theorem for universal approximation of continuous functions with neural networks with the tanh activation function  \citep{BAR1}, given , there exist weight matrices  and bias vector  such that the tanh neural network defined by,

approximates the underlying function  in the following manner, 

Similarly, one can readily approximate the identity function  with a tanh neural network of the form,

 such that


Next, we define the following dynamical system,

with initial states .

Using the approximation bound \eqref{eq:exp11}, we derive the following bound, 

Here,  is the Lipschitz constant of the function  on the compact set . Note that one can readily prove using the fact that , bounds \eqref{eq:exp11}, \eqref{eq:exp12} and the assumption , that . 


Iterating the above inequality over  leads to the bound,


Hence, using the Lipschitz continuity of the output function  in \eqref{eq:ods}, one obtains,

with  being the Lipschitz constant of the function  on the compact set .

Next, we can use the universal approximation theorem for neural networks again to conclude that given a tolerance , there exist weight matrices  and bias vector  such that the tanh neural network defined by,

approximates the underlying output function  in the following manner, 

Now defining, 

we obtain from \eqref{eq:exp19} and \eqref{eq:exp18} that, 


Next, we introduce the notation, 






From \eqref{eq:exp14}, we see that


Thus from \eqref{eq:exp17} and \eqref{eq:exp111}, we have

Define the function  by,

The function, defined above, is clearly Lipschitz continuous. We can apply the universal approximation theorem for tanh neural networks to find, for any given tolerance , weight matrices  and bias vector  such that the following holds,

Denote , then from \eqref{eq:exp113} and \eqref{eq:exp114}, we obtain that

Combining this estimate with \eqref{eq:exp111} yields,


Now, we collect all ingredients to define the LEM that can approximate the dynamical system \eqref{eq:ods}. To this end, we define hidden states  as 

with . These hidden states are evolved by the dynamical system,

and the output state is calculated by, 


Finally, we can recast the dynamical system \eqref{eq:exp116}, \eqref{eq:exp117} as a LEM of the form \eqref{eq:lem} for the hidden states , defined in \eqref{eq:exp116}, with the following parameters,
Now, define the hidden states  for all  by the LEM \eqref{eq:lem} with the following parameters, 

Here,  is defined as 

with  is such that 

The nature of the sigmoid function guarantees the existence of such a  for any . As  decays exponentially fast, we set it to  in the following for notational simplicity. 

It is straightforward to verify that the output state of the LEM \eqref{eq:lem} with parameters given in \eqref{eq:exp13} is . 

Therefore, from \eqref{eq:exp115} and by setting ,  and

we prove the desired bound \eqref{eq:exp1}. 




\end{proof}
\subsection{Proof of Proposition \ref{prop:exp2}}
\label{app:exp2pf}
\begin{proof}
The proof of this proposition is based heavily on the proof of Proposition \ref{prop:exp1}. Hence, we will highlight the main points of difference. 

As the steps for approximation of a general Lipschitz continuous output map are identical to the corresponding steps in the proof of proposition \ref{prop:exp1} (see the steps from Eqns. \eqref{eq:exp18} to \eqref{eq:exp115}), we will only consider the following linear output map for convenience herein, 

Let  and , be parameters to be defined later. By the theorem for universal approximation of continuous functions with neural networks with the tanh activation function , given , there exist weight matrices  and bias vector  such that the tanh neural network defined by,

approximates the underlying function  in the following manner, 

Next, we define the following map, 

for any .

By the universal approximation theorem, given 
, there exist weight matrices  and bias vector  such that the tanh neural network defined by,

approximates the function  \eqref{eq:exp22} in the following manner, 

Note that the sizes of the neural network  can be made independent of the small parameter  by simply taking the sum of the neural networks approximating the functions  and  with tanh neural networks. As neither of these functions depend on the small parameter , the sizes of the corresponding neural networks are independent of the small parameter too. 

Next, as in the proof of proposition \ref{prop:exp1}, one can readily approximate the identity function  with a tanh neural network of the form,

 such that

and with the same weights and biases, one can approximate the identity function  with the tanh neural network,

 such that


Next, we define the following dynamical system,

with hidden states  and with initial states .

We derive the following bounds,

and

where the last inequality follows by using the previous inequality together with \eqref{eq:exp25} and \eqref{eq:exp26}. 

As , it is easy to see from \eqref{eq:exp22} that . Therefore, the last inequality reduces to,

Adding we obtain,

where,


Iterating over  leads to the bound,

Here,  are the Lipschitz constants of the functions  on the compact set . Note that one can readily prove using the zero values of initial states, the bounds \eqref{eq:exp24}, \eqref{eq:exp26} and the assumption , that . 

Using the definition of the output function \eqref{eq:exp2} and the bound \eqref{eq:exp29} that, 

Defining the dynamical system,

By multiplying suitable matrices to \eqref{eq:exp25}, we obtain that,

Finally, in addition to  defined in \eqref{eq:exp15}, for any given , we introduce  defined by 

The existence of a unique  follows from the fact that the sigmoid function  is monotone. Next, we define the two vectors  as


We are now in a position to define the LEM of form \eqref{eq:lem}, which will approximate the two-scale dynamical system \eqref{eq:2sds}. To this end, we define the hidden states  such that  and . The parameters for the corresponding LEM of form \eqref{eq:lem} given by,

and with following parameters defining the output states,

yields an output state .  

It is straightforward to observe that . Hence, the desired bound \eqref{eq:exp2} follows from \eqref{eq:exp29} by choosing,


\end{proof}

The proof of proposition \ref{prop:exp2} can be readily extended to prove the following proposition about a general -scale dynamical system of the form, 

Here, , with , are the -time scales of the dynamical system \eqref{eq:msds}. We assume that the underlying maps  are Lipschitz continuous. We can prove the following proposition, 
\begin{proposition}
\label{prop:mts}
For all , let  be given by the -scale dynamical system \eqref{eq:msds} with input signal . Under the assumption that there exists a  such that , for all , then for any given , there exists a LEM of the form \eqref{eq:lem}, with hidden states  and output state  with  such that 
the following holds,

Moreover, the weights, biases and size (number of neurons) of the underlying LEM \eqref{eq:lem} are \emph{independent} of the time-scales .
\end{proposition}

\section{LEMs emulate Heterogeneous multiscale methods for ODEs}
\label{app:hmm}
Following \cite{Kuhn_book}, we consider the following prototypical example of a fast-slow system of ordinary differential equations, 

Here  are the fast and slow variables respectively and  is a small parameter. Note that we have rescaled time and are interested in the dynamics of the slow variable  in the time interval . 

A naive time-stepping numerical scheme for \eqref{eq:kode} requires a time step size . Thus, the computation will entail time updates . Hence, one needs a multiscale ODE solver to approximate the solutions of the system \eqref{eq:kode}. One such popular ODE solver can be derived by using the Heterogenous multiscale method (HMM); see \citet{Kuhn_book} and references therein. This in turns, requires using two time stepping schemes, a \emph{macro} solver for the slow variable, with a time step  of the form,

Here, the time step  is independent of the small parameter . 

Moreover, the fast variable is updated using a \emph{micro} solver of the form,

Note that the micro time step size  and the number of micro time steps  are assumed to independent of the small parameter .

It is shown in \cite{Kuhn_book} (Chapter 10.8) that for any given small tolerance , one can choose a macro time step , a micro time step , the number  of micro time steps, the number  of macro time steps, independent of , such that the discrete states  approximate the slow-variable  (with ) of the fast-slow system \eqref{eq:kode} to the desired accuracy of . 

Our aim is to show that we can construct a LEM of the form \eqref{eq:lem} such that the states , defined in \eqref{eq:hmm1}, \eqref{eq:hmm2} can be approximated to arbitrary accuracy. By combining this with the accuracy of HMM, we will prove that LEMs can approximate the solutions of the fast-slow system \eqref{eq:kode} to desired accuracy, independent of the small parameter  in \eqref{eq:kode}. 
\begin{proposition}
\label{prop:hmm}
Let , for , be the states defined by the HMM dynamical system \eqref{eq:hmm1}, \eqref{eq:hmm2}. For any given , there exists a LEM of the form \eqref{eq:lem} with hidden states , where  and output states  such that the following holds, 

\end{proposition}
\begin{proof}
We start by using iteration on the micro solver \eqref{eq:hmm2} from  to  to derive the following, 

As , we have that . 

By the universal approximation theorem for tanh neural networks, for any given tolerance , there exist weight matrices  and bias vector  such that the tanh neural network defined by,

approximates the underlying function  in the following manner, 


Next, we define the following map, 


By the universal approximation theorem, given 
, there exist weight matrices  and bias vector  such that the tanh neural network defined by,

approximates the function  \eqref{eq:phmm3} in the following manner, 


Next, as in the proof of propositions \ref{prop:exp1} \ref{prop:exp2}, one can readily approximate the identity function  with a tanh neural network of the form,

 such that

and with the same weights and biases, one can approximate the identity function  with the Tanh neural network,

 such that

Then, we define the following dynamical system, 

with hidden states  and with initial states .

Completely analogously as in the derivation of \eqref{eq:exp29}, we can derive the following bound,

with constant . 

Defining the dynamical system,

By multiplying suitable matrices to \eqref{eq:phmm9}, we obtain that,


In addition to  defined in \eqref{eq:exp15}, for , we introduce  defined by 

Similarly for , we introduce  defined by 

The existence of unique  and  follows from the fact that the sigmoid function  is monotone.

Next, we define the two vectors  as

We define the LEM of form \eqref{eq:lem}, which will approximate the HMM \eqref{eq:hmm1},\eqref{eq:hmm2}. To this end, we define the hidden states  such that  and . The parameters for the corresponding LEM of form \eqref{eq:lem} given by,

The output states are defined by,


It is straightforward to observe that . Hence, the desired bound \eqref{eq:phmm} follows from \eqref{eq:phmm8} by choosing,


\end{proof}





\end{document}
