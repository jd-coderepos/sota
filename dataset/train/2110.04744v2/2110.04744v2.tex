
\documentclass{article} \usepackage{iclr2022_conference,times}

\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}       \usepackage{amsmath,amsthm,amsfonts,thmtools}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{euscript}



\newcommand{\by}{{\bf y}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bM}{{\bf M}}
\newcommand{\bP}{{\bf P}}
\newcommand{\bS}{{\bf S}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bW}{{\bf W}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bV}{{\bf V}}
\newcommand{\bx}{{\bf x}}
\newcommand{\bA}{{\bf A}}
\newcommand{\bX}{{\bf X}}
\newcommand{\bY}{{\bf Y}}
\newcommand{\bB}{{\bf B}}
\newcommand{\bC}{{\bf C}}
\newcommand{\bc}{{\boldsymbol{\psi}}}
\newcommand{\bh}{{\boldsymbol{\phi}}}
\newcommand{\bo}{{\bf o}}
\newcommand{\ep}{\epsilon}
\newcommand{\ord}{{\mathcal O}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Dt}{{\Delta t}}
\newcommand{\ind}{\mathrm{I}}
\newcommand{\fref}[1] {Fig.~\ref{#1}}
\newcommand{\Tref}[1]{Table~\ref{#1}}
\newcommand{\E}{\EuScript{E}}
\newcommand{\bdel}{\overline{\Delta}}

\newcommand{\bF}{{\bf F}}
\newcommand{\cN}{\EuScript{N}}
\newcommand{\cR}{\EuScript{R}}


\newcommand{\bG}{{\bf G}}

\newcommand{\byli}{{\bf y}^{\ell,i}}
\newcommand{\bzli}{{\bf z}^{\ell,i}}
\newcommand{\cli}{\hat{\sigma}({\bf c}^{\ell,i})}
\newcommand{\Ali}{{\bf A}^{\ell,i}}
\newcommand{\sli}{\sigma({\bf A}^{\ell,i}_{n-1})}
\newcommand{\bwli}{{\bf w}^{\ell,i}}
\newcommand{\cLi}{\hat{\sigma}({\bf c}^{L,i})}
\newcommand{\ALi}{{\bf A}^{L,i}}
\newcommand{\bwLi}{{\bf w}^{L,i}}

\newcommand{\bif}{{\bf f}}

\newcommand{\bD}{{\bf D}}
\newcommand{\bE}{{\bf E}}
\newcommand{\bg}{{\bf g}}

\newcommand{\cW}{\EuScript{W}}
\newcommand{\bom}{{\bf \omega}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{maintheorem}[theorem]{Main Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator{\diag}{diag}

\title{Long Expressive Memory for Sequence \\Modeling}



\author{T. Konstantin Rusch \\
ETH Z\"urich\\
\texttt{trusch@ethz.ch}\\
\And 
Siddhartha Mishra\\
ETH Z\"urich \\
\texttt{smishra@ethz.ch}\\
\And
N. Benjamin Erichson \\
University of Pittsburgh \\
\texttt{erichson@pitt.edu}\\
\And
Michael W. Mahoney \\
ICSI and UC Berkeley\\
\texttt{mmahoney@stat.berkeley.edu}\\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
We propose a novel method called \emph{Long Expressive Memory} (LEM) for learning long-term sequential dependencies.  
LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps.
To derive LEM, we consider a system of \emph{multiscale ordinary differential equations}, as well as a \emph{suitable time-discretization} of this system.
For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a well-known challenge for gradient-based recurrent sequential learning methods. 
We also prove that LEM can approximate a large class of dynamical systems to high accuracy. 
Our empirical results, ranging from image and time-series classification through dynamical systems prediction to keyword spotting and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models.
\end{abstract}



\section{Introduction}
Learning tasks with sequential data as inputs (and possibly outputs) arise in a wide variety of contexts, including computer vision, text and speech recognition, natural language processing, and time series analysis in the sciences and engineering. 
While recurrent gradient-based models have been successfully used in processing sequential data sets, it is well-known that training these models to process (very) long sequential inputs is extremely challenging on account of the so-called \emph{exploding and vanishing gradients problem} \citep{vanish_grad}.
This arises as calculating hidden state gradients entails the computation of an iterative product of gradients over a large number of steps. 
Consequently, this (long) product can easily grow or decay exponentially in the number of recurrent~interactions. 

Mitigation of the exploding and vanishing gradients problem has received considerable attention in the literature. 
A classical approach, used in Long Short-Term Memory (LSTM) \citep{lstm} and Gated Recurrent Units (GRUs) \citep{gru}, relies on \emph{gating mechanisms} and leverages the resulting additive structure to ensure that gradients do not vanish. 
However, gradients might still explode, and learning very long-term dependencies remains a challenge for these architectures \citep{indrnn}. 
An alternative approach imposes constraints on the structure of the hidden weight matrices of the underlying recurrent neural networks (RNNs), for instance by requiring these matrices to be unitary or orthogonal \citep{orthornn,urnn,eurnn,nnRNN}. However, constraining the structure of these matrices might lead to significantly reduced expressivity, i.e., the ability of the model to learn complicated input-output maps. 
Yet another approach relies on enforcing the hidden weights to lie within pre-specified bounds, leading to control on gradient norms. 
Examples include \citet{indrnn}, based on \emph{independent neurons} in each layer, and \citet{coRNN}, based on a network of coupled oscillators. 
Imposing such restrictions on weights might be difficult to enforce, and weight clipping could reduce expressivity significantly. 

This brief survey highlights the challenge of \emph{designing recurrent gradient-based methods for sequence modeling which can mitigate the exploding and vanishing gradients problem, while at the same time being sufficiently expressive and possessing the ability to learn complicated input-output maps efficiently.} 
We seek to address this challenge by proposing a novel gradient-based~method. 

The starting point for our method is the observation that realistic sequential data sets often contain information arranged according to multiple (time, length, etc., depending on the data and task) scales. 
Indeed, if there were only one or two scales over which information correlated, then a simple model with a parameter chosen to correspond to that scale (or, e.g., scale difference) should be able to model the data well.
Thus, it is reasonable to expect that a \emph{multiscale model} should be considered to process efficiently such \emph{multiscale data}. 
To this end, we propose a novel gradient-based architecture, \emph{Long Expressive Memory} (LEM), that is based on a suitable time-discretization of a set of multiscale ordinary differential equations (ODEs). 
For this novel gradient-based method (proposed in Section~\ref{sxn:lem}): 
\begin{itemize}
    \item 
    we derive bounds on the hidden state gradients to prove that LEM mitigates the exploding and vanishing gradients problem (Section~\ref{sec:rig});
    \item 
    we rigorously prove that LEM can approximate a very large class of (multiscale) dynamical systems to arbitrary accuracy (Section~\ref{sec:rig}); and
    \item 
    we provide an extensive empirical evaluation of LEM on a wide variey of data sets, including image and sequence classification, dynamical systems prediction, keyword spotting, and language modeling, thereby demonstrating that LEM outperforms or is comparable to state-of-the-art RNNs, GRUs and LSTMs in each task (Section~\ref{sxn:empirical}).  
\end{itemize}
We also discuss a small portion of the large body of related work (Section~\ref{sxn:related}), and we provide a brief discussion of our results in a broader context (Section~\ref{sxn:discussion}). 
Much of the technical portion of our work is deferred to Supplementary Materials.


\section{Long Expressive Memory}
\label{sxn:lem}

We start with the simplest example of a system of \emph{two-scale ODEs},
\begin{equation}
	\label{eq:ode2}
	\frac{d\by}{dt} = \tau_y \left(\sigma\left(\bW_y \bz + \bV_y \bu + \bb_y\right) - \by\right), \quad
	\frac{d\bz}{dt} = \tau_z \left(\sigma\left(\bW_z \by + \bV_z \bu + \bb_z\right) - \bz\right).
\end{equation}
Here, $t \in [0,T]$ is the continuous time, $0 < \tau_y \leq \tau_z \leq 1$ are the two time scales, $\by(t) \in \R^{d_y},\bz(t)\in \R^{d_z}$ are the vectors of \emph{slow} and \emph{fast} variables and $\bu = \bu(t) \in \R^m$ is the \emph{input signal}. For simplicity, we set $d_y=d_z = d$. The dynamic interactions between the neurons are modulated by weight matrices $\bW_{y,z},\bV_{y,z}$, bias vectors $\bb_{y,z}$ and a \emph{nonlinear} tanh activation function $\sigma(u) = \tanh(u)$. Note that $\odot$ refers to the componentwise product of vectors.

However, two scales (one fast and one slow), may not suffice in representing a large number of scales that could be present in realistic sequential data sets. Hence, we need to generalize \eqref{eq:ode2} to a \emph{multiscale} version. One such generalization is provided by the following set of ODEs,
\begin{equation}
    \label{eq:ode}
    \begin{aligned}
    \frac{d\by}{dt} &= \hat{\sigma}\left(\bW_2\by + \bV_2\bu + \bb_2\right)\odot \left(\sigma\left(\bW_y \bz + \bV_y \bu + \bb_y\right) - \by\right), \\
    \frac{d\bz}{dt} &= \hat{\sigma}\left(\bW_1\by + \bV_1\bu + \bb_1\right)\odot \left(\sigma\left(\bW_z \by + \bV_z \bu + \bb_z\right) - \bz\right). 
    \end{aligned}
\end{equation}
In addition to previously defined quantities, we need additional weight matrices $\bW_{1,2},\bV_{1,2}$, bias vectors $\bb_{y,z}$ and sigmoid activation function $\hat{\sigma}(u) = 0.5(1+\tanh(u/2))$. 
As $\hat{\sigma}$ is monotone, we can set $\bW_{1,2} = \bV_{1,2} \equiv 0$ and $(\bb_1)_j = b_y, (\bb_2)_j = b_z$, for all $1 \leq j \leq d$, with $\hat{\sigma}(b_{y,z})=\tau_{y,z}$ to observe that the two-scale system \eqref{eq:ode2} is a special case of \eqref{eq:ode}. 
One can readily generalize this construction to obtain many different scales in \eqref{eq:ode}. 
Thus, we can interpret $\left({\boldsymbol{\tau}}_{z}(\by,t), {\boldsymbol{\tau}}_{y}(\by,t)\right)=  \left(\hat{\sigma}\left(\bW_{1}\by + \bV_1\bu + \bb_1\right),\hat{\sigma}\left(\bW_{2}\by + \bV_2\bu + \bb_2\right)\right)$ in \eqref{eq:ode} as input and state dependent gating functions, which endow ODE \eqref{eq:ode} with \emph{multiple time scales}.
These scales can be learned adaptively (with respect to states) and dynamically (in time). Moreover, it turns out that the multiscale ODE system \eqref{eq:ode} is of the same general form (see {\bf SM}\S\ref{app:HH}) as the well-known Hodgkin-Huxley equations modeling the dynamics of the action potential for voltage-gated ion-channels in biological neurons \citep{HH}. 



Next, we propose a time-discretization of the multiscale ODE system \eqref{eq:ode}, providing a circuit to our sequential model architecture. As is common with numerical discretizations of ODEs, doing so properly is important to preserve desirable properties. 
To this end, we fix $\Dt > 0$, and we discretize \eqref{eq:ode} with the following implicit-explicit (IMEX) time-stepping scheme to arrive at LEM, written in compact form as, 
\begin{equation}
    \label{eq:lem}
    \begin{aligned}
    {\bf \Dt}_n &= \Dt\hat{\sigma}(\bW_1\by_{n-1} + \bV_1\bu_{n} + \bb_1),  \\ {\bf \overline{\Dt}}_n &= \Dt\hat{\sigma}(\bW_2\by_{n-1} + \bV_2\bu_{n} + \bb_2),\\
    \bz_n &= (1 - {\bf \Dt}_n)\odot\bz_{n-1} + {\bf \Dt}_n \odot \sigma(\bW_z \by_{n-1} + \bV_z \bu_{n} + \bb_z), \\
    \by_n &= (1 - {\bf \overline{\Dt}}_n)\odot\by_{n-1} + {\bf \overline{\Dt}}_n\odot\sigma(\bW_y \bz_{n} + \bV_y \bu_{n} + \bb_y).
\end{aligned}
\end{equation}
For steps $1 \leq n \leq N$, the hidden states in LEM \eqref{eq:lem} are $\by_n,\bz_n \in \R^d$, with input state $\bu_n \in \R^m$. The weight matrices are $\bW_{1,2,z,y} \in \R^{d\times d}$ and $\bV_{1,2,z,y} \in \R^{d\times m}$ and the bias vectors are $\bb_{1,2,z,y} \in \R^d$. We also augment LEM \eqref{eq:lem} with a linear \emph{output state} $\bom_n \in \R^o$ with $\bom_n =\cW_y \by_n$, and $\cW_y \in \R^{o\times d}$.


\section{Related Work} 
\label{sxn:related}

We start by comparing our proposed model, LEM \eqref{eq:lem}, to the widely used LSTM of \citet{lstm}.
Observe that ${\bf \Dt}_n,\overline{\bf \Dt}_n$ in \eqref{eq:lem} are similar in form to the \emph{input, forget} and \emph{output} gates in an LSTM (see {\bf SM}\S\ref{app:lstm}), and that LEM \eqref{eq:lem} has exactly the same number of parameters (weights and biases) as an LSTM, for the same number of hidden units. 
Moreover, as detailed in {\bf SM}\S\ref{app:lstm}, we show that by choosing very specific values of the LSTM gates and the ${\bf \Dt}_n,{\bf \overline{\Dt}}_n$ terms in LEM \eqref{eq:lem}, the two models are equivalent. 
However, this analysis also reveals key differences between LEM \eqref{eq:lem} and LSTMs, as they are equivalent only under very stringent assumptions. 
In general, as the different gates in both LSTM and LEM \eqref{eq:lem} are \emph{learned} from data, one can expect them to behave differently. 
Moreover in contrast to LSTM, LEM stems from a discretized ODE system \eqref{eq:ode}, which endows it with (gradient) stable dynamics.  

The use of \emph{multiscale} neural network architectures in machine learning has a long history. 
An early example was provided in \citet{HinPla}, who proposed a neural network with each connection having a fast changing weight for temporary memory and a slow changing weight for long-term learning. 
More recently, one can view convolutional neural networks as multiscale architectures for processing multiple \emph{spatial} scales in data \citep{Kolter}. 

The use of ODE-based learning architectures has also received considerable attention in recent years with examples such as \emph{continuous-time} neural ODEs \citep{neuralODE,continuousnet_TR,queiruga2021compressing} and their recurrent extensions ODE-RNNs \citep{ode_rnn}, as well as RNNs based on discretizations of ODEs \citep{anti,lip_rnn,srnn,lim2021noisy,coRNN,unicornn}. In addition to the specific details of our archiecture, we differ from other discretized ODE-based RNNs in the explicit use of multiple (learned) scales in LEM. 





\section{Rigorous Analysis of LEM}
\label{sec:rig}
\paragraph{Bounds on hidden states.} 
The structure of LEM \eqref{eq:lem} allows us to prove (in {\bf SM}\S\ref{app:hsbdpf}) that its hidden states satisfy the following \emph{pointwise bound}.
\begin{proposition}
\label{prop:1}
Denote $t_n = n \Dt$ and assume that $\Dt \leq  1$. Further assume that the initial hidden states are $\bz_0 = \by_0 \equiv 0$.
Then, the hidden states $\bz_n,\by_n$ of LEM \eqref{eq:lem} are bounded pointwise as,
\begin{equation}
    \label{eq:hsb}
    \max\limits_{1 \leq i \leq d} \max \{|\bz_n^i|, |\by_n^i|\} \leq \min\left(1,\bdel\sqrt{t_n}\right), \quad \forall 1\leq n, ~{\rm with}~\bdel = \frac{1+\Dt}{\sqrt{2-\Dt}}.
\end{equation}
\end{proposition}

\paragraph{On the exploding and vanishing gradient problem.} 
For any $1 \leq n \leq N$, let $\bX_n \in \R^{2d}$, denoted the \emph{combined hidden state}, given by $\bX_n = \left[\bz_n^1,\by_n^1,\ldots \ldots,\bz_n^i,\by_n^i,\ldots \ldots,\bz_n^d,\by_n^d\right]$. 
For simplicity of the exposition, we 
consider a \emph{loss function}: $\E_n = \frac{1}{2}\|\by_n - \overline{\by}_n\|^2$, with $
\overline{\by}_n$ being the underlying \emph{ground truth}. The training of our proposed model \eqref{eq:lem} entails computing \emph{gradients} of the above loss function with respect to its underlying weights and biases $\theta \in \bf \Theta = [\bW_{1,2,y,z},\bV_{1,2,y,z},\bb_{1,2,y,z}]$,
at every step of the gradient descent procedure. Following \cite{vanish_grad}, one uses chain rule to show,
\begin{equation}
    \label{eq:chain}
    \frac{\partial \E_n}{\partial \theta} = \sum\limits_{1\leq k \leq n} \frac{\partial \E^{(k)}_n}{\partial \theta}, \quad \frac{\partial \E^{(k)}_n}{\partial \theta} = \frac{\partial \E_n}{\partial \bX_n} \frac{\partial \bX_n}{\partial \bX_k} \frac{\partial^{+} \bX_k}{\partial \theta}
\end{equation}

In general, for recurrent models, the partial gradient $\frac{\partial \E^{(k)}_n}{\partial \theta}$, which measures the contribution to the hidden state gradient at step $n$ arising from step $k$ of the model, can behave as $\frac{\partial \E^{(k)}_n}{\partial \theta} \sim \gamma^{n-k}$, for some $\gamma > 0$ \cite{vanish_grad}. If $\gamma > 1$, then the partial gradient grows \emph{exponentially} in sequence length, for long-term dependencies $k << n$, leading to the exploding gradient problem. On the other hand, if $\gamma < 1$, then partial gradients decays \emph{exponentially} for $k << n$, leading to the vanishing gradient problem. Thus, mitigation of the exploding and vanishing gradient problem entails deriving bounds on the gradients. We start with the following upper bound (proved in {\bf SM}\S\ref{app:hsgubpf}), 
\begin{proposition}
\label{prop:2}
Let $\bz_n,\by_n$ be the hidden states generated by LEM \eqref{eq:lem}. We assume that $\Dt << 1$ is chosen to be sufficiently small. Then, the gradient of the loss function $\E_n$ with respect to any parameter $\theta \in {\bf \Theta}$ is bounded as 
\begin{equation}
\label{eq:gbd}
\begin{aligned}
  \left| \frac{\partial \E_n}{\partial \theta} \right|  &\leq (1+\hat{\bY}) t_n + (1+\hat{\bY}) \Gamma t_n^2, \quad \hat{Y} = \|\overline{\by}_n\|_{\infty}, \\
  \eta = \max \{ \|\bW_1\|_{\infty},\|\bW_2\|_{\infty},&\|\bW_z\|_{\infty},\|\bW_y\|_{\infty} \}, \quad \Gamma=  2\left(1 + \eta\right)(1 + 3 \eta)
  \end{aligned}
\end{equation}
\end{proposition}
If we choose the hyperparameter $\Dt = \ord(n^{-1})$ (see SM \eqref{eq:ord} for the order-notation), then one readily observes from \eqref{eq:gbd} that the gradient $\partial_{\theta} \E_n$ is \emph{uniformly bounded} for any sequence length $n$ and the exploding gradient problem is clearly mitigated for LEM \eqref{eq:lem}. Even if one chooses $\Dt = \ord(n^{-s})$, for some $0 \leq s \leq 1$, we show in {\bf SM} Remark \ref{rem:gbd} that the gradient can only grow polynomially (e.g. as $\ord(n)$ for $s=1/2$), still mitigating the exploding gradient problem.

Following \cite{vanish_grad}, one needs a more precise characterization of the partial gradient 
$\partial_{\theta} \E^{(k)}_n$, for long-term dependencies, i.e., $k << n$, to show mitigation of the vanishing gradient problem. In {\bf SM}\S\ref{app:hsglb}, we state and prove proposition \ref{prop:3}, which provides a precise formula for the asymptotics of the partial gradient. Here, we illustrate this formula in a special case as a corollary,
\begin{proposition}
\label{prop:3cor}
Let $\by_n,\bz_n$ be the hidden states generated by LEM \eqref{eq:lem} and the ground truth satisfy $\overline{\by}_n \sim \ord(1)$.
Then, for any $k << n$ (long-term dependencies) we have,
\begin{equation}
\label{eq:glbo}
 \frac{\partial \E^{(k)}_n}{\partial \theta} = \ord\left(\Dt^{\frac{3}{2}}\right) . 
\end{equation}
Here, constants in $\ord(\Dt^{\frac{3}{2}})$ depend on only on $\eta$ \eqref{eq:gbd} and $\overline{\eta} = \|\bW_2\|_{1}$ and are independent of $n,k$.
\end{proposition}
This formula \eqref{eq:glbo} shows that although the partial gradient can be small, i.e.,
$\ord(\Dt^{\frac{3}{2}})$, it is in fact \emph{independent of $k$}, ensuring that long-term dependencies contribute to gradients at much later steps and mitigating the vanishing gradient problem.

\paragraph{Universal approximation of general dynamical systems.} 
The above bounds on hidden state gradients show that the proposed model LEM \eqref{eq:lem} mitigates the exploding and vanishing gradients problem. However, this by itself, does not guarantee that it can learn complicated and realistic input-output maps between sequences. To investigate the \emph{expressivity} of the proposed LEM, we will show in the following proposition that it can approximate \emph{any} dynamical system, mapping an input sequence $\bu_n$ to an output sequence $\bo_n$, of the (very) general form,
\begin{equation}
    \label{eq:ods}
    \begin{aligned}
    \bh_{n} &= \bif\left(\bh_{n-1},\bu_n\right), \quad
\bo_n = \bo(\bh_n), \quad \forall~1 \leq n \leq N,
    \end{aligned}
\end{equation}
 with $\bh_n \in \R^{d_h},\bo_n \in \R^{d_o}$ denoting the \emph{hidden} and \emph{output} states, respectively. The input signal is $\bu_n \in \R^{d_u}$ and maps $\bif:\R^{d_h}\times \R^{d_u} \mapsto \R^{d_h}$ and $\bo:\R^{d_h} \mapsto \R^{d_o}$ are Lipschitz continuous. For simplicity, we set the initial state $\bh_0 = 0$.
 \begin{proposition}
\label{prop:exp1}
For all $1\leq n \leq N$, let $\bh_n,\bo_n$ be given by the dynamical system \eqref{eq:ods} with input signal $\bu_n$. Under the assumption that there exists a $R > 0$ such that $\max\{\|\bh_n\|,\|\bu_n\|\} < R$, for all $1 \leq n \leq N$, then for any given $\epsilon > 0$ there exists a LEM of the form \eqref{eq:lem}, with hidden states $\by_n,\bz_n \in \R^{d_y}$ and output state $\bom_n = \cW_y \by_n\in \R^{d_o}$, for some $d_y$ such that 
the following holds,
\begin{equation}
    \label{eq:exp1}
    \|\bo_n - \bom_n\| \leq \epsilon, \quad \forall 1 \leq n \leq N.
\end{equation}
\end{proposition}
From this proposition, proved in {\bf SM}\S\ref{app:exp1pf}, we conclude that, in principle, the proposed LEM \eqref{eq:lem} can approximate a very large class of dynamical systems. 

\paragraph{Universal approximation of multiscale dynamical systems.} 
While expressing a general form of input-output maps between sequences, the dynamical system \eqref{eq:ods} does not explicitly model dynamics at multiple scales. Instead, here we consider the following two-scale \emph{fast-slow} dynamical system of the general form, 
\begin{equation}
    \label{eq:2sds}
    \begin{aligned}
    \bh_{n} = \bif(\bh_{n-1},\bc_{n-1},\bu_n), \quad
  \bc_{n} = \tau \bg(\bh_{n},\bc_{n-1},\bu_n), \quad
  \bo_n = \bo(\bc_n).
  \end{aligned}
\end{equation}
Here, $0 < \tau << 1$ and $1$ are the slow and fast time scales, respectively. The underlying maps $(\bif,\bg):\R^{d_h\times d_h \times d_u} \mapsto \R^{d_h}$ are Lipschitz continuous. In the following proposition, proved in {\bf SM}\S\ref{app:exp2pf}, we show that LEM \eqref{eq:lem} can approximate \eqref{eq:2sds} to desired accuracy. 
\begin{proposition}
\label{prop:exp2}
For any $0 < \tau << 1$, and for all $1\leq n \leq N$, let $\bh_n,\bc_n,\bo_n$ be given by the two-scale dynamical system \eqref{eq:2sds} with input signal $\bu_n$. Under the assumption that there exists a $R > 0$ such that $\max\{\|\bh_n\|,\|\bc_n\|,\|\bu_n\|\} < R$, for all $1 \leq n \leq N$, then for any given $\epsilon > 0$, there exists a LEM of the form \eqref{eq:lem}, with hidden states $\by_n,\bz_n \in \R^{d_y}$ and output state $\bom_n \in \R^{d_o}$ with $\bom_n = \cW \by_n$ such that 
the following holds,
\begin{equation}
    \label{eq:exp2}
    \|\bo_n - \bom_n\| \leq \epsilon, \quad \forall 1 \leq n \leq N.
\end{equation}
Moreover, the weights, biases and size (number of neurons) of the underlying LEM \eqref{eq:lem} are \emph{independent} of the time-scale $\tau$.
\end{proposition}
This argument can be readily generalized to more than two time scales (see {\bf SM} Proposition \ref{prop:mts}). Hence, we show that, in principle, the proposed model LEM \eqref{eq:lem} can approximate multiscale dynamical systems, with model size being \emph{independent} of the underlying timescales. These theoretical results for LEM \eqref{eq:lem} point to the ability of this architecture to learn complicated multiscale input-output maps between sequences, while mitigating the exploding and vanishing gradients problem. Although useful prerequisities, these theoretical properties are certainly not sufficient to demonstrate that LEM \eqref{eq:lem} is efficient in practice. To do this, we perform several benchmark evaluations, and we report the results below. 
\section{Empirical results}
\label{sxn:empirical}

We present a variety of experiments ranging from long-term dependency tasks to real-world applications as well as tasks which require high expressivity of the model.  
Details of the training procedure for each experiment can be found in {\bf SM}\S\ref{app:training_details}. 
As competing models to LEM, we choose two different types of architectures---LSTMs and GRUs---as they are known to excel at expressive tasks such as language modeling and speech recognition, while not performing well on long-term dependency tasks, possibly due to the exploding and vanishing gradients problem.  
On the other hand, we choose state-of-the-art RNNs which are tailor-made to learn tasks with long-term dependencies. Our objective is to evaluate the performance of LEM and compare it with competing models.
All code to reproduce our results can be found at \href{https://github.com/tk-rusch/LEM}{\textbf{https://github.com/tk-rusch/LEM}}.
\paragraph{Very long adding problem.}
We start with the well-known adding problem \citep{lstm}, proposed to test the ability of a model to learn (very) long-term dependencies. The input is a two-dimensional sequence of length $N$, with the first dimension consisting of random numbers drawn from $\mathcal{U}([0,1])$ and with two non-zero entries (both set to $1$) in the second dimension, chosen at random locations, but one each in both halves of the sequence. The output is the sum of two numbers of the first dimension at positions, corresponding to the two 1 entries in the second dimension. We consider three very challenging cases, namely input sequences with length $N=2000,5000$ and $10000$. The results of LEM together with competing models including state-of-the-art RNNs, which are explicitly designed to solve long-term dependencies, are presented in \fref{fig:adding_results}. We observe in this figure that while baseline LSTM is not able to beat the baseline mean-square error of $0.167$ (the variance of the baseline output $1$) in any of the three cases, a proper weight initialization for LSTM, the so-called \emph{chrono}-initialization of \cite{warp} leads to much better performance in all cases. For $N=2000$, all other architectures (except baseline LSTM) beat the baseline convincingly. However for $N=5000$, only LEM, chrono-LSTM and coRNN are able to beat the baseline. In the extreme case of $N=10000$, only LEM and chrono-LSTM are able to beat the baseline. Nevertheless, LEM outperforms chrono-LSTM by converging faster (in terms of number of training steps) and attaining a lower test MSE than chrono-LSTM in all three cases. 
\begin{figure}[ht!]
\begin{minipage}{.33\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_2000_new.pdf}
\end{minipage}\begin{minipage}{.33\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_5000_new.pdf}
\end{minipage}\begin{minipage}{.33\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_10000_new.pdf}
\end{minipage}
\caption{Results on the very long adding problem for LEM, coRNN, DTRIV$\infty$ \citep{dtriv}, FastGRNN \citep{fastrnn}, LSTM and LSTM with chrono initialization \citep{warp} based on three very long sequence lengths $N$, i.e., $N=2000$, $N=5000$ and $N=10000$.}
\label{fig:adding_results}
\end{figure}

\paragraph{Sequential image recognition.}
We consider three experiments based on two widely-used image recognition data sets, i.e., MNIST \citep{mnist} and CIFAR-10 \citep{cifar}, where the goal is to predict the correct label after reading in the whole sequence. The first two tasks are based on MNIST images, which are flattened along the rows to obtain sequences of length $N=784$. In sequential MNIST (sMNIST), the sequences are fed to the model one pixel at a time in streamline order, while in permuted sequential MNIST (psMNIST), a fixed random permutation is applied to the sequences, resulting in much longer dependency than for sMNIST. We also consider the more challenging noisy CIFAR-10 (nCIFAR-10) experiment \citep{anti}, where CIFAR-10 images are fed to the model row-wise and flattened along RGB channels, resulting in $96$-dimensional sequences, each of length $32$. Moreover, a random noise padding is applied after the first $32$ inputs to produce sequences of length $N=1000$. Hence, in addition to classifying the underlying image, a model has to store this result for a long time. In \Tref{tab:image}, we present the results for LEM on the three tasks together with other SOTA RNNs, which were explicitly designed to solve long-term dependency tasks, as well as LSTM and GRU baselines. We observe that LEM outperforms all other methods on sMNIST and nCIFAR-10. Additionally on psMNIST, LEM performs as well as coRNN, which has been SOTA among single-layer RNNs on this task.

\begin{table}[ht!]
\caption{Test accuracies on sMNIST, psMNIST and nCIFAR-10, where $M$ denotes the total number of parameters of the corresponding model. Results of other models are taken from the respective original paper referenced in the main text, except that the results for LSTM are taken from \citet{scornn}, for GRU from \citet{GRU_results} and the results indicated by $^*$ are added by us.}
\label{tab:image}
\centering
\begin{tabular}{llllll}
\toprule
\cmidrule(r){1-6}
\multirow{2}{*}{Model} &  \multicolumn{3}{c}{MNIST} & \multicolumn{2}{c}{CIFAR-10} \\
\cmidrule(r){2-4}\cmidrule(r){5-6}  
& sMNIST & psMNIST &\# units / $M$ & nCIFAR-10 & \# units / $M$ \\
\midrule
GRU & 99.1\% & 94.1\% & 256 / 201k & 43.8\%$^*$& 128 / 88k\\
LSTM & 98.9\% & 92.9\% & 256 / 267k & 11.6\% & 128 / 116k \\
chrono-LSTM & 98.9\%$^*$ & 94.6\%$^*$ & 128 / 68k & 55.9\%$^*$ & 128 / 116k \\
anti.sym. RNN &  98.0\% & 95.8\% & 128 / 10k & 48.3\% & 256 / 36k\\
Lipschitz RNN & 99.4\% & 96.3\% & 128 / 34k & 57.4\% & 128 / 46k \\
expRNN &  98.4\% &  96.2\%& 360 / 69k & 52.9\%$^*$& 360 / 103k \\
coRNN & 99.3\% & \textbf{96.6}\% & 128/ 34k & 59.0\% & 128 / 46k\\
\textbf{LEM} & \textbf{99.5}\% & \textbf{96.6}\% & 128 / 68k & \textbf{60.5}\% & 128 / 116k\\
    \bottomrule
  \end{tabular}
\end{table}
\paragraph{EigenWorms: Very long sequences for genomics classification.}
The goal of this task \citep{eigenworms} is to classify worms as belonging to either the wild-type or four different mutants, based on $259$ very long sequences (length $N=17984$) measuring the motion of a worm. In addition to the nominal length, it was empirically shown in \citet{unicornn} that the EigenWorms sequences exhibit actual very long-term dependencies (i.e., longer than 10k). 

Following \citet{log_ode} and \cite{unicornn}, we divide the data into a train, validation and test set according to a $70\%,15\%,15\%$ ratio. In \Tref{tab:worms}, we present results for LEM together with other models. As the validation and test sets, each consist of only $39$ sequences, we report the mean (and standard deviation of) accuracy over $5$ random initializations to rule out lucky outliers. We observe from this table that LEM outperforms all other methods, even the $2$-layer UnICORNN architecture, which has been SOTA on this task. 
\begin{table}[t!]
\caption{Test accuracies on EigenWorms using $5$ re-trainings of each best performing network (based on the validation set), where all other results are taken from \citet{unicornn} except that the NRDE result is taken from \cite{log_ode} and the results indicated by $^*$ are added by us.}
\label{tab:worms}
\centering
\begin{tabular}{llll}
\toprule
\cmidrule(r){1-4}
Model &  test accuracy & \# units & \# params \\
\midrule
NRDE & 83.8\% $\pm$ 3.0\% & 32 & 35k \\
expRNN & 40.0\% $\pm$ 10.1\% & 64 & 2.8k \\
IndRNN (2 layers) & 49.7\% $\pm$ 4.8\% & 32 & 1.6k \\
LSTM & 38.5\% $\pm$ 10.1\%$^*$ & 32 & 5.3k \\
BiLSTM+1d-conv & 40.5\% $\pm$ 7.3\%$^*$ & 22 & 5.8k \\
chrono-LSTM & 82.6 \% $\pm$ 6.4\%$^*$ & 32 & 5.3k \\
coRNN & 86.7\% $\pm$ 3.0\%&32 & 2.4k \\
UnICORNN (2 layers) & 90.3\% $\pm$ 3.0\% & 32 & 1.5k\\
\textbf{LEM} & \textbf{92.3}\% $\pm$ \textbf{1.8}\% & 32 & 5.3k \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Healthcare application: Heart-rate prediction.}
In this experiment, one predicts the heart rate from a time-series of measured PPG data, which is part of the TSR archive \citep{ai_healthcare} and has been collected at the Beth Isreal Deaconess medical center. The data set, consisting of $7949$ sequences, each of length $N=4000$, is divided into a train, validation and test set according to a $70$\%,$15$\%,$15$\% ratio, \citep{log_ode,unicornn}. The results, presented in \Tref{tab:medical}, show that LEM outperforms the other competing models, including having a test $L^2$ error of $35\%$ less than the SOTA UnICORNN. 
\begin{table}[ht!]
  \caption{Test $L^2$ error on heart-rate prediction using PPG data. All results are obtained by running the same code and using the same fine-tuning protocol.}
  \label{tab:medical}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-4}
    Model & test $L^2$ error & \# units & \# params \\
    \midrule
LSTM  & 9.93 & $128$ & $67$k \\
chrono-LSTM  & 3.31 & $128$ & $67$k \\
expRNN &1.63 & $256$ & $34$k \\
IndRNN (3 layers) & 1.94 & $128$ & $34$k \\
coRNN & 1.61 & $128$ & $34$k \\
UnICORNN (3 layers) & 1.31 & $128$ & $34$k \\
\textbf{LEM} & \textbf{0.85} & $128$ & $67$k \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Multiscale dynamical system prediction.}
The FitzHugh-Nagumo system \citep{Fitznag} \begin{equation}
\label{eq:FHN}
\begin{aligned}
    v^\prime &= v - \frac{v^3}{3} -w + I_{\text{ext}}, \quad 
    w^\prime = \tau(v + a - bw),
\end{aligned}
\end{equation}
is a prototypical model for a two-scale fast-slow nonlinear dynamical system, with fast variable $v$ and slow variable $w$ and $\tau << 1$ determining the slow-time scale. This \emph{relaxation-oscillator} is an approximation to the Hodgkin-Huxley model \citep{HH} of neuronal action-potentials under an external signal $I_{\text{ext}}\geq0$. With $\tau=0.02$, $I_{\text{ext}}=0.5$, $a=0.7$, $b=0.8$ and initial data $(v_0,w_0) = (c,0)$, with $c$ randomly drawn from $\mathcal{U}([-1,1])$, we numerically approximate \eqref{eq:FHN} with the explicit Runge-Kutta method of order $5(4)$ in the interval $[0,400]$ and generate $128$ training and validation and $1024$ test sequences, each of length $N=1000$, to complete the data set. The results, presented in \Tref{tab:FHN}, show that LEM not only outperforms LSTM by a factor of $6$ but also all other methods including coRNN, which is tailormade for oscillatory time-series. This reinforces our theory by demonstrating efficient approximation of multiscale dynamical systems with LEM.     
\begin{table}[t!]
  \caption{Test $L^2$ error on FitzHugh-Nagumo system prediction. All results are obtained by running the same code and using the same fine-tuning protocol.}
  \label{tab:FHN}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-4}
    Model &  error ($\times 10^{-2}$) & \# units & \# params \\
    \midrule
LSTM  & 1.2 & $16$ & $1$k\\
expRNN  & 2.3 & $50$ & $1$k  \\
LipschitzRNN  & 1.8 & $24$ & $1$k   \\
FastGRNN  & 2.2 & $34$ & $1$k \\
coRNN  & 0.4 & $24$ & $1$k \\
\textbf{LEM}  & {\bf 0.2} & $16$ & $1$k \\
\bottomrule
\end{tabular}
\end{table}
\paragraph{Google12 (V2) keyword spotting.}
The Google Speech Commands data set V2 \citep{google12} is a widely used benchmark for keyword spotting, consisting of $35$ words, sampled at a rate of $16$ kHz from $1$ second utterances of $2618$ speakers. We focus on the $12$-label task (Google12) and follow the pre-defined splitting of the data set into train/validation/test sets and test different sequential models. In order to ensure comparability of different architectures, we do not use performance-enhancing tools such as convolutional filtering or multi-head attention. From \Tref{tab:google12}, we observe that both LSTM and GRU, widely used models in this context, perform very well with a test accuracy of around $95\%$. Nevertheless, LEM is able to outperform both on this task and provides the best performance. 
\begin{table}[ht!]
  \caption{Test accuracies on Google12. All results are obtained by running the same code and using the same fine-tuning protocol.}
  \label{tab:google12}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-4}
    { Model} &  test accuracy & \# units & \# params \\
    \midrule
tanh-RNN & 73.4\% & $128$ & $27$k\\
anti.sym. RNN & 90.2\% & $128$ & $20$k\\
LSTM  & 94.9\% & $128$ & $107$k\\
GRU  & 95.2\% & $128$ & $80$k  \\
FastGRNN  & 94.8\%& $128$ & $27$k \\
expRNN & 92.3\% & $128$ & $19$k \\
coRNN  &94.7\% & $128$ & $44$k \\
\textbf{LEM}  & \textbf{95.7}\% & $128$ & $107$k \\
\bottomrule
\end{tabular}
\end{table}
\paragraph{Language modeling: Penn Tree Bank corpus.}
Language modeling with the widely used small scale Penn Treebank (PTB) corpus \citep{ptb_corpus}, preprocessed by \citet{ptb_prepro}, has been identified as an excellent task for testing the expressivity of recurrent models \citep{nnRNN}. To this end, in \Tref{tab:ptb_char}, we report the results of different architectures, with a similar number of hidden units, on the PTB char-level task and observe that RNNs, designed explicitly for learning long-term dependencies, perform significantly worse than LSTM and GRU. On the other hand, LEM is able to outperform both LSTM and GRU on this task by some margin (a test bpc of $1.25$ in contrast with approximately a bpc of $1.36$). In fact, LEM provides the smallest test bpc among all reported single-layer recurrent models on this task, to the best of our knowledge. 
This superior performance is further illustrated in \Tref{tab:ptb_word}, where the test perplexity for different models on the PTB word-level task is presented. 
We observe that not only does LEM significantly outperform (by around $40\%$) LSTM, but it also provides again the best performance among all single layer recurrent models, including the recently proposed TARNN \citep{tarnn}. 
Moreover, the single-layer results for LEM are better than reported results for multi-layer LSTM models, such as in \citet{var_drop} ($2$-layer LSTM, $1500$ units each: $75.2$ test perplexity) or \citet{tcn} ($3$-layer LSTM, $700$ units each: $78.93$ test perplexity).


\begin{table}[ht!]
  \caption{Test bits-per-character (bpc) on PTB character-level for single layer LEM and other single layer RNN architectures. Other results are taken from the papers cited accordingly in the table, while the results for coRNN are added by us.}
  \label{tab:ptb_char}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-2}
    { Model} &  test bpc & \# units & \# params \\
    \midrule
anti.sym RNN \citep{lip_rnn}  & 1.60 & 1437 & 1.3M\\
Lipschitz RNN \citep{lip_rnn} & 1.42 & 764 & 1.3M \\
expRNN \citep{nnRNN} &  1.51 & 1437 & 1.3M \\
coRNN  & 1.46 & 1024 & 2.3M\\
nnRNN \citep{nnRNN}  &  1.47 & 1437 & 1.3M \\
LSTM \citep{zoneout} & 1.36  & 1000 & 5M \\
GRU \citep{tcn} & 1.37  & 1024 & 3M \\
\textbf{LEM} & \textbf{1.25} & 1024 & 5M \\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[t!]
  \caption{Test perplexity on PTB word-level for single layer LEM and other single layer RNN architectures.}
  \label{tab:ptb_word}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-2}
    { Model} &  test perplexity & \# units & \# params \\
    \midrule
Lipschitz RNN \citep{lip_rnn} & 115.4 & 160 & 76k \\
FastRNN \citep{tarnn}  &  115.9 & 256 & 131k \\
LSTM \citep{tarnn} & 116.9 & 256 & 524k\\
SkipLSTM \citep{tarnn} &  114.2 & 256 & 524k \\
TARNN \citep{tarnn} & 94.6 & 256 & 524k \\
\textbf{LEM} & \textbf{72.8} & 256 & 524k \\
\bottomrule
\end{tabular}
\end{table}



\section{Discussion}
\label{sxn:discussion}

The design of a gradient-based model for processing sequential data that can learn tasks with long-term dependencies while retaining the ability to learn complicated sequential input-output maps is very challenging. 
In this paper, we have proposed \emph{Long Expressive Memory} (LEM), a novel recurrent architecture, with a suitable time-discretization of a specific multiscale system of ODEs \eqref{eq:ode} serving as the circuit to the model. By a combination of theoretical arguments and extensive empirical evaluations on a diverse set of learning tasks, we demonstrate that LEM is able to learn long-term dependencies while retaining sufficient expressivity for efficiently solving realistic learning tasks. 

It is natural to ask why LEM performs so well. A part of the answer lies in the mitigation of the exploding and vanishing gradients problem. Proofs for gradient bounds \eqref{eq:gbd},\eqref{eq:glbo} reveal a key role played by the hyperparameter $\Dt$. We observe from {\bf SM} \Tref{tab:hyperparameters_rounded} that small values of $\Dt$ might be needed for problems with very long-term dependencies, such as the EigenWorms dataset. On the other hand, no tuning of the hyperparameter $\Dt$ is necessary for several tasks such as language modeling, keyword spotting and dynamical systems prediction and a default value of $\Dt =1$ yielded very good performance. The role and choice of the hyperparameter $\Dt$ is investigated extensively in {\bf SM}\S\ref{app:dt}. However, mitigation of exploding and vanishing gradients problem alone does not explain high expressivity of LEM. In this context, we proved that LEMs can approximate a very large class of multiscale dynamical systems. Moreover, we provide experimental evidence in {\bf SM}\S\ref{app:ms} to observe that LEM not only expresses a range of scales, as it is designed to do, but also these scales contribute proportionately to the resulting multiscale dynamics. Furthermore, empirical results presented in {\bf SM}\S\ref{app:ms} show that this ability to represent multiple scales correlates with the high accuracy of LEM. We believe that this combination of gradient stable dynamics, specific model structure, and its multiscale resolution can explain the observed performance of LEM. 

We conclude with a comparison of LEM and the widely-used gradient-based LSTM model. 
In addition to having exactly the same number of parameters for the same number of hidden units, our experiments show that LEMs are better than LSTMs on expressive tasks such as keyword spotting and language modeling, while also providing significantly better performance on long-term dependencies. This robustness of the performance of LEM with respect to sequence length paves the way for its application to learning many different sequential data sets where competing models might not perform satisfactorily.

\clearpage

\section*{Acknowledgements.} The research of TKR and SM was performed under a project that has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 770880). NBE and MWM would like to
acknowledge IARPA (contract W911NF20C0035), NSF, and ONR for providing partial support of this work. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. 

The authors thank Dr. Ivo Danihelka (DeepMind) for pointing out that the hidden states for LEM satisfy the maximum principles \eqref{eq:mp2}, \eqref{eq:mp3}.





\bibliography{refs}
\bibliographystyle{iclr2022_conference}

\appendix
\newpage
\begin{center}
{\bf Supplementary Material for:}\\
Long Expressive Memory for Sequence Modeling
\end{center}
\section{Training details}
\label{app:training_details}
All experiments were run on CPU, namely Intel Xeon Gold 5118 and AMD EPYC 7H12, except for Google12, PTB character-level and PTB word-level, which were run on a GeForce RTX 2080 Ti GPU. All weights and biases of LEM \eqref{eq:lem} are initialized according to $\mathcal{U}(-1/\sqrt{d},1/\sqrt{d})$, where $d$ is the number of hidden units. 

\begin{table}[ht!]
  \caption{Rounded hyperparameters of the best performing LEM architecture for each experiment. If no value is given for $\Dt$, it means that $\Dt$ is fixed to $1$ and no fine-tuning is performed on this hyperparameter.}
  \label{tab:hyperparameters_rounded}
  \centering
  \begin{tabular}{lccc}
    \toprule
    \cmidrule(r){1-4}
    experiment & learning rate & batch size  & $\Dt$ \\
    \midrule
Adding ($N=10000$) & $2.6\times 10^{-3}$ & $50$ & $2.42 \times 10^{-2}$\\
sMNIST & $1.8\times 10^{-3}$ & $128$ & $2.1\times 10^{-1}$ \\
psMNIST & $3.5\times 10^{-3}$ & $128$ & $1.9\times10^0$ \\
nCIFAR-10  & $1.8\times 10^{-3}$ & $120$ & $9.5\times 10^{-1}$ \\
EigenWorms  & $2.3\times 10^{-3}$ & $8$ & $1.6\times 10^{-3}$ \\
Healthcare & $1.56\times 10^{-3}$ & $32$ & $1.9\times 10^{-1}$ \\
FitzHugh-Nagumo &$9.04\times 10^{-3}$ & $32$ & / \\
Google12 & $8.9\times 10^{-4}$ & $100$ & / \\
PTB character-level & $6.6\times 10^{-4}$ & $128$ & / \\
PTB word-level & $6.8\times 10^{-4}$ & $64$ & / \\

    \bottomrule
  \end{tabular}
\end{table}

The hyperparameters are selected based on a random search algorithm, where we present the rounded hyperparameters for the best performing LEM model (\emph{based on a validation set}) on each task in \Tref{tab:hyperparameters_rounded}.

We base the training for the PTB experiments on the following language modelling code: \href{https://github.com/deepmind/lamb}{https://github.com/deepmind/lamb}, where we fine-tune, based on a random search algorithm, only the learning rate, input-, output- and state-dropout, $L^2$-penalty term and the maximum gradient~norm.

We train LEM for $100$ epochs on sMNIST, psMNIST and nCIFAR-10, after which we decrease the learning rate by a factor of $10$ and proceed training for $20$ epochs. Moreover, we train LEM for $50$, $60$ as well as $400$ epochs on EigenWorms, Google12 and FitzHugh-Nagumo. We decrease the learning rate by a factor of 10 after $50$ epochs on Google12. On the Healthcare task, we train LEM for 250 epochs, after which we decrease the learning rate by a factor of $10$ and proceed training for $250$ epochs.
\section{Further Experimental Results}
\label{app:fer}
\subsection{On the choice of the hyperparameter $\Dt$.}
\label{app:dt}
The hyperparameter $\Dt$ in LEM \eqref{eq:lem} measures the maximum allowed (time) step in the discretization of the multi-scale ODE system \eqref{eq:ode}. In propositions \ref{prop:1}, \ref{prop:2} and \ref{prop:3cor}, this hyperparameter $\Dt$ plays a key role in the bounds on the hidden states \eqref{eq:hsb} and their gradients \eqref{eq:gbd}. In particular, setting $\Dt = \ord(N^{-1})$ will lead to hidden states and gradients, that are bounded uniformly with respect to the underlying sequence length $N$. However, these upper bounds on the hidden states and gradients account for \emph{worst-case} scenarios and can be very pessimistic for the problem at hand.

\begin{figure}[ht!]
\centering
\begin{minipage}[t]{.48\textwidth}
\includegraphics[width=1.\textwidth]{figures/eigenworms_ablation.pdf}
\caption{Sensitivity study on hyperparameter $\Dt$ in \eqref{eq:lem} using the EigenWorms experiment.}
\label{fig:eworms_abl}
\end{minipage}
\hspace{0.0025\textwidth}
\begin{minipage}[t]{.48\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_fixed_dt.pdf}
\caption{Average (over ten different initializations each) test mean-square error on the adding problem of LEM for different sequence lengths $N$, where the hyperparameter $\Dt$ of LEM \eqref{eq:lem} is fixed to $\Dt=1/\sqrt{N}$.}
\label{fig:adding_fixed_dt}
\end{minipage}
\end{figure}

Thus, in practice, we determine $\Dt$ through a hyperparameter tuning procedure as described in section \ref{app:training_details}. To this end, we perform a random search within $\Dt < 2$ and present the resulting optimal values of $\Dt$ for each of the considered data sets in \Tref{tab:hyperparameters_rounded}. From this table, we observe that for data sets such as PTB, FitzHugh-Nagumo and Google 12
we do not need any tuning of $\Dt$ and a default value of $\Dt=1$ resulted in very good empiricial performance. On the other data sets such as sMNIST, nCIFAR-10 and the healthcare example, where the sequence length ($N = \ord(10^3)$) is larger, we observe that values of $\Dt \approx 0.1$ yielded the best performance. The notable exception to this was for the EigenWorms data set, with a very long sequence length of $N=17984$ as well as demonstrated very long range dependencies in the data, see \cite{unicornn}. Here, a value of $\Dt = 1.6\times 10^{-3}$ resulted in the best observed performance. To further investigate the role of the hyperparameter $\Dt$ in the EigenWorms experiment, we perform a sensitivity study where the value of $\Dt$ is varied and the corresponding accuracy of the trained LEM is observed. The results of this sensitivity study are presented in \fref{fig:eworms_abl}, where we plot the test accuracy (Y-axis) vs. the value of $\Dt$ (X-axis). From this figure, we observe that the accuracy is rather poor for $\Dt \approx 1$ but improves monotonically as $\Dt$ is reduced till a value of approximately $10^{-2}$, after which it saturates. Thus, in this case, a value of $\Dt = \ord(N^{-\frac{1}{2})}$ (for sequence length $N$) suffices to yield the best empirical performance. 

Given this observation, we further test whether $\Dt = \ord(N^{-\frac{1}{2}})$ suffices for other problems with long-term dependencies. To this end, we consider the adding problem and vary the input sequence length by an order of magnitude, i.e., from $N=250$ to $N=2000$. The value of $\Dt$ is now fixed at $\Dt = \frac{1}{\sqrt{N}}$ and the resulting test loss (Y-axis) vs the number of training steps (X-axis) is plotted in \fref{fig:adding_fixed_dt}. We see from this figure that this value of $\Dt$ sufficed to yield very small average test errors for this problem for all considered sequence lengths $N$. Thus, empirically a value of $\Dt$ in the range $\frac{1}{\sqrt{N}} \leq \Dt \leq 1$ yields very good performance. 

Even if we set $\Dt = \ord(\frac{1}{\sqrt{N}})$, it can happen for very long sequences $N>>1$ that the gradient can be quite small from the gradient asymptotic formula \eqref{eq:glbo}. This might lead to saturation in training, resulting in long training times. However, we do not observe such long training times for very long sequence lengths in our experiment. To demonstrate this, we again consider \fref{fig:adding_fixed_dt} where the number of training steps (X-axis) is plotted for sequence lengths that vary an order of magnitude. The figure clearly shows that the approximately the same number of training steps are needed to attain a low test error, irrespective of the sequence length. This is further buttressed in \fref{fig:adding_results}, where similar number of training steps where needed for obtaining the same very low test error, even for long sequence lengths, with $N$ up to $10000$. Moreover, from section \ref{app:training_details}, we see that the number of epochs for different data sets is independent of the sequence length. For instance, only $50$ epochs were necessary for EigenWorms with a sequence length of $N=17984$ and $\Dt = 1.6 \times 10^{-3}$ whereas $400$ epochs were required for the FitzHugh-Nagumo system with a $\Dt =1$. 

\subsection{Multiscale Behavor of LEM.}
\label{app:ms}
LEM \eqref{eq:lem} is designed to represent multiple scales, with terms ${\bf \Dt}_n,\overline{\bf \Dt}_n$ being explicitly designed to learn possible multiple scales. In the following, we will investigate if in practice, LEM learns multiple scales and uses them to yield the observed superior empirical performance with respect to competing models. 

To this end, we start by recalling the proposition \ref{prop:exp2} where we showed that in principle, LEM can learn the two underlying timescales of a \emph{fast-slow} dynamical system (see proposition \ref{prop:mts} for a similar result for the universal approximation of a $r$-time scale (with $r \geq 2$) dynamical system with LEM). Does this hold in practice ? To further investigate this issue, we consider the FitzHugh-Nagumo dynamical system \eqref{eq:FHN} which serves as a prototype for a two-scale dynamical system. We consider this system \eqref{eq:FHN} with the two time-scales being $\tau = 0.02$ and $1$ and train LEM for this system. In \fref{fig:FHN_ms_hist}, we plot the empirical histogram that bins the ranges of learned scales ${\bf \Dt}_n,\overline{\bf \Dt}_n \leq \Dt =2$ (for all $n$ and $d$) and counts the number of occurrences of ${\bf \Dt}_n,\overline{\bf \Dt}_n$ in each bin. From this figure, we observe that there is a clear concentration of learned scales around the values $1$ and $\tau = 0.02$, which exactly correspond to the underlying fast and slow time scales. Thus, for this model problem, LEM is exactly learning what it is designed to do and is able to learn the underlying time scales for this particular problem.
\begin{figure}[ht!]
\centering
\includegraphics[width=7cm]{figures/FHN_ms_hist.pdf}
\caption{Histogram of $({\bf \Dt}_n)_i$ and $({\bf \overline{\Dt}}_n)_i$ for all $n=1,\dots,N$ and $i=1,\dots,d$ of LEM \eqref{eq:lem} after training on the FitzHugh-Nagumo fast-slow system \eqref{eq:FHN} using $\Dt=2$.}
\label{fig:FHN_ms_hist}
\end{figure}

Nevertheless, one might argue that these learnable mutliple scales ${\bf \Dt}_n,\overline{\bf \Dt}_n$ are not necessary and a single scale would suffice to provide good empirical performance. We check this possibility on the FitzHugh-Nagumo data set by simply setting ${\bf \Dt}_n,\overline{\bf \Dt}_n \equiv \Dt {\bf 1}$ (with $\bf 1$ being the vector with all entries set to $1$), for all $n$ and tuning the hyperparameter $\Dt$. The comparative results are presented in \Tref{tab:FHN_wo_ms}. We see from this table by not allowing for learnable ${\bf \Dt}_n,\overline{\bf \Dt}_n$ and simply setting them to a single scale parameter $\Dt$ and tuning this parameter only leads to results that are comparable to the baseline LSTM model. On the other hand, learning ${\bf \Dt}_n,\overline{\bf \Dt}_n$ resulted in an error that is a factor of $6$ less than the baseline LSTM test error. Thus, we demonstrate the importance of the ability of the proposed LEM model to learn multiple scales in this example.  
\begin{table}[h!]
\caption{Test $L^2$ error on FitzHugh-Nagumo system prediction.}
\label{tab:FHN_wo_ms}
\centering
\begin{tabular}{llll}
\toprule
\cmidrule(r){1-4}
Model &  error $(\times10^{-2})$ & \# units & \# params \\
\midrule
LSTM & 1.2 & 16 & 1k  \\
LEM w/o multi-scale & 1.1 & 16 & 1k \\
LEM & 0.2 & 16 & 1k  \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.48\textwidth]{figures/google12_hist_dt1.pdf}
\includegraphics[width=0.48\textwidth]{figures/google12_hist_dt2.pdf}
\caption{Histogram of $({\bf \Dt}_n)_i$ and $({\bf \overline{\Dt}}_n)_i$ for all $n=1,\dots,N$ and $i=1,\dots,d$ of LEM \eqref{eq:lem} after training on the Google12 data set}
\label{fig:g12_hist}
\end{figure}

Hence, the multiscale resolution of LEM seems essential for the fast-slow dynamical system. Does this multiscale resolution also appear for other datasets and can it explain aspects of the observed empirical performance ? To this end, we consider the Google12 Keyword spotting data set and start by pointing out that given the spatial (with respect to input dimension $d$) and temporal (with respect to sequence length $N$) heterogeneities, a priori, it is unclear if the underlying data has a multiscale structure. We plot the empirical histograms of ${\bf \Dt}_n,\overline{\bf \Dt}_n$ in \fref{fig:g12_hist} to observe that even for this problem, the terms ${\bf \Dt}_n,\overline{\bf \Dt}_n$ are expressed over a range of scales, amounting to $2-3$ orders of magnitude. Thus, a range of scales are present in the trained LEM even for this example, but do they affect the empirical performance of LEM ? We investigate this question by performing an ablation study and reporting the results in \fref{fig:clip_ms_g12}. In this study, we clip the values of ${\bf \Dt}_n,\overline{\bf \Dt}_n$ to lie within the range $[2^{-i},1]$, for $i=0,1,\ldots,7$ and plot the statistics of the observed test accuracy of LEM. We observe from \fref{fig:clip_ms_g12} that by clipping  ${\bf \Dt}_n,\overline{\bf \Dt}_n$ to lie near the default (single scale) value of $1$ results in very poor empirical performance of an accuracy of $\approx 65\%$. Then the accuracy jumps to around $90\%$ when an order of magnitude range for  ${\bf \Dt}_n,\overline{\bf \Dt}_n$ is considered, before monotonically and slowly increasing to yield the best empirical performance for the largest range of values of  ${\bf \Dt}_n,\overline{\bf \Dt}_n$, considered in this study. A closer look at the empirical histograms plotted in \fref{fig:g12_hist} reveal that the proportion of occurrences of ${\bf \Dt}_n,\overline{\bf \Dt}_n$ decays as a \emph{power law}, and not exponentially, with respect to the scale amplitude. This, together with results presented in \fref{fig:clip_ms_g12} suggest that not only do a range of scales occur in learned ${\bf \Dt}_n,\overline{\bf \Dt}_n$, the small scales also contribute proportionately to the dynamics and enable the increase in performance shown in \fref{fig:clip_ms_g12}.  
\begin{figure}[ht!]
\centering
\includegraphics[width=7cm]{figures/clip_ms_g12.pdf}
\caption{Average (and standard deviation of) test accuracies of 5 runs each for LEM on Google12, where ${\bf \Dt}_n$ and ${\bf \overline{\Dt}}_n$ in \eqref{eq:lem} are clipped to the ranges $[\frac{1}{2^{i}},1]$ for $i=0,\dots,7$ during training.}
\label{fig:clip_ms_g12}
\end{figure}

Finally, in \fref{fig:hist1}, we plot the empirical histograms of ${\bf \Dt}_n$ and ${\bf \overline{\Dt}}_n$ for the learned LEM on the sMNIST data set to observe that again a range of scales are observed and the observed occurrences of ${\bf \Dt}_n$ and ${\bf \overline{\Dt}}_n$ at each scale decays as a power law with respect to scale amplitude. Hence, we have sufficient empirical evidence to claim that the multi-scale resolution of LEM seems essential to its observed performance. However, further investigation is required to elucidate the precise mechanisms through this multiscale resolution enables superior performance, particularly on problems where the multiscale structure of the underlying data may not be explicit. 
\begin{figure}[ht!]
\centering
\includegraphics[width=0.48\textwidth]{figures/smnist_hist_dt1.pdf}
\includegraphics[width=0.48\textwidth]{figures/smnist_hist_dt2.pdf}
\caption{Histogram of $({\bf \Dt}_n)_i$ and $({\bf \overline{\Dt}}_n)_i$ for all $n=1,\dots,N$ and $i=1,\dots,d$ of LEM \eqref{eq:lem} after training on the sMNIST data set}
\label{fig:hist1}
\end{figure}
\subsection{On gradient-stable initialization.}
\label{sec:chrono}
Specialized weight initialization is a popular tool to increase the performance of RNNs on long-term dependencies tasks. One particular approach is the so-called chrono initialization \citep{warp} for LSTMs, where all biases are set to zero except for the bias of the forget gate as well as the input gate ($\bb_f$ and $\bb_i$ in the LSTM \eqref{eq:lstm}), which are sampled from
\begin{align*}
    \bb_f &\sim \log(\mathcal{U}[1,T_{\text{max}}-1]) \\
    \bb_i &= - \bb_f,
\end{align*}
where $T_{\text{max}}$ denotes the maximal temporal dependency of the underlying sequential data.
We can see in \Tref{tab:worms} that the chrono initialization significantly improves the performance of LSTM on the EigenWorms task. Hence, we are interested in extending the chrono initialization to LEMs. One possible manner for doing this is as follows: Initialize all biases of LEM to zero except for $\bb_1$ in \eqref{eq:lem}, which is sampled from
\begin{equation*}
    \bb_1 \sim -\log(\mathcal{U}[1,T_{\text{max}}\Dt-1]).
\end{equation*}


\begin{table}[h!]
\caption{Test accuracies on EigenWorms using $5$ re-trainings of each best performing network (based on the validation set), where we train LSTM and LEM with and without chrono intialization, as well as LEM without chrono initialization but with tuned $\Dt$.}
\label{tab:worms_chrono}
\centering
\begin{tabular}{llllll}
\toprule
\cmidrule(r){1-6}
Model &  test accuracy & \# units & \# params & chrono & tuning $\Dt$\\
\midrule
LSTM & 38.5\% $\pm$ 10.1\% & 32 & 5.3k & NO & / \\
LSTM & 82.6 \% $\pm$ 6.4\% & 32 & 5.3k & YES & / \\
LEM & 57.9\% $\pm$ 7.7\% & 32 & 5.3k & NO & NO \\
LEM & 88.2\% $\pm$ 6.9\% & 32 & 5.3k & YES & NO \\
LEM & 92.3\% $\pm$ 1.8\% & 32 & 5.3k & NO & YES \\
    \bottomrule
  \end{tabular}
\end{table}

We test the chrono initialization for LEM on the EigenWorms dataset, where we train LEM (without tuning $\Dt$, i.e., setting $\Dt=1$), with and without chrono initialization. We provide the results in \Tref{tab:worms_chrono}, where we show again the results of LSTM with and without chrono initialization as well as the LEM result with tuned $\Dt$ and without chrono initialization from \Tref{tab:worms} for comparison. 
We see from \Tref{tab:worms_chrono} that when $\Dt$ is fixed to $1$, the chrono initialization significantly improves the result of LEM. However, if we tune $\Dt$, but do not use the chrono initialization, we significantly improve the performance of LEM again. We further remark that tuning $\Dt$ as well as using chrono initialization for LEM does not improve the results obtained with simply tuning $\Dt$ in LEM. Thus, we conclude that chrono initialization can successfully be adapted to LEM. However, tuning $\Dt$ (which controls the gradients) is still advisable in order to obtain the best possible results.






\section{Relation between LEM and The Hodgkin-Huxley equations}
\label{app:HH}
We observe that the multiscale ODEs \eqref{eq:ode}, on which LEM is based, are a special case of the following ODE system,
\begin{equation}
    \label{eq:ode1}
    \begin{aligned}
    \frac{d\bz}{dt} &= {\mathbf{F}}_z\left(\by,t\right) - {\mathbf{G}}_z(\by,t)\odot \bz, \quad \frac{d\by}{dt} = {\mathbf{F}}_y\left(\bz,t\right)\odot{\mathbf{H}}\left(\by,t\right) - {\mathbf{G}}_y(\by,t)\odot \by.
    \end{aligned}
\end{equation}
As remarked in the main text, it turns out the well-known Hodgkin-Huxley equations \citet{HH}, modeling the the dynamics of the action potential of a biological neuron can also be written down in the abstract form \eqref{eq:ode1}, with $d_y = 1$, $d_z = 3$ and the variables $\by = y$ modeling the voltage and $\bz = (z_1,z_2,z_3)$ modeling the concentration of Potassium activation, Sodium activation and Sodium inactivation channels. 

The exact form of the different functions in \eqref{eq:ode1} for the Hodgkin-Huxley equations is given by,
\begin{equation}
    \label{eq:HH}
    \begin{aligned}
    {\mathbf{F}}_z(y) &= \left(\alpha_1(y),\alpha_2(y),\alpha_3(y)\right), \\
    {\mathbf{G}}_z(y) &= \left(\alpha_1(y)+\beta_1(y),\alpha_2(y)+\beta_2(y),\alpha_3(y)+\beta_3(y)\right), \\
    \alpha_1(y) &= \frac{0.01(10+\hat{y}-y)}{e^{\frac{10+\hat{y}-y}{10}}-1}, \quad \alpha_2(y) = \frac{0.1(25+\hat{y}-y)}{e^{\frac{25+\hat{y}-y}{10}}-1}, \quad \alpha_3(y) = 0.07e^{\frac{\hat{y}-y}{20}}, \\
    \beta_1(y) &= 0.125e^{\frac{\hat{y}-y}{80}}, \quad \beta_2(y) = 4e^{\frac{\hat{y}-y}{18}}, \quad \beta_3(y) = \frac{1}{1+ e^{1+\frac{\hat{y}-y}{10}}}, \\
    {\mathbf{F}}_y(z,t) &= u(t) + z_1^4 + z_2^3z_3, \quad
    {\mathbf{H}}(y) = c_1(\bar{y}-y) + c_2(\bar{y}-y), \quad {\mathbf{G}}_y(y) = c_3,
    \end{aligned}
\end{equation}
with input current $u$ and constants $\hat{y},\bar{y},c_{1,2,3}$, whose exact values can be read from \citep{HH}. 

Thus, the multiscale ODEs \eqref{eq:ode} and the Hodgkin-Huxley equations are special case of the same general family \eqref{eq:ode1} of ODEs. Moreover, the \emph{gating functions} $\mathbf{G}_{y,z}(\by)$, that model voltage-gated ion channels in the Hodgkin-Huxley equations, are similar in form to ${\bf \Dt}_n,\overline{\bf \Dt}_n$ in \eqref{eq:ode}.  

It is also worth highlighting the differences between our proposed model LEM (and the underlying ODE system \eqref{eq:ode}) and the Hodgkin-Huxley ODEs modeling the dynamics of the neuronal action potential. Given the complicated form of the nonlinearites $\mathbf{F}_{y,z},\mathbf{G}_{y,z},\mathbf{H}$ in the Hodgkin-Huxley equations \eqref{eq:HH}, we cannot use them in designing any learning model. Instead, building on the abstract form of \eqref{eq:ode1}, we propose \emph{bespoke} non-linearities in the ODE \eqref{eq:ode} to yield a tractable learning model, such as LEM \eqref{eq:lem}. Moreover, it should be emphasized that the Hodgkin-Huxley equations only model the dynamics of a single neuron (with a scalar voltage and 3 ion channels), whereas the hidden state dimension $d$ of \eqref{eq:ode} can be arbitrary. 

\section{Relation between LEM and LSTM}
\label{app:lstm}
The well-known LSTM \citep{lstm} (in its mainly-used version using a forget gate \citep{lstm_w_forget}) is given by, 
\begin{equation}
    \label{eq:lstm}
    \begin{aligned}
    {\bf f}_n &= \hat{\sigma}(\bW_f{\bf h}_{n-1} + \bV_f\bu_{n} + \bb_f) \\
    {\bf i}_n &= \hat{\sigma}(\bW_i{\bf h}_{n-1} + \bV_i\bu_{n} + \bb_i) \\
    {\bf o}_n &= \hat{\sigma}(\bW_o{\bf h}_{n-1} + \bV_o\bu_{n} + \bb_o) \\
    {\bf c}_n &= {\bf f}_n \odot {\bf c}_{n-1} + {\bf i}_n \odot\sigma(\bW {\bf h}_{n-1} + \bV \bu_{n} + \bb) \\
    {\bf h}_n &= {\bf o}_n \odot \sigma({\bf c}_n).
\end{aligned}
\end{equation}
Here, for any $1 \leq n \leq N$, ${\bf h}_n \in \R^d$ is the hidden state and ${\bf c}_n \in \R^d$ is the so-called \emph{cell state}. The vectors ${\bf i}_n,{\bf f}_n,{\bf o}_n \in \R^d$ are the \emph{input, forget} and \emph{output} gates, respectively. $\bu_n \in \R^m$ is the input signal and the weight matrices and bias vectors are given by $\bW, \bW_{f,i,o}\in \R^{d\times d}, \bV, \bV_{f,i,o}\in \R^{m \times d}$ and $\bb,\bb_{f,i,o} \in \R^d$, respectively. 

It is straightforward to relate LSTM \eqref{eq:lstm} and LEM \eqref{eq:lem} by first setting the cell state ${\bf c}_n = \bz_n$, for all $1 \leq n \leq N$ and the hidden state ${\bf h}_{n} = \by_n$.

We further need to assume that the input state ${\bf i}_n = {\bf \Dt}_n$ and the forget state has to be ${\bf f}_n = 1 - {\bf \Dt}_n$. Finally, the output state of the LSTM \eqref{eq:lstm} has to be
$$
{\bf o}_n = \overline{\bf \Dt}_n = 1, \quad \forall 1 \leq n \leq N.
$$
Under these assumptions and by setting $\Dt =1$, we can readily observe that the LEM \eqref{eq:lem} and LSTM \eqref{eq:lstm} are equivalent.

A different interpretation of LEM, in relation to LSTM, is as follows; LEM can be thought of a variant of LSTM but with two cell states $\by_n,\bz_n$ per unit and no output gate. The input gates are ${\bf \Dt}_n$ and ${\bf \overline{\Dt}}_n$ and the forget gates are coupled to the input gates. Given that the state $\bz_n$ is fed into the update for the state $\by_n$, one can think of one of the cell states sitting above the other, leading to a more sophisticated recursive update for LEM \eqref{eq:lem}, when compared to LSTM \eqref{eq:lstm}. 


\begin{table}[h!]
\caption{Test accuracies on EigenWorms using $5$ re-trainings of each best performing network (based on the validation set) for LSTMs with $\Dt$-scaled input and forget gates, as well as LSTMs with sub-sampling routines, baseline LSTM and LEM.}
\label{tab:dt_scaled_lstms}
\centering
\begin{tabular}{llll}
\toprule
\cmidrule(r){1-4}
Model & test accuracy & \# units & \# params \\
\midrule
t-BPTT LSTM & 57.9\% $\pm$ 7.0\% &32 & 5.3k\\
sub-samp. LSTM &  69.2\% $\pm$ 8.3\% &32 & 5.3k\\
LSTM & 38.5\% $\pm$ 10.1\% & 32 & 5.3k  \\
$\Dt$-LSTM v$1$ & 53.3\% $\pm$ 8.2\% & 32 & 5.3k  \\
$\Dt$-LSTM v$2$ & 56.9\% $\pm$ 6.7\% & 32 & 5.3k  \\
LEM & 92.3\% $\pm$ 1.8\% & 32 & 5.3k \\
    \bottomrule
  \end{tabular}
\end{table}

Another key difference between LEM and LSTM is the scaling of the learnable gates in LEM by the small hyperparameter $\Dt$. It is natural to examine whether scaling LSTM with such a small hyperparameter $\Dt$ will improve its performance on sequential tasks with long-term dependencies. To this end, we propose to \emph{scale} the input and forget gate of an LSTM with a small hyperparameter in two different ways, where we denote the new forget gate as $\hat{{\bf f}}_n$ and the new input gate as $\hat{{\bf i}}_n$. The first version is ($\Dt$-LSTM v$1$)
\begin{equation*}
    \hat{{\bf f}}_n = \Dt{\bf f}_n, \quad \hat{{\bf i}}_n = \Dt{\bf i}_n,
\end{equation*}
while the second version is ($\Dt$-LSTM v$2$)
\begin{equation*}
    \hat{{\bf f}}_n = (1-\Dt){\bf f}_n, \quad \hat{{\bf i}}_n = \Dt{\bf i}_n.
\end{equation*}

We can see in \Tref{tab:dt_scaled_lstms} that both $\Dt$-scaled versions of the LSTM lead to some improvements over the baseline LSTM for very long-sequence Eigenworms dataset, while still performing very poorly when compared to LEM. Moreover, we can see that standard sub-sampling routines, such as a truncation of the BPTT algorithm or random sub-sampling, applied to LSTMs lead to better improvements than $\Dt$-scaling the forget and input gate.


\section{Supplement to the rigorous analysis of LEM}
\label{sec:rigan}
In this section, we will provide detailed proofs of the propositions in Section \ref{sec:rig} of the main article. We start with the following simplifying notation for various terms in LEM \eqref{eq:lem},
\begin{align*}
    \bA_{n-1} &= \bW_1\by_{n-1} + \bV_1\bu_{n} + \bb_1, \\
    \bB_{n-1} &= \bW_2\by_{n-1} + \bV_2\bu_{n} + \bb_2, \\
    \bC_{n-1} &= \bW_z\by_{n-1} + \bV_z\bu_{n} + \bb_z, \\
    \bD_{n} &= \bW_y\bz_{n} + \bV_y\bu_{n} + \bb_y .
\end{align*}
Note that for all $1 \leq n \leq N$, $\bA_n,\bB_n, \bC_n,\bD_n \in \R^d$. 
With the above notation, LEM \eqref{eq:lem} can be written componentwise, for each component $1 \leq i \leq d$ as,
\begin{equation}
    \label{eq:lemc}
    \begin{aligned}
    \bz_{n}^i &= \bz_{n-1}^i + \Dt \hat{\sigma}(\bA_{n-1}^i) \sigma(\bC_{n-1}^i) - \Dt \hat{\sigma}(\bA_{n-1}^i) \bz_{n-1}^i, \\
    \by_{n}^i &= \by_{n-1}^i + \Dt \hat{\sigma}(\bB_{n-1}^i) \sigma(\bD_{n}^i) - \Dt \hat{\sigma}(\bB_{n-1}^i) \by_{n-1}^i .
    \end{aligned}
\end{equation}
Moreover, we will use the following \emph{order}-notation,
\begin{equation}
    \label{eq:ord}
    \begin{aligned}
     {\bf \beta} &= \ord(\alpha), {\rm for}~\alpha,\beta \in \R_+ \quad {\rm if~there~exists~constants}~ \overline{C},\underline{C}~{\rm such~that}~\underline{C} \alpha \leq \beta \leq \overline{C} \alpha. \\
   {\bf M} &= \ord(\alpha), {\rm for}~{\bf M} \in \R^{d_1 \times d_2}, \alpha \in \R_+ \quad {\rm if~there~exists~constant}~ \overline{C}~{\rm such~that}~\|{\bf M}\| \leq \overline{C} \alpha.
   \end{aligned}
\end{equation}


Note that the techniques of proof in this following three sub-sections burrow heavily from those introduced in \citet{coRNN}.
\subsection{Proof of Proposition \ref{prop:1} of main text.}
\label{app:hsbdpf}


First, we prove Proposition \ref{prop:1}, which yields the bound \eqref{eq:hsb} for the hidden states of LEM.
\begin{proof}
The proof of the bound \eqref{eq:hsb} is split into 2 parts. We start with the first equation in \eqref{eq:lemc} and rewrite it as,
\begin{align*}
    \bz_{n}^i &= \left(1 - \Dt \hat{\sigma}(\bA_{n-1}^i) \right) \bz_{n-1}^i + \Dt \hat{\sigma}(\bA_{n-1}^i) \sigma(\bC_{n-1}^i).
\end{align*}
Noting that the activation functions are such that $0 \leq \hat{\sigma}(x) \leq 1$, for all $x$ and $-1 \leq \sigma(x) \leq 1$, for all $x$ and using the fact that $\Dt \leq 1$, we have from the above expression that,
\begin{align*}
    \bz_{n}^i &\leq \left(1 - \Dt \hat{\sigma}(\bA_{n-1}^i) \right) \max\left(\bz_{n-1}^i,1\right) +  \Dt \hat{\sigma}(\bA_{n-1}^i) \max\left(\bz_{n-1}^i,1\right) , \\
    &\leq \max\left(\bz_{n-1}^i,1\right).
    \end{align*}
By a symmetric argument, one can readily show that,
\begin{align*}
     \bz_{n}^i &\geq \min(-1,\bz_{n-1}^i).
\end{align*}
Combining the above inequalities yields,
\begin{equation}
    \label{eq:mp1}
   \min(-1,\bz_{n-1}^i) \leq  \bz_{n}^i \leq  \max\left(\bz_{n-1}^i,1\right).
\end{equation}
Iterating \eqref{eq:mp1} over $n$ and using $z_0^i=0$ for all $1 \leq i \leq d$ leads to,
\begin{equation}
    \label{eq:mp2}
    -1 \leq \bz_n^i \leq 1, \quad \forall n, \quad \forall 1 \leq i \leq d. 
\end{equation}

An argument, identical to the derivation of \eqref{eq:mp2}, but for the hidden state $\by$ yields,
\begin{equation}
    \label{eq:mp3}
    -1 \leq \by_n^i \leq 1, \quad \forall n, \quad \forall 1 \leq i \leq d. 
\end{equation}
Thus, we have shown that the hidden states remain in the interval $[-1,1]$, irrespective of the sequence length. 

Next, we will use the following elementary identities in the proof,
\begin{equation}
    \label{eq:id}
    b(a-b) = \frac{a^2}{2} - \frac{b^2}{2} - \frac{1}{2}\left(a-b\right)^2,
\end{equation}
for any $a,b \in \R$, and also,
\begin{equation}
    \label{eq:id1}
    ab \leq \frac{\epsilon a^2}{2} + \frac{b^2}{2 \epsilon}, \quad \forall \epsilon > 0.
\end{equation}

We fix $1 \leq i \leq d$ and multiply the first equation in \eqref{eq:lemc} with $\bz^i_{n-1}$ and apply \eqref{eq:id} to obtain,

    \begin{align*}
    \frac{(\bz^i_n)^2}{2} &= \frac{(\bz^i_{n-1})^2}{2} +\Dt \hat{\sigma}(\bA^i_{n-1}) \sigma(\bC^i_{n-1}) \bz^i_{n-1} - \Dt \hat{\sigma}(\bA^i_{n-1}) (\bz^i_{n-1})^2 + \frac{(\bz^i_n-\bz^i_{n-1})^2}{2} \\
    &= \frac{(\bz^i_{n-1})^2}{2} +\Dt \hat{\sigma}(\bA^i_{n-1}) \sigma(\bC^i_{n-1}) \bz^i_{n-1} - \Dt \hat{\sigma}(\bA^i_{n-1}) (\bz^i_{n-1})^2 \\
    &+ \frac{\Dt^2}{2}\left(\hat{\sigma}(\bA^i_{n-1}) \sigma(\bC^i_{n-1})-\hat{\sigma}(\bA^i_{n-1})\bz_{n-1}^i\right)^2, \quad ({\rm from~\eqref{eq:lemc}})\\
    &\leq \frac{(\bz^i_{n-1})^2}{2} +\Dt \hat{\sigma}(\bA^i_{n-1}) |\sigma(\bC^i_{n-1})| |\bz^i_{n-1}| - \Dt \hat{\sigma}(\bA^i_{n-1}) (\bz^i_{n-1})^2 \\
    &+ \frac{\Dt^2}{2} (\hat{\sigma}(\bA^i_{n-1}) \sigma(\bC^i_{n-1}))^2 + \frac{\Dt^2}{2} \hat{\sigma}(\bA^i_{n-1})^2 (\bz^i_{n-1})^2 \\
    &+ \Dt^2\hat{\sigma}(\bA^i_{n-1})^2|\sigma(\bC^i_{n-1})||\bz^i_{n-1}|\quad ({\rm as}~(a-b)^2 \leq a^2+b^2+2|a||b|)
    \end{align*}
We fix $\epsilon = \frac{2-\Dt}{1+\Dt}$ in the elementary identity \eqref{eq:id1} to yield, 
\begin{align*}
|\sigma(\bC^i_{n-1})||\bz^i_{n-1}| \leq \frac{\sigma(\bC^i_{n-1})^2}{2 \epsilon} + \frac{\epsilon(\bz^i_{n-1})^2}{2}
\end{align*}
Applying this to the inequality for $(\bz^i_n)^2$ leads to,    
    
    \begin{align*}
    \frac{(\bz^i_n)^2}{2} &\leq 
     \frac{(\bz^i_{n-1})^2}{2} +  \left(\Dt \hat{\sigma}(\bA^i_{n-1}) + \Dt^2 \hat{\sigma}(\bA^i_{n-1})^2 \right) \frac{\sigma(\bC^i_{n-1})^2}{2 \epsilon}   \\
     &- \Dt \hat{\sigma}(\bA^i_{n-1}) \left[1 - \frac{\epsilon}{2} - \frac{\Dt \hat{\sigma}(\bA^i_{n-1})}{2} - \frac{\Dt \hat{\sigma}(\bA^i_{n-1}) \epsilon}{2} \right](\bz^i_{n-1})^2 . \\
    \end{align*}
Using the fact that $0 \leq \hat{\sigma}(x) \leq 1$ for all $x \in \R$, $\sigma^2 \leq 1$ and that $\epsilon = \frac{2-\Dt}{1+\Dt}$, we obtain from the last line of the previous equation that,
\begin{align*}
    (\bz_n^i)^2 &\leq (\bz_{n-1}^i)^2 + \frac{\Dt + \Dt^2}{\epsilon} \leq (\bz_{n-1}^i)^2 + \frac{\Dt(1+\Dt)^2}{2-\Dt} , \quad \forall 1 \leq n.
\end{align*}
Iterating the above estimate over $n=1,\ldots,\bar{n}$, for any $1 \leq \bar{n}$ and setting $\bar{n} = n$ yields,
\begin{align*}
    (\bz_n^i)^2 &\leq (\bz_0^i)^2 + n \frac{\Dt(1+\Dt)^2}{2-\Dt}, \\
    \Rightarrow\quad (\bz_n^i)^2 &\leq t_n\frac{(1+\Dt)^2}{2-\Dt} \quad {\rm as}~\bz^i_0 =0,~t_n = n \Dt. 
\end{align*}
Taking a square root in the above inequality yields,
\begin{equation}
    \label{eq:en100}
    |\bz^i_n| \leq \bdel \sqrt{t_n}, \quad \forall n, \quad \forall 1 \leq i \leq d.
\end{equation}
with $\bdel$ defined in the expression \eqref{eq:hsb}. 

We can repeat the above argument with the hidden state $\by$ to obtain,
\begin{equation}
    \label{eq:en101}
    |\by^i_n| \leq \bdel \sqrt{t_n}, \quad \forall n, \quad \forall 1 \leq i \leq d.
\end{equation}
Combining \eqref{eq:en100} and \eqref{eq:en101} with the pointwise bounds \eqref{eq:mp2} and \eqref{eq:mp3} yields the desired bound \eqref{eq:hsb}. 
\end{proof}
\subsection{Proof of Proposition \ref{prop:2} of main text.}
\label{app:hsgubpf}
\begin{proof}


We can apply the chain rule repeatedly (for instance as in \cite{vanish_grad}) to obtain,
\begin{equation}
\label{eq:grad2}
\frac{\partial \E_n}{\partial \theta} = \sum\limits_{1 \leq k \leq n} \underbrace{\frac{\partial \E_n}{\partial \bX_n} \frac{\partial \bX_n}{\partial \bX_k} \frac{\partial^{+} \bX_k}{\partial \theta}}_{\frac{\partial \E^{(k)}_n}{\partial \theta}}.
\end{equation}
Here, the notation $\frac{\partial^{+} \bX_k}{\partial \theta}$ refers to taking the partial derivative of $\bX_k$ with respect to the parameter $\theta$, while keeping the other arguments constant.

A straightforward application of the product rule yields,
\begin{equation}
\label{eq:grad5}
 \frac{\partial \bX_n}{\partial \bX_k} = \prod\limits_{k < \ell \leq n} \frac{\partial \bX_\ell}{\partial \bX_{\ell-1}} .
\end{equation}
For any $k < \ell \leq n$, a tedious yet straightforward computation yields the following representation formula,
\begin{equation}
    \label{eq:grad6}
    \frac{\partial \bX_\ell}{\partial \bX_{\ell-1}} = {\bf I}_{2d \times 2d} + \Dt \bE^{\ell,\ell-1} + \Dt^2 \bF^{\ell,\ell-1}.
\end{equation}
Here $\bE^{\ell,\ell-1} \in \R^{2d \times 2d}$ is a matrix whose entries are given below. For any $1 \leq i \leq d$, we have,
\begin{equation}
    \label{eq:grad7}
    \begin{aligned}
    \bE^{\ell,\ell-1}_{2i-1,2j-1} &\equiv 0, \quad j \neq i\\
    \bE^{\ell,\ell-1}_{2i-1,2i-1} &= - \hat{\sigma}(\bA^i_{\ell-1}), \\
    \bE^{\ell,\ell-1}_{2i-1,2j} &= (\bW_1)_{i,j}\hat{\sigma}^{\prime}(\bA^i_{\ell-1})\left(\sigma(\bC^i_{\ell-1})-\bz^i_{\ell-1}\right) + (\bW_z)_{i,j} \hat{\sigma}(\bA^i_{\ell-1})\sigma^{\prime}(\bC^i_{\ell-1}),~\forall 1 \leq j \leq d \\
    \bE^{\ell,\ell-1}_{2i,2j-1} &= (\bW_y)_{i,j} \hat{\sigma}(\bB^i_{\ell-1})\sigma^{\prime}(\bD^i_{\ell}), ~\forall 1 \leq j \leq d  \\
    \bE^{\ell,\ell-1}_{2i,2j} &= (\bW_2)_{i,j}\hat{\sigma}^{\prime}(\bB^i_{\ell-1})\left(\sigma(\bD^i_{\ell}) - \by^i_{\ell-1}\right), \quad j \neq i \\
    \bE^{\ell,\ell-1}_{2i,2i} &= -\hat{\sigma}(\bB^i_{\ell-1}) +  (\bW_2)_{i,i}\hat{\sigma}^{\prime}(\bB^i_{\ell-1})\left(\sigma(\bD^i_{\ell}) - \by^i_{\ell-1}\right). 
      \end{aligned}
\end{equation}
Similarly, $\bF^{\ell,\ell-1} \in \R^{2d \times 2d}$ is a matrix whose entries are given below. For any $1 \leq i \leq d$, we have,
\begin{equation}
    \label{eq:grad8}
    \begin{aligned}
    \bF^{\ell,\ell-1}_{2i-1,j} &\equiv 0, \quad \forall~1\leq j \leq 2d, \\
    \bF^{\ell,\ell-1}_{2i,2j-1} &= - (\bW_y)_{i,j}\hat{\sigma}(\bA^j_{\ell-1})\hat{\sigma}(\bB^i_{\ell-1})\sigma^{\prime}(\bD^i_{\ell}), \quad 1 \leq j \leq d, \\
   \bF^{\ell,\ell-1}_{2i,2j} &= \hat{\sigma}(\bB^i_{\ell-1})\sigma^{\prime}(\bD^i_\ell)\sum\limits_{\lambda=1}^d (\bW_y)_{i,\lambda} \left(\left(\sigma(\bC^\lambda_{\ell-1})-\bz^{\lambda}_{\ell-1}\right)\hat{\sigma}^{\prime}(\bA^{\lambda}_{\ell-1})(\bW_1)_{\lambda,j} + \hat{\sigma}(\bA^{\lambda}_{\ell-1})\sigma^{\prime}(\bC^{\lambda}_{\ell-1})(\bW_z)_{\lambda,j}\right)  .
    \end{aligned}
\end{equation}
Using the fact that,
$$
\sup\limits_{x\in \R} \max\left\{|\sigma(x)|,|\sigma^{\prime}(x)|,|\hat{\sigma}(x)|,|\hat{\sigma}^\prime(x)|\right\} \leq 1,
$$
the pointwise bounds \eqref{eq:hsb}, the notation $t_n= n \Dt$ for all $n$, the definition of $\eta$ \eqref{eq:gbd} and the definition of matrix norms, we obtain that,
\begin{equation}
    \label{eq:grad9}
    \begin{aligned}
    \|\bE^{\ell,\ell-1}\|_{\infty} &\leq \max \left\{1 + \|\bW_z\|_{\infty}+(1+\min(1,\bdel\sqrt{t_\ell})) \|\bW_1\|_{\infty}, 1 + \|\bW_y\|_{\infty}+(1+\min(1,\bdel\sqrt{t_\ell}))\|\bW_2\|_{\infty} \right\} \\
    &\leq 1 + (2+\min(1,\bdel\sqrt{t_\ell}))\eta. 
    \end{aligned}
\end{equation}
By similar calculations, we obtain,
\begin{equation}
    \label{eq:grad10}
    \begin{aligned}
    \|\bF^{\ell,\ell-1}\|_{\infty} &\leq \|\bW_y\|_{\infty}\left(1 + (1+\min(1,\bdel\sqrt{t_\ell})) \|\bW_1\|_{\infty}+ \|\bW_z\|_{\infty}\right) \\
    &\leq \eta(1 + (2+\min(1,\bdel\sqrt{t_\ell}))\eta). 
    \end{aligned}
\end{equation}
Applying \eqref{eq:grad9} and \eqref{eq:grad10} in the representation formula \eqref{eq:grad6} and observing that $\Dt \leq 1$ and $\ell \leq n$, we obtain.
\begin{align*}
   \left \| \frac{\partial \bX_\ell}{\partial \bX_{\ell-1}}\right\|_{\infty} &\leq 1 +   \left(1 + (2+\min(1,\bdel\sqrt{t_\ell}))\eta\right)\Dt+ \eta\left(1 + (2+\min(1,\bdel\sqrt{t_\ell}))\eta\right)\Dt^2, \\
   &\leq 1 + \frac{\Gamma}{2}\Dt, 
\end{align*}
With 
\begin{equation}
    \label{eq:gm}
    \Gamma = 2\left(1 + \eta\right)(1 + 3 \eta) \end{equation}

Using the expression \eqref{eq:grad5} with the above inequality yields,
\begin{equation}
    \label{eq:grad11}
    \left \| \frac{\partial \bX_n}{\partial \bX_{k}}\right\|_{\infty} \leq \left(1 +  \frac{\Gamma}{2}\Dt \right)^{n-k} .
\end{equation}

Next, we choose $\Dt << 1$ small enough such that the following holds, 
\begin{equation}
\label{eq:grad12}
\left(1 +  \frac{\Gamma}{2}\Dt \right)^{n-k} \leq 1 + \Gamma(n-k)\Dt, 
\end{equation}
for any $1 \leq k < n$.

Hence applying \eqref{eq:grad12} in \eqref{eq:grad11}, we obtain,
\begin{equation}
    \label{eq:grd1}
     \left \| \frac{\partial \bX_n}{\partial \bX_{k}}\right\|_{\infty} \leq 1 + \Gamma(n-k)\Dt .
\end{equation}

For the sake of definiteness, we fix any $1 \leq \alpha,\beta \leq d$ and set $\theta = (\bW_y)_{\alpha,\beta}$ in the following. The following bounds for any other choice of $\theta \in \Theta$ can be derived analogously. Given this, it is straightforward to calculate from the structure of LEM \eqref{eq:lem} that entries of the vector $\frac{\partial^+\bX_k}{\partial (\bW_y)_{\alpha,\beta}}$ are given by,
\begin{equation}
    \label{eq:grd2}
    \begin{aligned}
    \left(\frac{\partial^+\bX_k}{\partial (\bW_y)_{\alpha,\beta}}\right)_{j} &\equiv 0, \quad \forall~j \neq 2\alpha, \\
\left(\frac{\partial^+\bX_k}{\partial (\bW_y)_{\alpha,\beta}}\right)_{2\alpha}&= \Dt \hat{\sigma}(\bB^\alpha_{k-1})\sigma^{\prime}(\bD^\alpha_{k})\bz^\beta_{k}.
    \end{aligned}
\end{equation}
Hence, by the pointwise bounds \eqref{eq:hsb}, we obtain from \eqref{eq:grd2} that
\begin{equation}
    \label{eq:grd3}
    \left\|\frac{\partial^+\bX_k}{\partial (\bW_y)_{\alpha,\beta}}\right\|_{\infty} \leq \Dt \min(1,\bdel\sqrt{t_k}) .
\end{equation}

Finally, it is straightforward to calculate from the loss function $\E_n = \frac{1}{2}\|\by_n - \overline{\by}_n\|^2$ that
\begin{equation}
    \label{eq:grd4}
    \frac{\partial \E_n}{\partial \bX_n}=  \left[0,\by_n^1-\bar{\by}^1,\ldots \ldots,0,\by_n^d-\bar{\by}^d\right].
\end{equation}
Therefore, using the pointwise bounds \eqref{eq:hsb} and the notation $\hat{\bY} = \|\bar{\by}\|_{\infty}$, we obtain
\begin{equation}
    \label{eq:grd5}
    \left\|\frac{\partial \E_n}{\partial \bX_n}\right\|_{\infty} \leq \hat{\bY} +  \min(1, \bdel \sqrt{t_n}). 
\end{equation}

Applying \eqref{eq:grd1}, \eqref{eq:grd3} and \eqref{eq:grd5} in the definition \eqref{eq:grad2} yields,
\begin{equation}
    \label{eq:grd61}
    \left|\frac{\partial \E^{(k)}_n}{\partial (\bW_y)_{\alpha,\beta}}\right| \leq 
    \Dt \min(1,\bdel\sqrt{t_k})\left(\hat{\bY} +   \min(1,\bdel \sqrt{t_n})\right) \left(1 + \Gamma(n-k)\Dt\right) .
\end{equation}
Observing that $1 \leq k \leq n$, we see that $n - k \leq n$ and $t_k \leq t_n$. Therefore, \eqref{eq:grd6} can be estimated for any $1 \leq k \leq n$ by, 
\begin{equation}
    \label{eq:grd6}
    \left|\frac{\partial \E^{(k)}_n}{\partial (\bW_y)_{\alpha,\beta}}\right| \leq 
    \Dt \min(1,\bdel\sqrt{t_n})\left(\hat{\bY} +  \min(1,\bdel \sqrt{t_n})\right) \left(1 + \Gamma t_n \right), \quad 1 \leq k \leq n.
\end{equation}

Applying the bound \eqref{eq:grd6} in \eqref{eq:grad2} leads to the following bound on the total gradient,
\begin{equation}
    \label{eq:grd8}
    \begin{aligned}
     \left|\frac{\partial \E_n}{\partial (\bW_y)_{\alpha,\beta}}\right| &\leq \sum\limits_{k=1}^n  \left|\frac{\partial \E^{(k)}_n}{\partial (\bW_y)_{\alpha,\beta}}\right| \\
     &\leq  t_n \min(1,\bdel\sqrt{t_n})\left(\hat{\bY} +  \min(1,\bdel \sqrt{t_n})\right) \left(1 + \Gamma t_n \right) \\
     &\leq t_n (1+\hat{\bY})(1+\Gamma t_n) \\
     &\leq (1+\hat{\bY}) t_n + (1+\hat{\bY}) \Gamma t_n^2
     \end{aligned}
\end{equation}
which is the desired bound \eqref{eq:gbd} for $\theta = (\bW_y)_{\alpha,\beta}$.



Moreover, for \emph{long-term dependencies} i.e., $k << n$, we can set $t_k = k \Dt < 1$, with $k$ independent of sequence length $n$, in \eqref{eq:grd61} to obtain the following bound on the partial gradient, 
\begin{equation}
    \label{eq:grd601}
    \left|\frac{\partial \E^{(k)}_n}{\partial (\bW_y)_{\alpha,\beta}}\right| \leq 
    \Dt^{\frac{3}{2}} \bdel\sqrt{k}\left(1+\hat{\bY}\right) \left(1 + \Gamma t_n \right), \quad 1 \leq k << n.
\end{equation}
\end{proof}
\begin{remark}
\label{rem:gbd}
The bound \eqref{eq:gbd} on the total gradient depends on $t_n= n\Dt$, with $n$ being the sequence length and $\Dt \leq 1$, a hyperparameter which can either be chosen a priori or determined through a hyperparameter tuning procedure. The proof of the bound \eqref{eq:grd8} relies on $\Dt$ being sufficiently small. It would be natural to choose $\Dt \sim n^{-s}$, for some $s \geq 0$. Substituting this expression in \eqref{eq:gbd} leads to a bound of the form, 
\begin{equation}
    \label{eq:gbdord1}
    \left|\frac{\partial \E_n}{\partial \theta} \right| = \ord\left(n^{2(1-s)}\right)
\end{equation}

If $s =1$, then clearly $\left|\frac{\partial \E_n}{\partial \theta} \right| = \ord(1)$ i.e., the total gradient is bounded. Clearly, the exploding gradient problem is mitigated in this case.

On the other hand, if $s$ takes another value, for instance $s=\frac{1}{2}$ which is empirically observed during the hyperparameter training (see Section \ref{app:dt}, then we can readily observe from \eqref{eq:gbdord1} that $\left|\frac{\partial \E_n}{\partial \theta} \right| = \ord(n)$. Thus in this case, the gradient can grow with sequence length $n$ but only linearly and not exponentially. Thus, the exploding gradient problem is also mitigated in this case. 

\end{remark}
\subsection{Proof of Proposition \ref{prop:3cor} of main text.}
\label{app:hsglb}
To mitigate the vanishing gradient problem, we need to obtain a more precise characterization of the gradient $\frac{\partial \E^{(k)}_n}{\partial \theta}$ defined in \eqref{eq:grad2}. For the sake of definiteness, we fix any $1 \leq \alpha,\beta \leq d$ and set $\theta = (\bW_y)_{\alpha,\beta}$ in the following. The following formulas for any other choice of $\theta \in \Theta$ can be derived analogously. Moreover, for simplicity of notation, we set the target function $\bar{\bX}_n \equiv 0$. 

Proposition \ref{prop:3cor} is a straightforward corollary of the following, 
\begin{proposition}
Let $\by_n,\bz_n$ be the hidden states generated by LEM \eqref{eq:lem}, then we have the following representation formula for the hidden state gradient,
\label{prop:3}
\begin{equation}
    \label{eq:glb}
    \begin{aligned}
    &\frac{\partial \E^{(k)}_n}{\partial \theta} = 
    \Dt \hat{\sigma}(\bB^\alpha_{k-1})\sigma^{\prime}(\bD^\alpha_{k})\bz^\beta_{k}\left(\by^{\alpha}_n - \bar{\by}^\alpha_n\right) \\
&+\Dt^2\hat{\sigma}(\bB^\alpha_{k-1})\sigma^{\prime}(\bD^\alpha_{k})\bz^\beta_{k}\left[\sum\limits_{j=1}^d \left(\by_n^{j} - \bar{\by}_n^j\right)\sum\limits_{\ell=k+1}^n\hat{\sigma}^{\prime}(\bB^j_{\ell-1})\left(\sigma(\bD^j_{\ell}) - \by^j_{\ell-1}\right)(\bW_2)_{j,2\alpha}\right] \\
&+\Dt^2\hat{\sigma}(\bB^\alpha_{k-1})\sigma^{\prime}(\bD^\alpha_{k})\bz^\beta_{k}\left[\sum\limits_{\ell=k+1}^n \hat{\sigma}(\bB^{\alpha}_{\ell-1})\left(\by_n^{\alpha} - \bar{\by}_n^{\alpha}\right)\right]    + \ord(\Dt^3). 
    \end{aligned}
\end{equation}
Here, the constants in $\ord$ could depend on $\eta$ defined in \eqref{eq:gbd} (main text).
\end{proposition}
\begin{proof}
The starting point for deriving an asymptotic formula for the hidden state gradient $\frac{\partial \E^{(k)}_n}{\partial \theta}$ is to observe from the representation formula \eqref{eq:grad6}, the bound \eqref{eq:grad10} on matrices $\bF^{\ell,\ell-1}$ and the order notation \eqref{eq:ord} that,
\begin{equation}
    \label{eq:glb1}
    \frac{\partial \bX_\ell}{\partial \bX_{\ell-1}} = {\bf I}_{2d \times 2d} + \Dt \bE^{\ell,\ell-1} + \ord(\Dt^2),
\end{equation}
as long as $\eta$ is independent of $\Dt$.

By using induction and the bounds \eqref{eq:grad9},\eqref{eq:grad10}, it is straightforward to calculate the following representation formula for the product, 
\begin{equation}
    \label{eq:glb2}
    \frac{\partial \bX_n}{\partial \bX_k} = \prod\limits_{k < \ell \leq n} \frac{\partial \bX_\ell}{\partial \bX_{\ell-1}} = {\bf I}_{2d \times 2d} + \Dt \sum\limits_{\ell=k+1}^n \bE^{\ell,\ell-1} + \ord(\Dt^2).
\end{equation}
Recall that we have set $\theta = (\bW_y)_{\alpha,\beta}$. Hence, by the expressions \eqref{eq:grd4} and \eqref{eq:grd2}, a direct but tedious calculation leads to,
\begin{align}
\label{eq:glb3}
    \frac{\partial \E_n}{\partial \bX_n} {\bf I}_{2d \times 2d}\frac{\partial^{+} \bX_k}{\partial \theta} &= \Dt \hat{\sigma}(\bB^\alpha_{k-1})\sigma^{\prime}(\bD^\alpha_{k})\bz^\beta_{k}\left(\by^{\alpha}_n - \bar{\by}^\alpha_n\right), \\
    \sum\limits_{\ell=k+1}^n \frac{\partial \E_n}{\partial \bX_n} \bE^{\ell,\ell-1} \frac{\partial^{+} \bX_k}{\partial \theta}&=\\
    \Dt \hat{\sigma}(\bB^\alpha_{k-1})\sigma^{\prime}(\bD^\alpha_{k})\bz^\beta_{k}
    &\left[\sum\limits_{j=1}^d \left(\by_n^{j} - \bar{\by}_n^j\right)\sum\limits_{\ell=k+1}^n\hat{\sigma}^{\prime}(\bB^j_{\ell-1})\left(\sigma(\bD^j_{\ell}) - \by^j_{\ell-1}\right)(\bW_2)_{j,2\alpha} - \sum\limits_{\ell=k+1}^n \hat{\sigma}(\bB^{\alpha}_{\ell-1})\left(\by_n^{\alpha} - \bar{\by}_n^{\alpha}\right)\right] .
\end{align}
Therefore, by substituting the above expression into the representation formula \eqref{eq:glb2} yields the desired formula \eqref{eq:glb}.

In order to prove the formula \eqref{eq:glbo} (see Proposition \ref{prop:3cor} of main text), we focus our interest on long-term dependencies i.e., $k << n $. Then, a closer perusal of the expression in \eqref{eq:glb3}, together with the pointwise bounds \eqref{eq:hsb} which implies that $\by_{k-1} \approx \ord(\sqrt{\Dt})$, results in the following,
\begin{equation}
\label{eq:glb4}
\frac{\partial \E_n}{\partial \bX_n} {\bf I}_{2d \times 2d}\frac{\partial^{+} \bX_k}{\partial \theta} = \ord\left(\Dt^\frac{3}{2}\right).
\end{equation}
Similarly, we also obtain,
\begin{equation}
\label{eq:glb5}
\Dt\sum\limits_{\ell=k+1}^n \frac{\partial \E_n}{\partial \bX_n} \bE^{\ell,\ell-1} \frac{\partial^{+} \bX_k}{\partial \theta} = \ord\left(\Dt^\frac{3}{2}\right).
\end{equation}
Combining \eqref{eq:glb4} and \eqref{eq:glb5} results in the desired asymptotic bound \eqref{eq:glbo}.
\end{proof}
\begin{remark}
\label{rem:dtval}
The upper bound on the gradient \eqref{eq:gbd} and the gradient asymptotic formula \eqref{eq:glbo} impact the choice of the timestep hyperparameter $\Dt$. For sequence length $n$, if we choose $\Dt \sim n^{-s}$, with $s \geq 0$, we see from Remark \ref{rem:gbd} that the upper bound on the total gradient scales like $\ord(n^{2(1-s)})$. On the other hand, from \eqref{eq:glbo}, the gradient contribution from long-term dependencies will scale like $\ord(n^{\frac{-3s}{2}})$. Hence, a small value of $s \approx 0$, will ensure that the gradient with respect to long-term dependencies will be $\ord(1)$. However, the total gradient will behave like $\ord(n^2)$ and possibly blow up fast. Similarly, setting $s\approx 1$ leads to a bounded gradient, while the contributions from long-term dependencies decay as fast as $n^{\frac{-3}{2}}$. Hence, one has to find a value of $s$ that balances both these requirements. Equilibrating them leads to $s = \frac{4}{7}$, ensuring that the total gradient grows sub-linearly while long-term dependencies still contribute with a sub-linear decay. This value is very close to the empirically observed value of $s = \frac{1}{2}$ which also ensures that the total gradient grows linearly and the contribution of long-term dependencies decays sub-linearly in the sequence length $n$.
\end{remark}
\subsection{Proof of Proposition \ref{prop:exp1}}
\label{app:exp1pf}
\begin{proof}
To prove this proposition, we have to construct hidden states $\by_n,\bz_n$, output state $\bom_n$, weight matrices $\bW_{1,2,y,z},\cW_{y}, \bV_{1,2,y,z}$ and bias vectors $\bb_{1,2,y,z}$ such that LEM \eqref{eq:lem} with output state $\bom_n = \cW_y \by_n$ approximates the dynamical system \eqref{eq:ods}. 



Let $R^{\ast} > R >> 1$ and $\epsilon^{\ast} < \epsilon$, be parameters to be defined later. By the theorem for universal approximation of continuous functions with neural networks with the tanh activation function $\sigma = \tanh$ \citep{BAR1}, given $\epsilon^{\ast}$, there exist weight matrices $W_1 \in \R^{d_1 \times d_h}, V_1 \in \R^{d_1\times d_u}, W_2 \in \R^{d_h \times d_1}$ and bias vector $b_1 \in \R^{d_1}$ such that the tanh neural network defined by,
\begin{equation}
\label{eq:n1}
\cN_1(h,u) = W_2\sigma\left(W_1 h + V_1 u + b_1 \right),
\end{equation}
approximates the underlying function $\bif$ in the following manner, 
\begin{equation}
    \label{eq:exp11}
    \max\limits_{\max(\|h\|,\|u\|)< R^{\ast}} \|\bif(h,u) - \cN_1(h,u) \| \leq \epsilon^{\ast}.
\end{equation}
Similarly, one can readily approximate the identity function $\bg(h,u) = h$ with a tanh neural network of the form,
\begin{equation}
\label{eq:n2}
\bar{\cN}_2(h) = \bar{W}_2\sigma\left(\bar{W}_1 h \right),
\end{equation}
 such that
\begin{equation}
    \label{eq:exp12}
    \max\limits_{\|h\|,\|u\|< R^{\ast}} \|\bg(h) - \cN_2(h) \| \leq \epsilon^{\ast}.
\end{equation}

Next, we define the following dynamical system,
\begin{equation}
    \label{eq:exp14}
    \begin{aligned}
\bar{\bz}_n &= W_2\sigma\left(W_1 \bar{\by}_{n-1} + V_1 \bu_n + b_1 \right), \\
\bar{\by}_n &= \bar{W}_2\sigma\left(\bar{W}_1 \bar{\bz}_n \right),
    \end{aligned}
\end{equation}
with initial states $\bar{\bz}_0=\bar{\by}_0 = 0$.

Using the approximation bound \eqref{eq:exp11}, we derive the following bound, 
\begin{align*}
    \|\bh_n - \bar{\by}_n\| &= \left\|\bif\left(\bh_{n-1},\bu_n\right)- \bar{\bz}_n + \bar{\bz}_n - \bar{\by}_n \right\| \\
    &\leq \|\bif\left(\bh_{n-1},\bu_n\right)- W_2\sigma\left(W_1 \bar{\by}_{n-1} + V_1 \bu_n + b_1 \right)\| + \|\bg(\bar{\bz}_n) - \bar{W}_2\sigma\left(\bar{W}_1 \bar{\bz}_n \right)\| \\
    &\leq \|\bif\left(\bh_{n-1},\bu_n\right) -\bif\left(\bar{\by}_{n-1},\bu_n\right)\| + \|\bif\left(\bar{\by}_{n-1},\bu_n\right) -  W_2\sigma\left(W_1 \bar{\by}_{n-1} + V_1 \bu_n + b_1 \right)\| \\
    &+ \|\bg(\bar{\bz}_n) - \bar{W}_2\sigma\left(\bar{W}_1 \bar{\bz}_n \right)\| \\ 
    &\leq {\rm Lip}(\bif) \|\bh_{n-1} - \bar{\by}_{n-1}\| + 2 \epsilon^{\ast} \quad ({\rm from}~ \eqref{eq:exp11},\eqref{eq:exp12}). 
\end{align*}
Here, ${\rm Lip}(\bif)$ is the Lipschitz constant of the function $\bif$ on the compact set $\{(h,u) \in \R^{d_h\times d_u}: ~\|h\|,\|u\| < \R^{\ast}\}$. Note that one can readily prove using the fact that $\bar{\by}_0 = \bar{\bz}_0 = 0$, bounds \eqref{eq:exp11}, \eqref{eq:exp12} and the assumption $\|\bh_n\|,\|\bu_n\| < R$, that $\|\bar{\bz}_n\|,\|\bar{\by}_n\| < R^{\ast} = 2R$. 


Iterating the above inequality over $n$ leads to the bound,
\begin{equation}
    \label{eq:exp15}
    \|\bh_n - \bar{\by}_n\|  \leq 2 \epsilon^{\ast} \sum_{\lambda=0}^{n-1}{\rm Lip}(\bif)^\lambda.
\end{equation}

Hence, using the Lipschitz continuity of the output function $\bo$ in \eqref{eq:ods}, one obtains,
\begin{equation}
    \label{eq:exp18}
     \|\bo_n - \bo(\bar{\by}_n)\|  \leq 2 \epsilon^{\ast} {\rm Lip}(\bo)\sum_{\lambda=0}^{n-1}{\rm Lip}(\bif)^\lambda,
\end{equation}
with ${\rm Lip}(\bo)$ being the Lipschitz constant of the function $\bo$ on the compact set $\{h \in \R^{d_h}: ~\|h\| < R^{\ast}\}$.

Next, we can use the universal approximation theorem for neural networks again to conclude that given a tolerance $\bar{\epsilon}$, there exist weight matrices $W_3 \in \R^{d_2 \times d_h}, W_4 \in \R^{d_h \times d_2}$ and bias vector $b_2 \in \R^{d_2}$ such that the tanh neural network defined by,
\begin{equation}
\label{eq:n3}
\cN_3(h) = W_4\sigma\left(W_3 h + b_2 \right),
\end{equation}
approximates the underlying output function $\bo$ in the following manner, 
\begin{equation}
    \label{eq:exp19}
    \max\limits_{\|h\|< R^{\ast}} \|\bo(h) - \cN_3(h) \| \leq \bar{\epsilon}.
\end{equation}
Now defining, 
\begin{equation}
    \label{eq:exp110}
    \bar{\bom}_n = W_4\sigma\left(W_3 \bar{\by}_n + b_2 \right),
\end{equation}
we obtain from \eqref{eq:exp19} and \eqref{eq:exp18} that, 
\begin{equation}
    \label{eq:exp111}
     \|\bo_n - \bar{\bom}_n\| \leq \bar{\epsilon} + 2 \epsilon^{\ast} {\rm Lip}(\bo)\sum_{\lambda=0}^{n-1}{\rm Lip}(\bif)^\lambda.
\end{equation}

Next, we introduce the notation, 
\begin{equation}
    \label{eq:exp16}
    \tilde{\bz}_n = \sigma\left(W_1 \bar{\by}_{n-1} + V_1 \bu_n + b_1 \right), \quad \tilde{\by}_n =  \sigma\left(\bar{W}_1 \bar{\bz}_n \right).
\end{equation}





From \eqref{eq:exp14}, we see that
\begin{equation}
    \label{eq:exp17}
    \bar{\bz}_n = W_2 \tilde{\bz}_n, \quad \bar{\by}_n = \bar{W}_2 \tilde{\by}_n
\end{equation}

Thus from \eqref{eq:exp17} and \eqref{eq:exp111}, we have
\begin{equation}
    \label{eq:exp114}
\begin{aligned}
\bar{\bom}_n &= W_4\sigma\left(W_3W_2\tilde{\by}_n + b_2\right),\\
&= W_4\sigma\left(W_3W_2\sigma\left(\bar{W}_1 W_2\tilde{\bz}_n \right)+ b_2\right). \\
\end{aligned}
\end{equation}
Define the function $\cR: \R^{d_h} \times \R^{d_u} \mapsto \R^{d_o}$ by,
\begin{equation}
    \label{eq:exp112}
    \cR(z) = W_4\sigma\left(W_3W_2\sigma\left(\bar{W}_1 W_2z \right)+ b_2\right).
\end{equation}
The function, defined above, is clearly Lipschitz continuous. We can apply the universal approximation theorem for tanh neural networks to find, for any given tolerance $\tilde{\epsilon}$, weight matrices $W_5 \in \R^{d_3 \times d_4}, W_6 \in \R^{d_o \times d_3}, V_2 \in \R^{d_3 \times d_u}$ and bias vector $b_3 \in \R^{d_3}$ such that the following holds,
\begin{equation}
    \label{eq:exp113}
    \max\limits_{\max(\|z\|)< R^{\ast}} \|\cR(z) - W_6\sigma(W_5z + b_3) \| \leq \tilde{\epsilon} .
\end{equation}
Denote $\tilde{\bom}_n = W_6\sigma(W_5\tilde{\bz}_{n} + b_3)$, then from \eqref{eq:exp113} and \eqref{eq:exp114}, we obtain that
\begin{equation*}
\|\bar{\omega}_n - \tilde{\bom}_n\| \leq \tilde{\epsilon}.
\end{equation*}
Combining this estimate with \eqref{eq:exp111} yields,
\begin{equation}
    \label{eq:exp115}
     \|\bo_n - \tilde{\bom}_n\| \leq \tilde{\epsilon} + \bar{\epsilon} + 2 \epsilon^{\ast} {\rm Lip}(\bo)\sum_{\lambda=0}^{n-1}{\rm Lip}(\bif)^\lambda.
\end{equation}

Now, we collect all ingredients to define the LEM that can approximate the dynamical system \eqref{eq:ods}. To this end, we define hidden states $\bz_n,\by_n \in \R^{2d_h}$ as 
\begin{align*}
    \bz_n = \left[\tilde{\bz}_n,\hat{\bz}_n\right], \quad  \by_n = \left[\tilde{\by}_n,\hat{\by}_n\right],
\end{align*}
with $\tilde{\bz}_n,\hat{\bz}_n,\tilde{\by}_n,\hat{\by}_n \in \R^{d_h}$. These hidden states are evolved by the dynamical system,
\begin{equation}
    \label{eq:exp116}
    \begin{aligned}
    \bz_n^{\perp} &= \sigma \left( \begin{bmatrix}
    W_1\bar{W}_2 & 0 \\
    0 & 0
    \end{bmatrix} 
    \by_{n-1}^{\perp} + \left[V_1\bu_n,  0\right]^{\perp}  + \left[b_1, 0 \right]^{\perp} \right), \\
 \by_n^{\perp} &= \sigma \left( \begin{bmatrix}
    \bar{W}_1 W_2 & 0 \\
    W_5 & 0  
    \end{bmatrix} 
    \bz_{n}^{\perp} + [0,0]^{\perp} + [0,b_3]^{\perp}\right)
    \end{aligned}
\end{equation}
and the output state is calculated by, 
\begin{equation}
    \label{eq:exp117}
    \bom_n^{\perp} = [0, W_6]\by_n^{\perp} .
\end{equation}

Finally, we can recast the dynamical system \eqref{eq:exp116}, \eqref{eq:exp117} as a LEM of the form \eqref{eq:lem} for the hidden states $\by_n,\bz_n$, defined in \eqref{eq:exp116}, with the following parameters,
Now, define the hidden states $\bar{\by}_n,\bar{\bz}_n \in \R^{d_y}$ for all $1 \leq n \leq N$ by the LEM \eqref{eq:lem} with the following parameters, 
\begin{equation}
    \label{eq:exp13}
    \begin{aligned}
    \Dt &=1, \quad d_y = 2d_h, \\
    \bW_1 &= \bW_2=\bV_1=\bV_2 = 0\\
    \bb_1 &= \bb_2 = \bb_{\infty}, \\
    \bW_z &= \begin{bmatrix}
    W_1\bar{W}_2 & 0 \\
    0 & 0
    \end{bmatrix} , \quad \bV_z = [V_1,0], \quad \bb_z = [b_1,0] \\
     \bW_y &= \begin{bmatrix}
    \bar{W}_1 W_2 & 0 \\
    W_5 & 0
    \end{bmatrix} , \quad \bV_y = 0, \quad \bb_z = [0,b_3]. \\
    \cW_y &= [0, W_6]. 
    \end{aligned}
\end{equation}
Here, $\bb_{\infty}\in \R^{d_h}$ is defined as 
$$
\bb_{\infty} = [b_{\infty},b_{\infty},\ldots,\ldots,b_{\infty}],
$$
with $1 << b_{\infty}$ is such that 
\begin{equation}
    \label{eq:exp155}
    |1 - \hat{\sigma}(b_{\infty})| \leq \delta .
\end{equation}
The nature of the sigmoid function guarantees the existence of such a $b_{\infty}$ for any $\delta$. As $\delta$ decays exponentially fast, we set it to $0$ in the following for notational simplicity. 

It is straightforward to verify that the output state of the LEM \eqref{eq:lem} with parameters given in \eqref{eq:exp13} is $\bom_n = \tilde{\bom}_n$. 

Therefore, from \eqref{eq:exp115} and by setting $\bar{\epsilon} < \frac{\epsilon}{3}$, $\tilde{\epsilon} < \frac{\epsilon}{3}$ and
$$
\epsilon^{\ast} < \frac{\epsilon}{6{\rm Lip}(\bo)\sum\limits_{\lambda=0}^{N-1}{\rm Lip}(\bif)^\lambda},
$$
we prove the desired bound \eqref{eq:exp1}. 




\end{proof}
\subsection{Proof of Proposition \ref{prop:exp2}}
\label{app:exp2pf}
\begin{proof}
The proof of this proposition is based heavily on the proof of Proposition \ref{prop:exp1}. Hence, we will highlight the main points of difference. 

As the steps for approximation of a general Lipschitz continuous output map are identical to the corresponding steps in the proof of proposition \ref{prop:exp1} (see the steps from Eqns. \eqref{eq:exp18} to \eqref{eq:exp115}), we will only consider the following linear output map for convenience herein, 
\begin{equation}
    \label{eq:exp02}
    \bo(\bc_n) =  \cW_{c}\bc_n.
\end{equation}
Let $R^{\ast} > R >> 1$ and $\epsilon^{\ast} < \epsilon$, be parameters to be defined later. By the theorem for universal approximation of continuous functions with neural networks with the tanh activation function $\sigma = \tanh$, given $\epsilon^{\ast}$, there exist weight matrices $W^f_1,W^f_2 \in \R^{d_1 \times d_h}, V^f_1 \in \R^{d_1\times d_u}, W^f_3 \in \R^{d_h \times d_1}$ and bias vector $b^f_1 \in \R^{d_1}$ such that the tanh neural network defined by,
\begin{equation}
\label{eq:nf}
\cN_f(h,c,u) = W^f_3\sigma\left(W^f_1 h + W^f_2c+ V^f_1 u + b^f_1 \right),
\end{equation}
approximates the underlying function $\bif$ in the following manner, 
\begin{equation}
    \label{eq:exp21}
    \max\limits_{\max(\|h\|,\|c\|,\|u\|)< R^{\ast}} \|\bif(h,c,u) - \cN_f(h,c,u) \| \leq \epsilon^{\ast}.
\end{equation}
Next, we define the following map, 
\begin{equation}
\label{eq:exp22}
\bG(h,c,u) = \bg(h,c,u) + \left(1-\frac{1}{\tau}\right)c,
\end{equation}
for any $\tau > 0$.

By the universal approximation theorem, given 
$\epsilon^{\ast}$, there exist weight matrices $W^g_1,W^g_2 \in \R^{d_2 \times d_h}, V^g_1 \in \R^{d_2\times d_u}, W^g_3 \in \R^{d_h \times d_2}$ and bias vector $b^g_1 \in \R^{d_2}$ such that the tanh neural network defined by,
\begin{equation}
\label{eq:ng}
\cN_g(h,c,u) = W^g_3\sigma\left(W^g_1 h + W^g_2c+ V^g_1 u + b^g_1 \right),
\end{equation}
approximates the function $\bG$ \eqref{eq:exp22} in the following manner, 
\begin{equation}
    \label{eq:exp23}
    \max\limits_{\max(\|h\|,\|c\|,\|u\|)< R^{\ast}} \|\bG(h,c,u) - \cN_f(h,c,u) \| \leq \epsilon^{\ast}.
\end{equation}
Note that the sizes of the neural network $\cN_g$ can be made independent of the small parameter $\tau$ by simply taking the sum of the neural networks approximating the functions $g$ and $\hat{g}(h,c,u) = c$ with tanh neural networks. As neither of these functions depend on the small parameter $\tau$, the sizes of the corresponding neural networks are independent of the small parameter too. 

Next, as in the proof of proposition \ref{prop:exp1}, one can readily approximate the identity function $\hat{f}(h,c,u) = h$ with a tanh neural network of the form,
\begin{equation}
\label{eq:nfh}
\bar{\cN}_f(h) = \bar{W}_2\sigma\left(\bar{W}_1 h \right),
\end{equation}
 such that
\begin{equation}
    \label{eq:exp24}
    \max\limits_{\|h\|,\|c\|,\|u\|< R^{\ast}} \|\hat{f}(h,c,u) - \cN_f(h) \| \leq \epsilon^{\ast},
\end{equation}
and with the same weights and biases, one can approximate the identity function $\hat{g}(h,c,u) = c$ with the tanh neural network,
\begin{equation}
\label{eq:ngh}
\bar{\cN}_g(c) = \bar{W}_2\sigma\left(\bar{W}_1 c \right),
\end{equation}
 such that
\begin{equation}
    \label{eq:exp26}
    \max\limits_{\|h\|,\|c\|,\|u\|< R^{\ast}} \|\hat{g}(h,c,u) - \cN_g(c) \| \leq \epsilon^{\ast}.
\end{equation}

Next, we define the following dynamical system,
\begin{equation}
    \label{eq:exp25}
    \begin{aligned}
\hat{\bz}_n &= W^f_3\sigma\left(W^f_1 \tilde{\by}_{n-1} + W^f_2 \hat{\by}_{n-1} + V^f_1 \bu_n + b^f_1 \right), \\
\tilde{\bz}_n &= \bar{W}_2\sigma\left(\bar{W}_1 \hat{\by}_{n-1} \right), \\
\hat{\by}_n &= (1-\tau)\hat{\by}_{n-1} +\tau W^g_3\sigma\left(W^g_1 \hat{\bz}_{n} + W^g_2 \tilde{\bz}_{n} + V^g_1 \bu_n + b^g_1 \right), \\
\tilde{\by}_n &= \bar{W}_2\sigma\left(\bar{W}_1 \hat{\bz}_{n} \right),
    \end{aligned}
\end{equation}
with hidden states $\hat{\bz}_n,\tilde{\bz}_n, \hat{\by}_n,\tilde{\by}_n \in \R^{d_h}$ and with initial states $\hat{\bz}_0 = \tilde{\bz}_0=\hat{\by}_0 = \tilde{\by}_0 = 0$.

We derive the following bounds,
\begin{align*}
    \|\bh_n - \hat{\bz}_n\| &= \|\bif(\bh_{n-1},\bc_{n-1},\bu_n) - W^f_3\sigma\left(W^f_1 \tilde{\by}_{n-1} + W^f_2 \hat{\by}_{n-1} + V^f_1 \bu_n + b^f_1 \right)\| \\
    &\leq \|\bif(\bh_{n-1},\bc_{n-1},\bu_n)-\bif(\tilde{\by}_{n-1},\hat{\bz}_{n-1},\bu_n)\|, \\
    &+ \|\bif(\tilde{\by}_{n-1},\hat{\bz}_{n-1},\bu_n)- W^f_3\sigma\left(W^f_1 \tilde{\by}_{n-1} + W^f_2 \hat{\by}_{n-1} + V^f_1 \bu_n + b^f_1 \right)\|\\
    &\leq {\rm Lip}(\bif)\left(\|\bh_{n-1}-\hat{\bz}_{n-1}\|+2\|\tilde{\by}_{n-1}-\hat{\bz}_{n-1}\|+\|\bc_{n-1}-\tilde{\by}_{n-1}\|\right) + \epsilon^{\ast}\quad ({\rm by}~\eqref{eq:exp23}) \\
    &\leq {\rm Lip}(\bif)\left(\|\bh_{n-1}-\hat{\bz}_{n-1}\|+\|\bc_{n-1}-\tilde{\by}_{n-1}\|\right) +\left(1 + 2{\rm Lip}(\bif)\right)\epsilon^{\ast} \quad ({\rm by}~\eqref{eq:exp24},\eqref{eq:exp25}),
\end{align*}
and
\begin{align*}
    \|\bc_n - \hat{\by}_n\| &=  \|(1-\tau)(\bc_{n-1}-\hat{\by}_{n-1}) + \tau \left(\bG(\bh_{n},\bc_{n-1},\bu_n) - W^g_3\sigma\left(W^g_2 \tilde{\bz}_{n} + W^g_1 \hat{\bz}_{n} + V^g_1 \bu_n + b^g_1 \right)\right)\| \\
    &\leq \|\bc_{n-1}-\hat{\by}_{n-1}\| + \tau\|\bG(\bh_{n},\bc_{n-1},\bu_n)-\bG(\hat{\bz}_{n},\tilde{\bz}_n,\bu_n)\| 
    \\&\hspace{0.325cm}+\tau \|\bG(\hat{\bz}_{n},\tilde{\bz}_{n},\bu_n)-W^g_3\sigma\left(W^g_2 \tilde{\bz}_{n} + W^g_1 \hat{\bz}_{n} + V^g_1 \bu_n + b^g_1 \right)\|\\
    &\leq \|\bc_{n-1}-\hat{\by}_{n-1})\| + \tau{\rm Lip}(\bG)\left(\|\bh_n - \hat{\bz}_n\| + \|\tilde{\bz}_n - \hat{\by}_{n-1}\| + \|\bc_{n-1} - \hat{\by}_{n-1}\|\right) + \tau\epsilon^{\ast}, \\ 
    &\leq (1 + \tau{\rm Lip}(\bG))(1+{\rm Lip}(\bif))\|\bc_{n-1} - \hat{\by}_{n-1}\| + \tau{\rm Lip}(\bG){\rm Lip}(\bif)\|\bh_{n-1}-\hat{\bz}_{n-1}\|\\&\hspace{0.325cm}+ \tau(1 +{\rm Lip}(\bG)(2+2{\rm Lip}(\bif)))\epsilon^{\ast},
    \end{align*}
where the last inequality follows by using the previous inequality together with \eqref{eq:exp25} and \eqref{eq:exp26}. 

As $\tau < 1$, it is easy to see from \eqref{eq:exp22} that ${\rm Lip}(\bG) < {\rm Lip}(\bg)+\frac{2}{\tau}$. Therefore, the last inequality reduces to,
\begin{align*}
     \|\bc_n - \hat{\by}_n\| &\leq (3+\tau{\rm Lip}(\bg))(1+{\rm Lip}(\bif))\|\bc_{n-1} - \hat{\by}_{n-1}\| + (2+\tau{\rm Lip}(\bg)){\rm Lip}(\bif)\|\bh_{n-1}-\hat{\bz}_{n-1}\|\\&\hspace{0.325cm}+ \left(\tau + (2+\tau{\rm Lip}(\bg))(2+2{\rm Lip}(\bif)\right))\epsilon^{\ast}.
\end{align*}
Adding we obtain,
\begin{equation}
    \label{eq:exp27}
    \begin{aligned}
   \|\bh_n - \hat{\bz}_n\| + \|\bc_{n} - \hat{\by}_{n}\| &\leq C^{\ast}\left(\|\bh_{n-1} - \hat{\bz}_{n-1}\| + \|\bc_{n-1} - \hat{\by}_{n-1}\|\right) + D^{\ast} \epsilon^{\ast},  
   \end{aligned}
\end{equation}
where,
\begin{equation}
    \label{eq:exp28}
    \begin{aligned}
    C^{\ast} &= \max\limits\{(3+{\rm Lip}(\bg)){\rm Lip}(\bif), {\rm Lip}(\bif)(3+{\rm Lip}(\bg))(1+{\rm Lip}(\bif))\}, \\
    D^{\ast} &= 1 + (2 +{\rm Lip}(\bg))(2+2{\rm Lip}(\bif)).
    \end{aligned}
\end{equation}

Iterating over $n$ leads to the bound,
\begin{equation}
    \label{eq:exp29}
     \|\bh_n - \hat{\bz}_n\| + \|\bc_{n} - \hat{\by}_{n}\| \leq \epsilon^{\ast}D^{\ast}\sum_{\lambda=0}^{n-1}(C^{\ast})^{\lambda}.
\end{equation}
Here, ${\rm Lip}(\bif),{\rm Lip}(\bg)$ are the Lipschitz constants of the functions $\bif,\bg$ on the compact set $\{(h,c,u) \in \R^{d_h\times d_h\times d_u}: ~\|h\|,\|c\|,\|u\| < \R^{\ast}\}$. Note that one can readily prove using the zero values of initial states, the bounds \eqref{eq:exp24}, \eqref{eq:exp26} and the assumption $\|\bh_n\|,\|\bc_n\|,\|\bu_n\| < R$, that $\|\hat{\bz}_n\|,\|\tilde{\bz}_n\|,\|\hat{\by}_n\|,\|\tilde{\by}_n\| < R^{\ast} = 2R$. 

Using the definition of the output function \eqref{eq:exp2} and the bound \eqref{eq:exp29} that, 
\begin{equation}
    \label{eq:exp210}
    \|\bo_n -\bo(\hat{\by}_n)\| \leq \|\cW_c\| \epsilon^{\ast}D^{\ast}\sum_{\lambda=0}^{n-1}(C^{\ast})^{\lambda}.
\end{equation}
Defining the dynamical system,
\begin{equation}
    \label{eq:exp211}
    \begin{aligned}
    \bz^{\ast}_n &= \sigma\left(W^f_1 \bar{W}_2 \bar{\by}_{n-1} + W^f_2 W^g_3 \by^{\ast}_{n-1} + V^f_1 \bu_n + b^f_1 \right) \\
    \bar{\bz}_n &= \sigma\left(\bar{W}_1 W^g_3\by^{\ast}_{n-1} \right) \\
    \by^{\ast}_n &= (1-\tau)\by^{\ast}_{n-1} +\tau \sigma\left(W^g_1 W^f_3 \bz^{\ast}_{n} + W^g_2 \bar{W}_2 \bar{\bz}_{n} + V^g_1 \bu_n + b^g_1 \right), \\
    \bar{\by}_n &= \sigma\left(\bar{W}_1 W^f_3 \bz^{\ast}_{n} \right) .
    \end{aligned}
\end{equation}
By multiplying suitable matrices to \eqref{eq:exp25}, we obtain that,
\begin{equation}
    \label{eq:exp212}
    \hat{\bz}_n = W^f_3\bz_n^{\ast}, \quad 
    \tilde{\bz}_n = \bar{W}_2 \bar{\bz}_n, \quad 
    \hat{\by}_n = W^g_3\by_n^{\ast}, \quad 
    \tilde{\by}_n = \bar{W}_2 \bar{\by}_n.
\end{equation}
Finally, in addition to $b_{\infty}$ defined in \eqref{eq:exp15}, for any given $\tau \in (0,1]$, we introduce $b_{\tau} \in \R$ defined by 
\begin{equation}
    \label{eq:exp213}
    \hat{\sigma}(b_{\tau}) = \tau.
\end{equation}
The existence of a unique $b_{\tau}$ follows from the fact that the sigmoid function $\hat{\sigma}$ is monotone. Next, we define the two vectors $\bb_{\infty},\bb_{\tau} \in \R^{2d_h}$ as
\begin{equation}
\label{eq:exp215}
\begin{aligned}
\bb_{\infty}^i &= b_{\infty}, \quad \forall~1 \leq i \leq 2d_h, \\
\bb_{\tau}^i &= b_{\tau}, \quad \forall~ 1 \leq i \leq d_h, \\
\bb_{\tau}^i &= b_{\infty}, \quad \forall~ d_h+1 \leq i \leq 2d_h . \\
\end{aligned}
\end{equation}

We are now in a position to define the LEM of form \eqref{eq:lem}, which will approximate the two-scale dynamical system \eqref{eq:2sds}. To this end, we define the hidden states $\bz_n,\by_n \in \R^{2d_h}$ such that $\bz_n = [\bz_n^{\ast},\bar{\bz}_n]$ and $\by_n = [\by_n^{\ast},\bar{\by}_n]$. The parameters for the corresponding LEM of form \eqref{eq:lem} given by,
\begin{equation}
    \label{eq:exp214}
    \begin{aligned}
    \Dt &= 1, d_y = 2d_h\\
    \bW_1 &= \bW_2 = \bV_1 = \bV_2 \equiv 0, \\
    \bb_1 &= \bb_{\infty}, \quad \bb_2 = \bb_{\tau}, \\
    \bW_z &=\begin{bmatrix}
    W^f_2W^g_3 & W^f_1\bar{W}_2 \\
    \bar{W}_1W^g_3 & 0 
    \end{bmatrix}, \quad \bV_z = [V^f_1 0], \quad \bb_z = [b^f_1,0], \\
    \bW_y &=\begin{bmatrix}
    W^g_1W^f_3 & W^g_2\bar{W}_2 \\
    \bar{W}_1W^f_3 & 0 
    \end{bmatrix}, \quad \bV_z = [V^g_1 0], \quad \bb_z = [b^g_1,0], \\
    \end{aligned}
\end{equation}
and with following parameters defining the output states,
\begin{equation}
    \label{eq:exp216}
     \cW_y = \left[\cW_c W^g_3 ~0 \right], 
\end{equation}
yields an output state $\omega_n = \cW_y \by_n$.  

It is straightforward to observe that $\omega_n \equiv \bo(\hat{\by}_n)$. Hence, the desired bound \eqref{eq:exp2} follows from \eqref{eq:exp29} by choosing,
$$
\epsilon^{\ast} = \frac{\epsilon}{D^{\ast}\sum\limits_{\lambda=0}^{N-1}(C^{\ast})^{\lambda}}.
$$

\end{proof}

The proof of proposition \ref{prop:exp2} can be readily extended to prove the following proposition about a general $r$-scale dynamical system of the form, 
\begin{equation}
    \label{eq:msds}
    \begin{aligned}
    \bh^1_{n} &= \tau_1 \bif^1(\bh^1_{n-1},\bh^2_{n-1},\ldots,\bh^r_{n-1}\bu_n), \\
    \bh^2_{n} &= \tau_2 \bif^2(\bh^1_{n-1},\bh^2_{n-1},\ldots,\bh^r_{n-1}\bu_n), \\
    \ldots &\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots\\
    \ldots &\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
    \bh^r_{n} &= \tau_r \bif^r(\bh^1_{n-1},\bh^2_{n-1},\ldots,\bh^r_{n-1}\bu_n), \\
  \bo_n &= \bo(\bh^1_n,\bh^2_n,\ldots,\bh^r_n).
  \end{aligned}
\end{equation}
Here, $\tau_1 \leq \tau_2 \ldots \ldots \leq \tau_r \leq 1$, with $r > 1$, are the $r$-time scales of the dynamical system \eqref{eq:msds}. We assume that the underlying maps $\bif^{1,2,\ldots,r}$ are Lipschitz continuous. We can prove the following proposition, 
\begin{proposition}
\label{prop:mts}
For all $1\leq n \leq N$, let $\bh^{1,2,\ldots,r}_n,\bo_n$ be given by the $r$-scale dynamical system \eqref{eq:msds} with input signal $\bu_n$. Under the assumption that there exists a $R > 0$ such that $\max\{\|\bh^1_n\|, |\bh^2_n\|,\ldots, |\bh^r_n\|,\|\bu_n\|\} < R$, for all $1 \leq n \leq N$, then for any given $\epsilon > 0$, there exists a LEM of the form \eqref{eq:lem}, with hidden states $\by_n,\bz_n$ and output state $\bom_n$ with $\bom_n = \cW \by_n$ such that 
the following holds,
\begin{equation}
    \label{eq:expms}
    \|\bo_n - \bom_n\| \leq \epsilon, \quad \forall 1 \leq n \leq N.
\end{equation}
Moreover, the weights, biases and size (number of neurons) of the underlying LEM \eqref{eq:lem} are \emph{independent} of the time-scales $\tau_{1,2,\ldots,r}$.
\end{proposition}

\section{LEMs emulate Heterogeneous multiscale methods for ODEs}
\label{app:hmm}
Following \cite{Kuhn_book}, we consider the following prototypical example of a fast-slow system of ordinary differential equations, 
\begin{equation}
    \label{eq:kode}
    \begin{aligned}
    \bh^{\prime}(t) &= \frac{1}{\tau}\left(f(\bc) - \bh\right), \\
    \bc^{\prime}(t) &= g(\bh,\bc).
    \end{aligned}
\end{equation}
Here $\bh,\bc \in \R^m$ are the fast and slow variables respectively and $0 < \tau << 1$ is a small parameter. Note that we have rescaled time and are interested in the dynamics of the slow variable $\bc(t)$ in the time interval $[0,T]$. 

A naive time-stepping numerical scheme for \eqref{eq:kode} requires a time step size $\delta t \sim \ord(\tau)$. Thus, the computation will entail time updates $N \sim \ord(1/\tau)$. Hence, one needs a multiscale ODE solver to approximate the solutions of the system \eqref{eq:kode}. One such popular ODE solver can be derived by using the Heterogenous multiscale method (HMM); see \citet{Kuhn_book} and references therein. This in turns, requires using two time stepping schemes, a \emph{macro} solver for the slow variable, with a time step $\Dt$ of the form,
\begin{equation}
    \label{eq:hmm1}
    \bc_n = \bc_{n-1} + \tilde{\Dt} g(\bh_n,\bc_{n-1}).
\end{equation}
Here, the time step $\tilde{\Dt} < 1$ is independent of the small parameter $\tau$. 

Moreover, the fast variable is updated using a \emph{micro} solver of the form,
\begin{equation}
    \label{eq:hmm2}
    \begin{aligned}
    \bh^{(k)}_{n-1} &= \bh^{(k-1)}_{n-1} - \delta t (f(\bc_{n-1}) - \bh^{(k-1)}_{n-1}), \quad 1 \leq k \leq K.\\
    \bh_n &= \bh^K_{n-1}, \\
    \bh^{(0)}_{n-1} &= \bh_{n-1}.
    \end{aligned}
\end{equation}
Note that the micro time step size $\delta t$ and the number of micro time steps $K$ are assumed to independent of the small parameter $\tau$.

It is shown in \cite{Kuhn_book} (Chapter 10.8) that for any given small tolerance $\epsilon > 0$, one can choose a macro time step $\tilde{\Dt}$, a micro time step $\delta t$, the number $K$ of micro time steps, the number $N$ of macro time steps, independent of $\tau$, such that the discrete states $\bc_n$ approximate the slow-variable $\bc(t_n)$ (with $t_n = n \tilde{\Dt}$) of the fast-slow system \eqref{eq:kode} to the desired accuracy of $\epsilon$. 

Our aim is to show that we can construct a LEM of the form \eqref{eq:lem} such that the states $\bh_n,\bc_n$, defined in \eqref{eq:hmm1}, \eqref{eq:hmm2} can be approximated to arbitrary accuracy. By combining this with the accuracy of HMM, we will prove that LEMs can approximate the solutions of the fast-slow system \eqref{eq:kode} to desired accuracy, independent of the small parameter $\tau$ in \eqref{eq:kode}. 
\begin{proposition}
\label{prop:hmm}
Let $\bh_n,\bc_n \in \R^{m}$, for $1 \leq n \leq N$, be the states defined by the HMM dynamical system \eqref{eq:hmm1}, \eqref{eq:hmm2}. For any given $\epsilon > 0$, there exists a LEM of the form \eqref{eq:lem} with hidden states $[\bz_n,\by_n]$, where $\bz_n,\by_n \in \R^{d_m}$ and output states $\bom^h_n,\bom^c_n$ such that the following holds, 
\begin{equation}
    \label{eq:phmm}
    \max\left\{\|\bh_n - \bom^h_n\|, \|\bc_n - \bom^c_n\|   \right\} \leq \epsilon, \quad \forall 1 \leq n \leq N. 
\end{equation}
\end{proposition}
\begin{proof}
We start by using iteration on the micro solver \eqref{eq:hmm2} from $k=1$ to $k= K$ to derive the following, 
\begin{equation}
    \label{eq:phmm1}
    \begin{aligned}
    \bh_n &= \overline{\delta t} \bh_{n-1} + (1 - \overline{\delta t})f(\bc_{n-1}), \\
    \overline{\delta t}&= \left(1-\delta t\right)^K.
    \end{aligned}
\end{equation}
As $\delta t < 1$, we have that $\overline{\delta t} < 1$. 

By the universal approximation theorem for tanh neural networks, for any given tolerance $\epsilon^{\ast}$, there exist weight matrices $W^f_1 \in \R^{d_1 \times m}, W^f_2 \in \R^{m \times d_1}$ and bias vector $b^f_1 \in \R^{d_1}$ such that the tanh neural network defined by,
\begin{equation}
\label{eq:hmmnf}
\cN_f(c) = W^f_2\sigma\left(W^f_1c+ b^f_1 \right),
\end{equation}
approximates the underlying function $\bif$ in the following manner, 
\begin{equation}
    \label{eq:phmm2}
    \max\limits_{\|c\|< R^{\ast}} \|\bif(c) - \cN_f(c) \| \leq \epsilon^{\ast}.
\end{equation}

Next, we define the following map, 
\begin{equation}
\label{eq:phmm3}
\bG(h,c) = \bg(h,c) + c,
\end{equation}

By the universal approximation theorem, given 
$\epsilon^{\ast}$, there exist weight matrices $W^g_1,W^g_2 \in \R^{d_2 \times m}, W^g_3 \in \R^{m \times d_2}$ and bias vector $b^g_1 \in \R^{d_2}$ such that the tanh neural network defined by,
\begin{equation}
\label{eq:hmmng}
\cN_g(h,c) = W^g_3\sigma\left(W^g_1 h + W^g_2c+ b^g_1 \right),
\end{equation}
approximates the function $\bG$ \eqref{eq:phmm3} in the following manner, 
\begin{equation}
    \label{eq:phmm4}
    \max\limits_{\max(\|h\|,\|c\|)< R^{\ast}} \|\bG(h,c) - \cN_g(h,c) \| \leq \epsilon^{\ast}.
\end{equation}

Next, as in the proof of propositions \ref{prop:exp1} \ref{prop:exp2}, one can readily approximate the identity function $\hat{f}(h,c) = h$ with a tanh neural network of the form,
\begin{equation}
\label{eq:hmmnfh}
\bar{\cN}_f(h) = \bar{W}_2\sigma\left(\bar{W}_1 h \right),
\end{equation}
 such that
\begin{equation}
    \label{eq:phmm5}
    \max\limits_{\|h\|,\|c\| < R^{\ast}} \|\hat{f}(h,c) - \cN_f(h) \| \leq \epsilon^{\ast},
\end{equation}
and with the same weights and biases, one can approximate the identity function $\hat{g}(h,c) = c$ with the Tanh neural network,
\begin{equation}
\label{eq:hmmngh}
\bar{\cN}_g(c) = \bar{W}_2\sigma\left(\bar{W}_1 c \right),
\end{equation}
 such that
\begin{equation}
    \label{eq:phmm6}
    \max\limits_{\|h\|,\|c\|< R^{\ast}} \|\hat{g}(h,c) - \cN_g(c) \| \leq \epsilon^{\ast}.
\end{equation}
Then, we define the following dynamical system, 
\begin{equation}
    \label{eq:phmm7}
    \begin{aligned}
    \hat{\bz}_n &= \overline{\delta t}\hat{\bz}_n + (1-\overline{\delta t})W^f_2\sigma\left(W^f_1 \hat{\by}_{n-1} + b^f_1 \right), \\
\tilde{\bz}_n &= \bar{W}_2\sigma\left(\bar{W}_1 \hat{\by}_{n-1} \right), \\
\hat{\by}_n &= (1-\tilde{\Dt})\hat{\by}_{n-1} +\tilde{\Dt} W^g_3\sigma\left(W^g_1 \hat{\bz}_{n} + W^g_2 \tilde{\bz}_{n} + b^g_1 \right), \\
\tilde{\by}_n &= \bar{W}_2\sigma\left(\bar{W}_1 \hat{\bz}_{n} \right),
    \end{aligned}
\end{equation}
with hidden states $\hat{\bz}_n,\tilde{\bz}_n, \hat{\by}_n,\tilde{\by}_n \in \R^{m}$ and with initial states $\hat{\bz}_0 = \tilde{\bz}_0=\hat{\by}_0 = \tilde{\by}_0 = 0$.

Completely analogously as in the derivation of \eqref{eq:exp29}, we can derive the following bound,
\begin{equation}
    \label{eq:phmm8}
     \|\bh_n - \hat{\bz}_n\| + \|\bc_{n} - \hat{\by}_{n}\| \leq C^{\ast}\epsilon^{\ast},
\end{equation}
with constant $C^{\ast} = C^{\ast}\left(n,{\rm Lip}(f),{\rm Lip}(g)\right)$. 

Defining the dynamical system,
\begin{equation}
    \label{eq:phmm9}
    \begin{aligned}
    \bz^{\ast}_n &= \overline{\delta t}\bz^{\ast}_n + (1-\overline{\delta t})\sigma\left(W^f_1W^g_3 \hat{\by}_{n-1} + b^f_1 \right) \\
    \bar{\bz}_n &= \sigma\left(\bar{W}_1 W^g_3\by^{\ast}_{n-1} \right) \\
    \by^{\ast}_n &= (1-\tilde{\Dt})\by^{\ast}_{n-1} +\tilde{\Dt} \sigma\left(W^g_1 W^f_3 \bz^{\ast}_{n} + W^g_2 \bar{W}_2 \tilde{\bz}_{n} + b^g_1 \right)  \\
    \bar{\by}_n &= \sigma\left(\bar{W}_1 W^f_2 \bz^{\ast}_{n} \right) .
    \end{aligned}
\end{equation}
By multiplying suitable matrices to \eqref{eq:phmm9}, we obtain that,
\begin{equation}
    \label{eq:phmm10}
    \hat{\bz}_n = W^f_2\bz_n^{\ast}, \quad 
    \tilde{\bz}_n = \bar{W}_2 \bar{\bz}_n, \quad 
    \hat{\by}_n = W^g_3\by_n^{\ast}, \quad 
    \tilde{\by}_n = \bar{W}_2 \bar{\by}_n.
\end{equation}

In addition to $b_{\infty}$ defined in \eqref{eq:exp15}, for $\overline{\delta t} \in (0,1]$, we introduce $b_{\delta} \in \R$ defined by 
\begin{equation}
    \label{eq:phmm11}
    \hat{\sigma}(b_{\delta}) = \overline{\delta t}.
\end{equation}
Similarly for $\tilde{\Dt} \in (0,1]$, we introduce $b_{\Delta} \in \R$ defined by 
\begin{equation}
    \label{eq:phmm011}
    \hat{\sigma}(b_{\Delta}) = \tilde{\Dt}.
\end{equation}
The existence of unique $b_{\delta}$ and $b_{\Delta}$ follows from the fact that the sigmoid function $\hat{\sigma}$ is monotone.

Next, we define the two vectors $\bb_{\infty},\bb_{\delta},\bb_{\Delta} \in \R^{2m}$ as
\begin{equation}
\label{eq:pf1}
\begin{aligned}
\bb_{\delta}^i &= b_{\delta}, \quad \forall~ 1 \leq i \leq m, \\
\bb_{\delta}^i &= b_{\infty}, \quad \forall~ m+1 \leq i \leq 2m, \\
\bb_{\Delta}^i &= b_{\Delta}, \quad \forall~ 1 \leq i \leq m, \\
\bb_{\Delta}^i &= b_{\infty}, \quad \forall~ m+1 \leq i \leq 2m . \\
\end{aligned}
\end{equation}
We define the LEM of form \eqref{eq:lem}, which will approximate the HMM \eqref{eq:hmm1},\eqref{eq:hmm2}. To this end, we define the hidden states $\bz_n,\by_n \in \R^{2m}$ such that $\bz_n = [\bz_n^{\ast},\bar{\bz}_n^{\ast}]$ and $\by_n = [\by_n^{\ast},\bar{\by}_n^{\ast}]$. The parameters for the corresponding LEM of form \eqref{eq:lem} given by,
\begin{equation}
    \label{eq:pf2}
    \begin{aligned}
    \Dt &= 1, d_y = 2m\\
    \bW_1 &= \bW_2 = \bV_1 = \bV_2 \equiv 0, \\
    \bb_1 &= \bb_{\delta}, \quad \bb_2 = \bb_{\Delta}, \\
    \bW_z &=\begin{bmatrix}
    W^f_1W^g_3 & 0 \\
    \bar{W}_1W^g_3 & 0 
    \end{bmatrix}, \quad \bV_z = 0, \quad \bb_z = [b^f_1,0], \\
    \bW_y &=\begin{bmatrix}
    W^g_1W^f_3 & W^g_2\bar{W}_2 \\
    \bar{W}_1W^f_2 & 0 
    \end{bmatrix}, \quad \bV_z = 0, \quad \bb_z = [b^g_1,0] . \\
    \end{aligned}
\end{equation}
The output states are defined by,
\begin{equation}
    \label{eq:pf3}
    \bom^h_n = W^f_2 \bz^{\ast}_n, \quad \bom^h_n = W^g_3 \by^{\ast}_n
\end{equation}

It is straightforward to observe that $\bom^h_n = \hat{\bz}_n,~\bom^c_n = \hat{\by}_n$. Hence, the desired bound \eqref{eq:phmm} follows from \eqref{eq:phmm8} by choosing,
$$
\epsilon^{\ast} = \frac{\epsilon}{C^{\ast}}.
$$

\end{proof}





\end{document}
