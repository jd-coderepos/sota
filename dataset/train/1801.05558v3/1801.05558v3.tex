

\documentclass{article}

\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[toc,page]{}
\usepackage{warning}
\usepackage{microtype}

\usepackage{graphicx} \usepackage{subcaption} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[draft]{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{soul}
\usepackage[normalem]{ulem}

\usepackage[symbol]{footmisc}
\renewcommand{\thempfootnote}{\arabic{mpfootnote}}



\usepackage[accepted]{icml2018}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\newtheorem{prop}{Proposition}


\usepackage{mathrsfs}
\usepackage{flushend}
\usepackage{booktabs}
\usepackage{lipsum}





\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bolde}{\mathbf{e}}
\newcommand{\boldf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\boldm}{\mathbf{m}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}




\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}




\newcommand{\calA}{{\mathcal{A}}}
\newcommand{\calB}{{\mathcal{B}}}
\newcommand{\calC}{{\mathcal{C}}}
\newcommand{\calD}{{\mathcal{D}}}
\newcommand{\calE}{{\mathcal{E}}}
\newcommand{\calF}{{\mathcal{F}}}
\newcommand{\calG}{{\mathcal{G}}}
\newcommand{\calH}{{\mathcal{H}}}
\newcommand{\calI}{{\mathcal{I}}}
\newcommand{\calJ}{{\mathcal{J}}}
\newcommand{\calK}{{\mathcal{K}}}
\newcommand{\calL}{{\mathcal{L}}}
\newcommand{\calM}{{\mathcal{M}}}
\newcommand{\calN}{{\mathcal{N}}}
\newcommand{\calO}{{\mathcal{O}}}
\newcommand{\calP}{{\mathcal{P}}}
\newcommand{\calQ}{{\mathcal{Q}}}
\newcommand{\calR}{{\mathcal{R}}}
\newcommand{\calS}{{\mathcal{S}}}
\newcommand{\calT}{{\mathcal{T}}}
\newcommand{\calU}{{\mathcal{U}}}
\newcommand{\calV}{{\mathcal{V}}}
\newcommand{\calW}{{\mathcal{W}}}
\newcommand{\calX}{{\mathcal{X}}}
\newcommand{\calY}{{\mathcal{Y}}}
\newcommand{\calZ}{{\mathcal{Z}}}
\newcommand{\calbX}{\mathbf{\mathcal{X}}}
\newcommand{\calbY}{\mathbf{\mathcal{Y}}}


\newcommand{\bcalA}{\mbox{\boldmath }}
\newcommand{\bcalB}{\mbox{\boldmath }}
\newcommand{\bcalC}{\mbox{\boldmath }}
\newcommand{\bcalD}{\mbox{\boldmath }}
\newcommand{\bcalE}{\mbox{\boldmath }}
\newcommand{\bcalF}{\mbox{\boldmath }}
\newcommand{\bcalG}{\mbox{\boldmath }}
\newcommand{\bcalH}{\mbox{\boldmath }}
\newcommand{\bcalI}{\mbox{\boldmath }}
\newcommand{\bcalJ}{\mbox{\boldmath }}
\newcommand{\bcalK}{\mbox{\boldmath }}
\newcommand{\bcalL}{\mbox{\boldmath }}
\newcommand{\bcalM}{\mbox{\boldmath }}
\newcommand{\bcalN}{\mbox{\boldmath }}
\newcommand{\bcalO}{\mbox{\boldmath }}
\newcommand{\bcalP}{\mbox{\boldmath }}
\newcommand{\bcalQ}{\mbox{\boldmath }}
\newcommand{\bcalR}{\mbox{\boldmath }}
\newcommand{\bcalS}{\mbox{\boldmath }}
\newcommand{\bcalT}{\mbox{\boldmath }}
\newcommand{\bcalU}{\mbox{\boldmath }}
\newcommand{\bcalV}{\mbox{\boldmath }}
\newcommand{\bcalW}{\mbox{\boldmath }}
\newcommand{\bcalX}{\mbox{\boldmath }}
\newcommand{\bcalY}{\mbox{\boldmath }}
\newcommand{\bcalZ}{\mbox{\boldmath }}


\newcommand{\sfA}{\mathsf{A}}
\newcommand{\sfB}{\mathsf{B}}
\newcommand{\sfC}{\mathsf{C}}
\newcommand{\sfD}{\mathsf{D}}
\newcommand{\sfE}{\mathsf{E}}
\newcommand{\sfF}{\mathsf{F}}
\newcommand{\sfG}{\mathsf{G}}
\newcommand{\sfH}{\mathsf{H}}
\newcommand{\sfI}{\mathsf{I}}
\newcommand{\sfJ}{\mathsf{J}}
\newcommand{\sfK}{\mathsf{K}}
\newcommand{\sfL}{\mathsf{L}}
\newcommand{\sfM}{\mathsf{M}}
\newcommand{\sfN}{\mathsf{N}}
\newcommand{\sfO}{\mathsf{O}}
\newcommand{\sfP}{\mathsf{P}}
\newcommand{\sfQ}{\mathsf{Q}}
\newcommand{\sfR}{\mathsf{R}}
\newcommand{\sfS}{\mathsf{S}}
\newcommand{\sfT}{\mathsf{T}}
\newcommand{\sfU}{\mathsf{U}}
\newcommand{\sfV}{\mathsf{V}}
\newcommand{\sfW}{\mathsf{W}}
\newcommand{\sfX}{\mathsf{X}}
\newcommand{\sfY}{\mathsf{Y}}
\newcommand{\sfZ}{\mathsf{Z}}




\newcommand{\balpha}{\mbox{\boldmath }}
\newcommand{\bbeta}{\mbox{\boldmath }}
\newcommand{\bgamma}{\mbox{\boldmath }}
\newcommand{\bdelta}{\mbox{\boldmath }}
\newcommand{\bepsilon}{\mbox{\boldmath }}
\newcommand{\bvarepsilon}{\mbox{\boldmath }}
\newcommand{\bzeta}{\mbox{\boldmath }}
\newcommand{\boldeta}{\mbox{\boldmath }}
\newcommand{\btheta}{\mbox{\boldmath }}
\newcommand{\bvartheta}{\mbox{\boldmath }}
\newcommand{\biota}{\mbox{\boldmath }}
\newcommand{\bkappa}{\mbox{\boldmath }}
\newcommand{\blambda}{\mbox{\boldmath }}
\newcommand{\bmu}{\mbox{\boldmath }}
\newcommand{\bnu}{\mbox{\boldmath }}
\newcommand{\bxi}{\mbox{\boldmath }}
\newcommand{\bpi}{\mbox{\boldmath }}
\newcommand{\bvarpi}{\mbox{\boldmath }}
\newcommand{\brho}{\mbox{\boldmath }}
\newcommand{\bvarrho}{\mbox{\boldmath }}
\newcommand{\bsigma}{\mbox{\boldmath }}
\newcommand{\bvarsigma}{\mbox{\boldmath }}
\newcommand{\btau}{\mbox{\boldmath }}
\newcommand{\bupsilon}{\mbox{\boldmath }}
\newcommand{\bphi}{\mbox{\boldmath }}
\newcommand{\bvarphi}{\mbox{\boldmath }}
\newcommand{\bchi}{\mbox{\boldmath }}
\newcommand{\bpsi}{\mbox{\boldmath }}
\newcommand{\bomega}{\mbox{\boldmath }}




\newcommand{\bGamma}{\mbox{\boldmath }}
\newcommand{\bDelta}{\mbox{\boldmath }}
\newcommand{\bTheta}{\mbox{\boldmath }}
\newcommand{\bLambda}{\mbox{\boldmath }}
\newcommand{\bXi}{\mbox{\boldmath }}
\newcommand{\bPi}{\mbox{\boldmath }}
\newcommand{\bSigma}{\mbox{\boldmath }}
\newcommand{\bUpsilon}{\mbox{\boldmath }}
\newcommand{\bPhi}{\mbox{\boldmath }}
\newcommand{\bPsi}{\mbox{\boldmath }}
\newcommand{\bOmega}{\mbox{\boldmath }}




\newcommand{\veca}{{\vec{\ba}}}
\newcommand{\vecb}{{\vec{\bb}}}
\newcommand{\vecc}{{\vec{\bc}}}
\newcommand{\vecd}{{\vec{\bd}}}
\newcommand{\vece}{{\vec{\bolde}}}
\newcommand{\vecf}{{\vec{\boldf}}}
\newcommand{\vecg}{{\vec{\bg}}}
\newcommand{\vech}{{\vec{\bh}}}
\newcommand{\veci}{{\vec{\bi}}}
\newcommand{\vecj}{{\vec{\bj}}}
\newcommand{\veck}{{\vec{\bk}}}
\newcommand{\vecl}{{\vec{\bl}}}
\newcommand{\vecm}{{\vec{\bm}}}
\newcommand{\vecn}{{\vec{\bn}}}
\newcommand{\veco}{{\vec{\bo}}}
\newcommand{\vecp}{{\vec{\bp}}}
\newcommand{\vecq}{{\vec{\bq}}}
\newcommand{\vecr}{{\vec{\br}}}
\newcommand{\vecs}{{\vec{\bs}}}
\newcommand{\vect}{{\vec{\bt}}}
\newcommand{\vecu}{{\vec{\bu}}}
\newcommand{\vecv}{{\vec{\bv}}}
\newcommand{\vecw}{{\vec{\bw}}}
\newcommand{\vecx}{{\vec{\bx}}}
\newcommand{\vecy}{{\vec{\by}}}
\newcommand{\vecz}{{\vec{\bz}}}

\newcommand{\vecxi}{{\vec{\bxi}}}
\newcommand{\vecphi}{{\vec{\bphi}}}
\newcommand{\vecvarphi}{{\vec{\bvarphi}}}
\newcommand{\vecbeta}{{\vec{\bbeta}}}
\newcommand{\vecdelta}{{\vec{\bdelta}}}
\newcommand{\vectheta}{{\vec{\btheta}}}


\newcommand{\Real}{\mathbb R}
\newcommand{\Complex}{\mathbb C}
\newcommand{\Natural}{\mathbb N}
\newcommand{\Integer}{\mathbb Z}
\newcommand{\Ind}{\mathbb I}
\newcommand{\Prob}{\mathbb P}


\usepackage{color}
\definecolor{orange}{rgb}{1,0.3,0}
\definecolor{copper}{rgb}{1,.62,.40}
\definecolor{hotpink}{rgb}{1,0,0.5}
\definecolor{darkgreen1}{rgb}{0, .35, 0}
\definecolor{darkgreen}{rgb}{0, .6, 0}
\definecolor{darkred}{rgb}{.75,0,0}


\newcommand{\bone}{\mathbf{1}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\0}{{\bf 0}}

\newcommand{\be}{}
\newcommand{\bee}{}

\newcommand{\matrixb}{\left[ \begin{array}}
\newcommand{\matrixe}{\end{array} \right]}   

\newtheorem{theorem}{{Theorem}}
\newtheorem{lemma}{{Lemma}}
\newtheorem{corollary}{{Corollary}}
\newtheorem{definition}{{Definition}}
\newtheorem{proposition}{{Proposition}}
\newtheorem{remark}{{Remark}}
\newtheorem{example}{{Example}}


\newcommand{\argmax}{\operatornamewithlimits{\arg \max}}
\newcommand{\argmin}{\operatornamewithlimits{\arg \min}}

\newcommand{\mean}[1]{\left \langle #1 \right \rangle}
\newcommand{\expectation}[1]{ \E \left[ #1 \right] }
\newcommand{\ave}{\mathbb E}
\newcommand{\E}{\mathbb E}
\newcommand{\empha}[1]{{\color{red} \bf #1}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\incomplete}[1]{\textcolor{darkred}{#1}}
\newcommand{\defeq}{\stackrel{\mathrm{def}}{=}}



\def\doublespace{\renewcommand{\baselinestretch}{2}\large\normalsize}
\def\singlespace{\renewcommand{\baselinestretch}{1}\large\normalsize}
\def\onehalfspace{\renewcommand{\baselinestretch}{1.5}\large\normalsize}
\def\onequaterspace{\renewcommand{\baselinestretch}{1.3}\large\normalsize}
\def\threequaterspace{\renewcommand{\baselinestretch}{1.7}\large\normalsize}
\def\smallspace{\renewcommand{\baselinestretch}{-.9}\large\normalsize}
\def\tinyspace{\renewcommand{\baselinestretch}{-.7}\large\normalsize}

\newcommand{\tr} { \textrm{tr} }
\newcommand{\re} { \textrm{re} }
\newcommand{\im} { \textrm{im} }
\newcommand{\diag} { \textrm{diag} }
\newcommand{\ddiag} { \textrm{ddiag} }
\newcommand{\off} { \textrm{off} }
\newcommand{\vectxt} { \textrm{vec} }

\newcommand{\lla}{\left\langle}
\newcommand{\rra}{\right\rangle}
\newcommand{\llbr}{\left\lbrack}
\newcommand{\rrbr}{\right\rbrack}
\newcommand{\llb}{\left\lbrace}
\newcommand{\rrb}{\right\rbrace}
 
\newcommand{\x}{\mathbf{x}}
\newcommand{\xii}{\x^i}
\newcommand{\xl}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\yi}{\y^i}

\newcommand{\A}{\mathbf{A}}
\newcommand{\Ai}{\A^i}
\newcommand{\Al}{\mathbf{A}}
\newcommand{\Allr}{\Al^{\text{lr}}}

\newcommand{\W}{\mathbf{W}}
\newcommand{\Wi}{\W^i}
\newcommand{\Wiz}{\Wi_0}
\newcommand{\Wz}{\W_0}
\newcommand{\T}{\mathbf{T}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\Ti}{\T^i}

\newcommand{\logit}{\bzeta}
\newcommand{\logitj}{\zeta_j}
\newcommand{\logitij}{\zeta_j^i}

\newcommand{\temp}{\mathbf{c}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\mi}{\m^i}
\newcommand{\yhat}{\hat{\mathbf{y}}}

\newcommand{\loss}{\mathcal{L}_{\calT}}
\newcommand{\V}{\mathbf{V}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\mask}{\M}
\newcommand{\maski}{\mask^i}
\newcommand{\maskj}{\mask_j}
\newcommand{\maskT}{\mask_{\T}}
\newcommand{\Rn}{\mathbb{R}^n}




\icmltitlerunning{Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace}

\begin{document}

\twocolumn[
\icmltitle{Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yoonho Lee}{postech}
\icmlauthor{Seungjin Choi}{postech}
\end{icmlauthorlist}

\icmlaffiliation{postech}{Department of Computer Science and Engineering, Pohang University of Science and Technology, Korea}
\icmlcorrespondingauthor{Yoonho Lee}{einet89@postech.ac.kr}
\icmlcorrespondingauthor{Seungjin Choi}{seungjin@postech.ac.kr}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  


\begin{figure*} \begin{subfigure}{.9\columnwidth}
  \centering
  \includegraphics[height=1.6in]{figures/hook-a-netshown}
  \caption{}
  \label{fig:sfig1}
\end{subfigure}\begin{subfigure}{.65\columnwidth}
  \centering
  \includegraphics[height=1.6in]{figures/hook-b-notation}
  \caption{}
  \label{fig:sfig2}
\end{subfigure}
\begin{subfigure}{.45\columnwidth}
  \centering
  \includegraphics[height=1.6in]{figures/hook-c}
  \caption{}
  \label{fig:sfig3}
\end{subfigure}
\caption{
Task-specific learning in an MT-net.
(a) A cell (rounded rectangle) consists of two layers.
In addition to initial weights (black), the meta-learner specifies weights to be changed (dotted lines) by task-specific learners (colored).
(b) Activation of this cell has  dimensions, but activation of task-specific learners only change within a subspace (white plane).
(c) The value of  affects task-specific learning so that gradients of  are sensitive to task identity.
Best seen in color.
}
\label{fig:fig}
\end{figure*}
 
\iffalse
\begin{abstract}
Recent advances in meta-learning demonstrate that deep representations combined with
the gradient descent method have sufficient capacity to approximate any learning algorithm.
A promising approach is model-agnostic meta-learning (MAML) which embeds gradient descent into the meta-learner.
It optimizes for the initial parameters of the learner to warm-start the gradient descent updates,
such that new tasks can be solved using a small number of examples.
In this paper we elaborate the gradient-based meta-learning, developing two new schemes.
First, we present a feedforward neural network, referred to as {\em T-net}, where the linear transformation 
between two adjacent layers is decomposed as  such that  is learned by task-specific learners
and the transformation , which is shared across tasks, is meta-learned to speed up the convergence of gradient updates for task-specific learners.
Second, we present {\em MT-net} where gradient updates in the T-net are guided by a binary mask  that
is meta-learned, restricting the updates to be performed in a subspace.
Empirical results demonstrate that our method is less sensitive to the choice of initial learning rates than existing meta-learning methods,
and achieves the state-of-the-art or comparable performance on few-shot classification and regression tasks.
\end{abstract}
\fi
\begin{abstract}
Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks.
While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing.
Our primary contribution is the {\em MT-net}, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on.
Additionally, a task-specific learner of an {\em MT-net} performs gradient descent with respect to a meta-learned distance metric,
which warps the activation space to be more sensitive to task identity.
We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods.
Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

While recent deep learning methods achieve superhuman performance on various tasks including image classification 
\cite{Krizhevsky2012nips} or playing games \cite{MnihV2015nature}, they can only do so using copious amounts of data and computational resources.
In many problems of interest, learners may not have such luxuries.
\textit{Meta-learning} \cite{SchmidhuberJ87phd,SchmidhuberJ97mlj,ThrunS98book} methods are a potential solution to this problem; 
these methods leverage information gathered from prior learning experience to learn more effectively in novel tasks.
This line of research typically casts learning as a two-level process, each with a different scope.
The \emph{meta-learner} operates on the level of tasks, gathering information from several instances of task-specific learners.
A \emph{task-specific learner}, on the other hand, operates on the level of datapoints, and incorporates the meta-learner's knowledge in its learning process.

Model-agnostic meta-learning (MAML) \cite{FinnC2017icml} is a meta-learning method that directly optimizes the gradient descent procedure of task-specific learners.
All task-specific learners of MAML share initial parameters, and a meta-learner optimizes these initial parameters such that gradient descent 
starting from such initial parameters quickly yields good performance.
An implicit assumption in having the meta-learner operate in the same space as task-specific learners is that the two different scopes of learning
require equal degrees of freedom.

Our primary contribution is the MT-net (Figure 1), a neural network architecture and task-specific learning procedure.
An MT-net differs from previous gradient-based meta-learning methods in that the meta-learner determines a subspace and a corresponding metric 
that task-specific learners can learn in, thus setting the degrees of freedom of task-specific learners to an appropriate amount.
Note that the activation space of the cell shown in Fig.1(b) is -dimensional.
Because the task-specific learners can only change weights that affect two of the three intermediate activations, 
task-specific learning only happens on a subspace with  degrees of freedom.
Additionally, meta-learned parameters  alter the geometry of the activation space (Fig.1(c)) of task-specific parameters
so that task-specific learners are more sensitive to change in task.




\section{Background}
\label{sec:background}


\iffalse
{\color{red}[BH: This section needs to be completely revised. Discussing three background items is not good idea and makes readers bored.]}
\subsection{Few-shot Learning}

We formalize the few-shot learning problem in this subsection.
Assume we have a parameterized function  that maps observations  to actions .
A task is defined as a tuple  consisting of a distribution over initial observations , a transition distribution , a horizon , and a loss function  that maps a sequence of observations and actions to a real number.

We assume that all tasks come from a common distribution , and our goal is for our function  to be able to quickly minimize loss given any task in .
During (meta-)training, we sample new tasks .
The model sees  examples from  and makes updates and predictions using only that information.
This model is then evaluated on the loss function .
The model typically makes some sort of (meta-)update using this loss function.
At meta-test time, we sample new tasks  and report the loss of  after seeing  examples from .
Tasks used for meta-testing are typically held out during meta-training.
\fi

\subsection{Problem Setup}
\label{subsec:setup}
We briefly explain the meta-learning problem setup which we apply to few-shot tasks.

The problems of -shot regression and classification are as follows.
In the training phase for a meta-learner, we are given a (possibly infinite) set of tasks .
Each task provides a training set and a test set .
We assume here that the training set  has  examples per class, hence the name -shot learning.
A particular task  is assumed to be drawn from the distribution of tasks .
Given a task , the task-specific model  (our work considers a feedforward neural network)
is trained using the dataset  and 
its corresponding loss . 
Denote by  parameters obtained by optimizing .
Then, the meta-learner  is updated using the feedback from the collection of losses

where the loss of each task is evaluated using the test data .
Given a new task  (not considered during meta-training),
the meta-learner helps the model  to quickly adapt to the new task ,
by warm-starting the gradient updates.

\subsection{Model-Agnostic Meta-Learning}
We briefly review model-agnostic meta-learning (MAML) \cite{FinnC2017icml}, emphasizing commonalities and differences between MAML and our method.
MAML is a meta-learning method that can be used on any model that learns using gradient descent. 
This method is loosely inspired by fine-tuning, and it learns initial parameters of a network such that the network's loss after a few (usually ) gradient steps is minimized.

Consider a model with parameters .
MAML alternates between the two updates (\ref{eq:maml_update}) and (\ref{eq:maml_update1})
to determine initial parameters  
for task-specific learners to warm-start the gradient descent updates,
such that new tasks can be solved using a small number of examples.
Each task-specific learner updates its parameters by gradient descent (\ref{eq:maml_update}) 
using the loss evaluated with the training data . 
The meta-optimization across tasks (\ref{eq:maml_update1}) is performed such that the parameters  are updated
using the loss evaluated with .
Note that during meta-optimization (\ref{eq:maml_update1}), the gradient is computed with respect to initial parameters  but the test loss is computed with respect to task-specific parameters .

\be
\label{eq:maml_update}
\widetilde{\theta}_\calT &\leftarrow& \theta - \alpha \nabla_\theta \loss \left(\theta, \calD_{\calT,train} \right) \\
\theta &\leftarrow& \theta - \beta \nabla_{\theta}
\label{eq:maml_update1}
\left( \sum_{\calT \sim p(\calT)} \loss \left(\widetilde{\theta}_{\calT}, \calD_{\calT,test} \right) \right),
\ee
where  and  are learning rates and 
the summation in (\ref{eq:maml_update1}) is computed using minibatches of tasks sampled from .

Intuitively, a well-learned initial parameter  is close to some local optimum for every task .
Furthermore, the update (\ref{eq:maml_update}) is sensitive to task identity in the sense that  and 
have different behaviors for different tasks .

Recent work has shown that gradient-based optimization is a universal learning algorithm \cite{FinnC2017arxiv}, 
meaning that any learning algorithm can be approximated up to arbitrary accuracy using some parameterized model and gradient descent.
Thus, no expressiveness is lost by only considering gradient-based learners as in (\ref{eq:maml_update}). 
Note that since MAML operates using a single fixed model, one may have to go through trial and error to find such a good model.

Our method is similar to MAML in that our method also differentiates through gradient update steps to optimize performance after fine-tuning.
However, while MAML assumes a fixed model, our method actually chooses a subset of its weights to fine-tune.
In other words, it (meta-)learns which model is most suitable for the task at hand.
Furthermore, whereas MAML learns with standard gradient descent, a subset of our method's parameters effectively 'warp' the parameter space of the parameters to be learned during meta-testing to enable faster learning.







\begin{figure}[ht!]
\centering\includegraphics[width=\columnwidth]{figures/tnet_cell}
\caption{A diagram of the adaptation process of a Transformation Network (T-net).
Blue values are meta-learned and shared across all tasks. 
Orange values are different for each task.}
\end{figure}




\section{Meta-Learning Models}
\label{sec:main}

We present our two models in this section: Transformation Networks (T-net) and Mask Transformation Networks (MT-net),
 both of which are trained with gradient-based meta-learning.
A T-net learns a metric in its activation space;
this metric informs each task-specific learner's update direction and step size.
An MT-net additionally learns which subset of its weights to update for task-specific learning.
Therefore, an MT-net learns to automatically assign one of two roles (task-specific or task-mutual) to each of its weights.


\subsection{T-net}
\label{subsec:tnet}

We consider a model  with paramaters .
This model consists of  cells, where each cell is parameterized\footnote{
For convolutional cells,  is a convolutional layer with some size and stride and
and  is a  convolution that doesn't change the number of channels
} as :
\be
\label{eq:tmodel}
\lefteqn{ f_{\theta}(\x)} \nonumber \\
& =  \T^L \W^L \left( \sigma \left( \T^{L-1} \W^{L-1} \left( \ldots 
\sigma \left(  \T^1 \W^1 \x \right) \right) \right) \right),  
\ee
where  is an input, and  is a nonlinear activation function.
T-nets get their name from transformation matrices () because the linear transformation defined by a  plays a crucial role in meta-learning.
Note that a cell has the same expressive power as a linear layer.
Model parameters  are therefore a collection of 's and 's, i.e.,
\bee
\theta =\left\{ \underbrace{\W^1, \ldots, \W^L}_{\theta_{\W}}, 
\underbrace{\T^1, \ldots, \T^L}_{\theta_{\T}}  \right\}.
\eee
Transformation parameters , which are shared across task-specific models, are determined by the meta-learner.
All task-specific learners share the same initial  but update to different values since each uses their corresponding train set .
Thus we denote such (adjusted) parameters for task  as .
Though they may look similar,  denotes a task while  denotes a transformation matrix.

\begin{algorithm}[t]
\caption{Transformation Networks (T-net)}
\label{alg:tnet}
\begin{algorithmic}[1]
\REQUIRE 
\REQUIRE , 
\STATE randomly initialize 
\WHILE{not done}
    \STATE Sample batch of tasks  
    \FORALL{}
        \FOR{}
        	\STATE Compute  according to (\ref{eq:theta_wt})
\ENDFOR
        \STATE 
    \ENDFOR
    \STATE 
\ENDWHILE
\end{algorithmic}
\end{algorithm}

 Given a task , each  is adjusted with the gradient update
\be
\label{eq:theta_wt}
\widetilde{\W}_{\calT} \leftarrow \W - \alpha \nabla_{\W} 
\loss \left(\theta_{\W}, \theta_{\T},\calD_{\calT,train} \right).
\ee
Again,  is defined as .
Using the task-specific learner , the meta-learner improves itself with the gradient update
\be
\label{eq:theta}
\theta  \leftarrow \theta - \beta \nabla_{\theta}
 \left( \sum_{\calT \sim p(\calT)} \loss \left(\widetilde{\theta}_{\W,\calT}, \theta_{\T},\calD_{\calT,test} \right) \right).
\ee
 and  are learning rate hyperparameters.
We show our full algorithm in Algorithm \ref{alg:tnet}.

To evaluate on a new task , we do the following.
We compute task-specific parameters  using (\ref{eq:theta_wt}),
starting from the meta-learned initial value .
We report the loss of task-specific parameters  on the test set .

We now briefly examine a single cell:
\bee
\y = \T \W \x,
\eee
where  is the input to the cell and  its output.
The squared length of a change in output  is calculated as
\be
\| \Delta \y \|^2 = \left( (\Delta \W) \x \right)^{\top} \left( \T^{\top} \T \right) \left( (\Delta \W) \x \right),
\ee
where  is similarly defined as .
We see here that the magnitude of  is determined by the interaction between  and .
Since a task-specific learner performs gradient descent only on  and not , the change in  resulting from (\ref{eq:theta_wt})
is guided by the meta-learned value .
We provide a more precise analysis of this behavior in Section \ref{sec:analysis}.



\subsection{MT-net}
\label{subsec:mtnet}

\begin{figure}[ht!]
\centering\includegraphics[width=\columnwidth]{figures/mtnet_cell}
\caption{
A diagram of the adaptation process of a Mask Transformation Network (MT-net).
Blue values are meta-learned and shared across all tasks. 
Orange values are different for each task.
}
\end{figure}

The MT-net is built on the same feedforward model (\ref{eq:tmodel}) as the T-net:
\be
\label{eq:mtmodel}
\lefteqn{ f_{\theta}(\x)} \nonumber \\
& =  \T^L \W^L \left( \sigma \left( \T^{L-1} \W^{L-1} \left( \ldots 
\sigma \left(  \T^1 \W^1 \x \right) \right) \right) \right).  
\ee
The MT-net differs from the T-net in the binary mask applied to the gradient update 
to determine which parameters are to be updated.
The update rule for task-specific parameters  is given by
\be
\label{eq:mt_update}
\widetilde{\W}_{\calT} \leftarrow \W-\alpha \mask \odot \nabla_{\W}  \mathcal{L}(\theta_{\W}, \theta_{\T},\calD_{\calT,train}),
\ee
where  is the Hadamard (elementwise) product between matrices of the same dimension.
 is a binary gradient mask which is sampled each time the task-specific learner encounters a new task.
Each row of  is either an all-ones vector  or an all-zeros vector .
We parameterize the probability of row  in  being  with a scalar variable :
\be
\mask & = & [\m_1, \ldots, \m_n]^\top, \nonumber \\
\label{eq:bern}
\m_j^{\top}  & \sim & \text{Bern} \left(\frac{\exp \left( \logitj \right)}{\exp \left( \logitj \right)+1} \right) \bone^\top,
\ee
where  denotes the Bernoulli distribution.
Each logit  acts on a row of a weight matrix , so weights that contribute to the same immediate activation are updated or not updated together.

We approximately differentiate through the Bernoulli sampling of masks using the Gumbel-Softmax estimator \cite{Jang2017iclr,Maddison2017iclr}:
\be
g_1, g_2 & \sim & \text{Gumbel}(0, 1), \\
\label{eq:gs}
\m_{j}^{\top} & \leftarrow & \frac{\exp \left(\frac{\logitj + g_1}{\temp} \right)}
{\exp \left(\frac{\logitj + g_1}{\temp}\right)+\exp \left(\frac{g_2}{\temp} \right)} \bone^\top,
\ee
where  is a temperature hyperparameter.
This reparameterization allows us to directly backpropagate through the mask.
At the limit of , (\ref{eq:gs}) follows the behavior of (\ref{eq:bern}).

\begin{algorithm}[t]
\caption{Mask Transformation Networks (MT-net)}
\label{alg:mtnet}
\begin{algorithmic}[1]
\REQUIRE 
\REQUIRE , 
\STATE randomly initialize 
\WHILE{not done}
    \STATE Sample batch of tasks  
    \FORALL{}
        \FOR{}
        	\STATE Sample binary mask  according to (\ref{eq:gs})
\STATE Compute  according to (\ref{eq:mt_update})

        \ENDFOR
        \STATE 
    \ENDFOR
    \STATE 
\ENDWHILE
\end{algorithmic}
\end{algorithm}

 As in T-nets, we denote the collection of altered weights as .
The meta-learner learns all parameters :
\be
\theta =\left\{ 
\underbrace{\W^1, \ldots, \W^L}_{\theta_{\W}}, 
\underbrace{\T^1, \ldots, \T^L}_{\theta_{\T}},
\underbrace{\logit^1, \ldots, \logit^L}_{\theta_{\logit}},
  \right\}.
\ee
As in a T-net, the meta-learner performs stochastic gradient descent on :
\be
\label{eq:theta}
\theta  \leftarrow \theta - \beta \nabla_{\theta}
 \left( \sum_{\calT \sim p(\calT)} \loss \left(\widetilde{\theta}_{\W,\calT}, \theta_{\T},\theta_{\logit},\calD_{\calT,test} \right) \right).
\ee
The full algorithm is shown in Algorithm \ref{alg:mtnet}.

We emphasize that the binary mask used for task-specific learning () depends on meta-learned parameter weights ().
Since the meta-learner optimizes the loss in a task after a gradient step (), the matrix  gets assigned a high probability of
having value  for weights that are meant to encode task-specific information.
Furthermore, since we update  along with model parameters  and , the meta-learner is incentivized to
learn configurations of  and  in which there exists a clear divide between task-specific and task-mutual neurons.





\section{Analysis}
\label{sec:analysis}

In this section, we provide further analysis of the update schemes of T-nets and MT-nets.

We analyse how the activation space of a single cell of a T-net or MT-net behaves during task-specific learning.
More specifically, we make precise how  encodes a learned curvature matrix.
By using such an analysis to reason about a whole network consisting of several cells, 
we are impliticly approximating the full curvature matrix of the network by
a block-diagonal curvature matrix.
In this approximation, second-order interactions only occur among weights in the same layer (or cell).
Previous works \cite{Heskes2000neuralcomp,Martens2015icml,Desjardins2015nips} have used such an approximation of the curvature of a neural network.

\subsection{T-nets Learn a Metric in Activation Space}
\label{subsec:tnet_analysis}

We consider a cell in a T-net where the pre-activation value  is given by
\be
\y = \T \W \x = \A \x,
\ee
where  and  is the input to the cell. 
We omit superscripts throughout this section.

A standard feedforward network resorts to the gradient of a loss function  
(which involves a particular task ) with respect to the parameter matrix ,
to update model parameters.
In such a case, a single gradient step yields
\be
\y^\text{new} &=& (\A - \alpha \nabla_{\A} \loss) \x  \nonumber \\
&=& \y - \alpha \nabla_{\A} \loss \x.
\ee
The update of a T-net () results in the following new value of :
\be
\label{eq:tnet_update}
\y^\text{new} &=& \T \left(\T^{-1} \A - \alpha \nabla_{\T^{-1} \A} \loss \right) \x  \nonumber \\
&=& \y - \alpha  \left( \T \T^{\top} \right) \nabla_{\A} \loss \x,
\ee
where  is determined by the meta-learner.
Thus, in a T-net, the incremental change of  is proportional to the negative of the gradient 
, while the standard feedforward net resorts to a step proportional to 
the negative of .
Task-specific learning in the T-net is guided by a full rank metric in each cell's activation space,
which is determined by each cell's transformation matrix .
This metric  warps (scaling, rotation, etc.) the activation space of the model so that in this warped space,
a single gradient step with respect to the loss of a new task yields parameters that are well suited for that task.






\subsection{MT-nets Learn a Subspace with a Metric}
\label{subsec:mtnet_analysis}
We now consider MT-nets and analyze what their update () means from the viewpoint of .

MT-nets can restrict its task-specific learner to any subspace of its gradient space:

\begin{prop}
\label{prop}
Fix  and . 
Let  be a cell in an MT-net and let  be its corresponding mask parameters.
Let  be a d-dimensional subspace of  ().
There exist configurations of  and  such that the span of  is  while satisfying .
\end{prop}

\begin{proof}
See Appendix B.
\end{proof}

This proposition states that , and  have sufficient expressive power to restrict updates of  to any subspace.
Note that this construction is only possible because of the transformation ; if we only had binary masks , we would only be able to restrict gradients to axis-aligned subspaces.

In addition to learning a subspace that we project gradients onto (), we are also learning a metric in this subspace.
We first provide an intuitive exposition of this idea.

We unroll the update of an MT-net as we did with T-nets in (\ref{eq:tnet_update}):


Where  is an  matrix which has the same columns as .
Let's denote .
We see that the update of a task-specific learner in an MT-net performs the update .
Note that  is an  matrix that only has nonzero elements in rows and columns where  is .
By setting appropriate , we can view  as a full-rank  metric tensor.

This observation can be formally stated as:
\begin{prop}
\label{prop2}
Fix , , and a loss function .
Let  be a cell in an MT-net and let  be its corresponding mask parameters.
Let  be a d-dimensional subspace of , and  a metric tensor on .
There exist configurations of  and  such that the vector  is in 
the steepest direction of descent on  with respect to the metric .
\end{prop}

\begin{proof}
See Appendix B.
\end{proof}
Therefore, not only can MT-nets project gradients of task-specific learners onto a subspace of the pre-activation () space,
they can also learn a metric in that subspace and thereby learning a low-dimensional linear embedding of the activation space.
The MT-net update (\ref{eq:mt_update}) is gradient descent in this low-dimensional embedding,
so the meta-objective shown in (\ref{eq:theta}) is minimized when gradient descent in this embedding requires few steps to converge and is sensitive to task identity.



\section{Related Work}

\iffalse
A straightforward approach for applying meta-learning to deep networks is to meta-learn some pattern of the weights of a neural network or its updates; previous work has learned an update rule \cite{RaviS2017iclr, Li2016nips, Andrychowicz2016nips}, directly generated weights \cite{Ha2017iclr}, or learned good initial weights to fine-tune \cite{FinnC2017arxiv}.
Another line of research uses recurrent neural networks to encode a learning rule and sequentially inputs training data to this neural network \cite{SantoroA2016icml,Mishra2018iclr,MunkhdalaiT2017icml,Duan2016arxiv, Wang2016arxiv}.
An approach that has been successful in the domain of few-shot classification is to learn a distance metric between images and use this metric to find the most similar previously seen image to a new image \cite{KochG2015icml, VinyalsO2016nips, SnellJ2017nips}.
\fi

A successful line of research in few-shot learning uses feedforward neural networks as learners.
These approaches learn update rules \cite{RaviS2017iclr, Li2016nips, Andrychowicz2016nips} or directly generate weights \cite{Ha2017iclr}.
A related research direction is to learn initial parameters \cite{FinnC2017icml} while fixing the learning rule to gradient descent,
 or additionally learning learning rates for each weight \cite{Li2017arxiv}.
\cite{Grant2018iclr} interprets such gradient-based meta-learning as hierarchical bayesian inference, and
\cite{FinnC2017arxiv} states that such methods are expressive enough to approximate any learning algorithm.

Our work is closely related to this line of research.
Unlike previous work, MT-nets learn how many degrees of freedom the task-specific learner should have at meta-test time.
Additionally, while MT-nets learn update rules, these update rules are directly embedded in the network itself instead of being stored in a separate model.

Distance metric learning \cite{XingEP2002nips, WeinbergerKQ2005nips} methods learn a distance function between datapoints.
Similarly, MT-nets learn a full metric matrix.
Whereas those methods required constrained optimization techniques to enforce that the learned matrix represents a metric,
our parameterization allows us to directly learn such a metric using gradient descent.
Recently, neural networks have been used to learn a metric between images\cite{KochG2015icml, VinyalsO2016nips, SnellJ2017nips},
achieving state-of-the-art performance on few-shot classification benchmarks.
Unlike these methods, we learn a metric in feature space instead of input space.
Our method applies to a larger class of problems including regression and reinforcement learning, since all MT-nets require is a differentiable loss function.

Another line of research in few-shot learning is to use a recurrent neural network (RNN) as a learner \cite{SantoroA2016icml, MunkhdalaiT2017icml}.
Here, the meta-learning algorithm is gradient descent on an RNN, and the learning algorithm is the update of hidden cells.
The (meta-learned) weights of the RNN specify a learning strategy, which processes training data and uses the resulting hidden state vector to make decisions about test data.
A recent work that uses temporal convolutions for meta-learning\cite{Mishra2018iclr} is also closely related to this line of research.

\iffalse
The concept of fast weights\cite{fast} is also related to our method.
The idea here is that in addition to ordinary (slow) weights, having fast weights that rapidly activate and decay can result in robust yet adaptive learning.
This paper gives some evidence from physiology which suggests that similar mechanisms are inside the human brain as well.
\fi


\section{Experiments}
We performed experiments to answer:
\begin{itemize}
\setlength\itemsep{0em}
	\item Do our novel components ( etc) improve meta-learning performance? (6.1)
	\item Is applying a mask  row-wise actually better than applying one parameter-wise? (6.1)
	\item To what degree does  alleviate the need for careful tuning of step size ? (6.2)
	\item In MT-nets, does learned subspace dimension reflect the difficulty of tasks? (6.3)
	\item Can T-nets and MT-nets scale to large-scale meta-learning problems? (6.4)
\end{itemize}

Most of our experiments were performed by modifying the code accompanying \cite{FinnC2017icml},
and we follow their experimental protocol and hyperparameters unless specified otherwise.

\subsection{Toy Regression Problem}
\label{subsec:sine}
\begin{table}[t]
\begin{minipage}{\columnwidth}
\label{tab:sine}
  \centering
\begin{tabular}{llll}
\specialrule{.7pt}{1pt}{1pt}
    Models & 5-shot & 10-shot & 20-shot\\
    \midrule
    MAML\footnote{\label{metasgd} Reported by \cite{Li2017arxiv}.} &  1.07  0.11 & 0.71  0.07 & 0.50  0.05\\
    Meta-SGD\footref{metasgd} & 0.88  0.14 & 0.53  0.09 & 0.35  0.06\\
    \midrule
    M-net-full & 0.91  0.09 & 0.63  0.07 & 0.38  0.04\\
    M-net & 0.88  0.09 & 0.60  0.06 & 0.41  0.04\\
    T-net & 0.83  0.08 & 0.56  0.06 & 0.38  0.04\\
    MT-net-full & 0.81  0.08 & 0.51  0.05 & 0.35  0.04\\
    MT-net & \textbf{0.76  0.09} & \textbf{0.49  0.05} & \textbf{0.33  0.04}\\
\specialrule{.7pt}{1pt}{1pt}
  \end{tabular}
  \caption{
  Loss on sine wave regression.
  Networks were meta-trained using 10-shot regression tasks.
  Reported losses were calculated after adaptation using various numbers of examples.
  }
\end{minipage}
\end{table} 

\begin{figure*}[t]
\begin{subfigure}{.5\columnwidth}
  \centering
  \includegraphics[height=1.3in]{figures/0/0}
\end{subfigure}\begin{subfigure}{.5\columnwidth}
  \centering
  \includegraphics[height=1.3in]{figures/1/0}
\end{subfigure}\begin{subfigure}{.5\columnwidth}
  \centering
  \includegraphics[height=1.3in]{figures/2/0}
\end{subfigure}\begin{subfigure}{.5\columnwidth}
  \centering
  \includegraphics[height=1.3in]{figures/frac}
\end{subfigure}

\begin{subfigure}{2\columnwidth}
  \includegraphics[width=.75\columnwidth]{figures/legend}
\end{subfigure}\caption{
10-shot regression tasks to sets of polynomials of various degrees.
MT-nets choose to update a larger fraction of weights as the set of tasks gets more complex.
}
\end{figure*}
We start with a -shot regression problem and compare results to previous meta-learning methods \cite{FinnC2017icml, Li2017arxiv}.
The details of our regression task are the same as \cite{Li2017arxiv}.
Each individual task is to regress from the input x to the output y of a sine function

For each task,  are sampled uniformly from  and , respectively.
Each task consists of  training examples and  testing examples.
We sample  uniformly from  for both train and test sets.
Our regressor architecture has two hidden cells each with activation size .
After every  is a ReLU nonlinearity.
The loss function is the mean squared error (MSE) between the regressor's prediction  and the true value .
We used Adam \cite{KingmaDP2015iclr} as our meta-optimizer with a learning rate of .
Task-specifc learners used step size .
We initialize all  to , all  as identity matrices, and all  as truncated normal matrices with standard deviation .
While we trained our meta-learner with  examples, we tested using various numbers of examples ().

We show results in Table 1.
To see if each of our novel components increased meta-learning performance, we also performed the same experiments with variations of MT-nets.
An M-net uses a mask  like an MT-net, but each cell consists of a single matrix  instead of .
A model with "-full" at the end of its name learns a separate mask parameter for each weight of  instead of sharing a mask 
among weights that contribute to the same activation.
For example, if  has size , the corresponding  in an MT-net would be of dimension ,
but in MT-net-full, the dimension of  would be .
MT-nets outperform MAML, meta-SGD, and all variations of MT-nets.

\subsection{Robustness to learning rate change}
\begin{table}[t]
  \centering
\label{tab:sinelr}
\begin{tabular}{llll}
\specialrule{.7pt}{1pt}{1pt}
	 & MAML & T-net & MT-net \\
    \midrule
	10 & 171.92  25.04 & \textbf{4.08  0.30} & 4.18  0.30 \\
	1 & 5.81  0.49 & 4.15  0.30 & \textbf{0.61  0.07} \\
	0.1 & 1.05  0.11 & 0.68  0.06 & \textbf{0.54  0.05} \\
	0.01 & 0.71  0.07 & 0.56  0.06 & \textbf{0.49  0.05} \\
	0.001 & 0.82  0.08 & \textbf{0.59  0.06} & \textbf{0.59  0.06} \\
	0.0001 & 2.54  0.19 & \textbf{0.62  0.06} & 0.72  0.07\\
\specialrule{.7pt}{1pt}{1pt}
  \end{tabular}
  \caption{
  Loss on 10-shot sine wave regression.
  T-nets and MT-nets are bost robust to change in step size .
  This is due to the meta-learned matrix  inside each cell, which alters the effective step size.
  }
\end{table} The transformation  of our method can change the effective step size .
We performed experiments to see how robust our method is to variations in .
We perform the same sinusoid experiment as in section 6.1, but with various step sizes ().
We evaluate on  training examples, and all other settings are identical to the experiments in section 6.1.

We show losses after adaptation of both MAML and MT-nets in Table 2.
We can see that MT-nets are more robust to change in step size.
This indicates that as shown in section \ref{subsec:mtnet_analysis}, the matrix  is capable of warping the parameter space
to recover from suboptimal step size .

\subsection{Task Complexity and Subspace Dimension}

\begin{table*}[t]
  \centering
\begin{minipage}{\textwidth}
  \centering
  \label{tab:class}
  \begin{tabular}{lll}
\specialrule{.7pt}{1pt}{1pt}
    Models & 5-way 1-shot acc. () & 20-way 1-shot acc. () \\
    \hline
    \textbf{Matching Networks}\cite{VinyalsO2016nips} & 98.1 & 93.8 \\
    \textbf{Prototypical Networks}\cite{SnellJ2017nips}& 97.4 & 92.0 \\
    \textbf{mAP-SSVM}\cite{Triantafillou2017} & 98.6 & 95.4 \\
    \hline
    \textbf{MAML}\cite{FinnC2017icml} & 98.7  0.4 & 95.8  0.3 \\
    \textbf{Meta-SGD}\cite{Li2017arxiv} & \textbf{99.53  0.26} & 95.93  0.38\\
    \hline
    \textbf{T-net (ours)} & 99.4  0.3 & 96.1  0.3 \\
    \textbf{MT-net (ours)} & \textbf{99.5  0.3} & \textbf{96.2  0.4} \\
\specialrule{.7pt}{1pt}{1pt}
  \end{tabular} \\
  \vspace{10pt}
  \centering
  \begin{tabular}{ll}
\specialrule{.7pt}{1pt}{1pt}
    Models & 5-way 1-shot acc. () \\
    \hline
    \textbf{Matching Networks}\cite{VinyalsO2016nips}\footnote{\label{metalstm} Reported by \cite{RaviS2017iclr}.} & 43.56  0.84\\
    \textbf{Prototypical Networks}\cite{SnellJ2017nips}\footnote{ Reported results for 5-way 1-shot.} &  46.61  0.78 \\
    \textbf{mAP-SSVM}\cite{Triantafillou2017} & 50.32  0.80 \\
    \hline
    \textbf{Fine-tune baseline}\footref{metalstm} &  28.86  0.54 \\
    \textbf{Nearest Neighbor baseline}\footref{metalstm}&  41.08  0.70 \\
    \textbf{meta-learner LSTM}\cite{RaviS2017iclr} &  43.44  0.77 \\
    \textbf{MAML}\cite{FinnC2017icml} &  48.70  1.84 \\
    \textbf{L-MAML}\cite{Grant2018iclr} &  49.40  1.83 \\
    \textbf{Meta-SGD}\cite{Li2017arxiv} &  50.47  1.87 \\
    \hline
    \textbf{T-net (ours)} & 50.86  1.82 \\
    \textbf{MT-net (ours)} & \textbf{51.70  1.84} \\
\specialrule{.7pt}{1pt}{1pt}
\end{tabular}
  \centering
  \caption{Few-shot classification accuracy on (top) held-out Omniglot characters and (bottom) test split of MiniImagenet.  represents  confidence intervals.}
\end{minipage}
\end{table*} We performed this experiment to see whether the dimension of the learned subspace of MT-nets reflect the underlying complexity of its given set of tasks.

We consider 10-shot regression tasks in which the target function is a polynomial.
A polynomial regression meta-task consists of polynomials of the same order with various coefficients.
To generate a polynomial of order  (), we uniformly sampled  from .
We used the same network architecture and hyperparameters as in Section 6.1 and performed -shot regression for polynomial orders .
Since the number of free parameters is proportional to the order of the polynomial, 
we expect higher-order polynomials to require more parameters to adapt to.
The fraction of parameters that task-specific learners change is calculated as the expected value of 
 over all logits .

We show results in Figure 4, and additional results in Appendix C.
The number of weights that the meta-learner of an MT-net sets to be altered increases as the task gets more complex.
We interpret this as the meta-learner of MT-nets having an effect akin to Occam's razor:
it selects a task-specific model of just enough complexity to learn in a set of tasks.
This behavior emerges even though we do not introduce any additional loss terms to encourage such behavior.
We think this is caused by the noise inherent in stochastic gradient descent.
Since the meta-learner of an MT-net can choose whether or not to perform gradient descent in a particular direction, it is incentivized not to do so in directions that are not model-specific, because doing so would introduce more noise into the network parameters and thus (in expectation) suffer more loss.


\subsection{Classification }

To compare the performance of MT-nets to prior work in meta-learning, 
we evaluate our method on few-shot classification on the Omniglot \cite{Lake2015science} and MiniImagenet \cite{RaviS2017iclr} datasets.
We used the miniImagenet splits proposed by \cite{RaviS2017iclr} in our experiments.
\iffalse
The Omniglot dataset, sometimes referred to as the 'transpose of MNIST', consists of 1623 character classes from 50 different alphabets and has only 20 instances of each.
The MiniImagenet dataset is a subset of the full ImageNet dataset which has 100 different classes with 600 examples of each class.
\fi

Our CNN model uses the same architecture as \cite{FinnC2017icml}.
The model has  modules: each has  convolutions and  filters, followed by batch normalization \cite{Ioffe2015icml}.
As in \cite{FinnC2017icml}, we used 32 filters per layer in miniImagenet.
Convolutions have stride  on Omniglot, and  max-pooling is used after batch normalization
instead of strided convolutions on MiniImagenet.
We evaluate with 3, 5, and 10 gradient steps for Omniglot 5-way, Omniglot 20-way, and miniImagenet 5-way, respectively.

Results are shown in Table 3.
MT-nets achieve state-of-the-art or comparable performance on both problems.
Several works \cite{Mishra2018iclr,MunkhdalaiT2017icml,Sung2017arxiv} have reported improved performance on MiniImagenet using a significantly more expressive architecture.
We only report methods that have equal or comparable expressiveness to the model first described in \cite{VinyalsO2016nips}.
Not controlling for network expressivity, the highest reported accuracy so far on 5-way 1-shot miniImagenet classification is  \cite{Sung2017arxiv}.



\section{Conclusion}

We introduced T-nets and MT-nets.
One can transform any feedforward neural network into an MT-net, so any future architectural advances can take advantage of our method.
Experiments showed that our method alleviates the need for careful tuning of the learning rate in few-shot learning problems 
and that the mask  reflects the complexity of the set of tasks it is learning to adapt in.
MT-nets also showed state-of-the-art performance in a challenging few-shot classification benchmark (MiniImagenet).

While we think MT-nets are a gradient-based meta-learning method, our analysis has shown that it has some interesting commonalities with optimizer learning methods such as \cite{RaviS2017iclr}.
We will investigate this connection between two seemingly disparate approaches to meta-learning in future work.

One of the biggest weaknesses of deep networks is that they are very data intensive.
By learning what to learn when a new task is encountered, we can train networks with high capacity using a small amount of data.
We believe that designing effective gradient-based meta-learners will be beneficial not just for the few-shot learning setting, 
but also machine learning problems in general.


\clearpage

\section*{Acknowledgements}
SC was supported by Samsung DS Software Center, Samsung Research, and Naver.

\nocite{SnellJ2017nips}
\nocite{RaviS2017iclr}
\nocite{FinnC2017icml}
\nocite{FinnC2017arxiv}
\nocite{VinyalsO2016nips}
\nocite{AmariS98neco}
\nocite{KochG2015icml}
\nocite{SantoroA2016icml}
\nocite{RobbinsH51ams}
\nocite{ThrunS98book}
\nocite{MnihV2015nature}
\nocite{MunkhdalaiT2017icml}
\nocite{Sung2017arxiv}
\nocite{Edwards2017iclr}
\nocite{Kaiser2017iclr}
\nocite{Garcia2017arxiv}
\nocite{Li2017aaai}
\nocite{tsai2017arxiv}
\nocite{fort2017arxiv}
\nocite{Ren2018iclr}
\nocite{Hariharan2017iccv}
\nocite{Mishra2018iclr}
\nocite{Maddison2017iclr}

\bibliography{sjc}
\bibliographystyle{icml2018}


\clearpage



\end{document} 
