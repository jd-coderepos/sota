\documentclass[journal]{IEEEtran}
\usepackage{tikz,xcolor}


\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{multirow}
\usepackage{bm}
\usepackage{balance}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,     linkcolor=blue,      citecolor=blue,      }

\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{
\begin{tikzpicture}
\draw[lime, fill=lime] (0,0)
circle[radius=0.16]
node[white]{{\fontfamily{qag}\selectfont \tiny \.{I}D}};
\end{tikzpicture}
\hspace{-2mm}
}
\foreach \x in {A, ..., Z}{\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}{\noexpand\orcidicon}}
}

\newcommand{\orcidauthorA}{0000-0001-5968-5209}
\newcommand{\orcidauthorB}{0000-0002-6177-3862}
\begin{document}

\title{Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval}

\author{
Qing Ma,
Jiancheng Pan\hspace{-1.5mm}\orcidA{},~\IEEEmembership{Student Member,~IEEE},
Cong Bai\hspace{-1.5mm}\orcidB{},~\IEEEmembership{Member,~IEEE,}
\thanks{Qing Ma~(e-mail: maqing@zjut.edu.cn), Jiancheng Pan~(e-mail: jianchengpan@zjut.edu.cn), and Cong Bai~(e-mail: congbai@zjut.edu.cn) are with the College of Computer Science and Technology, Zhejiang University of Technology, 310023 Hangzhou, China and also with the Key Laboratory of Visual Media Intelligent Processing Technology of Zhejiang Province. (Corresponding authors: Cong Bai.)}
\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}


\IEEEpubid{0000--0000/00\I\bm{M}_v=[\bm{v}_1, \bm{v}_2,...,\bm{v}_{N_m}]^{\mathrm{T}} \in \mathbb{R}^{N_m \times d}\bm{R}_v=[\bm{u}_1, \bm{u}_2,...,\bm{u}_{N_r}]^{\mathrm{T}} \in \mathbb{R}^{N_r \times d/2}\timesN_m=4layer1layer2layer3layer4N_r=36\bm{F}_M \in \mathbb{R}^{N_m \times d}\bm{F}_R \in \mathbb{R}^{N_r \times d}\bm{W}_r\bm{b}_rT\{\bm{w}_1, \bm{w}_2,...,\bm{w}_{N_c}\}\bm{w}_i \in \mathbb{R}^{d}(i \in[1,N_c])\bm{e}_i  = \bm{W}_e\bm{w}_i(i \in[1,N_c])\bm{W}_e\bm{h}_i^f\bm{h}_i^bi\bm{\mathcal{H}}^f\bm{\mathcal{H}}^b\bm{F}_G \in \mathbb{R}^{N_c \times d}\bm{\mathcal{X}} \in \mathbb{R}^{N_c \times d}\tilde{\bm{\mathcal{X}}} \in \mathbb{R}^{N_c \times d}\bm{\mathcal{G}}(\cdot)\bm{\mathcal{Q}} \in \mathbb{R}^{N_c \times d}\bm{\mathcal{K}} \in \mathbb{R}^{N_c \times d}\bm{\mathcal{V}} \in \mathbb{R}^{N_c \times d}\bm{W}_\mathcal{Q} (\bm{W}_\mathcal{K},\bm{W}_\mathcal{V})\bm{b}_\mathcal{Q} (\bm{b}_\mathcal{K},\bm{b}_\mathcal{V})\bm{G} \in \mathbb{R}^{N_c \times d}\bm{W}_A \bm{b}_A\odot\sigma \left(\cdot \right)\bm{\mathcal{Q}}^{\prime} \in \mathbb{R}^{N_c \times d}\bm{\mathcal{K}}^{\prime} \in \mathbb{R}^{N_c \times d}\operatorname{Softmax}\left(\cdot \right)\tilde{\bm{\mathcal{H}}}^f\bm{\mathcal{H}}^f\bm{\mathcal{H}}^b\bm{\mathcal{F}}(\cdot)\bm{\mathcal{T}}_{f\odot b}\bm{\mathcal{T}}_{b\odot f}\bm{\mathcal{T}}_c = \bm{\mathcal{T}}_{f\odot b}+\bm{\mathcal{T}}_{b\odot f}\bm{F}_G \in \mathbb{R}^{N_c \times d}\bm{W}_M (\bm{W}_R)\bm{b}_M (\bm{b}_R)\bm{S}_{M R}\bm{F}_{R \oplus M}\bm{F}_{M \oplus R}\bm{F}_{M R} \in \mathbb{R}^{(N_m+N_r) \times d}\bm{W}_L\bm{b}_LConcat(\cdot)\bm{F}_R = [\bm{r}_1,\bm{r}_2,...,\bm{r}_{N_r}]^{\mathrm{T}} \in \mathbb{R}^{N_r \times d}\bm{F}_G = [\bm{g}_1,\bm{g}_2,...,\bm{g}_{N_c}]^{\mathrm{T}} \in \mathbb{R}^{N_c \times d}\bm{E}_R \in \mathbb{R}^{d}\bm{E}_G \in \mathbb{R}^{d}\bm{E}_R^{\prime} \in \mathbb{R}^{B_c \times d}\bm{E}_G^{\prime} \in \mathbb{R}^{B_c \times d}B_c\bm{W}_R (\bm{W}_G)\bm{b}_R (\bm{b}_G)\bm{F}_{R \oplus G}\bm{S}_{R G}\bm{F}_{R \oplus G}\bm{F}_{R G} \in \mathbb{R}^{B_c \times d}\bm{F}_M = [\bm{m}_1,\bm{m}_2,...,\bm{m}_{N_m}]^{\mathrm{T}} \in \mathbb{R}^{N_m \times d}\bm{T}_G\bm{V}_{M R}\bm{T}_{R G}\bm{F}_G\bm{F}_{M R}\bm{F}_{R G}\alpha[x]_{+} \equiv \max (x, 0)\hat{V}\hat{T}S\left(\cdot, \cdot \right)\mathcal{L}(\bm{V}_{M R}, \bm{T}_{R G})\mathcal{L}(\bm{V}_M, \bm{T}_G)\lambda_gR@K (K=1, 5, 10)mRR@KKmRR@K\lambda_g\lambda_g\alpha\lambda_g\bm{\mathcal{H}}^f\bm{\mathcal{H}}^f\bm{\mathcal{H}}^b\bm{\mathcal{H}}^b\bm{\mathcal{H}}^f\bm{\mathcal{H}}^b\bm{\mathcal{H}}^{\frac{f+b}{2}}\bm{\mathcal{H}}^{\frac{f+b}{2}}\lambda_g\lambda_gmR\lambda_g=0.01\lambda_g\lambda_g=10.0\lambda_g=10.0R@10\lambda_g=1.0\lambda_g=10.0\bm{\mathcal{H}}^f\bm{\mathcal{H}}^f\bm{\mathcal{H}}^b\bm{\mathcal{H}}^b\bm{\mathcal{H}}^f\bm{\mathcal{H}}^b\bm{\mathcal{H}}^{\frac{f+b}{2}}\bm{\mathcal{H}}^{\frac{f+b}{2}}\bm{\mathcal{H}}^f\bm{\mathcal{H}}^f\bm{\mathcal{H}}^b\bm{\mathcal{H}}^bR@1R@10R@1R@10\bm{\mathcal{H}}^f\bm{\mathcal{H}}^b\bm{\mathcal{H}}^f\bm{\mathcal{H}}^bR@1mRR@10mRR@1R@1R@1mR\bm{V}_{M}\bm{V}_{R}\bm{T}_G\bm{V}_{M R}\bm{T}_{R G}d(\bm{V}_{M R},\bm{T}_{R G}) < d(\bm{V}_{M},\bm{T}_{G})d(\bm{V}_{R},\bm{V}_{MR}) < d(\bm{V}_{R},\bm{V}_{M})d(\bm{V}_{R},\bm{T}_{R G}) < d(\bm{V}_{R},\bm{T}_{G})$); specifically, regional visual features guide the representation of the text by adjusting the spatial distance more slightly. The above results demonstrate that in our DOVE model, the regional visual embedding plays an oriented role in the latent semantic space; the multiscale visual embedding and word-level text embedding serve as an external global visual-semantic constraint that holds the distance between the final visual and textual embeddings.

\subsubsection{Visualization of Latent Embedding Space}
To visually evaluate the contribution of the DOVE model to the final visual and textual representations, we used the t-SNE \cite{van2008visualizing} to visualize the final visual and textual embeddings in latent embedding space, as shown in Fig. \ref{fig:fig7}. It can be observed that the latent embedding space visualization of remote sensing image-text retrieval presents a cluster-like distribution according to different scene types, and semantically similar images or sentences are close in latent embedding space. However, ``\emph{dense residential}'' and ``\emph{medium residential}'' differ only in housing density, and it is difficult to define their scene categories. This case tends to cause the model to learn an incorrect visual representation, and aggravate the visual-semantic imbalance. It can be found that matching images and sentences are close to each other and as far away from the mismatching ones as possible. Many mismatching image-text pairs of different scene categories are close to each other, such as ``\emph{farmland}'', ``\emph{park},'' and ``\emph{resort},'' which is an apparent inter-class similarity that can easily lead to visual-semantic imbalance. Similarly, the second example demonstrates the same conclusion. The above results reflects that our method can identify most of the scenes better, but there are still some scenes that are harder to distinguish.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{images/Figure10.pdf}
  \caption{Visualization of latent embedding space. Colored numbers represent images or text of different scenes; normal font: final textual embeddings;  bold font: final visual embeddings.}
  \label{fig:fig7}
\end{figure*}

\subsubsection{Exploring Semantic Localization}
Semantic localization \cite{yuan2022learning} refers to using text as query to obtain the best matching location in large-scale remote sensing images. We compared our method with AMFMN \cite{yuan2022exploring} and GaLR \cite{yuan2022remote} for semantic localization, with results as shown in Fig. \ref{fig:fig8}. For example, the text ``\emph{lots of cars parked in a parking lot surrounded by gray roads}'' retrieved the most semantically relevant areas in the image. From the semantic localization results, the DOVE model has a more realistic localization effect, whose relevant regions have more precise localization boundaries. Heat map results show that the DOVE model has more accurate edges in the dense region of ``\emph{cars}'' than AMFMN and GaLR, which indicates that our method has more accurate location identification in semantic localization. The blue part in the heat map shows model's filtering ability for redundancy. Compared with the other two methods, the DOVE model can identify the redundant features more accurately. The experiments qualitatively illustrate the superiority of our method in visual semantic understanding.
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/Figure11.pdf}
  \caption{Results of remote sensing image-text retrieval methods on semantic localization. First row: semantic localization result; second row: heat map, where colors closer to red indicate more semantic relevance and blue is the opposite.}
  \label{fig:fig8}
\end{figure}

\section{CONCLUSION}
In this paper, we proposed the DOVE model to solve the visual-semantic imbalance in remote sensing image-text retrieval. The ROAM module adaptively adjust the distance between final visual embedding and final textual embedding to mine the intrinsic connection between vision and language. To enhance textual representation, the DTGA model learns a better textual representation using forward and backward contextual semantics. A global visual-semantic constraint acts as an external constraint for the final visual and textual representations and reduce single visual dependency. Experiments demonstrated the effectiveness of DOVE, which outperformed state-of-the-art methods on the RSICD and RSITMD datasets.

Mining the semantics of remote-sensing images is a valuable and necessary task. In the future, we will continue to explore further applications and enhancements of image-text retrieval in remote sensing. A more integrated and unified model, adapted to the special environment of remote sensing, might be needed for the current remote sensing image-text retrieval.

\bibliographystyle{IEEEtran}

\bibliography{references}

\begin{IEEEbiography}[{\includegraphics[width=0.85in ,clip,keepaspectratio]{images/author/mq.jpg}}]{Qing Ma}
received the B.S. and M.S. degrees from Beijing Normal University, Beijing, China, in 2002 and 2005, respectively, and the Ph.D. degree from the Zhejiang University of Technology, Hangzhou, China, in 2021. Since 2005, she has been on the faculty of the College of Science, Zhejiang University of Technology. Her research interests include cross-modal retrieval and computer vision.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=0.85in ,clip,keepaspectratio]{images/author/pjc.jpeg}}]{Jiancheng Pan}
(Student Member, IEEE) received the B.E. degree from Jiangxi Normal University, Nanchang, China, in 2022. He is currently pursuing the M.E. degree with the Zhejiang University of Technology, Hangzhou, China. 

His research interests include vision and language, multimodal learning and computer vision.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=0.85in ,clip,keepaspectratio]{images/author/bc.jpg}}]{Cong Bai}
(Member, IEEE) received the B.E. degree from Shandong University, Jinan, China, in 2003, the M.E. degree from Shanghai University, Shanghai, China, in 2009, and the Ph.D. degree from the National Institute of Applied Sciences, Rennes, France, in 2013.

He is a Professor with the College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China. His research interests include computer vision and multimedia processing.
\end{IEEEbiography}

\end{document}
