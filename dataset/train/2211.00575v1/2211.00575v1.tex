
\begin{table*}
{ \small
\begin{tabular}{lcccccccc}
    \toprule
    \multirow{2}{50pt}{\textbf{Model}} &
    \multicolumn{4}{c}{Romantic} & 
    \multicolumn{4}{c}{Humorous} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9} &
    B@1 & B@3 & M & C & B@1 & B@3 & M & C \\
\midrule
    StyleNet & 13.3 & 1.5 & 4.5 & 7.2 & 13.4 & 0.9 & 4.3 & 11.3 \\
    \midrule
    MemCap & 21.2 & 4.8 & 8.4 & 22.4 & 19.9 & 4.3 & 7.4 & 19.4 \\
    \midrule
\modelname{} + Image-Text Pre-training & 27.9 & 8.9 & 12.6 & 52.2 & 29.4 & 8.8 & 13.2 & 55.1 \\
    \midrule
    \modelname{} + Text-Only Pre-training & 23.0 & 4.6 & 9.1 & 27.4 & 22.7 & 4.3 & 9.7 & 29.0 \\
    \midrule
    \modelname{}  & 21.4 & 5.0 & 9.6 & 26.9 & 24.9 & 6.0 & 10.2 & 34.1 \\
    \toprule
\end{tabular}
\caption{\textbf{Style-Guided captioning results on FlickrStyle10K }\cite{gan2017stylenet}. 
}
\vspace{-0.35cm}
\label{tab:style} 
}
\end{table*} \section{Results}
\label{sec:res} 

We next evaluate \modelname{} on several captioning tasks, demonstrating state-of-the-art results. See supplementary for additional details.









\paragraph{Image Captioning.}
We compare \modelname{} caption quality to several baselines with different supervision levels, as presented in \cref{tab:captioning}(A). Here, all methods were trained end evaluated over the same dataset, using the commonly used MS-COCO \cite{lin2014microsoft, chen2015microsoft} and Flickr30k \cite{young2014image}. We begin by evaluating fully supervised techniques: BUTD \cite{anderson2018bottom}, UniVLP \cite{zhou2020unified}, ClipCap \cite{mokady2021clipcap}, Oscar \cite{li2020oscar}, and Lemon \cite{hu2022scaling}. As expected, these achieve a better score than \modelname{}, as they exploit the additional supervision of image-text pairs. Nevertheless, compared to the unsupervised approaches of MAGIC \cite{su2022language} and ZeroCap \cite{tewel2022zerocap}, \modelname{} achieves superior scores. Note that ZeroCap does not require any training data, while MAGIC requires text data similar to our setting. 


\paragraph{Cross-Domain Captioning.}
We test our generalization ability by training on one dataset while evaluating on another, as in  \citet{su2022language}. Again, as can be seen in \cref{tab:captioning}(B), \modelname{} outperforms MAGIC \cite{su2022language}, which uses the same supervision as \modelname{}.





\paragraph{Style-Guided Captioning.}
Several works \cite{zhao2020memcap,gan2017stylenet} have studied the task of adapting a captioning model to a new style, such as ``romantic'' or ``humorous''. Since collecting paired examples for each style requires great effort, these consider the setting where the new style is only learned from text. This is easy to do in our setting, since we can train the decoder on any given style text. \cref{figures:examples.png} shows captions generated with \modelname{} in several styles (same setting and data as in \citet{zhao2020memcap}). \cref{tab:style} reports quantitative results for this setting, showing \modelname{} outperforms other baselines. To further analyze our approach, we present our results without pre-training (i.e., training on styled data only), with a text-only pre-training over COCO, and with text-image pre-training over COCO (similar to \cite{zhao2020memcap}). As can be seen, we outperform \cite{zhao2020memcap} even with considerably less supervision at pre-training. Moreover, both other variations improve results, demonstrating that CapDec can effectively use additional training data where available.











\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/examples.png}
\vspace{-0.35cm}
\caption{Example for styled captions of \modelname{} on FlickrStyle10K \cite{gan2017stylenet}.}
\vspace{-0.45cm}
\label{figures:examples.png}
\end{figure}

\paragraph{The Effect of Noise Level.}
A key element of \modelname{} is noise injection before decoding. 
To demonstrate the effect of noise, we report results as a function of the noise variance  in \cref{figures:varianceXmetrics.png}. It can be seen that too little or too much noise is suboptimal. We note that the noise variance we chose, =,\footnote{As mentioned in \cref{sec:method}, we estimated the optimal STD by the mean infinity-norm of embedding differences between captions that correspond to the same image, which is =} is based only on text, and not on the results in \cref{figures:varianceXmetrics.png} which are shown for analysis purposes only.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/varianceXmetrics.png}
\caption{\label{figures:varianceXmetrics.png}{The effect of the noise variance on MS-COCO performance.}}
\end{figure}















