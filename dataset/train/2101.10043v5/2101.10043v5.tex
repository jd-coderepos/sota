\def\year{2022}\relax
\documentclass[letterpaper]{article} \usepackage{aaai22}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{newfloat}
\usepackage{listings}
\lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

\newcommand{\beginsupplement}{\setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}\setcounter{figure}{0}
        \renewcommand{\thesection}{\Alph{section}}
     }


\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{ dsfont }
\usepackage{ cite }
\usepackage{float}
\usepackage{soul}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage[title]{appendix}
\usepackage{thmtools, thm-restate}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{question}[theorem]{Question}
\renewcommand{\thelemma}{\arabic{lemma}}\usepackage{xpatch}
\makeatletter
\xpatchcmd{\@thm}{\thm@headpunct{.}}{\thm@headpunct{}}{}{}
\makeatother













\pdfinfo{
/Title (Deep One-Class Classification via Interpolated Gaussian Descriptor)
/Author (Yuanhong Chen, Yu Tian, Guansong Pang, and Gustavo Carneiro)
}

\setcounter{secnumdepth}{0}
\title{Deep One-Class Classification via Interpolated Gaussian Descriptor}

\author{
Yuanhong Chen\textsuperscript{\rm 1}\equalcontrib,
Yu Tian\textsuperscript{\rm 1}\equalcontrib\thanks{Corresponding author.},
Guansong Pang\textsuperscript{\rm 2}, and
    Gustavo Carneiro\textsuperscript{\rm 1}
}
\affiliations{
\textsuperscript{\rm 1}Australian Institute for Machine Learning, University of Adelaide, Australia\\
\textsuperscript{\rm 2}School of Computing and Information Systems, Singapore Management University, Singapore
}



\begin{document}
\maketitle


\begin{abstract}
One-class classification (OCC) aims to learn an effective data description to enclose all normal training samples and detect anomalies based on the deviation from the data description. 
Current state-of-the-art OCC models learn a compact normality description by hyper-sphere minimisation, but they often suffer from overfitting the training data, especially when the training set is small or contaminated with anomalous samples. 
To address this issue, we introduce the interpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a one-class Gaussian anomaly classifier trained with adversarially interpolated training samples.  
The Gaussian anomaly classifier differentiates the training samples based on their distance to the Gaussian centre and the standard deviation of these distances, offering the model a discriminability w.r.t. the given samples during training. 
The adversarial interpolation is enforced to consistently learn a smooth Gaussian descriptor, even when the training data is small or contaminated with anomalous samples. 
This enables our model to learn the data description based on the representative normal samples rather than fringe or anomalous samples, resulting in significantly improved normality description. 
In extensive experiments on diverse popular benchmarks, including MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves better detection accuracy than current state-of-the-art models. IGD also shows better robustness
in problems with small or contaminated training sets. 






\end{abstract}

\section{Introduction}
Anomaly detection and segmentation are critical tasks in many real-world applications, such as the identification of defects on industry objects~\cite{mvtecad} or abnormalities from medical images~\cite{anogan,f-anogan}.
Given that most of the training sets available for this task contain only normal images, existing methods are typically formulated as one-class classifiers (OCC)~\cite{venkataramanan2019attention,dsvdd}.
OCCs aim to first learn a data description of normal samples in the training set and then use a criterion (e.g., distance to the one-class centre~\cite{dsvdd}) to detect and localise anomalies in test samples. 

\begin{figure}[t!]
  \centering
    \includegraphics[width=0.48\linewidth]{image/cifar_nc_new.pdf}
    \includegraphics[width=0.48\linewidth]{image/cifar_sts_new.pdf}
    
\caption{Mean testing AUC of DSVDD~\cite{dsvdd}, and our proposed IGD trained with the CIFAR10 training set contaminated with 1\%, 5\% and 10\% of anomalous samples (left), and small training sets, consisting of 20\%, 60\%, and 100\% of the CIFAR10 training set (right).
}
\label{fig:motivation1}
\end{figure}


















State-of-the-art (SOTA) OCC models are trained by minimising 
the radius of a hyper-sphere to enclose all training samples in the representation space~\cite{dsvdd,perera2019learning,ruff2019deep}. 
To avoid catastrophic collapse, where all training samples are projected to a single point in the representation space, these OCC models fix the hyper-sphere centre and remove the bias terms from the model. 
Even though these SOTA OCC models show accurate anomaly detection results in several benchmarks, 
they can overfit the training data, particularly when the training set is small or contaminated with anomalous samples, as shown by the results of DSVDD~\cite{dsvdd} in Fig.~\ref{fig:motivation1}.




In this paper, we introduce the interpolated Gaussian descriptor (IGD) method to address the overfitting issue presented in SOTA OCC models.
IGD is based on a one-class Gaussian anomaly classifier modelled with adversarially interpolated training samples.
The classifier is trained to
build a normality description to
discriminate training samples based on their distance to the Gaussian centre and the standard deviation of these distances.
The smoothness of the normality description is enforced by the adversarial interpolation of the training samples that constrains the training of IGD to be 
based on representative normal samples rather than fringe or anomalous samples.
This allows the normality description of IGD to be more robust than the SOTA OCC models, particularly when the training set is small or contaminated with anomalous samples, as shown in
Fig.~\ref{fig:motivation1} and t-SNE results in appendix.


In summary, our paper makes the following contributions:
\begin{itemize}
     \item One novel OCC model that targets the learning of an effective normality description based on representative normal samples rather than fringe or anomalous samples, resulting in an improved anomaly classifier, compared with the SOTA; 
     \item One new OCC optimisation approach based on a theoretically sound derivation of the expectation-maximisation (EM) algorithm that optimises a Gaussian anomaly classifier constrained by adversarially interpolated training samples and multi-scale structural and non-structural image reconstruction to enforce a smooth normality description; and
     \item One new OCC benchmark to assess the robustness of anomaly detectors to training sets that are small or contaminated with anomalous samples.
\end{itemize}
Extensive empirical results on six popular anomaly detection benchmarks for semantic anomaly detection, industrial defect detection,
and malignant lesion detection show that our model IGD
can generalise well across these diverse application domains and perform consistently better than current SOTA detectors. We also show that IGD is more robust than current OCC approaches
when dealing with small and contaminated training sets.


\section{Related Work}

\label{sec:unsupervised_anomaly_detection}
Unsupervised anomaly detection (UAD) is generally solved with OCCs~\cite{li2021cutpaste,tian2021constrained,tian2020few,tian2021weakly,dsvdd,bergmann2020uninformed,ocgan,Salehi_2021_CVPR,Wang_2021_CVPR,tian2021selfsupervised,bergman2020classification,golan2018deep,defard2021padim,Zavrtanik_2021_ICCV,wang2016s}.
A representative OCC model is DSVDD~\cite{dsvdd}, which forces normal image features to be inside a hyper-sphere with a pre-defined centre and a radius that is minimised to include all training images. Then, test images that fall inside the  hyper-sphere are classified as normal, and the ones outside are anomalous.
Although powerful, the hard boundary of SVDD can cause the model to overfit the training data -- this problem was tackled with a soft-boundary SVDD~\cite{dsvdd}, but it can still overfit given that it lacks enough generalisation constraints. 
OCC methods can also rely on generative models, such as generative adversarial network (GAN) or Auto-encoder (AE).
In~\cite{ocgan}, a GAN is trained to produce normal samples, and its discriminator is used to detect anomalies, but the complex training process of GANs represents a disadvantage of this approach.
An AE~\cite{ionescu2019object,gong2019memorizing,nguyen2019anomaly,sabokrou2017deep,sabokrou2018adversarially,venkataramanan2019attention} is trained to reconstruct normal data, and the anomaly score is defined as the reconstruction error between the input and reconstructed images.  AE approaches depend on the MSE reconstruction loss, which does not work well for structural anomalies.  Alternatively, single-scale SSIM loss~\cite{ae-ssim} tends to work well for structural anomalies of a specific size, but it may work poorly for non-structural anomalies and structural anomalies outside that specific size. A more detailed review of these methods can be found in \cite{pang2021deep}.

An important aspect of current UAD approaches is their dependence on pre-trained models to produce SOTA results. UAD models can be pre-trained on ImageNet~\cite{venkataramanan2019attention,bergmann2020uninformed} or self-supervised tasks~\cite{golan2018deep,bergman2020classification}.
To allow a fair comparison with current UAD methods, we pre-train IGD with self-supervision and ImageNet.




Unsupervised anomaly localisation targets the segmentation of anomalous image pixels or patches, containing, for example, lesions in medical images~\cite{Li_2019_CVPR}, defects in industry images~\cite{mvtecad,bergmann2020uninformed}, or road anomalies in traffic images~\cite{pathak2015anomaly,tian2021pixel}.
The main idea explored is based on extending the image based OCC to a pixel-based OCC, where testing produces a pixel-wise anomaly score map~\cite{baur2018deep,ae-ssim}. 
In general, methods that can localise anomalies~\cite{venkataramanan2019attention, bergmann2020uninformed} are tuned to particular anomaly sizes and structure, which can cause then to miss anomalies outside that range of sizes and structure.  
To avoid this issue, we design IGD to detect multi-scale structural and non-structural anomalies to improve the anomaly localisation accuracy.


\begin{figure*}[t!]
\begin{center}
\includegraphics[width=0.9\linewidth]{image/structure_long_v1.pdf}
\end{center}
\caption{
Our IGD consists of an encoder that transforms image  into representation , 
a decoder to reconstruct the image (trained with MS-SSIM and MAE losses), 
a Gaussian anomaly classifier trained to push the normal image representation close to the centre of the estimated normal image distribution (denoted by a Gaussian with mean  and standard deviation ), and a critic module that constrains the likelihood maximisation by predicting the interpolation coefficient  that produces a convex combination of training sample representations. Note that critic is a module similar to a GAN discriminator.
}
\label{fig:train}
\end{figure*}






\section{Method}

We denote the training set containing only normal samples by , where  represents an RGB image of width  and height  and sampled from the distribution of normal images as in .  The testing set contains normal and anomalous images, where anomalous images can have segmentation map annotations.  This testing set is defined by
, where  ( denotes a normal and  denotes an anomalous image), the segmentation map 
with the anomaly is denoted by  (i.e., a pixel-wise anomaly map for image ) if , and  if .

\subsection{Interpolated Gaussian Descriptor (IGD)}
\label{sec:EM}

As depicted in Fig.~\ref{fig:train}, the IGD model is represented by the general classifier  that consists of an encoder  that transforms a training sample from the image space  to a representation space , a Gaussian anomaly classifier  that takes the normal image distribution parameter  and image  to estimate the probability that it is normal, a decoder  that reconstructs an image from the representation space, and a critic module  that predicts the interpolation constraint parameter , with  obtained from the encoder .
The IGD parameter  represents all module parameters  and is estimated with maximum likelihood estimation (MLE):

We train the one-class classifier in~\eqref{eq:main_likelihood} using an
EM optimisation~\citep{dempster1977maximum}, where the mean and standard deviation of the normal image distribution are estimated during the E-step, instead of being explicitly optimised~\cite{dsvdd}, reducing the risk of overfitting.
To encourage the M-step to learn an effective normality description (such that the optimisation is robust to small and contaminated training sets), we add an adversarial interpolation constraint to enforce linear combinations of normal image representations to belong to the normal distribution. 
We further increase the robustness of IGD to overfitting by constraining the optimisation of the M-step to enforce accurate image reconstruction from its representation.
Below, we provide more details about the training process.



To formulate the EM optimisation, we re-write the log-likelihood in~\eqref{eq:main_likelihood} as

with  denoting the latent variables (mean and standard deviation) that describe the distribution of normal image representations (defined in more detail below).
In~\eqref{eq:log_likelihood_p_y_x_theta}, we remove the conditional dependence of  on  and  because
 is a variable for the whole training distribution defined as

where  ( approximates a Dirac delta function, and  approximates a uniform function),  and 
,
with  representing the  encoder; and in~\eqref{eq:log_likelihood_p_y_x_theta}, we also have

 where 
  denotes the Kullback-Leibler divergence, and  represents the variational distribution that approximates , defined in~\eqref{eq:p_omega_given_x}.


The E-step of the EM optimisation zeroes the KL divergence in~\eqref{eq:log_likelihood_p_y_x_theta} by setting 
, where  represents the previous EM iteration parameter value. In practice, the E-step sets  to  and  to , defined in~\eqref{eq:p_omega_given_x}.
Next, the M-step maximises  in~\eqref{eq:elbo}, with:

where  is removed from  because it depends only on the previous iteration parameter ,
 is defined in the E-step above, and 
the conditional dependence of  on  is removed because the information from that distribution is summarised in .  Therefore, \eqref{eq:M_step} has two components: 1) the classification term represented by the Gaussian anomaly classifier , with mean  and standard deviation ; and 2)  defined in~\eqref{eq:p_omega_given_x}, 
which approximates a uniform distribution to prevent the confirmation bias of the estimated  and  from~\eqref{eq:p_omega_given_x}.
To promote an effective normality description of IGD, we constrain the M-step~\eqref{eq:M_step} as follows:

where  is a constraint, defined in~\eqref{eq:loss_critic}, to enforce the adversarial linear interpolation of normal image representations to belong to the normal representation distribution, and  is a constraint, defined in~\eqref{eq:loss_AE}, to enforce accurate structural and non-structural multi-scale image reconstruction.
Note that the maximisation in~\eqref{eq:optimisation_IGD} constrains the optimisation in~\eqref{eq:M_step}, which means that we are maximising a lower bound to the original M-step. Using Lagrange multipliers, the optimisation in~\eqref{eq:optimisation_IGD} is reformulated to minimise the following loss function:

where 
  
with  defined in~\eqref{eq:M_step}, and  denoting the Lagrange multipliers.
The interpolation constrain  in~\eqref{eq:optimisation_IGD} and~\eqref{eq:loss} regularises the  training by linearly interpolating the  representations from training images, and estimating the interpolation coefficient with the critic network~\cite{berthelot2018understanding}.
This interpolation constrains
 the normal image distribution denser in the representation space, reducing the likelihood that anomalous representations may land in the same region of the representation space occupied by normal samples.
Unlike Mix-up~\cite{zhang2017mixup}, our interpolation constraint is a self-supervised method that does not rely on data augmentation on the input space and does not interpolate training labels, making it more adequate for our problem because it enforces a compact and dense distribution of normal samples to be estimated for the Gaussian anomaly classifier. 
The critic network is represented by

where  
represents the reconstruction of the interpolation of  and  (with , ,  , and  denoting a uniform distribution )~\cite{berthelot2018understanding}, and  denotes the decoder. The goal of the critic network  is to predict the interpolation coefficient . 
The critic network in~\eqref{eq:critic} is similar to the discriminator in GAN~\cite{gan}, and relies on the following adversarial loss to be optimised~\cite{berthelot2018understanding}

where 
 is defined in~\eqref{eq:critic}, and
, with 
and  denoting a reconstruction of  by the auto-encoder.
The first term of \eqref{eq:loss_critic} minimises the critic's prediction error for  and the second term regularises the training to ensure that the critic predicts  when the original image is interpolated with its own reconstruction in the image space . 

The image reconstruction constrain  in~\eqref{eq:optimisation_IGD} and~\eqref{eq:loss} is defined as

where  is a reconstruction of  by the auto-encoder, with the image reconstruction loss  to be defined below  in~\eqref{eq:global_reconstruction_loss}, and
 is a hyperparameter to weight the regularisation term. This regularisation fools the critic to output  for interpolated embeddings, independently of , following standard adversarial training~\cite{gan}.
In~\eqref{eq:loss_AE}, we also have

with  denoting the image lattice, ,
 representing the MAE loss, and
 being the MS-SSIM score~\cite{MS-SSIM}, with larger values indicating higher similarity between patches  of the original and reconstructed images. 
Please see details on how to compute the MS-SSIM score in the Supp. Material.




The loss in~\eqref{eq:loss} is used to train two models (see 'Global and Local IGD Models' section in the Supp. Material). A global model that works on the whole image , and a local model that works on image patches , with  and , centred at pixel  ( is the image lattice).
During inference, the results from the global and local models are combined to produce multi-scale anomaly detection and localisation. Please see the Supp. Material for a visual example of the results produced by the global and local models.








\subsection{Theoretical Guarantees}

 IGD maximises a constrained  in \eqref{eq:optimisation_IGD} rather than maximising  in \eqref{eq:main_likelihood}.  
 Using Theorem 1 in~\citep{dempster1977maximum}, Lemma~\ref{thm:correctness} demonstrates the correctness of IGD, where an increase to the constrained  implies an increase to .
Using Theorem 2 in~\citep{dempster1977maximum}, Lemma~\ref{thm:convergence} proves the convergence conditions of IGD. 

\begin{restatable}[]{lemma}{Correctness}
    \label{thm:correctness}
    Assuming that the maximisation of the constrained  in~\eqref{eq:optimisation_IGD} produces  that makes\\ 
    \scalebox{0.87}{
    ,} \\we have that 
    \scalebox{0.87}{} 
    is lower bounded by\\
    \scalebox{0.8}{
    ,} \\
    with .
 \end{restatable}
 
\begin{proof}
Please see proof in Supp. Material.
\end{proof}


\begin{restatable}[]{lemma}{Convergence}
    \label{thm:convergence}
    Assume that  denotes the sequence of trained model parameters from the constrained optimisation of  in~\eqref{eq:optimisation_IGD} such that: 1) the sequence  is bounded above, and 2) 
    \scalebox{0.79}{} \\
    \scalebox{0.8}{
    }, 
    for  and all , and .  
    Then  converges to some .
 \end{restatable}
 
\begin{proof}
Please see proof in Supp. Material.
\end{proof}


\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{image/inference_vv2.png}
   \caption{Example of the multi-scale structural and non-structural anomaly localisation result for an MVTec AD~\cite{mvtecad} image, using both the local and global IGD models. The global model tends to produce smooth results but with some mistakes, while the local model produces jagged results, but without the global mistakes, so by combining the two results, we obtain a smooth and correct anomaly heatmap.  }
    \label{fig:multi-test}
\end{figure}

\subsection{Training and Inference}

The global and local IGD models are trained separately (see Fig.~\ref{fig:multi-test}), following the EM optimisation, where the E-step estimates the the latent variable  in~\eqref{eq:p_omega_given_x}, and the M-step minimises the loss in~\eqref{eq:loss} to obtain .  


During inference, \textbf{anomaly detection} is performed by combining the global and local IGD anomaly scores for a testing image  as in:

The global score in~\eqref{eq:anomaly_score_image} is defined as

where  denotes the reconstruction loss from~\eqref{eq:global_reconstruction_loss} and  denotes the Gaussian anomaly classification loss from~\eqref{eq:loss_GSVDD} (both computed with the global IGD model using the whole images), and  is the reconstruction of  produced by the auto-encoder.  The local score in~\eqref{eq:anomaly_score_image} is defined as

where  and  are the reconstruction and Gaussian anomaly classification losses computed from the local model, with  denoting an image patch of size  at pixel .
The use of max pooling of the local scores in~\eqref{eq:local_score_detection} facilitates detection of images that contain anomalies covering a small region of the image.
\textbf{Anomaly localisation} is computed for each pixel  to produce a local score   

with

where  and  are defined in~\eqref{eq:global_reconstruction_loss} and 
 is a reconstruction of  produced by the global IGD model. The  in~\eqref{eq:localisation_loss^(G)lobal_local_MS-SSIM} is similarly defined using the local IGD model.
Thus, the anomaly localisation final map is a 
heatmap with high values representing regions that are likely to contain anomalies, as displayed in 'Global and Local IGD Models' section in the Supp. Material.







\section{Experiments}
\label{sec:datasets_evaluation}

\subsection{Datasets and Evaluation Metric}



\textbf{Datasets:} We use four computer vision and two medical image datasets to evaluate our methods. The computer vision datasets are MNIST~\cite{lecun2010mnist}, Fashion MNIST~\cite{fmnist}, CIFAR10~\cite{krizhevsky2014cifar} and MVTec AD~\cite{mvtecad}; and the medical image datasets are Hyper-Kvasir~\cite{borgli2020hyperkvasir} and LAG~\cite{li2019attention}. MNIST, Fashion MNIST and CIFAR10 have been widely used as benchmarks for image anomaly detection, and we follow the same experimental protocol as described in~\cite{dsvdd}. 
CIFAR10 contains 60,000 images with 10 classes. MNIST and Fashion MNIST contain 70,000 images with 10 classes of handwritten digits and fashion products, respectively. 
MVTec AD~\cite{mvtecad} contains 5,354 high-resolution real-world images of 15 different industry object and textures. The normal class of MVTec AD is formed by 3,629 training and 467 testing images without defects. The anomalous class has more than 70 categories of defects (such as dents, structural fails, contamination, etc.) and contains 1,258 testing images. MVTec AD provides pixel-wise ground truth annotations for all anomalies in the testing images, allowing the evaluation of anomaly detection and localisation. 
We also tested our method on two publicly available medical datasets: Hyper-Kvasir~\cite{borgli2020hyperkvasir} and LAG~\cite{li2019attention} for polyp and glaucoma detection, respectively. For Hyper-Kvasir, we has 1,600 normal images without polyps in the training set and 500 in the testing set; and 1,000 abnormal images containing polyps in the testing set. For LAG, we have 2,343 normal images without glaucoma in the training set; and 800 normal images and 1,711 abnormal images with glaucoma for testing. 






\textbf{Evaluation:} For \emph{anomaly detection}, we assess performance with the area under the receiver operating characteristic curve (AUC) and classification accuracy. On MNIST, Fashion MNIST and CIFAR10, we use the same protocol as other methods in Tab.~\ref{tab:auc_detection_mnist_cifar10_fmnist}, where training uses a single class as the normal data, with the nine remaining classes denoting as semantically anomalous samples, and inference relies on a non-augmented test image. 
We report the mean AUC over the 10 classes for the above three data sets. On MVTec AD~\cite{bergmann2020uninformed,venkataramanan2019attention}, we evaluate anomaly detection with mean AUC and accuracy. Follow previous works~\cite{tian2021constrained,tian2021selfsupervised}, we evaluate the methods using AUC for the Hyper-Kvasir and LAG. 
For \emph{anomaly localisation}, we follow~\cite{venkataramanan2019attention} and compute the mean pixel-level AUC between the generated heatmap and the ground truth segmentation map for each anomalous image in the testing set of MVTec AD.


\bgroup
\def\arraystretch{1.1}
\begin{table}[t!]
\centering
\resizebox{0.98\linewidth}{!}{\begin{tabular}{@{}c|c|ccc@{}}
\toprule
Pretrain &Method                   &	MNIST	        &	CIFAR10	 & FMNIST        \\		\midrule\midrule
\multirow{16}{*}{Scratch}
&DAE~\cite{hadsell2006dimensionality}                         &	0.8766	        &	0.5358	 &  -       \\	
&VAE~\cite{vae}	                        &	0.9696	        &	0.5833     &  - 	\\
&KDE~\cite{bishop2006pattern}                         & 0.8140       &		0.6100	   &  -     \\
&OCSVM~\cite{oc-svm}	        &	0.9510	        &	0.5860     &  - 	\\	
&AnoGAN~\cite{anogan}	                    &	0.9127	        &	0.6179     &  - 	\\	
&DSVDD~\cite{dsvdd}	                    &	0.9480	        &	0.6481      &  -	\\	
&OCGAN~\cite{ocgan}	                    &	0.9750	        &	0.6566	    &  -    \\
&PixelCNN~\cite{van2016conditional}	                    & 0.6180	  	&	0.5510 	   &  -     \\
&CapsNet\textsubscript{PP}~\cite{li2020exploring}	                    &0.9770	   	&0.6120		 &  0.7650       \\
&CapsNet\textsubscript{RE}~\cite{li2020exploring}	                    &0.9250	   	& 0.5310		&  0.6790        \\
&ADGAN~\cite{ADGAN}	                    & 0.9680	   	& 0.6340	&  -	        \\
&LSA~\cite{lsa}	                    & 0.9750	   	& 0.6410		 &  0.8760       \\
&MemAE~\cite{gong2019memorizing}	                    &	0.9751      	&	0.6088	   &  -       \\	
&GradCon~\cite{gradcon}	                    & 0.9730	   	&	0.6640	    &  -      \\
&-VAE\textsubscript{u}~\cite{lamda-vae}	                    & 0.9820	   	& 0.7170	    &  0.8730     \\
&ULSLM~\cite{ulslm}	                    & 0.9490	   	& 0.7360		 &  -         \\ 
&SCADN~\cite{yan2021learning}       & 0.9771    & 0.6690 & -    \\
&\textbf{Ours}	            & \textbf{0.9869}		&	\textbf{0.7433}	 & \textbf{0.9201}
\\\hline
\multirow{3}{*}{ImageNet} &CAVGA-D\textsubscript{u}~\cite{venkataramanan2019attention} 	&	0.9860        &	0.7370	    &  0.8850      \\	
&Student-Teacher~\cite{bergmann2020uninformed}	        &	\textbf{0.9935}	        &	0.8196	  &  -      \\
&\textbf{Ours}	            &	\textbf{0.9927}	&	\textbf{0.8368}	 & \textbf{0.9357} 
\\\hline
\multirow{3}{*}{SSL}
&Rot-Net~\cite{golan2018deep}	        &	-        &	0.8160	  &  0.9350     \\	
 &\citet{bergman2020classification}	        &	-        &	0.8820	  &  0.9410     \\
&\textbf{Ours}	            &	-	& 	\textbf{0.9125}	 & \textbf{0.9441}\\ 

\bottomrule
\end{tabular}}
\caption{\textbf{Anomaly detection:} mean AUC testing results on MNIST, CIFAR10 and Fashion MNIST. The results are split into 'Scratch' (without any pre-training), pretrained with 'ImageNet', and self-supervised learning ('SSL'). Bold numbers represent the best result (within 0.5\%) for each data set, discriminated by Scratch, SSL or ImageNet.}
\label{tab:auc_detection_mnist_cifar10_fmnist}
\end{table}

\subsection{Implementation Details}

We implement our framework using Pytorch. The model was trained with Adam optimiser using a learning rate of 0.0001, weight decay of , batch size of 64 images, 256 epochs for all dataset. We defined the representation space produced by the encoder to have  dimensions.
Following~\cite{depthestimation2017}, we set  to balance the contribution of MAE and MS-SSIM losses in~\eqref{eq:global_reconstruction_loss} and~\eqref{eq:localisation_map}. 
We set  in~\eqref{eq:loss} and   in \eqref{eq:loss_AE}, based on cross validation experiments. We use Resnet18 and its reverse architecture as the encoder and decoder for both the global and local IGD models. 
When computing the accuracy of anomaly detection in MVTec AD, the threshold of the anomaly detection score  in~\eqref{eq:anomaly_score_image} (to classify an image as anomalous) is set to 0.5~\cite{venkataramanan2019attention}.
To enable a fair comparison between our method and previous approaches in the field~\cite{bergmann2020uninformed,venkataramanan2019attention,bergman2020classification,golan2018deep}, we pre-train the encoders for the global and local IGD models either with self-supervised learning (SSL)~\cite{chen2020simple}
or ImageNet knowledge distillation (KD)~\cite{bergmann2020uninformed,gou2020knowledge}. 
For this SSL pre-training, we use the SGD optimiser with a learning rate of 0.01,  weight decay , batch size of 32, and 2,000 epochs. Once we obtain the pre-trained encoder with SSL, we remove the MLP layer and attach a linear layer to the backbone with fixed parameters. Note that this SSL is trained from scratch. 
In contrast to the vanilla self-supervised learning~\cite{chen2020simple} suggesting large batch size, we notice that a medium batch size yields significantly better performance for unsupervised anomaly detection. 


For the ImageNet KD pre-training, we minimise the  norm between the 512-dimensional feature vector output from encoder and an intermediate layer of the ImageNet pre-trained ResNet18 with the same 512-dimensional features.
For this ImageNet KD pre-training, we use the Adam optimiser with a learning rate of 0.0001,  weight decay , batch size of 64, and 50,000 iterations. Once we obtain the pre-trained encoder of KD, we fix the network parameters 
and attach a linear layer to reduce the dimensionality of the feature space to 128.






















\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{image/heatmap_hori.png}
    \caption{Qualitative results of our anomaly localisation results on the MVTec AD (red = high probability of anomaly). Top, middle and bottom rows show the testing images, ground-truth masks and predicted heatmaps, respectively. Please see additional results in the Supp. Material.} 
    \label{fig:mvtec_seg}
\end{figure}

\subsection{Experiments on MNIST, Fashion MNIST and CIFAR10}
Table~\ref{tab:auc_detection_mnist_cifar10_fmnist} compares the unsupervised anomaly detection mean AUC testing results between our method and the current SOTA on MNIST, Fashion MNIST and CIFAR10.
The rows labelled as `Scratch' show results of models that were not pre-trained, and the ones with `SSL' display results from models using self-supervised learning method~\cite{golan2018deep,bergman2020classification}. The ones with `ImageNet' show results from models that use ImageNet KD pre-training~\cite{venkataramanan2019attention,bergmann2020uninformed}. 
Our proposed IGD 
outperforms current SOTA methods for the majority of pre-training methods on all three datasets.
Please see additional results in the Supp. material.









\subsection{Experiments on MVTec AD}

We report the results, based on SSL and ImageNet KD pre-trained models, for both anomaly detection (Tab.~\ref{tab:auc_detection_mvtec_short}) and localisation (Tab.~\ref{tab:MVTec-localisation-AUC}) on MVTec AD, which contains real-world images of industry objects and textures containing different types of anomalies. Following~\cite{venkataramanan2019attention} the score threshold is set to 0.5 for calculating the mean accuracy of anomaly detection.
For anomaly detection, our method produces the best accuracy (at least 2\% better than previous SOTA) and AUC (at least 5\% better than previous SOTA) results independently of the pre-training technique.
For anomaly localisation, we compare our method and the SOTA using the mean pixel-level AUC of all anomalous images in the testing set of MVTec AD. Notice that our method with ImageNet and SSL pre-training are better than the previous SOTA CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention} by 2\% and 4\%, respectively.
Fig.~\ref{fig:mvtec_seg} shows anomaly localisation results on MVTec AD images, where red regions in the heatmap indicate higher anomaly probability. From this results, we can see that our approach can localise anomalous regions of different sizes and structures from different object categories.
Please see additional results in the Supp. material.


\bgroup
\def\arraystretch{1.1}
\begin{table}[t]
\centering
\resizebox{0.92\linewidth}{!}{\begin{tabular}{@{}c|cc@{}}
\toprule 
Metric   & Method & Mean  \\ \hline \hline
       \multirow{10}{*}{Accuracy}   & AVID~\cite{sabokrou2018adversarially}              
         & 0.730  \\
         & AE\textsubscript{SSIM}~\cite{ae-ssim}   
          & 0.630  \\
         & DAE~\cite{hadsell2006dimensionality}   & 0.710  \\
         & AnoGAN~\cite{anogan} & 0.550  \\ 
         & -VAE\textsubscript{u}~\cite{lamda-vae}  & 0.770  \\
         & LSA~\cite{lsa}  & 0.730  \\
         & CAVGA-D\textsubscript{u}~\cite{venkataramanan2019attention}  & 0.780  \\
         & CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention}  & 0.820  \\ 
         & \textbf{Ours - ImageNet} & \textbf{0.840}  \\
         & \textbf{Ours - SSL} & \textbf{0.850}  \\ \midrule
 \multirow{7}{*}{AUC}        & AnoGAN~\cite{anogan} & 0.503 \\
         & GANomaly~\cite{akcay2018ganomaly}& 0.782 \\
         & Skip-GANomaly~\cite{akccay2019skip}   & 0.805 \\
         & SCADN~\cite{yan2021learning} & 0.818 \\
     & U-Net~\cite{u-net}   & 0.819 \\
         & DAGAN~\cite{DAGAN}  & 0.873 \\
         & \textbf{Ours - ImageNet} & {\textbf{0.926}} \\
         & \textbf{Ours - SSL}  & {\textbf{0.934}} \\ \bottomrule
\end{tabular}}
\caption{
\textbf{Anomaly detection}: mean testing accuracy and AUC on MVTec AD produced by the SOTA and our IGD.}
\label{tab:auc_detection_mvtec_short}
\end{table}

\bgroup
\def\arraystretch{1.1}
\begin{table}[htp]
\centering
\small
\resizebox{0.82\linewidth}{!}{\begin{tabular}{@{}cc@{}}
\toprule
Method	                    &	MVTec AD      	\\	\midrule\midrule    DAE~\cite{hadsell2006dimensionality}	    &	0.82        	\\	
AE\textsubscript{SSIM}~\cite{ae-ssim}	    &	0.87	        \\	
AVID~\cite{sabokrou2018adversarially}	    & 0.78		        \\
SCADN~\cite{yan2021learning}                & 0.75  \\
LSA~\cite{lsa}	    &	0.79	        \\ 
-VAE\textsubscript{u}~\cite{lamda-vae}	    &	0.86	        \\
AnoGAN~\cite{anogan}	                    &	0.74	        \\	ADVAE~\cite{ADVAE}	                    &	0.86	        \\
CAVGA-D\textsubscript{u}~\cite{venkataramanan2019attention}	&	0.85	        \\	
CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention}	&	0.89	        \\	
\textbf{Ours - ImageNet}	            &	{\textbf{0.91}}	\\
\textbf{Ours - SSL} & {\textbf{0.93}}	\\\bottomrule
\end{tabular}}
\caption{\textbf{Anomaly localisation:} mean pixel-level AUC testing results on the anomalous images of MVTec AD. }
\label{tab:MVTec-localisation-AUC}
\end{table}
























\subsection{Experiments on Medical Datasets}

To show that our method can generalise to other domains, we evaluate our approach on two public medical datasets - Hyper-Kvasir for polyp detection and LAG for glaucoma detection. As shown in Tab.~\ref{tab:medical_auc}, our
SSL and ImageNet based results achieve the best AUC results on both datasets. Our methods surpass the recent proposed CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention} on both datasets by a minimum 0.9\% and maximum 3.8\%. Also, our model performs better compared to the anomaly detector specifically designed for medical data, such as f-anogan~\cite{f-anogan} and ADGAN~\cite{liu2019photoshopping}.
We show qualitative polyp segmentation results in the Supp. Material.
The abnormalities in medical data (i.e., colon polyps, glaucoma) are significantly different than the popular image benchmarks and MVTec AD in terms of appearance and structural anomalies, suggesting that our model works in disparate domains.
Please see more results in the Supp. material.


\bgroup
\def\arraystretch{1.1}
\begin{table}
\centering
\small
\resizebox{0.92\linewidth}{!}{\begin{tabular}{@{}ccc@{}}
\toprule 
Methods         & Hyper-Kvasir  & LAG  \\ \hline\hline
DAE~\cite{masci2011stacked}             & 0.705      & 0.651   \\
CAM~\cite{cam}                &           -      &   0.663      \\
GBP~\cite{springenberg2014striving}                &   -             & 0.787         \\
SmoothGrad~\cite{smilkov2017smoothgrad}                &  -               & 0.795  \\
OCGAN~\cite{perera2019ocgan}           & 0.813     &  0.737 \\
F-anoGAN~\cite{f-anogan}        & 0.907       & 0.778  \\
ADGAN~\cite{liu2019photoshopping}           & 0.913      &   0.752    \\
CAVGA-~\cite{venkataramanan2019attention}     & 0.928    & 0.819       \\
\textbf{Ours - ImageNet}      & {\textbf{0.931}}        & {\textbf{0.838}}   \\
\textbf{Ours - SSL}    & {\textbf{0.937}}        & {\textbf{0.857}}   \\\bottomrule
\end{tabular}
}
\caption{\textbf{Anomaly detection:} AUC testing results on two medical datasets: Hyper-Kvasir and LAG.}
\label{tab:medical_auc}
\end{table}



\subsection{Ablation Study}
To investigate the effectiveness of each component of our method, we show the mean AUC results of our method with different proposed variants in Tab.~\ref{tab:ablation}. Note that all results are based on the initialisation of knowledge distillation from ImageNet. For standard anomaly detection settings (AUC - Full), each proposed component of our IGD improves performance by a minimum 1.7\% and maximum 11.6\% mean AUC.  Tab.~\ref{tab:ablation} also shows the effectiveness of each component when trained with small (20\% of full training data) or anomaly contaminated (10\% of contamination rate) training sets, where our proposed Gaussian anomaly classifier (GAC) significantly improves over the REC (i.e., MS-SSIM+MAE losses) baseline by 13\% and 10.4\% mean AUC. The proposed adversarial interpolation regularisation (INTER) further improves the AUC by 3.7\% and 3.1\%. 







\begin{table}[t]
\centering
\scalebox{0.7}{
\begin{tabular}{cccc|ccc}
\toprule MSE	&	REC 	&	GAC	&	INTER	        &	AUC - Full  & AUC - ST &	AUC - AC	\\ \midrule\midrule \checkmark	&		&	&		                    &   0.615       & 0.552    &	0.565       \\
	&	\checkmark &			&		            &	0.731       & 0.655    &	0.677       \\
&	\checkmark	& 		\checkmark	&		        &	0.819       & 0.785    &	0.781       \\
	&	\checkmark	& 	\checkmark	&	\checkmark	&	0.836       & 0.822	   &    0.812       \\ \hline\bottomrule
\end{tabular}}
\caption{Ablation study of our method on CIFAR10 using anomaly detection mean testing AUC w.r.t standard OCC setup (AUC - Full), small training set containing 20\% of training data (AUC - ST), and anomaly contaminated training set with 10\% contamination (i.e., 10\% of the anomalous samples are removed from the testing set and inserted into the training set) (AUC - AC).
MSE denotes the baseline deep autoencoder with MSE loss, REC denotes the baseline deep autoencoder with MS-SSIM + MAE losses, GAC denotes our proposed Gaussian anomaly classifier, INTER represents our interpolation regularisation. The encoder of all above methods are initialised based on the knowledge distillation from ImageNet.  } 
\label{tab:ablation}
\end{table}


\subsection{Experiments on Small/Contaminated Training Sets}

\begin{table}[t]
\centering
\begin{center}
\resizebox{0.9\linewidth}{!}{\centering{\begin{tabular}{@{}c| cccc @{}}
\toprule
Dataset & Train Size &	DSVDD	& DSVDD+REC	&\textbf{IGD (Ours)}	\\ \hline	 \hline			
\multirow{3}{*}{CIFAR10}
& 20\%	&	0.7064	&	0.7462	&	0.8219	\\		
& 60\%	&	0.7367	&	0.7807	&	0.8298	\\ 					
& 100\%	&	0.7612	&	0.7950	&	0.8365	\\ 
\midrule 
\multirow{3}{*}{MVTec}
& 20\%	&	0.7994  &	0.7291  &	0.9043  \\
& 60\%	&	0.8467  &	0.7737  &	0.9246  \\
& 100\% &	0.8579  &	0.7826  &	0.9260  \\ 
\bottomrule \bottomrule
\end{tabular}}}
\end{center}
\caption{Mean testing AUCs on CIFAR10 and MVTec with small training sets, where REC=MS-SSIM+MAE losses.}
\label{tab:sample_efficiency}
\end{table}

To show the improved robustness of our approach to small training sets on CIFAR10 and MVTec, we compare the performance of DSVDD, DSVDD+REC (i.e., DSVDD combined with our reconstruction loss), and our proposed IGD, using less normal data in the training sets in Tab.~\ref{tab:sample_efficiency}.  In particular, we randomly sub-sample 20\%, 60\%, and 100\% of the original training sets of CIFAR10 and MVTec AD, to form a smaller training set. The results indicate that IGD achieves comparable performance under significantly less training data, while the performance of DSVDD and DSVDD+REC deteriorate dramatically when the number of training samples decreases.  
This result shows that IGD has better robustness than DSVDD and DSVDD+REC to small training sets. 

\begin{table}[t]
\centering
\begin{center}
\resizebox{0.9\linewidth}{!}{\centering{\begin{tabular}{@{}c| cccc @{}}
\toprule
Dataset  & Noise Ratio	&	DSVDD	& DSVDD+REC	& \textbf{IGD (Ours)}	\\ \hline	 \hline	
\multirow{3}{*}{CIFAR10}
& 1\%	 &	0.7502	&	0.7694	&	0.8252	\\												
& 5\%	 &	0.7124	&	0.7448	&	0.8193	\\												
& 10\%	 &	0.6717	&	0.7073	&	0.8122	\\ 
\midrule 
\multirow{3}{*}{MVTec}
& 1\%	 &	0.8523	&	0.7873	&   0.9363	\\
& 5\%	 &	0.8391	&	0.7733 	&	0.9319	\\			
& 10\%   &	0.8175	&	0.7687	&	0.9363	\\ 
\bottomrule \bottomrule
\end{tabular}}}
\end{center}
\caption{Mean testing AUCs on CIFAR10 and MVTec with different contamination noise rates. REC defined in Tab.~\ref{tab:sample_efficiency}.}
\label{tab:noise_conta}
\end{table}

To show the improved robustness of our approach contaminated training sets, in Tab.~\ref{tab:noise_conta},  we compare the performance of DSVDD, DSVDD+REC, and our IGD, using training sets corrupted with anomalous samples (this contamination facilitates overfitting).
In particular, we re-organise the original training and test data of CIFAR10 and MVTec AD by randomly sampling 1\%, 5\% and 10\% of anomalies from the test data to inject into the training data. With different rates of anomaly contamination, the maximum fluctuation of our IGD is 1.3\% on CIFAR10 and 0.44\% on MVTec AD. While the competing method DSVDD shows a much larger maximum fluctuation of 7.8\% and 3.5\% mean AUC, on CIFAR10 and MVTec AD, respectively.   The results show the substantially better robustness of IGD over DSVDD and DSVDD+REC for the anomaly-contaminated training data. 

\section{Discussion}
We do not compare some of the SOTA works~\cite{reiss2021panda,sohn2020learning,tack2020csi} in Table \ref{tab:auc_detection_mnist_cifar10_fmnist}, \ref{tab:auc_detection_mvtec_short}, and \ref{tab:MVTec-localisation-AUC} due to unfair comparison. 
In particular, the comparison with PANDA~\cite{reiss2021panda} is not fair because it uses a WideResNet50  2 for MVTec and ResNet152 for CIFAR, both being much larger backbones than our ResNet18. 
Regarding CSI~\cite{tack2020csi}, it has much slower inference (because of the  data augmentation of test images) and more complex training that needs a coreset and large batch size of 512 for pre-training, which challenges its use for problems with small training sets or high-resolution images.
For both CSI and DROC~\cite{sohn2020learning}, their gains are mostly from the SSL pre-training. To show that point for CSI, we use our training approach to fine-tune a pre-trained CSI model and obtain 94.6\% AUC on CIFAR10, which is higher than CSI (94.3\% AUC).
Also, for the vanilla SSL pre-training reported in DROC paper, their performance reduces from 92.5\% to 89.0\% AUC on CIFAR10, and from 86.5\% to 80.2\% AUC on MVTec. 
Note that all above results are collected from their published papers unless stated otherwise. 

Furthermore, on MVTec, our approach obtains (93.4\% AUC), which is much better than CSI (63.6\% AUC from Tab.2 of~\cite{reiss2021mean}) and PANDA (86.5\%).
For anomaly localisation on MVTec, our 93\% AUC is better than DROC (90\%) and worse than PANDA (96\%).
On high-resolution image datasets (e.g., Hyper-Kvasir), our approach (93.7\% AUC) is better than CSI (trained by us) that reaches 91.6\% AUC. 
Other important results shown by our paper, but missed by CSI, PANDA and DROC, are the ones with small training sets and contaminated training sets, which are new and important benchmarks for real-world industrial applications and early detection of medical diseases.























\section{Conclusion}



In this paper, we presented a new
OCC model, called interpolated Gaussian descriptor (IGD), to perform unsupervised anomaly detection and segmentation.
IGD learns a one-class Gaussian anomaly classifier trained with adversarially interpolated training samples to enable an effective normality description based on representative normal samples rather than fringe or anomalous samples.
The optimisation of IGD is formulated as an EM algorithm, which we show to be theoretically correct and to converge to a stationary solution under certain conditions.
To our knowledge, IGD is the first method that is able to achieve the best performance across diverse application datasets, including MNIST, CIFAR10, Fashion MNIST, MVTec AD, and two large scale medical datasets, in terms of anomaly detection and localisation. 
We also show that IGD is more robust than DSVDD and an image-reconstruction contrained DSVDD in problems with small or contaminated training sets.
We plan to study the use of Gaussian anomaly classifier in the pixel-wise localisation of anomalies and to investigate new self-supervised learning approaches specifically designed for anomaly detection.

\clearpage
\small{\bibliography{aaai22.bib}}

\clearpage
\newpage

\beginsupplement
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\newpage

\section{Datasets}
CIFAR10 contains 60,000 images with 10 classes. MNIST and Fashion MNIST contain 70,000 images with 10 classes of handwritten digits and fashion products, respectively. 
MVTec AD~\cite{mvtecad} contains 5,354 high-resolution real-world images of 15 different industry object and textures. The normal class of MVTec AD is formed by 3,629 training and 467 testing images without defects. The anomalous class has more than 70 categories of defects
(such as dents, structural fails, contamination, etc.) and contains 1,258 testing images. MVTec AD provides pixel-wise ground truth annotations for all anomalies in the testing images, allowing the evaluation of anomaly detection and localisation. 
Hyper-Kvasir has 1,600 normal images without polyps in the training set and 500 in the testing set; and 1,000 abnormal images containing polyps in the testing set. For LAG, we have 2,343 normal images without glaucoma in the training set; and 800 normal images and 1,711 abnormal images with glaucoma for testing. 



\section{Global and Local IGD Models}

Figure~\ref{fig:multi-test} shows an example of a multi-scale structural and non-structural anomaly localisation result for an MVTec AD image, using both the local and global IGD models.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{image/inference_vv2.png}
   \caption{Example of the multi-scale structural and non-structural anomaly localisation result for an MVTec AD~\cite{mvtecad} image, using both the local and global IGD models. The global model tends to produce smooth results but with some mistakes, while the local model produces jagged results, but without the global mistakes, so by combining the two results, we obtain a smooth and correct anomaly heatmap.  }
    \label{fig:multi-test}
\end{figure}

\section{Multi-scale Structure Similarity Index (MS-SSIM) Score}
The MS-SSIM loss uses the MS-SSIM global score, defined as 

where  denotes an image patch centred at  of size ,


 
 with  representing pre-defined constants,  denoting the mean intensities of , 
the variance of , and
 the covariance of  and . 
In~\eqref{eq:MS-SSIM}, 
 denotes the number of scales,  , , , , ~\cite{MS-SSIM}. We follow  (for ) according to~\cite{SSIM} and define  as the pixel range with ,  and .


The local score  is defined in the same way as in~\eqref{eq:MS-SSIM}, where  is an image patch centred at  of size ,
 scales with weights , , ,  modified based on the original proportion for . 


\section{Implementation Details}






For this SSL pre-training, we use the SGD optimiser with a learning rate of 0.01,  weight decay , batch size of 32, and 2,000 epochs. Once we obtain the pre-trained encoder with SSL, we remove the MLP layer and attach a linear layer to the backbone with fixed parameters. Note that this SSL is trained from scratch. 
In contrast to the vanilla self-supervised learning~\cite{chen2020simple} suggesting large batch size, we notice that a medium batch size yields significantly better performance for unsupervised anomaly detection. 


For the ImageNet KD pre-training, we minimise the  norm between the 512-dimensional feature vector output from encoder and an intermediate layer of the ImageNet pre-trained ResNet18~\cite{resnet} with the same 512-dimensional features.
For this ImageNet KD pre-training, we use the Adam optimiser with a learning rate of 0.0001,  weight decay , batch size of 64, and 50,000 iterations. Once we obtain the pre-trained encoder of KD, we fix the network parameters 
and attach a linear layer to reduce the dimensionality of the feature space to 128.



\section{Visualisation of the Distribution of Testing Samples}

Figure~\ref{fig:mvtec_tsne} shows the distribution of testing samples in the representation space, using the t-SNE visualisation, for DSVDD~\cite{dsvdd}, Gaussian anomaly classifier (GAC), and our IGD. Notice that the normal samples seem to be more compactly represented with fewer anomalous samples appearing inside the normal cluster. This suggests that IGD has a superior normality description, compared with DSVDD and GAC.

\begin{figure}[H]
  \centering
\includegraphics[width=0.32\linewidth]{tsne/tsne_dsvdd_bottle34.pdf}
    \includegraphics[width=0.32\linewidth]{tsne/tsne_GAC_bottle_aaai.pdf}
    \includegraphics[width=0.32\linewidth]{tsne/tsne_ours_bottle_big.pdf}
    \includegraphics[width=0.5\linewidth]{tsne/iccv_legend.pdf}
    \caption{
    t-sne visualisation from MVTec (class bottle). 
    } 
    \label{fig:mvtec_tsne}
\end{figure}
\vspace{-.33cm}


\bgroup
\def\arraystretch{1.4}
\begin{table*}[h]
\centering
\resizebox{1\linewidth}{!}{\begin{tabular}{@{}cccccccccccccccccc@{}}
\toprule 
Metric   & Method                                                     & Bottle & Hazelnut & Capsule & Metal Nut & Leather & Pill  & Wood  & Carpet & Tile  & Grid  & Cable & Transistor & Toothbrush & Screw & Zipper & Mean  \\ \hline \hline
       \multirow{10}{*}{Accuracy}   & AVID~\cite{sabokrou2018adversarially}                      
         & 0.85   & 0.86     & 0.85    & 0.63      & 0.58    & 0.86  & 0.83  & 0.70    & 0.66  & 0.59  & 0.64  & 0.58       & 0.73       & 0.66  & 0.84   & 0.73  \\
         & AE\textsubscript{SSIM~\cite{ae-ssim}}                       
         & 0.88   & 0.54     & 0.61    & 0.54      & 0.46    & 0.60   & 0.83  & 0.67   & 0.52  & 0.69  & 0.61  & 0.52       & 0.74       & 0.51  & 0.80    & 0.63  \\
         & DAE~\cite{hadsell2006dimensionality}                       
         & 0.80    & 0.88     & 0.62    & 0.73      & 0.44    & 0.62  & 0.74  & 0.50    & 0.77  & 0.78  & 0.56  & 0.71       & \textbf{0.98}       & 0.69  & 0.80    & 0.71  \\
         & AnoGAN~\cite{anogan}                                       
         & 0.69   & 0.50      & 0.58    & 0.50       & 0.52    & 0.62  & 0.68  & 0.49   & 0.51  & 0.51  & 0.53  & 0.67       & 0.57       & 0.35  & 0.59   & 0.55  \\ 
         & -VAE\textsubscript{u~\cite{lamda-vae}}             
         & 0.86   & 0.74     & \textbf{0.86}    & \textbf{0.78}      & 0.71    & 0.80   & 0.89  & 0.67   & 0.81  & 0.83  & 0.56  & 0.70        & 0.89       & 0.71  & 0.67   & 0.77  \\
         & LSA~\cite{lsa}                                             
         & 0.86   & 0.80      & 0.71    & 0.67      & 0.70     & 0.85  & 0.75  & \textbf{0.74}   & 0.70   & 0.54  & 0.61  & 0.50        & 0.89       & 0.75  & \textbf{0.88}   & 0.73  \\
         & CAVGA-D\textsubscript{u~\cite{venkataramanan2019attention}} 
         & 0.89   & 0.84     & 0.83    & 0.67      & 0.71    & \textbf{0.88}  & 0.85  & 0.73   & 0.70   & 0.75  & 0.63  & 0.73       & 0.91       & \textbf{0.77}  & 0.87   & 0.78  \\
         & CAVGA-R\textsubscript{u~\cite{venkataramanan2019attention}} 
         & \textbf{0.91}   & \textbf{0.87}     & \textbf{0.87}    & 0.71      & 0.75    & \textbf{0.91}  & 0.88  & \textbf{0.78}   & 0.72  & 0.78  & 0.67  & 0.75       & \textbf{0.97}       & \textbf{0.78}  & \textbf{0.94}   & 0.82  \\ 
         & \textbf{Ours - ImageNet}                                                  
         & {\textbf{0.95}}   & \textbf{0.93}     & 0.80     & \textbf{0.82}      & \textbf{0.87}    & 0.77  & \textbf{0.94}  & 0.69   & \textbf{0.90}   & \textbf{0.92}  & \textbf{0.73}  & \textbf{0.88}       & \textbf{0.98}       & 0.58  & 0.85   & \textbf{0.84}  \\
         & \textbf{Ours - SSL}                                                 
         & {\textbf{0.95}}   & \textbf{0.93}     & 0.81    & \textbf{0.82}      & \textbf{0.90}     & 0.74  & \textbf{0.89}  & 0.71   & \textbf{0.94}  & \textbf{0.90}   & \textbf{0.79}  & \textbf{0.85}       & \textbf{0.98}       & 0.67  & \textbf{0.88}   & \textbf{0.85}  \\ \midrule
 \multirow{7}{*}{AUC}        & AnoGAN~\cite{anogan}                                       
         & 0.800    & 0.259    & 0.442   & 0.284     & 0.451   & 0.711 & 0.567 & 0.337  & 0.401 & 0.871 & 0.477 & 0.692      & 0.439      & 0.100   & 0.715  & 0.503 \\
         & GANomaly~\cite{akcay2018ganomaly}                          
         & 0.794  & 0.874    & 0.721   & 0.694     & 0.808   & 0.671 & 0.920  & 0.821  & 0.720  & 0.743 & 0.711 & 0.808      & 0.700        & {\textbf{1.000}}     & 0.744  & 0.782 \\
         & Skip-GANomaly~\cite{akccay2019skip}                        & 0.937  & 0.906    & 0.718   & 0.790      & 0.908   & 0.758 & 0.919 & 0.795  & 0.850  & 0.657 & 0.674 & 0.814      & 0.689      & {\textbf{1.000}}    & 0.663  & 0.805 \\
     & U-Net~\cite{u-net}                                         & 0.863  & 0.996    & 0.673   & 0.676     & 0.870    & 0.781 & 0.958 & 0.774  & 0.964 & 0.857 & 0.636 & 0.674      & 0.811      & {\textbf{1.000}}     & 0.750   & 0.819 \\
         & DAGAN~\cite{DAGAN}                                         & {\textbf{0.983}}  & {\textbf{1.000}}        & 0.687   & 0.815     & {\textbf{0.944}}   & 0.768 & {\textbf{0.979}} & {\textbf{0.903}}  & 0.961 & 0.867 & 0.665 & 0.794      & {\textbf{0.950}}       & {\textbf{1.000}}     & 0.781  & 0.873 \\
         & SCADN~\cite{yan2021learning} & 0.957 & 0.856 & 0.765 & 0.504 & 0.983 & 0.833 & 0.659 & 0.624 & 0.814 & 0.831 & 0.792 & 0.981 & 0.863 & 0.968 & 0.846 & 0.818\\
         & \textbf{Ours - ImageNet}                                                  & {\textbf{1.000}}      & 0.986    & {\textbf{0.907}}   & {\textbf{0.886}}     & 0.922   & {\textbf{0.870}}  & {\textbf{0.982}} & {\textbf{0.828}}  & {\textbf{0.979}} & {\textbf{0.979}} & {\textbf{0.856}} & {\textbf{0.909}}     & {\textbf{0.997}}      & 0.815 & {\textbf{0.969 }} & {\textbf{0.926}} \\
         & \textbf{Ours - SSL}                                                 & {\textbf{1.000}}     & {\textbf{0.997}}    & {\textbf{0.915}}   & {\textbf{0.913}}     & {\textbf{0.958}}   & {\textbf{0.873}} & 0.946 & {\textbf{0.828 }} & {\textbf{0.991}} & {\textbf{0.978}} & {\textbf{0.906}} &{\textbf{0.906 }}     & {\textbf{0.997}}     & {\textbf{0.825}} & {\textbf{0.970 }}  & {\textbf{0.934}} \\ \bottomrule
\end{tabular}}
\caption{
\textbf{Anomaly detection}: mean testing accuracy and AUC on MVTec AD produced by the SOTA and our method.
}
\label{tab:auc_class_mvtec}
\end{table*}










\section{Correctness Proof}
\label{sec:correctness_analysis_proof}

\begin{restatable}[]{lemma}{Correctness}
    \label{thm:correctness}
    Assuming that the maximisation of the constrained  produces  that makes\\ 
    \scalebox{0.87}{
    ,} \\we have that 
    \scalebox{0.87}{} 
    is lower bounded by\\
    \scalebox{0.8}{
    ,} \\
    with .
 \end{restatable}

\begin{proof}
We follow the proof for Theorem 1 in~\citep{dempster1977maximum}. From the main paper, we have

where . Subtracting 
 and , we have

Since  and that
 \\
\scalebox{0.88}{
        ,} 
we conclude that 

because of the assumption in this Lemma.
\end{proof}



\section{Convergence Conditions Proof}
\label{sec:convergence_analysis_proof}


\begin{restatable}[]{lemma}{Convergence}
    \label{thm:convergence}
    Assume that  denotes the sequence of trained model parameters from the constrained optimisation of  such that: 1) the sequence  is bounded above, and 2) 
    \scalebox{0.79}{} \\
    \scalebox{0.8}{
    }, 
    for  and all , and .  
    Then  converges to some .
 \end{restatable}
\begin{proof}
We follow the proof for Theorem 2 in~\citep{dempster1977maximum}.
The sequence  is non-decreasing (from Lemma~\ref{thm:correctness}) and bounded  above (from assumption (1) in Lemma~\ref{thm:convergence}), so it converges to . 
Hence, using Cauchy criterion~\citep{nguyen2020tutorial}, for any , we have  such that, for  and all ,

From~\eqref{eq:difference_bound},

for  and . Hence, from \eqref{eq:bound_1_thm2},

for  and all .
Given assumption (2) in Lemma~\ref{thm:convergence} for , we
 have from \eqref{eq:bound_3_thm2},
 
so

 which is a requirement to prove the convergence of  to some .
\end{proof}


\bgroup
\def\arraystretch{1.4}
\begin{table*}[t!]
\centering
\resizebox{0.95\linewidth}{!}{\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
        &Method                                        &	0   &	1	    &	2	    &	3	    &	4	    &	5	    &	6	    &	7	    &	8	    &	9	    &	Mean	\\	\midrule\midrule
        &DAE~\cite{hadsell2006dimensionality} &	0.894	        &	0.999	&	0.792	&	0.851	&	0.888	&	0.819	&	0.944	&	0.922	&	0.740	&	0.917	&	0.8766	\\	\cmidrule{2-13}
        &VAE~\cite{vae}  &	0.997	        &	0.999	&	0.936	&	0.959	&	0.973	&	0.964	&	0.993	&	0.976	&	0.923	&	0.976	&	0.9696	\\	\cmidrule{2-13}
        &KDE~\cite{bishop2006pattern}&	0.885	        &	0.996	&	0.710	&	0.693	&	0.844	&	0.776	&	0.861	&	0.884	&	0.669	&	0.825	&	0.8140	\\	\cmidrule{2-13}
        &OCSVM~\cite{oc-svm}	                        &	0.988	        &	0.999	&	0.902	&	0.950	&	0.955	&	0.968	&	0.978	&	0.965	&	0.853	&	0.955	&	0.9510	\\	\cmidrule{2-13}
        &AnoGAN~\cite{anogan}	                        &	0.966	        &	0.992	&	0.850	&	0.887	&	0.894	&	0.883	&	0.947	&	0.935	&	0.849	&	0.924	&	0.9127	\\	\cmidrule{2-13}
        &DSVDD~\cite{dsvdd}                        &	0.980	        &	0.997	&	0.917	&	0.919	&	0.949	&	0.885	&	0.983	&	0.946	&	0.939	&	0.965	&	0.9480	\\	\cmidrule{2-13}
      &OCGAN~\cite{ocgan}	                        &	0.998	        &	0.999	&	0.942	&	0.963	&	0.975	&	0.980	&	0.991	&	0.981	&	0.939	&	0.981	&	0.9750	\\	\cmidrule{2-13}
        &PixelCNN~\cite{van2016conditional}	                    &	0.531	        &	0.995	&	0.476	&	0.517	&	0.739	&	0.542	&	0.592	&	0.789	&	0.340	&	0.662	&	0.6180	\\	\cmidrule{2-13}
        &CapsNet\textsubscript{PP}~\cite{li2020exploring}	    &	0.998	        &	0.990	&	0.984	&	0.976	&	0.935	&	0.970	&	0.942	&	0.987	&	\textbf{0.993}	&	0.990	&	0.9770	\\	\cmidrule{2-13}
        &CapsNet\textsubscript{RE}~\cite{li2020exploring}	    &	0.947	        &	0.907	&	0.970	&	0.949	&	0.872	&	0.966	&	0.909	&	0.934	&	0.929	&	0.871	&	0.9250	\\	\cmidrule{2-13}
        &ADGAN~\cite{ADGAN}	                        &	\textbf{0.999}	&	0.992	&	0.968	&	0.953	&	0.960	&	0.955	&	0.980	&	0.950	&	0.959	&	0.965	&	0.9680	\\	\cmidrule{2-13} &LSA~\cite{lsa}	                            &	0.993	        &	0.999	&	0.959	&	0.966	&	0.956	&	0.964	&	0.994	&	0.980	&	0.953	&	0.981	&	0.9750	\\	\cmidrule{2-13}
&GradCon~\cite{gradcon}	                        &	0.995	        &	0.999	&	0.952	&	0.973	&	0.969	&	0.977	&	0.994	&	0.979	&	0.919	&	0.973	&	0.9730	\\	\cmidrule{2-13}
        &-VAE\textsubscript{u}~\cite{lamda-vae}	&	0.991	        &	0.996	&	0.983	&	0.978	&	0.976	&	0.972	&	0.993	&	0.981	&	0.98	&	0.967	&	0.9820	\\	\cmidrule{2-13}
        &ULSLM~\cite{ulslm}	                        &	0.991	        &	0.972	&	0.919	&	0.943	&	0.942	&	0.872	&	0.988	&	0.939	&	0.96	&	0.967	&	0.9490	\\	\cmidrule{2-13}
        &CAVGA-D\textsubscript{u}~\cite{venkataramanan2019attention}	    &	0.994	        &	0.997	&	0.989	&	0.983	&	0.977	&	0.968	&	0.988	&	0.986	&	0.988	&	\textbf{0.991}	&	0.9860	\\	\cmidrule{2-13}
    	&	Student-Teacher~\cite{bergmann2020uninformed}	&	\textbf{0.999}	&	0.999	&	0.990	&	\textbf{0.993}	&	0.992	&	\textbf{0.993}	&	0.997	&	\textbf{0.995}	&	\textbf{0.986}	&	0.991	&	\textbf{0.9935}	\\	\cmidrule{2-13}
    	                        &	\textbf{Ours - ImageNet}                                       	&	0.998	&	\textbf{0.999}	&	\textbf{0.992}	&	0.991	&	\textbf{0.993}	&	0.991	&	\textbf{0.997}	&	0.990	&	0.984	&	\textbf{0.991}	&	\textbf{0.9927}	\\	\bottomrule

\end{tabular}}
\caption{\textbf{Anomaly detection:} class-level testing AUC on MNIST produced by the SOTA and our methods.}
\label{tab:mnist-auc}
\end{table*}

\bgroup
\def\arraystretch{1.4}
\begin{table*}[h]
\centering
\resizebox{0.95\linewidth}{!}{\begin{tabular}{@{}c|cccccccccccc@{}}
\toprule
	Method	&	0	&	1	&	2	&	3	&	4	&	5	&	6	&	7	&	8	&	9	&	Mean	\\	\midrule
	\midrule
	Ours - ImageNet	&	0.908	&	0.992	&	0.902	&	0.946	&	0.93	&	0.95	&	0.818	&	0.993	&	0.938	&	0.981	&	0.935	\\
	Ours - SSL	&	0.926	&	0.992	&	0.922	&	0.946	&	0.931	&	0.971	&	0.832	&	0.992	&	0.946	&	0.982	&	0.944	\\\bottomrule
\end{tabular}}
\caption{\textbf{Anomaly detection:} class-level testing AUC on FMNIST produced by our methods.}
\label{tab:fmnist-auc}
\end{table*}

\section{Class-level Results}
The class-level results are shown in Tables~\ref{tab:auc_class_mvtec},~\ref{tab:mnist-auc},~\ref{tab:fmnist-auc},~\ref{tab:cifar-auc},  and~\ref{tab:mvtec-loc-auc}.  
The mean accuracy and class-level anomaly detection accuracy on MVTec dataset is displayed in Tab.~\ref{tab:auc_class_mvtec}, where our ImageNet KD pre-trained model outperforms the previous SOTA methods CAVGA-D\textsubscript{u} and CAVGA-R\textsubscript{u}~\cite{venkataramanan2019attention} by 6\% and 2\%, respectively, and our SSL pre-trained model outperforms their approach by 7\% and 3\%, respectively. 
With ImageNet KD pre-training, our model achieves the best accuracy results in \textbf{ten categories} of the MVTec AD. The shallow generative baselines, such as DAE, AE-SSIM and AnoGAN yield sub-optimal results on MVTec AD. When compared with methods recently considered to be the MVTec AD SOTA, such as LSA~\cite{lsa} and -VAE\textsubscript{u}~\cite{lamda-vae}, our approach shows more than 7\% improvement. 
We also show the AUC anomaly detection results in Tab.~\ref{tab:auc_class_mvtec}, where our method, with SSL and ImageNet KD pre-training, surpasses all previous methods by at least 5.3\%, and produces the best results in eleven categories.  
The results of IGD for MNIST in Tab.\ref{tab:mnist-auc} show that our approach pre-trained with ImageNet KD is competitive with the Student-Teacher~\cite{bergmann2020uninformed}, and both are better than any of the previously proposed methods in the field. 
In Table~\ref{tab:fmnist-auc}, we only show the results of our approach because we could not find the class-level results for other approaches.
On the class-level results for CIFAR10, on Tab.~\ref{tab:cifar-auc}, we notice that our approach pre-trained with ImageNet and SSL shows the best AUC result in the field by a large margin (around 10\%) compared with the Student-Teacher~\cite{bergmann2020uninformed} approach. Finally, the class-level anomaly localisation AUC results for MVTec on Tab.~\ref{tab:mvtec-loc-auc} only shows the results of our approach because we could not find results from other approaches.



\begin{table*}[t]
\centering
\resizebox{0.95\linewidth}{!}{\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
	                            &	Method	&	Plane	&	Car	    &	Bird	&	Cat	&	Deer	&	Dog	&	Frog	&	Horse	&	Ship	&	Truck	&	Mean	\\	\midrule
	&	DAE~\cite{hadsell2006dimensionality}	                            &	0.411	&	0.478	&	0.616	&	0.562	&	0.728	&	0.513	&	0.688	&	0.497	&	0.487	&	0.378	&	0.5358	\\	\cmidrule{2-13}
	&	VAE~\cite{vae}	                            &	0.634	&	0.442	&	0.640	&	0.497	&	0.743	&	0.515	&	0.745	&	0.527	&	0.674	&	0.416	&	0.5833	\\	\cmidrule{2-13}
	&	 KDE~\cite{bishop2006pattern}	                        &	0.658	&	0.520	&	0.657	&	0.497	&	0.727	&	0.496	&	0.758	&	0.564	&	0.680	&	0.540	&	0.6100	\\	\cmidrule{2-13}
	&	OCSVM~\cite{oc-svm}	                        &	0.630	&	0.440	&	0.649	&	0.487	&	0.735	&	0.500	&	0.725	&	0.533	&	0.649	&	0.508	&	0.5860	\\	\cmidrule{2-13}
	&	AnoGAN~\cite{anogan}	                        &	0.671	&	0.547	&	0.529	&	0.545	&	0.651	&	0.603	&	0.585	&	0.625	&	0.758	&	0.665	&	0.6179	\\	\cmidrule{2-13}
	&	DSVDD~\cite{dsvdd}	                        &	0.617	&	0.659	&	0.508	&	0.591	&	0.609	&	0.657	&	0.677	&	0.673	&	0.759	&	0.731	&	0.6481	\\	\cmidrule{2-13}
    &	OCGAN~\cite{ocgan}	&	0.757	&	0.531	&	0.640	&	0.62	&	0.723	&	0.620	&	0.723	&	0.575	&	0.820	&	0.554	&	0.6566	\\	\cmidrule{2-13}
	&	PixelCNN~\cite{van2016conditional}	                    &	0.788	&	0.428	&	0.617	&	0.574	&	0.511	&	0.571	&	0.422	&	0.454	&	0.715	&	0.426	&	0.5510	\\	\cmidrule{2-13}
	&	CapsNet\textsubscript{PP}~\cite{li2020exploring}	    &	0.622	&	0.455	&	0.671	&	0.675	&	0.683	&	0.350	&	0.727	&	0.673	&	0.710	&	0.466	&	0.6120	\\	\cmidrule{2-13}
	&	CapsNet\textsubscript{RE}~\cite{li2020exploring}	    &	0.371	&	0.737	&	0.421	&	0.588	&	0.388	&	0.601	&	0.491	&	0.631	&	0.410	&	0.671	&	0.5310	\\	\cmidrule{2-13}
	&	ADGAN~\cite{ADGAN}	                        &	0.671	&	0.547	&	0.529	&	0.545	&	0.651	&	0.603	&	0.585	&	0.625	&	0.758	&	0.665	&	0.6180	\\	\cmidrule{2-13}
	&	LSA~\cite{lsa}	                            &	0.735	&	0.580	&	0.690	&	0.542	&	0.761	&	0.546	&	0.751	&	0.535	&	0.717	&	0.548	&	0.6410	\\	\cmidrule{2-13}
&	GradCon~\cite{gradcon}	                        &	0.760	&	0.598	&	0.648	&	0.586	&	0.733	&	0.603	&	0.684	&	0.567	&	0.784	&	0.678	&	0.6640	\\	\cmidrule{2-13}
	&	-VAE\textsubscript{u}~\cite{lamda-vae}	&	0.702	&	0.663	&	0.68	&	0.713	&	0.77	&	0.689	&	0.805	&	0.588	&	0.813	&	0.744	&	0.7170	\\	\cmidrule{2-13}
	&	ULSLM~\cite{ulslm}	                        &	0.740	&	0.747	&	0.628	&	0.572	&	0.678	&	0.602	&	0.753	&	0.685	&	0.781	&	0.795	&	0.7360	\\	\cmidrule{2-13}
	&	CAVGA-D\textsubscript{u}~\cite{venkataramanan2019attention}	    &	0.653       	&	0.784	        &	\textbf{0.761}	    &	\textbf{0.747}	&	0.775	&	0.552	&	0.813	&	0.745	&	0.701	&	0.741	&	0.7370	\\	\cmidrule{2-13}
	&	Student-Teacher~\cite{bergmann2020uninformed}	&	0.789	&	0.849	&	0.734	&	\textbf{0.748}	&	\textbf{0.851}	&	\textbf{0.793}	&	\textbf{0.892}	&	0.830	&	0.862	&	0.848	&	0.8196	\\	\cmidrule{2-13}
	                        &\textbf{Ours - ImageNet}	        &	\textbf{0.868}	&	\textbf{0.870}	&	\textbf{0.738}	&	0.716	&	0.850	&	0.766	&	0.890	&	\textbf{0.871}	&	\textbf{0.898}	&	\textbf{0.899}	&	\textbf{0.8368}
	                        \\ \cmidrule{2-13}
	                         &\textbf{Ours - SSL}	         &	\textbf{0.906}	&	\textbf{0.979}	&	\textbf{0.839}	&	\textbf{0.823}&	\textbf{0.886}	&	\textbf{0.899}	&	\textbf{0.909}	&	\textbf{0.964}	&	\textbf{0.969}	&	\textbf{0.948}	&	\textbf{0.9125} \\ \bottomrule


\end{tabular}}
\caption{\textbf{Anomaly detection:} class-level testing AUC on CIFAR10 produced by the SOTA and our methods.}
\label{tab:cifar-auc}
\end{table*}



\begin{table*}[t]
\centering
\resizebox{0.95\linewidth}{!}{\begin{tabular}{@{}c|ccccccccccccccccc@{}}
\toprule
Method     & Bottle & Hazelnut & Capsule & Metal Nut & Leather & Pill & Wood & Carpet & Tile & Grid & Cable & Transistor & Toothbrush & Screw & Zipper & Mean \\\midrule
Ours - ImageNet  & 0.928  & 0.981 & 0.967 & 0.902 & 0.983 & 0.962 & 0.827 & 0.901 & 0.727 & 0.916 & 0.835 & 0.843 & 0.974 & 0.960 & 0.932 & 0.909 \\
Ours - SSL & 0.922  & 0.980  & 0.977 & 0.926 & 0.995 & 0.973 & 0.891 & 0.947 & 0.780  & 0.977 & 0.847 & 0.844 & 0.977 & 0.970 & 0.967 & 0.931 \\\bottomrule
\end{tabular}}
\caption{\textbf{Anomaly localisation:} class-level testing pixel-wise localisation AUC results on the anomalous images of MVTec AD produced by our methods.}
\label{tab:mvtec-loc-auc}
\end{table*}


\section{Qualitative Localisation Results}

Figure~\ref{fig:colon_visual} shows the polyp  segmentation results on Hyper-Kvasir testing set images, and Figure~\ref{fig:my_label} displays the defect results on MVTec AD testing set images.

\begin{figure*}[h!]
  \centering
    \includegraphics[width=.95\textwidth]{iccv_colon.png}
    \caption{\label{fig:colon_visual}
    Qualitative visual results from Hyper-Kvasir testing set (red = anomaly). 
    } 
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\linewidth]{image/supply_heatmap.pdf}
    \caption{Qualitative results of our anomaly localisation results on the MVTec AD testing set (red = high probability of anomaly).}
    \label{fig:my_label}
\end{figure*}

\end{document}
