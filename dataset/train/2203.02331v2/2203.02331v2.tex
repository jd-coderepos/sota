









\documentclass[a4paper,conference]{IEEEtran}























\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\graphicspath{{./fig/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
\fi






\usepackage{amsmath}
\usepackage{multirow}



























\usepackage{subfig}





\usepackage{url}
\usepackage{hyperref}










\begin{document}
\title{F2DNet: Fast Focal Detection Network \\ for Pedestrian Detection}








\author{
    \IEEEauthorblockN{
        Abdul Hannan Khan\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
        Mohsin Munir\IEEEauthorrefmark{2},
        Ludger van Elst\IEEEauthorrefmark{2} and
        Andreas Dengel\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
    }
    \IEEEauthorblockA{
        \IEEEauthorrefmark{1}Fachbereich Informatik,
        Technische Universit√§t Kaiserslautern, \\
        67663 Kaiserslautern, Germany
    }
    \IEEEauthorblockA{
        \IEEEauthorrefmark{2}German Research Center for Artificial Intelligence (DFKI GmbH),\\ 
        67663 Kaiserslautern, Germany
    }
    \IEEEauthorblockA{
        Corresponding Author: hannan.khan@dfki.de
    }
}









\maketitle

\begin{abstract}
Two-stage detectors are state-of-the-art in object detection as well as pedestrian detection. However, the current two-stage detectors are inefficient as they do bounding box regression in multiple steps i.e. in region proposal networks and bounding box heads. Also, the anchor-based region proposal networks are computationally expensive to train. We propose F2DNet, a novel two-stage detection architecture which eliminates redundancy of current two-stage detectors by replacing the region proposal network with our focal detection network and bounding box head with our fast suppression head. We benchmark F2DNet on top pedestrian detection datasets, thoroughly compare it against the existing state-of-the-art detectors and conduct cross dataset evaluation to test the generalizability of our model to unseen data. Our F2DNet achieves 8.7\%, 2.2\%, and 6.1\%  on City Persons, Caltech Pedestrian, and Euro City Person datasets respectively when trained on a single dataset and reaches 20.4\% and 26.2\%  in heavy occlusion setting of Caltech Pedestrian and City Persons datasets when using progressive fine-tunning. Furthermore, F2DNet have significantly lesser inference time compared to the current state-of-the-art. Code and trained models will be available at \url{https://github.com/AbdulHannanKhan/F2DNet}.


\end{abstract} 





\IEEEpeerreviewmaketitle



\section{Introduction}

Pedestrian detection is a sub-domain of object detection where the target class is pedestrian and the rest is considered background. Pedestrian detection plays a vital role in autonomous driving as well as surveillance. In autonomous driving, one of the most important objectives is to avoid collision with pedestrians by detecting and tracking them. This objective is to be carried out in a limited resource scenario as limited computational power is available inside an autonomous vehicle due to compactness and power efficiency constraints. This requires the pedestrian detection model to be light and efficient. Also, the lesser the time model takes to process a single frame more frame per second it can process which yields better awareness of surroundings.

Region Proposal Networks were first proposed by Ross Girshick et al. \cite{fasterrcnn} to replace, slow, selective search-based region proposal generation with a faster, CNN-based network that can be trained end-to-end along with detection head. In the last decade, researchers have focused on improving two-stage detectors by proposing new detection heads \cite{mgan, betarcnn, cascadercnn} with little focus on region proposal network architecture. However, the role of region proposal networks in two-stage detectors is limited to proposing candidate regions with the purpose of objectness score produced by region proposal networks limited to proposal filtering. Also, proposed bounding boxes from region proposal network need rigorous refinement in their coordinates, for example, Cascade RCNN \cite{cascadercnn} applies three cascading heads to get refined detections.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{fig/prog.pdf}
    \caption{Evolution of pedestrian detectors over the years and their corresponding  on Caltech Pedestrian \cite{caltech} (orange), City Persons \cite{citypersons} (green) and Euro City Persons \cite{eurocitypersons} (pink) datasets in reasonable settings.}
    \label{fig:history}
\end{figure}


Compared to two-stage detectors, single-stage detectors are efficient as they split the image into a grid and perform detection per patch eliminating region proposal network \cite{ssd, yolo, focalloss, alfnet}. However, single-stage detectors do not perform as good as two-stage detectors in terms of accuracy, this can be attributed to a class imbalance between positive and negative samples \cite{focalloss}. Other than class imbalance, since each patch does not necessarily contain a full object, classifying if a patch contains enough parts of an object is sub-optimal, as a part may belong to multiple object classes. A common attribute of both single and two-stage detectors explained above is anchors. Both kinds of detectors rely on anchors with predefined aspect ratios.

In the last few years, anchor-free object detectors were proposed \cite{fcos, cornernet}. Motivated from anchor-free approaches in object detection, anchor-free approaches were proposed for pedestrian detection as well \cite{csp, acsp, apd}. These pedestrian-specific approaches, take the idea of single-stage detector to another level by predicting classes per pixel instead of per patch. However, this is done on a downscaled feature map to be efficient and robust \cite{csp}. Unlike anchor-based approaches, center and scale-based approaches classify if each pixel is a center pixel of an object and regress the possible scale of that object. In this way, center and scale-based approaches eliminate the idea of predicting rough bounding boxes and refining them later on. Further, center and scale-based approaches use the focal loss as classification loss to deal with class imbalance \cite{csp}.


Although center and scale-based approaches have optimal design and better convergence they produce more false positives due to penalty reduced focal loss, which does not punish much the false predictions in the neighborhood of positive pixels. This problem intensifies in the case of small and heavily occluded pedestrians.

Our method is different in nature from existing single and two-stage detectors. The closest single-stage detector is anchor-free, center and scale prediction \cite{csp}. However, we only use the head from CSP \cite{csp} with different loss settings as the CSP head \cite{csp} is stronger and efficient compared to region proposal networks, also we use fast suppression head to further refine detections. Compared to two-stage detectors, we replace the region proposal network with a stronger detection network, we do not call it another region proposal network because focal detection network produces strong detection candidates compared to proposals that need further bounding box refinement and classification. Also, we replace computationally expensive traditional second stage, which predicts bounding boxes as well as classifies them, with a simple and efficient suppression head to only suppress false positives without altering bounding boxes.


Contribution of this paper is three fold;
\begin{itemize}
 
\item First, we redesign a two-stage detection architecture to remove redundant and inefficient bounding box prediction and replace region proposal network with a strong detection network, followed by a light-weight suppression head instead of multiple bounding box heads.

\item Second, we propose focal detection network as our classification and bounding box regression head, which can independently produce satisfactory results.

\item Third, we propose fast suppression head to handle false positives produced by focal detection head in small and heavily occluded settings.
\end{itemize}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/n6.pdf}
    \caption{The network architecture of our F2DNet. The input image is passed through backbone and FPN to extract feature maps which are then passed to the focal detection network to obtain initial detections. The detected bounding boxes are then passed to the fast suppressed head along with feature maps to suppress false positives.}
    \label{fig:f2d}
\end{figure*} 















\section{Related Work}
Significant improvements have been made in recent years in the field of pedestrian detection using deep learning models \cite{pedestron, mgan, alfnet, csp} as shown in Fig. \ref{fig:history}. Most of the recent techniques follow general object detection workflow including a strong pre-trained backbone to extract features, an optional feature pyramid network (FPN) \cite{fpn} based feature enrichment layer, a region proposal network (RPN) \cite{fasterrcnn} in case of two-stage detectors and at the end, bounding box heads for bounding box regression and classification. Such pipelines are supported in modern object detection frameworks like mmdetection \cite{mmdetection}. Different types of pedestrian detectors have emerged in recent years which can be differentiated from each other based on how they use region proposal network and choice of bounding box heads.

\subsection{Anchor Based Pedestrian Detectors}
Region-based convolutional neural networks are two-staged object detectors, which were first proposed by Girshick et al. in \cite{rcnn} for object detection. Fast-RCNN and Faster-RCNN were proposed to improve the processing time of RCNN by using ROI pooling on features maps instead of raw image and CNN-based region proposal network respectively \cite{fastrcnn, fasterrcnn}. Mask Guided Attention Network incorporates additional visibility information of the object to handle occlusions better \cite{mgan}. Cascade R-CNN proposed by Cai and Vasconcelos in \cite{cascadercnn} uses multiple bounding box heads to refine detections in cascading manner. Another anchor-based but single-stage pedestrian detector is ALFNet, which is based on Single Shot Multibox Detector (SSD) \cite{alfnet, ssd}. RetinaNet is yet another single-stage detector, which is similar to SSD \cite{ssd} but introduces focal loss to handle foreground and background class imbalance \cite{retinanet}.

\subsection{Anchor Free Pedestrian Detectors}
Anchor-free pedestrian detectors are pedestrian-specific object detectors that do not use anchors or region proposal networks. Instead, they predict bounding box and class per pixel on down-scaled feature maps because performing detection per pixel on original resolution is costly. CornerNet \cite{cornernet} uses CNN based approach to predict paired keypoint heatmaps i.e. one heatmap for each top-left and bottom-right corner. Fully convolutional single-stage object detection network, FCOS was proposed in \cite{fcos}, which adopts classification and bounding box prediction of R-CNN heads to pixel-wise fashion with bounding box predictions being pixel distances from object center, which is calculated using centeredness predicted per pixel by FCOS. Center and Scale Prediction CSP \cite{csp} proposed for pedestrian detection uses a similar approach but instead predicts center heatmap, scale map and reconstructs the bounding boxes using the center and scale \cite{csp}. ACSP \cite{acsp} uses switchable normalization for better convergence on different batch sizes and
uses full resolution for training to improve recall. APD \cite{apd} tries to handle crowded pedestrians by additionally predicting density and diversity. BGCNet replaces normal convolutions with box-guided convolution for center heatmap subnet to incorporate predicted scale and offset information in center heatmap prediction \cite{bgcnet}. 

\subsection{ViT Based Methods}
Vision transformer (ViT) is the adaptation of transformers in the domain of computer vision. DETR \cite{detr} and DETR for pedestrian detection \cite{ped_detr} use ViT based bounding box heads to provide a wider receptive field. Recently proposed soft teacher based approach for object detection \cite{softteacher} uses SWin transformer \cite{swint} based backbone and semi-supervised training to produce promising results.
 
\section{F2DNET: Fast Focal Detection Network}
Current two-stage object detection architectures employ a weak region proposal network followed by strong bounding box heads. We take a different approach and use a strong detection head succeeded by a light suppression head. In this way, the detection head focuses on precise localization and high classification recall while the suppression head takes care of false positives. In short, our two-stage detection architecture attains high efficiency by eliminating the repetition contained in current two-stage architectures.

In this section, we explain the architecture of our fast focal detection network in detail and argue about our design choices. First, we elaborate on the feature extraction process followed by the detailed architecture of F2DNet and conclude the section by explaining the detection formation strategy. Fig. \ref{fig:f2d} shows complete architecture of our model.

\subsection{Feature Extraction}
To predict precise location and size high-resolution features are required which contain semantic and position information. Aggressive down and upscaling can result in loss of this vital information \cite{pedestron}. Therefore, we use the HRNetW32v2 backbone \cite{hrnet} for feature extraction as it extracts high-resolution features from images.
To obtain feature maps of a single scale, we take feature maps from different stages of backbone, upscale them to  using bilinear interpolation and apply convolution operations. In this way, the model stays light on memory as interpolation operation has no memory cost but is effective as succeeding convolution operations provide necessary learnable parameters.

\subsection{Focal Detection Network}
The architecture of the focal detection network is based on the idea of center and scale map prediction which eliminates explicit modeling of bounding boxes for detection \cite{csp}. Our approach is somewhat similar to that proposed in \cite{csp} however, we use different loss settings to fine-tune the architecture for better convergence and precise localization.

Center loss for focal detection network can be formulated as: 



where



 In equation above,  and  are predicted center probability and ground truth label respectively.  represents cross entropy loss with  being weight at each location .  represents gaussian based penalty reduction for surrounding pixels of true centers as designation of exact center brings difficulty in training \cite{csp}. The  and  terms define focus weight based on prediction confidence i.e. it reduces contribution of easy examples to the loss and helps optimizer to focus on hard examples. The  term reduces loss for false positives closer to true centers. We found  and  to work best in our experiments.

In \cite{fastrcnn} Smooth L1 loss is recommended for regression as it is robust to outliers. The Smooth L1 loss reduces penalty when the distance between predicted and actual height is small, which helps in better convergence. However, since we use  of height instead of actual height value it can cause smaller detections and ultimately result in false positives due to insufficient IoU. Therefore, we use Vanilla L1 Loss as regression loss to make height predictions more accurate.

We define loss for the focal detection head as:



Where ,  and  represent weights for regression, classification and offset loss respectively. We experimentally found ,  and  help model converge better than other weight settings.

\subsection{Fast Suppression Head}
Since, the focal detection network uses penalty-reduced focal loss as a center loss, false positives in the neighborhood of positive centers are not punished sufficiently. While most of these false positives are suppressed by Non-Maximum Suppression (NMS), it still needs another suppression step to suppress the rests i.e. where IoU with positive predictions is lower than . Therefore, we propose a simple and fast suppression head to further refine the detections. The fast suppression head comprises of ROI Align layer followed by convolutional and dense layers as shown in Fig. \ref{fig:f2d}. We train the fast suppression head in detached settings, i.e. the gradients from the fast suppression head do not flow back to feature maps or detection head. In this way, a simple, light yet effective suppression head is achieved. We use binary cross entropy as loss for our fast suppression head.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{fig/gm.pdf}
    \caption{(Left): Representation of pedestrian showing: the center , height , prediction from focal detection network  and prediction from fast suppression head . (Right): graphical representation of our pedestrian detection generative model where  represents pedestrian.}
    \label{fig:generative_model}
\end{figure}

\subsection{Pedestrian Detection}
Each prediction gets one score from the focal detection network and another from the fast suppression head. We eliminate thresholding hyperparameter by combining both scores using the generative model shown in Fig. \ref{fig:generative_model}. We are particularly interested in an event where pedestrian is detected an not suppressed i.e. . The detection model is derived from joint probability distribution of  and represented by following relation:



\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{Pedestrian detection datasets summary.}
    \label{tab:datasets}
    \centering
    \begin{tabular}{l|c|c|c|c}
         \hline
         Dataset & Images & Pedestrians & Density & Resolution \\
         \hline
         Caltech Pedestrians & 42,782 & 13,674 & 0.32 & 640  480 \\
         \hline
         City Persons & 2,975 & 19,238 & 6.47 & 2048  1024 \\
         \hline
         Euro City Persons & 21,795 & 201,323 & 9.2 & 1920  1024 \\
         \hline
    \end{tabular}
\end{table}

where  and  are the center and height of pedestrian respectively.  is the probability of a position detected as pedestrian center by focal detection head and  is the probability that given a bounding box detection it is suppressed by fast suppression head.
 
\section{Experimental Setup}
In this section, our experimental setup is detailed, which we follow in the rest of the paper unless stated otherwise. First, we briefly go through datasets, followed by the evaluation settings in which we evaluate results on these datasets, and finally, we explain the evaluation criteria we used to compare our models with the existing state-of-the-art.

\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{Evaluation settings for pedestrian datasets based on height and visibility.}
    \label{tab:eval_settings}
    \centering
    \begin{tabular}{l|c|c|c|c}
        \hline
        Setting & 
        \multicolumn{2}{c}{
            \begin{tabular}{c c}
             \multicolumn{2}{c}{\textbf{City Persons, Caltech}}  \\
             \hline
             Visibility & Height 
        \end{tabular}
        } &
        \multicolumn{2}{|c}{
            \begin{tabular}{c c}
             \multicolumn{2}{c}{\textbf{Euro City Persons}}  \\
             \hline
             Visibility & Height 
        \end{tabular}
        }
        \\
         \hline
         Reasonable & [0.65, ] & [50, ]& [0.6, ] & [40, ]\\
         \hline
         Small & [0.65, ] & [50, 75] & [0.6, ] & [30, 60] \\
         \hline
         Heavy Occlusion & [0.2, 0.65] & [50. ] & [0.2, 0.6] & [40. ] \\
         \hline
         All & [0.2, ] & [20. ] & [0.2, ] & [20. ]\\
         \hline
    \end{tabular}
\end{table}

\subsection{Datasets}
To benchmark, our model we present results on three commonly used pedestrian detection datasets i.e. City Persons \cite{citypersons}, Euro City Persons \cite{eurocitypersons} and Caltech Pedestrian dataset \cite{caltech}. Detailed statistics of these datasets can be seen in Table \ref{tab:datasets}.

All the results presented in this paper for Caltech pedestrian dataset \cite{caltech} are based on its test set, while for City Persons \cite{citypersons} and Euro City Persons \cite{eurocitypersons} results are based on their respective validation sets, unless stated otherwise. Also, new annotations for Caltech pedestrian dataset \cite{caltech} proposed in \cite{calnew} were used.

\subsection{Evaluation Settings}
In pedestrian detection, evaluation settings define different subsets of a dataset which are used to better judge the performance of a model in different scenarios. We use evaluation settings proposed in Caltech Pedestrian \cite{caltech} and Euro City Persons \cite{eurocitypersons} datasets. Based on visibility and height of annotations these evaluation settings form four groups where each annotation can belong to more than one group. Settings followed across the paper can be seen in the Table \ref{tab:eval_settings}. It is important to note that evaluation settings are different for Euro City Persons dataset \cite{eurocitypersons} while City Persons \cite{citypersons} and Caltech Pedestrian datasets \cite{caltech} share identical evaluation settings.

\subsection{Evaluation Criteria}
We use Log-average miss rate over false positive per image or  to compare our model against recent models as it has been suggested in pedestrian detection datasets \cite{citypersons, caltech, eurocitypersons} as well as followed by the state of the art \cite{pedestron, bgcnet, betarcnn, csp}.  is calculated by taking geometric mean of miss rates at 9 equally spaced  thresholds in log space i.e. .

\subsection{Weighted Averaging}
We used the mean teacher strategy of weighted averaging for better convergence and performance, as the model obtained after the weighted averaging performs better \cite{mean_teacher, csp}. All results of our models provided in this paper are based on the evaluation of the averaged model unless stated otherwise.

\subsection{Training Details}
We used the Nvidia RTXA6000 GPU cluster to train our models. We used Distributed Data-Parallel to achieve parallel training on multiple GPUs with a manual seed. We used  GPUs with  and  images per GPU for training model on Caltech Pedestrian \cite{caltech} and City Persons \cite{citypersons} datasets respectively. However, for training the model on Euro City Persons dataset \cite{eurocitypersons} we used  GPUs with  images per GPU. We used a constant learning rate throughout the training after warm-up iterations with a maximum of  epochs. 
\input{Results}

\section{Conclusion}
Two-stage detectors perform well in pedestrian detection. However, the region proposal network-based two-stage detectors are inefficient as the region proposal networks predict weak proposals, dependent on further refinement. We replaced the region proposal network with a per-pixel center and scale regression based focal detection network to produce high-quality, standalone detection candidates except for few false positives in small and occluded settings. We pass these strong detection candidates through a light yet fast suppression network, which with barely noticeable computational cost further refines the detections to produce promising results. Our model beats the state-of-the-art in most visibility and height settings while being on par in rest. Also, by using Euro City Persons \cite{eurocitypersons} and City Persons \cite{citypersons} datasets as extra training data, our model achieves the lowest  in a heavy occluded setting, in a multi-dataset setup. 






\section*{Acknowledgment}
This work was supported by the German Ministry for Economic Affairs and Climate Action (BMWK) project KI Wissen under Grant 19A20020G.











\bibliographystyle{IEEEtran}
\bibliography{root}




\end{document}
