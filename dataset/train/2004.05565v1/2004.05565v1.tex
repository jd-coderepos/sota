\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}



\usepackage{bbm}
\usepackage{caption} 
\captionsetup[table]{skip=5pt}

\usepackage{tabularx}  \usepackage{booktabs}  \setlength{\belowcaptionskip}{-10pt}  

\renewcommand{\arraystretch}{1.2}  

\usepackage[breaklinks=true,bookmarks=false,colorlinks]{hyperref}


\cvprfinalcopy 

\def\cvprPaperID{7029} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{FBNetV2: Differentiable Neural Architecture Search \\for Spatial and Channel Dimensions} 
\author{
Alvin Wan\thanks{Work done while interning at Facebook.}, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu, \\
Matthew Yu, Tao Xu, Kan Chen, Peter Vajda, Joseph E. Gonzalez\\
UC Berkeley, Facebook Inc.\\
{\tt\small \{alvinwan,jegonzal\}@berkeley.edu}\\
{\tt\small \{xiaoliangdai,stzpz,zijian,yuandong,s9xie,bichen,mattcyu,xutao,kanchen18,vadjap\}@fb.com}
}

\maketitle




\begin{abstract}
Differentiable Neural Architecture Search (DNAS) has demonstrated great success in designing state-of-the-art, efficient neural networks. However, DARTS-based DNAS's search space is small when compared to other search methods', since all candidate network layers must be explicitly instantiated in memory. To address this bottleneck, we propose a memory and computationally efficient DNAS variant: DMaskingNAS.  This algorithm expands the search space by up to  over conventional DNAS, supporting searches over spatial and channel dimensions that are otherwise prohibitively expensive: input resolution and number of filters. We propose a masking mechanism for feature map reuse, so that memory and computational costs stay nearly constant as the search space expands. Furthermore, we employ  effective shape propagation to maximize per-FLOP or per-parameter accuracy. The searched FBNetV2s yield state-of-the-art performance when compared with all previous architectures.
With up to 421 less search cost, DMaskingNAS finds models with 0.9\% higher accuracy, 15\% fewer FLOPs than MobileNetV3-Small; and with similar accuracy but 20\% fewer FLOPs than Efficient-B0. Furthermore, our FBNetV2 outperforms MobileNetV3 by 2.6\% in accuracy, with equivalent model size. FBNetV2 models are open-sourced at \hyperlink{https://github.com/facebookresearch/mobile-vision}{\color{blue}{https://github.com/facebookresearch/mobile-vision}}.



\vspace{-0.2in}



\end{abstract}

\section{Introduction}
Deep neural networks have led to significant progress in many research areas and applications, such as computer vision and autonomous driving. Despite this, designing an efficient network for resource-constrained settings remains a challenging problem. Initial directions involved compressing existing networks~\cite{deepcompression} or building small networks~\cite{shufflenetv2, mobilenetv2}. However, the design space can easily contain more than  candidate architectures~\cite{fbnet, single_path}, making manual design choices sub-optimal and difficult to scale. In lieu of manual tuning, recent work uses neural architecture search (NAS) to design networks automatically.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/Main.pdf}
    \caption{\textbf{DNAS}: Adding all possible numbers of filters to DNAS (top-right) increases computational and memory costs drastically, exacerbating DNAS's memory bottleneck on search space size. \textbf{Pruning}: Channel pruning (bottom-left) is limited to training one architecture at a time. \textbf{Ours}: With our weight-sharing approximation, DNAS can explore all possible number of filters simultaneously with negligible memory and computation overhead. See Fig.~\ref{fig:method} for details.}
    \label{fig:main}
\end{figure}

Previous NAS methods utilize reinforcement learning (RL) techniques or evolutionary algorithms (EAs). However, both methods are computationally expensive and consume thousands of GPU hours~\cite{nasnet, mnasnet}. As a result, recent NAS literature~\cite{fbnet, darts, enas} focuses on differentiable neural architecture search (DNAS); DNAS searches over a supergraph that encompasses all candidate architectures, selecting a single path as the final neural network. Unlike conventional NAS, DNAS can search large combinatorial spaces in the time it takes to train a single model~\cite{darts, snas, fbnet, single_path}. One class of DNAS methods, based on DARTS~\cite{darts}, suffer from two significant limitations~\cite{nassurvey}:

\begin{itemize}
    \item \textbf{Memory costs bound the search space.} Short of paging in and out tensors, the supergraph and feature maps must reside in GPU memory for training, which limits the search space.
    \item \textbf{Cost grows linearly with the number of options per layer.} This means that each new search dimension introduces combinatorially more options and combinatorial memory and computational costs.
\end{itemize}

The other class of DNAS methods, not based on DARTS, suffer from similar issues: For example, ProxylessNAS tackles the memory constraint by training only one path in the supergraph each iteration. However, this means ProxylessNAS would take a prohibitively long time to converge on an order-of-magnitude larger search space. These memory and computation issues, for all DNAS methods, prevent us from expanding the search space to explore larger spaces of configurations. Noting that feature maps typically dominate memory cost~\cite{nvda_guide}, we propose a formulation of DNAS (Fig.~\ref{fig:main}) called DMaskingNAS (Fig.~\ref{fig:method}) that increases the search space size by orders of magnitude. To accomplish this, we represent multiple channel and input resolution options in the supergraph with masks, which carry negligible memory and computational costs. Furthermore, we reuse feature maps for all options in the supergraph, which enables nearly constant memory cost with increasing search space sizes. These optimizations yield the following three contributions:

\begin{itemize}
    \item \textbf{A memory and computationally efficient DNAS} that optimizes both macro- (resolution, channels) and micro- (building blocks) architectures jointly in a  larger search space using differentiable search. To the best of our knowledge, we are the first to tackle this problem using a differentiable search framework supergraph, with substantially less computational cost and roughly constant memory cost.
    \item \textbf{A masking mechanism and effective shape propagation} for feature map reuse. This is applied to both the spatial and channel dimensions in DNAS.
    \item \textbf{State-of-the-art results} on ImageNet classification. With only 27 hours on 8 GPUs, our searched compact models lead to substantial per-parameter, per-FLOP accuracy improvements. The searched models outperform all previous state-of-the-art neural networks, both manually and automatically designed, small and large.
\end{itemize}

\begin{table}[ht] \small
\centering
\caption{The number of DMaskingNAS design choices eclipses that of previous search spaces: number of channels , kernel size , number of layers , bottleneck type , input resolution , and expansion rate .}

\begin{tabular*}{0.48\textwidth}{l@{\extracolsep{\fill}}llllll}
\toprule
NAS algorithm                       & c & k & l & b & r & e \\
\midrule
MnasNet~\cite{mnasnet}              &\checkmark
  &  &\checkmark  &\checkmark  &  &\checkmark    \\
ProxylessNAS~\cite{proxylessnas}    &   &\checkmark  &\checkmark  &\checkmark  &   &\checkmark   \\
Single-Path NAS~\cite{single_path}  &   &\checkmark  &\checkmark  &   &   &\checkmark   \\
ChamNet~\cite{chamnet}              &\checkmark  &   &\checkmark  &   &\checkmark  &\checkmark    \\
FBNet~\cite{fbnet}                  &   &\checkmark  &\checkmark  &\checkmark  &   &\checkmark  \\
DMaskingNAS                         &\checkmark  &\checkmark  &\checkmark  &\checkmark  &\checkmark  &\checkmark  \\
\bottomrule
\end{tabular*}
\label{tab:searchspace}
\vspace{-0.2in}
\end{table}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/Method.pdf}
    \caption{\textbf{Channel Masking} for channel search: A column vector mask  is the weighted sum of several masks , with Gumbel Softmax weights . Each  has ones (white) in the first  entries and zeros (blue) in the next  entries, for some . Multiplication with this mask speeds up channel search, using a weight-sharing approximation described in Fig.~\ref{fig:channel_challenges}. \textbf{Resolution Subsampling} for input resolution:  is an intermediate output feature map for the network.  is subsampled from  using nearest neighbors. Values at the blue pixels in column  are assembled to create the smaller feature map in . Next, run the operation . Finally, each value in  is placed back into a larger feature map in . Note we put values back () into pixels where we pulled values from (). This process is motivated in Fig.~\ref{fig:spatial_challenges}.}
    \label{fig:method}
\end{figure*}

\section{Related Work}

Hand-crafted, efficient neural networks see two predominant approaches: (1) compressing existing architectures and (2) designing compact architectures from scratch.

\textbf{Network compression} includes both architectural and non-architectural modifications. One non-architectural approach is low-bit quantization, where weights and activations alike may be represented with fewer bits.  For example, Wang et al.~\cite{haq} propose hardware-aware automated quantization, which achieves a - latency reduction on MobileNet~\cite{mobilenet}. These techniques are orthogonal to and can be combined with the methods in this paper. Alternatively, architectural modifications include network pruning~\cite{netprune, structured_sparsity, energy_aware}, where various heuristics govern layer-wise or channel-wise pruning. For example, Han et al.~\cite{netprune} show that magnitude-based pruning can reduce parameter count by orders of magnitude without accuracy loss, and NetAdapt~\cite{netadapt} utilizes a filter pruning algorithm that achieves a 1.2 speedup for MobileNetV2. However, with heuristics-based simplifications, pruning methods train potential architectures separately, one after another -- in some cases, pruning methods consider only one architecture \cite{liu2017slimming, he2018sfp}.

\textbf{Compact architecture design} aims to directly construct efficient networks, rather than trim an expensive one~\cite{squeezenet, squeezedet}.  For example, MobileNet~\cite{mobilenet} and MobileNetV2~\cite{mobilenetv2} achieve substantial efficiency improvements by exploiting a depth-wise convolution and an inverted residual block, respectively.  ShuffleNetV2~\cite{shufflenetv2} shrinks the model size utilizing low-cost group convolutions. Tan et al. propose a compound scaling method, obtaining a family of architectures that achieve state-of-the-art accuracy with an order of magnitude fewer parameters than previous convolutional networks~\cite{efficientnet}. However, these models rely on finely-tuned, manual decisions that are bested by automatic design.

\textbf{Neural architecture search} automates the design of state-of-the-art neural networks. Zoph et al. first proposed using RL for automated neural network design in~\cite{NASRL}. This and other early NAS approaches are based on RL~\cite{NASRL, mnasnet} and EA~\cite{evolution}. However, both approaches consume substantial computational resources.

Later works utilize various techniques to reduce the computational cost of search. One such technique formulates the architecture search problem as a path-finding process in a supergraph ~\cite{fbnet, darts, one-shot, single_path}. Among them, gradient-based NAS has emerged as a promising tool. Wu et al. show that gradient-based, differentiable NAS yields state-of-the-art compact architectures with  less search cost than RL-based approaches. Another direction is to exploit a performance predictor to guide the search process~\cite{chamnet, progressive}. Such approaches explore the search space by trimming progressively and lead to significant reductions in search cost. 

Stamoulis et al.~\cite{singlepathnas} introduce weight-sharing to further reduce the computational cost of search. However, kernel weight-sharing doesn't address the primary drawback of DARTS, namely a memory bottleneck yielding small search space size: Say a ``mixed kernel" contains weights shared between a  and . Since it is impossible to extract a  convolution's outputs from a 's (and vice versa), this mixed kernel still convolves  and still stores 2 feature maps for backpropagation. Thus, 2 kernel-weight-sharing convolutions induce memory and computational costs of 2 vanilla convolutions.



\textbf{Searching along spatial and channel dimensions} has been studied both with and without NAS. Liu et al~\cite{autodeeplab} develop a NAS variant that searches over varying strides for semantic segmentation. However, this method suffers from increasing memory cost as the number of possible input resolutions grows. As described above, network pruning suffers from inefficient and sequential exploration of architectures, one-by-one. Yu et al~\cite{slimmable} amend this partially by creating a batchnorm invariant to the number input channels; after training the ``supergraph" they see competitive accuracy without further training, for each possible subset of channels. Yu et al~\cite{autoslim} expand on these slimmable networks by introducing a test-time greedy channel selection procedure. However, these methods are orthogonal to and can be combined with DMaskingNAS, as we train the sampled architecture from scratch. To address these concerns, our algorithm jointly optimizes over multiple input resolutions and channel options simultaneously, increasing memory cost only negligibly as the number of options grows. This allows DMaskingNAS to support orders of magnitude more possible architectures, under existing memory constraints.

\section{Method}

We propose DMaskingNAS to search over spatial and channel dimensions, summarized in Fig.~\ref{fig:method}. 
The search space would be computationally prohibitive and ill-formed without the optimizations described below; our approach makes it possible to search this expanded search space (Table~\ref{tab:searchspace}) over channels and input resolutions.

\subsection{Channel Search}\label{sec:channel-search}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/Channel_Challenges_Vertical.pdf}
    \caption{\textbf{Channel Search Challenges}: \textbf{Step A}: Consider 3 convolutions with varying numbers of filters. Each output (gray) will have varying numbers of channels. Thus, the outputs cannot be naively summed. \textbf{Step B}: Zero-padding (blue) outputs allows them to be summed. However, both FLOP and memory cost increases sub-linearly with the number of channel options. \textbf{Step C}: This is equivalent to running three convolutions with equal numbers of filters, multiplied by masks of zeros (blue) and ones (white). \textbf{Step D}: We approximate using weight sharing -- all three convolutions are represented by one convolution. \textbf{Step E}: This is equivalent to summing the masks first, before multiplying by the output. Now, FLOP and memory cost are effectively constant w.r.t. the number of channel options. }
    \label{fig:channel_challenges}
\end{figure}

To support searches over varying numbers of channels, previous DNAS methods simply instantiate a block for every channel option in the supergraph. For a convolution with  filters, this could mean up to  convolutions. Previous channel pruning methods ~\cite{autoslim} suffer from a similar drawback: each option must be trained separately, finding the ``optimal" channel count in one shot or iteratively.
Furthermore, even without saturating the maximum number of possibilities, there are two problems, the first of which makes this search impossible:

\begin{enumerate}
    \item \textbf{Incompatible dimensions}: DNAS is divided into several ``cells". In each cell, we consider a number of different block options; the outputs of all options are combined in a weighted sum. This means that all block outputs must align dimensions. If each block adopts convolutions with different number of filters, each output will have a different number of channels. As a result, DNAS could not perform a weighted sum.
    \item \textbf{Slower training, increased memory cost}: Even with a workaround, with this naïve instantiation, each convolution with a different channel option must be run separately, resulting in a  increase in FLOP cost. Furthermore, each output feature map must be stored separately in memory.\end{enumerate}

To address the aforementioned issues, we handle the incompatibility (Fig.~\ref{fig:channel_challenges}, Step A): consider a block  with varying numbers of filters, where  denotes this block with  filters. The maximum number of filters is . The outputs of all blocks are then zero-padded to have  channels (Fig.~\ref{fig:channel_challenges}, Step B). Given input , the Gumbel Softmax output is thus the following, with Gumbel weights :


Note that this is equivalent to increasing the number of filters for all convolutions to , and masking out the extra channels (Fig.~\ref{fig:channel_challenges}, Step C).  is a column vector with  leading 1s and  trailing zeros. Note that the search method is invariant to the ordering of 1s and 0s. Since all blocks  have the same number of filters, we can approximate by sharing weights, so that  (Fig.~\ref{fig:channel_challenges}, Step D).



Finally, with this approximation, we can handle the computational complexity of the naïve channel search approach: this is equivalent to computing the aggregate mask and running the block  only once (Fig.~\ref{fig:channel_challenges}, Step E).



This approximation only requires one forward pass and one feature map, inducing no additional FLOP or memory costs other than the negligible  term in Eq.~\ref{eqn:channel-search-3} (Fig.~\ref{fig:method}, Channel Masking). Furthermore, the approximation falls short of equivalence only because weights are shared, which is shown to reduce train time and boost accuracy in DNAS ~\cite{singlepathnas}. This allows us to search the number of output channels for any block, including related architectural decisions such as the expansion rate in an inverted residual block.

\subsection{Input Resolution Search}

For spatial dimensions, we search over input resolutions. As with channels, previous DNAS methods would simply instantiate each block with every input resolution. This naïve method's downfalls are twofold: increased memory cost and incompatible dimensions. As before, we address both issues directly by zero-padding the result. However, there are two caveats:

\begin{enumerate}
    \item \textbf{Pixel misalignment}: means padding cannot occur naïvely as before. It would not make sense to zero-pad the periphery of the image, since the sum in Eq.~\ref{eqn:channel-search-1} would result in misaligned pixels (Fig.~\ref{fig:spatial_challenges}, B). To handle pixel misalignment, we zero-pad such that zeros are interspersed spatially (Fig.~\ref{fig:spatial_challenges}, C). This zero-padding pattern is uniform; except for the zeros, this is a nearest neighbors upsampling. For example, a  increase in size would involve zero-padding every other row and column. Zero-padding instead of upsampling minimizes ``pixel contamination" across input resolutions (Fig.~\ref{fig:spatial_challenges_conflicts}).
    \item \textbf{Receptive field misalignment}: Since subsets of the feature map correspond to different resolutions, naïvely convolving over the full feature map would result in a reduced receptive field (Fig.~\ref{fig:spatial_challenges}, D). To handle receptive field misalignment, we convolve over subsampled input instead. (Fig.~\ref{fig:spatial_challenges}, E). Using Gumbel Softmax, we arrive at ``resolution subsampling" in Fig.~\ref{fig:method}.
\end{enumerate}

NASNet~\cite{nasnet} introduces a similar notion of combining hidden states. These combinations are also used to efficiently explore a combinatorially large search space but are used to determine -- instead of input resolution or channels -- the number of times to repeat a searched cell. With the above insights, the input resolution search thus incurs constant memory cost, regardless of the number of input resolutions. On the other hand, computational cost increases sub-linearly as the number of resolutions grows.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/Spatial_Challenges_Vertical.pdf}
    \caption{\textbf{Spatial Search Challenges}: \textbf{A}: Tensors with different spatial dimensions cannot be summed due to incompatible dimensions. \textbf{B}: Zero-padding along the periphery of the smaller feature map makes summing possible. However, the top-right pixels (blue) are not aligned correctly. \textbf{C}: Interspersing zero-padding spatially results in a sum that aligns pixels correctly. Note the top-right pixels of both feature maps are correctly overlapping in the sum. \textbf{D}: Say  is a convolution with  kernels. Convolving naïvely with the feature map, containing a subset (gray), results in reduced receptive field (, blue) for the subset. \textbf{E}: To preserve receptive field for all searched input resolutions, the input must be subsampled before convolving. Note the receptive field (blue) is still . Furthermore, note we can achieve the same effect, without the need to construct a smaller tensor, with appropriately-strided dilated convolutions; we subsample to avoid modifying the operation .}
    \label{fig:spatial_challenges}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/Spatial_Challenges_Conflicts.pdf}
    \caption{\textbf{Minimizing Pixel ``Contamination"}: On the far left, we have the original  feature map. The blue  is a feature map subsampled with nearest neighbors and zero-padded uniformly. The yellow  is also subsampled and zero-padded. Summing the  with the  yields the combined feature map to the far right. Only the green pixels in the corners hold values from both feature map sizes; these green values are ``contaminated" by the lower resolution feature maps.}
    \label{fig:spatial_challenges_conflicts}
\end{figure}

\subsection{Effective Shape Propagation}

Note this calculation for effective shape is only used during training. In our formulation of the weighted sum Eq.~\ref{eqn:channel-search-1}, the output  retains the maximum number of channels. However, there exists a non-integral number of \textit{effective} channels: say a 16-channel output has Gumbel weight  and a 12-channel output has weight . This means the effective number of channels is . These effective channels are necessary for both FLOP and parameter computation, as assigning higher weight to more channels should incur a larger cost penalty. This effective shape is how we realize effective resource costs introduced in previous works ~\cite{fbnet, snas}: First, define the gumbel softmax weights as



with sampling parameter , Gumbel noise , temperature . For a convolution with Gumbel Softmax in the  layer, we define its effective output shape  in Eq.~\ref{eqn:effective_out} using effective output channel (, Eq.~\ref{eqn:effective_channel}), and effective height, width (, Eq.~\ref{eqn:effective_spatial}).



with batch size , effective input width  and height .

For a convolution layer without a Gumbel Softmax, effective output shape simplifies to Eq.~\ref{eqn:noneffective_channel}, where effective channel count is equal to actual channel count. For a depth-wise convolution, effective output shape simplifies to Eq.~\ref{eqn:depthwise_conv_cost}, where effective channel count is simply propagated.


with actual output channel count , effective input channel count . Then, we define the cost function for the  layer as follow:


with  convolution groups. The effective input channels for the  layer are . The total training loss consists of (1) cross-entropy loss and (2) total cost, which is the sum of cost from all layers: 
.

In the forward pass, for all convolutions, we calculate and return both the output tensor and effective output shape. Additionally,  in the Gumbel Softmax Eq.~\ref{eqn:gumbel} decreases throughout training,~\cite{gumbel}, forcing  to approach a one-hot distribution.  would thus select a path of blocks in the supergraph; a single channel and expansion rate option for each block; and a single input resolution for the entire network. This final architecture is then trained. Note this final model does not employ masking or require effective shapes. 

\section{Experiments}

We use DMaskingNAS to search for convolutional network architectures under different objectives.  We compare our search space, performance of searched models, and search cost to previously state-of-the-art networks. Detailed numerical results are listed in Table~\ref{tab:imagenet}.

\subsection{Experimental Setup}
We implement DMaskingNAS using PyTorch on 8 Tesla V100 GPUs with 16GB memory. We use DMaskingNAS to search for convolutional neural networks on the ImageNet (ILSVRC 2012) classification dataset~\cite{imagenet}, a widely-used NAS evaluation benchmark. We use the same training settings as reported in~\cite{fbnet}: we randomly select 10\% of classes from the original 1000 classes and train the supergraph for 90 epochs. In each epoch, we train the network weights with 80\% of training samples using SGD. We then train the Gumbel Softmax sampling parameter  with the remaining 20\% using Adam~\cite{kingma2014adam}. We set initial temperature  to 5.0 and exponentially anneal by  every epoch.

\subsection{Search Space}
Previous cell-level searches produced fragmented, complicated, and latency-unfriendly blocks. Thus, we adopt a layer-wise search space for known, latency-friendly blocks. 

\begin{figure}
    \centering
    \includegraphics[width=0.46\textwidth]{figures/Architectures.pdf}
    \caption{Searched FBNetV2 architectures, with colors denoting different kernel sizes and heights denoting different expansion rates. The heights are drawn to scale.}
    \label{fig:architectures}
\end{figure}

\begin{table}[h]
\footnotesize
\centering
\caption{\textbf{Macro-architecture} for our largest search space, describing block type , block expansion rate , number of filters , number of blocks , stride of first block . ``TBS'' means layer type needs to be searched. Tuples of three values represent the lowest value, highest, and steps between options (low, high, steps). The maximum input resolution for FBNetV2-P models is 288, for FBNetV2-F is 224, and for FBNetV2-L is 256. See supplementary material for all search spaces. }



\begin{tabular*}{0.48\textwidth}{l @{\extracolsep{\fill}} lllll}
\toprule
Max. Input              & b           & e         & f          & n        & s \\ \midrule
          & 3x3         & 1         & 16         & 1        & 2 \\

 	 & TBS 	 & 1 	 & (12, 16, 4) 	 & 1 	 & 1 \\
 	 & TBS 	 & (0.75, 3.25, 0.5) 	 & (16, 28, 4) 	 & 1 	 & 2 \\
 	 & TBS 	 & (0.75, 3.25, 0.5) 	 & (16, 28, 4) 	 & 2 	 & 1 \\
 	 & TBS 	 & (0.75, 3.25, 0.5) 	 & (16, 40, 8) 	 & 1 	 & 2 \\
 	 & TBS 	 & (0.75, 3.25, 0.5) 	 & (16, 40, 8) 	 & 2 	 & 1 \\
 	 & TBS 	 & (0.75, 3.75, 0.5) 	 & (48, 96, 8) 	 & 1 	 & 2 \\
 	 & TBS 	 & (0.75, 3.75, 0.5) 	 & (48, 96, 8) 	 & 2 	 & 1 \\
 	 & TBS 	 & (0.75, 4.5, 0.75) 	 & (72, 128, 8) 	 & 4 	 & 1 \\
 	 & TBS 	 & (0.75, 4.5, 0.75) 	 & (112, 216, 8) 	 & 1 	 & 2 \\
 	 & TBS 	 & (0.75, 4.5, 0.75) 	 & (112, 216, 8) 	 & 3 	 & 1 \\
        & 1x1         & -         & 1984       & 1        & 1           \\
       & avgpl     & -         & -          & 1        & 1           \\
                  & fc          & -         & 1000       & 1        & -      \\ \bottomrule
\end{tabular*}

\label{tab:macro-space}
\end{table}

\begin{table}[]
\small
\centering
\caption{\textbf{Micro-architecture} search space for block design: non-linearities, kernel sizes, and Squeeze-and-Excite~\cite{hu2018squeeze}.}
\begin{tabular*}{0.48\textwidth}{l @{\extracolsep{\fill}} lll}
\toprule
block type          & kernel     & squeeze-and-excite & non-linearity           \\ \midrule
ir\_k3              & 3     & N  & relu        \\
ir\_k5              & 5     & N  & relu        \\
ir\_k3\_hs          & 3     & N  & hswish      \\
ir\_k5\_hs          & 5     & N  & hswish      \\
ir\_k3\_se          & 3     & Y  & relu    \\
ir\_k5\_se          & 5     & Y  & relu    \\
ir\_k3\_se\_hs      & 3     & Y  & hswish      \\
ir\_k5\_se\_hs      & 5     & Y  & hswish      \\
skip                & -     & -  & -   \\ \bottomrule
\end{tabular*}
\label{tab:micro-space}
\end{table}

Table~\ref{tab:micro-space} describes the micro-architecture search space: the block structure is inspired by~\cite{mobilenetv2, mobilenetv3} and sequentially consists of a  point-wise convolution, a  or  depth-wise convolution, and another  point-wise convolution. Table~\ref{tab:macro-space} describes the macro-architecture. The search space contains more than  candidate architectures, which is  larger than DNAS's~\cite{fbnet}.

\subsection{Memory Cost}

Our memory optimizations yield a 1MB increase in memory cost for every 2 orders of magnitude the channel search space grows by; for context, this 1 MB increase is just ~0.1\% of the total memory cost during training. This is due to our feature map reuse as described in Sec.~\ref{sec:channel-search}. We compare memory costs for DNAS and DMaskingNAS as the number of channel options increases (Fig.~\ref{fig:memory-cost}, left). With only 8 channel options for each convolution, DNAS fails to fit in memory during training, exceeding the 16GB memory supported by a Tesla V100 GPU. On the other hand, DMaskingNAS supports 32-option channel search, for a  in search space size (given our 22-layer search space), at nearly constant memory cost. Here, -option channel search means that for each convolution with  channels, we search over  channels. To compare larger numbers of channel options, we reduce the number of blocks options in the search space (Fig.~\ref{fig:memory-cost}, right). To compute memory cost, we average the maximum memory allocated during each training step, across 10 epochs.

\begin{figure}
    \centering
    \includegraphics[width=0.23\textwidth]{figures/memory_cost.pdf}
    \includegraphics[width=0.23\textwidth]{figures/memory_cost_debug.pdf}
    \caption{\textbf{Memory Cost of DNAS vs. DMaskingNAS} (Left) Conventional DNAS does not fit into memory with just 8 options per block in channel search. On the other hand, DMaskingNAS's memory cost remains roughly constant, even with 32 channel options per block. (Right) We reduce the number of block options in the search space to fit conventional DNAS into memory. The memory cost growth, as the search space increases, is significantly steeper than that of DMaskingNAS; in fact, DMaskingNAS's memory cost is nearly constant.}
    \label{fig:memory-cost}
\end{figure}

\subsection{Search for ImageNet Models}

\textbf{FLOP-efficient models}: We first use DMaskingNAS to find compact models (Fig.~\ref{fig:architectures}) for low computational budgets, with models ranging from 50 MFLOPs to 300 MFLOPs in Fig.~\ref{fig:results-flop-efficient}. The searched FBNetV2s outperform all existing networks.



\begin{figure}
    \centering
    \includegraphics[width=0.46\textwidth]{figures/accuracy_flop.pdf}
    \caption{\textbf{ImageNet Accuracy vs. Model FLOPs.} We refer to these FLOP-efficient FBNetV2s as FBNetV2-F\{1, 2, 3, 4\} from left to right.} 
    \label{fig:results-flop-efficient}
\end{figure}

\begin{table*}[t]
\small
\centering
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} lllll}
\toprule
Model & \multicolumn{3}{c}{Search} & FLOPs & Top-1 \\ 
 \cmidrule{2-4}
 & Method & Space & Cost (GPU hours) & & Acc (\%) \\
\midrule
MobileNetV2-~\cite{mobilenetv2} & manual & - & - & 59M & 60.3 \\
ShuffleNetV2-~\cite{shufflenetv2} & manual  & - & - & 41M  & 60.3 \\
MnasNet-~\cite{mnasnet} & RL  & stage-wise & 91K & 63M & 64.1    \\
ChamNet-E~\cite{chamnet}& EA & stage-wise  & 28K & 54M & 64.2 \\
FBNet-~\cite{fbnet}  & gradient & layer-wise  & 0.2K  & 72M & 65.3 \\ 
MobileNetV3-Small~\cite{mobilenetv3} & RL/NetAdapt & stage-wise & 91K  & 66M & 67.4 \\
\textbf{FBNetV2-F1 (ours)} & \textbf{gradient} & \textbf{layer-wise}  & \textbf{0.2K} & \textbf{56M}    & \textbf{68.3}   \\ \hline
MobileNetV2- \cite{mobilenetv2} & manual & - & - & 300M & 72.0 \\
ShuffleNetV2- \cite{shufflenetv2} & manual  & - & - & 299M    & 72.6 \\
DARTS  \cite{darts}  & gradient & cell & 0.3K & 595M & 73.1   \\ 
\textbf{FBNetV2-F3 (ours)} & \textbf{gradient} & \textbf{layer-wise}  & \textbf{0.2K} & \textbf{126M} & \textbf{73.2}   \\
\hline
ChamNet-B~\cite{chamnet}& EA & stage-wise  & 28K & 323M & 73.8 \\
FBNet-B~\cite{fbnet} & gradient & layer-wise & 0.2K & 295M & 74.1 \\
One-Shot NAS~\cite{one-shot} & EA & layer-wise & 0.3K & 295M & 74.2 \\
ProxylessNAS~\cite{proxylessnas} & gradient/RL  & layer-wise & 0.2K & 320M & 74.6 \\
MobileNetV3-Large~\cite{mobilenetv3} & RL/NetAdapt & stage-wise & 91K & 219M & 75.2 \\
MnasNet-A1 \cite{mnasnet} & RL  & stage-wise & 91K & 312M & 75.2    \\ 
\textbf{FBNetV2-F4 (ours)} & \textbf{gradient} & \textbf{layer-wise}  & \textbf{0.2K} & \textbf{238M} & \textbf{76.0}   \\ \hline
ResNet-50~\cite{resnet} & manual & - & - & 4.1B & 76.0 \\
DenseNet-169~\cite{densenet} & manual & - & - & 3.5B & 76.2 \\
EfficientNet-B0~\cite{efficientnet} & RL/scaling & stage-wise & 91K & 390M & 77.3 \\
\textbf{FBNetV2-L1 (ours)} & \textbf{gradient} & \textbf{layer-wise} & \textbf{0.6K} & \textbf{325M} & \textbf{77.2} \\ \bottomrule
\end{tabular*}
\caption{\small{\textbf{ImageNet classification performance}: For baselines, we cite statistics on ImageNet from the original papers. Our results are bolded. : The search cost is estimated based on the experimental setup in~\cite{mnasnet}.  :~\cite{chamnet} discovers 5 models with the cost of training 240 networks. : The cost estimation is a lower bound. ~\cite{mobilenetv3} and~\cite{efficientnet} combines the approach proposed in~\cite{mnasnet} with~\cite{netadapt} and compound scaling.}}
\label{tab:imagenet}

\end{table*}

\textbf{Storage-efficient models}: 
Many real world scenarios face limited on-device storage space. Thus, we next perform searches for models minimizing parameter count, in Fig.~\ref{fig:results-param-efficient}. With similar or smaller model size (4M parameters), FBNetV2 achieves 2.6\% and 2.9\% absolute accuracy gains over MobileNetV3~\cite{mobilenetv3} and FBNet~\cite{fbnet}, respectively.

\begin{figure}
    \centering
    \includegraphics[width=0.46\textwidth]{figures/accuracy_param.pdf}
    \caption{\textbf{ImageNet Accuracy vs. Model Size.} We refer to these as parameter-efficient FBNetV2s as FBNetV2-P\{1, 2, 3\} from left to right.}
    \label{fig:results-param-efficient}
\end{figure}

\textbf{Large models}: 
We finally use DMaskingNAS to explore larger models for high-end devices. We compare FBNetV2-Large with networks of 300+ MFLOPs in Fig.~\ref{fig:results-large}.

\begin{figure}
    \centering
    \includegraphics[width=0.46\textwidth]{figures/accuracy_flop_large_model.pdf}
    \caption{\textbf{ImageNet Accuracy vs. Model FLOPs for Large Models.} We refer to these large FBNetV2s as FBNetV2-L\{1, 2\} from left to right.}
    \label{fig:results-large}
\end{figure}






\section{Conclusions}
We propose a memory-efficient algorithm, drastically expanding the search space for DNAS by supporting searches over spatial and channel dimensions. These contributions target the main bottleneck for DNAS -- high memory cost that induces constraints on the search space size -- and yield state-of-the-art performance.

\textbf{Acknowledgements} In addition to NSF CISE Expeditions Award CCF-1730628, UC Berkeley research is supported by gifts from Alibaba, Amazon Web Services, Ant Financial, CapitalOne, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, Nvidia, Scotiabank, Splunk and VMware. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1752814.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{bib}
}

\end{document}
