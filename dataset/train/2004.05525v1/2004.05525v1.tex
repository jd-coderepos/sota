
\documentclass{article} \usepackage{iclr2020_conference,times}
\usepackage{graphicx}
\usepackage{subcaption}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{longtable}
\usepackage{hyperref}
\usepackage{url}


\title{Building Disaster Damage Assessment in Satellite Imagery with Multi-Temporal Fusion}



\author{Ethan Weber \\
Massachusetts Institue of Technology \\
\texttt{ejweber@mit.edu} \\
\And
Hassan Kané \\
WL Research \\
\texttt{hassanmohamed@alum.mit.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
Automatic change detection and disaster damage assessment are currently procedures requiring a huge amount of labor and manual work by satellite imagery analysts. In the occurrences of natural disasters, timely change detection can save lives. In this work, we report findings on problem framing, data processing and training procedures which are specifically helpful for the task of building damage assessment using the newly released xBD dataset. Our insights lead to substantial improvement over the xBD baseline models, and we score among top results on the xView2 challenge leaderboard. We release our code used for the competition\footnote{\url{https://github.com/ethanweber/xview2}}.
\end{abstract}

\section{Introduction}

When a natural disaster occurs, quick and accurate information is critical to an effective response. To better deploy resources in affected areas, it is important for emergency responders to know the location and severity of the damages.

Satellite imagery offers a powerful source of information and can be used to assess the extent and areas of damages. However, a common bottleneck in the current workflows involves the time it takes for human analysts to observe an affected area and identify damaged zones. This process can take hours in a situation where time is of the essence. It therefore presents room to be accelerated through leveraging artificial intelligence.

In the last several years, Convolutional Neural Networks \citep{krizhevsky2012imagenet} have achieved human level performance on a variety of computer vision tasks, including object recognition and image segmentation \citep{lecun2015deep}. These techniques are highly relevant and applicable in the case of satellite image analysis for disaster damage assessment (\cite{ji2018identifying}; \cite{cooner2016detection}).

The main contributions of this work is a characterization of the importance of mono-temporal vs. multi-temporal settings when performing damage assessment with natural disaster along with insights on helpful image pre-processing techniques. The specific insights in this work are that substantially better performance is obtained by independently feeding the pre and post-disaster images through a CNN with shared weights. The features are then fused before a semantic segmentation final layer. Furthermore, working on smaller image crops and weighting error on damage classes inversely proportional to their statistical occurrence in the training dataset leads to models strongly improving over baseline models.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{xBD_Architecture.png}
\caption{\textbf{Our model architecture.} For building damage assessment, we feed pre and post-disaster images through a ResNet50 backbone with shared weights. We then concatenate features before semantic segmentation to obtain our final building damage predictions. The output is a 5-class damage map ranging from no building (0) to destroyed building (4).}
\label{fig:our_model}
\end{figure*}

\section{Related Work}

In this section, we highlight related work in computer vision and large-scale datasets for disaster damage analysis. In particular, we focus on building disaster damage assessment.

\subsection{Computer Vision Techniques for Building Damage Assessment}

Researchers have applied machine learning approaches to building damage assessment in satellite
imagery. \cite{xu2019building} described a method to build convolutional neural networks that automatically detect damaged buildings in satellite images using pre and post images and showed that the model can generalize well to new regions and disasters if it is fine-tuned on a small set of examples from that region. Their work is done on a proprietary dataset spanning three disasters and is framed as a binary pixel classification over damaged/not damaged buildings.

\cite{cooner2016detection} compared the performance of multiple machine learning methods in building damage detection with both pre and post-event satellite imagery of the 2010 Haiti earthquake, and found that a feed-forward neural network achieved the lowest error rate of 40\%. \cite{ji2018identifying} developed a convolutional network to identify collapsed buildings from post-event satellite imagery of the Haiti earthquake, and reached an overall accuracy of 78.6\%.  \cite{duarte2018satellite} combine drone and satellite images of disasters to improve the accuracy of their convolutional networks, with a best reported accuracy of 94.4\%. In these settings damaged building detection over images is framed as a binary pixel classification problem.

\cite{yang2018multi} show that convolutional neural network (CNN) can perform feature-based multi-temporal remote sensing image registration and can outperform four state-of-the-art methods in most scenarios. \cite{nia2017building} show that convolutional neural networks can perform damage assessment on post disaster image. However, their dataset uses ground-level images instead of satellite images.

\subsection{Datasets for damage assessment}
Until xBD \citep{gupta2019xbd}, adequate satellite imagery addressing building damage was not generally available. Other satellite imagery datasets were often limited to single disaster types and did not have common criteria for assessing damage assessment (\cite{fujita2017damage}; \cite{chen2018benchmark}; \cite{foulser2012use}). This made comparing model and agreeing on a problem framing difficult.

The xview dataset \citep{lam2018xview} is a precursor to the xBD dataset. It is one of the largest and most diverse publicly available object detection datasets, with over 1 million objects across 60 classes in over 1,400 km of imagery. It focuses on object detection and not on building damage assessment.

The complete xBD dataset contains satellite images from 19 different natural disasters across 22,068 images and contains 850,736 building polygons. Each image has a 1024 by 1024 pixels resolution. Images are collected from WorldView-3 satellites at 0.3m spatial resolution. The imagery covers a total of 45,361.79 km. It also introduces a four-level damage annotation scale.

\section{Model}

In this section, we explain the task of building damage assessment and data pre-processing. Given the nature of the xBD dataset, with pre-disaster and post-disaster images, we exploit the multi-temporal information to predict both building locations and damage level with the same network. For localization the task is to predict 0 or 1 (background or building) and for damage assessment the task is to predict a 5 dimensional output: 0 is no building and 1-4 is the damage level (1: undamaged, 2: minor, 3: major: 4: destroyed). An alternative approach, which we found to be the best, is focus on only damage assessment but say damage levels 1-4 are ``buildings". This avoids the problem of using completely separate networks, as done in the xBD baselines \citep{gupta2019xbd}. We find this very important to our final results.

\begin{table*}[ht]
\resizebox{\textwidth}{!}{\begin{tabular}{| c c c c c | c c c |}
\hline
Architectures & Two-image input (pre and post) & 512 x 512 images & Joint prediction & Class-specific weighting & Overall F1 & Localization F1 & Damage F1 \\ \hline
Instance seg. &                             &                 &                 &                         & 0.492      & 0.705           & 0.401     \\
Semantic seg. &                             &                 &                 &                         & 0.536      & 0.819           & 0.414     \\
Semantic seg. &                             & \checkmark      &                 &                         &            & 0.835           &           \\
Semantic seg. & \checkmark                  & \checkmark      & \checkmark                &                          & 0.729      & 0.847           & 0.679     \\
Semantic seg. & \checkmark                  & \checkmark                & \checkmark                & \checkmark                        & 0.738      & 0.835           & 0.697   \\ \hline
\end{tabular}}
\caption{\textbf{Our ablation study.} In this table, we show the experiments that inform our final model. The best model uses semantic segmentation, both pre and post image as input, 512 x 512 image crops, joint building localization and damage prediction, and cross-entropy loss weightings to handle class-imbalance.}\label{tab:ablation_study}
\end{table*}

\subsection{Baseline Architecture}

The first decision in this work is related to problem framing. For this work, we are focused on building damage assessment, which means we care about both (i) building localization and (ii) building per-pixel damage classification. Because we have two tasks (i.e. identifying the building and determining the damage level), we decided to use the same model architecture for both tasks. However, the baseline model uses two separate networks.

The task and baseline models are described in the xBD paper \citep{gupta2019xbd}. For localization, a U-Net \citep{ronneberger2015unet} architecture is used for binary pixel classification of ``background" or ``building". For damage assessment, the model is less straightforward. It uses post-damage images fed into a ResNet-50 backbone \citep{he2016deep} pre-trained on ImageNet \citep{deng2009imagenet} and additional features from a shallow CNN. All convolutional layers use a ReLU activation. The output is a one-hot encoded vector where each element represents the probability of an ordinal class. The model uses an ordinal cross-entropy loss function. Unlike traditional cross-entropy, ordinal cross-entropy penalizes relative to the distance between true and predicted ordinal class. Since the difference between any two classes is not interchangeable, this loss function allows the model to distinguish between the different levels of damage.

Extending beyond this baseline, we decided that using the same network for both building detection and damage assessment was a more natural formulation of the problem. This way the model can jointly reason about similar features. For this reason, we use a Mask R-CNN backbone \citep{he2017mask} augmented with a Feature Pyramid Network module \citep{lin2017feature} and a semantic segmentation head. The model was pretrained on the ImageNet dataset \citep{deng2009imagenet}. The implementation of this Mask R-CNN architecture was obtained using the Detectron2 \citep{wu2019detectron2} library from Facebook AI Research which has PyTorch \citep{paszke2019pytorch} implementations of other models as well. With this backbone, we tried both instance segmentation and semantic segmentation to generate the per-pixel classification output. Our final model uses semantic segmentation, as it's a more natural damage-assessment formulation without the notion of instances. Often time, the buildings are too small for instance segmentation to be appropriate.

For the loss function, we use cross-entropy loss on the predicted classes with the ground truth labels.

\subsection{Data Processing Technique}

With the xBD dataset, we have both pre-disaster and post-disaster images available, which can be used for the two tasks of building localization and damage assessment. The images are 1024 by 1024, but we found that the buildings were often too small resolution for the model to accurately draw building boundaries. For this reason, we trained and ran models on 4 512 by 512 images forming the top-left, top-right, bottom-left and bottom-right quadrants.

Furthermore, given this pre and post data, damage assessment can be framed as either a mono-temporal and multi-temporal task \citep{xu2019building}. In the mono-temporal setting, only the post images are fed to a model that has to predict damage level per pixel. In the multi-temporal setting, both the pre and post images are fed to the model that has to predict damage levels on the post images. In this case we fed both the pre and post images independently through the R-CNN base, with shared weights. These features were then concatenated before being fed through the semantic segmentation head of the networks. We show our best architecture in Figure \ref{fig:our_model}.

\begin{table*}[ht]
\resizebox{\textwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
& F-1 score (overall) & No Damage F1 & Minor Damage F1 & Major Dmg F1 & Destroyed F1 \\  \hline
xBD Baseline  &         0.265  &         0.663  &         0.144  &         0.009  &         0.466 \\   \hline
Our model &         \textbf{0.741}  &         \textbf{0.906}  &         \textbf{0.493}  &        \textbf{0.722}  &         \textbf{0.837} \\
\hline
\end{tabular}}
\caption{\textbf{Comparison with xBD baseline.} On all metrics, our best model outperforms the xBD baseline model by a large margin.}\label{tab:correlations}
\end{table*}

\section{Experiments}
In this section, we explain the dataset and experiments used to inform the final model design. The metrics used are for building localization and damage assessment from the xView2 competition\footnote{\url{https://xview2.org/}} (\citep{gupta2019xbd}).

\subsection{xBD Dataset}
This work is one of the first to apply recent deep learning techniques to the xBD dataset. xBD is the largest and first building damage assessment dataset released to date. It contains 1024 x 1024 satellite imagery with 30cm per pixel resolution covering a diverse set of disasters--including earthquakes, floods, volcanic eruption, wildfire and wind--and geographical locations (16) with over 850,000 building annotations (on a 1 to 4 damage scale) across over 45,000 km  of imagery before and after disasters.

\subsection{Ablation Study}
For the task of multi-class pixel classification, both instance segmentation and semantic segmentation are common solutions. The goal of instance segmentation is assign a class and instance ID to every pixel. Similarly, the goal of semantic segmentation is to assign a class to every pixel, but in this case the notion of instance ID is not used. The nature of damage assessment is more suited for semantic segmentation, where instances IDs are not relevant.

Given the success of instance segmentation networks such as Mask R-CNN \citep{he2017mask}, we perform experiments with both instance segmentation and semantic segmentation architectures. Furthermore, we experiment with using image crops, both pre and post images as the input, predicting localization and damage together, and weighting the class loss inversely proportional to the training set distribution.

Looking at Table \ref{tab:ablation_study}, we see the different experiments used to choose our best model. Notice that the F1 score is computed with the xBD test set withheld for the xView2 competition. Localization F1 score is for building detection (0-1) and damage F1 is for damage assessment (0-4). When using a joint prediction, we classify all pixels with damage level at least 1, to be a building. This is how we use the same network for both tasks. Overall F1 is a weighted combination of 30\% localization F1 and 70\% damage F1.

In the first row (row 1), instance segmentation is used on full 1024 x 1024 images. In the case of overlapping bounding boxes, we use the label with higher damage prediction. By switching to semantic segmentation (row 2), we notice a large improvement in localization (0.114). This is due to the building box predictions suffering on small building sizes. By using 4 512 x 512 crops per image (row 3), we obtain a boost in localization F1 of 0.016. By using our multi-temporal input with pre and post images and adding the joint prediction (row 4), we see an increase for all metrics. Lastly (row 5), we add class-specific cross-entropy weighting to obtain an increase in damage and overall F1, while only losing slight performance in localization F1. This is because we put more weighting on building damage classification. The reason for this weighting is that the 73.6\% of polygons have no damage. Furthermore, most of the image is the ``no building" class.

We don't show this in the table, but simple concatenation of pre and post images to produce a 6-channel input produced very poor results. Similarly, subtracting the pre and post images before input did not work well either. Our best results were obtained when processing pre and post images individually, and then concatenating the features before segmentation. Instead of a single-network 256 channel feature embedding being used with the semantic segmentation head, we use 512 features from stacking the pre and post image features coming from the FPN before the semantic segmentation head, which predicts the final 5 classes.

\subsection{Training}

We performed most experiments on a machine with 4 NVIDIA 1080 TIs, and training takes roughly 6 hours to convergence when using ImageNet pretrained weights. When training for too long, the network will collapse into predicting all 0 (no building) labels. Our inversely proportional class weightings allows the network to train longer before collapsing, but it doesn't solve the issue entirely. Code will be released for replication of our work.

\section{Results}

With our final model, we obtain a localization F1 of 0.835 and damage F1 of 0.697. These metrics are combined to obtain an overall F1 of 0.738 in the xView2 competition with the xBD test set. In Table \ref{tab:correlations}, we show the breakdown of the building damage assessment result compared to the xBD baseline. The metrics are computed on the holdout set, and we outperform baselines by a large margin. Notice that this is compared to the xBD baseline, but our results are competitive on the xView2 leaderboard, in which we were ranked place 2 in Track 3: "Evaluation Only", and 40th place before non-validated submissions were removed. Our code is available at \url{https://github.com/ethanweber/xview2}.

\section{Conclusion}
In conclusion, the four main insights behind our performance include working with image quadrants instead of the full image, using one architecture trained on both the pre and post images and fused before the final segmentation layer, using the Mask R-CNN with FPN architecture and engineering our loss function to weight errors on classes inversely proportional to their occurrence on the dataset. Future research directions include using an ordinal cross-entropy loss function to penalize errors in the damage scales differently and also experimenting with other ways to combine the information from the pre and post images at different stages of feature extraction. Incorporating the disaster type (e.g. flood, fire, etc.) into the building damage assessment prediction could also be an interesting direction.

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}
\end{document}
