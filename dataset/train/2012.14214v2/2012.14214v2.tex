\documentclass{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{color}
\usepackage{pifont}
\usepackage{subfigure}
\usepackage{palatino}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\begin{document}
\title{TransPose: Towards Explainable Human Pose Estimation by Transformer}
\author{Sen Yang\thanks{All authors are with Southeast University, Nanjing, China. E-mail: yangsenius@seu.edu.cn} \quad Zhibin Quan \quad Mu Nie \quad Wankou Yang
}
\date{}
\maketitle\vspace*{-0.2in}

\begin{abstract}
	Deep Convolutional Neural Networks (CNNs) have made remarkable progress on human pose estimation task. However, there is no explicit understanding of how the locations of body keypoints are predicted by CNN, and it is also unknown what spatial dependency relationships between structural variables are learned in the model. To explore these questions, we construct an explainable model named TransPose based on Transformer architecture and low-level convolutional blocks. Given an image, the attention layers built in Transformer can capture long-range spatial relationships between keypoints and explain what dependencies the predicted keypoints locations highly rely on. We analyze the rationality of using attention as the explanation to reveal the spatial dependencies in this task. The revealed dependencies are image-specific and variable for different keypoint types, layer depths, or trained models. The experiments show that TransPose can accurately predict the positions of keypoints. It achieves state-of-the-art performance on COCO dataset, while being more interpretable, lightweight, and efficient than mainstream fully convolutional architectures. 
\end{abstract}\vspace*{-0.2in}

\section{Introduction}
Deep Convolutional Neural Networks have dominated the field of human pose estimation, with DeepPose~\cite{toshev2014deeppose} being the early classic method. Afterwards, fully convolutional networks such as ~\cite{wei2016convolutional,long2015fully, wei2016convolutional, newell2016stacked,yang2017learning,chen2018cascaded,papandreou2018PersonLabPP,xiao2018simple,sun2019hrnet} have become the mainstream by predicting keypoints heatmaps, which \emph{implicitly} learn spatial relationships between body parts. Most prior works take deep CNN as a powerful \emph{black box predictor} and focus on improving the network structure to boost the performances, but what exactly happens inside the model or how these convolutional architectures capture the dependencies between body parts remains unexplained. From the scientific and practical perspectives, such interpretability of the model is imperative. It can provide insights into why the models behave the way they do, increase the safety and transparency of the model, help model users for decision-making, and help researchers understand how the structural variables, \emph{e.g.}, body parts in this task, are jointly related to each other to reach the final prediction. 
\begin{figure}[t]
	\begin{center}
\includegraphics[width=0.8\linewidth]{transpose.pdf}
	\end{center}
	\caption{A schematic diagram of TransPose. \textbf{Below:} The inference pipeline of the model. \textbf{Above:} Dependencies areas for all predicted keypoints by inspecting the attention map from the predicted locations, which are annotated by white pentagrams. In this example, the person's left-ankle is occluded by a dog, but the model can still accurately infer its location. \textbf{\emph{Why?}} the attention map (red box) may give a meaningful explanation: the predicted location of the left ankle relies on the image clues provided by the left knee and some joints on the other side of the symmetry.}\vspace*{-0.1in}
	\label{beginning fig}
\end{figure}

We empirically summarize some reasons that might hinder the explainability of human pose estimation models. 
\begin{itemize}
\item \emph{Deepness}. Since the used convolutional neural networks are usually very deep such as~\cite{wei2016convolutional,newell2016stacked,xiao2018simple,sun2019hrnet}, one cannot easily figure out what role each layer plays, especially the high-level layers. 
\item \emph{Relationships and features are coupled}. The global spatial relationships between body parts are not only implicitly encoded in the parameters of convolution kernels, but also expressed in the activations of neurons. Only looking at the intermediate feature activations like~\cite{li2015heterogeneous, zhang2018occluded} cannot reveal the spatial relationship. It is also challenging to decouple and understand these coupled relationships solely from numerical values. 
\item \emph{Limited memory and expressiveness for large numbers of images}. The explanations we expect should be variable, image-specific, and fine-grained. When CNN does inference, however, the \emph{static} convolution kernels parameters are limited in the ability to represent variables due to their limited working memory~\cite{graves2014neural,graves2016hybrid, hochreiter1997long}, thereby making it difficult for CNN to explicitly express the variability of the image-dependent spatial dependencies as the postures of the human body vary in a large joint configuration space.
\item \emph{Lack of tools.} Although there are already visualization techniques like Maximizing Activations~\cite{erhan2009visualizing}, DeconvNet \cite{zeiler2014visualizing}, Saliency Map~\cite{simonyan2013deep, fong2017interpretable}, Feature Visualization~\cite{olah2017feature}, or Class Activation Mapping~\cite{zhou2016learning}, etc., most of them aim to find out class-specific input patterns or saliency maps rather than to explain the fine-grained dependencies between related and structural predicted variables. By far, how to construct or develop explainable pose estimation models is still an open question. 
\end{itemize}


\begin{figure}[t]
	\begin{center}
\includegraphics[width=0.7\linewidth]{attention_and_convolution.pdf}
	\end{center}
	\caption{CNN vs. Attention. \textbf{Left:} The receptive filed enlarges in the deeper convolutional layer. \textbf{Right:} One self-attention layer can capture the pairwise relationship between any pair of locations.}
	\label{conv_vs_attention}\vspace*{-0.1in}
	
\end{figure}

In this work, we aim to build a human pose estimation model that can explain its predictions and directly reveal the image-dependent spatial relationships between keypoints, as illustrated in Fig.~\ref{beginning fig}. We argue that convolution has advantages in extracting low-level features, but deeply stacking convolutional layers at high-level to enlarge the receptive field is not efficient to capture global dependencies. And such deepness increases the difficulty in interpreting CNN predictions. The Transformer architecture~\cite{vaswani2017attention} commonly used in NLP tasks, has a natural advantage over convolutional architectures in terms of drawing pairwise or higher-order interactions. As shown in Fig.~\ref{conv_vs_attention}, one attention layer can capture interactions between any pairwise locations, and the attention map acts as an immediate memory to store these dependencies. Thus Transformer would be eligible for human pose estimation since this task requires the model to be able to capture long-range dependencies. 
                  
Based on these considerations, we design a novel model called \emph{TransPose}, using convolutions to extract features at low-level and Transformer encoder layers to capture global dependencies at high-level. More importantly, the attention maps store the pairwise dependencies measured by the similarities of query-keys transformed from feature vectors at pairs of locations, the model can explain which regions the predicted keypoints locations highly rely on. Given a specific image, we can explicitly understand what image clues significantly contribute to the predictions (maximum activation position) and which parts are affected by the other ones, by unfolding and visualizing the attention map in each attention layer. 

TransPose model is simple and lightweight. It can precisely predict keypoints positions based on heatmaps and obtain excellent performances. It is on par with state-of-the-art -- SimpleBaseline~\cite{xiao2018simple}, HRNet~\cite{sun2019hrnet}, and DarkPose~\cite{zhang2020distribution} baseline via fewer model parameters and faster speeds. In summary, our contributions are as follow: 

\begin{itemize}
	\item To the best of our knowledge, we are the first to use Transformer architecture to capture the spatial relationships between structural human body parts. \vspace*{-0.05in}
	\item TransPose can explain what spatial dependencies the predicted keypoints rely on. Qualitative analysis reveals  the fine-grained spatial dependencies, which are image-specific and variable for different keypoint types, layer depths, and trained models.\vspace*{-0.05in}
	
	\item TransPose achieves 75.8 AP and 75.0 AP on COCO validation set and test-dev set, with 73 fewer parameters and 1.4 faster than HRNet-W48.\vspace*{-0.05in}
	
	\item We find that position embedding is important for accurately predicting the 2D positions of keypoints and helps model generalize better on unseen input resolutions.
\end{itemize}


\section{Related Works}


\subsection{Explainability}
Explainability means a better understanding for human of how the model makes predictions. Many works define the goal for explanation is often to determine what inputs are the most relevant to the prediction, as surveyed by~\cite{samek2019explainable}. Activation Maximizing~\cite{erhan2009visualizing, li2015heterogeneous} performs gradient descent in the input space to find out what input patterns can maximize (explain) a given unit. ~\cite{simonyan2013deep, fang2017rmpe} further consider generating the image-specific class saliency maps. ~\cite{zeiler2014visualizing} uses DeConvNet to generate feature activities to show what has been learned in each convolutional layer. ~\cite{li2015heterogeneous} visualizes the feature maps and finds that some maximally activated neurons work like body parts patterns/templates detector. ~\cite{zhang2018occluded} uses channel-wise attention mechanism to find out the channels of feature activations associated with the occlusion patterns for pedestrian detection. There are also works like Network Dissection~\cite{bau2017network}, Feature Visualization~\cite{olah2017feature}, Excitation Backprop~\cite{zhang2016top}, LRP~\cite{bach2015pixel}, CAM~\cite{zhou2016learning}, and Grad-CAM~\cite{selvaraju2017grad}, which aim to explain the prediction of CNN classifier or visualize the saliency area significantly affecting the class. Different from most prior works, we aim to explain the fine-grained dependencies between body joints variables in the structural skeleton. And our model can directly exploit the ingredient of itself to explain predictions without the help of external tools. It is worth noting that there are some works to explain how the neural network predicts the positions and stores the position information by designing proxy tasks such as CoordConv~\cite{liu2018an} and Zero Padding~\cite{islam2020how}. Our work is carried out on real-world task, explores the importance of position embedding for predicting the locations and its generalization on unseen input scales.

\subsection{Human Pose Estimation}
Deep CNNs have achieved great success in human pose estimation. The inductive biases of vanilla convolution kernel~\cite{lecun1998gradient, krizhevsky2012imagenet} are locality and translation equivariance. It proves to be efficient to extract low-level image feature. For human pose estimation, capturing global dependencies is crucial~\cite{ramakrishna2014pose,tompson2014joint,wei2016convolutional,papandreou2018PersonLabPP}, but the locality nature of convolution makes it impossible to capture long-range interactions. To address this issue, an effective but brute solution is to enlarge the receptive field, \emph{e.g.} by downsampling feature map size, increasing the model depth or expanding the kernel size, etc. Further, more sophisticated design strategies are proposed such as multi-scale feature fusion~\cite{newell2016stacked, pfister2015flowing, yang2017learning, chen2018cascaded, sun2019hrnet, chu2017multi, cheng2020higher}, stacking modules and multiple stages to refine the prediction~\cite{wei2016convolutional,xiao2018simple, newell2016stacked}, or high-resolution representation~\cite{sun2019hrnet}, etc; meanwhile, many successful architectures have emerged such as CPM~\cite{wei2016convolutional}, Stacked Hourglass Network~\cite{newell2016stacked}, FPN~\cite{yang2017learning}, CPN~\cite{chen2018cascaded}, SimpleBaseline~\cite{xiao2018simple}, HRNet~\cite{sun2019hrnet}, RSN~\cite{cai2020learning}, even automated architectures~\cite{yang2019pose,gong2020autopose,mcnally2020evopose2d, cheng2020scalenas, zhang2020efficientpose}. But as the model architecture design becomes more complex, it is more challenging but imperative than ever to seek the transparency and interpretability of the human pose estimation model. Our model, in contrast, can capture the spatial relationship in an efficient yet interpretable way. 


\subsection{Transformer}

	Transformer architecture was proposed by Vaswani \emph{et al.}~\cite{vaswani2017attention} for neural machine translation (NMT) task~\cite{sutskever2014sequence}. Recently, Transformer or attention-augmented layers have merged as new choices for vision tasks such as image classification~\cite{parmar2018image, ramachandran2019stand, bello2019attention, dosovitskiy2020an,touvron2020deit} and object detection~\cite{carion2020detr}. Unlike DETR~\cite{carion2020detr}, ViT~\cite{dosovitskiy2020an}, and DeiT~\cite{touvron2020deit} applying Transformer to predict the object instances set or image categories, we use Transformer to predict the heatmaps represented with 2D spatial distributions of keypoints, and use attention maps as the explanations for this 2D-structural prediction task. 

\section{Method}
Our goal is to build a model that can capture the spatial dependencies between human body parts and explain its predictions.
In this section, we describe the architecture of the proposed model, and then we recall the attention mechanism and further analyze how it explains what the predicted keypoints depend on when given a specific image.
\subsection{Architecture}
\begin{figure*}[t]
 	\begin{center}
\includegraphics[width=1\linewidth]{architecture.pdf}
 	\end{center}
 	\caption{The TransPose model architecture. Firstly, the feature maps are extracted by a CNN backbone and flattened into a sequence. Next, the Transformer encode layers iteratively capture dependencies from the current sequence by query-key-value attention. Then, a simple head is used to predict the keypoints heatmaps. The model can explain what regions or joints significantly contribute to the predicted locations by the attention maps in Transformer encoder layers, and the spatial dependencies can be further revealed.}\vspace*{-0.1in}
 	\label{architecture}
\end{figure*}

As illustrated in Fig.~\ref{architecture}, TransPose model consists of three components: a CNN backbone to extract low-level image feature; a Transformer Encoder to capture long-range spatial interactions between feature vectors across the locations; a head to predict the keypoints heatmaps.

{\bf Backbone.} Many common CNNs can be taken as the backbone. For better comparisons, we choose two typical CNN architectures: ResNet~\cite{he2016deep} and HRNet~\cite{sun2019hrnet}. To keep the simplicity of the model, we only retain the initial part of the original ImageNet pretrained CNN as the initial layers to extract low-level feature from input image. We name them ResNet-S and HRNet-S (including HRNet-S-W32 and HRNet-S-W48). The numbers of parameters of candidate CNNs are only 1.4M, 7.3M and 16.7M, which are 5.5\%, 25.6\% and 24.3\% of the original ResNet-50 (25.6M), HRNet-W32 (28.5M), and HRNet-W48 (68.6M). 

{\bf Transformer.} We follow the standard Transformer architecture~\cite{vaswani2017attention} as closely as possible. And only the encoder is employed, as we believe that pure heatmaps prediction task is simply an encoding task, which compresses the original image information into a compact positions representation of human body keypoints. Given an input image , we assume that the CNN backbone outputs a 2D spatial structure image feature  whose feature dimension has been transformed to  by a 11 convolution. Then, the image feature map is flattened into a sequence , \emph{i.e.},  -dimensional feature vectors where . It enters Transformer Encoder and goes through  attention layers and feed-forward networks (FFN). We provide details in Section~\ref{attention mechanism}.

{\bf Head.} A simple and lightweight head is attached to Transformer Encoder output  to predict  types of keypoints heatmaps  where  by default. We firstly reshape  back to  shape. If  equal , only a 11 convolution reduces the channel dimension of  from  to ; if  equal , we use a bilinear interpolation or a 44 deconvolution to do 2 upsampling before 11 convolution\footnote{In fact, a 11 convolution is completely equivalent to a position-wise linear layer that reduces the dimension of vector in each position of the output sequence.}. 

\subsection{Resolution Settings.}

The computational complexity of per self-attention layer is . Considering the trade-off between the memory footprint for attention layers and the loss in detailed information, we restrict the attention layers to operate at a resolution with  downsampling rate w.r.t. the original input, \emph{i.e.}, ; we adopt  and  setting for ResNet-S and HRNet-S backbone. In the common human pose estimation architectures~\cite{wei2016convolutional, newell2016stacked, xiao2018simple, sun2019hrnet},  downsampling is usually adopted as a standard setting to obtain a very low resolution map containing global information. By contrast, our model can directly capture long-range interactions at a higher resolution, while preserving the fine-grained local feature information.



\subsection{Position Embedding}

Without the position information embedded in the input sequence, the Transformer Encoder is a permutation-equivariant architecture:

where  is any permutation for the pixel locations or the order of sequence. To make the order of sequence or the spatial structure of the image feature map matter, the original Transformer adds sine and cosine positional encodings to the input embeddings.

{\bf 2D Sine position embedding.}  Likewise, we follow the sine positional encodings~\cite{vaswani2017attention} but further hypothesize that the position information is independent at  (horizontal) and  (vertical) direction of an image. Thus we adopt both -direction and -direction position embedding for image feature, like~\cite{parmar2018image,carion2020detr}. It is injected into the input sequence before self-attention layer. We use 2D sine position embedding by default for all TransPose models. See Appendix for how to construct 2D sine position embedding.

{\bf Learnable position embedding and w/o position embedding.} In experiments, we also use a learnable position embedding  to explore whether the model can implicitly learn the position information. In addition, we also conduct the experiment without adding any position information to explore the importance of the position information for predicting 2D-structure heatmaps.
\begin{table*}\scriptsize 
 	\begin{center}
 		\renewcommand{\arraystretch}{1}
 		\setlength{\tabcolsep}{0.3mm}
 		\begin{tabular}{l|ccc|cccc|c}
 			\toprule
 			Model Name & Backbone & Resolution for Attention & Upsampling  &\#layers &heads& d & h & Params \\
 			\midrule
 			TransPose-R* &  ResNet-S*  & 1/8 &  Bilinear Interpolation &3 & 8 & 256 & 512 &5.0M \\
 			TransPose-R-A3 &  ResNet-S  & 1/8 &Deconvolution &3 & 8 & 256 & 1024 &5.2M \\
 			TransPose-R-A4 &  ResNet-S  & 1/8 &Deconvolution &4 & 8 & 256 & 1024 &6.0M \\
 			
 			TransPose-H-S &  HRNet-S-W32  & 1/4 & None& 4& 1 & 64 & 128    & 8.0M \\
 			TransPose-H-A4 &  HRNet-S-W48   & 1/4 &None& 4& 1 & 96 & 192    & 17.3M \\
TransPose-H-A6 &  HRNet-S-W48   & 1/4 &None& 6& 1 & 96 & 192    & 17.5M \\	
 			\bottomrule
 		\end{tabular}
 	\end{center}
 	\caption{Architecture configurations for different TransPose models. More details about the backbones are described in Appendix.}
 	\label{architecture configurations}
\end{table*}


\begin{table*}\small
 	\begin{center}
 		\setlength{\tabcolsep}{0.4mm}
 		\renewcommand{\arraystretch}{1.1}
 		\begin{tabular}{l|c|cc|l|c|c}
 			\toprule
 			Method &Input Size &AP &AR &Params &FLOPs & FPS \\
 			\midrule
 			SimpleBaseline-Res50~\cite{xiao2018simple} &256192 &70.4& 76.3 &34.0M &8.9G & 114\\
 			SimpleBaseline-Res50 + DarkPose &256192 &72.0& 77.6 &34.0M &8.9G & 114\\
 			\hline
 			TransPose-R* & 256192 & 71.5 & 76.9& 5.0M ({\color{black}85\%}) &5.4G & 137 ({\color{black}20\%})\\
 			TransPose-R-A3 & 256192 & 71.7 & 77.1& 5.2M ({\color{black}85\%}) &8.0G & 141 ({\color{black}23\%})\\
 			TransPose-R-A4 & 256192 & \textbf{72.6} & \textbf{78.0}& 6.0M ({\color{black}82\%}) &8.9G & 138 ({\color{black}21\%})\\
 			\midrule
 			HRNet-W32~\cite{sun2019hrnet} &256192 &74.4& 73.7 &28.5M &7.2G & 28\\
 			HRNet-W32 + DarkPose~\cite{zhang2020distribution} &256192 &75.6& 76.7 &28.5M &7.2G & 28\\
 			HRNet-W48~\cite{sun2019hrnet} &256192 &75.1& 80.4 &63.6M &14.6G & 27\\
 			\hline
 			TransPose-H-S &256192 & 74.2 & 78.0& 8.0M ({\color{black}72\%})& 10.2G & 45 ({\color{black}61\%})\\
TransPose-H-A4 &256192 & 75.3 & 80.3& 17.3M ({\color{black}73\%})& 17.5G & 41 ({\color{black}52\%})\\
TransPose-H-A6 &256192 & \textbf{75.8} & \textbf{80.8}& 17.5M ({\color{black}73\%})& 21.8G & 38 ({\color{black}41\%})\\
 			\bottomrule
 		\end{tabular}
 	\end{center}
 	\caption{Results on COCO validation set, provided with the same detected human boxes. TransPose-R, TransPose-H-S, and TransPose-H-A* achieve competitive results to SimpleBaseline, HRNet, and HRNet-DarkPose, with fewer parameters and faster speeds.}\vspace*{-0.1in}
 	\label{state-of-the-art}
\end{table*}

\subsection{Explaining by Attention}

\label{attention mechanism}
{\bf Attention mechanism.} The core mechanism of Transformer~\cite{vaswani2017attention} is multi-head self-attention. It first projects an input sequence  into queries , keys  and values  by three matrices . Note that in this work the position embedding will be added into the input sequences except for computing the values . Then, the attention scores matrix\footnote{Here we consider single-head self attention. For multi-head self-attention, the attention matrix is the average of attention maps in all heads.}  is computed by: 

Each query  belonging to the token (feature vector)  computes similarities with all the keys to achieve a weight vector , which determines how much dependency is needed from each token in the previous sequence for a new token. Then an incremental update (residual connection) for  is achieved by linearly combining each value in Value matrix  with the corresponding weight in . By doing this, the attention maps can be seen as \emph{dynamic weights} that store the similarities dependent on the current input context or feature activations, and weight the distributions in the forward propagation. This mechanism plays a crucial role in capturing and explaining how much contribution the final predictions aggregate from the context token at each location of the sequence. Note that \emph{the pairwise and global interactions mostly occur at the attention layers}\footnote{We assume that the used convolutions are responsible to extract feature in a limited local patch.}. The subsequent layers in feed-forward network (FFN) and head only serve as \emph{position-wise} transformation. They are unable to capture global interactions but approximately linearly transform the contributions from all positions by shared weights. 

Specifically, the last attention layer in Transformer Encoder, whose attention scores are seen as the dynamic (\emph{image-dependent}) weights, has the most direct effect on the predictions. Although the weights in FFN or head cannot be ignored, they are static (\emph{image-independent}) during inference and shared across all locations. We make \emph{gradient analysis} in Appendix~\ref{appendix::grad} to analyze the rationality of using the attention scores of the last attention layer as the explanations for this task. 

\label{paper::grad}
{\bf Explaining the predicted locations by attention maps. } Similar with the ideas in ~\cite{erhan2009visualizing,simonyan2013deep}, the interpretability behind TransPose lies in: the regions which can maximize a given prediction (location) can explain what this prediction is looking for or depend on. 

In this task, the learning target is to expect the output value  in the heatmap to be maximally activated where  represents the groundtruth location of a keypoint:

Assuming the model has been optimized with parameters  and it predicts the location of a particular keypoint as  (maximally activated in heatmaps), why the model predicts such prediction can be explained by the fact that those locations , whose element  has higher attention score () with , are the dependencies that significantly contribute to the prediction. These locations can be found by:

where  is the attention map of the last attention layer and also a function w.r.t  and . Given an image  and a query location ,  can reveal what dependencies a predicted location  highly relies on, we named it \emph{dependency area};  can reveal what positions a specified location  mostly affects, we name it \emph{affected area}.

Different from Activation Maximizing  methods~\cite{erhan2009visualizing, li2015heterogeneous, zeiler2014visualizing} or Saliency Map Visualization~\cite{simonyan2013deep}, we do not need external tools or extra training costs to learn some explainable patterns, but only need to inspect the ingredients of the trained models to reveal what spatial relationships the TransPose has learned. In addition, unlike the visualizations on the fixed patterns that the convolutional kernels prefer to look for, the dynamic characteristic of attention map can show changeable dependencies w.r.t input images, layer depths, types of keypoints or trained models. We make further analysis in the Section~\ref{Explainability Analysis}.
\begin{figure*}[t]
	\centering
	\subfigure[\textbf{TP-R:} predictions and dependencies of each keypoints for \textbf{input A}.]{
		\includegraphics[width=0.45\linewidth]{final_attention_map_286_T-R-A4.pdf}
		\label{tpr-a}
	}\vspace*{-0.1in}
	\quad
	\subfigure[\textbf{TP-H:} predictions and dependencies of each keypoint for \textbf{input A}.]{
		\includegraphics[width=0.45\linewidth]{final_attention_map_286_T-H.pdf}
		\label{tph-a}
	}		
	\subfigure[\textbf{TP-R:} predictions and dependencies of each keypoint for \textbf{input B}.]{
		\includegraphics[width=0.45\linewidth]{final_attention_map_6_T-R-A4.pdf}
\label{tpr-b}
	}\vspace*{-0.1in}
	\quad
	\subfigure[\textbf{TP-H:} predictions and dependencies of each keypoint for \textbf{input B}.]{
		\includegraphics[width=0.45\linewidth]{final_attention_map_6_T-H.pdf}
\label{tph-b}
	}
	\quad
	\subfigure[\textbf{TP-R:} predictions and dependencies of each keypoint for \textbf{input C}.]{
		\includegraphics[width=0.45\linewidth]{final_attention_map_249_T-R-A4.pdf}
\label{tpr-d}
	}\vspace*{-0.01in}
	\quad
	\subfigure[\textbf{TP-H:} predictions and dependencies of each keypoints for \textbf{input C}.]{
		\includegraphics[width=0.45\linewidth]{final_attention_map_249_T-H.pdf}
\label{tph-d}
	}
\quad
\subfigure[\textbf{TP-R:} predictions and dependencies of each keypoint for \textbf{input D}.]{
	\includegraphics[width=0.45\linewidth]{final_attention_map_53_T-R-A4.pdf}
\label{tpr-c}
}\vspace*{-0.01in}
\quad
\subfigure[\textbf{TP-H:} predictions and dependencies of each keypoints for \textbf{input D}.]{
	\includegraphics[width=0.45\linewidth]{final_attention_map_53_T-H.pdf}
\label{tph-c}
}
	\caption{In each sub-figure, the first one is the original input plotted with predicted skeleton. The other maps are the unfolded attention maps in the \text{final layer} of TP-R and TP-H inspected by the predicted locations of keypoints, each position of which is annotated by a white pentagram. All the maps are visualized by the unnormalized attention maps.}\vspace*{-0.1in}
	\label{final_attention_1}
\end{figure*}

\section{Experiments} 


{\bf Dataset.} We evaluate our models on COCO dataset~\cite{lin2014microsoft}, which contains more than 200k images in the wild and 250k person instances. We only use COCO train2017 for training, which consists of 57k images and 150k person instances. Val2017 set contains 5k images and test-dev2017 consists of 20k images. Object keypoint similarity (OKS) is the evaluation metric for keypoints locating accuracy \footnote{http://cocodataset.org/}.  



{\bf Technical details.} We follow the top-down human pose estimation paradigm. The training samples are the cropped images with single person. We resize all input images into  resolution. We use the same training strategies, data augmentation and person detected results as~\cite{sun2019hrnet}. We also adopt the coordinate decoding strategy proposed by~\cite{zhang2020distribution} to reduce the quantisation error when decoding from downscaled heatmaps. The feed forward layers are trained with 0.1 dropout and ReLU activate function. Next, we name the models based on ResNet-S and HRNet-S \emph{TransPose-R} and \emph{TransPose-H}, abbreviated as \emph{\textbf{TP-R}} and \emph{\textbf{TP-H}}. The architecture details are reported in the Tab.~\ref{architecture configurations}. We use Adam optimizer for all models. Training epochs are 230 for TP-R and 240 for TP-H. The cosine annealing learning rate decay is used. The learning rates for TP-R-A4 and TP-H-A6 models decay from 0.0001 to 0.00001, we recommend to use such a schedule for all models. Considering the compatibility with backbone and the memory consumption, we adjust the hyperparameters of Transformer encoder to make the model capacity not very large. 

{\bf Comparisons with state-of-the-art methods.} We compare TransPose based on ResNet-S and HRNet-S with SimpleBaseline, HRNet and the stronger models proposed by~\cite{zhang2020distribution}. Specially, we trained the SimpleBaseline-ResNet50-DarkPose on our machines according to the official code. The others results showed in Tab.~\ref{state-of-the-art} are come from the papers. We test all models on a single NVIDIA 2080Ti GPU with the same experimental conditions to compute the average FPS. As shown in Tab.~\ref{state-of-the-art}, TransPose-R-A4 and TransPose-H-A6 have obviously overperformed Simple-Res50-DarkPose (+0.6AP), HRNet-W32-Dark (+0.2AP), and HRNet-W48 (+0.7AP) baseline models, with significantly fewer model parameters and faster speeds. Tab.~\ref{coco-test} shows the results on COCO test set.

\begin{figure*}[h]
	\begin{minipage}[t]{1\textwidth}
		\centering
\includegraphics[width=0.97\linewidth]{attention_map_249_dependency_T-R-A4.pdf}\vspace*{-0.05in}
		\caption{\textbf{Dependency areas} for the particular positions in the different layers of \textbf{TP-R} for \textbf{input E}.} 
		\label{tp-r-dependency}
		
	\end{minipage}
	
	\begin{minipage}[t]{1\textwidth}
		\centering
		
\includegraphics[width=0.97\linewidth]{attention_map_249_dependency_T-H.pdf}\vspace*{-0.05in}
		
		\caption{\textbf{Dependency areas} for the particular positions in the different layers of \textbf{TP-H} for \textbf{input E}.}\vspace*{-0.15in}
		\label{Tp-H-dependency}
	\end{minipage}
	
\end{figure*}

\begin{figure*}
	\centering
	\subfigure[\textbf{TP-R:} predictions and affected areas for \textbf{Input E}.]{
		\includegraphics[width=0.45\linewidth]{attention_map_249_affect_T-R-A4.pdf}
	}\vspace*{-0.1in}
	\quad
	\subfigure[\textbf{TP-H:} predictions and affected areas for \textbf{Input E}.]{
		\includegraphics[width=0.45\linewidth]{attention_map_249_affect_T-H.pdf}
	}
	\subfigure[\textbf{TP-R:} predictions and affected areas for \textbf{Input F}.]{
		\includegraphics[width=0.45\linewidth]{attention_map_694_affect_T-R-A4.pdf}
	}\vspace*{-0.05in}
	\quad
	\subfigure[\textbf{TP-H:} predictions and affected areas for \textbf{Input F}.]{
		\includegraphics[width=0.45\linewidth]{attention_map_694_affect_T-H.pdf}
	}
	\caption{\textbf{Affected areas} for the particular positions in the different depths of attention layer.}\vspace*{-0.1in}
	\label{affected_area}
\end{figure*}

\label{position embedding}
\begin{table}[h]
	\begin{center}
		\setlength{\tabcolsep}{2mm}
		\renewcommand{\arraystretch}{0.8}
		\begin{tabular}{c|ccc}
			\toprule
			Position Embedding & Params & FLOPs &AP   \\
			
			\midrule
			\ding{55}& 4.999M & 7.975G& 70.4  \\ 
			Learnable & 5.195M & 7.976G& 70.9 \\ 
			2D Sine (Fixed)& 5.195M&7.976G&71.7 \\ 
			\bottomrule
		\end{tabular}
	\end{center}
	\caption{Results for different position embedding schemes for TransPose models. The input size is .}
	\setlength{\belowcaptionskip}{-2pt}
	\label{position embedding tab}
\end{table}

\subsection{Ablations}
{\bf The importance of position embedding.} Without position embedding, the 2D spatial structure information loses in Transformer. To explore the importance of position embedding, we conduct experiments on TransPose-R-A3 models with three position embedding strategies: 2D sine position embedding, learnable position embedding, and no position embedding. As expected, the models with position embedding have better performances, particularly for 2D sine position embedding, as shown in Tab.~\ref{position embedding tab}. But note that TransPose without position embedding can also perform well with only losing 1.3 AP. The reason perhaps is that the sparse position encoding in the target heatmaps makes the 2D spatial structure less important. We also observe what has been learned in the learnable position embedding, by computing the cosine similarities between the vectors in any paired locations of the learnable position embedding. The results (see Appendix) indicate that each vector in the learnable embedding has similar values with its neighbours in the 2D grid, which means the coarse 2D position information is learned and exploited in the model. 


{\bf Position embedding helps generalize better on unseen input resolutions.} The top-down paradigm scales all the cropped images containing a single person to a fixed size. But for some cases even with a fixed input size or the bottom-up multi-person pose estimation paradigm, the scale of the human body in the input might be various, the robustness of handling with different scales is also important. To test the generalization of models, we design an extreme experiment: we test SimpleBaseline-ResNet50-DarkPose and TransPose-R-A3 models on unseen 12896, 384288, 512388 input resolutions, all of which only have been trained with 256192 input images. Interestingly, the results in Fig.~\ref{generalization_fig} demonstrate that SimpleBaseline and TransPose-R without position embedding have obvious performance collapses on these unseen input resolutions, particularly on 12896, but TransPose-R with learnable or 2D Sine position embedding have significantly better generalization, especially for the Sine position embedding with minimal performance degradation. We conjecture that 1) it is hard for the models with a fixed receptive field to adapt the changes in the feature scale; 2) building associations with \emph{relative position information} encoded in Sine position embedding~\cite{vaswani2017attention} may help the model generalize on different sizes of inputs or feature maps.

\begin{table}\small 
	
	\centering
	\label{table:coco_test_dev}
	\renewcommand{\arraystretch}{1}
	\setlength{\tabcolsep}{0.9mm}
	\begin{tabular}{c|c|cc|cccccc}
		\toprule
		Method & Input size & Params & FLOPs& 
		 &  &  &  &  & \\
		\midrule
	
		G-RMI~\cite{papandreou2017towards}  & 353257 &42.6M & 57.0G
		&64.9 & 85.5&71.3&62.3&70.0&69.7\\
		Integral~\cite{sun2018integral} & 256256 &45.0M &11.0G
		&67.8 & 88.2&74.8&63.9&74.0&-\\

		CPN~\cite{chen2018cascaded}& 384288 &- &-
		& 72.1 & 91.4&80.0&68.7&77.2&78.5\\
		RMPE~\cite{fang2017rmpe} & 320256 &28.1M &26.7G
		&72.3 & 89.2&79.1&68.0&78.6&-\\

		SimpleBaseline~\cite{xiao2018simple} &384288  &68.6M & 35.6G
		&73.7 & 91.9&81.1&70.3&80.0&79.0\\
HRNet-W32~\cite{sun2019hrnet} & 384288 &28.5M& 16.0G&74.9&92.5&82.8&71.3&80.9&80.1\\
		HRNet-W48~\cite{sun2019hrnet} & 384288 &63.6M&32.9G& 75.5&92.5&83.3&71.9&81.5&80.5\\
		DarkPose~\cite{zhang2020distribution} & 384288 &63.6M&32.9G& 76.2&92.5&83.6&72.5&82.4&81.1\\
		\midrule
		TransPose-H-S & 256192 &5.2M&10.3G& 73.4&91.6&81.1&70.1&79.3&78.6\\
		TransPose-H-A4 & 256192 &17.3M&17.5G& 74.7&91.9&82.2&71.4&80.7&79.9\\
		TransPose-H-A6 & 256192 &17.5M&21.8G& 75.0&92.2&82.3&71.3&81.1&80.1\\
		\bottomrule

		
	\end{tabular}
	\caption{Comparisons with state-of-the-art on COCO test-dev set.}\vspace*{-0.1in}
	
	\label{coco-test}
	
\end{table}

	\begin{figure}
	\begin{center}
\includegraphics[width=1.0\linewidth]{generalization.pdf}
	\end{center}\vspace*{-0.15in}
	\caption{Generalization performances on unseen input resolutions. SimpleBaseline and TransPose without Position Embedding (PE) generalize worse than TranPose with PE obviously.} \vspace{-0.1in}
	\label{generalization_fig}
\end{figure}

\subsection{Explainability Analysis} 

\label{Explainability Analysis}
In this section, we show the dependencies exhibited by the attention maps are dynamic across different trained models, types of predicted keypoints, depths of attention layers, and input images. We choose cropped images by GT boxes from COCO val dataset and use the trained models TP-R (TP-R-A4 model) and TP-H (TransPose-H-S model) with 75.1AP and 76.1 AP performances as exemplars, to make qualitative analysis by controlling variables.

{\bf The ranges of dependencies for predictions are longer in the model with higher performance.} The attention map in the final attention layer directly reflects the dependencies for predicting the keypoints locations. For example, comparing Fig.~\ref{tpr-a} with Fig.~\ref{tph-a}, although TP-R and TP-H predict exactly the same locations of keypoints in the input A, TP-H obviously exploits the image cues from the longer-distance joints to predict most keypoints. In contrast, TP-R prefers to attend to the short-range (local) image cues around the target keypoint. This characteristic can be further confirmed by the affected areas illustrated in Fig.~\ref{affected_area}, in which the keypoints have larger affected areas in TP-H. The larger receptive field and information fusion of HRNet-S might account for this.

{\bf Dependencies and influences vary for different types of keypoints.} Overall, the upper limb and the keypoints belonging to it, especially the head, have a greater impact on predicting positions of keypoints in the lower limb. Such influences are more obvious in TP-H, mainly due to its stronger long-distance capturing ability than TP-R model. As shown in Fig.~\ref{tph-a}, Fig.~\ref{tph-b}, Fig.~\ref{tph-c}, Fig.~\ref{tph-d}, and Fig.~\ref{Tp-H-dependency}, we can further observe that a \emph{well-performed model might gather more significant clues from more other parts to predict the target keypoint}.  We thus suppose that learning strong associations between accurate keypoints locations can reinforce the model to localize any one of them more accurately. This can explain why the model still can predict the location of an occluded keypoint accurately, and the occluded keypoint with ambiguity feature would have less impact on the other predictions, such as the left-ankle joint shown in Fig.~\ref{tpr-c} and Fig.~\ref{tph-c}. Moreover, the joints with a high degree of freedom, \emph{e.g.}, elbows, wrists, knees, and ankles, also depend on the nearby limbs and joints on the symmetrical side.  



{\bf Attentions gradually focus on the more fine-grained dependencies with the depth of layer increasing.}  Observing all attention layers, as shown in the 1,2,3-th rows of Fig.~\ref{Tp-H-dependency}, we surprisingly find that \emph{even without the intermediate GT locations supervision}, TP-H model can still attend to the accurate locations of joints in the early layers. For both models, with the depth of layer increasing, the predictions gradually depend on the more fine-grained body part image clues or keypoints positions, as shown in Fig.~\ref{tp-r-dependency} and Fig.~\ref{Tp-H-dependency}.

{\bf The dependencies slightly change for different input image context.} Different from the static dependencies encoded in the weights and bias of CNN after training, the attention maps are dynamic to inputs. By choosing a model alone and testing different inputs for it, \emph{e.g.}, input A, B, C or D for TP-R in Fig.~\ref{final_attention}, we can observe that despite the statistical commonalities on the dependency relationships for the predicted keypoints, the fine-grained dependencies would slightly change according to the image context. With the existence of occlusion or invisibility in a given image such as input D (Fig.~\ref{tph-c}), the model can still localize the position of the partially obscured keypoint by looking for more significant image clues and reduces reliance on the invisible keypoint to predict the other ones. Such a dynamic characteristic can depict the unique dependencies existing in an unique image when inferring.

\section{Conclusion}

In this work, we explored using Transformer encoder with low-level convolutional blocks to construct an explainable human pose estimation model. We analyzed and showed that the attention mechanism is very promising to capture and explain the image-dependent spatial relationships in this structure prediction task. With lightweight architectures, TransPose models match the state-of-the-art on COCO Keypoint Detection task that has been dominated by deep fully convolutional architectures, and there seems to have further space to improve the upper limit of model performance by expanding the size of TransPose. Furthermore, we validate the importance of position embedding, and our qualitative analysis reveals the image-specific and variable spatial dependencies for different layer depths, keypoints types, or trained models. We believe that these ideas could help researchers or users have a deeper understanding of how neural networks make structural predictions for human pose estimation task, and further improve the model design. 




{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\appendix
\section{What position information has been learned in the TransPose model with learnable position embedding?}

\begin{figure}[h]
\begin{center}
\includegraphics[height=0.5\linewidth]{cosine_similarities_learnable_pos_embedding.pdf}
\end{center}
\caption{The cosine similarities between the learned position embedding vectors, which have been reshaped into 2D grid and interpolated with 0.25 scale factor for a better illustration (the original shape is ). Each map in -row and -col of the figure represents the cosine similarities between the embedding vector in position  and the embedding vectors at other locations.}
\label{lpe}
\end{figure}

We show what position information has been learned in the TransPose (TransPose-R) with learnable position embedding. It has been discussed in the Section 4.2 of the paper. As shown in Fig.~\ref{lpe}, we visualize the similarities by calculating the cosine similarity between vectors at any pair of locations of the learnable position embedding and reshaping it into a 2D grid-like map. We find that the embedding in each location of learnable position embedding has a unique vector value in the -dim vector space, but it has relatively higher cosine similarity values with the neighbour locations in 2D-grid and lower values with those far away from it. The results indicate the coarse 2D position information has been implicitly learned in the learnable position embedding. We suppose that the learning sources of the position information might be the 2D-structure groundtruth heatmaps and the similar features existing in the 1D-structure sequences. The model learns to build associations between position embedding and input sequences, as a result it can predict the target heatmaps with 2D Gaussian peaking at groundtruth keypoints locations.


\section{2D Sine Position Embedding}
We follow the sine positional encodings but further hypothesize that the position information is independent at  (horizontal) and  (vertical) direction of an image. Concretely, we keep the original 2D-structure respectively with  channels for -direction:

where ,  or  is the position index along  or -direction. Then they are stacked and flattened into a shape .


\section{Transformer Encoder Layer}
The Transformer Encoder layer~\cite{vaswani2017attention} we used can be formulated as:

where  is the original input sequence that has not yet been added with position embedding. The position embedding will be added to  for computing querys and keys.   is the output sequence of the current Transformer Encoder layer, as the input sequence of next encoder layer.	 The formulations of Multihead Self-Attention and FFN are defined in~\cite{vaswani2017attention}.

\section{Detailed Results on COCO test-dev2017 Set}
In Tab.~\ref{appendix::coco-test}, we show the results on COCO test-dev2017 set on AP (Average Precision) and AR (Average Recall) metrics, which are reported by the COCO official server.


\begin{figure*}[h]
\centering
\subfigure[\textbf{TP-R:} predictions and dependencies of each keypoint for \textbf{input 1}.]{
	\includegraphics[width=0.45\linewidth]{final_attention_map_204_T-R-A4.pdf}
	
}
\quad
\subfigure[\textbf{TP-H:} predictions and dependencies of each keypoint for \textbf{input 1}.]{
	\includegraphics[width=0.45\linewidth]{final_attention_map_204_T-H.pdf}
	
	
}
\quad
\subfigure[\textbf{TP-R:} predictions and dependencies of each keypoint for \textbf{input 2}.]{
	\includegraphics[width=0.45\linewidth]{final_attention_map_307_T-R-A4.pdf}
	
}
\quad
\subfigure[\textbf{TP-H:} predictions and dependencies of each keypoints for \textbf{input 2}.]{
	\includegraphics[width=0.45\linewidth]{final_attention_map_307_T-H.pdf}
	\label{tph-x}
}


\subfigure[\textbf{TP-R:} predictions and dependencies of each keypoint for \textbf{input 3}.]{
	\includegraphics[width=0.45\linewidth]{final_attention_map_394_T-R-A4.pdf}
	
}
\quad
\subfigure[\textbf{TP-H:} predictions and dependencies of each keypoints for \textbf{input 3}.]{
	\includegraphics[width=0.45\linewidth]{final_attention_map_394_T-H.pdf}
}



\caption{In each sub-figure, the first one is the original input plotted with predicted skeleton. The other maps are the unfolded attention maps in the \text{last layer} of TP-R and TP-H inspected by the predicted locations of keypoints, each position of which is annotated by a white pentagram. All the maps are visualized by the unnormalized attention maps.}
\label{final_attention}
\end{figure*}

\begin{figure*}
\centering
\subfigure[\textbf{TP-R:} predictions and dependency areas of each keypoint in different attention layers.]{
	\includegraphics[width=0.45\linewidth]{attention_map_91_dependency_T-R-A4.pdf}
}
\quad
\subfigure[\textbf{TP-H:} predictions and dependency areas of each keypoint in different attention layers.]{
	\includegraphics[width=0.45\linewidth]{attention_map_91_dependency_T-H.pdf}
}
\subfigure[\textbf{TP-R:} predictions and dependency areas of each keypoint in different attention layers.]{
	\includegraphics[width=0.45\linewidth]{attention_map_123_dependency_T-R-A4.pdf}
}
\quad
\subfigure[\textbf{TP-H:} predictions and dependency areas of each keypoint in different attention layers.]{
	\includegraphics[width=0.45\linewidth]{attention_map_123_dependency_T-H.pdf}
}
\subfigure[\textbf{TP-R:} predictions and dependency areas of each keypoint in different attention layers.]{
	\includegraphics[width=0.45\linewidth]{attention_map_286_dependency_T-R-A4.pdf}
}
\quad
\subfigure[\textbf{TP-H:} predictions and dependency areas of each keypoint in different attention layers.]{
	\includegraphics[width=0.45\linewidth]{attention_map_286_dependency_T-H.pdf}
}
\subfigure[\textbf{TP-R:} predictions and affected areas of each keypoint in different attention layers.]{
	\includegraphics[width=0.45\linewidth]{attention_map_53_affect_T-R-A4.pdf}
}
\quad
\subfigure[\textbf{TP-H:} predictions and affected areas of each keypoint in different attention layers.]{
	\includegraphics[width=0.45\linewidth]{attention_map_53_affect_T-H.pdf}
}
\caption{\textbf{Dependency areas} and \textbf{Affected areas} for different input images.}
\label{attention layers}
\end{figure*}

\section{Architecture Details}
We report the architecture details of ResNet-S and HRNet-S-W32(48) in Tab.~\ref{resnet} and Tab.~\ref{hrnet}. The ResNet-S* only differs from ResNet-S in that ResNet-S* has 10 Bottleneck-c128 blocks. More details about HRNet-W32 and HRNet-W48 are described in~\cite{sun2019hrnet}.


\begin{table}
\renewcommand{\arraystretch}{0.9}
\centering

\begin{tabular}{c|c}
	\toprule[0.1em]
	
	Backbone &ResNet-S\\
	\midrule \multirow{2}{*}{Stem}&Conv-k7-s2-c64, BN, ReLU\\
	&Pooling-k3-s2\\
	\hline
	\multirow{4}{*}{Blocks}& 3Bottleneck-c64\\
	&  Conv-k3-s2-c128, BN \\
	&  4Bottleneck-c128\\
	& Conv-k1-s1-c256 \\
	
	\bottomrule[0.1em]
\end{tabular}
\caption{The detailed configurations for ResNet-S. Conv-k7-s2-c64 means a convolutional layer with 77 kernel size, 2 stride, and 64 output channels, followed by a BN and ReLU; the same below. The Bottleneck-c64 includes Conv-k1-s1-c64-BN-ReLU, Conv-k3-s1-c64-BN-ReLU, and Conv-k1-s1-c256-BN. Bottleneck-c128 includes Conv-k1-s1-c128-BN-ReLU, Conv-k3-s1-c128-BN-ReLU, and Conv-k1-s1-c512-BN. See details in~\cite{he2016deep}.}
\label{resnet}
\end{table}

\begin{table}
\renewcommand{\arraystretch}{0.9}
\centering
\begin{tabular}{c|c}
	\toprule[0.1em]
	
	Backbone &HRNet-S-W32(48)\\
	\midrule \multirow{3}{*}{Stem}&Conv-k3-s2-c64, BN, ReLU\\
	&Conv-k3-s2-c64, BN, ReLU\\
	&4Bottleneck-c64\\
	\hline
	\multirow{3}{*}{Blocks}& transition1stage2\\
	&  transition2stage3 \\
	& Conv-k1-s1-c64(92) \\
	
	\bottomrule[0.1em]
\end{tabular}
\caption{The detailed configurations for HRNet-S-W32(48). More detailed information about the transition layer and stage blocks are described in the HRNet paper~\cite{sun2019hrnet}.}
\label{hrnet}
\end{table}
\begin{table*}\scriptsize
\centering
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{0.5mm}
\begin{tabular}{c|c|cc|ccccc|ccccc}
	\toprule
	Method & Input size & Params & FLOPs& 
	 &  &  &  &  & &  &  &  & \\
	\midrule
	TransPose-H-S & 256 192 &5.2M&10.3G& 73.4&91.6&81.1&70.1&79.3&78.6&95.0&85.6&74.5&84.3\\
	TransPose-H-A6 & 256192 &17.5M&21.8G& 75.0&92.2&82.3&71.3&81.1&80.1&95.4&86.7&75.9&85.9\\
	\bottomrule
\end{tabular}
\caption{Comparisons with state-of-the-art on COCO test-dev set.}
\label{appendix::coco-test}
\end{table*}


\begin{table}
\centering
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{1mm}
\begin{tabular}{c|ccc|ccc|cc}
	\toprule
	Model & \#Layers &  &  & Params & FLOPs& FPS& AP & AR\\
	\midrule
	
	\multirow{5}{*}{TransPose-R}&2& 256& 1024& 4.4M & 7.0G & 174 & 69.6 & 75.0\\
	&3& 256& 1024 & 5.2M & 8.0G & 141 & 71.7 & 77.1 \\
	&4& 256& 1024 & 6.0M & 8.9G & 138 & \textbf{72.6} & \textbf{78.0} \\
	&5& 256& 1024 & 6.8M & 9.9G & 126 & 72.2 & 77.6 \\
	&6& 256& 1024 & 7.6M & 10.8G & 109 & 72.2 & 77.5 \\
	\midrule
	\multirow{5}{*}{TransPose-H}&4& 64& 128 & 17.0M & 14.6G & - & 75.1 & 80.1 \\
	&4& 192& 384 & 18.5M & 27.0G & - & 75.4 & 80.5 \\
	
	&4& 96& 192 & 17.3M & 17.5G & 41 & 75.3 & 80.3 \\
	&5& 96& 192 & 17.4M & 19.7G & 40 & 75.6 & 80.6 \\
	&6& 96& 192 & 17.5M & 21.8G & 38 & \textbf{75.8} & \textbf{80.8} \\
	\bottomrule
\end{tabular}
\caption{Ablation study on the hyperparameters of Transformer Encoder. \#Layers,  and  are the number of encoder layers, the dimensions , and the number of hidden units of FFN. }\vspace*{-0.1in}
\label{layers}
\end{table}




\section{Ablation Study on the Size of Transformer Encoder}

We analyze the effect of the size of encoder layers on the model performances, as shown in Tab.~\ref{layers}. For TransPose-R models, with the number of layers increasing to 6, the performance improvement gradually tends to saturate or degenerate. But we have not observed such a phenomenon on TransPose-H models (base on HRNet-S-W48). We partially attribute this to the limited model capacity of TransPose-R based on the very lightweight backbone ResNet-S. And there seems to have a room to improve the performance of TransPose-H by expanding the model sizes, \emph{e.g.}, increasing the number of attention layers, the dimensions , or hidden units in FFN. 


\section {Gradient Analysis}
\label{appendix::grad}
As revealed by~\cite{simonyan2013deep, bach2015pixel, selvaraju2017grad}, the gradient information indicates how much importance a token  (feature vector) at location  affects the final prediction in the heatmap. That assumption is based on that tiny change in the input token with the most important feature value causes a large change in what the output of the model would be. 

Assuming that  is the scores for all  types of keypoints at location ;  is the intermediate feature outputted by the last attention layer before being fed into FFN. There is only a ReLU excluding the linear and convolutions (head) layers after the last attention layer. ReLU (rectified linear unit) activation function in FFN can be empirically regarded as the negative contribution filter, which only retains positive contributions. Next we choose numerator layout for computing the derivative of a vector with respect to a vector.  We thus assume the mapping from  to  can be approximated as a linear function  with learned weights  and bias  by computing the first-order Taylor expansion at a given point , \emph{i.e.}, , .   Then we compute the derivative of  w.r.t the token  at location  of the input sequence of the last attention layer:

 where  is the value vector transformed from .  is a scalar value that is computed by the dot-product between  and . We assume  as a function w.r.t. a given attention score , then:

where  are static weights shared across all positions.  We can see that how  affects  could be approximated as a linear combination of dynamic weights -- attention score  and learned static weights, \emph{i.e.}, the subsequent layers almost linearly and equally transform the attention scores from tokens across all the positions. 
The last attention layer in Transformer Encoder, whose attention scores are seen as the image-dependent weights, has the most direct effect on the predictions. Though the weights in FFN or head cannot be ignored, they are image-independent during inference and shared across all locations. Note that  where  is the position embedding. Because , the position embedding values also affect the attention score to some extent.  This section is associated with Section~\ref{paper::grad}.  



\section{More Attention Maps Visualizations}
In this section, we show more visualization results of the attention maps from TP-R (TransPose-R) and TP-H (TransPose-H-S) models. The attention maps of the last attention layers of two models are shown in Fig.~\ref{final_attention}. The attention maps in different attention layers of two models are shown in Fig.~\ref{attention layers}.



\end{document}
