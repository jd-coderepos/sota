





\documentclass[11pt]{article}


\usepackage{amsthm}
\usepackage{amsmath}
\usepackage[boxed]{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath, amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsxtra}
\parindent 0pt
\parskip .1in
\setlength{\textheight}{8.8in}
 \addtolength{\oddsidemargin}{-.45in} \setlength{\textwidth}{6.0in}
 \setlength{\topmargin}{-.4in}



\newcommand{\tensor}{\otimes}
\newcommand{\calO}{{\cal{O}}}
\newcommand{\e}{ \ = \ }
\newcommand{\leqs}{ \ \leq \ }
\newcommand{\geqs}{ \ \geq \ }
\newcommand{\per}{\ \mathrm{per}}
\newcommand{\ldotdot}{\,.\,.\,}


\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{claim}{Claim}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exs}[thm]{Example}
\newtheorem{exss}[thm]{Examples}
\newtheorem{cor}[thm]{Corollary}


\newtheorem{rems}[thm]{Remarks}
\newtheorem{ques}{Question}
\newtheorem{conj}{Conjecture}
\newtheorem{dfn}[thm]{Definition}




\newcommand{\rnn}{\mathbb R^{n,n}}
\newcommand{\be}{}
\newcommand{\bd}{}
\newcommand{\nek}{,\ldots,}
\newcommand{\cnn}{\mathbb C^{n,n}}
\newcommand{\match}{\mathrm{match}}


\newcommand{\ci}{\mathbb C}
\newcommand{\zi}{\mathbb Z}
\newcommand{\R}{\mathbb R}

\newcommand{\eps}{\varepsilon}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\lemref}[1]{Lemma \ref{#1}}
\newcommand{\thmref}[1]{Theorem \ref{#1}}
\newcommand{\propref}[1]{Proposition \ref{#1}}

\newcommand{\mtrx}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\reff}[1]{(\ref{#1})}
\def\reff#1{{\rm
(\ref{#1})}}
\long\def\symbolfootnote[#1]#2{\begingroup \def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}




\newcommand{\envert}[1]{\left\lvert#1\right\rvert}
\let\abs=\envert

\newcommand{\enVert}[1]{\left\lVert#1\right\rVert}
\let\norm=\enVert



\newcommand{\bfE}{\textbf{E}}
\newcommand{\bfe}{\textbf{e}}
\newcommand{\Prob}{\Pr}
\newcommand{\dist}{\textbf{d}}
\newcommand{\Dist}{\textbf{Dist}}

\newcommand{\junk}[1]{}


\begin{document}
\title{Approximating General Metric Distances Between a Pattern and a Text}






\author{
Ely Porat\thanks{Department of Computer Science, Bar-Ilan
University, 52900 Ramat-Gan, Israel; {\tt porately@cs.biu.ac.il}}
\and Klim Efremenko\thanks{Department of Computer Science, Bar-Ilan
University, 52900 Ramat-Gan, Israel } \thanks{ Weizmann Institute,
Rehovot 76100, Israel}}





\maketitle


\begin{abstract}
Let  be a text and 
a pattern taken from some finite alphabet set , and let
 be a metric on . We consider the problem of
calculating the sum of distances between the symbols of  and
the symbols of substrings of  of length  for all possible
offsets. We present an -approximation algorithm for
this problem which runs in time .










\end{abstract}


\setlength{\parindent}{0.0in} \setlength{\parskip}{0.1 in}
\section{Introduction \label{sec:intro}}
\emph{String matching}, the problem of finding all occurrences of a given
pattern in a given text, is a classical problem in computer
science. The problem has pleasing theoretical features and a
number of direct applications to ``real world'' problems.



 Advances in multimedia, digital libraries, and computational
biology, have shown that a much more generalized theoretical basis
of string matching could be of tremendous
benefit~\cite{Pentland-92,olson:95}.
 To this end, string matching has had to adapt itself to
increasingly broader definitions of ``matching''. Two types of
problems need to be addressed -- {\it generalized matching} and
{\it approximate matching}. In generalized matching, one seeks all
exact occurrences of the pattern in the text, but the ``matching''
relation is defined differently. The output is all locations in
the text where the pattern ``matches'' according to the new
definition of a match. The different applications define the
matching relation. Examples can be seen in Baker's {\it
parameterized matching}~(\cite{bak:93}) or Amir and Farach's {\it
less-than matching}~(\cite{AF-95}). The second model, and the one
we are concerned with in this paper, is that of {\em approximate
matching}. In approximate matching, one defines a distance metric
between the objects (e.g.\ strings, matrices) and seeks to
calculate this distance for all text locations. Usually we seek
locations where this distance is small enough.

One of the earliest and most natural metrics is the {\it Hamming
distance}, where the distance between two strings is the number of
mismatching characters exists algorithm calculating this distance
exactly ~\cite{ALP0:00} and approximating it ~\cite{karloff:93}.
Levenshtein~\cite{L-66} identified three types of errors:
mismatches, insertions, and deletions. These operations are
traditionally used to define the {\it edit distance} between two
strings. The edit distance is the {\it minimum} number of edit
operations one needs to perform on the pattern in order to achieve
an exact match at the given text location. Lowrance and
Wagner~\cite{lw-75,wagner-75} added the {\it swap} operation to
the set of operations defining the distance metric.
 Much of the recent research in string matching
concerns itself with understanding the inherent ``hardness'' of
the various distance metrics, by seeking upper and lower bounds
for string matching under these conditions.

A natural subset of these problems is when the distance is defined
only on the alphabet, and therefore, the distance between two
strings is the sum of the distances between the corresponding characters
in both strings. It is possible to solve this problem in time
 by employing the naive approach of summing the
distances between each character of the pattern, and its
corresponding character in the text, for each possible alignment
of the pattern. This problem was first defined by Muthukrishnan
in~\cite{muthu-open} and has been open since. In this paper we
present an approximation algorithm for this problem.

This algorithm
 consists of two parts: the first part is a preprocessing phase in which
random hash functions on the alphabet is constructed. We use same
hashing which Bartal used for tree embedding at ~\cite{Bar96}. We
use this hashing in order to separate the places where distance
between letters is large and places where this distance is small.



 The second part of the algorithm is an application of sampling~(\cite{CPER07}),
which allows us to give an approximation of the distance between
the text and the pattern, in time .

The contributions of this paper are twofold: on the technical
side, we have solved a problem that has been {\em open} for over a
decade, by presenting the {\em fastest} known approximation
algorithm for many metrics; additionally, and this is perhaps the
more important contribution of this paper, we have identified and
exploited a new technique -- {\it sampling}, that has been used in
some recent papers~(\cite{CPER07}) only implicitly. We employ
sampling in a much more sophisticated manner and show how to use
this important tool for approximating distances. We also present a
novel way of using embeddings and geometry tools in pattern
matching. This technique possesses a wide range of applications.
For example, one can easily extend it to calculate the
-norm distance (in other words, when

is the distance measure), or it can be extended for many infinite
metrics. Our algorithm also allows for symbols in a text to be
wildcards.


We believe that this new method for solving approximate string matching
problems -- embedding metric in some suitable space and sampling
-- may actually yield efficient algorithms for many more problems in the future.























\section{Problem definition and Preliminaries }\label{s:def}
\begin{dfn} A {metric space}
 is a pair  where  is a set of points and  is a {\em metric} satisfying the
following axioms:

\end{dfn}
Let  be a metric space, and  , be any two strings of the same
length with symbols from . We define the \emph{distance}
of  and  by .
Given a text  and a pattern , our goal is to calculate the array , for each possible offset . Calculating the exact values of  can be done in 
time, using the naive approach, In most cases it is enough to know
only an approximation of the distance; we therefore present an
efficient algorithm which approximates the values of .




Convolutions can be used in the standard fashion to improve the time
for finite fixed alphabets.
\begin{dfn}  \label{def:FFT}
Let  and  be arrays of
natural numbers.The {\em discrete convolution (polynomial multiplication) of  and
} is  where:
 where
. We denote  as . We will choose  and
, i.e.\ we will treat  and  as the coefficients of
polynomials of degrees  and , respectively.

By standard tricks, namely, (1) reversing the text to obtain ;
(2) calculating ; (3) reverse the result; (4) discard the
first  values, and last  values of the result, we obtain
an array  where for each ,

In other words, for every possible offset ,  is the sum of
the pattern symbols multiplied, each with its corresponding text
symbol. For convenience, we define 
\end{dfn}
A convolution can be computed in time , in a
computational model with word size , by using the Fast
Fourier Transform (FFT)~\cite{CLR-92}.
\begin{rem}
Using FFT we can compute general pattern distances, in time
, by using the following method: for
every , we define an array  by setting
, where  if  and 
otherwise. Set . Computing  gives us the sum of the distances of the letter  from
the text. The sum of all convolution results is the desired
distance.
\end{rem}

We provide some required definitions regarding metric spaces:
\begin{dfn}Let  be a mapping , where
 and  are metric spaces.  is an
\emph{isometry} if .
\end{dfn}
Unfortunately, we cannot always construct an isometry. Therefore we
consider weaker conditions:
\begin{dfn}
\label{def:D-emb} Given two metric spaces  and
, and a value , a mapping  is called a {\em c-distortion embedding}, if for all ,
\be \dist_1(x,y)\leq \dist_2(\rho(x),\rho(y))\leq
c\cdot\dist_1(x,y).
 \ee
\end{dfn}






\section{The One-Mismatch Algorithm}
In this section we describe the \emph{one-mismatch} algorithm,
in itself a very useful general tool in pattern matching. The
one-mismatch algorithm had been described before in
~\cite{CC07,CPER07}.

Given a numeric text  and a numeric pattern , and we want to
find exact matches of  in . One way to do so is by
calculating for each location  the value:

Notice this sum will be zero iff there is an exact match at location
. Furthermore this sum can be computed efficiently for all 's
in  time using convolutions.  Notice that if 
and  are not numeric then an arbitrary one-to-one mapping can be
chosen from the alphabet to the set of positive integers
.

This method can be extended to the case of matching with ``don't
cares''~\cite{CC07}, by simply calculating instead

where  (resp.\ ) if  (resp.\ ) is a ``don't
care'' symbol and  otherwise. Wherever there is an exact match with
``don't cares'' this sum will be exactly . This can also be computed
with convolutions in time .

Again, this scheme can be further extended to the one-mismatch
problem, which is to determine if  matches 
with at most one mismatch. Furthermore, we can identify the location
of the mismatch for each such . This is done by also computing,
for each ,

by using the convolution. Then if  matches the text at offset
 with one mismatch then eventually  and
 where  is a location of a mismatch.
Therefore, by calculating , we find the supposed
mismatch location and verify it. Finally, locations where exact
matches occur will be labeled ``match'', location where a single
mismatch occurs will be labeled with its location, and location
where more than one mismatch occurs will be labeled . The
one-mismatch algorithm is therefore as follows:
\begin{enumerate}
    \item compute the array      using FFT.
    \item  compute the array  using FFT.
    \item  If  set  else set
    .
    \item For each  s.t.\ , check to see if
    . If this is not the case then
    set .
\end{enumerate}
The running time of this algorithm is .

\section{The Sampling Method}\label{s:naive}
In this section we present a general method referred to as the
\emph{sampling} method. It allows us, for every possible offset,
to sample (i.e.\ choose) a {\em random mismatch} from the set of all
mismatches w.r.t.\ this offset.
We show how to utilize the previously described algorithm for this
purpose.

First fix some probability , and define subpattern
 of  by:

\be \label{ri}
 p^{*}_i=
 \begin{cases}
 p_i  & \mbox{with probability } q;\\
\phi \mbox{(don't care)}             & \mbox{otherwise.}
 \end{cases}
 \ee

In the algorithm referred to as Sample(), we simply create
 as defined in~\eqref{ri} and run the one-mismatch algorithm on
 and . Now, for every offset , let  be the number of
mismatches between  and  w.r.t.\ this offset. The following
lemma trivially follows:

\begin{lem}\label{sample}
Let  be the array returned by Sample(). For some
location ,

and

\end{lem}

Another important property of this algorithm is that the mismatch
returned w.r.t.\ offset  is uniformly distributed over the set
of all mismatches w.r.t.\ offset . We will show how to use this
algorithm to sample a random location for which the distance is
not . Notice that for , the probability
of finding a mismatch w.r.t.\ offset  is . Therefore,
we can enumerate on . Then, for
every location  there exists some  which is
. Therefore, the next algorithm finds for
each location a mismatch with constant probability, which is
uniformly distributed over all the mismatches.
\begin{enumerate}
\item {\bf{for}} ;  ;  \item \ \ \
Sample(q,T,P) \item For every offset  if a mismatch is found
return it.
\end{enumerate}





\section{Motivation for the Algorithm}\label{s:Motiv}
\begin{rem}
In this paper we assume that the ratio of maximal and minimal
distances is bounded by . Therefore, w.l.o.g.\ we can
assume that the minimal nonzero distance is , i.e.\  and . That is because if  is the minimal
distance, then we can use the metric 
instead.
\end{rem}
A first naive approach to approximate the distance is as follows:
say we wish to provide an approximation only for some offset ,
and let  be a random variable which is equal to
, where  is chosen uniformly from
. We can sample  by choosing a random  and
calculating . The expectation of  is
the desired sum. Therefore, the way to compute  is sample
 several times and return the average. The problem with this
approach is that the variance of  may be very large: for example,
if  matches  except for a few mismatches, then w.h.p.\ we will
not sample even a single mismatch.

The second attempt to reduce the variance of variable , is to
use the sampling algorithm described in Sect.~\ref{s:naive}. As a
result,  will be distributed only over locations where
. That is because the sampling algorithm
returns only relative locations  for which .
This sampling approach reduces the variance of , but still it
may happen that for some offset , all distances are very small
except for a single one which is even greater than the sum of all
others. With high probability, only the smaller distances will be
sampled, thus affecting the final outcome. This approach can
provide us with an algorithm which runs in  time, however,
 may be very large.

All the above leads us to search for a way to sample only
locations  for which, when some  is fixed, . Then, with an additional multiplicative
factor of , we can enumerate on
, each step approximating the
expectation of the variable  which uniformly ranges over . Notice
that for any value ,

Hence it follows that

A hypothetic way to sample  would be to design a mapping
 on  for which

If so, we could have run the sampling algorithm on
 and , applying  in
the obvious way, and obtain samples of . Then, the average of
sampled distances will approximate . The approximation
of  would have been also simple: it is a number
of approximated mismatches divided by  (i.e.\ the length of the
pattern). Unfortunately, we cannot design such a mapping. However,
we can design a set of mappings such that for a random mapping,
this condition holds with high probability.



\section{Probabilistically Separating Hashing} \label{s:mapping}
 In this section, our goal is to construct a random
hash  for a given  such that \eqref{Mapping} holds with
good probability. The set of hash functions  called
-\emph{Probabilistically Separating Hashing }if it admits next
two conditions:
\begin{enumerate}
\item If the distance between  is greater than , then they
their hashing is different  i.e.\

  .
\item  .
\end{enumerate}

Bartal at ~\cite{Bar96} gave a construction of -{Probabilistically Separating Hashing } for finite
metrics after it was extended for graphs embedded in real normed
spaces at ~\cite{CCGGP98}. In section ~\ref{explicit} we will give
a simple construction for the case when alphabet is normed space
 with small .

Notice that we only need to build such a hashing only once for
every alphabet. Therefore it can be done as a preprocessing
measure.



We are able to use  in order to sample a subset of indices
for which the distance is not too small. We will now show that this
will also allow us to sample  as we desire.

\begin{lem} \label{map}
Fix an offset . Let 
be the set of mismatches under  and  be the set of indices we are really
interested in sampling from them. Then:
\begin{enumerate}
\item  (where the
expectation is over the choice of )
\item  and .
\end{enumerate}
Where  and .
\end{lem}
\begin{proof}
By linearity of expectation:

By definition of Probabilistically Separating Hashing we have:

 This proves (1).

 follows from theorem ~\ref{emmbed} and
 but
 for  so

\end{proof}

\section{The Algorithm}
At this point, we have all the tools necessary in order to describe
the algorithm. The algorithm is based upon the application of
sampling algorithm, described previously,
 to the -probabilistically separating hash provided in Sect.~\ref{s:mapping}.
 As a preprocessing phase, we construct for the metric space , samples of hashing  for .
The preprocessing algorithm therefore gets a metric space
, where  is the alphabet and  is
the metric on it, and produces the
 hash
functions  chosen at random from .

The main (i.e.\ query) algorithm gets a text  and a pattern  over the alphabet
. For a fixed offset  the result will be in
 with probability
. The output of the algorithm is an array  where  is an -approximation to
 i.e:

We will now outline the idea of the algorithm. We want to approximate

We will enumerate , increasing it each time by a factor of 2,
and approximate . Fix some  and
some offset , let as before  , where  depends on the random mapping ,
and . Recall that , and that  is not too
small.

In order to approximate  we will use the sampling
algorithm on  and . We get a random element in
, and we check if this element is also in . In order to
approximate  we average the distances of elements found
in .

In order to approximate  we use lemma
\ref{sample}. The probability that the sampling algorithm returns
``match'' is , and the probability that
it returns a mismatch from the set  is
. So,
.

Let's assume that we run the sampling algorithm  times; then the total
number of matches is  and the total number of
elements in  is . Let  be an array of the
elements in  which were found, including repetitions of elements
from . .

We will approximate  by  because:
\be \label {1eq}
\abs{B} = \frac{\bfE(m_1) (1-q)}{q \bfE(m_0)}\ ,
\ee
 and approximate  by . Therefore:


We will need to show that this approximation is narrow, i.e. that
the variance of the approximation is small. In order to do so, we
will need to choose  s.t. .
In order to find such a , we try a series of 's, increasing
by a factor of 2 each time, and choose  s.t.  is large
enough and  is maximal. We prove that this produces a good
 w.h.p.

We now write the complete algorithm. Set .


\algsetup{indent=2em}
\begin{algorithm}[H]
\caption{General distance algorithm} \label{alg:general_distance}
\begin{algorithmic}[1]
    \FOR { ; ;  }\label{FOR1}
       \FOR{ ;  ;  }\label{FOR2}
        \FOR {; ; } \label{FOR3}
            \STATE Choose a random 
\STATE run Sample(). Save the result as the -th result for this .
        \ENDFOR
    \ENDFOR
        \FORALL {offset in text  }
            \STATE Calculate  for all 's - the number
            of matches
            \STATE Among all  such that  choose  s.t.\  is maximal

\STATE Set  to be the set of distances between  and  for this .
        \STATE Calculate
        


    \ENDFOR
  \ENDFOR
  \STATE for every offset  return 
\end{algorithmic}
\end{algorithm}

The running time of this algorithm is:
.
This is because the running time is mostly dominated by the
Sampling function, which takes  time. The
Sampling function is executed
 times.
\begin{thm}
For every offset 
\be
\Pr\left(\abs{R(i)-\sum_{j=0}^{m-1}\dist(p_j,t_{i+j})}\geq \varepsilon R(i)\right) \leq \bfe^{-t}\ ,
\ee
or in other words our algorithm returns -approximation w.h.p.
\end{thm}
The proof of this theorem appears in the appendix.


\section{Explicit hashing constructions for normed spaces}{\label{explicit}}
Now in this section let us construct explicit
-Probabilistically Separating Hashing for normed space
 with  norm. The
main problem with previous constructions ~\cite{CCGGP98} is that
this hashing can't be calculated efficiently and usually it takes
 time to calculate one hash function. In case that
points not given in advance this may be bottleneck of the
algorithm.
 An other reason why this construction important is:
  if we have  -Probabilistically Separating Hashing for space  and we have embedding  with distortion 
  then we can construct --Probabilistically Separating Hashing for space .
  The problem of embedding metric spaces to real normed spaces where deeply investigated.

Our construction is the same for every norm .
 Let  be a vector
of  independent random variables with uniform distribution on
. Define:



\begin{thm}\label{emmbed}
The above mapping  satisfies the next properties:
\begin{enumerate}
\item If the distance between  is greater than , then their mapping
  is different i.e.\
  .
\item  .
\end{enumerate}
\end{thm}
\begin{proof} As follows:
\begin{enumerate}
\item This is  trivial:

\item If  then this inequality is trivial.
Therefore assume  :

\end{enumerate}
\end{proof}
We choose to use  as our embedding, and notice  is easy
to calculate, assuming we already have the -embedding .
This calculation can be done in  time.
\begin{rem}
Consider the set . While each
of its members is a vector of length , when comparing these
vectors we are only interested in checking equality. Therefore, in
order to save space, we can replace each vector with a unique
number in .
\end{rem}
\section{Conclusions}
We have presented the first non-trivial algorithm for the
approximation of a large class of distances between text and
pattern. We believe that the techniques we have presented here have a wide
range of applications. A further interesting open question is to
generalize these techniques to the case where the distance is not necessary a metric.



\bibliographystyle{plain}
\begin{small}
\bibliography{paper}
\end{small}
\appendix
\section{The proof of the algorithm}
\begin{rem}
Here w.h.p. mean with probability more then 
\end{rem}
We will now prove that the algorithm indeed approximated the
distances for each  w.h.p. We will only sketch
the proof.
\begin{proof}(of Algorithm)
Fix some offset . Then for every  we set  and  two sets. Notice that  is a random variable.
\begin{claim}\label{1claim}
W.h.p.\ for every  there exist  s.t.\  and 
\end{claim}
\begin{proof}
There exist  s.t.\ . The probability of a match for this  is
 by Jensen's inequality
.
  have binomial distribution  and so w.h.p.
. For this   it also holds that . So for the  that the algorithm
chose also holds that .
\end{proof}
\begin{claim}
There exist a constant . Such
that.
 w.h.p.
\end{claim}
\begin{proof}
This follows from the Chernoff bound. This is because  is binomially
distributed variable  with .
\end{proof}
Lets  be a constant and 
\begin{claim}
 is close to  w.h.p.\ i.e.

\end{claim}
\begin{proof}
We can represent  as:

 
By the previous claim   is close to .
\end{proof}
\begin{claim}\label{expectation}

\end{claim}
\begin{proof}

 But we know that:


By \eqref{1eq} we have that: 
So we have that:

\end{proof}
\begin{claim}\label{bound}
There exists a universal constant   s.t.\ w.h.p.\ :

holds w.h.p.
\end{claim}
\begin{proof}
The previous claim showed us that  because  for . Therefore, it's enough to prove that \ .
By ~\ref{1claim} we know that . So we have:
\be
c_D=\frac{(1-q(D))D}{q(D)\tilde{m_0}(D)}\leq \frac{C \bfE(\abs{A})D}{K}\ .
\ee
By ~\ref{map} .  So
\be
\frac{C \bfE(\abs{A})D}{K}=\calO(\frac{\varepsilon^2}{t}S)
\ee
\end{proof}
We will state the
following lemma without proving it because it follows from the Chernoff bound:
\begin{lem}\label{e-bound0}
There exists a universal constant  s.t. for every sequence of
independent random variables  with , and a sequence of positive constants 
s.t. . Then:

\end{lem}
\begin{claim}
\be \Pr(\abs{\tilde{S}-\sum_{j=0}^{m-1} \dist(p_i,t_{i+j})} \geq
\varepsilon \tilde{S}) \leq \bfe^{-t}. \ee
\end{claim}
\begin{proof}
 by definition , by ~\ref{expectation}  by ~\ref{bound} follows that  and therefore ~\ref{e-bound0} proves the claim
\end{proof}

\end{proof} 




\end{document}
