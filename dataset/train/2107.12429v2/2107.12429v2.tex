\section{Experiments}

\noindent\textbf{Datasets.} We evaluate the proposed framework \textbf{\textit{MonoIndoor}} on two challenging indoor datasets: the EuRoC MAV~\cite{schonberger2016structure} dataset, the NYUv2 depth dataset~\cite{Silberman:ECCV12} and RGB-D 7-Scenes dataset~\cite{Shotton_2013_CVPR}. 

\noindent\textbf{Evaluation Metrics.} For evaluation, we follow~\cite{eigen2014depth} to use the mean absolute relative error (AbsRel), root mean squared error (RMS), and the accuracy under threshold ($\delta_{i}<1.25^{i}, i=1,2,3$) on both datasets.

\noindent\textbf{Implementation Details.} We implement our model using PyTorch~\cite{NEURIPS2019_9015}. In the depth factorization module, we use the same depth network as in~\cite{godard2019digging}; for the scale network, we use two basic residual blocks followed by three fully-connected layers with a dropout layer in-between. The dropout rate is set to 0.5. In the residual pose module, we let the residual pose networks use a common architecture~\cite{godard2019digging} which consists of a shared pose encoder and an independent pose regressor.  Each experiment is trained for 40 epochs using the Adam~\cite{kingma2015adam} optimizer and the learning rate is set to $10^{-4}$ for the first 20 epochs and it drops to $10^{-5}$ for remaining epochs. The smoothness term $\tau$ and consistency term $\gamma$ are set as 0.001 and 0.05, respectively.

\subsection{EuRoC MAV Dataset}
The EuRoC MAV Dataset~\cite{schonberger2016structure} contains 11 video sequences captured in two main scenes, a machine hall and a vicon room. Sequences are categorized as \textit{easy}, \textit{medium} and \textit{difficult} according to the varying illumination and camera motions.
For the training, we use three sequences of ``Machine hall" (MH\_01, MH\_02, MH\_04) and two sequences of ``Vicon room'' (V1\_01 and V1\_02). Images are rectified with provided camera intrinsics to remove image distortion. During training, images are resized to 512$\times$256. Following~\cite{gordon2019depth}, we use the Vicon room sequence V2\_01 for testing where the ground-truth depths are generated by projecting Vicon 3D scans onto the image planes.

\subsubsection{Ablation Study}
\label{sec:euroc_ablation}
We perform ablation studies for our design choices of the depth factorization module on the EuRoC MAV dataset. Firstly, we consider the following designs as the backbone of our scale network: I) a pre-trained ResNet-18~\cite{He_2016_CVPR} followed by a group of Conv-BN-ReLU layers; II) a pre-trained ResNet-18~\cite{He_2016_CVPR} followed by two residual blocks; III) a lightweight network with two residual blocks which shares the feature maps from the depth encoder as input. These three choices are referred to as the ScaleCNN, ScaleNet and ScaleRegressor, respectively in Table~\ref{tab:eurco_quan_ablation}. Next, we validate the effectiveness of adding new components into our backbone design. As described in Section~\ref{sec:depth_scale_factorization}, we mainly integrate two sub-modules: i) a self-attention block and ii) a probabilistic scale regression block.

\begin{table}[!t]
    \caption{Ablation results of design choices and the effectiveness of components in the depth factorization module of our model (\textbf{MonoIndoor})  on EuRoC~\cite{schonberger2016structure}. Porb. Reg.: the probabilistic scale regression block. Note: here we also use the residual pose estimation module when experimenting with different network designs for the depth factorization module.}
    \label{tab:eurco_quan_ablation}
    \centering
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \hline
    \multirow{2}{*}{Network Design} & 
    \multirow{2}{*}{Attention} &
    \multirow{2}{*}{\tabincell{c}{Prob.\\Reg.}} &
    \multicolumn{2}{c|}{Error Metric} &  \multicolumn{3}{c}{Accuracy Metric} \\
    \cline{4-8}
    ~ & ~ & ~ & AbsRel & RMSE  & $\delta_1$ & $\delta_2$ & $\delta_3$ \\
    \hline
    I. ScaleCNN & \cmark & \cmark & 0.140 & 0.518 & 0.821 & 0.956 & 0.985 \\
    II. ScaleNet & \cmark & \cmark & 0.141 & 0.519 & 0.817 & 0.959 & 0.988 \\
    \hline
    III. ScaleRegressor & \xmark & \xmark & 0.139 & 0.508 & 0.817 & 0.960 & 0.987 \\
    III. ScaleRegressor & \cmark & \xmark & 0.135 & 0.501 & 0.825 & 0.964 & 0.989 \\
III. ScaleRegressor & \cmark & \cmark & 0.125 & 0.466 & 0.840 & 0.965 & 0.993 \\
    \hline
    \end{tabular}
    }
\end{table}

As shown in Table~\ref{tab:eurco_quan_ablation}, the best performance is achieved by ScaleRegressor that uses self-attention and probabilistic scale regression. It proves that sharing features with the depth encoder is beneficial to scale estimation.
Comparing the results of three ScaleRegressor variants, the performance gradually improves as we add more components (\ie., attention and Prob. Reg.). Specifically, adding the self-attention block improves the overall performance over the baseline backbone; adding the probabilistic regression block leads to a further improvement, which validates the effectiveness of our proposed sub-modules.


\subsubsection{Quantitative Results}
Since there are not many public results reported on the EuRoC MAV~\cite{schonberger2016structure} dataset, we mainly compare our model with the baseline model Monodepth2~\cite{godard2019digging} and validate the effectiveness of each module of our MonoIndoor.
As shown in Table~\ref{tab:eurco_quan_full}, adding our depth factorization module reduces the AbsRel from 15.7\% to 14.9\%, and  our residual pose module decreases the AbsRel to 14.1\%, which verifies the usefulness of each module. Our full model achieves the best performance across all evaluation metrics. Specifically, compared to Monodepth2, the AbsRel by our {\bf MonoIndoor} is significantly decreased from 15.7\% to 12.5\% and the $\delta_1$ is improved by around 6\%, from 78.6\% to 84.0\%.


\begin{table}[!h]
    \caption{Ablation results of our MonoIndoor and quantitative comparison with the baseline on the test sequence V2\_01 of EuRoC. Best results are in \textbf{bold}.}
    \label{tab:eurco_quan_full}
    \centering
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \hline
    \multirow{2}{*}{Method} & 
    \multirow{2}{*}{\tabincell{c}{Depth\\Factorization}}  & 
    \multirow{2}{*}{\tabincell{c}{Residual \\Pose}} & 
    \multicolumn{2}{c|}{Error Metric} &  \multicolumn{3}{c}{Accuracy Metric} \\
\cline{4-8}
    ~ & ~ & ~ & AbsRel & RMSE  & $\delta_1$ & $\delta_2$ & $\delta_3$ \\
    \hline
    Monodepth2~\cite{godard2019digging} & \xmark & \xmark & 0.157  & 0.567  & 0.786 & 0.941 & 0.986 \\
    MonoIndoor & \cmark & \xmark & 0.149 & 0.535  & 0.805 & 0.955 & 0.987 \\
    MonoIndoor & \xmark & \cmark & 0.141 & 0.518 & 0.815 & 0.961 & 0.991 \\
    MonoIndoor & \cmark & \cmark & \textbf{0.125} & \textbf{0.466} & \textbf{0.840} & \textbf{0.965} & \textbf{0.993} \\
    \hline
    \end{tabular}
    }
\end{table}


\begin{figure}[!t]
\begin{center}
\includegraphics[width=\linewidth]{Euroc_qua_final.pdf}
\end{center}
\caption{Qualitative comparison of depth prediction on EuRoC. Our model produces more accurate and cleaner depth maps.}
\label{fig:euroc_qua_full}
\end{figure}

\subsubsection{Qualitative Results}
Figure~\ref{fig:euroc_qua_full} gives a qualitative comparison of depth maps predicted by Monodepth2~\cite{godard2019digging} and our \textbf{MonoIndoor}. From Figure~\ref{fig:euroc_qua_full}, it is clear that the depth maps generated by our model are much better than the ones by Monodepth2. For instance, in the first row, our model can predict precise depths for the \textbf{\textit{hole}} region at the right-bottom corner whereas such a hole structure in the depth map by Monodepth2 is missing. Besides, in the second row, our model can predict much sharper depth map of the \textit{\textbf{ladder}} at the right-top area while Monodepth2 cannot. These observations are also consistent with the better quantitative results in Table~\ref{tab:eurco_quan_full}, proving the superiority of our model. 





\subsection{NYUv2 Depth Dataset}
In this section, we evaluate our \textbf{MonoIndoor} on the NYUv2 depth dataset~\cite{Silberman:ECCV12} which contains 464 indoor video sequences captured by a hand-held Microsoft Kinect RGB-D camera with a resolution of 640$\times$ 480. We use the official training and validation splits which include 302 and 33 sequences respectively. We rectify the images with provided camera parameters to remove distortions. Following~\cite{zhao2020towards, bian2020unsupervised}, the raw dataset is firstly downsampled 10 times along the temporal dimension to remove redundant frames, resulting in $\sim20K$ images for training. During training, images are resized to 320$\times$256. We use officially provided 654 images with dense labelled depth maps for testing.

\begin{table}[h!]
    \caption{Ablation results of the effectiveness of each module of our MonoIndoor on NYUv2. ``No. Residual Pose Block'' means the number of residual poses we estimate in the residual pose estimation module.}
    \label{tab:nyuv2_quan_ablation}
    \centering
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \hline
    \multirow{2}{*}{Model} &
    \multirow{2}{*}{\tabincell{c}{Depth\\Factorization}}  & 
    \multirow{2}{*}{\tabincell{c}{No. Residual\\Pose Block}} & 
    \multicolumn{2}{c|}{Error Metric} &  \multicolumn{3}{c}{Accuracy Metric} \\
\cline{4-8}
    ~ & ~ & ~ & AbsRel & RMS & $\delta_1$ & $\delta_2$ & $\delta_3$ \\
    \hline
    Monodepth2~\cite{godard2019digging} & \xmark & 0 & 0.16 & 0.601 & 0.767 & 0.949 & 0.988 \\
    MonoIndoor & \cmark & 0 & 0.152 & 0.576 & 0.792 & 0.951 & 0.987 \\
    \hline
    MonoIndoor & \xmark & 1 & 0.142 & 0.553 & 0.813 & 0.958 & 0.988 \\
    MonoIndoor & \cmark & 1 & \textbf{0.134} & \textbf{0.526} & \textbf{0.823} & \textbf{0.958} & \textbf{0.989} \\
    \hline
    MonoIndoor & \xmark & 2 & 0.141 & 0.548 & 0.814 & 0.958 & 0.988 \\
    MonoIndoor & \cmark & 2 & 0.141 & 0.546 & 0.818 & 0.958 & 0.989 \\
    \hline
    \end{tabular}
    }
\end{table}

\subsubsection{Ablation Study}
We perform another ablation study for the depth factorization module on NYUv2~\cite{Silberman:ECCV12}. In Table~\ref{tab:nyuv2_quan_ablation}, comparing with Monodepth2 which predicts depth without any guidance of global scales, using the depth factorization module with a separate scale network can improve the performance, decreasing the AbsRel from 16\% to 15.2\% and increasing $\delta_1$ to 79.2\%. Next, we experiment to validate the effectiveness of the residual pose estimation module. Comparing the rows in Table~\ref{tab:nyuv2_quan_ablation}, by adding the residual pose estimation module with one residual pose block, we observe an improved performance from 16.0\% down to 14.2\% for the AbsRel and from 76.7\% up to 81.3\% for $\delta_1$. Furthermore, by applying both the depth factorization module and the residual pose estimation module (\ie, our full \textbf{MonoIndoor}), significant improvements can be achieved across all evaluation metrics. For instance, the AbsRel is reduced to 13.4\% and the $\delta_1$ is increased to 82.3\%. These ablation results clearly prove the effectiveness of the proposed modules, which also align with the qualitative results in Figure~\ref{fig:nyuv2_qua_ablations} where we visualize predictions on NYUv2 by our proposed modules. However, referring to the last two rows, when adding more residual pose blocks and training with/without the depth factorization module, the performance does not significantly improve or even becomes worse. We will leave the investigation of this phenomenon for future work.


\begin{figure*}[h]
\begin{center}
\includegraphics[width=\linewidth]{NYUv2_supp_qua_ablations.pdf}
\end{center}
\caption{Qualitative ablation comparisons of depth prediction on NYUv2. Our full model with both depth factorization and residual pose modules produce better depth maps.}
\label{fig:nyuv2_qua_ablations}
\end{figure*}

We further visualize intermediate and final synthesized views compared with the current view on NYUv2 in the Figure~\ref{fig:nyuv2_intermediate_views}. Highlighted regions show that final synthesized views are better than the intermediate synthesized views and closer to the current view.

\begin{figure*}[h]
\begin{center}
\includegraphics[width=\linewidth]{NYUv2_rebuttal_qua_interresults.pdf}
\end{center}
\caption{Intermediate synthesized views on NYUv2.}
\label{fig:nyuv2_intermediate_views}
\end{figure*}

\begin{table}[!t]
    \caption{Comparison of our method to existing supervised and self-supervised methods on NYUv2~\cite{Silberman:ECCV12}. Best results among supervised and self-supervised methods are in \textbf{bold}. }
    \label{tab:nyuv2_quan_full}
    \centering
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \hline
    \multirow{2}{*}{Methods} & 
    \multirow{2}{*}{Supervision} & \multicolumn{2}{c|}{Error Metric} & \multicolumn{3}{c}{Accuracy Metric} \\
\cline{3-7}
    ~ & ~ & AbsRel & RMS & $\delta_1$ & $\delta_2$ & $\delta_3$ \\
    \hline
    Make3D~\cite{saxena2008make3d} & \cmark & 0.349 & 1.214 & 0.447 & 0.745 & 0.897 \\
    Depth Transfer~\cite{Karsch:TPAMI:14} & \cmark & 0.349 & 1.210 & - & - & - \\
    Liu~\etal~\cite{Liu_2014_CVPR} & \cmark & 0.335 & 1.060 & - & - & - \\
    Ladicky~\etal~\cite{Ladicky_2014_CVPR} & \cmark & - & - & 0.542 & 0.829 & 0.941 \\
    Li~\etal~\cite{Li_2015_CVPR} & \cmark & 0.232 & 0.821 & 0.621 & 0.886 & 0.968 \\
    Roy~\etal~\cite{Roy_2016_CVPR} & \cmark & 0.187 & 0.744 & - & - \\
    Liu~\etal~\cite{Liu_2015_CVPR} & \cmark & 0.213 & 0.759 & 0.650 & 0.906 & 0.976 \\
    Wang~\etal~\cite{Wang_2015_CVPR} & \cmark & 0.220 & 0.745 & 0.605 & 0.890 & 0.970 \\
    Eigen~\etal~\cite{Eigen_2015_ICCV} & \cmark & 0.158 & 0.641 & 0.769 & 0.950 & 0.988 \\
    Chakrabarti~\etal~\cite{NIPS2016_f3bd5ad5} & \cmark & 0.149 & 0.620 & 0.806 & 0.958 & 0.987 \\
    Laina~\etal~\cite{laina2016deeper} & \cmark & 0.127 & 0.573 & 0.811 & 0.953 & 0.988 \\
    Li~\etal~\cite{Li_2017_ICCV} & \cmark & 0.143 & 0.635 & 0.788 & 0.958 & 0.991 \\
    DORN~\cite{Fu_2018_CVPR} & \cmark & 0.115 & 0.509 & 0.828 & 0.965 & 0.992 \\
    VNL~\cite{Yin_2019_ICCV} & \cmark & 0.108 & 0.416 & \textbf{0.875} & \textbf{0.976} & \textbf{0.994} \\
    Fang~\etal~\cite{Fang_2020_WACV} & \cmark & \textbf{0.101} & \textbf{0.412} & 0.868 & 0.958 & 0.986 \\
    \hline
    \hline
    Zhou~\etal~\cite{zhou2019moving} & \xmark & 0.208 & 0.712 & 0.674 & 0.900 & 0.968 \\
    Zhao~\etal~\cite{zhao2020towards} & \xmark & 0.189 & 0.686 & 0.701 & 0.912 & 0.978 \\
    Monodepth2~\cite{godard2019digging} & \xmark & 0.160 & 0.601 & 0.767 & 0.949 & 0.988 \\
    Bian~\etal~\cite{bian2020unsupervised} & \xmark & 0.147 & 0.536 & 0.804 & 0.950 & 0.986 \\
    \hline
    \textbf{MonoIndoor}(Ours) & \xmark & \textbf{0.134} & \textbf{0.526} & \textbf{0.823} & \textbf{0.958} & \textbf{0.989} \\
    \hline
    \end{tabular}
    }
\end{table}

\begin{table*}[!t]
    \caption{Comparison of our method to latest self-supervised methods on RGB-D 7-Scenes~\cite{Shotton_2013_CVPR}. Best results are in \textbf{bold}}.
    \label{tab:7scenes_quan_full}
    \centering
    \resizebox{0.7\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \hline
    \multirow{3}{*}{Scenes} &
    \multicolumn{4}{c|}{Bian~\etal~\cite{bian2020unsupervised}} &
    \multicolumn{4}{c}{\textbf{MonoIndoor} (Ours)} \\
    \cline{2-9}
    ~ & \multicolumn{2}{c|}{Before Fine-tuning} &
    \multicolumn{2}{c|}{After Fine-tuning} &
    \multicolumn{2}{c|}{Before Fine-tuning} &
    \multicolumn{2}{c}{After Fine-tuning} \\
\cline{2-9}
    ~ & AbsRel & Acc $\delta_1$ & AbsRel & Acc $\delta_1$ & AbsRel & Acc $\delta_1$ & AbsRel & Acc $\delta_1$ \\
    \hline
    Chess & 0.169 & 0.719 & 0.103 & 0.880 & 0.157 & 0.750 & \textbf{0.097} & \textbf{0.888} \\
    Fire & 0.158 & 0.758  & 0.089 & 0.916 & 0.150 & 0.768 & \textbf{0.077} & \textbf{0.939} \\
    Heads & 0.162 & 0.749 & 0.124 & 0.862 & 0.171 & 0.727 & \textbf{0.106} & \textbf{0.889} \\
    Office & 0.132 & 0.833 & 0.096 & 0.912 & 0.130 & 0.837 & \textbf{0.083} & \textbf{0.934} \\
    Pumpkin & 0.117 & 0.857 & 0.083 & \textbf{0.946} & 0.102 & 0.895 & \textbf{0.078} & 0.945 \\
    RedKitchen & 0.151 & 0.78 & 0.101 & 0.896 & 0.144 & 0.795 & \textbf{0.094} & \textbf{0.915} \\
    Stairs & 0.162 & 0.765 & 0.106 & 0.855 & 0.155 & 0.753 & \textbf{0.104} & \textbf{0.857} \\
    \hline
    \end{tabular}
    }
\end{table*}

\subsubsection{Quantitative Results}

We present the quantitative results of our model \textbf{MonoIndoor} and both state-of-the-art (SOTA) supervised and self-supervised methods on NYUv2 in Table~\ref{tab:nyuv2_quan_full}. It shows that our model outperforms previous self-supervised SOTA methods, reaching the best results across all metrics. Specifically, compared to a recent self-supervised method by Bian~\etal~\cite{bian2020unsupervised} which removes rotations via ``weak rectification'', our method reduces AbsRel by 1.3\% and increases $\delta_1$ by 1.9\%, reaching an AbsRel of 13.4\%  and $\delta_1$ of 82.3\%.  In addition to that, our model outperforms a group of supervised methods and close the performance gap between the self-supervised methods and fully-supervised methods. 

\subsubsection{Qualitative Results}

Figure~\ref{fig:nyuv2_qua_full} visualizes the predicted depth maps on NYUv2. Compared with the results from the Monodepth2~\cite{godard2019digging}, depth maps predicted from our model (\textbf{MonoIndoor}) are more precise and closer to the ground-truth. For instance, looking at the third column in the first row, the depth in the region of \textbf{\textit{chairs}} predicted from our model is much sharper and cleaner, 
being close to the ground truth (the last column). On the rightmost area of the same image where there is a \textbf{\textit{shelf}}, our model can produce better depth predictions that reflect its shape. These observations are consistent with our quantitative results in Table~\ref{tab:nyuv2_quan_full}.


\subsection{RGB-D 7-Scenes Dataset}
In this section, we evaluate our \textbf{MonoIndoor} on the RGB-D 7-Scenes dataset \cite{Shotton_2013_CVPR} which contains several video sequences with 500-1000 frame in each sequence. All scenes are recorded using a handheld Kinect RGB-D camera at 640Ã—480 resolution. We use the official train/test split. Following~\cite{bian2020unsupervised}, for training, we first pre-train our \textbf{MonoIndoor} on NYUv2 dataset, and then fine-tune the model on this dataset; for testing, we extract one image from every 30 frames. Images are resized to $320\times256$ during training.

\subsubsection{Quantitative Results}

We present the quantitative results of our model \textbf{MonoIndoor} and latest state-of-the-art (SOTA) self-supervised methods on 7-Scenes in Table 5. It shows that our model outperforms~\cite{bian2020unsupervised} on most scenes before and after fine-tuning, demonstrating better generalizability and capability of our model. Specifically, compared to a recent self-supervised method by Bian et al.~\cite{bian2020unsupervised}, on the scene ``Fire", our method reduces AbsRel by 1.2\% and increases $\delta_1$ by 2.3\%, reaching an AbsRel of 7.7\% and $\delta_1$ of 93.9\%; on the scene "Heads",our method reduces AbsRel by 1.8\% and increases $\delta_1$ by 2.7\%, reaching an AbsRel of 10.6\% and $\delta_1$ of 88.9\%.

\begin{figure*}[!t]
\begin{center}
\includegraphics[width=1.0\linewidth]{NYUv2_qua_final_v2.pdf}
\end{center}
\caption{Qualitative comparison on NYUv2~\cite{Silberman:ECCV12}. Compared with Monodepth2~\cite{godard2019digging}, our model produces accurate depth maps (in the third column) that are closer to the ground-truth.}
\vspace{-0.1cm}
\label{fig:nyuv2_qua_full}
\end{figure*}




