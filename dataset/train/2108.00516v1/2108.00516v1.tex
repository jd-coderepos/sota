

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  



\IEEEoverridecommandlockouts                              

\overrideIEEEmargins                                      

\pdfminorversion=4







\usepackage{float}
\usepackage{graphics} \usepackage{epsfig} \usepackage{mathptmx} \usepackage{times} \usepackage{amsmath} \usepackage{amssymb}  \usepackage{colortbl}
\usepackage[T1]{fontenc}
\usepackage{wrapfig,lipsum}
\usepackage{booktabs}
\usepackage{bigstrut}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[ruled,linesnumbered, noend]{algorithm2e}
\DeclareMathOperator*{\argminB}{argmin}   \usepackage{tikz}
\usepackage{yfonts}
\usepackage{graphicx}
\usepackage{import}
\usepackage{newtxtext}
\usepackage{newtxmath}
\usepackage{hhline}
\usepackage{stfloats}
\usepackage{atbegshi,picture} 
\usepackage{nccmath}
\usepackage{cite}
\usepackage[font={scriptsize}]{caption}
\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage[hidelinks]{hyperref}  

\DeclareMathAlphabet\urwscr{U}{urwchancal}{m}{n}\DeclareMathAlphabet\rsfscr{U}{rsfso}{m}{n}
\DeclareMathAlphabet\euscr{U}{eus}{m}{n}
\DeclareFontEncoding{LS2}{}{}
\DeclareFontSubstitution{LS2}{stix}{m}{n}
\DeclareMathAlphabet\stixcal{LS2}{stixcal}{m} {n}
\DeclareMathOperator*{\argmin}{argmin}   \newcommand{\red}[1]{{\color{red} #1}}

\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}

\addtolength{\skip\footins}{-0.1in}

\newenvironment{myitem}{\begin{list}{}
{\setlength{\itemsep}{-0pt}
\setlength{\topsep}{0pt}
\setlength{\labelwidth}{5pt}
\setlength{\leftmargin}{10pt}
\setlength{\parsep}{-0pt}
\setlength{\itemsep}{0pt}
\setlength{\partopsep}{0pt}}}{\end{list}}


\title{\LARGE \bf BundleTrack: 6D Pose Tracking for Novel Objects\\ without Instance or Category-Level 3D Models
}


\author{Bowen Wen and Kostas Bekris \thanks{The authors are with the Computer Science Dept. of Rutgers in NJ, USA. Email: \{bw344,kostas.bekris\}@cs.rutgers.edu. This work is supported by NSF NRI award 1734492. The results do not express the sponsor's positions.}}



\begin{document}

\AtBeginShipoutNext{\AtBeginShipoutUpperLeft{\put(\dimexpr\paperwidth-0.4cm\relax,-0.6cm){\makebox[0pt][r]{\framebox{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2021}}}}}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
Tracking the 6D pose of objects in video sequences is important for  robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes \textit{BundleTrack}, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions.  Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method's reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: \url{https://github.com/wenbowen123/BundleTrack}
 \end{abstract}


\section{INTRODUCTION}
Robot manipulation often requires information about the pose of the manipulated object. In some cases, this can be achieved through forward kinematics (FK), assuming the object's motion equivalent to the end-effector's motion. Frequently, however, FK is insufficient to accurately estimate the object's pose \cite{kappler2018real}. This can be due to slippage during grasping or in-hand manipulation \cite{wen2020robust}, or during handoffs or due to the compliance of a suction cup (Fig. \ref{fig:intro}). In these cases, dynamically estimating an object's pose from visual data is desirable. Single-image 6D pose estimation methods have been studied extensively \cite{xiang2017posecnn, park2019pix2pose, li2019cdpn, he2020pvn3d,mitash2020scene}. Some of them are fast and can re-estimate poses from scratch for every new frame \cite{tremblay2018deep, wang2019densefusion}. Nevertheless, this is redundant, less efficient, leading to less coherent estimations over consecutive frames and negatively impacts planning and control. On the other hand, given an initial pose estimate, tracking 6D object poses over image sequences can improve estimation speed while providing coherent and accurate poses by leveraging temporal consistency \cite{deng2019poserbpf,Wthrich2013ProbabilisticOT, schmidt2014dart}.

Most existing 6D object pose estimation or tracking approaches assume access to an  object instance's 3D model \cite{xiang2017posecnn,wang2019densefusion}. Having access to  such \textit{instance 3D models} complicates generalization to novel, unseen instances. To overcome this limitation, recent efforts have relaxed this assumption and require only \textit{category-level 3D models} for 6D pose estimation \cite{Wang_2019_CVPR,park2020latentfusion,chen2020category,chen2020learning} or tracking \cite{wang20196-pack}. They often achieve this by training over a large number of CAD models from the same category. While promising results have been demonstrated for previously seen object categories, there are still limitations. These methods are constrained by the variety of categories in the training database. Popular 3D model databases, such as \textit{ShapeNet} \cite{chang2015shapenet} and \textit{ModelNet40} \cite{wu20153d}, contain 55 and 40 categories respectively. This is still far from sufficient to cover diverse object categories present in the real world. Furthermore, 3D model databases often require nontrivial manual effort and expert domain knowledge to build, involving steps such as scanning \cite{newcombe2011kinectfusion}, mesh refinement \cite{cignoni2008meshlab} or CAD design.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\textwidth]{sections/figs/intro.pdf}
  \vspace{-0.25in}
  \caption{\textbf{Top:} \textit{NOCS Dataset} \cite{Wang_2019_CVPR} example: The target object exits the camera's frustum during tracking but \textit{BundleTrack} maintains its estimate without re-initialization. \textbf{Bottom:} \textit{YCBInEOAT Dataset} \cite{wense3tracknet} example: The object is successfully tracked during pick and place manipulation by a robotic arm, despite the lack of texture, severe self-occlusion and motions due to the arm and the compliant suction cup. Computing object pose from forward kinematics is unreliable in this setup due to the end-effector.}
  \label{fig:intro}
  \vspace{-0.3in}
\end{figure}

Another line of work from the SLAM literature has moved to address dynamic, object-aware challenges \cite{xu2019mid, runz2018maskfusion, ma2015simultaneous, runz2017co}, where dynamic objects are being reconstructed on-the-fly while being tracked without the need for object 3D models beforehand. However, tracking-via-reconstruction \cite{runz2017co,runz2018maskfusion} tends to accumulate errors when fusing observations with erroneous pose estimates into the global model. These errors adversely impact model tracking in subsequent frames.

Motivated by the above limitations, this work aims for accurate, robust 6D pose tracking that is generalizable to novel objects without \textit{instance or category-level 3D models}. It exploits recent advances in video segmentation as well as learning-based keypoint detection and matching for a coarse pose estimate, followed by a memory-augmented pose-graph optimization step to achieve spatiotemporal consistent pose output. Instead of aggregating into a global model, representative historical observations are maintained as keyframes in a memory pool, providing candidate nodes for future graphs so as to enable multi-pair data association together with the latest observation. An efficient implementation of this framework in CUDA allows to achieve competitive running times. Extensive experiments have been conducted on two large-scale public benchmarks, shown in Fig. \ref{fig:intro}. Both qualitative and quantitative results demonstrate a significant improvement over existing state-of-art approaches, including methods using \textit{instance or category-level 3D models} or SLAM-like methods. 

In summary, this work's contributions are the following:





1) A novel integration of methods that result in a 6D pose tracking framework that generalizes to novel objects without access to instance or category-level 3D models.

2) A memory-augmented pose graph optimization for low-drift accurate 6D object pose tracking. In particular, augmenting the memory pool with historical observations enables multi-hop data association and ameliorate the dearth of correspondences between a pair of consecutive frames. Additionally, maintaining keyframes as raw nodes instead of aggregating into a global model significantly reduces tracking drift.

3) An efficient CUDA implementation, which allows to execute online the computationally-heavy multi-pair feature matching as well as pose-graph optimization for 6D object pose tracking (for the first time to the best of the authors' knowledge). 

These contributions result in a new state-of-art performance by boosting the previous best accuracy from \textbf{33.3\%} to \textbf{87.4\%} under the ``5\textdegree5cm'' metric in the \textit{NOCS Dataset} \cite{Wang_2019_CVPR}, even when compared against approaches utilizing category-level 3D models for training.  They also result in comparable performance on the \textit{YCBInEOAT} dataset \cite{wense3tracknet}, even when compared against approaches utilizing instance-level 3D models \cite{wense3tracknet}.

 


\section{RELATED WORK}
\textbf{6D Object Pose Tracking} - For setups where object CAD 
models are available, significant progress has been made in 6D pose tracking. This includes techniques based on hand-crafted probabilistic filtering \cite{choi2013rgb,Wthrich2013ProbabilisticOT,issac2016depth}, optimization \cite{schmidt2014dart, joseph2015versatile, zhong2019robust, tjaden2018region}, and machine learning \cite{deng2019poserbpf,wense3tracknet}. The requirements, however, of such \textit{instance-level 3D models}, either for training offline or model-frame registration during tracking, complicate generalization to novel instances. More recently,  a 6D pose tracking approach \cite{wang20196-pack} relaxed the assumption to \textit{category-level 3D models} using 3D object CAD model databases for training \cite{chang2015shapenet}. During testing, the target object category needs to be identified and the corresponding network for that category is utilized for tracking. Instead of being limited to the number of categories such database is able to include, this work employs deep features that in principle can be trained on arbitrary 2D images. It allows generalization to diverse novel objects, as shown in the accompanying experiments. 

\textbf{Dynamic Object-aware SLAM} - In order to track dynamic objects' pose and decouple them from static background, frame-model Iterative Closest Point \textit{(ICP)} combined with color \cite{xu2019mid,runz2018maskfusion,ma2015simultaneous,runz2017co}, probabilistic data association \cite{strecke2019fusion}, or 3D level-set likelihood maximization \cite{yuheng2013star3d} has been applied. Object models are simultaneously reconstructed on-the-fly by aggregating the observed RGB-D data with the newly tracked pose. Nevertheless, frame-model tracking can be challenging for object reconstruction, since errors in pose estimation transfer to the reconstructed model and adversely affect the subsequent tracking \cite{slavcheva2016sdf}. This work does not fuse observed frames but instead maintains them as nodes in a pose graph, allowing to correct previously erroneous estimates, and reduces drift in long-term tracking. The aforementioned SLAM-family approaches may also face challenges in robot manipulation setups that involve small, textureless, flat or shiny objects due to the dearth of sufficient correspondences between the pair of consecutive frames. To ameliorate this issue, \textit{BundleTrack} searches correspondences among current and multiple historical frames, consisting of both feature and geometric terms, as the edges in the pose graph. Its effectiveness has been shown in extensive experiments including for such challenging manipulation scenarios.

\textbf{3D Hand-held Object Scanning} - Promising results have been demonstrated in scanning dynamic hand-held objects \cite{tzionas20153d,weise2011online,krainin2010manipulator,wang2019hand,weise2008accurate}, where the object's motion needs to be taken into account similar to the current setup. In particular, a framework for robot manipulation \cite{krainin2010manipulator} performs simultaneous object reconstruction and tracking, which leads to similar issues as the aforementioned dynamic SLAM methods. In addition, forward kinematics is required in its Kalman Filtering framework, preventing generalization in scenarios when objects are not held by the robotic manipulator. While estimating object poses is part of the scanning process, there are key differences from online 6D pose tracking. For the scanning application, external assistance including human interaction or deliberate motion is acceptable \cite{weise2011online,wang2019hand,weise2008accurate} but it is not assumed in the current work. Furthermore, time consuming global-optimization steps are often adopted at the end of scanning to polish the models and their poses while intermediate erroneous pose estimations and associated frames can be discarded and not fused into the global model \cite{weise2011online,wang2019hand,weise2008accurate}. In contrast, this work aims to provide fast and accurate pose tracking output online.

 
\section{PROBLEM FORMULATION}
\begin{figure*}[h!]
  \centering
  \vspace{+0.05in}
  \includegraphics[width=\textwidth]{sections/figs/pipeline.pdf}
  \vspace{-0.25in}\caption{\textit{BundleTrack} framework from left to right: (1) an image segmentation network returns the object mask given the prior one; (2) a network detects keypoints and their descriptors; (3) keypoints are matched and coarse registration is performed between consecutive frames to estimate an initial relative transform ; (4) keyframes are selected from a memory pool to participate in the pose graph optimization; (5) online pose graph optimization outputs a refined spatiotemporal consistent pose ; and (6) the latest frame is included in the memory pool, if it is a novel view to enrich diversity.}
  \label{fig:pipeline}
  \vspace{-0.2in}
\end{figure*}

\setlength{\columnsep}{0.1in}\setlength{\intextsep}{0.03in}\begin{wrapfigure}{r}{1.9in}
\vspace{-.1in}
  \centering
  \includegraphics[width=1.9in]{sections/figs/formulation.pdf}  
\end{wrapfigure}

Assume a rigid body for which there is no its corresponding 3D model, nor its category-level 3D model database for training. The objective is to continuously track its 6D pose change relative to the start of tracking, i.e., the relative transformation  in the camera's frame . The input is the following:

\begin{myitem}
  \item : A sequence of RGB-D data .
  \item : A binary mask on the first image , indicating the target object region to track in the image space. 
  \item  (optional):  The initial pose in the camera's frame . Used if the objective is to recover the object's absolute pose in , otherwise set to identity.
\end{myitem}
    
\noindent The initial mask  can be obtained in multiple different ways to initialize tracking. For instance, via semantic segmentation  \cite{long2015fully,chen2017deeplab,le2018deep} or non-semantic methods, such as image segmentation, \cite{meyer1992color,danielczuk2019segmenting,xiang2020learning}, point cloud segmentation/clustering \cite{rusu20113d,Papon13CVPR}, or plane fitting and removal \cite{rusu20113d}, etc.
    


The object's pose in the camera's frame  can be recovered at any timestamp by applying the relative transformation  in the camera's frame . For simplicity, the rest of this document will refer to  as the output of the process but  is what is actually computed as tracking.
 
\section{APPROACH}




An overview of the proposed \textit{BundleTrack} framework is depicted in Fig. \ref{fig:pipeline}. The currently observed RGB-D frame  and the object segmentation mask computed during the last timestamp   are forwarded to a video segmentation network to compute the current object mask . Based on  and  respectively, the target object regions in both  and  are cropped, resized and sent to a keypoint detection network to compute keypoints and feature descriptors. A data association process consisting of feature matching and outlier pruning in the manner of \textit{RANSAC} \cite{fischler1981random} identifies feature correspondences. Based on these correspondences, a registration between  and  can be solved in closed-form, which is then used to provide a coarse estimate  for the transform between the two snapshots. The estimate  is used to initialize the current node  as part of a pose graph optimization step. To define the rest of the nodes of the pose graph, no more than  keyframes are selected from a memory pool to participate in the optimization. The choice of  is made to balance an efficiency vs. accuracy tradeoff. Pose graph edges include both feature and geometric correspondences, which are computed in parallel on GPU. Given this information, the pose graph step outputs online the optimized pose for the current timestamp . If the last frame corresponds to a novel view, then it is also included in the memory pool. 


\subsection{Propagating Object Segmentation}
\label{sec:video_seg}




The first step is to segment the object's image region from the background. Prior work \cite{runz2018maskfusion} used \textit{Mask-RCNN} \cite{he2017mask} to compute the object mask in every frame of the video. It deals with each new frame independently, which is less efficient and results in temporal inconsistencies. 

To avoid these limitations, this work adopts an off-the-shelf \textit{transductive-VOS} network \cite{zhang2020a} for video object segmentation, which is trained on the \textit{Davis 2017} \cite{pont20172017} and \textit{Youtube-VOS} \cite{xu2018youtube} datasets.  The network uses dense long-term similarity dependencies between current and past feature embeddings to propagate the previous object mask to the latest frame. The object mask needed by \textit{BundleTrack} is simply binary, i.e.,  and distinguishes the object region from the background.  The only requirement is an initial mask  of interest. Neither the \textit{transductive-VOS} network nor the following steps of \textit{BundleTrack} require  to come from semantic/instance segmentation. Therefore, it can also be obtained in  alternative ways depending on the application, e.g., low-level image segmentation \cite{meyer1992color,lafferty2001conditional}, point cloud segmentation/clustering \cite{rusu20113d,Papon13CVPR}, or plane fitting and removal \cite{rusu20113d}, etc.  






While the current implementation uses \textit{transductive-VOS}, the following techniques do not depend on this specific network. If the object mask can be computed via simpler means, such as computing a region of interest (ROI) from forward kinematics followed by point cloud filtering in robot manipulation scenarios \cite{wen2020robust}, the segmentation module can be replaced.

\vspace{-0.05in}
\subsection{Keypoint Detection, Matching and Local Registration} 
\label{sec:local_registration}
\vspace{-0.05in}


Local registration is performed between consecutive frames  and  to compute a initial pose . To do so, correspondence between keyframes detected on each image is performed. Different from prior work \cite{wang20196-pack}, which relies on \textit{category-level 3D models} to learn a fixed number of category-level semantic keypoints, this work aims to use generalizable features not specific to certain instances or categories. The \textit{LF-Net} \cite{ono2018lf} is chosen given its satisfactory balance between performance and inference speed. It only requires training on general 2D images, such as the \textit{ScanNet dataset} \cite{dai2017scannet} used here, and generalizes to novel scenes. During testing, for the newly observed frame , \textit{LF-Net} receives the segmented image (Sec. \ref{sec:video_seg}) as input. It then outputs  keypoints  along with the feature descriptor , where  is 500 in all experiments. Due to the potentially imperfect segmentation in previous step, outlier keypoints can arise from the background. It is thus critical to perform feature matching and outlier pruning via \textit{RANSAC} \cite{fischler1981random}, executed in parallel on GPU in this work. Each registration sample consists of 3 pairs of keypoints matched between the two images. A pose hypothesis is generated from a sample via least squares \cite{arun1987least}. When evaluating samples, inlier correspondences have a distance between transformed point pairs below a threshold  and an angle formed by the normals within a threshold . The values of  and  are empirically set to  and  in all experiments. After \textit{RANSAC}, a preliminary pose is computed by  where  is the best sampled correspondence hypothesis. 



\subsection{Keyframe Selection}
\label{sec:keyframe_selection}

 is then refined during a pose graph optimization step. The number of keyframes participating in the  optimization is limited to  for the sake of efficiency, where  is the number used in the experiments. When the size of the keyframe memory pool  is larger than , the objective is to find the set of keyframes with the largest mutual viewing overlap to make good use of multi-view consistency. This challenge can be formulated as the minimum H-subgraph of an edge-weighted graph problem \cite{vassilevska2006finding}:

\vspace{-0.02in}


\noindent where  is the rotation matrix of the corresponding keyframe's pose. The goal is to find the optimal binary vector  that indicates the selections. The weight of the edge between frame pair  is the geodesic distance of their rotations. Mutual viewing overlap is maximized when the mutual rotation difference relative to the camera is minimized. Combinatorial optimization algorithms for solving this problem have a complexity of  \cite{vassilevska2006finding}. In practice, an iterative greedy selection is followed by starting with the keyframe set  until the number of selected keyframes reaches .  is chosen since the initial frame does not suffer from any tracking drift and serves as the reference frame. In each iteration, the keyframe with the smallest sum of geodesic distances against  as well as all previously selected keyframes is added. This reduces complexity to , making the selection practical (under a millisecond) without degrading performance.


\subsection{Online Pose Graph Optimization}
\label{sec:pose_graph}


The pose graph can be denoted as , where each node corresponds to the object pose in the camera's frame at the current and  selected timestamps . For simplicity, the subscripts of graph nodes will be denoted as simple indices  instead of the actual timestamp .  Each node's pose can then be denoted as .  Inspired by \cite{dai2017bundlefusion}, for the edges between each pair of nodes, two types of energies  and  are considered. The energy  relates to the residuals computed from feature correspondences and  relates to the geometric residuals measured by dense pixel-wise point-to-plane distance. The spatiotemporal consistency is achieved when the total energy of the graph  is minimized:



In order to compute , feature correspondences  between each pair of nodes  are determined. If  has been built during a previous pose graph optimization, it is reused. Otherwise, the data association process of Sec. \ref{sec:local_registration} is performed to compute . These multi-pair feature correspondences are built in parallel on GPU. In Eq. (\ref{eq:E_feat}) and (\ref{eq:E_geom}),  represents the unprojected 3D points in the camera's frame,  is the M-estimator, where Huber loss is used. 



For , dense pixel-wise correspondences are associated by point re-projection, while outliers are filtered based on the distance between the point pair and the angle formed by their normals;  is the perspective projection operation;  denotes the unprojection mapping, which recovers a 3D point in the camera's frame by looking up the depth value on the pixel location;  returns the normal of the pixel on the frame . 

In Eq. (\ref{eq:E}),   and  are the weights balancing  and . To emphasize the lack of sensitivity to the choice of these values,  and  are set to  in all experiments unless otherwise specified. Then, the goal is to find the optimal poses, such that:
\vspace{-0.in}

where  is the stacked energy residual vector,  is the stacked pose vector corresponding to the current frame and  selected past keyframes, while the pose corresponding to the initial frame  is kept constant as reference. Each block  is parametrized in Lie Algebra \cite{bourbaki2008lie}, consisting of 3 parameters for translation and 3 parameters for rotation. A common approach is to apply first-order Taylor expansion around , such that the iteratively re-weighted nonlinear least squares can be solved by a Gauss-Newton update:

where  is the Jacobian matrix with respect to ,  is a diagonal weight matrix computed by the M-estimator  and residual, which is updated in each iteration. To better take advantage of the sparsity of  and , inside each Gauss-Newton step, an iterative PCG (Preconditioned Conjugate Gradient) \cite{hildebrand1987introduction} solver is leveraged, where the diagonal matrix  is used as the preconditioner. Incremental pose updates are accumulated in the tangent space after each iteration . The entire pose graph optimization is implemented in CUDA for parallel computation. 

At the end of the optimization, the object pose corresponding to each graph node is obtained by . The one corresponding to the current timestamp  becomes the output tracked pose , while poses corresponding to the historical keyframes are updated in the memory pool. The entire process is causal, i.e. past frames' corrected poses cannot be updated in the output. However, their corrected pose estimates provide better initialization in following pose graph optimization steps to benefit the solution of new observations. This significantly reduces long-term drift compared against tracking-via-reconstruction \cite{runz2018maskfusion}, where any intermediate erroneous pose estimation introduces noise when fused into the global model and adversely affects the subsequent tracking. 

\subsection{Augmenting the Keyframe Memory Pool}
The initial frame  is always selected as it does not suffer from any tracking drift. For later frames, once the current object pose  is determined, its rotation geodesic distance against each existing keyframe in the pool is compared. If all pair-wise distances are larger than  ( in all experiments),  is added into the keyframe memory pool. This encourages to add frames from novel views, such that multi-view diversity is enriched.  
\section{EXPERIMENTS}
This  section  evaluates  the  proposed  approach  and  compares  against state-of-the-art 6D pose tracking and estimation methods on two public benchmarks, the \textit{NOCS dataset} \cite{Wang_2019_CVPR} and the \textit{YCBInEOAT dataset} \cite{wense3tracknet}. Experiments are performed over diverse types of objects and various tracking scenarios (e.g., moving camera or moving objects). Both quantitative and qualitative results demonstrate that \textit{BundleTrack} achieves comparable or even superior performance relative to alternatives, although it does not require \textit{instance or category-level 3D models}. Concretely, no CAD models or training data from a 3D object database are used by \textit{BundleTrack}. All  experiments are  conducted on a standard desktop with Intel  Xeon(R) E5-1660 v3@3.00GHz  processor and a single NVIDIA RTX 2080 Ti GPU. 

\subsection{Datasets}


\noindent \textbf{NOCS dataset \cite{Wang_2019_CVPR}:} Among existing datasets, this is the closest to the setup here, where \textit{instance 3D models} are not provided during evaluation. The dataset contains 6 object categories: bottle, bowl, camera, can, laptop, and mug.  The training set consists of: (1) 7 real videos containing 3 instances of each category in total, annotated with ground truth poses; and (2) 275K frames of synthetic data generated using 1085 instances from the above 6 categories using a 3D model database \textit{ShapeNetCore} \cite{chang2015shapenet} with random poses and object combinations in each scene.  The testing set has 6 real videos containing 3 different unseen instances within each category, resulting in 18 different object instances and 3,200 frames in total. 


\noindent \textbf{YCBInEOAT dataset \cite{wense3tracknet}:} This dataset helps verify the effectiveness of 6D pose tracking during robot manipulation. It was originally developed to evaluate approaches relying on CAD models. The available CAD models, however, are not used by \textit{BundleTrack}. In contrast to the \textit{NOCS dataset} where objects are statically placed on a tabletop and captured by a moving camera, \textit{YCBInEOAT} contains 9 video sequences captured by a static RGB-D camera, while objects are dynamically manipulated. There are three types of manipulation: (1) single arm pick-and-place, (2) within-hand manipulation, and (3) pick to hand-off between arms to placement. These scenarios and the end-effectors used make directly computing poses from forward kinematics unreliable.  The manipulation videos involve 5 \textit{YCB Objects} \cite{calli2015benchmarking}: mustard bottle, tomato soup can, sugar box, bleach cleanser and cracker box. 

\subsection{Results on the NOCS Dataset} 
\label{sec:eval_nocs}
\import{tables/}{nocs}


\begin{figure}[h]
  \centering
  \definecolor{blue}{RGB}{0,112,192}
  \definecolor{green}{RGB}{0, 176, 80}
  \includegraphics[width=0.48\textwidth]{sections/figs/nocs_qual.pdf}
  \vspace{-0.25in}\caption{Example qualitative results of \textit{BundleTrack} and representative comparison points on \textit{NOCS Dataset}. In all methods, each object is tracked individually and depicted in the same image for visualization. Methods' names are colored in \textcolor{blue}{blue} and \textcolor{green}{green} to denote assumption on \textit{category-level 3D model} and \textit{no model} respectively. For more qualitative results, please refer to the supplementary video.}
  \label{fig:nocs_qual}
  \vspace{-0.3in}
\end{figure}

\begin{figure}[h]
  \centering
  \vspace{+0.05in}
  \definecolor{red}{RGB}{255, 0, 0}
  \definecolor{blue}{RGB}{0,112,192}
  \definecolor{green}{RGB}{0, 176, 80}
  \includegraphics[width=0.48\textwidth]{sections/figs/eoat_qual.pdf}
  \vspace{-0.25in}\caption{Example qualitative results of \textit{BundleTrack} and representative comparison points on \textit{YCBInEOAT Dataset}. Methods' names are colored in \textcolor{red}{red}, \textcolor{blue}{blue} and \textcolor{green}{green} to denote assumption on \textit{instance 3D model}, \textit{category-level 3D model} and \textit{no model} respectively. For more qualitative results, please refer to the supplementary video.}
  \vspace{-0.3in}
  \label{fig:eoat_qual}
\end{figure}

Table \ref{tab:nocs} and Fig. \ref{fig:nocs_qual} present the quantitative  and qualitative results of state-of-art methods on the \textit{NOCS dataset} respectively. The comparison points include learning-based methods relying on a \textit{category-level prior}, such as \textit{NOCS} \cite{Wang_2019_CVPR}, \textit{KeypointNet} \cite{suwajanakorn2018discovery},  and \textit{6-PACK} with or without temporal prediction \cite{wang20196-pack}. These methods are offline trained on both real and synthetic training sets, which are rendered with 3D object models extracted from the same categories of \textit{ShapeNetCore} \cite{chang2015shapenet}.
In contrast, \textit{ICP} \cite{zhou2018open3d}, \textit{MaskFusion} \cite{runz2018maskfusion}, \textit{TEASER++\textsuperscript{*}} \cite{Yang20troteaser} and the proposed \textit{BundleTrack} have no access to any training data based on 3D models. 

The evaluation protocol is the same as in prior work  \cite{wang20196-pack}. A perturbed ground-truth object pose is used for initialization. The perturbation adds a uniformly sampled random translation within a 4cm range to evaluate robustness against a noisy initial pose \cite{wang20196-pack}. No re-initialization is allowed during tracking. To evaluate robustness against missing frames, the same uniformly sampled 450 frames out of 3200 in the testing videos are dropped \cite{wang20196-pack}. Four metrics are adopted: 1) \textbf{5\degree5cm}: percentage of estimates with orientation error < 5\degree and translation error < 5cm - the higher the better; 2) \textbf{IoU25} (Intersection over Union): percentage of cases where the overlapping prediction and ground-truth 3D bounding box volume is larger than 25\% of their union - the higher the better; 3) \textbf{R\textsubscript{err}}: mean orientation error in degrees - the lower the better; and 4) \textbf{T\textsubscript{err}}: mean translation error in centimeters - the lower the better. For  R\textsubscript{err} and T\textsubscript{err}, estimates with IoU25 are not counted when computing averages\footnote{\href{https://github.com/j96w/6-PACK/blob/master/benchmark.py}{https://github.com/j96w/6-PACK/blob/master/benchmark.py}} \cite{wang20196-pack}. 
 


The results of comparison points other than \textit{MaskFusion} and \textit{TEASER++\textsuperscript{*}} come from the literature \cite{wang20196-pack}.  The open-sourced code\footnote{https://github.com/martinruenz/maskfusion} of \textit{MaskFusion} is used for evaluation, where the global SLAM module is disabled to avoid inferring object poses from the camera's estimated ego-motion. The dynamic object tracking module is kept to solely evaluate object pose tracking effectiveness. Its original segmentation module \textit{Mask-RCNN} \cite{he2017mask} is fine-tuned on the real training data provided in the \textit{NOCS dataset} for better performance while the synthetic data rendered using category-level 3D models are not used, as this method is also agnostic to any 3D models \cite{runz2018maskfusion}. In addition to \textit{ICP} reported in \cite{wang20196-pack},  another state-of-art 3D registration approach \cite{Yang20troteaser} is included for comparison and denoted as \textit{TEASER++\textsuperscript{*}}, which is robust to outlier correspondences and agnostic to 3D models. It takes as input the segmented point cloud and feature correspondences that are computed using the same modules proposed in \textit{BundleTrack}. 
For \textit{BundleTrack}, an initial mask  is required as input to the framework and is provided via the aforementioned \textit{Mask-RCNN}. During execution, \textit{BundleTrack} does not require external mask input nor any form of re-initialization. As exhibited in Table \ref{tab:nocs}, \textit{BundleTrack} significantly outperforms the comparison points under all metrics and over all object categories, despite not accessing \textit{instance or category-level 3D models}.



\import{tables/}{eoat}





\subsection{Results on YCBInEOAT Dataset}
\label{sec:eval_eoat}



Evaluation exclusively on static objects captured by a moving camera cannot completely reflect the properties of a 6D pose tracking method \cite{wense3tracknet}. For this reason, the \textit{YCBInEOAT dataset} is chosen to evaluate tracking in scenarios where objects are moving in front of the camera.  The same evaluation protocol is followed as in prior work \cite{wense3tracknet}. Results are computed from accuracy-threshold AUC (Area Under Curve) measured by , which performs exact model matching, and - \cite{xiang2017posecnn} designed for evaluating symmetric objects. Similar to prior work \cite{wense3tracknet}, the ground-truth object's pose in the camera's frame is provided as initialization. No re-initialization is allowed during the tracking process. 


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.48\textwidth]{sections/figs/analysis.pdf}
  \vspace{-0.25in}\caption{Experimental analysis performed on \textit{NOCS dataset} as described in Sec \ref{sec:sys_analysis}. (a) Ablation study investigating effectiveness of pose graph optimization and each energy term. (b) Sensitivity of \textit{BundleTrack} to inaccurate initial pose by deliberately introducing different translation and rotation noise levels. (c) Average running time decomposition of different modules. (d) Rotation and translation error w.r.t. timestamps compared against representative related works \cite{wang20196-pack,runz2018maskfusion,Yang20troteaser} for tracking drift study.}
  \label{fig:analysis}
  \vspace{-0.3in}
\end{figure}




Quantitative and qualitative results are shown in Table \ref{tab:eoat} and Fig. \ref{fig:eoat_qual} respectively. Comparison points include state-of-art 6D pose tracking methods that use object CAD models, such as \textit{RGF} \cite{issac2016depth}, \textit{dbot PF} \cite{Wthrich2013ProbabilisticOT} and \textit{-TrackNet} \cite{wense3tracknet}. \textit{6-PACK} \cite{wang20196-pack} is a state-of-art 6D pose tracking approach relying on \textit{category-level 3D models}. Its evaluation on objects ``021\_bleach\_cleanser'', ``006\_mustard\_bottle'' and ``005\_tomato\_soup\_can'' are performed by using the officially released\footnote{https://github.com/j96w/6-PACK} networks trained on ``bottle'' and ``can'' category respectively . For the rest of the objects ``003\_cracker\_box'' and ``004\_sugar\_box'', no suitable corresponding category can be found in existing 3D model database \cite{chang2015shapenet} and thus \textit{6-PACK} is not able to be retrained and evaluated on them. For \textit{6-PACK}, 3D bounding box of the object model, computed from forward kinematics, is provided in every frame to crop ROI from point cloud, since it is more reliable than its default module of extrapolating the 3D bounding box by estimated motion.
For \textit{MaskFusion} \cite{runz2018maskfusion} and \textit{BundleTrack}, the initial object mask is obtained by table fitting and removal, followed by Euclidean Clustering implemented in PCL \cite{rusu20113d}. The original \textit{MaskFusion}'s segmentation module \textit{Mask-RCNN} cannot be retrained on this benchmark due to the lack of training set. Therefore, during tracking, the target object mask is computed by segmenting out the region of robot arm and end-effector from forward kinematics. For instances of irregular shapes or colors (``021\_bleach\_cleanser'', ``006\_mustard\_bottle'')  within the ``bottle'' category that \textit{6-PACK} has been trained on, it struggles to get satisfactory result. Nevertheless, \textit{BundleTrack} consistently demonstrates high quality tracking without any retraining or fine-tuning. This establishes generalizability of \textit{BundleTrack} to novel object instances regardless of their out-of-distribution properties within the category.  \textit{BundleTrack} also achieves comparable or superior performance even when compared against methods relying on object instance CAD models \cite{issac2016depth,Wthrich2013ProbabilisticOT,wense3tracknet}. 





\vspace{-0.0in}
\subsection{Analysis}\label{sec:sys_analysis}
\vspace{-0.05in}
\noindent \textbf{Ablations Study:} An ablation study investigates the effectiveness of the online global pose graph optimization and each energy term, presented in Fig. \ref{fig:analysis} (a).

\noindent \textbf{Sensitivity to Initial Pose:} As mentioned, random translation noise within 4cm range is added to the initial pose. This part further investigates robustness under different translation and rotation noise levels, shown in Fig. \ref{fig:analysis} (b).


\noindent \textbf{Computation Time:} The average running time of modules are given in Fig. \ref{fig:analysis} (c). The entire framework runs at 10Hz on average including video segmentation. The \textit{6-PACK} \cite{wang20196-pack}, \textit{TEASER++\textsuperscript{*}} \cite{Yang20troteaser} and \textit{MaskFusion} \cite{runz2018maskfusion} methods from related work run at 4Hz, 11Hz and 17Hz respectively on the same machine.


\noindent \textbf{Tracking Drift Analysis:} Fig. \ref{fig:analysis} (d) presents the rotation and translation error w.r.t. timestamps compared against representative related works \cite{runz2018maskfusion,wang20196-pack,Yang20troteaser}. Results are averaged across all videos on the \textit{NOCS Dataset}. 


\noindent \textbf{Generalization:} The neural networks' weights and hyper-parameters in \textit{BundleTrack} are fixed without any retraining or fine-tuning across all evaluations (Sec. \ref{sec:eval_nocs}, \ref{sec:eval_eoat}). When applied to novel instances, the framework does not require access to \textit{instance or category-level 3D models} for training or registration.




\noindent \textbf{Failure Cases:}  While \textit{BundleTrack} is able to robustly keep tracking in all experiments without lost or re-initialization, intermediate imprecise estimates are observed, such as the cases illustrated in Fig. \ref{fig:failure}. 




\begin{figure}[h]
  \centering
  \includegraphics[width=0.48\textwidth]{sections/figs/failure.pdf}
  
  \vspace{-0.1in}\caption{Some of the most challenging cases for \textit{BundleTrack} on the \textit{NOCS Dataset}. \textbf{Top:} Severe self-occlusion prevents data association around the mug's handle, introducing challenges for solving the orientation around the green axis. Nevertheless, with better visibility in subsequent frames, \textit{BundleTrack} is able to recover from drifts and continue tracking, thanks to the memory-augmented pose graph optimization. \textbf{Bottom:} Near the end of video, noisy segmentation (purple mask) falsely ignores the side of the bowl, preventing relevant feature extraction and leads to slight translation offset. With future development of more advanced segmentation module, the overall tracking performance is expected to be boosted.}
  \label{fig:failure}
\end{figure}
 

\section{CONCLUSION}
\vspace{-0.05in}
This  work  presents \textit{BundleTrack}, a general framework for tracking the 6D pose of novel objects without any assumptions on \textit{instance or category-level 3D models}. Extensive experiments demonstrate that it is able to perform long-term accurate tracking under various challenging scenarios. It even achieves comparable performance to state-of-art methods that depend on the target object's CAD model. Future research includes the exploration of combining \textit{BundleTrack} with model-free grasping methods \cite{ten2017grasp, murali20206}, to perform robust pick-and-place \cite{morgan2021visiondriven,mitash2020task} or in-hand dexterous manipulation for a wide variety of novel objects. 



\bibliographystyle{IEEEtran}
\bibliography{ref.bib}



\end{document}
