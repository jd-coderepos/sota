\section{Experiments}
\label{sec:exp}
\begin{table*}[t]
\vspace{-4mm}
\centering{
{\midsepremove
\begin{tabular}[c]{lcccccc}
\toprule
 & \multicolumn{2}{c}{Motion Quality} & \multicolumn{2}{c}{Motion Diversity} & \multicolumn{1}{c}{Motion-Music Corr} & User Study  \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7}
 & \tabincell{c}{\FIDkArrow} & \tabincell{c}{\FIDmArrow} & \tabincell{c}{\DivkArrow} & \tabincell{c}{\DivmArrow} & \tabincell{c}{\BeatMetricArrow} & \tabincell{c}{FACT WinRate} \\
\hline
AIST++ & -- & -- & 9.057 & 7.556 & 0.292 & --  \\
AIST++ (random)  & -- & -- & -- & -- & 0.213 & 25.4\% \\
\hline
Li~\etal\cite{li2020learning} & 86.43 & 20.58 & 6.85* & 4.93 & 0.232 & 80.6\% \\
Dancenet~\cite{zhuang2020music2dance} & 69.18 & 17.76 & 2.86 & 2.72 & 0.232 & 71.1\%\\
DanceRevolution~\cite{huang2021} & 73.42 & 31.01 & 3.52 & 2.46 & 0.220 & 77.0\% \\
\hline
\rowcolor{Gray}
FACT (ours) & \textbf{35.35} & \textbf{12.40} & \textbf{5.94} & \textbf{5.30}  & \textbf{0.241} & -- \\
\bottomrule
\end{tabular}}}
\midsepdefault
\caption{\textbf{Conditional Motion Generation Evaluation on AIST++ dataset.} {Comparing to the three recent state-of-the-art methods, our model generates motions that are more realistic, better correlated with input music and more diversified when conditioned on different music. }
*Note Li~\etal~\cite{li2020learning}'s generated motions are discontinuous making its average kinetic feature distance (\FIDk) abnormally high. }
\label{tab:exp_comparison}
\vspace{-3mm}
\end{table*} 
\subsection{AIST++ Motion Quality Validation}
\label{sec:motion_eval}
We first carefully validate the quality of our 3D motion reconstruction. 
Possible error sources that may affect the quality of our 3D reconstruction include inaccurate 2D keypoints detection and the estimated camera parameters.
As there is no 3D ground-truth for AIST dataset, our validation here is based-on the observation that the re-projected 2D keypoints should be consistent with the predicted 2D keypoints which have high prediction confidence in each image. 
We use the 2D mean per joint position error MPJPE-2D, commonly used for 3D reconstruction quality measurement~\cite{kocabas2019vibe, h36m_pami, multiviewpose}) to evaluate the consistency between the predicted 2D keypoints and the reconstructed 3D keypoints along with the estimated camera parameters. 
Note we only consider 2D keypoints with prediction confidence over 0.5 to avoid noise. 
The  of our entire dataset is  pixels on the  image resolution, and over  of those has less than  pixels of error. 
Besides, we also calculate the PCKh metric introduced in~\cite{andriluka20142d} on our AIST++. The PCKh@0.5 on the whole set is , meaning the reconstructed 3D keypoints are highly consistent with the predicted 2D keypoints. 
Please refer to the Appendix for detailed analysis of MPJPE-2D and PCKh on AIST++.

\subsection{Music Conditioned 3D Motion Generation}

\subsubsection{Experimental Setup}
\paragraph{Dataset Split}
All the experiments in this paper are conducted on our AIST++ dataset, which to our knowledge is the largest dataset of this kind. We split AIST++ into \emph{train} and \emph{test} set, and report the performance on the \emph{test} set only. We carefully split the dataset to make sure that the music and dance motion in the \emph{test} set does not overlap with that in the \emph{train} set. 
To build the \emph{test} set, we first select one music piece from each of the 10 genres. Then for each music piece, we randomly select two dancers, each with two different choreographies paired with that music, resulting in total  unique choreographies in the \emph{test} set. The \emph{train} set is built by excluding all test musics and test choreographies from AIST++, resulting in total  unique choreographies in the \emph{train} set. Note that in the test set we \textit{intentionally} pick music pieces with different BPMs so that it covers all kinds of BPMs ranging from  to  in AIST++.  

\vspace{-3mm}
\paragraph{Implementation Details}
In our main experiment, the input of the model contains a seed motion sequence with  frames (2 seconds) and a music sequence with  frames (4 seconds), where the two sequences are aligned on the first frame. The output of the model is the future motion sequence with  frames supervised by  loss.
During inference we continually generate future motions in a auto-regressive manner at  FPS, where only the first predicted motion is kept in every step.
We use the publicly available audio processing toolbox Librosa~\cite{mcfee2015librosa} to extract the music features including: 1-dim \emph{envelope}, 20-dim \emph{MFCC}, 12-dim \emph{chroma}, 1-dim \emph{one-hot peaks} and 1-dim \emph{one-hot beats}, 
resulting in a 35-dim music feature. 
We combine the 9-dim rotation matrix representation for all  joints, along with a 3-dim global translation vector, resulting in a 219-dim motion feature. Both these raw audio and motion features are first embedded into -dim hidden representations with linear layers, then added with learnable positional encoding, before they were input into the transformer layers.
All the three (audio, motion, cross-modal) transformers have  attention heads with  hidden size. The number of attention layers in each transformer varies based on the experiments, as described in Sec.~\ref{sec:ablation_study}. 
We disregard the last linear layer in the audio/motion transformer and the positional encoding in the cross-modal transformer, as they are not necessary in the FACT model.
All our experiments are trained with  batch size using Adam~\cite{kingma2014adam} optimizer. 
The learning rate starts from  and drops to \{, \} after \{, \} steps. The training finishes after , which takes  days on  TPUs. 
For baselines, we compare with the latest work on 3D dance generation that take music and seed motion as input,
including Dancenet~\cite{zhuang2020music2dance} and Li~\etal~\cite{li2020learning}. For a more comprehensive evaluation we also compare with the recent state-of-the-art 2D dance generation method DanceRevolution~\cite{huang2021}. We adapt this work to output 3D joint locations which can be directly compared with our results quantitatively, though joint locations do not allow immediate re-targeting.
We train and test these baselines on the same dataset with ours using the \emph{official} code provided by the authors.


\subsubsection{Quantitative Evaluation}
In this section, we evaluate our proposed model FACT on the following aspects: (1) motion quality, (2) generation diversity and (3) motion-music correlation. 
Experiments results (shown in Table~\ref{tab:exp_comparison}) show that our model out-performs state-of-the-art methods~\cite{li2020learning, huang2021, zhuang2020music2dance}, on those criteria.


\paragraph{Motion Quality} 
Similar to prior works~\cite{li2020learning, huang2021}, we evaluate the generated motion quality by calculating the distribution distance between the generated and the ground-truth motions using Frechet Inception Distance (FID)~\cite{heusel2017gans} on the extracted motion features. {As prior work used motion-encoders that are not public, we measure FID with two well-designed motion feature extractors~\cite{muller2005efficient, onuma2008fmdistance} implemented in \emph{fairmotion}~\cite{gopinath2020fairmotion}: (1) a geometric feature extractor that produces a boolean vector  expressing geometric relations between certain body points in the motion sequence , (2) a kinetic feature extractor~\cite{onuma2008fmdistance} that maps a motion sequence  to , which represents the kinetic aspects of the motion such as velocity and accelerations. We denote the FID based on these geometric and kinetic features as \FIDm and \FIDk, respectively.
The metrics are calculated between the real dance motion sequences in AIST++ test set and  generated motion sequences each with  frames (20 secs). }
As shown in Table~\ref{tab:exp_comparison}, our generated motion sequences have a much closer distribution to ground-truth motions compared with the three baselines.
We also visualize the generated sequences from the baselines in our supplemental video.
\begin{figure}[t]
\vspace{-8mm}
\centering
\includegraphics[width=0.46\textwidth]{figs/diversity_visualization.pdf}
\vspace{-1mm}
\caption{\textbf{Diverse Generation Results.} Here we visualize 4 different dance motions generated using \textit{different} music but the \textit{same} seed motion. 
On the left we illustrate the 2 second seed motion and on the right we show the generated 3D dance sequences subsampled by 2 seconds.  
For rows top to bottom, the genres of the conditioning music are: Break, Ballet Jazz, Krump and Middle Hip-hop. 
Note that the seed motion come from hip-hop dance.Our model is able to adapt the dance style when given a more modern dance music (second row: Ballet Jazz). Please see more results in the supplementary video. 
} 
\label{fig:diversity_vis}
\vspace{-5mm}
\end{figure} 
\vspace{-3mm}
\paragraph{Generation Diversity}
We also evaluate our model's ability to generate {diverse dance motions when given various input music} compared with the baseline methods. Similar to the prior work~\cite{huang2021}, we calculate the {average Euclidean distance in the feature space across}  generated motions on the AIST++ \emph{test} set  to measure the diversity. The motion diversity in the geometric feature space and in the kinetic feature space are noted as \Divm and \Divk, respectively.
Table~\ref{tab:exp_comparison} shows that our method generates more diverse dance motions comparing to the baselines {except Li~\etal~\cite{li2020learning}, which discretizes the motion, leading to discontinuous outputs that results in high \Divk.} 
Our generated diverse motions are visualized in Figure~\ref{fig:diversity_vis}.

\vspace{-3mm}
\paragraph{Motion-Music Correlation} Further, we evaluate how much the generated 3D motion correlates to the input music. 
As there is no well-designed metric to measure this property, we propose a novel metric, Beat Alignment Score (BeatAlign), to evaluate the motion-music correlation in terms of the similarity between the kinematic beats and music beats.
The music beats are extracted using \emph{librosa}~\cite{mcfee2015librosa} and the kinematic beats are computed as the local minima of the kinetic velocity, as shown in Figure~\ref{fig:beat_curve}.
The Beat Alignment Score is then defined as the average distance between every kinematic beat and its nearest music beat. 
Specifically, our Beat Alignment Score is defined as:

where  is the kinematic beats,  is the music beats and  is a parameter to normalize sequences with different FPS. 
We set  in all our experiments as the FPS of all our experiments sequences is 60.
A similar metric Beat Hit Rate was introduced in~\cite{lee2019dance, huang2021}, but this metric requires a dataset dependent handcrafted threshold to decide the alignment (``hit'') while ours directly measure the distances. 
This metric is explicitly designed to be uni-directional as dance motion does not necessarily \emph{have to} match with every music beat. On the other hand, every kinetic beat is expected to have a corresponding music beat.
To calibrate the results, we compute the correlation metrics on the entire AIST++ dataset (upper bound) and on the random-paired data (lower bound).
As shown in Table~\ref{tab:exp_comparison}, our generated motion is better correlated with the input music compared to the baselines. We also show one example in Figure~\ref{fig:beat_curve} that the kinematic beats of our generated motion align well with the music beats.
However, when comparing to the real data, all four methods including ours have a large space for improvement. This reflects that music-motion correlation is still a challenging problem.

\begin{figure}[t]
\vspace{-5mm}
\centering
\includegraphics[width=0.45\textwidth]{figs/beat_curve.pdf}
\vspace{-3mm}
\caption{\textbf{Beats Alignment between Music and Generated Dance.} Here we visualize the kinetic velocity (blue curve) and kinematic beats (green dotted line) of our generated dance motion, as well as the music beats (orange dotted line). The kinematic beats are extracted by finding local minima from the kinetic velocity curve.
}
\label{fig:beat_curve}
\vspace{-5mm}
\end{figure} 
\vspace{-3mm}
\subsubsection{Ablation Study}
\label{sec:ablation_study}
We conduct the following ablation experiments to study the effectiveness of our key design choices: Full-Attention Future-N supervision, and early cross-modal fusion. Please refer to our supplemental video for qualitative comparison.
The effectiveness of different model architectures is measured quantitatively using the motion quality (\FIDk, \FIDm) and the music-motion correlation (BeatAlign) metrics, as shown in Table~\ref{tab:exp_crossmodal} and Table~\ref{tab:exp_decoder}.

\vspace{-3mm}
\paragraph{Full-Attention Future-N Supervision}
\label{sec:attn_comp}
Here we dive deep into the attention mechanism and our future-N supervision scheme.
We set up four different settings: causal-attention shift-by-1 supervision, and full-attention with future- supervision.
Qualitatively, we find that the motion generated by the causal-attention with shift-by-1 supervision {(as done in \cite{li2020learning, radford2018improving,aksan2020attention})
starts to freeze after several seconds (please see the supplemental video).
Similar problem was reported in the results of ~\cite{aksan2020attention}.
Quantitatively (shown in the Table~\ref{tab:exp_decoder}), when using causal-attention shift-by-1 supervision, the FIDs are large meaning that the difference between generated and ground-truth motion sequences is substantial.} 
For the full-attention with future-1 supervision setting, the results rapidly drift during long-range generation. 
However, when the model is supervised with  or  future frames, it pays more attention to the temporal context. Thus, it learns to generate good quality (non-freezing, non-drifting) long-range motion.

\begin{table}[t]
\vspace{-5mm}
\centering{
\resizebox{1.0\linewidth}{!}{\midsepremove
\begin{tabular}[c]{lccc}
\toprule
Attn-Supervision & \FIDkArrow & \FIDmArrow & \BeatMetricArrow \\
\hline
Causal-Attn-Shift-by-1 & 111.69 & 21.43 & 0.217\\
Full-Attn-F1 (FACT-1) & 207.74 & 19.35 & 0.233\\
Full-Attn-F10 (FACT-10) & \textbf{35.10} & 15.17 & 0.239 \\
\hline
\rowcolor{Gray}
Full-Attn-F20 (FACT-20) & 35.35 & \textbf{12.39} & \textbf{0.241} \\
\bottomrule
\end{tabular}}}
\midsepdefault
\caption{\textbf{Ablation Study on Attention and Supervision Mechanism.} Causal-attention shift-by-1 supervision tends to generate freezing motions in the long-term. While Full-attention supervised more future frames boost the ability of generating more realistic dance motions. }
\label{tab:exp_decoder}
\vspace{-2mm}
\end{table} \begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{figs/atten_weights.pdf}
\caption{\textbf{Attention Weights Visualization.} We compare the attention weights from the last layer of the (a) 12-layer cross-modal transformer and (b) 1-layer cross-modal transformer. Deeper cross-modal transformer pays equal attention to motion and music, while a shallower one pays more attention to motion.
}
\label{fig:atten_weights}
\vspace{-4mm}
\end{figure} 
\vspace{-3mm}
\paragraph{Early Cross-Modal Fusion} 
Here we investigate when to fuse the two input modalities.
We conduct experiments in three settings, (1) \emph{No-Fusion}: 14-layer motion transformer only; (2) \emph{Late-Fusion}: 13-layer motion/audio transformer with 1-layer cross-modal transformer; (3) \emph{Early-Fusion}: 2-layer motion/audio transformer with 12-layer cross-modal transformer. 
For fair comparison, we change the number of attention layers in the motion/audio transformer and the cross-modal transformer but keep the total number of the attention layers fixed.
Table~\ref{tab:exp_crossmodal} shows that the early fusion between two input modalities is critical to generate motions that are well correlated with input music. Also we show in Figure~\ref{fig:atten_weights} that Early-Fusion allows the cross-model transformer pays more attention to the music, while Late-Fusion tend to ignore the conditioning music.
This also aligns with our intuition that the two modalities need to be fully fused for better cross-modal learning, as contrast to prior work that uses a single MLP to combine the audio and motion~\cite{li2020learning}.


\vspace{-3mm}
\subsubsection{User Study}
\label{sec:user_study}
Finally, we perceptually evaluate the motion-music correlation with a user study to compare our method with the three baseline methods and the ``random" baseline, which randomly combines AIST++ motion-music. (Refer to the Appendix for user study details.)
In this study, each user is asked to watch 10 videos showing one of our results and one random counterpart, and answer the question \emph{``which person is dancing more to the music? LEFT or RIGHT''} for each video. 
For user study on each of the four baselines, we invite 30 participants, ranging from professional dancers to people who rarely dance. 
We analyze the feedback and the results are: (1)  of our generated dance motion is better than Li~\etal~\cite{li2020learning}; (2)  of our generated dance motion is better than Dancenet~\cite{zhuang2020music2dance}; {(3)  of our generated dance motion is better than DanceRevolution~\cite{huang2021}; }(4)  of the unpaired AIST++ dance motion is better than ours. 
Clearly we surpass the baselines in the user study. 
But because the ``random'' baseline consists of real advanced dance motions that are extremely expressive, participants are biased to prefer it over ours. 
However, quantitative metrics show that our generated dance is more aligned with music.

\begin{table}[t]
\vspace{-5mm}
\centering{
\midsepremove
\begin{tabular}[c]{lccc}
\toprule
Cross-Modal Fusion & \FIDkArrow & \FIDmArrow & \BeatMetricArrow\\
\hline
No-Fusion & 45.66 & 13.27 & 0.228* \\
Late-Fusion  & 45.76 & 14.30 & 0.234 \\
\hline
\rowcolor{Gray}
Early-Fusion & \textbf{35.35} & \textbf{12.39} & \textbf{0.241} \\
\bottomrule
\end{tabular}}
\midsepdefault
\caption{{\textbf{Ablation Study on Cross-modal Fusion.} Early fusion of the two modalities allows the model to generate motion sequences align better with the conditioning music. *Note this number is calculated using the music paired with the input motion.}}
\label{tab:exp_crossmodal}
\vspace{-2mm}
\end{table} 