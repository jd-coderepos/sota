Our ultimate goal is to learn an effective classifier from the noisy labeled dataset without any form of human supervision (i.e., label verification), prior knowledge on the target domain, or label/noise distribution. In this section, we elaborate our multi-step model training framework. As is illustrated in Fig. \ref{fig:iterative_labelnet}, we first describe vector representation, which is necessary for similarity measure and label prediction in our framework. Next, our proposed probabilistic dependence model ``NoiseRank'' is elaborated for ranking dataset examples based on their likelihood of being noisy. Finally, we discuss the iterative model training steps. 














\subsection{Vector Representation} \label{sec:rep_learning}

The vector space representation (i.e., embeddings) is a core component of our design because we rely on the vector representations to determine content similarity between examples in the given noisy dataset. Our framework is agnostic to any modality as well as the solution for learning representations. However, a high-quality representation improves the similarity measure and thus our unsupervised method for label noise detection. In Sec. \ref{sec:labelnet_classification}, we will discuss how we improve the representation through iterative training.

With the vector representation, we could determine the content similarity. More formally, let the noisy labeled dataset be denoted as  where  is the vector representation for example , and  is the given label (potentially incorrect) with  being the total number of classes in the dataset. In this work, we define instance similarity in terms of Euclidean distance between  and  as .






































\subsection {Label Noise Detection}



In this section, we first describe our process for generating class prototypes, that are representative instances selected for each of the  classes in the dataset; followed by our non-parametric approach to generate label predictions, , for each prototype . Let  denote the predictions. Next, we elaborate the proposed dependence model, named NoiseRank, to globally rank the dataset examples based on their likelihood of having incorrect labels given their vector representations, labels and predictions. 





\subsubsection {Generating class prototypes} 
\label{sec:class_prototypes} Each of the  classes in the dataset can be represented by a set of class prototypes, i.e. a representative subset of instances in that class. We select the prototypes using K-means clustering on the vector space representations of instances in each class, given by the noisy labels. As a rule of thumb \cite{kodinariya2013review}, we select  cluster centroids per class, where  is the average number of instances per class in the dataset. Selecting class prototypes is beneficial towards improving scalability when the number of dataset instances grows.
We find that it is also important for robustness in high noise datasets, and K-means based selection is effective compared to randomly selected prototypes.

\subsubsection {Generating label predictions} 
\label{sec:label_pred} For each prototype instance  represented by vector , we generate the predicted label   by a weighted k nearest neighbor classifier, as specified in Eq. \ref{weighted_knn}.



where  is the indicator function, and the distance kernel function  is used to weigh the contribution of each neighbor  in the neighborhood  comprising the  nearest neighbors of :  


where  is the distance function discussed in Sec.~\ref{sec:rep_learning}, and  and  are parameters for the bias and weight exponent, respectively. The kernel function is negatively correlated to the distance function. For example, when , the kernel will be inversely proportional to the squared distance between the instances. Since , by setting a positive bias , we can prevent  from being undefined when .

\subsubsection {Dependence Model Formulation} \label{sec:dep_formulate}

Our formulation is to estimate the posterior probability  that indicates the likelihood of label noise for all examples  in the dataset  and rank them based on this estimate. 
For this purpose, we use Markov Random Fields (also known as MRFs or ``dependence models'' \cite{MetzlerMRF05,YalnizPAMI19}) which provide a generic framework for modeling the joint distribution of a large set of random variables. 

In dependence models, conditional dependencies are defined only for certain groups of random variables called ``cliques'', and are represented with edges in an undirected graph. We represent the graph with  and the cliques in the graph as  in our formulations. For each type of clique , we define a non-negative potential function  parameterized by . The joint probabilities are estimated based on the Markov assumption as follows:


where  is a normalization term.
Computing  is very expensive due to the large number of summands. Since our aim is to rank examples in the dataset based on their posterior probabilities  and ignoring  in this formulation does not change the ranking result, the posterior probability is estimated as follows:  

where  indicates rank equivalence. The formulation is a sum of logarithm of potential functions over all cliques. For simplification purposes, the potential function is assumed to be ,
where  is the feature function over the clique  and
 is the weight for the feature function.
The final ranking function is computationally tractable and linear over feature functions:


Depending on the choice of the feature functions and their corresponding weights, the final ranking score in~\ref{eq:rankfunction} can be negative. In the next subsection, we elaborate our dependence model for the task of label noise detection by explicitly defining each clique, its feature function  and the corresponding weight .







\subsubsection{Dependence Graph Construction}

\begin{figure}[t]
\centering
\includegraphics[width=85mm]{img/dependence_graph_fixed.png}
\caption{The dependence graph illustrated for a given example  in a database containing five examples. Clique types and weights are determined based on the given  and  and predicted labels . Edge lengths indicate distance in the vector representation space}
\label{fig:dependence_graph}
\end{figure}








In our formulation, we define cliques between all pairs of examples  where  and  for dataset  with size  and set of all prototypes . There are  cliques defined in the dependence graph. 
Each example is associated with its given label  and each prototype is associated with its given label  and predicted label , which are used for determining the clique weights as shown in Fig. \ref{fig:dependence_graph} and explained below.
All cliques are assumed to share the same feature function  as defined by the kernel function in Eq. \ref{eq:dist_kernel}. 











We differentiate cliques into four types based on the values of the given and predicted labels of the examples. 
The first clique type, denoted by , is for all pairs of examples  that share the same given label. If the examples share the same label (i.e., ), we assign a negative ``blame'' score (i.e., reward) weighted by  to example  so that it ranks lower in the final rank list of examples sorted by their overall MRF scores. For this clique type, we set the clique weight parameter as . 

The second clique type is denoted by . It is defined for all pairs of examples  with different labels (i.e., ) where . In this case, example  blames example  for providing an incorrect prediction vote, even though the false vote did not change the prediction output  which is consistent with example 's original label . For this type, we set , where  is a hyper-parameter defined in the range  to control the impact of incorrect vote (i.e.,  on the blame score. 







The third clique type, denoted by , is for all pairs of examples  with different labels (i.e., ) where  and . In other words, the prediction output  is different from both its own label  and example 's label . While example  did not directly influence the mispredicted label, it did not contribute towards the correct prediction. By setting , example  assigns a scaled blame score to example .



The fourth clique type, denoted by , is for all pairs of examples  with different labels (i.e., ) where  and . Example  blames example  strongly for supporting a prediction different from its own label  which is the same as its prediction . For this type, we set  where  in  is the ``blame factor'' and controls the strength of the blame.










\subsubsection{NoiseRank Score Function}




The final ranking function in Eq. \ref{eq:combined_score_func} is the sum of all blame and reward scores accumulated for example  according to Eq. \ref{eq:rankfunction}. 



The aggregate score  is the basis for ranking instances in the dataset. The rank reflects the relative likelihood of being mislabeled and the impact on mispredictions, accounted for in the penalty function. Since the score function is unbounded, detecting label noise given the ranked list requires threshold  to determine if an instance with detected label noise should be retained () or removed () from the dataset.










It should be noted that as the dataset size increases, computing the ranking function over all  pairs is less efficient. Moreover, the value of the feature function  approaches zero as distances between pairs increase. We therefore limit the cliques to the  closest neighbors of example  for assigning blame and reward scores as defined above. This approximation is quite effective especially because the blame and reward scores diminish rapidly and approach zero as distances get larger. 
Another note is that we use the same  value in the score function and in the kernel function (Eq.~\ref{eq:dist_kernel}) for both computing the aggregate rank score function and generating label predictions  for simplification purposes. 












\subsection{Iterative Training}\label{sec:labelnet_classification}
We provide a generic iterative framework to learn classifiers with label noise reduction. As described earlier, the vector space representations of examples in the dataset are used in determining content similarity for noise ranking. Initially, representations can be learned with the available (potentially noisy) labels. In order to improve the learned representations, we can iterate over representation learning, noise ranking and reduction, model training, in order.

The framework is agnostic to the model used for representation learning and classification, depending on the content modality. At a first step, we train the classifier model (eg. standard CNN with simple cross-entropy loss) with the available noisy dataset . The classifier can be used to extract representations for examples in the dataset, which are used to run label noise detection with \algName and remove the examples that are ranked as noisy (i.e., ). Finally, we fine-tune the trained model with the denoised subset of the dataset. 








