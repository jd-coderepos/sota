
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{hyperref}
\usepackage{url}
\usepackage{float}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[pdftex]{graphicx}

\title{StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training}
\iclrfinalcopy
\hypersetup{hidelinks}


\author{Yuechen Yu\textsuperscript{\rm }, Yulin Li\textsuperscript{\rm }, Chengquan Zhang\textsuperscript{\rm }\thanks{\rm  Equal contribution. Correspondence to: Chengquan Zhangzhangchengquan@baidu.com.}, Xiaoqiang Zhang, Zengyuan Guo, \\
\textbf{Xiameng Qin, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang} \\
Department of Computer Vision Technology (VIS), Baidu Inc.\\
\texttt{\href{https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT}{https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT}
}
}
\makeatletter
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
In this paper, we present StrucTexTv2, an effective document image pre-training framework, by performing masked visual-textual prediction. It consists of two self-supervised pre-training tasks: masked image modeling and masked language modeling, \emph{based on text region-level image masking}. The proposed method randomly masks some image regions according to the bounding box coordinates of text words. The objectives of our pre-training tasks are reconstructing the pixels of masked image regions and the corresponding masked tokens simultaneously. Hence the pre-trained encoder can capture more textual semantics in comparison to the masked image modeling that usually predicts the masked image patches. Compared to the masked multi-modal modeling methods for document image understanding that rely on both the image and text modalities, StrucTexTv2 models image-only input and potentially deals with more application scenarios free from OCR pre-processing. Extensive experiments on mainstream benchmarks of document image understanding demonstrate the effectiveness of StrucTexTv2. It achieves competitive or even new state-of-the-art performance in various downstream tasks such as image classification, layout analysis, table structure recognition, document OCR, and information extraction under the end-to-end scenario. 




\end{abstract}

\section{Introduction}
In the Document Artificial Intelligence, how to understand visually-rich document images and extract structured information from them has gradually become a popular research topic. Its main associated tasks include document image classification~\cite{icdar15cdip}, layout analysis~\cite{zhong2019publaynet}, form understanding~\cite{jaume2019funsd}, document OCR (also called text spotting)~\cite{li2017towards,lyu2018mask}, and end-to-end information extraction (usually composed of OCR and entity labelling phrase)~\cite{WangLJT0ZWWC21}, etc. To solve these tasks well, it is necessary to fully exploit both visual and textual cues. Meanwhile, large-scale self-supervised pre-training~\cite{li2021structurallm,appalaraju2021docformer,xu2020layoutlm,xu2020layoutlmv2,huang2022layoutlmv3,nips2021udoc} is a recently rising technology to enhance multi-modal knowledge learning of document images.


\begin{figure*}
 \centering
 \includegraphics*[width=1.0\textwidth]{imgs/intro_fig_v6.pdf}
\caption{Comparisons with the main-stream pre-training models of document image understanding. (a) It shows the masked multi-modal modeling methods which input both OCR results and image embeddings. (b) The framework that inputs image-only embeddings is suitable for vision-dominated tasks like document image classification and layout analysis. (c) StrucTexTv2 learns visual-textual representations using only the information from images in the pre-training step and then optimizes various downstream tasks of document image understanding end-to-end.}
\label{fig:intro_arch}
\vspace{-1em}
\end{figure*}

There are two mainstream self-supervised pre-training frameworks for document image understanding. As illustrated in Fig.~\ref{fig:intro_arch}: (a) The first category is the masked multi-modal modeling such as the proposed pre-training tasks: MLM~\cite{devlin2018bert}, MVLM~\cite{xu2020layoutlmv2}, MM-MLM~\cite{appalaraju2021docformer} and MSM~\cite{nips2021udoc}, whose inputs mainly consists of OCR-extracted texts and image embeddings. The methods collect semantic information from text and image, depending heavily on front-end OCR engines with certain computing costs. Additionally, the two components of OCR engine and document understanding module are separately optimized, which is hard to ensure performance of the whole system. (b) The second category is the masked image modeling (MIM) that inherits the concept of vision-based self-supervised learning such as BEiT~\cite{bao2021beit}, SimMIM~\cite{Xie00LBYD022}, MAE~\cite{he2021masked}, CAE~\cite{chen2022context}, and DiT~\cite{li2022dit}, etc. MIM is a powerful image-only pre-training technique to learn the visual contextualized representations of document images. Because of the insufficient consideration of textual contents, MIM often applies to some vision-dominated tasks~\cite{li2022dit} like image classification, layout analysis and table detection. (c) To take full advantage of multi-modality knowledge on the basis of MIM, we propose the third pre-training framework by learning visual-textual representations with image-only inputs, to optimize the performance of document image understanding tasks in an end-to-end manner.


 Due to the great disparities between vision and language, the existing document understanding methods either consider a single modality or introduce an OCR engine to capture textual content in advance. Researchers used text tokens as the input in language modeling, or selected fixed-size image patches as the granularity of vision pre-training tasks. However, the textual content is visually-situated in a document and extracted from the image. Thus, we propose the text region-level image masking scheme corresponding to document content to bridge vision modeling to language modeling with the shared representations.







This paper proposes StrucTexTv2, a novel multi-modal knowledge learning framework for document image understanding by performing text region-level image masking with dual parallel self-supervised tasks of image reconstruction and language modeling (as shown in Fig.~\ref{fig:method_arch}). First off, we adopt an off-the-shelf OCR toolkit to perform word-level text detection and text recognition on the pre-training dataset (IIT-CDIP Test Collection~\cite{sigir06cdip}). Next, we randomly mask some text word regions given the input images and fed them into the encoder. Finally, the pre-training objectives of StrucTexTv2 learn to reconstruct image pixels and text content of the masked words. In support of the proposed pre-training tasks, we introduce a new backbone network for StrucTexTv2. In particular, a CNN-based network with the RoI-Align~\cite{he2017mask} operation produces visual features for the masked regions. Inspired by ViBERTGrid~\cite{lin2021vibertgrid}, the backbone uses FPN~\cite{lin2017feature} to integrate features of CNN. The following transformer model enables capturing semantical and contextualized representations from the visual features. We evaluate and verify our pre-trained model in five tasks including document image classification, layout analysis, table structure recognition, document OCR, and end-to-end information extraction, all of which have achieved significant gains. The experimental results have also confirmed that the framework of StrucTexTv2 can construct fundamental pre-trained models for document image understanding.

The major contributions of our work can be summarized as following:
\begin{itemize}
 \item A novel self-supervised pre-training framework by performing text region-level document image masking, named StrucTexTv2, is proposed to learn visual-textual representations in an end-to-end manner.
 
 \item Superior performance in five downstream tasks demonstrates the effectiveness of the StrucTexTv2 pre-trained model in document image understanding.
\end{itemize}

\section{Related Work}
\noindent\textbf{Self-supervised Learning} 
Thanks to the development of self-supervised tasks and Transformer architectures, in the past few years, the computer vision (CV) and natural language processing (NLP) have achieved breakthroughs in knowledge learning from large-scale unlabeled data. In the domain of NLP, Masked Language Modeling (MLM) task has been widely used in pre-trained models~\cite{devlin2018bert,radford2018gpt} due to its efficiency and effectiveness. The MLM task randomly masks a set of text tokens of input and reconstructs them according to the context around the masked tokens. In the CV domain, taking a similar idea, Masked Image Modeling (MIM) has also been successfully verified. There are several variants of MIM, for example, BEiT~\cite{bao2021beit} and CAE~\cite{chen2022context} randomly mask a percentage of image patches and predict discrete image patch tokens learned by dVAE~\cite{ramesh2021dvae}. MAE~\cite{he2021masked} and SimMIM~\cite{Xie00LBYD022} take a simpler behavior, predicting RGB values of raw pixels by direct regression.

\noindent\textbf{Document pre-trained models} 
As stated above, the cutting-edge document pre-training models can be roughly separated into two categories: masked multi-modal modeling and masked image modeling approaches. The representative works of the former are LayoutLM series~\cite{xu2020layoutlm,xu2020layoutlmv2,huang2022layoutlmv3}, Docformer~\cite{appalaraju2021docformer}, UDoc~\cite{nips2021udoc} and StrucTexT~\cite{li2021structext}, they usually depend on a mature OCR engine to extract textual content from document images, and fed them to the encoder combined with image embeddings. These OCR-based methods promise excellent performance, but they are cumbersome to solve some vision-dominated tasks. The latter methods, such as DiT~\cite{li2022dit} almost following the idea of BEiT~\cite{bao2021beit}, directly use a general CV pre-training framework to learn on large-scale document image data. Due to the lack of guidance for text information, the model is likely to be deficient in semantic understanding. Recently, prompt-learning~\cite{LiuYFJHN23} has been studied for adapting pre-trained vision-language models. Those prompt-based methods~\cite{kim2021donut,DavisMPTWM22} directly generate textual output from documents and achieve competitive performance on downstream tasks.

In this paper, the proposed StrucTexTv2 is a new solution that integrates the advantages of CV and NLP pre-training methods in an end-to-end manner. Benefiting from image-only input of the encoder, our framework can avoid the interference of false OCR results compared with the OCR-based pre-trained models. For our pre-training, although the supervision labels partially come from OCR results, only the high-confidence words from OCR results are randomly selected. The impacts of the OCR quality on our pre-trained model is alleviated to a certain extent.


\section{Approach}


\subsection{Model Architecture}
As illustrated in Fig.~\ref{fig:method_arch}, there are two main components of StrucTexTv2: a encoder network using FPN to integrate visual features and semantic features, and the pre-training framework containing two objectives: Mask Language Modeling and Mask Image Modeling.


The proposed encoder consists of a visual extractor (CNN) and a semantic module (Transformer). Given an input document image, StrucTexTv2 extracts visual-textual representations through this backbone network. Specifically, the features of the last four down-sampled stages on CNN are extracted from the visual extractor. In the semantic module, following ViT~\cite{DBLP:journals/corr/abs-2010-11929} to handle 2D feature maps, the features of the last stage in CNN are flattened in patch-level and are linearly projected to obtain 1D sequence of patch token embeddings, which also serves as the effective input for the Transformer. A relative position embedding representing the token index is added to the token embeddings. 
Then, the standard Transformer receives the input token embeddings and outputs the semantic enhanced features. We reshape the output features back to context feature maps in the 2D visual space and up-sample the feature maps with the factors of 8. We adopt FPN strategy~\cite{lin2017feature} to merge visual features of different resolutions from CNN and then concatenate context feature maps with them, deriving a feature map with 1/4 size of the input image. Finally, a fusion network which consists of two successive 11 convolutional layers is introduced to take a further multi-modal fusion.

\begin{figure*}[t]
 \centering
 \includegraphics[width=1.0\textwidth]{imgs/arch_fig_v2.pdf}
\caption{The overview of StrucTexTv2. Its encoder network consists of a visual extractor (CNN) and a semantic module (Transformer). Given a document image, the encoder extracts the visual feature of the whole image by CNN and obtains the semantic enhanced feature through a Transformer. Subsequently, a lightweight fusion network is utilized to generate the final representation of the image. With the help of ROI Alignment, the multi-modal feature of each masked text region is processed by the MIM branch and the MLM branch to reconstruct the pixels and text, respectively.}
\label{fig:method_arch}
\vspace{-0.5em}
\end{figure*}













\subsection{Pre-training}

To enhance the document image understanding of StrucTexTv2, we perform pre-training on a large-scale document dataset IIT-CDIP Test Collection 1.0 dataset~\cite{sigir06cdip}. Unlike MAE~\cite{he2021masked}, BEiT~\cite{bao2021beit}, and DIT~\cite{li2022dit}, we do not use patch-level masking strategy for pre-training. Instead, we use a novel text region-level masking strategy and employ two self-supervised pre-training tasks to learn visual-textual representations. The subsequent experiment results suggest that the fine-grained text region-level masking strategy is more suitable than the coarse-grained patch-level masking strategy in document understanding.

\subsubsection{Task \#1: Masked Language Modeling}
To encourage the model to learn contextual representation, similar to BERT~\cite{devlin2018bert}, we mask a portion of the text region of the input document with RGB values [255, 255, 255] at random. We find that 30\% of masking ratio is the best choice in our experiments. According to the contextual information of the unmasked regions in a document, the encoder-decoder architecture learns to predict the indexes of text tokens (in a pre-defined vocabulary) of the masked text regions with cross-entropy loss. To avoid the interference of sub-words produced by WordPiece~\cite{song2020fast}, we only choose the first sub-word in each word for keeping the number of words in the document images. Notice that the Masked Language Modeling task in StrucTexTv2 does not require the text input, however, which is essential for the NLP domain.


\noindent{\textit{Decoder for Task \#1.}}
For Mask Language Modeling, we employ an extremely 2-layer MLP as a decoder to project the encoding feature. The multi-model feature of each text region is extracted from the fused feature maps  by ROI-Align~\cite{ren2015faster}, which is computed as follows,

where  is the bounding box of the th text region, and the  is optimized by cross-entropy loss with 30,522 token categories.

\subsubsection{Task \#2: Masked Image Modeling}
In MAE~\cite{he2021masked}, BEiT~\cite{bao2021beit}, and SimMIM~\cite{Xie00LBYD022}, patch-level Masked Image Modeling has shown strong potential in representation learning. However, in the document understanding domain, patch-level feature learning is too coarse to represent the details of text or word region. Therefore, we introduce a text region-level visual representation learning task called Masked Image Modeling to enhance document representation and understanding. Instead of classifying the classes defined by tokenization like LayoutLMv3 and BEiT, we regress the raw pixel values of the masked text region with mean square error loss following SimMIM and MAE. Specifically, we mask the rectangle text regions and predict the RGB values of missing pixels, leading to significant improvement for performance in representation learning.

\noindent{\textit{Decoder for Task \#2.}} 
We develop a Fully Convolutional Network (FCN) with Transpose Convolution to carry out the document image reconstruction of masked text regions. Specifically, we apply the global average pooling to aggregate each text region's feature and generate the embedding  that mainly represents the visual "style" for each masked text region. To strengthen its text information, we encode the MLM prediction  to  by using an embedding layer, denoting the "content" knowledge. At last, we concatenate  and  and feed it to a FCN, generating the final restored image prediction . The procedure of Mask Image Modeling can be formulated as follows,



where GAP is the global average pooling operator. In MIM, we follow MAE and predict the missing pixels of masked text regions. For example, we resize the spatial resolutions of each masked text region to fixed 6464, and each text-region's regression target is 12,288=64643 pixels of RGB values. The  is optimized by MSE loss in the pre-training phrase.

\subsection{Downstream Tasks}

The StrucTexTv2 pre-training scheme contributes to a visual-textual representation with input of image-only. The multi-modal representation is available to model fine-tuning and profits numerous downstream tasks.

\subsubsection{Task \#1: Document Image Classification}
Document image classification aims to predict the category of each document image, which is one of the fundamental tasks in office automation. For this task, we downsample the output feature maps of backbone network by four 33 convolutional layers with stride 2. Next, the image representation is fed into the final linear layer with softmax to predict the class label.


\subsubsection{Task \#2: Document Layout Analysis}
Document layout analysis aims to identify the layout components of document images by object detection. Following DiT, we leverage Cascade R-CNN~\cite{cai2018cascade} as the detection framework to perform layout element detection and replace its backbone with StrucTexTv2. Thanks to the multi-scale context design of backbone networks, four resolution-modifying features (P2P5) of FPN fusion layers on backbone networks are sent into the iterative structure heads of the detector.

\subsubsection{Task \#3 Table Structure Recognition}
Table structure recognition aims to recognize the internal structure of a table which is critical for document understanding. Specifically, We employ Cascade R-CNN for cell detection in our table structure recognition framework while replacing the feature encoder with backbone networks. Since some table images are collected by cameras and many cells are deformed, we modify the final output of Cascade R-CNN to the coordinate regression of four vertexes of cells.

\subsubsection{Task \#4: Document OCR}
We tend to read the text in an end-to-end manner based on StrucTexTv2.
Our OCR method consists of both the word-level text detection and recognition modules. They share the features of backbone networks and are connected by ROI-Align operations. The text detection module adopts the standard DB~\cite{liao2020real} algorithm, which predicts the binarization mask for word-level bounding boxes. Similar to NRTR~\cite{sheng2019nrtr}, the text recognition module is composed of multi-layer Transformer decoders to predict character sequences for each word.

\subsubsection{Task \#5: End-to-end Information Extraction}
The aim of the task is to extract entity-level content of key fields from given documents without predefined OCR information. We evaluate the StrucTexTv2 model based on the architecture of document OCR and devise a new branch for semantic entity extraction. Concretely, another DB detection is developed to identify the entity bounding boxes. An additional MLP block is performed with the ROI features to classify entity label. These bounding boxes are utilized for word grouping to merge the text content from Task \#4. At length the key information is obtained by grouping words according to the reading order.


\section{Experiments}


\subsection{Datasets} 
\noindent\textbf{Pre-training Data} By following DiT~\cite{li2022dit}, we pretrain StrucTexTv2 on the IIT-CDIP Test Collection 1.0 dataset~\cite{sigir06cdip}, whose 11 million multi-page documents are split into single pages, totally 42 million document images. 


\noindent\textbf{RVL-CDIP}~\cite{icdar15cdip} contains 400,000 grayscale document images in 16 classes, with 25,000 images per class. 
We adopt RVL-CDIP as the benchmark to carry out experiments on document classification task. Average classification accuracy is used evaluate model performance.

\noindent\textbf{PubLayNet}~\cite{zhong2019publaynet} consists of more than 360,000 paper images built by automatically parsing PubMed XML files. The five typical document layout elements (text, title, list, figure, and table) are annotated with bounding boxes. Mean average precision (mAP) @ intersection over union (IOU) is used as the evaluation metric of document layout analysis.

\noindent\textbf{WTW}~\cite{long2021parsing} covers unconstrained table in natural scene, requiring table structure recognizer to have both discriminative and generative capabilities. It has a total of 14581 images in a wide range of real business scenarios and the corresponding full annotation (including cell coordinates and row/column information) of tables. 

\noindent\textbf{FUNSD}~\cite{jaume2019funsd} is a form understanding dataset that contains 199 forms, which refers to extract four predefined semantic entities (questions, answers, headers, and other) and their linkings presented in the form. We focus on two tasks of document OCR and end-to-end information extraction on FUNSD. For evaluation, we compute the normalized Levenshtein similarity (1-NED) between the predictions and the ground truth.


\subsection{Implementation Details}
\noindent\textbf{Pre-training on IIT-CDIP} The proposed encoder network of StrucTexTv2 is composed mainly of the CNN and Transformer. To balance efficiency and effectiveness,  consists of ResNet-50 and 12-layer Transformers (128 hidden size and 8 attention heads) and introduces only 28M parameters. A larger version of  is set up as ResNeXt-101~\cite{xie2017aggregated} and 24-layer Transformers (768 hidden size and 8 attention heads), which total parameters are 238M. We use the networks trained with ImageNet~\cite{deng2009imagenet} as the initialization of CNNs. The Transformers are initialized from the language models~\cite{SunWLFTWW20}. 
 and  take 17 hours and 52 hours to train one epoch of the IIT-CDIP data, respectively. The whole pre-training phase takes nearly a week with 32 Nvidia Tesla 80G A100 GPUs.

\noindent\textbf{Fine-tuning on RVL-CDIP} We evaluate StrucTexTv2 for document image classification. We fine-tune the model on RVL-CDIP for 20 epochs with cross-entropy loss. The learning rate is set for 3e-4 and the batch size is 28. The input images are resized to 960960 and maintain its aspect ratio. We use label smoothing=0.1 in the loss function. Besides, the data augmentation methods such as CutMix and MixUp with 0.3 probability are applied on the training phase.

\noindent\textbf{Fine-tuning on PubLayNet} We evaluate on the validation set of PubLayNet for document layout analysis. We fine-tune the Cascade R-CNN and initialize the backbone with our pre-trained model. The detector is trained 8 epochs with Momentum optimizer and a batch size of 8. The learning rate is set to 1e-2, while it decays to 1e-3 on 3 epoch and decays 1e-4 on 6 epoch. We use random resized cropping to augment the training images while the short edges does not exceed 800 pixels.

\noindent\textbf{Fine-tuning on WTW} We conduct experiments on WTW for table structure recognition. We also employ Cascade R-CNN to detect the cells of table whose backbone is replaced by pre-trained StrucTexTv2. We fine-tune our model end-to-end using ADAM~\cite{kingma2014adam} optimizer for 20 epochs with a batch size of 16 and a learning rate of 1e-4. The input images are resized to 640  640 after random scaling and the long size being resized to 640 pixels.

\noindent\textbf{Fine-tuning on FUNSD} On account of the full annotations, both the document OCR and end-to-end information extraction tasks are measured on FUNSD. We set the text recognition network as a 6-layers Transformer and fine-tune the whole model for 1200 epochs with a batch size of 32. We follow a cosine learning rate policy and set the initial learning rate to 5e-4. Extra position embeddings are appended to roi-features and we pass it to each layer of decoders. The training losses except DB detector are the cross-entropy function. Additionally, the same loss is estimated for each decoder layer in the text recognition module for better convergence training.

\subsection{Comparisons with the State-of-the-art}
To investigate the effect of visual-textual representations, we benchmark StrucTexTv2 with several state-of-the-art techniques on different downstream tasks. Since only small datasets (149 training documents on FUNSD) and to avoid exceeding GPU memory (a tremendous number of table cells on WTW), we only evaluate  on the FUNSD and WTW datasets.



\begin{table}[t]
\caption{Performance comparisons on the RVL-CDIP dataset. We report classification accuracy on the test set.  and  denote the text and image modality of input. The proposed StrucTexTv2 achieves a comparable accuracy to the state-of-the-art models with image-only input.}
\vspace{-1em}
\label{RVL-CDIP}
\begin{center}
\setlength{\tabcolsep}{8pt}{
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{l c c c}
\toprule
\textbf{Methods} & \textbf{Modalities} & \textbf{Accuracy} & \textbf{\#Param.} \\
\midrule
~\cite{devlin2018bert} &  & 89.81\% & 110M \\
~\cite{devlin2018bert} &  & 89.92\% & 340M \\
~\cite{li2021structurallm} &  & 96.08\% & 355M \\
\midrule
SelfDoc~\cite{li2021selfdoc} &  & 92.81\% & - \\
~\cite{xu2020layoutlm} &  & 94.43\% & 390M \\
UDoc~\cite{nips2021udoc} &  & 95.05\% & 272M \\
~\cite{PowalskiBJDPP21} &  & 95.52\% & 780M \\
~\cite{xu2020layoutlmv2} &  & 95.64\% & 426M \\
~\cite{huang2022layoutlmv3} &  & 95.93\% & 368M \\
~\cite{appalaraju2021docformer} &  & \textbf{96.17\%} & 183M \\
\midrule
~\cite{bao2021beit} &  & 91.09\% & 87M \\
~\cite{li2022dit} &  & 92.11\% & 87M \\
~\cite{li2022dit} &  & 92.69\% & 304M \\
 &  & 93.40\% & \textbf{28M} \\
 &  & 94.62\% & 238M \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-1em}
\end{table}

\noindent\textbf{RVL-CDIP} As the Tab.~\ref{RVL-CDIP} shows, OCR-based approaches such as DocFormer~\cite{appalaraju2021docformer} with multi-modal modeling takes superior performance on RVL-CDIP. Compared with image-only methods, it relies on stand-alone OCR systems which are low efficiency for practical applications. Our  achieves an accuracy of 93.40\% and outperforms previous image-only methods with a much smaller model size. Besides,  drives an accuracy of 94.62\%, which brings a 1.93\% improvement over ~\cite{li2022dit}. Intuitively, the results show that StrucTexTv2 is effective for semantic understanding on document image classification.

\noindent\textbf{PubLayNet} The experiment results on PubLayNet are presented in Tab.~\ref{PubLayNet}. 
It is observed that StrucTexTv2 achieves new state-of-the-art performance of 95.4\% and 95.5\% mAP scores for both small and large settings.  beats even ~\cite{huang2022layoutlmv3} (the result of  is not released in the paper) which contains multi-modal inputs by 0.3\%. We suggest that our dual-modal pre-training tasks can learn rich visual-textual representations of document images and performs excellently in confusing situations. Notably, 
gets 0.1\% mAP gain over that on . We attribute this subtle improvement to the over-fitting to a certain extent with increasing model size.

\noindent\textbf{WTW} Tab.\ref{WTW} shows quantitative results of table structure recognition on the WTW dataset. StrucTexTv2 achieves the physical 78.9\% F1-score among all published methods. We reconstruct the table structure based on the detection results of table cells. The superior performance of StrucTexTv2 largely due to the proposed pre-training framework.

\begin{table}[t]
\begin{minipage}[t]{0.4\linewidth}
\caption{Performance comparisons on the PubLayNet validation set. The mAP @ IOU [0.50:0.95] is used as the metric.}
\label{PubLayNet}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c}
\toprule
\textbf{Methods} & \textbf{mAP} \\
\midrule
 & 93.1\% \\
UDoc & 93.9\% \\
 & 94.9\% \\
 & 95.1\% \\
 & 95.4\%  \\
 & \textbf{95.5\%}  \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}[t]{0.57\linewidth}
\caption[0.9\linewidth]{Performance comparisons on the WTW dataset. The F1-Score is used to measure the accuracy of cell coordinate when IOU=0.9.}
\label{WTW}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c}
\toprule
\textbf{Methods} & \textbf{F1-Score} \\
\midrule
Split+Heuristic~\cite{tensmeyer2019deep} & 3.4\% \\
Faster-RCNN~\cite{ren2015faster}  & 64.4\% \\
TridenNet~\cite{li2019scale} & 65.0\% \\
CenterNet~\cite{duan2019centernet} & 73.1\% \\
Cycle-CenterNet~\cite{long2021parsing}  & 78.3\%  \\
 & \textbf{78.9\%} \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\vspace{-0.5em}
\end{table}

\noindent\textbf{FUNSD} We evaluate  on both the document OCR and end-to-end information extraction tasks. As shown in Tab.~\ref{FUNSD},  achieves outstanding performance, 84.1\% 1-NED for document OCR and 55.0\% 1-NED for information extraction. Significantly, the whole network is end-to-end trainable. Compared to StrucTexT~\cite{li2021structext} and LayoutLMv3 with need of separately stage-wise training strategies, our model alleviates the error propagation in a documental system with key information parsing.

\begin{table}[ht]
\caption{Performance comparisons on FUNSD. We present the Normalized Edit Distance (1-NED) for the word-level document OCR and the entity-level information extraction. The \textbf{*} denotes a multi-stage process in which the methods are applied using our OCR results and entity boxes for word grouping in information extraction.}
\label{FUNSD}
\begin{minipage}[c]{0.53\linewidth}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c}
\multicolumn{2}{c}{The results of Document OCR} \\
\toprule
\textbf{Methods} & \textbf{1-NED} \\
\midrule
DB+NRTR~\cite{sheng2019nrtr} & 73.8\% \\
Google Vision~\cite{goolevision2019} & 76.4\% \\
 & \textbf{84.1\%} \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\begin{minipage}[c]{0.45\linewidth}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c} 
\multicolumn{2}{c}{The results of Information Extraction} \\
\toprule
\textbf{Methods} & \textbf{1-NED} \\
\midrule
 & 46.8\% \\
 & 53.1\% \\
 & \textbf{55.0\%} \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\vspace{-0.5em}
\end{table}

\subsection{Ablation Study}
To further examine the different contributions of StrucTexTv2, we conduct several ablation experiments, such as document layout analysis on PubLayNet, document image classification on RVL-CDIP, and end-to-end information extraction on FUNSD. All models in ablation study are pre-trained for 1 epoch with only 1M documents sampled from the IIT-CDIP dataset .

\begin{table}[ht]
\caption{The ablation study on pre-training tasks and different encoding structures.}
\label{table:Ablation_BackboneArchitectures_Tasks}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l l l c c c c}
\toprule
\textbf{Model} & \textbf{\#Param.} & \textbf{MIM} & \textbf{MLM} & \textbf{RVL-CDIP} & \textbf{PubLayNet} \\
\midrule
 & 176M & \checkmark & \checkmark   & 86.6\%  & 92.2\% \\
   & 116M & \checkmark & \checkmark   & 88.6\%  & 93.2\% \\

     & 238M & \checkmark & \checkmark & \textbf{94.1\%}  & \textbf{95.6\%} \\
\midrule
 & 28M & \checkmark &      & 91.8\%  & 94.1\% \\
 & 28M & & \checkmark  & 92.0\%  & 94.5\% \\
 & 28M & \checkmark & \checkmark  & \textbf{92.5\%}  & \textbf{94.9\%} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}

\noindent\textbf{Encoding Structures} In this study, we evaluate the impact of encoding structures by replacing the backbone of StrucTexTv2 with ViT~\cite{DBLP:journals/corr/abs-2010-11929} and SwinTransformer~\cite{liu2021swin}. As shown in Tab.~\ref{table:Ablation_BackboneArchitectures_Tasks}, the proposed network of  even achieves the better results of 92.5\% accuracy and 94.9\% mAP on RVL-CDIP and PubLayNet, respectively. The performance of two benchmarks dropped by 3.9\% accuracy and 1.7\% mAP with the . Replaced with the , the degradation is more obvious. In addition,  improves performance by 1.6\% on RVL-CDIP and 0.7\% on PubLayNet.

\noindent\textbf{Pre-training Tasks} In this study, we identify the contributions of different pre-training tasks. As shown in the bottom of Tab.~\ref{table:Ablation_BackboneArchitectures_Tasks}, the MIM-only pre-trained model achieves an accuracy of 91.8\% on RVL-CDIP and an mAP of 94.1\% on PubLayNet. The MLM-only pre-trained model achieves 92.0\% and 94.5\% for the two datasets. The MLM and MIM can jointly exploit the multi-modal feature representations in StrucTexTv2. By combining both the proposed pre-training tasks, the accuracy is improved to 92.5\% on RVL-CDIP and the mAP achieves 94.9\% on PubLayNet.

\begin{table}[ht]
\begin{minipage}[t]{0.35\linewidth}
\caption{The ablation study on the influence of masking ratios (MR.) with StrucTexTv2 on RVL-CDIP and PubLayNet.}
\label{table:Ablation_MaskingRates}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c c}
\toprule
\textbf{MR.}  & \textbf{RVL-CDIP} & \textbf{PubLayNet} \\
\midrule
0.15 & 92.1\%  & 94.7\% \\
\textbf{0.30} & \textbf{92.5\%}  & \textbf{94.9\%} \\
0.45 & 91.7\%  & 94.8\% \\
0.60 & 92.4\%  & 94.8\% \\
\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}[t]{0.62\linewidth}
\caption{Consumption analysis on RVL-CDIP. We re-implement  with open-source OCR engines to provide text. {} denotes the cost of OCR process. All the models are inferred on a NVIDIA Tesla 80G A100.}
\label{table:Ablation_Consumption}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l l c c}
\toprule
\textbf{Methods} & \textbf{OCR} & \textbf{GPU(MB)} & \textbf{Time(ms)}\\
\midrule
 & Tesseract & 2,184+n/a\textsuperscript{\rm } & 22+1,105\textsuperscript{\rm } \\
 & PaddleOCR & 2,184+3,450\textsuperscript{\rm } & 22+252\textsuperscript{\rm } \\
 & None & 2,276 & 56 \\
 & None & 4,058 & 284 \\

\bottomrule
\end{tabular}}
\end{center}
\end{minipage}
\end{table}

\noindent\textbf{Masking Ratios} We investigate the effect of training with different masking ratios. As shown in Tab.\ref{table:Ablation_MaskingRates}, by replacing the masking ratio with 0.15, 0.30, 0.45 and 0.60, the accuracy of RVL-CDIP is 92.1\%, 92.5\%, 91.7\% and 92.4\%, respectively. We also report the results on PubLayNet, the mAP of PubLayNet is 94.7\%, 94.9\%, 94.8\%, and 94.8\%, respectively. It suggests that the best masking ratio of our pre-training tasks is 0.30. At the same time, It also suggests that the performance of downstream tasks is less sensitive to the selection of masking ratio.

\noindent\textbf{Consumption Analysis} As shown in Tab.~\ref{table:Ablation_Consumption},  consumes 56ms and 2,276MB GPU memory to infer one image on RVL-CDIP, while  spends more GPU memory or time with different OCR engines. It is observed that the OCR process of the two-stage method accounts for the vast majority of computation overhead. Thus, our OCR-free framework can achieve a better trade-off between performance and efficiency.

\noindent\textbf{Masking Strategies} The impact of adjusting text region-level masking to patch-level masking is evaluated in Tab.\ref{table:Ablation_MaskingStrategies}. The performance drops a 4.2\% accuracy score on RVL-CDIP and a 1.0\% mAP on PubLayNet, which demonstrates the effectiveness of the proposed Text-Region masking strategy.

\begin{table}[ht]
\caption{Comparison between performance of different masking strategies on RVL-CDIP and PubLayNet. The model is only pre-trained with the MIM task.}
\label{table:Ablation_MaskingStrategies}
\vspace{-1em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c c c }
\toprule
\textbf{Model} & \textbf{Masking strategy} & \textbf{RVL-CDIP} & \textbf{PubLayNet} \\
\midrule
 & patch-level & 87.4\% & 93.1\% \\
 & region-level & \textbf{91.8}\% & \textbf{94.1\%} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-1em}
\end{table}









\section{Conclusion}
This work successfully explores a novel pre-training framework named StrucTexTv2 to learn visual-textual representations for document image understanding with image-only input. By performing text region-based image masking, and then predicting both corresponding visual and textual content, the proposed encoder can benefit from large-scale document images efficiently. Extensive experiments on five document understanding tasks demonstrate the superiority of StrucTexTv2 over state-of-the-art methods, especially an improvement in both efficiency and effectiveness.

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}



\appendix
\clearpage
\section{Appendix}
\subsection{StrucTexTv2 for Form Understanding}
To more accurately verify the capability of StrucTexTv2 on semantic learning, we evaluate the performance of our pre-trained model on FUNSD for form understanding. We build a classification layer to predict four categories (question, answer, header, or other) for each semantic entity. As shown in Tab.~\ref{form_understanding}, the entity-level F1-score of  is 89.23\% and the entity-level F1-Score of  is 91.82\%, which achieve comparable performance with the latest state-of-the-art methods. StrucTexTv2 achieves great semantic learning while the ground-truth OCR is utilized on FUNSD. However, it needs to be explained that our method can present better end-to-end information extraction results than the two-stage methods with the actual OCR engine, which is more referential for practical applications.

\begin{table}[ht]
\caption{Experimental results and performance comparison of the form understanding task on FUNSD. The entity-level F1-Score is used to measure model accuracy.}
\label{form_understanding}
\vspace{-0.5em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l l c}
\toprule
\textbf{Methods} & \textbf{\#Params.} & \textbf{FUNSD} \\
\midrule
~\cite{appalaraju2021docformer} & 183M & 83.34\% \\
SelfDoc~\cite{li2021selfdoc} & - & 83.36\% \\
~\cite{xu2020layoutlmv2} & 426M & 84.20\% \\
~\cite{appalaraju2021docformer} & 536M & 84.55\% \\
~\cite{li2021structurallm} & 355M & 85.14\% \\
UDoc~\cite{nips2021udoc} & 272M	& 87.93\% \\
 & \textbf{28M} & 89.23\% \\
XDoc~\cite{chen2022xdoc} & 146M	& 89.40\% \\
~\cite{huang2022layoutlmv3} & 133M & 90.29\% \\
 & 238M & 91.82\% \\
~\cite{huang2022layoutlmv3} & 368M & \textbf{92.08\%} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}

\subsection{StrucTexTv2 in Chinese}
To further analyze StrucTexTv2 in the Chinese language, we pre-train  with extra collected Chinese document images and evaluate the model on XFUND-ZH~\cite{XuL0WLFZW22}. XFUND is a multi-lingual extended dataset of the FUNSD~\cite{jaume2019funsd} in 7 languages.  directly adopts the text recognition model from PaddleOCR since only 149 train images of XFUND-ZH can not fulfill the recognition branch training for the end-to-end information extraction task. Experiment results in Tab.~\ref{chinese_ie} show that StrucTexTv2 can benefit the task of document image understanding in Chinese.

\begin{table}[ht]
\caption{Experimental results on XFUND-ZH. We re-implement  with PaddleOCR to provide OCR results. The Normalized Edit Distance (1-NED) is used to evaluate for the end-to-end information extraction task.}
\label{chinese_ie}
\vspace{-0.5em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l l c}
\toprule
\textbf{Methods} & \textbf{OCR} & \textbf{1-NED} \\
\midrule
~\cite{abs-2104-08836} & PaddleOCR & 60.40\% \\
 & Text Recognition Only & 67.46\% \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}


\subsection{Ablation Study for Image Resolution}
In this ablation, we investigate the impact of training with different image resolutions. As shown in Tab.\ref{table:Ablation_ImageResolution}, by replacing the resolution of input images with , , and , the accuracy of RVL-CDIP is 89.5\%, 90.4\%, and 92.5\%, respectively. We also report the results on PubLayNet, the F1-Score of PubLayNet is 94.9\%, 92.1\%, and 85.9\%, respectively. It suggests that the best image resolution of our pre-training tasks is . The performance of image-only vision tasks is more sensitive to the selection of resolution of the input.

\begin{table}[ht]
\caption{Ablation study for Different Image Resolution.}
\label{table:Ablation_ImageResolution}
\vspace{-0.5em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{c c c c}
\toprule
\textbf{Image Resolution} & \textbf{Pre-train Tasks} & \textbf{RVL-CDIP} & \textbf{PubLayNet} \\
\midrule
224*224       & MLM+MIM     & 89.5\%  & 85.9\% \\
512*512       & MLM+MIM     & 90.4\%  & 92.1\% \\
960*960       & MLM+MIM     & 92.5\%  & 94.9\% \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}


\subsection{Ablation Study for OCR Selection}
In this ablation, we look at the impact of various thresholds used in OCR selection for pre-training. We choose a commercial OCR engine to get the OCR results of the pre-training data. Besides the coordinates and text content, we keep the confidence score for each OCR word. For our pre-training, only the high-confidence word with a score above 0.8 was retained. Then the masked words are selected within the preserved text. We conduct experiments to analyze the impacts of the quality of OCR results on our pre-trained model via performing different score thresholds. As shown in Tab.\ref{table:Ablation_Threshold}, the performance decreases from 55.0\% to 54.9\% and 54.8\% by replacing the score threshold from 0.8 to 0.6 and 0.4 on FUNSD, respectively. Similarly, on RVL-CDIP, only slight modifications in accuracy by replacing the score threshold from 0.8 to 0.6 and 0.4. The subtle performance changes in downstream tasks suggest that StrucTexTv2 is robust against the selection of word's thresholds.

\begin{table}[ht]
\caption{Ablation study for the impacts of different thresholds used in OCR selection. The model is pre-trained with only MIM task.}
\label{table:Ablation_Threshold}
\vspace{-0.5em}
\begin{center}
\setlength{\tabcolsep}{1mm}{
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l c c}
\toprule
\textbf{OCR} & \textbf{RVL-CDIP} & \textbf{FUNSD} \\
\midrule
threshold=0.8 & 93.4\% & 55.0\% \\
threshold=0.6 & 93.4\% & 54.9\% \\
threshold=0.4 & 93.3\% & 54.8\% \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.5em}
\end{table}


\begin{figure*}
 \centering
 \includegraphics[width=0.9\textwidth]{appendix_imgs/appendix_fig.pdf}
 \caption{The qualitative results of Mask Image Modeling. From left to right: the masked document image, document reconstruction without content information from MLM, and document reconstruction with content information from MLM.}
\label{fig:grid_4figs_1cap_4subcap}
\end{figure*}

\subsection{Qualitative Results of Mask Image Modeling}
The qualitative results of Mask Image Modeling, which are shown in Fig. ~\ref{fig:grid_4figs_1cap_4subcap}, express that StrucTexTv2 is capable of recovering the RGB value of the randomly selected masked text regions according to the surrounding context.

\end{document}