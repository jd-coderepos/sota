\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}




\usepackage[final]{neurips_2019}



\usepackage[T1]{fontenc}    \usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=black]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsmath,amssymb,amsfonts}       \usepackage{dsfont}         \usepackage{mathtools}
\usepackage{microtype}      \usepackage{pifont}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{adjustbox}
\usepackage{comment}
\usepackage{array}
\usepackage{todonotes}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{caption}
\usepackage{diagbox}
\usepackage[capitalize]{cleveref}

\graphicspath{{figures/}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmiccomment}[1]{\hfill// #1}

\newcommand{\Reals}{\mathds{R}}
\newcommand{\Domains}{\mathcal{D}}
\newcommand{\TrainDomains}{\Domains_\mathrm{tr}}
\newcommand{\TestDomains}{\Domains_\mathrm{te}}
\newcommand{\TaskLoss}{\mathcal{L}_\mathrm{task}}
\newcommand{\LabelLoss}{\mathcal{L}_\mathrm{global}}
\newcommand{\LabelLossPair}{\ell_\mathrm{global}}
\newcommand{\MetricLoss}{\mathcal{L}_\mathrm{local}}
\newcommand{\MetaLoss}{\mathcal{L}_\mathrm{meta}}
\newcommand{\KLDiv}{D_\mathrm{KL}}
\newcommand{\grad}[1][]{\nabla_{\!#1}}
\newcommand{\given}{\,|\,}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\expec}{\mathbb{E}}
\newcommand{\softmax}{\operatorname{softmax}}
\newcommand{\indicator}[1]{\mathbf{1}[#1]}
\def\*#1{\mathbf{#1}}
\def\SoftLabels_#1#2{\*s^{(#1)}_{#2}}

\newcommand{\changed}[1]{\textcolor{red}{#1}}

\setlength{\marginparwidth}{3cm}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         \newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   







\title{Domain Generalization via\\Model-Agnostic Learning of Semantic Features}



\author{
Qi Dou
\quad Daniel C. Castro
\quad Konstantinos Kamnitsas
\quad Ben Glocker \\
Biomedical Image Analysis Group, Imperial College London, UK \\
\texttt{\{qi.dou,dc315,kk2412,b.glocker\}@imperial.ac.uk}
}




\begin{document}

\maketitle

\begin{abstract}

Generalization capability to unseen domains is crucial for machine learning models when deploying to real-world conditions. We investigate the challenging problem of domain generalization, i.e., training a model on multi-domain source data such that it can directly generalize to target domains with unknown statistics. 
We adopt a model-agnostic learning paradigm with gradient-based meta-train and meta-test procedures to expose the optimization to domain shift.
Further, we introduce two complementary losses which explicitly regularize the semantic structure of the feature space. Globally, we align a derived soft confusion matrix to preserve general knowledge about inter-class relationships. Locally, we promote domain-independent class-specific cohesion and separation of sample features with a metric-learning component.
The effectiveness of our method is demonstrated with new state-of-the-art results on two common object recognition benchmarks. Our method also shows consistent improvement on a medical image segmentation task.



\end{abstract}

\section{Introduction}

Machine learning methods have achieved remarkable success, under the assumption that training and test data are sampled from the same distribution. In real-world applications, this assumption is often violated as conditions for data acquisition may change, and a trained system may fail to produce accurate predictions for unseen data with domain shift. To tackle this issue, domain adaptation algorithms normally learn to align source and target data in a domain-invariant discriminative feature space~\citep{kumar2010co,ganin2016domain,hoffman2018cycada,long2016unsupervised,luo2018taking,saenko2010adapting,saito2018maximum,tzeng2015simultaneous,tzeng2017adversarial}. These methods rely on access to a few labelled~\citep{kumar2010co,saenko2010adapting,tzeng2015simultaneous} or unlabelled~\citep{ganin2016domain,hoffman2018cycada,long2016unsupervised,luo2018taking,saito2018maximum,tzeng2017adversarial} data samples from the target distribution during training.

An arguably harder problem is domain generalization, which aims to train a model using multi-domain source data, such that it can directly generalize to new domains without need of retraining.
This setting is very different to domain adaptation as no information about the new domains is available, a scenario that is encountered in real-world applications.
In the field of healthcare, for example, medical images acquired at different sites can differ significantly in their data distribution, due to varying scanners, imaging protocols or patient cohorts.
At deployment, each new hospital can be regarded as a new domain but it is impractical to collect data each time to adapt a trained system. Learning a model which directly generalizes to new clinical sites would be of great practical value.

Domain generalization is an active research area with a number of approaches being proposed. As no \textit{a priori} knowledge of the target distribution is available, the key question is how to guide the model learning to capture information which is discriminative for the specific task but insensitive to changes of domain-specific statistics. For computer vision applications, the aim is to capture general semantic features for object recognition.
Previous work has demonstrated that this can be investigated through regularization of the feature space, e.g., by minimizing divergence between marginal distributions of data sources~\citep{muandet2013domain}, or joint consideration of the class conditional distributions~\citep{li2018deep}. \citet{li2018domain} use adversarial feature alignment via maximum mean discrepancy. 
Leveraging distance metrics of feature vectors is another method~\citep{hsu2017learning,motiian2017unified}.
Model-agnostic meta-learning~\citep{finn2017model} is a recent gradient-based method for fast adaptation of models to new conditions, e.g., a new task at few-shot learning.
Meta-learning has been introduced to address domain generalization~\citep{balaji2018metareg,li2018learning,li2019feature}, by adopting an episodic training paradigm, i.e., splitting the available source domains into meta-train and meta-test at each iteration, to simulate domain shift.
Promising performance has been demonstrated by deriving the loss from a task error~\citep{li2018learning}, a classifier regularizer~\citep{balaji2018metareg}, or a predictive feature-critic module~\citep{li2019feature}.


We introduce two complementary losses which explicitly regularize the semantic structure of the feature space via a model-agnostic episodic learning procedure.
Our optimization objective encourages the model to learn semantically consistent features across training domains that may generalize better to unseen domains.
Globally, we align a derived soft confusion matrix to preserve inter-class relationships. Locally, we use a metric-learning component to encourage domain-independent while class-specific cohesion and separation of sample features.
The effectiveness of our approach is demonstrated with new state-of-the-art performance on two common object recognition benchmarks. Our method also shows consistent improvement on a medical image segmentation task.
Code for our proposed method is available at: \url{https://github.com/biomedia-mira/masf}.


 
\section{Related Work}
\label{sec:related}
\textbf{Domain adaptation} is based on the central theme of bounding the target error by the source error plus a discrepancy
metric between the target and the source~\citep{ben2010theory}. This is practically performed by narrowing the domain shift between the target and source either in input space~\citep{hoffman2018cycada}, feature space~\citep{kumar2010co,ganin2016domain,long2016unsupervised,saenko2010adapting,tzeng2017adversarial}, or output space~\citep{luo2018taking,saito2018maximum,tsai2018learning}, generally using maximum mean discrepancy~\citep{gretton2012optimal,sejdinovic2013equivalence} or adversarial learning~\citep{goodfellow2014generative}.
The success of methods operating on feature representations motivates us to optimize the semantic feature space for domain generalization in this paper.



\textbf{Domain generalization} aims to generalize models to unseen domains without knowledge about the target distribution during training. Different methods have been proposed for learning generalizable and transferable representations.
A promising direction is to extract task-specific but domain-invariant features~\citep{ghifary2015domain,li2018domain,li2018deep,motiian2017unified,muandet2013domain}. 
\citet{muandet2013domain} propose a domain-invariant component analysis method with a kernel-based optimization algorithm to minimize the dissimilarity across domains.
\citet{ghifary2015domain} learn multi-task auto-encoders to extract invariant features which are robust to domain variations.
\citet{li2018deep} consider the conditional distribution of label space over input space, and minimize discrepancy of a joint distribution. \citet{motiian2017unified} use contrastive loss to guide samples from the same class being embedded nearby in latent space across data sources.
\citet{li2018domain} extend adversarial autoencoders by imposing maximum mean discrepancy measure to align multi-domain distributions.
Instead of harmonizing the feature space, others use low-rank parameterized CNNs~\citep{li2017deeper} or decompose network parameters to domain-specific/-invariant components~\citep{khosla2012undoing}.
Data augmentation strategies, such as gradient-based domain perturbation~\citep{shankar2018generalizing} or adversarially perturbed samples~\citep{volpi2018generalizing}
demonstrate effectiveness for model generalization.
A recent method with state-of-the-art performance is JiGen~\cite{carlucci2019domain}, which leverages self-supervised signals by solving jigsaw puzzles.



\textbf{Meta-learning} (a.k.a. learning to learn~\citep{schmidhuber1987evolutionary,Thrun:1998:LL:296635}) is a long standing topic exploring the training of a meta-learner that learns how to train particular models~\citep{finn2017model,li2016learning,nichol2018first,ravi2016optimization}. 
Recently, gradient-based meta-learning methods~\citep{finn2017model,nichol2018first} have been successfully applied to few-shot learning, with a procedure purely leveraging gradient descent.
The episodic training paradigm, originated from model-agnostic meta-learning (MAML)~\citep{finn2017model}, has been introduced to address domain generalization~\citep{balaji2018metareg,li2018learning,li2019episodic,li2019feature}.
Epi-FCR~\citep{li2019episodic} alternates domain-specific feature extractors and classifiers across domains via episodic training, but without using inner gradient descent update.
The method of MLDG~\citep{li2018learning} closely follows the update rule of MAML, 
back-propagating the gradients from an ordinary task loss on meta-test data.
A limitation is that using the task objective might be sub-optimal, as it is highly abstracted from the feature representations (only using class probabilities). Moreover, it may not well fit the scenario where target data are unavailable (as pointed out by~\citet{balaji2018metareg}).
A recent method, MetaReg~\citep{balaji2018metareg}, learns a regularization function (e.g., weighted $L_1$ loss) particularly for the network's classification layer, excluding the feature extractor. Instead, \citet{li2019feature} propose a feature-critic network which learns an auxiliary meta loss (producing a non-negative scalar) depending on output of the feature extractor. Both~\citep{balaji2018metareg} and~\citep{li2019feature} lack notable guidance from semantics of feature space, which may contain crucial domain-independent `general knowledge' for model generalization.
Our method is orthogonal to previous work,
proposing to enforce semantic features via
global class alignment and local sample clustering, 
with losses explicitly derived in an episodic learning procedure. 

 
\section{Method}


\begin{figure}[t]
    \centering
    \def\svgwidth{\textwidth}
\includegraphics[width=0.97\textwidth]{fig1_png_backup.png}
     \vspace{-1mm}
    \caption{An overview of the proposed model-agnostic learning of semantic features (MASF): (a)~episodic training under simulated domain shift, with gradient flows indicated; (b)~global alignment of class relationships; (c)~local sample clustering, towards cohesion and separation. $F_\psi$ and $T_\theta$ are the feature extractor and the task net, $F_{\psi'}$ and $T_{\theta'}$ are their updated versions by inner gradient descent on the task loss $\TaskLoss$, the $M_\phi$ is a metric embedding net, and $D_k$ denotes different source domains.}
    \label{fig:overview}
    \vspace{-2mm}
\end{figure}


In the following, we denote input and label spaces by $\mathcal{X}$ and $\mathcal{Y}$, the domains $\Domains=\{D_1,D_2,\dots,D_K\}$ are different distributions on the joint space $\mathcal{X} \times \mathcal{Y}$. Since domain generalization involves a common predictive task, the label space is shared by all domains.
In each domain, samples are drawn from a dataset $D_k=\{(\*x_n^{(k)},y_n^{(k)})\}_{n=1}^{N_k}$ where $N_k$ is the number of labeled data points in the $k$-th domain.
The domain generalization (DG) setting further assumes the existence of domain-invariant patterns in the inputs (e.g.\ \emph{semantic features}), which can be extracted to learn a label predictor that performs well across seen and unseen domains. Unlike domain adaptation, DG assumes no access to observations from or explicit knowledge about the target distribution.

In this work, we consider a classification model composed of a feature extractor, $F_\psi:\mathcal{X}\to\mathcal{Z}$, where $\mathcal{Z}$ is a feature space (typically much lower-dimensional than $\mathcal{X}$), and a task network, $T_\theta:\mathcal{Z}\to\Reals^C$, where $C$ is the number of classes in $\mathcal{Y}$. The final class predictions are given by $p(y \given \*x; \psi,\theta)=\hat{\*y} = \softmax(T_\theta(F_\psi(\*x)))$, where  $\softmax(\*a)=e^\*a/\sum_r e^{a_r}$.\footnote{For image segmentation, $F_\psi$ extracts \emph{feature maps} and the task network $T_\theta$ is applied pixel-wise.}
The parameters $(\psi,\theta)$ are optimized with respect to a task-specific loss $\TaskLoss$, e.g.\ cross-entropy: $\ell_\mathrm{task}(y,\hat{\*y})=-\sum_c \indicator{y=c}\log\hat{y}_c$.

Although the minimization of $\TaskLoss$ may produce highly discriminative features $\*z=F_\psi(\*x)$, and hence an excellent predictor for data from the training domains, nothing in this process prevents the model from overfitting to the source domains and suffering from degradation on unseen test domains. We therefore propose to optimize the feature space such that its semantic structure is insensitive to different training domains, and generalize better to new unseen domains. 
\Cref{fig:overview} gives an overview of our
\textbf{m}odel-\textbf{a}gnostic learning of \textbf{s}emantic \textbf{f}eatures (MASF),
which we will detail in this section.


\subsection{Model-Agnostic Learning with Episodic Training}


The key of our learning procedure is an episodic training scheme, originated from model-agnostic meta-learning~\citep{finn2017model}, to expose the model optimization to distribution mismatch.
In line with our goal of domain generalization, the model is trained on a sequence of simulated \emph{episodes} with domain shift. 
Specifically, at each iteration, the available domains $\Domains$ are randomly split into sets of meta-train $\TrainDomains$ and meta-test $\TestDomains$ domains.
The model is trained to semantically perform well on held-out $\TestDomains$ after being optimized with one or more steps of gradient descent with $\TrainDomains$ domains.
In our case, the feature extractor's and task network's parameters, $\psi$ and $\theta$, are first updated from the task-specific supervised loss $\TaskLoss$ (e.g.\ cross-entropy for classification), computed on meta-train: 
\begin{equation}
    (\psi',\theta') = (\psi,\theta) - \alpha \grad[\psi,\theta] \TaskLoss(\TrainDomains; \psi,\theta) \,,
\end{equation}
where $\alpha$ is a learning-rate hyperparameter. This results in a predictive model $T_{\theta'} \circ F_{\psi'}$ with improved task accuracy on the meta-train source domains, $\TrainDomains$.

Once this optimized set of parameters has been obtained, we can apply a meta-learning step, aiming to enforce certain properties that we desire the model to exhibit on held-out domain $\TestDomains$. Crucially, the objective function quantifying these properties, $\MetaLoss$, is computed based on the updated parameters, $(\psi',\theta')$, and the gradients are computed towards the original parameters, $(\psi,\theta)$. 
Intuitively, besides the task itself, the training procedure is learning how to generalize under domain shift.
In other words, parameters are updated such that future updates with given source domains also improve the model regarding some generalizable aspects on unseen target domains.

In particular, we desire the feature space to encode \emph{semantically relevant} properties: features from different domains should respect inter-class relationships, and they should be compactly clustered by class labels regardless of domains (cf. Alg.~\ref{alg}). In the remainder of this section we describe the design of our semantic meta-objective, $\MetaLoss = \beta_1 \LabelLoss + \beta_2 \MetricLoss$, composed of a \emph{global} class alignment term and a \emph{local} sample clustering term, with weighting coefficients $\beta_1, \beta_2 > 0$.



\begin{algorithm}[t]
\caption{Model-agnostic learning of semantic features for domain generalization}
\begin{algorithmic}[1]
    \REQUIRE Source training domains $\Domains=\{D_k\}_{k=1}^K$; hyperparameters $\alpha, \eta, \gamma, \beta_1, \beta_2 > 0$
    \ENSURE Feature extractor $F_\psi$, task network $T_\theta$, embedding network $M_\phi$
    \REPEAT
        \STATE Randomly split source domains $\Domains$ into disjoint meta-train $\TrainDomains$ and meta-test $\TestDomains$
        \STATE $(\psi',\theta') \gets (\psi,\theta) - \alpha \grad[\psi,\theta] \TaskLoss(\TrainDomains; \psi,\theta)$
\STATE Compute global class alignment loss: \\
         $\LabelLoss \gets \frac{1}{|\TrainDomains|}\sum_{D_i\in\TrainDomains} \frac{1}{|\TestDomains|}\sum_{D_j\in\TestDomains} \LabelLossPair(D_i,D_j;\psi',\theta')$
        \COMMENT{\cref{sec:global_loss}}
\STATE Compute local sample clustering loss: \\
            $\MetricLoss(\Domains; \psi',\phi) \gets
            \expec_\Domains[\ell_\mathrm{con}^{n,m}] \;\;\text{or}\;\;
            \expec_\Domains[\ell_\mathrm{tri}^{a,p,n}]$
        \COMMENT{\cref{sec:local_loss}}
        \STATE $\MetaLoss \gets \beta_1 \LabelLoss + \beta_2 \MetricLoss$
        \STATE $(\psi,\theta) \gets (\psi,\theta) - \eta \grad[\psi,\theta] (\TaskLoss + \MetaLoss)$
        \STATE $\phi \gets \phi - \gamma \grad[\phi] \MetricLoss$
    \UNTIL{convergence}
\end{algorithmic}
\label{alg}
\end{algorithm}


\subsection{Global Class Alignment Objective}\label{sec:global_loss}

Relationships between class concepts exist in purely semantic space, independent of changes in the observation domain. In light of this, compared with individual hard label prediction,
aligning class relationships can promote more transferable knowledge towards model generalization. This is also noted by \citet{tzeng2015simultaneous} in the context of domain adaptation, by aggregating the output probability distribution when fine-tuning the model on a few labelled target data. In contrast to their work, our goal is to structure the feature space itself to preserve learned class relationships on unseen data, by means of explicit regularization.


Specifically, we formulate this objective in a manner that imposes a \emph{global} layout of extracted features, such that the relative locations of features from different classes embody the inherent similarity in semantic structures. Inspired by knowledge distillation from neural networks~\citep{hinton2014distilling}, we exploit what the model has learned about class ambiguities---in the form of per-class soft labels---and enforce them to be consistent between $\TrainDomains$ and $\TestDomains$ domains.
For each domain $k$, we summarize the model's current `concept' of each class $c$ by computing the class-specific mean feature vectors $\bar{\*z}^{(k)}_c$:
\begin{equation}
    \bar{\*z}^{(k)}_c = \frac{1}{N_k^{(c)}} \sum_{n: y^{(k)}_n=c} F_{\psi'}(\*x^{(k)}_n)
        \approx \expec_{D_k}[F_{\psi'}(\*x) \given y=c] \,,
\end{equation}
where $N_k^{(c)}$ is the number of samples in domain $\mathcal{D}_k$ labelled as class $c$. 
The obtained $\bar{\*z}^{(k)}_c$ conveys how samples from a particular class are generally represented. It is then forwarded to the task network $T_{\theta'}$, for computing soft label distributions $\SoftLabels_kc$ with a `softened' softmax at temperature $\tau>1$ \citep{hinton2014distilling}:
\begin{equation}\label{eq:soft_labels}
    \SoftLabels_kc = \softmax(T_{\theta'}(\bar{\*z}^{(k)}_c) / \tau) \,.
\end{equation}
The collection of soft labels $[\SoftLabels_kc]_{c=1}^C$ represents a kind of `soft confusion matrix' associated with a particular domain, encoding the inter-class relationships learned by the model.
Such relationships should be preserved as general semantics on meta-test after updating the classification model on meta-train (e.g., cartoon dogs are more easily misclassified as horses than as houses, which likely holds in unseen domains).
Standard supervised training with $\TaskLoss$ focuses only on the dominant hard label prediction, there is no reason \textit{a priori} for consistency of such inter-class alignment.
We therefore propose to align the soft class confusion matrix between two domains $D_i \in \TrainDomains$ and $D_j \in \TestDomains$, by minimising their symmetrized Kullback--Leibler (KL) divergence, averaged over all $C$ classes:
\begin{equation}\label{eq:label_loss}
    \LabelLossPair(D_i, D_j; \psi', \theta') = \frac{1}{C} \sum_{c=1}^C \frac{1}{2} [\KLDiv(\SoftLabels_ic \,\|\, \SoftLabels_jc) + \KLDiv(\SoftLabels_jc \,\|\, \SoftLabels_ic)] \,,
\end{equation}
where $\KLDiv(\*p\,\|\,\*q) = \sum_r p_r \log\frac{p_r}{q_r}$. Other symmetric divergences such as Jensen--Shannon (JS) could also be considered, although our preliminary experiments showed no significant difference with JS over symm.\ KL. Finally, the global class alignment loss, $\LabelLoss(\TrainDomains,\TestDomains; \psi',\theta')$, is calculated as the average of $\LabelLossPair(D_i,D_j; \psi',\theta')$ over all pairs of available meta-train and meta-test domains, $(D_i,D_j) \in \TrainDomains\times\TestDomains$ (cf. Alg.~\ref{alg}).
The complexity of this computation is not problematic in practice, since the number of domains selected in a training mini-batch is limited (as with the form in MAML \citep{finn2017model}), and in our experiments we took $|\TrainDomains|=2$ and $|\TestDomains|=1$.



\subsection{Local Sample Clustering Objective}\label{sec:local_loss}

\newcommand{\Margin}{\xi}

In addition to promoting the alignment of class relationships across domains with $\LabelLoss$ as defined above, we further encourage robust semantic features that \emph{locally} cluster according to class regardless of the domain. This is crucial, as neither of the class-prediction-based losses ($\TaskLoss$ or $\LabelLoss$) ensure that features of samples in the same class will lie close to each other and away from those of different classes, a.k.a. feature compactness~\cite{kamnitsas2018semi}.
If the model cannot project the inputs to the semantic feature clusters with domain-independent class-specific cohesion and separation, the predictions may suffer from ambiguous decision boundaries, and still be sensitive to unseen kinds of domain shift. We therefore propose a local regularization objective $\MetricLoss$ to boost robustness, by increasing the compactness of class-specific clusters while reducing their overlap. Note how this is complementary to the global class alignment of semantically structuring the relative locations among class clusters.


Our preliminary experiments revealed that applying such regularization explicitly onto the features may constrain the optimization for $\TaskLoss$ and $\LabelLoss$ too heavily, hurting generalization performance on unseen domain. We thus take a \emph{metric-learning} approach, introducing an embedding network $M_\phi$ that operates on the extracted features, $\*z=F_{\psi'}(\*x)$. This component represents a learnable distance function~\citep{chopra2005learning} between feature vectors (rather than between raw inputs):
\begin{equation}
    d_\phi(\*z_n, \*z_m) = \norm{\*e_n - \*e_m}_2
        = \norm{M_\phi(\*z_n) - M_\phi(\*z_m)}_2 \,.
\end{equation}
The sample pairs $(n,m)$ are randomly drawn from all source domains $\Domains$, because we expect the updated $F_{\psi'}$ will harmonize the semantic feature space of $\TestDomains$ with that of $\TrainDomains$, in terms of class-specific clustering regardless of domains.
The computed embeddings, $\*e=M_\phi(\*z)$, can then be optimized with any suitable metric-learning loss $\MetricLoss(\Domains; \psi', \phi)$ to regularize the local sample clustering.
Under mild domain shift, the contrastive loss~\citep{hadsell2006dimensionality} is a sensible choice, as it attempts to separately collapse each group of same-class exemplars to a distinct single point. It might however be over-restrictive for more extreme situations, wherein domains are related rather semantically, but with wildly distinct low-level statistics. For such cases, we propose instead to employ the triplet loss~\citep{schroff2015facenet}.

\vspace{1mm}

\textbf{Contrastive loss}
is computed for pairs of samples, attracting samples of the same class and repelling samples of different classes~\citep{hadsell2006dimensionality}. Instead of pushing clusters apart to infinity, the repulsion range is bounded by a distance margin $\Margin$. 

Our contrastive loss for a pair of samples $(n,m)$ is defined as:
\begin{equation}
\ell_\mathrm{con}^{n,m} = \begin{cases}
        d_\phi(\*z_n,\*z_m)^2\,,                         & \text{if } y_n=y_m \\
        (\max\{0,\:\Margin-d_\phi(\*z_n,\*z_m)\})^2\,,   & \text{if } y_n\neq y_m
    \end{cases} \,.
\end{equation}

The total loss for a training mini-batch, $\MetricLoss$, is normally averaged over all pairs of samples. In cases where full $O(N^2)$ enumeration is intractable---e.g.\ image segmentation, which would involve all pairs of pixels in all images---we can obtain an unbiased $O(N)$ estimator of the loss by e.g.\ shuffling the samples and iterating over $(2i-1,2i)$ pairs with $i=1,\dots,\lfloor N/2\rfloor$.
\changed{}



\vspace{1mm}

\textbf{Triplet loss} aims to make pairs of samples from the same class closer than pairs from different classes, by a certain margin $\Margin$~\citep{schroff2015facenet}. Given one `anchor' sample $a$, one `positive' sample $p$ (with $y_a=y_p$), and one `negative' sample $n$ (with $y_a\neq y_n$), we compute their triplet loss as follows:
\begin{equation}
    \ell_\mathrm{tri}^{a,p,n} = \max\{0,\: d_\phi(\*z_a, \*z_p)^2 - d_\phi(\*z_a, \*z_n)^2 + \Margin\} \,.
\end{equation}

\Citet{schroff2015facenet} argue that judicious triplet selection is essential for good convergence, as many triplets may already satisfy this constraint and others may be too hard to contribute meaningfully to the learning process. Here we adopt their proposed online `semi-hard' triplet mining strategy, and $\MetricLoss$ is the average over all selected triplet pairs.
 
\section{Experiments}

We evaluate and compare our method on three datasets: 1) the classic VLCS domain generalization benchmark for image classification, 2) the recently introduced PACS benchmark for object recognition with challenging domain shift, 3) a real-world medical imaging task of tissue segmentation in brain MRI. Results with an in-depth analysis and ablation study are presented in the following.

\subsection{VLCS Dataset}

VLCS~\citep{fang2013unbiased} is a classic benchmark for domain generalization, which includes images from four datasets: PASCAL VOC2007 (V)~\citep{everingham2010pascal}, LabelMe (L)~\citep{russell2008labelme}, Caltech (C)~\citep{fei2007learning}, and SUN09 (S)~\citep{choi2010exploiting}. The multi-class object recognition task includes five classes: bird, car, chair, dog and person.
We follow previous work~\citep{carlucci2019domain,li2019episodic,motiian2017unified} of using the publicly available pre-extracted DeCAF$_{6}$ features (4096-dimensional vector) for leave-one-domain-out validation with randomly dividing each domain into $70\%$ training and $30\%$ test, inputting to two fully connected layers with output size of 1024 and 128 with ReLU activation.
For our metric embedding $M_\phi$ (inputting the 128-dimensional vector), we use two fully connected layers with output size of 128 and 64. The triplet loss is adopted for computing $\MetricLoss$, with coefficient $\beta_2=0.005$, such that it is in a similar scale to $\TaskLoss$ and $\LabelLoss$ ($\beta_1=1$).
We use the Adam optimizer~\citep{kingma2015adam} with $\eta$ initialized to $10^{-3}$ and exponentially decayed by $2\%$ every $1k$ iterations.
For the inner optimization to obtain $(\psi',\theta')$, we clip the gradients by norm (threshold by $2.0$) to prevent them from exploding, since this step uses plain, non-adaptive gradient descent (with learning rate $\alpha=10^{-5}$).
Note that, although performing gradient descent on $\MetaLoss$ involves second-order gradients on $(\psi,\theta)$, their computation does not incur a substantial overhead in training time \citep{finn2017model}.
We also employ an Adam optimizer for the meta-updates of $\phi$ with learning rate $\gamma=10^{-5}$ without decay.
The batch size is $128$ for each source domain, with an Nvidia TITAN Xp 12 GB GPU.
The metric-learning margin hyperparameter $\xi$ was chosen heuristically based on observing the distances within and between the clusters of class features. For our results, we report the average and standard deviation over three independent runs.

\textbf{Results.} Table~\ref{tab:vlcs} shows the object recognition accuracies on different target domains. Our DeepAll baseline---i.e., merging all source domains and training $F_\psi \! \circ \! T_\theta$ by standard supervised learning on $\TaskLoss$ with the same hyperparameters---achieves an average accuracy of $72.19\%$ over four domains. Using our episodic training paradigm with regularizations on semantic feature space, we improve the performance to $74.11\%$, setting the
state-of-the-art accuracy on VLCS.
We compare with eight different methods (cf.~\cref{sec:related}) which report previous best results on this benchmark.
CCSA~\citep{motiian2017unified} combines contrastive loss together with ordinary cross-entropy without using episodic meta-update paradigm.
Notably, our approach outperforms MLDG~\cite{li2018learning}, indicating that explicitly encouraging semantic properties in the feature space is superior to using a highly-abstracted task loss on meta-test.



\vspace{-2mm}
\begin{table}[h]
    \centering
    \caption{Domain generalization results on VLCS dataset with object recognition accuracy (\%).}
    \setlength{\tabcolsep}{3pt}
    \begin{adjustbox}{scale=.80}
    \begin{tabular}{cc|cccccccc|cc}
    \toprule
    \multirow{2}{*}{Source}  & \multirow{2}{*}{Target}  & D-MTAE  & CIDDG & CCSA & DBADG  & MMD-AAE & MLDG  & Epi-FCR  & JiGen & DeepAll  & MASF \\
     & & \citep{ghifary2015domain} & \citep{li2018deep}  & \citep{motiian2017unified} & \citep{li2017deeper} & \citep{li2018domain} & \citep{li2018learning}   & \citep{li2019episodic}  & \citep{carlucci2019domain} & (Baseline) & (Ours) \\
    \hline
          {L},{C},{S}    &  V      & 63.90  & 64.38 & 67.10 & 69.99  & 67.70 & 67.7   & 67.1   & 70.62  & 68.67$\pm$0.09  & ~69.14$\pm$0.19  \Tstrut \\
          {V},{C},{S}    &  L      & 60.13  & 63.06 & 62.10 & 63.49  & 62.60 & 61.3   & 64.3   & 60.90  & 63.10$\pm$0.11  & 64.90$\pm$0.08   \\
          {V},{L},{S}    &  C      & 89.05  & 88.83 & 92.30 & 93.63  & 94.40 & 94.4   & 94.1   & 96.93  & 92.86$\pm$0.13  & 94.78$\pm$0.16   \\
          {V},{L},{C}    &  S      & 61.33  & 62.10 & 59.10 & 61.32  & 64.40 & 65.9   & 65.9   & 64.30  & 64.11$\pm$0.17  & 67.64$\pm$0.12   \\
    \hline
\multicolumn{2}{c|}{Average}       & 68.60  & 69.59 & 70.15 & 72.11   & 72.28  & 72.3   & 72.9   & 73.19  & 72.19  & 74.11  \Tstrut \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:vlcs}
\end{table}
\vspace{-2mm}


\subsection{PACS Dataset}
\label{sec:exp_pacs}

The PACS dataset~\citep{li2017deeper} is a recent benchmark with more severe distribution shift between domains, making it more challenging than VLCS. It consists of four domains: art painting, cartoon, photo, sketch, with objects from seven classes: dog, elephant, giraffe, guitar, house, horse, person.
Following practice in the literature~\citep{balaji2018metareg,carlucci2019domain,li2018learning,li2019episodic}, we also use leave-one-domain-out cross-validation, i.e., training on three domains and testing on the remaining unseen one, and adopt an AlexNet~\citep{krizhevsky2012imagenet} pre-trained on ImageNet~\citep{ILSVRC15}. The metric embedding $M_\phi$ is connected to the last fully connected layer (i.e., fc7 layer with a 4096-dimesional vector), by stacking two fully connected layers with output size of 1024 and 256.
For the $\MetricLoss$, we also use the triplet loss with $\beta_2=0.005, \beta_1=1.0$, particularly considering the severe domain shift.
We initialize learning rates $\alpha=\eta=\gamma=10^{-5}$ and clip inner gradients by norm. The batch size is 128 for each source domain.

\begin{table}[t]
\centering
    \caption{Domain generalization results on PACS dataset with recognition accuracy (\%) using AlexNet.}
    \setlength{\tabcolsep}{3pt}
    \begin{adjustbox}{scale=.85}
    \begin{tabular}{cl|ccccccc|cc}
    \toprule
   \multirow{2}{*}{Source} & \multirow{ 2}{*}{Target}  & D-MTAE   & CIDDG   & DBADG & MLDG & Epi-FCR & MetaReg  & JiGen & DeepAll & MASF \\
    &  & \citep{ghifary2015domain} & \citep{li2018deep} & \cite{li2017deeper} & \citep{li2018learning}  & \cite{li2019episodic} & \citep{balaji2018metareg}  & \cite{carlucci2019domain} & (Baseline) & (Ours) \\
    \hline
{C},{P},{S}  & Art painting  & 60.27  & 62.70  & 62.86  & 66.23    & 64.7  & 69.82  & 67.63  & 67.60$\pm$0.21  & 70.35$\pm$0.33  \Tstrut  \\
{A},{P},{S}  & Cartoon       & 58.65  & 69.73  & 66.97  & 66.88    & 72.3  & 70.35  & 71.71  & 68.87$\pm$0.22  & 72.46$\pm$0.19   \\
{A},{C},{S}  & Photo         & 91.12  & 78.65  & 89.50  & 88.00    & 86.1  & 91.07  & 89.00  & 89.20$\pm$0.24  & 90.68$\pm$0.12    \\
{A},{C},{P}  & Sketch        & 47.68  & 64.45  & 57.51  & 58.96    & 65.0  & 59.26  & 65.18  & 61.13$\pm$0.30  & 67.33$\pm$0.12   \\
    \hline
\multicolumn{2}{c|}{Average} & 64.48  & 68.88  & 69.21  & 70.01   & 72.0   & 72.62  & 73.38  & 71.70  & 75.21 \Tstrut   \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:pacs}
    \vspace{5mm}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{embedding.pdf}
\caption{The t-SNE visualization of extracted features from $F_\psi$, using our proposed (a-b) MASF and the (c-d) DeepAll model on PACS dataset. In (a) and (c), the different colors indicate different classes; correspondingly in (b) and (d), the different colors indicate different domains.}
    \label{fig:tsne}
\end{figure}

\textbf{Results.} \cref{tab:pacs} summarizes the results of object recognition on PACS dataset with a comparison to previous work (noting that not all compared methods reported results on both VLCS and PACS). MLDG~\citep{li2018learning} and MetaReg~\citep{balaji2018metareg} employ episodic training with meta-learning, but from different angles in terms of the meta learner's objective (\citet{li2018learning} minimize task error, \citet{balaji2018metareg} learn a classifier regularizer).
The promising results for~\citep{balaji2018metareg,li2018learning,li2019episodic} indicate that exposing the training procedure to domain shift benefits model generalization to unseen domains. Our method further explicitly considers the semantic structure, regarding both global class alignment and local sample clustering, yielding improved accuracy. Across all domains, our method increases average accuracy by $3.51\%$ over the baseline.
Note that current state-of-the-art JiGen~\citep{carlucci2019domain} improves $1.86\%$ over its own baseline.
In addition, we observe an improvement of $6.20\%$ when the unseen domain is \textit{sketch}, which has a distinct style and requires more general knowledge about semantic concepts.

\textbf{Ablation analysis.} We conduct an extensive study using PACS benchmark to investigate two key points: 1) the contribution of each component to our method's performance, 2) how the semantic feature space is influenced by our proposed meta losses.
First, we test all possible combinations of including the key components: episodic meta-learning simulating domain shift, global class alignment loss and local sample clustering loss. 
Accuracies averaged over three runs when using different combinations are given in \cref{tab:pacs-ablation}. For example, first row corresponds to the DeepAll baseline with standard training by aggregating all source data. The fifth row is directly adding the $\LabelLoss, \MetricLoss$ losses on top of $\TaskLoss$ with standard optimization scheme, i.e., without splitting $\Domains$ to meta-train and meta-test domains. From the ablation study, we observe that each component plays its own role in a complementary way. Specifically, the proposed losses that encourage semantic structure in feature space yield improvement over DeepAll, as well as over pure episodic training (the second row) that corresponds to our implementation of MLDG thus enabling straightforward comparison. By further leveraging the gradient-based update paradigm, performance is further improved across all settings.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{curves_new.pdf}
\caption{Analysis of the learning procedure: (a) margin of feature distance between sample negative pairs (with different classes) and positive pairs (with the same class), (b) class relationships alignment loss between unseen target domain and source domain, (c) Silhouette plot of the embeddings from meta metric-learning. Detailed analysis is in \cref{sec:exp_pacs} for (a-b) and \cref{sec:exp_brain} for (c).}
    \label{fig:curves}
\end{figure}

\begin{table}[th]
\centering
    \caption{Ablation study on key components of our method with the PACS dataset (accuracy, \%).}
    \label{tab:pacs-ablation}
    \begin{adjustbox}{scale=.85}
    \begin{tabular}{ccc|cccc|c}
    \toprule
      Episodic   & $\LabelLoss$  & $\MetricLoss$ & Art            & Cartoon         & Photo            & Sketch           & Average \Bstrut \\
    \hline
       -         &  -          &    -        & 67.60$\pm$0.21 & 68.87$\pm$0.22  & 89.20$\pm$0.24   & 61.13$\pm$0.30   & 71.70  \Tstrut  \\
    \hline
      \checkmark &  -          &   -         & 69.19$\pm$0.10 & 70.66$\pm$0.37  & 90.36$\pm$0.18   & 59.89$\pm$0.26   & ~72.52   \Tstrut  \\
       -         &  \checkmark &    -        & 69.43$\pm$0.29 & 70.22$\pm$0.21  & 90.64$\pm$0.15   & 60.11$\pm$0.17   & 72.60    \\
       -         &      -      & \checkmark  & 69.50$\pm$0.15 & 70.25$\pm$0.13  & 90.12$\pm$0.12   & 63.02$\pm$0.12   & 73.22  \\
       -         &  \checkmark & \checkmark  & 69.48$\pm$0.20 & 71.15$\pm$0.16  & 90.16$\pm$0.15   & 64.73$\pm$0.34   & 73.88    \\ 
      \checkmark &  \checkmark & -           & 69.94$\pm$0.15 & 72.16$\pm$0.28  & 90.10$\pm$0.12   & 63.54$\pm$0.13   & 73.93    \\  
      \checkmark &   -         & \checkmark  & 69.50$\pm$0.20 & 71.44$\pm$0.34  & 90.16$\pm$0.15   & 64.97$\pm$0.28   & 74.02    \\ 
    \hline
      \checkmark &  \checkmark & \checkmark  & 70.35$\pm$0.33 & 72.46$\pm$0.19  & 90.68$\pm$0.12   & 67.33$\pm$0.12   & ~75.21  \Tstrut  \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\begin{table}[h!]
    \centering
    \let\oldpm\pm
    \renewcommand{\pm}{\,\oldpm\,}
    \caption{PACS results with deep residual network architectures (accuracy, \%).}
    \label{tab:resnet_jigen}
    \begin{adjustbox}{scale=.85}
    \begin{tabular}{llcccc}
        \toprule
   \multirow{2}{*}{Source}  &  \multirow{2}{*}{Target}  &  \multicolumn{2}{c}{ResNet-18} & \multicolumn{2}{c}{ResNet-50} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
       &   & DeepAll & MASF (ours)      & DeepAll & MASF (ours) \\
\midrule
{C},{P},{S}       &  Art-painting        & 77.38$\pm$0.15      & 80.29$\pm$0.18    & 81.41$\pm$0.16    & 82.89$\pm$0.16 \\
{A},{P},{S}       &  Cartoon             & 75.65$\pm$0.11      & 77.17$\pm$0.08    & 78.61$\pm$0.17    & 80.49$\pm$0.21 \\
{A},{C},{S}       &  Photo               & 94.25$\pm$0.09      & 94.99$\pm$0.09    & 94.83$\pm$0.06    & 95.01$\pm$0.10 \\
{A},{C},{P}       &  Sketch              & 69.64$\pm$0.25      & 71.69$\pm$0.22    & 69.69$\pm$0.11    & 72.29$\pm$0.15 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

We utilize t-SNE~\citep{maaten2008visualizing} to analzye the feature space learned with our proposed model and the DeepAll baseline (cf.~\cref{fig:tsne}). It appears that our MASF model yields a better separation of classes. We also note that the \emph{sketch} domain is further apart from \emph{art painting} and \emph{cartoon}, although all three are source domains in this experiment, possibly explained by the unique characteristics of sketches. In \Cref{fig:curves} (a), we plot the difference of feature distances between samples of negative pairs and positive pairs, i.e., $\expec[\norm{\*z_a- \*z_n}_2 - \norm{\*z_a - \*z_p}_2]$. 
For the two magenta lines, respectively for MASF and DeepAll, sample pairs are drawn from different training source domains.
We see that both distance margins naturally increase as training progresses. The shaded area highlights that MASF yeilds a higher distance margin between classes compared to DeepAll, indicating that sample clusters are better separated with MASF.
Similarly, for the two blue lines, sample pairs are from the unseen target domain and a source domain (randomly selected at each iteration). As expected, the margin is not as large as between training domains, yet our method still presents a notably bigger margin than the baseline. In \Cref{fig:curves} (b), we plot $\LabelLossPair$ quantifying differences of average class posteriors between unseen target domain and a source domain during training. We observe that the semantic inter-class relationships, conveying general knowledge about a recognition task, would not naturally converge and generalize to the unseen domain without explicit guidance.


\textbf{Deeper architectures.}
In the interest of providing stronger baseline results, we perform additional preliminary experiments using more up-to-date deep residual architectures~\cite{he2016resnet} with ResNet-18 and ResNet-50.
\cref{tab:resnet_jigen} shows strong and consistent improvements of MASF over the DeepAll baseline in all PACS splits for both network architectures. This suggests our proposed algorithm is also beneficial for domain generalization with deeper feature extractors.



\subsection{Tissue Segmentation in Multi-site Brain MRI}
\label{sec:exp_brain}

We evaluate our method on a real-world medical imaging task of brain tissue segmentation in T1-weighted MRI. Data was acquired from four clinical centers (denoted as Set-A/B/C/D). Domain shift occurs due to differences in scanners, acquisition protocols and many other factors, posing severe limitations for translating learning-based methods to clinical practice~\cite{glocker2019multisite}. \Cref{fig:brain-hist} shows example images and intensity histograms.
We adapt MASF for the segmentation of four classes: background, grey matter (GM), white matter (WM), cerebrospinal fluid (CSF). We employ a U-Net~\citep{ronneberger2015u}, commonly used for this task. 
For $\LabelLoss$, the $\bar{\*z}^{(k)}_c$ is computed by averaging over all pixels of a class.
Our metric-embedding has two layers of $1\!\times\!1$ convolutions, with contrastive loss for $\MetricLoss$.
We randomly split each domain to $80\%$ for training and $20\%$ for testing in experimental settings.



\textbf{Results.} For easier comparison, we average the evaluated Dice scores achieved for the three foreground classes (GM/WM/CSF) and report it in \cref{tab:brain-dice}.
Although hard to notice visually from the gray-scale images, the domain shift from data distribution degrades segmentation significantly by up to $10\%$. DeepAll is a strong baseline, yet our model-agnostic learning scheme provides consistent improvement over naively aggregating data from multiple sources, especially when generalizing to a new clinical site with relatively poorer imaging quality (i.e., Set-D).
\Cref{fig:curves} (c) is the Silhouette plot~\citep{rousseeuw1987silhouettes} of the embeddings from $M_\phi$, demonstrating that the samples within the same class cluster are tightly grouped, as well as clearly separated from those of other classes.



\begin{table}[t]
	\begin{minipage}[t][][c]{0.49\linewidth}
		\centering
		\includegraphics[width=.9\textwidth]{brain.pdf}
		\captionof{figure}{Different brain MRI datasets with example images and intensity histograms.}
		\label{fig:brain-hist}
	\end{minipage}  \hfill
	\begin{minipage}[t][][c]{0.49\linewidth}
		\centering
		\caption{Evaluation of brain tissue segmentation (Dice coefficient, \%) in different settings: \emph{columns 1--4:} train model on single source domain, test on all domains; \emph{columns 5--6:} train on three source domains, test on remaining domain.}
		\label{tab:brain-dice}
        \setlength{\tabcolsep}{3pt}
		\begin{adjustbox}{width=\textwidth}
		\begin{tabular}{l|cccc|cc}
        \toprule
        \diagbox[width=4em,trim=l]{Test}{Train}   & Set-A & Set-B & Set-C & Set-D  & DeepAll  & MASF \\
        \hline
                    Set-A     & 90.62  & 88.91  & 88.81  & 85.03  & 89.09  & 89.82  \Tstrut \\
                    Set-B     & 85.03  & 94.22  & 81.38  & 88.31  & 90.41  & 91.71 \\
                    Set-C     & 93.14  & 92.80  & 95.40  & 88.68  & 94.30  & 94.50 \\
                    Set-D     & 76.32  & 88.39  & 73.50  & 94.29  & 88.62  & 89.51 \\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
	\end{minipage}
	\vspace{-5mm}
\end{table}
 
\vspace{-2mm}
\section{Conclusions}
\vspace{-2mm}
We have presented promising results for a new approach to domain generalization of predictive models by incorporating global and local constraints for learning semantic feature spaces. The better generalization capability is demonstrated by new state-of-the-art results on popular benchmarks and a dense classification task (i.e., semantic segmentation) for medical images.
The proposed loss functions are generally orthogonal to other algorithms, and evaluating the benefit of their integration is an appealing future direction.
Our learning procedure could also be interesting to explore in the context of generative models, which may greatly benefit from semantic guidance when learning low-dimensional data representations from multiple sources.
 
\vspace{-2mm}
\section*{Acknowledgements}
\vspace{-2mm}
This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant No 757173, project MIRA, ERC-2017-STG) and is supported by an EPSRC Impact Acceleration Award (EP/R511547/1). DCC is also partly supported by CAPES, Ministry of Education, Brazil (BEX 1500/2015-05).

\vspace{-2mm}
\bibliographystyle{plainnat}
\bibliography{ref_clean.bib}

\end{document}
