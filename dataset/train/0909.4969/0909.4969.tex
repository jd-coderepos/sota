In this section we briefly present the background behind tensors and low rank approximations. 
Table~\ref{tab:symbol} shows the symbols and the abbreviations we use and their explanation.

\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|l|} \hline
Symbol & Definition and Description \\ \hline \hline
 & number of modes \\ \hline
 & dimensionality of \\ 
      &  -th mode \\ \hline 
 & -mode  \\ 
         & tensor (calligraphic) \\ \hline
 & tensor obtained upon  \\ 
                    & applying MACH on       \\ \hline
 & matrices (upper case)  \\ \hline
 & scalars (lower case)  \\ \hline
   & matricization of  \\
            & along mode  \\ \hline
 &   rank approximation  \\ 
            &  of the matricizations \\
			&  \\ \hline
 & mode- product  \\  \hline
HOOI & Higher Order Orthogonal \\ 
     & Iteration \cite{354405}  \\  \hline
HOSVD & Higher Order Singular \\
      & Value Decomposition \cite{citeulike:4308452}  \\ \hline
\end{tabular}
\caption{Symbols}
\label{tab:symbol}
\end{center}
\end{table}

\subsection{Tensors}

\paragraph{Historical Remarks}  Tensors traditionally have been used in physics (e.g., stress and strain tensors). 
After Einstein presented the theory of general relativity tensor analysis became popular.
Certain ideas on multi-way analysis data back in 1944 and 1952 and are due to Raymond Cattell \cite{cat1,cat2}.
Tucker introduced tensor analysis in psychometrics \cite{tucker3} (Tucker family). 
Harshman \cite{harshman} and Carrol and Chang \cite{carroll} independently
proposed the canonical decomposition of a tensor (CANDECOMP family). 
These two families of decompositions come with different names, see \cite{tamarasurvey}.
The difference between them is visualized for a three way tensor in figure~\ref{fig:fig2}.
In the following we will focus on Tucker decompositions. 

\begin{figure}[h]
\begin{tabular}{c}
\psfig{figure=Slide3.eps,width=0.45\textwidth} \\
\psfig{figure=parafacfinal.eps,width=0.45\textwidth} 
\end{tabular}
\label{fig:fig2}
\caption{CANDECOMP/PARAFAC and Tucker tensor decompositions.  }
\end{figure}


\paragraph{Tensor Concepts} 


Let  be a multiway array.
We will call  a tensor, i.e., we will use the terms multiway array and tensor
interchangeably. The order of a tensor is the number of dimensions, also known as ways, modes
or aspects and is equal to  for tensor . The dimensionality of the -th mode is
equal to . 

The norm of tensor  is defined to be the square root of the sum of all entries of the tensor
squared, i.e.,

As we see the norm of a tensor is the straight-forward generalization of the Frobenius norm of a matrix
(2 modes) to  modes. 


The inner product of two tensors with the same number of modes and equal dimensionality per mode,
, is defined by the following
equation:

Observe that  equation~\ref{eq:norm} can equivalently be written as 
A tensor fiber (slice) is a one (two)-dimensional fragment of a tensor, obtained by fixing all indices but one (two).
For more details on tensor fibers and slices, see \cite{tamarasurvey}. 

Matricization along mode , results in a  matrix. The  element
is mapped to  where  where . 
Figure~\ref{fig:fig3} shows the concept of matricization for a three-way tensor. 
The operation of matricization naturally introduces the concept of a  vector containing ranks :  is equal
to the rank of the , the matrix resulting by the matricization of the tensor  along the -th mode.

\begin{figure}[h]
\begin{tabular}{c}
\psfig{figure=Slide1.eps,width=0.45\textwidth} \\
\psfig{figure=Slide2.eps,width=0.45\textwidth} 
\end{tabular}
\label{fig:fig3}
\caption{Matricization of a three-way , , tensor along the first mode. The three 
slices are denoted with different color.}
\end{figure}


The -mode product of  with a matrix  is denoted by 
and is a tensor of size .
Specifically,

Some important facts concerning -mode products, is the following: 



The importance of this equation lies in the fact that the order of execution
of the tensor matrix products does not play any role, as long as the multiplications
are along different modes. 
When we multiply a tensor and two matrices along the same mode the following equation holds:



Furthermore, if  then the following equation holds: 


The rank  of the -way tensor  is the minimum number of -linear components
to fit  exactly, i.e.,: 


where  are the  components for the -th mode and 
 denotes the tensor product. 
Even if the above generalization is a straightforward generalization of the rank of a matrix,
the concept of the tensor rank is special.  For example,  
for a matrix  the column rank  and the row rank 
are equal  to the matrix rank . Furthermore, . 
However for a tensor  the rank can be 2 or 3 \cite{120567}.
Therefore the word rank can have different meanings:
a) The individual rank, i.e., for a specific instance of a tensor what is ?
b) The typical rank is the rank that we almost surely observe. For example for 
tensors the typical rank is . c) Vector of ranks . The value of 
is equal to the rank of the matricized version  of the tensor. 

Consider figure~\ref{fig:fig2}, which depicts a three mode tensor . 
The PARAFAC/CANDECOMP model is given by equation~\ref{eq:parafac}, whereas the Tucker model is given by equation~\ref{eq:tucker}.





Few brief remarks on the above two models: 
a) In terms of the fit, the Tucker family is at least as good as the PARAFAC/CANDECOMP
since as we see from the above equations, the PARAFAC model can be viewed as a restrictive Tucker model, where the core tensor  
is superdiagonal, i.e.,  only if . 
However, it is worth noting that better fit is not necessarily optimal (see \cite{brobook}, Ch.7)
b) The Tucker model does not result in unique solutions since it has rotational freedom. 
Typically one chooses a solution that satisfies a certain criterion, as the all-orthogonality
core tensor:  when  (\cite{citeulike:4308452}). 
c) Basic concepts as the uniqueness of the canonical tensor decomposition, degeneracy of the rank, border rank are not discussed. A good reference is \cite{tamarasurvey}
and the related references therein. 

In the following we focus on the Tucker family.
Compressing  out of the  modes of a tensor results in a Tucker- decomposition (\cite{kiers2000}). 
For example, for a three mode tensor we can have the Tucker1, Tucker2 and Tucker3 decomposition. 
In the following we discuss algorithms for the Tucker3 decomposition and briefly state some facts about Tucker2 and Tucker1
decompositions. Generalization to  modes is straightforward. 

\paragraph{Tucker3 Algorithms} 

The algorithm which should be used to compute the Tucker3 decomposition of a tensor 
depends on whether or not the data is noise free. In the former case, an exact, closed
form solution exists, whereas in the latter case the alternating least squares algorithm (ALS)
is frequently used. However, it is worth noting that even in cases where there is noise in the data, 
the closed form solution a.k.a. as HOSVD \cite{tamarasurvey,citeulike:4308452} is satisfactory in practice \cite{luo-2009}. 

Let  and  the vector containing 
the desired approximation ranks along each mode. 
In the case of noise-free data, the algorithm matricizes the tensor along each mode
and computes the  top left singular vectors . Let  be the  matrix
containing in its columns those vectors. The core tensor is computed with the following equation:



In the case of noise in the data, one performs the alternating least squares algorithm. 
To solve the nonlinear optimization problem that tries to optimize the fit of the low rank approximation
with respect to the original tensor, one converts the problem into a linear one, by ``fixing'' all modes
but one and optimizing along that mode. This method is also known as Higher Order Orthogonal Iteration (HOOI). 
This procedure is continued until some stopping criterion is met, i.e.,  improvement in terms of fit.


\paragraph{Further Remarks}
H{\aa}stad proved that the tensor rank is an NP-complete problem \cite{DBLP:journals/jal/Hastad90}.
Lek-Heng Lim has proposed a theory for eigenvalues, eigenvectors, singular values  and singular vectors
\cite{DBLP:journals/corr/abs-math-0607648}. Maximum constraint satisfaction problems  (MAX-rCSP)
have been casted as a tensor decomposition problem (sum of rank one components).
In \cite{1060701} is proved that there is a PTAS (polynomial time approximation scheme) for a family 
of MAX-rCSP (i.e., core-dense). Sheehan and Saad in \cite{saad} give a unified view of different
dimensionality reduction techniques under the tensor framework.
A wealth of applications that use tensor decompositions exist, \cite{tamarasurvey} contains a wealth of such references.




\subsection{SVD and Fast Low Rank Approximation} 

Any matrix   can be written as a sum of rank one matrices, i.e.,
,
where  (left singular vectors) and  (right singular vectors) 
are orthonormal and the singular values are ordered in decreasing order
. Here  is the rank of . 
We denote with  the -rank approximation of , i.e., . 
Among all matrices  of rank at most , 
 is the one that minimizes  (\cite{Horn:1985:MA}).
Since the computational cost of the SVD is high,  for the full SVD 
approximation algorithms that give a close to the optimal solution  have been developed. 
Frieze, Kannan and Vempala showed in a breakthrough paper \cite{1039494} that an approximate SVD 
can be computed by a randomly chosen submatrix of . It is remarkable that the complexity
does not depend at all on . Their Monte-Carlo algorithm with probability at least  
outputs a matrix  of rank at most  
that satisfies the following equation: 

Drineas et al. in \cite{petros} showed how to find such a low rank approximation in  time. 
A lot of work has followed on this problem. Here, we present the results of Achlioptas-McSherry \cite{Achlioptas01fastcomputation} 
which are used in our work\footnote{We call our proposed method MACH, to acknowledge the fact that 
it is based on the \textbf{Ach}lioptas-\textbf{M}cSherry work. }. The main theorem that is of interest to us is theorem~\ref{thm:ach}.



\begin{theorem}[Achlioptas-McSherry \cite{Achlioptas01fastcomputation}]
Let  be any  matrix where  and let .
For .
Let  be a random  matrix whose entries are independently distributed, with 
 with probability  and 0 with probability . 
Then with probability at least 1-exp, the matrix  satisfies the following two equations:

 



\label{thm:ach}
\end{theorem}


\paragraph{Randomized Tensor Algorithms}

As already discussed, the most computationally expensive step for the Tucker decomposition 
is the SVD part. To alleviate this cost, two randomized algorithms which select columns according to a biased probability 
distribution for tensor decompositions \cite{Drineas05arandomized} have been proposed, extending the results of \cite{1109681}and \cite{Drineas04fastmonte}
to the multiway case and TensorCUR \cite{DBLP:conf/kdd/MahoneyMD06}, the extension of the CUR method \cite{md-cmdid-2009} in -modes.
Roughly speaking, the bounds proved are of the form~\ref{eq:frieze}.
Another approach to approximating the Tucker decomposition for the case of a three-way tensor is presented in \cite{1461977}.
The proposed method matricizes the tensor as in all aforementioned algorithms and employes appropriately the 
matrix approximation described in \cite{goreinov}.
