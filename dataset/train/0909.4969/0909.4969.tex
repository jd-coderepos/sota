In this section we briefly present the background behind tensors and low rank approximations. 
Table~\ref{tab:symbol} shows the symbols and the abbreviations we use and their explanation.

\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|l|} \hline
Symbol & Definition and Description \\ \hline \hline
$d$ & number of modes \\ \hline
$I_j$ & dimensionality of \\ 
      &  $j$-th mode \\ \hline 
${\cal X,Y},\ldots \in \field{R}^{I_1 \times \ldots \times I_d}$ & $d$-mode  \\ 
         & tensor (calligraphic) \\ \hline
$ {\cal \hat{X}}$ & tensor obtained upon  \\ 
                    & applying MACH on ${\cal X}$      \\ \hline
$A,U,\ldots \in \field{R}^{m\times n}$ & matrices (upper case)  \\ \hline
$\alpha,\beta,a_{i,j},x_{i_1,\ldots,i_d}$ & scalars (lower case)  \\ \hline
$X_{(i)},\hat{X}_{(i)}$   & matricization of ${\cal X},{\cal \hat{X}}$ \\
            & along mode $i$ \\ \hline
$X_{(i),r_i}, \hat{X}_{(i),r_i}$ &  $r_i$ rank approximation  \\ 
            &  of the matricizations \\
			& $X_{(i)},\hat{X}_{(i)}$ \\ \hline
$\times_n$ & mode-$n$ product  \\  \hline
HOOI & Higher Order Orthogonal \\ 
     & Iteration \cite{354405}  \\  \hline
HOSVD & Higher Order Singular \\
      & Value Decomposition \cite{citeulike:4308452}  \\ \hline
\end{tabular}
\caption{Symbols}
\label{tab:symbol}
\end{center}
\end{table}

\subsection{Tensors}

\paragraph{Historical Remarks}  Tensors traditionally have been used in physics (e.g., stress and strain tensors). 
After Einstein presented the theory of general relativity tensor analysis became popular.
Certain ideas on multi-way analysis data back in 1944 and 1952 and are due to Raymond Cattell \cite{cat1,cat2}.
Tucker introduced tensor analysis in psychometrics \cite{tucker3} (Tucker family). 
Harshman \cite{harshman} and Carrol and Chang \cite{carroll} independently
proposed the canonical decomposition of a tensor (CANDECOMP family). 
These two families of decompositions come with different names, see \cite{tamarasurvey}.
The difference between them is visualized for a three way tensor in figure~\ref{fig:fig2}.
In the following we will focus on Tucker decompositions. 

\begin{figure}[h]
\begin{tabular}{c}
\psfig{figure=Slide3.eps,width=0.45\textwidth} \\
\psfig{figure=parafacfinal.eps,width=0.45\textwidth} 
\end{tabular}
\label{fig:fig2}
\caption{CANDECOMP/PARAFAC and Tucker tensor decompositions.  }
\end{figure}


\paragraph{Tensor Concepts} 


Let ${\cal X} \in \field{R}^{I_1 \times I_2 \times \ldots \times I_d}$ be a multiway array.
We will call ${\cal X}$ a tensor, i.e., we will use the terms multiway array and tensor
interchangeably. The order of a tensor is the number of dimensions, also known as ways, modes
or aspects and is equal to $d$ for tensor ${\cal X}$. The dimensionality of the $j$-th mode is
equal to $I_j$. 

The norm of tensor ${\cal X}$ is defined to be the square root of the sum of all entries of the tensor
squared, i.e.,
\begin{equation}
|| {\cal X}||= \sqrt{ \sum_{j_1=1}^{I_1} \sum_{j_2=1}^{I_2} \ldots \sum_{j_d=1}^{I_d} x_{j_1,\ldots,j_d}^2}
\label{eq:norm}
\end{equation}
As we see the norm of a tensor is the straight-forward generalization of the Frobenius norm of a matrix
(2 modes) to $N$ modes. 


The inner product of two tensors with the same number of modes and equal dimensionality per mode,
${\cal X},{\cal Y} \in \field{R}^{I_1 \times I_2 \times \ldots \times I_d}$, is defined by the following
equation:
\begin{equation}
\left\langle X,Y \right\rangle = \sum_{j_1=1}^{I_1} \sum_{j_2=1}^{I_2} \ldots \sum_{j_d=1}^{I_d} x_{j_1,\ldots,j_d} y_{j_1,\ldots,j_d}
\end{equation}
Observe that  equation~\ref{eq:norm} can equivalently be written as $|| {\cal X}||= \sqrt{\left\langle X,X \right\rangle}$
A tensor fiber (slice) is a one (two)-dimensional fragment of a tensor, obtained by fixing all indices but one (two).
For more details on tensor fibers and slices, see \cite{tamarasurvey}. 

Matricization along mode $k$, results in a $I_k  \times \prod_{j=1, j \neq k}^d I_j$ matrix. The $(i_1,\ldots,i_d)$ element
is mapped to $(i_k,j)$ where $j=1 + \sum_{q=1, q \neq k}^d (i_q-1) J_q$ where $J_q= \prod_{m=1, m \neq k}^{q-1} I_m$. 
Figure~\ref{fig:fig3} shows the concept of matricization for a three-way tensor. 
The operation of matricization naturally introduces the concept of a  vector containing ranks $(r_1,\ldots,r_d)$: $r_i$ is equal
to the rank of the $X_{(i)}$, the matrix resulting by the matricization of the tensor ${\cal X}$ along the $i$-th mode.

\begin{figure}[h]
\begin{tabular}{c}
\psfig{figure=Slide1.eps,width=0.45\textwidth} \\
\psfig{figure=Slide2.eps,width=0.45\textwidth} 
\end{tabular}
\label{fig:fig3}
\caption{Matricization of a three-way $I_1 \times I_2 \times I_3$, $I_3=3$, tensor along the first mode. The three 
slices are denoted with different color.}
\end{figure}


The $n$-mode product of ${\cal X}$ with a matrix $M \in \field{R}^{J \times I_n}$ is denoted by ${\cal X} \times_n M$
and is a tensor of size $I_1 \times I_2 \times \ldots I_{n-1} \times J \times I_{n+1} \times \ldots I_d$.
Specifically,
\begin{equation}
({\cal X} \times_n M )_{i_1 \ldots i_{n-1} j i_{n+1} \ldots i_d} = \sum_{i_n=1}^{I_n} x_{i_1 i_2 \ldots i_d} m_{ji_n}
\end{equation}
Some important facts concerning $n$-mode products, is the following: 

\begin{equation}
{\cal X} \times_m A \times_n B = {\cal X} \times_n B \times_m A, m\neq n
\end{equation}

The importance of this equation lies in the fact that the order of execution
of the tensor matrix products does not play any role, as long as the multiplications
are along different modes. 
When we multiply a tensor and two matrices along the same mode the following equation holds:

\begin{equation}
{\cal X} \times_m A \times_m B = {\cal X} \times_m (BA)
\label{eqb}
\end{equation}

Furthermore, if $UU^T=I$ then the following equation holds: 
\begin{equation}
|| A \times_n U ||= ||A||
\label{eq:eqa}
\end{equation}

The rank $R$ of the $d$-way tensor ${\cal X}$ is the minimum number of $d$-linear components
to fit ${\cal X}$ exactly, i.e.,: 
\begin{equation}
{\cal X} = \sum_{m=1}^R c_m^{(1)} \circ c_m^{(2)} \circ \ldots \circ c_m^{(d)} 
\end{equation}

where $c_1^{(j)},\ldots,c_R^{(j)}$ are the $R$ components for the $j$-th mode and 
$\circ$ denotes the tensor product. 
Even if the above generalization is a straightforward generalization of the rank of a matrix,
the concept of the tensor rank is special.  For example,  
for a matrix $A \in \field{R}^{2 \times 2}$ the column rank $R_c$ and the row rank $R_r$
are equal $R_c=R_r=r$ to the matrix rank $r$. Furthermore, $r \leq 2$. 
However for a tensor ${\cal X} \in \field{R}^{2 \times 2 \times 2}$ the rank can be 2 or 3 \cite{120567}.
Therefore the word rank can have different meanings:
a) The individual rank, i.e., for a specific instance of a tensor what is $R$?
b) The typical rank is the rank that we almost surely observe. For example for $2 \times 2 \times 2$
tensors the typical rank is $\{ 2,3\}$. c) Vector of ranks $(r_1,\ldots,r_d)$. The value of $r_i$
is equal to the rank of the matricized version $X_{(i)}$ of the tensor. 

Consider figure~\ref{fig:fig2}, which depicts a three mode tensor ${\cal X} \in \field{R}^{I_1 \times I_2 \times I_3}$. 
The PARAFAC/CANDECOMP model is given by equation~\ref{eq:parafac}, whereas the Tucker model is given by equation~\ref{eq:tucker}.

\begin{equation}
{\cal X}_{ijk} = \sum_{r=1}^R \alpha_{ir} b_{jr} c_{qr} \lambda_r + e_{ijk}
\label{eq:parafac}
\end{equation}

\begin{equation}
{\cal X}_{ijk} = \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R \alpha_{ip} b_{jq} c_{kr} g_{pqr} + e_{ijk}
\label{eq:tucker}
\end{equation}

Few brief remarks on the above two models: 
a) In terms of the fit, the Tucker family is at least as good as the PARAFAC/CANDECOMP
since as we see from the above equations, the PARAFAC model can be viewed as a restrictive Tucker model, where the core tensor ${\cal G}$ 
is superdiagonal, i.e., $g_{pqr} \neq 0$ only if $p=q=r$. 
However, it is worth noting that better fit is not necessarily optimal (see \cite{brobook}, Ch.7)
b) The Tucker model does not result in unique solutions since it has rotational freedom. 
Typically one chooses a solution that satisfies a certain criterion, as the all-orthogonality
core tensor: $\left\langle G(m,:,:),G(n,:,:) \right\rangle = \left\langle G(:,m,:),G(:,n,:) \right\rangle= \left\langle G(:,:,m),G(:,:,n) \right\rangle=0$ when $m\neq n$ (\cite{citeulike:4308452}). 
c) Basic concepts as the uniqueness of the canonical tensor decomposition, degeneracy of the rank, border rank are not discussed. A good reference is \cite{tamarasurvey}
and the related references therein. 

In the following we focus on the Tucker family.
Compressing $n$ out of the $d$ modes of a tensor results in a Tucker-$n$ decomposition (\cite{kiers2000}). 
For example, for a three mode tensor we can have the Tucker1, Tucker2 and Tucker3 decomposition. 
In the following we discuss algorithms for the Tucker3 decomposition and briefly state some facts about Tucker2 and Tucker1
decompositions. Generalization to $d$ modes is straightforward. 

\paragraph{Tucker3 Algorithms} 

The algorithm which should be used to compute the Tucker3 decomposition of a tensor 
depends on whether or not the data is noise free. In the former case, an exact, closed
form solution exists, whereas in the latter case the alternating least squares algorithm (ALS)
is frequently used. However, it is worth noting that even in cases where there is noise in the data, 
the closed form solution a.k.a. as HOSVD \cite{tamarasurvey,citeulike:4308452} is satisfactory in practice \cite{luo-2009}. 

Let ${\cal X} \in \field{R}^{I_1 \times I_2 \times I_3}$ and $(r_1,r_2,r_3)$ the vector containing 
the desired approximation ranks along each mode. 
In the case of noise-free data, the algorithm matricizes the tensor along each mode
and computes the $r_k$ top left singular vectors $k=1,2,3$. Let $A_k$ be the $I_k \times r_k$ matrix
containing in its columns those vectors. The core tensor is computed with the following equation:
\begin{equation}
{\cal G} = {\cal X} \times_1 A_1^T \times_2 A_2^T \times_3 A_3^T
\label{eq:eqcore}
\end{equation}


In the case of noise in the data, one performs the alternating least squares algorithm. 
To solve the nonlinear optimization problem that tries to optimize the fit of the low rank approximation
with respect to the original tensor, one converts the problem into a linear one, by ``fixing'' all modes
but one and optimizing along that mode. This method is also known as Higher Order Orthogonal Iteration (HOOI). 
This procedure is continued until some stopping criterion is met, i.e., $\epsilon$ improvement in terms of fit.


\paragraph{Further Remarks}
H{\aa}stad proved that the tensor rank is an NP-complete problem \cite{DBLP:journals/jal/Hastad90}.
Lek-Heng Lim has proposed a theory for eigenvalues, eigenvectors, singular values  and singular vectors
\cite{DBLP:journals/corr/abs-math-0607648}. Maximum constraint satisfaction problems  (MAX-rCSP)
have been casted as a tensor decomposition problem (sum of rank one components).
In \cite{1060701} is proved that there is a PTAS (polynomial time approximation scheme) for a family 
of MAX-rCSP (i.e., core-dense). Sheehan and Saad in \cite{saad} give a unified view of different
dimensionality reduction techniques under the tensor framework.
A wealth of applications that use tensor decompositions exist, \cite{tamarasurvey} contains a wealth of such references.




\subsection{SVD and Fast Low Rank Approximation} 

Any matrix  $A \in \field{R}^{m \times n}$ can be written as a sum of rank one matrices, i.e.,
$A = \sum_{i=1}^r \sigma_i u_i v_i^T$,
where $u_i, i = 1 \ldots r$ (left singular vectors) and $v_i, i = 1 \ldots r$ (right singular vectors) 
are orthonormal and the singular values are ordered in decreasing order
$\sigma_1 \geq \ldots \geq \sigma_r > 0$. Here $r$ is the rank of $A$. 
We denote with $A_k$ the $k$-rank approximation of $A$, i.e., $A_k = \sum_{i=1}^k \sigma_i u_i v_i^T$. 
Among all matrices $C \in \field{R}^{m \times n}$ of rank at most $k$, 
$A_k$ is the one that minimizes $||A-C||_F $ (\cite{Horn:1985:MA}).
Since the computational cost of the SVD is high, $O( \min{(m^2n, n^2m)})$ for the full SVD 
approximation algorithms that give a close to the optimal solution $A_k$ have been developed. 
Frieze, Kannan and Vempala showed in a breakthrough paper \cite{1039494} that an approximate SVD 
can be computed by a randomly chosen submatrix of $A$. It is remarkable that the complexity
does not depend at all on $m,n$. Their Monte-Carlo algorithm with probability at least $1-\delta$ 
outputs a matrix $\hat{A}$ of rank at most $k$ 
that satisfies the following equation: 
\begin{equation} 
||A-\hat{A}||_F^2 \leq ||A-A_k||_F^2 + \epsilon ||A||_F^2
\label{eq:frieze}
\end{equation}
Drineas et al. in \cite{petros} showed how to find such a low rank approximation in $O(mk^2)$ time. 
A lot of work has followed on this problem. Here, we present the results of Achlioptas-McSherry \cite{Achlioptas01fastcomputation} 
which are used in our work\footnote{We call our proposed method MACH, to acknowledge the fact that 
it is based on the \textbf{Ach}lioptas-\textbf{M}cSherry work. }. The main theorem that is of interest to us is theorem~\ref{thm:ach}.



\begin{theorem}[Achlioptas-McSherry \cite{Achlioptas01fastcomputation}]
Let $A$ be any $m\times n$ matrix where $ 76 \leq m \leq n$ and let $b= \max_{ij}|A_{ij}|$.
For $p \geq (8 \log{n})^4/n$.
Let $\hat{A}$ be a random $m \times n$ matrix whose entries are independently distributed, with 
$\hat{A}_{ij} = A_{ij}/p$ with probability $p$ and 0 with probability $1-p$. 
Then with probability at least 1-exp$(19(\log n)^4)$, the matrix $N=A-\hat{A}$ satisfies the following two equations:

\begin{equation} 
||N_k||_2 < 4b\sqrt{\frac{n}{p}} 
\label{eq:2normach} 
\end{equation} 

\begin{equation} 
||N_k||_F < 4b\sqrt{\frac{nk}{p}} 
\label{eq:eqfrob} 
\end{equation}

\label{thm:ach}
\end{theorem}


\paragraph{Randomized Tensor Algorithms}

As already discussed, the most computationally expensive step for the Tucker decomposition 
is the SVD part. To alleviate this cost, two randomized algorithms which select columns according to a biased probability 
distribution for tensor decompositions \cite{Drineas05arandomized} have been proposed, extending the results of \cite{1109681}and \cite{Drineas04fastmonte}
to the multiway case and TensorCUR \cite{DBLP:conf/kdd/MahoneyMD06}, the extension of the CUR method \cite{md-cmdid-2009} in $n$-modes.
Roughly speaking, the bounds proved are of the form~\ref{eq:frieze}.
Another approach to approximating the Tucker decomposition for the case of a three-way tensor is presented in \cite{1461977}.
The proposed method matricizes the tensor as in all aforementioned algorithms and employes appropriately the 
matrix approximation described in \cite{goreinov}.
