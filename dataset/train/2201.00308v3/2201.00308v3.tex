
\documentclass[10pt]{article} \usepackage[accepted]{tmlr}




\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
\newcommand{\remove}[1]{} 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{algorithm,algpseudocode}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}

\graphicspath{ {./images/} }


\title{DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents}



\author{\name Kushagra Pandey \email pandeyk1@uci.edu \\
      \addr Department of Computer Science\\
      University of California, Irvine
      \AND
      \name Avideep Mukherjee \email avideep@cse.iitk.ac.in \\
      \addr Department of Computer Science\\
      Indian Institute of Technology, Kanpur
      \AND
      \name Piyush Rai \email piyush@cse.iitk.ac.in\\
      \addr Department of Computer Science\\
      Indian Institute of Technology, Kanpur
      \AND
      \name Abhishek Kumar \email abhishk@google.com\\
      \addr Google Research, Brain Team}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{11}  \def\year{2022} \def\openreview{\url{https://openreview.net/forum?id=ygoNPRiLxw}} 


\begin{document}


\maketitle

\begin{abstract}
Diffusion probabilistic models have been shown to generate state-of-the-art results on several competitive image synthesis benchmarks but lack a low-dimensional, interpretable latent space, and are slow at generation. On the other hand, standard Variational Autoencoders (VAEs) typically have access to a low-dimensional latent space but exhibit poor sample quality. We present DiffuseVAE, a novel generative framework that integrates VAE within a diffusion model framework, and leverage this to design novel conditional parameterizations for diffusion models. We show that the resulting model equips diffusion models with a low-dimensional VAE inferred latent code which can be used for downstream tasks like controllable synthesis. The proposed method also improves upon the speed vs quality tradeoff exhibited in standard unconditional DDPM/DDIM models (for instance, \textbf{FID of 16.47 vs 34.36} using a standard DDIM on the CelebA-HQ-128 benchmark using \textbf{T=10} reverse process steps) without having explicitly trained for such an objective. Furthermore, the proposed model exhibits synthesis quality comparable to state-of-the-art models on standard image synthesis benchmarks like CIFAR-10 and CelebA-64 while outperforming most existing VAE-based methods. Lastly, we show that the proposed method exhibits inherent generalization to different types of noise in the conditioning signal. For reproducibility, our source code is publicly available at \url{https://github.com/kpandey008/DiffuseVAE}.
\end{abstract}



\section{Introduction}
\label{sec:intro}

Generative modeling is the task of capturing the underlying data distribution and learning to generate novel samples from a posited explicit/implicit distribution of the data in an unsupervised manner. Variational Autoencoders (VAEs) \citep{kingma2014autoencoding, rezende2016variational} are a type of explicit-likelihood based generative models which are often also used to learn a low-dimensional latent representation for the data. The resulting framework is very flexible and can be used for downstream applications, such as learning disentangled representations \citep{Higgins2017betaVAELB, chen2019isolating, burgess2018understanding}, semi-supervised learning \citep{kingma2014semisupervised}, anomaly detection \citep{pol2020anomaly}, among others. However, in image synthesis applications, VAE generated samples (or reconstructions) are usually blurry and fail to incorporate high-frequency information \citep{dosovitskiy2016generating}. Despite recent advances \citep{oord2018neural, razavi2019generating, vahdat2021nvae, child2021deep, xiao2021vaebm} in improving VAE sample quality, most VAE-based methods require large latent code hierarchies. Even then, there is still a significant gap in sample quality between VAEs and their implicit-likelihood counterparts like GANs \citep{goodfellow2014generative, karras2018progressive, karras2019stylebased, karras2020analyzing}.

\par In contrast, Diffusion Probabilistic Models (DDPM) \citep{sohldickstein2015deep, ho2020denoising} have been shown to achieve impressive performance on several image synthesis benchmarks, even surpassing GANs on several such benchmarks \citep{dhariwal2021diffusion, ho2021cascaded}. However, conventional diffusion models require an expensive iterative sampling procedure and lack a low-dimensional latent representation, limiting these models' practical applicability for downstream applications.

\par
We present DiffuseVAE, a novel framework which combines the best of both VAEs and DDPMs in an attempt to alleviate the aforementioned issues with both types of model families. We present a novel two-stage conditioning framework where, in the first stage, any arbitrary conditioning signal () can be first modeled using a standard VAE. In the second stage, we can then model the training data () using a DDPM conditioned on  and the low-dimensional VAE latent code representation of . With some simplifying design choices, our framework reduces to a \textit{generator-refiner} framework which involves fitting a VAE on the training data () itself in the first stage followed by modeling  in the second stage using a DDPM conditioned on the VAE reconstructions () of the training data,. The main contributions of our work can be summarized as follows:

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{images/diffusevae_tmlr-main.pdf}
    \caption{DiffuseVAE generated samples on the CelebA-HQ-256 (Left), CelebA-HQ-128 (Middle), CIFAR-10 (Right, Top) and CelebA-64 (Right, Bottom) datasets using just \textbf{25}, \textbf{10}, \textbf{25} and \textbf{25} time-steps in the reverse process for the respective datasets. The generation is entirely driven by low dimensional latents -- the diffusion process latents are fixed and shared between samples after the model is trained (See Section \ref{subsec:exp_controllable} for more details).}
    \label{fig:main}
\end{figure*}

\begin{enumerate}
    \item \textbf{A novel conditioning framework}: We propose a generic DiffuseVAE conditioning framework and show that our framework can be reduced to a simple \textit{generator-refiner} framework in which blurry samples generated from a VAE are \textit{refined} using a conditional DDPM formulation (See Fig.\ref{fig:architecture}). This effectively equips the diffusion process with a low dimensional latent space. As a part of our conditioning framework, we explore two types of conditioning formulations in the second stage DDPM model.
    
    \item \textbf{Controllable synthesis from a low-dimensional latent}: We show that, as part of our model design, major structure in the DiffuseVAE generated samples can be controlled directly using the low-dimensional VAE latent space while the diffusion process noise controls minor stochastic details in the final generated samples.
    
    \item \textbf{Better speed vs quality tradeoff}: We show that DiffuseVAE inherently provides a better speed vs quality tradeoff as compared to a standard DDPM model on several image benchmarks. Moreover, combined with DDIM sampling \citep{song2021denoising}, the proposed model can generate plausible samples in as less as 10 reverse process sampling steps (For example, the proposed method achieves an FID \citep{heusel2018gans} of 16.47 as compared to 34.36 by the corresponding DDIM model at T=10 steps on the CelebA-HQ-128 benchmark \citep{karras2018progressive}).
    
    \item \textbf{State of the art comparisons}: We show that DiffuseVAE exhibits synthesis quality comparable to recent state-of-the-art on standard image synthesis benchmarks like CIFAR-10 \citep{krizhevsky2009learning}, CelebA-64 \citep{liu2015faceattributes}) and CelebA-HQ \citep{karras2018progressive} while maintaining access to a low-dimensional latent code representation.


    \item \textbf{Generalization to different noises in the conditioning signal}: We show that a pre-trained DiffuseVAE model exhibits generalization to different noise types in the DDPM conditioning signal exhibiting the effectiveness of our conditioning framework.
\end{enumerate}




\section{Background}
\label{sec:background}

\subsection{Variational Autoencoders}

\label{subsec:bg_vae}
VAEs \citep{kingma2014autoencoding, rezende2016variational} are based on a simple but principled encoder-decoder based formulation. Given data  with a latent representation , learning the VAE is done by maximizing the evidence lower bound (ELBO) on the data log-likelihood,  (which is intractable to compute in general). The VAE optimization objective can be stated as follows

Under amortized variational inference, the approximate posterior on the latents, i.e., (),  and the likelihood ()  distribution can be modeled using deep neural networks with parameters  and , respectively, using the reparameterization trick \citep{kingma2014autoencoding, rezende2016variational}. The choice of the prior distribution  is flexible and can vary from a standard Gaussian \citep{kingma2014autoencoding} to more expressive priors \citep{berg2019sylvester, grathwohl2018ffjord, kingma2017improving}.

\subsection{Denoising Diffusion Probabilistic Models}

DDPMs \citep{sohldickstein2015deep, ho2020denoising} are latent-variable models consisting of a forward noising process () which gradually destroys the structure of the data  and a reverse denoising process (()) which learns to recover the original data  from the noisy input. The forward noising process is modeled using a first-order Markov chain with Gaussian transitions and is fixed throughout training, and the noise schedules  to  can be fixed or learned. The form of the forward process can be summarized as follows:






The reverse process  can also be parameterized using a first-order Markov chain with a learned Gaussian transition distribution as follows


Given a large enough  and a well-behaved variance schedule of , the distribution  will approximate an isotropic Gaussian. The entire probabilistic system can be trained end-to-end using variational inference. During sampling, a new sample can be generated from the underlying data distribution by sampling a latent (of the same size as the training data point ) from  (chosen to be an isotropic Gaussian distribution) and running the reverse process. We highly encourage the readers to refer to Appendix \ref{sec:appendix_a} for a more detailed background on diffusion models.





\section{DiffuseVAE: VAEs meet Diffusion Models}
\label{sec:DiffuseVAE}

\subsection{DiffuseVAE Training Objective}
\label{subsec:DiffuseVAE_training_objective}

Given a high-resolution image , an auxiliary conditioning signal  to be modelled using a VAE, a latent representation  associated with , and a sequence of  representations  learned by a diffusion model, the DiffuseVAE joint distribution can be factorized as:

where  and  are the parameters of the VAE decoder and the reverse process of the conditional diffusion model, respectively.
Furthermore, since the joint posterior  is intractable to compute, we approximate it using a surrogate posterior  which can also be factorized into the following conditional distributions:

where  are the parameters of the VAE recognition network (). As considered in previous works \citep{sohldickstein2015deep, ho2020denoising} we keep the DDPM forward process () non-trainable throughout training. The log-likelihood of the training data can then be obtained as:

Since this estimate is intractable to estimate analytically, we optimize the ELBO corresponding to the log-likelihood. It can be shown that the log-likelihood estimate of the data can be approximated using the following lower bound (See Appendix \ref{subsec:appendix_c_1} for the proof)

We next discuss the choice of the conditioning signal , some simplifying design choices and several parameterization choices for the VAE and the DDPM models.

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{images/method.png}
    \caption{Proposed DiffuseVAE generative process under the simplifying design choices discussed in Section ~\ref{subsec:design_choices}. DiffuseVAE is trained in a two-stage manner: The VAE encoder takes the original image  as input and generates a reconstruction  which is used to condition the second stage DDPM.}
    \label{fig:architecture}
\end{figure*}

\subsection{Simplifying design choices}
\label{subsec:design_choices}
In this work we are interested in unconditional modeling of data. To this end, we make the following simplifying design choices:

\begin{enumerate}
    \item \textbf{Choice of the conditioning signal }: We assume the conditioning signal  to be  itself which ensures a deterministic mapping between  and . Given this choice, we do not condition the reverse diffusion process on  and take it as  in Eq. \ref{eqn:elbo}.
    
    \item \textbf{Choice of the conditioning signal }: Secondly, instead of conditioning the reverse diffusion directly on the VAE inferred latent code , we condition the second stage DDPM model on the VAE reconstruction  which is a deterministic function of . 


    \item \textbf{Two-stage training}: We train Eq. \ref{eqn:elbo} in a sequential two-stage manner, i.e., first optimizing  and then optimizing for  in the second stage while fixing  and  (i.e. freezing the VAE encoder and the decoder).
\end{enumerate}

With these design choices, as shown in Fig. \ref{fig:architecture}, the DiffuseVAE training objective reduces to simply training a VAE model on the training data  in the first stage and conditioning the DDPM model on the VAE reconstructions in the second stage. We next discuss the specific parameterization choices for the VAE and DDPM models. We also justify these design choices in Appendix \ref{app:appendix_d}.

\subsection{VAE parameterization}

In this work, we only consider the standard VAE (with a single stochastic layer) as discussed in Section \ref{subsec:bg_vae}. However, in principle, due to the flexibility of the DiffuseVAE two-stage training, more sophisticated, multi-stage VAE approaches as proposed in \citep{razavi2019generating, child2021deep, vahdat2021nvae} can also be utilized to model the input data . One caveat of using multi-stage VAE approaches is that we might no longer have access to the useful low-dimensional representation of the data.

\subsection{DDPM parameterization}

In this section, we discuss the two types of conditional DDPM formulations considered in this work.
\subsubsection{Formulation 1}
\label{subsec:form1}
In this formulation, we make the following simplifying assumptions
\begin{enumerate}
    \item The forward process transitions are conditionally independent of the VAE reconstructions  and the latent code information  i.e. .
    \item The reverse process transitions are conditionally dependent on only the VAE reconstruction, i.e., 
\end{enumerate}
A similar parameterization has been considered in recent work on conditional DDPM models \citep{ho2021cascaded, saharia2021image}. We concatenate the VAE reconstruction to the reverse process representation  at each time step  to obtain .

\subsubsection{Formulation 2}
\label{subsec:form2}
In this formulation, we make the following simplifying assumptions
\begin{enumerate}
    \item The forward process transitions are conditionally dependent on the VAE reconstruction, i.e., 
    \item The reverse process transitions are conditionally dependent on only the VAE reconstruction, i.e., 
\end{enumerate}
Specifically, we design the forward process transitions to incorporate the VAE reconstruction  as follows:


It can be shown that the forward conditional marginal in this case becomes (See Appendix \ref{subsec:appendix_c_2} for proof)

For  and a \textit{well-behaved} noise schedule ,  which implies . Intuitively, this means that the Gaussian  becomes our base measure () during inference on which we need to run our reverse process. Since the simplified denoising training formulation proposed in \citep{ho2020denoising} depends on the functional form of the forward process posterior , this formulation results in several modifications in the standard DDPM training and inference which are discussed in Appendix \ref{sec:appendix_b}.



\section{Experiments}
\label{sec:experiments}



We now investigate several properties of the DiffuseVAE model. We use a mix of qualitative and quantitative evaluations for demonstrating these properties on several image synthesis benchmarks including CIFAR-10 \citep{krizhevsky2009learning}, CelebA-64 \citep{liu2015faceattributes}, CelebA-HQ \citep{karras2018progressive} and LHQ-256 \citep{alis} datasets. For quantitative evaluations involving sample quality, we use the FID \citep{heusel2018gans} metric. We also report the Inception Score (IS) metric \citep{Salimans2016ImprovedTF} for state-of-the-art comparisons on CIFAR-10. For all the experiments, we set the number of diffusion time-steps () to 1000 during training. The noise schedule in the DDPM forward process was set to a linear schedule between  and  during training. More details regarding the model and training hyperparameters can be found in Appendix \ref{sec:appendix_e}. Some additional experimental results are presented in Appendix \ref{app:appendix_f}.
\begin{figure*}
  \centering
  \begin{subfigure}{0.49\linewidth}
  \centering
    \includegraphics[width=0.9\textwidth]{images/gr_imgs_form1.pdf}
  \caption{Formulation-1}
  \label{fig:gr_form1}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
  \centering
    \includegraphics[width=0.9\linewidth]{images/gr_imgs_form2.pdf}
    \caption{Formulation-2}
    \label{fig:gr_form2}
  \end{subfigure}
  \caption{Illustration of the generator-refiner framework in DiffuseVAE. The VAE generated samples (Bottom row) are refined by the Stage-2 DDPM model with T=1000 during inference (Top Row).}
  \label{fig:gr}
\end{figure*}

\subsection{Generator-refiner framework}

Fig. \ref{fig:gr} shows samples generated from the proposed DiffuseVAE model trained on the CelebA-HQ dataset at the 128 x 128 resolution and their corresponding Stage-1 VAE samples. For both DiffuseVAE formulations-1 and 2, DiffuseVAE generated samples (Fig. \ref{fig:gr} (Top row) are a refinement of the \textit{blurry} samples generated by our single-stage VAE model (Bottom row).

\begin{table}[H]
\small
\centering
\begin{tabular}{@{}cc@{}}
\toprule
                    & FID@10k  \\ \midrule
Baseline VAE        & 87.28   \\
Baseline VAE + DDPM Refiner (Form-1) & \textbf{10.87}   \\
Baseline VAE + DDPM Refiner (Form-2) & \textbf{11.44}   \\ \bottomrule
\end{tabular}
\caption{Quantitative Illustration of the generator-refiner framework in DiffuseVAE for the CelebA-HQ (128 x 128) dataset. FID reported on 10k samples (Lower is better)}
\label{table:gr}
\end{table}

This observation qualitatively validates our \textit{generator-refiner} framework in which the Stage-1 VAE model acts as a generator and the Stage-2 DDPM model acts as a refiner. The results in Table \ref{table:gr} quantitatively justify this argument where on the CelebA-HQ-128 benchmark, DiffuseVAE improves the FID score of a baseline VAE by about eight times. Additional qualitative results demonstrating this observation can be found in Fig. \ref{fig:gr_add}.

\begin{figure*}
  \centering
    \includegraphics[width=0.9\linewidth]{images/diffusevae_tmlr-vae_interp_form1.pdf}
    \caption{DiffuseVAE samples generated by linearly interpolating in the VAE latent space (Formulation-1, T=1000).  denotes the interpolation factor. \emph{Middle row:} VAE generated interpolation between two samples. \emph{Top row:} Corresponding DDPM refinements for VAE samples in the Middle Row. \emph{Bottom row:} DDPM refinements for VAE samples in the Middle Row with shared DDPM stochasticity among all samples.}
    \label{fig:interpolate_vae}
\end{figure*}

\subsection{Controllable synthesis via low-dimensional DiffuseVAE latents}
\label{subsec:exp_controllable}

\noindent
\subsubsection{DiffuseVAE Interpolation}
The proposed DiffuseVAE model consists of two types of latent representations: the low-dimensional VAE latent code  and the DDPM intermediate representations  associated with the DDPM reverse process (which are of the same size of the input image  and thus might not be beneficial for downstream tasks). We next discuss the effects of manipulating both  and . Although, it is possible to inspect interpolations on the intermediate DDPM representations , we do not investigate this case in this work. We consider the following interpolation settings:

\textbf{Interpolation in the VAE latent space }: We first sample two VAE latent codes  and  using the standard Gaussian distribution. We then perform linear interpolation between  and  to obtain intermediate VAE latent codes  for , which are then used to generate the corresponding DiffuseVAE samples.

Fig. \ref{fig:interpolate_vae} (Middle Row) shows the VAE samples generated by interpolating between two sampled VAE codes as described previously. The corresponding DiffuseVAE generated samples obtained by interpolating in the   space are shown in Fig. \ref{fig:interpolate_vae} (Top Row). It can be observed that the refined samples corresponding to the blurry VAE samples preserve the overall structure of the image (facial expressions, hair style, gender etc). However, due to the stochasticity in the reverse process sampling in the second stage DDPM model, minor image details (like lip color and minor changes in skin tone) do not vary smoothly between the interpolation samples due to which the overall interpolation is not smooth. This becomes more clear when interpolating the DDPM latent  while keeping the VAE code  fixed as discussed next.

\textbf{Interpolation in the DDPM latent space with fixed }: Next, we sample the VAE latent code  using the standard Gaussian distribution. With a fixed , we then sample two initial DDPM representations  and  from the reverse process base measure . We then perform linear interpolation between  and  with a fixed  to generate the final DiffuseVAE samples (Note that interpolation is not performed on other DDPM latents, , which are obtained using ancestral sampling from the corresponding 's as usual).

\begin{figure*}
  \centering
    \includegraphics[width=1.0\linewidth]{images/diffusevae_tmlr-ddpm_interp_form1.pdf}
    \caption{DiffuseVAE samples generated by linearly interpolating in the  latent space (Formulation-1, T=1000).  denotes the interpolation factor.}
    \label{fig:interpolate_ddpm}
\end{figure*}

Fig \ref{fig:interpolate_ddpm} shows the DiffuseVAE generated samples with a fixed  and the interpolated . As can be observed, interpolating in the DDPM latent space leads to changes in minor features (skin tone, lip color, collar color etc.) of the generated samples while major image structure (face orientation, gender, facial expressions) is preserved across samples. This observation implies that the low-dimensional VAE latent code mostly controls the structure and diversity of the generated samples and has more entropy than the DDPM representations , which carry minor stochastic information. Moreover, this results in non-smooth DiffuseVAE interpolations. We discuss a potential remedy next.

\textbf{Handling the DDPM stochasticity}:
The stochasticity in the second stage DDPM sampling process can occasionally result in artifacts in DiffuseVAE samples which might be undesirable in downstream applications. To make the samples generated from DiffuseVAE deterministic (i.e. controllable only from ), we simply share all stochasticity in the DDPM reverse process (i.e. due to  and ) across all generated samples. This simple technique adds more consistency in our latent interpolations as can be observed in Fig. \ref{fig:interpolate_vae} (Bottom Row) while also enabling deterministic sampling. This observation is intuitive as initializing the second stage DDPM in DiffuseVAE with different stochastic noise codes during sampling might be understood as imparting different styles to the refined sample. Thus, sharing this stochasticity in DDPM sampling across samples implies using the same stylization for all refined samples leading to smoothness between interpolations. Having achieved more consistency in our interpolations, we can now utilize the low-dimensional VAE latent code for controllable synthesis which we discuss next.

\begin{figure*}
  \centering
    \includegraphics[width=0.9\linewidth]{images/cs256.pdf}
    \caption{Controllable generation on DiffuseVAE generated samples on the CelebA-HQ 256 dataset. Red and green arrows indicate vector subtract and addition operations respectively. Top and Bottom panels show single edits and composite edits respectively.}
    \label{fig:cs256}
\end{figure*}


\subsubsection{From Interpolation to Controllable Generation}
Since DiffuseVAE gives us access to the entire low dimensional VAE latent space, we can perform image manipulation by performing vector arithmetic in the VAE latent space (See Appendix \ref{sec:app_controllable} for details). The resulting latent code can then be used to sample from DiffuseVAE to obtain a refined manipulated image. As discussed in the previous section, we share the DDPM latents across samples to prevent the generated samples from using different styles. Fig. \ref{fig:cs256} demonstrates single-attribute image manipulation using DiffuseVAE on several attributes like \textit{Gender}, \textit{Age} and \textit{Hair texture}. Moreover, the vector arithmetic in the latent space can be composed to generate composite edits (See Fig. \ref{fig:cs256}), thus signifying the usefulness of a low-dimensional latent code representation. Some additional results on image manipulation are illustrated in Fig. \ref{fig:controllable_add}.



\subsection{Better Sampling Speed vs Quality tradeoffs with DiffuseVAE}
\remove{The sample speed vs quality trade-off in DDPMs represents a trade-off between the number of reverse process sampling steps vs the quality of the generated samples.} There exists a trade-off between the number of reverse process sampling steps vs the quality of the generated samples in DDPMs. Usually the best sample quality is achieved when the number of reverse process steps used during inference matches the number of time-steps used during training. However, this can be very time-consuming \citep{song2021denoising}. On the other hand, as the number of reverse process steps is reduced, the sample quality gets worse. We next examine this trade-off in detail.

\textbf{Comparison with a baseline unconditional DDPM}: Table \ref{table:svq_1} compares the sample quality (in terms of FID) vs the number of sampling steps between DiffuseVAE and our unconditional DDPM baseline on the CelebA-HQ-128 dataset. For all time-steps  to , DiffuseVAE outperforms the standard DDPM by large margins in terms of FID. Between DiffuseVAE formulations, the sample quality is similar with Formulation-1 performing slightly better. More notably, the FID score of DiffuseVAE at  and 50 is better than that of unconditional DDPM at  and 100 respectively. Thus, in low time-step regimes, the speed vs quality tradeoff in DiffuseVAE is significantly better than an unconditional DDPM baseline. It is worth noting that this property is intrinsic to DiffuseVAE as the model was not specifically trained to reduce the number of reverse process sampling steps during inference \citep{salimans2022progressive}.

However, at  the unconditional DDPM baseline performs better than both DiffuseVAE formulations-1 and 2. We hypothesize that this gap in performance can be primarily attributed to the prior-hole problem, i.e., the mismatch between the VAE prior  and the aggregated posterior  \citep{BauMni18, dai2019diagnosing, Ghosh2020FromVT} due to which VAEs can generate poor samples from regions of the latent space unseen during training. DDPM refinement of such samples can affect the FID scores negatively. We confirm this hypothesis next.

\begin{table}[]
\scriptsize
\centering
\begin{tabular}{@{}cccccc@{}}
\toprule
                             & 10             & 25             & 50             & 100            & 1000          \\ \midrule
DDPM (Uncond)                & 41.25          & 27.83          & 21.40          & 16.29          & \textbf{8.93} \\\midrule
DiffuseVAE (Form-1)          & 31.11          & \textbf{19.44}          & \textbf{15.31}          & \textbf{13.68}          & 12.63         \\
DiffuseVAE (Form-2)          & \textbf{31.08}          & 19.67          & 15.96          & 13.96          & 13.20         \\ \midrule
DiffuseVAE (Form-1, GMM=100) & 30.74          & \textbf{18.55} & \textbf{14.10} & \textbf{12.12} & 10.87         \\
DiffuseVAE (Form-2, GMM=100) & \textbf{30.66} & 18.98          & 14.45          & 12.50          & 11.44\\\bottomrule
\end{tabular}
\caption{Comparison of sample quality (FID@10k) vs speed on the CelebA-HQ-128 dataset (DiffuseVAE vs unconditional DDPM). Top Row represents the number of reverse process sampling steps.}
\label{table:svq_1}
\end{table}

\textbf{Improving DiffuseVAE sample quality using post-fitting}: One way to alleviate the prior-hole problem is to fit a density estimator (denoted by Ex-PDE) on the training latent codes and sample from this estimator during inference as in \citep{10.5555/3295222.3295378,razavi2019generating, Ghosh2020FromVT}. Along similar lines, we fit a GMM on the VAE latent code representations of the training data. We then use this estimator to sample VAE latent codes during DiffuseVAE sampling. Table \ref{table:svq_1} shows the FID scores on the CelebA-HQ-128 dataset for both DiffuseVAE formulations using a GMM with 100 components. Across all time-steps, using Ex-PDE during sampling leads to a reduced gap in sample quality at , thereby confirming our hypothesis. We believe that the remaining gap can be closed by using stronger density estimators which we do not explore in this work. Moreover, a side benefit of using a Ex-PDE during sampling is further improvement in the speed-quality tradeoff.

\textbf{Further improvements with DDIM}: DDIM \citep{song2021denoising} employs a non-Markovian forward process and achieves a better speed-quality tradeoff than DDPM along with deterministic sampling. Since DiffuseVAE employs a DDPM model in the refiner stage, we found DDIM sampling to be complementary with the DiffuseVAE framework. Notably, since the forward process for DiffuseVAE (Form-2) is different, we derive the DDIM updates for this formulation in Appendix \ref{sec:appendix_b}. Table \ref{table:svq_2} compares the speed-quality tradeoff between DDIM and DiffuseVAE (with DDIM sampling) on the CelebA-HQ-128 and CelebA-64 datasets. DiffuseVAE (both formulations) largely outperforms the standard unconditional DDIM at all time-steps. For the CelebA-HQ-128 benchmark, similar to our previous observation, DiffuseVAE (with DDIM sampling and Ex-PDE using GMMs) at  and 50 steps performs better than the standard DDIM at  and 100 steps respectively. In fact, at , DiffuseVAE (with Formulation-2) achieves a FID of 16.47 which is better than DDIM with  steps, thus providing a speedup of almost 10x. Similarly for the CelebA-64 benchmark, at , DiffuseVAE (Formulation-2) performs similarly to the unconditional DDIM at , thus providing a 4x speedup. Lastly, it can be observed from Tables \ref{table:svq_2}, \ref{table:svq_add_1} and \ref{table:svq_add_2} that in the low time-step regime, DiffuseVAE (Form-2) usually performs better than Form-1 and that the speed-quality trade-off in DiffuseVAE becomes better with increasing image resolutions.


\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{@{}ccccccccc@{}}
\toprule
         \multicolumn{1}{c|}{}                   & \multicolumn{4}{c|}{CelebAHQ-128}                                  & \multicolumn{4}{c}{CelebA-64}                                  \\ \midrule
                            & 10             & 25             & 50             & 100            & 10             & 25            & 50            & 100           \\ \midrule
DDIM (uncond)               & 34.36          & 25.04          & 19.83          & 16.69          & 14.14          & 7.88          & 6.77          & 6.38          \\ \midrule
DiffuseVAE (Form-1)         & 19.42          & 15.12          & 14.53          & 14.53          & 10.79          & 6.87          & 6.08          & 5.82          \\
DiffuseVAE (Form-1, Ex-PDE) & \textbf{18.01} & \textbf{13.21} & \textbf{12.40} & \textbf{12.28} & \textbf{10.44} & \textbf{6.59} & \textbf{5.81} & \textbf{5.55} \\ \midrule
DiffuseVAE (Form-2)         & 17.51          & 13.45          & 12.56          & 12.51          & 9.81           & 6.34          & 5.83          & 5.59          \\
DiffuseVAE (Form-2, Ex-PDE) & \textbf{16.47} & \textbf{11.62} & \textbf{10.83} & \textbf{10.28} & \textbf{9.56}  & \textbf{5.90} & \textbf{5.43} & \textbf{5.21} \\ \bottomrule
\end{tabular}
\caption{Comparison of sample quality (FID@10k) vs speed between DiffuseVAE and the unconditional DDIM on the CelebA-HQ-128 and CelebA-64 datasets. DiffuseVAE with Form-2 shows a better speed-quality tradeoff than Form-1. Overall, DiffuseVAE achieves upto 4x and 10x speedups on the CelebA-64 and the CelebA-HQ-128 datasets respectively as compared to the uncondtional DDIM}.
\label{table:svq_2}
\end{table}

\begin{table}[]
\scriptsize
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
\multicolumn{1}{l}{} & \textbf{Method} & \textbf{FID@50k}  & \textbf{IS}  \\ \midrule
\multirow{7}{*}{\textbf{Ours}} & DiffuseVAE (Form-1, T=1000) & 2.95 & 9.60  0.11 \\
& DiffuseVAE (Form-2, T=1000) & 2.86 & 9.59  0.13 \\
& DiffuseVAE (Form-1, T=1000, GMM=50) & 2.84 & 9.51  0.08 \\
& DiffuseVAE (Form-2, T=1000, GMM=50) & 2.80 & 9.51  0.08 \\
& DiffuseVAE-72M (Form-2, T=1000, GMM=50) & 2.62 & 9.75  0.08 \\
& DDPM (T=1000, Our impl.) & 3.01 & 9.55  0.16 \\
& VAE Baseline & 139.50 &  \\
& VAE Baseline (GMM=50) & 137.68 &  \\ \midrule
\multirow{4}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}VAE-based\\ methods\end{tabular}}} & \begin{tabular}[c]{@{}c@{}}VAEBM \citep{xiao2021vaebm} (w/ PC)\end{tabular} & 12.19 & 8.43 \\
 & DC-VAE \citep{parmar2021dual} & 17.90 & 8.2 \\
 & NVAE \citep{vahdat2021nvae} & 51.67 & 5.51 \\
 & NCP-VAE \citep{Aneja2020NCPVAEVA} & 24.08 & - \\
 & LSGM (FID) \citep{vahdat2021scorebased} & 2.10 & - \\
 & D2C \citep{sinha2021d2c} & 10.15 & - \\ \midrule
\multirow{6}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}GAN-based \\ methods\end{tabular}}} & AutoGAN \citep{cao2020auto} & 12.4 &  \\
 & ProGAN \citep{karras2018progressive} & 15.52 &  \\ 
 & \begin{tabular}[c]{@{}c@{}}StyleGAN2 (w/o ADA) \citep{karras2019stylebased}\end{tabular} & 8.32 &  \\
 & \begin{tabular}[c]{@{}c@{}}StyleGAN2-ADA \citep{https://doi.org/10.48550/arxiv.2006.06676}\end{tabular} & 2.92 &  \\
 & SNGAN \citep{miyato2018spectral}  & 21.7 &  \\
 & SNGAN + DDLS \citep{che2021gan} & 15.42 &  \\ \midrule
\multirow{6}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Score-based\\ methods\end{tabular}}} & NCSN \citep{song2020generative} & 25.32 &   \\
 & \begin{tabular}[c]{@{}c@{}}NCSNv2 (w/denoising) \citep{song2020improved}\end{tabular} & 10.87 &  \\
 & DDPM \citep{ho2020denoising} & 3.17 &  \\
 & \begin{tabular}[c]{@{}c@{}}SDE (NCSN++) \citep{song2021scorebased}\end{tabular} & 2.45 & 9.73 \\
 & SDE (DDPM++) \citep{song2021scorebased} & 2.78 & 9.64 \\  \midrule
\end{tabular}
\caption{Generative performance on unconditional CIFAR-10. FID and IS computed on 50k samples}
\label{table:cifar10_sota}
\end{table}





\subsection{State-of-the-art comparisons}
For reporting comparisons with the state-of-the-art we primarily use the FID \citep{heusel2018gans} metric to assess sample quality. We compute FID on 50k samples for CIFAR-10 and CelebA-64. For comparisons on the CelebA-HQ-256 dataset, we report the FID only for 10k samples (as opposed to 30k samples which is the norm on this benchmark) due to compute limitations. Due to this, we anticipate the true FID score on this benchmark using our method to be lower. However, as we show, the FID score obtained by DiffuseVAE on this benchmark on 10k samples is still comparable to state-of-the-art.

Table \ref{table:cifar10_sota} shows quantitative comparison between DiffuseVAE and other state-of-the-art unconditional generative models in terms of sample quality (FID@50k) and sample diversity (IS) on the CIFAR-10 dataset. Interestingly, our unconditional DDPM baseline achieves better FID scores on CIFAR-10 than reported in \citep{ho2020denoising}. DiffuseVAE clearly outperforms the DDPM baseline (with and without Ex-PDE) in terms of FID while maintaining a competitive IS score with continuous score based methods indicating good sample diversity. Notably, with the exception of LSGM \citep{vahdat2021scorebased}, DiffuseVAE outperforms all prior state-of-the-art VAE-based methods \citep{vahdat2021nvae,xiao2021vaebm,sinha2021d2c}, even when most of these methods utilize powerful hierarchical VAE-based backbones. In contrast, DiffuseVAE utilizes a simple VAE backbone with a very poor baseline FID score and it would be interesting to benchmark LSGM using a simple VAE backbone as ours (some initial evaluations on CIFAR-10 already suggest that LSGM might perform much worse than DiffuseVAE with a simple VAE baseline \footnote{See \url{https://openreview.net/forum?id=P9TYG0j-wtG&noteId=Z7AYukcBJ_q}}). In this work our CIFAR-10 model is the same size as in \citep{ho2020denoising} which is an order of magnitude smaller than LSGM (See Table \ref{table:model_compare}). Indeed, like LSGM, DiffuseVAE can also take advantage of larger model sizes (DiffuseVAE-72M with Ex-PDE achieves a FID of \textbf{2.62} and a mean IS of \textbf{9.75} on CIFAR-10. See Appendix \ref{sec:app_sota}). \textit{Moreover, to the best of our knowledge, DiffuseVAE is the first model to outperform StyleGAN2-ADA \citep{https://doi.org/10.48550/arxiv.2006.06676} on this benchmark while being trained using non-adversarial losses and retaining access to a low-dimensional latent code}.

We also benchmarked DiffuseVAE (with Ex-PDE) on two popular face image benchmarks: CelebA-64 and CelebA-HQ-256. On the CelebA-64 benchmark, DiffuseVAE performs comparably with the DDPM baseline. Similar to CIFAR10, DiffuseVAE outperforms other VAE-based methods \citep{sinha2021d2c, Aneja2020NCPVAEVA, xiao2021vaebm} by a significant margin. We observed similar trends on the CelebA-HQ-256 dataset where DiffuseVAE outperforms competing VAE based methods except LSGM and is comparable to VQGAN \citep{https://doi.org/10.48550/arxiv.2012.09841}. However, when comparing with LSGM on this benchmark, similar arguments as pointed out for CIFAR-10 hold. Interestingly, we found that for CelebA-HQ-256 dataset, samples generated during intermediate training stages (and even after convergence) suffer from color bleeding. We found that this problem can be alleviated by using temperature sampling in the second stage DDPM latents (Appendix \ref{sec:app_sota}). Therefore, only for , we report the FID scores on this benchmark with a scaling factor of 0.8.



\begin{table}[]
\scriptsize
\centering
\begin{minipage}{0.49\linewidth}
\centering
\begin{tabular}{@{}cc@{}}
\toprule
\textbf{Method}                      & \textbf{FID@50k}           \\ \midrule
DiffuseVAE (Form-1, T=1000, GMM=75) & 4.05 \\
DiffuseVAE (Form-2, T=1000, GMM=75) & 3.97 \\
DDPM (T=1000, Our impl.)    & 3.93          \\
VAE Baseline (GMM=75)                & 72.11         \\ \midrule
D2C \citep{sinha2021d2c}                         & 5.7           \\
NCP-VAE \citep{Aneja2020NCPVAEVA}                     & 5.25          \\
VAEBM \citep{xiao2021vaebm}                       & 5.31          \\
NVAE \citep{vahdat2021nvae}                        & 14.74         \\
NCSN \citep{song2020generative}                    & 25.30         \\
NCSNv2 \citep{song2020improved}                     & 10.23         \\
QA-GAN  \citep{NEURIPS2019_b59a51a3}                    & 6.42          \\
COCO-GAN  \citep{lin2020cocogan}                  & 4.0           \\ \bottomrule
\end{tabular}
\caption{Generative performance on CelebA-64}
\label{table:celeba64_sota}
\end{minipage}
\hfill
\begin{minipage}{0.49\linewidth}
\centering
\begin{tabular}{@{}cc@{}}
\toprule
\textbf{Method}              & \textbf{FID}  \\ \midrule
DiffuseVAE (T=1000, GMM=100, FID@10k) &  11.28              \\
VAE Baseline (GMM=100, FID@10k)       &     97.07             \\ \midrule
LSGM \citep{vahdat2021scorebased}                        & 7.22             \\
VQGAN + Transformer \citep{https://doi.org/10.48550/arxiv.2012.09841}          & 10.2             \\
D2C \citep{sinha2021d2c}                          & 18.74            \\
DCVAE \citep{parmar2021dual}                       & 15.81            \\
VAEBM \citep{xiao2021vaebm}                        & 20.38            \\
NCP-VAE \citep{Aneja2020NCPVAEVA}                     & 24.8             \\
NVAE \citep{vahdat2021nvae}                         & 40.26            \\ \bottomrule
\end{tabular}
\caption{Generative performance on CelebA-HQ-256}
\label{table:celebahq_sota}
\end{minipage}
\end{table}

\subsection{Generalization to different noise types}
\label{sec:generalize}
To test if DiffuseVAE can generalize over different types of noisy conditioning signals during sample generation, we condition the second stage DDPM model in DiffuseVAE (pre-trained on the CIFAR-10 dataset) on different types of noisy conditioning signals (instead of the VAE reconstruction). More specifically, we experiment with two such types of conditioning signals obtained by adding noise to CIFAR-10 test samples: downsampling CIFAR samples to 16x16 resolution (effectively blurring them when scaled back) and adding Gaussian noise (with standard deviation = 0.3). Final DiffuseVAE samples obtained after conditioning on these noisy inputs are visualized in Fig. \ref{fig:generalize} (with additional results on the CelebA-HQ-128 samples illustrated in Fig. \ref{fig:generalize_extended}). We observed that DiffuseVAE is able to recover the original samples from the noisy inputs which demonstrates generalization to different noisy conditioning inputs.

Intuitively, these results can be expected since, during training, the proposed DiffuseVAE method learns to refine VAE reconstructions which lack a lot of detail. Hence the task of refining these reconstructions might be more challenging, thus allowing the network to generalize to \textit{simpler} tasks inherently as illustrated above. However, it is worth noting that certain artifacts in the generated refinements are evident (For instance in Figure \ref{fig:generalize_extended}, the sample quality shows a sharp degradation as more noise is added to the conditioning signal), leaving scope for design of more stronger conditioning mechanisms in diffusion models that allow to adapt conditional diffusion models on downstream tasks like image super-resolution in an out-of-the-box fashion.



\section{Related Work}

Following the seminal work of \citep{sohldickstein2015deep, ho2020denoising} on diffusion models, there has been a lot of recent progress in both unconditional \citep{nichol2021improved, dhariwal2021diffusion, kingma2021variational} and conditional diffusion models \citep{ho2021cascaded, saharia2021image, choi2021ilvr, chen2020wavegrad} (including score-based models \citep{song2021scorebased, song2020generative}) for a variety of downstream tasks including image synthesis, audio synthesis and likelihood estimation among others. Here we only compare DiffuseVAE to recent methods which attempt to combine VAEs with diffusion models. We refer the readers to Appendix \ref{sec:appendix_related} for a detailed comparison of DiffuseVAE to other types of model families.

Among recent advances, there are several works which apply diffusion models in the latent space of powerful autoencoding baselines. D2C \citep{sinha2021d2c} utilizes a learned diffusion-based prior over the NVAE \citep{vahdat2021nvae} latent representations while also refining the latent space using a contrastive loss. LSGM \citep{vahdat2021scorebased} performs score-based generative modeling in the latent space of NVAE baseline. Similarly, Latent Diffusion Models (LDM) \citep{rombach2021highresolution} apply diffusion models in the latent space of a powerful pretrained VQ-GAN \citep{https://doi.org/10.48550/arxiv.2012.09841} autoencoding baseline. In contrast, our method refines “blurry” reconstructions generated by an extremely lightweight VAE using a downstream diffusion model. A possible benefit of having a generator-refiner framework in contrast to the latent diffusion framework could be the requirement of a powerful VAE baseline as a pre-requisite to generate high-quality samples. Since there exists a trade-off between latent code disentanglement and high quality reconstructions \citep{Higgins2017betaVAELB}, the need of a high fidelity autoencoding baseline can be disadvantageous in situations where a fine-grained control over the generated samples is required. We hypothesize that this problem is alleviated in DiffuseVAE since our first stage model can readily tradeoff more disentanglement for lower fidelity reconstructions due to a powerful second stage diffusion-based refiner model. Lastly, we hypothesize that the latent diffusion framework is complementary to DiffuseVAE since the prior used in our VAE training can be modeled using a diffusion model.

\citep{luo2021diffusion} present a probabilistic autoencoding framework for point cloud generation via a VAE-like encoder and a diffusion model based decoder. Notably, the most closest to our approach is the concurrent work on DiffAE \citep{preechakul2021diffusion} which uses an end-to-end autoencoding framework for conditioning the diffusion process decoder on the latent code output of an encoder. This equips the diffusion model with a low-dimensional latent space. However, since the model is non-probabilistic, DiffAE relies on fitting a powerful DDIM density estimator on the latent space of the encoder to enable sampling. Moreover, it's unclear if DiffAE exhibits good sample quality when fitting simple density estimators on the encoder latent space. In contrast, sampling in DiffuseVAE is straightforward due to a probabilistic formulation. Additionally, DiffuseVAE can also take advantage of fitting external density estimators on the latent space as demonstrated in this work.

\begin{figure*}[t]
  \centering
    \includegraphics[width=0.9\linewidth]{images/diffusevae_tmlr-cifar_superres.png}
    \caption{Illustration of DiffuseVAE generalization to different noise types in the conditioning signal on the CIFAR-10 test set.}
    \label{fig:generalize}
\end{figure*}



\section{Limitations and Discussion}

In this work, we presented a novel unifying framework for training VAEs and diffusion models and demonstrated its effectiveness in generating high-quality samples, providing a better sample quality vs number of steps trade-off while equipping DDPM with a low dimensional latent code which can be used for controllable synthesis using DDPM, and generalizing to different types of noise in the conditioning signal. However, the DiffuseVAE model is not without its limitations:

\begin{enumerate}
    \item Due to a generator-refiner framework, the semantics of the final generated samples depends largely on the coarse sample generated by the \textit{generator} model (a simple VAE in our case). Therefore, if the coarse sample is not semantically meaningful, this will propagate to the final generated sample after refinement. This can be expected from VAEs due to a mismatch between the aggregated posterior  and the prior  during VAE training which we alleviate using Ex-PDE estimation but the problem still persists (which is evident from the gap in sample quality between an unconditional DDPM baseline and DiffuseVAE even after Ex-PDE).
    
    \item We also observed that when the conditioning signal provided by the first stage VAE is uninformative (too blurry), the second stage DDPM model can generate unpredictable refinements. On this note, it would be interesting to explore the impact of the choice of VAE on the overall sample quality of the model. Moreover, since we work with vanilla VAEs, some artifacts in controllable synthesis results are evident due to correlated attribute-specific latent directions (See Figure \ref{fig:cs_debug}). Using variants like -VAEs \citep{Higgins2017betaVAELB} can help achieve more disentanglement between image attributes leading to better controllable synthesis results.
    
    \item In this work, since we focus on sample quality, we did not explore the impact of the diffusion model training on the latent space of the VAE when trained end-to-end. It would be interesting to explore if end-to-end training might alleviate some problems with VAE's.
    
    \item Lastly, it would be interesting to explore stronger conditioning mechanisms in the context of diffusion models which reduce the reliance of the final sample on the stochastic DDPM sub-code. In the context of DiffuseVAE, this can also be useful in improving model generalization to downstream tasks like image super-resolution and denoising as presented in Section \ref{sec:generalize}
\end{enumerate}





\section*{Broader Impact Statement}
In addition to modelling images, our proposed approach can also be used to model data of other modalities like speech, text, etc. It has the potential to mitigate bias and privacy issues for related ML models that require data collection and annotation. However, such techniques could also be misused to produce fake or misleading information, and researchers should be aware of these risks and explore the proposed approaches responsibly.



\subsubsection*{Acknowledgments}
We would like to thank Ben Poole for his insightful comments and suggestions through the course of this project. We would also like to thank Google Cloud for supporting our research in the form of cloud compute credits.

\bibliography{main}
\bibliographystyle{tmlr}

\newpage

\appendix
\section{Background on Diffusion models}
\label{sec:appendix_a}

\begin{figure*}
  \centering
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=1.0\linewidth]{images/forward_process.pdf}
    \caption{Forward Process}
    \label{fig:forward_process}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=1.0\linewidth]{images/reverse_process.pdf}
    \caption{Reverse Process}
    \label{fig:reverse_process}
  \end{minipage}
\end{figure*}

DDPMs \citep{sohldickstein2015deep, ho2020denoising} are latent-variable models consisting of a forward noising process () (corresponding to an inference model in other generative model families like VAEs \citep{kingma2014autoencoding, rezende2016variational}. See Fig. \ref{fig:forward_process}) and a reverse denoising process () (corresponding to a generator or decoder in VAEs. See Fig. \ref{fig:reverse_process}). The forward process is modeled using a Markov chain which gradually destroys the structure of the data  over a number of time-steps T. Similarly, the reverse process is also modeled as a Markov chain which learns to recover the original data  from the noisy input . The form of the forward process and some notable properties of the forward process conditional distributions are summarized in the following equations ( Eqs.~(\ref{eqn:for1}-\ref{eqn:for2})).



The forward process of DDPMs admits a closed form for  for any , as follows:



The forward process posteriors are also tractable and are given by



The reverse process  can also be parameterized using a first-order Markov chain with a learned Gaussian transition distribution as follows



Given a large enough  and a well-behaved variance schedule of , the distribution  will approximate an isotropic Gaussian. We can generate a new sample from the underlying data distribution  by sampling a latent from  (chosen to be an isotropic Gaussian distribution) and running the reverse process.
As proposed in \citep{ho2020denoising}, the reverse process in DDPM is trained to minimize the following upper bound over the negative log-likelihood (See \citep{sohldickstein2015deep} for detailed proofs):

A notable aspect of the above objective is that all the KL divergences involve Gaussians and, consequently, are available in closed form. Notably, \citep{ho2020denoising} parameterize the reverse process conditional  using the forward process posterior . \citep{ho2020denoising} show that such a parameterization simplifies the second term in Eq. \ref{eqn:ddpm_elbo} at any given time-step  to the following objective in Eq. \ref{eqn:ddpm_simplified}.


where  and . Intuitively, this means that the reverse process in DDPM is trained to predict the noise added to the input  at any time-step . 
We use this \textit{simplified} training formulation throughout our work to train all proposed parameterizations of diffusion models as \citep{ho2020denoising} show that this formulation yields superior sample quality than other forms of reverse process parameterizations. For further details on the exact training and inference processes, we encourage the readers to refer to \citep{ho2020denoising}.

\newpage
\section{Discussion of DiffuseVAE (Formulation-2)}
\label{sec:appendix_b}

\begin{minipage}[t]{0.5\linewidth}
\null
\begin{algorithm}[H]
\begin{algorithmic}
\Repeat
      \State 
      \State 
      \State 
      \State 
      \State \text{Take gradient descent step on:}
      \State 
    \Until{convergence}
	\caption{DDPM Training (Form. 2)}
	\label{algo:form2_training}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{0.5\linewidth}
\null
\begin{algorithm}[H]
\begin{algorithmic}
\State 
      \State 
	  \State 
	  \For{t = T \textbf{to} 1}
    	\State 
    	\State 
    	\State 
    	\State 
    	\EndFor\\
    	\State return 
	\caption{DDPM Inference (Form. 2)}
	\label{algo:form2_inference}
\end{algorithmic}
\end{algorithm}
\end{minipage}

\noindent
The DDPM training objective proposed in \citep{ho2020denoising}, has the following form:

\subsection{Reverse Process parameterization} 

Following \citep{ho2020denoising}, we parameterize the reverse process transition  using the functional form of the forward process posterior . For the DiffuseVAE formulation proposed in Section 3.4.2 in our paper, the forward process conditional distributions can be specified as:


The posterior distribution  will also be a Gaussian distribution with the following form:

where,


Hence the forward process posterior in this DiffuseVAE formulation is a shifted version of the forward process posterior proposed in \citep{ho2020denoising}. Since the VAE reconstruction  for an image  is constant during DDPM training, we can parameterize the reverse process posterior as . Additionally, we keep the variance of the reverse process conditional fixed and equal to  as proposed in \citep{ho2020denoising}. Since , the DDPM training objective in our formulation remains unchanged from the simplified denoising score matching objective proposed in \citep{ho2020denoising}.\\

\noindent
\subsection{Choice of the decoder, }
One possible choice for the decoder is to set  to be a discrete independent decoder derived from the Gaussian  \citep{ho2020denoising}. However, at , we have . Therefore, to account for the VAE reconstruction bias in the final DDPM output, we set our decoder . Without using this adjustment, we found the final DDPM samples to be a bit blurry in our initial experiments.The final training and inference algorithms are summarized in Algorithms \ref{algo:form2_training} and \ref{algo:form2_inference} respectively. In Algorithm \ref{algo:form2_inference}, the coefficients  denote the coefficients of the forward process posterior in Eqn. \ref{eqn:fwd_post}.

\noindent
\subsection{Integration with DDIM}
We now derive the updates for the DiffuseVAE formulation-2 when combined with DDIM sampling. Given the form of the forward process marginal as in Eqn. \ref{eqn:marg_form2_app}, we assume the following form of the forward process posterior:




We now have,

Since both the distributions within the integral are gaussians, the resulting marginal will also be a gaussian with the following form:



However, we already know the form of the marginal  from Eqn. \ref{eqn:marg_form2_app} as follows:

Therefore it implies that,

This completes the analysis of the modified DDIM forward process posterior which is compatible with DiffuseVAE formualation-2
\noindent
\subsection{Primary Intuition}
The primary intuition behind constructing such a formulation is that by initializing the base distribution from a VAE reconstruction, we can hope to speed up the reverse diffusion process. In the low time-step regime, DiffuseVAE (Form-2) usually performs better than (Form-1) (See Tables \ref{table:svq_2}, \ref{table:svq_add_1}  and \ref{table:svq_add_2}). These results indicate our hypothesis might hold valid in the low-time-step regime in diffusion models.

\section{Related Work}
\label{sec:appendix_related}
Recent work in DDPMs also includes improving the speed vs sample quality tradeoff in the DDPM sampling process \citep{song2021denoising, watson2021learning, luhman2021knowledge, salimans2022progressive, xiao2022tackling}. We consider these advances in speeding up diffusion models are complementary to our work and can also be used to improve the sampling efficiency of DiffuseVAE. However, on the contrary, a majority of such methods were designed for improving sampling speeds in DDPMs while DiffuseVAE improves this tradeoff inherently. Similarly for VAEs \citep{kingma2014autoencoding,rezende2016variational}, there has also been progress in improving the ELBO estimates \citep{sinha2021consistency, burda2016importance, masrani2021thermodynamic}  and image synthesis \citep{child2021deep, vahdat2021nvae, lee2020highfidelity, xiao2021vaebm}. Next, we compare our proposed approach in detail with several of these related existing model families.

\noindent
\textbf{Unconditional DDPM}: DDPM/DDIM as introduced in \citep{ho2020denoising, song2021denoising} lacks a low-dimensional latent code which limits model application scope in several downstream tasks. In contrast, DiffuseVAE equips diffusion models with a low dimensional latent code that can be utilized for downstream tasks including but not limited to controllable synthesis. Moreover, we demonstrate a better speed vs quality tradeoff in DiffuseVAE as compared to standard unconditional DDPM/DDIM models and that the conditioning signal in DiffuseVAE helps in generalization to noisy conditioning signals.

\noindent
\textbf{Conditional DDPM}:
Conditional DDPM as introduced in \citep{ho2021cascaded} and \citep{saharia2021image} uses a cascade of multiple diffusion models (CDMs) for generating high-resolution images. However, for even a two-stage pipeline, the sampling time of such models would be effectively much higher than DiffuseVAE. Given the flexibility of our approach, we hypothesize that a single-stage VAE can also be replaced by a complex multi-stage VAE architecture as proposed in \citep{child2021deep, vahdat2021nvae} for comparable sample quality to cascaded diffusion models without affecting the sampling time significantly. Moreover, such cascades lack a low-dimensional latent code which might be a limiting factor for certain downstream applications. It is worth noting that, \citep{ho2021cascaded} use a conditioning augmentation scheme where the high-resolution image is generated by conditioning on a blurred/noisy low resolution image. In contrast, our model is already conditioned on a reconstruction generated by a VAE (which is inherently blurry) and in some sense resembles the heuristic employed in CDMs.

\noindent
\textbf{VAE based methods}
Hierarchical VAEs \citep{sonderby2016ladder, vahdat2021nvae, child2021deep, razavi2019generating} can suffer from posterior collapse and heuristics like gradient skipping and spectral normalization \citep{miyato2018spectral} might be required to stabilize training. Moreover, these models require a large dimensionality of the latent codes to generate high-fidelity samples \citep{vahdat2021nvae, razavi2019generating}. In contrast, DiffuseVAE training does not suffer from such instabilities and provides access to a single latent code layer (with dimensionality comparable to GANs) to generate high-fidelity samples. Among other recent works, VAEBM \citep{xiao2021vaebm} uses EBMs \citep{du2020implicit, nijkamp2019learning} to refine VAE samples while LSGM \citep{vahdat2021nvae} perform score-based modeling in the latent space of a VAE backbone. However, both VAEBM and LSGM use NVAE \citep{vahdat2021nvae} as the base VAE architecture which also lacks a low-dimensional latent code. \citep{lee2020highfidelity} \textit{distill} the disentanglement properties in the VAE latent code to the latent space of a GAN-based generator. However, this approach would also suffer from existing problems of training stability and mode-collapse in GAN-based models. On the other hand, DiffuseVAE does not suffer from such problems
\newpage
\section{Detailed Proofs}
\label{sec:appendix_c}

\subsection{Derivation of the DiffuseVAE objective}
\label{subsec:appendix_c_1}
Given a high-resolution image , an auxiliary conditioning signal  to be modelled using a VAE, a latent representation  associated with , and a sequence of  representations  learned by a diffusion model, the DiffuseVAE generative process,  can be factorized as follows:


where  and  are the parameters of the VAE decoder and the reverse process of the conditional diffusion model, respectively.The log-likelihood of the training data can then be obtained as:

Furthermore, since the joint posterior  is intractable to compute, we approximate it using a surrogate posterior  which can also be factorized into the following conditional distributions:

where  are the parameters of the VAE recognition network (). Since computation of the likelihood in Eq.~(\ref{eqn:likelihood}) is intractable, we can approximate it by computing a lower bound (ELBO) with respect to the joint posterior over the unknowns (, ) as:


Plugging the factorial forms of the DiffuseVAE generative process and the joint posterior defined above in eqn. (\ref{eqn:elbo_app}), we can simplify the ELBO as follows:



\subsection{Derivation of the DiffuseVAE (Formulation-2) marginals}
\label{subsec:appendix_c_2}
\noindent Given:


From Eqn.(\ref{eqn:fwd_app}), we can write,

Taking expectations both sides,

Substituting  from Eqn.(\ref{eqn:first_app}) into the above formulation we get,


\noindent Similarly it can be shown that . Therefore,

\newpage

\section{Justification of the design choices in DiffuseVAE}
\label{app:appendix_d}
Here we justify the design choices made in the DiffuseVAE model specification.
\begin{enumerate}
    \item \textbf{Choice of the conditioning signal }: The choice of assuming the conditioning signal  in Eq. \ref{eqn:elbo} to be the training data  is motivated by the task of \textit{refining} the blurry samples generated by a simple VAE model using a DDPM model.
    
    \item \textbf{Choice of the conditioning signal }: The choice of conditioning the DDPM model on  (the VAE reconstruction of the training data ) instead of the VAE inferred latent code  (usually lower-dimensional) allows us to condition the second stage DDPM directly on samples drawn from another model (not necessarily VAE) or on real images, which can be quite useful as illustrated in Section \ref{sec:generalize}. Additionally, there can be a variant of our method in which the DDPM model is conditioned on both  and . We conditioned the DDPM decoder on  using Adaptive group normalization layers \citep{dhariwal2021diffusion, https://doi.org/10.48550/arxiv.1803.08494} as follows:
    
    
    where  is the output of the first convolution in the residual block and  is obtained from the latent code  and the time-step embedding .
    On benchmarking this DiffuseVAE (Formulation-1) variant on CIFAR-10 trained for around 1.1M steps, we found that the resulting model exhibited slightly worse performance compared to the DiffuseVAE variant conditioned only on the VAE reconstructions () (See Table \ref{table:z_cond}). Therefore, we only condition the DDPM model in DiffuseVAE only on the VAE generated reconstructions.

    \begin{table}[t]
    \centering
    \begin{minipage}{0.49\linewidth}
    \centering
    \begin{tabular}{@{}cc@{}}
    \toprule
    \textbf{Method}             & \textbf{FID@10k}  \\ \midrule
    DiffuseVAE ()     & \textbf{5.94}    \\
    DiffuseVAE ( + Latent code) & 6.07    \\ \bottomrule
    \end{tabular}
    \caption{FID (10k samples) comparison between different DiffuseVAE conditioning schemes on CIFAR10.}
    \label{table:z_cond}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
    \centering
    \begin{tabular}{@{}cc@{}}
    \toprule
    \textbf{Method}             & \textbf{FID@10k}  \\ \midrule
    DiffuseVAE (Two-stage)     & \textbf{6.81}    \\
    DiffuseVAE (End-to-end) &  8.12   \\ \bottomrule
    \end{tabular}
    \caption{FID (10k samples) comparison between two-stage and end-to-end training on CIFAR10.}
    \label{table:e2e}
    \end{minipage}
    \end{table}
    
    \item \textbf{Two-stage training}: The choice of a two-stage training approach in DiffuseVAE is motivated by two reasons. Firstly, in our early experiments on CIFAR-10, we observed that the end-to-end model exhibited much worse performance than its two-stage counterpart during inference (See Table \ref{table:e2e}) where both models were trained for 400k steps. Secondly, from a computational standpoint, using a two-stage training formulation would be more amenable to training on limited compute resources as end-to-end training would require both models to fit in memory.
\end{enumerate}

\newpage
\section{Training and Hyperparameter details}
\label{sec:appendix_e}
\begin{table}[t]
\tiny
\centering
\begin{tabular}{@{}ccccccc@{}}
\toprule
 &  & \textbf{CIFAR-10} & \textbf{CelebA-64} & \textbf{CelebA-HQ-128} & \textbf{CelebA-HQ-256} & \textbf{LHQ-256}\\ \midrule
\multicolumn{7}{c}{\textbf{Stage-I VAE Hyperparameters}} \\ \midrule
\multirow{3}{*}{\textbf{Data}} & Resolution & 32 x 32 & 64 x 64 & 128 x 128 & 256 x 256 & 128 x 128 \\
 & Data Range & {[}0, 1{]} & {[}0, 1{]} & {[}0, 1{]} & {[}0, 1{]} & {[}0, 1{]}\\ \midrule
\multirow{2}{*}{\textbf{Model}} & Architecture & See Code & See Code & See Code & See Code & See Code \\
 & \# of parameters & 9.2M & 14M & 21.1M & 32.7M & 36.3M\\ \midrule
\multirow{8}{*}{\textbf{Training}} & Random Seed & 0 & 0 & 0 & 0 & 0\\
 & Mixed Precision & No & No & No & No & No \\
 & Effective Batch Size & 128 & 128 & 128 & 32 & 256 \\
 & \# of epochs & 500 & 250 & 500 & 500 & 500 \\
 & Optimizer & Adam(lr=1e-4) & Adam(lr=1e-4) & Adam(lr=1e-4) & Adam(lr=1e-4) & Adam(lr=1e-4) \\
 & Latent code size & 512 & 512 & 1024 & 1024 & 1024 \\
 & Ex-PDE & GMM(N=50) & GMM(N=75) & GMM(N=100) & GMM(N=100) & GMM(N=100) \\
 & KL-weight & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\ \midrule
\multicolumn{7}{c}{\textbf{Stage-II DDPM Hyperparameters}} \\ \midrule
\multirow{3}{*}{\textbf{Data}} & Resolution & 32 x 32 & 64 x 64 & 128 x 128 & 256 x 256 & 256 x 256 \\
 & Horizontal Flip & Yes & Yes & Yes & Yes & Yes \\
 & Data Range & {[}-1, 1{]} & {[}-1, 1{]} & {[}-1, 1{]} & {[}-1, 1{]} & {[}-1, 1{]} \\ \midrule
\multirow{9}{*}{\textbf{Model}} & \# of channels & 128 & 128 & 128 & 128 & 128 \\
 & Scale(s) of attention block & [16] & [16] & [16] & [16] & [16,8] \\
 & \# of attention heads & 8 & 8 & 8 & 8 & 8 \\
 & \# of residual blocks per scale & 2 & 2 & 2 & 2 & 2 \\
 & Channel multipliers & (1,2,2,2) & (1,2,2,2,4) & (1,2,2,3,4) & (1,1,2,2,4,4) & (1,1,2,2,4,4) \\
 & \# of parameters & 35.7M & 84.6M & 95.2M & 113M & 114M \\
 & Dropout & 0.3 & 0.1 & 0.1 & 0.1 & 0.1 \\
 & Noise Schedule (default) & Linear(1e-4, 0.02) & Linear(1e-4, 0.02) & Linear(1e-4, 0.02) & Linear(1e-4, 0.02) & Linear(1e-4, 0.02) \\
 & \# of time-steps (T) & 1000 & 1000 & 1000 & 1000 & 1000 \\ \midrule
\multirow{9}{*}{\textbf{Training}} & Random seed & 0 & 0 & 0 & 0 & 0 \\
 & Mixed Precision & No & No & No & No & No \\
 & EMA decay rate & 0.9999 & 0.9999 & 0.9999 & 0.9999 & 0.9999 \\
 & Effective batch size & 128 & 128 & 64 & 64 & 64 \\
 & \# of steps & 1.1M & 0.54M & 0.46M & 0.36M & 0.35M \\
 & Optimizer & Adam(lr=2e-4) & Adam(lr=2e-4) & Adam(lr=2e-5) & Adam(lr=2e-5) & Adam(lr=2e-5) \\
 & Grad. Clip Threshold & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
 & \# of lr annealing steps & 5000 & 5000 & 5000 & 5000 & 5000 \\
 & Diffusion loss type & Noise prediction (L2) & Noise prediction (L2) & Noise prediction (L2) & Noise prediction (L2) & Noise prediction (L2) \\ \midrule
\multirow{1}{*}{\textbf{Evaluation}} & Variance & fixedlarge & fixedlarge & fixedsmall & fixedsmall & fixedsmall\\ \bottomrule
\end{tabular}
\caption{Hyperparameters for the training setup in DiffuseVAE}
\label{table:hyp_table}
\end{table}
All hyperparameters details related to VAE and DDPM training in DiffuseVAE are listed in Table \ref{table:hyp_table}. Moreover, all hyperparameters (model and training) were shared between both DiffuseVAE formulations. \\\\
\noindent
\textbf{Data preprocessing}: During the first stage VAE training, all training data was normalized between [0.0, 1.0]. For the second stage DDPM training, the training data was scaled between [-1.0, 1.0] (including unconditional baselines and DiffuseVAE formulations). We also applied random horizontal flips as a form of data augmentation to the training images during the second stage DDPM training\\

\noindent
\textbf{Model architecture}: We use the same network architectures as explored in prior work in diffusion models \citep{ho2020denoising, dhariwal2021diffusion, nichol2021improved}. The VAE architecture used for Stage-1 training consists of residual block architectures inspired from \citep{child2021deep} (Refer to our code for exact architectural details). The VAE latent code size was set to 1024 for LHQ-256 and CelebA-HQ (both 128 and 256 resolution variants) and 512 for the CIFAR-10 and CelebA (64 x 64) datasets. We do not investigate the effect of the size of the latent code in this work. Similar to prior work \citep{ho2020denoising}, for all datasets except CIFAR-10 models used in SoTA comparisons, we use the U-Net \citep{ronneberger2015unet} decoder implementation from \citep{nichol2021improved} in the reverse process in Stage-II DDPM training. For the CIFAR-10 dataset, we used the U-Net decoder implementation from DDIM \citep{song2021denoising} (\url{https://github.com/ermongroup/ddim/blob/main/models/diffusion.py}). The U-Net decoder model hyperparameters are listed in Table \ref{table:hyp_table}.\\

\noindent
\textbf{Training and Inference}: Unless specified otherwise, we use the same hyperparameters during training as proposed in \citep{ho2020denoising}. All DDPM models were trained using the simplified objective proposed in \citep{ho2020denoising}. We used a mix of 4 Nvidia 1080Ti GPUs (44GB memory), a cloud TPUv2-8 (64GB memory) and a cloud TPUv3-8 (128GB memory) for training the models. Specifically, we used the GPU setup for training our CIFAR-10 and CelebA-64 models while we utilized the TPUv2-8 for training CelebA-HQ models at the 128 x 128 resolutions. Finally, we utilized the TPUv3-8 model for training on CelebA-HQ and LHQ models at 256 x 256 resolution.\\

\noindent
\textbf{Evaluation}: For FID \citep{heusel2018gans} score computation, we utilized 10k samples for the CelebA-HQ-128 dataset and 50k samples for state-of-the-art comparisons on the CIFAR-10 and the CelebA-64 datasets. For CelebA-HQ 256 comparisons we computed FID scores on 30k samples since the CelebA-HQ dataset contains 30k images. We used the \texttt{torch-fidelity} \citep{obukhov2020torchfidelity} package for FID and IS score computations. In this work, when saving samples to disk, we used standard denormalization (i.e. ) for all datasets. We used our GPU setup primarily for evaluation.

\newpage
\section{Additional Results}
\label{app:appendix_f}
\subsection{Generator-Refiner Framework}
Some additional qualitative results demonstrating the generator-refiner framework in VAEs are shown in Fig. \ref{fig:gr_add}. Table \ref{table:gr_add} further supports our qualitative results for several other benchmarks by comparing the FID scores between Stage-1 VAE generated samples and the corresponding final DiffuseVAE samples.

\subsection{Controllable synthesis}
\label{sec:app_controllable}
The directions for meaningful concepts (or image attributes like gender, age, hair style) are obtained by considering pairs of attribute negative and positive training samples. For each such pair, we compute the latent code representation for the positive and the negative sample and compute the difference between the attribute positive and the negative latent. We repeat this procedure for all such pairs and compute the average of the difference between the latent codes to obtain the direction vector for the attribute. Formally, given an attribute of interest  and the a set of tuples  of attribute positive and negative images, the latent direction  is given by:


where  denotes a mapping from the image to the latent space (the VAE encoder in this case). Given this latent direction, we can manipulate an attribute negative image by simply adding this vector to the latent code representation of the attribute negative image and decoding the resulting latent code representation as follows:

where  is the latent code representation of the atribute negative image,  is the new latent code containing the missing attribute and  is a scalar which controls the coarseness of the controllable generation (higher values usually result in more coarse generations).
In this work, we use the attribute annotations provided by the CelebAMask-HQ dataset \citep{CelebAMask-HQ} and a value of N=100 to construct the set of positive and negative samples for any attribute of interest. Additional controllable synthesis (including single attribute manipulation and composite manipulations) results for the CelebA-HQ dataset at the 128 x 128 resolution are shown in Fig. \ref{fig:controllable_add}. Figure \ref{fig:cs_debug} compares between composite edit-based samples generated from our first stage VAE and the corresponding refined samples generated from DiffuseVAE.


\begin{table}[t]
\footnotesize
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
             & \begin{tabular}[c]{@{}c@{}}CIFAR-10\\ (FID@50k)\end{tabular} & \begin{tabular}[c]{@{}c@{}}CelebA-64\\ (FID@50k)\end{tabular} & \begin{tabular}[c]{@{}c@{}}CelebA-HQ-256\\ (FID@10k)\end{tabular} \\ \midrule
Baseline VAE & 137.68   & 72.11     &   97.07            \\ 
DiffuseVAE (Form-1, Ex-PDE)   & \textbf{2.83}     & \textbf{4.05}      &     \textbf{11.28}          \\ \bottomrule
\end{tabular}
\caption{Quantitative comparison between sample quality of first stage VAEs in DiffuseVAE (Generator) and the final DiffuseVAE samples (Refiner)}
\label{table:gr_add}
\end{table}



\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{@{}ccccccccc@{}}
\toprule
       \multicolumn{1}{c|}{}                     & \multicolumn{4}{c|}{CelebA-64}                                   & \multicolumn{4}{c}{CIFAR-10}                                    \\ \midrule
                            & 10             & 25             & 50            & 100           & 10             & 25             & 50            & 100           \\\midrule
DDPM (uncond)               & 37.31          & 17.06          & 10.99         & 8.26          & 42.66          & \textbf{15.97} & \textbf{9.98} & \textbf{7.76} \\
DiffuseVAE (Form-1, Ex-PDE) & 26.09          & 14.16          & 9.58          & 7.54          & \textbf{34.19}          & 16.74          & 11.00         & 8.48          \\
DiffuseVAE (Form-2, Ex-PDE) & \textbf{25.79} & \textbf{13.89} & \textbf{9.09} & \textbf{7.15} & \textbf{34.22} & 17.36          & 11.00         & 8.28          \\ \bottomrule
\end{tabular}
\caption{Speed vs quality tradeoff comparison between DDPM and DiffuseVAE for the CIFAR-10 and CelebA-64 datasets. FID reported using 10k samples}
\label{table:svq_add_1}
\end{table}




\begin{table}[]
\footnotesize
\centering
\begin{tabular}{@{}ccccc@{}}
\toprule
                                 & 10             & 25            & 50            & 100           \\ \midrule
DDIM (uncond)                    & 15.19 & 8.00 & 6.76 & 6.24 \\
DiffuseVAE (DDIM, Form1, Ex-PDE) & \textbf{11.79}         & \textbf{7.44}         & \textbf{6.51}         & \textbf{6.14}          \\
DiffuseVAE (DDIM, Form2, Ex-PDE) & 12.15          & 7.63         & 6.62          & 6.22          \\ \bottomrule
\end{tabular}
\caption{Speed vs quality tradeoff comparison between DDIM and DiffuseVAE for the CIFAR-10 dataset. FID reported using 10k samples}
\label{table:svq_add_2}
\end{table}

\subsection{Speed vs quality tradeoffs}

\textbf{Reverse Process subsequence selection}: We use the \textit{linear} and \textit{quadratic} time-step selection as discussed in DDIM \citep{song2021denoising}, when running the reverse process for only a subsample of the time-steps for efficient sampling. We call this \textit{spaced} sampling. For benchmarking both DiffuseVAE and the baseline DDPM/DDIM models for the speed vs quality tradeoff, we selected the scheme which yielded lower FID values. Hence, we use the quadratic time-step schedule for all datasets when benchmarking DiffuseVAE while we used the quadratic schedule for the CIFAR-10 and the CelebA-64 datasets and linear schedule for the CelebA-HQ dataset when benchmarking the baseline DDPM/DDIM. There is also a possibility of using \textit{truncated} sampling in which only the last  time-steps are used for sampling. However, we found that the latter yielded inferior results than spaced sampling, so we do not report the FID scores for truncated sampling here.

\textbf{Additional results on speed vs quality tradeoff}:
Table \ref{table:svq_add_1} shows a speed vs quality tradeoff comparison between DiffuseVAE (with Ex-PDE) and the DDPM baseline for the CIFAR-10 and the CelebA-64 benchmarks. Both methods use the \textit{fixedsmall} variance type as discussed in \citep{ho2020denoising}. On the CelebA-64 dataset, DiffuseVAE again provides a much better speed vs quality tradeoff than a standard DDPM. However, on the CIFAR10 dataset, DiffuseVAE lags behind the standard DDPM (except at T=10) in terms of FID scores. This is surprising, since for T=1000, our DiffuseVAE model outperforms our baseline DDPM. However, when using DDIM sampling, DiffuseVAE outperforms the unconditional DDIM (See Table \ref{table:svq_add_2}).
For completeness, we also report the FID scores on 10k samples for our CelebA-HQ-256 DiffuseVAE (Form-1) model using DDIM sampling in Table \ref{table:chq256_ddim}


\begin{table}[]
\footnotesize
\centering
\begin{tabular}{@{}ccccc@{}}
\toprule
                                   & 10             & 25             & 50             & 100            \\ \midrule
DiffuseVAE (DDIM, Form-1)          & 26.07          & 19.75          & 18.90          & 18.85          \\
DiffuseVAE (DDIM, Form-1, GMM=100) & \textbf{24.09} & \textbf{17.47} & \textbf{16.65} & \textbf{16.63} \\ \bottomrule
\end{tabular}
\caption{FID scores (on 10k samples) for DiffuseVAE (Form-1) using DDIM sampling for the CelebA-HQ-256 dataset} 
\label{table:chq256_ddim}
\end{table}


\subsection{State-of-the-art Comparisons}
\label{sec:app_sota}
\textbf{Model size and Runtime comparison: LSGM and DiffuseVAE}: Here we compare model sizes between DiffuseVAE and LSGM \citep{vahdat2021scorebased}. Table \ref{table:model_compare} compares the model sizes between the LSGM and DiffuseVAE models on the CIFAR-10 and the CelebA-HQ-256 benchmarks. LSGM utilizes an order of magnitude larger VAE backbones and denoising decoders in comparison to DiffuseVAE. When computing the LSGM model size, we compute the size of the best FID model (See \url{https://github.com/NVlabs/LSGM}). To examine the performance gains when using larger models, we trained a DiffuseVAE (Form-1) model with an unchanged VAE baseline but with a larger DDPM decoder with around 73M parameters on CIFAR-10. Indeed, when using a larger model, DiffuseVAE with Ex-PDE achieves a FID of \textbf{2.62} and a mean IS of \textbf{9.75} on CIFAR-10 which shows that our model can take advantage of larger model sizes as well.

We further benchmarked DiffuseVAE and LSGM CIFAR-10 models in terms of the wall-clock time and memory required for sample generation on a batch size of 64 samples on a single Nvidia 1080Ti GPU. In terms of memory consumption, the LSGM model consumes 5.1GB in comparison to around 2.00GB consumed by DiffuseVAE. This is to be expected due to a larger LSGM model size. Interestingly, LSGM (using 140 NFEs) only takes 67.03s to generate a batch of 64 samples as compared to around 103.13s required by DiffuseVAE (using 1000 NFEs). We hypothesize that this gain is primarily due to the efficacy of applying diffusion in the latent space in LSGM as compared to the pixel-space in DiffuseVAE. However, this design choice also prevents access to a compact latent space in LSGM.

\begin{table}[t]
\centering
\footnotesize
\begin{tabular}{@{}ccc@{}}
\toprule
                               & CIFAR-10 & CelebA-HQ-256 \\ \midrule
LSGM (VAE backbone)            & 86.6M    & 50.9M         \\
LSGM (Denoising decoder)       & 375.6M   & 408.4M        \\
DiffuseVAE (VAE backbone)      & 9.2M     & 32.7M         \\
DiffuseVAE (Denoising decoder) & 35.7M    & 113.3M        \\ \bottomrule
\end{tabular}
\caption{Model size comparison (in terms of the number of parameters) between DiffuseVAE and LSGM on the CIFAR-10 and CelebA-HQ-256 benchmark}
\label{table:model_compare}
\end{table}

\subsection{Temperature Sampling in DiffuseVAE}:
We experiment with a temperature scaling technique where during the DDPM sampling stage in DiffuseVAE, we sample the initial DDPM latent  from a base Gaussian distribution with standard deviation scaled by . This is a common technique utilized in prior works \citep{vahdat2021nvae, Kingma2018GlowGF} to tradeoff between sample quality and diversity. Interestingly, we found that for CelebA-HQ-256 dataset, samples generated during intermediate training stages (and even after convergence) suffer from color bleeding as shown in Fig. \ref{fig:temp_chq256} (Top Row). We found that by applying temperature annealing in the second stage DDPM latents alleviates this problem (See Fig. \ref{fig:temp_chq256}(Bottom Row)). Therefore, we compute FID for state-of-the-art comparisons on this benchmark with a scaling factor of 0.8. We did not observe such color channel bleeding in samples of other benchmarks. In such cases, we observed that temperature scaling did not help and thus was not used to report FID scores.

\begin{figure*}[h]
  \centering
    \includegraphics[width=0.8\linewidth]{images/temp_chq256.pdf}
    \caption{Effect of temperature sampling in DDPM latents in DiffuseVAE. (Top Row) Samples generated with . (Bottom Row) Samples generated with }
    \label{fig:temp_chq256}
\end{figure*}

\subsection{DiffuseVAE Training Dynamics and Stability}

\begin{figure*}
  \centering
    \includegraphics[width=0.8\linewidth]{images/vae_training.pdf}
    \caption{Illustration of VAE training dynamics on the CIFAR-10 (Top Row) and the CelebA-HQ 256 dataset (Bottom Row) datasets. The columns from left to right represent the variation in KL loss, Reconstruction Loss and Total Loss during training respectively.}
    \label{fig:vae_training}
\end{figure*}
Although hierarchical VAEs \citep{vahdat2021nvae, child2021deep} generate significantly better samples than a standard VAE (with a single stochastic layer) \citep{kingma2014autoencoding}, the former can be unstable to train and often require carefully designed heuristics like spectral normalization, gradient clipping etc. However, even with these heuristics, stable training is not guaranteed. In contrast, standard VAEs often do not suffer from training instability issues. Indeed our empirical results in Figure \ref{fig:vae_training} suggest the same. Figure \ref{fig:vae_training} shows VAE training dynamics during training for the CIFAR-10 (Top Row) and the CelebA-HQ 256 (Bottom Row) datasets. As expected, the reconstruction loss (Middle column) and the total loss (Right column) for both the datasets decrease as training progresses. On the other hand, the KL loss increases for both the datasets early during training (Left column). This can be expected since during training, the VAE posterior  becomes more complex so as to obtain a better reconstruction loss. Therefore, the divergence between  and the prior  (in our case a standard gaussian) increases, leading to a higher KL Loss. Therefore, DiffuseVAE is more stable to train than the corresponding hierarchical VAE and GAN-based counterparts.

\subsection{Learning curve comparison between DiffuseVAE formulations}
Figure \ref{fig:fidvsepochs} shows comparison between the learning curves between DiffuseVAE formulations for the CIFAR-10 and CelebA-HQ 128 benchmarks. We used this analysis to assess model convergence. For the CIFAR-10 dataset, our models started to slightly overfit after 2000 epochs, so we utilize the corresponding checkpoint for all analysis. For the CelebA-HQ 128 dataset, we stopped training after exhaustion of our maximum compute budget of 1000 epochs and utilize the corresponding checkpoint for subsequent analysis and comparisons.
\begin{figure*}[h]
  \centering
    \includegraphics[width=0.6\linewidth]{images/fidvsepochs.pdf}
    \caption{Learning curve (FID vs epochs) comparison between DiffuseVAE formulations for the CIFAR-10 (Left) and the CelebA-HQ-128 dataset (Right). T=1000 during inference}
    \label{fig:fidvsepochs}
\end{figure*}

\subsection{Additional Samples}
To demonstrate generalization to more complex scenes, some qualitative samples generated using a DiffuseVAE model trained on the LHQ-256 dataset with T=1000 and a temperature scaling factor of 0.8 are shown in Figure \ref{fig:add_samples_lhq}.
Some additional samples from our CelebA-HQ model using DDIM sampling with 50 steps in the reverse process are shown in Figure \ref{fig:add_samples_1}. Figure \ref{fig:add_samples_2} shows some additional samples from the same model with T=1000 and a temperature scaling factor of 0.8. Lastly, Figure \ref{fig:add_samples_3} shows samples from the CelebAHQ-256 model but with shared latents in the DDPM stage (so effectively the generation is driven completely by low-dimensional latents from the VAE model).
\clearpage

\begin{figure*}
  \centering
    \includegraphics[width=1.0\linewidth]{images/gr_supp.pdf}
    \caption{Additional results demonstrating the generator-refiner framework in DiffuseVAE}
    \label{fig:gr_add}
\end{figure*}

\begin{figure*}
  \centering
    \includegraphics[width=1.0\linewidth]{images/cs.pdf}
    \caption{Additional results demonstrating controllable synthesis in the CelebA-HQ-128 dataset. \textcolor{green}{Green} boxes denote the vector addition operation while \textcolor{red}{Red} boxes denote the vector subtract operation}
    \label{fig:controllable_add}
\end{figure*}

\begin{figure*}
  \centering
    \includegraphics[width=1.0\linewidth]{images/controllable_debugging.pdf}
    \caption{Comparison between composite edit samples generated using the first stage VAE vs the corresponding refined samples generated by DiffuseVAE.}
    \label{fig:cs_debug}
\end{figure*}

\begin{figure*}[t]
  \centering
    \includegraphics[width=1.0\linewidth]{images/generalize_extended.pdf}
    \caption{Illustration of DiffuseVAE generalization to different noise types in the conditioning signal on CelebA-HQ-128.  denotes the standard deviation of the gaussian noise added to the conditioning signal. As noise becomes more severe, the output generated by DiffuseVAE becomes significantly worse. All final samples generated using T=100 with DDIM sampling.}
    \label{fig:generalize_extended}
\end{figure*}

\begin{figure*}
  \centering
    \includegraphics[width=1.0\linewidth]{images/lhq_qualitative.pdf}
    \caption{Samples generated from DiffuseVAE trained on the LHQ-256 dataset. T=1000 during sampling}
    \label{fig:add_samples_lhq}
\end{figure*}

\begin{figure*}
  \centering
    \includegraphics[width=1.0\linewidth]{images/chq_256_50steps.pdf}
    \caption{Additional samples from generated from DiffuseVAE trained on the CelebAHQ-256 dataset. T=50 using DDIM sampling.}
    \label{fig:add_samples_1}
\end{figure*}

\begin{figure*}
  \centering
    \includegraphics[width=1.0\linewidth]{images/chq_256_1000steps.pdf}
    \caption{Additional samples from generated from DiffuseVAE trained on the CelebAHQ-256 dataset (T=1000, Temp. Scaling factor was set to 0.8 during sampling).}
    \label{fig:add_samples_2}
\end{figure*}

\begin{figure*}
  \centering
    \includegraphics[width=1.0\linewidth]{images/chq_256_1000steps_shared.pdf}
    \caption{Additional samples from generated from DiffuseVAE trained on the CelebAHQ-256 dataset with the DDPM latents shared between samples. The generation is effectively driven by low-dimensional VAE latent space (T=1000, Temp. Scaling factor was set to 0.8 during sampling).}
    \label{fig:add_samples_3}
\end{figure*}
\end{document}
