\section{Experiment}









\subsection{Experimental Setup}
We perform experiments on two public benchmark datasets for temporal relation extraction: (1) \tbd~\cite{cassidy2014annotation}, which is a densely annotated dataset with 6 types of relations: \textit{Before}, \textit{After}, \textit{Simultaneous}, \textit{Includes}, \textit{Is\_included} and \textit{Vague}. (2) \matres~\cite{NingWuRo18}, which annotates verb event mentions along with 4 types of temporal relations: \textit{Before}, \textit{After}, \textit{Simultaneous} and \textit{Vague}. Additionally, we use POS tag information from~\matres{} provided by~\cite{ning2019improved}. For~\tbd, we use spacy annotation for predicting POS tag information which is based on Universal POS tag set\footnote{\url{https://spacy.io/api/data-formats}}.  For both benchmark datasets, we use the same train/dev/test splits as previous studies~\cite{ning2019improved,ning2019structured,han2019deep,han2019joint}. Note that, for evaluation, similar as previous work, we disregard the \textit{Vague} relation from both datasets (in the evaluation phase, we simply remove all ground truth \textit{Vague} relation pairs).  In addition, we will only consider event pairs from adjacent sentences due to the fact that it will require an exponential number of annotations if we also consider event pairs from non-adjacent sentences, which is beyond the scope of this study. Table~\ref{tab:corpora stats} shows statistics of the two datasets and Table~\ref{tab:label_distribution} shows the label distribution. 

\begin{table}[h]
\small
\centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{Corpora} & Train & Dev & Test \\ 
\midrule
\multicolumn{1}{c|}{\multirow{2}{*}{\tbd}} & \multicolumn{1}{c|}{\# Documents} & \multicolumn{1}{c|}{22} & \multicolumn{1}{c|}{5} & 9 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\# Relation Pairs} & \multicolumn{1}{c|}{4,032} & \multicolumn{1}{c|}{629} & 1,427 \\ \midrule \multicolumn{1}{c|}{\multirow{2}{*}{\matres}} & \multicolumn{1}{c|}{\# Documents} & \multicolumn{1}{c|}{255} & \multicolumn{1}{c|}{20} & 25 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\# Relation Pairs} & \multicolumn{1}{c|}{13K} & \multicolumn{1}{c|}{2.6K} & 837 \\ 
\toprule
\end{tabular}
\caption{Data statistics for \tbd~and \matres}
\label{tab:corpora stats}
\end{table}

\begin{table}[h]
\small
\centering
\begin{tabular}{l|r|r|r|r}
\toprule
Labels & \multicolumn{2}{c|}{\tbd} & \multicolumn{2}{c}{\matres} \\ \midrule Before & 384 & 26.9\% & 417 & 49.8\% \\
After & 274 & 19.2\% & 266 & 31.8\% \\
Includes & 56 & 3.9\% & - & - \\
Is\_Included & 53 & 3.7\% & - & - \\
Simultaneous & 22 & 1.5\% & 31 & 3.7\% \\
Vague & 638 & 44.7\% & 133 & 15.9\% \\ 
\toprule
\end{tabular}
\caption{Label distribution for \tbd~and \matres. For each dataset, the first column shows the number of instances of each relation type while the second column shows the percentage.}
\label{tab:label_distribution}
\end{table}

\paragraph{Implementation Details} For fair comparisons with previous baseline approaches, we use the pre-trained bert-large-cased model\footnote{\url{https://huggingface.co/transformers/pretrained_models.html}} for fine-tuning and optimize our model with BertAdam. We optimize the parameters with grid search: training epoch 10, learning rate $\in \{3e\text{-}6,1e\text{-}5\}$, training batch size $\in\{16, 32\}$, encoder layer size $\in\{4, 12\}$, number of heads $\in\{1, 8\}$. During training, we first optimize the event extraction module for 5 epochs to warm up, and then jointly optimize both event extraction and temporal relation extraction modules using gold event pairs for another 5 epochs.


























































\subsection{Results}




\begin{table*}[!tp]
\small
\centering
\scalebox{1}{
\begin{tabular}[width=\linewidth]{c|p{4.7cm}<{\centering}|c|c|c}
\toprule
Dataset & Model & Pre-trained Model & Event Detection & Relation Extraction \\ \midrule
\multirow{2}{*}{\tbd} & HNP19~\cite{han2019joint} & BERT Base &  90.9 & 49.4 \\  & Our Approach & BERT Base &  \textbf{91.0} & \textbf{51.8} \\ \midrule
\multirow{3}{*}{\matres} & CogCompTime2.0~\cite{ning2019improved} & BERT Base & 85.2 & 52.8 \\  & HNP19~\cite{han2019joint} & BERT Base &  87.8 & 59.6 \\ \  & Our Approach & BERT Base &  \textbf{90.5} & \textbf{62.3} \\ \toprule
\end{tabular}
}
\caption{Comparison of various approaches on joint event and relation extraction with F-score (\%). Note that HPN19 fixes BERT embeddings but relies on BiLSTM to capture the contextual features.}



\label{tab:joint score}  
\end{table*}

\begin{table*}[t]
\small
\centering
\scalebox{1}{
\begin{tabular}{c|c|c|c}
\toprule
\multicolumn{1}{c|}{Dataset} & Model & Pre-trained Model & Relation Classification (F-score \%) \\ \midrule
\multirow{6}{*}{\tbd} 
& LSTM~\cite{cheng2017classifying} & BERT Base & 62.2 \\
& HNP19~\cite{han2019joint} & BERT Base & 64.5 \\
& Our Approach & BERT Base & \textbf{66.7} \\
\cmidrule{2-4}
 & PSL~\cite{zhou2020clinical} & RoBERTa Large & 65.2 \\
 & DEER~\cite{han2020deer} & RoBERTa Large & 66.8 \\
 & Our Approach & BERT Large & \textbf{67.1} \\ \midrule
\multirow{7}{*}{\matres} & CogCompTime2.0~\cite{ning2019improved} & BERT Base & 71.4 \\
& LSTM~\cite{cheng2017classifying} & BERT Base & 73.4 \\
 & HNP19~\cite{han2019joint} & BERT Base & 75.5 \\
 & Our Approach & BERT Base & \textbf{79.3} \\
\cmidrule{2-4}
& HMHD20~\cite{wang2020joint} & RoBERTa Large & 78.8 \\
 & DEER~\cite{han2020deer} & RoBERTa Large & 79.3 \\
 & Our Approach & BERT Large & \textbf{80.3} \\ \toprule
\end{tabular}
}
\caption{Comparison of various approaches on temporal relation classification with gold event mentions as input.}
\label{tab:gold event mention}
\end{table*}

We 
evaluate \this{} against two public benchmark datasets under two settings: (1) joint event and temporal relation extraction (Table~\ref{tab:joint score}); (2) temporal relation classification, where the gold event mentions are known beforehand (Table~\ref{tab:gold event mention}). Note in the ``joint'' setting, we adopt the same strategy proposed in \cite{han2019joint}: we first train the event extraction module, and then jointly optimize both event extraction and temporal relation extraction (using gold event pairs as input to ensure training quality) modules.
Overall, we observe that our approach significantly outperforms baseline systems in both settings, with up to 7.9\% absolute F-score gain on~\matres{} and 2.4\% on~\tbd{}. 

From Table~\ref{tab:joint score}, we see that our approach achieves better performance on event detection than baseline methods though they are based on the same BERT encoder. This is possibly because, during joint training, our approach leverages the dependency parsing trees, which improves the contextual representations of the BERT encoder.
In Table~\ref{tab:gold event mention}, unlike other models which are based on larger contextualized embeddings such as RoBERTa, our approach with BERT base achieves comparable performance, and further surpasses the state-of-the-art baseline methods using BERT-large embeddings, which demonstrate the effectiveness of our Syntax-guided Graph Transformer network.

Some studies~\cite{ning2019improved,han2019joint,wang2020joint,zhou2020clinical} focus on resolving the inconsistency in terms of the symmetry and transitivity of the temporal relations. For example, if event A and event B are predicted as \textit{Before}, event B and event C are predicted as \textit{Before}, then if event A and event C are predicted as \textit{Vague} or \textit{After}, it will be considered as inconsistent. However, our approach shows consistent predictions with few inconsistent cases when \textit{Simultaneous} relation is involved. This analysis also demonstrates that our approach can correctly capture the temporal cues between two event mentions. 



\begin{table}[tp]
\small
\centering
\scalebox{0.8}{
\begin{tabular}{p{2.8cm}|p{1.6cm}<{\centering}|p{1.6cm}<{\centering}|p{1.6cm}<{\centering}}
\toprule
Model &  Original Test & Contrast & Consistency \\
\midrule
CogCompTime2.0 \cite{ning2019improved} & 73.2 & 63.3  & 40.6 \\
\midrule
Our Approach & \textbf{77.0} & \textbf{64.8} & \textbf{49.8} \\
\toprule
\end{tabular}}
\caption{Evaluation on the contrast set of~\matres. Original Test indicates the accuracy on 100 examples sampled from the original~\matres~test set following~\cite{Gardner2020Evaluating}. Contrast shows the accuracy score on 401 examples perturbed from the original 100 examples. Consistency is defined as the percentage of the original 100 examples for which the model's predictions of the perturbed examples are all correct in the contrast set.}  
\label{tab:contrast}
\end{table}


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth,keepaspectratio]{Figure/comparison.pdf}
    \caption{Comparison of the predictions from BERT, BERT-GT and our approach.}
\label{fig:comparison}
\end{figure*}

We also examine the correctness and robustness of our approach on a contrast set of \matres~\cite{Gardner2020Evaluating}, which is created with small manual perturbation based on the original test set of \matres~in a meaningful way, such as rephrasing the sentence or simply changing a word of the sentence to alter the relation type. The contrast set provides a local view of a model’s decision boundary, thus it can be used to more accurately evaluate a model’s true linguistic capabilities.
Table~\ref{tab:contrast} shows that our approach significantly outperforms the baseline model on both the original test set and the corresponding contrast set. 
The contrast consistency in Table~\ref{tab:contrast} also indicates how well a model's decision boundary aligns with the actual decision boundary of the test instances, based on which we can see that by explicitly capturing temporal cues, our approach is more accurate and robust than the baseline method.






\paragraph{Ablation Study}
We further conduct ablation studies to compare the performance of our approach with two ablated versions of our method: (1) BERT with Graph Transformer (BERT-GT), for which we remove the syntaxic-guided attention and only rely on the standard multi-head self-attention to obtain graph-based contextual representations of two event mentions and then predict their relation; (2) BERT, where we further remove the Graph Transformer, and only use the pre-trained BERT language model to encode the sentence and predict the temporal relationship of two event mentions based on their contextual representations.






\begin{table}[h]
\small
\centering
\begin{tabular}{l|c|c}
\toprule
Ablation & F-score (\%) & Gain (\%) \\ \midrule BERT-SGT & 79.3 & 0 \\ BERT-GT & 77.5 & -2.0 \\ BERT & 75.5 & -3.8\\ \toprule
\end{tabular}
\caption{Ablation study on \matres. We use BERT base as the comparison basis.}
\label{tab:ablation study}
\end{table}


Table~\ref{tab:ablation study} also shows that by adding Graph Transformer, BERT-GT achieves 2.0\% absolute F-score improvement over the BERT baseline model, demonstrating the benefit of dependency parsing trees to temporal relation prediction. By further adding the new syntax-guided attention into Graph Transformer, the absolute improvement on F-score (1.8\%) shows the effectiveness of our new Syntax-guided Graph Transformer and the importance of capturing temporal cues from the connection of two event mentions as well as their surround contexts.












Figure~\ref{fig:comparison} shows two examples as qualitative analysis. In S1, BERT mistakenly predicts the temporal relation as \textit{Before} probably because it's confused by the context word \textit{Before}. However, by incorporating the dependency graph, especially the triples \{\textit{worked}, \textit{prep}, \textit{Before}\}, \{\textit{Before}, \textit{pcomp}, \textit{retiring}\} and the path between the two event mentions, \textit{worked}$\rightarrow$\textit{prep}$\rightarrow$\textit{Before}$\rightarrow$\textit{pcomp}$\rightarrow$\textit{retiring}, both BERT-GT and our approach correctly determine the relation as \textit{After}. In S2, both BERT and BERT-GT mistakenly predict the temporal relation as \textit{Before} as the context between the two event mentions is very wide and complicated, and these two event mentions are not close within the dependency graph. However, by explicitly considering and understanding the connection between the two event mentions, $\textit{sought}_{e_1}$$\rightarrow$\textit{on}$\rightarrow$\textit{Marmara}$\rightarrow$\textit{was}$\rightarrow$\textit{part}$\rightarrow$\textit{Flotilla}
$\rightarrow$\textit{sought}$_{e_2}$, our approach correctly predicts the temporal relation between the two event mentions.







\subsection{SGT on Temporal Cues}


To analyze the source of temporal cues for relation prediction, we randomly sample 100 correct event relation predictions given gold event mentions from~\matres~and select the triple that has the highest temporal attention weight from the last layer of the Syntax-guided Graph Transformer network as a temporal cue candidate. We manually evaluate the validity of each temporal cue candidate, and further analyze if the cue is from the dependency path between two event mentions, their surrounding context, or both. Our analysis shows that about 64\% of the temporal cues are valid, 37\% of them come from the dependency path, 17\% are from local context, and the remaining 10\% are from both. This verifies our initial observation that most of the temporal cues are from the dependency path between two event mentions as well as their surrounding context. It also demonstrates the effectiveness of our new syntax-guided attention mechanism. 











\subsection{Impact of Wide Context}




We further illustrate the impact of context width to both baseline model and our approach. For fair comparison, we use three context width category, [context length $<$ 10, 10 $<$ context length $<$ 20, context length $>$ 20 ]. As we can see in Fig.~\ref{fig:context width}, the first category has 267 pairs, the second category has 343 pairs and the third category has 817 pairs. From our results, we observe that the BERT baseline cannot predict the temporal relation of two event mentions with wide context but rather working well when the event mentions are close to each other. Our model overall performs slightly worse in the second category but in general is very good at predicting the temporal relationship for the event mentions with short and context width. This also proves the benefit of syntactic parsing trees to the prediction of temporal relationship. For the second category where the context length is within [10, 20], the performance of our approach slightly drops due to two reasons: (1) the training samples within this range are not as sufficient as the other two categories; (2) for most event pairs from this category, their dependency path is very
long and there is no explicit temporal indicative features within their context or dependency path, making it more difficult for the model to predict their temporal relationship.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth, height=8cm, keepaspectratio]{Figure/context-width-draft.png}
\caption{Context width analysis on \tbd. The X axis shows the number of tokens between two events mentions. The left Y axis shows the data distribution of each width category indicating with blue bars. The right Y axis denotes the micro F-score for each width category.}\label{fig:context width}
\end{figure}

\begin{figure*}[]
    \centering
    \includegraphics[width=\textwidth]{Figure/errors_final.pdf}
\caption{Types of remaining errors}
\label{fig:errors}
\end{figure*}




\subsection{Remaining Errors}

We randomly sample 100 classification errors from the output of our approach and categorize them into four categories. As Figure~\ref{fig:errors} shows, the first category is due to the complex or ambiguous context (54\% of the total errors). The second category is due to the complicated subordinate clause structure, especially the clauses that are related to quote or reported speech, e.g., S2 in Figure~\ref{fig:errors}. The third error category is that our approach cannot correctly differentiate the actual events from the hypothetical and intentional events, while in most cases, the temporal relation among hypothetical and intentional events is annotated as \textit{Vague}. The last category is due to the lack of sufficient annotation. We observe that none of the \textit{Simultaneous} relation can be correctly predicted for \matres~dataset as the percentage of \textit{Simultaneous} (3.7\%) is much lower than other relation types. In \tbd~dataset, labels are even more imbalanced as the percentage of \textit{Vague} relation is over 50\% while the percentage of \textit{Includes}, \textit{Is\_Included} and \textit{Simultaneous} are all less than 4\%.  













%
