\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{emnlp2021}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}





\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{array}
\usepackage{subcaption}
\usepackage{enumitem}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\usepackage{microtype}


\graphicspath{ {figures/} }

\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\louis}[1]{{\color{violet}\textbf{louis}:#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\angela}[1]{{\color{blue}\textbf{angela}: #1}}

\newcommand{\ccnet}{CCNet\xspace}
\newcommand{\wikilarge}{WikiLarge\xspace}
\newcommand{\asset}{ASSET\xspace}
\newcommand{\newsela}{Newsela\xspace}
\newcommand{\alector}{ALECTOR\xspace}
\newcommand{\simplextcorpus}{SIMPLEXT Corpus\xspace}
\newcommand{\turkcorpus}{TurkCorpus\xspace}
\newcommand{\turkcorpusabbr}{TurkCor.\xspace}

\newcommand{\muss}{\textsc{MUSS}\xspace}
\newcommand{\bart}{\textsc{BART}\xspace}
\newcommand{\mbart}{\textsc{mBART}\xspace}
\newcommand{\access}{\mbox{\textsc{ACCESS}}\xspace}
\newcommand{\bartaccess}{\textsc{BART}+\textsc{ACCESS}\xspace}
\newcommand{\mbartaccess}{\textsc{mBART}+\textsc{ACCESS}\xspace}
\newcommand{\laser}{LASER\xspace}

\newcommand{\mined}{\textsc{Mined}\xspace}
\newcommand{\minedwikilarge}{\textsc{Mined}+\textsc{WikiLarge}\xspace}
\newcommand{\minedparaphrases}{Para.}
\newcommand{\minedsimplifications}{Simpl.}

\newcommand{\numem}[2]{}
\newcommand{\tnumem}[2]{&\numem{}{#2}}

\title{MUSS: Multilingual Unsupervised Sentence Simplification\\ by Mining Paraphrases}
\author{Louis Martin\quad Angela Fan\\ \large\textbf{\'Eric de la Clergerie\quad Antoine Bordes\quad Beno\^it Sagot}\\
  Facebook AI Research, Paris, France \\
  Inria, Paris, France \\
  LORIA, Nancy, France\\
  \texttt{\{louismartin, angelafan, abordes\}@fb.com}\\
  \texttt{\{eric.de\_la\_clergerie, benoit.sagot\}@inria.fr}}
\pdfinfo{
/Title (MUSS: Multilingual Unsupervised Sentence Simplification)
/Author (Louis Martin, Angela Fan, Eric de la Clergerie, Antoine Bordes, Benoit Sagot)
/TemplateVersion (2021.2)
} 


\begin{document}
\maketitle
\begin{abstract}
Progress in sentence simplification has been hindered by a lack of labeled  parallel simplification data, particularly in languages other than English.
We introduce \muss, a Multilingual Unsupervised Sentence Simplification system that does not require labeled simplification data.
\muss uses a novel approach to sentence simplification that trains strong models using sentence-level paraphrase data  instead of proper simplification data.
These models leverage unsupervised pretraining and controllable generation mechanisms to flexibly adjust attributes such as length and lexical complexity at inference time.
We further present a method to mine such paraphrase data in any language from Common Crawl using semantic sentence embeddings, thus removing the need for labeled data.
We evaluate our approach on English, French, and Spanish simplification benchmarks and closely match or outperform the previous best supervised results, despite not using any labeled simplification data.
We push the state of the art further by incorporating labeled simplification data.
\end{abstract} \section{Introduction}

Sentence simplification is the task of making a sentence easier to read and understand by reducing its lexical and syntactic complexity, while retaining most of its original meaning. Simplification has a variety of important societal applications, for example increasing accessibility for those with cognitive disabilities such as aphasia \cite{carroll1998practical}, dyslexia \cite{rello2013simplify}, and autism \cite{evans2014evaluation}, or for non-native speakers \cite{paetzold2016unsupervised}. Research has mostly focused on English simplification, where source texts and  associated simplified texts exist and can be automatically aligned, such as English Wikipedia and Simple English Wikipedia \cite{zhang2017sentence}. However, such data is limited in terms of size and domain, and difficult to find in other languages.
Additionally, simplifying a sentence can be achieved in multiple ways, and depend on the target audience. Simplification guidelines are not uniquely defined, outlined by the stark differences in English simplification benchmarks \cite{alva2020asset}.
This highlights the need for more general models that can adjust to different simplification contexts and scenarios.


In this paper, we propose to train controllable models using sentence-level paraphrase data only, i.e. parallel sentences that have the same meaning but phrased differently.
In order to generate simplifications and not paraphrases, we use \access \cite{martin2020controllable} to control attributes such as length, lexical and syntactic complexity.
Paraphrase data is more readily available, and opens the door to training flexible models that can adjust to more varied simplification scenarios.
We propose to gather such paraphrase data in any language by mining sentences from Common Crawl using semantic sentence embeddings.
We show that simplification models trained on mined paraphrase data perform as well as models trained on existing large paraphrase corpora (cf.~Appendix~\ref{section:comparison_with_paraphrase_dataset}). Moreover, paraphrases  are  more straightforward to mine than simplifications, and we show that they lead to models with better performance than equivalent models trained on mined simplifications (cf.~Section~\ref{subsection:ablations}).

Our resulting Multilingual Unsupervised Sentence Simplification method, \muss, is \textit{unsupervised} because it can be trained without relying on {\em labeled} simplification data,\footnote{We use the term {\em labeled simplifications} to refer to parallel datasets where texts were manually simplified by humans.} even though we mine using supervised sentence embeddings.\footnote{Previous works have also used the term {\em unsupervised simplification} to describe works that do not use any labeled parallel simplification data while leveraging supervised components such as constituency parsers and knowledge bases \citep{kumar-etal-2020-iterative}, external synonymy lexicons \cite{surya2018unsupervised}, and databases of simplified synonyms \cite{Zhao2020SemiSupervisedTS}. We shall come back to these works in Section~\ref{sec:related_work}.}
We additionally incorporate unsupervised pretraining  \cite{liu2019unsupervised,liu2020multilingual} and apply \muss on English, French, and Spanish to closely match or outperform the supervised state of the art in all languages. \muss further improves the state of the art on all English datasets by incorporating additional labeled simplification data.


To sum up, our contributions are as follows:
\begin{itemize}
    \item We introduce a novel approach to training simplification models with paraphrase data only and propose a mining procedure to create large paraphrase corpora for any language.
    \item Our approach obtains strong performance. Without any labeled simplification data, we match or outperform the supervised state of the art in English, French and Spanish. We further improve the English state of the art by incorporating labeled simplification data.
    \item We release \muss pretrained models, paraphrase data, and code for mining and training\footnote{\url{https://github.com/facebookresearch/muss}}.
\end{itemize}
 \section{Related work}\label{sec:related_work}


Data-driven methods have been predominant in {\bf English sentence simplification} in  recent years \cite{alva2020data}, requiring large supervised training corpora of complex-simple aligned sentences \cite{wubben2012sentence,xu2016optimizing,zhang2017sentence,zhao2018integrating,martin2020controllable}.
Methods have relied on English and Simple English Wikipedia with automatic sentence alignment from similar articles \cite{zhu2010monolingual,coster2011learning,woodsend2011learning,kauchak2013improving,zhang2017sentence}. Higher quality datasets have been proposed such as the \newsela corpus \cite{xu2015problems}, but they are rare and come with restrictive licenses that hinder reproducibility and widespread usage.

{\bf Simplification in other languages} has been explored in Brazilian Portuguese \cite{aluisio2008towards}, Spanish \cite{saggion2015making,vstajner2015automatic}, Italian \cite{brunato2015design,tonelli2016simpitiki}, Japanese \cite{goto2015japanese,kajiwara2018text,katsuta2019improving}, and French \cite{gala2020alector}, but the lack of a large labeled parallel corpora has slowed research down.
In this work, we show that a method trained on automatically mined corpora can reach state-of-the-art results in each language.

When labeled parallel simplification data is unavailable, systems rely on {\bf unsupervised simplification} techniques, often inspired from machine translation.
The prevailing approach is to split a monolingual corpora into disjoint sets of complex and simple sentences using readability metrics. Then simplification models can be trained by using automatic sentence alignments \cite{kajiwara-komachi-2016-building,kajiwara2018text}, auto-encoders \cite{surya2018unsupervised,Zhao2020SemiSupervisedTS}, unsupervised statistical machine translation \cite{katsuta2019improving}, or back-translation \cite{aprosio2019neural}.
Other unsupervised simplification approaches iteratively edit the sentence until a certain simplicity criterion is reached \cite{kumar-etal-2020-iterative}.
The performance of unsupervised methods are often below their supervised counterparts.
\muss bridges the gap with supervised method and removes the need for deciding in advance how complex and simple sentences should be separated, but instead trains directly on paraphrases mined from the raw corpora.

Previous work on {\bf parallel dataset mining} have been used mostly in machine translation using document retrieval \cite{munteanu-marcu-2005-improving}, language models \cite{koehn-etal-2018-findings,koehn2019findings},
and embedding space alignment \cite{artetxe2019massively} to create large corpora \cite{tiedemann2012parallel,schwenk2019ccmatrix}.
We focus on paraphrasing for sentence simplifications, which presents new challenges. Unlike machine translation, where the same sentence should be identified in two languages, we develop a method to identify varied paraphrases of sentences, that have a wider array of surface forms, including different lengths, multiple sentences, different vocabulary usage, and removal of content from more complex sentences. 

Previous {\bf unsupervised paraphrasing} research has aligned sentences from various parallel corpora \cite{barzilay2003learning} with multiple objective functions \cite{liu2019unsupervised}.
Bilingual pivoting relied on MT datasets to create large databases of word-level paraphrases \cite{pavlick2015ppdb}, lexical simplifications \cite{pavlick2016simple,kriz-etal-2018-simplification}, or sentence-level paraphrase corpora \cite{wieting-gimpel-2018-paranmt}.
This has not been applied to multiple languages or to sentence-level simplification. Additionally, we use raw monolingual data to create our paraphrase corpora instead of relying on parallel MT datasets.

 \section{Method}

In this section we describe \muss, our approach to mining paraphrase data and training controllable simplification models on paraphrases.




\begin{table}
\centering\small
\resizebox{\columnwidth}{!}{
\begin{tabular}{llrr}
\toprule
 & \multicolumn{1}{c}{\textbf{Type}} & \multicolumn{1}{c}{\textbf{\# Sequence}} & \multicolumn{1}{c}{\textbf{\# Avg. Tokens}} \\
 &  & \multicolumn{1}{c}{\textbf{Pairs}} & \multicolumn{1}{c}{\textbf{per Sequence}}\\ \midrule
 \textbf{\wikilarge} & Labeled Parallel & 296,402 & original: 21.7 \\
 \bf (English)& Simplifications&& simple: 16.0 \\
 \textbf{\newsela} & Labeled Parallel & 94,206 & original: 23.4 \\
 \bf (English)& Simplifications&& simple: 14.2 \\
 \midrule
\textbf{English} & Mined & 1,194,945 & 22.3 \\
\textbf{French} & Mined & 1,360,422 & 18.7 \\
\textbf{Spanish} & Mined & 996,609 & 22.8 \\
\bottomrule
\end{tabular}
}
\caption{\label{table:data_stats} Statistics on our mined paraphrase training corpora compared to standard simplification datasets (see section \ref{subsection:training_data} for more details).}
\end{table}


\subsection{Mining Paraphrases in Many Languages}




\paragraph{Sequence Extraction}
Simplification consists of multiple rewriting operations, some of which span  multiple sentences (e.g.~sentence splitting or fusion).
To allow such operations to be represented in our paraphrase data, we extract chunks of texts composed of multiple sentences, we refer to these small pieces of text by \emph{sequences}.

We extract such sequences by first tokenizing a document into individual sentences  using the NLTK sentence tokenizer \cite{loper2002nltk}.
We then extract sequences of adjacent sentences with maximum length of 300 characters: e.g. .
We can thus align two sequences that contain a different number of sentences, and represent sentence splitting or sentence fusion operations.

These sequences are further filtered to remove noisy sequences with more than 10\% punctuation characters and sequences with low language model probability  according to a 3-gram Kneser-Ney language model trained with \texttt{kenlm} \cite{heafield2011kenlm} on Wikipedia.

We extract these sequences from \ccnet \cite{wenzek2019ccnet}, an extraction of Common Crawl (an open source snapshot of the web) that has been split into different languages using \texttt{fasttext} language identification \cite{joulin2017bag} and various language modeling filtering techniques to identify high quality, clean sentences. For English and French, we extract 1 billion sequences from \ccnet. For Spanish we extract 650 millions sequences, the maximum for this language in \ccnet after filtering out noisy text.

\paragraph{Creating a Sequence Index Using Embeddings}
To automatically mine our paraphrase corpora, we first compute -dimensional embeddings for each extracted sequence using \laser \cite{artetxe2019massively}. \laser provides joint multilingual sentence embeddings in 93 languages that have been successfully applied to the task of bilingual bitext mining \cite{schwenk2019ccmatrix}. In this work, we show that \laser can also be used to mine monolingual paraphrase datasets.



\paragraph{Mining Paraphrases}

After computing the embeddings for each language, we index them for fast nearest neighbor search using \texttt{faiss}. 

Each of these sequences is then used as a query  against the billion-scale index that returns a set of top-8 nearest neighbor sequences according to the semantic \laser embedding space using L2 distance, resulting in a set of candidate paraphrases are .

We then use an upper bound on L2 distance and a margin criterion following \cite{artetxe-schwenk-2019-margin} to filter out sequences with low similarity.
We refer the reader to Appendix Section~\ref{sec:mining_details} for technical details.

The resulting nearest neighbors constitute a set of aligned paraphrases of the query sequence: .
We finally apply poor alignment filters. We remove sequences that are almost identical with character-level Levenshtein distance  20\%, when they are contained in one another, or when they were extracted from two overlapping sliding windows of the same original document.

We report statistics of the mined corpora in English, French and Spanish in Table~\ref{table:data_stats}, and qualitative examples of the resulting mined paraphrases in Appendix Table~\ref{table:mining_examples}.
Models trained on the resulting mined paraphrases obtain similar performance than models trained on existing paraphrase datasets (cf. Appendix Section~\ref{section:comparison_with_paraphrase_dataset}).



\subsection{Simplifying with \access} \label{subsection:access}
In this section we describe how we adapt \access \cite{martin2020controllable} to train controllable models on mined paraphrases, instead of labeled parallel simplifications.

\access is a method to make any seq2seq model controllable by conditioning on simplification-specific control tokens.
We apply it on our seq2seq pretrained transformer models based on the \bart \cite{lewis2019bart} architecture (see next subsection).

\paragraph{Training with Control Tokens} At training time, the model is provided with control tokens that give oracle information on the target sequence, such as the amount of compression of the target sequence relative to the source sequence (length control). For example, when the target sequence is 80\% of the length of the original sequence, we provide the NumChars\_80\% control token. At inference time we can then control the generation by selecting a given target control value.
We adapt the original Levenshtein similarity control to only consider replace operations but otherwise use the same controls as \citet{martin2020controllable}.
The controls used are therefore character length ratio, \textit{replace-only} Levenshtein similarity, aggregated word frequency ratio, and dependency tree depth ratio.
For instance we will prepend to every source in the training set the following 4 control tokens with sample-specific values, so that the model learns to rely on them: NumChars\_XX\%
LevSim\_YY\% WordFreq\_ZZ\% DepTreeDepth\_TT\%.
We refer the reader to the original paper \cite{martin2020controllable} and Appendix~\ref{subsection:training_details} for details on \access and how those control tokens are computed.

\paragraph{Selecting Control Values at Inference}
Once the model has been trained with oracle controls, we can adjust the control tokens to obtain the desired type of simplifications.
Indeed, sentence simplification often depends on the context and target audience \cite{martin2020controllable}.
For instance shorter sentences are more adapted to people with cognitive disabilities, while using more frequent words are useful to second language learners.
It is therefore important that supervised and unsupervised simplification systems can be adapted to different conditions: \citep{kumar-etal-2020-iterative} do so by choosing a set of operation-specific weights of their unsupervised simplification model for each evaluation dataset, \citep{surya2018unsupervised} select different models using SARI on each validation set.
Similarly, we set the 4 control hyper-parameters of \access using SARI on each validation set and keep them fixed for all samples in the test set.\footnote{Details in Appendix~\ref{subsection:training_details}}.
These 4 control hyper-parameters are intuitive and easy to interpret: when no validation set is available, they can also be set using prior knowledge on the task and still lead to solid performance (cf. Appendix~\ref{section:prior_knowledge_values}).









 
\subsection{Leveraging Unsupervised Pretraining}
We combine our controllable models with unsupervised pretraining to further extend our approach to text simplification. For English, we finetune the pretrained generative model \bart \cite{lewis2019bart} on our newly created training corpora. \bart is a pretrained sequence-to-sequence model that can be seen as a generalization of other recent pretrained models such as BERT \cite{devlin2018bert}.
For non-English, we use its multilingual generalization \mbart \cite{liu2020multilingual}, which was pretrained on 25 languages.
\iffalse We show in Section~\ref{sub@subfigure:ablation_bart_and_access} that \bart and \access work particularly well when used together.\fi{} \section{Experimental Setting}
We assess the performance of our approach on three languages: English, French, and Spanish.
We detail our experimental procedure for mining and training in Appendix Section~\ref{sec:experimental_details}.
In all our experiments, we report scores on the test sets averaged over 5 random seeds with 95\% confidence intervals.

\begin{table*}[!htbp]
\centering\small
\begin{tabular}{l|r@{}lr@{}l|r@{}lr@{}l|r@{}lr@{}l}
\multicolumn{1}{c}{} & \multicolumn{12}{c}{\textbf{English}} \\
\toprule
 & \multicolumn{4}{c|}{\textbf{\asset}} & \multicolumn{4}{c|}{\textbf{\turkcorpus}} & \multicolumn{4}{c}{\textbf{\newsela}} \\

  & \multicolumn{2}{c}{SARI } & \multicolumn{2}{c|}{FKGL } & \multicolumn{2}{c}{SARI } & \multicolumn{2}{c|}{FKGL } & \multicolumn{2}{c}{SARI } & \multicolumn{2}{c}{FKGL }\\
\midrule

\multicolumn{2}{l}{}\-2mm]  \multicolumn{2}{l}{\textbf{\textit{Unsupervised Systems}}} \\
\midrule
BTRLTS {\tiny \cite{Zhao2020SemiSupervisedTS}} &  & &  & &  & &  & &  & &  \\
UNTS {\tiny \cite{surya2018unsupervised}} & & & & & & & & & ---& & ---& \\
RM+EX+LS+RO {\tiny \cite{kumar-etal-2020-iterative}} &  & &  & &  & &  & &  & &  \\
\midrule
\muss & \tnumem{\mathbf{42.65}}{0.23} & \tnumem{8.23}{0.62} & \tnumem{\mathbf{40.85}}{0.15} & \tnumem{8.79}{0.30} & \tnumem{\mathbf{38.09}}{0.59} & \tnumem{5.12}{0.47} \\
\midrule

\multicolumn{2}{l}{}\-2mm]
\multicolumn{2}{l}{\textbf{\textit{Supervised Systems (This Work)}}} \\
\midrule


Seq2Seq & \wikilarge & \numem{32.71}{1.55} & \numem{88.56}{1.06} & \numem{8.62}{0.34} & \numem{35.79}{0.89} & \numem{90.24}{2.52} & \numem{8.63}{0.34} & \numem{22.23}{1.99} & \numem{21.75}{0.45} & \numem{8.00}{0.26} \\
\muss & \wikilarge & \numem{43.63}{0.71} & \numem{76.28}{4.30} & \numem{6.25}{0.42} & \numem{42.62}{0.27} & \numem{78.28}{3.95} & \numem{6.98}{0.95} & \numem{40.00}{0.63} & \numem{14.42}{6.85} & \numem{3.51}{0.53} \\
\muss & \wikilarge + \mined & \numem{44.15}{0.56} & \numem{72.98}{4.27} & \numem{6.05}{0.51} & \numem{42.53}{0.36} & \numem{78.17}{2.20} & \numem{7.60}{1.06} & \numem{39.50}{0.42} & \numem{15.52}{0.99} & \numem{3.19}{0.49} \\
\muss & \newsela & \numem{42.91}{0.58} & \numem{71.40}{6.38} & \numem{6.91}{0.42} & \numem{41.53}{0.36} & \numem{74.29}{4.67} & \numem{7.39}{0.42} & \numem{42.59}{1.00} & \numem{18.61}{4.49} & \numem{2.74}{0.98} \\
\muss & \newsela + \mined & \numem{41.36}{0.48} & \numem{78.35}{2.83} & \numem{6.96}{0.26} & \numem{40.01}{0.51} & \numem{83.77}{1.00} & \numem{8.26}{0.36} & \numem{41.17}{0.95} & \numem{16.87}{4.55} & \numem{2.70}{1.00} \\

\midrule
\-2mm]
\multicolumn{2}{l}{\textbf{\textit{Supervised Systems (This Work)}}} \\
\midrule

Seq2Seq & \wikilarge & \numem{33.87}{1.90} & \numem{90.21}{1.14} & \numem{8.31}{0.34} & \numem{35.87}{1.09} & \numem{91.06}{2.24} & \numem{8.31}{0.34} & \numem{20.89}{4.08} & \numem{20.97}{0.53} & \numem{8.27}{0.46} \\
\muss & \wikilarge & \numem{45.58}{0.28} & \numem{78.85}{4.44} & \numem{5.61}{0.31} & \numem{43.26}{0.42} & \numem{78.39}{3.08} & \numem{6.73}{0.38} & \numem{39.66}{1.80} & \numem{14.82}{7.17} & \numem{4.64}{1.85} \\
\muss & \wikilarge + \mined & \numem{45.50}{0.69} & \numem{73.16}{4.41} & \numem{5.83}{0.51} & \numem{43.17}{0.19} & \numem{77.52}{3.01} & \numem{7.19}{1.02} & \numem{40.50}{0.56} & \numem{16.30}{0.97} & \numem{3.57}{0.60} \\
\muss & \newsela & \numem{43.91}{0.10} & \numem{70.06}{10.05} & \numem{6.47}{0.29} & \numem{41.94}{0.21} & \numem{74.03}{7.51} & \numem{6.99}{0.53} & \numem{42.36}{1.32} & \numem{19.18}{6.03} & \numem{3.20}{1.01} \\
\muss & \newsela + \mined & \numem{42.48}{0.41} & \numem{77.86}{3.13} & \numem{6.41}{0.13} & \numem{40.77}{0.52} & \numem{83.04}{1.16} & \numem{7.68}{0.30} & \numem{41.68}{1.60} & \numem{17.23}{5.28} & \numem{2.97}{0.91} \\

\midrule
\-2mm]
\multicolumn{2}{l}{\textbf{\textit{Unsupervised Systems (This Work)}}} \\
\midrule
Seq2Seq & \mined & \numem{38.88}{0.22} & \numem{61.80}{0.94} & \numem{8.63}{0.13} & \numem{37.51}{0.10} & \numem{62.04}{0.91} & \numem{8.64}{0.13} & \numem{30.35}{0.23} & \numem{13.04}{0.45} & \numem{8.87}{0.12} \\
\muss (\mbart) & \mined & \numem{41.68}{0.72} & \numem{77.11}{2.02} & \numem{6.56}{0.21} & \numem{39.60}{0.44} & \numem{75.64}{2.85} & \numem{8.04}{0.40} & \numem{34.59}{0.59} & \numem{18.19}{1.26} & \numem{5.76}{0.22} \\
\muss (\bart) & \mined & \numem{43.01}{0.23} & \numem{67.65}{4.32} & \numem{7.75}{0.53} & \numem{40.61}{0.18} & \numem{63.56}{4.30} & \numem{8.28}{0.18} & \numem{38.07}{0.22} & \numem{14.43}{0.97} & \numem{5.40}{0.41} \\



\bottomrule
\end{tabular}

}
\caption{\label{table:full_english_results_valid} \textbf{English Results on Validation Sets.} We display SARI, BLEU, and FKGL on \asset, \turkcorpus and \newsela English datasets (validation sets).}
\end{table*}







 

\end{document}