
\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\usepackage{subcaption}
\usepackage{caption}
\captionsetup{font=footnotesize}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{multirow}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}m{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}m{#1}}


\title{CURVATURE-GUIDED DYNAMIC SCALE \\ NETWORKS FOR MULTI-VIEW STEREO}





\author{Khang Truong Giang\textsuperscript{a} \quad \quad Soohwan Song\textsuperscript{b} \quad \quad Sungho Jo\textsuperscript{a} \\
\textsuperscript{a} {\small School of Computing, KAIST, Daejeon, 34141, Republic of Korea} \\
\textsuperscript{b} {\small Intelligent Robotics Research Division, ETRI, Daejeon 34129, Republic of Korea} \\
\texttt{\{khangtg,shjo\}@kaist.ac.kr, soohwansong@etri.re.kr}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most recent studies tried to improve the performance of matching cost volume in MVS by designing aggregated 3D cost volumes and their regularization. This paper focuses on learning a robust feature extraction network to enhance the performance of matching costs without heavy computation in the other steps. In particular, we present a dynamic scale feature extraction network, namely, CDSFNet. It is composed of multiple novel convolution layers, each of which can select a proper patch scale for each pixel guided by the normal curvature of the image surface. As a result, CDFSNet can estimate the optimal patch scales to learn discriminative features for accurate matching computation between reference and source images. By combining the robust extracted features with an appropriate cost formulation strategy, our resulting MVS architecture can estimate depth maps more precisely. Extensive experiments showed that the proposed method outperforms other methods on complex outdoor scenes. It significantly improves the completeness of reconstructed models. As a result, the method can process higher resolution inputs within faster run-time and lower memory than other MVS methods.
\end{abstract}

\section{Introduction}

A challenging problem in multi-view stereo (MVS) is accurately estimating dense correspondences across a collection of high-resolution images. Many MVS studies have tried to solve ill-posed MVS problems such as matching ambiguity and high computational complexity. In such problems, learning-based MVS methods \citep{chen2019point,luo2019p,chen2020visibility,xu2020learning,xue2019mvscrf} usually outperform traditional methods \citep{galliani2015massively,schonberger2016structure}. The learning-based methods are generally composed of three steps, including feature extraction, cost volume formulation, and cost volume regularization. Most studies made an effort to improve the performance of cost formulation \citep{luo2019p, zhang2020visibility, xu2020pvsnet, yi2020pyramid} or cost regularization \citep{luo2019p,luo2020attention,wang2021patchmatchnet}. The visibility information \citep{zhang2020visibility,xu2020pvsnet} or attention techniques \citep{yi2020pyramid, luo2020attention} are considered for cost aggregation and feature matching, respectively. Furthermore, some studies employed a hybrid 3D UNet structure \citep{luo2019p,sormann2020bp} for cost volume regularization. While these approaches significantly improve MVS performances, their feature extraction networks have some drawbacks that degrade the quality of 3D reconstruction. First, they observed a restricted size of receptive fields from the fixed size of convolutional kernels. This leads to difficulty in learning a robust pixel-level representation when an object’s scale varies extremely in images. As a result, the extracted features cause the low quality of matching costs, especially when the difference of camera poses between reference and source images is large. Second, MVS networks are often trained on low-resolution images because of limited memory and restricted computation time. Therefore, the existing feature extraction methods utilizing fixed-scale feature representation on deep networks could not generalize to high-resolution images in prediction; the MVS performance is downgraded on a high-resolution image set. 

To address these issues, we propose a new feature extraction network that can be adapted to various object scales and image resolutions. We refer the proposed network as curvature-guided dynamic scale feature network (CDSFNet). The proposed network is composed of novel convolution layers, curvature-guided dynamic scale convolution (CDSConv), which select a suitable patch scale for each pixel to learn robust representation. This selection is achieved by computing the normal curvature of the image surface at several candidate scales and then analyzing the computed outputs via a classification network. The pixel-level patch scales estimated from the proposed network are dynamic with respect to textures, scale of objects, and epipolar geometry. Therefore, it trains more discriminative features than existing networks \citep{gu2020cascade,cheng2020deep,yang2020cost,zhang2020visibility,luo2020attention} for accurate matching cost computation. Fig. \ref{fig1a} illustrates the dynamic pixel-level patch scales estimated from CDSConv where the window size is ranged from  to . As shown in the figure, CDSConv predicts the adaptive patch scales where the scale is small in thin structure or texture-rich regions and large in the texture-less regions. Moreover, the scale is dynamic according to image resolution. CDSConv generally produces small scale values in a low-resolution image and large scale values in a high-resolution image.

We also formulate a new MVS framework, CDS-MVSNet, which aims at improving the quality of MVS reconstruction while decreasing computation time and memory consumption. Similar to previous works \citep{gu2020cascade,cheng2020deep,yang2020cost,wang2021patchmatchnet}, CDS-MVSNet is composed of a cascade network structure to estimate high-resolution depth maps in a coarse-to-fine manner. For each cascade stage, it formulates a 3D cost volume based on the output features of CDSFNet, which reduces the matching ambiguity by using proper pixel’s scale. For instance, consider a 3D point on the scene that needs to be reconstructed, CDSFNet can capture the same context information in both reference and source views for this point to extract the matchable features. Furthermore, CDS-MVSNet applies visibility-based cost aggregation to improve the performance of stereo matching. The pixel-wise visibility is estimated from the curvature information that encodes the matching ability of extracted features implicitly. Therefore, CDS-MVSNet performs accurate stereo matching and cost aggregation even on low-resolution images. Finally, the proposed network can generate high quality 3D models only by processing half-resolution images. This approach can significantly reduce the computation time and memory consumption of MVS reconstruction. 

\begin{figure*}[t]
\begin{subfigure}{0.35\linewidth}
\centering
\includegraphics[scale=0.35]{figures/fig1.png}
\caption{Illustration of scale selection for each pixel in reference image from our proposed CDSConv}
\label{fig1a}
\end{subfigure}
\hfill
\begin{subfigure}{0.65\linewidth}
\centering
\includegraphics[scale=0.18]{figures/fig2_cdsconv.png}
\caption{CDSConv layer}
\label{fig1b}
\end{subfigure}
\hfill
\vspace{-0.25cm}
\caption{The results extracted by the proposed CDSConv and overall pipeline of CDSConv}
\label{fig1}
\vspace{-0.5cm}
\end{figure*}

The contribution of this paper is summarized as follows:
\begin{itemize}
    \item We present the CDSConv that learns dynamic scale features guided by the normal curvature of image surface. This operation is implemented by approximating surface normal curvatures in several candidate scales and choosing a proper scale via a classification network.
    \item We propose a new feature extraction network, CDSFNet, which is composed of multiple CDSConv layers for learning robust representation at pixel level. CDFSNet estimates the optimal pixel’s scale to learn features. The scale is selected adaptively with respect to structures, textures, and epipolar constraints.
    \item We present CDS-MVSNet for MVS depth estimation. CDS-MVSNet performs accurate stereo matching and cost aggregation by handling the ambiguity and visibility in the image matching process. It also significantly reduces the run-time and memory consumption by processing the half-resolution images while maintaining the reconstruction quality.
    \item We verify the effectiveness of our method on two benchmark datasets; DTU \citep{aanaes2016large} and Tanks \& Temples \citep{knapitsch2017tanks}. The results demonstrate that the proposed feature learning method boosts the performance of MVS reconstruction.
\end{itemize}

\section{Related Work}
\vspace{-0.25cm}
\textbf{Multi-view Stereo.} Traditional MVS studies can be divided into three categories: volumetric, point cloud-based, and depth map-based methods \citep{furukawa2015multi}. Comparatively, depth map-based methods are more concise and flexible, they estimate depth maps for all reference images and then perform depth fusion to achieving a 3D model \citep{galliani2015massively,schonberger2016pixelwise,xu2019multi}. The popular MVS methods in this category are usually based on PatchMatch Stereo algorithm \citep{barnes2009patchmatch}.  Recently, many studies have applied a learning-based approach, which has achieved significant performance improvements over traditional methods. Several volumetric methods \citep{kar2017learning,ji2017surfacenet} employed 3D CNNs to explicitly predict global 3D surfaces. \citet{yao2018mvsnet} proposed a depth-map-based method, MVSNet, which is an end-to-end trainable architecture with three steps. However, these methods still confronted the matching ambiguity problem and could not process high-resolution images due to limited memory. Many methods based on the three-step architecture of MVSNet are proposed to address these issues. Several studies \citep{gu2020cascade,cheng2020deep,yang2020cost,wang2021patchmatchnet,zhang2020visibility} applied a coarse-to-fine framework which estimates the depths through multiple stages to reduce the computational complexity. They used the estimated depth of each stage to adjust the depth hypothesis range for cost formulation in the next stage. 
To enhance the matching cost volume, other methods introduced a novel strategy for cost volume formulation. \citet{luo2019p} proposed a patch-wise feature matching instead of pixel-wise matching to compute the cost. \citet{yi2020pyramid} and \citet{luo2020attention} applied an attention technique to improve feature matching. \citet{zhang2020visibility,xu2020pvsnet} estimated visibility information to score the view weights for cost aggregation. 

All these methods \citep{gu2020cascade,cheng2020deep,yang2020cost,wang2021patchmatchnet,zhang2020visibility,luo2020attention,yu2020fast} had a trade-off between computation and performance; the methods with high performance usually require expensive computations. Also, they only applied standard CNNs with static scale representation for feature extraction. In this work, we aim to learn dynamic scale representation for more accurate feature matching. We also estimate pixel-wise visibility information to remove noises and wrong matching pixels in cost aggregation. These goals are achieved by analyzing the normal curvature of the image surface. 

Recently, \cite{xu2020marmvs} proposed an MVS method that considers the normal curvature information for feature extraction. This method handles ambiguity in the patch-match stereo method \citep{barnes2009patchmatch}. It selects a patch scale for each pixel by thresholding the normal curvatures computed in several candidate scales. Although the method effectively improves the quality of patch matching, the use of its handcrafted features limits the overall MVS performance. In contrast, our method does not require extracting handcrafted features and determining the threshold for scale selection. Furthermore, CDSConv operates not only on the color information of images but also on the high dimensional features. Multiple CDSConv operations can be stacked to form a very deep architecture for robust feature extraction in MVS.

\textbf{Multi-scale features.} Due to large-scale variations of objects and textures in complex scenes, multi-scale representations are adapted for accurate and effective dense pixel-level prediction in many computer vision tasks. For the MVS task, Feature Pyramid Network (FPN) \citep{lin2017feature,gu2020cascade,cheng2020deep,zhang2020visibility,luo2020attention} is a common approach for multi-scale feature extraction. It is designed as the UNet-liked architecture \citep{ronneberger2015u} to fuse features from different scales into a single one. However, the multi-scale of FPN is achieved at an image level through down-upsampling operations. It cannot capture the large-scale variation of objects at a pixel level. Several studies in image segmentation can learn pixel-level multi-scale features by using multiple kernels with different sizes \citep{he2019dynamic,liu2016ssd,chen2017deeplab,zhao2017pyramid}. However, these methods prefer a large receptive field for segmentation tasks and usually ignore the details of objects essential for MVS tasks. In contrast to the aforementioned methods \citep{gu2020cascade,cheng2020deep,zhang2020visibility,luo2020attention,he2019dynamic}, our proposed CDSConv can select an optimal scale for each pixel to produce a robust representation and enhance the matching uncertainty. Moreover, our feature extraction CDSFNet can cover denser scales by utilizing the power of a deep CNN.

\textbf{Dynamic filtering.} Instead of observing static receptive fields, several studies modified standard convolution to choose the adaptive receptive field for each pixel \citep{wu2018dynamic,jia2016dynamic,han2018face}. SAC \citep{zhang2017scale} was proposed to modify the fixed-size receptive field by learning position-adaptive scale coefficients. Deformable ConvNets \citep{dai2017deformable,zhu2019deformable} learned offsets for each pixel in a regular sampling grid of standard convolution to enlarge the sampling field with an arbitrary form that can discover geometric-invariant features. Recently, \citet{chen2020dynamic} proposed a dynamic convolution composed of multiple kernels with the same size to increase model complexity without increasing the network depth or width. All these existing works learn dynamic filters directly from the input images. They are suitable for segmentation or recognition tasks. For MVS task, they ignore the epipolar constraint between reference and source images. In contrast, our dynamic filtering is guided by normal curvature computed in the direction of epipolar line, which can measure the matching capacity between reference and source image patches. To the best of our knowledge, this is the first work that introduces dynamic filters to MVS. 



\section{PROPOSED FEATURE EXTRACTION NETWORK}
\label{headings}
\subsection{Normal Curvature}
\label{normal_curvature}

Normal curvature is used to estimate how much a surface is curved at a specific point along a particular direction \citep{do2016differential}. Let  be a patch centered at pixel  and having a scale  which is proportional to the window size.  is denoted as the epipolar line on the reference image , which passes through  and is related to a source image . To reduce the matching ambiguity between  in  and the correct patch  in , a proper scale  should be chosen when the corresponding patch  is less linearly correlated to its surrounding patches along the epipolar line . From the scale space theory \citep{lindeberg1994scale}, the patch  can be considered as a point on the image surface represented at the scale . Therefore, the scale  is optimal for stereo matching if the image surface represented in that scale is curved significantly at the point  along the epipolar line direction \citep{xu2020marmvs}. This requires to compute the normal curvature for the patch  along the direction  of the epipolar line . The formula can be expressed in terms of the values of the first and second fundamental forms of the image surface:

where  are the first-order and second-order derivatives of the image  along the  and , respectively.

\subsection{CURVATURE-GUIDED DYNAMIC SCALE CONVOLUTION}
\label{cdsconv}
This section describes the novel convolutional module CDSConv to extract dynamic scale features. Given a set of  convolutional kernels with different size  corresponding to K candidate scales , CDSConv aims to select a proper scale for each pixel . Hence, it can produce a robust output feature  from the input  based on the selected scale. Fig. \ref{fig1b} shows our overall pipeline of CDSConv including two steps. First, CDSConv estimates approximately normal curvatures at  candidate scales. Second, it performs a scale selection step to output the optimal scale from  estimated curvatures. This selection is implemented by a classification network composed of two convolutional blocks and a Softmax activation for the output.

\textbf{Learnable normal curvature.} The formula of normal curvature of a patch centered at  along the direction of epipolar line  in Eq. \ref{eq1} can be re-written as follows

where  is the image intensity of pixel  in the image scale ; it is determined by convolving  with a Gaussian kernel  with the window size/scale  \citep{lindeberg1994scale}. The derivatives , , , , and  can be computed by convolution between the original image  and the derivatives of Gaussian kernel 

where  is the convolution operator.
There are two main drawbacks when embedding the normal curvature in Eq. \ref{eq2} into a deep neural network. First, the computation is heavy because of five convolution operations for computing the derivatives , , , , and . Second, using Eq. \ref{eq2} to compute curvature is infeasible when the pixel  is a latent feature  instead of the image intensity . For these reasons, we derive an approximate form of the normal curvature, which reduces the computational cost and can handle the high-dimensional feature input. 

To address the heavy computation issue, we notice that the curvature  and derivatives   are proportional to Gaussian kernel , following Eq. \ref{eq2} and Eq. \ref{eq3}. Moreover, we use a classification network to select the patch scale automatically from the curvature inputs. Therefore, we can perform normalization for the curvatures by rescaling the kernel . We restrict the gaussian kernel in a small range, i.e., . As a result, the derivatives , , , , and  are also much smaller than 1. We can approximate the denominator in Eq. \ref{eq2} by 1. Eq. \ref{eq2} can be re-defined as

where , ,  and   is much smaller than 1, which implies that their values should be restricted in a small range. The computation in Eq. \ref{eq4} is reduced by haft compared to Eq. \ref{eq2}.

To make Eq. \ref{eq4} work with the high-dimensional image feature , we propose to use learnable kernels instead of using fixed derivatives of Gaussian kernel. In particular, for each scale , we introduce three learnable convolution kernels , ,  to replace , , and  respectively. These kernels adapt to the input features to approximately compute the second-order derivatives of the image surface. They are trained implicitly by backpropagation when training the end-to-end networks. Weights of these kernels need to be restricted to a small value  because of the assumption about the Gaussian kernel. We enforce this constraint by adding a regularization term to the loss in Section \ref{loss_function}.

In summary, we propose learnable normal curvature having the formula as follows (written in the matrix form)

where s are the learnable kernels and ,  is the input feature.

\textbf{Scale selection.} After obtaining  normal curvatures  in  candidate scales, this step selects a proper scale  for each pixel  and then outputs the feature  from the corresponding convolution kernel . The proper scale is selected by analyzing the curvatures estimated in  candidate scales. It is observed that a small scale cannot capture enough context information to learn discriminative feature, especially in low-textured regions. Meanwhile, a large scale smooths local structure in rich texture regions. MARMVS \citep{xu2020marmvs} chose the proper scale by searching the normal curvature with a threshold . However, using of fixed threshold  prevents generalization on various scenes and structures. 
Instead, we propose a classification strategy that automatically selects the proper scale from  curvature inputs. This is achieved by using a lightweight CNN with two convolutional blocks. For each pixel , the CNN outputs one-hot vector  for scale selection by applying a Softmax with small temperature parameter, which is a differentiable approximation to argmax operator \citep{DBLP:conf/iclr/JangGP17}.



Finally, the feature output  is produced from the feature input  with  candidate kernels  by using weighted sum

where  is the convolution operator. Also, we can extract the normal curvature corresponding to the selected scale by


\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.26]{figures/fig3.png}
\end{center}
\vspace{-0.25cm}
\caption{Dynamic scale according to different captured viewpoints and  relative camera pose between reference and source views. Figure a shows the scale maps when images is captured at the far view and close-up view. Figure b shows the results when the difference of camera poses is small (left) and large (right).}
\label{fig3}
\vspace{-0.5cm}
\end{figure}


\subsection{CURVATURE-GUIDED DYNAMIC SCALE FEATURE NETWORK}
\label{cdsfnet}
This section introduces the Curvature-guided dynamic scale feature network, CDSFNet, which is used as feature extraction step for our MVS framework. CDSFNet is composed of multiple CDSConv layers instead of standard convolution layers. By expanding the searching scale-space, CDSFNet can select the optimal scale for each pixel to learn robust representation that reduces matching ambiguity. 

\textbf{Architecture.} CDSFNet is an Unet-liked architecture \citep{ronneberger2015u}. It is organized into three levels of spatial resolution  to adapt the coarse-to-fine MVS framework. Given the inputs including image  and its estimated epipole , the network outputs three features for three levels  and three estimated normal curvatures . 

\textbf{Understanding CDSFNet.} The main goal of CDSFNet is to select pixel-level scale for robust feature extraction. The scale changes dynamically, depending on the detail of image objects and the epipolar constraint, i.e. the relative camera pose between reference and source images. Therefore, our method is more appropriate for MVS than the other dynamic scale networks \citep{he2019dynamic,wu2018dynamic,jia2016dynamic} proposed in image segmentation task.

To have a low computational complexity, the number of candidate scales in CDSConv layer should be small, we only use 2 or 3 candidates. However, the searching scale-space is expanded profoundly when multiple CDSConv layers are stacked in CDSFNet. Fig. \ref{fig3} show the patch scale or window size for each pixel estimated roughly from the first two layers of CDSFNet. For each reference and source views, we draw the reference and source scale maps respectively. There are two main advantages of CDSFNet shown in Fig. \ref{fig3}. First, the estimated scales in rich texture, thin structure, and near-edge regions are often small while those in untextured regions are large. Second, the scales are changed with respect to viewpoints (Fig. \ref{fig3}a) and relative camera pose between reference and source views (Fig. \ref{fig3}b). In Fig. \ref{fig3}a, the closer viewpoint is, the larger scales are estimated. In Fig. \ref{fig3}b, the reference and source scale maps are similar when the difference of camera poses is not large. Otherwise, when the difference is large, the scale map of reference view is changed to adapt the source view, which was marked in the red circle. This is a superiority of CDSFNet because it estimates scale based on epipolar constraint between reference and source views. 

\section{CDS-MVSNet}
In this section, we describe the proposed MVS network referred to as CDS-MVSNet, which predicts a depth map  from the reference  and  source images . We adopted the cascade structure of CasMVSNet \citep{gu2020cascade} as the baseline structure of CDS-MVSNet. The network is composed of multiple cascade stages to predict the depth maps in a coarse-to-fine manner. Each stage estimates a depth through three steps: feature extraction, cost volume formulation, cost volume regularization \& depth regression. 

CDS-MVSNet formulates a 3D cost volume based on the output features of CDSFNet. The features of CDSFNet effectively reduce the matching ambiguity by considering the proper pixel scales. Due to the robustness of our feature extraction and cost formulation, CDS-MVSNet can produce more accurate depths in coarser stages compared to other cascade MVS methods \citep{gu2020cascade,cheng2020deep,yang2020cost,wang2021patchmatchnet}. There is even little difference in the accuracy of the depths estimated at half resolution and full resolution. Therefore, differently from CasMVSNet, CDS-MVSNet only computes the depths of half-resolution images using three-step MVS computation and then upsamples the depths to the original resolution in the final stage. The upsampled depths at the original resolution can be refined by a 2D CNN to obtain the final depths. This approach can drastically reduce the computation time and GPU memory requirements. \subsection{DETAILS OF ARCHITECTURE}
\label{three_step_MVS}
\textbf{Robust Feature Extraction.} Given the reference image  and  source images ,  CDSFNet extracts a robust feature pair  for each image pair . Note that the feature of the reference image  varies according to each source image i due to the guidance of epipolar geometry in normal curvature estimation. This is our superiority compared to the state-of-the-art MVS networks which learn an identical feature for the reference image.

\textbf{Cost Volume Formulation.} We first compute two-view matching costs and then aggregate them into a single cost. The two-view matching cost is computed per sampled depth hypothesis to form cost volume \citep{yao2018mvsnet,gu2020cascade,wang2021patchmatchnet}. Given a sampled depth hypothesis  along with all camera parameters, we warp the feature maps of source image  into this depth plane. Let  be the warped feature map of source image  at the depth hypothesis . The two-view matching cost  between the reference image  and source image  at the depth hypothesis  is determined as , where  is the pixel-wise inner product for measuring the similarity of two feature maps.

Next, we perform cost aggregation over the computed two-view cost volumes. Inspired from visibility-awareness for cost aggregation \citep{zhang2020visibility,xu2020pvsnet}, we employ pixel-wise view weight prediction. However, instead of predicting directly from two-view cost volume \citep{zhang2020visibility,xu2020pvsnet,yi2020pyramid}, we additionally utilize the normal curvature maps, , estimated by CDSFNet in Eq. \ref{eq7}. The normal curvature can implicitly provide information about level details of surface. For instance, a pixel with small normal curvature indicates that it belongs to a large untextured region. In this case, its feature is unmatchable and needs to be eliminated from cost aggregation to reduce noises for cost regularization step.

In summary, the estimated normal curvature encodes implicitly the matching ability of learned feature. Let  be the estimated normal curvature of the reference view related to source view , and  be the entropy computed from two-view matching cost volume .  We use a simple 2D CNN denoted as  to predict a view weight map  from two inputs  and . Finally, the aggregated cost  is computed by weighted mean.


\textbf{Cost volume regularization \& Depth regression.} Following most recent learning-based MVS methods, a 3D CNN is applied to obtain a depth probability volume  from the aggregated matching cost volume . The depth is then regressed from   by 



\subsection{LOSS FUNCTION}
\label{loss_function}
We propose a feature loss for training CDSFNet effectively. Given a ground truth depth  of the reference image, we warp the feature of source images into this depth map and then compute the matching cost . We label this map as positive, hence defines its classification label as . To generate the matching costs with negative labels , we randomly sample  neighboring depths  around the ground truth  and then compute the matching costs  for these depths.  Finally, binary cross entropy is used to define the feature loss. However, we need to add regularization terms to restrict the convolutional weights of CDSFNet, , and estimated normal curvature  to a small range as mentioned in Section \ref{cdsconv}. Therefore, the final feature loss is defined as:

where  is the sigmoid function,  is total number of pixels ,  and  are the regularized hyperparameters.

Similar to most learning-based MVS methods \citep{yao2018mvsnet,gu2020cascade}, the depth loss  is defined by  loss between the predicted depth and the ground truth depth. Because our method includes 4 stages, there are four depth loss . Finally, the total loss  is defined as a sum of two mentioned losses 

\section{EXPERIMENTAL RESULTS}
\subsection{DATASETS}
The datasets used in our evaluation are DTU \citep{aanaes2016large}, BlendedMVS \citep{yao2020blendedmvs}, and Tanks \& Temples \citep{knapitsch2017tanks}. Due to the simple camera trajectory of all scenes in DTU, we additionally utilize the BlendedMVS dataset with diverse camera trajectories for training. Tanks \& Temples is provided as a set of video sequences in realistic environments. We train the model on the training sets of DTU and BlendedMVS, then evaluate the testing set of DTU and intermediate set of Tanks \& Temples. Our source code is available at \url{https://github.com/TruongKhang/cds-mvsnet}. 

\subsection{BENCHMARK PERFORMANCE}
\label{benchmark performance}
\begin{table}[!t]
\begin{minipage}[t]{.5\textwidth}
\caption{Quantitative results on the DTU evaluation dataset (lower is better).}
\label{table1}
\centering
\resizebox{7cm}{!}{\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|L{20em} | C{3.4em} C{3.4em} C{3.4em} |}
\hline
\multirow{2}{6em}{\textbf{Methods}} & \multicolumn{3}{c|}{\textbf{Mean error distance (mm)}} \\
\cline{2-4}
 & \textbf{Acc.} & \textbf{Comp.} & \textbf{Overall} \\
 \hline
\textbf{Gipuma} \citep{galliani2015massively} & \textbf{0.283} & 0.873 & 0.578 \\ 
\textbf{Colmap} \citep{schonberger2016pixelwise} & 0.400 & 0.664 & 0.532 \\
\textbf{P-MVSNet} \citep{luo2019p} & 0.406 & 0.434 & 0.420 \\
\textbf{Fast-MVSNet} \citep{yu2020fast} & 0.336 & 0.403 & 0.370 \\
\textbf{Vis-MVSNet} \citep{zhang2020visibility}& 0.369 & 0.361 & 0.365 \\
\textbf{AttMVS} \citep{luo2020attention} & 0.383 & 0.329 & 0.356 \\
\textbf{CasMVSNet} \citep{gu2020cascade} & 0.325 & 0.385 & 0.355 \\
\textbf{UCSNet} \citep{cheng2020deep} & 0.338 & 0.349 & 0.344 \\
\textbf{PatchMatchNet} \citep{wang2021patchmatchnet} & 0.427 & \textbf{0.277} & 0.352 \\
\textbf{PVA-MVSNet} \citep{yi2020pyramid} & 0.379 & 0.336 & 0.357 \\
\textbf{CVP-MVSNet} \citep{yang2020cost} & \underline{0.296} & 0.406 & 0.351 \\
\textbf{BP-MVSNet} \citep{sormann2020bp} & 0.333 & \underline{0.320} & \underline{0.327} \\
\hline
\textbf{Ours} (DTU only) & 0.352 & \textbf{0.280} & \textbf{0.316} \\
\textbf{Ours} (DTU+BlendedMVS) & 0.351 & \textbf{0.278} & \textbf{0.315} \\ 
\hline
\multicolumn{4}{c}{{\footnotesize  traditional method,  trained on training set of DTU,  trained on a modified version of DTU}}
\end{tabular}
}
\end{minipage}
\quad
\begin{minipage}[t]{.48\textwidth}
\caption{Evaluation of estimated depth with different image resolutions on DTU (higher is better).}
\label{table2}
\centering
\resizebox{5.9cm}{!}{\renewcommand{\arraystretch}{1.45}
\begin{tabular}{|L{1em} | L{6em} | C{3em} | C{3em} | C{3em} | C{3em} | C{3em} |}
\hline
& \multirow{2}{4em}{\textbf{Methods}} & \multicolumn{5}{c|}{\textbf{Resolutions (width x height)}} \\
\cline{3-7}
& & \textbf{832 x 576} & \textbf{1152 x 832} & \textbf{1280 x 960} & \textbf{1408 x 1088} & \textbf{1536 x 1152} \\
\hline
\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Prec. 2mm (\%)}}}} & CasMVSNet & \textbf{76.54} & \textbf{75.35} & 74.01 & 72.22 & 70.92 \\ 
& PatchMatchNet & 69.52 & 71.23 & 71.37 & 71.20 & 70.99 \\ 
& UCSNet & 74.11 & 73.83 & 72.95 & 71.58 & 70.49 \\
& CVP-MVSNet & 66.80 & 69.75 & 70.71 & 71.03 & 71.16 \\ 
& Ours & 72.71 & \underline{75.04} & \textbf{75.09} & \textbf{74.91} & \textbf{74.64} \\
\hline
\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Prec. 4mm (\%)}}}} & CasMVSNet & 82.11 & 80.47 & 79.02 & 77.32 & 76.15 \\ 
& PatchMatchNet & 78.31 & 78.32 & 78.03 & 77.67 & 77.34 \\ 
& UCSNet & \textbf{82.18} & 80.10 & 78.59 & 76.91 & 75.61 \\
& CVP-MVSNet & 75.28 & 77.41 & 78.18 & 78.68 & 78.76 \\ 
& Ours & 80.86 & \textbf{81.38} & \textbf{80.99} & \textbf{80.48} & \textbf{80.02} \\
\hline
\end{tabular}
}
\end{minipage}
\end{table}
\textbf{Results for DTU dataset.} We predict the depth at the highest resolution . The performance of the proposed method was compared with state-of-the-art MVS methods, including conventional and learning-based methods, using the DTU dataset. However, we especially notice the learning-based methods including CasMVSNet, UCSNet, CVP-MVSNet, and Vis-MVSNet, which adopted the cascade structure for MVS depth estimation. Similar to us, these methods used the same 3D CNN cost regularization. Therefore, we can validate our feature extraction and cost formulation by comparing with them. 

Table \ref{table1} presents the quantitative results achieved by the methods on the DTU evaluation dataset. We calculated the three standard error metrics \citep{aanaes2016large} given on the official site of DTU: accuracy, completeness, and overall error. Following the same setups with most of the baselines, we trained our model on the DTU training set and then used this model for evaluation. Moreover, we also provided the results which produced by training on both DTU and BlendedMVS simultaneously. In both cases, our method exhibited the best performance in terms of completeness and overall error. We also achieved good results for both accuracy and completeness, while the other methods often produced a large trade-off between these two metrics. For example, Gipuma had a high accuracy at  but it produced very low completeness, . In contrast, PatchMatchNet was with a high completeness  and very low accuracy . Moreover, our method outperformed all similar methods using the cascade structure. This indicates that our feature representation and cost aggregation guided by normal curvature can remarkably improve MVS depth prediction remarkably. 

To validate the dynamic scale property of our CDSFNet, we measure depth precision on different image resolutions. Depth precision is the average percentage of depths with errors lower than a defined threshold \citep{gu2020cascade}. Here we computed precision with the thresholds of 2mm and 4mm. We compared to CasMVSNet, PatchMatchNet, UCSNet, and CVP-MVSNet, which applied the cascade structure to predict high-resolution depths. Note that all methods are trained on the datasets with image-resolution of . Table \ref{table2} presents the quantitative results for the depth estimated on five image resolutions. The baseline methods degraded the performance when the image-resolution was increased while our method could maintain a stable performance at any image resolution. The reason is that our feature extraction CDSFNet can utilize dynamic scales for learning features. The dynamic scale adapts to various image-resolutions and object scales in image.

\begin{table}[t]
\caption{Performance comparisons (F-score) of various reconstruction algorithms on the intermediate sequences of the Tanks \& Temples benchmark. The higher value of F-score implies better reconstruction results. Our method achieves the best performance in terms of mean F-score.}
\label{table3}
\centering
\resizebox{13cm}{!}{\renewcommand{\arraystretch}{1.0}
\begin{tabular}{L{55mm}|C{12mm}|C{12mm}C{12mm}C{12mm}C{15mm}C{12mm}C{12mm}C{15mm}C{12mm}}
\Xhline{1.1pt}
\textbf{Method} & \textbf{Mean} & \textbf{Family} & \textbf{Francis} & \textbf{Horse} & \textbf{Lighthouse} & \textbf{M60} & \textbf{Panther} & \textbf{Playground} & \textbf{Train} \\
\Xhline{1.0pt}
ACMM \citep{xu2019multi} & 57.27 &	69.24 & 51.45 & 46.97 & 63.20 & 55.07 & 57.64 & 60.08 & 54.48 \\ 
ACMP \citep{xu2020planar} &  58.41 & 70.30 & 54.06 & 54.11 & 61.65 & 54.16 & 57.60 & 58.12 & 57.25 \\ 
Fast-MVSNet \citep{yu2020fast} & 47.39 & 65.18 & 39.59 & 34.98 & 47.81 & 49.16 & 46.20 & 53.27 & 42.91 \\ 
Vis-MVSNet \citep{zhang2020visibility}      & 60.03 & \underline{77.40} & 60.23 & 47.07 & \underline{63.44} & \underline{62.21} & 57.28 & 60.54 & 52.07 \\
AttMVS \citep{luo2020attention}    & \underline{60.05} & 73.90 & \underline{62.58} & 44.08 & \textbf{64.88} & 56.08 & \textbf{59.39} & \textbf{63.42} & \textbf{56.06} \\
CasMVSNet \citep{gu2020cascade}   & 56.84 & 76.37 & 58.45 & 46.26 & 55.81 & 56.11 & 54.06 & 58.18 & 49.51 \\ 
UCSNet \citep{cheng2020deep}      & 54.83 & 76.09 & 53.16 & 43.03 & 54.00 & 55.60 & 51.49 & 57.38 & 47.89 \\
PVA-MVSNet \citep{yi2020pyramid}  & 54.46 & 69.36 & 46.80 & 46.01 & 55.74 & 57.23 & 54.75 & 56.70 & 49.06 \\
CVP-MVSNet \citep{yang2020cost}  & 54.03 & 76.50 & 47.74 & 36.34 & 55.12 & 57.28 & 54.28 & 57.43 & 47.54 \\
BP-MVSNet \citep{sormann2020bp}  & 57.60 & 77.31 & 60.90 & \underline{47.89} & 58.26 & 56.00 & 51.54 & 58.47 & 50.41 \\
\hline
Ours (fine-tuning on BlendedMVS) & \textbf{60.82} & \textbf{78.17} & 61.74 & \textbf{53.12} & 60.25 & 61.91 & \underline{58.45} & \underline{62.35} & 50.58 \\
Ours (DTU+BlendedMVS) & \textbf{61.58} & \textbf{78.85} & \textbf{63.17} & \textbf{53.04} & 61.34 & \textbf{62.63} & \underline{59.06} & \underline{62.28} & 52.30 \\ 
\Xhline{1.1pt}
\multicolumn{10}{c}{{\footnotesize  traditional method,  only training on the training set of DTU,  training on DTU and then fine-tuning on BlendedMVS,  training on a modified version of DTU}}
\end{tabular}
}
\vspace{-0.25cm}
\end{table}

\textbf{Generalization on Tanks \& Temples Dataset.} To verify the generalization capability of our CDS-MVSNet, we also provided two evaluations similar to the evaluation of DTU. First, we fine-tuned the pre-trained model on DTU by using BlendedMS dataset. Second, we used directly the model trained on both datasets for evaluation. The reconstructed point clouds were submitted to the benchmark website of Tanks \& Temples \citep{knapitsch2017tanks} to receive the F-score, which is a combination of precision and recall of the reconstructed 3D model. Table \ref{table3} lists the quantitative results of our method and other state-of-the-art methods. As shown in Table \ref{table3}, our method achieved the best mean Fscores for both evaluations,  and , respectively. Our method outperformed both the state-of-the-art traditional methods such as ACMM, ACMP and the learning-based methods including AttMVS and Vis-MVSNet. The qualitative results of all scenes are shown in detail in the supplemental materials. Compared to CasMVSNet and UCSNet, which are most similar to our method except the feature extraction and cost formulation, we obtained better mean F-score with high margins. This demonstrates the effectiveness of our feature extraction CDSFNet and cost aggregation strategy on complex outdoor scenes.

\begin{figure*}
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[scale=0.13]{figures/mem_plot.png}
\label{fig6a}
\end{subfigure}
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[scale=0.13]{figures/time_plot.png}
\label{fig6b}
\end{subfigure}
\caption{Relating GPU memory and run-time to the input resolution on DTU's evaluation set. The original image resolution is  (100\%).  Here we evaluate at the highest resolution  (95.2\%). }
\vspace{-0.5cm}
\label{fig6}

\end{figure*}
\textbf{Run-time and Memory.} This section evaluates the memory consumption and run-time compared to several state-of-the-art learning-based methods that achieve competing performance and employ the cascade structure similar to us: CasMVSNet, UCS-Net, CVP-MVSNet, and FastMVSNet. Fig. \ref{fig6} shows the memory and run-time of all methods measured on DTU evaluation set with different image resolutions. We observe that the run-time and memory of all methods are more required when the image-resolution increases higher. However, we notice that the memory consumption of our method was dropped from the resolution of 80\% while our run-time was slightly increased. This indicates that our method can release a significant amount of GPU memory by only sacrificing a small amount of run-time. Our method achieves significantly faster run-time and lower required memory at the very high image-resolution compared to the other baselines. For example, at a resolution of  (92.2\%), the memory consumption is reduced by \textbf{54.0\%}, 34.1\%, and \textbf{54.0\%} compared to CasMVSNet, UCSNet, and CVP-MVSNet respectively. The corresponding comparison results of run-time are  43.2\%, 37.8\%, and \textbf{74.9\%}. Finally, we conclude that our method is more efficient in memory consumption and run-time than most state-of-the-art learning-based methods. 
\vspace{-0.25cm}
\section{Conclusion}
\vspace{-0.25cm}
We propose a dynamic scale network CDSFNet for MVS task.  CDSFNet can extract the discriminative features to improve the matching ability by analyzing the normal curvature of the image surface. We then design an efficient MVS architecture, CDS-MVSNet, which achieves state-of-the-art performance on various outdoor scenes 









































































\subsubsection*{Acknowledgments}
This work was supported by the National Research Foundation of Korea (NRF) funded by the Ministry of Education under Grant 2016R1D1A1B01013573.


\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}

\appendix
\section{Appendix}
\subsection{Derived formulas of normal curvature}
First, we re-derive Eq. \ref{eq1} proposed by \cite{xu2020marmvs}. The image surface can be represented by 
where  is a pixel coordinate and  is the color information of image. The unit normal vector of surface  at point  is determined as

where  and  are the first partial derviatives with respect to  and ,  is the cross-product operator. Then, the first and second fundamental form of surface  at point  can be written by

Note that "" is the dot-product operation between two vectors. Finally, the normal curvature of  at point  along the direction  of epipolar line is defined as

Because  is an unit vector, . Therefore,

This formula can be applied to an image patch  with a scale  centered at . Following the scale space theory \citep{lindeberg1994scale}, the patch p can be regarded as a pixel when the image I is represented at the scale . Let  be the image intensity at the scale .  can be computed from  by using a Gaussian kernel with size , 

Therefore, we can finally write the normal curvature computed for  at the scale  as

which was Eq. \ref{eq2} mentioned in Section \ref{cdsconv}

\subsection{Detail of MVS architecture}
Fig. \ref{fig5} depicts the multi-stage architecture of CDS-MVSNet. First, CDS-MVSNet uses the feature extraction CDSFNet to extract outputs at three resolutions  corresponding to the first three stages . For each of these stages , CDS-MVSNet predicts the depth at the corresponding image-resolution  by using three-step MVS as described in section \ref{three_step_MVS}. CDS-MVSNet also uses an independent 3D CNN for each stage to regularize 3D cost volume. The estimated depth of current stage is upsampled and used to reduce the depth hypothesis range of 3D cost volume in the next stage. For the last stage , CDS-MVSNet outputs the full-resolution depths by performing depth refinement from the upsampled depth and the color image.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.3]{figures/fig5.png}
\end{center}
\caption{CDS-MVSNet architecture. For the first three stages , the estimated depth of current stage is used to reduce the depth hypothesis range of a cost volume in the next stage. For the last stage , we perform depth refinement from the estimated depth of previous stage and the color information}
\label{fig5}
\end{figure}

\textbf{CDSFNet}
Given a reference image  and  source images  with corresponding camera parameters , we can compute an epipole pair  for each reference-source image pair  based on the epipolar geometry. We use the epipole to determine the epipolar line for each pixel. Therefore, given the inputs including image  and epipole , CDSFNet extracts three feature maps corresponding to the three levels  of CDS-MVSNet. Fig. \ref{cdsfnet} describes the Unet-liked architecture of CDSFNet. Fig. \ref{fig4} and \ref{estimated_norm_curv} show the results of dynamic scale estimation and normal curvature estimation respectively.  

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.5]{figures/cdsfnet.png}
\end{center}
\caption{The Unet-liked architecture of CDSFNet. It is organized into three levels: quater-resolution, haft-resolution, and full-resolution}
\label{cdsfnet}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.3]{figures/fig4.png}
\end{center}
\caption{Illustrations for dynamic scale estimation according to epipolar constraint between reference and source views. Consider a pixel of reference view marked in red color, its epipolar line, marked in yellow, varies with each source view. Therefore, the normal curvatures estimated at that pixel are different, this leads to the different estimated scale in the reference view when it is matched to a source view.}
\label{fig4}
\end{figure}
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.44]{figures/est_norm_curv.png}
\end{center}
\caption{Visualization of the normal curvature maps estimated from our feature extraction CDSFNet by Eq. \ref{eq7}. For each pixel , the normal curvature computed at  is large when  belongs to a rich texture region. In this case, the estimated scale for  is small as shown in Fig. \ref{fig4} to preserve the local detail of the scenes. Otherwise, the curvature is small when  belongs to an untextured region; therefore, a large scale is chosen to increase the completeness. These figures demonstrate that the normal curvature can provide guidance to choose the optimal scale to improve the matching ability.}
\label{estimated_norm_curv}
\end{figure}

\textbf{Depth Refinement.} Instead of using three-step MVS depth estimation as described for the finest resolution level (stage 3), we directly up-sample the depth estimated in previous stage (from resolution  to ) and refine the upsampled depth with the RGB image. Following MSG-Net \citep{hui2016depth} and PatchMatchNet \citep{wang2021patchmatchnet}, we implement a depth residual network to perform refinement. The network uses a 2D CNN to output a residual from the upsampled depth. This residual depth is then added to the up-sampled depth, to get the refined depth map .
\subsection{Implementation Details}
\textbf{Training.} We applied the same image-resolution  in both DTU and BlendedMVS datasets. The preprocessing of these datasets for view selection strategy and ground-truth depth maps was provided by \cite{yao2018mvsnet}. For CDS-MVSNet, we set the number of input views to 3, the number of depth hypothesis planes to  with the corresponding depth interval scales . To train CDFSNet effectively, we used the feature loss as mentioned in Section \ref{loss_function}. Besides, we implemented the scale selection step of CDSConv by first initializing the Softmax temperature by . This hyperparameter was then decreased over training time until it reached  to produce the sparse output for scale selection. The entire network CDS-MVSNet was trained with  epochs, using the SGD optimizer with an initial learning rate of , and on two NVIDIA Titan V GPU with a batch size of . We provided two experimental scenarios. First, we trained the model on DTU training set and then evaluated on DTU test set. This pre-trained model is then fine-tuned on BlendedMVS dataset for 15 epochs with a learning rate of  to evaluate on Tanks \& Temples. Second, we trained a single model on both DTU and BlendedMVS datasets and use this model for evaluation. This training strategy can improve the performance as indicated in Table \ref{table1} and \ref{table3}. 

\textbf{Point cloud generation.} To reconstruct the final point cloud, we filtered out the low confidence depths based on the probability maps estimated from the depth probability volumes of all stages \citep{zhang2020visibility}. Similar to \citep{gu2020cascade,zhang2020visibility}, we then integrated all the depth maps based on the fusion method proposed in \citep{galliani2015massively,yao2018mvsnet}. 
\subsection{Ablation study}
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.32]{figures/learnable_vs_original_curv.png}
\end{center}
\caption{A qualitative comparison on DTU evaluation set \citep{aanaes2016large} between the learnable and original curvature. We visualize the curvature maps at the scale  (kernel size is 3) extracted from the first four layers of CDSFNet. The first and second layers output the curvature maps at the full resolution, the third and fourth layers extract the maps at the half-resolution. The red circles denote the effect of attention mechanism, which is benefited from the learnable kernels  as presented in Section \ref{cdsconv}}
\label{learnable_vs_original}
\end{figure}

\begin{table}[t]
\caption{Comparison between our learnable curvature and the original curvature \citep{xu2020marmvs}. All models are trained on a subset of DTU training set, using the same setups and hyperparameters. The evaluation of depth map and pointcloud is collected on DTU validation and test set, respectively.}
\label{table4}
\centering
\resizebox{14cm}{!}{
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{L{20mm}|C{15mm}C{25mm}|C{10mm}C{10mm}C{10mm}|C{10mm}C{10mm}C{10mm}|C{10mm}C{10mm}}
\Xhline{1.1pt}
\multirow{2}{10em}{\textbf{Methods}} & \multicolumn{2}{c|}{\textbf{Design Options}} & \multicolumn{3}{c|}{\textbf{Depth Map}} & \multicolumn{3}{c|}{\textbf{Pointcloud}} & \multicolumn{2}{c}{\textbf{Time \& Memory}} \\
 & \textbf{curv. type} & \textbf{vis. with curv.} & \textbf{Prec. 2mm} & \textbf{Prec. 4mm} & \textbf{MAE (mm)} & \textbf{Acc. (mm)} & \textbf{Comp. (mm)} & \textbf{Overall (mm)} & \textbf{Time (s)} & \textbf{Mem (Mb)} \\
\Xhline{1.0pt}
Model A & original &  & 74.26 & 84.89 & 6.84 & 0.378 & 0.315 & 0.347 & 0.539 & 7993 \\ 
Model B & original & \checkmark & 74.12 & 84.77 & 5.99 & 0.391 & 0.327 & 0.359 & 0.539 & 7993 \\ 
Model C & learnable &  & 76.14 & \textbf{86.08} & 6.30 & 0.373 & \textbf{0.302} & \textbf{0.338} & \textbf{0.421} & \textbf{4389} \\ 
Model D & learnable & \checkmark & \textbf{76.23} & 85.92 & \textbf{5.89} & \textbf{0.372} & 0.305 & \textbf{0.339} & \textbf{0.421} & \textbf{4389} \\ 
\Xhline{1.1pt}
\end{tabular}}
\end{table}

\begin{table}[t]
\begin{minipage}[t]{.51\textwidth}
\caption{Impact of the number of candidate scales in CDSConv layer. The models are trained on DTU training set at the resolution , without 3D cost regularization. The statistics below are collected on DTU validation set.}
\label{table5}
\centering
\resizebox{7.4cm}{!}{\renewcommand{\arraystretch}{1.1}
\begin{tabular}{L{31mm}|C{8mm}C{8mm}C{8mm}C{8mm}|C{8mm}}
\Xhline{1.1pt}
\textbf{\#scales/layer ()} & \textbf{Prec. 2mm} & \textbf{Prec. 4mm} & \textbf{Prec. 8mm} & \textbf{MAE (mm)} & \textbf{Time (s)} \\
\Xhline{1.0pt}
1 (CDSFNet  FPN) & 40.87 & 55.95 & 66.72 & 21.41 & \textbf{0.114} \\ 
2 & 44.46 & 58.10 & 67.89 & 18.89 & 0.134 \\ 
3 & 45.27 & 59.14 & 69.17 & 17.46 & 0.155 \\ 
4 & \textbf{46.77} & \textbf{60.63} & \textbf{70.61} & \textbf{16.10} & 0.195 \\ 
\Xhline{1.1pt}
\end{tabular}
}
\end{minipage}
\quad
\begin{minipage}[t]{.45\textwidth}
\caption{The accuracy of estimated depth over different stages in our CDS-MVSNet. The entire MVS network is trained on DTU training set. The trained model is used to evaluate on the DTU test set at the resolution }
\label{table6}
\centering
\resizebox{5.4cm}{!}{\renewcommand{\arraystretch}{1.1}
\begin{tabular}{C{10mm}|C{10mm}C{10mm}C{10mm}C{10mm}}
\Xhline{1.1pt}
\textbf{Stages} & \textbf{Prec. 1mm} & \textbf{Prec. 2mm} & \textbf{Prec. 4mm} & \textbf{MAE (mm)} \\
\Xhline{1.0pt}
0 & 39.99 & 61.89 & 76.35 & 11.43  \\ 
1 & 58.01 & 72.92 & 80.21 & 10.80  \\ 
2 & 63.97 & 75.43 & 80.92 & 10.67  \\ 
3 & \textbf{64.26} & \textbf{75.52} & \textbf{80.94} & \textbf{10.62}  \\ 
\Xhline{1.1pt}
\end{tabular}
}
\end{minipage}
\end{table}

\textbf{Effectiveness of learnable curvature.} We demonstrate the advantage of our proposed learnable curvature in comparison to the original curvature \citep{xu2020marmvs}. We replace the curvature estimation in Eq.\ref{eq5} with Eq.\ref{eq1} and then re-train the MVS network on the DTU dataset with identical setups. Table \ref{table4} presents four model variants denoted as A, B, C, and D. Each variant implements a specific version of curvature (original or learnable) and then uses the visibility aggregation with or without the curvature prior. We consider a comparison of the two groups, models A and B versus models C and D. To evaluate the depth maps, we measure the percentage depth precision with the thresholds mm and Mean Absolute Error (MAE). To evaluate the reconstructed models, we use the accuracy, completeness, and overall metrics. We also provide the runtime and memory consumption to evaluate the computational efficiency. 

Table \ref{table4} indicates that the models with learnable curvature (C and D) achieve a better performance than the original curvature (A and B) for all evaluation metrics. The learnable curvature has a low computation cost and boosts the quality of depth and reconstruction significantly. The original curvature degrades the performance because it cannot handle the high-dimensional features. The original method treats each feature channel equally and averages all feature channels when computing the derivatives by Gaussian filters. Therefore, it may produce a high curvature estimation even though the neighboring feature vectors are visually similar to each other, as shown in Fig. \ref{learnable_vs_original}. 

Fig. \ref{learnable_vs_original} shows the normal curvatures estimated by the original method \citep{xu2020marmvs} and our learnable method. The curvature maps are extracted from the first four CDSConv layers of CDSFNet. As shown in Fig. \ref{learnable_vs_original}, the original method provides a ground truth estimation only in the first layer when the input is a color image (i.e., the estimated curvatures are small in untextured regions while large in near-edge or rich-textured regions). However, it produces noisy estimation when the inputs are the feature maps in the subsequent layers, especially in the third and fourth layers where the curvatures do not reflect the edge or texture information. On the other hand, although the learnable curvature approximates the ground truth (the original curvature in the first layer), it consistently preserves the properties of normal curvature concerning the untextured and rich-textured regions in every layer. We further notice that our learning-based estimation takes advantage of the attention mechanism, as shown in red circles. It focuses on the rich texture region to improve the completeness of reconstruction, which is illustrated in Fig. \ref{dtu_results} in Appendix \ref{a5}. Fig. \ref{learnable_vs_original} also implies that the estimated curvature tends to be higher at the deeper layers because it encodes a larger patch scale.   

\textbf{Curvature-guided visibility aggregation.} We proposed a pixel-wise visibility prediction method for cost aggregation in Section \ref{three_step_MVS}. Previous works generally predicted the visibility weights only from the two-view cost volume \citep{zhang2020visibility, xu2020pvsnet}. Compared to these works, our method additionally considers the curvature prior estimated by CDSFNet. To evaluate the effectiveness of curvature prior to visibility prediction, we compared the performance of our method with the baseline method \cite{zhang2020visibility}. To implement the baseline method, we removed the curvature input from our visibility prediction network and only used the entropy information of two view cost volumes. 
We compared the models using curvature prior (Models B and D) with those without using it (Models A and C). Table \ref{table4} shows the experimental results. As shown in Table \ref{table4}, Model B had the worst performance in terms of overall. Because Model B produced the noisy curvature, this noisy estimation degraded the performance of Model B compared to Model A. Model D remarkably improves the depth estimation performances remarkably and achieves the similar performance of reconstruction compared to Model C without increasing runtime and memory consumption. These results demonstrate the effectiveness of the proposed learnable curvature and curvature-guided visibility prediction in the MVS process.


\textbf{Analysis of CDSFNet and the cascade structure.} This section first analyzes the effectiveness of proposed CDSFNet when increasing the number of candidate scales. To measure explicitly the matchability of features extracted by CDSFNet, we remove the cost-volume regularization step in the MVS architecture and then regress the depth directly from the aggregated 3D cost volume. Table \ref{table5} presents the results when increasing the number candidate scales  in each CDSConv layer. When  is equal to 1, CDSFNet is considered as Feature Pyramid Network (FPN), which is applied to most of the state-of-the-art learning-based methods \citep{gu2020cascade, zhang2020visibility}. As shown in Table \ref{table5},  CDSFNet achieved better performance when using more candidate scales. However, it suffered from heavy computation. Therefore, we chose the design with two or three scales for each CDSConv (CDSFNet architecture as shown in Fig. \ref{cdsfnet}), to guarantee both the performance of reconstruction and computational efficiency.

Next, we investigate the improvement of depth estimation over the cascade stages. Table \ref{table6} provides the accuracy of depth estimation over all stages of our CDS-MVSNet. The first three stages mainly contribute to the final quality of estimated depth. The last stage  is used for depth upsampling; it outputs the full-resolution depth while maintaining the same depth accuracy compared to the previous stage. Suppose this stage is implemented by the three-step MVS depth estimation pipeline mentioned in Section \ref{three_step_MVS}, the entire MVS network suffers from a considerable computation and cannot fit into a limited GPU memory. Therefore, we only apply the depth refinement with a 2D CNN to the last stage for efficiency in time and memory.
\newpage
\subsection{Qualitative results on DTU and Tanks \& Temples}
\label{a5}
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.32]{figures/dtu.PNG} \\
\includegraphics[scale=0.475]{figures/dtu_qualitative_results.png}
\end{center}
\caption{Qualitative results on DTU evaluation dataset. Our method achieved the high completeness of reconstructed models. The visualization of reconstructed models for DTU evaluation set are shown in the top figure. A comparison with the other state-of-the-art methods is shown in the bottom figure.}
\label{dtu_results}
\end{figure}
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.45]{figures/tanksandtemples.png} \\
\includegraphics[scale=0.32]{figures/tt_recall.png}
\end{center}
\caption{Qualitative results on the intermediate set of Tanks \& Temples. Our method achieved the high completeness of reconstruction for complex outdoor scenes. The bottom figure shows the error visualization for Recall scores compared to AttMVS \citep{luo2020attention} and Vis-MVSNet \citep{zhang2020visibility}}
\label{tt_results}
\end{figure}
\end{document}
