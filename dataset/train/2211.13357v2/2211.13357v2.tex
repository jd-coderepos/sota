\section{Experimental Results}\label{sec:exp}

In this section, we first discuss the datasets we used in pre-training and fine-tuning. We then present the performance comparison with existing state-of-the-arts on public benchmarks. Finally, we provide a detailed ablation study to verify the effectiveness of the proposed training scheme.



\subsection{Datasets and Evaluation Metrics}

We conduct pre-training on AMASS collection~\cite{mahmood2019amass}, which consists of  MoCap datasets. AMASS provides a unified human pose and mesh representations based on SMPL~\cite{loper2015smpl}. Each sequence records a motion movement of a subject. In total, there are  subjects with  motions. The total length of all the sequences is about  minutes. In total, there are about  frames, and each frame has a human mesh. To avoid the redundancy between the neighbor frames, we sparsely sample 2 million meshes from AMASS. After projection using 4 virtual cameras, we obtain 8 million heatmap-mesh pairs for pre-training. 


We fine-tune our model using a mixture of 2D and 3D datasets, including Human3.6M~\cite{ionescu2014human3}, MuCo-3DHP~\cite{mehta2018single}, UP-3D~\cite{lassner2017unite},  COCO~\cite{lin2014microsoft}, and MPII~\cite{andriluka14cvpr}. Note that these datasets are commonly used in literature~\cite{Choi_2020_ECCV_Pose2Mesh,lin2020end,lin2021mesh,cho2022FastMETRO}. After that, we evaluate our model on Human3.6M using P2 protocol~\cite{kanazawa2018end,kolotouros2019learning}. When conducting experiments on 3DPW~\cite{vonMarcard2018}, we follow the prior works~\cite{kocabas2019vibe,lin2020end,lin2021mesh,cho2022FastMETRO} and fine-tune with 3DPW training data. We then evaluate the results on 3DPW test set. 


Following literature~\cite{kanazawa2018end, kolotouros2019convolutional,kolotouros2019learning,cho2022FastMETRO,li2022cliff}, we use three standard metrics for evaluation, including Mean Per Joint Position Error (MPJPE)~\cite{ionescu2014human3}, Procrustes Analysis with MPJPE (PA-MPJPE)~\cite{zhou2018monocap}, and Mean Per Vertex Error (MPVE)~\cite{pavlakos2018learning}. The unit of the metrics is millimeter (mm).


\begin{table}[t]
\centering
\tablestyle{0.8pt}{1.1} 
\begin{tabular}{lcccccc}
    \toprule[1.5pt]
    \multirow{1}{*}{} & \multicolumn{3}{c}{3DPW} & & \multicolumn{2}{c}{Human3.6M}\\ 
    \cline{2-4}\cline{6-7}
	Method  & MPVE  & MPJPE  & PA-MPJPE  & &  MPJPE  & PA-MPJPE  \\
	\midrule
	HMR~\cite{kanazawa2018end} &  &  & 81.3 && 88.0 & 56.8  \\
	GraphCMR~\cite{kolotouros2019convolutional} &  &  & 70.2 &&  & 50.1\\
	SPIN~\cite{kolotouros2019learning} & 116.4 & 96.9 & 59.2 && 62.5 & 41.1\\
	Pose2Mesh~\cite{Choi_2020_ECCV_Pose2Mesh} &  & 89.2 & 58.9 && 64.9 & 47.0\\
	I2LMeshNet~\cite{Moon_2020_ECCV_I2L-MeshNet} &  & 93.2 & 57.7 && 55.7 & 41.1\\
	PyMAF~\cite{zhang2021pymaf} & 110.1 & 92.8 & 58.9 && 57.7 & 40.5\\
	ROMP~\cite{sun2021monocular} & 93.4 & 76.7 & 47.3 &&  & \\
	VIBE~\cite{kocabas2019vibe} & 99.1 & 82.0 & 51.9 && 65.6 & 41.4\\
    METRO~\cite{lin2020end} & 88.2 & 77.1 & 47.9 && 54.0 & 36.7\\
    THUNDER~\cite{zanfir2021thundr} & 88.0 & 74.8 & 51.5 && 48.0 & 34.9\\
    PARE~\cite{kocabas2021pare} & 88.6 & 74.5 & 46.5 &&  & \\
    Graphormer~\cite{lin2021mesh} & 87.7 & 74.7 & 45.6 && 51.2 & 34.5\\
	FastMETRO~\cite{cho2022FastMETRO} & 84.1 & 73.5 & 44.6 && 52.2 & 33.7\\
	CLIFF~\cite{li2022cliff} & 81.2 & 69.0 & 43.0 && 47.1 & 32.7\\
    \midrule
    MPT (Ours) &  &  &  &&  & \\
	\bottomrule[1.5pt]
\end{tabular}
\caption{Performance comparison with the previous state-of-the-art methods on 3DPW and Human3.6M datasets.}
\label{tbl:compare-h36m-3dpw}
\vspace{-4mm}
\end{table}

 
\subsection{Main Results}

We compare our method with the existing state-of-the-art approaches on Human3.6M~\cite{ionescu2014human3} and 3DPW~\cite{vonMarcard2018} datasets. We present our pretrain-then-finetune results in Table~\ref{tbl:compare-h36m-3dpw}. Our method outperforms the previous works on both datasets, including the recent transformer-based methods~\cite{lin2020end,lin2021mesh,zanfir2021thundr,cho2022FastMETRO}. 

It is worth noting that, CLIFF~\cite{li2022cliff} was the state-of-the-art approach on the two datasets. CLIFF additionally leverages the camera focal length and bounding box information to calculate 2D re-projection loss on the non-cropped input image. In contrast, we do not have such post-processing, and still achieve better results, especially on MPJPE.


\subsection{Analysis}

\Paragraph{Effectiveness of Mesh Pre-Training:} We study whether our pre-training is useful for performance improvements. We conduct experiments on Human3.6M, and Table~\ref{tbl:pretrain-analysis} shows the comparison with different training configurations including with or without mesh pre-training, and different datasets for fine-tuning. Because our model architecture is similar to METRO~\cite{lin2020end}, we include it as the reference. We also include the well-known HMR~\cite{kanazawa2018end} as reference.

In Table~\ref{tbl:pretrain-analysis}, our MPT improves the performance across different training configurations considered. When fine-tuning our pre-trained model using Human3.6M only, as shown in the sixth row, MPT achieves  PA-MPJPE, which is better than  PA-MPJPE of our non-pretrain model. We observe similar findings when using multiple datasets for fine-tuning. Our MPT achieves  PA-MPJPE, and is better than  PA-MPJPE of our non-pretrain model. 



\Paragraph{Testing without Fine-Tuning:} Given the pre-trained MPT model, we directly evaluate MPT model on real images without any fine-tuning. As shown in the fifth row of Table~\ref{tbl:pretrain-analysis}, we obtain a performance of  PA-MPJPE. Although it lags behind the supervised fully fine-tuned models, the result is comparable to the well-known HMR~\cite{kanazawa2018end}. Figure~\ref{fig:zero-shot} shows the qualitative results. We see that MPT is capable of generating human meshes on real images without the need of fine-tuning.

\begin{table}[t]
\tablestyle{1pt}{1.1} 
\centering
\begin{tabular}{lcccc}
    \toprule[1.5pt]
	Method  & MPT & FT & MPJPE  & PA-MPJPE  \\
	\midrule
	HMR~\cite{kanazawa2018end} & \xmark & Mixed Datasets & 88.0 & 56.8\\
	METRO~\cite{lin2020end} & \xmark & Mixed Datasets & 54.0 & 36.7\\
	\midrule
	MPT  & \xmark & Human3.6M & 59.1 & 39.2\\
	MPT  & \xmark & Mixed Datasets & 46.6 & 32.4\\
	\midrule
	MPT (Test w/o fine-tune) & \cmark & \xmark & 88.9 & 58.4\\
	MPT  & \cmark & Human3.6M & 53.3 & 35.5\\
	MPT  & \cmark & Mixed Datasets & 45.3 & 31.7\\
	\bottomrule[1.5pt]
\end{tabular}
\caption{Pre-training analysis. We conduct training with different configurations, and then evaluate results on Human3.6M validation set. MPT: Mesh Pre-Training. FT: Fine-Tuning.}
\label{tbl:pretrain-analysis}
\vspace{-4mm}
\end{table}
 \begin{figure}[t]
\vspace{-2mm}
\centering
 \subfloat[(a)][Learning Convergence\label{fig:converge}]{
   \includegraphics[trim=0 0 0 0, clip,width=0.48\linewidth]{figs/convergence.pdf}
 }
 \subfloat[(b)][Fine-Tune with Less Data\label{fig:fewshot}]{
   \includegraphics[trim=0 0 0 0, clip,width=0.48\linewidth]{figs/fewshot.pdf}
 }
\caption{
Fine-tuning behavior on Human3.6M. \textbf{(a)} We conduct a 1-epoch fine-tuning and report PA-MPJPE for each fine-tuning step. \textbf{(b)} We use a percentage of Human3.6M data for fine-tuning and report PA-MPJPE. }
\vspace{-4mm}
\end{figure}

\begin{figure*}[t]
\begin{center}
	\centering
\includegraphics[trim=0 0 0 0, clip,width=0.495\textwidth]{figs/compare1.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.495\textwidth]{figs/compare2.jpg}\\
\includegraphics[trim=0 0 0 0, clip,width=0.495\textwidth]{figs/compare3.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.495\textwidth]{figs/compare4.jpg}
\resizebox{1.\textwidth}{!}{
\setlength{\tabcolsep}{25pt}
\begin{tabular}{cccccc}
\footnotesize{Input} & \footnotesize{CLIFF} &  \footnotesize{Ours}  &  \footnotesize{Input} &  \footnotesize{CLIFF} & \footnotesize{Ours}
\end{tabular}}
\vspace{-4mm}
\captionof{figure}{
	Qualitative comparison. For each example, we show the results from CLIFF~\cite{li2022cliff} and our proposed MPT. Both CLIFF and MPT generate good quality human meshes, but MPT has more favorable body pose. Tow row: 3DPW. Bottom row: Human3.6M.}
	\label{fig:qualitative-compare}
\end{center}\vspace{-6mm}
\end{figure*}




\begin{figure*}
\begin{center}
\rotatebox{90}{ \ \ \ \ \ \ Freeze} \includegraphics[trim=0 0 0 0, clip,width=0.97\textwidth]{figs/heatmaps_freeze/10-Joshua-Farris_meshpt_pred.jpg.all.jpg_sampled.jpg}
\rotatebox{90}{ \ \ \ \ FT (Ours)} \includegraphics[trim=0 0 0 0, clip,width=0.97\textwidth]{figs/heatmaps_ours/10-Joshua-Farris_meshpt_pred.jpg.all.jpg_sampled.jpg}
\rotatebox{90}{ \ \ \ }\resizebox{1.\textwidth}{!}{
\setlength{\tabcolsep}{20pt}
\begin{tabular}{ccccccccc}
 \ \ Input & \ \ L-Ear & \ \ L-Sho & \ \ L-Elb & \ \ L-Wri & \ \ L-Hip & L-Ank & Output
\end{tabular}}
\vspace{-4mm}
\caption{
\revise{Visualization of pose feature maps. In the top row, we freeze the backbone and fine-tune only the mesh regression transformer. However, we observe that the pose feature maps behave like conventional heatmaps, detecting one joint per map and resulting in incorrect reconstruction. In the bottom row, we unfreeze the backbone and perform end-to-end fine-tuning. Our model learns to extract pose feature maps where each map captures information on multiple body joints, leading to improved reconstruction and more accurate pose estimation. 
More visualizations are provided in the supplementary material.}
} 
\label{fig:heatmap}
\vspace{-12mm}
\end{center}
\end{figure*}  
\begin{table*}[t!]\centering
\subfloat[Comparison of Different Pre-Training: MoCap-generated Heatmaps vs. 2D Coordinates 
\label{tbl:pose2mesh}]{\tablestyle{1pt}{1.1}
\begin{tabular}{lcc}
    \toprule[1.5pt]
	Pre-Training & MPJPE  & PA-MPJPE  \\ \midrule
	2D Coordinates  & 139.3 & 88.9\\
	MoCap-gen. Heatmaps &  88.9 & 58.4\\
	\bottomrule[1.5pt]
\end{tabular}
}
\hfill
\subfloat[Different Inputs During Fine-Tuning \label{tbl:module-pretrain}]{\tablestyle{2pt}{1.1}
\begin{tabular}{lcccc}
\multicolumn{1}{c}{} \\ \toprule[1.5pt]
	Fine-Tuning & MPT & FT & MPJPE  & PA-MPJPE  \\
	\midrule
	Image Features & \xmark & \cmark & 78.0 & 47.0\\
	Pose FeatMaps & \xmark & \cmark & 58.7 & 39.7\\
	Pose FeatMaps & \cmark & \cmark & 53.3 & 35.5\\
	\bottomrule[1.5pt]
\end{tabular}
}
\hfill
\subfloat[Masked Heatmap Modeling 
\label{tbl:mjm}]{\tablestyle{4pt}{1.1}
\begin{tabular}{ccc}
    \multicolumn{1}{c}{} \\ \toprule[1.5pt]
	Method & MPJPE  & PA-MPJPE  \\
	\midrule
	MPT w/o MHM & 97.7 & 64.4\\
	MPT w/ MHM & 88.9 & 58.4\\
	\bottomrule[1.5pt]
	\multicolumn{1}{c}{} \\ \end{tabular}


}
\caption{
\textbf{(a)} We study pre-training using different input representations to the mesh regression transformer. We evaluate pre-training performance (\textit{i.e.,} testing without fine-tuning) on Human3.6M. \textbf{(b)} During fine-tuning, we study the effect of different input representations to the mesh regression transformer, including image features and pose feature maps. We conduct fine-tuning and evaluation on Human3.6M. \textbf{(c)} Ablation study of Masked Heatmap Modeling. We evaluate our pre-trained MPT on Human3.6M without any fine-tuning.
\label{tab:ablation_heatmaps}
}
\vspace{-4mm}
\end{table*}
 

\begin{table}[t]
\tablestyle{2pt}{1.1} 
\centering
\begin{tabular}{lccc}
    \toprule[1.5pt]
	Backbone & FT & MPJPE  & PA-MPJPE  \\
 	\midrule
	Freeze & \cmark & 78.1 & 53.5\\
        Unfreeze & \cmark & 53.3 & 35.5\\
	\bottomrule[1.5pt]
\end{tabular}
\caption{Comparison between freeze and unfreeze backbone during fine-tuning. We conducted fine-tuning on Human3.6M.
}
\label{tbl:ft_before_after}
\vspace{-5mm}
\end{table} \Paragraph{Learning Convergence:}
We study the impact of MPT during fine-tuning. We perform a 1-epoch fine-tuning on Human3.6M, and report results for each fine-tuning step. Figure~\ref{fig:converge} shows that, with our pre-training, the fine-tuning converges better than that without pre-training.

\Paragraph{Fine-Tuning with Less Data:} We select 0.1\%, 1\%, and 10\% of Human3.6M training data for fine-tuning, respectively. Figure~\ref{fig:fewshot} shows that our pre-training helps improve the learning performance when less data is used during fine-tuning. For example, our pre-trained model with 0.1\% fine-tune data achieves 56.1 PA-MPJPE, which is much better than 210.4 PA-MPJPE of our non-pretrained model with 10\% fine-tune data. 




\Paragraph{Pre-Training with MoCap-generated Heatmaps:} Since we use MoCap-generated heatmaps during our pre-training, one important question is that what if we directly input 2D joint coordinates to the mesh regression transformer. Table~\ref{tbl:pose2mesh} shows the comparison of the pre-training performance (\textit{i.e.,} testing without fine-tuning) on Human3.6M. We observe that the use of MoCap-generated heatmaps effectively improves the pre-training performance.


\Paragraph{Fine-Tuning with Pose Feature Maps:} \revise{During the fine-tuning stage, our mesh regression transformer takes pose feature maps as inputs. One may wonder what if we use image feature maps instead, as discussed in Mesh Graphormer~\cite{lin2021mesh}. To investigate the impact of this design choice, we conducted an ablation study on Human3.6M, as shown in Table~\ref{tbl:module-pretrain}. The results indicate that using pose feature maps during the fine-tuning stage leads to better performance across all considered metrics.}

\Paragraph{Analysis of Pose Feature Maps:} \revise{
In Figure~\ref{fig:heatmap}, we present a visualization of the pose feature maps during fine-tuning. The top row of Figure~\ref{fig:heatmap} shows that when the backbone is frozen during fine-tuning, the pose feature maps behave like traditional heatmaps, capturing a single body joint location per map. We can see that the reconstructed mesh is not correct due to the side view angle and self occlusions. In contrast, as shown in the bottom row of Figure~\ref{fig:heatmap}, when we fine-tune both the backbone and the mesh regression transformer, our model learns to extract pose feature maps where each map captures information on multiple body joints, resulting in improved reconstruction and more accurate pose estimation. For example, in order to predict the location of the left ear, our model pays attention to not only the keypoints on the face, but also the body joints on the arms and legs. In Table~\ref{tbl:ft_before_after}, we provide a quantitative comparison between freezing and unfreezing backbone during fine-tuning. The results suggest end-to-end fine-tuning improves the reconstruction performance.}


\begin{figure*}
\begin{center}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch112.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch248.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch250.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch272.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch320.jpg}\\
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch334.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch342.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch355.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch370.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch92.jpg}\\
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch4.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch42.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch346.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch262.jpg}
\includegraphics[trim=0 0 0 0, clip,width=0.195\textwidth]{figs/hand/freihand_results_sc10_rot0_batch304.jpg}
\caption{
Qualitative results of MPT applying to 3D hand reconstruction on FreiHAND dataset.  
} 
\label{fig:hand}
\vspace{-6mm}
\end{center}
\end{figure*}  

\begin{table}
\tablestyle{8pt}{1.1} 
\centering
\resizebox{1.\columnwidth}{!}{
    \begin{tabular}{lccccc}
	\toprule[1.5pt]
	 Percentage of PT Data &  &  &  &  & \\
	 \midrule
	 PA-MPJPE  & 63.4 & 62.4 & 59.9 & 59.4 & 58.4\\
	\bottomrule[1.5pt]
\end{tabular}
}	
\caption{Analysis of pre-training data size. We conduct pre-training using different percentage of data, and evaluate the pre-trained MPT on Human3.6M without fine-tuning.}
\label{tbl:num_pt_data}
\vspace{-4mm}
\end{table} \begin{table}[t]
\tablestyle{0.8pt}{1.1} 
\centering
\begin{tabular}{lcccc}
    \toprule[1.5pt]
	Method  & PA-MPVPE  & PA-MPJPE  & F@5 mm  & F@15 mm \\
	\midrule
	Hasson et al~\cite{hasson2019learning} & 13.2 &  & 0.436 & 0.908\\
	Boukhayma et al.~\cite{boukhayma20193d} & 13.0 &  & 0.435 & 0.898\\
	FreiHAND~\cite{zimmermann2019freihand} & 10.7 &  & 0.529 & 0.935\\
	Pose2Mesh~\cite{Choi_2020_ECCV_Pose2Mesh} & 7.8 & 7.7 & 0.674 & 0.969\\
	I2LMeshNet~\cite{Moon_2020_ECCV_I2L-MeshNet} & 7.6 & 7.4 & 0.681 & 0.973\\
    METRO~\cite{lin2020end}  & 6.8 & 6.7 & 0.717 & 0.981\\
    Tang \textit{et al.}~\cite{tang2021towards}  & 6.7 & 6.7 & 0.724 & 0.981\\
    FastMETRO~\cite{cho2022FastMETRO} &  & 6.5 &  & 0.982\\
    Graphormer~\cite{lin2021mesh}   & 6.0 & 5.9 & 0.764 & 0.986\\
    MobRecon~\cite{MobRecon} & 5.7 & 5.8 & 0.784 & 0.986\\
    \midrule
    MPT (Ours) &  &  &  & \\
	\bottomrule[1.5pt]
\end{tabular}
\caption{Performance comparison with the previous state-of-the-art methods on FreiHAND dataset. }
\label{tbl:compare-hand}
\vspace{-4mm}
\end{table} 


\Paragraph{Effectiveness of Masked Heatmap Modeling:} Table~\ref{tbl:mjm} shows an ablation study of the proposed Masked Heatmap Modeling (MHM), tested on Human3.6M. We observe MHM improves the reconstruction performance by a large margin, especially on PA-MPJPE metric.




\Paragraph{Pre-Training Data Size:}
Since we use 2 million meshes for pre-training, an interesting question is how much data should we use for pre-training. To answer the question, we conduct pre-training using different percentage of the 2 million meshes. We then evaluate our MPT model directly on Human3.6M validation set without fine-tuning. Table~\ref{tbl:num_pt_data} shows that increasing the number of meshes for pre-training can improve the performance on Human3.6M. The results suggest that we could scale-up our pre-training data for further performance improvements, and we leave it as future work. 

\Paragraph{Number of Virtual Camera Views:}
We also study the effect of different number of virtual camera views. Table~\ref{tbl:cam_views} shows that adding more camera views can improve the performance. The results suggest increasing the training data diversity is beneficial to our pre-training. 



\Paragraph{Backbone Analysis:} As we use HigherHRNet as our backbone to extract feature maps, one may wonder what if we use other backbones. In Table~\ref{tbl:backbones}, we replace HigherHRNet with an early baseline called SimpleBaseline~\cite{xiao2018simple}. Two backbones have quite different mAPs on COCO keypoint dataset. But after adding the backbone to MPT for training, both MPT variants give similar results, which suggests MPT is not very sensitive to the backbone choices.


\Paragraph{Qualitative Results:} Figure~\ref{fig:qualitative-compare} shows the  qualitative results of MPT compared with CLIFF~\cite{li2022cliff} on Human3.6M and 3DPW datasets. While both methods can generate good quality human meshes, MPT has more favorable body poses when there are self-occlusions in the images. 

\begin{table}
\tablestyle{8pt}{1.1} 
\centering
\begin{tabular}{lccc}
	\toprule[1.5pt]
	 Number of Views & 1 & 2 & 4\\
	 \midrule
	 PA-MPJPE  & 61.7 & 61.4 & 58.4 \\
	\bottomrule[1.5pt]
\end{tabular}
\caption{Ablation study of numbers of virtual camera views. We evaluate the pre-trained MPT on Human3.6M without fine-tuning.}
\vspace{-0mm}
\label{tbl:cam_views}
\end{table} \begin{table}[t]
\tablestyle{2pt}{1.1} 
\centering
\begin{tabular}{lccc}
    \toprule[1.5pt]
	Backbone & & MPJPE  & PA-MPJPE  \\
 	\midrule
	SimpleBaseline~\cite{xiao2018simple} &  & 45.8 & 32.8\\
	HigherHRNet~\cite{cheng2020higherhrnet} & & 46.6 & 32.4\\
	\bottomrule[1.5pt]
\end{tabular}
\caption{Comparison of different human pose networks. We conduct end-to-end fine-tuning on mixed 2D/3D datasets, and evaluate on Human3.6M. No pre-training in this ablation study. 
}
\label{tbl:backbones}
\vspace{-2mm}
\end{table} 
\Paragraph{Applying to 3D Hand Reconstruction:} MPT is a generic pre-training scheme for human mesh regression task. We demonstrate the flexibility of MPT on 3D hand reconstruction, and we conduct the experiments on FreiHAND~\cite{zimmermann2019freihand} dataset. Since there is less MoCap data for 3D hands, we use a recently proposed synthetic dataset called Complement~\cite{MobRecon} for pre-training. Table~\ref{tbl:compare-hand} shows the performance comparison with previous works. MPT outperforms the previous state-of-the-art methods, including MobRecon~\cite{MobRecon} which also uses Complement as training data. Figure~\ref{fig:hand} shows the qualitative examples of our hand reconstruction.


