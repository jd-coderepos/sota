\documentclass[copyright, creativecommons]{eptcs}
\providecommand{\event}{AFL 2014} \usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{subfigure}

\newcommand{\qed}{}
\newcommand{\cBC}{C_\mathrm{BC}}
\newcommand{\cMEBC}{C_\mathrm{MEBC}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\title{Boolean Circuit Complexity of Regular Languages}
\author{Maris Valdats
\institute{University of Latvia\\
Faculty of Computing\\
Riga, Rai\c{n}a Bulv. 19, Latvia\\}
\email{d20416@lanet.lv}}

\def\titlerunning{Boolean Circuit Complexity of Regular Languages}
\def\authorrunning{M. Valdats}
\begin{document}
\maketitle

\begin{abstract}
In this paper we define a new descriptional complexity measure for
Deterministic Finite Automata, BC-complexity, as an alternative to the state complexity.
We prove that for two DFAs with the same number of states BC-complexity can differ exponentially.
In some cases minimization of DFA can lead to an exponential increase in BC-complexity,
on the other hand BC-complexity of DFAs
with a large state space which are obtained by some standard constructions (determinization of NFA, language operations),
is reasonably small.
But our main result is the analogue of the "Shannon effect" for finite automata:
almost all DFAs with a fixed number of states have BC-complexity that is close to the maximum.
\end{abstract}
\label{1nod}
 State complexity of deterministic finite automata (DFA)~\cite{B70}\cite{H79} has been analyzed for more than
50 years and all this time has been
the main measure to estimate the descriptional complexity of finite automata. Minimization algorithm~\cite{H71}
for it was developed as well as methods to prove upper and lower bounds for various languages.

It is hard to
find any evidence of another complexity measure for finite automata.
Transition complexity~\cite{G05} could be one, it counts the number
of transitions, but there is not much use of it for DFAs (it is proportional to the state complexity),
it is used in the nondeterministic case.

But intuitively not all DFAs with the same number of states have the same complexity.
We try to illustrate it with the following example.

Consider a DFA that recognizes a language in the binary alphabet
which consists of words in which there is an even number of ones
among the last 1000 input letters. One can easily prove that it needs  states,
however such a DFA can easily be implemented by keeping its
state space in a 1000 bit register which remembers the last 1000 input letters.

On the other hand, consider a "random" DFA with a binary input tape and  states. There is essentially no better way
to describe it as with its state transition table which consists of  lines which (as it is widely
assumed) is more than particles in our universe.


It is easy to represent a large number of states in a compact form:  states fit into  state bits
of the state register. This is true for the "random" DFA as well. But the computation performed by the transition function
on this register can be very easy in some cases and hard in some other.
Therefore it seems natural to introduce a complexity measure for DFAs which
measures the complexity of the transition function. 

Automata with a large state space which is kept in a state register have been used before, but
not in the widest sense. One example of such a usage is FAPKC~\cite{TR09}
(Finite Automata Public Key Cryptosystem), a public key cryptosystem
developed in the 80's by Renji Tao. In FAPKC the state space of an automaton is
considered to be a vector space and the transition function is expressed as a polynomial over a finite field.

In this paper we consider the following model: we arbitrarily encode the state space into
a bit vector (state register) and express the transition function as a Boolean circuit.
The BC-complexity of the DFA is (approximately) the complexity of this circuit and this notion extends to regular
languages in a natural way.

BC-complexity was first analyzed in~\cite{V11} where it was considered for transducers. Here we define
it for DFAs what allows to extend the definition to regular languages.

The main result of this paper is the Shannon effect for the BC-complexity of regular languages:
it turns out that most of the languages have BC-complexity that is close to the maximum. To obtain it we first
estimate upper (and lower) bounds for BC-complexity compared to state complexity (Theorem.~\ref{apaksRobMinim}),
afterwards by counting argument we show that the complexity of most of the languages is around this upper bound.

Influence of state minimization to BC-complexity were analyzed already in~\cite{V11} for transducers and for DFAs
it is essentially the same: it turns out that for some regular languages BC-complexity of their minimal automaton
is much (superpolynomially) larger than BC-complexity for some other (non-minimal) DFA that recognizes it.
Finally we look how BC-complexity behaves if we do some standard constructions on automata
(determinization of an NFA, language operations).



\section{Preliminaries}
\label{2nod}
\subsection{Finite Automata and Regular Languages}
We use a standard notion of DFA~\cite{M55}, it is a tuple , where
 is the state space,
 is the input alphabet,
 is the transition function,
 is the start state and
 is the set of accepting states.


DFA starts computation
in the state  and in each step it reads an input letter   and changes its state.
If the current state of a DFA is  and it reads an input letter  then it
moves to state . If after reading the input word DFA
is in a state  then this word is accepted, otherwise it is rejected. DFA  recognizes
language  iff it accepts all words from this language and rejects all words not in the language.
Two DFAs that recognize the same language are called equivalent.
 
The state complexity of a DFA is the number of states in its state space . For each DFA 
there is a unique minimal DFA  which is equivalent to  and has minimal state complexity.
There is an effective minimization algorithm for finding it~\cite{B70}.

We will need the estimation of the number of DFAs with  states.
Denote  to be
the number of pairwise non-equivalent minimal DFAs with  states over -letter alphabet.
In \cite{S01} it is estimated to be larger
than , we will use the following reduced estimation (true for ) which will be sufficient for us:
\begin{theorem}[\cite{S01}]
\label{autSkaits}
 for .
\end{theorem} 

\subsection{Boolean circuits}

We will use the standard notion of a Boolean circuit
and restrict our attention to circuits in the standard base (, , ).
The size of the circuit  is
the number of gates plus the number of outputs of the circuit .
Boolean circuit  with  inputs and  outputs represents a Boolean function  in a natural way.

Each function  can be represented by a Boolean circuit in (infinitely many) different ways.
The complexity of this function  is the size of the smallest circuit that represents this function.

We will also need a formula for the upper bound of the number of different Boolean circuits with a given complexity .
Denote  to be the number of circuits with  input variables,
 output variables and no more than  gates, that correspond to different Boolean
functions. Then:
\begin{theorem}
\label{shemuSkaits}

\end{theorem}
\begin{proof}
Assign to inputs numbers from 1 to , and numbers from  to  to the gates. Each gate is characterised
with its two inputs (at most  possibilities) and type (AND, OR, NOT, 3 possibilities). There are no more
than  ways how to assign outputs of the circuit and each circuit is counted  times, one for each
numbering of gates. Therefore the total number of circuits can be estimated as:

here we have used, that  for all .

Further, as  for arbitrary , then

from where the result follows.
\qed
\end{proof}

A classical result about Boolean functions states that most of functions  have
approximately the same circuit complexity which is close to maximum. This property is called Shannon effect.
\begin{theorem}[\cite{L84}]
\label{ShTeorema}
For any Boolean function 

For almost all  Boolean functions 

\end{theorem}
Here and further  and we use the notation












\section{Encodings and Representations of a DFA}
\label{3nod}
Classical representations of automata are table forms or state transition diagrams. They are essentially
the same, a state diagram can be thought of as a visualization of a table form. Table form lists
the transition function of an automaton as a table where each line corresponds to a pair of state and input letter.
In state transition diagram each state is denoted by a circle and for each
transition  an arrow is drawn from state  to state  above which letter  is written.

Both of these representations show each state of an automaton separately, therefore with these methods
it is not possible to effectively describe an automaton with a large number of states.

One can encode  states into  (or more) state bits which can be kept in a
 \textit{state register}. Also, input letters can be encoded as a bit vectors. Every automaton
has infinitely many such encodings.

The transition function in this case will take as an input a state register and
an encoded input letter, and produce a (next) state register.
It is thus a Boolean function and it is natural to represent it with a Boolean circuit.

Another question is how to represent the set of accepting states . We represent it
by a Boolean circuit implementing its characteristic function.
Therefore a representation of
a DFA will consist of an encoding of its state space and input alphabet and two circuits:
one for its transition function and one for the characteristic function
of the set of accepting states.
We call these circuits \textit{transition circuit} and \textit{acceptance circuit}, respectively.


An encoding  of a set X onto a binary string is an injective mapping 
where  is the length of the encoding. As the mapping is injective then .

An encoding of a DFA consists of an encoding of its input alphabet  and an encoding of the state space 
which we call input encoding and state encoding, respectively. Additionally for the state encoding
we ask that the start state is encoded as a string of all zeros .


\begin{definition}
Let  be a given DFA and  be its encoding.
A pair of Boolean circuits  is a representation of  under encoding  iff
\begin{itemize}
\item  has  input variables and  output variables,
\item  has  input variables and one output variable,
\item for all  and  if , then ,
\item  for all .
\end{itemize}
\end{definition}

In other words, transition circuit 
reads encoded input  as its first  input bits, encoded state  as following
 input bits and has encoded next state  as its  output bits.
Acceptance circuit  reads encoded state   and
outputs 1 as its only output bit iff .


As noted before minimal values for  and  are 
and  respectively,
but they can be larger as well.
Whether allowing them to be larger gives a possibility to construct smaller representations of DFAs,
is an interesting open question.

It is natural to encode the state space  with  lexicographically first bit strings 
of length , in such a case we will say that the state encoding is minimal.
The notion of minimal input encoding is introduced similarly.
We call an encoding of a DFA \textit{minimal encoding} if both encodings: state and input are minimal.


\section{BC-complexity}
\label{4nod}
In this section we define the main concept of this paper, BC-complexity of a DFA.
We start from the bottom:
\begin{definition}
BC-complexity of a representation of a DFA  is the sum of complexities
of its transition circuit and acceptance circuit and the number of state bits:

\end{definition}

The number of state bits  is included in the definition to avoid situation that an automaton
has a large number of states but zero BC-complexity. It is natural to assume that it costs something
to create a circuit even if it has no gates and this is one of the possibilities how to reflect
this in the definition. Another possibility would be to use the complexity of "wires"
instead of the complexity of gates for the underlying circuits, but we prefer to use the standard
complexity for the circuits.


\begin{definition}
BC-complexity of a DFA ,
, is the minimal BC-complexity of its representations:

\end{definition}

Although the name "circuit complexity" also sounds reasonable,
we use the abbreviation "BC-complexity" to avoid confusion
with the circuit complexity of regular languages.
\begin{definition}
BC-complexity of a regular language  is the minimal BC-complexity of all DFAs
that recognize :

\end{definition}


First we observe that we can optimize our acceptance circuit by rearranging states. If we encode states
in such a way that all accepting states have smaller index than rejecting states (or vice versa) then the acceptance circuit
can be reduced to a comparison operation whose complexity is not greater than  where  is the number of state bits. 


But this is not the best optimization that can be achieved by rearranging states.
For the upper bound in the following Theorem~\ref{PFrobezas} different arrangement is used.

\begin{theorem}
\label{PFrobezas}
If  then for any DFA  with  states, 

If  then for any DFA  with  states, 


\end{theorem}
\begin{proof}
Lower bound. For any representation  there are  state bits,
therefore BC-complexity cannot be smaller than .

For upper bound if we just construct an optimal representation  under
some arbitrary minimal encoding (with  state bits)
then the BC-complexity of this representation according to Theorem~\ref{ShTeorema} can be estimated as


To improve the result to  we will choose a specific minimal encoding where states are ordered in
a way that for one input letter the corresponding transition function is simple.
Denote  to be the encoding of the current state,  to be the encoding of the next state and  to be
the encoding of the input letter.
We split the transition circuit  in two parts  and  where part 
computes the next state for one specific input letter  and part  does it for other  input letters.


If we look at the state transition graph for the letter  then it
splits into connected components each of which has the form

where .
Each such component is uniquely defined with two numbers  (the number of states in it) and 
(the length of the "tail"), we call  to be the length of a component.
We order all these components by  and  lexicographically what naturally leads
to the ordering of states. Consider all components with parameters  and denote by  the index (encoding)
of the first state of the first such component and by  the index of the last state of the last such component.

The transition function is  except for the last state  of each component
for which it is . As each of these components have  states then 
corresponds to the last state of some component iff .

The circuit  should compute the following function :
\begin{verbatim}
q' = q+1
for all pairs (m, j)
  if M(m, j)<=q<=N(m, j) and q+1 == M(m, j) mod m:
    q' = q-(m-j)
\end{verbatim}

Here  and  are the boundaries within which all components with parameters  are placed.
It is easy to check that circuits for subtraction  and comparison  are of size ,
for modulo comparison  it is of size . Therefore
the total size of the circuit  is

where  is the number of different pairs  that correspond to some components
that are present in the transition graph and 
is some constant. We need to estimate the maximum value of .

One can easily see that maximum value of  is obtained when
each component with parameters  appears exactly once and all the smallest components are used.
Let  be the maximum length of a component (maximal value of ) under the condition
that all the possible smallest components are used.
For each  there are  possible different types of components () therefore 
. 

On the other hand the total length of all components up to the length  should be less than :

whence it follows that .
Therefore
.

The size of the transition circuit  for the other  input letters can be estimated from Theorem~\ref{ShTeorema}:


The size of the acceptance circuit can be estimated (from Theorem~\ref{ShTeorema}) as
.

After reordering of states we also have to ensure that the start state is 0. This
can increase the complexity of both circuits by no more than  (it is
necessary to add at most  negations to the input of the transition circuit , the output of the transition
circuit , and the input of the acceptance circuit ). There are also
 state bits which are included in the computation of BC-complexity.
We omit these terms of logarithmic order in the computation of BC-complexity because
asymptotically they are negligible.

The BC-complexity of the automaton therefore can be estimated as


If  then the dominant term of this expression is  and .
For one letter alphabet the dominant term is , therefore .
\qed
\end{proof}


Consider language  in binary alphabet  such that 
iff  and  (the -th letter from the end is "1"). The state complexity
of this language is , one has to remember in a state register the last  input letters. But the BC-complexity of it is .
Circuits  that represent the natural encoding of a DFA  that recognizes  have no gates,
they are shown in figure~\ref{ShiftN}. Therefore the BC-complexity of (the representation  of) 
is the number of state bits which is .
This example shows that the lower bound of Theorem~\ref{PFrobezas} is strictly reachable.
\begin{figure}[htb]
	\centering
		\includegraphics[width=0.4\textwidth]{bildes/ShiftNbezI.eps}
	\caption{Representation  of the DFA }
	\label{ShiftN}
\end{figure}

Further we try to reach the upper bound. First we find a language
(based on the Shannon function) for which the BC-complexity is at least ,
afterwards by counting argument we show that BC-complexity for most
languages is close to . That matches the upper bound of Theorem~\ref{PFrobezas}
and can be thought of as the Shannon effect for BC-complexity.

Denote by  the Shannon function on  bits: lexicographically first Boolean function with  input
bits and one output bit with maximal complexity of its minimal circuit.
Consider a language  that consists of all words  in binary alphabet such that
. 
State complexity of this language is not larger than : it is enough to remember the last  input letters.
But its BC-complexity is at least .

\begin{theorem}
\label{e2}
BC-complexity of  is at least .
\end{theorem}
\begin{proof}
Let  be a pair of Boolean circuits that represents some DFA  recognizing .
Assume  has one input bit that represents input letter from the tape and  state bits. 
By concatenating  circuits  together with one circuit  as in figure~\ref{ShFpieradijums}.
(state bit output of -th circuit is passed as state bit input
of -st) one can obtain a circuit whose size is not larger than  and which
computes Shannon function  on its  input bits.
From Theorem~\ref{ShTeorema} the complexity of this circuit is at least .
From  we get that .
\qed
\end{proof}

\begin{figure}[htb]
	\centering
		\includegraphics[width=0.7\textwidth]{bildes/ShFpieradijums.eps}
	\caption{Circuit construction for the Shannon function }
	\label{ShFpieradijums}
\end{figure}



Theorem~\ref{e2} shows that for some language  with  states its BC-complexity is at least .
Next theorem is an extension of this result. With the use of nonconstructive methods (counting argument)
one can show that this value can be raised up to . But in the beginning we will need a formula to estimate
the number of automata with a given BC-complexity.

\begin{theorem}
\label{apaksRobMinim}
Fix  and denote  to be the class of those minimal DFAs
whose BC-complexity is less than . If  then for any  


If  then for any 

\end{theorem}

\begin{proof}
By Theorem~\ref{autSkaits} .
Denote , it is clear that no more than  input bits for data input will be used for the
representation for which BC-complexity is minimal. If more bits are used, then some of them will be equal as there
are only  functions that maps  inputs letters to .

\begin{figure}[htb]
	\centering
		\includegraphics[width=0.3\textwidth]{bildes/FGH.eps}
	\caption{Merged acceptance and transition circuits}
	\label{FGH}
\end{figure}

Consider a representation  of some encoding  of .
Merge these two circuits  and  and obtain one circuit  with  inputs and  output bits,
the first  of which correspond to the output of the transition circuit , but the last output bit corresponds to the output of the acceptance circuit  (Figure~\ref{FGH}).
The complexity of this circuit  is ,
for any two minimal automata these "merged" circuits will be different.

Now we want to estimate the number of representations with BC-complexity less that .
Such representations have at least  and no more than  state bits.
The complexity of the "merged" circuit  for a representation with  state bits cannot be more than ,
the number of such circuits  from theorem~\ref{shemuSkaits} is not larger than


Therefore the number of representations with complexity  is not larger than


To prove the theorem we have to show that

or what is equivalent to that

for the stated values of .

For  if we substitute  then after simplification we obtain an
equation of the form 

which is true.
The same happens in the case  if we substitute .
\qed
\end{proof}

We have shown in Theorem~\ref{PFrobezas} that BC-complexity for any regular language with state complexity 
and input alphabet of size  is not "much larger" than . Theorem~\ref{apaksRobMinim} states that for minimal
encodings recognition of almost all such languages would require circuits of size around . This can be thought of
as the "Shannon effect" for the BC-complexity of automata: for almost all automata its value is close
to the maximum.


\section{Minimization of BC-complexity}
\label{6nod}

For the state complexity of DFA an efficient minimization algorithm~\cite{H71} is well known which, given a DFA, finds
the state complexity of it as well as the the minimal DFA itself.
This is in a big contrast with complexity measures of general programs (Turing machines) for which their complexity
(space or time) cannot be determined by any means in the general case.

It is easy to notice that finding the BC-complexity of a DFA is NP-hard.
\begin{theorem}
Finding the BC-complexity of a DFA given its arbitrary representation is NP-hard.
\end{theorem}
\begin{proof}
We will reduce SAT problem to finding minimal BC-complexity of a DFA. Given a SAT problem instance that contains  variables,
consider a DFA with  state bits ( states), that works in one letter alphabet, its state transition
function is a "circle", that goes through all the states, and accepting states are those, for which this SAT
instance gives positive output.

Now assume that this SAT instance is not satisfiable --- then this DFA never accepts and therefore its minimal DFA
has 1 state (0 state bits) and its BC-complexity is 0. If this SAT instance
is satisfiable, then any representation of it  will have some state bits and therefore its BC-complexity will be at least 1.
Therefore if one could efficiently find BC-complexity of a given DFA, he could also solve any SAT problem.
\qed
\end{proof}

Further we show one interesting property of BC-complexity --- that
for some DFAs  BC-complexity is significantly smaller than for
their equivalent minimal DFAs. 
The theorem is based on the conjecture that .
The proof of this theorem for transducers can be found in~\cite{V11}, for DFAs it is almost the same and is omitted here.
Denote by  the minimal DFA recognizing language .

\begin{theorem}
If there is a polynomial  such that

for all regular languages  then .
\label{superTeorem}
\end{theorem}

It means that in some cases by minimizing the number of states
(minimizing state complexity) BC-complexity
of the transition function can increase superpolynomially. And on the other hand, sometimes allowing equivalent states
in the automaton helps to keep BC-complexity small. 


\section{BC-complexity applications}
\subsection{Nondeterministic automata}
Theorems~\ref{apaksRobMinim} and \ref{PFrobezas} suggest that for most DFAs in -letter alphabet with  states BC-complexity is around . But in many cases when DFAs with a large state space
are constructed by some standard method, it turns out that their BC-complexity is exponentially smaller
than this maximal expected value --- it is of order . Further we look at some of these
standard constructions starting with the determinization of an NFA.

\begin{theorem}
\label{nedetTrans}
If a language  over alphabet ,  can be recognized by an NFA  with  states
and  transitions,
then it can also be recognized
by a DFA  for which
.
\end{theorem}

\begin{proof}
Consider a DFA  that is obtained by a standard construction from NFA . Its set of states is the
powerset of the set of states of . The state space of  will consist of  states
(may be some of them will not be reachable), which can be encoded in  state bits. Each state
bit of an encoding of  will correspond to one state of . For input letters we choose arbitrary
minimal input encoding into  bits.

The transition circuit of  can be obtained from the transition function of .
NFA  after reading input letter  will be in state ,
if there is a state , in which it was before (NFA can be in many states simultaneously)
and from which reading input letter  leads to state . Denote by  subset of states of  from which
reading letter  leads to state . Denote by  a subset of states in which  is after reading  letters.
If  reads input letter  in step  then:

In the circuit it means that if  denotes the encoded input letter then



To construct all  subcircuits  we need  negations and  conjunctions.

The size of the block  is the number of transitions entering state  on input  therefore the total number of these inner disjunctions and conjunctions for all output bits  is . There are also  outer disjunctions
.
In total the complexity of the transition circuit is not larger than .

Acceptance circuit  is a disjunction of all the final states
of , the complexity of this it is not larger than . Also  have to be added to the BC-complexity.
Therefore the total BC-complexity of  is not larger than .
\qed
\end{proof}

As the number of transitions is not larger than  then
\begin{corollary}
\label{nedet}
If a language  in alphabet ,  can be recognized with an NFA  with  states,
then it can also be recognized
with a DFA  for which
.
\end{corollary}



\subsection{Language operations}

State complexity of language operations has been studied long ago, e.g. in~\cite{SY00}. The result of some of the operations
(e.g. reversing) can lead to exponentially larger automata than the original one. Here we analyze how BC-complexity
changes with languages operations and observe that in those cases when the state complexity increases exponentially
it leads to automata whose state transition function is very structured therefore its BC-complexity is exponentially smaller
than state complexity.

For all operations we assume that we are given two languages  and  and ,
, , , . 
We start with the union and intersection.
\begin{theorem}
If  or  then .
\end{theorem}
\begin{proof}
Assume circuits  represent a DFA recognizing  and  represent a DFA recognizing .
The transition function for a DFA recognizing 
would consist of circuits  and  working in parallel. The acceptance circuit consists of circuits  and 
working on corresponding parts of bit vector followed by a disjunction (for union) or conjunction (for intersection) gate.
The number of state bits is the sum of state bits for representations  and .
The complexity of such a representation is .
\qed
\end{proof}

The complement of the language can be computed by the same pair of circuits as the language itself with
negation added at the end
of the acceptance circuit.
\begin{theorem}
If  then .
\end{theorem}

A word  belongs to the reverse language  iff  belongs to .
NFA  recognizing  can be obtained from the DFA  recognizing  by setting the
start state of  to be any accepting state of , setting  of  to be the only accepting state of 
and reversing all the arrows.
DFA recognizing  can be obtained from  by running the standard process of determinization. 


\begin{theorem}

\end{theorem}
\begin{proof}
This follows directly from Theorem~\ref{nedetTrans} and the fact, that NFA obtained by reversing all the
transitions has exactly  transitions.
\qed
\end{proof}

Language  which is the concatenation of languages  and  consists of all words  such that
 and .
\begin{theorem}

\end{theorem}
\begin{proof}
Assume DFA  recognizes , DFA  recognizes .
NFA that recognizes  can be obtained from  and   by adding -transitions
from all the accepting states of  to the start state of .
The standard construction of DFA from this NFA can be optimized --- it will consist of circuits  and  representing
 together with a circuit  constructed from  as from NFA as in Theorem~\ref{nedetTrans}.
Circuit  sets state bit corresponding to state  of  to "1" iff  is in accepting state.

By Theorem~\ref{nedetTrans}  and, since  is a deterministic automaton, . Together
it gives that .
\qed
\end{proof}


\begin{theorem}
.
\end{theorem}
\begin{proof}
NFA recognizing  can be obtained from DFA recognizing  by adding -transitions from
all the accepting states to the start state. The resulting NFA therefore also has  states and the result
follows from Corollary~\ref{nedet}.
\qed
\end{proof}

\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|}\hline
Operation & State complexity & BC-complexity\\\hline
 &  & \\\hline
 &  & \\\hline
 &  & \\\hline
 &  & \\\hline
 &  & \\\hline
 &  & \\\hline
\end{tabular}
\caption{State complexity and BC-complexity of language operations}
\label{valoduOperacijas}
\end{table}



\section{Conclusions and open problems}
\label{7nod}
In this paper a new measure of complexity, BC-complexity of DFAs and regular languages, was considered.
Transition function of a DFA as well as the characteristic function of the set of accepting states are expressed
as Boolean circuits and their circuit complexity is taken as a complexity measure (BC-complexity) of this DFA.
It turns out that BC-complexity can vary exponentially for DFA with the same number of states (Theorem~\ref{PFrobezas}).
Theorem~\ref{apaksRobMinim} states that almost all DFAs
BC-complexity is close to maximum ("Shannon effect"). 

In all asymptotic  constructions minimal encodings for state and input alphabet where used,
but it is not known if minimal encodings are always optimal. We think that sometimes they are not, but showing
an example where other encoding
than minimal would be more
efficient (in the sense of minimizing BC-complexity) is an interesting open question.


In section~\ref{6nod} it was shown that BC-complexity of a regular language can be much smaller
than the BC-complexity of the minimal DFA that recognizes it.
On the other hand, DFAs with a large state space that are obtained in many standard operations
(determinization of NFA, language operations), have a "good" structure so
that their BC-complexity can be relatively small.



\nocite{*}
\bibliographystyle{eptcs}
\bibliography{MarisValdatsAFL}


\end{document}
