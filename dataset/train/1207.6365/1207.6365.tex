
\newif\ifSTOC
\STOCfalse

\ifSTOC
\documentclass{sig-alternate}
\renewcommand\baselinestretch{0.9625}
\else
\documentclass{article}
\usepackage{fullpage}
\fi

\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{stmaryrd}

\usepackage[colorlinks,urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}
\newcommand{\normF}[1]{{\| #1 \|}_F}
\newcommand{\norm}[1]{{\| #1 \|}}
\newcommand{\support}[1]{\mathrm{support}(#1)}
\newcommand{\cE}{\mathcal{E}}
\DeclareMathOperator{\nnz}{\mathtt{nnz}}
\DeclareMathOperator{\tr}{\mathtt{tr}} 
\DeclareMathOperator{\rank}{\mathtt{rank}}
\DeclareMathOperator{\argmin}{\mathrm{argmin}}
\DeclareMathOperator{\E}{\mathbf{E}}
\DeclareMathOperator{\Var}{\mathbf{Var}}
\newcommand{\ZZ}{S}
\DeclareMathOperator{\JL}{\mathtt{JL}}
\DeclareMathOperator{\cond}{\mathrm{cond}}
\newcommand{\Ibr}[1]{\llbracket#1\rrbracket}
\newcommand\scn{_{s:n}}
\newcommand\vk{1/a}
\newcommand\kv{a}
\newcommand\fvk{\frac{1}{a}}
\newcommand\gdelta{\delta}
\newcommand\tO{\tilde{O}}



\newcommand\STOComitedproof[1] {\ifSTOC \begin{proof} Omitted in this version\end{proof}\else #1 \fi }

\newcommand{\poly}{{\mathrm{poly}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\R}{{\mathbb R}}
\newcommand{\polylog}{{\mathrm{polylog}}}

\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{openprob}[theorem]{Open Problem}
\newtheorem{remk}[theorem]{Remark}
\newtheorem{exmp}[theorem]{Example}
\newtheorem{apdxlemma}{Lemma}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{exercises}[theorem]{Exercises}
\newtheorem{assumption}[theorem]{Assumption}

\newenvironment{example}{\begin{exmp}
\begin{normalfont}}{\end{normalfont}
\end{exmp}}

\newenvironment{remark}{\begin{remk}
\begin{normalfont}}{\end{normalfont}
\end{remk}}
\newtheorem{sublemma}[theorem]{Sublemma}




\def\FullBox{\hbox{\vrule width 8pt height 8pt depth 0pt}}

\def\qed{\ifmmode\qquad\FullBox\else{\unskip\nobreak\hfil
\penalty50\hskip1em\null\nobreak\hfil\FullBox
\parfillskip=0pt\finalhyphendemerits=0\endgraf}\fi}

\def\qedsketch{\ifmmode\Box\else{\unskip\nobreak\hfil
\penalty50\hskip1em\null\nobreak\hfil
\parfillskip=0pt\finalhyphendemerits=0\endgraf}\fi}

\ifSTOC\else
\newenvironment{proof}{\begin{trivlist} \item {\bf Proof:~~}}
  {\qed\end{trivlist}}
\fi

\newenvironment{proofsketch}{\begin{trivlist} \item {\bf
Proof Sketch:~~}}
  {\qedsketch\end{trivlist}}

\newenvironment{proofof}[1]{\begin{trivlist} \item {\bf Proof
#1:~~}}
  {\qed\end{trivlist}}

\newenvironment{claimproof}{\begin{quotation} \noindent
{\bf Proof of claim:~~}}{\qedsketch\end{quotation}}








\begin{document}

\ifSTOC
\conferenceinfo{STOC'13,} {June 14, 2013, Palo Alto, California, USA.}
\CopyrightYear{2013}
\crdata{978-1-4503-2029-0/13/06}
\clubpenalty=10000
\widowpenalty = 10000
\fi

\title{Low Rank Approximation and Regression in \\ Input Sparsity Time}
\ifSTOC
\numberofauthors{1} 
\author{\alignauthor Kenneth L. Clarkson and David P. Woodruff \\
\affaddr{IBM Research - Almaden} \\ \affaddr{ San Jose, CA} \\
\email{klclarks@us.ibm.com, dpwoodru@us.ibm.com}
} 
\else \author{Kenneth L. Clarkson\\IBM Almaden \and David P. Woodruff\\IBM Almaden}
\fi
\maketitle
\begin{abstract}
We design a new distribution over  matrices  so that 
for any fixed  matrix  of rank , with probability at least , 
 simultaneously for all .
Such a matrix  is called a \emph{subspace embedding}.
Furthermore,  can be computed in 
time, where  is the number of non-zero entries of 
. 
This improves over all previous subspace embeddings, which required at least  time
to achieve this property. We call our matrices  \emph{sparse embedding matrices}. 

Using our sparse embedding matrices, we obtain the fastest known algorithms for 
overconstrained least-squares
regression, low-rank approximation, approximating all leverage scores, and -regression:
\begin{itemize}
\item to output an  for which 

for an 
matrix  and an  column vector , 
we obtain an algorithm running in  time,
and another in  time.
(Here .)
\item to obtain a decomposition of an  matrix  into a product of
an  matrix , a  diagonal matrix , and an  matrix ,
for which
 
where  is the best rank- approximation, our algorithm runs 
in

time.
\item to output an approximation to all leverage scores of an  
input matrix  simultaneously, with constant relative error,
our algorithms run in  time.
\item to output an  for which

for an 
matrix  and an  column vector , 
we obtain an algorithm running in  time, for any 
constant . 
\end{itemize}
We optimize the polynomial factors in the above stated running times, and show various tradeoffs. 
Finally, we provide preliminary experimental results which suggest that our algorithms are of interest in practice. 
\end{abstract}

\ifSTOC
\category{F.2.1}{Numerical Algorithms and Problems}{Computations on matrices}
\terms{Algorithms, Theory }
\fi

\section{Introduction}
A large body of work has been devoted to the study of fast randomized 
approximation algorithms for problems in numerical linear algebra. Several well-studied
problems in this area include
least squares regression, low rank approximation, 
and approximate computation of leverage scores. These problems have many applications
in data mining \cite{afkms01}, recommendation systems \cite{dkr02}, 
information retrieval \cite{prtv00},
web search \cite{afkm01,k99}, clustering \cite{dfkvv04,m01}, and learning mixtures of distributions \cite{ksv08,am05}. 
The use of randomization
and approximation allows one to solve these problems much faster than with deterministic
methods. 

For example, in the overconstrained least-squares regression problem, we are given an 
matrix  of rank  as input, , together with an  column vector . The goal is to 
output a vector  so that with high probability, 
.
The minimizing vector  can be expressed in terms of the Moore-Penrose pseudoinverse  of , 
namely, . If  has full column rank, this simplifies to . 
This minimizer can be computed deterministically in  time, but
with randomization and approximation, 
this problem can be solved in  time \cite{s06,dmms11}, 
which is much faster for  and  not too small. The generalization of this problem
to -regression is to output a vector  so that with high probability
. This can be solved exactly using convex programming,
though with randomization and approximation it is possible to achieve 
time \cite{CDMMMW} for any constant , . 

Another example is low rank approximation. Here we are given an  matrix (which can
be generalized to ) and an input parameter , and the goal is to find
an  matrix  of rank at most  for which ,
where for an  matrix ,  is the
squared Frobenius norm, and .
Here  can be computed deterministically using the singular value decomposition in  time. 
However, using randomization and approximation, this problem can be solved in   time\cite{s06, cw09}, where   denotes the number of non-zero entries of . 
The problem can also be solved using randomization and approximation in
 time \cite{s06}, 
which may be faster than the former for dense matrices and large . 

Another problem we consider is approximating the {\it leverage scores}. 
Given an  matrix  with , one can write
 in its singular value decomposition, where the columns of  are the left singular vectors,
 is a diagonal matrix, and the columns of  are the right singular vectors. Although 
has orthonormal columns, not much can be immediately said about the squared lengths  of its rows.
These values are known as the leverage scores, and measure the extent to which the singular vectors of  
are correlated with the standard basis. The leverage scores are basis-independent, since
they are equal to the diagonal elements of the projection matrix onto the
span of the columns of ; see \cite{DMMW12} for background on leverage scores 
as well as a list of applications. 
The leverage scores will also play a crucial role in our work, as we shall see. 
The goal of approximating the leverage scores is to, 
simultaneously for each , output a constant factor approximation to . 
Using randomization, this can be solved in  time \cite{DMMW12}.

There are also solutions for these problems based on sampling. They either get a weaker additive error
\cite{fkv04,prtv00,am07,dkm06,dkm06a,dkm06b,dm05,rv07,drvw06},
or they get bounded relative error but are slow \cite{dv06,dmm06,dmm06b,dmm06c}.
Many of the latter algorithms
were improved independently by Deshpande and Vempala \cite{dv06} and Sarl\'os \cite{s06}, and in followup work
\cite{dmms11,ndt09,mz11}. 
There are also solutions based on iterative and conjugate-gradient methods, see, e.g., \cite{tb_nla},
or \cite{zf12} as recent examples. These methods repeatedly compute matrix-vector products
 for various vectors ; in the most common setting, such products require 
time. Thus the work per iteration of these methods is ,
and the number of iterations  that are performed depends on the desired accuracy,
spectral properties of , numerical stability issues, and other concerns,
and can be large. A recent survey suggests that  is typically 
for Krylov methods (such as Arnoldi
and Lanczos iterations) to approximate
the  leading singular vectors \cite{HMT}.
One can also use some of these techniques together, for example by first obtaining a preconditioner using the
Johnson-Lindenstrauss (JL) transform, and then running an iterative method. 




While these results illustrate the power of randomization and approximation, their main drawback
is that they are not optimal. For example, for regression, ideally we could hope for 
time. While the  time algorithm for least squares regression
is almost optimal for {\it dense} matrices, if , say , as commonly
occurs, this could be much worse than an  time algorithm.
Similarly, for low rank approximation, the best known algorithms that are condition-independent run in 
 time, while we could hope
for  time. 

\subsection{Results}
We resolve the above gaps by achieving algorithms
for least squares regression, low rank approximation,
and approximate leverage scores, whose time complexities have 
a leading order term that is
, sometimes up to a log factor,
with constant factors that are independent of any numerical properties of .
Our results are as follows:
\begin{itemize}
\item {\bf Least Squares Regression:} We present several algorithms 
for an  matrix  with rank  and given
. One 
has running time bound of
,
stated at Theorem~\ref{thm:it reg}. (Note the logarithmic dependence on ;
a variation of this algorithm has 
running time.)
Another has 
running time , stated at
Theorem~\ref{thm:lin reg}; note that the dependence on  is
is linear.
We also give an algorithm for generalized (multiple-response)
regression, where  is found for ,
in time 

see Theorem~\ref{thm:renRegAlg}.
We also note improved results for constrained regression,
\S\ref{subsec:constrained}.
\item {\bf Low Rank Approximation:}
We achieve running time

to find an orthonormal  and diagonal  matrix
with  within  of the error of the best rank-
approximation. More specifically,
Theorem~\ref{thm:SVD} gives a time bound
of

\item {\bf Approximate Leverage Scores:} For any fixed constant , we simultaneously
-approximate all  leverage scores in 
 time. 
This can be generalized to sub-constant  to achieve  time, 
though in the applications we are aware of, such
as coresets for regression \cite{ddhkm09},  is typically constant 
(in the applications of this, a general  can be achieved
by over-sampling \cite{dmm06,ddhkm09}).
\item {\bf -Regression:}
For 
we achieve
running time 
in 
Theorem \ref{thm:lp-running} as an immediate corollary of our results and a recent connection between 
 and -regression given in \cite{CDMMMW}
(for , the  term can be improved to  as 
stated above).
\end{itemize}
\subsection{Techniques}
All of our results are achieved by improving the time complexity of computing what is known 
as a {\it subspace embedding}. For a given  matrix , call 
a \emph{subspace embedding matrix} for  if, for all , .
That is,  embeds the column space  into 
while approximately preserving the norms of all vectors in that subspace.

The \emph{subspace embedding problem} is to find such an embedding matrix obliviously, that is,
 to design a distribution  over linear maps  such that for any
 fixed  matrix , if we choose  then with large probability,
  is an embedding matrix for . The goal is to minimize
 as a function of  and , while also allowing the matrix-matrix product
 to be computed quickly.

(A closely related construction, easily derived from a subspace embedding,
is an \emph{affine embedding}, involving an additional matrix ,
such that
\ifSTOC
,
\else

\fi
for all ;
see \S\ref{subsec:genAff}. These affine embeddings are used for our low-rank
approximation results, and immediately imply approximation algorithms
for constrained regression.)

By taking  to be a Fast Johnson Lindenstrauss transform, one can set  and achieve  time
for  for any constant . One can also take  to be a subsampled randomized Hadamard transform, or SRHT
(see, e.g., Lemma 6 of \cite{BG}) and set ,
to achieve  time.
These were the fastest known subspace embeddings achieving any value of 
not depending polynomially on . Our main result improves this to achieve  for matrices 
for which  can be computed in  time! Given our new subspace embedding, we plug it into known methods of solving
the above linear algebra problems given a subspace embedding as a black box.

In fact, our subspace embedding is nothing other than the {\sf CountSketch} matrix in the data stream literature \cite{ccf04}, see also
\cite{tz04}. This matrix was also studied by Dasgupta, Kumar, and Sarl\'os \cite{dks10}. Formally,  has a single randomly chosen non-zero
entry  in each column , for a random mapping .
With probability , , and with probability , . 


While such matrices  have been studied before, the surprising fact is that they actually provide subspace embeddings.
Indeed, the usual way of proving that a random  is a subspace embedding is to show that for any fixed vector ,
. One then puts a net (see, e.g., \cite{ahk06}) on the unit vectors in the column
space  , and argues by a union bound that  for all net points . This then implies, for a net that
is sufficiently fine, and using the linearity of the mapping, 
that  for all vectors . 

We stress that our choice of matrices  does not preserve the norms of 
an arbitrary
set of  vectors with high probability, and so
the above approach cannot work for our choice of matrices .
We instead critically use that these  vectors all come
from a -dimensional subspace (namely, ), and
therefore have a very special structure.
The structural fact we use is that there is a fixed set  of size  which depends only on the
subspace, such that for any unit vector ,  contains the indices
of all coordinates of  larger than  in magnitude.
The key property here is that the set 
is independent of , or in other words, only a small set of coordinates could ever be large
as we range over all unit vectors in the subspace.
The set  selects exactly the set of large leverage scores of the columns space !


Given this observation, by setting  for a large enough constant , we have that with probability , there are no
two distinct  with  for which . That is, we avoid the birthday paradox,
and the coordinates in  are ``perfectly hashed'' with large
probability. Call this event , which we condition on. 

Given a unit vector  in the subspace, we can write it as , where  consists of  with
the coordinates in  replaced with , while  consists of  with the coordinates in  replaced with . We seek
to bound 

Since  occurs,
we have the isometry . Now, , and so we can apply Theorem 2 of 
\cite{dks10} which shows that for mappings of our form, if the input vector has small infinity norm, then  preserves the norm of the
vector up to an additive  factor with high probability. Here, it suffices to set . 

Finally, we can bound
 as follows. Define  to be the set of coordinates  for
which  for a coordinate , that is, those coordinates in  which ``collide'' with an element of~.
Then, , where  is a vector which agrees with  on coordinates
, and is  on the remaining coordinates. By Cauchy-Schwarz, this is at most .
We have already argued that  for unit vectors . Moreover, we can again apply Theorem 2 of \cite{dks10}
to bound , since, conditioned on the coordinates of  hashing to the set of items that the coordinates of  hash to, they
are otherwise random, and so we again have a mapping of our form (with a smaller  and applied to a smaller ) applied to a vector
with small infinity-norm. Therefore,  with high probability. Finally, by Bernstein bounds, since the
coordinates of  are small and  is sufficiently large,  with high probability. Hence, conditioned on event ,
 with probability , and we can complete the argument by union-bounding over a sufficiently
fine net. 

We note that an inspiration for this work comes from work on estimating norms in a data stream with efficient update time 
by designing separate data structures for the heavy and the light components of a vector \cite{nw10,knpw11}. A key concept here is to
characterize the heaviness of coordinates in a vector space in terms of its leverage scores. 
\\\\
{\bf Optimizing the additive term:}
The above approach already illustrates the main idea behind our subspace embedding, providing the first known subspace
embedding that can be implemented in  time. This is sufficient
to achieve our numerical linear algebra results in time 
for regression and  for low rank approximation. However, 
for some applications  or  may also be large, 
and so it is important to achieve a small degree 
in the additive  and  factors. 
The number of rows of the matrix  is ,
and the simplest analysis described above would give roughly 
. We now show how to optimize this.

The first idea
for bringing this down is that the analysis of \cite{dks10} can itself 
be tightened by using that we are applying it on vectors coming 
from a subspace instead of on a set of arbitrary vectors. This involves observing that in the analysis of \cite{dks10}, if on
input vector  and for every ,  is small then the remainder of the analysis of \cite{dks10}
does not require that  be small. Since our vectors come from a subspace, it suffices to show that for every , 
 is small, where  is the -th leverage score of . Therefore we do not
need to perform this analysis for each , but can condition on a single event, and this effectively allows us to 
increase  in the outline above, thereby reducing the size of , and also the size of  since we have
. In fact, we instead follow a simpler and slightly tighter analysis of \cite{KN12} based on the Hanson-Wright
inequality. 

Another idea is that the estimation of , the contribution from the 
``heavy coordinates'', is inefficient since it
requires a perfect hashing of the coordinates, which can be optimized to reduce the
additive term to . In the worst case, there are  leverage scores of value
about ,  of value about ,  of value about , etc. While the top  leverage
scores need to be perfectly hashed (e.g., if  contains the  identity matrix as a
submatrix), it is not necessary that the leverage scores of smaller value, yet still larger than ,
be perfectly hashed. Allowing a small number of collisions is okay provided all vectors in the subspace have
small norm on these collisions, which just corresponds to the spectral norm of a submatrix of . This 
gives an additive term of  instead of . 
This refinement is discussed in Section \S\ref{sec:partition}.

There is yet another way to optimize the additive term to roughly , which is useful in its own
right since the error probability of the mapping can now be made very low, namely, . This low error
probability bound is needed for our application to -regression, see Section \ref{sec: ell_p}.
By standard balls-and-bins analyses, if we have  bins and  balls, then with 
high probability each bin will contain  balls. We thus make  roughly  and think
of having  bins. In each bin ,  heavy coordinates  
will satisfy .
Then, we apply a separate JL transform on the coordinates that hash to each bin . 
This JL
transform maps a vector  to an -dimensional vector  for which
 with probability at least . Since there are only 
heavy coordinates mapping to a given bin, we can put a net on all vectors on such coordinates of size only 
. We can do this for each of the 
 bins and take a union bound. It follows 
that the -norm of the vector of coordinates that hash to each bin is preserved, and so the entire
vector  of heavy coordinates has its -norm preserved. By a result of \cite{KN12}, 
the JL
transform can be implemented in  time, giving total time , and
this reduces  to roughly . 


We also note that for applications such as least squares regression, it suffices to set
 to be a constant in the subspace embedding, since we can use an approach in \cite{dmm06,ddhkm09}
which, given constant-factor 
approximations to all of the
leverage scores, can then achieve a -approximation to least squares regression by slightly 
over-sampling rows of the adjoined matrix  proportional to its leverage scores,
and solving the induced subproblem. This results in a better dependence on . 

We can also compose our subspace embedding with a fast JL transform
to further reduce  to the optimal value of about . Since  already has small
dimensions, applying a fast JL transform is now efficient. 

Finally, we can use a recent result of \cite{ckl12} to replace most dependencies on  in 
our running times for regression with a dependence on the rank  of , which may be smaller. 

Note that when a matrix  is input that has leverage scores that are roughly equal to each other,
then the set  of heavy coordinates is empty. Such a leverage score condition is assumed, for example,
in the analysis of matrix completion algorithms. For such matrices, the sketching dimension can
be made , slightly improving our  dimension above.


\subsection{Recent Related Work}
In the first version of our technical report on these ideas (July, 2012), 
the additive  terms 
were not optimized, while in the second version, the additive terms were more refined,
and results on -regression for general  were given,
but the analysis of sparse embeddings in \S\ref{sec:partition} was absent.
In the third version, we refined the dependence still further, with the
partitioning in \S\ref{sec:partition}.
Recently, a number of authors have told us of
followup work, all building upon our initial technical report. 

Miller and Peng showed that -regression can be done
with the additive term sharpened to sub-cubic dependence
on , and with linear dependence on  \cite{MP}.
More fundamentally, they showed that a subspace embedding
can be found in  time, to
dimension

here  is the exponent for asymptotically fast matrix multiplication,
and  is an arbitrary constant. (Some constant factors here are increasing
in .)

Nelson and Nguyen obtained similar results for regression, and
showed that sparse embeddings can embed into dimension 
in  time; this considerably improved on our dimension bound for that running time,
at that point (our second version),
although our current bound is within  of their result. They also showed a
dimension bound of

for , with work  for a particular
function of . Their analysis techniques are quite different from ours \cite{NN}.

Both of these papers use fast matrix multiplication to achieve
sub-cubic dependence on  in applications, where our cubic term involves a JL transform,
which may have favorable properties in practice. Regarding subspace embeddings
to dimensions near-linear in , note that by computing leverage scores
and then sampling based on those scores, we can obtain subspace
embeddings to  dimensions in 
time; this may be incomparable to the results just mentioned, for which the running 
times increase as , possibly significantly.

Paul, Boutsidis, Magdon-Ismail, and Drineas \cite{sbmd} implemented 
our subspace embeddings and found that
in the TechTC-300 matrices, a collection of 300 sparse matrices of document-term data, with an 
average of 150 to 200 rows and 15,000 columns, our subspace embeddings as used
for the projection step in their SVM classifier are about 20 times faster than the
Fast JL Transform, while maintaining the same classification accuracy. Despite this
large improvement in the time for projecting the data, further research is needed
for SVM classification, as the JL Transform empirically possesses additional properties important 
for SVM which make it faster to classify the projected data, 
even though the time to project the data using our method is faster. 

Finally, Meng and Mahoney improved on the first version of our additive terms for 
subspace embeddings, and
showed that these ideas can also be applied to -regression, 
for  \cite{MengMahoney}; our work on this in \S\ref{sec: ell_p}
achieves  and was done independently.
We note that our algorithms for -regression require constructions
of embeddings that are successful with high probability, as we obtain
for generalized embeddings, and so some of the constructions in \cite{MP,NN} 
(as well as our non-generalized embeddings) will not yield such  results. 



\subsection{Outline}

We introduce basic notation and definitions in \S\ref{sec:sparse embed}, and then the basic 
analysis in \S\ref{sec:analysis}. A more refined analysis is given in \S\ref{sec:partition}, and then
generalized embeddings, with high probability guarantees, in \S\ref{sec:generalized}.
In these sections, we generally follow the framework discussed above, splitting coordinates
of columnspace vectors into sets of ``large'' and ``small'' ones, analyzing each such set 
separately, and then bringing these analyses together.
Shifting to applications, we discuss leverage score approximation in
\S\ref{sec:leverage}, and regression in \S\ref{sec:regression}, including the use of leverage
scores and the algorithmic machinery used to estimate them, and considering
affine embeddings in \S\ref{subsec:genAff}, constrained regression in \S\ref{subsec:constrained},  and iterative methods 
in \S\ref{subsec:iterative}.
Our low-rank approximation algorithms are given in \S\ref{sec:low rank},
where we use constructions and analysis based on leverage
scores and regression.
We next apply generalized sparse embeddings to -regression, in \S\ref{sec: ell_p}.
\ifSTOC
\else
Finally, in \S\ref{sec:exper}, we give some preliminary experimental results.
\fi




\section{Sparse Embedding Matrices}\label{sec:sparse embed}

We let  or  denote the Frobenius norm of matrix ,
and  denote the spectral norm of .

Let . We assume . 
Let  denote the number of non-zero entries of . We can assume
 and that there are no all-zero rows or columns in . 

For a parameter ,
we define a random linear map  as follows:
\begin{itemize}
\item  is a random map so that for each ,  for  with
probability .
\item  is a  binary 
matrix with , and all remaining entries .
\item  is an  random diagonal matrix, with each diagonal
entry independently chosen to be  or  with equal probability.
\end{itemize} 
We will refer to a matrix of the form  as a {\it sparse embedding matrix}.

\section{Analysis}\label{sec:analysis}
Let  have columns that form an
orthonormal basis for the column space . Let  be the rows
of , and let .


It will be convenient to regard the rows of  and  to be re-arranged so
that the  are in non-increasing order, so  is largest; of course
this order is unknown  and un-used by our algorithms.

For  and ,
let  denote the vector with 'th coordinate equal to  when ,
and zero otherwise.

Let  be a parameter. Throughout, we let ,
and .

We will use the notation , a function on event , that returns 1 when
 holds, and 0 otherwise.


The following variation of Bernstein's inequality\footnote{See Wikipedia entry
on Bernstein's inequalities (probability theory).}
will be helpful.

\begin{lemma}\label{lem:Bern}
For  and independent random variables  with
, if , then

\end{lemma}

\STOComitedproof{
\begin{proof}
Here Bernstein's inequality says that for ,
so that  and ,

By the quadratic
formula, the latter is no more than  when

which holds for  and .
\end{proof}
}

\subsection{Handling vectors with small entries}\label{sec:small}
We begin the analysis by considering  for fixed unit vectors .
Since , there must be a unit vector  so that ,
and so by Cauchy-Schwartz,
.
This implies that . 
We extend this to all unit vectors in subsequent sections.  

The following is similar to Lemma~6 of \cite{dks10},
and is a standard balls-and-bins analysis.



\begin{lemma}\label{lem:even hash}
For , and ,
let  be the event that

where .
If 
 then
.
\end{lemma}

\STOComitedproof{
\begin{proof}
We will apply Lemma~\ref{lem:Bern} to
prove that the bound holds for fixed  with failure probability
, and then apply a union bound. 

Let  denote the random variable .
We have , , and
.
Applying Lemma~\ref{lem:Bern} with  gives

when ,
or .
\end{proof}
}

\begin{lemma}\label{lem:HW use}
For  as in Lemma~\ref{lem:even hash}, suppose
the event  holds. Then for unit vector , and any ,
with failure probability ,
,
where  is an absolute constant.
\end{lemma}

\begin{proof}
We will use the following theorem, due to Hanson and Wright.

\begin{theorem}\cite{HW}\label{thm:hw}
Let  be a vect
or of i.i.d.  random values. For any symmetric 
and , 
\ifSTOC
,
where
	,

\else

where

\fi
and  is a universal constant.
\end{theorem}

We will use Theorem~\ref{thm:hw} to prove a bound on the 'th moment
of  for large . Note that
 can be written as , where  has entries
from the diagonal of , and 
 has .
Here .

Our analysis uses some ideas from the proofs for Lemmas 7 and 8 of \cite{KN12}.

Since by assumption event  of Lemma \ref{lem:even hash} occurs,
and for unit ,  for all ,
we have for  that
. Hence

For , observe that for given ,
 with  is an eigenvector of 
with eigenvalue ,
and the set of such eigenvectors spans the column space of . It follows
that 

Putting this and \eqref{eq:B F} into the  of Theorem~\ref{thm:hw},
we have, 
where we used . 
By a Markov bound applied to 
with ,

\end{proof}







\subsection{Handling vectors with large entries}\label{sec:large}

A small number of entries can be handled directly.

\begin{lemma}\label{lem:birthday}
For given , let  denote the event that  for all .
Then .
Given event , we have that for any ,

\end{lemma}


\begin{proof}
Since , the probability that some such 
has  is at most . The last claim follows by a union bound.
\end{proof}



\subsection{Handling all vectors}\label{sec:all}

We have seen that  preserves the norms for vectors with small entries
(Lemma~\ref{lem:HW use}) and large entries (Lemma~\ref{lem:birthday}). Before proving
a general bound, we need to prove a bound on the ``cross terms''.

\begin{lemma}\label{lem:cross terms}
For  as in Lemma~\ref{lem:even hash}, suppose
the event  and  hold. Then for unit vector ,
with failure probability at most ,

for an absolute constant .
\end{lemma}

\ifSTOC
\begin{proof}
The proof applies Khintchine's inequality to the sum making up the
dot product of the two sketched vectors, obtaining a moment bound
that implies the tail estimate. Please see the full paper for details.
\end{proof}
\else
\begin{proof}
With the event , for each  there is at most one  with ;
let , and  otherwise. 
We have for integer  using Khintchine's inequality

where , and the last inequality uses
the assumption that  holds, and .
Putting  and applying the 
Markov inequality, we have

Therefore, with failure probability at most ,
we have

for an absolute constant .
\end{proof}
\fi 








\begin{lemma}\label{lem:concentrate y}
Suppose
the events  and  hold, and  is as in Lemma~\ref{lem:even hash}.
Then for  there is an absolute constant
 such that, if ,
then for unit vector ,
with failure probability ,
,
when .
\end{lemma}

\ifSTOC
\begin{proof}
The proof pulls together the bounds for the large and small cases.
Please see the full paper for details.
\end{proof}
\else
\begin{proof}
Assuming  and , we apply Lemmas~\ref{lem:birthday}, \ref{lem:HW use}, and \ref{lem:cross terms}
, and have with failure probability at most ,

for the given , putting  and assuming .
Thus  suffices.
\end{proof}
\fi 









\begin{lemma}\label{lem:subspaceBound}
Suppose ,  is an -dimensional subspace of , and 
is a linear map. If for any fixed ,  with probability
at least , then there is a constant 
for which with probability at least , for all , .
\end{lemma}
\ifSTOC
\begin{proof}
The proof is a standard -net argument.
Please see the full paper for details.
\end{proof}
\else
\begin{proof}
We will need the following standard lemmas for making a net argument.
Let  be the unit sphere in
 
and let  be the set of points in  defined by

where  is the -dimensional integer lattice and  is a parameter.
\begin{fact}[Lemma 4 of \cite{ahk06}]\label{lem:netsize}
 for .
\end{fact}
\begin{fact}[Lemma 4 of \cite{ahk06}]\label{lem:netprod}
For any  matrix , 
if for every  we have , 
then for every unit vector , we have
. 
\end{fact}
Let  be such that the columns
are orthonormal and the column space equals . Let  be the  identity matrix. Define
. Consider the set  in Fact \ref{lem:netsize} and Fact \ref{lem:netprod}. 
Then, for any , we have by the statement of the lemma that with probability
at least , , ,
and . 
Since  and , it follows that . By Fact \ref{lem:netsize},
for  and sufficiently large , we have by a union bound that with probability at least 
 that 
 for every . Hence, with this probability, by Fact \ref{lem:netprod}, 
for every unit vector , which by definition of  means that for all , .
\end{proof}
\fi The following is our main theorem in this section. 
\begin{theorem}\label{thm:main}
There is   such that with probability at least ,
 is a subspace embedding matrix for ; that is, for all , 
.  The embedding  can be applied in 
time. For , where  is a parameter in ,
it suffices if .
\end{theorem}
\begin{proof}
For suitable , , and ,
with failure probability at most , events  and  both hold.
Conditioned on this, and assuming  is sufficiently small as in Lemma~\ref{lem:concentrate y},
we have with failure probability  for any fixed  that .
Hence by Lemma \ref{lem:subspaceBound}, with failure probability
, 
 for all .
We need , and the parameter conditions of Lemmas~\ref{lem:even hash}, 
Lemma \ref{lem:HW use}, and Lemma \ref{lem:concentrate y} holding. Listing these conditions:
\begin{enumerate}
\item , where  can be set to be ;
\item ;
\item \label{it:s} ;
\item  (corresponding to the condition  of Lemma \ref{lem:HW use} 
since we set )
\item .
\end{enumerate}
We put , , and require .
For the last condition it suffices that ,
and . The last condition implies the fourth condition for small enough constant . 
Also, since ,
the bound for  implies that  suffices for Condition \ref{it:s}.
Thus when the leverage scores are such that
 is small,  can be .
Since ,  suffices, and so 
suffices for the conditions of the theorem.
\end{proof}





\ifSTOC
\section{Partitioning Leverage\\ Scores}\label{sec:partition}
\else
\section{Partitioning Leverage Scores}\label{sec:partition}
\fi
We can further optimize our low order additive  term by refining the analysis for large leverage scores (those larger than ). 
We partition the scores into groups that are equal up to a constant factor, and analyze the error resulting from the 
relatively small number of collisions that may occur, using also the leverage scores to bound the error.
\ifSTOC
We obtain the following theorem; the proof is omitted in this version.
\else
 In what follows we 
have not optimized the  factors. 

Let . 
We partition the leverage scores  with  into
groups , , where

Let , and .
Since , we have for all  that .

We may also use  to refer to the collection of rows of 
with leverage scores in .

For given hash function  and corresponding ,
let  denote the collision indices of ,
those  such that  for some .
Let .

First, we bound the spectral norm of a submatrix of the orthogonal basis  of ,
where the submatrix comprises rows of .

\subsection{The Spectral Norm}\label{subsec:spectral}
We have a matrix , with , and
each row of  has squared Euclidean norm at least 
and at most , for some .

We want to bound the spectral norm of the matrix  whose
rows comprise those rows of  in the collision set .
We let 
be the number of hash buckets. 
The expected number of collisions in the  buckets is

Let  be the event that the number  of such collisions in the  buckets is at most .
Let . By a 
Markov and a union bound, . We will assume that 
occurs.

While each row in  has some independent probability 
of participating in a collision, we first analyze a sampling scheme with
replacement.

We generate independent
random matrices  for , for a parameter ,
by picking  uniformly at random, and letting
. Note that .






Our analysis will use a special case of the version of matrix Bernstein inequalities
described by Recht.

\begin{fact}[paraphrase of Theorem 3.2 \cite{Recht}]
Let  be an integer parameter. 
For , let  be independent symmetric zero-mean random matrices.
Suppose
 and .
Then for any ,

\end{fact}

We apply this fact with  and ,
so that

Also
.

Applying the above fact with these bounds for  and ,
we have

We will assume that , as discussed in lemma \ref{lem:collide norm} 
below (otherwise we have perfect hashing). With this
assumption, setting  gives
a probability bound of .
(Here we use that , , and
.)
We therefore have that with probability at least ,

where we use that
	, and use again
	.




We can now prove the following lemma. 
\begin{lemma}\label{lem:collide norm}
With probability , for all leverage score groups ,
and for  an orthonormal basis of , the submatrix 
of  consisting of rows in , that is, those in  that
collide in a hash bucket with another
row in  under , has squared spectral norm 
.
\end{lemma}

\begin{proof}
Fix a . If , then with probability , the items
in  are perfectly hashed into the  bins. So with probability , for all
, if , then there are no collisions. Condition on this event.

Now consider a  for which . Then 

When sampling with replacement,
the expected number of distinct items is

By a standard
application of Azuma's inequality, using that  is sufficiently large, 
we have that the number
of distinct items is at least  with probability at least . By a union
bound, with probability , for all , if , then at least 
 distinct items are sampled when sampling  items with replacement from . Since
, it follows that at least  distinct items are sampled from each .

By the analysis above, for a fixed  we have that the submatrix of  consisting of the 
sampled rows in  has squared spectral norm  with probability at least
 (notice that  is the square of the spectral
norm of the submatrix of  consisting of the  sampled rows from ). Since the probability of 
this event is at least  for a fixed , we can conclude that it holds for all 
simultaneously with probability .
Finally, using that the spectral norm of a submatrix of a matrix is at most that of the matrix, we 
have that for each , the squared spectral norm of a submatrix of  random distinct rows among the 
sampled rows of  from  is at most . 
\end{proof}


\subsection{Within-Group Errors}

Let  denote the set of vectors  so that 
for  not in the collision set ,
and there is some unit  such that 
for . (Note that the error for such vectors is that
same as that for the corresponding set of vectors 
with zeros outside of .)

In this subsection, we show that for all ,
the error in estimating  using 
is at most . 

For , the error in estimating
 by using  contributed
by collisions among coordinates  for  is

and we need a bound on this quantity that holds with high probability.



By a standard balls-and-bins analysis, every bucket has 
collisions, with high probability,
since ; we assume this event.

The squared Euclidean norm of the vector
of all  that appear in the summands, that is, with , is
at most  by Lemma~\ref{lem:collide norm}.
Thus the squared Euclidean norm
of the vector comprising all summands in \eqref{eq:big terms}
is at most


By Khintchine's inequality, for ,

and therefore  is less than the last quantity, with failure probability
at most .

Putting , with failure probability at
most , 
for any fixed vector , the squared error in estimating
 using the sketch of  is at most
.
Assuming the event  from the section above,
we have . 
We have, using ,

and ,
and finally

using .
Putting these bounds on the terms together,
the squared error is , or , for
, so that the error is .

Since the dimension of 
is bounded by , it follows from the net argument of Lemma~\ref{lem:subspaceBound}
that for all , ,
and so the total error for unit  is .

We thus have the following theorem. 

\begin{theorem}\label{thm:partition within}
There is an absolute constant  for which 
for any parameters , , and for sparse embedding dimension 
, for all unit ,
,
with failure probability at most ,
where  denotes the member of  derived from .
\end{theorem}

\subsection{Handling the Cross Terms}

To complete the optimization, we must also handle the error due to ``cross terms".

Let  be an arbitrary parameter. 
For , let the event 
 be that the number of bins containing both an item in 
and in  is at most 
Let , the
event that no pair of groups has too many inter-group collisions.
\begin{lemma}\label{lem:E}

\end{lemma}

\begin{proof}
Fix a . Then the expected number of
bins containing an item in both  and in  is at most

and so by a Markov bound the number of bins containing an item in both  and 
is at most  with probability at least . The
lemma follows by a union bound over the  choices of . 
\end{proof}

In the remainder of the analysis, we set  for a parameter .
\\\\
Let  be the event that no bin contains more than  elements of
, where  is an absolute constant. 
\begin{lemma}\label{lem:F}
. 
\end{lemma}

\begin{proof}
Observe that 
By standard balls and bins analysis with the given , with ,
with probability at least  no bin contains more than  elements,
for a constant . 
\end{proof}

\begin{lemma}\label{lem:individual}
Condition on events  and  occurring. Consider 
any unit vector  in the column space of . Consider any .
Define the vector :  for , and  otherwise. Then,

\end{lemma}

\begin{proof}
Since  occurs, the number of bins containing both an item in  and  is at most
. Call this set of bins . Moreover, since  occurs, for each bin ,
there are at most  elements from  in the bin and at most  elements from  in
the bin. Hence, for any , we have, using  for all ,

\end{proof}

The following is our main theorem concerning cross-terms in this section.
\begin{theorem}\label{thm:partition cross}
There is an absolute constant  for which 
for any parameters , , and for sparse embedding dimension 
, the event

occurs with failure probability at most ,
where  are as defined in Lemma \ref{lem:individual}.
\end{theorem}
\begin{proof}
The theorem follows at once by combining Lemma \ref{lem:E}, Lemma \ref{lem:F}, and Lemma \ref{lem:individual}.
\end{proof}

\subsection{Putting it together}

Putting the bounds for within-group and cross-term errors together, and replacing
the use of Lemma~\ref{lem:birthday} in the proof of Theorem~\ref{thm:main},
we have the following theorem.

\fi 

\begin{theorem}\label{thm:partition main}
There is an absolute constant  for which 
for any parameters , , and for sparse embedding dimension 
, for all unit ,
,
with failure probability at most .
\end{theorem}

\section{Generalized Sparse Embedding Matrices}\label{sec:generalized}

\ifSTOC
As discussed in the introduction, we can use small JL transforms within each hash bucket,
to obtain the following theorem, where the term in the running time
dependent on  is more expensive, but the quality bounds hold with high probability.
\else

\subsection{Johnson-Lindenstrauss transforms}
We start with a theorem of Kane and Nelson \cite{KN12}, restated here in our notation. We also
present a simple corollary that we need concerning very low dimensional subspaces. 
Let , and .
Let  be defined as follows. 
We view  as the concatenation (meaning, we stack the rows on top of each other) 
of matrices , 
each 
being a linear map from  to , which is an independently
chosen sparse embedding matrix of Section \ref{sec:analysis} with
associated hash function . 
\begin{theorem}(\cite{KN12})\label{thm:general}
For any ,
there are  and 
for which for any fixed , a randomly chosen  of the form above satisfies
 with probability at least . 
\end{theorem}
\begin{corollary}\label{cor:subspace}
Let .
Suppose  is an -dimensional subspace of .
Let  be any constant.
Then for any , there 
are  and  
such that with failure probability at most 
 ,  for all .
\end{corollary}
\begin{proof}
We use Theorem \ref{thm:general} together with Lemma \ref{lem:subspaceBound};
for the latter, we need that for any fixed ,  with probability at least
. By Theorem \ref{thm:general}, we have this for  for an arbitrarily large constant .
Hence, by Lemma \ref{lem:subspaceBound}, there is a constant  
so that with probability at least
,
for all , 
. Here we use that  can be made arbitrarily large, independent of . 
\end{proof} 
\subsection{The construction}
We now define a {\it generalized sparse embedding matrix} . 
Let  with rank . 


Let  and
,  be such that Theorem \ref{thm:general} 
and Corollary \ref{cor:subspace} apply
with parameters  and , for a sufficiently large constant .  
Further, let  
where  is a sufficiently large absolute constant, 
and let . 

Let  be a random hash function. For , 
define . Note that . 

We choose independent matrices 
, with each  as in Theorem \ref{thm:general} 
with parameters  and . Here  is a  matrix. Finally, 
let  be an  permutation matrix which, when applied to a matrix , maps the rows
of  in the set 
to the set of rows , maps the rows of  in the set  to the
set of rows , and for a general ,
maps the set of rows of  in the set  
to the set of rows . 

The map  is defined to be the product of a block-diagonal matrix and the
matrix :


\begin{lemma}\label{lem:time}
 can be computed in  time.
\end{lemma}
\begin{proof}
As  is a permutation matrix,  can be computed in  time and has
the same number of non-zero entries of . For each non-zero entry of ,
we multiply it by  for some , which takes  time. Hence, the total time
to compute  is . 
\end{proof}

\subsection{Analysis}
We adapt the analysis given for sparse embedding matrices to generalized sparse embedding matrices.
Again let  have columns that form an
orthonormal basis for the column space . Let  be the rows
of , and let . For , we set the parameter:

where  is a sufficiently large absolute constant. 

\subsubsection{Vectors with small entries}
Let , and for  of at most unit norm, let .
Since ,
this implies that . Since  is a permutation matrix, 
we have . 

In this case, we can reduce the analysis to that of a sparse embedding matrix. Indeed, observe
that the matrix  is the concatenation of matrices 
, where
each  is a sparse embedding matrix. Now fix a value  and consider
the block-diagonal matrix :

\begin{lemma}\label{lem:reduce}
 is a random sparse embedding matrix with  rows and  columns.
\end{lemma}
\begin{proof}
 has a single non-zero entry in each column, and the value of this non-zero entry
is random in . Hence, it remains to show that the distribution of locations
of the non-zero entries of  is the same as that in a sparse embedding matrix. This follows
from the distribution of the values , and the definition of . 
\end{proof}

\begin{lemma}\label{lem:HW JL}
Let .
For , let  be the event  of Lemma 
\ref{lem:even hash}, applied to matrix , with ,
and .
Suppose  holds.
This event has probability at least .
Then there is an absolute constant  such that
with failure probability at most ,

\end{lemma}

\begin{proof}
We apply Lemma~\ref{lem:HW use} with  the sparse embedding matrix ,
and , the number of rows of , taking on the role of  in Lemma~\ref{lem:even hash}, 
so that the parameter
 as in the lemma statement.
(And since , , so .)
Since , it suffices for Lemma~\ref{lem:even hash}
if  is at
least , or .

With , by a union bound 
occurs with failure probability , as claimed.

We have, for given , that with failure probability ,
.
Applying a union bound, and using

the result follows.
\end{proof}





\subsubsection{Vectors with large entries}

Again, let . Since ,
we have


The
following is a standard non-weighted balls-and-bins analysis.
\begin{lemma}\label{lem:non-weighted}
Suppose the previously defined constant  is sufficiently large. Let 
be the event that , for all .
Then .
\end{lemma}
\begin{proof}
For any given ,

Hence, by a Chernoff bound, for a constant ,

The lemma now follows by a union bound over all . 
\end{proof}

\begin{lemma}\label{lem:large JL}
Assume that  holds. Let 
be the event that for all ,
.
Then .
\end{lemma}

\begin{proof}
For , let  be the at most -dimensional subspace
which is the restriction of the column space  to coordinates  with
 and . By Corollary \ref{cor:subspace},
for any fixed , with probability at least , for all
, .
By a union bound and sufficiently large , 
this holds
for all  with probability at least
. This condition
implies , since  can be expressed as ,
where each , and letting  denote the  rows
of  corresponding to entries from ,

A re-scaling to  completes the proof.
\end{proof}


\subsection{Putting it all together}
Now consider any unit vector  in , and write it as .
We seek to bound
. For notational convenience, define 
the block-diagonal matrix  to be the matrix 

Then . Notice that since the set of non-zero rows of
 and  are disjoint for ,

where by Lemma \ref{lem:reduce}, each  is a sparse embedding matrix with  rows and  columns.

\begin{lemma}\label{lem:cross terms JL}
For  as in Lemma~\ref{lem:HW JL}, and assuming events ,
, and ,
there is absolute constant  such that with failure probability ,

\end{lemma}

\begin{proof}
We generalize Lemma~\ref{lem:cross terms} slightly
to bound each summand . 

For a given , and for each , 
let

where  is the hash function for .
We have for integer  using Khintchine's inequality,

where ,
and , and the last inequality uses
the assumption that  holds.
Putting  and applying the 
Markov inequality, we have for all  that

Moreover, ,
which under  is at most .
Therefore, with failure probability at most ,
we have

for an absolute constant .
\end{proof}



\fi 

The following is our main theorem in this section.
\begin{theorem}\label{thm:jlmain}
For given , 
with probability at least , for ,
 is an embedding matrix for ;
that is, for all , . 
 can be applied to  in  time.
\end{theorem}
\STOComitedproof{
\begin{proof}
Note that

yielding the bound claimed.
From Lemma~\ref{lem:HW JL}, event  occurs with
failure probability at most .
From Lemma~\ref{lem:non-weighted} and \ref{lem:large JL}
the joint occurrence of 
and  holds with failure probability at most .
Given these events, from Lemmas~\ref{lem:cross terms JL} and
\ref{lem:HW JL}, we have with failure probability at most 
that

where .

Setting ,
where  is from Lemma~\ref{lem:subspaceBound},
and recalling that ,
we have 

for absolute constant .
Using Lemma~\ref{lem:subspaceBound},
we have that with failure probability at most
,
that

for suitable choice of . Adjusting  by a constant factor gives the result.
\end{proof}
}



\ifSTOC
\section{Approximating Leverage \\ Scores}\label{sec:leverage}
\else
\section{Approximating Leverage Scores}\label{sec:leverage}
\fi
Let  with rank . Let  be an
orthonormal basis for . In \cite{dmmw11} it was shown how to obtain 
a -approximation  to the leverage score  for all , for a constant ,
in time . Here we improve the running time of this task as
follows. We state the running time for constant , though for general  the running time
would be . 
\begin{theorem}\label{thm:icml}
For any constant , there is an algorithm which with probability at least , outputs
a vector  so that for all , . The running time
is \ifSTOC
.
\else

\fi
The success probability can be amplified by independent repetition and taking the coordinate-wise
median of the vectors  across the repetitions. 
\end{theorem}
\STOComitedproof{
\begin{proof}
We first run the algorithm of Theorem 2.6 and Theorem 2.7 of \cite{ckl12}. The first theorem 
gives an algorithm which outputs the rank  of , while the second theorem gives an algorithm
which also outputs the indices  of linearly independent columns of . The algorithm
takes  time and succeeds with probability at least .
Hence, in what follows, we can assume that  has full rank. 

We follow the same procedure as Algorithm 1 in \cite{dmmw11}, using our improved subspace embedding.
The proof of \cite{dmmw11} proceeds by choosing a subspace embedding , computing ,
then computing a change of basis matrix  so that  has orthonormal columns. The analysis
there then shows that the row norms  are equal to . To obtain
these row norms quickly, an  Johnson-Lindenstrauss matrix  is sampled,
and one first computes , followed by . Using a fast Johnson-Lindenstrauss transform
, one can compute  in  time.  has  rows, and
one can compute the  matrix  in  time by computing a 
QR-factorization. Computing 
can be done in  time, and computing  can be done in  time.

Our only change to this procedure is to use a different matrix , which is the composition
of our subspace embedding matrix  of Theorem \ref{thm:jlmain} with parameter , together with a fast
Johnson Lindenstrauss transform . That is, we set . Here,  is an
 matrix, see Section 2.3 of \cite{dmmw11} for an instantiation of . 
Then,  can be computed in  time by Lemma \ref{lem:time}. Moreover,
 can be computed in  time. One can then 
compute the matrix  above in  time by computing a QR-factorization of . Then
one can compute  in  time, and computing  can be done in 
 time.
Hence, the total time is  time. 

Notice that
by Theorem \ref{thm:jlmain}, with probability at least , 
 for all , and by
Lemma 3 of \cite{dmmw11}, with probability at least , 
 for all . Hence,
 for all  
with probability at least . There is
also a small  probability of failure that 
 for
some value of . Hence, the overall success probability is at least . 

The rest of the correctness proof is identical to the analysis in \cite{dmmw11}. 
\end{proof}
} 

\section{Least Squares Regression}\label{sec:regression}
Let  and  be a matrix and
vector for the regression problem: . We assume . Again,
let  be the rank of . 
We show that with probability at least , 
we can find an  for which


We will give several different algorithms.
First, we give an algorithm showing that the dependence on  can be linear.
Next we shift to the generalized case, with multiple right-hand-sides,
and after some analytical preliminaries, give an algorithm based on sampling
using leverage scores. Finally, we discuss affine embeddings, constrained regression,
and iterative methods.




\begin{theorem}\label{thm:lin reg}
The -regression problem can be solved up to a -factor with probability at least 
 in  time.
\end{theorem}
\begin{proof}
By Theorem \ref{thm:main} applied to the column space ,
where  is  adjoined with the vector ,
it suffices to compute  and  and output 
argmin. We use the fact that , and apply
Theorem \ref{thm:partition main} with . 

The theorem implies that with probability at least , 
all vectors  in the
space spanned by the columns of  and  
have their norms preserved up to a -factor. 
Notice that  and  can be computed in 
 time. Now we have a regression problem with  rows
and  columns. 
Using the Fast Johnson-Lindenstrauss
transform, this can be solved in  time, see, Theorem 12 of \cite{s06}. The
success probability is at least . This is  time. 
\end{proof}




Our remaining algorithms will be stated for generalized regression.

\subsection{Generalized Regression and Affine Embeddings}

The regression problem can be slightly generalized to

where  and  are matrices rather than vectors. This problem,
also called \emph{multiple-response} regression,
is important in the analysis of our low-rank approximation algorithms,
and also of independent interest. Moreover, while an analysis involving
the embedding of  is not significantly different than for an embedding
involving  alone, this is not true for : different techniques must 
be considered. This subsection gives the needed theorems
needed for analyzing algorithms for generalized regression,
and also gives a general result for \emph{affine embeddings}.


Another form of sketching matrix relies on sampling based on leverage scores; it
will be convenient to define it using sampling with replacement: for 
given sketching dimension , for  let  have
, where ,
and  with probability .


The following fact is due to Rudelson\cite{Rudelson}, but has since seen many proofs,
and follows readily from Noncommutative Bernstein inequalities \cite{Recht},
which are very similar to matrix Bernstein inequalities \cite{Zouzias}.

\begin{fact}\label{fact:lev embed}
For rank-  with row leverage scores ,
there is  such that leverage-score sketching matrix  is
an -embedding matrix for .
\end{fact}

\subsection{Preliminaries}

We collect a few standard lemmas and facts in this
\ifSTOC
subsection, where the main lemma needed is the following,
which gives well-known bounds for approximate matrix multiplication
using sketches.
\else
subsection.
\fi \begin{lemma}\label{lem:tail}{\bf (Approximate Matrix Multiplication})
For  and  matrices with  rows, where  has  columns, and given
, there is ,
so that for 
a  generalized sparse embedding matrix ,
or  fast JL matrix, or  subsampled randomized Hadamard matrix,
or leverage-score sketching matrix for  under the condition
that  has orthonormal columns,

for any fixed .
\end{lemma}

\ifSTOC\else
\begin{proof}
For a generalized sparse embedding
matrix with parameters  and , first suppose ,
so that  is the embedding matrix of \S\ref{sec:sparse embed}.
Let . Then , where  is the -th column of 
and  is the -th column of . Thorup and Zhang \cite{tz04} have shown that  
and  Consequently, 
 from which
for an appropriate , the lemma follows by Chebyshev's inequality.
For , ,
see \eqref{eq:blockTZ},
so that`

and similarly the lemma follows for the sparse embedding matrices.
The result for fast JL matrices was shown by Sarl{\'o}s\cite{s06},
and for subsampled Hadamard by Drineas et al.\cite{dmms11}, proof of Lemma~5.
(The claim also follows from norm-preserving properties of these
transforms, see \cite{kn10}.)

For leverage-score sampling, first note that

we have ,
and using the independence of the ,
the second moment of 
is the expectation of

which is

or using the cyclic property of the trace, the fact that , and
the fact that ,

and so the lemma follows for large enough  in ,
by Chebyshev's inequality.
\end{proof}


\begin{fact}\label{fact:subspace jl}
Given  matrix  of rank  for
, and , an 
fast JL matrix  with 
is a subspace embedding for  with failure probability at most ,
for any fixed ,
and requires  time to apply to .
\end{fact}

A similar fact holds for subsampled Hadamard transforms.

\begin{fact}\label{fact:pyth}{\bf (Pythagorean Theorem)}
If  and  matrices with the same number of rows and columns,
then  implies .
\end{fact}

\begin{fact}\label{fact:normal}{\bf (Normal Equations)}
Given  matrix , and  matrix
 consider the problem

The solution to this problem is , where  is the Moore-Penrose
inverse of .  Moreover, , and so
if  is any vector in the column space of ,
then . Using Fact~\ref{fact:pyth}, for any ,

\end{fact}
\fi 

\subsection{Generalized Regression: Conditions}

The main theorem in this subsection is the following. It could be regarded as a
generalization of Lemma 1 of \cite{dmms11}.
\begin{theorem}\label{thm:genReg}
Suppose  and  are matrices with  rows,
and  has rank at most .
Suppose
 is a  matrix,
and the event occurs that  satisfies Lemma~\ref{lem:tail} with error
parameter ,
and also that
 is a subspace embedding for  with 
error parameter .
Then
if  is the solution to

and  is the solution to

then

\end{theorem}
\ifSTOC\else
Before proving Theorem \ref{thm:genReg}, we will need the following lemma.  
\begin{lemma}\label{lem:betabound}
For  and  as in Theorem \ref{thm:genReg},
assume that  has orthonormal columns. Then

\end{lemma}

\begin{proof} The proof is in the appendix.\end{proof}
\fi 

\begin{proofof}{of Theorem \ref{thm:genReg}}
\ifSTOC
Omitted in this version.
\else
Let  have the thin SVD .
Since  is a basis for ,
there are  and  so that
 
and , and therefore

implies the theorem: we can assume without loss of generality
that  has orthonormal
columns. With this assumption,
and using the Pythagorean Theorem (Fact~\ref{fact:pyth})
with the normal equations (Fact~\ref{fact:normal}),
and then Lemma~\ref{lem:betabound},

and taking square roots and adjusting  by a constant factor
completes the proof.
\fi \end{proofof}

\subsection{Generalized Regression: Algorithm}

Our main algorithm for regression is given in the proof of the following theorem.

\begin{theorem}\label{thm:renRegAlg}
Given  of rank , and , the regression
problem  can be solved up to  relative error
with probability at least ,
in time

and obtaining a coreset of size .

\end{theorem}

\begin{proof}
We estimate the leverage scores of  to relative error ,
using the algorithm of
Theorem~\ref{thm:icml}, which has the side effect of finding
 independent columns of , so that we can assume that .

If  is a basis for , then
for any  there is a  so that ,
and vice versa, so that conditions satisfied by  are satisfied
by . That is, we can (and will hereafter)
assume that  has  orthonormal columns, when
considering products .

We construct a leverage-score sketching matrix  for 
with , so that Lemma~\ref{lem:tail}
is satisfied for error parameter at most . With this ,
 will also be an -embedding matrix with
, using Lemma~\ref{fact:lev embed}.
These conditions and Theorem~\ref{thm:genReg} imply that
the solution  to 
has


The running time is that for computing the leverage scores, plus
the time needed for finding , which
can be done by computing a  factorization
of  and then computing ,
which requires ,
and the cost bound follows.
\end{proof}

\subsection{Affine Embeddings}\label{subsec:genAff}

We also use \emph{affine embeddings}  for which a stronger condition
than Theorem~\ref{thm:genReg} is satisfied.

\begin{theorem}\label{thm:genAff}
Suppose  and  are matrices with  rows,
and  has rank at most .
Suppose
 is a  matrix,
and the event occurs that  satisfies Lemma~\ref{lem:tail} with error
parameter , and also that
 is a subspace embedding for  with 
error parameter .
Let  be the solution of , and .
For all  of appropriate shape,

for .
So  is an affine embedding with~ relative error up to an additive constant.
(That is, a \emph{weak} embedding.)
If also , then

and  is a -affine embedding.
\end{theorem}

Note that even when only the weaker first statement holds, the sketch still can be used
for optimization, since adding a constant to the objective function of an
optimization does not change the solution. Note also that 

\STOComitedproof{
\begin{proof}
If  is a basis for , then
for any  there is a  so that ,
and vice versa, so that conditions satisfied by  are satisfied
by . That is, we can (and will hereafter)
assume that  has  orthonormal columns.

Using the fact that  for any ,
the embedding property, the fact that ,
and the matrix product approximation condition
of Lemma~\ref{lem:tail},

The normal equations (Fact~\ref{fact:normal}) imply
that ,
and using the observation that  for ,

and the first statement of the theorem follows. When ,
the second statement follows, since then

using  for all .
\end{proof}
} 

To apply this theorem to sparse embeddings, we will need the following lemma.

\begin{lemma}\label{lem:len fixed sparse}
Let  be an  matrix. Let  be a 
randomly chosen sparse embedding matrix for an appropriate . 
Then with probability at least ,

\end{lemma}
\begin{proof}
Please see the appendix.
\end{proof}

\begin{lemma}\label{lem:len fixed srht}
Let  be an  matrix. Let  be an
SRHT matrix for an appropriate . 
Then with probability at least ,

\end{lemma}
\begin{proof}
Please see the appendix.
\end{proof}


\begin{theorem}\label{thm:genAff res}
Let  and  be matrices with  rows, and  has rank
at most .
The following conditions hold with fixed nonzero
probability.
If  is a  sampled randomized Hadamard transform (SRHT) matrix, there is

such that  is an -affine embedding for  and . If  is a 
sparse embedding, there is 
such that  is an -affine embedding.
If  is a  leverage-score sampling matrix, there is 
such that  is a weak
-affine embedding. If the row norms of  are available,
a modified leverage-score sampler is an -embedding. (Here 
is as in Theorem~\ref{thm:genAff}.)
\end{theorem}

Note that none of the dimensions  depend on the number of columns of .

\begin{proof}
To apply Theorem~\ref{thm:genAff}, we need each given sketching matrix
to satisfy conditions on multiplicative error, subspace embedding, and
preservation of . As in that theorem, we can
assume without loss of generality that  has  orthonormal columns.

Regarding the multiplicative error bound of ,
Lemma~\ref{lem:tail} tells us that SRHT achieves this bound for ,
and the other two need .

Regarding subspace embedding, as noted in the introduction,
an SRHT matrix achieves this
for .
A sparse embedding requires ,
as in Theorem~\ref{thm:partition main}, and leverage score samplers
need , as mentioned in Fact~\ref{fact:lev embed}.

Regarding preservation of the norm of , Lemma~\ref{lem:len fixed srht}
gives the claim for SRHT matrices, and Lemma~\ref{lem:len fixed sparse} gives the claim for
sparse embeddings, where the ``'' of those lemmas
is .


Thus the conditions are satisfied for Theorem~\ref{thm:genAff}
to yield the the claims for SRHT and for sparse embeddings, and for the weak condition
for leverage score samplers.

We give only a terse version of the argument for the last statement of
the theorem.
When the squared row norms  of  are available,
a sampler which picks row  with probability ,
and scales that row with , will yield a matrix whose
Frobenius norm will be  with high probability.
If the leverage score sampler picks rows with probability ,
create a new sampler that picks rows with probability ,
and scales by . The resulting sampler will satisfy
the norm preserving property for , and also
satisfy the same properties as the leverage score sampler, up to a constant
factor. The resulting sampler is thus an -affine embedding.
\end{proof}


\subsection{Affine Embeddings and Constrained Regression}\label{subsec:constrained}


From the condition \eqref{eq:aff embed}, an affine embedding
can be used to reduce the work needed to achieve small
error in regression problems, even when there are constraints
on . We consider the constraint , that the entries
of  are nonnegative. The problem
,
for  and ,
arises among other places as a subroutine in finding
a nonnegative approximate factorization of .

For an affine embedding ,

yielding an immediate reduction yielding a solution with relative
error : just solve the sketched version of the problem.

From Theorem~\ref{thm:genAff res}, suitable
sketching matrices for constrained regression
include a sparse embedding, 
an SRHT matrix, or a leverage
score sampler. (The latter may not need the condition
of preserving the norm of  if a high-accuracy
solver is used for the sketched solution, or if otherwise
the additive constant is not an obstacle for that solver.)

Since it's immediate that affine embeddings can be composed
to obtain an affine embedding (with a constant factor loss),
the most efficient approach might be use a sketch
that first applies a sparse embedding, and then
applies an SRHT matrix, resulting
in a sketched problem with  rows,
and where computing the
sketch takes  time,
for . When  is unknown, the upper
bound  can of course be used.

For low-rank approximation, discussed in \S\ref{sec:low rank},
we require  to satisfy a rank condition; the same techniques apply.


\iffalse
Another way to apply our techniques, which is more expensive
but yields higher quality answers, is as follows:
first factor  as , with
 having orthormal columns and  upper triangular.
From Fact~\ref{fact:normal}, the normal equations,

where ,
using here the fact that  has orthonormal columns.

Thus after solving a generalized regression problem,
it remains to solve ,
that is, to find for each column of  the
nearest vector in the cone generated by the columns of~.
For the given column, this problem can be solved in  time,
so that the cost for all columns is . 
\fi




\subsection{Iterative Methods for Regression}\label{subsec:iterative}

\ifSTOC
We use the matrix  obtained for leverage score approximation
in \S\ref{sec:leverage} as a pre-conditioner for standard
iterative, conjugate-gradient methods; such iterative methods
have a running time dependent on the condition number
of the input matrix. Our pre-conditioner reduces that method to 
constant, and the resulting algorithm implies the following
theorem, whose proof is given in the full paper.
\else

A classical approach to finding 
is to solve the normal equations (Fact~\ref{fact:normal})
 via Gaussian elimination;
for  and ,
this requires  to
form ,  to form ,
and  to solve the resulting linear systems.
(Another method is to factor ,
where  has orthonormal columns and  is
upper triangulation; this typically trades a slowdown for a higher-quality solution.)

Another approach to regression is to apply an iterative method
(from the general class of Krylov, CG-like methods) to a 
pre-conditioned version of the problem.  In such methods,
an estimate  of a solution is maintained,
for iterations ,
using data obtained from previous iterations.
The convergence of these methods depends
on the \emph{condition number}

from the input matrix.
A classical result (\cite{Luen} via \cite{MSM} or Theorem 10.2.6,\cite{GvL}),
is that

Thus the running time of CG-like methods, such as {\tt CGNR} \cite{GvL},
depends on the (unknown)
condition number. The running time per iteration is the time needed
to compute matrix vector products  and ,
plus  for vector arithmetic, or .

Pre-conditioning reduces the number of iterations needed for a given accuracy:
suppose
for non-singular matrix , the condition number 
is small. Then a CG-like method applied to  would converge quickly,
and moreover for iterate  that has error 
small, the corresponding  would have .
The running time per iteration would have an
additional  for computing products involving .

Consider the matrix  obtained for leverage score approximation
in \S\ref{sec:leverage}, where a subspace embedding matrix 
is applied to , and  is computed so that   has
orthonormal columns. Since  is a subspace
embedding matrix to constant accuracy ,
for all unit ,
.
It follows that the condition number

That is,  is very well-conditioned. Plugging this
bound into \eqref{eq:CG accuracy}, after  iterations
 is at most 
times its starting value.

Thus starting with a solution  with 
relative error at most 1, and applying  iterations
of a CG-like method with , the relative error is reduced to 
and the work is 
(where we assume  has been reduced to , as in the leverage computation),
plus the work to find . We have
\fi

\begin{theorem}\label{thm:it reg}
The -regression problem can be solved up to a -factor with probability at least 
 in

time.
\end{theorem}

\ifSTOC\else

Note that only the matrix  from the leverage score computation is needed, not
the leverage scores, so the  term in the running time 
need not have a 
factor; however, since reducing  to  columns requires that factor,
the resulting running time without that factor
is ,
depends on .


The matrix  is so well-conditioned that a simple iterative improvement scheme
has the same running time up to a constant factor. Again start with a solution  with 
relative error at most 1, and for ,
let .
Then using the normal equations,

where  is the SVD of .

For all unit ,
, and so
we have that all singular values  of  are ,
and the diagonal entries of 
are all at most  for 
. Hence 

and by choosing , say,  iterations suffice for this
scheme also to attain  relative error.

This scheme can be readily extended to generalized (multiple-response)
regression, using the iteration
.
The initialization cost then includes that of computing
, which is ,
where again . The product ,
used implicitly per iteration, could be computed in ,
and then applied per iteration in time ,
or applied each iteration in time . 

That is, this method is never much worse than CG-like methods, but
comparable in running time when ; when ,
it is a little worse in asymptotic running time than solving the normal equations.
\fi 


\section{Low Rank Approximation}\label{sec:low rank}

This section gives algorithms for low-rank approximation, understood
using generalized regression analysis, as in earlier work such as \cite{s06,cw09}.
Let , where  denotes
the best rank- approximation to .
We seek low-rank matrices whose distance to  is within  of .

While Theorem \ref{thm:main} and Theorem \ref{thm:jlmain} 
are stated in terms of specific constant probability of success,
they can be re-stated and proven so that the failure probabilities are arbitrarily small, but still
constant. In the following we'll assume that adjustments have been done, so that the sum
of a fixed number of such failure probabilities is at most .

We will apply embedding matrices composed of products of such matrices, so we need to check
that this operation preserves the properties we need.

\begin{fact}\label{fact:prod compose}
If  approximates matrix products and is a subspace
embedding with error 
and failure probability ,
and  approximates matrix products with error 
and failure probability ,
then  approximates matrix products with error 
and failure probability at most .
\end{fact}

\begin{proof}
This follows from two applications of Lemma~\ref{lem:tail}, together with
the observation that  for basis vectors
 implies that .
\end{proof}

\begin{fact}\label{fact:embed compose}
If  is a subspace embedding with error 
and failure probability ,
and  is a subspace embedding with error 
and failure probability ,
then  is a subspace embedding with error 
and failure probability at most .
\end{fact}


The following lemma implies a regression algorithm that is
linear in , but has a worse dependence in
its additive term.

\begin{lemma}\label{lem:genReg sparse}
Let  of rank , , 
and .
For  a sparse embedding matrix,
 a sampled randomized Hadamard matrix,
there is  and
 such that
for ,
 has
.
The operator  can be applied in  time.
\end{lemma}



\begin{theorem}\label{thm:SVD}
For , there is an algorithm that with failure
probability  finds matrices 
with orthonormal columns,
and diagonal , so that
.
The algorithm runs in time

\end{theorem}

\begin{proof}
The algorithm is as follows:
\begin{enumerate}
\item Compute  and an orthonormal basis  for ,
	where  is as in Lemma~\ref{lem:genReg sparse} with ;
\item Compute  and  for  the product of a 
SRHT matrix with a  sparse embedding,
where 
and .
(Instead of this affine embedding construction,
an alternative might use leverage score sampling,
where even the weaker claim of Theorem~\ref{thm:genAff res}
would be enough.)
\item Compute the SVD of ;
\item Compute the SVD  of ,
	where again  denotes the best rank- approximation to matrix ;
\item Return , , and .
\end{enumerate}

{\bf Running time.}
Computing  in the first step takes
 time,
and then  to compute the 
matrix . 
Computing  and  requires
 time.
Computing the SVD of
the   matrix 
requires . Computing 
requires  time.
Computing the SVD of the
 matrix of the next step requires
 time, as does computing .

\paragraph{Correctness}
Apply Lemma~\ref{lem:genReg sparse} with  of that lemma
mapping to  and  mapping to .
Taking transposes, this implies that with small fixed failure probability,

has

and so

Since  is a basis for ,

With the given construction of ,
Theorem~\ref{thm:genAff res} applies (twice), with  taking the role
of , and  taking the role of , so that  is
an -affine embedding, after adjusting constants.
It follows that
for ,

using \eqref{eq:AR good}.
From lemma 4.3 of \cite{cw09},
the solution to

is , where this denotes the best rank-
approximation to .
It follows that  is a solution to
to .
Moreover, the rank- matrix 
has ,
and , , and  have the properties promised.
\end{proof}













\section{-Regression for any } \label{sec: ell_p}
Let  and  be a matrix and
vector for the regression problem: . We assume . 
Let  be the rank of . 
We show that with probability at least , 
we can quickly find an  for which

Here  is any constant in . 

This theorem is an immediate corollary of Theorem \ref{thm:jlmain} 
and the construction given in 
section 3.2 of \cite{CDMMMW}, which shows how to solve -regression given a subspace embedding
(for ) as a black box.
\ifSTOC
The details are omitted in this version.
\else
We review the construction of \cite{CDMMMW} below for completeness. 

As in the proof of Theorem \ref{thm:icml}, in  time we can 
replace the input matrix  with a new matrix with the same column space of  and 
full column rank,
where  is rank of . We therefore assume  has full rank in what follows. 

Let  and assume . 
Split  into  matrices ,
each , so that  is the submatrix of  indexed by the -th block of 
 rows. 

We invoke Theorem \ref{thm:jlmain} with the parameters , , , 
and , choosing a generalized sparse embedding matrix matrix  with 
 rows. Theorem \ref{thm:jlmain} has the guarantee that for each fixed ,
 is a subspace embedding with probability at least . It follows by a union
bound that with probability at least , for all ,  
is a subspace embedding. We condition on this event occurring. 

Consider the matrix
, which 
is a block-diagonal matrix comprising  blocks along the diagonal.
Each block is the  matrix  given above. 

We will need the following theorem.
\begin{theorem}[Theorem 5 of \cite{ddhkm09}, restated]
\label{thm:basisOld}
Let  be an  matrix, and let .
Then there exists an -well-conditioned basis for
the column space of  such that if , then  and ; 
if , then  and , and if  then 
 and . An  change of basis matrix
 for which  is a well-conditioned basis can be
computed in  time. 
\end{theorem}
\noindent
The specific conditions satisfied by a well-conditioned basis are given (and used)
in the proof of the theorem below. We use the following algorithm {\sf Condition}() 
given a matrix :
\begin{enumerate}
\item Compute ;
\item Apply Theorem \ref{thm:basisOld} to  to obtain an  change of basis
matrix  so that  is an -well-conditioned basis of the
column space of matrix ;
\item Output ,
where  for ,
and  for .
\end{enumerate}
The following lemma is the analogue of that in \cite{CDMMMW} proved for the Fast Johnson Lindenstauss
Transform. However, the proof in \cite{CDMMMW} only used that the Fast Johnson Lindenstrauss Transform is a subspace
embedding. We state it here with our new parameters,
\ifSTOC
omitting the proof in this version.
\else and give the analogous proof in the Appendix for completeness.
\fi \begin{lemma}\label{lem:lpl2}
With probability at least , the output  of {\sf Condition}
is guaranteed to be a basis that is -well-conditioned,
that is, an -well-conditioned basis.
The time to compute  is . 
\end{lemma}
The following text is from \cite{CDMMMW}, we state it here for completeness. 
A well-conditioned basis can be used 
to solve  regression problems, via an algorithm based on sampling the rows
of  with probabilities proportional to the norms of the rows of the corresponding
well-conditioned basis. This entails
using for speed a second projection  applied to  on the right
to estimate the row norms, where  can be an  matrix of 
i.i.d. normal random variables, which is the same as is done in \cite{dmmw11}. 
This allows fast estimation of the  norms of the rows of ;
however, we need the  norms of those rows, which we thus know up
to a factor of .
We use these norm estimates
in the sampling algorithm of \cite{ddhkm09}; as discussed for
the running time bound of that paper, Theorem 7, this algorithm samples
a number of rows proportional to ,
when an -well-conditioned
basis is available. This factor, together with
a sample complexity increase of  needed to compensate
for error due to using , gives a sample complexity increase for our algorithm
over that of \cite{ddhkm09} of a factor
of

while the leading term in the complexity (for )
is reduced from  to . 

Observe that if ,
then  is less than , which is . 
Hence, the overall time complexity is . 

We adjust Theorem 4.1 of \cite{ddhkm09} 
and obtain the following.
\fi 

\begin{theorem}\label{thm:lp-running}
Given , a constant , 
 and ,
there is a sampling algorithm for  regression that
constructs a coreset specified by a diagonal sampling matrix , and 
a solution vector  that minimizes the weighted regression objective
.  The solution  satisfies, with probability
at least , the relative error bound that

for all .
Further, with probability
, the entire algorithm to construct   runs in time

\end{theorem}

\ifSTOC\else
\section{Preliminary Experiments}\label{sec:exper}

Some preliminary experiments show that a low-rank approximation technique that is a simplified
version of these algorithms is promising, and in practice may perform much better
than the general bounds of our results.

Here we apply the algorithm of Theorem~\ref{thm:SVD}, except that we skip the
randomized Hadamard and simply use a sparse embedding  
and leverage score sampling. We compare the Frobenius error of the resulting 
with that of the best rank- approximation.

In our experiments, the matrices tested
are .


The resulting low-rank approximation
was tested for  (the number of columns of )
taking values of the form, for integer
, while .
The number  of rows of  was chosen such that the condition number of  was at most . (Since  has orthogonal columns, its condition number is 1, so a large enough leverage
score sample will have this property.)
For such  and , we took the ratio  of the Frobenius norm of the error to the Frobenius norm of the error of the
best rank- approximation. The resulting points  were generated,
for all test matrices, for three independent trials, resulting in a set of points .

The test matrices are from the University of Florida 
Sparse Matrix Collection,
essentially most of those with at most  nonzero entries, and with  up to about  7000.
There were 1155 matrices tested, from 70 sub-collections of matrices, each such
sub-collection representing a particular application area.

The curve in Figure 1 represents the results of these tests, where for a particular point
 on the curve, at most one percent of points  gave a result where
 but .

Figure~2 shows a similar curve for the points ; thus the necessary
ratio , so that , as for the results in FIgure~1, need be no smaller than
about .



\begin{figure*}
\begin{center}
  \includegraphics[scale=0.8]{Err_ratio_SU_cond_1_2_percent_Pareto_1.pdf} 
 \end{center}
\caption{A ``1\%-Pareto'' curve of error as a function of the size of }
\end{figure*}

\begin{figure*}
\begin{center}
  \includegraphics[scale=0.8]{SU_cond_1p2_Pareto_1percent.pdf} 
 \end{center}
\caption{A 1\%-Pareto curve of   as a function of the size of  relative to }
\end{figure*}





\fi 




\section*{Acknowledgements}
We acknowledge the support from XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C-0323.
We thank Jelani Nelson and the anonymous STOC referees for helpful comments.



\bibliographystyle{plain}
\bibliography{main}


\ifSTOC\else
\appendix

\section{Deferred proofs}

\begin{proofof}{of Lemma \ref{lem:betabound}}
Since by assumption  has orthonormal columns,
,
so it suffices to bound the latter, or  where
.
By Fact \ref{fact:normal}, we have 

To bound , we bound
, and then show that
this implies that  is small.
Using that  and (\ref{eq:approxlsnormal}), we have

Using the hypothesis of the theorem, 

To show that this bound implies that 
is small, we use the subadditivity of  and
the property of any conforming matrices  and ,
that ,
to obtain

By hypothesis,
 for all ,
so that  has eigenvalues bounded in magnitude by ,
which implies singular values with the same bound, so that 
.
Thus
,
or

since . This bounds ,
and so proves the lemma.
\end{proofof}

\begin{proofof}{of Lemma~\ref{lem:len fixed sparse}}
Let  with associated hash function . 
For  denoting the -th column of , 
let  denote the column vector whose -th coordinate is  if 
, and whose -th coordinate is  if .
We use the second moment method to bound . For the expectation,

For the second moment, 

We handle the first term in (\ref{eqn:var}) as follows:

For the second term in (\ref{eqn:var}), for ,

where the constant  arises because if we choose indices  from

we need to choose the same  and  from

in order to have a non-zero expectation, and there are  ways of doing this for distinct .
Continuing, 

Combining (\ref{eqn:exp}) with (\ref{eqn:var}) and the bounds on the terms in (\ref{eqn:var}) above,

The lemma now follows by Chebyshev's inequality, for appropriate . 
\end{proofof}

\begin{proofof}{of Lemma~\ref{lem:len fixed srht}}
Lemma 15 of \cite{BG} shows that 
with arbitrarily low failure probability,
and the other direction follows from a similar argument. Briefly:
the expectation of  is , by construction,
and Lemma 11 of \cite{BG} implies that with arbitrarily small
failure probability, all rows of  will have squared norm
at most , where  is a value
in . Assuming that this bound holds,
it follows from Hoeffding's inequality
that the probability that 
is at most , or
, so that  suffices
to make the failure probability at most .
\end{proofof}


\begin{proofof}{of Lemma \ref{lem:lpl2}}
This is almost exactly the same as in \cite{CDMMMW}, we simply adjust notation and parameters. 
Applying Theorem \ref{thm:jlmain}, we have that
with probability at least , for all , if we consider  and
write , then for all ,  

By relating the -norm and the -norm, for , we have

and similarly,

If , then 

and similarly,

Since 
and ,
for  we have with probability 

and for  with probability 

In either case,




Applying Theorem \ref{thm:basisOld}, we have, from the definition
of a -well-conditioned basis, that 

and for all ,


Combining (\ref{eqn:first}) and (\ref{eqn:second}), we have that with probability at least ,

Combining (\ref{eqn:first}) and (\ref{eqn:third}), we have that with probability at least ,
for all ,

Hence  is an 
-well-conditioned
basis. The time to compute
 is  by Theorem \ref{thm:jlmain}. Notice
that  is an  matrix, which is , 
and so the time to 
compute  from  is , 
since .  
\end{proofof}
\fi 




\end{document}
