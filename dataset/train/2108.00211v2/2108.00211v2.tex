\begin{table}
\centering
\normalsize
\begin{adjustbox}{max width=80 mm}
\begin{tabular}{l| c c c| c|c }
\toprule[1pt]
     \multirow{2}*{Methods}&\multicolumn{3}{c|}{PF-PASCAL}&CUB&time\\
     & 0.05&0.1&0.15& 0.1& (ms)\\\hline
     PF \cite{ham2017proposal}  & 31.4 & 62.5 & 79.5&-&- \\ 
     CNNGeo \cite{cnngeo}  & 41.0 & 69.5 & 80.4&-&- \\ 
     A2Net \cite{a2net}  & 42.8 & 70.8 & 83.3& - &- \\ 
     SFNet \cite{sfnet}  & 53.6 & 81.9 & 90.6& - &- \\ 
     DCTM \cite{dctm}  & 34.2 & 69.6 & 80.2&-&-   \\ 
     WeakAlign \cite{rocco2018end}  & 49.0 & 74.8 & 84.0&-&-  \\ 
    SCNet \cite{han2017scnet}  & 36.2 & 72.2 & 82.0&- &- \\ 
    RTNs \cite{rtns} & 55.2 & 75.9 & 85.2&-&-\\
    UCN \cite{ucn} & -&55.6&-&48.3&-\\
    UCN \cite{ucn} & -&75.1&-&52.1&-\\
    NC-Net \cite{Rocco18b}  & 54.3 & 78.9 & 86.0& 64.7&393\\
    DCCNet \cite{dccnet}  & 55.6 & 82.3 & 90.5&66.1 &-\\
    HPF \cite{min2019hyperpixel}   & 60.5 & 83.4& 92.1&- &-\\
    HPF \cite{min2019hyperpixel}   & 60.1 & 84.8& 92.7& - &-\\
    HPF \cite{min2019hyperpixel}   & 63.5& 88.3& 95.4&-  &- \\
    DHPF \cite{min2020learning}   & 72.6 & \textbf{88.9} & \textbf{94.3}&-  &55\\
    DHPF \cite{min2020learning}   & 75.7 & \textbf{90.7}& \textbf{95.0}&- &95\\
    SCOT \cite{liu2020semantic}   & 63.1 & 85.4& 92.7& - &180\\
    SCOT \cite{liu2020semantic}   & 67.3 & 88.8& 95.4&- &109 \\
    ANC-Net \cite{li2020correspondence}   & - & 83.7& -& 69.6 &600 \\
    ANC-Net \cite{li2020correspondence}   & - & 88.7& -&74.1 &- \\
    ANC-Net \cite{li2020correspondence}   & - & 86.1& -& 72.4 &- \\
    \hline
    MMNet  & \textbf{75.3} & 88.0& 93.2&\textbf{80.6} &51\\
    MMNet  & \textbf{77.6} & 89.1& 94.3&\textbf{81.8} &86\\
    MMNet  & \textbf{78.9} & \textbf{90.3}& \textbf{94.4}&\textbf{83.1}& 101\\
    MMNet  & \textbf{81.1} & \textbf{91.6}& \textbf{95.9}&\textbf{87.0} &87\\
    \bottomrule[1pt]
\end{tabular}
\end{adjustbox}
\vspace{2pt}
\caption{Comparison with state-of-the-art algorithms in PCK and speed on PF-PASCAL~\cite{ham2017proposal} and CUB ~\cite{cub} dataset. Subscripts of the method names indicates the backbone used.}\label{Tab:quant_results}
\end{table}


\begin{table*}
\centering
\normalsize
\begin{adjustbox}{max width=180mm}
\begin{threeparttable}



\begin{tabular}{l| c c c c c c c c c c c c c c c c c c|c}
\toprule[1pt]
    Methods & aero & bike & bird & boat & bottle & bus & car & cat & chair & cow & dog& horse& mbike& person& plant& sheep& train & tv & all\\ \hline
    CNNGeo \cite{cnngeo}  & 23.4 & 16.7 & 40.2& 14.3& 36.4& 27.7& 26.0& 32.7& 12.7& 27.4& 22.8& 13.7& 20.9& 21.0& 17.5& 10.2& 30.8& 34.1& 20.6\\ 
    A2Net \cite{a2net}  & 22.6 & 18.5& 42.0& 16.4& 37.9& 30.8& 26.5& 35.6& 13.3& 29.6& 24.3& 16.0& 21.6& 22.8& 20.5& 13.5& 31.4& 36.5&22.3 \\
    WeakAlign \cite{rocco2018end}  & 22.2 & 17.6& 41.9& 15.1& 38.1& 27.4& 27.2 & 31.8& 12.8& 26.8& 22.6& 14.2& 20.0& 22.2&17.9&10.4&32.2&35.1&20.9\\
    NC-Net \cite{Rocco18b}   & 17.9 & 12.2& 32.1& 11.7 & 29.0 & 19.9&16.1 &39.2 &9.9&23.9&18.8&15.7&17.4&15.0&14.8&9.6&24.2&31.1&20.1 \\
    HPF \cite{min2019hyperpixel}   & 25.3 & 18.5& 47.6& 14.6& 37.0& 22.9& 18.3& 51.1& 16.7& 31.5& 30.8& 19.1& 23.7& 23.8& 23.5&14.4&30.8&37.2&27.2\\
    HPF \cite{min2019hyperpixel}   & 25.2 & 18.9& 52.1& 15.7& 38.0& 22.8& 19.1& 52.9& 17.9& 33.0& 32.8& 20.6& 24.4& 27.9& 21.1& 1.9& 31.5& 35.6&28.2\\
    DHPF \cite{min2020learning}   & 38.4 & 23.8& \textbf{68.3}& 18.9& 42.6& 27.9& 20.1& 61.6& 22.0& 46.9 & 46.1& 33.5& 27.6& \textbf{40.1}& 27.6&28.1&49.5&46.5&37.3 \\
    SCOT \cite{liu2020semantic}   & 34.9 & 20.7& 63.8& 21.1& 43.5& 27.3& 21.3& \textbf{63.1}& 20.0& 42.9& 42.5& 31.1& 29.8& 35.0& 27.7& 24.4&48.4& 40.8& 35.6\\
    \hline
    MMNet  & 43.5 & 27.0& 62.4&27.3 & 40.1 & 50.1&37.5 & 60.0 & 21.0& 56.3& 50.3& 41.3&30.9&19.2&30.1&33.2&64.2&43.6&40.9 \\
    MMNet-FCN  & \textbf{55.9} & \textbf{37.0}& 65.0&\textbf{35.4} & \textbf{50.0} & \textbf{63.9}&\textbf{45.7} & 62.8 & \textbf{28.7}& \textbf{65.0}& \textbf{54.7}&\textbf{51.6}&\textbf{38.5}&34.6&\textbf{41.7}&\textbf{36.3}&\textbf{77.7}&\textbf{62.5}&\textbf{50.4} \\
    \bottomrule[1pt]
\end{tabular}

   \end{threeparttable}

\end{adjustbox}
\vspace{1pt}
\caption{Comparisons on SPair-71k~\cite{spair} with state-of-art methods. The backbone in methods listed is ResNet101~\cite{he2016deep}. The best results are reported in bold.}\label{Tab:quant_spair}
\end{table*}  



\paragraph{Dataset.}
We conduct experiments on three popular benchmarks for semantic correspondence: PF-PASCAL~\cite{ham2017proposal}, CUB ~\cite{cub} and SPair-71k~\cite{spair}. The PF-PASCAL contains 1351 image pairs which are selected from all the 20 categories in PASCAL VOC \cite{pascal-voc}. We split the dataset as done in~\cite{han2017scnet} where approximately 700 image pairs are used for training, 300 image pairs are used for validation and 300 image pairs are used for test. The CUB dataset~\cite{cub} contains 11,788 images of 200 bird species with large intra-class variations. Each image is annotated with the locations of 15 key-parts. Following the protocol in~\cite{li2020correspondence}, we randomly sample 10,000 pairs from the CUB as training data and use the same test set provided by~\cite{cub-test}. SPair-71k is composed of total 70,958 image pairs in 18 categories with large view-point and scale variations. We use the same split proposed in~\cite{spair} where 53340, 5384, 12234 image pairs are used for training, validation and testing respectively. 

\paragraph{Evaluation metric.}Performances of different methods are evaluated using the percentage of correct key-points (PCK@). A point is considered \textit{correct} if the predicted point is within the circle of radius  centering at the ground-truth point, where d is the longer side of an image or an object bounding box as in \cite{han2017scnet,Rocco18b,li2020correspondence,liu2020semantic,min2020learning}.

\paragraph{Implementation Details}

For fair comparison with state-of-the art methods, we use four different backbones including ResNet-50~\cite{he2016deep}, ResNet-101~\cite{he2016deep}, ResNeXt-101~\cite{xie2017aggregated} and ResNet101-FCN~\cite{he2016deep}. All backbone networks are pretrained on Image-Net1k classification set~\cite{krizhevsky2012imagenet} and then fine-tuned for corrrespondence task. 

The multi-scale matching network structure is visualized in Figure~\ref{Fig:mmn}. We only introduce additional parameters in feature enhancement. As shown in Figure~\ref{Fig:mmn} (b), we have many SEMs  each of which is followed by a  convolutional layer. We upscale the low resolution feature with a  deconvolutional layer whose stride is 2 at different scales. Then the upsampled feature map is concatenated with the output of the intra-scale feature enhancement, and pass a  convolution layer. Note that the SEM in our MMNet is in the same settings as in BDCN~\cite{he2019bi}, and the output channel numbers of both the convolutional layers and the deconvolutional layers is set to 21 to save computation cost.


During the training, we adopt SGD with momentum as our optimizer. The learning rate is set to 0.0005 for initialization and is decreased by 10 times every 10000 iterations. Momentum and weight decay are set to 0.9 and 0.0002 respectively. Learning rate is decreased by 10 times every 10,000 iterations. The batch-size is set to 5 for all experiments. The training will converge within 10000, 32000 and 30000 iterations for PF-PASCAL~\cite{ham2017proposal}, CUB ~\cite{cub} and SPair-71k~\cite{spair} respectively. All experiments are implemented with PyTorch~\cite{pytorch}, and run on NVidia TITAN RTX GPUs.



\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/alpha-vis.pdf}
	\caption{The PCK- curves of our method and compared works on PF-PASCAL~\cite{ham2017proposal}.}
	\label{Fig:alpha}
\end{figure}



\subsection{Comparisons with State-of-the-art Methods}\label{comparisons}

For PF-PASCAL\cite{ham2017proposal}, our MMNet with ResNet101-FCN as the backbone outperforms all previous state-of-art methods with 81.1\% PCK@0.05, 91.6\% PCK@0.1, and 95.9\% PCK@0.15. When compared with ANC-Net~\cite{li2020correspondence} which also conducts end-to-end training with 4-D correlation, MMNet gets 5.4\%, 1.6\%, and 5.5\% increases on PCK@0.1 with three different backbones respectively. It demonstrates the effectiveness of the multi-scale feature learning and matching complementation. When compared with previous best SCOT~\cite{liu2020semantic} with ResNet101-FCN as the backbone, we achieve a significant improvement on PCK@0.05 by 13.8\%. We attribute this to that the end-to-end training of deep neural networks has a higher efficiency that optimization based methods to enforce one-on-one matching with discriminative features. We also compare with the multi-scale feature fusion based methods  HPF~\cite{min2019hyperpixel} and DHPF~\cite{min2020learning}. MMNet with ResNet101-FCN as the backbone outperforms HPF with the same backbone by 17.6\% PCK@0.05, 3.3\% PCK@0.1 and 0.5\% PCK@0.15. Since HPF doesn't conduct end-to-end training, it is reasonable that our MMNet gets better results. DHPF~\cite{min2020learning} selects features from the backbone, and get slightly better results on PCK@0.1 and PCK@0.15 with ResNet50, ResNet101 backbone. However, when , our MMNet get 1.9\% improvement on PCK. It may because MMNet are much more sensitive to small difference between neighborhood, and thus get better matching results with much more strict matching criteria. For CUB\cite{cub}, our MMNet outperforms all state-of-art algorithms with 87.0\% on PCK@0.1 and achieve prominent betters on three different backbone compared with ANCNet~\cite{li2020correspondence}.For SPair-71k\cite{spair}, our MMNet with ResNet101-FCN backbone outperforms the state-of-art algorithms by at least 13.1\% on PCK@0.1, which is a huge improvement. Among all listed methods in Table~\ref{Tab:quant_results}, our algorithm surpasses all state-of-art by a large margin on 15 of the 18 classes. This proves the effectiveness and robustness of our MMNet in establishing reliable matching. 

Figure~\ref{Fig:alpha} shows the results in comparison with state-of-art method with varying . When  is small, only points matched with the destination point closely is treated as a correct match, otherwise failure. When we increase , lager matching displacement will be allowed. It can be found that our MMNet achieves the best performance when  varies from 0.02 to 0.3. When  varies from 0.02 to 0.1, all algorithms will get improvements on PCK fast. It means the allowed match displacement influences the performance greatly. When  varies from 0.15 to 0.3, all methods get almost the same results with very high matching accuracy. This indicates too large  can not be used to measure the performance of different methods accurately. When  varies from 0.01 to 0.05, our MMNet outperforms other state-of-art methods by a clear margin all the time. It indicates the strong ability of MMNet in identifying neighborhood pixels. 

Besides, we also report the running speed to compare the computational efficiency of state-of-art methods. In Table~\ref{Tab:quant_results}, we get the comparable test speed with the previous best DHPF~\cite{min2020learning}, and are much faster than other methods.

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/iccv2021_fig8.pdf}
	\caption{Visualization of the semantic correspondence. The odd rows are the source image, and the even rows are the target images. Destination key point are denoted with crosses. From left to right: (a) DCC-Net~\cite{dccnet}, (b) ANC-Net~\cite{li2020correspondence}, (c) SCOT~\cite{liu2020semantic}, (d) DHPF~\cite{min2020learning}, (e) ours MMNet and (f) the ground truth.}
	\label{Fig:msmc}
\end{figure*}

\subsection{Module Analysis with Ablations}\label{ablations}
To investigate the effectiveness of different modules, we conduct ablation study by replacing or removing a single component. All experiments are conducted on PF-PASCAL~\cite{ham2017proposal} with ResNet101-FCN as the backbone. PCKs are evaluated with  = 0.05, 0.1 and 0.15. Results are listed in Table~\ref{Tab:ablation_data}. First, we remove the local self attention, the performance drops by 1.2\% on PCK@0.05 which indicates that the contextual information around neighborhood pixels is very important. Then we remove the dense connections and only get the output of the last layer in every convolutional group, the performance of PCK@0.05 is 74.9\% which is 6.2\% lower than MMNet with dense connections. It shows fusing information at different layers in a convolutional group is greatly helpful. When we remove the cross-scale feature fusion, PCK@0.05, PCK@0.10 and PCK@0.15 decrease by 2.6\%, 1.9\% and 1.6\% respectively. If we don't conduct the complementary matching learning and get the matching score at every layer independently, PCK@0.05 drops by 1\%. At last, if we only supervise the learning at the last matching complementation layer with the highest resolution, the performance drops to 31.4\% on PCK@0.05 which is 49.7\% lower than the MMNet with rich supervision during training. It may indicate that supervision in multiple scales are very important to learn semantics at different levels.  




\begin{table}
\centering
\normalsize
\begin{tabular}{l |c c c}
\toprule[1pt]
     Methods & 0.05 & 0.1& 0.15 \\ \hline 
    MMNet  & 81.1 & 91.6 & 95.9 \\ \hline
MMNet w/o local self attention   & 79.9 & 90.7& 95.1\\
    MMNet w/o dense connections   & 74.9 & 88.6 & 93.9\\
    MMNet w/o cross-scale fusion & 78.5 & 89.7& 94.3\\
    MMNet w/o complementation  & {80.0} & {89.3} & {94.3}\\ 
    MMNet w/o rich supervision   & 31.4 & 45.6 & 57.0\\ 
    MMNet w/o last layer & 77.9&90.9 &94.6 \\
    \bottomrule[1pt]
\end{tabular}
 \vspace{5pt}
\caption{Ablation results on various setting on PF-PASCAL~\cite{ham2017proposal} with ResNet101-FCN as backbone.}
\label{Tab:ablation_data}
\end{table}


\subsection{Qualitative Results and Visualization}\label{qualitatives}
We visualize the correspondence result by drawing the point-to-point matches and warping images with the predicted key point pairs respectively. In Figure~\ref{Fig:msmc}, the point-to-point matches are drawn by linking key point pairs with line segments. The ground truth matching is given at first as reference for visual comparison. It can be find that our MMNet matches all key points on horses correctly. Other state-of-the art methods, such as DCC-Net~\cite{dccnet}, ANC-Net~\cite{li2020correspondence}, SCOT~\cite{liu2020semantic} and DHPF~\cite{min2020learning}, will lead to large mismatch displacement or  many-to-one match. In Figure~\ref{Fig:warping}, images are warped based on matched key point pairs. For convenience, we warp the ground truth annotations as the reference. It can be found that our MMNet can matches the objects in images accurately. Especially in the first row, tables in the source and target images has very large viewpoint and appearance variations. But our MMNet can still match the corresponding key points accurately when other methods fail.
