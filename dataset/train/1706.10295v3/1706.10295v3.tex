\documentclass{article}
\usepackage{iclr2018_conference,times}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\newcommand{\argmax}{\operatorname*{argmax}}
\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\var}{\operatorname*{Var}}
\newcommand{\lstd}{\operatorname*{LSTD}}
\newcommand{\diag}{\operatorname*{diag}}
\newcommand{\sgn}{\operatorname*{sgn}}
\newcommand{\lin}{\operatorname*{lin}}
\newcommand{\supp}{\operatorname*{Supp}}
\newcommand{\todo}[2]{{\color{yellow}{#1 TODO: #2}}}
\newcommand{\algoinit}{NoisyNet}
\newcommand{\algofull}{Noisy Networks for Reinforcement Learning}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\title{Noisy Networks for Exploration}
\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\author{Meire Fortunato\thanks{Equal contribution.}
\quad Mohammad Gheshlaghi Azar\footnotemark[1]
\quad Bilal Piot \footnotemark[1] \And
\textbf{Jacob Menick
\quad  Matteo Hessel
\quad Ian Osband
\quad Alex Graves
\quad Volodymyr Mnih} \And
\textbf{Remi Munos
\quad Demis Hassabis 
\quad Olivier Pietquin
\quad Charles Blundell 
\quad Shane Legg}  \\ \\
DeepMind
  \texttt{\{meirefortunato,mazar,piot,} \\
  \texttt{jmenick,mtthss,iosband,gravesa,vmnih,}\\
  \texttt{munos,dhcontact,pietquin,cblundell,legg\}@google.com} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy
\begin{document}

\maketitle


\newcommand{\Exp}{\mathds{E}}
\newcommand{\Expk}{\mathds{E}_{k}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\Ind}{\mathds{1}}
\newcommand{\Rmax}{R_{\rm max}}
\newcommand{\riskyopt}{\succcurlyeq_{\rm ro}}
\newcommand{\cid}{\succcurlyeq_{\rm CID}}
\newcommand{\so}{\succcurlyeq_{\rm so}}
\newcommand{\single}{\succcurlyeq_{\rm sc}}
\newcommand{\ssd}{\succcurlyeq_{\rm ssd}}
\newcommand{\pp}{\mathrel{+}\mathrel{+}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\A}{\mathcal A}
\renewcommand{\S}{\mathcal S}
\newcommand{\X}{\mathcal X}
\newcommand{\D}{\mathcal D}
\newcommand{\G}{\mathcal G}
\newcommand{\K}{\mathcal K}
\newcommand{\calP}{\mathcal P}
\newcommand{\calI}{\mathcal I}
\newcommand{\barH}{\overline{H}}
\newcommand{\hh}{\hat h}
\renewcommand{\L}{\mathcal L}
\newcommand{\Hyp}{\mathcal H}
\newcommand{\Y}{\mathcal Y}
\newcommand{\B}{\mathcal B}
\newcommand{\C}{\mathcal C}
\newcommand{\F}{\mathcal F}
\newcommand{\E}{\mathbb E}
\newcommand{\W}{\mathcal W}
\newcommand{\Z}{\mathcal Z}
\newcommand{\calE}{\mathcal E}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\Cov}{\textnormal{Cov}}
\newcommand{\V}{\mathbb V}
\newcommand{\Prob}{\mathbb P}
\newcommand{\I}{\mathbb I}
\newcommand{\N}{\mathcal N}
\newcommand{\tM}{\widetilde{M}}
\newcommand{\balpha}{\boldsymbol \alpha}
\newcommand{\bmu}{\boldsymbol \mu}
\newcommand{\bSigma}{\boldsymbol \Sigma}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bhP}{\widehat{\mathbf{P}}}
\newcommand{\bT}{\boldsymbol{T}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\MV}{\textnormal{MV}}
\newcommand{\hMV}{\widehat{\textnormal{MV}}}
\newcommand{\barmu}{\bar\mu}
\newcommand{\hpi}{\hat\pi}
\newcommand{\tDelta}{\widetilde{\Delta}}
\newcommand{\hmu}{\widehat{\mu}}
\newcommand{\hrho}{\hat\rho}
\newcommand{\heps}{\hat\eps}
\newcommand{\hnu}{\hat\nu}
\newcommand{\trho}{\tilde\rho}
\newcommand{\brho}{\bar\rho}
\newcommand{\hM}{\widehat{M}}
\newcommand{\hN}{\widehat{N}}
\newcommand{\tmu}{\widetilde{\mu}}
\newcommand{\tpi}{\widetilde{\pi}}
\newcommand{\barvar}{\bar\sigma^2}
\newcommand{\tvar}{\tilde\sigma^2}
\newcommand{\R}{\mathcal{R}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand{\hR}{\widehat{\mathcal{R}}}
\newcommand{\tR}{\widetilde{\mathcal R}}
\newcommand{\invdelta}{1/\delta}
\newcommand{\boldR}{\mathbb R}
\newcommand{\eps}{\varepsilon}
\newcommand{\mvlcb}{\textnormal{\texttt{MV-LCB }}}
\newcommand{\ucb}{\textnormal{\textsl{UCB }}}
\newcommand{\ucbv}{\textnormal{\textsl{UCB-V }}}
\newcommand{\mvlcbt}{\textnormal{\texttt{MV-LCB(t) }}}
\newcommand{\mom}{\textnormal{MoM}}
\newcommand{\me}{\textnormal{ME}}
\newcommand{\mt}{\textnormal{MT}}
\newcommand{\eh}{1/(1-\gamma)}
\newcommand{\ehf}{\frac1{1-\gamma}}


\newcommand{\cvar}{\textnormal{C}}
\newcommand{\hcvar}{\widehat{\textnormal{C}}}
\newcommand{\hvar}{\widehat{\textnormal{V}}}


\newcommand{\avg}[2]{\frac{1}{#2} \sum_{#1=1}^{#2}}
\newcommand{\hDelta}{\widehat{\Delta}}
\newcommand{\hGamma}{\widehat{\Gamma}}






\newcommand{\beq}{}

\newcommand{\beqa}{}

\newcommand{\beqan}{}


\renewcommand{\P}{\mathbb{P}}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Esp}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\indic}[1]{\mathbb{I}\{#1\}}
\newcommand{\EE}[1]{\E\left[#1\right]}
\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}

\let\R\undefined \newcommand{\R}{\mathbb{R}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Normal}{\mathcal{N}}

\newcommand{\eqdef}{\stackrel{\rm def}{=}}

\newcommand{\cl}[2][ (]{
\ifthenelse{\equal{#1}{ (}}{\left (#2\right)}{}
\ifthenelse{\equal{#1}{[}}{\left[#2\right]}{}
\ifthenelse{\equal{#1}{\{}}{\left\{#2\right\}}{}
}

\newcommand{\inset}[3][C]{
\ifthenelse{\equal{#1}{C}}{{#2}\in\mathcal{#3}}{}
\ifthenelse{\equal{#1}{T}}{{#2}\in{#3}}{}
}

\newcommand{\ESum}[4][C]
{\ifthenelse{\equal{#1}{C}}{\underset{\inset{#3}{#4}}{\sum}#2}{}
 \ifthenelse{\equal{#1}{T}}{\underset{\inset[N]{#3}{#4}}{\sum}#2}{}
\ifthenelse{\equal{#1}{U}}{\overset{#4}{\underset{#3}{\sum}}#2}{}
\ifthenelse{\equal{#1}{X}}{\sideset{}{_{#3}^{#4}}{\sum}#2}{}
\ifthenelse{\equal{#1}{S}}{\sideset{}{_{#3}^{#4}}{\sum}#2}{}
\ifthenelse{\equal{#1}{O}}{{\underset{ (#3,#4)}{\sum}}#2}{}
\ifthenelse{\equal{#1}{I}}{{\underset{#3}{\sum}}#2}{}}

\newcommand{\VF}[2][N]{
\ifthenelse{\equal{#1}{L}}{V^{\pi}_{\lambda} (#2)}{}
\ifthenelse{\equal{#1}{C}}{V{^{\pi} (#2)}}{}
\ifthenelse{\equal{#1}{T}}{V^*_{\bar{\pi}} (#2)}{}
\ifthenelse{\equal{#1}{CO}}{V{^{*} (#2)}}{}
\ifthenelse{\equal{#1}{Mx}}{V^{\pi}_{\infty} (#2)}{}
\ifthenelse{\equal{#1}{Mxo}}{V^{\pi^*}_{\infty} (#2)}{}
\ifthenelse{\equal{#1}{Mn}}{V^{\pi}_{-\infty} (#2)}{}
\ifthenelse{\equal{#1}{Mno}}{V^{\pi^*}_{-\infty} (#2)}{}
}

\newcommand{\Eval}[1][null]{
\ifthenelse{\equal{#1}{null}}{\mathbb{E}}{\mathbb{E}_{#1}}
}

\newcommand{\M}[1][]{
\mathcal{M}_{#1}
}


\newcommand{\qv}[1][null]{
\ifthenelse{\equal{#1}{null}}{Q^*}{Q^{#1}}
}

\newcommand{\T}[1][null]{
\ifthenelse{\equal{#1}{null}}{\mathcal{T}}{\mathcal{T}^{#1}}
}



\newcommand{\subLim}[2]{
\underset{#1\rightarrow#2}{\lim}
}

\newcommand{\Norm}[2][]
{
\left\|#2\right\|_{#1}
}

\newcommand{\bldsym}{\boldsymbol}


\newtheorem{assumption}{Assumption}


\newcommand{\TODO}[1]{(\textbf{TODO: {#1}})}


 
\begin{abstract}
We introduce \algoinit{}, a deep reinforcement learning agent with  parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration.
The parameters of the noise are learned with gradient descent along with the remaining network weights. \algoinit{} is straightforward to implement and adds little computational overhead.
We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling  agents (entropy reward and -greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.
\end{abstract}
\section{Introduction}
\label{sec:introduction}
Despite the wealth of research into efficient methods for exploration in Reinforcement Learning (RL) \citep{kearns2002near,jaksch2010near}, most exploration heuristics rely on random perturbations of the agent's policy, such as -greedy \citep{sutton1998reinforcement} or entropy regularisation \citep{williams1992simple}, to induce novel behaviours. 
However such local `dithering' perturbations are unlikely to lead to the large-scale behavioural patterns needed for efficient exploration in many environments \citep{osband2017deep}.

\textit{Optimism in the face of uncertainty} is a common exploration heuristic in reinforcement learning.
Various forms of this heuristic often come with theoretical guarantees on agent performance \citep{azar2017minimax,lattimore2013sample,jaksch2010near,auer2007logarithmic,kearns2002near}.
However, these methods are often limited to small state-action spaces or to linear function approximations and are not easily applied with more complicated function approximators such as neural networks (except from work by \citep{geist2010kalman,geist2010managing} but it doesn't come with convergence guarantees). A more structured approach to exploration is to augment the environment's reward signal with an additional \textit{intrinsic motivation} term \citep{singh2004intrinsically} that explicitly rewards novel discoveries.
Many such terms have been proposed, including learning progress \citep{oudeyer07intrinsic}, compression progress \citep{schmidhuber2010formal}, variational information maximisation \citep{houthooft2016vime} and prediction gain \citep{bellemare2016unifying}.
One problem is that these methods separate the mechanism of generalisation from that of exploration; the metric for intrinsic reward, and--importantly--its weighting relative to the environment reward, must be chosen by the experimenter, rather than learned from interaction with the environment.
Without due care, the optimal policy can be altered or even completely obscured by the intrinsic rewards; furthermore, dithering perturbations are usually needed as well as intrinsic reward to ensure robust exploration \citep{ostrovski2017count}. 
Exploration in the policy space itself, for example, with evolutionary or black box  algorithms~\citep{moriarty1999evolutionary,fix2012monte,2017arXiv170303864S}, usually requires many prolonged interactions with the environment. Although these algorithms are quite generic and can apply to any type of parametric policies (including neural networks), they are usually not data efficient and require a simulator to allow many policy evaluations.

We propose a simple alternative approach, called \algoinit{}, where learned perturbations of the network weights are used to drive exploration.
The key insight is that a single change to the weight vector can induce a consistent, and potentially very complex, state-dependent change in policy over multiple time steps -- unlike dithering approaches where decorrelated (and, in the case of  -greedy, state-independent) noise is added to the policy at every step.
The perturbations are sampled from a noise distribution.
The variance of the perturbation is a parameter that can be considered as the energy of the injected noise.
These variance parameters are learned using gradients from the reinforcement learning loss function, along side the other parameters of the agent.
The approach differs from parameter compression schemes such as variational inference \citep{hinton1993keeping,bishop1995training,graves2011practical,blundell2015weight,gal16} and flat minima search \citep{hochreiter1997flat} since we do not maintain an explicit distribution over weights during training but simply inject noise in the parameters and tune its intensity automatically. Consequently, it also differs from Thompson sampling \citep{thompson1933likelihood,lipton2016efficient} as the distribution on the parameters of our agents does not necessarily converge to an approximation of a posterior distribution.


At a high level our algorithm is a randomised value function, where the functional form is a neural network. Randomised value functions provide a provably efficient means of exploration \citep{osband2014generalization}. Previous attempts to extend this approach to deep neural networks required many duplicates of sections of the network \citep{osband2016deep}.
By contrast in our \algoinit{} approach while the number of parameters in the linear layers of the network is doubled, as the weights are a simple affine transform of the noise, the computational complexity is typically still dominated by the weight by activation multiplications, rather than the cost of generating the weights. Additionally, it also applies to policy gradient methods such as A3C out of the box \citep{mnih2016asynchronous}.
Most recently (and independently of our work) \citet{plappert2017parameter} presented a similar technique where constant Gaussian noise is added to the parameters of the network. Our method thus differs by the ability of the network to adapt the noise injection with time and it is not restricted to Gaussian noise distributions. We need to emphasise that the idea of injecting noise to improve the optimisation process has been thoroughly studied in the literature of supervised learning and optimisation under different names (e.g., Neural diffusion process ~\citep{mobahi2016training} and graduated optimisation  ~\citep{hazan2016graduated}). These methods often rely on a noise of vanishing size that is non-trainable, as opposed to NoisyNet which tunes the amount of noise by gradient descent. 


NoisyNet can also be adapted to any deep RL algorithm and we demonstrate this versatility by providing \algoinit{} versions of DQN~\citep{mnih2015human}, Dueling~\citep{wang2016Dueling} and A3C~\citep{mnih2016asynchronous} algorithms. Experiments on 57 Atari games show that NoisyNet-DQN and NoisyNetDueling achieve striking gains when compared to the baseline algorithms without significant extra computational cost, and with less hyper parameters to tune. Also the noisy version of A3C provides some improvement over the baseline.
\section{Background}
\label{sec:background}
This section provides mathematical background for Markov Decision Processes (MDPs) and deep RL with Q-learning, dueling and actor-critic methods.
\subsection{Markov Decision Processes and Reinforcement Learning}
MDPs model stochastic, discrete-time and finite action space control problems~\citep{bellman1965dynamic,bertsekas1995dynamic,puterman1994markov}.
An MDP is a tuple  where  is the state space,  the action space,  the reward function,  the discount factor and  a stochastic kernel modelling the one-step Markovian dynamics ( is the probability of transitioning to state  by choosing action  in state ). A stochastic policy  maps each state to a distribution over actions  and gives the probability  of choosing action  in state . The quality of a policy  is assessed by the action-value function  defined as:

where  is the expectation over the distribution of the admissible trajectories  obtained by executing the policy  starting from  and . Therefore, the quantity  represents the expected -discounted cumulative reward collected by executing the policy  starting from  and .
A policy is optimal if no other policy yields a higher return.
The action-value function of the optimal policy is .

The value function  for a policy is defined as , and represents the expected -discounted return collected by executing the policy  starting from state .
\subsection{Deep Reinforcement Learning}

Deep Reinforcement Learning uses deep neural networks as function approximators for RL methods. Deep Q-Networks (DQN)~\citep{mnih2015human}, Dueling architecture \citep{wang2016Dueling}, Asynchronous Advantage Actor-Critic (A3C)~\citep{mnih2016asynchronous}, Trust Region Policy Optimisation~\citep{schulman2015trust}, Deep Deterministic Policy Gradient~\citep{lillicrap2015continuous} and distributional RL (C51)~\citep{bellemare2017distributional} are examples of such algorithms. They frame the RL problem as the minimisation of a loss function , where  represents the parameters of the network. In our experiments we shall consider the DQN, Dueling and A3C algorithms. 

DQN~\citep{mnih2015human} uses a neural network as an approximator for the action-value function of the optimal policy .
DQN's estimate of the optimal action-value function, , is found by minimising the following loss with respect to the neural network parameters :
 
where  is a distribution over transitions  drawn from a replay buffer of previously observed transitions. 
Here  represents the parameters of a fixed and separate target network which is updated () regularly to stabilise the learning.
An -greedy policy is used to pick actions greedily according to
the action-value function  or, with probability , a random action is taken.

The Dueling DQN  \citep{wang2016Dueling} is an extension of the DQN  architecture. The main difference is in using Dueling network architecture as opposed to the Q network in DQN. Dueling network estimates the action-value function using two parallel sub-networks, the value and advantage sub-network, sharing a convolutional layer. Let , , and  be, respectively, the parameters of the convolutional encoder , of the value network , and of the advantage network ; and  is their concatenation. The output of these two networks are combined  as follows for every  :



The Dueling algorithm then makes use of the double-DQN update rule \citep{van2016deep} to optimise :

 

where the definition distribution  and the target network parameter set  is identical to DQN. 

In contrast to DQN and Dueling, A3C~\citep{mnih2016asynchronous} is a policy gradient algorithm.
A3C's network directly learns a policy   and a value function  of its policy.
The gradient of the loss on the A3C policy at step  for the roll-out  is:

 denotes the entropy of the policy  and  is a hyper parameter that trades off between optimising the advantage function and the entropy of the policy.
The advantage function  is the difference between observed returns and estimates of the return produced by A3C's value network: ,  being the reward at step  and  being the agent's estimate of value function of state .

The parameters of the value function are found to match on-policy returns; namely we have 

\noindent where  is the return obtained by executing policy  starting in state .
In practice, and as in \cite{mnih2016asynchronous}, we estimate  as  where  are rewards observed by the agent, and  is the th state observed when starting from observed state .
The overall A3C loss is then  where  balances optimising the policy loss relative to the baseline value function loss.


\section{NoisyNets for Reinforcement Learning}
\label{sec:noisynets}

NoisyNets are neural networks whose weights and biases are perturbed by a parametric function of the noise. These parameters are adapted with gradient descent. More precisely, let  be a neural network parameterised by the vector of \emph{noisy} parameters  which takes the input  and outputs .  We represent the noisy parameters  as , where   is a set of vectors of learnable parameters,  is a vector of zero-mean noise with fixed statistics and   represents element-wise multiplication. The usual loss of the neural network is wrapped by expectation over the noise : . Optimisation now occurs with respect to the set of parameters .

Consider a linear layer of a neural network with  inputs and  outputs, represented by 
\beq
\label{eq:lin.layer}
y = wx + b,
\eeq
where  is the layer input,  the weight matrix, and  the bias.
The corresponding noisy linear layer is defined as:
\beqa
\label{eq:param}
y&\eqdef&(\mu^w+\sigma^w\odot\eps^w) x+ \mu^b  +\sigma^b\odot\eps^b ,
\eeqa
 where   and  replace  and  in Eq.~\eqref{eq:lin.layer}, respectively.  The parameters , ,  and  are learnable whereas   and  are noise random variables (the specific choices of this distribution are described below). We provide a graphical representation of a noisy linear layer in Fig.~\ref{fig:noisy linear layer} (see Appendix~\ref{sec:noisy linear layer}). 

We now turn to explicit instances of the noise distributions for linear layers in a noisy network. We explore two options: Independent Gaussian noise, which uses an independent Gaussian noise entry per weight and Factorised Gaussian noise, which uses an independent noise per each output and another independent noise per each input. The main reason to use factorised Gaussian noise is to reduce the compute time of random number generation in our algorithms. This computational overhead is especially prohibitive in the case of single-thread agents such as DQN and Duelling. For this reason we use factorised noise for DQN and Duelling and independent noise for the distributed A3C, for which the compute time is not a major concern.

\begin{enumerate}[(a)]
    \item Independent Gaussian noise: the noise applied to each weight and bias is independent, where each entry  (respectively each entry ) of the random matrix  (respectively of the random vector ) is drawn from a unit Gaussian distribution.
    This means that for each noisy linear layer, there are  noise variables (for  inputs to the layer and  outputs).
    \item Factorised Gaussian noise: by factorising , we can use  unit Gaussian variables  for noise of the inputs and
and  unit Gaussian variables  for noise of the outputs (thus  unit Gaussian variables in total). 
Each  and  can then be written as:

where  is a real-valued  function.
In our experiments we used . Note that for the bias Eq.~\eqref{eq:bias} we could have set , but we decided to keep the same output noise for weights and biases.
\end{enumerate}
Since the loss of a noisy network, , is an expectation over the noise, the gradients are straightforward to obtain:

We use a Monte Carlo approximation to the above gradients, taking a single sample  at each step of optimisation:

\subsection{Deep Reinforcement Learning with NoisyNets}
\label{sec:Reinforcement Learning using Randomised Networks}
We now turn to our application of noisy networks to exploration in deep reinforcement learning.
Noise drives exploration in many methods for reinforcement learning, providing a source of stochasticity external to the agent and the RL task at hand.
Either the scale of this noise is manually tuned across a wide range of tasks (as is the practice in general purpose agents such as DQN or A3C) or it can be manually scaled per task.
Here we propose automatically tuning the level of noise added to an agent for exploration, using the noisy networks training to drive down (or up) the level of noise injected into the parameters of a neural network, as needed.

A noisy network agent samples a new set of parameters after every step of optimisation.
Between optimisation steps, the agent acts according to a fixed set of parameters (weights and biases).
This ensures that the agent always acts according to parameters that are drawn from the current noise distribution.


\paragraph{Deep Q-Networks (DQN) and Dueling.}
We apply the following modifications to both  DQN and Dueling: first, -greedy is no longer used, but instead the policy greedily optimises the (randomised) action-value function.
Secondly, the fully connected layers of the value network are parameterised as a noisy network, where the parameters are drawn from the noisy network parameter distribution after every replay step. We used factorised Gaussian noise as explained in (b) from Sec.~\ref{sec:noisynets}.
For replay, the current noisy network parameter sample is held fixed across the batch.
Since DQN and Dueling take one step of optimisation for every action step, the noisy network parameters are re-sampled before every action.
We call the new adaptations  of DQN and Dueling, \algoinit{}-DQN and \algoinit{}-Dueling, respectively.

We now provide the details of the loss function that our variant of DQN is minimising.  When replacing the linear layers by noisy layers in the network (respectively in the target network), the parameterised action-value function  (respectively ) can be seen as a random variable and the DQN loss becomes the \algoinit{}-DQN loss:
 
\noindent where the outer expectation is with respect to distribution of the noise variables  for the noisy value function  and the noise variable  for the noisy target value function .
 Computing an unbiased estimate of the loss is straightforward as we only need to compute, for each transition in the replay buffer, one instance of the target network and one instance of the online network. We generate these independent noises to avoid bias due to the correlation between the noise in the target network and the online network. Concerning the action choice, we generate another independent sample   for the online network and we act greedily with respect to the corresponding  output action-value function. 


Similarly the loss function for \algoinit{}-Dueling is defined as:
 
 
 Both algorithms are provided in Appendix~\ref{sec: algo DQN}.


\paragraph{Asynchronous Advantage Actor Critic (A3C).}
A3C is modified in a similar fashion to DQN: firstly, the entropy bonus of the policy loss is removed.
Secondly, the fully connected layers of the policy network are parameterised as a noisy network. We used independent Gaussian noise as explained in (a) from Sec.~\ref{sec:noisynets}.
In A3C, there is no explicit exploratory action selection scheme (such as -greedy); and the chosen action is always drawn from the current policy. For this reason, an entropy bonus of the policy loss is often added to discourage updates leading to deterministic policies. However, when adding noisy weights to the network, sampling these parameters corresponds to choosing a different current policy which naturally favours exploration.
As a consequence of direct exploration in the policy space, the artificial entropy loss on the policy can thus be omitted. New parameters of the policy network are sampled after each step of optimisation, and since A3C uses  step returns, optimisation occurs every  steps. We call this modification of A3C, \algoinit{}-A3C.


Indeed, when replacing the linear layers by noisy linear layers (the parameters of the noisy network are now noted ), we obtain the following estimation of the return via a roll-out of size :

As A3C is an on-policy algorithm the gradients are unbiased when noise of the network is consistent for the whole roll-out.
Consistency among action value functions  is ensured by letting letting the noise be the \emph{same} throughout each rollout, i.e., .
Additional details are provided in the Appendix~\ref{sec:A3Cimplementation} and the algorithm is given in Appendix~\ref{sec: algo A3C}.

\subsection{Initialisation of Noisy Networks}

In the case of an unfactorised noisy networks, the parameters  and  are initialised as follows.
Each element  is sampled from independent uniform distributions , where  is the number of inputs to the corresponding linear layer, and each element  is simply set to  for all parameters. This particular initialisation was chosen because similar values worked well for the supervised learning tasks described in \cite{fortunato2017bayesian}, where the initialisation of the variances of the posteriors and the variances of the prior are related. We have not tuned for this parameter, but we believe different values on the same scale should provide similar results.

For factorised noisy networks, each element  was initialised by a sample from an independent uniform distributions  and each element  was initialised to a constant . The hyperparameter  is set to . 
\section{Results}
\begin{centering}
\begin{figure}[ht!]
    \subfigure[Improvement in percentage of \algoinit{}-DQN over DQN~\citep{mnih2015human} ]{\includegraphics[width=1.0\textwidth]{plotbar_DQN}}
    \subfigure[Improvement in percentage of \algoinit{}-Dueling over Dueling~\citep{wang2016Dueling} ]{\includegraphics[width=1.0\textwidth]{plotbar_Dueling}}
    \subfigure[Improvement in percentage of \algoinit{}-A3C over A3C~\citep{mnih2016asynchronous} ]{\includegraphics[width=1.0\textwidth]{ICLR_barplot_a3c}}
    \caption{Comparison of \algoinit{} agent versus the baseline according to Eq.~\eqref{eq:relative.human.norm}. The maximum score is truncated at 250\%.}
    \label{fig:normalised.bar} 
\end{figure}
\end{centering} 
We evaluated the performance of noisy network agents on 57 Atari games \citep{bellemare2015arcade} and compared to baselines that, without noisy networks, rely upon the original exploration methods (-greedy and entropy bonus).

\subsection{Training details and performance}
We used the random start no-ops scheme for training and evaluation as described the original DQN paper \citep{mnih2015human}. 
The mode of evaluation is identical to those of~\cite{mnih2016asynchronous} where randomised restarts of the games are used for evaluation after training has happened. The raw average scores of the agents are evaluated during training, every 1M frames in the environment, by suspending learning and evaluating the latest agent for 500K frames. Episodes are truncated at 108K frames (or 30 minutes of simulated play)~\citep{van2016deep}.

We consider three baseline agents: DQN \citep{mnih2015human}, duel clip variant of Dueling algorithm~\citep{wang2016Dueling} and A3C \citep{mnih2016asynchronous}. The DQN and A3C agents were training for 200M and 320M frames, respectively.
In each case, we used the neural network architecture from the corresponding original papers for both the baseline and \algoinit{} variant. For the \algoinit{} variants we used the same hyper parameters as in the respective original paper for the baseline.

We compared absolute performance of agents using the human normalised score:

where human and random scores are the same as those in \cite{wang2016Dueling}.
Note that the human normalised score is zero for a random agent and  for human level performance. Per-game maximum scores are computed by taking the maximum raw scores of the agent and then averaging over three seeds. However, for computing the human normalised scores in Figure~\ref{fig:learning curves median}, the raw scores are evaluated every 1M frames and averaged over three seeds. The overall agent performance is measured by both mean and median of the human normalised score across all 57 Atari games.

The aggregated results across all 57 Atari games are reported in Table~\ref{tab:totalhuman}, while the individual scores for each game are in Table~\ref{tl:raw.score} from the Appendix~ \ref{sec:learning_curves}.
The median human normalised score is improved in all agents by using \algoinit{}, adding at least  (in the case of A3C) and at most  (in the case of DQN)  percentage points to the median human normalised score.
The mean human normalised score is also significantly improved for all agents.
Interestingly the Dueling case, which relies on multiple modifications of DQN, demonstrates that \algoinit{} is orthogonal to several other improvements made to DQN.
\begin{table}[ht!]
\centering
\begin{tabular}{rccccc}
\toprule
 & \multicolumn{2}{c}{Baseline} & \multicolumn{2}{c}{\algoinit{}}&Improvement \\
 & Mean & Median & Mean & Median& (On median) \\
 \hline
 \\
DQN & 319 & 83 & \textbf{379} & \textbf{123} & 48\%\\ 
Dueling & 524 & 132 & \textbf{633} & \textbf{172} & 30\% \\
A3C & 293 & 80 & \textbf{347} & \textbf{94} &  18\%\\
\bottomrule
\end{tabular}
\vspace{1em}
 \caption{Comparison between the baseline DQN, Dueling and A3C  and their \algoinit{} version in terms of median and mean human-normalised scores defined in Eq.~\eqref{eq:absolute.human.norm}. We report on the last column the percentage  improvement on the baseline in terms of median human-normalised score.}\label{tab:totalhuman} 
\end{table}
We also compared relative performance of \algoinit{} agents to the respective baseline agent without noisy networks:

As before, the per-game score is computed by taking the maximum performance for each game and then averaging over three seeds. 
The relative human normalised scores are shown in Figure~\ref{fig:normalised.bar}.
As can be seen, the performance of \algoinit{} agents (DQN, Dueling and A3C) is better for the majority of games relative to the corresponding baseline, and in some cases by a considerable margin. Also as it is evident from the learning curves of Fig.~\ref{fig:learning curves median}  NoisyNet agents produce superior performance compared to their corresponding baselines throughout the learning process. This improvement is especially significant in the case of NoisyNet-DQN and NoisyNet-Dueling. Also in some games, \algoinit{} agents provide an order of magnitude improvement on the performance of the vanilla agent; as can be seen in Table \ref{tl:raw.score} in the Appendix~\ref{sec:learning_curves} with detailed breakdown of individual game scores and the learning curves plots from Figs \ref{fig:all_games_dqn}, \ref{fig:all_games_Dueling} and ~\ref{fig:all_games_a3c}, for DQN, Dueling and A3C,  respectively.
We also ran some experiments evaluating the performance of NoisyNet-A3C with factorised noise. We report the corresponding learning curves and the scores in Fig.~\ref{fig:learning curves median factorised} and Table \ref{table:factorised}, respectively (see Appendix \ref{sec:appx.factor.A3C}). This result shows that using factorised noise does not lead to any significant decrease in the performance of A3C. On the contrary it seems that it has positive effects in terms of improving the median score as well as speeding up the learning process. 
\begin{centering}
\begin{figure}[htp!]
    \includegraphics[width=0.33\textwidth]{DQN_median_scores}
    \includegraphics[width=0.33\textwidth]{Dueling_median_scores}
    \includegraphics[width=0.33\textwidth]{A3C_median_scores}
    \caption{Comparison of the learning curves of \algoinit{} agent versus the baseline according to the median human normalised score.}
    \label{fig:learning curves median} 
\end{figure}
\end{centering} 
\subsection{Analysis of Learning in Noisy Layers}
In this subsection, we try to provide some insight on how noisy networks affect the learning process and the exploratory behaviour of the agent. In particular, we  focus on analysing the evolution of the noise weights   and  throughout the learning process. We first note that, as  is a positive and continuous function of , there always exists a \emph{deterministic} optimiser for the loss  (defined in Eq.~\eqref{eq:Noisy.DQN.loss}). Therefore, one may expect that, to obtain the deterministic optimal solution, the neural network may learn to discard the noise entries by eventually pushing s and  towards . 

To test this hypothesis we track the changes in s throughout the learning process. Let   denote the  weight of a noisy layer. We then define , the mean-absolute of the s of a noisy layer, as

Intuitively speaking  provides some measure of the stochasticity of the Noisy layers. We report the learning curves of the average of  across  seeds in Fig.~\ref{fig:sigma_curves} for a selection of Atari games in NoisyNet-DQN agent. We observe that  of the last layer of the network decreases as the learning proceeds in all cases, whereas in the case of the penultimate layer this only happens for 2 games out of 5 (Pong and Beam rider) and in the remaining 3 games  in fact increases. This shows that in the case of NoisyNet-DQN the agent does not necessarily evolve towards a deterministic solution as one might have expected. Another interesting observation is that the way  evolves significantly differs from one game to another and in some cases from one seed to another seed, as it is evident from the error bars. This suggests that NoisyNet produces a problem-specific exploration strategy as opposed to fixed exploration strategy used in standard DQN.      
\begin{centering}
\begin{figure}[ht!]
    \includegraphics[width=0.5\textwidth]{penultimate_layer_error_bar}
    \includegraphics[width=0.5\textwidth]{last_layer_error_bars}
    \caption{Comparison of the learning curves of the average noise parameter  across five Atari games in NoisyNet-DQN. The results are averaged across 3 seeds and error bars (+/- standard deviation) are plotted.}
    \label{fig:sigma_curves} 
\end{figure}
\end{centering} 
\section{Conclusion}
\label{sec:conclusion}
We have presented a general method for exploration in deep reinforcement learning that shows significant performance improvements across many Atari games in three different agent architectures. In particular, we observe that in games such as Beam rider, Asteroids and Freeway that the standard DQN, Dueling and A3C perform poorly compared with the human player, \algoinit{}-DQN, \algoinit{}-Dueling  and \algoinit{}-A3C achieve super human performance, respectively. Although the improvements in performance might also come from the optimisation aspect since the cost functions are modified, the uncertainty in the parameters of the networks introduced by NoisyNet is the \textit{only} exploration mechanism of the method. Having weights with greater uncertainty introduces more variability into the decisions made by the policy, which has potential for exploratory actions, but further analysis needs to be done in order to disentangle the exploration and optimisation effects. 
 


Another advantage of NoisyNet is that the amount of noise injected in the network is tuned automatically by the RL algorithm. This alleviates the need for any hyper parameter tuning (required with standard entropy bonus and -greedy types of exploration). 
This is also in contrast to many other methods that add intrinsic motivation signals that may destabilise learning or change the optimal policy. 
Another interesting feature of the \algoinit{} approach is that the degree of exploration is contextual and varies from state to state based upon per-weight variances.
While more gradients are needed, the gradients on the mean and variance parameters are related to one another by a computationally efficient affine function, thus the computational overhead is marginal.
Automatic differentiation makes implementation of our method a straightforward adaptation of many existing methods.
A similar randomisation technique can also be applied to LSTM units~\citep{fortunato2017bayesian} and is easily extended to reinforcement learning, we leave this as future work.

Note \algoinit{} exploration strategy is not restricted to the baselines considered in this paper. In fact, this idea can be applied to any deep RL algorithms that can be trained with gradient descent, including DDPG~\citep{lillicrap2015continuous}, TRPO~\citep{schulman2015trust} or distributional RL (C51) \citep{bellemare2017distributional}. As such we believe this work is a step towards the goal of developing a universal exploration strategy. 

\paragraph{Acknowledgements}
We would like to thank Koray Kavukcuoglu, Oriol Vinyals,  Daan Wierstra, Georg Ostrovski, Joseph Modayil, Simon Osindero, Chris Apps, Stephen Gaffney and many others at DeepMind for insightful discussions, comments and feedback on this work.

\bibliography{iclr2018_conference}
\bibliographystyle{iclr2018_conference}
\newpage
\appendix

\section{\algoinit{}-A3C implementation details}

 \label{sec:A3Cimplementation}
 In contrast with value-based algorithms, policy-based methods such as A3C~\citep{mnih2016asynchronous} parameterise the policy  directly and update the parameters  by performing a gradient ascent on the mean value-function  (also called the expected return)~\citep{sutton1999policy}. A3C uses a deep neural network  with weights  to parameterise the policy  and the value . The network has one softmax output for the policy-head  and one linear output for the value-head , with all non-output layers shared. The parameters  (resp. ) are relative to the shared layers and the policy head (resp. the value head). A3C is an asynchronous and online algorithm that uses roll-outs of size  of the current policy to perform a policy improvement step. 

For simplicity, here we present the A3C version with only one thread. For a  multi-thread implementation, refer to the pseudo-code \ref{sec: algo A3C} or to the original A3C paper~\citep{mnih2016asynchronous}. In order to train the policy-head, an approximation of the policy-gradient is computed for each state of the roll-out :
 
 where  is an estimation of the return  The gradients are then added to obtain the cumulative gradient of the roll-out:
 
 A3C trains the value-head by minimising the error between the estimated return and the value . Therefore, the network parameters  are updated after each roll-out as follows:
 
 where  are hyper-parameters. As mentioned previously, in the original A3C algorithm, it is recommended to add an entropy term  to the policy update, where . Indeed, this term encourages exploration as it favours policies which are uniform over actions. 
 When replacing the linear layers in the value and policy heads by noisy layers (the parameters of the noisy network are now  and ), we obtain the following estimation of the return via a roll-out of size :
 
 We would like  to be a consistent estimate of the return of the current policy. To do so, we should force . As A3C is an on-policy algorithm, this involves fixing the noise of the network for the whole roll-out so that the policy produced by the network is also fixed. Hence, each update of the parameters  is done after each roll-out with the noise of the whole network held fixed for the duration of the roll-out:
 



\newpage

\section{Noisy linear layer}
 \label{sec:noisy linear layer}
 
 In this Appendix we provide a graphical representation of noisy layer.
 
\begin{figure}[!htp]
\begin{centering}
    \includegraphics[width=0.6\textwidth]{noisy_linear_layer.png}
    \caption{Graphical representation of a noisy linear layer. The parameters , ,  and  are the learnables of the network whereas   and  are noise  variables which can be chosen in factorised or non-factorised fashion. The noisy layer functions similarly to the standard fully connected linear layer. The main difference is that in the noisy layer both the weights vector and the bias is perturbed by some parametric zero-mean noise, that is, the noisy weights and the noisy bias can be expressed as  and , respectively. The output of the noisy layer is then simply obtained as .}
    \label{fig:noisy linear layer} 
    \end{centering}
\end{figure}

\newpage


\section{Algorithms}
\subsection{\algoinit{}-DQN and \algoinit{}-Dueling}
\label{sec: algo DQN}
\begin{algorithm}[!ht]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{ Environment;  set of random variables of the network}
\Input{DUELING Boolean; "true" for \algoinit{}-Dueling and "false" for \algoinit{}-DQN}
\Input{ empty replay buffer;  initial network parameters;  initial target network parameters}
\Input{ replay buffer size;  training batch size;  target network replacement frequency}
\Output{ action-value function}
\BlankLine
\For{ episode }{    
    \BlankLine
    Initialise state sequence \;
    \For{}{
    \tcc{ is the last element of the list }
        Set \;    
        Sample a noisy network \;
        Select an action \;
        Sample next state , receive reward \ and set \;
        Add transition  to the replay buffer \;
        \If{}{
        Delete oldest transition from \;
        }
        \tcc{ is a distribution over the replay, it can be uniform or implementing prioritised replay}
        Sample a minibatch of  transitions \;
        \tcc{Construction of the target values.}
        Sample the noisy variable for the online network  \;
        Sample the noisy variables for the target network \;
        \uIf{DUELING}{
        Sample the noisy variables for the action selection network \;
        }
        \For{}{

            \uIf{ is a terminal state}{
                \;
            }
            \uIf{DUELING}{
                \;
                \;
            }  
            \uElse{
                \;
            }  
            
            Do a gradient step with loss \;
        }
        
        \If{}{
        Update the target network: \;
        }
        
            
    }    
}
\caption{\algoinit{}-DQN / \algoinit{}-Dueling}
\end{algorithm}

\newpage
\subsection{\algoinit{}-A3C}
\label{sec: algo A3C}
\begin{algorithm}[!ht]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Environment , Global shared parameters , global shared counter  and maximal time .}
\Input{Thread-specific parameters , Set of random variables , thread-specific counter  and roll-out size .}
\Output{ the policy and  the value.}
\BlankLine
Initial thread counter \;
\Repeat{}{
    Reset cumulative gradients:  and .\;
    Synchronise thread-specific parameters:  and . \;
    .\;
    Get state  from \;
    Choice of the noise: \;
    \tcc{ is a list of rewards}
    \;
    \tcc{ is a list of actions}
    \;
    \tcc{ is a list of states}
     and \;
    \Repeat{ terminal or }{
        Policy choice: \;
        \; 
        Receive reward  and new state \;
         and \;
         and \;
        
        }
    \uIf{ is a terminal state}{
        \;
    }
    \uElse{
      \;
    }   
    \For{}{
        Update : .\;
        Accumulate policy-gradient: .\;
        Accumulate value-gradient: .\;
    }
    Perform asynchronous update of : \;
    Perform asynchronous update of : \;
      
}
\caption{\algoinit{}-A3C for each actor-learner thread}
\end{algorithm}


\newpage
\section{Comparison between NoisyNet-A3C (factorised and non-factorised noise) and A3C}

\label{sec:appx.factor.A3C}

\begin{figure}[!htp]
\begin{centering}
    \includegraphics[width=0.8\textwidth]{A3C_factorised_median_scores_all.png}
    \caption{Comparison of the learning curves of factorised and non-factorised NoisyNet-A3C versus the baseline according to the median human normalised score.}
    \label{fig:learning curves median facotrised} 
    \end{centering}
\end{figure}


\begin{table}[ht!]
\centering
\begin{tabular}{rccccc}
\toprule
 & \multicolumn{2}{c}{Baseline} & \multicolumn{2}{c}{\algoinit{}}&Improvement \\
 & Mean & Median & Mean & Median& (On median) \\
 \hline
 \\
DQN & 319 & 83 & \textbf{379} & \textbf{123} & 48\%\\ 
Dueling & 524 & 132 & \textbf{633} & \textbf{172} & 30\% \\
A3C & 293 & 80 & \textbf{347} & \textbf{94} &  18\%\\
A3C (factorised) & \textbf{293} & 80 & 276 & \textbf{99} & 24  \%\\
\bottomrule
\end{tabular}
\caption{Comparison between the baseline DQN, Dueling and A3C   and their \algoinit{} version in terms of median and mean human-normalised scores defined in Eq.~\eqref{eq:absolute.human.norm}. In the case of A3C we inculde both factorised and non-factorised variant of the algorithm. We report on the last column the percentage  improvement on the baseline in terms of median human-normalised score.}
\label{table:factorised}
\end{table}



\newpage
\section{Learning curves and raw scores}
\label{sec:learning_curves}

Here we directly compare the performance of DQN, Dueling DQN and A3C and their NoisyNet counterpart by presenting the maximal score in each of the 57 Atari games (Table \ref{tl:raw.score}), averaged over three seeds. In Figures~\ref{fig:all_games_dqn}-\ref{fig:all_games_a3c} we show the respective learning curves. 

\begin{table}[!ht]
\centering
\tiny
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
 Games & Human & Random & DQN & NoisyNet-DQN & A3C & NoisyNet-A3C & Dueling & NoisyNet-Dueling \\
\hline
 alien & \bf{7128} & 228 & 2404  242 & 2403  78 & 2027  92 & 1899  111 & 6163  1077 & 5778  2189 \\
 amidar & 1720 & 6 & 924  159 & 1610  228 & 904  125 & 491  485 & 2296  154 & \bf{3537  521} \\
 assault & 742 & 222 & 3595  169 & 5510  483 & 2879  293 & 3060  101 & 8010  381 & \bf{11231  503} \\
 asterix & 8503 & 210 & 6253  154 & 14328  2859 & 6822  181 & \bf{32478  2567} & 11170  5355 & 28350  607 \\
 asteroids & 47389 & 719 & 1824  83 & 3455  1054 & 2544  523 & 4541  311 & 2220  91 & \bf{86700  80459} \\
 atlantis & 29028 & 12580 & 876000  15013 & 923733  25798 & 422700  4759 & 465700  4224 & 902742  17087 & \bf{972175  31961} \\
 bank heist & 753 & 14 & 455  25 & 1068  277 & 1296  20 & 1033  463 & \bf{1428  37} & 1318  37 \\
 battle zone & 37188 & 2360 & 28981  1497 & 36786  2892 & 16411  1283 & 17871  5007 & 40481  2161 & \bf{52262  1480} \\
 beam rider & 16926 & 364 & 10564  613 & \bf{20793  284} & 9214  608 & 11237  1582 & 16298  1101 & 18501  662 \\
 berzerk & \bf{2630} & 124 & 634  16 & 905  21 & 1022  151 & 1235  259 & 1122  35 & 1896  604 \\
 bowling & \bf{161} & 23 & 62  4 & 71  26 & 37  2 & 42  11 & 72  6 & 68  6 \\
 boxing & 12 & 0 & 87  1 & 89  4 & 91  1 & \bf{100  0} & 99  0 & 100  0 \\
 breakout & 30 & 2 & 396  13 & \bf{516  26} & 496  56 & 374  27 & 200  21 & 263  20 \\
 centipede & \bf{12017} & 2091 & 6440  1194 & 4269  261 & 5350  432 & 8282  685 & 4166  23 & 7596  1134 \\
 chopper command & 7388 & 811 & 7271  473 & 8893  871 & 5285  159 & 7561  1190 & 7388  1024 & \bf{11477  1299} \\
 crazy climber & 35829 & 10780 & 116480  896 & 118305  7796 & 134783  5495 & 139950  18190 & 163335  2460 & \bf{171171  2095} \\
 defender & 18689 & 2874 & 18303  2611 & 20525  3114 & 52917  3355 & \bf{55492  3844} & 37275  1572 & 42253  2142 \\
 demon attack & 1971 & 152 & 12696  214 & 36150  4646 & 37085  803 & 37880  2093 & 61033  9707 & \bf{69311  26289} \\
 double dunk & -16 & -19 & -6  1 & 1  0 & 3  1 & 3  1 & \bf{17  7} & 1  0 \\
 enduro & 860 & 0 & 835  56 & 1240  83 & 0  0 & 300  424 & \bf{2064  81} & 2013  219 \\
 fishing derby & -39 & -92 & 4  4 & 11  2 & -7  30 & -38  39 & 35  5 & \bf{57  2} \\
 freeway & 30 & 0 & 31  0 & 32  0 & 0  0 & 18  13 & \bf{34  0} & 34  0 \\
 frostbite & \bf{4335} & 65 & 1000  258 & 753  101 & 288  20 & 261  0 & 2807  1457 & 2923  1519 \\
 gopher & 2412 & 258 & 11825  1444 & 14574  1837 & 7992  672 & 12439  16229 & 27313  2629 & \bf{38909  2229} \\
 gravitar & \bf{3351} & 173 & 366  26 & 447  94 & 379  31 & 314  25 & 1682  170 & 2209  99 \\
 hero & 30826 & 1027 & 15176  3870 & 6246  2092 & 30791  246 & 8471  4332 & \bf{35895  1035} & 31533  4970 \\
 ice hockey & 1 & -11 & -2  0 & -3  0 & -2  0 & -3  1 & -0  0 & \bf{3  1} \\
 jamesbond & 303 & 29 & 909  223 & 1235  421 & 509  34 & 188  103 & 1667  134 & \bf{4682  2281} \\
 kangaroo & 3035 & 52 & 8166  1512 & 10944  4149 & 1166  76 & 1604  278 & 14847  29 & \bf{15227  243} \\
 krull & 2666 & 1598 & 8343  79 & 8805  313 & 9422  980 & \bf{22849  12175} & 10733  65 & 10754  181 \\
 kung fu master & 22736 & 258 & 30444  1673 & 36310  5093 & 37422  2202 & \bf{55790  23886} & 30316  2397 & 41672  1668 \\
 montezuma revenge & \bf{4753} & 0 & 2  3 & 3  4 & 14  12 & 4  3 & 0  0 & 57  15 \\
 ms pacman & \bf{6952} & 307 & 2674  43 & 2722  148 & 2436  249 & 3401  761 & 3650  445 & 5546  367 \\
 name this game & 8049 & 2292 & 8179  551 & 8181  742 & 7168  224 & 8798  1847 & 9919  38 & \bf{12211  251} \\
 phoenix & 7243 & 761 & 9704  2907 & 16028  3317 & 9476  569 & \bf{50338  30396} & 8215  403 & 10379  547 \\
 pitfall & \bf{6464} & -229 & 0  0 & 0  0 & 0  0 & 0  0 & 0  0 & 0  0 \\
 pong & 15 & -21 & 20  0 & \bf{21  0} & 7  19 & 12  11 & 21  0 & 21  0 \\
 private eye & \bf{69571} & 25 & 2361  781 & 3712  161 & 3781  2994 & 100  0 & 227  138 & 279  109 \\
 qbert & 13455 & 164 & 11241  1579 & 15545  462 & 18586  574 & 17896  1522 & 19819  2640 & \bf{27121  422} \\
 riverraid & 17118 & 1338 & 7241  140 & 9425  705 & 8135  483 & 7878  162 & 18405  93 & \bf{23134  1434} \\
 road runner & 7845 & 12 & 37910  1778 & 45993  2709 & 45315  1837 & 30454  13309 & 64051  1106 & \bf{234352  132671} \\
 robotank & 12 & 2 & 55  1 & 51  5 & 6  0 & 36  3 & 63  1 & \bf{64  1} \\
 seaquest & \bf{42055} & 68 & 4163  425 & 2282  361 & 1744  0 & 943  41 & 19595  1493 & 16754  6619 \\
 skiing & \bf{-4337} & -17098 & -12630  202 & -14763  706 & -12972  2846 & -15970  9887 & -7989  1349 & -7550  451 \\
 solaris & 12327 & 1263 & 4055  842 & 6088  1791 & \bf{12380  519} & 10427  3878 & 3423  152 & 6522  750 \\
 space invaders & 1669 & 148 & 1283  39 & 2186  92 & 1034  49 & 1126  154 & 1158  74 & \bf{5909  1318} \\
 star gunner & 10250 & 664 & 40934  3598 & 47133  7016 & 49156  3882 & 45008  11570 & 70264  2147 & \bf{75867  8623} \\
 surround & 6 & -10 & -6  0 & -1  2 & -8  1 & 1  1 & 1  3 & \bf{10  0} \\
 tennis & -8 & -24 & \bf{8  7} & 0  0 & -6  9 & 0  0 & 0  0 & 0  0 \\
 time pilot & 5229 & 3568 & 6167  73 & 7035  908 & 10294  1449 & 11124  1753 & 14094  652 & \bf{17301  1200} \\
 tutankham & 168 & 11 & 218  1 & 232  34 & 213  14 & 164  49 & \bf{280  8} & 269  19 \\
 up n down & 11693 & 533 & 11652  737 & 14255  1658 & 89067  12635 & \bf{103557  51492} & 93931  56045 & 61326  6052 \\
 venture & 1188 & 0 & 319  158 & 97  76 & 0  0 & 0  0 & \bf{1433  10} & 815  114 \\
 video pinball & 17668 & 16257 & 429936  71110 & 322507  135629 & 229402  153801 & 294724  140514 & \bf{876503  61496} & 870954  135363 \\
 wizard of wor & 4756 & 564 & 3601  873 & 9198  4364 & 8953  1377 & \bf{12723  3420} & 6534  882 & 9149  641 \\
 yars revenge & 54577 & 3093 & 20648  1543 & 23915  13939 & 21596  1917 & 61755  4798 & 43120  21466 & \bf{86101  4136} \\
 zaxxon & 9173 & 32 & 4806  285 & 6920  4567 & \bf{16544  1513} & 1324  1715 & 13959  613 & 14874  214 \\
\hline
\end{tabular}
\caption{Raw scores across all games with random starts.}
\label{tl:raw.score}
\end{table}

\newpage


\begin{figure}[!ht]
\begin{centering}
    \includegraphics[width=\textwidth]{DQN_allgames.jpeg}
    \caption{Training curves for all Atari games comparing DQN and \algoinit{}-DQN.}
    \label{fig:all_games_dqn}    
    \end{centering}
\end{figure}

\newpage

\begin{figure}[!ht]
\begin{centering}
    \includegraphics[width=\textwidth,height=0.96\textheight]{Dueling_allgames.jpeg}
    \caption{Training curves for all Atari games comparing Duelling and \algoinit{}-Dueling.}
    \label{fig:all_games_Dueling}    
    \end{centering}
\end{figure}

\newpage

\begin{figure}[!htp]
\begin{centering}
    \includegraphics[width=\textwidth,height=0.96\textheight]{ICLR_all_a3c.jpeg}
    \caption{Training curves for all Atari games comparing A3C and \algoinit{}-A3C.}
    \label{fig:all_games_a3c}    
    \end{centering}
\end{figure}


 
\end{document}
