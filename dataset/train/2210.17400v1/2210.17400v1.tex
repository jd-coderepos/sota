\documentclass[runningheads]{styles/llncs}
\usepackage[svgnames]{xcolor}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{colortbl}
\usepackage[accsupp]{axessibility} 
\usepackage{capt-of}




\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\usepackage{subcaption}
\usepackage{float} \usepackage{tabularx} 
\usepackage{xcolor}
\usepackage{blindtext}
\usepackage{graphicx}
\captionsetup{font= footnotesize} \usepackage[export]{adjustbox}
\usepackage{algorithm2e}
\RestyleAlgo{ruled}
\SetKwComment{Comment}{/* }{ */}
\usepackage{pifont}

\makeatletter
\newcommand\tabcaption{\def\@captype{table}\caption}
\newcommand\figcaption{\def\@captype{figure}\caption}
\makeatother
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\orcid}[1]{\href{https://orcid.org/#1}{\includegraphics[width=10pt]{images/orcid.png}}}
\begin{document}
\pagestyle{headings}
\mainmatter


\title{Max Pooling with Vision Transformers reconciles class and shape in weakly supervised semantic segmentation } 


\titlerunning{ViT-PCM for WSSS by Image-Class Labels}


\author{Simone Rossetti\inst{1,2}\orcid{0000-0002-5344-7872},
Damiano Zappia\inst{1}\orcid{0000-0002-5726-2488},
Marta Sanzari\inst{2}\orcid{0000-0002-4640-9122},\\
Marco Schaerf\inst{1,2}\orcid{0000-0002-2016-1966},
Fiora Pirri\inst{1,2}\orcid{0000-0001-8665-9807}}
\authorrunning{Rossetti et al.}
\institute{DeepPlants,
\email{@deepplants.com}\\
\and
DIAG, Sapienza
\email{@diag.uniroma1.it}}


\maketitle
\typeout{--------------------Abstract -----------------------}
\begin{abstract}
Weakly Supervised Semantic Segmentation (WSSS) research
has explored many directions to improve the typical pipeline CNN plus
class activation maps (CAM) plus refinements, given the image-class label as the only supervision.
Though the gap with the fully supervised methods is reduced,
further abating the spread seems unlikely within this framework. 
On the other hand, WSSS methods based on Vision Transformers (ViT) have not yet explored valid alternatives to CAM. ViT features have been shown to retain a scene layout, and object boundaries in self-supervised learning. To confirm these findings, we prove that the advantages of transformers in self-supervised methods are further strengthened by Global Max Pooling (GMP), which can leverage patch features to negotiate pixel-label probability with class probability. This work proposes a new WSSS method dubbed ViT-PCM (ViT Patch-Class Mapping), not based on CAM.
The end-to-end presented network learns with a single optimization process, refined shape and proper localization for segmentation masks. Our model outperforms the state-of-the-art on baseline pseudo-masks (BPM), where we achieve 69.3\% mIoU on PascalVOC 2012 \textit{val} set. We show that our approach has the least set of parameters, though obtaining
higher accuracy than all other approaches. In a sentence, quantitative and qualitative results
of our method reveal that ViT-PCM is an excellent alternative to
CNN-CAM based architectures.

\keywords{weakly-supervised semantic segmentation, Vision Transformers, Global Max Pooling, Image class-labels supervision}
\end{abstract}


\typeout{---------------------Introduction -----------------------}
\section{Introduction}\label{sec:intro}
\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{images/CAM-PCM}
\caption{The figure compares the basic structure of a CNN-CAM method, above in light blue, with our proposed ViT-PCM method, below in light green. ViT-PCM learns to estimate the BPM, shown in the last two strips, with a single optimization.  Our BPM  are then refined with a CRF (see Figure \ref{fig:ablatiob_argmax}) and, without  further processing, are passed to the verification task (DeepLab). Differently from ViT-PCM, a CAM-based method demands a multi-stage optimization. All recent approaches require boosting the BPM, improved by the CRF, before passing them to the verification task.} \label{fig:good-seeds}
\end{figure}
Weakly supervised semantic segmentation (WSSS) is about segmenting object classes with no pixel-label supervision and using the less demanding supervision possible. The most economic supervision is via image-level class labels, out of which a WSSS method computes pseudo-masks for each object class in an image. To test a WSSS method accuracy,  a supervised segmentation network, such as DeepLab \cite{Chen2018DeepLabSI}, is trained on the devised pseudo-masks, and the induced accuracy is compared with the fully supervised methods. The segmentation task, supervised by the pseudo-mask labels, is a {\em verification task} aiming at demonstrating the computed pseudo-mask quality. In principle, the verification task adds equal improvement to all methods.

So far, methods based on image-level class labels generate pseudo-mask using class activation maps (CAM) \cite{zhou2016learning}. CAM are obtained from a multi-label classification network, such as a CNN. 

CAM limitations in estimating both shape and localization of the classes of interest \cite{chen2022class,guo2019visual,Bae2020,wang2020self} induce many researchers to resort to extra refinements between the baseline pseudo-masks (BPM), often called  {\em seeds}, and the final pseudo-masks production for test verification. These refinements mostly often bring into play multi-stage architectures, as noted in PAMR \cite{araslanov2020single}. 
Several authors resort to saliency maps as subsidiary supervision for good localization \cite{wei2016stc,Lee_2021_CVPR,xu2021leveraging,sun2022inferring,zhou2022regional}. Other authors adopt image operations such as region erasing \cite{singh2017hide,wei2017object}, or region growing 
 to expand the
seed region during training \cite{kolesnikov2016seed,huang2018weakly}, and  multi-scale map fusion to improve background and foreground \cite{wei2018revisiting}. 
Jang {\em et Al.} \cite{jiang2019integral} reviewed the  feature layers selection for CAMs using attribute propagation methods \cite{montavon2017explaining}.  Sun  {\em et Al.} \cite{sun2020mining}  estimate the similarity of the foreground features of the same class with two co-attention networks to capture better the context and \cite{Fan2020CIANCA} look into relations across different images. 

Yet, the greatest success in refinement strategies has been earned  by IRNet \cite{ahn2019weakly},  PSA \cite{ahn2018learning},  and AdvCAM \cite{lee2021anti}. Also,  PAC \cite{su2019pixel} and BENet \cite{chen2020weakly}, have been recently used. For example, SEAM \cite{wang2020self}, Chang  
{\em et Al.} \cite{chang2020weakly} and \cite{shimoda2019self} use PSA;  CONTA \cite{dong_2020_conta} and ReCAM \cite{chen2022class} use IRNet while \cite{lee2021anti} using both.  AFA \cite{ru2022learning} use PAC \cite{su2019pixel}. 

CRF\cite{krahenbuhl2011efficient} are trained on PascalVOC, fully supervised, and introduced in WSSS by \cite{kolesnikov2016seed}. CRF used as post-processing out of a training loop, improve the BPM, on average, 3-4\% mIoU, on Pascal VOC 2012. On the other hand, multi-stage methods, refining BPM with IRNet \cite{ahn2019weakly},  PSA \cite{ahn2018learning},  and AdvCAM \cite{lee2021anti} use dense CRF in the training loop, which gives a substantial boost in accuracy.   Using dense CRF, optimized on PascalVOC, likewise using saliency (e.g. \cite{hou2017deeply}, which operates dense CRF too)   in the refinement loop to obtain the final pseudo-mask, beside being resource intensive,  fails to generalize a method beyond the PascalVOC dataset. This lack of generalization power is common to any  WSSS approach using biased methods in a refinement training loop. 

\begin{figure}[t!]
  \centering
  \includegraphics[width=1\linewidth]{images/networklegend.jpg}
   \caption{The above schema shows the end-to-end ViT-PCM, a semantic segmentation method supervised by image-level class labels . The plate in (a) shows the core network  implementing the {\em linear search method}, which maps the image-level class labels to patch-labels. The plate (b) shows the two-branches architecture, including  in both branches.  }
   \label{fig:network}
\end{figure}

The challenge is to raise the bar of the baseline pseudo-mask accuracy so that the only supervision truly sticks to the image-level label. To this end, we introduce a new model for computing pseudo-masks, which bypasses the CAM bottleneck.
The main contributions of the paper are the followings:
\begin{itemize} \setlength{\parskip}{0pt}  \setlength{\itemsep}{0pt plus 1pt}
\item  We introduce a novel model for weakly supervised semantic segmentation (WSSS) based on ViT \cite{dosovitskiy2021image}. The model, dubbed ViT-PCM, is represented in Figure \ref{fig:network}. 
\item We propose a new pseudo-mask computation method {\em Explicit Search}  without resorting to CAM. The method leverages the locality properties of ViT to come close to an effective mapping between multi-label classification and semantic segmentation. We use the Global Max Pooling (GMP) to fetch the relevance of each patch, given the patches' categorical distribution over the classes of interest. This way, we project the patch features to class predictions (PCM) using a multi-label BCE loss (MCE).   We ensure equivariance to translation and scaling transformations defining two branches, see Figure \ref{fig:network}. 
\item The proposed pseudo-mask computation outperforms all state-of-the-art methods: we obtain BPM accuracy of 67.7 mIoU\% on Pascal VOC 2012 \textit{train} set which improves the current best BPM (\cite{ru2022learning}) of 3.91\% . On average, we improve more than 5\% mIoU than all the other competitors. On MS-COCO 2014 we obtain 45.03\% mIoU on \textit{val} set.
\item For the verification task, using  DeepLab as a segmentation method,  we do not need to boost our BPM to obtain masks more suitable for DeepLab, yet we obtain comparable validation and test scores.
\item We also prove the advantages of our method in terms of computational effort. In particular, we obtain the final segmentation with 89.4 M of parameter size, the minimal cost amid competitors. 
\end{itemize}
\vspace{-\topsep}
Beyond the novelty of our contribution, which is the first proposal to compute pseudo-mask baselines bypassing CAM, we show that both quantitative and qualitative results prove that exploring new methods for baseline pseudo-masks can be rewarding. We establish a new state of art on baseline pseudo-mask computation, using image-level class labels without refinement. 

\typeout{---------------------Related -----------------------}
\section{Related Works}\label{sec:related}


Current WSSS methods mostly operate with image-level class labels as the cheapest supervision. Approaches using image-level class labels are based so far on CAM \cite{zhou2016learning} methods using a plain multi-label classification network. The class activation maps are obtained via the global average pooling (GAP) averaging the feature maps of the last layer, further concatenated into a weights vector. This last is connected with the class prediction, using a BCE prediction loss. More recently,  Vision Transformers \cite{dosovitskiy2021image}  are emerging as an alternative to generate CAM \cite{xu2022multi,ru2022learning}. Our method is the first one using only  ViT without CAM to generate baseline pseudo-masks. 
\vskip 0.5\baselineskip
\noindent
{\bf CNN plus CAM.}\  These methods contribute to two complementary research directions:  {\em Baseline Pseudo-Mask generation}, to control and expand the activation of CAM regions, and  {\em Pseudo-Mask refinement} to obtain the full mask of objects.   

\noindent
{\em Baseline Pseudo-Mask generation} extends  CAM by revising the loss, or by augmenting the dataset, or by perturbing CAM devised regions, or using pretrained saliency maps. In ReCAM \cite{chen2022class} the authors propose softmax cross-entropy (SCE)  as a valid solution for CAM, since it bypasses the non-exclusive class problem of BCE. In OoD \cite{lee2022weakly} the authors propose an out of distribution dataset taken from OpenImages \cite{kuznetsova2020open}, to better capture background semantics. Other methods to expand CAM perturb the generated regions to capture new areas \cite{kweon2021unlocking,stammes2021find,lee2021anti}, by either erasing or masking. 
Since \cite{wei2016stc}, pretrained methods for saliency detection and saliency maps have been adopted in \cite{oh2017exploiting,zhang2020splitting,Lee_2021_CVPR,yao2021non,wu2021embedded,xu2021leveraging}, and in \cite{jiang2019integral,jiang2021online}. The latter propose an online attention accumulation (OAA) strategy based on attribute propagation methods.  Pseudo-mask generation is contaminated by self-supervised learning in \cite{wang2020self},  via downstream tasks and transformations ensuring CAM features equivariance, or via contrastive representation learning, as in RCA \cite{zhou2022regional}, C2AM \cite{xie2022c2am} and PPC \cite{du2022weakly}.

\noindent
{\em Pseudo-Mask refinement.} In recent works, all CAM-based approaches explore refinement strategies, ensuring some control on pixel-level labelling. The most common strategies are PSA \cite{ahn2018learning},  AdvCAM \cite{lee2021anti} and IRNet \cite{ahn2019weakly}. PSA  refines the baseline masks by propagating pixel semantic values to their neighbours, collecting confidence for the target classes. AdvCAM \cite{lee2021anti} uses iterative adversarial climbing performed on an image to iteratively involve its features in the classification to increase CAM confidence in activated regions. IRNet \cite{ahn2019weakly} explores class equivalence relations of pixels and refines pixel-labels by evaluating the displacement w.r.t. computed centroids. 
Recently BENet \cite{chen2020weakly} has been used for pseudo-mask baseline refinement, too; it refines object boundaries, together with foreground and background. We observed in the introduction that all these strategies use in the training loop dense CRF of \cite{krahenbuhl2011efficient}, which is trained on PascalVOC2012.


 
\vskip 0.5\baselineskip
\noindent
{\bf Transformers.} ViT have so far gathered a significant success with self-supervised learning \cite{dosovitskiy2021image}, as witnesses Dino \cite{caron2021emerging}, \cite{chen2021empirical,li2021efficient}, and recently SDMP \cite{ren2022simple}.  Dino \cite{caron2021emerging} downstream task segments foreground from background for single class images, differing from WSSS. Only recently ViT contributed to WSSS with MCTformer \cite{xu2022multi} and AFA \cite{ru2022learning}, though both resort to CAM. MCTformer exploits ViT attention mechanism to obtain localization maps. To generate pseudo-masks, they resort to PSA \cite{ahn2018learning}. AFA uses ViT multi-head-self-attention (MSA) to capture global dependencies and develop an affinity-based attention module to propagate the initial pseudo-masks, namely the obtained CAM. Refinement of the initial pseudo-mask is attained by affinity propagation with RAWK \cite{vernaza2017learning}, in turn, pretrained on a scribble dataset. 

Differently from the above approaches, we use ViT as a backbone for building our explicit search method. Indeed, we devise an end-to-end internal refinement to obtain a baseline pseudo-mask (BMP) without resorting to external strategies. 





\typeout{---------------------Motivations -----------------------}
\section{Motivations of using ViT and bypass CAM}\label{sec:motivations}
\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{images/linearSearch}
\caption{PCM: Patch Class Mapping.} \label{fig:lls}
\end{figure}
At the core of semantic segmentation, supervised by image-level labels, is the mapping between multilabel image classification and pixel-level classification. 
This mapping requires linking the abstract image feature space, encoding classes into an index vector, to a completely different space in which features encode classes into a fine grid structure.  
How could this be possible? CNN have an inductive bias on the image features local structure because of convolution kernels,  which  CAM leverages. The inductive bias of CNN entitles CAM to indicate the pixels which mainly contribute to the specific class prediction. The produced map is appealing though misleading: it does not induce a mapping between image features and pixels. 

On the other hand, ViT \cite{dosovitskiy2021image} have much less bias because images are split into flattened patches and encoded. Thus, the spatial relations are learned from scratch using attention and position embedding. This learning from a {\em tabula rasa} generates a number of basis functions for each patch, specifying their internal structure. These basis functions account implicitly for the class a patch belongs to. On these grounds, the mapping problem amounts to unravelling the implicit class representation brought on by the patch principal components. Our proposed {\em explicit search method} models this mapping.

We describe here the intuition. Let us assume that patches are pixels, the classes (categories) are denoted by , having cardinality  and  is an image. Let also assume that the ViT inferring the image multiclass labels is the function  with parameters , mapping an image  to a vector of values in  for each category in . On the other hand, let us represent the basis functions specifying the patches' internal structure, implicitly accounting for the patch classes, by a tensor . We shall see below how Z is computed.  has height and width as the image , and it also has a third axis for the categories . We make  a stochastic tensor along the categories axis: summing up along that axis, we obtain a matrix of ones. Let , with parameter , play the role of the segmentation model; namely, it evaluates the likelihood that a patch of the original image belongs to some precise class in . 

We argue that  Global Max Pooling (GMP) relates the two models  and  as follows. 
Let  be the slice of , along the categories axis, which should specify the patches internal structure for the category .   selects the most relevant element of , namely the element with the highest confidence to belong to the category , and returns a probability value  that it  belongs to class .
The selected element ,  at the same time, is the one in highest consideration to tell whether or not the category  appears in the image. In this way, GMP links image class prediction and patch class prediction. 
\typeout{--------------------Method -----------------------}

\section{The explicit search method}\label{sec:explicit_search}


This section considers the optimization method leading to estimating the map between image classes and patch classes. The  end-to-end architecture enclosing the method is described in Figure \ref{fig:network}, and in Section \ref{sec:architecture}. 

Let us indicate by  the network taking inputs from a dataset .  Here  indicates an input images, possibly obtained from an augmented and transformed set,   are the ground truth binary labels, and  is the number of classes defined by the category set . The output of  is a tensor  which is a {\em baseline pseudo-mask}.

ViT is part of . We recall that ViT partitions the image , resized image of the original  ,  into  patches of size . In particular, we are interested in the feature maps , with , with .  The feature maps   are the encoded representations of the patches, obtained by ViT.   represent the basis functions specifying the patches internal structure.  


\subsubsection{Explicit search by Global Max-Pooling}
Given , we consider also a weight matrix   whose weights are taken into account in the  optimization method described below. More precisely, we estimate the baseline pseudo-mask , training the weights   with only image-level class labels as supervision,  minimizing the multilabel classification error. 

The first objective  is to minimize the multilabel classification prediction error (MCE). Thus, given the ground truth binary labels  defined above, and recalling that  are the number of classes,  we model the multi-label classification using  independent Bernoulli distributions and  binary cross-entropy losses (BCE):

Let us consider first how   is obtained.  Let:

 represents the semantic segmentation predictions, needing to be projected into class predictions\footnote{Note that we are representing here  as a matrix, which is simply a reshaping of the tensor  discussed in Section \ref{sec:motivations}.}. We do so using Global Max Pooling (GMP):

Here:
 
The feature maps   are the encoded representation of patches , and  is the feature map of patch , while   is the logit of patch ,  with respect to class . 

Given the vector , we show how the optimization obtains the terms separating the feature space by the relative error backpropagation of , with respect to weights . Computing the gradient of Eq. (\ref{eq:bce}) w.r.t. the weight , we obtain:

Let us analyze the gradient of the weights , with respect to each column , of size , with .  Applying the chain rule, w.r.t. the generic class :


Here we used the fact that , and   from eq. (\ref{eq:gmp}). Therefore,  the gradient dimension is . 
The derivation of each term is provided in the supplementary.



Let   us select, now,  the column  of the weights , this column will be updated by the quantity:

Note that here the subscripts  in  and  indicate, respectively, the indexes at which   have maximum value, w.r.t classes  and , where  is obtained by the last two terms of equation \ref{eq:chain}, r.h.s.  We are using these indexes only in the updating rule for the weights; we are not using them in the derivation.

Eq. \ref{eq:backproploss} specifies the linear-search mechanism of the proposed optimization, iteratively selecting the most representative features  of each category . At each step, the optimization  updates the full column rank matrix  and returns the minimum error norm solution, which  separates the feature vector space  into  linear sub-spaces. 
Considering the optimization manifold,   the vector   moves  in the direction of the best representative feature vector , with either  being of the same category of the chosen column , or not. More precisely, at each iteration,  moves in the direction of   according to the error value , and in the direction  according to the term , for any category , with .


More specifically, when the term ,  and the category  is considered,    moves in the direction opposite to the best representative feature vector . On the other hand, when   the term considered is  which is added to , for its  updating. Note that, in this case,  the update term is increasingly small, since  as .  
This optimization method, based on iterative learning and stochastic gradient descent, induces a separation in the space of patch features, according to the multilabel classification.


 
\section{ViT-PCM model  structure}\label{sec:architecture}



The model architecture has two branches, as shown in Figure \ref{fig:network}. We describe its components in the following.


\noindent
{\bf Augmentation.} The batch of input images is augmented as usual in the first branch. In the second branch, images are translated, rotated and scaled. Furthermore, we merge four images from the batch into a single image after scaling them to have a different tiling of the images into patches.  


 
 \noindent
{\bf ViT patch encoder.} The Vision Transformer encoder takes as input the augmented batch of images and returns the features  and the  patches described in the {\em explicit search} method, Section \ref{sec:explicit_search}.

\noindent
{\bf HV-BiLSTM patch conditioning.} Two bidirectional LSTM (BiLSTM) process row-wise and   column-wise  the features  transformed to a tensor grid.  The two BiLSTM outputs are concatenated into a HV-BiLSTM (for Horizontal and Vertical), and their feature maps  are fed to the Patch Classifier. The HV-BiLSTM improves information amid neighbour patches by conditioning each patch on all other ones in horizontal (H)  and vertical directions (V) \cite{van2016pixel}.  
 


\noindent
{\bf Patch Classifier (PC).} While ViT and the two BiLSTM encode class information into the patch features, the Patch Classifier  implements the BPM generation, as described in the explicit search method, Section \ref{sec:explicit_search}. 

\noindent
{\bf Two branches for Equivariant regularization.} \label{par:equiv}
 ViT are not equivariant to translations because of the absolute positional encoding used for self-attention. Romero {\em et Al.} \cite{romero2020group} show that for self-attention to be equivariant to group transformations, they must act directly on positional encoding. 
 In our ViT-based method, though GMP is independent of the positional encoding and is invariant to transformations, the BPM generation is not. 
To remedy we resort to typical self-supervised learning tasks, using two branches enabling the network to learn equivariance properties. Equivariance encourages the feature representation to change coherently to the transformation applied to the input \cite{dangovski2021equivariant}. As discussed above, we apply affine transformations to both the network branches in the preprocessing step.  
After the same processing steps of the main branch, the sibling one applies an inverse merging of the features and upscales them to obtain the   patches feature maps as in the main branch. Finally,  inverse affine transformations are applied to both branches. 

The outcome is that these transformations cope both with positional encoding and spatial transformations.
The loss to be minimized is the cross entropy  loss , taking into account the transformations in the two branches:
 

Here,  is the images domain,   are affine transformations in the first and second branch,  is the above defined merging operation, and . 

\noindent
{\bf Final loss}
We have the  loss, conveying the mapping between image classification and patch classification, and  , which ensures equivariance and scales the images so that patches get pixel dimension. The  final loss is then:

Training the end-to-end network by minimizing this final loss obtains the baseline pseudo-mask.
\typeout{---------------------Experiments and results -----------------------}
\section{Experiments and results}\label{sec:exp}
\subsection{Set-Up}\label{subsec:impdet}

\begin{figure}[t]
\centering
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_000129}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_000129_1}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_000129_2}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_000129_3}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_000129_argmax}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_000129_crf}
\end{subfigure}\\
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_004380}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_004380_1}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_004380_2}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_004380_3}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_004380_argmax}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2007_004380_crf}
\end{subfigure}\\
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2010_002929}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2010_002929_1}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2010_002929_2}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2010_002929_3}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2010_002929_argmax}
\end{subfigure}
\begin{subfigure}[t]{.15\linewidth}
\includegraphics[width=\linewidth]{images/ablation/2010_002929_crf}
\end{subfigure}

\includegraphics[width=0.97\linewidth,left]{images/ablation/fig3_captions}
\caption{Columns (b)-(c)-(d) show the BPM inferred by our ViT-PCM, with probabilities highlighted by  heatmaps: values in yellow indicate the pixels' probability of belonging to the predicted class. Column (e)  is the scaled BPM,  obtained by selecting from the distribution of each patch the category indices with maximum probability (argmax). Column  (f) displays the BPM argmax refined by CRF.}
\label{fig:ablatiob_argmax}
\end{figure} 
\noindent
{\bf Datasets.}
We conducted our experiments on Pascal VOC 2012 \cite{everingham2010pascal} (20 categories) and on MS COCO 2014\cite{lin2014microsoft} (80 categories), the additional background class is inferred. The Pascal VOC 2012 Dataset \cite{everingham2010pascal} is usually augmented with the SBD dataset \cite{hariharan2011semantic}. The images in train sets of PASCAL VOC and MS COCO are annotated with image-level labels
only. We report mean Intersection-Over-Union
(mIoU) as the evaluation criteria.

\vskip0.3\baselineskip
\noindent
{\bf Networks Configuration.}
For the ViT transformer  backbones \cite{dosovitskiy2021image} we used ViT-S/16 and ViT-B/16 architectures, pre-trained on ImageNet22K  and fine-tuned on ImageNet2012 \cite{russakovsky2015imagenet}. We designed an MLP layer projecting the patch features into a categorical distribution on the  classes as a baseline model for ablation purposes. For the verification task, we used DeepLab V2\cite{chen2018encoder}.\

\noindent
{\bf Reproducibility.}
Images are resized to  for training and augmented by random colour jitter, random grayscale,   rotation, and vertical and horizontal flip. 
Initially, we freeze the backbone and ignore the output feature for the  token. At the same time, we preserve the  encoded patch features as input to the BiLSTM conditioning, whose outcome features are passed to the Patch Classifier. 
We initialize the MLP layer with standard Gaussian distribution and use L2 regularization with coefficient . 
We ran our training sessions iterating over the entire dataset, each epoch measuring the mIoU(\%) progresses on the PascalVOC 2012 and MS COCO2014 validation sets. We keep the input resolution to  to hasten the evaluation on a 4 NVIDIA
Titan V GPUs with 12GB RAM each, a deliberately limited resources setup. We use Adam optimization and schedule the learning rate as follows:   learning rate for the first two epochs with a frozen backbone; then, we unfreeze the last four backbone layers and keep training until convergence with  learning rate. At inference time, we scale the input image to  to get pseudo-label segmentation maps of shape . As expected, we noticed an increase in performance of about  mIoU scores for validation in the training session, confirming that ViTs scales very well on larger input size.

\subsection{Ablation studies}\label{subsec:ablations}
In Table \ref{tab:pseg} we evaluate ViT-PCM computation both with backbone ViT-S/16 and ViT-B/16, considering each component of the end-to-end network. We adopted a patch size of  since the memory requirements grow quadratically with the number of patches. The low scores of the () in Table \ref{tab:pseg} are due to the difficulty in encoding the background without equivariance. We observe that with the equivariance,  there is an improvement of 15.2 mIoU\% on the \textit{train} set and 13.1 mIoU\% on the \textit{val} set for PascalVOC 2012. A further improvement of 4.4 on the \textit{train} set and 5.4 mIoU\% on the \textit{val} set is obtained by conditioning the patches with HV-BiLSTM. Finally, we add the dCRF\cite{krahenbuhl2011efficient}  as post-processing obtaining an improvement of 3.5 mIoU\% on \textit{train} set. \noindent
\begin{table}[tbp]
  \centering
  \caption{Ablation on our ViT-PCM model for baseline pseudo-mask production, on PASCAL-VOC 2012 values in mIoU\%.}
   \resizebox{0.85\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \hline
{\bf Backbone} &  &   & HV-BiLSTM & CRF   & {\bf train} & {\bf val}\\
   \hline
   \hline
     ViT-S/16 &  &       &       &       &   44.0    & 43.3\\
     \cline{2-7}
  &  &  &       &       &   59.2 {\color{ForestGreen} {+}15.2}   & 56.4 {\color{ForestGreen} {+}13.1} \\
\cline{2-7}          
   &  &  & &       & 63.6 {\color{ForestGreen} {+}4.4} & 61.8{\color{ForestGreen} {+}5.4} \\
\cline{2-7}          
 &  &  &  &  & 67.1 {\color{ForestGreen} {+}3.5} & 64.9{\color{ForestGreen} {+}3.1} \\
\hline
   ViT-B/16 &  &       &       &       &   45.6    & 44.1\\
     \cline{2-7}
  &  &  &       &       &   65.1 {\color{ForestGreen} {+}19.5}   & 62.4 {\color{ForestGreen} {+}18.3} \\
\cline{2-7}          
   &  &  & &       & 67.7 {\color{ForestGreen} {+}2.6} & 66.0{\color{ForestGreen} {+}3.6} \\
\cline{2-7}          
  &  &  &  &  & 71.4 {\color{ForestGreen} {+}3.7} & 69.3{\color{ForestGreen} {+}3.3} \\
    \hline
    \end{tabular}}
  \label{tab:pseg}\end{table}\begin{table}[htbp]
\caption{ mIoU(\%) of BPM on  PascalVOC 2012 \textit{val}  set. w/wo CRF}
\label{tab:ablation_categories}
\resizebox{\textwidth}{!}{\begin{tabular}{c | c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c|  c} 
 \hline
 Method & bkg & plane & bike & bird & boat & btl & bus & car & cat & chair & cow & table & dog & horse & mbk & person & plant & sheep & sofa & train & tv &  mIoU(\%)\\ 
 \hline
 pseudo-masks w/o CRF & 87.2& 66.4 & 36.9 & 61.0 & 61.1 & 63.0 & 86.8 & 76.0 & 76.9 & 41.1 & 80.7 & 39.0 & 82.3 & 77.4 & 75.7 & 55.9 & 50.6 & 85.0 & 50.9.6 & 78.9 & 54.7 & 66.0 \\
 pseudo-masks w/ \ CRF & 88.8 &    78.2 &    39.1 &    69.2 &    67.2 &    67.2 &    88.0 &    77.7 &    78.5 &    42.5 &    83.9 &   39.2 &    85.2 &    82.8 &    79.8 &    56.2 &    51.0 &   91.3 &    51.0 &    81.9 &    57.0 & 69.3 \\
 \hline
\end{tabular}
}
\end{table}

\noindent
Figure \ref{fig:ablatiob_argmax} shows the BPM heat-maps for each class in the second, third and fourth columns, inferred by our end-to-end network, including the background. The BPM heat map highlights each pixel's likelihood of belonging to a specific category. Column (e) shows the pseudo-masks obtained by selecting the indices of the classes with maximum probability. Column (f) shows the pseudo-masks improved by CRF. We use these last masks for the verification task as input to DeepLab \cite{Chen2018DeepLabSI}.
 
\noindent
In Table \ref{tab:ablation_categories} we report the BPM mIoU\%  on Pascal VOC val set for each category,  w and w/o CRF. 
\subsection{Comparisons with state-of-the-art}
{\bf Comparison on baseline pseudo-masks}. We compare the mIoU(\%) accuracy of our ViT-PCM method with other methods, which compute BPM and post-process them with CRF \cite{krahenbuhl2011efficient}  similarly. Some methods such as CIAN \cite{Fan2020CIANCA} and EDAM \cite{wu2021embedded}  also incorporate saliency.

Results are reported in Table {\color{red} A}. Here we can observe that CRF, used as BPM post-processing,  improves the BPM, on average, by 3.97\%, with a standard deviation of 1.87. The statistics show that CRF out of a training loop behaves similarly on all methods. Observe that we improved BPM  state-of-the-art by 3.91 mIou\% points and BPM+CRF by 5.4 mIoU\%, both w.r.t. AFA\cite{ru2022learning}, owning so far the best accuracy on both. 

\noindent
\begin{minipage}[t]{\textwidth}
  \begin{minipage}[b]{0.499\textwidth}\label{tab:BPM}
     \centering
     \resizebox{0.98\textwidth}{!}
     {\begin{tabular}{|c | c | c | c | } 
        \multicolumn{4}{c}{{\bf Table A}: mIoU(\%) on PascalVOC2012 train set.}\\
        \hline
        \rowcolor[gray]{.85}
         Method &  Backbone & BPM             & BPM{+}CRF \\ \hline\hline
             ICD \cite{Fan_2020_CVPR}\tiny{\textsc{CVPR'20} } &  VGG16 &  57.00 &   62.20 \\
             SCE\cite{chang2020weakly}\tiny{\textsc{CVPR'20}} &  ResNet38 &  50.90 & -  \\
             SEAM \cite{wang2020self}\tiny{\textsc{CVPR'20} }&  ResNet38 & 55.41 & 56.83  \\
             CIAN\cite{Fan2020CIANCA}\tiny{\textsc{AAAI'20}} & ResNet101 &  58.10 & 62.50 \\ 
             ECSNet\cite{sun2021ecs}\tiny{\textsc{ICCV'20}} & ResNet38 &  56.60 & 58.60 \\
             PAMR\cite{araslanov2020single}\tiny{\textsc{CVPR'20} } &ResNet38 & 59.7  & 62.7\\
             AdvCAM\cite{lee2021anti}\tiny{\textsc{CVPR'21} } & ResNet50 & 55.60 &  62.10 \\ 
             CPN\cite{zhang2021complementary}\tiny{\textsc{ICCV'21} } & ResNet38 &  57.43 & - \\
             CSE\cite{kweon2021unlocking}\tiny{\textsc{ICCV'21} } & ResNet38 &56.0 & 62.8\\
             EDAM\cite{wu2021embedded}\tiny{\textsc{CVPR'21} } &  ResNet101 &  52.83 &  58.18 \\
             MCTformer\cite{xu2022multi}\tiny{\textsc{CVPR'22} } &  DeiT-S &  61.70 &  - \\
             PPC\cite{du2022weakly}\tiny{\textsc{CVPR'22} } & Resnet38 &  61.50 &  64.00\\
             CLIMS\cite{xie2022clims}\tiny{\textsc{CVPR'22} } & Resnet50 &  56.60 &  -\\
             SIPE\cite{chen2022self}\tiny{\textsc{CVPR'22} } & Resnet50 &  58.60 & 64.70\\
             AFA\cite{ru2022learning}\tiny{\textsc{CVPR'22} } & MiT-B1 &  63.80 & 66.00\\
             IRN+W-OoD\cite{lee2022weakly}\tiny{\textsc{CVPR'22} } & Resnet50&  53.30 & 58.40\\
       \bf{ViT-PCM Ours} &ViT-B/16 &  \textbf{67.71} &  {\bf 71.4} \\
     \hline
\multicolumn{4}{c}{{\bf Table C}: mIoU(\%) on MS-COCO 2014 val  set.}\\
         \hline 
         \rowcolor[gray]{.85}
         Method &  Backbone &  \multicolumn{2}{c|}{Val}    \\ \hline
         \hline
         MCTformer\cite{xu2022multi}\tiny{\textsc{CVPR'22} } &  Resnet38 &  \multicolumn{2}{c|}{42.0} \\
          SIPE\cite{chen2022self}\tiny{\textsc{CVPR'22} } & Resnet38 &  \multicolumn{2}{c|}{43.6} \\
          {\bf ViT-PCM Ours}  & ViT-B/16 & \multicolumn{2}{c|}{\bf 45.0}\\
          \hline
\end{tabular}
    
     }\end{minipage}
\hfill
 \begin{minipage}[b]{0.495\textwidth}\label{tab:Deeplab}
   \centering
         \resizebox{0.99\textwidth}{!}
         {\begin{tabular}{|c | c | c | c | }
          \multicolumn{4}{c}{{\bf Table B}: mIoU(\%) on PascalVOC2012 val and test set.}\\
              \hline
              \rowcolor[gray]{.85}
              Method &  Backbone &      Val & Test \\ \hline\hline
             IRNet\cite{ahn2019weakly}\tiny{\textsc{CVPR'19} } & ResNet50 & 63.5 & 64.8\\
             SCE\cite{chang2020weakly}\tiny{\textsc{CVPR'20}} & ReseNet101 & 66.1 & 65.9\\
             SEAM\cite{wang2020self}\tiny{\textsc{CVPR'20} } &  ResNet38 & 64.5 & 65.7  \\
             CIAN\cite{Fan2020CIANCA}\tiny{\textsc{AAAI'20}} & ResNet101 &  64.3 & 65.3 \\ 
             ECSNet\cite{sun2021ecs}\tiny{\textsc{ICCV'20}} & ResNet38 &  66.6 & 67.6 \\
             CONTA\cite{dong_2020_conta}\tiny{\textsc{Nuerips'20}} &ResNet101 &66.1 & 66.7\\
             BES\cite{chen2020weakly}\tiny{\textsc{ECCV'20}} &ResNet101 &65.7 & 66.6\\
             AdvCAM\cite{lee2021anti}\tiny{\textsc{CVPR'21} } & ResNet50 & 68.1 &  68.0 \\ 
             CPN\cite{zhang2021complementary}\tiny{\textsc{ICCV'21} } & ResNet38 &  67.8 & 68.5 \\
             EDAM\cite{wu2021embedded}\tiny{\textsc{CVPR'21} } &  ResNet101 &  52.83 &  58.18 \\
             CSE\cite{kweon2021unlocking}\tiny{\textsc{ICCV'21} } & ResNet38 &68.4 & 68.2\\
             MCTformer\cite{xu2022multi}\tiny{\textsc{CVPR'22} } &  Resnet38 &  71.9 & 71.6 \\
             CLIMS\cite{xie2022clims}\tiny{\textsc{CVPR'22} } & Resnet50 &  70.4 &  70.0\\
             SIPE\cite{chen2022self}\tiny{\textsc{CVPR'22} } & Resnet101 &  68.8 & 69.7\\
             AdvCAM+W-OoD\cite{lee2022weakly}\tiny{\textsc{CVPR'22} } & Resnet38 &  70.7 & 70.1\\
            \hline
               \rowcolor{gray!10}
            PAMR\cite{araslanov2020single}\tiny{\textsc{CVPR'20} } &ResNet38 & 62.7  & 64.3\\
              \rowcolor{gray!18}
             MCIS\cite{sun2020mining}\tiny{\textsc{ECCV'20} }& ResNet101 &66.2 & 66.9\\
               \rowcolor{gray!10}
            ICD \cite{Fan_2020_CVPR}\tiny{\textsc{CVPR'20} } &  Resnet101 &  64.1 &   64.3 \\
              \rowcolor{gray!18}
            AFA\cite{ru2022learning}\tiny{\textsc{CVPR'22} } & MiT-B1 &  66.0 & 66.3\\
              \rowcolor{gray!10}
             MCTformer\cite{xu2022multi}\tiny{\textsc{CVPR'22} } &  Resnet38 &  68.2 & 68.4 \\  
               \rowcolor{gray!18}
             {\bf ViT-PCM Ours} & ResNet 101 &  {\bf 70.3} &  {\bf 70.9} \\
         \hline
\multicolumn{4}{c}{{\bf Table D}: mIoU(\%) on PascalVOC2012 val set.}\\
         \hline
       \rowcolor[gray]{.85}
         Method &  ViT-S/8 & ViT-S/16 & ViT-B/16\\ \hline\hline
         DINO\cite{caron2021emerging}\tiny{\textsc{ICCV'21} } & 44.7 & 45.9 & -\\
{\bf ViT-PCM Ours} & - & {\bf 74.55} &  {\bf 77.25}\\
         \hline
         


          
         
        \end{tabular}
        }\end{minipage}
\end{minipage} \\
\noindent
{\bf Semantic Segmentation Verification Tasks} The verification task of the WSSS methods on PascalVOC 2012 tests the final pseudo-mask (FPM), and the results are reported in Table {\color{red}B}. We divide the methods into two:   those which are boosted (or, according to the definition in PAMR \cite{araslanov2020single} are multi-stage) and those which are end-to-end, highlighted in grey. For the methods considered, the boosted ones improve the mIoU\% w.r.t. the BPM  on average of 9.8\%, while the end-to-end methods improve on average 4.2\%. Our ViT-PCM not being boosted improves by 2.39\% on the val set and decreases on the test set. Our ViT-PCM has the best accuracy among the end-to-end methods, with 70.3\% and 70.9\% on \textit{val} and \textit{test} sets.  Our method   is second to MCT-Former\cite{xu2022multi} on the test set w.r.t. all methods (boosted and end-to-end). However, MCT-Former end-to-end version is second to ViT-PCM, on both the val and test sets.


\noindent 
In Table {\color{red}C} we also evaluate our method on MS-COCO 2014 dataset \cite{lin2014microsoft}. Our ViT-PCM achieves 45.03 mIoU\% on \textit{val} set. We reported only the last methods (2022) with the highest performance.
\noindent
Table {\color{red}D} compares our foreground maps with DINO \cite{caron2021emerging} maps on the PascalVOC 2012 val set.
\noindent
\begin{figure}
\begin{minipage}[b]{0.49\textwidth}
\centering
 \includegraphics[width=0.9\linewidth]{images/Acc+Pars}
\caption{Networks parameters consumed from the BPM to the final-segmentation in ours and other methods, against mIoU\% on PascalVoc2012 val. set. }
\label{fig:cost}
\end{minipage}\hfill \begin{minipage}[b]{0.49\textwidth}
\centering
\resizebox{0.99\textwidth}{!}{\begin{tabular}{c|c|c|c|c}
{\bf Backbone} & Params (M) & Localization &  mIoU (\%)   & pixAcc (\%)\\
   \hline
   \hline
     Resnet50v2    &   25 &   CAM        & \textbf{27.8}  & 72.7\\
          &   &   PCM     & 25.2  & \textbf{76.0}\\
     \cline{1-5}
     Xception    &   23 &   CAM        & \textbf{37.8}  & 76.5\\
         &   & PCM    & 36.5  & \textbf{79.5}\\
     \cline{1-5}
     ViT-S/16  &   \textbf{22} &   CAM        & 29.3  & 55.0\\
         &   & PCM        & \textbf{43.3}  & \textbf{80.1}\\
    \hline
    \end{tabular}}
\tabcaption{Comparison between CAM \cite{zhou2016learning} and PCM (our Patch Class Mapping) on PascalVOC2012 \textit{val} set. The Table reports the best results obtained with Multi-Label BCE loss and L2 regularization loss in all experiments, for both CAM and PCM.}
\label{table:CAM-PCM}
\end{minipage}\end{figure}
\noindent
Figure \ref{fig:cost} shows the ratio between the parameters consumed to obtain the BPM and the final segmentation mask, against the mIoU\% on the val set of PascalVOC2012. 
A  marker specifies the BPM, and a  marker specifies the final segmentation mask, ours in red and the others in blue. Our ViT-PCM, with backbone ViT-S/16,  is green-dashed, and ViT-B/16 is green-continuous. We can observe that most of the shown methods are multi-stage (see also \cite{araslanov2020single,ru2022learning}), and boosting the BPM asks for a significant increase of parameters. 
\noindent
Table \ref{table:CAM-PCM} shows the accuracy between CAM and PCM on different backbones and the amount of parameters required. We made this table to understand whether it would be profitable to use CAM with ViT. As shown in the table, we can see that the combination ViT and PCM is the best solution.

\begin{figure}[t]
\centering
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2010_005174_RGB.jpg}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2010_005174_GT.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2010_005174_ICD.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2010_005174_CIAN.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2010_005174_MCIS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2010_005174_EPS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2010_005174_EDAM.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2010_005174_Simone_v2.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_001491_RGB.jpg}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_001491_GT.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_001491_ICD.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_001491_CIAN.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_001491_MCIS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_001491_EPS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_001491_EDAM.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_001491_Simone_v2.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_000673_RGB.jpg}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_000673_GT.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_000673_ICD.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_000673_CIAN.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_000673_MCIS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_000673_EPS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_000673_EDAM.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/first figure/2008_000673_Simone_v2.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2011_000813_RGB.jpg}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2011_000813_GT.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2011_000813_ICD.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2011_000813_CIAN.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2011_000813_MCIS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2011_000813_EPS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2011_000813_EDAM.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2011_000813_Simone_v2.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_008469_RGB.jpg}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_008469_GT.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_008469_ICD.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_008469_CIAN.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_008469_MCIS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_008469_EPS.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_008469_EDAM.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_008469_Simone.png}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_004654_RGB.jpg}
\caption{rgb}\label{fig:dola}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_004654_GT.png}
\caption{GT}\label{fig:dolb}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_004654_ICD.png}
\caption{ICD}\label{fig:dolc}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_004654_CIAN.png}
\caption{CIAN}\label{fig:dold}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_004654_MCIS.png}
\caption{MCIS}\label{fig:dole}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_004654_EPS.png}
\caption{EPS}\label{fig:dolf}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_004654_EDAM.png}
\caption{EDAM}\label{fig:dolg}
\end{subfigure}
\begin{subfigure}[b]{.11\linewidth}
\includegraphics[width=\linewidth]{images/chapter 4/segmentation/second figure/2008_004654_Simone.png}
\caption{Ours}\label{fig:dolh}
\end{subfigure}

\caption{Qualitative comparison on Pascal VOC 2012 validation set.}\label{fig:qual6}
\end{figure}

 \noindent
Figure \ref{fig:qual6} compares our qualitative results on Pascal VOC 2012 val set with other approaches whose implementation we have used to generate the images; therefore, they might be biased.

\subsection{Limitations}
We observed that ViT-PCM is biased on the most discriminative features.
Many approaches to WSSS highlight the improvements due to processing pixel relations, boundaries, and neighbourhoods. We have used only the conditioning from HV-BiLSTM, which might not be the best solution. On the other hand, some recent approaches have explored contrastive loss for foreground-background learning with no image-level supervision. Since the background is our Achille's heel, we could have explored this idea. Another bottleneck of our approach is the final scaling to map patches to pixels, where we perform a rough scaling to keep the resources limited.  


\typeout{---------------------Conclusions -----------------------}
\section{Conclusions}


We presented an innovative, simple and end-to-end method, ViT-PCM, based on ViT for generating baseline pseudo-masks (BPM) with precise localization and higher quality than those obtained from the more involved CAM CNN-based architectures. We obtained new state-of-the-art in BPM generation with 67.7 \% mIoU on PascalVOC 2012 {\em train set}  and 71.4\% mIoU using  CRF only in post-processing. These results demonstrate this work's high contribution to the field of WSSS. Therefore, we hope that others will continue in this direction. In the supplementary files, we report more analysis and results. The code is available at \url{https://github.com/deepplants/ViT-PCM}.
\bibliographystyle{styles/splncs04}
\bibliography{egbib.bib}

\end{document}
