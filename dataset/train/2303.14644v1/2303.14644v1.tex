\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr}
\usepackage[accsupp]{axessibility} 
\usepackage{graphicx, amsmath, amssymb, caption, subcaption, multirow, overpic, textpos}
\usepackage[table]{xcolor}
\usepackage[pagebackref=false, breaklinks=true, letterpaper=true, colorlinks, citecolor=citecolor, linkcolor=linkcolor, bookmarks=false]{hyperref}
\definecolor{citecolor}{HTML}{0071BC}
\definecolor{linkcolor}{HTML}{ED1C24}

\def\cvprPaperID{5135}
\def\confName{CVPR}
\def\confYear{2023}

\makeatletter
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother

\newcommand{\authorskip}{\hspace{5mm}}

\begin{document}

\title{Affordance Grounding from Demonstration Video to Target Image \vspace{-3mm}}

\author{Joya Chen \authorskip Difei Gao \authorskip Kevin Qinghong Lin \authorskip Mike Zheng Shou\thanks{Corresponding Author.}\
\boldsymbol{F}(V, I) \rightarrow (H, A). \label{equation1}

\boldsymbol{F_e}(V) \rightarrow \{E_l^V\}, \boldsymbol{F_e}(I) \rightarrow \{E_l^I\}, \label{equation2}

    \boldsymbol{F_d}(\{E_l^V\}, \{E_l^I\}) \rightarrow D_{l_{min}}. \label{equation3}

\begin{aligned}
\boldsymbol{MCA}(\hat{Q_l},\hat{K_l},\hat{V_l}) = \sigma(\frac{\hat{Q_l}(\hat{K_l})^\mathrm{T}}{\sqrt{C}} + R_l)\hat{V_l}W_l^{MCA},
\end{aligned}

D_l = flatten(E^I_l) + \boldsymbol{MSA}(\hat{Q_l}), \label{equation5} 

D_l = D_l + \boldsymbol{MCA}(D_l,\hat{K_l},\hat{V_l}), \label{equation6}

D_l = D_l + \boldsymbol{MLP}(D_l). \label{equation7}

D_l =  flatten(E^I_l) + \boldsymbol{MSA}(\hat{Q_l} + UP(D_{l+1})), \label{equation8} 

D_l = D_l + \boldsymbol{MCA}(D_l,\hat{K_l}^{C3D},\hat{V_l}^{C3D}), \label{equation9}

D_l = D_l + \boldsymbol{MLP}(D_l). \label{equation10}

    \mathcal{L}_{h} = \sum_i^{h^I}\sum_j^{w^I} 1_{g_{i,j}>0} \sigma_{g}(g)_{i,j}\log\frac{\sigma_g(g)_{i,j}}{\sigma_{h}(h)_{i,j}},  \label{equation11}


\noindent where  represents the ground-truth heatmap,  normalizes by sum, and  normalizes by softmax. The total loss for optimization is .

Our proposed Afformer attains fine-grained heatmap decoding via multi-scale cross-attention, making it an effective model for video-to-image affordance grounding. However, the limited available training data~\cite{demo2vec,hotspot,assistq} () restricts context variation and may impair the Afformer's ability to ground affordances across video-image discrepancies. While some modified cross-attention techniques~\cite{crossattn_fewshot,cat} aim to address the low-data issue~\cite{crossattn_fewshot}, they focus on feature aspects and are limited to few-shot settings. We propose to tackle the problem from data, yielding a more versatile solution applicable to various challenges. In the following section, we introduce a self-supervised pre-training method for video-to-image affordance grounding.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/figure3.pdf}
    \caption{MaskAHand pre-training focuses on ``video-to-image masked interaction hand grounding'', acting as a proxy task for video-to-image affordance grounding. There are two steps: (1) video and target image synthesis including hand interaction, and (2) training Afformer with the generated data to learn ``video-to-image masked interaction hand grounding''. GT stands for ``ground-truth''.}
    \label{figure3}
\end{figure*}

\section{Masked Affordance Hand Grounding}

We introduce Masked Affordance Hand (MaskAHand), a self-supervised pre-training approach for video-to-image affordance grounding. As depicted in Figure~\ref{figure3}, our method leverages the consistency between hand interaction and affordance regions, approximating affordance grounding to hand interaction grounding. MaskAHand can be viewed as an extension of the ``masked image modeling'' paradigm~\cite{maskfeat,mae,oatrans} to ``masked hand grounding''. We now elaborate on the MaskAHand method in detail.

\subsection{Formulation} \label{section4.1}

As described in Section~\ref{section3.1}, the video-to-image affordance grounding can be expressed as Equation~\ref{equation1}: . 
Since action prediction is a well-established problem in action recognition~\cite{kinetics400,c3d,slowfast,stmae,morphmlp}, we focus on the challenging task of affordance heatmap grounding, denoted by . However, annotating for this heatmap is labor-intensive, as it necessitates thoroughly reviewing the entire video, correlating it with the image, and pinpointing affordances. Consequently, current video-to-image affordance grounding datasets contain limited training samples, with fewer than  in total.

We consider solving the limited data problem in the task . We observe that all affordance regions are interacted with by hands. The human hand exhibits a distinct visual pattern, making it easier to detect than irregular affordance regions. The interaction state can also be readily distinguished using the hand interaction detector~\cite{100doh}. Consequently, we focus on a related task, , which is simpler to gather data for, where  represents the "imagined" hand in the target image that interacts with the affordance region shown in the video. As demonstrated, the capabilities of  and  are closely related, allowing us to obtain  by fine-tuning the  network or even considering  as  for zero-shot grounding.

However, training  still needs the demonstration video , the target image , and interaction hand annotation heatmap  (\eg, hand box mask). But thanks to our approximation for the original task, the data preparation becomes much simpler and allows us to do self-supervised pre-training. We refer to our approach as Masked Affordance Hand (MaskAHand), illustrated in Figure~\ref{figure3} and described in the following section.

\subsection{Affordance-related Data Synthesis}

\noindent\textbf{Hand Interaction Detection.} 
Our MaskAHand relies solely on a hand interaction detector, eliminating the need for an object detector as required by \cite{jointhand,probes}. We employ a Faster R-CNN~\cite{faster_rcnn,fpn} trained on the 100DOH dataset~\cite{100doh,epic-kitchens-55,charades-ego,egtea} to detect hand bounding boxes and output binary hand states (\ie, interacting or not). Our trained detector achieves an 84.9 AP in hand interaction detection on the 100DOH test set, demonstrating its accuracy and reliability for synthesizing hand interaction data.

\noindent\textbf{Hand Interaction Clip Mining.}
We extract multiple hand interaction clips from a long demonstration video, each containing 32 consecutive frames and regarding as , guaranteeing the presence of an interacting hand in at least one frame. To avoid redundancy, we apply a stride of 16 frames between successive clips. We only set a high interaction score threshold 0.99 to reduce false positives.

\noindent\textbf{Target Image Synthesis and Transformation.} 
Inspired by SuperPoint~\cite{superpoint}, we synthesize the target image  from  by simulating video-image context changes, as illustrated in Figure~\ref{figure3}. The process consists of four steps: (1) Select the corresponding frame from  involving the interaction hand; (2) Apply a mask to conceal the hand, as the target image should not include it, making the hand context mask  larger than the detected hand box by a factor of  (\eg, 1.5 times); (3) Introduce a random context mask  with the same scale as  to enhance MaskAHand pre-training difficulty, preventing the model from simply predicting the masked region; (4) Apply a random perspective transformation to simulate perspective change between the demonstration video and target image (\eg, egocentric vs. exocentric). 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/figure4.pdf}
    \caption{Afformer's video-to-image affordance grounding visualization. MaskAHand's visualization is in the supplementary material. \vspace{-3mm}}
    \label{figure4}
\end{figure*}

\subsection{Video-to-Image Masked Hand Grounding}

Given a masked and perspective-transformed interaction frame as the target image  and a mined interaction clip as the video , the Afformer without action predictor  takes them as input and produces a heatmap . Unlike the supervised setting, the ground truth for  is derived from the detected hand box mask, subjected to the same perspective transformation as  (see Figure~\ref{figure3}). During MaskAHand pre-training, the network is tasked with video-to-image masked hand grounding, which requires observing the video to ``match'' the unmasked context in the target image and predict the precise hand box position. Thus, this pre-training shares abilities essential for video-to-image affordance grounding, including context matching between the video and image to ground affordances in the image.

For MaskAHand pre-training, we still utilize the KLD loss, as shown in Equation~\ref{equation11}. The ground-truth hand box masks are gaussian blurred, following video-to-image affordance grounding~\cite{demo2vec}. After training Afformer with MaskAHand, we can directly perform zero-shot evaluation on video-to-image affordance grounding, or fine-tune using supervised data and subsequently conduct evaluation.

\section{Experiments}

\subsection{Experimental settings}

\noindent\textbf{Datasets.} We conduct experiments on three datasets:

\noindent
 \textbf{OPRA}~\cite{demo2vec} consists of YouTube product review videos for appliances (\eg, washing machine, stove). Each data sample consists of a video clip  (\eg, holding a frying pan), paired with a target image  (\eg, ads picture of the frying pan), an action label  (\eg, holding) belonging to a total of  actions, and ten annotated points on the image representing the affordance region (\eg, ten points around the frying pan handle). The ten points are always produced as ground-truth heatmap  by applying a gaussian blur with a kernel size of 3~\cite{demo2vec,hotspot,hagnet}. The dataset comprises roughly  training samples and  testing samples, each represented in the form of .

\noindent
 \textbf{EPIC-Hotspot}~\cite{hotspot} is made up of EPIC-Kitchens~\cite{epic-kitchens-55}, which contains egocentric videos of kitchen activities. EPIC-Kitchens provides an action label  and annotations for interacted objects in video , but no target image. EPIC-Hotspot chooses one frame in  to be the target image  that corresponds to the object class and appearance. Follow \cite{hotspot} for more information. They crowdsource annotations for ground-truth heatmaps  after  is chosen, yielding  annotated instances across  actions and  objects. The data sample format in EPIC-Hotspot is , which is the same as OPRA.

\noindent
 \textbf{AssistQ~\cite{assistq}} is a benchmark to solve user egocentric query~\cite{assistsr} according to instructional videos. It includes fine-grained button regions for a wide range of everyday devices (\eg, microwave), which require precise affordance prediction to distinguish very close buttons. In AssistQ~\cite{assistq}, we consider the instructional video to be the active video and the user view to be the inactive image. Using transcript timestamps, we divide the instructional video into multiple clips  and manually annotate the interacted button on the inactive image . We finally get 650 training samples from 80 videos and 91 testing samples from 20 videos, and each of sample contains active video clip, inactive image, and active-to-inactive button bounding-boxes. The ground-truth heatmap  is generated using gaussian blur for the button center point map. Because the action class in AssistQ is mostly limited to ``press'' or ``turn'', the data sample format is  without action.

We report results on the test sets of these datasets and perform ablation studies on the largest OPRA dataset.

\noindent\textbf{Implementation Details.} We train the Afformer model with a ResNet encoder using a batch size of 16 and 5 iterations, employing the AdamW optimizer~\cite{adamw} and a cosine annealing learning rate scheduler~\cite{cosineanealinglr} with an initial learning rate of . As per \cite{demo2vec}, we set the spatial size of both images and videos to 256. The ground-truth heatmap is generated using annotation points (box center for AssistQ) mapped to a Gaussian blur kernel size of , following~\cite{demo2vec,hotspot}. For the Afformer with a ViTDet encoder, we adjust the learning rate to  and the spatial size to 1024 to accommodate the pre-trained positional encodings from \cite{vitdet}. All encoders are initialized with COCO~\cite{coco} detection weights~\cite{faster_rcnn,vitdet}, following \cite{demo2vec}. These hyperparameters remain consistent across all datasets, including MaskAHand pre-training.

\noindent\textbf{Evaluation.} We report saliency metrics~\cite{saliency_metrics} as KLD, SIM, and AUC-J. Please refer to \cite{demo2vec,hotspot} for more details.

\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{\multirow{2}{*}{Variants}} & \multicolumn{2}{c}{OPRA ()} \\
& & & Heatmap KLD  & Action Top-1 Acc \\
\hline
\multirow{4}{*}{Demo2Vec~\cite{demo2vec}}  
& \multicolumn{2}{c|}{LSTM} & 3.45 & 20.41 \\
& \multicolumn{2}{c|}{ConvLSTM} & 3.31 & 30.20 \\
& \multicolumn{2}{c|}{TSA + ConvLSTM} & 3.34 & 38.47 \\
& \multicolumn{2}{c|}{Motion + TSA + ConvLSTM} & 2.34 & 40.79 \\
\hline
Naive Baseline (Ours) & \multicolumn{2}{c|}{ResNet-50-Deconv} & 2.20 & 45.66 \\
\hline
\multirow{2}{*}{\shortstack{Afformer (Ours)}}
& \multicolumn{2}{c|}{ViTDet-B-Afformer} & \textbf{1.51} &  \textbf{52.27} \\
& \multicolumn{2}{c|}{ResNet-50-Afformer} & \textbf{1.55} & \textbf{52.14} \\
\hline
\multirow{4}{*}{\shortstack{MaskAHand (Ours)}} & \multirow{2}{*}{Zero-shot} & ResNet-50-Deconv & 2.89 & n/a \\
&& ResNet-50-Afformer & \textbf{2.36} & n/a \\
\cline{2-5}
& \multirow{2}{*}{Fine-tune} & ResNet-50-Deconv & 1.74 & 48.93 \\
&& ResNet-50-Afformer & \textbf{1.48} & \textbf{52.50} \\
\hline
\end{tabular}
\vspace{-1mm}
\caption{Video-to-image affordance grounding performance of our Afformer and MaskAHand models on the OPRA dataset (fine-grained, ): Afformer reduces heatmap KLD errors by over 30\%; MaskAHand's zero-shot pre-training results are comparable to \cite{demo2vec} (2.36 \textit{vs.} 2.34); further fine-tuning yields the best performance on OPRA.}
\label{table1}
\end{table*}

\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{c|c|ccc|ccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{OPRA ()} & \multicolumn{3}{c}{EPIC ()}  \\
 &  & KLD  & SIM  & AUC-J & KLD  & SIM  & AUC-J \\
\hline
\multirow{6}{*}{\shortstack{Weakly\\Supervised}}  
& EGOGAZE~\cite{egogaze,hotspot} & 2.43 & 0.25 & 0.65 & 2.24 & 0.27 & 0.61 \\
& MLNET~\cite{mlnet,hotspot} & 4.02 & 0.28 & 0.76 & 6.12 & 0.32 & 0.75 \\
& DEEPGAZEII~\cite{deepgaze,hotspot} & 1.90 & 0.30 & 0.72 & 1.35 & 0.39 & 0.75 \\
& SALGAN~\cite{salgan,hotspot} & 2.12 & 0.31 & 0.77 & 1.51 & 0.40 & 0.77 \\
& Hotspot~\cite{hotspot} & 1.42 & 0.36 & 0.81 & 1.26 & 0.40 & 0.79 \\
& HAG-Net (+Hand Box)~\cite{hagnet} & 1.41 & 0.37 & 0.81 & 1.21 & 0.41 & 0.80 \\
\hline
\multirow{2}{*}{\shortstack{Self-supervised \\ Zero-shot}} 
& Center Bias (action agnostic) & 11.13 & 0.21 & 0.63 & 10.66 & 0.22 & 0.63 \\ 
& MaskAHand (action agnostic) & 1.86 & 0.28 & 0.76 & 1.32 & 0.37 & 0.76 \\
\hline
\multirow{3}{*}{Supervised}  
& Img2heatmap~\cite{hotspot} & 1.47 & 0.36 & 0.82 & 1.40 & 0.36 & 0.80 \\
& Demo2Vec~\cite{demo2vec} & 1.20 & 0.48 & 0.85 & n/a & n/a & n/a \\
& \textbf{Afformer~(Ours)} & \textbf{1.05} & \textbf{0.53} & \textbf{0.89} & \textbf{0.97} & \textbf{0.56} & \textbf{0.88} \\
\hline
\end{tabular}
\vspace{-1mm}
\caption{Performance of Afformer and MaskAHand models on OPRA and EPIC-Hotspot datasets (coarse-grained, ). MaskAHand can surpass many weakly-supervised methods in KLD. Afformer achieve the best performance among supervised methods. }
\label{table2}
\end{table*}

\begin{table}[t]
\small
\centering
\begin{tabular}{c|ccc}
\hline
Method & KLD  & SIM  & Top-1 Acc  \\
\hline
Afformer & 1.13 & 0.44 & 0.41 \\
+MaskAHand & \textbf{1.01(-12\%)} & \textbf{0.54(+23\%)} & \textbf{0.57 (+28\%)} \\
\hline
\end{tabular}
\vspace{-1mm}
\caption{Results on AssistQ Buttons~\cite{assistq}. ``Acc'' refers to the accuracy of button classification.}
\label{table3}
\end{table}

\subsection{Main Results}

\noindent\textbf{Fine-grained Video-to-image Affordance Grounding.} 
In Table~\ref{table1}, the prior method Demo2Vec~\cite{demo2vec} achieves a KLD error of 2.34 on OPRA, which is comparable to our naive ResNet-50~\cite{resnet} + deconvolution baseline with 2.20 KLD. However, our proposed Afformer significantly reduces KLD error. Utilizing ResNet-50 and ViTDet-B~\cite{vitdet} backbones, Afformer attains 1.55 and 1.51 KLD, respectively, surpassing previous results by over 30\%. We attribute Afformer's success to its better design for fine-grained affordance heatmaps, which also boosts action classification accuracy.

We also assess MaskAHand pre-training as Table~\ref{table1} reveals, surprisingly, its zero-shot results are already comparable to Demo2Vec (2.36 \textit{vs.} 2.34), demonstrating its effectiveness as a proxy task for supervised video-to-image affordance tasks. Furthermore, fine-tuning the MaskAHand pre-trained Afformer on OPRA leads to the lowest KLD error of 1.48, a 37\% improvement over Demo2Vec.

\noindent\textbf{Coarse-grained Affordance Grounding on OPRA, EPIC-Hotspots.} 
We adopt the evaluation protocol from \cite{hotspot} to assess our method's performance on coarse-grained affordance grounding at low resolution (). We downsample the standard resolution  prediction heatmap to  using bilinear interpolation during both training and inference phases. Table~\ref{table2} demonstrates that Afformer, despite not being explicitly designed for lower resolution, outperforms other methods. Moreover, the self-supervised MaskAHand zero-shot results surpass some weakly-supervised approaches.

\noindent\textbf{Video-to-image Grounding on small-scale Data.} 
We train our Afformer and fine-tune MaskAHand models on AssistQ Buttons, which contains only 600+ training samples, posing challenges for data-hungry deep neural networks. As demonstrated in Table~\ref{table3}, MaskAHand pre-training substantially reduces the heatmap KLD error and increases SIM metric, increasing the relative button classification accuracy by 28\%. Thus, MaskAHand self-supervised pre-training is a viable option when dealing with limited video-to-image affordance grounding data.

\begin{table*}
\footnotesize
\centering
\begin{subtable}[t]{0.25\textwidth}
\centering
\begin{tabular}{c|c|c}
\hline
Spatial I & Spatial V & KLD  \\
\hline
 & \multirow{4}{*}{} & 1.88\\
 &  & 1.73 \\
 &  & \textbf{1.65} \\
 &  & \textbf{1.65} \\
\hline
\multirow{3}{*}{} &  & 1.63  \\ 
 &  & \textbf{1.62} \\
 &  & 1.63 \\ 
 \hline
\end{tabular}
\subcaption{Single pyramid. I: image, V: video.} 
\end{subtable}
\begin{subtable}[t]{0.35\textwidth}
\centering
\begin{tabular}{c|c|c}
\hline
Spatial I & Spatial V & KLD \\
\hline
\multirow{2}{*}{} &  & \textbf{1.62} \\
 &  & 1.63  \\
\hline
 & \multirow{2}{*}{} & 1.62 \\
 & & \textbf{1.60}  \\
\hline
 & \multirow{3}{*}{} & 1.59  \\
 &  & \textbf{1.57} \\
 & & \textbf{1.57} \\
\hline
\end{tabular}
\subcaption{Multiple pyramids. I: image, V: video.}
\end{subtable}
\begin{subtable}[t]{0.35\linewidth}
\centering
\begin{tabular}{c|c|c|cc}
\hline
\multirow{2}{*}{Spatial} & \multirow{2}{*}{Temporal} & \multirow{2}{*}{KLD } & \multirow{2}{*}{Mem } \\
& & & \\
\hline
\multirow{6}{*}{\shortstack{ \\  \\  \\  \\ }} & \multirow{2}{*}{} & \multirow{2}{*}{1.57} & \multirow{2}{*}{ G} \\
& & & \\
\cline{2-4}
& \multirow{2}{*}{} & \multirow{2}{*}{1.56} & \multirow{2}{*}{ G} \\
& & & \\
\cline{2-4}
& \multirow{2}{*}{} & \multirow{2}{*}{\textbf{1.55}} & \multirow{2}{*}{ G} \\
& & & \\
\hline
\end{tabular}
\subcaption{Temporal pyramids and reduced memory.}
\end{subtable}
\vspace{-1mm}
\caption{Ablation study results for Afformer on the OPRA dataset at a  scale, comparing default settings (image spatial pyramids: , video spatial pyramid: single , no temporal downsampling, and one cross/self-attention module in decoding). Enhanced by multi-scale, high-resolution decoding, performance significantly improves (1.57 vs. 1.88), while temporal pyramids further reduce KLD error and decrease GPU memory consumption. \vspace{-2mm}}\label{table4}
\end{table*}

\vspace{-0.2cm}

\begin{table}
\centering
\footnotesize
\begin{subtable}{1.0\linewidth}
\centering
\begin{tabular}{ccc|cc}
\hline
\multirow{2}{*}{\shortstack{\# Hand Mask \\ ()}} & \multirow{2}{*}{\shortstack{\# Random\\ Mask ()}} & \multirow{2}{*}{\shortstack{Mask \\ Scale}}  & \multirow{2}{*}{Zero-shot KLD} \\
&&&\\
\hline
0 & 0 & n/a &  4.35 \\
\hline
1 & 0 & 1.0 & 4.29 \\
1 & 0 & 1.5 & 2.68  \\
1 & 0 & 2.0 & 2.54 \\
1 & 0 & 3.0 & 3.42 \\
\hline
1 & 1 & 1.5 & \textbf{2.48} \\
1 & 1 & 2.0 & 2.75 \\
1 & 2 & 2.0 & 2.98 \\
\hline
\end{tabular}
\subcaption{Ablations on masking ratio and number of masks.}
\end{subtable}
\begin{subtable}{1.0\linewidth}
\centering
\begin{tabular}{c|c|cc}
\hline
Masking & Distortion & Zero-shot KLD & Fine-tune KLD \\
\hline
\multirow{4}{*}{\shortstack{, \\ , \\ 1.5}} & 0 & 2.48 & 1.53 \\
& 0.25 & 2.40 & 1.50 \\
& 0.5 & \textbf{2.36} & \textbf{1.48} \\
& 1.0 & n/a & n/a \\
\hline
\end{tabular}
\subcaption{Ablations on perspective transformation. ``n/a'': network divergence.}
\end{subtable}
\vspace{-1mm}
\caption{Ablation studies of MaskAHand pre-training on OPRA. \vspace{-2mm}}
\label{table5}
\end{table}

\subsection{Ablation Studies}

\noindent\textbf{Afformer Fine-grained Decoder.} 
We evaluate our fine-grained decoder on the OPRA dataset ( resolution). Table~\ref{table4}(a) reveals that the highest image pyramid resolution () reduces the KLD error; however, for videos, a  resolution is more effective, potentially due to weaker semantics in high-resolution pyramids. Table~\ref{table4}(b) indicates that preserving a fixed video pyramids when building low-to-high resolution image pyramids also decreases KLD error. As per Table~\ref{table4}(c), constructing video temporal pyramids results in considerable memory savings and slight performance enhancement. These findings suggest the significance of our fine-grained decoder in heatmap decoding.

\noindent\textbf{Context Masking in MaskAHand.} 
We examine the context masking effects in MaskAHand (Table~\ref{table5}(a)). Without masking ( and ), pre-training degenerates into hand saliency detection (Figure~\ref{figure3}), providing no benefit to video-to-image affordance grounding and resulting in a large KLD error 4.35. A similar outcome occurs with hand masking only: when  and , the network can directly predict the masked region for a low training loss, yielding a meaningless result (KLD 4.29).

However, increasing the hand mask region to  significantly reduces the zero-shot KLD error to , indicating that the network learns a useful representation for video-to-image affordance grounding. Extending the mask scale to  causes performance degradation, likely due to the overly large masked region creating challenging context matching between video and image. An additional random mask offers a similar effect as enlarging the hand mask but with more diversity, preventing the network from merely predicting a region within the hand mask. Therefore, we use , , and  hand box masking as the default MaskAHand pre-training setting.
 
\noindent\textbf{Perspective Transformation in MaskAHand.} 
Our context masking strategy is to enhance the grounding ability across the video-image context differences. However, it is also crucial to consider perspective transformation, such as enabling simulation of egocentric and exocentric views. As shown in Table~\ref{table5}(b), perspective transformation can lead to performance improvements when the distortion ratio is in a reasonable range.

\section{Conclusion}
In this paper, we introduce the Affordance Transformer (Afformer), a simple and effective model for video-to-image affordance grounding, utilizing multi-scale decoding to generate fine-grained affordance heatmaps. We also propose a pre-training technique, Masked Affordance Hand (MaskAHand), that employs a proxy task of masked hand interaction grounding, facilitating data collection while benefiting video-to-image affordance grounding. Our extensive experiments show Afformer significantly outperforms previous methods, and MaskAHand pre-training impressively improves performance on small datasets.

\vspace{2mm}
\noindent\textbf{\large{Acknowledgment}} \quad
This project is supported by the National Research Foundation, Singapore under its NRFF Award NRF-NRFF13-2021-0008, and Mike Zheng Shou’s Start-Up Grant from NUS. The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{ai}
}

\clearpage

\noindent\large{\textbf{Supplementary Material}}
\vspace{1mm}

We provide supplementary material to complement the main paper. The contents include:

 Section~\ref{sectionb}: We supplement the implementation details of main paper. 

 Section~\ref{sectionc}: We present the additional ablation studies for Afformer and MaskAHand.

 Section~\ref{sectiond}: We show MaskAHand generated samples on OPRA~\cite{demo2vec}, EPIC-Hotspot~\cite{hotspot}, and AssistQ~\cite{assistq} Buttons.

 Section~\ref{sectione}: We present MaskAHand zero-shot visualization results on OPRA test set.

\begin{table}[h]
\footnotesize
\centering
\begin{subtable}{\linewidth}
\centering
\begin{tabular}{l|l}
\hline
    Config & Value \\
    \hline
    Optimizer & AdamW, weight decay  \\
    Learning rate & , cosine decay \\
    Training batch size \& epochs & 16 batch size in 7 epochs \\ 
    Automatic mixed precision & 16-bit \\
    Backbone learning rate factor & 0.1 \\
    Backbone Initialization & COCO Detection~\cite{faster_rcnn,coco} \\
    Attention Heads \& Channels & 4 \& 256 \\
    Maximum Video Frames & 64 by uniform sampling\\
    \hline
\end{tabular}
\subcaption{Setting for supervised training of Afformer on OPRA~\cite{demo2vec}, EPIC-Hotspots~\cite{hotspot}, and AssistQ~\cite{assistq} Buttons. Note that EPIC-Hotspots and AssistQ Buttons are only experimented with R50-FPN backbone. \vspace{2mm}}
\end{subtable}
\begin{subtable}{\linewidth}
\centering
\begin{tabular}{l|l}
\hline
    Config & Value \\
    \hline
    Backbone & R50-FPN~\cite{resnet,fpn} \\
    Batch size & 32 \\
    Mined video frames & 32 by sequential sampling\\
    Stride between mined videos & 16 frames \\
    Interaction score threshold & 0.99 \\
    \hline
\end{tabular}
\subcaption{Setting for MaskAHand pre-training on OPRA~\cite{demo2vec}, EPIC-Hotspots~\cite{hotspot}, and AssistQ~\cite{assistq} Buttons. Other settings follow (a).}
\end{subtable}
\caption{Implementation details of Afformer and MaskAHand.}\label{tablea}
\end{table}

\begin{table}[h]
\footnotesize
\centering
\begin{subtable}{\linewidth}
\centering
\begin{tabular}{cc|c}
\hline
Module & I,V Shared & KLD  \\
\hline
\multirow{2}{*}{Backbone} &  & 1.80 \\
 & \checkmark & \textbf{1.57} \\
\hline
\multirow{2}{*}{Input Proj} &  & 1.65 \\
& \checkmark & \textbf{1.57} \\
\hline
\end{tabular}
\subcaption{Sharing parameters for image and video encoding has better results. \vspace{2mm}}
\end{subtable}
\begin{subtable}{\linewidth}
\centering
\begin{tabular}{cc|c}
\hline
Module & Pyramid Shared & KLD  \\
\hline
\multirow{2}{*}{Input Proj} &  & \textbf{1.57} \\
 & \checkmark & 1.60 \\
\hline
\multirow{2}{*}{Decoder} &  & 1.61 \\
& \checkmark & \textbf{1.57} \\
\hline
\end{tabular}
\subcaption{Different pyramids need different input projections, but the decoder can be shared.}
\end{subtable}
\caption{Additional ablation studies for Afformer on OPRA.}\label{tableb}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{c|cc}
Mask & Zero-shot KLD  & Fine-tune KLD  \\
\hline
Zero () & 2.65 & 1.50 \\
Random () & \textbf{2.36} & \textbf{1.48} \\
\end{tabular}
\caption{Experiments on masked fill value of MaskAHand pre-training on OPRA.}\label{tablec}
\end{table}

\section{Implementation Details}\label{sectionb}

Due to limited pages, some implementation details are not presented in the main paper. We complement them in Table~\ref{tablea}. The initialization for Afformer's backbone follows Demo2Vec\cite{demo2vec}, which also initializes the backbone network by COCO detection~\cite{coco} weights. We use \texttt{PyTorch}~\cite{pytorch} and \texttt{torchvision} to implement our model. 

\section{Additional Ablation Studies}\label{sectionc}

Table~\ref{tableb} shows the additional ablation studies for Afformer, which suggests that the backbone and input projector should be shared for video and target image. Furthermore, we observe that the input projector should be different for different pyramid resolution levels, but the transformer decoder can be shared. The sharing of the backbone and decoder makes our Afformer parameter efficient. 

We also investigate the masked filled value of MaskAHand pre-training, as shown in Table~\ref{tablec}. In the masked region of the target image, filling random value (\ie random noise mask) is much better than zero value (\ie black mask). We hypothesize that the fixed zero filling makes model overfitting easier.

\section{Training Samples from MaskAHand}\label{sectiond}

Figure~\ref{figureb} shows MaskAHand generated training samples on different datasets. Each training sample include a video clip, a target image, and a ground-truth heatmap.

\begin{figure*}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figure6a.pdf}
        \caption{A training sample case generated by MaskAHand, on OPRA training set's raw videos.}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figure6b.pdf}
        \caption{A training sample case generated by MaskAHand, on EPIC-Hotspots training set's raw videos (\ie EPIC-KITCHEN frames~\cite{epic-kitchens-55}).}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figure6c.pdf}
        \caption{A training sample case generated by MaskAHand, on AssistQ training set's raw videos.}
    \end{subfigure}
    \caption{MaskAHand generated training sample cases on OPRA~\cite{demo2vec}, EPIC-Hotspots~\cite{hotspot}, and AssistQ~\cite{assistq}. The hand interaction detection is made by our trained hand interaction R-CNN, mentioned in Section 4.2. ``WithObj'' means the hand is interacting with objects, whereas ``NoObj'' means not interacting. The mined video clip contains 32 frames (with 5 FPS). If there are multiple interaction frames detected (\eg, (a) and (c)), the target image generation will be randomly picked from these frames.}\label{figureb}
\end{figure*}


\section{MaskAHand Zero-shot Visualization}\label{sectione}

Figure~\ref{figurec} visualizes MaskAHand zero-shot results, demonstrating that the representation learned by MaskAHand can support video-to-image affordance grounding.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/figure7a.pdf}
    \caption{MaskAHand zero-shot results on OPRA test set. For visualization, we select the top 100 points on the ground-truth and zero-shot prediction heatmap. It can be seen that the hotspot on the zero-shot heatmap is close to that on the ground-truth heatmap.}\label{figurec}
\end{figure*}


\end{document}
