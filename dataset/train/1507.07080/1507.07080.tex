\documentclass[11pt,runningheads]{llncs}

\usepackage{epsfig}
\usepackage{verbatim}
\usepackage{url}
\usepackage{fullpage}

\def\algoname#1{\mbox {\textsc{#1}}}
\def\Oh{\mathcal{O}}

\def\itbf#1{\textit{\textbf{#1}}}
\def\definition#1{\textit{#1}}
\def\reverse#1{\hat{#1}}

\def\ST{\mbox{\rm {\sf ST}}}
\def\SA{\mbox{\rm {\sf SA}}}
\def\ISA{\mbox{\rm {\sf ISA}}}
\def\LZ{\mbox{\rm {\sf LZ}}}
\def\LF{\mbox{\rm {\sf LF}}}
\def\FMI{\mbox{\rm {\sf FMI}}}
\def\FM{\mbox{\rm {\sf FM}}}

\def\rank{\mbox{\rm {\sf rank}}}
\def\select{\mbox{\rm {\sf select}}}
\def\access{\mbox{\rm {\sf access}}}

\def\pathlabel{\mbox{\rm {\sf pathlabel}}}
\def\lcp{\mbox{\rm {\sf lcp}}}
\def\rmq{\mbox{\rm {\sf rmq}}}
\def\x{\mbox{\rm {\sf x}}}
\def\X{\mathsf{X}}
\def\MS{\mbox{\rm {\sf MS}}}
\def\B{\mathsf{B}}
\def\ssB{\mbox{\rm {\sf {\scriptsize B}}}}
\def\A{\mathsf{A}}
\def\Pr{\mbox{\rm {\sf P}}}
\def\R{\mbox{\rm {\sf R}}}
\def\Y{\mbox{\rm {\sf Y}}}
\def\Y{\mathsf{Y}}
\def\ssY{\mbox{\rm {\sf {\scriptsize Y}}}}
\def\Z{\mathsf{Z}}
\def\C{\mbox{\rm {\sf C}}}
\def\D{\mbox{\rm {\sf D}}}
\def\W{\mbox{\rm {\sf W}}}
\def\BWT{\mbox{\rm {\sf L}}}
\def\LCP{\mbox{\rm {\sf LCP}}}
\def\LPF{\mbox{\rm {\sf LPF}}}
\def\MBA{\mbox{\rm {\sf M_{{\scriptsize B|A}}}}}
\def\AB{\mbox{\rm {\sf {\scriptsize AB}}}}
\def\BA{\mbox{\rm {\sf {\scriptsize BA}}}}
\def\YZ{\mbox{\rm {\sf {\scriptsize YZ}}}}
\def\ZY{\mbox{\rm {\sf {\scriptsize ZY}}}}
\def\POS{\mbox{\rm POS}}
\def\LEN{\mbox{\rm LEN}}
\def\PLCP{\mbox{\rm PLCP}}
\def\slink{\mbox{\rm {\sf slink}}}
\def\wlink{\mbox{\rm {\sf wlink}}}
\def\NSV{\mbox{\rm {\sf NSV}}}
\def\PSV{\mbox{\rm {\sf PSV}}}
\def\RMQ{\mbox{\rm {\sf RMQ}}}
\def\LZSCAN{\mbox{\sf LZscan}}
\def\EMLZSCAN{\mbox{\sf EM-LZscan}}
\def\LZFMIRMQ{\mbox{\sf LZ-FMI-RMQ}}
\def\LZFMIBPR{\mbox{\sf LZ-FMI-BPR}}
\def\LZFMIISA{\mbox{\sf LZ-FMI-ISA}}

\def\O{\mbox{\rm O}}
\def\o{\mbox{\rm o}}

\def\codetab{\ \ \ \ }
\def\balgorithm#1{{\bf Algorithm #1}}
\def\bproc{{\bf procedure\ }}
\def\bfunc{{\bf function\ }}
\def\bvar{{\bf var\ }}
\def\barray{{\bf array\ }}
\def\bof{{\bf of\ }}
\def\bfor{{\bf for\ }}
\def\bforeach{{\bf foreach\ }}
\def\bto{{\bf to\ }}
\def\bdownto{{\bf downto\ }}
\def\bwhile{{\bf while\ }}
\def\brepeat{{\bf repeat\ }}
\def\buntil{{\bf until\ }}
\def\band{{\bf and\ }}
\def\bor{{\bf or\ }}
\def\bdo{{\bf do\ }}
\def\bif{{\bf if\ }}
\def\bthen{{\bf then\ }}
\def\belse{{\bf else\ }}
\def\belsif{{\bf elsif\ }}
\def\bnot{{\bf not\ }}
\def\bgoto{{\bf goto\ }}
\def\breturn{{\bf return\ }}
\def\boutput{{\bf output\ }}
\def\bbreak{{\bf break\ }}
\def\bmax{{\bf max\ }}
\def\la{\leftarrow}
\def\ra{\rightarrow}
\def\llra{\longleftrightarrow}
\def\q{\quad}
\def\qq{\qquad}
\def\+{\!+\!}
\def\-{\!-\!}
\def\com#1{\hspace{6pt}{\bf ---}\hspace{6pt}{\sl #1}}
\def\rem#1{\hspace{24pt}{\sl #1}}
\def\var#1{\textrm{#1}}

\begin{document}
\title{Range Predecessor and Lempel-Ziv Parsing\thanks{This research is
    supported by Academy of Finland through grants 258308 and 284598.}}

\author{
Djamal Belazzougui
\and
Simon J. Puglisi
}

\institute{
    Department of Computer Science,University of Helsinki\\
    Helsinki Institute for Information Technology (HIIT)\\
    Helsinki, Finland\\
    \email{\{belazzou,puglisi\}@cs.helsinki.fi}\ otherwise. We also define  iff , except when , in which case .
 Clearly, if we know , we can {\em invert} the BWT to obtain the original string 
 right-to-left via repeated applications of , outputing , then
 , then , and so on. Note that, after , 
 this process also visits the positions in  of suffixes , then , and so on.
 
 Let , for symbol , be the number of symbols
 in  lexicographically smaller than .  The function
 , for string , symbol , and integer , returns
 the number of occurrences of  in .  It is well known that
  and that this ``special''
 form of rank (where ) can be computed in  time after
  time preprocessing to build a data structure of size ~\cite{BN2014}.

 Furthermore, we can
 compute the left extension using  and .  If  is
 the -interval,
 then
  is
 the -interval.
 This is called \emph{backward search}~\cite{fm2005}. Note that backward
 search either requires general rank queries, which can be answered in  time after
  time preprocessing to build a data structure of size ~\cite{GOR10}
or can be solved using a more sophisticated data structure which occupies the same asymtotic space 
but requires  randomized time preprocessing~\cite{BN2014}.

There are many BWT construction algorithms (see~\cite{pst2007} for a survey),
and some of them operate in compact space. In particular, 
Hon, Sadakane, and Sung~\cite{hss2009} show 
how to construct the BWT in  time and  
bits of working space. More recently, Belazzougui~\cite{b2014} showed how the BWT can be computed 
in  time (randomized) and  bits of working space.

\paragraph{Wavelet Trees.}
Wavelet trees~\cite{GGV03} are a tool from compressed data structures~\cite{nm2007} 
that encode a string  on alphabet in  bits and allow 
fast computation of various queries on the original string such as  to the
th symbol, , , and various range queries~\cite{navarro2014wavelet}. 
The operation  is as defined above, while operation  
for symbol , and integer  returns the position of the 
th occurrence of  in .  

Let  be a string of  symbols, where each symbol is in
the range .  The wavelet tree  of  is a
perfect binary tree with  leaves. The leaves are labelled
left-to-right with the symbols  in increasing
order. For a given internal node  of the tree, let  be the
subsequence of  consisting of only the symbols on the leaves in the
subtree rooted at . We store at  a bitvector  of 
bits, setting  if symbol  appears in the right
tree of , and  otherwise. Note that  is not
actually stored, only . Clearly  requires 
bits.

\paragraph{LZ77.}
Before defining the LZ77 factorization, we introduce the concept of a
{\em longest previous factor} (LPF).  The LPF at position  in
string  is a pair  such that, ,
, and  is maximized.
In other words,  is the longest
prefix of  which also occurs at some position  in
.

The LZ77 factorization (or LZ77 parsing) of a string  is then just
a greedy, left-to-right parsing of  into longest previous
factors. More precisely, if the th LZ factor (or {\em phrase}) in
the parsing is to start at position , then we output 
(to represent the th phrase), and then the th phrase starts
at position . The exception is the case , which
happens iff  is the leftmost occurrence of a symbol in . In
this case we output  (to represent ) and the next
phrase starts at position .  When , the substring
 is called the {\em source} of phrase
. We denote the number of phrases in the LZ77 parsing
of  by .

\begin{theorem}[e.g., K{\"a}rkk{\"a}inen~\cite{k1999}]
The number of phrases  in the LZ77 parsing of a string of  symbols
on an alphabet of size  is 
\end{theorem}

\section{Related Work}
\label{sec-related}

There have been many LZ parsing algorithms published, especially recently. 
Most of these results make no promise about the rank of the previous factor occurrence they 
output.
The current fastest algorithm in practice (ignoring memory constraints) is due to 
K{\"a}rkk{\"a}inen et al.~\cite{kkp2013}. Their algorithm runs in optimal  time and uses  
bits of space. Goto and Bannai~\cite{gb2014} improve space usage to  
bits, while maintaining linear runtime.

Compact-space algorithms are due to Ohlebusch and Gog~\cite{og2011}, Kreft and Navarro~\cite{kn2012}, 
K{\"a}rkk{\"a}inen et al.~\cite{kkp2013-sea}, and Yamamoto et al.~\cite{yibit2014}. 
All these approaches
take in  time. Very recently a solution with  space and  time has been proposed in~\cite{kosolobov2015faster}. 
We show a significant improvement --- to  time --- is possible in the compact
setting. If we allow randomization, then our time becomes linear. 

There are significantly fewer results on the rightmost problem. Early algorithmic work is
due to Amir, Landau and Ukkonen~\cite{alu2002}, who provide an  time and space
solution, but it should be noted that selection of rightmost factors had already been used as a heuristic in the data
compression community for years (for example, in the popular {\em gzip} compressor). Recently,
Larsson~\cite{l2014} showed how to compute the rightmost parsing online in the same
 time and space bounds as Amir et al.. 
Currently the best prior result for rightmost parsing is an
 space and
 time
algorithm due to Ferragina, Nitto and Venturini~\cite{fnv2013}. 
We provide a faster algorithm that uses significantly less space.

Our result for rightmost makes use of an improved technique for 
{\em range predecessor queries}\footnote{Elsewhere these queries are variously called 
range successor~\cite{nn2012} and range next value queries~\cite{CIKRTW12}.}.
By combining text indexing with range reporting our thus work continues a long tradition in string 
processing, recently surveyed by Lewenstein~\cite{l2013}.
Given a set of two-dimensional points , 
the answer to an orthogonal range predecessor query 
is the point  with largest -coordinate among all points that are in the rectangle
. 

If one is allowed  bits of space, a data structure due to Yu, Hon, and Wang~\cite{yhw2011}
supports range predecessor queries in  time and takes 
 time to construct. Navarro and Nekrich~\cite{nn2012}
subsequently improved query time to ,
where  is an arbitrarily small positive constant, however their structure 
has  construction time~\cite{nn2013-private}.
Multiary wavelet trees are also capable of answering range predecessor queries, and
recently Munro, Nekrich, and Vitter~\cite{MNV14} (and contemporaneously Babenko, Gawrychowski, Kociumaka, and 
Starikovskaya~\cite{BGKS15}) showed how to construct wavelet trees 
in  time using  bits of working space, and
supporting queries in  time.

Finally, we note that a data structure for range predecessor queries immediately implies one 
for the classic and much studied 2D orthogonal range reporting problem from computational 
geometry~\cite{chan2011orthogonal},
in which we seek a data structure to report all points contained in a four-sided query rectangle 
. Our range predecessor result is a -bit data structure 
with query time  (for any constant )
that can be built in  time. This matches the best 
known query time of for this problem when using  bits of space, 
and to our knowledge is the first to offer construction time .
To put our result in context, Chan and P{\u a}tra{\c s}cu~\cite{CP2010} have shown that a 
2D range {\em counting} data structure with  query time and  bits of space can be 
built in  time. 

\section{Lempel-Ziv Parsing in Compact Space}
\label{sec-basic}

Assume the next LZ factor starts at position  in the string. Our basic approach 
to compute the factor is to treat  as a pattern and perform a prefix search
for it in , which we simulate via backward search steps on the FM-index of the 
reverse text . Consider a generic step  of this backward search, in which we 
have a range  of the BWT and SA of . The factor beginning at  
has length at least  if and only if  contains a value .
To see this, observe that the presence of such a  in  means
there is a substring  that occurs {\em after} substring 
 in , which in turn implies there is an occurence of 
 before position  in  (starting at position , in fact).

Our problem now is to be able to determine if  contains a value 
larger than  at any given step in the above process. One solution is to preprocess 
SA for range maximum queries. However, this requires that we either first store SA in 
plain form, which requires  bits, or that we obtain the values of SA 
left-to-right  (the order in which they are required for RMQ 
preprocessing) via repeated decoding using the SA sample, requiring  time. 
Either of these straightforward methods uses more space or time than we desire. 

Our approach instead then is to logically divide the BWT into equal-sized blocks 
of size . We then invert the BWT, and during the inversion we record for 
each block the maximum
value of  that we see over the whole inversion process. We store an array,
 of these block maxima. Storing  requires  bits.
We now build the succinct RMQ data structure of Fischer and Heun~\cite{FischerH11} on the
array , which requires  bits (including during construction) and 
time. So far we have used  bits and  time for the inversion.

We are now ready to describe the LZ factorization algorithm, which will involve another 
inversion of the BWT. This time we maintain a bit vector . If, during
inversion, we visit position  in the BWT, then we set . At the same
time (i.e. while) we are inverting we will perform a backward search for the next LZ factor,
using the as yet unprocessed portion of  as the pattern.

Say we have factorized part of the string and we are now looking for the LZ
factor that starts at . We match symbols of  using backward search. At a given point
in this process we have matched  symbols and have an interval of the , say .
We need to decide if there exists a , which will tell us there is an
occurrence of  before .

 can be divided into at most three subranges: one that is covered by a series of block
maxima (i.e. a subarray of ), and at most two small subranges at each end, 
and , each of size at most . We compute the maximum value
covered by the block maxima in  time using the RMQ data structure we built over .
For the two small subranges at each end of  that are not fully covered by block
maxima we consult . If there is a bit set in either of the  or  then
we know there is some suffix in there that is greater than  (because it has already been
visited in the inversion process). Because the (sub)bitvectors 
 or  are so small (), we can use
a lookup table to determine if there is a set bit in  time, and further we can have
the lookup table return the position of one of these set bits. In this way we are able to 
determine in constant time whether we have reached the end of the current factor.

Having determined the length of the current factor, it is then simply a matter 
of using a sampled SA of size  elements that allows us to extract 
arbitrary SA elements in  time~\cite{fm2005} to obtain one of the candidate SA 
values from the previous round 
in the search for the current factor (so that we can output a  value for the factor). 
This takes  time per factor, which over all the 
factors takes . However, runtime can be further reduced to  over all 
factors if we first record the positions of the candidate  values for each factor 
(using  bits of space) and obtain them all in a single further inversion 
of the BWT.

As described, our factorization algorithm requires  time and  bits 
of space in addition to the resources needed to construct the BWT and perform  
backward search steps. We thus have the following theorem.

\begin{theorem}
\label{lz-compact-space}
Given a string  of  symbols on an ordered alphabet of size  we can compute 
the LZ factorization of  using  bits of space and  
time or  time (randomized).
\end{theorem}

\section{Faster preprocessing for range-predecessor queries}
\label{sec-rangepred}

In the {\em range predecessor} problem in rank space, we are to preprocess 
 points on a  grid, where all points  
differ in both coordinates, so as to answer to the following 
kind of query: given integers  and  find the point 
such that ,  and  is maximal. 


Navarro and Nekrich presented a solution to this problem~\cite{nn2012} 
that uses space  bits and  
answers queries in time .
However, they did not show how to efficiently construct their data structure. 




We now show how 
to efficiently build a variant of the known solutions 
for range predecessor. The solution we describe here has query 
time . We later show how to generalize it 
to have query time  for arbitrary . 

We start by defining the sequence  of length  over alphabet  
obtained by setting  for every point  in the set of input points. 
We similarly define the sequence  such that  for every input 
point .

At a high-level, the solution uses a top-level data structure that resembles 
a multiary wavelet tree~\cite{FMMN07} with arity 
and depth . We note that a standard wavelet tree can answer 
range predecessor queries in time . 
Without loss of generality, we assume that  is 
integral and that  is divisible by . 
The top-level data structure is a tree 
with  levels. The arity of the tree is exactly 
. 
At any level , we will have 
nodes labelled with values 
(note that at level  we will only have the root 
node, which is labelled by ). 
Any node  at level  will have as children all nodes 
 at level  such that . 


To every node  in the tree we associate 
a sequence  of length . 
The sequence  will be over alphabet ,  
while the sequence  is a sequence of integers from . 
Every node of the tree will contain the following substructures: 
\begin{enumerate}
\item A plain representation of the sequence . This sequence occupies  bits of space. 
\item A regular wavelet tree~\cite{GGV03}  over the sequence . This wavelet tree will have 
depth . It can be used to answer to range predecessor queries over the sequence 
in time . 
\item Exactly  predecessor data structures that support 
predecessor () queries in  time. For each character, , 
we store its positions of occurrence in  in a predecessor data structure
denoted . The predecessor data structures are implemented using Elias-Fano 
data structure in such a way that they occupy in total  bits of space. 
\item A range minimum query data structure on the sequence  denoted . 
The data structure will occupy  bits and answer queries in constant tile. 
\item A range maximum query data structure on the sequence  denoted .
This data structure will also occupy  bits and answer queries in constant tile. 
\end{enumerate}
All data structures of a node  will use  bits 
of space  (for predecessor data structures we count the space as if it was 
one single structure)
except for range minimum and range maximum 
data structures, which will use  bits. 
A detailed description of the predecessor data structure 
is given in Appendix~\ref{sec:Elias_Fano_pred}. The space for all nodes 
at the same level will sum up to  and since we have 
 levels, the total space will sum up to 
bits. 

We now define how the sequences  and  
are built\footnote{Note that the sequence  is not used 
explicitly in the data structure. It will however be used later, 
when we show how the data structure is queried and constructed.}. 
At any level  we will have 
nodes. For , the sequence 
is built by first constructing the subsequence  of  values in  whose 
most significant bits equal  (i.e. ), 
and then removing the most significant  bits from every element in 
(that is  for all 
). 
Then  is obtained from 
 by taking the most significant  bits from every element 
of  (that is  for all 
). Notice that for the root node we will have . 
The total number of nodes will be dominated by the nodes at the lowest level, 
summing up to . 

With our data structure now defined, we will next show how to construct it efficiently. 
The description of how queries are answered is given in Appendix~\ref{sec:range_succ_queries}. 

\subsection{Construction of the range-predecessor data structure}

Building all subsequences  and wavelet trees  can be done in time 
using a variation of the algorithm shown in~\cite{MNV14,BGKS15}. Details are shown in 
Appendix~\ref{sec:wavelet_tree}. The construction of Elias-Fano data structures can also 
easily be done in overall time . Details are shown in Appendix~\ref{sec:simple_Elias_Fano_build}.

Each range minimum and maximum query data structure 
can be constructed in time  using the algorithm of Fischer and Heun~\cite{FischerH11}.
When summed over all the nodes , the construction  
and range minimum (maximum) data structures takes time , since the total 
number of elements stored in all the structures is . 
We have thus proved the following theorem.

\begin{theorem}
\label{range_pred_theo1}
Given  points from the grid , we can in 
 time build a data structure that occupies 
 bits of space and that answers 
range predecessor queries in  time.
The construction uses  bits of working space. 
\end{theorem}

We can generalize the data structure as follows. 

\begin{theorem}
\label{range_pred_theo2}
Assume that we have available space  
and preprocessing time  with word-length 
. Then given  points from the grid , 
with  and a parameter , we can in  time build 
a data structure that occupies  bits of space and that answers range predecessor 
queries in  time. 
The construction of the data structure uses  bits of working space 
and a precomputed global table 
of size  that can be built in  time. The precomputed table can be shared 
by many instances of the data structure. 
\end{theorem}
\begin{proof}
The proof is more involved. To prove the result we will use multiple levels 
of granularity. 
At the top level, we will have a tree of  (tree) levels
(to avoid confusion we call these {\em tree levels}), where 
each node handles a subsequence of  over alphabet 
~\footnote{We assume without loss of generality that 
 is integral}. 
The level of granularity of this tree is . 
For each node the data structures are exactly the same as the ones in Theorem~\ref{range_pred_theo2}, 
except that the wavelet tree of each sequence is replaced by tree at level of granularity 
, which contains  (tree) levels, 
each of which handles a sequence over alphabet . 
The recursion continues in the same way until we get to trees at level of granularity~1, 
which is implemented using a wavelet tree. 
Queries are answered in two phases. In the first we determine the longest common 
prefix between the  coordinate of the query and the  coordinate of the answer 
by traversing trees at decreasing levels of granularities
(tree at level , then level of granularity  and so on)
and querying the range-maximum and predecessor data structures at each traversed node. 
Then the remaining bits of the answer are determined by traversing trees of increased levels 
of granularity, querying range-maximum and predecessor data structures. 
The time bound , follows because we have 
 levels of granularity and at most 
nodes are traversed at each level of granularity, where queries at each node 
cost  time.
Details of how queries are answered are given in Appendix~\ref{subsec:faster_range_pred}.

The main challenge is to quickly construct the Elias-Fano data structures. This 
is shown in Appendix~\ref{sec:bit_parallel_Elias_Fano_construction}. 
The construction for range minimum (maximum) 
or queries is shown in Appendix~\ref{sec:sampled_rmq}. 
Both construction methods make use of bit-level parallelism 
to accelerate processing. 
\qed
\end{proof}
As an immediate corollary, we have the following: 

\begin{corollary}
\label{range_pred_corollary}
Given  points from the grid , for any integer , 
we can in  time build a data structure that occupies 
 bits of space and that answers 
range predecessor 
queries in  time. 

\end{corollary}


\section{Rightmost Parsing}
\label{sec-rightmost}

We will now apply the construction for range predecessor detailed above 
to obtain a faster algorithm for computing rightmost previous occurrences 
of LZ phrases. An algorithm by Ferragina et al.~\cite{fnv2013} achieves 
 time, but requires  bits 
of space that can not trivially be reduced.

In this section, we will achieve time 
and  bits of space, significantly improving the bounds 
achieved by Ferragina et al.~\cite{fnv2013}. We first present a preliminary 
solution from~\cite{fnv2013} in Section~\ref{sec:basic_rightmost} and then present an initial version of our 
solution that uses  space (sections~\ref{sec:rightmost_long_factors},~\ref{sec:rightmost_sparse_tree}, and~\ref{sec:full_picture_and_opt_space}). 
The solution works by decomposing the phrases into  categories. 
Finding the rightmost occurrences will be fast 
for different reasons. For the first two categories, the reason 
is that the number of phrases is small, while for the last the reason 
is that the rightmost occurrence for each individual phrase will be easier 
to find. 

We divide the full range  in the suffix array into blocks of a certain size . 
Phrases longer than 
a certain length   are handled in Section~\ref{sec:rightmost_long_factors}), phrases 
shorter than a certain length  whose suffix array ranges cross a block boundary
are handled in Section~\ref{sec:rightmost_sparse_tree}. Finally the remaining phrases are handled 
in Section~\ref{sec:rightmost_rem_factors}. 

\subsection{Basic solution of Ferragina et al.}
\label{sec:basic_rightmost}

We present the basic solution of~\cite{fnv2013}. 
The original algorithm uses  time and 
 space. Here, we describe a slightly improved 
version that uses only  bits. 
The algorithm works as follows. To each phrase, we can associate a leaf 
and an internal node in the suffix tree. 
The leaf corresponds to the suffix (text position) that starts at the same 
position as the phrase and the internal node corresponds to the 
range of suffixes prefixed by the phrase. We mark in the suffix 
tree all the internal nodes that are associated with phrases. 
To each leaf, we associate the nearest marked ancestor. This requires  bits 
and  time in total. Also, to each leaf associated with a phrase, we keep a pointer 
to the internal node that corresponds to that phrase. This association can be 
coded in  bits, since we can use a bitvector to 
mark leaves together with an array of at most 
pointers, each of  bits. All the structures can be built 
in  time. 
We keep only the marked nodes in the suffix tree. To each 
marked node, we keep all the phrases that correspond to it. 
To each marked node , we keep a text position 
that will point to the rightmost position among all the leaves that 
have node  as their nearest marked ancestor. 
Overall the space occupied by all data structures is 
bits. The algorithm works by scanning the inverse suffix array
in left-to-right order. That is, at every step , we extract the 
suffix array position (suffix tree leaf) that points to text position ,
and update the variable , where  is the nearest marked ancestor 
of the leaf. When we arrive at a leaf that corresponds to a phrase,  
we go to the corresponding node, and then explore the entire subtree under that 
node and take the maximum of all variables  for all nodes 
in the subtree. The scanning of the inverse suffix array, can be done 
by inverting the Burrows-Wheeler transform in increasing text order. 
This can be done using the select operation which can be answered 
in constant time. Thus the inversion takes time . 
The overall space is . 
The time bound comes from the 
fact that each internal node corresponds to a phrase of length , and can only 
have at most  ancestors. Since at most  phrases are associated with each 
of the ancestors, the node can only be explored  times:  times 
for each of its  ancestors. Since the total length of the phrases is , 
we conclude that the total running time is . 
We thus have the following theorem.

\begin{theorem}[space improved from~\cite{fnv2013}]
We can find the right-most positions of all phrases in time 
and working space  bits. 
\end{theorem}


\subsection{Long factors}
\label{sec:rightmost_long_factors}

Because factors do not overlap, there can only be  factors of length at least .
We thus can afford to use time  to find the rightmost occurrence 
of each factor. We sample every th position in the text
( and  will be set later). We then build a five-sided  range maximal query data structure
as follows. We will have the text  with split points at positions 
. We then store  points as follows. 
For every , we store point  where 
 represents the lexicographic rank of the reverse of substring 
among all substrings , and  the rank of the suffix . A query will consist 
of a triplet  and will return the point
 with maximal coordinate  among all points that satisfy 
that ,  and . 
In this way we store  points in total. 
We store the set  of reverse of 
all substrings  for  
in a table  sorted in lexicographic order. 
Given any string , we can to determine the range of elements 
of  which have reverse of  as a prefix. The table  
can be built in . 
Reverting every string of  can be done by using a lookup table  which stores
the reverse of every possible string of length . The space used 
by the lookup table will be  bits and will allow to revert 
every string of length  in constant time. 


The data structure we use occupies 
bits of space and answers queries in time . 
This is obtained by building  data structures 
for  range maximal queries~\cite{farzan2012succinct}. 
By building a perfect binary search tree on the third dimension 
, then building a 2D range maximum query data structure for all points 
that fall in one subtree (we use only coordinates  and ), one 
multiplies the space and the query time by factor . 
Since the original data structure uses space  words 
and answers in  time, 
we obtain the bounds above 
by multiplying the time and space bounds by . 
By replacing  by , the total space usage is
 bits and the query time is . 

Given a factor  of length at least , we will 
issue  queries each of which will take 
 time. The final result will be maximum 
over all the results of the queries. 
In order to determine the queries 
to the  range-max structure, we will binary search 
the table  for every suffix of   of length 
(we first revert the suffix in time  
using the table ). This will determine 
the range . The ranges  are determined by querying 
the BWT of  in total time  (by backward searching). 

Thus the total query 
time will be  and the space . 
Choosing  and  ensures that the total 
time per factor is  which amortizes to 
time per character of the factor. 
The total space is dominated by the space used by  
which is  bits, and the total preprocessing time is dominated by the time needed 
to construct  which is  . 


\subsection{Sparsified tree}
\label{sec:rightmost_sparse_tree}
If we divide the universe  into blocks 
of equal size  and moreover only solve queries
for factors  
whose suffix array range crosses a boundary and whose phrase 
lengths is at most , then the number of nodes considered 
can not be more than . To justify 
this, consider for every boundary the deepest node that crosses 
a specific boundary. Obviously this node is unique, since if two nodes 
cross the same boundary, one has to be parent of the other
and then one of them would not be the deepest. 
Thus there can be not more than  such nodes. 
We call those nodes {\em basic nodes}. On the other hand, 
any node that crosses a boundary has to be ancestor of one of the basic 
nodes. Since, by definition a basic node can not have more than  ancestors, 
we deduce that the total number of nodes is . 
Recall now that the algorithm described in Section~\ref{sec:rightmost_sparse_tree} traverses 
the tree of phrases and for each leaf updates the minimum of the nearest 
marked ancestor and then for each phrase computes the rightmost pointer 
by traversing the whole subtree under the node of that phrase. 
Since, there are at most  phrases per node, 
each of the  nodes will be traversed  
times, at most  times for each of its (at most)  ancestors. 
Thus, the total cost will be . 
Choosing  ensures  overall running time. 
The total additional used space will be  bits dominated by the space
needed to store the nearest-marked ancestor information (see Section~\ref{sec:basic_rightmost}). 

\subsection{Remaining factors}
\label{sec:rightmost_rem_factors}

We will use Theorem~\ref{range_pred_theo2} for short factors 
that do not cross a block boundary. 
For each block we build a range-predecessor data structure. 
We can use parameter  and use a global precomputed table that adds 
 bits of space. Since each block contains at most
 points, construction takes 
 time. 
Each query is solved in time . 
Choosing  means total construction time 
adds up to .
This dominates the total query time, which adds up to 
. 
Notice that the  coordinates in each block are originally in . 
In addition to the range-predecessor structure, we will use a predecessor 
structure to reduce the  coordinate of a query to the interval . 
For that we assume that we have available all the
values  coordinates of the points that fall in the block sorted 
in increasing order. We also assume that the  coordinates 
of the points stored in the range-predecessor data structure have been reduced  
to the interval . That is, instead of storing the original  coordinate
of each, we store the rank of that coordinate among all values of  coordinates 
that appear in the block. 

\subsection{Putting pieces together and getting optimal space}
\label{sec:full_picture_and_opt_space}
Combining together the three categories above, we can get 
total time  and space  bits. Details 
are shown in Appendix~\ref{sec:time_efficient_rightmost}. 
The space can be reduced to optimal  bits. 
This is shown in Appendix~\ref{sec:opt_space_rightmost}. 
We thus have proved the following.
\begin{theorem}
We can find the rightmost occurrences of Lempel-Ziv factors in 
time  and space  
bits. The time is  if randomization 
is allowed. 
\end{theorem}

\section{Conclusions and Open Problems}
\label{sec-conclusion}

We leave two main open problems. Firstly, is it possible to compute the rightmost parsing
in  time, independent of the alphabet size? Note that even using  memory 
and  time would be interesting. The algorithm introduced in Section~\ref{sec-rightmost} 
is the current fastest running in  time (and using 
compact space). Secondly, are the time bounds we achieve, or anything  for 
that matter, possible if processing must be online?

\section*{Acknowledgements} 
Our deep thanks go to: Hideo Bannai for feedback 
on an early rendering of the algorithm of Section~\ref{sec-basic};
and to Juha K{\"a}rkk{\"a}inen, Dominik Kempa, and Travis Gagie, for frequent,
wide-ranging and inspiring discussions on LZ parsing and its properties.

\newpage

\bibliographystyle{splncs03}
\bibliography{lz}
\newpage
\appendix

\section{Wavelet tree construction}
\label{sec:wavelet_tree}
We first describe the core procedure used to build the wavelet tree and 
then describe the wavelet tree construction itself. We then describe
how range-predecessor queries are solved using the wavelet tree. 

\subsection{Core procedure}
\label{subsubsec:core_procedure}
Suppose that we have are given a parameter  bits, and that we 
can spend  time preprocessing to build (universal) tables that occupy  bits of space. 
The core procedure to build the wavelet tree~\cite{CP2010,MNV14,BGKS15} is as follows.
We are given an array of integers  and a bit position , where 
 for all , 
and the goal is to produce two arrays  and  
such that  is the subsequence of integers of  whose th 
most significant bit equals  and  is the subsequence of elements of  whose 
th most significant bit equals . We will describe a procedure that runs 
in time .

In order to fill the vectors 
and  we use two counters  and ,
initially set to . 
We scan the array  and read it in 
blocks of  elements. Suppose that 
a block contains  elements whose bit number  equals~
and  whose bit number  equals~. 
We denote by  the block and by  (resp. ), 
the subsequence of elements of  whose bit number  equals 
 (resp. ). We append the blocks  (resp. ) 
at the end of  (resp. ) respectively at positions 
indicated by  (resp. ) and then set 
(resp. ). 

We will use a lookup table  that produces tuples 
 for every possible 
block  of  elements of length  bits each. 
Since we have 
 possible blocks and every 
element in the table uses  bits, the size of the lookup table will 
be  bits. 

Reading a block , reading the entry , incrementing  (), and appending  () 
to  (), can all be done in constant time, since each of
the blocks  fit in  bits and reading or writing 
blocks of this size requires only a constant number of bit shifts and bitwise logical operations. 

\subsection{Construction}
A wavelet tree for a sequence  over alphabet  can be built 
in time  by repeating the 
core procedure for each node of the wavelet tree. 

More precisely at first level, 
we use the procedure to produce two arrays  and  
such that  is the subsequence of integers of  whose 
most significant bit equals  and  is the subsequence of elements of  whose 
most significant bit equals . Notice that the bitvector stored at the root 
can be trivially obtained from  in  time, by just scanning  and 
extracting the most significant 
bit of every element of  and append it to the bitvector. This process can be accelerated 
to run in , by using again a lookup table that givens the 
sequence of  most significant bits for every possible blocks of 
blocks of characters. 

At the second level, we will apply the same algorithm to 
(), to get the subsequence to produce two arrays  and 
( and ), 
such that () is the subsequence of integers of  whose 
second most significant bit equals  and () is the subsequence of elements of  whose 
second most significant bit equals . At that point, we can generate the bitvector  () 
that contains the second 
most significant bit of  () in time  () and finally throw  (). The generation of those two bitvectors (which are to be stored at the two children of the root of the tree) can also be done in total time . Once a bitvector has been generated 
we index it so as to support  and  queries. This is done in times  and  respectively for  and  using the technique described in~\cite{BGKS15}, which uses lookup tables of size  bits. 
We continue applying the algorithm in the same way for every node at every level, until we get to a leaf of the wavelet tree. 

Since we have  nodes and  levels, the total running time is 
. The total space used for the lookup tables is 
 bits and the total temporary space used during the construction 
is  bits, since only bitvectors are kept after a given level is constructed. 

We thus get the following lemma: 

\begin{lemma}
\label{lemma:wavelet_tree_build}
Given a sequence  of length  over alphabet  
and global precomputed tables of total size , where , we can build the wavelet tree
over  in time , 
using  bits of temporary space. 
\end{lemma}

\subsection{Range-predecessor queries using wavelet tree} 

We now show how range-predecessor queries are solved using a wavelet tree. 
We are given integers  and  and must find the point 
such that ,  and  is maximal. 
We assume that there is no point  such that . 
Otherwise, the answer to the query is trivial. 
The query proceeds in two phases. In the first phase, we find the longest 
common prefix between  and , and in second phase, we determine the 
remaining bits of . At this second phase, a bit number  is determined 
by traversing a node at level  and its value is the maximal value
for which the query issued at that node gives a non-empty answer 
(queries and their answers are defined more precisely below). 

The first phase proceeds as follows. At the root level, we 
check whether there interval  in the bitvector  stored at the root 
contains an occurrence of , 
by checking that . 
If that is the case, we continue to child number  of the root 
and recurse using the interval . 
Let  be the bitvector associated with child number . 
At the next level, we use two  queries on  with symbol  and
points  and  and check whether 
the interval  is non-empty. 
We continue in the same way down the tree, until we reach a node  for which we have an empty 
interval. Let  be the level of that node. Suppose that , then the answer is in the subtree 
of , and we can deduce that the longest common prefix between  and 
of . If , then we go up the tree decrementing  at each step and at a node  at level , check whether , and if so requery the bitvector at that node with the interval with 
which we already queried it before, but this time with bit 
value . If the query is successful, we stop climbing the tree and we will have determined that the longest 
common prefix between  and  is . 

We now describe the second phase of the query. 
The remaining  bits of  can be completed by traversing down from node . 

For that we  continue by querying the bitvector at node  with the same interval which we already used for querying node , but this time using bit value  instead of . 

We then continue to traverse down the tree, for each 
traversed node querying the bitvector for bit value . If the returned interval is non-empty, we continue 
traversing down the tree with the interval. Otherwise we query the for bit value  and continue traversing with 
the returned interval (which is necessarily non-empty). When we reach the leaf, we will have constructed 
the whole value of coordinate  and will have a singleton interval . In order to reconstruct the value of , we climb up the tree retraversing (in reverse order) the nodes we already traversed, and for a node at level  issue a  query using the bit value  for the single position that we obtained from the previous  query (or position  for the first  query). 
At this point we will have determined both coordinates  and  of the answer. 


\section{Range-predecessor query answering}
\label{sec:range_succ_queries}
We now describe how we answer range predecessor queries with the data structure
used in Theorem~\ref{range_pred_theo1}. The algorithm can be thought of as a generalization 
of the query algorithm used for the wavelet tree. 
We are given integers  and  and must find the point 
such that ,  and  is maximal. 
A query will first proceed by traversing the tree top-down, 
where at level  a  query at a node  will allow 
to determine the range in  
from the range in , where  is the next node at level . 
The range minimum query data structure at level  will allow to determine 
whether the  coordinate of the answer shares at least  chunks with . 
Once it has been determined that the  coordinate shares a chunk of length 
 with , then the value of the next chunk (chunk number ) 
of  should be smaller than the corresponding chunk of  and then the next chunks 
will all need to have maximal values. Hence, we will use range maximum 
queries for at all next levels. With the  coordinate determined
we can read the  coordinate from . 

We now give a detailed description of how the queries are solved. 
Before delving into the details, we first show how to handle the easy case 
in which the answer is a point  such that . To eliminate 
the case, it suffices to test that , and if it is,
then the answer is . We now show how 
queries are answered under the assumption that . 

We traverse the tree top-down, successively for the root node, 
then the node , then the node 
and so on. For the root node, we first compute the value . 
Then query the predecessor data structure  to check whether 
 contains the value . The predecessor data structure will then be able to return 
a pair of values  where  (resp. ) 
is the leftmost (resp. rightmost) position in  such that  
(resp. ). If the interval  does not contain the value , 
we stop at the first level. Otherwise, we go to the second level to node , compute , and query the predecessor 
data structure  for the pair 
 to check whether 
contains the value . If that is the case, then
the predecessor data structure will return a pair  such 
that  (resp. ) is the leftmost (resp. rightmost) position 
in  with  
(resp. ). 
We continue the traversal of the tree in the same way until the  
predecessor query fails at a certain level \footnote{Note that 
one of the predecessor queries will have to fail, because we have 
eliminated the case that the  component of the query answer 
equals .} 

We let  be the deepest level such that . This tells us 
that the final value for  is prefixed by , 
but is followed by a chunk that differs from  (more precisely, is strictly smaller than) 
. 

Then we query the wavelet tree  with
 to find the range predecessor of 
in the interval . This will produce the next  bits
of  (denoted by ) and an interval . We then continue to the node 
, but this time the next bits of  will be produced 
by the range maximum query data structure over interval . 
We continue traversing the tree in this way until the bottom node,  
at which point we will have induced the full value of . To get  we simply read . 
 
\subsection{Faster queries}
\label{subsec:faster_range_pred}
We now show how queries are answered using the data structure
from Theorem~\ref{range_pred_theo2}. The algorithm can be thought of as generalization 
of the query algorithm presented in beginning of Section~\ref{sec:range_succ_queries}. 
Recall that we are given integers  and  and must find the point 
such that ,  and  is maximal. 
As usual we eliminate the trivial case that , 
in which case the answer is . 
As before the query proceeds in two phases. The first phase
determines the longest common prefix between  and 
and the second phase allows us to determine the remaining bits 
of . Finally, the value of  can be determined by reading . 
We now give details of the two phases. 
In the first phase, the longest common prefix between  and , is determined 
in chunks of  bits by traversing the tree of granularity 
. Then at most  bits will remain to be determined, 
and we continue from a node in the tree at level of granularity , 
labelled by , for some integer .
Since we have to determine less than  bits, the number of traversed 
tree levels will be less than . We then continue refining the length of 
longest common prefix by traversing trees at decreasing levels of granularity until 
we reach a node at level of granularity , which is an ordinary wavelet tree. 
As before at each node, we will use a range-minimum query to determine whether the label 
of the node is a prefix of the longest common prefix of  and  and use the  
predecessor query to determine whether we continue exploring the next node at the next tree level
and the interval to be used at that next node. 
In the second phase, we will traverse trees of increasing level of granularities, 
from the node at which the first phase has stopped. 
This time we will use range-maximum queries to determine both the next node 
to explore and the next chunk of bits of  (the chunk length being the 
level of granularity). We switch from a level of granularity  to the next 
one of granularity , whenever the number of determined bits of  
is multiple of  and the first node at that level will be the one 
labelled by the bits of  which have been determined so-far. 
As before the range to be used at next tree level will be determined using predecessor 
query on the range at current node, using the current chunk determined from the 
range-maximum query. 
It is easy to see that the query time of both phases is , 
since we have  levels of granularity and at each such level we traverse 
at most  nodes, spending  time at each node. 
This finishes the description and analysis of the queries. 

We traverse the tree top-down, successively for the root node, 
then the node , then the node 
and so on. For root node, we first compute the value . 
Then query the predecessor data structure  to check whether 
 contains the value . The predecessor data structure will then be able to return 
a pair of values  such that  (resp. ) 
is the leftmost (resp. rightmost) position in  such that  
(resp. ). If the interval  does not contain the value , 
we stop at the first level. Otherwise, we go to the second level to node , compute  and query the predecessor 
data structure  for the pair 
 to check whether 
contains the value . If that is the case, then
the predecessor data structure will return a pair  such that 
 (resp. ) is the leftmost (resp. rightmost) position 
in  such that  
(resp. ). We continue the traversal 
of the tree in the same way until the predecessor query fails at a certain level 
\footnote{note that one of the predecessor queries will have to fail, because 
we have eliminated the case that the  component of the query answer equals . 
To see why notice that the last query is for node  
where  and the last query 
is for }.


\section{Elias-Fano based predecessor data structure}
\label{sec:Elias_Fano_pred}
We now show how Elias-Fano predecessor data structures are built. 
Suppose that we have a set  of  keys from interval 
(where for simplicity  and  are powers of two). We can 
show a data structure that uses  bits of space 
and that allows us to answer to predecessor () queries
in  time as well as finding the key of rank  ( queries) 
in constant time. The Elias-Fano encoding is composed of 
two substructure. Let  be the sequence 
of keys to be encoded with  for all . The first substructure is an array 
, where  contains the least significant 
 bits of . The second substructure is a bitvector
V of length  bits which contains the sequence , 
where . In other words, the bitvector  encodes the most significant  bits 
of elements . In order to support the  operation for position 
(computing ), we can first go to  to retrieve the least significant 
bits of  and then do a  query to find the location of the th one 
in . If that location is  then the value of the most significant  bits 
of  are equal to the number of zeros before position , which is 
. Thus a  query can be answered in constant time. 
To answer to  queries, we will build a -fast trie 
predecessor data structure~\cite{Wi83} on the set  of  
keys . 
This data structure will occupy  bits of space and allows 
to determine the prededecessor of a query key  in  in  time. 
This will allow to restrict the predecessor search in  to a small interval 
of size . The predecessor search can then be completed by a binary search 
over that set, by doing  queries. This binary search also takes 
 time. 

The data structure can be generalized as follows. 
Given a parameter  (again assume that  is a power of two), 
we can build a data structure that occupies  bits 
of space and that answers to  and  
queries within the same amount of time. To implement the data structure, 
we will use a bitvector  of size  bits with  ones 
and  zeros (the ones and zeros are stored as was defined before
except that ) and store in  the 
least significant  bits of each key. The query time bounds
are preserved. 
\begin{lemma}
Given a set  with  and a number , 
we can build a data structure which occupies
 (assuming , and  are powers of 
two) and answers to  queries in time  
and  queries in constant time. 
\end{lemma}

\subsection{Simple Construction}
~\label{sec:simple_Elias_Fano_build}
We can now show the following lemma: 
\begin{lemma}
\label{lemma:simple_Elias_Fano_build}
Given a sequence  of length  over alphabet  (where  and 
both  and  are powers of two), we can build  predecessor data structures 
so that the data structure number  stores the positions of character  in the sequence  such that: 
\begin{enumerate} 
\item The total space occupied by all predecessor data structures is  bits of space. 
\item The total construction time of the data structures is  time. 
\item A predecessor () query is answered in time  and 
a  query is answered in constant time
\end{enumerate}
\end{lemma}
\begin{proof}
We will build  Elias-Fano data structures (generalized as above)
denoted  for . 
For each data structure we set . The space used by data structure 
 is . 
Since  and  sum up to , we get that the total space usage is 
bits of space. The vectors  and  can be built easily in . 
For that, we can first build the sequence of positions  from  (initially the sequences 
are empty), by scanning  and appending for each  append position  to sequence . 
Then, building  and  is done by scanning  and for each element 
writing its least significant  bits in  and writing a number of zeros 
and a one in 
(the number of zeros is based on the  most significant bits). 

In order to support  queries, for each  containing at least  elements, we sample every th occurrences of character  in  storing the resulting positions in a -fast trie . A predecessor query on  is then solved by first querying , which will answer in time  and complete with binary search for an area of length at most  doing   queries on . The construction of  clearly takes  and the space is clearly  bits of space. When added over all , the construction time for  and  structures on  sums up to  and for all  sums up to . This finishes the proof of the lemma.  
\qed
\end{proof}

\subsection{Bit-parallel Construction}
~\label{sec:bit_parallel_Elias_Fano_construction}
We now exploit the bitparallelism to show a faster construction, showing the following lemma: 
\begin{lemma}
\label{lemma:bit_parallel_elias_fano_build}
Given a sequence  of length  over alphabet  
(where  and  are both powers of two) and a global precomputed table of size  
such that , we can build  predecessor data structures 
so that the data structure number  stores the positions of character   in the sequence  such that: 
\begin{enumerate} 
\item The total space occupied by all predecessor data structures is  bits of space. 
\item The total construction time of the data structures is . 
\item The temporary space used during the construction is  bits. 
\item A predecessor () query is answered in time  and 
a  query is answered in constant time
\end{enumerate}
The global precomputed table can be shared by many instances of the data structures. 
\end{lemma}
\begin{proof}
We will build  Elias-Fano data structures (generalized as above)
denoted  for . 
For each data structure we set . The space used by data structure 
 is . 
Since  and  sum up to , we get that the total space usage is 
bits of space. 
As in Lemma~\ref{lemma:simple_Elias_Fano_build}, we will build  Elias-Fano data structures (generalized as above)
denoted  for , in which we set we set  for each data structure. 
As shown above the total space usage of the data structures sums up to . 

We now describe the construction procedure. The construction proceeds in 
phases. We describe the first phase, the subsequent phases will be (almost) identical, but 
will have different input and output. 
Before doing the first phase, we construct a vector 
such that  and a bitvector . Notice that these two vectors represent together the Elias-Fano representation of the sequence . 
The phase will have as input the arrays ,  and the sequence  and will output a pair of vectors , 
a pair of bitvectors , and a pair of sequences . The sequence  (resp. ) will store the subsequence 
of characters from  which belong to the first (resp. second) half of the alphabet. The array  (resp. ) will store the values from  whose corresponding positions in  belong to first (resp. second) half of the alphabet. Finally the bits of  are copied into  and . More precisely every  in  is copied to both  both  while every  is copied to either  or . The vector  is scanned right-to-left and the th  in  is in correspondence with the th element of . If  belongs to the first (resp. second) half of the alphabet (this can be checked by looking at most significant bit of ), then it is appended to  (resp. ). Whenever a  is encountered in , it is appended to both  and . One can now easily see that  and  (resp.  and ) is the Elias-Fano representation of the occurrences of characters from the first (resp. second) half of the alphabet in the sequence . The first phase can thus easily construct the output by simultaneously scanning ,  and  in left-to-right order and appending at the end of , , , ,  and . In order to efficiently implement the first phase, we will make use of the four Russian technique. We read ,  and  into consecutive blocks of  consecutive elements, which occupy  bits of space. We use a lookup table which for each combination of  blocks from ,  and , will indicate the blocks to be appended at the end of , , , ,  as well as by how much we advance the scanning pointers into  and  (which will be the number of s in the block read from ). The information stored for every combination easily fits into  bits of space and the total number of possible combinations 
is  and thus the lookup table occupies  bits of space. In the second phase, we build a pair of vectors  (resp. ), 
a pair of bitvectors  (resp. ), and a pair of sequences  (resp. ) based on ,  and  (resp. ,  and ). The procedure is similar to the one  used in the first phase, except that now the distribution of elements in the output is based on the second most significant bit of elements of  (resp. ). The lookup table for the second occupies the same space as the one built in the first phase. At the end of the second phase the pairs , , ,   will represent Elias-Fano representations of the occurrences of characters from respective subalphabets , , 
 and  in the original sequence . We continue in the same way doing  more phase, where at phase  we will have built Elias-Fano for sub-alphabets of size . At the end we will get the Elias-Fano representation of the occurrences of each distinct character in the sequence . 
We now analyze the time and space used by the algorithm. 
The total space used by all the  lookup tables will be  bits of space. For the running time it is easy to see that a phase  runs in time  time, since we have  sub-alphabets and we process  elements of each processed vector in constant time. Over all  phases the total running 
time sums up to . 
Let  be the Elias-Fano representation for occurrences of character . 
In order to complete the construction, we need to construct the support for  and  
queries on each vector . This can be done in time  for the bitvector , by using the construction in~\cite{BGKS15}. This allows us to support  queries on  in constant time. 
In order to support  queries, for each  containing at leat  elements, we sample every th occurrences of character  in  by doing  queries on  for positions  and store the resulting positions in a -fast trie . A predecessor query on  is then solved by first querying  which will answer in time  and complete with binary search for an area of length at most  doing   queries on . The construction of  clearly takes  and the space is clearly  bits of space. When added over all , the construction time for  and  structures on  sums up to  and for all  sums up to . This finishes the proof of the lemma.  
\qed
\end{proof} 

\section{Sampled range-minimum queries}
\label{sec:sampled_rmq}
We first start by constructing a sampled sequence  from the sequence  we want to index. 
Initially  is empty. Suppose that we allow  precomputation of a table of size  bits 
that can be shared by all instances of the range minimum or maximum queries. We assume that . 
We divide  into blocks of size  elements each, then scan it in left-to-right order, compute minimum element in each block and append the result at the end of . We then build 
a rmq  on top of  in time . To compute the minimum in each block, 
we use a lookup table that returns the minimum element on all possible blocks of size . The table stores  
elements each of length  bits, for a total space of  bits. 

To answer a rmq query for , we first check whether the query is contained in one or two blocks of . If that is the case, then we read the blocks  compute the minimum in each block in constant time, and compute the minimum of all the  block also in constant time. For that we use a precomputed table that will contain  elements each using  bits. The table contains the answers to all possible queries () over all possible blocks (). 
The space is  bits. 
If the query spans more than two blocks, then we can divide it into three parts. A head and tails parts which are contained in one block each and a central part, whose length is multiple of a block length. We answer compute the minimum on head and tail 
in constant time, using the precomputed table and compute the minimum on the central part using , and finally take the minimum of the three. 

We thus have proved the following lemma: 
\begin{lemma}
\label{fast_rmq_build_lemma}
Given a sequence  of length  over alphabet  (where  and both  and  
are powers of two) and global precomputed tables total of size , where , we can build a range min data structure on top of : 
\begin{enumerate} 
\item The space used by the data structure is  bits. 
\item A query is answered in constant time. 
\item The data structure can be constructed in time . 
\item The temporary space used during the construction is  bits. 
\end{enumerate}
The precomputed tables can be shared by many instances of the data structure and can be built in  time. 
\end{lemma}
\section{Construction of the fast range-predecessor data structure}
In this section, we show the construction of the data structure used in Theorem~\ref{range_pred_theo2}. 
The main ingredients are lemmas~\ref{fast_rmq_build_lemma} and~\ref{lemma:bit_parallel_elias_fano_build}. 
We let  be the level of granularity  such that  and 
. For level  and higher, we will use simple linear time 
algorithms to build the predecessor and range minimum (maximum) data structures. The time for these levels 
is dominated by the time used for level  for which the total number of elements stored in all sequences (and hence in predecessor and range minimum or maximum data structures) at all nodes will be . The sum of alphabet sizes at all nodes is clearly . Hence the construction time at that level will be . For the next higher level the construction time will be . Continuing the same way we deduce that the construction time for these levels will be , 
where  is the number of levels. 

For level  and lower we will use bitparallel construction algorithm. For an given node  containing 
 elements, the construction time will be  (which is dominated by the time for construction the Elias-Fano predecessor data structure), where . The total sum of the  term over all nodes is  and the total sum of the terms 
 will be , since the total
number of elements stored in all nodes is . Thus, we have . Since 
we have , we conclude that . We notice that  will decrease 
by factor  each time we go to the next smaller level of granularity. The term  remains stable. Since, the term  of level  will dominate the terms  of all lower levels, and the term  remains stable over the lower levels, we conclude that the total construction 
time for level  and lower levels will be , where  is the number of levels. 
Summing up the construction times of all levels we get the construction time stated in Theorem~\ref{range_pred_theo2}.


\section{Rightmost Parsing}

\subsection{The full picture}
\label{sec:time_efficient_rightmost}
We now present the complete algorithm. 
We first categorize each query.
Queries that fall in the same block are redirected 
to the range-predecessor data structure responsible 
for handling that block and queries that are longer 
than  are put aside to be solved by the data structure 
for handling long factors. 
For the remaining factors (short factors with ranges 
that cross block boundaries) we will construct the tree of queries. 
This can be done 
in  time, by sorting the query ranges 
first by starting positions and then by the negation
of their ending positions. The sorting can be done 
in  time 
using 
radix-sort. 

The rest of the algorithm is straightforward, except for 
few details on how the points 
stored in the range-predecessor data structure are constructed. 
This is done as follows. The generated 
points are pairs consisting of a suffix array position ( coordinate)
and text position ( coordinate). 
We first notice that 
we need to divide the points according
to their  coordinates, such that the points
whose  coordinate is in  go to the first range-predecessor 
data structure, points with coordinate in  
go to second data structure and so on. 
The points are generated by inverting the Burrows-Wheeler 
transform in left-to-right order. This generates 
the values of  coordinates that appear in each block in sorted order. 
Finally, for each block, we need as input to building its range-predecessor 
data structure, the 
points sorted by  coordinate and for each point the  coordinate 
replaced by the rank of the value among all values of  that appear 
inside the block. Since we extract the points by increasing value 
of , we can keep an array  that stores the number of points 
that have been extracted so far for each block. The counter is incremented 
each time we extract a point, and the rank of the  value of a point 
that has been just extracted in block  will equal the counter . 
Finally we need to sort all the extracted points by their  coordinates, 
and this done via radix-sort. 
The algorithm uses  bits and  time 
overall~\footnote{Notice the time is deterministic and not randomized.  
The source of randomization is the construction and indexation of the BWT, 
which is subsequently inverted. However this is not needed anymore, since we can 
build the suffix array of  in deterministic linear time and  bits of space.}.

\subsection{Optimal space}
\label{sec:opt_space_rightmost}
It remains to show how to reduce the space to the optimal  bits. 
The additional space used for long factors (Section~\ref{sec:rightmost_long_factors}) 
and short factors crossing block boundaries (Section~\ref{sec:rightmost_sparse_tree}) 
is  bits. The bottleneck is for the factors with small range (
see Section~\ref{sec:rightmost_rem_factors}) in which the used space is 
. In what follows, we show how to reduce the space to 
bits. 

To this end we divide the range of  into  equal sub-ranges. 
We now build the tree of all queries that do not cross 
a block boundary in the same way we did for the short factors that cross a block boundary
in Section~\ref{sec:rightmost_sparse_tree}. 
As before, for every leaf of the suffix tree, 
we associate the nearest ancestor that corresponds to a query range. Additionally for each node 
in the query tree, we associate a bitmap  of size  bits. The 
query tree occupies  bits. We now invert the BWT and, for each leaf that has 
been extracted, we mark the bit , where  is the block in which the text position 
falls and  is the internal node pointed to by the leaf. We finally do a complete postorder 
traversal of the tree so that the bitmap of each node is bitwise OR'd with all the bitmaps of its 
children. At the end, for every query range  corresponding to a node , 
 will mark every subrange of  where there exists a point  such that 
 and  belongs to the subrange of . 

We now build  query trees (henceforth local query trees). For every node of the tree
(henceforth main query tree), 
we know that all range-predecessor queries will have the same interval 
 on the  axis, but a different  on the  axis. 

For every query, the answer can only be in two sub-ranges: the sub-range 
 that contains  if  is marked  
or the largest  such  contains a marked bit. 
The  local query trees are built via a preorder traversal 
of the main query tree and for every query determining the one or two 
target local query trees to which the query should be copied. 
That is a query will be copied to the local query trees  
(if  is marked) and  (If  exists). 

The nodes of the query trees (and the queries attached to them) 
are built on-the-fly during the traversal 
of the main query tree, by keeping a stack for every query tree. 
For every query in a local query tree, we keep a pointer to the originating 
query in the main query tree.
Note that the total space used by all the local query trees remains bounded 
by  bits, which is the same as the main query tree. 

We will now apply the algorithm described in Section~\ref{sec:rightmost_rem_factors} 
on every subrange of . In more detail, we invert the BWT left-to-right. 
We do this in  phases, where in phase  we will 
compute all the positions in  that point to text positions in 
 with . That is, we output all pairs , 
such that . During the course of phase , we apply 
the algorithm of Section~\ref{sec:rightmost_rem_factors} verbatim, 
except that now the points are only the one with  
and the queries are from the th local query tree and not 
the main query tree. 

It is easy to see that the time complexity stays exactly as it was before. 
The construction time of the range-predecessor and predecessor data structures 
for every phase will now be . 
In particular the radix sort done on the  values can now be executed 
in  time and using  words of space. 
The space usage of the range-predecessor and predecessor structures over all 
phases will be just  bits. Both structures are destroyed at the end of 
each phase, so that the total peak space usage of the whole algorithm is  bits. 


\end{document}
