\section{Experiments}
In this section, we show the superiority of GAP by comparing it with the state-of-the-art PGD-MAML family and the MAML family. 

\subsection{Implementation Details}
For the reproducibility, we provide the details of implementation. Our implementations are based on Torchmeta~\cite{deleu2019torchmeta} library. 

\subsubsection{Hyper-parameters for the few-shot learning}
\label{sec:5.1.1}
For the few-shot learning experiments, we use the hyper-parameters in Table~\ref{hyperparameters1}.
\begin{table}[h!]
  \caption{Hyper-parameters used for training GAP on various few-shot learning experiments.}
  \centering
  \resizebox{0.99\columnwidth}{!}{\begin{tabular}{lccccccccc}
    \toprule
    Hyper-parameter                 &\multicolumn{3}{c}{Sinusoid} &\multicolumn{2}{c}{mini-ImageNet} &\multicolumn{2}{c}{tiered-ImageNet} & \multicolumn{2}{c}{Cross-domain}  \\
    \midrule
                                        & 5 shot & 10 shot & 20 shot & 1 shot & 5 shot      & 1 shot & 5 shot    & 1 shot & 5 shot\\
    Bathc size                          & 4     & 4       & 4        & 4     & 2            & 4     & 2          & 4     & 2      \\
    Total training iteration            & 70000 & 70000   & 70000    & 80000 & 80000        & 130000& 200000     & 80000 & 80000  \\
    inner learning rate $\alpha$        & $10^{-2}$ & $10^{-2}$ & $10^{-2}$ & $10^{-2}$  & $10^{-2}$ & $10^{-2}$ & $10^{-2}$ & $10^{-2}$ & $10^{-2}$ \\
    outer learning rate $\beta_1$       & $10^{-3}$ & $10^{-3}$ & $10^{-3}$ & $10^{-4}$ & $10^{-4}$ & $10^{-4}$ & $10^{-4}$ & $10^{-4}$ & $10^{-4}$ \\
    outer learning rate $\beta_2$       & $10^{-3}$ & $10^{-3}$ & $10^{-4}$   & $3\times10^{-3}$ & $10^{-4}$ & $3\times10^{-3}$ & $10^{-4}$ & $3\times10^{-3}$ & $10^{-4}$ \\
    The number of training inner steps  & 5     & 5       & 5        & 5     & 5            & 5     & 5          & 5     & 5      \\
    The number of testing inner steps   & 10    & 10      & 10       & 10    & 10           & 10    & 10         & 10    & 10     \\
    Data augmentation                   & \multicolumn{3}{c}{None}   & \multicolumn{2}{c}{random flip} & \multicolumn{2}{c}{random flip} & \multicolumn{2}{c}{random flip} \\
    \bottomrule
  \end{tabular}}
  \label{hyperparameters1}
\end{table}

\subsubsection{Hyper-parameters for the few-shot domain generalization and reinforcement learning}
For the few-shot domain generalization and reinforcement learning experiments, we use the hyper-parameters in Table~\ref{hyperparameters2}. In the few-shot domain generalization experiments, we use the data augmentation used in Meta-Dataset~\cite{triantafillou2019meta}.
\begin{table}[h!]
  \caption{Hyper-parameters used for training GAP on the few-shot domain generalization and reinforcement learning experiments.}
  \centering
  \resizebox{0.99\columnwidth}{!}{\begin{tabular}{lccccccc}
    \toprule
    Hyper-parameter                 &\multicolumn{4}{c}{Meta-Dataset} & Half-cheetah Dir & Half-cheetah Vel & 2D Navigation  \\
    \midrule
                                        &\multicolumn{2}{c}{ImageNet only} &\multicolumn{2}{c}{All dataset} \\
                                        & +fo-MAML  & +Proto-MAML   & +fo-MAML  & +Proto-MAML      \\
    Bathc size                          & 16        & 16            & 16        & 16         & 40        & 40        & 40 \\
    Total training iteration            & 50000     & 50000         & 50000     & 50000      & 500       & 500       & 500\\   
    inner learning rate $\alpha$        & $10^{-1}$ & $10^{-1}$     & $10^{-1}$ & $10^{-1}$  & $10^{-1}$ & $10^{-1}$ & $10^{-1}$ \\
    outer learning rate $\beta_1$       & $10^{-5}$ & $10^{-4}$     & $10^{-5}$ & $10^{-4}$  & $10^{-3}$ & $10^{-3}$ & $10^{-4}$\\
    outer learning rate $\beta_2$       & $10^{-5}$ & $10^{-4}$     & $10^{-5}$ & $10^{-5}$  & $10^{-3}$ & $10^{-3}$ & $10^{-4}$\\
    The number of training inner steps  & 10        & 10            & 10        & 10         & 1, 2, 3   & 1, 2, 3   & 1, 2, 3\\
    The number of testing inner steps   & 40        & 40            & 40        & 40         & 1, 2, 3   & 1, 2, 3   & 1, 2, 3\\
    \bottomrule
  \end{tabular}}
  \label{hyperparameters2}
\end{table}


\subsubsection{Backbone Architecture}
\label{sec:5.1.2}
\textbf{2-layer MLP network:}
For the few-shot regression experiment, we use a simple Multi-Layer Perceptron (MLP) with two hidden layers of size $40$, with ReLU  nonlinearities as in~\cite{finn2017model}. For the reinforcement learning experiment, we also use a simple MLP with two hidden layers of size $100$ with ReLU as in~\cite{finn2017model}.
\newline
\textbf{4-Conv network:}
For the few-shot classification and cross-domain few-shot classification experiments, we use the standard Conv-4 backbone used in~\cite{vinyals2016matching}, comprising 4 modules with $3 \times 3$ convolutions, with 128 filters followed by batch normalization~\cite{ioffe2015batch}, ReLU non-linearity, and $2 \times 2$ max-pooling.
\newline
\textbf{ResNet-18:}
For the few-shot domain generalization experiments, we employ ResNet-18 as the general feature extractor, following the methodology of previous few-shot domain generalization studies~\cite{triantafillou2019meta, baik2023learning}. 
In GAP+fo-MAML, the weights and biases of the linear layer are initialized to zero and are not meta-trained, consistent with~\cite{triantafillou2019meta}. This means that the linear layer is adapted from zero initialization during the inner-loop optimization. 
In GAP+Proto-MAML, the linear layer is initialized as described in~\cite{triantafillou2019meta}.

\subsubsection{Optimization}
We use ADAM optimizer~\cite{kingma2014adam}. 
For tiered-ImageNet experiment, the learning rate (LR) is scheduled by the cosine learning rate decay~\cite{loshchilov2016sgdr} for every 500 iterations. 
For Meta-Dataset, we decay the learning rate of each parameter by a factor of $0.8$ every $10,000$ iterations. In all the experiments except for tiered-ImageNet and Meta-Dataset, the learning rate is unscheduled.

\subsubsection{Pre-training}
For the few-shot domain generalization experiments, we initialize the feature extractor using the weights of the k-NN Baseline model trained on ImageNet, as described in~\cite{triantafillou2019meta}.

\subsubsection{Preconditioning}
In the few-shot regression and the reinforcement learning experiment, we apply preconditioner only to the hidden layer. In both few-shot classification and the few-shot domain generalization experiments, we only apply preconditioner to convolutional layers.

\subsection{Few-shot regression}
\subsubsection{Datasets and Experimental setup}
The goal of few-shot regression is to fit an unknown target function for the given $K$ sample points from the function.
For the evaluation of few-shot regression, we use the sinusoid regression benchmark~\cite{finn2017model}.
In this benchmark, sinusoid is used as the target function. Each task has a sinusoid $y(x)=A\sin(\omega x + b)$ as the target function, where the parameter values are within the following range: amplitude $A\in\left[0.1, 5.0\right]$, frequency $\omega\in\left[0.8, 1.2\right]$, and phase $b\in\left[0, \pi\right]$. For each task, input data point $x$ is sampled from $\left[-5.0, 5.0\right]$. 
In the experiment, we use a simple Multi-Layer Perceptron (MLP), following the setting in~\cite{finn2017model}. 
The details of the architecture are provided in Section~\ref{sec:5.1.2}.

\subsubsection{Results}
We evaluate GAP and compare it with MAML family and PGD-MAML family on a regression task. As shown in Table~\ref{tab:regression}, GAP consistently achieves the lowest mean squared error (MSE) scores, with the lowest confidence intervals in all three cases. The performance of GAP is improved by 89\% on 10-shot and 94\% on 20-shot compared to the performance of state-of-the-art algorithms.

\begin{table}[t!]
    \caption{Few-shot regression for the sinusoid regression benchmark with a \textit{2-layer MLP} backbone. We report MSE $\pm$ 95\% confidence intervals(ci) for 600 tasks following the setup in \cite{finn2017model}. $^{\dagger}$ denotes PGD-MAML family.}
  \centering
  \resizebox{0.5\textwidth}{!}{\begin{tabular}{lcccc}
    \toprule
    Algorithm                           &\quad\; 5-shot         &\quad\; 10-shot             &\quad\; 20-shot\\
    \midrule
    MAML~\cite{finn2017model}          &\quad\; $1.13\pm0.18$  &\quad\; $0.77\pm0.11$       &\quad\; $0.48\pm0.08$\\
    Meta-SGD$^{\dagger}$~\cite{li2017meta}         &\quad\; $0.90\pm0.16$  &\quad\; $0.53\pm0.09$       &\quad\; $0.31\pm0.05$\\
    MT-Net~\cite{lee2018gradient}      &\quad\; $0.76\pm0.09$  &\quad\; $0.49\pm0.05$       &\quad\; $0.33\pm0.04$\\
    ALFA~\cite{baik2020meta}           &\quad\; $0.92\pm0.19$  &\quad\; $0.62\pm0.16$       &\quad\; $0.34\pm0.07$\\
    L2F~\cite{baik2020learning}        &\quad\; $0.71\phantom{.} \pm $ N/A   &\quad\; $0.37\phantom{.} \pm $ N/A  &\quad\; $0.16\phantom{.} \pm $ N/A\\
    PAMELA$^{\dagger}$~\cite{rajasegaran2020meta}  &\quad\; $0.54\pm0.06$  &\quad\; $0.41\pm0.04$       &\quad\; $0.17\pm0.03$\\
    MeTAL~\cite{baik2021meta}          &\quad\; $0.74\pm0.18$  &\quad\; $0.44\pm0.11$       &\quad\; $0.21\pm0.06$\\
    \midrule
    GAP$^{\dagger}$                                &\quad\; $\mathbf{0.16\pm0.04}$  &\quad\; $\mathbf{0.04\pm0.01}$       &\quad\; $\mathbf{0.01\pm0.01}$\\
    \bottomrule
  \end{tabular}}
  \label{tab:regression}
\end{table}

\subsection{Few-shot classification}
\subsubsection{Datasets and Experimental setup}
For the few-shot classification, we evaluate two benchmarks:
(1) mini-ImageNet~\cite{vinyals2016matching}; this dataset has 100 classes and it is a subset of ImageNet~\cite{russakovsky2015imagenet}, and we use the same split as in~\cite{ravi2016optimization}, with 64, 16 and 20 classes for train, validation and test, respectively. (2) tiered-ImageNet~\cite{ren2018meta}; this is also a subset of ImageNet with 608 classes grouped into 34 high-level categories, and divided into 20, 6 and 8 for train, validation, and test, respectively.
For all the experiments, our model follows the standard \textit{Conv-4} backbone used in~\cite{vinyals2016matching}.
The details of the architecture are provided in Section~\ref{sec:5.1.2}.
Following the experimental protocol in~\cite{finn2017model}, we use 15 samples per class in the query-set to compute the meta gradients. In meta training and meta testing, the inner-loop optimization is updated in five and ten steps, respectively.

\subsubsection{Results}
Table~\ref{tab:miniImageNet}~\&~\ref{tab:tiered-ImageNet} present the performance of GAP, the state-of-the-art PGD-MAML family, and the state-of-the-art MAML-family on mini-ImageNet and tiered-ImageNet under two typical settings: 5-way 1-shot and 5-way 5-shot. 
The GAP outperforms all of the previous PGD-MAML family and MAML family. 
Compared to the state-of-the-art MAML family, GAP improves the performance with a quite significant margin for both mini-ImageNet and tiered-ImageNet datasets.
Compared to the state-of-the-art PGD-MAML family, GAP shows that the 1- and 5-shot accuracy can be increased by 1.4~\% and 1.5~\% on mini-ImageNet dataset, and by 0.7~\% and 0.68~\% on tiered-ImageNet dataset, respectively. 
We also evaluated Approximate GAP that is introduced in Section~\ref{sec:3.3}. 
The results show that the approximated version can perform comparably to the original GAP. 
Although Approximate GAP shows slightly lower accuracies than the original,  its performance is superior to most of the existing algorithms because of its Riemannian metric property.

\begin{table}[t!]
    \caption{5-way few-shot classification accuracy (\%) on mini-ImageNet with a \textit{Conv-4} backbone. We report mean $\pm$ 95\% confidence intervals(ci) for 600 tasks according to \cite{finn2017model}. $^{\dagger}$ denotes PGD-MAML family.}
  \centering
  \resizebox{0.5\textwidth}{!}{\begin{tabular}{lccc}
    \toprule
    Algorithm                                          & \quad \; 5-way 1-shot                         & \quad \; 5-way 5-shot\\
    \midrule
    MAML~\cite{finn2017model}                           & \quad \; $47.89 \pm 1.20$                     & \quad \; $64.59 \pm 0.88$\\
    Meta-SGD$^{\dagger}$~\cite{li2017meta}                         & \quad \; $50.47 \pm 1.87$                     & \quad \; $64.00 \pm 0.90$\\
    BMAML~\cite{yoon2018bayesian}              & \quad \; $53.80 \pm 1.46$                     & \quad \; $64.23 \pm 0.69$\\
    ANIL~\cite{raghu2019rapid}                         & \quad \; $46.70 \pm 0.40$                     & \quad \; $61.50 \pm 0.50$\\
    LLAMA~\cite{grant2018recasting}.                   & \quad \; $49.40 \pm 1.83$                     & \quad \quad N/A\\
    PLATIPUS~\cite{finn2018probabilistic}              & \quad \; $50.13 \pm 1.86$                     & \quad \quad -\\
    T-net~\cite{lee2018gradient}                       & \quad \; $50.86 \pm 1.82$                     & \quad \quad N/A\\
    MT-net~\cite{lee2018gradient}                      & \quad \; $51.70 \pm 1.84$                    & \quad \quad N/A\\
    MAML++~\cite{antoniou2018train}                    & \quad \; $52.15 \pm 0.26$                     & \quad \; $68.32 \pm 0.44$\\ 
    iMAML-HF~\cite{rajeswaran2019meta}                 & \quad \; $49.30 \pm 1.88$                     & \quad \quad N/A\\
    WarpGrad~\cite{flennerhag2019meta}                 & \quad \; $52.30 \pm 0.90$                     & \quad \; $68.40 \pm 0.60$\\
    MC1$^{\dagger}$~\cite{park2019meta}                            & \quad \; $53.74 \pm 0.84$                     & \quad \; $68.01 \pm 0.73$\\
    MC2$^{\dagger}$~\cite{park2019meta}                            & \quad \; $54.08 \pm 0.88$                     & \quad \; $67.99 \pm 0.73$\\
    MH-C$^{\dagger}$~\cite{zhao2020meta}                           & \quad \; $48.64 \pm 0.33$                     & \quad \; $64.52 \pm 0.51$\\
    MH$^{\dagger}$~\cite{zhao2020meta}                             & \quad \; $49.41 \pm 0.96$                     & \quad \; $67.16 \pm 0.42$\\
    BOIL~\cite{oh2020boil}                             & \quad \; $49.61 \pm 0.16$                     & \quad \; $66.46 \pm 0.37$\\
    
    
    ARML~\cite{yao2020automated}                       & \quad \; $50.42 \pm 1.79$                     & \quad \; $64.12 \pm 0.90$\\
    ALFA~\cite{baik2020meta}                           & \quad \; $50.58 \pm 0.51$                     & \quad \; $69.12 \pm 0.47$\\
    
    
    L2F~\cite{baik2020learning}                        & \quad \; $52.10 \pm 0.50$                     & \quad \; $69.38 \pm 0.46$\\
    ModGrad$^{\dagger}$~\cite{simon2020modulating}                 & \quad \; $53.20 \pm 0.86$                     & \quad \; $69.17 \pm 0.69$\\
    PAMELA$^{\dagger}$~\cite{rajasegaran2020meta}                  & \quad \; $53.50 \pm 0.89$                     & \quad \; $70.51 \pm 0.67$\\
    SignMAML~\cite{fan2021sign}                        & \quad \; $42.90 \pm 1.50$                     & \quad \; $60.70 \pm 0.70$\\
    CTML~\cite{peng2021clustered}                      & \quad \; $50.47 \pm 1.83$                     & \quad \; $64.15 \pm 0.90$\\
    MeTAL~\cite{baik2021meta}                        & \quad \; $52.63 \pm 0.37$                     & \quad \; $70.52 \pm 0.29$\\
    ECML~\cite{hiller2022enforcing}                    & \quad \; $48.94 \pm 0.80$                     & \quad \; $65.26 \pm 0.67$\\
    Sharp-MAML\_{up}~\cite{abbas2022sharp}              & \quad \; $49.56\phantom{.} \pm $ N/A          & \quad \; $63.06\phantom{.} \pm $ N/A\\
    Sharp-MAML\_{low}~\cite{abbas2022sharp}             & \quad \; $49.72\phantom{.} \pm $ N/A          & \quad \; $63.18\phantom{.} \pm $ N/A\\
    Sharp-MAML\_{both}~\cite{abbas2022sharp}            & \quad \; $50.28\phantom{.} \pm $ N/A          & \quad \; $65.04\phantom{.} \pm $ N/A\\
    FBM~\cite{yang2022calibrating}                     & \quad \; $50.62 \pm 1.79$                     & \quad \; $64.78 \pm 0.35$\\
    CxGrad~\cite{lee2022contextual}                    & \quad \; $51.80 \pm 0.46$                     & \quad \; $69.82 \pm 0.42$\\
    HyperMAML~\cite{przewikezlikowski2022hypermaml}     & \quad \; $51.84 \pm 0.57$                     & \quad \; $66.29 \pm 0.43$\\
    
    
    
EEML~\cite{li2022eeml}                             & \quad \; $52.42 \pm 1.75$                     & \quad \; $68.40 \pm 0.95$\\
    
   
    
    MH-O$^{\dagger}$~\cite{zhao2020meta}                           & \quad \; $52.50 \pm 0.61$                     & \quad \; $67.76 \pm 0.34$\\
    Sparse-MAML$^{\dagger}$~\cite{von2021learning}                 & \quad \; $50.35 \pm 0.39$                     & \quad \; $67.03 \pm 0.74$\\
    Sparse-ReLU-MAML$^{\dagger}$~\cite{von2021learning}            & \quad \; $50.39 \pm 0.89$                     & \quad \; $64.84 \pm 0.46$\\
    Sparse-MAML+$^{\dagger}$~\cite{von2021learning}           & \quad \; $51.04 \pm 0.59$                     & \quad \; $67.03 \pm 0.74$\\
    \midrule
    Approximate GAP$^{\dagger}$                                    & \quad \; $53.52 \pm 0.88$                     & \quad \; $70.75 \pm 0.67$\\
    GAP$^{\dagger}$                                              & \quad \; $\mathbf{54.86} \pm \mathbf{0.85}$   & \quad \; $\mathbf{71.55}\pm\mathbf{0.61}$\\
    \bottomrule
  \end{tabular}}
  \label{tab:miniImageNet}
\end{table}

\begin{table}[t!]
    \caption{5-way few-shot classification accuracy (\%) on tiered-ImageNet dataset with a \textit{Conv-4} backbone. We report mean $\pm$ 95\% confidence intervals(ci) for 600 tasks according to \cite{finn2017model}. $^{\dagger}$ denotes PGD-MAML family.}
  \centering
  \resizebox{0.5\textwidth}{!}{\begin{tabular}{lccc}
    \toprule
    Algorithm                                       & \quad \; 5-way 1-shot                           & \quad \; 5-way 5-shot\\
    \midrule
    Meta-SGD$^{\dagger}$~\cite{li2017meta}                  & \quad \; $50.92 \pm 0.93$                     & \quad \; $69.28 \pm 0.80$\\
    MAML~\cite{finn2017model}                   & \quad \; $51.70 \pm 1.80$                     & \quad \; $70.30 \pm 1.80$\\
    MT-net~\cite{lee2018gradient}               & \quad \; $51.95 \pm 1.83$                     & \quad \quad N/A\\
    WarpGrad~\cite{flennerhag2019meta}           & \quad \; $57.20 \pm 0.90$                     & \quad \; $74.10 \pm 0.70$\\
    BOIL~\cite{oh2020boil}.                    & \quad \; $48.58 \pm 0.27$                     & \quad \; $69.37 \pm 0.12$\\
    ALFA~\cite{baik2020meta}                    & \quad \; $53.16 \pm 0.49$                     & \quad \; $70.54 \pm 0.46$\\
    L2F~\cite{baik2020learning}                 & \quad \; $54.40 \pm 0.50$                     & \quad \; $73.34 \pm 0.44$\\
    ARML~\cite{yao2020automated}                & \quad \; $52.91 \pm 1.83$                     & \quad \quad N/A\\
    PAMELA$^{\dagger}$~\cite{rajasegaran2020meta}           & \quad \; $54.81 \pm 0.88$                     & \quad \; $74.39 \pm 0.71$\\
    Sparse-ReLU-MAML$^{\dagger}$~\cite{von2021learning}      & \quad \; $53.18 \pm 0.52$                     & \quad \; $69.06 \pm 0.28$\\
    Sparse-MAML$^{\dagger}$~\cite{von2021learning}          & \quad \; $53.47 \pm 0.53$                     & \quad \; $68.83 \pm 0.65$\\
    Sparse-MAML+$^{\dagger}$~\cite{von2021learning}     & \quad \; $53.91 \pm 0.67$                     & \quad \; $69.92 \pm 0.21$\\
    MeTAL~\cite{baik2021meta}                   & \quad \; $54.34 \pm 0.31$                     & \quad \; $70.40 \pm 0.21$\\
    
    CxGrad~\cite{lee2022contextual}             & \quad \; $55.55 \pm 0.46$                     & \quad \; $73.55 \pm 0.41$\\
    ECML~\cite{hiller2022enforcing}             & \quad \; $47.34 \pm 0.88$                     & \quad \; $64.77 \pm 0.75$\\
    \midrule
   Approximate GAP$^{\dagger}$                                & \quad \; $56.86 \pm 0.91$                     & \quad \; $74.41 \pm 0.72$\\
   GAP$^{\dagger}$                                      & \quad \; $\mathbf{57.60} \pm \mathbf{0.93}$   & \quad \; $\mathbf{74.90} \pm \mathbf{0.68}$\\
    \bottomrule
  \end{tabular}}
  \label{tab:tiered-ImageNet}
\end{table}

\subsection{Cross-domain few-shot classification}
The cross-domain few-hot classification introduced by~\cite{chen2019closer} addresses a more challenging and practical few-shot classification scenario in which meta-train and meta-test tasks are sampled from different task distributions. These scenarios are designed to evaluate meta-level overfitting of meta-learning algorithms by creating a large domain gap between meta-trains and meta-tests. In particular, an algorithm can be said to be meta-overfitting if it relies too much on the prior knowledge of previously seen meta-train tasks instead of focusing on a few given examples to learn a new task. This meta-level overfitting makes the learning system more likely to fail to adapt to new tasks sampled from substantially different task distributions.

\subsubsection{Datasets and Experimental setup}
To evaluate the level of meta-overfitting for GAP, we evaluate a cross-domain few-shot classification experiment.
The mini-ImageNet is used for the meta-train task, and the tiered-ImageNet~\cite{ren2018meta}, CUB-200-2011~\cite{wah2011caltech}, Cars~\cite{bertinetto2018meta} datasets are used for the meta-test task. The CUB dataset has 200 fine-grained classes and consists of a total of 11,788 images; it is further divided into 100 meta-train classes, 50 meta-validation classes, and 50 meta-test classes. 
The Cars~\cite{krause20133d} dataset consists of 16,185 images of 196 classes of cars; it is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in 50-50. The classes are typically at the level of Make, Model, Year, e.g., 2012 Tesla Model S or 2012 BMW M3 coupe.
As with the few-shot classification experiment, we use the standard \textit{Conv-4} backbone and follow the same experimental protocol.

\subsubsection{Results.}
Table~\ref{tab:cross_domain} presents the cross-domain few-shot performance for GAP, MAML family, and PGD-MAML family. GAP significantly outperforms the state-of-the-art algorithms on 5-way 1-shot and 5-way 5-shot cross-domain classification tasks. In particular, for the tiered-ImageNet dataset, the performance was improved by 8.6\% and 4.1\% on 1-shot and 5-shot classification tasks, respectively. Because GAP can simultaneously consider a task's individuality and optimization trajectory in the inner-loop optimization, it can overcome meta-overfitting better than the existing methods. 
However, Approximate GAP shows more performance degradation in cross-domain few-shot classification than in few-shot classification. 
In particular, when the domain difference with the meta-train is more significant (i.e., the tiered-ImageNet dataset) than when the domain difference with the meta-train is marginal (i.e., CARS and CUB datasets), it shows a more considerable performance drop. 
We can see that full adaptation plays an important role in cross-domain few-shot classification. 

\begin{table*}[t!]
    \caption{5-way few-shot cross domain classification accuracy (\%) with a \textit{Conv-4} backbone, meta training on mini-ImageNet dataset, and meta-testing on tiered-ImageNet, CUB, or Cars datasets. We report mean $\pm$ 95\% confidence intervals(ci) for 600 tasks according to \cite{finn2017model}. $^{\dagger}$ denotes PGD-MAML family.
  }
  \centering
  \resizebox{1.\textwidth}{!}{
  \begin{tabular}{lcccccc}
    \toprule                                     
                       & \multicolumn{2}{c}{tiered-ImageNet} & \multicolumn{2}{c}{CUB} & \multicolumn{2}{c}{Cars} \\
                       \cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7}
    Algorithm             & 1-shot  & 5-shot & 1-shot  & 5-shot & 1-shot  & 5-shot \\
    \midrule
    MAML~\cite{finn2017model}               & $51.61\pm0.20$ & $65.76\pm0.27$   & $40.51\pm0.08$ & $53.09\pm0.16$ & $33.57\pm0.14$ & $44.56\pm0.21$ \\
    ANIL~\cite{raghu2019rapid}               & $52.82\pm0.29$ & $66.52\pm0.28$   & $41.12\pm0.15$ & $55.82\pm0.21$ & $34.77\pm0.31$ & $46.55\pm0.29$ \\
    BOIL~\cite{oh2020boil}               & $53.23\pm0.41$ & $69.37\pm0.23$   & $44.20\pm0.15$ & $60.92\pm0.11$ & $36.12\pm0.29$ & $50.64\pm0.22$\\
    BMAML~\cite{yoon2018bayesian}              & N/A              & N/A            & $33.52\pm0.36$ & $51.35\pm0.16$ & N/A            & N/A \\
    ALFA~\cite{baik2020meta}               & N/A              & N/A            & N/A             & $58.35\pm0.25$ & N/A           & N/A \\
    L2F~\cite{baik2020learning}                & N/A              & N/A            & N/A             & $60.89\pm0.22$ & N/A           & N/A \\
    MeTAL~\cite{baik2021meta}              & N/A              & N/A            & N/A             & $58.20\pm0.24$ & N/A           & N/A \\
    HyperMAML~\cite{przewikezlikowski2022hypermaml}          & N/A              & N/A            & $36.52\pm0.61$ & $49.43\pm0.14$ & N/A            & N/A \\
    CxGrad~\cite{lee2022contextual}             & N/A              & N/A            & N/A             & $63.92\pm0.44$ & N/A           & N/A \\
    Sparse-MAML$^{\dagger}$~\cite{von2021learning}         & $53.47\pm0.53$ & $68.83\pm0.65$   & $41.37\pm0.73$ & $60.58\pm1.10$ & $35.90\pm0.50$ & $52.63\pm0.56$ \\
    Sparse-ReLU-MAML$^{\dagger}$~\cite{von2021learning}    & $53.77\pm0.94$ & $68.12\pm0.69$   & $42.89\pm0.45$ & $57.53\pm0.94$ & $36.04\pm0.55$ & $49.95\pm0.42$ \\
    Sparse-MAML+$^{\dagger}$~\cite{von2021learning}        & $53.91\pm0.67$ & $69.92\pm0.21$   & $43.43\pm1.04$ & $62.02\pm0.78$ & $37.14\pm0.77$ & $53.18\pm0.44$ \\
    \midrule
    Approximate GAP$^{\dagger}$   & $57.47 \pm 0.99$ & $71.66 \pm 0.76$ & $43.77 \pm 0.79$ & $62.92 \pm 0.73$ & $37.00 \pm 0.75$ & $53.28 \pm 0.76$\\
    GAP$^{\dagger}$             & $\mathbf{58.56\pm0.93}$ & $\mathbf{72.82\pm0.77}$   & $\mathbf{44.74\pm0.75}$ & $\mathbf{64.88\pm0.72}$ & $\mathbf{38.44\pm0.77}$ & $\mathbf{55.04\pm0.77}$ \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:cross_domain}
\end{table*}

\subsection{Few-shot domain generalization}
\subsubsection{Datasets and Experimental setup}
We use the Meta-Dataset~\cite{triantafillou2019meta} which is the standard benchmark for the few-shot domain generalization. It is a large-scale benchmark that has been widely used in recent years for few-shot domain generalization through multiple domains. It contains a total of ten diverse datasets: ImageNet~\cite{russakovsky2015imagenet}, Omniglot~\cite{lake2015human}, FGVC-Aircraft~(Aircraft)~\cite{maji2013fine}, CUB-200-2011~(Birds)~\cite{wah2011caltech}, Describable Textures~(DTD)~\cite{cimpoi2014describing}, QuickDraw~\cite{jongejan2016quick}, FGVCx Fungi~(Fungi)~\cite{schroeder2018fgvcx}, VGG Flower~(Flower)~\cite{nilsback2008automated}, Traffic Signs~\cite{houben2013detection} and MSCOCO~\cite{lin2014microsoft}.
Following the previous works~\cite{requeima2019fast, bateni2020improved, li2021universal, triantafillou2021learning, li2022cross}, we also add three additional datasets including MNIST~\cite{lecun1998gradient}, CIFAR10~\cite{krizhevsky2009learning} and CIFAR100~\cite{krizhevsky2009learning}.
We follow the standard training procedure in~\cite{triantafillou2019meta} and consider both the `Training on all datasets'~(multi-domain learning) and `Training on ImageNet only'~(single-domain learning) settings. 
In ‘Training on all datasets’ setting, we follow the standard procedure and use the first eight datasets for meta-training, in which each dataset is further divided into train, validation and test set with disjoint classes.
While the evaluation within these datasets is used to measure the generalization ability in the seen domains, the remaining five datasets are reserved as unseen domains in meta-test for measuring the cross-domain generalization ability. In `Training on ImageNet only' setting, we follow the standard procedure and only use train split of ImageNet for meta-training. The evaluation of models is performed using the test split of ImageNet and the other 12 datasets. For all the experiments, we adopt \textit{ResNet-18}~\cite{he2016deep} as the general feature extractor following the previous few-shot domain generalization works~\cite{requeima2019fast, bateni2020improved, li2021universal, triantafillou2021learning, li2022cross}. We apply GAP to fo-MAML~\cite{finn2017model} and Proto-MAML~\cite{triantafillou2019meta}. Following the experimental protocol in~\cite{triantafillou2019meta}, we use 600 randomly sampled tasks for each dataset with varying number of ways and shots. In meta training and meta-testing, the inner-loop optimization is updated in ten and forty steps, respectively.

\subsubsection{Results}
For Approximate GAP, GAP, and MAML family, Table~\ref{tab:img_metadataset}~\&~\ref{tab:all_metadataset} present the performance of models trained on ImageNet only and trained on all dataset, respectively. 
The results demonstrate that GAP can consistently outperform fo-MAML~(first-order MAML) and Proto-MAML, which is a MAML variant proposed by~\cite{triantafillou2019meta} that substantially improves the MAML initialization at fc-layer using class prototypes. 
For models trained on ImageNet, GAP demonstrates that the performance of fo-MAML and Proto-MAML can be improved by $10.82\%$ and $7.13\%$ on the seen domains, and by $11.48\%$ and $7.85\%$ on the unseen domains without the three datasets~(MNIST, CIFAR-10, and CIFAR-100).
For models trained on all datasets, the performance of fo-MAML and Proto-MAML improved by $7.58\%$ and $2.87\%$ on the seen domains, and by $19.65\%$ and $10.31\%$ on the unseen domains when excluding the three datasets. 
Additionally, GAP+fo-MAML and GAP+Proto-MAML significantly outperform ALFA+fo-MAML and ALFA+Proto-MAML, which are known as part of the state-of-the-art MAML family, on both seen and unseen domains. 
An interesting aspect of these results is that Approximate GAP and GAP exhibit similar performance despite the domain differences, unlike in cross-domain few-shot classification. 
This similarity in performance can be attributed to Approximate GAP approximating GAP as the architecture size increases.

\begin{table*}[t!]
    \caption{
    Few-shot domain generalization for the Meta-Dataset benchmark with a ResNet-18 backbone trained on \textit{ImageNet only}. 
    The ImageNet is seen during meta-training and the other twelve datasets are unseen and used for test only. 
    We report the three average accuracy as follows: seen domains, unseen domains, all domains. 
    To ensure a fair comparison, we report the average accuracy for unseen domains and all domains under two settings: one including MNIST, CIFAR-10, and CIFAR-100 datasets and the other without them.}
  \centering
  \resizebox{1.\textwidth}{!}{\begin{tabular}{lccccccccccccc}
    \toprule
    Datasets &\quad\; fo-MAML &\quad\; Proto-MAML &\quad\; \begin{tabular}{@{}c@{}}ALFA\\+fo-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}ALFA\\+Proto-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}Approximate GAP\\+fo-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}Approximate GAP\\+Proto-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}GAP\\+fo-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}GAP\\+Proto-MAML\end{tabular}\\
    \midrule
    ImageNet &\quad\; $45.51$ &\quad\; $49.53$ &\quad\; $51.09$ &\quad\; $52.80$ &\quad\; $55.34$ &\quad\; $56.65$ &\quad\; $56.33$ &\quad\; $\mathbf{56.66}$\\
    \midrule
    Omniglot &\quad\; $55.55$ &\quad\; $63.37$ &\quad\; $67.89$ &\quad\; $61.87$ &\quad\; $75.17$ &\quad\; $76.30$ &\quad\; $77.04$ &\quad\; $\mathbf{77.57}$\\
    Aircraft &\quad\; $56.24$ &\quad\; $55.95$ &\quad\; $66.34$ &\quad\; $63.43$ &\quad\; $67.40$ &\quad\; $67.61$ &\quad\; $68.03$ &\quad\; $\mathbf{68.50}$\\
    Birds &\quad\; $63.61$ &\quad\; $68.66$ &\quad\; $67.67$ &\quad\; $69.75$ &\quad\; $72.35$ &\quad\; $73.29$ &\quad\; $72.01$ &\quad\; $\mathbf{73.51}$\\
    Textures &\quad\; $68.04$ &\quad\; $66.49$ &\quad\; $65.34$ &\quad\; $70.78$ &\quad\; $71.12$ &\quad\; $71.01$ &\quad\; $\mathbf{71.82}$ &\quad\; $71.42$\\
    Quick Draw &\quad\; $43.96$ &\quad\; $51.52$ &\quad\; $60.53$ &\quad\; $59.17$ &\quad\; $63.49$ &\quad\; $63.71$ &\quad\; $64.36$ &\quad\; $\mathbf{65.36}$\\
    Fungi &\quad\; $32.10$ &\quad\; $39.96$ &\quad\; $37.41$ &\quad\; $41.49$ &\quad\; $37.19$ &\quad\; $38.30$ &\quad\; $37.46$ &\quad\; $\mathbf{38.57}$\\
    VGG Flower &\quad\; $81.74$ &\quad\; $87.15$ &\quad\; $84.28$ &\quad\; $85.96$ &\quad\; $84.44$ &\quad\; $86.73$ &\quad\; $85.31$ &\quad\; $\mathbf{86.79}$\\
    Traffic Sign &\quad\; $50.93$ &\quad\; $48.83$ &\quad\; $60.86$ &\quad\; $60.78$ &\quad\; $66.82$ &\quad\; $66.44$ &\quad\; $67.89$ &\quad\; $\mathbf{66.90}$\\
    MSCOCO &\quad\; $35.30$ &\quad\; $43.74$ &\quad\; $40.05$ &\quad\; $48.11$ &\quad\; $45.99$ &\quad\; $46.54$ &\quad\; $\mathbf{46.87}$ &\quad\; $46.76$\\
    MNIST$^{\star}$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $93.46$ &\quad\; $92.27$ &\quad\; $93.53$ &\quad\; $\mathbf{93.97}$\\
    CIFAR-10$^{\star}$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $75.08$ &\quad\; $74.05$ &\quad\; $\mathbf{75.96}$ &\quad\; $74.54$\\
    CIFAR-100$^{\star}$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $65.65$ &\quad\; $62.66$ &\quad\; $\mathbf{65.94}$ &\quad\; $63.23$\\
    \midrule
    Average seen &\quad\; $45.51$ &\quad\; $49.53$ &\quad\; $51.09$ &\quad\; $52.80$ &\quad\; $55.34$ &\quad\; $56.65$ &\quad\; $56.33$ &\quad\; $\mathbf{56.66}$\\
    \midrule
    Average unseen w/o $\star$ &\quad\; $54.16$ &\quad\; $58.41$ &\quad\; $61.15$ &\quad\; $62.37$ &\quad\; $64.89$ &\quad\; $65.65$ &\quad\; $65.64$ &\quad\; $\mathbf{66.26}$\\
    Average all w/o $\star$ &\quad\; $53.30$ &\quad\; $57.52$ &\quad\; $60.15$ &\quad\; $61.41$ &\quad\; $63.93$ &\quad\; $64.66$ &\quad\; $64.71$ &\quad\; $\mathbf{65.30}$\\
    \midrule
    Average unseen w/ $\star$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $69.56$ &\quad\; $68.39$ &\quad\; $\mathbf{70.04}$ &\quad\; $69.28$\\
    Average all w/ $\star$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $67.25$ &\quad\; $67.35$ &\quad\; $67.89$ &\quad\; $\mathbf{68.06}$\\
    \bottomrule
  \end{tabular}
  }
  \label{tab:img_metadataset}
\end{table*}

\begin{table*}[t!]
    \caption{
    Few-shot domain generalization for the Meta-Dataset benchmark with a ResNet-18 backbone trained on \textit{eight datasets}. 
    The first eight datasets are seen during meta-training and the other five datasets are unseen and used for test only. We report the three average accuracy as follows: seen domains, unseen domains, all domains. 
    To ensure a fair comparison, we report the average accuracy for unseen domains and all domains under two settings: one including MNIST, CIFAR-10, and CIFAR-100 datasets and the other without them.}
  \centering
  \resizebox{1.\textwidth}{!}{\begin{tabular}{lccccccccccccc}
    \toprule
    Datasets &\quad\; fo-MAML &\quad\; Proto-MAML &\quad\; \begin{tabular}{@{}c@{}}ALFA\\+fo-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}ALFA\\+Proto-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}Approximate GAP\\+fo-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}Approximate GAP\\+Proto-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}GAP\\+fo-MAML\end{tabular} &\quad\; \begin{tabular}{@{}c@{}}GAP\\+Proto-MAML\end{tabular}\\
    \midrule
    ImageNet &\quad\; $37.83$ &\quad\; $46.52$ &\quad\; $48.88$ &\quad\; $49.87$ &\quad\; $51.57$ &\quad\; $53.04$ &\quad\; $51.72$ &\quad\; $\mathbf{53.54}$\\
    Omniglot &\quad\; $83.92$ &\quad\; $82.69$ &\quad\; $76.67$ &\quad\; $78.40$ &\quad\; $88.13$ &\quad\; $85.82$ &\quad\; $\mathbf{88.29}$ &\quad\; $86.54$\\
    Aircraft &\quad\; $76.41$ &\quad\; $75.23$ &\quad\; $68.82$ &\quad\; $71.88$ &\quad\; $81.05$ &\quad\; $79.30$ &\quad\; $\mathbf{81.52}$ &\quad\; $78.59$\\
    Birds &\quad\; $62.43$ &\quad\; $69.88$ &\quad\; $66.32$ &\quad\; $68.57$ &\quad\; $73.85$ &\quad\; $74.06$ &\quad\; $74.16$ &\quad\; $\mathbf{74.57}$\\
    Textures &\quad\; $64.14$ &\quad\; $68.25$ &\quad\; $68.72$ &\quad\; $70.23$ &\quad\; $70.85$ &\quad\; $68.12$ &\quad\; $\mathbf{70.92}$ &\quad\; $68.73$\\
    Quick Draw &\quad\; $59.73$ &\quad\; $66.84$ &\quad\; $66.07$ &\quad\; $63.72$ &\quad\; $67.26$ &\quad\; $68.92$ &\quad\; $67.95$ &\quad\; $\mathbf{69.67}$\\
    Fungi &\quad\; $33.54$ &\quad\; $41.99$ &\quad\; $37.52$ &\quad\; $43.76$ &\quad\; $38.78$ &\quad\; $42.22$ &\quad\; $38.88$ &\quad\; $\mathbf{43.61}$\\
    VGG Flower &\quad\; $79.94$ &\quad\; $88.72$ &\quad\; $86.79$ &\quad\; $89.09$ &\quad\; $84.48$ &\quad\; $87.42$ &\quad\; $85.10$ &\quad\; $\mathbf{87.90}$\\
    \midrule
    Traffic Sign &\quad\; $42.91$ &\quad\; $52.42$ &\quad\; $65.13$ &\quad\; $58.46$ &\quad\; $67.72$ &\quad\; $68.12$ &\quad\; $68.32$ &\quad\; $\mathbf{68.73}$\\
    MSCOCO &\quad\; $29.37$ &\quad\; $41.74$ &\quad\; $43.05$ &\quad\; $46.17$ &\quad\; $43.26$ &\quad\; $45.76$ &\quad\; $43.26$ &\quad\; $\mathbf{46.04}$\\
    MNIST$^{\star}$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $95.74$ &\quad\; $94.73$ &\quad\; $\mathbf{95.92}$ &\quad\; $94.86$\\
    CIFAR-10$^{\star}$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $73.04$ &\quad\; $72.62$ &\quad\; $\mathbf{73.50}$ &\quad\; $73.25$\\
    CIFAR-100$^{\star}$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $60.72$ &\quad\; $59.00$ &\quad\; $\mathbf{61.29}$ &\quad\; $60.72$\\
    \midrule
    Average seen &\quad\; $62.24$ &\quad\; $67.52$ &\quad\; $64.97$ &\quad\; $66.94$ &\quad\; $69.50$ &\quad\; $69.86$ &\quad\; $69.82$ &\quad\; $\mathbf{70.39}$\\
    \midrule
    Average unseen w/o $\star$ &\quad\; $36.14$ &\quad\; $47.08$ &\quad\; $54.09$ &\quad\; $52.32$ &\quad\; $55.49$ &\quad\; $56.94$ &\quad\; $55.79$ &\quad\; $\mathbf{57.39}$\\
    Average all w/o $\star$ &\quad\; $57.02$ &\quad\; $63.43$ &\quad\; $62.80$ &\quad\; $64.02$ &\quad\; $66.70$ &\quad\; $66.77$ &\quad\; $67.01$ &\quad\; $\mathbf{67.79}$\\
    \midrule
    Average unseen w/ $\star$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $68.10$ &\quad\; $68.05$ &\quad\; $\mathbf{68.46}$ &\quad\; $68.39$\\
    Average all w/ $\star$ &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; N/A &\quad\; $68.96$ &\quad\; $69.16$ &\quad\; $69.29$ &\quad\; $\mathbf{69.62}$\\
    \bottomrule
  \end{tabular}
  }
  \label{tab:all_metadataset}
\end{table*}


\subsection{Reinforcement Learning}
\subsubsection{Datasets and Experimental setup}
For the reinforcement learning, we evaluate GAP on two benchmarks: Half-cheetah locomotion~\cite{todorov2012physics} and 2D-Navigation~\cite{finn2017model}. 
The first benchmark, half-cheetah locomotion task, aims to predict direction and velocity. 
In the goal velocity experiments, the reward is the negative absolute value between the current velocity of the agent and a goal. The goal is randomly chosen from a uniform distribution ranging between $0.0$ and $2.0$. 
In the goal direction experiments, the reward is determined based on the magnitude of the velocity in either the forward or backward direction. The direction is randomly chosen for each task. 
Each task consists of rollouts with a length of $200$, and during training, $20$ rollouts are utilized per gradient step. 
The second benchmark, 2D Navigation task, aims to enable a point agent in a 2D environment to quickly learn a policy for moving from a starting position to a goal position. 
The observation consists of the current 2D position and the actions correspond to velocity commands clipped to be in the range of $[-0.1, 0.1]$. 
A goal position is randomly selected within the unit square $[-0.5, 0.5] \times [-0.5, 0.5]$ for each task. 
The reward is calculated as the negative squared distance to the goal. 
In total, $20$ trajectories are used for one gradient update. 
For all the experiments, our model follows a neural network policy used in~\cite{finn2017model}. The details of the architecture are provided in Section~\ref{sec:5.1.2}. 
Following the experimental protocol in~\cite{finn2017model}, we employ Trust-Region Policy Optimization~(TRPO) as the meta-optimizer~\cite{schulman2015trust}, compute the Hessian-vector products for TRPO using finite differences, and utilize the standard linear feature baseline~\cite{duan2016benchmarking}. 
The feature baseline is fitted separately at each iteration for every sampled task in the batch.

\subsubsection{Results}
The results in Fig.~\ref{fig:rl} show the average reward with respect to the update steps for MAML~\cite{finn2017model}, CAVIA~\cite{zintgraf2019fast}, ModGrad~\cite{simon2020modulating}, and GAP on Half-cheetah locomotion and 2D-Navigation.
These results demonstrate that GAP significantly outperforms the state-of-the-art algorithms even after a single gradient update step. Furthermore, GAP continues to improve with additional update steps across the three benchmarks. 
In Half-cheetah direction tasks~(Fig.~\ref{fig:rl_a}), GAP achieves rewards exceeding 600 with only one step, while MAML, CAVIA, and ModGrad fall short, reaching rewards below 600. 
Additionally, for Half-cheetah velocity tasks~(Fig.~\ref{fig:rl_b}), GAP attains rewards surpassing $-80$ with a single step, whereas MAML, CAVIA, and ModGrad only reach around $-80$, $-90$, and $-100$, respectively. 
For 2D Navigation tasks, GAP consistently achieves larger rewards than MAML, CAVIA, and ModGrad with just one step, as illustrated in Fig.~\ref{fig:rl_c}. 
Across all reinforcement learning tasks, GAP achieves larger rewards than the other methods as the number of updates increases.

\begin{figure*}[!t]
\centering
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{Figures/Half-Cheetah_Dir.png}
    \caption{Half-Cheetah Direction}
    \label{fig:rl_a}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{Figures/Half-Cheetah_Vel.png}
    \caption{Half-Cheetah Velocity}
    \label{fig:rl_b}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{Figures/2D_Navigation.png}
    \caption{2D Navigation}
    \label{fig:rl_c}
\end{subfigure}
\caption{The average reward performance of MAML family and GAP models for reinforcement learning on half-cheetah direction, half-cheetah velocity, and 2D navigation. We report the performance as the number of gradient updates increases.}
\label{fig:rl}
\end{figure*}