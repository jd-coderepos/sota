\documentclass{sig-alternate}

\DeclareMathAlphabet{\mathitbf}{OML}{cmm}{b}{it}

\begin{document}

\title{Building a Balanced k-d Tree in O(kn log n) Time} 



\author{Russell A. Brown}

\date{01 April 2015}
\maketitle
\begin{abstract}

The original description of the \emph{k}-d tree recognized that rebalancing techniques, such as are used to build an AVL tree or a red-black tree, are not applicable to a \emph{k}-d tree.  Hence, in order to build a balanced \emph{k}-d tree, it is necessary to find the median of the data for each recursive subdivision of those data.  The sort or selection that is used to find the median for each subdivision strongly influences the computational complexity of building a \emph{k}-d tree.

This paper discusses an alternative algorithm that builds a balanced \emph{k}-d tree by presorting the data in each of  dimensions prior to building the tree.  It then preserves the order of these  sorts during tree construction and thereby avoids the requirement for any further sorting.  Moreover, this algorithm is amenable to parallel execution via multiple threads.  Compared to an algorithm that finds the median for each recursive subdivision, this presorting algorithm has equivalent performance for four dimensions and better performance for three or fewer dimensions.

\end{abstract}







\section{Introduction} 
\label{sec:introduction}

Bentley introduced the \emph{k}-d tree as a binary tree that stores \emph{k}-dimensional data \cite{Bentley}.  Like a standard binary tree, the \emph{k}-d tree subdivides data at each recursive level of the tree.  Unlike a standard binary tree that uses only one key for all levels of the tree, the \emph{k}-d tree uses  keys and cycles through these keys for successive levels of the tree.  For example, to build a \emph{k}-d tree from three-dimensional points that comprise  coordinates, the keys would be cycled as   for successive levels of the \emph{k}-d tree.  A more elaborate scheme for cycling the keys chooses the coordinate that has the widest dispersion or largest variance to be the key for a particular level of recursion \cite{Friedman}.

Due to the use of different keys at successive levels of the tree, it is not possible to employ rebalancing techniques, such as are used to build an AVL tree \cite{Adelson} or a red-black tree \cite{Bayer,Guibas}, when building a \emph{k}-d tree.  Hence, the typical approach to building a balanced \emph{k}-d tree finds the median of the data for each recursive subdivision of the data.  Bentley showed that if the median of  elements were found in  time, it would be possible to build a depth-balanced \emph{k}-d tree in  time.  Blum \emph{et al.} proposed an elegant but slightly complicated algorithm that finds the median in guaranteed  time \cite{Blum}.  Quicksort \cite{Hoare} finds the median in  time in the best case but in  time in the worst case \cite{Wirth}. Merge sort \cite{Neumann} and heap sort \cite{Williams} find the median in guaranteed  time, which leads to  time for building a balanced \emph{k}-d tree \cite{Wald}.

An alternative approach to building a balanced \emph{k}-d tree presorts the data prior to building the tree and then avoids resorting for each recursive subdivision.  Two algorithms have been reported that sort triangles for three-dimensional graphics ray tracing and that have best-case complexity of  but undetermined worst-case complexity \cite{Havran,Wald}.  The algorithm described in the present article presorts points in each of  dimensions prior to building the \emph{k}-d tree, then maintains the order of the  sorts when building a balanced \emph{k}-d tree, and thereby achieves a worst-case complexity of .  Procopiuc \emph{et al.} have outlined an algorithm \cite{Procopiuc} that appears to be similar to the algorithm described in the present article.


\section{Implementation}
\subsection{The  Algorithm}
\label{sec:knlogn_algorithm}

Consider the 15  tuples that are stored in elements 0 through 14 of the ``Tuples" array that is shown at the left side of Figure \ref{fig:IndexArrays}.  The \emph{k}-d tree-building algorithm begins by presorting the tuples in their -, - and -coordinates via three executions of merge sort.  These three sorts do not in fact sort the -, - and -coordinates by using these coordinates as sort keys entirely independently of one another; instead, ,  and  form the most significant portions of the respective super keys ::, :: and :: that represent cyclic permutations of ,  and .  The symbols for these super keys use a colon to designate the concatenation of the individual ,  and  values.  Hence, for example, the symbol :: represents a super key wherein  is the most significant portion of the super key,  is the middle portion of the super key, and  is the least significant portion of the super key.

The merge sorts employ super keys, instead of keys that are merely the individual -, - and -coordinates, in order to detect and remove duplicate tuples, as will be explained later.  The merge sorts do not reorder the Tuples array; rather, they reorder three index arrays whose elements are indices into the Tuples array.  The initial order of these index arrays is established by the merge sorts and is shown in Figure \ref{fig:IndexArrays} in the ,  and  columns under ``Initial Indices".  In this figure, ,  and  are shorthand notations for the super keys ::, :: and :: respectively. 

The ,  and  columns under ``Initial Indices" represent the initial order of the -, - and -index arrays that indicate the results of the three merge sorts.  For example, elements 0, 1, ... 13, 14 of the -index array contain the sequence 11, 13, ... 2, 8 that represents the respective tuples ; ; ... ;  that were ordered via merge sort using the respective super keys 1:6:8, 2:1:3, ... 9:6:7, 9:7:8.  Similarly, elements 0, 1, ... 13, 14 of the -index array contain the sequence 13, 4, ... 8, 3 that represents the respective tuples ; ; ... ;  that were ordered via merge sort using the respective super keys 1:3:2, 1:5:8, ... 7:8:9, 7:9:4.  Lastly, elements 0, 1, ... 13, 14 of the -index array contain the sequence 9, 6, ... 8, 3 that represents the respective tuples ; ; ... ;  that were ordered via merge sort using the respective super keys 1:6:3, 1:9:4, ... 8:9:7, 9:4:7.

\begin{figure}[h]
\centering
\centerline{\includegraphics[width=3.4in]{figures/BuildingKdTreeFigure1.pdf}}
\caption{An  tuple array and -, - and -index arrays.}
\label{fig:IndexArrays}
\end{figure}

The next step of the \emph{k}-d tree-building algorithm partitions the  tuples in  using the :: super key that is specified by the median element of the -index array under ``Initial Indices".  This median element is located at address 7 of this array; its value is 5 and specifies the tuple  for which the :: super key is 7:2:6.  The partitioning does not reorder the Tuples array; instead, it reorders the - and -index arrays.  The -index array requires no partitioning because it is already sorted in .  However, the - and -index arrays require partitioning in  using the :: super key 7:2:6 that is specified by the median element of the -index array.

This partitioning is accomplished for the -index array as follows.  The elements of the -index array are retrieved in order of increasing address from 0 to 14.  The :: super key that is specified by each element of the -index array is compared to the 7:2:6 super key that is specified by the median element of the -index array.  Each element of the -index array is copied to either the lower or upper half of a temporary index array, depending on the result of this comparison.  After all of the elements of the -index array have been processed in this manner, the temporary index array replaces the -index array and becomes the new -index array that is depicted in Figure \ref{fig:IndexArrays} under ``After First Split."  The partitioning of the first six elements of the -index array is discussed below and provides insight into the details of the \emph{k}-d tree-building algorithm.

The element at address 0 of the -index array is 13 and specifies the tuple  for which the :: super key is 2:1:3.  This super key is less than the median super key 7:2:6; hence, the element at address 0 of the -index array is copied to address 0 in the new -index array, which is the lowest address in the lower half of the new -index array.  The element at address 1 of the -index array is 4 and specifies the tuple  for which the :: super key is 8:1:5.  This super key is greater than the median super key 7:2:6; hence, the element at address 1 of the -index array is copied to address 8 in the upper half of the new -index array, which is the lowest address in the upper half of the new -index array.  The element at address 2 of the -index array is 5 and specifies the tuple  for which the :: super key is 7:2:6.  This super key equals the median super key 7:2:6; hence, the element at address 2 in the -index array is ignored and not copied to the new -index array.

The element at address 3 of the -index array is 9 and specifies the tuple  for which the :: super key is 6:3:1.  This super key is less than the median super key 7:2:6; hence, the element at address 3 of the -index array is copied to address 1 in the lower half of the new -index array, which is the second lowest address in the lower half of the new -index array.  The element at address 4 of the -index array is 0 and specifies the tuple  for which the :: super key is 2:3:3.  This super key is less than the median super key 7:2:6; hence, the element at address 4 of the -index array is copied to address 2 in the lower half of the new -index array, which is the third lowest address in the lower half of the new -index array.  The element at address 5 of the -index array is 6 and specifies the tuple  for which the :: super key is 9:4:1.  This super key is greater than the median super key 7:2:6; hence, the element at address 5 of the -index array is copied to address 9 in the upper half of the new -index array, which is the second lowest address in the upper half of the new -index array.

Partitioning continues for the remaining eight elements of the -index array in the manner that is described above.  The partitioning of the first six elements of the -index array reveals that the -index array has been partitioned in  relative to the median element of the -index array; this partitioning preserves the initial merge-sorted order in :: within the lower and upper halves of the new -index array.

Next, the -index array is partitioned in  relative to the median element of the -index array, which preserves the initial merge-sorted order in :: for the lower and upper halves of the new -index array.  The reader is encouraged to audit the partitioning of the first few elements of the -index array under ``Initial Indices" in order to verify that these elements are correctly assigned to the lower and upper halves of the new -index array that is shown in Figure \ref{fig:IndexArrays} under ``After First Split."  Because the partitioning in  preserves the initial merge-sorted order for the lower and upper halves of the - and -index arrays, there is no requirement for any further sorting after the  initial merge sorts.

The lower and upper halves of the new -, - and -index arrays in Figure \ref{fig:IndexArrays} under ``After First Split" reveal that the index value 5 is absent from the lower and upper halves of these index arrays.  This value is absent from these index arrays because it is the value of the median element of the -index array that specified the :: super key 7:2:6 relative to which the - and -index arrays were partitioned in .  In order to record this partitioning, a reference to the tuple  is stored in the root of the nascent \emph{k}-d tree, as shown in Figure \ref{fig:FinalTree}.

\begin{figure}[h]
\centering
\centerline{\includegraphics[width=3.4in]{figures/BuildingKdTreeFigure2.pdf}}
\caption{A \emph{k}-d tree that is built from the  tuples of Figure  \ref{fig:IndexArrays}.}
\label{fig:FinalTree}
\end{figure}

Next, the lower and upper halves of the -, - and -index arrays are processed recursively and partitioned in  to create the ``less than" and ``greater than" subtrees of the root of the \emph{k}-d tree.  Consider the lower half of the -index array that is depicted in Figure \ref{fig:IndexArrays} under ``After First Split."  The median element of this array is located at address 3; its value is 1 and specifies the tuple  for which the :: super key is 4:2:5.  The lower half of the -index array is already sorted in ::.  However, the lower halves of the - and -index arrays require partitioning in  relative to the :: super key 4:2:5 that is specified by the median element of the lower half of the -index array.  The reader is encouraged to verify the result of this partitioning by inspection of the first and second fourths of the new -, - and -index arrays that are depicted in Figure \ref{fig:IndexArrays} under ``After Second Split."  The upper halves of the - and -index arrays are partitioned in a similar manner relative to the :: super key 5:3:9 that is formed from the tuple  that is specified by the value 12 of the median element at address 11 of the upper half of the -index array.  References to the tuples  and  are stored in the ``less than" and ``greater than" children of the root of the nascent \emph{k}-d tree, as shown in Figure \ref{fig:FinalTree}.

Recursion terminates when an index array comprises one, two or three elements.  In the case of one element, a reference to the corresponding tuple is stored in a new node of the \emph{k}-d tree.  For two or three elements, the elements are already in sorted order in the index array, so the determination of which tuple to reference from a new node of the \emph{k}-d tree and which tuple or tuples to reference from children of that node is trivial.  For example, consider the four fourths of the -index arrays under ``After Second Split" in Figure \ref{fig:IndexArrays}.  Each fourth comprises three elements, so recursion terminates.  The tuples  and  that are specified respectively by the median elements 13 and 11 at addresses 1 and 5 of the -index array are referenced by the respective ``less than" and ``greater than" children of node  of the nascent \emph{k}-d tree.  Similarly, the tuples  and  that are specified respectively by the median elements 7 and 2 at addresses 9 and 13 of the -index array are referenced by the respective ``less than" and ``greater than" children of node  of the nascent \emph{k}-d tree.  The children and grandchildren of nodes  and  are shown in Figure \ref{fig:FinalTree}.

The foregoing discussion reveals that the \emph{k}-d tree includes ``less than" and ``greater than" children but no ``equal" children.  For this reason, duplicate  tuples must be removed from the data prior to building the \emph{k}-d tree.  After the  initial merge sorts have reordered the -, - and -index arrays, each index array is traversed once in order to discard all but one index from each set of contiguous indices that reference identical  tuples.  In order that adjacent indices reference identical  tuples, the  initial merge sorts employ ::, :: and :: super keys instead of keys that are merely the individual -, - and -coordinates.  If the  initial merge sorts employed keys that were only the individual -, - and -coordinates, adjacent indices within an index array could reference non-identical  tuples for which one or more, but not all, of the -, - and -coordinates were identical.  The ::, :: and :: super keys guarantee that each group of identical  tuples is indexed by a set of contiguous indices within each index array.  These super keys enable the removal of duplicate  tuples via one pass through each index array that discards adjacent indices that reference identical  tuples.

It is possible to optimize the use of the temporary index array such that only one temporary index array is required and such that the -, - and -index arrays may be reused to avoid allocation of new index arrays at each level of recursion.  This optimization operates as follows.  The -index array is copied to the temporary index array.  Then the -index array is partitioned in  and the result is stored in the two halves of the -index array.  Next, the -index array is partitioned in  and the result is stored in the two halves of the -index array.  Finally, the temporary index array is copied to the -index array.  This optimization permutes the -, - and -index arrays cyclically at each level of recursion, as is required to cycle the keys in the order  for successive levels of the \emph{k}-d tree.  Moreover, it guarantees that the ::, :: or :: super key that is required for partitioning at a particular level of the \emph{k}-d tree is always specified by the median element of the -index array.  The additional computational cost of this index array optimization is the copying of one index array at each level of recursion. (The Appendix describes a more sophisticated approach to permuting the index arrays.)

Recursive partitioning occurs for  levels of the nascent \emph{k}-d tree.  The computational complexity of this \emph{k}-d tree-building algorithm includes a  term for the  initial merge sorts plus a  term for partitioning or copying  elements of  index arrays at each of the  levels of recursion.  This  \emph{k}-d tree-building algorithm requires storage for a Tuples array of  \emph{k}-dimensional tuples, plus an -element temporary array, plus  -element index arrays.  The Tuples array is immutable.  The index and temporary arrays are ephemeral and are no longer required after construction of the \emph{k}-d tree.

\subsection{Parallel Execution}
\label{sec:parallel_execution}

The merge-sorting function \cite{Sedgewick} and the  \emph{k}-d tree-building function both subdivide index arrays and process non-overlapping halves of each index array via recursive calls to these functions.  Hence, these functions (or Java methods) are amenable to parallel execution via multiple threads that occurs as follows.

One thread executes a recursive call of the method; this thread is designated as the parent thread.  The parent thread subdivides one or more index arrays, then calls the method recursively to process the lower and upper halves of each index array.  The parent thread does not execute the recursive call that processes the lower half of each index array; instead, it launches a child thread to execute that recursive call.  After launching the child thread, the parent thread executes the recursive call that processes the upper half of each index array, then waits for the child thread to finish execution of the recursive call that processes the lower half of each index array.

For a balanced \emph{k}-d tree, the number of threads  that are required by this thread-allocation strategy is  where  represents the deepest level of recursion and  represents the number of tuples.  A large number of tuples would require a prohibitively large number of threads; hence, child threads are launched to only the maximum level of recursion that is allowed by the number of available threads.  Beyond this maximum level of recursion, the current thread processes both halves of each index array.

Two threads permit launching a child thread at the first level of recursion.  Four threads permit launching child threads at the first two levels of recursion. Eight threads permit launching child threads at the first three levels of recursion, \emph{etc}.  Because child threads are launched at the highest levels of the tree (\emph{i.e.,} the lowest levels of recursion), each thread processes the maximum possible workload.  Because index arrays are subdivided by their median elements at each level of recursion, all threads share the workload equally.

A disadvantage of this thread allocation strategy is that it limits the number of threads to an integer power of two.  Because the level of recursion determines the number of threads, it is not possible to employ, for example, three or ten threads.  An advantage of this thread allocation strategy is that it is simple and robust because synchronization involves only a parent thread and one child thread.

\subsection{Results for the  Algorithm}
\label{sec:knlogn_results}

The  \emph{k}-d tree-building algorithm was implemented in the Java language, and the single-threaded performance of the merge sorting, duplicate tuple removal, and \emph{k}-d tree-building methods was measured using a 2.3 GHz Intel i7 processor.  Figure \ref{fig:BuildingTime} shows the total time in seconds that was required to perform the initial merge sorts, remove the duplicate tuples, and build the \emph{k}-d tree, plotted versus  for   tuples of randomly-generated 32-bit integers. The dashed line of Figure \ref{fig:BuildingTime} shows the least-squares fit of the total time  to the function  where  is the slope of the line.  The correlation coefficient  indicates an adequate least-squares fit; hence, the execution times are proportional to .  The question of whether these execution times are proportional to  will be addressed in Section \ref{sec:discussion} of this article.

\begin{figure}[h]
\centering
\centerline{\includegraphics[width=3.4in]{figures/BuildingKdTreeFigure3.pdf}}
\caption{The total of merge sorting, duplicate tuple removal, and \emph{k}-d tree-building times (seconds) is plotted versus  for the application of the  \emph{k}-d tree-building algorithm to   tuples of randomly-generated 32-bit integers.}
\label{fig:BuildingTime}
\end{figure}

The  \emph{k}-d tree-building algorithm was parallelized via Java threads and its performance was measured for one to eight threads using a 2.3 GHz Intel quad-core i7 processor.  Figure \ref{fig:BuildingParallel} shows the total time in seconds that was required to perform the initial merge sorts, remove the duplicate tuples, and build the \emph{k}-d tree, plotted versus the number of threads  for   tuples of randomly-generated 32-bit integers. The dashed curve of Figure \ref{fig:BuildingParallel} shows the least-squares fit of the total time  to the equation

This equation will be discussed in Section \ref{sec:discussion} of this article.  The correlation coefficient  indicates an acceptable least-squares fit.

\section{Comparative Performance}
\subsection{The  Algorithm}
\label{sec:nlogn_algorithm}

In order to understand the performance of the  \emph{k}-d tree-building algorithm relative to that of other algorithms, the performance was compared to a  \emph{k}-d tree-building algorithm that incorporates a  median-finding algorithm \cite{Blum}.  Most of the complexity of the  algorithm is limited to the  median-finding algorithm.  The application of this  \emph{k}-d tree-building algorithm to sort  tuples is described as follows.

\begin{figure}[h!]
\centering
\centerline{\includegraphics[width=3.4in]{figures/BuildingKdTreeFigure4.pdf}}
\caption{The total of merge sorting, duplicate tuple removal and \emph{k}-d tree-building times (seconds) is plotted versus the number of threads for the application of the  \emph{k}-d tree-building algorithm to    tuples of randomly-generated 32-bit integers.}
\label{fig:BuildingParallel}
\end{figure}

First, an index array is created and merge sorted in ,  or  via one of the ::, :: and :: super keys; the choice of super key is arbitrary.  The initial merge sort does not reorder the tuples array; instead, it reorders the index array whose elements are indices into the tuples array.  Next, duplicate  tuples are removed via one pass through the index array, as discussed in Section \ref{sec:knlogn_algorithm} of this article.

The subsequent \emph{k}-d tree-building step partitions the index array recursively.  At each level of recursion, the median element of the index array is found in  time using the ::, :: or :: super key that is appropriate to that level of recursion.  A convenient feature of the  median-finding algorithm is that the index array is partitioned relative to the median element during the search for the median element.  Hence, once the median element has been found, a reference to the  tuple that the median element specifies is stored in the root of the nascent \emph{k}-d tree, as shown in Figure \ref{fig:FinalTree}.  The lower and upper halves of the index array are processed recursively to create the ``less than" and ``greater than" subtrees of the root of the \emph{k}-d tree.  The  \emph{k}-d tree-building method processes non-overlapping halves of the index array via recursive calls to this method.  Hence, this method is amenable to parallel execution via multiple threads in the manner that was explained in Section \ref{sec:parallel_execution} of this article.

Recursion terminates when the index array comprises one, two or three elements.  In the case of one element, a reference to the corresponding tuple is stored in a new node of the \emph{k}-d tree.  For two elements, a reference to the tuple that corresponds to the first element is stored in a new node of the \emph{k}-d tree, then the super keys of the two elements are compared to decide whether to reference the tuple that corresponds to the second element from the ``less than" or ``greater than" child of that node.  For three elements, the index array is sorted via insertion sort \cite{Bentley2} to determine which tuple to reference from a new node of the \emph{k}-d tree and which tuples to reference from the children of that node.

Recursive partitioning occurs for  levels of the nascent \emph{k}-d tree.  The computational complexity of this \emph{k}-d tree-building algorithm includes a  term for the initial merge sort plus another  term for partitioning  elements of the index array at each of the  levels of recursion.  This  \emph{k}-d tree-building algorithm requires storage for a Tuples array of  \emph{k}-dimensional tuples, plus an -element index array, plus an -element temporary array.  The Tuples array is immutable.  The index and temporary arrays are ephemeral and are no longer required after construction of the \emph{k}-d tree.

\subsection{Results for the  Algorithm}
\label{sec:nlogn_results}

The  \emph{k}-d tree-building algorithm was implemented in the Java language, and the single-threaded performance of the merge sorting, duplicate tuple removal, and \emph{k}-d tree-building methods was measured using a 2.3 GHz Intel i7 processor.  Figure \ref{fig:ComparativeTime} shows the total time in seconds that was required to perform the initial merge sort, remove the duplicate tuples, and build the \emph{k}-d tree, plotted versus  for   tuples of randomly-generated 32-bit integers.  The dashed line of Figure \ref{fig:ComparativeTime} shows the least-squares fit of the total time  to the function  where  is the slope of the line.  The correlation coefficient  indicates an adequate least-squares fit.

\begin{figure}[h!]
\centering
\centerline{\includegraphics[width=3.4in]{figures/BuildingKdTreeFigure5.pdf}}
\caption{The total of merge sorting, duplicate tuple removal, and \emph{k}-d tree-building times (seconds) is plotted versus  for the application of the  \emph{k}-d tree-building algorithm to   tuples of randomly-generated 32-bit integers.}
\label{fig:ComparativeTime}
\end{figure}

\newpage

The  \emph{k}-d tree-building algorithm was parallelized via Java threads and its performance was measured for one to eight threads using a 2.3 GHz Intel quad-core i7 processor.  Figure \ref{fig:ComparativeParallel} shows the total time in seconds that was required to perform the initial merge sort, remove the duplicate tuples, and build the \emph{k}-d tree, plotted versus the number of threads  for   tuples of randomly-generated 32-bit integers. The dashed curve of Figure \ref{fig:ComparativeParallel} shows the least-squares fit of the total time  to Equation \ref{eq:gunther}.  The correlation coefficient  indicates an acceptable least-squares fit.

\begin{figure}[h]
\centering
\centerline{\includegraphics[width=3.4in]{figures/BuildingKdTreeFigure6.pdf}}
\caption{The total of merge sorting, duplicate tuple removal, and \emph{k}-d tree-building times (seconds) is plotted versus the number of threads for the application of the  \emph{k}-d tree-building algorithm to    tuples of randomly-generated 32-bit integers.}
\label{fig:ComparativeParallel}
\end{figure}

\section{Discussion}
\label{sec:discussion}

Figures \ref{fig:BuildingTime} and \ref{fig:ComparativeTime} demonstrate that the execution times of the  and  \emph{k}-d tree-building algorithms are proportional to .  Figures \ref{fig:BuildingParallel} and \ref{fig:ComparativeParallel} show that the \emph{k}-d tree-building algorithms scale for multiple threads.  For either algorithm, the execution by eight threads on an Intel quad-core i7 processor, which supports concurrent execution of two threads per core, increases the execution speed by approximately three times relative to the speed of one thread.  The execution time  does not adhere to the Amdahl \cite{Amdahl} model

but rather to the model that is expressed by Equation \ref{eq:gunther} in Section \ref{sec:knlogn_results} of this article

In this equation,  is the number of threads,  represents the time required to execute the serial or non-parallelizable portion of the algorithm,  represents the time required to execute the parallelizable portion of the algorithm via one thread, and  models an additional limitation to the performance of multi-threaded execution that the Amdahl model fails to capture.

This additional limitation to performance may occur due to cache contention in a shared cache wherein multiple threads attempt to access the cache simultaneously.  The cache miss term  of Equation \ref{eq:gunther} models the performance limitation, which results from cache contention, as a linear function of  \cite{Gunther}.

Differentiating Equation \ref{eq:gunther} with respect to  yields

Setting  to zero in Equation \ref{eq:derivative} and solving for  predicts that the minimum execution time should occur at  threads.  Substituting into this equation for  the respective values of  and  that were obtained via least-squares fitting for the  and  algorithms predicts minima at  and  threads respectively.  These minima are depicted in Figures \ref{fig:BuildingParallel} and \ref{fig:ComparativeParallel}, which predict decreased performance of both \emph{k}-d tree building algorithms for greater than eight threads.  The decreased performance for as few as eight threads is a consequence of the relatively large values for  (1.96 and 1.75 seconds per thread respectively) that were obtained via least-squares fitting.  Figure \ref{fig:Optimized} in Section \ref{sec:least_squares} plots execution time data obtained using an Intel 6-core i7 processor that supports concurrent execution of up to 12 threads. The least-squares fit to the data demonstrates a minimum execution time at  threads.

The similar parallelizable times  for both algorithms (67.48 and 66.16 thread-seconds) indicate that their single-threaded performance is about equal.  This effect is due to a fortuitous choice of  that specifies test data that comprise  tuples.  Because the execution time of the  algorithm should be proportional to  but the execution time of the  algorithm should not, these two algorithms are expected to have unequal performance for a different choice of .  In order to test this hypothesis, each algorithm was utilized to build five different \emph{k}-d trees.  For each \emph{k}-d tree,  \emph{k}-dimensional tuples of randomly-generated 32-bit integers were created using a different value of .  The performance of each algorithm was measured using a single thread of a 2.3 GHz Intel i7 processor.  The results are shown in Figure \ref{fig:Dimensions}.

\begin{figure}[h]
\centering
\centerline{\includegraphics[width=3.4in]{figures/BuildingKdTreeFigure7.pdf}}
\caption{The total of merge sorting, duplicate tuple removal, and \emph{k}-d tree-building times (seconds) is plotted versus the number of dimensions  for the application of the  algorithm (solid line and circles) and the  algorithm (dashed line and diamonds) to build a \emph{k}-d tree from  \emph{k}-dimensional tuples of randomly-generated 32-bit integers.}
\label{fig:Dimensions}
\end{figure}

Figure \ref{fig:Dimensions} shows the total time in seconds that was required to perform the initial merge sorts, remove the duplicate tuples, and build the \emph{k}-d tree via the  and  algorithms for  \emph{k}-dimensional tuples of randomly-generated 32-bit integers, plotted versus the number of dimensions .  This figure demonstrates that the execution time of the  algorithm is proportional to  but the execution time of the  algorithm is not.  In this figure, the slope of the solid line ( seconds per dimension) indicates that for  tuples, each additional dimension increases the execution time of the  algorithm by 18 seconds.  For , the two algorithms have equal performance.

In Figure \ref{fig:Dimensions}, the slope of the dashed line ( seconds per dimension) suggests that the execution time of the  algorithm might be proportional to .  However, this apparent proportionality is related to the time required to generate random 32-bit integers.

The storage requirements of the  and  algorithms differ.  Although both algorithms require storage for a tuples array of  \emph{k}-dimensional tuples, the  algorithm requires storage for an -element temporary array plus  -element index arrays, whereas the  algorithm requires storage for an -element temporary array plus only one -element index array.
  
The  median-finding algorithm is complicated and requires careful implementation to achieve optimum performance.  For example, the median-finding algorithm utilizes a sorting algorithm to sort large numbers of five-element arrays.  For the initial implementation of the  median-finding algorithm, these small arrays were sorted via the merge sort algorithm that is used for the initial sort of the index array \cite{Sedgewick}.  That merge sort algorithm is not optimized for sorting small arrays and hence resulted in poor performance of the  median-finding algorithm and consequent poor performance of the  \emph{k}-d tree-building algorithm.  Replacing the merge sort algorithm with an insertion sort algorithm that is optimized for sorting small arrays \cite{Bentley2} conferred a 30 percent improvement to the performance of the   \emph{k}-d tree-building algorithm.  (The Appendix describes a better alternative than insertion sort.)

\section{Conclusion}
\label{sec:conclusion}

The \emph{k}-d tree-building algorithm proposed in this article guarantees a computational complexity of  for  points and  dimensions.  For , the performance of this algorithm equals the performance of a  \emph{k}-d tree-building algorithm that employs a  median-finding algorithm.  For either algorithm, an improvement in performance by a factor of three relative to single-threaded performance is achieved via parallel execution by eight threads of an Intel quad-core i7 processor that supports concurrent execution of two threads per core.

\subsection*{Source Code}

 The source code that was provided with the original version of this article \cite{Brown} and that implemented the  and  \emph{k}-d tree-building algorithms is superseded by the optimized source code described in Section \ref{sec:source_code}.

\subsection*{Acknowledgements}

The author thanks Paul McJones, Gene McDaniel, Joseph Wearing and John Robinson for helpful comments.

\section{Appendix}
\label{sec:appendix}

\hfill \break \textbf{Russell A. Brown and John A. Robinson} \hfill \break

Six modifications that optimize the performance of the  and  \emph{k}-d tree-building algorithms are described below.

\subsection{Optimized Super Key Comparison}

Both tree-building algorithms depend on the comparison of super keys. This comparison is performed by comparing respective portions of two super keys serially in a loop, beginning with the most significant portion of each super key, proceeding towards the least significant portion of each super key, and exiting the loop as soon as the super keys differ in their respective portions. 

A more efficient comparison may be accomplished by comparing the most significant portions of the super keys prior to entering the loop and thereby avoiding execution of the loop for the common case where the two super keys differ in their most significant portions. This simple change significantly improves the performance of both tree-building algorithms by eliminating the overhead of loop execution.

\subsection{Optimized Merge Sort}

Both tree-building algorithms depend on merge sort \cite{Sedgewick} that copies elements between arrays.  An optimized merge sort minimizes copying via four variants of merge sort \cite{Sedgewick2}.

As discussed in Section \ref{sec:parallel_execution} of this article, merge sort uses a separate thread to process each half of the index array recursively so that the two halves of the index array are processed concurrently by two threads.  However, subsequent to this concurrent recursive processing, only one thread merges the two halves of the index array. 

Concurrent merging may be accomplished using two threads.  One thread merges beginning at the lowest address of the lower half of the index array and at the highest address of the upper half of the index array \cite{Sedgewick}; the addresses move towards one another as they are updated. A second thread merges beginning at the highest address of the lower half of the index array and at the lowest address of the upper half of the index array; the addresses move away from one another as they are updated.

Concurrent merging and minimization of copying improve the performance of both tree-building algorithms.

\newpage
\subsection{Optimized Partitioning}

As discussed in Section \ref{sec:parallel_execution} of this article, the  tree-building algorithm uses a separate thread to process each half of the index array recursively so that the two halves of the index array are processed concurrently by two threads.  However, prior to this concurrent recursive processing, only one thread partitions the index array into two halves. Concurrent partitioning may be performed using two threads. One thread begins at the lowest address of the the index array and ascends. A second thread begins at the highest address of the index array and descends.

As discussed in Section \ref{sec:knlogn_algorithm} of this article, the  algorithm partitions  index arrays and copies one index array at each level of the tree.  An optimization avoids copying the index array and instead only partitions the  index arrays to achieve  performance. This optimization requires precomputing an array of  vectors, where each vector contains  elements that specify permutation of the index arrays for a given level of the tree.

 Concurrent partitioning and avoidance of copying improve the performance of the  tree-building algorithm.

\subsection{Precomputing the Modulus}

Both tree-building algorithms select the most significant portion of the super key via a modulus during each recursive call of the tree-building function. This modulus may be precomputed and stored in an array of  elements, where each element contains the modulus for a given level of the tree, thus eliminating  modulus computations.

\subsection{Optimized Median Selection}

The  tree-building algorithm uses insertion sort \cite{Bentley2} to select a median. The performance of this tree-building algorithm may be improved by median selection via an explicit decision tree \cite{Stepanov} instead of via insertion sort that requires more comparisons and memory accesses.

\subsection{Deferred Merge Sort and Partitioning}

The  tree-building algorithm presorts the data in each of  dimensions prior to building the tree and then partitions the data in each of  dimensions about a median at each level of the tree. The performance of this algorithm may be improved by presorting in only \emph{one} dimension prior to building the tree.  Sorting in each additional dimension is deferred until the level of the tree where the sort is necessary. Moreover, partitioning the data in a given dimension is deferred until after that dimension has been sorted.

Deferred sorting and partitioning are illustrated by the following algorithm for building a \emph{k}-d tree from a set of  tuples.  First, the tuples are presorted using their ::: super keys to populate the  index array.

At the first level of the nascent tree, the  index array is split at its median element to create two half-index arrays. These half-index arrays are then sorted using a ::: super key to populate two  half-index arrays.

At the second level of the tree, each  half-index array is split at its median element to create four quarter-index arrays. These quarter-index arrays are then sorted using a ::: super key to populate four  quarter-index arrays.  Also at this second level of the tree, each  half-index array is partitioned about a -median to create four  quarter-index arrays.

At the third level of the tree, each  quarter-index array is split at its median element to create eight eighth-index arrays. These eighth-index arrays are then sorted using a ::: super key to populate eight  eighth-index arrays.  Also at this third level of the tree, each  quarter-index array and each  quarter-index array is partitioned about a -median to create eight  eighth-index arrays and eight  eighth-index arrays respectively.

At this point, the data have been sorted in each of the  dimensions, so beginning with the fourth level of the tree, the  \emph{k}-d tree-building algorithm proceeds as described in Section \ref{sec:knlogn_algorithm} of this article. The performance of this algorithm improves due to deferred sorting and partitioning to the degree indicated below.

Without deferred sorting, the computational complexity of presorting in  dimensions is . With deferred sorting, the computational complexity is

that reduces to

and refactors to

that sums to

and simplifies to

Hence, deferred sorting confers to the sorting performance a fractional improvement of


Without deferred partitioning, the computational complexity of partitioning  elements of  index arrays at each of the  levels of the tree is . With deferred partitioning, the computational complexity is

that sums and simplifies to

Hence, deferred partitioning confers to the partitioning performance a fractional improvement of


Equations \ref{eq:presort4} and \ref{eq:partition3} predict that deferral improves the performance of sorting and partitioning by a factor that is inversely proportional to  and directly proportional to  and  respectively.

\subsection{Performance Measurements}

The performance of optimized  and  \emph{k}-d tree-building algorithms that benefit from the six above-described modifications was compared to the performance of the non-optimized  algorithm described in Section \ref{sec:knlogn_algorithm} of this article. C++ implementations were benchmarked using an Intel quad-core i7 processor executing eight threads to process  4-dimensional randomly-generated tuples of 64-bit integers.  The optimized  and  algorithms achieved 28 percent and 26 percent performance improvements respectively, relative to the non-optimized  algorithm.  For this benchmark test case, equations \ref{eq:presort4} and \ref{eq:partition3} predict performance improvements of 6 percent and 8 percent respectively.  Hence, the optimizations discussed above (other than deferred sorting and partitioning) significantly improve the performance of the  algorithm and presumably the performance of the  algorithm as well.

A second benchmark used an Intel quad-core i7 processor executing eight threads to process  8-dimensional randomly-generated tuples of 64-bit integers.  The optimized  and  algorithms achieved 65 percent and 34 percent performance improvements respectively, relative to the non-optimized  algorithm.   The greater performance improvement of the  algorithm relative to the  algorithm is due to the fact that the computational complexity of the  algorithm is proportional to the number of dimensions  whereas the computational complexity of the  algorithm is independent of , as demonstrated by Figure \ref{fig:Dimensions}.

\subsection{Nearest Neighbors}
The \emph{k}-d tree may be used to find the nearest neighbor to a query point in  expected time \cite{Friedman}. The nearest-neighbor search algorithm finds the  nearest neighbors to a query point and sorts those  nearest neighbors according to their distances to the query point via a priority queue \cite{Sedgewick3}. In addition, the nearest neighbors can be used to find the \emph{reverse} nearest neighbors to a query point, where the reverse nearest neighbors to a given point are defined as the set of points to which that point is a nearest neighbor \cite{Korn}.

The first step of the reverse-nearest-neighbors search algorithm finds the  nearest neighbors to each point in the \emph{k}-d tree in  expected time by walking the tree recursively and treating each point in the tree as a query point. This nearest-neighbors search algorithm executes in parallel via multiple threads in the manner explained in Section \ref{sec:parallel_execution} for the \emph{k}-d tree-building algorithms. As the  nearest neighbors to each query point are found, the second step of the reverse-nearest-neighbors search algorithm appends that query point to a unique reverse-nearest-neighbors list for each of the  nearest neighbors. The reverse-nearest-neighbors search algorithm executes in parallel via multiple threads and requires a unique lock for each reverse-nearest-neighbors list because any point in the \emph{k}-d tree may be a reverse nearest neighbor to any other point in the \emph{k}-d tree.

The execution times of the nearest-neighbors and reverse-nearest-neighbors search algorithms were measured for one to 12 threads using a 3.2 GHz Intel six-core i7 processor.  Figure \ref{fig:Neighbors} shows the time in seconds required to search the \emph{k}-d tree for nearest neighbors and reverse nearest neighbors, plotted versus the number of threads  for   tuples of randomly-generated 64-bit integers.

\begin{figure}[h]
\centering
\centerline{\includegraphics[width=3.4in]{figures/BuildingKdTreeFigure8.pdf}}
\caption{The total execution time (seconds) is plotted versus the number of threads for the application of the nearest-neighbor and reverse-nearest-neighbor search algorithms to a \emph{k}-d tree built from   tuples of randomly-generated 64-bit integers.}
\label{fig:Neighbors}
\end{figure}

\subsection{Source Code for Optimized Algorithms}
\label{sec:source_code}

Provided under the BSD 3-Clause License are C++ implementations of the optimized  and  algorithms described as follows.

The ``kdTreeNlogn.cpp" and ``kdTreeKnlogn.cpp" source-code files implement algorithms to build a \emph{k}-d tree via the  and  algorithms respectively. The ``kdTreeMapNlogn.cpp" and ``kdTreeMapKnlogn.cpp" source-code files implement algorithms to build a \emph{k}-d tree-based key-to-value map via the  and  algorithms respectively. The ``kdTreeKmapNlogn.cpp" and ``kdTreeKmapKnlogn.cpp" source-code files also implement algorithms to build a \emph{k}-d tree-based key-to-value map via the  and  algorithms respectively. The latter two key-to-value map implementations build the \emph{k}-d tree twice as fast as the former two. This improved performance is achieved by storing a tuple in each node of the \emph{k}-d tree, instead of in the ``Tuples" array depicted in Figure \ref{fig:IndexArrays}, to avoid one degree of indirection when accessing the tuple.

All six implementations include algorithms that search a \emph{k}-d tree (1) for all points that lie inside a \emph{k}-dimensional hyper-rectangular region \cite{Bentley}, (2) for the nearest neighbors to a query point \cite{Friedman}, and (3) for the nearest neighbors and reverse nearest neighbors to each point in the \emph{k}-d tree \cite{Korn}.

All six implementations build a \emph{k}-d tree and search a hyper-rectangular region using multiple threads. The implementations search for the nearest neighbors to a single query point using a single thread and they search for the nearest neighbors and reverse nearest neighbors to all points in the \emph{k}-d tree using multiple threads.

Also provided under the BSD 3-Clause License are the R programs and data used to create Figures \ref{fig:Neighbors} and \ref{fig:Optimized}.

\newpage

\subsection{Least-Squares Curve Fitting}
\label{sec:least_squares}

Figures \ref{fig:BuildingParallel} and \ref{fig:ComparativeParallel} plot the least-squares fits of Java execution time data for the non-optimized  and  \emph{k}-d tree-building algorithms respectively to the model defined by Equation \ref{eq:gunther} in Section \ref{sec:knlogn_results}

where  is the execution time,  is the number of threads,  is the non-parallelizable execution time,  is the parallelizable execution time for a single thread, and  extends the Amdahl model to include the effects of cache contention.

To facilitate least-squares curve fitting, the above equation is rewritten as

where  and .

A least-squares fit \cite{Thomas} of the  tuples of measured data is performed by constructing the error function

and then obtaining the partial derivatives of the error function with respect to ,  and  as follows.


Each partial derivative equals zero at the minimum of the error function. Rearranging Equation \ref{eq:partials} yields the following three simultaneous linear equations that may be solved for the three unknowns ,  and  via Cramer's rule.

However, for a number of threads , the \emph{average} values  and  are calculated for  and  respectively because the number of threads  is not constant across all levels of the nascent \emph{k}-d tree, as illustrated by the following example.

A \emph{k}-d tree built from  tuples requires 21 levels of recursion. Building the tree via a single thread executes one thread at all 21 levels of recursion.  Building the tree via two threads executes two threads at all 21 levels of recursion. Building the tree via four threads executes two threads at the first level and four threads at the remaining 20 levels of recursion. Building the tree via eight threads executes two threads at the first level, four threads at the second level, and eight threads at the remaining 19 levels of recursion. Building the tree via 12 threads executes two threads at the first level, four threads at the second level, eight threads at the third level, and 12 threads at the remaining 18 levels of recursion. For building the tree via 12 threads, 16 threads are created and hence are potentially executable at the remaining 18 levels of recursion but the Intel 6-core i7 processor actually executes at most 12 threads concurrently.

Given the execution by different numbers of threads at different levels of recursion, the average values  and  are calculated using ,  and  as follows.

For  the respective average values  and  of  and  are calculated as


For  the respective average values  and  of  and  are calculated as


For  the respective average values  and  of  and  are calculated as


The execution time of the optimized  \emph{k}-d tree-building algorithm was measured for one to 12 threads using a 3.2 GHz Intel 6-core i7 processor.  Figure \ref{fig:Optimized} shows the total time in seconds that was required to perform the initial merge sort, remove the duplicate tuples, and build the \emph{k}-d tree, plotted versus the number of threads  for   tuples of randomly-generated 64-bit integers.

\begin{figure}[h]
\centering
\centerline{\includegraphics[width=3.4in]{figures/BuildingKdTreeFigure9.pdf}}
\caption{The total of merge sorting, duplicate tuple removal, and \emph{k}-d tree-building times (seconds) is plotted versus the number of threads for the application of the optimized  \emph{k}-d tree-building algorithm to    tuples of randomly-generated 64-bit integers.}
\label{fig:Optimized}
\end{figure}

\newpage

The dashed curve of Figure \ref{fig:Optimized} plots the equation

where the average values  and  are calculated using  in the interval  as follows.

For ,  and  are calculated as


For ,  and  are calculated as


For ,  and  are calculated as


\bibliographystyle{abbrv}
\bibliography{building_balanced_kd_tree.bib}  


\end{document}
