\pdfoutput=1
\PassOptionsToPackage{numbers, compress}{natbib}

\documentclass{article}

\usepackage[final]{nips_2017}

\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}       
\usepackage{microtype}      
\usepackage{graphicx}       
\usepackage[titletoc,title]{appendix}
\usepackage{graphics}
\usepackage{subfigure}
\usepackage{amsmath}
\title{PixelGAN Autoencoders}



\author{
Alireza Makhzani, Brendan Frey\\
University of Toronto\\
\texttt{\{makhzani,frey\}@psi.toronto.edu}
}


\newcommand{\myeq}[1]{\hyperref[eq:#1]{Equation~\ref*{eq:#1}}}
\newcommand{\mysec}[1]{\hyperref[sec:#1]{Section~\ref*{sec:#1}}}
\newcommand{\mytable}[1]{\hyperref[table:#1]{Table~\ref*{table:#1}}}
\newcommand{\myfig}[1]{\hyperref[fig:#1]{Figure~\ref*{fig:#1}}}
\newcommand{\myfigg}[2]{\hyperref[fig:#1]{Figure~\ref*{fig:#1}#2}}
\newcommand{\myfiggg}[3]{\hyperref[fig:#1]{Figure~\ref*{fig:#1}#2,#3}}
\newcommand{\myappendix}[1]{\hyperref[appendix:#1]{Appendix~\ref*{appendix:#1}}}
\DeclareRobustCommand{\parhead}[1]{\textbf{#1}~}

\begin{document}

\maketitle

\begin{abstract}
In this paper, we describe the ``PixelGAN autoencoder'', a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels (PixelCNN) that is conditioned on a latent code, and the recognition path uses a generative adversarial network (GAN) to impose a prior distribution on the latent code. We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder. For example, by imposing a Gaussian distribution as the prior, we can achieve a global vs. local decomposition, or by imposing a categorical distribution as the prior, we can disentangle the style and content information of images in an unsupervised fashion. We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi-supervised settings and achieve competitive semi-supervised classification results on the MNIST, SVHN and NORB datasets.
\end{abstract}


\section{Introduction}
In recent years, generative models that can be trained via direct back-propagation have enabled remarkable progress in modeling natural images. One of the most successful models is the generative adversarial network (GAN)~\citep{gan}, which employs a two player min-max game. The generative model, , samples the prior  and generates the sample . The discriminator, , is trained to identify whether a point  is a sample from the data distribution or a sample from the generative model. The generator is trained to maximally confuse the discriminator into believing that generated samples come from the data distribution. The cost function of GAN is

GANs can be considered within the wider framework of implicit generative models~\citep{mohamed2016learning,ference,dustin}. Implicit distributions can be sampled through their generative path, but their likelihood function is not tractable. Recently, several papers have proposed another application of GAN-style algorithms for approximate inference~\citep{,mohamed2016learning,ference,dustin,ranganath2016operator,aae,avb,ali,bigan}. These algorithms use implicit distributions to learn posterior approximations that are more expressive than the distributions with tractable densities that are often used in variational inference. For example, adversarial autoencoders (AAE)~\citep{aae} use a universal approximator posterior as the implicit posterior distribution and use adversarial training to match the aggregated posterior of the latent code to the prior distribution. Adversarial variational Bayes~\citep{ference,avb} uses a more general amortized GAN inference framework within a maximum-likelihood learning setting. Another type of GAN inference technique is used in the ALI~\citep{ali} and BiGAN~\citep{bigan} models, which have been shown to approximate maximum likelihood learning~\citep{ference}. In these models, both the recognition and generative models are implicit and are jointly learnt by an adversarial training process.


Variational autoencoders (VAE)~\citep{vae,rezende} are another state-of-the-art image modeling technique that use neural networks to parametrize the posterior distribution and pair it with a top-down generative network. Both networks are jointly trained to maximize a variational lower bound on the data log-likelihood. A different framework for learning density models is autoregressive neural networks such as NADE~\citep{nade}, MADE~\citep{made}, PixelRNN~\citep{pixelrnn} and PixelCNN~\citep{pixelcnn}. Unlike variational autoencoders, which capture the statistics of the data in hierarchical latent codes, the autoregressive models learn the image densities directly at the pixel level without learning a hierarchical latent representation.


In this paper, we present the PixelGAN autoencoder as a generative autoencoder that combines the benefits of latent variable models with autoregressive architectures. The PixelGAN autoencoder is a generative autoencoder in which the generative path is a PixelCNN that is conditioned on a latent variable. The latent variable is inferred by matching the aggregated posterior distribution to the prior distribution by an adversarial training technique similar to that of the adversarial autoencoder~\citep{aae}. However, whereas in adversarial autoencoders the statistics of the data distribution are captured by the latent code, in the PixelGAN autoencoder they are captured jointly by the latent code and the autoregressive decoder.
We show that imposing different distributions as the prior results in different factorizations of information between the latent code and the autoregressive decoder. For example, in \mysec{pixelgan_gaussian}, we show that by imposing a Gaussian distribution on the latent code, we can achieve a global vs. local decomposition of information. In this case, the global latent code no longer has to model all the irrelevant and fine details of the image, and can use its capacity to capture more relevant and global statistics of the image.
Another type of decomposition of information that can be learnt by PixelGAN autoencoders is a discrete vs. continuous decomposition. In \mysec{pixelgan_cat}, we show that we can achieve this decomposition by imposing a categorical prior on the latent code using adversarial training. In this case, the categorical latent code captures the discrete underlying factors of variation in the data, such as class label information, and the autoregressive decoder captures the remaining continuous structure, such as style information, in an unsupervised fashion.
We then show how PixelGAN autoencoders with categorical priors can be directly used in clustering and semi-supervised scenarios and achieve very competitive classification results on several datasets in \mysec{experiments}.
Finally, we present one of the main potential applications of PixelGAN autoencoders in learning cross-domain relations between two different domains in \mysec{cross-domain}.


\section{PixelGAN Autoencoders}\label{sec:pixelgan}

\begin{figure}[t]
\begin{center}
\includegraphics[scale=.4]{fig/pixelgan_gaussian}
\end{center}
\vspace{-.3cm}
\caption{\label{fig:pixelgan_gaussian}Architecture of the PixelGAN autoencoder.}
\end{figure}
Let  be a datapoint that comes from the distribution  and  be the hidden code. The recognition path of the PixelGAN autoencoder (\myfig{pixelgan_gaussian}) defines an implicit posterior distribution  by using a deterministic neural function  that takes the input  along with random noise  with a fixed distribution  and outputs . The aggregated posterior  of this model is defined as follows:

This parametrization of the implicit posterior distribution was originally proposed in the adversarial autoencoder work~\citep{aae} as the universal approximator posterior. We can sample from this implicit distribution , by evaluating  at different samples of , but the density function of this posterior distribution is intractable. \myappendix{input_noise} discusses the importance of the input noise in training PixelGAN autoencoders. 
The generative path  is a conditional PixelCNN~\citep{pixelcnn} that conditions on the latent vector  using an adaptive bias in PixelCNN layers. The inference is done by an amortized GAN inference technique that was originally proposed in the adversarial autoencoder work~\citep{aae}. In this method, an adversarial network is attached on top of the hidden code vector of the autoencoder and matches the aggregated posterior distribution, , to an arbitrary prior, . Samples from  and  are provided to the adversarial network as the negative and positive examples respectively, and the generator of the adversarial network, which is also the encoder of the autoencoder, tries to match  to  by the gradient that comes through the discriminative adversarial network. 

The adversarial network, the PixelCNN decoder and the encoder are trained jointly in two phases -- the \emph{reconstruction} phase and the \emph{adversarial} phase -- executed on each mini-batch. In the reconstruction phase, the ground truth input  along with the hidden code  inferred by the encoder are provided to the PixelCNN decoder. The PixelCNN decoder weights are updated to maximize the log-likelihood of the input . The encoder weights are also updated at this stage by the gradient that comes through the conditioning vector of the PixelCNN. In the adversarial phase, the adversarial network updates both its discriminative network and its generative network (the encoder) to match  to . Once the training is done, we can sample from the model by first sampling  from the prior distribution , and then sampling from the conditional likelihood  parametrized by the PixelCNN decoder.
 


We now establish a connection between the PixelGAN autoencoder cost and maximum likelihood learning using a decomposition of the aggregated evidence lower bound (ELBO) proposed in~\citep{surgery}:

The first term in \myeq{elbo} is the reconstruction term and the second term is the marginal KL divergence between the aggregated posterior and the prior distribution. The third term is the mutual information between the latent code  and the input . This is a regularization term that encourages  and  to be decoupled by removing the information of the data distribution from the hidden code.
If the training set has  examples,  is bounded as follows (see~\citep{surgery}).

In order to maximize the ELBO, we need to minimize all the three terms of \myeq{elbo}.
We consider two cases for the decoder :

\parhead{Deterministic Decoder.} If the decoder  is deterministic or has very limited stochasticity such as the simple factorized decoder of the VAE, the mutual information term acts in the complete opposite direction of the reconstruction term. This is because the only way to minimize the reconstruction error of  is to learn a hidden code  that is relevant to , which results in maximizing . Indeed, it can be shown that minimizing the reconstruction term maximizes a variational lower bound on ~\citep{im,infogan}. For example, in the case of the VAE trained on MNIST, since the reconstruction is precise, the mutual information term is dominated and is close to its maximum value ~\citep{surgery}. 

\parhead{Stochastic Decoder.} If we use a powerful decoder such as the PixelCNN, the reconstruction term and the mutual information term will not compete with each other anymore and the network can minimize both independently. In this case, the optimal solution for maximizing the ELBO would be to model  solely by  and thereby minimizing the reconstruction term, and at the same time, minimizing the mutual information term by ignoring the latent code. As a result, even though the model achieves a high likelihood, the latent code does not learn any useful representation, which is undesirable. This problem has been observed in several previous works~\citep{bowman,vlae} and different techniques such as annealing the weight of the KL term~\citep{bowman} or weakening the decoder~\citep{vlae} have been proposed to make  and  more dependent. 



As suggested in~\citep{ference_ml,vlae}, we think that the maximum likelihood objective by itself is not a useful objective for representation learning especially when a powerful decoder is used. 
In PixelGAN autoencoders, in order to encourage learning more useful representations, we modify the ELBO (\myeq{elbo}) by removing the mutual information term from it, since this term is explicitly encouraging  to become independent of . So our cost function only includes the reconstruction term and the marginal KL term. The reconstruction term is optimized by the reconstruction phase of training and the marginal KL term is approximately optimized by the adversarial phase\footnote{The original GAN formulation optimizes the Jensen-Shannon divergence~\citep{gan}, but there are other formulations that optimize the KL divergence, e.g. \citep{ference}.}.
Note that since the mutual information term is upper bounded by a constant (), we are still maximizing a lower bound on the log-likelihood of data. However, this bound is weaker than the ELBO, which is the price that is paid for learning more useful latent representations by balancing the decomposition of information between the latent code and the autoregressive decoder.

For implementing the conditioning adaptive bias in the PixelCNN decoder, we explore two different architectures~\citep{pixelcnn}. In the \emph{location-invariant} bias, for each PixelCNN layer, we use the latent code to construct a vector that is broadcasted within each feature map of the layer and then added as an adaptive bias to that layer. In the \emph{location-dependent} bias, we use the latent code to construct a spatial feature map that is broadcasted across different feature maps and then added only to the first layer of the decoder as an adaptive bias. We will discuss the effect of these architectures on the learnt representation in \myfig{mnist_code} of \mysec{pixelgan_gaussian} and their implementation details in \myappendix{conditioning_of_pixelcnn}.

\subsection{PixelGAN Autoencoders with Gaussian Priors}\label{sec:pixelgan_gaussian}
Here, we show that PixelGAN autoencoders with Gaussian priors can decompose the global and local statistics of the images between the latent code and the autoregressive decoder. \myfigg{mnist}{a} shows the samples of a PixelGAN autoencoder model with the location-dependent bias trained on the MNIST dataset.
For the purpose of better illustrating the decomposition of information, we have chosen a 2-D Gaussian latent code and a limited the receptive field of size 9 for the PixelGAN autoencoder. \myfigg{mnist}{b} shows the samples of a PixelCNN model with the same limited receptive field size of 9 and \myfigg{mnist}{c} shows the samples of an adversarial autoencoder with the 2-D Gaussian latent code. The PixelCNN can successfully capture the local statistics, but fails to capture the global statistics due to the limited receptive field size. In contrast, the adversarial autoencoder, whose sample quality is very similar to that of the VAE, can successfully capture the global statistics, but fails to generate the details of the images. However, the PixelGAN autoencoder, with the same receptive field and code size, can combine the best of both and generates sharp images with global statistics. 

\begin{figure}[t]
\centering
\hspace*{0.8cm}\subfigure[PixelGAN Samples \newline \hspace{\linewidth} \text{ } \hspace{-.3cm} {\fontsize{7}{1}\selectfont (2D code, limited receptive field)}]{
\hspace*{-0.8cm}\includegraphics[scale=.11]{fig/pixelgan_samples}}
\hspace{.13cm}
\hspace*{0.8cm}\subfigure[PixelCNN Samples \newline \hspace{\linewidth} \text{ } \hspace{.2cm} {\fontsize{7}{1}\selectfont (limited receptive field)}]{
\hspace*{-0.8cm}\includegraphics[scale=.11]{fig/pixelcnn_samples}}
\hspace{.13cm}
\hspace*{1cm}\subfigure[AAE Samples \newline \hspace{\linewidth} \text{ } \hspace{.65cm}{\fontsize{7}{1}\selectfont (2D code)}]{
\hspace*{-1cm}\includegraphics[scale=.1463]{fig/aae_samples}}
\vspace{-.3cm}
\caption{\label{fig:mnist}(a) Samples of the PixelGAN autoencoder with 2D Gaussian code and limited receptive field of size 9. (b) Samples of the PixelCNN (c) Samples of the adversarial autoencoder.}
\end{figure}


In PixelGAN autoencoders, both the PixelCNN depth and the conditioning architecture affect the decomposition of information between the latent code and the autoregressive decoder. 
We investigate these effects in \myfig{mnist_code} by training a PixelGAN autoencoder on MNIST where the code size is chosen to be  for the visualization purpose. 
As shown in \myfiggg{mnist_code}{a}{b}, when a shallow decoder is used, most of the information will be encoded in the hidden code and there is a clean separation between the digit clusters. As we make the PixelCNN more powerful (\myfiggg{mnist_code}{c}{d}), we can see that the hidden code is still used to capture some relevant information of the input, but the separation of digit clusters is not as sharp when the limited code size of 2 is used. In the next section, we will show that by using a larger code size (e.g., 30), we can get a much better separation of digit clusters even when a powerful PixelCNN is used. 


The conditioning architecture also affects the decomposition of information. In the case of the location-invariant bias, the hidden code is encouraged to learn the global information that is location-invariant (the \emph{what} information and not the \emph{where} information) such as the class label information. For example, we can see in \myfiggg{mnist_code}{a}{c} that the network has learnt to use one of the axes of the 2D Gaussian code to explicitly encode the digit label even though a continuous prior is imposed. In this case, we can potentially get a much better separation if we impose a discrete prior. This makes this architecture suitable for the discrete vs. continuous decomposition and we use it for our clustering and semi-supervised learning experiments. In the case of the location-dependent bias (\myfiggg{mnist_code}{b}{d}), the hidden code is encouraged to learn the global information that has location dependent information such as low-frequency content of the image, similar to what the hidden code of an adversarial or variational autoencoder would learn (\myfigg{mnist}{c}). This makes this architecture suitable for the global vs. local decomposition experiments such as \myfigg{mnist}{a}.



From \myfig{mnist_code}, we can see that the class label information is mostly captured by  while the style information of the images is captured by both  and . 
This decomposition of information has also been studied in other works that combine the latent variable models with autoregressive decoders such as PixelVAE~\citep{pixelvae} and variational lossy autoencoders (VLAE)~\citep{vlae}. For example, the VLAE model~\citep{vlae} proposes to use the depth of the PixelCNN decoder to control the decomposition of information. In their model, the PixelCNN decoder is designed to have a shallow depth (small local receptive field) so that the latent code  is forced to capture more global information. This approach is very similar to our example of the PixelGAN autoencoder in \myfig{mnist}. However, the question that has remained unanswered is whether it is possible to achieve a complete decomposition of content and style in an unsupervised fashion, where the class label or discrete structure information is encoded in the latent code , and the remaining continuous structure such as style is captured by a powerful and deep PixelCNN decoder. This kind of decomposition is particularly interesting as it can be directly used for clustering and semi-supervised classification. In the next section, we show that we can learn this decomposition of content and style by imposing a categorical distribution on the latent representation  using adversarial training. 
Note that this discrete vs. continuous decomposition is very different from the global vs. local decomposition, because a continuous factor of variation such as style can have both global and local effect on the image. 
Indeed, in order to achieve the  discrete vs. continuous decomposition, we have to use very deep and powerful PixelCNN decoders (up to 20 residual blocks) to capture both the global and local statistics of the style by the PixelCNN while the discrete content of the image is captured by the categorical latent variable.

\begin{figure}[t]
\centering
\hspace*{0.3cm}\subfigure[Shallow PixelCNN \newline Location-invariant bias]{
\hspace*{-0.3cm}\includegraphics[scale=.113]{fig/shallow_invariant}}
\hspace{-.15cm}
\hspace*{0.3cm}\subfigure[Shallow PixelCNN \newline Location-dependent bias]{
\hspace*{-0.3cm}\includegraphics[scale=.113]{fig/shallow_dependent}}
\hspace{-.15cm}
\hspace*{0.3cm}\subfigure[Deep PixelCNN \newline Location-invariant bias]{
\hspace*{-0.3cm}\includegraphics[scale=.113]{fig/deep_invariant}}
\hspace{-.15cm}
\hspace*{0.3cm}\subfigure[Deep PixelCNN \newline Location-dependent bias]{
\hspace*{-0.3cm}\includegraphics[scale=.113]{fig/deep_dependent}}
\vspace{-.3cm}
\caption{\label{fig:mnist_code} The effect of the PixelCNN decoder depth and the conditioning architecture on the learnt representation of the PixelGAN autoencoder. (\emph{Shallow}=3 ResBlocks, \emph{Deep}=12 ResBlocks)}
\end{figure}




\subsection{PixelGAN Autoencoders with Categorical Priors}\label{sec:pixelgan_cat}




In this section, we present an architecture of the PixelGAN autoencoder that can separate the discrete information (e.g., class label) from the continuous information (e.g., style information) in the images. 
We then show how our architecture can be naturally adopted for the semi-supervised settings.

The architecture that we use is similar to \myfig{pixelgan_gaussian}, with the difference that we impose a categorical distribution as the prior rather the Gaussian distribution (\myfig{pixelgan_cat}) and also use the location-independent bias architecture. Another difference is that we use a convolutional network as the inference network  to encourage the encoder to preserve the content and lose the style information of the image. The inference network has a softmax output and predicts a one-hot vector whose dimension is the number of discrete labels or categories that we wish the data to be clustered into. The adversarial network is trained directly on the continuous probability outputs of the softmax layer of the encoder. 

Imposing a categorical distribution at the output of the encoder imposes two constraints. The first constraint is that the encoder has to make confident decisions about the class labels of the inputs. The adversarial training pushes the output of the encoder to the corners of the softmax simplex, by which it ensures that the autoencoder cannot use the latent vector  to carry any continuous style information. The second constraint imposed by adversarial training is that the aggregated posterior distribution of  should match the categorical prior distribution with uniform outcome probabilities. This constraint enforces the encoder to evenly distribute the class labels across the corners of the softmax simplex. Because of these constraints, the latent variable will only capture the discrete content of the image and all the continuous style information will be captured by the autoregressive decoder. 

\begin{figure}[t]
\begin{center}
\hspace{4cm}\includegraphics[scale=0.38]{fig/pixelgan_cat}
\end{center}
\vspace{-.3cm}
\caption{\label{fig:pixelgan_cat}Architecture of the PixelGAN autoencoder with the categorical prior.  captures the class label and  is a multi-modal distribution that captures the style distribution of a digit conditioned on the class label of that digit.}
\end{figure}

\begin{figure}[b]
\centering
\subfigure[Without GAN Regularization]{
\includegraphics[scale=.15]{fig/toy_no_gan}}
\hspace{1cm}
\subfigure[With GAN Regularization]{
\includegraphics[scale=.15]{fig/toy_pixelgan}}
\vspace{-.3cm}
\caption{\label{fig:pixelgan_cluster_toy}{Effect of GAN regularization on the code space of PixelGAN autoencoders: (a) no distribution is imposed on the hidden code. (b) a categorical prior is imposed on the hidden code.}}
\end{figure}


In order to better understand and visualize the effect of the adversarial training on shaping the hidden code distribution, we train a PixelGAN autoencoder on the first three digits of MNIST (18000 training and 3000 test points) and choose the number of clusters to be 3. Suppose  is the hidden code which in this case is the output probabilities of the softmax layer of the inference network. In \myfigg{pixelgan_cluster_toy}{a}, we project the 3D softmax simplex of  onto a 2D triangle and plot the hidden codes of the training examples when no distribution is imposed on the hidden code. We can see from this figure that the network has learnt to use the surface of the softmax simplex to encode style information of the digits and thus the three corners of the simplex do not have any meaningful interpretation. \myfigg{pixelgan_cluster_toy}{b} corresponds to the code space of the same network when a categorical distribution is imposed using the adversarial training. In this case, we can see the network has successfully learnt to encode the label information of the three digits in the three corners of the simplex, and all the style information has been separately captured by the autoregressive decoder. This network achieves an almost perfect test error-rate of  on the first three digits of MNIST, even though it is trained in a purely unsupervised fashion.

Once the PixelGAN autoencoder is trained, its encoder can be used for clustering new points and its decoder can be used to generate samples from each cluster. Figure \ref{fig:pixelgan_cluster} illustrates the samples of the PixelGAN autoencoder trained on the full MNIST dataset. The number of clusters is set to be 30 and each row corresponds to the conditional samples of one of the clusters (only 16 are shown). We can see that the discrete latent code of the network has learnt discrete factors of variation such as class label information and some discrete style information. For example digit s are put in different clusters based on how much tilted they are. The network is also assigning different clusters to digit s (based on whether they have a loop) and digit s (based on whether they have a dash in the middle). In \mysec{experiments:unsup}, we will show that by using the encoder of this network, we can obtain about 5\% error rate in classifying digits in an unsupervised fashion, just by matching each cluster to a digit type.


\begin{figure}[t]
\centering
\includegraphics[scale=.14]{fig/disentangle_1}
\hspace{.3cm}
\includegraphics[scale=.14]{fig/disentangle_2}
\vspace{-.1cm}
\caption{\label{fig:pixelgan_cluster}Disentangling the content and style in an unsupervised fashion with PixelGAN autoencoders. Each row shows samples of the model from one of the learnt clusters.}
\end{figure}


\parhead{Semi-Supervised PixelGAN Autoencoders.} The PixelGAN autoencoder can be used in a semi-supervised setting. In order to incorporate the label information, we add a \emph{semi-supervised} training phase. Specifically, we set the number of clusters to be the same as the number of class labels and after executing the reconstruction and the adversarial phases on an unlabeled mini-batch, the semi-supervised phase is executed on a labeled mini-batch, by updating the weights of the encoder  to minimize the cross-entropy cost. The semi-supervised cost also reduces the mode-missing behavior of the GAN training by enforcing the encoder to learn all the modes of the categorical distribution. In \mysec{experiments:semi}, we will evaluate the performance of the PixelGAN autoencoders on the semi-supervised classification tasks.


\section{Experiments}\label{sec:experiments}



In this paper, we presented the PixelGAN autoencoder as a generative model, but the currently available metrics for evaluating the likelihood of GAN-based generative models such as Parzen window estimate are fundamentally flawed~\citep{theis}. So in this section, we only present the performance of the PixelGAN autoencoder on downstream tasks such as unsupervised clustering and semi-supervised classification. The details of all the experiments can be found in \myappendix{experiment}.

\subsection{Unsupervised Clustering}\label{sec:experiments:unsup}
We trained a PixelGAN autoencoder in an unsupervised fashion on the MNIST dataset (\myfig{pixelgan_cluster}). We chose the number of clusters to be 30 and used the following evaluation protocol: once the training is done, for each cluster , we found the validation example  that maximizes , and assigned the label of  to all the points in the cluster . We then computed the test error based on the assigned class labels to each cluster. As shown in the first column of \mytable{semi}, the performance of PixelGAN autoencoders is on par with other GAN-based clustering algorithms such as CatGAN~\citep{catgan}, InfoGAN~\citep{infogan} and adversarial autoencoders~\citep{aae}.


\begin{figure}[t]
\centering
\includegraphics[scale=0.31]{fig/mnist_plot}
\hspace{0.3cm}
\includegraphics[scale=0.31]{fig/svhn_plot}
\vspace{-.3cm}
\caption{\label{fig:plot}Semi-supervised error-rate of PixelGAN autoencoders on the MNIST and SVHN datasets.}
\end{figure}
\begin{center}
\begin{table}[t]
\centering
\resizebox{14cm}{!}{
\begin{tabular}{l|cccc|cc|c}
  \toprule
  & MNIST & MNIST & MNIST & MNIST & SVHN & SVHN & NORB\\
  & \small(Unsupervised) & \small(20 labels) & \small(50 labels) & \small(100 labels) & \small(500 labels) & \small(1000 labels) & \small(1000 labels) \\
  \midrule

VAE~\citep{semi-vae}                            & -                   &-                  & -                 &   & -                               &                &               \\
VAT~\citep{vat}                                 & -                   &-                  & -                 & & -                               &              & \\
ADGM~\citep{adgm}                               & -                   &-                  & -                 &   & -                               &              &               \\
SDGM~\citep{adgm}                               & -                   &-                  & -                 &   & -                               &                &   \\
Adversarial Autoencoder~\citep{aae}             &   &-                  & -                 &   & -                               &                & -                                \\   
Ladder Networks~\citep{ladder}                  & -                   &-                  & -                 &   & -                               & -                                 & -                                \\  
Convolutional CatGAN~\citep{catgan}             & &-                  & -                 &   & -                               & -                                 & -                                \\  
InfoGAN~\citep{infogan}                         & &-                  & -                 & -                   & -                               & -                                 & -                                \\    
Feature Matching GAN~\citep{improved-gan}       & -                   && &   &              &    & -                                \\    
Temporal Ensembling~\citep{temporal-ensembling} & -                   &-                  & -                 & -                   &  &    & -                                \\     
\midrule
\textbf{PixelGAN Autoencoders}                  &   && &   &              &    &    \\
  \bottomrule
\end{tabular}
}
\vspace{.1ex}
\caption{\label{table:semi}Semi-supervised learning and clustering error-rate on MNIST, SVHN and NORB datasets.}
\end{table}
\end{center}

\subsection{Semi-supervised Classification}\label{sec:experiments:semi}
\mytable{semi} and \myfig{plot} report the results of semi-supervised classification experiments on the MNIST, SVHN and NORB datasets. On the MNIST dataset with 20, 50 and 100 labels, our classification results are highly competitive.
Note that the classification rate of unsupervised clustering of MNIST is better than semi-supervised MNIST with 20 labels. This is because in the unsupervised case, the number of clusters is 30, but in the semi-supervised case, there are only 10 class labels which makes it more likely to confuse two digits. 
On the SVHN dataset with 500 and 1000 labels, the PixelGAN autoencoder outperforms all the other methods except the recently proposed temporal ensembling work~\citep{temporal-ensembling} which is not a generative model.
On the NORB dataset with 1000 labels, the PixelGAN autoencoder outperforms all the other reported results.

\begin{figure}[b]
\centering
\hspace*{2.2cm}\subfigure[SVHN \newline \hspace{\linewidth} (1000 labels)]{
\hspace*{-2.2cm}\includegraphics[scale=.18]{fig/svhn_semi}}
\hspace*{2.2cm}\subfigure[MNIST \newline \hspace{\linewidth}(100 labels)]{
\hspace*{-2.2cm}\includegraphics[scale=.18]{fig/mnist_semi}}
\hspace*{.7cm}\subfigure[NORB \newline (1000 labels)]{
\hspace*{-.7cm}\includegraphics[scale=.18]{fig/norb_semi}}
\vspace{-.3cm}
\caption{\label{fig:disentangle}Conditional samples of the semi-supervised PixelGAN autoencoder.}
\end{figure}


\myfig{disentangle} shows the conditional samples of the semi-supervised PixelGAN autoencoder on the MNIST, SVHN and NORB datasets. Each column of this figure presents sampled images conditioned on a fixed one-hot latent code. We can see from this figure that the PixelGAN autoencoder can achieve a rather clean separation of style and content on these datasets with very few labeled data.


\section{Learning Cross-Domain Relations with PixelGAN Autoencoders}\label{sec:cross-domain}


In this section, we discuss how the PixelGAN autoencoder can be viewed in the context of learning cross-domain relations between two different domains. We also describe how the problem of clustering or semi-supervised learning can be cast as the problem of finding a smooth cross-domain mapping from the data distribution to the categorical distribution. 

Recently several GAN-based methods have been developed to learn a cross-domain mapping between two different domains~\citep{discogan,cyclegan,cross-domain-ilya,aae,cross-domain-nlp}. In~\citep{cross-domain-ilya}, an unsupervised cost function called the output distribution matching (ODM) is proposed to find a cross-domain mapping  between two domains  and  by imposing the following unsupervised constraint on the uncorrelated samples from  and : 



where  denotes the distribution of the random variable . The adversarial training is proposed as one of the methods for matching these distributions. If we have access to a few labeled pairs , then  can be further trained on them in a supervised fashion to satisfy . For example, in speech recognition, we want to find a cross-domain mapping from a sequence of phonemes to a sequence of characters. By optimizing the ODM cost function in \myeq{odm}, we can find a smooth function  that takes phonemes at its input and outputs a sequence of characters that respects the language model. However, the main problem with this method is that the network can learn to ignore part of the input distribution and still satisfy the ODM cost function by its output distribution. This problem has also been observed in other works such as~\citep{discogan}. One way to avoid this problem is to add a reconstruction term to the ODM cost function by introducing a reverse mapping from the output of the encoder to the input domain. The is essentially the idea of the adversarial autoencoder~\citep{aae} which learns a generative model by finding a cross-domain mapping between a Gaussian distribution and the data distribution. Using the ODM cost function along with a reconstruction term to learn cross-domain relations have been explored in several previous works. For example, InfoGAN~\citep{infogan} adds a mutual information term to the ODM cost function and optimizes a variational lower bound on this term. It can be shown that maximizing this variational bound is indeed minimizing the reconstruction cost of an autoencoder~\citep{im}. Similarly, in~\citep{cross-domain-nlp, zhangadversarial}, an adversarial autoencoder is used to learn the cross-domain relations of the vector representations of words from two different languages. The architecture of the recent works of DiscoGAN~\citep{discogan} and CycleGAN~\citep{cyclegan} are also similar to an adversarial autoencoder in which the latent representation is enforced to have the distribution of the other domain. Here we describe how our proposed PixelGAN autoencoder can be potentially used in all these application areas to learn better cross-domain relations. Suppose we want to learn a mapping from domain  to . In the architecture of \myfig{pixelgan_gaussian}, we can use independent samples of  at the input and instead of imposing a Gaussian distribution on the latent code, we can impose the distribution of the second domain using its independent samples . Unlike adversarial autoencoders, the encoder of PixelGAN autoencoders does not have to retain all the input information in order to have a lossless reconstruction. So the encoder can use all its capacity to learn the most relevant mapping from  to  and at the same time, the PixelCNN decoder can capture the remaining information that has been lost by the encoder.

We can adopt the ODM idea for semi-supervised learning by assuming  is the image domain and  is the label domain (\myfigg{related}{a}). Independent samples of  and  correspond to samples from the data distribution  and the categorical distribution. The function  can be parametrized by a neural network that is trained to satisfy the ODM cost function by matching the aggregated distribution  to the categorical distribution using adversarial training. The few labeled examples are used to further train  to satisfy . However, as explained above, the problem with this method is that the network can learn to generate the categorical distribution by ignoring some part of the input distribution. The adversarial autoencoder (\myfigg{related}{b}) solves this problem by adding an inverse mapping from the categorical distribution to the data distribution. However, the main drawback of the adversarial autoencoder architecture is that due to the reconstruction term, the latent representation now has to model all the underlying factors of variation in the image. For example, in the architecture of \myfigg{related}{b}, while we are only interested in the one-hot label representation to do semi-supervised learning, we also need to infer the style of the image so that we can have a lossless reconstruction of the image. The PixelGAN autoencoder solves this problem by enabling the encoder to only infer the factor of variation that we are interested in (i.e., label information), while the remaining structure of the input (i.e., style information) is automatically captured by the autoregressive decoder.

\begin{figure}[t]
\centering
\subfigure[Output Distribution Matching~\citep{cross-domain-ilya}]{
\includegraphics[scale=0.35]{fig/odm}}
\hspace{1.5cm}
\subfigure[Adversarial Autoencoders~\citep{aae}]{
\includegraphics[scale=0.35]{fig/aae}}
\vspace{-.3cm}
\caption{\label{fig:related}Learning cross-domain relations with (a) output distribution matching and (b) adversarial autoencoders.}
\end{figure}

\section{Conclusion}
In this paper, we proposed the PixelGAN autoencoder, which is a generative autoencoder that combines a generative PixelCNN with a GAN inference network that can impose arbitrary priors on the latent code. We showed that imposing different distributions as the prior enables us to learn a latent representation that captures the type of statistics that we care about, while the remaining structure of the image is captured by the PixelCNN decoder. Specifically, by imposing a Gaussian prior, we were able to disentangle the low-frequency and high-frequency statistics of the images, and by imposing a categorical prior we were able to disentangle the style and content of images and learn representations that are specifically useful for clustering and semi-supervised learning tasks.
While the main focus of this paper was to demonstrate the application of PixelGAN autoencoders in downstream tasks such as semi-supervised learning, we discussed how these architectures have many other potentials such as learning cross-domain relations between two different domains.

\section*{Acknowledgments}
We would like to thank Nathan Killoran for helpful discussions. We also thank NVIDIA for GPU donations.

\bibliographystyle{unsrt}
{\fontsize{10}{1}\selectfont \bibliography{pixelgan}}
\newpage


\begin{appendices}



\section{Implementation Details}\label{appendix:implementation}
In this section, we describe two important architecture design choices for training PixelGAN autoencoders.

\subsection{Input noise}
\label{appendix:input_noise}

In all the semi-supervised experiments, we found it crucial to use the universal approximator posterior discussed in \mysec{pixelgan}, as opposed to a deterministic posterior. Specifically, the input noise that we use is an additive Gaussian noise, which results in a posterior distribution  that is more expressive than that of a model without the input corruption. This is similar to the denoising criterion idea proposed in~\citep{denoising-vae}. We believe this additive noise is also playing an important role in preventing the mode-missing behavior of the GAN when imposing a degenerate distribution such as the categorical distribution. Similar related ideas have been used to stabilize GAN training such as instance noise~\citep{instance} or one-sided label noise~\citep{improved-gan}.

\subsection{Conditioning of PixelCNN}
\label{appendix:conditioning_of_pixelcnn}

There are three methods to implement how the PixelCNN conditions on the latent vector.

\parhead{Location-Invariant Bias.} This is the method that was proposed in the conditional PixelCNN model~\citep{pixelcnn}. Suppose the size of the convolutional layer of the decoder is \texttt{(batch, width, height, channels)}. 
Then the PixelCNN can use a linear mapping to convert the conditioning tensor of size \texttt{(batch, condition\char`_size)} to generate a tensor of size \texttt{(batch, channels)} that is then broadcasted and added to the feature maps of all the layers of the PixelCNN decoder as an adaptive bias. In this method, the hidden code is encouraged to learn the global information that is location-invariant (the \emph{what} information and not the \emph{where} information) such as the class label information. We use this method in all the clustering and semi-supervised learning experiments. 


\parhead{Location-Dependent Bias.} Suppose the size of the convolutional layer of the PixelCNN decoder is \texttt{(batch, width, height, channels)}. Then the PixelCNN can use a one layer neural network to convert the conditioning tensor of size \texttt{(batch, condition\char`_size)} to generate a spatial tensor of size \texttt{(batch, width, height, k)} followed by a  convolutional layer to construct a tensor of size \texttt{(batch, width, height, channels)} that is then added only to the feature maps of the first layer of the decoder as an adaptive bias (similar to the VPN model~\citep{vpn}). When , we can simply broadcast the tensor of size \texttt{(batch, width, height, k=1)} to get a tensor of size \texttt{(batch, width, height, channels)} instead of using the  convolution. In this method, the latent vector has spatial and location-dependent information within the feature map. This is the method that we used in experiments of \myfigg{mnist}{a}.

\parhead{Input Channel.} Another method for conditioning is proposed in the PixelVAE~\citep{pixelvae} and the variational lossy autoencoder (VLAE)~\citep{vlae}. In this method, first a tensor of size \texttt{(batch, width, height, k)} is constructed using the conditioning tensor similar to the location-dependent bias. This tensor is then concatenated to the input of the PixelCNN. The performance and computational complexity of this method is very similar to that of the location-dependent bias method.



\section{Experiment Details}\label{appendix:experiment}
We used TensorFlow~\citep{tensorflow2015-whitepaper} in all of our experiments. As suggested in~\citep{gan}, in order to improve the stability of GAN training, the generator of the GAN in all our experiments is trained to maximize  rather than minimizing .
\subsection{MNIST Dataset}\label{appendix:mnist}
The MNIST dataset has 50K training points, 10K validation points and 10K test points. We perform experiments on both the binary MNIST and the real-valued MNIST. In the real valued MNIST experiments, we subtract 127.5 from the data points and then divide them by 127.5 and use the discretized logistic mixture likelihood~\citep{pixelcnn++} as the cost function for the PixelCNN. In the case of binary MNIST, the data points are binarized by setting pixel values larger than 0.5 to 1, and values smaller than 0.5 to 0.

\subsubsection{PixelGAN Autoencoders with Gaussian Prior on MNIST}\label{appendix:gaussian-mnist}
Here we describe the model architecture used for training the PixelGAN autoencoder with a Gaussian prior on the binary MNIST dataset in \myfigg{mnist}{a}.
The PixelCNN decoder uses both the vertical and horizontal stacks similar to~\citep{pixelcnn}. The cost function of the PixelCNN is the cross-entropy cost function. 
The PixelCNN uses the location-dependent bias as described in \myappendix{conditioning_of_pixelcnn}. Specifically, a tensor of size \texttt{(batch, width, height, 1)} is constructed from the conditioning vector by using a one-layer neural network with 1000 hidden units, \texttt{ReLU} activation and linear output. This tensor is then broadcasted and added only to the feature maps of the first layer of the PixelCNN decoder. 
The PixelCNN is designed to have a local receptive field by having 3 residual blocks (filter size of 3x5, 32 feature maps, \texttt{ReLU} non-linearity as in ~\citep{pixelcnn}). The adversarial discriminator has two layers of 2000 hidden units with \texttt{ReLU} activation function. 
The encoder architecture has two fully-connected layers of size 2000 with \texttt{ReLU} non-linearity. 
The last layer of the encoder  has a linear activation function. On the latent representation of size , we impose a Gaussian distribution with standard deviation of . 
We used the gradient descent with momentum algorithm for optimizing all the cost functions of the network. 
For the PixelCNN reconstruction cost, we used the learning rate of 0.001 and the momentum value of 0.9. After 25 epochs we reduce the learning rate to 0.0001. For both of the generator and the discriminator costs, the learning rates and the momentum values were set to 0.1.

\subsubsection{Unsupervised Clustering of MNIST}\label{appendix:cluster-mnist}
Here we describe the model architecture used for clustering the binary MNIST dataset in \myfig{pixelgan_cluster} and \mysec{experiments:unsup}. The PixelCNN decoder uses both the vertical and horizontal stacks similar to~\citep{pixelcnn}. The cost function of the PixelCNN is the cross-entropy cost function. 
The PixelCNN uses the location-invariant bias as described in \myappendix{conditioning_of_pixelcnn} and has 15 residual blocks (filter size of 3x5, 32 feature maps, \texttt{ReLU} non-linearity as in~\citep{pixelcnn}). The adversarial discriminator has two layers of 3000 hidden units with \texttt{ReLU} activation function. 
The encoder architecture has a convolutional layer (filter size of 7, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2), followed by another convolutional layer (filter size of 7, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2) with no fully-connected layer. 
The last layer of the encoder  has the softmax activation function. We found it important to use batch-normalization~\citep{batch} for all the layers of the encoder \emph{including} the softmax layer. 
The number of clusters is chosen to be . The clusters are represented by a discrete one-hot variable of size 30. On the continuous probability output of the softmax, we impose a categorical distribution with uniform probabilities. 
We use Adam~\citep{Adam} optimizer with learning rate of  for optimizing the PixelCNN reconstruction cost function, but we found it important to use the gradient descent with momentum algorithm for optimizing the generator and the discriminator costs of the adversarial network. 
For both of the generator and the discriminator costs, the momentum values were set to 0.1 and the learning rates were set to 0.01. 
We use an input dropout noise with the keep probability of  at the input layer and only at the training time. 

The model architecture used for \myfig{pixelgan_cluster_toy} is the same as this architecture except that the number of clusters is chosen to be .


\subsubsection{Semi-Supervised MNIST}\label{appendix:semi-mnist}
We performed semi-supervised learning experiments on both binary and real-valued MNIST dataset. We found that the semi-supervised error-rate of the real-valued MNIST is roughly the same as the binary MNIST (about 1.10\% with 100 labels), but it takes longer to train due to the logistic mixture likelihood cost function~\citep{pixelcnn++}. So in \mytable{semi}, we only report the performance with the binary MNIST, but in \myfigg{disentangle}{b} we are showing the samples of the real-valued MNIST with 100 labels.

\parhead{Binary MNIST.} Here we describe the model architecture used for the semi-supervised learning experiments on the binary MNIST in \mysec{experiments:semi} and \mytable{semi}. 
The PixelCNN decoder uses both the vertical and horizontal stacks similar to~\citep{pixelcnn} and uses the cross-entropy cost function.
The PixelCNN uses the location-invariant bias as described in \myappendix{conditioning_of_pixelcnn}.
The PixelCNN has 6 residual blocks (filter size of 3x5, 32 feature maps, \texttt{ReLU} non-linearity as in~\citep{pixelcnn}). 
The adversarial discriminator has two layers of 1000 hidden units with \texttt{ReLU} activation function. 
The encoder architecture has three convolutional layers (filter size of 5, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2), followed by another three convolutional layers (filter size of 5, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2) with no fully-connected layer.
The last layer of the encoder  has the softmax activation function. All the convolutional layers of the encoder except the softmax layer use batch-normalization~\citep{batch}. 
On the latent representation, we impose a categorical distribution with uniform probabilities. 
The semi-supervised cost is the cross-entropy cost function at the output of . We use Adam~\citep{Adam} optimizer with learning rate of  for optimizing the PixelCNN cost and the cross-entropy cost, but we found it important to use the gradient descent with momentum algorithm for optimizing the generator and the discriminator costs of the adversarial network. 
For both of the generator and the discriminator costs, the momentum values were set to 0.1 and the learning rates were set to 0.1. 
We add a Gaussian noise with standard deviation of  to the input layer as described in \myappendix{input_noise}. 
The labeled examples were chosen at random but evenly distributed across the classes.

\parhead{Real-valued MNIST.} 
Here we describe the model architecture used for the semi-supervised learning experiments on the real-valued MNIST in \myfigg{disentangle}{b}.
The PixelCNN decoder uses both the vertical and horizontal stacks similar to~\citep{pixelcnn} and uses a discretized logistic mixture likelihood cost function with 10 logistic distribution as proposed in~\citep{pixelcnn++}.
The PixelCNN uses the location-invariant bias as described in \myappendix{conditioning_of_pixelcnn}.
The PixelCNN has 20 residual blocks (filter size of 2x3, 64 feature maps, gated \texttt{sigmoid-tanh} non-linearity as in~\citep{pixelcnn}). 
The adversarial discriminator has two layers of 1000 hidden units with \texttt{ReLU} activation function. 
The encoder architecture has three convolutional layers (filter size of 5, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2), followed by another three convolutional layers (filter size of 5, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2) with no fully-connected layer.
The last layer of the encoder  has the softmax activation function. All the convolutional layers of the encoder except the softmax layer use batch-normalization~\citep{batch}. 
On the latent representation, we impose a categorical distribution with uniform probabilities. 
The semi-supervised cost is the cross-entropy cost function at the output of . We use Adam~\citep{Adam} optimizer with learning rate of  for optimizing the PixelCNN cost and the cross-entropy cost, but we found it important to use the gradient descent with momentum algorithm for optimizing the generator and the discriminator costs of the adversarial network. 
For both of the generator and the discriminator costs, the momentum values were set to 0.1 and the learning rates were set to 0.1. After 150 epochs, we divide all the learning rates by 10.
We add a Gaussian noise with standard deviation of  to the input layer as described in \myappendix{input_noise}. 
The labeled examples were chosen at random but evenly distributed across the classes.







\subsection{SVHN Dataset}\label{appendix:svhn}
The SVHN dataset has about 530K training points and 26K test points. We use 10K points for the validation set.
Similar to~\citep{vat}, we downsample the images from  to  and then subtracte 127.5 from the data points and then divide them by 127.5. 

\subsubsection{Semi-Supervised SVHN}\label{appendix:semi-svhn}
Here we describe the model architecture used for the semi-supervised learning experiments on the SVHN dataset in \mysec{experiments:semi}.
The PixelCNN decoder uses both the vertical and horizontal stacks similar to~\citep{pixelcnn}. 
The cost function of the PixelCNN is a discretized logistic mixture likelihood cost function with 10 logistic distribution as proposed in~\citep{pixelcnn++}.
The PixelCNN uses the location-invariant bias as described in \myappendix{conditioning_of_pixelcnn} and has 20 residual blocks (filter size of 3x5, 32 feature maps, gated \texttt{sigmoid-tanh} non-linearity as in~\citep{pixelcnn}).
The adversarial discriminator has two layers of 1000 hidden units with \texttt{ReLU} activation function. 
The encoder architecture has two convolutional layers (filter size of 5, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2), followed by another two convolutional layers (filter size of 5, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2) with no fully-connected layer. The last layer of the encoder  has the softmax activation function. 
All the convolutional layers of the encoder except the softmax layer use batch-normalization~\citep{batch}. 
On the latent representation, we impose a categorical distribution with uniform probabilities. 
The semi-supervised cost is the cross-entropy cost function at the output of . We use Adam~\citep{Adam} optimizer for optimizing all the cost function. 
For the PixelCNN cost and the cross-entropy cost we use the learning rate of  and for the generator and the discriminator costs of the adversarial network we use the learning rate of .
We add a Gaussian noise with standard deviation of  to the input layer as described in \myappendix{input_noise}.

\subsection{NORB Dataset}\label{appendix:norb}
The NORB dataset has about 24K training points and 24K test points. We use 4K points for the validation set. This dataset has 5 object categories: animals, human figures, airplanes, trucks and cars. We downsample the images to have the size of , subtract 127.5 from the data points and then divide them by 127.5.

\subsubsection{Semi-Supervised NORB}\label{appendix:semi-norb}
The PixelCNN decoder uses both the vertical and horizontal stacks similar to~\citep{pixelcnn}. The cost function of the PixelCNN is a discretized logistic mixture likelihood cost function with 10 logistic distribution as proposed in~\citep{pixelcnn++}.
The PixelCNN uses the location-invariant bias as described in \myappendix{conditioning_of_pixelcnn} and has 15 residual blocks (filter size of 3x5, 32 feature maps, gated \texttt{sigmoid-tanh} non-linearity as in~\citep{pixelcnn}).
The adversarial discriminator has two layers of 1000 hidden units with \texttt{ReLU} activation function. 
The encoder architecture has a convolutional layer (filter size of 7, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2), followed by another convolutional layer (filter size of 7, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2), followed by another convolutional layer (filter size of 7, 32 feature maps, \texttt{ReLU} activation) and a max-pooling layer (pooling size 2) with no fully-connected layer. The last layer of the encoder  has the softmax activation function. 
All the convolutional layers of the encoder except the softmax layer use batch-normalization~\citep{batch}. 
On the latent representation, we impose a categorical distribution with uniform probabilities. 
The semi-supervised cost is the cross-entropy cost function at the output of . We use Adam~\citep{Adam} optimizer for optimizing all the cost function. 
For the PixelCNN cost and the cross-entropy cost we use the learning rate of  and for the generator and the discriminator costs of the adversarial network we use the learning rate of . 
We add a Gaussian noise with standard deviation of  to the input layer as described in \myappendix{input_noise}.
The labeled examples were chosen at random but evenly distributed across the classes. In the case of NORB with 1000 labels, the test error after 10 epochs is 12.97\%, after 100 epochs is 11.63\% and after 500 epochs is 8.17\%.

\end{appendices}
\end{document}
