
\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/qualitative-uncond}
\end{center}
\vspace{-5pt}
\caption{\textbf{\name~unconditional image generation results on ImageNet 256256 without classifier-free guidance.} \name~can generate images with both high fidelity and diversity without conditioning on any human annotations.}
\vspace{-10pt}
\label{fig:qualitative-uncond}
\end{figure*}

\begin{table}[t]
\caption{\textbf{Image generation performance on ImageNet 256256 without guidance.} \name~outperforms all class-conditional and class-unconditional baselines while requiring similar or less computational costs as later shown in \Cref{tab:computation}.}
\vspace{-15pt}
\label{tab:main}
\begin{center}{
\sisetup{detect-weight=true,detect-inline-weight=math}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{ l|S[table-format=3.3] S[table-format=3.2]}
\toprule
Methods w/o Guidance & {FID} & {Inception Score} \\
\midrule
\textit{ Class-conditional Generation} & \\
ADM \cite{dhariwal2021diffusion} & 10.94 & 101.0 \\
LDM-4 \cite{rombach2022high} & 10.56 & 103.5 \\
DiT-XL/2 \cite{peebles2023scalable} & 9.62 & 121.5 \\
BigGAN-deep \cite{brock2018large} & 6.95 & 198.2 \\
MDT-XL/2 \cite{gao2023masked} & 6.23 & 143.0 \\
MaskGIT \cite{chang2022maskgit} & 6.18 & 182.1 \\
CDM \cite{ho2022cascaded} & 4.88 & 158.7 \\
\midrule
\textit{ Class-unconditional Generation} & \\
BigGAN \cite{donahue2019large} & 38.61 & 24.7 \\
ADM \cite{dhariwal2021diffusion} & 26.21 & 39.7 \\
MaskGIT \cite{chang2022maskgit} & 20.72 & 42.1 \\
RCDM \cite{bordes2021high} & 19.0 & 51.9 \\
IC-GAN \cite{casanova2021instance} & 15.6 & 59.0 \\
ADDP \cite{tian2023addp} & 8.9 & 95.3 \\
MAGE-L \cite{li2023mage} &  7.04 & 123.5 \\
\textbf{\name-L} & \bfseries 3.56 & \bfseries 186.9  \\

\bottomrule
\end{tabular}
}
}
\end{center}
\vspace{-15pt}
\end{table}

\section{Results}

\subsection{Setup}
We evaluate \name~on ImageNet 256256 \cite{deng2009imagenet} which is a common benchmark dataset for image generation. We generate 50K images and report the Frechet Inception Distance (FID) \cite{heusel2017gans} and Inception Score (IS) \cite{salimans2016improved} as standard metrics to measure the fidelity and diversity of the generated images. The FID is measured against the ImageNet validation set. During the training of \name's pixel generator, the image is resized so that the smaller side is of length 256, and then randomly flipped and cropped to 256256. The input to the SSL encoder is further resized to 224224 to be compatible with its positional embedding size. For our main results, \name-L uses vision Transformers (ViT-L) \cite{vit} pre-trained with Moco v3 \cite{chen2021empirical} as the image encoder, a network with 12 blocks and 1536 hidden dimensions as the backbone of RDM, and MAGE-L \cite{li2023mage} as the image generator. The RDM is trained for 200 epochs with a constant learning rate and MAGE-L is trained for 800 epochs with cosine learning rate scheduling. More implementation details and hyper-parameters are provided in \Cref{sec:implementation}.

\subsection{Class-unconditional Generation}
\label{sec:main-results}

In \Cref{tab:main}, we compare \name~with state-of-the-art generative models on ImageNet 256256. Since traditional class-unconditional generation does not support either classifier or classifier-free guidance \cite{dhariwal2021diffusion, ho2022classifier}, all results in \Cref{tab:main} are reported without such guidance.

As shown in \Cref{fig:qualitative-uncond} and \Cref{tab:main}, \name~can generate images with both high fidelity and diversity, achieving 3.56 FID and 186.9 Inception Score, which significantly outperforms previous state-of-the-art class-unconditional image generation methods. Moreover, such a result also outperforms the previous state-of-the-art class-conditional generation method (4.88 FID achieved by CDM \cite{ho2022cascaded}), bridging the historical gap between class-conditional and class-unconditional generation. We further show in \Cref{sec:cond} that our representation diffusion model can effortlessly facilitate class-conditional representation generation, thereby enabling \name~to also adeptly perform class-conditional image generation. This result demonstrates the effectiveness of \name~and further highlights the great potential of self-conditioned image generation.


\begin{table}[t]
\caption{\textbf{Image generation performance on ImageNet 256256 with guidance.} \name~seamlessly enables classifier-free guidance for unconditional image generation, achieving results on par with state-of-the-art class-conditional generative models with guidance.}
\vspace{-15pt}
\label{tab:cfg}
\sisetup{detect-weight=true,detect-inline-weight=math}
\begin{center}{
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|S[table-format=3.4] S[table-format=3.2]}
\toprule
Methods w/ Guidance & {FID} & {Inception Score} \\
\midrule
\textit{ Class-conditional Generation} & \\
ADM-G, U \cite{dhariwal2021diffusion}  & 3.94 & 215.8 \\
LDM-4-G \cite{rombach2022high} & 3.60 & 247.7 \\
U-ViT-L-G \cite{bao2022all} & 3.40 & {-} \\
DiT-XL-G \cite{peebles2023scalable} & 2.27 & 278.2 \\
MDT-XL/2-G \cite{gao2023masked} & 1.79 & 283.0 \\
\midrule
\textit{ Class-unconditional Generation} & \\
\name-L-G & 3.31 & 253.4  \\
\bottomrule
\end{tabular}
}
}
\end{center}
\vspace{-10pt}
\end{table}

\subsection{Classifier-free Guidance}
\label{sec:cfg}

Traditional frameworks for class-unconditional image generation lack the ability to employ classifier guidance \cite{dhariwal2021diffusion} in the absence of class labels. Moreover, they are also incompatible with classifier-free guidance as the guidance itself is from unconditional generation. A significant advantage of \name~lies in its ability to integrate classifier-free guidance into its pixel generator. As shown in \Cref{tab:cfg}, \name's performance is notably improved by classifier-free guidance, reaching levels comparable to leading class-conditional image generation methods that utilize guidance. We also ablate our classifier-free guidance scale , as shown in \Cref{tab:cfg-scale}.  can both improve FID and IS, and a larger  keeps improving the Inception Score.

\begin{table}[t]
\caption{\textbf{FID and Inception Score with different classifier-free guidance scales .} The FID is stable for , while a larger  keeps improving the Inception Score.}
\vspace{-15pt}
\label{tab:cfg-scale}
\begin{center}{
\resizebox{0.48\textwidth}{!}{
\tablestyle{4pt}{1.05}
\begin{tabular}{l|ccccccccc}
\toprule
 & 0.0 & 1.0 & 2.0 & 3.0 & 4.0 & 5.0 & 6.0 & 7.0 \\
\midrule
FID & 3.56  & \textbf{3.29} & 3.37  & 3.44 & 3.31 & 3.33 & 3.31 & 3.39 \\
IS  & 186.9 & 228.5 & 242.4 & 251.3 & 250.5 & 252.7 & \textbf{253.4} & 252.6 \\
\bottomrule
\end{tabular}
}
}
\end{center}
\vspace{-10pt}
\end{table}

\begin{table*}[t]

\centering

\subfloat[
\textbf{Pre-training method.} \name~achieves good performance with encoders pre-trained with different contrastive learning methods.
\label{tab:pretrain-method}
]{
\centering
\begin{minipage}{0.27\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{y{48}z{18}z{18}}
Method & \multicolumn{1}{c}{FID} & \multicolumn{1}{c}{IS} \\
\shline
No condition &  14.23 & 57.7 \\
Moco v3 \cite{chen2021empirical} & \baseline{\textbf{5.07}} & \baseline{142.5} \\
DINO \cite{caron2021emerging} & 7.53 & \textbf{160.8} \\
iBOT \cite{zhou2021ibot} & 8.05 & 148.7 \\
\multicolumn{3}{c}{~}\\
\end{tabular}
\end{center}}\end{minipage}
}
\hspace{2em}
\subfloat[
\textbf{Model size.} \name~scales up with larger pre-trained encoders with better linear probing accuracy.
\label{tab:pretrain-size}
]{
\begin{minipage}{0.32\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{x{24}x{24}x{18}x{18}x{18}}
Model & params & lin. & FID & IS\\
\shline
ViT-S & 22M & 73.2 & 5.77 & 120.8 \\
ViT-B & 86M & 76.7 & \baseline{5.07} & \baseline{142.5} \\
ViT-L & 304M & 77.6 & \textbf{5.06} & \textbf{148.2} \\
\multicolumn{3}{c}{~}\\
\multicolumn{3}{c}{~}\\
\end{tabular}
\end{center}}\end{minipage}
}
\hspace{2em}
\subfloat[
\textbf{Projection dimension.} The dimensionality of the image representation is important in \name's performance.
\label{tab:pretrain-projdim}
]{
\begin{minipage}{0.27\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{x{56}x{18}z{18}}
Projection Dim & FID & \multicolumn{1}{c}{IS} \\
\shline
32 & 9.14 & 81.0 \\
64 & 6.09 & 119.2 \\
128 & 5.19 & \textbf{143.3} \\
256 & \baseline{\textbf{5.07}} & \baseline{142.5} \\
768 & 6.10 & 112.7 \\

\end{tabular}
\end{center}}\end{minipage}
}

\vspace{-1.2em}
\caption{\textbf{Pre-trained encoder ablation experiments on ImageNet 256256}. If not specified, the default pre-trained encoder is Moco v3 ViT-B with 256 projection dimension. Default settings are marked in \colorbox{baselinecolor}{gray}.}
\label{tab:ablations-pretrain}

\vspace{1.2em}
\centering

\subfloat[
\textbf{Model depth.} A deeper RDM can improve generation performance.
\label{tab:rdm-depth}
]{
\centering
\begin{minipage}{0.20\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{x{24}x{18}x{18}}
\#Blocks & FID & IS \\
\shline
3  & 7.53 & 113.5 \\
6  & 5.40 & 132.9 \\
12 & \baseline{\textbf{5.07}} & \baseline{\textbf{142.5}} \\
18 & 5.20 & 141.9 \\
24 & 5.13 & 141.5 \\

\end{tabular}
\end{center}}\end{minipage}
}
\hspace{2em}
\subfloat[
\textbf{Model width.} A wider RDM can improve generation performance.
\label{tab:rdm-width}
]{
\begin{minipage}{0.23\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{x{40}z{18}z{18}}
Hidden Dim & \multicolumn{1}{c}{FID} & \multicolumn{1}{c}{IS} \\
\shline
256  & 12.99 & 67.3 \\
512  & 9.07 & 99.8 \\
1024 & 5.35 & 132.0 \\
1536 & \baseline{\textbf{5.07}} & \baseline{142.5} \\
2048 & 5.09 & \textbf{142.8} \\

\end{tabular}
\end{center}}\end{minipage}
}
\hspace{2em}
\subfloat[
\textbf{Training epochs.} Training RDM longer can improve generation performance.
\label{tab:rdm-epochs}
]{
\begin{minipage}{0.18\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{x{24}x{18}x{18}}
Epochs & FID & IS \\
\shline
10 &  5.94 & 124.4 \\
50 & 5.21 & 138.3 \\
100 & \baseline{5.07} & \baseline{142.5} \\
200 & 5.07 & \textbf{145.1} \\
300 & \textbf{5.05} & 144.3 \\
\end{tabular}
\end{center}}\end{minipage}
}
\hspace{2em}
\subfloat[
\textbf{Diffusion steps.} More sampling steps can improve generation performance.
\label{tab:rdm-steps}
]{
\begin{minipage}{0.20\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{x{24}x{18}x{18}}
\#Steps & FID & IS \\
\shline
20 &  5.80 & 120.3 \\
50 & 5.28 & 133.0 \\
100 & 5.15 & 138.1 \\
250 & \baseline{\textbf{5.07}} & \baseline{142.5} \\
500 & \textbf{5.07} & \textbf{142.9} \\

\end{tabular}
\end{center}}\end{minipage}
}
\vspace{-.3em}
\caption{\textbf{RDM ablation experiments on ImageNet 256256}. If not specified, the default RDM backbone is of 12 blocks and 1536 hidden dimensions, trained for 100 epochs, and takes 250 sampling steps during generation. Default settings are marked in \colorbox{baselinecolor}{gray}.}
\label{tab:ablations-rdm}

\vspace{1.2em}
\centering
\subfloat[
\textbf{Conditioning.} Conditioning on generated representations improves over both unconditional and class-conditional baselines.
\label{tab:mage-conditioning}
]{
\begin{minipage}{0.27\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{y{48}z{18}z{18}}
Conditioning & \multicolumn{1}{c}{FID} & \multicolumn{1}{c}{IS} \\
\shline
No condition &  14.23 & 57.7 \\
Class label & 5.83 & 147.3 \\
Generated rep. & \baseline{\textbf{5.07}} & \baseline{\textbf{142.5}} \\
\textcolor{DarkGrey}{Oracle rep.} & \textcolor{DarkGrey}{4.37} & \textcolor{DarkGrey}{149.0} \\
\multicolumn{3}{c}{~}\\
\multicolumn{3}{c}{~}\\
\multicolumn{3}{c}{~}\\
\end{tabular}
\end{center}}\end{minipage}
}
\hspace{2em}
\subfloat[
\textbf{Pixel generator framework.} \name~consistently improves unconditional image generation performance with different generative models as its pixel generator.
\label{tab:mage-method}
]{
\begin{minipage}{0.35\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{y{64} @{\hskip -0.07in} S[table-format=3.3]@{\hskip -0.03in} l S[table-format=3.3] @{\hskip -0.08in} l}
Method & \multicolumn{1}{r}{FID} & & \multicolumn{1}{r}{IS} & \\
\shline
LDM \cite{li2023mage} & 39.13 & & 22.8 & \\
LDM+RDM & 9.08 & \textcolor{DarkGreen}{(30.05)} & 101.9 & \textcolor{DarkGreen}{(79.1)}  \\
\midrule
ADM \cite{dhariwal2021diffusion} & 26.21 & & 39.7 & \\
ADM+RDM & 7.21 & \textcolor{DarkGreen}{(19.00)} & 108.9 & \textcolor{DarkGreen}{(69.2)} \\
\midrule
MAGE \cite{li2023mage} &  8.67 & & 94.8 & \\
MAGE+RDM &  4.18 & \textcolor{DarkGreen}{(4.49)} & 177.8 & \textcolor{DarkGreen}{(83.0)} \\
\end{tabular}
\end{center}}\end{minipage}
}
\hspace{2em}
\subfloat[
\textbf{Training epochs.} Training the MAGE pixel generator longer can improve generation performance.
\label{tab:mage-epochs}
]{
\begin{minipage}{0.26\linewidth}{\begin{center}
\tablestyle{4pt}{1.05}
\begin{tabular}{x{24}x{18}x{18}}
Epochs & FID & IS \\
\shline
100 & 6.03 & 127.7 \\
200 & \baseline{5.07} & \baseline{142.5} \\
400 & 4.48 & 158.8 \\
800 & \textbf{4.15} & \textbf{172.0} \\
\multicolumn{3}{c}{~}\\
\multicolumn{3}{c}{~}\\
\multicolumn{3}{c}{~}\\
\end{tabular}
\end{center}}\end{minipage}
}
\vspace{-1.2em}
\caption{\textbf{Pixel generator ablation experiments on ImageNet 256256}. If not specified, the default pixel generator is MAGE-B trained for 200 epochs. In \Cref{tab:mage-method}, ADM+RDM is trained for 100 epochs, LDM+RDM is trained for 40 epochs, and MAGE+RDM is trained for 800 epochs. The LDM paper does not include class-unconditional generation results on ImageNet, so we report its re-implementation result in \cite{li2023mage}. Default settings are marked in \colorbox{baselinecolor}{gray}.}
\label{tab:ablations-pixel} \vspace{-7pt}
\end{table*}


\subsection{Ablations}
This section provides a comprehensive ablation study of the three core components of \name. Our default setup uses Moco v3 ViT-B as the pre-trained image encoder, an RDM with a 12-block, 1536-hidden-dimension backbone trained for 100 epochs, and a MAGE-B pixel generator trained for 200 epochs. The default setting is marked with \colorbox{baselinecolor}{gray} throughout \Cref{tab:ablations-pretrain,tab:ablations-rdm,tab:ablations-pixel}. Unless otherwise stated, all other properties and modules are set to the default settings during each component's individual ablation.

\noindent\textbf{Pre-trained Encoder.} We explore different pre-trained image encoder setup in \Cref{tab:ablations-pretrain}. \Cref{tab:pretrain-method} compares image encoders trained via various SSL methods (Moco v3, DINO, and iBOT), highlighting their substantial improvements over the unconditional baseline. Additionally, an encoder trained with DeiT \cite{touvron2021training} in a supervised manner also exhibits impressive performance (5.51 FID and 211.7 IS), indicating \name's adaptability to both supervised and self-supervised pre-training approaches.

\Cref{tab:pretrain-size} assesses the impact of model size on the pre-trained encoder. Larger models with better linear probing accuracy consistently enhance generation performance, although a smaller ViT-S model (22M parameters) still achieves decent results (5.77 FID and 120.8 IS).

We further analyze the effect of image representation dimensionality, using Moco v3 ViT-B models trained with different output dimensions from their projection head. \Cref{tab:pretrain-projdim} shows that neither excessively low nor high-dimensional representations are ideal -- too low dimensions lose vital image information, while too high dimensions pose challenges for the representation generator.

\noindent\textbf{Representation Generator.} \Cref{tab:ablations-rdm} ablates the representation diffusion model. The RDM's architecture consists of fully connected blocks, with the network's depth and width determined by the number of blocks and hidden dimensions. \Cref{tab:rdm-depth} and \Cref{tab:rdm-width} ablate these parameters, indicating an optimal balance at 12 blocks and 1536 hidden dimensions. Further, \Cref{tab:rdm-epochs} and \Cref{tab:rdm-steps} suggest that RDM's performance saturates at around 200 training epochs and 250 diffusion steps. Despite incurring only marginal computational costs, the RDM proves highly effective in generating SSL representations as evidenced in \Cref{tab:mage-conditioning}.

\noindent\textbf{Pixel Generator.} \Cref{tab:ablations-pixel} ablates \name's pixel generator. \Cref{tab:mage-conditioning} experiments with class-unconditional, class-conditional, and self-conditioned MAGE-B, assessing different conditioning during generation. Without any conditioning, the class-unconditional MAGE-B trained for 200 epochs yields only 14.23 FID and 57.7 IS. On the other hand, when conditioned on generated representations, MAGE-B achieves 5.07 FID and 142.5 IS, which significantly surpasses the class-unconditional baseline and further outperforms the class-conditional baseline in FID. This shows that representations could provide even more guidance than class labels. It is also quite close to the ``upper bound'' which is conditioned on oracle representations from ImageNet \textit{real} images during pixel generation, demonstrating the effectiveness of RDM in generating realistic SSL representations.

Prior works in self-conditioned image generation have primarily focused on categorizing images into clusters within the representation space, using these clusters as pseudo class-conditioning \cite{liu2020diverse, bao2022conditional, Hu_2023_CVPR}. We also evaluate the performance of this clustering-based conditioning in \name, employing -means within the Moco v3 ViT-B representation space to form 1000 clusters. Such conditioning achieves 6.60 FID and 121.9 IS, which falls short of the results achieved by conditioning on generated representations. This is because of the limited information contained within such discrete clusters, which is insufficient for providing detailed guidance for pixel generation. It is also important to note that this clustering approach relies on prior knowledge about the total number of classes, a piece of information that is often not available in general unlabeled datasets.

Conceptually, \name's pixel generator can integrate with various generative models. We validate this by testing ADM, LDM, and MAGE as pixel generators. As shown in \Cref{tab:mage-method}, conditioning on representations significantly improves the class-unconditional generation performance of all three generators. Additionally, \Cref{tab:mage-epochs} indicates that extending training epochs further improves performance, aligning with existing research \cite{dhariwal2021diffusion, rombach2022high, li2023mage}. These results show that \name~is a general self-conditioned image generation framework, seamlessly improving class-unconditional generation performance when combined with different modern generative models.

\subsection{Computational Cost}

In \Cref{tab:computation}, we present a detailed evaluation of \name's computational costs, including the number of parameters, training costs, and generation throughput. The training cost is measured using a cluster of 64 V100 GPUs. The generation throughput is measured on a single V100 GPU. As LDM and ADM measure their generation throughput on a single NVIDIA A100 \cite{rombach2022high}, we convert it to V100 throughput by assuming a 2.2 speedup of A100 vs V100 \cite{v100vsa100}.

\name-L~uses a pre-trained Moco v3 ViT-L encoder, an RDM with 12 blocks and 1536 hidden dimensions, and a MAGE-L pixel generator. The training phase involves 200 epochs for the RDM and 800 epochs for the MAGE-L. During the generation process, the RDM undergoes 250 diffusion steps, while MAGE-L performs 20 parallel decoding steps. We also report \name-B's computational costs and FID with less training costs and smaller number of parameters (Moco v3 ViT-B as image encoder, MAGE-B as pixel generator). Given that the Moco v3 ViT encoder is pre-trained and not needed for generation, its parameters and training costs are excluded. As indicated in the table, the RDM module adds only minor costs in comparison to the pixel generator. This demonstrates \name's compatibility with modern generative models, highlighting its ability to enhance generation performance with minimal computational burdens.

\begin{table}[t]
\caption{\textbf{Computational cost on ImageNet 256256}. \name~achieves a much smaller FID with similar or less computational cost as baseline methods.}
\vspace{-15pt}
\centering
\begin{center}{
\resizebox{0.48\textwidth}{!}{
\tablestyle{1.5pt}{1.05}
\begin{tabular}{y{48}|x{44}x{52}x{24}x{44}z{18}}
\toprule
Method & \#Params (M) & Training Cost (days) & Epochs & Throughput (samples/s) & \multicolumn{1}{c}{FID} \\
\midrule
LDM-8 \cite{rombach2022high} & 395 & 1.2 & 150 & 0.9 & 39.13 \\
ADM \cite{dhariwal2021diffusion} & 554 & 14.3 & 400 & 0.05 & 26.21 \\
MAGE-B \cite{li2023mage} & 176 & 5.5 & 1600 & 3.9 & 8.67 \\
MAGE-L \cite{li2023mage} & 439 & 10.7 & 1600 & 2.4 & 7.04 \\
\name-B & 63+176 & 0.3+0.8 & 100+200 & 3.6 &  5.07 \\
\name-B & 63+176 & 0.6+3.3 & 200+800 & 3.6 &  4.18 \\
\name-L & 63+439 & 0.3+1.5 & 100+200 & 2.2 &  4.23 \\
\name-L & 63+439 & 0.6+6.0 & 200+800 & 2.2 & 3.56 \\
\bottomrule
\end{tabular}
}}
\end{center}
\label{tab:computation} \vspace{-15pt}
\end{table}



\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/qualitative-recon}
\end{center}
\vspace{-10pt}
\caption{\textbf{\name~image generation results conditioned on representations extracted from images in ImageNet.} The generated images follow the same semantics as the original image but with diverse appearances.}
\label{fig:qualitative-recon}

\begin{center}
\vspace{-5pt}
\includegraphics[width=1.0\textwidth]{figures/qualitative-interpolate}
\end{center}
\vspace{-10pt}
\caption{\textbf{\name~image generation results conditioned on interpolated representations from two images.} The semantics of the interpolated images gradually transfer between the two images.}
\vspace{-2pt}
\label{fig:qualitative-interpolate}
\end{figure*}

\subsection{Qualitative Results}

\noindent\textbf{Representation Reconstruction.} \Cref{fig:qualitative-recon} illustrates \name's ability to generate images that align semantically with given representations. We extract SSL representations using examples from ImageNet 256256. For each representation, we generate a variety of images by varying the random seed for the generation process. The images generated by \name, while differing in specific details, consistently capture the semantic essence of the original images. This result highlights \name's capability to leverage semantic information in image representations to guide the generation process, without compromising the diversity that is important in unconditional image generation.

\noindent\textbf{Representation Interpolation.} Leveraging \name's dependency on representations, we can semantically transit between two images by linearly interpolating their respective representations. \Cref{fig:qualitative-interpolate} showcases such interpolation between pairs of ImageNet images. The interpolated images remain realistic across varying interpolation rates, and their semantic contents smoothly transition from one image to the other. This shows that the representation space of \name~is both smooth and semantically rich. This also demonstrates \name's potential in manipulating image semantics within a low-dimensional representation space, offering new possibilities to control image generation.