\RequirePackage{snapshot}
\documentclass[nospthms]{svjour3}                     



\def\makeheadbox{}  

\usepackage{microtype}
\usepackage[colorlinks,citecolor={blue},urlcolor={blue},linkcolor={blue}]{hyperref}
\usepackage{fix-cm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{cite}
\usepackage{braket}
\numberwithin{equation}{section}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}





\RequirePackage{amsmath}
\RequirePackage{amssymb}


\makeatletter
\newcounter{parenttheorem}\newenvironment{subassumptions}{\refstepcounter{theorem}\protected@edef\theparenttheorem{\thetheorem}\setcounter{parenttheorem}{\value{theorem}}\setcounter{theorem}{0}\def\thetheorem{\theparenttheorem\normalfont\alph{theorem}}\ignorespaces
}{\setcounter{theorem}{\value{parenttheorem}}\ignorespacesafterend
}
\makeatother

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\tild{\raisebox{-0.5ex}{\~{}}}

\def\minim{\mathop{\hbox{minimize}}}
\def\maxim{\mathop{\hbox{maximize}}}
\def\minimize#1{\displaystyle\minim_{#1}}
\def\maximize#1{\displaystyle\maxim_{#1}}
\def\st{\mathop{\hbox{subject to}}}
\def\argmin{\mathop{\hbox{arg\,min}}}
\def\argmax{\mathop{\hbox{arg\,max}}}

\makeatletter
\def\uncprob#1#2#3{\fbox
  {\begin{tabular*}{0.84\textwidth}
      {@{}l@{\extracolsep{\fill}}l@{\extracolsep{9pt}}l@{\extracolsep{\fill}}l@{}}
      #1 &  &  & 
    \end{tabular*}}}
\def\problem#1#2#3#4{\fbox
  {\begin{tabular*}{0.84\textwidth}
      {@{}l@{\extracolsep{\fill}}l@{\extracolsep{6pt}}l@{\extracolsep{\fill}}l@{}}
      #1 &  &  &  \5pt]
         &  &  & 
    \end{tabular*}}}
\def\probwide#1#2#3#4{\fbox
  {\begin{tabular*}{.98\textwidth}
      {@{}l@{\extracolsep{\fill}}l@{\extracolsep{6pt}}l@{\extracolsep{\fill}}l@{}}
      #1 &  &  &  \5pt]
         &  &  & 
    \end{tabular*}}}
\makeatother

\def\innerprod#1#2{\langle #1,#2\rangle}
\def\dps\displaystyle
\def\clink#1{\mkern2mu\overline{\mkern-2mu c\mkern2mu}\mkern-2mu\k(#1)}
\def\ul#1{\underline{#1}}
\def\Real{\mathbb{R}}
\def\Symm{\mathbb{S}}
\def\Complex{\mathbb{C}}
\def\m{\phantom-}
\def\pmat#1{\begin{pmatrix}#1\end{pmatrix}}
\def\bmat#1{\begin{bmatrix}#1\end{bmatrix}}
\def\comp#1#2{[#2]_{#1}}
\def\compi#1{[#1]_i}
\def\compj#1{[#1]_j}
\def\defd{\mathrel{\mathop:}=} \def\abs#1{|#1|}
\def\norm#1{\|#1\|}
\def\onenorm#1{\|#1\|_1}
\def\twonorm#1{\|#1\|_2}
\def\infnorm#1{\norm{#1}_\infty}
\def\blackslug{\hbox{\hskip 1pt \vrule width 4pt height 6pt depth 1.5pt
  \hskip 1pt}}
\def\cross{{\scriptscriptstyle\times}}
\def\cond{\hbox{\rm cond}}
\def\det{\mathop{\hbox{\rm det}}}
\def\diag{\operatorname{diag}}
\def\dim{\mathop{\hbox{\rm dim}}}  
\def\boundary{\mathop{\hbox{\rm bnd}}}
\def\interior{\mathop{\hbox{\rm int}}}
\def\Null{\mathop{\hbox{\rm null}}}
\def\rank{\mathop{\hbox{rank}}}
\def\op{\mathop{\hbox{\rm op}}}
\def\range{\mathop{\hbox{\rm range}}}
\def\sign{\operatorname{sign}}
\def\sgn{\operatorname{sgn}}
\def\Span{\mbox{\rm span}}
\def\trace{\mathop{\hbox{\rm trace}}}
\def\drop{^{\null}}
\def\float{fl}
\def\grad#1{\nabla_{#1}}
\def\Hess#1{\nabla^2_{#1}}
\def\one{\mathbf{1}}
\def\half{{\textstyle{\frac{1}{2}}}}
\def\third{{\textstyle{\frac{1}{3}}}}
\def\fourth{{\textstyle{\frac{1}{4}}}}
\def\inv{^{-1}}
\def\invsq{^{-2}}
\def\limk{\lim_{k\to\infty}}
\def\mystrut{\vrule height10.5pt depth5.5pt width0pt}
\def\spose#1{\hbox to 0pt{#1\hss}}
\def\sub#1{^{\null}_{#1}}
\def\text #1{\hbox{\quad#1\quad}}
\def\textt#1{\hbox{\qquad#1\qquad}}
\def\invT{^{-T}\!}
\def\T{^T\!}
\def\pT{^{+T}}
\def\xint{{x_{\rm int}}}

\def\pthinsp{\mskip  2   mu}    \def\pmedsp {\mskip  2.75mu}    \def\pthiksp{\mskip  3.5 mu}    \def\nthinsp{\mskip -2   mu}
\def\nmedsp {\mskip -2.75mu}
\def\nthiksp{\mskip -3.5 mu}

\def\Asubk{A\sub{\nthinsp k}}
\def\Bsubk{B\sub{\nthinsp k}}
\def\Fsubk{F\sub{\nmedsp k}}
\def\Gsubk{G\sub{\nthinsp k}}
\def\Hsubk{H\sub{\nthinsp k}}
\def\HsubB{H\sub{\nthinsp  \scriptscriptstyle{B}}}
\def\HsubS{H\sub{\nthinsp  \scriptscriptstyle{S}}}
\def\HsubBS{H\sub{\nthinsp \scriptscriptstyle{BS}}}
\def\Hz{H\sub{\nthinsp \scriptscriptstyle{Z}}}
\def\Jsubk{J\sub{\nmedsp k}}
\def\Psubk{P\sub{\nthiksp k}}
\def\Qsubk{Q\sub{\nthinsp k}}
\def\Vsubk{V\sub{\nmedsp k}}
\def\Vsubone{V\sub{\nmedsp 1}}
\def\Vsubtwo{V\sub{\nmedsp 2}}
\def\Ysubk{Y\sub{\nmedsp k}}
\def\Wsubk{W\sub{\nthiksp k}}
\def\Zsubk{Z\sub{\nthinsp k}}
\def\Zsubi{Z\sub{\nthinsp i}}

\def\submax{_{\max}}
\def\submin{_{\min}}
\def\subminus{_{\scriptscriptstyle -}}
\def\subplus {_{\scriptscriptstyle +}}

\def\A{_{\scriptscriptstyle A}}
\def\B{_{\scriptscriptstyle B}}
\def\C{_{\scriptscriptstyle C}}
\def\D{_{\scriptscriptstyle D}}
\def\E{_{\scriptscriptstyle E}}
\def\F{_{\scriptscriptstyle F}}
\def\G{_{\scriptscriptstyle G}}
\def\H{_{\scriptscriptstyle H}}
\def\I{_{\scriptscriptstyle I}}
\def\J{_{\scriptscriptstyle J}}
\def\K{_{\scriptscriptstyle K}}
\def\L{_{\scriptscriptstyle L}}
\def\M{_{\scriptscriptstyle M}}
\def\N{_{\scriptscriptstyle N}}
\def\O{_{\scriptscriptstyle O}}
\def\P{_{\scriptscriptstyle P}}
\def\Q{_{\scriptscriptstyle Q}}
\def\R{_{\scriptscriptstyle R}}
\def\U{_{\scriptscriptstyle U}}
\def\V{_{\scriptscriptstyle V}}
\def\W{_{\scriptscriptstyle W}}
\def\Y{_{\scriptscriptstyle Y}}
\def\Z{_{\scriptscriptstyle Z}}
\def\k{_k}
\def\kp#1{_{k+#1}}
\def\km#1{_{k-#1}}
\def\j{_j}
\def\jp#1{_{j+#1}}
\def\jm#1{_{j-#1}}
\def\subb{\B}
\def\subc{\C}
\def\subf{\F}
\def\subh{\H}
\def\subk{\K}
\def\subm{\M}
\def\subn{\N}
\def\subp{\P}
\def\subs{_{\scriptscriptstyle S}}
\def\subt{_{\scriptscriptstyle T}}
\def\subr{\R}
\def\subz{\Z}

\def\alphabar{\bar \alpha}
\def\alphahat{\skew3\hat \alpha}
\def\alphatilde{\skew3\tilde \alpha}
\def\betabar{\skew{2.8}\bar\beta}
\def\betahat{\skew{2.8}\hat\beta}
\def\betatilde{\skew{2.8}\tilde\beta}
\def\deltabar{\bar\delta}
\def\deltilde{\skew5\tilde \delta}
\def\deltatilde{\deltilde}
\def\etabar{\bar\eta}
\def\gammabar{\bar\gamma}
\def\lambar{\bar\lambda}
\def\lamhat{\skew{2.8}\hat \lambda}
\def\lambdahat{\lamhat}
\def\lambdatilde{\tilde{\lambda}}
\def\lambdabar{\bar \lambda}
\def\mubar{\skew3\bar \mu}
\def\muhat{\skew3\hat \mu}
\def\mutilde{\skew3\tilde\mu}
\def\nubar{\skew3\bar\nu}
\def\nuhat{\skew3\hat\nu}
\def\nutilde{\skew3\tilde\nu}
\def\omegabar{\bar\omega}
\def\omegahat{\skew3\hat\omega}
\def\omegatilde{\tilde\omega}
\def\phibar{\bar\phi}
\def\phitilde{\skew3\widetilde\phi}
\def\pibar{\bar\pi}
\def\pihat{\skew1\widehat \pi}
\def\sigmabar{\bar\sigma}
\def\sigmatilde{\widetilde\sigma}
\def\rhobar{\bar\rho}
\def\rhohat{\widehat\rho}
\def\taubar{\bar\tau}
\def\tautilde{\tilde\tau}
\def\tauhat{\hat\tau}
\def\thetabar{\bar\theta}
\def\xibar{\skew4\bar\xi}

\def\abar{\skew3\bar a}
\def\ahat{\skew2\widehat a}
\def\atilde{\skew2\widetilde a}
\def\Abar{\skew7\bar A}
\def\Ahat{\widehat A}
\def\Atilde{\widetilde A}
\def\bbar{\skew3\bar b}
\def\bhat{\skew2\widehat b}
\def\btilde{\skew2\widetilde b}
\def\Bbar{\bar B}
\def\Bhat{\widehat B}
\def\cbar{\skew3\bar c}
\def\chat{\skew3\widehat c}
\def\ctilde{\widetilde c}
\def\Cbar{\bar C}
\def\Chat{\widehat C}
\def\Ctilde{\widetilde C}
\def\dbar{\bar d}
\def\dhat{\widehat d}
\def\dtilde{\widetilde d}
\def\Dbar{\bar D}
\def\Dhat{\widehat D}
\def\Dtilde{\widetilde D}
\def\ehat{\skew3\widehat e}
\def\ebar{\bar e}
\def\Ebar{\bar E}
\def\Ehat{\widehat E}
\def\Etilde{\widetilde E}
\def\fbar{\bar f}
\def\fhat{\widehat f}
\def\ftilde{\widetilde f}
\def\Fbar{\bar F}
\def\Fhat{\widehat F}
\def\Ftilde{\widetilde F}
\def\gbar{\skew{4.3}\bar g}
\def\ghat{\skew{3}\widehat g}
\def\gtilde{\skew{4.5}\widetilde g}
\def\Gbar{\bar G}
\def\Ghat{\widehat G}
\def\hbar{\skew{4.2}\bar h}
\def\hhat{\skew2\widehat h}
\def\htilde{\skew3\widetilde h}
\def\Hbar{\skew5\bar H}
\def\Hhat{\widehat H}
\def\Htilde{\widetilde H}
\def\Ibar{\skew5\bar I}
\def\Itilde{\widetilde I}
\def\Jbar{\skew6\bar J}
\def\Jhat{\widehat J}
\def\Jtilde{\widetilde J}
\def\kbar{\skew{4.4}\bar k}
\def\Khat{\widehat K}
\def\Kbar{\skew{4.4}\bar K}
\def\Ktilde{\widetilde K}
\def\ellbar{\bar \ell}
\def\lhat{\skew2\widehat l}
\def\lbar{\skew2\bar l}
\def\Lbar{\skew{4.3}\bar L}
\def\Lhat{\widehat L}
\def\Ltilde{\widetilde L}
\def\mbar{\skew2\bar m}
\def\mhat{\widehat m}
\def\Mbar{\skew{4.4}\bar M}
\def\Mhat{\widehat M}
\def\Mtilde{\widetilde M}
\def\Nbar{\skew{4.4}\bar N}
\def\Ntilde{\widetilde N}
\def\nbar{\skew2\bar n}
\def\pbar{\skew2\bar p}
\def\phat{\skew2\widehat p}
\def\ptilde{\skew2\widetilde p}
\def\Pbar{\skew5\bar P}
\def\Phat{\widehat P}
\def\Ptilde{\skew5\widetilde P}
\def\qbar{\bar q}
\def\qhat{\skew2\widehat q}
\def\qtilde{\widetilde q}
\def\Qbar{\bar Q}
\def\Qhat{\widehat Q}
\def\Qtilde{\widetilde Q}
\def\rbar{\skew3\bar r}
\def\rhat{\skew3\widehat r}
\def\rtilde{\skew3\widetilde r}
\def\Rbar{\skew5\bar R}
\def\Rhat{\widehat R}
\def\Rtilde{\widetilde R}
\def\sbar{\bar s}
\def\shat{\widehat s}
\def\stilde{\widetilde s}
\def\Shat{\widehat S}
\def\Sbar{\skew2\bar S}
\def\tbar{\bar t}
\def\ttilde{\widetilde t}
\def\that{\widehat t}
\def\Tbar{\bar T}
\def\That{\widehat T}
\def\Ttilde{\widetilde T}
\def\ubar{\skew3\bar u}
\def\uhat{\skew3\widehat u}
\def\utilde{\skew3\widetilde u}
\def\Ubar{\skew2\bar U}
\def\Uhat{\widehat U}
\def\Utilde{\widetilde U}
\def\vbar{\skew3\bar v}
\def\vhat{\skew3\widehat v}
\def\vtilde{\skew3\widetilde v}
\def\Vbar{\skew2\bar V}
\def\Vhat{\widehat V}
\def\Vtilde{\widetilde V}
\def\wbar{\skew3\bar w}
\def\what{\skew3\widehat w}
\def\wtilde{\skew3\widetilde w}
\def\Wbar{\skew3\bar W}
\def\What{\widehat W}
\def\Wtilde{\widetilde W}
\def\xbar{\bar{x}}
\def\xhat{\skew{2.8}\widehat x}
\def\xtilde{\skew3\widetilde x}
\def\Xhat{\widehat X}
\def\ybar{\skew3\bar y}
\def\yhat{\skew2\widehat y}
\def\ytilde{\skew3\widetilde y}
\def\Ybar{\skew2\bar Y}
\def\Yhat{\widehat Y}
\def\zbar{\skew{2.8}\bar z}
\def\zhat{\skew{2.8}\widehat z}
\def\ztilde{\skew{2.8}\widetilde z}
\def\Zbar{\skew5\bar Z}
\def\Zhat{\widehat Z}
\def\Ztilde{\widetilde Z}

\def\lamstar{\lambda^*}
\def\lamstark{\lambda^*\k}
\def\lambarstark{\lambar^*\k}
\def\pistar{\pi^*}
\def\pistark{\pi^*_k}
\def\rhostar{\rho^*}
\def\mustar{\mu^*}
\def\taustar{\tau^*}
\def\cstar{c^*}
\def\dstar{d^*}
\def\gstar{g^*}
\def\Jstar{J^*}
\def\pstar{p^*}
\def\rstar{r^*}
\def\rstark{r^*\k}
\def\vstar{v^*}
\def\vstark{v^*\k}
\def\wstar{w^*}
\def\wstark{w^*\k}
\def\xstar{x^*}
\def\xstarhatk{\hat x^*\k}
\def\xstark{x^*\k}
\def\xstarzero{x^*_0}
\def\xstarkm1{x^*\km1}
\def\ystar{y^*}
\def\ystarhatk{\hat y^*\k}
\def\ystark{y^*\k}
\def\ystarkm1{y^*\km1}
\def\zstar{z^*}
\def\zstark{z^*\k}

\def\mit{\mathit}
\def\Deltait{{\mit \Delta}}
\def\Gammait{{\mit \Gamma}}
\def\Lambdait{{\mit \Lambda}}
\def\Sigmait{{\mit \Sigma}}
\def\Lambarit{\skew5\bar{\mit \Lambda}}
\def\Omegait{{\mit \Omega}}
\def\Omegaitbar{\skew5\bar{\mit \Omega}}
\def\Thetait{{\mit \Theta}}
\def\Piitbar{\skew5\bar{\mit \Pi}}
\def\Piit{{\mit \Pi}}
\def\Phiit{{\mit \Phi}}

\def\lscr{\ell}
\def\Ascr{\mathcal{A}}
\def\Bscr{\mathcal{B}}
\def\Cscr{\mathcal{C}}
\def\Escr{\mathcal{E}}
\def\Fscr{\mathcal{F}}
\def\Dscr{\mathcal{D}}
\def\Iscr{\mathcal{I}}
\def\Jscr{\mathcal{J}}
\def\Kscr{\mathcal{K}}
\def\Lscr{\mathcal{L}}
\def\Mscr{\mathcal{M}}
\def\Nscr{\mathcal{N}}
\def\Oscr{\mathcal{O}}
\def\Pscr{\mathcal{P}}
\def\Qscr{\mathcal{Q}}
\def\Rscr{\mathcal{R}}
\def\Sscr{\mathcal{S}}
\def\Tscr{\mathcal{T}}
\def\Uscr{\mathcal{U}}
\def\Vscr{\mathcal{V}}
\def\Wscr{\mathcal{W}}
\def\Zscr{\mathcal{Z}}

\def\dd{\Deltait d}
\def\dx{\Deltait x}
\def\dy{\Deltait y}
\def\dz{\Deltait z}

\def\Matlab{\textsc{Matlab}}
\newcommand{\code}[1]{{\small#1}}
\def\KNOSSOS{\code{KNOSSOS}}
\def\MINOS  {\code{MINOS}}
\def\LANCE  {\code{LANCELOT}}
\def\SNOPT  {\code{SNOPT}}
\def\SLQP   {SQP}
\def\QPSOL  {\code{QPSOL}}
\def\LUSOL  {\code{LUSOL}}
\def\LSSOL  {\code{LSSOL}}
\def\GAMS   {\code{GAMS}}
\def\AMPL   {\code{AMPL}}
\def\COPS   {\code{COPS}}
\def\CUTE   {\code{CUTE}}
\def\CUTEr  {\code{CUTEr}}
\def\ADIC   {\code{ADIC}}
\def\CONOPT {\code{CONOPT}}
\def\NEOS   {\code{NEOS}}
\def\cpp    {C\raisebox{0.1ex}{\hbox{\small++}}}

\def\Sec{\S}
\def\seq#1{\ensuremath{\{#1\}}}
\newcommand{\e}[1]{e\raisebox{.2ex}{\tiny}} \newcommand{\ozgur}{{\"O}zg{\"u}r Y{\i}lmaz}  % \usepackage{boxedminipage}
\usepackage{color}
\definecolor{softblue}{rgb}{0.90,0.92,1.00}
\makeatletter\newenvironment{btheorem}{\begin{lrbox}{\@tempboxa}\begin{minipage}{0.97\textwidth}\begin{theorem}}{\end{theorem}\end{minipage}\end{lrbox}\par\hbox{}\noindent {\setlength{\fboxsep}{0pt}\colorbox{softblue}{\setlength{\fboxsep}{4pt}\begin{boxedminipage}{\textwidth}\usebox{\@tempboxa}\end{boxedminipage}}}\vspace{0.5\baselineskip}}
\makeatother
\makeatletter\newenvironment{blemma}{\begin{lrbox}{\@tempboxa}\begin{minipage}{0.97\textwidth}\begin{lemma}}{\end{lemma}\end{minipage}\end{lrbox}\par\hbox{}\noindent {\setlength{\fboxsep}{0pt}\colorbox{softblue}{\setlength{\fboxsep}{4pt}\begin{boxedminipage}{\textwidth}\usebox{\@tempboxa}\end{boxedminipage}}}\vspace{0.5\baselineskip}}
\makeatother
\makeatletter\newenvironment{bproposition}{\begin{lrbox}{\@tempboxa}\begin{minipage}{0.97\textwidth}\begin{proposition}}{\end{proposition}\end{minipage}\end{lrbox}\par\hbox{}\noindent {\setlength{\fboxsep}{0pt}\colorbox{softblue}{\setlength{\fboxsep}{4pt}\begin{boxedminipage}{\textwidth}\usebox{\@tempboxa}\end{boxedminipage}}}\vspace{0.5\baselineskip}}
\makeatother
\def\expval{\mathbb{E}}
\def\var{\mathbb{V}}
\def\sample{\Sscr}
\def\gs{g_{\scriptscriptstyle\sample}}
\def\phis{\phi_{\scriptscriptstyle\sample}}

\title{Robust inversion, dimensionality reduction, \\and randomized sampling\thanks{This work was in part financially supported by the
    Natural Sciences and Engineering Research Council of Canada
    Discovery Grant (22R81254) and the Collaborative Research and
    Development Grant DNOISE II (375142-08). This research was carried
    out as part of the SINBAD II project with support from the
    following organizations: BG Group, BPG, BP, Chevron, Conoco
    Phillips, Petrobras, PGS, Total SA, and WesternGeco.}
}


\author{Aleksandr Aravkin
  \\\hbox{Michael P. Friedlander}
  \\\hbox{Felix J. Herrmann}
  \\\hbox{Tristan van Leeuwen}}

\authorrunning{A.\@ Aravkin, M. P. Friedlander, F. J. Herrmann, and
  T. van Leeuwen} 

\institute{A. Aravkin, F. J. Herrmann, and T. van Leeuwen \at
              Dept. of Earth and Ocean Sciences, University of British Columbia, Vancouver, BC, Canada \\\email{\{saravkin,fherrmann,tleeuwen\}@eos.ubc.ca}
          \and
              M. P. Friedlander \at Dept. of Computer Science, University of British Columbia, Vancouver, BC, Canada \\\email{mpf@cs.ubc.ca}
}

\date{March 1, 2012}

\begin{document}

\maketitle

\begin{abstract}
  We consider a class of inverse problems in which the forward model
  is the solution operator to linear ODEs or PDEs. This class admits
  several di\-men\-sionality-reduction techniques based on data averaging
  or sampling, which are especially useful for large-scale
  problems. We survey these approaches and their connection to
  stochastic optimization. The data-averaging approach is only viable,
  however, for a least-squares misfit, which is sensitive to outliers
  in the data and artifacts unexplained by the forward model. This
  motivates us to propose a robust formulation based on the Student's
  t-distribution of the error. We demonstrate how the corresponding
  penalty function, together with the sampling approach, can obtain
  good results for a large-scale seismic inverse problem with 50\%
  corrupted data.












  \keywords{inverse problems \and seismic inversion
    \and stochastic optimization \and robust estimation}
\end{abstract}

\section{Introduction}
\label{sec:intro}

Consider the generic parameter-estimation scheme in which we conduct 
experiments, recording the corresponding experimental input vectors
 and observation vectors .
We model the data for given parameters  by

where observation  is obtained by the linear action of the
forward model  on known source parameters , and
independent errors  capture
the discrepancy between  and prediction
. The class of models captured by this representation
includes solution operators to any linear (partial) differential
equation with boundary conditions, where the  are the
right-hand sides of the equations.  A special case arises when , i.e., the forward model is the same for each experiment.


Inverse problems based on these forward models arise in a variety of
applications, including medical imaging and seismic exploration, in
which the parameters  usually represent particular physical
properties of a material. We are particularly motivated by the
full-waveform inversion (FWI) application in seismology, which is used
to image the earth's subsurface~\cite{Tarantola1984}.  In
full-waveform inversion, the forward model  is the solution
operator of the wave equation composed with a restriction of the full
solution to the observation points (receivers);  represents
sound-velocity parameters for a (spatial) 2- or 3-dimensional mesh;
the vectors  encode the location and signature of the th
source experiment; and the vectors  contain the corresponding
measurements at each receiver.  A typical survey in exploration
seismology may contain thousands of experiments (shots), and global
seismology relies on natural experiments provided by measuring
thousands of earthquakes detected at seismic stations around the
world.  Standard data-fitting algorithms may require months of CPU
time on large computing clusters to process this volume of data and
yield coherent geological information.

Inverse problems based on the forward models that
satisfy~\eqref{eq:Fi} are typically solved by minimizing some measure
of misfit, and have the general form

where each  is some measure of the residual

between the observation and prediction of the th experiment.  The
classical approach is based on the least-squares penalty

This choice can be interpreted as finding the maximum
  likelihood (ML) estimate of , given the
assumptions that the errors  are independent and follow a
Gaussian distribution.

Formulation~\eqref{eq:5} is general enough to capture a variety of models,
including many familiar examples. If the  and  are
scalars, and the forward model is linear, then standard least-squares

easily fits into our general formulation.
More generally, ML estimation is based on the
form

where  is a particular probability density function of .



\subsection{Dimensionality reduction} \label{sec:dr}

Full-waveform inversion is a prime example of an application in which
the cost of evaluating each element in the sum of  is very
costly: every residual vector ---required to evaluate one
element in the sum of \eqref{eq:5}---entails solving a partial
differential equation on a 2D or 3D mesh with thousands of grid points
in each dimension.  The scale of such problems is a motivation for
using dimensionality reduction techniques that address small portions
of the data at a time.

The least-squares objective~\eqref{eq:15} allows for a powerful form
of data aggregation that is based on randomly fusing groups of
experiments into ``meta'' experiments, with the effect of
reducing the overall problem size. The aggregation scheme is based on Haber et
al.'s~\cite{haber10} observation that for this choice of penalty, the
objective is connected to the trace of a residual matrix. That is, we can
represent the objective of~\eqref{eq:5} by

where

collects the residual vectors~\eqref{eq:2}. Now consider a small
sample of  weighted averages of the data, i.e.,

where  and  are random variables, and collect the
corresponding  residuals 
into the matrix . Because the residuals are linear in the data, we can
write compactly

Thus, we may consider the sample function

based on the  averaged residuals.
Proposition~\ref{prop:generalSampling} then follows directly from
Hutchinson's~\cite[\S2]{Hutchinson:1990} work on stochastic trace
estimation.
\begin{bproposition}
\label{prop:generalSampling}
If , then

\end{bproposition}

Hutchinson proves that if the weights  are drawn independently
from a Rademacher distribution, which takes the values  with
equal probability, then the stochastic-trace estimate has minimum
variance. Avron and Toledo~\cite{AvronToledo:2011} compare the quality
of stochastic estimators obtained from other distributions. Golub and
von Matt~\cite{GoluMatt:1991} report the surprising result that the
estimate obtained with even a single sample () is often of high
quality. Experiments that use the approach in FWI give evidence that
good estimates of the true parameters can be obtained at a fraction of
the computational cost required by the full
approach~\cite{Krebs09,leeuwen2011,HFY:2011}.

\subsection{Approach}

Although the least-squares approach enjoys widespread use, and
naturally accommodates the dimensionality-reduction technique just
described, it is known to be unsuitable for non-Gaussian errors,
especially for cases with very noisy or corrupted data often encountered in practice.
The least-squares formulation also breaks down in the face of systematic features of the data that
are unexplained by the model .

Our aim is to characterize the benefits of robust inversion and to
describe randomized sampling schemes and optimization algorithms
suitable for large-scale applications in which even a single evaluation
of the forward model and its action on  is computationally
expensive. (In practice, the product  is evaluated as a
single unit.) We interpret these sampling schemes, which include
the well-known incremental-gradient
algorithm~\cite{nedic2000convergence}, as dimensionality-reduction
techniques, because they allow algorithms to make progress using only
a portion of the data.

This paper is organized into the following components:

{\it Robust statistics} (\S\ref{sec:mle}). We survey robust approaches
from a statistical perspective, and present a robust approach based on
the heavy-tailed Student's t-distribution.  We show that all
log-concave error models share statistical properties that
differentiate them from heavy-tailed densities (such as the Student's
t) and limit their ability to work in regimes with large outliers or
significant systematic corruption of the data.  We demonstrate that
densities outside the log-concave family allow extremely robust
formulations that yield reasonable inversion results even in the face
of major data contamination.

{\it Sample average approximations} (\S\ref{sec:sampling}). We propose
a dimensionality-reduction technique based on sampling the available
data, and characterize the statistical properties that make it
suitable as the basis for an optimization algorithm to solve the
general inversion problem~\eqref{eq:5}. These techniques can be used
for the general robust formulation described in \S\ref{sec:mle}, and
for formulations in which forward models  vary with .

{\it Stochastic optimization} (\S\ref{sec:semistochastic}) We review
stochastic-gradient, randomized in\-cremental-gradient, and
sample-average methods. We show how the assumptions required by each
method fit with the class of inverse problems of interest, and can be
satisfied by the sampling schemes discussed in \S\ref{sec:sampling}.

{\it Seismic inversion} (\S\ref{sec:full-wavef-invers}) We test the
proposed sample-average approach on the robust formulation of the FWI
problem. We compare the inversion results obtained with the new
heavy-tailed approach to those obtained using robust log-concave
models and conventional methods, and demonstrate that a useful
synthetic velocity model can be recovered by the heavy-tailed robust
method in an extreme case with 50\% missing data.  We also compare the
performance of stochastic algorithms and deterministic approaches, and
show that the robust result can be obtained using only 30\% of the
effort required by a deterministic approach.


\section{Robust Statistics}
\label{sec:mle}

A popular approach in robust regression is to replace the
least-squares penalty~\eqref{eq:15} on the residual with a penalty
that increases more slowly than the 2-norm. (Virieux and
Operto~\cite{VirieuxOperto2009} discuss the difficulties with
least-squares regression, which are especially egregious in seismic
inversion.)


One way to derive a robust approach of this form is to assume that the
noise  comes from a particular non-Gaussian probability
density, , and then find the maximum likelihood (ML)
estimate of the parameters  that maximizes the
likelihood that the residual vectors  are realizations of the
random variable , given the observations . Because
the negative logarithm is monotone decreasing, it is natural to
minimize the negative log of the likelihood function rather than
maximizing the likelihood itself. In fact, when the distribution of
the errors  is modeled using a log-concave density  with a convex loss function ,
the ML estimation problem is equivalent to the
formulation~\eqref{eq:5}, with


One could also simply start with a penalty  on , without
explicitly modelling the noise density; estimates obtained this way
are generally known as M-estimates~\cite{Huber:1981}.  A popular
choice that follows this approach is the Huber
penalty~\cite{Huber:1981,HubRon,Mar}.

Robust formulations are typically based on convex penalties ---or
equivalently, on log-concave densities for ---that look quadratic near  and increase linearly far from zero.
Robust penalties, including the 1-norm and Huber, for electromagnetic
inverse problems are discussed by Farquaharson and Oldenburg
in~\cite{GJI:GJI555}.  Guitton and Symes~\cite{Symes2003} consider the
Huber penalty in the seismic context, and they cite many previous
examples of the use of 1-norm penalty in geophysics. Huber and 1-norm
penalties are further compared on large-scale seismic problems by
Brossier et al.~\cite{Brossier2010}, and a Huber-like (but strictly
convex) hyperbolic penalty is described by Bube and
Nemeth~\cite{Bube2007}, with the aim of avoiding possible
non-uniqueness associated with the Huber penalty.

Clearly, practitioners have a preference for convex formulations.
However, it is important to note that
\begin{itemize}
\item for nonlinear forward models , the optimization
  problem~\eqref{eq:5} is typically nonconvex even for convex
  penalties  (it is difficult to satisfy the compositional
  requirements for convexity in that case);
\item even for linear forward models , it may be beneficial to
  choose a nonconvex penalty in order to guard against
  outliers in the data.
\end{itemize}
We will justify the second point from a statistical
perspective. Before we proceed with the argument, we introduce the
Student's t-density, which we use in designing our robust method for
FWI.






\subsection{Heavy-tailed distribution: Student's t}
Robust formulations using the Student's t-distribution have been shown
to outperform log-concave formulations in various
applications~\cite{AravkinThesis2010}.  In this section, we introduce
the Student's t-density, explain its properties, and establish a
result that underscores how different heavy-tailed distributions are
from those in the log-concave family.

The scalar Student's t-density function with mean  and
positive degrees-of-freedom parameter  is given by

The density is depicted in Figure~\ref{fig:distributions}(a).
The parameter  can be understood by recalling the origins of the
Student's t-distribution.  Given  i.i.d. Gaussian variables 
with mean , the normalized sample mean

follows the Student's t-distribution with , where the
sample variance  is
distributed as a  random variable with  degrees of
freedom.  As , the
characterization~\eqref{StudentChar} immediately implies that the
Student's t-density converges pointwise to the density of
. Thus,  can be interpreted as a tuning parameter: for
low values one expects a high degree of non-normality, but as 
increases, the distribution behaves more like a Gaussian
distribution. This interpretation is highlighted
in~\cite{Lange1989}. 

For a zero-mean Student's t-distribution (), the log-likelihood
of the density~\eqref{eq:st_pdf} gives rise to the nonconvex penalty
function

which is depicted in Figure~\ref{fig:distributions}(b).
The nonconvexity of this penalty is equivalent to the sub-exponential
decrease of the tail of the Student's t-distribution, which goes to 
at the rate  as .

The significance of these so-called \emph{heavy tails} in outlier
removal becomes clear when we consider the following question:
Given that a scalar residual deviates from the mean by more than ,
what is the probability that it actually deviates by more than ?

The 1-norm is the slowest-growing convex penalty, and is induced by
the Laplace distribution, which is proportional to
.  A basic property of the scalar Laplace
distribution is that it is memory free. That is, given a Laplace
distribution with mean , then the probability relationship

holds for all . Hence, the probability that a scalar
residual is at least  away from the mean, given that it is at
least  away from the mean, decays exponentially fast with .
For large , it is unintuitive to make such a strong claim for a
residual already known to correspond to an outlier.

Contrast this behavior with that of the Student's t-distribution.
When , the Student's t-distribution is simply the Cauchy
distribution, with a density proportional to .  Then we
have that

Remarkably, the conditional probability is independent of  for
large residuals. This cannot be achieved with any probability density
arising from a convex penalty, because~\eqref{memoryFree} provides a
lower bound for this family of densities, as is shown in the following
theorem.





\begin{btheorem}
\label{UniversalLowerBound}
Consider any scalar density  arising from a symmetric proper closed convex
penalty  via , and take any point  with positive right derivative
 .
Then for all , the conditional tail
distribution induced by  satisfies

\end{btheorem}
\begin{proof}
  Define  to be the (global)
  linear under-estimate for  at , where
   is the right derivative of  at
  . Define . We first note
  that  is log-concave (apply~\cite[Theorem 3]{Prekopa1971},
  taking the set ).  Then  is
  concave, and so its derivative
  
  is non-increasing. Therefore, the ratio  is nondecreasing,
   and in particular
  
  By assumption on the functions  and ,
  
  which implies that
  
To complete the proof, note that the right derivative
 is nondecreasing~\cite[Theorem 24.1]{RTR}. Then we
have  for .
\end{proof}




For differentiable log-concave densities, the
\emph{influence function} is defined to be , and for a
general distribution it is the derivative of the negative log of the
density. These functions provide further insight into the difference
between the behaviors of log-concave densities and heavy-tailed
densities such as the Student's. In particular, they measure the
effect of the size of a residual on the negative log likelihood.
The Student's t-density has a so-called {\it redescending} influence
function: as residuals grow larger, they are effectively ignored by
the model.  Figure~\ref{fig:distributions} shows the relationships
among densities, penalties, and influence functions of two
log-concave distributions (Gaussian and Laplacian) and those of the
Student's t, which is not log-concave. If we examine the derivative

of the Student's t-penalty \eqref{eq:19}, it is clear that large
residuals have a small influence when . For small , on
the other hand, the derivative resembles that of the least-squares
penalty.  See Hampel et al.~\cite{Hampel} for a discussion of
influence-function approaches to robust statistics, and redescending
influence functions in particular, and Shevlyakov et
al.~\cite{Shevlyakov2008} for further connections.
\begin{figure}[t]
  \centering
  \begin{tabular}{@{}ccc@{}}
  \includegraphics[width=.3\linewidth]{scripts/dist_pdf}&
  \includegraphics[width=.3\linewidth]{scripts/dist_pen}&
  \includegraphics[width=.3\linewidth]{scripts/dist_inf}
  \\{\small (a)}&{\small (b)}&{\small (c)}
  \end{tabular}
  \caption{The Gaussian ({\small}), Laplace ({\small}), and Student's t-
    ({\small ---}) distributions: (a) densities, (b) penalties, and (c)
    influence functions.}
  \label{fig:distributions}
\end{figure}

There is an implicit tradeoff between convex and non-convex penalties
(and their log-concave and non-log-concave counterparts). Convex
models are easier to characterize and solve, but may be wrong in a
situation in which large outliers are expected.
Nonconvex penalties are particularly useful with large outliers.

\begin{figure}
  \centering
\subfloat[True model residual and solution]{\label{fig:fwiresids_true}
    \includegraphics[width=.38\textwidth]{./robust/res}
    \begin{picture}(0,0)
      \put(-145,40){\rotatebox{90}{Frequency}}
    \end{picture}
    \qquad
    \includegraphics[width=.38\textwidth]{./robust/mt}}
  \1pt]
\subfloat[Huber residual and solution]{\label{fig:fwiresids_huber}
    \includegraphics[width=.38\textwidth]{./robust/reshb}
    \begin{picture}(0,0)
      \put(-145,40){\rotatebox{90}{Frequency}}
    \end{picture}
    \qquad
    \includegraphics[width=.38\textwidth]{./robust/mhb}}
  \
 \phi(x) = \frac1m\sum_{i=1}^m\phi_i(x)
 \text{and}
 \nabla\phi(x) = \frac1m\sum_{i=1}^m \nabla\phi_i(x),
\label{eq:sample-phi}
 \phis(x) = \frac1{s}\sum_{i\in\sample}\phi_i(x)
 \text{and}
 \nabla\phis(x) = \frac1{s}\sum_{i\in\sample}\nabla\phi_i(x),
\label{eq:g-sample-avg}
\expval[\phis(x)] = \phi(x) \text{and} \expval[\nabla\phis(x)] = \nabla\phi(x).
 \label{eq:sample-e}
 e = \nabla\phis - \nabla\phi
 \label{eq:var-sample-avg}
\expval\big[\norm{e}^2\big] = \var\big[ \norm{\nabla\phis} \big].
 \label{FiniteGradientSampleVariance}
 \sigma_g^2 := \frac{1}{m-1}\sum_{i=1}^m \norm{\nabla\phi_i - \nabla\phi}^2
 \label{WithoutReplacementVariance}
  \expval[\norm{e_n}^2] = \frac1s\left(1 - \frac{s}{m}\right) \sigma^2_g\,;

  \label{WithReplacementVariance}
  \expval[\norm{e_r}^2] = \frac{1}{s}\sigma^2_g.

\expval[\norm{e_n}^2] = \left(1-\frac{s}{m}\right)\expval[\norm{e_r}^2],

\phi\W(x) = \frac1s\sum_{j=1}^s\phitilde_i(x),
\text{with}
\phitilde_i(x) := \|R(x)w_i\|^2,
e_w = \nabla\phi\W -
\phi
\expval[\norm{e_w}^2] = \frac1s\sigmatilde_g^2,

 \begin{aligned}
   \sigmatilde_g^2
   &\leq \expval\left[\|\nabla\phitilde_{i}\|^2\right]
 \\&=
   4\expval
   \left[\,
     \left\|
       \left( \sum_i w_i \nabla  r_i(x)  \right)
       \left( \sum_i w_i r_i(x)         \right)
     \right\|^2
   \right]
 \\&\leq
  4 \expval
   \left[\,
     \left(\left\|
        \sum_i w_i \nabla  r_i(x) \right\|_2
\left\|       \sum_i w_i r_i(x)
     \right\|\right)^2
   \right]
\\&\leq
  4 \expval
   \left[\,
     \left(\sum_i\left\|
        w_i \nabla  r_i(x)  \right\|_2
\sum_i\left\|      w_i r_i(x)
     \right\|\right)^2
   \right]
\\ &=
  4 \expval
   \left[\,
     \left(\sum_i|w_i|\left\|
         \nabla  r_i(x)  \right\|_2
\sum_i |w_i|\left\|      r_i(x)
     \right\|\right)^2
   \right]
 \\&\leq
 4\max_i m^2\|\nabla r_i(x)\|_2^2\cdot\max_i\|r_i(x)\|^2
  \expval \bigg[\sum_{ij}w_i^2w_j^2\bigg]\;.
 \end{aligned}
 
  \expval[\norm{e_n}^2] < \expval[\norm{e_w}^2]
  \quad
  \hbox{for all  large enough.}

  \label{eq:sgIter}
  x\kp1 = x\k - \alpha\k d\k \text{with} d\k := s\k + e\k,

\lim_{k\to\infty}\nabla \phi(x\k)=0,

 \norm{\nabla\phi(x)-\nabla\phi(y)}\le L\norm{x-y}
  \text{for all  and ;}
 \label{eq:stoch-reqs}

\label{eq:6}
  \sum_{k=0}^\infty \alpha\k=\infty
  \text{and}
  \sum_{k=0}^\infty \alpha_k^2<\infty.

d\k = \nabla{\phis}(x\k).

s\k = \nabla\phi(x\k) \text{and} e\k = \nabla\phis(x\k) - \nabla\phi(x\k).

 \frac{\mu}2\norm{x\k-\xstar}^2 \le \phi(x\k) - \phi(\xstar),
\label{eq:9}
  \expval[\norm{x\k - \xstar}] = \Oscr(1/k).

  \label{eq:4}
  x\kp1 = x\k - \alpha\k\nabla\phi_{i_k}(x\k),

\norm{\nabla\phi(\xbar)}=\Oscr\big(\inf_k\alpha\k\big).

\expval[\norm{x\k-\xstar}^2] \le \Oscr([1-\mu/L]^k) + \Oscr(m/L).

  \label{eq:10}
  x\kp1 = x\k - \alpha g\k, \quad \alpha = 1/L,

  \label{eq:11}
  g\k = \nabla\phi(x\k) + e\k

    \label{eq:12}
    \expval[\norm{x\k-\xstar}^2] \le \Oscr([1-\mu/L]^k) + \Oscr(C\k),
  
  \label{eq:16}
  H\k p=g\k,

 g\k = \frac1{s\k}\sum_{i\in\sample\k}\phi_i(x\k)

 \norm{\nabla\phi_i(x)}^2 \le \beta_1 + \beta_2\norm{\nabla\phi(x)}^2
 \quad
 \hbox{for all  and }

  \label{eq:14}
  

  A_{\omega}(x)u = [\omega^2 x + \nabla^2]u = q,

  F(x) = PA^{-1}(x),

 \phi(x)=\sum_{i=1}^{m}\rho(r_i(x))
 \text{and}
 \nabla\phi(x)=\sum_{i=1}^m \nabla F(x,q_i)^*\nabla\rho(r_i(x)),

\nabla F(x, q_i)^*y = G(x,u_i)^*v_i,

A(x)u_i = q_i \text{and} A(x)^*v_i = Py.

 \rho(r) = \sum_i \zeta_i,
 \text{where}
 \zeta_i = \begin{cases}
      r_i^2/{2\mu} & \hbox{if }
    \\|r_i|-\mu/2  & \hbox{otherwise.}
  \end{cases}

 \rho(r)=\sum_i\log(1+r_i^2/\nu).

 x_{k+1} = x_k - \alpha_k p\k,

\Delta x\k := x\kp1 - x\k
\text{and}
\Delta g\k := g\kp1 - g\k;

s\kp1 = \min\{\, m,\,s\k + 1\,\}.

The members of the batch are redrawn at every iteration, and we use an
Armijo backtracking linesearch based on the sampled function
.

The convergence plots for several runs of the sampling method and the
stochastic gradient method with  are shown in
Figure~\ref{fig:fwiconvergence}(a).
Figure~\ref{fig:fwiconvergence}(b) plots the evolution of the amounts
of data sampled.




\section{Discussion and conclusions}
\label{sec:disc-concl}

The numerical experiments we have conducted using the Student's t-penalty are
encouraging, and indicate that this approach can overcome some of the
limitations of convex robust penalties such as the Huber norm. Unlike the
least-squares and Huber penalties, the Student t-penalty does not
force the residual into a shape prescribed by the corresponding
distribution. The sampling method successfully combines the
steady convergence rate of the full-gradient method with the
inexpensive iterations provided by the incremental-gradient method.



The convergence analysis of the sampling method, based on
Theorem~\ref{th:g-w-error}, relies on bounding the second moment of
the error in the gradient, and hence the variance of the sample
average (see~\eqref{eq:var-sample-avg}). The bound on the
second-moment arises because of our reliance on the concept of an
\emph{expected} distance to optimality
. However, other probabilistic measures
of distance to optimality may be more appropriate; this would
influence our criteria for bounding the error in the gradient. For
example, Avron and Toledo~\cite{AvronToledo:2011} measure the quality
of a sample average using an ``epsilon-delta'' argument that provides
a bound on the sample size needed to achieve a particular accuracy
 with probability .

Other refinements are possible. For example, van den Doel and
Ascher~\cite{AschervdDoel:2011} advocate an adaptive approach for
increasing the sample size, and Byrd et al.~\cite{byrd:2011} use a
sample average-approximation of the
Hessian.

\bibliographystyle{siam}
\bibliography{seg2011}

\end{document}
