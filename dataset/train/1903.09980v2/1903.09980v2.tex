\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow,tabulary,graphicx,subcaption,wrapfig}
\usepackage{booktabs}
\usepackage[english]{babel}
\usepackage{tabularx} 

\newcommand{\lyc}[1]{{\color{green}{\bf\sf [YC: #1]}}}
\newcommand{\junz}[1]{{\color{blue}{\bf\sf [jz: #1]}}}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{2613} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi
\begin{document}

\title{Supplementary Material:\\
Cluster Alignment with a Teacher for Unsupervised Domain Adaptation}

\author{Zhijie Deng, Yucen Luo, Jun Zhu\thanks{Corresponding author.}\\
Dept. of Comp. Sci. \& Tech., Institute for AI, BNRist Lab, THBI Lab, Tsinghua University\\
{\tt\small \{dzj17, luoyc15\}@mails.tsinghua.edu.cn, dcszj@tsinghua.edu.cn}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\appendix

\section{Class-conditional cluster structure on more tasks}

First, we visualize the learned feature spaces of CAT, RevGrad [7] and MSTN [49] on the imbalanced \emph{SVHN} to \emph{MNIST} task using t-SNE [27], as shown in Fig.\ref{afig:1}. It is obvious that CAT can force the samples from the same class to concentrate together to form tighter clusters than those of RevGrad and MSTN, and the clusters present strip pattern in the 2-D space. CAT can also align the class-conditional distributions of the source and the target domains correctly. However, RevGrad and MSTN tend to align the `0' images in \emph{SVHN} with the `1' images in \emph{MNIST}, thus the learned feature spaces of them are confusing and not discriminative. This visualization verifies the results in Table. 1.

Second, we plot the feature spaces learned by CAT+rRevGrad and RevGrad on \emph{MNIST} to \emph{USPS} and \emph{USPS} to \emph{MNIST} tasks in Fig.~\ref{afig:2} using t-SNE [27]. CAT+rRevGrad can deliver more discriminative feature spaces with separable and tight class-conditional clusters. Therefore, it is sufficient to use the first-order statistics based matching loss  to match the class-conditional distributions of the two domains. The aligned clusters of the source and the target domains also verify the effectiveness of the loss .

Furthermore, we examine the feature space learned by CAT on more challenging tasks in \emph{Office-31} dataset and \emph{ImageCLEF-DA} dataset, and results are demonstrated in Fig.~\ref{afig:5}. These features are outputs of AlexNet trained with rRevGrad+CAT. The class-conditional distributions are shaped to be tight and separable clusters, and the corresponding cluters from the source domain and the target domain are aligned. Therefore, CAT can achieve the objectives of discriminative learning and class-conditional alignment, thus can perform well on the extensive experiments on \emph{Office-31} and \emph{ImageCLEF-DA} datasets.


\begin{figure}[t]
\vspace{-0.4cm}
\centering
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{catim.pdf}
  \caption{CAT}
  \label{afig:im-3}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{rgim.pdf}
  \caption{RevGrad}
  \label{afig:im-1}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{mstnim.pdf}
  \caption{MSTN}
  \label{afig:im-2}
\end{subfigure}
\vspace{-0.2cm}
\caption{(Best viewed in color.) Feature space learned on imbalanced \emph{SVHN} to \emph{MNIST} task. Green, red, blue and orange points represent `0' images from \emph{SVHN}, `1' images from \emph{SVHN}, `0' images from \emph{MNIST} and `1' images from \emph{MNIST}, respectively.}
\vspace{-0.4cm}
\label{afig:1}
\end{figure}

\begin{figure}[t]
\centering
\vspace{-0.3cm}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{m2urg.pdf}
  \caption{RevGrad}
  \label{afig:2-1}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{m2ucat.pdf}
  \caption{rRevGrad+CAT}
  \label{afig:2-2}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{u2mrg.pdf}
  \caption{RevGrad}
  \label{afig:3-1}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{u2mcat.pdf}
  \caption{rRevGrad+CAT}
  \label{afig:3-2}
\end{subfigure}
\vspace{-0.2cm}
\caption{(Best viewed in color.) Feature space learned on \emph{MNIST} to \emph{USPS} (Fig.~\ref{afig:2-1} and Fig.~\ref{afig:2-2}) and \emph{USPS} to \emph{MNIST} (Fig.~\ref{afig:3-1} and Fig.~\ref{afig:3-2}) tasks. Blue violet denotes the source domain and the other colors denote different classes of target domain.}
\label{afig:2}
\end{figure}




\begin{figure}[t]
\vspace{-0.cm}
\centering
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{a2wcat.pdf}
  \caption{Amazon to Webcam}
  \label{afig:5-1}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{a2dcat.pdf}
  \caption{Amazon to DSLR}
  \label{afig:5-2}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{p2icat.pdf}
  \caption{p to i}
  \label{afig:5-3}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{i2pcat.pdf}
  \caption{i to p}
  \label{afig:5-4}
\end{subfigure}
\vspace{-0.2cm}
\caption{(Best viewed in color.) Feature space learned on four challenging tasks. Blue violet (in (a) and (b)) and deep sky blue (in (c) and (d)) denote the source domain and the other colors denote different classes of target domain.}\vspace{-0.3cm}
\label{afig:5}
\end{figure}

\begin{figure}[t]
\vspace{-0.3cm}
\centering
\begin{subfigure}{0.22\textwidth}
  \centering
  \includegraphics[width=\linewidth]{latex/d2ajsd.pdf}\vspace{-0.1cm}
  \caption{DSLR to Amazon}
  \label{afig:4-1}
\end{subfigure}
\begin{subfigure}{0.22\textwidth}
  \centering
  \includegraphics[width=\linewidth]{latex/a2djsd.pdf}\vspace{-0.1cm}
  \caption{Amazon to DSLR}
  \label{afig:4-2}
\end{subfigure}

\caption{Jensen-Shannon divergence (JSD) curves during training.}
\label{afig:4}
\end{figure}

\begin{figure}[t]

\centering
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{s2mearly.pdf}\vspace{-0.1cm}
  \caption{\emph{SVHN} to \emph{MNIST}}
  \label{afig:7-1}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{a2dearly.pdf}\vspace{-0.1cm}
  \caption{Amazon to DSLR}
  \label{afig:7-2}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{p2iearly.pdf}\vspace{-0.1cm}
  \caption{p to i}
  \label{afig:7-3}
\end{subfigure}
\vspace{-0.2cm}
\caption{(Best viewed in color.) Feature space in the early stages of training. \textbf{Different} from the above feature spaces, blue violet (in (a) and (b)) and deep sky blue (in (c)) denote the \textbf{target} domain and the other colors denote different classes of \textbf{source} domain.}
\label{afig:7}
\end{figure}

\begin{figure}[t]

\centering
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{s2mcr.pdf}\vspace{-0.1cm}
  \caption{\emph{SVHN} to \emph{MNIST}}
  \label{afig:6-1}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{a2dcr.pdf}\vspace{-0.1cm}
  \caption{Amazon to DSLR}
  \label{afig:6-2}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
  \centering
  \includegraphics[width=\linewidth]{p2icr.pdf}\vspace{-0.1cm}
  \caption{p to i}
  \label{afig:6-3}
\end{subfigure}
\vspace{-0.2cm}
\caption{The selection rate of the confidence-thresholding technique on different tasks.}
\label{afig:6}
\end{figure}


\section{Quantitative estimate of the divergence between domains}
When aligning the source domain and target domain via the combination of RevGrad and CAT, the loss  which is maximized w.r.t. the critic  can be viewed as a lower bound of  (see [9] for the details) where  denotes the Jensen-Shannon divergence between distributions. Therefore, we plot  to quantitatively estimate the divergence between the two domains, following [49]. The results are shown in Fig.~\ref{afig:4} and we use the AlexNet as the classifier here. CAT can boost RevGrad significantly, leading to faster and better convergence. This group of experiments verifies that when combining CAT with the marginal distribution alignment approaches, it can provide a discriminative class-conditional alignment and bias the existing approaches to align the cluster-structure marginal distributions better.


\section{Verification of confidence-thresholding technique}
Since the source classification loss and the source discriminative clustering loss can produce strong gradients and converge quickly, the discriminative cluster structure will form in the source domain in the early stages of training. However, the classifier has not been adapted for the target domain, so a notable part of the target features will lie in the gaps between the source clusters and have low classification confidence. Therefore, the marginal alignment approaches may easily map these features into incorrect clusters, as stated in Sec. 3.2.3. To address this problem, we propose the confidence-thresholding technique which includes the fine-level structure information into marginal alignment approaches. We claim that in the training procedure, the discriminative class-conditional alignment between the two domains forms gradually, so more and more samples are going to be selected into the marginal alignment training. Here we prove these through experiments on tasks in \emph{SVHN-MNIST-USPS}, \emph{Office-31} and \emph{ImageCLEF-DA}. At first, we train the RevGrad+CAT models on the three tasks with limited iterations (\ie, 2000 iterations on \emph{SVHN} to \emph{MNIST} task, 100 iterations on Amazon to DSLR task and 100 iterations on p to i task) and plot the feature spaces of them in Fig~\ref{afig:7}. Obviously, a notable part of target samples lie in the gaps between the source clusters, especially on the \emph{SVHN} to \emph{MNIST} task which has large source and target domains. Then, we train the rRevGrad+CAT models on these tasks following the same settings, and we plot the selection rate of the confidence-thresholding technique w.r.t. the number of iterations in Fig.~\ref{afig:6}. When using this technique, we note that the selection rate monotonically increases with the number of iterations and after several thousands of iterations, the selection rate will be almost  on the Amazon to DSLR and p to i tasks. On \emph{SVHN} to \emph{MNIST} task, we use a ramp-up function  as  after 5000 iterations, suggested by related SSL works. Therefore, after around 15000 iterations, the discriminative clustering structure forms, and then the samples are pushed far away from the decision boundaries. So almost all the samples will have confidence more than  and will be selected into the domain adversarial training.

\section{Convergence}
To inspect how CAT converges, we plot the test accuracy with respect to the number of iterations in Fig.~\ref{fig:4}. On the two adaptation tasks using AlexNet, CAT shows similar convergence rate with RevGrad [7] but better performance.
\begin{figure}[t]
\centering
\begin{subfigure}{0.22\textwidth}
  \centering
  \includegraphics[width=\linewidth]{d2a.pdf}
  \caption{DSLR to Amazon}
  \label{fig:4-1}
\end{subfigure}
\begin{subfigure}{0.22\textwidth}
  \centering
  \includegraphics[width=\linewidth]{a2d.pdf}
  \caption{Amazon to DSLR}
  \label{fig:4-2}
\end{subfigure}
\vspace{-0.3cm}
\caption{Test accuracy curves.}\vspace{-0.6cm}
\label{fig:4}
\end{figure}



\section{Experimental details}
On digits adaptation tasks, we use the simple LeNet with Batch Normalization after the convolutional layers and use the probability logits as features for adaptation, following [49, 44]. When combining with RevGrad [7] and rRevGrad, the critic model has a  architecture.

On more challenging tasks, we conduct experiments based on the AlexNet [15] and ResNet-50 [12] equipped with 256-D bottleneck layers after the  and  layers respectively (following [24, 49]). We use the features outputted by the bottleneck layers as image representations for adaptation and use a three-layer critic with  architecture. We finetune all the layers before the bottleneck layers in AlexNet and ResNet-50 and train the bottleneck layers and the classification layers via back propagation.

We use the stochastic gradient descent with 0.9 momentum with an annealed learning rate  where p changes from 0 to 1 in the training progress [7, 49] when using LeNet and AlexNet as the classifiers. The learning rate for finetuned layers is set to be the ten percent of that for layers trained from scratch. We use batches with 128 elements in experiments using LeNet, batches with 200 elements in experiments using AlexNet and batches with 36 elements in experiments using ResNet-50.

We use the same architectures and optimization settings (\eg, batch size, learning rate, optimizer and weight decay) as those of the original methods [41, 37] when combining CAT with them.

The pseudo labels are not initialized randomly. Specifically, in the first 5000 iterations, we pre-train CAT by setting . During this, the classifier is trained to fit source data but won't overfit, thus its implicit ensemble can perform well on some target samples and provide a reliable initial set of pseudo labels. Then, we ramp-up Î± to activate the clustering and alignment losses to impose conditional alignment.

\end{document}
