\documentclass[sigconf]{acmart}
\pagenumbering{gobble}
\settopmatter{printacmref=false}




\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}














\usepackage{adjustbox}
\usepackage{colortbl}
\usepackage{pgfplots}
\usepackage{multirow}
\definecolor{Gray}{gray}{0.88}
\definecolor{DarkGray}{gray}{0.8}
\definecolor{LightGray}{gray}{0.93}
\definecolor{Green}{RGB}{95,150,105}
\definecolor{Purple}{RGB}{122,95,156}
\definecolor{LightBlue}{RGB}{99,150,216}
\definecolor{LightRed}{RGB}{217,133,148}
\pgfplotsset{compat=1.8}
\usepgfplotslibrary{statistics}
\usepackage{color,soul}

\usepackage{dblfloatfix} 

\begin{document}

\acmYear{2023}\copyrightyear{2023}
\setcopyright{acmlicensed}
\acmConference[ICCPS '23]{ACM/IEEE 14th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2022)}{May 9--12, 2023}{San Antonio, TX, USA}
\acmBooktitle{ACM/IEEE 14th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2022) (ICCPS '23), May 9--12, 2023, San Antonio, TX, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3576841.3585933}
\acmISBN{979-8-4007-0036-1/23/05}

\title{Pishgu: Universal Path Prediction Network Architecture for Real-time Cyber-physical Edge Systems}

\author{Ghazal Alinezhad Noghre}
\email{galinezh@uncc.edu}
\affiliation{\institution{University of North Carolina at Charlotte}
\state{North Carolina}
  \country{USA}
}
\authornote{Both authors contributed equally to this research.}

\author{Vinit Katariya}
\authornotemark[1]
\email{vkatariya@uncc.edu}
\affiliation{\institution{University of North Carolina at Charlotte}
\state{North Carolina}
  \country{USA}
}

\author{Armin Danesh Pazho}
\affiliation{\institution{University of North Carolina at Charlotte}
\city{Charlotte}
  \state{North Carolina}
  \country{USA}}
\email{adaneshp@uncc.edu}

\author{Christopher Neff}
\affiliation{\institution{University of North Carolina at Charlotte}
\city{Charlotte}
  \state{North Carolina}
  \country{USA}}
\email{cneff@uncc.edu}

\author{Hamed Tabkhi}
\affiliation{\institution{University of North Carolina at Charlotte}
\city{Charlotte}
  \state{North Carolina}
  \country{USA}}
\email{htabkhiv@uncc.edu}



\begin{abstract}
Path prediction is an essential task for many real-world Cyber-Physical Systems (CPS) applications, from autonomous driving and traffic monitoring/management to pedestrian/worker safety. These real-world CPS applications need a robust, lightweight path prediction that can provide a universal network architecture for multiple subjects (e.g., pedestrians and vehicles) from different perspectives. However, most existing algorithms are tailor-made for a unique subject with a specific camera perspective and scenario. This article presents Pishgu, a universal lightweight network architecture, as a robust and holistic solution for path prediction. Pishgu's architecture can adapt to multiple path prediction domains with different subjects (vehicles, pedestrians), perspectives (bird's-eye, high-angle), and scenes (sidewalk, highway). Our proposed architecture captures the inter-dependencies within the subjects in each frame by taking advantage of Graph Isomorphism Networks and the attention module. We separately train and evaluate the efficacy of our architecture on three different CPS domains across multiple perspectives (vehicle bird's-eye view, pedestrian bird's-eye view, and human high-angle view). Pishgu outperforms state-of-the-art solutions in the vehicle bird's-eye view domain by 42\% and 61\% and pedestrian high-angle view domain by 23\% and 22\% in terms of ADE and FDE, respectively. Additionally, we analyze the domain-specific details for various datasets to understand their effect on path prediction and model interpretation. Finally, we report the latency and throughput for all three domains on multiple embedded platforms showcasing the robustness and adaptability of Pishgu for real-world integration into CPS applications.
\end{abstract}





\keywords{path prediction, graph isomorphism, cyber-physical systems, deep learning}








\maketitle
\pagestyle{plain}
\input{tex/Introduction}
\vspace*{-0.3cm}

\section{Related Work}\label{sec:RelatedWork}

Deep Learning based prediction architectures are rapidly becoming an integral part of modern edge-based CPS \cite{Zhou2021wide_atten, Isern2020Reconfig, deniz2022efficient}. The Transportation CPS (TCPS) uses deep learning for predicting traffic flow density \cite{Jeong2013traffFlow, chen2017research}, work zone safety frameworks \cite{sabeti2021toward, fang2018detecting, cai2020context}, intelligent traffic monitoring \cite{Chen2020WITM}, and autonomous driving \cite{cui2019multimodal, hussain2022vision}. The industrial CPS (ICPS) and city-based CPS (CCPS)  use these prediction models for smart surveillance \cite{Isern2020Reconfig, wu2019deep, pazho2023ancilia}, human/pedestrian/worker safety \cite{deniz2022efficient, wang2022intelligent}, anomaly detection and prediction \cite{jones2014anomaly, pustokhina2021automated}, and path planning \cite{farooq2021flow}. 

Path prediction algorithms are vital to the decision-making process in TCPS, ICPS, and CCPS feedback systems. They are used in safety applications for predicting future actions and potential positions, autonomous vehicles for avoiding obstacles and defining a safe path and velocity, and anomaly detection to predict future anomalous behaviors from past trajectories. These algorithms are used at the local edge nodes for real-time detection and prediction \cite{Isern2020Reconfig, sanchez2021real, jeon2020scale}. However, as the path prediction architectures are often designed for specific domains and subjects in focus, they struggle to adapt to changes in the domain. In the following, we review existing path prediction approaches in three primary domains and their {challenges}: (1) Vehicle Bird's-eye View, (2) Pedestrian Bird's-eye View, and (3) Pedestrian High-angle View.

\begin{figure*}[!b]
        \centering
               \includegraphics[width=1\linewidth, trim={20px 200px 20px 200px}, clip]{Figs/Drawing1.pdf}
                
                \caption{Pishgu formulation visualization. The input $N_i$ refers to a vector of size $T_{in}\times2$ for each node/subject where $T_{in}$ is the input window size. $\Delta N_i$ is the relative vector for each node. $fc$ refers to a fully-connected layer. The output is a $n\times T_{out}\times 2$ vector, with $T_{out}$ being the output window size. Best seen in color.}
                                
                \label{fig:pishgu}
                \vspace{-10pt}
\end{figure*}

\subsection{Vehicle Bird's-eye View Path Prediction}
The bird's-eye view is utilized by TCPS \cite{chwa2018closing, katariya2021deeptrack} and ICPS \cite{lian2020cyber} frameworks to generate a local map of the environment to comprehend the interdependencies of subjects moving at high speeds. Many approaches focus on predicting the future path of vehicles in a bird's-eye view setting on highways \cite{GSTCN2022, STALSTM2021, 2020gripplus}. These methods use real-world coordinates and measure the error in meters. CS-LSTM \cite{CSlstm18}, a pioneering paper in vehicle path prediction, used Long Short-term Memory (LSTM) encoder-decoder model with convolutional social pooling. GRIP++ \cite{2020gripplus}, an extension to \cite{grip2019}, uses fixed and dynamic graphs with an LSTM encoder-decoder model to grasp the surrounding dynamics and predict the trajectories of the vehicles in a scene. STA-LSTM \cite{STALSTM2021} utilizes Spatio-temporal attention along with LSTM to predict a vehicles' future path and increase the explainability of the predictions. \cite{Pip2021} incorporates the ego vehicle's planned path into the prediction of the surrounding vehicles' future paths to enhance predictions in an autonomous vehicle setting. \cite{GSAN} uses a graph self-attention network to understand both spatial and temporal interactions among the many vehicles in a scene and is used for both path prediction and lane-changing classification. In \cite{HEAT}, a three-channel framework with a heterogeneous edge-enhanced graph attention network is proposed to deal with the inherent heterogeneity of the different vehicles in a given scene. DeepTrack \cite{katariya2021deeptrack} introduces temporal and depthwise convolutions to provide a more robust encoding of vehicle dynamics while reducing computation and parameters, resulting in a faster, lighter-weight network with competitive accuracy. \cite{GSTCN2022} proposed recently, utilizes a graph-based spatial-temporal convolutional network and a gated recurrent unit to predict future vehicle paths. They also propose a weighted adjacency matrix for understanding the intensity of influence between different vehicles.



\subsection{Pedestrian Bird's-eye View Path Prediction}
Like vehicle path prediction, pedestrian path prediction often relies on a bird's-eye view of real-world coordinates and measures the error in meters. Many works have tackled the problem of path prediction in recent years. Most of them only focus on multi-future path prediction \cite{yue2022human, zhou2021sliding, mangalam2021goals,wong2021view} (k=5, 8, 15, 20, ..., where k shows the number of predicted trajectories for each subject), and evaluate the model by picking the best out of several predicted paths. However, generating multiple outputs per subject in real-time scenarios is not helpful in real-world applications. Therefore, we primarily focus on the works that perform single future path prediction analysis. \cite{alahi2016social} uses LSTM modules to predict the trajectories of all pedestrians in a scene jointly. Several works have used attention mechanisms to solve path prediction problems better. \cite{vemula2017social} introduces attention to model the importance of social interactions without relying on proximity. \cite{zhao2020spatial} also leverages a spatio-temporal attention module to learn which social interaction has a more critical role in predicting the pedestrian's path. Trajectron++ \cite{salzmann2020trajectron++}, designed for integration with robotic frameworks, utilizes a graph-structured recurrent model and heterogeneous data. While the focus is on pedestrians, Trajectron++ also predicts the future paths of vehicles to understand better how pedestrians might react to them. \cite{mendieta2021carpe} utilizes graph isomorphism networks and a lightweight Convolutional Neural Network (CNN) for path prediction, considerably reducing computation and model size and targeting real-time applications. SSAGCN \cite{lv2021ssagcn} uses an attention graph convolutional network and defines a new formulation to consider both social interactions and environmental factors as they can change the path pedestrians may choose.




\subsection{Pedestrian High-angle View Path Prediction}
Applications that require pedestrian path prediction only sometimes have access to bird's-eye view cameras or real-world coordinates. It is logical to use high-angle views and pixel space $(x,y)$ coordinates in addition to the traditional bird's-eye view models. This is common for CCPS \cite{Isern2020Reconfig} and ICPS \cite{deniz2022efficient, Chen2021TowardsOH} as the cameras are often mounted at the top of buildings or traffic lights and on the walls in respective systems. \cite{li2022graph} integrates multiple graph-based spatial transforms and trajectory smoothing to exploit temporal information and correct temporally inconsistent trajectories. SimAug \cite{liang2020simaug} utilizes synthetic data to improve the robustness of learned representations with the goal of better generalization in unseen contexts. Peeking into the Future \cite{liang2019peeking} proposes a multi-task model to predict both future paths and future activities, with the belief that understanding the future activity is highly informative to the future path. Multiverse \cite{liang2020garden} builds upon \cite{liang2019peeking} by introducing synthetic data and multi-scale location information and replacing graphs with Recurrent Neural Networks (RNNs). ScePT \cite{chen2022scept} proposes predicting paths at the scale of "cliques" rather than for each individual. Similar to Trajectron++ \cite{salzmann2020trajectron++}, and GAIN \cite{liu2022multi}, ScePT predicts vehicle and pedestrian trajectories. 


Some of the works have tried to tackle the problem of path prediction in more than one domain \cite{liu2022multi, salzmann2020trajectron++, liang2019peeking, zhao2020spatial}. However, they prefer not to evaluate their model on all three discussed domains. On the other hand, although path prediction has many CPS applications, more work needs to be done to benchmark the path prediction architectures' performance on edge devices. Works such as \cite{Chen_2021_ICCV, Shafiee_2021_CVPR, Wang_2021_WACV, peng2021sra} report the inference time of the models on sophisticated and powerful hardware resources such as GTX 2080Ti GPU, NVIDIA Quadro RTX 6000 GPU, and Intel Core i9-9880H Processor. To this end, it is not feasible for CPS edge-based applications to use such a framework, as edge platforms have limited power and system resources. Thus, previous approaches fail to generalize across several different domains and adapt to the stringent requirements of the CPS applications.

 \section{Pishgu} \label{sec:pishgu}
Path prediction in different domains inherently depends upon the position, past movements, and end goals of the subjects present in a scene. The end goals and movement patterns vary significantly between different environments (highway, sidewalk, etc.) and subjects (vehicle, pedestrian). However, graph neural networks assist in understanding the varying patterns in the respective domains. Pishgu uses GIN to grasp the interdependencies of all subjects in a scene. Attention-based convolutions are utilized to highlight the important interdependencies and predict the future paths of all the subjects jointly. As we focus on real-time applications of our approach, Pishgu is designed to predict a single path ($K=1$) for each subject present in the scene. The overall structure of Pishgu can be seen in Figure \ref{fig:pishgu}.


\subsection{Problem Formulation}
Our goal is to predict the future paths of all the subjects in a scene, regardless of the domain of prediction. Keeping that in mind, we use both absolute and relative coordinates as inputs to Pishgu, defined as follows:

\begin{equation}
     \label{eq:input}
    \boldsymbol{N_i}=[N_{i}^{t_{-T_{in}}} ,N_{i}^{t_{1-T_{in}}} ,\cdots , N_{i}^{t_{0}}]
\end{equation}    
\begin{equation}
\label{eq:rel_input}
\boldsymbol{\Delta N_i} = [\Delta N_{i}^{t_{-T_{in}}} ,\Delta N_{i}^{t_{1-T_{in}}} \cdots  ,\Delta N_{i}^{t_{0}}]
\end{equation}        
    
where $N_{i}^{t}=\left<x_{i}^{t}, y_{i}^{t}\right>$ is the position of subject $i$ (vehicle or pedestrian)  at time $t$ and $T_{in}$ is the number of time steps that the model observes for prediction. $\Delta N_{i}^{t} = N_i^t - N_i^{t_{-T_{in}}}$ in equation \ref{eq:rel_input} represents the relative coordinates of subject $i$ in time $t$ with respect to the location of the subject at time $-T_{in}$. This approach is specifically adopted for exploiting the features of the graph-based architecture. The model is aware of the surrounding environment which plays a part in the path prediction process similar to humans' decision-making process while driving and walking. 

As mentioned previously, Pishgu predicts a single path for each subject at every time step, and the outputs of the model are formulated as:
\begin{equation}
\label{eq:output}
    \boldsymbol{\hat{Y_{i}}}=\left[
        \hat{Y_{i}}^{t_{1}},\hat{Y_{i}}^{t_{2}}, \cdots, \hat{Y_{i}}^{T_{out}}
    \right],
\end{equation}
where $ \hat{Y_{i}}^{t}$ represents the predicted trajectory of subject $i$ at time t in the future up to $T_{out}$ time steps.

\subsection{Architecture}

Pishgu's architecture (see Figure \ref{fig:pishgu}) is relatively simple compared to many modern path prediction models. This simplicity in our design is intentional; with CPS applications in mind, optimizing the number of parameters can improve performance in real-world scenarios. After the calculation of input features, $\boldsymbol{N}$ and $\boldsymbol{\Delta N}$ are concatenated and passed through a single fully-connected layer $fc$ as shown in Figure \ref{fig:pishgu}. In the next step, Pishgu leverages a Graph Neural Network (GNN) for embedding the input features. There has been a surge in curiosity towards Graph Neural Networks in recent years because of their power to represent complex interactions, and non-Euclidean data \cite{xu2018powerful,velivckovic2017graph, kipf2016semi}. Many different approaches have been proposed with neighbor aggregation and message-passing methods. The final goal of GNNs is to construct a maximally discriminative representation. This means that two nodes are mapped to the same location in the embedding space only if they are identical. \cite{xu2018powerful} came up with a simple yet powerful new formulation of GNNs, namely GIN that is shown to be as powerful as the Weisfeiler-Lehman test \cite{leman1968reduction} which is a test that answers the question of whether two graphs are identical or not. We draw motivation from the work of \cite{xu2018powerful} and adapt the network to our specific requirements. Our model constructs a fully connected graph $\mathcal{G}$ = ($\mathcal{V}, \mathcal{E}$) where the nodes ($\mathcal{V}$) are the subjects of interest present in the frame, and the edges ($\mathcal{E}$) represent their interactions. The fully connected structure assures that all the possible interactions between subjects are considered, and the network can extract all the important information from other neighbors. Pishgu utilizes a modified version of the aggregation function introduced by \cite{xu2018powerful} and constructs $f_{i}^{\prime}$ (the aggregated feature for node $i$) as follows:
\begin{equation}
f_{i}^{\prime}=MLP_{0}\left((1+\theta) \cdot f_{i}\right)+ MLP_{1}\left(\sum_{j\in \mathcal{V}(i)} f_{j}\right)
\label{eq:GIN}
\end{equation}

where $i$ is $i^{th}$ subject in the scene, $MLP_{0}$ and $MLP_{1}$ are Multi-layer Perceptron (MLP) operators each with a single hidden layer, $\mathcal{V}(i)$ is the set of node $i$ neighbors and $\theta$ is a trainable parameter. $MLP_{0}$  and $MLP_{1}$ are applied to the features of node $i$ and the aggregation of the features from neighbor nodes, respectively. Having two separate MLPs improves the network's ability to extract richer features and better integrate neighboring nodes in the context \cite{mendieta2021carpe}. Keeping the real-time performance in mind, a single graph operation is performed across the entire graph, which is enough since the graph is fully connected and all the features can be propagated in one step. Before the final task of path prediction, the output vector of the GIN block (equation \ref{eq:GIN}) for each subject is concatenated with the respective relative coordinate (equation \ref{eq:rel_input}).


Pishgu makes use of an attentive convolutional neural network for the final path predictions. Studies have shown that adding attention to CNNs can improve their representation power and help them generalize better \cite{cbam18}. The attention enables the convolutional predictor to dynamically decide the importance of embedded features generated by the GIN Encoder in the previous step and how much of this information should be used in the final prediction. Keeping the computational complexity in mind for CPS applications, the predictor structure consists of only seven layers, with three layers of 2D convolution each followed by a layer of attention module \cite{cbam18} and a final $1\times1$ convolutional layer for forming the predicted trajectories. The first convolutional layer uses a $2 \times 2$ kernel, and the subsequent two convolutional layers use a $2\times1$ kernel size. The architecture is designed to capture the fast and slow movements of the subjects. The attention module works based on two pillars: channel attention and spatial attention. Channel attention tries to perceive what is essential in the input feature map \cite{NIU202148}. To accomplish this task, the channel attention module first performs average pooling and max pooling on the input feature map to encapsulate the input features. These pooling operations improve the module's efficiency by reducing the feature map size. 


In the next step, pooled features are fed to an MLP with one hidden layer to create the channel attention map. The formulation of the channel attention module can be summarized as:
\begin{equation}
\begin{aligned}
\mathbf{AT_{c}}(\mathbf{F}) = {} & MLP_{2} (Pool_{avg}(\mathbf{F})) \\
                                & +MLP_{2} (Pool_{max}(\mathbf{F}))
\end{aligned}
\end{equation}
Where $\mathbf{F}$ is the input feature map, $MLP_2$ is an MLP with one hidden layer, and $\mathbf{AT}_{c}$ is the channel attention module. 
On the other hand, the goal of the spatial attention module is to locate the important features in the input \cite{NIU202148}. For efficiency, average pooling and max pooling are used again, but this time in the channel axis. The pooled features are concatenated and passed to a convolutional layer for generating the spatial attention map. In summary, the spatial attention module works based on the following equation:
\begin{equation}
\begin{aligned}
&\mathbf{AT_s}(\mathbf{F}) \\
&=\sigma\left(Conv^{7 \times 7}[(Pool_{avg}(\mathbf{F})) ; Pool_{max}(\mathbf{F})])\right) 
&
\end{aligned}
\end{equation}

where $\mathbf{F}$ is the input feature map, $\sigma$ is the sigmoid activation function, $Conv^{7 \times 7}$ is the convolutional layer with a kernel size of 7, and $\mathbf{AT}_{c}$ is the spatial attention module. 

Incorporating channel attention and spatial attention sequentially has shown significant improvements in the performance of CNNs \cite{woo2018cbam}. Thus, between each convolutional layer, there is an additional module consisting of the channel and spatial attention to improve the predictor's performance while keeping the network lightweight.  





 \section{Domain Analysis}
Challenges and requirements found in path prediction are often domain-specific. The interactions and behaviors vary drastically from predicting a vehicle's path on a highway with hundreds of vehicles in a scene to predicting the path of a single pedestrian walking on a sidewalk with less than a dozen other people. A universal model should be able to adapt across these domains. Thus, understanding each domain's characteristics is vital for designing a universal path prediction architecture. Table \ref{tab:stat} highlights the complexities of various domains by presenting the number of subjects and samples available from widely adopted datasets in the respective domains.

\begin{table}[!htbp]
\renewcommand{\arraystretch}{1.2}
\centering
\caption{The statistics of four datasets used for path prediction. These parameters are reported after doing the conventional preprocessing steps discussed in section \ref{sec:domain}. $V_B$, $P_B$, and $P_H$ refer to Vehicle Bird's-eye view, Pedestrian Bird's-eye view, and Pedestrian High-angle view respectively. FPS is Frames Per Second.}
\vspace{-10pt}
\label{tab:stat}
\begin{adjustbox}{max width=1\columnwidth,center}
\begin{tabular}{c||c|c|c|c}
\rowcolor{DarkGray}
 & \begin{tabular}[c]{@{}c@{}} NGSIM \\ \cite{ NGSIM_i80, NGSIM_US101} \end{tabular} & \begin{tabular}[c]{@{}c@{}} ETH \\ \cite{pellegrini2009you} \end{tabular} & \begin{tabular}[c]{@{}c@{}} UCY \\ \cite{lerner2007crowds} \end{tabular} & \begin{tabular}[c]{@{}c@{}}ActEv/\\ VIRAT \cite{awad2018benchmarking}\end{tabular} \\ \hline \hline
Domain & $V_B$ & $P_B$ & $P_B$ & $P_H$ \\ \hline
\#Subjects & 19,698 & 749 & 1,456 & 1,059 \\ \hline
\#Samples & 13.3M & 12,035 & 62,393 & 130,289 \\ \hline
\#Frames & 108,033 & 2,044 & 4,397 & 41,199 \\ \hline
FPS & 5 & 2.5 & 2.5 & 2.5 
\end{tabular}
\end{adjustbox}
\vspace{-10pt}
\end{table}
\label{sec:domain}




\subsection{Vehicle Bird's-eye View}


Most deep learning architectures used for predicting vehicle paths in a highway environment \cite{CSlstm18, highway2022, congestion2021} use NGSIM datasets \cite{NGSIM_i80, NGSIM_US101}. NGSIM provides complex real-world scenarios and driver behaviors for various traffic patterns. In this paper, we use US-101 \cite{NGSIM_US101} and I-80 \cite{NGSIM_i80} from NGSIM, each with millions of data samples in bird's-eye view. Using a bird's-eye view for path prediction is natural as it provides an overall perspective of the surrounding environment. NGSIM data is collected from cameras mounted on the buildings around the freeways. However, the final dataset provides the data in bird's-eye format. This conversion to a bird's-eye view helps deep learning models learn complicated driver behaviors and their reactions to the surroundings. In this evaluation we exclude the trajectories of vehicles going for exits and merging lanes for both \cite{NGSIM_i80} and \cite{NGSIM_US101}.



\subsection{Pedestrian Bird's-eye View}
Two widely used pedestrian path prediction datasets are ETH \cite{pellegrini2009you} and UCY \cite{lerner2007crowds}. They consist of bird's-eye view annotations of several crowded scenes with complicated nonlinear pedestrian paths. The position of the pedestrians in these datasets is gathered in real-world coordinates in meters. The data points generally used for training are sampled at a rate of 2.5 Frames Per Second (FPS). ETH looks at two different scenes, ETH and HOTEL, whereas UCY looks at three different scenes, UNIV, ZARA1, and ZARA2. These datasets are especially useful for training models that focus on drone-related applications or environment monitoring. They do not include any data for other points of view such as high-angle or side views.

\begin{figure}[h]
\vspace{-10pt}
\begin{tikzpicture}
  \begin{axis}
    [
    xmajorgrids=true,
width= \linewidth,
    trim left = 1cm,
    ytick={1,2,3,4},
    xlabel = Number of Samples,
    yticklabels={NGSIM, ETH , UCY, \begin{tabular}[c]{@{}c@{}}ActEv/\\ VIRAT  \end{tabular}},
    xticklabels={0, 1, 3, 10, 32, 100, 315},
    height = 0.7\linewidth,
    ]
    \addplot+[
    boxplot prepared={
      median=2.1,
      upper quartile=2.2,
      lower quartile=2,
      upper whisker=2.4,
      lower whisker=0
    }, draw=black, fill=Green,
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      median=0.7,
      upper quartile=0.9,
      lower quartile=0.5,
      upper whisker=1.4,
      lower whisker=0
    }, draw=black, fill=Purple,
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      median=0.9,
      upper quartile=1.2,
      lower quartile=0.6,
      upper whisker=1.9,
      lower whisker=0
    }, draw=black, fill=LightBlue,
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      median=0.5,
      upper quartile=0.6,
      lower quartile=0.3,
      upper whisker=1.1,
      lower whisker=0
    }, draw=black, fill=LightRed,
    ] coordinates {};
  \end{axis}
\end{tikzpicture}
\vspace{-10pt}
\caption{Distribution of number of samples in a frame for ActEv/VIRAT \cite{awad2018benchmarking}, UCY \cite{lerner2007crowds}, ETH \cite{pellegrini2009you} and NGSIM \cite{NGSIM_i80, NGSIM_US101} datasets. X-axis shows the number of samples in the frame. Please note that the box and whiskers are drawn in logarithmic space for better visualization, and the number of samples per frame can be translated to the number of unique subjects in one frame.}
\label{fig:data_dist}
\vspace{-15pt}
\end{figure}





\subsection{Pedestrian High-angle View}

\begin{table*}[!b]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{Vehicle path prediction error and complexity comparison of Pishgu with contemporary approaches. Best is in bold text. Second best is
underlined.}
\label{tab:ngsim_results}\vspace{-10pt}
  \begin{adjustbox}{max width = \linewidth,center}
    \begin{tabular}{c||c|c|c|c|c||c|c||c}
    \rowcolor{DarkGray} {} & \multicolumn{5}{c||}{RMSE (m)}        &  &  &  \\  \cline{2-9} \rowcolor{Gray}
          & 1s    & 2s    & 3s    & 4s    & 5s    &   {ADE (m)}    &   {FDE (m)}    & \multicolumn{1}{c}{Params (K)}  \\ \hline \hline
    
    CS-LSTM \cite{CSlstm18} & 0.63  & 1.27  & 2.09  & 3.10   & 4.37 & 2.29  & 3.34   & \multicolumn{1}{c}{191} \\ \hline
    DeepTrack \cite{katariya2021deeptrack} & 0.47  & 1.08  & 1.83  & 2.75  & 3.89  & 2.01  & 3.25  & \multicolumn{1}{c}{\underline{109}}  \\ \hline
    
    GRIP++ \cite{2020gripplus} &  0.38  &   0.89 &    {1.45}   & 2.14       &    \underline{2.94}   &    1.61       &    -   & \multicolumn{1}{c}{-}  \\ \hline
Pip \cite{Pip2021}  & 0.55  & 1.18  & 1.94  & 2.88  & 4.04  &   2.18    &    -   & \multicolumn{1}{c}{-}   \\ \hline
GSTCN \cite{GSTCN2022}& 0.44  & \underline{0.83}  & \underline{1.33}   & \underline{2.01}  & 2.98  &   \underline{1.52 }   &   -    & \multicolumn{1}{c}{\textbf{49.8}}   \\ \hline
STA-LSTM \cite{STALSTM2021} & \underline{0.37}  & 0.98  & 1.71  & 2.63  & 3.78  & 1.89  & \underline{3.16}  & \multicolumn{1}{c}{124} \\ \hline
\rowcolor{LightGray} \textbf{Pishgu (Ours)} & \textbf{0.15}  & \textbf{0.46}  & \textbf{0.82}  & \textbf{1.25}  & \textbf{1.74}  & \textbf{0.88}  & \textbf{1.96}  & \multicolumn{1}{c}{132}  \\
    \end{tabular}\end{adjustbox}
\vspace{-10pt}
\end{table*}

It is unrealistic to assume that a bird's-eye view angle is always available. Most surveillance cameras are placed at a high location (e.g. the side of a building, on a light pole) to have an overview of the scene from a high-angle view. Thus, path prediction models in real-world scenarios should be able to work with different camera views and angles specially if they want to cover normal video surveillance setups. However, most of the works in the path prediction field focus on the bird's-eye view. On the other hand, in end-to-end real-world environment monitoring and surveillance systems, real-world coordinates are unavailable and the locations of subjects are represented in pixel space. This disconnect between real-world scenarios and most existing algorithms makes many proposed methods ill-suited for realistic path prediction. To address this issue, works such as \cite{liang2019peeking, liang2020garden, liang2020simaug, li2022graph} have used ActEv/VIRAT \cite{awad2018benchmarking} for path prediction. Since then, it has become a standard benchmark for path prediction in pixel space and high-angle view. The primary use of ActEV/VIRAT is activity recognition in challenging real-world scenarios in the surveillance domain. However, the diversity of camera angles and locations in this dataset is advantageous for real-world path prediction as well. Following the same preprocessing steps as Next \cite{liang2019peeking}, we downsample the frame rate to 2.5 FPS. The center of the bounding box for a person is used for the extraction of the subject's location.



Looking at Table \ref{tab:stat}, we can see that in the human bird's-eye view domain, the two publicly available datasets ETH \cite{pellegrini2009you} and UCY \cite{lerner2007crowds} are relatively small compared to datasets available in other domains. NGSIM \cite{NGSIM_i80, NGSIM_US101} has the most number of samples by far and is a much more crowded dataset in terms of the number of samples in each frame. Figure \ref{fig:data_dist} clearly shows that the number of samples in the frame for ETH and UCY datasets is more than ActEv/VIRAT \cite{awad2018benchmarking}, but still much less than the number of samples in NGSIM dataset which even goes up to 248 vehicle in a single frame. Due to these distinctive characteristics that each domain has, a general path prediction method should be able to adapt to each domain to be able to satisfy each domain's requirements.
 

\section{Experiments} \label{sec:Experiments}

This section evaluates our path prediction method in all three domains discussed in Section \ref{sec:domain}. We compare Pishgu with SotA models that report their performance in the respective domains in three different error measurements and model sizes. All of the evaluations in this article have been conducted on a server containing an NVIDIA Tesla V100 GPU, 2x AMD EPYC 7513 CPUs, and 256GB of RAM. Additionally, to prove the usability of our architecture for CPS applications, latency and throughput through real-time inference are reported on NVIDIA Jetson Xavier NX and Jetson Nano embedded platforms. The following sub-sections discuss the definitions of each error measurement and the performance of Pishgu against current path prediction approaches in all the domains.


\subsection{Evaluation Metrics}

\begin{table*}[!t] \caption{Path prediction error comparison of Pishgu in pedestrian bird's-eye view domain with contemporary approaches. ADE and FDE are reported in meters. Best is in bold text. Second best is
underlined.}
\vspace{-8pt}
\label{tab:human_bird}
\renewcommand{\arraystretch}{1.3}
\centering
\begin{adjustbox}{max width = \linewidth,center}
\begin{tabular}{c||cc||cc||cc||cc||cc||cc||c}
\rowcolor{DarkGray}
                                  & \multicolumn{2}{|c||}{ETH}                  & \multicolumn{2}{c||}{HOTEL}                & \multicolumn{2}{c||}{UNIV}                 & \multicolumn{2}{c||}{ZARA1}                & \multicolumn{2}{c||}{ZARA2}                & \multicolumn{2}{c||}{AVG}      &             \\ \cline{2-14}
                                  \rowcolor{Gray}
                                  & \multicolumn{1}{|c|}{ADE}  & FDE  & \multicolumn{1}{c|}{ADE}  & FDE  & \multicolumn{1}{c|}{ADE}  & FDE  & \multicolumn{1}{c|}{ADE}  & FDE  & \multicolumn{1}{c|}{ADE}  & FDE  & \multicolumn{1}{c|}{ADE}  & FDE & Params (M) \\ \hline \hline
\multicolumn{1}{c||}{Linear}       & \multicolumn{1}{c|}{1.33}          & 2.94          & \multicolumn{1}{c|}{0.39}          & 0.72          & \multicolumn{1}{c|}{0.82}          & 1.59          & \multicolumn{1}{c|}{0.62}          & 1.21          & \multicolumn{1}{c|}{0.77}          & 1.48          & \multicolumn{1}{c|}{0.79}          & 1.59     & -     \\ \hline
\multicolumn{1}{c||}{LSTM}         & \multicolumn{1}{c|}{1.09}          & 2.41          & \multicolumn{1}{c|}{0.86}          & 1.91          & \multicolumn{1}{c|}{0.61}          & 1.31          & \multicolumn{1}{c|}{0.41}          & 0.88          & \multicolumn{1}{c|}{0.52}          & 1.11          & \multicolumn{1}{c|}{0.70}           & 1.52     & -     \\ \hline
\multicolumn{1}{c||}{Social-LSTM \cite{alahi2016social}} & \multicolumn{1}{c|}{1.09}          & 2.35          & \multicolumn{1}{c|}{0.79}          & 1.76          & \multicolumn{1}{c|}{0.67}          & 1.40           & \multicolumn{1}{c|}{0.47}          & 1.00             & \multicolumn{1}{c|}{0.56}          & 1.17          & \multicolumn{1}{c|}{0.72}          & 1.54     & -     \\ \hline
\multicolumn{1}{c||}{Next \cite{liang2019peeking}}         & \multicolumn{1}{c|}{0.88}          & 1.98          & \multicolumn{1}{c|}{{0.36}}          & {0.74}          & \multicolumn{1}{c|}{0.62}          & 1.32          & \multicolumn{1}{c|}{0.42}          & 0.90           & \multicolumn{1}{c|}{0.34}          & 0.75          & \multicolumn{1}{c|}{{0.52}}          & 1.14   & 3.95       \\ \hline
\multicolumn{1}{c||}{ST-Attention \cite{zhao2020spatial}} & \multicolumn{1}{c|}{{{0.85}}} & {1.85}     & \multicolumn{1}{c|}{{0.32}} & {{0.66}} & \multicolumn{1}{c|}{{0.63}} & {1.33} & \multicolumn{1}{c|}{{0.42}} & {0.91} & \multicolumn{1}{c|}{{0.34}} & {0.73} & \multicolumn{1}{c|}{{0.51}} & 1.10  & 1.98\\ \hline
\multicolumn{1}{c||}{CARPe \cite{mendieta2021carpe}}        & \multicolumn{1}{c|}{0.80}           & {1.48} & \multicolumn{1}{c|}{0.52}          & 1.00             & \multicolumn{1}{c|}{{0.61}}          &{ 1.23 }         & \multicolumn{1}{c|}{{0.42}}          & {0.84}          & \multicolumn{1}{c|}{{0.34}}          & {0.69}          & \multicolumn{1}{c|}{0.54}          & 1.05   & \textbf{0.10}       \\ \hline
\multicolumn{1}{c||}{Trajectron++ \cite{salzmann2020trajectron++}} & \multicolumn{1}{c|}{{0.71}} & {1.66}          & \multicolumn{1}{c|}{\underline{0.22}} & \underline{0.46} & \multicolumn{1}{c|}{{0.44}} & \underline{1.17} & \multicolumn{1}{c|}{{0.30}} & \underline{0.79} & \multicolumn{1}{c|}{{0.23}} & \underline{0.59} & \multicolumn{1}{c|}{{0.38}} & \underline{{0.93}} & - \\ \hline
\multicolumn{1}{c||}{ScePT \cite{chen2022scept}} & \multicolumn{1}{c|}{\textbf{0.19}} & \underline{1.33}          & \multicolumn{1}{c|}{\textbf{0.18}} & {1.12} & \multicolumn{1}{c|}{\textbf{0.19}} & {1.19} & \multicolumn{1}{c|}{\textbf{0.18}} & {1.10} & \multicolumn{1}{c|}{\underline{0.19}} & {1.20} & \multicolumn{1}{c|}{\textbf{0.19}} & {{1.19}} & - \\ \hline
\multicolumn{1}{c||}{SSAGCN \cite{lv2021ssagcn}} & \multicolumn{1}{c|}{{\underline{0.30}}} & \textbf{0.59}          & \multicolumn{1}{c|}{\underline{0.22}} & {\textbf{0.42}} & \multicolumn{1}{c|}{\underline{0.25}} & \textbf{0.47} & \multicolumn{1}{c|}{\underline{0.20}} & \textbf{0.39} & \multicolumn{1}{c|}{\textbf{0.14}} & \textbf{0.28} & \multicolumn{1}{c|}{\underline{0.22}} & {\textbf{0.43}}  & -\\ \hline
\rowcolor{LightGray}
\multicolumn{1}{c||}{\textbf{Pishgu (Ours)}}       & \multicolumn{1}{c|}{1.10}           & 2.24          & \multicolumn{1}{c|}{1.17}          & 2.17          & \multicolumn{1}{c|}{0.67}          & 1.37          & \multicolumn{1}{c|}{0.45}          & 0.91          & \multicolumn{1}{c|}{0.36}          & 0.73          & \multicolumn{1}{c|}{0.75}          & 1.48 & \underline{0.11} \\ 

\end{tabular}
\end{adjustbox}
\vspace{-10pt}
\end{table*}




\textbf{Average Displacement Error (ADE): } The average $L_2$ distance between the predicted coordinates ($\hat{Y}$) and the ground truth coordinates ($Y$) over all $T_{out}$ predicted time steps and all subjects of interest (N) available in the scene:
\begin{equation}
\mathrm{ADE}=\frac{\sum_{i=1}^{N} \sum_{t=1}^{T_{out}}\left\|Y_{i}^{t}-\hat{Y}_{i}^{t}\right\|_{2}}{N * T_{out}}
\end{equation}

\textbf{Final Displacement Error (FDE): } The average $L_2$ distance between the predicted coordinates ($\hat{Y}$) and the ground truth coordinates ($Y$) of the last predicted time step over all subjects of interest (N) available in the scene:

\begin{equation}
\mathrm{FDE}=\frac{\sum_{i=1}^{N}\left\|Y_{i}^{T_{out}}-\hat{Y}_{i}^{T_{out}}\right\|_{2}}{N}
\end{equation}

\textbf{Root Mean Square Error (RMSE):}
The RMSE at time $t$ is the square root of the mean square error between the predicted path ($\hat{Y}$) and the ground truth path ($Y$) of the subjects of interest in the scene:
\begin{equation}
    RMSE^{t}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}{(Y_{i}^{t}-\hat{Y}_{i}^{t})}^{2}}
\end{equation}


We report and compare the complexity of our model against the SotA approaches in terms of number parameters. In Section \ref{sec:realtime}, we calculate the latency of prediction per sample in milliseconds and the throughput in FPS.










\subsection{Vehicle Bird's-eye View}

Recent works \cite{GSTCN2022, aitp2022,dginet2022} have used the NGSIM public datasets for vehicle path prediction as it provides many complex highway scenarios. The dataset is split into a 70\% training set, 10\% validation set, and 20\% test set. Pishgu has been trained for 40 epochs with a learning rate of 0.01 and the Adam optimizer \cite{kingma2014adam}.


As shown in Table \ref{tab:ngsim_results}, we review the performance of Pishgu in terms of RMSE, ADE, FDE, and model size against best-in-class models \cite{CSlstm18, katariya2021deeptrack, 2020gripplus, Pip2021, GSTCN2022, STALSTM2021} in vehicle bird's-eye view domain. Reporting RMSE is crucial in vehicle path prediction as autonomous vehicles evaluate the future trajectories of surrounding subjects for every second up to a few seconds. Pishgu, like the other models, observes the input for 15-time steps (3 seconds) and predicts the path of all the vehicles in a scene for 25-time steps (5 seconds). Regarding RMSE, Pishgu performs better at all the time steps than the other models. It reduces the RMSE by 44\%, 38\%, and 37\% for 2$^{nd}$, 3$^{rd}$, and 4$^{th}$ sec as compared to \cite{GSTCN2022}. Compared to best-performing models for 1$^{st}$ and 5$^{th}$ sec, Pishgu reduces the RMSE by 50\% and 40\%. Similar performance is observed in ADE and FDE, where we improve the SotA error rates by 42\% and 61\%, respectively. As most of the frames in NGSIM are crowded, it helps the attention module grasp the most important relational behaviors between the subjects for path prediction. Thus, the attention module is the most effective for the NGSIM dataset.

In model size, Pishgu is comparable to most contemporary deep-learning models. However, \cite{GSTCN2022} reports less than half the parameters than Pishgu. The smaller size of GSTCN is due to using a simple CNN for path prediction and restricting the graph to two laterally adjacent lanes and $\pm$100 meters. In contrast, Pishgu uses attention-based CNN with a graph of the entire scene to grasp the overall environment provided by the NGSIM dataset.



\subsection{Pedestrian Bird's-eye View}


In the pedestrian bird's-eye view domain, we evaluated our model on ETH \cite{pellegrini2009you}, and UCY \cite{lerner2007crowds}, the characteristics of which were detailed in Section \ref{sec:domain}. We will adopt the same strategy for evaluation as previous works in this domain \cite{gupta2018social} and train our model with a leave-one-out approach on combined ETH and UCY datasets. As input, the model observes 8-time steps (3.2 seconds) and predicts the coordinates of the pedestrian for the future 12-time steps (4.8 seconds). The model has been trained for 80 epochs with a learning rate of $0.01$ and Adam optimizer \cite{kingma2014adam}.

In the field of path prediction, it is common for models to generate 20 outputs (k=20) for each subject in the scene and report the ADE and FDE based on the best of the 20 predicted paths for each pedestrian. However, this method is unsuitable for real-time scenarios, especially those deployed on embedded platforms with limited resources. Thus, we will compare our model only to the approaches that report their single-future path prediction evaluation in the pedestrian bird's-eye view domain. In our comparisons, we include a simple Linear regressor used for path prediction. Another baseline method we consider is LSTM, a simple LSTM encoder-decoder model used for path prediction. Table \ref{tab:human_bird} shows that Pishgu struggles with path prediction in this domain. This is due to the fact that ETH\cite{pellegrini2009you} and UCY \cite{lerner2007crowds} are relatively small datasets and can not provide a sufficient amount of training data for the convolution attention mechanism to be trained well, as discussed in Section \ref{sec:domain}. 

We also report the model complexity in terms of the number of parameters, but most previous works do not examine this crucial factor. Looking at Table \ref{tab:human_bird}, we can see that Pishgu has a competitive model size compared to \cite{mendieta2021carpe} and around $36\times $ smaller model size compared to Next \cite{liang2019peeking}.


\begin{table*}[!t]
\centering
\caption{Experiment results on ActEV/VIRAT \cite{awad2018benchmarking} dataset. The results are reported in pixel space. Best is in bold text. Second best is underlined.}
\vspace{-10pt}
\label{tab:pixel}
\renewcommand{\arraystretch}{1.3}
\centering
\begin{adjustbox}{max width = \linewidth,center}
\begin{tabular}{c||c|c|c|c|c|c|c|c|c}
\rowcolor{DarkGray}
 & \begin{tabular}[c]{@{}c@{}}Social-LSTM \\ \cite{alahi2016social} \end{tabular} & \begin{tabular}[c]{@{}c@{}}Social-GAN\\ (V) \cite{gupta2018social} \end{tabular} & \begin{tabular}[c]{@{}c@{}}Social-GAN \\ (PV) \cite{gupta2018social} \end{tabular} & \begin{tabular}[c]{@{}c@{}}Next \\ \cite{liang2019peeking} \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Multiverse \\ \cite{liang2020garden} \end{tabular}  & \begin{tabular}[c]{@{}c@{}} SimAug \\ \cite{liang2020simaug} \end{tabular}   & \begin{tabular}[c]{@{}c@{}} ST-MR \\ \cite{li2022graph} \end{tabular} & \begin{tabular}[c]{@{}c@{}} ST-Attention \\ \cite{zhao2020spatial} \end{tabular} & \textbf{Pishgu (Ours)} \\ \hline \hline
ADE & 23.10 & 30.40 & 30.42 & 19.78 & {18.51} & 21.73 & 18.58 & \underline{18.39} &\cellcolor{Gray} \textbf{14.11} \\ \hline
FDE & 44.27 & 61.93 & 60.70 & 42.43 & \underline{35.84} & 42.22 & 36.08 & 38.11 & \cellcolor{Gray} \textbf{27.96} \\ \hline
Params (M) & - & - & - & {3.95} & - & - & - & \underline{1.98} &\cellcolor{Gray} \textbf{0.11}
\end{tabular}
\end{adjustbox}
\vspace{-10pt}
\end{table*}



\begin{table*}[b!]
\centering
\caption{Latency and throughput evaluation on Jetson Xavier NX CPU and  Jetson Nano CPU in milliseconds and FPS, respectively. Samples per Frame is an average. FPS is Frames Per Second, and $V_B$, $P_B$, and $P_H$ refer to Vehicle Bird's-eye view, Pedestrian Bird's-eye view, and Pedestrian High-angle view, respectively.}
\vspace{-10pt}
\label{tab:latency}
\renewcommand{\arraystretch}{1.3}
\begin{adjustbox}{max width = \linewidth,center}
\begin{tabular}{c||c|c||c|c||c|c}
    \rowcolor{DarkGray}
    \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} Samples & \multicolumn{2}{c||}{\cellcolor{DarkGray}  Jetson Xavier NX}  & \multicolumn{2}{c}{\cellcolor{DarkGray}  Jetson Nano} \\ \cline{4-7} 
    \rowcolor{Gray}
     & Domain & per   & Latency & FPS & Latency & FPS \\
    \rowcolor{Gray}
     &        & Frame & (ms)    &     & (ms)    &     \\ \hline \hline
NGSIM \cite{NGSIM_i80, NGSIM_US101}     & $V_B$  & 158 & 2.69  & 2.35   & 3.50  & 1.81   \\ \hline
UCY \cite{pellegrini2009you}            & $P_B$  & 8   & 1.44  & 86.81  & 2.41  & 51.81  \\ \hline
ETH \cite{lerner2007crowds}             & $P_B$  & 3   & 1.75  & 190.48 & 2.06  & 161.81 \\ \hline
ActEv/                                  & \multirow{2}{*}{$P_H$}  & \multirow{2}{*}{5}   & \multirow{2}{*}{2.83}  & \multirow{2}{*}{70.67}  & \multirow{2}{*}{3.40}   & \multirow{2}{*}{58.82}  \\
VIRAT \cite{awad2018benchmarking}       &        &     &       &        &       &        \\
\end{tabular}
\end{adjustbox}
\vspace{-10pt}
\end{table*}


\subsection{Pedestrian High-angle View}

Following the previous works \cite{liang2019peeking, liang2020garden, liang2020simaug, li2022graph}, we analyze the efficacy of Pishgu by training and testing it on the ActEV/VIRAT \cite{awad2018benchmarking} challenge dataset discussed in Section \ref{sec:domain}. We report ADE and FDE for evaluating the displacement error. The official training and validation sets have been used for collecting the mentioned evaluation results. As input, the model observes 8-time steps, or 3.2 seconds and predicts the path for the future 12-time steps or 4.8 seconds.

We compare our model to other models that make single-future path predictions in this domain since having multiple predicted paths per pedestrian in real-time applications is unrealistic. Results reported in pixel space can be seen in Table \ref{tab:pixel}. Social-LSTM \cite{alahi2016social}, and Social-GAN (both V and PV variants) results are based on the tests performed by \cite{liang2019peeking}. Comparison with other approaches makes evident the advantage of Pishgu with a $23.6\%$ and $22.6\%$ decrease in ADE and FDE with respect to the previous SotA model. These results push Pishgu to the top position in single-future path prediction by a large margin and show the benefit of an attentive CNN predictor in grasping the meaningful features generated by the GIN in the embedding stage.

Regarding the number of parameters, we can only compare Pishgu to Next \cite{liang2019peeking} and ST-Attention \cite{zhao2020spatial} since other works do not mention their model sizes. Pishgu, with around $18\times$ smaller model size compared to ST-Attention and the outstanding ADE and FDE results, is much more suitable for real-time deployment than previous works. 

\subsection{Real-time Evaluation} \label{sec:realtime}



Real-time path prediction applications are often implemented on resource-constrained embedded devices. Limited memory, limited power, and real-time implementation make lightweight, low-latency models necessary. We report the latency performance of Pishgu on multiple embedded platforms: Nvidia Jetson Xavier NX with a 15W dual-core Nvidia Carmel processor with 8GB of RAM and Jetson Nano with the 10W quad-core ARM Cortex-A57 processor with 2GB of RAM. We report the performance of two embedded platforms to demonstrate the adaptability of Pishgu in different resource-constrained environments. Both platforms are utilized at their respective highest power capacities. All the results on embedded platforms are calculated for the batch size. We test Pishgu for all three domains and report the latency per subject in milliseconds (ms) and throughput in terms of FPS.

The number of samples per frame is utilized to calculate the throughput to demonstrate the performance of Pishgu in real-world, real-time applications. As shown in Table \ref{tab:latency}, high numbers of vehicles in each frame of the NGSIM datasets ensure dense graphs, and millions of data samples help attention mechanism in feature map refinement \cite{cbam18}. However, the number of path predictions for the NGSIM dataset using Pishgu goes to more than 200 vehicles for a single frame. This, in turn, affects the overall throughput of the model even with low latency per sample. As the number of samples per frame for all the other datasets is meager compared to NGSIM, the throughput for other datasets is considerably higher. 

The throughput of NGSIM for the Jetson Xavier NX platform is 2.35 FPS which is 29.83\% better than 1.81 FPS for the lower-power Jetson Nano platform. Similarly, for pedestrian tracking, the throughput for UCY \cite{pellegrini2009you}, ETH \cite{lerner2007crowds}, and ActEv/VIRAT \cite{awad2018benchmarking} on the Jetson Xavier NX is 67.55\%, 17.72\%, and 20.15\% better than that on Jetson Nano embedded platform respectively. The superior performance of the Jetson Xavier NX can be credited to a higher operating power capacity of 15W with a higher clock frequency of 1.9 GHz. On the other hand, Jetson Nano with ARM cortex operates at 10W with a frequency of 1.5 GHz. Higher power distribution among the dual-core of Carmel processors also plays a role in justifying its performance. The average latency per sample for a bird's-eye vehicle view on Jetson Xavier NX is 2.69 ms, which is 23\% better than that of the Jetson Nano platform. Similar trends are observed in all the latency improvements comparison between two embedded platforms in pedestrian bird's-eye and high-angle view datasets. Hence, we demonstrate that Pishgu can be utilized in real-world applications using embedded platforms with reasonable latency and throughput performance.

 \section{Conclusion}\label{sec:conclusion}
This paper proposes Pishgu, a universal path prediction architecture that leverages graph isomorphism networks and attention mechanisms for real-time CPS applications. We evaluate the competency of our architecture in three domains and present extensive domain analysis with their effects on the performance of our architecture in terms of error and complexity. The advantage of the proposed architecture is that it can adapt to multiple domains with best-in-class performance when trained and inferred on datasets in three different domains without any changes in the architecture. This is beneficial for the CPS applications as single architecture can be used for different application domains just by adjusting the model weights. Moreover, Pishgu is designed to be integrated at the local nodes of the real-world edge-based applications in mind, performing in real-time on embedded platforms. Pishgu achieves SotA performance for path prediction in vehicle bird's-eye and pedestrian high-angle view domains on NGSIM \cite{NGSIM_i80, NGSIM_US101}, and ActEV/VIRAT \cite{awad2018benchmarking}, respectively, by a considerable margin. Although vehicle high-angle view domain path prediction has many CPS applications, such as traffic monitoring and safety, there needs to be more work done in this domain. As a next step, we plan to focus on the vehicle high-angle view domain for path prediction. \section*{Acknowledgements}
This research is supported by the National Science Foundation (NSF) under Award Numbers 1831795 and 1932524. 
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
