We provide details omitted in the main text.

\begin{itemize}
	\item \autoref{sec:sup_sdn}: details on constructing the depth cost volume (\autoref{sec:approach} of the main paper).
	\item \autoref{sec:sup_gdc}: detailed implementation of the \GDC algorithm (\autoref{sec:depth_correction} of the main paper).
	\item \autoref{ssec:ES}: additional details of experimental setups (\autoref{sec:exp_setup} of the main paper).
	\item \autoref{ssec:AR}: additional results, analyses, and discussions (\autoref{sec:exp_result} of the main paper).
\end{itemize}

\section{Depth Cost Volume}
\label{sec:sup_sdn}
With \autoref{eq_disp_depth}, we know where each grid  in  corresponds to in  (may not be on a grid). We can then obtain features for each grid in  (\ie, ) by bilinear interpolation over features on grids of  around the non-grid location (\ie, ). We applied the ``grid\_sample'' function in PyTorch for bilinear interpolation.

We use \PSMNet~\citep{chang2018pyramid} as the backbone for our stereo depth estimation network (\SDN). The only change is to construct the depth cost volume before performing 3D convolutions.

\section{Graph-based Depth Correction (\GDC) Algorithm}
\label{sec:sup_gdc}
Here we present the \GDC algorithm in detail (see \autoref{alg::GDC}). The two steps described in the main paper can be easily turned into two (sparse) linear systems and then solved by using Lagrange multipliers. For the first step (\ie, \autoref{eq:qp1}), we solve the same problem as in the main text but we switch the objective to minimizing the -norm of  and set  as a constraint\footnote{These two problems yield identical solutions but we found the second one is easier to solve in practice. We note that, \autoref{eq:qp1} is an under-constrained problem, with infinitely many solutions. To identify a single solution, we add a small  regularization term to the objective (as mentioned in the main text). }. For the second step (\ie, \autoref{eq:qp2}), we use the \emph{Conjugate Gradient} (CG) to iteratively solve the sparse linear system.

\begin{algorithm*}
	\caption{Graph-based depth correction (\GDC). ``'' stands for column-wise concatenation. \label{alg::GDC}}
	\KwIn{Stereo depth map , the corresponding pseudo-LiDAR (PL) point cloud , and LiDAR depths  on the first the  pixels.}
	\KwOut{Corrected depth map }
	\SetAlgoLined
	\SetKwBlock{Begin}{function}{end function}
	\Begin(){
		Solve: \\
		\hspace{2.55em} s.t. \hspace{0.7em} ,\\
		\hspace{5em}  if  (\ie, the set of neighbors of the  point) according to ,\\
		\hspace{5em}  for .\\
		Solve: \\
		\Return 
	}
\end{algorithm*}

\section{Experimental Setup}
\label{ssec:ES}
\subsection{Sparse LiDAR generation}
\label{ssec:sparse}
In this section, we explain how we generate sparser LiDAR with fewer beams from a 64-beam LiDAR point cloud from KITTI dataset in detail. For every point  of the point cloud in one scene (in LiDAR coordinate system (: front, : left, : up, and  is the location of the LiDAR sensor)), we compute the elevation angle  to the LiDAR sensor as

We order the points by their elevation angles and slice them into separate lines by step , starting from  (close to the Velodyne 64-beam LiDAR SPEC). We select LiDAR points whose elevation angles fall within  to be the 2-beam LiDAR signal, and similarly  to be the 4-beam LiDAR signal. We choose them in such a way that consecutive lines has a  interval, following the SPEC of the ``cheap'' 4-beam LiDAR ScaLa.  We visualize these sparsified LiDAR point clouds from the bird's-eye view on one example scene in \autoref{fig::bevsparsed}.

\begin{figure*}[!htb]
	\centering
	\begin{subfigure}[b]{0.29\textwidth}
	    \includegraphics[width=\textwidth]{figures/v_lidar2L.png}
	    \caption{2-beam}
	\end{subfigure}
	\begin{subfigure}[b]{0.29\textwidth}
	    \includegraphics[width=\textwidth]{figures/v_lidar4L.png}
	    \caption{4-beam}
	\end{subfigure}
	\begin{subfigure}[b]{0.29\textwidth}
	    \includegraphics[width=\textwidth]{figures/v_lidar64L.png}
	    \caption{64-beam (full)}
	\end{subfigure}
	\caption{\textbf{Bird's-eye views of sparsified LiDAR on an example scene.} The observer is on the bottom side looking up. We filter out points invisible from the left image. (One floor square is 10m  10m.) \label{fig::bevsparsed}}
\end{figure*}

\subsection{3D object detection algorithms}
\label{ssec:3D}
In this section, we provide more details about the way we train 3D object detection models on pseudo-LiDAR point clouds.
For \AVOD, we use the same model as in~\citep{pseudoLiDAR}.
For \PRCNN, we use the implementation provided by the authors. Since the \PRCNN model exploits the sparse nature of LiDAR point clouds, when training it with pseudo-LiDAR input, we will first sparsify the point clouds into 64 beams using the method described in \autoref{ssec:sparse}.
For \vPIXOR, we implement the same base model structure and data augmentation specified by \cite{yang2018pixor}, but without the ``decode fine-tune'' step and focal loss. Inspired by the trick in~\citep{liang2018deep}, we add another image feature (ResNet-18 by~\cite{he2016deep}) branch along the LiDAR branch, and concatenate the corresponding image features onto the LiDAR branch at each stage. We train \vPIXOR using RMSProp with momentum , learning rate  (decay by 10 after 50 and 80 epochs) for 90 epochs. The BEV evaluation results are similar to the reported results (see \autoref{tbMain}).

\section{Additional Results, Analyses, and Discussions}
\label{ssec:AR}

\subsection{Ablation study}
\label{ssec:AS}

In \autoref{stbl:ablation_sdn} and \autoref{stbl:ablation_lidar} we provide more experimental results aligned with experiments in \autoref{sec:exp_result} of the main paper. We conduct the same experiments on two other models, \AVOD and \vPIXOR, and observe similar trends of improvements brought by learning with the depth loss (from \PSMNet to \PSMNet+DL), constructing the depth cost volume (from \PSMNet+DL to \SDN), and applying \GDC to correct the bias in stereo depth estimation (comparing \SDN+\GDC with \SDN).

We note that, in \autoref{stbl:ablation_lidar}, results of \AVOD (or \vPIXOR) with \SDN + L\# are worse than those with L\# at the moderate and hard settings. This observation is different from that in \autoref{tbl:ablation_lidar}, where \PRCNN with \SDN + L\# outperforms \PRCNN with L\# in 5 out of 6 comparisons. We hypothesize that this is because \PRCNN takes sparsified inputs (see \autoref{ssec:3D}) while \AVOD and \vPIXOR take dense inputs. In the later case, the four replaced LiDAR beams in \SDN+ L\# will be dominated by the dense stereo depths so that \SDN+ L\# is worse than L\#.

\subsection{Using fewer LiDAR beams}
\label{ssec:FewerLiDAR}
In PL++ (\ie, \SDN+ \GDC), we use 4-beam LiDAR to correct the predicted point cloud. In \autoref{tbl:ablation_beam}, we investigate using fewer (and also potentially cheaper) LiDAR beams for depth correction. We observe that even with 2 beams, \GDC can already manage to combine the two signals and yield a better performance than using 2-beam LiDAR or pseudo-LiDAR alone.

\begin{table*}[!htb]
	\centering
	\caption{\textbf{Ablation study on stereo depth estimation.} We report \APBEV ~/ \AP (in \%) of the \textbf{car} category at IoU on the KITTI validation set. DL stands for depth loss. \label{stbl:ablation_sdn}}
	\tabcolsep 2.5pt
	\begin{tabular}{l|c|c|c|c|c|c}
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Depth Estimation}}                                                           & \multicolumn{3}{c|}{\vPIXOR} & \multicolumn{3}{c}{\AVOD} \\ \cline{2-7}
		\multicolumn{1}{c|}{}                                                   & Easy        & Moderate   & Hard   & Easy        & Moderate   & Hard     \\ \hline
		\PSMNetpd                                                                  &  73.9 / - \hspace{10pt}   & 54.0 / - \hspace{10pt}  & 46.9 / -  \hspace{10pt} & 74.9 / 61.9 &  56.8 / 45.3 & 49.0 / 39.0  \\
		\PSMNetpd + DL                                                             &  75.8 / - \hspace{10pt}   & 56.2 / - \hspace{10pt}  & 51.9 / - \hspace{10pt}  & 75.7 / 60.5 & 57.1 / 44.8 & 49.2 / 38.4  \\
		\SDN    & 79.7 / - \hspace{10pt}   & 61.1 / - \hspace{10pt}  & 54.5 / - \hspace{10pt}  & 77.0 / 63.2 & 63.7 / 46.8 & 56.0 / 39.8  \\ \hline
	\end{tabular}
\end{table*}

\begin{table*}[!htb]
	\centering
	\caption{\textbf{Ablation study on leveraging sparse LiDAR.} We report \APBEV ~/ \AP (in \%) of the \textbf{car} category at IoU on the KITTI validation set. L\# stands for 4-beam LiDAR signal. \SDN+L\# means we replace the depth of a portion of pseudo-LiDAR points (\ie, landmark pixels) by L\#. \label{stbl:ablation_lidar}}
	\tabcolsep 2.5pt
	\begin{tabular}{l|c|c|c|c|c|c}
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Depth Estimation}} &  \multicolumn{3}{c|}{\vPIXOR} & \multicolumn{3}{c}{\AVOD} \\ \cline{2-7}
		\multicolumn{1}{c|}{}  &  Easy        & Moderate   & Hard     & Easy        & Moderate   & Hard  \\ \hline
		\SDN    &  79.7 / - \hspace{10pt}   & 61.1 / - \hspace{10pt}  & 54.5 / - \hspace{10pt}  & 77.0 / 63.2 & 63.7 / 46.8 & 56.0 / 39.8  \\
		L\#    & 72.0 / - \hspace{10pt}  & 64.7 / - \hspace{10pt} & 63.6 / - \hspace{10pt}  & 77.0 / 62.1 & 68.8 / 54.7 & 67.1 / 53.0  \\
		\SDN + L\#   & 75.6 / - \hspace{10pt}  & 59.4 / - \hspace{10pt} & 53.2 / - \hspace{10pt}  & 84.1 / 66.0 & 67.0 / 53.1 & 58.8 / 46.4  \\
		\SDN + \GDC    & 84.0 / - \hspace{10pt}  & 71.0 / - \hspace{10pt} & 65.2 / - \hspace{10pt}  & 86.8 / 70.7 & 76.6 / 56.2 & 68.7 / 53.4  \\ \hline
	\end{tabular}
\end{table*}

\begin{table*}[!htb]
	\centering
	\caption{\textbf{Ablation study on the sparsity of LiDAR.} We report \APBEV ~/ \AP (in \%) of the \textbf{car} category at IoU on the KITTI validation set. L\# stands for using sparse LiDAR signal alone. The number in brackets indicates the number of beams in use. \label{tbl:ablation_beam}}
	\tabcolsep 2.5pt
	\begin{tabular}{l|c|c|c|c|c|c}
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Depth Estimation}} & \multicolumn{3}{c|}{\PRCNN}                                                            & \multicolumn{3}{c}{\vPIXOR} \\ \cline{2-7}
		\multicolumn{1}{c|}{}                                                   & Easy                            & Moderate                        & Hard                             & Easy        & Moderate   & Hard  \\ \hline
		SDN & 82.0 / 67.9 & 64.0 / 50.1 & 57.3 / 45.3 & 79.7 / - \hspace{10pt} & 61.1 / - \hspace{10pt} & 54.5 / - \hspace{10pt}  \\ \hline
		L\# (2)  & 69.2 / 46.3 & 62.8 / 41.9 & 61.3 / 40.0 & 66.8 / - \hspace{10pt}  & 55.5 / - \hspace{10pt} & 53.3 / - \hspace{10pt}    \\
		L\# (4)  & 73.2 / 56.1 & 71.3 / 53.1 & 70.5 / 51.5 & 72.0 / - \hspace{10pt}  & 64.7 / - \hspace{10pt} & 63.6 / - \hspace{10pt}    \\
		\hline
		\SDN + \GDC (2)    & 87.2 / 73.3 & 72.0 / 56.6 & 67.1 / 54.1 & 82.0 / - \hspace{10pt}  & 65.3 / - \hspace{10pt} & 61.7 / - \hspace{10pt}  \\
		\SDN + \GDC (4)   & 88.2 / 75.1 & 76.9 / 63.8 & 73.4 / 57.4 & 84.0 / - \hspace{10pt}  & 71.0 / - \hspace{10pt} & 65.2 / - \hspace{10pt}  \\
		\hline
	\end{tabular}
\end{table*}


\begin{table}[!h]
	\centering
	\tabcolsep 2.5pt
	\caption{\textbf{Comparison of \GDC and \PnP for 3D object detection.}  We report \APBEV ~/ \AP (in \%) of the \textbf{car} category at IoU on the KITTI validation set, using \SDN+ \PnP or \SDN+ \GDC for depth estimation and \PRCNN or \vPIXOR for detection.} \label{tbDC}
	\begin{tabular}{l|c|c|c|c|c|c}
		\hline
		\multicolumn{1}{c|}{\multirow{2}{*}{Input signal}} & \multicolumn{3}{c|}{\PRCNN}                                                            & \multicolumn{3}{c}{\vPIXOR} \\ \cline{2-7}
		\multicolumn{1}{c|}{}                                                   & Easy                            & Moderate                        & Hard                             & Easy        & Moderate   & Hard  \\ \hline
		\SDN+ \PnP    & 86.3 / 72.1 & 73.3 / 58.9 & 67.2 / 54.2 & 79.1 / - \hspace{10pt}  & 64.2  / - \hspace{10pt} & 54.0 / - \hspace{10pt}  \\
		\SDN+ \GDC   & 88.2 / 75.1 & 76.9 / 63.8 & 73.4 / 57.4 & 84.0 / - \hspace{10pt}  & 71.0 / - \hspace{10pt} & 65.2 / - \hspace{10pt}  \\
		\hline
	\end{tabular}
\end{table}

\subsection{Depth correction vs. depth completion}
\begin{figure}
	\centering
	\begin{minipage}[t]{.53\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/medians_random_compare}
		\caption{\textbf{Comparison of \GDC and \PnP for depth correction.} We report the median of absolute errors on the KITTI validation set. See text for details. \label{fig:depth_correction}}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{.43\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/error_by_dist.png}
		\caption{\textbf{Median depth estimation errors w.r.t. the shortest distances to 4-beam LiDAR points on KITTI validation set.} \label{fig:DepthValue_shortdist}}
	\end{minipage}
\end{figure}
We compare our \GDC algorithm for depth correction to depth completion algorithms, which aim to ``densify'' LiDAR data beyond the beam lines~\citep{wang2018plug,tomasello2018dscnet,ma2019self,yang2019dense,cheng2018depth, torres2004statistical}\footnote{\cite{torres2004statistical} use MRFs and may thus require less (or even no) training data compared to deep learning algorithms: a property shared by GDC.}. We note that most depth completion approaches take as input a 64-beam LiDAR and a single image, while our focus is on fusing a much sparser 4-beam LiDAR and stereo depths. As such, the two problems are not commensurate. Also, our \GDC algorithm is a general, simple, inference-time approach that \emph{requires no training}, unlike prior learning-based approaches to depth completion.



Here we empirically compare to \PnP~\citep{wang2018plug}, a recently proposed depth completion algorithm compatible with any (even stereo) depth estimation network, similar to \GDC. We use \SDN for initial depth estimation, and evaluate \GDC and \PnP by randomly selecting a fraction of LiDAR points as provided ground truths and calculating the median absolute depth errors on the remaining LiDAR points.
As shown in \autoref{fig:depth_correction}, \GDC outperforms \PnP by a large margin.
\autoref{tbDC} shows a further comparison to \PnP on 3D object detection. We apply \PnP and \GDC respectively to correct the depth estimates obtained from \SDN, train a \PRCNN or \vPIXOR using the resulting pseudo-LiDAR points on the KITTI training set, and compare the detection results on the KITTI validation set. In either case, \SDN + \GDC outperforms \SDN + \PnP by a notable margin.



\begin{table}
	\centering
	\caption{\textbf{Comparison of 3D object detection using the naive and optimized implementation of \GDC.} We report \APBEV ~/ \AP (in \%) of the \textbf{car} category at IoU on the KITTI validation set, using \PRCNN for detection.} \label{tbOPT}
	\begin{tabular}{=l|+c|+c|+c}
		& Easy & Moderate & Hard \\ \hline
		Naive & 88.2 / 75.1 & 76.9 / 63.8 & 73.4 / 57.4 \\
		Optimized & 87.6 / 75.0 & 76.3 / 63.4 & 73.1 / 57.0\\
		\hline
	\end{tabular}
\end{table}

\subsection{Run time}
With the following optimizations for implementation,
\begin{enumerate}
	\item Sub-sampling pseudo-LiDAR points: keeping at most one point within a cubic of size 
	\item Limiting the pseudo-LiDAR points for depth correction: keeping only those whose elevation angles are within  (the range of 4-beam LiDAR plus ; see \autoref{ssec:sparse} for details)
	\item After performing \GDC for depth correction, combining the corrected pseudo-LiDAR points with those outsides the elevation angles of 
\end{enumerate}
\GDC runs in  ms/frame using a single GPU (ms for KD-tree construction and search, ms for solving , and ms for solving ) with negligible performance difference (see \autoref{tbOPT}). For consistency, all results reported in the main paper are based on the naive implementation.
Further speedups can be achieved by CUDA programming for GPUs.


\subsection{Stereo depth vs. detection}
\label{sec::supp_stereo_error}



We quantitatively evaluate the stereo depths by median errors in \autoref{fig:depth_comp} of the main text (numerical values are listed in \autoref{tbDepthValue}). In \autoref{tbDepthValue_mean} we further show mean errors with standard deviation (the large standard deviation likely results from outliers such as occluded pixels around object boundaries). For both tables, we divide pixels into beams according to their truth depths, and evaluate on pixels not on the 4-beam LiDAR. The improvement of \SDN (+ \GDC) over \PSMNetpd becomes larger as we consider pixels farther away. \autoref{tbRange} further demonstrates the relationship between depth quality and detection accuracy: \SDN (+ \GDC) significantly outperforms \PSMNetpd for detecting faraway cars. We note that, for very faraway cars (\ie, 50-70 m), the number of training object instances are extremely small, which suggests that the very poor performance might partially cause by over-fitting.

Further, we apply the same evaluation procedure but group the errors by the shortest distance between each \PL point and the 4-beam LiDAR points in \autoref{fig:DepthValue_shortdist}. We can see that the closer the \PL points are to the 4-beam LiDAR points, the bigger improvement GDC can bring.

\begin{table}[t]
	\centering
	\caption{\textbf{Median depth estimation errors over various depth ranges (numerical values of \autoref{fig:depth_comp}).} \label{tbDepthValue}}
	\begin{tabular}{l|c|c|c|c|c|c|c}
		\multicolumn{1}{c|}{\multirow{2}{*}{Signal}} & \multicolumn{7}{c}{range (m)}                                                                                                                                                                         \\ \cline{2-8}
		\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{0-10} & \multicolumn{1}{c|}{10-20} & \multicolumn{1}{c|}{20-30} & \multicolumn{1}{c|}{30-40} & \multicolumn{1}{c|}{40-50} & \multicolumn{1}{c|}{50-60} & \multicolumn{1}{c}{60-70} \\ \hline
		PSMNet                       & 0.04                      & 0.11                       & 0.36                       & 0.83                       & 1.24                       & 1.98                       & 2.43                      \\
		SDN                                          & 0.07                      & 0.12                       & 0.30                       & 0.60                       & 0.89                       & 1.31                       & 1.73                      \\
		SDN + GDC                                    & 0.07                      & 0.12                       & 0.27                       & 0.51                       & 0.74                       & 1.03                       & 1.53                      \\ \hline
	\end{tabular}
\end{table}
\begin{table}[t]
	\centering
	\caption{\textbf{Mean depth estimation errors (with standard deviation) over various depth ranges.} \label{tbDepthValue_mean}}
	\tabcolsep 2pt
	\begin{tabular}{l|c|c|c|c|c|c|c}
		\multicolumn{1}{c|}{\multirow{2}{*}{Signal}} & \multicolumn{7}{c}{range (m)}                                                                                                                                                                         \\ \cline{2-8}
		\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{0-10} & \multicolumn{1}{c|}{10-20} & \multicolumn{1}{c|}{20-30} & \multicolumn{1}{c|}{30-40} & \multicolumn{1}{c|}{40-50} & \multicolumn{1}{c|}{50-60} & \multicolumn{1}{c}{60-70} \\ \hline
		PSMNet                       & 0.180.93                      & 0.361.20                     & 0.972.32                     & 2.024.05                     & 2.945.64                     & 4.618.03                     & 6.0310.32                    \\
		SDN                                          & 0.210.89                    & 0.351.16                     & 0.872.31                     & 1.804.22                     & 2.676.00                     & 4.278.78                     & 5.8211.23                    \\
		SDN + GDC                                    & 0.210.90                    & 0.351.17                     & 0.842.34                     & 1.744.27                     & 2.596.06                     & 4.148.85                     & 5.7211.29                    \\ \hline
	\end{tabular}
\end{table}

\begin{table}[t]
	\centering
	\caption{\textbf{3D object detection at various depth ranges.} We compare different input signals. We report \APBEV ~/ \AP (in \%) of the \textbf{car} category at IoU on the KITTI validation set, using \PRCNN for detection. In the last two rows we show the number of car objects in KITTI object train and validation sets within different ranges.} \label{tbRange}
	\vskip-5pt
	\begin{tabular}{=l|+c|+c|+c}
		Input signal & 0-30 m & 30-50 m & 50-70 m \\ \hline
		\PSMNet & 65.6 / 54.0 & 15.8 / \hspace{2pt} 6.9  &  \hspace{2pt} 0.0 / \hspace{2pt} 0.0 \\
		\SDN & 68.6 / 56.7 & 27.4 / 11.3 & \hspace{2pt} 0.7 / \hspace{2pt} 0.0 \\
		\SDN+ \GDC & 84.7 / 67.8 & 49.9 / 31.5 & \hspace{2pt} 2.5 / \hspace{2pt} 1.0\\
		\rowstyle{\color{gray}}
		\textsc{LiDAR} & 88.5 / 84.0 & 69.9 / 51.5 & \hspace{2pt} 8.9 / \hspace{2pt} 3.4 \\ \hhline{====}
		\textsc{\#~Objects-Train} & 6903 & 3768 & 76 \\
		\textsc{\#~Objects-Val} & 7379 & 3542 & 39 \\ \hline
	\end{tabular}
	\vspace{-10pt}
\end{table}

\subsection{Connected Components in KNN graphs of \PL points by SDN}
\label{sec::supp_connected_components}
Here, we provide empirical analysis on the relationship between the  we choose in building the K-nearest-neighbor graph of \PL points by SDN and the number of connected components of that graph. We show the results on KITTI validation set in \autoref{fig:cc}. It can be seen that with , the average number of connected components in the graph is smaller than .

\begin{figure}
	\centering
	\begin{minipage}[t]{.48\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{figures/IoU_vs_AP}
		\caption{\textbf{IoU vs. \APBEV on KITTI validation set on the car category (moderate).}\label{fig:IoU}}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{.48\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{figures/cc}
		\caption{\textbf{ vs. average number of connected components in KNN graphs of \PL points by SDN.}\label{fig:cc}}
	\end{minipage}

\end{figure}
\subsection{Failure cases and weakness}
There is still a gap between our approach and LiDAR for faraway objects (see \autoref{tbRange}). We further analyze \APBEV at different IoU in \autoref{fig:IoU}. For low IoU (0.2-0.5), \SDN (+\GDC) is on par with LiDAR, but the gap increases significantly at high IoU thresholds.
This suggests that the predominant gap between our approach and LiDAR is because of mislocalization, perhaps due to residual inaccuracies in depth.

\subsection{Qualitative results}
\label{ssec:QR}
In Figure~\ref{fig:qualitative},\ref{fig:qualitative_2},\ref{fig:qualitative_3} and \autoref{fig:qualitative_4}, we show detection results using \PRCNN (with confidence ) with different input signals on four randomly chosen scenes in the KITTI object validation set. Specifically, we show the results from the frontal-view images and the bird's-eye view (BEV) point clouds. In the BEV map, the observer is on the left-hand side looking to the right. It can be seen that the point clouds generated by \PL++ (\SDN alone or \SDN+\GDC) align better with LiDAR than those generated by \PL (\PSMNet).
For nearby objects (\ie, bounding boxes close to the left in the BEV map), we see that \PRCNN with any point cloud performs fairly well in localization. However, for faraway objects (\ie, bounding boxes close to the right), \PL with depth estimated from \PSMNet predicts objects (red boxes) deviated from the ground truths (green boxes). Moreover, the noisy \PSMNet points also leads to several false positives or negatives. In contrast, the detected boxes by our \PL++, either with \SDN alone or with \SDN+\GDC, align pretty well with the ground truth boxes, justifying our targeted improvement in estimating faraway depths. In \autoref{fig:qualitative_2}, we see one failure case for both \PL and \PL++: the most faraway car is missed, while LiDAR signal can still detect it, suggesting that for very faraway objects stereo-based methods may still have limitation.



\begin{figure*}[htb!]
	\centering
	\includegraphics[width=.85\linewidth]{figures/0031_compressed}
	\caption{\textbf{Qualitative Comparison.} We show the detection results on a KITTI validation scene by \PRCNN with different input point clouds. We visualize them from both frontal-view images and bird's-eye view (BEV) point maps. Ground-truth boxes are in green and predicted bounding boxes are in red. The observer is at the left-hand side of the BEV map looking to the right. In other words, ground truth boxes on the right are more faraway (\ie, deeper) from the observer, and hence hard to localize. Best viewed in color. \label{fig:qualitative_2}}
\end{figure*}

\begin{figure*}[htb!]
	\centering
	\includegraphics[width=.85\linewidth]{figures/0545_compressed}
	\caption{\textbf{Qualitative Comparison - another example.} The same setup as in \autoref{fig:qualitative_2} \label{fig:qualitative_3}}
\end{figure*}

\begin{figure*}[htb!]
	\centering
	\includegraphics[width=.85\linewidth]{figures/0626_compressed}
	\caption{\textbf{Qualitative Comparison - another example.} The same setup as in \autoref{fig:qualitative_2} \label{fig:qualitative_4}}
\end{figure*}

