\noindent
\section*{Supplementary Material}
In this supplementary material, we provide further details on our training parameters in~\Cref{sup:sec1}. \Cref{sup:sec2} contains the closed form solution of the fitting of the flow model $\theta$. Expanded experiments and ablations are found in \Cref{sup:sec3}. Finally, more qualitative results are presented in \cref{sup:sec4}. See the project page, \url{https://www.robots.ox.ac.uk/~vgg/research/gwm}, for additional visualizations, code and models.



\section{Experimental Setup}\label{sup:sec1}
\paragraph{Network.} We use MaskFormer~\cite{cheng2021maskformer} as our segmentation network\footnote{Implementation from  \url{https://github.com/facebookresearch/MaskFormer}.}, and use only the segmentation head. As MaskFormer predicts masks at 4 times lower resolution than input, we modify the \texttt{PixelDecoder} by appending [\textit{Conv}(3), \textit{UpsampleNN(2)}, \textit{Conv}(1)]$\times 2$ to its output layers to bring the masks back up to the input resolution.

For the backbone and appearance features $V$, we leverage a ViT-8 transformer, 
pre-trained on ImageNet~\cite{russakovsky2015imagenet} in a self-supervised manner using DINO~\cite{Caron_2021_ICCV}
to avoid any external sources of supervision. For the hierarchical backbone features to decoder we use the key feature outputs from layers 6, 8, 10, 12.

The input RGB images are interpolated (bi-cubic) to $128\times224$ resolution for input to the network. We interpolate (nearest neighbor) the optical flow to $480\times854$ for the loss.
Output segmentation logits are up-sampled using bi-linear interpolation to the flow resolution for training and again to annotation resolution for evaluation.


\paragraph{Training Hyperparameters.}\label{sup:hparam}
The networks are optimised using AdamW~\cite{loshchilov2018decoupled}, 
with learning rate of $1.5\times10^{-4}$, a schedule of linear warm-up from $1.0\times10^{-6}$ to $1.5\times10^{-4}$ over 1.5k iteration and polynomial decay afterwards. We use batch size of 8 and train for 15k iterations. We additionally employ gradient clipping when the 2-norm exceeds 0.01 for stability. The loss multiplier is $0.03$. 

\paragraph{UNet.} For experiments using U-Net\footnote{Implementation from \url{https://github.com/milesial/Pytorch-UNet}.}, we use the standard 4-layer version. The batch-size is increased to 16 and learning rate to $7.0\times10^{-4}$. We also clip the gradients only when 2-norm exceeds 5.0. All other settings, including optimizer and learning rate schedules, are kept the same. U-Net is not pre-trained and trained from scratch.  

\paragraph{Optical Flow.}
Our method derives its learning signal from optical flow estimated using off-the-shelf frozen networks.
We estimate optical flow for all frames on \DAVIS, \ST, and \FBMS following the practice of MotionGrouping~\cite{yang2021self-supervised}.
We employ RAFT~\cite{teed2020raft} (supervised) using the original resolution for our main experiments, and gaps between frames of $\{-2, -1, 1, 2\}$ for \DAVIS and \ST, and $\{-6, -3, 3, 6\}$ on \FBMS{}\@. When multiple flows are associated with a single frame (multiple gaps), we sample one at random for each iteration.


\section{Quadratic Flow Model: Closed Form Solution} \label{sup:sec2}

\newcommand{\USigma}{\Lambda}

Consider one of $K$ regions $m$ and define $w_u \propto P(m_u=k|I,\Phi)$ the posterior probability for that region, normalized so that $\sum_{u\in\Omega} w_u = 1$ (the scaling factor does not matter for the purpose of finding the minimizer).
We can obtain the minimizer $(A^*,b^*)$ and minimum of the energy
\begin{equation}\label{e:energy}
E(A,b)
=
\sum_{u \in \Omega} w_u \|F_u - Au-b\|^2
\end{equation}
as follows.
Defining
$$
\bar u := \begin{bmatrix} u \\ 1 \end{bmatrix},
~~~~
M := \begin{bmatrix} A & ~~b \end{bmatrix} \in \mathbb{R}^{2\times 6}
$$
allows rewriting the energy as
$$
E(M)
=
\sum_{u \in \Omega} w_u \|F_u - M \bar u\|^2
=
\operatorname{tr}\left(
\USigma_{FF} 
- M \USigma_{\bar \Omega F} 
- \USigma_{F \bar \Omega} M^\top 
+ M \USigma_{\bar \Omega \bar \Omega} M^\top
\right)  ,
$$
where
$$
\USigma_{FF} = \sum_{u \in \Omega} w_u F_uF_u^\top,~~
\USigma_{F\bar \Omega} = \sum_{u \in \Omega} w_u F_u\bar u^\top,~~
\USigma_{\bar \Omega F} = \USigma_{F\bar \Omega}^\top,~~
\USigma_{\bar \Omega \bar \Omega} = \sum_{u \in \Omega} w_u \bar u \bar u^\top.
$$
are the (uncentered) second moment matrices of the flow $F_u$ and homogeneous coordinate vectors $\bar u$.
By inspection of the trace term, the gradient of the energy is given by:
$$
\frac{dE(M)}{dM}
=
2\left(
\USigma_{F \bar\Omega}
-
M \USigma_{\bar\Omega \bar\Omega}
\right)
$$
Hence, the optimal regression matrix $M^*$ and corresponding energy value are
$$
M^* = \USigma_{F \bar \Omega} \USigma_{\bar \Omega \bar \Omega}^{-1},~~~~
E(M^*) = \operatorname{tr}\left(
\USigma_{FF} 
- M^* \USigma_{\Omega \bar F}
\right).
$$

Somewhat more intuitive results can be obtained by centering the moments and resolving for $A$ and $b$ instead of $M$.
Specifically, define:
$$
\mu_\Omega := \sum_{u\in\Omega} w_u u,~~~
\mu_F := \sum_{u\in\Omega} w_u F_u.
$$
The covariance matrices of the vectors are:
\begin{multline*}
\Sigma_{FF} = \sum_{u \in \Omega} w_u (F_u - \mu_F)(F_u-\mu_F)^\top,~~~
\Sigma_{F \Omega} = \sum_{u \in \Omega} w_u (F_u-\mu_F)(u-\mu_\Omega)^\top,\\
\Sigma_{\Omega F} = \USigma_{F\Omega}^\top,~~~
\Sigma_{\Omega \Omega} = \sum_{u \in \Omega} w_u (u - \mu_\Omega) (u - \mu_\Omega)^\top.
\end{multline*}
It is easy to check that
\begin{multline*}
\USigma_{FF} = \Sigma_{FF} + \mu_F\mu_F^\top, ~~~
\USigma_{F\bar \Omega} = \begin{bmatrix}
\Sigma_{F\Omega} + \mu_F\mu_\Omega^\top &~~ \mu_F
\end{bmatrix}, ~~~
\USigma_{\bar\Omega \bar \Omega} = \begin{bmatrix}
\Sigma_{\Omega\Omega} + \mu_\Omega\mu_\Omega^\top &~~ \mu_\Omega \\
\mu_\Omega^\top & ~~1
\end{bmatrix}.
\end{multline*}
From this:
\begin{multline*}
M^* 
= \USigma_{F \bar \Omega} \USigma_{\bar \Omega \bar \Omega}^{-1} =
\begin{bmatrix}
\Sigma_{F\Omega} + \mu_{F}\mu_{\Omega}^\top & ~~\mu_{F}
\end{bmatrix}
\begin{bmatrix}
\Sigma_{\Omega\Omega} + \mu_{\Omega}\mu_{\Omega}^\top & ~~\mu_{\Omega}\\
\mu_{\Omega}^\top & ~~1 \\
\end{bmatrix}^{-1}
\\
=
\begin{bmatrix}
\Sigma_{F\Omega} + \mu_{F}\mu_{\Omega}^\top & ~~\mu_{F}
\end{bmatrix}
\begin{bmatrix}
\Sigma_{\Omega\Omega}^{-1} & ~~-\Sigma_{\Omega\Omega}^{-1} \mu_{\Omega}\\
- \mu_{\Omega}^\top\Sigma_{\Omega\Omega}^{-1}  & ~~1 + \mu_{\Omega}^\top \Sigma_{\Omega\Omega}^{-1}  \mu_{\Omega}\\
\end{bmatrix}
\\
=
\begin{bmatrix}
\Sigma_{F\Omega} \Sigma_{\Omega\Omega}^{-1} & 
~~\mu_F  - \Sigma_{F\Omega}\Sigma_{\Omega\Omega}^{-1}\ \mu_\Omega
\end{bmatrix}
= \begin{bmatrix} A^* & ~~b^*\end{bmatrix}.
\end{multline*}
Hence, the optimal regression coefficients and energy value are also given by:
$$
A^* = \Sigma_{F\Omega} \Sigma_{\Omega\Omega}^{-1}, ~~~
b^* = \mu_F - A^* \mu_\Omega.$$


\section{Further Experiments} \label{sup:sec3}
\subsection{Generalization in Unsupervised Video Segmentation} 
\label{sup:video_gen}
\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{rlccc}
\toprule
& Model & Flow & \textbf{\DAVIS}~($\mathcal{J}$$\uparrow$) & \textbf{\FBMS}~($\mathcal{J}$$\uparrow$) \\
\midrule
\cite{liu2021emergence} & AMD (100 steps) & \xmark & 57.8 & 47.5 \\
& \textbf{Ours} (Zero shot) & ARFlow & 62.5 & 65.4 \\
& \textbf{Ours} (20 steps) & ARFlow & 65.2 & 67.6 \\
\midrule
\cite{meunier2022em-driven} & EM & RAFT & 69.3 & 57.8 \\
& \textbf{Ours} (Zero shot) & RAFT & 66.8 & 73.2 \\
& \textbf{Ours} (20 steps) & RAFT & 76.3 & 77.1 \\
\bottomrule
\end{tabular}
\end{center}   
\caption{\textbf{Generalization performance on unseen \emph{videos}.} 
Few unsupervised methods operate in this setting. AMD trains on YT-VOS, followed by 100 test-time adaptation steps, while EM trains on FlyingThings3D using flow as input. We use (fully unsupervised) ARFlow for fair comparison with AMD. Our method shows better performance after observing motion. (Test-time adaptation uses the training loss. No GT is involved at any point.)}
\label{tab:vid_gen}
\end{table} We also test our model in a video \emph{generalization} setting. In contrast to the protocol of~\cite{yang-loquercio2019unsupervised,yang2021self-supervised}, where evaluation set is observed together with training to infer masks jointly\footnote{Note, no annotations are observed at any point.}, here we train only on frames from the training set.
We report performance on unseen videos. In this case, our method independently segments a collection of frames from a new video, with no way to incorporate motion information.

To ``observe'' motion on \emph{unseen} inputs, we also report results after taking 20 test-time adaptation steps (using our unsupervised loss) for each evaluation sequence in isolation (c.f. AMD \cite{liu2021emergence} takes 100 test-time adaptations steps). That is after training, 
we follow our training setup (optimizer, rate, batch size) and feed frames from the evaluation video and corresponding optical flow, calculate loss and take gradient steps. 
Despite other methods using much larger training sets, our approach shows better performance (\cref{tab:vid_gen}). 



\subsection{Ablation Studies}\label{sup:ablation}

\paragraph{Pretraining.}\label{sup:pretraining}

\begin{table}[t]
\begin{center}
\footnotesize
\begin{tabular}{lccccc}
\toprule
Backbone & Backbone    &      & \textbf{\DAVIS} & \textbf{\ST} & \textbf{\FBMS} \\
model & pretraining & Sup. & $\mathcal{J}$$\uparrow$ & $\mathcal{J}$$\uparrow$ & $\mathcal{J}$$\uparrow$ \\  
\midrule
ViT-8 & ImageNet DINO & \xmark & 79.5 & 78.3 & 77.4 \\ 
UNet & None & \xmark & {78.3}     & {76.8}        & {72.0} \\
\midrule
SWIN-tiny & ImageNet MOBY  & \xmark & 78.3 & 77.4 & 74.6 \\
SWIN-tiny & ImageNet CLS  & \cmark & 78.9 & 77.7 & 75.5 \\
SWIN-tiny & None & \xmark & 78.3 & 75.2 & 68.8 \\
Resnet-50 & ImageNet CLS  & \cmark & 77.5 & 75.8 & 72.9 \\


\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Effect of Pretraining/Backbone.} Our method with MaskFormer benefits from pretraining, with slight improvement offered by supervised (\textit{CLS}) over unsupervised (\textit{MOBY}) pretraining (usng SWIN transformer). Comparable results can be obtained with training from scratch. Best results are obtained using DINO features.}
\label{tab:weights_ablation}
\end{table} \begin{table}[t]
\begin{center}
\footnotesize
\begin{tabular}{lccccc}
\toprule
 &     &      & \textbf{\DAVIS} & \textbf{\ST} & \textbf{\FBMS} \\
Model & $K$ &  Merge & $\mathcal{J}$$\uparrow$ & $\mathcal{J}$$\uparrow$ & $\mathcal{J}$$\uparrow$ \\  
\midrule
Ours & $K=4$ & \cmark & \textbf{79.5} & \textbf{78.3} & \textbf{77.4} \\
\midrule
Spectral clustering & $K=2$ & \xmark & 15.79 & 14.89 & 27.45 \\
\midrule
K-Means & $K=4$  & \cmark & 41.79 & 34.84 & 48.80 \\
K-Means & $K=2$  & \xmark & 20.24 & 21.14 & 38.25 \\ 


\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Feature Clustering without Motion.} We experiment with offline clustering of DINO features to assess the importance of our motion-based formulation. Simply clustering DINO features using K-Means or spectral clustering \cite{melas-kyriazi2022deep} into 2 clusters performs worse. Over-clustering and merging using our cluster-merging approach performs better but still fails to reach our performance.}
\label{tab:dino_ablation}
\end{table} 
Compared to recent methods for video segmentation~\cite{yang2021self-supervised,meunier2022em-driven}, one of the benefits of our formulation is that we can leverage unsupervised pretraining for the segmentation network (\eg, for the ViT backbone of MarkFormer).
This enables our method to be trained in only 15k iterations.
Here, we investigate the importance of the backbone. To this end we replace ViT with Swin-tiny pretrained using MOBY (self-supervised) in \cref{tab:weights_ablation}. The performance differences are small.

Additionally, we investigate the effect of other pretraining strategies on the performance.
Switching to a model pretrained on ImageNet with image-level supervision (\ie a classification task) 
only slightly improves performance showing that the method does not need to rely on supervised pre-training. Finally, we train the model using same settings for 20k iterations from scratch, without any pre-training.
This results in comparable performance on \DAVIS but reduced performance on the smaller datasets.
Comparing backbones without pre-training, UNet gives better results than SWIN-tiny, likely due to smaller networks being easier to train on small datasets.








\paragraph{Feature Clustering without Motion.}
To demonstrate the potential of using motion for discovering objects, in \cref{tab:dino_ablation}, we compare to additional baselines that only rely on clustering visual features. Spectral feature clustering with $K=2$ (based on~\cite{melas-kyriazi2022deep}), on the same visual features we use to merge segments (\ie, DINO) after over-clustering, shows (somewhat unsurprisingly) that learning from motion is important for motion segmentation. Similarly, K-means ($K=2$) on the same features also falls behind our method. Yet, we show that K-means also benefits from over-clustering ($K=4$) and then merging. 

\paragraph{Flow Estimation.}\label{sup:flow}

\begin{table}[t]
\parbox{.48\linewidth}{
    \begin{center}
    \footnotesize
    \begin{tabular}{rlccc}
    \toprule
    \multicolumn{2}{l}{Opt. Flow} & Sup. & \textbf{\DAVIS}~($\mathcal{J}$$\uparrow$) \\ 
    \midrule
    \cite{liu2020learning} & ARFlow    & \xmark & 66.9 \\
    \cite{sun2018pwcnet}   & PWCNet    & \cmark & 74.9 \\
    \cite{teed2020raft}    & RAFT      & \cmark & 79.5 \\
    \bottomrule
    \end{tabular}\end{center}
    \caption{\textbf{Choice of Optical Flow Method.} Measuring the influence of the method to extract optical flow.}
    \label{tab:flow_ablation}
}
\hfill
\parbox{.48\linewidth}{
    \begin{center}
    \footnotesize
    \begin{tabular}{rlccc}
    \toprule
    \multicolumn{2}{l}{Method} & \textbf{\DAVIS}~($\mathcal{J}$$\uparrow$) \\ 
    \midrule
    \cite{yang2021self-supervised} & MG & 53.2 \\
    \cite{liu2021emergence} & AMD & 57.8 \\
    \midrule
     & {\bf Ours}     & {\bf 66.9} \\
    \bottomrule
    \end{tabular}\end{center}
    \caption{\textbf{Fully Unsupervised Video Object Segmentation.} Comparison to the state of the art in unsupervised VOS without reliance on \textit{any} supervision
    }
    \label{tab:unsup_flow_results}
}
\end{table} 

\begin{table}[t]
\footnotesize
\begin{center}
\setlength{\tabcolsep}{4.5pt}
\renewcommand*{\arraystretch}{0.95}
\begin{tabular}{@{}r@{\hspace{2pt}}lccc@{\hspace{10pt}}ccc@{\hspace{10pt}}ccc@{\hspace{10pt}}ccc} 
\toprule
& \multicolumn{1}{c}{}  & \multicolumn{3}{c}{\hspace{-20pt}\textbf{CUB}} & \multicolumn{3}{c}{\hspace{-20pt}\textbf{DUTS}} & \multicolumn{3}{c}{\hspace{-20pt}\textbf{ECSSD}}& \multicolumn{3}{c}{\hspace{-3pt}\textbf{OMRON}} \\ 
\cmidrule(r{10pt}){3-5}
\cmidrule(r{10pt}){6-8}
\cmidrule(r{10pt}){9-11}
\cmidrule{12-14}
                                    &                                                         & Acc           & $\mathcal{J}\uparrow$ & $max F_{\beta}\uparrow$  & Acc           & $\mathcal{J}\uparrow$  & $F_{\beta}\uparrow$  & Acc           & $\mathcal{J}\uparrow$  & $F_{\beta}\uparrow$   &    Acc           &$\mathcal{J}\uparrow$ & $F_{\beta}\uparrow$  \\
\midrule
{}\cite{xia2017wnet}                & WNet$^{\dagger}$                                        & --            & 24.8
& -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
{}\cite{ji19invariant}              & IIC-seg                                                 & --            & 36.5
& -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
{}\cite{bielski2019emergence}       & PertGAN                                                 & --            & 38.0
& -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
{}\cite{chen2019unsupervised}       & ReDO                                                    & 84.5          & 42.6
& -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
{}\cite{kanezaki2018unsupervised}   & UISB                                                    & --            & 44.2
& -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
{}\cite{benny2020onegan}            & OneGAN                                                  & --            & 55.5 
& -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
{}\cite{yu2021unsupervised}        & DRC & --            & 56.4
& -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
{}\cite{he2021ganseg}               & GANSeg       & --            & 62.9 & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\       
{}\cite{voynov2021object}         & Voynov \emph{et~al.}                                                 & 94.0          & 71.0    & 80.7              & 88.1          & 51.1     & 60.0             & 90.6          & 68.4      & 79.0            & 86.0          & 46.4 & 53.3\\
{}\cite{liu2021emergence} & AMD & -- & -- & -- & -- & -- & 60.2 & -- & -- & -- & -- & -- & -- \\
{}\cite{melas-kyriazi2022finding}   & Kyriazi \emph{et~al.}                                            & 92.1          & 66.4      & 78.3            & 89.3          & 52.8      & 61.4            & 91.5 & 71.3    & 80.6     & 88.3          & 50.9 & 58.3\\
{}\cite{melas-kyriazi2022deep}   & Kyriazi \emph{et~al.}                                            & --          & 76.9       & --           & --          & 51.4          & --        & -- & 73.3         & --          & 56.7 & --\\
{}\cite{yang2021dystab} & DyStaB$^\dagger$ & -- & -- & -- & -- & -- & -- & -- & -- & 88.1 & -- & -- & 73.9 \\
{}\cite{wang2022selfsupervised} & TokenCut    & -- & -- & -- & 90.3 & 57.6 & -- & 91.8 & 71.2 & -- & 88.0 & 53.3 & -- \\
{}\cite{shin2022unsupervised} & SelfMask    & -- & -- & -- & 92.3 & 62.6 & -- & 94.4 & 78.1 & -- & 90.1 & 58.2 & -- \\
\midrule
\multicolumn{2}{l}{ \textbf{Ours}}                                            & 93.5 & 64.6 & 80.9     & 91.5 & 49.2 & 65.6  & 88.5 & 56.1 & 74.3                 & 89.3 & 41.31 & 56.3  \\
\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Expanded unsupervised object segmentation} benchmark CUB and three saliency detection benchmarks: DUTS, ECSSD, and DUT-OMRON (\textit{OMRON}). $\dagger$ DyStaB uses CRF post-processing, supervised pre-training, and self-training on each dataset.}
\label{table:unsup_semseg_expanded}
\end{table} 
Finally, our method relies on optical flow estimated by frozen, off-the-shelf networks.
So far we have been using RAFT~\cite{teed2020raft}, as such optical flow network was adopted in our baselines.
In \cref{tab:flow_ablation}, we also consider PWCNet~\cite{sun2018pwcnet} and fully-unsupervised ARFlow~\cite{liu2020learning}.
We observe that the performance of the flow estimator has an impact on the final performance of our method.
Finally, we compare our \emph{fully} unsupervised model (which uses self-supervised pretraining and flow) to fully unsupervised state-of-the-art methods.
Appearance-Motion Decomposition (AMD)~\cite{liu2021emergence} works end-to-end and directly extracts motion features from pairs of images with a PWCNet-like architecture, while MotionGrouping (MG)~\cite{yang2021self-supervised} and our method use ARFlow~\cite{liu2020learning} for optical flow estimation.
In \cref{tab:unsup_flow_results} we show that our method achieves a significant improvement over previous approaches.


\section{Additional Results and Discussion}\label{sup:sec4}
We provide a further breakdown of our results in \cref{tab:supp_seq_davis,tab:supp_seq_stv2,tab:supp_seq_fbms}, reporting per sequence evaluation results on the video segmentation tasks.

\paragraph{Video object segmentation and egomotion.}
We note that some sequences have pronounced egomotion (\eg, camera shaking in \texttt{libby} of DAVIS or inside a moving car in \texttt{camel01} of FBMS). 
Our model performs well on these sequences, demonstrating that it can handle egomotion. 
When \emph{only} the camera is moving, the resulting optical flow would still highlight objects due to parallax.
This provides a learning signal, however, it would likely be
weaker for objects farther away from the camera.
As our method works on a per-frame basis and does not \emph{require} flow during inference, this should not have an impact at test time. 
However, fine-tuning on scenes with only egomotion (see \cref{sup:video_gen} for experiments investigating test-time adaptation)   and only small or far away objects, might lead to the model learning to ignore them.

\paragraph{Image segmentation.}
For unsupervised image segmentation, we show some additional qualitative results for CUB in \cref{fig:unsup_seg_supp1}, DUT-OMRON in \cref{fig:unsup_seg_supp2}, DUTS in \cref{fig:unsup_seg_supp3}, and ECSSD in \cref{fig:unsup_seg_supp4}. Our model, trained on a combined dataset of \DAVIS,~\FBMS~and~\ST, is robust enough to handle a wide array of classes from the above datasets in varying context. Our model can segment both stationary and non-stationary objects and works well when multiple objects are in the foreground.
In \cref{fig:unsup_seg_supp5}, we show a few failure cases for all datasets, where the model struggles mostly with ambiguous foreground objects and, in particular, with close-ups of stationary objects, \eg signs (ECSSD) and buildings (DUT-OMRON).
The model also has issues with boundaries for many objects, \ie the foreground objects are correctly identified but the model fails to fully segment them. 
For example, in DUTS, the snake in the first image has a well segmented head, however, the model does not segment its body accurately.



\begin{table}[t]
    \begin{center}
    \begin{tabular}{cccc@{\hspace{10pt}}ccc}
        \toprule
         & \multicolumn{3}{c}{\textit{w/o CRF}} & \multicolumn{3}{c}{\textit{w/ CRF}} \\
         Sequence & $\mathcal{J}$(M) & $\mathcal{J}$(R) & $\mathcal{J}$(D) & $\mathcal{J}$(M) & $\mathcal{J}$(R) & $\mathcal{J}$(D)  \\
         \midrule
    blackswan      &  67.0 &  100.0 &  -0.8 & 67.4 &  100.0 &    1.1 \\
    bmx-trees      &  58.2 &   76.9 &  19.9 & 59.8 &   76.9 &   17.5 \\
    breakdance     &  86.2 &  100.0 &   4.9 & 87.4 &  100.0 &    5.2 \\
      camel        &  89.4 &  100.0 &   5.7 & 90.6 &  100.0 &    5.5 \\
  car-roundabout   &  81.4 &   90.4 &  26.7 & 81.2 &   90.4 &   25.8 \\
    car-shadow     &  84.3 &  100.0 &   9.0 & 83.9 &  100.0 &    8.0 \\
       cows        &  90.4 &  100.0 &   3.4 & 91.3 &  100.0 &    3.2 \\
   dance-twirl     &  87.4 &  100.0 &  -7.1 & 88.8 &  100.0 &   -6.2 \\
       dog         &  92.9 &  100.0 &  -1.7 & 93.9 &  100.0 &   -1.6 \\
  drift-chicane    &  78.6 &   98.0 &   2.2 & 82.0 &  100.0 &    2.6 \\
  drift-straight   &  80.6 &  100.0 &   7.2 & 82.1 &  100.0 &    8.2 \\
       goat        &  78.6 &  100.0 &   1.7 & 75.8 &  100.0 &    4.5 \\
  horsejump-high   &  84.9 &  100.0 &   6.4 & 88.0 &  100.0 &    4.6 \\
    kite-surf      &  64.4 &   97.9 &   4.5 & 67.5 &   97.9 &    3.1 \\
      libby        &  82.9 &  100.0 &   8.6 & 84.5 &  100.0 &    8.6 \\
  motocross-jump   &  74.1 &   78.9 &   4.1 & 75.1 &   81.6 &    4.1 \\
paragliding-launch &  62.2 &   65.4 &  33.5 & 64.1 &   66.7 &   35.8 \\
     parkour       &  86.1 &  100.0 &  -4.5 & 88.1 &  100.0 &   -3.1 \\
  scooter-black    &  82.1 &   97.6 &  -4.3 & 82.1 &  100.0 &   -4.3 \\
     soapbox       &  79.2 &  100.0 &  -2.8 & 81.0 &  100.0 &   -0.4 \\
\midrule
     Average       &  79.5 &   95.3 &   5.8 & 80.7 &   95.7 &    6.1 \\
            \bottomrule
    \end{tabular}
    \end{center}
    \caption{\textbf{Result breakdown on DAVIS16 validation sequences.} (\textit{M}), (\textit{R}), and (\textit{D}) are mean, recall and decay of IoU, respectively}
    \label{tab:supp_seq_davis}
\end{table}

\begin{table}[t]
\parbox{.48\linewidth}{
    
    \begin{center}
    \begin{tabular}{ccc}
        \toprule
         & \textit{w/o CRF}  & \textit{w/ CRF} \\
         Sequence & $\mathcal{J}$(M) & $\mathcal{J}$(M)  \\
         \midrule
drift            & 86.1 & 86.5 \\
birdfall         & 67.8 & 57.1 \\
girl             & 84.5 & 86.3 \\
cheetah          & 57.0 & 50.8 \\
worm             & 83.7 & 84.0 \\
parachute        & 90.6 & 93.2 \\
monkeydog        & 22.9 & 22.6 \\
hummingbird      & 57.3 & 57.2 \\
soldier          & 77.4 & 77.4 \\
bmx              & 76.4 & 77.5 \\
frog             & 84.1 & 86.7 \\
penguin          & 77.7 & 76.8 \\
monkey           & 75.0 & 75.8 \\
bird of paradise & 92.3 & 94.0 \\
\midrule
Seq. Avg.          & 73.8 & 73.3 \\
Frame Avg.         & 78.3 & 78.9 \\
            \bottomrule
    \end{tabular}
    \end{center}
    \caption{Sequence breakdown on SegTrackv2 dataset. }
    \label{tab:supp_seq_stv2}
}
\hfill
\parbox{.48\linewidth}{
    \begin{center}
    \begin{tabular}{ccc}
        \toprule
         & \textit{w/o CRF}  & \textit{w/ CRF} \\
         Sequence & $\mathcal{J}$(M) & $\mathcal{J}$(M)  \\
         \midrule
camel01 & 86.8 & 91.0 \\
cars1 & 86.9 & 86.8 \\
cars10 & 64.6 & 64.8 \\
cars4 & 81.5 & 82.4 \\
cars5 & 81.6 & 82.1 \\
cats01 & 87.7 & 89.5 \\
cats03 & 69.4 & 63.2 \\
cats06 & 66.5 & 67.4 \\
dogs01 & 76.3 & 75.6 \\
dogs02 & 85.3 & 86.4 \\
farm01 & 90.8 & 90.5 \\
giraffes01 & 82.1 & 83.9 \\
goats01 & 79.9 & 83.7 \\
horses02 & 80.4 & 83.6 \\
horses04 & 59.8 & 60.5 \\
horses05 & 72.8 & 74.5 \\
lion01 & 75.1 & 75.0 \\
marple12 & 81.9 & 81.6 \\
marple2 & 84.4 & 85.9 \\
marple4 & 81.1 & 82.4 \\
marple6 & 95.1 & 95.1 \\
marple7 & 76.6 & 77.6 \\
marple9 & 95.4 & 96.3 \\
people03 & 90.1 & 91.0 \\
people1 & 85.3 & 87.2 \\
people2 & 88.1 & 89.7 \\
rabbits02 & 91.2 & 91.2 \\
rabbits03 & 81.5 & 84.4 \\
rabbits04 & 43.8 & 44.1 \\
tennis & 73.3 & 74.2 \\
\midrule
Seq. Avg.  & 79.8 & 80.7 \\
Frame Avg. & 77.4 & 78.4 \\
            \bottomrule
    \end{tabular}
    \end{center}
    \caption{Sequence breakdown on FBMS59 dataset}
    \label{tab:supp_seq_fbms}
}
\end{table} \begin{figure}[t]
\centering
\begin{tabular}{p{4pt}c}
    & \textbf{CUB} \\[-1pt]
    \rotatebox[origin=l]{90}{\scriptsize \hspace{10pt} Ours \hspace{28pt} GT  \hspace{30pt} Image \hspace{25pt} Ours \hspace{30pt} GT  \hspace{26pt} Image \hspace{38pt} Ours \hspace{30pt} GT  \hspace{30pt} Image} 
     & \includegraphics[page=1,width=.55\textheight,valign=l]{figures/images/unsup-seg-supp-bmvc.pdf} 
\end{tabular}
\caption{\textbf{Qualitative Comparison on CUB}. We train our model on a combined dataset of \DAVIS,~\FBMS~and~\ST. Our method can extract birds in different environments and poses. Our model can segment different species of birds}
\label{fig:unsup_seg_supp1}
\end{figure}
\begin{figure}[t]
\centering
\begin{tabular}{p{4pt}c}
    & \textbf{DUT-OMRON} \\[-1pt]
    \rotatebox[origin=l]{90}{\scriptsize  \hspace{5pt} Ours \hspace{20pt} GT  \hspace{20pt} Image \hspace{26pt} Ours \hspace{35pt} GT  \hspace{37pt} Image \hspace{30pt} Ours \hspace{31pt} GT  \hspace{30pt} Image} 
     & \includegraphics[page=2,width=.55\textheight,valign=l]{figures/images/unsup-seg-supp-bmvc.pdf} 
\end{tabular}
\caption{\textbf{Qualitative Comparison on DUT-OMRON}. We train our model on a combined dataset of \DAVIS,~\FBMS~and~\ST. Our model can segment both stationary and non-stationary objects and is robust enough to work on a wide range of classes }
\label{fig:unsup_seg_supp2}
\end{figure}
\begin{figure}[t]
\centering
\begin{tabular}{p{4pt}c}
    & \textbf{DUTS} \\[-1pt]
    \rotatebox[origin=l]{90}{\scriptsize  \hspace{15pt} Ours \hspace{22pt} GT  \hspace{35pt} Image \hspace{26pt} Ours \hspace{35pt} GT  \hspace{24pt} Image \hspace{25pt} Ours \hspace{25pt} GT  \hspace{30pt} Image} 
     & \includegraphics[page=3,width=.55\textheight,valign=l]{figures/images/unsup-seg-supp-bmvc.pdf} 
\end{tabular}
\caption{\textbf{Qualitative Comparison on DUTS}. We train our model on a combined dataset of \DAVIS,~\FBMS~and~\ST. We can segment a wide array of classes. Our model performs well on scenes where multiple objects are in the foreground}
\label{fig:unsup_seg_supp3}
\end{figure}
\begin{figure}[t]
\centering
\begin{tabular}{p{4pt}c}
    & \textbf{ECSSD} \\[-1pt]
    \rotatebox[origin=l]{90}{\scriptsize  \hspace{15pt} Ours \hspace{35pt} GT  \hspace{38pt} Image \hspace{35pt} Ours \hspace{25pt} GT  \hspace{27pt} Image \hspace{25pt} Ours \hspace{32pt} GT  \hspace{35pt} Image} 
     & \includegraphics[page=4,width=.6\textheight,valign=l]{figures/images/unsup-seg-supp-bmvc.pdf} 
\end{tabular}
\caption{\textbf{Qualitative Comparison on ECSSD}. We train our model on a combined dataset of \DAVIS,~\FBMS~and~\ST. Our model can segment objects from different classes in complex poses}
\label{fig:unsup_seg_supp4}
\end{figure}
\begin{figure}[t]
\centering
\begin{tabular}{p{4pt}c}

    \rotatebox[origin=l]{90}{\scriptsize  \hspace{5pt} Ours \hspace{15pt} GT  \hspace{14pt} Image \hspace{22pt} Ours \hspace{18pt} GT  \hspace{12pt} Image \hspace{22pt} Ours \hspace{16pt} GT  \hspace{11pt} Image \hspace{25pt} Ours \hspace{22pt} GT  \hspace{25pt} Image} 
     & \includegraphics[page=5,width=.6\textheight,valign=l]{figures/images/unsup-seg-supp-bmvc.pdf} 
\end{tabular}
\caption{\textbf{Qualitative Comparison of Failure Cases}. We train our model on a combined dataset of \DAVIS,~\FBMS~and~\ST. Our method can extract salient object in various environments. The model has difficulty where the foreground object is ambiguous\,---\,when there are multiple prominent objects but only few are annotated as salient object. The model also has issues with predicting the object boundaries well for some instances  }
\label{fig:unsup_seg_supp5}
\end{figure}

% 




%