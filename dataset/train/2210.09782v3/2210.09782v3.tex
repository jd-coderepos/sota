\documentclass{article}









\usepackage[final,nonatbib]{neurips_2022}




\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \hypersetup{colorlinks}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{array}
\usepackage{cite}
\setlist[itemize]{leftmargin=*}

\def\eg{\emph{e.g.}}
\def\ie{\emph{i.e.}}
\def\etc{\emph{etc}}
\def\vs{\emph{vs}~}
\newcommand{\ycnote}[1]{\textcolor{red}{\small [#1 --YC]}}
\newcommand{\zongxin}[1]{#1}
\newcommand{\pub}[1]{{\color{gray}{\tiny{[{#1}]}}}}



\title{Decoupling Features in Hierarchical Propagation \\ for Video Object Segmentation}



\author{Zongxin Yang, Yi Yang \\
  \\
  ~CCAI, College of Computer Science and Technology, Zhejiang University \hspace{1mm}~Baidu Research\\
  \texttt{\{yangzongxin, yangyics\}@zju.edu.cn} \\
}

\begin{document}

\maketitle

\let\thefootnote\relax\footnote{: the corresponding author.}

\begin{abstract}
This paper focuses on developing a more effective method of hierarchical propagation for semi-supervised Video Object Segmentation (VOS). Based on vision transformers, the recently-developed Associating Objects with Transformers (AOT) approach introduces hierarchical propagation into VOS and has shown promising results. The hierarchical propagation can gradually propagate information from past frames to the current frame and transfer the current frame feature from object-agnostic to object-specific. However, the increase of object-specific information will inevitably lead to the loss of object-agnostic visual information in deep propagation layers. To solve such a problem and further facilitate the learning of visual embeddings, this paper proposes a Decoupling Features in Hierarchical Propagation (DeAOT) approach. Firstly, DeAOT decouples the hierarchical propagation of object-agnostic and object-specific embeddings by handling them in two independent branches. Secondly, to compensate for the additional computation from dual-branch propagation, we propose an efficient module for constructing hierarchical propagation, \ie, Gated Propagation Module, which is carefully designed with single-head attention. Extensive experiments show that DeAOT significantly outperforms AOT in both accuracy and efficiency. On YouTube-VOS, DeAOT can achieve 86.0\% at 22.4fps and 82.0\% at 53.4fps. Without test-time augmentations, we achieve new state-of-the-art performance on four benchmarks, \ie, YouTube-VOS (86.2\%), DAVIS 2017 (86.2\%), DAVIS 2016 (92.9\%), and VOT 2020 (0.622). Project page: \url{https://github.com/z-x-yang/AOT}.
\end{abstract} 
\section{Introduction}\label{sec:introduction}
Video Object Segmentation (VOS), which aims at recognizing and segmenting one or multiple objects of interest in a given video, has attracted much attention as a fundamental task of video understanding. This paper focuses on semi-supervised VOS, which requires algorithms to track and segment objects throughout a video sequence given objects' annotated masks at one or several frames. 

Early VOS methods are mainly based on finetuning segmentation networks on the annotated frames~\cite{osvos,onavos,premvos} or constructing pixel-wise matching maps~\cite{pml,feelvos}. Based on the advance of attention mechanisms~\cite{att,transformer,nonlocal}, many attention-based VOS algorithms have been proposed in recent years and achieved significant improvement. STM~\cite{spacetime} and the following works~\cite{KMN,cheng2021stcn,hmmn} leverage a memory network to store and read the target features of predicted past frames and apply a non-local attention mechanism to match the target in the current frame. Furthermore, AOT~\cite{aot,aot_workshop,aost} introduces hierarchical propagation into VOS based on transformers~\cite{transformer,detr} and can associate multiple objects collaboratively by utilizing the IDentification (ID) mechanism~\cite{aot}. The hierarchical propagation can gradually propagate ID information from past frames to the current frame and has shown promising VOS performance with remarkable scalability.

Fig.~\ref{fig:aot_prop} shows that AOT's hierarchical propagation can transfer the current frame feature from an object-agnostic visual embedding to an object-specific ID embedding by hierarchically propagating the reference information into the current frame. The hierarchical structure enables AOT to be structurally scalable between state-of-the-art performance and real-time efficiency. Intuitively, the increase of 
ID information will inevitably lead to the loss of initial visual information since the dimension of features is limited. However, matching objects' visual features, the only clues provided by the current frame, is crucial for attention-based VOS solutions. To avoid the loss of visual information in deeper propagation layers and facilitate the learning of visual embeddings, a desirable manner (Fig.~\ref{fig:deaot_prop}) is to decouple object-agnostic and object-specific embeddings in the propagation.

Based on the above motivation, this paper proposes a novel hierarchical propagation approach for VOS, \ie, Decoupling Features in Hierarchical Propagation (DeAOT). Unlike AOT, which shares the embedding space for visual (object-agnostic) and ID (object-specific) embeddings, DeAOT decouples them into different branches using individual propagation processes while sharing their attention maps. To compensate for the additional computation from the dual-branch propagation, we propose a more efficient module for constructing hierarchical propagation, \ie, Gated Propagation Module (GPM). By carefully designing GPM for VOS, we are able to use single-head attention to match objects and propagate information instead of the stronger multi-head attention~\cite{transformer}, which we found to be an efficiency bottleneck of AOT~\cite{aot}.


To evaluate the proposed DeAOT approach, a series of experiments are conducted on three VOS benchmarks (YouTube-VOS~\cite{youtubevos}, DAVIS 2017~\cite{davis2017}, and DAVIS 2016~\cite{davis2016}) and one Visual Object Tracking (VOT) benchmark (VOT 2020~\cite{vot2020}). On the large-scale VOS benchmark, YouTube-VOS, the DeAOT variant networks remarkably outperform AOT counterparts in both accuracy and run-time speed as shown in Fig.~\ref{fig:speed_acc}. Particularly, our R50-DeAOT-L can achieve \textbf{86.0\%} at a nearly real-time speed, \textbf{22.4fps}, and our DeAOT-T can achieve \textbf{82.0\%} at \textbf{53.4fps}, which is superior compared to AOT-T~\cite{aot} (80.2\%, 41.0fps). Without any test-time augmentations, our SwinB-DeAOT-L achieves top-ranked performance on four VOS/VOT benchmarks, \ie, YouTube-VOS 2018/2019 (\textbf{86.2\%/86.1\%}), DAVIS 2017 Val/Test (\textbf{86.2\%/82.8\%}), DAVIS 2016 (\textbf{92.9\%}), and VOT 2020 (\textbf{0.622 EAO}).

\begin{figure}[t!]
\begin{center}

\begin{subfigure}[b]{.35\textwidth}
			\centering
			\includegraphics[height=3.8cm]{Sources/prop1.pdf}
			\caption{AOT-like hierarchical propagation}\label{fig:aot_prop}
\end{subfigure}
\begin{subfigure}[b]{.39\textwidth}
			\centering
			\includegraphics[height=3.8cm]{Sources/prop2.pdf}
			\caption{Decoupling features (ours)}\label{fig:deaot_prop}
\end{subfigure}
\begin{subfigure}[b]{.24\textwidth}
			\centering
			\includegraphics[height=3.7cm]{Sources/speed_accuracy.pdf}
			\caption{Comparison}\label{fig:speed_acc}
\end{subfigure}

\end{center}

\caption{(a) AOT~\cite{aot} hierarchically propagates (Prop) \textit{object-specific} information (\ie, specific to the given object(s)) into the \textit{object-agnostic} visual embedding. (b) By contrast, DeAOT decouples the propagation of visual and ID embeddings in two branches. (c) Speed-accuracy comparison. All the results were fairly recorded on the same device, 1 Tesla V100 GPU.} \label{fig:better}
\end{figure} 

Overall, our contributions are summarized below: 
\begin{itemize}
\vspace{-0.5em}
\item  We propose a highly-effective VOS framework, DeAOT, by decoupling object-agnostic and object-specific features in hierarchical propagation. DeAOT achieves top-ranked performance and efficiency on four VOS/VOT benchmarks~\cite{youtubevos,davis2017,davis2016,vot2020}.
\vspace{-0.5em}
\item  We design an efficient module, GPM, for constructing hierarchical matching and propagation. By using GPM, DeAOT variants are consistently faster than AOT counterparts, although DeAOT's propagation processes are twice as AOT's.
\end{itemize}



 
\section{Related Work}
\noindent\textbf{Semi-supervised Video Object Segmentation.}
Given a video with one or several annotated frames (the first frame in general), semi-supervised VOS~\cite{wang2021survey} requires algorithms to propagate the mask annotations to the entire video. Traditional methods often solve an optimization problem with an energy defined over a graph structure~\cite{tradition1,tradition3,tradition2}. Based on deep neural networks (DNN), deep learning based VOS methods have achieved significant progress and dominated the field in recent years.

\textit{Finetuning-based Methods.} Early DNN-based methods rely on fine-tuning pre-trained segmentation networks at test time to make the networks focus on the given object. Among them, OSVOS~\cite{osvos} and MoNet~\cite{xiao2018monet} propose to fine-tune pre-trained networks on the first-frame annotation. OnAVOS~\cite{onavos} extends the first-frame fine-tuning by introducing an online adaptation mechanism. Following these approaches, MaskTrack~\cite{masktrack} and PReM~\cite{premvos} further utilize optical flow to help propagate the segmentation mask from one frame to the next.

\textit{Template-based Methods.} To avoid using the test-time fine-tuning, many researchers regard the annotated frames as templates and investigate how to match with them. For example, OSMN~\cite{osmn} employs a network to extract object embedding and another one to predict segmentation based on the embedding. PML~\cite{pml} learns pixel-wise embedding with the nearest neighbor classifier, and VideoMatch~\cite{videomatch} uses a matching layer to map the pixels of the current frame to the annotated frame in a learned embedding space. Following these methods, FEELVOS~\cite{feelvos} and CFBI(+)~\cite{cfbi,cfbip} extend the pixel-level matching mechanism by additionally doing local matching with the previous frame, and RPCM~\cite{rpcm} proposes a correction module to improve the reliability of pixel-level matching. Instead of using matching mechanisms, LWL~\cite{LWLVOS} proposes to use an online few-shot learner to learn to decode object segmentation.

\textit{Attention-based Methods.} Based on the advance of attention mechanisms~\cite{att,transformer,nonlocal}, STM~\cite{spacetime} and the following works (\eg, KMN~\cite{KMN} and STCN~\cite{cheng2021stcn}) leverage a memory network to embed past-frame predictions into memory and apply a non-local attention mechanism on the memory to propagate mask information to the current frame. Differently, SST~\cite{sstvos} proposes to calculate pixel-level matching maps based on the attention maps of transformer blocks~\cite{transformer}. Recently, AOT~\cite{aot,aot_workshop,aost} introduces hierarchical propagation into VOS and can associate multiple objects collaboratively with the proposed ID mechanism. 

\noindent\textbf{Visual Transformers.} Transformers~\cite{transformer} was initially proposed to build hierarchical attention-based networks for natural language processing (NLP). Compared to RNNs, transformer networks model global correlation or attention in parallel, leading to better memory efficiency, and thus have been widely used in NLP tasks~\cite{devlin2018bert,radford2019language,synnaeve2019end}. Similar to Non-local Neural Networks~\cite{nonlocal}, transformer blocks compute correlation with all the input elements and aggregate their information by using attention mechanisms~\cite{att}. Recently, transformer blocks were introduced
to computer vision and have shown promising performance in many tasks, such as image classification~\cite{vit,vaswani2021scaling,swin}, object detection~\cite{detr}/segmentation~\cite{vistr,zhu2022instance,pan2022n,liang2022local}, image generation~\cite{parmar2018image}, and video understanding~\cite{arnab2021vivit,videoswin,Liang_2022_CVPR}.

Based on transformers, AOT~\cite{aot} proposes a Long Short-Term Transformer (LSTT) structure for constructing hierarchical propagation. By hierarchically propagating object information, AOT variants~\cite{aot} have shown promising performance with remarkable scalability. Unlike AOT, which shares the embedding space for object-agnostic and object-specific embeddings, we propose to decouple them into different branches using individual propagation processes. Such a dual-branch paradigm avoids the loss of object-agnostic information and achieves significant improvement. Besides, a more efficient structure, GPM, is proposed for hierarchical propagation.


 
\section{Rethinking Hierarchical Propagation for VOS}

Attention-based VOS methods~\cite{spacetime,KMN,cheng2021stcn,aot} are dominating the field of VOS. In these methods, STM~\cite{spacetime} and following algorithms~\cite{KMN,cheng2021stcn} uses a single attention layer to propagate mask information from memorized frames to the current frame. The use of only a single attention layer restricts the scalability of algorithms. Hence, AOT~\cite{aot} introduces hierarchical propagation into VOS by proposing the Long Short-term Transformer (LSTT) structure, which can propagate the mask information in a hierarchical coarse-to-fine manner. By adjusting the layer number of LSTT, AOT variants can be ranged from state-of-the-art performance to real-time run-time speed.

Let  and  denote the query embedding of the current frame, the key embedding, and the value embedding of the memorized frames respectively, where , , ,  represent the temporal, height, width, and channel dimensions. The formula of a common attention-based VOS propagation is,

where the matching (or attention) map is calculated by the correlation function, . 

To formulate a hierarchical propagation with  layers, we further define  as the input feature embedding of -th propagation layer () at  frame. Moreover,  and  stands for the feature embeddings and object masks in the memorized frames with indices . Then, the formulation of -th propagation layer in AOT's hierarchical propagation can be simplified as,

where  denotes the IDentification (ID) embedding~\cite{aot} function used to encode masks. Besides,  and  are trainable parameters for projecting features into matching space and propagation space, respectively. For simplicity, the formulation keeps only the parts related to mask propagation in LSTT.

Obviously, before all the propagation layers, the current frame feature, , is an object-agnostic feature extracted from an image encoder (\eg, ResNet-50~\cite{resnet}). Nevertheless, the mask information  will be gradually and hierarchically propagated into the current frame, and the output feature, , will become object-specific and can be decoded into the ID/mask prediction by a decoder network (\eg, FPN~\cite{fpn}). In other words, step by step, the hierarchical propagation transfers the current frame feature, , from an object-agnostic visual embedding to an object-specific ID embedding, as demonstrated in Fig.~\ref{fig:aot_prop}.

\begin{wrapfigure}[16]{r}{0.35\textwidth}
\begin{center}
\vspace{-6.5mm}
\includegraphics[width=0.35\textwidth]{Sources/id_accuracy.pdf}
\caption{The performance of AOT~\cite{aot} will be degraded by increasing ID's maximum number. }\label{fig:id_acc}
\end{center}
\end{wrapfigure} 
Intuitively, the absorption of object-specific ID information will inevitably lead to the oblivion of object-agnostic visual information within  since the channel dimension of  is limited. Such a phenomenon can also be observed by increasing the ID information directly. As shown in Fig.~\ref{fig:id_acc}, the performance of AOT heavily drops as we increase the information amount of  by containing more IDs inside. On the other hand, the significant progress of VOS in recent years is mainly based on matching object-agnostic visual embeddings (\eg, pixel-level matching methods~\cite{cfbi,cfbip,rpcm} and single-layer attention-based methods~\cite{spacetime,KMN,cheng2021stcn} mentioned above). Hence, we argue that the loss of visual information in deeper propagation layers limits the performance of hierarchical propagation.

\textit{How to design a hierarchical propagation structure which can keep or even refine the initial object-agnostic visual information?} Fig.~\ref{fig:deaot_prop} shows a simple, straightforward, and desirable approach, \ie, propagating object-agnostic and object-specific information in two different branches (Visual Branch and ID Branch). The object-agnostic branch is responsible for gathering visual information, refining visual features, and matching objects. By contrast, the object-specific branch is responsible for absorbing ID information propagated from memorized frames. These two branches share the attention maps used to match objects and propagate features. Compared to the single-branch LSTT, our dual-branch approach can keep and further refine visual features in the hierarchical propagation and thus can further facilitate the learning of visual embeddings. 
\begin{figure}[t!]
\begin{center}

\begin{subfigure}[b]{.33\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Sources/overview.pdf}
			\caption{Overview}\label{fig:overview}
\end{subfigure}
\begin{subfigure}[b]{.46\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Sources/gpm.pdf}

			\caption{Gated Propagation Module (GPM)}\label{fig:gpm}
\end{subfigure}
\begin{subfigure}[b]{.19\textwidth}
			\centering
			\includegraphics[width=0.95\textwidth]{Sources/gp.pdf}

			\caption{GP function}\label{fig:gp}
\end{subfigure}

\end{center}

\caption{(a) Overview. Decoupling Features in Hierarchical Propagation (DeAOT) decouples the propagation of visual embedding and IDentification (ID) embedding~\cite{aot} in two branches, \ie, Visual Branch and ID Branch. The propagation module is the proposed efficient GPM module. (b) A demonstration of the Gated Propagation Module (GPM) in both Visual and ID branches. LN: Layer Normalization~\cite{ln}. (c) We propose to use the Gated Propagation (GP) function to construct GPM. DW-Conv: depth-wise convolution. Mul: matrix multiplication.}
\end{figure} 
\section{Decoupling Features in Hierarchical Propagation}
This section will introduce a new framework, Decoupling Features in Hierarchical Propagation (DeAOT), for solving semi-supervised video object segmentation. We show an overview of DeAOT in Fig.~\ref{fig:overview}. Given a video with a reference frame annotation, DeAOT propagates the annotation to the entire video frame-by-frame. The multi-object annotation is encoded by the IDentification (ID) mechanism~\cite{aot}. Different from AOT, DeAOT decouples the hierarchical propagation of visual embedding and ID embedding, \ie, DeAOT propagates these two embeddings in two branches. Furthermore, DeAOT constructs the hierarchical propagation by using the proposed Gated Propagation Module (GPM), which is more efficient and effective than the LSTT block used in AOT.

\subsection{Hierarchical Dual-branch Propagation}
Different from the previous attention-based VOS methods~\cite{spacetime,KMN,hmmn,aot}, DeAOT propagates objects' visual features and mask features in two parallel branches. In detail, the visual branch is responsible for matching objects, gathering past visual information, and refining object features. To re-identify the objects, the ID branch reuses the matching maps (attention maps) calculated by the visual branch to propagate the ID embedding (encoded by the ID mechanism~\cite{aot}) from past frames to the current frame. Both the branches share the same hierarchical structure with  propagation layers.

\noindent\textbf{Visual Branch} is responsible for matching objects by calculating attention maps on patch-wise visual embeddings. The visual embeddings in the memorized frames will be propagated to the current frame regarding the attention maps. Since the propagation is not directly related to the object-specific ID embedding, the visual branch can learn to refine visual embeddings to be more contrastive but avoid being biased toward the given object-specific information. Let  denote visual embeddings, we modify Eq.~\ref{equ:lstt} into a layer of object-agnostic visual propagation,

which doesn't leverage the object-specific ID embedding, . Thus, the visual branch can learn to keep and refine the visual embedding in the hierarchical propagation.


\noindent\textbf{ID Branch} is designed for propagating the object-specific information from past frames to the current frame. The prediction of object-specific segmentation is essential for VOS and can not be processed by the above object-agnostic visual propagation branch. Let  denote the object-specific embeddings in our identification branch, the formulation of our object-specific ID propagation is,

where  is a trainable projection matrix for the identification propagation. Particularly, the identification propagation shares the same attention maps, , from the visual branch, since the identification of objects is mainly based on objects' visual features instead of their ID indices. Without the visual information, the tracking of objects is inapplicable.

\subsection{Gated Propagation Module}
Instead of using the LSTT block~\cite{aot}, which employs multi-head attention in propagation, we stack the hierarchical propagation based on the proposed Gated Propagation Module (GPM), which is designed based on more efficient single-head attention. 

\noindent\textbf{LSTT Block}~\cite{aot} includes four parts, \ie, a long-term attention responsible for propagating information from the memorized frames (in ), a short-term attention responsible for propagating information from a spatial neighborhood in the previous () frame, a self-attention module for associating objects in the current () frame, and a feed-forward module. The three kinds of attention modules are built on the multi-head~\cite{transformer} extension of Eq.~\ref{equ:att} or Eq.~\ref{equ:lstt}. According to the experiments in Table~\ref{tab:head_num}, reducing the head number from multiple heads (8 heads in default) to a single head will decrease the performance of AOT but can significantly improve the run-time speed, which means the multi-head attention is an efficiency bottleneck of LSTT. Concretely, the computational complexity of long-term attention is , which is proportional to the head number  since each head contains a correlation function, . 

\noindent\textbf{Gated Propagation Function.} To avoid using multiple attention heads but not decrease the network performance, we redesign the attention-based VOS propagation defined in Eq.~\ref{equ:att} and propose a gated propagation function as demonstrated in Fig.~\ref{fig:gp}. Let  denotes a gating embedding, the function is

where  is a non-linear gating function,  denotes element-wise multiplication,  stands for a depth-wise 2D convolution layer~\cite{xception}, and  is the trainable weight of output projection. Firstly, we augment the attention-based propagation (Eq.~\ref{equ:att}) by using a conditional gate, , which we empirically found to be effective in VOS. Notably, the presence
of gating in weak attention mechanisms (\eg, single-head attention) is also beneficial in some transformer-based methods~\cite{liu2021pay,gau} for NLP. Moreover, we leverage a depth-wise convolution  to enhance the modeling of local spatial context in a lightweight manner.

\noindent\textbf{Gated Propagation Module} consists of three kinds of gated propagation, self-propagation, long-term propagation, and short-term propagation. Compared with LSTT, GPM removes the feed-forward module for further saving computation and parameters. All the propagation processes employ the gated propagation function defined in Eq.~\ref{equ:gated_prop}. In DeAOT, both the propagation branches (\ie, visual branch and identification branch) are stacked by GPM as shown in Fig.~\ref{fig:gpm}. 

Based on the formulation of visual propagation (Eq.~\ref{equ:vis_prop}) and ID propagation (Eq.~\ref{equ:id_prop}), the \textbf{Long-term Propagation} can be formulated as


for the visual branch and ID branch, respectively. The ID propagation reuses the attention maps of the visual propagation as discussed in Eq.~\ref{equ:id_prop}. Based on the long-term propagation, we can formulate the \textbf{Short-term Propagation} at spatial location  to be


where  are the feature of  at location  respectively, and  stands for a  spatial neighbourhood centered at location . The short-term propagation for each location  is restricted in its spatial neighbourhood ( or ) of the previous () frame. Since the object motions across several contiguous video frames are always smooth, non-local propagation processes becomes inefficient and not necessary in short-term information propagation~\cite{cfbi}.

Finally, the \textbf{Self-Propagation} can also be formulated similar to the long-term propagation, \ie,


where  is a concatenation process on the channel dimension. In the self-propagations, both the visual embedding  and ID embedding  are used in the calculation of attention maps (\ie, ). Here, the object-specific  performs like a positional embedding~\cite{transformer} additional to the visual embedding . We found that such a process can help associate the objects in the current frame more effectively. Apart from this, the current frame segmentation  is unavailable before being decoded and is not used in the ID self-propagation . For simplicity, we reuse the parameter symbols in Eq.~\ref{equ:lt_vis_prop} and~\ref{equ:lt_id_prop}, but the trainable parameters are not shared with long-term propagation.





 
\section{Implementation Details}\label{sec:implementation}

\noindent\textbf{Network Details:} Consistent with AOT~\cite{aot}, three kinds of encoders are used in our experiments, \ie, MobileNet-V2~\cite{sandler2018mobilenetv2} (in default), ResNet-50 (R50)~\cite{resnet}, and Swin-B~\cite{swin}. The decoder is the same FPN~\cite{fpn} network. Besides, the spatial neighborhood size  is set to 15, and the maximum object number within the ID embedding is 10. In our GPM module, the channel dimension  of visual and ID embeddings is 256, the matching features' dimension  is 128, and the propagation features' dimension  is 512. Moreover, the kernel size of  is 5, and the gating function  is SiLU/Swish~\cite{elfwing2018sigmoid,ramachandran2017searching}.

To make fair comparisons with AOT's variants~\cite{aot}, we build corresponding DeAOT variants with different GPM number  or long-term memory size . The hyper-parameters of these variants are: \textbf{DeAOT-T}: , ; \textbf{DeAOT-S}: , ; \textbf{DeAOT-B}: , ; \textbf{DeAOT-L}: , . DeAOT-T/S/B considers only the reference frame as the long-term memory, leading to consistent run-time speeds. DeAOT-L updates the long-term memory per  (set to 2/5 for training/testing) frames as AOT-L~\cite{aot}. 

\noindent\textbf{Training Details:} Following~\cite{rgmp,spacetime,KMN,hmmn,aot}, we first pre-train DeAOT on synthetic video sequence generated from static image datasets~\cite{voc,coco,cheng2014global,shi2015hierarchical,semantic} by randomly applying multiple image augmentations~\cite{rgmp}. Then, we do main training on the VOS benchmarks~\cite{youtubevos,davis2017} by randomly applying video augmentations~\cite{cfbi,aot}. Besides, we keep our optimization strategies and related hyper-parameters the same as AOT. More details are supplied in Supplementary.





 
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{Sources/comparison.pdf}

    \caption{Qualitative results. (top) DeAOT performs better than AOT~\cite{aot} on tiny or scale-changing objects. (bottom) DeAOT fails to track highly similar objects when serious occlusion happens.}
    \label{fig:comparisons}

\end{figure} 
\section{Experimental Results}\label{sec:experiments}
We conduct experiments on three popular VOS benchmarks (YouTube-VOS~\cite{youtubevos}, DAVIS 2017~\cite{davis2017}, and DAVIS 2016~\cite{davis2016}) and one challenging Visual Object Tracking (VOT) benchmark (VOT 2020~\cite{vot2020}), which gives segmentation annotations and can be used to evaluate VOS algorithms. 

To validate DeAOT's generalization ability, all the benchmarks share the same model parameters. When evaluating YouTube-VOS, we use the default 6fps videos, which are restricted to be smaller than  resolution. On DAVIS, the default 480p 24fps videos are used. For evaluating VOT 2020, more details can be found in the supplementary material.

The evaluation metrics for VOS benchmarks include the  score (calculated as the average IoU score between the prediction and the ground truth mask), the  score (calculated as an average boundary similarity measure between the boundary of the prediction and the ground truth), and their mean value (denoted as \&). As to VOT 2020, we use the official EAO criteria~\cite{vot2020}. We evaluate all the results on official evaluation servers or with official tools.

\subsection{Compare with the State-of-the-art Methods}

\begin{table}[t!]
	\centering
	\caption{The quantitative evaluation on multi-object benchmarks, YouTube-VOS~\cite{youtubevos} and DAVIS 2017~\cite{davis2017}. ///: / on seen/unseen classes.
    : timing extrapolated from single-object speed assuming linear scaling in the number of objects. : recorded on our device.
	}\label{tab:comparisons}

\vspace{1mm}
\scriptsize

\centering
\setlength{\tabcolsep}{2.8pt}

\begin{tabular}{l|ccccc|ccccc|c|ccc|ccc|c}
	
\toprule[1.5pt]
            &    \multicolumn{5}{c|}{YouTube-VOS 2018 Val}  & \multicolumn{6}{c|}{YouTube-VOS 2019 Val} & \multicolumn{3}{c|}{DAVIS-17 Val}  & \multicolumn{4}{c}{DAVIS-17 Test}  \\
\midrule[1pt]
 Method & Avg &  &  &  &   & Avg &  &  &  &  & fps & Avg &  &   & Avg &  &  & fps \\
\midrule[1pt]
KMN\pub{ECCV20}~\cite{KMN}  &  81.4  &  81.4  &  85.6  &  75.3  &  83.3 & - & - & - & - & - & - &  82.8  & 80.0  & 85.6   &  77.2  & 74.1  & 80.3 & -  \\
CFBI\pub{ECCV20}~\cite{cfbi} &  81.4  &  81.1  & 85.8  & 75.3  & 83.4 &  81.0  &  {80.6}  & {85.1}  & {75.2}  & {83.0} & 3.4 & 81.9  & 79.3  & 84.5 &  76.6  & 73.0  & 80.1 & 2.9  \\
SST\pub{CVPR21}~\cite{sstvos} & 81.7  &  81.2  &  -  &  76.0  &  - & 81.8  &  80.9  &  -  &  76.6  &  - & - &  82.5  & 79.9  & 85.1 & - & - & - & - \\
HMMN\pub{ICCV21}~\cite{hmmn} & 82.6  &  82.1  &  87.0  &  76.8  &  84.6 & 82.5  &  81.7  &  86.1  &  77.3  &  85.0 & -  &  84.7  & 81.9  & 87.5  &  78.6  & 74.7  & 82.5 & 3.4 \\
CFBI+\pub{TPAMI21}~\cite{cfbip} &  82.8  &  81.8  & 86.6  & 77.1  & 85.6  &  82.6  &  81.7  & 86.2  & 77.1  & 85.2 & 4.0  &  82.9  & 80.1  & 85.7  &  78.0  & 74.4  & 81.6 & 3.4 \\
STCN\pub{NeurIPS21}~\cite{cheng2021stcn} &  83.0  &  81.9  & 86.5  & 77.9  & 85.7  &  82.7  &  81.1  & 85.4  & 78.2  & 85.9 & 8.4  &  85.4  & 82.2  & 88.6   &  76.1  & 72.7  & 79.6 & 19.5  \\
RPCM\pub{AAAI22}~\cite{rpcm} &  84.0  &  83.1  & 87.7  & 78.5  & 86.7   &  83.9  &  82.6  & 86.9  & 79.1  & 87.1 & -  &  83.7  & 81.3  & 86.0  & 79.2  & 75.8  & 82.6 & - \\
\hline
\hline
AOT-T~\cite{aot} & 80.2  &  80.1  & 84.5  & 74.0  & 82.2  &  79.7  &  79.6  & 83.8  & 73.7  & 81.8 & {41.0}   & 79.9 & 77.4 & 82.3  & 72.0 & 68.3 & 75.7 & {51.4} \\
DeAOT-T & \textbf{82.0}  &  \textbf{81.6}  & \textbf{86.3}  & \textbf{75.8}  & \textbf{84.2}  & \textbf{82.0}  &  \textbf{81.2}  & \textbf{85.6}  & \textbf{76.4}  & \textbf{84.7} & \textbf{53.4}  & \textbf{80.5} & \textbf{77.7} & \textbf{83.3}  & \textbf{73.7} & \textbf{70.0} & \textbf{77.3} & \textbf{63.5} \\
\hline
AOT-S~\cite{aot}  & 82.6  &  82.0  & 86.7  & 76.6  & 85.0  &  82.2 &  81.3  & 85.9  & 76.6  & 84.9 & {27.1}  & \textbf{81.3} & \textbf{78.7} & \textbf{83.9}  &  73.9  & 70.3 & 77.5 & 40.0  \\
DeAOT-S & \textbf{84.0}  &  \textbf{83.3}  & \textbf{88.3}  & \textbf{77.9}  & \textbf{86.6}  & \textbf{83.8}  &  \textbf{82.8}  & \textbf{87.5}  & \textbf{78.1}  & \textbf{86.8} & \textbf{38.7}  & {80.8} & {77.8} & {83.8}  & \textbf{75.4} & \textbf{71.9} & \textbf{79.0} & \textbf{49.2} \\
\hline
AOT-B~\cite{aot}  & 83.5  &  {82.6}  & 87.5  & {77.7}  & 86.0  &  83.3 &  {82.4}  & {87.1}  & 77.8  & 86.0 & 20.5  & \textbf{82.5} & \textbf{79.7} & \textbf{85.2}   &  75.5  & 71.6 & 79.3 & 29.6  \\
DeAOT-B & \textbf{84.6}  &  \textbf{83.9}  & \textbf{88.9}  & \textbf{78.5}  & \textbf{87.0}  & \textbf{84.6}  &  \textbf{83.5}  & \textbf{88.3}  & \textbf{79.1}  & \textbf{87.5} & \textbf{30.4}  & {82.2} & {79.2} & {85.1}  & \textbf{76.2} & \textbf{72.5} & \textbf{79.9} & \textbf{40.9} \\
\hline
AOT-L~\cite{aot}  & {83.8}  &  {82.9}  & {87.9}  & {77.7}  & {86.5}  &  {83.7} &  {82.8}  & {87.5}  & {78.0}  & \textbf{86.7} & 16.0  & {83.8} & \textbf{81.1} & {86.4}   &  \textbf{78.3}  & \textbf{74.3} & \textbf{82.3} & 18.7 \\
DeAOT-L & \textbf{84.8}  &  \textbf{84.2}  & \textbf{89.4}  & \textbf{78.6}  & \textbf{87.0}  & \textbf{84.7}  &  \textbf{83.8}  & \textbf{88.8}  & \textbf{79.0}  & \textbf{87.2} & \textbf{24.7}  & \textbf{84.1} & {81.0} & \textbf{87.1}   & {77.9} & {74.1} & {81.7} & \textbf{28.5}  \\
\hline
R50-AOT-L~\cite{aot}  & {84.1}  &  {83.7}  & {88.5}  & {78.1}  & {86.1}  & {84.1}  &  {83.5}  & {88.1}  & \textbf{78.4}  & {86.3} & 14.9  & {84.9} & \textbf{82.3} & {87.5}  & {79.6} & {75.9} & {83.3} & 18.0 \\
R50-DeAOT-L & \textbf{86.0}  &  \textbf{84.9}  & \textbf{89.9}  & \textbf{80.4}  & \textbf{88.7}  & \textbf{85.9}  &  \textbf{84.6}  & \textbf{89.4}  & \textbf{80.8}  & \textbf{88.9} & \textbf{22.4}  & \textbf{85.2} & {82.2} & \textbf{88.2}  & \textbf{80.7} & \textbf{76.9} & \textbf{84.5} & \textbf{27.0} \\
\hline
SwinB-AOT-L~\cite{aot}  & {84.5}  &  {84.3}  & {89.3}  & {77.9}  & {86.4}  & {84.5}  &  {84.0}  & {88.8}  & {78.4}  & {86.7} & 9.3  & {85.4} & {82.4} & {88.4}   & {81.2} & {77.3} & {85.1} & 12.1  \\
SwinB-DeAOT-L & \textbf{86.2}  &  \textbf{85.6}  & \textbf{90.6}  & \textbf{80.0}  & \textbf{88.4}  & \textbf{86.1}  &  \textbf{85.3}  & \textbf{90.2}  & \textbf{80.4}  & \textbf{88.6} & \textbf{11.9}  & \textbf{86.2} & \textbf{83.1} & \textbf{89.2}   & \textbf{82.8} & \textbf{78.9} & \textbf{86.7} & \textbf{15.4} \\
\bottomrule[1.5pt]
\end{tabular}



\end{table}





 
\noindent \textbf{YouTube-VOS}~\cite{youtubevos} is a large-scale multi-object VOS benchmark, which contains 3471 videos in the training split with 65 categories and 474/507 videos in the Validation 2018/2019 split with additional 26 unseen categories. Table~\ref{tab:comparisons} shows that DeAOT variants remarkably outperforms AOT counterparts in both accuracy and run-time speed on YouTube-VOS 2018/2019. For example, our R50-DeAOT-L achieves \textbf{86.0\%/85.9\%} (\&) at \textbf{22.4fps}, which is superior compared to R50-AOT-L~\cite{aot} (84.1\%/84.1\% at 14.9fps). Particularly, our SwinB-DeAOT-L achieves new state-of-the-art performance (\textbf{86.2\%/86.1\%}), surpassing previous methods by more than 1.7\%/1.6\%. In addition, our smallest variant, DeAOT-T, precedes SST~\cite{sstvos} (\textbf{82.0\%/82.0\%} \vs 81.7\%/81.8\%) and runs about \textbf{15} faster than CFBI~\cite{cfbi} (\textbf{53.4fps} \vs 3.4fps).

\begin{wraptable}[20]{r}{0.49\textwidth}
\begin{center}
\vspace{-8.5mm}
\caption{The quantitative evaluation on the single-object benchmarks, DAVIS 2016~\cite{davis2016} and VOT 2020~\cite{vot2020}. EAO: real-time EAO metric~\cite{vot2020}.}\label{tab:davis2016}
\scriptsize
\setlength{\tabcolsep}{2.7pt}
\begin{tabular}{l| c c c| c| c| c }
\toprule[1.5pt]
    & \multicolumn{4}{c|}{DAVIS 2016} & \multicolumn{2}{c}{VOT 2020} \\
\midrule[1pt]
 Method  & Avg &  &  & fps & EAO & EAO \\
\midrule[1pt]
CFBI+~\cite{cfbip}   & 89.9  & 88.7 & 91.1  & 5.9 & - & -  \\
RPCM~\cite{rpcm}  &  90.6  & 87.1  & 94.0 & 5.8 & - & - \\
HMMN~\cite{hmmn}   &  90.8 & {89.6}  & {92.0}  & 10.0 & - & - \\
STCN~\cite{cheng2021stcn}   &  91.6 & {90.8}  & {92.5}  & 27.2 & - & -  \\
\hline
AlphaRef~\cite{alpharef} &  - & - & - & - & 0.482 & 0.486  \\
RPT~\cite{rpt} &  - & - & - & - & 0.530 & 0.290  \\
MixFormer-L~\cite{mixformer} &  - & - & - & - & 0.555 & -  \\
\hline
\hline
AOT-T~\cite{aot}  &  86.8  & 86.1 & 87.4 & {51.4} & 0.435 & 0.433  \\
DeAOT-T  & \textbf{88.9} & \textbf{87.8} & \textbf{89.9} & \textbf{63.5} & \textbf{0.472} & \textbf{0.463}  \\
\hline
AOT-S~\cite{aot}  &  \textbf{89.4}  & \textbf{88.6} & 90.2 & 40.0 & 0.512 & 0.499  \\
DeAOT-S  & {89.3} & {87.6} & \textbf{90.9} & \textbf{49.2} & \textbf{0.593} & \textbf{0.559}  \\
\hline
AOT-B~\cite{aot}  &  89.9  & 88.7 & 91.1 & 29.6 & 0.541 & 0.533 \\
DeAOT-B  & \textbf{91.0} & \textbf{89.4} & \textbf{92.5} & \textbf{40.9} & \textbf{0.571} & \textbf{0.542} \\
\hline
AOT-L~\cite{aot}  &  {90.4}  & {89.6} & {91.1} & 18.7 & 0.574 & 0.560  \\
DeAOT-L  & \textbf{92.0} & \textbf{90.3} & \textbf{93.7} & \textbf{28.5} & \textbf{0.591} & \textbf{0.554}  \\
\hline
R50-AOT-L~\cite{aot}  &  {91.1}  & {90.1} & {92.1} & 18.0 & 0.569 & 0.540  \\
R50-DeAOT-L  & \textbf{92.3} & \textbf{90.5} & \textbf{94.0} & \textbf{27.0} & \textbf{0.613} & \textbf{0.571}  \\
\hline
SwinB-AOT-L~\cite{aot}  &  {92.0}  & {90.7} & {93.3} & 12.1 & 0.586 & 0.523 \\
SwinB-DeAOT-L  & \textbf{92.9} & \textbf{91.1} & \textbf{94.7} & \textbf{15.4} & \textbf{0.622} & \textbf{0.559}  \\
\bottomrule[1.5pt]

\end{tabular}
\end{center}
\end{wraptable} 

\noindent \textbf{DAVIS 2017}~\cite{davis2017} is a multi-object extension of DAVIS 2016. The training/validation split consists of 60/30 videos with 138/59 objects, and the test split contains 30 more challenging videos with 89 objects. As shown in Table~\ref{tab:comparisons}, DeAOT variants can generalize to DAVIS 2017 well. R50-DeAOT-L achieves \textbf{85.2\%/80.7\%} on the validation/test split at a real-time speed (\textbf{27fps}), surpassing R50-AOT-L in accuracy and efficiency. Also, SwinB-DeAOT-L achieves the top-ranked performance on DAVIS 2017 (\textbf{86.2\%/82.8\%}).



\noindent \textbf{DAVIS 2016}~\cite{davis2016} is a single-object benchmark containing 20 videos in the validation split, and we show related experiments in Table~\ref{tab:davis2016}. Although AOT-like methods focus on multi-object scenarios, our DeAOT-L is faster and more robust than STCN~\cite{cheng2021stcn}, whose architecture was designed for single-object VOS. Besides, SwinB-DeAOT-L achieves \textbf{92.9\%} and outperforms all the VOS methods as well.



\noindent \textbf{VOT 2020}~\cite{vot2020} consists of 60 single-object videos with challenging scenarios including fast motion, occlusion, etc. The average frame number of VOT 2020 is 327, which is much longer than the maximum video length of the above VOS benchmarks. DeAOT shows superior performance on VOT 2020 in Table~\ref{tab:davis2016}. The DeAOT variants larger than DeAOT-T outperform MixFormer-L~\cite{mixformer} (the state-of-the-art tracker), RPT~\cite{rpt} (VOT 2020 short-term challenge winner), and AlphaRef~\cite{alpharef} (VOT 2020 real-time challenge winner) in both EAO and real-time EAO scores. Specifically, SwinB-DeAOT-L achieves \textbf{0.622} EAO, outstandingly exceeding MixFormer-L by \textbf{0.067}, and R50-DeAOT-L achieves \textbf{0.571} EAO under a \textbf{real-time} requirement, impressively overtaking AlphaRef by \textbf{0.085}.

\noindent \textbf{Qualitative results:} Fig.~\ref{fig:comparisons} give qualitative comparisons to AOT. By introducing the dual-branch propagation, R50-DeAOT-L performs better than R50-AOT-L on tiny or scale-changing objects (\textit{ski poles} or \textit{ski board}). Nevertheless, R50-DeAOT-L still may fails to track multiple highly similar objects (\textit{dancer} and \textit{cow}) when serious occlusion happens.


\begin{table}[t!]
	\centering
	\caption{Ablation study. The experiments are conducted on YouTube-VOS 2018~\cite{youtubevos} and based on DeAOT-S without pre-training on static images. De: decoupling features. : the channel dimension. Prop: propagation type. LT/ST: long-term/short-term. : kernel size.}\label{tab:ablation}

\begin{subtable}{.245\textwidth}
\center
\caption{Propagation module}\label{tab:prop}
\setlength{\tabcolsep}{2pt}
\scriptsize
\begin{tabular}{l |c| c c c}
    \toprule[1.5pt]
    Module &  &  &  &  \\
    \midrule[1pt]
    \textbf{GPM} & \textbf{256} & \textbf{82.5} & \textbf{82.3} & \textbf{76.1} \\
    \hline
    \textit{w/o} De & 256 & {81.5} & {81.4} & {75.0} \\
    \textit{w/o} De & 512 & {82.0} & {82.1} & {75.4} \\
    \hline
    \hline
    LSTT & 256 & 80.3 & 80.6 & 73.7 \\
    \bottomrule[1.5pt]
\end{tabular}
\end{subtable}
\begin{subtable}{.29\textwidth}
\center
\caption{Head number ()}\label{tab:head_num}
\setlength{\tabcolsep}{2pt}
\scriptsize
\begin{tabular}{l |c| c c c |c}
    \toprule[1.5pt]
    Model &  &  &  &  & fps \\
    \midrule[1pt]
    \textbf{DeAOT} & \textbf{1} & \textbf{82.5} & \textbf{82.3} & \textbf{76.1} & 38.7 \\
    DeAOT & 8 & \textbf{82.5} & \textbf{82.3} & 75.8 & 24.7 \\
    \hline
    \hline
    AOT & 1 & 79.6 & 80.1 & 72.6 & \textbf{44.6} \\
    AOT & 8 & 80.3 & 80.6 & 73.7 & 27.1 \\
    \bottomrule[1.5pt]
\end{tabular}
\end{subtable}
\begin{subtable}{.25\textwidth}
\center
\caption{Attention map}\label{tab:att_map}
\setlength{\tabcolsep}{2pt}
\scriptsize
\begin{tabular}{l |c c| c c c}
    \toprule[1.5pt]
    Prop & Vis & ID &  &  &  \\
    \midrule[1pt]
    \textbf{LT/ST} & \checkmark &  & \textbf{82.5} & \textbf{82.3} & \textbf{76.1} \\
    LT/ST & \checkmark & \checkmark & {82.1} & {82.2} & {75.7} \\
    \hline
    \hline
    \textbf{Self} & \checkmark & \checkmark & \textbf{82.5} & \textbf{82.3} & \textbf{76.1} \\
    Self & \checkmark &  & {82.2} & {82.1} & {75.7} \\
    \bottomrule[1.5pt]
\end{tabular}
\end{subtable}
\begin{subtable}{.195\textwidth}
\center
\caption{ of }\label{tab:ks}
\setlength{\tabcolsep}{2pt}
\scriptsize
\begin{tabular}{l| c c c}
    \toprule[1.5pt]
     &  &  &  \\
    \midrule[1pt]
    \textbf{5} & \textbf{82.5} & \textbf{82.3} & \textbf{76.1} \\
    \hline
     0 & {81.1} & {81.5} & {74.2} \\
     3 & {82.2} & {82.2} & {76.1} \\
     9 & 82.4 & 82.2 & 75.8 \\
    \bottomrule[1.5pt]
\end{tabular}
\end{subtable}

\end{table} \subsection{Ablation Study}
This section analyzes the necessity of dual-branch propagation and GPM of DeAOT in Table~\ref{tab:ablation}.

\textbf{Propagation module:} Table~\ref{tab:prop} shows that the performance of DeAOT drops from 82.5\% to 81.5\% by coupling the propagation of visual and ID embeddings (\textit{w/o} De) like AOT. Furthermore, doubling the channel dimensions only partially relieves the performance loss. Moreover, the performance will be seriously degraded to 80.3\% by replacing our GPM with the LSTT module of AOT. In conclusion, the dual-branch propagation approach and the GPM module are crucial in improving VOS performance.

\textbf{Head number:} According to the results in Table~\ref{tab:head_num}, the head number () of attention-based modules is negatively correlated with the efficiency of AOT/De-AOT. The single-head AOT (44.6fps) runs much faster than the default AOT (=8, 27.1fps) but loses 0.7\% accuracy. By contrast, DeAOT is robust to the head number by using our proposed GPM module.

\textbf{Attention map:} Our DeAOT shares the attention maps between two propagation branches. Table~\ref{tab:att_map} shows the study of different kinds of attention maps. Concretely, visual embeddings are essential in building attention maps in the long-term/short-term propagation, whose attention maps are used to match objects. Introducing ID embeddings does not help learn better visual embeddings and will decrease the performance (82.5\% \vs 82.1\%). In the self-propagation, however, utilizing the ID embedding as a positional embedding will facilitate the association of objects (82.2\% \vs 82.5\%) in the current frame.

\textbf{Kernel size of :} Large receptive fields have been proved to be critical in segmentation-related tasks~\cite{deeplabv3p}. The depth-wise convolution, , is an important part of GPM for enlarging the receptive fields. Without , the performance of DeAOT drops from 82.5\% to 81.1\%, as shown in Table~\ref{tab:ks}. We empirically found the best kernel size of  is 5 among . 
\section{Conclusion}
This paper proposes a highly effective and efficient framework, Decoupling Features in Hierarchical Propagation (DeAOT), for video object segmentation. Based on the rethinking of AOT-like hierarchical propagation, we propose to decouple the propagation of visual and ID embeddings into two network branches and thus avoid the loss of visual information in deep propagation layers. Besides, we propose the Gated Propagation Module (GPM), an efficient module for constructing hierarchical VOS propagation. Applying GPM to the dual-branch propagation, our DeAOT variant networks achieve new state-of-the-art performance on four VOS/VOT benchmarks with superior run-time speed compared to previous solutions. 
\noindent \textbf{Acknowledgements.} This work is partly supported by the Fundamental Research Funds for the Central Universities (No. 226-2022-00051).




{
\bibliographystyle{splncs04}
\bibliography{reference}
}



\end{document}
