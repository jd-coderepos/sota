
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{fourier} 
\usepackage{makecell}
\usepackage{tablefootnote}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

\hypersetup{colorlinks, breaklinks, citecolor=[rgb]{0.0007, 0.44, 0.737},anchorcolor=[rgb]{0.0007, 0.44, 0.737}, linkcolor=[rgb]{0.0007, 0.44, 0.737},}
\title{Robust and accelerated single-spike spiking neural network training with applicability to challenging temporal tasks}





\author{Luke Taylor\\
  University of Oxford\\
  Oxford, United Kingdom \\
  \texttt{luke.taylor@hertford.ox.ac.uk} \\
\And
  Andrew King \\
  University of Oxford\\
  Oxford, United Kingdom \\
  \texttt{andrew.king@dpag.ox.ac.uk} \\
  \AND
  Nicol Harper \\
  University of Oxford\\
  Oxford, United Kingdom \\
  \texttt{nicol.harper@dpag.ox.ac.uk} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Spiking neural networks (SNNs), particularly the single-spike variant in which neurons spike at most once, are considerably more energy efficient than standard artificial neural networks (ANNs). However, single-spike SSNs are difficult to train due to their dynamic and non-differentiable nature, where current solutions are either slow or suffer from training instabilities. These networks have also been critiqued for their limited computational applicability such as being unsuitable for time-series datasets. We propose a new model for training single-spike SNNs which mitigates the aforementioned training issues and obtains competitive results across various image and neuromorphic datasets, with up to a  training speedup and up to an  reduction in spikes compared to the multi-spike SNN. Notably, our model performs on par with multi-spike SNNs in challenging tasks involving neuromorphic time-series datasets, demonstrating a broader computational role for single-spike SNNs than previously believed.
\end{abstract}

\section{Introduction}
Artificial neural networks (ANNs) have achieved impressive feats over recent years, 
obtaining human-level performance on visual and auditory tasks \citep{hinton2012deep, he2016deep}, natural language processing \citep{brown2020language} and challenging games \citep{mnih2015human, silver2017mastering, vinyals2019grandmaster}. However, as the difficulty and complexity of the tasks increase, so has the size of the networks required to solve them, demanding a substantial and unsustainable amount of energy \citep{strubell2019energy, schwartz2020green}. Inspired by the extreme energy efficiency of the brain \citep{sokoloff1960metabolism}, spiking neural networks (SNNs) emulated on neuromorphic computers attempt to solve this dilemma, requiring significantly less energy than ANNs \citep{wunderlich2019demonstrating}. These networks are of growing interest, obtaining noteworthy results on visual \citep{fang2021deep, zhou2021spiking}, auditory \citep{yin2020effective, yao2021temporal} and reinforcement learning problems \citep{patel2019improved, tang2020deep, bellec2020solution}.

A particular class of SNNs in which individual neurons respond with at most one spike aims to further amplify the energy and scaling advantages of SNNs. Inspired by the sparse spike processing shown to exist at least for certain stimuli in the auditory and visual systems \citep{heil2004first, gollisch2008rapid}, and forming a class of universal function approximator \citep{comsa2020temporal}, these networks obtain extreme energy efficiency due to their single-spike nature \citep{oh2021spiking, liang20211}. Although providing a promising path toward building very large and energy-efficient networks, we are yet to understand how to properly train these SNNs. The success of the backprop training algorithm in ANNs does not naturally transfer to single- and multi-spike SNNs due to their non-differentiable activation function. Current attempts at training are either slow (as time is sequentially simulated) or suffer from training instabilities (e.g. the dead neuron problem) and idiosyncrasies (e.g. requiring particular regularisation) \citep{eshraghian2021training}. Additionally, it has been argued that single-spike networks have limited applicability and are not suited for temporal problems, as recently pointed out by \cite{eshraghian2021training}: "[...] it enforces stringent priors upon the network (e.g., each neuron must fire only once) that are incompatible with dynamically changing input data" and \cite{zenke2021visualizing}: "[...] only using single spikes in each neuron has its limits and is less suitable for processing temporal stimuli, such as electroencephalogram (EEG) signals, speech, or videos".
		
In this work we address these shortcomings by proposing a new model for training single-spike networks, for which the main contributions are summarised as follows.
\begin{enumerate}
	\item Our model for training single-spike SNNs eschews all sequential dependence on time and exclusively relies on GPU parallelisable non-sequential operations. We experimentally validate this to obtain faster training times over sequentially trained control models on synthetic benchmarks (up to  speedup) and real datasets (up to  speedup). 
	\item We obtain competitive accuracies on various image and neuromorphic datasets with extreme spike sparsity (up to  fewer spikes than standard multi-spike SNNs), with our model being insensitive to the dead neuron problem and not requiring careful network regularisation. In other single-spike training methods, but not in our model, the dead neuron problem tends to halt learning due to reduced network activity.
	\item We showcase our model's applicability in deeper and convolutional networks, and through the inclusion of trainable membrane time constants manage to solve difficult temporal problems otherwise thought to be unsolvable by single-spike networks.
\end{enumerate}

\section{Background and related work}
\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{figures/figure1}
	\centering
	\caption{Spiking neuron dynamics. \textbf{a.} Left: A multi-spike neuron emitting and receiving (per presynaptic terminal) multiple spikes. Right: Input and output activity of the neuron (bottom panel: Input raster, middle panel: Input current  and top panel: Membrane potential . Dotted line represents the firing threshold and a dot above denotes a spike). \textbf{b.} Left: A single-spike neuron emitting and receiving (per presynaptic terminal) at most one spike per stimulus. Right: Input and output activity of the neuron).}
	\label{fig:intro}
\end{figure}
\subsection{Single-spike model}
\label{sec:21}
A spiking neural network (SNN) consists of artificial neurons which output binary signals known as spikes (Figure \ref{fig:intro}a). Assume a feedforward network architecture of  fully connected layers, where each layer  consists of  neurons that are fully connected to the next layer  via synaptic weights . Neuron  in layer  emits a spike  at time  if its membrane potential  reaches firing threshold .

Membrane potentials evolve according to the leaky integrate and fire (LIF) model

where  is the membrane time constant and  is the input resistance \citep{gerstner2014neuronal}.\footnote{Note, we use  to refer to continuous time and  to refer to discrete time.} Without loss of generality the LIF model is normalised ( by ; see Appendix) and discretised using the forward Euler method (see Appendix), from which the membrane potential can be computed at every discrete simulation time step  for  using the difference equation below.

The membrane potential is charged from the current induced by the incoming presynaptic spikes  and from the constant bias current source . Over time, this potential dissipates, and the degree of dissipation is captured by  (for simulation time-step size ). The neuron's membrane potential is at resting state  in the absence of any input current and emits a spike  if the potential rises above firing threshold  (after which it is reduced back close to resting state).

To enforce the single-spike constraint, we keep track if a neuron has spiked prior to time  using the variable , which is zero before the first spike and one thereafter (). We then redefine the output spikes as , thus ensuring that no more than a single spike is emitted during simulation (Figure \ref{fig:intro}b).

\subsection{Single-spike training techniques}
The main problem with training single- and multi-spike SNNs is the non-differentiable nature of their activation function. This precludes the direct use of the backprop algorithm \citep{rumelhart1986learning}, which has underpinned the successful training of ANNs. Various SNN training solutions have been proposed, which we group into three categories.

\paragraph{Shadow training} Instead of directly training a SNN, an already trained ANN is mapped to a SNN. This approach has actively been explored in the multi-spike setting \citep{o2013real, esser2015backpropagation, rueckauer2016theory, rueckauer2017conversion}, with recent work extending this to single-spike networks \citep{stockl2019recognizing, park2020t2fsnn}. Although these approaches permit the training of large networks, they come with various shortcomings. Some shortcomings are method specific, such as \cite{stockl2019recognizing} who outline how a single ANN unit can be represented as a network of spiking units. However, this leads to an undesirable blowup of network parameters in their conversion process (which is avoided by our approach). Other shortcomings are more general, such as the lack of support for training neural parameters besides synaptic weights (which our approach permits) or inference accuracy being lost in the conversion process, where mapped SNNs perform worse than the original ANNs (which we avoid).

\paragraph{Training using the spike times} An approach used to directly train SNNs using backprop involves passing gradients through the time of spiking, which sidesteps the aforementioned non-differentiability issue \citep{bohte2002error, mostafa2017supervised, comsa2020temporal, kheradpisheh2020temporal, zhang2021rectified, zhou2021spiking, zhou2021temporal}. Although commonly used for training single-spike SNNs, this approach suffers from various shortcomings, such as 1. the dead neuron problem, where a lack of spiking activity halts the learning process (which we overcome), 2. being usually constrained to integrate and fire (IF) neurons (where we support both the IF and LIF model), 3. having performance dependent on the computationally costly processing of presynaptic spikes using postsynaptic potential (PSP) kernels (which we show not to be necessary) and 4. requiring careful network regularisation (which we avoid).

\paragraph{Training using the membrane potentials} Another approach to directly training SNNs using backprop is by replacing the undefined gradient of the non-differentiable spike function with a surrogate gradient \citep{esser2016convolutional, hunsberger2015spiking, zenke2018superspike, lee2016training}, which permits the flow of gradient through every membrane potential in time \citep{bellec2018long, shrestha2018slayer, neftci2019surrogate}. This method has been shown to circumvent the dead neuron problem and permit the training of other neural parameters besides synaptic connectivity (such as membrane time constants) that have been shown to improve network performance \citep{perez2021neural}. However, these results have not been replicated in the single-spike setting (which we do). A shortcoming of this method is its slow training speed, as the network needs to sequentially be simulated at every point in time (which we overcome).

\section{Proposed training speedup algorithm}
\label{sec:our_model}
\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{figures/paper1_fig2_v3.pdf}
	\centering
	\caption{Illustration of our model. \textbf{a.} The computational graph of our model for  time steps. Input spikes  induce currents , which charge the membrane potential without reset . These no-reset membrane potentials are mapped to erroneous output spikes , which are then transformed to a latent representation  encoding an ordering of spikes and finally mapped to the correct output spikes  (same coloured edges denote output from same source). \textbf{b.} Example activity of our model throughout the different stacks of processing.}
	\label{fig:model}
\end{figure}

We propose a new model for training SNNs in which individual neurons spike at most once. Our solution overcomes the slow training speeds of prior training algorithms by eschewing all sequential dependence and recasting the standard single-spike model to exclusively rely on parallelisable non-sequential operations. This recast model provides the same transformation from input to output spikes as the standard single-spike model, but is substantially faster to train. Our model is comprised of three main steps which are readily implementable in modern auto differentiation frameworks \citep{abadi2016tensorflow, paszke2017automatic, jax2018github}. For illustration purposes, we provide a diagram of the model's computational graph (Figure \ref{fig:model}a) and an example of how input spikes are transformed throughout the model's different layers of processing (Figure \ref{fig:model}b).
\paragraph{1. Convert presynaptic spikes to input current} As in the standard model, we map the time series of presynaptic spikes  \footnote{Bold face variables denotes arrays as opposed to scalar values.} to a time series of input currents , which is achieved using a tensor multiplication.

\paragraph{2. Calculate membrane potentials without reset} In contrast to the standard model, we calculate modified membrane potentials  from the input current  by excluding the reset mechanism. By dropping the reset term  in Equation \ref{eq:disc_lif} and unrolling this altered equation (see Appendix), we obtain a convolutional form allowing us to calculate these no-reset membrane potentials  without any sequential operations (where ).

\paragraph{3. Map no-reset membrane potentials to output spikes} We map the time series of no-reset membrane potentials  to output spikes  (which contains at most one spike). We obtain a time series of erroneous output spikes  by passing no-reset membrane potentials  through the spike function  (Equation \ref{eq:f})

Due to the removal of the spike reset mechanism, only the first spike occurrence in  follows the dynamics set out by the LIF model (Equation \ref{eq:disc_lif}) and thus all spikes succeeding the first spike occurrence are removed (compliant with the single spike assumption). We achieve this by constructing correct output spikes  with  for  except  for the smallest  satisfying  (if such  exists). A straightforward solution would be to iterate over all elements in  and set all spikes succeeding the first to zero, but such sequential calculation is the very problem we set out to remediate. We propose a vectorised solution to this problem which is comprised of two steps: 

\begin{enumerate}
	\item Map the erroneous output spikes  to a latent representation , where every element therein encodes an ordering of the spikes. This is achieved by passing the erroneous output spikes  through proposed function  (Proposition \ref{prop1}), which maps all elements besides the first spike occurrence to a value other than one ( for all  except for the smallest  satisfying  if such  exists). \item Obtain the correct output spikes  by passing the latent representation  through function , which uses the encoded spike ordering to produce the correct outputs spikes  by mapping every value besides one to zero. \footnote{We still permit gradients to flow through the points where .}
 
\end{enumerate}

\begin{proposition}
\label{prop1}
Function  acting on  contains at most one element equal to one  for the smallest  satisfying  (if such  exists).
\begin{proof}
Firstly, if  for all  then  for all  (follows from substitution). Secondly, if  for smallest  then  (follows from substitution) and there can exist no  such that  as

Thus  for all  as .
\end{proof}
\end{proposition}

\section{Experiments and results}

We investigate our model's speedup advantages and performance on real datasets in comparison to prior work. All models were implemented using PyTorch \citep{paszke2017automatic} with benchmarks and training conducted on a cluster of NVIDIA Tesla A100 GPUs.

\subsection{Speedup benchmarks}
\label{sec:speedup}

\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{figures/figure3.pdf}
	\centering
	\caption{Training speedup of our model over the standard model. \textbf{a.} Total training speedup as a function of the number of hidden neurons  and simulation steps  (left), alongside the corresponding forward and backward pass speedups (right). \textbf{b.} Training durations of both models for fixed hidden neurons  and variable batch size . \textbf{c.} Training speedup over different number of layers for fixed time steps  and batch size . \textbf{d.} Training speedup over large number of hidden neurons  for fixed time steps  and variable batch size . \textbf{e.} Forward pass speedup for fixed time steps  and variable batch size . \textbf{f.} Forward vs the backward pass speedup of our model for fixed time steps  and variable batch size . \textbf{b-f} use a  sample average with the mean and s.d. plotted.}
	\label{fig:speedup}
\end{figure}

We evaluate the speedup advantages of our model over the standard single-spike model trained using surrogate gradients, by simulating the forward and backward passes for different numbers of hidden units, layers, simulation steps and batch sizes on a synthetic spike dataset (see Appendix).

\paragraph{Robust speedup for different numbers of hidden units and simulation steps} We observe a considerable training speedup across a range of hidden units and simulation steps in a single layer (Figure \ref{fig:speedup}a). We obtain an optimal speedup of  for  units and  time steps, where our model takes ms compared to the ms it takes the standard model to complete a training pass (Figure \ref{fig:speedup}b). Our model still obtains a reasonable speedup of  for largest benchmarked  units and  time steps (albeit the forward pass speedup being slower).\footnote{This is due to the convolutional algorithm chosen by cudnn \citep{chetlur2014cudnn}.} These speedups are even more pronounced when the membrane time constants are fixed (obtaining a maximum speedup of ) or when using smaller batch sizes (with batch sizes  and  obtaining a maximum speedup of  and , respectively; See Appendix).

\paragraph{Applicability to deeper networks} We find our model to obtain substantial training speedups when using multiple layers (Figure \ref{fig:speedup}c) and layers containing thousands of neurons (Figure \ref{fig:speedup}d). The training speedups remain similar across an increasing numbers of layers for different number of hidden units (Figure \ref{fig:speedup}c). Furthermore, we obtain a speedup of  when using a large number of neurons (ranging between  to  neurons) in a single layer (Figure \ref{fig:speedup}d). Interestingly, these speedups remain approximately the same across the different number of neurons, even when the batch size is changed.\footnote{Again, this is due to the convolutional algorithm chosen by cudnn.}
	
\paragraph{Speedup advantages and room for improvement} Previous attempts at accelerating SNN training either speed up the backward pass \citep{perez2021sparse} or remove it completely \citep{bellec2020solution}. These methods however still sequentially compute the forward pass, which our model is able to accelerate (Figure \ref{fig:speedup}e). Furthermore, we observe the backward pass to slow down relative to the forward pass for increasing time steps (Figure \ref{fig:speedup}f). Further training speedup may therefore be achieved using sparse gradient descent, as auto differentiation frameworks are not optimised for the sparse nature of SNNs \citep{perez2021sparse}.

\subsection{Performance on real datasets} 
We investigate the applicability of our model to classify real data from different domains and of varying complexity (Table \ref{table:results}). These include the Yin-Yang dataset \citep{kriener2022yin} in which the goal is to classify spatial coordinates belonging to different groups, and the MNIST \citep{lecun1998mnist} and Fashion-MNIST (F-MNIST) \citep{xiao2017fashion} image datasets, where the objective is to classify images of handwritten digits and fashion items. All these analog datasets were converted into a spike representation using the time-to-first-spike encoding (see Appendix). We also test performance on two neuromorphic datasets, being the vision N-MNIST \citep{orchard2015converting} and the more difficult auditory SHD dataset \citep{cramer2020heidelberg}. The N-MNIST dataset is the MNIST dataset mapped onto a spike code using a neuromorphic vision sensor and the SHD dataset comprises spoken digit waveforms converted into spikes using a model of auditory bushy neurons in the cochlear nucleus.

\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{figures/figure4}
	\centering
	\caption{Analysis of our models performance on real datasets. \textbf{a.} Difference in accuracy between the standard multi-spike and our model. \textbf{b.} Training speedup of our model vs the standard single-spike model. \textbf{c.} Reduction in spikes of our single-spike model vs the standard multi-spike model (\textbf{a-c} use a  sample average with the mean and s.d. plotted). \textbf{d.} Training robustness of our model to solve different datasets when starting with zero network activity, which is fatal to other single-spike training methods. Top panel: Normalised training loss over time. Bottom panel: Normalised network activity over time, where the red cross denotes the absence of any spikes.}
	\label{fig:datasets}
\end{figure}

\begin{table}[h]
\caption{Performance comparison to existing literature (* denotes self-implementation,  denotes data augmentation and  denotes trainable time constants).}
\label{table:results}
\begin{center}
\resizebox{\textwidth}{!}{
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cccccccccc}
\hline
\multicolumn{1}{c}{\bf Dataset}  &\multicolumn{1}{c}{\bf Model} &\multicolumn{1}{c}{\bf Spike code} &\multicolumn{1}{c}{\bf Architecture} &\multicolumn{1}{c}{\bf Neuron model} &\multicolumn{1}{c}{\bf Accuracy (\%)} &\multicolumn{1}{c}{\bf Epoch time (s)}\\
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Yin-Yang}}
	& \cite{goltz2021fast} &single &120-10 &LIF (alpha-PSP) & &- \\
	& \textbf{our model} &single &120-10 &LIF & &\\
	& \cite{neftci2019surrogate}* &single &120-10 &LIF & &\\
\hline

\multicolumn{1}{c}{\multirow{2}{*}{MNIST}}
	& \cite{zhang2021rectified} &single &800-10 &IF (ReL-PSP) & &-\\
	& \cite{comsa2020temporal} &single &340-10 &IF (alpha-PSP) & &-\\
	& \textbf{our model} &single &1000-10 &LIF & &\\
	& \cite{neftci2019surrogate}* &single &1000-10 &LIF & &\\
	\cline{2-7}
	& \cite{zhang2021rectified} &single &16C5-P2-32C5-P2-800-128-10 &IF (ReL-PSP) & &-\\
	& \cite{mirsadeghi2021spike} &single &40C5-P2-1000-10 &IF (PL-PSP) & &-\\
    & \textbf{our model} &single &32C5-P2-64C5-P2-1000-10 &LIF & &\\
    & \cite{neftci2019surrogate}* &single &32C5-P2-64C5-P2-1000-10 &LIF & &\\
    \hline
   
\multicolumn{1}{c}{\multirow{2}{*}{FMNIST}}
	& \cite{zhang2021rectified} &single &1000-10 &IF (ReL-PSP) & &-\\
	& \cite{kheradpisheh2020temporal}\tablefootnote{Results reported by \cite{kheradpisheh2022bs4nn}.}  &single &1000-10 &IF & &-\\
	& \textbf{our model} &single &1000-10 &LIF & &\\
	& \cite{neftci2019surrogate}* &single &1000-10 &LIF & &\\
	\cline{2-7}
	& \cite{zhang2021rectified} &single &16C5-P2-32C5-P2-800-128-10 &IF (ReL-PSP) &&-\\
	& \cite{mirsadeghi2021spike} &single &20C5-P2-40C5-P2-1000-10 &IF (PL-PSP) &&-\\
    & \textbf{our model} &single &32C5-P2-64C5-P2-1000-10 &LIF & &\\
    & \cite{neftci2019surrogate}* &single &32C5-P2-64C5-P2-1000-10 &LIF & &\\
    \hline
    
\multicolumn{1}{c}{\multirow{2}{*}{N-MNIST}}
&\multicolumn{1}{c}{\multirow{1}{*}{\textbf{our model}}} &single &300-10 &LIF & &\\
    &&&&LIF & &\\
	& \cite{neftci2019surrogate}* &single &300-10 &LIF & &\\
\hline

\multicolumn{1}{c}{\multirow{2}{*}{SHD}}
& \cite{cramer2020heidelberg} &multi &128-20 &LIF &&-\\
& \cite{neftci2019surrogate}* &multi &300-20 &LIF &&\\
& \cite{cramer2020heidelberg} &multi &128-20 &recurrent LIF &&-\\
& \cite{perez2021neural} &multi &128-20 &recurrent LIF &&-\\
\cline{2-7}
&\multicolumn{1}{c}{\multirow{1}{*}{\textbf{our model}}} &single &300-20 &LIF & &\\
    &&&&LIF & & \\
& \cite{neftci2019surrogate}* &single &300-20 &LIF & &\\
\hline
\end{tabular}}
\end{center}
\end{table}

\paragraph{Obtaining competitive results across different image and neuromorphic datasets} The results of our model across all datasets are comparable or superior to prior reported results using single-spike SNNs. We reach an accuracy of ,  and  using a single hidden layer network on the Yin-Yang, MNIST and F-MNIST datasets respectively, where best performing prior work reported an accuracy of ,  and  respectively. Furthermore, our single-spike model nearly obtains the same accuracies to those obtained in the standard multi-spike SNN on these datasets (Yin-Yang and MNIST  difference; F-MNIST  difference; Figure \ref{fig:datasets}a).

\paragraph{Single-spike neurons solve challenging temporal problems using neural heterogeneity} It has been noted that single-spike SNNs are well suited for static datasets (such as spike encoded images) and less suited for processing temporally complex stimuli (such as audio or video) due to the single-spike constraint \citep{zenke2021visualizing, eshraghian2021training}. Prior single-spike SNN training techniques have attempted to optimise network connectivity without learning other neural parameters, such as membrane time-constants, which have shown to improve performance in multi-spike SNNs \citep{perez2021neural}. We explored the effect of learning the membrane time-constants in our single-spike model. We obtained an accuracy of  using a network trained with fixed time constants on the temporally-complex auditory SHD dataset. However, by including learnable time constants we were able to obtain a much higher accuracy of , which is similar to the performance obtained by a standard SNN with trainable time constants  or recurrent connections .

\paragraph{Drastic speedup in training} We obtain over a four-fold training speedup across all datasets, with a maximum speedup of  on the Yin-Yang dataset (Figure \ref{fig:datasets}b). Differences in speedups are due to the different temporal lengths and input dimensions of the datasets, as well the different network architectures employed (see section \ref{sec:speedup}).

\paragraph{Increased spike sparsity} Our single-spike SNN is able to solve various datasets with a large reduction in spikes compared to a standard multi-spike SNN (Figure \ref{fig:datasets}c), with over a  and up to a  reduction in spikes. This corroborates the value of obtaining more energy-efficient computations using single- rather than multi-spike neuromorphic systems \citep{liang20211, oh2021spiking, zhou2021temporal}, as energy consumption scales approximately proportional to the number of emitted spikes \citep{panda2020toward}.

\paragraph{Training deeper convolutional architectures} We evaluate our model in deeper convolutional architectures, which to date remains largely unexplored in single-spike SNNs \citep{mirsadeghi2022ds4nn}. We trained a multi-layer convolutional network on the MNIST and F-MNIST datasets, obtaining accuracies (MNIST:  and F-MNIST: ) similar to best performing prior work (MNIST:  and F-MNIST: ), whilst being faster to train in comparison to the control (MNIST-speedup   and F-MNIST-speedup ).

\paragraph{Robust learning and bypassing the dead neuron problem} A limitation of current single-spike SNN training methods is the dead neuron problem, referring to the hinderance in learning when neurons do not spike, as the learning signal is dependent on the occurrence thereof \citep{eshraghian2021training}. Our model is able to overcome this problem as we use surrogate gradients for training, in which the learning signal is instead passed through the membrane potentials. We experimentally verified this by showing how networks instantiated with zero starting activity (fatal to other single-spike training methods) still manage to solve different datasets (Figure \ref{fig:datasets}d).

\section{Discussion}
SNNs emulated on neuromorphic hardware are a promising avenue towards addressing the energy and scaling constraints of ANNs \citep{wunderlich2019demonstrating}. Single-spike SNNs further amplify these energy improvements through extreme spike sparsity, as energy consumption scales approximately proportionally to the number of emitted spikes \citep{panda2020toward}. To date, SNN training remains challenging due to the non-differentiable nature of the spike function, prohibiting the direct use of the backprop training algorithm which underpins the success of ANNs. Various extensions of backprop for SNNs have been proposed, but fall short in particular aspects. Gradients can be passed through the timing of spikes \citep{bohte2002error, mostafa2017supervised, kheradpisheh2020temporal}, yet this method suffers from the dead neuron problem, requires careful regularisation or imposes computationally-expensive modelling constraints. Alternatively, gradients can be passed through the membrane potentials using surrogate gradients \citep{shrestha2018slayer, neftci2019surrogate}, and although this method improves upon the problems of passing gradients through the spike times, it is painfully slow.

In this work, we address these problems by proposing a new general model (e.g. neurons can be IF or LIF) for training single-spike SNNs, without imposing any modelling (e.g. requiring PSP kernels) or training constraints (e.g. requiring careful regularisation) and support training of neural parameters other than synaptic connectivity (e.g. membrane time constants). We mathematically show how training can be sped up by replacing the slow sequential operations with faster convolutional ones. We experimentally validate this speedup across various numbers of units, time steps, layers and batch sizes, obtaining up to a  speedup. We show that our model can be trained across different network architectures (e.g. feedforward, hierarchical and convolutional) and obtain competitive results on different image and neuromorphic datasets. Our results compare well against multi-spike SNNs ( accuracy difference on all datasets) and obtain up to an  reduction in spike counts. Furthermore, our method circumvents the dead neuron problem and, for the first time, we show how single-spike SNNs can solve temporally-complex datasets on a par with multi-spike SNNs by including trainable membrane time constants. Our findings therefore challenge the dogma that single-spike SNNs are only suited to non-temporal problems \citep{eshraghian2021training, zenke2021visualizing}.

We obtain training speedups on all datasets, however, find that the backward pass slows down relative to the forward pass for longer timespans. Future work could mitigate this bottleneck and accelerate training using sparse gradient descent, which has shown to accelerate the backward pass in standard SNNs by taking advantage of spike sparsity \citep{perez2021sparse}. Currently, our single-spike model performs slightly worse compared to its multi-spike counterpart, where better performance could be achieved by extending our model to the multi-spike setting and permitting recurrent connectivity. Although we obtain impressive network performance without synaptic dynamics, future investigations could also benchmark the effect of including different PSP kernels on network performance. Finally, it remains an open question how the inclusion of trainable membrane time constants in our single-spike network obtains performance on a par with an equivalent multi-spike network on challenging temporal datasets such as the SHD dataset. Heterogenous time constants permit neurons to integrate information over various time scales \citep{perez2021neural}, but how such dynamics coupled with extreme spike sparsity solve challenging temporal tasks requires more rigorous theoretical analysis.

\section{Reproducibility statement}
The theoretical construction and derivations of our model are outlined in section \ref{sec:our_model} and we provide accompanying derivations in the Appendix. All code is publicly available at \href{https://github.com/webstorms/FastSNN}{https://github.com/webstorms/FastSNN} under the BSD 3-Clause Licence. This includes instructions on installation, data processing and running experiments to reproduce all results and figures portrayed in the paper. Training details are also provided in the Appendix.

\bibliography{fastsnn_single_spike}
\bibliographystyle{iclr2023_conference}

\appendix
\section{Appendix}

\subsection{Spiking neural network derivations}

\subsubsection{Normalising the leaky integrate and fire model}

\begin{proposition}
Any leaky integrate and fire (LIF) model  (with membrane potential , resting potential ,  firing threshold , resistance , input current  and membrane time constant ) can be normalised to a LIF model of the form  (such that , with firing threshold , resting potential  and a resistance equal to one).
\begin{proof}	

This mapping from any LIF model to the normalised LIF model is achieved using the following transformation (taken from \cite{hunsberger2018spiking}).



Rearranging this expression with respect to  and substituting this into  the LIF model we obtain



This new LIF form has a resting potential  and firing threshold  (obtained by substituting  and  in Equation \ref{eq:lif_norm_mapping} respectively). Thus, without loss of generality, any LIF model can be mapped to a normalised form using linear transformation Equation \ref{eq:lif_norm_mapping}.
\end{proof}
\end{proposition}

\subsubsection{Discretising the leaky integrate and fire model}

\begin{proposition}
The normalised continuous time leaky integrate and fire model  (with membrane potential , input current  and membrane time constant ) can numerically be approximated by discrete time difference equation , where  (for simulation time resolution ).
\begin{proof}
We proceed using the forward Euler method. Let  be constant with respect to time, for which the ordinary differential equation becomes separable.



For initial solution  at time  we derive . Then for constant  and initial solution  we obtain solution. 



To obtain the discretised update equation, we define simulation update time step , decay factor , assign continuous time points to discretised time steps  and  and assume the input current to be approximately constant and equal to  between discretised update steps  to .



\end{proof}
\end{proposition}

\subsubsection{Unrolling the leaky integrate and fire model without the reset term}

\begin{proposition}
Equation  is equivalent to difference equation  for .
\begin{proof}	
We proceed to proof equivalence by induction. For  we obtain



Hence the relation holds true for the base case . Assume the relation holds true for , then for  we derive



This implies equivalence for  if  holds true. By the principle of induction, equivalence is established given that both the base case and inductive step hold true.

\end{proof}
\end{proposition}

\subsection{Additional dataset details}
\subsubsection{Synthetic spike dataset for the speed benchmarks}

We generated binary input spike tensors of shape  ( being the batch size,  the number of input neurons and  the number of simulation steps). For every batch dimension  a firing rate  was uniformly sampled (with Hz and Hz), from which a random binary spike matrix of shape  was constructed, such that every input neuron in this matrix had an expected firing rate of Hz.

\subsubsection{Time-to-first-spike encoding}

We encoded all analog non-spiking input data into a spike raster using the time-to-first-spike coding method \citep{kheradpisheh2020temporal}. Here, every scalar value  within an input tensor is converted into a spike train with a single spike, where the time of spike  is determined by the following equation



\subsection{Training details and hyperparameters}

\subsubsection{Readout neurons}
The output layer  of every trained network contained the same number of neurons as the number of classes contained within the dataset being trained on. As suggested by \cite{zenke2021remarkable}, every neuron had a firing threshold set to infinity (\textit{i.e.} the spiking and reset mechanism was removed) from which the output  of readout neuron  to input sample  was either taken to be the maximum membrane potential over time  or the summated membrane potential over time  (see table \ref{table:hyperparams}).

\subsubsection{Beta clipping}
As the beta  (a transformation of the membrane time constant) of every neuron was optimised, we had to enforce correct neuron dynamics by clipping the values into the range . Note that   implies no memory i.e. a binary neuron,  implies decaying memory i.e. a LIF neuron and  implies full memory i.e. an IF neuron.


\subsubsection{Weight initialisation}

The network weights in a layer were sampled from a uniform distribution , except for the Yin-Yang dataset for which the weights were sampled from . For the feedforward layers  was set to the number of afferent connections to the layer and for the convolutional layers  for kernel shape . The bias terms were initialised to  in all networks. All neurons in the hidden layers were initialised with a membrane time constant ms and ms for the readout neurons.

\subsubsection{Supervised training loss}

All networks were trained to minimise a cross-entropy loss 



with  and  being the number of batch samples and dataset classes respectively, and  and  being the one hot target vector and network prediction probabilities respectively. The prediction probabilities  were obtained by passing the readout neuron outputs  through the softmax function.



\subsubsection{Surrogate gradient}
The backprop algorithm requires all nodes within the computational graph of optimisation to be differentiable. This requirements is however violated in a SNN due to the non-differentiable heavy stepwise spike function . To permit the use of backprop, we replaced the undefined derivate  of the function  with a surrogate gradient  \citep{zenke2018superspike}, which has been shown to work well in practice \citep{zenke2021remarkable}. Here hyperparameter  (which we set to  in all experiments) defines the slope of the gradient.



\subsubsection{Training procedure}

All models were trained using the Adam optimiser (with default parameters) \citep{kingma2014adam}. Training started with an initial learning rate, which was decayed by a factor of  every time the number of epochs reached a new milestone, after which the best performing model (that achieved lowest training loss) was loaded and training continued.

\subsubsection{Training hyperparameters}

\begin{table}[h]\label{table:hyperparameters}
\caption{Dataset and corresponding training parameters.}
\label{table:hyperparams}
\begin{center}
\resizebox{\textwidth}{!}{
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cccccccccc}
\hline
&\multicolumn{1}{c}{\bf Yin-Yang} &\multicolumn{1}{c}{\bf MNIST} &\multicolumn{1}{c}{\bf conv MNIST} &\multicolumn{1}{c}{\bf F-MNIST} &\multicolumn{1}{c}{\bf conv F-MNIST} &\multicolumn{1}{c}{\bf N-MNIST} &\multicolumn{1}{c}{\bf SHD}\\
\hline
\multicolumn{1}{c}{\multirow{1}{*}{Dataset (train/test)}} & & & & & && \\
\multicolumn{1}{c}{\multirow{1}{*}{Input neurons}} & & & & & & & \\
\multicolumn{1}{c}{\multirow{1}{*}{Dataset classes}} & & & & & & & \\
\multicolumn{1}{c}{\multirow{1}{*}{Epochs}} & & & & & & & \\
\multicolumn{1}{c}{\multirow{1}{*}{Learning rate}} & & & & & & & \\
\multicolumn{1}{c}{\multirow{1}{*}{Batch size }} & & & & & & & \\
\multicolumn{1}{c}{\multirow{1}{*}{Simulation steps }} & & & & & & & \\
\multicolumn{1}{c}{\multirow{1}{*}{Time resolution } (ms)} & & & & & & & \\
\multicolumn{1}{c}{\multirow{1}{*}{Milestones}} & & & & & & & \\
\multicolumn{1}{c}{\multirow{1}{*}{Output function}} &sum &sum &max &sum &sum &sum &sum \\
\hline
\end{tabular}}
\end{center}
\end{table}

\subsection{Extended speedup results}
\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{figures/sup_figure2}
	\centering
	\caption{Total training speedup using smaller batch sizes as a function of the number of hidden neurons  and simulation steps  (left), alongside the corresponding forward and backward pass speedups (right). \textbf{a.} Speedups using batch size . \textbf{b.} Speedups using batch size .}
	\label{fig:speedup_batch}
\end{figure}
\begin{figure}[h!]
    \includegraphics[width=1\linewidth]{figures/sup_figure1}
	\centering
	\caption{Training speedup of our model over the standard model (using fixed membrane time constants). \textbf{a.} Total training speedup as a function of the number of hidden neurons  and simulation steps  (left), alongside the corresponding forward and backward pass speedups (right). \textbf{b.} Training durations of both models for fixed hidden neurons  and variable batch size . \textbf{c.} Training speedup over different number of layers for fixed time steps  and batch size . \textbf{d.} Training speedup over large number of hidden neurons  for fixed time steps  and variable batch size . \textbf{e.} Forward pass speedup for fixed time steps  and variable batch size . \textbf{f.} Forward vs the backward pass speedup of our model for fixed time steps  and variable batch size . \textbf{b-f} use a  sample average with the mean and s.d. plotted.}
	\label{fig:speedup_fixed}
\end{figure}

\end{document}
