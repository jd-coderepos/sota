\RequirePackage{fix-cm}

\documentclass[twocolumn]{svjour3}          \smartqed  \usepackage{graphicx}

\journalname{IJCV}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{booktabs} \usepackage{multirow}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xspace}
\usepackage{framed}
\usepackage{color, colortbl}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{xcolor} 
\usepackage[colorlinks]{hyperref}
\usepackage[misc]{ifsym}
\usepackage{multicol}

\newcommand{\cmark}{}
\newcommand{\xmarkg}{}

\def \ours {CAE*\xspace} 
\def \oursdvae {CAE\xspace} 
\definecolor{cornflowerblue}{RGB}{100, 149, 237}

\newcommand*{\rowstyle}[1]{
  \gdef\@rowstyle{#1}\@rowstyle\ignorespaces }
\newcolumntype{=}{
  >{\gdef\@rowstyle{}}}
\newcolumntype{+}{
  >{\@rowstyle}}
\makeatother


\begin{document}

\title{Context Autoencoder for Self-Supervised Representation Learning}


\author{Xiaokang Chen \and Mingyu Ding \and Xiaodi Wang \and Ying Xin \and Shentong Mo \and Yunhao Wang \and Shumin Han \and Ping Luo \and \\ Gang Zeng \and Jingdong Wang}

\authorrunning{Xiaokang Chen et al.}

\institute{Peking University \\  University of Hong Kong \\ UC Berkeley \\ Baidu \\ \Letter { wangjingdong@outlook.com}}


\date{Received: date / Accepted: date}



\maketitle

\begin{abstract}
We present a novel masked image modeling
(MIM) approach,
context autoencoder (CAE),
for self-supervised representation pretraining.
We pretrain an encoder
by making predictions
in the encoded representation space.
The pretraining tasks include
two tasks: masked representation prediction - predict the representations
for the masked patches,
and masked patch reconstruction - reconstruct the masked patches.
The network is an encoder-regressor-decoder architecture:
the encoder takes the visible patches as input;
the regressor predicts the representations of the masked patches,
which are expected to be aligned
with the representations
computed from the encoder,
using the representations of visible patches and the positions of visible and masked patches;
the decoder reconstructs the masked patches
from the predicted encoded representations.
The CAE design encourages 
the separation of learning the encoder (representation) 
from completing the pertaining tasks: masked representation prediction and masked patch reconstruction tasks,
and making predictions in the encoded representation space
empirically shows
the benefit
to representation learning.
We demonstrate the effectiveness of our CAE
through 
superior transfer 
performance in downstream tasks:
semantic segmentation, object detection and instance segmentation, and classification.
The code will be available at~\url{https://github.com/Atten4Vis/CAE}.
\end{abstract}

\keywords{Self-Supervised Representation Learning, Masked Image Modeling, Context Autoencoder}

\section{Introduction}\label{sec1}






We study the masked image modeling (MIM) task 
for self-supervised representation learning.
It aims to learn an encoder
through masking some patches of the
input image and 
making predictions for the masked patches
from the visible patches.
It is expected that
the resulting encoder 
pretrained through solving the MIM task 
is able to extract the patch representations
taking on semantics
that are transferred to solving downstream tasks.

\begin{figure}[t]
\centering
\centerline{\includegraphics[width=0.98\columnwidth]{CAEv18.pdf}}
\caption{The pipeline of context autoencoder.
Our approach 
(a) feeds visible patches into
the encoder
and extracts their representations 
and then (b) completes the pretext tasks:
predict
the representations 
of the masked patches
from the visible patches
in the encoded representation space
through 
latent contextual regressor and prediction alignment,
and reconstruct the masked patches from the predicted 
representations 
of masked patches.
The pretrained encoder in (a) 
is applied to downstream tasks
by simply replacing the pretext task part
(b)
with the downstream task part.
 means stop gradient.
}
\label{fig:CAE}
\end{figure}

The typical MIM methods,
such as 
BEiT~\cite{bao2021beit},
the method studied in the ViT paper~\cite{DosovitskiyB0WZ21},
and iBoT~\cite{zhou2021ibot},
use a single ViT architecture 
to solve the pretraining task i.e., reconstructing the patch tokens or the pixel colors.
These methods mix the two tasks: learning the encoder (representation)
and reconstructing the masked patch.
The subsequent method,
masked autoencoder (MAE)~\cite{he2021masked}
adopts an encoder-decoder architecture,
partially decoupling the two tasks.
As a result,
the representation quality is limited.
Most previous methods, except iBoT~\cite{zhou2021ibot}, lack an explicit modeling
between encoded representations 
of visible patches and masked patches.



We present a context autoencoder (CAE) approach,
illustrated in Figure~\ref{fig:CAE},
for improving the encoding quality.
We pretrain the encoder
through making predictions for the masked patches
in the encoded representation space.
The pretraining task
is a combination of masked representation prediction
and masked patch reconstruction.
the pretraining network
is an encoder-regressor-decoder architecture.
The encoder
takes only the visible patches as input
and learns the representations
only for the visible patches.
The regressor
predicts the masked patch representations,
which is expected to
be aligned with the representations
of the masked patches computed from the encoder,
from the visible patch representations.
The decoder reconstructs 
the masked patches
from the predicted masked patch representations
without receiving the representations
of the visible patches.




The prediction in the encoded representation space
from the visible patches
to the masked patches
generates a plausible semantic guess
for the masked patches,
which lies in 
the same semantic space
for the visible patches.
We assume that 
the prediction is easier
if the encoded representations take higher semantics
and 
that the accurate prediction encourages
that the encoded representations
take on a larger extent of semantics,
empirically validated
by the experiments.


The CAE design also encourages 
the separation
of learning the encoder
and completing the pretraining tasks:
the responsibility
of representation learning is 
mainly
taken by the encoder
and the encoder is only for representation learning.
The reasons include: the encoder in the top stream in Figure~\ref{fig:CAE}
operates only on visible patches,
only focusing on learning semantic representations;
the regression is done
on the encoded representation space,
as a mapping between the representations
of the visible patches 
and the masked patches;
the decoder operates
only on the predicted representations
of the masked patches.



We present the empirical performance of our approach
on downstream tasks, semantic segmentation, object detection and instance segmentation, and classification.
The results show that our approach outperforms supervised pretraining, contrastive self-supervised pretraining, and other MIM methods.


\section{Related Work}
Self-supervised representation learning
has been widely studied 
in computer vision
, including:
context prediction~\cite{CarlDoersch2015UnsupervisedVR,tian2021semantic},
clustering-based methods~\cite{xie2016unsupervised,yang2016joint,caron2018deep,asano2019self,zhuang2019local,huang2019unsupervised,caron2019unsupervised,PriyaGoyal2021SelfsupervisedPO},
contrastive self-supervised learning~\cite{li2020prototypical,AaronvandenOord2018RepresentationLW,henaff2020data,wang2022repre},
instance discrimination~\cite{dosovitskiy2014discriminative,dosovitskiy2015discriminative},
image discretization~\cite{gidaris2020learning,gidaris2020online},
masked image modeling~\cite{li2021mst,fang2022corrupted,tian2022beyond},
and information maximization~\cite{ermolov2021whitening,zbontar2021barlow,bardes2021vicreg}.
The following mainly reviews closely-related methods.


\vspace{1mm}
\noindent\textbf{Autoencoding.}
Traditionally, autoencoders were used for dimensionality reduction or feature learning~\cite{phdthesis_LeCun,gallinari1987memoires,hinton1994autoencoders,hinton2006reducing,ranzato2007efficient,vincent2008extracting,kingma2013auto}.
The denoising autoencoder (DAE) is an autoencoder that receives a corrupted data point as input and is trained to estimate the original, uncorrupted data point as its output.
The variants or modifications
of DAE were adopted
for self-supervised representation learning,
e.g., corruption by masking pixels~\cite{VincentLLBM10,pathak2016context,chen2020generative},
removing color channels~\cite{zhang2016colorful},
shuffling image patches~\cite{noroozi2016unsupervised}, denoising pixel-level noise~\cite{atito2021sit}
and so on.

\vspace{1mm}
\noindent\textbf{Contrastive self-supervised learning.}
Contrastive self-supervised learning,
referring 
in this paper
to the self-supervised approaches
comparing random views
with contrastive loss
or simply MSE loss
that are related
as shown in~\cite{GarridoCBNL22},
has been popular
for self-supervised representation learning~\cite{ChenK0H20,He0WXG20,YonglongTian2020WhatMF,ChenXH21,grill2020bootstrap,CaronTMJMBJ21,chen2021exploring,caron2020unsupervised_swav,wu2018unsupervised,XiangyuPeng2022CraftingBC}.
The basic idea is to 
maximize the similarity
between the views augmented
from the same image
and optionally minimize
the similarity
between the views
augmented from different images.
Random cropping
is an important augmentation scheme,
and thus
typical contrastive self-supervised learning methods 
(e.g., MoCo v3) tend to learn knowledge
mainly from the central regions of the original images.
Some dense variants~\cite{wang2021dense,xie2021propagate} 
eliminate the tendency 
in a limited degree
by
considering an extra contrastive loss
with dense patches.

\vspace{1mm}
\noindent\textbf{Masked image modeling.}
Motivated by BERT for masked language modeling~\cite{DevlinCLT19}, 
the method studied in~\cite{DosovitskiyB0WZ21} and BEiT~\cite{bao2021beit} use the ViT structure
to solve the masked image modeling task,
e.g., estimate the pixels
or the discrete tokens.
The follow-up work, iBOT~\cite{zhou2021ibot},
combines the MIM method (BEiT) and 
a contrastive self-supervised approach (DINO~\cite{CaronTMJMBJ21}).
But they do not have explicitly an encoder 
for representation learning 
or a decoder for pretraining task completion,
and the ViT structure is essentially a mixture of encoder and decoder,
limiting the representation learning quality.

 
Several subsequent MIM methods
are developed to improve the encoder quality, 
such as designing pretraining architectures:
Masked Autoencoder (MAE)~\cite{he2021masked},
SplitMask~\cite{el2021large},
and Simple MIM (SimMIM)~\cite{xie2021simmim};
adopting new reconstruction targets: 
Masked Feature Prediction (MaskFeat)~\cite{wei2021masked},
Perceptual Codebook for BEiT (PeCo)~\cite{dong2021peco},
and
data2vec~\cite{BaevskiHXBGA22}.
The technical report~\footnote{\url{https://arxiv.org/abs/2202.03026}} of our approach
was initially published as an arXiv paper~\cite{CAE2022},
and was concurrent to data2vec~\cite{BaevskiHXBGA22}, MAE~\cite{he2021masked},
and other methods, such as~\cite{el2021large,xie2021simmim}.
After that, MIM methods have developed rapidly,
e.g., extended to frequency/semantic domain~\cite{xie2022masked,liu2022devil,wei2022mvp,li2022mc},  
combined with contrastive self-superivsed learning~\cite{tao2022siamese,jing2022masked,yi2022masked,huang2022contrastive}, 
efficient pretraining~\cite{zhang2022hivit,huang2022green,chen2022efficient}, 
mask strategy design~\cite{kakogeorgiou2022hide,li2022semmae,li2022uniform},
scalability of MIM~\cite{xie2022data},
and interpretation of MIM~\cite{xie2022revealing,li2022architecture,kong2022understanding}.



The core idea of our approach 
is making predictions in the encoded representation space.
We jointly solve two pretraining tasks:
masked representation prediction - predict the representations for the masked patches, 
where the representations lie in the representation space output from the encoder,
and masked patch reconstruction - reconstruct the masked patches.

Our approach is clearly different from MAE~\cite{he2021masked} (Figure~\ref{fig:MAEiBoT} (top)).
Our approach introduces an extra pretraining task, masked representation prediction,
and encourages the separation 
of two roles: learning the encoder and completing pretraining tasks;
in contrast,
MAE partially mixes the two roles,
and has no explicit prediction of masked patch representations.

On the other hand,
our approach differs from data2vec~\cite{BaevskiHXBGA22}
and iBoT~\cite{zhou2021ibot} (Figure~\ref{fig:MAEiBoT} (bottom)).
Similar to BEiT,
in data2vec and iBoT,
there is no explicit module separation  
of learning the encoder and estimating the mask patch representations,
and 
the target representations are formed from the full view (as the teacher)
with 
the same network as
the student network for processing the masked view and predicting the masked patch representations (except a centering process in iBoT for the teacher following DINO).
In contrast, our approach is simple:
form the target representations 
merely from the output of the encoder,
and the encoder-regressor design is straightforward
and explainable:
the regressor predicts the representations of masked patches to match the representations 
computed directly from the encoder.

\begin{figure}[t]
\footnotesize`
\centering
\centerline{\includegraphics[width=0.98\columnwidth]{MAE-arch.pdf}}\\
\centerline{\includegraphics[width=0.98\columnwidth]{iBoT-MIMarch.pdf}}
\caption{
The pipeline of MAE (top), and the MIM part of iBoT (bottom).
The centering module is not depicted in the bottom stream. The pretrained encoder in (a) 
is applied to downstream tasks
by simply replacing the pretext task part
(b)
with the downstream task part.  means stop gradient.
}
\label{fig:MAEiBoT}
\end{figure}



\section{Approach}
\label{sec:cae}

\subsection{Architecture}
Our context autoencoder (CAE)
is a masked image modeling approach.
The network shown in Figure~\ref{fig:CAE}
is an encoder-regressor-decoder architecture.
The key is to
make predictions from visible patches
to masked patches
in the encoded representation space.
The pretraining tasks include:
masked representation prediction
and masked patch reconstruction. 

We randomly split an image
into two sets of patches:
visible patches 
and masked patches .
The encoder takes the visible patches as input;
the regressor predicts the representations of the masked patches,
which are expected to be aligned
with the representations
computed from the encoder,
from the representations of the visible patches conditioned on the positions of masked patches;
the decoder reconstructs the masked patches
from the predicted encoded representations.



\vspace{1mm}
\noindent\textbf{Encoder.}
The encoder  maps the visible patches

to the latent representations\
.
It only handles the visible patches.
We use the ViT to form our encoder.
It first embeds the visible patches 
by linear projection
as patch embeddings,
and adds the positional embeddings .
Then it sends the combined embeddings
into a sequence of transformer blocks
that are based on {self-attention}, 
generating .

\vspace{1mm}
\noindent\textbf{Regressor.}
The latent contextual regressor 
predicts the latent representations

for the masked patches
from the latent representations

of the visible patches output from the encoder
conditioned on the positions of
the masked patches.
We form the latent contextual regressor 
using a series of transformer blocks
that are based on {cross-attention}.

The initial queries ,
called mask queries,
are mask tokens
that are learned as model parameters
and are the same for all the masked patches. 
The keys and the values are the same before linear projection and
consist of 
the visible patch representations 
and the output of the previous cross-attention layer
(mask queries for the first cross-attention layer).
The corresponding positional embeddings
of the masked patches
are considered
when computing the cross-attention weights
between the queries and the keys.
In this process,
the latent representations  of the visible patches 
are not updated.




\vspace{1mm}
\noindent\textbf{Decoder.}
The decoder  maps the latent representations 
of the masked patches
to some forms of masked patches,
.
The decoder, similar to the encoder,
is a stack of transformer blocks
that are based on {self-attention},
followed by a linear layer predicting the targets.
The decoder only receives
the latent representations
of the masked patches
(the output of the latent contextual regressor),
and the positional embeddings of the masked patches
as input
without directly using the information of
the visible patches.
 



\subsection{Objective Function}

\noindent\textbf{Masking.}
Following BEiT~\cite{bao2021beit}, we adopt the random block-wise masking strategy 
(illustrated in Figure~\ref{fig:maskingandcropping}) to split the input image into two sets of patches,
visible and masked patches. For each image,  of  () patches are masked.

\vspace{1mm}
\noindent\textbf{Targets.}
The targets 
for the representations
of the masked patches
are formed as follows.
We feed the masked patches 
into the encoder,
which is the same as the one for encoding visible patches,
and generate the representations 
of the masked patches
as the representation targets.

The targets 
for the patch reconstruction 
are formed by 
the discrete tokenizer,
e.g.,
the tokenizer trained with d-VAE
on ImageNet-K without using the labels
or the DALL-E tokenizer
(trained with d-VAE on M images)~\cite{RameshPGGVRCS21}
used in BEiT~\cite{bao2021beit}.
The input image is fed into the tokenizer,
assigning a discrete token to each patch
for forming the reconstruction targets .

\vspace{1mm}
\noindent\textbf{Loss function.}
The loss function 
consists of
a reconstruction loss:
,
and an alignment loss:
,
corresponding to masked patch reconstruction and masked representation prediction, respectively.
The whole loss is a weighted sum:

We use the MSE loss for 
and the cross-entropy loss for . 
 stands for stop gradient.
 is  in our experiments.




\begin{figure}[t]
\centering
\includegraphics[width=0.48\linewidth]{0_crop.pdf}~
\includegraphics[width=0.48\linewidth]{1_crop.pdf}
\caption{ Illustration
of random block-wise sampling (st and rd images)
and random cropping (nd and th images). The colored regions are masked regions. The boxes correspond to cropped regions.
Random block-wise sampling is used in our approach.
Random cropping
is a key data-augmentation scheme
for contrastive self-supervised pretraining. 
}
\label{fig:maskingandcropping}
\end{figure}



\begin{figure*}
\centering
\includegraphics[width=.105\linewidth]{68_origin.png}
\includegraphics[width=.105\linewidth]{68_recons.png}
\includegraphics[width=.105\linewidth]{68_noise.png}~
\includegraphics[width=.105\linewidth]{origin_1.png}
\includegraphics[width=.105\linewidth]{recons_1.png}
\includegraphics[width=.105\linewidth]{noise_1.png}~
\includegraphics[width=.105\linewidth]{901_origin.png}
\includegraphics[width=.105\linewidth]{901_recons.png}
\includegraphics[width=.105\linewidth]{901_noise.png}
\caption{
Illustrating that
predictions are made
in the representation space.
We reconstruct the image 
by feeding the full image 
(st, th, and th)  into the pretrained CAE encoder
and then the pretrained CAE decoder 
outputting the reconstructed image (nd, th, and th).
It can be seen that
the image can be constructed
with the semantics kept
when skipping latent contextual regressor,
verifying the input and the predicted representations lie in the same space.
We also show the reconstructed images
(rd, th, and th)
from the encoder and the decoder
pretrained without the alignment constraint.
We can see that those images are meaningless,
indicating that the alignment constraint
is critical for ensuring 
that predictions are made
in the representation space.
}
\label{fig:nonmaskedimagereconstruction}
\end{figure*}


\section{Discussions}
\label{sec:discussionAnalysis}


\subsection{Analysis}
\noindent\textbf{Predictions
are made in the encoded representation space.}
Our CAE 
attempts to make predictions in the encoded representation space:
predict
the representations
for the masked patches
from the encoded representations of the visible patches.
In other words,
it is expected that
the output representations
of the latent contextual regressor
also lie in the encoded representation space,
which is ensured
by prediction alignment.
{This encourages the learned representation to take on a large extent of semantics for prediction
from visible patches
to masked patches, benefiting the representation learning of the encoder.}




We empirically verify
that the predicted representations
lie in the encoded representation space
through image reconstruction.
We train the CAE
using the pixel colors as the prediction targets,
for two cases:
with and without the alignment,
i.e., masked representation prediction.
For reconstruction, 
we feed all the patches (without masking,
all the image patches are visible)
of an image (from the ImageNet validation set)
into the pretrained encoder,
then skip the latent contextual regressor
and directly send all the encoded patch representations
to the pretrained decoder 
for reconstructing the whole image.


Figure~\ref{fig:nonmaskedimagereconstruction}
provides reconstruction results
for several examples randomly sampled 
from the ImageNet-K validation set.
One can see that
our approach can successfully reconstruct the images,
implying that the input and  output representations of latent contextual regressor
are in the same space. 
In contrast,
without the alignment,
the reconstructed images are noisy,
indicating 
the input and output representations of latent contextual regressor
are in different spaces.
The results suggest that
the explicit prediction alignment
is critical for
ensuring that predictions are made 
in the encoded representation space. 

\vspace{1mm}
\noindent\textbf{Representation alignment in CAE and contrastive self-supervised learning.}
Representation alignment is also used in contrastive self-supervised learning methods,
such as MoCo, BYOL, SimCLR,
and methods mixing contrastive self-supervised learning and masked image modeling,
such as iBOT, and MST.
The alignment loss could be the MSE loss or 
the contrastive loss that CAE may also take advantage of.

In the CAE, the alignment is imposed
over the representations
 - 
predicted from the representations 
of visible patches through the regressor ,
and the representations 
 
- computed from the encoder .
Both \emph{ and  are about the masked patches,
and lie in the representation space
output from the encoder}.

Differently,
the alignment in the  most 
contrastive self-supervised learning methods
is imposed 
over the representations ,
where  is a projector,
and some views may be processed
with the EMA version of the encoder
and the projector.
The  representations to be aligned
are about \emph{different views} 
(in iBoT and MST,
the views are masked views and full views),
and are not directly output from the encoder.
It is not quite clear how the projector works,
and it is reported in~\cite{MIMPart2022}
that the projector is a part-to-whole process
mapping the object part representation
to the whole object representation
for contrastive self-supervised learning.




\subsection{Connection}
\noindent\textbf{Relation to autoencoder.}
The original autoencoder~\cite{phdthesis_LeCun,gallinari1987memoires,hinton1994autoencoders} consists of
an encoder and a decoder.
The encoder maps the input into a latent representation,
and the decoder reconstructs the input
from the latent representation.
The denoising autoencoder (DAE)~\cite{VincentLLBM10},
a variant of autoencoder,
corrupts the input by adding noises
and still reconstructs
the non-corrupted input.

Our CAE encoder is similar
to the original autoencoder
and also contains an encoder and a decoder.
Different from the autoencoder
where the encoder and the decoder
process the whole image,
our encoder takes a portion of patches as input
and our decoder takes
the estimated latent representations
of the other portion of patches
as input.
Importantly,
the CAE makes predictions
in the latent space
from the visible patches
to the masked patches.


\definecolor{cornflowerblue}{RGB}{100, 149, 237}
\begin{figure*}[t]
\vskip 0.1in
\centering
\footnotesize
\subfigure[]{\fbox{\includegraphics[scale=0.55]{CAE-CGv2.pdf}}}~~~
\subfigure[]{\fbox{\includegraphics[scale=0.55]{BEiT-CGv2.pdf}}}~~~
\subfigure[]{\fbox{\includegraphics[scale=0.55]{AE-CGv2.pdf}}}~~~
\subfigure[]{\fbox{\includegraphics[scale=0.55]{MAE-CGv2.pdf}}}
\caption{The computational graphs
for (a) a context autoencoder (CAE),
(b) BEiT~\cite{bao2021beit},
(c) a denoising autoencoder (DAE),
and (d) MAE~\cite{he2021masked}
and the one stream in SplitMask~\cite{el2021large}.
The parts in {\color{cornflowerblue}cornflower blue}
are for loss function.
(a) 
The encoder 
receives visible patches

and outputs their latent representations 
.
The latent contextual regressor

predicts the latent representations 

for masked patches 
from .
The decoder predicts the targets 
for masked patches
from .
 and 
are the loss functions.
During training, the gradient is stopped for .
See the detail in Section~\ref{sec:cae}.
(b) 
The input includes both visible patches

and mask queries  representing masked patches,
and the representations for them are updated within the function .
(c) The function  is a noising function
generating the noisy version 
from the input .
 and  are the normal encoder and decoder, respectively.
(d) The two functions,  
and ,
are both based on self-attention.
 
(called encoder in MAE)
only processes the visible patches ,
and 
(called decoder in MAE)
processes 
both the latent representations 
of the visible patches
and the mask queries ()
and updates them simultaneously.
For simplicity,
the positional embeddings 
are not included in computational graphs.
\emph{
(a) CAE and (c) DAE 
perform
the encoding and  MIM task completion roles explicitly
and separately,
(b) BEiT and (d) MAE perform the encoding and MIM task completion roles implicitly
and simultaneously.}
}
\label{fig:ComputationGraph}
\end{figure*}


\begin{figure*}[t]
\centering
\footnotesize
\includegraphics[width=.1\linewidth]{253_origin.png}~
\includegraphics[width=.1\linewidth]{273_origin.png}~
\includegraphics[width=.1\linewidth]{322_origin.png}~
\includegraphics[width=.1\linewidth]{315_origin.png}~
\includegraphics[width=.1\linewidth]{137_origin.png}~
\includegraphics[width=.1\linewidth]{22_origin.png}~
\includegraphics[width=.1\linewidth]{161_origin.png}~
\includegraphics[width=.1\linewidth]{24_origin.png}~
\includegraphics[width=.1\linewidth]{92_origin.png}\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{moco_253_scope.png}~
\includegraphics[width=.1\linewidth]{moco_273_scope.png}~
\includegraphics[width=.1\linewidth]{moco_322_scope.png}~
\includegraphics[width=.1\linewidth]{moco_315_scope.png}~
\includegraphics[width=.1\linewidth]{moco_137_scope.png}~
\includegraphics[width=.1\linewidth]{moco_22_scope.png}~
\includegraphics[width=.1\linewidth]{moco_161_scope.png}~
\includegraphics[width=.1\linewidth]{moco_24_scope.png}~
\includegraphics[width=.1\linewidth]{moco_92_scope.png}\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{mae_253_scope.png}~
\includegraphics[width=.1\linewidth]{mae_273_scope.png}~
\includegraphics[width=.1\linewidth]{mae_322_scope.png}~
\includegraphics[width=.1\linewidth]{mae_315_scope.png}~
\includegraphics[width=.1\linewidth]{mae_137_scope.png}~
\includegraphics[width=.1\linewidth]{mae_22_scope.png}~
\includegraphics[width=.1\linewidth]{mae_161_scope.png}~
\includegraphics[width=.1\linewidth]{mae_24_scope.png}~
\includegraphics[width=.1\linewidth]{mae_92_scope.png}\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{253_scope.png}~
\includegraphics[width=.1\linewidth]{273_scope.png}~
\includegraphics[width=.1\linewidth]{322_scope.png}~
\includegraphics[width=.1\linewidth]{315_scope.png}~
\includegraphics[width=.1\linewidth]{137_scope.png}~
\includegraphics[width=.1\linewidth]{22_scope.png}~
\includegraphics[width=.1\linewidth]{161_scope.png}~
\includegraphics[width=.1\linewidth]{24_scope.png}~
\includegraphics[width=.1\linewidth]{92_scope.png}\\
\caption{Illustrating
the attention map
averaged over  attention heads
between the class token
and the patch tokens 
in the last layer
of the ViT encoder
pretrained on ImageNet-K.
The region inside the blue contour is obtained by thresholding the attention weights to keep  of the mass.
The four rows are: (1) input image,
(2) MoCo v3,
a typical contrastive self-supervised learning method,
(3) MAE, and (4) our CAE.
One can see that 
MoCo v3 tends to focus 
mainly on the centering regions 
and little on other patches,
and our CAE tends to consider
almost all the patches.
}
\label{fig:patchimportance}
\vspace{-0.4cm}
\end{figure*}





\vspace{1mm}
\noindent\textbf{Relation to BEiT, iBoT and MAE.}
The CAE encoder processes
the visible patches,
to extract their representations,
without making predictions for masked patches.
Masked representation prediction 
is made through the regressor and the prediction alignment,
ensuring that the output of the regressor
lies in the representation space same with 
the encoder output.
The decoder only processes
the predicted representations
of masked patches.
Our approach encourages that 
the encoder takes the responsibility
of 
and is only for 
representation learning.

In contrast,
BEiT~\cite{bao2021beit}
and the MIM part of iBOT do not separate 
the representation extraction role
and the task completion role
and uses a single network,
with both the visible and masked patches
as the input,
simultaneously for the two roles.
In MAE~\cite{he2021masked}, the so-called decoder
may play a partial role
for representation learning
as the representations of the visible patches
are also updated in the MAE decoder. 
Unlike CAE, MAE, iBoT, BEiT
do not explicitly 
predict the representations
of masked patches from
the representations of visible patches
(that lie in the encoded representation space)
for masked patches.

When the pretrained encoder is applied to downstream tasks,
one often replaces the pretext task completion part
using the downstream task layer,
e.g., segmentation layer or detection layer.
The separation of representation learning (encoding)
and pretext task completion
helps that downstream task applications
take good advantage of representation pretraining.



We provide 
the computational graph 
for CAE, BEiT~\cite{bao2021beit}, denoising autoencoder, Masked Autoencoder~\cite{he2021masked}
and SplitMask~\cite{el2021large}
(one stream)
in Figure~\ref{fig:ComputationGraph}.
Compared to our CAE,
the main issue of MAE is that the so-called decoder 
might have also the encoding role,
i.e.,
learning semantic representations of
the visible patches.





 \vspace{1mm}
\noindent\textbf{Comparison to contrastive self-supervised learning.}
Typical contrastive self-supervised learning methods,
e.g., SimCLR~\cite{ChenK0H20}
and MoCo~\cite{He0WXG20,ChenXH21},
pretrain the networks
by solving the pretext task,
maximizing the similarities between augmented views
(e.g., random crops)
from the same image
and minimizing the similarities
between augmented views from different images.


It is shown in~\cite{ChenK0H20} that
random cropping
plays an important role in view augmentation 
for contrastive self-supervised learning.
Through analyzing random crops (illustrated in Figure~\ref{fig:maskingandcropping}),
we observe that
the center pixels 
in the original image space
have large chances to belong to random crops.
We suspect that
the global representation,
learned by contrastive self-supervised learning
for a random crop 
possibly
with other augmentation schemes,
tends to 
focus mainly on the center pixels
in the original image,
so that the representations
of different crops 
from the same image
can be possibly similar.
Figure~\ref{fig:patchimportance}
(the second row)
shows that 
the center region of the original image
for the typical contrastive self-supervised learning approach, MoCo v3,
is highly attended.
The part in random crops corresponding
to the center of the original image
is still attended
as shown in Figure~\ref{fig:patchimportance_crop}.



In contrast,
our CAE method
(and other MIM methods)
randomly samples the patches
from the augmented views
to form the visible and masked patches.
All the patches are possible
to be randomly masked
for the augmented views and accordingly the original image.
Thus, the CAE encoder needs to learn good representations
for all the patches,
to make good predictions
for the masked patches
from the visible patches.
Figure~\ref{fig:patchimportance} (the third row)
illustrates that almost all the patches in the original images
are considered in our CAE encoder.


Considering that
the instances of the  categories
in ImageNet-K
locate mainly around the center of the original images~\cite{russakovsky2015imagenet},
typical contrastive self-supervised learning methods, e.g., MoCo v3,
learn the knowledge mainly about the  categories,
which is similar to supervised pretraining.
But our CAE and other MIM methods
are able to  
learn more knowledge beyond
the  categories
from the non-center image regions.
This indicates that
the CAE has the potential to perform better
for downstream tasks.




\begin{figure}[t]
\centering

\setlength{\fboxsep}{1pt}
\setlength{\fboxrule}{0.4pt}
\fbox{\includegraphics[width=0.45\columnwidth]{tsne_cae_final.png}}~~~~
\fbox{\includegraphics[width=0.45\columnwidth]{tsne_random_final.png}}
\caption{t-SNE visualization
(one color for one category)
of representations extracted
from the images in ADEK. Left: ViT pretrained with our CAE; Right: ViT with random weights.
}
\label{fig:representationclusters}
\end{figure}

\subsection{Interpretation}
\noindent\textbf{Intuitive Interpretation for CAE.}
Humans are able to
hallucinate 
what appears in the masked regions
and how they appear
according to the visible regions.
We speculate that
humans do this
possibly 
in a way similar as
the following example:
given that only the region of the dog's head
is visible and the remaining parts are missing,
one can (a) recognize the visible region 
to be about a dog,
(b) predict the regions 
where the other parts of the dog appear,
and (c) guess what the other parts look like.

Our CAE encoder is in some sense
like the human recognition step (a).
It understands the content
by
mapping the visual patches
into latent representations
that lie in the subspace that corresponds to the category dog\footnote{Our encoder does not know that the subspace is about a dog, and just separates it from the subspaces of other categories.}.
The latent contextual regressor
is like step (b).
It produces a plausible hypothesis
for the masked patches,
and describes
the regions corresponding to the other parts of the dog
using latent representations.
The CAE decoder is like step (c),
mapping the latent representations
to the targets.
It should be noted that
the latent representations 
might contain other information
besides the semantic information,
e.g., the part information
and the information for making predictions.

We adopt t-SNE~\cite{van2008visualizing}
to visualize the high-dimensional patch representations 
output from our CAE encoder 
on ADEK~\cite{zhou2017scene} in Figure~\ref{fig:representationclusters}.
ADEK has a total of  categories. 
For each patch in the image, 
we set its label to be the category
that more than half of the pixels
belong to.
We collect up to  patches for each category
from sampled  images. As shown in the figure, the latent representations of CAE are clustered to some degree for different categories (though not perfect as our CAE is pretrained on ImageNet-1K).
Similar observations could be found for other MIM methods.




\begin{figure*}[t]
\centering
\footnotesize
\includegraphics[width=.1\linewidth]{cae_crop_253_origin.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_273_origin.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_322_origin.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_315_origin.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_137_origin.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_22_origin.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_161_origin.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_24_origin.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_92_origin.png}~~
\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{moco_crop_253_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop_273_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop_322_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop_315_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop_137_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop_22_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop_161_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop_24_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop_92_scope.png}~~
\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{mae_253_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_273_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_322_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_315_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_137_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_22_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_161_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_24_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_92_scope.png}~~
\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{cae_crop_253_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_273_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_322_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_315_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_137_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_22_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_161_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_24_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop_92_scope.png}~~
\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{moco_crop1_253_origin.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_273_origin.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_322_origin.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_315_origin.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_137_origin.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_22_origin.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_161_origin.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_24_origin.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_92_origin.png}~~
\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{moco_crop1_253_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_273_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_322_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_315_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_137_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_22_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_161_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_24_scope.png}~~
\includegraphics[width=.1\linewidth]{moco_crop1_92_scope.png}~~
\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{mae_crop1_253_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_crop1_273_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_crop1_322_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_crop1_315_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_crop1_137_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_crop1_22_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_crop1_161_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_crop1_24_scope.png}~~
\includegraphics[width=.1\linewidth]{mae_crop1_92_scope.png}~~
\\
\vspace{1mm}
\includegraphics[width=.1\linewidth]{cae_crop1_253_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop1_273_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop1_322_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop1_315_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop1_137_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop1_22_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop1_161_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop1_24_scope.png}~~
\includegraphics[width=.1\linewidth]{cae_crop1_92_scope.png}~~
\\
\caption{The attention maps over two sets of randomly cropped images (the st the th rows)
for MoCo v3 (the nd the th rows), MAE (the rd the th rows), and our CAE (the th the th rows)
pretrained on ImageNet-K.
The contrastive self-supervised learning method, MoCo v3, tends to focus 
mainly on the object region 
and little on other regions.
In contrast, MIM-based models, CAE and MAE, tend to consider
almost all the patches.
The attention maps over the original images
are shown in Figure~\ref{fig:patchimportance}.}
\label{fig:patchimportance_crop}
\end{figure*}



\vspace{1mm}
\noindent\textbf{Probabilistic interpretation for CAE.}
The MIM problem can be formulated 
in the probabilistic form, maximizing
the probability of the predictions  
of the masked patches
given the conditions, the visible patches ,
the positions  of the visible patches,
and the positions  of the masked patches:
.
It can be solved by 
introducing latent representations 
and ,
with the assumption 
that 
and  
(
and ) are conditionally independent
(the probabilistic graphical model is 
given in Figure~\ref{fig:CAEPGM}):

Here,
the equation from (2) to (3)
is obtained from the probabilistic graphical model
of CAE shown in Figure~\ref{fig:CAEPGM},
and the removal of the condition 
(from 
to ),
and the condition 
(from 
to )
from (3) to (4)
is based on the conditional independence assumption.
The three terms in (4)
correspond to three parts of our CAE:
the encoder,
the latent contextual regressor,
and the decoder, respectively.

\begin{figure}
\centering
\footnotesize
\includegraphics[scale=0.8]{CAE-PGM}
\caption{The probabilistic graphical model of CAE.
The other conditions
of ,
,
and ,
the positions 
and  
of the visible and masked patches,
are not plotted for simplicity.}
\label{fig:CAEPGM}
\end{figure}


Similarly, the latent representation alignment constraint
can be written as a conditional probability,
,
where  is the masked patch representations
computed from the encoder.


\vspace{1mm}
\noindent\textbf{Intuitive interpretation for the contrastive self-supervised learning.}
We consider the case in ImageNet-K that the object mainly lies in the center of an image\footnote{There are a few images
in which the object does not lie in the center in ImageNet-K.
The images are actually viewed as noises
and have little influence for contrastive self-supervised learning.
}.
There are  randomly sampled crops from an image,
and each crop  contains a part of the center object, .
To maximize
the similarity between
two crops  and ,
the pretraining 
might contain the processes:
select the regions 
and 
from the two crops  and ,
extract their features 
and ,
and predict the feature of the object, ,
from the part features

and .
In this way, the features of the crops from the same image
could be similar.
Among the  random crops, 
most crops contain a part of the object in the center,
and a few crops that do not contain a part of the center object
could be viewed as noises
when optimizing the contrastive loss.

After pretrained on ImageNet-K
(where the object mainly lies in the center)
the encoder is able to learn the knowledge of the  classes
and localize the region containing the object belonging
to the  classes.
It is not necessary that
the object lies in the center for the testing image,
which is verified in Figure~\ref{fig:patchimportance_crop}.
This further verifies that
MoCo v3 (contrastive self-supervised pretraining)
pretrained on ImageNet-K
tends to attend to the object region,
corresponding to the center region of the original image as shown in Figure~\ref{fig:patchimportance}.







\section{Experiments}
\subsection{Implementation}
We study the standard ViT small, base and large architectures,
ViT-S ( transformer blocks with dimension ), ViT-B ( transformer blocks with dimension ) and ViT-L ( transformer blocks with dimension ). 
The latent contextual regressor
consists of  transformer blocks based on cross-attention
in which self-attention over masked tokens
and encoded visible patch representations
is a choice but with slightly higher computation cost and a little lower performance,
and the decoder consists of  transformer blocks based on self-attention,
and an extra linear projection
for making predictions.




\subsection{Training Details}
\label{sec:trainingdetails}

\noindent\textbf{Pretraining.}
The pretraining settings are almost the same as~BEiT~\cite{bao2021beit}.
We train the CAE
on ImageNet-K.
We partition the image 
of 
into  patches
with the patch size being . 
We use standard random cropping and horizontal flipping 
for data augmentation.
We use AdamW~\cite{loshchilov2017adamw} for optimization
and train the CAE for // epochs 
with the batch size being .
We set the learning rate 
as e-
with cosine learning rate decay.
The weight decay is set as . The warmup epochs for // epochs pre-training are //, respectively.
We employ drop path~\cite{huang2016stochastic_depth} rate  and dropout rate .


\vspace{1mm}
\noindent\textbf{Linear probing.} 
We use the LARS~\cite{you2017large} optimizer with momentum . 
The model is trained for  epochs. The batch size is , the warmup epoch is  and the learning rate is .
Following ~\cite{he2021masked}, we adopt an extra BatchNorm layer~\cite{SergeyIoffe2015BatchNA} without affine transformation () before the linear classifier.
We do not use mixup~\cite{HongyiZhang2017mixupBE}, cutmix~\cite{SangdooYun2019CutMixRS}, drop path~\cite{huang2016stochastic_depth}, or color jittering, and we set weight decay as zero.


\vspace{1mm}
\noindent\textbf{Attentive probing.} 
The parameters of the encoder are fixed during attentive probing.
A cross-attention module, a BatchNorm layer (), and a linear classifier are appended after the encoder.
The extra class token representation in
cross-attention
is learned as model parameters. 
The keys and the values are the patch
representations
output from the encoder. 
There is no MLP or 
skip connection operation
in the extra cross-attention module.
We use the SGD optimizer with momentum  and train the model for  epochs. The batch size is , the warmup epoch is  and the learning rate is . 
Same as linear probing,
we do not use mixup~\cite{HongyiZhang2017mixupBE}, cutmix~\cite{SangdooYun2019CutMixRS}, drop path, or color jittering, and we set weight decay as zero.



\vspace{1mm}
\noindent\textbf{Fine-tuning on ImageNet.}
We follow the fine-tuning protocol in BEiT to use layer-wise learning rate decay, weight decay and AdamW. 
The batch size is , the warmup epoch is  and the weight decay is . For ViT-S, we train  epochs with learning rate e- and layer-wise decay rate . For ViT-B, we train  epochs with learning rate e- and layer-wise decay rate .
For ViT-L, we train  epochs with learning rate e- and layer-wise decay rate .


\vspace{1mm}
\noindent\textbf{Semantic segmentation on ADEK.}
We use AdamW
as the optimizer. The input resolution is . The batch size is .
For the ViT-B, the layer-wise decay rate is  and the drop path rate is . We search from four learning rates, e-, e-, e- and e-, for all the results in Table~\ref{tab:segmentation}.
For the ViT-L, the layer-wise decay rate is  and the drop path rate is . We search from three learning rates for all the methods, e-, e-, and e-,
We conduct fine-tuning for K steps. We do not use multi-scale testing.






\vspace{1mm}
\noindent\textbf{Object detection and instance segmentation on COCO.}
We utilize multi-scale training 
and resize the
image with the size
of the short side
between  and  
and the long side no larger than .  
The batch size is .
For the ViT-S, the learning rate is e-, the layer-wise decay rate is , and the drop path rate is . 
For the ViT-B, the learning rate is e-, the layer-wise decay rate is , and the drop path rate is . 
For the ViT-L, the learning rate is e-, the layer-wise decay rate is , and the drop path rate is .
We train the network 
with the  schedule:
 epochs with the learning rate decayed
by  at epochs  and .
We do not use multi-scale testing.
The Mask R-CNN implementation follows MMDetection~\cite{mmdetection}.





 \begin{figure}[ht]
\centering
\includegraphics[width=.18\columnwidth]{2_origin.png}~
\includegraphics[width=.18\columnwidth]{845_origin.png}~
\includegraphics[width=.18\columnwidth]{199_origin.png}~
\includegraphics[width=.18\columnwidth]{249_origin.png}~
\includegraphics[width=.18\columnwidth]{280_origin.png}\\
\vspace{1mm}
\includegraphics[width=.18\columnwidth]{2_attentive.png}~
\includegraphics[width=.18\columnwidth]{845_attentive.png}~
\includegraphics[width=.18\columnwidth]{199_attentive.png}~
\includegraphics[width=.18\columnwidth]{249_attentive.png}~
\includegraphics[width=.18\columnwidth]{280_attentive.png}\\
\caption{
Illustrating 
the cross-attention unit
in attentive probing.
The attention map
(bottom)
is the average 
of cross-attention maps
over  heads
between the extra class token and the patches.
One can see that the attended region lies mainly in the object,
which helps image classification.
}
\label{fig:attentiveprobing}
\end{figure}




\subsection{Pretraining Evaluation}
\noindent\textbf{Linear probing.}
Linear probing is widely used 
as a proxy of pretraining quality evaluation
for self-supervised representation learning.
It learns a linear classifier
over the image-level representation output from the pretrained encoder 
by using the labels of the images,
and then tests the performance
on the validation set.

\vspace{1mm}
\noindent\textbf{Attentive probing.}
The output of the encoder
pretrained with MIM methods
are representations
for all the patches.
It is not suitable
to linearly probe the representation,
averagely-pooled from patch representations,
because the image label in ImageNet-K
only corresponds to a portion of patches.
It is also not suitable
to use the default class token within the encoder
because the default class token serves as
a role of aggregating the patch representations
for better patch representation extraction
and is not merely for the portion of patches
corresponding to the image label.


To use the image-level label
as a proxy of
evaluating the pretraining quality
for the encoder pretrained with MIM methods,
we need to attend the patches 
that are related to the label. 
We introduce a simple modification
by using a cross-attention unit 
with an extra class token (that is different from the class token in the encoder)
as the query
and  
the output patch representations of the encoder as the keys and the values,
followed by a linear classifier.
The introduced cross-attention unit
is able to care mainly about the patches belonging
to the  classes in ImageNet-K
and remove the interference
of other patches.
Figure~\ref{fig:attentiveprobing} illustrates the effect
of the cross-attention unit,
showing that the extra cross-attention unit
can to some degree
attend the regions
that are related to the  ImageNet-K classes.


\begin{table}[t]
  \centering  
  \caption{Pretraining quality evaluation
  in terms of 
  fine-tuning (FT),
  linear probing (LIN),
  and attentive probing (ATT). 
   means the number of effective epochs in~\cite{zhou2021ibot} as they adopt multi-crop augmentation (equivalently take a larger number of epochs compared to one-crop augmentation).
  We report the top- accuracy
  (in the column ATT)
  of the supervised training approach 
  DeiT~\cite{touvron2020deit}
  to show how far the ATT score is from supervised training.
 The scores
  for other models 
  and our models 
  are based on our implementations
  if not specified.
    {
    Except that * denotes using the DALL-E tokenizer, CAE adopts
    the d-VAE tokenizer trained on ImageNet-1K only.
    }
  } 
  \setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
    \begin{tabular}{l c c c  c c c}
      \toprule
      Method  & \#Epochs & \#Crops & FT  & LIN & ATT \\
      \midrule
      \multicolumn{5}{l}{\emph{Methods using ViT-S}:}\\
      DeiT &  &-&-&-&   \\
      MoCo v3  &  &  &  &  &  \\
      BEiT   &    &  &  &  & \\
    \ours &  &  &  &  & \\
      \midrule
      \multicolumn{5}{l}{\emph{Methods using ViT-B}:}\\
      DeiT &  &-&-&-&    \\
      MoCo v3   &  &  &  &  & \\
      DINO  &   &  &  &  &  \\
      BEiT   &   &  &  &  & \\
      MAE &    &  &  &  &  \\
      MAE &    &  &  &  &  \\
      SimMIM &  &  &  &  & - \\
      iBOT &   &  &   &   &  \\
      \ours &  &  &  &  &  \\
     \ours &  &  &  &  &  \\
     \ours &  &  &  &  &  \\
     \oursdvae &  &  &  &  &  \\
      \midrule
      \multicolumn{5}{l}{\emph{Methods using ViT-L}:}\\
      MoCo v3 &  &  &  & - & - \\
      BEiT &  &  &  & - & - \\
      MAE &    &  &  &  &  \\
      \ours &    &  &  &  &  \\
      \oursdvae &    &  &  &  &  \\
      \bottomrule
  \end{tabular}
  \label{tab:pretrainingvaluation}
\end{table}



\vspace{1mm}
\noindent\textbf{Results.}
Table~\ref{tab:pretrainingvaluation}
shows the results 
with three schemes,
linear probing (LIN),
attentive probing (ATT),
and fine-tuning (FT)
for representative contrastive self-supervised pretraining
(MoCo v3 and DINO)
and MIM (BEiT and MAE) methods,
as well as our approach
with the targets
formed with the DALL-E tokenizer
(trained on M images)
and the d-VAE tokenizer
(trained on ImageNet-K without using the labels),
denoted as CAE* and CAE, respectively.
The models of MAE with  epochs
and BEiT
are pretrained by us
using the official implementations,
and other models are officially released models.

We highlight a few observations.
The fine-tuning performance
for these methods are very similar
and there is only a minor difference
similar to the observation~\cite{zhou2021ibot}.
We think that
the reason is that
self-supervised pretraining 
and fine-tuning are conducted
on the same dataset
and no extra knowledge
is introduced for image classification.
The minor difference might come from the optimization aspect:
different initialization
(provided by pretrained models)
for fine-tuning.







In terms of linear probing,
the scores of the contrastive self-supervised learning methods,
MoCo v3 and DINO, are higher than the MIM methods.
This is as expected because 
contrastive self-supervised learning focuses mainly on
learning the representations for  classes
(See discussion in Section~\ref{sec:discussionAnalysis}).
The pretraining is relatively easier 
than existing MIM methods
as contrastive self-supervised learning mainly cares about the  classes
and MIM methods may care about the classes
beyond the  classes.





For the MIM methods,
the scores of attentive probing
are much larger
than linear probing.
This validates our analysis:
the MIM methods extract the representations
for all the patches,
and the classification task
needs to attend the corresponding portion of patches. 


The LIN and ATT scores are similar
for contrastive self-supervised pretraining on ViT-B, e.g.,

for MoCo v3
and  for DINO.
This means that the extra cross-attention in attentive probing
does not make a big difference,
which is one more evidence for our analysis in Section~\ref{sec:discussionAnalysis} that
they already focus
mainly 
on the region where the instance in the  categories lies.












\subsection{Downstream Tasks}


\begin{table}[t]
    \centering
    \caption{Semantic segmentation on ADEK.
    All the results are based on the same
    implementation
    for semantic segmentation. \#Epochs refers to the number of pretraining epochs.
     means the number of effective epochs in~\cite{zhou2021ibot} as the method uses multi-crop pretraining augmentation (See Table~\ref{tab:pretrainingvaluation}).
    SplitMask~\cite{el2021large} is pretrained on ADE20K for 21000 epochs.
    { 
    : these results are from \cite{he2021masked}.
    }
    } 
    \setlength{\tabcolsep}{23pt}
\renewcommand{\arraystretch}{1.1}
        \begin{tabular}{l c c}
        \toprule
        Method  
        & \#Epochs  & mIoU  \\
         \midrule
      \multicolumn{3}{l}{\emph{Methods using ViT-B}:}\\
        SplitMask & -- &  \\
        BEiT &  &   \\
        BEiT &  &     \\ 
        mc-BEiT & 800 &  \\
        DeiT &  &   \\
        MoCo v3 &  &   \\
        DINO &  &   \\
        MAE &  & \\
        MAE &  &  \\
        Ge-AE &  &  \\
        AMIM &  &  \\
         iBOT &  &  \\
        \ours &  &   \\ 
        \ours &   &   \\
        \ours &  &   \\
        \oursdvae &  &   \\
    \midrule
      \multicolumn{3}{l}{\emph{Methods using ViT-L}:}\\
      MoCo v3 &   & \\
      BEiT &   & \\
      MAE &   & \\
      \ours &   &   \\
      \oursdvae &   &   \\
        \bottomrule     
    \end{tabular} 
    \label{tab:segmentation}
\end{table}


\begin{table*}[t]
    \centering
    \caption{Object detection and instance segmentation on COCO. 
    Mask R-CNN is adopted
    and trained with the  schedule.
    All the results are based on
    the same implementation
    for object detection and instance segmentation.
    \#Epochs refers to the number of pretraining epochs on ImageNet-K.
     means the number of effective epochs in~\cite{zhou2021ibot} (See Table~\ref{tab:pretrainingvaluation}).
    }
\setlength{\tabcolsep}{9.8pt}
\renewcommand{\arraystretch}{1}
    \small
\begin{tabular}{l c c c  c c c   c  c c}
        \toprule
        \multirow{2}{*}{Method} & 
        \multirow{2}{*}{\#Epochs} & 
        \multirow{2}{*}{Supervised} & 
        \multirow{2}{*}{Self-supervised} &
        \multicolumn{3}{c}{Object detection}& 
        \multicolumn{3}{c}{Instance segmentation}\\ 
        \cline{5-10}
          &  &  &   &{ } & {} & {} & 
        {} & 
        {} & 
        {}\\ 
        \midrule
      \multicolumn{9}{l}{\emph{Methods using ViT-S}:}\\
        DeiT  &  &\cmark & \xmarkg  &  &  &  &  &  &  \\
        MoCo v3  &  & \xmarkg & \cmark &   &  &  &  &  &  \\
        BEiT &  &\xmarkg & \cmark &  &  &  &  &  &  \\
        \ours  &  &    \xmarkg & \cmark &   &  &  &  &  &  \\
        \midrule
      \multicolumn{9}{l}{\emph{Methods using ViT-B}:}\\
        DeiT  &  &\cmark & \xmarkg  &  &  &  &  &  &  \\
        MoCo v3 &  & \xmarkg & \cmark &  &  &  &  &  &  \\
        DINO &   & \xmarkg & \cmark &   &  &  &  &  &  \\
        BEiT &   &\xmarkg & \cmark &  &  &  &  &  &  \\
        BEiT &    &\xmarkg & \cmark &  &  &  &  &  &  \\
        MAE &     & \xmarkg & \cmark &   &  &  &  &  &  \\
        MAE &     & \xmarkg & \cmark &   &  &  &  &  &  \\
        iBOT &     & \xmarkg & \cmark &   &  &  &  &  &  \\
        \ours &   & \xmarkg & \cmark &   &  &  &  &  &  \\
        \ours &   & \xmarkg & \cmark &   &  &  &  &  &  \\
        \ours &   & \xmarkg & \cmark &   &  &  &  &  &  \\
        \oursdvae &   & \xmarkg & \cmark &   &  &  &  &  &  \\
        \midrule
      \multicolumn{9}{l}{\emph{Methods using ViT-L}:}\\
        MAE &   & \xmarkg & \cmark &   &  &  &  &  &  \\
        \ours &   & \xmarkg & \cmark &   &  &  &  &  &  \\
        \oursdvae &   & \xmarkg & \cmark &   &  &  &  &  &  \\
        \bottomrule     
    \end{tabular} 
    \vspace{-0.2cm}
    \label{tab:cocodetection}
\end{table*}



 
\noindent\textbf{Semantic segmentation on ADEK}~\cite{zhou2017scene}\textbf{.}
We follow the implementation~\cite{bao2021beit}
to use UperNet~\cite{xiao2018unified}.
The CAE with the tokenizers learned over ImageNet-K
performs almost the same as the tokenizers learned over M images provided by DALL-E (CAE*),
implying that the tokenizer trained
on ImageNet-K (without using the labels) or a larger dataset
does not affect the pretraining quality and
accordingly the downstream task performance.

Table~\ref{tab:segmentation} shows that using the ViT-B, 
our CAE* with  training epochs
performs better
than DeiT,
MoCo v3, DINO,
MAE ( epochs)
and BEiT. 
Our CAE* ( epochs)
further improves the segmentation scores
and outperforms MAE ( epochs), MoCo v3 and
DeiT by ,  and , respectively. Using ViT-L, our CAE* ( epochs) outperforms BEiT ( epochs) and MAE ( epochs) by  and , respectively.

The superior results over supervised and contrastive self-supervised pretraining methods,
DeiT,
MoCo v3 and DINO,
stem from 
that our approach captures the knowledge beyond the  classes in ImageNet-K.
The superior results over BEiT
and MAE stems from that
our CAE makes predictions 
in the encoded representation space
and that representation learning
and pretext task completion
are separated. 


\begin{table*}[t]
  \centering
  \caption{Top-1 classification accuracy on the Food-, Clipart and Sketch datasets. The backbone is ViT-B.}
  \setlength{\tabcolsep}{21pt}
\renewcommand{\arraystretch}{1.1}
        \begin{tabular}{l  c  c  c  c c }
            \toprule
            {Method} & {Supervised} & {Self-supervised} & Food- & Clipart  & Sketch \\
              \hline
              Random Init. &   \xmarkg &  \xmarkg & 82.77 &  &   \\
              DeiT & \cmark &  \xmarkg &  &  &   \\
              DINO &  \xmarkg & \cmark &  &   &     \\
              MAE & \xmarkg & \cmark &  &   &     \\
             \ours & \xmarkg & \cmark &  &   &  \\
            \bottomrule
        \end{tabular} 
        \label{tab:more_classification}
\end{table*}


\begin{table*}[t]
\caption{Ablation studies
for the decoder and the alignment constraint 
in our CAE. 
All the models are pretrained on ImageNet-K with  epochs.
}
\label{tab:ablation}
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.1}
\centering
\begin{tabular}{ccccccccc}
\toprule
 Decoder & Alignment & LIN & ATT & FT  &  ADE Seg.  &  COCO Det. & \#Params & Training Time \\
\midrule
 \xmarkg & \xmarkg &  &  &  &  &   &  M &  \\
 \cmark & \xmarkg &  &  &  &  &   &  M &  \\
 \xmarkg & \cmark &  &  &  &  &   &  M &  \\
 \cmark & \cmark &  &  &  &  &   &  M & \\
\bottomrule
\end{tabular} 
\end{table*} 



\begin{table}[t]
    \centering
    \caption{
    The results of object detection and instance segmentation on COCO with the Cascaded Mask-RCNN framework ( schedule). 
    ViT-B is used for all experiments.
    All the detection results
    are from our implementation.
    } 
    \setlength{\tabcolsep}{13pt}
\renewcommand{\arraystretch}{1.1}
    \small
        \begin{tabular}{l c c c}
        \toprule
            \multirow{1}{*}{Method} & \multirow{1}{*}{\#Epochs} &
             &  \\
        \midrule
        MAE~\cite{he2021masked}  &  &  &  \\
        mc-BEiT~\cite{zhou2021ibot}   &  &   &  \\
        iBOT~\cite{zhou2021ibot}  &   &   &  \\ 
        \ours  &   &  &  \\
        \ours  &   &  &   \\
        \ours  &  &  &   \\
        \bottomrule     
    \end{tabular} 
    \label{tab:det_cascade}
\end{table}






 \vspace{1mm}
\noindent\textbf{Object detection and instance segmentation on COCO}~\cite{lin2014microsoft}\textbf{.}
We adopt the Mask R-CNN approach~\cite{he2017mask}
that produces bounding boxes and instance masks simultaneously,
with the ViT as the backbone.
The results are given in Table~\ref{tab:cocodetection}.
We report the box AP for object detection and the mask AP for
instance segmentation.
The observations are consistent with those for semantic segmentation
in Table~\ref{tab:segmentation}.
Our CAE* ( epochs, ViT-B)
is superior to
all the other models except 
that a little lower than
MAE ( epochs).
Our approach ( epochs)
outperforms MAE ( epochs),
MoCo v3 and
DeiT by ,  and , respectively. Using ViT-L, our CAE achieves  box AP and outperforms MAE by .

We also report the results of object detection and instance segmentation on COCO with the Cascaded Mask R-CNN framework~\cite{ZhaoweiCai2021CascadeRH} in Table~\ref{tab:det_cascade}. Results show that our CAE performs better than other methods.


In addition, we conduct experiments on the scaling ability of CAE on the detection task. The detection model is built upon ViT-Huge~\cite{DosovitskiyB0WZ21}, DINO~\cite{HaoZhang2023DINODW}, and Group DETR~\cite{QiangChen2022GroupDF}
(see~\cite{groupdetrv2} for more details). The ViT-Huge is pretrained on ImageNet-K~\cite{deng2009imagenet} using CAE. 
We are the first to obtain  mAP on COCO \textit{test-dev}, 
which outperforms previous methods with larger models and more training data (e.g., BEIT-3~\cite{WenhuiWang2023ImageAA} ( mAP) and SwinV2-G~\cite{ZeLiu2021SwinTV} ( mAP)).



\begin{table}[t]
  \centering
  \caption{The effect of mask ratios. The backbone is ViT-B. Models are trained for  epochs.} 
  \setlength{\tabcolsep}{15pt}
\renewcommand{\arraystretch}{1.1}
        \begin{tabular}{ c  c  c c }
            \toprule
            {Mask Ratio} & {LIN} & {ATT} & ADE Seg \\
              \hline
               &  & 		&  \\
               &  & 		&  \\
               &  & 		&  \\
            \bottomrule
        \end{tabular} 
        \label{tab:mask_ratio}
\end{table}




\vspace{1mm}
\noindent\textbf{Classification.} 
We conduct fine-tuning experiments on three datasets: Food-~\cite{bossard14}, Clipart~\cite{castrejon2016learning}, and Sketch~\cite{castrejon2016learning}. Results in Table~\ref{tab:more_classification} show that the proposed method outperforms the previous supervised method (DeiT) and self-supervised methods (DINO, MAE).






\subsection{Ablation Studies}



\noindent \textbf{Decoder and alignment.}
The CAE architecture contains
several components for pretraining the encoder:
regressor
and alignment for masked representation prediction,
decoder with a linear layer for masked
patch reconstruction.
We observe that
if the pretraining task,
masked patch reconstruction, is not included,
the training collapses, leading to a trivial solution. 
We thus study
the effect of 
the decoder
(when the decoder is removed, we
use a linear layer to predict the targets),
which is helpful for target reconstruction,
and the alignment,
which is helpful for representation prediction.  



Table~\ref{tab:ablation} shows the ablation results.
We report the scores for 
linear probing,
attentive probing, fine-tuning
and downstream tasks: semantic segmentation on the ADEK dataset
and object detection on COCO
with the DALL-E tokenizer as the target.
One can see that 
the downstream task performance is almost the same
when only the decoder is added
and that the performance
increases
when the decoder and the alignment are both added.
This also verifies that
the alignment is important
for ensuring that
the predicted representations 
of masked patches
lie in the encoded representation space
and thus the predictions are made in the encoded representation space,
and accordingly improving the representation quality. 
Without the decoder, 
the performance drops. 
This is because the reconstruction from the semantic representation
to the low-level targets
cannot be done through a single linear layer,
and no decoder will deteriorate the semantic quality of the encoder.
The additional computational cost, i.e. the number of parameters and training time, brought by the decoder and alignment is relatively small, e.g., increasing the number of parameters to  and training time to .

\vspace{1mm}
\noindent \textbf{Mask ratio.}
We also conduct experiments with different mask ratios including , , and . Results are listed in Table~\ref{tab:mask_ratio}. We find that ratio  gets better results than ratio . Adopting a higher mask ratio () could further improve the performance of linear probing and attentive probing, while the semantic segmentation performance is reduced by \%. We choose  in our work unless specified.



\begin{table}[t]
  \centering
  \caption{The effect of reconstruction targets on the performance of CAE. The backbone is ViT-B. Models are trained for  epochs.} 
  \setlength{\tabcolsep}{11pt}
\renewcommand{\arraystretch}{1.1}
        \begin{tabular}{ c  c  c c }
            \toprule
            {Targets} & {LIN} & {ATT} & ADE Seg \\
              \hline
              DALL-E tokenizer &  & 		&  \\
              d-VAE tokenizer &  & 		&  \\
              RGB pixel value &  & 		&  \\
            \bottomrule
        \end{tabular} 
        \label{tab:abla_target}
\end{table}



\vspace{1mm}
\noindent\textbf{\#layers in the regressor and decoder.} 
For the number of layers in the latent contextual regressor and decoder, we tried four choices: -layer, -layers, -layer, and -layer. The results for linear probing are , , , and . The results for attentive probing are , , , and .
We empirically observed that -layer outperforms other choices overall.

\vspace{1mm}
\noindent\textbf{Loss tradeoff parameter.}
There is a tradeoff variable 
in the loss function given in Equation~\ref{eqn:lossfunction}.
We did not do an extensive study
and only tried three choices, 
,  and .
The linear probing results are ,  and , respectively.
The choice  works also well, slightly worse than  that is adopted in our experiment.


\vspace{1mm}
\noindent\textbf{Reconstruction targets.}
To study the impact of different pretraining targets on model performance, we conduct additional experiments on the RGB pixel value target. Comparing the results with DALL-E tokenizer and d-VAE tokenizer trained on ImageNet-1K, the model shows better linear probe and segmentation results but inferior in attentive probe, as shown in Table~\ref{tab:abla_target}. Pretraining with these three targets obtains similar performance, illustrating that CAE does not rely on specific pretraining targets.


\section{Conclusion}
The core design
of our CAE architecture 
for masked image modeling
is that predictions are made
from visible patches
to masked patches
in the encoded representation space.
We adopt two pretraining tasks:
masked representation prediction
and masked patch reconstruction.
Experiments demonstrate
the effectiveness of the CAE design.
In addition,
we also point out that
the advantage of MIM methods
over typical contrastive self-supervised pretraining
and supervised pretraining on ImageNet-K
is that MIM learns the representations
for all the patches,
while 
typical contrastive self-supervised pretraining (e.g., MoCo and SimCLR)
and supervised pretraining
tend to learn semantics
mainly from center patches
of the original images
and little from non-center patches.

Possible extensions, 
as mentioned in the arXiv version~\cite{CAE2022},
include:
investigating the possibility only 
considering the pretraining task,
masked representation prediction,
without masked patch reconstruction,
pretraining a depth-wise convolution network with masked convolution,
and pretraining with the CLIP targets~\cite{CAEv22022}.

\noindent \textbf{Potential limitations.} 
The proposed method may face challenges when dealing with
large and contiguous masked regions in an image,
e.g., the whole object region is almost masked. Obtaining plausible and high-quality reconstruction for large areas can be particularly difficult, as the model has to infer the missing information based on limited available context. This is a common limitation of Masked Image Modeling methods, and our proposed method is not exempt from it.


\section*{Acknowledgments}
We would like to acknowledge Hangbo Bao, Xinlei Chen, Li Dong, Qi Han, Zhuowen Tu, Saining Xie, and Furu Wei for the helpful discussions.



\section*{Declarations}
\begin{itemize}
\item Funding

This work is partially supported by the National Key Research and Development
Program of China (2020YFB1708002), National Natural Science Foundation of China (61632003, 61375022, 61403005), Grant SCITLAB-20017 of Intelligent Terminal Key Laboratory of SiChuan Province, Beijing Advanced Innovation Center for Intelligent Robots and Systems (2018IRS11), and PEK-SenseTime Joint Laboratory of Machine Vision. Ping Luo is supported by the General Research Fund of HK No.27208720, No.17212120, and No.17200622.

\item Code availability 

Our code will be available at \url{https://github.com/Atten4Vis/CAE}.

\item Availability of data and materials

The datasets used in this paper are publicly available. 
ImageNet: \url{https://www.image-net.org/},\\ ADEK: \url{https://groups.csail.mit.edu/vision/datasets/ADE20K/}, \\
COCO: \url{https://cocodataset.org/}, \\
{Food-101}: \url{https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/}, \\
{Clipart}: \url{http://projects.csail.mit.edu/cmplaces/download.html},\\
{Sketch}: \url{http://projects.csail.mit.edu/cmplaces/download.html}.



\end{itemize}




\bibliographystyle{plain}
\bibliography{_sn-bibliography}



\end{document}
