\pdfoutput=1
\documentclass{LMCS}

\def\dOi{10(4:19)2014}
\lmcsheading {\dOi}
{1--23}
{}
{}
{Jan.~29, 2014}
{Dec.~29, 2014}
{}

\ACMCCS{[{\bf Theory of computation}]: Formal languages and
  automata theory---Automata over infinite objects; Theory and
  algorithms for application domains---Machine learning
  theory---Active learning}



\usepackage{graphicx}
\usepackage{longtable}
\usepackage{amssymb,amsmath}
\usepackage{xspace} 
\usepackage{stmaryrd}
\usepackage[ruled,algo2e,norelsize]{algorithm2e}
\usepackage{mathtools}
\usepackage{hyperref}




\def\N{\ensuremath{\mathbb N}\xspace}
\def\Nz{\ensuremath{\N_{{}>0}}\xspace}
\newcommand\df{\ensuremath{\mathrel{\smash{\stackrel{\scriptscriptstyle{
    \text{def}}}{=}}}}}
\newcommand{\pto}{\rightharpoonup}
\newcommand{\set}[1]{\{1,\ldots,#1\}}
\newcommand\powerset[1]{2^{#1}}
\newcommand\emptyInj{\emptyset}
\newcommand\domain[1]{dom(#1)}

\newcommand{\Data}{D}
\newcommand{\gr}[1]{\textcolor[gray]{0.5}{\scalebox{0.6}{#1}}}
\newcommand{\firstocc}[1]{\mathit{first}_{#1}}
\newcommand{\lastocc}[1]{\mathit{last}_{#1}}


\def\A{\mathcal A}
\def\B{\mathcal B}
\def\cB{\mathcal B}
\newcommand{\Reg}{\mathcal{R}}
\newcommand{\rReg}{\Reg^\uparrow}
\newcommand{\lReg}{\Reg^\odot}
\newcommand{\gReg}{\Reg^\circledast}
\newcommand{\extReg}{\gReg \mathrel{\cup} \lReg \mathrel{\cup} \rReg}
\newcommand{\rreg}[1]{#1^\uparrow}
\newcommand{\lfresh}[1]{#1^\odot}
\newcommand{\gfresh}[1]{#1^\circledast}
\def\Prom{P}
\newcommand{\regof}[1]{\textsf{reg}(#1)}
\newcommand{\opof}[1]{\textsf{op}(#1)}
\newcommand{\Reusable}{\mathit{Free}}
\newcommand{\fStates}{F}
\newcommand{\States}{S}
\newcommand{\init}{\iota}
\newcommand{\Trans}{\Delta}
\newcommand{\xregtrans}[1]{\xrightarrow{#1}}
\newcommand{\conf}{\gamma}
\newcommand{\regm}{\tau}
\newcommand{\Used}{U}
\newcommand{\xconfrel}[1]{\xRightarrow{#1}}
\newcommand{\ack}{\mathsf{ack}}
\newcommand{\req}{\mathsf{req}}
\newcommand{\cOm}{\mathsf{com}}
\newcommand{\forw}{\mathsf{forw}}
\newcommand{\join}{\mathsf{join}}


\def\concretization{\gamma}
\newcommand{\Nat}{\Gamma}
\newcommand{\kNat}[1]{\Gamma_{#1}}
\newcommand{\symbL}{L_\mathit{symb}}
\def\Injection{\mathrm{Inj}}
\newcommand{\can}[1]{#1^{C}}
\newcommand{\snf}{\mathit{snf}}
\newcommand{\DWords}{\gamma}
\newcommand{\DWordsof}[1]{\DWords(#1)}
\newcommand{\WF}{\mathsf{WF}}
\newcommand{\Bounded}[1]{\mathsf{DW}_{#1}}
\newcommand\NF{\mathsf{SNF}}


\def\fresh{\mathit{fresh}}
\newcommand{\mylabel}{\textup{label}}
\renewcommand{\phi}{\varphi}
\newcommand{\ttrue}{\mathit{true}}
\newcommand{\equal}{\textup{equal}}
\newcommand{\newk}{k}
\newcommand{\regpos}{\mathit{reg}}
\newcommand{\param}{\pi}
\newcommand{\runf}{\alpha}
\newcommand{\dataf}{\beta}
\newcommand{\kphi}{k_\dataf}
\newcommand{\tupleI}{\overline{I}}

\def\figurename{Figure}





\usepackage[pdflatex=true,recompilepics=false]{gastex} 
\gasset{frame=false}

\begin{document}

\title[A Robust Class of Data Languages and an Application to Learning]{A Robust Class of Data Languages and an Application to Learning\rsuper*}

\author[B.~Bollig]{Benedikt Bollig\rsuper a}
\address{{\lsuper a}LSV, ENS Cachan, CNRS \& Inria, France}
\email{bollig@lsv.ens-cachan.fr}

\author[P.~Habermehl]{Peter Habermehl\rsuper b}
\address{{\lsuper b}Univ Paris Diderot, Sorbonne Paris Cit\'e, LIAFA, CNRS, France}
\email{haberm@liafa.univ-paris-diderot.fr}

\author[M.~Leucker]{Martin Leucker\rsuper c}
\address{{\lsuper c}ISP, University of L\"ubeck, Germany}
\email{leucker@isp.uni-luebeck.de}

\author[B.~Monmege]{Benjamin Monmege\rsuper d}
\address{{\lsuper d}Universit{\'e} Libre de Bruxelles, Belgium}
\email{bmonmege@ulb.ac.be}

\keywords{Register Automata; Data words; Angluin-style learning; 
  Freshness.}
\titlecomment{{\lsuper*}This paper is an extended and revised version of the paper
``A Fresh Approach to Learning Register Automata'' which appeared in DLT 2013}

\begin{abstract}
\noindent
  We introduce \emph{session automata}, an automata model to process data words, i.e., words over an infinite alphabet.
  Session automata support the notion of \emph{fresh} data
  values, which are well suited for modeling protocols in which sessions using
  fresh values are of major interest, like in security protocols or
  ad-hoc networks. Session automata have an expressiveness partly extending, partly reducing that of
  classical register automata. We show that, unlike register automata and their various extensions, session automata are robust: They
\,(i) are closed under intersection, union, and (resource-sensitive) complementation,
  \,(ii) admit a symbolic regular representation,
  \,(iii) have a decidable inclusion problem
    (unlike register automata), and
  \,(iv) enjoy logical characterizations.
Using these results, we establish a learning algorithm to infer
  session automata through membership and equivalence queries.
\end{abstract}

\maketitle

\section{Introduction}

The study of automata over data words, i.e., words over an infinite
alphabet, has its origins in the seminal work by Kaminski and Francez
\cite{Kaminski1994}. Their finite-memory automata (more commonly
called \emph{register automata}) equip finite-state machines with
registers in which data values (from the infinite alphabet) can be
stored and be reused later. Register automata preserve some of the
good properties of finite automata: they have a decidable emptiness
problem and are closed under union and intersection. On the other
hand, register automata are neither determinizable nor closed under
complementation, and they have an undecidable equivalence/inclusion
problem. There are actually several variants of register automata,
which all have the same expressive power but differ in the complexity
of decision problems \cite{DL-tocl08,Bjorklund10}.
In the sequel, many more automata models have been introduced (not
necessarily with registers), aiming at a good balance between
expressivity, decidability, and closure properties
\cite{Neven2004,DL-tocl08,KaminskiZ10,BL2010,DBLP:conf/atva/GrumbergKS13,DBLP:conf/lata/GrumbergKS10}.
Some of those models extend register automata, inheriting their
drawbacks such as undecidability of the equivalence problem.

We will follow the work on register automata and study a model that
supports the notion of \emph{freshness}. When reading a data value, it
may enforce that the data value is \emph{fresh}, i.e., it has not
occurred in the whole history of the run. This feature has been
proposed in \cite{DBLP:conf/popl/Tzevelekos11} to model computation
with names in the context of programming-language semantics. Actually,
fresh names are needed to model object creation in object-oriented
languages, and they are
important ingredients in modeling security protocols which often make
use of so-called fresh nonces to achieve their security assertions
\cite{DBLP:conf/ccs/KurtzKW07}. Fresh names are also crucial in the
field of network protocols, and they are one of the key features of
the -calculus \cite{MPW92}. Like ordinary register automata,
fresh-register automata preserve some of the good properties of finite
automata. However, they are not closed under complement and also come
with an undecidable equivalence problem.

In this paper, we propose \emph{session automata}, a robust automata
model over data words.
Like register automata, session automata are a syntactical restriction
of fresh-register automata, but in an orthogonal way. Register
automata drop the feature of checking \emph{global freshness} (referring to the
whole history) while keeping a local variant (referring to the
registers). Session automata, on the other hand, discard local
freshness, while keeping the global one. Session automata are
well-suited whenever fresh values are important for a finite period,
for which they will be stored in one of the registers. They correspond
to the model from \cite{BCGK-fossacs12} without stacks. 

Not surprisingly, we will show that session automata and register automata
describe incomparable classes of languages of data words, whereas both
are strictly weaker than fresh-register automata. Contrary to
finite-state unification based automata introduced in~\cite{KamTan06},
session automata (like fresh-register automata) do not have the
capability to reset the content of a register. However, they can test
global freshness which the model of \cite{KamTan06} cannot. The
\emph{variable automata} from \cite{DBLP:conf/lata/GrumbergKS10} do not
employ registers, but rather use bound and free
variables. However, variable automata are close to our model: they use a
finite set of bound variables to track the occurrences of some data values, and a
single free variable for all other data values (that must be different
from data values tracked by bound variables). Contrary to our model,
variable automata cannot test for global freshness, but we are not
able to recognize the language of all data words, contrary to them.

In this paper, we show that session automata
\,(i) are closed under intersection, union, and resource-sensitive
  complementation\footnote{A notion similar to \cite{KST2012}, but for
    a different model.},
\,(ii) have a unique canonical form (analogous to minimal deterministic
  finite automata),
\,(iii) have a decidable equivalence/inclusion problem, and
\,(iv) enjoy logical characterizations.
Altogether, this provides a versatile framework for languages over
infinite alphabets.

In a second part of the paper, we present an application of our
automata model in the area of learning, where decidability of the
equivalence problem is crucial.  Learning automata deals with the
inference of automata based on some partial information, for example
samples, which are words that either belong to the accepted language
or not. A popular framework is that of active learning defined by
Angluin \cite{Angluin:regset} in which a learner may consult a teacher
for so-called membership and equivalence queries to eventually infer
the automaton in question.  Learning automata has many applications in
computer science. Notable examples are the use in model checking
\cite{DBLP:conf/sigsoft/GiannakopoulouM03} and testing
\cite{BergGJLRS05}. See \cite{DBLP:conf/fmco/Leucker07} for an
overview.

While active learning of regular languages is meanwhile well
understood and is supported by freely available libraries such as
LearnLib \cite{MRSL07} and libalf \cite{BKKLNP10}, extensions beyond
plain regular languages are still an area of active
research. Recently, automata dealing with potentially infinite data as
basis objects have been studied. Seminal works in this area are that
of \cite{DBLP:conf/fm/AartsHKOV12,DBLP:conf/sfm/Jonsson11} and
\cite{HowarSJC12}. While the first two use abstraction and refinement
techniques to cope with infinite data, the second approach learns a
sub-class of register automata.  Note that session automata are
incomparable with the model from \cite{HowarSJC12}.  Thanks to their
closure and decidability properties, a conservative extension of
Angluin's classical algorithm will do for their automatic inference.


\subsubsection*{Outline.}
The paper is structured as follows. In Section~\ref{sec:data} we
introduce session automata. Section~\ref{sec:snf-can} presents the
main tool allowing us to establish the results of this paper, namely
the use of data words in symbolic normal form and the construction of
a canonical session automaton. The section also presents some closure
properties of session automata and the decidability of the equivalence
problem. Section~\ref{sec:logic} gives logical characterizations of
our model.  In Section~\ref{sec:learning}, we present an active
learning algorithm for session automata. This paper is an extended
version of \cite{BHLM-dlt2013}.


\section{Data Words and Session Automata}\label{sec:data}

We let  be the set of natural numbers and  be the set of
non-zero natural numbers. In the following, we fix a non-empty finite
alphabet  of \emph{labels} and an infinite set  of
\emph{data values}.  In examples, we usually use .  A
\emph{data word} over  and  is a sequence  of pairs . In other words,  is an element from . For
, we let  denote the
position  where  occurs for the first time,
i.e., such that  and there is no  such that . Accordingly, we define  to be the last position
where  occurs.

An example data word over  and  is given by
 . We have
 and .

This section recalls two existing automata models over data words --
namely register automata, previously introduced in
\cite{Kaminski1994}, and fresh-register automata, introduced in
\cite{DBLP:conf/popl/Tzevelekos11} as a generalization of register
automata. Moreover, we introduce the new model of session automata,
our main object of interest.

Register automata (initially called finite-memory automata) equip finite-state
machines with registers in which data values can be stored and be read
out later. Fresh-register automata additionally come with an oracle that
can determine if a data value is \emph{fresh}, i.e., has not occurred in the
history of a run. Both register and fresh-register automata
are closed under union and intersection, and they have a decidable emptiness
problem. However, they are not closed under complementation, and their
equivalence problem is undecidable, which limits their application in areas such
as model checking and automata learning.
Session automata, on the other hand, are closed under (resource-sensitive)
complementation, and they have a decidable inclusion/equivalence problem.

Given a set , we let ,
, and .
In the automata models that we are going to introduce,  will be the set of registers.
Transitions will be labeled with an element
from , which determines a register and the operation that is
performed on it. More precisely,  writes a globally fresh value into , 
writes a locally fresh value into , and  uses the value that is currently stored in . For , we let  if
. Similarly,


\begin{defi}[Fresh-Register Automaton, cf.\ \cite{DBLP:conf/popl/Tzevelekos11}]
  A \emph{fresh-register automaton} (over  and ) is a tuple  where
  \begin{itemize}
  \item  is the non-empty finite set of \emph{states},
  \item  is the non-empty finite set of \emph{registers},
  \item  is the \emph{initial state},
  \item  is the set of \emph{final
      states}, and
  \item  is a finite set of \emph{transitions}: each
    transition is a tuple of the form  where  are the source and target state, respectively, , and . We call  the
    \emph{transition label}.
  \end{itemize}
\end{defi}

\noindent For a transition , we also write . When taking this transition, the automaton
moves from state  to state  and reads a symbol . If , then  is the
data value that is currently stored in register . If , then  is some \emph{globally fresh} data
value, which has not been read in the \emph{whole} history of the run;
 is then written into register . Finally, if , then  is some \emph{locally fresh} data
value, which is \emph{currently} not stored in the registers; it will
henceforth be stored in register .

Let us formally define the semantics of . A \emph{configuration} is a
triple  where  is the current
state,  is a partial mapping encoding the
current register assignment, and  is the set of data values that have
been used so far. By , we denote the set of registers  such that  is defined.
Moreover, .
We say that  is \emph{final} if . As usual, we define a transition relation over
configurations and let , where , if there is
a transition  such that the following
conditions hold:
\begin{enumerate}
\item 
\item  and
  ,
\item  and  for all
  .
\end{enumerate}
A run of  on a data word  is a sequence
 for suitable configurations
 with 
(here the partial mapping~ represents the mapping with
empty domain).
The run is \emph{accepting} if  is a final configuration. The
\emph{language}  of 
is then defined as the set of data words for which there is an
accepting run.

Note that fresh-register automata cannot distinguish
between data words that are equivalent up to permutation of data
values. More precisely, given , we write 
if  and  such that, for all , we have  iff\ .
For instance, .
In the following, the equivalence class of a data word
 wrt.\  is written .
We call  a \emph{data language} if,
for all  such that , we have  if, and only if, . In particular,  is a data language for every fresh-register automaton .

We obtain natural subclasses of fresh-register automata when we
restrict the transition labels  in the transitions.

\begin{defi}[Register Automaton, \cite{Kaminski1994}]
  A \emph{register automaton} is a fresh-register automaton where
  every transition label is from .
\end{defi}

Like register automata, session automata are a syntactical restriction
of fresh-register automata, but in an orthogonal way. Instead of local
freshness, they include the feature of global freshness.

\begin{defi}[Session Automaton]
  A \emph{session automaton} is a fresh-register automaton where every
  transition label is from .
\end{defi}

We first compare the three models of automata introduced above in
terms of expressive power.

\begin{exa}\label{ex:req-ack}
  Consider the set of labels  and the set of
  data values , representing an infinite supply of process
  identifiers (pids). We model a simple (sequential) system where
  processes can approach a server and make a request, indicated by
  , and where the server can acknowledge these requests,
  indicated by . More precisely,  means that the process with pid  performs a request, which is
  acknowledged when the system executes .

  \figurename~\ref{fig:multiples3}(a) depicts a register
  automaton that recognizes the language  of data words verifying
  the following conditions:
  \begin{itemize}
  \item there are at most two open requests at a time;
  \item a process waits for an acknowledgment before making another request;
  \item every acknowledgment is preceded by a request;
  \item requests are acknowledged in the order they are received.
  \end{itemize}
  In the figure, an edge label of the form  shall denote that there are two transitions, one
  labeled with , and one labeled with
  . Whereas a transition labeled with
   is taken when the current data value does not
  appear currently in the registers (but could have appeared before in
  the data word) and store it in , transition labeled with
   simply checks that the current data is stored in
  register . The automaton models a server that can store two
  requests at a time and will acknowledge them in the order they are
  received. For example, it accepts
  .

  When we want to guarantee that, in addition, every process makes at most one request, we
  need the global freshness
  operator. \figurename~\ref{fig:multiples3}(b)
  hence depicts a session automaton recognizing the language  of
  all the data words of  in which every process makes at most one
  request. Notice that the transition from  to  is now
  labeled with , so that this transition can only
  be taken in case the current data value has never been seen
  before. We obtain  from  by replacing every occurrence
  of  with . While
  
  is no longer contained in , 
   is still accepted.

  As a last example, consider the language  of data words in
  which every process makes at most one request (without any other
  condition). A fresh-register automaton recognizing it is given in
  \figurename~\ref{fig:fra}.

\begin{gpicture}[name=registerAutomaton,ignore]
    \unitlength=4
    \gasset{Nframe=y,Nw=4,Nh=4,Nmr=4,ilength=3.5,flength=3.5,AHangle=30}
    \node[Nmarks=if,iangle=90,fangle=-90](0)(0,0){}
    \gasset{curvedepth=4}\node(1)(-20,0){}\node(2)(20,0){}\node(12)(0,13){}\node(21)(0,-13){}

    \drawedge[curvedepth=-1.5,ELside=r,ELpos=47](0,1){}
    \drawedge[curvedepth=-1.5,ELside=r](1,0){}
    \drawedge[ELdist=-0.5](1,12){}
    \drawedge[ELdist=-1,ELside=l](2,21){}
    \drawedge[ELdist=0.3](21,1){}
    \drawedge[ELdist=0.3,ELside=l](12,2){}
    \drawedge[curvedepth=1.5,ELside=l,ELpos=47](0,2){}
    \drawedge[curvedepth=1.5,ELside=l](2,0){}
  \end{gpicture}
  \begin{gpicture}[name=sessionAutomaton,ignore]
    \unitlength=4
    \gasset{Nframe=y,Nw=4,Nh=4,Nmr=4,ilength=3.5,flength=3.5,AHangle=30}
    \node[Nmarks=if,iangle=90,fangle=-90](0)(0,0){}
    \gasset{curvedepth=4}\node(1)(-20,0){}\node(2)(20,0){}\node(12)(0,13){}\node(21)(0,-13){}

    \drawedge[curvedepth=-1.5,ELside=r](0,1){}
    \drawedge[curvedepth=-1.5,ELside=r](1,0){}
    \drawedge[ELdist=0.3](1,12){}
    \drawedge[ELdist=0.3,ELside=l](2,21){}
    \drawedge[ELdist=0.3](21,1){}
    \drawedge[ELdist=0.3,ELside=l](12,2){}
    \drawedge[curvedepth=1.5,ELside=l](0,2){}
    \drawedge[curvedepth=1.5,ELside=l](2,0){}
  \end{gpicture}
  \begin{figure}[tb]
    \centering 
     \begin{tabular}{cc}
\scalebox{0.9}{\gusepicture{registerAutomaton}} &
\hspace{1em}\scalebox{0.9}{\gusepicture{sessionAutomaton}}\\
      (a) & (b)
     \end{tabular}
\caption{(a) Register automaton  for , (b) Session
      automaton  for }
    \label{fig:multiples3}
  \end{figure}
   \begin{gpicture}[name=freshRegisterAutomaton,ignore]
    \unitlength=4
    \gasset{Nframe=y,Nw=4,Nh=4,Nmr=4,ilength=3.5,flength=3.5,AHangle=30}
    
    \node[Nmarks=if,fangle=-90](s0)(0,0){}
    
    \drawloop[ELside=r,loopCW=n,loopdiam=4,ELdist=-0.5,loopangle=0](s0){
      }
  \end{gpicture}
  \begin{figure}[tb]
    \centering
    \scalebox{.9}{\gusepicture{freshRegisterAutomaton}}
    \caption{Fresh-register automaton  for }
    \label{fig:fra} 
  \end{figure}
\end{exa}

\begin{prop}
  Register automata and session automata are incomparable in terms of expressive power.  Moreover,
  fresh-register automata are strictly more expressive than both
  register automata and session automata.
\end{prop}
\proof
  We use the languages , , and  defined in
  Example~\ref{ex:req-ack} to separate the different automata models.

  First, the language , recognizable by a register automaton, is not
  recognized by any session automaton. Indeed, denoting  the data
  word , no session automaton using  registers
  can accept
  
  Intuitively, the session automaton must store all  data values
  of the requests in order to check the acknowledgement, and cannot
  discard any of the  first data values to store the th
  since all of them have to be reused afterwards (and at that time
  they are not globally fresh anymore). More precisely, after reading
   the configuration must be of the form
   with  being a permutation of
  . Reading , with fresh data value ,
  must then replace the content of a register with . Suppose it
  is register . Then, when reading the second occurrence of ,
  data value  is not globally fresh anymore, yet it is not stored
  anymore in the registers, which does not allow us to accept this
  data word.
  
  Then, the language , recognizable by a session automaton, is
  indeed not recognizable by a register automaton, for the same
  reasons as already developed in Proposition~5 of
  \cite{Kaminski1994}. Intuitively, the automaton needs to register
  every data value encountered since it has to ensure the freshness of
  every pid.

  Finally, language , recognized by a fresh-register automaton,
  is not recognized by any register automaton (see again Proposition~5
  of \cite{Kaminski1994}) nor by any session automaton. In particular, no
  session automaton with  registers can accept the data word
   since when reading the letter , all
  the  data values seen so far should be registered to accept the
  suffix afterwards. A formal proof can be done in the same spirit as
  for . \qed

\begin{exa}\label{ex:p2p}
  To conclude the section, we present a session automaton with  registers that models a P2P
  protocol. A user can join a host with address , denoted by action
  . The request is either forwarded by  to another host
  , executing , or acknowledged by
  . In the latter case, a connection between the user and
   is established so that they can communicate, indicated by action
  . Note that the sequence of actions
   should be considered as an encoding of a
  single action  and is a way of dealing with actions
  that actually take two or more data values, as considered, e.g., in
  \cite{HowarSJC12}. An example execution of our protocol is
  . In
  \figurename~\ref{fig:P2P}, we show the session automaton for the P2P
  protocol: it uses 2 registers. Following \cite{BCGK-fossacs12}, our
  automata can be easily extended to multi-dimensional data
  words. This also holds for the learning algorithm that will be
  presented in Section~\ref{sec:learning}.


\begin{gpicture}[name=P2P,ignore]
\unitlength=4
\gasset{AHangle=30,ilength=3.5,flength=3.5,Nw=4,Nh=4,Nmr=4}
\node[Nmarks=i](0)(5,0){}
\node(1)(20,0){}
\node(2)(40,0){}
\node[Nmarks=f,fangle=-90](3)(20,-10){}
\node[Nmarks=f,fangle=-90](4)(40,-10){}

\gasset{Nw=5,Nh=5,Nmr=5,loopdiam=4}
\drawedge(0,1){}
\drawedge[curvedepth=1.5](1,2){}
\drawedge[curvedepth=1.5](2,1){}
\drawedge[ELside=r](1,3){}
\drawedge[ELside=l](2,4){}
\drawloop[loopCW=n,ELside=r,loopangle=180](3){}
\drawloop[loopangle=180](4){}
\end{gpicture}
\begin{figure}[tb]
  \centering
  \scalebox{.9}{\gusepicture{P2P}}
  \caption{Session automaton for the P2P protocol}
  \label{fig:P2P}
\end{figure}

\end{exa}


\section{Symbolic Normal Form and Canonical Session Automata}
\label{sec:snf-can}

Closure properties of session automata, decidability of inclusion/equivalence
and the learning algorithm will be established by means of a symbolic
normal form of a data word, as well as a canonical session automaton
recognizing those normal forms. The crucial observation is that data
equality in a data word recognized by a session automaton only depends
on the transition labels that generate it. In this section, we suppose
that the set of registers of a session automaton is of the form . In the following, we let  and, for , .

\subsection{Data Words in Symbolic Normal Forms}

Suppose a session automaton reads a sequence  of transition labels. We
call  a \emph{symbolic word}.  It ``produces'' a data word if, and
only if, a register is initialized before it is used. Formally, we say
that  is \emph{well-formed} if, for all positions 
with , there is 
such that .  Let  be the set of all well-formed words.

With , we can associate an equivalence
relation  over , letting  if,
and only if,
\begin{itemize}
\item , and
\item  and there is no position  such that , or\\
   and there is no position  such
  that .
\end{itemize}
If  is well-formed, then the data values of every data word
 that a session automaton ``accepts
via''  conform with the equivalence relation , that is, we
have  iff\ .
This motivates the following definition.
Given a well-formed word , we call 
a \emph{concretization} of  if it is of the form 
such that, for all , we have 
iff\ . For
example,  is a concretization of
.

Let  denote the set of all concretizations of
. Observe that, if  is a data word from , then
. Concretization is extended to sets  of well-formed words, and we let
. Note
that, here, we first filter the well-formed words before applying the
operator.
Now, let  be a session
automaton. In the obvious way, we may consider  as a finite
automaton over the finite alphabet . We then obtain a regular language  (indeed,  if ).  It is not
difficult to verify that .

Though we have a symbolic representation of data languages recognized
by session automata, it is in general difficult to compare their
languages, since different symbolic words may give rise to the
same concretizations. For example, we have
. However, we can
associate, with every data word, a symbolic normal form, producing
the same set of concretizations. Intuitively, the normal form uses the
first (according to the natural total order) register whose current
data value \emph{will not be used anymore}. In the above example,
 would be in symbolic normal
form: the data value stored at the first position in register  is
not reused so that, at the second position, register 1 \emph{must} be
overwritten. For the same reason,
 is not in symbolic normal
form, in contrast to
 where register
 is read at the end of the word.

Formally, given a data word , we define
its symbolic normal form  inductively, along with sets
 indicating the registers that are reusable
after executing position . Setting , we define
 and
 We canonically extend  to data languages ,
setting .

\begin{exa}
Let .
Then, we have .
\end{exa}

\begin{gpicture}[name=dataWordSNF,ignore]
  \gasset{Nframe=y,Nmr=0,Nh=14,AHLength=1.5,AHangle=22}
  \unitlength=1.1mm

  \newcommand{\lettersize}{0.9}

  \node[Nadjust=w](a1)(0,0){}
  \node[Nadjust=w](a2)(50,0){}

  \drawedge[curvedepth=1.5,ELdist=1.5](a1,a2){}
  \drawedge[curvedepth=1.5](a2,a1){}

\end{gpicture}

The relation between the mappings  and  is illustrated 
below
\begin{center}
  \gusepicture{dataWordSNF}
\end{center}
One easily verifies that , for all data
languages .  Therefore, equality of data languages reduces to
equality of their symbolic normal forms:

\begin{lem}\label{lem:dataeq}
  Let  and  be data languages. Then,  if, and only if,
  .
\end{lem}

Of course, symbolic normal forms may use any number of registers so
that the set of symbolic normal forms is a language over an infinite
alphabet as well. However, given a session automaton , the
symbolic normal forms that represent the language  uses only a
bounded (i.e., finite) number of registers. Indeed, an important
notion in the context of session automata is the \emph{bound} of a
data word. Intuitively, the bound of  is the minimal number of registers
that a session automaton needs in order to execute . Or, in other
words, the bound is the maximal number of overlapping \emph{sessions}.
A session is an interval delimiting the occurrence of one particular
data value. Formally, a session of  is a set  of the
form 
with  a data value appearing in . Given , we say that  is -\emph{bounded} if every position  is contained in at most  sessions.  Let
 denote the set of -bounded data words, and let
 denote the set of symbolic normal
forms of all -bounded data words.

One can verify that a data word  is -bounded if, and only if,
 is a word over the alphabet . 
Notice that .
Indeed, inclusion  is trivial. If, on the other hand, , we must have
, which implies that
.

A data language  is said to be -\emph{bounded} if . It is \emph{bounded} if it is -bounded for some .
Note that the set of all data words is not bounded.

\figurename~\ref{fig:multiples2}(a) illustrates a
data word  with four different sessions. It is 2-bounded, as no
position shares more than 2 sessions.

\begin{gpicture}[name=simpleSA,ignore]
  \unitlength=4 \gasset{Nw=4,Nh=4,Nmr=4,loopdiam=4}
  \gasset{AHangle=30,ilength=3.5,flength=3.5}
  \node[Nmarks=if,iangle=90,fangle=-90](0)(0,0){}
  \drawloop[loopangle=180](0){}
\end{gpicture}
\begin{figure}[t]
\centering
\begin{tabular}{cc}
\begin{minipage}{.4\linewidth}
  \\\vspace{-0.8ex}
\hspace*{0.3ex}
  \hspace*{3ex}\\
\hspace*{4ex}
 \hspace*{2.5ex}
\end{minipage} &
\hspace{2em}
\begin{minipage}{.2\linewidth}
    \raisebox{-2em}{\scalebox{.9}{\gusepicture{simpleSA}}}
    \\\vspace{.3em}
\end{minipage} \ \Trans\big((i,\Prom),(a,\rreg{r})\big) &=
    \begin{cases} (i,\Prom\setminus\{r\}) &\text{if } r\leq i\\
      \text{not defined} &\text{otherwise}
    \end{cases}\\ \Trans\big((i,\Prom),(a,\gfresh{r})\big) &=
    \begin{cases} (\max(i,r),\Prom\cup\set{r-1}) &\text{if } r-1\leq i
      \land r\notin \Prom\\ \text{not defined} &\text{otherwise}
    \end{cases}
  \tilde L = \{u\in\WF \cap (\Sigma \times
  \Gamma_k)^\ast \mid \text{there is } u'\in L \text{ such that }
  \concretization(u)=\concretization(u')\}
    \tilde\Trans =
    &\phantom{{}\cup{}}\big\{\big((s_1,\sigma),(a,\rreg{\sigma(r)}),
    (s_2,\sigma)\big)
    \mid \big(s_1,(a,\rreg{r}),s_2\big)\in\Trans\big\}\\
    & {}\cup
    \big\{\big((s_1,\sigma_1),(a,\gfresh{r_2}),(s_2,\sigma_2)\big)
    \mid \big(s_1,(a,\gfresh{r_1}),s_2\big)\in\Trans
    \land \sigma_2 = \sigma[r_1\mapsto r_2]{}\\
    & \hspace{5em}\text{ with } \sigma \text{ maximal sub-mapping of }
    \sigma_1~ \text{s.t.\ } \sigma[r_1\mapsto r_2] \text{ injective}\big\}
  \phi ~~::=~~ \mylabel(x)=a \mid x=y \mid y=x+1\mid x\sim
y\mid x\in X\mid \neg \phi \mid \phi\lor\phi \mid \exists x \; \phi\mid \exists
X\; \phi\exists X_1 \cdots \exists X_m\; (\runf \wedge \forall
  x\forall y\; (x \sim y \leftrightarrow \dataf))d_i = d_j
~\Longleftrightarrow~ (a_1 \cdots a_n,i,j,\tupleI) \in
\symbL(\dataf)\,.\tag{}\label{lab:deq}

Suppose, towards a contradiction, that  is not
-bounded. Then, there are  and a position  such that  is contained in  distinct sessions
. For , let  and , so that . Note that the  are pairwise distinct,
and so are the . By~\eqref{lab:deq}, for every , we have . Thus, for every such word , there is a
unique accepting run of , say, being in state  after
executing position . As  has only  states, there
are  such that . Thus, there is an accepting
run of  either on a word where one of the first-order
components is not unique, which is a contradiction, or on . The latter contradicts (\ref{lab:deq}),
since  and  are distinct sessions. \qed



\section{Learning Session Automata}
\label{sec:learning}

In this section, we introduce an active learning algorithm for session
automata. In the usual active learning setting (as introduced by
Angluin \cite{Angluin:regset}, see \cite{Hig10} for a general overview
of active learning techniques), a \emph{learner} interacts with a
so-called minimally adequate \emph{teacher} (MAT), an oracle which can
answer \emph{membership} and \emph{equivalence queries}.  In our case,
the learner is given the task to infer the data language 
defined by a given session automaton .  We suppose here that the
teacher knows the session automaton or any other device accepting . In practice, this might not be the case ---  could be a
black box --- and equivalence queries could be (approximately)
answered, for example, by extensive testing.  The learner can ask if a
\emph{data} word is accepted by  or not.  Furthermore it can ask
equivalence queries which consist in giving an \emph{hypothesis}
session automaton to the teacher who either answers yes, if the
hypothesis is equivalent to  (i.e., both data languages are the
same), or gives a data word which is a counterexample, i.e., a data
word that is either accepted by the hypothesis automaton but should
not, or vice versa.

Given the data language  accepted by a session automaton 
over  and , our algorithm will learn the canonical session
automaton , that uses  registers, i.e., the minimal
symbolically deterministic automaton recognizing the language 
and the regular language  over .  Therefore one can consider that the learning target is
 and use an arbitrary active learning algorithm for
regular languages. However, as the teacher answers only questions over
data words, queries have to be adapted. Since  only accepts
symbolic words which are in normal form, a membership query for a
given symbolic word  not in normal form will be answered negatively
(without consulting the teacher); otherwise, the teacher will be given
one data word included in  (all the answers on words of
 are the same).  Likewise, before submitting an equivalence
query to the teacher, the learning algorithm checks if the current
hypothesis automaton accepts symbolic words not in normal
form\footnote{This can be checked in polynomial time over the trimmed
  hypothesis automaton with a fixed point computation labelling the
  states with the registers that should be used again before
  overwriting them.}. If yes, one of those is taken as a
counterexample, else an equivalence query is submitted to the
teacher. Since the number of registers needed to accept a data
language is a priori not known, the learning algorithm starts by
trying to learn a session automaton with  register and increases
the number of registers as necessary.

Every active learning algorithm for regular languages may be adapted
to our setting.  Here we describe a variant of Rivest and Schapire's
algorithm \cite{RiSh:inference} which is itself a variant of Angluin's
L algorithm \cite{Angluin:regset}.  An overview of learning
algorithms for deterministic finite state automata can be found, for
example, in \cite{BergR05}.

The algorithm is based on the notion of \emph{observation table} which
contains the information accumulated by the learner during the
learning process.  An observation table over a given alphabet  is a triple  with  two
sets of words over  such that  and  is a mapping .  A table is
partitioned into an upper part  and a lower part .  We define for each  a mapping  where .  An observation table
must satisfy the following property: for all  such that  we have , i.e., there exists 
such that . This means that the rows of the
upper part of the table are pairwise distinct.  A table is
\emph{closed} if for all 
there exists  such that .  From a closed
table we can construct a symbolically deterministic session automaton
whose states correspond to the rows of the upper part of the table:

\begin{defi}
  \label{def:autfromtable}
  For a closed table  over a finite alphabet
  , we define a symbolically deterministic
  session automaton  over  by , , , ,
  and for all  and ,  if . This is well defined as the table is closed.
\end{defi}

\setlength{\algomargin}{.2em} 
\begin{algorithm2e}[bt]\footnotesize
  initialize  and  by 
  and  for all  with membership queries\;

  \Repeat{equivalence test succeeds}{
    \While{ is not closed}{
      find   and 
      such that for all \;
      extend table to  by  membership queries\;
    }
    from  construct the hypothesized automaton \tcp*[r]{cf.\
    Definition~\ref{def:autfromtable}}
    \eIf{ accepts symbolic words not in normal form}
    {
      let  be one of those\;
    }
    {
      \eIf{}
      {
        equivalence test succeeds\;
      }
      {
        get counterexample \; 
        set \; 
        find minimal  such that \;
        \If{}
        {
          set \;
          extend table to  over  by
          membership queries\;
        }
      }
    }
    \If(\tcp*[f]{is true if }){ is closed}
    {
      find a break-point for  where  is the distinguishing word\;
      extend table to  by membership queries\;
    }
  }
  \Return{}
  \caption{The learning algorithm for a session automaton }
  \label{table:algorithm}
\end{algorithm2e}


We now describe in detail our active learning algorithm for a given
session automaton  given in Table~\ref{table:algorithm}. It is
based on a loop which repeatedly constructs a closed table using
membership queries, builds the corresponding automaton and then asks
an equivalence query. This is repeated until  is learned.  An
important part of a active learning algorithm is the treatment of
counterexamples provided by the teacher as an answer to an equivalence
query.  Suppose that for a given  constructed from a
closed table  the teacher answers by a
counterexample data word .  Let .  If  uses more
registers than available in the current alphabet, we extend the
alphabet and then the table. If the obtained table is not closed, we
restart from the beginning of the loop.  Otherwise -- and also if 
does not use more registers -- we use Rivest and Schapire's
\cite{RiSh:inference} technique to extend the table by adding a
suitable  to  making it non-closed.  The technique is based on
the notion of break-point that we now recall.  As  is a
counterexample, (1) .  Let , with .  Then, for all  with , let
 be decomposed as  with , where , 
and the length of  is equal to  (we have also  for all  such that ).  Let  be the state visited by  just before reading the th letter,
along the computation of  on :  is a
break-point if . Because of (1) such a
break-point must exist and can be obtained with 
membership queries by a binary search.  The word  is called
the distinguishing word.  If  is extended by  the table is
not closed anymore ( and  become
different). Now, the algorithm closes the table again, then asks
another equivalence query and so forth until termination. At each
iteration of the loop the number of rows (each of those correspond to
a state in the automaton ) is increased by at least one.
Notice that the same counterexample might be given several times.  The
treatment of the counterexample only guarantees that the table will
contain one more row in its upper part.  We obtain the following:

\begin{thm}
  Let  be a session automaton over  and , using 
  registers. Let  be the corresponding canonical session
  automaton. Let  be its number of states,  its number of
  registers and  the length of the longest counterexample returned
  by an equivalence query. Then, the learning algorithm for 
  terminates with at most  membership
  and  equivalence queries.
\end{thm}
\proof
  This follows directly from the proof of correctness and complexity
  of Rivest and Schapire's algorithm \cite{BergR05,RiSh:inference}.
  Notice that the equivalence query cannot return a counterexample
  whose normal form uses more than  registers, as such a word is
  rejected by both  (by definition) and by 
  (by construction). \qed

Let us discuss the complexity of our algorithm. In terms of the
canonical session automaton, the number of required membership and
equivalence queries is polynomial. When the session automaton  is
data deterministic, using the discussion after the proof of
Theorem~\ref{thm:saregular} over the size of , the overall
complexity of the learning algorithm is polynomial in the number of
states of , but exponential in the number of registers it uses
(with constant base). As usual, we have to add one exponent when we
consider session automata which are not data deterministic. In
\cite{HowarSJC12}, the number of equivalence queries is polynomial in
the size of the underlying automaton. In contrast, the number of
membership queries contains a factor  where  is the number of
states and  the number of registers.  This may be seen as a
drawback, as  is typically large.  Note that \cite{HowarSJC12}
restrict to deterministic automata, since classical register automata
are not determinizable.

\begin{figure}[ht]
\begin{center}
{\footnotesize
\noindent
\begin{tabular}{lllllllllllllll}

&\raisebox{-2ex}{}&

&\raisebox{-2ex}{}&

&\raisebox{-2ex}{}&
\end{tabular}

\vspace*{2ex}

\begin{tabular}{llllllllllll}

&\raisebox{-2ex}{}&

\end{tabular}}
\end{center}
\caption{\label{fig:learning}The successive observation tables}
\end{figure}

  \begin{gpicture}[name=hypo1,ignore]
    \unitlength=4 \gasset{Nw=4,Nh=4,Nmr=4,loopdiam=4}
    \gasset{AHangle=30,ilength=3.5,flength=3.5}
    \node[Nmarks=if,iangle=180,fangle=-90](0)(0,0){}
    \drawloop[loopangle=90](0){}
  \end{gpicture}
  \begin{gpicture}[name=hypo2,ignore]
    \unitlength=4
    \gasset{Nw=4,Nh=4,Nmr=4,loopdiam=4}
    \gasset{AHangle=30,ilength=3.5,flength=3.5}
    \node[Nmarks=if,iangle=180,fangle=-90](0)(0,0){}
    \node[Nmarks=f,fangle=0](1)(15,0){}
\drawedge(0,1){}
    \drawloop[loopangle=90](1){}
  \end{gpicture}
  \begin{gpicture}[name=hypo3,ignore]
    \unitlength=4
    \gasset{Nw=4,Nh=4,Nmr=4,loopdiam=4}
    \gasset{AHangle=30,ilength=3.5,flength=3.5}
    \node[Nmarks=if,iangle=180,fangle=-90](0)(0,0){}
    \node[Nmarks=f,fangle=-90](1)(15,0){}
    \node(3)(30,0){}
\drawedge(0,1){}
    \drawloop[loopangle=90](1){}
    \drawedge[curvedepth=2](1,3){}
    \drawloop[loopangle=90](3){}
    \drawedge[curvedepth=2](3,1){}
  \end{gpicture}
  \begin{gpicture}[name=hypo4,ignore]
    \unitlength=4 \gasset{Nw=4,Nh=4,Nmr=4,loopdiam=4}
    \gasset{AHangle=30,ilength=3.5,flength=3.5}
    \node[Nmarks=if,iangle=180,fangle=-90](0)(0,0){}
    \node[Nmarks=f,fangle=-90](1)(15,0){} 
    \node(3)(30,0){}
    \node[Nmarks=f,fangle=-90](5)(45,0){}
\drawedge(0,1){}
    \drawloop[loopangle=90](1){}
    \drawedge(1,3){}
    \drawloop[loopangle=90](3){}
    \drawedge[curvedepth=2](3,5){}
    \drawedge[curvedepth=2](5,3){}
    \drawloop[loopangle=0](5){}
  \end{gpicture}
  \begin{figure}[t]
    \centering
    \begin{tabular}{c@{\hspace{2em}}c@{\hspace{2em}}c}
      : \raisebox{-1.7em}{\scalebox{.8}{\gusepicture{hypo1}}} & 
      : \raisebox{-1.7em}{\scalebox{.8}{\gusepicture{hypo2}}} & 
      : \raisebox{-1.7em}{\scalebox{.8}{\gusepicture{hypo3}}}
    \end{tabular}\\
    : \raisebox{-1.7em}{\scalebox{.8}{\gusepicture{hypo4}}}
    \caption{\label{fig:hypos}The successive hypothesis automata}
  \end{figure}  

\begin{exa}
  We apply our learning algorithm on the data language given by the
  automaton  of
  \figurename~\ref{fig:exampleCanonical}(a).  In
  \figurename~\ref{fig:learning} the successive observation tables
  constructed by the algorithm are given. To save space some letters
  whose rows contain only 's are omitted.  In
  \figurename~\ref{fig:hypos} the successive automata constructed from
  the closed observation tables are given.  For sake of clarity we
  omit the sink states.  We start with the alphabet . We omit letters  and
  .  Table  is obtained after
  initialization and closing by adding  to the top.  We
  use  to indicate that all letters will lead to the same row.
  From  the first hypothesis automaton  is
  constructed.  We suppose that the equivalence query gives back as
  counterexample the data word  whose normal form is
  .  Here the break-point yields the
  distinguishing word .  We add it to . The obtained
  table is not closed anymore. We close it by adding 
  to the top and get table  yielding hypothesis
  automaton . Notice that .  This means that the equivalence
  query must give back a data word whose normal form is using at least
   registers (here  with normal form
  ).  As the word uses 
  registers, we extend the alphabet to  and
  obtain table .  We close the table and get .  From there we obtain the hypothesis automaton .  After
  the equivalence query we get
   as normal
  form of the data word counterexample . After
  adding  to  and closing the table by moving
   to the top we get finally
  the table  from which the canonical automaton
   is obtained and the equivalence query succeeds.
\end{exa}


\section{Conclusion}

In this paper, we developed a theory of session automata, which
form a robust class of data languages. In particular, they are closed under
union, intersection, and resource-sensitive complementation. Moreover, they
enjoy logical characterizations in terms of (a fragment of) MSO logic with
a predicate to compare data values for equality. Finally, unlike most other
automata models for data words, session automata have a decidable inclusion
problem. This makes them attractive for verification and learning.
In fact, we provided a complete framework for algorithmic learning of
session automata, making use of their canonical normal form. An
interesting direction to follow would be to try to apply those methods
to other models of automata dealing with data values like data
automata \cite{BojanczykDMSS11,Bjorklund10} or variable automata
\cite{DBLP:conf/lata/GrumbergKS10}.  As a next step, we plan to employ
our setting for various verification tasks. In particular, the next
step is to implement our framework, using possibly other learning
algorithms than the one of Rivest and Shapire that we presented in
this article, for instance using the LearnLib platform \cite{MRSL07}
or libalf \cite{BKKLNP10}.

\subsubsection*{Acknowledgments.} We are grateful to Thomas Schwentick
for suggesting the symbolic normal form of data words, and to the reviewers
for their valuable comments.



\def\Nst#1{}\def\Nnd#1{}\def\Nrd#1{}\def\Nth#1{}
\begin{thebibliography}{10}

\bibitem{DBLP:conf/fm/AartsHKOV12}
F.~Aarts, F.~Heidarian, H.~Kuppens, P.~Olsen, and F.~W. Vaandrager.
\newblock Automata learning through counterexample guided abstraction
  refinement.
\newblock In {\em FM}, volume 7436 of {\em Lecture Notes in Computer Science},
  pages 10--27. Springer, 2012.

\bibitem{Angluin:regset}
D.~Angluin.
\newblock Learning regular sets from queries and counterexamples.
\newblock {\em Information and Computation}, 75(2):87--106, 1987.

\bibitem{BergGJLRS05}
T.~Berg, O.~Grinchtein, B.~Jonsson, M.~Leucker, H.~Raffelt, and B.~Steffen.
\newblock On the correspondence between conformance testing and regular
  inference.
\newblock In {\em FASE}, volume 3442 of {\em Lecture Notes in Computer
  Science}, pages 175--189. Springer, 2005.

\bibitem{BergR05}
T.~Berg and H.~Raffelt.
\newblock Model checking.
\newblock In {\em Model-based Testing of Reactive Systems}, volume 3472 of {\em
  Lecture Notes in Computer Science}. Springer, 2005.

\bibitem{Bjorklund10}
H.~Bj{\"o}rklund and {\relax Th}.~Schwentick.
\newblock On notions of regularity for data languages.
\newblock {\em Theoretical Computer Science}, 411(4-5):702--715, 2010.

\bibitem{BojanczykDMSS11}
M.~Bojanczyk, C.~David, A.~Muscholl, T.~Schwentick, and L.~Segoufin.
\newblock Two-variable logic on data words.
\newblock {\em ACM Trans. Comput. Log.}, 12(4):27, 2011.

\bibitem{BL2010}
M.~Boja{\'n}czyk and S.~Lasota.
\newblock An extension of data automata that captures {XPath}.
\newblock In {\em {LICS 2010}}, pages 243--252. IEEE Computer Society, 2010.

\bibitem{BCGK-fossacs12}
B.~Bollig, A.~Cyriac, P.~Gastin, and K.~Narayan~Kumar.
\newblock Model checking languages of data words.
\newblock In L.~Birkedal, editor, {\em {P}roceedings of {FoSSaCS}'12}, volume
  7213 of {\em Lecture Notes in Computer Science}, pages 391--405. Springer,
  2012.

\bibitem{BHLM-dlt2013}
B.~Bollig, P.~Habermehl, M.~Leucker, and B.~Monmege.
\newblock A~fresh approach to learning register automata.
\newblock In {\em {P}roceedings of the 17th {I}nternational {C}onference on
  {D}evelopments in {L}anguage {T}heory ({DLT}'13)}, volume 7907 of {\em
  Lecture Notes in Computer Science}, pages 118--130. Springer, 2013.

\bibitem{BKKLNP10}
B.~Bollig, J.-P. Katoen, C.~Kern, M.~Leucker, D.~Neider, and D.~Piegdon.
\newblock {libalf}: the automata learning framework.
\newblock In {\em CAV}, volume 6174 of {\em Lecture Notes in Computer Science},
  pages 360--364. Springer, 2010.

\bibitem{CobleighGP03}
J.~M. Cobleigh, D.~Giannakopoulou, and C.~S. Pasareanu.
\newblock Learning assumptions for compositional verification.
\newblock In {\em TACAS}, volume 2619 of {\em Lecture Notes in Computer
  Science}, pages 331--346. Springer, 2003.

\bibitem{Colcombet2011}
T.~Colcombet, C.~Ley, and G.~Puppis.
\newblock On the use of guards for logics with data.
\newblock In {\em Proceedings of MFCS'11}, volume 6907 of {\em Lecture Notes in
  Computer Science}, pages 243--255. Springer Berlin / Heidelberg, 2011.

\bibitem{Hig10}
C.~de~la Higuera.
\newblock {\em Grammatical Inference. Learning Automata and Grammars}.
\newblock Cambridge University Press, 2010.

\bibitem{DL-tocl08}
S.~Demri and R.~Lazi{\'c}.
\newblock {LTL} with the freeze quantifier and register automata.
\newblock {\em ACM Transactions on Computational Logic}, 10(3), 2009.

\bibitem{DBLP:conf/sigsoft/GiannakopoulouM03}
D.~Giannakopoulou and J.~Magee.
\newblock Fluent model checking for event-based systems.
\newblock In {\em ESEC / SIGSOFT FSE}, pages 257--266. ACM, 2003.

\bibitem{DBLP:conf/lata/GrumbergKS10}
O.~Grumberg, O.~Kupferman, and S.~Sheinvald.
\newblock Variable automata over infinite alphabets.
\newblock In {\em LATA}, volume 6031 of {\em Lecture Notes in Computer
  Science}, pages 561--572. Springer, 2010.

\bibitem{DBLP:conf/atva/GrumbergKS13}
O.~Grumberg, O.~Kupferman, and S.~Sheinvald.
\newblock An automata-theoretic approach to reasoning about parameterized
  systems and specifications.
\newblock In {\em ATVA}, volume 8172 of {\em Lecture Notes in Computer
  Science}, pages 397--411. Springer, 2013.

\bibitem{HV-infinity04}
P.~Habermehl and T.~Vojnar.
\newblock Regular model checking using inference of regular languages.
\newblock {\em Electronic Notes in Theoretical Computer Science},
  138(3):21--36, 2005.

\bibitem{HowarSJC12}
F.~Howar, B.~Steffen, B.~Jonsson, and S.~Cassel.
\newblock Inferring canonical register automata.
\newblock In {\em VMCAI}, volume 7148 of {\em Lecture Notes in Computer
  Science}, pages 251--266. Springer, 2012.

\bibitem{DBLP:conf/sfm/Jonsson11}
B.~Jonsson.
\newblock Learning of automata models extended with data.
\newblock In {\em SFM}, volume 6659 of {\em Lecture Notes in Computer Science},
  pages 327--349. Springer, 2011.

\bibitem{Kaminski1994}
M.~Kaminski and N.~Francez.
\newblock Finite-memory automata.
\newblock {\em Theoretical Computer Science}, 134(2):329--363, 1994.

\bibitem{KamTan06}
M.~Kaminski and T.~Tan.
\newblock Regular expressions for languages over infinite alphabets.
\newblock {\em Fundamenta Informaticae}, 69(3):301--318, 2006.

\bibitem{KaminskiZ10}
M.~Kaminski and D.~Zeitlin.
\newblock Finite-memory automata with non-deterministic reassignment.
\newblock {\em International Journal of Foundations of Computer Science},
  21(5):741--760, 2010.

\bibitem{DBLP:conf/ccs/KurtzKW07}
K.~O. K{\"u}rtz, R.~K{\"u}sters, and T.~Wilke.
\newblock Selecting theories and nonce generation for recursive protocols.
\newblock In P.~Ning, V.~Atluri, V.~D. Gligor, and H.~Mantel, editors, {\em
  FMSE}, pages 61--70. ACM, 2007.

\bibitem{KST2012}
A.~Kurz, T.~Suzuki, and E.~Tuosto.
\newblock On nominal regular languages with binders.
\newblock In L.~Birkedal, editor, {\em {P}roceedings of {FoSSaCS}'12}, volume
  7213 of {\em Lecture Notes in Computer Science}, pages 255--269. Springer,
  2012.

\bibitem{DBLP:conf/fmco/Leucker07}
M.~Leucker.
\newblock Learning meets verification.
\newblock In {\em FMCO}, volume 4709 of {\em Lecture Notes in Computer
  Science}, pages 127--151. Springer, 2007.

\bibitem{MRSL07}
T.~Margaria, H.~Raffelt, B.~Steffen, and M.~Leucker.
\newblock The {LearnLib} in {FMICS-jETI}.
\newblock In {\em ICECCS}, pages 340--352. IEEE Computer Society Press, 2007.

\bibitem{MPW92}
R.~Milner, J.~Parrow, and D.~Walker.
\newblock A calculus of mobile processes, {P}arts {I} and {II}.
\newblock {\em Information and Computation}, 100:1--77, Sept. 1992.

\bibitem{Neven2004}
F.~Neven, {\relax Th}.~Schwentick, and V.~Vianu.
\newblock Finite state machines for strings over infinite alphabets.
\newblock {\em ACM Transactions on Computational Logic}, 5(3):403--435, 2004.

\bibitem{RiSh:inference}
R.~Rivest and R.~Schapire.
\newblock Inference of finite automata using homing sequences.
\newblock {\em Information and Computation}, 103:299--347, 1993.

\bibitem{SakIke00}
H.~Sakamoto and D.~Ikeda.
\newblock Intractability of decision problems for finite-memory automata.
\newblock {\em Theoretical Computer Science}, 231:297--308, 2000.

\bibitem{Segoufin06}
L.~Segoufin.
\newblock Automata and logics for words and trees over an infinite alphabet.
\newblock In Z.~{\'E}sik, editor, {\em CSL 2006}, volume 4207 of {\em LNCS},
  pages 41--57. Springer, 2006.

\bibitem{DBLP:conf/popl/Tzevelekos11}
N.~Tzevelekos.
\newblock Fresh-register automata.
\newblock In T.~Ball and M.~Sagiv, editors, {\em POPL}, pages 295--306. ACM,
  2011.

\end{thebibliography}

\end{document}
