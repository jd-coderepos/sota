

\documentclass{article}

\usepackage{hhline}

\usepackage{microtype}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage{amsmath}
\usepackage{mathtools}
\usepackage{adjustbox}
\DeclareMathOperator*{\argmax}{arg\, max}
\newcommand{\minus}{\scalebox{0.75}[1.0]{}}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2020}

\icmltitlerunning{Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search}

\begin{document}

\twocolumn[
\icmltitle{Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jaehyeon Kim}{kakao}
\icmlauthor{Sungwon Kim}{snu,inmc}
\icmlauthor{Jungil Kong}{kakao}
\icmlauthor{Sungroh Yoon}{snu,inmc}
\end{icmlauthorlist}

\icmlaffiliation{kakao}{Kakao Enterprise, Seongnam-si, Gyeonggi-do, Korea}
\icmlaffiliation{snu}{Electrical and Computer Engineering, Seoul National University, Seoul, Korea}
\icmlaffiliation{inmc}{INMC, Institute of Engineering Research, Seoul National University, Seoul, Korea}

\icmlcorrespondingauthor{Sungroh Yoon}{sryoon@snu.ac.kr}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}

Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantages, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. We introduce Monotonic Alignment Search (MAS), an internal alignment search algorithm for training Glow-TTS. By leveraging the properties of flows, MAS searches for the most probable monotonic alignment between text and the latent representation of speech. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive TTS model, Tacotron 2, at synthesis with comparable speech quality, requiring only 1.5 seconds to synthesize one minute of speech in end-to-end. We further show that our model can be easily extended to a multi-speaker setting. Our demo page and code are available at public.\footnote{Demo page at \href{https://bit.ly/2LSGPXv}{https://bit.ly/2LSGPXv}.}\footnote{Code at \href{https://bit.ly/2LD7O9a}{https://bit.ly/2LD7O9a}.}

\end{abstract}

\section{Introduction}
Text-to-Speech (TTS) is the task to generate speech from text, and deep-learning-based TTS models have succeeded in producing natural speech indistinguishable from human speech.
Among neural TTS models, autoregressive models such as Tacotron 2 \cite{shen2018natural} or Transformer TTS \cite{li2019neural}, show the state-of-the-art performance. Based on these autoregressive models, there have been many advances in generating diverse speech in terms of modelling different speaking styles or various prosodies \cite{wang2018style,skerry2018towards,jia2018transfer}.

Despite the high quality of autoregressive TTS models, there are a few difficulties in deploying end-to-end autoregressive models directly in real-time services. As the synthesizing time of the models grows linearly with the output length, undesirable delay caused by generating long speech can be propagated to the multiple pipelines of TTS systems without designing sophisticated frameworks \cite{ma2019incremental}. 
In addition, most of the autoregressive models show lack of robustness in some cases \cite{ren2019fastspeech}. For example, when the input text includes the repeated words, autoregressive TTS models often produce serious attention errors. 

To overcome such limitations of the autoregressive TTS models, parallel TTS models such as FastSpeech have been proposed. These models can synthesize mel-spectrogram significantly faster than the autoregressive TTS models. In addition to the fast sampling, FastSpeech reduces the failure cases for the extremely hard sentences by enforcing its alignment monotonic.


However, these strengths of parallel TTS models come from the well-aligned attention map between text and speech, which is extracted from their external aligner. Recently proposed parallel models address these challenges by extracting attention maps from the pre-trained autoregressive models. Therefore, the performance of the parallel TTS models critically depends on that of the autoregressive TTS models. Furthermore, since the parallel TTS models assume this alignment to be given during training, they cannot be trained without the external aligners.


In this work, our goal is to eliminate the necessity of any external aligner from the training procedure of parallel TTS models. Here, we propose Glow-TTS, a flow-based generative model for parallel TTS that can internally learn its own alignment. Glow-TTS is directly trained to maximize the log-likelihood of speech given text, and its sampling process is totally parallel due to the properties of the generative flow. In order to eliminate any dependency on other networks, we introduce Monotonic Alignment Search (MAS), a novel method to search for the most probable monotonic alignment with only text and latent representation of speech. This internal alignment search algorithm simplifies the entire training procedure of our parallel TTS models so that it requires only 3 days for training on two GPUs.


Without any external aligner, our parallel TTS model can generate mel-spectrograms 15.7 times faster than the autoregressive TTS model, Tacotron 2, while maintaining the comparable performance. Glow-TTS also provides diverse speech synthesis, in contrast to other TTS models, which have their stochasticities only in dropout operations. We can control some properties of synthesized samples from Glow-TTS by altering the latent variable of normalizing flows. We further show that our model can be extended to a multi-speaker setting with only a few modifications.


\section{Related Work}


\textbf{Text-to-Speech (TTS) Models.} TTS models are a family of generative models that synthesize speech from text. One subclass of TTS models, including Tacotron 2 \cite{shen2018natural}, Deep Voice 3 \cite{ping2017deep} and Transformer TTS \cite{li2019neural}, generates a mel-spectrogram, a compressed representation of audio, from text. They produce natural speech comparable to the human voice.
Another subclass, also known as vocoder, has been developed to transform mel-spectrograms into high-fidelity audio waveform \cite{shen2018natural, van2016wavenet} with fast synthesis speed \cite{kalchbrenner2018efficient, oord2017parallel, prenger2019waveglow}. It has also been studied to enhance expressiveness of TTS models. Auxiliary embedding methods have been proposed to generate diverse speech by controlling some factors such as intonation and rhythm \cite{skerry2018towards, wang2018style}, and some studies have aimed at synthesizing speech in the voices of various speakers \cite{jia2018transfer, gibiansky2017deep}.


\textbf{Parallel Decoding Models.} There are challenges for sequence-to-sequence (seq2seq) models to decode output sequence in parallel. One of the challenges is the lack of information about how many output tokens have to be generated from each input token. For example, most TTS datasets do not contain the duration value for each phoneme in speech. Another challenge is the difficulty of modelling conditional dependency between output tokens in parallel, which makes parallel models struggle to match the performance of autoregressive models. 

Parallel decoding models have been proposed to address these challenges in various domains. In natural language processing, \citet{gu2017non} tackle the challenges by using an external aligner to segment output tokens according to each input token. Furthermore, they use sequence-level knowledge distillation \cite{kim2016sequence} to reduce the performance gap between the autoregressive teacher network and the parallel model. In TTS, \citet{ren2019fastspeech} similarly extract alignment information from the autoregressive TTS model, Transformer TTS, as well as utilizing sequence-level knowledge distillation for performance boosting. Another parallel TTS model, ParaNet \cite{peng2019parallel}, utilizes soft attention map from its teacher network. Our Glow-TTS differs from all the previous approaches, which rely on the autoregressive teacher network or the external aligner. 

\textbf{Flow-based Generative Models.} Flow-based generative models have received a lot of attention due to their advantages \cite{hoogeboom2019emerging,durkan2019neural,serra2019blow}. They can estimate the exact likelihood of the data by applying some invertible transformations. Generative flows are simply trained to maximize this likelihood. In addition to efficient density estimation, the transformations proposed in \cite{dinh2014nice,dinh2016density,kingma2018glow} guarantee fast and efficient sampling. \citet{prenger2019waveglow} and \citet{kimflowavenet} introduce these transformations for raw audio speech synthesis to overcome slow sampling speed of an autoregressive vocoder, WaveNet \cite{van2016wavenet}. Their proposed models, WaveGlow and FloWaveNet, both synthesize raw audio significantly faster than WaveNet. By applying these transformations, Glow-TTS can synthesize a mel-spectrogram given text in parallel. 

In parallel with our work, Flowtron \cite{valle2020flowtron} and Flow-TTS \cite{miao2020flow} are proposed. Flowtron is a flow-based TTS model which exhibits diverse applications of the model such as style transfer and control of speech variation by using flow properties. The main difference from our work is that Flowtron uses autoregressive flows as its objective is not about fast speech synthesis. Flow-TTS is a parallel TTS synthesizer using flow-based decoder and multi-head soft attention with positional encoding, in contrast with our work, which predicts hard monotonic alignments by estimating duration of each input token.

\section{Glow-TTS}
\begin{figure*}
    \centering
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[width=7cm]{Figure1_a}
        \caption{An abstract diagram of training procedure.}
        \label{fig1:figure_a}
    \end{subfigure}\hfill \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[width=6.7cm]{Figure1_b.pdf}
        \caption{An abstract diagram of inference procedure.}
        \label{fig1:figure_b}
    \end{subfigure}
    \caption{Training and inference procedures of Glow-TTS.}
    \label{fig1}
\end{figure*}

In this section, we describe a new type of parallel TTS model, Glow-TTS, which is directly trained to maximize likelihood without any other networks. Glow-TTS achieves the parallel sampling via inverse transformation of the generative flow. In Section \ref{3.1}, we formulate the training and inference procedures of our model and these procedures are illustrated in Figure \ref{fig1}. We present our novel alignment search algorithm in Section \ref{3.2}, which removes the necessity of other networks from training Glow-TTS, and all components of Glow-TTS, text encoder, duration predictor, and flow-based decoder, are covered in Section \ref{3.3}.

\subsection{Glow-TTS}

\label{3.1}
In general, normalizing flows for conditional density estimation incorporates a given condition into each flow and maps data into a known prior with the conditional flows. However, Glow-TTS incorporates the text condition into the statistics of the prior distribution rather than into each flow.  

Given a mel-spectrogram , Glow-TTS transforms the mel-spectrogram  into the latent variable  with the flow-based decoder  without any text information, and the latent variable  follows some isotropic Gaussian distribution . Then, text encoder  maps the text condition  into the high-level representation of text , and projects  into the statistics,  and , of Gaussian distribution. Thus, each token of text sequence has its corresponding distribution, and each frame of the latent variable  follows one of these distributions predicted by the text encoder.

We define this correspondence between the latent variable and the distribution as an alignment . Thus, if the latent variable  follows the predicted distribution of -th text token , then we define   . This alignment can be interpreted as a hard attention in sequence-to-sequence modelling. Thus, given an alignment , we can calculate the exact log-likelihood of the data as follows: 




Since text and speech are monotonically aligned, we assume the alignment  to be monotonically increasing. 


\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figure2_a.pdf}
        \caption{An example of alignment, which cover all tokens of text representation .}
        \label{fig2:figure_a}
    \end{subfigure}\hspace*{\fill}
    \begin{subfigure}[b]{.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figure2_b.pdf}
        \caption{The process of calculating the maximum log-likelihood .}
        \label{fig2:figure_b}
    \end{subfigure}\hspace*{\fill}
    \begin{subfigure}[b]{.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figure2_c.pdf}
        \caption{The process of searching for the most probable alignment .}
        \label{fig2:figure_c}
    \end{subfigure}
    \caption{Illustrations for Monotonic Alignment Search}
    \label{fig2}
\end{figure*}

Our goal is to find the parameters  and the alignment  that maximize the log-likelihood, as in Equation \ref{global}. However, it is computationally intractable to find the global solution of Equation \ref{global}. To tackle the intractability, we reduce the search space of the parameters  and the monotonic alignments  by modifying our objective from Equation \ref{global} to Equation \ref{local}.





Thus, training Glow-TTS can be decomposed into two subsequent problems:  to search for the most probable alignment  with the latent representation  and the predicted distributions given the current parameter  as in Equation \ref{argmax} and \ref{argmax2}, and  to update the current parameters  to maximize log probability  given . In practice, we handle these two problems using an iterative approach. At each training step, we first find , and then update parameter  using gradient descent. Our modified objective does not guarantee the global solution of Equation \ref{global}, but it still provides a good lower bound of the global solution.


To solve the alignment search problem , we introduce a novel alignment search algorithm, Monotonic Alignment Search (MAS), described in Section \ref{3.2}. Note that Glow-TTS can also be trained as FastSpeech by maximizing , where  is extracted from an external aligner, but MAS totally removes the necessity of the external aligner from our training procedure.

In addition to maximizing the log-likelihood, we also train a duration predictor , which predicts how many frames of mel-spectrogram are aligned to each text token. To train the duration predictor, a duration label is needed for each text token. We simply extract this label from the most probable alignment , the output of the MAS, though MAS would provide a poor alignment at the beginning of training.

From the alignment , we can count how many speech frames are aligned to each text as in Equation \ref{count} and use the number of frames  as the duration label for the j-th input token. Given a high-level representation of text , our duration predictor  is learned with the means squared error (MSE) loss as in Equation \ref{duration_loss}. As with FastSpeech \cite{ren2019fastspeech}, we train  with the duration  in the logarithmic domain. We also apply stop gradient operator , which removes the gradient of input in backward pass \cite{oord2017neural}, to the input of the duration predictor to avoid affecting the maximum likelihood objective. Therefore, our final objective function is Equation \ref{final_obj}.







During inference, as shown in Figure \ref{fig1:figure_b}, Glow-TTS predicts the statistics of prior distributions and the duration of each text token with the text encoder  and the duration predictor . We round up these predicted durations to integer and duplicate each distribution by the corresponding duration. This extended distribution is the prior of Glow-TTS during inference. Then, Glow-TTS samples the latent variable  from this prior and synthesizes a mel-spectrogram in parallel by applying inverse transformation of the decoder  to the latent variable .


\subsection{Monotonic Alignment Search}
\label{3.2}

\begin{algorithm}[tb]
   \caption{Monotonic Alignment Search}
   \label{alg_mas}
\begin{algorithmic}
   \STATE {\bfseries Input:} latent representation , the statistics of prior distribution  , , the mel-spectrogram length , the text length 
   \STATE {\bfseries Output:} monotonic alignment 
   \STATE Initialize , a cache to store the maximum log-likelihood calculations
   \STATE Compute the first raw  
   \FOR{ {\bfseries to} }
   \FOR{ {\bfseries to} }
   \STATE 
   \ENDFOR
   \ENDFOR
   \STATE Initialize 
   \FOR{ {\bfseries to} }
   \STATE 
   \ENDFOR
\end{algorithmic}
\end{algorithm}

As mentioned in Section \ref{3.1}, MAS searches for the most probable alignment  between the latent variable  and the text representation . As there are numerous alignments to explore, we restrict the properties of them based on our assumption. We suppose that text and speech are monotonically aligned and all tokens of text are covered in the speech. Therefore, we only concern alignments that are monotonic and does not skip any element of . Figure \ref{fig2:figure_a} shows one of the possible alignments that we concern.

We present our alignment search algorithm in Algorithm \ref{alg_mas}. We first derive recursive solution over partial alignments up to -th element of  and -th element of , then find the most probable alignment  by using the derivation.

Let  be the maximum log-likelihood where  and  are partially given up to -th and -th elements, respectively. As  should be monotonically covered by our assumption, the alignment  is aligned to  and  is aligned to  or . This means that  can be calculated based on the possible partial alignments: 


This process is illustrated in Figure \ref{fig2:figure_b}. We iteratively calculate all the values of  up to . Note that  is the maximum log-likelihood of all possible monotonic alignments. 

Similarly, the most probable alignment  can be retrieved by determining which  value is greater in the recurrence relation, Equation \ref{masmax}. Thus, we backtrack from the end of the alignment, , to find all the values of  (Figure \ref{fig2:figure_c}).

The time complexity of our algorithm is . Even though our method is difficult to parallelize, it runs fast on CPU without need of GPU execution. In our experiments, it spends less than 20ms for each iteration, which amounts to less than 2\% of the total training time. Furthermore, we do not need to use MAS during inference, as there is a duration predictor to estimate the duration of each input token.

\subsection{Model Architecture}
\label{3.3}
Overall architecture of Glow-TTS is visualized in Appendix \hyperref[appa1]{A.1}. We also list model configurations in Appendix \hyperref[appa2]{A.2}.
\paragraph{Decoder.} The core part of Glow-TTS is the flow-based decoder. In training, we need to efficiently transform a mel-spectogram into the latent representation for maximum likelihood estimation and our internal alignment search. During inference, it is necessary to invert the prior distribution into the mel-spectrogram distribution efficiently for parallel decoding. Therefore, our decoder is composed of a family of flows that can perform forward and inverse transformation in parallel. Affine coupling layer \cite{dinh2014nice,dinh2016density}, invertible 1x1 convolution, and activation normalization \cite{kingma2018glow} are included in them. Specifically, our decoder is a stack of multiple blocks, each of which consists of activation normalization, invertible 1x1 convolution, and affine coupling layer. We follow affine coupling layer architecture of WaveGlow \cite{prenger2019waveglow}, except that we do not use the local conditioning \cite{van2016wavenet}.

For computational efficiency, we split 80 channel mel-spectrogram frames into two halves along the time dimension and group them into one 160 channel feature map, before the decoder operation. We also modify 1x1 convolution to reduce time-consuming calculation of the log determinant of Jacobian of it. Before every 1x1 convolution, we split the feature map into 40 groups along channel dimension and perform 1x1 convolution to them separately.

\paragraph{Encoder.}We follow the encoder structure of Transformer TTS \cite{li2019neural} with two slight modifications. We remove the positional encoding and add relative position representations \cite{shaw2018self} into self-attention modules instead. We also add residual connection to encoder pre-net. To estimate the statistics of the prior distribution, we just append a linear layer at the end of the encoder. Duration predictor is composed of two convolutional layers with ReLU activation, layer normalization and dropout followed by a projection layer. The architecture and configuration of our duration predictor is the same as that of FastSpeech \cite{ren2019fastspeech}.


\section{Experiments}

To evaluate our proposed methods, we conduct experiments on two different datasets. For single speaker TTS, we train our model on the widely used single female speaker dataset LJSpeech \cite{ljspeech17}, which consists of 13100 short audio clips with a total duration of approximately 24 hours. We randomly split the dataset into training set (12500 samples), validation set (100 samples), and test set (500 samples). For multi-speaker TTS, we use the train-clean-100 subset of the LibriTTS corpus \cite{zen2019libritts}, which consists of about 54 hours audio recording of 247 speakers. We first trim the beginning and ending silence of all the audio clips in the data, then filter out all data of which text lengths are larger than 190, and split it into three datasets for training (29181 samples), validation (88 samples), and test (442 samples). Additionally, we collect out-of-distribution text data for robustness test. Similar to \cite{battenberg2019location}, we extract 227 utterances from the first two chapters of the book, \textit{Harry Potter and the Philosopher's Stone}.

To compare Glow-TTS with an autoregressive TTS model, we set our baseline as Tacotron 2, which is the most widely used, and follows the configuration of \cite{repo2018tacotron}. For all the experiments, we choose phoneme as input text token. We train both Glow-TTS and Tacotron 2 conditioned on the phoneme sequence. We initialize all the parameters except the text embedding layer of our baseline as same as those of the pre-trained baseline\footnote{\href{https://github.com/NVIDIA/tacotron2}{https://github.com/NVIDIA/tacotron2}}. We follow the configuration of mel-spectrogram of \cite{repo2019WaveGlow} for training, and all the generated mel-spectrograms from both models are transformed to raw waveforms by using the pre-trained vocoder, WaveGlow\footnote{\href{https://github.com/NVIDIA/waveglow}{https://github.com/NVIDIA/waveglow}}.
 
During training, we simply set the variance  of the learned prior to be a constant 1. Glow-TTS was trained for 240K iterations using the Adam Optimizer with the same learning rate schedule in \cite{vaswani2017attention}. This required only 3 days with mixed precision training on 2 NVIDIA V100 GPUs.

To train Glow-TTS for a multi-speaker setting, we add the speaker embedding and increase all hidden dimensions of text encoder and the decoder. We condition our model on a speaker embedding by applying global conditioning \cite{van2016wavenet} to all affine coupling layers of the decoder. The rest of the settings are the same as for the single speaker setting. In this case, we trained our Glow-TTS for 480K iterations for convergence.
\begin{table}[t]
\caption{The Mean Opinion Score (MOS) of single speaker TTS models with 95 confidence intervals.}
\label{singlemos}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{\begin{tabular}{lc}
\toprule
Method & 9-scale MOS\\
\midrule
GT & 4.54  0.06 \\
\addlinespace[0.1cm]
GT (Mel + WaveGlow) & 4.19  0.07 \\
\addlinespace[0.1cm]
Tacotron2 (Mel + WaveGlow) & 3.88  0.08 \\
\addlinespace[0.1cm]
Glow-TTS (, Mel + WaveGlow) & 4.01  0.08 \\
\addlinespace[0.1cm]
Glow-TTS (, Mel + WaveGlow) & 3.96  0.08 \\
\addlinespace[0.1cm]
Glow-TTS (, Mel + WaveGlow) & 3.97  0.08 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{speed.pdf}}
\caption{The inference time comparison for Tacotron 2 and Glow-TTS (yellow: Tacotron2, blue: Glow-TTS).}
\label{speed}
\end{center}
\end{figure}
\section{Results}
\subsection{Audio Quality}
\label{audioquality}

We measure the mean opinion score (MOS) to compare the quality of all the audios including ground truth (GT), and our synthesized samples via Amazon Mechanical Turk (AMT); the results are shown in Table \ref{singlemos}. Fifty sentences are randomly chosen from our test dataset and used for the evaluation. The quality of the raw audio converted from ground-truth mel-spectrogram (4.190.07) is the upper limit of TTS models. We measure the performance of Glow-TTS for various standard deviations (i.e., temperatures) of the prior distribution; the temperature of 0.333 shows the best performance. For any temperature, our Glow-TTS shows comparable performance to the strong autoregressive baseline, Tacotron 2.

\subsection{Sampling Speed}

We use the test dataset, which has 500 sentences, to measure the sampling speed of TTS models. Figure \ref{speed} demonstrates that the inference time of our parallel TTS model is almost constant at 40ms, regardless of the length, whereas that of Tacotron 2 linearly increases with the length due to the sequential sampling. Based on the inference time for average length of speech, Glow-TTS synthesizes mel-spectrogram 15.7 times faster than Tacotron 2.

We also measure the total inference time for synthesizing one-minute speech from the text in an end-to-end manner. For this measurement, Glow-TTS synthesizes a mel-spectrogram that is longer than 5000 frames, and WaveGlow converts the mel-spectrogram into the raw waveform of one-minute speech. The total inference time is only 1.5 seconds to synthesize one-minute speech\footnote{We generated a speech sample from our abstract paragraph, which we mention as the one-minute speech. Visit our demo page to listen to it.}, and the inference time of Glow-TTS and WaveGlow accounts for 4 and 96 of the total inference time, respectively. That is, the inference time of the Glow-TTS still takes only 55ms to synthesize a very long mel-spectrogram and is negligible compared to that of vocoder, WaveGlow.

\subsection{Diversity}
\label{diversity}

For the sample diversity, most of previous TTS models such as Tacotron 2 or FastSpeech only rely on dropout at inference time. However, as Glow-TTS is a flow-based generative model, it can synthesize a variety of speech given an input text. This is because each latent representation  sampled from an input text is converted to a different mel-spectrogram . 

We can express this latent representation  with a noise  sampled from the standard Normal distribution, and the mean  and temperature  of the prior distribution as follows:



Thus, we can synthesize diverse speech by varying the  and the temperature T. Figure \ref{fig4:a} demonstrates that by changing the temperature for the same , we can control the pitch of speech while maintaining the trend of the fundamental frequency (F0) contour. In addition, in Figure \ref{fig4:b}, we show that Glow-TTS can generate various speeches which has a different F0 contour shapes by only altering .

\begin{figure}
\begin{subfigure}{\linewidth}
\includegraphics[width=\textwidth]{diversity_T.pdf}
\caption{Pitch tracks for the generated speech samples from same sentence with different temperatures  and same gaussian noise .} \label{fig4:a}
\end{subfigure}
\hspace*{\fill} 
\begin{subfigure}{\linewidth}
\includegraphics[width=\textwidth]{diversity_E.pdf}
\caption{Pitch tracks for the generated speech samples from same sentence with different gaussian noise  and same temperature .} \label{fig4:b}
\end{subfigure}
\caption{The fundamental frequency (F0) contours of synthesized speech samples from Glow-TTS trained on the LJ dataset.} \label{fig:diversity}
\end{figure}

\subsection{Length Robustness and Controllability}
\label{robust_and_control}
\textbf{Length Robustness.}
To investigate the ability of TTS models to generalize to long texts, we synthesize speech from utterances extracted from the book, \textit{Harry Potter and the Philosopher's Stone}. The maximum length of the collected data exceeds 800. It is much greater than the maximum length of input characters in LJ dataset, which is less than 200.

We measure the character error rate (CER) of synthesized samples from the out-of-distribution utterances via Google speech recognition API, Google Cloud Speech-To-Text\footnote{\href{https://cloud.google.com/speech-to-text}{https://cloud.google.com/speech-to-text}}. Figure \ref{robust} shows a similar result to \cite{battenberg2019location}. The CER of Tacotron 2 starts to grow when the length of input characters exceeds about 260. On the other hand, even though our model has not seen such long text during training, it shows robustness to input length.

In addition to the results of length robustness, we also analysis attention errors on specific sentences. The results are shown in Appendix \hyperref[appb1]{B.1}.

\textbf{Length Controllability.}
As Glow-TTS shares the same duration predictor architecture with FastSpeech, our model is also able to control the speaking rate of the output speech. We multiply a positive scalar value across the predicted duration from the duration predictor. We visualize the result in Figure \ref{lengthcontrol}. We multiply different values to the predicted duration as: 1.25, 1.0, 0.75, and 0.5 respectively. As shown in Figure \ref{lengthcontrol}, our model generates mel-spectrograms of lengths. Despite our model have not seen such extremely fast or slow speech during training, the model can control the velocity of the speech without quality degradation.

\subsection{Multi-speaker TTS}

\begin{figure}
\begin{subfigure}{\linewidth}
\includegraphics[width=\textwidth]{length_robust.pdf}
\caption{Robustness to the length of input utterance (yellow: Tacotron2, blue: Glow-TTS).} \label{robust}
\end{subfigure}
\hspace*{\fill} 
\begin{subfigure}{\linewidth}
\includegraphics[width=\textwidth]{length_control.pdf}
\caption{Mel-spectrograms with different speaking duration (x1.25, x1.0, x0.75, and x0.5 from top to bottom, respectively).} \label{lengthcontrol}
\end{subfigure}
\caption{Length Robustness and Controllability} \label{length}
\end{figure}



\begin{figure*}
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=\linewidth]{fig_multi.pdf}
\caption{Pitch tracks for the generated speech samples from same sentence with different speaker identities.} \label{fig_multi}
\end{subfigure}
\hspace*{\fill}
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=\linewidth]{fig_vc.pdf}
\caption{Pitch tracks for the voice conversion samples with different speaker identities.} \label{fig_vc}
\end{subfigure}
\caption{The fundamental frequency (F0) contours of synthesized speech samples from Glow-TTS trained on the LibriTTS dataset.} \label{fig:multi}
\end{figure*}

\begin{table}[t]
\caption{The Mean Opinion Score (MOS) of a multi-speaker TTS with 95 confidence intervals.}
\label{multimos}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{\begin{tabular}{lc}
\toprule
Method & 9-scale MOS\\
\midrule
GT & 4.29  0.06 \\
\addlinespace[0.1cm]
GT (Mel + WaveGlow) & 4.06  0.07 \\
\addlinespace[0.1cm]
Glow-TTS (, Mel + WaveGlow) & 3.40  0.09 \\
\addlinespace[0.1cm]
Glow-TTS (, Mel + WaveGlow) & 3.54  0.09 \\
\addlinespace[0.1cm]
Glow-TTS (, Mel + WaveGlow) & 3.52  0.09 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\textbf{Audio Quality.} We measure MOS similar to Section \ref{audioquality}. Fifty sentences are randomly chosen from our test dataset for evaluation. We compare the audio quality with two different settings of GT: ground truth audios and synthesized audios from ground truth mel-spectrograms. The results are presented in Table \ref{multimos}. The quality of the raw audio converted from ground-truth mel-spectrogram (4.060.07) is the upper limit of TTS models. Our model with the best configuration achieves about 3.5 MOS, which demonstrates Glow-TTS can model the diverse speaker styles.



\textbf{Speaker Dependent Duration.}
Figure \ref{fig_multi} shows the mel-spectrograms of generated speech from a same sentence with different speaker identities. As the only different input for the duration predictor is speaker embedding, the result demonstrates that our model predicts the duration of each input tokens differently with respect to the speaker identity.

\textbf{Voice Conversion.}
As we do not provide speaker identity into the encoder, the prior distribution is forced to be independent from speaker identity. In other words, Glow-TTS learns to disentangle the latent representation  and the speaker identity. To investigate the degree of disentanglement, we transform a ground truth mel-spectrogram into the latent reprersentation with correct speaker identity, then invert the transformation with different speaker identities. The result is presented in Figure \ref{fig_vc}. It shows converted speeches maintain the similar trend of the fundamental frequency, but with diverse pitch.


\section{Conclusion}

We propose Glow-TTS, a new type of parallel TTS model, which provides fast and high quality speech synthesis. Glow-TTS is a flow-based generative model that is directly trained with maximum likelihood estimation and generates a mel-spectrogram given text in parallel. By introducing our novel alignment search algorithm, Monotonic Alignment Search (MAS), we simplify the whole training procedure of our parallel TTS model so that it requires only 3 days to train. 

In addition to the simple training procedure, we show that Glow-TTS synthesizes mel-spectrograms 15.7x faster than the autoregressive baseline, Tacotron 2, while showing comparable performance. We also demonstrate additional advantages of Glow-TTS, such as controlling the speaking rate or the pitch of synthesized speech, robustness to long utterances, and extensibility to a multi-speaker setting. Thanks to these advantages, we present Glow-TTS as an alternative to existing TTS models.


\nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2020}

\onecolumn
















\section*{Appendix A}
\label{appa}
\subsection*{A.1. Details of Model Architecture}
\label{appa1}
The detailed encoder architecture is depicted in Figure \ref{fig_app_1}.
Some implementation details that we use in the decoder, and the decoder architecture are depicted in Figure \ref{fig_app_2}.
\newline
\newline
\newline
\begin{figure}[h]
    \centering
    \includegraphics[width=.49\linewidth]{Figure_app_1.pdf}
    \caption{The encoder architecture of Glow-TTS. The encoder gets text sequence and processes it through encoder pre-net and Transformer encoder. Then, the last projection layer and the duration predictor of the encoder use the hidden representation  to predict statistics of prior distribution and duration, respectively.}
    \label{fig_app_1}
\end{figure}
\newline
\newline
\begin{figure}[h]
    \centering
    \begin{subfigure}{.31\textwidth}
        \centering
        \includegraphics[width=1.\linewidth]{Figure_app_2a.pdf}
        \caption{The decoder architecture of Glow-TTS. The decoder gets mel-spectrogram and squeezes it. The, the decoder processes it through a number of flow blocks. Each flow block contains activation normalization layer, affine coupling layer, and invertible 1x1 convolution layer. The decoder reshapes the output to make equal to the input size.}
        \label{fig_app_2:figure_a}
    \end{subfigure}\hfill \begin{subfigure}{.31\textwidth}
        \centering
        \includegraphics[width=1.\linewidth]{Figure_app_2b.pdf}
        \caption{An illustration of  and  operations. When squeezing, the channel size doubles up and the number of time steps becomes a half. If the number of time steps is odd, we simply ignore the last element of mel-spectrogram sequence. It corresponds to about 11ms audio, which makes no difference in quality.}
        \label{fig_app_2:figure_b}
    \end{subfigure}\hfill \begin{subfigure}{.31\textwidth}
        \centering
        \includegraphics[width=1.\linewidth]{Figure_app_2c.pdf}
        \caption{An illustration of our invertible 1x1 convolution. If input channel size is 8 and the number of groups is 2, we share a small 4x4 matrix as a kernel of the invertible 1x1 convolution layer. We split the input into each groups, and perform 1x1 convolution separately.}
        \label{fig_app_2:figure_c}
    \end{subfigure}
    \caption{The decoder architecture of Glow-TTS and the implementation details used in the decoder.}
    \label{fig_app_2}
\end{figure}

\newpage
\subsection*{A.2. Hyper-parameters}
\label{appa2}
Hyper-parameters of Glow-TTS are listed in Table \ref{tab_hyper}.
\newline
\begin{table}[h]
    \centering
    \begin{tabular}{l|c}
        \toprule
        \textbf{Hyper-parameter} & \textbf{Glow-TTS (LJ Dataset)}\\
        \hline
        Embedding Dimension & 192 \\
        \hline
        Pre-net Layers & 3 \\
        \hline
        Pre-net Hidden Dimension & 192 \\
        \hline
        Pre-net Kernel Size & 5 \\
        \hline
        Pre-net Dropout & 0.5 \\
        \hline
        Encoder Blocks & 6 \\
        \hline
        Encoder Multi-Head Attention Hidden Dimension & 192 \\
        \hline
        Encoder Multi-Head Attention Heads & 2 \\
        \hline
        Encoder Multi-Head Attention Maximum Relative Position & 4 \\
        \hline
        Encoder Conv Kernel Size & 3 \\
        \hline
        Encoder Conv Filter Size & 768 \\
        \hline
        Encoder Dropout & 0.1 \\
        \hline
        Duration Predictor Kernel Size & 3 \\
        \hline
        Duration Predictor Filter Size & 256 \\
        \hline
        Decoder Blocks & 12 \\
        \hline
        Decoder Activation Norm Data-dependent Initialization & True \\
        \hline
        Decoder Invertible 1x1 Conv Groups & 40 \\
        \hline
        Decoder Affine Coupling Dilation & 1 \\
        \hline
        Decoder Affine Coupling Layers & 4 \\
        \hline
        Decoder Affine Coupling Kernel Size & 5 \\
        \hline
        Decoder Affine Coupling Filter Size & 192 \\
        \hline
        Decoder Dropout & 0.05 \\
        \hhline{=|=}
        Total Number of Parameters & 28.6M \\
        \bottomrule
    \end{tabular}
    \caption{Hyper-parameters of Glow-TTS. The total number of parameters is lower than that of FastSpeech (30.1M).}
    \label{tab_hyper}
\end{table}

\section*{Appendix B}
\subsection*{B.1. Attention error analysis}
\label{appb1}
\begin{table}[h]
    \centering
    \begin{tabular}{lc|ccc|c}
        \toprule
        \textbf{Model} & \textbf{Attention Mask} & \textbf{Repeat} & \textbf{Mispronounce} & \textbf{Skip} & \textbf{Total}\\
        \hline
        DeepVoice 3 \cite{peng2019parallel} & X & 12 & 10 & 15 & 37 \\
        \hline
        DeepVoice 3 \cite{peng2019parallel} & O & 1 & 4 & 3 & 8 \\
        \hline
        ParaNet \cite{peng2019parallel} & X & 1 & 4 & 7 & 12 \\
        \hline
        ParaNet \cite{peng2019parallel} & O & 2 & 4 & 0 & 6 \\
        \hline
        Tacotron 2 & X & 0 & 2 & 1 & 3 \\
        \hline
        Glow-TTS & X & 0 & 3 & 1 & 4 \\
        \hline
    \end{tabular}
    \caption{Attention error counts for TTS models for 100 test sentences.}
    \label{tab_attn}
\end{table}
We measured attention alignment results using 100 test sentences used in ParaNet \cite{peng2019parallel}. The average length and maximum length of test sentences are 59.65 and 315, respectively. Results are shown in Table \ref{tab_attn}. The results of DeepVoice 3 and ParaNet are taken from \cite{peng2019parallel} and are not directly comparable due to the difference of grapheme-to-phoneme conversion tools.

Attention mask \cite{peng2019parallel} is a method of computing attention only over a fixed window around target position at inference time. When constraining attention to be monotonic by applying attention mask technique, models make fewer attention errors.

Tacotron 2, which uses location sensitive attention, also makes little attention errors. Though Glow-TTS perform slightly worse than Tacotron 2 on the test sentences, Glow-TTS does not lose its robustness to extremely long sentences while Tacotron 2 does as we show in Section \ref{robust_and_control}.

\section*{Appendix C}
\subsection*{C.1. Samples}
Our demo page that contains generated samples of Glow-TTS is at \href{https://bit.ly/2LSGPXv}{https://bit.ly/2LSGPXv}.
\subsection*{C.2. Code}
Our repository that contains model and training code is at \href{https://bit.ly/2LD7O9a}{https://bit.ly/2LD7O9a}.


\end{document}
