\documentclass{tlp}

\usepackage{amsmath} 
\usepackage{times}
\usepackage{url}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{paralist}
\usepackage{ifpdf}
\usepackage{appendix}
\usepackage{multirow}
 \usepackage{booktabs}


\newcommand{\naf}{\ensuremath{\mathrm{not}}}

\newcommand{\dlv}{\texttt{DLV}\xspace}
\newcommand{\asp}{\texttt{ASP}\xspace}
\newcommand{\tool}[1]{\texttt{#1}\xspace}

\newcommand{\preceqv}{\preceq^\mathsf{v}}
\newcommand{\camouv}{\tau^\mathsf{v}}
\newcommand{\preceqp}{\preceq^\mathsf{p}}
\newcommand{\camoup}{\tau^\mathsf{p}}
\newcommand{\camouc}{\tau^\mathsf{c}}
\newcommand{\camouo}{\tau^\mathsf{o}}

\newcommand{\Pol}{{\rm P}}
\newcommand{\PNP}{\Pol^{\rm NP}}
\newcommand{\NP}{\mbox{\rm NP}}
\newcommand{\NPNP}{\NP^{\rm NP}}
\newcommand{\CONP}{\mbox{\rm coNP}}
\newcommand{\coNP}{\mbox{\rm coNP}}
\newcommand{\SigmaP}[1]{{\Sigma}_{#1}^{P}}
\newcommand{\PiP}[1]{{\Pi}_{#1}^{P}}
\newcommand{\DeltaP}[1]{{\Delta}_{#1}^{P}}
\newcommand{\PSPACE}{{\rm PSPACE}}

\newcommand{\deq}{=_{\mathit{df}}}     
\renewcommand{\deq}{=}     

\def\AND     { \wedge                 }
\def\OR      { \vee                   }
\def\XOR     { \stackrel{.}{\vee}     }
\def\IMPLIES { \rightarrow            }
\def\IMPL    { \rightarrow            }
\def\IFF     { \leftrightarrow        }
\def\IFFdef  { \stackrel{\rm def}{\longleftrightarrow} }
\newcommand{\quantifier}{\mbox{\sf Q}}

\newcommand{\la}{\leftarrow}
\newcommand{\ra}{\rightarrow}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\R}{\rho}
\newcommand{\at}[1]{\mathit{At}(#1)}

\newcommand{\meta}{\odot}

\newcommand{\viz}[0]{viz.\ }
\newcommand{\wrt}[0]{with respect to}
\newcommand{\iec}[0]{i.e.,\ }
\newcommand{\Iec}[0]{I.e.,\ }
\newcommand{\egc}[0]{e.g.,\ }
\newcommand{\eg}[0]{e.g.\ }
\newcommand{\resp}[0]{resp.,\ }
\newcommand{\commadots}[0]{,\ldots ,}
\newcommand{\card}[1]{\vert #1 \vert}
\newcommand{\nop}[1]{}
\newcommand{\kato}[0]{\texttt{Kato}\xspace}

\newcommand{\head}[1]{\mathit{H}(#1)}
\newcommand{\body}[1]{\mathit{B}(#1)}
\newcommand{\posbody}[1]{\mathit{B^{+}}(#1)}
\newcommand{\negbody}[1]{\mathit{B^{-}}(#1)}
\renewcommand{\max}[1]{\mathrm{max}\,{#1}}
\newcommand{\subst}{\vartheta}
\newcommand{\len}[1]{\mathrm{len}({#1})}

\newcommand{\rules}{\mathcal{R}}
\newcommand{\identity}{\tau^{\mathsf{id}}}

\newcommand{\rulesim}[2]{\sigma({#1},{#2})}
\newcommand{\progsim}[3]{\mathcal{S}_{#3}({#1},{#2})}
\newcommand{\conf}[4]{\mathcal{C}_{{#1},{#2}}({#3},{#4})}

\newtheorem{definition}{Definition} 
\newtheorem{example}{Example} 

\usepackage{xspace}



\begin{document}
\label{firstpage}

\submitted{7 February 2010}
\revised{10 April 2010}
\accepted{22 April 2010}

\title[The System Kato: Detecting Cases of Plagiarism for Answer-Set Programs]{The System Kato: Detecting Cases of Plagiarism for Answer-Set Programs}

\author[J. Oetsch, J. P{\"u}hrer, M. Schwengerer,  and H. Tompits]
{JOHANNES OETSCH, J{\"O}RG P{\"U}HRER,\and MARTIN SCHWENGERER, AND
HANS TOMPITS\thanks{This work was partially supported by the Austrian Science Fund~(FWF) under grant P21698.}\\
Technische Universit\"at Wien,\\
Institut f\"ur Informationssysteme 184/3,\\
Favoritenstra\ss{}e\ 9-11,
A-1040 Vienna, Austria \\
\email{\{oetsch,puehrer,schwengerer,tompits\}@kr.tuwien.ac.at}
}


\maketitle
                                              
\begin{abstract}
Plagiarism detection is a growing need among educational institutions and solutions for different purposes exist.
An important field in this direction is detecting cases of \emph{source-code plagiarism}. 
In this paper, we present the tool \kato\ for supporting the detection of this kind of plagiarism in the area of answer-set programming (ASP).
Currently, the tool is implemented for \dlv\ programs but it is designed to handle other logic-programming dialects as well. We 
review the basic features of \kato, 
introduce its theoretical underpinnings,  and discuss an application  of \kato\ for plagiarism detection 
in the context of   courses on logic programming at the Vienna University of Technology.
\end{abstract}
\begin{keywords}
answer-set programming, program analysis, plagiarism detection
\end{keywords}


\section{Introduction}\label{sec:intro}


With the rise of the Internet and its easy access to information, plagiarism is a growing problem not only in academia but also in science and technology in general.
In software development, plagiarism involves copying (parts of) a program
without revealing the source
where it was copied from. The relevance of plagiarism detection for conventional 
program development is well acknowledged~\cite{clough00}---it is not only motivated by an
academic setting to prevent students from violating good academic standards, but
also by the urge to retain the control of program code in industrial software development 
projects.

In this paper, we deal with plagiarism detection in the context of answer-set programming (ASP), an important logic-programming formalism for declarative problem solving.
We discuss the main features and formal underpinnings of the system \kato, a tool developed for supporting the detection of source-code plagiarism for answer-set programs.\footnote{The name of the tool derives, with all due acknowledgements, from Inspector Clouseau's loyal servant and side-kick, Kato.}  
Currently, the tool is realised for logic
 programs adhering to the syntax supported by the well-known ASP solver \dlv, but it is designed to handle other syntactic dialects  as well.\footnote{See \url{http://www.dlvsystem.com/} for details about \dlv.}

Programming in ASP is characterised by the feature that solutions of encoded problems are determined by certain models (the ``answer sets'') of the corresponding programs. 
Moreover, the declarative nature of logic programs also marks a notable difference to 
imperative languages like C++ or Java: 
a logic program is an executable  specification rather than an  instruction on how to solve a problem;
the order of the rules and the order of the literals within the heads and bodies of the 
rules do not affect  the semantics of a program.
As well, logic programs are devoid of any control flow.
Hence, as far as plagiarism detection is concerned, someone who copies code has other means to 
disguise the deed.
Consequently, plagiarism detection tools for
imperative programming languages, 
like, e.g., \texttt{YAP3}~\cite{verco96}, \texttt{Sim}~\cite{gitchell-tran99}, \texttt{JPlag} \cite{prechelt-etal00}, \texttt{XPlag}~\cite{arwin06}, and others \cite{jones01}, are
not adequate for ASP and thus dedicated methods are needed.

The need for tools for plagiarism detection in ASP
can be motivated by the growing application in academia and industry, but our primary interest to have such a tool is to use it in connection with laboratory courses on logic programming and knowledge-based systems at our university each involving more than 100 students.
Since a manual inspection of student programs resulting from such a large body of participants is rather time consuming, we developed the tool \kato to support our grading efforts.
\kato\ implements program-based features, like string-based comparisons of  program comments, string-based comparisons of entire program sources, 
 fingerprint tests, and structure-based comparisons of programs, as well as
a context-dependent confidence measure regarding suspicious code.

A plagiarism detection system applicable in the realm of declarative programming is \texttt{Match}, implementing front-ends for Prolog and SML and following a general approach for plagiarism detection~(\citeANP{luk-sze05} \citeyearNP{luk-sze05,Lukacsy}). 
For Prolog, \texttt{Match} basically compares the  program structure given by a  program's call graph.
Although this approach can be readily adopted for ASP, the call graph of a program as discrimination criterion is somewhat too weak for the particular ASP setting we are considering.
A more detailed discussion of the {\tt Match} approach and its relation to ASP and our setting, as well as a discussion of a related idea to
compute similarities between predicate definitions due to \citeN{serebrenik}, is given in Section~\ref{sec:background}.

\section{Preliminaries on answer-set programming}\label{sec:prel}


We are concerned with \emph{disjunctive logic programs under the answer-set semantics}~\cite{gelf-lifs-91}, a widely used realisation of the ASP paradigm, consisting of 
rules of form

where all  are literals, 
\iec atoms possibly preceded by the symbol for classical negation ,
over a function-free first-order language
and ``'' denotes default negation.
The set of all rules of form~(\ref{eq:rule}) is denoted by .
Given a rule  of form~(\ref{eq:rule}), we define the \emph{head} of  as
, the \emph{positive body} as , and the \emph{negative body} as .
We call  a \emph{constraint} if  and .

\begin{figure}
\hrule
\medskip
\begin{center}

\medskip
\hrule
\caption{Example of a program  and a  disguised copy .}\label{fig:example}
\end{center}
\end{figure}


The \emph{answer sets} of a program are defined by a fixed-point construction involving the \emph{Gelfond-Lifschitz reduct} of a program \cite{gelf-lifs-91}; we omit details because we are interested in syntactic issues only.
Important to note, however, is that the order of literals in a rule, as well as the order of rules in a program, is not relevant for the semantics. 

Besides the basic syntax of rules as described above, the language of the system \dlv, which \kato\ is able to process,   
contains also \emph{built-in  predicates} for comparisons and basic integer arithmetics:
 {\tt =}, {\tt +}, {\tt *}, and 
{\tt !=}. For  {\tt !=}, the alternative syntactic form {\tt <>} is supported. 
Further built-in predicates for comparisons that are supported by  are
{\tt <=}, {\tt <}, {\tt >=}, and {\tt >}.
Moreover, \dlv supports \emph{aggregate functions} to express certain properties of sets, like   or , and \emph{weak constraints} 
that allow to represent optimisation problems.

The two programs in Figure~\ref{fig:example} illustrate, on the one hand, the syntax of \dlv\ and, on the other hand, an attempt to disguise a copied program in ASP:  is a modified version of  resulting from the latter by a combination of permuting and renaming expressions in
, rewriting  arithmetic expressions to equivalent ones,  and  adding the  auxiliary predicate {\tt c}.
We will use  and  as running example in the remainder of this paper.

\section{Background and related work on plagiarism detection}\label{sec:background}


\subsection{Text and program plagiarism}
A multitude of different approaches towards plagiarism detection for documents
exists in the literature~\cite{maurer2006plagiarism}. 
The  two main application areas of text-based plagiarism detection are
\emph{literature} (or \emph{plain text}) \emph{plagiarism} and \emph{program} (or \emph{source-code}) \emph{plagiarism}.
Interestingly, program plagiarism detection has a  longer tradition than literature plagiarism,
dating back to the 1970s~\cite{ottenstein76},
whereas extensive research on literature plagiarism detection started in the late 1990s~\cite{austinbrown99,Farringdon96analysingfor}.
The reason for this is because, on the one hand,  computer programs are well structured
and therefore easier to analyse and to compare than natural language
and, on the other hand, the  interest in literature plagiarism
is related to the boost of plagiarism that came with
the increased availability of information on the Internet.

In both cases, the key task is determining whether a given document 
is similar to an original work.
It is not sufficient for plagiarism detection systems to check for  exact copies only---they need to account
for different camouflage strategies. 
To this end, they employ diverse techniques that
provide a similarity measure with respect to some metric,
some of which are tailored towards the language of the considered document 
whereas others are language independent.
Methods that require knowledge of the language include the
comparison of writing style or frequency of spelling errors in literature plagiarism or checking similarities in the control flow in program plagiarism (when imperative languages are considered).

Among language independent methods are basic string similarity checks 
such as \emph{greedy string tiling}~\cite{greedy93} or \emph{longest common subsequence} (\emph{LCS}) \emph{tests}~\cite{bergroth00} which
are implemented in many plagiarism detectors and which can be used for plain text as well as for  programming languages.

In program plagiarism detection, two general kinds of similarity tests for programs are distinguished:
\emph{fingerprint tests} and \emph{structure tests}~\cite{whale90}.
Fingerprint-based (or \emph{attribute-based}) methods search for copies by comparing characteristic attributes
such as the numbers of unique and total operators,
which provide the basis for the program similarity metrics by \citeN{Halstead1977}.
Structure tests detect similarities based on the actual content and layout of the compared programs.
That most modern tools are mainly based on structure-based techniques~\cite{VercoWise96,Mozgovoy08} suggests
that they are  more accurate than fingerprint tests in certain applications.

A common pattern for the realisation of structure tests is applying string-similarity tests after
a preprocessing step, where the program representation is harmonised \wrt\ certain criteria.
An important preprocessing technique in this respect is \emph{tokenisation},
where the source code is translated into a token string such that certain code strings are replaced by generic tokens.
The resulting token strings are then used for further comparisons by searching for common substrings. However, 
the structure of a \dlv\ program is rather homogeneous---there are not many built-in predicates---which makes this technique rather unsuitable for detecting copies.

In general, plagiarism detection tools can be classified into systems that
operate within a given \emph{corpus}, \iec a collection of documents to check for plagiarism,
and systems that check documents also with external sources such as the World Wide Web
or textbooks \cite{lancaster-classifications}.
Plagiarism detection for programming assignments is typically done intra-corpal,
as the problems to solve are usually very specific such that it is hard for 
students to find adequate external solutions, thus they copy work from their peers.

\subsection{Plagiarism detection  for logic programs}
We next discuss structure-based approaches suitable for declarative languages and relate them to our ASP setting.
To start with, \citeANP{luk-sze05}~\citeNN{luk-sze05,Lukacsy} introduced a general framework for plagiarism detection that is applicable for
both procedural and declarative languages. 
There, the basic idea is to translate  source programs into a suitable formal representation and to apply similarity measures defined on these abstract representations. 
The system {\tt Match} instantiates this framework for, among others, Prolog  as source language.
A logic program   is translated into its \emph{call graph}, also known as \emph{predicate-dependency graph}, which is 
a directed graph  where the nodes are the predicate symbols in , and there is a directed edge between any  two
nodes  and  whenever  contains a rule with  in its head and  in its body.
Then, the similarity between two programs is assessed by computing a graph similarity measure based on graph isomorphism
between the corresponding graph representations. 
The framework also allows to rise the level of abstraction by certain reduction steps on the graph representation. 

To use a dependency-graph representation for programs along with graph-theoretic similarity measures is certainly
an elegant way to counter many common plagiarism covering tricks. 
Changing names of identifiers and variables, changing the arity of  predicates by introducing dummy parameters, or
reordering rules and predicates in rule bodies 
are countered simply by the abstraction due to the graph translation. Other tricks like adding useless rules  or putting
some auxiliary or intermediate definitions into a program are reflected by respective structural properties of the graph representations.

In principle, the framework implemented by {\tt Match} can be adapted for answer-set programs as well. 
However, we follow an alternative approach 
due to certain particularities of ASP and the 
setting we are interested in. 
More specifically,  ASP encodings tend to be quite concise in general, and especially the programs we had to check in the context of our courses on logic
programming (the primary application area of our tool) consist of few rules only. Moreover, the language and the 
programs are rather rigorously specified. 
Hence, the structure of the dependency graph is almost identical for most pairs of programs which disqualifies the form of program abstraction used in {\tt Match} for our purposes.   
Instead, we address the similarity between two programs at the \emph{rule level}, where rules in one program are matched with
 similar rules in the other program, and a rather fine-grained control over the abstraction level of this
rule-matching procedure is introduced.
 
A further approach that can be adapted for plagiarism detection for logic programs is one by \citeN{serebrenik} who investigated methods to measure the similarity between two predicate definitions.
Interestingly, the initial motivation of this work was to detect and eliminate duplicated code that possibly resulted from refactoring steps.
In that approach, two predicate definitions are considered to be similar if they have the same recursive structure modulo renamings and 
permutations of argument positions. 
The actual  similarity  of two predicate definitions is computed, put simply,  as the sum of corresponding clause similarities that are calculated  
recursively  for the involved body predicates and reflect the amount
of common structure that can be preserved when using suitable  generalisations.
To ease the comparison of predicates, the authors introduce a fingerprinting technique, where a fingerprint is an abstraction of the recursive 
structure of a predicate. 
Thus,  this  notion of predicate similarity accounts for consistent 
renamings and permutations and
seems to be promising  for plagiarism detection for declarative logic-based languages as well.

A possible shortcoming of that approach when used for plagiarism detection is, however, that the notion of similarity between two predicates is rather semantic than syntactic.
That is to say, the form of two definitions can be quite different although their similarity is quite high. 
This is adequate for detecting duplicated code but we aim for a similarity notion that is closer tied to
the  syntactic form of the two programs that are compared.   


\section{The System \kato: Architecture and basic features}\label{sec:impl}



\begin{figure}[t]
\begin{center}
\includegraphics[width=9.5cm]{overview}
\end{center}
\caption{Overview of how programs are  compared in \kato.}
\label{fig:kato}
\end{figure}


\kato\ was developed to find pairs of programs which are suspicious \wrt\ plagiarism. The considered programs stem
from student assignments from a course on logic programming at the Vienna University of Technology. \kato\ thus  can perform pairwise similarity tests on  rather large collections of 
 programs. 
 In what follows, we provide basic information
concerning the implemented features of \kato\ and how they are realised. 

The system was entirely developed in Java (Version 6.0) and
 provides a {graphical user interface} for controlling
test runs (a test run is a  series of pairwise program comparisons on collections of programs or single program pairs).
The user can modify the applied tests and configure how the system displays results. 
The results of a test run can be saved in a file or exported into a MySQL database.
Test results  are displayed in a table where each line gives the results of  the pairwise program comparisons. For  results of single tests, \kato\ provides 
detailed views regarding the computed similarities.
Additional information about the tool 
can  be found at
\begin{center}
\url{http://www.kr.tuwien.ac.at/research/systems/kato}.
\end{center}

Following a hybrid approach, \kato\ performs four kinds
of comparison tests, 
realising different layers of granularity:
(i)~a string-based comparison of the program comments, 
(ii)~a string-based comparison of entire program sources, 
(iii)~a fingerprint test, and 
(iv)~a structure-based comparison of  programs. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{screenshot}
\end{center}
\caption{A screen-shot showing how \kato\ displays the results of  a structure test. }
\label{fig:screenshot}
\end{figure}


For the string-based tests, comparisons based on a {longest common subsequence} (LCS) metric are applied \cite{bergroth00}.
Long common substrings are usually an indicator for a copy, however a major drawback of this method is its vulnerability \wrt\  non-matching  symbols in a sequence which break the substrings apart~\cite{LBPS08}.
On the other hand, as the LCS of two strings is the longest sequence of symbols appearing in both strings with the same succession, the LCS metric tolerates injected non-matching objects and is more robust in this respect.
To illustrate the notion of LCS, consider the two strings
``\kato\ is a great tool for plagiarism detection in ASP'' and
``the system \kato\ is currently applied in a course on logic programming to reveal cases of plagiarism''.
The LCS of these strings is ``\kato is a plagiarism''.
The \emph{LCS similarity} of two strings  and  is the 
 length of the LCS of  and  normalised by the length of .

We note that tests (i) and (ii) are language independent whilst (iii) and (iv) need to be adapted for
different languages.
All of these tests, outlined in more detail below, compare files pairwise and return a similarity value between 0 (no similarities) and 1 (perfect match). 

Figure~\ref{fig:kato} shows the basic working steps needed to compare programs in \kato.
First, the source code  of a collection of  programs is
cleansed, \iec white spaces are removed, and parsed  in a  preprocessing step. 
Then, the cleansed source code, as well as the abstract program representations resulting from the parsing step, are
 subject to a preparation step where fingerprints are created and comments are extracted. 
In a comparison phase, LCS comment tests, LCS program tests, fingerprint tests, and structure tests are applied. 
Finally,
the results of the comparisons are presented to the user. 
More specifically, the results are displayed in 
tabular form with features like sorting and filtering.
For the structure tests, the tool shows program pairs and 
highlights similar rules; see Figure~\ref{fig:screenshot} to get a rough impression. 

In what follows, we describe the comparison tests (i)--(iv) in more detail.

\paragraph{LCS comment test.}
When programs are copied, it is surprisingly quite frequent that program comments are copied as well and usually little effort is spent to mask such copied comments.
Consequently, the LCS comment test aims at revealing similar parts in program comments.
More specifically, this test reveals similarities between two programs by comparing the (concatenated) comments
in the two programs via the LCS similarity measure.

\paragraph{LCS program test.}
This test is very similar to the LCS comment test but considers entire programs.
Thus, this test computes the LCS similarity of the  cleansed source code of two programs when interpreted as strings.
It turned out that this test is an efficient method to detect
cases of plagiarism where not much time has been spent to camouflage the deed.  
It is  especially well-suited to detect perfectly matching sequences in programs which is usually a
clear hint for plagiarism.

\paragraph{Fingerprint test.}
Recall that a fingerprint of a program is a collection of relevant program attributes, \iec statistical data like 
hash codes, the number of rules, the number of predicates, the number of constants, program size, and so
on. After fingerprints of all programs are generated in a preparation step, the fingerprints are compared
pairwise. 
This test is quantified by normalising the number of matching attributes by the total number of considered attributes for each pair
of programs.
This gives a simple yet convenient way to collect further evidence for plagiarism.

\paragraph{Structure test.}
This test does not operate on the cleansed program sources but on the program representation that was built in
a preceding parsing step.
We here rely on a set representation of programs and of rule heads and bodies as well as on a structured representation of literals and term lists.
While the LCS comment test, the LCS program test, and the fingerprint test are, more or less, standard techniques to detect plagiarism in general, the structure test is developed specifically for our ASP setting.

The central notion of \emph{program similarity}  that underlies  
the structure test,  formally defined in the next section,
is designed to thwart disguising strategies like permuting rules or literals
within rules. 
However, a more advanced plagiarist will apply additional camouflage techniques, \egc  uniformly renaming variables in rules or renaming auxiliary predicates in programs. 
Therefore, our similarity test comes with different levels of abstraction, so-called \emph{camouflage techniques}, to counter such efforts.
Without going into details at this point, renaming is handled by finding and applying suitable substitution functions.

To make the similarity function sensitive to common rule patterns, we also implemented a context dependent notion 
of \emph{confidence}
regarding plagiarism:
When whole collections of programs are examined for suspicious pairs of programs,
a \emph{global occurrence table} gives
additional information on how specific two rules are. The main idea is that rare rules yield better evidence for a copy than common ones. Therefore, \kato\ collects and counts all rules
in the considered corpus of programs
and stores this information in an {occurrence table} which is then used to compute a measure of confidence regarding
plagiarism for two programs based on the frequency of common rules in the programs relative to their frequency in the
considered corpus. 

\section{The formal similarity model of the structure test}\label{sec:theory}


In this section, we present a syntactic measure of program similarity between logic programs that forms the theoretical basis of the central plagiarism-detection features of \kato.
Furthermore, we introduce a  confidence measure that aims at quantifying the  reliability  of our similarity measure for detecting actual cases of plagiarism in ASP. 

\subsection{A syntactic measure for rule and program similarity}

Our basic assumption is that a plagiarist camouflages a copy by changing the form of the copied program, \iec by \emph{program transformations at a syntactic level}.  
Examples for rather simple camouflage techniques  are changing the formatting of the source code,
permuting rules of the program, and changing the order of literals within rule heads or bodies. 
More advanced techniques include uniformly renaming variables within rules, renaming auxiliary atoms
within programs, and rewriting   arithmetic expressions to equivalent ones.  
Clearly, all these basic techniques can be combined to build more complex operations.

A key concept for detecting copied programs is thus measuring the similarity between programs. 
Formally, this boils down to the definition of a suitable similarity function that quantifies the syntactic similarity between two programs  with respect to specific combinations of camouflage techniques. 
To cover different camouflage techniques in a uniform way and to allow for the composition of different techniques, we handle them as functions that map pairs of rules to pairs of rules.  

\begin{definition}\label{def:technique}
A \emph{camouflage technique}, or \emph{technique}, is a function
with domain and co-domain . 
In particular, by the \emph{identity technique}, , we understand the identity function over , \iec  satisfies , for each .
\end{definition} 

Obviously, the notion of camouflage technique is closed under functional composition, \iec
for any two techniques  and , the composition  is
a technique as well.
From an algebraic point of view, the set of techniques and the composition operator form a monoid  with
 as its identity element. 
Hence, the above definition allows to express more complex combinations of different techniques, as used, \egc to disguise program  
in Figure~\ref{fig:example}, by a composition of less complex  techniques. 
  
We next introduce our central similarity measures.
Roughly speaking, we define the similarity between two single program rules  and  as
the number of common head and body literals of  and  normalised by the total number of
literals in . 

\begin{definition}\label{def:rulesim}
For any two rules , 
the \emph{rule similarity between  and } is given by

\end{definition}

For example, consider the two rules
\begin{center}
\begin{tabular}{l@{}c@{}l}
 &   &{\tt \small p(X) :- r(X), s(X,Z), not t(c)} \ and \\
 &  & {\tt \small p(X) v q(Y) :- not\ t(c), s(X,Z), r(Y)} .
\end{tabular}
\end{center}
According to the above definition, .

The notion of rule similarity extends to entire programs as follows:

\begin{definition}\label{def:progsim}
Given two programs  and  together with a camouflage technique ,
 the \emph{program similarity between  and  \wrt\ } is given by

\end{definition}

Clearly, for any program  and  and for any technique , it holds that
.
Note that rule similarity and  program similarity are not symmetric in their arguments.
 Roughly speaking, the significance of this asymmetry is that it allows to express
to which extent  is subsumed by  by similar rules \wrt\  and .
This can be further interpreted as hints concerning to which extent one program resulted from another program
by adding or splitting certain rules which can help to assess who copied from whom.

Using the  identity technique to the two programs  and  from
Figure~\ref{fig:example} to compute its similarity yields
. 
Hence, we need more advanced techniques than
 to get a similarity function of practical use. 
Accordingly, we next introduce some basic techniques which can serve as building blocks for more complex ones.

\subsection{Basic camouflage techniques}\label{subsec:techniques}

\subsubsection{Variable renaming}

The first technique we consider is the uniform renaming of variables within rules which
is a  simple way to change the form of a rule. 
Formally, a \emph{variable renaming} for a rule  is a bijection 
from the set  of variables occurring in  to a set  consisting of  arbitrary variables.
For any variable renaming   for some rule ,  denotes the result of 
replacing each occurrence of a variable  in  by . 
We assume that  is a globally fixed well-ordering on variable renamings
that  extends to tuples of variable renamings in a lexical way.
Informally, the following technique applies variable renamings to a pair of rules that results in a maximal 
rule similarity. Since such maximal variable renamings are not unique in general, we need   to define
a proper function.

\begin{definition}\label{def:varrenaming}
The camouflage technique  is the function assigning to
each rule pair  the pair
,
where  are the variable renamings for  and  such that

\end{definition}

For example, consider programs  and  from Figure~\ref{fig:example}.  Let  be the last rule  in   and  the first 
rule in .
Rule  contains the variables , , , , and , and  contains the variables , , , , and . 
Variable renamings  that yield maximal rule similarity are
 for  and the identity function for . 

Applying  to  results in the rule

Assuming a suitable well-ordering, we get , and thus
.
Note that the similarity between  and  under  is  . 

\subsubsection{Predicate renaming}
Next, we address a technique to deal with renaming efforts at the predicate level. 
Given a program ,  a bijection  from the set  of predicate symbols occurring  in   to  a set 
consisting of  arbitrary predicate symbols is a \emph{predicate renaming} for  if
 and  have the same arity, 
for each .
Let  be a predicate renaming for a program and 
a program or a rule. Then,  denotes the result of 
replacing each occurrence of a predicate symbol  in  by , provided  is defined for .  

Note that predicate renamings are not applied to single rules but to entire programs.
To express predicate renamings in terms of camouflage techniques, we 
introduce a  technique that is parameterised by two programs that are used to determine
 the predicate renaming that is applied.\footnote{Formally, this means that we introduce a family of techniques, one technique for each  program pair, respectively.}
Like for variable renamings, we fix a well-ordering  defined on predicate renamings
and assume that   extends to tuples of predicate renamings in a lexical way.

\begin{definition}\label{def:predrenaming}
Given two programs  and , the camouflage technique  is the function assigning to
each rule pair  the pair
,
where  and  are the predicate renamings for  and  such that

\end{definition}


Intuitively, the above definition formalises the idea of applying predicate renamings that result in maximal
program similarity when also variable renamings are considered for the rule comparisons. 

Recall again programs  and  from Figure~\ref{fig:example}. 
Predicate renamings that yield maximal program similarity are
the renaming  for  which maps each predicate symbol in  to itself except
for , which is mapped to , and the renaming  for  given as the identity function. 
 
Let us consider the technique .
Then,
.
Observe
that , for , which
illustrates that
 is almost entirely subsumed (syntactically) by  but not vice versa due to the additional third rule in .

\subsubsection{Canonisation of built-in predicates}

The next technique is designed to counter program transformations that exploit that
some \dlv\ built-in predicates have  syntactically  different 
representations.
For each built-in predicate, we define a unique \emph{canonical  representation}.
E.g., the canonical representation of {\tt Z = X + Y} and {\tt +(X,Y,Z)} is {\tt +(X,Y,Z)} (the canonical representations for {\tt  *}, {\tt  =}, {\tt  <>}, {\tt  !=}, {\tt  <=}, {\tt  <}, {\tt  >=}, and {\tt  >} are omitted for space reasons).

\begin{definition}\label{def:canonisation}
The camouflage technique  is the function assigning to each rule pair  the pair ,
where  results from  by rewriting all occurrences of built-in predicates in  into their respective canonical forms, and likewise for  and .
\end{definition}

Concerning our running example from Figure~\ref{fig:example},
if we pool the techniques in our  arsenal of camouflage techniques considered so far, we get a program similarity of
, for .

\subsubsection{Ordering the arguments of commutative predicates}
For most \dlv\ built-in predicates, viz.\ for {\tt =}, {\tt +}, {\tt *}, 
{\tt !=}, and {\tt <>}, (some of) their arguments can be commuted. 
Swapping arguments of such \emph{commutative predicates} is addressed by the  next technique.

\begin{definition}\label{def:swapping}
The camouflage technique  is the function assigning to each rule pair  the pair
,
where   is the rule that results from   by replacing each commutative predicate  in 
 with  a rewriting of  where all commutable arguments of  are lexically ordered according to their
string representations, and  results from  in an analogous fashion. 
\end{definition}
 
For programs  and  from  Figure~\ref{fig:example},
we eventually obtain , for ,
and  , for .

\subsubsection{Further issues}
\paragraph{Customisation of techniques in {\em \kato}.}
Versions of all the basic camouflage techniques discussed so far are implemented in \kato. 
Note, however, that the composition operation for these techniques
is not commutative.
Thus, different compositions result in different similarity values between programs in general.
As well, the user can easily define further techniques from the basic ones, but our current implementation is restricted to compositions subject to the order
 which turned out to be particularly useful in practice.
We plan to extend \kato such that this limitation is removed in future versions.
Furthermore, \kato\ provides the following hierarchy of predefined techniques:
\begin{center}
, , , and .
\end{center}
Each of these techniques realises a level
of abstraction: On the one hand, for two programs  and , usually 
 holds  if 
. 
On the other hand,  the significance of a high similarity value relative to  is indirect proportional to .
It often makes sense to consider a sequence of comparisons, starting with  and ending with , and to consider
similarity and confidence relative to the level ---\kato\
can perform such hierarchical tests, if required.
We will discuss an empirical analysis employing these predefined techniques in Section~\ref{sec:eval}.

\paragraph{Complexity aspects.}
Concerning the inherent  complexity of applying a technique to a pair of rules, 
we note that the variable and predicate renaming techniques tend to be computationally expensive while
the other techniques can be applied in linear time \wrt\ the size of the rules.
 Without going into details,  the problem of finding variable or predicate substitutions is  related to the 
homomorphism problem for relational structures which is known to be \NP-complete (when formulated as a decision problem). However, high complexity
 of the 
involved reasoning tasks does not
turn out to be a bottle-neck of our approach since this worst-case complexity is relative to the size of rules or 
 programs and
problem encodings in ASP tend to 
be quite concise and rarely involve a larger number of rules---this is witnessed, \egc by the collection of benchmark programs
used for the recent ASP solver competition~\cite{competition09}.

\subsection{A corpus-based confidence measure}


We now introduce a method how to further evaluate the outcome of a program comparison regarding its
potential to actually reveal a  case of plagiarism.
One important aspect in this regard is the influence of the used camouflage technique  on the program similarity.
In general,
we can observe a trade-off between the complexity of  (in terms of operations on rules) and
the significance of a high similarity value for revealing plagiarism. 
As illustrated, the similarity between programs  and  from Figure~\ref{fig:example} ranges from  to , depending on the used technique.
However, the technique used to obtain a similarity of 1 is rather demanding and presumes that 
a plagiarist applied more advanced methods to cover a copy.
Thus, the significance of the similarity outcome is rather low.

A common and in practice important setting  is when we have a collection, or corpus, of programs, and our goal is
to detect suspicious pairs of programs within this corpus.  Such a setting allows to factor in further information
that can increase our confidence that a high program similarity actually indicates a case of plagiarism.  
Here, the basic idea is to consider the relative frequency of rule occurrences \wrt\ the considered program collection.
If two programs have a high similarity but share only very common rules, our confidence 
will be accordingly low. 
Likewise, if two programs contain the same extremely rare rule,
our confidence for plagiarism will be rather high.
Clearly, the notion of ``sameness'' of a rule depends on the considered camouflage technique .
Formally, we introduce a suitable equivalence relation based on   and consider the induced equivalence class to define the relative frequency of a rule \wrt\ a program corpus.

Given a camouflage technique    composed from techniques in , let us
define the  relation  as 

for any rule .
It can be shown that   is an equivalence relation.
For a set  of rules, a rule , and a relation  , let  be the  equivalence class of 
 in  under .
The \emph{relative frequency} of  \wrt\  and  is given by 

The notion of confidence regarding two programs is then defined as
follows:

\begin{definition}\label{def:confidence}
Let  be a collection of programs and  a camouflage technique   composed from techniques in .
Furthermore, let  be the set of all rules occurring in .
Then, for programs ,
the \emph{confidence regarding  and  relative to  and } is given by

\end{definition}

\section{Application of \kato and empirical analysis}\label{sec:eval}


As mentioned earlier, our original motivation for the development of \kato\ was to have a tool to support detecting cases of plagiarism in the context of several laboratory courses.

The particular setting of the courses was as follows:
Students were assigned with exercises and had to solve them 
 by means of ASP. The size of the program parts from the student solutions ranged from a few rules to dozens of rules.
Besides evaluating the correctness of the solutions, human supervisors had
to check for plagiarism. 
Since the number of students increased from year to year, with typically over 100 students attending the courses, 
the task to manually inspect all pairs of programs for plagiarism became too time consuming and laborious.
We thus decided in early 2009 to develop \kato, which became 
operational in the summer term of that year.
 \kato\ was used to pre-select pairs of programs if their program similarity was beyond a fixed threshold, and the selected programs were then further manually inspected by the supervisors. 
\kato had to perform tens of thousands of program comparisons 
which was feasible within 
hours.
Most importantly, the application of the system yielded dramatical savings in labour time.

To provide a more detailed assessment of \kato, 
in what follows we give an evaluation of \kato in terms of the well-known notions of  \emph{precision} and \emph{recall}.
In our setting,
precision is the ratio between the number of program pairs that are correctly classified by \kato as copies 
and the total number of 
pairs that are classified by \kato as copies (\iec the number of true positives divided by the sum of true positives and false positives), and
recall is the ratio between the number of program pairs that are correctly classified as copies and the total number of pairs that are 
actual copies (\iec the number of true positives divided by the sum of true positives and false negatives).
The purpose of the evaluation is to show the effects of different combinations of techniques for the structure test on precision and recall.

For the present evaluation, we used data from the 
logic programming course that took place in 2008 where we had to assess 109 programs from the students.
We did not use assignments from the courses after 2008 because these were used as reference data in the development phase of \kato and thus are 
unsuitable for a fair evaluation as the tool is trained on them.
In the considered program collection, we manually identified 26 program pairs suspected for plagiarisms.

\begin{table}
	\caption{Evaluation of techniques for the structure test.}
	\label{tab:eval}
{\footnotesize
	\begin{center}
	\begin{tabular}{ccccccc}
	  \hline
	  	     {Technique} & 
	  	     {Time} & 
	  	    {Threshold} & 
	  	    {Classified} & 
	  	    {Actual} & 
	  	    {Recall} & 
	  	    {Precision} \\
	    & (sec.) & & as copies & copies & & \\
	    \toprule
	  \multirow{4}{*}{ } & \multirow{4}{*}{ } &  &  &  &  &   \\ 
	  &  &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	\midrule
	  \multirow{4}{*}{ } & \multirow{4}{*}{ } &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	\midrule
	  \multirow{4}{*}{ } & \multirow{4}{*}{ } &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	\midrule
	  \multirow{4}{*}{ } & \multirow{4}{*}{ } &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	  &  &  &  &  &  &  \\ 
	  &  &  &  &  &  &   \\ 
	  \bottomrule
	\end{tabular}
	\end{center}
}
\vspace{-1\baselineskip}
\end{table}

Table~\ref{tab:eval} shows the results of our evaluation of different techniques for the structure test, where
we considered the predefined techniques -- from above. Recall that  is the simplest technique while  is the most
complex one.
The table is organised as follows.
In each of the four main rows, we give the precision and recall results for one technique -- as well as the time to run the test.
For each technique, we considered different values, ranging from  to ,  for the similarity threshold used to
classify whether a pair of programs counts as a case of plagiarism.
For each threshold value, we give the number of program pairs that are classified as copies by , \iec that showed a program
similarity above the threshold value, as well as the number of actual copies, \iec the number of program pairs from the ones classified
 as copies by \kato that are indeed cases of plagiarism according to a human supervisor.
The last two columns depict the actual precision and recall values.

The evaluation clearly shows the trade-off between precision and recall relative to the abstraction level imposed by the considered
camouflage technique. 
For the simplest technique~, precision is maximal and recall is minimal.
When going from the simpler techniques to
the more complex ones, precision decreases while recall increases.
This observation nicely illustrates how precision and recall can be controlled by \kato's system of composable camouflage techniques.
Furthermore, the results indicate also that each of our considered camouflage techniques is used by students, although some of them occur usually in combination with others. 
For example, students who renamed auxiliary predicates 
usually also rename variables. 
According to our evaluation, many students who change the names of auxiliary predicates also perform other changes, such as reordering commutative operators or renaming variables. 

Concerning  the different plagiarism detection strategies realised in ,
our experience suggests that the structure test provides the most accurate results when  
students spent some effort to camouflage the copy. 
The LCS program test, on the other hand, revealed  exact or almost exact copies, while the LCS comment test seems to be a good additional indicator for plagiarism. 
The fingerprint test  provided a fast way to 
 detect exact copies because of  identical hash values.
However, it is to mention that the exercises were rather rigorously specified which implies that it was rather common
that programs agreed on some attributes like, \egc the number of different predicate symbols.

\section{Conclusion }\label{sec:concl}


 In this paper, 
 we presented the tool \kato for plagiarism detection  in ASP, reviewing its underlying  methodology and its
 basic features.
 In particular, we introduced the formal basis of \kato:   syntactic notions of program similarity between programs 
 along with a formal measure of confidence regarding
 plagiarism detection for programs. Both concepts are based on the notion of a camouflage technique which is
 basically a strategy to  disguise
 the form of a program and which provides
 a flexible means to deal with different strategies and combinations thereof in a uniform manner.
 Currently, \kato\ is designed for \dlv's language dialect but it can easily be  extended
 to other dialects.
 For \dlv programs, \kato supports also weak constraints and aggregates, and it can also deal with programs for the  planning front-end of  \dlv~\cite{eiter03}. 
The system was successfully applied for laboratory courses 
at our university and helped to save valuable labour time.  

 For future work, we plan  to develop means to visualise the comparison results, e.g.,  
 to spot clusters of cooperating plagiarists more easily.
 A further  aspect of \kato\ worth mentioning is a possible use for  the development of logic programs:
 If a team is working on a program, different versions can emerge. Then, assessing 
 the actual differences between two versions can be necessary. \kato\ can be adapted for
such an application scenario.
 


\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Arwin and Tahaghoghi}{Arwin and
  Tahaghoghi}{2006}]{arwin06}
{\sc Arwin, C.} {\sc and} {\sc Tahaghoghi, S. M.~M.} 2006.
\newblock Plagiarism detection across programming languages.
\newblock In {\em Proceedings of the Twenty-Ninth Australasian Computer Science
  Conference {\rm (}ACSC 2006{\rm )}}. CRPIT, vol.~48. Australian Computer
  Society, Inc., Darlinghurst, Australia, 277--286.

\bibitem[\protect\citeauthoryear{Austin and Brown}{Austin and
  Brown}{1999}]{austinbrown99}
{\sc Austin, M.} {\sc and} {\sc Brown, L.} 1999.
\newblock Internet plagiarism: Developing strategies to curb student academic
  dishonesty.
\newblock {\em The Internet and Higher Education\/}~{\em 2,\/}~1, 21--33.

\bibitem[\protect\citeauthoryear{Bergroth, Hakonen, and Raita}{Bergroth
  et~al\mbox{.}}{2000}]{bergroth00}
{\sc Bergroth, L.}, {\sc Hakonen, H.}, {\sc and} {\sc Raita, T.} 2000.
\newblock A survey of longest common subsequence algorithms.
\newblock In {\em Proceedings of the Seventh International Symposium on String
  Processing Information Retrieval {\rm (}SPIRE 2000{\rm )}}. IEEE Computer
  Society, Los Alamitos, California, 39--48.

\bibitem[\protect\citeauthoryear{Clough}{Clough}{2000}]{clough00}
{\sc Clough, P.} 2000.
\newblock Plagiarism in natural and programming languages: An overview of
  current tools and technologies.
\newblock Tech. Rep. CS-00-05, Department of Computer Science, University of
  Sheffield, UK.

\bibitem[\protect\citeauthoryear{Denecker, Vennekens, Bond, Gebser, and
  Truszczynski}{Denecker et~al\mbox{.}}{2009}]{competition09}
{\sc Denecker, M.}, {\sc Vennekens, J.}, {\sc Bond, S.}, {\sc Gebser, M.}, {\sc
  and} {\sc Truszczynski, M.} 2009.
\newblock The second answer set programming competition.
\newblock In {\em Proceedings of the Tenth International Conference on Logic
  Programming and Nonmonotonic Reasoning {\rm(}LPNMR 2009{\rm)}}. Lecture Notes
  in Computer Science, vol. 5753. Springer, Berlin-Heidelberg, 637--654.

\bibitem[\protect\citeauthoryear{Eiter, Faber, Leone, Pfeifer, and
  Polleres}{Eiter et~al\mbox{.}}{2003}]{eiter03}
{\sc Eiter, T.}, {\sc Faber, W.}, {\sc Leone, N.}, {\sc Pfeifer, G.}, {\sc and}
  {\sc Polleres, A.} 2003.
\newblock A logic-programming approach to knowledge-state planning {II}: The
  {\sc dlv} system.
\newblock {\em Artificial Intelligence\/}~{\em 144,\/}~2, 157--211.

\bibitem[\protect\citeauthoryear{Farringdon}{Farringdon}{1996}]{Farringdon96analysingfor}
{\sc Farringdon, J.~M.} 1996.
\newblock {\em Analyzing for Authorship: A Guide to the Cusum Technique}.
\newblock University of Wales Press, Cardiff.

\bibitem[\protect\citeauthoryear{Gelfond and Lifschitz}{Gelfond and
  Lifschitz}{1991}]{gelf-lifs-91}
{\sc Gelfond, M.} {\sc and} {\sc Lifschitz, V.} 1991.
\newblock Classical negation in logic programs and disjunctive databases.
\newblock {\em New Generation Computing\/}~{\em 9,\/}~3/4, 365--385.

\bibitem[\protect\citeauthoryear{Gitchell and Tran}{Gitchell and
  Tran}{1999}]{gitchell-tran99}
{\sc Gitchell, D.} {\sc and} {\sc Tran, N.} 1999.
\newblock Sim: {A} utility for detecting similarity in computer programs.
\newblock In {\em Proceedings of the Thirtieth SIGCSE Technical Symposium on
  Computer Science Education {\rm (}SIGCSE 1999{\rm )}}. SIGCSE Bulletin,
  vol.~31. ACM Press, New York, NY, USA, 266--270.

\bibitem[\protect\citeauthoryear{Halstead}{Halstead}{1977}]{Halstead1977}
{\sc Halstead, M.~H.} 1977.
\newblock {\em Elements of Software Science}.
\newblock Elsevier Science Inc., New York, NY, USA.

\bibitem[\protect\citeauthoryear{Jones}{Jones}{2001}]{jones01}
{\sc Jones, E.~L.} 2001.
\newblock Metrics based plagiarism monitoring.
\newblock {\em Journal of Computing Sciences in Colleges\/}~{\em 16,\/}~4,
  253--261.

\bibitem[\protect\citeauthoryear{Lancaster, and Culwin}{Lancaster
  et~al\mbox{.}}{2005}]{lancaster-classifications}
{\sc Lancaster, T.} {\sc and} {\sc Culwin, F.} 2005.
\newblock Classifications of plagiarism detection engines.
\newblock {\em ITALICS\/}~{\em 4,\/}~2, \url{http://www.ics.heacademy.ac.uk/italics/vol4iss2.htm}.

\bibitem[\protect\citeauthoryear{Loose, Becker, Potthast, and Stein}{Loose
  et~al\mbox{.}}{2008}]{LBPS08}
{\sc Loose, F.}, {\sc Becker, S.}, {\sc Potthast, M.}, {\sc and} {\sc Stein,
  B.} 2008.
\newblock {Retrieval-Technologien f\"{u}r die Plagiaterkennung in Programmen}.
\newblock Tech. rep., University of W\"{u}rzburg, Germany.

\bibitem[\protect\citeauthoryear{Luk\'{a}csy and Szeredi}{Luk\'{a}csy and
  Szeredi}{2005}]{luk-sze05}
{\sc Luk\'{a}csy, G.} {\sc and} {\sc Szeredi, P.} 2005.
\newblock A generic framework for plagiarism detection in programs.
\newblock In {\em Proceedings of the 4th Japanese-Hungarian Symposium on
  Discrete Mathematics and Its Applications}. 189--198.

\bibitem[\protect\citeauthoryear{Luk{\'a}csy and Szeredi}{Luk{\'a}csy and
  Szeredi}{2009}]{Lukacsy}
{\sc Luk{\'a}csy, G.} {\sc and} {\sc Szeredi, P.} 2009.
\newblock Plagiarism detection in source programs using structural
  similarities.
\newblock {\em Acta Cybernetica\/}~{\em 19,\/}~1, 191--216.

\bibitem[\protect\citeauthoryear{Maurer, Kappe, and Zaka}{Maurer
  et~al\mbox{.}}{2006}]{maurer2006plagiarism}
{\sc Maurer, H.}, {\sc Kappe, F.}, {\sc and} {\sc Zaka, B.} 2006.
\newblock Plagiarism: {A} survey.
\newblock {\em Journal of Universal Computer Science\/}~{\em 12,\/}~8,
  1050--1084.

\bibitem[\protect\citeauthoryear{Mozgovoy}{Mozgovoy}{2008}]{Mozgovoy08}
{\sc Mozgovoy, M.} 2008.
\newblock {\em Enhancing Computer-Aided Plagiarism Detection}.
\newblock VDM Verlag, Saar\-br\"{u}cken, Germany.

\bibitem[\protect\citeauthoryear{Ottenstein}{Ottenstein}{1976}]{ottenstein76}
{\sc Ottenstein, K.~J.} 1976.
\newblock An algorithmic approach to the detection and prevention of
  plagiarism.
\newblock {\em SIGCSE Bulletin\/}~{\em 8,\/}~4, 30--41.

\bibitem[\protect\citeauthoryear{Prechelt, Malpohl, and Philippsen}{Prechelt
  et~al\mbox{.}}{2000}]{prechelt-etal00}
{\sc Prechelt, L.}, {\sc Malpohl, G.}, {\sc and} {\sc Philippsen, M.} 2000.
\newblock {JP}lag: Finding plagiarisms among a set of programs.
\newblock Tech. Rep. 2000-1, Fakult{\"{a}}t f{\"{u}}r Informatik
  Universit{\"{a}}t Karlsruhe, Germany.

\bibitem[\protect\citeauthoryear{Serebrenik and Vanhoof}{Serebrenik and
  Vanhoof}{2007}]{serebrenik}
{\sc Serebrenik, A.} {\sc and} {\sc Vanhoof, W.} 2007.
\newblock Fingerprinting logic programs.
\newblock {\em CoRR\/}~{\em abs/cs/0701081}.

\bibitem[\protect\citeauthoryear{Verco and Wise}{Verco and
  Wise}{1996a}]{VercoWise96}
{\sc Verco, K.~L.} {\sc and} {\sc Wise, M.~J.} 1996a.
\newblock Plagiarism a la mode: A comparison of automated systems for detecting
  suspected plagiarism.
\newblock {\em The Computer Journal\/}~{\em 39,\/}~9, 741--750.

\bibitem[\protect\citeauthoryear{Verco and Wise}{Verco and
  Wise}{1996b}]{verco96}
{\sc Verco, K.~L.} {\sc and} {\sc Wise, M.~J.} 1996b.
\newblock {YAP3}: Improved detection of similarities in computer program and
  other texts.
\newblock In {\em Proceedings of the Twenty-Seventh {SIGCSE} Technical
  Symposium on Computer Science Education {\rm (}SIGCSE 1996{\rm )}}. ACM
  Press, New York, NY, USA, 130--134.

\bibitem[\protect\citeauthoryear{Whale}{Whale}{1990}]{whale90}
{\sc Whale, G.} 1990.
\newblock Identification of program similarity in large populations.
\newblock {\em The Computer Journal\/}~{\em 33,\/}~2, 140--146.

\bibitem[\protect\citeauthoryear{Wise}{Wise}{1993}]{greedy93}
{\sc Wise, M.~J.} 1993.
\newblock {Running Karp-Rabin matching and greedy string tiling}.
\newblock Tech. rep., Basser Department of Computer Science, University of
  Sydney, Australia.

\end{thebibliography}

\end{document}
