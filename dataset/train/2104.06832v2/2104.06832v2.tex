

Given an RGB image $x$ of size $W\times H \times 3$, we aim for a multi-head deep network $G$ that not only determines whether the image has been manipulated, but also pinpoints its manipulated pixels. Let $G(x)$ be the network-estimated probability of the image being manipulated. In a similar manner we define $G(x_{i})$ as pixel-wise probability, with $i=1,\ldots,W \times H$. Accordingly, we denote a full-size segmentation map as $\{G(x_{i})\}$. As the image-level decision is naturally subject to pixel-level evidence, we obtain $G(x)$ by Global Max Pooling (GMP) over the segmentation map, \ie 
\begin{equation} \label{eq:gmp}
G(x) \leftarrow \mbox{GMP}\left(\{G(x_{i})\}\right).
\end{equation}








In order to extract generalizable manipulation detection features, we present a new network that accepts both RGB and noise views of the input image. To strike a proper balance between detection sensitivity and specificity, the multi-view feature learning process is jointly supervised by annotations of three scales, \ie pixel, edge and image. 





\subsection{Multi-View Feature Learning} \label{ssec:mvfl}

As shown in Fig. \ref{fig:model},  \model~consists of two branches, with ResNet-50 as their backbones. The edge-supervised branch (ESB) at the top is specifically designed to exploit subtle boundary artifact around tampered regions, whilst the noise-sensitive branch (NSB) at the bottom aims to capture the inconsistency between tampered and authentic regions. Both clues are meant to be semantic-agnostic. 
\subsubsection{Edge-Supervised Branch }\label{sssec:esb}


Ideally, with edge supervision, we hope the response area of the network will be more concentrated on tampered regions. Designing such an edge-supervised network is nontrivial. As noted in Section \ref{sec:relate}, the main challenge is how to construct an appropriate input for the edge detection head. On one hand, directly using features from the last ResNet block is problematic, as this will enforce the deep features to capture low-level edge patterns and consequently affect the main task of manipulation segmentation. While on the other hand, using features from the initial blocks is also questionable, as subtle edge patterns contained in these shallow features can vanish with ease after multiple deep convolutions. A joint use of both shallow and deep features is thus necessary. However, we argue that simple feature concatenation as previously used in \cite{2020GSR} is suboptimal, as the features are mixed and there is no guarantee that the deeper features will receive adequate supervision from the edge head. To conquer the challenge, we propose to construct the input of the edge head in a shallow-to-deep manner.




As illustrated in Fig. \ref{fig:model}, features from different ResNet blocks are combined in a progressive manner for manipulation edge detection. In order to enhance edge-related patterns, we introduce a Sobel layer, see Fig. \ref{fig:sobel}. Features from the $i$-th block first go through the Sobel layer followed by an edge residual block (ERB), see Fig. \ref{fig:erb}, before they are combined (by summation) with their counterparts from the next block. To prevent the effect of accumulation, the combined features go through another ERB (top in Fig. \ref{fig:model}) before the next round of feature combination. We believe such a mechanism helps prevent extreme cases in which deeper features are over-supervised or fully ignored by the edge head. By visualizing feature maps of the last ResNet block in Fig. \ref{fig:visual_esb}, we observe that the proposed ESB indeed produces \add{a} more focused response near tampered regions.  



\begin{figure}[htbp]
\begin{center}
\subfigure[Sobel Layer]{
\begin{minipage}[t]{\linewidth}
\begin{center}
\includegraphics[width=0.8\columnwidth]{figure/sobel2.0.png}
\end{center}
\label{fig:sobel}
\end{minipage}}

\subfigure[Edge Residual Block (ERB)]{
\begin{minipage}[t]{\linewidth}
\begin{center}
\includegraphics[width=0.8\columnwidth]{figure/erb.png}
\end{center}
\label{fig:erb}
\end{minipage}}\end{center}
\caption{\textbf{Diagrams of (a) Sobel layer and (b) edge residual block}, used in ESB for manipulation edge detection.}
\label{fig:sobel-erb}
\end{figure}

\begin{figure}[htpb]
    \begin{center}
    \includegraphics[width=0.95\columnwidth]{figure/esb3.0.pdf}
    \end{center}
    \caption{\textbf{Visualization of averaged feature maps of the last ResNet block}, brighter color indicating \add{a} higher response.  Manipulation from the top to bottom is inpainting, copy-move and splicing. Read from the third column  are \textit{w/o edge}, \ie ResNet without any edge residual block, \textit{GSR-Net}, \ie ResNet with the GSR-Net alike edge branch, and the proposed \textit{ESB}, which produces \add{a} more focused response near tampered regions.}
    \label{fig:visual_esb}
\end{figure}








The output of ESB has two parts: feature maps from the last ResNet block, denoted as $\{f_{esb,1}, \ldots, f_{esb,k}\}$, to be used for the main task, and the predicted manipulation edge map, denoted as $\{G_{edge}(x_{i})\}$, obtained by transforming the output of the last ERB with a sigmoid ($\sigma$) layer. The data-flow of this branch is conceptually expressed by Eq. \ref{eq:esb},
\begin{equation}\label{eq:esb}
\left. {\begin{array}{*{20}{r}}
{\left[ {{f_{esb,1}}, \!\ldots ,{f_{esb,k}}} \right]}\\
\{G_{edge}\left( {{x_{i}}} \right)\}
\end{array}} \right\} \!\leftarrow \!\mbox{ERB-ResNet}(x).
\end{equation}





\subsubsection{Noise-Sensitive Branch }\label{sssec:nsb}



In order to fully exploit the noise view, we build a noise-sensitive branch (NSB) parallel to ESB. NSB is implemented as a standard FCN (another ResNet-50 as its backbone). Regarding the choice of noise extraction, we adopt BayarConv \cite{Bayar-journal}, which is found to be better than the SRM filter \cite{2020Constrained}. The output of this branch is an array of $k$ feature maps from the last ResNet block of its backbone, \ie
\begin{equation} \label{eq:nsb}
\{f_{nsb,1}, \ldots, f_{nsb,k}\} \leftarrow \mbox{ResNet}(\mbox{BayarConv}(x)).
\end{equation}







\subsubsection{Branch Fusion by Dual Attention}\label{sssec:fusion}


Given two arrays of feature maps $\{f_{esb,1},\ldots,f_{esb,k}\}$ and $\{f_{nsb,1}, \ldots, f_{nsb,k}\}$ from ESB and NSB, we propose to fuse them by a trainable Dual Attention (DA) module \cite{danet}. This is new, because previous work \cite{2018rgbn} uses bilinear pooling for feature fusion, which is non-trainable.

The DA module has two attention mechanisms working in parallel: channel attention (CA) and position attention (PA), see Fig. \ref{fig:da}. CA  associates channel-wise  features to selectively emphasize interdependent channel feature maps. Meanwhile, PA selectively updates features at each position by a weighted sum of the features at all positions. The outputs of CA and PA are summed up, and transformed into a feature map of size $\frac{W}{16}\times \frac{H}{16}$, denoted as $\{G^{'}(x_{i})\}$, by a $1 \times 1$ convolution. With parameter-free bilinear upsampling followed by an element-wise sigmoid function, $\{G^{'}(x_{i})\}$ is transformed into the final segmentation map $\{ G(x_{i})\}$. Fusion by dual attention is conceptually expressed as
\begin{equation}
\left\{\! {\begin{array}{*{5}{l}}
\{G^{'}\!(x_{i})\} \!\leftarrow\! DA([f_{esb,1}\!, \!\ldots \!,\!f_{esb,k},\!f_{nsb,1}, \ldots ,f_{nsb,k}]),\\

\{ G(x_{i})\} \!\leftarrow\! \sigma (\mbox{bilinear-upsampling}( \{G^{'}(x_{i})\} )).
\end{array}} \right.\label{eq:da}
\end{equation}




\begin{figure}[htpb]
    \begin{center}
    \includegraphics[width=\columnwidth]{figure/dual-attention6.6.pdf}
    \end{center}
    \caption{\textbf{Dual Attention }, with its channel attention module shown in blue and its position attention module shown in green.}
    \label{fig:da}
\end{figure}







\subsection{Multi-Scale Supervision} \label{ssec:loss} 

We consider losses at three scales, each with its own target, \ie a pixel-scale loss for improving the model's sensitivity for pixel-level manipulation detection, an edge loss for learning semantic-agnostic features and an image-scale loss for improving the model's specificity for image-level manipulation detection. 

\textbf{Pixel-scale loss}. As manipulated pixels are typically in minority in a given image, we use the Dice loss, found to be effective for learning from extremely imbalanced data \cite{lesion}:
\begin{equation} 
loss_{seg}(x) = 1 - \frac{2 \cdot \sum\nolimits_{i = 1}^{W \times H} G(x_i) \cdot y_i}{\sum\nolimits_{i = 1}^{W \times H} G^2(x_i)  + \sum\nolimits_{i = 1}^{W \times H} y^2_i},
\label{eq:dice}
\end{equation}
where $y_i\in\{0,1\}$ is a binary label indicating whether the $i$-th pixel is manipulated. 







\textbf{Edge loss}. As pixels of an edge are overwhelmed by non-edge pixels, we again use the Dice loss for manipulation edge detection, denoted as $loss_{edg}$. Since manipulation edge detection is an auxiliary task, we do not compute the $loss_{edg}$ at the full size of $W \times H$. Instead, the loss is computed at a much smaller size of $\frac{W}{4} \times \frac{H}{4}$, see Fig. \ref{fig:model}. This strategy reduces computational cost during training, and in the meanwhile, improves the performance slightly. 


\textbf{Image-scale loss}. In order to reduce false alarms, authentic images have to be taken into account in the training stage. This is however nontrivial for the current works \cite{mantranet,HPFCN,2020GSR,2017MFCN} as they all rely on segmentation losses. Consider the widely used binary cross-entropy (BCE) loss for instance. An authentic image with a small percent of its pixels misclassified contributes marginally to the BCE loss, making it difficult to effectively reduce false alarms. Also note that the Dice loss cannot handle the authentic image by definition. Therefore, an image-scale loss is needed. 
We adopt the image-scale BCE loss:
\begin{equation} \label{eq:loss-clf}
loss_{clf}(x) = - ( y \cdot \log G(x)  + (1 - y) \cdot \log(1 - G(x)) )
\end{equation}
where $y=\max(\{y_i\})$.

\textbf{Combined loss}. We use a convex combination of the three losses:
\begin{equation}
    Loss = \alpha \cdot loss_{seg} + \beta \cdot loss_{clf} + (1 - \alpha - \beta) \cdot loss_{edg}
\end{equation} 
where $\alpha, \beta \in(0,1)$ are weights. Note that authentic images are only used to compute $loss_{clf}$.
