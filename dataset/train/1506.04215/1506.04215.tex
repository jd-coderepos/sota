\documentclass[letterpaper,10pt]{article}

\usepackage{etoolbox}
\newtoggle{article}
\toggletrue{article}

\usepackage{amsthm,amsmath,amssymb,amsfonts}
\usepackage{mathpazo}
\usepackage{adjustbox}
\usepackage{placeins}
\usepackage{natbib}
\usepackage{authblk}
\def\cite{\citep}

\usepackage[svgnames]{xcolor}
\usepackage{graphicx}

\usepackage[
  pdfauthor={Mohamed Khochtali, Daniel S. Roche, Xisen Tian},
  pdftitle={Parallel sparse interpolation using small primes},
  colorlinks,
  linkcolor=DarkBlue,
  citecolor=DarkGreen,
  urlcolor=DarkBlue,
  bookmarksnumbered,
  pdfpagelabels=true,
]{hyperref}
\usepackage{doi}

\usepackage[vlined,ruled,boxed,linesnumbered]{algorithm2e}
\DontPrintSemicolon

\usepackage[subject={TODO}]{pdfcomment}
\newcommand{\todo}[1]{\pdfmargincomment[color=blue,icon=Note]{#1}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newtheorem{theorem}{Theorem}
\newtheorem{claim}[theorem]{Conjecture}
\newcommand{\ZZ}{\ensuremath{\mathbb{Z}}}
\newcommand{\softoh}{\ensuremath{\widetilde{O}}}

\title{Parallel sparse interpolation\\ using small primes}

\author{Mohamed Khochtali}
\author{Daniel S.\ Roche}
\author{Xisen Tian}
\affil{\small Computer Science Department\\United States Naval Academy\\
Annapolis, Maryland, USA}


\begin{document}

\maketitle


\begin{abstract}
  To interpolate a supersparse polynomial with integer coefficients, two
  alternative approaches are the Prony-based ``big prime'' technique,
  which acts over a single large finite field, or the more
  recently-proposed ``small primes'' technique, which reduces the unknown
  sparse polynomial to many low-degree dense polynomials.
  While the latter technique
  has not yet reached the same theoretical efficiency as Prony-based
  methods, it has an obvious potential for parallelization. We present a
  heuristic ``small primes'' interpolation algorithm and report on a
  low-level C implementation using FLINT and MPI.
\end{abstract}



\section{Introduction}

Given a way to evaluate or \emph{sample} an unknown function or
procedure, interpolation is the fundamental and important problem of
recovering a formula which accurately and completely describes that
unknown function. As discovering an arbitrary unknown function from a
finite set of evaluations with any reliability would be impossible, some
constraints on the size and form of the output are inevitably required.

Here we consider the problem of \emph{sparse polynomial interpolation},
in which we are guaranteed that the unknown function is a multivariate
polynomial with bounded degree. Sparse interpolation algorithms date to
the 18th century, but have been the focus of considerable recent work in
numeric and symbolic computation,
with applications ranging from power consumption in medical devices, 
to reducing intermediate expression swell in mathematical computations
\cite{CL08,Kal10a,KLY11,BCK12}.

Specifically, this work focuses on algorithms to interpolate an unknown
\emph{supersparse} polynomial with integer coefficients, which make
efficient use of modern \emph{parallel} computing hardware. Focusing on
the ``supersparse'' (a.k.a. ``lacunary'') case means that our running
time will be in terms of the number of variables, number of nonzero
terms, and the logarithm of the output degree. 

The first algorithms to solve this problem in polynomial-time were based
on the exponential sums technique of Prony, and can efficiently solve
the integer problem by working in a large finite field modulo a single
``big prime.'' A number of theoretical improvements and practical
implementations have been made in this vein, including work on fast
parallel implementations \cite{JM10,HL15}.

We consider another type of approach, whereby the unknown sparse
integer polynomial is reduced in degree modulo many small primes. This
technique, first used by Garg and Schost to avoid the need for discrete
logarithm computations in arbitrary finite fields, can also be applied
to the integer polynomial case. While it cannot yet match the
theoretical efficiency of the big primes algorithms, we will show that the
``small primes'' method is very effectively parallelized. Furthermore, we
develop a practical heuristic version of this method which reduces
further the
size and number of primes required based on experimental results rather
than on the theoretical worst-case bounds.

\subsection{Related work}

While it would be impossible to list all of the related work on sparse
interpolation, we will mention some of the most recent results which are
most closely connected to the current study, and which may provide the
reader with useful background.

The now-classical approach to sparse interpolation is variously credited
to Prony, Blahut \cite{Bla79}, or Ben-Or and Tiwari \cite{BT88}. Only
the last of these considered explicitly the case of integer
coefficients, but all share the key property of requiring the minimal
number  of evaluations in order to recover a polynomial with 
 nonzero terms.

We refer to these approaches as ``big prime'' techniques, as the more
modern variants \cite{KY89,Kal10a} adapt to the case of integer
coefficients by choosing carefully a single large modulus, then perform
the interpolation over a finite field in order to avoid exponential
growth in the bit-length of evaluations.

The approach which we take in this work is based on that of Garg and
Schost \cite{GS09}, who developed the first polynomial-time supersparse
interpolation algorithm over an arbitrary finite field. By reducing the
unknown polynomial modulo , the full coefficients
are discovered immediately, but the exponents are only discovered after
repeating for multiple values of . This ``small primes'' approach,
described in more details below, has the considerable advantage of
relying only on low-degree, dense polynomial arithmetic.

There are, of course, other sparse interpolation methods which do not
fit nicely into this big/small prime characterization. Most notable are
Zippel's algorithm and hybrid variants of it \cite{Zip90,KL03}, the
symbolic-numeric method of \cite{Man95}, and the Newton-Hensel
lifting approach of \cite{AKP06}.

We point out some recent work on efficient implementations which are of
particular interest to the current study. In \cite{JM10}, a new variant
of the big prime approach is developed which can be performed variable
by variable, in parallel. More recently, \cite{HL15} investigated a
number of tricks and techniques towards practical, efficient sparse
interpolation, and posed some new benchmark problems. Their methods are
also based on the big prime approach; to our knowledge there has been no
reported implementation work on the small primes technique other than
the numerical interpolation code reported in \cite{GR11a}.

\subsection{Our contributions}

Suppose  is an unknown polynomial in 
variables, with partial degrees less than  in each variable,
at most  nonzero terms, and coefficients less than  in absolute
value. Then  can be written as

where each exponent  and each .

Given a way to evaluate , for any
modulus  and -tuple of evaluation points
,
the \emph{sparse integer polynomial interpolation problem} is to
determine the coefficients  and exponent tuples 
, for each .

We will actually consider a slight relaxation of this problem, wherein
evaluations are of the form

That is, the  variables are replaced by a single indeterminate ,
all coefficients are reduced modulo , and the exponents are
reduced modulo . The reduction modulo  
is possible without affecting the overall
complexity whenever the unknown polynomial  is given as a
straight-line program or algebraic circuit, or if the prime  is
chosen so that  has a th root of unity.

An algorithm for this problem is said to handle the \emph{supersparse} case
if it requires a number of evaluations and running time which are
polynomial in , , , and . This corresponds to the size of
the sparse representation of  as a list of coefficient-exponent
tuples, which requires

bits in memory.

We present a randomized algorithm for the sparse integer polynomial
interpolation problem, derived from the existing literature, whose
running time\footnote{
  Here and throughout we use the \emph{soft-oh} notation in order to
  simplify the stated running times: a running time is said to be 
   for some function  if and only if it is
  .
} for provable correctness is

where  is the number of parallel processors available for the
task. Furthermore, we demonstrate a heuristic variant on this algorithm,
which works well in our experimental testing, and reduces the running
time further to

in the typical case that  and  are both .

We have implemented this heuristic approach using the C library FLINT
for dense polynomial arithmetic and MPI for parallelization. Our
experiments demonstrate the smallest effective settings for the
parameters in our heuristic approach. With those parameters, we show
that the heuristic method is competitive with the state of the art in
the single-processor setting, and that its running
time scales well with increasing numbers of parallel processors.

Specifically, our contributions are:
\begin{enumerate}
  \item A sparse interpolation algorithm whose potential parallel
    speedup is , compared with the  parallel speedup
    that has been shown in previous work for other sparse interpolation algorithms.
  \item A heuristic variant of our algorithm, which is demonstrated to
    be effective
    on our (limited) random experiments, that brings the
    running time complexity of the small primes approach to be
    competitive with that of the big prime approach.
  \item An efficient C implementation of our interpolation algorithm
    which demonstrates its competitiveness on a standard benchmark
    problem.
\end{enumerate}

\subsection{Organization of the paper}

We first outline the two main classes of existing approaches to
supersparse integer polynomial interpolation, which we call the ``big
prime'' and the ``small primes'' methods, in Section~\ref{sec:exist}. We
then present our own heuristic method in Section~\ref{sec:ouralg}, 
which is based
on previously-known ``small primes'' algorithms, but goes beyond
theoretical worst-case bounds on the sizes needed in order to further
improve the efficiency.

Section~\ref{sec:impl} reports the details of our parallel
implementation of this heuristic method, and Section~\ref{sec:exper}
presents the preliminary experimental results which demonstrate the
efficacy of this approach. Finally, we state some conclusions and directions for
further investigation in Section~\ref{sec:conc}.

\section{Existing algorithms for supersparse interpolation}
\label{sec:exist}

\subsection{``Big prime'' methods}

The original sparse interpolation 
algorithm of \cite{BT88} used evaluations at powers of the first 
prime numbers along with Prony's method to deterministically recover the
nonzero integer coefficients and exponents of an unknown polynomial.
This cannot be considered a ``supersparse'' algorithm, as it
performs the evaluations over the integers directly and
requires working with integers with more than  bits.

However, it was soon recognized that, by choosing a single large prime
, with  smooth\footnote{
  An integer is said to be \emph{smooth} if it has only small prime
  factors.
} so as to facilitate discrete logarithms,
a supersparse integer polynomial could be interpolated efficiently
modulo  \cite{KLW90,Kal10a}. The basic steps of this approach are as
follows:

\begin{enumerate}
  \item Choose prime  so that  has a large ``smooth''
    factor greater than  and let  be a primitive element
    modulo .
  \item For , evaluate
    
  \item Compute the minimum polynomial  of the 
    sequence  with the Berlekamp-Massey algorithm.
  \item Factor  over ; each root of  can be
    written as
    , corresponding to
    a single term  of .
  \item Compute  discrete logarithms of the roots, and then the
    -adic expansion of each one, to discover the actual exponents
    , for .
  \item Once the exponents are known, the coefficients can be computed
    from the evaluations  by solving a transposed
    Vandermonde system of dimension .
\end{enumerate}

The two steps which can be trivially parallelized are the evaluations
and discrete log computations on steps 2 and 5. However, the dominating
cost in the complexity is factoring a polynomial over  on step
4. This is also confirmed to be the dominating cost in practice by
\cite{HL15}, and it is not clear how to efficiently parallelize the
factorization. Using the fastest known algorithms, the running time of
this step is  \cite{GHL15}, which is at least 
 bit operations from the condition
.

In the description above, the evaluations at powers of  on step 2 amount to a
Kronecker substitution from multivariate to univariate. Of note is
the algorithm of \cite{JM10}, which uses a different approach than
Kronecker substitution in order to work one variable at a time, gaining
a potential -fold parallel speedup.

\subsection{``Small primes'' methods}

The algorithm described above works the same over an arbitrary finite
field, except that the discrete logarithms required in step 5 cannot be
performed in polynomial-time in general. This difficulty was first
overcome in \cite{GS09}, where the idea is as follows:

\begin{enumerate}
  \item Choose a series of small primes 
  \item Apply the Kronecker substitution and for each 
    compute .
  \item Each  of maximal sparsity contains all the coefficients of
    , and all the exponents modulo . Collect sufficiently many
    modular images of the exponents in order to recover the full
    exponents over .
  \item Recover the multivariate exponents by -adic expansion of each
    univariate exponent, and use any  of maximal sparsity to
    discover the coefficient of each term.
\end{enumerate}

There are two significant challenges of this approach. The first is
that, in reducing the polynomial modulo , it is possible that
some exponents are equivalent modulo , and then multiple terms in
the original polynomial will \emph{collide} to form a single term in
. By choosing random primes whose values are roughly
, the probability of encountering any collisions can
be made arbitrarily small.

The second challenge is how to correlate the exponents from different
's in step 3, in order to recover the full exponents via Chinese
remaindering. The approach of \cite{GS09} was to compute an auxiliary
polynomial whose roots are the unknown exponents; however this
increases the overall running time to .

The technique of \emph{diversification} in \cite{GR11a} is another
randomization which chooses a random element  and interpolates
 instead of  itself. With high probability, the
``diversified'' polynomial  has distinct coefficients,
which can then be used to correlate the exponents in different 's.
This avoids the factoring step and reduces the complexity by a factor of
.

Subsequently, and separately, \cite{AGR13} showed how to allow the
magnitude of each  to decrease by a factor of , by allowing some
constant fraction of the terms in each  to collide. These separate
approaches are combined and further improved in \cite{AGR15},
which in the typical case that
, brings the theoretical complexity down to
. Observe that this is competitive with the
``big primes'' algorithm, but could be slower by up to a factor of
. 

The algorithm we describe next first shows how to effectively
parallelize this small primes approach, and then further reduces the
complexity through a heuristic argument. After the gains from
parallelization or from the heuristic improvement, the complexity of our
algorithm will be less than the ``big primes'' approach as well.

\section{Our parallel small primes algorithm}
\label{sec:ouralg}

\begin{procedure}[tb]
\caption{SparseInterp()\label{proc:interp}}
\KwIn{Bounds  for an -variate sparse polynomial  in 
variables, with  nonzero terms, partial degrees less than , and
integer coefficients less than  in absolute value; and parameters
.}
\KwOut{A list of  coefficients  and exponent tuples
. If  are sufficiently large, these
coefficients and exponents comprise the sparse representation of 
with high probability.}

 random prime in the range  \;
 random element of  \;
 \;
 \;
 \;
 thread-safe list of integer triples \;
\For{ in parallel}{
   random prime in the range  \;
   
    \label{step:Dpow} \;
   using dense polynomial arithmetic \label{step:eval} \;
  \For{}{
     coefficient of  in  \;
    \lIf{}{Add  to 
    \label{step:growL}}
  }
}
Sort  in parallel by the coefficients  in each triple \;
 thread-safe list of coefficient/exponent tuples \;
\ForEach{Unique coefficient  in  in parallel}{
   
    the  exponents and primes appearing with coefficient 
    in  \;
  \If{}{
     least integer s.t.\  \;
     least integer s.t.\ 
       for  via
      Chinese remaindering \;
    , stored as an integer in the range
       \;
     -adic expansion of  \;
    Add  and  to \;
  }
}
Sort  in parallel by the exponents and \textbf{return} the resulting sparse
polynomial
\end{procedure}

Our algorithm, which is detailed in procedure \ref{proc:interp},
is based on the reduction idea in \cite{GS09}, with the
diversification method introduced by \cite{GR11a} and the partial
collisions handling of \cite{AGR13}. It depends crucially on the
parameters  and , which in theory grow as  and
, respectively, but according to our heuristic method can both be
treated as constants.

\subsection{Algorithm overview}

The main idea is first to choose a prime  large enough to recover the
coefficients, and then to apply the Kronecker substitution so that we
are really interpolating a univariate polynomial 
with coefficients in :

Each term  of the original polynomial 
maps uniquely to a term with exponent
 and coefficient
 in .

The parameter  controls the size of the prime , and so should
always be set larger than  in order to recover the full precision of
the coefficients. However, it may be necessary to set  even larger
than this when the height bound  is very small. We comment that
choosing a random prime  (rather than one with special properties, as
in the ``big primes'' algorithm) is important for the probabilistic
analysis below.

The evaluation phase of the algorithm computes the polynomial
 modulo  using
dense arithmetic, for many small primes . The details of 
how this evaluation is performed
will depend on the particular application. In our
multivariate multiplication application below, the exponents of the
original multiplicands are reduced modulo , followed by dense
polynomial arithmetic in the ring of polynomials modulo . More
generally, if the unknown polynomial  is given as a straight-line
program or arithmetic circuit, each operation in the circuit can be
computed over that ring , as in
\cite{GS09}. In our complexity analysis below, we assume a cost of
 for each evaluation on
this step, according to the cost of dense
degree- polynomial arithmetic over .

The next phase of the algorithm is to gather the images of each term,
discard those which appear infrequently (and thus were resulting from
collisions in the reduction modulo ), and use Chinese
remaindering to recover the exponents of each nonzero term in .

Observe that the list  can be implemented in any convenient way
according to the details of the parallel implementation; it serves only
as an unordered accumulator of coefficient-exponent-prime tuples.
The output polynomial  could also be considered as an unordered
accumulator, followed by another parallel sort before the final return
statement.

\subsection{Parallel complexity analysis}

We state the parameterized running time of the algorithm as follows:

\begin{theorem}\label{thm:rtime}
  Given bounds  on the sparsity and degree of an unknown
  polynomial , and parameters ,
  Algorithm \ref{proc:interp} has worst-case running time
  
\end{theorem}

\begin{proof}
  The computation of  at the beginning incurs the 
  cost in the complexity, which for our ultimate choices of  will
  never actually dominate the complexity.

  The first loop executes 
  
  times in each thread.
  Computing the powers of  modulo  on Step~\ref{step:Dpow}
  requires  bit operations. The subsequent
  evaluations using dense arithmetic on Step~\ref{step:eval} will
  usually dominate the complexity of the entire algorithm, as each costs
  , which is .
  (The addition of this term makes the  factor in the cost of
  Step~\ref{step:Dpow} become .)

  Both parallel sorts are on lists of size at most , which means
  their cost of  does not
  dominate the complexity.

  The final for loop executes  times in each
  thread, but the nested if statement can only be triggered  times
  overall. Within it, the most expensive step is computing each
  , requiring 
  bit operations. This contributes only
   to the overall complexity, as
  the term  is already dominated by the
  parallel cost of Step~\ref{step:eval}.
\end{proof}

The key feature of procedure \ref{proc:interp} is that its potential
parallel speedup, from the previous theorem, is a factor of 
, depending on the number of parallel processors 
that are available. This exceeds the  parallel speedup of previous
approaches, and means that our algorithm should scale better to a large
number of processors when the number of variables and/or degree are
sufficiently large.

\subsection{Correctness and probability analysis}

We first use prior work to prove the bounds necessary to ensure
correctness with provably high probability. 

\begin{theorem}\label{thm:bounds}
  If the parameters  and bounds  satisfy 
  
  then with
  probability at least , procedure \ref{proc:interp} correctly
  computes the coefficients and exponents of .
\end{theorem}
\begin{proof}
  The condition  guarantees that positive or negative
  coefficients whose absolute value is at most  will all be distinct
  modulo , for any prime  as chosen in the algorithm.

  Because , Lemma 8 from \cite{AGR13}
  tells us that, for each , the probability that more than 
  terms collide modulo  is at most .
  Since the most number of terms that could collide is , this means
  the \emph{expected} number of collisions, for each , is at
  most .

  The total number of nonzero coefficients in all polynomials  examined on
  step~\ref{step:growL} is at most . Many of
  these correspond to single terms in  itself, but some will be
  collisions of terms in . Using the reasoning of Lemma 4.1 in
  \cite{AGR14}, and the proof of Theorem 3.1 from \cite{GR11a},
  every coefficient of a single term in , or a collision that appears
  in any of the 's, is distinct modulo , due to the bound on
   and because .

  We conclude that, with probability at least , each term in 
  appears un-collided in at least  of the polynomials , and
  furthermore that these coefficients are all distinct and are the only
  ones that repeat in the list . This guarantees that the correct
  coefficients and exponents are recovered in the final loop, and the
  algorithm outputs the correct interpolated polynomial.
\end{proof}

Applying the bounds in Theorem~\ref{thm:bounds} to the analysis in
Theorem~\ref{thm:rtime} gives the provable complexity bound for the
algorithm as stated in \eqref{eqn:rtg}.

As usual, the probability of success in either the provable or the
heuristic version of the algorithm (described below)
can be increased arbitrarily high by
running the same algorithm repeatedly and choosing the most common
polynomial returned among all runs to be the most likely candidate for
.

\subsection{Heuristic approach}

The heuristic version of this procedure is simply to choose
appropriate constants for the parameters  and , with the
intuition that there can be some trade-off between the size of each prime
(governed by ) and the number of chosen primes (governed by ).
Furthermore, the bound on  required in theory to obtain
diversification between the coefficients in  and in any collisions is
unnecessarily high for most ``typical'' polynomials. There do exist
pathological counterexamples, but they require the degrees of many terms
to be equivalent modulo . As the prime  is also chosen randomly in
our approach, we have a good indication that this heuristic approach
will work with high probability for a randomly-chosen sparse polynomial.
We state this as a conjecture, which is also backed by the experimental
evidence reported in Section~\ref{sec:exper} below.

\begin{claim}
  For any sufficiently large height bound , and using , 
  there exist constants
   such that, for a polynomial 
  chosen at random with at most  nonzero terms, the probability that
  algorithm \ref{proc:interp} successfully interpolates  is at least
  1/2.
\end{claim}

The heuristic complexity under this conjecture is stated in \eqref{eqn:rth}.


\subsection{Example}



 Consider an unknown bivariate
 polynomial  with 3 nonzero terms and degree less
 than 10.

The primes that we are going to use are going to be small for the sake of this 
example. The primes are  7, 13, and 17.

\begin{enumerate}


 \item Now we compute  modulo , we get:

 

  

  We notice here that a collision happened with the prime 13. This won't affect our 
  calculation later because resulting coefficient  doesn't appear anywhere else 
  (For this example, we are going to assume that we have a good coefficient if it 
  appears twice or more).
    
  Now we use these values to fill the list . Every triple in 
  consists of the coefficient, the prime, and the exponent.
  
  
    
  We then sort  based on coefficient values.

  
  
  We use the Chinese remainder theorem to get back the exponent that corresponds 
  to each coefficient.

  

  
    
  
    
  This results in the univariate polynomial
  

  Finally, inverting the Kronecker map ,
  we obtain
  

\end{enumerate}

\iftoggle{article}{\FloatBarrier}{}

\section{Parallel implementation}
\label{sec:impl}

We completed a low-level implementation of Procedure \ref{proc:interp}
written in the C programming language. Our complete source code, as well
as the exact source we tested for the comparisons and benchmark problems
listed later, is available upon request by email. 

We give a few details
here on the choices of our implementation, in particular the libraries
that were utilized.

\subsection{FLINT for sparse and dense polynomial arithmetic}

The key advantage to the ``small primes'' approach which we employed is
the reliance on fast subroutines for dense polynomial arithmetic. The
experiments we ran always used a word-sized modulus , and so the most
expensive computations involved computing with dense, low-degree
polynomials with word-sized modular coefficients.

FLINT (\url{http://flintlib.org/}) is a free, open-source C library for
fast number theoretic computations \cite{flint}. Our dense polynomial
arithmetic, which is the dominating cost both in theory and in practice
in our experiments, was performed using the \verb|nmod_poly| data type.

In order to store the result of sparse interpolation and complete the
correctness testing in our experiments, we also added rudimentary
support for sparse integer polynomials on top of FLINT. We created a new
data type, \verb|fmpz_sparse|, to represent sparse univariate
polynomials in  for testing purposes, using FLINT's multiple
precision type \verb|fmpz| as both the coefficient and exponent storage
for sparse polynomials. 

Note that using multiple-precision integers for
exponents is especially important, as we have \emph{not} yet completed
full multivariate polynomial support within FLINT. Instead, for the
purposes of our experiments, we always used a standard Kronecker
substitution to store -variate, degree- 
multivariate polynomials as univariate sparse
polynomials with degree bounded by . Employing a multiple-precision
data type allows for the largest possible degree and number of
variables, which is crucial as it is precisely this \emph{supersparse}
case in which our approach has the greatest potential parallel speedup.

\subsection{MPI for multi-processor parallelism}
 Message Passing Interface (MPI) allows us to parallelize our algorithm. 
 As stated earlier, since the slowest part of our algorithm involves calculating 
 the unknown polynomial  modulo many polynomials , we
 use MPI to perform each of these evaluations in parallel.

 The function \verb|MPI_Init|
 is called at the beginning of the program to spawn an arbitrary number
 of processes, as specified on the command line.
 Each of the allocated processes will be
 executing separately with separate copies of all variables in the original 
 process. All processes will have unique id numbers. The root process
 will have id number 0. By knowing processes id's we can separate what each
 process executes. 
 
 We used a master-slave model for our algorithm. The master 
 process evenly distributes how many primes each slave calculates. After getting 
 the primes, the slave process will compute the following for every prime:
 , then
 traverse the resulting univariate polynomial 
 and save all nonzero terms, along with the prime , to an array.
 This array of triples is sent back to the master process, which later
 sorts all the concatenated evaluation arrays
 and uses Chinese remaindering to recover the full polynomial
 , as described above.

 While our experiments were performed on a multi-core machine, and hence
 using a simpler threading library would have also worked, our goal in
 using MPI was to demonstrate the full parallel potential of this
 approach by explicitly detailing the inter-process communication.
 Furthermore, the MPI implementation could also be used without
 modification on heterogeneous clusters or other architectures besides
 multi-core.
  
\section{Experimental results}
\label{sec:exper}

We ran our tests on a machine using an Intel(R) Core(TM) i7-3930K CPU @ 3.20GHz 
simulating 12 hyper-threaded cores on 6 physical cores, with 32 GB of RAM. 
We used a Debian GNU system, running the ``unstable'' branch, 
with Linux kernel version 3.16.0-4-amd64. This is a bleeding-edge system
with the most current versions of all software available within the
Debian repositories.

\subsection{Determining the parameters  and }

The first task was to determine experimentally what kind of settings for
the parameters  and  would be appropriate for our heuristic
interpolation method. We found that  and  worked for a
wide range of problem sizes with almost no failures in the randomized
algorithm.

The only exceptions we found here were that when the bound on the number of
terms  in the output was very small, even setting  the number
of primes  in the range  was simply not
sufficient to ensure a high success probability. In these small-size
extremes, the value of  was increased to accommodate; in particular, 
we settled on  and
 when , and when  we had to select 
. Under these parameter settings, no failures were
observed in any of our experiments.

We comment that a smaller  value and a
larger  value would be preferable, because that 
would increase the number of primes  and
hence the potential parallel speedup
for the algorithm, while decreasing the size  of each prime
. However, we found that modestly larger values for  did not
allow for  to be reduced and consistently result in correct output.
We consider the exploration of some better balance between the
parameters  and  as future work.

\subsection{Single-threaded performance}

\begin{table*}[tbp]
\iftoggle{article}{\begin{adjustbox}{center}}{\begin{center}}

\begin{tabular}{*{6}{r}*{3}{|r}}
\multicolumn{1}{c}{} &
\multicolumn{1}{c}{variables} &
\multicolumn{1}{c}{terms} &
\multicolumn{1}{c}{max-degree} &
\multicolumn{1}{c}{} &
\multicolumn{1}{c}{} &
\multicolumn{1}{|c|}{Mathemagix} &
\multicolumn{1}{|c|}{Ours (single thread)} &
\multicolumn{1}{|c}{Ours (12 threads)} \\
\hline
1 & 20 & 3    & 40  & 14 & 50\,000  & 0.078 & 0.035 & 0.029 \\
2 & 20 & 9    & 80  & 18 & 5\,000   & 0.155 & 0.151 & 0.048 \\
3 & 20 & 27   & 120 & 18 & 6\,900   & 0.305 & 0.329 & 0.116 \\
4 & 20 & 81   & 160 & 18 & 4\,900   & 0.598 & 0.323 & 0.085 \\
5 & 20 & 243  & 200 & 17 & 9\,900   & 2.156 & 0.785 & 0.175 \\
6 & 20 & 729  & 240 & 15 & 39\,900  & 5.053 & 3.084 & 0.814 \\
7 & 20 & 2187 & 280 & 14 & 80\,050  & 13.333 & 8.714 & 2.225 \\
8 & 20 & 6561 & 320 & 13 & 321\,300 & 41.070 & 43.911 & 10.605 \\
\end{tabular}

\iftoggle{article}{\end{adjustbox}}{\end{center}}
\caption{Sparse interpolation benchmark times (in
seconds)\label{tab:comp}}
\end{table*}

\begin{figure}[tbp]
\includegraphics[width=\linewidth]{oursVsThem.png}
\caption{Comparison With Mathemagix Sparse Interpolation Program\label{fig:comparison}}
\end{figure}

The first experiment was to test our algorithm 
without parallelization against
the efficient ``big primes'' implementation in Mathemagix 
(\url{http://www.mathemagix.org/}), as
reported in \cite{HL15}.
We downloaded the source code for the Mathemagix sparse
interpolation program, then ran it with  polynomials 
being multiplied. We kept each polynomial at 20 variables, with degree 40, 3 terms 
and coefficients up to . Then we ran our algorithm with the same parameters. 
The results are shown in Taible~\ref{tab:comp} and summarized in
Figure~\ref{fig:comparison}. 
(We also
attempted a comparison against the \texttt{sinterp} function in Maple 2015,
but it was more than an order of magnitude slower in all experiments.)

Our non-parallelized algorithm seems to be on par with the Mathemagix
implementation for this range of problem sizes. As a comparison, and to
emphasize the main point of our paper, we also show the full parallel
speedup for the same problems in Figure~\ref{fig:comparison}. 

\subsection{Parallel speedup}

\begin{figure}[tbp]
\includegraphics[width=\linewidth]{multiSpeed_med.png}
\caption{Parallel speedup for our implementation\label{fig:speedup}}
\end{figure}

The second experiment was to test the parallel speedup for our implementation by 
varying the degree size  and leaving , , partial , and  constant. 
was varied from , , . This test was performed 3 separate times 
and the resultant data was calculated from median of the three trials. 
The results are seen in Figure~\ref{fig:speedup}. 

Figure~\ref{fig:speedup} shows a linear speedup increase as the number of 
parallel processes used gets closer to the physical number of cores on the machine.
Additionally, we can see the most significant parallel speedup occurs for the highest
degree tested. Recall that on our machine, there are only 6 physical
cores that are hyper-threaded to 12 virtual cores. Furthermore, when
running only six threads, the ``turbo mode'' clock rate is increased to
3.8GHz. This may help to
explain the dip in performance seen around  parallel processes.

Observe also that the parallel speedup is best, and continues the
furthest, in the most extremely sparse case with very high degree.

Our parallel speedup is demonstrated in Figure~\ref{fig:speedup}.

\section{Conclusions and future work}
\label{sec:conc}

We have shown that the ``small primes'' sparse interpolation algorithm
is competitive with the state of the art, even without parallelization,
especially for very sparse problem instances. Furthermore, there is
greater potential for parallelism in the small primes technique. These
theoretical results are borne out in practice in our experimental results
compared to other available software implementations.

There is significantly more work to be done, however, before we might
suggest widespread adoption of our heuristic sparse interpolation
method. We would like to understand the theory behind the heuristic
approach in order to have a less \emph{ad hoc} way of determining the
parameters  and . On the other hand, our implementation could
be greatly enhanced with further experimentation on a wider range of
benchmark problems and incorporating true multivariate sparse polynomial
representations.



\section*{Acknowledgments}
This work was supported by the National Science Foundation under
award number \#1319994. We thank Andrew Arnold and the anonymous PASCO
2015 reviewers for their comments on an earlier draft of this paper.

\renewcommand{\bibpreamble}{\addcontentsline{toc}{section}{References}}
\bibliographystyle{plainnat}

\newcommand{\Gathen}{\relax}\newcommand{\Hoeven}{\relax}
\begin{thebibliography}{21}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arnold et~al.(2013)Arnold, Giesbrecht, and Roche]{AGR13}
Andrew Arnold, Mark Giesbrecht, and Daniel~S. Roche.
\newblock Faster sparse interpolation of straight-line programs.
\newblock In Vladimir~P. Gerdt, Wolfram Koepf, Ernst~W. Mayr, and Evgenii~V.
  Vorozhtsov, editors, \emph{Proc. Computer Algebra in Scientific Computing
  (CASC 2013)}, volume 8136 of \emph{Lecture Notes in Computer Science}, pages
  61--74. Springer, September 2013.
\newblock \doi{10.1007/978-3-319-02297-0_5}.

\bibitem[Arnold et~al.(2014)Arnold, Giesbrecht, and Roche]{AGR14}
Andrew Arnold, Mark Giesbrecht, and Daniel~S. Roche.
\newblock Sparse interpolation over finite fields via low-order roots of unity.
\newblock In \emph{Proceedings of the 39th International Symposium on Symbolic
  and Algebraic Computation}, ISSAC '14, pages 27--34, New York, NY, USA, 2014.
  ACM.
\newblock \doi{10.1145/2608628.2608671}.

\bibitem[Arnold et~al.(2015)Arnold, Giesbrecht, and Roche]{AGR15}
Andrew Arnold, Mark Giesbrecht, and Daniel~S. Roche.
\newblock Faster sparse multivariate polynomial interpolation of straight-line
  programs.
\newblock \emph{CoRR}, abs/1412.4088, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.4088}.

\bibitem[Avenda{\~n}o et~al.(2006)Avenda{\~n}o, Krick, and Pacetti]{AKP06}
Mart{\'{\i}}n Avenda{\~n}o, Teresa Krick, and Ariel Pacetti.
\newblock Newton-{H}ensel interpolation lifting.
\newblock \emph{Found. Comput. Math.}, 6\penalty0 (1):\penalty0 81--120, 2006.
\newblock ISSN 1615-3375.
\newblock \doi{10.1007/s10208-005-0172-3}.

\bibitem[Ben-Or and Tiwari(1988)]{BT88}
Michael Ben-Or and Prasoon Tiwari.
\newblock A deterministic algorithm for sparse multivariate polynomial
  interpolation.
\newblock In \emph{Proceedings of the twentieth annual ACM symposium on Theory
  of computing}, STOC '88, pages 301--309, New York, NY, USA, 1988. ACM.
\newblock ISBN 0-89791-264-0.
\newblock \doi{10.1145/62212.62241}.

\bibitem[Blahut(1979)]{Bla79}
R.E. Blahut.
\newblock Transform techniques for error control codes.
\newblock \emph{IBM Journal of Research and Development}, 23\penalty0
  (3):\penalty0 299--315, May 1979.
\newblock \doi{10.1147/rd.233.0299}.

\bibitem[Boyer et~al.(2012)Boyer, Comer, and Kaltofen]{BCK12}
Brice Boyer, Matthew~T. Comer, and Erich~L. Kaltofen.
\newblock Sparse polynomial interpolation by variable shift in the presence of
  noise and outliers in the evaluations.
\newblock In \emph{Electr. Proc. Tenth Asian Symposium on Computer Mathematics
  (ASCM 2012)}, 2012.

\bibitem[Cuyt and Lee(2008)]{CL08}
Annie Cuyt and {Wen-shin} Lee.
\newblock A new algorithm for sparse interpolation of multivariate polynomials.
\newblock \emph{Theoretical Computer Science}, 409\penalty0 (2):\penalty0
  180--185, 2008.
\newblock ISSN 0304-3975.
\newblock \doi{10.1016/j.tcs.2008.09.002}.
\newblock Symbolic-Numerical Computations.

\bibitem[Garg and Schost(2009)]{GS09}
Sanchit Garg and {\'E}ric Schost.
\newblock Interpolation of polynomials given by straight-line programs.
\newblock \emph{Theoretical Computer Science}, 410\penalty0 (27-29):\penalty0
  2659--2662, 2009.
\newblock ISSN 0304-3975.
\newblock \doi{10.1016/j.tcs.2009.03.030}.

\bibitem[Giesbrecht and Roche(2011)]{GR11a}
Mark Giesbrecht and Daniel~S. Roche.
\newblock Diversification improves interpolation.
\newblock In \emph{Proceedings of the 36th international symposium on Symbolic
  and algebraic computation}, ISSAC '11, pages 123--130, New York, NY, USA,
  2011. ACM.
\newblock ISBN 978-1-4503-0675-1.
\newblock \doi{10.1145/1993886.1993909}.

\bibitem[Grenet et~al.(2015)Grenet, \Hoeven{van der Hoeven}, and Lecerf]{GHL15}
Bruno Grenet, Joris \Hoeven{van der Hoeven}, and Gr{\'e}goire Lecerf.
\newblock Randomized root finding over finite {FFT}-fields using tangent
  {G}raeffe transforms.
\newblock In \emph{Proc. 40th International Symposium on Symbolic and Algebraic
  Computation}, ISSAC '15, page to appear, 2015.

\bibitem[Hart et~al.(2013)Hart, Johansson, and Pancratz]{flint}
W.~Hart, F.~Johansson, and S.~Pancratz.
\newblock {FLINT}: {F}ast {L}ibrary for {N}umber {T}heory, 2013.
\newblock Version 2.4.0, \url{http://flintlib.org}.

\bibitem[\Hoeven{van der Hoeven} and Lecerf(2015)]{HL15}
Joris \Hoeven{van der Hoeven} and Gr{\'e}goire Lecerf.
\newblock Sparse polynomial interpolation in practice.
\newblock \emph{ACM Commun. Comput. Algebra}, 48\penalty0 (3/4):\penalty0
  187--191, February 2015.
\newblock \doi{10.1145/2733693.2733721}.

\bibitem[Javadi and Monagan(2010)]{JM10}
Seyed Mohammad~Mahdi Javadi and Michael Monagan.
\newblock Parallel sparse polynomial interpolation over finite fields.
\newblock In \emph{Proceedings of the 4th International Workshop on Parallel
  and Symbolic Computation}, PASCO '10, pages 160--168, New York, NY, USA,
  2010. ACM.
\newblock ISBN 978-1-4503-0067-4.
\newblock \doi{10.1145/1837210.1837233}.

\bibitem[Kaltofen and Lee(2003)]{KL03}
Erich Kaltofen and {Wen-shin} Lee.
\newblock Early termination in sparse interpolation algorithms.
\newblock \emph{Journal of Symbolic Computation}, 36\penalty0 (3-4):\penalty0
  365--400, 2003.
\newblock ISSN 0747-7171.
\newblock \doi{10.1016/S0747-7171(03)00088-9}.
\newblock ISSAC 2002.

\bibitem[Kaltofen and Yagati(1989)]{KY89}
Erich Kaltofen and Lakshman Yagati.
\newblock Improved sparse multivariate polynomial interpolation algorithms.
\newblock In P.~Gianni, editor, \emph{Symbolic and Algebraic Computation},
  volume 358 of \emph{Lecture Notes in Computer Science}, pages 467--474.
  Springer Berlin / Heidelberg, 1989.
\newblock \doi{10.1007/3-540-51084-2_44}.

\bibitem[Kaltofen et~al.(1990)Kaltofen, Lakshman, and Wiley]{KLW90}
Erich Kaltofen, Y.~N. Lakshman, and John-Michael Wiley.
\newblock Modular rational sparse multivariate polynomial interpolation.
\newblock In \emph{Proceedings of the international symposium on Symbolic and
  algebraic computation}, ISSAC '90, pages 135--139, New York, NY, USA, 1990.
  ACM.
\newblock ISBN 0-201-54892-5.
\newblock \doi{10.1145/96877.96912}.

\bibitem[Kaltofen(2010)]{Kal10a}
Erich~L. Kaltofen.
\newblock Fifteen years after {DSC} and {WLSS2}: {W}hat parallel computations
  {I} do today [invited lecture at {PASCO} 2010].
\newblock In \emph{Proceedings of the 4th International Workshop on Parallel
  and Symbolic Computation}, PASCO '10, pages 10--17, New York, NY, USA, 2010.
  ACM.
\newblock ISBN 978-1-4503-0067-4.
\newblock \doi{10.1145/1837210.1837213}.

\bibitem[Kaltofen et~al.(2011)Kaltofen, Lee, and Yang]{KLY11}
Erich~L. Kaltofen, Wen-shin Lee, and Zhengfeng Yang.
\newblock Fast estimates of hankel matrix condition numbers and numeric sparse
  interpolation.
\newblock In \emph{Proceedings of the 2011 International Workshop on
  Symbolic-Numeric Computation}, SNC '11, pages 130--136, New York, NY, USA,
  2011. ACM.
\newblock ISBN 978-1-4503-0515-0.
\newblock \doi{10.1145/2331684.2331704}.

\bibitem[Mansour(1995)]{Man95}
Yishay Mansour.
\newblock Randomized interpolation and approximation of sparse polynomials.
\newblock \emph{SIAM Journal on Computing}, 24\penalty0 (2):\penalty0 357--368,
  1995.
\newblock \doi{10.1137/S0097539792239291}.

\bibitem[Zippel(1990)]{Zip90}
Richard Zippel.
\newblock Interpolating polynomials from their values.
\newblock \emph{Journal of Symbolic Computation}, 9\penalty0 (3):\penalty0
  375--403, 1990.
\newblock ISSN 0747-7171.
\newblock \doi{10.1016/S0747-7171(08)80018-1}.
\newblock Computational algebraic complexity editorial.

\end{thebibliography}

\end{document}  
