\documentclass{article} \usepackage{iclr2019_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}

\iclrfinalcopy

\title{Global-to-local Memory Pointer Networks for Task-Oriented Dialogue}



\author{Chien-Sheng Wu\thanks{All work was done while the first author was an intern at Salesforce Research.} , Richard Socher \& Caiming Xiong \\
Salesforce Research \\
\texttt{\{rsocher,cxiong\}@salesforce.com} \\
The Hong Kong University of Science and Technology \\
\texttt{jason.wu@connect.ust.hk} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.






\end{abstract}

\section{Introduction}
Task-oriented dialogue systems aim to achieve specific user goals such as restaurant reservation or navigation inquiry within a limited dialogue turns via natural language. Traditional pipeline solutions are composed of natural language understanding, dialogue management and natural language generation~\citep{young2013pomdp,wen2016network}, where each module is designed separately and expensively. In order to reduce human effort and scale up between domains, end-to-end dialogue systems, which input plain text and directly output system responses, have shown promising results based on recurrent neural networks ~\citep{zhao2017generative,lei2018sequicity} and memory networks~\citep{sukhbaatar2015end}. These approaches have the advantages that the dialogue states are latent without hand-crafted labels and eliminate the needs to model the dependencies between modules and interpret knowledge bases (KB) manually.

However, despite the improvement by modeling KB with memory network~\citep{bordes2016learning,mem2seq}, end-to-end systems usually suffer from effectively incorporating external KB into the system response generation. The main reason is that a large, dynamic KB is equal to a noisy input and hard to encode and decode, which makes the generation unstable. Different from chit-chat scenario, this problem is especially harmful in task-oriented one, since the information in KB is usually the expected entities in the response. For example, in Table~\ref{TB:EXAMPLE} the driver will expect to get the correct address to the gas station other than a random place such as a hospital. Therefore, pointer networks~\citep{vinyals2015pointer} or copy mechanism~\citep{guEtAl2016} is crucial to successfully generate system responses because directly copying essential words from the input source to the output not only reduces the generation difficulty, but it is also more like a human behavior. For example, in Table~\ref{TB:EXAMPLE}, when human want to reply others the \textit{Valero}'s address, they will need to ``copy'' the information from the table to their response as well. 


Therefore, in the paper, we propose the global-to-local memory pointer (GLMP) networks, which is composed of a global memory encoder, a local memory decoder, and a shared external knowledge. Unlike existing approaches with copy ability~\citep{gulcehreEtAl2016,guEtAl2016,eric-manning:2017:EACLshort,mem2seq}, which the only information passed to decoder is the encoder hidden states, our model shares the external knowledge and leverages the encoder and the external knowledge to learn a global memory pointer and global contextual representation. Global memory pointer modifies the external knowledge by softly filtering words that are not necessary for copying. Afterward, instead of generating system responses directly, the local memory decoder first uses a sketch RNN to obtain sketch responses without slot values but sketch tags, which can be considered as learning a latent dialogue management to generate dialogue action template. Then the decoder generates local memory pointers to copy words from external knowledge and instantiate sketch tags. 

We empirically show that GLMP can achieve superior performance using the combination of global and local memory pointers. In simulated out-of-vocabulary (OOV) tasks in the bAbI dialogue dataset~\citep{bordes2016learning}, GLMP achieves 92.0\% per-response accuracy and surpasses existing end-to-end approaches by 7.5\% in full dialogue. In the human-human dialogue dataset~\citep{ericKVR2017}, GLMP is able to surpass the previous state of the art on both automatic and human evaluation, which further confirms the effectiveness of our double pointers usage.

\begin{table}[t]
\setlength{\abovecaptionskip}{-1pt} 
\caption{An in-car assistant example on the navigation domain. The left part is the KB information and the right part is the conversation between a driver and our system.}
\label{TB:EXAMPLE}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|l|l|l|ll|l}
\cline{1-5} \cline{7-8}
\textbf{Point of interest (poi)} & \textbf{Distance} & \textbf{Traffic} & \textbf{Poi type} & \textbf{Address} &  & Driver & I need gas \\ \cline{1-5} \cline{7-8} 
Toms house & 3 miles & heavy & friend's house & 580 Van Ness Ave &  & System & GLMP: There is a gas station locally Valero is 4 miles away \\ \cline{1-5}
Coupa & 2 miles & moderate & coffee or tea place & 394 Van Ness Ave &  &  & Gold: Valero is 4 miles away \\ \cline{1-5} \cline{7-8} 
Panda express & 2 miles & no & Chinese restaurant & 842 Arrowhead Way &  & Driver & What is the address ? \\ \cline{1-5} \cline{7-8} 
Stanford express care & 5 miles & no & hospital & 214 El Camino Real &  & System & GLMP: Valero is located at 200 Alester Ave \\ \cline{1-5}
Valero & 4 miles & heavy & gas station & 200 Alester Ave &  &  & Gold: Valero is at 200 Alester Ave \\ \cline{1-5} \cline{7-8} 
Starbucks & 1 miles & heavy & coffee or tea place & 792 Bedoin St &  & Driver & Thank you! \\ \cline{1-5} \cline{7-8} 
\end{tabular}
}
\end{center}
\vspace{-1.0\baselineskip}
\end{table}

\section{GLMP Model}
Our model~\footnote{https://github.com/jasonwu0731/GLMP} is composed of three parts: global memory encoder, external knowledge, and local memory decoder, as shown in Figure~\ref{FIG:MODEL1}(a). The dialogue history  and the KB information  are the input, and the system response  is the expected output, where  are the corresponding lengths. First, the global memory encoder uses a context RNN to encode dialogue history and writes its hidden states into the external knowledge. Then the last hidden state is used to read the external knowledge and generate the global memory pointer at the same time. On the other hand, during the decoding stage, the local memory decoder first generates sketch responses by a sketch RNN. Then the global memory pointer and the sketch RNN hidden state are passed to the external knowledge as a filter and a query. The local memory pointer returns from the external knowledge can copy text from the external knowledge to replace the sketch tags and obtain the final system response.

\subsection{External Knowledge}
Our external knowledge contains the global contextual representation that is shared with the encoder and the decoder. To incorporate external knowledge into a learning framework, end-to-end memory networks (MN) are used to store word-level information for both structural KB (KB memory) and temporal-dependent dialogue history (dialogue memory), as shown in Figure~\ref{FIG:MODEL1}(b). In addition, the MN is well-known for its multiple hop reasoning ability ~\citep{sukhbaatar2015end}, which is appealing to strengthen copy mechanism. 

\textbf{Global contextual representation.} In the KB memory module, each element  is represented in the triplet format as (Subject, Relation, Object) structure, which is a common format used to represent KB nodes~\citep{millerEtAl2016,ericKVR2017}. For example, the KB in the Table~\ref{TB:EXAMPLE} will be denoted as (\textit{Tom's house, distance, 3 miles), ..., (Starbucks, address, 792 Bedoin St)}. On the other hand, the dialogue context  is stored in the dialogue memory module, where the speaker and temporal encoding are included as in ~\cite{bordes2016learning} like a triplet format. For instance, the first utterance from the driver in the Table~\ref{TB:EXAMPLE} will be denoted as (\textit{\user, turn1, need), (\\}Object(.)C = (C^1,\dots,C^{K+1})C^k \in \R^{|V|\times d_{emb}}K|V|d_{emb}M = [B; X] = (m_1,\dots,m_{n+l})m_iq^1Kkc^k_i = B(C^k(m_i)) \in \R^{d_{emb}}i^{th}C^kq^kkB(.)p^k \in \R^{n+l}o^kc^{k+1}q^{k+1}XH = (h_1^e,\dots,h_1^e)h_n^eHG = (g_1,\dots,g_{n+l})Gh^e_nGq^{K+1}G^{label} = (g^l_1,\dots,g^l_{n+l})YLoss_{g}GG^{label}h^e_nq^{K+1}Gh^e_nq^{K+1}Y^s = (y^s_1,\dots,y^s_m)h^e_nq^{K+1}th^d_tP^{vocab}_tLoss_vYSTL = (L_1,\dots,L_m)tGh^d_tL_ttL^{label}tnly_tLL^{label}R \in \R^{n+l}RR\hat{y}_t\odot\alpha, \beta, \gamma1e^{-3}1e^{-4}K\alpha, \beta, \gammaGHK = 1HHGGG$ is shown in the left column. One can observe that in the right column, the final memory pointer successfully copy the entity \textit{chevron} in step 0 and its address \textit{783 Arcadia Pl} in step 3 to fill in the sketch utterance. On the other hand, the memory attention without global weighting is reported in the middle column. One can find that even if the attention weights focus on several point of interests and addresses in step 0 and step 3, the global memory pointer can mitigate the issue as expected. More dialogue visualization and generated results including several negative examples and error analysis are reported in the Appendix.

\section{Related Works}
\textbf{Task-oriented dialogue systems.}
Machine learning based dialogue systems are mainly explored by following two different approaches: modularized and end-to-end. For the modularized systems~\citep{williams2007partially, wen2016network}, a set of modules for natural language understanding~\citep{young2013pomdp, chen2016end}, dialogue state tracking~\citep{lee2016task, zhong2018global}, dialogue management~\citep{su2016line}, and natural language generation~\citep{sharma2016natural} are used. These approaches achieve good stability via combining domain-specific knowledge and slot-filling techniques, but additional human labels are needed. On the other hand, end-to-end approaches have shown promising results recently. Some works view the task as a next utterance retrieval problem, for examples, recurrent entity networks share parameters between RNN~\citep{wu2017dstc6}, query reduction networks modify query between layers~\citep{seo2016query}, and memory networks ~\citep{bordes2016learning, perez2016gated,dqmem8461426} perform multi-hop design to strengthen reasoning ability. In addition, some approaches treat the task as a sequence generation problem. ~\cite{lei2018sequicity} incorporates explicit dialogue states tracking into a delexicalized sequence generation. ~\cite{serban2016building,zhao2017generative} use recurrent neural networks to generate final responses and achieve good results as well. Although it may increase the search space, these approaches can encourage more flexible and diverse system responses by generating utterances token-by-token.

\textbf{Pointer network.}
\cite{vinyals2015pointer} uses attention as a pointer to select a member of the input source as the output. Such copy mechanisms have also been used in other natural language processing tasks, such as question answering~\citep{Dehghani2017, heEtAl2017Long1}, neural machine translation~\citep{gulcehreEtAl2016,guEtAl2016}, language modeling~\citep{merity2016pointer}, and text summarization~\citep{seeliumanning2017Long}. In task-oriented dialogue tasks, \cite{eric-manning:2017:EACLshort} first demonstrated the potential of the copy-augmented Seq2Seq model, which shows that generation-based methods with simple copy strategy can surpass retrieval-based ones. Later, \cite{ericKVR2017} augmented the vocabulary distribution by concatenating KB attention, which at the same time increases the output dimension. Recently, \cite{mem2seq} combines end-to-end memory network into sequence generation, which shows that the multi-hop mechanism in MN can be utilized to improve copy attention. These models outperform utterance retrieval methods by copying relevant entities from the KBs.

\textbf{Others.}
~\cite{zhao2017generative} proposes entity indexing and ~\cite{dqmem8461426} introduces recorded delexicalization to simplify the problem by record entity tables manually. In addition, our approach utilized recurrent structures to query external memory can be viewed as the memory controller in Memory augmented neural networks (MANN)~\citep{graves2014neural, graves2016hybrid}. Similarly, memory encoders have been used in neural machine translation~\citep{wangEtAl2016} and meta-learning applications~\citep{KaiserNRB17}. However, different from other models that use a single matrix representation for reading and writing, GLMP leverages end-to-end memory networks to perform multiple hop attention, which is similar to the stacking self-attention strategy in the Transformer~\citep{vaswani2017attention}. 


\section{Conclusion}
In the work, we present an end-to-end trainable model called global-to-local memory pointer networks for task-oriented dialogues. The global memory encoder and the local memory decoder are designed to incorporate the shared external knowledge into the learning framework. We empirically show that the global and the local memory pointer are able to effectively produce system responses even in the out-of-vocabulary scenario, and visualize how global memory pointer helps as well. As a result, our model achieves state-of-the-art results in both the simulated and the human-human dialogue datasets, and holds potential for extending to other tasks such as question answering and text summarization.



\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\newpage
\appendix
\section{Tables}

\subsection{Training parameters}

\begin{table}[h]
\centering
\setlength{\belowcaptionskip}{5pt} 
\caption{Selected hyper-parameters in each dataset for different hops. The values is the embedding dimension and the GRU hidden size, and the values between parenthesis is the dropout rate. For all the models we used learning rate equal to 0.001, with a decay rate of 0.5. }
\begin{tabular}{|r|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{} & T1 & T2 & T3 & T4 & T5 & SMD \\ \hline
GLMP K1 & 64 (0.1) & 64 (0.3) & 64 (0.3) & 64 (0.7) & 128 (0.3) & 128 (0.2) \\ \hline
GLMP K3 & 64 (0.3) & 64 (0.3) & 64 (0.3) & 64 (0.7) & 128 (0.1) & 128 (0.2) \\ \hline
GLMP K6 & 64 (0.3) & 64 (0.3) & 64 (0.5) & 64 (0.5) & 128 (0.1) & 128 (0.3) \\ \hline
\end{tabular}
\end{table}

\subsection{Dataset Statistics}
\begin{table}[h]
\setlength{\belowcaptionskip}{5pt} 
\caption{Dataset statistics for 2 datasets.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|r|c|c|c|c|c|c|c|c|}
\hline
\textbf{Task} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \multicolumn{3}{c|}{\textbf{SMD}} \\ \cline{7-9} 
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Calendar} & \multicolumn{1}{l|}{Weather} & \multicolumn{1}{l|}{Navigation} \\ \hline
\textit{Avg. User turns} & 4 & 6.5 & 6.4 & 3.5 & 12.9 & \multicolumn{3}{c|}{2.6} \\ \hline
\textit{Avg. Sys turns} & 6 & 9.5 & 9.9 & 3.5 & 18.4 & \multicolumn{3}{c|}{2.6} \\ \hline
\textit{Avg. KB results} & 0 & 0 & 24 & 7 & 23.7 & \multicolumn{3}{c|}{66.1} \\ \hline
\textit{Avg. Sys words} & 6.3 & 6.2 & 7.2 & 5.7 & 6.5 & \multicolumn{3}{c|}{8.6} \\ \hline
\textit{Max. Sys words} & 9 & 9 & 9 & 8 & 9 & \multicolumn{3}{c|}{87} \\ \hline
\textit{Nb. Slot Types} & \multicolumn{5}{c|}{7} & 6 & 4 & 5 \\ \hline
\textit{Nb. Distinct Slot values} & \multicolumn{5}{c|}{-} & 79 & 65 & 140 \\ \hline
\textit{Vocabulary} & \multicolumn{5}{c|}{3747} & \multicolumn{3}{c|}{1601} \\ \hline
\textit{Train dialogues} & \multicolumn{5}{c|}{1000} & \multicolumn{3}{c|}{2425} \\ \hline
\textit{Val dialogues} & \multicolumn{5}{c|}{1000} & \multicolumn{3}{c|}{302} \\ \hline
\textit{Test dialogues} & \multicolumn{5}{c|}{1000 + 1000 OOV} & \multicolumn{3}{c|}{304} \\ \hline
\textit{Total Nb. Dialogues} & 4000 & 4000 & 4000 & 4000 & 4000 & 1034 & 997 & 1000 \\ \hline
\end{tabular}
}
\label{Tab:statistic}
\end{table}

\subsection{Human Evaluation}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{amt.png} \caption{Appropriateness and human-likeness scores according to 200 dialogue scenarios. }
\end{figure}

Appropriateness \\
5: Correct grammar, correct logic, correct dialogue flow, and correct entity provided \\
4: Correct dialogue flow, logic and grammar but has slightly mistakes in entity provided \\
3: Noticeable mistakes about grammar or logic or entity provided but acceptable \\
2: Poor grammar, logic and entity provided \\
1: Wrong grammar, wrong logic, wrong dialogue flow, and wrong entity provided

Human-Likeness (Naturalness) \\
5: The utterance is 100\% like what a person will say \\
4: The utterance is 75\% like what a person will say \\
3: The utterance is 50\% like what a person will say \\
2: The utterance is 25\% like what a person will say \\
1:  The utterance is 0\% like what a person will say \\

\section{Error Analysis}
For bAbI dialogues, the mistakes are mainly from task 3, which is recommending restaurants based on their rating from high to low. We found that sometimes the system will keep sending those restaurants with the higher score even if the user rejected them in the previous turns. On the other hand, SMD is more challenging for response generation. First, we found that the model makes mistakes when the KB has several options corresponding to the user intention. For example, once the user has more than one doctor appointment in the table, the model can barely recognize. In addition, since we do not include the domain specific and user intention supervision, wrong delexicalized responses may be generated, which results in an incorrect entity copy. Lastly, we found that the copied entities may not be matched to the generated sketch tags. For example, an address tag may result in a distance entity copy. We leave the space of improvement to future works.

\section{Additional Discussion}
One of the reviewers suggested us to compare our work to some existing dialogue framework such as PyDial~\footnote{http://www.camdial.org/pydial/}. To the best of our knowledge, in the PyDial framework, it requires to have the dialogue act’s labels for the NLU module and the belief states’ labels for the belief tracker module. The biggest challenge is we do not have such labels in the SMD and bAbI datasets. Moreover, the semi tracker in PyDial is rule-based, which need to re-write rules whenever it encounters a new domain or new datasets. Even its dialogue management module could be a learning solution like policy networks, the input of the policy network is still the hand-crafted state features and labels. Therefore, without the rules and labels predefined in the NLU and belief tracker modules, PyDial could not learn a good policy network. 

Truly speaking, based on the data we have (not very big size) and the current state-of-the-art machine learning algorithms and models, we believe that a well and carefully constructed task-oriented dialogue system using PyDial in a known domain using human rules (in NLU and Belief Tracker) with policy networks may outperform the end-to-end systems (more robust). However, in this paper, without additional human labels and human rules, we want to explore the potential and the advantage of end-to-end systems. Besides easy to train, for multi-domain cases, or even zero-shot domain cases, we believe end-to-end approaches will have better adaptability compared to any rule-based systems.

\pagebreak

\section{Visualization}
\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{2-6.pdf} \caption{Memory attention visualization from the SMD navigation domain.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{4-5.pdf} \caption{Memory attention visualization from the SMD navigation domain.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{4-4.pdf} \caption{Memory attention visualization from the SMD navigation domain.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{1-6.pdf} \caption{Memory attention visualization from the SMD navigation domain.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{5-1.pdf} \caption{Memory attention visualization from the SMD schedule domain.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{10-5.pdf} \caption{Memory attention visualization from the SMD schedule domain.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{14-6.pdf} \caption{Memory attention visualization from the SMD schedule domain.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{8-5.pdf} \caption{Memory attention visualization from the SMD schedule domain.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[height=\textheight]{12-6.pdf} \caption{Memory attention visualization from the SMD weather domain.}
\setlength{\abovecaptionskip}{-5pt} 
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[height=\textheight]{15-7.pdf} \caption{Memory attention visualization from the SMD weather domain.}
\setlength{\abovecaptionskip}{-5pt} 
\end{figure}

\end{document}
