\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{booktabs}
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding}
\name{Yingzhi Wang \qquad Abdelmoumene Boumadane \qquad Abdelwahab Heba}
\address{ Zaion Lab, Zaion, Paris, France}

\begin{document}
\maketitle
\begin{abstract}
Speech self-supervised models such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, they have not been totally proven to produce better performance on tasks other than ASR. In this work, we explored partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks: Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. With simple proposed downstream frameworks, the best scores reached 79.58\% weighted accuracy on speaker-dependent setting and 73.01\% weighted accuracy on speaker-independent setting for Speech Emotion Recognition on IEMOCAP, 2.36\% equal error rate for Speaker Verification on VoxCeleb1, 89.38\% accuracy for Intent Classification and 78.92\% F1 for Slot Filling on SLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning prosodic, voice-print and semantic representations.
\end{abstract}
\begin{keywords}
wav2vec 2.0, HuBERT, speech emotion recognition, speaker verification, spoken language understanding
\end{keywords}
\section{Introduction}

Nowadays, people are expecting less labeled data to train well-generalized models for supervised tasks, since data labeling is a very time- and money-consuming process. Furthermore, people have been attempting to find a powerful feature embedding that can assist the fine-tuning and multi-task training for downstream tasks. The appearance of self-supervised learning meets the above two requirements exactly. In speech domain, excellent self-supervised models are emerging \cite{APC, NPC, MOCKINGJAY, TERA, PASE+, wav2vec, vq-wav2vec, wavLM, unispeech}, among which the most high-performing and the most widely used are wav2vec 2.0 \cite{wav2vec2} and HuBERT \cite{HuBERT}. Many wav2vec 2.0/HuBERT pretrained models have also been published and this has greatly promoted their applications in the field of speech. Therefore, we chose wav2vec2.0 and HuBERT as our research objects in this work.


\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{entire-ft.png}
  \caption{Partial fine-tuning (left) and entire fine-tuning (right) of wav2vec 2.0/HuBERT.}
  \label{fig:speech_production}
\end{figure*}

The wav2vec 2.0 model architecture contains mainly three modules. A convolutional neural network (CNN) \textbf{feature encoder} encodes the raw waveform inputs into latent speech representations. Mask operations are applied before they are fed to the Transformer-based \textbf{contextualized encoder}. A \textbf{quantization module} is used to quantize the latent speech representations from the CNN encoder into a discretized embedding which is then used as the target. The objective is to optimize the contrastive loss, which enforces the model to identify the true quantized latent speech representations.

HuBERT shares the same architecture as wav2vec 2.0. Instead of constructing a contrastive loss, HuBERT uses an offline clustering step to generate noisy labels for Masked Language Model pretraining. Specifically, HuBERT consumes masked continuous speech features to predict predetermined cluster assignments. The predictive loss is applied over the masked regions, forcing the model to learn good high-level representations of unmasked inputs in order to infer the targets of masked ones correctly.

Wav2vec 2.0 and HuBERT outperformed all existing ASR models at that time, proving that they can construct a better verbal embedding. However, speech also contains other important information such as emotion, speaker and semantics, for which the industry also has high expectations. In the field of \textbf{Speech Emotion Recognition} (\textbf{SER}), \textbf{Speaker Verification} (\textbf{SV}) and \textbf{Spoken Language Understanding} (\textbf{SLU}), it is still vague whether self-supervised models can produce better performance compared with traditional supervised models (spectral features + CNN-based feature extraction + RNN/Transformer based time series modeling) \cite{tr-emo-1, tr-emo-2, tr-sv-1, tr-sv-2, tr-slu-1}. However, meaningful attempts have been made in some previous works, which we will introduce below.

In SUPERB \cite{superb}, the performance of different frozen self-supervised encoders were benchmarked across a wide range of speech tasks. For SER, the HuBERT large model stood out from other self-supervised encoders with 67.62\% accuracy (ACC) on IEMOCAP \cite{IEMOCAP}. For SV, the HuBERT base model obtained the best Equal Error Rate (EER) 5.11\% on VoxCeleb1 \cite{VoxCeleb}. SLU contains two separate subtasks: \textbf{Intent Classification} (\textbf{IC}) and \textbf{Slot Filling}   (\textbf{SF}). The HuBERT large model achieved the best results on both IC and SF tasks with 98.76\% ACC on Fluent Speech Commands dataset \cite{FSC} and 89.81\% F1 score on SNIPS \cite{SNIPS} respectively.

For SER, \cite{Interspeech-emotion} combined the features from frozen wav2vec2.0 with other hand-crafted prosodic features and then fed them into a 1d-CNN for a deeper extraction. \cite{Interspeech-emotion-2} explored wav2vec fine-tuning strategies and 65.4\% WA on IEMOCAP was achieved. For SV, \cite{Interspeech-speaker, speaker-sota} both explored fine-tuned wav2vec 2.0, \cite{Interspeech-speaker} obtained 3.61\% EER on VoxCeleb1, while \cite{speaker-sota} also obtained 1.91\% EER on VoxCeleb1 by adding VoxCeleb2 into the training set.




We notice that self-supervised models were only used as frozen feature extractors in SUPERB and some other works. Believing that only by fine-tuning can we show the real power of self-supervised models, we explored the fine-tuning of wav2vec2.0/HuBERT on three speech tasks and provided full fine-tuning experiment details. Taking inspiration from \cite{wav2vec2} and \cite{HuBERT}, we added another fine-tuning method by splitting a pre-trained wav2vec 2.0/HuBERT model into two parts: the CNN feature encoder and the Transformer contextualized encoder. We froze the CNN feature encoder and only fine-tuned the Transformer contextualized encoder. We then tested partially fine-tuned wav2vec2.0/HuBERT pre-trained models together with the entirely fine-tuned ones with the following tasks below: 
\begin{itemize}
    \item  Speech Emotion Recognition on IEMOCAP
    \item  Speaker Verification on VoxCeleb1
    \item  Spoken Language Understanding on SLURP \cite{SLURP}
\end{itemize}


The results show that our fine-tuned models achieved excellent results on the three tasks, which further proves their strong capacity on constructing problem-agnostic representations. The code and fine-tuned models for SER and SLU have been open-sourced on SpeechBrain \cite{SpeechBrain} \footnote{https://github.com/speechbrain/speechbrain/tree/develop/recipes}.



\section{Method}

In this section, we will first introduce the pre-training of wav2vec 2.0/HuBERT model, then we will show our fine-tuning methods and downstream models for each task.

 \subsection{Pretrained wav2vec 2.0}
The wav2vec 2.0 pre-training is similar to the masked language modelling in BERT \cite{Bert} and is carried out under a self-supervised setting. Contiguous time steps from the CNN encoder representations are randomly masked, and the model is trained to reproduce the quantized local encoder representations for masked frames at the output of the contextualized encoder.



The training objective is illustrated in Eq.1, where  is the cosine similarity between the contextualized encoder outputs  and the quantized CNN encoder representations ,  is the masked time step,  is the union of candidate representations  which includes  and  distractors,  is the temperature which is set to 0.1. The distractors are outputs of the local encoder sampled from masked frames belonging to the same utterance as . The contrastive loss is then given by  summed over all masked frames. At the end, an  regularization is added to the contrastive loss, as well as a diversity loss to increase the use of the quantized codebook representations.

The pre-training process is optimized with Adam \cite{Adam} and the learning rate decays linearly after a waming up. In \cite{wav2vec2}, wav2vec 2.0 is also fine-tuned on ASR aiming to improve ASR performance. For ASR fine-tuning, a randomly initialized linear projection is added to the output of the contextual encoder and the CTC (Connectionist Temporal Classification \cite{CTC}) loss is minimized. For more details of the pre-training and ASR fine-tuning of wav2vec 2.0, please refer to \cite{wav2vec2}.

In this work, we compare four released wav2vec 2.0 pre-trained models: the wav2vec 2.0 base model (12 transformer blocks and 768 embedding dimension) and its ASR fine-tuned version, the wav2vec 2.0 large model (24 transformer blocks and 1024 embedding dimension) and its ASR fine-tuned version. Both base and large models are pre-trained on 960h LibriSpeech \cite{LibriSpeech} data, which is also used for their ASR fine-tuning. ASR fine-tuned models for both wav2vec 2.0 and HuBERT are taken into consideration because we assume that some tasks may benefit from the ASR fine-tuning.


\subsection{Pretrained HuBERT}
In the same way as wav2vec 2.0, CNN-encoded audio features are randomly masked in HuBERT. To generate labels for the first iteration of HuBERT pre-training, a k-means clustering is applied on 39-dimensional MFCC features. To generate better targets for the subsequent iterations, k-means clustering then works on the latent features extracted from the HuBERT model pre-trained in the previous iteration. A projection layer is added over transformer blocks to predict cluster labels. Cross-entropy loss is computed over masked timestamps, which can be defined as: 

 denotes the set of indices to be masked for a length- sequence , and  denotes a corrupted version of  where  is replaced with a mask embedding  if . A masked prediction model  takes as input  and predicts a distribution over the target indices at each timestep . To improve target quality, cluster ensembles are utillized in case that an individual clustering model performs badly,  then denotes the target sequences generated by the -th clustering model.

HuBERT pre-training uses the same optimizer and learning rate scheduler as wav2vec 2.0. For ASR fine-tuning, the projection layer is removed and replaced by a randomly initialized softmax layer, then the CTC loss is optimized. For more details of the pre-training of HuBERT, please refer to \cite{HuBERT}.

Like wav2vec 2.0, we compare three released HuBERT pre-trained models: the HuBERT base model (12 transformer blocks and 768 embedding dimension, of which no ASR fine-tuned version is released), the HuBERT large model (24 transformer blocks and 1024 embedding dimension) and its ASR fine-tuned version. The HuBERT base model is pre-trained on 960h LibriSpeech data, while the large model is pre-trained on 60k hours Libri-Light \cite{LibriLight} data. The ASR fine-tuning is also based on 960h LibriSpeech data.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{downstream.png}
  \caption{Simple downstream models for SER, SID and SLU. For SER and SID, an average time pooling and a linear classifier is built over wav2vec 2.0/HuBERT. For SLU, an attentional decoder decodes intents and slots directly from the fine-tuned wav2vec2.0/HuBERT embedding.}
  \label{fig:speech_production}
\end{figure}

\subsection{Fine-tuning}
As is shown in Figure 1 on the left for partial fine-tuning, the wav2vec 2.0/HuBERT model is divided into two parts: the CNN-based feature encoder and the transformer-based contextualized encoder. We froze the CNN-based feature encoder, fixing all the parameters of these CNN blocks, and only fine-tuned the parameters of the transformer blocks. Partial fine-tuning can be understood as a domain adaptation training for the top level, which aims to prevent interference and damage to the bottom CNN layers that already have an expressive ability.

For entire fine-tuning which is shown on the right in Figure 1, the CNN and Transformer modules are both fine-tuned during the downstream training process. By training general features at the bottom level, entire fine-tuning allows higher-level expressions to be more complete and more targeted.



Then, assuming that fine-tuned wav2vec 2.0/HuBERT are already powerful enough to capture information, we directly added simple downstream adaptors (classifier/decoder) to wav2vec 2.0/HuBERT without adding another heavy and redundant encoder. The downstream adaptors for each task are presented as below.

For SER, an average time pooling and one linear layer are added as a simple downstream classifier (Fig.2). The average time pooling compresses variant time lengths into one, then the linear layer effectuates an utterance-level classification minimizing the cross-entropy loss.

For SV, a \textbf{Speaker Identification} (\textbf{SID}) task is first implemented using the same downstream framework as SER. Pairwise cosine-similarity scores are then produced for SV on the pre-trained SID embeddings before the linear classification layer.

For SLU (Fig.2), another attentional GRU-based decoder is added to decode semantic information directly from the fine-tuned wav2vec2.0/HuBERT embedding. In our work, intents and slots are both treated as a sequence-to-sequence ASR task and are both decoded from the attentional decoder. The Negative Log-Likelihood (NLL) loss is then calculated over a character-level token generation. Following the observations of \cite{timers_and_such}, we utilized a beam-search with a beam of 80 without coverage penalty to identify the optimum sequence for validation set and test set.


\section{Experiments}

In the experiment section, we will first introduce the datasets used for the three tasks, then we will list the models we compared and add details of our experiment settings. Finally, we will show the results for each task and compare with the existing state-of-the-art baselines.

\subsection{Datasets}
The three most widely used and most representative datasets were chosen in our experiments, which are IEMOCAP for SER, VoxCeleb1 for SV and SLURP for SLU.

The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset has approximately 12 hours of data and consists of scripted and improvised dialogues by 10 speakers. In order to form a contrast in this work, we used 4 emotional classes as in SUPERB: anger, happiness, sadness and neutral, following the work of \cite{IEMOCAP-relabel}. The evaluation metric is weighted accuracy (WA) and the experiments were carried out on two different split settings: Speaker-Dependent (\textbf{SD}) setting and Speaker-Independent (\textbf{SI}) setting. For SD, the results were averaged on 5 different random seeds for train-validation-test split. For SI, a 10-fold cross-validation was performed with a leave-two-speaker-out strategy (one for validation and one for test).





VoxCeleb1 contains over 100,000 utterances from 1,251 speakers, with approximately 351 hours of audio in total. In our work, a Speaker Identification task was first implemented, encouraging the model to learn to distinguish 1211 different voice-prints. A verification was then carried out on the vox1-o test set of 40 speakers by calculating cosine similarity on the embeddings from the pre-trained Speaker Identification model. VoxCeleb2 and noise augmentation were not used in our experiments. We used equal error rate (EER) as the evaluation metric and the results were averaged on 5 different seeds for train-validation split.




The Spoken Language Understanding Resource Package (SLURP) Dataset is a collection of 72K audio recordings of single turn user interactions with a home assistant, annotated with three levels of semantics: Scenario, Action and Entities. The training and evaluation are based on its official training, validation and test sets.

\subsection{Fine-tuning settings}
We rename the models we compare with a method as below.
\begin{itemize}
    \item  EF/PF/Frozen: Entirely Fine-tuned/Partially Fine-tuned/Not fine-tuned
    \item  w2v/hbt: wav2vec 2.0/HuBERT based model
    \item  base/large: base/large pre-trained model
    \item  -/960h: with/without ASR fine-tuning using 960h LibriSpeech data
\end{itemize}

For example, \textit{EF-w2v-base} refers to an entirely fine-tuned wav2vec 2.0 base model, while \textit{PF-hbt-large-960h} refers to a partially fine-tuned HuBERT large model with an ASR fine-tuning. For more detailed parameters of released pre-trained wav2vec 2.0/HuBERT models, please refer to \cite{wav2vec2} and \cite{HuBERT}.

During the fine-tuning process, we applied two different schedulers to respectively adjust the fine-tuning learning rate of the wav2vec 2.0/HuBERT encoder and the learning rate of the downstream model. Both the schedulers use an Adam Optimizer and linearly anneal the learning rates according to the performance of validation stage. For SER and SV, the initialized fine-tuning learning rate and the downstream learning rate are set to  and . For SLU, these values are set to  and .

\begin{table}[th]
  \centering
  \caption{Benchmark results for two utterance-level tasks: Speech Emotion Recognition (SER) on weighted accuracy (WA\%) and Speaker Verification (SV) on Equal Error Rate (EER\%). In relation to SER, it is divided into SER-SD (Speaker-Dependent settng) and SER-SI (Speaker-Independent setting).
}
  \label{tab:example}
  \centering
  \begin{tabular}{cccc}
    \toprule
    \multicolumn{1}{c}{\textbf{Model}} &   \multicolumn{1}{c}{\textbf{SER-SD}}   &
    \multicolumn{1}{c}{\textbf{SER-SI}}   &
    \multicolumn{1}{c}{\textbf{SV}}\\
    \midrule
    EF-w2v-base          & 75.90 & 70.75 & 2.77 \\
    PF-w2v-base          & 77.02 & 70.21 & 3.15 \\
    EF-w2v-base-960h     & 73.64 & 64.20 & 4.46  \\
    PF-w2v-base-960h     & 73.84 & 68.34 & 4.38  \\
    \midrule
    EF-w2v-large          & 77.00 & 70.96 & 3.42  \\
    PF-w2v-large          & 77.47 & 70.99 & 3.85  \\
    EF-w2v-large-960h     & 73.00 & 68.18 & 4.27  \\
    PF-w2v-large-960h     & 76.75 & 69.08 & 4.47  \\
    \midrule
    EF-hbt-base          & 76.53 & 69.83 & 2.84 \\
    PF-hbt-base          & 76.60 & 69.68 & 3.13 \\
    \midrule
    EF-hbt-large          & 78.52 & 72.31 & 2.86  \\
    PF-hbt-large          & \textbf{79.58} & \textbf{73.01} & 3.21  \\
    EF-hbt-large-960h     & 78.78 & 72.71 & \textbf{2.36} \\
    PF-hbt-large-960h     & 78.96 & 72.98 & 2.38 \\
    \midrule
    Frozen-w2v-base\cite{superb} & - & 63.43 & 6.02 \\
    Frozen-w2v-large\cite{superb} & - & 65.64 & 5.65 \\
    Frozen-hbt-base\cite{superb} & - & 64.92 & 5.11 \\
    Frozen-hbt-large\cite{superb} & - & 67.62 & 5.98 \\
    \midrule
    Head Fusion\cite{head-fusion} & 76.18 & - & -  \\
    Attention Pooling\cite{attention_pooling} & - & 71.75 & -  \\
    Siamese Capsule\cite{siamese} & - & - & 3.14  \\
    \bottomrule
  \end{tabular}
  
\end{table}

\subsection{Results and discussion}
\subsubsection{Speech Emotion Recognition \& Speaker Verification}
The results of SER and SV are shown in Table 1. For comparison, we show SUPERB's results as a non-fine-tuned baseline (marked with \cite{superb} in Table 1). Furthermore, we took Head-Fusion ACNN \cite{head-fusion} for SER-SD (Speaker-Dependent setting), Attention Pooling based representation \cite{attention_pooling} for SER-SI (Speaker-Independent setting) and Siamese Capsule network \cite{siamese} for SV as state-of-the-art baselines respectively. Compared with other more recent works, \cite{attention_pooling} provides a comparable result by reporting a competitve Weighted Accuracy using only speech, and \cite{siamese} also provides a comparable result using only Voxceleb1 as the training set (the same as SUPERB). First of all, from an overall perspective, we notice a significant improvement on the results of fine-tuned models over those of SUPERB's non-fine-tuned models. Then, for SER, we are surprised to find that all of our fine-tuned models performed well, where the partially fine-tuned HuBERT large model reached a best WA of 79.58\% for SER-SD and a best WA of 73.01\% for SER-SI, improving by 3.40\% and 1.26\% on the state-of-the-art baselines respectively. Moreover, we observe that partial fine-tuning appeared to be a better fine-tuning method than entire fine-tuning. We consider that IEMOCAP is a small dataset with only 12 hours of data and training too many parameters may easily cause an overfitting. Additionally, we noticed that the ASR fine-tuning was not helping the downstream SER task, suggesting a loss of prosodic information during the ASR fine-tuning. 

In the case of SV, the entirely fine-tuned HuBERT with ASR fine-tuning reached a best 2.36\% Equal Error Rate, surpassing the baseline by 0.78\%. However, contrary to SER, entire fine-tuning outperforms partial fine-tuning as can be seen from the results. Due to the large amount of data (351 hours) of VoxCeleb1 that are also acoustically similar to the data used for pre-training, the pre-trained encoder parameters provide an ideal initialization for the downstream SV task, releasing all the layers and fine-tuning with a low learning rate can lead to a good result. Finally, we find that HuBERT turned out to be a better self-supervised encoder compared to wav2vec 2.0 for both SER and SV tasks.


\begin{table}[th]
  \centering
  \caption{Benchmark results for Spoken Language Understanding: Intent Classification (IC) on accuracy (ACC\%) and Slot Filling (SF) on F1 score (F1\%).
}
  \label{tab:example}
  \centering
  \begin{tabular}{ccc}
    \toprule
    \multicolumn{1}{c}{\textbf{Model}} &   \multicolumn{1}{c}{\textbf{IC (ACC\%)}}   &
    \multicolumn{1}{c}{\textbf{SF (F1\%)}}\\
    \midrule
    EF-w2v-base          & 87.13 & 74.32 \\
    PF-w2v-base          & 86.58 & 74.73 \\
    EF-w2v-base-960h     & 85.89 & 74.33  \\
    PF-w2v-base-960h     & 86.13 & 73.78  \\
    \midrule
    EF-w2v-large          & 85.80 & 72.45 \\
    PF-w2v-large          & 86.29 & 73.16 \\
    EF-w2v-large-960h     & 86.10 & 73.39  \\
    PF-w2v-large-960h     & 86.35 & 74.03  \\
    \midrule
    EF-hbt-base          & 87.44 & 75.06 \\
    PF-hbt-base          & 87.51 & 75.32 \\
    \midrule
    EF-hbt-large          &  \textbf{89.38} & 78.43 \\
    PF-hbt-large          & 89.22 & \textbf{78.92} \\
    EF-hbt-large-960h     & 88.71 & 78.89  \\
    PF-hbt-large-960h     & 88.32 & 78.17  \\
    \midrule
    Frozen-w2v-base & 47.15 & 37.66 \\
    Frozen-w2v-large & 3.88 & 3.85 \\
    Frozen-hbt-base & 68.74 & 57.06 \\
    Frozen-hbt-large & 74.42 & 60.07 \\
    
    \midrule
    CTI\cite{CTI} & 86.92 & 74.66  \\
    \bottomrule
  \end{tabular}
  
\end{table}


\subsubsection{Spoken Language Understanding}
For SLU, the results of its two subtasks are shown in Table 2. Likewise, we carried out experiments of frozen wav2vec 2.0/HuBERT models to form a contrast. However, the performance of frozen models on this task drops significantly, especially for wav2vec 2.0 large model the loss cannot even converge, demonstrating that the frozen wav2vec 2.0/HuBERT cannot hold complete semantic information. Continuous Token Interface \cite{CTI} is chosen as the state-of-the-art baseline. The best ACC for IC is 89.38\% with the entirely fine-tuned HuBERT large model, while the partially fine-tuned HuBERT large model reached the best F1 78.92\% for SF. For SLU, the gap between the two fine-tuning methods is not obvious. A slight drop is observed on ASR fine-tuned models, which implies that ASR fine-tuning will also result in a loss of semantic information.




\section{Conclusions}
\label{sec:conclusions}

In this work we explored different fine-tuning methods on two of the most powerful self-supervised models (wav2vec 2.0 and HuBERT), then benchmarked their performance on Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding tasks. State-of-the-art results were achieved for all the three tasks, proving the excellent generalizability of wav2vec 2.0/HuBERT on learning prosodic, voice-print and semantic representations. We hope to show the broad prospects of self-supervised learning and also provide some useful insights for its industrial applications. 




\clearpage

\bibliographystyle{IEEEbib}
\bibliography{mybib}

\end{document}
