\documentclass{sig-alternate}
\pagenumbering{arabic}







\setlength{\paperheight}{11in}

\usepackage{microtype}
\usepackage{epstopdf}
\usepackage{graphicx} \usepackage{subfigure} 
\graphicspath{{.}{./figures/}}

\newcommand{\todo}[1]{{\textcolor{red}{#1}}}

\usepackage{natbib}
\renewcommand{\cite}{\citep}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[pdfpagelabels=false,plainpages=false,colorlinks,citecolor=black,linkcolor=black,urlcolor=black]{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{paralist}
\usepackage{booktabs}
\usepackage{listings}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}

\renewcommand{\algorithmicrequire}{\textsc{input}}
\renewcommand{\algorithmicrepeat}{\textsc{repeat}}
\renewcommand{\algorithmicuntil}{\textsc{until}}

\newcommand{\sA}{\mathcal{A}}
\newcommand{\sS}{\mathcal{S}}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\sign}{sign}

\newcommand{\algo}[1]{\textsc{\lowercase{#1}}}

\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}

\newenvironment{sbmatrix}{\left[\!\begin{smallmatrix}}
{\end{smallmatrix}\!\right]}

\newcommand{\itn}[1]{^{(#1)}}
\newcommand{\eps}{\varepsilon}
\newcommand{\mat}{\boldsymbol}
\renewcommand{\vec}[1]{\boldsymbol{\mathrm{#1}}}
\providecommand{\mYhat}{\ensuremath{\mat{\hat{\mY}}}}
\providecommand{\mSigma}{\ensuremath{\mat{\Sigma}}}
\providecommand{\mA}{\ensuremath{\mat{A}}}
\providecommand{\mB}{\ensuremath{\mat{B}}}
\providecommand{\mC}{\ensuremath{\mat{C}}}
\providecommand{\mD}{\ensuremath{\mat{D}}}
\providecommand{\mE}{\ensuremath{\mat{E}}}
\providecommand{\mF}{\ensuremath{\mat{F}}}
\providecommand{\mG}{\ensuremath{\mat{G}}}
\providecommand{\mH}{\ensuremath{\mat{H}}}
\providecommand{\mI}{\ensuremath{\mat{I}}}
\providecommand{\mJ}{\ensuremath{\mat{J}}}
\providecommand{\mK}{\ensuremath{\mat{K}}}
\providecommand{\mL}{\ensuremath{\mat{L}}}
\providecommand{\mM}{\ensuremath{\mat{M}}}
\providecommand{\mN}{\ensuremath{\mat{N}}}
\providecommand{\mO}{\ensuremath{\mat{O}}}
\providecommand{\mP}{\ensuremath{\mat{P}}}
\providecommand{\mQ}{\ensuremath{\mat{Q}}}
\providecommand{\mR}{\ensuremath{\mat{R}}}
\providecommand{\mS}{\ensuremath{\mat{S}}}
\providecommand{\mT}{\ensuremath{\mat{T}}}
\providecommand{\mU}{\ensuremath{\mat{U}}}
\providecommand{\mV}{\ensuremath{\mat{V}}}
\providecommand{\mW}{\ensuremath{\mat{W}}}
\providecommand{\mX}{\ensuremath{\mat{X}}}
\providecommand{\mY}{\ensuremath{\mat{Y}}}
\providecommand{\mZ}{\ensuremath{\mat{Z}}}
\providecommand{\mLambda}{\ensuremath{\mat{\Lambda}}}
\providecommand{\ones}{\vec{e}}
\providecommand{\va}{\ensuremath{\vec{a}}}
\providecommand{\vb}{\ensuremath{\vec{b}}}
\providecommand{\vc}{\ensuremath{\vec{c}}}
\providecommand{\vd}{\ensuremath{\vec{d}}}
\providecommand{\ve}{\ensuremath{\vec{e}}}
\providecommand{\vf}{\ensuremath{\vec{f}}}
\providecommand{\vg}{\ensuremath{\vec{g}}}
\providecommand{\vh}{\ensuremath{\vec{h}}}
\providecommand{\vi}{\ensuremath{\vec{i}}}
\providecommand{\vj}{\ensuremath{\vec{j}}}
\providecommand{\vk}{\ensuremath{\vec{k}}}
\providecommand{\vl}{\ensuremath{\vec{l}}}
\providecommand{\vm}{\ensuremath{\vec{l}}}
\providecommand{\vn}{\ensuremath{\vec{n}}}
\providecommand{\vo}{\ensuremath{\vec{o}}}
\providecommand{\vp}{\ensuremath{\vec{p}}}
\providecommand{\vq}{\ensuremath{\vec{q}}}
\providecommand{\vr}{\ensuremath{\vec{r}}}
\providecommand{\vs}{\ensuremath{\vec{s}}}
\providecommand{\vt}{\ensuremath{\vec{t}}}
\providecommand{\vu}{\ensuremath{\vec{u}}}
\providecommand{\vv}{\ensuremath{\vec{v}}}
\providecommand{\vw}{\ensuremath{\vec{w}}}
\providecommand{\vx}{\ensuremath{\vec{x}}}
\providecommand{\vy}{\ensuremath{\vec{y}}}
\providecommand{\vz}{\ensuremath{\vec{z}}}
\providecommand{\vpi}{\ensuremath{\vecalt{\pi}}} 
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator{\subjectto}{subject\ to}
\providecommand{\MINof}[1][]{{\displaystyle \minimize_{#1}}}
\providecommand{\MIN}[2][]{\begin{array}{ll} \MINof[#1] & #2 \\ \end{array}}
\providecommand{\MINone}[3]{\begin{array}{ll} \MINof[#1] & #2 \\ \subjectto  & #3 \end{array}}
\providecommand{\MINtwo}[4]{\begin{array}{ll} \MINof[#1] & #2 \\ \subjectto  & #3 \\ & #4 \end{array}}
\newcommand{\sstretchsym}[3]{\ensuremath{\left#1 #3 \right#2}}
\newcommand{\nstretchsym}[3]{\ensuremath{#1 #3 #2}}
\newcommand{\bstretchsym}[3]{\ensuremath{\bigl#1 #3 \bigr#2}}
\newcommand{\Bstretchsym}[3]{\ensuremath{\Bigl#1 #3 \Bigr#2}}
\newcommand{\hstretchsym}[3]{\ensuremath{\biggl#1 #3 \biggr#2}}
\newcommand{\Hstretchsym}[3]{\ensuremath{\Biggl#1 #3 \Biggr#2}}
\newcommand{\newstrechsymset}[4][s]{\expandafter\def\csname s#2of\endcsname{\sstretchsym{#3}{#4}}\expandafter\def\csname n#2of\endcsname{\nstretchsym{#3}{#4}}\expandafter\def\csname b#2of\endcsname{\bstretchsym{#3}{#4}}\expandafter\def\csname B#2of\endcsname{\Bstretchsym{#3}{#4}}\expandafter\def\csname h#2of\endcsname{\hstretchsym{#3}{#4}}\expandafter\def\csname H#2of\endcsname{\Hstretchsym{#3}{#4}}\expandafter\def\csname #2of\endcsname{\csname #1#2of\endcsname}}
\newcommand{\normof}[2][]{\sstretchsym{\|}{\|}{#2}_{#1}}
\newcommand{\nnormof}[2][]{\nstretchsym{\|}{\|}{#2}_{#1}}
\newstrechsymset{abs}{|}{|}

\def\clap#1{\hbox to 0pt{\hss#1\hss}}
\def\mathllap{\mathpalette\mathllapinternal}
\def\mathrlap{\mathpalette\mathrlapinternal}
\def\mathclap{\mathpalette\mathclapinternal}
\def\mathllapinternal#1#2{\llap{}}
\def\mathrlapinternal#1#2{\rlap{}}
\def\mathclapinternal#1#2{\clap{}} 

\begin{document} 

\title{Rank Aggregation via Nuclear Norm Minimization}


\numberofauthors{2} 

\author{
\alignauthor
David F.~Gleich\\
       \affaddr{Sandia National Laboratories\thanks{{\crnotice
       Sandia National Laboratories is a multi-program laboratory
       managed and operated by Sandia Corporation, a wholly owned
       subsidiary of Lockheed Martin Corporation, for the U.S.
       Department of Energy's National Nuclear Security Administration
       under contract DE-AC04-94AL85000.}}}\\
       \affaddr{Livermore, CA}\\
\email{dfgleic@sandia.gov}
\alignauthor
Lek-Heng Lim\\
			 \affaddr{University of Chicago}\\
			 \affaddr{Chicago, IL}\\
			 \email{lekheng@galton.chicago.edu}
}

\maketitle


\begin{abstract} 
The process of rank aggregation is intimately intertwined with
the structure of skew-symmetric matrices.  
We apply recent advances in the theory and algorithms of matrix completion
to skew-symmetric matrices.  This combination of ideas
produces a new method for ranking a set of items.  The essence
of our idea is that a rank aggregation describes a partially
filled skew-symmetric matrix.  We extend an algorithm for
matrix completion to handle skew-symmetric data and use that
to extract ranks for each item.  
Our algorithm applies to both pairwise comparison and rating data.
Because it is based on matrix completion, it is robust to
both noise and incomplete data.  We show a formal
recovery result for the noiseless case and 
present a detailed study of the algorithm on synthetic
data and Netflix ratings.
\end{abstract} 

\keywords{nuclear norm, skew symmetric, rank aggregation}

\section{Introduction}






One of the classic data mining problems is to identify the
important items in a data set; see \citet{Tan-2004-ordering}
for an interesting example of how these might be used. For this task, 
we are concerned with rank aggregation.
Given a series of votes on a set of items by a group of voters,
rank aggregation is the process of permuting the set of items so that
the first element is the best choice in the set, the second
element is the next best choice, and so on.  
In fact, rank aggregation is an old problem and has a history stretching 
back centuries~\cite{condorcet1785-essai}; one famous 
result is that any rank aggregation requires some 
degree of compromise~\cite{arrow1950-impossibility}.  
Our point in this introduction
is not to detail a history of all the possible methods of
rank aggregation, but to give some perspective on our approach to
the problem.

Direct approaches involve finding a permutation
explicitly -- for example, computing the Kemeny optimal
ranking \cite{kemeny1959-math-without-numbers} or
the minimum feedback arc set problem.  
These problems are NP-hard 
\cite{dwork2001-rank-aggregation,ailon2005-ranking,alon2006-ranking}.
An alternate approach is to assign a score to 
each item, and then compute a permutation based on
ordering these items by their score, e.g.\ \citet{saaty1987-perron}.
In this manuscript, we focus on the second approach.  A
key advantage of the computations we propose
is that they are \emph{convex} problems and efficiently
solvable.

While
the problem of rank aggregation is old, modern applications --
such as those found in web-applications like Netflix and Amazon --
pose new challenges. 
First, the data collected are usually
cardinal measurements on the quality of each
item, such as  1--5 stars, received from voters.
Second, the voters are neither experts in the rating domain nor
experts at producing useful ratings.  These properties
manifest themselves in a few ways, including skewed
and indiscriminate voting behaviors \cite{ho2008-ratings}.
We focus on using aggregate pairwise 
data about items to develop a score for each item that
predicts the pairwise data itself.  This approach
eliminates some of the issues with directly utilizing
voters ratings, and we argue this point more precisely
in Section~\ref{sec:pairwise}.





To explain our method, consider a set of  items, labeled from  to .  
Suppose that each of these
items has an unknown intrinsic quality ,
where  implies that item  is better
than item .  While the 's are unknown, 
suppose we are given
a matrix  where .  By 
finding a rank-2 factorization of , for example

we can extract unknown scores.  The matrix 
is  skew-symmetric and describes any score-based global pairwise
ranking.  (There are other possible rank-2 factorizations
of a skew-symmetric matrix, 
a point we return to later in Section~\ref{sec:mc-algs}).



Thus, given a measured , the goal is to find
a minimum rank approximation of  that models the elements, and ideally
one that is rank-.  Phrased
in this way, it is a natural candidate for recent developments in the
theory of matrix completion~\cite{candes2009-matrix-completion,recht2009-nuclear-norm}.
In the matrix
 completion problem, certain elements of the matrix are presumed to be known.
 The goal is to produce a low-rank matrix that respects these elements -- or at
  least minimizes the deviation from the known elements.  One catch, 
 however, is that we require
 matrix completion over skew-symmetric matrices for pairwise
ranking matrices.  Thus, we must
solve the matrix completion problem inside a structured class of 
matrices.  This task is a novel contribution of our work.
Recently, \citet{Gross2010-low-rank} also developed a technique
for matrix completion with Hermitian matrices.

With a ``completed'' matrix , the norm of the residual 
gives us a certificate for the validity of our fit -- an additional piece of
information available in this model.

To continue, we briefly summarize our main contributions and 
our notational conventions.  

\paragraph{Our contributions}
\begin{compactitem}
 \item We propose a new method for computing a rank aggregation based on 
 matrix completion, which is tolerant to noise and incomplete data.
 \item We solve a structured matrix-completion problem
 over the space  of skew-symmetric matrices.
 \item We prove a recovery theorem detailing when our approach
 will work.
 \item We perform a detailed evaluation of our approach with
   synthetic data and an anecdotal study with Netflix ratings.
\end{compactitem}

\paragraph{Notation} We try to follow standard
notation conventions.  Matrices are bold, upright roman letters, vectors
are bold, lowercase roman letters, and scalars are unbolded roman or Greek
letters.  The vector  consists of all ones,
and the vector  has a  in the th position and 's
elsewhere.
Linear maps on matrices are written as script letters.  
An index set 
is a group of index pairs.  Each 
is a pair  and we assume that the 's are
numbered arbitrarily, i.e. .
Please refer to Table~\ref{tab:notation} for reference.

\begin{table}
\caption{Notation for the paper.}
\label{tab:notation}
\setlength{\tabcolsep}{1ex}
\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\linewidth}{cX}
\toprule
  \textbf{Sym.} & \textbf{Interpretation} 
\\ \midrule
    & a linear map from a matrix to a vector 
\\  & a vector of all ones
\\  & a vector with  in the th entry, 0 elsewhere
\\  & the nuclear norm
\\  & a rating matrix (voters-by-items)
\\  & a fitted or model pairwise comparison matrix 
\\  & a measured pairwise comparison matrix
\\  & an index set for the known entries of a matrix
\\ \bottomrule
\end{tabularx}
\end{table}


\begin{table*}
\caption{The top 15 movies from Netflix generated by our ranking method 
(middle and right).  The left list is the ranking using the
mean rating of each movie and is
emblematic of the problems global ranking methods face when
infrequently compared items rocket to the top.  We prefer the middle and
right lists.  See Section~\ref{sec:results} and Figure~\ref{fig:netflix-residuals}
for information about the conditions and additional discussion.  
LOTR III appears twice because of the 
two DVDs editions, theatrical and extended.}
\label{tab:netflix-top-movies}
\begin{tabularx}{\linewidth}{XXX}
\toprule
		Mean	& 		Log-odds (all)	&		Arithmetic Mean (30)
\\ \midrule		
		  LOTR III: Return \ldots	&		LOTR III: Return \ldots	&		LOTR III: Return \ldots
\\		LOTR I: The Fellowship \ldots	&		LOTR I: The Fellowship \ldots	&		LOTR I: The Fellowship \ldots
\\		LOTR II: The Two \ldots	&		LOTR II: The Two \ldots	&		LOTR II: The Two \ldots
\\		Lost: Season 1	&		Star Wars V: Empire \ldots	&		Lost: S1
\\		Battlestar Galactica: S1	&		Raiders of the Lost Ark	&		Star Wars V: Empire \ldots
\\		Fullmetal Alchemist	&		Star Wars IV: A New Hope	&		Battlestar Galactica: S1
\\		Trailer Park Boys: S4	&		Shawshank Redemption	&		Star Wars IV: A New Hope
\\		Trailer Park Boys: S3	&		Star Wars VI: Return ...	&		LOTR III: Return \ldots
\\		Tenchi Muyo! \ldots	&		LOTR III: Return \ldots	&		Raiders of the Lost Ark
\\		Shawshank Redemption	&		The Godfather	&		The Godfather
\\		Veronica Mars: S1	&		Toy Story	&		Shawshank Redemption
\\		Ghost in the Shell: S2	&		Lost: S1	&		Star Wars VI: Return ...
\\		Arrested Development: S2	&		Schindler's List	&		Gladiator
\\		Simpsons: S6	&		Finding Nemo	&		Simpsons: S5
\\		Inu-Yasha	&		CSI: S4	&		Schindler's List
\\ \bottomrule
\end{tabularx}
\end{table*}

Before proceeding further, let us outline the rest of the paper.
First, Section~\ref{sec:pairwise} describes a few 
methods to take voter-item ratings and produce an aggregate 
pairwise comparison matrix.  Additionally, we argue 
why pairwise aggregation is
a superior technique when the goal is to produce
an ordered list of the alternatives.  
Next, in Section~\ref{sec:ranking-nn}, we describe  
formulations of the noisy matrix completion problem 
using the nuclear norm.
In our setting, the 
\algo{lasso} formulation is the best choice, and we use
it throughout the remainder.  We 
briefly describe algorithms for matrix completion and focus
on the \algo{svp} algorithm  \cite{Jain-2010-SVP} 
in Section~\ref{sec:mc-algs}.
We then show that the \algo{svp} algorithm preserves skew-symmetric structure.
This process
involves studying the singular value decomposition of skew-symmetric
matrices.  Thus, by the end of the section, we've shown how to
formulate and solve for a scoring vector based on the nuclear norm.
The following sections describe alternative approaches and show our
recovery results.  At the end, we show our experimental results.
In summary, our overall methodology is 
\begin{center}
Ratings \rlap{()} \\
 \rlap{(\S \ref{sec:pairwise})}\\
 Pairwise comparisons \rlap{()}\\
 \rlap{(\S \ref{sec:ranking-nn})}\\
Ranking scores \rlap{()} \\
 \rlap{(sorting)} \\
Rank aggregations.
\end{center}
An example of our rank aggregations 
is given in Table~\ref{tab:netflix-top-movies}.  We comment
further on these in Section~\ref{sec:netflix}.

Finally, we provide our computational and experimental
codes so that others may reproduce our results:\\
\url{https://dgleich.com/projects/skew-nuclear}



\section{Pairwise Aggregation Methods}
\label{sec:pairwise}

To begin, we describe methods to aggregate the votes
of many voters, given by the matrix , 
into a measured pairwise comparison matrix .  
These methods have been well-studied in statistics
\cite{david1988-paired}.
In the next
section, we show how to extract a score for each item from the 
matrix .

Let  be a voter-by-item matrix.  This matrix has  rows corresponding
to each of the  voters and  columns corresponding to the  items
of the dataset.  In all of the applications we explore, the matrix 
 is highly incomplete.  That is, only a few items are rated by
each voter.  Usually all the items have a few votes, but there is
no consistency in the number of ratings per item.

Instead of using  directly, we compute a pairwise aggregation.  
Pairwise comparisons have a lengthy history, dating back to the 
first half of the previous century \cite{Kendall-1940-paired-comparison}.
They also have many nice properties.
First, \citet{miller1956-seven} observes that most people can evaluate only
5 to 9 alternatives at a time.  This fact may relate to the common choice
of a -star rating (e.g.\ the ones used by Amazon, eBay, 
Netflix, YouTube).  Thus, comparing pairs of
movies is easier than ranking a set of  movies.  Furthermore,
only pairwise comparisons are possible in certain settings
such as tennis tournaments. 
Pairwise comparison methods are thus natural for analyzing ranking data.
Second, pairwise comparisons are a relative measure and help reduce 
bias from the rating scale. For these reasons, pairwise
comparison methods have been popular in psychology, statistics, and social
choice theory \cite{david1988-paired,arrow1950-impossibility}. Such
methods have also been adopted by the learning to rank community;
see the contents of \citet{li2008-learning-to-rank}.
A final advantage of pairwise methods is that they are much more complete
than the ratings matrix.  For Netflix,  is 99\% incomplete, whereas
 is only 0.22\% incomplete and most entries are supported by
\emph{many} comparisons.  See Figure~\ref{fig:pairwise} for information
about the number of pairwise comparisons in Netflix and MovieLens.

More critically, an incomplete array of user-by-product ratings
is a strange matrix -- not every 2-dimensional array of
numbers is best viewed as a matrix --
and using the rank of this matrix (or its convex relaxation) as a key
feature in the modeling needs to be done with care.  Consider,
if instead of rating values 1 to 5, 0 to 4 are used to
represent the exact same information, the rank of this
new rating matrix will change.
Furthermore, whether we use a rating scale where 1 is the best
rating and 5 is worst, or one where 5 is the best
and 1 is the worst, a low-rank model would give the
exact same fit with the same input values, even
though the connotations of the numbers is reversed.

On the other hand, the pairwise ranking matrix that
we construct below is invariant under monotone transformation
of the rating values and depends only on the degree of
relative preference of one alternative over another.
It circumvents the previously mentioned pitfalls and is
a more principled way to employ a rank/nuclear norm model.

We now describe five techniques to build an aggregate pairwise
matrix  from the rating matrix .  
Let  denote the index of a voter, and  and  the 
indices of two items.  The entries of  are .
To each voter, we associate a pairwise comparison matrix .
The aggregation
is usually computed by something like a mean over .



\begin{compactenum}
\item \textbf{Arithmetic mean of score differences}\quad The score difference
is . The arithmetic mean
of all voters who have rated both  and  is

These comparisons  are translation invariant.

\item \textbf{Geometric mean of score ratios}\quad Assuming , the score ratio
refers to . The (log) geometric
mean over all voters who have rated both  and  is

These are  scale invariant.

\item \textbf{Binary comparison}\quad Here . Its average is the probability difference
that the alternative  is preferred to  than vice versa

These are invariant to a monotone transformation.

\item \textbf{Strict binary comparison}\quad This method is almost the same
as the last method, except that we eliminate cases where users rated
movies equally.  That is, 

Again, the average  has a similar interpretation to
binary comparison, but only among people who expressed a strict 
preference for one item over the other.  
Equal ratings are ignored.

\item \textbf{Logarithmic odds ratio}\quad This idea translates
binary comparison to a logarithmic scale:

\end{compactenum}





\begin{figure*}
\centering
\subfigure[MovieLens - 85.49\% of total pairwise comparisons]{\includegraphics[width=0.45\linewidth]{movielens-10M-pairwise-hist}}  
\quad 
\subfigure[Netflix - 99.77\% of total pairwise comparisons]{\includegraphics[width=0.45\linewidth]{netflix-pairwise-hist}}  
\caption{A histogram of the number of pairwise comparisons 
between movies in MovieLens (left) and Netflix (right).  The 
number of pairwise comparisons is the number of users with ratings
on both movies.  These histograms show that most items
have more than a small number of comparisons between them.  
For example, 18.5\% and 34.67\% of all possible pairwise
entries have more than 30 
comparisons between them.  Largely speaking, this figure
justifies dropping infrequent ratings from the comparison.
This step allows us to take advantage of the ability
of the matrix-completion methods to deal with incomplete
data. }
\label{fig:pairwise}
\end{figure*}



\section{Rank Aggregation with the\\ Nuclear Norm} 
\label{sec:ranking-nn}

Thus far, we have seen how to compute an aggregate
pairwise matrix  from ratings data.
While  has fewer missing entries than  --
roughly 1-80\% missing instead of almost 99\% missing
-- it is still not nearly complete.  
In this section, we discuss how to use the theory
of matrix completion to estimate the scoring
vector underlying the comparison matrix .
These same techniques apply even when  is not
computed from ratings and is measured through
direct pairwise comparisons.  

Let us now state the matrix completion problem formally
\cite{candes2009-exact-completion,recht2009-nuclear-norm}.
Given a matrix  where only a subset of the entries
are known, the goal is to find the 
lowest rank matrix  that agrees with  in all
the non-zeros.  
Let  be the index set corresponding
to the known entries of .
Now define  as a linear map corresponding to the
elements of , i.e.  is a vector where
the th element is defined to be

and where we interpret  as the entry of the 
matrix  for the index pair .
Finally, let  be the values of the specified 
entries of the matrix .  
This idea of matrix completion corresponds with the
solution of 

Unfortunately, like the direct
methods at permutation minimization, this approach 
is NP-hard \cite{vandenberghe1996-semidefinite}.

To make the problem tractable, an increasingly well-known technique is
to replace the  function with the nuclear norm
 \cite{fazel2002-phdthesis}.
For a matrix , the nuclear norm is defined 

where  is the th singular value of
.
The nuclear norm has a few other names:
the Ky-Fan -norm, the Schatten -norm,
and the trace norm (when applied to symmetric matrices),
but we will just use the term nuclear norm here.
It is a convex underestimator of the
rank function on the unit spectral-norm ball , i.e.\ 
and is the largest convex function with this property.
Because the nuclear norm is convex,

is a convex relaxation of \eqref{eq:mc-rank}
analogous to how the -norm is a convex
relaxation of the -norm.  

In \eqref{eq:mc-nn}
we have , which is called a
noiseless completion problem.  Noisy completion
problems only require .  
We present four possibilities inspired by similar
approaches in compressed sensing.
For the compressed sensing problem with noise:

there are four well known formulations: 
\algo{Lasso} \cite{tibshirani1996-lasso}, 
\algo{QP}  \cite{chen1998-atomic}, 
\algo{DS}  \cite{candes2007-dantzig}
and \algo{BPDN}  \cite{fuchs2004-noise}.  
For the noisy matrix completion problem, the same variations apply,
but with the nuclear norm taking the place of the -norm:
\begin{center}
\begin{minipage}{\linewidth}
\makebox[4em][l]{\textsc{lasso}} \2ex]
\begin{minipage}{\linewidth}
\makebox[4em][l]{\textsc{ds}}  \2ex]
\begin{minipage}{\linewidth}
\makebox[4em][l]{\textsc{qp}} \citet{mazumder2009-regularization} \2ex]
\begin{minipage}{\linewidth}
\makebox[4em][l]{\textsc{bpdn}} \citet{mazumder2009-regularization} \ \label{eq:ranking-mc}
 \MINtwo{}{\normof[2]{\sA(\mX) - \vb}}{\normof[*]{\mX} \le 2}{\mX = -\mX^T}

  \mA = \mU 
    \begin{sbmatrix} \lambda_1 \\ 
                    & \lambda_1 \\
                    & & \lambda_2 \\
                    & & & \lambda_2 \\
                    & & & & \ddots \\
                    & & & & & \lambda_j \\
                    & & & & & & \lambda_j 
    \end{sbmatrix} \mV^T
 \mA = \mX \mT \mX^T  \mT = \begin{sbmatrix}
           0 & \lambda_1 \\
           -\lambda_1 & 0 \\
           & & 0 & \lambda_2 \\
           & & -\lambda_2 & 0 \\
           & & & & \ddots \\
\end{sbmatrix}.  \begin{bmatrix} 0 & \lambda_1 \\ -\lambda_1 & 0 \end{bmatrix} = 
\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_1 \end{bmatrix}
\begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix}.

\begin{aligned}
\mA & = 
\underbrace{
	\mX 
	\begin{sbmatrix}
	   0 & 1 \\
	   1 & 0 \\
	   & & 0 & 1 \\
	   & & 1 & 0 \\
	   & & & & \ddots \\
\end{sbmatrix}
}_{= \mU} 
\begin{sbmatrix} 
  	\lambda_1 \\ 
	& \lambda_1 \\
	& & \lambda_2 \\
	& & & \lambda_2 \\
	& & & & \ddots \\
\end{sbmatrix} 
\underbrace{
	\begin{sbmatrix}
	   -1 & 0 \\
	   0 & 1 \\
	   & & -1 & 0 \\
	   & & 0 & 1 \\
	   & & & & \ddots \\
\end{sbmatrix}
	\mX^T
}_{= \mV^T}
. 
\end{aligned}  \MINone{}{ \normof[*]{\mX} }{ \tr(\mX^* \mW_i) = \tr(\mA^* \mW_i) \quad i \in \Omega}  \begin{aligned}
\mathcal{H} & = \mathcal{S} \cup \mathcal{K} \cup \mathcal{D} \text{ where } \\
\mathcal{S} & = \{ 1/\sqrt{2}(\ve_i \ve_j^T + \ve_j \ve_i^T) : 1 \le i < j \le n \}; \\
\mathcal{K} & = \{ \imath/\sqrt{2}(\ve_i \ve_j^T - \ve_j \ve_i^T) : 1 \le i < j \le n \}; \\
\mathcal{D} & = \{ \ve_i \ve_i^T : 1 \le i \le n \}.
\end{aligned}  \MINone{}{\normof[*]{\mX}}{\tr(\mX^* \mW_i) = \tr((\imath \mY)^* \mW_i), \quad \mW_i \in \Omega}
 \begin{aligned}
\max\nolimits_i \tr(\mW_i \mU \mU^* \mW_i) & \le 2 \nu r/n, \text{ and} \\
\max\nolimits_i \tr(\sign(\mA) \mW_i)^2 & \le \nu r/n^2. \\
\end{aligned}  
\mU\mU^* = \frac{\vs \vs^T}{\vs^T \vs} - \frac{1}{n} \ve \ve^T \text{ and }
\sign(\mA) = \frac{1}{\normof{\vs}\sqrt{n}} \mA.
 
\begin{aligned}
\tr(\mS_p \mU \mU^* \mS_p) & = \frac{1}{n} + \frac{s_i^2 + s_j^2}{2 \vs^T \vs} 
	\le (1/n) + \theta\\
\tr(\mD_p \mU \mU^* \mD_p) & = \frac{1}{n} + \frac{s_i^2}{\vs^T \vs} 
	\le (1/n) + \theta\\	
\tr(\sign(\mA) \mK_p)^2 &= \frac{2(s_i - s_j)^2}{n \vs^T \vs} \le (2/n)\rho^2.\\
\end{aligned}
 R_{i,j} = L[ a_i + b_i t_j + E_{i,j} ]  L[\alpha] = 
\begin{cases} 
  1 & \alpha < 1.5 \\
	2 & 1.5 \le \alpha < 2.5 \\
	3 & 2.5 \le \alpha < 3.5 \\
	4 & 3.5 \le \alpha < 4.5 \\
	5 & 4.5 \le \alpha, \end{cases} 
and  is a noise parameter.	
In our experiment, we draw , 
, ,
and .  Here,
 is a standard normal, and
 is a noise
parameter.  As input to our algorithm, 
we sample ratings uniformly at random by
specifying a desired number of average ratings
per user.  We then look at
the Kendall  correlation coefficient
between the true scores  and the output
of our algorithm using the arithmetic mean
pairwise aggregation.  A  value of
1 indicates a perfect ordering correlation
between the two sets of scores.

Figure~\ref{fig:synthetic} shows the results
for  users and  items with 
 and  ratings 
per user on average.  We also vary the
parameter  between  and .
Each thick line with markers plots the 
median value of  in 50 trials.  The
thin adjacency lines show the th and
th percentiles of the 50 trials.  
At all error levels, our algorithm
outperforms the mean rating.  Also,
when there are few ratings per-user
and moderate noise, 
our approach is considerably more
correlated with the true score.  
This evidence supports the
anecdotal results from Netflix in 
Table~\ref{tab:netflix-top-movies}.

\subsection{Netflix}
\label{sec:netflix}

See Table~\ref{tab:netflix-top-movies}
for the top movies produced by our technique in a few circumstances
using all users.  The arithmetic mean results in that table
use only elements of  with at least  pairwise comparisons
(it is a \texttt{am all 30} model in the code below).
And see Figure~\ref{fig:netflix-residuals} for an analysis
of the residuals generated by the fit for different constructions
of the matrix .  
Each residual evaluation of Netflix is described by a code.
For example, \texttt{sb all 0} is a strict-binary pairwise
matrix  from all Netflix users and  in Algorithm~\ref{fig:nnra}
(i.e. accept all pairwise comparisons).  Alternatively,
\texttt{am 6 30} denotes an arithmetic-mean pairwise matrix 
from Netflix users with at least 6 ratings, where each entry in 
had 30 users supporting it.  The other abbreviations are
\texttt{gm}: geometric mean; \texttt{bc}: binary comparison; and
\texttt{lo}: log-odds ratio.

These residuals show that we get better rating
fits by only using frequently compared movies, but that there 
are only minor changes in the fits when excluding users that
rate few movies.  The difference between
the score-based residuals  (red points) and the \algo{svp} residuals 
 (blue points) show that 
excluding comparisons leads to ``overfitting'' 
in the \algo{svp} residual.  This suggests that increasing
the parameter  should be done with care and good
checks on the residual norms.

\enlargethispage{\baselineskip}
To check that a rank- approximation is reasonable,
we increased the target rank in the \algo{svp} solver
to  to investigate.  For the arithmetic mean (6,30) 
model, the relative residual at rank- is  and
at rank- is .  Meanwhile, the nuclear
norm increases from around 14000 to around 17000.
These results show that the change in the fit is minimal
and our rank-2 approximation and its scores
should represent a reasonable ranking.

 \begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{netflix-residuals-2}
  \caption{The labels on each residual show the method to generate
   the pairwise scores and how we truncated the Netflix data. 
	 Red points are the residuals from the scores, and blue 
	 points are the final residuals from the SVP algorithm.
	 Please see the discussion in \S\ref{sec:netflix}.} 
   \label{fig:netflix-residuals}
	 \vspace{-\baselineskip}
 \end{figure}

\section{Conclusion}




Existing principled techniques such
as computing a Kemeny optimal ranking or finding a minimize feedback 
arc set are NP-hard.  These approaches
are inappropriate in large scale rank aggregation settings.
Our proposal is (i) measure pairwise scores  and (ii)
solve a matrix completion problem to determine the quality
of items.  This idea is both principled and functional with
significant missing data.  The results of our rank aggregation
on the Netflix problem (Table~\ref{tab:netflix-top-movies}) reveal
popular and high quality movies.  These are interesting results
and could easily have a home on a ``best movies in Netflix''
web page.  Computing a rank aggregation
with this technique is not NP-hard.  It only requires
solving a convex optimization problem with a unique global
minima.  Although we did not record computation times,
the most time consuming piece of work is computing the pairwise
comparison matrix .  In a practical setting, this could
easily be done with a MapReduce computation.

To compute these solutions, we adapted the \algo{svp} solver
for matrix completion \cite{Jain-2010-SVP}.  This process
involved (i) studying the singular value decomposition of a 
skew-symmetric matrix (Lemmas~\ref{lem:sssvd} and \ref{lem:rank-k-sssvd})
and (ii) showing that the \algo{svp} solver preserves a skew-symmetric
approximation through its computation (Theorem~\ref{thm:sssvp}).
Because the \algo{svp} solver computes with an explicitly chosen rank,
these techniques work well for large scale rank aggregation problems.

We believe the combination of pairwise aggregation and matrix completion
is a fruitful direction for future research.  We
plan to explore optimizing the \algo{svp} algorithm to exploit
the skew-symmetric constraint,
extending our recovery result to the noisy case,
and investigating additional data.


\let\thefootnote\relax
\footnotetext{\textit{Funding.} \scriptsize
David F.\ Gleich was supported in 
part by the Natural Sciences and Engineering Research
 Council of Canada along with the Department of Energy's 
John von Neumann fellowship.
Lek-Heng Lim acknowledges the support of NSF CAREER Award DMS 1057064.
}

 
\bibliography{all-bibliography}
\bibliographystyle{abbrvnat}

\end{document} 
