\documentclass[sigconf]{acmart}

\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{graphicx}


\hypersetup{colorlinks=true,urlcolor=blue}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\setlist[itemize,1]{leftmargin=2em}

\fancyhead{}
\settopmatter{printacmref=false, printfolios=false}
\setcopyright{none}
\acmConference{WWW'18}{April 23--27, 2018}{Lyon, France.}
\acmYear{2018}
\copyrightyear{2018}
\acmPrice{15.00}


\begin{document}
\title[DKN: Deep Knowledge-Aware Network for News Recommendation]{DKN: Deep Knowledge-Aware Network\\for News Recommendation}


\author{Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo}
\authornote{M. Guo is the corresponding author. This work was partially sponsored by the National Basic Research 973 Program of China under Grant 2015CB352403.}
\affiliation{Shanghai Jiao Tong University, Shanghai, China}
\affiliation{Microsoft Research Asia, Beijing, China}
\email{wanghongwei55@gmail.com,{fuzzhang,xingx}@microsoft.com,guo-my@cs.sjtu.edu.cn}


\renewcommand{\shortauthors}{H. Wang et al.}


\begin{abstract}
	Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users.
	In general, news language is highly condensed, full of knowledge entities and common sense.
	However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news.
	The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably.
To solve the above problem, in this paper, we propose a \textit{deep knowledge-aware network} (DKN) that incorporates knowledge graph representation into news recommendation.
	DKN is a content-based deep recommendation framework for click-through rate prediction.
	The key component of DKN is a multi-channel and word-entity-aligned \textit{knowledge-aware convolutional neural network} (KCNN) that fuses semantic-level and knowledge-level representations of news.
	KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution.
	In addition, to address users' diverse interests, we also design an \textit{attention} module in DKN to dynamically aggregate a user's history with respect to current candidate news.
	Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models.
\end{abstract}


\keywords{News recommendation; knowledge graph representation; deep neural networks; attention model}

\maketitle



\section{Introduction}
	With the advance of the World Wide Web, people's news reading habits have gradually shifted from traditional media such as newspapers and TV to the Internet.
	Online news websites, such as Google News\footnote{\url{https://news.google.com/}} and Bing News\footnote{\url{https://www.bing.com/news}}, collect news from various sources and provide an aggregate view of news for readers.
	A notorious problem with online news platforms is that the volume of articles can be overwhelming to users.
	To alleviate the impact of information overloading, it is critical to help users target their reading interests and make personalized recommendations \cite{phelan2009using, li2010contextual, liu2010personalized, son2013location, bansal2015content, okura2017embedding}.
	
	Generally, news recommendation is quite difficult as it poses three major challenges.
	First, unlike other items such as movies \cite{diao2014jointly} and restaurants \cite{fu2014user}, news articles are highly time-sensitive and their relevance expires quickly within a short period (see Section \ref{sec:dd}).
	Out-of-date news are substituted by newer ones frequently, which makes traditional ID-based methods such as collaborative filtering (CF) \cite{wang2011collaborative} less effective.
	Second, people are topic-sensitive in news reading as they are usually interested in multiple specific news categories (see Section \ref{sec:cs}).
	How to dynamically measure a user's interest based on his diversified reading history for current candidate news is key to news recommender systems.
	Third, news language is usually highly condensed and comprised of a large amount of knowledge entities and common sense.
	For example, as shown in Figure \ref{fig:buzzfeed}, a user clicks a piece of news with title ``\textsf{Boris Johnson Has Warned Donald Trump To Stick To The Iran Nuclear Deal}" that contains four knowledge entities: ``\textsf{Boris Johnson}'', ``\textsf{Donald Trump}'', ``\textsf{Iran}'' and ``\textsf{Nuclear}''.
	In fact, the user may also be interested in another piece of news with title ``\textsf{North Korean EMP Attack Would Cause Mass U.S. Starvation, Says Congressional Report}'' with high probability, which shares a great deal of contextual knowledge and is strongly connected with the previous one in terms of commonsense reasoning.
	However, traditional semantic models \cite{mikolov2013distributed} or topic models \cite{blei2003latent} can only find their relatedness based on co-occurrence or clustering structure of words, but are hardly able to discover their latent knowledge-level connection.
	As a result, a user's reading pattern will be narrowed down to a limited circle and cannot be reasonably extended based on existing recommendation methods.
	
	\begin{figure}[t]
		\centering
  		\includegraphics[width=.4\textwidth]{figures/buzzfeed.pdf}
  		\caption{Illustration of two pieces of news connected through knowledge entities.}
  		\label{fig:buzzfeed}
  		\vspace{-0.1in}
	\end{figure}
	
	To extract deep logical connections among news, it is necessary to introduce additional \textit{knowledge graph} information into news recommendations.
	A knowledge graph is a type of directed heterogeneous graph in which nodes correspond to \textit{entities} and edges correspond to \textit{relations}.
	Recently, researchers have proposed several academic knowledge graphs such as NELL\footnote{\url{http://rtw.ml.cmu.edu/rtw/}} and DBpedia\footnote{\url{http://wiki.dbpedia.org/}}, as well as commercial ones such as Google Knowledge Graph\footnote{\url{https://www.google.com/intl/bn/insidesearch/features/search/knowledge.html}} and Microsoft Satori\footnote{\url{https://searchengineland.com/library/bing/bing-satori}}.
	These knowledge graphs are successfully applied in scenarios of machine reading\cite{yang2017leveraging}, text classification\cite{wang2017combining}, and word embedding\cite{xu2014rc}.
	
	Considering the above challenges in news recommendation and inspired by the wide success of leveraging knowledge graphs, in this paper, we propose a novel framework that takes advantage of external knowledge for news recommendation, namely the \textit{deep knowledge-aware network} (DKN).
	DKN is a content-based model for click-through rate (CTR) prediction, which takes one piece of candidate news and one user's click history as input, and outputs the probability of the user clicking the news.
	Specifically, for a piece of input news, we first enrich its information by associating each word in the news content with a relevant entity in the knowledge graph.
	We also search and use the set of contextual entities of each entity (i.e., its immediate neighbors in the knowledge graph) to provide more complementary and distinguishable information.
	Then we design a key component in DKN, namely \textit{knowledge-aware convolutional neural networks} (KCNN), to fuse the word-level and knowledge-level representations of news and generate a knowledge-aware embedding vector.
	Distinct from existing work \cite{wang2017combining}, KCNN is:
	1) \textit{multi-channel}, as it treats word embedding, entity embedding, and contextual entity embedding of news as multiple stacked channels just like colored images;
	2) \textit{word-entity-aligned}, as it aligns a word and its associated entity in multiple channels and applies a transformation function to eliminate the heterogeneity of the word embedding and entity embedding spaces.


	Using KCNN, we obtain a knowledge-aware representation vector for each piece of news.
	To get a dynamic representation of a user with respect to current candidate news, we use an \textit{attention} module to automatically match candidate news to each piece of clicked news, and aggregate the user's history with different weights.
	The user's embedding and the candidate news' embedding are finally processed by a deep neural network (DNN) for CTR prediction.
	
	Empirically, we apply DKN to a real-world dataset from Bing News with extensive experiments.
	The results show that DKN achieves substantial gains over state-of-the-art deep-learning-based methods for recommendation.
	Specifically, DKN significantly outperforms baselines by  to  on F1 and  to  on AUC with a significance level of .
	The results also prove that the usage of knowledge and an attention module can bring additional  and  in improvement, respectively, in the DKN framework.
	Moreover, we present a visualization result of attention values to intuitively demonstrate the efficacy of the usage of the knowledge graph in Section \ref{sec:cs}.



\section{Preliminaries}
	In this section, we present several concepts and models related to this work, including knowledge graph embedding and convolutional neural networks for sentence representation learning.
	
	\subsection{Knowledge Graph Embedding}
	\label{sec:kge}
		A typical knowledge graph consists of millions of entity-relation-entity triples , in which ,  and  represent the head, the relation, and the tail of a triple, respectively.
Given all the triples in a knowledge graph, the goal of knowledge graph embedding is to learn a low-dimensional representation vector for each entity and relation that preserves the structural information of the original knowledge graph.
		Recently, translation-based knowledge graph embedding methods have received great attention due to their concise models and superior performance.
		To be self-contained, we briefly review these translation-based methods in the following.
		
		\begin{itemize}
			\item
			\textbf{TransE} \cite{bordes2013translating} wants  when  holds, where ,  and  are the corresponding representation vector of ,  and .
			Therefore, TransE assumes the score function
			
			is low if  holds, and high otherwise.
		
			\item
			\textbf{TransH} \cite{wang2014knowledge} allows entities to have different representations when involved in different relations by projecting the entity embeddings into relation hyperplanes:

			where  and  are the projections of  and  to the hyperplane , respectively, and .
		
			\item
			\textbf{TransR} \cite{lin2015learning} introduces a projection matrix  for each relation  to map entity embeddings to the corresponding relation space.
			The score function in TransR is defined as
			
			where  and .
		
			\item
			\textbf{TransD} \cite{ji2015knowledge} replaces the projection matrix in TransR by the product of two projection vectors of an entity-relation pair:
			
			where , , ,  and  are another set of vectors for entities and relations, and  is the identity matrix.
		\end{itemize}
		
		To encourage the discrimination between correct triples and incorrect triples, for all the methods above, the following margin-based ranking loss is used for training:
		
		where  is the margin,  and  are the set of correct triples and incorrect triples.
		
		
	\subsection{CNN for Sentence Representation Learning}
	\label{sec:cnn_srl}
		\begin{figure}[t]
			\centering
  			\includegraphics[width=.43\textwidth]{figures/KimCNN.pdf}
  			\caption{A typical architecture of CNN for sentence representation learning \cite{kim2014convolutional}.}
  			\label{fig:kim_cnn}
		\end{figure}
		
		Traditional methods \cite{agarwal2009regression, wang2017joint} usually represent sentences using the bag-of-words (BOW) technique, i.e., taking word counting statistics as the feature of sentences.
		However, BOW-based methods ignore word orders in sentences and are vulnerable to the sparsity problem, which leads to poor generalization performance.
		A more effective way to model sentences is to represent each sentence in a given corpus as a distributed low-dimensional vector.
		Recently, inspired by the success of applying convolutional neural networks (CNN) in the filed of computer vision \cite{krizhevsky2012imagenet}, researchers have proposed many CNN-based models for sentence representation learning \cite{kim2014convolutional, kalchbrenner2014convolutional, zhang2015character, conneau2016very} \footnote{Researchers have also proposed other types of neural network models for sentence modeling such as recurrent neural networks \cite{tai2015improved}, recursive neural networks \cite{socher2013recursive}, and hybrid models \cite{lai2015recurrent}. However, CNN-based models are empirically proven to be superior than others \cite{hong2015sentiment}, since they can detect and extract specific local patterns from sentences due to the convolution operation. To keep our presentation focused, we only discuss CNN-based models in this paper.}.
		In this subsection, we introduce a typical type of CNN architecture, namely Kim CNN \cite{kim2014convolutional}.
		
		Figure \ref{fig:kim_cnn} illustrates the architecture of Kim CNN.
		Let  be the raw input of a sentence of length , and  be the word embedding matrix of the input sentence, where  is the embedding of the -th word in the sentence and  is the dimension of word embeddings.
		A convolution operation with filter  is then applied to the word embedding matrix , where  () is the window size of the filter.
		Specifically, a feature  is generated from a sub-matrix  by
		
		where  is a non-linear function,  is the convolution operator, and  is a bias.
		After applying the filter to every possible position in the word embedding matrix, a feature map
		
		is obtained, then a max-over-time pooling operation is used on feature map  to identify the most significant feature:
		
		One can use multiple filters (with varying window sizes) to obtain multiple features, and these features are concatenated together to form the final sentence representation.
		
	
	
	\section{Problem Formulation}
		We formulate the news recommendation problem in this paper as follows.
		For a given user  in the online news platform, we denote his click history as , where  () is the title\footnote{In addition to title, it is also viable to use abstracts or snippets of news. In this paper, we only take news titles as input, since a title is a decisive factor affecting users' choice of reading. But note that our approach can be easily generalized to any sort of news-related texts.} of the -th news clicked by user , and  is the total number of user 's clicked news.
		Each news title  is composed of a sequence of words, i.e., , where each word  may be associated with an entity  in the knowledge graph.
		For example, in the title ``\textsf{Trump praises Las Vegas medical team}'', ``\textsf{Trump}'' is linked with the entity ``\textsf{Donald Trump}", while ``\textsf{Las}'' and ``\textsf{Vegas}'' are linked with the entity ``\textsf{Las Vegas}".
		Given users' click history as well as the connection between words in news titles and entities in the knowledge graph, we aim to predict whether user  will click a candidate news  that he has not seen before.



\section{Deep Knowledge-Aware Network}
	In this section, we present the proposed DKN model in detail.
	We first introduce the overall framework of DKN, then discuss the process of knowledge distillation from a knowledge graph, the design of knowledge-aware convolutional neural networks (KCNN), and the attention-based user interest extraction, respectively.
	
	\subsection{DKN Framework}
		\begin{figure*}[t]
			\centering
  			\includegraphics[width=.87\textwidth]{figures/framework.pdf}
  			\caption{Illustration of the DKN framework.}
  			\label{fig:framework}
		\end{figure*}
	
		The framework of DKN is illustrated in Figure \ref{fig:framework}.
		We introduce the architecture of DKN from the bottom up.
		As shown in Figure \ref{fig:framework}, DKN takes one piece of candidate news and one piece of a user's clicked news as input.
		For each piece of news, a specially designed KCNN is used to process its title and generate an embedding vector.
		KCNN is an extension of traditional CNN that allows flexibility in incorporating symbolic knowledge from a knowledge graph into sentence representation learning.
		We will detail the process of knowledge distillation in Section \ref{sec:kd} and the KCNN module in Section \ref{sec:kcnn}, respectively.
		By KCNN, we obtain a set of embedding vectors for a user's clicked history.
		To get final embedding of the user with respect to the current candidate news, we use an attention-based method to automatically match the candidate news to each piece of his clicked news, and aggregate the user's historical interests with different weights.
		The details of attention-based user interest extraction are presented in Section \ref{sec:auie}.
		The candidate news embedding and the user embedding are concatenated and fed into a deep neural network (DNN) to calculate the predicted probability that the user will click the candidate news.
	
	
	\subsection{Knowledge Distillation}
	\label{sec:kd}
		\begin{figure}[t]
			\centering
  			\includegraphics[width=.4\textwidth]{figures/knowledge_distillation.pdf}
  			\caption{Illustration of knowledge distillation process.}
  			\label{fig:knowledge_distillation}
		\end{figure}
		
		The process of knowledge distillation is illustrated in Figure \ref{fig:knowledge_distillation}, which consists of four steps.
		First, to distinguish knowledge entities in news content, we utilize the technique of \textit{entity linking} \cite{milne2008learning, sil2013re} to disambiguate mentions in texts by associating them with predefined entities in a knowledge graph.
		Based on these identified entities, we construct a sub-graph and extract all relational links among them from the original knowledge graph.
		Note that the relations among identified entities only may be sparse and lack diversity.
		Therefore, we expand the knowledge sub-graph to all entities within one hop of identified ones.
		Given the extracted knowledge graph, a great many knowledge graph embedding methods, such as TransE \cite{bordes2013translating}, TransH \cite{wang2014knowledge}, TransR \cite{lin2015learning}, and TransD \cite{ji2015knowledge}, can be utilized for entity representation learning.
		Learned entity embeddings are taken as the input for KCNN in the DKN framework.
		
		It should be noted that though state-of-the-art knowledge graph embedding methods could generally preserve the structural information in the original graph, we find that the information of learned embedding for a single entity is still limited when used in subsequent recommendations.
		To help identify the position of entities in the knowledge graph, we propose extracting additional contextual information for each entity.
		The ``context'' of entity  is defined as the set of its immediate neighbors in the knowledge graph, i.e.,
		
		where  is a relation and  is the knowledge graph.
		Since the contextual entities are usually closely related to the current entity with respect to semantics and logic, the usage of context could provide more complementary information and assist in improving the identifiability of entities.
		Figure \ref{fig:context} illustrates an example of context.
		In addition to use the embedding of ``\textsf{Fight Club}'' itself to represent the entity, we also include its contexts, such as ``\textsf{Suspense}'' (genre), ``\textsf{Brad Pitt}'' (actor), ``\textsf{United States}'' (country) and ``\textsf{Oscars}'' (award), as its identifiers.
		Given the context of entity , the \textit{context embedding} is calculated as the average of its contextual entities:
		
		where  is the \textit{entity embedding} of  learned by knowledge graph embedding.
		We empirically demonstrate the efficacy of context embedding in the experiment section.
		
		\begin{figure}[t]
			\centering
  			\includegraphics[width=.4\textwidth]{figures/context.pdf}
  			\caption{Illustration of context of an entity in a knowledge graph.}
  			\label{fig:context}
		\end{figure}
		
		
	\subsection{Knowledge-aware CNN}
	\label{sec:kcnn}
		Following the notations used in Section \ref{sec:cnn_srl}, we use  to denote the raw input sequence of a news title  of length , and  to denote the word embedding matrix of the title, which can be pre-learned from a large corpus or randomly initialized.
		After the knowledge distillation introduced in Section \ref{sec:kd}, each word  may also be associated with an entity embedding  and the corresponding context embedding , where  is the dimension of entity embedding.
		
		Given the input above, a straightforward way to combine words and associated entities is to treat the entities as ``pseudo words'' and concatenate them to the word sequence \cite{wang2017combining}, i.e.,
		
		where  is the set of entity embeddings associated with this news title.
		The obtained new sentence  is fed into CNN \cite{kim2014convolutional} for further processing.
		However, we argue that this simple concatenating strategy has the following limitations:
		1) The concatenating strategy breaks up the connection between words and associated entities and is unaware of their alignment.
		2) Word embeddings and entity embeddings are learned by different methods, meaning it is not suitable to convolute them together in a single vector space.
		3) The concatenating strategy implicitly forces word embeddings and entity embeddings to have the same dimension, which may not be optimal in practical settings since the optimal dimensions for word  and entity embeddings may differ from each other.
		
		Being aware of the above limitations, we propose a \textit{multi-channel} and \textit{word-entity-aligned} KCNN for combining word semantics and knowledge information.
		The architecture of KCNN is illustrated in the left lower part of Figure \ref{fig:framework}.
		For each news title , in addition to use its word embeddings  as input, we also introduce the \textit{transformed entity embeddings}
		
		and \textit{transformed context embeddings}
		
		as source of input\footnote{ and  are set as zero if  has no corresponding entity.}, where  is the transformation function.
		In KCNN,  can be either linear
		
		or non-linear
		
		where  is the trainable transformation matrix and  is the trainable bias.
		Since the transformation function is continuous, it can map the entity embeddings and context embeddings from the entity space to the word space while preserving their original spatial relationship.
		Note that word embeddings , transformed entity embeddings  and transformed context embeddings  are the same size and serve as the multiple channels analogous to colored images. We therefore align and stack the three embedding matrices as
		
		
		After getting the multi-channel input , similar to Kim CNN \cite{kim2014convolutional}, we apply multiple filters  with varying window sizes  to extract specific local patterns in the news title.
		The local activation of sub-matrix  with respect to  can be written as
		
		and we use a max-over-time pooling operation on the output feature map to choose the largest feature:
		
		All features  are concatenated together and taken as the final representation  of the input news title , i.e.,
		
		where  is the number of filters.
		
		
	\subsection{Attention-based User Interest Extraction}
	\label{sec:auie}
		Given user  with clicked history , the embeddings of his clicked news can be written as , , ..., .
		To represent user  for the current candidate news , one can simply average all the embeddings of his clicked news titles:
		
		However, as discussed in the introduction, a user's interest in news topics may be various, and user 's clicked items are supposed to have different impacts on the candidate news  when considering whether user  will click .
		To characterize user's diverse interests, we use an attention network \cite{wang2017dynamic, zhou2017deep} to model the different impacts of the user's clicked news on the candidate news.
		The attention network is illustrated in the left upper part of Figure \ref{fig:framework}.
		Specifically, for user 's clicked news  and candidate news , we first concatenate their embeddings, then apply a DNN  as the attention network and the softmax function to calculate the normalized impact weight: 
		
		The attention network  receives embeddings of two news titles as input and outputs the impact weight.
		The embedding of user  with respect to the candidate news  can thus be calculated as the weighted sum of his clicked news title embeddings:
		
		
		Finally, given user 's embedding  and candidate news 's embedding , the probability of user  clicking news  is predicted by another DNN :
		
		
		We will demonstrate the efficacy of the attention network in the experiment section.
		

\section{Experiments}
	In this section, we present our experiments and the corresponding results, including dataset analysis and comparison of models.
	We also give a case study about user's reading interests and make discussions on tuning hyper-parameters.
	
	\subsection{Dataset Description}
	\label{sec:dd}			
		Our dataset comes from the server logs of Bing News.
		Each piece of log mainly contains the timestamp, user id, news url, news title, and click count (0 for no click and 1 for click).
		We collect a randomly sampled and balanced dataset from October 16, 2016 to June 11, 2017 as the training set, and from June 12, 2017 to August 11, 2017 as the test set.
		Additionally, we search all occurred entities in the dataset as well as the ones within their one hop in the Microsoft Satori knowledge graph, and extract all edges (triples) among them with confidence greater than 0.8.
		The basic statistics and distributions of the news dataset and the extracted knowledge graph are shown in Table \ref {table:statistics} and Figure \ref{fig:statistics}, respectively.
		
		\begin{table}
			\centering
			\small
			\caption{Basic statistics of the news dataset and the extracted knowledge graph.}
			\vspace{-0.1in}
			\begin{tabular}{|c|r||c|r|}
				\hline
				\# users & 141,487 & \# triples & 7,145,776\\
				\hline
				\# news & 535,145 & avg. \# words per title & 7.9\\
				\hline
				 \# logs & 1,025,192 & avg. \# entities per title & 3.7\\
				\hline
				\# entities & 336,350  & \multirow{2}{*}{\tabincell{l}{avg. \# contextual\\entities per entity}} & \multirow{2}{*}{42.5}\\
				\cline{1-2}
				\# relations & 4,668  &  &\\
				\hline
			\end{tabular}
			\label{table:statistics}
			\scriptsize \flushleft{``\#'' denotes ``the number of''.}
		\end{table}
		
		Figure \ref{fig:statistics_a} illustrates the distribution of the length of the news life cycle, where we define the life cycle of a piece of news as the period from its publication date to the date of its last received click.
		We observe that about  of news are clicked within two days, which proves that online news is extremely time-sensitive and are substituted by newer ones with high frequency.
		Figure \ref{fig:statistics_b} illustrates the distribution of the number of clicked pieces of news for a user.
		 of users clicked no more than five pieces of news, which demonstrates the data sparsity in the news recommendation scenario.
		
		Figures \ref{fig:statistics_c} and \ref{fig:statistics_d} illustrate the distributions of the number of words (without stop words) and entities in a news title, respectively.
		The average number per title is  for words and  for entities, showing that there is one entity in almost every two words in news titles on average.
		The high density of the occurrence of entities also empirically justifies the design of KCNN.
		
		Figures \ref{fig:statistics_e} and \ref{fig:statistics_f} present the distribution of occurrence times of an entity in the news dataset and the distribution of the number of contextual entities of an entity in extracted knowledge graph, respectively.
		We can conclude from the two figures that the occurrence pattern of entities in online news is sparse and has a long tail ( of entities occur no more than ten times), but entities generally have abundant contexts in the knowledge graph: the average number of context entities per entity is  and the maximum is .
		Therefore, contextual entities can greatly enrich the representations for a single entity in news recommendation.
		
		\begin{figure}
			\centering
            		\begin{subfigure}[b]{0.23\textwidth}
                		\includegraphics[width=\textwidth]{charts/dataset_1.eps}
                		\caption{Distribution of the length of news life cycle}
                		\label{fig:statistics_a}
            		\end{subfigure}
            		\hfill
            		\begin{subfigure}[b]{0.23\textwidth}
                		\includegraphics[width=\textwidth]{charts/dataset_2.eps}
                		\caption{Distribution of the number of clicked news of a user}
                		\label{fig:statistics_b}
            		\end{subfigure}
            		\hfill
            		\begin{subfigure}[b]{0.23\textwidth}
                		\includegraphics[width=\textwidth]{charts/dataset_3.eps}
                		\caption{Distribution of the number of words in a news title}
                		\label{fig:statistics_c}
            		\end{subfigure}
        		\hfill
            		\begin{subfigure}[b]{0.23\textwidth}
                		\includegraphics[width=\textwidth]{charts/dataset_4.eps}
                		\caption{Distribution of the number of entities in a news title}
                		\label{fig:statistics_d}
            		\end{subfigure}
            		\hfill
            		\begin{subfigure}[b]{0.23\textwidth}
                		\includegraphics[width=\textwidth]{charts/dataset_5.eps}
                		\caption{Distribution of the occurrence times of an entity in the news dataset}
                		\label{fig:statistics_e}
            		\end{subfigure}
            		\hfill
            		\begin{subfigure}[b]{0.23\textwidth}
                		\includegraphics[width=\textwidth]{charts/dataset_6.eps}
                		\caption{Distribution of the number of contextual entities of an entity in the knowledge graph}
                		\label{fig:statistics_f}
            		\end{subfigure}
            		\caption{Illustration of statistical distributions in news dataset and extracted knowledge graph.}
            		\label{fig:statistics}
        	\end{figure}
		
	
	\subsection{Baselines}
		We use the following state-of-the-art methods as baselines in our experiments:
		\begin{itemize}
			\item
				\textbf{LibFM} \cite{rendle2012factorization} is a state-of-the-art feature-based factorization model and widely used in CTR scenarios.
				In this paper, the input feature of each piece of news for LibFM is comprised of two parts: TF-IDF features and averaged entity embeddings.
				We concatenate the feature of a user and candidate news to feed into LibFM.
			\item
				\textbf{KPCNN} \cite{wang2017combining} attaches the contained entities to the word sequence of a news title and uses Kim CNN to learn representations of news, as introduced in Section \ref{sec:kcnn}.
			\item
				\textbf{DSSM} \cite{huang2013learning} is a deep structured semantic model for document ranking using word hashing and multiple fully-connected layers.
				In this paper, the user's clicked news is treated as the query and the candidate news are treated as the documents.
			\item
				\textbf{DeepWide} \cite{cheng2016wide} is a general deep model for recommendation, combining a (wide) linear channel with a (deep) non-linear channel.
				Similar to LibFM, we use the concatenated TF-IDF features and averaged entity embeddings as input to feed both channels.
			\item
				\textbf{DeepFM} \cite{guo2017deepfm} is also a general deep model for recommendation, which combines a component of factorization machines and a component of deep neural networks that share the input.
				We use the same input as in LibFM for DeepFM.
			\item
				\textbf{YouTubeNet} \cite{covington2016deep} is proposed to recommend videos from a large-scale candidate set in YouTube using a deep candidate generation network and a deep ranking network.
				In this paper, we adapt the deep raking network to the news recommendation scenario.
			\item
				\textbf{DMF} \cite{xue2017deep} is a deep matrix factorization model for recommender systems which uses multiple non-linear layers to process raw rating vectors of users and items.
				We ignore the content of news and take the implicit feedback as input for DMF.
		\end{itemize}
		
		Note that except for LibFM, other baselines are all based on deep neural networks since we aim to compare our approach with state-of-the-art deep learning models.
		Additionally, except for DMF which is based on collaborative filtering, other baselines are all content-based or hybrid methods.
		
	
	\subsection{Experiment Setup}
	\label{sec:es}
		We choose TransD \cite{ji2015knowledge} to process the knowledge graph and learn entity embeddings, and use the non-linear transformation function in Eq. (\ref{eq:nonlinear}) in KCNN.
		The dimension of both word embeddings and entity embeddings are set as .
		The number of filters are set as  for each of the window sizes , , , .		
		We use Adam \cite{kingma2014adam} to train DKN by optimizing the log loss.
		We will further study the variants of DKN and the sensitivity of key parameters in Sections \ref{sec:cm} and \ref{sec:ps}, respectively.
		To compare DKN with baselines, we use \textit{F1} and \textit{AUC} value as the evaluation metrics.
		
		The key parameter settings for baselines are as follows.
		For KPCNN, the dimensions of word embeddings and entity embeddings are both set as .
		For DSSM, the dimension of semantic feature is set as .
		For DeepWide, the final representations for deep and wide components are both set as .
		For YouTubeNet, the dimension of final layer is set as .
		For LibFM and DeepFM, the dimensionality of the factorization machine is set as .
		For DMF, the dimension of latent representation for users and items is set as .
		The above settings are for fair consideration.
		Other parameters in the baselines are set as default.
		Each experiment is repeated five times, and we report the average and maximum deviation as results.
	
	
		\begin{table}[t]
			\centering
			\small
			\caption{Comparison of different models.}
			\vspace{-0.1in}
			\begin{tabular}{l|l|l|c}
				\hline
				\makecell[c]{Models} & \makecell[c]{F1} & \makecell[c]{AUC} & \textit{p}-value\\
				\hline
				DKN & \textbf{68.9  1.5} & \textbf{65.9  1.2} &  \\
				LibFM & 61.8  2.1 (-10.3) & 59.7  1.8 (-9.4) &  \\
				LibFM(-) & 61.1  1.9 (-11.3) & 58.9  1.7 (-10.6) &  \\
				KPCNN & 67.0  1.6 (-2.8) & 64.2  1.4 (-2.6) & 0.098 \\
				KPCNN(-) & 65.8  1.4 (-4.5) & 63.1  1.5 (-4.2) & 0.036 \\
				DSSM & 66.7  1.8 (-3.2) & 63.6  2.0 (-3.5) & 0.063 \\
				DSSM(-) & 66.1  1.6 (-4.1) & 63.2  1.8 (-4.1) & 0.045 \\
				DeepWide & 66.0 1.2 (-4.2) & 63.3  1.5 (-3.9) & 0.039 \\
				DeepWide(-) & 63.7  0.9 (-7.5) & 61.5  1.1 (-6.7) & 0.004 \\
				DeepFM & 63.8  1.5 (-7.4) & 61.2  2.3 (-7.1) & 0.014 \\
				DeepFM(-) & 64.0  1.9 (-7.1) & 61.1  1.8 (-7.3) & 0.007 \\
				YouTubeNet & 65.5  1.2 (-4.9) & 63.0  1.4 (-4.4) & 0.025 \\
				YouTubeNet(-) & 65.1  0.7 (-5.5) & 62.1  1.3 (-5.8) & 0.011 \\
				DMF & 57.2  1.2 (-17.0) & 55.3  1.0 (-16.1) &  \\
				\hline
			\end{tabular}
			\label{table:comparison}
			\scriptsize \flushleft{* ``(-)'' denotes ``without input of entity embeddings''.}
			\scriptsize \flushleft{** -value is the probability of no significant difference with DKN on AUC by \textit{t}-test.}
		\end{table}
			
		\begin{figure}[t]
			\centering
  			\includegraphics[width=.38\textwidth]{charts/result.eps}
  			\caption{AUC score of DKN and baselines over ten days (Sep. 01-10, 2017).}
  			\label{fig:result}
		\end{figure}
			
		\begin{table}[t]
			\centering
			\small
			\caption{Comparison among DKN variants.}
			\vspace{-0.1in}
			\begin{tabular}{l|l|l}
				\hline
				\makecell[c]{Variants} & \makecell[c]{F1} & \makecell[c]{AUC}\\
				\hline
				DKN with entity and context emd. & \textbf{68.8  1.4} & \textbf{65.7  1.1}\\
				DKN with entity emd. only & 67.2  1.2 & 64.8  1.0\\
				DKN with context emd. only & 66.5  1.5 & 64.2  1.3\\
				DKN without entity nor context emd. & 66.1 1.4 & 63.5  1.1\\
				\hline
				DKN + TransE & 67.6  1.6 & 65.0  1.3\\
				DKN + TransH & 67.3  1.3 & 64.7  1.2 \\
				DKN + TransR & 67.9  1.5 & 65.1  1.5\\
				DKN + TransD & \textbf{68.8  1.3} & \textbf{65.8  1.4}\\
				\hline
				DKN with non-linear mapping & \textbf{69.0  1.7} & \textbf{66.1  1.4}\\
				DKN with linear mapping & 67.1  1.5 & 64.9  1.3\\
				DKN without mapping & 66.7  1.6 & 63.7  1.6\\
				\hline
				DKN with attention & \textbf{68.7  1.3} & \textbf{65.7  1.2}\\
				DKN without attention & 67.0  1.0 & 64.8  0.8\\
				\hline
			\end{tabular}
			\label{table:variants}
		\end{table}
		
		\begin{table*}
			\centering
			\small
			\setlength{\tabcolsep}{4pt}
			\caption{Illustration of training and test logs for a randomly sampled user (training logs with label 0 are omitted).}
			\vspace{-0.1in}
			\begin{tabular}{c|c|c|l|l|c||c}
				\hline
				& \textbf{No.} & \textbf{Date} & \makecell[c]{\textbf{News title}} & \makecell[c]{\textbf{Entities}} & \textbf{Label} & \textbf{Category}\\
				\hline
				\multirow{9}{*}{\rotatebox{90}{training}} & 1 & 12/25/2016 & Elon Musk teases huge upgrades for Tesla's supercharger network & Elon Musk; Tesla Inc. & 1 & Cars\\
				& 2 & 03/25/2017 & Elon Musk offers Tesla Model 3 sneak peek & Elon Musk; Tesla Model 3 & 1 & Cars\\
				& 3 & 12/14/2016 & Google fumbles while Tesla sprints toward a driverless future & Google Inc.; Tesla Inc. & 1 & Cars\\
				& 4 & 12/15/2016 & Trump pledges aid to Silicon Valley during tech meeting & Donald Trump; Silicon Valley & 1 & Politics\\
				& 5 & 03/26/2017 & Donald Trump is a big reason why the GOP kept the Montana House seat & Donald Trump; GOP; Montana & 1 & Politics\\
				& 6 & 05/03/2017 & North Korea threat: Kim could use nuclear weapons as ``blackmail'' & North Korea; Kim Jong-un & 1 & Politics\\
				& 7 & 12/22/2016 & Microsoft sells out of unlocked Lumia 950 and Lumia 950 XL in the US & Microsoft; Lumia; United States & 1 & Other\\
				& 8 & 12/08/2017 & 6.5 magnitude earthquake recorded off the coast of California & earthquake; California & 1 & Other\\
				& & & ...... & & &\\
				\hline
				\multirow{4}{*}{\rotatebox{90}{test}} & 1 & 07/08/2017 & Tesla makes its first Model 3 & Tesla Inc; Tesla Model 3 & 1 & Cars\\
				& 2 & 08/13/2017 & General Motors is ramping up its self-driving car: Ford should be nervous & General Motors; Ford Inc. & 1 & Cars\\
				& 3 & 06/21/2017 & Jeh Johnson testifies on Russian interference in 2016 election & Jeh Johnson; Russian & 1 & Politics\\
				& 4 & 07/16/2017 & ``Game of Thrones'' season 7 premiere: how you can watch & Game of Thrones & 0 & Other\\
				\hline
			\end{tabular}
			\label{table:case_study}
		\end{table*}
	
	\subsection{Results}
	\label{sec:cm}
		In this subsection, we present the results of comparison of different models and the comparison among variants of DKN.
	
		\subsubsection{Comparison of different models.}
			The results of comparison of different models are shown in Table \ref{table:comparison}.
			For each baseline in which the input contains entity embedding, we also remove the entity embedding from input to see how its performance changes (denoted by ``(-)'').
			Additionally, we list the improvements of baselines compared with DKN in brackets and calculate the \textit{p}-value of statistical significance by \textit{t}-test.
			Several observations stand out from Table \ref{table:comparison}:
			\begin{itemize}
				\item
					The usage of entity embedding could boost the performance of most baselines.
					For example, the AUC of KPCNN, DeepWide, and YouTubeNet increases by ,  and , respectively.
					However, the improvement for DeepFM is less obvious.
					We try different parameter settings for DeepFM and find that if the AUC drops to about , the improvement brought by the usage of knowledge could be up to .
					The results show that FM-based method cannot take advantage of entity embedding stably in news recommendation.
				\item
					DMF performs worst among all methods.
					This is because DMF is a CF-based method, but news is generally highly time-sensitive with a short life cycle.
					The result proves our aforementioned claim that CF methods cannot work well in the news recommendation scenario.
				\item
					Except for DMF, other deep-learning-based baselines outperform LibFM by  to  on F1 and by  to  on AUC, which suggests that deep models are effective in capturing the non-linear relations and dependencies in news data.
				\item
					The architecture of DeepWide and YouTubeNet is similar in the news recommendation scenario, thus we can observe comparable performance of the two methods.
					DSSM outperforms DeepWide and YouTubeNet, the reason for which might be that DSSM models raw texts directly with word hashing.
				\item
					KPCNN performs best in all baselines.
					This is because KPCNN uses CNN to process input texts and can better extract the specific local patterns in sentences.
				\item
					Finally, compared with KPCNN, DKN can still have a  AUC increase.
					We attribute the superiority of DKN to its two properties:
					1) DKN uses word-entity-aligned KCNN for sentence representation learning, which could better preserve the relatedness between words and entities;
					2) DKN uses an attention network to treat users' click history discriminatively, which better captures users' diverse reading interests.
			\end{itemize}
					
		Figure \ref{fig:result} presents the AUC score of DKN and baselines for additional ten test days.
		We can observe that the curve of DKN is consistently above baselines over ten days, which strongly proves the competitiveness of DKN.
		Moreover, the performance of DKN is also with low variance compared with baselines, which suggests that DKN is also robust and stable in practical application.
				
		\subsubsection{Comparison among DKN variants.}			
			Further, we compare among the variants of DKN with respect to the following four aspects to demonstrate the efficacy of the design of the DKN framework: the usage of knowledge, the choice of knowledge graph embedding method, the choice of transformation function, and the usage of an attention network.
			The results are shown in Table \ref{table:variants}, from which we can conclude that:
			\begin{itemize}
				\item
					The usage of entity embedding and contextual embedding can improve AUC by  and , respectively, and we can achieve even better performance by combining them together.
					This finding confirms the efficacy of using a knowledge graph in the DKN model.
				\item
					DKN+TransD outperforms other combinations.
					This is probably because, as presented in Section \ref{sec:kge}, TransD is the most complicated model among the four embedding methods, which is able to better capture non-linear relationships among the knowledge graph for news recommendation.
				\item
					DKN with mapping is better than DKN without mapping, and the non-linear function is superior to the linear one.
					The results prove that the transformation function can alleviate the heterogeneity between word and entity spaces by self learning, and the non-linear function can achieve better performance.
				\item
					The attention network brings a  gain on F1 and  gain on AUC for the DKN model.
					We will give a more intuitive demonstration on the attention network in the next subsection.
			\end{itemize}
	
		
	
	\subsection{Case Study}
	\label{sec:cs}        	
        	To intuitively demonstrate the efficacy of the usage of the knowledge graph as well as the the attention network, we randomly sample a user and extract all his logs from the training set and the test set (training logs with label 0 are omitted for simplicity).
        	As shown in Table \ref{table:case_study}, the clicked news clearly exhibits his points of interest: No. 1-3 concern cars and No. 4-6 concern politics (categories are not contained in the original dataset but manually tagged by us).
        	We use the whole training data to train DKN with full features and DKN without entity nor context embedding, then feed each possible pair of training logs and test logs of this user to the two trained models and obtain the output value of their attention networks.
        	The results are visualized in Figure \ref{fig:case_study}, in which the darker shade of blue indicates larger attention values.
        	From Figure \ref{fig:cs_a} we observe that, the first title in test logs gets high attention values with ``\textsf{Cars}'' in the training logs since they share the same word ``\textsf{Tesla}'', but the results for the second title are less satisfactory, since the second title shares no explicit word-similarity with any title in the training set, including No. 1-3.
        	The case is similar for the third title in test logs.
        	In contrast, in Figure \ref{fig:cs_b} we see that the attention network precisely captures the relatedness within the two categories ``\textsf{Cars}'' and ``\textsf{Politics}''.
        	This is because in the knowledge graph, ``\textsf{General Motors}'' and ``\textsf{Ford Inc.}'' share a large amount of context with ``\textsf{Tesla Inc.}'' and ``\textsf{Elon Musk}'', moreover, ``\textsf{Jeh Johnson}'' and ``\textsf{Russian}'' are also highly connected to ``\textsf{Donald Trump}''.
        	The difference in the response of the attention network also affects the final predicted results: DKN with knowledge graph (Figure \ref{fig:cs_b}) accurately predicts all the test logs, while DKN without knowledge graph (Figure \ref{fig:cs_a}) fails on the third one.
		
		
	
	\subsection{Parameter Sensitivity}
	\label{sec:ps}		
		DKN involves a number of hyper-parameters.
		In this subsection, we examine how different choices of hyper-parameters affect the performance of DKN.
		In the following experiments, expect for the parameter being tested, all other parameters are set as introduced in Section \ref{sec:es}.
		
		\subsubsection{Dimension of word embedding  and dimension of entity embedding .}
		We first investigate how the dimension of word embedding  and dimension of entity embedding  affect performance by testing all combinations of  and  in set .
		The results are shown in Figure \ref{fig:ps_a}, from which we can observe that, given dimension of entity embedding , performance initially improves with the increase of dimension of word embedding .
		This is because more bits in word embedding can encode more useful information of word semantics.
		However, the performance drops when  further increases, as a too large  (e.g., ) may introduce noises which mislead the subsequent prediction.
		The case is similar for  when  is given.
		
		\subsubsection{Window sizes of filters and the number of filters .}
		We further investigate the choice of windows sizes of filters and the number of filters for KCNN in the DKN model.
		As shown in Figure \ref{fig:ps_b}, given windows sizes, the AUC score generally increases as the number of filters  gets larger, since more filters are able to capture more local patterns in input sentences and enhance model capability.
		However, the trend changes when  is too large () due to probable overfitting.
		Likewise, we can observe similar rules for window sizes given : a small window size cannot capture long-distance patterns in sentences, while a too large window size may easily suffer from overfitting the noisy patterns.
		
		\begin{figure}[t]
			\centering
            \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[width=\textwidth]{figures/case_study_1.pdf}
                \caption{without knowledge graph}
                \label{fig:cs_a}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.22\textwidth}
                \includegraphics[width=\textwidth]{figures/case_study_2.pdf}
                \caption{with knowledge graph}
                \label{fig:cs_b}
            \end{subfigure}
            \caption{Attention visualization for training logs and test logs for a randomly sampled user.}
            \label{fig:case_study}
            \vspace{-0.05in}
        \end{figure}
        	
        	

\section{Related Work}
	\subsection{News Recommendation}
		News recommendation has previously been widely studied.
		Non-personalized news recommendation aims to model relatedness among news \cite{lv2011learning} or learn human editors' demonstration \cite{wang2017dynamic}.
		In personalized news recommendation, CF-based methods \cite{wang2011collaborative} often suffer from the cold-start problem since news items are substituted frequently.
		Therefore, a large amount of content-based or hybrid methods have been proposed \cite{kompan2010content, liu2010personalized, bansal2015content, phelan2009using, son2013location}.
		For example, \cite{phelan2009using} proposes a Bayesian method for predicting users' current news interests based on their click behavior, and \cite{son2013location} proposes an explicit localized sentiment analysis method for location-based news recommendation.
		Recently, researchers have also tried to combine other features into news recommendation, for example, contextual-bandit \cite{li2010contextual}, topic models \cite{luostarinen2013using}, and recurrent neural networks \cite{okura2017embedding}.
		The major difference between prior work and ours is that we use a knowledge graph to extract latent knowledge-level connections among news for better exploration in news recommendation.
		
		
	\subsection{Knowledge Graph}
		Knowledge graph representation aims to learn a low-dimensional vector for each entity and relation in the knowledge graph, while preserving the original graph structure.
		In addition to translation-based methods \cite{bordes2013translating, wang2014knowledge, lin2015learning, ji2015knowledge} used in DKN, researchers have also proposed many other models such as Structured Embedding \cite{bordes2011learning}, Latent Factor Model \cite{jenatton2012latent}, Neural Tensor Network \cite{socher2013reasoning} and GraphGAN \cite{wang2017graphgan}.
		Recently, the knowledge graph has also been used in many applications, such as movie recommendation\cite{zhang2016collaborative}, top-N recommendation \cite{palumbo2017entity2rec}, machine reading\cite{yang2017leveraging}, text classification\cite{wang2017combining} word embedding\cite{xu2014rc}, and question answering \cite{dong2015question}.
		To the best of our knowledge, this paper is the first work that proposes leveraging knowledge graph embedding in news recommendation.
		
		\begin{figure}[t]
			\centering
            \begin{subfigure}[b]{0.232\textwidth}
                \includegraphics[width=\textwidth]{charts/ps_1.eps}
                \caption{AUC score w.r.t dimension of entity embedding  and dimension of word embedding }
                \label{fig:ps_a}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.232\textwidth}
                \includegraphics[width=\textwidth]{charts/ps_2.eps}
                \caption{AUC score w.r.t window sizes of filters and the number of filters }
                \label{fig:ps_b}
            \end{subfigure}
            \caption{Parameter sensitivity of DKN.}
            \label{fig:ps}
            \vspace{-0.05in}
        \end{figure}	
	
	\subsection{Deep Recommender Systems}
		Recently, deep learning has been revolutionizing recommender systems and achieves better performance in many recommendation scenarios.
		Roughly speaking, deep recommender systems can be classified into two categories: using deep neural networks to process the raw features of users or items, or using deep neural networks to model the interaction among users and items.
		In addition to the aforementioned DSSM \cite{huang2013learning}, DeepWide \cite{cheng2016wide}, DeepFM \cite{guo2017deepfm}, YouTubeNet \cite{covington2016deep} and DMF \cite{xue2017deep}, other popular deep-learning-based recommender systems include Collaborative Deep Learning \cite{wang2015collaborative}, SHINE \cite{wang2017shine}, Multi-view Deep Learning \cite{elkahky2015multi}, and Neural Collaborative Filtering \cite{he2017neural}.
		The major difference between these methods and ours is that DKN specializes in news recommendation and could achieve better performance than other generic deep recommender systems.



\section{Conclusions}
	In this paper, we propose DKN, a deep knowledge-aware network that takes advantage of knowledge graph representation in news recommendation.
	DKN addresses three major challenges in news recommendation:
	1) Different from ID-based methods such as collaborative filtering, DKN is a content-based deep model for click-through rate prediction that are suitable for highly time-sensitive news.
	2) To make use of knowledge entities and common sense in news content, we design a KCNN module in DKN to jointly learn from semantic-level and knowledge-level representations of news.
	The multiple channels and alignment of words and entities enable KCNN to combine information from heterogeneous sources and maintain the correspondence of different embeddings for each word.
	3) To model the different impacts of a user's diverse historical interests on current candidate news, DKN uses an attention module to dynamically calculate a user's aggregated historical representation.
	We conduct extensive experiments on a dataset from Bing News.
	The results demonstrate the significant superiority of DKN compared with strong baselines, as well as the efficacy of the usage of knowledge entity embedding and the attention module.




\bibliographystyle{ACM-Reference-Format}
\bibliography{sigproc} 

\end{document}
