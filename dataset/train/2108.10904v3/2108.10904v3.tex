
\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{todonotes}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{url}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{pifont}
\usepackage{fancyvrb}
\usepackage{todonotes}
\usepackage{times}
\usepackage{latexsym}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{bbm}
\usepackage{color}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath,amsfonts,amsthm,bm}
\usepackage{enumitem}
\usepackage{url}
\usepackage{xcolor}
\usepackage{sidecap}
\usepackage{booktabs,siunitx}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage{algorithm}

\newcommand{\todob}[1]{\todo[inline,caption={},backgroundcolor=yellow]{#1}}

\newcommand{\ours}{SimVLM}
\newcommand{\reb}[1]{\textcolor{blue}{{#1}}}

\title{{\ours}: Simple Visual Language Model Pretraining with Weak Supervision}




\author{Zirui Wang\thanks{This work was conducted at Google.}, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao \\
Carnegie Mellon University\\ 
\texttt{\{ziruiw\}@cs.cmu.edu}\\
Google Research, Brain Team\\
\texttt{\{jiahuiyu,adamsyuwei,zihangd,yuancao\}@google.com} \\ 
University of Washington\\
\texttt{\{yuliats\}@cs.washington.edu}\\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
With recent progress in joint modeling of visual and textual representations,
Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives.
In this work, we relax these constraints and present a minimalist pretraining framework, named \textbf{Sim}ple \textbf{V}isual \textbf{L}anguage \textbf{M}odel ({\bf\ours}).
Unlike prior work, {\ours} reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74\% vqa-score), NLVR2 (+1.17\% accuracy), SNLI-VE (+1.37\% accuracy) and image captioning tasks (+10.1\% average CIDEr score). Furthermore,
we demonstrate that {\ours} acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.
\end{abstract}

\section{Introduction}\label{sec:intro}

Self-supervised textual representation learning \citep{devlin2018bert,radford2018improving,radford2019language,liu2019roberta,yang2019xlnet,raffel2019exploring,brown2020language}
based on Transformers \citep{vaswani2017attention}
has pushed the state of the art on a wide range of natural language processing (NLP) tasks \citep{rajpurkar2016squad,wang2018glue,sarlin2020superglue}.
One successful approach is to first pretrain the model (e.g. BERT) on large-scale unlabled text corpora using masked language modeling (MLM) objective  \citep{devlin2018bert},
followed by finetuning on downstream tasks.
While this pretraining-finetuning paradigm has been widely adopted,
recent work on autoregressive language models (LM) \citep{radford2019language,brown2020language} such as GPT-3  
has shown strong performance without finetuning by utilizing few-shot prompts \citep{liu2021pretrain},
suggesting the text guided zero-shot generalization is a promising alternative.


Motivated by the success of textual representation pretraining,
various efforts have been made to build the multi-modal (visual and textual) counterpart.
A line of work \citep{tan-bansal-2019-lxmert,jiasen2019vilbert,li2019visualbert,chen2020uniter,li2020oscar,Su2020VL-BERT,Zhang_2021_CVPR}
has explored vision-language pretraining (VLP) that learns a joint representation of both modalities to be finetuned on vision-language (VL) benchmarks,
such as visual question answering (VQA) \citep{goyal2017making}.
In order to capture the alignment between images and text,
previous methods have extensively exploited two types of human-labeled datasets from multiple sources, which typically consist of the following steps.
Firstly, object detection datasets are used to train a supervised object detector (OD) which allows further extracting region-of-interest (ROI) features from images.
Next, datasets with aligned image-text pairs are used for MLM pretraining of a fusion model that usually takes as input the concatenation of the extracted ROI features and the paired text. In addition, due to the limited scale of human annotated data,
various task-specific auxiliary losses have been introduced in order to improve performance. These design choices complicate the pretraining protocol of VLP,
creating a bottleneck for further quality improvement.
What is more, such pretraining-finetuning based approaches usually lack the zero-shot capability, just like their language counterparts.
In comparison, another line of work \citep{radford2021learning,ramesh2021zero,jia2021scaling}
utilizes weakly labeled/aligned data crawled from the web to perform pretraining,
achieving good performance and certain zero-shot learning capability on image classification and image-text retrieval.
Nonetheless, these methods mainly focus on specific tasks of consideration and thus may not serve as a generic pretraining-finetuning representation for VL benchmarks.


In light of these disadvantages of the existing techniques,
we are interested in building a VLP model that:
(1) can be seamlessly plugged into the pretraining-finetuning paradigm and achieve competitive performance on standard VL benchmarks;
(2) does not require a complicated pretraining protocol as in previous methods;
and (3) has the potential towards text guided zero-shot generalization in cross-modal settings.
To this end,
we propose \textbf{{\ours}}, standing for \textbf{Sim}ple \textbf{V}isual \textbf{L}anguage \textbf{M}odel, which significantly simplifies VLP by \textit{solely} exploiting language modeling objectives on weakly aligned image-text pairs~\citep{jia2021scaling}. In a nutshell, {\ours} consists of the following components:
\begin{itemize}
    \item \textbf{Objective}. It is trained end-to-end from scratch with a single objective of Prefix Language Modeling (PrefixLM), which can not only naturally perform text generation as GPT-3, but also process contextual information in a bidirectional manner as BERT does.
    \item \textbf{Architecture}. The framework employs ViT/CoAtNet \citep{dosovitskiy2021an,dai2021coatnet} and directly takes raw images as inputs. These models can also fit the large-scale data and are readily compatible with the PrefixLM objective.    
    \item \textbf{Data}. These setups relieve the requirement for object detection and allow the 
    model to utilize the large-scale weakly labeled dataset, which has better potential towards zero-shot generalization.
\end{itemize}

Not only is {\ours} simpler, requiring neither object detection pretraining nor auxiliary losses,
but it also obtains better performance than previous work.
Empirically,
{\ours} consistently outperforms existing VLP models and achieves new state-of-the-art results on 6 VL benchmarks without additional data nor task-specific customization.
Besides,
it acquires stronger generalization in visual-language understanding that empowers
zero-shot image captioning and open-ended VQA.
In particular, {\ours} learns unified multimodal representation that enables zero-shot cross-modality transfer,
where the model is finetuned on text-only data and directly evaluated on image-and-text test examples without further training.
Our results suggest that generative VLP can not only match existing MLM-based methods on VL tasks but also demonstrate promising zero-shot potential.






\section{Related Work}

Recent years have seen a rapid progress made in vision-language pretraining \citep{uppal2020multimodal,han2021survey,khan2021transformers}. 
While a variety of approaches have been proposed, 
a large portion of them require object detection for image region feature regression or tagging as part of the pre-training objectives \citep{tan-bansal-2019-lxmert,Su2020VL-BERT,li2019visualbert,chen2020uniter,gan2020large,li2020oscar,yu2021ernie,li-etal-2021-unimo,Zhang_2021_CVPR,hu2021vivo,cho2021unifying}.
These methods rely on a strong object detection model like Fast(er) R-CNN \citep{NIPS2015_14bfa6bb}, 
which is often trained on human annotated data sets like Visual Genome \citep{krishnavisualgenome}. 
Using such labeled training data as a prerequisite increases the cost of building the training pipeline, and makes the approach less scalable.
Some recent efforts have also explored VLP without object detection module \citep{xu-etal-2021-e2e, kim2021vilt, huang2021seeing},
but they only use clean pretraining data with small scales and thus their zero-shot capability is limited.



On the other hand,
multiple cross-modality loss functions have been proposed as part of the training objectives, for example
image-text matching \citep{tan-bansal-2019-lxmert,jiasen2019vilbert,xu-etal-2021-e2e},
masked region classification/feature regression \citep{tan-bansal-2019-lxmert,chen2020uniter},
object attribute prediction \citep{xu-etal-2021-e2e}, 
contrastive loss \citep{li2020oscar,li-etal-2021-unimo},
word-region alignment \citep{chen2020uniter}
word-patch alignment \citep{kim2021vilt}.
They are often mixed with other objectives including image caption generation and masked language modeling to form compound pre-training losses.
This creates the challenge of balancing among different losses and datasets,
and thus complicates the optimization procedure.

Our work by contrast, 
follows a minimalist approach that takes raw image inputs and makes use of only the language modeling loss,
without resorting to auxiliary models like faster R-CNN for image region detection.
Motivated by recent works \citep{radford2021learning,ramesh2021zero,jia2021scaling,tsimpoukelli2021multimodal} that illustrate zero-shot learning in certain image-text tasks,
we train our model using large-scale weakly labeled data only.
While concurrent work \citep{shen2021much} has explored building on top of models pretrained with such dataset,
we focus on pretraining from scratch to explore the limit of generative VLP.



\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.75\textwidth]{model.png}
\end{center}
\caption{Illustration of the SimVLM model. This shows an example of training with PrefixLM of an image-text pair. For text-only corpora, it is straightforward to remove the image patches and utilize textual tokens only.}
\label{fig:model}
\end{figure}

\section{{\ours}} 
\label{sec:approach}

\subsection{Background}

The bidirectional \textbf{Masked Language Modeling (MLM)} has been one of the most popular self-supervised training objectives for textual representation learning.
As demonstrated by BERT \citep{devlin2018bert}, it is based on the idea of denoising autoencoder such that the model is trained to recover the corrupted tokens in a document.
Specifically, given a text sequence , a subset of tokens  are randomly sampled and a corrupted sequence  is constructed by replacing tokens in  with a special \verb+[MASK]+ token.
The training objective is to reconstruct  from the context  by minimizing the negative log-likelihood:

where  is the trainable parameters of the model and  is the pretraining data.
This approach learns contextualized representations that can be further finetuned for downstream tasks.
The MLM-style pretraining has been widely adopted in previous VLP models, 
whereby the input is an image-text pair and the model needs to predict masked tokens by leveraging image ROI features.

Alternatively,
the unidirectional \textbf{Language Modeling (LM)} trains the model to directly maximize the likelihood of the sequence  under the forward autoregressive factorization:

Compared with MLM,
the LM pretraining has also been shown to be highly effective for multiple NLP tasks \citep{radford2018improving}.
More importantly, it facilitates the model with strong generation capability that enables text induced zero-shot generalization without finetuning \citep{brown2020language}.
While MLM has become the de facto approach in VLP models reviewed above, the generative LM has been understudied.


\subsection{Proposed Objective: Prefix Language Modeling}

Motivated by the zero-shot capability introduced by pre-training with LM loss,
we propose to pretain vision-language representation using 
the \textbf{Prefix Language Modeling (PrefixLM)}.
PrefixLM differs from the standard LM such that
it enables bi-directional attention on the prefix sequence (e.g.  in Eq. (\ref{eq:prefix})),
and only conducts autoregressive factorization on the remaining tokens (e.g.  in Eq. (\ref{eq:prefix})).
During pretraining,
a prefix sequence of tokens of (a randomly selected) length  is truncated from input sequence
and the training objective becomes:

Intuitively, images can be considered as prefix for their textual descriptions as they often appear before text in a web document.
Therefore, for a given image-text pair,
we prepend image feature sequence of length  to the text sequence, and enforce the model to sample a prefix of length  to calculate LM loss on text data only (an example is shown in Figure \ref{fig:model}).
Compared to prior MLM style VLP methods,
our PrefixLM model under the sequence-to-sequence framework not only enjoys the bidirectional contextualized representation as in MLM,
but also can perform text generation similar to LM.



\subsection{Architecture}

We adopt Transformer as the backbone of our model due to its success for both language and vision tasks \citep{devlin2018bert,dosovitskiy2021an}.
Differently from standard LM, PrefixLM enables bidirectional attention within the prefix sequence,
and thus it is applicable for both decoder-only and encoder-decoder sequence-to-sequence language models.
In our preliminary experiments,
we found that the inductive bias introduced by encoder-decoder model which decouples encoding from generation is conducive to the improvement of downstream task.

An overview of our model architecture is depicted in Figure \ref{fig:model}.
For the visual modality, inspired by ViT \citep{dosovitskiy2021an} and CoAtNet \citep{dai2021coatnet},
our model receives the raw image  
and maps it into flattened 1D sequence of patches  as input for the transformer,
where  is the fixed hidden size of the transformer layers and 
is the length of the image tokens for a given patch size .
Following \cite{dai2021coatnet},
we use a convolution (Conv) stage consist of the first three blocks of ResNet \citep{he2016deep} to extract contextualized patches,
which we find advantageous over the naive linear projection (equivalent to 11 Conv layer) used in ViT,
consistent with the observation from \citep{xiao2021early}.
For the textual modality, we follow the standard practice to tokenize the input sentence into sub-word tokens \citep{kudo2018sentencepiece},
and the embeddings are learned for a fixed vocabulary.
To retain positional information,
we add two trainable 1D positional embeddings for image and text inputs separately,
and we additionally add 2D relative attention for the image patches within transformer layers \citep{dai2021coatnet}.
Notice that we do not add extra modality type embeddings for which we found no improvement in our experiment.
We study the effects of various components of the model in Section~\ref{sec:ablation}.


\subsection{Datasets}

Since our approach does not rely on an object detection module and only operates with raw image patch inputs,
we pretrain all model parameters from scratch using large-scale noisy image-text data,
which has better potential for zero-shot generalization.
Specifically,
we use the image and alt-text pairs introduced in \cite{jia2021scaling},
which are crawled from the web with minimal post-processing.
On the other hand,
our formulation of PrefixLM
is modality-agnostic and thus
we can additionally include text-only corpora to compensate for noisy text supervision in the alt-text data.
As shown later in our experiments,
this unified PrefixLM formulation reduces the modality discrepancy and improves the model quality.

Compared to prior VLP methods consisting of two pretraining stages and multiple auxiliary objectives,
our model only requires one-pass pretraining using a single language modeling loss in an end-to-end manner, 
hence the name Simple Visual Language Model (SimVLM).



\section{Experiments}

We conduct systematic experiments on a diversified set of visual-linguistic benchmarks,
including visual question answering, 
image captioning, 
visual reasoning,
visual entailment,
and multimodal translation.
We not only examine our model as a general-purpose VL representation learning in the pretraining-finetuning paradigm,
but also study its zero-shot generalization towards open-ended VL understanding.

\subsection{Setup}

Our models are implemented with the Lingvo framework \citep{shen2019lingvo}. 
We follow the setup in ViT \citep{dosovitskiy2021an} to explore 3 variants of \ours,
namely ``Base'', ``Large'', and ``Huge'',
such that each variant follows the same setting as its corresponding ViT variant.
All models are pretrained from scratch for about 1M steps on the training set of ALIGN \citep{jia2021scaling} and the Colossal Clean Crawled Corpus (C4) dataset presented in \citet{raffel2019exploring}.
We mix the two pretraining datasets within each batch, which contains 4,096 image-text pairs (ALIGN) and 512 text-only documents (C4), sharded across 512 TPU v3 chips \citep{jouppi2017indatacenter}.
More pretraining settings are detailed in Appendix \ref{sec:pretrain}.

After pretrained, our model is finetuned and evaluated on six vision-language benchmarks,
including three discriminative tasks: VQA v2 \citep{goyal2017making}, SNLI-VE \citep{xie2019visual}, and NLVR2 \citep{suhr2018corpus}; as well as three generative tasks: CoCo captioning \citep{chen2015microsoft}, NoCaps \citep{agrawal2019nocaps}, and Multi30k \citep{elliott2016multi30k}.
We additionally examine its zero-shot generalization and performance on single-modality tasks.
Details of tasks considered and the finetuning process are outlined in Appendix \ref{sec:finetuning}.

\subsection{Comparison with existing approaches}
To examine the quality of vision-language pretraining, we first compare {\ours} on the popular multi-modal tasks with state-of-the-art (SOTA) VLP methods including
LXMERT \citep{tan-bansal-2019-lxmert}, VL-T5 \citep{cho2021unifying},
UNITER \citep{chen2020uniter}, OSCAR \citep{li2020oscar}, Villa \citep{gan2020large}, SOHO \citep{huang2021seeing}, UNIMO \citep{li-etal-2021-unimo}, and VinVL \citep{Zhang_2021_CVPR}.

\begin{table*}[t!]
\begin{center}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{l | c c | c c | c c | c c c c | c c | c }
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c|}{VQA} & \multicolumn{2}{c|}{NLVR2} & \multicolumn{2}{c|}{SNLI-VE} & \multicolumn{4}{c|}{CoCo Caption} & \multicolumn{2}{c|}{NoCaps} & Multi30k \\
    & test-dev & test-std & dev & test-P & dev & test & B@4 & M & C & S & C & S & En-De \\                         
\bottomrule
\multicolumn{14}{c}{Base-sized Models}\\
\toprule
LXMERT & 72.42 & 72.54 & 74.90 & 74.50 & - & - & - & - & - & - & - & - & - \\
VL-T5 & - & 70.30 & 74.6 & 73.6 & - & - & - & - & 116.5 & - & - & - & 45.5 \\
SOHO & 73.25 & 73.47 & 76.37 & 77.32 & 85.00 & 84.95 & - & - & - & - & - & - & - \\
 & 77.87 & 78.14 & 81.72 & 81.77 & 84.20 & 84.15 & 39.0 & 32.9 & 134.8 & 24.0 & 94.8 & 13.1 & 46.6 \\
\bottomrule
\multicolumn{14}{c}{Large-sized Models}\\
\toprule
UNITER & 73.82 & 74.02 & 79.12 & 79.98 & 79.39 & 79.38 & - & - & - & - & - & - & - \\
OSCAR & 73.61 & 73.82 & 79.12 & 80.37 & - & - &\bf  41.7 & 30.6 & 140.0 & 24.5 & 80.9 & 11.3 & - \\
Villa & 74.69 & 74.87 & 79.76 & 81.47 & 80.18 & 80.02 & - & - & - & - & - & - & - \\
UNIMO & 75.06 & 75.27 & - & - & 81.11 & 80.63 & 39.6 & - & 127.7 & - & - & - & - \\
VinVL & 76.56 & 76.60 & 82.67 & 83.98 & - & - & 41.0 & 31.1 & 140.9 & 25.2 & 92.5 & 13.1 & - \\
 & 79.32 & 79.56 & 84.13 & 84.84 & 85.68 & 85.62  & 40.3 & 33.4 & 142.6 & 24.7 & 108.5 & 14.2 & 47.5 \\
\bottomrule
\multicolumn{14}{c}{Huge-sized Models}\\
\toprule
 & \bf 80.03 & \bf 80.34 & \bf 84.53 & \bf 85.15 & \bf 86.21 & \bf 86.32 & 40.6 & \bf 33.7 & \bf 143.3 & \bf 25.4 & \bf 110.3 & \bf 14.5 & \bf 47.6 \\
\bottomrule
\end{tabular}
}
\end{center}
\caption[caption]{Single model results for vision-language pretraining methods on popular VL banchmarks. We report vqa-score for VQA, accuracy for NLVR2 and SNLI-VE, BLEU@4 for Multi30k and various metrics for image captioning (B@4: BLEU@4, M: METEOR, C: CIDEr, S: SPICE).}
\label{tab:main}
\vskip -0.1in
\end{table*}

As can be seen in Table \ref{tab:main}, {\ours} outperforms all existing models and achieves new SOTA results on all tasks considered, often by a significant margin. 
This demonstrates our generative pretraining approach is competitive with MLM-based models and that simple framework with weak supervision is sufficient to learn high-quality multi-modal representations.

For the discriminative tasks,
the  already outperforms all prior methods while using less capacity, and the  obtains almost 4 points absolute score improvement compared to the previous SOTA (VinVL), pushing the single model performance above 80\% on VQA for the first time.
In addition, {\ours} also consistently outperforms prior methods on NLVR2 and SNLI-VE,
illustrating its capability of processing more complex visual-linguistic reasoning.
For the generation tasks including image captioning and image translation, {\ours} also shows large improvements using naive finetuning techniques.
Our model outperforms on 3 out of 4 metrics on the public ``Karpathy'' 5k test split of CoCo captioning as well as the NoCaps benchmark than prior methods trained with more complex reinforcement learning approach of CIDEr optimization \citep{rennie2017self}.
Finally, {\ours} is also effective for image translation of Multi30k from English to German. These experiments demonstrate that our model can be seamlessly plugged into the pretraining-finetuning paradigm with superior performance, utilizing minimalist pretraining and finetuning procedures.

\subsection{Zero-Shot Generalization}

A crucial benefit of generative modeling and scaling with weak supervision is the potential of zero-shot generalization.
Models \citep{brown2020language,radford2021learning,jia2021scaling} have been shown capable of performing few-shot or zero-shot transfer from pretrained models to downstream datasets,
even across language boundaries \citep{lample2019cross}.
In this section,
we showcase three different settings of zero-shot applications less explored in prior VLP work,
including transferring to unseen tasks, modalities and/or testing instances.


\subsubsection{Zero-shot/Few-shot Image Captioning}

The pretraining procedure of {\ours} can be interpreted as a noisy image captioning objective on real-world web corpus. Thus, it is natural to ask how well this caption ability generalizes to other datasets in a zero-shot/few-shot manner.
To this end, we take the pretrained {\ours} model, and directly decode on image captioning benchmarks for the zero-shot setting while finetune on 1\% training data for 5 epochs for the few-shot setting.
We also found that using a prefix prompt ``\verb+A picture of+'' improves the quality of decoded captions,
similar to the finding in \cite{radford2021learning}.


\begin{table*}[t!]
\begin{center}
\small
\resizebox{0.75\textwidth}{!}{\begin{tabular}{l | c | c c c c | c c c c}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{Setup} & \multicolumn{4}{c|}{CoCo Caption} & \multicolumn{4}{c}{NoCaps} \\
 & & B@4 & M & C & S & In & Near & Out & Overall \\
\bottomrule
\toprule
BUTD & \multirow{3}{*}{supervised} & 36.3 & 27.7 & 120.1 & 21.4 & - & - & - & - \\
AoANet & & 39.5 & 29.3 & 129.3 & 23.2 & - & - & - & - \\
M2 Transformer & & 39.1 & 29.2 & 131.2 & 22.6 & 81.2 & - & 69.4 & 75.0 \\
\bottomrule
\toprule 
 & \multirow{3}{*}{zero-shot} & 9.5 & 11.5 & 24.0 & 7.5 & 83.2 & 84.1 & 82.5 & 83.5 \\
 & & 10.5 & 12.0 & 24.9 & 8.3 & 97.6 & 96.5 & 96.3 & 96.6 \\
 & & 11.2 & 14.7 & 32.2 & 8.5 & 101.2 & 100.4 & 102.3 & 101.4 \\
\bottomrule
\toprule 
 & \multirow{3}{*}{few-shot} & 34.7 & 29.2 & 118.7 & 21.9 & 95.0 & 91.9 & 98.5 & 93.7 \\
 & & 35.4 & 30.2 & 124.1 & 22.7 & 102.5 & 100.9 & 106.0 & 102.2 \\
 & & 36.8 & 31.5 & 131.3 & 24.0 & 111.8 & 110.6 & 111.0 & 110.4 \\
\bottomrule
\toprule 
OSCAR & \multirow{3}{*}{pretrain-finetune} & \bf 41.7 & 30.6 & 140.0 & 24.5 & 85.4 & 84.0 & 80.3 & 83.4 \\
VinVL & & 41.0 & 31.1 & 140.9 & 25.2 & 103.7 & 95.6 & 83.8 & 94.3 \\
 & & 40.6 & \bf 33.7 & \bf 143.3 & \bf 25.4 & \bf 113.7 & \bf 110.9 & \bf 115.2 & \bf 112.2 \\
\bottomrule
\end{tabular}
}
\end{center}
\caption[caption]{Image captioning results on CoCo Karpathy-test split and NoCaps validation split. For NoCaps, \{In, Near, Out\} refer to in-domain, near-domain and out-of-domain respectively.  indicates Cider optimization. Model references: \citet{anderson2018bottom} \citet{huang2019attention} \citet{cornia2020meshed}.} 
\label{tab:caption}
\vskip -0.1in
\end{table*}

As shown in Table \ref{tab:caption},
the zero-shot/few-shot performance (Appendix \ref{appexdix:erratum}) of {\ours} is competitive with fully supervised baselines on CoCo,
and it also demonstrates strong generalization on the concept-rich NoCaps benchmark by achieving better scores than pretrained models.
Figure \ref{fig:example} (a) illustrates sample captions generated by our model (Appendix \ref{appendix:examples}). {\ours} is able to not only capture real-world concepts but also provide a detailed description of the visual input.
For example, the decoded samples are able to explain complex scenes with multiple objects (e.g. ``people'', ``table with drinks'', ``dark restaurant''). Besides, the model also shows understanding of fine-grained abstraction such as specific car brand and model (e.g. ``Aston Martin'', ``Vantage''). {\ours} even performs robustly on challenging images that could be tricky for human, such as abstract or dark pictures.
These all illustrate that our model learns a wide range of real-world concepts that generalize well in a zero-shot manner.

\subsubsection{Zero-shot cross-modality Transfer}

Existing pretraining methods have been shown to be successful in transferring knowledge across heterogeneous data spaces. For example, multilingual language models \citep{devlin2018bert,lample2019cross} enable zero-shot cross-lingual transfer such that the model is only finetuned using training data from a source language (typically English) and evaluated on the target language without further training. Inspired by this setup, we explore a novel zero-shot cross-modality transfer paradigm of utilizing VLP models, and evaluate how well our model generalizes across modalities. Since text training data are usually cheaper to obtain compared to visual data, we finetune {\ours} on text-only downstream data and then directly evaluate the zero-shot transfer on joint VL tasks.

Specifically, We utilize SNLI-VE and Multi30k to examine the zero-shot transfer performance. For SNLI-VE, we finetune on three text-only NLI datasets such that the premise sentence is used as the encoder's input while the hypothesis is fed to the decoder, and a similar classifier head is trained on the embedding of the last token in the decoder. At inference, the finetuned model is evaluated by taking the premise image as the encoder input and the corresponding hypothesis sentence to the decoder.
As shown in Table \ref{tab:zero-shot-transfer}, {\ours} performs competitively with fully supervised baselines including UNITER under the zero-shot setting.
As a sanity check, we also mask out the image feature to predict using the hypothesis only,
and find our models can only obtain results close to random guess (average scores of 34.31 / 34.62). 
This results in performance close to random guess hence demonstrating the effectiveness of {\ours}'s cross-modality transfer ability.

In addition, {\ours} is also capable of domain adaption by transferring from the MNLI dataset to SNLI-VE, whereby data comes not only from a different modality but also another domain. We also find it possible to transfer across different languages and modalities using {\ours}.
Specifically, we utilize the German image captioning task from WMT 2016 of Multi30k for evaluation, where our model is finetuned on English-German text-only translation data followed by decoding with image-only input in the encoder. Table \ref{tab:zero-shot-transfer} shows that {\ours} is capable of transferring knowledge across modalities and languages in generative tasks,
achieving comparable performance to supervised baselines (decoded examples shown in Figure \ref{fig:example} (b)). These results suggest zero-shot cross-modality transfer emerges with the scaling of weakly labeled data.

\begin{table*}[t!]
\begin{center}
\resizebox{0.7\textwidth}{!}{\small
\begin{tabular}{l | c c c | c c}
\toprule
\multirow{2}{*}{} & \multicolumn{3}{c|}{SNLI-VE} & \multicolumn{2}{c}{Multi30k} \\
 & SNLI-VE (T) & SNLI & MNLI  & \multicolumn{2}{c}{Multi30k (T)} \\
 & & / & & B@4 & M \\
\bottomrule
\multicolumn{6}{c}{Fully Supervised Baseline}\\
\toprule
EVE-Image & \multicolumn{3}{c|}{71.56 / 71.16} & - & -\\
UNITER & \multicolumn{3}{c|}{78.59 / 78.28}  & - & -\\
SOHO & \multicolumn{3}{c|}{85.00 / 84.95}  & - & - \\
LIUM & \multicolumn{3}{c|}{-} & 23.8 & 35.1 \\
GroundedTrans & \multicolumn{3}{c|}{-} & 15.8 & 31.2 \\
\bottomrule
\multicolumn{6}{c}{Zero-Shot Cross-Modality Transfer}\\
\toprule
 & 71.35 / 71.02 & 72.65 / 72.24 & 64.37 / 63.98 & 15.0 & 24.8 \\
 & 72.85 / 72.44 & 73.62 / 73.23 & 66.97 / 66.31 & 17.7 & 30.1 \\
 & 73.56 / 73.08 & 74.24 / 73.86 & 67.45 / 66.97 & 18.2 & 32.6 \\
\bottomrule
\end{tabular}
}
\end{center}
\caption[caption]{Zero-shot cross-modality transfer results on SNLI-VE and Multi30k. For SNLI-VE, the zero-shot model is finetuned on three source datasets: text-only SNLI-VE \citep{xie2019visual}, SNLI \citep{bowman2015large}, and MNLI \citep{williams2017broad}. For Multi30k, the model is finetuned on text-only Multi30k data. Model reference: \citep{specia2016shared}.} 
\label{tab:zero-shot-transfer}
\vskip -0.1in
\end{table*}


\begin{table*}[t!]
\begin{center}
\begin{small}
\resizebox{0.7\textwidth}{!}{\begin{tabular}{l | c | c c c | c c c}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{Dev} & \multicolumn{3}{c|}{Karpathy-test} & \multicolumn{3}{c}{Partial Train} \\
 & & In-domain & Out-domain & Overall & In-domain & Out-domain & Overall \\
\bottomrule
\multicolumn{8}{c}{Discriminative}\\
\toprule
UNITER & - & 74.4 & 10.0 & 70.5 & - & - & - \\
VL-T5 & - & 70.2 & 7.1 & 66.4 & - & - & - \\
VL-BART & - & 69.4 & 7.0 & 65.7 & - & - & - \\
 & 73.8 & 79.0 & 16.7 & 75.3 & 78.4 & 10.3 & 70.5 \\
 & 76.0 & 80.4 & 17.3 & 76.7 & 79.5 & 11.0 & 71.8\\
 & \bf 76.5 & \bf 81.0 & 17.5 & \bf 77.2 & \bf 80.2 & 11.1 & 72.2 \\
\bottomrule
\multicolumn{8}{c}{Generative}\\
\toprule
VL-T5 & - & 71.4 & 13.1 & 67.9 & - & - & - \\
VL-BART & - & 72.1 & 13.2 & 68.6 & - & - & - \\
 & 73.2 & 78.3 & 25.8 & 75.2 & 77.1 & 27.1 & 71.3 \\
 & 75.2 & 79.5 & 29.6 & 76.5 & 78.7 & 28.4 & 72.5 \\
 & 75.5 & 79.9 & \bf 30.3 & 77.0 & 79.1 & \bf 28.8 & \bf 73.0 \\
\bottomrule
\end{tabular}
}
\end{small}
\end{center}
\caption[caption]{Comparison of discriminative and generative VQA methods. ``Dev'' refers to standard vqa-score on the VQA validation split. ``Karpathy-test'' is the setup used in \cite{cho2021unifying} for evaluation on the Karpathy split with rare answers. ``Partial Train'' refers to train the model only on partial training data which contain subset of all candidate answers.} 
\label{tab:open-vqa}
\vskip -0.1in
\end{table*}



\subsubsection{Open-ended VQA}

On the VQA benchmark, the best performing models to date formulate the problem as a discriminative task of multi-label classification over a predefined 3,129 answer candidates, often consisting of short factual terms. In real-world applications, however, it is hard to define a closed set of candidate answers that covering all possible scenarios, making the true open-ended VQA a challenging setup.
Generative models such as {\ours} provide an alternative solution towards this challenge by generating free-form textual answers without being constrained to predefined answers. To this end, we finetune {\ours} using the PrefixLM loss described above where we treat the concatenation of the image and the question as the prefix, and train the model to generate answers.

\begin{wraptable}{r}{0.5\textwidth}
\begin{center}
\begin{small}
\begin{tabular}{l | c }
\toprule
Method & Acc@1 \\
\bottomrule
\toprule
SimCLRv2 \citep{chen2020big} & 79.8\\
DINO \citep{caron2021emerging} & 80.1 \\
CLIP \citep{radford2021learning} & 85.4 \\
ALIGN \citep{jia2021scaling} & \bf 85.5 \\
 & 80.6\\
 & 82.3\\
 & 83.6\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\caption[caption]{Linear evaluation on ImageNet classification, compared to state-of-the-art representation learning methods.} 
\label{tab:imagenet}
\end{wraptable}

We then compare the generative approach with classification methods in Table \ref{tab:open-vqa}.
Firstly, we follow \citet{cho2021unifying} and evaluate model performance on questions with rare answers in the Karpathy-test split.
Here, out-of-domain questions are defined as those with best-scoring answer not included in the 3,129 candidates. Results show that {\ours} outperforms both discriminative and generative baselines on all splits.
More importantly, the generative {\ours} significantly improves on the out-of-domain split by over 17 points, demonstrating its strong generalization. However, this setup mainly focuses on rare answers and it remains unclear how well the model generalizes to common unseen answers.
We therefore proceed to investigate a more challenging setup where we randomly select 2,085 (about two-thirds of 3,129) in-domain answers and partition both train and validation sets into two splits based on whether their best-scoring answers are included in the selected set or not. We then only finetune {\ours} on the in-domain split of the train set and evaluate on the entire validation set. The ``Partial Train'' column in Table \ref{tab:open-vqa} shows that the generative {\ours} is also competent in this setup by scoring reasonably well on over 1,000 unseen answers. 
Overall, we found the generative {\ours} performs competitively with its discriminative counterpart in the standard setup, and works generally better in the out-of-domain case.

Note that we use the exact matching between generated answers and human labels for score calculation in the above experiment, however it is possible that the model generates appropriate answers in different formats or synonyms.
Therefore, in addition to the quantitative study above, 
we show qualitative generation results in Figure \ref{fig:example} (c).
It can be observed that {\ours} is able to generate answers not included in the 3,129 candidate set (e.g. ``surgeon'' and ``wood carving''), demonstrating that {\ours} can transfer knowledge from the pretraining corpus to VQA.
It is thus natural to ask whether {\ours} can perform zero-shot VQA without finetuning at all. In our experiments, we found that {\ours} is able to ``answer'' by completing prompting sentences, as shown in Figure \ref{fig:example} (d). Nonetheless, we also observed that the model falls short in generating meaningful answers to the real questions. We hypothesize that this is due to the low quality of the pretraining data in which most textual descriptions are short and noisy. To verify our assumption, we continue the pretraining process on the cleaner WIT dataset \citep{srinivasan2021wit} for 50k steps. Examples in Figure \ref{fig:example} (e) show that open-ended VQA ability emerges in {\ours} such that it can generate related responses after finetuning on the knowledge-rich wikipedia dataset. 


\subsection{Analysis}
\label{sec:ablation}



\begin{wraptable}{r}{0.5\textwidth}
\begin{footnotesize}
\begin{center}
\begin{tabular}{l | c}
\toprule
Method & VQA score  \\
\bottomrule
\toprule
No Pretraining & 49.70  \\
Decoder-only & 65.23 \\
\quad w/ LM & 64.48 \\
\bottomrule
\toprule
 & 67.43  \\
\quad w/o Image2Text & 49.23   \\
\quad w/o Text2Text & 65.25   \\
\quad w/o conv stage & 63.11  \\
\quad w/ span corruption & 66.23  \\
\quad w/ 2 conv blks & 65.57  \\
\quad w/ 4 conv blks & 66.55 \\
\quad w/ 10\% ALIGN & 66.71  \\
\quad w/ CC-3M & 63.32 \\
\bottomrule
\end{tabular}
\end{center}
\caption[caption]{Ablation study on VQA. ``w/ LM'' and ``w/ span corruption'' denote replacing the proposed PrefixLM loss with a different pretraining objective. ``Image2Text'' and ``Text2Text'' refer to the noisy image-text data and the text-only data used for pretraining. ``conv blks'' denotes number of ResNet blocks.} 
\label{tab:ablation}
\end{footnotesize}
\end{wraptable}


\textbf{Single-Modality Tasks.}
Since {\ours} performs well on joint vision-language benchmarks,
it is natural to ask how well the learned representations perform on tasks of single modality. We hope to gain deeper insights into the model behavior by examining its performance on these benchmarks, but it is not our intention to achieve state-of-the-art on single-modality tasks.
In Table \ref{tab:glue} (Appendix \ref{appexdix:language}),
we compare {\ours} with existing VLP models on the GLUE benchmark \citep{wang2018glue}, where we mainly follow the text processing procedure in \cite{raffel2019exploring} and train our model to classify the fully formatted input without token type embeddings. {\ours} performs better than existing VLP methods and competitively with BERT, indicating that it has good language understanding ability. Additionally,
we also compute the top-1 accuracy on ImageNet following the linear evaluation protocol in Table \ref{tab:imagenet}. Note that our model is not pretrained with a discriminative task such as the contrastive loss, hence we use an average pooling of encoder outputs as image features. Results verify that our model has also learned high-quality image representation.


\textbf{Ablation Study.}
To study the contributions from each model component, we conduct ablation study on  models with an embedding dimension of 512 and 8 layers. We make comparisons on VQA in Table \ref{tab:ablation}. 
First, we compare encoder-decoder models with decoder-only models of comparable model size, and find that decoder-only model performs significantly worse on VQA. 
This suggests the inductive bias of separating bidirectional encoding from unidirectional decoding is beneficial for joint VL representation learning.
Next, we study the effectiveness of pretraining objectives and results show that the PrefixLM objective outperforms both span corruption \citep{raffel2019exploring} and naive LM,
illustrating the importance of using a unified objective formulation for both image-text and text-only data.
Moreover, we ablate the contribution of datasets.
While weakly aligned image-text data are required for bridging the gap between visual and textual representations, text-only corpora also improves the model quality. 
This is probably because textual signals are extremely noisy in the former and thus the model relies on the later to acquire better language understanding.
In addition, we experimented with 10\% ALIGN and CC-3M \citep{sharma-etal-2018-conceptual} datasets, and confirms the importance of data scaling.
We then study the effect of the convolution stage and find it critical for VL performance. 
Following \cite{dai2021coatnet},
we experiment with using either the first 2/3/4 ResNet Conv blocks,
and empirically observe that the 3 conv block setup works best.
This indicates that image and text have different levels of representation granularity and thus utilizing contextualized patches is beneficial.


\section{Conclusion}
In this work, we present a simple yet effective framework of vision-language pretraining. Unlike prior works using object proposal systems and auxiliary losses,
our model processes whole image as patches and is trained end-to-end with a single prefix language modeling objective.
Our work suggests a promising alternative to existing VLP paradigm and we hope our work may inspire future research on generative VLP.



\subsubsection*{Acknowledgments}
We would like to thank Hieu Pham, Chao Jia, Andrew Dai, Bowen Zhang, Zhifeng Chen, Ruoming Pang, Douglas Eck, Claire Cui and Yonghui Wu for helpful discussions,
Krishna Srinivasan, Samira Daruki, Nan Du and Aashi Jain for help with data preparation,
Chao Jia, Zhen Li, Jonathan Shen, Colin Raffel and Sharan Narang for assistance on experimental settings,
and others in the Google Brain team for support throughout this project.

\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}

\clearpage
\appendix

\section{Generated Examples}
\label{appendix:examples}
Examples generated by {\ours} of various types are shown in Figure \ref{fig:example}. We use either image-only or image-text prefix inputs in the encoder, and use the decoder to generate suffix text.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{example.png}
\end{center}
\caption{Generated examples of {\ours} of various applications: (a) zero-shot image captioning (b) zero-shot cross-modality transfer on German image captioning (c) generative VQA (d) zero-shot visual text completion (e) zero-shot open-ended VQA.}
\label{fig:example}
\end{figure}


\begin{table*}[t!]
\begin{center}
\begin{tabular}{l | c c c c c c c c}
\toprule
& CoLA & SST-2 & RTE & MRPC & QQP & MNLI & QNLI & WNLI \\
\bottomrule
\toprule
BERT & \bf 54.6 & \bf 92.5 & 62.5 & \bf 81.9/87.6 & \bf 90.6/87.4 & \bf 84.2 & \bf 91.0 & 48.8\\
\bottomrule
\toprule
VisualBERT & 38.6 & 89.4 & 56.6 & 71.9/82.1 & 89.4/86.0 & 81.6 & 87.0 & 53.1 \\
UNITER & 37.4 & 89.7 & 55.6 & 69.3/80.3 & 89.2/85.7 & 80.9 & 86.0 & 55.4 \\
VL-BERT & 38.7 & 89.8 & 55.7 & 70.6/81.8 & 89.0/85.4 & 81.2 & 86.3 & 53.1 \\
VilBERT & 36.1 & 90.4 & 53.7 & 69.0/79.4 & 88.6/85.0 & 79.9 & 83.8 & 55.4 \\
LXMERT & 39.0 & 90.2 & 57.2 & 69.8/80.4 & 75.3/75.3 & 80.4 & 84.2 & 46.0 \\
 & \underline{46.7} & \underline{90.9} & \bf \underline{63.9} & \underline{75.2/84.4} & \underline{90.4/87.2} & \underline{83.4} & \underline{88.6} & \bf \underline{58.1} \\
\bottomrule
\end{tabular}
\end{center}
\caption[caption]{Text-only task performance on the GLUE benchmark (Dev set). Results for BERT and other VLP methods are obtained from \cite{iki2021effect}. The overall best result is \textbf{bolded} while \underline{underline} signifies the best VLP model.} 
\label{tab:glue}
\end{table*}

\section{Experimental Details}

\subsection{Pretraining}\label{sec:pretrain}

Our models are pretrained according to the methodology described in Section \ref{sec:approach}.
For the Transformer,
each variant follows the same setting as its corresponding ViT variant.
For the Conv stage,
we use the first three blocks (excluding the Conv stem) of ResNet-101 and ResNet-152 \citep{he2016deep} for our Base and Large models respectively,
and a larger variant of ResNet-152 with more channels for the Huge model (matching its hidden dimension size).
We always use a fixed patch size of 1616.
During pretraining,
we utilize the resolution of 224224,
resulting in a patch sequence of length 1414 as visual tokens.
For the textual input,
we use a vocabulary size of 32,000 and a max sequence length of 256 in both the encoder and the decoder.
We also share parameters between the embedding and the decoder softmax output layer \citep{press2016using}.
All parameters are shared across visual and textual inputs except the Conv stage and positional embeddings.

We pretrain on large-scale web datasets for both image-text and text-only inputs.
For joint vision and language data,
we exploit the training set of ALIGN \citep{jia2021scaling},
which contains about 1.8B noisy image-text pairs.
Notice that we do not use any extra data preprocessing or filtering,
except simple random resized cropping.
For the text-only copora,
we use the Colossal Clean Crawled Corpus (C4) dataset presented in \cite{raffel2019exploring} and followed their preprocessing steps.
The dataset contains about 800GB of web crawled documents.

All models are pretrained for about 1M steps from scratch to optimize for the single PrefixLM objective in Eq.\ref{eq:prefix}.
We use the AdamW optimizer \citep{loshchilov2017decoupled} with  and weight decay of 0.01.
We warm up the learning rate for the first 2\% of updates to a peak value of 5, and then linearly decay it afterwards.
Dropout is not used during the pretraining stage.
We mix the two pretraining datasets within each batch, which contains 4,096 image-text pairs and 512 text-only documents, sharded across 512 TPU v3 chips \citep{jouppi2017indatacenter}.

\subsection{Finetuning}\label{sec:finetuning}

After pretraining, our model is finetuned on various downstream tasks.
Similar to the pretraining stage,
we use the AdamW optimizer with the same Beta values,
while we tune the learning rate in \{1, 2, 5\}.
We also enable regularization methods of Dropout (set to 0.1) and stochastic depth (only applied to Conv stage and encoder with a fixed dropout rate of 0.1) \citep{huang2016deep} during the finetuning stage.
Following standard practice,
we use the corresponding dev split to find the best setting and report the result on the test split.
We consider 5 types of downstream tasks listed below:

\textbf{Visual question answering:}
This task requires the model to answer questions about input images,
and has been the most widely used VL benchmark.
Following prior work, we use the VQA v2 \citep{goyal2017making} and formulate the task as a classification problem over 3,129 most frequent answers in the training set.
The raw image and the corresponding question are used as inputs to the encoder and the decoder respectively, and a task-specific linear classifier is trained to predict answer based on activation corresponding to the last question token from the decoder. We use a resolution of 480480 for the image and all positional parameters are adapted using linear interpolation.

\textbf{Visual entailment:}
The SNLI-VE \citep{xie2019visual} dataset is adapted from SNLI \citep{bowman2015large}, 
which is originally designed to predict the relation between a premise sentence and a hypothesis sentence as either entailment, neutral or contradiction,
a task known as natural language inference (NLI). For the VL variant, the premise is based on the content of an image rather than textual descriptions. We finetune {\ours} similarly to VQA, such that the image and the sentence are fed to encoder and decoder separately, and the classifier is trained to predict the three relations.

\textbf{Visual reasoning:}
The NLVR2 \citep{suhr2018corpus} dataset tests the model's ability of jointly reasoning over the language and multiple images by asking whether a textual description is true based on a pair of two images. Following \cite{Zhang_2021_CVPR},
we create two input pairs, each consisting of one image and the textual description,
and generate output embeddings for both using the same setup above.
The two embeddings are then concatenated for final prediction.

\textbf{Image captioning:}
The captioning task requires a model to generate natural language descriptions of input images. We consider two datasets CoCo \citep{chen2015microsoft} and NoCaps \citep{agrawal2019nocaps}, both finetuned using the CoCo training data.
For {\ours}, it is straightforward to first encode the image in the encoder and then generate captions using the decoder. Note that in contrast to prior work that apply task-specific tricks such as CIDEr optimization \citep{rennie2017self}, our model is trained with naive cross-entropy loss only.

\textbf{Multimodal translation:}
The goal of multimodal translation is to translate image descriptions in source language to target language, for which image inputs can be taken advantage of as grounding signal. We train and evaluate on the Multi30k \citep{elliott2016multi30k} dataset. We utilize the PrefixLM described in previous sections such that the source sentence, together with the image inputs, are fed to the encoder, which will be translated to the target language by the decoder.


\section{Model Performance on Language-only Task}
\label{appexdix:language}
We compare our model with prior VLP methods on natural language understanding (NLU) tasks on the GLUE benchmark \citep{wang2018glue} in Table \ref{tab:glue}.


\section{Erratum}
\label{appexdix:erratum}
We found an error in reporting the zero-shot COCO evaluations in the first version of this paper. 
This mistake does NOT affect all other results and the numbers have been updated.
Meanwhile, we also added few-shot results in addition to zero-shot results on both MsCOCO and NoCaps in Table \ref{tab:caption}, to provide a more comprehensive view of capacities in SimVLM models.
Hence, our main claims and conclusions still hold.


\end{document}
