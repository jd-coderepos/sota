\newcommand\ROBERTABASE{RoBERTa$_{\small \texttt{BASE}}$\xspace}
\newcommand\BERTBASE{BERT$_{\small \texttt{BASE}}$\xspace}
\newcommand\BERTLARGE{BERT$_{\small \texttt{LARGE}}$\xspace}
\newcommand\RERNIE{ERNIE$_{\small \texttt{RoBERTa}}$\xspace}
\newcommand\RKNOWBERT{KnowBert$_{\small \texttt{RoBERTa}}$\xspace}
\section{Experiments}
\label{sec:exp}

In this section, we introduce the experiment settings and results of our model on various NLP and KE tasks, along with some analyses on KEPLER.

\subsection{Experimental Setting}
\label{sec:expsetting}


\paragraph{Baselines} In our experiments, \textbf{RoBERTa} is an important baseline since KEPLER is based on it (all mentioned models are of \texttt{BASE} size if not specified). 
As we cannot afford the full RoBERTa corpora (126~GB, and we only use 13~GB) in KEPLER pre-training, we implement \textbf{Our RoBERTa} for direct comparisons to KEPLER. It is initialized by \ROBERTABASE and is further trained with the MLM objective on the same corpora as KEPLER.

We also evaluate recent knowledge-enhanced PLMs, including \textbf{ERNIE$_{\small \texttt{BERT}}$}~\citep{zhang-etal-2019-ernie} and \textbf{KnowBert$_{\small \texttt{BERT}}$}~\citep{peters-etal-2019-knowledge}. 
As ERNIE and our principal model KEPLER-Wiki only use Wikidata, we take KnowBert-Wiki in the experiments to ensure fair comparisons with the same knowledge source. Considering KEPLER is based on RoBERTa, we reproduce the two models with RoBERTa too (\textbf{\RERNIE} and \textbf{\RKNOWBERT}). The reproduction of KnowBert is based on its original implementation\footnote{\url{https://github.com/allenai/kb}}. 
On relation classification, we also compare with MTB~\citep{baldini-soares-etal-2019-matching}, which adopts ``matching the blank'' pre-training. Different from other baselines, the original MTB is based on \BERTLARGE (denoted by \textbf{MTB (\BERTLARGE)}). For a fair comparison under the same model size, we reimplement MTB with \BERTBASE (\textbf{MTB}).

\paragraph{Hyper-parameter}
The pre-training settings are in Section~\ref{sec:pretraining}. For fine-tuning on downstream tasks, we set KEPLER hyper-parameters the same as reported in KnowBert on TACRED and OpenEntity. On FewRel, we set the learning rate as $2$e-$5$ and batch size as $20$ and $4$ for the Proto and PAIR frameworks respectively. For GLUE, we follow the hyper-parameters reported in RoBERTa. For baselines, we keep their original hyper-parameters unchanged or use the best trial in KEPLER searching space if no original settings are available.

\subsection{NLP Tasks}
\label{sec:nlpexp}
In this section, we demonstrate the performance of KEPLER and its baselines on various NLP tasks. 

\begin{comment}
\begin{table}[t]
    \centering
        \begin{adjustbox}{max width=0.98\linewidth}

        \tablefont
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{P} & \textbf{R} & \textbf{F-1} \\
        \midrule
        BERT & $67.2$ & $64.8$ & $66.0$ \\
        ERNIE$_{\small \texttt{BERT}}$ & $70.0$ & $66.1$ & $68.0$ \\
        KnowBert$_{\small \texttt{BERT}}$ & $\bm{73.5}$ & $64.1$ & $68.5$ \\
        BERT-LARGE & - & - & $70.1$ \\
        MTB$_{\small \texttt{BERT-LARGE}}$ & - & - & $71.5$ \\
        RoBERTa & $70.4$ & $71.1$ & $70.7$ \\
        \RERNIE & $73.5$ & $68.0$ & $70.7$ \\
        \RKNOWBERT & $71.9$ & $69.9$ & $70.9$ \\ 
        \midrule
        Our RoBERTa & $70.8$ & $69.6$ & $70.2$ \\
        KEPLER-Wiki & $71.5$ & $\bm{72.5}$ & $\bm{72.0}$ \\
        KEPLER-WordNet & $71.4$ & $71.3$ & $71.3$ \\
        KEPLER-W+W & $71.1$ & $72.0$ & $71.5$ \\
        KEPLER-Rel & $71.3$ & $70.9$ & $71.1$ \\
        KEPLER-Cond & $72.1$ & $70.7$ & $71.4$ \\
        KEPLER-OnlyDesc & $72.3$ & $69.1$ & $70.7$ \\
        KEPLER-KE & $63.5$ & $60.5$ & $62.0$ \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Precision, recall and F-1 scores on TACRED (\%). KnowBert results are different with the original paper since different task settings are used.}
    \label{tab:tacred}
\end{table}
\end{comment}





\subsubsection*{Relation Classification}

Relation classification requires models to classify relation types between two given entities from text. We evaluate KEPLER and other baselines on two widely-used benchmarks: TACRED and FewRel.

\begin{table}[t]
    \centering
        \begin{adjustbox}{max width=0.98\linewidth}

        \tablefont
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{P} & \textbf{R} & \textbf{F-1} \\
        \midrule
        BERT & $67.2$ & $64.8$ & $66.0$ \\
        \BERTLARGE & - & - & $70.1$ \\
        MTB & $69.7$ & $67.9$ & $68.8$ \\
        MTB (\BERTLARGE) & - & - & $71.5$ \\
        ERNIE$_{\small \texttt{BERT}}$ & $70.0$ & $66.1$ & $68.0$ \\
        KnowBert$_{\small \texttt{BERT}}$ & $\bm{73.5}$ & $64.1$ & $68.5$ \\
        RoBERTa & $70.4$ & $71.1$ & $70.7$ \\
        \RERNIE & $\bm{73.5}$ & $68.0$ & $70.7$ \\
        \RKNOWBERT & $71.9$ & $69.9$ & $70.9$ \\ 
        \midrule
        Our RoBERTa & $70.8$ & $69.6$ & $70.2$ \\
        KEPLER-Wiki & $71.5$ & $\bm{72.5}$ & $\bm{72.0}$ \\
        KEPLER-WordNet & $71.4$ & $71.3$ & $71.3$ \\
        KEPLER-W+W & $71.1$ & $72.0$ & $71.5$ \\
        KEPLER-Rel & $71.3$ & $70.9$ & $71.1$ \\
        KEPLER-Cond & $72.1$ & $70.7$ & $71.4$ \\
        KEPLER-OnlyDesc & $72.3$ & $69.1$ & $70.7$ \\
        KEPLER-KE & $63.5$ & $60.5$ & $62.0$ \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Precision, recall and F-1 on TACRED (\%). KnowBert results are different from the original paper since different task settings are used.}
    \label{tab:tacred}
\end{table}

\textbf{TACRED} \citep{zhang-etal-2017-position} has $42$ relations and $106,264$ sentences. Here we follow the settings of~\citet{baldini-soares-etal-2019-matching}, where we add four special tokens before and after the two entity mentions, and concatenate the representations at the beginnings of the two entities for classification. 
Note that the original KnowBert also takes entity types as inputs, which is different from \citet{zhang-etal-2019-ernie,baldini-soares-etal-2019-matching}. To ensure fair comparisons, we re-evaluate KnowBert with the same setting as other baselines, thus the reported results are different from the original paper. 

\begin{table*}[th]

\centering
    \tablefont
\begin{adjustbox}{max width=0.98\linewidth}
    \begin{tabular}{l|cccc|cccc}
        \toprule
        \multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{4}{c|}{\textbf{FewRel 1.0}} & \multicolumn{4}{c}{\textbf{FewRel 2.0}}\\
& \textbf{5-1} & \textbf{5-5} & \textbf{10-1} & \textbf{10-5} & \textbf{5-1} & \textbf{5-5} & \textbf{10-1} & \textbf{10-5}\\
        \midrule
        MTB (\BERTLARGE)$^\dagger$ & $93.86$ & $97.06$ & $89.20$ & $94.27$ & $-$ & $-$ & $-$ & $-$ \\
        \midrule
        Proto (BERT) & $80.68$ & $89.60$ & $71.48$ & $82.89$ & $40.12$ & $51.50$ & $26.45$ & $36.93$\\
        Proto (MTB) & $81.39$ &	$91.05$&	$71.55$&	$83.47$&	$52.13$&	$76.67$&	$48.28$ &	$69.75$ \\
        Proto (ERNIE$_{\small \texttt{BERT}}$)$^\dagger$ & $\textbf{89.43}$ & $94.66$ & $\textbf{84.23}$ & $90.83$ & $49.40$ & $65.55$ & $34.99$ & $49.68$\\
        Proto (KnowBert$_{\small \texttt{BERT}}$)$^\dagger$ & $86.64$ & $93.22$ & $79.52$ & $88.35$ & $64.40$ & $79.87$ & $51.66$ & $69.71$\\
        Proto (RoBERTa) & $85.78$ & $95.78$ & $77.65$ & $92.26$ & $64.65$ & $82.76$ & $50.80$ & $71.84$\\
        Proto (Our RoBERTa) & $84.42$ & $95.30$ & $76.43$ & $91.74$ & $61.98$ & $83.11$ & $48.56$ & $72.19$ \\
        Proto (\RERNIE)$^\dagger$ & $87.76$ & $95.62$ & $80.14$ & $91.47$ & $54.43$ & $80.48$ & $37.97$ & $66.26$ \\
        Proto (\RKNOWBERT)$^\dagger$ & $82.39$ & $93.62$ & $76.21$ & $88.57$ & $55.68$ & $71.82$ & $41.90$ & $58.55$\\
        Proto (KEPLER-Wiki) & $88.30$ & $\textbf{95.94}$ & $81.10$ & $\textbf{92.67}$ & $\textbf{66.41}$ & $\textbf{84.02}$ & $\textbf{51.85}$ & $\textbf{73.60}$ \\
\midrule
        PAIR (BERT) & $88.32$ & $93.22$ & $80.63$ & $87.02$& $\textbf{67.41}$ & $78.57$ & $\textbf{54.89}$ & $66.85$ \\
        PAIR (MTB) & $83.01$ & $87.64$ & $73.42$ & $78.47$ & $46.18$ & $70.50$ & $36.92$ & $55.17$\\
        PAIR (ERNIE$_{\small \texttt{BERT}}$)$^\dagger$ & $\textbf{92.53}$ & $94.27$ & $\textbf{87.08}$ & $89.13$ & $56.18$ & $68.97$ & $43.40$ & $54.35$\\
        PAIR (KnowBert$_{\small \texttt{BERT}}$)$^\dagger$ & $88.48$ & $92.75$ & $82.57$ & $86.18$ & $66.05$ & $77.88$ & $50.86$ & $67.19$\\
        PAIR (RoBERTa) & $89.32$ & $93.70$ & $82.49$ & $88.43$ & $66.78$ & $81.84$ & $53.99$ & $70.85$\\
        PAIR (Our RoBERTa) & $89.26$ & $93.71$ & $83.32$ & $89.02$ & $63.22$ & $77.66$ & $49.28$ & $65.97$ \\
        PAIR (\RERNIE)$^\dagger$ & $87.46$ & $94.11$ & $81.68$ & $87.83$ & $59.29$ & $72.91$ & $48.51$ & $60.26$ \\
        PAIR (\RKNOWBERT)$^\dagger$ & $85.05$ & $91.34$ & $76.04$ & $85.25$ & $50.68$ & $66.04$ & $37.10$ & $51.13$\\
        PAIR (KEPLER-Wiki) & $90.31$ & $\textbf{94.28}$ & $85.48$ & $\textbf{90.51}$ & $67.23$ & $\textbf{82.09}$ & $54.32$ & $\textbf{71.01}$ \\ 
\bottomrule
    \end{tabular}
    \end{adjustbox}


\caption{Accuracies ($\%$) on the FewRel dataset. $N$-$K$ indicates the $N$-way $K$-shot setting. 
MTB uses the \texttt{LARGE} size and all the other models use the \texttt{BASE} size. $^\dagger$ indicates oracle models which may have seen facts in the FewRel 1.0 test set during pre-training.}
    \label{tab:fewrel}

\end{table*}

From the TACRED results in Table~\ref{tab:tacred}, 
we can observe that: (1) KEPLER-Wiki is the best one among KEPLER variants and significantly outperforms all the baselines, while other versions of KEPLER also achieve good results. It demonstrates the effectiveness of KEPLER on integrating factual knowledge into PLMs. Based on the result, we use KEPLER-Wiki as the principal model in the following experiments. (2) KEPLER-WordNet shows a marginal improvement over Our RoBERTa, while KEPLER-W+W underperforms KEPLER-Wiki. It suggests that pre-training with WordNet only has limited benefits in the KEPLER framework. We will explore how to better combine different KGs in our future work.




\textbf{FewRel}~\citep{han-etal-2018-fewrel} is a few-shot relation classification dataset with $100$ relations and $70,000$ instances, which is constructed with Wikipedia text and Wikidata facts. Furthermore, \citet{gao-etal-2019-fewrel} propose \textbf{FewRel 2.0}, adding a domain adaptation challenge with a new medical-domain test set.

FewRel takes the $N$-way $K$-shot setting. Relations in the training and test sets are disjoint. For every evaluation episode, $N$ relations, $K$ supporting samples for each relation, and several query sentences are sampled from the test set. The models are required to classify queries into one of the $N$ relations only given the sampled $N\times K$ instances.

We use two state-of-the-art few-shot frameworks: \textbf{Proto}~\citep{snell2017prototypical} and \textbf{PAIR}~\citep{gao-etal-2019-fewrel}. We replace the text encoders with our baselines and KEPLER and compare the performance. Since FewRel 1.0 is constructed with Wikidata, we remove all the triplets in its test set from Wikidata5M to avoid information leakage for KEPLER. However, we cannot control the KGs used in our baselines. We mark the models utilizing Wikidata and have information leakage risk with $^\dagger$ in Table~\ref{tab:fewrel}.

\begin{table}[t]
    \tablefont
    \centering
        \begin{adjustbox}{max width=0.98\linewidth}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{P} & \textbf{R} & \textbf{F-1} \\
        \midrule
        UFET~\citep{choi-etal-2018-ultra} & $77.4$ & $60.6$ & $68.0$\\
        BERT & $76.4$ & $71.0$ & $73.6$ \\
        ERNIE$_{\small \texttt{BERT}}$ & $78.4$ & $72.9$ & $75.6$ \\
        KnowBert$_{\small \texttt{BERT}}$ & $77.9$ & $71.2$ & $74.4$ \\
        RoBERTa & $77.4$ & $73.6$ & $75.4$ \\
        \RERNIE & $80.3$ & $70.2$ & $74.9$ \\
        \RKNOWBERT & $78.7$ & $72.7$ & $75.6$ \\
        \midrule
        Our RoBERTa & $75.1$ & $73.4$ & $74.3$ \\
        KEPLER-Wiki & $77.8$ & $74.6$ & $\textbf{76.2}$\\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Entity typing results on OpenEntity ($\%$).}
    \label{tab:openentity}
\end{table}

As Table~\ref{tab:fewrel} shows, KEPLER-Wiki achieves the best performance over the \texttt{BASE}-size PLMs in most settings. From the results, we also have some interesting observations: 
(1) RoBERTa consistently outperforms BERT on various NLP tasks~\citep{liu2019roberta}, yet the RoBERTa-based models here are comparable or even worse than BERT-based models in the PAIR framework. Since PAIR uses sentence concatenation, this result may be credited to the next sentence prediction (NSP) objective of BERT. (2) KEPLER brings improvements on FewRel 2.0, while ERNIE and KnowBert even degenerate in most of the settings. It indicates that the paradigms of ERNIE and KnowBert cannot well generalize to new domains which may require much different entity linkers and entity embeddings. On the other hand, KEPLER not only learns better entity representations but also acquires a general ability to extract factual knowledge from the context across different domains. We further verify this in Section~\ref{sec:uos}. 
(3) KnowBert underperforms ERNIE in FewRel while it typically achieves better results on other tasks. This may be because it uses the TuckER~\citep{balazevic-etal-2019-tucker} KE model while ERNIE and KEPLER follow TransE~\citep{bordes2013translating}. We will explore the effects of different KE methods in the future. 

We also have another two observations with regard to ERNIE and MTB: (1) ERNIE performs the best on $1$-shot settings of FewRel 1.0. We believe this is because that the knowledge embedding injection of ERNIE has particular advantages in this case, since it directly brings knowledge about entities. When using 5-shot (supporting text provides more information) and FewRel 2.0 (ERNIE does not have knowledge for biomedical entities), KEPLER outperforms ERNIE. (2) Though MTB (\BERTLARGE) is the state-of-the-art model on FewRel, its \BERTBASE version does not outperform other knowledge-enhanced PLMs, which suggests that using large models contributes much to its gain. We also notice that when combined with PAIR, MTB suffers an obvious performance drop, which may be because its pre-training objective degenerates sentence-pair tasks.

\subsubsection*{Entity Typing}

Entity typing requires to classify given entity mentions into pre-defined types. For this task, we carry out evaluations on OpenEntity~\citep{choi-etal-2018-ultra} following the settings in \citet{zhang-etal-2019-ernie}. OpenEntity has 6 entity types and 2,000 instances for training, validation and test each.

To identify the entity mentions of interest, we add two special tokens before and after the entity spans, and use the representations of the first special tokens for classification. As shown in Table~\ref{tab:openentity}, KEPLER-Wiki achieves state-of-the-art results. Note that the KnowBert results are different from the original paper since we use KnowBert-Wiki here rather than KnowBert-W+W to ensure the same knowledge resource and fair comparisons. KEPLER does not perform linking or entity embedding pre-training like ERNIE and KnowBert, which bring them special advantages in entity span tasks. However, KEPLER still outperforms these baselines, which proves its effectiveness. 



\subsubsection*{GLUE}

The General Language Understanding Evaluation (GLUE)~\citep{wang-etal-2018-glue} collects several natural language understanding tasks and is widely used for evaluating PLMs. In general, solving GLUE does not require factual knowledge~\citep{zhang-etal-2019-ernie} and we use it to examine whether KEPLER harms the general language understanding ability.

Table~\ref{tab:glue} shows the GLUE results. We can observe that KEPLER-Wiki is close to Our RoBERTa, suggesting that while incorporating factual knowledge, KEPLER maintains a strong language understanding ability. However, there are significant performance drops of KEPLER-OnlyDesc, which indicates that the small-scale entity description data are not sufficient for training KEPLER with MLM.

For the small datasets STS-B, MRPC and RTE, directly fine-tuning models on them typically result in unstable performance. Hence we fine-tune models on a large-scale dataset (here we use MNLI) first and then further fine-tune them on the small datasets.
The method has been shown to be effective~\citep{wang-etal-2019-tell} and is also used in the original RoBERTa paper~\citep{liu2019roberta}.


\subsection{KE Tasks}
\label{sec:KEexp}
We show how KEPLER works as a KE model, and evaluate it on Wikidata5M in both the transductive link prediction setting and the inductive setting.

\subsubsection*{Experimental Settings}
In link prediction, the entity and relation embeddings of KEPLER are obtained as described in Section~\ref{sec:ke} and \ref{sec:pretraining}. The evaluation method is described in Section~\ref{sec:benchmark}.  We also add RoBERTa and Our RoBERTa as baselines. They adopt Equation~\ref{equ:only_entity} and \ref{equ:rhat} to acquire entity and relation embeddings, and use Equation~\ref{equ:transe} as their scoring function.

In the transductive setting, we compare our models with \textbf{TransE}~\citep{bordes2013translating}. We set its dimension as $512$, negative sampling size as $64$, batch size as $2048$ and learning rate as $0.001$ after hyper-parameter searching.  The negative sampling size is crucial for the performance on KE tasks, but limited by the model complexity, KEPLER can only take a negative size of $1$. For a direct comparison to intuitively show the benefits of pre-training, we set a baseline \textbf{TransE$^\dagger$}, which also uses $1$ as the negative sampling size and keeps the other hyper-parameters unchanged.
    
Since conventional KE methods like TransE inherently cannot provide embeddings for unseen entities, we take \textbf{DKRL}~\citep{Xie:2016:RLK:3016100.3016273} as our baseline in the KE experiments, which utilizes convolutional neural networks to encode entity descriptions as embeddings. We set its dimension as $768$, negative sampling size as $64$, batch size as $1024$ and learning rate as $0.0005$. 


\begin{table}[t]
    \centering
    \tablefont
    \begin{adjustbox}{max width=0.98\linewidth}
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \textbf{MNLI (m/mm)} & \textbf{QQP} &  \textbf{QNLI}  & \textbf{SST-2} \\
              & {\scriptsize \textbf{392K}} & {\scriptsize \textbf{363K}} & {\scriptsize \textbf{104K}} & {\scriptsize \textbf{67K}}\\
        \midrule
        RoBERTa & 87.5/87.2 &  91.9 & 92.7 & 94.8\\
        Our RoBERTa & 87.1/86.8 & 90.9 & 92.5 & 94.7 \\
        KEPLER-Wiki & 87.2/86.5  & 91.7 & 92.4 & 94.5\\
        KEPLER-OnlyDesc & 85.9/85.6  & 90.8 & 92.4 & 94.4\\
        \bottomrule
        \toprule
        \multirow{2}{*}{\textbf{Model}} &  \textbf{CoLA} & \textbf{STS-B} & \textbf{MRPC} & \textbf{RTE} \\
              & {\scriptsize \textbf{8.5K}} & {\scriptsize \textbf{5.7K}} & {\scriptsize \textbf{3.5K}} & {\scriptsize \textbf{2.5K}}  \\
        \midrule
        RoBERTa &   63.6 & 91.2   & 90.2 & 80.9\\
        Our RoBERTa &  63.4 & 91.1  & 88.4 & 82.3\\
        KEPLER-Wiki &  63.6 & 91.2 & 89.3 & 85.2  \\
        KEPLER-OnlyDesc & 55.8 & 90.2 & 88.5 & 78.3\\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    
    \caption{GLUE results on the \texttt{dev} set (\%). All the results are medians over $5$ runs. We report F-1 scores for QQP and MRPC, Spearman correlations for STS-B, and accuracy scores for the other tasks. The ``m/mm'' stands for matched/mismatched evaluation sets for MNLI~\citep{N18-1101}.} \label{tab:glue}
\end{table}


\begin{table*}[t]
    \tablefont
	\begin{subtable}[h]{0.98\textwidth}
	\centering
	\begin{adjustbox}{max width=0.98\linewidth}
	\begin{tabular}{lrrrrr}
		\toprule
		\textbf{Model} & \textbf{MR} & \textbf{MRR} & \textbf{HITS@1} & \textbf{HITS@3} & \textbf{HITS@10}\\
		\midrule
		TransE \cite{bordes2013translating} & $109370$    & $\textbf{25.3}$ & $17.0$     & $\textbf{31.1}$     & $\textbf{39.2}$     \\
		TransE$^\dagger$ & $406957$ & $6.0$ & $1.8$ & $8.0$ & $13.6$ \\
		DKRL \citep{Xie:2016:RLK:3016100.3016273} & $31566$ & $16.0$ & $12.0$ & $18.1$ & $22.9$  \\
		RoBERTa & $1381597$ & $0.1$ & $0.0$ & $0.1$ & $0.3$ \\
		Our RoBERTa & $1756130$ & $0.1$ & $0.0$ & $0.1$ & $0.2$ \\
		KEPLER-KE & $76735$ & $8.2$ & $4.9$ & $8.9$ & $15.1$ \\
		KEPLER-Rel & $15820$ & $6.6$ & $3.7$ & $7.0$ & $11.7$ \\
		KEPLER-Wiki & $\textbf{14454}$ & $15.4$ & $10.5$ & $17.4$ & $24.4$ \\
		KEPLER-Cond & $20267$ & $21.0$ & $\textbf{17.3}$ & $22.4$ & $27.7$  \\
		\bottomrule
	\end{tabular}
	\end{adjustbox}
	\caption{Transductive results on Wikidata5M (\% except MR). TransE$^\dagger$ denotes a TransE modeled trained with the same negative sampling size ($1$) as KEPLER.}
	\label{tab:trans}
	\end{subtable}
	
	
	\medskip
	
		\begin{subtable}[h]{0.98\textwidth}
	\centering
	\begin{tabular}{lrrrrr}
		\toprule
		\textbf{Model} & \textbf{MR} & \textbf{MRR} & \textbf{HITS@1} & \textbf{HITS@3} & \textbf{HITS@10}\\
		\midrule
		DKRL \citep{Xie:2016:RLK:3016100.3016273} & 78 & 23.1 & 5.9 & 32.0 & 54.6\\
		RoBERTa & $723$ & $7.4$ & $0.7$ & $1.0$ & $19.6$ \\
		Our RoBERTa & $1070$ & $5.8$ & $1.9$ & $6.3$ & $13.0$ \\
		KEPLER-KE & $138$ & $17.8$ & $5.7$ & $22.9$ & $40.7$ \\
		KEPLER-Rel & $35$ & $33.4$ & $15.9$ & $43.5$ & $66.1$ \\
		KEPLER-Wiki & $32$ & $35.1$ & $15.4$ & $46.9$ & $71.9$ \\
		KEPLER-Cond & $\textbf{28}$ & $\textbf{40.2}$ & $\textbf{22.2}$ & $\textbf{51.4}$ & $\textbf{73.0}$ \\
		\bottomrule
	\end{tabular}
	\caption{Inductive results on Wikidata5M (\% except MR).}
	\label{tab:induct}
	\end{subtable}
	\caption{Link prediction results on Wikidata5M transductive and inductive settings. }
	\label{tab:keplerkg}
\end{table*}


\subsubsection*{Transductive Setting}




Table~\ref{tab:trans} shows the results of the transductive setting. We observe that: 

(1) KEPLER underperforms TransE. It is reasonable since KEPLER is limited by its large model size, and thus cannot use a large negative sampling size ($1$ for KEPLER, while typical KE methods use $64$ or more) and more training epochs ($30$ vs $1000$ for TransE), which are crucial for KE~\citep{zhu2019graphvite}. On the other hand, KEPLER and its variants perform much better than TransE$^\dagger$ (with a negative sampling size of $1$), showing that using the same negative sampling size, KEPLER can benefit from pre-trained language representations and textual entity descriptions so that outperform TransE. In the future, we will explore reducing the model size of KEPLER to take advantage of both large negative sampling size and pre-training.

(2) The vanilla RoBERTa perform poorly in KE while KEPLER achieves favorable performances, which demonstrates the effectiveness of our multi-task pre-training to infuse factual knowledge. 

(3) Among the KEPLER variants, KEPLER-Cond has superior results, which substantiates the intuition in Section~\ref{sec:ke}. KEPLER-Rel performs worst, which we believe is due to the short and homogeneous relation descriptions of Wikidata. KEPLER-KE significantly underperforms KEPLER-Wiki, which suggests that the MLM objective is necessary as well for the KE tasks to build effective language representation.

(4) We also notice that DKRL performs well on the transductive setting and the result is close to KEPLER. We believe this is because DKRL takes a much smaller encoder (CNN) and thus is easier to train. In the more difficult inductive setting, the gap between DKRL and KEPLER is larger, which better shows the language understanding ability of KEPLER to utilize textual entity descriptions.



\subsubsection*{Inductive Setting}

Table \ref{tab:induct} shows the Wikidata5M inductive results. KEPLER outperforms DKRL and RoBERTa by a large margin, demonstrating the effectiveness of our joint training method. But KEPLER results are still far from ideal performances required by practical applications (constructing KG from scratch, etc.), which urges further efforts on inductive KE. Comparisons among KEPLER variants are consistent with in the transductive setting. 

In addition, we clarify why results in the inductive setting are much higher than the transductive setting, while the inductive setting is more difficult: As shown in Table~\ref{tab:dataset_statistics} and~\ref{tab:inductive_statistics}, the entities involved in the inductive evaluation is much less than the transductive setting ($7,475$ vs. $4,594,485$). Considering the KE evaluation metrics are based on entity ranking, it is reasonable to see higher values in the inductive setting. The performance in different settings should not be directly compared.




\begin{table}[t]
    \tablefont
	\centering
	\begin{adjustbox}{max width=0.98\linewidth}
	\begin{tabular}{lccc}
	\toprule
		\textbf{Model} & \textbf{P} & \textbf{R} & \textbf{F-1}\\
	\midrule
Our RoBERTa & $70.8$ & $69.6$ & $70.2$\\
		KEPLER-KE & $63.5$ & $60.5$ & $62.0$ \\
		KEPLER-Wiki & $71.5$ & $72.5$ & $72.0$\\
	\bottomrule
	\end{tabular}
	\end{adjustbox}
	\caption{Ablation study results on TACRED (\%).}\label{tab:abl}
\end{table}
\section{Analysis}
In this section, we analyze the effectiveness and efficiency of KEPLER with experiments. All the hyper-parameters are the same as reported in Section~\ref{sec:expsetting}, including models in the ablation study.

\subsection{Ablation Study}
\label{sec:ablation}
As shown in Equation~\ref{equ:loss}, KEPLER takes a multi-task loss. To demonstrate the effectiveness of the joint objective, we compare full KEPLER with models trained with only the MLM loss (\textbf{Our RoBERTa}) and only the KE loss (\textbf{KEPLER-KE}) on TACRED. As demonstrated in Table~\ref{tab:abl}, compared to KEPLER-Wiki, both ablation models suffer significant drops. It suggests that the performance gain of KEPLER is credited to the joint training towards both objectives. 



\subsection{Knowledge Probing Experiment}

Section~\ref{sec:nlpexp} shows that KEPLER can achieve significant improvements on NLP tasks requiring factual knowledge. To further verify whether KEPLER can better integrate factual knowledge into PLMs and help to recall them, we conduct experiments on LAMA~\citep{petroni2019language}, a widely-used knowledge probe. LAMA examines PLMs' abilities on recalling relational facts by cloze-style questions. For instance, given a natural language template ``Paris is the capital of \texttt{<mask>}'', PLMs are required to predict the masked token without fine-tuning. 
LAMA reports the micro-averaged precision at one (P@1) scores. However, \citet{Poerner2019EBERT} present that LAMA contains some easy questions which can be answered with superficial clues like entity names. Hence we also evaluate the models on LAMA-UHN~\citep{Poerner2019EBERT}, which filters out the questionable templates from the Google-RE and T-REx corpora of LAMA.





\begin{table*}[t]
\centering
\begin{adjustbox}{max width=0.98\linewidth}
\begin{tabular}{l|cccc|cc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{LAMA}}                                         & \multicolumn{2}{c}{\textbf{LAMA-UHN}} \\ \cmidrule{2-7} 
                                & \textbf{Google-RE} & \textbf{T-REx} & \textbf{ConceptNet} & \textbf{SQuAD} & \textbf{Google-RE}  & \textbf{T-REx}  \\ \midrule
BERT                       & $9.8$              & $31.1$         & $15.6$              & $14.1$         &  $4.7$         & $21.8$              \\
RoBERTa                         & $5.3$              & $24.7$         & $19.5$              & $9.1$          & $2.2$               & $17.0$          \\ \midrule
Our RoBERTa                     & $7.0$              & $23.2$         & $\bm{19.0}$              & $8.0$          & $2.8$               & $15.7$          \\
KEPLER-Wiki                     & $\textbf{7.3}$              & $\textbf{24.6}$         & $18.7$              & $\textbf{14.3}$         & $3.3$               & $16.5$          \\
KEPLER-W+W                      & $\textbf{7.3}$              & $24.4$         & $17.6$              & $10.8$         & $\textbf{4.1}$               & $\textbf{17.1}$          \\ \bottomrule
\end{tabular}
\end{adjustbox}
	\caption{P@1 results on knowledge probing benchmark LAMA and LAMA-UHN.}
	\label{tab:lama}
\end{table*}


The evaluation results are shown in Table~\ref{tab:lama}, from which we have the following observations: (1)~KEPLER consistently outperforms the vanilla PLM baseline Our RoBERTa in almost all the settings except ConceptNet, which focuses on commonsense knowledge rather than factual knowledge. It indicates that KEPLER can indeed better integrate factual knowledge. (2) Although KEPLER-W+W cannot outperform KEPLER-Wiki on NLP tasks (Section~\ref{sec:nlpexp}), it shows significant improvements in LAMA-UHN, which suggests that we should explore which kind of knowledge is needed on different scenarios in the future. (3) All the RoBERTa-based models perform worse than vanilla \BERTBASE by a large margin, which is consistent with the results of~\citet{Wang2020KAdapter}. This may be due to different vocabularies used in BERT and RoBERTa, which presents the vulnerability of LAMA-style probing again~\citep{kassner-schutze-2020-negated}. We will leave developing a better knowledge probing framework as our future work.



\subsection{Running Time Comparison}

\begin{table}[t]
\centering
\begin{adjustbox}{max width=0.98\linewidth}
\begin{tabular}{lccc}
\toprule
\textbf{Model}    & \textbf{Entity} & \textbf{Fine-} & \textbf{Inference} \\
\textbf{}    & \textbf{Linking} & \textbf{tuning} & \textbf{} \\ \midrule
\RERNIE    & 780s           & 730s        & 194s      \\
\RKNOWBERT & 190s           & 677s        & 235s      \\
KEPLER   & \textbf{0s}    & \textbf{508s} & \textbf{152s}    \\ \bottomrule
\end{tabular}
\end{adjustbox}
\caption{Three parts of running time for one epoch of TACRED training set.} \label{tab:time}
\end{table}

Compared to vanilla PLMs, KEPLER does not introduce any additional parameters or computations during fine-tuning and inference, which is efficient for practice use. We compare the running time of KEPLER and other knowledge-enhanced PLMs (ERNIE and KnowBert) in Table~\ref{tab:time}. The time is evaluated on TACRED training set for one epoch with one NVIDIA Tesla V100 (32~GB), and all models use $32$ batch size and $128$ sequence length. The ``entity linking'' time of KnowBert is for entity candidate generation. We can observe that KEPLER requires much less running time since it does not need entity linking or entity embedding fusion, which will benefit time-sensitive applications. 

\subsection{Correlation with Entity Frequency}
\label{sec:entFreq}

\begin{table*}[t]
\centering
    \begin{adjustbox}{max width=0.98\linewidth}
\begin{tabular}{l|ccccc}
\toprule
\textbf{Entity Frequency} & \textbf{0\%-20\%} & \textbf{20\%-40\%} & \textbf{40\%-60\%} & \textbf{60\%-80\%} & \textbf{80\%-100\%} \\ \midrule
KEPLER-Wiki      & $64.7$     & $64.4$      & $64.8$     & $64.7$      & $68.8$       \\
Our RoBERTa          & $64.1$     & $64.3$      & $64.5$      & $64.3$      & $68.5$       \\ \midrule
Improvement      & $+0.6$     & $+0.1$      & $+0.3$      & $+0.4$      & $+0.3$       \\ \bottomrule
\end{tabular}
\end{adjustbox}
\caption{F-1 scores on TACRED (\%) under different settings by entity frequencies. We sort the entity mentions in TACRED by their corresponding entity frequencies in Wikipedia. The ``0\%-20\%'' setting indicates only keeping the least frequent 20\% entity mentions and masking all the other entity mentions (for both training and validation), and so on. The results are averaged over 5 runs.}
\label{tab:freq}
\end{table*}

\begin{figure}[t]
\centering
\includegraphics[width = 0.48\textwidth]{fig/entFreq.pdf}
\caption{TACRED performance (F-1) of KEPLER and RoBERTa change with the rate of entity mentions being masked.}
\label{fig:entFreq}
\end{figure}

To better understand how KEPLER helps the entity-centric tasks, we provide analyses on the correlations between KEPLER performance and entity frequency in this section. The motivation is to verify a natural hypothesis that KEPLER improvements mainly come from better representing the entity mentions in text, especially the rare entities, which do not show up frequently in the pre-training corpora and thus cannot be well learned by the language modeling objectives. 

We perform entity linking for the TACRED dataset with BLINK~\cite{wu2019zero} to link the entity mentions in text to their corresponding Wikipedia identifiers. Then we count the occurrences of the entities in Wikipedia with the hyperlinks in rich text, denoting the entity frequencies. We conduct two experiments to analyze the correlations between KEPLER performance and entity frequency: (1) In Table~\ref{tab:freq}, we divide the entity mentions into five parts by their frequencies, and compare the TACRED performances while only keeping entities in one part and masking the other. (2) In Figure~\ref{fig:entFreq}, we sequentially mask the entity mentions in the ascending order of entity frequencies and see the F-1 changes.

From the results, we can observe that: 

(1) Figure~\ref{fig:entFreq} shows that when the entity masking rate is low, the improvements of KEPLER over RoBERTa are generally much higher than when the entity masking rate is high. It indicates that the improvements of KEPLER do mainly come from better modeling entities in context. However, even when all the entity mentions are masked, KEPLER still outperforms RoBERTa. We claim this is because the KE objective can also help to learn to understand fact-related text since it requires the model to recall facts from textual descriptions. This claim is further substantiated in Section~\ref{sec:uos}. 

(2) From Table~\ref{tab:freq}, we can observe that the improvement in the ``0\%-20\%'' setting is marginally higher than the other settings, which demonstrates that KEPLER does have special advantages on modeling rare entities compared to vanilla PLMs. But the improvements in the frequent settings are also significant and we cannot say that the overall improvements of KEPLER are mostly from the rare entities. In general, the results in Table~\ref{tab:freq} show that KEPLER can better model all the entities, no matter rare or frequent.

\begin{table}[h]
    \tablefont
	\centering
\begin{adjustbox}{max width=0.98\linewidth}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{ME} & \textbf{OE}\\
        \midrule
        Our RoBERTa & $54.0$ & $46.8$\\
        KEPLER-KE & $40.2$ & $47.0$ \\
        KEPLER-Wiki & $54.8$ & $48.9$\\
        \bottomrule
    \end{tabular}
\end{adjustbox}
	\caption{Masked-entity (ME) and only-entity (OE) F-1 scores on TACRED (\%).}\label{tab:casestudy}
\end{table}

\subsection{Understanding Text or Storing Knowledge}


\label{sec:uos}

We argue that by jointly training the KE and the MLM objectives, KEPLER (1) can better understand fact-related text and better extract knowledge from text, and also (2) can remember factual knowledge. To investigate the two abilities of KEPLER in a quantitative aspect, we carry out an experiment on TACRED, in which the head and tail entity mentions are masked (masked-entity, ME) or only head and tail entity mentions are shown (only-entity, OE). The ME setting shows to what extent the models can extract facts only from the textual context without the clues in entity names. The OE setting demonstrates to what extent the models can store and predict factual knowledge, as only the entity names are given to the models.

As shown in Table~\ref{tab:casestudy}, KEPLER-Wiki shows significant improvements over Our RoBERTa in both settings, which suggests that KEPLER has indeed possessed superior abilities on both extracting and storing knowledge compared to vanilla PLMs without knowledge infusion. And the KEPLER-KE model performs poorly on the ME setting but achieves marginal improvements on the OE setting. It indicates that without the help of the MLM objective, KEPLER only learns the entity description embeddings and degenerates in general language understanding, while it can still remember knowledge into entity names to some extent.

