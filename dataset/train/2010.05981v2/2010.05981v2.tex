
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{booktabs}       \usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{xcolor}
\usepackage{array}
\usepackage{soul}
\usepackage{multirow}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.2em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother
\newcommand{\app}{\raise.17ex\hbox{$\scriptstyle\sim$}}

\makeatletter

\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\definecolor{Highlight}{HTML}{39b54a}
\definecolor{shapecolor}{HTML}{7030A0}
\definecolor{texturecolor}{HTML}{00B0F0}
\newcommand{\WS}[1]{ \textcolor{red}{(WS: #1)}  }
\newcommand{\yingwei}[1]{ \textcolor{red}{(yingwei: #1)}  }
\newcommand{\cihang}[1]{{\color{magenta}[cihang: #1]}}
\newcommand{\mx}[1]{{\color{blue}[mingxing: #1]}}
\makeatother

\title{Shape-Texture Debiased Neural Network Training}

\author{Yingwei Li$^1$, Qihang Yu$^1$, Mingxing Tan$^2$, Jieru Mei$^1$, Peng Tang$^1$, Wei Shen$^3$\\
\textbf{Alan Yuille$^1$ \& Cihang Xie$^4$} \vspace{.3em}\\
$^1$Johns Hopkins University \quad
$^2$Google Brain \quad 
$^3$Shanghai Jiaotong University \quad \\
$^4$University of California, Santa Cruz \vspace{-0.5em}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
    Shape and texture are two prominent and complementary cues for recognizing objects. 
    Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset.
    Our ablation shows that such bias degenerates model performance. 
    Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning.
    To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (\eg, an image of chimpanzee shape but with lemon texture) and, \emph{most importantly, provide the corresponding supervisions from shape and texture simultaneously}. 
   
    Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2\%), ImageNet-A  (+5.2\%), ImageNet-C (+8.3\%) and Stylized-ImageNet (+11.1\%), and on defending against FGSM adversarial attacker on ImageNet (+14.4\%). Our method also claims to be compatible with other advanced data augmentation strategies, \eg, Mixup and CutMix. The code is available here: \url{https://github.com/LiYingwei/ShapeTextureDebiasedTraining}.
\end{abstract}


\section{Introduction}
It is known that both shape and texture serve as essential cues for object recognition. 
A decade ago, computer vision researchers had explicitly designed a variety of hand-crafted features, either based on shape (\eg, shape context~\citep{belongie2002shape} and inner distance shape context~\citep{ling2007shape}) or texture (\eg,~textons~\citep{MalikBLS01}), for object recognition. 
Moreover, researchers found that properly combining shape and texture can further recognition performance~\citep{shotton2009textonboost,ZhengTY07}, demonstrating the superiority of possessing both features.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/fig1-new-crop.pdf}
    \vspace{-2em}
    \caption{Both shape and texture are essential cues for object recognition, and biasing towards either one degenerates model performance.
    As shown above, when classifying this fur coat image, the shape-biased model is confounded by the cloth-like shape therefore predict it as a \emph{poncho}, and the texture-biased model confuses it as an \emph{Egyptian cat} because of the misleading texture. Nonetheless, our debiased model can successfully recognize it as a \emph{fur coat} by leveraging both shape and texture.
    }
    \label{fig:pred_comp1}
    \vspace{-1em}
\end{figure}

Nowadays, as popularized by Convolutional Neural Networks (CNNs) \citep{Krizhevsky2012}, the features used for object recognition are automatically learned, rather than manually designed. This change not only eases human efforts on feature engineering, but also yields much better performance on a wide range of visual benchmarks \citep{simonyan2014very,he2016deep,Girshick2014,Girshick2015,Ren2015,Long2015,Chen2014}. But interestingly, as pointed by \citet{geirhos2018imagenettrained}, the features learned by CNNs tend to bias toward either shape or texture, depending on the training dataset. 

We verify that such biased representation learning (towards either shape or texture) weakens CNNs' performance.\footnote{Biased models are acquired similar to \citet{geirhos2018imagenettrained}, see Section~\ref{sec:biased} for details.} Nonetheless, surprisingly, we also find (1) the model with shape-biased representations and the model with texture-biased representations are highly complementary to each other, \eg, they focus on completely different cues for predictions (an example is provided in Figure \ref{fig:pred_comp1}); and (2) being biased towards either cue may inevitably limit model performance, \eg, models may not be able to tell the difference between a lemon and an orange without texture information. These observations altogether deliver a promising message---biased models (\eg, ImageNet trained (texture-biased) CNNs \citep{geirhos2018imagenettrained} or (shape-biased) CNNs \citep{shi2020informative}) are improvable.

To this end, we hereby develop a shape-texture debiased neural network training framework to guide CNNs for learning better representations. Our method is a data-driven approach, which let CNNs automatically figure out how to avoid being biased towards either shape or texture from their training samples. Specifically, we apply style transfer to generate cue conflict images, which breaks the correlation between shape and texture, for augmenting the original training data.
The most important recipe of training a successful shape-texture debiased model is that we need to provide supervision from both shape and texture on these generated cue conflict images, otherwise models will remain being biased.

Experiments show that our proposed shape-texture debiased neural network training significantly improves recognition models. For example, on the challenging ImageNet dataset \citep{ILSVRC15}, our method helps ResNet-152 gain an absolute improvement of 1.2\%, achieving 79.8\% top-1 accuracy. Additionally, compared to its vanilla counterpart,  this debiased ResNet-152 shows better generalization on ImageNet-A \citep{hendrycks2019nae} (+5.2\%), ImageNet-C \citep{hendrycks2018benchmarking} (+8.3\%) and Stylized ImageNet \citep{geirhos2018imagenettrained} (+11.1\%), and stronger robustness on defending against FGSM adversarial attacker on ImageNet (+14.4\%). Our shape-texture debiased neural network training is orthogonal to other advanced data augmentation strategies, \eg, it further boosts CutMix-ResNeXt-101 \citep{yun2019cutmix} by 0.7\% on ImageNet, achieving 81.2\% top-1 accuracy.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/fig4.pdf}
    \vspace{-2em}
    \caption{Illustration of the our training pipeline for acquiring (a) a \textcolor{shapecolor}{shape}-biased model, (b) a \textcolor{texturecolor}{texture}-biased model, and (c) a shape-texture debiased model. Specifically, these models share the same training samples, \ie images with conflicting texture and shape information, generated by style transfer between two randomly selected images; but apply  
    distinct labelling strategies: in (a) \& (b), labels are determined by the images that provides \textcolor{shapecolor}{shape} (or \textcolor{texturecolor}{texture}) information in style transfer, for guiding models to learn more \textcolor{shapecolor}{shape} (or \textcolor{texturecolor}{texture}) representations; in (c), labels are jointly determined by the pair of images in style transfer, for avoiding bias in representation learning.}
    \label{fig:train_models}
    \vspace{-1em}
\end{figure}

\section{Shape/Texture Biased Neural Networks} \label{sec:biased}

The biased feature representation of CNNs mainly stems from the training dataset, \eg, \citet{geirhos2018imagenettrained} point out that models will be biased towards shape if trained on Stylized-ImageNet dataset.
Following \citet{geirhos2018imagenettrained}, we hereby present a similar training pipeline to acquire shape-biased models or texture-biased models. By evaluating these two kinds of models, we observe the necessity of possessing both shape and texture representations for CNNs to better recognize objects.


\subsection{Model Acquisition} \label{sec:biased_acq}
\paragraph{Data generation.} Similar to \citet{geirhos2018imagenettrained}, we apply images with conflicting shape and texture information as training samples to obtain shape-biased or texture-biased models. But different from  \citet{geirhos2018imagenettrained}, an important change in our cue conflict image generation procedure is that we override the original texture information \emph{with the informative texture patterns from another randomly selected image, rather than with the uninformative style of randomly selected artistic paintings}.
That being said, to create a new training sample, we need to first select a pair of images from the training set uniformly at random, and then apply style transfer to blend their shape and texture information. Such a generated example is shown in Figure \ref{fig:train_models}, \ie, the image of chimpanzee shape but with lemon texture.


\paragraph{Label assignment.}
The way of assigning labels to cue conflict images controls the bias of learned models. Without loss of generality, we show the case of learning a texture-biased model. To guide the model to attend more on texture, the labels assigned to the cue conflict images here will be exclusively based on the texture information, \eg, \emph{the image of chimpanzee shape but with lemon texture will be labelled as lemon}, shown in Figure \ref{fig:train_models}(b).  By this way, the texture information is highly related to the ``ground-truth'' while the shape information only serves as a nuisance factor during learning.  Similarly, to learn a shape-biased model, the label assignment of cue conflict images will be based on shape only, \eg, \emph{the image of chimpanzee shape but with lemon texture now will be labelled as chimpanzee}, shown in Figure \ref{fig:train_models}(a).



\subsection{Evaluation and Observation}
To reduce the computational overhead in this ablation, all models are trained and evaluated on ImageNet-200, which is a 200 classes subset of the original ImageNet, including 100,000 images (500 images per class) for training and 10,000 images (50 images per class) for validation.  Akin to \citet{geirhos2018imagenettrained}, we observe that the models with biased feature representations tend to have inferior accuracy than their vanilla counterparts. For example, our shape-biased ResNet-18 only achieves 73.9\% top-5 ImageNet-200 accuracy, which is much lower than the vanilla ResNet-18 with 88.2\% top-5 ImageNet-200 accuracy.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/fig2-crop-new.pdf}
    \vspace{-1.4em}
    \caption{The shape-biased model and the texture-biased model attend on complementary cues for predictions. We use Class Activation Mapping to visualize which image regions are attended by models. Redder regions indicates more attentions are paid by models. 
    }
    \label{fig:pred_comp2}
    \vspace{-0.3em}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/fig3-crop-compressed.pdf}
    \vspace{-2em}
    \caption{
    The shape-biased model and the texture-biased model are good/bad at classifying different object categories. We sort these object categories according to the model's corresponding top-1 accuracy, where the righter one indicates a lower accuracy achieved by the model.
    }
    \label{fig:pred_comp3}
    \vspace{-1em}
\end{figure}


Though biased representations weaken the overall classification accuracy, surprisingly, we find they are highly complementary to each other. We first visualize the attended image regions of biased models, via Class Activation Mapping~\citep{zhou2016learning}, in Figure~\ref{fig:pred_comp2}. As we can see here, the shape-biased model and the texture-biased model concentrate on different cues for predictions. For instance, on the leftmost tabby cat image, the shape-biased model mainly focuses on the cat head, while the texture-biased model mainly focuses on the lower body and the front legs of the cat. Such attention mechanisms are correlated to their learned representations---the shape-biased model extracts the shape of the cat head as an important signal for predictions, while the texture-biased model relies on the texture information of cat fur for predictions. 

As distinct cues are picked by shape-biased/texture-biased models, a more concrete observation is they are good/bad at classifying quite different object categories. 
As showed in Figure~\ref{fig:pred_comp3}, the shape-biased model is good at recognizing objects with representative shape structure like \emph{obelisk}, but is bad at recognizing objects whose shape is uninformative or almost indistinguishable from others like \emph{fur coat}. Similarly, the texture-biased model can effectively recognize objects with unique texture patterns like \emph{brain coral} but may fail to recognize objects with unpredictable texture like \emph{trolleybus} (as its side body can be painted with different advertisements). Besides, biased models may inevitably perform poorly on certain categories as insufficient cues are applied.
For examples, it is challenging to distinguish between a lemon and an orange if texture information cannot be utilized, or to distinguish between an lion and a tabby cat without shape information. 


Given the analysis above, we can conclude that biased representations limit models' recognition ability. But meanwhile, our ablation delivers a promising message---the features learned by biased models are highly complementary to each other. This observation indicates the current training framework is improvable (as the resulted models are biased towards texture \citep{geirhos2018imagenettrained} or shape \citep{shi2020informative}), and offers a potential direction for building a stronger one---we should train models to properly acquire both shape and texture feature representations. We will introduce a simple method for doing so next.

\section{Shape-Texture Debiased Neural Network Training}
Recall that when obtaining a biased model, 
the strategy of label assignment is pivot---when the labels are exclusively determined by the images that provide shape (or texture) information in style transfer, we will obtain a shape-biased (or texture-biased) model.
Therefore, to guide models for leveraging both shape and texture for predictions, we hereby propose a simple way, which is inspired by Mixup \citep{zhang2018mixup}, to softly construct labels during training.
In other words, given the one-hot label of the shape-source image $y_s$ 
and the one-hot label of the texture-source image $y_t$, the new label that we assigned to the cue conflict image is
\begin{equation}\label{eq:label_mixup}
    \widetilde{y} = \gamma * y_s + (1 - \gamma) * y_t,
\end{equation}
where $\gamma \in [0, 1]$ is a manually selected hyperparameter to control the relative importance between shape and texture. 
By ranging the shape-texture coefficient $\gamma$ from $0$ to $1$,
we obtain a path to evolve the model from being a texture-biased one (\ie, $\gamma=0$) to being a shape-biased one (\ie, $\gamma=1$).
Although the two extreme ends lead to biased models with inferior performance, we empirically show that there exist a sweet point along this interpolation path, \ie, the learned models can properly acquires both shape and texture feature representations and achieve superior performance on a wide range of image recognition benchmarks.


We name this simple method as shape-texture debiased neural network training, and illustrate the training pipeline in   Figure~\ref{fig:train_models}(c). 
It is worth to mention that, although Figure~\ref{fig:train_models} only shows the procedure of applying our method to the image classification task, this training framework is general and has the potential to be extended to other computer vision tasks, \eg, a simple showcase on semantic segmentation is presented in Section~\ref{sec:exp_seg}.





\section{Experiments}\label{sec:exp}

\subsection{Experiments Setup}
\label{sec:setup}
\paragraph{Datasets.} 
We evaluate models on ImageNet classification and PASCAL VOC semantic segmentation. \emph{ImageNet} dataset~\citep{ILSVRC15} consists of 1.2 million images for training, and 50,000 for validation, from 1,000 classes.  \emph{PASCAL VOC} 2012 segmentation dataset~\citep{pascal-voc-2012} with extra annotated images from~\citep{BharathICCV2011} involves 20 foreground object classes and one background class, including 10,582 training images and 1,449 validation images. 

Going beyond the standard benchmarks, we further evaluate models' generalization on ImageNet-A, ImageNet-C and Stylized-ImageNet, and robustness by defending against FGSM adversarial attacker on ImageNet. \emph{ImageNet-C}~\citep{hendrycks2018benchmarking} is a benckmark dataset that measures models' corruption robustness. It is constructed by applying 75 common visual corruptions to the ImageNet validation set. \emph{ImageNet-A}~\citep{hendrycks2019nae} includes 7,500 natural adversarial examples that successfully attacks unseen classifiers. These examples are much harder than original ImageNet validation images due to scene complications encountered in the long tail of scene configurations and by exploiting classifier blind spots~\citep{hendrycks2019nae}. \emph{Stylized-ImageNet}~\citep{geirhos2018imagenettrained} is a stylized version of ImageNet that constructed by re-rendering the original images by AdaIN stylizer~\citep{huang2017arbitrary}. The generated images keep the original global shape information but removes the local texture information.
\emph{FGSM} \citep{explaining2015goodfellow} is a widely used adversarial attacker to evaluate model robustness.
We set the maximum perturbation change per pixel $\epsilon = 16/255$ for FGSM. 

\paragraph{Implementation details.} We choose ResNet~\citep{he2016deep} as the default architecture. For image classification tasks, our implementation is based on the publicly available framework in PyTorch\footnote{\url{https://github.com/bearpaw/pytorch-classification}}. 
To generate cue conflict images, we follow~\citet{geirhos2018imagenettrained} to use Adaptive Instance Normalization~\citep{huang2017arbitrary} in style transfer, and set stylization coefficient $\alpha = 0.5$. Importantly, to increase the diversity of training samples, we generate these cue conflict images on-the-fly during training. We choose the shape-texture coefficient $\gamma = 0.8$ when assigning labels.  

When training shape-biased, texture-biased and our shape-texture debiased models, we always apply the auxiliary batch normalization (BN) design \citep{xie2019adversarial,Xie2020intriguing,chen2021robust} to bridge the domain gap between the original data and the augmented data, \ie, the main BN is exclusively running on original ImageNet images and the auxiliary BN is exclusively running on cue conflict images. We follow \citet{xie2019adversarial} to always apply the main BN for performance evaluation. Besides, since our biased models and debiased models are all trained with both the original data and the augmented data (\ie, $2\times$ data are used in training), we also consider a stronger baseline (\ie, $2\times$ epochs training) which doubles the schedule of the vanilla training baseline, for the purpose of matching the total training cost.

\begin{table}[t]
\renewcommand\arraystretch{0.8}
\small
\centering
\begin{tabular}{l|cc|cc|cH}
\toprule
           & \scshape{Vanilla} & \scshape{$2\times$Epochs}  & \scshape{S-biased} & \scshape{T-biased} & \scshape{Debiased} & \scshape{$2\times$Epochs} \\
\midrule
ResNet-50  & 76.4    & 76.4 {\footnotesize \color{Highlight} \textbf{(+0.0)}}   & 76.2 {\footnotesize \color{red} \textbf{(-0.2)}}         & 75.3 {\footnotesize \color{red} \textbf{(-1.1)}}              & 76.9 {\footnotesize \color{Highlight} \textbf{(+0.5)}} & 78.0 {\footnotesize \color{Highlight} \textbf{(+1.6)}} \\
ResNet-101 & 78.0    & 78.0 {\footnotesize \color{Highlight} \textbf{(+0.0)}}   & 78.0 {\footnotesize \color{red} \textbf{(-0.0)}}            & 77.4 {\footnotesize \color{red} \textbf{(-0.6)}}              & 78.9 {\footnotesize \color{Highlight} \textbf{(+0.9)}} & -   \\
ResNet-152 & 78.6    & 79.1 {\footnotesize \color{Highlight} \textbf{(+0.5)}}   & 78.6 {\footnotesize \color{red} \textbf{(-0.0)}}         & 78.1 {\footnotesize \color{red} \textbf{(-0.5)}}              & 79.8 {\footnotesize \color{Highlight} \textbf{(+1.2)}} & - \\
\bottomrule
\end{tabular}
\vspace{-0.9em}
\caption{The performance of the vanilla training, the shape-biased (S-biased) training, the texture-biased (T-biased) training, and our shape-texture debiased training on ImageNet. For all ResNet models, our debiased training shows the best performance among others.}
\label{tab:imagenet_acc}
\vspace{-0.8em}
\end{table}


\begin{table}[t]
\renewcommand\arraystretch{0.8}
\small
\centering
\begin{tabular}{l|HHcccc}
\toprule
& FLOPs / & \scshape{IN} & \scshape{IN-A} & \scshape{IN-C} & \scshape{S-IN} & \scshape{FGSM} \\ 
& \#Params & {\footnotesize Acc. \color{red}$\mathbf{\uparrow}$} & {\footnotesize Acc. \color{red}$\mathbf{\uparrow}$} & {\footnotesize mCE \color{red}$\mathbf{\downarrow}$} & {\footnotesize Acc. \color{red}$\mathbf{\uparrow}$} & {\footnotesize Acc. \color{red}$\mathbf{\uparrow}$} \\
\midrule
ResNet-50     &  4G / 98M &    76.4 & 2.0      &   75.0         &      7.4             &   17.1   \\

+ Debiased       & 4G / 98M &  76.9 {\footnotesize \color{Highlight} \textbf{(+0.5)}} &   3.5 {\footnotesize \color{Highlight} \textbf{(+1.5)}}     &    67.5 {\footnotesize \color{Highlight} \textbf{(-7.5)}}        &       17.4 {\footnotesize \color{Highlight} \textbf{(+10.0)}}            &   27.4 {\footnotesize \color{Highlight} \textbf{(+10.3)}}   \\
\midrule
ResNet-101      & 8G / 170M &   78.0 &    5.6    &       69.8     &           9.9        &   23.1   \\
+ Debiased          &   8G / 170M & 78.9 {\footnotesize \color{Highlight} \textbf{(+0.9)}} &    9.1 {\footnotesize \color{Highlight} \textbf{(+3.5)}}    &       62.2 {\footnotesize \color{Highlight} \textbf{(-7.6)}}     &            22.0 {\footnotesize \color{Highlight} \textbf{(+12.1)}}       &   34.4 {\footnotesize \color{Highlight} \textbf{(+11.3)}}   \\ \midrule
ResNet-152      &  11G / 230M &  78.6 & 7.4        &   67.2         &       11.3            &   25.2   \\
+ Debiased   &  11G / 230M &  79.8 {\footnotesize \color{Highlight} \textbf{(+1.2)}} & 12.6 {\footnotesize \color{Highlight} \textbf{(+5.2)}}        &   58.9 {\footnotesize \color{Highlight} \textbf{(-8.3)}}         &      22.4 {\footnotesize \color{Highlight} \textbf{(+11.1)}}             &   39.6 {\footnotesize \color{Highlight} \textbf{(+14.4)}}   \\
\bottomrule
\end{tabular}
\vspace{-0.9em}
\caption{The model robustness on ImageNet-A (IN-A), ImageNet-C (IN-C), Stylized-ImageNet (S-IN), and on defending against FGSM adversarial attacker on ImageNet. Our shape-texture debiased neural network training significantly boosts the model robustness over the vanilla training baseline.}
\label{tab:imagenet_robustness}
\vspace{-1.6em}
\end{table}


\begin{table}[t]
\renewcommand\arraystretch{0.8}
\small
\centering
\begin{tabular}{l|Hccccc}
\toprule
         & FLOPs / & \scshape{IN} & \scshape{IN-A} & \scshape{IN-C} & \scshape{S-IN} & \scshape{FGSM} \\ 
& \#Params & {\footnotesize Acc. \color{red}$\mathbf{\uparrow}$} & {\footnotesize Acc. \color{red}$\mathbf{\uparrow}$} & {\footnotesize mCE \color{red}$\mathbf{\downarrow}$} & {\footnotesize Acc. \color{red}$\mathbf{\uparrow}$} & {\footnotesize Acc. \color{red}$\mathbf{\uparrow}$} \\
\midrule
ResNet-50     &  4G / 98M &    76.4 & 2.0      &   75.0         &      ~~7.4             &   17.1   \\
\midrule
CutMix + MoEx~\citep{li2020feature}       & 4G / 98M &  \textcolor{Highlight}{79.0}  &   \textcolor{Highlight}{8.0}      &    \textcolor{gray}{74.8}         &       \textbf{\textcolor{red}{5.0}}             &   \textcolor{Highlight}{41.0}    \\
DeepAugment + AugMix \citep{hendrycks2020many}       & 4G / 98M &  \textbf{\textcolor{red}{75.8}}  &   \textcolor{Highlight}{3.9}      &    \textcolor{Highlight}{53.6}         &       \textcolor{Highlight}{21.2}             &   \textcolor{gray}{18.8}    \\
SIN~\citep{geirhos2018imagenettrained}       & 4G / 98M &  \textbf{\textcolor{red}{60.2}}  &   \textcolor{gray}{2.4}      &    \textbf{\textcolor{red}{77.3}}         &       \textcolor{Highlight}{56.2}             &   \textbf{\textcolor{red}{5.6}}    \\
\textbf{Shape-Texture Debiased Training (ours)}       & 4G / 98M &  \textcolor{Highlight}{76.9}  &   \textcolor{Highlight}{3.5}      &    \textcolor{Highlight}{67.5}         &       \textcolor{Highlight}{17.4}             &   \textcolor{Highlight}{27.4}    \\

\bottomrule
\end{tabular}
\vspace{-0.9em}
\caption{Compare with state-of-the-art methods using ResNet-50 on ImageNet (IN), ImageNet-A (IN-A), ImageNet-C (IN-C), Stylized-ImageNet (S-IN), and on defending against FGSM on ImageNet. We use \textcolor{Highlight}{green} to denote significant improvement, \textcolor{red}{red} to denote performance drop, and \textcolor{gray}{gray} to denote similar performance. We observe our shape-texture debiased training is the \textbf{only} method that successfully leads to improvements over the vanilla baseline on all benchmarks.}
\vspace{-0.5em}
\label{tab:compare_sota}
\end{table}


\subsection{Results}\label{sec:main_results}

\paragraph{Model accuracy.} 
Table~\ref{tab:imagenet_acc} shows the results on ImageNet. 
For all ResNet models, the proposed shape-texture debiased neural network training consistently outperforms the vanilla training baseline. For example, it helps ResNet-50 achieve 76.9\% top-1 accuracy, beating its vanilla counterpart by 0.5\%. Our method works better for larger models, \eg, it further improves the vanilla ResNet-152 by 1.2\%, achieving 79.8\% top-1 accuracy. 

We then compare our shape-texture debiased training to the 2$\times$ epochs training baseline. We find that simply doubling the schedule of the vanilla training baseline cannot effectively lead to improvements like ours. For examples, compared to the vanilla ResNet-101, this 2$\times$ epochs training fails to provide additional improvements, while ours furthers the top-1 accuracy by 1.0\%. This result suggests that it is non-trivial to improve performance even if more computational budgets are given.


Lastly, we compare ours to the biased training methods. Though the only difference between our method and the biased training methods is the strategy of label assignment (as shown in Figure~\ref{fig:train_models}), it imperatively affects model performance. For example, compared to the vanilla baseline, both the shape-biased training and the texture-biased training fail to improve (sometimes even slightly hurt) the model accuracy, while our shape-texture debiased neural network training successfully leads to consistent and substantial accuracy improvements.

\paragraph{Model robustness.}
Next, we evaluate models' generalization on ImageNet-A, ImageNet-C and Stylized-ImageNet, and robustness on defending against FGSM on ImageNet. 
We note these tasks are much more challenging than the original ImageNet classification, \eg, the ImageNet trained ResNet-50 only achieves $2.0\%$ accuracy  on ImageNet-A,  $75.0\%$ mCE on ImageNet-C,  $7.4\%$ accuracy on Stylized-ImageNet, and $17.1\%$ accuracy on defending against FGSM adversarial attacker.
As shown in Table~\ref{tab:imagenet_robustness}, our shape-texture debiased neural network training beats the vanilla training baseline by a large margin on all tasks for all ResNet models. 
For example, it substantially boosts ResNet-152's performance on ImageNet-A ($+5.2\%$, from 7.4\% to 12.6\%), ImageNet-C ($-8.3\%$, from 67.2\% to 58.9\%, the lower the better) and Stylized-ImageNet ($+11.1\%$, from 11.3\% to 22.4\%), and on defending against FGSM on ImageNet ($+14.4\%$, from 25.2\% to 39.6\%). These results altogether suggest that our shape-texture debiased neural network training is an effective way to mitigate the issue of shortcut learning \citep{geirhos2020shortcut}. 


\paragraph{Comparing to SoTAs.}
We further compare our shape-texture debiased model with the SoTA on ImageNet and ImageNet-A (CutMix + MoEx \citep{li2020feature}), the SoTA on ImageNet-C (DeepAugment + AugMix \citep{hendrycks2020many}), and the SoTA on Stylized-ImageNet (SIN \citep{geirhos2018imagenettrained}). 
Interestingly, we note the improvements of all these SoTAs are not consistent across different benchmarks. For example, as shown in Table \ref{tab:compare_sota}, SIN significantly improves the results on Stylized-ImageNet, but at the cost of huge performance drop on ImageNet (-16.2\%) and ImageNet-C (-2.3\%). Our shape-texture debiased training stands as the only method that can improve the vanilla training baseline holistically.


\subsection{Ablations}





\begin{table}[]
\renewcommand\arraystretch{0.8}
\small
\centering
\begin{tabular}{l|cccc}
\toprule
Datasets & \scshape{Vanilla} & \scshape{S-biased} & \scshape{T-biased} & \scshape{Debiased} \\
\midrule
ImageNet-Sketch & 23.8 & \ul{27.9} & 24.3 & \textbf{28.4} \\
ImageNet-R & 36.2 & \ul{40.6} & 36.7 & \textbf{40.8} \\
\midrule
Kylberg Texture & 99.5 & 99.1 & \textbf{99.6} & \ul{99.5} \\
Flicker Material & 74.6 & 73.3 & \textbf{79.2} & \ul{75.8} \\
\bottomrule
\end{tabular}
\vspace{-0.9em}
\caption{The performance comparison between Vanilla, Shape-biased, Texture-biased, and Shape-Texture Debiased models on ImageNet-Sketch, ImageNet-R, Kylberg Texture, and Flicker Material datasets. We note the shape-biased and the shape-texture debiased models perform better on shape datasets (ImageNet-Sketch and ImageNet-R); the texture-biased and the shape-texture debiased models perform better on texture datasets (Kylberg Texture and Flicker Material).}
\label{tab:shape_or_texture_dataset}
\vspace{-1.3em}
\end{table}

\paragraph{Comparing to model ensembles.}
An alternative but na\"ive way for obtaining the model with both shape and texture information is to ensemble a shape-biased model and a texture-biased model. We note this ensemble strategy yields a model of on-par performance with our shape-texture debiased model on ImageNet (77.2\% \vs 76.9\%). Nonetheless, interestingly, 
when measuring model robustness, such model ensemble strategy is inferior than ours. For example, compared to our proposed debiased training, this ensemble strategy is 1.5\% worse on ImageNet-A (2.0\% \vs 3.5\%), 1.1\% worse on ImageNet-C (68.6 mCE \vs 67.5 mCE), 1.1\% worse on Stylized-ImageNet (16.3\% \vs 17.4\%), and 7.0\% worse on defending against FGSM (20.4\% \vs 27.4\%). Moreover, due to model  ensemble, this strategy is 2$\times$ expensive at the inference stage. These evidences clearly demonstrate the effectiveness and efficiency of the proposed shape-texture debiased training.



\paragraph{Does our method help models to learn debiased shape-texture representations?}
Here we take a close look at whether our method indeed prevents models from being biased toward shape or texture during learning. We evaluate models in Section~\ref{sec:main_results} on two kinds of datasets: (1) ImageNet-Sketch dataset~\citep{wang2019learning} and ImageNet-R~\citep{hendrycks2020many} for examining how well models can capture shape; and
(2) Kylberg Texture dataset~\citep{kylberg2011kylberg} and Flicker Material dataset~\citep{Sharan-JoV-14} for examining how well models can capture texture. Specifically, since object categories from two texture datasets are not compatible with that from ImageNet dataset, we retrain the last fc-layer (while keeping all other layers untouched) of all models on Kylberg Texture dataset or Flicker Material dataset for 5 epochs. The results are shown in Table~\ref{tab:shape_or_texture_dataset}.

We first analyze results on ImageNet-Sketch dataset. We observe our shape-texture debiased models are as good as the shape-biased models, and significantly outperforms the texture-biased models and the vanilla training models. For instance, using ResNet-50, our shape-texture debiased training and shape-biased training achieve 28.4\% top-1 accuracy and 27.9\% top-1 accuracy, while texture-biased training and vanilla training only get 24.3\% top-1 accuracy and 23.8\% top-1 accuracy. A similar observation can be seen from ImageNet-R. These results support that our method helps models acquire stronger shape representations than the vanilla training. 

We next analyze results on Kylberg Texture dataset. Similarly, we observe that our debiased model are comparable to the texture-biased model and the vanilla training model, and get better performance than the shape-biased model. On Flicker Material dataset, we observe that our debiased models are better than the vanilla training model and the shape-biased model. This phenomenon suggests texture information is effectively caught by our shape-texture debiased training.
As a side note, it is expected that vanilla training are better than shape-biased training on these texture datasets, as \citet{geirhos2018imagenettrained} point out that ImageNet trained models (\ie, vanilla training) also tend to be biased towards texture.

With the analysis above, we conclude that, compared to vanilla training, our shape-texture debiased training successfully helps networks effectively acquire both shape and texture representations.



\paragraph{Combining with other data augmentation methods.}
Our shape-texture debiased neural network training can be viewed as a data augmentation method, which trains models on cue conflict images. Nonetheless, our method specifically guides the model to learn debiased shape and texture representations, which could potentially serve as a complementary feature to other data augmentation methods. To validate this argument, we train models using a combination of our method and an existing data augmentation method (\ie, Mixup~\citep{zhang2018mixup} or CutMix~\citep{yun2019cutmix}). 


We choose ResNeXt-101 \citep{xie2017aggregated} as the backbone network, which reports the best top-1 ImageNet accuracy in both the Mixup paper, \ie, 79.9\%, and the CutMix paper, \ie, 80.5\%. 
Though building upon very strong baselines, our shape-texture debiased neural network training still leads to substantial improvements, \eg, it furthers ResNeXt-101-Mixup's accuracy to 80.5\% (+0.6\%), and ResNeXt-101-CutMix's accuracy to 81.2\% (+0.7\%). Meanwhile, models' generalization also get greatly improved. For example, by combining CutMix and our method, ResNeXt-101 gets additional improvements on ImageNet-A (+1.4\%), ImageNet-C (-5.9\%, the lower the better) and Stylized ImageNet (+7.5\%).
These results support that our shape-texture debiased neural network training
is compatible with existing data augmentation methods.





\paragraph{Shape-texture coefficient $\gamma$.}
We set $\gamma=0.8$ in our shape-texture debiased training. This value is found via the grid search over ImageNet-200 using ResNet-18. We now ablate its sensitivity on ImageNet using ResNet-50, where $\gamma$ is linearly interpolated between 0.0 and 1.0. By increasing the value of $\gamma$, we observe that the corresponding accuracy on ImageNet first monotonically goes up, and then monotonically goes down. The sweet point can be reached by setting $\gamma=0.7$, where ResNet-50 achieves 77.0\% top-1 ImageNet accuracy. Besides, we note that by setting $\gamma \in [0.5, 0.9]$ can always lead to performance improvements over the vanilla baseline. These results demonstrate the robustness of our shape-texture debiased neural network training w.r.t. the coefficient $\gamma$.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figs/fig-seg-crop-compressed.pdf}
    \vspace{-2em}
    \caption{Illustration of the data preparation pipeline of our shape-texture debiased neural network training on the semantic segmentation task.}
    \vspace{-1em}
    \label{fig:seg}
\end{figure}

\subsection{Semantic Segmentation Results} \label{sec:exp_seg}
We extend our shape-texture debiased neural network training to the segmentation task. 
We select DeepLabv3-ResNet-101 \citep{chen2017rethinking} as our backbone.
To better incorporate our method with the segmentation task, the following changes are made when generating cue conflict images: (1) unlike in the classification task where the whole image is used as the texture source, we use a specific object (which can cropped from the background using the segmentation ground-truth) to provide texture information in style transfer; (2) when composing the soft label for the cue conflict image, we set the label mask from texture source as the full image (since the pattern from the texture source will fill the whole image after style transfer); 
and (3) we
set stylization coefficient $\alpha=0.2$ and shape-texture coefficient $\gamma=0.95$ to prevent object boundaries from being overly blurred in style transfer. Figure~\ref{fig:seg} shows an illustration of our data preparation pipeline.

\paragraph{Results.} Our shape-texture debiased training can also effectively improve segmentation models. For example, our method helps DeepLabv3-ResNet-101 achieve 77.6\% mIOU, significantly beating its vanilla counterpart by 1.1\%. Our method still shows advantages when compared to the $2\times$ epochs training baseline. Doubling the learning schedule of the vanilla training can only lead to an improvement of 0.2\%, which is still 0.9\% worse than our shape-texture debiased training.
These results demonstrate the potential of our methods in helping recognition tasks in general.

\section{Related Work}

\paragraph{Data augmentation.}
Data augmentation is essential for the success of deep learning~\citep{lecun1998gradient,Krizhevsky2012,simonyan2014very,zhong2017random,
cubuk2019autoaugment,lim2019fast,cubuk2020randaugment}.
Our shape-texture debiased neural network training is related to a specific family of data augmentation, called Mixup \citep{zhang2018mixup}, which blends pairs of images and their labels in a convex manner, either at pixel-level \citep{zhang2018mixup,yun2019cutmix} or feature-level \citep{verma2019manifold,li2020feature}.
Our method can be interpreted as a special instantiation of Mixup which blends pairs of images at the abstraction level---images' texture information and shape information are mixed.  Our method successfully guides CNNs to learn better shape and texture representations, which is an important but missing piece in existing data argumentation methods.

\paragraph{Style transfer.} Style transfer, closely related to texture synthesis and transfer, means generating a stylized image by combining a shape-source image and a texture-source image~\citep{efros1999texture,efros2001image,elad2017style}. The seminal work ~\citep{gatys2016image} demonstrate impressive style transfer results by matching feature statistics in convolutional layers of a CNN. Later follow-ups further improve the generation quality and speed
\citep{huang2017arbitrary,chen2016fast,ghiasi2017exploring,li2017universal}. 
In this work, we follow \citet{geirhos2018imagenettrained} to use AdaIN \citep{huang2017arbitrary} to generate stylized images. Nonetheless, instead of applying style transfer between an image and an artistic paintings as in \citet{geirhos2018imagenettrained}, we directly apply style transfer on a pair of images to generate cue conflict images.
This change is vital as it enables us to provide supervisions from both shape and texture during training.

\section{Conclusion}
There is a long-time debate about which cue dominates the object recognition. By carefully ablate the shape-biased model and the texture-biased model, we found though biased feature representations lead to performance degradation, they are complementary to each other and are both necessary for image recognition.
To this end, we propose shape-texture debiased neural network training for guiding CNNs to learn better feature representations. The key in our method is that we should not only augment training set with cue conflict images, but also provide supervisions from both shape and texture. 
We empirically demonstrate the advantages of our shape-texture debiased neural network training on boosting both accuracy and robustness. Our method is conceptually simple and is generalizable to different image recognition tasks. We hope our work will shed light on understanding and improving convolutional neural networks.

\section*{Acknowledgement}
This project is partially supported by ONR N00014-18-1-2119 and ONR N00014-20-1-2206. Cihang Xie is supported by the Facebook PhD Fellowship and a gift grant from Open Philanthropy. Yingwei Li thanks Zhiwen Wang for suggestions on figures.

\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\newpage


\end{document}
