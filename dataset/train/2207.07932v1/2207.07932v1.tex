
SuperRetina is a deep neural network that takes as input a (gray-scale) $h\times w$ retinal image $I$, detects and describes keypoints in the given image with high repeatability and reliability in a single forward pass. We describe the network architecture in  \cref{ssec:network}, followed by the proposed  training algorithms in  \cref{ssec:training}. The use of SuperRetina for RIM is given in \cref{ssec:tasks}.

\subsection{Network Architecture} \label{ssec:network}

We adapt the SuperPoint network. Conceptually, our network consists of an encoder to extract down-sized feature maps $F$ from the given image $I$. The feature map is then fed in parallel into two decoders, one for keypoint detection and the other for keypoint description, which we term Det-Decoder and Des-Decoder, respectively. The Det-Decoder generates a full-sized probabilistic map $P$, where $P_{i,j}$ indicates the probability of a specific pixel being a keypoint, $i=1,\ldots,h$ and $j=1,\ldots,w$. The Des-Decoder produces a $h \times w \times d$ tensor $D$, where $D_{i,j}$ denotes a $d$-dimensional descriptor. Note that in the inference stage, Non-Maximum Suppression (NMS) is applied on $P$ to obtain a binary mask $\widehat{P}$ as the final detection result.  We formalize the above process as follows:
\begin{equation} \label{eq:general}
\left\{ \begin{array}{ll}
 F & \leftarrow \mbox{Encoder}(I), \\
 P & \leftarrow \mbox{Det-Decoder}(F), \\
 D & \leftarrow \mbox{Des-Decoder}(F), \\
 \widehat{P} & \leftarrow \mbox{NMS}(P). \\
\end{array}
\right.
\end{equation}
As illustrated in  \cref{fig:network}, we modify both Det-Decoder and Des-Decoder for RIM.

\textbf{U-Net as Det-Decoder}. 
Effectively capturing low-level patterns such as crossover and bifurcation on the vascular tree is crucial for detecting retinal keypoints in a reliable and repeatable manner. We therefore opt to use U-Net \cite{ronneberger2015u}, originally developed for biomedical image segmentation with its novel design of re-using varied levels of features from the encoder in the decoder by skip connections. In order to support high-resolution input, our encoder is relatively shallow, with a conv layer to generate low-level full-sized feature maps, followed by three conv blocks, each consisting of two conv layers, $2\times2$ max pooling and ReLU. Consequently, the high-level feature maps $F$ have a size of $\frac{h}{8} \times \frac{w}{8} \times 128$.  In order to recover full-sized feature maps, our Det-Decoder uses three conv blocks, each having two conv layers, followed by bilinear upsampling\footnote{We use bilinear upsampling, as transposed convolutions originally used by U-Net are  computationally more expensive, and introduce unwanted checkerboard artifact\cite{M2U-Net}.}, ReLU and concatenation to merge the corresponding feature maps from the encoder. Lastly, a conv. block consisting of three conv. layers and one sigmoid activation is applied on the full-sized feature maps to produce the  detection map $P$. 




\textbf{Full-sized Des-Decoder}.
Different from SuperPoint which computes its description loss on a down-sized tensor of $\frac{h}{8} \times \frac{w}{8} \times d$, we target optimizing the descriptors on the full size of $h \times w$, where each pixel is associated with a $d$-dimensional descriptor. Naturally, such dense results are obtained by interpolation, meaning gradient correlation between each keypoint and its neighborhood during backpropagation. Enlarging the neighborhood enhances the correlation, and is thus helpful for training with a larger receptive field \cite{chen2017deeplab}. In that regard, our Des-Decoder first downsizes $F$ to more compact feature maps of $\frac{h}{16}\times \frac{w}{16}\times d$, and then uses an upsampling block (using transposed conv) to generate the full-sized descriptor tensor $D$ of $h \times w \times d$. All the descriptors are $l_2$-normalized.

Our network adaption may seem to be conceptually trivial. Note that producing a full-sized descriptor tensor is computationally prohibitive for a pixel-based description loss as used in SuperPoint and NCNet. A keypoint-based description loss is needed. Nonetheless, keypoint-based training is nontrivial, as inadequate annotations will make the network quickly converge to a local, suboptimal solution. However, having many training images adequately labeled is known to be expensive. To tackle the practical challenge, we develop a semi-supervised training algorithm that works with a small amount of incompletely labeled images.


\newlength{\twosubht}
\newsavebox{\twosubbox}

\begin{figure}[htb!]
\sbox\twosubbox{\resizebox{\dimexpr\textwidth-1em}{!}{\includegraphics[height=3cm]{figures/fig_overall_a.pdf}
    \includegraphics[height=3cm]{figures/fig_overall_b.pdf}
  }}
\setlength{\twosubht}{\ht\twosubbox}



\centering

\subcaptionbox{\label{fig:pke} Training with PKE}{\includegraphics[height=\twosubht]{figures/fig_overall_a.pdf}} \quad
\subcaptionbox{\label{fig:network} Network architecture}{\includegraphics[height=\twosubht]{figures/fig_overall_b.pdf}}

\sbox\twosubbox{\resizebox{\dimexpr\textwidth-1em}{!}{\includegraphics[height=3cm]{figures/fig_overall_c.pdf}
    \includegraphics[height=3cm]{figures/fig_overall_d.pdf}
  }}
\setlength{\twosubht}{\ht\twosubbox}

\centering

\subcaptionbox{\label{fig:double-matching} Double-matching strategy}{\includegraphics[height=\twosubht]{figures/fig_overall_c.pdf}}
\quad
\subcaptionbox{\label{fig:pke-demo} Newly added keypoints $S_t$ (red dots)}{\includegraphics[height=\twosubht]{figures/fig_overall_d.pdf}}
\caption{\textbf{Proposed SuperRetina}. Green/orange markers in (c) indicate genuine/fake keypoints. Blue/red dots in (d) indicate the initial keypoints (auto-detected by PBO \cite{oinonen2010identity}) / iteratively detected keypoints for training.}

\label{fig:training-detector}

\end{figure}
 
\subsection{Training Algorithm}  \label{ssec:training}

\textbf{Semi-Supervised Training of Det-Decoder}.
We formulate keypoint detection as a pixel-level binary classification task \cite{detone2018superpoint, truong2019glampoints}. Due to the sparseness and incompleteness of manually labeled keypoints,  training Det-Decoder using a common binary cross-entropy (CE) loss is difficult. To attack the sparseness (and the resultant class imbalance) issue, we leverage two tactics. The first tactic, borrowed from Pose Estimation \cite{wei2016convolutional}, is to convert the binary labels $Y$ to soft labels $\tilde{Y}$ by 2D Gaussian blur, where each keypoint is a peak surrounded by neighbors with their values decaying exponentially. The second tactic is to use the Dice loss \cite{milletari2016v}, found to be  more effective than the weighted CE loss and the Focal loss to handle extreme class imbalance \cite{icpr20-lesion-net}. The Dice-based classification loss $\ell_{clf}$ per image is computed as 
\begin{equation} \label{eq:clfloss}
\ell_{clf}(I; Y)=1 - \frac{2 \cdot \sum_{i,j} {(P  \circ \tilde{Y})_{i,j}}}{\sum_{i,j}{ (P \circ P)_{i,j}}+\sum_{i,j}{(\tilde{Y} \circ \tilde{Y})_{i,j}}},
\end{equation}
where $\circ$ denotes element-wise multiplication.

To attack the incompleteness issue, we propose \textbf{Progressive Keypoint Expansion} (PKE). The basic idea is to progressively expand the labeled keypoint set $Y$ by adding novel and reliable keypoints found by  Det-Detector, which itself is continuously improving after each epoch. To distinguish from such a dynamic $Y$, for each training image we now use $Y_0$ to indicate its initial keypoints, and $S_t$ to denote keypoints detected at the $t$-th epoch, $t=1, 2, \ldots$. We obtain the expanded keypoint set $Y_t$ as $Y_0 \cup S_t$, which \xredit{is used for training at the $t$-th} epoch. 

As $S_t$ is auto-constructed, improper keypoints are inevitable, in particular at the early stage when the Det-Decoder is relatively weak. Given that a good detector shall detect the same keypoint under different viewpoints and scales, GLAMpoints performs a geometric matching to identify keypoints that can be repeatedly detected from a given image and its projective transformations. We improve over GLAMpoints by adding a content-based matching, making it a \emph{double}-matching strategy. As \cref{fig:double-matching} shows, suppose a keypoint detected in a non-vascular area in $I$ (orange circle) has a geometrically matched keypoint (orange square) in $I'=\mathcal{H}(I)$, with $\mathcal{H}$ as a specific homography. Non-vascular areas lack specificity in visual appearance, meaning descriptors extracted such areas are relatively close. Hence, even if the square is the best match to the circle in the descriptor space, it is not sufficiently different from the second-best match to pass Lowe's ratio test \cite{lowe2004distinctive}. Double matching is thus crucial. 

\begin{figure}[htb!]
  \centering
\includegraphics[width=0.65\linewidth]{figures/fig_pke.pdf}
   \caption{\textbf{Key dataflow within the PKE module}.}
   \label{fig:pke-flow}
\end{figure} 
As illustrated in \cref{fig:pke-flow}, the PKE module works as follows: \\
1) Construct $I^\prime$, a geometric mapping of $I$, using $I^\prime = \mathcal{H}(I)$.\\
2) Feed $I^\prime$ to SuperRetina to obtain its probabilistic detection map $P^\prime$. The inverse projection of the map \wrt $I$ is obtained as  $P^\prime_*=\mathcal{H}^{-1}(P^{\prime})$.  \\
3) Geometric matching: For each point $(i,j)$ in $\widehat{P}$, add it to $S_t$ if ${(P^\prime_*)}_{i,j}>0.5$. \\
4) Content-based matching: For each point $(i,j)$ in $S_t$, we obtain its descriptor by directly sampling the output of the Des-Decoder, resulting in a descriptor set $D_t$. Similarly, we extract $D^\prime_t$ from $I^\prime$ based on $\mathcal{H}(S_t)$. Each descriptor in $D_t$ is used as a query to perform the nearest neighbor search on $D^\prime_t$. A point $(i,j)$ will be preserved in $S_t$, only if its spatial correspondence $(i^\prime,j^\prime)$ passes the ratio test.

The above procedure allows us to progressively find new and reliable keypoints, see \cref{fig:pke-demo}. Moreover, in order to improve the holistic consistency between the detection maps of $I$ and its geometric transformation $I^\prime$, we additionally compute the Dice loss between $P$ and $P^\prime_*$, termed as $\ell_{geo}(I,\mathcal{H})$.  Our detection loss $\ell_{det}$ conditioned on  $Y_t$ and $\mathcal{H}$ is computed as 
\begin{equation} \label{eq:det-loss}
\ell_{det}(I; Y_t, \mathcal{H}) = \ell_{clf}(I; Y_t) + \ell_{geo}(I,\mathcal{H}).    
\end{equation}


\textbf{Self-Supervised Training of Des-Decoder}. 
Ideally, the output of the Des-Decoder shall be invariant to homography. That is, for each keypoint $(i,j)$ detected in $I$, its descriptor shall be identical to the descriptor extracted at the corresponding location $(i^\prime, j^\prime)$ in $I^\prime$. To avoid a trivial solution of yielding a constant descriptor, we choose to optimize a triplet loss \cite{schroff2015facenet} such that the distance between paired keypoints shall be smaller than the distance between unpaired keypoints. Recall that  keypoints are automatically provided by the Det-Decoder, our Des-Decoder is trained in a fully self-supervised manner. Such a property lets the Des-Decoder learn from unlabeled data with ease.

Feeding $I$ and $I^\prime$ separately into SuperRetina allows us to access their full-sized descriptor tensors $D$ and $D^\prime$. For each element $(i,j)$ in the non-maximum suppressed keypoint set $\widehat{P}$, let $D_{i,j}$ be its descriptor. As $(i,j)$ and $(i^\prime, j^\prime)$ shall be paired, the distance of their descriptors, denoted as $\phi_{i,j}$, has to be reduced. With  $(i^\prime, j^\prime)$ excluded, we use $\phi^{rand}_{i,j}$ to indicate the descriptor distance between $(i,j)$ and a point chosen randomly from $\mathcal{H}(\widehat{P})$. Let $\phi^{hard}_{i,j}$ be  the minimal distance. 
We argue that using $\phi^{rand}_{i,j}$ or $\phi^{hard}_{i,j}$  alone as the negative term in the triplet loss is problematic. As the requirement of $\phi_{i,j}<\phi^{rand}_{i,j}$ is relatively easy to fulfill, using $\phi^{rand}_{i,j}$ alone is inadequate to obtain descriptors of good discrimination. Meanwhile, as the network at its early training stage lacks ability to produce good descriptors, using $\phi^{hard}_{i,j}$ exclusively will make the network hard to train. To resolve the issue, we propose a simple  trick by using the mean of  $\phi^{rand}_{i,j}$ and $\phi^{hard}_{i,j}$ as the negative term. Our description loss $\ell_{des}$ is thus defined as
\begin{equation} \label{eq:des-loss}
\ell_{des}(I; \mathcal{H})= \sum_{(i,j) \in \widehat{P}} \max (0, m+\phi_{i,j} -\frac{1}{2}(\phi^{rand}_{i,j}+\phi^{hard}_{i,j})),
\end{equation}
where $m>0$ is a hyper-parameter controlling the margin. Note that $\ell_{des}$ has a quadratic time complexity \wrt the size of $\widehat{P}$, which is much smaller than $h \times w$. Hence, our description loss is much more efficient than its counterpart in SuperPoint, which is quadratic  \wrt   $h \times w$. As such, given the same amount of GPU resources, SuperRetina can be trained on higher-resolution images.

While we describe the training algorithms of Det-Decoder and Des-Decoder separately, they are jointly trained by minimizing the following combined loss:
\begin{equation}
    \ell(I; Y_t, \mathcal{H}) = \ell_{det}(I; Y_t,  \mathcal{H}) + \ell_{des}(I; \mathcal{H}),
\end{equation}
where the homography $\mathcal{H}$ varies per mini-batch.  


\subsection{Keypoint-based Retinal Image Matching} \label{ssec:tasks}

Once trained, the use of SuperRetina for RIM is simple. Given a query image $I_q$ and a reference image $I_r$, we feed them separately into SuperRetina to obtain their keypoint probabilistic maps $P_q$ and $P_r$ and associated descriptor tensors $D_q$ and $D_r$. NMS is performed on $P_q$ and $P_r$ to obtain keypoints as $Kp_q$ and $Kp_r$. Recall that $D_q$ and $D_r$ are full-sized, so the corresponding descriptors $desc_q$ and $desc_r$ are fetched directly from the two tensors. Initial matches between $Kp_q$ and $Kp_r$ are obtained by an OpenCV brute-force matcher. The homography matrix $\mathcal{H}$ are then computed using the matched pairs to register $q$ \wrt $r$. As for identity verification, $\mathcal{H}$ is reused to remove outliers. The two images are accepted as \emph{genuine}, \ie from the same eye, if the number of matched points exceeds a predetermined threshold, and \emph{impostor} otherwise. The above process can be written in just a few lines of Python-style code, see the supplement. 

