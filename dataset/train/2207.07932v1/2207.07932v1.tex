
SuperRetina is a deep neural network that takes as input a (gray-scale)  retinal image , detects and describes keypoints in the given image with high repeatability and reliability in a single forward pass. We describe the network architecture in  \cref{ssec:network}, followed by the proposed  training algorithms in  \cref{ssec:training}. The use of SuperRetina for RIM is given in \cref{ssec:tasks}.

\subsection{Network Architecture} \label{ssec:network}

We adapt the SuperPoint network. Conceptually, our network consists of an encoder to extract down-sized feature maps  from the given image . The feature map is then fed in parallel into two decoders, one for keypoint detection and the other for keypoint description, which we term Det-Decoder and Des-Decoder, respectively. The Det-Decoder generates a full-sized probabilistic map , where  indicates the probability of a specific pixel being a keypoint,  and . The Des-Decoder produces a  tensor , where  denotes a -dimensional descriptor. Note that in the inference stage, Non-Maximum Suppression (NMS) is applied on  to obtain a binary mask  as the final detection result.  We formalize the above process as follows:

As illustrated in  \cref{fig:network}, we modify both Det-Decoder and Des-Decoder for RIM.

\textbf{U-Net as Det-Decoder}. 
Effectively capturing low-level patterns such as crossover and bifurcation on the vascular tree is crucial for detecting retinal keypoints in a reliable and repeatable manner. We therefore opt to use U-Net \cite{ronneberger2015u}, originally developed for biomedical image segmentation with its novel design of re-using varied levels of features from the encoder in the decoder by skip connections. In order to support high-resolution input, our encoder is relatively shallow, with a conv layer to generate low-level full-sized feature maps, followed by three conv blocks, each consisting of two conv layers,  max pooling and ReLU. Consequently, the high-level feature maps  have a size of .  In order to recover full-sized feature maps, our Det-Decoder uses three conv blocks, each having two conv layers, followed by bilinear upsampling\footnote{We use bilinear upsampling, as transposed convolutions originally used by U-Net are  computationally more expensive, and introduce unwanted checkerboard artifact\cite{M2U-Net}.}, ReLU and concatenation to merge the corresponding feature maps from the encoder. Lastly, a conv. block consisting of three conv. layers and one sigmoid activation is applied on the full-sized feature maps to produce the  detection map . 




\textbf{Full-sized Des-Decoder}.
Different from SuperPoint which computes its description loss on a down-sized tensor of , we target optimizing the descriptors on the full size of , where each pixel is associated with a -dimensional descriptor. Naturally, such dense results are obtained by interpolation, meaning gradient correlation between each keypoint and its neighborhood during backpropagation. Enlarging the neighborhood enhances the correlation, and is thus helpful for training with a larger receptive field \cite{chen2017deeplab}. In that regard, our Des-Decoder first downsizes  to more compact feature maps of , and then uses an upsampling block (using transposed conv) to generate the full-sized descriptor tensor  of . All the descriptors are -normalized.

Our network adaption may seem to be conceptually trivial. Note that producing a full-sized descriptor tensor is computationally prohibitive for a pixel-based description loss as used in SuperPoint and NCNet. A keypoint-based description loss is needed. Nonetheless, keypoint-based training is nontrivial, as inadequate annotations will make the network quickly converge to a local, suboptimal solution. However, having many training images adequately labeled is known to be expensive. To tackle the practical challenge, we develop a semi-supervised training algorithm that works with a small amount of incompletely labeled images.


\newlength{\twosubht}
\newsavebox{\twosubbox}

\begin{figure}[htb!]
\sbox\twosubbox{\resizebox{\dimexpr\textwidth-1em}{!}{\includegraphics[height=3cm]{figures/fig_overall_a.pdf}
    \includegraphics[height=3cm]{figures/fig_overall_b.pdf}
  }}
\setlength{\twosubht}{\ht\twosubbox}



\centering

\subcaptionbox{\label{fig:pke} Training with PKE}{\includegraphics[height=\twosubht]{figures/fig_overall_a.pdf}} \quad
\subcaptionbox{\label{fig:network} Network architecture}{\includegraphics[height=\twosubht]{figures/fig_overall_b.pdf}}

\sbox\twosubbox{\resizebox{\dimexpr\textwidth-1em}{!}{\includegraphics[height=3cm]{figures/fig_overall_c.pdf}
    \includegraphics[height=3cm]{figures/fig_overall_d.pdf}
  }}
\setlength{\twosubht}{\ht\twosubbox}

\centering

\subcaptionbox{\label{fig:double-matching} Double-matching strategy}{\includegraphics[height=\twosubht]{figures/fig_overall_c.pdf}}
\quad
\subcaptionbox{\label{fig:pke-demo} Newly added keypoints  (red dots)}{\includegraphics[height=\twosubht]{figures/fig_overall_d.pdf}}
\caption{\textbf{Proposed SuperRetina}. Green/orange markers in (c) indicate genuine/fake keypoints. Blue/red dots in (d) indicate the initial keypoints (auto-detected by PBO \cite{oinonen2010identity}) / iteratively detected keypoints for training.}

\label{fig:training-detector}

\end{figure}
 
\subsection{Training Algorithm}  \label{ssec:training}

\textbf{Semi-Supervised Training of Det-Decoder}.
We formulate keypoint detection as a pixel-level binary classification task \cite{detone2018superpoint, truong2019glampoints}. Due to the sparseness and incompleteness of manually labeled keypoints,  training Det-Decoder using a common binary cross-entropy (CE) loss is difficult. To attack the sparseness (and the resultant class imbalance) issue, we leverage two tactics. The first tactic, borrowed from Pose Estimation \cite{wei2016convolutional}, is to convert the binary labels  to soft labels  by 2D Gaussian blur, where each keypoint is a peak surrounded by neighbors with their values decaying exponentially. The second tactic is to use the Dice loss \cite{milletari2016v}, found to be  more effective than the weighted CE loss and the Focal loss to handle extreme class imbalance \cite{icpr20-lesion-net}. The Dice-based classification loss  per image is computed as 

where  denotes element-wise multiplication.

To attack the incompleteness issue, we propose \textbf{Progressive Keypoint Expansion} (PKE). The basic idea is to progressively expand the labeled keypoint set  by adding novel and reliable keypoints found by  Det-Detector, which itself is continuously improving after each epoch. To distinguish from such a dynamic , for each training image we now use  to indicate its initial keypoints, and  to denote keypoints detected at the -th epoch, . We obtain the expanded keypoint set  as , which \xredit{is used for training at the -th} epoch. 

As  is auto-constructed, improper keypoints are inevitable, in particular at the early stage when the Det-Decoder is relatively weak. Given that a good detector shall detect the same keypoint under different viewpoints and scales, GLAMpoints performs a geometric matching to identify keypoints that can be repeatedly detected from a given image and its projective transformations. We improve over GLAMpoints by adding a content-based matching, making it a \emph{double}-matching strategy. As \cref{fig:double-matching} shows, suppose a keypoint detected in a non-vascular area in  (orange circle) has a geometrically matched keypoint (orange square) in , with  as a specific homography. Non-vascular areas lack specificity in visual appearance, meaning descriptors extracted such areas are relatively close. Hence, even if the square is the best match to the circle in the descriptor space, it is not sufficiently different from the second-best match to pass Lowe's ratio test \cite{lowe2004distinctive}. Double matching is thus crucial. 

\begin{figure}[htb!]
  \centering
\includegraphics[width=0.65\linewidth]{figures/fig_pke.pdf}
   \caption{\textbf{Key dataflow within the PKE module}.}
   \label{fig:pke-flow}
\end{figure} 
As illustrated in \cref{fig:pke-flow}, the PKE module works as follows: \\
1) Construct , a geometric mapping of , using .\\
2) Feed  to SuperRetina to obtain its probabilistic detection map . The inverse projection of the map \wrt  is obtained as  .  \\
3) Geometric matching: For each point  in , add it to  if . \\
4) Content-based matching: For each point  in , we obtain its descriptor by directly sampling the output of the Des-Decoder, resulting in a descriptor set . Similarly, we extract  from  based on . Each descriptor in  is used as a query to perform the nearest neighbor search on . A point  will be preserved in , only if its spatial correspondence  passes the ratio test.

The above procedure allows us to progressively find new and reliable keypoints, see \cref{fig:pke-demo}. Moreover, in order to improve the holistic consistency between the detection maps of  and its geometric transformation , we additionally compute the Dice loss between  and , termed as .  Our detection loss  conditioned on   and  is computed as 



\textbf{Self-Supervised Training of Des-Decoder}. 
Ideally, the output of the Des-Decoder shall be invariant to homography. That is, for each keypoint  detected in , its descriptor shall be identical to the descriptor extracted at the corresponding location  in . To avoid a trivial solution of yielding a constant descriptor, we choose to optimize a triplet loss \cite{schroff2015facenet} such that the distance between paired keypoints shall be smaller than the distance between unpaired keypoints. Recall that  keypoints are automatically provided by the Det-Decoder, our Des-Decoder is trained in a fully self-supervised manner. Such a property lets the Des-Decoder learn from unlabeled data with ease.

Feeding  and  separately into SuperRetina allows us to access their full-sized descriptor tensors  and . For each element  in the non-maximum suppressed keypoint set , let  be its descriptor. As  and  shall be paired, the distance of their descriptors, denoted as , has to be reduced. With   excluded, we use  to indicate the descriptor distance between  and a point chosen randomly from . Let  be  the minimal distance. 
We argue that using  or   alone as the negative term in the triplet loss is problematic. As the requirement of  is relatively easy to fulfill, using  alone is inadequate to obtain descriptors of good discrimination. Meanwhile, as the network at its early training stage lacks ability to produce good descriptors, using  exclusively will make the network hard to train. To resolve the issue, we propose a simple  trick by using the mean of   and  as the negative term. Our description loss  is thus defined as

where  is a hyper-parameter controlling the margin. Note that  has a quadratic time complexity \wrt the size of , which is much smaller than . Hence, our description loss is much more efficient than its counterpart in SuperPoint, which is quadratic  \wrt   . As such, given the same amount of GPU resources, SuperRetina can be trained on higher-resolution images.

While we describe the training algorithms of Det-Decoder and Des-Decoder separately, they are jointly trained by minimizing the following combined loss:

where the homography  varies per mini-batch.  


\subsection{Keypoint-based Retinal Image Matching} \label{ssec:tasks}

Once trained, the use of SuperRetina for RIM is simple. Given a query image  and a reference image , we feed them separately into SuperRetina to obtain their keypoint probabilistic maps  and  and associated descriptor tensors  and . NMS is performed on  and  to obtain keypoints as  and . Recall that  and  are full-sized, so the corresponding descriptors  and  are fetched directly from the two tensors. Initial matches between  and  are obtained by an OpenCV brute-force matcher. The homography matrix  are then computed using the matched pairs to register  \wrt . As for identity verification,  is reused to remove outliers. The two images are accepted as \emph{genuine}, \ie from the same eye, if the number of matched points exceeds a predetermined threshold, and \emph{impostor} otherwise. The above process can be written in just a few lines of Python-style code, see the supplement. 

