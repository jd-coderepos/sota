\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{stmaryrd}
\usepackage{wasysym}

\usepackage{enumerate}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{footnote}

\usepackage{algorithm2e}











\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{claim}[lemma]{Claim}
\newtheorem{fact}[lemma]{Fact}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{redrule}{Rule}
\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{example}[lemma]{Example}
\theoremstyle{remark}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{observation}[lemma]{Observation}

\renewcommand{\thetheorem}{\Roman{theorem}}



\newcommand{\depth}{\alpha}


\newcommand{\findTD}{\mathtt{FindTD}}
\newcommand{\findPTD}{\mathtt{FindPartialTD}}
\newcommand{\whatsep}{\mathtt{whatsep}}


\newcommand{\compress}[1]{\mathtt{Compress}_{#1}}
\newcommand{\alg}[1]{\mathtt{Alg}_{#1}}

\newcommand{\build}{\mathtt{build}}
\newcommand{\ds}{\mathcal{DS}}

\newcommand{\oldS}{\mathrm{old_S}}
\newcommand{\oldPin}{\mathrm{old_\pi}}
\newcommand{\sep}{\mathtt{sep}}
\newcommand{\children}{\mathrm{children}}
\newcommand{\pins}{\mathtt{pins}}
\newcommand{\bags}{\mathtt{bags}}




\newcommand{\qnei}{\textnormal{findNeighborhood}}
\newcommand{\qUsep}{\textnormal{findUSeparator}}
\newcommand{\qSsep}{\textnormal{findSSeparator}}
\newcommand{\qpin}{\textnormal{findNextPin}}

\newcommand{\unknown}{\mathfrak{U}}
\newcommand{\forgotten}{\mathfrak{F}}
\newcommand{\ext}{\textrm{ext}}
\newcommand{\true}{\top}
\newcommand{\false}{\bot}
\newcommand{\pin}{\pi}

\newcommand{\getS}{\mathrm{get}_S}
\newcommand{\getPin}{\mathrm{get}_{\pi}}
\newcommand{\insertF}{\mathrm{insert}_{F}}
\newcommand{\insertS}{\mathrm{insert}_{S}}
\newcommand{\insertX}{\mathrm{insert}_{X}}

\newcommand{\removeF}{\mathrm{remove}_{F}}

\newcommand{\setPin}{\mathrm{set}_{\pi}}

\newcommand{\clearX}{\mathrm{clear}_{X}}
\newcommand{\clearF}{\mathrm{clear}_{F}}
\newcommand{\clearS}{\mathrm{clear}_{S}}

\newcommand{\td}{\mathcal{T}} \newcommand{\tw}{\mathrm{tw}} \newcommand{\w}{\mathrm{w}}   

\newcommand{\R}{\mathbb{R}}   




\newcommand{\defquery}[3]{
  \vspace{1mm}
\noindent\fbox{
  \begin{minipage}{0.97\textwidth}
  #1 \\
  {\bf{Output:}} #2  \\
  {\bf{Time:}} #3
  \end{minipage}
}\vspace{1mm}
}

\newcommand{\apx}{\textrm{apx}}



\begin{document}

\title{A  5-Approximation Algorithm for Treewidth}
\author{
  Hans L.\ Bodlaender\thanks{Department of Information and
    Computing Sciences, Utrecht University, P.O. Box 80.089, 3508 TB
    Utrecht, the Netherlands.  {h.l.bodlaender@uu.nl}}
  \and 
  Pål Grønås Drange\thanks{Department of Informatics, Univerity of
    Bergen, Norway.  \{Pal.Drange, Markus.Dregi, fomin,
      Daniel.Lokshtanov, Michal.Pilipczuk\}@ii.uib.no}
  \and
  Markus S.\ Dregi\footnotemark[2]
  \and
  Fedor V. Fomin\footnotemark[2]
  \and
  Daniel Lokshtanov\footnotemark[2]
  \and
  Micha\l{} Pilipczuk\footnotemark[2]}

\maketitle

\begin{abstract}
  We give an algorithm that for an input -vertex graph  and
  integer , in time  either outputs that the
  treewidth of  is larger than , or gives a tree decomposition
  of  of width at most .  This is the first algorithm
  providing a constant factor approximation for treewidth which runs
  in time single-exponential in  and linear in .
  
  Treewidth based computations are subroutines of numerous algorithms.
  Our algorithm can be used to speed up many such algorithms to work
  in time which is single-exponential in the treewidth and linear in
  the input size.
\end{abstract}

\section{Introduction}
\label{section:introduction}
Since its invention in the 1980s, the notion of treewidth has come to
play a central role in an enormous number of fields, ranging from very
deep structural theories to highly applied areas.  An important (but
not the only) reason for the impact of the notion is that many graph
problems that are intractable on general graphs become efficiently
solvable when the input is a graph of bounded treewidth.  In most
cases, the first step of an algorithm is to find a tree decomposition
of small width and the second step is to perform a dynamic programming
procedure on the tree decomposition.

In particular, if a graph on  vertices is given together with a
tree decomposition of width , many problems can be solved by
dynamic programming in time , i.e., single-exponential in
the treewidth and linear in .  Many of the problems admitting such
algorithms have been known for over thirty years~\cite{Bodlaender87}
but new algorithmic techniques on graphs of bounded
treewidth~\cite{BodlaenderCKN12, CyganKN12} as well as new problems
motivated by various applications (just a few of many examples
are~\cite{AbrahamBDR12,Gildea11,KosterHK02,RinaudoPBD12}) continue to
be discovered.  While a reasonably good tree decomposition can be
derived from the properties of the problem sometimes, in most of the
applications, the computation of a good tree decomposition is a
challenge.
Hence the natural question here is what can be done when no tree
decomposition is given.  In other words, is there an algorithm that
for a given graph  and integer , in time  either
correctly reports that the treewidth of  is at least , or finds
an optimal solution to our favorite problem (finds a maximum
independent set, computes the chromatic number, decides if  is
Hamiltonian, etc.)?  To answer this question it would be sufficient to
have an algorithm that in time  either reports correctly
that the treewidth of  is more that , or construct a tree
decomposition of width at most  for some constant .

However, the lack of such algorithms has been a bottleneck, both in
theory and in practical applications of the treewidth concept.  The
existing approximation algorithms give us the choice of running times
of the form , , or
, see Table~\ref{table:tw_history}.  Remarkably, the
newest of these current record holders is now almost 20 years old.
This ``newest record holder'' is the linear time algorithm of
Bodlaender~\cite{Bodlaender93s,Bodlaender96} that given a graph ,
decides if the treewidth of  is at most , and if so, gives a
tree decomposition of width at most  in  time.  The
improvement by Perkovi{\'{c}} and Reed~\cite{PerkovicR00} is only a
factor polynomial in  faster (but also, if the treewidth is larger
than , it gives a subgraph of treewidth more than  with a tree
decomposition of width at most , leading to an  algorithm
for the fundamental disjoint paths problem).  Recently, a version
running in logarithmic space was found by Elberfeld et
al.~\cite{ElberfeldJT10}, but its running time is not linear.



\begin{savenotes}
\begin{table}[htdp]
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
      \hline
      Reference                                 & Approximation   &   &   \\ \hline
      Arnborg  et al.~\cite{ArnborgCP87}        & exact         &  &  \\
      Robertson \& Seymour~\cite{RobertsonS13}  &       &    & \\
      Lagergren~\cite{Lagergren96}              &       &  & \\
      Reed~\cite{Reed92}                        & \footnotemark & &  \\
      Bodlaender~\cite{Bodlaender96}            & exact      & & \\
      Amir~\cite{Amir10}                        &         &  &  \\
      Amir~\cite{Amir10}                        &     &  &  \\
      Amir~\cite{Amir10}                        &  &  &  \\
      Feige et al.~\cite{FeigeHL08}             &  &   &  \\
      This paper                                &       &   &  \\
      This paper                                &       &   &  \\
      \hline
    \end{tabular}
  \end{center}
  \caption{Overview of treewidth  algorithms.  Here  is the treewidth
    and  is the number of vertices of an input graph .
    Each of the algorithms outputs in time  a
    decomposition of width given in the Approximation column.}
  \label{table:tw_history}
\end{table}
\end{savenotes}

\footnotetext{Reed~\cite{Reed92} does not state the approximation ratio of his algorithm explicitly. However, a careful analysis of his manuscript show that the algorithm can be implemented to give a tree decomposition of width at most .}


In this paper, we give the first constant factor approximation
algorithm for the treewidth graph such that its running time is single
exponential in treewidth and linear in the size of the input
graph. Our main result is the following theorem.


\begin{theorem}\label{thm:mainThm}
  There exists an algorithm, that given an -vertex graph  and
  an integer , in time  either outputs that the treewidth
  of  is larger than , or constructs a tree decomposition of 
  of width at most .
\end{theorem}

Of independent interest are a number of techniques that we use to
obtain the result and the intermediate result of an algorithm that
either tells that the treewidth is larger than  or outputs a tree
decomposition of width at most  in time .



\paragraph{Related results and techniques.}
The basic shape of our algorithm is along the same lines as about all
of the treewidth approximation algorithms~\cite{Amir10,
  BodlaenderGHK95, FeigeHL08, Lagergren96, Reed92, RobertsonS13},
i.e., a specific scheme of repeatedly finding separators.  If we ask
for polynomial time approximation algorithms for treewidth, the
currently best result is that of~\cite{FeigeHL08} that gives in
polynomial (but not linear) time a tree decomposition of width  where  is the treewidth of the graph.  Their
work also gives a polynomial time approximation algorithm with ratio
 for -minor free graphs.
By Austrin et al.~\cite{AustrinPW12}, assuming the Small Set Expansion
Conjecture, there is no polynomial time approximation algorithm for
treewidth with a constant performance ratio.

An important element in our algorithms is the use of a data structure
that allows to perform various queries in time  each,
for some constant .  This data structure is obtained by adding
various new techniques to old ideas from the area of dynamic
algorithms for graphs of bounded treewidth~\cite{Bodlaender92b,
  CohenSTV93, ChaudhuriZ98, ChaudhuriZ00, Hagerup00}.

A central element in the data structure is a tree decomposition of the
input graph of bounded (but too large) width such that the tree used
in the tree decomposition is binary and of logarithmic depth.  To
obtain this tree decomposition, we combine the following techniques:
following the scheme of the exact linear time
algorithms~\cite{Bodlaender96, PerkovicR00}, but replacing the call to
the dynamic programming algorithm of Bodlaender and
Kloks~\cite{BodlaenderK96} by a recursive call to our algorithm, we
obtain a tree decomposition of  of width at most  (or
, in the case of the  algorithm of
Section~\ref{section:nlogn}.)  Then, we use a result by Bodlaender and
Hagerup~\cite{BodlaenderH98} that this tree decomposition can be
turned in a tree decomposition with a logarithmic depth binary tree in
linear time, or more precisely, in  time and  operations on an EREW PRAM.
The cost of this transformation is increasing the width of the decomposition roughly three times.
The latter result is an application of classic results from parallel computing for solving problems on trees,
in particular Miller-Reif tree contraction~\cite{MillerR89,MillerR91}.

Using the data structure to ``implement'' the algorithm of Robertson
and Seymour~\cite{RobertsonS13} already gives an 
3-approximation for treewidth (Section~\ref{section:nlogn}).
Additional techniques are needed to speed this algorithm up.  We build
a series of algorithms, with running times of the forms , , etc.  Each algorithm
``implements'' Reeds algorithm~\cite{Reed92}, but with a different
procedure to find balanced separators of the subgraph at hand, and
stops when the subgraph at hand has size .  In the latter
case, we call the previous algorithm of the series on this subgraph.


Finally, to obtain a linear time algorithm, we consider two cases, one
case for when  is ``small'' (with respect to ), and one case
when  is ``large'', where we consider  to be small if

For small values of , we apply  algorithm
from Section~\ref{section:logi}.  This will yield a linear running
time in  since .  For larger values of , we
show that the linear time algorithms of~\cite{Bodlaender96}
or~\cite{PerkovicR00} can be implemented in truly linear time, without any overhead depending on . This seemingly surprising result can be roughly obtained as follows. 
We explicitly construct the finite state tree automaton of the dynamic
programming algorithm in sublinear time before processing the graph, and then view the dynamic programming routine as a run of the automaton, where productions are implemented as constant time table lookups. Viewing a dynamic programming algorithm on a tree decomposition as a finite state automaton traces
back to early work by Fellows and Langston~\cite{FellowsL89}, see e.g., also~\cite{AbrahamsonF93}. Our algorithm assumes the RAM model of computation~\cite{SavageBook}, and the only aspect of the RAM model which is exploited by our algorithm is the ability to look up an entry in a table in constant time, independently of the size of the table. This capability is crucially used in almost every linear time graph algorithm including breadth first search and depth first search.






\paragraph{Overview of the paper.}
In Section~\ref{sec:outline} we give the outline of the main algorithms, focusing on explaining main intuitions rather than formal details of the proofs.
Some concluding remarks and open questions are made in Section~\ref{section:conclusions}. 
Sections~\ref{section:nlogn}, \ref{section:logi}
and~\ref{section:linear} give the formal descriptions of the main algorithms: first the  algorithm, then the series of 
algorithms, before describing the  algorithm.  Each of the
algorithms described in these sections use queries to a data structure
which is described in Section~\ref{section:datastructure}.



















\paragraph{Notation.}
We give some basic definitions and notation, used throughout the paper.  For
, the function  is defined
as follows: , and for ,
.

For the presentation of our results, it is more convenient when we
regard tree decompositions as rooted.  This yields the following
definition of tree decompositions.
\begin{definition}
  \label{def:prelim:treewidth}
  A \emph{tree decomposition} of a graph  is a pair  where  is a rooted tree,
  and  is a family of subsets of , such that
  \begin{itemize}
  \item ,
  \item for all , there exists an  with ,
    and
  \item for all ,  induces a
    subtree of .
  \end{itemize}
  The \emph{width} of , denoted
   is .  The \emph{treewidth} of a
  graph , denoted by , is the minimum width of a tree
  decomposition of .
\end{definition}
For each , the tree induced by  is denoted by ; the
root of this tree, i.e., the node in the tree with smallest distance
to the root of  is denoted by .

The sets  are called the \emph{bags} of the decomposition.  For
each node , we define  to be the union of bags contained
in the subtree of  rooted in , including .  Moreover, we
denote  and , .
Note that by the definition of tree decomposition,  separates
 from .


\section{Proof outline}
\label{sec:outline}
Our algorithm builds on the constant factor approximation algorithm
for treewidth described in Graph Minors XIII~\cite{RobertsonS13} with
running time .  We start with a brief explanation of a
variant of this algorithm.

\subsection{The  time -approximation algorithm from
  Graph Minors XIII.}\label{sec:nSquareAppx}
The engine behind the algorithm is a lemma that states that graphs of
treewidth  have balanced separators of size .
In particular, for any way to assign non-negative weights to the
vertices there exists a set  of size at most  such that the
total weight of any connected component of  is at most
half of the total weight of .  We will use the variant of the lemma
where some vertices have weight  and some have weight .

\begin{lemma}[Graph Minors II~\cite{RobertsonS2}]
  \label{lemma:halfhalf}
  If  and , then there exists  with
   such that every component of  has at
  most  vertices which are in .
\end{lemma}

We note that the original version of~\cite{RobertsonS2} is seemingly
stronger: it gives bound  instead of
.  However, we do not need this stronger version and
we find it more convenient to work with the weaker.  The set  with
properties ensured by Lemma~\ref{lemma:halfhalf} will be called a
{\emph{balanced -separator}}, or a {\emph{-balanced
    -separator}}.  More generally, for an {\emph{-balanced
    -separator}}  every connected component of 
contains at most  vertices of .  If we omit the set ,
i.e., talk about separators instead of -separators, we mean
 and balanced separators of the whole vertex set.

The proof of Lemma~\ref{lemma:halfhalf} is not too hard; start with a
tree decomposition of  with width at most  and orient every edge
of the decomposition tree towards the side which contains the larger
part of the set .  Two edges of the decomposition can not point
``in different directions'', since then there would be disjoint parts
of the tree, both containing more than half of .  Thus there has to
be a node in the decomposition tree such that all edges of the
decomposition are oriented towards it.  The bag of the decomposition
corresponding to this node is exactly the set  of at most 
vertices whose deletion leaves connected components with at most
 vertices of  each.

The proof of Lemma~\ref{lemma:halfhalf} is constructive if one has
access to a tree decomposition of  of width less than .  The
algorithm does not have such a decomposition at hand, after all we are
trying to compute a decomposition of  of small width.  Thus we have
to settle for the following algorithmic variant of
lemma~\cite{RobertsonS2}.

\begin{lemma}[\cite{RobertsonS13}]
  \label{lem:halfhalfalg}
  There is an algorithm that given a graph , a set  and a  either concludes that  or outputs a set 
  of size at most  such that every component of 
  has at most  vertices which are in  and runs in
  time .
\end{lemma}

\begin{proof}[Proof sketch.]
  By Lemma~\ref{lemma:halfhalf} there exists a set  of size at
  most  such that every component of  has at most
   vertices which are in .  A simple packing
  argument shows that the components can be assigned to left or right
  such that at most  vertices of  go left and at
  most  go right.  Let  be  and let
   and  be the vertices of  that were put left and right
  respectively.  By trying all partitions of  in three parts the
  algorithm correctly guesses ,  and .  Now 
  separates  from  and so the minimum vertex cut between
   and  in  is at most .  The algorithm finds using max-flow a set  of
  size at most  that separates  from  in .  Since we are only interested in a set  of size
  at most  one can run max-flow in time .
  Having found , ,  and  the algorithm sets ,  to contain all components of  that
  contain vertices of  and  to contain all other vertices.
  Since every component  of  is fully contained in
   or , the bound on  follows.
  
  If no partition of  into , ,  yielded a cutset 
  of size , this means that , which the
  algorithm reports.
\end{proof}

The algorithm takes as input ,  and a set  on at most 
vertices, and either concludes that the treewidth of  is larger
than  or finds a tree decomposition of width at most  such
that the top bag of the decomposition contains .

On input , ,  the algorithm starts by ensuring that
.  If  the algorithm just adds arbitrary
vertices to  until equality is obtained.  Then the algorithm
applies Lemma~\ref{lem:halfhalfalg} and finds a set  of size at
most  such that each component  of  satisfies
.  Thus for each  we
have .  For each component  of
 the algorithm runs itself recursively on .

If either of the recursive calls returns that the treewidth is more
than  then the treewidth of  is more than  as well.
Otherwise we have for every component  a tree decomposition of
 of width at most  such that the top bag contains
.  To make a tree decomposition of  we make a
new root node with bag , and connect this bag to the roots
of the tree decompositions of  for each component
.  It is easy to verify that this is indeed a tree decomposition
of .  The top bag contains , and the size of the top bag is at
most , and so the width if the decomposition is at
most  as claimed.

The running time of the algorithm is governed by the recurrence
 which solves to  since  and there always are at least
two non-empty components of .  Finally, we use the
following observation about the number of edges in a graph of
treewidth .

\begin{lemma}[\cite{BodlaenderF05a}]
  \label{lemma:prelim:nk-edges}
  Let  be a graph with treewidth at most .  Then .
\end{lemma}


Thus if  the algorithm can safely output that .
After this, running the algorithm above takes time
.



\subsection{The  time approximation algorithm of
  Reed.}\label{sec:reedAlg}
Reed~\cite{Reed92} observed that the running time algorithm of
Robertson and Seymour~\cite{RobertsonS13} can be sped up from 
for fixed  to  for fixed , at the cost of a worse
(but still constant) approximation ratio, and a  dependence
on  in the running time, rather than the  factor in the
algorithm of Robertson and Seymour.  We remark here that
Reed~\cite{Reed92} never states explicitly the dependence on  of
his algorithm, but a careful analysis shows that this dependence is in
fact of order .  The main idea of this algorithm is that the
recurrence in Equation~\ref{eqn:gm13} only solves to  for
fixed  if one of the components of  contains almost
all of the vertices of .  If one could ensure that each component
 of  had at most  vertices for some
fixed , the recurrence in Equation~\ref{eqn:gm13} solves to
 for fixed .  To see that this is true we simply
consider the recursion tree.  The total amount of work done at any
level of the recursion tree is  for a fixed .  Since the size
of the components considered at one level is always a constant factor
smaller than the size of the components considered in the previous
level, the number of levels is only  and we have  work in total.

By using Lemma~\ref{lemma:halfhalf} with  we see that if 
has treewidth , then there is a set  of size at most 
such that each component of  has size at most
.  Unfortunately if we try to apply
Lemma~\ref{lem:halfhalfalg} to {\em find} an  which splits  in a
balanced way using , the algorithm of
Lemma~\ref{lem:halfhalfalg} takes time , which is exponential in .  Reed~\cite{Reed92} gave
an algorithmic variant of Lemma~\ref{lemma:halfhalf} especially
tailored for the case where .

\begin{lemma}[\cite{Reed92}]
  \label{lem:reedBalSep} There is an algorithm that given  and ,
  runs in time  and either concludes that  or
  outputs a set  of size at most  such that that every
  component of  has at most  vertices
  which are in .
\end{lemma}
Let us remark that Lemma~\ref{lem:reedBalSep} as stated here is never
explicitly proved in~\cite{Reed92}, but it follows easily from the
arguments given there.

Having Lemmata~\ref{lem:halfhalfalg} and~\ref{lem:reedBalSep} at hand,
we show how to obtain an -approximation of treewidth in time
.  The algorithm takes as input ,  and a set
 on at most  vertices, and either concludes that the
treewidth of  is at least , or finds a tree decomposition of
width at most  such that the top bag of the decomposition
contains .

On input , ,  the algorithm starts by ensuring that
.  If  the algorithm just adds vertices to 
until equality is obtained.  Then the algorithm applies
Lemma~\ref{lem:halfhalfalg} and finds a set  of size at most
 such that each component  of  satisfies
.  Now the algorithm
applies Lemma~\ref{lem:reedBalSep} and finds a set  of size at
most  such that each component  of 
satisfies .  Set .  For each component  of  we have
that .  For each component  of
 the algorithm runs itself recursively on .

If either of the recursive calls returns that the treewidth is more
than  then the treewidth of  is more than  as well.
Otherwise we have for every component  a tree decomposition of
 of width at most  such that the top bag contains
.  Similarly as before, to make a tree
decomposition of  we make a new root node with bag , and
connect this bag to the roots of the tree decompositions of  for each component .  It is easy to verify that this is
indeed a tree decomposition of .  The top bag contains , and the
size of the top bag is at most , and the width of the decomposition is at
most  as claimed.

The running time of the algorithm is governed by the recurrence

which solves to  since each  has
size at most .  By Lemma~\ref{lemma:prelim:nk-edges} we
have  and so the running time of the algorithm is upper
bounded by .




\subsection{A new  time 3-approximation
  algorithm}\label{sec:recursiveScheme}
The goal of this section is to sketch a proof of the following
theorem.  A full proof of Theorem~\ref{theorem:nlogn} can be found in
Section~\ref{section:nlogn}.
\begin{theorem}
  \label{theorem:nlogn}
  There exists an algorithm which given a graph  and an integer
  , either computes a tree decomposition of  of width at most
   or correctly concludes that , in time
   for some .
\end{theorem}

The algorithm employs the same recursive compression scheme which is
used in Bodlaender's linear time
algorithm~\cite{Bodlaender93s,Bodlaender96} and the algorithm
of~Perkovi{\'{c}} and Reed~\cite{PerkovicR00}.  The idea is to solve
the problem recursively on a smaller instance, expand the obtained
tree decomposition of the smaller graph to a ``good, but not quite
good enough'' tree decomposition of the instance in question, and then
use this tree decomposition to either conclude that  or
find a decomposition of  which is good enough.  A central concept
in this recursive approach of~\cite{Bodlaender96} is the definition of
an improved graph:


\begin{definition}\label{def:prelim:improved-graph}
  Given a graph  and an integer , the \emph{improved}
  graph of , denoted , is obtained by adding an edge between
  each pair of vertices with at least  common neighbors of degree
  at most  in .
\end{definition}

Intuitively, adding the edges during construction of the improved
graph cannot spoil any tree decomposition of  of width at most ,
as the pairs of vertices connected by the new edges will need to be
contained together in some bag anyway.  This is captured in the
following lemma.

\begin{lemma}
  \label{lem:improvedGraphTw}
  Given a graph  and an integer ,  if
  and only if .
\end{lemma}

If , which is the case in graphs of treewidth at most ,
the improved graph can be computed in  time using
radix sort~\cite{Bodlaender96}.

A vertex  is -simplicial, if it is simplicial in the improved
graph .  The intuition behind -simplicial vertices is as
follows: all the neighbors of an -simplicial vertex must be
simultaneously contained in some bag of any tree decomposition of
 of width at most , so we can safely remove such vertices from
the improved graph, compute the tree decomposition, and reintroduce
the removed -simplicial vertices.  The crucial observation is that
if no large set of -simplicial vertices can be found, then one can
identify a large matching, which can be also used for a robust
recursion step.  The following lemma, which follows from the work of
Bodlaender~\cite{Bodlaender96}, encapsulates all the main ingredients
that we will use.

\begin{lemma}\label{lem:prelim:bodlaender}
  There is an algorithm working in  time that, given a
  graph  and an integer , either
  \begin{enumerate}[(i)]
  \item returns a maximal matching in  of cardinality at least
    ,
  \item returns a set of at least  -simplicial
    vertices, or,
  \item correctly concludes that the treewidth of  is larger than
    .
  \end{enumerate}
  Moreover, if a set  of at least 
  -simplicial vertices is returned, and the algorithm is in
  addition provided with some tree decomposition  of
   of width at most , then in 
  time one can turn  into a tree decomposition  of  of
  width at most , or conclude that the treewidth of  is larger
  than .
\end{lemma}

Lemma~\ref{lem:prelim:bodlaender} allows us to reduce the problem to a
\emph{compression} variant where we are given a graph , an integer
 and a tree decomposition of  of width , and the goal is
to either conclude that the treewidth of  is at least  or output
a tree decomposition of  of width at most .  The proof of
Theorem~\ref{theorem:nlogn} has two parts: an algorithm for the
compression step and an algorithm for the general problem that uses
the algorithm for the compression step together with
Lemma~\ref{lem:prelim:bodlaender} as black boxes.  We now state the
properties of our algorithm for the compression step in the following
lemma.

\begin{lemma}\label{lemma:nlogn-compression}
  There exists an algorithm which on input , where
  (i) , , (ii)  and  are connected, and (iii)  is a tree decomposition of
   of width at most , in  time for some
   either computes a tree decomposition  of 
  with  and  as the root bag, or correctly
  concludes that .
\end{lemma}

We now give a proof of Theorem~\ref{theorem:nlogn}, assuming the
correctness of Lemma~\ref{lemma:nlogn-compression}.  The correctness
of the lemma will be argued for in Sections~\ref{sec:firstCompress}
and~\ref{sec:proofOfCompress}.
 
\begin{proof}[Proof of Theorem~\ref{theorem:nlogn}]
  Our algorithm will in fact solve a slightly more general problem.
  Here we are given a graph , an integer  and a set  on at
  most  vertices, with the property that  is
  connected.  The algorithm will either conclude that  or
  output a tree decomposition of width at most  such that 
  is the root bag.  To get a tree decomposition of any (possibly
  disconnected) graph it is sufficient to run this algorithm on each
  connected component with .  The algorithm proceeds
  as follows.  It first applies Lemma~\ref{lem:prelim:bodlaender} on
  .  If the algorithm of Lemma~\ref{lem:prelim:bodlaender}
  concludes that  the algorithm reports that .
  
  If the algorithm finds a matching  in  with at least
   edges, it contracts every edge in  and
  obtains a graph .  Since  is a minor of  we know that
  .  The algorithm runs itself recursively on
  , and either concludes that 
  (implying ) or outputs a tree decomposition of  of
  width at most .  Uncontracting the matching in this tree
  decomposition yields a tree decomposition  of  of width
  at most ~\cite{Bodlaender96}.  Now we can run the algorithm of
  Lemma~\ref{lemma:nlogn-compression} on  and
  either obtain a tree decomposition of  of width at most 
  and  as the root bag, or correctly conclude that .
  
  If the algorithm finds a set  of at least 
  -simplicial vertices, it constructs the improved graph  and
  runs itself recursively on .  If
  the algorithm concludes that  then
   implying  by
  Lemma~\ref{lem:improvedGraphTw}.  Otherwise we obtain a tree
  decomposition of  of width at most .  We may
  now apply Lemma~\ref{lem:prelim:bodlaender} and obtain a tree
  decomposition  of  with the same width.  Note that we
  can not just output  directly, since we can not be sure
  that  is the top bag of .  However we can run the
  algorithm of Lemma~\ref{lemma:nlogn-compression} on
   and either obtain a tree decomposition of 
  of width at most  and  as the root bag, or correctly
  conclude that .
  
  It remains to analyze the running time of the algorithm.  Suppose
  the algorithm takes time at most  on input  where
  .  Running the algorithm of
  Lemma~\ref{lem:prelim:bodlaender} takes  time.  Then
  the algorithm either halts, or calls itself recursively on a graph
  with at most  vertices
  taking time .  Then the algorithm takes
  time  to either conclude that  or to
  construct a tree decomposition  of  of width .
  In the latter case we finally run the algorithm of
  Lemma~\ref{lemma:nlogn-compression}, taking time .  This gives the following recurrence:
  
  The recurrence leads to a geometric series and solves to , completing the proof.  For a
  thorough analysis of the recurrence, see
  Equations~\ref{eqn:recurrenceOneD} and~\ref{eqn:recurrenceTwoD} in
  Section~\ref{section:nlogn}.  Pseudocode for the algorithm described
  here is given in Algorithm~\ref{alg:nlogn-alg-1} in
  Section~\ref{section:nlogn}.
\end{proof}

\subsubsection{A compression algorithm}\label{sec:firstCompress}
We now proceed to give a sketch of a proof for a slightly weakened
form of Lemma~\ref{lemma:nlogn-compression}.  The goal is to give an
algorithm that given as input a graph , an integer , a set 
of size at most  such that  is connected, and a
tree decomposition  of , runs in time 
and either correctly concludes that  or outputs a tree
decomposition of  of width at most .  The paper does not
contain a full proof of this variant of
Lemma~\ref{lemma:nlogn-compression} --- we will discuss the proof of
Lemma~\ref{lemma:nlogn-compression} in
Section~\ref{sec:proofOfCompress}.  The aim of this section is to
demonstrate that the recursive scheme of
Section~\ref{sec:recursiveScheme} together with a nice trick for
finding balanced separators is already sufficient to obtain a factor
 approximation for treewidth running in time .  A
variant of the trick used in this section for computing balanced
separators turns out to be useful in our final  time
-approximation algorithm.

The route we follow here
is to apply the algorithm of Reed described in
Section~\ref{sec:reedAlg}, but instead of using
Lemma~\ref{lem:reedBalSep} to find a set  of size  such that
every connected component of  is small, finding  by
dynamic programming over the tree decomposition  in time
.  There are a few technical difficulties with this approach.

The most serious issue is that, to the best of our knowledge, the only
known dynamic programming algorithms for balanced separators in graphs
of bounded treewidth take time  rather than : in
the state, apart from a partition of the bag, we also need to store
the cardinalities of the sides which gives us another dimension of
size .  We now explain how it is possible to overcome this issue.
We start by applying the argument in the proof of
Lemma~\ref{lemma:halfhalf} on the tree decomposition  and
get in time  a partition of  into ,  and
 such that there are no edges between  and ,
 and .  For every way of writing  and
every partition of  into  with  we do the following.

First we find in time  using dynamic programming over the
tree decomposition  a partition of  into
 such that there are no edges
from  to , , ,  and  and the size  is maximized.  Then we find in
time  using dynamic programming over the tree decomposition
 a partition of  into  such that there are no edges from  to
, , ,
 and  and the size
 is maximized.  Let ,  and .  The sets , ,  form a partition
of  with no edges from  to  and .

It is possible to show using a combinatorial argument (see
Lemma~\ref{lem:c4vc} in Section~\ref{section:datastructure}) that if
 then there exists a choice of , ,  such
that  and partition of  into  with  such that the above algorithm will output
a partition of  into ,  and  such that .  Thus we have an algorithm that in time 
either finds a set  of size at most  such that each connected
component of  has size at most  or
correctly concludes that .

\smallskip

The second problem with the approach is that the algorithm of Reed is
an -approximation algorithm rather than a -approximation.  Thus,
even the sped up version does not quite prove
Lemma~\ref{lemma:nlogn-compression}.  It does however yield a version
of Lemma~\ref{lemma:nlogn-compression} where the compression algorithm
is an -approximation.  In the proof of Theorem~\ref{theorem:nlogn}
there is nothing special about the number  and so one can use this
weaker variant of Lemma~\ref{lemma:nlogn-compression} to give a
-approximation algorithm for treewidth in time .
We will not give complete details of this algorithm, as we will
shortly describe a proof of Lemma~\ref{lemma:nlogn-compression} using
a quite different route.

It looks difficult to improve the algorithm above to an algorithm with
running time .  The main hurdle is the following: both the
algorithm of Robertson and Seymour~\cite{RobertsonS13} and the
algorithm of Reed~\cite{Reed92} find a separator  and proceed
recursively on the components of .  If we use  time to find the separator , then the total running time
must be at least  where  is the depth of
the recursion tree of the algorithm.  It is easy to see that the depth
of the tree decomposition output by the algorithms equals (up to
constant factors) the depth of the recursion tree.  However there
exist graphs of treewidth  such that no tree decomposition of depth
 has width  (take for example powers of paths).  Thus
the depth of the constructed tree decompositions, and hence the
recursion depth of the algorithm must be at least .

Even if we somehow managed to reuse computations and find the
separator  in time  on average, we
would still be in trouble since we need to pass on the list of
vertices of the connected components of  that we will
call the algorithm on recursively.  At a first glance this has to take
 time and then we are stuck with an algorithm with running time
, where  is the
recursion depth of the algorithm.  For  this is still
 which is slower than what we are aiming at.  In
Section~\ref{sec:proofOfCompress} we give a proof of
Lemma~\ref{lemma:nlogn-compression} that {\em almost} overcomes these
issues.

\subsubsection{A better compression algorithm}
\label{sec:proofOfCompress}
We give a sketch of the proof of Lemma~\ref{lemma:nlogn-compression}.
The goal is to give an algorithm that given as input a connected graph
, an integer , a set  of size at most  such that
 is connected, and a tree decomposition  of
, runs in time  and either correctly concludes that
 or outputs a tree decomposition of  of width at most
 with top bag .

Our strategy is to implement the  time -approximation
algorithm described in Section~\ref{sec:nSquareAppx}, but make some
crucial changes in order to (a) make the implementation run in
 time, and (b) make it a -approximation rather than
a -approximation.  We first turn to the easier of the two changes,
namely making the algorithm a -approximation.

To get an algorithm that satisfies all of the requirements of
Lemma~\ref{lemma:nlogn-compression}, but runs in time 
rather than  we run the algorithm described in
Section~\ref{sec:nSquareAppx} setting  in the beginning.
Instead of using Lemma~\ref{lem:halfhalfalg} to find a set  such
that every component of  has at most 
vertices which are in , we apply Lemma~\ref{lemma:halfhalf} to show
the {\em existence} of an  such that every component of  has at most  vertices which are in ,
and do dynamic programming over the tree decomposition  in
time  in order to find such an .  Going through the
analysis of Section~\ref{sec:nSquareAppx} but with  satisfying that
every component of  has at most 
vertices which are in  shows that the algorithm does in fact output
a tree decomposition with width  and top bag  whenever
.

It is somewhat non-trivial to do dynamic programming over the tree
decomposition  in time  in order to find an 
such that every component of  has at most
 vertices which are in .  The problem is that  could potentially have many components and we do not have
time to store information about each of these components individually.
The following lemma, whose proof can be found in
Section~\ref{sec:qssep}, shows how to deal with this problem.


\begin{lemma}\label{lem:balanced-3coloring}
  Let  be a graph and .  Then a set  is a balanced
  -separator if and only if there exists a partition
   of , such that there is no edge
  between  and  for , and 
  for .
\end{lemma}
Lemma~\ref{lem:balanced-3coloring} shows that when looking for a
balanced -separator we can just look for a partition of  into
four sets  such that there is no edge between  and
 for , and  for .  This
can easily be done in time  by dynamic programming over the
tree decomposition .  This yields the promised algorithm
that satisfies all of the requirements of
Lemma~\ref{lemma:nlogn-compression}, but runs in time 
rather than .

\medskip We now turn to the most difficult part of the proof of
Lemma~\ref{lemma:nlogn-compression}, namely how to improve the running
time of the algorithm above from  to  in a
way that gives hope of a further improvement to running time
.  The  time algorithm we describe now is
based on the following observations; (a) In any recursive call of the
algorithm above, the considered graph is an induced subgraph of .
Specifically the considered graph is always  where  is
a set with at most  vertices and  is a connected component of
.  (b) The only computationally hard step, finding the
balanced -separator , is done by dynamic programming over the
tree decomposition  of .  The observations (a) and (b)
give some hope that one can reuse the computations done in the dynamic
programming when finding a balanced -separator for  during the
computation of balanced -separators in induced subgraphs of .
This plan can be carried out in a surprisingly clean manner and we now
give a rough sketch of how it can be done.

We start by preprocessing the tree decomposition using an algorithm of
Bodlaender and Hagerup~\cite{BodlaenderH98}.  This algorithm is a
parallel algorithm and here we state its sequential form.
\begin{proposition}[Bodlaender and Hagerup~\cite{BodlaenderH98}]
  \label{proposition:parallel}
  There is an algorithm that, given a tree decomposition of width 
  with  nodes of a graph , finds a rooted binary tree
  decomposition of  of width at most  with depth 
  in  time.
\end{proposition}
Proposition~\ref{proposition:parallel} lets us assume without loss of
generality that the tree decomposition  has depth .

In Section~\ref{section:datastructure} we will describe a data
structure with the following properties.  The data structure takes as
input a graph , an integer  and a tree decomposition 
of width  and depth .  After an initialization step
which takes  time the data structure allows us to do certain
operations and queries.  At any point of time the data structure is in
a certain {\em state}.  The operations allow us to change the state of
the data structure.  Formally, the state of the data structure is a
-tuple  of subsets of  and a vertex  called
the ``pin'', with the restriction that .  The initial
state of the data structure is that , , and
 is an arbitrary vertex of .  The data structure
allows operations that change ,  or  by inserting/deleting a
specified vertex, and move the pin to a specified vertex in time
.

For a fixed state of the data structure, the {\em active component}
 is the component of  which contains .  The
data structure allows the query \qSsep{} which outputs in time  either an -balanced separator  of 
of size at most , or , which means that .

The algorithm of Lemma~\ref{lemma:nlogn-compression} runs the
 time algorithm described above, but uses the \emph{data
  structure} to find the balanced -separator in time  instead of doing dynamic programming over .  All we need
to make sure is that the  in the state of the data structure is
always equal to the set  for which we want to find the balanced
separator, and that the active component  is set such that  is equal to the induced subgraph we are working on.  Since we
always maintain that  we can change the set  to
anywhere in the graph (and specifically into the correct position) by
doing  operations taking  time each.

At a glance, it looks like that if we assume the data structure as a
black box, this is sufficient to obtain the desired 
time algorithm.  However, we haven't even used the sets  and  in
the state of the data structure, or described what they mean! The
reason for this is of course that there is a complication.  In
particular, after the balanced -separator  is found ---
how can we recurse into the connected components of ? We need to move the pin into each of
these components one at a time, but if we want to use 
time in each recursion step, we cannot afford to spend 
time to compute the connected components of .  We resolve this issue by pushing the problem into the
data structure, and showing that the appropriate queries can be
implemented there.  This is where the sets  and  in the state of
the data structure come in.

The rôle of  in the data structure is that when queries to the data
structure depending on  are called,  equals the set ,
i.e., the balanced -separator found by the query \qSsep{}.  The set
 is a set of ``finished pins'' whose intention is the following:
when the algorithm calls itself recursively on a component  of
 after it is done with
computing a tree decomposition of  with  as
its top bag, it selects an arbitrary vertex of  and inserts it
into .  

The query \qpin{} finds a new pin  in a component  of  that does not contain any vertices
of .  And finally, the query \qnei{} allows us to find the
neighborhood , which in turn allows us to call the algorithm
recursively in order to find a tree decomposition of 
with  as its top bag.

At this point it is possible to convince oneself that the 
time algorithm described in the beginning of this section can be
implemented using  calls to the data structure in each
recursive step, thus spending only  time in each
recursive step.  Pseudocode for this algorithm can be found in
Algorithm~\ref{alg:nlogn-findtd}.  The recurrence bounding the running
time of the algorithm then becomes
 Here  are the connected
components of .  This
recurrence solves to , proving
Lemma~\ref{lemma:nlogn-compression}.  A full proof of
Lemma~\ref{lemma:nlogn-compression} assuming the data structure as a
black box may be found in Section~\ref{sec:nlogncompress}.

\subsection{The data structure}
\label{sec:dsoutline}
We sketch the main ideas in the implementation of the data structure.
The goal is to set up a data structure that takes as input a graph
, an integer  and a tree decomposition  of width
 and depth , and initializes in time .  The
{\em state} of the data structure is a -tuple  where
,  and  are vertex sets in  and .  The initial state of the data structure is
 where  is an arbitrary vertex in
.  The data structure should support operations that
insert (delete) a single vertex to (from) ,  and , and an
operation to change the pin  to a specified vertex.  These
operations should run in time .  For a given state of
the data structure, set  to be the component of 
that contains .  The data structure should also support the
following queries in time .
\begin{itemize}\setlength\itemsep{-.7mm}
\item \qSsep{}: Assuming that , return a set  of
  size at most  such that every component of  contains at most  vertices of
  , or conclude that .
\item \qpin{}: Return a vertex  in a component  of  that does not contain any vertices of .
\item \qnei{}: Return  if  and  otherwise.
\end{itemize}

Suppose for now that we want to set up a much simpler data structure.
Here the state is just the set  and the only query we want to
support is \qSsep{} which returns a set  such that every
component of  contains at most
 vertices of , or conclude that .  At
our disposal we have the tree decomposition  of width 
and depth .  To set up the data structure we run a standard
dynamic programming algorithm for finding  given .  Here
we use Lemma~\ref{lem:balanced-3coloring} and search for a partition
of  into  such that , there is no
edge between  and  for , and  for .  This can be done in time  and
the tables stored at each node of the tree decomposition have size
.  This finishes the initialization step of the data
structure.  The initialization step took time .

We will assume without loss of generality that the top bag of the
decomposition is empty.  The data structure will maintain the
following invariant: after every change has been performed the tables
stored at each node of the tree decomposition correspond to a valid
execution of the dynamic programming algorithm on input .  If
we are able to maintain this invariant, then answering \qSsep{}
queries is easy: assuming that each cell of the dynamic programming
table also stores solution sets (whose size is at most ) we can
just output in time  the content of the top bag of the
decomposition!

But how to maintain the invariant and support changes in time
? It turns out that this is not too difficult: the
content of the dynamic programming table of a node  in the tree
decomposition depends only on  and the dynamic programming tables
of 's children.  Thus, when the dynamic programming table of the
node  is changed, this will only affect the dynamic programming
tables of the  ancestors of .  If the dynamic program is
done carefully, one can ensure that adding or removing a vertex
to/from  will only affect the dynamic programming tables for a
single node  in the decomposition, together with all of its  ancestors.  Performing the changes amounts to recomputing the
dynamic programming tables for these nodes, and this takes time
.

It should now be plausible that the idea above can be extended to work
also for the more complicated data structure with the more advanced
queries.  Of course there are several technical difficulties, the main
one is how to ensure that the computation is done in the connected
component  of  without having to store ``all
possible ways the vertices in a bag could be connected below the
bag''.  We omit the details of how this can be done in this outline.
The full exposition of the data structure can be found in
Section~\ref{section:datastructure}.

\subsection{Approximating treewidth in 
  time.}
We now sketch how the algorithm of the previous section can be sped
up, at the cost of increasing the approximation ratio from  to .
In particular we give a proof outline for the following theorem.
\begin{theorem}\label{theorem:nlogin}
  For every , there exists an algorithm which,
  given a graph  and an integer , in  time for some  either computes a
  tree decomposition of  of width at most  or correctly
  concludes that .
\end{theorem}
The algorithm of Theorem~\ref{theorem:nlogn} satisfies the conditions
of Theorem~\ref{theorem:nlogin} for .  We will show how
one can use the algorithm for  in order to obtain an
algorithm for .  In particular we aim at an algorithm
which given a graph  and an integer , in  time for some  either computes a tree
decomposition of  of width at most  or correctly concludes
that .

We inspect the  algorithm for the compression step
described in Section~\ref{sec:proofOfCompress}.  It uses the data
structure of Section~\ref{sec:dsoutline} in order to find balanced
separators in time .  The algorithm uses  time on each recursive call regardless of the size of the induced
subgraph of  it is currently working on.  When the subgraph we work
on is big this is very fast.  However, when we get down to induced
subgraphs of size  the algorithm of Robertson and
Seymour described in Section~\ref{sec:nSquareAppx} would spend  time in each recursive call, while our presumably
fast algorithm still spends  time.  This suggests that
there is room for improvement in the recursive calls where the
considered subgraph is very small compared to .

The overall structure of our  time algorithm is
identical to the structure of the  time algorithm of
Theorem~\ref{theorem:nlogn}.  The only modifications happen in the
compression step.  The compression step is also similar to the  algorithm described in Section~\ref{sec:proofOfCompress}, but
with the following caveat.  The data structure query \qpin{} finds the
{\em largest} component where a new pin can be placed, returns a
vertex from this component, and also returns the size of this
component.  If a call of \qpin{} returns that the size of the largest
yet unprocessed component is less than  the algorithm does not
process this component, nor any of the other remaining components in
this recursive call.  This ensures that the algorithm is never run on
instances where it is slow.  Of course, if we do not process the small
components we do not find a tree decomposition of them either.  A bit
of inspection reveals that what the algorithm will do is either
conclude that  or find a tree decomposition of an induced
subgraph of  of width at most  such that for each connected
component  of , (a) , (b)
, and (c)  is fully contained in some bag
of the tree decomposition of .

How much time does it take the algorithm to produce this output? Each
recursive call takes  time and adds a bag to the tree
decomposition of  that contains some vertex which was not yet in
.  Thus the total time of the algorithm is upper bounded by
.  What happens if we run this algorithm,
then run the  time algorithm of
Theorem~\ref{theorem:nlogn} on each of the connected components of ? If either of the recursive calls return that the
treewidth of the component is more than  then .
Otherwise we have a tree decomposition of each of the connected
components with width .  With a little bit of extra care we find
tree decompositions of the same width of  for each
component , such that the top bag of the decomposition contains
.  Then all of these decompositions can be glued together with
the decomposition of  to yield a decomposition of width  for
the entire graph .

The running time of the above algorithm can be bounded as follows.  It
takes  time to find the partial tree
decomposition of , and
 
time to find the tree decompositions of all the small components.
Thus, if  the running time of the
first part would be  and the total running time would be
.

How big can  be? In other words, if we inspect the algorithm
described in Section~\ref{sec:nSquareAppx}, how big part of the graph
does the algorithm see before all remaining parts have size less than
? The bad news is that the algorithm could see almost the
entire graph.  Specifically if we run the algorithm on a path it could
well be building a tree decomposition of the path by moving along the
path and only terminating when reaching the vertex which is 
steps away from from the endpoint.  The good news is that the
algorithm of Reed described in Section~\ref{sec:reedAlg} will get down
to components of size  after decomposing only  vertices of .  The reason is that the algorithm of Reed also
finds balanced separators of the considered subgraph, ensuring that
the size of the considered components drop by a constant factor for
each step down in the recursion tree.

Thus, if we augment the algorithm that finds the tree decomposition of
the subgraph  such that that it also finds balanced separators of
the active component and adds them to the top bag of the decomposition
before going into recursive calls, this will ensure that  and that the total running time of the algorithm
described in the paragraphs above will be .  The
algorithm of Reed described in Section~\ref{sec:reedAlg} has a worse
approximation ratio than the algorithm of Robertson and Seymour
described in Section~\ref{sec:nSquareAppx}.  The reason is that we
also need to add the balanced separator to the top bag of the
decomposition.  When we augment the algorithm that finds the tree
decomposition of the subgraph  in a similar manner, the
approximation ratio also gets worse.  If we are careful about how the
change is implemented we can still achieve an algorithm with running
time  that meets the specifications of
Theorem~\ref{theorem:nlogin} for .

The approach to improve the running time from  to
 also works for improving the running time from
 to .  Running the algorithm that finds in  time the tree
decomposition of the subgraph  such that all components of  have size  and running the  time algorithm on each of these components yields
an algorithm with running time .

In the above discussion we skipped over the following issue.  How can
we compute a small balanced separator for the active component in time
? It turns out that also this can be handled by the
data structure.  The main idea here is to consider the dynamic
programming algorithm used in Section~\ref{sec:firstCompress} to find
balanced separators in graphs of bounded treewidth, and show that this
algorithm can be turned into a  time data structure
query.  We would like to remark here that the implementation of the
trick from Section~\ref{sec:firstCompress} is significantly more
involved than the other queries: we need to use the approximate tree
decomposition not only for fast dynamic programming computations, but
also to locate the separation  on which the trick is
employed.  A detailed explanation of how this is done can be found at
the end of Section~\ref{sec:queries}.  This completes the proof sketch
of Theorem~\ref{theorem:nlogin}.  A full proof can be found in
Section~\ref{section:logi}.

\subsection{-approximation in  time.}
The algorithm(s) of Theorem~\ref{section:logi} are in fact already
 algorithms unless  is astronomically large compared to
.  If, for example,  then 
and so .  Thus, to get an algorithm
which runs in  it is sufficient to consider the cases when
 is really, really big compared to .  The recursive scheme of
Section~\ref{sec:recursiveScheme} allows us to only consider the case
where (a)  is really big compared to  and (b) we have at our
disposal a tree decomposition  of  of width .

For this case, consider the dynamic programming algorithm of
Bodlaender and Kloks~\cite{BodlaenderK96} that given  and a tree
decomposition  of  of width  either computes a tree
decomposition of  of width  or concludes that  in
time .  The dynamic programming algorithm can be
turned into a tree automata based
algorithm~\cite{FellowsL89,AbrahamsonF93} with running time
 if one can inspect an arbitrary entry of a
table of size  in constant time.  If  then inspecting an arbitrary entry of a table
of size , means inspecting an arbitrary entry of a
table of size , which one can do in constant time in the RAM
model.  Thus, when  we can find an
optimal tree decomposition in time .  When  the  time algorithm of
Theorem~\ref{section:logi} runs in time .  This concludes
the outline of the proof of Theorem~\ref{thm:mainThm}.  A full
explanation of how to handle the case where  is much bigger than
 can be found in Section~\ref{section:linear}.

\section{Conclusions}
\label{section:conclusions}
In this paper we have presented an algorithm that gives a constant
factor approximation (with a factor 5) of the treewidth of a graph,
which runs in single exponential time in the treewidth and linear in
the number of vertices of the input graph.  Here we give some
consequences of the result, possible improvements and open problems.

\subsection{Consequences, corollaries and future work}
A large number of computational results use the following overall
scheme: first find a tree decomposition of bounded width, and then run
a dynamic programming algorithm on it.  Many of these results use the
linear-time exact algorithm of Bodlaender~\cite{Bodlaender96} for the
first step.
If we aim for algorithms whose running time dependency on treewidth is
single exponential, however, then our algorithm is preferable over the
exact algorithm of Bodlaender~\cite{Bodlaender96}.  Indeed, many
classical problems like \textsc{Dominating Set} and
\textsc{Independent Set} are easily solvable in time 
when a tree decomposition of width  is provided.  Furthermore,
there is an on-going work on finding new dynamic programming routines
with such a running time for problems seemingly not admitting so
robust solutions; the fundamental examples are \textsc{Steiner Tree},
\textsc{Traveling Salesman} and \textsc{Feedback Vertex
  Set}~\cite{BodlaenderCKN12}.  The results of this paper show that
for all these problems we may also claim  running time
even if the decomposition is not given to us explicitly, as we may
find its constant factor approximation within the same complexity
bound.

Our algorithm is also compatible with the celebrated Courcelle's
theorem~\cite{Courcelle90}, which states that every graph problem
expressible in monadic second-order logic (MSOL) is solvable in time
 when a tree decomposition of width  is
provided, where  is the formula expressing the problem and
 is some function.  Again, the first step of applying the
Courcelle's theorem is usually computing the optimum
tree-decomposition using the linear-time algorithm of
Bodlaender~\cite{Bodlaender96}.  Using the results of this paper, this
step can be substituted with finding an approximate decomposition in
 time.  For many problems, in the overall running time
analysis we may thus significantly reduce the factor dependent on the
treewidth of the graph, while keeping the linear dependence on  at
the same time.

It seems that the main novel idea of this paper, namely treating the
tree decomposition as a data structure on which logarithmic-time
queries can be implemented, can be similarly applied to all the
problems expressible in MSOL.  Extending our results in this direction
seems like a thrilling perspective for future work.

Concrete examples where the results of this paper can be applied, can
be found also within the framework of \emph{bidimensionality
  theory}~\cite{DemaineFHT05a, DemaineH08}.  In all parameterized
subexponential algorithms obtained within this framework, the
polynomial dependence of  in the running time becomes linear if our
algorithm is used.  For instance, it follows immediately that every
parameterized minor bidimensional problem with parameter , solvable
in time  on graphs of treewidth , is solvable in time
 on graphs excluding some fixed graph as a minor.


\subsection{Improvements and open problems}
Our result is mainly of theoretical importance due to the large
constant  at the base of the exponent.  One immediate open problem
is to obtain a constant factor approximation algorithm for treewidth
with running time , where  is a \emph{small} constant.

Another open problem is to find more efficient \emph{exact} FPT
algorithms for treewidth.  Bodlaender's algorithm \cite{Bodlaender96}
and the version of Reed and Perkovi{\'{c}} both use 
time; the dominant term being a call to the dynamic programming
algorithm of~\cite{BodlaenderK96}.  In fact, no exact FPT algorithm
for treewidth is known whose running time as a function of the
parameter  is asymptotically smaller than this; testing the
treewidth by verifying the forbidden minors can be expected to be
significantly slower.  Thus, it would be very interesting to have an
exact algorithm for testing if the treewidth of a given graph is at
most  in  time.

Currently, the best approximation ratio for treewidth for algorithms
whose running time is polynomial in  and single exponential in the
treewidth is the 3-approximation algorithm from
Section~\ref{section:nlogn}.  What is the best approximation ratio for
treewidth that can be obtained in this running time? Is it possible to
give lower bounds?

\section{An  3-approximation algorithm for treewidth}
\label{section:nlogn}

In this section, we provide formal details of the proof of
Theorem~\ref{theorem:nlogn}:
\begin{theorem}[Theorem~\ref{theorem:nlogn}, restated]
  There exists an algorithm which, given a graph  and an integer
  , in  time for some 
  either computes a tree decomposition of  of width at most  or correctly concludes that .
\end{theorem}

In fact, the algorithm that we present, is slightly more general.  The
main procedure, , takes as input a connected graph , an
integer , and a subset of vertices  such that .  Moreover, we have a guarantee that not only  is connected,
but  as well.   runs in  time for some  and either concludes that , or returns a tree decomposition of  of width ,
such that  is the root bag.  Clearly, to prove
Theorem~\ref{theorem:nlogn}, we can run  on every connected
component of  separately using .  Note that
computation of the connected components takes  time,
since if , then we can safely output that .

The presented algorithm  uses two subroutines.  As described
in Section~\ref{sec:outline},  uses the reduction approach
developed by the first author~\cite{Bodlaender96}; in short words, we
can either apply a reduction step, or find an approximate tree
decomposition of width  on which a compression subroutine
 can be employed.  In this compression step we are
either able to find a refined, compressed tree decomposition of width
at most , or again conclude that .

The algorithm  starts by initializing the data structure
(see Section~\ref{sec:outline} for an intuitive description of the
role of the data structure), and then calls a subroutine .
This subroutine resembles the algorithm of Robertson and Seymour (see
Section~\ref{sec:outline}): it divides the graph using balanced
separators, recurses on the different connected components, and
combines the subtrees obtained for the components into the final tree
decomposition.

\subsection{The main procedure }

Algorithm , whose layout is proposed as
Algorithm~\ref{alg:nlogn-alg-1}, runs very similarly to the algorithm
of the first author~\cite{Bodlaender96}; we provide here all the
necessary details for the sake of completeness, but we refer
to~\cite{Bodlaender96} for a wider discussion.

First, we apply Lemma~\ref{lem:prelim:bodlaender} on graph  for
parameter .  We either immediately conclude that ,
find a set of I-simplicial vertices of size at least
, or a matching of size at least .
Note that in the application of Lemma~\ref{lem:prelim:bodlaender} we
ignore the fact that some of the vertices are distinguished as .

If a matching  of size at least  is found, we
employ a similar strategy as in~\cite{Bodlaender96}.  We first
contract the matching  to obtain ; note that if  had
treewidth at most  then so does .  Then we apply 
recursively to obtain a tree decomposition  of  of width at
most , and having achieved this we decontract the matching 
to obtain a tree decomposition  of  of width at most :
every vertex in the contracted graph is replaced by at most two
vertices before the contraction.  Finally, we call the sub-procedure
, which given  and the decomposition  (of
width ), either concludes that , or provides a tree
decomposition of  of width at most , with  as the root
bag.   is given in details in the next section.

In case of obtaining a large set  of I-simplicial vertices, we
proceed similarly as in~\cite{Bodlaender96}.  We compute the improved
graph, remove  from it, apply  on 
recursively to obtain its tree decomposition  of width at most
, and finally reintroduce the missing vertices of  to obtain
a tree decomposition  of  of width at most  (recall that
reintroduction can fail, and in this case we may conclude that
).  Observe that the decomposition  satisfies all the
needed properties, with the exception that we have not guaranteed that
 is the root bag.  However, to find a decomposition that has
 as the root bag, we may again make use of the subroutine
, running it on input  and the tree
decomposition .  Lemma~\ref{lem:prelim:bodlaender} ensures that
all the described steps, apart from the recursive calls to 
and , can be performed in  time.
Note that the -simplicial vertices can safely be reintroduced since
we used Lemma~\ref{lem:prelim:bodlaender} for parameter  instead
of .

\begin{algorithm}[h!]

  
\KwIn{A connected graph , an integer , and  s.t.
     and  is connected.} \KwOut{A tree
    decomposition  of  with  and  as
    the root bag, or conclusion that .}  \Indp \BlankLine
  \BlankLine Run algorithm of Lemma~\ref{lem:prelim:bodlaender} for
  parameter  \BlankLine \If{Conclusion that }{
    \KwRet{} } \BlankLine
  
  \If{ has a matching  of cardinality at least
    } {
    Contract  to obtain .\\
     \hspace{1cm}/*\textsf{\footnotesize{ 
    } }*/\\
    \eIf{}{ \KwRet{} } {
      Decontract the edges of  in  to obtain .\\
      \KwRet{ } } }
  
  \BlankLine \If{ has a set  of at least 
    I-simplicial vertices}{
    Compute the improved graph  and remove  from it.    \\
     
    \hspace{1cm}/*\textsf{\footnotesize{ } }*/\\
    \If{}{ \KwRet{} }
    Reintroduce vertices of  to  to obtain .\\
    \eIf{Reintroduction failed}{ \KwRet{} }{
      \KwRet{ } } } \Indm
  \caption{}
  \label{alg:nlogn-alg-1}
\end{algorithm}

Let us now analyze the running time of the presented algorithm,
provided that the running time of the subroutine  is
 for some .  Since all the
steps of the algorithm (except for calls to subroutines) can be
performed in  time, the time complexity satisfies
the following recurrence relation:

Here  is the constant hidden in the -notation in
Lemma~\ref{lem:prelim:bodlaender}.  By unraveling the recurrence into
a geometric series, we obtain that

for some .


\subsection{Compression}\label{sec:nlogncompress}

In this section we provide the details of the implementation of the
subroutine .  The main goal is encapsulated in the
following lemma.

\begin{lemma}[Lemma~\ref{lemma:nlogn-compression}, restated]
  There exists an algorithm which on input , where
  (i) , , (ii)  and  are connected, and (iii)  is a tree decomposition of
   of width at most , in  time for some
   either computes a tree decomposition  of 
  with  and  as the root bag, or correctly
  concludes that .
\end{lemma}

The subroutine's layout is given as
Algorithm~\ref{alg:nlogn-compress}.  Shortly speaking, we first
initialize the data structure , with  as input,
and then run a recursive algorithm  that constructs the
decomposition itself given access to the data structure.  The
decomposition is returned by a pointer to the root bag.  The data
structure interface will be explained in the following paragraphs, and
its implementation is given in Section~\ref{section:datastructure}.
We refer to Section~\ref{sec:outline} for a brief, intuitive outline.

\begin{algorithm}

  \KwIn{Connected graph , , a set  s.t.
     and  is connected, and a tree
    decomposition  with }
  
  \KwOut{Tree decomposition of  of width at most  with
     as the root bag, or conclusion that .}  \Indp
  \BlankLine
  Initialize data structure  with \\
  
  \KwRet{}
  
  \caption{.}
  
  \label{alg:nlogn-compress}
\end{algorithm}

The initialization of the data structure takes  time (see
Lemma~\ref{lemma:ds:init-time}).  The time complexity of ,
given in Section~\ref{section:findTDlogn}, is .








\subsection{The recursive algorithm
  }\label{section:findTDlogn}

Subroutine  works on the graph  with two disjoint vertex
sets  and  distinguished.  Intuitively,  is small (of size at
most ) and represents the root bag of the tree decomposition
under construction.   in turn, stands for the part of the graph to
be decomposed below the bag containing , and is always one of the
connected components of .  As explained in
Section~\ref{sec:outline}, we cannot afford storing  explicitly.
Instead, we represent  in the data structure by an arbitrary vertex
 (called the {\emph{pin}}) belonging to it, and implicitly
define  to be the connected component of  that
contains .  Formally, behavior of the subroutine  is
encapsulated in the following lemma:

\begin{lemma}
  \label{lemma:nlogn-s-root-bag}
  There exists an algorithm that, given access to the data structure
   in a state such that , computes a tree
  decomposition  of  of width  with 
  as a root bag, or correctly reports that that .  If the algorithm is run on  and , then its
  running time is  for some .
\end{lemma}

The data structure is initialized with  and  set to an
arbitrary vertex of ; as we have assumed that
 is connected, this gives  after
initialization.  Therefore, Lemma~\ref{lemma:nlogn-s-root-bag}
immediately yields Lemma~\ref{lemma:nlogn-compression}.

\paragraph{A gentle introduction to the data structure}
Before we proceed to the implementation of the subroutine ,
we give a quick description of the interface of the data structure
: what kind of queries and updates it supports, and what is the
running time of their execution.  The details of the data structure
implementation will be given in Section~\ref{section:datastructure}.

The state of the data structure is, in addition to , three
subsets of vertices, ,  and , and the pin  with the
restriction that .   and  uniquely imply the
set , defined as the connected component of  that
contains .  The intuition behind these sets and the pin is the
following:

\begin{itemize}
\item  is the set that will serve as a root bag for some subtree,
\item  is a vertex which indicates the current active component,
\item  is the current active component, the connected component of
   containing ,
\item  is a balanced -separator (of ) and
\item  is a set of vertices marking the connected components of
   as ``finished''.
\end{itemize}

The construction of the data structure  is heavily based on the
fact that we are provided with some tree decomposition of width
.  Given this tree decomposition, the data structure can be
initialized in  time for some .  At
the moment of initialization we set  and  to be
an arbitrary vertex of .  During the run of the algorithm, the
following updates can be performed on the data structure:
\begin{itemize}
\item insert/remove a vertex to/from , , or ;
\item mark/unmark a vertex as a pin .
\end{itemize}
All of these updates will be performed in  time
for some .

The data structure provides a number of queries that are used in the
subroutine .  The running time of each query is  for some , and in many cases it is actually
much smaller.  We find it more convenient to explain the needed
queries while describing the algorithm itself.





\paragraph{Implementation of }
The pseudocode of the algorithm  is given as
Algorithm~\ref{alg:nlogn-findtd}.  Its correctness is proven as
Claim~\ref{claim:findtd-nlogn-correct}, and its time complexity is
proven as Claim~\ref{claim:findtd-nlogn-complexity}.  The subroutine
is provided with the data structure , and the following
invariants hold at each time the subroutine is called and exited:
\begin{itemize}
\item , ,
\item  exists, is unique and ,
\item  and
\item the state of the data structure is the same on exit as it was
  when the function was called.
\end{itemize}
The latter means that when we return, be it a tree decomposition or
, the algorithm that called  will have , , 
and  as they were before the call.

We now describe the consecutive steps of the algorithm ; the
reader is encouraged to follow these steps in the pseudocode, in order
to be convinced that all the crucial, potentially expensive
computations are performed by calls to the data structure.

First we apply query \qSsep, which either finds a
-balanced -separator in  of size at most
, or concludes that .  The running time of this query
is .  If no such separator can be found, by
Lemma~\ref{lemma:halfhalf} we infer that  and we
can terminate the procedure.  Otherwise we are provided with such a
separator , which we add to  in the data structure.
Moreover, for a technical reason, we also add the pin  to 
(and thus also to ), so we end up with having .

The next step is a loop through the connected components of .  This part is performed using the query
\qpin.  Query \qpin, which runs in constant time, either finds an
arbitrary vertex  of a connected component of  that does not contain any vertex from , or concludes
that each of these components contains some vertex of .  After
finding , we mark  by putting it to  and proceed further,
until all the components are marked.  Having achieved this, we have
obtained a list , containing exactly one vertex from each
connected component of .  We remove
all the vertices on this list from , thus making  again empty.

It is worth mentioning that the query \qpin{} not only returns some
vertex  of a connected component of  that does not contain any vertex from , but also provides
the size of this component as the second coordinate of the return
value.  Moreover, the components are being found in decreasing order
with respect to sizes.  In this algorithm we do not exploit this
property, but it will be crucial for the linear-time algorithm.

The set  will no longer be used, so we remove all the vertices of
 from , thus making it again empty.  On the other hand, we
add all the vertices from  to .  The new set  obtained in
this manner will constitute the new bag, of size at most
.  We are left with computing the tree
decompositions for the connected components below this bag, which are
pinpointed by vertices stored in the list .

We iterate through the list  and process the components one by
one.  For each vertex , we set  as the new pin by
unmarking the old one and marking .  Note that the set  gets
redefined and now is the connected component containing considered
.  First, we find the neighborhood of  in .  This is done
using query \qnei, which in  time returns either this
neighborhood, or concludes that its cardinality is larger than .
However, as  was a -balanced -separator, it follows
that this neighborhood will always be of size at most  (a formal
argument is contained in the proof of correctness).  We continue with
 as our new  and recursively call  in order
to decompose the connected component under consideration, with its
neighborhood in  as the root bag of the constructed tree
decomposition.   either provides a decomposition by returning
a pointer to its root bag, or concludes that no decomposition can be
found.  If the latter is the case, we may terminate the algorithm
providing a negative answer.

After all the connected components are processed, we merge the
obtained tree decompositions.  For this, we use the function
 which, given sets of vertices  and  and a set of
pointers , constructs two bags  and , makes
 the children of ,  the child of  and returns a pointer
to .  This pointer may be returned from the whole subroutine, after
doing a clean-up of the data structure.

\begin{algorithm}[h!]



  \KwData{Data structure }
  \KwOut{Tree decomposition of width at most  of  with  as root bag or conclusion that .}
\BlankLine
\Indp
  \\
  \\
  \\
  \If{}{
    \KwRet  \hspace{1cm}/*\textsf{\footnotesize{ safe to return: the 
    state not changed }}*/\\
  }
  \\
  \\
  \\
  \While{}{
    \\
    
  }
  \\
  \\
  \\
  \\
  \For{}{
    \\
    \\
  }
  \\
  \For{}{
    \\
    \\
    \\
    
  }
  \\
  \\
  \\
  \If{}{
    \KwRet  \hspace{1cm}/*\textsf{\footnotesize{ postponed because of 
    rollback of  and }} */\\
  }  
  \KwRet 
  \caption{}
  \label{alg:nlogn-findtd}
\end{algorithm}

\paragraph{Invariants}
Now we show that the stated invariants indeed hold.  Initially  and , so clearly the invariants are
satisfied.  If no -separator is found, the algorithm returns
without changing the data structure and hence the invariants trivially
hold in this case.  Since both  and  are empty or cleared before
return or recursing,  holds.  Furthermore, as 
is reset to  (consult Algorithm~\ref{alg:nlogn-findtd} for the
variable names used) and the pin to  before returning, it
follows that the state of the data structure is reverted upon
returning.

The size of  is trivially less than  when initialized.
Assume that for some call to  we have that .  When recursing,  is the neighborhood of some component 
of  (note that we refer to
 before resetting the pin).  This component is contained in some
component  of , and all the
vertices of  adjacent to  must be contained in .  Since
 is a -balanced -separator, we know that
 contains at most  vertices of .
Hence, when recursing we have that  and, since  is
an integer, it follows that .

Finally, we argue that the pin  is never contained in .  When
we obtain the elements of  (returned by query \qpin) we know
that  and the data structure guarantees that the pins will
be from .  When recursing,
 and , so it follows
that .  Assuming , it follows that
 is not in  when returning, and our argument is complete.
From here on we will safely assume that the invariants indeed hold.

\paragraph{Correctness}
\begin{claim}
  \label{claim:findtd-nlogn-correct}
  The algorithm  is correct, that is
  \begin{enumerate}[(a)]
  \item\label{item:1:claim:findtd-nlogn-correct} if ,
     returns a valid tree decomposition of  of
    width at most  and
  \item\label{item:2:claim:findtd-nlogn-correct} if  returns
     then .
  \end{enumerate}
\end{claim}

\begin{proof}
  We start by proving (\ref{item:2:claim:findtd-nlogn-correct}).
  Suppose the algorithm returns .  This happens when at some
  point we are unable to find a balanced -separator for an induced
  subgraph .  By Lemma~\ref{lemma:halfhalf} the
  treewidth of  is more than .  Hence  as well.
  
  To show (\ref{item:1:claim:findtd-nlogn-correct}) we proceed by
  induction.  In the induction we prove that the algorithm creates a
  tree decomposition, and we therefore argue that the necessary
  conditions are satisfied, namely
  \begin{itemize}
  \item the bags have size at most ,
  \item every vertex and every edge is contained in some bag and
  \item for each  the subtree of bags containing  is
    connected.
  \end{itemize}
  The base case is at the leaf of the obtained tree decomposition,
  namely when .  Then we return a tree
  decomposition containing two bags,  and  where 
  and .  Clearly, every edge and every vertex of
   is contained in the tree
  decomposition.  Furthermore, since the tree has size two, the
  connectivity requirement holds and finally, since 
  (invariant) and  it follows that .  Note that due to the definition of the base case, the
  algorithm will find no pins and hence it will not recurse further.
  
The induction step is as follows.  Since ,
  the algorithm have found some pins  and
  the corresponding components  in .  Let .
  By the induction hypothesis the algorithm gives us valid tree
  decompositions  of .  Note that the root bag
  of  consists of the vertices in .  By the same argument
  as for the base case, the two bags  and 
  that we construct have appropriate sizes.


  Let  be an arbitrary vertex of .  If , then
  it is contained in .  Otherwise there exists a unique  such
  that .  It then follows from the induction hypothesis
  that  is contained in some bag of .
  
It remains to show that the edge property and the connectivity
  property hold.  Let  be an arbitrary edge of .  If
   and  both are in , then the edge is contained in
  .  Otherwise, assume without loss of generality that  is in
  some component .  Then  and  are in  and
  hence they are in some bag of  by the induction hypothesis.
  
Finally, for the connectivity property, let  be some vertex in .  If , then there is a unique  such
  that , hence we can apply the induction hypothesis.  So
  assume that .  Let  be some bag of 
  containing .  We will complete the proof by proving that there is
  a path of bags containing  from  to .  If  is  or
  , then this follows directly from the construction.  Otherwise
  there exists a unique  such that  is a bag in .
  Observe that  is in  as it is in .  By the
  induction hypothesis the bags containing  in  are
  connected and hence there is a path of bags containing  from 
  to the root bag  of .  By construction  contains 
  and the bags  and  are adjacent.  Hence there is a path of
  bags containing  from  to  and as  was arbitrary
  chosen, this proves that the bags containing  form a connected
  subtree of the decomposition.  This concludes the proof of
  Claim~\ref{claim:findtd-nlogn-correct}.
\end{proof}

\paragraph{Complexity}
\begin{claim}
  \label{claim:findtd-nlogn-complexity}
  The invocation of  in the algorithm  runs in
   time for some .
\end{claim}
\begin{proof}
  We will prove the complexity of the algorithm by first arguing that
  the constructed tree decomposition contains at most  bags.  Then
  we will partition the used running time between the bags, charging
  each bag with at most  time.  It then follows
  that that  runs in  time.
  


  To bound the number of bags we simply observe that at each recursion
  step, we add the previous pin to  and create two bags.  Since a
  vertex can only be added to  one time during the entire process,
  at most  bags are created.
  
  \newcommand{\reccall}{\mathcal{C}}
  
  It remains to charge the bags.  For a call  to ,
  let  and  be as previously and let  be the root bag of
  .  We will charge  and  for the time
  spent on .  Notice that as  will correspond to  in
  the next recursion step, each bag will only be charged by one call
  to .  We charge  with everything in  not
  executed in the two loops iterating through the components, plus
  with the last call to  that returned .  Since every
  update and query in the data structure is executed in  time, and there is a constant number of queries charged to
  , it follows that  is charged with 
  time.  For each iteration in one of the loops we consider the
  corresponding  and charge the bag  with the time spent
  on this iteration.  As all the operations in the loops can be
  performed in  time, each  is charged with
  at most  time.  Since our tree decomposition has
  at most  bags and each is charged with at most  time, it follows that  runs in 
  time and the proof is complete.
\end{proof}

\newcommand{\forg}{\texttt{forgotten}}

\section{ 5-approximation algorithm for
  treewidth}
\label{section:logi}

In this section we provide formal details of the proof of
Theorem~\ref{theorem:nlogin}
\begin{theorem}[Theorem~\ref{theorem:nlogin}, restated]
  For every , there exists an algorithm which,
  given a graph  and an integer , in  time for some  either computes
  a tree decomposition of  of width at most  or correctly
  concludes that .
\end{theorem}

In the proof we give a sequence of algorithms  for
;  has been already presented in the
previous section.  Each  in fact solves a slightly more
general problem than stated in Theorem~\ref{theorem:nlogin}, in the
same manner as  solved a more general problem than the one
stated in Theorem~\ref{theorem:nlogn}.  Namely, every algorithm
 gets as input a connected graph , an integer  and
a subset of vertices  such that  and  is connected, and either concludes that  or constructs
a tree decomposition of width at most  with  as the root
bag.  The running time of  is  for some ; hence, in order to
prove Theorem~\ref{theorem:nlogin} we can again apply 
to every connected component of  separately, using .

The algorithms  are constructed inductively; by that we
mean that  will call , which again will
call , and all the way until , which was
given in the previous section.  Let us remark that a closer
examination of our algorithms in fact shows that the constants  in
the bases of the exponents of consecutive algorithms can be bounded by
some universal constant.  However, of course the constant factor
hidden in the -notation depends on .

In the following we present a quick outline of what will be given in
this section.  For , we refer to the previous section, and
for ,  and  are described
in this section, in addition to the subroutine .



\begin{itemize}
\item  takes as input a graph , an integer  and a
  vertex set  with similar assumptions as in the previous
  section, and returns a tree decomposition  of  of width at
  most  with  as the root bag.  The algorithm is almost
  exactly as  given as Algorithm~\ref{alg:nlogn-alg-1},
  except that it uses  for the compression step.
\item  is an advanced version of 
  (see Algorithm~\ref{alg:nlogn-compress}), it allows  to be of
  size up to  and gives a tree decomposition of width at most
   in time .  It starts by
  initializing the data structure, and then it calls , which
  returns a tree decomposition  of an induced subgraph .  The properties of  and  are as follows.
  All the connected components  of 
  are of size less than .  Furthermore, for every connected
  component , the neighborhood  in  is contained in a
  bag of .  Intuitively, this ensures that we are able to
  construct a tree decomposition of  and attach it to 
  without blowing up the width of .  More precisely, for every
  connected component , the algorithm constructs the induced
  subgraph  and calls  on
  , , and . The size of  will be bounded by
  , making the recursion valid with respect to the invariants of
  . If this call returned a tree decomposition 
  with a root bag , we can conveniently attach  to
  ; otherwise we conclude that  so
   as well.
\item  differs from  in two ways.  First, we use
  the fact that when enumerating the components separated by the
  separator using query , these components are identified in
  the descending order of cardinalities.  We continue the construction
  of partial tree decomposition in the identified components only as
  long as they are of size at least , and we terminate the
  enumeration when we encounter the first smaller component.  It
  follows that all the remaining components are smaller then ;
  these remainders are exactly the components  that
  are left not decomposed by , and on which
   is run.
  
  The other difference is that the data structure has a new
  \emph{flag}, , which is set to either  or  and is
  alternated between calls.  If , we use the same type
  of separator as  did, namely , but if , then we use the (new) query .  Query , instead
  of giving a balanced -separator, provides a
  -balanced -separator, that is, a separator that
  splits the whole set  of vertices to be decomposed in a balanced
  way.  Using the fact that on every second level of the decomposition
  procedure the whole set of available vertices shrinks by a constant
  fraction, we may for example observe that the resulting partial tree
  decomposition will be of logarithmic depth.  More importantly, it
  may be shown that the total number of constructed bags is at most
   and hence we can spend  time
  constructing each bag and still obtain running time linear in .
\end{itemize}

In all the algorithms that follow we assume that the cardinality of
the edge set is at most  times the cardinality of the vertex set,
because otherwise we may immediately conclude that treewidth of the
graph under consideration is larger than  and terminate the
algorithm.

\subsection{The main procedure }

The procedure  works exactly as , with the
exception that it applies Lemma~\ref{lem:prelim:bodlaender} for
parameter  instead of , and calls recursively
 and  instead of  and
.  The running time analysis is exactly the same, hence
we omit it here.

\subsection{Compression algorithm}

The following lemma explains the behavior of the compression algorithm
.

\begin{lemma}
  \label{lemma:nlogin-compression}
  For every integer  there exists an algorithm, which on
  input , where (i) , , (ii)  and  are connected, and (iii)
   is a tree decomposition of  of width at most ,
  in  time for some  either computes a tree decomposition  of  with
   and  as the root bag, or correctly
  concludes that .
\end{lemma}

The outline of the algorithm  for  is
given as Algorithm~\ref{alg:nlogin-compress-i}.  Having initialized
the data structure using , the algorithm asks  for
a partial tree decomposition , and then the goal is to decompose
the remaining small components and attach the resulting tree
decompositions in appropriate places of .

First we traverse  in linear time and store information on where
each vertex appearing in  is forgotten in .  More
precisely, we compute a map ,
where for every vertex  of  we either store  if it is not
contained in , or we remember the top-most bag  of 
such that  (the connectivity requirement of the tree
decomposition ensures that such  exists and is unique).  The map
 may be very easily computed via a DFS traversal of the tree
decomposition: when accessing a child node  from a parent , we
put  for each .  Moreover, for
every , where  is the root node, we put .
Clearly, all the vertices not assigned a value in  in this
manner, are not contained in any bag of , and we put value
 for them.  Let  be the set of vertices contained in ,
i.e., .

Before we continue, let us show how the map  will be used.
Suppose that we have some set , and we have a guarantee
that there exists a node  of  such that  contains the
whole .  We claim the following: then one of the bags associated
with  for  contains the whole .  Indeed, take the
path from  to the root of the tree decomposition , and
consider the last node  of this path whose bag contains the whole
.  It follows that  for some  and , so the claim follows.  Hence, we can locate the bag
containing  in  time by testing each of 
candidate nodes  for .

The next step of the algorithm is locating the vertices which has not
been accounted for, i.e., those assigned  by .  The
reason each of these vertices has not been put into the tree
decomposition, is precisely because the size of its connected
component  of , is smaller than .  The
neighborhood of this component in  is , and this neighborhood
is guaranteed to be of size at most  and contained in some bag
of  (a formal proof of this fact will be given when presenting
the algorithm , i.e., in Lemma~\ref{lemma:findPTD}).

Let  be all the connected components of ,
i.e., the connected components \emph{outside} the obtained partial
tree decomposition .  To complete the partial tree decomposition
into a tree decomposition, for every connected component , we
construct a graph  that we then aim to
decompose.  These graphs may be easily identified and constructed in
 time using depth-first search as follows.

We iterate through the vertices of , and for each vertex  such
that  and  was not visited yet, we apply a
depth-first search on  to identify its component .  During this
depth-first search procedure, we terminate searching and return from a
recursive call whenever we encounter a vertex from .  In this
manner we identify the whole component , and all the visited
vertices of  constitute exactly .  Moreover, the edges
traversed while searching are exactly those inside  or between 
and .  To finish the construction of , it remains to
identify edges between vertices of .  Recall that we have a
guarantee that  and  is contained in some bag
of .  Using the map  we can locate some such bag in
 time, and in  time check which vertices of
 are adjacent in it, thus finishing the construction of .
Observe that during the presented procedure we traverse each edge of
the graph at most once, and for each of at most  components  we
spend  time on examination of .  It follows that
the total running time is .



Having constructed , we run the algorithm  on
 using .  Note that in this manner we have that both
 and  are connected, which are requirements of
the algorithm .  If  concluded that
, then we can consequently answer that  since
 is an induced subgraph of .  On the other hand, if
 provided us with a tree decomposition  of
 having  as the root bag, then we may simply attach this
root bag as a child of the bag of  that contains the whole
.  Any such bag can be again located in  time
using the map .

\begin{algorithm}\KwIn{Connected graph , , a set  s.t.
     and  is connected, and a tree decomposition
     with }
  
  \KwOut{Tree decomposition of  of width at most  with
     as the root bag, or conclusion that .}
  
  \Indp \BlankLine
  Initialize data structure  with \\
  
  \\
  \If{}{ \KwRet{} }
  
  Create the map  using a DFS traversal of \\
  
  Construct components  of , and graphs  for \\
  
  \BlankLine \For{}{
    
     on \\
    
    \If{}{
      
      \KwRet{}
      
    }
    
    Locate a node  of  s.t.  , by checking
     for each \\
    Attach the root of  as a child of  }
  
  \KwRet{}
  
  \caption{}
  \label{alg:nlogin-compress-i}
\end{algorithm}

\subsubsection{Correctness and complexity}
In this section we prove Lemma~\ref{lemma:nlogin-compression} and
Theorem~\ref{theorem:nlogin}, and we proceed by induction on .
To this end we will assume the correctness of
Lemma~\ref{lemma:findPTD}, which will be proved later, and which
describes behavior of the subroutine .

For the base case, , we use  given as
Algorithm~\ref{alg:nlogn-compress}.  When its correctness was proved
we assumed  and this is no longer the case.  However,
if  is applied with  it will conclude that
 or give a tree decomposition of width at most .
The reason is as follows; Assume that  is applied with the
invariant  instead of .  By the same argument as
in the original proof this invariant will hold, since
.  The only part of the correctness
(and running time analysis) affected by this change is the width of
the returned decomposition, and when the algorithm adds the separator
to  it creates a bag of size at most  and
hence our argument for the base case is complete.  For the induction
step, suppose that the theorem and lemma hold for .  We
show that  is correct and runs in  time.  This immediately implies correctness and
complexity of , in the same manner as in
Section~\ref{section:nlogn}.





To prove correctness of , suppose that  is a
valid tree decomposition for some  that we have
obtained from .  Observe that if , then  by Lemma~\ref{lemma:findPTD}.  Otherwise, let 
be the connected components of , and let  for .  Let  be the tree decompositions
obtained from application of the algorithm  on graphs
.  If  for any , we infer that  and,
consequently, .  Assume then that for all the components we
have indeed obtained valid tree decompositions, with  as root
bags.  It can be easily seen that since  separates  from
the rest of , then attaching the root of  as a child of any
bag containing the whole  gives a valid tree decomposition;
the width of this tree decomposition is the maximum of widths of
 and , which is at most .  Moreover, if we perform
this operation for all the components , then all the vertices and
edges of the graph will be contained in some bag of the obtained tree
decomposition.

We now proceed to the time complexity of .  The
first thing done by the algorithm is the initialization of the data
structure and running  to obtain .  Application of
 takes  time by Lemma~\ref{lemma:findPTD}, and so
does initialization of the data structure (see
Section~\ref{section:datastructure}).  As discussed, creation of the
 map and construction of the graphs  takes
 time.



Now, the algorithm applies  to each graph .  Let
 be the number of vertices of .  Note that

Moreover, as , it follows from concavity of  that


By the induction hypothesis, the time complexity of 
on  is , hence we spend
 time for .  Attaching
each decomposition  to  can be done in 
time.

Let  denote the complexity of  and
 the complexity of .  By applying the
induction hypothesis we analyze the complexity of 
(we use a constant  to hide polynomial factors depending on
):


We conclude that  is both correct and that it runs
in  time for some .
The correctness and time complexity  of
 follow in the same manner as in the previous section.
And hence our induction step is complete and the correctness of
Lemma~\ref{lemma:nlogn-compression} and Theorem~\ref{theorem:nlogin}
follows.  The only assumption we made was that of the correctness of
Lemma~\ref{lemma:findPTD}, which will be given immediately.


\subsection{The algorithm }

The following lemma describes behavior of the subroutine .

\begin{lemma}
  \label{lemma:findPTD}
  There exists an algorithm that, given data structure  in a
  state such that  if  or 
  if , in time  either concludes that , or give a tree decomposition  of  such that
  \begin{itemize}
  \item the width of the decomposition is at most  and  is
    its root bag;
  \item for every connected component  of , the
    size of the component is less than , its neighborhood is
    of size at most , and there is a bag in the decomposition
     containing this whole neighborhood.
  \end{itemize}
\end{lemma}

The pseudocode of  is presented as
Algorithm~\ref{alg:nlogin-findptd}.  The algorithm proceeds very
similarly to the subroutine , given in
Section~\ref{section:nlogn}.  The main differences are the following.
\begin{itemize}
\item We alternate usage of  and  between the levels
  of the recursion to achieve that the resulting tree decomposition is
  also balanced.  A special flag in the data structure, ,
  that can be set to  or , denotes whether we are currently
  about to use  or , respectively.  When initializing
  the data structure we set , so we start with finding a
  balanced -separator.
\item When identifying the next components using query , we
  stop when a component of size less than  is discovered.  The
  remaining components are left without being decomposed.
\end{itemize}
The new query , provided that we have the data structure with
 and  distinguished, gives a -balanced separator
of  in  of size at most .  That is, it returns a subset
 of vertices of , with cardinality at most , such that
every connected component of  has at most
 vertices.  If such a separator cannot be found (which
is signalized by ), we may safely conclude that 
and, consequently .  The running time of query  is
.

We would like to remark that the usage of balanced -separators make
it not necessary to add the pin to the obtained separator.  Recall
that this was a technical trick that was used in
Section~\ref{section:nlogn} to ensure that the total number of bags of
the decomposition was linear.

\begin{algorithm}[h!]
\KwData{Data structure } \KwOut{Partial tree decomposition of
    width at most  of  with  as root bag or conclusion
    that .}
\Indp
  \BlankLine
  \\
  \\
  
  
  \eIf{}{
    \\
     }{
    \\
     }

  \If{}{
    \\
    \KwRet  \hspace{1cm}/*\textsf{\footnotesize{ safe to return: the 
    state not changed } }*/\\
  }
  \\
\\
  
  
  \While{}{
    \\
    \\
  }
  
  \\
  \\
  \\
  \\
  \For{}{
    \\
    \\
  }
  \\
  
    
  \For{}{
    \\
    \\
    \\
     }
  
  
  
   \\
  \\
  \\
  \If{}{
    \KwRet   \hspace{1cm}/*\textsf{\footnotesize{ postponed because of 
    rollback of  and  } }*/\\ }
    \KwRet 
 
  \caption{}
  \label{alg:nlogin-findptd}
\end{algorithm}

\subsubsection{Correctness}
The invariants of Algorithm~\ref{alg:nlogin-findptd} are as for
Algorithm~\ref{alg:nlogn-findtd}, except for the size of , in which
case we distinguish whether  is  or .  In the case of
 the size of  is at most  and for  the size of  is
at most .

If  then, since  and we add an -separator
of size at most  and make this our new , the size of the new
 will be at most  and we set .  For every
component  on which we recurse, the cardinality of its
neighborhood ( at the moment of recursing) is therefore bounded by
.  So the invariant holds when .

We now show that the invariant holds when .  Now
.  We find -balanced -separator
 of size at most .  When recursing, the new  is the
neighborhood of some component  of  (note that we refer to  before resetting the
pin).  This component is contained in some component  of , and all the vertices of  adjacent to
 must be contained in .  Since  is a
-balanced -separator, we know that  contains
at most  vertices of .  Hence, when
recursing we have that  and, since  is an
integer, it follows that .  Hence, the invariant also
hold when .

Note that in both the checks we did not assume anything about the size
of the component under consideration.  Therefore, it also holds for
components on which we do not recurse, i.e., those of size at most
, that the cardinalities of their neighborhoods will be
bounded by .

The fact that the constructed partial tree decomposition is a valid
tree decomposition of the subgraph induced by vertices contained in
it, follows immediately from the construction, similarly as in
Section~\ref{section:nlogn}.  A simple inductive argument also shows
that the width of this tree decomposition is at most : at each
step of the construction, we add two bags of sizes at most
 to the obtained decompositions of the
components, which by inductive hypothesis are of width at most .

Finally, we show that every connected component of 
has size at most  and that the neighborhood of each of these
connected component is contained in some bag on the partial tree
decomposition .  First, by simply breaking out of the loop shown
in Algorithm~\ref{alg:nlogin-findptd} at the point we get a pair
 such that , we are guaranteed that the
connected component of  containing 
has size less than , and so does every other connected
component of  not containing a vertex from  and which
has not been visited by .  Furthermore, since immediately
before we break out of the loop due to small size we add 
to a bag, we have ensured that the neighborhood of any such small
component is contained in this bag.  The bound on the size of this
neighborhood has been already argued.




\subsubsection{Complexity}
Finally, we show that the running time of the algorithm is .  The data structure operations all take time  and
we get the data structure  as input.

The following combinatorial lemma will be helpful to bound the number
of bags in the tree decomposition produced by .  We aim to
show that the tree decomposition  contains at most  bags, so we will use the lemma with , where
 is a node in a tree decomposition  and  is the number
of vertices in  when  is added to .  Having proven the
lemma, we can show that the number of bags is bounded by , where  is the root node of .

\begin{lemma}
  \label{lemma:bounded-tree}
  Let  be a rooted tree with root .  Assume that we are given a
  measure  with the following properties:
  \begin{enumerate}[(i)]
  \item[(i)]  for every ,
  \item[(ii)] for every vertex , let  be its
    children, we have that , and
  \item[(iii)] there exists a constant  such that for for every
    two vertices  such that  is a parent of , it holds
    that .
  \end{enumerate}
  Then .
\end{lemma}
\begin{proof}
  We prove the claim by induction with respect to the size of .
  If , the claim trivially follows from property (i).  We
  proceed to the induction step.
  
  Let  be the children of  and let
   be subtrees rooted in ,
  respectively.  If we apply the induction hypothesis to trees
  , we infer that for each  we have
  that .  By
  summing the inequalities we infer that:
  
  We now consider two cases.  Assume first that ; then:
  
  and we are done.  Assume now that ; then
  
  and we are done as well.
\end{proof}

We now prove the following claim.

\newcommand{\sm}{\textrm{small}}
\newcommand{\lr}{\textrm{large}}
\newcommand{\inte}{\textrm{int}}
\newcommand{\leaf}{\textrm{leaf}}

\begin{claim}~\label{claim:sublinear-bags} The partial tree
  decomposition  contains at most  nodes.
\end{claim}
\begin{proof}
  Let us partition the set of nodes  into two subsets.  At
  each recursive call of , we create two nodes: one
  associated with the bag , and one associated with the bag
  .  Let  be the set of nodes associated with
  bags , and let  the the set of remaining bags,
  associated with bags .  As bags are always
  constructed in pairs, it follows that
  .  Therefore, it remains to
  establish a bound on .
  
  For a node , let  be the number of vertices
  strictly below  in the tree decomposition , also counting
  the vertices outside the tree decomposition.  Note that by the
  construction it immediately follows that  for each
  .
  
  We now partition  into three parts: ,
  , and .   consists of all
  the nodes created in recursive calls where .
   consists of all the nodes created in recursive
  calls where , and moreover the algorithm did not make
  any more recursive calls to  (in other words, all the
  components turned out to be of size smaller than ).
   consists of all the remaining nodes created in
  recursive calls where , that is, such that the algorithm
  made at least one more call to .  We aim at bounding the
  size of each of the sets , , and
   separately.
  
  We first claim that .  Indeed, we
  have that the sets of vertices strictly below nodes of
   are pairwise disjoint.
  And since any bag in  is a subset of its parent and the
  recursive call to create the bag was made we know that there is at least
   vertices below. As their total union is of size at most , the
  claim follows.
  
  We now claim that .  Indeed, if with
  every node  we associate any of its grandchild
  belonging to , whose existence is guaranteed by the
  definition of , we obtain an injective map from
   into .
  
  We are left with bounding .  For this, we make use of
  Lemma~\ref{lemma:bounded-tree}.  Recall that vertices of 
  are exactly those that are in levels whose indices are congruent to
   modulo , where the root has level ; in particular, .  We define a rooted tree  as follows.  The vertex set
  of  is , and for every two nodes 
  such that  is an ancestor of  exactly  levels above
  (grand-grand-grand-parent), we create an edge between  and .
  It is easy to observe that  created in this manner is a rooted
  tree, with  as the root.
  
  We can now construct a measure  by taking
  .  Let us check that  satisfies the
  assumptions of Lemma~\ref{lemma:bounded-tree} for .
  Property (i) follows from the fact that  for every
  .  Property (ii) follows from the fact that the parts of
  the components on which the algorithm recurses below the bags are
  always pairwise disjoint.  Property (iii) follows from the fact that
  between every pair of parent, child in the tree  we have used a
  -balanced -separator.  Application of
  Lemma~\ref{lemma:bounded-tree} immediately gives that , and hence .
\end{proof}

To conclude the running time analysis of , we provide a
similar charging scheme as in Section~\ref{section:nlogn}.  More
precisely, we charge every node of  with 
running time; Claim~\ref{claim:sublinear-bags} ensures us that then
the total running time of the algorithm is then .

Let  and  be the two bags constructed at some
call of .  All the operations in this call, apart from the
two loops over the components, take  time and are
charged to .  Moreover, the last call of , when a component
of size smaller than  is discovered, is also charged to .
As this call takes  time,  is charged with  time in total.

We now move to examining the time spent while iterating through the
loops.  Let  be the root bag of the decomposition created for
graph .  We charge  with all the operations that were done
when processing  within the loops.  Note that thus every such
 is charged at most once, and with running time .  Summarizing, every bag of  is charged with  running time, and we have at most  bags, so the
total running time of  is .










\section{An  5-approximation algorithm for treewidth}
\label{section:linear}
In this section we give the main result of the paper.  The algorithm
either calls  for  or a version of
Bodlaender~\cite{Bodlaender96} applying a table lookup implementation
of the dynamic programming algorithm by Bodlaender and
Kloks~\cite{BodlaenderK96}, depending on how  and  relate.
These techniques combined will give us a -approximation algorithm
for treewidth in time single exponential in  and linear in .

\begin{theorem}[Theorem~\ref{thm:mainThm}, restated]
  There exists an algorithm, that given an -vertex graph  and an
  integer , in time  either outputs that the treewidth
  of  is larger than , or constructs a tree decomposition of 
  of width at most .
\end{theorem}



As mentioned above, our algorithm distinguishes between two cases.
The first case is when  is ``sufficiently small'' compared to .
By this, we mean that .  The other case is
when this is \emph{not} the case.  For the first case, we can apply
 and since  is sufficiently small compared to  we can
observe that , resulting in a  time
algorithm.  For the case when  is large compared to , we
construct a tree automata in time double exponential in .  In this
case, double exponential in  is in fact also linear in .  This
automaton is then applied on a nice expression tree constructed from
our tree decomposition and this results in an algorithm running in
time .

\begin{lemma}[Bodlaender and Kloks~\cite{BodlaenderK96}]
  There is an algorithm, that given a graph , an integer , and a
  nice tree decomposition of  of width at most  with 
  bags, either decides that the treewidth of  is more than , or
  finds a tree decomposition of  of width at most  in time
  .
  \label{lemma:BodlaenderKloks}
\end{lemma}

Our implementation with table lookup of this result gives the
following:

\begin{lemma}
  There is an algorithm, that given a graph , an integer , and a
  nice tree decomposition of  of width at most  with
   bags, either decides that the treewidth of  is more than 
  , or finds a tree decomposition of  of width at most , in time
  , for constants
   and .
  \label{lemma:tablelookupBodlaenderKloks}
\end{lemma}

The proof of Lemma~\ref{lemma:tablelookupBodlaenderKloks} will be
given later.  We first discuss how
Lemma~\ref{lemma:tablelookupBodlaenderKloks} combined with the results
in other sections imply the main result of the paper.  First we handle
the case when .  We then call  on
each connected component of  separately, with .  This
algorithm runs in time  time for some constants  and .

For the remaining of this section we will assume .
An inspection of Bodlaender's algorithm~\cite{Bodlaender96} shows that it
contains the following parts:
\begin{itemize}
\item A recursive call to the algorithm is made on a graph with  vertices, for .
\item The algorithm of Lemma~\ref{lemma:BodlaenderKloks} is called
  with .
\item Some additional work that uses time linear in  and
  polynomial in .
\end{itemize}

The main difference of our algorithm in the case of ``large '' is
that we replace the call to the algorithm of
Lemma~\ref{lemma:BodlaenderKloks} with a call to the algorithm of
Lemma~\ref{lemma:tablelookupBodlaenderKloks}, again with .  At some point in the recursion, instances will have size less
than .  At that point, we call  on this
instance; with the main difference that at one level higher in the
recursion, the algorithm of
Lemma~\ref{lemma:tablelookupBodlaenderKloks} is called with .

The analysis of the running time is now simple.  A call to the
algorithm of Lemma~\ref{lemma:tablelookupBodlaenderKloks} uses time
which is bounded by , i.e., time linear in  and polynomial in .  The
total work of the algorithm is thus bounded by a function  that
fulfills , for
constant , i.e., time polynomial in  and linear in .  This
proves our main result.  What remains for this section is a proof of
Lemma~\ref{lemma:tablelookupBodlaenderKloks}.

\subsection{Nice expression trees}
The dynamic programming algorithm in~\cite{BodlaenderK96} is described
with help of so called {\em nice tree decompositions}.  As we need to
represented a nice tree decomposition as a labeled tree with the label
alphabet of size a function of , we use a slightly changed notion
of {\em labeled nice tree decomposition}.  The formalism is quite
similar to existing formalisms, e.g., the operations on -terminal
graphs by Borie~\cite{Borie88}.

A labeled terminal graph is a 4-tuple , with  a
graph,  a set of {\em terminals}, and  an injective mapping of the terminals to
non-negative integers, which we call {\em labels}.  A -labeled
terminal graph is a labeled terminal graph with the maximum label at
most , i.e., .  Let  be the set of
the following operations on -terminal graphs.


\vskip 0.3cm
\noindent{\bf{Leaf():}} gives a -terminal graph with one vertex ,
  no edges, with  a terminal with label 
  \vskip 0.3cm
  \noindent{\bf{Introduce():}}  is a
  -terminal graph,  a non-negative integer, and  a set of labels.  If there is a terminal vertex in
   with label , then the operation returns , otherwise it
  returns the graph, obtained by adding a new vertex , making  a
  terminal with label , and adding edges  for each
  terminal  with .  I.e., we make the new vertex
  adjacent to each existing terminal whose label is in .  \vskip
  0.3cm
  \noindent{\bf{Forget{}():}} Again  is a
  -terminal graph.  If there is no vertex  with
  , then the operation returns , otherwise, we 'turn 
  into a non-terminal, i.e., we return the -terminal graph
   for the vertex  with , and  is
  the restriction of  to .  \vskip 0.3cm
  \noindent{\bf{Join(,):}}  and  are
  -terminal graphs.  If the range of  and  are not equal,
  then the operation returns .  Otherwise, the result is obtained
  by taking the disjoint union of the two graphs, and then identifying
  terminals with the same label.
\vskip 0.3cm Note that for given ,  is a collection of  operations.  When the treewidth is , we
  work with -terminal graphs.  The set of operations mimics
  closely the well known notion of nice tree decompositions (see
  e.g.,~\cite{Kloks93,Bodlaender98}).


\begin{proposition}
  Suppose a tree decomposition of  is given of width at most 
  with  bags.  Then, in time, linear in  and polynomial in ,
  we can construct an expression giving a graph isomorphic to  in
  terms of operations from  with the length of the expression
  .
\end{proposition}

\begin{proof}
  First build with standard methods a nice tree decomposition of 
  of width ; this has  bags, and  join nodes.  Now,
  construct the graph , with for all , , if and only if there is a bag  with .  It is well
  known that  is a chordal super graph of  with maximum clique
  size  (see e.g.,~\cite{Bodlaender98}).  Use a greedy linear
  time algorithm to find an optimal vertex coloring  of 
  (see~\cite[Section 4.7]{Golumbic80}.)
  
  Now, we can transform the nice tree decomposition to the expression
  as follows: each leaf bag that contains a vertex  is replaced by
  the operation Leaf, i.e., we label the vertex by its color
  in .  We can now replace bottom up each bag in the nice tree
  decomposition by the corresponding operation; as we labeled vertices
  with the color in , we have that all vertices in a bag have
  different colors, which ensures that a Join indeed performs
  identifications of vertices correctly.  Bag sizes are bounded by
  , so all operations belong to .
\end{proof}

View the expression as a labeled rooted tree, i.e., each node is
labeled with an operation from ; leaves are labeled with a
Leaf operation, and binary nodes have a Join-label.  To each node of
the tree , we can associate a graph , and the graph 
associated to the root node  is isomorphic to .  Call such a
labeled rooted tree a {\em nice expression tree} of width .

\subsection{Dynamic programming and finite state tree automata}
The discussion in this paragraph holds for all problems invariant
under isomorphism,.  Note that the treewidth of a graph is also
invariant under isomorphisms.  We use ideas from the early days of
treewidth, see e.g.,~\cite{FellowsL89,AbrahamsonF93}.

A dynamic programming algorithm on nice tree decompositions can be
viewed also as a dynamic programming algorithm on a nice expression
tree of width .  Suppose that we have a dynamic programming
algorithm that computes in bottom-up order for each node of the
expression tree a table with at most  bits per table, and to
compute a table, only the label of the node (type of operation) and
the tables of the children of the node are used.  We remark that the
DP algorithm for treewidth from Bodlaender and
Kloks~\cite{BodlaenderK96} is indeed of this form, if we see  as a
fixed constant.  Such an algorithm can be seen as a finite state tree
automaton: the states of the automaton correspond to the at most  different tables; the alphabet are the  different labels
of tree nodes.

To decide if the treewidth of  is at most , we first explicitly
build this finite state tree automaton, and then execute it on the
expression tree.  For actually building the corresponding tree
decomposition of  of width at most , if existing, some more work
has to be done, which is described later.

\subsection{Table lookup implementation of dynamic programming}
The algorithm of Bodlaender and Kloks~\cite{BodlaenderK96} builds for
each node in the nice tree decomposition of {\em table of
  characteristics}: each characteristic represents the `essential
information' of a tree decomposition of width at most  of the graph
associated with the bag.

Inspection of the algorithm~\cite{BodlaenderK96} easily shows that the
number of different characteristics is bounded by  when we are given an expression tree of width  and
want to test if the treewidth is at most .  (See~\cite[Definition
5.9]{BodlaenderK96}.)

We now use that we represent the vertices in the bag, i.e., the
terminals, by a label from  if  is the
width of the nice expression tree.  Thus, we have a set 
(only depending of  and ) that contains all possible
characteristics that belong to a table; each table is just a subset of
, i.e., an element of .  I.e., we
can view the decision variant of the dynamic programming algorithm of
Bodlaender and Kloks as a finite state tree automaton with alphabet
 and state set .

The first step of the algorithm is now to explicitly construct this
finite state tree automaton.  We can do this as follows.  Enumerate
all characteristics in , and number them , .  Enumerate all elements of
, and number them , ; store with  the elements of its set.

Then, we compute a transition function .  In
terms of the finite state automaton view,  gives the state of a
node given its symbol and the states of its children.  (If a node has
less than two children, the third, and possibly the second argument
are ignored.) In terms of the DP algorithm, if we have a tree node 
with operation , and the children of  have tables
corresponding to  and , then 
gives the number of the table obtained for  by the algorithm.  To
compute one value of , we just execute one part of the algorithm of
Bodlaender and Kloks~\cite{BodlaenderK96}.  Suppose we want to compute
.  If  is a shift operation, then a simple
renaming suffices.  Otherwise, build the tables  and
 corresponding to  and , and execute the
step of the algorithms from~\cite{BodlaenderK96} for a node with
operation  whose children have tables  and .
(If the node is not binary, we ignore the second and possibly both
tables.)  Then, lookup what is the index of the resulting table; this
is the value of .

We now estimate the time to compute .  We need to compute  values; each
executes one step of the DP algorithm and does a lookup in the table,
which is easily seen to be bounded again by , so the total time to compute  is still bounded by
.  To decide if the treewidth of  is
at most , given a nice tree decomposition of width at most ,
we thus carry out the following steps:
\begin{itemize}
\item Compute .
\item Transform the nice tree decomposition to a nice expression tree
  of width .
\item Compute bottom-up (e.g., in post-order) for each node  in the
  expression tree a value , with a node  labeled by operation
   and children with values , , we
  have .  If  has less than two
  children, we take some arbitrary argument for the values of missing
  children.  In this way,  corresponds to the table that is
  computed by the DP algorithm of~\cite{BodlaenderK96}.
\item If the value  for the root of the expression tree
  corresponds to the empty set, then the treewidth of  is more than
  , otherwise the treewidth of  is at most .
  (See~\cite[Section 4.6 and 5.6]{BodlaenderK96}.)
\end{itemize}

If our decision algorithm decides that the treewidth of  is more
than , we reject, and we are done.  Otherwise, we need to do
additional work to construct a tree decomposition of  of width at
most , which is described next.

\subsection{Constructing tree decompositions}
After the decision algorithm has determined that the treewidth of 
is at most , we need to find a tree decomposition of  of width
at most .  Again, the discussion is necessarily not self contained
and we refer to details given in~\cite[Chapter 6]{BodlaenderK96}.

Basically, each table entry (characteristic) in the table of a join
node is the result of a combination of a table entry in the table of
the left child and a table entry of the table of the right child.
Similarly, for nodes with one child, each table entry is the result of
an operation to a table entry in the table of the child node.  Leaf
nodes represent a graph with one vertex, and we have just one tree
decomposition of this graph, and thus one table entry in the table of
a leaf node.

If we select a characteristic of the root bag, this recursively
defines one characteristic from each table.  In the first phase of the
construction, we make such a selection.  In order to do this, we first
pre-compute another function , that helps to make this selection.
 has four arguments: an operation from , the index of a
characteristic (a number between  and ), and the indexes of two
states (numbers between  and ).  As value,  yields
 or a pair of two indexes of characteristics.  The intuition
is as follows: suppose we have a node  in the nice expression tree
labeled with , an index  of a characteristic of a (not yet
known) tree decomposition of , and indexes of the tables of the
children of , say  and .  Now,
 should give a pair  such
that  is the result of the combination of  and 
(in case that  is the join operation) or of the operation as
indicated above to  (in case  is an operation with one
argument;  can have any value and is ignored).  If no such
pair exists, the output is .

To compute , we can perform the following steps for each 4-tuple
.  Let  be the set
corresponding to , and  be the set
corresponding to .  For each  and , see
if a characteristic  and a characteristic  can be combined (or,
in case of a unary operation, if the relevant operation can be applied
to ) to obtain .  If we found a pair, we return it; if no
combination gives , we return .  Again, in case of unary
operations , we ignore .  We do not need  in case  is a
leaf operation, and can give any return values in such cases.  One can
easily see that the computation of  uses again  time.

The first step of our construction phase is to build , as
described.  After this, we select a characteristic from 
for each node in the nice expression tree, as follows.  As we arrived
in this phase, the state of the root bag corresponds to a nonempty set
of characteristics, and we take an arbitrary characteristic from this
set (e.g., the first one from the list).  Now, we select top-down in
the expression tree (e.g., in pre-order) a characteristic.  Leaf nodes
always receive the characteristic of the trivial tree decomposition of
a graph with one vertex.  In all other cases, if node  has
operation  and has selected characteristic , the left child of
 has state  and the right child of  has state 
(or, take any number, e.g., 1, if  has only one child, i.e.,  is
a unary operation), look up the precomputed value of
.  As  is a characteristic in the table
that is the result of , , so
suppose  is the pair .  We associate  as
characteristic with the left child of , and (if  has two
children)  as characteristic with the right child of .

At this point, we have associated a characteristic with each node in
the nice expression tree.  These characteristics are precisely the
same as the characteristics that are computed in the constructive
phase of the algorithm from~\cite[Section 6]{BodlaenderK96}, with the
sole difference that we work with labeled terminals instead of the
`names' of the vertices (i.e., in~\cite{BodlaenderK96}, terminals /
bag elements are identified as elements from ).

From this point on, we can follow without significant changes the
algorithm from~\cite[Section 6]{BodlaenderK96}: bottom-up in the
expression tree, we build for each node , a tree decomposition of
 whose characteristic is the characteristic we just selected for
, together with a number of pointers from the characteristic to the
tree decomposition.  Again, the technical details can be found
in~\cite{BodlaenderK96}, our only change is that we work with
terminals labeled with integers in  instead of
bag vertices.

At the end of this process, we obtain a tree decomposition of the
graph associated with the root bag  whose characteristic
belongs to the set corresponding to the state of .  As we only work
with characteristics of tree decompositions of width at most , we
obtained a tree decomposition of  of width at most .

All work we do, except for the pre-computation of the tables of 
and  is linear in  and polynomial in ; the time for the
pre-computation does not depend on , and is bounded by .  This ends the description of the proof of
Lemma~\ref{lemma:tablelookupBodlaenderKloks}.

\section{A data structure for queries in  time}
\label{section:datastructure}

\subsection{Overview of the data structure}
Assume we are given a tree decomposition  of  of width . First we turn our tree decomposition
into a tree decomposition of depth , keeping the width to
, by the work of Bodlaender and
Hagerup~\cite{BodlaenderH98}.  Furthermore, by standard arguments we
turn this decomposition into a {\emph{nice}} tree decomposition in
 time, that is, a decomposition of the same width
and satisfying following properties:
\begin{itemize}
\item All the leaf bags, as well as the root bag, are empty.
\item Every node of the tree decomposition is of one of four different
  types:
  \begin{itemize}
  \item {\bf{Leaf node}}: a node  with  and no children.
  \item {\bf{Introduce node}}: a node  with exactly one child 
    such that  for some vertex ; we say
    that  is {\emph{introduced}} in .
  \item {\bf{Forget node}}: a node  with exactly one child  such
    that  for some vertex ; we say
    that  is {\emph{forgotten}} in .
  \item {\bf{Join node}}: a node  with two children  such
    that .
  \end{itemize}
\end{itemize}
The standard technique of turning a tree decomposition into a nice one
includes (i) adding paths to the leaves of the decomposition on which
we consecutively introduce the vertices of corresponding bags; (ii)
adding a path to the root on which we consecutively forget the
vertices up to the new root, which is empty; (iii) introducing paths
between every non-root node and its parent, on which we first forget
all the vertices that need to be forgotten, and then introduce all the
vertices that need to be introduced; (iv) substituting every node with
 children with a balanced binary tree of  depth.  It
is easy to check that after performing these operations, the tree
decomposition has depth at most  and contains at most
 bags. Moreover, using folklore preprocessing routines,
in  time we may prepare the decomposition for
algorithmic uses, e.g., for each bag compute and store the list of
edges contained in this bag.  We omit here the details of this
transformation and refer to Kloks~\cite{Kloks93}.

In the data structure, we store a number of tables: three special
tables that encode general information on the current state of the
graph, and one table per each query.  The information stored in the
tables reflect some choice of subsets of , which we will call the
{\emph{current state of the graph}}.  More precisely, at each moment
the following subsets will be distinguished:  and a single
vertex , called the pin.  The meaning of these sets is described
in Section~\ref{section:nlogn}. On the data structure we can perform
the following updates: adding/removing vertices to  and
marking/unmarking a vertex as a pin.  In the following table we gather
the tables used by the algorithm, together with an overview of the
running times of updates.  The meaning of the table entries uses
terminology that is described in the following sections.

The following lemma follows from each of the entries in the table
below, and will be proved in this section:
\begin{lemma}
  \label{lemma:ds:init-time}
  The data structure can be initialized in  time.
\end{lemma}

\vskip 0.3cm

\noindent\begin{tabular}{| c | c | c | c |}
\hline
{\bf{Table}} & {\bf{Meaning}} & {\bf{Update}} & {\bf{Initialization}} \\
\hline
 & Boolean value  &  &  \\
\hline
 & Connectivity information on  &  &  \\
\hline
 & Integer value  &  &  \\
\hline
 & Table for query \qnei &  &  \\
\hline
 & Table for query \qSsep &  &  \\
\hline
 & Table for query \qpin &  &  \\
\hline
 & Table for query \qUsep &  &  \\
\hline
\end{tabular}

\vskip 0.3cm

We now proceed to description of the description of the table , and
then to the two tables  and  that handle the important
component .  The tables  are described together with the
description of realization of the corresponding queries.  Whenever
describing the table, we argue how the table is updated during updates
of the data structure, and initialized in the beginning.

\subsection{The table }

In the table , for every node  of the tree decomposition we store a
boolean value  equal to .  We now show how to
maintain the table  when the data structure is updated. The table 
needs to be updated whenever the pin  is marked or unmarked.
Observe, that the only nodes  for which the information whether
 changed, are the ones on the path from  to the
root of the tree decomposition.  Hence, we can simply follow this path
and update the values.  As the tree decomposition has depth , this update can be performed in  time.  As when
the data structure is initialized, no pin is assigned,  is
initially filled with .

\subsection{Maintaining the important component }

Before we proceed to the description of the queries, let us describe
what is the reason of introducing the pin .  During the
computation, the algorithm recursively considers smaller parts of the
graph, separated from the rest via a small separator: at each step we
have distinguished set  and we consider only one connected
component  of .  Unfortunately, we cannot afford
recomputing the tree decomposition of  at each recurrence call, or
even listing the vertices of .  Therefore we employ a different
strategy for identification of .  We will distinguish one vertex of
 as a representative pin , and  can then be defined as the
set of vertices reachable from  in .  Instead of
recomputing  at each recursive call we will simply change the
pin.

In the tables, for each node  of the tree decomposition we store
entries for each possible intersection of  with , and in this
manner we are prepared for every possible interaction of  with
.  In this manner, changing the pin can be done more efficiently.
Information needs to be recomputed on two paths to the root in the
tree decomposition, corresponding the previous and the next pin, while
for subtrees unaffected by the change we do not need to recompute
anything as the tables stored there already contain information about
the new  as well --- as they contain information for {\emph{every
    possible}} new .  As the tree decomposition is of logarithmic
depth, the update time is logarithmic instead of linear.

We proceed to the formal description.  We store the information about
 in two special tables:  and .  As we intuitively
explained, tables  and  store information on the
connectivity behavior in the subtree, for every possible interaction
of  with the bag.  Formally, for every node of the tree
decomposition  we store an entry for every member of the family of
{\emph{signatures}} of the bag .  A signature of the bag  is
a pair , such that  are disjoint subsets of .
Clearly, the number of signatures is at most .

Let  be a node of the tree decomposition.  For a signature
 of , let  and
 consists of all the vertices reachable in  from  or , providing that it belongs to .
Sets  and  are called {\emph{extensions}} of
the signature ; note that given  and , the extensions
are defined uniquely.  We remark here that the definition of
extensions depend not only on  but also on the node ; hence,
we will talk about extensions of signatures only when the associated
node is clear from the context.

We say that signature  of  with extensions  and
 is {\emph{valid}} if it holds that
\begin{itemize}
\item[(i)] ,
\item[(ii)] if  and  (equivalently,  is 
  true), then the
  component of  that contains  contains also at
  least one vertex of .
\end{itemize}
Intuitively, invalidity means that  cannot contain consistent
information about intersection of  and .  The second condition
says that we cannot fully forget the component of , unless the
whole  is already forgotten.

Formally, the following invariant explains what is stored in tables
 and :
\begin{itemize}
\item if  is invalid then ;
\item otherwise,  contains an equivalence relation 
  consisting of all pairs of vertices  that are
  connected in , while  contains
  .
\end{itemize}

Note that in this definition we actually ignore the information about
alignment of vertices of  to set  in the current state of
the graph: the stored information depends only on the alignment of forgotten
vertices and the signature of the bag that overrides the actual
information in the current state.  In this manner we are prepared for
possible changes in the data structure, as after an update some other
signature will reflect the current state of the graph.  Moreover, it
is clear from this definition that during the computation, the
alignment of every vertex  in the current state of the graph is
being checked only in the single node  when this vertex is being
forgotten; we use this property heavily to implement the updates
efficiently enough.

We now explain how for every node , values in tables  and
 can be computed using entries for the children of .
These formulas will be crucial both for implementing updates and
initialization.  We consider different cases, depending on the type of
node . 

\vskip 0.3cm
\noindent{\bf{Case 1: Leaf node.}} If  is a leaf node then
 and .  \vskip 0.3cm

\noindent{\bf{Case 2: Introduce node.}} Let  be a node that
introduces vertex , and  be its only child.  Consider some
signature  of ; we would like to compute
.  Let  be a natural projection of  onto
, that is, .  Let
.  We consider some sub-cases, depending on the
alignment of  in .

\vskip 0.1cm {\bf{Case 2.1: .}} If we introduce a vertex from
, then it follows that extensions of  are equal.
Therefore, we can put  and
.

\vskip 0.1cm {\bf{Case 2.2: .}} In the beginning we check
whether conditions of validity are not violated.  First, if  is the
only vertex of  and , then we simply put
: condition (ii) of validity is violated.  Second, we
check whether  is adjacent only to vertices of  and ; if
this is not the case, we put  as condition (i) of
validity is violated.

If the validity checks are satisfied, we can infer that the extension
 of  is extension  of  with 
added; this follows from the fact that  separates  from ,
so the only vertices of  adjacent to  are already
belonging to .  Now we would like to compute the equivalence
relation  out of .  Observe that  should be basically
 augmented by connections introduced by the new vertex 
between its neighbors in .  Formally,  may be obtained from
 by merging equivalence classes of all the neighbors of  from
, and adding  to the obtained equivalence class; if  does
not have any neighbors in , we put it as a new singleton
equivalence class.  Clearly, .

\vskip 0.1cm

{\bf{Case 2.3: .}} We first check whether the
validity constraints are not violated.  As  is separated from 
by , the only possible violation introduced by  is that  is
adjacent to a vertex from .  In this situation we put
, and otherwise we can put
 and , because
extensions of  and  are equal.

\vskip 0.3cm

\noindent{\bf{Case 3: Forget node.}} Let  be a node that forgets
vertex , and  be its only child.  Consider some signature
 of  and define extensions ,
 for this signature.  Observe that there is at most one
valid signature  of  for which
 and , and this
signature is simply  with  added possibly to  or ,
depending whether it belongs to  or : the
three candidates are ,
 and .
Moreover, if  is valid then so is .  Formally, in the
following manner we can define signature , or conclude that 
is invalid:
\begin{itemize}
\item if , then ;
\item otherwise, if  then ;
\item otherwise, we look into entries  and .  If
  \begin{itemize}
  \item[(i)]  then  is invalid, and we 
    put
    ;
  \item[(ii)] if  or , we take 
     or
    , respectively;
  \item[(iii)] if  and , it follows 
    that 
    must be a member of a component of  that
    is fully contained in  and does not contain .  Hence we
    take .
  \end{itemize}
\end{itemize}
The last point is in fact a check whether : whether 
is connected to a vertex from  in , can be looked up in
table  by adding or not adding  to , and checking the
stored connectivity information.  If  or , we should be using the information for the signature with
 or  updated with , and otherwise we do not need to add
 anywhere.

As we argued before, if  is valid then so does , hence if
 then we can take .
On the other hand, if  is valid, then the only possibility for
 to be invalid is when condition (ii) cease to be satisfied.
This could happen only if  and  is in a singleton
equivalence class of  (note that then the connected
component corresponding to this class needs to necessarily contain
, as otherwise we would have ).
Therefore, if this is the case, we put
, and otherwise we conclude that
 is valid and move to defining  and .

Let now .  As extensions of  and  are equal, it
follows directly from the maintained invariant that  is equal to
 with  removed from its equivalence class.  Moreover,
 is equal to , possibly incremented
by  if we concluded that .

\vskip 0.3cm

\noindent{\bf{Case 4: Join node.}} Let  be a join node and
 be its two children.  Consider some signature
 of .  Let  be a signature of
 and  be a signature of .  From
the maintained invariant it follows that  is a minimum
transitive closure of , or 
if any of these entries contains .  Similarly,
.

\vskip 0.3cm

We now explain how to update tables  and  in  time.  We perform a similar strategy as with
table : whenever some vertex  is included or
removed from , or marked or unmarked as a pin, we follow the path
from  to the root and fully recompute the whole tables 
in the traversed nodes using the formulas presented above.  At each
step we recompute the table for some node using the tables of its
children; these tables are up to date since they did not need an
update at all, or were updated in the previous step.  Observe that
since the alignment of  in the current state of the graph is
accessed only in computation for , the path from  to the
root of the decomposition consists of all the nodes for which the
tables should be recomputed.  Note also that when marking or unmarking
the pin , we must first update  and then  and .
The update takes  time:
re-computation of each table takes  time, and we
perform  re-computations as the tree decomposition has
depth .

Similarly, tables  and  can be initialized in  time by processing the tree in a bottom-up manner:
for each node of the tree decomposition, in 
time we compute its table based on the tables of the children, which
were computed before.

\subsection{Queries}

In our data structure we store one table per each query.  When
describing every query, we first introduce the formal invariant on
storage of table's entry, and how this stored information can be
computed based on the entries for children.  We then shortly discuss
performing updates and initialization of the tables, as they are in
all the cases based on the same principle as with tables  and
.  Queries themselves can be performed by reading a single
entry of the data structure, with the exception of query \qUsep, whose
implementation is more complex.



\subsubsection{Query \qnei}

\newcommand{\tldr}{\XBox}

We begin the description of the queries with the simplest one, namely
\qnei{}.  This query lists all the vertices of  that are adjacent
to .  In the algorithm we have an implicit bound on the size of
this neighborhood, which we can use to cut the computation when the
accumulated list grows too long.  We use  to denote this bound;
in our case we have that .

\defquery{\qnei}{A list of vertices of , or marker ''
  if their number is larger than .}{}

Let  be a node of the tree decomposition, let  be a
signature of , and let  be extensions of
this signature.  In entry  we store the following:
\begin{itemize}
\item if  is invalid then ;
\item otherwise  stores the list of elements of
   if there is at most  of them,
  and  if there is more of them.
\end{itemize}
Note that the information whether  is invalid can be looked up in
table .  The return value of the query is stored in .

We now present how to compute entries of table  for every node
 depending on the entries of children of .  We consider
different cases, depending of the type of node .  For every case,
we consider only signatures that are valid, as for the invalid ones we
just put value .  

\vskip 0.3cm
\noindent{\bf{Case 1: Leaf node.}} If  is a leaf node then
.  \vskip 0.3cm

\noindent{\bf{Case 2: Introduce node.}} Let  be a node that
introduces vertex , and  be its only child.  Consider some
signature  of ; we would like to compute
.  Let  be a natural intersection of 
with , that is, .  Let
.  We consider some sub-cases, depending on the
alignment of  in .

\vskip 0.1cm {\bf{Case 2.1: .}} If we introduce a vertex from
, we have that -extensions of  and  are equal.
It follows that  should be simply list  with  appended if
it is adjacent to any vertex of .  Note here that  cannot
be adjacent to any vertex of , as 
separates  from .  Hence, we copy the list  and append
 if it is adjacent to any vertex of  and .
However, if the length of the new list exceeds the  bound, we
replace it by .  Note that copying the list takes 
time, as its length is bounded by .

\vskip 0.1cm {\bf{Case 2.2: .}} If we introduce a vertex from
, then possibly some vertices of  gain a neighbor in
.  Note here that vertices of 
are not adjacent to the introduced vertex , as  separates 
from .  Hence, we copy list  and append to it all the
vertices of  that are adjacent to , but were not yet on .
If we exceed the  bound on the length of the list, we put
 instead.  Note that both copying the list and checking whether
a vertex of  is on it can be done in  time, as its
length is bounded by .

\vskip 0.1cm

{\bf{Case 2.3: .}} In this case extensions of 
and  are equal, so it follows from the invariant that we may
simply put .

\vskip 0.3cm

\noindent{\bf{Case 3: Forget node.}} Let  be a node that forgets
vertex , and  be its only child.  Consider some signature
 of .  Define  in the same manner as in
the Forget step in the computation of .  As extensions of 
and  are equal, it follows that .

\vskip 0.3cm

\noindent{\bf{Case 4: Join node.}} Let  be a join node and
 be its two children.  Consider some signature
 of .  Let  be a signature of
 and  be a signature of .  It
follows that  should be the merge of lists
 and , where we remove the
duplicates.  Of course, if any of these entries contains , we
simply put .  Otherwise, the merge can be done in 
time due to the bound on lengths of  and
, and if the length of the result exceeds the bound
, we replace it by .

\vskip 0.3cm

Similarly as before, for every addition/removal of vertex  to/from
, or marking/unmarking  as a pin, we can update table  in
 time by following the path from
 to the root and recomputing the tables in the traversed nodes.
Also,  can be initialized in  time
by processing the tree decomposition in a bottom-up manner and
applying the formula for every node.  Note that updating/initializing
table  must be performed after updating/initializing tables 
and .

\subsubsection{Query \qSsep}\label{sec:qssep}



We now move to the next query, namely finding a balanced
-separator.  By Lemma~\ref{lemma:halfhalf}, as  has 
treewidth at most , such a -balanced -separator
of size at most  always exists.  We therefore implement the following query.

\defquery{\qSsep}{A list of elements of a -balanced -separator of
   of size at most , or  if no such
  exists.}{}

Before we proceed to the implementation of the query, we show how to
translate the problem of finding a -balanced separator into a
partitioning problem.

\begin{lemma}[Lemma~\ref{lem:balanced-3coloring}, restated]
  Let  be a graph and .  Then a set  is a balanced
  -separator if and only if there exists a partition
   of , such that there is no edge
  between  and  for , and 
  for .
\end{lemma}

The following combinatorial observation is crucial in the proof of
Lemma~\ref{lem:balanced-3coloring}.

\begin{lemma}\label{lem:comb-balanced-3coloring}
  Let  be non-negative integers such that  and  for .  Then there exists
  a partition of these integers into three sets, such that sum of
  integers in each set is at most .
\end{lemma}
\begin{proof}
  Without loss of generality assume that , as otherwise the claim
  is trivial.
  We perform a greedy procedure as follows.  At each time step of the
  procedure we have a number of sets, maintaining an invariant that each
  set is of size at most .  During the procedure we gradually
  merge the sets, i.e., we take two sets and replace them with their
  union.  We begin with each integer in its own set.  If we arrive at
  three sets, we end the procedure, thus achieving a feasible
  partition of the given integers.  We therefore need to present how
  the merging step is performed.
  
  At each step we choose the two sets with smallest sums of elements
  and merge them (i.e., replace them by their union).  As the number
  of sets is at least , the sum of elements of the two chosen ones
  constitute at most half of the total sum, so after merging them we
  obtain a set with sum at most .  Hence, unless the number of
  sets is at most , we can always apply this merging step.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:balanced-3coloring}]
  One of the implications is trivial: if there is a partition
   of  with the given properties, then
  every connected component of  must be fully contained
  either in , , or , hence it contains at most 
  vertices of .  We proceed to the second implication.
  
  Assume that  is a balanced -separator of  and let
   be connected components of .  For
  , let .  By
  Lemma~\ref{lem:comb-balanced-3coloring}, there exists a partition
  of integers  into three sets, such that the sum of elements of
  each set is at most .  If we partition vertex sets of
  components  in the same manner, we obtain a
  partition  of  with postulated properties.
\end{proof}

Lemma \ref{lem:balanced-3coloring} shows that, when looking for a
balanced -separator, instead of trying to bound the number of
elements of  in each connected component of 
separately, which could be problematic because of connectivity
condition, we can just look for a partition of  into four
sets with prescribed properties that can be checked locally.  This
suggest the following definition of table .

In table  we store entries for every node  of the tree
decomposition, for every signature  of , and for
every -tuple  where
\begin{itemize}
\item  is a partition of ,
\item  are integers between  and ,
\item and  is an integer between  and .
\end{itemize}
This -tuple  will be called the {\emph{interface}}, and
intuitively it encodes the interaction of a potential solution with
the bag.  Observe that the set  is not given in our graph directly but
rather via connectivity information stored in table , so we need to
be prepared also for all the possible signatures of the bag; this is
the reason why we introduce the interface on top of the signature.
Note however, that the number of possible pairs  is at
most , so for every bag  we store
 entries.

We proceed to the formal definition of what is stored in table .
For a fixed signature  of , let
 be its extension, we say that partitioning
 of  is an {\emph{extension consistent}} with interface
, if:
\begin{itemize}
\item  and  for ;
\item there is no edge between vertices of  and
   for ;
\item  and  for .
\end{itemize}

In entry  we store:
\begin{itemize}
\item  if  is invalid or no consistent extension of  exists;
\item otherwise, a list of length  of vertices of 
  in some consistent extension of .
\end{itemize}

The query \qSsep can be realized in  time by checking entries
in the table , namely 

for all possible values  and , and
outputting the list contained in any of them that is not equal to
, or  if all of them are equal to .

We now present how to compute entries of table  for every node
 depending on the entries of children of .  We consider
different cases, depending of the type of node .  For every case,
we consider only signatures that are valid, as for the invalid ones we
just put value .  

\vskip 0.3cm
\noindent{\bf{Case 1: Leaf node.}} If  is a leaf node then
,
and all the other interfaces are assigned .  \vskip 0.3cm

\noindent{\bf{Case 2: Introduce node.}} Let  be a node that
introduces vertex , and  be its only child.  Consider some
signature  of  and an interface
; we would like to compute
.  Let  be natural intersections
of  with , respectively, that is,  and .  Let .  We consider
some sub-cases, depending on the alignment of  in  and .
The cases with  belonging to ,  and  are symmetric,
so we consider only the case for .

\vskip 0.1cm {\bf{Case 2.1: .}} Note that every extension
consistent with interface  is an extension consistent with
 after trimming to .  On the other hand, every extension
consistent with  can be extended to an extension consistent
with  by adding  to the extension of .  Hence, it follows
that we can simply take .

\vskip 0.1cm {\bf{Case 2.2: .}} Similarly as in the previous
case, every extension consistent with interface  is an extension
consistent with  after trimming to .  On the other hand,
if we are given an extension consistent with , we can add 
to  and make an extension consistent with  if and only if
 is not adjacent to any vertex of  or ; this follows from
the fact that  separates  from , so the only vertices
from ,  that  could be possibly adjacent
to, lie in .  However, if  is adjacent to a vertex of  or
, we can obviously put , as there is no extension
consistent with : property that there is no edge between
 and  is broken already in the
bag.  Otherwise, by the reasoning above we can put .

\vskip 0.1cm

{\bf{Case 2.3: .}} Again, in this case we have
one-to-one correspondence of extensions consistent with  with
 after trimming to , so we may simply put .

\vskip 0.3cm

\noindent{\bf{Case 3: Forget node.}} Let  be a node that forgets
vertex , and  be its only child.  Consider some signature
 of , and some interface
; we would like to compute
.  Let  be the only extension of 
signature  to  that has the same extension as
;  can be deduced by looking up which signatures are
found valid in table  in the same manner as in the forget step for
computation of table .  We consider three cases depending on
alignment of  in :

\vskip 0.1cm {\bf{Case 3.1: .}} If  is not in , then it follows that we may put :
extensions of  consistent with  correspond one-to-one to
extensions consistent with .

\vskip 0.1cm {\bf{Case 3.2: .}} Assume that there exist some
extension  consistent
with .  In this extension, vertex  is either in ,
, , or in .  Let us define the
corresponding interfaces:
\begin{itemize}
\item ;
\item ;
\item ;
\item .
\end{itemize}
If any of integers  turns out to be negative,
we do not consider this interface.  It follows that for at least one
 there must be an extension
consistent with : it is just the extension .  On the other hand, any extension
consistent with any of interfaces  is
also consistent with .  Hence, we may simply put
, and append  on the list in case
.

\vskip 0.1cm {\bf{Case 3.3: .}} We proceed in the same manner
as in Case 3.2, with the exception that we do not decrement  by
 in interfaces  for .

\vskip 0.3cm

\noindent{\bf{Case 4: Join node.}} Let  be a join node and
 be its two children.  Consider some signature
 of , and an interface
; we would like to compute
.  Let  be a signature of
 and  be a signature of .  Assume
that there is some extension  consistent with .  Define 
and  for  and ; note that
 for  and .  It follows that in
,  there are some extensions consistent with
 and
, respectively --- these are
simply extension 
intersected with , respectively.  On the other hand, if we
have some extensions in ,  consistent with
 and
 for numbers  such
that  for  and , then the
point-wise union of these extensions is an extension consistent with
.  It follows that in order to compute
, we need to check if for any such choice of  we have
non- entries in
 and
.  This is
the case, we put the union of the lists contained in these entries as
, and otherwise we put .  Note that computing the union of
these lists takes  time as their lengths are bounded by , and
there is  possible choices of  to check.

\vskip 0.3cm

Similarly as before, for every addition/removal of vertex  to/from
 or marking/unmarking  as a pin, we can update table  in
 time by following the path from
 to the root and recomputing the tables in the traversed nodes.
Also,  can be initialized in  time
by processing the tree decomposition in a bottom-up manner and
applying the formula for every node.  Note that updating/initializing
table  must be performed after updating/initializing tables 
and .

\subsubsection{Query \qpin}

We now proceed to the next query.  Recall that at each point, the
algorithm maintains the set  of vertices marking components of
 that have been already processed.  A
component is marked as processed when one of its vertices is added to
.  Hence, we need a query that finds the next component to process
by returning any of its vertices.  As in the linear-time approximation
algorithm we need to process the components in decreasing order of
sizes, the query in fact provides a vertex of the largest component.

\defquery{\qpin}{A pair , where (i)  is a vertex of a
  component of  that does not contain a
  vertex from  and is of maximum size among such components, and
  (ii)  is the size of this component; or,  if no such
  component exists.}{}

To implement the query we create a table similar to table , but
with entry indexing enriched by subsets of the bag corresponding to
possible intersections with  and .  Formally, we store entries
for every node , and for every signature ,
which is a quadruple of subsets of  such that (i) , (ii) , (iii) .  The number of such signatures is equal to
.

For a signature , we say that
 is the extension of
 if (i)  is the extension of
 as in the table , (ii) 
and .  We may now state what is stored
in entry :
\begin{itemize}
\item if  is invalid then we store ;
\item otherwise we store:
  \begin{itemize}
  \item an equivalence relation  between vertices of ,
    such that  if and only if  are connected
    in ;
  \item for every equivalence class  of , an integer  equal
    to the number of vertices of the connected component of
     containing , which are
    contained in , or to  if this connected component
    contains a vertex of ;
  \item a pair , where  is equal to the size of the largest
    component of  not containing
    any vertex of  or , while  is any vertex of
    this component; if no such component exists, then .
  \end{itemize}
\end{itemize}

Clearly, query \qpin may be implemented by outputting the pair 
stored in the entry
, or  if this
pair is equal to .

We now present how to compute entries of table  for every node
 depending on the entries of children of .  We consider
different cases, depending of the type of node .  For every case,
we consider only signatures  for which 
is valid, as for the invalid ones we just put value . 

\vskip 0.3cm
\noindent{\bf{Case 1: Leaf node.}} If  is a leaf node then
.  \vskip 0.3cm

\noindent{\bf{Case 2: Introduce node.}} Let  be a node that
introduces vertex , and  be its only child.  Consider some
signature  of ; we would like to compute
.  Let  be a
natural projection of  onto , that is, .  Let
; note that this
entry we know, but entry  we would like to compute.  We
consider some sub-cases, depending on the alignment of  in .

\vskip 0.1cm {\bf{Case 2.1: .}} If we introduce a
vertex from , then the extension of 
is just the extension of  plus vertex  added to
.  If we consider the equivalence classes of , then
these are equivalence classes of  but possibly some of them have
been merged because of connections introduced by vertex .  As 
separates  from ,  could only create connections between
two vertices from .  Hence, we can obtain
 from  by merging all the equivalence classes of vertices of
 adjacent to ; the corresponding entry in
sequence  is equal to the sum of entries from the
sequence  corresponding to the merged classes.  If
any of these entries is equal to , we put simply .  If 
was not adjacent to any vertex of , we put  in a
new equivalence class  with .  Clearly, we can also put
.

\vskip 0.1cm {\bf{Case 2.2: .}} We perform in the
same manner as in Case 2.2, with the exception that the new entry in
sequence  will be always equal to , as the
corresponding component contains a vertex from .

\vskip 0.1cm {\bf{Case 2.3: .}} In this case we can
simply put  as the extensions of 
and  are the same with the exception of  being included into
 and/or into , which does not influence
information to be stored in the entry.

\vskip 0.1cm

{\bf{Case 2.4: .}} In this case we can simply put
 as the extensions of  and  are equal.

\vskip 0.3cm

\noindent{\bf{Case 3: Forget node.}} Let  be a node that forgets
vertex , and  be its only child.  Consider some signature
 of ; we would like to compute
.  Let
 be extension of
.  Observe that there is exactly one signature
 of  with the same extension as ,
and this signature is simply  with  added possibly to ,
,  or , depending whether it belongs to ,
, , or .  Coloring  may be
defined similarly as in case of forget node for table ; we just
need in addition to include  in  or  if it
belongs to  or , respectively.

Let .  As the extensions
of  and  are equal, it follows that we may take 
equal to  with  possibly excluded from its equivalence class.
Similarly, for every equivalence class  we put  equal
to , where  is the corresponding equivalence class of
, except the class that contained  which should get the
previous number incremented by , providing it was not equal to
.  We also put  except the situation, when
we forget the last vertex of a component of : this is the case when  is in  and
constitutes a singleton equivalence class of .  Let then
 be the corresponding entry in sequence .  If , we simply put .
Else, if  or , we put
, and otherwise we put
.

\vskip 0.3cm

\noindent{\bf{Case 4: Join node.}} Let  be a join node and
 be its two children.  Consider some signature
 of ; we would like to compute
.  Let
 be a signature of  and
 be a signature of .  Let
 and
.  Note that equivalence relations
 and  are defined on the same set .  It
follows from the definition of  that we can put:
\begin{itemize}
\item  to be the minimum transitive closure of ;
\item for every equivalence class  of ,  equal to the
  sum of (i) numbers  for ,  being
  an equivalence class of , and (ii) numbers 
  for ,  being an equivalence class of ;
  if any of these numbers is equal to , we put ;
\item  to be equal to  or
  , depending whether  or  is
  larger; if any of these numbers is equal to , we take the
  second one, and if both are equal to , we put .
\end{itemize}

\vskip 0.3cm

Similarly as before, for every addition/removal of vertex  to/from
, to/from , to/from , or marking/unmarking  as a pin, we
can update table  in  time by
following the path from  to the root and recomputing the tables
in the traversed nodes.  Also,  can be initialized in  time by processing the tree decomposition in a
bottom-up manner and applying the formula for every node.  Note that
updating/initializing table  must be performed after
updating/initializing tables  and .

\subsubsection{Query \qUsep}\label{sec:queries}

In this section we implement the last query, needed for the
linear-time algorithm; the query is significantly more involved than
the previous one.  The query specification is as follows:

\defquery{\qUsep}{A list of elements of a -balanced
  separator of  of size at most , or  if no such
  exists.}{}

  Note that Lemma~\ref{lemma:halfhalf} guarantees that in fact  contains 
  a
-balanced separator of size at most .  Unfortunately,
we are not able to find a separator with such a good guarantee on the
sizes of the sides; the difficulties are explained in 
Section~\ref{sec:outline}.
Instead, we again make use of the precomputed approximate tree
decomposition to find a balanced separator with slightly worse
guarantees on the sizes of the sides.

In the following we will also use the notion of a {\emph{balanced separation}}. 
For a graph , we say that a partition  of  is an 
{\emph{-balanced separation of }}, if there is no edge between  
and , and . The {\emph{order}} of a separation is 
the size of . Clearly, if  is an -balanced separation of 
, then  is an -balanced separator of . By folklore [see the 
proof of Lemma~\ref{lem:halfhalfalg}] we know that every graph of treewidth at 
most  has a -balanced separation of order at most .

\paragraph{Expressing the search for a balanced separator as a
  maximization problem.}

Before we start explaining the query implementation, we begin with a
few definitions that enable us to express finding a balanced separator
as a simple maximization problem.

\begin{definition}
  Let  be a graph, and  be disjoint sets of terminals in
  .  We say that a partition  of  is a
  {\emph{terminal separation of  of order }}, if the
  following conditions are satisfied:
  \begin{itemize}
  \item[(i)]  and ;
  \item[(ii)] there is no edge between  and ;
  \item[(iii)] .
  \end{itemize}
  We moreover say that  is {\emph{left-pushed}}
  ({\emph{right-pushed}}) if  () is maximum among possible terminal 
  separations of order .
\end{definition}

Pushed terminal separations are similar to important separators of
Marx~\cite{Marx06}, and their number for fixed  can be
exponential in .  Pushed terminal separations are useful for us
because of the following lemma, that enables us to express finding a
small balanced separator as a maximization problem, providing that
some separator of a reasonable size is given.

\begin{lemma}\label{lem:c4vc}
  Let  be a graph of treewidth at most  and let  be
  some separation of , such that
  .  Then there exists a partition
   of  and integers  with
  , such that if  are  and  with terminals
  , then
  \begin{itemize}
  \item[(i)] there exist a terminal separations of  of orders
    , respectively;
  \item[(ii)] for any left-pushed terminal separation 
    of order  in  and any right-pushed separation
     of order  in , the triple  is a
    terminal separation of  of order at most  with .
  \end{itemize}
\end{lemma}
\begin{proof}
  As the treewidth of  is at most , there is a separation
   of  such that  and  
  by folklore [see the proof of lemma~\ref{lem:halfhalfalg}].  Let us set 
  ,
   and .  Observe that 
  and  are terminal separations in  and  of
  orders  and , respectively, hence we are done with (i).
  We proceed to the proof of (ii).
  
  Let us consider sets , ,  and .
  Since  and  are - and
  - balanced separations, respectively, we know that:
  \begin{itemize}
  \item ;
  \item ;
  \item ;
  \item .
  \end{itemize}
  We claim that either , or .  Assume first that .  Observe that then .  Similarly, .  The case when  is symmetric.  Without loss
  of generality, by possibly flipping separation , assume
  that .
  
  Let  be any left-pushed terminal separation of order
   in  and  be any right-pushed terminal
  separation of order  in .  By the definition of being
  left- and right-pushed, we have that  and .  Therefore, we
  have that  and .
\end{proof}

The idea of the rest of the implementation is as follows.  First,
given an approximate tree decomposition of with  in the data
structure, in logarithmic time we will find a bag  that
splits the component  in a balanced way.  This bag will be used as
the separator  in the invocation of Lemma~\ref{lem:c4vc}; the right
part of the separation will consist of vertices contained in the
subtree below , while the whole rest of the tree will
constitute the left part.  Lemma~\ref{lem:c4vc} ensures us that we may
find some balanced separator of  by running two maximization
dynamic programs: one in the subtree below  to identify a
right-pushed separation, and one on the whole rest of the tree to find
a left-pushed separation.  As in all the other queries, we will store
tables of these dynamic programs in the data structure, maintaining them
with  update times.

\paragraph{Case of a small }
At the very beginning of the implementation of the query we read
, which is stored in the entry .
If it turns out that , we perform the following
explicit construction.  We apply a DFS search from  to identify
the whole ; note that this search takes  time, as  and
 are bounded linearly in .  Then we build subgraph , which
again takes  time.  As this subgraph has  vertices and
treewidth at most , we may find its -balanced
separator of order at most  in  time using a brute-force
search through all the possible subsets of size at most .  This
separator may be returned as the result of the query.  Hence, from now
on we assume that .

\paragraph{Tracing }
We first aim to identify bag  in logarithmic time.  The
following lemma encapsulates the goal of this subsection.  Note that
we are not only interested in the bag itself, but also in the
intersection of the bag with of  and  (defined as the connected
component of  containing ).  While intersection
with  can be trivially computed given the bag, we will need to
trace the intersection with  inside the computation.

\begin{lemma}\label{lem:tracing}
  There exists an algorithm that, given access to the data structure,
  in  time finds a node  of the tree
  decomposition such that 
  together with two subsets  of  such that  and .
\end{lemma}
\begin{proof}
  The algorithm keeps track of a node  of the tree decomposition
  together with a pair of subsets 
  being the intersections of the bag associated to the current node
  with  and , respectively.  The algorithm starts with the root
  node  and two empty subsets, and iteratively traverses down the
  tree keeping an invariant that .
  Whenever we consider a join node  with two sons , we
  choose to go down to the node where 
  is larger among .  In this manner, at each step
   can be decreased by at most  in case of a
  forget node, or can be at most halved in case of a join node.  As
  , it follows that the first node  when the
  invariant  ceases to hold, satisfies
  , and therefore
  can be safely returned by the algorithm.
  
  It remains to argue how sets  can be updated at each step
  of the traverse down the tree.  Updating  is trivial as we
  store an explicit table remembering for each vertex whether it
  belongs to .  Therefore, now we focus on updating .
  
  The cases of introduce and join nodes are trivial.  If  is an
  introduce node with son , then clearly .
  Similarly, if  is a join node with sons , then
  .  We are left with the forget node.
  
  Let  be a forget node with son , and let .  We
  have that  or , depending whether  or not.  This information can be read from the table  as
  follows:
  \begin{itemize}
  \item if , then  and ;
  \item if , then  and ;
  \item otherwise, both  and 
    are not equal to ; this follows from the fact that at least
    one of them, corresponding to the correct choice whether 
    or , must be not equal to .  Observe that in this
    case  is in a singleton equivalence class of , and the connected component of  in the extension
    of  cannot contain the pin .  It follows that
     and we take .
  \end{itemize}
  
  Computation at each step of the tree traversal takes 
  time.  As the tree has logarithmic depth, the whole algorithm runs
  in  time.
\end{proof}

\paragraph{Dynamic programming for pushed separators}
In this subsection we show how to construct dynamic programming tables
for finding pushed separators.  The implementation resembles that of
table , used for balanced -separators.

In table  we store entries for every node  of the tree
decomposition, for every signature  of , and for
every -tuple , called again the {\emph{interface}},
where
\begin{itemize}
\item  is a partition of ,
\item  is an integer between  and .
\end{itemize}
Again, the intuition is that the interface encodes the interaction of
a potential solution with the bag.  Note that for every bag  we
store at most  entries.

We proceed to the formal definition of what is stored in table .
Let us fix a signature  of , and let
 be its extension.  For an interface
, we say that a terminal separation  in  with terminals  is an
{\emph{extension consistent}} with interface  if
\begin{itemize}
\item ,  and ;
\item .
\end{itemize}
Then entry  contains the pair  where  is the
maximum possible  among extensions consistent with
, and  is the corresponding set  for
which this maximum was attained, or  if the signature  is
invalid or no consistent extension exists.

We now present how to compute entries of table  for every node
 depending on the entries of children of .  We consider
different cases, depending of the type of node .  For every case,
we consider only signatures that are valid, as for the invalid ones we
just put value .  

\vskip 0.3cm
\noindent{\bf{Case 1: Leaf node.}} If  is a leaf node then
,
and all the other entries are assigned .  \vskip 0.3cm

\noindent{\bf{Case 2: Introduce node.}} Let  be a node that
introduces vertex , and  be its only child.  Consider some
signature  of  and an interface ;
we would like to compute .  Let
 be natural intersections of  with ,
respectively, that is,  and
.  Let
.  We consider some sub-cases,
depending on the alignment of  in  and .  The cases
with  belonging to  and  are symmetric, so we consider only
the case for .

\vskip 0.1cm {\bf{Case 2.1: .}} Note that every extension
consistent with interface  is an extension consistent with
 after trimming to .  On the other hand, every extension
consistent with  can be extended to an extension consistent
with  by adding  to the extension of .  Hence, it follows
that we can simply take .

\vskip 0.1cm {\bf{Case 2.2: .}} Similarly as in the previous
case, every extension consistent with interface  is an extension
consistent with  after trimming to .  On the other hand,
if we are given an extension consistent with , then we can add
 to  and make an extension consistent with  if and only if
 is not adjacent to any vertex of ; this follows from the fact
that  separates  from , so the only vertices from
 that  could be possibly adjacent to, lie in .
However, if  is adjacent to a vertex of , then we can obviously
put  as there is no extension consistent with
: property that there is no edge between  and  is broken
already in the bag.  Otherwise, by the reasoning above we can put
.

\vskip 0.1cm

{\bf{Case 2.3: .}} Again, in this case we have one-to-one
correspondence of extensions consistent with  and extensions
consistent with , so we may simply put .

\vskip 0.3cm

\noindent{\bf{Case 3: Forget node.}} Let  be a node that forgets
vertex , and  be its only child.  Consider some signature
 of , and some interface ; we
would like to compute .  Let
 be the only the extension of signature  to
 that has the same extension as ;  can be deduced by
looking up which signatures are found valid in table  in the same
manner as in the forget step for computation of table .  We
consider two cases depending on alignment of  in :

\vskip 0.1cm {\bf{Case 3.1: .}} If  is not in , then
it follows that we may put :
extensions consistent with  correspond one-to-one to extensions
consistent with .

\vskip 0.1cm {\bf{Case 3.2: .}} Assume that there exists some
extension  consistent with , and
assume further that this extension is the one that maximizes
.  In this extension, vertex  is either in
, , or in .  Let us define the
corresponding interfaces:
\begin{itemize}
\item ;
\item ;
\item .
\end{itemize}
If  turns out to be negative, we do not consider .  For
, let .  It
follows that for at least one 
there must be an extension consistent with : it is just the
extension .  On the other hand, any
extension consistent with any of interfaces  is
also consistent with .  Hence, we may simply put
, and define  as the corresponding
, with possibly  appended if .  Of course, in this
maximum we do not consider the interfaces  for which
, and if  for
all , we put .

\vskip 0.3cm

\noindent{\bf{Case 4: Join node.}} Let  be a join node and
 be its two children.  Consider some signature
 of , and an interface ; we would
like to compute .  Let
 be a signature of  and 
be a signature of .  Assume that there is some extension
 consistent with , and assume
further that this extension is the one that maximizes .  Define  and  for
; note that  and .  It follows that in
,  there are some extensions consistent with
 and , respectively --- these are simply
extension  intersected with ,
respectively.  On the other hand, if we have some extensions in
,  consistent with  and 
for numbers  such that , then the point-wise union of
these extensions is an extension consistent with .  It
follows that in order to compute , we need to iterate
through choices of  such that we have non- entries in
 and
, choose
 for which  is maximum, and
define .  Of course, if for no choice of  it is
possible, we put .  Note that computing the union of
the sets  for  takes  time as their sizes
are bounded by , and there is  possible choices of  to
check.

\vskip 0.3cm

Similarly as before, for every addition/removal of vertex  to/from
 or marking/unmarking  as a pin, we can update table  in
 time by following the path from
 to the root and recomputing the tables in the traversed nodes.
Also,  can be initialized in  time
by processing the tree decomposition in a bottom-up manner and
applying the formula for every node.  Note that updating/initializing
table  must be performed after updating/initializing tables 
and .

\paragraph{Implementing query \qUsep}
We now show how to combine Lemmata~\ref{lem:c4vc}
and~\ref{lem:tracing} with the construction of table  to
implement query \qUsep.

The algorithm performs as follows.  First, using
Lemma~\ref{lem:tracing} we identify a node  of the tree
decomposition, together with disjoint subsets
 of , such
that .  Let  and
.  Consider separation  of  and apply Lemma~\ref{lem:c4vc}
to it.  Let  be the partition of  and
 be the integers with , whose
existence is guaranteed by Lemma~\ref{lem:c4vc}.

The algorithm now iterates through all possible partitions
 of  and integers  with
.  We can clearly discard the partitions where
there is an edge between  and .  For a partition
, let  be defined as in Lemma~\ref{lem:c4vc}
for the graph .  For a considered tuple ,
we try to find:
\begin{itemize}
\item[(i)] a separator of a right-pushed separation of order  in
  , and the corresponding cardinality of the right side;
\item[(ii)] a separator of a left-pushed separation of order  in
  , and the corresponding cardinality of the left side.
\end{itemize}
Goal (i) can be achieved simply by reading entries
 for , and
taking the right-pushed separation with the largest right side.  We
are going to present how goal (ii) is achieved in the following
paragraphs, but firstly let us show that achieving both of the goals
is sufficient to answer the query.

Observe that if for some  and  we obtained
both of the separators, denote them , together with
cardinalities of the corresponding sides, then using these
cardinalities and precomputed  we may check whether  gives us a -separation of .  On the
other hand, Lemma~\ref{lem:c4vc} asserts that when
 and  are considered, we will find
some pushed separations, and moreover any such two separations will
yield a -separation of .  Note that this is indeed
the case as the sides of the obtained separation have cardinalities at
most , since .

We are left with implementing goal (ii).  Let  be  with
terminal sets swapped; clearly, left-pushed separations in 
correspond to right-pushed separations in .  We implement
finding a right-pushed separations in  as follows.

Let  be the path from  to the root 
of the tree decomposition.  The algorithm traverses the path ,
computing tables  for consecutive indexes .
The table  is indexed by signatures  and interfaces 
in the same manner as .  Formally, for a fixed signature
 of  with extension
, we say that this signature is
{\emph{valid with respect to }} if it is valid and
moreover .  For an interface  we say
that separation  in
 with terminals  is
{\emph{consistent with  with respect to }}, if it
is consistent in the same sense as in table , and moreover
.  Then entry  contains the
pair  where  is the maximum possible 
among extensions consistent with  with respect to
, and  is the corresponding set 
for which this maximum was attained, or  if the signature 
is invalid with respect to  or no such consistent
extension exists.

The tables  can be computed by traversing the path  using the
same recurrential formulas as for table .  When computing the
next , we use table  computed in the previous step
and possible table  from the second child of .  Moreover, as
 we insert the dummy table  defined as follows:
\begin{itemize}
\item ;
\item all the other entries are evaluated to .
\end{itemize}
It is easy to observe that table  exactly satisfies the
definition of .  It is also straightforward to check that the
recurrential formulas used for computing  can be used in the same
manner to compute tables  for .  The
definition of  and the method of constructing it show, that the
values
 for
, correspond to exactly right-pushed separations with
separators of size exactly  in the graph : insertion of the
dummy table removes  from the graph and forces the separation to
respect the terminals in .

Let us conclude with a summary of the running time of the query.
Algorithm of Lemma~\ref{lem:tracing} uses 
time.  Then we iterate through at most  tuples
 and , and for each of them we spend 
time on achieving goal (i) and  time
on achieving goal (ii).  Hence, in total the running time is
.


\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{AbrahamBDR12}
Ittai Abraham, Moshe Babaioff, Shaddin Dughmi, and Tim Roughgarden.
\newblock Combinatorial auctions with restricted complements.
\newblock In {\em Proceedings of the 13th ACM Conference on Electronic
  Commerce, EC 2012}, pages 3--16, 2012.

\bibitem{AbrahamsonF93}
Karl~R. Abrahamson and Michael~R. Fellows.
\newblock Finite automata, bounded treewidth and well-quasiordering.
\newblock In N.~Robertson and P.~Seymour, editors, {\em Proceedings of the AMS
  Summer Workshop on Graph Minors, Graph Structure Theory, Contemporary
  Mathematics vol. 147}, pages 539--564. American Mathematical Society, 1993.

\bibitem{Amir10}
Eyal Amir.
\newblock Approximation algorithms for treewidth.
\newblock {\em Algorithmica}, 56:448--479, 2010.

\bibitem{ArnborgCP87}
Stefan Arnborg, Derek~G. Corneil, and Andrzej Proskurowski.
\newblock Complexity of finding embeddings in a -tree.
\newblock {\em SIAM Journal on Algebraic and Discrete Methods}, 8:277--284,
  1987.

\bibitem{AustrinPW12}
Per Austrin, Toniann Pitassi, and Yu~Wu.
\newblock Inapproximability of treewidth, one-shot pebbling, and related layout
  problems.
\newblock In Anupam Gupta, Klaus Jansen, Jos{\'e} D.~P. Rolim, and Rocco~A.
  Servedio, editors, {\em Proceedings APPROX 2012 and RANDOM 2012}, volume 7408
  of {\em Lecture Notes in Computer Science}, pages 13--24. Springer Verlag,
  2012.

\bibitem{Bodlaender87}
Hans~L. Bodlaender.
\newblock Dynamic programming algorithms on graphs with bounded tree-width.
\newblock In Timo Lepist\"{o} and Arto Salomaa, editors, {\em Proceedings of
  the 15th International Colloquium on Automata, Languages and Programming,
  ICALP'88}, volume 317 of {\em Lecture Notes in Computer Science}, pages
  105--119. Springer Verlag, 1988.

\bibitem{Bodlaender93s}
Hans~L. Bodlaender.
\newblock A linear time algorithm for finding tree-decompositions of small
  treewidth.
\newblock In {\em Proceedings of the 25th Annual Symposium on Theory of
  Computing, STOC'93}, pages 226--234, 1993.

\bibitem{Bodlaender92b}
Hans~L. Bodlaender.
\newblock Dynamic algorithms for graphs with treewidth 2.
\newblock In Jan van Leeuwen, editor, {\em Proceedings of the 19th
  International Workshop on Graph-Theoretic Concepts in Computer Science,
  WG'93}, volume 790 of {\em Lecture Notes in Computer Science}, pages
  112--124. Springer Verlag, 1994.

\bibitem{Bodlaender96}
Hans~L. Bodlaender.
\newblock A linear time algorithm for finding tree-decompositions of small
  treewidth.
\newblock {\em SIAM Journal on Computing}, 25:1305--1317, 1996.

\bibitem{Bodlaender98}
Hans~L. Bodlaender.
\newblock A partial -arboretum of graphs with bounded treewidth.
\newblock {\em Theoretical Computer Science}, 209:1--45, 1998.

\bibitem{BodlaenderCKN12}
Hans~L. Bodlaender, Marek Cygan, Stefan Kratsch, and Jesper Nederlof.
\newblock Solving weighted and counting variants of connectivity problems
  parameterized by treewidth deterministically in single exponential time.
\newblock Report on arXiv 1211.1505, 2012.

\bibitem{BodlaenderF05a}
Hans~L. Bodlaender and Fedor~V. Fomin.
\newblock Equitable colorings of bounded treewidth graphs.
\newblock {\em Theoretical Computer Science}, 349:22--30, 2005.

\bibitem{BodlaenderGHK95}
Hans~L. Bodlaender, John~R. Gilbert, H.~Hafsteinsson, and Ton Kloks.
\newblock Approximating treewidth, pathwidth, frontsize, and minimum
  elimination tree height.
\newblock {\em Journal of Algorithms}, 18:238--255, 1995.

\bibitem{BodlaenderH98}
Hans~L. Bodlaender and Torben Hagerup.
\newblock Parallel algorithms with optimal speedup for bounded treewidth.
\newblock {\em SIAM Journal on Computing}, 27:1725--1746, 1998.

\bibitem{BodlaenderK96}
Hans~L. Bodlaender and Ton Kloks.
\newblock Efficient and constructive algorithms for the pathwidth and treewidth
  of graphs.
\newblock {\em Journal of Algorithms}, 21:358--402, 1996.

\bibitem{Borie88}
Richard~B. Borie.
\newblock {\em Recursively Constructed Graph Families}.
\newblock PhD thesis, School of Information and Computer Science, Georgia
  Institute of Technology, 1988.

\bibitem{ChaudhuriZ98}
Shiva Chaudhuri and Christos~D. Zaroliagis.
\newblock Shortest paths in digraphs of small treewidth. {P}art {II}: {O}ptimal
  parallel algorithms.
\newblock {\em Theoretical Computer Science}, 203:205--223, 1998.

\bibitem{ChaudhuriZ00}
Shiva Chaudhuri and Christos~D. Zaroliagis.
\newblock Shortest paths in digraphs of small treewidth. {P}art {I}:
  {S}equential algorithms.
\newblock {\em Algorithmica}, 27:212--226, 2000.

\bibitem{CohenSTV93}
Robert~F. Cohen, S.~Sairam, Roberto Tamassia, and J.~S. Vitter.
\newblock Dynamic algorithms for optimization problems in bounded tree-width
  graphs.
\newblock In Giovanni Rinaldi and Laurence~A. Wolsey, editors, {\em Proceedings
  of the 3rd Conference on Integer Programming and Combinatorial Optimization,
  IPCO'93}, pages 99--112, 1993.

\bibitem{Courcelle90}
Bruno Courcelle.
\newblock The monadic second-order logic of graphs {I}: {R}ecognizable sets of
  finite graphs.
\newblock {\em Information and Computation}, 85:12--75, 1990.

\bibitem{CyganKN12}
Marek Cygan, Stefan Kratsch, and Jesper Nederlof.
\newblock Fast {H}amiltonicity checking via bases of perfect matchings.
\newblock Report on arXiv 1211.1506, 2012.
\newblock To appear in Proceedings STOC 2013.

\bibitem{DemaineFHT05a}
Erik~D. Demaine, Fedor~V. Fomin, Mohammadtaghi Hajiaghayi, and Dimitrios~M.
  Thilikos.
\newblock Subexponential parameterized algorithms on graphs of bounded genus
  and {}-minor-free graphs.
\newblock {\em Journal of the ACM}, 52:866--893, 2005.

\bibitem{DemaineH08}
Erik~D. Demaine and MohammadTaghi Hajiaghayi.
\newblock The bidimensionality theory and its algorithmic applications.
\newblock {\em The Computer Journal}, 51:292--302, 2008.

\bibitem{ElberfeldJT10}
Michael Elberfeld, Andreas Jakoby, and Till Tantau.
\newblock Logspace versions of the theorems of {B}odlaender and {C}ourcelle.
\newblock In {\em Proceedings of the 51st Annual Symposium on Foundations of
  Computer Science, FOCS 2010}, pages 143--152, 2010.

\bibitem{FeigeHL08}
Uriel Feige, MohammadTaghi Hajiaghayi, and James~R. Lee.
\newblock Improved approximation algorithms for minimum weight vertex
  separators.
\newblock {\em SIAM Journal on Computing}, 38:629--657, 2008.

\bibitem{FellowsL89}
Michael~R. Fellows and Michael~A. Langston.
\newblock An analogue of the {M}yhill-{N}erode theorem and its use in computing
  finite-basis characterizations.
\newblock In {\em Proceedings of the 30th Annual Symposium on Foundations of
  Computer Science, FOCS'89}, pages 520--525, 1989.

\bibitem{Gildea11}
Daniel Gildea.
\newblock Grammar factorization by tree decomposition.
\newblock {\em Computational Linguistics}, 37:231--248, 2011.

\bibitem{Golumbic80}
Martin~Charles Golumbic.
\newblock {\em Algorithmic Graph Theory and Perfect Graphs}.
\newblock Academic Press, New York, 1980.

\bibitem{Hagerup00}
Torben Hagerup.
\newblock Dynamic algorithms for graphs of bounded treewidth.
\newblock {\em Algorithmica}, 27:292--315, 2000.

\bibitem{Kloks93}
Ton Kloks.
\newblock {\em Treewidth. Computations and Approximations}, volume 842 of {\em
  Lecture Notes in Computer Science}.
\newblock Springer Verlag, Berlin, 1994.

\bibitem{KosterHK02}
Arie M. C.~A. Koster, Stan P.~M. van Hoesel, and Antoon W.~J. Kolen.
\newblock Solving partial constraint satisfaction problems with tree
  decomposition.
\newblock {\em Networks}, 40(3):170--180, 2002.

\bibitem{Lagergren96}
Jens Lagergren.
\newblock Efficient parallel algorithms for graphs of bounded tree-width.
\newblock {\em Journal of Algorithms}, 20:20--44, 1996.

\bibitem{Marx06}
D\'{a}niel Marx.
\newblock Parameterized graph separation problems.
\newblock {\em Theoretical Computer Science}, 351:394--406, 2006.

\bibitem{MillerR89}
Gary~L. Miller and John Reif.
\newblock Parallel tree contraction. {P}art 1: {F}undamentals.
\newblock In S.~Micali, editor, {\em Advances in Computing Research 5:
  Randomness and Computation}, pages 47--72. JAI Press, Greenwich, CT, 1989.

\bibitem{MillerR91}
Gary~L. Miller and John Reif.
\newblock Parallel tree contraction. {P}art 2: {F}urther applications.
\newblock {\em SIAM Journal on Computing}, 20:1128 -- 1147, 1991.

\bibitem{PerkovicR00}
Ljubomir Perkovi{\'{c}} and Bruce Reed.
\newblock An improved algorithm for finding tree decompositions of small width.
\newblock {\em International Journal of Foundations of Computer Science},
  11:365--371, 2000.

\bibitem{Reed92}
Bruce Reed.
\newblock Finding approximate separators and computing tree-width quickly.
\newblock In {\em Proceedings of the 24th Annual Symposium on Theory of
  Computing, STOC'92}, pages 221--228, New York, 1992. ACM Press.

\bibitem{RinaudoPBD12}
Rhilipped Rinaudo, Yann Ponty, Dominique Barth, and Alain Denise.
\newblock Tree decomposition and parameterized algorithms for {RNA}
  structure-sequence alignment including tertiary interactions and pseudoknots
  (extended abstract).
\newblock In Benjamin~J. Raphael and Jijun Tang, editors, {\em Proceedings of
  the 12th International Workshop on Algorithms in Bioinformatics, WABI 2012},
  volume 7534 of {\em Lecture Notes in Computer Science}, pages 149--164.
  Springer Verlag, 2012.

\bibitem{RobertsonS2}
Neil Robertson and Paul~D. Seymour.
\newblock Graph minors. {II}. {A}lgorithmic aspects of tree-width.
\newblock {\em Journal of Algorithms}, 7:309--322, 1986.

\bibitem{RobertsonS13}
Neil Robertson and Paul~D. Seymour.
\newblock Graph minors. {XIII}. {T}he disjoint paths problem.
\newblock {\em Journal of Combinatorial Theory, Series B}, 63:65--110, 1995.

\bibitem{SavageBook}
John~E. Savage.
\newblock {\em Models of computation - exploring the power of computing}.
\newblock Addison-Wesley, 1998.

\end{thebibliography}

\end{document}
