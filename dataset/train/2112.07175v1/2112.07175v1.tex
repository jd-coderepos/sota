
To motivate our suggested change, we first describe the typical spatio-temporal transformer framework, paying special attention to improving generalisation by pre-training on image data and fine-tuning on video data. Next, we build on this analysis to present co-training image and video for action recognition (\ourmethod). \ourmethod changes the typical training paradigm by leveraging multiple image and video datasets to train a spatio-temporal transformer model.


\subsection{Video Transformer for Action Recognition}
The action recognition framework is composed of two components, an action recognition model  and a training policy. In this paper, we describe a simple spatio-temporal attention factorized transformer (\ie, TimeSFormer\cite{bertasius2021space}) as a prototypical action recognition model.

\vspace{-10pt}
\paragraph{TimeSFormer}
TimeSFormer is an extension of Visual Transformer\cite{dosovitskiy2020image}. Similar to a ViT model, TimeSformer can be reduced to a sequence of self-attention blocks; however rather than using a single spatial self-attention mechanism, TimeSformer augments the self-attention block with a temporal attention mechanism. Analogous to the Transformer model introduced for natural language processing, both spatial and temporal attention mechanisms are formulated as Multi-Head Attention (MHA). MHA takes three sets of input elements, \ie, the key set , the query set , and the value set , and performs scaled dot-product attention as:

\vspace{-10pt}

Here, \texttt{FFN} is a feed-forward neural network and  is the dimension of  and . With different choices of  and , MHA can be categorized as either spatial attention or temporal attention. Spatial attention corresponds to  and  sampled from the same frame and temporal attention corresponds to  and  sampled across the frames of a video clip. Each TimeSFormer block contain one layer of temporal attention and one layer of spatial attention.

TimeSFormer takes  video frames  as input. The frames are first uniformly cropped into a sequence of image patches with size of , where  is the number of image patches within one frame.  and  represent the spatial resolution. The image patches are then fed into  TimeSFormer blocks through multiple spatial attention and temporal attention layers. An affine transformation is then applied to the learned representation  to attain a probability distribution across all label classes .

In this paper, we describe TimeSFormer using  and , where  represents the parameters within spatial attention layers and  represents the parameters within the temporal attention layers. We describe the classification layer as .

\vspace{-10pt}
\paragraph{Standard Training Paradigm}
Due to the large amount of parameters and the limited size of the video dataset, the standard training policy follows a classical pretraining and finetuning approach. The model is first pretrained on a large object recognition image dataset  and then finetuned on the target downstream video dataset . Specifically, during the pre-training stage, the temporal attention layers are all removed. Only the parameters of the spatial attention layers  are optimized, by minimizing the training loss 
 where   is the classification probability.  and  denote the videos and labels in a mini-batch, and are randomly sampled from the image dataset .  is the cross-entropy loss function.

After pre-training, both spatial attention layers and temporal attention layers are finetuned on the target video datasets.
 where  are sampled from the video dataset .

\subsection{Action Recognition Analysis}

A robust learned representation of video data should be descriptive in both the spatial and temporal dimension. Our empirical findings suggest the typical pre-training and fine-tuning paradigm may limit the model's capacity to construct generalizable representations by fine-tuning on a single, and relatively small, downstream action recognition task. 


Expanding on this last point, it is likely each video dataset incorporates a dataset-specific bias. Applying this hypothesis to two popular video datasets, K400 and SSv2, we find SSv2 focuses on object interaction and relies on complex temporal reasoning to achieve strong performance~\cite{arnab2021vivit}. On the other hand, K400 focuses on interactions among humans and objects. Given the strong performance of non-temporal action recognition models on this dataset, complex temporal reasoning may be significantly less important than learning robust representations comprised of spatial information. 


\begin{table}[t]
\centering
\begin{tabular}{@{}lccr@{}}

\toprule
Frame order & Normal  & Reversed & \\
\midrule
K400 & \textbf{78.1} & 78.0 & \textbf{\textcolor{red}{-0.1}} \\
SSv2 & \textbf{58.8} & 22.6 & \textbf{\textcolor{red}{-36.2}}\\
\bottomrule
\end{tabular}
\caption{Performance of TimeSFormer trained using  image resolution and evaluated on normal frame order and reversed frame order. Reversed frame order means the order of frames are reversed during test-time.}
\label{tab:shuffle_test}
\vspace{-10pt}
\end{table} To further analyze the inherent dataset bias hypothesis, we reverse the order of frames in a clip during test-time. We follow the standard paradigm by training a TimeSformer model~\cite{bertasius2021space} using a 224224 image resolution. We report our findings with both the normal and reversed ordering of frames along the temporal dimension in Table \ref{tab:shuffle_test}. Our findings indicate the model may learn strong temporal-based representations when training on SSv2, while temporal information appears less important for K400. For example, the difference in test accuracy on K400 is small at -0.1\%, but the difference in test accuracy on SSv2 is -36.2\%. 


Another facet of our analysis relates to the limited size and scope of publicly available video datasets. Given the difference in focus of each dataset, the representations learned on one dataset distribution may not generalize to that of other datasets. To analyze this dimension, we conduct an experiment by training a TimeSformer model to achieve strong performance on the Kinetics-400 dataset and then reinitialize the classification layer, freeze the feature extraction layers, and fine-tune the model on SSv2. Our findings indicate this approach would only yield an accuracy of 19.5\% on SSv2, and we interpret this result to indicate the feature extraction mechanisms learned by one action recognition model may have difficulty generalizing to other datasets.



A possible solution to mitigate inherent dataset biases would be to collect a single large-scale video dataset covering a diverse range of actions and events. However, such a collection would be challenging to design and time-consuming to create. An added layer of complexity relates to deciding the label classes of this dataset, and mapping a single video to one or more classes is non-trivial and would require careful design. A different approach is to learn representations applicable to many disparate action recognition datasets. Rather than learning video representations specific to a single dataset during fine-tuning, we instead suggest learning a single model across many action recognition dataset distributions. 


\subsection{\ourmethod: Co-train Videos and Images for Action Recognition}

\ourmethod leverages different action recognition dataset distributions, as well as large-scale object recognition, to construct a general-purpose feature-extraction model. We first introduce the mechanism of learning representations suited for multiple video datasets and then describe the process of integrating image data into the fine-tuning paradigm. 


\vspace{-10pt}

\subsubsection{Co-train Videos}

\label{sec:defacto}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figs/TimeSFormer.pdf}  
\caption{\ourmethod adopts multi-task learning strategy. Each dataset has its own classifier. For the image dataset, we consider images as single frame videos. Therefore, the temporal multi-head attention will not affect the image input.}
\label{fig:cover}
\vspace{-12pt}
\end{figure*}


To learn from  video datasets, we adopt a multi-task learning paradigm and equip the action recognition model  with  classification heads . However, pre-training is unchanged. Notably, we adopt the typical pre-training policy of learning all non-temporal parameters on a large-scale object recognition dataset by minimizing the coss entropy loss over Eq. \ref{image_cls}. 

In the co-training policy, we follow the pretraining and finetuning strategy. Similar to the standard training policy, the spatial attention layers  is learnt by minimizing the cross entropy loss over Eq. \ref{image_cls}. 


During fine-tuning, \ourmethod learns both spatial and temporal attention layers across the samples  from  datasets jointly, where  is the video and its label sampled from the dataset . All video samples are processed by the model  via the shared parameters  and . The sample representations are then distributed to the appropriate classification head to obtain the classification probability . We calculate the training loss for samples in video dataset  as




To optimize the parameters  and , we minimize the weighted sum of the loss function across all  datasets.

where  is the weight for the loss function of the dataset .

Jointly learning action recognition feature extractors across multiple video datasets conveys two advantages. First, as the model is directly trained on multiple datasets, the learned video representations are more general and can be directly evaluated on those datasets without additional fine-tuning. Second, and as emphasized in prior work~\cite{bertasius2021space}, there may be benefits from expanding the scope and quantity of action recognition examples. Attention-based models may easily overfit to a smaller video distribution, thus degrading the generalization of the learned representations. Training on multiple datasets mitigates this challenge by reducing the risk of overfitting. Finally, as indicated in Sect. \ref{sec:defacto}, certain datasets may focus on different inductive biases of video modeling. For example, one dataset may emphasize the modeling of temporal information while others emphasize spatial representational learning. Jointly learning on both distributions may lead to more robust feature extractors that encode both appearance and motion information to improve performance on action recognition benchmarks. 



\subsubsection{Co-train Video and Image Data}

To maintain strong spatial representations, \ourmethod trains a model  on both image and video datasets. Similar to the training policy of transformer-based video models, we first pre-train spatial attention layers  using a large object recognition dataset, and then fine-tune the entire model  using both video datasets  and image datasets  as Fig.~\ref{fig:cover}.  


We adapt an object recognition image task to an action recognition video paradigm with minimal modification by considering an image as a video with only one frame. In this context, we can directly create a batch of both image  and video  data as input to the TimeSformer model . With regards to the object recognition task, we obtain classification outputs , and for the video datasets, we denote dataset-specific classification outputs as . The weighted loss for co-training over both images and videos is 



where  and  represents the loss weights for the image dataset  and the video dataset . We minimize   to optimize parameters  and .

The comparison between the standard training policy and \ourmethod are summarized in Fig.~\ref{image_cls}


\subsection{Discussion}
An ideal video representation should capture both the appearance and motion information from a video. Although video datasets are informational sources for learning motion information, the spatial information contained within a video clip may be limited. This is due to redundancy among frames and the relatively ``small'' size of video datasets compared to classical image datasets. Therefore, even by simultaneously training a model on many video datasets, the model's capacity to learn appearance information may be hindered. Although image dataset pretraining may provide a good start for obtaining the appearance knowledge, it is possible the robust spatial representations are diluted during fine-tuning on highly spatially redundant video data. Reducing the robustness of learned spatial representations would likely decrease model performance; however, these representations may be maintained by continuing to train object recognition with action recognition during the fine-tuning stage of learning. 
