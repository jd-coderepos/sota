\appendix
\section*{Appendix}
\section{Summary Collection}
\label{appedix:summay_annotation}
\begin{figure*}
    \centering
    \includegraphics[width=0.88\textwidth]{figures/summary_collection_page.pdf}
    \vspace{-3pt}
    \caption{A part of the Amazon Mechanical Turk webpage used for collecting summaries.}
    \label{fig:summary_collection}
    \vspace{-10pt}
\end{figure*}

Figure~\ref{fig:summary_collection} illustrates the questions we asked human annotators on Amazon Mechanical Turk during summary collection. 
Before these questions, here are some important instructions we listed on the webpage: (1) Long summary MUST be longer than short summary; (2) Summary length can be dynamically decided based on the content of the thread; (3) Short summary should be a \emph{concise and abstractive} description of what the thread is mainly talking about; (4) Long summary can be a narrative of what happens. But do NOT simply summarize each email separately. The summary should be \emph{coherent}; (5) It is NOT necessary to summarize every email in the long summary, i.e., it is OK to skip unimportant ones and merge similar ones if needed; (6) You are \emph{encouraged} to include important sender and/or receiver names in long summary; (7) You are \emph{disencouraged} to copy a lot from emails for both short and long summaries; You are supposed to write in your own words as much as you can; (8) You may find some content are technical. We do NOT expect any background knowledge. Just focus on the major concerns, decisions, and consensus. (9) In the thread, emails are ordered by time. However, one email does NOT necessarily reply to the previous one. It can reply to an earlier email OR forward to new receivers. In other words, the structure is NOT always continuous, so please be careful when you read.



\section{Fast Abs RL}
\label{appedix:fastabs}
The original Fast Abs RL method \cite{chen2018fast} uses ROUGE-L$_{recall}$ to align extracted source sentences and target summary sentences. In our case, we extract emails and align them with summary sentences. Since the emails and summary sentences usually follow the same temporal order, we enhance the alignment procedure by the Neeleman-Wunsch algorithm \cite{needleman1970general, rameshkumar2020storytelling} to imposing strict order constraints, e.g., there should not be ``email$_i$ is aligned to sentence$_j$ while email$_{i+1}$ is aligned to sentence$_{j-1}$'' cases. Meanwhile, we modify it to allow one email to be aligned with multiple summary sentences but avoid one summary sentence aligning with multiple emails. Specifically, we first obtain the similarity matrix $M$ of size $n_e \times n_s$ between each email and summary sentence by ROUGE-L$_{recall}$ ($n_e$ is the number of emails, $n_s$ is the number of summary sentences); then the alignment score matrix $H$ of size $(n_e + 1) \times (n_s + 1)$ is initialized as all-zero then computed as follows for $1 \leq x \leq n_e$, $1 \leq y \leq n_s$:
\begin{align*}
    H_{x,y} = \max \left\{
                \begin{array}{ll}
                  H_{x-1, y-1} + M_{x-1, y-1}\\
                  H_{x, y-1} + M_{x-1, y-1}\\
                  H_{x-1, y}
                \end{array}
              \right.
\end{align*}
Then we traceback from $H_{n_e,n_s}$ to $H_{0,0}$ to obtain the final alignment. As shown in Table~\ref{tab:dev}, the ``Fast Abs RL (default)'' model refers to this method with the default setting which works mostly worse than our enhanced Fast Abs RL. 

\section{Experimental Details \& Additional Results}
\label{appedix:exp}
We implement the TextRank \cite{mihalcea2004textrank} model via the \texttt{summa} python package\footnote{\url{https://github.com/summanlp/textrank}} and set the summarization ratio as the average $\frac{summary\ length}{thread\ length}$ ratio in the training set, which is 0.22 for short summary and 0.38 for long summary. We test Fast Abs RL \cite{chen2018fast} via the author's open-source code.\footnote{\url{https://github.com/ChenRocks/fast_abs_rl}} Most of our models are built on T5 \cite{raffel2020exploring} and we use the base version that has 220 million parameters. Our hierarchical T5 shares the same T5 encoder parameters between the token-level and email-level encoders. The only new parameters added are from the first cross attention between decoder and email-level encoder. We use \texttt{Transformers} \cite{wolf-etal-2020-transformers}\footnote{\url{https://github.com/huggingface/transformers}} to run all the T5 based models. We run experiments on a single Tesla V100 GPU. We set the max input sequence length as 512 tokens and max output length as 56 tokens during training (200 tokens during evaluation). The total batch size (with gradient accumulation) is 128. The learning rate is 5e-4, except for training the T5$_{base}$ from scratch, we use 1e-4 instead. Since our training set only contains 1.8K examples,  it only takes 2-4 minutes per epoch. We train models for 70 epochs. 

Our model selection is based on each of the five evaluation metrics, ROUGE-1/ROUGE-2/ROUGE-L/summary-level ROUGE-L/BERTScore. We select the best checkpoints for each of the five metrics on our development set, then test those checkpoints on the testing set to report the final numbers for each metric. Table~\ref{tab:dev}
shows all the results on our development set. Table~\ref{tab:high-example} shows two examples that have high-rating model-generated summaries. 


\begin{savenotes}
\begin{table*}
\begin{center}
\small
\begin{tabular}{lllllllllll}
\toprule 
Models & \multicolumn{5}{c}{\textsc{EmailSum}$_{short}$} & \multicolumn{5}{c}{\textsc{EmailSum}$_{long}$} \\
 \cmidrule(lr){2-6} \cmidrule(lr){7-11}
& R1 & R2 & RL & RLsum & BertS & R1 & R2 & RL & RLsum & BertS \\
 \midrule
 \emph{Oracle} &  \emph{39.8} &  \emph{13.05} &  \emph{31.17} &  \emph{36.15} & \emph{28.50} &  \emph{46.74} &  \emph{17.13} &  \emph{33.92} &  \emph{43.1} & \emph{28.38}  \\
Lead-1 & 25.63  & 6.56 & 19.97 & 21.51 & 13.55 & 20.72 & 5.87 & 15.23 & 18.01 & 8.09 \\
Lead-1-Email & 26.37 & 5.88 & 19.68 & 23.61 & 12.98 & 36.65 & 10.44 & 26.00 & 33.27 & 18.11 \\
TextRank & 21.91 & 4.20 & 16.12 & 19.57 & 6.56 & 29.00 & 7.15 & 20.00 & 25.92 & 10.44 \\
BertSumExt & 25.76 & 6.02 & 18.74 & 22.59 & 8.34 & 30.90 & 8.29 & 20.91 & 27.55 & 8.92 \\
 \midrule
Fast Abs RL (default)  & 29.67 & 6.08 & 22.68 & 27.66 & 6.92 & 39.43 & 11.08 & 25.78 & 36.81 & 7.14 \\
Fast Abs RL  & 31.56 & 6.52 & 23.01 & 29.51 & 5.59 & 39.24 & 11.25 & 27.77 & 36.72 & 9.63 \\
T5$_{base}$ (from scratch) & 19.71 & 1.95 & 14.88 & 16.75 & 22.52 & 24.51 & 3.72 & 15.72 & 21.91 & 9.70 \\
T5$_{base}$ & 36.78 & 11.93 & 29.50 & 33.58 & 34.92 & 44.94 & 15.94 & 32.33 & 41.22 & 33.67 \\
\cmidrule(lr){1-1}
\ CNNDM$_{pre}$ & 37.00 & 11.26 & 28.97 & 33.49 & 35.09 & 44.83 & 15.88 & 32.02 & 41.25 & 33.89 \\
\ XSum$_{pre}$ & 36.63 & 11.43 & 29.43 & 33.75 & 35.29 & 44.55 & 15.29 & 31.50 & 40.87 & 33.47 \\
\ SAMSum$_{pre}$ & 36.72 & 11.1 & 28.73 & 33.21 & 35.82 & 44.31 & 15.36 & 31.45 & 40.63 & 33.60  \\
\ CRD3$_{pre}$ &  36.84 & 11.57 & 29.19 & 33.38 & 35.37 & 44.57 & 15.73 & 31.87 & 40.91 & 33.47   \\
\cmidrule(lr){1-1}
\ CNNDM$_{joint}$ & 35.89 & 10.41 & 28.02 & 32.41 & 34.02 & 43.92 & 14.48 & 30.54 & 39.99 & 31.67 \\
\ XSum$_{joint}$ & 35.07 & 9.26 & 27.18 & 31.53 & 34.27 & 43.36 & 13.35 & 29.44 & 39.45 & 30.97 \\
\ SAMSum$_{joint}$ & 36.59 & 11.20 & 29.20 & 33.49 & 35.44 & 44.38 & 15.23 & 31.68 & 40.69 & 33.65 \\
\ CRD3$_{joint}$ & 36.24 & 10.43 & 28.55 & 32.72 & 35.52 & 44.25 & 14.87 & 31.24 & 40.38 & 33.57  \\
\cmidrule(lr){1-1}
\ SemiSup$_{w3c}$ & 37.03 & 11.92 & 29.30 & 33.78 & 35.60 & 45.03 & 16.09 & 32.50 & 41.52 & 33.95 \\
\ SemiSup$_{avocado}$ & 37.78 & 12.56 & 30.09 & 34.50 & 34.88 & 45.49 & 16.21 & 32.97 & 41.82 & 34.42 \\
\ SemiSup$_{together}$ & 37.43 & 12.26 & 29.84 & 34.32 & 35.08 & 45.73 & 16.27 & 32.65 & 41.91 & 34.09 \\
\cmidrule(lr){1-1}
Hier. T5$_{base}$ & 36.67 & 11.79 & 29.13 & 33.58 & 35.71 & 45.26 & 16.13 & 32.62 & 41.55 & 33.99 \\
\toprule
\end{tabular}
\end{center}
\vspace{-12pt}
\caption{Summarization performance of different models on the development set.}
\label{tab:dev}
\vspace{-12pt}
\end{table*}
\end{savenotes}


\begin{table*}
\begin{center}
\small
\begin{tabular}{p{\textwidth}}
\toprule 
\textbf{Examples of high-quality summaries generated by the model.} \\
\midrule
\emph{Thread}: Subject: faa demos $\vert\vert\vert$ Dan: PM Team, Attached are some general ideas and issues around developing new demos for our new target markets. Please review and provide feedback. Also, please provide links where we can learn more about various FAA applications. Thanx, Dan. $\vert\vert\vert$ Dan, Thanks for putting the high level descriptions together. My questions are: *Is it practical to do an EAI demo given the inherent complexity of application integration? ... *Should we delay looking at Outlook for now?... *What do you think that timelines are developing these demos? ... Alex $\vert\vert\vert$ Alex, Thanks for the feedback, please see my comments below:\\
\emph{Generated Short Summary}: Dan asks the PM team to review and provide feedback on FFA demos. Alex responds with questions. Dan thanks Alex and gives his feedback. (\textbf{salience=4.3, faithfulness=4.7}) \\
\emph{Ground-truth}: Dan talks about general ideas about demos to his PM team. Alex provides some feedback and asks questions. Dan thanks Alex for the feedback and adds comments. \\
\midrule
\emph{Thread}: Subject: sun performance report $\vert\vert\vert$ Mahesh: Hi, I am attaching the draft of the performance/sizing report for EMAS on Sun. Please send me your comments. I am also attaching a list of features that would be good to have. Thanks, Mahesh $\vert\vert\vert$ Amitabh: do we have a side-by-side comparison of solaris, hp-ux, and nt? also, a price-performance comparison might also be useful  $\vert\vert\vert$ Rajeev: Dan, Please consider Amitabh's suggestions for the sizing requirement document that you are prepaing... $\vert\vert\vert$ Mahesh: we do not have comparison stats. It would be good to have them. $\vert\vert\vert$ Dan: Good points, we should have side-by-side comparisons and also price/performance...\\
\emph{Generated Long Summary}: Mahesh is attaching a draft of the performance/sizing report for EMAS on Sun and asking for comments.
Amitabh asks if there is a side-by-side comparison of solaris, hp-ux, and nt.
Rajeev asks Dan to consider Amibh's suggestions for the sizing requirement document.
Mahesesh says there are no comparison stats, but it would be good to have them.
Dan says there should be side- by-side comparies and also price/performance. (\textbf{salience=4.3, faithfulness=5})\\
\emph{Ground-truth}: Mahesh shows everyone a performance report for a future meeting and attaches his feedback. Amitabh gives feedback which Rajeev asks Dan to consider in a different task. Mahesh and Dan make suggestions about comparisons. \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-12pt}
\caption{Examples of high-quality summaries generated by model. Emails are separated by `$\vert\vert\vert$' and some content are omit by `...'. (\textbf{salience=xx, faithfulness=xx}) gives the average human rating for that summary.}
\vspace{-12pt}
\label{tab:high-example}
\end{table*}

\section{Human Evaluation}
\label{appendix:human}
Figure~\ref{fig:human_eval} shows the questions we asked to human judges to evaluate the quality of model-generated summaries. Before these questions, we instruct annotators how to rate on a 5-point Likert scale for ``salience'' and ``faithfulness'': (1) Rate \emph{salience} from 1 to 5: 1 is the worst, none of the points in the summary is important enough to be summarized; 5 is the best, all of the points mentioned in the summary are important and worth to be summarized; (2) Rate \emph{faithfulness} from 1 to 5: 1 is the worst, all of the sentences in the summary are either wrong or not existing in the email thread; 5 is the best, all of the points mentioned in the summary are true to the thread. Plus, we also prompt examples of ``non-salient'' and ``unfaithful'' summaries on the webpage. We pay annotators \$0.60 per HIT.


\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/human_eval_page.pdf}
    \vspace{-10pt}
    \caption{A part of the Amazon Mechanical Turk webpage used for human evaluation.}
    \label{fig:human_eval}
    \vspace{-12pt}
\end{figure*}

