\begin{figure*}[t]
\begin{center}
    {
    \subfigure[CIFAR-LT-10]
    {
    \includegraphics[width=0.15\textwidth]{figures/CIFAR10_dist_cvpr.pdf}
    \label{fig:CIFAR-10}}
    \subfigure[CIFAR-LT-100]
    {
    \includegraphics[width=0.15\textwidth]{figures/CIFAR100_dist_cvpr.pdf}
    \label{fig:CIFAR-100}} 
    \subfigure[CelebA-5]
    {
    \includegraphics[width=0.15\textwidth]{figures/CelebA_dist_cvpr.pdf}
    \label{fig:celeba}
    }
    \subfigure[SUN397]
    {
    \includegraphics[width=0.15\textwidth]{figures/SUN397_dist_cvpr.pdf}
    \label{fig:Sun}
    }
    \subfigure[Twitter]
    {
    \includegraphics[width=0.15\textwidth]{figures/Twitter_dist_cvpr.pdf}
    \label{fig:Twitter}}
    \subfigure[Reuters]
    {
    \includegraphics[width=0.15\textwidth]{figures/Reuter_dist_cvpr.pdf}
    \label{fig:Reuter}}
    }
\end{center}
\vspace{-0.05in}
\caption{An illustration of histograms on training sample sizes for the datasets used in this paper.}
\label{fig:stat}
\vspace{-0.1in}
\end{figure*}

We evaluate our method on various class-imbalanced classification tasks:
synthetically-imbalanced variants of CIFAR-10/100~\citep{dataset/cifar},
ImageNet-LT\footnote{Results on ImageNet-LT can be found in the supplementary material.}~\citep{liu2019large}, CelebA~\cite{liu2015faceattributes}, SUN397~\cite{xiao2010sun}, Twitter~\citep{gimpel2010part}, and Reuters~\citep{lewis2004rcv1} datasets.\footnote{Code is available at \url{https://github.com/alinlab/M2m}} Figure~\ref{fig:stat} illustrates the class-wise sample distributions for the datasets considered in our experiments.
The more details on the tested datasets are given in the supplementary material. To evaluate the classification performance of the models on the balanced test distribution, we mainly report two popular metrics: the \emph{balanced accuracy} (bACC) \citealt{huang2016learning, wang2017learning} and the \emph{geometric mean scores} (GM) \citealt{kubat1997addressing, branco2016survey}, which are defined by the arithmetic and geometric mean over class-wise sensitivity (\ie, recall), respectively. 
We remark that bACC is essentially equivalent to the standard accuracy metric for balanced datasets. All the values and error bars in this section are mean and standard deviation across three random trials, respectively.
Overall, our results clearly demonstrate that minority synthesis via translating from majority consistently improves the efficiency of over-sampling, in terms of the significant improvement of the generalization in minority classes compared to other re-sampling baselines, across all the tested datasets. We also perform an ablation study to verify the effectiveness of our main ideas. 

\subsection{Experimental setup}
\label{ss:setup}

\paragraph{Baseline methods.}
We consider a wide range of baseline methods, as listed in what follows: 
(\textit{a}) empirical risk minimization (ERM): training on the cross-entropy loss without any re-balancing; 
(\textit{b}) re-sampling (RS) \citealt{japkowicz2000class}: balancing the objective from different sampling probability for each sample; 
(\textit{c}) SMOTE \citep{chawla2002smote}: a variant of re-sampling with data augmentation;
(\textit{d}) re-weighting (RW) \citealt{huang2016learning}: balancing the objective from different weights on the sample-wise loss;
(\textit{e}) class-balanced re-weighting (CB-RW) \citealt{cui2019class}: a variant of re-weighting that uses the inverse of effective number for each class, defined as \mbox{}. Here, we use ;
(\textit{f}) deferred re-sampling (DRS) \citealt{cao2019learning} and (\textit{g}) deferred re-weighting (DRW) \citealt{cao2019learning}: re-sampling and re-weighting is deferred until the later stage of the training, repsectively;
(\textit{h}) focal loss (Focal) \citealt{lin2017focal}: the objective is up-weighted for relatively hard examples to focus more on the minority; 
(\textit{i}) label-distribution-aware margin (LDAM) \citealt{cao2019learning}: the classifier is trained to impose larger margin to minority classes. 
Roughly, the considered baselines can be classified into three categories: (i) ``re-sampling'' based methods - (\textit{b, c, f}), (ii) ``re-weighting'' based methods - (\textit{d, e, g}), and (iii) different loss functions - (\textit{a, h, i}). 

\vspace{-0.15in}
\paragraph{Training details.}
We train every model via stochastic gradient descent (SGD) with momentum of weight . The initial learning rate is set to , and ``step decay'' is performed during training where the exact scheduling across datasets is specified in the supplementary material. Although it did not affect much to our method, we also adopt the ``linear warm-up'' learning rate strategy \citep{goyal2017accurate} in the first 5 epochs, as the performance of some baseline methods, \eg, re-weighting, highly depends on the use of this strategy. 
For CIFAR-10/100 and CelebA, we train ResNet-32 \citep{he2016identity} for 200 epochs with mini-batch size 128, and set a weight decay of . In case of SUN397, the pre-activation ResNet-18 model is used instead.\footnote{We remark this model is larger than ResNet-32 used for CIFAR and CelebA datasets, as it has roughly 4 more channels.}  
We ensure that all the input images are normalized over the training dataset, and have the size of 3232 either by cropping or re-sizing, to be compatible with the given architectures.
For Twitter and Reuters datasets, we train 2-layer fully-connected networks for 15 epochs with mini-batch size 64, and with a weight decay of .

\vspace{-0.15in}
\paragraph{Details on M2m.}
When our method is applied, we use another classifier  of the same architecture to  that is pre-trained on the given (imbalanced) dataset via standard ERM training.  
Also, in a similar manner to that of \citet{cao2019learning}, we use the {deferred} scheduling to our method, \ie, we start to apply our method after the standard ERM training for a fixed number of epochs. The actual scheduling across datasets is specified in the supplementary material.
We choose hyperparameters in our method from a fixed set of candidates, namely ,  and  based on the validation set. Unless otherwise stated, we fix  and  when performing a single generation step.

\subsection{Long-tailed CIFAR datasets} 

We consider a ``synthetically long-tailed'' variant of CIFAR~\citep{dataset/cifar} datasets (CIFAR-LT-10/100) in order to evaluate our method on various levels of imbalance, where the original datasets are class-balanced.
To simulate the long-tailed distribution frequently appeared in imbalanced datasets, we control the \emph{imbalance ratio}  and artificially reduce the training sample sizes of each class except the first class, so that: (a)  equals to , and (b)  in between  and  follows an exponential decay across . We keep the test dataset unchanged during this process, \ie, it is still perfectly balanced, thereby measuring accuracy on this dataset is equivalent to measuring the balanced accuracy.
We consider two imbalance ratios  each for CIFAR-LT-10 and 100. See Figure~\ref{fig:CIFAR-10} and \ref{fig:CIFAR-100} for a detailed illustration of the sample distribution. 
 
\begin{table*}[t]
	\vspace{-0.05in}
	\begin{center}
	\begin{tabular}{cccccccccc}
		\toprule
		\multicolumn{2}{c}{Dataset}   &  \multicolumn{4}{c}{CIFAR-LT-10} &  \multicolumn{4}{c}{CIFAR-LT-100} \\
		\cmidrule(r){3-6} \cmidrule(l){7-10}
		\multicolumn{2}{c}{Imbalance ratio}   &  \multicolumn{2}{c}{} &  \multicolumn{2}{c}{}  &  \multicolumn{2}{c}{} &  \multicolumn{2}{c}{} \\
        \cmidrule(r){3-4} \cmidrule(l){5-6} \cmidrule(r){7-8} \cmidrule(l){9-10}
		Loss      &   Re-balancing    &   bACC & \echo{GM}    &   bACC  & \echo{GM}  &   bACC & \echo{GM}    &   bACC  & \echo{GM}
		\\ \midrule
		ERM       &       -         &    68.7\ms{1.43} &  \echo{{66.4}\ms{1.69}}     
		          &   86.0\ms{0.69}  &  \echo{85.8\ms{0.50}}  &   37.2\ms{1.12}    &  \echo{21.5\ms{1.66}}     
		          &    56.2\ms{0.69}  &  \echo{51.8\ms{0.63}}  \\ 
        ERM       &   RS            &   70.4\ms{1.15}  &  \echo{69.0\ms{1.36}} 
		          &     86.6\ms{0.37} &  \echo{86.4\ms{0.37}} &  31.6\ms{1.26}   &  \echo{17.7\ms{1.33}}    
		          &  54.8\ms{0.47}  &  \echo{50.3\ms{0.68}} \\
		ERM       &   SMOTE         &   71.5\ms{0.57}  &  \echo{70.2\ms{0.93}} 
		          &    85.7\ms{0.25} &  \echo{85.5\ms{0.26}} &   34.0\ms{0.33}  &  \echo{19.6\ms{0.36}}     
		          &   53.8\ms{0.93}    &  \echo{49.4\ms{1.15}}  \\
		ERM       &   RW           &   72.8\ms{0.33}   &  \echo{72.0\ms{0.29}}
		          &   86.6\ms{0.18}  &  \echo{86.5\ms{0.16}} &   30.1\ms{0.59}   &  \echo{17.6\ms{0.85}}    
		          &   56.0\ms{0.35}    &  \echo{52.0\ms{0.51}}    \\
		ERM       &   CB-RW        &   71.2\ms{1.14} &  \echo{70.0\ms{1.28}}   
		          &    86.8\ms{0.49}  &  \echo{86.6\ms{0.53}} &     38.6\ms{0.46}   &  \echo{22.5\ms{0.49}}    
		          &     55.9\ms{0.24}    &  \echo{52.0\ms{0.42}} \\
		ERM       &   DRS           &   \underline{75.2}\ms{0.26}   &  \echo{\underline{73.9}\ms{0.17}}
		          &  \underline{87.1}\ms{0.26}  &  \echo{\underline{87.0}\ms{0.29}} &   \underline{41.5}\ms{0.21}  &  \echo{\underline{31.0}\ms{0.21}}      
		          &    \underline{57.7}\ms{0.40}   &  \echo{\underline{54.8}\ms{0.33}}  \\
		\textbf{ERM} &   \textbf{M2m (ours)} &   \textbf{78.3}\ms{0.16} &  \echo{\textbf{77.8}\ms{0.16}}
		          &  \textbf{87.9}\ms{0.21}  &  \echo{\textbf{87.5}\ms{0.15}} &  \textbf{42.9}\ms{0.16}  &  \echo{\textbf{33.0}\ms{0.11}}    
		          &    \textbf{58.2}\ms{0.08}   &  \echo{\textbf{55.3}\ms{0.05}}  \\ \midrule
		Focal     &       -         &     68.3\ms{1.19}  &  \echo{{65.5}\ms{1.71}}
		          &  85.3\ms{0.47}  &  \echo{85.1\ms{0.47}} &   37.7\ms{1.38}   &  \echo{22.1\ms{1.49}}    
		          &   55.3\ms{0.42}   &  \echo{\underline{50.7}\ms{0.43}}    \\
		LDAM      &       -         &   72.8\ms{0.37} &  \echo{70.8\ms{0.65}}    
		          &   86.2\ms{0.12}  &  \echo{86.0\ms{0.15}}   &   {39.5}\ms{0.69}  &  \echo{20.8\ms{0.49}}        
		          &   54.7\ms{0.16}   &  \echo{44.1\ms{0.53}}     \\
		LDAM      &   DRW           &  \underline{77.1}\ms{0.49} &  \echo{\underline{76.7}\ms{0.59}}     
		          &   \underline{87.1}\ms{0.28}  &  \echo{\underline{86.9}\ms{0.28}}   &    \underline{42.1}\ms{0.09}      &  \echo{\underline{29.2}\ms{0.27}}   
		          &   \underline{56.9}\ms{0.15}   &  \echo{50.4\ms{0.29}}    \\ 
		\textbf{LDAM} &   \textbf{M2m (ours)}  &  \textbf{79.1}\ms{0.19}  &  \echo{\textbf{78.6}\ms{0.19}}
		          &  \textbf{87.5}\ms{0.15} &  \echo{\textbf{87.4}\ms{0.19}}  &   \textbf{43.5}\ms{0.22}  &  \echo{\textbf{34.2}\ms{0.62}}    
		          &   \textbf{57.6}\ms{0.14}  &  \echo{\textbf{51.8}\ms{0.38}}  
		\\ \bottomrule
	\end{tabular}
    \end{center}
    \vspace{-0.05in}
    \caption{Comparison of classification performance on the four different types of long-tailed CIFAR-10/100 datasets.}
	\label{table:cifar}
    \vspace{-0.02in}
\end{table*} \begin{table*}[t]
	\begin{center}
	\begin{tabular}{cccccccccccccc}
		\toprule
		\multicolumn{2}{c}{Datasets}   &  \multicolumn{2}{c}{CelebA-5} &  \multicolumn{2}{c}{SUN397} &  \multicolumn{2}{c}{Twitter} &  \multicolumn{2}{c}{Reuters}  \\
		\cmidrule(r){3-4} \cmidrule(l){5-6} \cmidrule(l){7-8} \cmidrule(l){9-10}
		\multicolumn{2}{c}{Imbalance ratio}   & 
		\multicolumn{2}{c}{} &  
		\multicolumn{2}{c}{} &  \multicolumn{2}{c}{} &  \multicolumn{2}{c}{} \\
        \cmidrule(r){3-4} \cmidrule(l){5-6} \cmidrule(l){7-8}  \cmidrule(l){9-10}
		Loss      &   Re-balancing    &   bACC   & \echo{GM} &   bACC   & \echo{GM}   &   bACC  & \echo{GM} &   bACC  & \echo{GM} \\ \midrule
		ERM       &       -         
		          &    72.7\ms{1.24}  &  \echo{69.4\ms{0.97}}
		          &    31.5\ms{0.07}  &  \echo{20.2\ms{0.74}}
		          &    74.7\ms{0.46}  &  \echo{65.2\ms{1.10}}  
		          &    59.8\ms{1.17}  & \echo{53.8\ms{1.75}}  \\ 
        ERM       &   RS  
                  &    72.5\ms{0.93}   & \echo{70.4\ms{1.37}}
                  &    28.4\ms{0.19}   & \echo{19.8\ms{1.10}}
                  &    75.8\ms{0.30}   & \echo{70.4\ms{1.67}}
		          &    63.3\ms{0.90} &  \echo{57.4\ms{1.03}} \\
		ERM       &   SMOTE         
		          &    72.8\ms{1.07}  & \echo{70.7\ms{0.84}}
		          &    23.7\ms{0.09}  & \echo{14.8\ms{0.39}}
		          &    75.8\ms{0.38}  & \echo{69.5\ms{0.30}}
		          &    62.5\ms{1.30}  & \echo{56.8\ms{1.69}} \\
		ERM       &   RW           
		          &    \underline{74.5}\ms{0.50}  &  \echo{\underline{73.4}\ms{0.87}}
		          &    31.3\ms{0.20}  &  \echo{\underline{25.3}\ms{0.12}}
		          &    76.2\ms{0.95}  &  \echo{73.5\ms{1.46}}
		          &    \underline{65.0}\ms{1.08}  &  \echo{\underline{59.2}\ms{1.84}}\\
		ERM       &   CB-RW           
		          &   74.2\ms{0.59}  & \echo{72.3\ms{0.50}}
		          &   \underline{31.7}\ms{0.13}  & \echo{25.1\ms{0.51}}
		          &   77.5\ms{0.40}  & \echo{73.6\ms{0.79}}
		          &    64.8\ms{0.45} & \echo{57.6\ms{1.62}}  \\
		ERM       &   DRS           
		          &   73.1\ms{0.68}  & \echo{71.2\ms{0.62}} 
		          &   30.7\ms{0.34}  & \echo{24.2\ms{0.40}} 
		          &     \underline{77.8}\ms{0.85} &  \echo{\underline{74.3}\ms{1.48}}
		          &    62.4\ms{0.39} & \echo{56.0\ms{1.34}}  \\
		\textbf{ERM} &   \textbf{M2m (ours)} 
		          &   \textbf{75.6}\ms{0.16} & \echo{\textbf{74.6}\ms{0.34}}
		          &   \textbf{32.4}\ms{0.17} & \echo{\textbf{25.8}\ms{0.29}}
		          &   \textbf{78.2}\ms{0.35} & \echo{\textbf{74.8}\ms{0.78}}
		          &   \textbf{66.3}\ms{0.42}  & \echo{\textbf{60.5}\ms{0.52}} \\ \midrule
		Focal     &       -         
		          &  72.7\ms{0.57} &  \echo{69.7\ms{1.42}} 
		          &  31.2\ms{0.14} &  \echo{21.3\ms{0.71}} 
		          &  74.2\ms{2.35} &  \echo{70.4\ms{4.03}} 
		          &     59.4\ms{0.42}  & \echo{53.0\ms{0.74}}  \\
		LDAM      &       -         
		          &   73.0\ms{1.14} &  \echo{68.0\ms{2.19}}  
		          &   30.2\ms{0.10} &  \echo{14.4\ms{0.83}}   
		          &   74.6\ms{0.40} &  \echo{66.1\ms{2.28}}   
		          &    63.0\ms{1.36}  &  \echo{\underline{57.6}\ms{0.50}} \\ 
		LDAM      &   DRW           
		          &   \underline{74.4}\ms{0.33} &  \echo{\underline{72.3}\ms{0.82}}   
		          &   \underline{31.6}\ms{0.10} &  \echo{\underline{23.6}\ms{0.36}}   
		          &   \underline{78.0}\ms{0.87}  &  \echo{\underline{74.4}\ms{1.28}}
		          &   \underline{64.1}\ms{0.31} & \echo{56.9\ms{1.08}}   \\
		\textbf{LDAM} &   \textbf{M2m (ours)}  
		          &   \textbf{75.9}\ms{1.09}  & \echo{\textbf{75.0}\ms{0.94}}
		          &   \textbf{33.3}\ms{0.20}  & \echo{\textbf{24.9}\ms{0.76}}
		          &   \textbf{78.8}\ms{0.21}  & \echo{\textbf{76.0}\ms{0.23}}
		          &     \textbf{70.0}\ms{0.68}    & \echo{\textbf{63.9}\ms{0.49}}
		\\ \bottomrule
	\end{tabular}
    \end{center}
    \vspace{-0.05in}
    \caption{Comparison of classification performance on the four naturally imbalanced datasets: CelebA-5, SUN397, Twitter and Reuters. In case of Reuters,  is adjusted to 1.0 when training M2m models regarding the numerical range of the dataset.}
	\label{table:nlp_tasks}
    \vspace{-0.1in}
\end{table*} 
Table \ref{table:cifar} summarizes the main results.
In overall, the results show that our method consistently improves the bACC by a large margin, across all the tested baselines. 
These results even surpass the ``LDAM+DRW'' baseline \citep{cao2019learning}, which is known to be the state-of-the-art to the best of our knowledge. 
Moreover, we point out, in most cases, our method could further improve bACC when applied upon the LDAM training scheme (see ``LDAM+M2m''): this indicates that the performance gain from our method is fairly orthogonal to that of LDAM, \ie, the margin-based approach, which suggests a new promising direction of improving the generalization when a neural network model suffers from a problem of small data. 

\vspace{-0.02in}
\subsection{Real-world imbalanced datasets}
\label{ss:real}

We further verify the effectiveness of M2m on four well-known, \emph{naturally} imbalanced datasets: 
CelebA \citep{liu2015faceattributes}, SUN397 \citep{xiao2010sun}, Twitter \citep{gimpel2010part} and Reuters \citep{lewis2004rcv1} datasets. More detailed information for each of these datasets is demonstrated in Figure~\ref{fig:stat} and the supplementary material.

CelebA is originally a multi-labeled dataset, and we port this to a 5-way classification task by filtering only the samples with five non-overlapping labels about hair colors. We also subsampled the full dataset by  while maintaining the imbalance ratio , in attempt to make the task more difficult. We denote the resulting dataset by CelebA-5.

Although Twitter and Reuters datasets are from natural language processing, we also evaluate our method on them to test the effectiveness under much extreme imbalance. Here, we remark that the imbalance ratio  of these two datasets are about 150 and 710, respectively, which are much higher than the other image datasets tested. In case of Reuters, we exclude the classes having less than 5 samples in the test set for more reliable evaluation, resulting a dataset of 36 classes. 

Table~\ref{table:nlp_tasks} shows the results. Again, M2m performs best amongst other baseline methods, demonstrating the effectiveness of our method under natural imbalance, as well as {wider} applicability of our algorithm beyond image classification. Remarkably, the significant results on Reuters dataset compared to the others suggest that our method can be even more effective under a regime of ``extremely'' imbalanced datasets, as Reuters has a much larger imbalance ratio than the others.


\subsection{Ablation study}
\label{ss:ablation}

We conduct an extensive ablation study to present a detailed analysis of the proposed method. All the experiments in this section are performed with ResNet-32 models, trained on CIFAR-LT-10 with the imbalance ratio . We additionally report the balanced test accuracy over \emph{majority} and \emph{minority} classes, namely ``Major'' and ``Minor'' respectively, to further identify the relative impacts on those two classes separately. We divide the whole classes into ``majority'' and ``minority'' classes, so that the majority classes consist of top- frequent classes with respect to the training set where  is the minimum number that  exceeds  of the total. We denote the minority classes as the remaining classes. We provide more discussion in the supplementary material.

\begin{table}[t]
    \begin{center}
    \small
	\begin{tabular}{ccc}
		\toprule
		\# Seeds             &  bACC ()  &  {GM ()} \\ \midrule
		10           &   {74.9}{\ms{0.29}} {(-4.34\%)}  & {73.7{\ms{0.33}} {(-5.27\%)}}   \\
		50           &   76.2{\ms{0.30}} (-2.68\%) & {75.3{\ms{0.29}} (-3.21\%)} \\ 
        100          &   76.5{\ms{0.34}} (-2.30\%) & {75.6{\ms{0.41}} (-2.83\%)} \\ 
        200          &   76.7{\ms{0.51}} (-2.04\%) & {75.9{\ms{0.59}} (-2.44\%)} \\ 
        500          &   77.4{\ms{0.38}} (-1.15\%) & {76.8{\ms{0.31}} (-1.29\%)}
        \\ \midrule
        \textbf{Full} &  
        \textbf{78.3\ms{0.16}}  (-0.00\%)  & 
        {\textbf{77.8}\ms{0.16}} (-0.00\%) \\ 
        
        \bottomrule
	\end{tabular}
	\end{center}
	\vspace{-0.05in}
	\caption{Comparison of classification performance across various number of samples allowed to be a seed sample .  indicates the relative gap from the original result presented in ``Full''.}
	\label{table:abl2}
	\vspace{-0.05in}
\end{table}

\begin{table}[t]
	\begin{center}
    \begin{adjustbox}{width=1\linewidth}
	\begin{tabular}{lcccc}
		\toprule
		Methods             &   Major (2)      &    Minor (8)    &  bACC  &  {GM} \\ \midrule
        M2m ()   &   92.8\ms{0.97}       &    73.0\ms{0.10}      &  76.9\ms{0.15} & {76.5\ms{0.11}} \\ 
        M2m-Clean    &   78.4\ms{2.45}       &    72.7\ms{0.60}      &  73.5\ms{0.81}  & {73.0\ms{0.93}} \\ \midrule
        ERM-RS  &  92.8\ms{1.50}       &    64.8\ms{1.18}      &  70.4\ms{1.15}  &  {69.0\ms{1.36}} \\
        M2m-RS   &   92.9\ms{2.99}       &    69.4\ms{0.84}      &  74.1\ms{0.10}  &   {73.1\ms{0.14}} \\ 
        M2m-RS-Rand   &   93.6\ms{2.34}       &    66.1\ms{1.04}      &  71.6\ms{0.36}  &   {70.3\ms{0.80}} \\ \midrule
        \textbf{M2m}         &   \textbf{93.3}\ms{0.85}       &    \textbf{74.6}\ms{0.34}      &  \textbf{78.3}\ms{0.16} & {\textbf{77.8}\ms{0.16}}    \\ 
        \bottomrule
	\end{tabular}
	\end{adjustbox}
	\end{center}
	\vspace{-0.05in}
	\caption{Comparison of classification performance across various types of ablations. We report the number of majority and minority classes in the parentheses.}
	\label{table:abl1}
	\vspace{-0.15in}
\end{table} 
\vspace{-0.15in}
\paragraph{Diversity on seed samples.} In Section~\ref{ss:overview}, we hypothesize that the effectiveness of our method mainly comes from utilizing a much diversity in the majority samples to prevent the over-fitting to the minority classes. To verify this, we consider an ablation that the candidates of ``seed samples'' are limited: more concretely, we control the size of seed sample pools per each class to a fixed subset of the training set, made before training . In Table~\ref{table:abl2}, the accuracy of minority classes is progressively increased as seed sample pools become diverse. This clear trend indicates that M2m makes use of the diversity of majority classes for preventing the over-fitting to the minority classes. 

\vspace{-0.15in}
\paragraph{The effect of .} In the optimization objective \eqr{eq:obj} for the generation step in M2m, we impose a regularization term  to improve the quality of synthetic samples: they might confuse  if themselves still contain important features of the original class in a viewpoint of . To verify the effect of this term, we consider an ablation that  is set to , and compare the performance to the original method. As reported in Table~\ref{table:abl1}, we found a certain level of degradation in the balanced test accuracy at this ablation, which shows the effectiveness of the proposed regularization.

\vspace{-0.15in}
\paragraph{Over-sampling from the scratch.} 
As specified in Section~\ref{ss:setup}, we use the ``deferred'' scheduling to our method by default, \ie, we start to apply our method after the standard ERM training for a fixed number of epochs. We have also considered a simple ablation where this strategy is not used, namely ``M2m-RS''. The results in Table~\ref{table:abl1} show that M2m-RS still outperforms any other baselines (reported in Table~\ref{table:cifar}) except the ones that the deferred scheduling is used, \ie, DRS and DRW, and this further verifies the effectiveness of our method.

\vspace{-0.15in}
\paragraph{Labeling as a targeted class.} 
Our primary assumption on the pre-trained classifier  does not require that  itself to generalize well on the minority classes (see Section~\ref{ss:overview}). This implies that solving \eqr{eq:obj} with  may not end up with a synthetic sample that contains generalizable features of the target minority class. To examine how much the generated samples would be correlated to the target classes, we consider another ablation upon M2m-RS:\footnote{Here, we attempt to opt out any potential effect from using DRS, for more clearer evaluation.} 
instead of labeling the generated sample as the target class, the ablated method ``M2m-RS-Rand'' labels it to a ``random'' class chosen from all the possible classes (except for the target and original classes). The results shown in Table~\ref{table:abl1} indicate that M2m-RS-Rand generalizes much worse than its counterpart M2m-RS on the minority classes, which indeed confirms that the correctly-labeled synthetic samples could improve the generalization of the minority classes. 

\begin{figure*}[t]
\begin{center}
    {
    \subfigure[ERM]
        {
        \includegraphics[width=0.19\textwidth]{figures/tsne_erm50.pdf}
        \label{fig:tSNE-ERM}}
        \subfigure[ERM-SMOTE]
        {
        \includegraphics[width=0.19\textwidth]{figures/tsne_smote50.pdf}
        \label{fig:tSNE-SMOTE}} 
        \subfigure[LDAM-DRW]
        {
        \includegraphics[width=0.19\textwidth]{figures/tsne_ldam50_v2.pdf}
        \label{fig:tSNE-LDAM}}
        \subfigure[ERM-M2m (ours)]
        {
        \includegraphics[width=0.19\textwidth]{figures/tsne_ours50.pdf}
        \label{fig:tSNE-AO}
        }
    }
\end{center}
\vspace{-0.12in}
\caption{Visualization of the penultimate features via t-SNE computed from a balanced subset of CIFAR-LT-10 with ResNet-32.}
\label{fig:tSNE}
\vspace{-0.12in}
\end{figure*}

\vspace{-0.15in}
\paragraph{Comparison of t-SNE embeddings.} 
To further validate the effectiveness of our method, we visualize and compare the penultimate features learned from various training methods (including ours) using t-SNE \citep{maaten2008visualizing}. Each embedding is computed from a randomly-chosen subset of training samples in the CIFAR-LT-10 (), so that it consists of 50 samples per each class.
Figure~\ref{fig:tSNE} illustrates the results, and shows that the embedding from our training method (M2m) is of much separable features compared to other methods:
one could successfully distinguish each cluster under the M2m embedding (even though they are from minority classes), while others have some obscure region.

\begin{figure}[t]
\begin{center}
    \subfigure[CIFAR-LT-10]
        {
        \includegraphics[width=0.47\linewidth]{figures/CIFAR10_cum_v3.pdf}
        \label{fig:cumul-10}
        }
    \subfigure[CIFAR-LT-100]
        {
        \includegraphics[width=0.47\linewidth]{figures/CIFAR100_cum_v3.pdf}
        \label{fig:cumul-100}
        }
    \end{center}
    \vspace{-0.05in}
    \caption{Comparisons on cumulative number of false positive samples across class indices () on CIFAR-LT-10/100 test set in ResNet-32. Each plot reaches the total number of mistakes, \ie, the sum of off-diagonal entries in the confusion matrix.}
    \label{fig:cumul}
    \vspace{-0.15in}
\end{figure}

\vspace{-0.15in}
\paragraph{Comparison of cumulative false positive.} In Figure ~\ref{fig:cumul}, we plot how the number of false positive (FP) samples increases as summed over classes, namely  , from the most frequent class to the least one. Here,  indicates the number of misclassified samples by predicting them to class~ in the test set. We compute each plot with the \emph{balanced} test sets of CIFAR-LT-10/100, thereby a well-trained classifier would show a plot close to linear: it indicates the classifier mistakes more evenly over the classes. Overall, one could see that the curve made by our method consistently below the others with much linearity. This implies our method makes less false positives, and even better, they are more uniformly distributed over the classes. This is a desirable property in the context of imbalanced learning.

\vspace{-0.15in}
\paragraph{The use of adversarial examples.}
As mentioned in Section~\ref{ss:intuition}, the generation under M2m often ends up with a synthetic minority sample that is very close to the original (before translation) as like the \emph{adversarial example}. This indeed happens when  and  are neural networks as assumed here, \ie, ResNet-32, as illustrated in Figure~\ref{fig:generation}.
To understand more on how such adversarial perturbations affect our method, we consider a simple ablation, which we call ``M2m-Clean'': recall that our method synthesizes a minority sample  from a seed majority sample . This ablation uses the ``clean''  instead of  for over-sampling.
Under the identical training setup, we notice a significant reduction in the balanced accuracy of M2m-Clean compared to the original M2m (see Table~\ref{table:abl1}). This observation reveals that the adversarial perturbations ablated are extremely crucial to make our method to work, regardless of a small noise. 

\begin{figure}[t]
    \begin{center}
	    \includegraphics[width=0.75\linewidth]{figures/Generation8.pdf}
    \end{center}
    \vspace{-0.12in}
    \caption{An illustration of a synthetic minority sample by M2m, where  is assumed to be ResNet-32 trained by standard ERM. The noise image is amplified by 10 for better visibility.}
    \label{fig:generation}
    \vspace{-0.15in}
\end{figure}
