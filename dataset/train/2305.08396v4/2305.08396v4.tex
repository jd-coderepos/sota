\documentclass{article}

\pdfoutput=1

\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{fancyhdr}       \usepackage{graphicx}       \graphicspath{{media/}}     

\usepackage[square,numbers]{natbib}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{tabu}

\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

\fancyhead[LO]{MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation}




  
\title{MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation
}

\author{
  Abdul Rehman Khan\textsuperscript{1, 2}, Asifullah Khan\textsuperscript{1, 2, 3*} \\
  \textsuperscript{1} Pattern Recognition Lab, Department of Computer \& Information Sciences, Pakistan Institute of \\Engineering \& Applied Sciences, Nilore, Islamabad, 45650, Pakistan \\
  \textsuperscript{2} PIEAS Artificial Intelligence Center (PAIC), Pakistan Institute of Engineering \& Applied Sciences, \\Nilore, Islamabad, 45650, Pakistan \\
  \textsuperscript{3} Center for Mathematical Sciences, Pakistan Institute of Engineering \& Applied Sciences, Nilore, \\Islamabad, 45650, Pakistan \\
  \texttt{Corresponding Author: \textsuperscript{*}Asifullah Khan, asif@pieas.edu.pk} \\
}


\begin{document}
\maketitle


\begin{abstract}
Since their emergence, Convolutional Neural Networks (CNNs) have made significant strides in medical image analysis. However, the local nature of the convolution operator may pose a limitation for capturing global and long-range interactions in CNNs. Recently, Transformers have gained popularity in the computer vision community and also in medical image segmentation due to their ability to process global features effectively. The scalability issues of the self-attention mechanism and lack of the CNN-like inductive bias may have limited their adoption. Therefore, hybrid Vision transformers (CNN-Transformer), exploiting the advantages of both Convolution and Self-attention Mechanisms, have gained importance. In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision transformer (CNN-Transformer) for medical image segmentation. The proposed Hybrid Decoder, based on MaxViT-block, is designed to harness the power of both the convolution and self-attention mechanisms at each decoding stage with a nominal memory and computational burden. The inclusion of multi-axis self-attention, within each decoder stage, significantly enhances the discriminating capacity between the object and background regions, thereby helping in improving the segmentation efficiency. In the Hybrid Decoder block, the fusion process commences by integrating the upsampled lower-level decoder features, obtained through transpose convolution, with the skip-connection features derived from the hybrid encoder. Subsequently, the fused features undergo refinement through the utilization of a multi-axis attention mechanism. The proposed decoder block is repeated multiple times to progressively segment the nuclei regions. Experimental results on MoNuSeg18 and MoNuSAC20 dataset demonstrates the effectiveness of the proposed technique. Our MaxViT-UNet outperformed the previous CNN-based (UNet) and Transformer-based (Swin-UNet) techniques by a considerable margin on both of the standard datasets. The following \href{https://github.com/PRLAB21/MaxViT-UNet}{github} (https://github.com/PRLAB21/MaxViT-UNet) contains the implementation and trained weights.
\end{abstract}


\keywords{Image Segmentation \and Cancer Diagnostics \and Medical Image Analysis \and CNN-Transformer \and MaxViT \and Multi-Axis Attention \and Sparse Attention \and Vision Transformer \and UNet Architecture \and Auto Encoder}


\section{Introduction}\label{Introduction}
Image segmentation is critical in medical image analysis, which involves identifying and delineating specific structures or regions of interest from medical images \cite{kumar2019multi}. Nuclei segmentation, in particular, is a critical task that involves identifying the boundaries of nuclei cells in microscopic histopathology images \cite{kumar2019multi}. Nuclei segmentation is critical for improving the accuracy and dependability of medical diagnosis and treatment. It can aid in the development of personalized treatment plans and improve patient outcomes by enabling precise detection and quantification of nuclei.

Deep learning algorithms have shown exceptional performance in a variety of applications \cite{tayyab2022survey, sohail2023deep, khan2023survey_covid, Rauf2023-tc}, particularly picture segmentation \cite{long2015fully, ronneberger2015u, cao2022swin, rauf2023attention, ali2022channel, aziz2020channel, sohail2021mitotic, khan2023segmentation, khan2023survey, zhang2022multi, vu2019dense, liu2023mestrans}, in recent years. They have shown to be incredibly effective in doing this complicated task with accuracy and efficiency. Convolutional Neural Networks' capacity to automatically capture low-level to high-level characteristics hierarchically from a dataset makes them extremely useful for medical picture analysis \cite{khan2020survey}. Deep CNNs, in particular, are seeing increased use in medical image segmentation. Deep learning models such as Fully Convolutional Neural Network (FCNN) \cite{long2015fully} and UNet-like encoder-decoder architectures have emerged as powerful tools in medical picture segmentation, outperforming older methods \cite{ronneberger2015u, ibtehaz2020multiresunet, zhou2019unet++, oktay2018attention, cciccek20163d, cao2022swin}. Such networks extract deep features and combine high-resolution information using a combination of convolutional and down-sampling layers in the encoder and up-sampling layers in the decoder to provide pixel-level semantic predictions. They can extract features and understand complicated patterns from medical images, making them ideal for medical image processing tasks like segmentation.

More recently, vision transformers \cite{dosovitskiy2021an, chen2021transunet, valanarasu2021medical, cao2022swin, wang2022uctransnet} have also emerged as a powerful tool for the medical domain and have shown impressive results on a variety of segmentation tasks in medical imaging. The self-attention mechanism of vision transformers allows them to focus on relevant image regions while suppressing irrelevant features, giving them the potential to significantly improve the segmentation accuracy of medical images. It helps in effectively extracting the most important features in an image and capturing the long-range dependencies between them \cite{dosovitskiy2021an}. Recently, Swin-UNet \cite{cao2022swin} adapted the Swin Transformer for medical image segmentation by utilizing the shifted window attention mechanism in the decoder. Their architecture is purely transformer-based and lacks the inductive bias of convolutions. Hybrid approaches \cite{zhang2021transfuse, li2023attransunet, guo2022cmt, li2022next, yao2022transclaw, ji2021multi, zhou2021nnformer} try to tackle this problem by using convolutions along with self-attention in their encoder but either they suffer from quadratic nature of self-attention or use convolution-based decoders. To the best of our knowledge, Hybrid Decoder has never gained much attention for medical image segmentation tasks before. Our proposed idea utilizes a hybrid technique in both encoder and decoder and uses multi-axis attention with linear time complexity with respect to image dimensions.

Inspired by the impressive outcomes achieved by Multi-Axis self-attention (Max-SA) in previous research \cite{tu2022maxvit}, we have harnessed its potential for medical image segmentation and introduced a novel architecture called MaxViT-UNet, which adopts a UNet-style framework. By incorporating Max-SA, we have enhanced the multi-head self-attention mechanism, enabling the extraction of local and global level features in a computationally efficient manner. The hybrid design of the MaxViT-UNet encoder and decoder enables the generation of contextually rich features at higher levels and noise-free features at lower levels, which are crucial for achieving accurate medical image segmentation. The main outcomes of the proposed methodology are as follows:

\begin{enumerate}
\item MaxViT-UNet, a comprehensive hybrid image segmentation system based on the encoder-decoder architecture similar to UNet, is proposed. The framework is made up of two major components: the MaxViT-Encoder and the Proposed Hybrid Decoder. The encoder and suggested decoder are hybrid modules that use the MaxViT-block and skip connections to effectively process features.
\item The proposed Hybrid Decoder block, Hybrid Decoder, consists of two steps: (1) In the merging step, up-sampled features from a higher semantic level and encoder features from high spatial level are simply concatenated, (2) In the fusion step, MaxViT-blocks are utilized for efficiently processing and fusing concatenated information.
\item The proposed Hybrid Decoder, made by simply repeating the decoder block, is designed to be parameter efficient and computationally lightweight without compromising on the segmentation performance. The local and global attention throughout the proposed decoder helps in discarding irrelevant features for high-quality image segmentation.
\item The effectiveness of the proposed MaxViT-UNet has been demonstrated through experiments conducted on multiple datasets. Additionally, the ablation results indicate the promising prospects of utilizing a Hybrid Decoder for medical image segmentation.
\end{enumerate}

The following is the breakdown of the sections: In Section \ref{Related Works}, the recent CNN-based, Transformer-based, and Hybrid-based techniques in medical image segmentation are highlighted. In Section \ref{Proposed Methodology for MaxViT-UNet Framework}, the proposed MaxViT-UNet details are discussed in detail. In Section \ref{Experiments}, the dataset, results, and discussion are provided. The last section concludes the paper with possible future directions.

\section{Related Works}\label{Related Works}

The conventional approaches for medical image segmentation primarily relied on morphological operations (erosion, dilation, opening, and closing), or color, contour, and watershed-based techniques and traditional machine learning \cite{yang2006nuclei, veta2013automatic, tsai2003shape, held1997markov}. These approaches do not generalize well and suffer from different sources of variations in medical images such as variation in nuclei shape across various organs and tissue types, variation in color across crowded and sparse nuclei, variation in imaging equipment and hospitals/clinics protocol \cite{kumar2019multi}.

\subsection{CNN Based Techniques}\label{CNN Based Techniques}

In deep learning based medical image segmentation, one of the earlier approaches includes FCN (fully convolutional network) \cite{long2015fully}. Although FCN outperformed conventional techniques, its pooling operation resulted in loss of texture and edge information, necessary for segmentation. Therefore, Ronneberger et al. \cite{ronneberger2015u} proposed an encoder-decoder structure called UNet by improving the idea of FCN. Its U-shaped architecture connected the encoder and decoder at various stages via skip-connections to compensate for the semantic loss. The simple and unique architecture of UNet provided superior performance and led to the development of many variants in different medical image domains for segmentation. MultiResUNet \cite{ibtehaz2020multiresunet} replaced skip-connections with residual paths to extract semantic information at multiple scales. M-Net \cite{fu2018joint} captured multi-level semantic details by injecting rich multi-scale input features to different layers and processing them through a couple of downsampling and upsampling layers. UNet++ \cite{zhou2019unet++} proposed a new variant of UNet that incorporates dense connections nested together to effectively represent the fine-grained object information. DenseRes-Unet \cite{kiran2022denseres} used a dense bottleneck layer in Unet architecture for nuclei segmentation. With the help of channel-wise stitching in the encoder and skip connections in the decoder, AlexSegNet \cite{singha2023alexsegnet} uses an encoder-decoder framework based on the AlexNet architecture to combine low-level and high-level features. Recently, the idea of an attention mechanism has been applied to enhancing the segmentation performance of medical systems: AttentionUNet \cite{oktay2018attention} improved the segmentation performance of medical images using soft attention by introducing an attention gate module. The CA-Net \cite{gu2020net} merged the Spital, Channel, and Scale attentions into one comprehensive attention mechanism. Attention Assisted UNet \cite{wang2019sclerasegnet} also improved the attention mechanism for accurate segmentation of sclera images. NucleiSegNet \cite{lal2021nucleisegnet} utilized attention in the decoding stage. Cell-Net \cite{shi2022fine} uses multiscale and dilated convolutions to capture both global and local characteristics. Some notable work for end-to-end 3D medical image segmentation includes: 3D UNet \cite{cciccek20163d} which replaced 2D convolution with 3D convolution. V-net \cite{milletari2016v} also improved the UNet with the help of 3D convolution and proposed a dice loss for better segmentation masks. Recently, the idea of channel boosting-based CNNs (CB-CNN) has also emerged for medical image segmentation tasks \cite{ali2022channel, aziz2020channel, sohail2021mitotic, ali2023cb}, where diverse feature spaces from multiple encoders are fused together for improving the quality of segmentation models.

\subsection{ViT Based Techniques}\label{ViT Based Techniques}

The CNN-based U-shaped networks are effective but the convolution operation captures only the local information and discards global information. To prevent misclassification in segmentation, it is crucial to learn the long-range dependency between background and mask pixels. However, building deeper networks or using larger convolution kernels to capture long-range dependencies results in an explosion of parameters and the training process becomes costly. To address these issues, Dosovitskiy et al. \cite{dosovitskiy2021an} introduced Vision Transformers (ViT) with a multi-headed self-attention mechanism capable of capturing long-range dependencies in computer vision tasks. After the success of ViT in natural images and large datasets, medical image processing is also evolving with transformer-based techniques. One of the first techniques that combined transformer and UNet is TransUNet \cite{chen2021transunet}. Later, to efficiently handle smaller-sized medical image datasets, Medt \cite{valanarasu2021medical} improved the self-attention mechanism using a gated axial attention module. Swin-Unet \cite{cao2022swin}, a pure transformer architecture, adopted the Swin Transformer into a U-shaped encoder-decoder segmentation framework to extract local and global semantic features in a hierarchical fashion. UCTransNet \cite{wang2022uctransnet} replaced the skip-connection with the channel transformer (CTrans) module. Karimi et. al. \cite{karimi2021convolution} applied self-attention between neighboring image patches by modifying the MHSA mechanism of vision transformers. Despite the excellent performance in multiple image segmentation tasks. Vision Transformers suffer from the problem of computational overload, which has never been solved.

\subsection{Hybrid Based Techniques}\label{Hybrid Based Techniques}

The Transformer-based architecture outperforms CNN in capturing long-range dependencies, but it has the drawback of lacking interaction with surrounding feature information due to the division of images into fixed-size patches. Surpassing the current performance of medical image segmentation systems is difficult using transformer-based, or CNN-based UNet architectures. To overcome this challenge and enhance segmentation performance, researchers have combined Transformer with CNNs by utilizing the self-attention mechanism along with convolution operation to create a lightweight hybrid image segmentation framework \cite{zhang2021transfuse,li2023attransunet,guo2022cmt,li2022next}. Chen et al. \cite{chen2021transunet} proposed a strong encoder, by combining Transformer with CNN, for segmenting 2D medical images. Claw UNet \cite{yao2022transclaw} used the complementarity of Transformer and CNN to create hybrid blocks in the encoder for multi-organ dataset segmentation. Multi-Compound Transformer \cite{ji2021multi} achieved cutting-edge performance in six different benchmarks by integrating semantic information of hierarchical scales into a unified framework. Zhou et al \cite{zhou2021nnformer} applied CNN and transformer blocks in a crosswise manner to achieve better performance.

Above mentioned hybrid techniques still suffer from the quadratic nature of the self-attention mechanism and are unable to properly utilize the local feature extraction capability of CNNs with the global feature extraction capability of Transformers. In the proposed technique we interleaved convolution and self-attention efficiently at each stage and the linear nature of the multi-axis attention mechanism utilized makes our technique fast and robust for medical image segmentation tasks.

\section{Proposed Methodology for MaxViT-UNet Framework}\label{Proposed Methodology for MaxViT-UNet Framework}

\subsection{Architecture Overview of MaxViT-UNet Framework}\label{Architecture Overview of MaxViT-UNet Framework}

The proposed MaxViT-UNet consists of an encoder, bottleneck layer, proposed decoder, and skip connections. Figure \ref{fig:maxvit-unet-architecture} presents the complete architectural details of the proposed methodology. Throughout our encoder-decoder architecture, we utilized the identical MaxViT block structure, consisting of a parameter-efficient \texttt{MBConv} \cite{sandler2018mobilenetv2} and scalable \texttt{Max-SA} mechanisms \cite{tu2022maxvit}. The stem stage of the encoder (\(S0\)) downsamples the input image of shape \(C \times H \times W\) into \(64 \times \frac{H}{4} \times \frac{W}{4}\) using \(\texttt{Conv}3\times3\) layers. The input sequentially passes through four encoder stages \(S1\) to \(S4\). In each stage, the number of feature channels are doubled (\(64, 128, 256, 512\)) and the spatial size is reduced by half (\(\frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{32}\)), creating hierarchical features like UNet. The first \texttt{MBConv} block in each stage is responsible for doubling the input channels using \(\texttt{Conv}1\times1\) layer and halving the spatial size using \(\texttt{Depthwise Conv}3\times3\) layer. The last encoder layer, also called bottleneck, contains contextually rich features and provides a bridge from encoder to decoder.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=250pt]{maxvit_unet_02.jpg}
  \caption{Encoder-Decoder architecture of the Proposed MaxViT-UNet. The encoder generates hierarchical features at four scales. The proposed decoder first upscales the bottom-level features, merges them with skip-connection features, and applies the MaxViT-block a couple of times to produce an output mask image for the "C" number of classes.}
  \label{fig:maxvit-unet-architecture}
\end{figure}

The proposed Hybrid Decoder is composed of Transpose Convolutions and MaxViT-blocks. Inspired by UNet \cite{ronneberger2015u} it is symmetric in architecture, made up of three stages \(D1\) to \(D3\), matching with \(S1\) to \(S3\) stages of encoder respectively. The down-sampling operation in the encoder causes the loss of spatial information. To overcome this information, at each stage of the decoder, contextually rich features from the lower decoder stage are concatenated with locality-rich features of the encoder transferred through skip-connection. In contrast to the \(\texttt{Conv}3\times3\) layer in the encoder that shrinks the spatial size, a Transpose Convolution layer is employed for up-scaling the feature channels of the previous decoder stage. The concatenated features are transformed through a couple of MaxViT blocks before passing to the next stage. The MaxSA processing helps in reducing the noise information and simultaneously modeling the local-global differences between background and nuclei pixels. After the feature processing stages of the decoder, the feature maps of shape \(64 \times 4 \times 4\) are up-sampled four times to match the dimensions of the output mask with that of the input image and true mask \(H \times W\). The last convolution layer reduces the channels from 64 to C (number of classes) to generate the pixel-level segmentation probabilities for each class. The proposed MaxViT-UNet, though hybrid in nature, consists of only 24.72 million parameters, lighter than UNet with 29.06 million parameters and Swin-UNet with 27.29 million parameters. In terms of computation, the proposed MaxViT-UNet takes 7.51 GFlops as compared to UNet and Swin-UNet which take 50.64 and 11.31 GFlops. Table \ref{table:maxvit-unet-architecture} summarizes the architectural configurations of the MaxViT-UNet.

\begin{table}[ht!]
  \centering
  \caption{Configuration of the Proposed MaxViT-UNet architecture.}\label{table:maxvit-unet-architecture}
  \begin{tabular*}{\textwidth}{@{\extracolsep\fill}ccc}
    \toprule Encoder Level & Ouput Size & MaxViT Encoder \\
    \midrule
      Stem & (64, 128, 128) & $\begin{tabular}{c}
      Conv(k=3, s=2) \\
      Conv(k=3, s=1)
      \end{tabular}$ \\
    \midrule
      S1 & (64, 64, 64) & $\left\{\begin{tabular}{c}
      MBConv(E=4, R=4) \\
      Window-Rel-MSA(P=8, H=2) \\
      Grid-Rel-MSA(G=8, H=2)
      \end{tabular}\right\} \times 2 $ \\
    \midrule
      S2 & (128, 32, 32) & $\left\{\begin{tabular}{c}
      MBConv(E=4, R=4) \\
      Window-Rel-MSA(P=8, H=2) \\
      Grid-Rel-MSA(G=8, H=2)
      \end{tabular}\right\} \times 2 $ \\
    \midrule
      S3 & (256, 16, 16) & $\left\{\begin{tabular}{c}
      MBConv(E=4, R=4) \\
      Window-Rel-MSA(P=8, H=2) \\
      Grid-Rel-MSA(G=8, H=2)
      \end{tabular}\right\} \times 2/5 $ \\
    \midrule
      S4 & (512, 8, 8) & $\left\{\begin{tabular}{c}
      MBConv(E=4, R=4) \\
      Window-Rel-MSA(P=8, H=2) \\
      Grid-Rel-MSA(G=8, H=2)
      \end{tabular}\right\} \times 2 $ \\
    \midrule
      Decoder Level & Ouput Size & Hybrid Decoder \\
    \midrule
      \multirow{2}{*}{D1} & \multirow{2}{*}{(64, 64, 64)} & ConvTranspose(k=2, s=2) \\
      & & $\left\{\begin{tabular}{c}
      MBConv(E=4, R=4) \\
      Window-Rel-MSA(P=8, H=2) \\
      Grid-Rel-MSA(G=8, H=2)
      \end{tabular}\right\} \times 2 $ \\
    \midrule
      D2 & (128, 32, 32) & $\left\{\begin{tabular}{c}
      MBConv(E=4, R=4) \\
      Window-Rel-MSA(P=8, H=2) \\
      Grid-Rel-MSA(G=8, H=2)
      \end{tabular}\right\} \times 2 $ \\
    \midrule
      D3 & (256, 16, 16) & $\left\{\begin{tabular}{c}
      MBConv(E=4, R=4) \\
      Window-Rel-MSA(P=8, H=2) \\
      Grid-Rel-MSA(G=8, H=2)
      \end{tabular}\right\} \times 2 $ \\
    \hline
  \end{tabular*}
\end{table}

\subsection{MaxViT Block}\label{MaxViT Block}

The hybrid MaxViT-block effectively blends the multi-axis attention (MaxSA) mechanism with convolution, as shown in Figure \ref{fig:maxvit-unet-decoder-block}. It is based on the observation that convolution complements transformer attention by improving the generalization and the training speed of the network \cite{xiao2021early}. To this end, \texttt{MBConv} sub-block \cite{sandler2018mobilenetv2}, containing squeeze-and-excitation (SE) \cite{hu2018squeeze} attention, is used for feature processing before applying the multi-axis attention (MaxSA). Another benefit of the \texttt{MBConv} layer is that it eliminates the need for explicit positional encoding layers by acting as conditional position encoding (CPE) \cite{chu2023conditional} using \texttt{depth-wise convolutions}. In \texttt{MBConv} the expansion for the inverted bottleneck layer was set to 4 and the shrink rate for the squeeze-excitation layer was set to 0.25. After the \texttt{MBConv} layer, the block and grid self-attentions are stacked sequentially to model the local and global feature interactions simultaneously in a single block. Following the good design practices \cite{dosovitskiy2021an,cao2022swin}, the MaxViT block contains \texttt{LayerNorm} \cite{ba2016layer}, \texttt{Feed-Forward Networks} (FFNs) \cite{dosovitskiy2021an,cao2022swin}, and skip-connections in \texttt{MBConv}, block and grid attentions sub-blocks.

Let \(\textbf{Z}\) represent the feature tensor, given as input to MaxViT-block. The \texttt{MBConv} block, without downsampling, is given as:
\begin{align}
\textbf{Z} &= \textbf{Z} + \texttt{PROJ}(\texttt{SE}(\texttt{DWCONV}(\texttt{CONV}(\texttt{BN}(\textbf{Z})))))
\end{align}

where \texttt{BN} represents \texttt{BatchNormalization} layer \cite{ioffe2015batch}, \texttt{CONV} is expanding layer consisting of \(\texttt{Conv}1\times1\), \texttt{BatchNorm} and \texttt{GELU} \cite{hendrycks2016gaussian} activation function. \texttt{DWCONV} is processing layer consisting of \(\texttt{Depthwise Conv}3\times3\), \texttt{BatchNorm} and \texttt{GELU}. \texttt{SE} represents Squeeze-Excitation layer \cite{hu2018squeeze}, while \texttt{PROJ} reduces the number of channels using \(\texttt{Conv}1\times1\).

In each stage, the first \texttt{MBConv} downsample the input tensor \(\textbf{Z}\) using \(\texttt{Depthwise Conv}3\times3\) with a stride of 2, while the residual connection comprises of max-pool and channel-wise projection layers:
\begin{align}
\textbf{Z} &= \texttt{PROJ}(\texttt{MAXPOOL}(\textbf{Z})) + \texttt{PROJ}(\texttt{SE}(\texttt{DWCONV}\downarrow(\texttt{CONV}(\texttt{BN}(\textbf{Z})))))
\end{align}

\subsection{Max-SA: Multi-Axis Self-Attention}\label{Max-SA: Multi-Axis Self-Attention}

The self-attention introduced by transformer \cite{vaswani2017attention} and utilized by vision transformer \cite{dosovitskiy2021an} fall into the category of dense attention mechanism due to its quadratic complexity. Considering the effectiveness of sparse approaches for self-attention \cite{tu2022maxim,zhao2021improved}, Tu et al. \cite{tu2022maxvit} introduced an efficient and scalable self-attention module called multi-axis self-attention, which decomposes the original self-attention into sparse forms. (1) Window Attention for blocked local feature extraction and (2) Grid Attention for dilated global feature processing. Max-SA provides linear complexity without losing locality information.

The blocked local or window attention follows the idea of Swin Transformer \cite{cao2022swin}. Let \(\textbf{Z}\) represent a feature tensor of shape \(C \times H \times W\). The window partition layer reshapes \(\textbf{Z}\) into shape  (\(N, P \times P, C\)), where \(N=\frac{H}{P} \times \frac{W}{P}\) represents the total number of non-overlapping local windows, each of spatial shape \(P \times P\) and channel dimension \(C\). Each local window is passed through standard multi-head self-attention (MHSA) to model the local interactions. Finally, the window reverse layer reshapes \(\textbf{Z}\) back to \(C \times H \times W\).

In order to model global interactions, Max-SA incorporates grid attention, a simple and effective way of obtaining global relations in a sparse manner. The grid partition layer reshapes \(\textbf{Z}\) into shape (\(G \times G, N, C\)), to obtain \(G \times G\) number of global windows, each having dynamic spatial size represented with \(N=\frac{H}{G} \times \frac{W}{G}\), and channel dimension \(C\). Each global window is passed through standard multi-head self-attention (MHSA) to model the global interactions. Finally, the window reverse layer reshapes \(\textbf{Z}\) back to \(C \times H \times W\). The utilization of the grid-attention on the decomposed grid axis enables the global mixing of spatial tokens through dilated operations. With consistent window and grid sizes, the Max-SA mechanism ensures linear complexity in relation to the spatial size.

The CNNs are famous for their location equivariance inductive bias, a property lacking by vanilla self-attention mechanism \cite{dosovitskiy2021an,han2021transformer}. Max-SA attention blocks adopted the pre-normalized relative self-attention \cite{dai2021coatnet} as a solution to this problem. The relative self-attention mechanism \cite{dai2021coatnet,shaw2018self,jiang2021transgan,cao2022swin} adds a learnable relative bias to the attention weights, which proved to improve the self-attention mechanism.

Max-SA allows global-local feature interactions on various feature resolutions throughout the encoder and decoder architecture. In the proposed encoder-decoder architecture, both the window and grid sizes were fixed to \(8\) to make it compatible with \(256 \times 256\) image size, used for training and testing. The number of attention heads was set to \(32\) for all attention blocks.

\subsection{MaxViT-UNet Encoder}\label{Encoder}

The encoder of the proposed MaxViT-UNet framework is made of MaxViT architecture \cite{tu2022maxvit}, by simply stacking \texttt{MBConv} and \texttt{Max-SA} modules alternatively in a hierarchical fashion. Unlike the MaxViT \cite{tu2022maxvit}, where the number of blocks and channel dimensions are increased per stage to scale up the model. We used two MaxViT blocks per stage, to obtain a small and efficient encoder. Additionally, the third stage was repeated 2 times and 5 times for the MoNuSeg18 and MoNuSAC20 datasets, respectively. The multi-class nature of the MoNuSAC20 dataset demands higher-level discriminating features obtained by repeating the third stage 5 times. The four stages of our encoder produce hierarchical feature representation just like UNet. MaxViT takes advantage of the local-global receptive fields via convolution and local-global attention mechanisms, throughout the encoder from earlier to deeper stages, and shows better generalization ability and model capacity. The last stage of the encoder is named bottleneck as it contains semantic-rich features and provides a bridge from encoder to decoder.

\subsection{Proposed Hybrid Decoder}\label{Decoder}

The proposed Hybrid Decoder is designed by stacking layers of MaxViT-block in a hierarchical architecture, with a \texttt{Transpose Conv} layer at the start of each stage, as shown in Figure \ref{fig:maxvit-unet-decoder-block}. Similar to the encoder, we created a parameter-efficient decoder by using only two MaxViT blocks per stage. The decoder also enjoys the global and local receptive fields at all stages and is able to better reconstruct output masks as compared to previous approaches. Similar to Swin-UNet \cite{cao2022swin}, our decoder contains three stages that are connected with the corresponding top three stages of the encoder. Inside a single decoder block, features from the previous decoder layer are passed through the \texttt{Transpose Conv} layer for up-sampling and matching their shape with features coming from the skip-path. The up-sampled features are concatenated with the corresponding skip-connection features to obtain semantic and spatial-rich features. The MaxViT blocks further enhance them using \texttt{MBConv}, \texttt{local attention}, and \texttt{global attention} sub-block. Let \(\textbf{Y}^{\textbf{(i-1)}}\) the represents features coming from the previous decoder stage having dimension \(C \times H \times W\), and \(\textbf{Z}^{\textbf{(i)}}\) represent features coming from skip-connection at the same stage having dimensions \(C \times 2H \times 2W\), then the following equations represent the first block of each decoder stage:
\begin{align}
\textbf{Y}^{\textbf{(i)}} &= \texttt{UPCONV}(\textbf{Y}^{\textbf{(i-1)}}) \\
\textbf{Y}^{\textbf{(i)}} &= \texttt{GRID}(\texttt{BLOCK}(\texttt{MBCONV}(\texttt{CONCAT}(\textbf{Y}^{\textbf{(i)}}, \textbf{Z}^{\textbf{(i)}}))))
\end{align}

where \texttt{UPCONV}, consists of \texttt{Transpose Conv} layer, \texttt{BatchNorm} layer \cite{ioffe2015batch} and \texttt{Mish} activation function \cite{misra2019mish}, the \texttt{CONCAT} operator represents \texttt{concatenation}, and \texttt{MBCONV}, \texttt{GRID}, and \texttt{BLOCK} are sub-blocks of MaxViT-block for feature processing.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\linewidth]{maxvit_unet_decoder_block_detailed.jpg}
  \caption{Detailed architecture of the proposed Hybrid Decoder block. Features from the  \(i^{th}\) decoder stage are upscaled using \texttt{ConvTranspose2D} layer to match with size of the \((i+1)^{th}\) encoder stage coming from skip-connection. After the concatenation (\texttt{concat}) operation, the MaxViT-block is used a couple of times to merge the features efficiently.}
  \label{fig:maxvit-unet-decoder-block}
\end{figure}

\subsection{Loss Functions of the Proposed MaxViT-UNet}\label{Loss Functions of the Proposed MaxViT-UNet}

The models are penalized using the composite loss function consisting of \texttt{Cross-Entropy} and \texttt{Dice} loss functions with loss weights of \(\lambda1=1\) and \(\lambda2=3\) respectively in all the experiments. Both \texttt{Cross-Entropy} and \texttt{Dice} losses are calculated pixel-wise. In \texttt{dice} loss, only the nuclei classes were considered and the background was ignored to force the model to focus on nuclei regions more strongly. Given true mask \(\textbf{Y}_{t}\) and predicted mask \(\hat{\textbf{Y}}_{p}\) images, the mathematical form of both the loss functions is as follows:
\begin{align}
\texttt{Loss}(\textbf{Y}_{t}, \hat{\textbf{Y}}_{p}) &= \lambda1 * \texttt{CELoss}(\textbf{Y}_{t}, \hat{\textbf{Y}}_{p}) + \lambda2 * \texttt{DiceLoss}(\textbf{Y}_{t}, \hat{\textbf{Y}}_{p}) \\
\texttt{CELoss}(\textbf{Y}_{t}, \hat{\textbf{Y}}_{p}) &= -\sum_{i=1}^{H \times W} (\textbf{Y}_{t}^{i} * \texttt{log}(\hat{\textbf{Y}}_{p}^{i})) \\
\texttt{DiceLoss}(\textbf{Y}_{t}, \hat{\textbf{Y}}_{p}) &= 1 - \sum_{c=1}^{C} 2 \times \frac{|\textbf{Y}_{t}^{c} \cap \hat{\textbf{Y}}_{p}^{c}|}{|\textbf{Y}_{t}^{c}| + |\hat{\textbf{Y}}_{p}^{c}|}
\end{align}

In \texttt{CELoss}, \(\textbf{Y}_{t}^{i}\) represents the \(\textbf{i}^{\textbf{th}}\) pixel of true mask image and \(\hat{\textbf{Y}}_{p}^{i}\) represents the \(\textbf{i}^{\textbf{th}}\) pixel of predicted mask image, and summation is performed over all pixels (\(H \times W\)) to accumulate the error for a complete image. In \texttt{DiceLoss}, \(\textbf{Y}_{t}^{c}\) represents the \(\textbf{c}^{\textbf{th}}\) class channel of true mask image, and \(\hat{\textbf{Y}}_{p}^{c}\) represents the \(\textbf{c}^{\textbf{th}}\) class channel of predicted mask image, and summation is performed over all classes (C) to accumulate the error for all classes and all pixels.

\section{Experiments}\label{Experiments}

To demonstrate the efficacy of the proposed MaxViT-UNet segmentation framework, extensive experiments were performed on medical image segmentation tasks. The following section provides the details of the dataset used, pre-processing steps performed, the working environment, hyper-parameters, and performance metrics utilized for evaluation. Finally, the quantitative and qualitative results of the proposed MaxViT-UNet are compared with previous image segmentation techniques.

\subsection{Dataset Description}\label{Dataset Description}

To advance the research in this area, numerous competitions for medical picture segmentation tasks have been organized during the last few years. We decided to use the MoNuSeg 2018 \cite{kumar2019multi} and MoNuSAC 2020 \cite{verma2021monusac2020} challenge datasets to demonstrate the efficacy of our suggested MaxViT-UNet system. The information for both datasets was summarized in table \ref{table:dataset-summary}. Both datasets have their own challenges and deal with varying degrees of issues. The details of both datasets are highlighted in the following sections.

\begin{table}[ht!]
  \centering
  \caption{Summary of the datasets used to train and evaluate the proposed MaxViT-UNet.}\label{table:dataset-summary}
  \begin{tabular*}{\textwidth}{@{\extracolsep\fill}cccccc}
    \toprule Dataset & Classes & Subset & Images & Nuclei & Organs \\
    \hline
      \multirow{4}{*}{MoNuSeg18} & \multirow{4}{*}{\begin{tabular}{@{\ }c@{}} Background, \\Nuclei \end{tabular}} & Train & 30 & 21,623 & \begin{tabular}{@{\ }c@{}} Breast, Liver, Kidney, \\Prostate, Bladder, Colon, \\Stomach \end{tabular} \\
      \cmidrule{3-6}
      & & Test & 14 & 7,223 & \begin{tabular}{@{\ }c@{}} Breast, Kidney, Prostate, \\Bladder, Colon, Lung, \\Brain \end{tabular} \\
    \hline
      \multirow{4}{*}{MoNuSAC20} & \multirow{4}{*}{\begin{tabular}{@{\ }c@{}} Epithelial, \\Lymphocytes, \\Macrophoges, \\Neutrophils \end{tabular}} & Train & 46 & 31,411 & \begin{tabular}{@{\ }c@{}} Breast, Kidney, \\Lung, Prostate \end{tabular} \\
      \cmidrule{3-6}
      & & Test & 25 & 15,498 & \begin{tabular}{@{\ }c@{}} Breast, Kidney, \\Lung, Prostate \end{tabular} \\
    \hline
  \end{tabular*}
\end{table}

\subsubsection{MoNuSeg18}

The MoNuSeg 2018 challenge provided a challenging dataset \cite{kumar2019multi} comprising images from 7 different organs: (1) breast, (2) colon, (3) bladder, (4) stomach, (5) kidney, (6) liver, and (7) prostate. Also, images acquired from 18 different hospitals, practicing different staining techniques and image acquisition equipment, add another source of variation and ensure the diversity of nuclear appearances. The training data consists of 30 tissue images (1000×1000 resolution), 7 validation images, and 14 test images. The training dataset consists of 21623 annotated manually nuclear boundaries. For each selected individual patient from TCGA \cite{tcga}, an image was extracted from a distinct whole slide image (WSI) that was scanned at 40× magnification. Sub-images were selected from regions containing a high density of nuclei. To ensure diversity in the dataset, only one crop per WSI and patient was included. The test comprises 14 images spanning 5 organs common with the training set: (1) breast, (2) colon, (3) bladder, (4) kidney, (5) liver, and 2 organs different from the testing set: (1) lung, (2) brain, to make the test set more challenging. The test set contains approximately 7,223 annotated nuclear boundaries.

\subsubsection{MoNuSAC20}

The MoNuSAC20 dataset \cite{verma2021monusac2020} was designed to be representative of various organs and nucleus types relevant to tumor research. Specifically, it included Lymphocytes, Epithelial, Macrophages, and Neutrophils. The training data consisted of cropped whole slide images (WSIs) obtained from 32 hospitals and 46 patients from TCGA \cite{tcga} data portal, scanned at a \(40\times\) magnification. The dataset provides nuclei class labels along with nuclear boundary annotations. The testing data followed a similar preparation procedure but included annotations for ambiguous regions. These are regions with faint nuclei, unclear boundaries, or where the true class is not confirmed by annotators. The testing data comprised 25 patient samples from 19 different hospitals, with 14 hospitals overlapping with the training dataset.

\subsection{Dataset Pre-processing}\label{Dataset Pre-processing}

\subsubsection{MoNuSeg18}

For the MoNuSeg18 dataset \cite{kumar2019multi}, \(256 \times 256\) dimension patches (images and masks) from \(1000 \times 1000\) images were extracted to use for training and testing purposes of segmentation models. It was also ensured that training patches remain in the training set and testing patches remain in the testing set, to avoid leaking of the testing set and faulty evaluation metrics. Various augmentation techniques are utilized in order to increase the size of the dataset, including \textit{RandomAffine}, \textit{PhotoMetricDistortion}, \textit{Random Horizontal} and \textit{Vertical Flip} with \(0.5\) flip probability. The step-by-step outcome of these pre-processing steps is shown in figure \ref{fig:data-pipeline} for MoNuSeg18 \cite{kumar2019multi} dataset. Considering the modality differences between ImageNet and histopathology images, we calculated normalization parameters \texttt{(mean=[171.31, 119.69, 157.71], std=[56.04, 59.61, 47.69])} and used them for image normalization during training and testing phases.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{maxvit_unet_data_pipeline.jpg}
    \caption{Data Pre-processing Pipeline visualized for MoNuSeg18 dataset. From left to right: Original Image (resized to \(256 \times 256\)), Random Affine (combination of Shift, Scale, and Rotate), Random Flip (either Horizontal or Vertical), PhotoMetric Distortion (changes the intensity of pixels), Padding (to ensure \(256 \times 256\) image size), Final Augmented Input and Mask image are shown.}
    \label{fig:data-pipeline}
\end{figure}

\subsubsection{MoNuSAC20}

For the MoNuSAC20 dataset \cite{verma2021monusac2020}, the same pre-processing was applied as for the MoNuSeg18, i.e. \(256 \times 256\) dimension patches (images and masks) were extracted for training and testing. The same augmentation techniques were applied to increase the dataset size and robustness of the model. For MoNuSAC20, the ImageNet normalization parameters \texttt{mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375]} were utilized during training and testing phases as they obtained good results on this dataset.

\subsection{Working Environment}\label{Working Environment}

For the implementation, training, and evaluation of our proposed MaxViT-UNet and baseline models, we used MMSegmentation \cite{contributors2020openmmlab} (v0.24.1) and PyTorch \cite{pytorch} (v1.12.1) frameworks. We used conda (v4.12.0) for setting up our environment. All the trainings were done using NVIDIA DGX Station with 4 Tesla V100 GPUs, 120GB GPU memory, 256 GB RAM and Intel Xeon E5-2698 CPU.

\subsection{Training Details of the Proposed MaxViT-UNet}\label{Training Details of the Proposed MaxViT-UNet}

For fast training speed, we utilized the distributed training provided by MMSegmentation \cite{contributors2020openmmlab}. The batch size for a single GPU was set to 4, with an effective batch size of 16 due to distributed training on 4 GPUs. We experimented with SGD, Adam \cite{kingma2014adam}, AdaBelief \cite{zhuang2020adabelief}, and AdamW \cite{loshchilov2018decoupled} optimizers, and found the results of AdamW \cite{loshchilov2018decoupled} better than others. For all our final training, AdamW was used with a 0.005 starting learning rate, weight decay of 0.01, and values of betas were set to (0.9, 0.999) for optimizing our model through back-propagation. The Cosine learning rate scheduler was applied to gradually decrease the learning rate and allow the model to settle at the optima.

\subsection{Performance Metrics}\label{Performance Metrics}

The MoNuSeg18 and MoNuSAC20 datasets were evaluated using \texttt{Dice} and \texttt{IoU} evaluation metrics. Both the \texttt{Dice} and \texttt{IoU} are widely used segmentation metrics that produce values between 0 and 1. The \texttt{Dice} is equivalent to \texttt{F1-Score} in image segmentation tasks, and \texttt{IoU} is also referred to as the Jaccard Index. Mathematically, \texttt{Dice} and \texttt{IoU} are defined as:
\begin{align}
\texttt{Dice}(\textbf{Y}_{t}, \hat{\textbf{Y}}_{p}) &= \sum_{c=1}^{C} 2 \times \frac{|\textbf{Y}_{t}^{c} \cap \hat{\textbf{Y}}_{p}^{c}|}{|\textbf{Y}_{t}^{c}| + |\hat{\textbf{Y}}_{p}^{c}|} \\
\texttt{IoU}(\textbf{Y}_{t}, \hat{\textbf{Y}}_{p}) &= \sum_{c=1}^{C} \frac{|\textbf{Y}_{t}^{c} \cap \hat{\textbf{Y}}_{p}^{c}|}{|\textbf{Y}_{t}^{c} \cup \hat{\textbf{Y}}_c|}
\end{align}

where \(\textbf{Y}_{t}^{c}\) represents the \(\textbf{c}^{\textbf{th}}\) class channel of true mask image and \(\hat{\textbf{Y}}_{p}^{c}\) represents the \(\textbf{c}^{\textbf{th}}\) class channel of predicted mask image. The summation is performed over all classes (C) to accumulate the evaluation metric for a complete image.

\subsection{Results and Discussions}\label{Results and Discussions}

The proposed MaxViT-UNet is compared with previous techniques on both the MoNuSeg18 and MoNuSAC20 datasets. The following sections discuss the experimental results on each dataset in detail.

\subsubsection{Performance Evaluation of the Proposed MaxViT-UNet}\label{Performance Evaluation of the Proposed MaxViT-UNet}

\begin{table}[t!]
  \centering
  \caption{Comparative results of the proposed MaxViT-UNet framework with previous techniques on MoNuSeg 2018 Challenge Dataset}\label{table:results-monuseg18}
  \begin{tabular*}{\textwidth}{@{\extracolsep\fill}cccccc}
    \toprule Method & Dice & IoU  \\ \hline
      U-Net \cite{ronneberger2015u} & 0.8185 & 0.6927 \\
      U-net++ \cite{zhou2019unet++} & 0.7528 & 0.6089 \\
      AttentionUnet \cite{oktay2018attention} & 0.7620 & 0.6264 \\
      MultiResUnet \cite{ibtehaz2020multiresunet} & 0.7754 & 0.6380 \\
      Bio-net \cite{xiang2020bio} & 0.7655 & 0.6252 \\
      TransUnet \cite{chen2021transunet} & 0.7920 & 0.6568 \\
      ATTransUNet \cite{li2023attransunet} & 0.7916 & 0.6551 \\
      MedT \cite{valanarasu2021medical} & 0.7924 & 0.6573 \\
      UCTransnet \cite{wang2022uctransnet} & 0.7987 & 0.6668 \\
      FSA-Net \cite{zhan2023fsa} & 0.8032 & 0.6699 \\
      MBUTransUNet \cite{qiao2023mbutransnet} & 0.8160 & 0.6902 \\
      DSREDN \cite{chanchal2022deep} & 0.8065 & - \\
      Swin-Unet \cite{cao2022swin} & 0.7956 & 0.6471 \\
      \textbf{MaxViT-UNet (Proposed)} & \textbf{0.8378} & \textbf{0.7208} \\
    \hline
  \end{tabular*}
\end{table}

\begin{table}[t!]
  \centering
  \caption{Comparative results of the proposed MaxViT-UNet framework with previous techniques on MoNuSAC 2020 Challenge Dataset}\label{table:results-monusac20}
  \begin{tabular*}{\textwidth}{@{\extracolsep\fill}cccccc}
    \toprule Method & Dice & IoU  \\ \hline
      UNet \cite{ronneberger2015u} & 0.7197 & 0.5874 \\
      Hover-net \cite{wang2023improved} & 0.7626 & - \\
      Dilated Hover-net w/o ASPP \cite{wang2023improved} & 0.7571 & - \\
      Dilated Hover-net w/ ASPP \cite{wang2023improved} & 0.7718 & - \\
      MulVerNet \cite{vo2023mulvernet} & 0.7660 & - \\
      NAS-SCAM \cite{liu2020scam} & 0.6501 & - \\
      PSPNet \cite{zhao2017pyramid} & 0.7893 & 0.6594 \\
      Swin-Unet \cite{cao2022swin} & 0.4689 & 0.3924 \\
      \textbf{MaxViT-UNet (Proposed)} & \textbf{0.8215} & \textbf{0.7030} \\
    \hline
  \end{tabular*}
\end{table}

The comparative results of the proposed MaxViT-UNet along with the previous methodologies are presented in Table \ref{table:results-monuseg18} on the MoNuSeg18 dataset and Table \ref{table:results-monusac20} on the MoNuSAC20 dataset. For comparison on both datasets, UNet and Swin-UNet were trained using MMSegmentation \cite{contributors2020openmmlab} with the same hyper-parameters as the proposed technique. For the MoNuSeg18 dataset, we performed binary semantic segmentation. Whereas the MoNuSAC20 challenge contains four types of nuclei, we performed multi-class semantic segmentation for the MoNuSAC20 dataset. The proposed MaxViT-UNet beats the previous techniques by a large margin on both datasets and proves the significance of the hybrid encoder-decoder architecture. It surpassed the CNN-based UNet \cite{ronneberger2015u} by 2.36\% Dice score and 4.06\% IoU score; and Transformer-based Swin-UNet \cite{cao2022swin} by 5.31\% Dice score and 11.40\% IoU score on MoNuSeg18 dataset. Whereas, on the MoNuSAC20 dataset, the proposed framework surpassed UNet \cite{ronneberger2015u} by 14.14\% Dice and 19.68\% IoU scores; and Swin-UNet \cite{cao2022swin} by a large margin on Dice and IoU metrics as evident from Table \ref{table:results-monusac20}. The large improvement in both mDice and mIoU scores shows the significance of hybrid encoder-decoder architecture.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{MoNuSeg18_mDice.jpg}
    \caption{}
    \label{fig:monuseg18-mDice}
  \end{subfigure}
  \hfill
  \begin{subfigure}[]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{MoNuSeg18_mIoU.jpg}
    \caption{}
    \label{fig:monuseg18-mIoU}
  \end{subfigure}
  \hfill
  \begin{subfigure}[]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{MoNuSeg18_loss.jpg}
    \caption{}
    \label{fig:monuseg18-loss}
  \end{subfigure}
  \hfill
  \begin{subfigure}[]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{MoNuSAC20_mDice.jpg}
    \caption{}
    \label{fig:monusac20-mDice}
  \end{subfigure}
  \hfill
  \begin{subfigure}[]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{MoNuSAC20_mIoU.jpg}
    \caption{}
    \label{fig:monusac20-mIoU}
  \end{subfigure}
  \hfill
  \begin{subfigure}[]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{MoNuSAC20_loss.jpg}
    \caption{}
    \label{fig:monusac20-loss}
  \end{subfigure}
  \caption{Comparative plots of the proposed MaxViT-UNet with previous techniques on MoNuSeg18 and MoNuSAC20 challenge datasets. The top row displays the (a) Dice, (c) IoU, and (e) Training Loss on the MoNuSeg18 dataset, whereas the bottom row displays the (b) Dice, (d) IoU and (f) Training Loss on MoNuSAC20 dataset.}
  \label{fig:metric-plots}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{maxvit_unet_qualitative_monuseg-01.jpg}
    \caption{}
    \label{fig:qualitative-monuseg-01}
  \end{subfigure}
  \hfill
  \begin{subfigure}[]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{maxvit_unet_qualitative_monusac-01.jpg}
    \caption{}
    \label{fig:qualitative-monusac-01}
  \end{subfigure}
  \caption{Qualitative comparison of the proposed MaxViT-UNet with recent techniques on (a) MoNuSeg18 dataset; True-Positive, False-Positive, and False-Negative predictions are represented by the white, red, and blue, colors respectively. (a) MoNuSAC20 dataset; Epithelial, Lymphocyte, Macrophage, and Neutrophil are represented with Red, Yellow, Green, and Blue colors respectively.}
  \label{fig:qualitative-comparision}
\end{figure}

Figure \ref{fig:metric-plots} shows the learning curve plots of mDice (mean Dice), mIoU (mean IoU) and training loss on both MoNuSeg18 and MoNuSAC20 challenge datasets. The proposed MaxViT-UNet framework is represented by red curve lines in all six metric plots. The baselines are shown with different colors that are the same throughout all the metric plots. The training loss curve of the proposed MaxViT-UNet is very stable and lower than the baselines showing the stability and convergence of the proposed framework. The qualitative results on diverse images are presented on the MoNuSeg18 dataset in Figure \ref{fig:qualitative-monuseg-01} and the MoNuSAC20 dataset in Figure \ref{fig:qualitative-monusac-01}. The masks generated by MaxViT are less prone to error as compared to Swin-UNet \cite{cao2022swin} and vanilla UNet \cite{ronneberger2015u}, and it also gives relatively accurate boundaries. Figures \ref{fig:qualitative-monuseg-02} and \ref{fig:qualitative-monusac-02} compare the ground truth mask images and predicted mask images of the proposed MaxViT-UNet overlaid on histopathology images. For the MoNoSeg18 dataset \ref{fig:qualitative-monuseg-02}, the white color represents the true predicted regions whereas red and blue colors highlight the erroneous regions. For the MoNuSAC20 dataset \ref{fig:qualitative-monusac-02}, four different colors represent four types of nuclei classes in the dataset.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{maxvit_unet_qualitative_monuseg-02.jpg}
    \caption{}
    \label{fig:qualitative-monuseg-02}
  \end{subfigure}
  \hfill
  \begin{subfigure}[]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{maxvit_unet_qualitative_monusac-02.jpg}
    \caption{}
    \label{fig:qualitative-monusac-02}
  \end{subfigure}
  \caption{Comparison of True masks (top rows) and MaxViT-UNet predicted masks (bottom rows) overlaid on the histopathology images on (a) MoNuSeg18 dataset; True-Positive, False-Positive, and False-Negative predictions are represented by the white, red, and blue, colors respectively. (a) MoNuSAC20 dataset; Epithelial, Lymphocyte, Macrophage, and Neutrophil are represented with Red, Yellow, Green, and Blue colors respectively.}
  \label{fig:qualitative-overlay}
\end{figure}

\subsubsection{Ablation Study of the Proposed MaxViT-UNet}\label{Ablation Study of the Proposed MaxViT-UNet}

To inspect the efficacy of the proposed Hybrid Decoder, the MaxViT encoder was trained with UPerHead Decoder \cite{xiao2018unified} network. The UPerHead decoder takes advantage of the Pyramid Pooling Module (PPM) \cite{zhao2017pyramid} in the bottleneck and all of the decoding stages are based on the convolution layer. Table \ref{table:ablation-decoder} shows the comparative results on Dice and IoU metric for MoNuSeg18 and MoNuSAC20 datasets and proves the effectiveness of the proposed Hybrid Decoder. The large margin for the multi-class problem of MoNuSAC20 highlights that the proposed Hybrid Decoder has a better capacity at handling and distinguishing between regions of multiple classes. The hybrid nature of the proposed decoder may provide it with the ability to exploit large to small contexts at each scale. The symmetric decoder is designed to be used standalone in UNet-like encoder-decoder frameworks, and the improved performance shows that it has the potential for generating accurate segmentation masks with other types of encoders as well.

\begin{table}[ht!]
  \centering
  \caption{Ablation study results of the proposed MaxViT-UNet Decoder}\label{table:ablation-decoder}
  \begin{tabular*}{\textwidth}{@{\extracolsep\fill}ccccc}
    \toprule Method & Dataset & Image Size & Dice & IoU \\
    \midrule
      MaxViT with UPerNet Decoder & MoNuSeg18 & (256, 256) & 0.8176 & 0.6914 \\
      MaxViT-UNet (Proposed) & MoNuSeg18 & (256, 256) & \textbf{0.8378} & \textbf{0.7208} \\
    \midrule
      MaxViT with UPerNet Decoder & MoNuSAC20 & (256, 256) & 0.7148 & 0.5828 \\
      MaxViT-UNet (Proposed) & MoNuSAC20 & (256, 256) & \textbf{0.8215} & \textbf{0.7030} \\
    \hline
  \end{tabular*}
\end{table}

\section{Conclusion}\label{Conclusion}

In this work, we presented an encoder-decoder UNet-like hybrid vision transformer architecture, dubbed MaxViT-UNet, for medical image segmentation. The hybrid nature of the MaxViT, employed as an encoder, provides locally-globally rich hierarchical features at multiple scales. The proposed Hybrid Decoder effectively utilizes the MaxViT-block to generate output segmentation masks accurately. The building block of the proposed MaxViT-UNet framework is MaxViT-block, consisting of an MBConv convolution block followed by an efficient and scalable multi-axis attention mechanism (Max-SA). The proposed Hybrid Decoder is lightweight, computationally efficient, and designed as a plug-and-play module in UNet-like encoder-decoder architectures. Experiments on the MoNuSeg18 and MoNuSAC20 dataset reveals the effectiveness of the MaxViT hybrid encoder and proposed Hybrid Decoder. Our approach surpassed the previous approaches, specifically CNN-based (UNet) and Transformer-based (Swin-UNet), by a large margin on both datasets in terms of Dice, and IoU metrics.

In this study, currently we dealt with the segmentation problem of 2D histopathology images. In future, we intend to extend our proposed framework and Hybrid Decoder on other 2D/3D imaging modalities and real-world datasets. The concepts of channel boosting and ensemble of deep neural networks can be investigated in the future for developing more robust and generalized segmentation techniques.

\section*{Acknowledgements}

We thank Pattern Recognition Lab (PR-Lab) and the Pakistan Institute of Engineering and Applied Sciences (PIEAS), for providing the necessary computational resources and a healthy research environment.

\section*{Declarations}

\subsection*{Funding/Competing interests}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\subsection*{Availability of data and materials}
The datasets used in this work are publicly available.

\subsection*{Code availability}
The code is available on \href{https://github.com/PRLAB21/MaxViT-UNet}{github} (https://github.com/PRLAB21/MaxViT-UNet).

\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
