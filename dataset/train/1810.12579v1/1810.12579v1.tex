




\documentclass[11pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage[T1]{fontenc}
\usepackage[letterpaper]{geometry}
\usepackage[acceptedWithA]{tacl2018v2}
\makeatletter
\makeatother
\setlength\titlebox{6.5cm}    

\usepackage[mathscr]{euscript}
\usepackage{multirow}
\usepackage{url}

\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{forest} \usepackage{listings} \usepackage{calc}	  \usetikzlibrary{calc} \usetikzlibrary{arrows.meta} \usepackage{tikz}     \usetikzlibrary{arrows} \usepackage{pifont} \usepackage{booktabs,multirow} \usepackage{array,ragged2e}    \usepackage[normalem]{ulem} \usepackage{subcaption}
\usepackage{relsize} \usepackage{mymacros} \usepackage{multicol} 

\newcommand{\icolor}{\color{black}{}} \newcommand{\lcolor}{\color{black}{}} \newcommand{\mcolor}{\color{black}{}} \newcommand{\ecolor}{\color{black}{}}   \usepackage{pdfcolfoot} \newcommand{\addnote}[1]{\todo[size=\scriptsize]{#1}{}} \newcommand{\mytilde}{\raise.17ex\hbox{}}

\newcommand{\ttboxlab}[1]{\texttt{#1}}
\newcommand{\drgvar}[1]{\textbf{#1}}
\newcommand{\strout}[1]{\sout{\mbox{#1}}}
\newcommand{\wsp}{\hspace{3mm}}
\newcommand{\posn}[2]{#1\kern-0.15em.\kern-0.15em#2}
\newcommand{\bvar}[1]{#1}
\newcommand{\evar}[1]{#1}




\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Exploring Neural Methods for Parsing\\ Discourse Representation Structures}


\author{
 Rik van Noord \qquad Lasha Abzianidze \qquad Antonio Toral \qquad Johan Bos \\
 Center for Language and Cognition, University of Groningen \\
  {\sf \{r.i.k.van.noord, l.abzianidze, a.toral.ruiz, johan.bos\}@rug.nl} \\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Neural methods have had several recent successes in semantic parsing, though they have yet to face the challenge of producing meaning representations based on formal semantics.
We present a sequence-to-sequence neural semantic parser that is able to produce Discourse Representation Structures (DRSs) for English sentences with high accuracy, outperforming traditional DRS parsers. To facilitate the learning of the output, we represent DRSs as a sequence of flat clauses and introduce a method to verify that produced DRSs are well-formed and interpretable.
We compare models using characters and words as input and see (somewhat surprisingly) that the former performs better than the latter. We show that eliminating variable names from the output using De Bruijn-indices
increases parser performance. 
Adding silver training data boosts performance even further.
\end{abstract}

\section{Introduction}

Semantic parsing is the task of mapping a natural language expression to an interpretable meaning representation. Semantic parsing used to be the domain of symbolic and statistical approaches \cite{pereirashieber:1987,zelle1996learning,BlackburnBos:2005}. Recently however, neural methods, and in particular sequence-to-sequence models, have been successfully applied to a wide range of semantic parsing tasks. These include code generation \cite{ling:16}, question-answering  \cite{dong-lapata:16,he2016character} and Abstract Meaning Representation parsing \cite{konstas:17}.
Since these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree \cite{buys:17,alvarez:17}. These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations.

This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in Discourse Representation Theory (DRT), a well-studied formalism developed in formal semantics \cite{kamp:drt,van_der_sandt:92,kampreyle:drt,asher:drt,muskens:cdrt,eijckkamp:drt,kadmon:drt,asherlascarides}, dealing with many semantic phenomena: quantifiers, negation, scope ambiguities, pronouns, presuppositions, and discourse structure (see Figure~\ref{fig:semantic_parsing}). 
DRSs are recursive structures and form therefore a challenge for sequence-to-sequence models because they need to generate a well-formed structure and not something that looks like one but is not interpretable. 

The problem that we try to tackle bears similarities with the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR, \citealt{amr:13}). But there are notable differences between DRS and AMR.  
Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation.
And secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning \cite{damonte:17}.

DRS parsing has been attempted already in the 1980s for small fragments of English \cite{johnsonklein,wadaasher}. Wide-coverage DRS parsers based on supervised machine learning emerged later \cite{step2008:boxer,le:12,boxer,neural_drs_gmb:18}. 
The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following questions:

\begin{enumerate}[itemsep=-1.5mm, topsep=2mm]
    \item Are sequence-to-sequence models able to produce formal meaning representations (DRSs)?
    \item What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used?
    \item What is the best way of dealing with variables that occur in DRSs?
    \item Does adding silver data increase the performance of the neural parser?
    \item What parts of semantics are learned and what parts of semantics are still challenging?
\end{enumerate}

\noindent
We make the following contributions to semantic parsing:\footnote{The code is available here: \url{https://github.com/RikVN/Neural_DRS}.} 
(a) The output of our parser consists of interpretable scoped meaning representations, guaranteed by a specially designed checking tool (Section\,\ref{sec:method});
(b) We compare different methods of representing input and output in Section~\ref{sec:rep_exp};
(c) We show in Section~\ref{sec:exp_data} that employing additional, non-gold standard data can improve performance;
(d) We perform a thorough analysis of the produced output and compare our methods to symbolic/statistical approaches (Section~\ref{sec:discussion}).  

\begin{figure}[!t]
\centering
\parbox{0pt}{
\begin{tabbing}
12\=5\=\kill
{Raw input}:\\
\>\>\textsmaller[1]{\texttt{Tom isn't afraid of anything.}}
\1mm]
{The same DRS in a box format}:\-2mm]
\textsmaller[2]{\texttt{
\begin{tabular}[t]{l@{\kern5mm}l}\toprule
\drgvar{b0} DRS \drgvar{b1} &
\drgvar{b0} DRS \drgvar{b5}\\
\drgvar{b2} REF \drgvar{x1} &
\drgvar{b6} REF \drgvar{x3}\\
\drgvar{b2} male "\posn{n}{02}" \drgvar{x1} &
\drgvar{b6} female "\posn{n}{02}" \drgvar{x3}\\
\drgvar{b1} REF \drgvar{e1} & 
\drgvar{b5} REF \drgvar{e2}\\
\drgvar{b1} play "\posn{v}{03}" \drgvar{e1} &
\drgvar{b5} sing "\posn{v}{01}" \drgvar{e2}\\
\drgvar{b1} Agent \drgvar{e1} \drgvar{x1} &
\drgvar{b5} Agent \drgvar{e2} \drgvar{x3}\\
\drgvar{b1} Theme \drgvar{e1} \drgvar{x2} &
\drgvar{b5} Time \drgvar{e2} \drgvar{t2}\\
\drgvar{b3} REF \drgvar{x2} &
\drgvar{b7} REF \drgvar{t2}\\
\drgvar{b3} piano "\posn{n}{01}" \drgvar{x2} & 
\drgvar{b7} TPR \drgvar{t2} "now"\\
\drgvar{b4} REF \drgvar{t1} & 
\drgvar{b7} time "\posn{n}{08}" \drgvar{t2}\\
\drgvar{b4} time "\posn{n}{08}" \drgvar{t1} &
\drgvar{b0} CONTINUATION \drgvar{b1} \drgvar{b5}\\
\drgvar{b4} TPR \drgvar{t1} "now" &
\drgvar{b1} Time \drgvar{e1} \drgvar{t1}\\
\bottomrule
\end{tabular}
}}
\\\noalv{2mm}
\scalebox{.9}{
\begin{drstree}{-1}{4mm}{3mm}
[,phantom, s sep=3.2mm
[{\psdrs{b0}{}{
    \pdrs{b1}{}{
        play\\
        \wsp\\
        \wsp\\
        \wsp}
    \pdrs{b5}{}{
        sing\\
        \wsp\\
        \wsp}}
    {}}, name=b0, s sep=4mm
    [{\pdrs{b4}{}{
        \\
        \wsp}}
    ]
    [{\pdrs{b7}{}{
        \\
        \wsp}}
    ]    
    [{\pdrs{b2}{}{
        male}}, child anchor = north east
    ]
]    
    [{\pdrs{b3}{}{
        piano}}, name=b3, xshift=-28.3mm
    ]
    [{\pdrs{b6}{}{
        female}}, name=b6
    ]    
]
\draw[-{Stealth[scale=1.3]}] () -- (b3.north);
\draw[-{Stealth[scale=1.3]}] () -- (b6.north);
\end{drstree}}
\end{tabular}
}
\caption{A segmented DRS. Discourse relations are formatted with uppercase characters.}
\label{fig:drs_00_3008}
\end{figure} 


The DRS in Figure~\ref{fig:semantic_parsing} consists of a main box \ttboxlab{b0} and two presuppositional boxes, \ttboxlab{b1} and \ttboxlab{b2}. Note that \ttboxlab{b0} has no discourse referents but introduces negation via a single condition \ttboxlab{b3} with a nested box \ttboxlab{b3}.
The conditions of \ttboxlab{b3} represent unary and binary relations over discourse referents that are introduced either by \ttboxlab{b3} or the presuppositional DRSs.

A clausal form is another way of formatting DRSs. 
It represents a DRS as a set of clauses (see Figure~\ref{fig:semantic_parsing} and \ref{fig:drs_00_3008}).
This format is better suitable for machine learning than the box-format as it has a simple, flat structure and facilitates partial matching of DRSs which is useful for evaluation \cite{pmb-LREC:18}.
Conversion from the box-notation to the clausal form and vice versa is transparent: discourse referents, conditions, and discourse relations in the clausal form are preceded by the label of the box they occur in.
Notice that the variable letters in the semantic representations are automatically set and they simply serve for readability purposes. 
Throughout the experiments described in this paper, we employ clausal form DRSs.  

\section{Method}
\label{sec:method}

\subsection{Annotated Data}
\label{ssec:data}

We use the English DRSs from release 2.1.0 of the PMB \cite{PMBshort:2017}.\footnote{\url{http://pmb.let.rug.nl/data.php}} 
The release suggests to use the parts 00, 10, 20 and 30 as the development set, resulting in 3,998 train and 557 development instances. 
Basic statistics are shown in Table~\ref{pmb:stats}, while the number of occurrences of some of the semantic phenomena mentioned in Section~\ref{ssec:annot_corpora} are given in Table~\ref{pmb:semstats}. 

\begin{table}[t]
\centering
\scalebox{0.90}{
\begin{tabular}{lrrc}
\toprule
                     & \textbf{Sentences} & \textbf{Tokens} & \textbf{Avg tok/sent} \\ \midrule
\textbf{Gold train}            & 3,998             & 24,917     & 6.2       \\
\textbf{Gold test}              & 557              & 3,180  &  5.7          \\
\textbf{Silver}              & 73,778           & 638,610   & 8.7     \\ \bottomrule  
\end{tabular}
}
\caption{Number of documents, sentences and tokens for the English part of PMB release 2.1.0. Note that the number of tokens is based on the PMB tokenization, treating multi-word expressions as a single token.}
\label{pmb:stats}
\end{table}


\begin{table}[t]
\centering 
\scalebox{.92}{
\begin{tabular}{lrrr}
\toprule
\textbf{Phenomenon} & \textbf{Train} & \textbf{Test} & \textbf{Silver} \\  \midrule
\textbf{negation \& modals} 
    & 442 & 73 & 17,527\\
\textbf{scope ambiguity}    
    & 67 & 15 & 3,108\\
\textbf{pronoun resolution} 
    & 291 & 31 & 3,893\\
\textbf{discourse rel. \& imp.} 
    & 254 & 33 & 16,654\\
\textbf{embedded clauses} 
    & 160  & 30    & 46,458\\

\bottomrule 
\end{tabular}
}
\caption[]{Counts of relevant semantic phenomena for PMB release 2.1.0.\footnotemark{}
These phenomena are described and further discussed in Section\,\ref{sec:manual}.}
\label{pmb:semstats}
\end{table}


Since this is a rather small training set, we tune our model using 10-fold cross-validation (CV) on the training set, instead of tuning on a separate development set. This means that we will use the suggested development set as a test set (and refer to it as such). When testing on this set, we train a model on all available training data.The employed PMB release also comes with ``silver'' data, namely, 71,308 DRSs that are only partially manually corrected. 
In addition, we employ the DRSs from the silver data but without the manual corrections, which makes them ``bronze'' DRSs following the PMB terminology. Our experiments will initially use only the gold standard data, after which we will employ the silver or bronze data to further push the score of our best systems.\footnotetext{The phenomena are automatically counted based on clausal forms. The counting algorithm does not guarantee the exact number for certain phenomena, though it returned the exact counts of all the phenomena on the test data except the pronoun resolution (30).}

\subsection{Clausal Form Checker}
\label{ssec:clf_checker}

The clausal form of a DRS needs to satisfy a set of constraints in order to correspond to a semantically interpretable DRS, i.e., translatable into a first-order logic formula without free occurrences of a variable \cite{kampreyle:drt}.
For example, all discourse referents need to be explicitly introduced with a \texttt{REF} clause to avoid free occurrences of variables.

We implemented a clausal form checker that validates the clausal form if and only if it represents a semantically interpretable DRS.
Distinguishing box variables from entity variables is crucial for the validity checking, but automatically learned clausal forms are not expected to differentiate variable types.
First, the checker separately parses each clause in the form to induce variable types based on the fixed set of comparison and DRS operators.
After typing all the variables, the checker verifies whether the clauses collectively correspond to a DRS with well-formed semantics. 
For each box variable in a discourse relation, existence of the corresponding box inside the same segmented DRS is checked.
For each entity variable in a condition, an introduction of the binder (i.e., accessible) discourse variable is found.
The goal of these two steps is to prevent free occurrences of variables in DRSs.   
While binding the entity variables, necessary accessibility relations between the boxes are induced.
In the end, the checker verifies the transitive closure of the induced accessibility relation on loops and checks existence of a unique main box of the DRS.  

The checker is applied to every automatically obtained clausal form.
If a clausal form fails the test, it is considered as ill-formed and will not have a single clause matched with the gold standard when calculating the F-score.

\subsection{Evaluation}
\label{ssec:evaluation}

A DRS parser is evaluated by comparing its output DRS to a gold standard DRS using the Counter tool \cite{pmb-LREC:18}. Counter calculates an F-score over matching clauses. Since variable names are meaningless, obtaining the matching clauses essentially is a search for the best variable mapping between two DRSs. Counter tries to find this mapping by performing a hill-climbing search with a predefined number of restarts to avoid getting stuck in a local optimum, which is similar to the evaluation system \textsc{smatch} \cite{smatch:13} for AMR parsing.\footnote{Counter ignores \texttt{REF} clauses in the calculation of the F-score since they are usually redundant and therefore inflate the final score \cite{pmb-LREC:18}.} 
Counter generalises over WordNet synsets, i.e., a system is not penalised for predicting a word sense that is in the same synset as the gold standard word sense. 

To calculate whether there is a significant difference between two systems, we perform approximate randomization \cite{random-appr:89} with  = , \emph{R} =  and  as test statistic for each individual DRS pair. 

\subsection{Neural Architecture}
\label{sec:parameters}

We employ a recurrent sequence-to-sequence neural network (henceforth seq2seq) with two bidirectional LSTM layers and 300 nodes, implemented in OpenNMT \cite{opennmt:17}.
The network encodes a sequence representation of the natural language utterance, while the decoder produces the sequences of the meaning representation. We apply dropout \cite{dropout:14} between both the recurrent encoding and decoding layers to prevent overfitting, and use general attention \cite{luong15} to selectively give more weight to certain parts of the input sentence. An overview of the general framework of the seq2seq model is shown in Figure~\ref{fig:seq2seq}.

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.425]{seq2seq_bigtext.pdf}
  \caption{\label{fig:seq2seq}The sequence-to-sequence model with word-representation input. \texttt{SEP} is used as a special character to separate clauses in the output.}
\end{figure}

During decoding we perform beam search with length normalization, which in neural machine translation (NMT) is crucial to obtaining good results \cite{britz:17}. We experimented with a wide range of parameter settings, of which the final settings can be found in Table~\ref{tab:param}.

We opted against trying to find the best parameter settings for each individual experiment (next to impossible in terms of computing time necessary as a single 10-fold CV experiment takes 12 hours on GPU), but selected parameter settings that showed good performance for both the initial character and word-level representations (see Section~\ref{sec:rep_exp} for details). The parameter search was performed using 10-fold CV on the training set. Training is stopped when there is no more improvement in perplexity on the validation set, which in our case occurred after 13--15 epochs.

A powerful, well-known technique in the field of NMT is to use an ensemble of models during decoding \cite{sutskever:14,sennrich-wmt:16}. The resulting model averages over the predictions of the individual models, which can balance out some of the errors. In our experiments, we apply this method when decoding on the test set, but not for our experiments of 10-fold CV (this would take too much computation time).

\begin{table}[t]
\centering
\scalebox{0.815}{
\begin{tabular}{lrlr}
\toprule
\textbf{Parameter}   & \textbf{Value} & \textbf{Parameter}   & \textbf{Value}                 \\ \midrule
RNN-type             & LSTM           &  dropout              & 0.2     \\
encoder-type         & brnn           &  dropout type         & naive    \\
optimizer            & sgd            &  bridge               & copy    \\
layers               & 2              &  learning rate        & 0.7         \\
nodes                & 300            &  learning rate decay  & 0.7       \\
min freq source      & 3              &  max grad norm        & 5          \\
min freq target      & 3              &  beam size            & 10       \\
vector size          & 300            &  length normalisation & 0.9       \\ \bottomrule       
\end{tabular}
}
\caption{Parameters explored during training and testing with their final values. All other parameters have default values.}
\label{tab:param}
\end{table}

\section{Experiments with Data Representations}
\label{sec:rep_exp}

This section describes the experiments we conduct regarding the data representations of the input (English sentences) and output (a DRS) during training.

\subsection{Between Characters and Words}

We first try two (default) representations: character-level and word-level. Most semantic parsers use word-level representations for the input, but as a result are often dependent on pre-trained word embeddings or anonymization of the input
\footnote{This is done to keep the vocabulary small. An example is to change all proper names to \texttt{NAME} in both the sentence and meaning representation during training. When producing output, the original names are restored by switching \texttt{NAME} with a proper name found in the input sentence \cite{konstas:17}.}
to obtain good results. Character-level models avoid this issue but might be at a higher risk of producing ill-formed output.

\paragraph{Character-based model}

In the character-level model, the input (an English sentence) is represented as a sequence of individual characters. The output (a DRS in clause format) is linearized, with special characters indicating spaces and clause separators. The semantic roles  (e.g. \texttt{Agent}, \texttt{Theme}), DRS operators (e.g. \texttt{REF}, \texttt{NOT}, \texttt{POS}) and deictic constants (e.g. \texttt{"now"}, \texttt{"speaker"}, \texttt{"hearer"}) are not represented as character sequences, but treated as compound characters, meaning that \texttt{REF} is not treated as a sequence of \texttt{R}, \texttt{E} and \texttt{F}, but directly as \texttt{REF}.
All proper names, WordNet senses, time/date expressions, and numerals are represented as character sequences.

\paragraph{Word-based model}

In the word-level model, the input is represented as a sequence of words, using spaces as a separator (i.e., the original words are kept). The output is the same as for the character-based model, except that the character sequences are represented as words.
We use pre-trained GloVe embeddings \cite{glove:14}\footnote{The Common Crawl version trained on 840 billion tokens, vector size 300.} 
to initialise the encoder and decoder representations. In the DRS representation, there are semantic roles and DRS operators that might look like English words, but should not be interpreted as such (e.g. \texttt{Agent}, \texttt{NOT}). These entities are removed from the set of pre-trained embeddings, so that the model will learn them from scratch (starting from a random initialization). 

\paragraph{Hybrid representations: BPE}

We do not necessarily have to restrict ourselves to using only characters or words as input representation. In NMT, byte-pair encoding (BPE, \citealt{sennrich-haddow-birch:2016:P16-12}) is currently the \emph{de facto} standard \cite{bojar-EtAl:2017:WMT1}. 
This is a frequency-based method that automatically finds a representation that is in between character and word-level. 
It starts out with the character-level format and then does a predefined number of merges of frequently co-occurring characters. Tuning this number of merges determines if the resulting representation is closer to character or word-level. We explore a large range of merges (1k--100k), while applying a corresponding set of pre-trained BPE embeddings \cite{bpe_pretrain:18}. However, none of the BPE experiments improved on the character-level or word-level score (F-scores between 57 and 68), only coming close when using a small number of merges (which is very close to character-level anyway). Therefore this technique was disregarded for further experiments.			
			
\paragraph{Combined char and word}			
			
There is also a fourth possible representation of the input: concatenating the character and word-level representations. This is uncommon in NMT due to the large size of the embedding space (hence their preference for BPE), but possible here since the PMB data contains relatively short sentences. We simply add the word embedding vector after the sequence of character-embeddings for each word in the input and still initialise these embeddings using the pre-trained GloVe embeddings. 

\paragraph{Representation results}

The results of the experiments (10-fold CV) for finding the best representation are shown in Table~\ref{tab:rep}. Character representations are clearly better than word representations, though the word-level representation produces fewer ill-formed DRSs. Both representations are maintained for our further experiments. 
Although the combination of characters and words did lead to a small increase in performance over characters only (Table~\ref{tab:rep}), this difference is not significant. Hence, this representation is discarded in further experiments described in this paper.

\begin{table}[!htb]
\centering
\scalebox{0.95}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Prec} & \textbf{Rec} & \textbf{F-score} & \textbf{\%\,ill} \\ \midrule
Char           & 78.1 & 69.7 & 73.7  & 6.2            \\
Word           & 73.2 & 65.9 & 69.4   & 5.8             \\
Char + Word    & 78.9  & 69.7 & 74.0  & 7.5  \\ \bottomrule        
\end{tabular}
}
\caption{Evaluating different input representations. 
The percentage of ill-formed DRSs is denoted by  \%\,ill.}
\label{tab:rep}
\end{table}



\begin{figure*}[!t]
\hspace*{-3mm}
\centering
\begin{subfigure}{50mm}
\centering
\textsmaller[2]{\texttt{
\begin{tabular}[t]{@{}l@{}}
\midrule
\drgvar{b1} REF \drgvar{x1}\\
\drgvar{b1} male "\posn{n}{02}" \drgvar{x1}\\
\drgvar{b1} Name \drgvar{x1} "tom"\\
\drgvar{b2} REF \drgvar{t1}\\
\drgvar{b2} EQU \drgvar{t1} "now"\\
\drgvar{b2} time "\posn{n}{08}" \drgvar{t1}\\
\drgvar{b0} NOT \drgvar{b3}\\
\drgvar{b3} REF \drgvar{s1}\\
\drgvar{b3} Time \drgvar{s1} \drgvar{t1}\\
\drgvar{b3} Experiencer \drgvar{s1} \drgvar{x1}\\
\drgvar{b3} afraid "\posn{a}{01}" \drgvar{s1}\\
\drgvar{b3} Stimulus \drgvar{s1} \drgvar{x2}\\
\drgvar{b3} REF \drgvar{x2}\\
\drgvar{b3} entity "\posn{n}{01}" \drgvar{x2}\\
\midrule
\end{tabular}
}}
\caption{Standard naming}
\label{subfig:standard}
\end{subfigure}
\begin{subfigure}{50mm}
\centering
\textsmaller[2]{\texttt{
\begin{tabular}[t]{@{}l@{}}
\midrule
\drgvar{\1} male "\posn{n}{02}" \drgvar{@1}\\
\drgvar{\2} REF \drgvar{@2}\\
\drgvar{\2} time "\posn{n}{08}" \drgvar{@2}\\
\drgvar{\3}\\
\drgvar{\3} Time \drgvar{@3} \drgvar{@2}\\
\drgvar{\3} afraid "\posn{a}{01}" \drgvar{@3}\\
\drgvar{\3} REF \drgvar{@4}\\
\drgvar{\\lambda$-calculus, followed by pronoun and presupposition resolution \cite{candcboxer:2007,step2008:boxer}.
\textsc{spar} is a baseline parser that outputs the same (fixed) default DRS for each input sentence. 
We implemented a second baseline model, \textsc{sim-spar}, which outputs, for each sentence in the test set, the DRS of the most similar sentence in the training set. This similarity is calculated by taking the cosine similarity of the average word embedding vector (with removed stopwords) based on the Glove embeddings \cite{glove:14}. 

Table~\ref{tab:comparison} show the result of the comparison.
The neural models comfortably outperform the baselines.
We see that both our neural models outperform Boxer by a large margin when using the Boxer labelled silver data. However, even without this dependence, the neural models perform significantly better than Boxer.
It is worth noting that the character-level model significantly outperforms the word-level model, even though it cannot benefit from pre-trained word embeddings and from a tokenizer. 

Concurrently with our work, a neural DRS parser has been developed by \newcite{neural_drs_gmb:18}. They use a customised neural
seq2seq model, which produces the DRS in three stages. It first predicts the general (deep) structure of the DRSs, after which the conditions and referents are filled in. 
Unfortunately, they train and evaluate their parser on annotated data from the GMB rather than from the PMB (see Section~2). This, combined with the fact that their work is contemporaneous to the current paper, makes it difficult to compare the approaches. However, we see no apparent reason why their method should not work on the PMB data.

\subsection{Analysis}
\label{sec:analysis}

An intriguing question is what our
models actually learn, and what parts of meaning are still challenging for neural methods. We do this in two ways, by performing an automatic analysis and by doing a manual inspection on a variety of semantic phenomena. Table~\ref{tab:auto_eval} shows an overview of the different automatic evaluation metrics we implemented with corresponding scores of the three models.


\begin{table}[!t]
\centering
\scalebox{0.90}{
\begin{tabular}{lccc}
\toprule
                          & \textbf{Char} & \textbf{Word} & \textbf{Boxer} \\ \midrule
\textbf{All clauses}    & 83.6 & 83.1 & 74.3         \\ \midrule
\textbf{DRS Operators}      & 93.2 & 93.3 & 88.0          \\
\textbf{VerbNet roles}          & 84.1 & 82.5 & 71.4         \\
\textbf{WordNet synsets}   & 79.7  & 79.4  & 72.5      \\
\textbf{\quad nouns}   & 86.1  & 88.5  & 82.5      \\ 
\textbf{\quad verbs, adverbs, adj.} & 65.1  & 58.7  & 49.3      \\ \midrule
\textbf{Oracle sense numbers}   & 86.7  & 85.7 & 78.1       \\
\textbf{Oracle synsets} & 90.7  & 90.9 & 83.8        \\
\textbf{Oracle roles}    & 87.4  & 87.2 & 82.0
\\ \bottomrule 
\end{tabular}
}
\caption{F-scores of fine-grained evaluation on the test set of the three semantic parsers.}
\label{tab:auto_eval}
\end{table}

The character- and word-level systems perform comparably in all categories except for VerbNet roles, where the character-based parser shows a clear advantage (1.6\% absolute).
The score for WordNet synsets is similar, but the word-level model has more difficulty predicting synsets that are introduced by verbs than for nouns. It is clear that the neural models outperform Boxer consistently on each of these metrics (partly because Boxer picks the first sense by default). What also stands out is the impact of the word senses: with a perfect word sense disambiguation module (oracle senses) large improvements can be gained for all three parsers.

It is interesting to look at what errors the model makes in terms of producing ill-formed output. For both the neural parsers, only about 2\% of the ill-formed DRSs are ill-formed because of a syntactic error in an individual clause (e.g. \texttt{b1 Agent x1}, where a fourth argument is missing), while all the other errors are due to a violated semantic constraint (see Section~\ref{ssec:clf_checker}). In other words, the produced output is a syntactically well-formed DRS but is not interpretable. 

\begin{figure}[!t]
\hspace*{-2.2mm}
  \includegraphics[width=0.49\textwidth]{len_by_parser.pdf}
  \caption{\label{fig:senlen}Performance of each parser for sentences of different length.}
\end{figure}

To find out how sentence length affects performance, we plot in Figure~\ref{fig:senlen} the mean F-score obtained by each parser on input sentences of different lengths, from 3 to 10 words.\footnote{Shorter and longer sentences are excluded as there are fewer than 10 input sentences for any such length, e.g. there are only 3 sentences that have 2 words.}
We observe that all the parsers degrade with sentence length.
To find out whether any of the parsers degrades significantly more than any other, 
we build a regression model, in which we predict the F-score 
using as predictors the parser (char, word and Boxer), the sentence length and the number of clauses produced. According to the regression model, (i) the performance of all the three systems decreases with sentence length, thus corroborating the trends shown in Figure~\ref{fig:senlen} and (ii) the interaction between parser and sentence length is not significant, i.e., none of the parsers decreases significantly more than any other with sentence length. 
The fact that the performance of the neural parsers degrades with sentence length is not surprising, since they are based on the seq2seq architecture, and models built on this architecture for other tasks, such as machine translation, have been shown to have the same issue~\cite{toral-sanchezcartagena:2017:EACLlong}.



\subsection{Manual Inspection}

\label{sec:manual}

The automatic evaluation metrics provide overall scores but do not capture how the models perform on certain semantic phenomena present in the DRSs. Therefore, we manually inspected the test set output of the three parsers for the semantic phenomena listed in Table~\ref{pmb:semstats}. 
Below we describe each phenomenon and explain how the parser output is evaluated on them.


The \textit{negation \& modals} phenomenon covers possibility (\texttt{POS}), necessity (\texttt{NEC}), and negation (\texttt{NOT}).
The phenomenon is considered successfully captured if an automatically produced clausal form has the clause with the modal operator and the main concept is correctly put under the scope of the modal operator. 
For example, to capture the negation in Figure\,\ref{fig:semantic_parsing}, the presence of {\small \texttt{\drgvar{b0}\;NOT\;\drgvar{b3}}} and {\small\texttt{\drgvar{b3}\;afraid\;"\posn{a}{01}"\;\drgvar{s1}}} is sufficient.   
\textit{Scope ambiguity} counts nested pairs of scopal operators such as possibility (\texttt{POS}), necessity (\texttt{NEC}), negation (\texttt{NOT}), and implication (\texttt{IMP}).
\textit{Pronoun resolution} checks if an anaphoric pronoun and its antecedent are represented by the same discourse referent.   
\textit{Discourse relation \& implication} involves determining a discourse relation or an implication with a main concept in each of their scopes (i.e., boxes).
For instance, to get the discourse relation in Figure\,\ref{fig:drs_00_3008} correctly, a clausal form needs to include 
{\small\texttt{\drgvar{b0}\;CONTINUATION\;\drgvar{b1}\;\drgvar{b5}}}, {\small\texttt{\drgvar{b1}\;play\;"\posn{v}{03}"\;\drgvar{e1}}}, 
and {\small\texttt{\drgvar{b5}\;sing\;"\posn{v}{01}"\;\drgvar{e2}}}.
Finally, the \textit{embedded clauses} phenomenon verifies whether the main verb concept of an embedded clause is placed inside the propositional box (\texttt{PRP}).
This phenomenon also covers control verbs: it checks if a controlled argument of a subordinate verb is correctly identified as an argument of a control verb.

The results of the semantic evaluation of the parsers on the test set is given in Table\,\ref{tab:manual_eval}. 
The character-level parser performs better than the word-level parser on all the phenomena except 
one.
Even though both our neural parsers clearly outperformed Boxer in terms of F-score, they perform worse than Boxer on the selected semantic phenomena. Although the differences are not big, Boxer obtained the highest score for four out of five phenomena.
This suggests that just the F-score is perhaps not good enough as an evaluation metric, or that the final F-score should perhaps be weighted towards certain clauses. For example, it is arguably more important to capture a negation correctly than tense. 
Our current metric only gives a rough indication about the contents, but not about the inferential capabilities of the meaning representation.

\begin{table}[t]
\centering
\scalebox{.915}{
\begin{tabular}{@{~}l@{\kern4mm}r@{\kern4mm}c@{\kern3mm}c@{\kern3mm}c@{~}}
\toprule
\textbf{Phenomenon} & \textbf{\#} & \textbf{Char} & \textbf{Word} & \textbf{Boxer}
\\ \midrule
\textbf{negation \& modals} &73 &\bf 0.90 & 0.81  & 0.89    \\
\textbf{scope ambiguity}    &15 & 0.73   & 0.57   & \bf 0.80     \\
\textbf{pronoun resolution} &31 & 0.84     & 0.77  &\bf 0.90      \\
\textbf{discourse rel. \& imp.} &33 & 0.64  & 0.67    & \bf 0.82  \\
\textbf{embedded clauses}   &30 & 0.77       & 0.70    & \bf 0.87    \\
\bottomrule 
\end{tabular}
}
\caption{Manual evaluation of the output of the three semantic parsers on several semantic phenomena. Reported numbers are accuracies.}
\label{tab:manual_eval}
\end{table}

\section{Conclusions and Future Work}
\label{sec:conclusion}

We implemented a general, end-to-end neural seq2seq model that is able to produce well-formed DRSs with high accuracy (\textbf{RQ1}). Character-level models can outperform word-level models, even though they are not dependent on tokenization and pre-trained word embeddings (\textbf{RQ2}). It is beneficial to rewrite DRS variables to a more general representation (\textbf{RQ3}). Obtaining and employing additional data can benefit performance as well, though it might be better to use an external parser instead of doing a full self-training pipeline (\textbf{RQ4}). 
F-score is only a rough measure for semantic accuracy: Boxer still outperformed our best neural models on a subset of specific semantic phenomena (\textbf{RQ5}). 

We think there are a lot of opportunities for future work. Since the sentences in the PMB data set are relatively short, it makes sense to investigate seq2seq models performing well for longer texts. There are a few promising directions here that could combat the degrading performance on longer sentences. First, the Transformer model \cite{transformer:17} is an interesting candidate for exploration, a state-of-the-art neural model developed for MT that does not have worse performance for longer sentences. Second, a seq2seq model that is able to first predict the general structure of the DRS, after which it can fill in the details, similar to \newcite{neural_drs_gmb:18}, is something that could be explored. A third possibility is a neural parser that tries to build the DRS incrementally, producing clauses for different parts of the sentence individually, and then combining them to a final DRS.

Concerning the evaluation of DRS-parsers, we feel there are a couple of issues that could be addressed in future work. One idea is to facilitate computing F-scores tailored to specific semantic phenomena that are dubbed important, so the evaluation we performed in this paper manually could be carried out automatically.
Another idea is to evaluate the application of DRSs to improve performance on other linguistic or semantic tasks, in which DRSs that capture the full semantics will, presumably, have an advantage. A combination of glass-box and black-box evaluation seems a promising direction here \cite{Bos2008LREC,pmb-LREC:18}.

\section*{Acknowledgements}

This work was funded by the NWO-VICI grant ``Lost in Translation -- Found in Meaning'' (288-89-003). The Tesla K40 GPU used in this work was kindly donated to us by the NVIDIA Corporation. We also want to thank the three anonymous reviewers for their comments.

\bibliography{tacl}
\bibliographystyle{aclnatbib}

\appendix

\end{document}
