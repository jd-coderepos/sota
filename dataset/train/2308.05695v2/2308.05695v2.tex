
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs} \usepackage{multirow} \usepackage{pgfplots} \usepackage{scalefnt} \definecolor{color1}{RGB}{145,30,180}
\definecolor{color2}{RGB}{245,130,48}
\definecolor{color3}{RGB}{230,25,75}


\title{Masked Diffusion as Self-supervised \\ Representation Learner}



\author{Zixuan Pan$^1$, Jianxu Chen$^2$, Yiyu Shi$^1$\\
University of Notre Dame$^1$, Leibniz-Institut für Analytische Wissenschaften - ISAS - e.V.$^2$\\
\texttt{\{zpan3,yshi4\}@nd.edu, jianxu.chen@isas.de} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Denoising diffusion probabilistic models have recently demonstrated state-of-the-art generative performance and been used as strong pixel-level representation learners. This paper decomposes the interrelation between the generative capability and representation learning ability inherent in diffusion models. We present masked diffusion model (MDM), a scalable self-supervised representation learner that substitutes the conventional additive Gaussian noise of traditional diffusion with a masking mechanism. Our proposed approach convincingly surpasses prior benchmarks, demonstrating remarkable advancements in both medical and natural image semantic segmentation tasks, particularly within the context of few-shot scenario.
\end{abstract}

\section{Introduction}


Diffusion models \citep{DBLP:conf/icml/Sohl-DicksteinW15,DBLP:conf/nips/HoJA20} have recently demonstrated remarkable capabilities in synthesizing high quality images by iteratively generating progressively denoised images from initial noisy counterparts, following a Gaussian distribution.  Notably, certain efforts, such as cold diffusion \citep{bansal2022cold}, have attempted to replace the denoising step within diffusion with alternative corruption processes. However, these endeavors risk departing from the theoretical underpinnings of diffusion, as its foundational proofs predominantly rely on the Gaussian distribution—a distribution satisfied through the introduction of Gaussian noise into the images. Moreover, the unsatisfactory outcomes of cold diffusion underscore the irreplaceable nature of denoising within the diffusion process.

Nevertheless, when we narrow our focus to the self-supervised pre-training aspect of diffusion—akin to the approach undertaken in \citet{DBLP:conf/iclr/BaranchukVRKB22}, which leverages intermediate activations of a trained diffusion model for downstream segmentation tasks—the significance of denoising diminishes. 
As highlighted in the background section, the training process of a denoising diffusion model can be simplified as the reconstruction of the original image (or equivalently, the added noise) from a noisy version, with the noise being modulated by various timesteps. Consequently, when viewed as a representation learning technique, it is reasonable to interpret diffusion as an autoencoder performing denoising as a preliminary task under varying levels of complexity.
In this particular context, the denoising operation ceases to be uniquely pivotal, as there exists no conclusive evidence that denoising during pre-training inherently yields superior representations compared to repairing from alternative forms of corruption (e.g. masking).

Furthermore, we have identified a discrepancy between the pre-training generative task and the downstream dense prediction task, leading to a degradation in performance during fine-tuning. While the prediction task demands a focus on high-level, low-frequency structural aspects of images, generative models often allocate a significant portion of their capacity to capture low-level, high-frequency details, as highlighted in the work by \citet{DBLP:conf/icml/RameshPGGVRCS21}. 

Motivated by these insights, our study takes a departure from conventional denoising techniques within the diffusion framework. Instead, inspired by Masked Autoencoder (MAE) \citep{DBLP:conf/cvpr/HeCXLDG22}, we introduce a masking operation to replace adding noise to images and propose a novel self-supervised pre-training algorithm named masked diffusion model (MDM). Additionally, we propose a straightforward yet impactful strategy to bridge the gap between reconstructive pre-training tasks and downstream prediction tasks. This involves substituting the commonly used Mean Squared Error (MSE) loss in traditional denoising diffusion models and other reconstructive pre-training models with the Structural Similarity Index (SSIM) loss \citep{DBLP:journals/tip/WangBSS04}. Our approach achieved state-of-the-art accuracy, shedding light on the viability of alternative corruption strategies and highlighting the effectiveness of the SSIM loss in diffusion-based pre-training models.

Our contribution can be summarized as follows:

$\bullet$ We generalize denoising diffusion models as a self-supervised pre-training algorithm, employing denoising as a preliminary task to train an autoencoder for robust representation learning. Subsequently, we deviate from the conventional approach by eliminating the use of Gaussian noise in traditional diffusion models, introducing our novel masked diffusion model (MDM) instead.

$\bullet$ We adopt the Structural
Similarity Index (SSIM) loss to minimize the disparity between pre-training reconstruction task and downstream dense prediction task. This strategic adoption of SSIM not only narrows the disparity but also incentivizes the model to acquire more meaningful and semantically rich representations.

$\bullet$ Extensive experimental results show that MDM outperforms all existing baselines on multiple semantic segmentation datasets including medical images and natural images. Particularly noteworthy is the robust performance of MDM in the realm of few-shot segmentation tasks.


\section{Related Work}
\subsection{Diffusion Models}
Diffusion models \citep{DBLP:conf/icml/Sohl-DicksteinW15,DBLP:conf/nips/HoJA20} are a class of generative models which have been gaining significant popularity over recent years. Diffusion models define a Markov chain of diffusion steps to gradually add noise to data and train a deep neural network to invert the diffusion process from a known Guassian distribution. The more powerful architectures and more advanced objectives proposed by \citet{DBLP:conf/icml/NicholD21,DBLP:conf/nips/DhariwalN21} further improve the generative quality and diversity of diffusion models.

Numerous recent works  \citep{DBLP:conf/iclr/RissanenHS23,bansal2022cold,DBLP:conf/iclr/HoogeboomS23,daras2022soft} have endeavored to propose alternative degradation mechanisms for diffusion models, aiming to supplant the conventional additive Gaussian noise. Nevertheless, these works have encountered challenges: 
either they struggle to replicate the exceptional image quality achieved by traditional denoising diffusion \citep{bansal2022cold}, or they resort to blurring degradations, which essentially amount to introducing Gaussian noise in the frequency domain \citep{DBLP:conf/iclr/RissanenHS23,DBLP:conf/iclr/HoogeboomS23,daras2022soft}. There has not been a solid theoretical foundation or compelling empirical evidence showing that the Gaussian noise in diffusion models can be replaced. \looseness=-1

Our work revisits denoising diffusion models from a self-supervised representation learning perspective rather than image synthesising and takes the first step of actually removing the Gaussian noise in diffusion.

\subsection{Self-supervised Learning}
Self-supervised learning approaches aim to learn from unlabelled data via pre-text tasks \citep{DBLP:conf/iccv/DoerschGE15,DBLP:conf/iccv/WangG15,DBLP:conf/cvpr/PathakGDDH17,DBLP:conf/iclr/GidarisSK18}. Recently, masked autoencoder (MAE) \citep{DBLP:conf/cvpr/HeCXLDG22} shows remarkable capability as an effective pre-training strategy by reconstructing masked patches. \citet{DBLP:conf/iclr/BaranchukVRKB22} finds that denoising diffusion models (DDPM) learn semantic representations from a self-supervised denoising training process. Some concurrent work \citep{lei2023masked} tries to combine MAE and DDPM like us to get a more powerful and faster image synthesizer, while our work focus on the representation learning ability. \citet{wei2023diffusion} proposes DiffMAE and evaluates it on downstream recognition tasks. DiffMAE still keeps denoising process in DDPM and conditions diffusion models on masked input. DiffMAE does not achieve better fine-tuning performance than MAE. In contrast, our work fully removes the additive Gaussian noise in diffusion and outperforms both MAE and DDPM on downstream semantic segmentation tasks.

\section{Background}
This section briefly introduces the background knowledge of two pre-training models, on which our proposed method is mainly based. 

\subsection{Denoising Diffusion Probabilistic Models (DDPM)}

Given a data distribution $x_{0} \sim q(x_{0})$, each step of the Markovian \emph{forward process} (\emph{diffusion process}) can be defined by adding Gaussian noise according to a variance schedule $\beta_{1}, ... , \beta_{T}$:
\begin{equation}
    q(x_{t}|x_{t-1}) := \mathcal N(x_{t}; \sqrt{1 - \beta_{t}}x_{t}, \beta_{t}I) \nonumber
\end{equation}
A special property of the forward process allows sampling $x_{t}$ directly from a Gaussian distribution:
\begin{equation}
    \begin{split}
    q(x_{t}|x_{0}) := \mathcal N(x_{t}; \sqrt{\bar{\alpha}_{t}}x_{0}, (1-\bar{\alpha}_{t})I),&\\
    x_{t} = \sqrt{\bar{\alpha}}x_{0} + \epsilon\sqrt{1-\bar{\alpha}_{t}}, \epsilon \sim \mathcal N(0, I),& \label{diffusion}
    \end{split}
\end{equation}
where $\alpha_{t}:=1-\beta_{t}$ and $\bar{\alpha}_{t}:=\prod_{s=1}^{s}\alpha_{s}$.

To sample from the data distribution \(q(x_0)\), a reverse sampling procedure (the \emph{reverse process}) can be employed by first sampling from \(q(x_T)\) and then iteratively sampling reverse steps \(q(x_{t-1}|x_t)\) until reaching \(x_0\). By adopting the setting for \(\beta_{T}\) and $T$ in \citet{DBLP:conf/nips/HoJA20}, the outcome is that $q(x_{T}) \approx \mathcal N(x_{T}; 0, I)$, which makes sampling $x_{T}$ trivial. The only thing left is therefore training a model $p_{\theta}(x_{t-1}|x_t)$ to approximate the unknown $q_(x_{t-1}|x_t)$. 

Based on Bayes theorem, $q(x_{t-1}|x_{t})=\frac{q(x_{t}|x_{t-1})q(x_{t-1})} {q(x_t)}$ is intractable since both $q(x_{t-1})$ and $q(x_t)$ are unknown. \citet{DBLP:conf/iclr/0011SKKEP21} shows that the true $q(x_s|x_t) \rightarrow q(x_s|x_t, \hat{x_0})$ as $s \rightarrow t$ for $s<t$. Therefore, the tractable posterior conditioned on $x_0$ can be considered as a compromise:
\begin{equation}
    \begin{split}
        q(x_{t-1}|x_{t}, x_0) &= \frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_0)} {q(x_t|x_0)}\\
        &= \mathcal N(x_{t-1}; \widetilde{\mu}_t(x_t, x_0), \widetilde{\beta}_tI),\label{posterior}
    \end{split}
\end{equation}
where $\widetilde{\mu}_t(x_t, x_0):=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_{t}}x_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}x_t$, $\widetilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_t$. Assuming a well-trained neural network \(f_\theta\) that estimates \(x_0\) from \(x_t\), the targeted \(q(x_{t-1}|x_{t})\) can be obtained by substituting the actual \(x_0\) in Equation \ref{posterior} with the estimated \(\hat{x_0}\):
\begin{equation}
    q(x_{t-1}|x_{t}) \approx q(x_{t-1}|x_{t}, \hat{x_0}=f_{\theta}(x_t, t)) \nonumber
\end{equation}

To produce better samples, instead of directly predicting $x_0$, \citet{DBLP:conf/nips/HoJA20} proposes an equivalent training strategy which trains a neural network $\epsilon_\theta(x_t, t)$ to predict the noise $\epsilon_t$ from Equation \ref{diffusion}. Once $\hat{\epsilon_t} = \epsilon_\theta(x_t, t)$ is available, $\hat{x_0}$ can be easily derived by:
\begin{equation}
   \hat{x_0} = \frac{1}{\sqrt{\bar{\alpha}}}(x_t - \sqrt{1-\bar{\alpha}_{t}}\hat{\epsilon_t}) \nonumber
\end{equation}
The network is optimized by the commonly used simple objective:
\begin{equation}
   \mathcal{L}_{\textrm{DDPM}} := E_{t \sim [1, T], x_0 \sim q(x_0), \epsilon \sim \mathcal{N}(0, I)}[\Vert \epsilon - \epsilon_\theta(x_t, t)\Vert^{2}] \label{ddpm_loss}
\end{equation}

\subsection{Masked Autoencoders (MAE)} 

MAE is a powerful self-supervised pre-training algorithm which learns representations by reconstructing the masked patches based on the visible patches. In detail, the full image is firstly divided into non-overlapping patches following Vision Transformers (ViT) \citep{DBLP:conf/iclr/DosovitskiyB0WZ21}. Then a random subset of patches is sampled and used as input for the ViT-based MAE Encoder. The encoder embeds these visible patches by a linear projection and incorporates corresponding positional embeddings, followed by processing the resulting set using a sequence of Transformer blocks.

The MAE decoder takes a full set of tokens consisting of patch-wise representations from the encoder and learnable mask tokens with positional embeddings. Since the decoder is only used during pre-training for the reconstruction task, it can be designed in a manner that is much lighter than the encoder. \looseness=-1

In MAE, the loss function is determined by calculating the mean squared error (MSE) between the previously masked portion of the reconstructed images and the corresponding original images at the pixel level. In particular, MAE employs the normalized pixels as the reconstruction target, leading to an improvement in representation quality.



\section{Methodology}

\begin{figure*}[!tp]
\centering
\includegraphics[width=1\columnwidth]{model.pdf} \caption{\textbf{Overview of our proposed method.} During pre-training, only the masked diffusion model (Encoder and Decoder) in Step 1 is trained. During downstream segmentation task, the pre-trained model in Step 1 is frozen as a representation generator and the segmentation network in Step 2 is trained with the representations from Step 1.}
\label{Model_overview}
\end{figure*}


Our proposed masked diffusion model (MDM) is a variant of traditional denoising diffusion probabilistic models (DDPM) that reconstructs the whole image given partial observation. While DDPM serves as both an impressive image synthesizer and a robust self-supervised representation learner, MDM, in contrast, is solely designed for self-supervised representation learning. Following \citet{DBLP:conf/iclr/BaranchukVRKB22}, the pixel-level representations extracted from a self-supervised pre-trained MDM are used for downstream dense prediction tasks, e.g., image segmentation. An overview of our method is shown in Figure \ref{Model_overview}. 
The following subsections will provide a detailed explanation of the pre-training process and how the learned representations are applied to the downstream image segmentation task.


\subsection{Self-supervised Pre-training with Masked Diffusion Model}
According to Equation \ref{ddpm_loss}, DDPM is trained by constantly predicting the noises in the input noisy images. Well-trained DDPM produces high-quality representations for label efficient segmentation \citep{DBLP:conf/iclr/BaranchukVRKB22}. From the perspective of self-supervised pre-training, DDPM can be treated as a special kind of denoising autoencoders (DAE)\footnote{In denoising autoencoders (DAE), the term ``denoising'' represents restoring original data from data degraded by a certain kind of corruption. Conversely, the term ``denoising'' in other places within this paper explicitly refers to the process of removing introduced Gaussian noise in noisy images and reconstructing the original clean images.}. DAE degrades an input signal under different corruptions and learns to reconstruct the original signal. In particular, DDPM corrupts the original signal by adding a Gaussian noise controlled by a timestep $t$. Inspired by MAE and DDPM, our masked diffusion model (MDM) takes an image, where a random portion is masked based on the timestep $t$, and subsequently restores the image. 

\textbf{Architecture.}
Following guided diffusion \citep{DBLP:conf/nips/DhariwalN21}, MDM uses the UNet architecture, which has been found to be highly effective for diffusion models. 
The UNet model consists of a series of residual layers and downsampling convolutions, followed by another set of residual layers with upsampling convolutions. The layers with the same spatial size are connected by skip connections. All the residual blocks follow the design in BigGAN \citep{DBLP:conf/iclr/BrockDS19}. The model uses self-attention blocks at the $32\times32$, $16\times16$, and $8\times8$ resolutions. Diffusion step $t$ is embedded into each residual block.

\textbf{SSIM Loss.} Both MAE and DDPM calculate the Mean Squared Error (MSE) loss between the reconstructed and original images to assess the reconstruction quality. Nevertheless, we have observed that a generative model's ability to generate images with low MSE loss does not consistently guarantee the high-quality of semantic representations extracted from the model.
As a solution, we turn to Structural Similarity Index (SSIM) loss, which is also commonly employed in image restoration tasks. SSIM loss guides the reconstruction process to create images that are more similar to the original ones in terms of their structural information \citep{DBLP:journals/tip/WangBSS04}. By doing so, we aim to narrow the gap between the reconstruction task and the subsequent segmentation task, as structural information is often pivotal for accurate segmentation. The effectiveness of SSIM loss is further shown and discussed in ablation studies. MDM is finally optimized by the following objective:
\begin{align}
    &\mathcal{L}_{\textrm{MDM}} := E_{t \sim [1, T], x_0 \sim q(x_0)}[\mathcal{L}_{\textrm{SSIM}}(x_0, \mathcal{U}_{\theta}(x_t, t))], \label{MDM_Loss} \\ 
   &\mathcal{L}_{\textrm{SSIM}}(x, \hat{x}) := \frac{1 - \textrm{SSIM}(x, \hat{x})}{2},\notag\\
    &\textrm{SSIM}(x, \hat{x}) := \frac{(2\mu_x\mu_y+c_1)(2\sigma_{xy}+c_2)}{(\mu_{x}^{2} + \mu_{y}^{2} + c_1)(\sigma_{x}^{2} + \sigma_{y}^{2} + c_2)} \notag,
\end{align}
where $c_1=(k_1L)^2$ and $c_2=(k_2L)^2$ are two variables to stabilize the division with weak denominator, $L$ is the dynamic range of the pixel-values.

\textbf{Masking and Reconstructing.}
MDM is trained by iterative masking and reconstructing until the model converges. We describe the detailed training procedure given a single image and one sampled timestep below.
Given an image \(x_0 \in \mathbb{R}^{H \times W \times C}\) sampled from the data distribution \(q(x_0)\), we divide and reshape it into \(N = \frac{H \times W}{P^2}\) patches denoted as \(p_0^{(1)}, p_0^{(2)}, \ldots, p_0^{(N)}\). Each patch, denoted as \(p_{0}^{(i)}\), is represented as a vector with a size of \(P^2C\), where \(P\) is the patch size and \(C\) is the number of channels in the image. To initiate the diffusion process, we first sample a diffusion timestep \(t\) from the interval $ [1, T] $, and the masking ratio $R_m$ can be defined as $R_m = \frac{t}{T+1}$. Then we randomly shuffle the list of patches and replace the last $\left\lfloor R_m \times N \right\rfloor$ patches with zero values. Afterward, we unshuffle the list to its original order, resulting in the corrupted image \(x_t\). In accordance with the reverse procedure employed in DDPM, the UNet model denoted by \(\mathcal{U}_\theta\) takes both the corrupted image \(x_t\) and its corresponding timestep \(t\) as input and subsequently generates an estimate for the intact image, denoted as \(\hat{x_0} = \mathcal{U}_\theta(x_t, t)\). The optimization of the \(\mathcal{U}_\theta\) model is achieved using the \(\mathcal{L}_{\textrm{MDM}}\) loss function defined in Equation \ref{MDM_Loss}. 

\subsection{Downstream Few-shot Segmentation}
In this paper, we explore a few-shot semi-supervised setting where we first pre-train MDM on a large unlabelled image dataset \(\mathcal{X}_{\textrm{unlabelled}}=\{x_{0}^{1}, ..., x_{0}^{N} \} \in \mathbb{R}^{H \times W \times C}\) using the self-supervised approach as previously described, and subsequently, we  leverage the features extracted from the MDM decoder to train a segmentation head $S_\phi$ (MLP) on a smaller dataset \(\mathcal{X}_{\textrm{labelled}}=\{x_{0}^{1}, ..., x_{0}^{M}\}\in \mathbb{R}^{H \times W \times C}\) with K-class semantic labels \(\mathcal{Y}_{\textrm{labelled}}=\{y_{0}^{1}, ..., y_{0}^{M}\}\in \mathbb{R}^{H \times W \times K}\). 
It is worth noting that \(\mathcal{X}_{\textrm{labelled}}\) is not necessarily a subset of \(\mathcal{X}_{\textrm{unlabelled}}\). Our experiments demonstrate that despite the distinction between \(\mathcal{X}_{\textrm{unlabelled}}\) and \(\mathcal{X}_{\textrm{labelled}}\), the features learned from \(\mathcal{X}_{\textrm{unlabelled}}\) can still yield valuable benefits for accurately segmenting \(\mathcal{X}_{\textrm{labelled}}\), as long as both datasets belong to the same domain. 

To extract features for our analysis, we focus on a subset of UNet decoder blocks at a specific diffusion step \(t\). We feed the labelled $\mathcal{X}_{\textrm{labelled}}$ and the diffusion timestep $t$ into the pre-trained Diffusion \(\mathcal{U}_\theta\), extracting the features based on the specified blocks setting $\mathcal{B}$.
The extracted features are upsampled to match the image size and then concatenated, resulting in feature vectors \(\mathcal{F}_{\textrm{labelled}} = \{f_{t, \mathcal{B}}^{1}, ..., f_{t, \mathcal{B}}^{M}\} \in \mathbb{R}^{H \times W \times C_f}\), where \(C_f\) represents the number of channels, which varies depending on the blocks setting \(\mathcal{B}\). The loss of the segmentation network $S_\phi$ is defined as follows:
\begin{align}
    &\mathcal{L}_{\textrm{Seg}} := \frac{1}{M} \sum_{i}^{M} \textrm{CrossEntropy}(S_\phi(f_{t, \mathcal{B}}^i), y_0^i) \label{Seg_Loss} \nonumber
\end{align}

\subsection{Comparisons with MAE and DDPM}
Our MDM differentiates itself from MAE and DDPM in three key dimensions: (1) Purpose: While DDPM serves as a potent generative model and self-supervised pre-training algorithm, MDM and MAE concentrate on self-supervised pre-training. The generative ability of MDM can be further explored. (2) Architecture: MAE employs the Vision Transformer (ViT) architecture, whereas DDPM and our MDM utilize the U-Net architecture. (3) Corruption: DDPM introduces increasing additive Gaussian noise, MAE applies patch masking and only uses visible patches as input, and MDM employs a masking strategy guided by a sampled timestep $t$ from $[1, T]$, using the whole corrupted image as input.

\section{Experiments}
We assess the effectiveness of our approach on various datasets, spanning both medical images—the Gland segmentation dataset (GlaS) \citep{DBLP:journals/mia/Sirinukunwattana17} and MoNuSeg \citep{DBLP:journals/tmi/KumarVSBVS17,DBLP:journals/tmi/KumarVAZOTCHLHW20}—and natural images, including FFHQ-34 and CelebA-19 \citep{DBLP:conf/iclr/BaranchukVRKB22}, in the context of few-shot segmentation following the pre-training phase.
FFHQ-34 is a subset of FFHQ-256 dataset \citep{DBLP:conf/cvpr/KarrasLA19} with segmentation annotations and we use FFHQ-256 to pre-train for FFHQ-34 and CelebA-19 segmentation. We adopt in-domain pre-training for all pre-training methods on GlaS and MoNuSeg. A comprehensive overview of each dataset is provided in Appendix \ref{dataset}.

\subsection{Comparisons with State-of-the-art Methods}
\textbf{Medical Image Datasets.} We compare our MDM with the two types of current state-of-the-art methods on GlaS and MoNuSeg, covering six traditional segmentation methods: UNet \citep{DBLP:conf/miccai/RonnebergerFB15}, UNet++ \citep{DBLP:conf/miccai/ZhouSTL18}, Swin UNETR \citep{DBLP:conf/brainles-ws/HatamizadehNTYR21}, AttUNet \citep{10.1007/978-3-031-09002-8_27}, UCTransNet \citep{DBLP:conf/aaai/Wang0WZ22}, MedT \citep{DBLP:conf/miccai/ValanarasuOHP21} and two self-supervised pre-training methods allowing extracting intermediate activations like our method: MAE and DDPM. For a fair comparison, all the methods not elaborated in the Implementation Details section adhere to their respective official configurations.


In Table~\ref{Glas_and_Monuseg}, we report a comprehensive comparison of methods in terms of Dice, IoU, and AJI metrics on GlaS and MoNuSeg across two distinct scenarios. The qualitative examples of segmentation with our method and other comparable models are shown in Figure~\ref{Glas_seg} and Figure~\ref{Monuseg_seg}. 
Notably, in  Figure~\ref{Monuseg_seg}, we have marked regions of superior performance by our MDM with red boxes. The segmentation network, trained utilizing our proposed MDM representations, yields significant performance improvements over previous techniques for both GlaS and MoNuSeg datasets.  Furthermore, all the traditional segmentation methods fail to provide reasonable segmentation predictions when operating with only 10\% of the available labels for training. In contrast, our MDM not only surpasses the accuracy achieved by state-of-the-art self-supervised methods such as MAE and DDPM, but also attains accuracy levels close to those achieved with full label utilization, while using only 10\% of the labels.

\begin{table}[htp]
    \centering
    \caption{\textbf{Comparisons with previous methods on GlaS and MoNuSeg.} The results are presented in the format of ``mean$\pm$std''. Both DDPM and MDM are trained for 10000 iterations on GlaS and 5000 iterations on MoNuSeg.}
    \small \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccccc}
        \toprule
         \multirow{2}{*}{Method} & \multicolumn{2}{c}{GlaS 100\% (85)} &\multicolumn{2}{c}{GlaS 10\% (8)} & \multicolumn{2}{c}{MoNuSeg 100\% (30)} &\multicolumn{2}{c}{ MoNuSeg 10\% (3)}\\
         \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
        & Dice (\%) & IoU (\%) &Dice (\%) &IoU (\%) & Dice (\%) & AJI (\%) &Dice (\%) &AJI (\%) \\
        \midrule
        UNet       &85.93$\pm$2.92 &75.44$\pm$4.42 &53.46$\pm$11.09 &37.24$\pm$10.06                      &74.56$\pm$0.98 &60.22$\pm$1.31 &54.79$\pm$2.50 &41.06$\pm$2.14\\
        UNet++     &86.62$\pm$0.99 &76.41$\pm$1.56 &77.05$\pm$2.11  &62.72$\pm$2.7                        &80.33$\pm$0.69 &67.30$\pm$0.94 &74.26$\pm$1.62 &59.49$\pm$1.98\\
        Swin UNETR  &86.74$\pm$1.32 &76.61$\pm$2.05 &73.95$\pm$2.00  &58.71$\pm$2.54                       &79.99$\pm$0.42 &66.81$\pm$0.57 &72.47$\pm$2.72 &57.36$\pm$3.27\\
        AttUNet    &86.12$\pm$1.99 &75.68$\pm$3.04 &66.35$\pm$4.79  &49.83$\pm$5.16                       &79.74$\pm$1.03 &66.55$\pm$1.36 &70.87$\pm$3.46 &55.72$\pm$3.84\\
        UCTransNet &85.10$\pm$2.44 &74.14$\pm$3.65 &55.87$\pm$5.61  &38.98$\pm$5.44                       &78.80$\pm$1.15 &65.57$\pm$1.34 &64.33$\pm$5.19 &48.78$\pm$5.35\\
        MedT       &81.02$\pm$2.10 &68.14$\pm$2.98 &59.46$\pm$11.31 &43.18$\pm$10.86                      &77.55$\pm$1.02 &63.48$\pm$1.35 &64.49$\pm$2.87 &50.60$\pm$3.09\\
        \midrule
        MAE        &89.71$\pm$0.92 &81.35$\pm$1.51 &88.56$\pm$0.67  &76.19$\pm$0.18                       &73.68$\pm$1.48 &58.62$\pm$1.78 &76.19$\pm$0.18 &61.61$\pm$0.23\\
        DDPM       &90.45$\pm$0.37 &82.56$\pm$0.61 &90.30$\pm$0.47  &82.32$\pm$0.77                       &80.31$\pm$0.58 &67.31$\pm$0.77 &74.37$\pm$3.08 &60.03$\pm$3.48\\
\textbf{MDM(ours)} &\textbf{91.95$\pm$1.25}  &\textbf{85.13$\pm$2.09} 
                   &\textbf{91.60$\pm$0.69}  &\textbf{84.51$\pm$1.15} 
                   &\textbf{81.01$\pm$0.35}  &\textbf{68.25$\pm$0.49} 
                   &\textbf{79.71$\pm$0.75}  &\textbf{66.43$\pm$1.02}\\
        \bottomrule
    \end{tabular}
    }
    \label{Glas_and_Monuseg}
\end{table}


\begin{figure*}[!ht]
\centering
\includegraphics[width=0.85\columnwidth]{Glas.pdf} \caption{\textbf{Qualitative Visualization} on GlaS test sets under full training labels setting (fisrt 2 rows) and 10\% training labels setting (last 2 rows).}
\label{Glas_seg}
\end{figure*}



\begin{figure*}[!ht]
\centering
\includegraphics[width=0.85\columnwidth]{Monuseg.pdf} \caption{\textbf{Qualitative Visualization} on MoNuSeg test sets under full training labels setting (fisrt 2 rows) and 10\% training labels setting (last 2 rows). \textit{Please zoom in for a better view}.}
\label{Monuseg_seg}
\end{figure*}


\textbf{Natural Image Datasets.} We compare MDM with two categories of state-of-the-art methods on FFHQ-34 and CelebA-19, including two methods that employ extensive annotated synthetic image sets for segmentation training: DatasetGAN \citep{DBLP:conf/cvpr/ZhangLGYLB0F21}, DatasetDDPM \citep{DBLP:conf/iclr/BaranchukVRKB22} and four self-supervised methods similar to our approach: 
SwAV, SwAVw2 \citep{DBLP:conf/nips/CaronMMGBJ20}, MAE and DDPM.

Table \ref{FFHQ_and_CelebA} presents the experimental results in terms of mIoU on FFHQ-34 and CelebA-19. Our proposed MDM outperforms the previous state-of-the-art results. Additionally, we visualize the segmentation results in Figure \ref{FFHQ_seg}. 
In particular, MDM is the only method which correctly classifies the hat in the third row image, which proves the representations extracted from MDM are semantically rich.
\begin{table}[!htp]
    \centering
    \caption{\textbf{Semantic segmentation performance on FFHQ-34 and CelebA-19 in terms of mIoU.} Symbol $^{*}$ denotes the evaluation of CelebA-19 utilizing models pre-trained on FFHQ-256. MDM is pre-trained for 40000 iterations for FFHQ-34 and CelebA-19, while other pre-training methods use the checkpoints provided by \citet{DBLP:conf/iclr/BaranchukVRKB22}.}
    \small \begin{tabular}{lcc}
        \toprule
         Method &FFHQ-34 &CelebA-19$^*$\\
        \midrule
        DatasetGAN &55.52$\pm$0.24 &\text{-}\\
        DatasetDDPM &55.01$\pm$0.27 &\text{-}\\
        \midrule
        SwAV  &52.53$\pm$0.13 &51.99$\pm$0.20\\
        SwAVw2 &54.57$\pm$0.16 &51.17$\pm$0.14\\
        \midrule
        MAE        &57.06$\pm$0.20 &57.27$\pm$0.08\\
        DDPM       &59.36$\pm$0.17 &58.86$\pm$0.12 \\
\textbf{MDM(ours)} &\textbf{60.34$\pm$0.15}  &\textbf{59.57$\pm$0.13} \\
        \bottomrule
    \end{tabular}
    \label{FFHQ_and_CelebA}
\end{table}


\begin{figure}[!htp]
\centering
\includegraphics[width=0.85\columnwidth]{FFHQ.pdf} \caption{\textbf{Qualitative Visualization} on FFHQ-34 (fisrt 2 rows) and CelebA-19 (last 2 rows).}
\label{FFHQ_seg}
\end{figure}


\textbf{Key Observations.} We summarize the key observations as follows:

$\bullet$ The proposed MDM outperforms DDPM across all four datasets under different settings, which indicates that the denoising in diffusion is not irreplaceable, at least from a self-supervised representation learning perspective.

$\bullet$ MDM achieves better performance than MAE, even though both methods employ similar masking and restoration pre-training techniques. We attribute this improvement to MDM's utilization of different levels of masking ($T$ levels) which contributes to the acquisition of more robust and semantically meaningful representations, enhancing segmentation results.

$\bullet$ MDM excels in few-shot scenarios, while conventional segmentation methods struggle. This positions MDM as a competitive choice for tasks with limited labeled data. Notably, this advantage is particularly relevant in medical datasets, where acquiring pixel-level labels is costly, making MDM's label-efficient characteristic beneficial for such data.


\subsection{Ablation Studies}
Our ablation studies are based on 20000 iterations pre-training of DDPM and MDM for FFHQ-34 and 10000 iterations pre-training for GlaS. If not specified, other settings follow the implementation details in Appendix \ref{implementation}. Here, we provide our ablation studies focusing on diffusion, loss functions, and reconstruction targets. A comprehensive collection of additional ablation studies can be found in Appendix~\ref{ablation}.

\textbf{Diffusion.} Table \ref{Diffusion} studies the impact of diffusion on our MDM and DDPM pre-training. Diffusion models consist of $T$ timesteps, each corresponding to an incremental level of corruption. We fix $t$ to 250 for DDPM pre-training and 50 for MDM pre-training, then DDPM degrades to a vanilla denoising autoencoder and MDM degrades to a vanilla masked autoencoder (with a slight difference from MAE). The degraded DDPM and MDM show similar performance as MAE, while with diffusion, both DDPM and MDM outperform MAE. Surprisingly, MDM demonstrates a more substantial improvement than DDPM with diffusion and achieves the highest accuracy, even though MDM is not a standard diffusion model with powerful generation capability. This phenomenon further supports our conjecture: 
the efficacy of semantic representations derived from DDPM is not solely contingent on its generative prowess. Instead, we can view DDPM as a denoising autoencoder with $T$ levels of noises, and it can potentially be substituted with other forms of random corruptions.

\begin{table*}[!h]
    \centering
    \caption{\textbf{Ablation results of diffusion.} Fixed $t$ means a diffusion model degrades to  a vanilla autoencoder with a fixed level of corruption.}
    \small \begin{tabular}{lcccccc}
        \toprule
         \multirow{2}{*}{Method} & \multirow{2}{*}{Fixed $t$} &\multicolumn{2}{c}{GlaS 100\% (85)} &\multicolumn{2}{c}{GlaS 10\% (8)}\\
         \cmidrule(lr){3-4} \cmidrule(lr){5-6}
        & & Dice (\%) & IoU (\%) &Dice (\%) &IoU (\%)\\
        \midrule
        MAE &\text{-} &89.71$\pm$0.92 &81.35$\pm$1.51 &88.56$\pm$0.67 &76.19$\pm$0.18\\
        \midrule
        \multirow{2}{*}{DDPM} & \checkmark        &89.82$\pm$0.41 &81.52$\pm$0.76 &87.10$\pm$0.67  &77.15$\pm$1.06 \\
         & \textbf{$\times$} &\textbf{90.45$\pm$0.37}  &\textbf{82.56$\pm$0.61} 
                   &\textbf{90.30$\pm$0.47}  &\textbf{82.32$\pm$0.77} \\
        \midrule
        \multirow{2}{*}{MDM} & \checkmark        &88.68$\pm$0.54 &79.67$\pm$0.86 &86.82$\pm$1.04  &76.72$\pm$1.59 \\
         &$\times$        &\textbf{91.95$\pm$1.25} &\textbf{85.13$\pm$2.09} &\textbf{91.60$\pm$0.69}  &\textbf{84.51$\pm$1.15} \\
        \bottomrule
    \end{tabular}
    \label{Diffusion}
\end{table*}
\textbf{SSIM Loss and Reconstruction Targets.} The effectiveness of SSIM loss and reconstruction targets is studied in Table \ref{SSIM}. 
When applied to MDM and DDPM$^\dag$ pre-training, SSIM loss improves the accuracy on both GlaS and FFHQ-34 datasets compared to MSE loss. These findings highlight that choosing a suitable loss function is a straightforward yet effective way for minimizing the discrepancy between pre-training-acquired representations and those essential for subsequent segmentation tasks. 
Additionally, we investigate different reconstruction targets for DDPM pre-training. DDPM pre-trained with the noise prediction strategy achieves higher accuracy in downstream segmentation tasks compared to using the image prediction strategy. Our findings are consistent with previous research conclusions \citep{DBLP:conf/nips/HoJA20}, indicating that predicting the noise $\epsilon_t$ can lead to improved quality in diffusion models, as opposed to predicting the clean image $x_0$. As a result, all DDPM models in this study are trained by predicting noise.

\begin{table}[!htp]
    \centering
    \caption{\textbf{Ablation results of loss functions and reconstruction targets in terms of mIoU.} $^\dag$: DDPM predicts the original image $x_0$ instead of the noise $\epsilon_t$.}
    \small \begin{tabular}{lccc}
        \toprule
         Method &Loss Type &GlaS 10\% (8) &FFHQ-34\\
        \midrule
        DDPM       &MSE &82.32$\pm$0.77 &58.75$\pm$0.16\\
        \midrule
        \multirow{2}{*}{DDPM$^\dag$}       &MSE &78.77$\pm$1.01 &51.63$\pm$0.16\\
        &\textbf{SSIM} &\textbf{81.79$\pm$1.13} &\textbf{56.97$\pm$0.18}\\
        \midrule
        \multirow{2}{*}{MDM}       &MSE &82.70$\pm$0.79 &55.06$\pm$0.21\\
        &\textbf{SSIM} &\textbf{84.51$\pm$1.15} &\textbf{59.18$\pm$0.11}\\
        \bottomrule
    \end{tabular}
    \label{SSIM}
\end{table}



\section{Conclusion}
In this study, we revisit the denoising diffusion models (DDPM) within the framework of self-supervised pre-training and decompose the relation of semantic representation quality of DDPM and its generation capability. We introduce a novel pre-training strategy by stripping noise from diffusion models and integrating masking operations akin to MAE. Furthermore, we demonstrates the effectiveness of SSIM loss of narrowing the gap between reconstruction pre-training task and downstream dense prediction task. Our proposed masked diffusion model (MDM) with SSIM loss achieves state-of-the-art performance in semantic segmentation on multiple benchmark datasets. MDM's label-efficient attribute holds promising prospects for diverse few-shot dense prediction tasks. Other alternative random corruption strategies and optimization objectives for representation learning diffusion models tailored to specific datasets can be explored in the future.

\subsubsection*{Acknowledgments}
J. Chen is funded by the Federal Ministry of Education and Research (Bundesministerium für
Bildung und Forschung, BMBF) in Germany under the funding reference 161L0272 and supported by
the Ministry of Culture and Science of the State of North Rhine-Westphalia (Ministerium für Kultur
und Wissenschaft des Landes Nordrhein-Westfalen, MKW NRW).

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\appendix
\section{Experimental Details} 

\subsection{Datasets} 
\label{dataset}
\begin{table}[!htbp]
    \centering
    \caption{\textbf{A comprehensive overview of each dataset.} ``Training" and ``Test" columns denote available image-label pairs for the downstream segmentation task, while ``Classes" specifies the count of semantic classes. The ``Pre-training" column indicates the number of images utilized during the self-supervised pre-training phase. Notably, for the CelebA-19 segmentation, the pre-trained models for FFHQ-34 are directly used. This specific configuration is intentionally chosen to rigorously assess the generalization capabilities of models.}
    \small \begin{tabular}{lcccc}
        \toprule
         Dataset &Training &Test &Classes &Pre-training\\
        \midrule
        GlaS &85 &80 &2 &165\\
        MoNuSeg &30 &14 &2 &44\\
        FFHQ-34  &20 &20 &34 &70000\\
        CelebA-19 &20 &500 &19 &\text{-}\\
        \bottomrule
    \end{tabular}
    \label{Dataset}
\end{table}


\subsection{Implementation Details}
\label{implementation}
We leverage the MONAI library\footnote{https://github.com/Project-MONAI/MONAI} \citep{cardoso2022monai} implementations for UNet, UNet++, Swin UNETR, and AttUNet, while the remaining baseline models are evaluated using their official implementations\footnote{https://github.com/McGregorWwww/UCTransNet}\footnote{https://github.com/jeya-maria-jose/Medical-Transformer}. The training was performed on one Tesla V100 GPU with 16 GB memory. We train MAE, DDPM and MDM ourselves on GlaS and Monuseg. For FFHQ-256, if not specified, we only train MDM ourselves and use the pre-trained MAE, DDPM, SwAV and SwAVw2 in \citet{DBLP:conf/iclr/BaranchukVRKB22}\footnote{https://github.com/yandex-research/ddpm-segmentation}. 

In pre-training, we use the 165 training and test images for GlaS and the 44 training and test images for MoNuSeg. We use 70000 unlabelled $256 \times 256$ images from FFHQ-256 dataset for FFHQ-34 and CelebA-19. We randomly crop $256 \times 256$ patches as input for GlaS and MoNuSeg to make sure all the models are trained with $256 \times 256$ images. We only take random flip in pre-training. The patch size in MDM and MAE is set to 8. The batch size is 128 and the maximum diffusion step $T$ is 1000 for DDPM and MDM. All the other settings for DDPM and MDM follow the official implementation of guided diffusion\footnote{https://github.com/openai/guided-diffusion} \citep{DBLP:conf/nips/DhariwalN21}, with MAE following the official MAE implementation\footnote{https://github.com/facebookresearch/mae} \citep{DBLP:conf/cvpr/HeCXLDG22}. 

Then, we evaluate the pre-trained models across the four datasets for few-shot segmentation. In particular, for CelebA-19, we directly use the MDM (along with other pre-trained models) trained with FFHQ-256 to evaluate our method’s generalization ability.
For DDPM and MDM evaluations, we use the pixel-level representations from the middle blocks $\mathcal{B}=\{8, 9, 10, 11, 12\}$ and $\mathcal{B}=\{5, 6, 7, 8, 9, 10, 11, 12\}$ of the 18 decoder blocks for two medical image datasets and two natural image datasets respectively. We set the diffusion step to \(t=250\) for DDPM on medical datasets and \(t=50\) for MDM on all four datasets, while the settings outlined in \citet{DBLP:conf/iclr/BaranchukVRKB22} guided other methods and datasets. The selection strategy will be elaborated upon in Appendix \ref{ablation}. 

The segmentation network (MLP) undergoes training until the point of convergence, defined by the cessation of decreasing training loss over a specific number of steps, utilizing the Adam optimizer with a learning rate of 0.001. While dealing with medical datasets, a batch size of 65536 is employed,  whereas for FFHQ-34 and CelebA-19, a batch size of 64 is adopted in accordance with \citet{DBLP:conf/iclr/BaranchukVRKB22}. Each batch is constituted of the representation of one pixel. Notably, no data augmentation is applied during the training of the segmentation network. 

During inference, we use sliding window with size $256 \times 256$ to construct the segmentation predictions, ensuring alignment with the original image dimensions for both medical datasets. To make the results on the small datasets more convincing, we undertake 10 runs for all experiments, wherein the random seed is systematically varied from 0 to 9. We report the mean and std of the results. For GlaS we use dice coefficient (Dice) and Intersection over Union (IoU) as the evaluation metrics following UCTransNet \citep{DBLP:conf/aaai/Wang0WZ22} while for MoNuSeg we report the Dice and Aggregated Jaccard Index (AJI) following the official challenge\footnote{https://monuseg.grand-challenge.org} \citep{DBLP:journals/tmi/KumarVSBVS17,DBLP:journals/tmi/KumarVAZOTCHLHW20}. For FFHQ-34 and CelebA-19, we quantify performance using the mean Intersection over Union (mIoU) following \citet{DBLP:conf/iclr/BaranchukVRKB22}.

\section{Additional Ablation Studies} \label{ablation}
\textbf{Diffusion Timesteps.} We investigate the segmentation performance of representations extracted from MDM and DDPM across different diffusion timesteps $t$, as presented in Table \ref{Timestep}. On average, the best results of DDPM are observed at $t=250$ for the GlaS dataset, while MDM shows its highest performance with $t$ set to 50. The concatenation of three timesteps (50, 150, 250) does not always yield substantial enhancements but necessitates a 3$\times$ increase in computational resources.

In light of these findings, we select $t=250$ for DDPM on GlaS and MoNuSeg. Furthermore, we adhere to the official configuration in \citet{DBLP:conf/iclr/BaranchukVRKB22} wherein a concatenation of three timesteps (50, 150, 250) is employed for the FFHQ-34 and CelebA-19 datasets.
For MDM, we set $t$ to 50 across all four datasets. We intentionally did not perform tuning for $t$ on every dataset.

\begin{table}[!htp]
    \centering
    \caption{\textbf{Ablation results of different timesteps in terms of Dice and IoU.} 50$+$150$+$250 represents concatenating representations extracted at $t=50, 150, 250$.}
    \small \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccc}
        \toprule
         \multirow{2}{*}{Method} &\multirow{2}{*}{Timestep} &\multicolumn{2}{c}{GlaS 100\% (85)} &\multicolumn{2}{c}{GlaS 10\% (8)} &\multicolumn{2}{c}{Average}\\
         \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
         && Dice (\%) & IoU (\%) & Dice (\%) & IoU (\%) & Dice (\%) & IoU (\%)\\
        \midrule
        \multirow{6}{*}{DDPM}       &0  &88.96$\pm$0.54 &80.13$\pm$0.87 &87.98$\pm$0.51    &78.54$\pm$0.81 &88.47$\pm$0.72 &79.34$\pm$1.16\\
       &50 &90.40$\pm$0.24 &82.47$\pm$0.39 &88.54$\pm$0.77 &79.43$\pm$1.23 &89.47$\pm$1.10 &80.95$\pm$1.79 \\
       &150 &90.54$\pm$0.40 &82.71$\pm$0.67 &89.52$\pm$0.94 &81.35$\pm$1.52 &90.03$\pm$0.88 &0.87$\pm$1.34\\
       &\textbf{250} &90.45$\pm$0.37 &82.56$\pm$0.61 &\textbf{90.30$\pm$0.47} &\textbf{82.32$\pm$0.77} &\textbf{90.38$\pm$0.42} &\textbf{82.44$\pm$0.69} \\
       &650 &90.24$\pm$0.41 &82.24$\pm$0.72 &89.98$\pm$0.54 &81.85$\pm$1.15 &90.11$\pm$0.49 &82.05$\pm$0.96\\
       &50$+$150$+$250 &\textbf{90.76$\pm$0.44} &\textbf{83.10$\pm$0.75} &89.06$\pm$0.53 &80.21$\pm$1.18 &89.91$\pm$0.99 &81.66$\pm$1.77 \\
        \midrule
        \multirow{6}{*}{MDM}       &0 &91.93$\pm$1.50 &85.11$\pm$2.51 &91.32$\pm$0.84 &84.05$\pm$1.42 &91.63$\pm$1.22 &84.58$\pm$2.06 \\
        &\textbf{50} &91.95$\pm$1.25 &85.13$\pm$2.09 &\textbf{91.60$\pm$0.69} &\textbf{84.51$\pm$1.15} &\textbf{91.78$\pm$1.00} &\textbf{84.82$\pm$1.67} \\
        &150 &91.98$\pm$1.21 &85.17$\pm$2.05 &91.49$\pm$0.73 &84.32$\pm$1.23 &91.74$\pm$1.00 &84.75$\pm$1.70 \\
        &250 &91.55$\pm$1.63 &84.46$\pm$2.70 &90.97$\pm$0.60 &83.43$\pm$1.01 &91.26$\pm$1.23 &83.95$\pm$2.05 \\
        &650 &91.87$\pm$0.93 &84.98$\pm$1.57 &90.45$\pm$1.13 &82.58$\pm$1.88 &91.16$\pm$1.24 &83.78$\pm$2.09 \\
        &50$+$150$+$250 &\textbf{92.02$\pm$0.56} &\textbf{85.23$\pm$0.96} &91.23$\pm$0.87 &83.88$\pm$1.46 &91.63$\pm$0.82 &84.56$\pm$1.39 \\
        \bottomrule
    \end{tabular}
    }
    \label{Timestep}
\end{table}


\textbf{Blocks.} We display the results of k-means clustering ($k=5$) for the frozen features of both DDPM and MDM in Figure \ref{feature}.  
The features from the deeper layers (blocks with smaller values) exhibit coarse semantic masks, while those from shallower layers (blocks with larger values) reveal finer details yet lack the same level of semantic coherence for coarse segmentation. Therefore, we choose the middle blocks $\mathcal{B}=\{8, 9, 10, 11, 12\}$ and $\mathcal{B}=\{5, 6, 7, 8, 9, 10, 11, 12\}$ among the 18 decoder blocks for two medical image datasets and two natural image datasets respectively. This block configuration, as adopted in this study, has led to a slight enhancement in segmentation accuracy when using DDPM on FFHQ-34, in contrast to the official settings mentioned in \citet{DBLP:conf/iclr/BaranchukVRKB22}. Importantly, we deliberately avoided tuning the blocks for each dataset individually. 

A particularly intriguing observation is that due to the distinct reconstruction objectives of DDPM and MDM (noise for DDPM and image for MDM), the shallowest blocks (e.g., Block 16 and Block 17) of MDM yield meaningful semantic representations, whereas the same blocks for DDPM produce noise. For a fair comparison with DDPM, we did not use these shallow blocks for MDM. However, in practical scenarios, the judicious selection of blocks holds potential to further enhance the overall performance of our model.


\begin{figure}[htp]
\centering
\includegraphics[width=1\columnwidth]{feature.pdf} \caption{\textbf{K-means clustering of features} extracted from the UNet decoders of DDPM (first 5 rows) and MDM (last 5 rows) on FFHQ-34 at $t=50$. }
\label{feature}
\end{figure}




\textbf{Patch Size.} We compare different patch sizes for MDM in Table \ref{Patch_size}. MDM achieves the best segmentation results when patch size is 8 (our final value). Note that even the performance of MDM experiences a marginal decrease for patch sizes of 4 or 16, it still surpasses the performance of existing methods.

\begin{table*}[htp]
    \centering
    \caption{\textbf{Ablation results showcasing the impact of different patch sizes on MDM.}}
    \small \begin{tabular}{lccccc}
        \toprule
         \multirow{2}{*}{Method} &\multirow{2}{*}{Patch Size} & \multicolumn{2}{c}{GlaS 100\% (85)} &\multicolumn{2}{c}{GlaS 10\% (8)}\\
         \cmidrule(lr){3-4} \cmidrule(lr){5-6}
        && Dice (\%) & IoU (\%) &Dice (\%) &IoU (\%)\\
        \midrule
        \multirow{3}{*}{MDM} &4        &90.61$\pm$0.42 &82.83$\pm$0.71 &90.41$\pm$0.76  &82.51$\pm$1.28 \\
        &\textbf{8} &\textbf{91.95$\pm$1.25}  &\textbf{85.13$\pm$2.09} 
                   &\textbf{91.60$\pm$0.69}  &\textbf{84.51$\pm$1.15} \\
        &16       &90.97$\pm$0.60 &83.44$\pm$1.01 &90.60$\pm$0.41  &82.82$\pm$0.69 \\
        \bottomrule
    \end{tabular}
    \label{Patch_size}
\end{table*}


\textbf{Training Schedules.} 
The impact of the training schedule length on downstream segmentation for MDM is illustrated in Figure \ref{Training_schedule}. The accuracy exhibits a steady increase as the training duration extends, which suggests the scalability of MDM when applied to substantial unlabeled datasets.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.8\columnwidth]{iterations.pdf} \caption{\textbf{MDM training schedules.} FFHQ-256 is used for MDM pre-training and FFHQ-34 is used for downstream segmentation.}
\label{Training_schedule}
\end{figure}





\section{Additional Qualitative Results}
\begin{figure}[htbp]
\centering
\includegraphics[width=1\columnwidth]{reconstruct.pdf} \caption{\textbf{ Visualizations of MDM reconstruction} on FFHQ-34 test set (first 5 rows) and CelebA-19 test set (last 5 rows), using MDM trained on FFHQ-256 for 40000 iterations. The results at $t$ = 50, 250, 450, and 750 are presented. A greater value of $t$ corresponds to a higher masking ratio. For each pair, we show the masked image (left) and our MDM reconstruction (right).}
\label{reconstruction}
\end{figure}


\end{document}
