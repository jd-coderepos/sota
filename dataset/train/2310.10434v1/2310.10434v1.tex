
\documentclass{article} \usepackage{iclr2024_conference,times}

\usepackage{subcaption}
\usepackage{textcomp, gensymb}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \def\N{\mathbb{N}}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\Z{\mathbb{Z}}
\usepackage{multirow}
\usepackage{booktabs} \usepackage{url}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{enumitem}
\def\msets{{\rm msets}}

\usepackage[normalem]{ulem}

\usepackage{hyperref}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand*\ilyes[1]{\texttt{{\color{purple}Ilyes: #1}}}
\newcommand*\felix[1]{\texttt{{\color{purple}Felix: #1}}}
\newcommand*\lars[1]{\texttt{{\color{purple}Lars: #1}}}
\newcommand*\co[1]{{\color{purple} #1}}
\newcommand*\cco[1]{\texttt{ \small {\color{purple} CO: #1}}}
\newcommand*\gabor[1]{\texttt{{\color{purple}Gabor: #1}}}

\title{\Large Equivariant Matrix Function Neural Networks}



\author{Ilyes Batatia \ \
Lars L. Schaaf \ \
Huajie Chen \ \
Gábor Csányi \ \
Christoph Ortner \ \
Felix A. Faber  \\
\small{ University of Cambridge, UK \ \ 
 Beijing Normal University, China  \ \
 University of British Columbia, Canada }
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}

Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials. 
Although Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size.
The MFN architecture achieves state-of-the-art performance in standard graph benchmarks, such as the ZINC and TU datasets, and is able to capture intricate non-local interactions in quantum systems, paving the way to new state-of-the-art force fields. The code and the datasets will be made public.

\end{abstract}

\section{Introduction}

Graph Neural Networks (GNNs) have proven to be powerful architectures for learning on graphs on a wide range of applications. Various GNN architectures have been proposed, including message passing neural networks (MPNN)~\citep{gilmer2017neural, battaglia2018relational, kipf2016semi, velivckovic2017graph, wu2020comprehensive, deepmind2018protein, hu2019deep, nequip} and higher-order equivariant MPNNs~\citep{Batatia2022mace}. 

MPNNs struggle to model {\bf non-local} interactions effectively due to computational constraints and over-smoothing~\citep{di2023over}. 
Spectral Graph Neural Networks attempt to address the limitation of this kind by encoding the global structure of a graph using eigenvectors and eigenvalues of a suitable operator. These approaches predominantly focus on Laplacian matrices, exploiting the graph's inherent spectral features. Many spectral GNNs apply polynomial or rational filters~\citep{bianchi2021graph, gasteiger2018predict, wang2022powerful, he2021bernnet, defferrard2016convolutional, zhu2021interpreting, kreuzer2021rethinking} to eigenvalues of graph structures, reaching state-of-the-art accuracy on pure graphs tasks. However, these methods often exhibit rigid architectures that require extensive feature engineering, 
potentially limiting their adaptability to various types of graphs. Moreover, they have been restricted to non-geometric graphs, making them unsuitable for molecules and materials.

Traditional neural network architectures, such as recurrent neural networks~\citep{elman1990finding,hochreiter1997long,cho2014properties,graves2013generating} and transformers~\citep{vaswani2017attention} also face challenges when modeling non-local interactions. While transformers can capture non-local dependencies through their self-attention mechanisms, they come at a significant computational cost due to their quadratic complexity with respect to the input sequence length. Furthermore, transformers lack inherent structural relationships or positional information within input data, necessitating the use of additional techniques, such as positional encodings~\citep{shaw2018selfattention}.  

In chemistry and material science tasks, some models incorporate {\bf long-range} interactions through electrostatics~ \citep{Grisafi2019LODE1, behler2021four_gen_rev, Unke2019PhysNet:Charges}, dispersion, or reciprocal space~\citep{gao2022self_consistent_nn, huguenindumittan2023physicsinspired, kosmala2023ewaldbased}. However, no existing architecture effectively address \textbf{non-local} interactions, where effects can propagate over extensive distances through electronic delocalization, spin coupling, or other many-body non-local mechanisms. This is particularly problematic in systems such as large conjugated molecules, amorphous materials, or metals. 

Consequently, there is a need for new neural network architectures that can efficiently and accurately model complex non-local many-body interactions, while addressing the limitations of current approaches. We propose {\bf Matrix Function Networks} (MFN) as a possible solution to this challenge. Concretely, we make the following contributions.
\vspace{-5pt}
\begin{itemize}
    \item We introduce Matrix Function Networks (MFNs), a new graph neural network architecture able to model non-local interactions in a structured, systematic way and with potentially linear scaling with the size of the input.
    \vspace{-1pt}
    \item We introduce the resolvent expansion as a convenient and efficient mechanism to learn a general matrix function.
    \vspace{-1pt}
    \item We demonstrate the ability of our architecture to learn non-local interactions on a dataset of challenging non-local quantum systems. 
    \item We show that MFNs achieve state-of-the-art performance on ZINC and TU graph datasets.
\end{itemize}

\section{Related Work}

\paragraph{Overlap matrix fingerprints}~\citep{OMFPs2016} introduced overlap matrix fingerprints (OMFPs), a vector of spectral features of an atomic environment or more generally a point cloud. Given a point cloud, an overlap operator (identity projected on an atomic orbital basis) is constructed, and its ordered eigenvalues (or other invariants) are taken as the features of that point cloud. Although a theoretical understanding of OMFPs is still lacking, computational experiments have shown excellent properties as a distance measure~\citep{OMFPs2016, OMFPs2021}.

\paragraph{Spectral Graph Neural Networks}
Spectral GNNs~\citep{WUGNN2021} are GNNs that use spectral filters operating on the Fourier decomposition of the Laplacian operator of the graph. Spectral GNNs are categorized by the type of filters they apply to the spectrum of the Laplacian: ChebyNet~\citep{ChebyConv} approximates the polynomial function of the Laplacian using Chebychev expansions, GPRGNN~\citep{GPRGNN} directly fits coefficients of a fixed polynomial, while ARMA~\citep{ARMA} uses rational filters.

\paragraph{Equivariant Neural Networks} Equivariant neural networks are the general class of neural networks that respect certain group symmetries~\citep{Bronstein:2021mdi}. Noteably, convolutional neural networks (CNNs)~\citep{CNNlecun} are equivariant to translations, while -convolutions~\citep{CohenSteerable2016, s.2018spherical, pmlr-v80-kondor18a} generalized CNNs to equivariance of compact groups. Lately, equivariant message passing neural networks~\citep{Anderson2019CormorantCM, Welling2021EGNN, brandstetter2022geometric, nequip, Batatia2022mace, Batatia2022de} have emerged as a powerful architecture for learning on geometric point clouds. Most of these architectures have been shown to lie in a common design space~\cite{Batatia2022de, batatia2023general}.

\paragraph{Hamiltonian Learning} A natural application of equivariant neural network architectures is machine learning of (coarse-grained) Hamiltonian operators arising in electronic structure theory. This task of parameterizing the mapping from atomic structures to Hamiltonian operators is currently receiving increasing interest because of the potential extension in accessible observables over purely mechanistic models. The recent works of~\cite{Nigam2022-hamiltonians} and~\cite{2021-acetb1} introduce such parameterizations in terms of a modified equivariant Atomic Cluster Expansion (ACE)~\citep{Drautz2020-tensors}, a precursor to the architecture we employ in the present work. Alternative approaches include~\citep{Hegde2017-ay, Schutt2019-um, Unke-sg-2021, Gu2023-om}.



\section{Background}

\subsection{Spectral Graph Neural Networks}

We briefly review spectral graph neural networks and explain their limitations that our work will overcome.
Consider a graph  with node set  and edge set . A graph defined purely by its topology (connectivity) is called a pure graph {\bf}.
Let  denote the number of nodes in the graph, and let  be its adjacency matrix. Define a vector of ones as . The degree matrix of the graph is , and the Laplacian matrix is .
The Laplacian is a symmetric positive semidefinite matrix and admits a spectral decomposition, , where  is an orthogonal matrix of eigenvectors and  is a diagonal matrix of eigenvalues.

A popular approach to learning functions on graphs is to use convolutional neural networks on the graph.
Spectral graph convolutional networks (SGCNs)  are a class of graph convolutional networks that use the spectral decomposition of the Laplacian matrix to define convolutional filters.
Let  be a function of a graph  and  be a convolutional filter. 
SGCNs take advantage of the spectral decomposition of the Laplacian matrix of graph  to compute the convolution of  and : 

where  is a matrix function of the Laplacian matrix  of the graph  and  the Hadamard product. 
Various works have proposed different matrix functions , such as polynomial functions~\citep{ChebyConv}, rational functions~\citep{ARMA}, or neural networks~\citep{WUGNN2021}.
In a recent work, \cite{yang2023better} proposed to extend filter functions to matrix functions of other \textbf{fixed} matrix representations of the graph.
Let  denote the space of all graph operators on graph , i.e., the space of linear self-adjoint and permutation equivariant operators from nodes to nodes.
For any \textbf{fixed} , generalized spectral GNNs determine filters based on   with a matrix function:

where  is a matrix function of . 
Two non-isomorphic graphs can share the same spectrum of their Laplacian operators, while they have different spectra for other graph operators~\citep{JOHNSON198096}.
Therefore, the use of other graph operators as a basis for matrix functions can be beneficial for learning functions on graphs that are strictly more expressive than Laplacian SGCNs.
However, this approach has two main \textbf{limitations}.
\begin{itemize}
    \item \textbf{Expressiveness:} Performing only convolutions with a \textbf{fixed} graph operator  limits the expressivity of the model. Choosing the most expressive graph operator requires problem-dependent feature engineering.
    \item \textbf{Lie-group symmetries:} The approaches proposed so far have been restricted to \textbf{pure} graphs and in particular do not use additional symmetries for graphs embedded in a vector space. For example, graphs embedded in  often lead to -equivariant learning tasks.
\end{itemize}

To overcome these limitations, we propose MFNs in Section~\ref{sec:mfnns}, 
which allow parameterization of both the graph operator  and the matrix function , and can be formulated to preserve the equivariance under all known group actions. Since a learnable operator  prevents the precomputation of diagonalization of  during training, we also introduce a method that avoids diagonalization and allows (in principle) linear scaling with the number of nodes.  

\subsection{Equivariant Message Passing Neural Network}
\label{sec:mpnn}
Equivariant Message Passing Neural Networks (MPNNs)~\cite{nequip, Batatia2022mace} are graph neural networks that operate on graphs  embedded in a vector space . 
The nodes  are no longer only a list of indices, but belong to a configuration space  that extends the vector space .
For example, in the atomistic point cloud,  describing the positions and chemical species of each atom through which the graph is embedded into . 
The case of the  graph can be recovered by setting . 
We are interested in learning graph maps of the form

where  is an abstract target space, usually a vector space. As the input is a graph, we impose the mapping to be permutation invariant (invariant under relabeling of the nodes). In many applications, the target properties satisfy additional symmetries: When a group  acts on both  (and, therefore, on ) and , we say that  is -equivariant if,

where  is a representation of the group on the vector space . A typical strategy is then to embed the nodes  into a feature space, where a suitable representation of the group is available.

We represent the state of each node  in the layer  of the MPNN by a tuple,

where  defines the collection of node attributes of the graph as defined previously and  are its learnable features.
A forward pass of the network consists of multiple \textit{message construction}, \emph{update}, and \emph{readout} steps.
During message construction, a message  is created for each node by pooling over its neighbors,

where the individual operations have the following meaning: \-5mm]
    \item  is a learnable update function, transforming the message  into new features ; \-5mm]
    \item  is a global readout map, typically 
    .
\end{itemize}



Equivariant MPNNs are widely used for learning properties of 3D point clouds, such as molecules and materials. However, there are several limitations to their expressiveness,
\begin{itemize}
    \item \textbf{Non-local :} MPNNs are restricted to a finite range, dictated by the number of iterations .
    A large number of properties in physics are long-range, which requires . Adding a large number of layers leads to high computational overhead and poor expressivity due to oversmoothness \citep{di2023over}.
    \item \textbf{Correlation order:} Most MPNNs use two body messages, which means that the pooling in Equation~\ref{eqn:mpnn-equations} is just a sum. The MACE architecture~\citep{Batatia2022mace} extended the message construction to an arbitrary order, but increasing the order is still computationally demanding, especially for large .
\end{itemize}

\section{Matrix Function Neural Networks}
\vspace{-6pt}
\label{sec:mfnns}
\subsection{General Matrix Function Neural Networks}
\vspace{-6pt}
Our MFN models act in the space of group equivariant matrix operators of the graph, , with  a reductive Lie group. In the same setting as in section~\ref{sec:mpnn}, let  be an undirected graph with  nodes, and the states  are embedded in a configuration space . The architecture operates in three stages at each layer, the \textbf{matrix construction}, the \textbf{matrix function}, and the \textbf{update}.
\begin{figure}[tp]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/mfn-architecture-4.pdf}
    \label{fig:enter-label}
	\caption{\textbf{Matrix function network architecture.} Illustrating matrix construction and non-locality of matrix functions on a chain graph.}
    \label{fig:MFN-framework}
    \vspace{-16pt}
\end{figure}
\vspace{-5pt}
\paragraph{Matrix construction}

The space of matrix operators on graphs, , corresponds to the space of operators that are (1) \textbf{self-adjoint}, (2) \textbf{permutation equivariant}, and (3) \textbf{G-equivariant}. 
We consider operators expanded in a basis  with an available group action on each basis element. These operators correspond to square matrices  following basis truncation, where  is the basis size. Matrix blocks  have dimensions , and the basis is indexed by a tuple . An extra channel dimension, denoted , is introduced to learn multiple operators. Matrix entries in  can generally be -equivariant functions of the neighborhoods  and ,

The choice of basis  depends on the application (see the SI for detailed examples).
In the case of the rotation group, any such function can be approximated with arbitrary accuracy using an ACE or MACE layer~\citep{2021-acetb1, Batatia2022mace}.
This result has been generalized to any reductive Lie group, using the -MACE architecture~\citep{batatia2023general}. It is important to note that one could use any equivariant architecture to learn these matrix entries. However, the use of more expressive approximators will result in better coverage of the operator space  and therefore better general expressivity.
The full matrix inherits this equivariance of the basis, 

where  is an orthogonal matrix that denotes the group action of  on the basis element .


\paragraph{Matrix function}
The central operation in MFNs, which introduces long-range many-body effects, is the matrix function.
Any continuous function , with parameters , can be interpreted as acting on self-adjoint matrices  via their spectral decomposition. If  with  orthogonal and  diagonal. Alternatively, if the function is analytic, it can be extended to a matrix function by converting its power series into a formal power matrix series,

where  are the weights of the power series of . 
An essential observation is that \textbf{any} continuous matrix function  preserves equivariance, 

The matrix function can be related to a high-order many-body equivariant function via the Cayley--Hamilton theorem.
The eigendecomposition in Equation~\ref{eq:mfn-function} is responsible for the non-locality of our approach. 
In practice, computing matrix functions is expensive as it requires diagonalization, scaling as  with the number of nodes. 
Many approaches are available to approximate matrix functions, such as Chebyshev polynomials or rational approximation, which can leverage potentially cheaper evaluation schemes.
Furthermore, the matrix  is sparse in many applications, which can be further exploited to reduce computational cost. 
To further optimize this, we propose to employ a resolvent expansion to parameterize , detailed in Section~\ref{sec:resolvent}. Similar approaches have been successfully applied to large matrices in other fields such as electronic structure calculations~\citep{pexsi_CMS2009,pexsi_JCPM2013}.

\paragraph{Update}
Multiple updates can be defined from the matrix function. All of the following updates differ fundamentally from the standard spectral GNN update, which is a filtering operation. 

\begin{enumerate}[leftmargin=1cm]
    \item The \textbf{diagonal update} updates the state of each node with non-local features extracted from the diagonal blocks of the matrix function, 
    
This method is the most computationally efficient since selected inversion techniques~\citep{pexsi_CMS2009} can be employed to efficiently evaluate the diagonal blocks of a matrix function; cf.~Section~\ref{sec:resolvent}. 
    \item The \textbf{dense update} utilizes the entire matrix, including all \textbf{off-diagonal terms}, to update the next matrix function, 

This update can not be performed using selected inversion techniques, but it can offer additional expressiveness guarantees.
    \item The \textbf{sparse update:} uses only the parts of the matrix corresponding to the connected nodes to update the nodes and edges of the graph to obtain the matrix function in the next layer, 
    
\end{enumerate}
\paragraph{Readout}
The readout phase is the same as the usual MPNN readout in Equation~\ref{eqn:mpnn-equations}.

\subsection{Resolvent parameterization of matrix functions}
\label{sec:resolvent}
The evaluation of the matrix function in Equation~\ref{eq:mfn-function} is the practical bottleneck of our method.
The cost of the evaluation depends on the choice of parameterization of the univariate function . 
For a general analytic , resolvent calculus allows us to represent 

where  is a curve encircling the eigenvalues of  and excluding any poles of . Approximating the contour integration with a quadrature rule with nodes  and weights  yields 
, 
and merging  we arrive at the parameterization 

Pole expansions for evaluating matrix functions have a long and successful history, especially when the arguments are sparse matrices~\citep{higham2008functions}.
The novelty in \eqref{sec:poleexpansion} over standard usage of pole expansions in computing matrix functions~\citep{higham2008functions} is that both the {\em weights}  and the {\em poles}  are now learnable parameters.

The derivation shows that in the limit of infinitely many pole-weight pairs  any analytic matrix function can be represented. Since analytic functions are dense in the space of continuous functions, this means that all continuous matrix functions can be represented in that limit as well (at the same time letting the poles approach the spectrum).
In practice, the poles should be chosen with non-zero imaginary parts in order to avoid the spectrum of , which is real since  is assumed to be self-adjoint. Therefore, we choose adjoint pole weight pairs  and  to ensure that  is real when restricted to real arguments. 


\paragraph{Linear scaling cost}
The pole expansion framework is the first key ingredient in linear scaling electronic structure methods~\cite{RevModPhys.71.1085} such as PEXSI~\cite{pexsi_CMS2009,pexsi_JCPM2013}.
The second ingredient is the selected inversion of the resolvent. 
Instead of computing the full inverse, , one first computes a sparse  factorization and then selectively computes only the diagonal entries of the resolvent. The bottleneck in this approach is the  factorization. For a full matrix, it scales as  operations and  memory. 
The complexity improves considerably for sparse matrices. Suppose that the sparsity pattern is -dimensional, corresponding to a topologically -dimensional graph; e.g. the cumulenes in Section~\ref{sec:cumulenes} are topologically one-dimensional despite being embedded in . Using nested dissection ordering to minimize the fill-in, the cost of the  factorization reduces to  for  (e.g., natural language processing and quasi-1D molecular systems such as carbon-nano-tubes);  operations and  memory for  (e.g., image recognition); and  operations and  memory for . The final step to reduce the computational cost of our architecture to {\em linear scaling} is to replace the  factorization with an incomplete factorization, as proposed by~\citet{etter2020incomplete} in the context of electronic structure calculations. They demonstrated that the  factorization inherits the decay of the resolvents and, therefore, an incomplete factorization can be performed with controllable error. However, this scheme remains a theoretical idea, and incorporating it into our architecture results in a compromise between computational cost and non-locality that requires significant further investigation.

\subsection{Expressivity of Matrix Function Networks}


\subsubsection*{Non-local interaction}
\vspace{-5pt}
Purely formally, one may think of a matrix function  as an infinite power series.
This suggests that MFNs are inherently non-local and exhibit a convolution-like structure, similar to message-passing methods. 
This is of interest for modeling causal relationships or non-local interactions by proxy, such as in chemistry or natural language processing. In these cases, the propagation of local effects over long distances results in multiscale effects that are effectively captured by our method.

The degree of non-locality of the interaction can be precisely quantified. If  has a finite interaction range, then the Combe-Thomas theorem~\cite{Combes1973} implies that
, where  and  is the length of the shortest path from node  to node . Since we have taken  self-adjoint with real spectrum, an estimate for  is . As a result, if we constrain the poles in the parameterization of  to be at , then the resulting matrix function will satisfy 

Therefore, the degree of locality can be controlled by constraining  to be some fixed value. 
While \eqref{eq:asympt_decay} only gives an upper bound, it is not difficult to see that it can in fact be attained for a suitable choice of matrix function. 
In practice, however,  can be seen as approximating an unknown target  with learnable weights . If  is analytic in a neighborhood of , then  where  measures how smooth  is (distance of poles of  to )). For our parameterization  we therefore expect the same behavior  in the pre-asymptotic regime of moderate . 

\vspace{-6pt}
\subsubsection*{Geometric expressiveness}
\vspace{-5pt}
The WL-test quantifies a network's ability to distinguish non-isomorphic graphs. Its extension to graphs embedded in vector spaces is known as the geometric WL-test~\citep{joshi2023expressive}. The expressivity in the following discussion adheres to these definitions.
In the context of graphs in , practical relevance is high. A direct equivalence exists between equivariant MPNNs with linear updates and infinite layers and a one-layer MFN due to the matrix function's product structure.
For a one-layer MFN with a single matrix channel and two-body matrix entries, the expressiveness matches that of a two-body equivariant MPNN with \textbf{infinite layers} and \textbf{linear updates}. When matrix entries incorporate features beyond two-body, the one-layer MFN becomes more expressive than its two-body feature counterpart.

\vspace{-10pt}
\subsection{Matrix normalization}
\vspace{-5pt}
Batch normalization~\citep{ioffe2015batch} and layer normalization~\citep{ba2016layer} are two techniques widely used in deep learning to stabilize training, improve convergence, and provide some form of regularization.

In the context of MFNs, a similar normalization strategy is needed to ensure stable training and improved convergence. Instead of normalizing the features directly as conventional normalization layers do, we normalize the eigenvalues of a set of H matrices with batch dimension  batch, channels, , , where  is the size of the matrix. The aim is to adjust their mean and variance to 0 and 1, respectively, much like standardization in traditional batch or layer normalization. For the batch matrix norm, the mean is taken across the batch dimension, and for the layer matrix norm, the normalization is taken across the channel dimension.
The mean and the variance of the eigenvalues are computed using the following formulas,

This normalization of eigenvalues ensures that the spectral properties of the graph used for representation in the MFN are effectively standardized, contributing to better training stability and convergence.

\subsection{Multivariate Matrix function}
\label{sec:multivariate}
As MFNs extract features from multiple operators, it is useful to extend the univariate matrix function in Equation~\ref{eq:mfn-function} to a multivariate matrix function. The space of multivariate functions  is isomorphic to the closure of the span of the tensor product of functions of single operators ,
We call the number of variables , the correlation order of the matrix function. The resolvent expansion can be generalized to the multivariate matrix function, using matrix products of resolvents,

Multivariate matrix functions create higher order features. A one-layer MFN with a matrix function of correlation order  is as expressive as a \textbf{-body} equivariant MPNN with \textbf{infinite layers} and \textbf{linear updates}.
The space  is of very high dimension and it is possible to use standard compression techniques to find more efficient approximations such as tensor decomposition~\citep{TraceDarby2023}.
The use of linear combinations of matrices in Equation~\ref{eq:matrix-construction} approximates some of this space.
We leave further exploration of these kinds of network for future work.

\vspace{-8pt}
\subsection{Interpretability of the MFN operators}
\vspace{-6pt}
In the case of the Euclidean group, the matrices learned in MFNs have the same symmetries as Euclidean operators.
Euclidean operators play a crucial role in quantum mechanics. They are defined as self-adjoint operators (in the space of square-integrable functions) that are equivariant to the action of the group of rotations, translations, reflections, and permutations. When these operators are expended on the basis of the irreducible representations of the rotation group, they exhibit a block structure (see Appendix~\ref{sec:equivariant-op}) in which each entry is labeled with representations . 
\begin{figure}[h!]
    \centering
\includegraphics[width=0.6\textwidth]{figures/mfn-block.pdf}
    \caption{\textbf{Block structure} of a Euclidean Operator, H, learnt in the MFNs. Each entry in H corresponds to a different product of representations of the group of rotations  .}
    \label{fig:enter-label}
\end{figure}

The most central of these operators is the Hamiltonian operator. The energy of a quantum system is related to the trace of a specific matrix function of the Hamiltonian,

The Hamiltonian is usually computed as a fixed point of a self-consistent loop that minimizes the energy. This loop introduces many-body non-local effects.
This motivates us to use many-body functions to parameterize our matrices and to learn the fixed point directly via learning a matrix function of many matrices. This is in opposition to tight-binding methods that usually construct a single low-body Hamiltonian with physics-inspired functional forms but require several self-consistent iterations to converge. 


\vspace{-6pt}
\section{Results}
\label{sec:resuts}
\vspace{-6pt}
\subsection{Cumulenes: non-local 3D graphs}
\label{sec:cumulenes}
\vspace{-6pt}
We compare the non-locality of MFNs to local MPNNs and global attention MPNNs using linear carbon chains, called cumulenes. Cumulenes are made up of double-bonded carbon atoms terminated with two hydrogen atoms at each end. Cumulenes exhibit pronounced non-local behavior as a result of strong electron delocalization.  Small changes in chain length and relative angle between the terminating hydrogen atoms can result in large changes in the energy of the system, as visually represented in Figure~\ref{fig:cumulenes-combined}. These structures are known to illustrate the limited expressivity of local models~\citep{unke2021machine} and are similar to the k-chains introduced by~\cite{joshi2023expressive} in the context of the geometric WL test. \cite{frank2022so3krates} showed that global attention is capable of capturing the angular trends of cumulenes with fixed length. We go beyond and demonstrate that MFNs are capable of accurately extrapolating to longer chains, simultaneously capturing length and angle trends. In contrast, global attention models such as Spookynet~\citep{Unke2021}, are unable to extrapolate to longer chains, highlighting the benefit of the matrix function formalism.  For all MFN models in this section, we use MACE layers~\citep{Batatia2022mace} to form the matrix at each layer. We refer to the model as MFN (MACE). Details of the training set and the specific choice of parameterization of the matrix entries are included in the Appendix~\ref{sec:appendix-numerical}.

\begin{figure}[h!]
    \centering
    \vspace{-7pt}
    \includegraphics[width=0.99\linewidth]{figures/cumulene-expressivity-invar-4.pdf}
	\caption{\textbf{Visualizing MFN expressivity on cumulene chains.} The left panel depicts energy trends with respect to cumulene chain length at a fixed angle . The right panel shows the DFT (ground truth) and the predicted energy as a function of the dihedral angle  between the hydrogen atoms for a cumulene chain containing 12 carbon atoms. Local many-body equivariant models (MACE) are only able to capture average trends, even though test configurations are included in the training set. Invariant MFNs () capture only the trends with respect to length, while equivariant MFNs () capture both non-local trends. All models have a cutoff distance  of 3\AA{}, corresponding to the nearest neighbors, with two message-passing layers. The cutoff distance as well as MACE's receptive field for the first carbon atom is annotated in the left panel.}
    \label{fig:cumulenes-combined}
\end{figure}
\textbf{Trends with chain length and rotation}
The lowest energy structure of cumulenes alternates between 90- and 0-degree angles for odd and even carbon atom counts, respectively. Consequently, varying the number of carbon atoms at a fixed angle unveils a distinctive zigzag pattern in the ground truth energy (Fig.~\ref{fig:cumulenes-combined} left panel). Although the local model, with two message passing layers, is trained on exactly these configurations, this system is beyond its expressivity, as can be seen by the linear trend for  in (Figure~\ref{fig:cumulenes-combined} left panel). In contrast, the invariant and equivariant MFN models perfectly reproduce density functional theory (DFT) energies, thanks to their inherent non-locality. 
To demonstrate the necessity of equivariance, we train models on the energy of a fixed size cumulene as a function of the dihedral angle between the hydrogen atoms. Figure~\ref{fig:cumulenes-combined} demonstrates that only the equivariant MFN (L=1) captures the angular trend. 



\textbf{Guaranteed Non-Local dataset}
Datasets designed to test non-local effects often yield unexpectedly high accuracy when evaluated with local models~\citep{kovacsEvaluationMACEForce2023}, complicating the assessment of model non-locality. The dataset introduced here is based on cumulenes, whose strong electronic delocalization results in a directly observable non-locality. The training set contains geometry-optimized cumulenes with 3-10 and 13, 14 carbon atoms, which are then rattled and rotated at various angles. The test set contains cumulenes created in a similar fashion with the same number of carbons (in-domain) and cumulenes of unseen length, not present in the dataset (out-domain 11,12 and 15,16).  Table~\ref{tab:error_table} shows that the MFN architecture significantly outperforms both local and attention-based models (Spookynet). Attention captures some non-locality, resulting in marginally lower errors on the train and in-domain test set. However, the learned non-locality does not generalize to larger molecules, obtaining energy and forces worse than those obtained with a simple local model. The structured non-locality of MFNs enables generalization to larger unseen system sizes.
\begin{table}[htp]
    \centering
    \resizebox{0.79\linewidth}{!}{
    \begin{tabular}{lccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{}}  & \multicolumn{3}{c}{\textbf{E (meV/atom)}} & \multicolumn{3}{c}{\textbf{F (meV/A)}} \\
        & & MACE & \small{SpookyNet}& MFN (MACE) & MACE & \small{SpookyNet} & MFN (MACE) \\
        \midrule
        Train              & 3-10,13,14 & {41.1} &  \underline{31.4} & \textbf{2.0} & 179.6  & \underline{114.1} & \textbf{31.7} \\
        Test (In Domain)   & 3-10,13,14 & {41.8} & \underline{30.8} & \textbf{2.6} & {205.6} & \underline{162.3} &\textbf{34.0} \\
        Test (Out Domain)  & 11,12      & \underline{16.5} & 31.4 & \textbf{0.9} & \underline{108.5} &  {116.2} &\textbf{22.5} \\
        Test (Out Domain)  & 15,16      & \underline{12.0} & {23.4} &\textbf{2.6} & \underline{77.1} & {87.6} &\textbf{37.7} \\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Guaranteed non-local cumulene dataset} containing rattled cumulene chains, with various chain lengths () and hydrogen dihedral angles (). The table compares energy (E) and forces (F) RMSEs between local two-layer MPNNs (MACE), global attention MPNNs (SpookyNet), and equivariant MFNs. Train and in-domain test sets contain cumulenes of lengths 3-10 and 13,14. Two out-domain test sets compare different levels of extrapolation to unseen cumulene lengths, containing cumulenes with 11, 12 and 15, 16 carbon atoms, respectively. Bold is best and underline second best.}
    \vspace{-14pt}
    \label{tab:error_table}
\end{table}

\subsection{Performance on pure graphs}
\label{sec:sec-pure-graph}
\vspace{-5pt}
In this section, we evaluate the performance of our MFNs models in graph-level prediction tasks using GCN layers for the matrix construction. Detailed of the datasets and hyperparameters of the models can be found in the Appendix~\ref{sec:appendix-numerical}.  


\textbf{ZINC.}
We use the default dataset splits for ZINC, aligned with the leaderboard baseline settings, with approximately 500K parameters set. Table~\ref{tab:zinc_results} shows that MFN surpasses previous architectures, demonstrating the utility of learning various operators, even on pure graphs.
\begin{table}[h!t]
\centering
\caption{Results on ZINC with the MAE and number of parameters used, where the best results are in bold. Baselines are taken from~\citep{yang2023better} and model citations are in~\ref{sec:baseline-zinc}.}
\label{tab:zinc_results}
\vspace{-10pt}
\resizebox{0.9\linewidth}{!}{\begin{tabular}{l|cccccccc}
        \toprule
        Method & GCN & GAT & MPNN & GT & SAN & Graphormer & PDF & MFN (GCN) \\
        \midrule
         & 0.3670.011 & 0.3840.007 & 0.1450.007 & 0.2260.014 & 0.1390.006 & 0.1220.006 & 0.0660.004 & \textbf{0.0630.002}\\
         & 505k & 531k & 481k & NA & 509k & 489k & 472k & 512k \\
        \bottomrule
    \end{tabular}
}
\end{table}


\textbf{TU Datasests.}
We test our model on five TUDataset datasets involving both bioinformatics datasets (MUTAG, ENZYMES, PTC MR, and PROTEINS) and a social network dataset (IMDB-B). 
To ensure a fair comparison with baselines, we follow the standard 10-fold cross-validation and dataset split in table~\ref{tab:tu_results}.
\vspace{-4pt}
\begin{table*}[h!]
	\centering
	\caption{Results on TUDataset (Higher is better). Bold is best, and underlined second best within . Baselines are taken from~\citep{yang2023better} and model citations are in~\ref{sec:baseline-tu}}
	\label{tab:tu_results}
	\vspace{-5pt}
	\resizebox{0.70\textwidth}{!}{\begin{tabular}{l|cccccc}
			\toprule
			Method    & MUTAG & ENZYMES & PTC\_MR & PROTEINS & IMDB-B \\
			\midrule
			GK & 81.522.11 & 32.701.20 & 55.650.5 & 71.390.3 & - \\
			RW & 79.112.1 & 24.161.64 & 55.910.3 & 59.570.1 & - \\
			PK & 76.02.7 & - & 59.52.4 & 73.680.7 & - \\
			AWE & 87.879.76 & 35.775.93 & - & - & 74.455.80 \\
			\midrule
			PSCN & 88.954.4 & - & 62.295.7 & 752.5 & 712.3 \\
			ECC & 76.11 & 45.67 & - & - & - \\
			DGK & 87.442.72 & 53.430.91 & 60.082.6 & 75.680.5 & 66.960.6 \\
			GraphSAGE & 85.17.6 & 58.26.0 & - & - & 72.35.3 \\
			CapsGNN & 88.676.88 & 54.675.67 & - & \underline{76.23.6} & 73.14.8 \\
			GIN & 89.45.6 & - & 64.67.0 & 76.22.8 & \underline{75.15.1} \\
			-GNN & 86.1 & - & 60.9 & 75.5 & 74.2 \\
			IGN & 83.8912.95 & - & 58.536.86 & \underline{76.585.49} & 72.05.54 \\
			PPGNN & \underline{90.558.7} & - & 66.176.54 & \textbf{77.204.73} & 73.05.77 \\
			GCN & 89.391.60 & - & 66.841.79 & 71.711.04 & 74.802.01 \\
			
			PDF  & 89.914.35 & \textbf{73.50}\textbf{6.39} & \underline{68.368.38} & \underline{76.285.1} & \textbf{75.60}\textbf{2.69} \\
             \midrule
                MFN (GCN) &\textbf{91.57.35} &  \underline{72.97.55}&\textbf{68.98.09}& \underline{76.184.07}& 74.11.04\\
			\bottomrule
		\end{tabular}
	}
	\vspace{-10pt}
\end{table*}

\section{Conclusion}
\vspace{-8pt}
We have introduced Matrix Function Networks (MFNs), an architecture designed to address the limitations of existing GNNs and MPNNs in modeling non-local many-body interactions. Utilizing a resolvent expansion, MFNs achieve potentially linear scaling with respect to system size, offering a computationally efficient solution. Our evaluations indicate state-of-the-art performance on ZINC and TU graph datasets without human-designed features to capture the global graph topology. We also demonstrate that our architecture is capable of modeling the complex non-local interactions of cumulene quantum systems. Future work could focus on extending MFNs to other complex systems, further validating its adaptability and efficiency, and exploring its interpretability.

\subsubsection*{Reproducibility Statement}
To ensure reproducibility and completeness, we include detailed descriptions of the model used, hyperparameters, and data sets in the Appendix. The ZINC and TU datasets are publicly available. We also attach our cumulene datasets in a supplementary. The code will be made public.





\subsubsection*{Acknowledgments}
CO's work was supported by the NSERC Discovery Grant IDGR019381 and the NFRF Exploration Grant GR022937. 
LLS acknowledges support from the EPSRC Syntech CDT with grant reference EP/S024220/1. 
Computational resources were provided by the Cambridge Service for Data Driven Discovery (CSD3), which was accessed through the University of Cambridge EPSRC Core Equipment Award EP/X034712/1.

\subsubsection*{Ethics Statement}
FAF is employed by AstraZeneca at time of publication; however, none of the work presented in this manuscript was conducted at or influenced by this affiliation.

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\appendix
\section{Appendix}
\subsection{Equivariance}
\label{sec:equivariant-op}





\paragraph{Equivariance of the resolvent}
The pole expansion yields a straightforward proof  of  equivariance:  

For a general continuous function , the analogous result follows by density.







\begin{comment}
\subsection{Proof of infinite order message passing}

(This is a very rough sketch of the proof, and it needs polish)


Let's consider a graph, where its representation is given by a set of matrices  constructed from the graph's edge and node features.

The message update mechanism for our model can be defined as:


Since the resolvent form a complete basis, a sufficiently large basis set (i.e., a large number of 's and 's) allows one to rewrite the formula as follows:

where  are arbitrary weights.

Applying this process again layer, we get:


From the above equation, it can be observed that this type of message passing behaves as an infinite-body-order infinite-layer message passing model after just two updates.
\end{comment}

\subsection{Details of numerical experiments}
\label{sec:appendix-numerical}


\subsubsection{Cumulenes}



\paragraph{Dataset}

The cumulene dataset is designed to test the expressivity of graph neural networks and their non-local capabilities. The task is to regress energy and forces from 3D molecular configurations. The ground truth energy is obtained using the ORCA quantum chemistry code at the density functional theory level of accuracy using the \textit{wB97X-D3} functional, the \textit{def2-TZVP} basis set, and very tight SCF convergence criteria. 


\paragraph{Length and angle scans}
\begin{figure}[h!t]
    \centering
    \includegraphics[width=0.88\linewidth]{figures/cumulene-diagram.pdf}
    \caption{\textbf{Angles and distances that define a cumulene graph} used to test expressivity in Figure~\ref{fig:cumulenes-combined}. The carbon-hydrogen (), first carbon-carbon (), and remaining carbon-carbon distances () are set to 1.086~\AA{}, 1.315~\AA{} and 1.279~\AA{} respectively. The angle between the hydrogen-carbon-hydrogen () is fixed at 118.71 degrees and the dihedral angle  depends on the experiment as detailed in the main text.}
    \label{fig:appendix-cumulene-graph}
\end{figure}
The configurations for the length and angle scans consist of chain-like graphs as in Figure~\ref{fig:appendix-cumulene-graph}. The length scans range from 3 to 30 atoms fixed at an angle of 5 degrees. Additionally, we scan the angle between the two terminating hydrogen groups for a cumulene with twelve carbon atoms (). At increments of 3 degrees, we scan from 0 to 90 degrees. By symmetry, this covers the entire range of unique configurations from 0 to 360 degrees. For the expressivity scan, all internal coordinates (see Figure~\ref{fig:appendix-cumulene-graph}) that uniquely define the cumulene are kept constant and set to the geometry-optimised configuration of the length 30 cumulenes. The carbon-hydrogen, first carbon-carbon, and remaining carbon-carbon distances are set to 1.086~\AA{}, 1.315~\AA{} and 1.279~\AA{} respectively (see Figure~\ref{fig:appendix-cumulene-graph}). For the length 12 cumulene, the distance between the most distant carbons is 14.1\AA{}. Thus, it becomes impossible for a purely local equivariant MPNN, with cutoff 3\AA{} and two message-passing layers, to differentiate between two graphs at different angles. 

Note that the relative energies of~\ref{fig:cumulenes-combined} are obtained by subtracting individual atom energies. These are -16.3eV and -1036.1eV for each hydrogen and carbon atoms respectively.


\paragraph{Guaranteed Non-local dataset}

The GNL dataset tests how well a model can extrapolate to an unseen number of atoms. Furthermore, configurations are rattled to see how well the model can capture both local and non-local effects. Configurations are generated starting from the relaxed geometries of cumulenes with length 0-20. Relaxations are carried out with the smaller \textit{6-31G} basis set. The cumulenes are subsequently rotated by an equally spaced scan with increment 6 degrees, starting from a random angle, and the positions are subsequently randomly perturbed by Gaussian noise with standard deviation 0.01~\AA{}. The training and validation set contain cumulenes of length 0-10 and 13-14, with 10 and 3 samples, respectively. The test set contains in-domain configurations with two samples for each of the lengths present in the training set. Furthermore, it contains configurations with 11-12 and 15-16 carbon atoms, labeled out-of-domain. In total, the train, validation, and test set contain 200, 50 and 170 configurations.  


\paragraph{MACE and MFN Model}
Both the local MACE and MFN are trained on the same graphs, which means that the cutoff distance is fixed at 3\AA{}, including information from the nearest neighbor. Both models are trained with two layers, allowing the local model to distinguish changes that are separated by 12\AA{}. The MFN is trained using 16 matrix channels and 16 poles. For the matrix construction step, we use a MACE~\cite{Batatia2022mace} layer with a correlation order 3, ,  and  channels to generate node features . The node features of MACE are then used to construct the matrix using the following formula,

where  is a radial function of the distances between atoms  and , ,  is the real spherical harmonics and  the normalized vector between  and . We then use the diagonal update~\ref{eq:update} to update the node features of MACE and reiterate. The readout at the first layer is a linear and at the second it is a one layer MLP with 16 hidden dimensions.

\paragraph{Spookynet Model}

The Spookynet architecture was trained on the same dataset as the MFN model. We use a one-layer model with 5.5 \r{A} cutoff to reproduce the receptive field of MACE. We use the global attention and electrostatic interactions. The model has 128 channels.

\paragraph{Training}
All models underwent initial training on the GNL dataset before transfer learning was applied to the relevant length and angle scan data sets. The saved model corresponds to the epoch that exhibits the minimum loss values. Details on settings such as learning rate and epoch count are disclosed in Table~\ref{tab:model-parameters}. Training incorporated both energy and forces, with adjustable weights for each observable. In particular, for length and angle scans, an additional 100 epochs with zero force weight were used after initial training, ensuring the depiction of the minimal possible energy error in Figure~\ref{fig:cumulenes-combined}, as the lowest-loss model is saved.


\begin{table}[h]
    \centering
    \caption{Model training parameters. For the matrix functions the number of poles () and matrix channels () are indicated. }
    \label{tab:model-parameters}
    \resizebox{0.9\textwidth}{!}{\begin{tabular}{llrlrrrll}
        \toprule
        dataset & model & epochs & lr &  &  &  &  & other \\
        \midrule
        \textbf{Length Scan} & MFN (L=0) & 1240 & 1e-2 & 1000 & 100 & 2 & 3 & =16,=16 \\
        & MFN (L=1) & 5628 & 1e-5 & 1000 & 100 & 2 & 3 & =16,=16 \\
        & MACE & 5288 & 0.005 & 1000 & 100 & 2 & 3 & - \\
        \midrule
        \textbf{Angle Scan} & MFN (L=0) & 1240 & 1e-2 & 1000 & 100 & 2 & 3 & =16,=16 \\
        & MFN (L=1) & 656 & 1e-5 & 1000 & 100 & 2 & 3 & =16,=16 \\
        & MACE & 954 & 0.005 & 1000 & 100 & 2 & 3 & - \\
        \midrule
        \textbf{GNL} & MFN (L=1) & 4958 & 1e-2 & 100 & 1 & 2 & 3 & =16,=16 \\
        & MACE & 1260 & 1e-2 & 100 & 1 & 2 & 3 & - \\
        & Spookynet & 3500 & 1e-4 & 0.10 & 0.90 & 1 & 5.5 & attention \\
        \bottomrule
    \end{tabular}
    }
\end{table}


\subsubsection{ZINC}


\paragraph{Dataset}
The ZINC dataset ~\citep{Irwin2004} contains 12,000 small 2D molecular graphs with an average of 23 nodes with information on the node attributes and the edge attributes. The task is to regress the constrained solubility  where  is the octanol-water partition coefficients,
SA is the synthetic accessibility score, and  is the number of long cycles. For
each molecular graph, the node features are the species of heavy elements.

\paragraph{Model}
The model is made up of six layers.
Only the first two layers are MFN layers (in order to save on the number of parameters).
The initial edge features  are obtained by concatenating PDF~\cite{yang2023better} descriptors used for ZINC and the input edge features of the data set.  At each layer , the initial nodes features, , are computed using a convolutional GNN with 140 channels. 
For the first two layers, the matrix is then formed using a multilayer perceptron (MLP) of size  with the GELU activation function and a batch norm,

We compute the matrix function, using 16 poles and 16 channels. We extract the diagonal elements to update the node features, and use the off diagonal elements to update the edge features by passing them into a residual 2 layers MLP of size . We used an average pooling followed by a linear layer for the readout.

\paragraph{Training}
 Models were trained with AdamW,
with default parameters of  = 0.9,  = 0.999, and  and a weight decay of .
We used a learning rate of 0.001 and a batch size of 64.
The learning rate was reduced using an on-plateau scheduler.

\paragraph{Baselines} 
\label{sec:baseline-zinc}
The baseline models used for the ZINC dataset comparisons include: 
GCN~\citep{kipf2017semi},
GAT~\citep{velickovic2018graph},
MPNN~\citep{gilmer2017neural},
GT~\citep{dwivedi2020benchmarking},
SAN~\citep{kreuzer2021rethinking},
Graphormer~\citep{ying2021transformers},
PDF~\citep{yang2023better}




\subsubsection{TUDatasets}

\paragraph{Dataset} We train on a subset of the TUDatasets including the MUTAG, ENZYMES, PTC-MR, PROTEINS and IMDB-B subsets. The MUTAG, ENZYMES, PTC-MR, and PROTEINS datasets are molecular datasets that contain node-level information on the molecules. The IMDB-B a social network datasets.

\paragraph{Model}
The number of layers for each subset is in Table~\ref{tab:model-parameters-pure-graphs}.
We use MFN layers only for the first 3 layers (in order to save on the number of parameters). The number of hidden channels, , for each model is given in~\ref{tab:model-parameters-pure-graphs}.
The initial edge features  are obtained by concatenating PDF~\cite{yang2023better} descriptors used for each subset and the input edge features of the data set.  In each layer , the initial node features, , are computed using a convolutional GNN. 
For the first two layers, the matrix is then formed using a multilayer perceptron (MLP) of size  with the activation function GELU and a batch norm,

We compute the matrix function, using 16 poles and 16 channels. We extract the diagonal elements to update the node features, and use the off diagonal elements to update the edge features by passing them into a residual 2 layers MLP of size . We used an average pooling followed by a linear layer for the readout.
\vspace{-8pt}
\paragraph{Training}
 Models were trained with AdamW,
with default parameters of  = 0.9,  = 0.999, and  and a weight decay of .
We used a learning rate of 0.001 and a batch size of 64.
The learning rate was reduced using an on-plateau scheduler. 


\begin{table}[h]
    \centering
    \caption{Model training parameters. For the matrix functions the number of poles () and matrix channels () are indicated}
    \label{tab:model-parameters-pure-graphs}
    \resizebox{0.6\textwidth}{!}{\begin{tabular}{llrlll}
        \toprule
        dataset & epochs & lr &  &  & other \\
        \midrule
        \textbf{MUTAG} &  301 & 1e-3  & 6 & 256 & =16,=16 \\
        \midrule
        \textbf{ENZYMES} & 201 & 1e-3 & 6 & 256 & =16,=16 \\
        \midrule
        \textbf{PTC-MR} &  151 & 1e-3 & 6 & 128 & =16,=16 \\
        \midrule
        \textbf{PROTEINS} &  451 & 1e-3 & 6 & 128 & =16,=16 \\
        \midrule
        \textbf{IMDB-B} &  301 & 1e-3 & 3 & 256 & =16,=16 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\paragraph{Baseline}
\label{sec:baseline-tu}
The baseline models used for the TU datasets comparisons include:  
	GK~\citep{shervashidze2009efficient},
	RW~\citep{vishwanathan2010graph},
	PK~\citep{neumann2016propagation},
AWE~\citep{pmlr-v80-ivanov18a},
PSCN~\citep{niepert2016learning},
ECC~\citep{simonovsky2017dynamic},
	DGK~\citep{yanardag2015deep},
	CapsGNN~\citep{xinyi2018capsule},
GIN~\citep{xu2018how},
	-GNN~\citep{morris2019weisfeiler},
	IGN~\citep{maron2018invariant},
	PPGNN~\citep{maron2019provably},
	GCN~\citep{de2020natural}
	GraphSage~\citep{hamilton2017inductive},
        PDF~\citep{yang2023better}
\end{document}
