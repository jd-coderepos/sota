

\section{Experiments}
We validate our proposed approach by conducting comprehensive experiments on two FSS benchmarks: PASCAL-~\cite{Shaban2017} and COCO-~\cite{Nguyen2019}.

\subsection{Experimental Setup}\label{sec:exptsetup}
\parsection{Datasets}
We conduct experiments on the PASCAL-~\cite{Shaban2017} and COCO-~\cite{Nguyen2019} benchmarks. PASCAL- is composed of PASCAL VOC 2012~\cite{Everingham2010} with additional SBD~\cite{Hariharan2011} annotations. The dataset comprises 20 categories split into 4 folds. For each fold, 15 categories are used for training and the remaining 5 for testing. COCO-~\cite{Nguyen2019} is more challenging and is built from MS-COCO~\cite{Lin2014}. Similar to PASCAL-, the COCO- benchmark is split into 4 folds. For each fold, 60 base classes are used for training and the remaining 20 for testing. 

\begin{table}[ht!]
    \caption{State-of-the-art comparison on the PASCAL- and COCO- benchmarks in terms of mIoU (higher is better). In each case, the best two results are shown in magenta and cyan font, respectively. Asterisk  denotes re-implementation by Liu \etal~\cite{Liu2020_DE}. Our DGPNet achieves state-of-the-art results for 1-shot and 5-shot on both benchmarks. When using ResNet101, our DGPNet achieves absolute gains of 5.5 and 8.4 for 1-shot and 5-shot segmentation, respectively, on the challenging COCO- benchmark, compared to the best reported results in the literature.}
    \centering \vspace{-3mm}
    \resizebox{0.80\columnwidth}{!}{\begin{tabular}{llllll}
        \toprule 
        \multirow{2}{*}{Backbone} & \multirow{2}{*}{Method} & \multicolumn{2}{l}{PASCAL-} & \multicolumn{2}{l}{COCO-} \\
        &&  1-shot & 5-shot & 1-shot & 5-shot \\
        \midrule
        \multirow{14}{*}{ResNet50}      & CANet \cite{ChiZhang2019} \tiny{CVPR'19}       &  55.4  & 57.1 & 40.8 & 42.0\\
        & PGNet \cite{Zhang2019_PG}    \tiny{ICCV'19}        & 56.0 & 58.5 & 36.7 & 37.5\\
        & RPMM \cite{yang2020prototype} \tiny{ECCV'20}       & 56.3 & 57.3 & 30.6 & 35.5\\
        & CRNet \cite{Liu2020_CR}   \tiny{CVPR'20}           & 55.7 & 58.8 & - & -\\
        & DENet \cite{Liu2020_DE}   \tiny{MM'20}           & 60.1 & 60.5 & \second{42.8} & 43.0\\
        & LTM \cite{yang2020new}   \tiny{MM'20}            & 57.0 & 60.6 & - & -\\
        & PPNet \cite{Liu2020_PP}    \tiny{ECCV'20}          & 51.5 & 62.0 & 25.7 & 36.2\\
        & PFENet \cite{Tian2020}   \tiny{TPAMI'20}            & 60.8 & 61.9 & - & -\\
        & RePri \cite{boudiaf2021few} \tiny{CVPR'21}         & 59.1 & 66.8 & 34.0 & 42.1\\
        & SCL \cite{zhang2021self}  \tiny{CVPR'21}           & 61.8 & 62.9 & - & -\\
        & SAGNN \cite{xie2021scale} \tiny{CVPR'21}           & 62.1  & 62.8 & - & -\\
        & CWT \cite{lu2021simpler} \tiny{ICCV'21}            & 56.4 & 63.7 & 32.9 & 41.3\\
        & CMN \cite{xie2021few} \tiny{ICCV'21}               & 62.8 & 63.7 & 39.3 & 43.1\\
        & MLC \cite{yang2021mining} \tiny{ICCV'21}           & 62.1 & 66.1 & 33.9 & 40.6\\
        & MMNet \cite{wu2021learning} \tiny{ICCV'21}         & 60.2 & 61.8 & 37.2 & 37.4\\
        & HSNet \cite{min2021hypercorrelation} \tiny{ICCV'21} & \second{64.0} & \second{69.5} & 39.2 & \second{46.9}\\
        & CyCTR \cite{zhang2021few} \tiny{NeurIps'21}           & \first{64.2} & 65.6 & 40.3 & 45.6\\
        
        \cmidrule(lr){2-6}
        & \textbf{DGPNet} \textbf{(Ours) }     & 63.50.4 & \first{73.50.3} & \first{45.00.4} & \first{56.20.4}\\
        \midrule
        \multirow{12}{*}{ResNet101}
        & FWB \cite{Nguyen2019} \tiny{CVPR'19}&  56.2  & 60.0 & 21.2 & 23.7\\
        & DAN \cite{Wang2020}   \tiny{ECCV'20}&  58.2  & 60.5 & 24.4 & 29.6\\
        & PFENet \cite{Tian2020} \tiny{TPAMI'20}& 60.1 & 61.4 & 38.5 & 42.7\\
        & RePri \cite{boudiaf2021few} \tiny{CVPR'21}& 59.4 & 65.6 & - & -\\
        & VPI \cite{wang2021variational} \tiny{WACV'21}& 57.3 & 60.4 & 23.4 & 27.8\\
        & ASGNet \cite{li2021adaptive} \tiny{CVPR'21}& 59.3 & 64.4 & 34.6 & 42.5\\
        & SCL \cite{zhang2021self}  \tiny{CVPR'21}& -    & -    & 37.0 & 39.9\\
        & SAGNN \cite{xie2021scale} \tiny{CVPR'21}& -    & -    & 37.2 & 42.7 \\
        & CWT \cite{lu2021simpler}  \tiny{ICCV'21}& 58.0 & 64.7 & 32.4 & 42.0\\
        & MLC \cite{yang2021mining} \tiny{ICCV'21}& 62.6 & 68.8 & 36.4 & 44.4\\
        & HSNet \cite{min2021hypercorrelation} \tiny{ICCV'21} & \first{66.2} & \second{70.4} & \second{41.2} & \second{49.5}\\
        & CyCTR \cite{zhang2021few} \tiny{NeurIps'21}           & 64.3 & 66.6 & -    & -\\
        \cmidrule(lr){2-6}
        & \textbf{DGPNet} \textbf{(Ours)}      & \second{64.80.5}  & \first{75.40.4} & \first{46.70.3} & \first{57.90.3}\\
    \bottomrule
    \end{tabular}}
    \label{tab:sota-table}
    \vspace{-5mm}
\end{table}

\parsection{Implementation Details}
Following previous works, we employ ResNet-50 and ResNet-101~\cite{he2016deep} backbones pre-trained on ImageNet~\cite{ILSVRC15} as image encoders. We let the dense GP work with feature maps produced by the third and the fourth residual module. In addition, we place a single convolutional projection layer that reduces the feature map down to 512 dimensions. As mask encoder, we use a light-weight CNN (see Appendix~\ref{sec:appendix-impl-details}).  We use , , and  in our GP. We train our models for 20k and 40k iterations Ã  8 episodes for PASCAL- and COCO-, respectively. On one NVIDIA A100 GPU, this takes up to 10 hours. We use the AdamW~\cite{Kingma2015adam,loshchilov2018decoupled} optimizer with a weight decay factor of  and a cross-entropy loss with 1 to 4 background to foreground weighting. We use a learning rate of  for all parameters save for the image encoder, which uses a learning rate of . The learning rate is decayed with a factor of  when 10k iterations remain. We freeze the batch normalization layers of the image encoder. Episodes are sampled in the same way as we evaluate. We randomly flip images horizontally followed by resizing to a size of  for PASCAL- and  for COCO-.

\parsection{Evaluation}
We evaluate our approach on each fold by randomly sampling 5k and 20k episodes respectively, for PASCAL- and COCO-. This follows the work of Tian \etal~\cite{Tian2020} in which it is observed that the procedure employed by many prior works, using only 1000 episodes, yields fairly high variance. Additionally, following Wang \etal~\cite{Wang2019_PA}, Liu \etal~\cite{Liu2020_PP}, Boudiaf \etal~\cite{boudiaf2021few}, and Zhang \etal~\cite{zhang2021self}, our results in the state-of-the-art comparison are computed as the average of 5 runs with different seeds. We also report the standard deviation. Performance is measured in terms of mean Intersection over Union (mIoU). First, the IoU is calculated per class over all episodes in a fold. The mIoU is then obtained by averaging the IoU over the classes. As in the original work on FSS by Shaban \etal~\cite{Shaban2017}, we calculate the IoU on the original resolution of the images.

\subsection{State-of-the-Art Comparison}\label{sec:sota}

\begin{table}[t]
    \caption{Performance comparison (mIoU, higher is better), when performing cross-dataset evaluation from COCO- to PASCAL. When using the same ResNet50 backbone, our approach achieves significant improvements for both 1-shot and 5-shot settings, with absolute gains of 5.8 and 11.3 mIoU over RePRI~\cite{boudiaf2021few}.}\centering \vspace{-3mm}
    \resizebox{0.56\linewidth}{!}{\begin{tabular}{ll l l }
        \toprule
        Backbone & Method &        1-Shot         & 5-Shot  \\
        \midrule
        \multirow{4}{*}{ResNet50}&RPMM~\cite{yang2020prototype}  &49.6  &53.8\\
        &PFENet~\cite{Tian2020}         & 61.1  & 63.4\\
        &RePRI~\cite{boudiaf2021few} & \second{63.1}  & 66.2\\
        &HSNet~\cite{min2021hypercorrelation} & 61.6 & \second{68.7} \\
        &\textbf{DGPNet (Ours)}  & \first{68.90.4} &  \first{77.50.2}\\
        \midrule
        \multirow{2}{*}{ResNet101}&HSNet~\cite{min2021hypercorrelation} & 64.1 & 70.3 \\
        &\textbf{DGPNet (Ours)}  & \first{70.10.3} &  \first{78.50.3}\\
        \bottomrule
    \end{tabular}}
    \label{tab:domaintransfer}
    \vspace{-5mm}
\end{table}

We compare our proposed DGPNet with state-of-the-art FSS approaches on the PASCAL- and COCO- benchmarks, see Table~\ref{tab:sota-table}. Following prior works, we report results given a single support example, \emph{1-shot}, and given five support examples, \emph{5-shot}. On PASCAL- with a ResNet50 backbone, recently proposed approaches -- RePri~\cite{boudiaf2021few}, SCL~\cite{zhang2021self}, SAGNN~\cite{xie2021scale}, CWT~\cite{lu2021simpler}, CMN~\cite{xie2021few}, MLC~\cite{yang2021mining}, MMNet~\cite{wu2021learning}, HSNet~\cite{min2021hypercorrelation}, and CyCTR~\cite{zhang2021few} -- range from 56.4 mIoU to 64.2 mIoU. Our proposed DGPNet obtains a competitive performance of 63.5 mIoU. Where the powerful dense Gaussian process of DGPNet really shines, however, is when additional support examples are given. In the 5-shot setting, these recently proposed approaches range from 61.8 mIoU to 69.5 mIoU. Our proposed DGPNet obtains 73.5 mIoU, setting a new state-of-the-art for 5-shot few-shot segmentation on PASCAL-.

On the challenging COCO- benchmark, the previous best reported 1-shot result comes from the work of Liu \etal~\cite{Liu2020_DE}, 42.8 mIoU. In their work, however, no significant improvement is reported when additional support examples are given. Three other approaches -- CMN~\cite{xie2021few}, HSNet~\cite{min2021hypercorrelation}, and CyCTR~\cite{zhang2021few} -- obtain performance close to the work of Liu \etal~\cite{Liu2020_DE}, from 39.3 mIoU to 40.3 mIoU. Compared to the work of Liu \etal~\cite{Liu2020_DE}, CMN, HSNet, and CyCTR scale better with additional support examples, obtaining between 43.1 mIoU and 46.9 mIoU, with HSNet reporting the largest gain, 7.7 mIoU. Our proposed DGPNet obtains a 1-shot performance of 45.0 mIoU, setting a new state-of-the-art. As additional support examples are given, the gap to prior works increases. In the 5-shot setting, DGPNet obtains 56.2 mIoU, beating the previous best approach, HSNet, by 9.7 mIoU. Our per-fold results, including the standard deviation for each fold, are provided in appendix \ref{sec:appendix-sota-perfold}. Qualitative examples are found in Fig.~\ref{fig:appendix-qualitative-1-5-10} and Appendix~\ref{sec:appendix-qualitative}.

\parsection{Scaling Segmentation Performance with Increased Support Size}
As discussed earlier, the FSS approach is desired to effectively leverage additional support samples, thereby achieving superior accuracy and robustness for larger shot-sizes. We therefore analyze the effectiveness of the proposed DGPNet by increasing the number of shots on PASCAL- and COCO-. Fig.~\ref{fig:kshot-performance} shows the results on the two benchmarks. We use the model trained for 5-shot in all cases, except for 1-shot. Compared to most existing works, our DGPNet effectively benefits from additional support samples, achieving remarkable improvement in segmentation accuracy with larger support sizes.

We provide a qualitative comparison between different support set sizes in Fig.~\ref{fig:appendix-qualitative-1-5-10}. For this comparison, we use COCO-. In general, the 1-shot setting is quite challenging for several classes due to major variations in where the object appears and what it looks like. If the single support sample is too different from the query sample, the model tends to struggle. With five support samples, the model performs far better. At five, the benefit of additional support samples seems to saturate, with ten samples often bringing only marginal improvements. An example is the train episode in Fig.~\ref{fig:appendix-qualitative-1-5-10}, where minor improvements are obtained when going from five to ten support samples.

\begin{figure*}[h!]
    \centering
    \begin{tabular}{@{\hspace{0.33em}}c@{} @{\hspace{0.33em}}c@{} @{\hspace{0.33em}}c@{} @{\hspace{0.33em}}c@{} @{\hspace{0.33em}}c@{} @{\hspace{0.33em}}c@{} @{\hspace{0.33em}}c@{}}
         Support 1 & Support 2-5 & Support 6-10 & Query & 1-shot & 5-shot & 10-shot\\
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_0.png} &
         \begin{tabular}[b]{@{} c @{} c @{}}
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_1.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_2.png}\\
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_3.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_4.png}\\
         \end{tabular} &
         \begin{tabular}[b]{@{} c @{} c @{} c @{}}
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_5.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_6.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_7.png}\\
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_8.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_support_anno_9.png} &\\
         \end{tabular} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_query_anno.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_1shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_5shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_12_query_2142_query_pred.png}\\
         
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_0.png} &
         \begin{tabular}[b]{@{} c @{} c @{}}
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_1.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_2.png}\\
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_3.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_4.png}\\
         \end{tabular} &
         \begin{tabular}[b]{@{} c @{} c @{} c @{}}
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_5.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_6.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_7.png}\\
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_8.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_support_anno_9.png} &\\
         \end{tabular} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_query_anno.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_1shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_5shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_0_2/eval_class_68_query_4531_query_pred.png}\\
         
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_0.png} &
         \begin{tabular}[b]{@{} c @{} c @{}}
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_1.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_2.png}\\
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_3.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_4.png}\\
         \end{tabular} &
         \begin{tabular}[b]{@{} c @{} c @{} c @{}}
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_5.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_6.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_7.png}\\
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_8.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_support_anno_9.png} &\\
         \end{tabular} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_query_anno.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_1shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_5shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_2_query_132_query_pred.png}\\
         
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_0.png} &
         \begin{tabular}[b]{@{} c @{} c @{}}
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_1.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_2.png}\\
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_3.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_4.png}\\
         \end{tabular} &
         \begin{tabular}[b]{@{} c @{} c @{} c @{}}
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_5.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_6.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_7.png}\\
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_8.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_support_anno_9.png} &\\
         \end{tabular} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_query_anno.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_1shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_5shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_30_query_993_query_pred.png}\\
         
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_0.png} &
         \begin{tabular}[b]{@{} c @{} c @{}}
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_1.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_2.png}\\
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_3.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_4.png}\\
         \end{tabular} &
         \begin{tabular}[b]{@{} c @{} c @{} c @{}}
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_5.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_6.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_7.png}\\
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_8.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_support_anno_9.png} &\\
         \end{tabular} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_query_anno.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_1shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_5shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_58_query_9432_query_pred.png}\\
         
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_0.png} &
         \begin{tabular}[b]{@{} c @{} c @{}}
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_1.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_2.png}\\
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_3.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_4.png}\\
         \end{tabular} &
         \begin{tabular}[b]{@{} c @{} c @{} c @{}}
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_5.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_6.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_7.png}\\
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_8.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_support_anno_9.png} &\\
         \end{tabular} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_query_anno.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_1shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_5shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_6_query_2975_query_pred.png}\\
         
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_0.png} &
         \begin{tabular}[b]{@{} c @{} c @{}}
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_1.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_2.png}\\
           \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_3.png} & \includegraphics[width=.065\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_4.png}\\
         \end{tabular} &
         \begin{tabular}[b]{@{} c @{} c @{} c @{}}
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_5.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_6.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_7.png}\\
           \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_8.png} & \includegraphics[width=.043\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_support_anno_9.png} &\\
         \end{tabular} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_query_anno.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_1shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_5shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_query_pred.png} &
         \includegraphics[width=.13\columnwidth]{images/johan_1_5_10/aut21w38_10shot_coco_gpn_resnet101_coco_2_2/eval_class_74_query_249_query_pred.png}\\
    \end{tabular}
\caption{Right: Qualitative results of our approach given 1, 5, and 10 support samples. The 1-shot results are based on (see left) Support 1; the 5-shot results on Support 1 and Support 2-5; and the 10-shot Support 1, Support 2-5, and Support 6-10. The results are from COCO- and human faces have been pixelized in the visualization, but the model makes predictions on the non-pixelized images.}
\label{fig:appendix-qualitative-1-5-10}
\vspace{-5mm}
\end{figure*}



\parsection{Cross-dataset Evaluation}
In Table~\ref{tab:domaintransfer}, we present the cross-dataset evaluation capabilities of our DGPNet from COCO- to PASCAL. For this cross-dataset evaluation experiment, we followed the same protocol as in Boudiaf \etal~\cite{boudiaf2021few}, where the folds are constructed to ensure that there is no overlap with COCO- training folds. When using the same ResNet50 backbone, our approach obtains 1-shot and 5-shot mIoU of 68.9 and 77.5, respectively, with absolute gains of 5.8 and 11.3 over RePRI~\cite{boudiaf2021few}. Further, DGPNet achieves segmentation mIoU of 70.1 and 78.5 in 1-shot and 5-shot setting, respectively, using ResNet101. For more details on the experiment, see Appendix \ref{sec:appendix-domaintransfer}.

\subsection{Ablation Study}\label{sec:ablation} 
Here, we analyze the impact of the key components in our proposed architecture. We first investigate the choice of kernel, . Then, we analyze the impact of the predictive covariance provided by the GP and the benefits of learning the GP output space. Last, we experiment with dense GPs at multiple feature levels. All experiments in this section are conducted with the ResNet50 backbone. Detailed experiments are provided in the Appendix. In each experiment, we also report the mean gain over a baseline (Mean ), where the mean is computed over the 1-shot and 5-shot performance on PASCAL- and COCO-.

\begin{table}[t]
  \caption{Performance of different kernels on the PASCAL- and COCO- benchmarks. Notably, both the Exponential and SE kernels significantly outperform the linear kernel, confirming the need for a flexible learner. Measured in mIoU (higher is better). Best results are in bold.}\vspace{-3mm}
  \centering \resizebox{0.5\linewidth}{!}{\begin{tabular}{llllll}
      \toprule 
      \multirow{2}{*}{Kernel}& \multicolumn{2}{l}{PASCAL-} & \multicolumn{2}{l}{COCO-} &  \\
      &  1-shot & 5-shot & 1-shot & 5-shot & Mean \\
      \midrule
      Linear & 58.6 & 61.8 & 37.1& 45.3 & 0.0 \\
      Exponential & 59.3 & 67.3 & 40.9 & \textbf{51.8} & 4.1\\
      SE & \textbf{62.1} & \textbf{69.9} & \textbf{41.7} & 51.2 & \textbf{5.5}\\
      \bottomrule
  \end{tabular}}
  \label{tab:kernel-ablation}
  \vspace{-3mm}
\end{table}


\parsection{Choice of Kernel}
Going from a linear few-shot learner to a more flexible function requires an appropriate choice of kernel. We consider the \emph{homogenous linear kernel} as our baseline. Note that the homogenous linear kernel is equivalent to Bayesian linear regression under appropriate priors~\cite{Rasmussen2006}. To make our learner more flexible, we consider two additional choices of kernels, the \emph{Exponential} and \emph{Squared Exponential} (SE) kernels. The kernel equations are given in Appendix~\ref{sec:appendix-kernel}. In Table~\ref{tab:kernel-ablation}, results on both the PASCAL and COCO benchmarks are presented. Both the Exponential and SE kernels greatly outperform the linear kernel, with the SE kernel leading to a mean gain of 5.5 in mIoU. These results show the benefit of a more flexible and scalable learner.

\begin{table}[t]
  \parbox{.48\linewidth}{
    \caption{Analysis of learning the GP output space (GPO) and incorporating covariance (Cov). Performance in mIoU (higher is better). Best results are in bold.}
    \vspace{-3mm}
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}{cclllll}
        \toprule
        \multirow{2}{*}{Cov} & \multirow{2}{*}{GPO}& \multicolumn{2}{l}{PASCAL-} & \multicolumn{2}{l}{COCO-} \\
        & &  1-shot & 5-shot & 1-shot & 5-shot & Mean  \\
        \midrule
        && 62.1 & 69.9 & 41.7 & 51.2 & 0.0\\
        \checkmark & & \textbf{62.5} & 71.8 & 43.8 & 53.7 & 1.7\\
        & \checkmark & 61.7 & \textbf{72.7} & 43.1 & 54.2 & 1.7\\
        \checkmark&\checkmark & \textbf{62.5} & \textbf{72.6} & \textbf{44.7} & \textbf{55.0} & \textbf{2.5}\\
        \bottomrule
    \end{tabular}}
    \label{tab:covar-lwl-ablation}
  }
  \hfill
  \parbox{.48\linewidth}{
    \caption{Performance of different multilevel configurations of the dense GP few-shot learner on the PASCAL- and COCO- benchmarks. Measured in mIoU (higher is better). Best results are in bold.}
    \vspace{-3mm}
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}{cclllll}
        \toprule
        && \multicolumn{2}{l}{PASCAL-} & \multicolumn{2}{l}{COCO-} \\
        Str. 16 & Str. 32 &   1-shot & 5-shot & 1-shot & 5-shot & Mean  \\
        \midrule
        & \checkmark& 62.5 & 72.6 & 44.7 & 55.0 & 0.0\\
        \checkmark & & 60.4 & 69.3 & 40.7 & 51.1 & -3.9\\
        \checkmark & \checkmark & \textbf{63.9} & \textbf{73.6} & \textbf{45.3} & \textbf{56.4} & \textbf{1.4}\\
        \bottomrule
    \end{tabular}}
    \label{tab:multiscale}
  }
  \vspace{-5mm}
\end{table}


\parsection{Incorporating Uncertainty and Learning the GP Output Space} We adopt the SE kernel and analyze the performance of letting the decoder process the predictive covariance provided by the dense GP. With the covariance in a  window, we obtain a gain of 1.7 mIoU. Then, we add the mask-encoder and learn the GP output space (GPO). This leads to a 1.7 improvement in isolation, or a 2.5 mIoU gain together with the covariance.

\parsection{Multilevel Representations}
Finally, we investigate the effect of introducing a multilevel hierarchy of dense GPs. Specifically we investigate using combinations of stride 16 and 32 features. The results are reported in Table~\ref{tab:multiscale}. The results show the benefit of using dense GPs at different feature levels, leading to an average gain of 1.4 mIoU.








