\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{EMNLP2022}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{colortbl}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\yangmin}[1]{\textcolor{red}{#1}}
\newcommand{\liyunshui}[1]{\textcolor{blue}{{#1}}}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}


\usepackage{stfloats}
\usepackage{adjustbox}
\DeclareUnicodeCharacter{2212}{-}

\setlength{\textfloatsep}{0.3\baselineskip plus 0.2\baselineskip minus 0.3\baselineskip}
\setlength{\intextsep}{2pt plus 2pt minus 2pt}
\setlength{\dbltextfloatsep}{2pt plus 2pt minus 2pt}

\setlength{\textfloatsep}{3pt plus 1pt minus 1pt}
\setlength{\intextsep}{3pt plus 1pt minus 1pt}
\setlength{\dbltextfloatsep}{3pt plus 1pt minus 1pt}
\setlength{\abovecaptionskip}{1pt}
\setlength{\belowcaptionskip}{1pt}

\makeatletter
\g@addto@macro\normalsize{\abovedisplayskip 1pt plus 1pt minus 1pt\belowdisplayskip \abovedisplayskip
  \abovedisplayshortskip 2pt plus1pt  minus1pt\belowdisplayshortskip 2pt plus1pt minus1pt}
\g@addto@macro\small{\abovedisplayskip 2pt plus 1pt minus 1pt\belowdisplayskip \abovedisplayskip
  \abovedisplayshortskip 2pt plus1pt  minus1pt\belowdisplayshortskip 2pt plus1pt minus1pt}
\g@addto@macro\footnotesize{\abovedisplayskip 2pt plus 1pt minus 1pt\belowdisplayskip \abovedisplayskip
  \abovedisplayshortskip 2pt plus1pt  minus1pt\belowdisplayshortskip 2pt plus1pt minus1pt}
\makeatother


\title{Self-Distillation with Meta Learning for Knowledge Graph Completion}
\author{Yunshui Li \quad  Junhao Liu  \quad Chengming Li\footnotemark[1] \quad Min Yang\footnotemark[1] \\
        Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences \\
        University of Chinese Academy of Sciences \\
        School of Intelligent Systems Engineering, Sun Yat-sen University\\
        \texttt{\{ys.li, jh.liu, min.yang\}@siat.ac.cn, lichengming@mail.sysu.edu.cn}
        }




\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding author}
\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}
In this paper, we propose a self-distillation framework with meta learning~(MetaSD) for knowledge graph completion with dynamic pruning, which aims to learn compressed graph embeddings and tackle the long-tail samples. Specifically, we first propose a dynamic pruning technique to obtain a small pruned model from a large source model, where the pruning mask of the pruned model could be updated adaptively per epoch after the model weights are updated. The pruned model is supposed to be more sensitive to difficult-to-memorize samples (e.g., long-tail samples) than the source model. 
Then, we propose a one-step meta self-distillation method for distilling comprehensive knowledge from the source model to the pruned model, where the two models co-evolve in a dynamic manner during training.
\fnsymbol{footnote}
In particular, we exploit the performance of the pruned model, which is trained alongside the source model in one iteration, to improve the source model's knowledge transfer ability for the next iteration via meta learning.
Extensive experiments show that MetaSD achieves competitive performance compared to strong baselines, while being 10x smaller than baselines. \footnote{The dataset and code are publicly available at \url{https://github.com/pldlgb/MetaSD}}
\end{abstract}

\section{Introduction}
Knowledge graphs (KG) have become the key technology to represent structural relations between entities and play an important role in question answering \cite{shen2018knowledge}, dialogue systems \cite{yan2017building}, and entity disambiguation \cite{mulang2020evaluating,si2022scl,si2023santa}.
However, most KGs are growing at a rapid pace and are far from complete. Therefore, it is necessary to develop knowledge graph completion (KGC) approaches to add missing triples to the KGs, so as to improve the quality of KGs. 

Recent advances in KGC primarily work on knowledge graph embedding (KGE) by converting the entities and relations in KGs into low-dimensional vectors. Early studies on KGE introduce a margin-based pairwise ranking function to measure the Euclidean distance or similarity between the relational projection of entities \cite{nickel2011three,bordes2013translating,yang2014embedding,trouillon2016complex}. Among them, TransE \cite{bordes2013translating} is the most widely used KGE method, which views the relation as translation from a head entity to a tail entity. 
Recently, neural networks, such as neural tensor network (NTN) \cite{socher2013reasoning} and neural association model (NAM) \cite{liu2016probabilistic} have been proposed to encode semantic matching and achieved remarkable predictive performance for KGC. 

To increase the capacity of the KGE models, a larger embedding size with more parameters is a common technique in practice. As shown in Figure~\ref{fig:fbdim}, the prediction performance of the KGC models such as DURA~\citep{zhang2020duality} and RP~\citep{chen2021relation} can be largely improved by increasing the graph embedding size. Although the large graph embedding often bring obvious performance improvements, it may also become the major obstacle for model deployment and real-time prediction, especially for memory-limited and resource-constrained devices.

\begin{figure}[t]
\centering \includegraphics[width = 0.35\textwidth]{fbdim.pdf}
\caption{The MRR scores w.r.t. the graph embedding sizes of DURA and RP on FB15k-237.}
\label{fig:fbdim}
\end{figure}

In addition, the distribution of samples with long-tail is prevalent in KGs \cite{zhang2019long}, where a large portion of relations have much fewer triples than other relations. However, most previous studies mainly focus on the predictive performance on overall test data, without taking long-tail samples into consideration. These models suffer from robust and generalization performance in the practical scenario. 
Although several recent works \cite{xiong2018one,sheng2020adaptive} have been proposed for few-shot KGC, these models are not adapted to the model compression frameworks.



In this paper, we propose a self-distillation framework with meta learning~(MetaSD) for knowledge graph completion with dynamic pruning. First, we propose a dynamic pruning technique to obtain a small pruned model from a large source model at the start of each training epoch. Concretely, the pruning mask of the pruned model could be updated adaptively per epoch after updating the model weights. The pruned model is supposed to be more sensitive to the difficult-to-memorize samples (e.g., long-tail samples) than the source model. 
Second, we propose a one-step meta self-distillation method to distill comprehensive knowledge from the source model to the pruned model, where the two models co-evolve in a dynamic manner during the whole knowledge distillation process.
The key idea is to use the performance of the pruned model, which is trained alongside the source model in one iteration, to improve the source model for the next iteration by borrowing the idea of learning to learn from meta learning \cite{finn2017model}. 
In particular, we define the objectives of the source model as functions of the pruned modelâ€™s performance on a quiz set. The usage of ``gradient by gradient'' strategy makes the source model adjust to the learning state of the pruned model, and improves both the source and pruned models.  

The main contributions of our method can be three-fold. 
(1) We propose a self-distillation framework to compress KG embeddings for KGC. The source and pruned models co-evolve in a dynamic manner during training, thus we can avoid pre-training a large model in advance, and the performance of the pruned model is not limited to that of a pre-trained large model.
(2) We exploit the feedback from the pruned model to guide the source model with meta learning, making the source model transfer better knowledge to the pruned network. 
(3)  Experimental results on two benchmark datasets show that our model achieves competitive performance compared to strong baselines, while being 10x smaller than other KGC models. 


















\begin{figure*}[]
\centering \includegraphics[width = 16cm]{pic3.pdf}
\caption{The overview of MetaSD framework. (a) We prune the teacher  to obtain the student  and perform knowledge distillation on training data to update the temporary copy  from . Then, the source model  is optimized based on the feedback of  on a held-out quiz set ; (b) We discard  and optimize  the meta-updated  and real  alternately by performing mutual learning on the training data.}
\label{fig:figure1}
\end{figure*}






\iffalse
\section{Related works}
Knowledge graph embedding techniques are used to simplify the data manipulation while preserving the inherent structure of the KG~\citep{wang2017knowledge}. Earlier works~\citep{bordes2013translating,sun2019rotate} build upon the distance hypothesis and train the models to predict the incomplete information. And others~\citep{hit1927,nickel2011three,yang2014embedding,trouillon2016complex} exploit similarity-based paradigm by matching latent semantics of entities and relations embodied in their vector space representations. \citet{zhang2020duality} further  proposes a duality-induced regularizer to combine approaches based on distance and similarity. \citet{chen2021relation} proposes to conduct relation prediction as an auxiliary training objective for improving knowledge representations and achieves start-of-the-art performances on KGC tasks.

Recently, some works~\citep{xiong2018one,sheng2020adaptive} has focused on the prevalent long-tail distribution in knowledge graph completion. Namely, there are a large portion of relations that only have a few triples. But the difference is that they shifted the problem as one/few shot KGC tasks. 

Although there are many methods for model compression, such as knowledge distillation~\citep{hinton2015distilling} and pruning~\citep{han2015deep}, to our best knowledge, we are the first to propose knowledge graph model compression and use pruning model with meta knowledge distillation to alleviate the long-tail distribution. 
\fi

\section{Methodology}
\paragraph{Problem Definition}
Suppose a KG can be viewed as a graph , where  and  represent the entity (node) set and relation (edge) set respectively.  represents a triple, where ,  and  indicate head entity, tail entity and the relation between two entities, respectively. Given the KG , the goal of KGC is to infer missing links
based on existing triples in the KG.

\paragraph{Model Overview}
The overview of our MetaSD method is illustrated in Figure~\ref{fig:figure1}. 
We adopt ComplEx~\citep{trouillon2016complex} as our backbone model, which is treated as the source model . First, we use a magnitude-based weight pruning method~\citep{han2015deep,zhu2017prune} to obtain a pruned model  from the source model . Second, we propose a one-step meta self-distillation method for distilling comprehensive knowledge from the source model  to the pruned model , where the two models co-evolve in a dynamic manner during training. Next, we introduce the proposed MetaSD method in detail. 

\subsection{Network Pruning}
We use the magnitude-based weight pruning method~\citep{han2015deep,zhu2017prune} to create a self-competitive compressed model  by pruning the source model . In particular, 
we fix the pruning rate  during the whole training process. At each iteration, we first calculate the sum of parameter numbers of all layers be pruned as , and sort all the weights by their absolute values. Then, we prune a certain fraction (i.e., ) of weights that have lowest absolute weight values. In particular, to dynamically adjust the pruned network  during each iteration, we prune the chosen weight by setting the corresponding values in a binary mask to zero, instead of directly setting the weights to zero. 

\subsection{Self-Distillation via Meta Learning}
We exploit the performance of the pruned model, which is trained alongside the source model in one iteration, to improve the source model's knowledge transfer ability for the next iteration via meta learning. 
In particular, we alternately update the pruned model  based on the output of the source model  and optimize the source model  based on the pruned model's performance via meta learning. 

\paragraph{Model  with Knowledge Distillation}
Formally, we use the function  to denote the soft prediction of the compressed model, where  represents the parameters of the pruned model . We calculate the cross-entropy loss  on the training data in current batch as:

where  denotes the number of training samples.  represents the cross-entropy function. 

To further improve the performance of , we also design a knowledge distillation loss   that encourages the output of  to mimic that of . In particular, we minimize the Kullback-Leibler Divergence (KL-divergence) between the output distributions of  and  by:

where  represents the parameters of  .

The cross-entropy loss  and the knowledge distillation loss  are combined to form the overall loss  for the compressed model  as:

where  is a hyperparameter to balance the relative importance of the two loss functions. 




\paragraph{Model  with Meta Learning}
We exploit feedback from the compressed model's learning
state to improve the source model's knowledge transfer ability throughout the distillation process, instead of keeping the source model  fixed in the training process. 
We train both  and  in an iterative manner until convergence. This interaction between the two models can be seen as a form of meta learning with a bi-level optimization process, which comprises three steps: \textbf{Virtual-Train}, \textbf{Meta-Train}, and \textbf{Actual-Train} \cite{xu2021faster}.
That is, the compressed model  is the inner-learner and the source model  is the meta-learner. 

For each training step,
we first copy the parameters  of the compressed model  to a ``virtual'' compressed model , and then update the parameters  of the ``virtual'' compressed model  with SGD \cite{bottou2012stochastic} for the \textbf{Virtual-Train} as:


Then, the source model  is optimized based on the feedback of  on a held-out quiz set . We perform a derivative over a derivative (a Hessian matrix) to update , by using a retained computational graph of  
in order to compute derivatives with respect
to . The source model  is optimized by minimizing the cross-entropy loss over the quiz set   for the \textbf{Meta-Train} as:

where  is the training samples in the quiz set .  and  denote the input sample and corresponding label in quiz set , respectively. 

Finally, we update the source model  with SGD \cite{bottou2012stochastic} as follows:

where  is the learning rate for the Meta-Train. 

\paragraph{Mutual Update of  and  for Self-Distillation} 
In our self-distillation framework, the source model  and the compressed model  co-evolve in a dynamic manner during the whole KD process. Instead of updating  with cross-entropy loss, we learn both  and  models mutually. 

Formally, for the \textbf{Actual-Train}, we first update the compressed model's parameters  with the training data and the updated parameters  as:


The source model  is also optimized by the combination of the cross-entropy loss  and the knowledge distillation loss  as:

\vspace{-0.4cm}

\vspace{-0.4cm}

where  is a hyperparameter to balance the relative importance of the two loss functions. We first update the source model's parameters  with the training data and the updated parameters  as:


We train the source and compressed models in an iterative manner until convergence. 
Overall, the self-distillation with meta learning is defined in Algorithm~\ref{alg1}.

\begin{algorithm}[H]
\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Self-Distillation with Meta Learning}
	\label{alg1}
	\begin{algorithmic}[1]
		\REQUIRE train set , quiz set , source model 
		\REQUIRE learning rate , learning rate , 
		\REPEAT
\STATE 
		\STATE Sample a batch of training data  from 
		\STATE Get pruned model  by pruning  
		\STATE Copy pruned model's parameters  to a\\
		~~``virtual'' pruned model 
\STATE Update  with  and : \# \textit{Virtual-Train} \\ \hspace{\algorithmicindent} 
		\STATE Sample a batch of quiz data  from 
		\STATE Update  with  and : \# \textit{Meta-Train} \\ \hspace{\algorithmicindent} 
		\STATE Mutual update  and : \# \textit{Actual-Train} \\
		    \hspace{\algorithmicindent}\\
		    \hspace{\algorithmicindent} 
		\UNTIL  == \textit{max iterations}
\ENSURE source model  and pruned model 
	\end{algorithmic}  
\end{algorithm}





















\iffalse
\begin{table}[t]
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{p{1.8cm}P{0.8cm}P{0.8cm}P{0.8cm}P{0.8cm}P{0.8cm}}
    \toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Ent}} & \multirow{2}{*}{\textbf{Rel}} & \multicolumn{3}{c}{\textbf{Triple Nums}} \\
& & &Train& Dev & Test \\
    \hline
    WN18RR & 40,943 & 11 & 86k & 3k & 3k\\
    FB15k-237 & 14,541 & 237 & 272k & 17k & 20k\\
    \bottomrule
\end{tabular}
}
\caption{\label{tab:dataset}
Statistics of the experimental datasets.
}
\end{table}
\fi
\begin{table*}
    \centering
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{p{2.0cm}P{1.4cm}P{1.4cm}P{1.4cm}P{1.4cm}P{1.4cm}P{1.4cm}P{1.4cm}P{1.4cm}P{1.4cm}P{1.4cm}P{1.4cm}}
\toprule
    \multirow{2}{*}{\textbf{Model}}& \multicolumn{5}{c}{\textbf{FB15k-237}}&
    \multicolumn{5}{c}{\textbf{WN18RR}} &\multirow{2}{*}{\textbf{Dim}}\\
    \cmidrule(lr){2-6}
    \cmidrule(lr){7-11}
    & MRR & Hits@1 & Hits@3 & Hits@10 & \textbf{Size} & MRR & Hits@1 & Hits@3 & Hits@10 & \textbf{Size} & \\
    \hline
    \hline
    TransE  & 0.313&0.221&0.347&0.497&- & 0.228& 0.053& 0.368&0.520 &-&-\\ RotatE  & 0.333&0.240&0.368&0.522 &- & 0.478& 0.439& 0.494& 0.553 &-&- \\CP & 0.333 & 0.247& 0.360& 0.508 &50M &0.438 &0.414 &0.444& 0.485 &156M&2k\\
    RESCAL  & 0.353&0.264&0.383&0.528 &125M & 0.455& 0.419&0.460 &0.493 &26M&-\\ComplEx  &0.346&0.256 & 0.370 &0.525 &60M &0.460 &0.428 &0.475 &0.522 &156M&2k \\DURA  & 0.371 & 0.276 & 0.408 & 0.560 & 60M  & \textbf{0.491} & \textbf{0.449}  &0.503&\textbf{0.571}&156M&2k\\RP &0.388&0.298&0.425&0.568&60M &0.488&0.443&\textbf{0.505}&0.568&156M&2k \\KD &0.371&0.282&0.408&0.550&6M &0.470&0.427&0.485&0.530&15M&0.2k\\
    DML &0.373&0.280&0.410&0.563&6M &0.472&0.429&0.485&0.535&15M&0.2k\\
    \rowcolor{gray!20}MetaSD & \textbf{0.391}&\textbf{0.300}&\textbf{0.428}&\textbf{0.571} &6M&\textbf{0.491}&\underline{0.447}&\underline{0.504}&\underline{0.570} &15M&0.2k\\
\bottomrule
    \end{tabular} }
    \caption{\label{benchmark}
    Experimental results on FB15k-237 and WN18RR test sets for KGC. The results with  are taken from LibKGE~\citep{libkge}. CP, ComplEx and RESCAL are implemented by following~\citep{zhang2020duality}. }
\end{table*}


\section{Experimental Setup}
\subsection{Datasets}
We conduct experiments on two KGC benchmark datasets: WN18RR~\citep{toutanova2015observed} and FB15k-237~\citep{dettmers2018convolutional}. WN18RR consists of 40,943 entities and 11 relations, and there are 86k/3k/3k instances for training/validation/testing respectively. FB15k-237 contains 14,541 entities and 237 relations, and there are 272k/17k/20k instances for training/validation/testing.





\subsection{Baseline Methods}
We compare  MetaSD with several strong KGC baselines, including CP~\citep{hit1927},  RESCAL~\citep{nickel2011three}, TransE~\citep{bordes2013translating}, ComplEx~\citep{trouillon2016complex},
RotatE~\citep{sun2019rotate},
DURA~\citep{zhang2020duality}, and RP~\citep{chen2021relation}.  We also compare MetaSD with two widely used KD methods: knowledge distillation (KD)~\citep{hinton2015distilling} and deep mutual learning (DML)~\citep{zhang2018deep}, where the pre-trained RP~\citep{chen2021relation} is used as their teacher model. 

\subsection{Implementation Details}
The source model of MetaSD is initialized with ComplEx~\citep{trouillon2016complex}, following the previous work \citep{zhang2020duality}. Similar to~\citet{chen2021relation}, we add relation prediction as an auxiliary task. We set the pruning rate  to 0.9 to strike a balance between the effectiveness and efficiency of the model.
We set balance hyperparameters . We choose Adagrad~\citep{duchi2011adaptive} as the optimizer and the learning rate  to  and  to . The quiz set is randomly sampled from training data and then fixed. We adopt widely used \textit{filtered} evaluation metrics of mean reciprocal rank~(MRR), Hits@1, Hits@3, and Hits@10 as described in~\cite{bordes2013translating}.

\section{Experimental Results}
\subsection{Overall Performance}
As shown in Table~\ref{benchmark}, we report the results of the compressed model (denoted as MetaSD). Note that the parameters of RASCAL are proportional to the square of the number of relations, resulting in large differences in size between the two datasets. We observe that MetaSD achieves competitive performance compared to other high-dimensional baseline models on the two datasets for KGC, while being 10x smaller than baseline methods. 
In addition, MetaSD also outperforms than two widely used KD methods that have the same size and dimension with MetaSD.

\subsection{Long-tail Evaluation}
We investigate the effectiveness of MetaSD for dealing with the long-tail samples. In particular, we collect the long-tail samples from the FB15k-237 test set by choosing the relations that have fewer than 1000 training instances. In total, there are 187 relations, which accounts for 79\% of the total relation types but only 24\% of train set.  Table~\ref{longtail} reports the results of MetaSD and compared methods on the long-tail set. MetaSD significantly outperforms other models on the long-tail samples, which verifies the effectiveness of MetaSD in tackling the long-tail samples. 
\begin{table}
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{p{1.8cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}}
    \toprule
\textbf{Model} & \textbf{MRR} & \textbf{H@1} & \textbf{H@3} & \textbf{H@10} \\
    \hline\hline
    DURA & 0.452 & 0.354 & 0.498 & 0.644\\
    RP & 0.462& 0.372& 0.504& 0.645 \\
\rowcolor{gray!20}MetaSD- & 0.468 & 0.380 & 0.510 & 0.642\\
    \rowcolor{gray!20}MetaSD & \textbf{0.471} & \textbf{0.381} & \textbf{0.512} & \textbf{0.646} \\
    \bottomrule
\end{tabular}
}
\caption{\label{longtail}
Results on long-tail data from FB15k-237.
}
\end{table}








\begin{table}
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{p{1.8cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}}
    \toprule
\textbf{Model} & \textbf{MRR} & \textbf{H@1} & \textbf{H@3} & \textbf{H@10} \\
    \hline\hline
    \textbf{MetaSD} & \textbf{0.391} & \textbf{0.300} & \textbf{0.428} & \textbf{0.571}\\
    \ \ \  w/o P & 0.381& 0.292& 0.415& 0.561 \\
    \ \ \ w/o M & 0.378& 0.287&0.412& 0.555\\
    \ \ \ w/o P\&M & 0.373 & 0.280 & 0.410 &0.563\\
\bottomrule
\end{tabular}
}
\caption{\label{ablation}
Results of ablation study on FB15k-237.  and  denote the pruning and meta learning techniques.
}
\end{table}
\subsection{Ablation Study}
In order to verify the effectiveness of the pruning and meta learning modules, we conduct ablation evaluation on the proposed MetaSD on the FB15k-237 dataset. As shown in Table~\ref{ablation}, we can observe that the meta learning technique has great impact on the performance of the proposed MetaSD. This is because that meta learning can make the source network transfer rich knowledge to the pruned network effectively in the self-distillation process. In addition, the improvement of the self-pruning strategy is also significant since self-pruning can help the model learn discriminative representations and deal with the long-tail samples.  It is no surprise that combining both factors achieves the best performance on in terms of four evaluation metrics.



\subsection{Generalization}
To demonstrate the robustness of our framework, we also implement MetaSD on two additional backbone models (e.g., CP and RESCAL).
These two backbone models are implemented and initialized by following the paper~\citep{zhang2020duality}.
As shown in Table~\ref{generation}, our compressed model achieves substantially better performance than the larger baseline models based on two different backbone models.

\begin{table}
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{p{3.2cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}}
    \toprule
\textbf{Model} & \textbf{MRR} & \textbf{H@1} & \textbf{H@3} & \textbf{H@10}&\textbf{Size} \\
    \hline\hline
    CP &0.333&0.247&0.360&0.508& 50M\\
    RESCAL & 0.353 & 0.264 & 0.383 &0.528 & 125M\\
    \rowcolor{gray!20}\textbf{MetaSD-CP} &0.367 & 0.270&0.396&0.557& 5M\\
    \rowcolor{gray!20}\textbf{MetaSD-RESCAL} &0.372&0.276&0.405&0.561 & 12.5M\\
    
\bottomrule
\end{tabular}
}
\caption{\label{generation}
Results of MetaSD on FB15k-237 by using different backbone models.
}
\end{table}
\section{Conclusion}
In this paper, we proposed a self-distillation framework with meta learning for graph embedding compression. Concretely, we proposed a one-step meta self-distillation method for distilling comprehensive knowledge from the source model to the pruned model, where the two models co-evolved in a dynamic manner during training. Experimental results showed that our model achieved competitive performance compared to strong baseline methods, while being 10x smaller than baseline methods. 

\section*{Acknowledgements}
This work was partially supported by National Key R\&D Program of China (No. 2019YFB2102500), National Natural Science Foundation of China (No. 61906185), Youth Innovation Promotion Association of CAS China (No. 2020357), Shenzhen Science and Technology Innovation Program (Grant No. KQTD20190929172835662), Shenzhen Basic Research Foundation (No. JCYJ20210324115614039 and No. JCYJ20200109113441941). 




\section*{Limitations}
To better understand the limitations of the proposed model, we carry out an analysis of the error predictions made by MetaSD. In particular, we primarily analyze the relations in the FB15k-237 test set, whose MRR scores are less than 0.2. Most of the incorrectly predicted relations are the ``location'' and ``relationships'' related relation types, such as place of birth/death, spouse, and sibling. We reveal several reasons of the bad cases, which can be divided into two primarily categories. First, MetaSD fails to predict some instances that require the multi-hop reasoning to get the correct answers, since our model does not consider the complex multi-hop paths during the knowledge graph representation learning. Second, MetaSD fails to predict some instances, where there are a large number of candidate entities to reason for a relation type (e.g., the location relation). One possible solution is to devise a two-step ranking method by filtering most of the irrelevant entities in a coarse-grained way and then distinguish the confusing entities with a fine-grained method. 



\iffalse
\begin{table*}[!htbp]
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{cccc}
    \toprule
\texbf{Type} &\textbf{Relation} & \textbf{MRR} & \textbf{Nums} \\
    \hline\hline
    \multirow{7}{*}{\textbf{loaction}} \\
    &place lived  & 0.150  &305 \\
    &place of birth & 0.139  & 170 \\
    &place of death & 0.196 &78 \\
    &place founded & 0.185 &16 \\
    &film locations & 0.184 &100 \\
    &ceremony locations  & 0.091 & 35\\
    &release region & 0.135& 9 \\
    \hline


\bottomrule
\end{tabular}
}
\caption{\label{limitations}
Experimental results on relations of error predictions in FB15k-237 evaluation.  
}
\end{table*}
\fi

\bibliography{anthology}\bibliographystyle{acl_natbib}







\end{document}
