\section{Experiments}
\label{sec:experiments}

We evaluate our model on YouTubeVIS-2019~\cite{yang2019video}, YouTubeVIS-2021~\cite{youtubevis2021}, and OVIS\cite{qi2022occluded} datasets.
YouTubeVIS-2019 is a collection of short videos (up to 36 frames) with segmented objects from 40 classes.
YouTubeVIS-2021 extends its predecessor dataset with more videos, improved categories, and doubles the number of annotations. It also introduces longer videos of up to 84 frames.
OVIS is a more challenging dataset because it contains short and long videos (from 15 to 292 frames) and 25 classes. Its objects are frequently severely occluded and in crowded scenes.

The models are evaluated using the regular validation subsets on the official evaluation servers of their respective datasets. For the YoutubeVIS-2019/2021 and OVIS datasets, we report our model's mean average precision (AP) and mean average recall (AR) over all classes. We also report AP measured at  and  IOU thresholds as  and  respectively.


\subsection{Implementation Details}
\label{sec:build_model}

For the frame-level module, most experiments are performed on our own implementation of MaskDINO~\cite{li2022maskdino} by extending the official DINO detection implementation~\cite{zhang2022dino}. We generally find using MaskDINO as our frame-level module achieves the best segmentation accuracy in our experiments. The structure of the MaskDINO is largely retained, including its class, box, mask heads, and regular CDN. The weights for the classification and mask heads are shared with the corresponding heads in the temporal refinement module because we find that sharing weights between these two modules leads to better accuracy.





During training, the association algorithm uses a lower threshold of IoU and prediction confidence so that the training gets sufficient supervision signals, particularly at the beginning of training when the predictions are noisy.

Our model is trained with both ResNet-50 and Swin-L backbones. We use NVIDIA V-100 GPUs to train and evaluate our ResNet-50 models and A-100 GPUs for our Swin-L models. Batch sizes of 16 clips are used to train models with both types of backbones.



Our training schedule contains three phases: pretraining with MS-COCO images, pretraining with MS-COCO pseudo videos, and training with VIS clips. We first pre-train our image-level model on the MS-COCO 2017 dataset \cite{mscoco} for detection and segmentation following a standard image instance segmentation training procedure.
Next, we create pseudo videos from MS-COCO images as other recent works~\cite{IDOL,heo2022vita,wu2021seqformer} and pre-train the whole RefineVIS model for 30k iterations. 
Finally, we train the model on the target VIS dataset.

In both video pretraining and VIS training, we randomly sample two frames from the sequence that are at most 10 frames apart.
We use both frames to train our association and TAR modules but only compute the image-level losses on the first frame.
The training signals from the association and TAR modules still back-propagate to the image-level models in both frames.
We find this setup leads to more stable training empirically.
For both online and offline inference, the TAR module utilizes an attention window of 10 frames unless otherwise specified.
One-to-many Optimal Transport Assignment (OTA) is utilized instead of the traditional one-to-one Hungarian matcher for ground-truth matching for frame-level losses.

We use large-scale jittering (LSJ) for both VIS training and video pretraining. On VIS training, the same LSJ is applied to both frames preserving the consistency between objects across frames. On MS-COCO pseudo video pretraining, the same LSJ operation is applied independently over the two training frames, potentially leading to two very different-looking frames, which is desirable for pseudo videos. 

In video pretraining and VIS training phases, We train our model with resolutions up to 1024p for models with Resnet-50 backbones and resolutions up to 1280p for Swin-L backbones. When training models with Swin-L backbones, the number of feature levels of the backbone for the decoder is increased from 4 to 5, and the number of decoder transformer layers is increased from 6 to 9, which was shown to improve performance in MaskDINO.
The number of CDN queries is 100 for Resnet-50 models and 500 for Swin-L models.

MS-COCO image segmentation pretraining follows the settings of MaskDINO.
For the other two phases, the initial learning rate is set to  for the backbone and  for the rest of the network. 
For MS-COCO pseudo videos, the model is trained for 30k steps, with the learning rate being decreased to a tenth after 15k steps and again after 25k steps.
For the VIS training phase on all datasets, the model is trained for 13k iterations, with the learning rate being decreased to a tenth at the 10k iteration.


\subsection{Main results}

\begin{table*}[t]
	\small
	\centering
	\footnotesize
    \setlength{\tabcolsep}{5pt}
	\begin{tabular}{l|l|ccccc|ccccc|ccccc}
		\toprule
		\multicolumn{2}{l|}{\multirow{2}{*}{Method}} & \multicolumn{5}{c|}{YouTubeVIS-2019} & \multicolumn{5}{c|}{YouTubeVIS-2021} & \multicolumn{5}{c}{OVIS} \\
		\multicolumn{2}{l|}{} & AP &  &  &  &  & AP &  &  &  &  & AP &  &  &  & \\
		\midrule
		\parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{ResNet-50}}}
& SeqFormer~\cite{wu2021seqformer} & 47.4 & 69.8 & 51.8 & 45.5 & 54.8 & 40.5 & 62.4 & 43.7  & 36.1 & 48.1 & 15.1 & 31.9 & 13.8 & 10.4 & 27.1 \\
		& MinVIS~\cite{huang2022minvis} & 47.4 & 69.0 & 52.1 & 45.7 & 55.7 & 44.2 & 66.0 & 48.1 & 39.2 & 51.7 & 25.0 & 45.5 & 24.0 & 13.9 & 29.7 \\
		& VITA~\cite{heo2022vita} & 49.8 & 72.6 & 54.5 & \textbf{49.4} & \textbf{61.0} & 45.7 & 67.4 & 49.5 & 40.9 & 53.6 & 19.6 & 41.2 & 17.4 & 11.7 & 26.0 \\
		& IDOL~\cite{IDOL} & 49.5 & 74.0 & 52.9 & 47.7 & 58.7 & 43.9 & 68.0 & 49.6 & 38.0 & 50.9 & 30.2 & 51.3  & 30.0  &15.0 & 37.5 \\
& \textbf{RefineVIS\textsubscript{online}} & 49.4 & 74.4 & 53.8 & 46.0 & 56.4 & 48.9 & 72.6 & 54.0 & \textbf{54.7} & 54.7 & 33.4 & \textbf{57.1} & 32.6 & \textbf{16.3} & \textbf{40.2} \\
		& \textbf{RefineVIS\textsubscript{offline}} & \textbf{52.2} & \textbf{76.3} & \textbf{57.7} & 47.5 & 57.6 & \textbf{50.2} & \textbf{72.8} & \textbf{55.4} & 41.2 & \textbf{56.3} & \textbf{33.7} & 56.2 & \textbf{34.8} & 15.6 & 39.8 \\
		
        \midrule
		\parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{Swin-L}}}
		& SeqFormer~\cite{wu2021seqformer} & 59.4 & 82.1 & 66.4 & 51.7 & 64.4 & 51.8 & 74.6 & 58.2 & 42.8 & 58.1  & 15.1 & 31.9 &13.8 & 10.4 & 27.1 \\
        & MinVIS~\cite{huang2022minvis} & 61.6 & 83.3 & 68.6 & 54.8 & 66.6 & 55.3 & 76.6 & 62.0 & 45.9 & 60.8 & 39.4 & 61.5 & 41.3 & 18.1 & 43.3 \\
		& VITA~\cite{heo2022vita} & 63.0  & 86.9  & 67.9 & \textbf{56.3}  & 68.1 & 57.5 & 80.6 & 61.0 & 47.7 & 62.6 & 27.7 & 51.9 & 24.9 & 14.9 & 33.0\\
        & IDOL~\cite{IDOL} & 64.3 &  87.5 & 71.0 & 55.6 & \textbf{69.1} & 56.1 & 80.8 & 63.5 & 45.0 & 60.1 & 42.6 & 65.7 &  45.2 & 17.9  & 49.6 \\
& \textbf{RefineVIS\textsubscript{online}} & 64.3 & 87.6 & 70.9 & 55.8 & 68.2 & \textbf{61.4} & \textbf{84.1} & 68.5 & \textbf{48.3} & \textbf{65.2} & \textbf{46.1} & 69.7 & 47.8 & 19.0 & 50.8 \\
		& \textbf{RefineVIS\textsubscript{offline}} & \textbf{64.4} & \textbf{88.3} & \textbf{72.2} & 55.8 & 68.4 & 61.2 & 83.7 & \textbf{69.2} & 47.9 & 64.8 & 46.0 & \textbf{70.4} & \textbf{48.4} & \textbf{19.1} & \textbf{51.2} \\
		\bottomrule
	\end{tabular}
	\caption{Evaluation results on \textbf{YouTubeVIS-2019}, \textbf{YouTubeVIS-2021} and \textbf{OVIS} validation splits.}
	\label{tab:ytvis2021}
\end{table*}

For inference, the smaller size of the input frames is resized to 480p if it is a ResNet-50 model, 720p if Swin-L.
In Table \ref{sec:experiments}, we compare our model, both in online and offline mode, with other recent works on the three datasets.
Unless otherwise noted, all online and offline inference results used a refinement window of 10 frames, and all models were first pretrained with both regular MS-COCO and pseudo videos.

Across all the datasets, models with ResNet-50 backbone benefit from offline inference compared to online inference, while models with Swin-L backbone have a similar result.
The reason is that the frame-level predictions are noisier when a weaker backbone is utilized. The offline inference can use the information from the frames ahead of the current frame to reduce some noise. 


\textbf{YouTubeVIS-2019.} This is the easiest VIS dataset because it only contains short videos. The online variant of our model performs similarly to IDOL, because most of the masks in this dataset can be predicted accurately using frame-level models. However, our offline variant performs better than both IDOL and VITA on these datasets.


\textbf{YoutubeVIS-2021.} On this more extensive dataset, we see a more pronounced advantage of our RefineVIS compared to other models. The online variant delivers 3.2 and 3.9 and extra AP points than the previous best method with ResNet-50 and Swin-L backbones, respectively. Our offline variant brings 4.5 and 3.9 extra AP points than VITA, the previous best method, with ResNet-50 and Swin-L backbones, respectively.

\textbf{OVIS} is the most challenging dataset in the benchmark. This dataset also shows a clear advantage of the proposed framework.
When compared with IDOL, the previous leader, our scores are better by 3.5 AP points with both ResNet-50 and Swin-L.
We also notice that our online and offline methods perform similarly on this dataset.




\subsection{Ablation studies}

We focus our ablation studies on the OVIS and on the YouTube-2021 datasets, providing experimental results that consistently show the importance of each element that we introduce and how they provide state-of-the-art VIS results.


\textbf{Video Contrastive Denoising}. Table \ref{tab:ablation_denoising} shows the incremental gains of using the new CDN applications over OVIS models that were not pretrained with MS-COCO pseudo videos.
In the first row, we see the results of only using CDN at the frame level, like the standard MaskDINO.
We can see a gain of  AP after applying temporal refinement.
In the second row, we see the gains from just introducing Association CDN, which benefits both frame-level and refinement parts of our model and improves the frame-level AP by  and the refined AP by .
Again we see a gain in using refinement:  AP.
Finally, we see a gain of  AP by introducing Temporal CDN and improving refined mask predictions.
Temporal CDN does not affect frame-level results.

\begin{table}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{l|ccc|ccc}
        \toprule
        \multirow{2}{*}{Training mode} & \multicolumn{3}{c|}{Frame-level results} & \multicolumn{3}{c}{Refined results} \\
        & AP &  &  & AP &  &  \\
		\midrule
        Frame CDN         & 30.1 & 51.3 & 29.6 & 31.0 & 53.1 & 29.3 \\
        + Association CDN & 31.2 & 52.0 & 30.7 & 31.8 & 54.2 & 30.8 \\
        + Temporal CDN    & -    & -    & -    & 32.3 & 55.5 & 31.7 \\
    \bottomrule
    \end{tabular}
    \caption{Cumulative contributions of the CDN technique to both frame-level and video-level parts of the model. Offline results on the OVIS dataset of ResNet-50 models without MS-COCO pseudo video pretraining. Frame CDN is the original used by MaskDINO.}
    \label{tab:ablation_denoising}
\end{table}


\textbf{Temporal Attention Window}. We evaluate the choice of , the refinement window length, in Table \ref{tab:ablation_attn_win}. 
In this experiment, we experiment with different refinement window sizes from 2 to 20. A larger window size leads to better AP until a window size of 10. Using windows that are too small or too large decreases the AP.
Small windows add noise to the predictions, while large ones will cause object appearance to change too much inside the refinement window.
  
\begin{table}
    \centering 
    \footnotesize 
    \begin{tabular}{c|c|cccccc} 
        \toprule 
         & 0 & 2 & 4 & 8 & 10 & 16 & 20 \\ 
        \midrule 
        AP & 32.7 & 32.1 & 32.9 & 33.5 & 33.7 & 33.3 & 32.9 \\ 
    \bottomrule 
    \end{tabular} 
    \caption{OVIS offline results with a ResNet-50 showing how different values of  (the length of the window consumed by TAR) impact the overall performance.  means refinement is disabled.} 
    \label{tab:ablation_attn_win} 
\end{table} 


\textbf{Training with more frames.} Our study into the effects of training RefineVIS with additional frames is presented in Table \ref{tab:nframes_train}.
Unfortunately, using more frames leads to a decrease in AP, which our investigation suggests comes from instability in the training loss.
Another challenge from using more frames during training is the considerable increase in memory usage by the CDN technique, which requires accommodating hundreds of extra transformer queries.
Addressing these challenges remains an open research topic.

\begin{table}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{c|l|ccccc}
        \toprule
        \# of frames & AP &  &  &  &  \\
        \midrule
        2 & 50.2 & 72.8 & 55.4 & 41.2 & 56.3 \\
        3 & 49.9 & 72.8 & 55.5 & 41.2 & 55.3 \\
        5 & 49.4 & 72.1 & 55.0 & 41.1 & 55.5 \\
    \bottomrule
    \end{tabular}
    \caption{Effect of using different number of frames during training. Results from a ResNet-50 backbone on YouTubeVIS-2021.}
    \label{tab:nframes_train}
\end{table}


\textbf{Temporal Refinement Module}. Tables \ref{tab:ablation_denoising} and \ref{tab:ablation_attn_win} show temporal refinement module consistently delivers better results than the baseline frame-level model under different training schemes.
Table \ref{tab:ablation_ovis_swinl} shows another ablation study that highlights the impact of TAR on YouTubeVIS-2021.
Results are from RefineVIS with a ResNet-50 backbone both with and without TAR.
The same table also measures the impact of Large Scale Jittering (LSJ) to the training process and how TAR and LSJ synergistically contribute to the performance of RefineVIS.

\begin{table}
    \centering
    \footnotesize
    \begin{tabular}{cc|ccccc}
        \toprule
        TAR & LSJ & AP &  &  &  &  \\
        \midrule
        &                       & 48.1 & 70.5 & 53.0 & 39.3 & 53.2 \\
        & \checkmark            & 48.9 & 73.0 & 53.1 & 40.7 & 54.9 \\
        \checkmark &            & 49.1 & 71.9 & 54.1 & 39.8 & 54.3 \\
        \checkmark & \checkmark & 50.2 & 72.8 & 55.4 & 41.2 & 56.3 \\


    \bottomrule
    \end{tabular}
    \caption{YouTubeVIS-2021 offline results with a ResNet-50 backbone highlight the impact of TAR and LSJ.}
    \label{tab:ablation_ovis_swinl}
\end{table}


\textbf{Inference speed.} Table \ref{tab:ablation_fps} compares the speed of our ResNet-50 model, with and without TAR.
Numbers are the average FPS for the models to process the entire YouTubeVIS-2021 dataset on a NVIDIA V-100 GPU.
Regardless of the backbone, TAR adds only 2.8 MFLOPS per 10-frame window, which represents a minor impact.

\begin{table}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{c|cc}
        \toprule
        TAR & ResNet-50 &  Swin-L \\
        \midrule
& 22.27 & 4.63 \\
        \checkmark & 21.91 & 4.60 \\
    \bottomrule
    \end{tabular}
    \caption{FPS of different configurations on a V-100 GPU}
    \label{tab:ablation_fps}
\end{table}

\textbf{RefineVIS as a framework.} To further demonstrate the modularity of RefineVIS, we conduct studies with two other segmentation models.
The first uses Mask2Former \cite{cheng2021mask2former} and the second a CondInst \cite{tian2020conditional} mask head with Deformable DETR transformer \cite{zhu2020deformable}, similar to the one in IDOL.
We begin with pre-trained versions of these segmentation models and finetune on YouTubeVIS-2021 under the RefineVIS framework.
Results are shown in Table \ref{tab:framework} and demonstrate how much our framework can enhance these spatial-only segmentation models.

\begin{table}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{l|c|cccccc}
        \toprule
        Segm. model & TAR & AP &  &  &  &  \\
        \midrule
        \multirow{2}{*}{Mask2Former} & & 44.0 & 67.4 & 47.4 & 38.9 & 51.5 \\
                          & \checkmark & 45.2 & 69.3 & 49.8 & 39.5 & 52.6 \\
        \midrule
        \multirow{2}{*}{CondInst} & & 47.8 & 71.4 & 52.9 & 41.2 & 55.6 \\
                       & \checkmark & 48.3 & 72.4 & 53.9 & 41.4 & 55.3 \\
    \end{tabular}
    \caption{Results of other segmentation models trained under the RefineVIS framework show TAR is effective and the framework is extensible.}
    \label{tab:framework}
\end{table}
