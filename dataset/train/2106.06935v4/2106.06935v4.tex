\appendix

\section{Path Formulations for Traditional Methods}
\label{app:path_formulation}

Here we demonstrate our path formulation is capable of modeling traditional link prediction methods like Katz index~\cite{katz1953new}, personalized PageRank~\cite{page1999pagerank} and graph distance~\cite{liben2007link}, as well as graph theory algorithms like widest path~\cite{baras2010path} and most reliable path~\cite{baras2010path}.

Recall the path formulation is defined as

which can be written in the following compact form


\subsection{Katz Index}
The Katz index for a pair of nodes ,  is defined as a weighted count of paths between  and , penalized by an attenuation factor . Formally, it can be written as

where  denotes the adjacency matrix and ,  denote the one-hot vector for nodes ,  respectively. The term  counts all paths of length  between , and  and shorter paths are assigned with larger weights.

\katz*
\begin{proof}
We show that  can be transformed into a summation over all paths between  and , where each path is represented by a product of damped edge weights in the path.
Mathematically, it can be derived as

Therefore, the Katz index can be viewed as a path formulation with the \emph{summation} operator , the \emph{multiplication} operator  and the edge representations .
\end{proof}

\subsection{Personalized PageRank}
The personalized PageRank (PPR) for  computes the stationary distribution over nodes generated by an infinite random walker, where the walker moves to a neighbor node with probability  and returns to the source node  with probability  at each step. The probability of a node  from a source node  has the following closed-form solution~\cite{jeh2003scaling}

where  is the degree matrix and  is the (random walk) normalized adjacency matrix.
Note that  computes the probability of -step random walks from  to .

\pagerank*
\begin{proof}
We omit the coefficient , since it is always positive and has no effect on the ranking of different node pairs.
Then we have 

where the \emph{summation} operator is , the \emph{multiplication} operator is  and edge representations are random walk probabilities scaled by .
\end{proof}

\subsection{Graph Distance}
Graph distance (GD) is defined as the minimum length of all paths between  and .

\distance*
\begin{proof}
Since the length of a path is the sum of edge lengths in the path, we have

Here the \emph{summation} operator is , the \emph{multiplication} operator is  and the edge representations are the lengths of edges.
\end{proof}

\subsection{Widest Path}
The widest path (WP), also known as the maximum capacity path, is aimed at finding a path between two given nodes, such that the path maximizes the minimum edge weight in the path.

\widest*
\begin{proof}
Given two nodes  and , we can write the widest path as

Here the \emph{summation} operator is , the \emph{multiplication} operator is  and the edge representations are plain edge weights.
\end{proof}

\subsection{Most Reliable Path}
For a graph with non-negative edge probabilities, the most reliable path (MRP) is the path with maximal probability from a start node to an end node. This is also known as Viterbi algorithm~\cite{viterbi1967error} used in the maximum a posterior (MAP) inference of hidden Markov models (HMM).

\reliable*
\begin{proof}
For a start node  and an end node , the probaility of their most reliable path is 

Here the \emph{summation} operator is , the \emph{multiplication} operator is  and the edge representations are edge probabilities.
\end{proof}

\section{Generalized Bellman-Ford Algorithm}
\label{app:bellman_ford}

First, we prove that the path formulation can be efficiently solved by the generalized Bellman-Ford algorithm when the operators  satisfy a semiring.
Then, we show that traditional methods satisfy the semiring assumption and therefore can be solved by the generalized Bellman-Ford algorithm.

\subsection{Preliminaries on Semirings}

Semirings are algebraic structures with two operators, \emph{summation}  and \emph{multiplication} , that share similar properties with the natural summation and the natural multiplication defined on integers. Specifically,  should be commutative, associative and have an identity element .  should be associative and have an identity element . Mathematically, the \emph{summation}  satisfies
\begin{itemize}
    \setlength{\parskip}{0pt}
    \setlength{\itemsep}{0pt plus 1pt}
    \item \textbf{Commutative Property.} 
    \item \textbf{Associative Property.} 
    \item \textbf{Identity Element.} 
\end{itemize}
The \emph{multiplication}  satisfies
\begin{itemize}
    \setlength{\parskip}{0pt}
    \setlength{\itemsep}{0pt plus 1pt}
    \item \textbf{Associative Property.} 
    \item \textbf{Absorption Property.} 
    \item \textbf{Identity Element.} 
\end{itemize}
Additionally,  should be distributive over .
\begin{itemize}
    \setlength{\parskip}{0pt}
    \setlength{\itemsep}{0pt plus 1pt}
    \item \textbf{Distributive Property (Left).} 
    \item \textbf{Distributive Property (Right).} 
\end{itemize}
Note semirings differ from natural arithmetic operators in two aspects. First, the \emph{summation} operator  does not need to be invertible, e.g., min or max. Second, the \emph{multiplication} operator  does not need to be commutative nor invertible, e.g., matrix multiplication.

\subsection{Generalized Bellman-Ford Algorithm for Path Formulation}
Now we prove that the generalized Bellman-Ford algorithm computes the path formulation when the operators  satisfy a semiring. It should be stressed that the generalized Bellman-Ford algorithm for path problems has been proved in~\cite{baras2010path}, and not a contribution of this paper. Here we apply the proof to our proposed path formulation.

The generalized Bellman-Ford algorithm computes the following iterations for all 

\begin{lemma}\label{lem:induction}
After  Bellman-Ford iterations, the intermediate representation  aggregates all path representations within a length of  edges for all . That is

\end{lemma}
\begin{proof}
We prove Lemma~\ref{lem:induction} by induction. For the base case , there is a single path of length  from  to itself and no path to other nodes. Due to the product definition of path representations, a path of length  is equal to the \emph{multiplication} identity . Similarly, a summation of no path is equal to the \emph{summation} identity . Therefore, we have .

For the inductive case , we consider the second-to-last node  in each path if the path has a length larger than . To avoid overuse of brackets, we use the convention that  and  have a higher priority than  and . 

where Equation~\ref{eq:substitution} substitutes the inductive assumption for , Equation~\ref{eq:distributive} uses the distributive property of  over .
\end{proof}

By comparing Lemma~\ref{lem:induction} and Equation~\ref{eqn:compact}, we can see the intermediate representation converges to our path formulation . More specifically, at most  iterations are required if we only consider simple paths, i.e., paths without repeating nodes. In practice, for link prediction we find it only takes a very small number of iterations (e.g., ) to converge, since long paths make negligible contribution to the task.

\subsection{Traditional Methods}

\bellman*
\begin{proof}
Given that the generalized Bellman-Ford algorithm solves the path formulation when  satisfy a semiring, we only need to show that the operators of the path formulations for traditional methods satisfy semiring structures.

Katz index (Theorem~\ref{thm:katz_index}) and personalized PageRank (Theorem~\ref{thm:pagerank}) use the natural summation  and the natural multiplication , which obviously satisfy a semiring.

Graph distance (Theorem~\ref{thm:proximity}) uses  for \emph{summation} and  for \emph{multiplication}. The corresponding identities are  and . It is obvious that  satisfies the associative property and has identity element .
\begin{itemize}
    \setlength{\parskip}{0pt}
    \setlength{\itemsep}{0pt plus 1pt}
    \item \textbf{Commutative Property.} 
    \item \textbf{Associative Property.} 
    \item \textbf{Identity Element.} 
    \item \textbf{Absorption Property.} 
    \item \textbf{Distributive Property (Left).} 
    \item \textbf{Distributive Property (Right).} 
\end{itemize}

Widest path (Theorem~\ref{thm:widest_path}) uses  for \emph{summation} and  for \emph{multiplication}. The corresponding identities are  and . We have
\begin{itemize}
    \setlength{\parskip}{0pt}
    \setlength{\itemsep}{0pt plus 1pt}
    \item \textbf{Commutative Property.} 
    \item \textbf{Associative Property.} 
    \item \textbf{Identity Element.} 
    \item \textbf{Associative Property.} 
    \item \textbf{Absorption Property.} 
    \item \textbf{Identity Element.} 
    \item \textbf{Distributive Property (Left).} 
    \item \textbf{Distributive Property (Right).} 
\end{itemize}
where the distributive property can be proved by enumerating all possible orders of ,  and .

Most reliable path (Theorem~\ref{thm:reliable_path}) uses  for \emph{summation} and  for \emph{multiplication}.  The corresponding identities are  and , since all path representations are probabilities in . It is obvious that  satisfies the associative property, the absorption property and has identity element .
\begin{itemize}
    \setlength{\parskip}{0pt}
    \setlength{\itemsep}{0pt plus 1pt}
    \item \textbf{Commutative Property.} 
    \item \textbf{Associative Property.} 
    \item \textbf{Identity Element.} 
    \item \textbf{Distributive Property (Left).} 
    \item \textbf{Distributive Property (Right).} 
\end{itemize}
where the identity element and the distributive property hold for non-negative elements.
\end{proof}

\section{Time Complexity of GNN Frameworks}
\label{app:complexity}

Here we prove the time complexity for \method and other GNN frameworks.

\subsection{\method}

\begin{lemma}
\label{lem:bellmanford}
The time complexity of one \method run (Algorithm~\ref{alg:framework}) is .
\end{lemma}
\begin{proof}
We break the time complexity by \textsc{Indicator}, \textsc{Message} and \textsc{Aggregate} functions.

\textsc{Indicator} is called  times, and a single call to \textsc{Indicator} takes  time. \textsc{Message} is called  times, and a single call to \textsc{Message}, i.e., a relation operator, takes  time. \textsc{Aggregate} is called  times over a total of  messages with  dimensions. Each call to \textsc{Aggregate} additionally takes  time due to the linear transformations in the function.

Therefore, the total complexity is summed to .
\end{proof}

In practice, we find a small constant  works well for link prediction, and Lemma~\ref{lem:bellmanford} can be reduced to  time.

Now consider applying \method to infer the likelihood of all possible triplets. Without loss of generality, assume we want to predict the tail entity for each head entity and relation . We group triplets with the same condition  together, where each group contains  triplets. For triplets in a group, we only need to execute Algorithm~\ref{alg:framework} once to get their predictions. Therefore, the amortized time for a single triplet is .

\subsection{VGAE / RGCN}
RGCN is a message-passing GNN applied to multi-relational graphs, with the message function being a per-relation linear transformation. VGAE can be viewed as a special case of RGCN applied to single-relational graphs. The time complexity of RGCN is similar to Lemma~\ref{lem:bellmanford}, except that each call to the message function takes  time due to the linear transformation. Therefore, the total complexity is , where  refers to the number of layers in RGCN. Since , the complexity is reduced to \footnote{By moving the linear transformations from the message function to the aggregation function, one can also get an implementation of RGCN with  time, which is better for dense graphs but worse for sparse graphs. For knowledge graph datasets used in this paper, the above  implementation is better.}. In practice,  is a small constant and we get  complexity.

While directly executing RGCN once for each triplet is costly, a smart way to apply RGCN for inference is to first compute all node representations, and then perform link prediction with the node representations. The first step runs RGCN once for  triplets, while the second step takes  time. Therefore, the amortized time for a single triplet is . For large graphs and reasonable choices of , we have  and the amortized time can be reduced to .

\subsection{NeuralLP / DRUM}
DRUM can be viewed as a special case of \method with \textsc{Message} being Hadamard product and \textsc{Aggregate} being natural summation. NeuralLP is a special case of DRUM where the dimension  equals to 1. Since there is no linear transformation in their \textsc{Aggregate} functions, the amortized time complexity for the message passing part is . Both DRUM and NeuralLP additionally use an LSTM to learn the edge weights for each layer, which additionally costs  time for  layers.  is small and can be ignored like in other methods. Therefore, the amortized time of two parts is summed to .

\subsection{SEAL / GraIL}
GraIL first extracts a local subgraph surrounding the link, and then applies RGCN to the local subgraph. SEAL can be viewed as a special case of GraIL applied to single-relational graphs. Therefore, their amortized time is the same as that of one RGCN run, which is .

Note that one may still run GraIL on large graphs by restricting the local subgraphs to be very small, e.g., within 1-hop neighborhood of the query entities. However, this will severely harm the performance of link prediction. Moreover, most real-world graphs are small-world networks, and a moderate radius can easily cover a non-trivial number of nodes and edges, which costs a lot of time for GraIL.

\section{Number of Parameters}
\label{app:num_param}

\begin{table}[!h]
    \centering
    \caption{Number of parameters in \method. The number of parameters only grows with the number of relations , rather than the number of nodes  or edges . For FB15k-237 augmented with flipped triplets,  is 474. Our best configuration uses ,  and hidden dimension .}
    \label{tab:num_param}
    \begin{tabular}{lcccc}
        \toprule
                            & \multicolumn{2}{c}{\bf{\#Parameter}} \\
                            & \bf{Analytic Formula} & \bf{FB15k-237} \\
        \midrule
        \textsc{Indicator}  &               & 15,168        \\
        \textsc{Message}    &         & 3,003,264     \\
        \textsc{Aggregate}  &            & 80,448        \\
                  &          & 4,225         \\
        \midrule
        Total               &                       & 3,103,105     \\
        \bottomrule
    \end{tabular}
\end{table}

One advantage of \method is that it requires much less parameters than embedding methods. For example, on FB15k-237, \method requires 3M parameters while TransE requires 30M parameters. Table~\ref{tab:num_param} shows a break down of number of parameters in \method. Generally, the number of parameters in \method scales linearly w.r.t. the number of relations, regardless the number of entities in the graph, which makes \method more parameter-efficient for large graphs.

\section{Statistics of Datasets}
\label{app:dataset_stat}

Dataset statistics of two transductive settings, i.e., knowledge graph completion and homogeneous graph link prediction, are summarized in Table~\ref{tab:kg_statistics} and \ref{tab:homo_statistics}. Dataset statistics of inductive relation prediction is summarized in Table~\ref{tab:inductive_statistics}.

We use the standard transductive splits~\cite{toutanova2015observed, dettmers2018convolutional} and inductive splits~\cite{teru2020inductive} for knowledge graphs. For homogeneous graphs, we follow previous works~\cite{kipf2016variational, davidson2018hyperspherical} and randomly split the edges into train/validation/test sets with a ratio of 85:5:10. All the homogeneous graphs used in this paper are undirected. Note that for inductive relation prediction, the original paper~\cite{teru2020inductive} actually uses a \emph{transductive valid set} that shares the same set of fact triplets as the training set for hyperparameter tuning. The \emph{inductive test set} contains entities, query triplets and fact triplets that never appear in the training set. The same data split is adopted in this paper for a fair comparison.

\begin{table}[!h]
    \begin{minipage}[t]{0.58\textwidth}
        \centering
        \caption{Dataset statistics for knowledge graph completion.}
        \label{tab:kg_statistics}
        \begin{adjustbox}{max width=\textwidth}
            \begin{tabular}{lccccc}
                \toprule
                \multirow{2}{*}{\bf{Dataset}} & \multirow{2}{*}{\bf{\#Entity}} & \multirow{2}{*}{\bf{\#Relation}} & \multicolumn{3}{c}{\bf{\#Triplet}} \\
                & & & \bf{\#Train} & \bf{\#Validation} & \bf{\#Test} \\
                \midrule
                FB15k-237~\cite{toutanova2015observed} & 14,541 & 237 & 272,115 & 17,535 & 20,466 \\
                WN18RR~\cite{dettmers2018convolutional} & 40,943 & 11 & 86,835 & 3,034 & 3,134 \\
                \bottomrule
            \end{tabular}
        \end{adjustbox}
    \end{minipage}
    \hspace{0.3em}
    \begin{minipage}[t]{0.42\textwidth}
        \centering
        \caption{Dataset statistics for homogeneous link prediction.}
        \label{tab:homo_statistics}
        \begin{adjustbox}{max width=\textwidth}
            \begin{tabular}{lcccc}
                \toprule
                \multirow{2}{*}{\bf{Dataset}} & \multirow{2}{*}{\bf{\#Node}} & \multicolumn{3}{c}{\bf{\#Edge}} \\
                & & \bf{\#Train} & \bf{\#Validation} & \bf{\#Test} \\
                \midrule
                Cora~\cite{sen2008collective} & 2,708 & 4,614 & 271 & 544 \\
                CiteSeer~\cite{sen2008collective} & 3,327 & 4,022 & 236 & 474 \\
                PubMed~\cite{sen2008collective} & 19,717 & 37,687 & 2,216 & 4,435 \\
                \bottomrule
            \end{tabular}
        \end{adjustbox}
    \end{minipage}
\end{table}

\begin{table}[!h]
    \centering
    \caption{Dataset statistics for inductive relation prediction. Queries refer to the triplets that are used as training or test labels, while facts are the triplets used as training or test inputs. In the training sets, all queries are also provided as facts.}
    \label{tab:inductive_statistics}
    \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{llcccccccccc}
            \toprule
            \multirow{2}{*}{\bf{Dataset}} & & \multirow{2}{*}{\bf{\#Relation}} & \multicolumn{3}{c}{\bf{Train}} & \multicolumn{3}{c}{\bf{Validation}} & \multicolumn{3}{c}{\bf{Test}} \\
            & & & \bf{\#Entity} & \bf{\#Query} & \bf{\#Fact} & \bf{\#Entity} & \bf{\#Query} & \bf{\#Fact} & \bf{\#Entity} & \bf{\#Query} & \bf{\#Fact} \\
            \midrule
            \multirow{4}{*}{FB15k-237~\cite{teru2020inductive}}
            & v1 & 180 & 1,594 & 4,245 & 4,245 & 1,594 & 489 & 4,245 & 1,093 & 205 & 1,993\\
            & v2 & 200 & 2,608 & 9,739 & 9,739 & 2,608 & 1,166 & 9,739 & 1,660 & 478 & 4,145 \\
            & v3 & 215 & 3,668 & 17,986 & 17,986 & 3,668 & 2,194 & 17,986 & 2,501 & 865 & 7,406 \\
            & v4 & 219 & 4,707 & 27,203 & 27,203 & 4,707 & 3,352 & 27,203 & 3,051 & 1,424 & 11,714 \\
            \multirow{4}{*}{WN18RR~\cite{teru2020inductive}}
            & v1 & 9 & 2,746 & 5,410 & 5,410 & 2,746 & 630 & 5,410 & 922 & 188 & 1,618 \\
            & v2 & 10 & 6,954 & 15,262 & 15,262 & 6,954 & 1,838 & 15,262 & 2,757 & 441 & 4,011\\
            & v3 & 11 & 12,078 & 25,901 & 25,901 & 12,078 & 3,097 & 25,901 & 5,084 & 605 & 6,327\\
            & v4 & 9 & 3,861 & 7,940 & 7,940 & 3,861 & 934 & 7,940 & 7,084 & 1,429 & 12,334 \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}


\section{Implementation Details}
\label{app:implementation}

\begin{table}[!h]
    \centering
    \caption{Hyperparameter configurations of \method on different datasets. Adv. temperature corresponds to the temperature in self-adversarial negative sampling~\cite{sun2019rotate}. Note for FB15k-237 and WN18RR, we use the same hyperparameters for their transductive and inductive settings. We find our model configuration is robust across all datasets, therefore we only tune the learning hyperparameters for each dataset. All the hyperparameters are chosen by the performance on the validation set.}
    \label{tab:hyperparameter}
    \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{llccccc}
            \toprule
            \multicolumn{2}{l}{\bf{Hyperparameter}}
                                 & \bf{FB15k-237} & \bf{WN18RR} & \bf{Cora} & \bf{CiteSeer} & \bf{PubMed} \\
            \midrule
            \multirow{2}{*}{\bf{GNN}}
                                 & \#layer() & 6         & 6      & 6    & 6        & 6      \\
                                 & hidden dim.  & 32        & 32     & 32   & 32       & 32     \\
            \midrule
            \multirow{2}{*}{\bf{MLP}}
                                 & \#layer     & 2         & 2      & 2    & 2        & 2      \\
                                 & hidden dim. & 64        & 64     & 64   & 64       & 64    \\
            \midrule
            \multirow{2}{*}{\bf{Batch}}
                                 & \#positive  & 256       & 128    & 256  & 256    & 64     \\
                                 & \#negative/\#positive() & 32     & 32   & 1      & 1     & 1      \\
            \midrule
            \multirow{4}{*}{\bf{Learning}}
                                 & optimizer     & Adam   & Adam   & Adam   & Adam   & Adam    \\
                                 & learning rate & 5e-3   & 5e-3   & 5e-3   & 5e-3   & 5e-3    \\
                                 & \#epoch       & 20     & 20     & 20     & 20     & 20      \\
                                 & adv. temperature & 0.5 & 1      & -      & -      & -       \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

Our implementation generally follows the open source codebases of knowledge graph completion\footnote{\url{https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding}. MIT license.} and homogeneous graph link prediction\footnote{\url{https://github.com/tkipf/gae}. MIT license.}. Table~\ref{tab:hyperparameter} lists the hyperparameter configurations for different datasets. Table~\ref{tab:wall_time} shows the wall time of training and inference on different datasets.

\textbf{Data Augmentation.} For knowledge graphs, we follow previous works~\cite{yang2017differentiable, sadeghian2019drum} and augment each triplet \edge{u, q, v} with a flipped triplet \edge{v, q, u}. For homogeneous graphs, we follow previous works~\cite{kipf2016semi, kipf2016variational} and augment each node  with a self loop \edge{u, u}.

\textbf{Architecture Details.} We apply Layer Normalization~\cite{ba2016layer} and short cut connection to accelerate the training of \method. Layer Normalization is applied after each \textsc{Aggregate} function. The feed-forward network  is instantiated as a MLP. ReLU is used as the activation function for all hidden layers. For undirected graphs, we symmetrize the pair representation by taking the sum of  and .

\textbf{Training Details.} We train \method on 4 Tesla V100 GPUs with standard data parallelism. During training, we drop out edges that directly connect query node pairs to encourage the model to capture longer paths and prevent overfitting. We select the best checkpoint for each model based on its performance on the validation set. The selection criteria is MRR for knowledge graphs and AUROC for homogeneous graphs.

\textbf{Fused Message Passing.} To reduce memory footprint and better utilize GPU hardware, we follow the efficient implementation of GNNs~\cite{huang2020ge} and implement customized PyTorch operators that combines \textsc{Message} and \textsc{Aggregate} functions into a single operation, without creating all messages explicitly. This reduces the memory complexity of NBFNet from  to .

\begin{table}[!h]
    \centering
    \caption{Wall time of \method on different datasets and in different settings (Table~\ref{tab:kg_result}, \ref{tab:homo_result} and \ref{tab:inductive_result}). For inductive setting, the total time over 4 split versions is reported.}
    \label{tab:wall_time}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{lccccccc}
        \toprule
        \multirow{2}{*}{\bf{Wall Time}} & \multicolumn{5}{c}{\bf{Transductive}} & \multicolumn{2}{c}{\bf{Inductive}} \\
                       & FB15k-237 & WN18RR & Cora & CiteSeer & PubMed & FB15k-237 & WN18RR \\
        \midrule
        \bf{Training}  & 9.7 hrs  & 4.4 hrs  & 5.5 mins & 5.3 mins & 5.6 hrs & 23 mins & 41 mins \\
        \bf{Inference} & 4.0 mins & 2.4 mins & < 1 sec & < 1 sec & 25 secs & 6 secs & 20 secs \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\section{Experimental Results on OGB Datasets}
\label{app:ogb}

To demonstrate the effectiveness of \method on large-scale graphs, we additionally evaluate our method on two knowledge graph datasets from OGB~\cite{hu2020ogb}, ogbl-biokg and WikiKG90M. We follow the standard evaluation protocol of OGB link property prediction, and compute the mean reciprocoal rank (MRR) of the true entity against 1,000 negative entities.

\subsection{Results on ogbl-biokg}

Ogbl-biokg is a large biomedical knowledge graph that contains 93,773 entities, 51 relations and 5,088,434 triplets. We compare \method with 6 embedding methods on this dataset. Note by the time of this work, only embedding methods are available for such large-scale datasets. Table~\ref{tab:ogbl-biokg} shows the results on ogbl-biokg. NBFNet achieves the best result compared to all methods reported on the official leaderboard\footnote{\url{https://ogb.stanford.edu/docs/leader_linkprop/\#ogbl-biokg}} with much fewer parameters. Note the previous best model AutoSF is based on architecture search and requires more computation resource than NBFNet for training.

\begin{table}[!h]
    \centering
    \caption{Knowledge graph completion results on ogbl-biokg. Results of compared methods are taken from the OGB leaderboard.}
    \label{tab:ogbl-biokg}
    \footnotesize
    \begin{tabular}{llccc}
        \toprule
        \bf{Class} & \bf{Method} & \bf{Test MRR} & \bf{Validation MRR} & \bf{\#Params}\\
        \midrule
        \multirow{6}{*}{\bf{Embeddings}}
        & TransE~\cite{bordes2013translating} & 0.7452 & 0.7456 & 187,648,000 \\
        & DistMult~\cite{yang2015embedding} & 0.8043 & 0.8055 & 187,648,000 \\
        & ComplEx~\cite{trouillon2016complex} & 0.8095 & 0.8105 & 187,648,000 \\
        & RotatE~\cite{sun2019rotate} & 0.7989 & 0.7997 & 187,597,000 \\
        & AutoSF~\cite{zhang2020autosf} & 0.8309 & \best{0.8317} & 93,824,000 \\
        & PairRE~\cite{chao2021pairre} & 0.8164 & 0.8172 & 187,750,000 \\
        \midrule
        \bf{GNNs}
        & \method & \best{0.8317} & \best{0.8318} & 734,209\\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Results on WikiKG90M}

WikiKG90M is an extremely large dataset used in OGB large-scale challenge~\cite{hu2021ogb}, hold at KDD Cup 2021. It is a general-purpose knowledge graph containing 87,143,637 entities, 1,315 relations and 504,220,369 triplets.

To apply \method to such a large scale, we use a bidirectional breath-first-search (BFS) algorithm to sample a local subgraph for each query. Given a query, we generate a -hop neighborhood for each of the head entity and the candidate tail entities, based on a BFS search. The union of all generated neighborhoods is then collected as the sampled graph. With this sampling algorithm, any path within a length of  between the head entity and any tail candidate is guaranteed to present in the sampled graph. See Figure~\ref{fig:bfs_sampling} for illustration. While a standard single BFS algorithm computing the -hop neighborhood of the head entity has the same guarantee, a bidirectional BFS algorithm significantly reduces the number of nodes and edges in the sampled graph.

\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/before_sampling.pdf}
        \caption{Original graph}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/bfs_sampling.pdf}
        \caption{Bidirectional BFS}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/after_sampling.pdf}
        \caption{Sampled graph}
    \end{subfigure}
    \caption{Illustration of bidirectional BFS sampling. For a \textcolor{myblue}{head entity} and multiple \textcolor{myorange}{tail candidates}, we use BFS to sample a -hop neighborhood around each entity, regardless of the direction of edges. The neighborhood is denoted by dashed circles. The nodes and edges visited by the BFS algorithm are extracted to generate the sampled graph. Best viewed in color.}
    \label{fig:bfs_sampling}
\end{figure}

We additionally downsample the neighbors when expanding the neighbors of an entity, to tackle entities with large degrees. For each entity visited during the BFS algorithm, we downsample its outgoing neighbors and incoming neighbors to  entities respectively.

Table~\ref{tab:ogb_lsc} shows the results of NBFNet on WikiKG90M validation set. Our best single model uses  and . While the validation set requires to rank the true entity against 1,000 negative entities, in practice it is not mandatory to draw 1,000 negative samples for each positive sample during training. We find that reducing the negative samples from 1,000 to 20 and increasing the batch size from 4 to 64 provides a better result, although it creates a distribution shift between sampled graphs in training and validation. We leave further research of such distribution shift as a future work.



\begin{table}[!h]
    \begin{minipage}{0.58\textwidth}
    \centering
    \caption{Results of different \textsc{Message} and \textsc{Aggregate} functions on FB15k-237.}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{llccccc}
        \toprule
        \bf{\textsc{Message}} & \bf{\textsc{Aggregate}} & \bf{MR} & \bf{MRR} & \bf{H@1} & \bf{H@3} & \bf{H@10} \\
        \midrule
        \multirow{4}{*}{TransE~\cite{bordes2013translating}}
        & Sum & 191 & 0.297 & 0.217 & 0.321 & 0.453 \\
        & Mean & 161 & 0.310 & 0.218 & 0.339 & 0.496 \\
        & Max & 135 & 0.377 & 0.282 & 0.415 & 0.565 \\
        & PNA~\cite{corso2020principal} & 129 & 0.383 & 0.288 & 0.420 & 0.568 \\
        \midrule
        \multirow{4}{*}{DistMult~\cite{yang2015embedding}}
        & Sum & 136 & 0.388 & 0.294 & 0.427 & 0.574 \\
        & Mean & 132 & 0.384 & 0.287 & 0.425 & 0.577 \\
        & Max & 136 & 0.374 & 0.279 & 0.412 & 0.563 \\
        & PNA~\cite{corso2020principal} & \bf{114} & \bf{0.415} & \bf{0.321} & \bf{0.454} & \bf{0.599} \\
        \midrule
        \multirow{4}{*}{RotatE~\cite{sun2019rotate}}
        & Sum & 129 & 0.392 & 0.298 & 0.429 & 0.580 \\
        & Mean & 138 & 0.376 & 0.278 & 0.416 & 0.571 \\
        & Max & 139 & 0.385 & 0.290 & 0.423 & 0.572 \\
        & PNA~\cite{corso2020principal} & \bf{117} & \bf{0.414} & \bf{0.323} & \bf{0.454} & \bf{0.593} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:msg_agg_full}
    \end{minipage}
    \hspace{0.3em}
    \begin{minipage}{0.42\textwidth}
    \centering
    \caption{Knowledge graph completion results on WikiKG90M validation set.}
    \label{tab:ogb_lsc}
    \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{lcc}
            \toprule
            \bf{Model} & Single Model & 6 Model Ensemble \\
            \midrule
            \bf{MRR} & 0.924 & 0.930 \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \vspace{1.5em}
    
    \centering
    \caption{Results of different number of layers on FB15k-237.}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{lccccc}
        \toprule
        \bf{\#Layers ()} & \bf{MR} & \bf{MRR} & \bf{H@1} & \bf{H@3} & \bf{H@10} \\
        \midrule
        2 & 191 & 0.345 & 0.261 & 0.377 & 0.510 \\
        4 & 119 & 0.409 & 0.315 & 0.450 & 0.592 \\
        6 & \bf{114} & \bf{0.415} & \bf{0.321} & \bf{0.454} & \bf{0.599} \\
        8 & \bf{115} & \bf{0.416} & \bf{0.322} & \bf{0.457} & \bf{0.599} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:num_layer_full}
    \end{minipage}
\end{table}

\section{Ablation Study}
\label{app:ablation}

Table~\ref{tab:msg_agg_full} shows the full results of different \textsc{Message} and \textsc{Aggregate} functions. Table~\ref{tab:num_layer_full} shows the full results of \method with different number of layers.