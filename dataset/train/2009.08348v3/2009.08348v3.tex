
\documentclass{article} \usepackage{arxiv_style,times}

\usepackage{amsmath,amssymb} 

\usepackage[colorlinks=true,linkcolor=blue,citecolor=gray]{hyperref}
\usepackage{url}
\urlstyle{same}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{color}
\usepackage[utf8]{inputenc} \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       
\usepackage{microtype}      
\usepackage{color}

\usepackage{listings}
\usepackage{xcolor,colortbl}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{language=Python,morekeywords={self.},style=mystyle}

\definecolor{vvlightgray}{rgb}{0.9,0.9,0.9}
\definecolor{vlightgray}{rgb}{0.8,0.8,0.8}


\usepackage{wrapfig}
\graphicspath{ {./images/} }
\usepackage{multirow}
\usepackage{sidecap}
\newcommand{\Sam}[1]{\textcolor{blue}{[Sam: #1]}}
\newcommand{\Karsten}[1]{\textcolor{red}{[Karsten: #1]}}
\newcommand{\Timo}[1]{\textcolor{magenta}{[Timo: #1]}}
\newcommand{\Joseph}[1]{\textcolor{orange}{[Joseph: #1]}}
\newcommand{\Marzyeh}[1]{\textcolor{orange}{[Marzyeh: #1]}}


\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\minsort}{min\,sort}

\title{S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning}



\author{Karsten Roth, Timo Milbich, Björn Ommer, Joseph P. Cohen, Marzyeh Ghassemi\\
\footnotesize{University of Toronto, Vector Institute; Heidelberg University, IWR; Mila, Université de Montréal}}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle
\vspace{-16pt}

\begin{abstract}
Deep Metric Learning (DML) provides a crucial tool for visual similarity and zero shot retrieval applications by learning generalizing embedding spaces, although recent work in DML has shown strong performance saturation across training objectives.
However, generalization capacity is known to scale with the embedding space dimensionality. Unfortunately, high dimensional embeddings also create higher retrieval cost for downstream applications.
To remedy this, we propose \textit{S2SD} - \emph{Simultaneous Similarity-based Self-distillation}.  \textit{S2SD} extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining test-time cost and with negligible changes to the training time. 
Experiments and ablations across different objectives and standard benchmarks show \textit{S2SD} offering notable improvements of up to 7\% in Recall@1, while also setting a new state-of-the-art. 
\end{abstract}

\section{Introduction}
Deep Metric Learning (\textit{DML}) aims to learn embedding space () models in which a predefined distance metric reflects not only the semantic similarities between training samples, but also transfers to unseen classes. The generalization capabilities of these models are important for applications in image retrieval \citep{margin}, face recognition \citep{semihard}, clustering \citep{grouping} and representation learning \citep{moco}.
Still, transfer learning into unknown test distributions remains an open problem, with \cite{roth2020revisiting} and \cite{musgrave2020metric} revealing strong performance saturation across DML training objectives. However, 
\cite{roth2020revisiting} also show that embedding space dimensionality () can be a driver for generalization across objectives due to higher representation capacity. 
Indeed, this insight can be linked to recent work targeting other objective-independent improvements to DML via artificial samples \citep{hardness-aware}, higher feature distribution moments \citep{horde} or orthogonal features \citep{milbich2020diva}, which have shown promising relative improvements over selected DML objectives.
Unfortunately, these methods come at a cost; be it longer training times or limited applicability.
Similarly, drawbacks can be found when naively increasing the operating (\textit{base}) , incurring increased cost for data retrieval at test time, which is especially problematic on larger datasets. This limits realistically usable s and leads to benchmarks being evaluated against fixed, predefined s.

In this work, we propose \textit{Simultaneous Similarity-based Self-Distillation} (\textit{S2SD}) to show that complex higher-dimensional information can actually be effectively leveraged in DML without changing the base  and test time cost, which we motivate from two key elements. Firstly, in DML, an additional  can be spanned by a multilayer perceptron (MLP) operating over the feature representation shared with the base  (see e.g. \citep{milbich2020diva}). 
With larger , we can thus cheaply learn a secondary high-dimensional  simultaneously, also denoted as \textit{target} . 
Relative to the large feature backbone, and with the \emph{batchsize} capping the number of additional high dimensional operations, only little additional training cost is introduced. 
While we can not utilize the high-dim. target  at test time for aforementioned reasons, we may utilize it to boost the performance of the base .
Unfortunately, a simple connection of base and target s through the shared feature backbone is insufficient for the base  to benefit from the auxiliary, high-dimensional information.
Thus, secondly, to efficiently leverage the high-dimensional context, we use insights from knowledge distillation \citep{hinton2015distilling}, where a small ``student'' model is trained to approximate a larger ``teacher'' model.
However, while knowledge distillation can be found in DML \citep{chen2017darkrank}, few-shot learning \citep{tian2020rethinking} and self-supervised extensions thereof \citep{rajasegaran2020selfsupervised}, the reliance on additional, commonly larger teacher networks or multiple training runs \citep{furlanello2018born}, introduces much higher training cost.
Fortunately, we find that the target  learned \textit{simultaneously} at higher dimension can sufficiently serve as a ``teacher'' \textit{during} training - through knowledge distillation of its sample similarities, the performance of the base  can be improved notably.
Such distillation intuitively encourages the lower-dimensional base  to embed semantic similarities similar to the more expressive target  and thus incorporate dimensionality-related generalization benefits.

Furthermore, \textit{S2SD} makes use of the low cost to span additional  to introduce multiple target s. Operating each of them at higher, but varying dimensionality, joint distillation can then be used to enforce reusability in the distilled content akin to feature reusability in meta-learning \citep{raghu2019rapid} for additional generalization boosts. 
Finally, in DML, the base  is spanned over a penultimate feature space of much higher dimensionality, which introduces a dimensionality-based bottleneck \citep{milbich2020sharing}. By applying the distillation objective between feature and base embedding space in \textit{S2SD}, we further encourage better feature usage in base . This facilitates the approximation of high-dimensional context through the base  for additional improvements in generalization.





The benefits to generalization are highlighted in performance boosts across three standard benchmarks, CUB200-2011 \citep{cub200-2011}, CARS196 \citep{cars196} and Stanford Online Products \citep{lifted}, where \textit{S2SD} improves test-set recall@1 of already strong DML objectives by up to , while also setting a new state-of-the-art. Improvements are even more significant in very low dimensional base s, making \textit{S2SD} attractive for large-scale retrieval problems which can benefit from reduced s.
Importantly, as \textit{S2SD} is applied \textbf{during} the same DML training process on the \textbf{same} network backbone, no large teacher networks or additional training runs are required. Simple experiments even show that \textit{S2SD} can outperform comparable 2-stage distillation at much lower cost.

In summary, our contributions can be described as:\\
\textbf{1)} We propose \textit{Simultaneous Similarity-based Self-Distillation} (\textit{S2SD}) for DML, using knowledge distillation of high-dimensional context without large additional teacher networks or training runs.\\
\textbf{2)} We motivate and evaluate this approach through detailed ablations and experiments, showing that the method is agnostic to choices in objectives, backbones, and datasets.\\
\textbf{3)} Across benchmarks, we achieve significant improvements over strong baseline objectives and state-of-the-art performance, with especially large boosts for very low-dimensional embedding spaces.


\section{Related Work}
\textbf{Deep Metric Learning (DML)} has proven useful for zero-shot image/video retrieval \& clustering \citep{semihard,margin,Brattoli_2020_CVPR}, face verification \citep{sphereface,arcface} and contrastive (self-supervised) representation learning (e.g. \cite{moco,chen2020simple,pretextmisra}).
Approaches can be divided into \textbf{1)} improved ranking losses, \textbf{2)} tuple sampling methods and \textbf{3)} extensions to the standard DML training approach.\\
\textbf{1)} Ranking losses place constraints on relations in image tuples ranging from pairs (e.g. \cite{contrastive}) to triplets \citep{semihard} and more complex orderings \citep{quadtruplet,lifted,npairs,multisimilarity}.
\textbf{2)} The number of possible tuples scales exponentially with dataset size, leading to many tuple sampling approaches to ensure meaningful tuples presented during training. These tuple sampling methods can follow heuristics (\cite{semihard,margin}), be of hierarchical nature \citep{htl} or learned \citep{roth2020pads}. 
Similarly, learnable proxies to replace tuple members \citep{proxynca,kim2020proxy,softriple} can also remedy the sampling issue, which can be extended to tackle DML from a classification viewpoint \citep{zhai2018classification,arcface}.
\textbf{3)} Finally, extensions to the basic training scheme can involve synthetic data \citep{dvml,hardness-aware,daml}, complementary features \citep{mic,milbich2020diva}, a division into subspaces \citep{Sanakoyeu_2019_CVPR,dreml,abe,abier} or higher-order moments \citep{horde}.\\ 
While \textit{S2SD} is similarly an extension to DML, we specifically focus on capturing and distilling complex high-dimensional sample relations into a lower embedding space to improve generalization.\\
\\
\textbf{Knowledge Distillation} involves knowledge transfer from teacher to (usually smaller) student models, e.g. by matching network softmax outputs/logits \citep{bucilu2006model,hinton2015distilling}, (attention-weighted) feature maps \citep{romero2014fitnets,zagoruyko2016paying}, or latent representations \citep{Ahn_2019,Park_2019,tian2019contrastive,laskar2020dataefficient}.
Importantly, \cite{tian2019contrastive} show that under fair comparison, basic matching via Kullback-Leibler (KL) Divergences as used in \cite{hinton2015distilling} performs very well, which we also find to be the case for \textit{S2SD}. 
This is further supported in recent few-shot learning literature \citep{tian2020rethinking}, wherein KL-distillation alongside self-distillation \citep{furlanello2018born} in additional meta-training stages improves feature representation strength important for generalization \citep{raghu2019rapid}.\\
\textit{S2SD} is a form of self-distillation, i.e. distilling without a separate, larger teacher network. However, we leverage dimensionality-related context for distillation, which allows \textit{S2SD} to be used in the \textbf{same} training run. 
\textit{S2SD} also stands in contrast to existing knowledge distillation applications to DML, which are done in a generic manner with separate, larger teacher networks and additional training stages \citep{chen2017darkrank,Yu_2019,Han2019DeepDM,laskar2020dataefficient}.






\section{Method}\label{sec:methods}
We now introduce key elements for \textit{Simultaneous Similarity-based Self-Distillation }\textit{(S2SD)} to improve generalization of embedding spaces by utilizing higher dimensional context. We start with preliminary notation and fundamentals to Deep Metric Learning (\S\ref{sec:prelim}). We then define the three key elements to \textit{S2SD}: Firstly, the Dual Self-Distillation (DSD) objective, which uses KL-Distillation on a concurrently learned high-dimensional embedding space (\S\ref{sec:dual}) to introduce the high-dimensional context into a low-dimensional embedding space during training. We then extend this to Multiscale Self-Distillation (MSD) with distillation from several different high-dimensional embedding spaces to encourage reusability in the distilled context (\S\ref{sec:multi}). Finally, we shift to self-distillation from normalized feature representations to counter dimensionality bottlenecks (MSDF) (\S\ref{sec:feats}).

\subsection{Preliminaries}
\label{sec:prelim}
DML builds on generic Metric Learning which aims to find a (parametrized) distance metric  on the \textit{feature space}  over images  that best satisfy ranking constraints usually defined over class labels . This holds also for DML. However, while Metric Learning relies on a \textbf{fixed} feature extraction method to obtain , DML introduces deep neural networks to concurrently learn a feature representation. 
Most such DML approaches aim to learn Mahalanobis distance metrics, which cover the parametrized family of inner product metrics \citep{surez2018tutorial,chen2019curvilinear}. 
These metrics, with some restrictions \citep{surez2018tutorial}, can be reformulated as

with learned linear projection  from -dim. \textit{features}  to -dim. \textit{embeddings}  with embedding function . Importantly, this redefines the motivation behind DML as learning -dimensional image embeddings  s.t. their euclidean distance  is connected to semantic similarities in . This embedding-based formulation offers the significant advantage of being compatible with fast approximate similarity search methods (e.g. \cite{faiss}), allowing for large-scale applications at test time.
In this work, we assume  to be normalized to the unit hypersphere , which is commonly done \citep{margin,Sanakoyeu_2019_CVPR,sphereface,wang2020understanding} for beneficial regularizing purposes \citep{margin,wang2020understanding}. 
For the remainder we hence set  to refer to .\\
Common approaches to learn such a representation space involve training surrogates on ranking constraints defined by class labels. Such approaches start from pair or triplet-based ranking objectives \citep{contrastive,semihard}, where the latter is defined as

with margin  and the set of available triplets  in a mini-batch , with .
This can be extended with more complex ranking constraints or tuple sampling methods. 
We refer to Supp. \ref{supp:base_methods} and \cite{roth2020revisiting} for further insights and detailed studies.





\subsection{Embedding Space Self-Distillation}\label{sec:dual}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/setup3.png}
    \caption{\textit{S2SD.} We use a standard encoder , embedding , and multiple auxiliary embedding networks  (used only during training) depending on the \textit{S2SD} approach used. 
    During training, for each batch of embeddings produced by the respective embedding network , we compute DML losses while applying embedding distillation on the respective batch-similarity matrices (\textit{DSD/MSD}). We further distill from the feature representation space for additional information gain (\textit{MSDF}).}
    \label{fig:setup}
\end{figure*} For the aforementioned standard DML setting, generalization performance of a learned embedding space can be linked to the utilized embedding dimensionality. However, high dimensionality results in notably higher retrieval cost on downstream applications, which limits realistically usable dimensions.
In \textit{S2SD}, we show that high-dimensional context can be used as a teacher during the training run of the low-dimensional \textit{base} or \textit{reference} embedding space. As the base embedding model is also the one that is evaluated, test time retrieval costs are left unchanged.
To achieve this, we simultaneously train an additional high-dimensional \textit{auxiliary}/\textit{target} embedding space  spanned by a secondary embedding branch .  is parametrized by a MLP or a linear projection, similar to the base embedding space  spanned by , see \S\ref{sec:prelim}. Both  and  operate on the same large, shared feature backbone .
Note that for simplicity, we train  and  using the same DML training objective , though we are not limited to do so.\\ 
Unfortunately, higher expressivity and improved generalization of high-dimensional embeddings in  hardly benefit the base embedding space, even with a shared feature backbone. To explicitly leverage high-dimensional context for our base embedding space, we utilize knowledge distillation from target to base space.
However, while common knowledge distillation approaches match single embeddings or features between student and teacher, the different dimensionalities in  and  inhibit naive matching. Instead, \textit{S2SD} matches sample relations (see e.g. \cite{tian2019contrastive}) defined over batch-similarity matrices  in base and target space,  and , with batchsize . We thus encourage the base embedding space to relate different samples in a similar manner to the target space. 
To compute , we use a cosine similarity by default, given as , since  is normalized to the unit hypersphere. Defining  as the softmax operation and  as the Kullback-Leibler-divergence, we thus define the simultaneous self-distillation objective as

with temperature , as visualized in Figure \ref{fig:setup}. () denotes no gradient flow to target branches  as we only want the base space to learn from the target space.
By default, we match rows or columns of , , effectively distilling the relation of an anchor embedding  to all other batch samples. 
Embedding all batch samples in base dimension, , and higher dimension, , the (simultaneous) \emph{Dual Self-Distillation} (DSD) training objective then becomes








\subsection{Reusable Sample Relations by Multiscale Self-distillation}\label{sec:multi}
While \textit{DSD} encourages the reference embedding space to recover complex sample relations by distilling from a higher-dimensional target space spanned by , it is not known \textit{a priori} which distillable sample relations actually benefit generalization of the reference space.\\ 
To encourage the usage of sample relations that more likely aid generalization, we follow insights made in \cite{raghu2019rapid} on the connection between \textbf{reusability} of features across multiple tasks and better generalization thereof. We motivate reusability in \textit{S2SD} by extending \textit{DSD} to \emph{Multiscale Self-Distillation} (\textit{MSD}) with distillation instead from  multiple different target spaces spanned by . Importantly, each of these high-dimensional target spaces operate on different dimensionalities, i.e. . As this results in each target embedding space encoding sample relations differently, application of distillation across all spaces spanned by  pushes the base branch towards learning from sample relations that are reusable across all higher dimensional embedding spaces and thereby more likely to generalize (see also Fig. \ref{fig:setup}).\\
Specifically, given the set of target similarity matrices  and target batch embeddings , we then define the \textit{MSD} training objective as


\subsection{Tackling the Dimensionality Bottleneck by Feature Space Self-Distillation}\label{sec:feats}
As noted in \S\ref{sec:prelim}, the base embedding space  utilizes a linear projection  from the (penultimate) feature space  where  is commonly much larger than . While compressed semantic spaces encourage stronger representations \citep{alex2016deep,dai2019diagnosing} to be learned, \citet{milbich2020sharing} show that the actual test performance of the lower-dimensional embedding space  is inferior to that of the non-adapted, but higher-dimensional feature space . This supports a dimensionality-based loss of information beneficial to generalization, which can hinder the base embedding space to optimally approximate the high-dimensional context introduced in \S\ref{sec:dual} and \ref{sec:multi}.\\
To rectify this, we apply self-distillation (eq. \ref{eq:kl_distill}) on the normalized feature representations  generated by the normalized backbone  as well. 
With the batch of normalized feature representations  we get \emph{multiscale self-distillation with feature distillation} (\textit{MSDF}) (see also Fig.  \ref{fig:setup})

In the same manner, one can also address other architectural information bottlenecks such as through the generation of feature representations from a single global pooling operation. While not noted in the original publication, \cite{kim2020proxy} address this in the official code release by using both global max- and average pooling to create their base embedding space. While this naive usage changes the architecture at test time, in \textit{S2SD} we can \textit{fairly} leverage potential benefits by \textit{only} spanning the auxiliary spaces (and distilling) from such feature representations (denoted as \textit{DSDA}/\textit{MSDA}/\textit{MSDFA}).




\section{Experimental Setup}\label{sec:experiments}
We study \textit{S2SD} in four experiments to establish 1) method ablation performance \& relative improvements, 2) state-of-the-art, 3) comparisons to standard 2-stage distillation, benefits to low-dimensional embedding spaces \& generalization properties and 4) motivation for architectural choices.

\textbf{Method Notation.} We abbreviate ablations of \textit{S2SD} (see \S\ref{sec:methods}) in our experiments as: 
\textit{DSD} \& \textit{MSD} for \textbf{D}\textit{ual} (\ref{sec:dual}) \& \textbf{M}\textit{ultiscale} \textbf{S}\textit{elf}-\textbf{D}\textit{istillation} (\ref{sec:multi}), \textit{MSDF} the addition of \textbf{F}\textit{eature distillation} (\ref{sec:feats}) and \textit{DSDA}/\textit{MSD(F)A} the inclusion of multiple pooling operations in the auxiliary branches (also \S\ref{sec:feats}).

\subsection{Experiments}
\textbf{Fair Evaluation of Ablations.} \S\ref{sec:baselines} specifically applies \textit{S2SD} and its ablations to three DML baselines. To show realistic benefit, \textit{S2SD} is applied to best-performing objectives evaluated in \cite{roth2020revisiting}, namely \textit{(i)} Margin loss with Distance-based Sampling \citep{margin}, \textit{(ii)} their proposed Regularized Margin loss and \textit{(iii)} Multisimilarity loss \citep{multisimilarity}, following their experimental training pipeline. This setup utilizes no learning rate scheduling and fixes common implementational factors of variation in DML pipelines such as batchsize, base embedding dimension, weight decay or feature backbone architectures to ensure comparability in DML (more details in Supp. \ref{supp:impl_dets}). 
As such, our results are directly comparable to their large set of examined methods and guaranteed that relative improvements solely stem from the application of \textit{S2SD}.

\textbf{Evaluation Across Architectures and Embedding Dimensions.} \S\ref{sec:comp} further highlights the benefits of \textit{S2SD} by comparing \textit{S2SD}'s boosting properties across literature standards, with different backbone architectures and base embedding dimensions: \textit{(1)} ResNet50 with  = 128 \citep{margin,mic} and \textit{(2)}  = 512 \citep{zhai2018classification} as well as \textit{(3)} variants to Inception-V1 with Batch-Normalization at  = 512 \citep{multisimilarity,softriple,milbich2020diva}.
Only here do we conservatively apply learning rate scheduling, since all references noted in Table \ref{tab:sota} employ scheduling as well. We categorize published work based on backbone architecture and embedding dimension for fairer comparison. Note that this is a less robust comparison than done in \S \ref{sec:baselines}, due to potential implementation differences between our pipeline and reported literature results.

\textbf{Comparison to 2-Stage Distillation and Generalization Study.} \S\ref{sec:further_exp} compares \textit{S2SD} to 2-stage distillation, investigates benefits to very low dimensional reference spaces and examines the connection between improvements and increased embedding space feature richness, measured by density and spectral decay (see Supp. \ref{supp:gen_metrics}), which are linked to improved generalization in \cite{roth2020revisiting}.

\textbf{Investigation of Method Choices.} \S\ref{sec:ablations} finally ablates and motivates specific architectural choices in \textit{S2SD} used throughout \S \ref{sec:experiments}. Pseudo code and detailed results are available in Supp. \ref{supp:pseudo_code}, \ref{supp:detailed_1}, and \ref{supp:detailed_2}.

\subsection{Implementation}
\textbf{Datasets \& Evaluation.}
In all experiments, we evaluate on standard DML benchmarks: \textit{CUB200-2011} \citep{cub200-2011}, \textit{CARS196} \citep{cars196} and \textit{Stanford Online Products (SOP)} \citep{lifted}.
Performance is measured in \textit{recall at 1} (R@1) and \textit{at 2} (R@2) \citep{recall} as well as \textit{Normalized Mutual Information} (NMI) \citep{nmi}. More details in Supp. \ref{supp:bench_dets} \& \ref{supp:eval_metrics}.

\textbf{Experimental Details.}
Our implementation follows \citet{roth2020revisiting}, with more details in Supp. (\ref{supp:bench_dets}). For \S\ref{sec:baselines}-\ref{sec:ablations}, we only adjust the respective pipeline elements in questions.
For \textit{S2SD}, unless noted otherwise (s.a. in \S \ref{sec:ablations}), we set  for all objectives on CUB200 and CARS196, and  on SOP. \textit{DSD} uses target-dim.  and MSD target-dims. .
We found it beneficial to activate the feature distillation after  iterations for CUB200, CARS196 and SOP, respectively, to ensure that meaningful features are learned first before feature distillation is applied. The additional embedding spaces are generated by two layer MLPs with row-wise KL-distillation of similarities (eq. \ref{eq:kl_distill}), applied as in  (eq. \ref{eq:multi}). By default, we use Multisimilarity Loss as stand-in for .


\begin{table*}[t]
    \caption{\textit{S2SD comparison against strong baseline objectives.} \textbf{Bold} denotes best results per objective, \blue{\textbf{bluebold}} marks best overall results. mAP@R results as proposed in \cite{roth2020revisiting} and \cite{musgrave2020metric} as well as ProxyAnchor evaluations (\citet{kim2020proxy}, using a different setup) can be found in the Supplementary (Table \ref{tab:relative_results_map} and \ref{tab:proxyanchor_results}), further showing the notable benefits of S2SD.}
 \footnotesize
  \setlength\tabcolsep{1.4pt}
  \centering
\resizebox{\textwidth}{!}{
  \begin{tabular}{l || c | c || c | c || c | c}
     \toprule
     \multicolumn{1}{l}{\textsc{Benchmarks}} & \multicolumn{2}{c}{\textsc{CUB200-2011}} & \multicolumn{2}{c}{\textsc{CARS196}} & \multicolumn{2}{c}{\textsc{SOP}} \\
     \midrule
     \textsc{Approaches}  & R@1 & NMI & R@1 & NMI & R@1 & NMI\\
    \midrule
    \rowcolor{vvlightgray}
    \textbf{Margin}, , \citep{margin} &  &  &  &  &  & \\        
    + DSD &  &  &  &  &  & \\
    + MSD &  &  &  &  &  & \\
    + MSDF &  &  &  &  & \blue{} & \blue{} \\
    + MSDFA &  &  &  &  &  &  \\
    \midrule
    \rowcolor{vvlightgray}
    \textbf{R-Margin}, , \citep{roth2020revisiting} &  &  &  &  &  & \\ 
    + DSD &  &  &  &  &  & \\
    + MSD &  &  &  &  &  & \\
    + MSDF &  &  &  &  &  & \\
    + MSDFA &  &  &  &  &  & \\
    \midrule
    \rowcolor{vvlightgray}
    \textbf{Multisimilarity} \citep{multisimilarity} &  &  &  &  &  & \\
    + DSD &  &  &  &  &  & \\
    + MSD &  &  &  &  &  & \\        
    + MSDF &  &  &  &  &  & \\
    + MSDFA &  &  &  &  &  &  \\
         
\bottomrule
    \end{tabular}}
    \label{tab:relative_results}
\end{table*}






 



 
\section{Results}
\subsection{S2SD Improves Performance under Fair Evaluation} \label{sec:baselines}
In Tab. \ref{tab:relative_results} (full table in Supp. Tab. \ref{tab:relative_results_long}), we show that under the fair experimental protocol used in \cite{roth2020revisiting}, utilizing \textit{S2SD} and its ablations gives an objective and benchmark independent, significant boost in performance by up to  opposing the exisiting DML objective performance plateau. This holds even for regularized objectives s.a. R-Margin loss, highlighting the effectiveness of \textit{S2SD} for DML. Across objectives, \textit{S2SD}-based changes in wall-time do not exceed negligible . 


\subsection{S2SD Achieves SOTA Across Architecture and Embedding Dimension} 
\label{sec:comp}


\begin{table*}[t]
    \caption{\textit{State-of-the-art comparison.} We show that \textit{S2SD}, represented by its variants \textit{MSDF(A)}, boosts baseline objectives to state-of-the-art across literature. () stands for Inception-V1 with frozen Batch-Norm. \textbf{Bold}: best results per literature setup. \blue{\textbf{Bluebold}}: best results per overall benchmark.}\setlength\tabcolsep{1.5pt}
    \footnotesize
    \centering

\resizebox{\textwidth}{!}{

 \begin{tabular}{l || c | c | c || c | c | c || c | c | c}
     \toprule
     \multicolumn{1}{l}{\textsc{Benchmarks} } & \multicolumn{3}{c}{\textsc{CUB200} \citep{cub200-2011}} & \multicolumn{3}{c}{\textsc{CARS196} \citep{cars196}} & \multicolumn{3}{c}{\textsc{SOP} \citep{lifted}}\\
     \midrule
     \textsc{Methods}  & R@1 & R@2 & NMI & R@1 & R@2 & NMI & R@1 & R@10 & NMI\\
     \midrule
     \hline
\multicolumn{10}{>{\columncolor[gray]{.8}}l}{\textbf{ResNet50-128}} \\
     \hline
     \rowcolor{vvlightgray}
     Div\&Conq \citep{Sanakoyeu_2019_CVPR}  & 65.9 & 76.6 & 69.6 & 84.6 & 90.7 & 70.3 & 75.9 & 88.4 & 90.2\\
     \rowcolor{vvlightgray}
     MIC \citep{mic}                        & 66.1 & 76.8 & 69.7 & 82.6 & 89.1 & 68.4 & 77.2 & 89.4 & 90.0\\
     \rowcolor{vvlightgray}
     PADS \citep{roth2020pads}              & 67.3 & 78.0 & 69.9 & 83.5 & 89.7 & 68.8 & 76.5 & 89.0 & 89.9\\
     \hline
     Multisimilarity+\textit{S2SD}         & 68.0    0.2 & 78.7  0.1 & 71.7  0.4 & 86.3  0.1 & 91.8  0.3 & 72.0  0.3 & 79.0  0.2 & 90.2  0.1 & 90.6  0.1\\
     Margin+\textit{S2SD}                  & 67.6    0.3 & 78.2  0.2 & 70.8  0.3 & 86.0  0.2 & 91.8  0.2 & 72.2  0.2 & \textbf{80.2}  \textbf{0.2} & \textbf{91.5}  \textbf{0.1} & \textbf{90.9}  \textbf{0.1}\\
     R-Margin+\textit{S2SD}                & \textbf{68.9}    \textbf{0.3} & \textbf{79.0}  \textbf{0.3} & \textbf{72.1}  \textbf{0.4} & \textbf{87.6}  \textbf{0.2} & \textbf{92.7}  \textbf{0.2} & \textbf{72.3}  \textbf{0.2}& 79.2  0.2 & 90.3  0.1 & 90.8  0.1\\
     \midrule
     \hline
\multicolumn{10}{>{\columncolor[gray]{.8}}l}{\textbf{ResNet50-512}} \\
     \hline
     \rowcolor{vvlightgray}
     EPSHN \citep{epshn}                     & 64.9 & 75.3 &  -   & 82.7 & 89.3 &  -  & 78.3 & 90.7 &  -    \\
     \rowcolor{vvlightgray}
     NormSoft \citep{zhai2018classification} & 61.3 & 73.9 &  -   & 84.2 & 90.4 &  -   & 78.2 & 90.6 &  -    \\
     \rowcolor{vvlightgray}
     DiVA \citep{milbich2020diva}            & 69.2 & 79.3 & 71.4 & 87.6 & 92.9 & 72.2 & 79.6 & 91.2 & 90.6 \\
     \hline
     Multisimilarity+\textit{S2SD}          & 69.2  0.1 & 79.1  0.2 & 71.4  0.2 & 89.2  0.2 & 93.8  0.2 & \blue{\textbf{74.0}  \textbf{0.2}} & 80.8  0.2 & \blue{\textbf{92.2}  \textbf{0.2}} & 90.5  0.3\\
     Margin+\textit{S2SD}                   & 68.8  0.2 & 78.5  0.2 & \blue{\textbf{72.3}  \textbf{0.1}} & 89.3  0.2 & 93.8  0.2 & 73.7  0.3 & \blue{\textbf{81.0}  \textbf{0.2}} & 92.1  0.2 & \textbf{91.1}  \textbf{0.3}\\
     R-Margin+\textit{S2SD}                 & \blue{\textbf{70.1}  \textbf{0.2}} & \blue{\textbf{79.7}  \textbf{0.2}} & 71.6  0.2 & \blue{\textbf{89.5}  \textbf{0.2}} & \blue{\textbf{93.9}  \textbf{0.3}} & 72.9  0.3 & 80.0  0.2 & 91.4  0.2 & 90.8  0.1\\
     \midrule
     \hline
\multicolumn{10}{>{\columncolor[gray]{.8}}l}{\textbf{Inception-BN-512}} \\
     \hline
\rowcolor{vvlightgray}
     DiVA \citep{milbich2020diva}     & 66.8 & 77.7 & 70.0 & 84.1 & \textbf{90.7} & 68.7 & 78.1 & \textbf{90.6} & 90.4\\
     \hline
     Multisimilarity+\textit{S2SD}   & 66.7  0.3 & 77.5  0.3 & \textbf{70.5}  \textbf{0.2} & 83.8  0.3 & 90.3  0.2 & \textbf{69.8}  \textbf{0.3} & \textbf{78.5}  \textbf{0.2} & \textbf{90.6}  \textbf{0.2} & \textbf{90.6}  \textbf{0.1}\\
     Margin+\textit{S2SD}            & 66.8  0.2 & 77.9  0.2 & 69.9  0.3 & \textbf{84.3}  \textbf{0.2} & \textbf{90.7}  \textbf{0.2} & \textbf{69.8}  \textbf{0.2} & 78.4  0.2 & 90.5  0.2 & 90.4  0.1\\
     R-Margin+\textit{S2SD}          & \textbf{67.4}  \textbf{0.3} & \textbf{78.0}  \textbf{0.4} & 70.3  0.2 & 83.9  0.3 & 90.3  0.2 & 69.4  0.2 & 78.1  0.2 & 90.4  0.3 & 90.3  0.2\\
     \hline
     \rowcolor{vvlightgray}
     Softtriple \citep{softriple} & 65.4 & 76.4 & 69.3 & 84.5 & 90.7 & 70.1 & 78.3 & 90.3 & \blue{\textbf{92.0}}\\
     \rowcolor{vvlightgray}
     Multisimilarity \citep{multisimilarity} & 65.7 & 77.0 & - & 84.1 & 90.4 & -  & 78.2 & 90.5 & -   \\
     \hline
     Multisimilarity+\textit{S2SD}             & 68.2  0.3 & 79.1  0.2 & \textbf{71.6}  \textbf{0.2} & 86.3  0.2 & 92.2  0.2 & 72.0  0.3 & 78.9  0.2 & 90.8  0.2 & 90.6  0.1\\
     Margin+\textit{S2SD}                      & 68.3  0.2 & 78.8  0.2 & 71.2  0.2 & \textbf{87.1}  \textbf{0.2} & \textbf{92.4}  \textbf{0.1} & \textbf{72.2}  \textbf{0.2} & \textbf{79.1}  \textbf{0.2} & \textbf{91.0}  \textbf{0.3} & 90.4  0.1\\
     R-Margin+\textit{S2SD}                    & \textbf{69.6}  \textbf{0.3} & \textbf{79.6}  \textbf{0.3} & 71.2  0.1 & 86.6  0.3 & 92.1  0.3 & 70.9  0.2 & 78.5  0.1 & 90.5  0.2 & 90.0  0.2\\
     \bottomrule
\end{tabular}}

\label{tab:sota}
\end{table*}






































 Motivated by Tab. \ref{tab:relative_results}, we use \textit{MSDFA} for CUB200/CARS196 and \textit{MSDF} for SOP. Table \ref{tab:sota} shows that \textit{S2SD} can boost baseline objectives to reach and even surpass SOTA, in parts with a notable margin, even when reported with confidence intervals, which is commonly neglected in DML. \textit{S2SD} outperforms much more complex methods with feature mining or RL-policies s.a. MIC \citep{mic}, DiVA \citep{milbich2020diva} or PADS \citep{roth2020pads}. 

\subsection{\textit{S2SD} is a strong substitute for normal distillation \& learns generalizing embedding spaces across dimensionalities.}\label{sec:further_exp}
\textbf{Comparison to standard distillation.} 
With student \textit{S} (same objective/embed. 
dim. as the reference branch in \textit{DSD})
and a teacher \textit{T} at highest optimal dim. , 
we find separating \textit{DSD} into standard 2-stage distillation degenerates performance (see Fig. \ref{fig:ablations_1}A, compare to \textit{Dist.}). 
\textit{S2SD} also allows for easy integration of teacher ensembles, realized by \textit{MSD(F,A)}, to even outperform the teacher notably \textit{while}
operating on the embedding dimensionality of the student.

\textbf{Benefits to lower base dimensions.} We show that our module is able to vastly boost networks limited to very low embedding dimensions (c.f. \ref{fig:ablations_1}B). 
For example,  networks trained with \textit{S2SD} can match the performance of embed. dimensions \textit{four or eight times} the size. For , \textit{S2SD} even outperforms the highest dimensional baseline at  notably.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{images/metrics_2.png}
    \caption{\textit{Generalization metrics.} \textit{S2SD} increases embedding space density and lowers spectral decay.}
    \label{fig:metrics}
\end{figure}
\vspace{-10pt}     \textbf{Embedding space metrics.} 
We now look at relative changes in embedding space density
and spectral decay as in \cite{roth2020revisiting}, although we investigate changes within the same objectives.
Fig. \ref{fig:metrics} shows \textit{S2SD} increasing embedding space density and lowering the spectral decay (hence providing a more feature-diverse embedding space) across criteria. 

\subsection{Motivating S2SD Architecture Choices}\label{sec:ablations}
\textbf{Is distillation in \textit{S2SD} important?} Fig. \ref{fig:ablations_1}A (\textit{Joint}) and Fig. \ref{fig:ablations_1}F () highlight how crucial self-distillation is, as using a secondary embedding space without hardly improves performance. Fig. \ref{fig:ablations_1}A (\textit{Concur.}) shows that joint training of a detached reference embedding  while otherwise training in high dimension also doesn't offer notable improvement. Finally, Figure \ref{fig:ablations_1}F shows robustness to changes in , with peaks around  and  for CUB200/CARS196 and SOP. We also found best performance for temperatures  and hence set  by default.

\textbf{Best way to enforce reusability.} To motivate our many-to-one self-distillation  (eq. \ref{eq:multi}, here also dubbed ), we evaluate against other distillation setups that could support reusability of distilled sample relations: 
\textit{(1)} \textit{Nested} distillation, where instead of distilling all target spaces only to the reference space, we distill from a target space to \textit{all} lower-dimensional embedding spaces:

In the second term,  denotes the base embedding . 
\textit{(2)} \textit{Chained} distillation, which distills from a target space only to the lower-dim. embedding space closest in dimensionality:

Figure \ref{fig:ablations_1}E shows that while either distillation method provides strong benefits, a many-to-one distillation performs notably better, supporting the reusability aspect and  as our default method.

\textbf{Choice of distillation method \& branch structures.} Fig.  \ref{fig:ablations_1}C evaluates various distillation objectives, finding KL-divergence between vectors of similarities to perform better than KL-divergence applied over full similarity matrices or row-wise means thereof, as well as cosine/euclidean distance-based distillation (see e.g. \citep{Yu_2019}). 
Figure \ref{fig:ablations_1}D shows insights into optimal auxiliary branch structures, with two-layer MLPs giving the largest benefit, although even a linear target mapping reliably boosts performance. This coincides with insights made by \cite{chen2020simple}. Further network depth only deteriorates performance.  


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/Ablations.png}
    \caption{\textit{S2SD study and ablations.} \textbf{(A)} \textit{DSD} outperforms comparable two-stage distillation on student \textit{S} (\textit{Dist.}) using teacher (\textit{T}), with \textit{MSD(FA)} even outperforming \textit{T}. We further see that distillation is essential - training multiple spaces in parallel (\textit{Joint.}) or a detached lower-dimensional base embedding (\textit{Concur.}) gives little benefit. \textbf{(B)} We see benefits across base dimensionalities, especially in the low-dimensional regime. \textbf{(C)} We find KL-distillation between similarity vectors (\textit{R-KL}) to work best. \textbf{(D)} An additional non-linearity in aux. branches  gives a boost, but going deeper hurts generalization. \textbf{(E)} Distilling each aux. embed. space (\textit{Multi}) separately compares favourable against other distillation setups s.a. \textit{Nested} and \textit{Chained} distillation. \textbf{(F)} Performance is robust to changes in weight values.}
    \label{fig:ablations_1}
\end{figure*}
 \section{Conclusion}
In this paper, we propose a novel knowledge-distillation based DML training paradigm, \textit{Simultaneous Similarity-based Self-Distillation} (\textit{S2SD)}, to utilize high-dimensional context for improved generalization. 
\textit{S2SD} solves the standard DML objective simultaneously in higher-dimensional embedding spaces while applying knowledge distillation concurrently between these high-dimensional teacher spaces and a lower-dimensional reference space. \textit{S2SD} introduces little additional computational overhead, with no extra cost at test time. Thorough ablations and experiments show \textit{S2SD} significantly improving the generalization performance of existing DML objectives regardless of embedding dimensionality, while also setting a new state-of-the-art on standard benchmarks.

\newpage
\section*{Acknowledgements}
We would like to thank Samarth Sinha (University of Toronto, Vector), Matthew McDermott (MIT) and Mengye Ren (University of Toronto, Vector) for insightful discussions and feedback on the paper draft. This work was funded in part by a CIFAR AI Chair at the Vector Institute, Microsoft Research, and an NSERC Discovery Grant. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute \url{www.vectorinstitute.ai/#partners}.

\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\newpage
\appendix
\onecolumn
\icmltitle{Supplementary: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning}

\section{More Benchmark \& Implementation Details}\label{supp:bench_dets}
In this part, we report all relevant benchmark details omitted in the main document as well as further implementation details.\\
\subsection{Benchmarks}
\textbf{CUB200-2011} \citep{cub200-2011} contains 200 bird classes over 11,788 images, where the first and last 100 classes with 5864/5924 images are used for training and testing, respectively.\\ 
\textbf{CARS196} \citep{cars196} contains 196 car classes and 16,185 images, where again the first and last 98 classes with 8054/8131 images are used to create the training/testing split.\\ 
\textbf{Stanford Online Products (SOP)} \citep{lifted} is build around 22,634 product classes over 120,053 images and contains a provided split: 11318 selected classes with 59551 images are used for training, and 11316 classes with 60502 images for testing.
\subsection{Implementation}\label{supp:impl_dets}
We now provide further details regarding the training and testing setup utilized. For any study except the comparison against the state-of-the-art (Table \ref{tab:sota}) which uses different backbones and embedding dimensions, we follow the setup used by \cite{roth2020revisiting}\footnote{Repository: \url{github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch}}: This includes a ResNet50 \cite{resnet} with frozen Batch-Normalization \cite{batchnorm}, normalization of the output embeddings with dimensionality  and optimization with Adam \cite{adam} using a learning rate of  and weight decay of . The input images are randomly resized and cropped from the original image size to  for training. Further augmentation by random horizontal flipping with  is applied. During testing, center crops of size  are used. The batchsize is set to .

Training runs on CUB200-2011 and CARS196 are done over 150 epochs and 100 epochs for SOP for all experiments without any learning rate scheduling, except for the state-of-the-art experiments (see again \ref{tab:sota}). For the latter, we made use of slightly longer training to account for conservative learning rate scheduling, which is similarly done across reference methods noted in tab. \ref{tab:sota}. Schedule and decay values are determined over validation subset performances.
All baseline DML objectives we apply our self-distillation module \textit{S2SD} on use the default parameters noted in \cite{roth2020revisiting} with the single exception of Margin Loss on SOP, where we found class margins  to be more beneficial for distillation than the default . This was done as changing from  to  had no notable impact on the baseline performance.
Finally, similar to \cite{kim2020proxy}, we found a warmup epoch of all MLPs to improve convergence on SOP. Spectral decay computations in \S\ref{sec:further_exp} follow the setting described in Supp. \ref{supp:gen_metrics}.

We implement everything in PyTorch \citep{pytorch}. Experiments are done on GPU servers containing Nvidia Titan X, P100 and T4s, however memory usage never exceeds 12GB.
Each result is averaged over five seeds, and for the sake of reproducibilty and result validity, we report mean and standard deviation, even though this is commonly neglected in DML literature.


\newpage
\section{Baseline Methods}\label{supp:base_methods}
This section provides a more detailed explanation of the DML baseline objectives we used alongside our self-distillation module \textit{S2SD} in the experimental section \ref{sec:experiments}. For additional details, we refer to the supplementary material in \cite{roth2020revisiting}. For the mathematical notation, we refer to Section \ref{sec:prelim}. We use  to denote the feature network  with embedding , and  the embedding of a sample . Finally, alongside the method descriptions we provide the used hyperparameters.

\textbf{Margin Loss} \citep{margin} builds on triplet/pair-based losses, but introduces both class-specific, learnable boundaries  (with number of classes ) between positive and negative pairs, as well as distance-based sampling for negatives:


where  denotes the available pairs in minibatch , and  the embedding dimension. Throughout this work, we use  except for \textit{S2SD} on SOP, where we found  to work better without changing the baseline performance. We set the learning rate for the class boundaries as , and margin .

\textbf{Regularized Margin Loss} \citep{roth2020revisiting} proposes a simple regularization scheme on the margin loss that increases the number of directions of significant variance in the embedding space by randomly exchanging a negative sample with a positive one with probability . For ResNet-backbones, we use  for CUB200,  for CARS196 and  for SOP as done in \cite{roth2020revisiting}. For Inception-based backbones, we set  for CUB200 and CARS196 and  for SOP.

\textbf{Multisimilarity Loss} \cite{multisimilarity} incorporates more similarities into training by operating directly on all positive and negative samples for an anchor , while also incorporating a sampling operation that encourages the usage of harder training samples:



where  denotes the cosine similarity instead of the euclidean distance, and  the set of positives and negatives for  in the minibatch, respectively. We use the default values , ,  and .



\section{Evaluation Metrics}\label{supp:eval_metrics}
The evaluation metrics used throughout this work are recall @ 1 (R@1), recall @ 2 (R@2) and Normalized Mutual Information (NMI), capturing two distinct embedding space properties.

\textbf{Recall@K}, see e.g. in \cite{recall}, especially Recall@1 and Recall@2, is the primary metric used to compare the performance of DML methods and approaches, as it offers strong insights into retrieval performances of the learned embedding spaces. Given the set of embedded samples  with  and , and the sorted set of  nearest neighbours for any sample ,

Recall@K is measured as

which evaluates how likely semantically corresponding pairs (as determined here by the labelling ) will occur in a neighbourhood of size .


\textbf{Normalized Mutual Information (NMI)}, see \cite{nmi}, evaluates the clustering quality of the embedded samples  (taken from ). It is computed by first clustering with  cluster centers, usually corresponding to the number of classes available, using a cluster method of choice s.a. K-Means \citep{kmeans}. This assigns each sample  a cluster label/id  based on the nearest cluster centroid. With  the set of samples with cluster label,  the set of cluster sets,  the set of samples with true label  and  the set of class label sets, the Normalized Mutual Information is given as

with mutual information  and entropy .


\section{Generalization Metrics}\label{supp:gen_metrics}
\textbf{Embedding Space Density.} Given sets of embeddings , we first define the average inter-class distance as

which measures the average distances between groups of embeddings with respective classes  and , estimated by the respective class centers .  denotes a normalization constant based on the number of available classes. We also introduce the average intra-class distance as the mean distance between samples within their respective class

again with normalization constant  and set of embeddings with class , . Given these two quantities, the embedding space density is then defined as 

and effectively measured how densely samples and classes are grouped together. \cite{roth2020revisiting} show that optimizing the DML problem while keeping the embedding space density high, i.e. without aggressive clustering, encourages better generalization to unseen test classes.

\textbf{Spectral Decay.}
The spectral decay metric  defines the KL-divergence between the (sorted) spectrum of  singular values  (obtained via Singular Value Decomposition (SVD)) and a -dimensional uniform distribution , and is inversly related to the entropy of the embedding space:

It does not account for class distributions. \cite{roth2020revisiting} show that doing DML while encouraging a high-entropy feature space notably benefits the generalization performance. In our experiments, we disregard the first 10 singular vectors (out of 128) to
highlight the feature diversity. This is important, as we evaluate the spectral decay within the same objectives, which results in the first few singular values to be highly similar.


\newpage
\section{Additional Experiments}\label{supp:add_exps}
This part extends the set of ablations experiments performed in section \ref{sec:ablations} in the main paper.\\
\textbf{a. Detaching target spaces for distillation.} We examine whether it is preferable to detach the target embeddings from the distillation loss (see eq. \ref{eq:kl_distill}), as we want the reference embedding space to approximate the higher-dimensional relations. Similarly, we do not want the target embedding networks  to reduce high-dimensional to lower-dimensional relations to optimizer for the distillation constraint. As can be seen in fig \ref{fig:ablations_2}C, it is indeed the case that detaching the target embedding spaces is notably beneficial for a stronger reference embedding, supporting the previous motivation.\\
\textbf{b. Influence of varying target dimensions.} As noted at the beginning of section \ref{sec:experiments}, we set the target dimension for dual self-distillation (\textbf{DSD}) to , which we motivate through a small ablation study in fig. \ref{fig:ablations_2}A, with \textit{TD} denoting the target dimension of choice. As can be seen, benefits plateau when the target dimension reaches more than four times the reference dimension. However, to be directly comparable to high-dimensional reference settings, we set  as default.\\
\textbf{c. Ablating multiple distillation scales.} Going further, we extend the module with additional embedding branches to the multiscale self-distillation approach (\textbf{MSD}), all operating in different, but higher-than-reference dimension. As already shown in Figure \ref{fig:ablations_1}B in the main paper, there is a benefit of multiscale distillations by encouraging reusable sample relations. In this part, we motivate the choice of four target branches (as noted in sec. \ref{sec:experiments}). Looking at figure \ref{fig:ablations_2}A, where  denotes the number of additional target spaces, we can see a benefit in multiple additional target spaces of ascending dimension. As the improvements saturate after , we simply set this as the default value. However, the additional benefits of going to multiscale from dual distillation are not as high as going from no to dual target space distillation, showcasing the general benefit of high-dimensional concurrent self-distillation. Finally, we highlight that a multiscale approach slightly outperforms a multibranch distillation setup (Fig. \ref{fig:ablations_2}A, \textit{Multi-B}) where each target branch has the same target dimension of  while introducing less additional parameters.\\
\textbf{d. Finer-grained feature distillation.} As already shown in section \ref{sec:experiments} and again in figure \ref{fig:ablations_2}B, we see benefits of feature distillation, using the (globally averaged) normalized penultimate feature space. It therefore makes sense to investigate the benefits of distilling even more fine-grained feature representation. Defining  as the pooling window size applied to the non-average penultimate feature representation, we investigate less compressed feature representation space. As can be seen in fig. \ref{fig:ablations_2}B, where  denotes the index to , there appears to be no benefits in distilling feature representations higher up the network.\\ 
\textbf{e. Runtime comparison of base dimensionalities.} We highlight relative retrieval times at different base dimensionalities in Tab. \ref{tab:test_time} using \textsf{faiss} \citep{faiss} on a NVIDIA 1080Ti and a synthetic set of  embeddings of dimensionality . With \textit{S2SD} matching  to base dimensionalities  (see \S\ref{sec:further_exp}), runtime can be reduced by up to a magnitude.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{images/Ablations_no2.png}
\caption{\small{\textit{Additional ablations.} \textbf{(A)} Increasing target dimensions offers notable improvements. We opt for a target dimension of 2048 due to slightly higher mean improvements. For multiple embedding branches (\#B), there seems to be an optimum at four branches. \textbf{(B)} Furthermore, feature distillation gives another notable boost. However, this only holds for the globally averaged penultimate feature representation. When distilling more fine-grained feature representations, performance degenerates (where \#P denotes smaller pooling windows applied to the penultimate feature representation). \textbf{(C)} We show that detached auxiliary branches for distillation are crucial to higher improvements, as we want the reference embedding space to approximate the higher-dimensional one.}}
\label{fig:ablations_2}
\end{figure}



 
\begin{table}[h!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c|c|c|c}
    \toprule
         \textbf{Dimensionality}  & 32   &  64  &  128 &  256 &  512 &  1024 & 2048  \\
         \hline
         \textbf{Runtime} (\textit{s})        & 1.540.00 & 1.980.00 & 2.710.00 & 4.350.00 & 7.380.01 & 13.830.02 & 27.210.17 \\
    \bottomrule
    \end{tabular}}
    \caption{\small{Sample retrieval times for 250000 embeddings with varying base dimensionalities.}}
    \label{tab:test_time}
\end{table}

\newpage
\section{Pseudo-Code}\label{supp:pseudo_code}
\begin{lstlisting}[language=Python, morekeywords={self,nn,norm,F}, caption=PyTorch Implementation for \textit{S2SD}.]
import torch, torch.nn as nn, torch.nn.functional as F
from F import normalize as norm

"""
Parameters:
    self.base_criterion: base DML objective
    self.trgt_criteria: list of DML objectives for target spaces
    self.trgt_nets: Module list of auxiliary embedding MLPs
    self.dist_gamma: distillation weight
    self.it_before_feat_distill: iterations before feature distill  
"""

def forward(self, batch, labels, pre_batch, **kwargs):
    """
    Args:
        batch: image embeddings, shape: bs x d
        labels: image labels, shape: bs
        pre_batch: penultimate network features, shape: bs x d*
    """
    bs, batch = len(batch), norm(batch, dim=-1)

    ### Compute ref. sample relations and loss on ref. embedding space
    base_smat = batch.mm(batch.T)
    base_loss = self.base_criterion(batch, labels, **kwargs)

    ### Do global average pooling (and max. pool if wanted)
    avg_pre_batch  = nn.AdaptiveAvgPool2d(1)(pre_batch).view(bs,-1)
    avg_pre_batch += nn.AdaptiveMaxPool2d(1)(pre_batch).view(bs,-1)

    ### Computing MSDA loss (Targets & Distillations)
    dist_losses, trgt_losses  = [], []
    for trgt_crit,trgt_net in zip(self.trgt_criteria,self.trgt_nets):
        trgt_batch     = norm(trgt_net(avg_pre_batch),dim=-1)
        trgt_loss      = trgt_crit(trgt_batch, labels, **kwargs)
        trgt_smat      = trgt_batch.mm(trgt_batch.T)
        base_trgt_dist = self.kl_div(base_smat, trgt_smat.detach())
        trgt_losses.append(trgt_loss)
        dist_losses.append(base_trgt_dist)

    ### MSDA loss
    multi_dist_loss  = (base_loss+torch.stack(trgt_losses).mean())/2.
    multi_dist_loss += self.dist_gamma*torch.stack(dist_losses).mean()

    ### Distillation of penultimate features -> MSDFA 
    src_feat_dist = 0
    if self.it_count>=self.it_before_feat_distill:
        n_avg_pre_batch = norm(avg_pre_batch, dim=-1).detach()
        avg_feat_smat   = n_avg_pre_batch.mm(n_avg_pre_batch.T)
        src_feat_dist   = self.kl_div(base_smat, avg_feat_smat.detach())

    ### Total S2SD training objective 
    total_loss = multi_distill_loss + self.dist_gamma*src_feat_dist
    self.it_count+=1
    return total_loss

def kl_div(self, A, B, T=1):
    log_p_A = F.log_softmax(A/self.T, dim=-1)
    p_B     = F.softmax(B/self.T, dim=-1)
    kl_d    = F.kl_div(log_p_A, p_B,reduction='sum')*T**2/A.size(0)
    return kl_d
\end{lstlisting}

\newpage
\section{Detailed Evaluation Results}\label{supp:detailed_1}
This table contains all method ablations for a fair evaluation as used in Section \ref{sec:comp} and Table \ref{tab:relative_results}.
\begin{table}[h]
\caption{\textit{Detailed Comparison of Recall@1 and NMI performances against well performing DML objectives examined in section \ref{sec:comp}.} This is the complete version to table \ref{tab:relative_results}. All results are computed over 5-run averages. () For Margin Loss and SOP, we found  to give better distillation results without notably influencing baseline performance.}
 \footnotesize
   \setlength\tabcolsep{1.4pt}
   \centering
\begin{tabular}{l|c|c||c|c||c|c}
     \toprule
     \multicolumn{1}{l}{\textbf{Benchmarks}} & \multicolumn{2}{c}{\textsc{CUB200-2011}} & \multicolumn{2}{c}{\textsc{CARS196}} & \multicolumn{2}{c}{\textsc{SOP}} \\
     \midrule
     \textbf{Approaches}  & R@1 & NMI & R@1 & NMI & R@1 & NMI\\
    \midrule
    \textbf{Margin}() &  &  &  &  &  & \\        
    + DSD &  &  &  &  &  & \\
    + DSDA &  &  &  &  &  & \\
    + MSD &  &  &  &  &  & \\
    + MSDA &  &  &  &  &  & \\
    + MSDF &  &  &  &  &  & \\
    + MSDFA &  &  &  &  &  & \\
    
    \midrule
    \textbf{R-Margin} &  &  &  &  &  & \\ 
    + DSD &  &  &  &  &  & \\
    + DSDA &  &  &  &  &  & \\
    + MSD &  &  &  &  &  & \\
    + MSDA &  &  &  &  &  & \\
    + MSDF &  &  &  &  &  & \\
    + MSDFA &  &  &  &  &  & \\
    \midrule
    \textbf{Multisimilarity} &  &  &  &  &  & \\
    + DSD &  &  &  &  & & \\
    + DSDA &  &  &  &  &  & \\
    + MSD &  &  &  &  &  & \\
    + MSDA &  &  &  &  &  & \\
    + MSDF &  &  &  &  &  & \\
    + MSDFA &  &  &  &  &  & \\
         
\bottomrule
    \end{tabular}
    \label{tab:relative_results_long}
 \end{table}




 
\section{Evaluation Results using mAP@R}\label{supp:detailed_map}
This table measures performance of methods investigated in Table \ref{tab:relative_results} using the mAP@R(@1000) metric used in \cite{roth2020revisiting}. The results here coincide with those measured using Recall@1. This comes at no surprise, as both metrics are strongly correlated when measuring the performance of Deep Metric Learning methods \citep{roth2020revisiting}.
\begin{table}[h]
\caption{\textit{Detailed Comparison of mAP@R} (as used in \cite{roth2020revisiting} and \cite{musgrave2020metric} and based on the formulation used in \cite{roth2020revisiting}) against well performing DML objectives examined in section \ref{sec:comp}.. All results are computed over 5-run averages. () For Margin Loss and SOP, we found  to give better distillation results without notably influencing baseline performance. \textbf{Bold} denotes best results per objective and dataset. \blue{\textbf{Bluebold}} denotes best performance per dataset.}
 \footnotesize
   \setlength\tabcolsep{1.4pt}
   \centering
\begin{tabular}{l|c||c||c}
     \toprule
     \multicolumn{1}{l}{\textbf{Benchmarks}} & \multicolumn{1}{c}{\textsc{CUB200-2011}} & \multicolumn{1}{c}{\textsc{CARS196}} & \multicolumn{1}{c}{\textsc{SOP}} \\
     \midrule
     \textbf{Approaches}  & mAP & mAP & mAP\\
    \midrule
    \rowcolor{vvlightgray}
    \textbf{Margin}() &  &  &  \\        
    + DSD &  &  & \\
    + MSD &  &  & \\
    + MSDF &  &  & \blue{}\\
    + MSDFA &  &  & \\
    \midrule
    \rowcolor{vvlightgray}
    \textbf{R-Margin} &  &  & \\ 
    + DSD &  &  & \\
    + MSD &  &  & \\
    + MSDF &  &  & \\
    + MSDFA & \blue{} & \blue{} & \\
    \midrule
    \rowcolor{vvlightgray}
    \textbf{Multisimilarity} &  &  & \\
    + DSD &  &  & \\
    + MSD &  &  & \\
    + MSDF &  &  & \\
    + MSDFA &  &  & \\
         

    \bottomrule
    \end{tabular}
    \label{tab:relative_results_map}
 \end{table}
 

 
 
\begin{table}[ht!]
\centering
\caption{Additional ProxyAnchor \citep{kim2020proxy} results with and without \textit{S2SD} variants using the proposed, but different, default architecture in \citep{kim2020proxy} to highlight that \textit{S2SD} works equally well on already strong proxy-based objectives objectives with different architectural settings as well.}
\centering
\resizebox{0.95\textwidth}{!}{   
\begin{tabular}{l||c|c||c|c||c|c}
 \toprule
 \multicolumn{1}{l}{\textbf{Benchmarks}} & \multicolumn{2}{c}{\textsc{CUB200-2011}} & \multicolumn{2}{c}{\textsc{CARS196}} & \multicolumn{2}{c}{\textsc{SOP}} \\
\midrule
 \textbf{Setting} & R@1 & NMI & R@1 & NMI & R@1 & NMI \\
\midrule     
\rowcolor{vvlightgray}
\textbf{ProxyAnchor} &  &  &  &  &  & \\    
+ DSD &  &  &  &  &  & \\
+ MSD &  &  &  &  &  & \\
+ MSDF &  &  &  &  &  & \\
\end{tabular}}
\label{tab:proxyanchor_results}
\end{table} 
\section{Detailed Ablation Results}\label{supp:detailed_2}
Detailed values to the ablation experiments done in section \ref{sec:ablations} and \ref{supp:add_exps}.






 
 
 
\begin{table}[h]
 \small
   \centering
    \caption{\textit{Experiment: Comparison of concurrent self-distillation against standard 2-stage distillation.} This table also shows that training without distillation (\textit{Joint}) or training in high dimension while learning a detached low-dimensional embedding layer (\textit{Concur.}) does not benefit performance notably. See fig. \ref{fig:ablations_1}A. All results are computed over 5-run averages.}   
   \begin{tabular}{l||c|c}
     \toprule
     Experiment & Setting & R@1\\
     \midrule
     \multirow{9}{*}{Distillation} & Best Teacher (d=1024) & \\
     & Base Student (d=128) & \\
     & Distill Student (d=128) & \\
     & Concur. Student (d=128) & \\
     & Joint Student (d=128) & \\
     & DSD   (d=128) & \\
     & DSDA  (d=128) & \\
     & MSDA (d=128) & \\
     & MSDFA (d=128) & \\
    \bottomrule
    \end{tabular}
    \label{tab:ablations_6}
 \end{table}



\begin{table}[h]
 \small
   \centering
    \caption{\textit{Experiment: Benefit of self-distillation across embedding dimensionalities.} These results go along with \ref{fig:ablations_1}B. All results are computed over 5-run averages.}   
   \begin{tabular}{l||c|c|c|c}
     \toprule
     Experiment & Setting & R@1 & Setting & R@1\\
     \midrule
\multirow{28}{*}{Embedding Dimensionality} & Base (d=16) & & MSD (d=256) & \\
 & Basic (d=32) & & MSD (d=512) & \\     
 & Basic (d=64) & & MSD (d=1024) & \\ 
 & Basic (d=128) & & MSD (d=2048) & \\ 
 & Basic (d=256) & & MSDA (d=16) & \\ 
 & Basic (d=512) & & MSDA (d=32) & \\ 
 & Basic (d=1024) & & MSDA (d=64) & \\ 
 & Basic (d=2048) & & MSDA (d=128) & \\ 
 & DSD (d=16) & & MSDA (d=256) & \\ 
 & DSD (d=32) & & MSDA (d=512) & \\ 
 & DSD (d=64) & & MSDA (d=1024) & \\ 
 & DSD (d=128) & & MSDA (d=2048) & \\ 
 & DSD (d=256) & & MSDF (d=16) & \\ 
 & DSD (d=512) & & MSDF (d=32) & \\ 
 & DSD (d=1024) & & MSDF (d=64) & \\ 
 & DSD (d=2048) & & MSDF (d=128) & \\ 
 & DSDA (d=16) & & MSDF (d=256) & \\ 
 & DSDA (d=32) & & MSDF (d=512) & \\ 
 & DSDA (d=64) & & MSDF (d=1024) & \\ 
 & DSDA (d=128) & & MSDF (d=2048) & \\ 
 & DSDA (d=256) & & MSDFA (d=16) & \\ 
 & DSDA (d=512) & & MSDFA (d=32) & \\ 
 & DSDA (d=1024) & & MSDFA (d=64) & \\ 
 & DSDA (d=2048) & & MSDFA (d=128) & \\ 
 & MSD (d=16) & & MSDFA (d=256) & \\ 
 & MSD (d=32) & & MSDFA (d=512) & \\ 
 & MSD (d=64) & & MSDFA (d=1024) & \\ 
 & MSD (d=128) & & MSDFA (d=2048) & \\ 
    \bottomrule
    \end{tabular}
    \label{tab:ablations_5}
 \end{table}
 
 
\begin{table}[h]
 \small
   \centering
    \caption{\textit{Experiment: Methods of distillation between reference and target embedding spaces.} See fig. \ref{fig:ablations_1}C. Used Method: \textbf{DSDA}. All results are computed over 5-run averages.}   
   \begin{tabular}{l||c|c}
     \toprule
     Experiment & Setting & R@1\\
     \midrule
\multirow{7}{*}{Distillation Methods} & R-KL & \\
 & Cos & \\
 & Eucl & \\
 & KL-Full & \\
 & KL-Mean & \\
 & Basic & \\
    \bottomrule
    \end{tabular}
    \label{tab:ablations_3}
 \end{table}
 
 
\begin{table}[h]
 \small
   \centering
    \caption{\textit{Experiment: Structure of the secondary branch.} More specifically, this table contains specific values used in fig. \ref{fig:ablations_1}D. Used Method: \textbf{DSDA}. All results are computed over 5-run averages.}   
   \begin{tabular}{l||c|c}
     \toprule
     Experiment & Setting & R@1\\
     \midrule
\multirow{5}{*}{Secondary Branch Structure} & 2 Layers & \\
 & 3 Layers & \\
 & 4 Layers & \\
 & Linear & \\
 & Basic & \\
    \bottomrule
    \end{tabular}
    \label{tab:ablations_2}
 \end{table}
 

\begin{table}[h]
 \small
   \centering
    \caption{\textit{Experiment: Different distillation hierarchies.} See fig. \ref{fig:ablations_1}E. Used Method: \textbf{MSDA}. All results are computed over 5-run averages.}   
   \begin{tabular}{l||c|c}
     \toprule
     Experiment & Setting & R@1\\
     \midrule
\multirow{4}{*}{Distillation Hierarchies} & Basic & \\
 & Straight & \\
 & Fully & \\
 & Stacked & \\
    \bottomrule
    \end{tabular}
    \label{tab:ablations_10}
 \end{table}
 
 
\begin{table}[h]
 \small
   \centering
    \caption{\textit{Experiment: Influence of distillation weight .} See fig. \ref{fig:ablations_1}F. Used Method: \textbf{DSD}. All results are computed over 5-run averages.}   
   \begin{tabular}{l||c|c|c|c}
     \toprule
     Experiment & Setting & R@1 CUB200 & R@1 CARS196 & R@1 SOP\\
     \midrule
    \multirow{10}{*}{Weight Ablation} & 0.0 &  &  & \\
     & 0.2 &  &  & \\
     & 1.0 &  &  & \\
     & 5.0 &  &  & \\
     & 20.0 &  &  & \\
     & 50.0 &  &  & \\
     & 250.0 &  &  & \\
     & 1000.0 &  &  & \\
     & 2000.0 &  &  & \\
     & 5000.0 &  &  & \\  

    \bottomrule
    \end{tabular}
    \label{tab:ablations_4}
 \end{table}



 
\begin{table}[h]
 \small
   \centering
    \caption{\textit{Experiment: Evaluation target dimensions and levels of multiscale distillation.} See fig. \ref{fig:ablations_2}A. All results are computed over 5-run averages.}   
   \begin{tabular}{l||c|c}
     \toprule
     Experiment & Setting & R@1\\
     \midrule
\multirow{5}{*}{Target Dimensionalities} & Basic & \\
 & DSDA, TD=256 & \\
 & DSDA, TD=512 & \\
 & DSDA, TD=1024 & \\
 & DSDA, TD=2048 & \\
\multirow{4}{*}{MultiScale distillation} & MSDA, \#B=2 & \\
 & MSDA, \#B=4 & \\
 & MSDA, \#B=8 & \\
 & MSDA, \#B=16 & \\
    \bottomrule
    \end{tabular}
    \label{tab:ablations_7}
 \end{table}



\begin{table}[h]
 \small
   \centering
    \caption{\textit{Experiment: Is it beneficial to distill more fine-grained features?} See fig. \ref{fig:ablations_2}B. All results are computed over 5-run averages.}
    \label{tab:ablations_8}   
   \begin{tabular}{l||c|c}
     \toprule
     Experiment & Setting & R@1\\
     \midrule
\multirow{7}{*}{Earlier Features} & Basic & \\
 & MSDA & \\
 & MSDFA & \\
 & MSDFA, \#P=1 & \\
 & MSDFA, \#P=2 & \\
 & MSDFA, \#P=3 & \\
 & MSDFA, \#P=4 & \\
    \bottomrule
    \end{tabular}
 \end{table}
 
 
\begin{table}[h]
 \small
   \centering
    \caption{\textit{Experiment: Is it necessary to detach auxiliary branches for distillation?} See fig. \ref{fig:ablations_2}C. All results are computed over 5-run averages.}   
   \begin{tabular}{l||c|c}
     \toprule
     Experiment & Setting & R@1\\
     \midrule
\multirow{5}{*}{Branch Detaching} & Basic & \\
 & DSDA, Detached & \\
 & DSDA, Non-Detach & \\
 & MSDA, Detached & \\
 & MSDA, No-Detach & \\
    \bottomrule
    \end{tabular}
    \label{tab:ablations_9}
 \end{table}

  

   
  \end{document}
