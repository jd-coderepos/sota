\section{Experiments} \label{sec.exp}

\subsection{Experimental Setup}
We conduct experiments on three multimodal tasks: (a) movie genre classification on MM-IMDB \cite{imdb}; (b) sentiment analysis on CMU-MOSEI \cite{mosei}; (c) semantic segmentation on NYU Depth V2 \cite{nyu}. To demonstrate the wide applicability of our proposed DynMM, we select the above three tasks that include different modalities (\ie, image and text in task (a), video, audio and text in task (b), RGB and depth images in task (c)). We adopt modality-level DynMM for the first two tasks and fusion-level DynMM for the more challenging semantic segmentation task. Due to space limitations, we present: (1) implementation details; (2) visualization of the gating network decision; (3) an analysis of varying regularization strength $\lambda$; and (4) an ablation study on training strategies of DynMM in the Appendix.

\subsection{Movie Genre Classification}
MM-IMDB is the largest publicly available multimodal dataset for genre prediction on movies. It comprises 25,959 movie titles, metadata and movie posters. We select two movie genres (\ie, drama and comedy) for multi-label
classification from posters (image modality) and text descriptions (text modality). We follow the original data split in \cite{imdb}, and use 15,552 data for training, 2,608 for validation and 7,799 for testing. For preprocessing, we adopt the same method as \cite{imdb,multibench} to extract text and image features.



\begin{table}[t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
Method & Modality & \makecell[c]{Micro\\ F1 (\%)} & \makecell[c]{Macro\\ F1 (\%)} & \makecell[c]{MAdds\\ (M)} \\
\midrule
Image Network & I & 39.99 & 25.26 & 5.0\\
Text Network ($E_1$) & T & 59.16 & 47.21 & 0.7 \\
\hdashline
Late Fusion~\cite{multibench} ($E_2$) & & 59.55 & 50.94 & 10.3 \\ LRTF~\cite{lrfusion} & & 59.18 & 49.26 & 10.3 \\ MI-Matrix~\cite{jayakumar2020multiplicative}  & & 58.45 & 48.36 & 10.3 \\ \midrule
DynMM-a & \multirow{4}{*}{I+T} & \textBF{59.57} & 48.84 & 1.6 \\
DynMM-b & & \textBF{59.59} & 50.42 & 7.8 \\
DynMM-c & & \textBF{59.72} & \textBF{51.20} & 9.8 \\
DynMM-d & & \textBF{60.35} & \textBF{51.60} & 12.1 \\
\bottomrule
\end{tabular}}
\caption{Results on the MM-IMDB Movie Genre Classification. Modalities I and T denote image and text, respectively. The computation cost is measured by multiply-add operations (MAdds) with one image-text pair as the input. M denotes million. Each DynMM variant is obtained using a different value of the regularization hyperparameter $\lambda$ during training.}
\label{tab.imdb}
\end{table}



We adopt two expert networks for this task, namely, a unimodal network $E_1$ that takes textual features as input and another multimodal network $E_2$ that adopts late fusion \cite{multibench} to combine image and text features. We do not consider the use of an image-only network here due to its poor performance on this task. The gating network is a 2-layer MLP with hidden dimension of 128, which takes concatenated image and text features as input and outputs a 2-dimensional vector for expert network selection. We set the temperature of Gumbel-softmax as 1 and adopt straight-through training (\ie, the gating network outputs a one-hot decision vector in the forward propagation). 

Table \ref{tab.imdb} provides the comparison of our proposed modality-level DynMM with static unimodal networks and multimodal networks. We provide results of DynMM under different resource requirements (\ie, use different $\lambda$ in the loss). From Table \ref{tab.imdb}, we can see that DynMM achieves a good balance between computational efficiency and performance. Compared to the static $E_2$ network, DynMM-c improves both MAdds and macro F1 score. DynMM-d provides maximum representation power by using soft gates (which leads to more computation) and achieves best micro and macro F1 scores. On the other hand, DynMM-a involves much less computation, while still maintaining good performance (outperforms $E_1$ by 1.6\% in macro F1). This demonstrates the great flexibility and efficacy of DynMM.







\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/imdb.pdf}
  \caption{Analysis of DynMM with varying resource regularization strength ($\lambda$) on MM-IMDB. (a): Comparison of DynMM with static unimodal (UM) and multimodal (MM) baselines. (b): Branch selection ratio in DynMM with respect to $\lambda$. DynMM offers a wide range of choices that balance computation and learning behavior well.}
  \label{fig.imdb}
\end{figure}



In addition, we vary $\lambda$ in Equation (\ref{eq.loss1}) to control the importance of resource loss during training. The resulting DynMM models have varying computation costs and performance, as shown in Figure \ref{fig.imdb} (a). On one hand, when compared against a multimodal baseline that is computationally heavy, DynMM maintains good performance with much fewer MAdds. On the other hand, DynMM has better representation power than a unimodal network and thus improves the F1 score. Figure \ref{fig.imdb} (b) shows the selection ratio of each expert network in DynMM with respect to $\lambda$. We observe that as $\lambda$ increases, DynMM focuses more on reducing computation and thus is more likely to select expert network 1 ($E_1$) with a small computation cost. Note that for the $\lambda=0$ case, we adopt soft gates, \ie, every expert network is activated and the output is a weighted combination of predictions given by the two expert networks. Thus, DynMM achieves the best performance at the cost of increased computation. This also demonstrates the flexibility of DynMM, as we can easily adjust $\lambda$ to target high performance or high inference efficiency. 

\subsection{Sentiment Analysis}
CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) is the largest dataset of sentiment analysis and emotion recognition. It contains 3,228 real-world online videos from more than 1000 speakers and 250 topics.  Each video is split into short segments of 10-20 seconds. Each segment is annotated for a sentiment from -3 (strongly negative) to 3 (strongly positive). The task is to predict the sentiment scores from video, audio and text. Following \cite{multibench}, we use 16,265 data for training, 1,869 data for validation and 4,643 data for testing. The feature extraction steps are the same as \cite{multibench}. 



As text is the best performing modality in this task, we adopt a unimodal network that takes textual features as input to be the expert network $E_1$. The second expert network ($E_2$) of our DynMM is selected as a late fusion network \cite{multibench} that receives inputs from three modalities. The gating network is designed as a lightweight transformer network with hidden dimension equal to 512 and 2 attention heads, followed by a linear layer. The gating network receives concatenated features from three modalities and generates sample-wise decisions on which expert network to activate during inference time. We set temperature of Gumbel-softmax as 1 and adopt straight-through training.

\begin{table}[!tb]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
Method & Modality & Acc$^2$ (\%) & MAE & MAdds (M) \\
\midrule
Video Network & V & 69.02 & 0.80 & 123.1 \\
Audio Network & A & 67.68 & 0.82 & 123.3 \\
Text Network ($E_1$) & T & 78.35 & 0.62 & 124.7 \\
\hdashline
Early Fusion~\cite{multibench} &\multirow{2}{*}{V+A+T} & 78.45 & 0.65 & 313.5 \\ Late Fusion~\cite{multibench} ($E_2$)  & & 79.54 & \textBF{0.60} & 309.6 \\ \midrule
DynMM-a & \multirow{3}{*}{V+A+T} & 79.07 & 0.62 & 165.5 \\
DynMM-b & & \textBF{79.73} & 0.61 & 254.5 \\
DynMM-c & & \textBF{79.75} & \textBF{0.60} & 295.8 \\
\bottomrule
\end{tabular}}
\caption{Results on CMU-MOSEI Sentiment Analysis. Modalities V, A, T represent video, audio and text, respectively. Acc$^2$ denotes binary accuracy (\ie, positive/negative sentiments) and MAE represents mean absolute error. MAdds are measured with a video-audio-text tuple. Each DynMM variant is obtained using a different value of the regularization hyperparameter $\lambda$ during training.}
\label{tab.mosei}
\end{table}

\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/viz_mosei.pdf}
  \caption{We visualize a few test instances on CMU-MOSEI for a negative sentiment. DynMM identifies sentences marked with red as ``easy'' instances and only uses textual information for prediction. For sentences marked with blue, DynMM takes multimodal inputs (\ie, video+audio+text) for more accurate predictions. }
  \label{fig.viz_mosei}
\end{figure}

Results are summarized in Table \ref{tab.mosei}. We provide three DynMM networks trained with different $\lambda$. Compared with the best performing static network (\ie, Late Fusion), DynMM-a can reduce computations by 46.5\% with a slightly decreased accuracy (\ie, -0.47\%). By allowing more computation, DynMM-b improves both inference efficiency (\ie, reduce MAdds by 17.8\%) and prediction accuracy. Finally, DynMM-c further improves the accuracy by trading off some computation; it achieves best accuracy and smallest mean absolute error with reduced computation cost. These results demonstrate the great advantages of dynamic multimodal fusion. Since multimodal data naturally brings redundancy, we observe that many computations can be reduced without loss in accuracy. 


To have an intuitive sense of our gating network decision on which modality to select, we provide visualization results of several test instances in Figure \ref{fig.viz_mosei}. For simplicity only the text modality is shown here, and the other two modalities (\ie, video and audio) are omitted. The gating network chooses $E_1$ for sentences marked with red and $E_2$ for sentences marked with dark blue. We find that the sentences marked with red often possess strong evidence indicating the sentiments of this sample, \eg, `horrible', `amazingly good'. Therefore, they belong to the ``easy'' samples category that can be correctly predicted using the text modality alone. On the contrary, the sentences marked with dark blue are vague and require additional modalities to help with the prediction. These results indicate that the gating function is well trained and can provide reasonable decisions based on input characteristics.

\subsection{Semantic Segmentation}

NYU Depth V2 is an indoor dataset for semantic segmentation. It contains 1,449 RGB-D images with 40-class labels; 795 images are used for training and 654 images are for testing. The two modalities are RGB and depth images.     






\begin{table}[!htb]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
Method & \makecell[c]{mIoU\\ (\%)} & \makecell[c]{Depth Enc\\ MAdds (G)} & \makecell[c]{MAdds \\Reduction (\%)} \\
\midrule
ESANet~\cite{esanet} (baseline) & 50.5 & 24.7 & - \\ DynMM (Stage I) & 48.5 & 11.7 & 52.6\% \\
DynMM-a (Stage II) & 49.9 & 11.1 & 55.1\% \\
DynMM-b (Stage II) & 51.0 & 19.5 & 21.1\% \\
\bottomrule
\end{tabular}}
\caption{Results on RGB-D semantic segmentation. mIoU denotes mean Intersection-over-Union. MAdds are calculated for input size of $3\times480\times640$. G stands for Giga.}
\label{tab.nyu}
\end{table}

\begin{table}[t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
Method & Modality & Backbone & mIoU (\%) & MAdds (G) \\
\midrule
LW-RefineNet~\cite{lwrefinenet}  & \multirow{2}{*}{RGB} & ResNet-50 & 41.7 & \textBF{38.5}\\ LW-RefineNet~\cite{lwrefinenet}  & & ResNet-101 & 43.6  & 61.2\\
\hdashline ACNet~\cite{hu2019acnet}  & \multirow{4}{*}{RGB+D} & ResNet-50 & 48.3 & 126.2 \\ SA-Gate~\cite{sagate} &  & ResNet-50 & 50.4 & 147.6 \\ CEN~\cite{cen}  & & ResNet-101 & \textBF{51.1} & 618.3\\ ESANet~\cite{esanet}  &  & ResNet-50 & 50.5 & 56.9 \\
\midrule DynMM-a & \multirow{2}{*}{RGB+D} & ResNet-50 & 49.9 & \textBF{43.4} \\ 
DynMM-b & &  ResNet-50 & \textBF{51.0} & 52.2 \\
\bottomrule
\end{tabular}}
\caption{Comparison of our approach with SOTA methods for RGB-D semantic segmentation on NYU Depth V2 test data.}
\label{tab.nyusota}
\end{table}

We adopt fusion-level DynMM for this task and base our dynamic architecture design on a (static) efficient architecture, ESANet \cite{esanet}. As illustrated in Figure \ref{fig.fusion}, we incorporate four fusion cells in the encoder design, where each fusion cell contains two operations. Operation 1 is an identity mapping of RGB features, \ie, $O_1 = x_1$. For the second operation, we use channel attention fusion, where features from both modalities are first reweighted with a Squeeze and Excitation module \cite{hu2018squeeze} and then added element-wisely. Two ResNet-50 \cite{resnet} are used as feature extraction models for RGB and depth modality. The decoder design is identical to \cite{esanet}. The gating network comprises a pipeline of 2 convolution blocks with kernel size 5$\times$5 and stride size 2, a global average pooling and a linear layer. RGB and depth features after the first convolutional layer are concantenated together and passed to the convolutional gate. The gating network outputs a 4-dimensional vector per sample that determines which operation to select for each fusion cell. We experiment with two training strategies: (1) DynMM-a in Table \ref{tab.nyu} is trained with straight-through technique with Gumbel-softmax temperature $\tau=1$; (2) We obtain DynMM-b in Table \ref{tab.nyu} by exponentially decaying $\tau$ from 1 to 0.0001 during 500 epochs.

Table \ref{tab.nyu} provides the detailed results of fusion-level DynMM. We report performance of DynMM after first-stage training in the second row; its great performance validates the design of our random gating function in the pre-training stage. This also lends support to our claim that there exists a lot of redundancy in multimodal networks. Utilizing the finding that depth modality plays an auxiliary role in this task, fusion-level DynMM effectively reduces computations of the depth encoder. DynMM-a reduces MAdds by 55.1\% with only -0.4\% mIoU drop. Furthermore, DynMM-b achieves a mIoU improvement of 0.7\% and 21.1\% reduction in MAdds at the same time, thus demonstrating the superiority of DynMM over static fusion. 


Table \ref{tab.nyusota} presents a comparison of the resulting DynMM-a and DynMM-b with SOTA semantic segmentation methods. For baseline methods, we list mIoU reported in their original papers and report MAdds. These results clearly show that our proposed method achieves the best balance between performance and efficiency. The computation cost of DynMM is similar to a unimodal lightweight RefineNet, yet its performance can match methods that use ResNet-101 as the backbone and involve significantly larger MAdds. 



Finally, we conduct experiments to demonstrate the improved robustness of DynMM compared to ESANet. We consider three settings by injecting random Gaussian noise with probability 1/3 to (1) RGB modality; (2) depth modality and (3) both modalities. We experiment with different degrees of random Gaussian noise and plot the performance degradation of two approaches in Figure \ref{fig.noise}. From the figure, we observe that the performance gap between DynMM and ESANet becomes larger when the noise level of depth images increases; This demonstrates another advantage of DynMM in reducing data noise and improving robustness. Figure \ref{fig.seg} shows some qualitative segmentation results. While ESANet generates reasonable predictions in the normal setting (\ie, first and third row), its performance becomes significantly worse when multimodal data is perturbed by noise (\ie, the second and fourth row). On the contrary, our DynMM is robust to noise and provides a good prediction for both scenarios. These results suggest the potential of a dynamic neural network architecture for improving robustness of multimodal fusion.

\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/noise.pdf}
  \caption{DynMM vs. ESANet on NYU Depth V2 with different degrees of Gaussian noise injected into RGB / depth images. }
  \label{fig.noise}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/seg.pdf}
  \caption{Qualitative segmentation results on NYU Depth V2. DynMM is more robust to noisy multimodal data compared with the static ESANet.}
  \label{fig.seg}
\end{figure}






