

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{bm}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression}

\begin{document}

\twocolumn[
\icmltitle{MLIC$^{++}$: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression}







\begin{icmlauthorlist}
\icmlauthor{Wei Jiang}{pku}
\icmlauthor{Ronggang Wang}{pku,pcl}
\\\tt{wei.jiang1999@outlook.com, rgwang@pkusz.edu.cn}
\end{icmlauthorlist}

\icmlaffiliation{pku}{Shenzhen Graduate School, Peking University}
\icmlaffiliation{pcl}{Pengcheng Laboratory}

\icmlcorrespondingauthor{Ronggang Wang}{rgwang@pkusz.edu.cn}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{} 

\begin{abstract}
  Recently, multi-reference entropy model has been proposed,
  which captures channel-wise, local spatial, and global spatial correlations.
  Previous works adopt attention for global correlation capturing,
  however, the quadratic
  complexity limits the potential of high-resolution image coding.
  In this paper, we propose the linear complexity global correlations capturing,
  via the decomposition of softmax operation. 
  Based on it, we propose the MLIC$^{++}$, a learned image compression 
  with linear complexity for multi-reference entropy modeling.
  Our MLIC$^{++}$ is more efficient and it reduces BD-rate by $13.39\%$ on the Kodak dataset
  compared to VTM-17.0 when measured in PSNR. Code is available at \url{https://github.com/JiangWeibeta/MLIC}.
  \end{abstract}
  
  \section{Introduction}
  \label{sec:intro}
  In recent years, we have seen much progress in entropy model design~\cite{minnen2018joint,minnen2020channel,he2022elic,jiang2022mlic}.
  Most entropy models capture correlations in one dimension,
  however, there are channel-wise, local spatial, and global spatial
  correlations, leading to sub-optimal performance.
  To overcome this limitation, Jiang \textit{et al.} 
  introduce the multi-reference entropy model (MEM)~\cite{jiang2022mlic} and propose the learned 
  image compression models MLIC and MLIC$^+$. In MLIC and MLIC$^+$,
  the latent representation $\hat{\boldsymbol{y}}$ is divided into slices $\{\hat{\boldsymbol{y}}^0, \hat{\boldsymbol{y}}^1, \cdots\}$
  ~\cite{minnen2020channel} for channel-wise correlation
  capturing. For the $i$-th slice, they propose the checkerboard attention
  for local spatial correlations capturing with two-pass decoding, where the slice is divided into 
  anchor part $\hat{\boldsymbol{y}}^i_{ac}$ and non-anchor part $\hat{\boldsymbol{y}}^i_{na}$.
  In addition, they propose to use the attention map of the previous 
  slice to predict the global correlations in the current slice.
  The process is
  $\textrm{softmax}\left(\hat{\boldsymbol{y}}^{i-1}_{na}\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)^{\top}\right)\hat{\boldsymbol{y}}^{i}_{ac}$.
  The main drawback of MLIC and MLIC$^+$ is the quadratic complexity
  of global correlations capturing, caused by the 
  softmax operation in attention, which specifies the order of matrix calculation. 
  In this paper, we investigate the global correlations capturing
  with linear complexity. We decompose the softmax operation~\cite{shen2021efficient} into two softmax operations
  $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)^{\top}\hat{\boldsymbol{y}}^{i}_{ac}$.
  Since $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)^{\top}\geq 0$,
  it can be treated as the global similarity.
  Such decomposition enables global correlations capturing with linear complexity.
  Based on such decomposition, we propose linear complexity
  intra-slice and inter-slice global spatial context modules, and 
  propose learned image compression model MLIC$^{++}$, denoting
  linear complexity multi-reference entropy modeling for learned image compression.
  \begin{figure*}
      \centering
      \includegraphics[width=\linewidth]
      {mlic++_arch.pdf}
      \caption{The overall architecture of MLIC$^{++}$.
      $\downarrow$ means down-sampling.
      $\uparrow$ means up-sampling.
      / means stride equals $1$.
      Red line is the dataflow during decoding.
      ${\boldsymbol{x}}$ is the input image and $\hat{\boldsymbol x}$ is the reconstructed image.
      $\boldsymbol{y}$ is the latent representation and $\hat{\boldsymbol{y}}$ is
      the quantized latent representation. $\hat{\boldsymbol y}^i$ is the $i$-th slice of $\hat{\boldsymbol{y}}$.
      $\hat{\boldsymbol y}^i_{ac}$ is the anchor part of $\hat{\boldsymbol{y}}^i$.
      $\hat{\boldsymbol{y}}^i_{na}$ is the non-anchor part of $\hat{\boldsymbol{y}}^i$.
      We use “Channel-wise” to denote channel-wise context module,
      “Local Spatial” to denote local spatial context module,
      “Intra Global” to denote intra-slice global context module,
      “Inter Global” to denote inter-slice global context module.
      We set $M$ to $320$ and set $N$ to $192$ in our MLIC$^{++}$.}
      \label{fig:arch}
\end{figure*}
  \section{Related Works}
  \label{sec:related}
  Learned image compression~\cite{theis2017lossy, balle2020nonlinear} aims to optimize the trade-off between 
  bit-rate and distortion. Given a specific Lagrange multiplier $\lambda$,
  analysis transform $g_a$, and synthesis transform $g_s$,
  the optimization target is 
  \begin{equation}
      \mathcal{L} = \mathcal{R}(\hat{\boldsymbol y}) + \lambda \times \mathcal{D}(\boldsymbol{x}, \hat{\boldsymbol{x}}),
  \end{equation}
  where the $\boldsymbol{x}$ is the input image, $\boldsymbol{y}$ is the
  latent representation, $\hat{\boldsymbol y} = \lceil g_a(\boldsymbol x) \rfloor$,$\lceil \cdot \rfloor$ is 
  quantization, $\hat{\boldsymbol x} = g_s(\hat{\boldsymbol y})$.\par
  Ball{\'e} \textit{et al.}~\cite{balle2016end} propose to use convolutional layers
  to build $g_a$ and $g_s$ for non-linear transform. They adopt adding 
  uniform noise $\mathcal{U}(-0.5, 0.5)$ to approximate quantization during training.
  Later, Ball{\'e} \textit{et al.}~\cite{balle2018variational} introduce hyperprior $\hat{\boldsymbol z}$ to estimate entropy parameters.
  To further capture correlations within $\hat{\boldsymbol{y}}$,
  Minnen \textit{et al.}~\cite{minnen2018joint} adopt a serial
  pixel-cnn-like~\cite{van2016conditional} context model.
  To accelerate decoding,
  Minnen \textit{et al.}~\cite{minnen2020channel} propose to capture
  channel-wise contexts by group the symbol channels into several chunks,
  while He \textit{et al.} adopt the checkerboard pattern
  ~\cite{he2021checkerboard} for two-pass decoding.\par
  Recently, multi-reference entropy modeling has been explored.
  Some works~\cite{ma2021cross,he2022elic} aim to capture local spatial and channel-wise contexts,
  however, they ignore the global spatial correlations.
  Jiang \textit{et al.} propose MLIC and MLIC$^+$~\cite{jiang2022mlic}
  to capture local spatial, global spatial, and channel-wise contexts, 
  however, the quadratic computational complexity of global spatial contexts
  capturing makes it hard to be employed for high-resolution image coding.
  \section{Method}
  \label{sec:method}
  \subsection{Overall Architecture}
  \label{sec:method:overview}
  The overall framework of our MLIC$^{++}$ is similar with
  MLIC$^+$~\cite{jiang2022mlic}. We briefly introduce our framework first.
  In our framework, we adopt the transform modules of MLIC$^+$,
  a simplified version of the transform modules of Cheng'20~\cite{cheng2020learned}.
  We adopt the same transform modules to prove the effectiveness of 
  our proposed linear complexity multi-reference entropy model MEM$^{++}$.
  In our MEM$^{++}$, we adopt the checkerboard attention~\cite{jiang2022mlic} for local spatial
  context capturing and use the same settings of MLIC$^{+}$~\cite{jiang2022mlic} 
  for channel-wise context capturing .
  we use $\Phi_{h}$ to denote the hyperprior. 
  We first divide the latent representation $\hat{\boldsymbol{y}}$
  into slices $\{\hat{\boldsymbol{y}}^0,\hat{\boldsymbol{y}}^1,\cdots \}$~\cite{minnen2020channel}.
  We take the $i$-th slice as an example. We divide the $\hat{\boldsymbol{y}}^i$
  into anchor part $\hat{\boldsymbol{y}}_{ac}^i$ and non-anchor part
  $\hat{\boldsymbol{y}}_{na}^i$. We extract channel-wise contexts
  $\Phi_{ch}^i$ from slices $\hat{\boldsymbol{y}}^{\textless i}$.
  $\hat{\boldsymbol{y}}_{ac}^i$ is local-spatial-context-free.
  We extract local spatial context $\Phi_{lc}^i$ of $\hat{\boldsymbol{y}}_{na}^i$
  from $\hat{\boldsymbol{y}}_{ac}^i$. Slice different slices 
  share the similar global similarity~\cite{jiang2022mlic,guo2021causal},
We extract the intra-slice global context $\Phi^{i}_{gc,intra}$
  of $\hat{\boldsymbol{y}}_{na}^i$ from $\hat{\boldsymbol{y}}_{ac}^i$
  via the global similarity of $\hat{\boldsymbol{y}}^{i-1}$.
  Besides, we extract the inter-slice global context $\Phi^{i}_{gc,inter}$
  from slices $\hat{\boldsymbol{y}}^{\textless i}$ via the
  global similarity of slices $\hat{\boldsymbol{y}}^{\textless i}$.
  Therefore, the rate of $\hat{\boldsymbol{y}}^i_{ac}$ and 
  the rate of $\hat{\boldsymbol{y}}^i_{na}$ are:
  \begin{small}
  \begin{equation}
  \begin{aligned}
      \mathcal{R}^i_{ac} &= \mathbb{E}\left[-\log_2p_{\hat{\boldsymbol{y}}^i_{ac}}\left(\hat{\boldsymbol{y}}^i_{ac}|\Phi_{h}, \Phi_{ch}^i, \Phi_{gc,inter}^i \right) \right]\\   
      \mathcal{R}^i_{na} &= \mathbb{E}\left[-\log_2p_{\hat{\boldsymbol{y}}^i_{na}}\left(\hat{\boldsymbol{y}}^i_{na}|\Phi_{h}, \Phi_{ch}^i, \Phi_{gc,intra}^i, \Phi_{gc,inter}^i \right) \right]
  \end{aligned}
  \end{equation}
  \end{small}
  \subsection{Explicit Global Context in MLIC$^+$}
  \label{sec:method:ggc_mlicp}
  We take the intra-slice global context module in MLIC$^+$~\cite{jiang2022mlic} as an example.
  To extract the intra-slice global context of $\hat{\boldsymbol{y}}_{na}^i \in \mathbb{R}^{L\times C}$,
  global similarity between $\hat{\boldsymbol{y}}^{i-1}_{na} \in \mathbb{R}^{L\times C}$
  and $\hat{\boldsymbol{y}}^{i-1}_{ac} \in \mathbb{R}^{L\times C}$ is employed to predict the global correlations
  between $\hat{\boldsymbol{y}}^i_{na} \in \mathbb{R}^{L\times C}$ and $\hat{\boldsymbol{y}}^i_{ac} \in \mathbb{R}^{L\times C}$,
  where $L=H\times W$.
  $C$ is the channel number of $\hat{\boldsymbol{y}}^i$, $H$ is 
  the height of $\hat{\boldsymbol{y}}^i$, and $W$ is the width of $\hat{\boldsymbol{y}}^i$.
  \begin{equation}
    \textrm{Attention} = \underbrace{\textrm{softmax}\left(\frac{\hat{\boldsymbol{y}}^{i-1}_{na}    \left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)^{\top} }{\sqrt{d_k}}\right)}_{\textrm{non-negative}}\hat{\boldsymbol{y}}^i_{ac}
    \label{eq:mlic_intra}
\end{equation}
  In MLIC$^+$\cite{jiang2022mlic}, the attention map is employed 
  as a similarity metric, because of the non-negative property of the attention map.
  The process is shown in Equation~\ref{eq:mlic_intra}.
  However, attention map leads to \textbf{quadratic} complexity.
  The complexity of Equation~\ref{eq:mlic_intra} is $O(CL^2)$.
  The quadratic complexity makes it quite difficult to apply MLIC$^+$ to high-resolution images.
  \begin{figure*}
    \centering
    \subfloat{
      \includegraphics[scale=0.45]{kodak_psnr.pdf}}
    \subfloat{
      \includegraphics[scale=0.45]{kodak_msssim.pdf}}
    \caption{PSNR-Bit-rate curve (opt.MSE) and MS-SSIM-Bit-rate curve (opt.MS-SSIM) on Kodak dataset.}
    \label{fig:rd}
  \end{figure*}
  \subsection{Linear Attention for Implicit Global Context}
  \label{sec:method:linear}
  In Equation~\ref{eq:mlic_intra}, 
  the quadratic complexity is caused by the softmax operation, which
  leads to computing $\hat{\boldsymbol{y}}^{i-1}_{na}\hat{\boldsymbol{y}}^{i-1}_{ac}$ first.
  To solve the quadratic complexity, we employ efficient attention
  ~\cite{shen2021efficient}.
  In efficient attention~\cite{shen2021efficient}, 
  we employ the softmax operation on $\hat{\boldsymbol{y}}^{i-1}_{na}$ in 
  row and the softmax operation on $\hat{\boldmath{y}}^{i-1}_{ac}$ in 
  column.
  \begin{equation}
      \textrm{Attention} = \underbrace{\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)^{\top}}_{\textrm{non-negative}}\hat{\boldsymbol{y}}^i_{ac}
      \label{eq:efficient}
  \end{equation}
  The process is illustrated in Equation~\ref{eq:efficient}.
  Since we use softmax operation on $\hat{\boldsymbol{y}}^{i-1}_{na}$ and 
  $\hat{\boldsymbol{y}}^{i-1}_{ac}$ separately, we can compute
  $\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)^{\top}\hat{\boldsymbol{y}}^{i}_{ac}$ first.
  The complexity of it is $O(C^2L)$, which is linear with the resolution.\par
  We explain why such linear attention works for global correlation capturing.
  In Equation~\ref{eq:efficient}, we can get
  $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right) \geq 0$.
  The non-negative property makes it can be treated as a similarity metric.
  The metric is implicit  because we compute
  $\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)\hat{\boldsymbol{y}}^{i}_{ac}$ first
  in practice.
  \begin{table*}
      \scriptsize
      \centering
      \begin{tabular}{@{}cccccccccccccc@{}}
      \toprule
      \multicolumn{1}{c|}{\multirow{2}{*}{Methods}}                            & \multicolumn{2}{c}{Kodak}    & \multicolumn{2}{c}{Tecnick~\cite{tecnick2014TESTIMAGES}}  & \multicolumn{2}{c}{CLIC Pro Valid~\cite{CLIC2020}}  \\
      \multicolumn{1}{c|}{}                                                     & \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{MS-SSIM}& \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{MS-SSIM}& \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{MS-SSIM} \\ \midrule
      \multicolumn{1}{c|}{VTM-17.0~\cite{vtm2019}}                                        & $0.00$       & $0.00$ & $0.00$       & $0.00$ & $0.00$       & $0.00$       \\\midrule
      \multicolumn{1}{c|}{SwinT-Charm (ICLR'22)~\cite{zhu2022transformerbased}}                & $-1.73$      & $-$   & $-$& $-$& $-$& $-$  \\\midrule
      \multicolumn{1}{c|}{STF (CVPR'22)~\cite{zou2022the}}         & $-2.48$      & $-47.72$  & $-2.75$      & $-$   & $+0.42$      & $-$      \\\midrule
      \multicolumn{1}{c|}{ELIC (CVPR'22)~\cite{he2022elic}}                             & $-5.95$      & $-44.60$   & $-$      & $-$ & $-$      & $-$   \\\midrule
      \multicolumn{1}{c|}{Contextformer (ECCV'22)~\cite{koyuncu2022contextformer}}        & $-5.77$      & $-46.12$ & $-9.05$      & $-42.29$  & $-$      & $-$     \\\midrule
      \multicolumn{1}{c|}{MLIC (ACMMM'23)~\cite{jiang2022mlic}}                                                    & $-8.05$      & $-49.13$ & $-12.73$      & $-47.26$ & $-8.79$      & $-45.79$    \\\midrule
      \multicolumn{1}{c|}{MLIC$^+$ (ACMMM'23)~\cite{jiang2022mlic}}                                                & $-11.39$      & $-52.75$    & $-16.38$      & $ \bm{-53.54}$    & $-12.56$      & $-48.75$      \\\midrule
      \multicolumn{1}{c|}{MLIC$^{++}$ (Ours)}                                                & \bm{$-13.39$}      & \bm{$-53.63$}  & \bm{$-17.59$} &{$-53.49$}& \bm{$-13.08$}& \bm{$-50.78$}   \\\midrule
      \end{tabular}
      \caption{BD-Rate $(\%)$ comparison for PSNR (dB) and MS-SSIM (dB). The anchor is VTM-17.0 Intra.}
      \label{tab:rd}
    \end{table*}
    \begin{table*}
      \scriptsize
      \centering
      \setlength{\tabcolsep}{2mm}{
      \begin{tabular}{@{}cccccccccccccc@{}}
      \toprule
      \multicolumn{1}{c|}{\multirow{2}{*}{Input Resolution}}                            & \multicolumn{3}{c}{MLIC~\cite{jiang2022mlic}} & \multicolumn{3}{c}{MLIC$^+$~\cite{jiang2022mlic}} & \multicolumn{3}{c}{MLIC$^{++}$(Ours)}      \\
      \multicolumn{1}{c|}{}                                                     & \multicolumn{1}{c}{Enc.Tot (s)} & \multicolumn{1}{c}{Dec.Tot (s)} & \multicolumn{1}{c}{Peak Mem (GB)} & \multicolumn{1}{c}{Enc.Tot (s)} & \multicolumn{1}{c}{Dec.Tot (s)}  & \multicolumn{1}{c}{Peak Mem (GB)} & \multicolumn{1}{c}{Enc.Tot (s)} & \multicolumn{1}{c}{Dec.Tot (s)}  & \multicolumn{1}{c}{Peak Mem (GB)}\\ \midrule
      \multicolumn{1}{c|}{$512\times 512$}                                        & $0.0984$       & $0.1376$  & $0.8929$  & $0.1676$       & $0.2355$ & $1.9759$ & $0.1687$       & $0.2363$  & $1.9603$ \\\midrule
      \multicolumn{1}{c|}{$768\times 768$}                & $0.1564$      & $0.2167$ & $1.1808$ & $0.2795$       & $0.3640$ &$2.5027$ & $0.2286$       & $0.3083$  & $2.2089$ \\\midrule
      \multicolumn{1}{c|}{$1024\times 1024$}         & $0.2429$      & $0.3101$  & $1.6873$  & $0.4748$      & $0.5854$   & $3.6110$ & $0.3204$      & $0.4177$   & $2.5483$  \\\midrule
      \multicolumn{1}{c|}{$1536\times 1536$}                            & $0.4858$      & $0.5929$ & $3.7926$ & $1.0460$      & $1.1515$ &$9.1465$ & $0.6266$      & $0.7604$ & $3.5151$   \\\midrule
      \multicolumn{1}{c|}{$2048\times 2048$}        & $0.8896$      & $1.0281$ &$9.9478$ & $2.0211$      & $2.1461$ &$24.3720$ & $1.0998$      & $1.2339$  & $4.8625$   \\\midrule
      \multicolumn{1}{c|}{$2560\times 2560$}        & OM    & OM & $\textgreater 32$ (OM) & OM      & OM & $\textgreater 32$ (OM) & $1.6511$      & $1.8568$  &$6.5996$   \\\midrule
      \end{tabular}}
      \caption{Complexity comparison among MLIC~\cite{jiang2022mlic}, MLIC$^+$~\cite{jiang2022mlic} and MLIC$^{++}$.
      “Enc.Tot” and “Dec.Tot” mean the total encoding and decoding time, including entropy coding time.
      “Peak Mem” means the peak memory during the encoding and decoding. “OM” means out of memory.
      We evaluate on a single Tesla V100-32G GPU and a Xeon(R) 8260 CPU.}
      \label{tab:complex}
    \end{table*}
  \subsection{Improvements on Global Context Modules}
  We illustrate the intra-slice global context module and 
  inter-slice global context module in Figure~\ref{fig:arch}.
  In MLIC$^+$, there is no position encoding in intra-slice and inter-slice global spatial
  context modules.
  In our modules, we employ a $1\times 1$ convolutional layer for embedding and a $3\times 3$
  depth-wise convolutional layer for learnable position embedding.
  Following MLIC$^+$~\cite{jiang2022mlic}, we also adopt a $5\times 5$
  convolutional layer to aggregate the global correlations among adjacent symbols.
  We employ a residual bottleneck~\cite{jiang2023slic} to fuse the global context further instead of a MLP in MLIC$^{+}$~\cite{jiang2022mlic}.
  The difference between our intra-slice global context module and
  our inter-slice global context module is the input.
  The input of intra-slice global context module is $\hat{\boldsymbol{y}}^{i-1}_{na}$,
  $\hat{\boldsymbol{y}}^{i-1}_{ac}$, and $\hat{\boldsymbol{y}}^i_{ac}$. 
  Different from MLIC$^{+}$~\cite{jiang2022mlic}, we capture inter-slice global contexts from slices
  $\hat{\boldsymbol{y}}^{\textless i}$, instead of using $\hat{\boldsymbol{y}}^{i}_{ac}$ as
  approximation and capture inter-slice global contexts from $\hat{\boldsymbol{y}}^{i-1}$.
  This is because the different slices share the similar global correlations
  and capturing inter-slice global contexts from more slices leads to better performance.
  Note that the skip connection is deprecated because the global context capturing
  is based on calculating the similarity first and then calculating the weighted sum based on the similarity.
  Since the depth of our attention block is $1$, deprecating the skip connection
  will not leads to gradient vanishing. Deprecating the skip connection also reduces the complexity.
  Based on the linear complexity attention and improvements on the architecture,
  our MLIC$^{++}$ even performs better than MLIC$^+$ at some bit-rates with lower complexity.
  \section{Experiments}
  \label{sec:exp}
  \subsection{Setup}
  \label{sec:exp:setup}
  We build our MLIC$^{++}$ on Pytorch~\cite{paszke2019pytorch} and 
  CompressAI~\cite{begaint2020compressai}.
  We train our MLIC$^{++}$ on $10^5$ images\footnote[1]{\url{https://github.com/JiangWeibeta/MLIC/blob/main/train_list.txt}} with a 
  resolution larger than $512\times 512$
  from ImageNet~\cite{deng2009imagenet},
  COCO~\cite{lin2014microsoft}, DIV2K~\cite{Agustsson2017NTIRE2C}, and Flickr2K~\cite{lim2017enhanced}.
  We set $\lambda$ to $\{18, 35, 67, 130, 250, 483\} \times 10^{-4}$ for MSE
  and set $\lambda$ to $\{2.4, 4.58, 8.73, 16.64, 31.73, 60.5\}$ for MS-SSIM~\cite{wang2003multiscale}.
  We set the batch size to $32$ and train models on a single Tesla A100-80G GPU.
  We adopt the training strategy of MLIC and MLIC$^+$~\cite{jiang2022mlic} to train our MLIC$^{++}$.


  \subsection{Performance}
  \label{sec:exp:perf}
  We evalute the performance of our MLIC$^{++}$ on Kodak~\cite{kodak}, Tecnick~\cite{tecnick2014TESTIMAGES}, and CLIC Pro Valid~\cite{CLIC2020}.
  We use VTM-17.0~\cite{vtm2019} as anchor.
  We compare our MLIC$^{++}$ with recent models~\cite{jiang2022mlic, koyuncu2022contextformer, he2022elic, zou2022the, zhu2022transformerbased, minnen2020channel, cheng2020learned}.
  The results are illustrated in Table~\ref{tab:rd} and Figure~\ref{fig:rd}.
  Our MLIC$^{++}$ outperforms other models and achieves state-of-the-art performance.
  However, it has to be admitted that, the difference in rate-distortion performance between MLIC$^{++}$ and MLIC$^+$ is very small.
  \subsection{Complexity}
  We evaluate the complexity on encoding time, decoding time, and peak memory consumption.
  We compare our MLIC$^{++}$ with MLIC and MLIC$^+$~\cite{jiang2022mlic}.
  We report the results in Table~\ref{tab:complex}.
  On $2048\times 2048$ images, 
  the encoding time and decoding time of our MLIC$^{++}$ is one half of that of MLIC$^+$~\cite{jiang2022mlic},
  and MLIC$^{++}$ consumes one-fifth of the peak memory consumed by MLIC$^+$~\cite{jiang2022mlic}.
  On $2560\times 2560$ images, MLIC and MLIC$^+$~\cite{jiang2022mlic}
  cannot encode and decode on a Tesla V100-32G, while our MLIC$^{++}$
  can encode and decode successfully and only consume $6.6$ GB Memory.
  Our MLIC$^{++}$ is more efficient.
  \subsection{Ablation Studies}
  \label{sec:ablation}
  We conduct ablation studies on Kodak~\cite{kodak}.
  We evaluate the contribution of linear complexity intra-slice global context model
  on MLIC~\cite{jiang2022mlic}.
  We evaluate the contribution of linear complexity inter-slice global
  context model on MLIC$^+$~\cite{jiang2022mlic}.
  The ablation results are illustrated in Figure~\ref{fig:ablation}.
  Our linear complexity intra-slice global spatial context module 
  leads to almost no performance degradation.
  Compared to the inter-slice global spatial context module in
  MLIC$^+$, our MLIC$^{++}$ performs better at some bit-rates.
  The almost no performance drop can be attributed to learnable positional encoding, 
  and residual bottlenecks. 
  Our MLIC$^{++}$ performs better than MLIC$^+$ at some bit-rates because MLIC$^{++}$ capture 
  inter-slice global contexts from more slices.
  \begin{figure}[t]
      \centering
      \subfloat{
        \includegraphics[scale=0.33]{ablation_intra.pdf}}
      \subfloat{
        \includegraphics[scale=0.33]{ablation_inter.pdf}}
      \caption{Ablation Studies on Kodak dataset. “PE” means positional encoding.
      “RB” means residual bottleneck.}
      \label{fig:ablation}
    \end{figure}
  \section{Limitations}
  \label{sec:limit}
  One drawback of our MLIC$^{++}$ is its transform modules.
  We find the transform modules lead to slight
  performance degradation when the bit-rate on Kodak~\cite{kodak} $\geq 1$.
  We think employing better transform modules~\cite{jiang2023slic,he2022elic,zou2022the,zhu2022transformerbased}
  will address this problem.
  The other drawback is the parameters. In MLIC$^{++}$,
  we use separate modules for each slice, which leads to more parameters, however, it is harmless because the parameters have no
  high relevance with FLOPs, encoding time, and decoding time.
  \section{Conclusion}
  \label{sec:conclusion}
  In this paper, we propose linear complexity intra-slice and inter-slice
  global context modules, which further improve the performance and 
  reduce the complexity.
  We build the multi-reference entropy model MEM$^{++}$ with 
  the support of linear complexity intra-slice and inter-slice global
  context modules.
  Based on MEM$^{++}$, we obtain state-of-the-art model MLIC$^{++}$. 
  To make our MLIC$^{++}$ more practical,
  in the future, we will investigate the asymmetrical design~\cite{yang2023asymmetrically}
  between analysis and synthesis transform, and lighter multi-reference entropy model.


\nocite{langley00}

\bibliography{mlicpp}
\bibliographystyle{icml2023}


\newpage
\appendix
\onecolumn





\end{document}
