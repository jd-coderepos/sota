\documentclass[nohyperref]{article}

\usepackage[final]{neurips_2022}

\usepackage{microtype}
\usepackage{graphicx,color}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{amsthm,threeparttable}
\usepackage[capitalize,noabbrev]{cleveref}
 \usepackage{multicol}
 \usepackage{multirow}
\usepackage{pifont} 
\usepackage{amsbsy}
\usepackage[normalem]{ulem}
\usepackage{algorithmic}
\usepackage[ruled]{algorithm2e}
\usepackage{enumitem}

\usepackage{selectp}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\crefname{section}{sec.}{sec.} 

\newcommand{\xmark}{\textcolor{red}{\ding{55}}}  
\newcommand{\colorcheck}{\textcolor{green}{\pmb{\checkmark}}} 

\usepackage[textsize=tiny]{todonotes}

\title{Generalization Properties of NAS under Activation and Skip Connection Search}

\author{Zhenyu Zhu, \quad Fanghui Liu, \quad Grigorios G Chrysos, \quad Volkan Cevher\vspace{2mm} \\
{\hspace*{\fill}EPFL, Switzerland\hspace*{\fill}}\\
{\hspace*{\fill}\texttt{\{[first name].[surname]\}@epfl.ch}\hspace*{\fill}}
}

\begin{document}
\maketitle

\begin{abstract}
Neural Architecture Search (NAS) has fostered the automatic discovery of state-of-the-art neural architectures. Despite the progress achieved with NAS, so far there is little attention to theoretical guarantees on NAS. In this work, we study the generalization properties of NAS under a unifying framework enabling (deep) layer skip connection search and activation function search. To this end, we derive the lower (and upper) bounds of the minimum eigenvalue of the Neural Tangent Kernel (NTK) under the (in)finite-width regime using a certain search space including mixed activation functions, fully connected, and residual neural networks. We use the minimum eigenvalue to establish generalization error bounds of NAS in the stochastic gradient descent training. Importantly, we theoretically and experimentally show how the derived results can guide NAS to select the top-performing architectures, even in the case without training, leading to a train-free algorithm based on our theory. Accordingly, our numerical validation shed light on the design of computationally efficient methods for NAS. Our analysis is non-trivial due to the coupling of various architectures and activation functions under the unifying framework and has its own interest in providing the lower bound of the minimum eigenvalue of NTK in deep learning theory.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Neural Architecture Search (NAS)~\citep{45826} is a powerful technique that enables the automatic design of neural architectures. NAS defines a set of operations (referred to as the \emph{search space}), that include various activation functions and layer types, or potential connections among layers~\citep{JMLR:v20:18-598, Ren2020ACS}. Optimization over the search space returns the optimal architecture as a subset of the possible combinations of operations. NAS\footnote{\label{foot:generalization_nas_architecture} In the sequel, we interchangeably refer to NAS as the ``architecture obtained from NAS'' or the framework to design the neural architecture.} obtains state-of-the-art results in image recognition~\citep{liu2019auto, ding2020autospeech, zhang2019customizable, chen2019detnas} or can be used to further improve architectures defined by a human expert~\citep{tan2019efficientnet}. The spectacular results obtained by NAS have led to a significant interest in the community to further improve the NAS algorithms, the search space etc. However, to date little focus has been provided in the following question: \emph{Can NAS\textsuperscript{~\ref{foot:generalization_nas_architecture}} achieve generalization guarantees similar to a typical neural network?} 


Neural tangent kernel (NTK)-based analysis \citep{jacot2018neural} is a powerful method for analyzing the optimization and the generalization of deep networks~\citep{pmlr-v97-allen-zhu19a, cao2019generalization, chen2020generalized, arora2019fine}. The minimum eigenvalue of NTK has been used in previous work to demonstrate the global convergence of gradient descent, such as two-layer networks~\citep{du2018gradient}, and deep networks with polynomially wide layers~\citep{pmlr-v97-allen-zhu19a}. Besides, the minimum eigenvalue of NTK is also used to prove generalization bounds~\citep{arora2019fine} and memorization~\citep{montanari2020interpolation}. However, previous work mainly focuses on a limited set of architectures, e.g., fully-connected (FC) neural networks ~\citep{allen2018learning, bartlett2017spectrally} or residual neural networks \citep{7780459,huang2020deep}, in which a single activation function is used throughout the network. These off-the-shelf theoretical results cannot be directly applied to analyze the rich search space (of NAS) that is covering various/mixed architectures and parameters. That makes the non-trivial analysis on NAS worth of study on its own right. 


The recent work of \citet{pmlr-v139-oymak21a} is the first work to provide generalization guarantees on a related problem, i.e., activation functions search. The study provides generalization results on two-layer networks relying on the minimum eigenvalue with a strictly larger than zero assumption, i.e.,  for the NTK matrix .


In this work, we introduce the first theoretical guarantees for multilayer NAS where the search space includes activation functions and skip connections. We study the upper/lower bound of the minimum eigenvalue of NTK (in the (in)finite regime) under mixed activation functions and architectures which evade the minimum eigenvalue assumption of \citet{pmlr-v139-oymak21a}. Then, we provide optimization and generalization guarantees of deep neural networks (DNNs) equipped with NAS. 
Our results indicate that the minimum eigenvalue estimation can act as a powerful metric for NAS. This method, called Eigen-NAS, is train-free, but still effective with experimental validation when compared to recent promising algorithms \citep{pmlr-v139-xu21m, chen2021neural, mellor2021neural}. Formally, our main contribution and findings are summarized below:




i) We build a general theoretical framework based on NTK for NAS with search on popular activation functions in each layer, fully-connected, and skip connections. We derive the NTK formula of these architectures in the (in)finite-width regime under the unifying framework.

ii) We derive the upper and lower bounds of the minimum eigenvalue of the NTK under the (in)finite-width regime for the considered architectures. We introduce a new technique to ensure the probability of concentration inequality remains positive. Our analysis highlights how the upper and lower bounds differs under activation function search and skip connection search and can guide NAS. 

iii) We establish a connection between the minimum eigenvalue and generalization of the searched DNN trained by stochastic gradient descent (SGD). Our theoretical results show that the generalization performance largely depends on the minimum eigenvalue of NTK for NAS, which provides theoretical guarantees for the searched architecture.




iv) Our theoretical results are supported by thorough experimental validations with the following findings: 1) our upper and lower bounds on the minimum eigenvalue largely depend on the activation function in the first layer rather than the activation functions in deeper layers. 2) The applied NAS algorithm always picks up ReLU (Rectified Linear Unit) and LeakyReLU in the optimal architecture, which coincides with our theory that predicts ReLU and LeakyReLU achieve the largest minimum eigenvalues. 3) The skip connections are required in each layer under our not very large DNNs. Furthermore, our experimental evidence on Eigen-NAS indicates that the minimum eigenvalue is a promising metric to guide NAS (without training) as suggested by our theory. 




{\bf Technical challenges.} 
The technical challenges of this paper mainly focus on how to analyze activation functions with different properties and skip connections under a unifying framework. This work is non-trivial; previous works mainly focus on the ReLU activation function~\citep{pmlr-v139-nguyen21g, cao2019generalization, pmlr-v97-allen-zhu19a} in optimization and generalization of a single fully-connected neural network. Their proofs heavily depend on the properties of , e.g., homogeneity and  which are invalid when other commonly-used activation functions, e.g., Tanh, Sigmoid, and Swish, are used. 
This problem becomes harder when mixed activation functions and residual connections are considered.
To tackle these technical challenges, we develop the following techniques: a) to handle the non-homogeneous property of Tanh, Sigmoid, and Swish, we develop a new integral estimation approach for the minimal eigenvalue estimation. b) To establish the connection between the minimum eigenvalues of NTK and generalization errors, we use the Lipschitz continuity to avoid the special property of ReLU.
More importantly, we introduce a new way to use Gershgorin circle theorem for minimum eigenvalue estimation, which avoids concentration inequalities with negative probability in some certain cases \citep{pmlr-v139-nguyen21g}.\looseness-1 \section{Related work}
\label{sec:related_work}

{\bf Network architecture search (NAS):} The idea of NAS stems from \citet{45826}, while the idea of cell search, i.e., searching core building blocks and composing them together, emerged in \citet{zoph2018learning}. The earlier literature used discrete optimization techniques for obtaining the architecture. DARTS~\citep{liu2019darts} considers NAS as a continuous bi-level optimization task. Recent variants of DARTS~\citep{xu2019pc, wu2019fbnet} and several train-free methods~\citep{mellor2021neural, chen2021neural, pmlr-v139-xu21m} have demonstrated success in reducing the search time or improving the search algorithm. However, the aforementioned works have not provided generalization guarantees for the optimal architecture. 


{\bf Optimization and generalization of DNNs via NTK:} In the NTK framework \citep{jacot2018neural,du2019gradient,chen2020much}, the training dynamics of (in)finite-width networks can be exactly characterized by kernel tools. Leveraging NTK facilitates studies on the global convergence of GD~\cite{pmlr-v97-allen-zhu19a, du2019gradient, nguyen2021proof} in DNNs via the minimum eigenvalue of NTK. In fact, it also controls the generalization performance of DNNs \citep{du2018gradient,cao2019generalization,allen2018learning}, which is further studied in \citet{bachspaper}. \section{Problem Settings}
\label{sec:preliminaries}
In this section we introduce the problem setting of our NAS framework based on the search space and algorithm (search strategy) for our paper.


Let  be a compact metric space and . We assume that the training set  is drawn from a probability measure  on , with its marginal data distribution denoted by .
The goal of a supervised learning task is to find a hypothesis (i.e., a neural network used in this work)  such that  parameterized by 
is a good approximation of the label  corresponding to a new sample .
In this paper, we consider the classification task, evaluated by minimizing the expected risk 



where  is the classification loss  as a surrogate of the expected 0-1 loss . In this paper, we employ the cross-entropy loss, which is
defined as .


\emph{Notation:}  For an integer , we use the shorthand . The multivariate standard Gaussian distribution is  with the zero-mean vector  and the identity-variance matrix . We denote the direct sum by . We follow the standard Bachmann–Landau notation in complexity theory e.g., , , , and  for order notation.

\subsection{Neural Networks and Search Space}
\label{ssec:search_space}

In this work, we consider a particular parametrization of  as a deep neural network (DNN) with depth  ()\footnote{Our results hold for the  setting corresponding to one-hidden layer neural network with slight modifications on notation, so we focus on  for simplicity.} which includes the fully-connected (FC) neural networks setting and the residual neural networks setting, and various activation functions in each layer. This enables a quite general NAS setting.
Formally, we define a single-output DNN with the output  in each layer

where the weights of the neural networks are , ,  and . The binary parameter  is for layer search, and the activation function is . 
The neural network output is .

{\bf Architecture search:} A binary vector  represents the skip connections, where the  in~\cref{eq:network} indicates whether there is a skip connection in the -th layer.
Notice that we unify FC and residual neural networks under the same framework.

{\bf Activation function search:} 
We select five representative activation functions defined by  used in~\cref{eq:network},  that can be bounded, unbounded, smooth, non-smooth, monotonic, or non-monotonic, as reported in~\cref{tab:activation_functions}.
We define  with  for any  as the indicator to show which activation function is selected in each layer.
Our NAS framework allows for a different activation function in each layer, which enlarges the search space.




 
In our setting, we conduct the architecture search and the skip connection search independently, and accordingly, our search space is defined as the direct sum of them:

where  represents the collection of weight matrices and indicator for skips and selected activation functions for all layers.

\begin{table*}[t]
\small
\centering
\begin{threeparttable}
\caption{Formula of different activation functions, definitions of relevant constants and some intermediate results.}
\label{tab:activation_functions}
\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{c|c|c|c|c|c}
    \toprule[1pt]
     & ReLU & LeakyReLU & Sigmoid\tnote{[1]} & Tanh\tnote{[2]} & Swish\\
    \midrule
    Formula &   &  &  & &   \
K^{(L)}(\bm{x},\widetilde{\bm{x}}) := \mathbb{E}_{\bm{W}}\left \langle \frac{\partial f(\bm{x};\bm{W})}{\partial\bm{W}},\frac{\partial f(\widetilde{\bm{x}};\bm{W})}{\partial\bm{W}}  \right \rangle\,,

\small
    \begin{split}
        & \bm{G}^{(1)}=\bm{XX}^\top\,,\quad\bm{A}^{(2)} = \bm{G}^{(2)}=2\mathbb{E}_{\bm{w} \sim \mathcal N(\bm 0,\mathbb{I}_{d})}[\sigma_1(\bm{Xw})\sigma_1(\bm{Xw})^\top]\,,\\
        & \bm{G}^{(l)}=2\mathbb{E}_{\bm{w} \sim \mathcal N(\bm 0,\mathbb{I}_{N})}[\sigma_{l-1}(\sqrt{\bm{A}^{(l-1)}} \bm{w})\sigma_{l-1}(\sqrt{\bm{A}^{(l-1)}} \bm{w})^\top]\,,\quad\bm{A}^{(l)}=\bm{G}^{(l)}+\alpha_{l-2}\bm{A}^{(l-1)}\,,\\
        & \dot{\bm{G}}^{(s)} = 2\mathbb{E}_{\bm{w} \sim \mathcal N(\bm{0},\mathbb{I}_{N})}[{\sigma}'_{s-1}(\sqrt{\bm{A}^{(s-1)}} \bm{w}){\sigma}'_{s-1}(\sqrt{\bm{A}^{(s-1)}} \bm{w})^\top]\,.
    \end{split}

    \bm{K}^{(L)}=\bm{G}^{(L)} + \sum_{l=1}^{L-1}\bm{G}^{(l)}\circ \dot{\bm{G}}^{(l+1)} \circ (\dot{\bm{G}}^{(l+2)}+  \alpha_{l}\bm{1}_{N \times N})\circ \cdots \circ (\dot{\bm{G}}^{(L)}+ \alpha_{L-2}\bm{1}_{N \times N})\,.

\lambda _{\min}(\bm{K}^{(L)}) \geq \mu_{r}(\sigma_1)^{2}\prod_{p=3}^{L}\Bigg(\beta_3(\sigma_{p-1})+\alpha_{p-2}\Bigg)\,,

\lambda _{\min}(\bm{K}^{(L)}) \leq\sum_{l=1}^{L}\Bigg(\beta_1(\sigma_{l-1}) \prod_{p=2}^{l-1} \big(\beta_1(\sigma_{p-1})+\alpha_{p-2}) \big) \prod_{p=l+1}^{L}(\beta_2(\sigma_{p-1})+\alpha_{p-2}) \Bigg) \,, 

    \Theta \bigg(\prod_{i=2}^{L-1}(\beta_3(\sigma_i)+\alpha_{i-1} ) \bigg) \leq \lambda_{\min}(\bm{JJ}^{\top})\leq \sum_{k=0}^{L-1}\Theta \left(\prod_{i=k+2}^{L-1}(\beta_2(\sigma_i)+\alpha_{i-1} ) \right)\,,

\mathbb{E}[\ell_{\mathcal{D} }^{0-\!1}\!(\hat{\bm{W}}\!)] \leq \tilde{\mathcal{O} } \left(\! C_2\sqrt{\frac{\bm{y}^{\top}  ({\bm{K}^{(L)}})^{-1} \bm{y}}{N}} \!\right) + \mathcal{O}\!\left(  \sqrt{\frac{\log(1/\delta )}{N} }  \right)\,,

\mathcal{B} (\bm{W},\omega ) :=\left \{ \bm{W}'\in \mathcal{W}: \left \| \bm{W}'_l - \bm{W}_l \right \|_{\mathrm{F}} \leq \omega ,\bm{\alpha}'=\bm{\alpha},\bm{\sigma}'=\bm{\sigma}, l \in [L]   \right \}\,.

\small
\begin{matrix}
\widetilde{\bm{g}}_{i,1} = \widetilde{\bm{W}}_1 \bm{x}_i \,, & \bm{g}_{i,1} = \bm{W}_1\bm{x}_i \,, & for\ i \in [N],\\
\widetilde{\bm{f}}_{i,1} = \sigma_1  (\widetilde{\bm{W}}_1\bm{x}_i) \,, & \bm{f}_{i,1} = \sigma_1 (\bm{W}_1\bm{x}_i) \,,& for\ i \in [N],\\
\widetilde{\bm{g}}_{i,l} = \widetilde{\bm{W}}_{l}\widetilde{\bm{f}}_{i,l-1} \,,   & \bm{g}_{i,l} = \bm{W}_{l}\bm{f}_{i,l-1} \,, & for\ i \in [N] \ \text{and}\  l \!=\! 2,\dots, L-1,\\
\widetilde{\bm{f}}_{i,l} = \sigma_l (\widetilde{\bm{W}}_{l}\widetilde{\bm{f}}_{i,l-1})+\alpha_{l-1}\widetilde{\bm{f}}_{i,l-1} \,,  & \bm{f}_{i,l} = \sigma_l (\bm{W}_{l}\bm{f}_{i,l-1})+\alpha_{l-1}\bm{f}_{i,l-1} \,, & for\ i \in [N] \ \text{and}\  l \!=\! 2,\dots, L-1\,.
\end{matrix}

\bigcirc_{i=1}^{r}(\bm{X}_i) = \bm{X}_1 \circ \bm{X}_2 \circ \cdots \circ \bm{X}_r\,.

\begin{split}
    &\bm{A}^{(1)}=\bm{XX}^\top\,,\\
    &\bm{A}^{(2)}=2\mathbb{E}_{\bm{w} \sim \mathcal N(\bm 0,\mathbb{I}_{d})}[\sigma_1(\bm{Xw})\sigma_1(\bm{Xw})^\top]\,,\\
    &\bm{A}^{(l)}=2\mathbb{E}_{\bm{w} \sim \mathcal N(\bm 0,\mathbb{I}_{N})}[\sigma_{l-1}(\sqrt{\bm{A}^{(l-1)}} \bm{w})\sigma_{l-1}(\sqrt{\bm{A}^{(l-1)}} \bm{w})^\top]+\alpha_{l-2}\bm{A}^{(l-1)}\,.
\end{split}

    \begin{split}
        & \bm{A}^{(1)}=\bm{G}^{(1)}=\bm{XX}^\top\,,\\
        & \bm{A}^{(2)} = \bm{G}^{(2)}=2\mathbb{E}_{\bm{w} \sim \mathcal N(\bm 0,\mathbb{I}_{d})}[\sigma_1(\bm{Xw})\sigma_1(\bm{Xw})^\top]\,,\\
        & \bm{G}^{(l)}=2\mathbb{E}_{\bm{w} \sim \mathcal N(\bm 0,\mathbb{I}_{N})}[\sigma_{l-1}(\sqrt{\bm{A}^{(l-1)}} \bm{w})\sigma_{l-1}(\sqrt{\bm{A}^{(l-1)}} \bm{w})^\top]\,,\\
        & \bm{A}^{(l)}=\bm{G}^{(l)}+\alpha_{l-2}\bm{A}^{(l-1)}\,.    
    \end{split}

    \bm{K}^{(L)}=\sum_{l=1}^{L}\bm{G}^{(l)}\circ \dot{\bm{G}}^{(l+1)} \circ (\dot{\bm{G}}^{(l+2)}+  \alpha_{l}\bm{1}_{N \times N})\circ \cdots \circ (\dot{\bm{G}}^{(L)}+ \alpha_{L-2}\bm{1}_{N \times N})\,,

    G_{ii}^{(l)} \leq \begin{cases}
1  & \text{ if } l=1 \\
2(2+\eta^2)^{l-2}  & \text{ if } l \geq 2\,.
\end{cases}
\label{eq:Gii_upper_bound}

\begin{split}
    G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2] =\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\max(0,x)^2 \mathrm{d}x\\
    &=\int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}x^2 \mathrm{d}x \\
    &=A_{ii}^{(l-1)}\,.
    \end{split}
\label{eq:Gii_upper_bound_RelU}

\begin{split}
    G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2] =\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\max(\eta x,x)^2 \mathrm{d}x\\
    &=\int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}x^2 \mathrm{d}x +\int_{-\infty}^{0}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\eta^2 x^2 \mathrm{d}x\\
    &=(1+\eta^2)A_{ii}^{(l-1)}\,.
\end{split}
\label{eq:Gii_upper_bound_LeakyReLU}

\begin{split}
    G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2]=\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f_{\mathrm{Sigmoid}}(x)^2 \mathrm{d}x\\
    & \leq \int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}(\frac{1}{2})^2 \mathrm{d}x\\
    & = \frac{1}{2}\,.
\end{split}
\label{eq:Gii_upper_bound_Sigmoid}

\begin{split}
    G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2]=\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f_{\mathrm{Tanh}}(x)^2 \mathrm{d}x\\
    & \leq \int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\mathrm{d}x\\
    & = 2\,.
\end{split}
\label{eq:Gii_upper_bound_Tanh}

\small
\begin{split}
G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2]=\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f_{\mathrm{Swish}}(x)^2 \mathrm{d}x\\
& = \int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\frac{x^2}{(1+e^{-x})^2} \mathrm{d}x\\
& = \int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}x^2\times \bigg(\frac{1}{(1+e^{-x})^2}+\frac{1}{(1+e^{x})^2}\bigg) \mathrm{d}x\\
& \leq \int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}x^2\mathrm{d}x\\
& = A_{ii}^{(l-1)}\,.
\end{split}
\label{eq:Gii_upper_bound_Swish}

A_{ii}^{(l)}=G_{ii}^{(l)}+ \alpha_{l-2}A_{ii}^{(l-1)} = (1+\alpha_{l-2})A_{ii}^{(l-1)}\,.
\label{eq:Aii_bound_ReLU}

A_{ii}^{(l)}=G_{ii}^{(l)}+\alpha_{l-2}A_{ii}^{(l-1)} = (1+\alpha_{l-2}+\eta^2)A_{ii}^{(l-1)}\,.
\label{eq:Aii_bound_LeakyReLU}

\small
\begin{split}
G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2] =\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f_{\mathrm{Swish}}(x)^2 \mathrm{d}x\\
& = \int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\frac{x^2}{(1+e^{-x})^2} \mathrm{d}x\\
& = \int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}x^2\times \bigg(\frac{1}{(1+e^{-x})^2}+\frac{1}{(1+e^{x})^2}\bigg) \mathrm{d}x\\
& \geq \int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}x^2\times \frac{1}{2}\mathrm{d}x\\
& = \frac{1}{2}A_{ii}^{(l-1)}\,,
\end{split}
\label{eq:Gii_lower_bound_Swish}

\left(\frac{1}{2}+\alpha_{l-2}\right) A_{ii}^{(l-1)} \leq A_{ii}^{(l)}\leq (1+\alpha_{l-2})A_{ii}^{(l-1)}\,.
\label{eq:Aii_bound_Swish}

\begin{split}
    G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2] =\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f_{\mathrm{Sigmoid}}(x)^2 \mathrm{d}x\\
    & \leq \int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\bigg(\frac{1}{4} - e^{-\frac{x^2}{4}}\bigg)\mathrm{d}x = \frac{1}{2}-\frac{1}{2\sqrt{1+\frac{A_{ii}^{(l-1)}}{2}}}\\
    & \leq \frac{A_{ii}^{(l-1)}}{8}\,,\quad\quad\quad\text{holds for }\,.
\end{split}
\label{eq:Gii_upper_bound_Sigmoid_2}

\begin{split}
    G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2] =\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f_{\mathrm{Sigmoid}}(x)^2 \mathrm{d}x\\
    & \geq \int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\bigg(\frac{1}{4} - e^{-\frac{x^2}{8}}\bigg)\mathrm{d}x\\
    & =\frac{1}{2}-\frac{1}{2\sqrt{1+\frac{A_{ii}^{(l-1)}}{4}}} \\
    & \geq \left(\frac{1}{2}-\frac{1}{2\sqrt{1+\frac{G_{\max}}{4}}}\right)\frac{A_{ii}^{(l-1)}}{G_{\max}}\,,
\end{split}
\label{eq:Gii_lower_bound_Sigmoid}

\left( \left[\frac{1}{2}-\frac{1}{2\sqrt{1+\frac{G_{\max}}{4}}}\right]\frac{1}{G_{\max}}+\alpha_{l-2}\right)A_{ii}^{(l-1)} \leq A_{ii}^{(l)}\leq \left(\frac{1}{8}+\alpha_{l-2}\right)A_{ii}^{(l-1)} \,.
\label{eq:Aii_bound_Sigmoid}

\begin{split}
    G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2] =\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f_{\mathrm{Tanh}}(x)^2 \mathrm{d}x\\
    & \leq \int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}(1 - e^{-x^2})\mathrm{d}x = 2-\frac{2}{\sqrt{1+2A_{ii}^{(l-1)}}}\\
    & \leq 2A_{ii}^{(l-1)}\,, \quad\quad\quad\text{holds for }\,.
\end{split}
\label{eq:Gii_upper_bound_Tanh_2}

\begin{split}
    G_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[\sigma_{l-1}(w)^2] =\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f_{\mathrm{Tanh}}(x)^2 \mathrm{d}x\\
    & \geq \int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}(1 - e^{-\frac{x^2}{2}})\mathrm{d}x\\
    & = 2-\frac{2}{\sqrt{1+A_{ii}^{(l-1)}}}\\
    & \geq\bigg(2-\frac{2}{\sqrt{1+G_{\max}}}\bigg)\frac{A_{ii}^{(l-1)}}{G_{\max}}\,.
    \end{split}
\label{eq:Gii_lower_bound_Tanh}

\bigg(\bigg[2-\frac{2}{\sqrt{1+G_{\max}}}\bigg]\frac{1}{G_{\max}}+\alpha_{l-2}\bigg)A_{ii}^{(l-1)} \leq A_{ii}^{(l)}\leq (2+\alpha_{l-2})A_{ii}^{(l-1)}\,.
\label{eq:Aii_bound_Tanh}

\begin{split}
    \dot{G}_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[{\sigma}'_{l-1}(w)^2] =\int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}} \mathrm{d}x\\
    &=1\,.
\end{split}
\label{eq:dotGii_RelU}

\begin{split}
    \dot{G}_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[{\sigma}'_{l-1}(w)^2] \\
    &=\int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}} \mathrm{d}x +\int_{-\infty}^{0}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}} \eta^2 \mathrm{d}x\\
    &=1+\eta^2\,.
\end{split}
\label{eq:dotGii_LeakyReLU}

f_{\mathrm{S}}(G_{\max})\leq\dot{G}_{ii}^{(l)}=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[{\sigma}'_{l-1}(w)^2] =f_{\mathrm{S}}(A_{ii}^{(l-1)})\leq f_{\mathrm{S}}(0) \leq \frac{1}{8}\,.
\label{eq:dotGii_Sigmoid_1}

f_{\mathrm{T}}(G_{\max})\leq\dot{G}_{ii}^{(l)}=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[{\sigma}'_{l-1}(w)^2] =f_{\mathrm{T}}(A_{ii}^{(l-1)})\leq f_{\mathrm{T}}(0)\leq 2\,.
\label{eq:dotGii_Tanh_1}

\begin{split}
\dot{G}_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[{\sigma}'_{l-1}(w)^2]=\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f'_{\mathrm{Swish}}(x)^2 \mathrm{d}x\\
&\leq \int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\bigg(\sup_x f'_{\mathrm{Swish}}(x)\bigg)^2 \mathrm{d}x\\
&\quad + \int_{-\infty}^{0}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\bigg(\inf_x f'_{\mathrm{Swish}}(x)\bigg)^2 \mathrm{d}x\\
&=\bigg(\inf_x f_{\mathrm{Swish}}'(x)\bigg)^2+\bigg(\sup_x f'_{\mathrm{Swish}}(x)\bigg)^2\\
&\leq 1.22\,,
\end{split}
\label{eq:dotGii_Swish_1}

\begin{split}
\dot{G}_{ii}^{(l)}&=2\mathbb{E}_{w \sim \mathcal N(0,A_{ii}^{(l-1)})}[{\sigma}'_{l-1}(w)^2]=\int_{-\infty}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}f'_{\mathrm{Swish}}(x)^2 \mathrm{d}x\\
& = \int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\bigg(f'_{\mathrm{Swish}}(x)^2+f'_{\mathrm{Swish}}(-x)^2\bigg)\mathrm{d}x\\
&\geq \int_{0}^{\infty}\frac{2}{\sqrt{2\pi A_{ii}^{(l-1)}}}e^{-\frac{x^2}{2A_{ii}^{(l-1)}}}\frac{1}{2}\mathrm{d}x\\
&= \frac{1}{2}\,.
\end{split}
\label{eq:dotGii_Swish_2}

    \bm{K}^{(L)}=\bm{G}^{(L)} + \sum_{l=1}^{L-1}\bm{G}^{(l)}\circ \dot{\bm{G}}^{(l+1)} \circ (\dot{\bm{G}}^{(l+2)}+  \alpha_{l}\bm{1}_{N \times N})\circ \cdots \circ (\dot{\bm{G}}^{(L)}+ \alpha_{L-2}\bm{1}_{N \times N})\,.

\lambda _{\min}(\bm{K}^{(L)})\geq \sum_{l=1}^{L}\lambda _{\min}(\bm{G}^{(l)})\min_{i\in[N]}\prod_{p=l+1}^{L}\bigg(\dot{G}^{(p)}_{ii}+\alpha_{p-2}\bigg)\,.

\begin{split}
\lambda_{\min}(\bm{G}^{(2)}) & = \lambda_{\min}\bigg(2\mathbb{E}_{\bm{w} \sim \mathcal N(0,\mathbb{I}_{d})}[\sigma_1(\bm{Xw})\sigma_1(\bm{Xw})^\top]\bigg)\\
& = 2\lambda_{\min}\bigg(\sum_{s=0}^{\infty}\mu_{s}(\sigma_1)^{2}\bigcirc_{i=1}^{s}(\bm{XX}^{\top})\bigg) \quad \text{\citep[Lemma D.3]{NEURIPS2020_8abfe8ac}}\\
& \geq 2\mu_{r}(\sigma_1)^{2}\lambda_{\min}(\bigcirc_{i=1}^{r}\bm{XX}^{\top}) \quad \bigg(\mbox{taking~}r \geq \frac{\log (2N)}{1-C_{\text{max}}}\bigg)\\
& \geq 2\mu_{r}(\sigma_1)^{2}\bigg(\min_{i\in [N]}\left \| \bm{x}_i \right \|_{2}^{2r}-(N-1)\max_{i\neq j}\left | \left \langle \bm{x}_i, \bm{x}_j \right \rangle \right |^r\bigg) \quad \text{(Gershgorin circle theorem)}\\
& \geq 2\mu_{r}(\sigma_1)^{2}\bigg(1-(N-1)C_{\text{max}}^r\bigg) \quad \left( \mbox{using~\cref{assumption:distribution_1}}  \right) \\
& \geq 2\mu_{r}(\sigma_1)^{2}\bigg(1-(N-1)\bigg(1-\frac{\log (2N)}{r}\bigg)^r\bigg) \\
& \geq 2\mu_{r}(\sigma_1)^{2}\bigg(1-(N-1)\exp(-\log(2N))\bigg) \\
& \geq \mu_{r}(\sigma_1)^{2}\,,
\end{split}

\begin{split}
\lambda _{\min}(\bm{K}^{(L)}) & \geq \sum_{l=1}^{L}\lambda _{\min}(\bm{G}^{(l)})\min_{i\in[N]}\prod_{p=l+1}^{L}(\dot{G}^{(p)}_{ii}+\alpha_{p-2}) \geq \lambda _{\min}(\bm{G}^{(2)})\min_{i\in[N]}\prod_{p=3}^{L}(\dot{G}^{(p)}_{ii}+\alpha_{p-2})\\
& \geq  \mu_{r}(\sigma_1)^{2} \min_{i\in[N]}\prod_{p=3}^{L}(\dot{G}^{(p)}_{ii}+\alpha_{p-2}), \quad \bigg(r \geq \frac{\log (2n)}{1-C_{\text{max}}}\bigg) \,.
\end{split}
\label{eq:NTK_infinity_lower_bound}

\prod_{p=l+1}^{L}(\dot{G}^{(p)}_{ii}+\alpha_{p-2}) \leq \prod_{p=l+1}^{L}\bigg(\beta_2(\sigma_{p-1})+\alpha_{p-2}\bigg)\,,
\label{eq:dot_gii_prod_bound_1}

\prod_{p=3}^{L}(\dot{G}^{(p)}_{ii}+\alpha_{p-2}) \geq \prod_{p=3}^{L}\bigg(\beta_3(\sigma_{p-1})+\alpha_{p-2}\bigg)\,.
\label{eq:dot_gii_prod_bound_2}

\lambda _{\min}(\bm{K}^{(L)}) \leq \frac{1}{N} \sum_{i=1}^{N} \sum_{l=1}^{L}(G^{(l)}_{ii})\prod_{p=l+1}^{L}(\dot{G}^{(p)}_{ii}+\alpha_{p-2})\,.
\label{eq:NTK_infinity_upper_bound}

\small
G^{(l)}_{ii} = \beta_1(\sigma_{l-1}) A^{(l-1)}_{ii} \leq \beta_1(\sigma_{l-1}) \prod_{p=3}^{l-1} \bigg(\beta_1(\sigma_{p-1})+\alpha_{p-2}\bigg) A^{(2)}_{ii}\leq \beta_1(\sigma_{l-1}) \prod_{p=2}^{l-1} \bigg(\beta_1(\sigma_{p-1})+\alpha_{p-2}\bigg)\,.
\label{eq:gii_bound}

\lambda _{\min}(\bm{K}^{(L)})\leq \sum_{l=1}^{L}\bigg(\beta_1(\sigma_{l-1}) \prod_{p=2}^{l-1} \bigg[\beta_1(\sigma_{p-1})+\alpha_{p-2}\bigg]\prod_{p=l+1}^{L}\bigg[\beta_2(\sigma_{p-1})+\alpha_{p-2}\bigg]\bigg)\,.
\label{eq:NTK_infinity_upper_bound_final}

\lambda _{\min}(\bm{K}^{(L)}) \geq  \mu_{r}(\sigma_1)^{2}\prod_{p=3}^{L}\bigg(\beta_3(\sigma_{p-1})+\alpha_{p-2}\bigg), \quad \bigg(r \geq \frac{\log (2n)}{1-C_{\text{max}}}\bigg)\,.

\mu_{r}(\sigma_1)^{2}\prod_{p=3}^{L}(1+\alpha_{p-2}) \leq\lambda _{\min}(\bm{K}^{(L)})\leq \sum_{l=1}^{L}\bigg(\frac{\prod_{p=2}^{L} (1+\alpha_{p-2})}{1+\alpha_{l-2}}\bigg)\,.

\mu_{r}(\sigma_1)^{2}\prod_{p=3}^{L}(1+\eta^2+\alpha_{p-2}) \leq \lambda _{\min}(\bm{K}^{(L)})\leq (1+\eta^2)\sum_{l=1}^{L} \bigg(\frac{\prod_{p=2}^{L} (1+\eta^2+\alpha_{p-2})}{1+\eta^2+\alpha_{l-2}}\bigg)\,.

\mu_{r}(\sigma_1)^{2}\prod_{p=3}^{L}(f_{\mathrm{S}}(\frac{1}{2})+\alpha_{p-2}) \leq \lambda _{\min}(\bm{K}^{(L)})\leq\frac{1}{8} \sum_{l=1}^{L}\bigg(\frac{\prod_{p=2}^{L} (\frac{1}{8}+\alpha_{p-2})}{\frac{1}{8}+\alpha_{l-2}}\bigg)\,.
\label{eq:lambda_min_inf_Sigmoid}

\mu_{r}(\sigma_1)^{2}\prod_{p=3}^{L}(f_{\mathrm{T}}(2)+\alpha_{p-2}) \leq \lambda _{\min}(\bm{K}^{(L)})\leq 2\sum_{l=1}^{L}\bigg(\frac{\prod_{p=2}^{L}(2+\alpha_{p-2})}{2+\alpha_{l-2}}\bigg)\,.
\label{eq:lambda_min_inf_Tanh}

\mu_{r}(\sigma_1)^{2}\prod_{p=3}^{L}(\frac{1}{2}+\alpha_{p-2}) \leq \lambda _{\min}(\bm{K}^{(L)})\leq \sum_{l=1}^{L}\bigg(\prod_{p=2}^{l-1} (1+\alpha_{p-2})\prod_{p=l+1}^{L}(1.22+\alpha_{p-2})\bigg)\,.

\bar{\bm{K}}^{(L)}=\bm{JJ}^{\top}=\sum_{l=1}^{L}\bigg[\frac{\partial \bm{F}}{\partial \mathrm{vec}(\bm{W}_l)}\bigg]\bigg[\frac{\partial \bm{F}}{\partial \mathrm{vec}(\bm{W}_l)}\bigg]^{\top}\,.

\bm{JJ}^{\top}=\sum_{k=0}^{L-1}\bm{F}_k\bm{F}_k^{\top}\circ \bm{B}_{k+1}\bm{B}_{k+1}^{\top}\,,

(\bm{B}_k)_{i:}=\left\{\begin{matrix}
\bm{D}_{i,k}\prod _{l=k+1}^{L-1}(\bm{W}_l \bm{D}_{i,l}+\alpha_{l-1}\bm{I}_{m\times m})\bm{W}_L, k \in [L-2]\,,\\
\bm{D}_{i,L-1} \bm{W}_L,\qquad\qquad\qquad\qquad\qquad\qquad k = L-1\,,\\ 
1,\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \  k = L\,.
\end{matrix}\right.
\label{eq:NTK_finite_3}

\left \| \bm{f}_k(\bm{x}) \right \|_2^2 = \Theta (1)\,,

\mathbb{E}_{\bm{x}}\left \| \bm{f}_k(\bm{x}) \right \|_2^2 = \Theta (1)\,,

\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2 = \Theta (1)\,,

\left \| \bm{f}_k(\bm{x}) \right \|_2^2 = \sum_{j = 1}^{m} f_{k}^{[j]}(\bm{x})^2\,.
\label{eq:proof_lemma_C.1_in_ICML_1}

\begin{split}
    \mathbb{E}_{\bm{W}_k} \left \| \bm{f}_k(\bm{x}) \right \|_2^2 & = \sum_{j = 1}^{m} \mathbb{E}_{\bm{w}_j} [f_{k}^{[j]}(\bm{x})^2]\\
    & = \sum_{j = 1}^{m}\mathbb{E}_{\bm{w}_j}\bigg(\bigg[\sigma_k\bigg(\left \langle \bm{w}_j, \bm{f}_{k-1}(\bm{x}) \right \rangle \bigg)+\alpha_{k-1}f_{k-1}^{[j]}(\bm{x})\bigg]^2\bigg) \quad\quad ~\cref{eq:network}\\
    & = \sum_{j = 1}^{m}\bigg(\mathbb{E}_{\bm{w}_j}\bigg[\bigg(\sigma_k(\left \langle \bm{w}_j, \bm{f}_{k-1}(\bm{x}) \right \rangle \bigg)^2\bigg]+\mathbb{E}_{\bm{w}_j}\bigg(\alpha_{k-1}^2 f_{k-1}^{[j]}(\bm{x})^2\bigg)\\
    & +\mathbb{E}_{\bm{w}_j}\bigg[2\sigma_k\bigg(\left \langle \bm{w}_j, \bm{f}_{k-1}(\bm{x}) \right \rangle\bigg) \alpha_{k-1} f_{k-1}^{[j]}(\bm{x})\bigg]\bigg)\\
    & =m\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w)^2)+\sum_{j = 1}^{m}\alpha_{k-1}^2\mathbb{E}_{\bm{w}_j}\bigg( f_{k-1}^{[j]}(\bm{x})^2\bigg)\\
    & +2\sum_{j = 1}^{m}\alpha_{k-1}\mathbb{E}_{\bm{w}_j}\bigg(\sigma_k\bigg[\left \langle \bm{w}_j, \bm{f}_{k-1}(\bm{x}) \right \rangle\bigg]\bigg)\mathbb{E}_{\bm{w}_j}\bigg(f_{k-1}^{[j]}(\bm{x})\bigg)\\
    & = m\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w)^2)+\alpha_{k-1}^2\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2\\
    & +2\alpha_{k-1}\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w))\sum_{j = 1}^{m}f_{k-1}^{[j]}(\bm{x})\,.
\end{split}
\label{eq:proof_lemma_C.1_in_ICML_2}

m\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w)^2) = m\Theta \bigg(\frac{\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2}{m}\bigg) = \Theta(\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2)\,.
\label{eq:proof_lemma_C.1_in_ICML_3}

\frac{1}{2}\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2 \leq m\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w)^2) \leq (1+\eta^2)\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2\,,

0<\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w))\leq\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(f_{\mathrm{ReLU}}(w))=\frac{2\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2}{5\sqrt{m}}\,.

-\sqrt{m}\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2\leq\sum_{j = 1}^{m}f_{k-1}^{[j]}(\bm{x})\leq \sqrt{m}\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2\,.

-\frac{2\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2}{5}\leq2\alpha_{k-1}\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w))\sum_{j = 1}^{m}f_{k-1}^{[j]}(\bm{x})\leq \frac{2\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2}{5}\,.

\begin{split}
    \mathbb{E}_{\bm{W}_k} \left \| \bm{f}_k(\bm{x}) \right \|_2^2 & =m\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w)^2)+\alpha_{k-1}^2\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2\\
    & +2\alpha_{k-1}\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w))\sum_{j = 1}^{m}f_{k-1}^{[j]}(\bm{x})\\
    & \leq \bigg(1+\eta^2+\alpha_{k-1}+\frac{2}{5}\bigg)\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2\\
    & \leq \bigg(\eta^2+\frac{12}{5}\bigg)\Theta(1)\,,
\end{split}

\begin{split}
    \mathbb{E}_{\bm{W}_k} \left \| \bm{f}_k(\bm{x}) \right \|_2^2 & =m\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}(\sigma_k(w)^2)+\alpha_{k-1}^2\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2\\
    & +2\alpha_{k-1}\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m)}(\sigma_k(w))\sum_{j = 1}^{m}f_{k-1}^{[j]}(\bm{x})\\
    & \geq \bigg(\frac{1}{2}+\alpha_{k-1}-\frac{2}{5}\bigg)\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2\\
    & \geq \frac{1}{10}\Theta(1)\,.
\end{split}

    \mathbb{E}_{\bm{W}_k} \left \| \bm{f}_k(\bm{x}) \right \|_2^2 =\Theta(1)\,.
\label{eq:proof_lemma_C.1_in_ICML_10}

\mathbb{E}_{w\sim \mathcal N(0,\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2/m )}[\sigma_k(w)] = 0\,.

\begin{split}
    \mathbb{E}_{\bm{W}_k} \left \| f_k(\bm{x}) \right \|_2^2 & = \Theta(\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2) + \alpha_{k-1}^2\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2\\
    & = \Theta(\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2)\,.
\end{split}
\label{eq:proof_lemma_C.1_in_ICML_12}

    \mathbb{E}_{\bm{W}_k} \left \| \bm{f}_k(\bm{x}) \right \|_2^2 =\Theta(1).

\frac{1}{2} \mathbb{E}_{\bm{W}_k} \left \| \bm{f}_k(\bm{x}) \right \| _2^2 \leq \left \| \bm{f}_k(\bm{x}) \right \| _2^2 \leq \frac{3}{2} \mathbb{E}_{\bm{W}_k} \left \| \bm{f}_k(\bm{x}) \right \| _2^2\,,

\left \| \bm{f}_{k}(\bm{x}) \right \|_2^2 = \Theta (1)\,,

\begin{split}
    \mathbb{E}_{\bm{W}_k}\left \| \bm{D}_k \right \|_{\mathrm{F}}^2 = m\mathbb{E}_{\bm{w}_1}[{\sigma'_k}^2(\left \langle \bm{f}_{k-1}(\bm{x}),\bm{w}_1 \right \rangle)] = m\mathbb{E}_{w\sim \mathcal N(0,{\left \| \bm{f}_{k-1}(\bm{x}) \right \|_2^2}/{m} )}[{\sigma'_k}^2(w)]\,.
\end{split}

\mathbb{E}_{\bm{W}_k}\left \| \bm{D}_k \right \|_{\mathrm{F}}^2 =m\Theta (1) = \Theta (m)\,.

    \mathbb{P} \left(\left | \left \| \bm{D}_k \right \|_{\mathrm{F}}^2 - \mathbb{E}_{\bm{W}_k}\left \| \bm{D}_k \right \|_{\mathrm{F}}^2 \right |>t \right)\leq 2\exp \left(-\frac{2t^2}{m} \right)\,.

\small
\Theta \bigg(m\prod_{i=k+1}^{p}(\beta_3(\sigma_i)+\alpha_{i-1} ) \bigg) \leq \left \| \bm{D}_{k}\prod_{l=k+1}^{p}\bigg(\bm{W}_l \bm{D}_{l}+\alpha_{l-1}\bm{I}_{m\times m}\bigg) \right \|_F^2 \leq \Theta \bigg(m\prod_{i=k+1}^{p}(\beta_2(\sigma_i)+\alpha_{i-1} ) \bigg)\,,

    \left \| \bm{S}_p \right \| _{\mathrm{F}}^2 = \sum_{j=1}^{m} \left \| \bm{S}_{p-1} \bm{w}_j \right \| _2^2{\sigma'_p}(\left \langle \bm{f}_{p-1}(\bm{x}),\bm{w}_j \right \rangle )^2 + \alpha_{p-1}\left \| \bm{S}_{p-1} \right \| _{\mathrm{F}}^2 + 2 \alpha_{p-1}\left \langle \bm{S}_{p-1}\bm{W}_p \bm{D}_p, \bm{S}_{p-1} \right \rangle \,.

\begin{split}
    \mathbb{E}_{\bm{W}_p} \left \| \bm{S}_p \right \| _{\mathrm{F}}^2 & = m \mathbb{E}_{\bm{w} \sim \mathcal N(0,\mathbb{I}_{m}/m )}\left \| \bm{S}_{p-1}\bm{w} \right \| _2^2{\sigma'_p}(\left \langle \bm{f}_{p-1}(\bm{x}),\bm{w} \right \rangle )^2 + \alpha_{p-1}\left \| \bm{S}_{p-1} \right \| _{\mathrm{F}}^2 + \mathbb{E}_{\bm{W}_p} 2 \alpha_{p-1}\left \langle \bm{S}_{p-1}\bm{W}_p \bm{D}_p, \bm{S}_{p-1} \right \rangle\\
    & = m \mathbb{E}_{\bm{w} \sim \mathcal N(0, \mathbb{I}_{m}/m )}\left \| \bm{S}_{p-1}\bm{w} \right \| _2^2\mathbb{E}_{\bm{w} \sim \mathcal N(0, \mathbb{I}_{m}/m )}{\sigma'_p}(\left \langle \bm{f}_{p-1}(\bm{x}),\bm{w} \right \rangle )^2+ \alpha_{p-1}\left \| \bm{S}_{p-1} \right \| _{\mathrm{F}}^2+0\\
    & = \left \| \bm{S}_{p-1} \right \| _{\mathrm{F}}^2\mathbb{E}_{\bm{w} \sim \mathcal N(0, \left \| \bm{f}_{p-1}(\bm{x}) \right \|_2^2/m)}{\sigma'_p}(w)^2+\alpha_{p-1}\left \| \bm{S}_{p-1} \right \| _{\mathrm{F}}^2\,.
\end{split}

    \beta_3(\sigma_p) \leq \mathbb{E}_{w \sim \mathcal N(0, \left \| \bm{f}_{p-1}(\bm{x}) \right \|_2^2/m)}{\sigma'_p}(w)^2 \leq \beta_2(\sigma_p)\,.

    (\beta_3(\sigma_p)+\alpha_{p-1})\left \| \bm{S}_{p-1} \right \| _{\mathrm{F}}^2 \leq\mathbb{E}_{\bm{W}_p} \left \| \bm{S}_p \right \| _{\mathrm{F}}^2 \leq (\beta_2(\sigma_p)+\alpha_{p-1})\left \| \bm{S}_{p-1} \right \| _{\mathrm{F}}^2\,.

\left \| \left \| \bm{S}_{p-1}\bm{w}_j \right \| _2^2{\sigma'_p}(\left \langle \bm{f}_{p-1}(\bm{x}),\bm{w}_j \right \rangle )^2 \right \| _{\psi _1} \leq \left \| \left \| \bm{S}_{p-1}\bm{w}_j \right \| _2 \right \| _{\psi _2}^2 \leq \frac{c}{m}\left \| \bm{S}_{p-1} \right \| _{\mathrm{F}}^2\,.

\frac{1}{2} \mathbb{E}_{\bm{W}_p} \left \| \bm{S}_p \right \| _{\mathrm{F}}^2 \leq \left \| \bm{S}_p \right \| _{\mathrm{F}}^2 \leq \frac{3}{2} \mathbb{E}_{\bm{W}_p} \left \| \bm{S}_p \right \| _{\mathrm{F}}^2\,,

\small
\Theta \bigg(\prod_{i=k+1}^{L-1}(\beta_3(\sigma_i)+\alpha_{i-1} ) \bigg) \leq \left \| \bm{D}_{k}\prod_{l=k+1}^{L-1}\bigg(\bm{W}_l \bm{D}_{l}+\alpha_{l-1}\bm{I}_{m\times m}\bigg) \bm{W}_L \right \|_2^2 \leq \Theta \bigg(\prod_{i=k+1}^{L-1}(\beta_2(\sigma_i)+\alpha_{i-1} )\bigg) \,,

\Theta \bigg(m\prod_{i=k+1}^{L-1}(\beta_3(\sigma_i)+\alpha_{i-1} ) \bigg) \leq \left \| \bm{B} \right \|_{\mathrm{F}}^2 \leq \Theta \bigg(m\prod_{i=k+1}^{L-1}(\beta_2(\sigma_i)+\alpha_{i-1} )\bigg) \,,
\label{eq:proof_lemma_C.6_in_ICML_1_}

\frac{1}{2m}\left \| \bm{B} \right \| _{\mathrm{F}}^2=\frac{1}{2}\mathbb{E}_{\bm{W}_L} \left \| \bm{B}\bm{W}_L \right \| _2^2 \leq \left \| \bm{B}\bm{W}_L \right \| _2^2 \leq \frac{3}{2}\mathbb{E}_{\bm{W}_L} \left \| \bm{B}\bm{W}_L \right \| _2^2 = \frac{3}{2m}\left \| \bm{B} \right \| _{\mathrm{F}}^2\,,
\label{eq:proof_lemma_C.6_in_ICML_2_}

\begin{split}
\lambda_{\min}(\bm{JJ}^{\top})&\geq \sum_{k=0}^{L-1}\lambda_{\min}(\bm{F}_k \bm{F}_k^{\top})\min_{i\in[N]}\left \| (\bm{B}_{k+1})_{i:} \right \|_2^2\\
&\geq \lambda_{\min}(\bm{F}_0 \bm{F}_0^{\top})\min_{i\in[N]}\left \| (\bm{B}_{1})_{i:} \right \|_2^2\\
&\geq \Theta \bigg(\prod_{i=2}^{L-1}(\beta_3(\sigma_i)+\alpha_{i-1} ) \bigg)\,,
\end{split}

\begin{split}
    \lambda_{min}(\bm{JJ}^{\top})\leq \sum_{i=0}^{N}(\bm{JJ}^{\top})_{ii}/N & = \frac{1}{N}\sum_{i=0}^{N}\sum_{k=0}^{L-1}\left \| (\bm{F}_k)_{1:} \right \|_2^2 \left \| (\bm{B}_{k+1})_{1:} \right \|_2^2\\
    & = \frac{1}{N}\sum_{i=0}^{N}\sum_{k=0}^{L-1}\left \| \bm{f}_k(\bm{x_1}) \right \|_2^2 \left \| (\bm{B}_{k+1})_{1:} \right \|_2^2\,.
    \end{split}

\lambda_{min}(\bm{JJ}^{\top}) \leq \sum_{k=0}^{L-1}\Theta \bigg(\prod_{i=k+2}^{L-1}(\beta_2(\sigma_i)+\alpha_{i-1} ) \bigg)\,,

s(\bm{A})_{\max} \leq \sqrt{N} + \sqrt{n} +t\,.

\begin{split}
\left \| \hat{\bm{f}}_{i,1} \right \| _2 & = \left \| \widetilde{\bm{f}}_{i,1} - \bm{f}_{i,1} \right \| _2 = \left \| \sigma_1(\widetilde{\bm{W}}_1\bm{x}_i) - \sigma_1(\bm{W}_1\bm{x}_i) \right \| _2\\
& \leq \mathrm{Lip}_{\sigma_{1}}\left \| \widetilde{\bm{W}}_1 - \bm{W}_1 \right \| _2 \left \|\bm{x}_i \right \| _2 \leq \omega \mathrm{Lip}_{\sigma_{1}}\\ 
& = \mathcal{O}(1)\,.
\end{split}

\small
\begin{split}
\left \| \hat{\bm{f}}_{i,l} \right \| _2 & = \left \| \widetilde{\bm{f}}_{i,l} - \bm{f}_{i,l} \right \| _2\\
& = \left \| \sigma_{l}(\widetilde{\bm{W}}_l\widetilde{\bm{f}}_{i,l-1}) + \alpha_{l-1}\widetilde{\bm{f}}_{i,l-1} - \sigma_{l}(\bm{W}_l\bm{f}_{i,l-1})-\alpha_{l-1}\bm{f}_{i,l-1} \right \| _2\\
& \leq \left \| \sigma_{l}(\widetilde{\bm{W}}_l\widetilde{\bm{f}}_{i,l-1}) - \sigma_{l}(\bm{W}_l\bm{f}_{i,l-1}) \right \| _2 + \alpha_{l-1}\left \| \hat{\bm{f}}_{i,l-1} \right \| _2\\ & \leq \mathrm{Lip}_{\sigma_{l}}\left \| \widetilde{\bm{W}}_l\widetilde{\bm{f}}_{i,l-1} - \bm{W}_l\bm{f}_{i,l-1} \right \| _2 + \left \| \hat{\bm{f}}_{i,l-1} \right \| _2\quad\quad\text{[Lipschitz continuity of ]}\\
& = \mathrm{Lip}_{\sigma_{l}}\left \| \bm{W}_l(\widetilde{\bm{f}}_{i,l-1} - \bm{f}_{i,l-1})+(\widetilde{\bm{W}}_l-\bm{W}_l)\widetilde{\bm{f}}_{i,l-1} \right \| _2 + \left \| \hat{\bm{f}}_{i,l-1} \right \| _2\\
& \leq \mathrm{Lip}_{\sigma_{l}}\left\{\left \| \bm{W}_l(\widetilde{\bm{f}}_{i,l-1} - \bm{f}_{i,l-1})\right \| _2+\left \| (\widetilde{\bm{W}}_l-\bm{W}_l)\widetilde{\bm{f}}_{i,l-1} \right \| _2\right\} + \left \| \hat{\bm{f}}_{i,l-1} \right \| _2\\ & \leq \mathrm{Lip}_{\sigma_{l}}\left\{\left \| \bm{W}_l\right \| _2 \left \|\widetilde{\bm{f}}_{i,l-1} - \bm{f}_{i,l-1}\right \|_2+\left \| \widetilde{\bm{W}}_l-\bm{W}_l\right \| _2 \left \|\widetilde{\bm{f}}_{i,l-1} \right \| _2\right\} + \left \| \hat{\bm{f}}_{i,l-1} \right \| _2\\& \leq (\mathrm{Lip}_{\sigma_{l}}\left \| \bm{W}_l\right \| _2+1) \left \|\hat{\bm{f}}_{i,l-1}\right \|_2+\mathrm{Lip}_{\sigma_{l}}\omega\bigg  (\left \|\widetilde{\bm{f}}_{i,l-1} - \bm{f}_{i,l-1}\right \| _2 + \left \| \bm{f}_{i,l-1}\right \| _2\bigg)\\& = \left\{\mathrm{Lip}_{\sigma_{l}}(\left \| \bm{W}_l\right \| _2+\omega)+1\right\} \left \|\hat{\bm{f}}_{i,l-1}\right \|_2+\mathrm{Lip}_{\sigma_{l}}\omega \left \| \bm{f}_{i,l-1}\right \| _2\,.
\end{split}
\label{eq:bound_for_perturbation_2}

\left \| \bm{W}_l\right \| _2 = s(\bm{W}_l)_{\max} \leq \frac{\sqrt{m} + \sqrt{m} +\sqrt{m}}{\sqrt{m}} = 3\,.

\begin{split}
\left \| \hat{\bm{f}}_{i,l} \right \| _2 & \leq \bigg((3 + \omega) \mathrm{Lip}_{\max}+1\bigg) \left \| \hat{\bm{f}}_{i,l-1}\right \|_2+ \mathrm{Lip}_{\max} \omega \left \| \bm{f}_{i,l-1}\right \| _2\\
& \leq \bigg([(3 + \omega) \mathrm{Lip}_{\max}+1]^{l-1}-1\bigg)\bigg(\mathrm{Lip}_{\sigma_{1}}\omega + \frac{\mathrm{Lip}_{\max}\omega \left \| \bm{f}_{i,l-1}\right \| _2}{(3+\omega)\mathrm{Lip}_{\max}}\bigg)+\mathrm{Lip}_{\sigma_1}\omega \\
& \leq (3\mathrm{Lip}_{\max}+1)^{L-1}\Theta (1)\omega+\mathrm{Lip}_{\sigma_1}\omega\\
& = \mathcal{O}(1)\Theta (1)+ \mathcal{O}(1)\\
& = \mathcal{O}(1)\,,
\end{split}

\left | f(\bm {x}_i;\bm{W}') - f(\bm {x}_i;\bm{W}) - \left \langle \nabla  f(\bm {x}_i;\bm{W}), \bm{W}' -\bm{W}  \right \rangle \right | = \mathcal{O}(1)\,.

\small
\begin{split}
    & \left | f(\bm {x}_i;\bm{W}') - f(\bm {x}_i;\bm{W}) - \left \langle \nabla  f(\bm {x}_i;\bm{W}), \bm{W}' -\bm{W}  \right \rangle \right | \\
    = & \left |\sum_{l=1}^{L-1}\bm{W}_{L}\prod_{r=l+1}^{L-1}(\bm{D}_{i,r}\bm{W}_r+\alpha_{r-1}\bm{I}_{m\times m})\bm{D}_{i,l}(\bm{W}'_l-\bm{W}_l)\bm{f}_{i,l-1}+\bm{W}'_L(\bm{f}'_{i,L-1}-\bm{f}_{i,L-1})  \right |\\
    \leq & \sum_{l=1}^{L-1}\left |\bm{W}_{L}\prod_{r=l+1}^{L-1}(\bm{D}_{i,r}\bm{W}_r +\alpha_{r-1}\bm{I}_{m\times m})\bm{D}_{i,l}(\bm{W}'_l-\bm{W}_l)\bm{f}_{i,l-1}\right |+\left | \bm{W}'_L(\bm{f}'_{i,L-1}-\bm{f}_{i,L-1})   \right |\\\leq & \sum_{l=1}^{L-1}\left \| \bm{W}_{L} \right \|_2\left \|  \prod_{r=l+1}^{L-1}(\bm{D}_{i,r}\bm{W}_r +\alpha_{r-1}\bm{I}_{m\times m})\bm{D}_{i,l}(\bm{W}'_l-\bm{W}_l)\bm{f}_{i,l-1} \right \|_2 \\
    & +\left \| \bm{W}'_L\right \|_2\left \| \bm{f}'_{i,L-1}-\bm{f}_{i,L-1} \right \|_2 \\\leq & \sum_{l=1}^{L-1}\left \| \bm{W}_{L} \right \|_2  \prod_{r=l+1}^{L-1}(\left \|  \bm{D}_{i,r}\right \|_2\left \|\bm{W}_r\right \|_2 +\alpha_{r-1})\left \|  \bm{D}_{i,l}\right \|_2\left \| \bm{W}'_l-\bm{W}_l \right \|_2 \left \|\bm{f}_{i,l-1} \right \|_2 +\left \| \bm{W}'_L\right \|_2\left \| \bm{f}'_{i,L-1}-\bm{f}_{i,L-1} \right \|_2\,.
\end{split}
\label{eq:proof_lemma_4.1_in_GuQuanquan_1}

\left \| \bm{W}_{L} \right \|_2\leq \sqrt{2}\,,

\begin{split}
    & \left | f(\bm {x}_i;\bm{W}') - f(\bm {x}_i;\bm{W}) - \left \langle \nabla  f(\bm {x}_i;\bm{W}), \bm{W}' -\bm{W}  \right \rangle \right | \\
    & \leq \sum_{l=1}^{L-1}(3\mathrm{Lip}_{\max} + 1)^{L-l-1}\omega\sqrt{2}\mathrm{Lip}_{\max}\Theta(1)  +(\sqrt{2}+\omega)\mathcal{O}(1)\\
    &  = \frac{(3\mathrm{Lip}_{\max}+1)^{L-1}-1}{3\mathrm{Lip}_{\max}}\omega\sqrt{2}\mathrm{Lip}_{\max}\Theta(1)  +(\sqrt{2}+\omega)\mathcal{O}(1)\\
    &  = \mathcal{O}(1)\,.
\end{split}

L_i(\bm{W}') \geq L_i(\bm{W}) + \left \langle \nabla_{\bm{W}} L_i(\bm{W}), \bm{W}'-\bm{W} \right \rangle -\mathcal{O}(1)\,,

\small
    L_i({\bm{W}}') - L_i(\bm{W}) =  \ell[y_i  f(\bm{x}_i;{\bm{W}}') ] - \ell[ y_i f(\bm{x}_i;\bm{W}) ] \geq  \ell'[ y_i f(\bm{x}_i;\bm{W})] \cdot y_i\cdot [ f(\bm{x}_i;{\bm{W}}') - f(\bm{x}_i;\bm{W}) ]\,.

\sum_{l=1}^L \left \langle  \nabla_{\bm{W}_l} L_i(\bm{W}), \bm{W}_l' - \bm{W}_l \right \rangle  = \ell'[ y_i f(\bm{x}_i;\bm{W})] \cdot y_i\cdot \left \langle  \nabla f(\bm{x}_i;\bm{W}) , {\bm{W}}' - \bm{W} \right \rangle\,.

\small
\begin{split}
        \ell'[ y_i f(\bm{x}_i;\bm{W})]\cdot y_i\cdot[ f(\bm{x}_i;{\bm{W}}') - f(\bm{x}_i;\bm{W}) ] & \geq \ell'[ y_i f(\bm{x}_i;\bm{W})] \cdot y_i\cdot  \left \langle  \nabla f(\bm{x}_i;\bm{W}) , {\bm{W}}' - \bm{W}  \right \rangle   - \varepsilon \\
    & = \textstyle \sum_{l=1}^L  \left \langle  \nabla_{\bm{W}_l} L_i(\bm{W}), \bm{W}_l' - \bm{W}_l  \right \rangle  - \varepsilon\,,
\end{split}

\begin{split}
    L_i({\bm{W}}') - L_i(\bm{W}) & \geq \sum_{l=1}^L \left \langle \nabla_{\bm{W}_l} L_i(\bm{W}), \bm{W}_l' - \bm{W}_l \right \rangle - \varepsilon\\
     & = \sum_{l=1}^L \left \langle \nabla_{\bm{W}_l} L_i(\bm{W}), \bm{W}_l' - \bm{W}_l \right \rangle - \mathcal{O}(1)\,.
\end{split}

\left \| \nabla_{\bm{W}_l} f(\bm {x}_i;\bm{W}) \right \| _2, \left \| \nabla_{\bm{W}_l} L_i(\bm{W}) \right \| _2 \leq  \Theta(3\mathrm{Lip}_{\max}+1)^{L-l}\,.

\begin{split}
    \left \| \nabla_{\bm{W}_l} f(\bm {x} _i;\bm W  ) \right \| _2 & = \left \| \bm{f}_{i,l-1}\bm{W}_L\prod_{r=l+1}^{L-1}(\bm{D}_{i,r}\bm{W}_r+\alpha_{r-1}\bm{I}_{m\times m})\bm{D}_{i,l} \right \| _2\\
    & \leq \left \| \bm{f}_{i,l-1}\right \| _2 \left \|\bm{W}_L\prod_{r=l+1}^{L-1}(\bm{D}_{i,r}\bm{W}_r+\alpha_{r-1}\bm{I}_{m\times m})\bm{D}_{i,l} \right \| _2\\
    & \leq \left \| \bm{f}_{i,l-1}\right \| _2 \left \| \bm{W}_{L} \right \|_2  \prod_{r=l+1}^{L-1}(\left \|  \bm{D}_{i,r}\right \|_2\left \|\bm{W}_r\right \|_2 +\alpha_{r-1})\left \|  \bm{D}_{i,l}\right \|_2\,.
\end{split}

    \left \| \nabla_{\bm{W}_l} f(\bm {x} _i;\bm W ) \right \| _2 \leq \Theta(1)(3\mathrm{Lip}_{\max}+1)^{L-l-1}\sqrt{2}\mathrm{Lip}_{\max} = \Theta(3\mathrm{Lip}_{\max}+1)^{L-l-1}\,,

\small
\left \| \nabla_{\bm{W}_l} L_i(\bm{W}) \right \| _2 \leq \left | {\ell}'[y_i\cdot f(\bm {x}_i;\bm{W})]\cdot y_i \right | \cdot \left \| \nabla_{\bm{W}_l} f(\bm {x}_i;\bm{W}) \right \| _2 \leq \left \| \nabla_{\bm{W}_l} f(\bm {x} _i;\bm W) \right \| _2 \leq \Theta(3\mathrm{Lip}_{\max}+1)^{L-l-1}\,,

    m^{\star}= \frac{(3\mathrm{Lip}_{\max}+1)^{4L-4}L^2R^4}{4\varepsilon^2}\,,
    
        \sum_{i=1}^N L_i(\bm{W}^{(i)}) \leq \sum_{i=1}^N L_i(\bm{W}^{*}) + 3N\epsilon\,.
    
    \big\| \bm{W}_l^{(i+1)} - \bm{W}_l^{(1)} \big\|_2 \leq \sum_{j  = 1}^i\big\| \bm{W}_l^{(j+1)} - \bm{W}_l^{(j)} \big\|_2 \leq \Theta((3\mathrm{Lip}_{\max}+1)^{L-l-1}\gamma N)\,.

\big\| \bm{W}_l^{(i+1)} - \bm{W}_l^{(1)} \big\|_{\mathrm{F}} \leq\Theta\bigg(\sqrt{m}(3\mathrm{Lip}_{\max}+1)^{L-l-1}\frac{LR^2}{2m\varepsilon}\bigg) \leq \omega\,,

    L_i(\bm{W}^{(i)}) - L_i(\bm{W}^{*}) &\leq  \left \langle  \nabla_{\bm{W}} L_i(\bm{W}^{(i)}), \bm{W}^{(i)} - \bm{W}^*  \right \rangle  + \epsilon \\
    &= \sum_{l=1}^L \frac{\left \langle  \bm{W}_l^{(i)} - \bm{W}_l^{(i+1)}, \bm{W}_l^{(i)} - \bm{W}_l^*  \right \rangle  }{ \gamma } + \epsilon\,.
L_i(\bm{W}^{(i)}) - L_i(\bm{W}^{*}) \leq \sum_{l=1}^L \frac{ \| \bm{W}_l^{(i)} - \bm{W}_l^{(i+1)} \|_{\mathrm{F}}^2 + \| \bm{W}_l^{(i)} - \bm{W}_l^* \|_{\mathrm{F}}^2 - \| \bm{W}_l^{(i+1)} - \bm{W}_l^* \|_{\mathrm{F}}^2 } {2\gamma} + \epsilon\,.

\small
    L_i(\bm{W}^{(i)}) - L_i(\bm{W}^{*}) \leq \sum_{l=1}^L \frac{ \| \bm{W}_l^{(i)} - \bm{W}_l^* \|_{\mathrm{F}}^2 - \| \bm{W}_l^{(i+1)} - \bm{W}_l^* \|_{\mathrm{F}}^2 } {2\gamma} + \Theta ((3\mathrm{Lip}_{\max}+1)^{2L-2}\gamma m) + \epsilon\,.

    \frac{1}{N}\sum_{i=1}^N L_i(\bm{W}^{(i)}) &\leq \frac{1}{N}\sum_{i=1}^N L_i(\bm{W}^{*}) + \sum_{l=1}^L \frac{ \| \bm{W}_l^{(1)} - \bm{W}_l^* \|_{\mathrm{F}}^2 } {2N\gamma} + \Theta ((3\mathrm{Lip}_{\max}+1)^{2L-2}\gamma m) + \epsilon\\
    &\leq \frac{1}{N}\sum_{i=1}^N L_i(\bm{W}^{*}) + \frac{ L R^{2} } {2\gamma mN} + \Theta ((3\mathrm{Lip}_{\max}+1)^{2L-2}\gamma m) + \epsilon\,,
\frac{1}{N}\sum_{i=1}^N L_i(\bm{W}^{(i)}) &\leq \frac{1}{N}\sum_{i=1}^N L_i(\bm{W}^{*}) + 3\epsilon\,,

\mathbb{E}[\ell_{\mathcal{D} }^{0-\!1}\!(\hat{\bm{W}}\!)] \!\leq\! \tilde{\mathcal{O} }\! \left(\! C_2\sqrt{\frac{\bm{y}^{\top}  ({\bm{K}^{(L)}})^{-1} \bm{y}}{N}} \!\right) + \mathcal{O}\!\left( \! \sqrt{\frac{\log(1/\delta )}{N} } \! \right)\!.





\end{proof}


\section{Discussion on the key points and the motivation of the NTK analysis}
\label{sec:discussion}


In this section, we discuss the motivation and few key points in the proof of this paper and we also explain how the proof differs from previous results.

\textbf{The motivation for studying the minimum eigenvalue of NTK}:

To make this clearer, we provide an illustrative example on the significance of the minimum eigenvalue. Let us consider the square loss . A simple calculation shows that . Thus if the minimum eigenvalue of NTK is strictly greater than 0, then minimizing the gradient on the LHS will drive the loss to zero. The larger the minimum eigenvalue, the smaller the loss.

Therefore, in this work, we are using the minimum eigenvalue to derive the generalization bound of NAS. 


    
\textbf{Key points in the proof}:
\begin{itemize}
    \item \emph{Minimum eigenvalue}: Our proof framework is motivated by \citet{pmlr-v139-nguyen21g} on minimal eigenvalue of NTK of ReLU neural networks. However, our proofs differ from them in two aspects. Firstly, as we discussed in~\cref{sec:introduction}, extension to mixed activation functions is non-trivial due to the special properties of . More importantly, we remark that the lower bound of the minimal eigenvalue of NTK in \citep[Theorem 3.2]{pmlr-v139-nguyen21g} holds with probability at least , where  is some constant.
    It can be found that, this concentration probability decreases as the number of training data increases. Thus, it could be negative for a large .
    This is due to the use of Gershgorin circle theorem leading to a loose probability estimation. Instead, in this paper, we do not use this theorem, and we develop a tighter estimation based on~\citet{10.1214/ECP.v19-3807} under the assumption of isotropic data distribution. Accordingly, we achieve the reasonable  probability, \emph{c.f.} \cref{thm:lambda_min_inf_mixed}.
    \item \emph{Generalization}: Our proof framework is based on ~\citet{cao2019generalization} for generalization guarantees of deep ReLU neural networks requiring . Their results cannot be directly extended to other activation functions as the nice homogeneity and the derivative property of ReLU are used in their proof.
    To make our result feasible to various activation functions, we employ Lipschitz continuous properties of all activation functions, and achieve the generalization guarantees with , \emph{c.f.} \cref{thm:NTK_Generalization} and~\cref{lemma:lemma_4.3_in_GuQuanquan}.
    Admittedly, our result is in an exponential increasing order of the depth. However, in practice, the depth of neural networks in NAS is usually smaller than , or even ~\citep{liu2018hierarchical, dong2021nats}, which leads to  in this case when compared to their result.
    This result makes our theory reasonable and fair for NAS.
\end{itemize}

\section{Auxiliary numerical validations}
\label{sec:additional_experiments}

\subsection{Dataset details and algorithm}
\label{ssec:dataset}
We describe here the datasets that we have used for the numerical validation of our theory. Those are the following five datasets:

\begin{enumerate}
    \item \emph{Fashion-MNIST}~\citep{xiao2017fashion} includes grayscale images of clothing. The training set consists of  examples and the test set of  examples. The resolution of each image is , with each image belonging to one of the  classes. 

    \item \emph{MNIST}~\citep{726791} includes handwritten digits images. MNIST has a training set of  examples and a test set of  examples. The resolution of each image is .

    \item \emph{CIFAR-10 and CIFAR-100~\citep{krizhevsky2014cifar}} depicts images of natural scenes. CIFAR-100 has a training set of  examples and a test set of  examples. The resolution of each RGB image is .
    
    \item \emph{ImageNet-16}~\citep{chrabaszcz2017downsampled} is the down-sampled version of ImageNet~\citep{deng2009imagenet} with image size  on  classes. 
\end{enumerate}



Our Eigen-NAS algorithm used in~\cref{ssec:NAS_201_experiment} is summarized as below.

\begin{algorithm}[H]
\caption{Eigen-NAS Algorithm}
\label{alg:algorithm_NAS}
\begin{algorithmic}
\REQUIRE Search space , training data , validation data .\\
Initialize max\_iteration \\
Initialize candidate set \\
\FOR{search\_iteration in max\_iteration}
\STATE{Randomly sample architecture  from search space .\\Compute .\\\\update  to kept top-K best architectures}
\ENDFOR \\
 \# Choose the best architecture
based on validation error after training 20 epochs.\\
\textbf{Output} 
\end{algorithmic}
\end{algorithm}

\subsection{Compared algorithms}

We provide a thorough comparison with the following baselines: 

\begin{enumerate}[leftmargin=3.3mm]
    \itemsep-0.2em 
    \item \textit{Classical network:} ResNet ~\citep{7780459}, which is the default baseline used widely in image-related tasks. 
    \item \textit{Reinforcement learning based algorithm:} NAS-RL~\citep{45826} with the validation accuracy as a reward, which is an classical and representative NAS Algorithm.
    \item \textit{Differentiable algorithm:} DARTS~\citep{liu2019darts}\footnote{We directly use the results from ~\citet{pmlr-v139-xu21m}.}, which is the earliest and basic gradient-based NAS algorithm.
    \item \textit{Train-free algorithms using metrics to guide NAS:} A new type of NAS algorithm, they use some special metrics to pick models directly from candidate models. Common Train-free algorithms are: NASWOT~\citep{mellor2021neural} using the output of ReLU; TE-NAS~\citep{chen2021neural} leveraging the spectrum of NTK and linear partition of the input space; KNAS~\citep{pmlr-v139-xu21m} employing the Frobenius norm of NTK. Our Eigen-NAS algorithm also belongs to this type.
\end{enumerate}




\subsection{Training/test accuracy of DNNs by NAS}

Here we evaluate the classification results with 5 runs of the obtained architecture by DARTS under varying widths  and depths  on Fashion-MNIST.
\cref{fig:DARTS_experiment} shows that nearly  accuracy is achieved on the test set under different depth and width settings.
The result is competitive on FC/residual networks within  layers and without training tricks, e.g., data augmentation, batch norm and drop out. 
We find that when compared to the depth, the network width also contributes on test accuracy. As suggested by~\cref{eq:problem_setting_parameter_space}, the amount of parameters in the neural network is approximately proportional to the depth, but squared to the width. 

\begin{figure}[t]
\centering
    \includegraphics[width=0.9\linewidth]{figures/DARTS_experiment.pdf}\vspace{-5mm}
\caption{The accuracy of neural networks by NAS under different widths and depths.}
\label{fig:DARTS_experiment}
\end{figure}

\subsection{Simulation of minimum eigenvalues of NTK}
\label{ssec:Simulation_of_NTK}

We calculate the minimum eigenvalue of each NTK matrix under different architectures with activation functions, skip connections and depths, according to~\cref{lemma:NTK_matrix_recursive_form}. 
We consider four special cases on skip connections: a) no skip connections with , i.e., fully connected neural network in~\cref{fig:NTK_simulationa}; b) skip connections between all consecutive layers with ,  in~\cref{fig:NTK_simulationb}; c) the alpha of the first half (of the network) is  and the alpha of the second half is  shown in~\cref{fig:NTK_simulationc}; d) The alpha of the first half is  and the alpha of the second half is . The results are shown in~\cref{fig:NTK_simulationd}.

\cref{fig:NTK_simulationa} indicates that as the network depth increases, the minimum eigenvalue of NTK will become larger when LeakyReLU, ReLU, Swish and Tanh employed, but Sigmoid leads to a decreasing minimum eigenvalue, which is consistent with the upper bound shown in~\cref{thm:lambda_min_inf_mixed}. 
The LeakyReLU, ReLU and Swish generate the fastest increasing rate of depth, while Tanh and Sigmoid are slow, which coincides with the derived lower bound in~\cref{thm:lambda_min_inf_mixed} and previous work \cite{bachspaper}. \cref{fig:NTK_simulationb} shows that, under the skip connection, the tendency of the minimum eigenvalue of NTK is similar to that of FC neural networks when various activation functions are employed. However, the specific values and the growth rate are significantly larger than FC neural networks.
This result is consistent with the conclusion we state in~\cref{thm:lambda_min_inf_mixed} about skip layers leading to the increase of minimum eigenvalue of NTK with respect to the depth. Moreover,~\cref{fig:NTK_simulation_2} show similar growth speed.

Then, we plot the comparison figure of NTK under above two settings and two settings in main paper for the same activation function in~\cref{fig:DARTS_experiment_}. In addition to reconfirming the order between different activation functions, we can also see that the effect of adding an activation layer in the second half of the -layer neural network is better than the first half of the neural network. This verifies the experimental results in~\cref{fig:DARTS_heatmap2}.


\begin{figure}[t]
\centering
    \subfigure[no skip connections]{\includegraphics[width=0.495\linewidth]{figures/NTK_simulation_1.pdf}\label{fig:NTK_simulationa}\hspace{-2mm}}
    \subfigure[skip connections]{\includegraphics[width=0.495\linewidth]{figures/NTK_simulation_2.pdf}\label{fig:NTK_simulationb}}\vspace{-3mm}
\caption{Minimum eigenvalue of NTK \emph{vs.} depth () under various activation functions with/without skip connections in each layer. }
\label{fig:NTK_simulation}
\end{figure}


\begin{figure}[t]
\centering
    \subfigure[the first half has a skip layer]{\includegraphics[width=0.48\linewidth]{figures/NTK_simulation_1_5.pdf}\label{fig:NTK_simulationc}\hspace{-2mm}}
    \subfigure[the second half has a skip layer]{\includegraphics[width=0.48\linewidth]{figures/NTK_simulation_6_10.pdf}\label{fig:NTK_simulationd}}\vspace{-3mm}
\caption{Minimum eigenvalue of NTK \emph{vs.} depth () under various activation functions with/without skip connections in each layer. }
\label{fig:NTK_simulation_2}
\end{figure}


\begin{figure}[t]
\centering
    \subfigure[ReLU]{\includegraphics[width=0.495\linewidth]{figures/NTK_simulation_ReLU.pdf}\label{fig:DARTS_experiment_ReLU}\hspace{-2mm}}
    \subfigure[LeakyReLU]{\includegraphics[width=0.495\linewidth]{figures/NTK_simulation_leakyReLU.pdf}\label{fig:DARTS_experiment_LeakyReLU}}\vspace{-2mm}\\
    \subfigure[Sigmoid]{\includegraphics[width=0.33\linewidth]{figures/NTK_simulation_Sigmoid.pdf}\label{fig:DARTS_experiment_Sigmoid}\hspace{-2mm}}
    \subfigure[Tanh]{\includegraphics[width=0.33\linewidth]{figures/NTK_simulation_Tanh.pdf}\label{fig:DARTS_experiment_Tanh}}\vspace{-2mm}
    \subfigure[Swish]{\includegraphics[width=0.33\linewidth]{figures/NTK_simulation_Swish.pdf}\label{fig:DARTS_experiment_Swish}}\vspace{-2mm}\\
\caption{Minimum eigenvalue of NTK for different activation function. The red line have skip connections in each layer, green line does not contain any skip connections, the blue line represents the skip connections in the first half and the cyan line represents the skip connections in the second half.}
\label{fig:DARTS_experiment_}
\end{figure}




\subsection{Additional experiments on NAS-Bench-101 and ranking correlations}
\label{ssec:bench101_experiment}

In this section, we conduct more experiments on two new benchmarks NAS-Bench-101~\citep{ying2019bench} and Network Design Spaces (NDS)~\citep{radosavovic2019network} using the same setting as~\cref{ssec:NAS_201_experiment}.

\cref{tab:NAS_benchmark_experiment_101} provides a comparison of the accuracy of Eigen-NAS, KNAS and NASWOT on four new search spaces. For all of four search spaces, our method achieves the best results with  accuracy improvement.

\begin{table*}[tb]
\centering
\caption{New results on NAS-Benchmark-101, NDS-DARTS and NDS-PNAS using CIFAR-10 and ImageNette2, a subset of ImageNet.}
\begin{tabular}{l@{\hspace{0.25cm}} c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}} c} 
    \hline
    Benchmark  & NAS-Bench-101 & NDS-DARTS & NDS-PNAS & NDS-PNAS\\
    \hline
    Dataset  & CIFAR-10 & CIFAR-10 & CIFAR-10 & ImageNette2\\
    \hline
    \textbf{Eigen-NAS} ()  &  &  &  &  \\
    KNAS ()&  &  &  &  \\
    NASWOT &  &  &  &  \\
    \hline
\end{tabular}
\label{tab:NAS_benchmark_experiment_101}
\end{table*}

Moreover, we conduct more detailed experiments using the CIFAR-10 dataset on NAS-Bench-101.~\cref{tab:NAS_benchmark_experiment_time} provides the running time and Kendall rank correlation coefficient between minimum eigenvalues and accuracy for the above three train-free NAS algorithm. We can see that our Eigen-NAS method can get the best rank correlation coefficient with the fastest speed among three methods. The scatter plot of the relationship between the minimum eigenvalue and the accuracy is shown in~\cref{fig:ranking_correlation}.

\begin{table*}[tb]
\centering
\caption{Running time (in Second) and the Kendall rank correlation coefficient on NAS-Bench-101, CIFAR-10 (the larger the absolute value of Rank correlation, the stronger the correlation between the guide used by the algorithm and the network accuracy).}
\begin{tabular}{l@{\hspace{0.25cm}} c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}} c} 
    \hline
    Method  & Eigen-NAS () & KNAS () & NASWOT\\
    \hline
    Running time &  &  &  \\
    Rank correlation &  &  &   \\
    \hline
\end{tabular}
\label{tab:NAS_benchmark_experiment_time}
\end{table*}


\begin{figure}[t]
\centering
    \includegraphics[width=0.6\linewidth]{figures/CR.pdf}\vspace{-5mm}
\caption{The standard scatter plot on the kendall rank correlation coefficient.}
\label{fig:ranking_correlation}
\end{figure}

\subsection{Transfer learning experiment}
\label{ssec:transfer_learning_experiment}
Here we evaluate the proposed NAS framework on transfer learning. The algorithm from~\cref{ssec:DARTS_experiment} is employed for this experiment, e.g., the same search space and search strategy. 
The experiment setting is the following: we train the model on FashionMNIST for  epochs, then we use the pretrained weights and fine-tune them for  epochs on MNIST, with repeated three times.

\cref{tab:transfer_learning_experiment} show that, after the fine-tuning for just  epochs, the method obtains up to  accuracy and after fine-tuning for  epochs it obtains up to  accuracy. This verifies our intuition that the proposed NAS framework can obtain architectures that generalize well beyond the dataset they were optimized on. 



\begin{table*}[tb]
\centering
\caption{Transfer learning result of our network for different width () which training in FashionMNIST (domain dataset) for  epochs and then training in MNIST (target dataset) for  or  epochs. (the accuracy in the table are displayed in percentages)}
\begin{tabular}{c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}} c@{\hspace{0.2cm}} c@{\hspace{0.2cm}} c} 
    \hline
    Epochs &  &  &  &  & \\
    \hline
     &   & &  & & \\
     &   & &  & & \\
    \hline
\end{tabular}
\label{tab:transfer_learning_experiment}
\end{table*}




\subsection{DARTS experiment on CNN}
\label{ssec:DARTS_experiment_CNN}

Our theory relies on fully-connected matrices and we have indeed verified experimentally the validity of our theoretical findings. To scrutinize our method even further, we attempt to extend our results to the popular convolutional neural networks. We believe this will provide some further insights on future extensions of our theory. 
In particular, we use DARTS (similarly with the experiment in \cref{ssec:DARTS_experiment}) with convolutional layers. The standard dataset of CIFAR-10 is selected; the details of the dataset are shared in \cref{ssec:dataset}. The search space and search strategy follow~\cref{ssec:DARTS_experiment} with one differentiating point: we use convolutional layers instead of fully connected layers in~\cref{eq:problem_setting_parameter_space}.


We select DARTS on a Convolutional Neural Network with  and , while we repeat the experiment for 5 times. After training, the probability of these activation functions and skip connections in each layer are reported in Figure~\ref{fig:DARTS_heatmap_cnn_1} and~\ref{fig:DARTS_heatmap_cnn_2}, respectively. Compared with the~\cref{fig:DARTS_heatmap}, the activation function search exhibits similar characteristics with the results of the fully connected network. Namely: (1) ReLU and LeakyReLU have the highest probability to be selected, (2) the difference of probability between different activation functions in the first layer is the largest. But for skip layer search, CNN exhibits the opposite results with fully connected network, that is, almost all of the skip connections have a probability of being selected less than . 

Based on the above results, our theory can still explain some of the phenomena observed in CNNs, e.g., activation functions search.
Nevertheless, our theory on skip connections search on CNNs mismatches with experimental demonstration in practice to some extent, which motivates us to conduct a refined analysis on CNNs for NAS.   

\begin{figure}[t]
\centering
    \subfigure[activation functions ]{\includegraphics[width=0.45\linewidth]{figures/heatmap_activation_cnn.pdf}\label{fig:DARTS_heatmap_cnn_1}\hspace{-1mm}}
    \subfigure[skip connections ]{\includegraphics[width=0.45\linewidth]{figures/heatmap_skip_cnn.pdf}\vspace{-9mm}
    \label{fig:DARTS_heatmap_cnn_2}}
\caption{Architecture search results on activation functions indicated by the probability of  in (a) and skip connections indicated by  in (b). We notice that for each layer, ReLU and LeakyReLU are selected with the higher probability.}
\label{fig:DARTS_heatmap_cnn}
\end{figure}


\subsection{-DARTS experiment on MLP}
\label{ssec:B_DARTS_experiment}

In this section, we use an improved DARTS-based algorithm, -DARTS~\citep{https://doi.org/10.48550/arxiv.2203.01665}, for doing the activation function search. Our experiments are performed on a 5-layers MLP and the experimental results are presented in~\cref{fig:B_DARTS_experiment}. Compared with the results of DARTS in~\cref{fig:DARTS_heatmap}, the experimental results of -DARTS indicate that the probability difference between different activation functions is smaller, which may verify that DART is more easily to overfit . This is also the advantage mentioned in the -DARTS paper.


\begin{figure}[t]
\centering
    \includegraphics[width=0.6\linewidth]{figures/heatmap_activation_bdarts.pdf}\vspace{-5mm}
\caption{Architecture search results using -DARTS on activation functions indicated by the probability of . We notice that for each layer, ReLU and LeakyReLU are selected with the higher probability.}
\label{fig:B_DARTS_experiment}
\end{figure}


\section{Societal impact}
\label{sec:nas_societal_impact}

This is a theoretical work that derived generalization bounds for the architectures obtained by NAS. As such, we do not expect our work to have negative societal bias, as we do not focus on obtaining state-of-the-art results in a particular task. On the contrary, our work can have various benefits for the community: 
\begin{itemize}
    \item We provide the first generalization bounds for the class of NAS architectures, which is expected to have a positive impact on the understanding and the application of such architectures. 
    \item As we illustrate in~\cref{sec:experiment}, we can use the minimum eigenvalue as a  promising metric to guide NAS. This can lead to further investigation on techniques for efficient evaluation of NAS by avoiding solving the intensive bi-level optimization of NAS explicitly. 
\end{itemize}

Nevertheless, we encourage researchers to further investigate the impact of different architectures and their inductive biases on the society. 
\end{document}