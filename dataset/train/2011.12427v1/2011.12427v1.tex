


\documentclass[journal]{IEEEtran}


\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[hyphens]{url}
\usepackage[acronym]{glossaries}
\usepackage[colorlinks=true,allcolors=black]{hyperref} 
\usepackage[sort,compress]{cite}
\usepackage{xspace}
\usepackage{soul} 

\usepackage[caption=false,farskip=0pt]{subfig} 
\usepackage{balance}


\newacronymstyle{long-short-br}
{\GlsUseAcrEntryDispStyle{long-short}}{\GlsUseAcrStyleDefs{long-short}\renewcommand*{\genacrfullformat}[2]{\glsentrylong{##1}##2~\textup{(\firstacronymfont{\glsentryshort{##1}})}}\renewcommand*{\Genacrfullformat}[2]{\Glsentrylong{##1}##2~\textup{(\firstacronymfont{\glsentryshort{##1}})}}\renewcommand*{\genplacrfullformat}[2]{\glsentrylongpl{##1}##2~\textup{(\firstacronymfont{\glsentryshortpl{##1}})}}\renewcommand*{\Genplacrfullformat}[2]{\Glsentrylongpl{##1}##2~\textup{(\firstacronymfont{\Glsentryshortpl{##1}})}}}
\setacronymstyle{long-short-br}


\hyphenation{Figure Table UFPR VISOB CNN}

\sloppy
\begin{document}

\title{UFPR-Periocular: A Periocular Dataset Collected by Mobile Devices in Unconstrained Scenarios}


\author{Luiz~A.~Zanlorensi,
        Rayson~Laroca,
        Diego~R.~Lucio,
        Lucas~R.~Santos,
        Alceu~S.~Britto~Jr.,
        and~David~Menotti
\thanks{Luiz~A. Zanlorensi, Rayson~Laroca, Diego~R.~Lucio, Lucas~R.~Santos, and David~Menotti are with Federal University of Paraná (UFPR), Brazil.
E-mails: \{lazjunior, rblsantos, drlucio, lrs14, menotti\}@inf.ufpr.br}
\thanks{Alceu~S.~Britto~Jr. is with Pontifical Catholic University of Paraná (PUCPR), Brazil.
E-mail: alceu@ppgia.pucpr.br}}

\maketitle

\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{nir}{NIR}{near-infrared}
\newacronym{vis}{VIS}{visible}
\newacronym{safe}{SAFE}{Symmetry Patterns}
\newacronym{gabor}{GABOR}{Gabor Spectral Decomposition}
\newacronym{sift}{SIFT}{Scale-Invariant Feature Transform}
\newacronym{lbp}{LBP}{Local Binary Patterns}
\newacronym{hog}{HOG}{Histogram of Oriented Gradients}
\newcommand{\upol}{UPOL\xspace}
\newcommand{\ubirisvOne}{UBIRIS.v1\xspace}
\newcommand{\utiris}{UTIRIS\xspace}
\newcommand{\ubirisvTwo}{UBIRIS.v2\xspace}
\newcommand{\ubipr}{UBIPr\xspace}
\newcommand{\bdcp}{BDCP\xspace}
\newcommand{\iiitdMSP}{IIITD Multi-spectral Periocular\xspace}
\newcommand{\polyu}{PolyU Cross-Spectral\xspace}
\newcommand{\miche}{MICHE-I\xspace}
\newcommand{\vssiris}{VSSIRIS\xspace}
\newcommand{\csip}{CSIP\xspace}
\newcommand{\visob}{VISOB\xspace}
\newcommand{\crossEyed}{CROSS-EYED\xspace}
\newcommand{\qutMP}{QUT Multispectral Periocular\xspace} \newcommand{\urldataset}{https://web.inf.ufpr.br/vri/databases/ufpr-periocular/} 

\begin{abstract}
Recently, ocular biometrics in unconstrained environments using images obtained at visible wavelength have gained the researchers' attention, especially with images captured by mobile devices.
Periocular recognition has been demonstrated to be an alternative when the iris trait is not available due to occlusions or low image resolution.
However, the periocular trait does not have the high uniqueness presented in the iris trait.
Thus, the use of datasets containing many subjects is essential to assess biometric systems' capacity to extract discriminating information from the periocular region.
Also, to address the within-class variability caused by lighting and attributes in the periocular region, it is of paramount importance to use datasets with images of the same subject captured in distinct sessions.
As the datasets available in the literature do not present all these factors, in this work, we present a new periocular dataset containing samples from 1,122 subjects, acquired in 3 sessions by 196 different mobile devices.
The images were captured under unconstrained environments with just a single instruction to the participants: to place their eyes on a region of interest.
We also performed an extensive benchmark with several~\gls{cnn} architectures and models that have been employed in state-of-the-art approaches based on Multi-class Classification, Multi-task Learning, Pairwise Filters Network, and Siamese Network.
The results achieved in the closed- and open-world protocol, considering the identification and verification tasks, show that this area still needs research and development.
\end{abstract}

\begin{IEEEkeywords}
Mobile ocular biometric, Periocular dataset, Periocular recognition, Deep representations.
\end{IEEEkeywords}



\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{B}{iometric} systems that use ocular images have been extensively investigated due to the high level of singularity in the iris and because the periocular region can provide discriminative patterns even in noisy images~\cite{DeMarsico2017, Proenca2017irina, proenca2019inset, zanlorensi2020attnormalization, zanlorensi2020deep}.
There are two main modes that an ocular biometric system can operate: identification (: comparison) and verification (:~comparison). 
The identification task consists of determining a subject's identity, whereas the verification one verifies whether a subject is who she/he claims to be.
There are also two main protocols to evaluate biometric systems: closed-world and open-world.
In the former, the training and test sets have different samples from exactly the same subjects.
On the other hand, in the open-world protocol, the training and test sets must have samples from different subjects.
With these modes and protocols, it is possible to evaluate some characteristic of biometric approaches to produce discriminative features and generalization capability.

\begin{table}[!ht]
\scriptsize
\setlength{\tabcolsep}{8pt}
\centering
\caption{Comparison of the available ocular datasets containing \gls{vis} images with our dataset (UFPR-Periocular).}

\vspace{-1.5mm}

\label{tab:datasets}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrccc}
\toprule
Dataset                                     & Subjects & Images    & Sessions & Sensors \\
\midrule
\vssiris \cite{Raja2015}                     &    &        &       &   \\
\csip \cite{Santos2015}                      &    &    & N/A      &   \\
QUT~\cite{Algashaam2017}                     &    &        & N/A      &   \\
IIITD \cite{Sharma2014}                      &    &    & N/A      &   \\
\upol \cite{Dobes2004}                       &    &        & N/A      &   \\
\utiris \cite{Hosseini2010}                  &    &    &       &   \\
\miche \cite{DeMarsico2015}                  &    &    &       &   \\
\crossEyed \cite{Sequeira2016, Sequeira2017} &   &    & N/A      &   \\
\polyu \cite{Nalla2017}                      &   &   &       &   \\
\ubirisvOne \cite{Proenca2005}               &   &    &       &   \\
\ubirisvTwo \cite{Proenca2010}               &   &   &       &   \\
\ubipr \cite{Padole2012}                     &   &   &       &   \\
\visob \cite{Rattani2016}                    &   & \boldmath{} &       &   \\
\midrule
\textbf{UFPR-Periocular}                     & \boldmath{} &  & \boldmath{} & \boldmath{} \\


\bottomrule

\end{tabular}
}
\end{table}

Nowadays, with the advancement of deep learning-based techniques, several methodologies applying them to ocular images have been proposed for several tasks, for example, spoofing detection~\cite{Menotti2015, He2016}, iris and periocular region detection~\cite{Silva2015, lucio2019simultaneous, severo2018benchmark}, iris and sclera segmentation~\cite{lucio2018fully, bezerra2018robust}, and iris and periocular recognition~\cite{Du2016, Luz2018, Zhao2019capsule, diaz2020spectrum, zanlorensi2018impact, silva2018multimodal, hern2020crossspectral}.
The advancement of these technologies can be observed by the recent contests that have been conducted to evaluate the evolution of the state-of-the-art methods for different applications, such as iris recognition in heterogeneous lighting conditions (NICE.I and NICE.II)~\cite{Proenca2010, Proenca2012}, iris recognition using mobile images (MICHE.I and MICHE.II)~\cite{DeMarsico2015, DeMarsico2017}, iris and periocular recognition in cross-spectral scenarios (Cross-Eyed 1 and 2)~\cite{Sequeira2016,Sequeira2017}, and periocular recognition using mobile images captured in different lighting conditions (VISOB 1 and 2)~\cite{Rattani2016}.
Note that all these contests used datasets containing images obtained in the visible wavelength.
The most recent contests also used images captured by mobile devices~\cite{Rattani2016, DeMarsico2017}.
The results achieved by the proposed methods have shown that it is challenging to develop a robust biometric system in such conditions, mainly due to the high intra-class variability.
Based on recent works~\cite{DeMarsico2017, zanlorensi2019ocular, zanlorensi2020attnormalization}, we can state that developing an ocular biometric system that operates in unconstrained environments is still a challenging task, especially with images obtained by mobile devices.
In this condition, the images captured by the volunteer may present several variations caused by occlusion, pose, eye gaze, off-angle, distance, resolution, and image quality (affected by the mobile device).

With the existing ocular datasets, it is difficult to assess the scalability performance of biometric applications, i.e., if an approach can produce discriminative features even in a large dataset in terms of the number of subjects.
As we can see in Table~\ref{tab:datasets}, the datasets in the literature do not present a large number of subjects and have few sensors and session captures.
As described in some previous works~\cite{zanlorensi2020deep, zanlorensi2020attnormalization}, one common problem in ocular biometric systems is the within-class variability, which is generally affected by noises and attributes present in the same individual images.
A robust biometric system must handle images obtained from different sensors, extracting distinctive representations regardless of the source and environments.
In this sense, samples from the same subject obtained in different sessions are of paramount importance to capture the intra-class variation caused by various noise factors.

Considering the above discussion, in this work, we introduce a new periocular dataset, called \emph{UFPR-Periocular}.
The subjects themselves collected the images that compose our dataset through a mobile application~(app).
In this way, the images were captured in unconstrained environments, with a minimum of cooperation from the participant, and have real noises caused by poor lighting, occlusion, specular reflection, blur, and motion blur.
Fig.~\ref{fig:datasetsamples} shows some samples from the UFPR-Periocular. 
As part of this work, we also present an extensive benchmark, employing several state-of-the-art architectures of~\gls{cnn} models that have been explored to develop ocular biometric~systems.

\begin{figure}[!ht]
\centering
\begin{tabular}{cccccccc}
    \hspace{-2.7mm}
	{\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0006S1I04L.jpg}} 
	{\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0009S1I05R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0011S2I09R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0012S1I01R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0014S1I04L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0022S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0023S3I12L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0027S1I04R.jpg}} 
    \\
    \hspace{-2.7mm}
	{\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0033S1I02R.jpg}}
	{\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0037S1I01L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0038S1I03L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0040S2I07L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0042S1I01R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0045S2I06L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0047S1I02R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0048S3I13R.jpg}} 
    \\
    \hspace{-2.7mm}
	{\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0049S2I07L.jpg}}
	{\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0050S2I09R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0056S1I05R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0058S2I10L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0060S2I10R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0062S2I06R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0064S1I02R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0068S3I11R.jpg}} 
    \\
    \hspace{-2.7mm}
	{\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0077S3I12L.jpg}}
	{\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0083S2I07R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C0841S1I03R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1001S2I09L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1049S2I06R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1050S2I06L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1055S1I03L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1060S3I12L.jpg}} 
    \\
	\hspace{-2.7mm} {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1069S3I14L.jpg}}
	{\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1074S2I08R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1076S1I02L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1079S3I11L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1088S1I01R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1113S2I10L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1119S2I06R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/datasetsamples/C1120S1I02L.jpg}} 
\end{tabular}

\vspace{-0.25mm}

\caption{Sample images from the UFPR-Periocular dataset. Observe that there is great diversity in terms of lighting conditions, age, gender, eyeglasses, specular reflection, occlusion, resolution, eye gaze, and ethnic diversity.}
\label{fig:datasetsamples}
\end{figure}


Note that our dataset is the largest one in terms of the number of subjects, sessions, and sensors, as shown in Table~\ref{tab:datasets}.
It also has more images than all datasets except VISOB.
Another key feature is that the proposed dataset has images captured by  different mobile devices.
The samples captured with less cooperation of the participant in unconstrained environments have several variations on the ocular images since they are obtained during three different sessions.
To the best of our knowledge, this is the first ocular dataset with more than  subject samples and the largest one in different sensors in the literature.
Thus, we believe that it can provide a new benchmark to evaluate and develop new robust ocular biometric approaches. 

The remainder of this work is organized as follows.
In Section~\ref{sec:related}, we describe the ocular datasets containing~\gls{vis} images for ocular biometrics.
In Section~\ref{sec:database}, we present information about the UFPR-Periocular dataset and the proposed protocol to evaluate biometric systems.
Section~\ref{sec:benchmark} presents the~\gls{cnn} architectures used to perform the  benchmark.
In Section~\ref{sec:results}, we present and discuss the benchmark results. 
Finally, the conclusions are given in~Section~\ref{sec:conclusion}.



 
\section{Related Work}
\label{sec:related}

In recent years, several ocular contests and datasets have been released to evaluate state-of-the-art methods for many applications.
Zanlorensi et al.~\cite{zanlorensi2019ocular} detailed and described several datasets and contests for iris and periocular recognition.
Different problems have been addressed by the researchers, such as ocular recognition in unconstrained environments, ocular recognition on cross-spectral scenarios, iris/periocular region detection, iris/periocular region segmentation, and sclera~segmentation.

Existing ocular datasets can be organized into constrained (or controlled) or unconstrained (or non-controlled) environments.
The quality of the images is different in constrained and unconstrained environments, as some noise can occur in the images captured in unconstrained environments such as lighting variation, occlusion, blur, specular reflection, and distance.
Images can also be acquired cooperatively and non-cooperatively in relation to some image capture restrictions imposed on the subject.
Ocular non-cooperative images can have some problems caused by off-angle, focus, distance, motion blur, and occlusions by some attributes such as eye-glasses, contact lenses, and makeup. 

As described in~\cite{zanlorensi2019ocular}, datasets containing images obtained at the~\gls{nir} wavelength were created mainly to investigate the intricate patterns present in the iris region~\cite{Phillips2008, Phillips2010}.
There are also other studies on~\gls{nir} ocular images, such as generating synthetic iris images~\cite{Shah2006, Zuo2007}, spoofing and liveness detection~\cite{Ruiz-Albacete2008, Czajka2013, Gupta2014, Kohli2016}, contact lens detection~\cite{Baker2010, Kohli2013, Doyle2013, Doyle2015}, and template aging~\cite{Fenker2012, Baker2013}.
The use of~\gls{nir} ocular images captured in controlled environments by biometric systems has been studied for several years.
Thus, it can be considered a mature technology that has been successfully employed in several applications~\cite{bowyer2008survey, Phillips2008, Phillips2010, Proenca2017irina, Proenca2019segmentation}.

In general, better results can be achieved on biometric methods using \gls{vis} images by exploring the periocular region instead of the iris trait, as the iris is rich in melanin pigment that absorbs the most visible lights --~not reflecting the iris features as occur with \gls{nir} lights~\cite{bowyer2008survey}.
Also, the small resolution of ocular images is a common problem that makes it almost impracticable to use the iris trait alone.
Regarding these problems, the use of \gls{vis} ocular images captured in a non-cooperative way under unconstrained environments became a recent challenge. 
In this sense, several studies have been carried out on ocular biometric recognition using images obtained by mobile devices in uncontrolled environments using different sensors~\cite{DeMarsico2015, Raja2015, Rattani2016}.
The following datasets were developed to investigate the use of iris and periocular traits in \gls{vis} images: \upol~\cite{Dobes2004}, \ubirisvOne~\cite{Proenca2005}, \ubirisvTwo~\cite{Proenca2010} and \ubipr~\cite{Padole2012}.
There are also datasets of iris and periocular region images for cross-spectral recognition, i.e., match ocular images obtained at different wavelengths (\gls{nir} against \gls{vis} and vice-versa): \utiris~\cite{Hosseini2010}, \iiitdMSP~\cite{Sharma2014}, \polyu~\cite{Nalla2017}, \crossEyed~\cite{Sequeira2016, Sequeira2017}, and \qutMP~\cite{Algashaam2017}.
Focusing specifically on ocular recognition using non-cooperative images obtained in uncontrolled environments by mobile devices, we highlight the following datasets: \miche~\cite{DeMarsico2015}, \vssiris~\cite{Raja2015}, \csip~\cite{Santos2015} and~\visob~\cite{Rattani2016}.

Nowadays, it is difficult to evaluate the scalability factor of the state-of-the-art biometric approaches due to the size in terms of subjects and images on the available datasets.
As shown in Table~\ref{tab:datasets}, the most extensive dataset regarding subjects and images is  \visob~\cite{Rattani2016}, which has  images from  subjects.
The ICIP 2016 Competition on mobile ocular biometric recognition~\cite{Rattani2016} employed this dataset, and in the WCCI/IJCNN2020 challenge\footnote{VISOB 2.0 Dataset and Competition results available at: \url{https://sce.umkc.edu/research-sites/cibit/dataset.html}.}, a second version of the dataset was launched.
Both contests evaluated the periocular recognition using \gls{vis} images obtained by mobile devices.
The second contest's main difference is that the input images were a stack with  ocular images belonging to the same subject.
The best methods achieved an EER of \% and \% on the first and second contests, respectively.

Also using \gls{vis} ocular images, other contests were carried out to evaluate iris and periocular recognition: NICE.II~\cite{Proenca2012}, MICHE.II~\cite{DeMarsico2017}, and CROSS-EYED I~\cite{Sequeira2016} and II~\cite{Sequeira2017}.
The NICE.II contest evaluated iris recognition using images containing noise within the iris region.
The winner method fused features extracted from the iris and the periocular region using ordinal measures, color histograms, texton histograms, and semantic information.
The MICHE.II contest also evaluated iris and periocular recognition, but using images captured by mobile devices.
The winner approach extracted features from the iris and the periocular region, using the rubber sheet model normalization~\cite{Daugman1993} and 1-D Log-Gabor filter and Multi-Block Transitional Local Binary Patterns, respectively.
Lastly, the CROSS-EYED I and II contests evaluated iris and periocular recognition on the cross-spectral scenario.
In both contests, the winner approach employed handcrafted features based on~\gls{safe},~\gls{gabor},~\gls{sift},~\gls{lbp}, and~\gls{hog}.

Inspired by impressive results achieved by deep learning-based techniques in multiple domains~\cite{lecun2015deep}, several methods proposing and applying such techniques have been developed to address different tasks using ocular images~\cite{Silva2015, lucio2019simultaneous, severo2018benchmark, lucio2018fully, bezerra2018robust, Menotti2015, He2016, Du2016, proenca2019inset, Zhao2019capsule, diaz2020spectrum, zanlorensi2020attnormalization, zanlorensi2018impact, zanlorensi2020deep, Luz2018, silva2018multimodal, hern2020crossspectral}.
Also, as found in the literature, deep learning frameworks for ocular biometric systems are a recent technology that still needs improvement~\cite{zanlorensi2019ocular}.
The use of ocular datasets containing images captured by mobile devices in unconstrained environments is a challenging task that has gained attention in recent years~\cite{DeMarsico2017, Rattani2016, reddy2018comparison, zanlorensi2019ocular, zanlorensi2020attnormalization}.
 
\section{Dataset}
\label{sec:database}

The UFPR-Periocular dataset was created to obtain images in unconstrained scenarios that contain realistic noises caused by occlusion, blur, and variations in lighting, distance, and angles.
To this end, we developed a mobile application~(app) enabling the participants to collect their pictures using their smartphones\footnote{Project approved by the Ethics Committee Board from the Health Science Sector of the Federal University of Paraná, Brazil -- Process CAAE 02166918.2.0000.0102, registered in the \textit{Plataforma Brazil} system -- \url{https://plataformabrasil.saude.gov.br/}}.
The single instructions to the participants is to place their eyes on a region of interest marked by a rectangle drawn in the app, as illustrated in ``Picture'' in Fig.~\ref{fig:picprocess}.
We also restricted the images to be captured in  sessions, with  images per session and a minimum interval of  hours between sessions.
In this way, we guarantee that the dataset has samples of the same subject with different noises, mainly due to different lighting and environments.
Furthermore, imposing this minimum time interval between sessions, it is possible to collect different attributes in the periocular region of the same subject, as the images are captured at different times of the day, e.g., subjects wearing and not wearing glasses and makeup.
Another attractive feature of this dataset is that all participants are Brazilian, and as Brazil has great ethnic diversity, there are images of subjects from different races, making this one of the first periocular datasets with such cultural~diversity.

The images were collected from June 2019 to January 2020.
The gender distribution of the subjects is ()~male and ()~female, and approximately  of the subjects are under  years old.
In total, the dataset has images captured from  different mobile devices --~the five most used device models were: \textit{Apple iPhone~8}~(\%), \textit{Apple iPhone~9}~(\%), \textit{Xiaomi Mi~8 Lite}~(\%), \textit{Apple iPhone~7}~(\%), and \textit{Samsung Galaxy J7~Prime}~(\%).

We remark that each subject captured all of their images using the same device model.
The distribution of age, gender, and image resolutions present in our dataset is shown in Fig.~\ref{fig:stats}.

\begin{figure*}[!tb]
\centering
\begin{tabular}{cc}

    {\includegraphics[width=.80\columnwidth]{figs/agegenderdistNew.pdf}}&
    {\includegraphics[width=1.20\columnwidth]{figs/image_resolution2.pdf}\vspace{-0.75mm}} \\

    \footnotesize (a) gender distribution among the age ranges & \footnotesize (b) image resolutions grouped into  intervals
    \normalsize

\end{tabular}

\caption{Age, gender and image resolution distributions in the UFPR-Periocular dataset. (a) note that gender has a balanced distribution, but the age range is concentrated under 30 years old (64\% of the subjects). (b) more than \% of the images have a resolution between  and  pixels, and more than \% of the images have resolution higher than  pixels.}
\label{fig:stats}
\end{figure*}


The dataset has  images of both eyes from  subjects.
Image resolutions vary from  to  pixels --~depending on the mobile device that was used to capture the image.
We crop/separate the periocular regions of the right and left eyes to perform the benchmark, assigning a unique class to each side.
Note that, once the image is cropped, the remainder image region is discarded as claimed in our project request to the Ethics Committee Board to preserve at maximum the identity of the participants.
We manually annotated the eye corners with  points per image (inside and outside eye corners), and used these points to normalize the periocular region regarding scale and rotation.
This process is detailed in Fig.~\ref{fig:picprocess}.

All the original and cropped periocular images along with the eye corner annotations are publicly available for the research community (upon request) at \textit{\url{\urldataset}}.

\begin{figure}[!ht]
\centering

   	\includegraphics[width=\columnwidth]{figs/normalization.pdf}

\vspace{-1.5mm}

\caption{Image acquisition and normalization process.
First, after the subject takes the shot, the rectangular region (outlined in blue) is cropped and stored. 
Then, the images are normalized in terms of rotation and scale using the manual annotations of the corners of the eyes.
Finally, the normalized images are cropped, generating the periocular regions of the left and right~eyes.}
\label{fig:picprocess}
\end{figure}

Using the center point of each eye (average corners point), the images were rotated and scaled to normalize the eye positions in a size of  pixels.
Then, the images were split into  patches to create the left and right eye sides, generating  periocular images from  classes.
The intra- and inter-class variability in this dataset is mainly caused by lighting, occlusion, specular reflection, blur, motion blur, eyeglasses, off-angle, eye-gaze, makeup, and facial expression.

\subsection{Experimental Protocols}
\label{sec:protocol}

We propose protocols for the two most common tasks in biometric systems: identification~(:) and verification~(:).
The identification task consists of determining a subject sample identity~(probe) within a known dataset or a cluster~(gallery).
The probe is compared against all the gallery samples, considering the closest match as the subject's identity.
Furthermore, probabilistic models can be employed/trained using the gallery data to determine the probe subject's identity based on the highest confidence output.
The verification task refers to the problem of verifying whether a subject is who she/he claims to be.
If two samples match sufficiently, the identity is verified; otherwise, it is rejected~\cite{bowyer2008survey}.
Verification is usually used for positive recognition, where the goal is to prevent multiple people from using the same identity.
The identification is a critical component in negative recognition, where the goal is to prevent a single person from using multiple identities~\cite{jain2008introbiometrics}.
Furthermore, the proposed protocol also encompasses two different scenarios: closed-world and open-world.
In the closed-world protocol, the dataset is split through different samples from the same subject, i.e., training and test sets have samples of the same subjects.
In the open-world protocol, there are different subjects both in the training and test sets.
The identification task is performed in the closed-world protocol, while the verification task can be performed in both closed and open-world protocols.
In the open-world protocol, we also propose two different splits regarding the training and validation sets.
Note that we do not change the test set, keeping it in the open-world protocol, and only vary the training protocols.
The first split uses the closed-world protocol, in which the training and validation sets have samples from the same subjects.
The second split, on the other hand, has different subjects in the training and validation sets, i.e., in an open-world protocol.
With these two training/validation splits, it is possible to use multi-class networks~(classification/identification) and also models based on the similarity of two distinct inputs~(verification task): Siamese networks, triplet networks, and pairwise filters.
Although models built for the verification task can be trained through the closed-world protocol, the design can be better improved using the open-world protocol to split the training and validation sets, as it is a more realistic scenario regarding the test set.
Table~\ref{tab:protocol} summarizes the proposed protocols.

\begin{table*}[!ht]
\centering
\caption{Images, Classes, and Pairwise comparison distributions for the closed-world~(CW) and open-world~(OW) protocols. Values for each fold (3 folds).}

\vspace{-1.5mm}

\label{tab:protocol}
\resizebox{2\columnwidth}{!}{
\begin{tabular}{@{}lccccccc@{}}
\toprule
\centering \multirow{3}{*}{Protocol} & \multirow{3}{*}{Train/Val} & \multicolumn{3}{c}{Images / Classes} & \multicolumn{3}{c}{Genuine pairs / Impostor pairs} \\

\cmidrule{3-8}


            & & Train               & Validation         & Test                & Train                        & Validation                 & Test                      \\
\midrule
CW & CW/CW    &   &   &   &     &   &  \\
OW & OW/CW    &   &   &       &     &   &   \\
OW & OW/OW    &   &       &       &   &   &   \\

\bottomrule
\end{tabular}}
\end{table*}


We defined  folds with a stratified split into training, validation, and test sets for both biometric tasks (identification and verification) for all protocols.
The test set comprises all against all comparisons for genuine pairs and aiming to reduce the pairwise comparisons only impostor pairs using the images of all subjects with the same sequence index, i.e., the -th images of each subject are combined two at-a-time to generate all impostor pairs, for , where .
As the UFPR-Periocular dataset has images captured under  sessions, we designated one session as a test set for each fold in the \textit{closed-world protocol}.
Thus, we have images from sessions  and ,  and ,  and  for training/validation, and sessions , , and  for testing, respectively for each of the three folds.  
To evaluate the ability of the models to recognize subjects samples at different environments, for all folds, we employed samples of both sessions in the training and validation sets to fed the models with images from the same subject varying the capture conditions.
For each subject, we employed the first  images of each session for training and the remaining  for validation ( for training/validation splits).
The test set contains new images from the subjects present in the training/validations sets with different noises caused by the environment, lighting, occlusion, and facial attributes.

For the \textit{open-world protocol} we generate the training, validation, and test sets by splitting the dataset through different subjects.
Thus, for each fold, the test set has samples of subjects not present in the training/validation set.
Splitting sequentially by the subject index for each fold, we have samples of  subjects for training/validation and  subjects for testing.
Moreover, we propose two different splits for the training/validation splits, the first one containing images of the same subject in the training and validation sets (closed-world validation).
The second one contains samples from different subjects in the training and validation sets (open-world validation).
Both training/validation protocols have pros and cons.
The advantage of using the closed-world validation is that the training has samples of more subjects than the open-world validation protocol.
However, in this scenario, the models can only learn distinctive features for the gallery samples and may not extract distinctive features for subjects not present in the training process.
On the other hand, the open-world validation has samples of fewer subjects than the closed-world validation protocol, presenting a more realistic scenario since samples of subjects not known in the training stage are present in the validation set.
In the closed-world validation protocol, for each one of the  subjects in the training set, we used the first  images of each session for training, and the remaining  for validation ( for training/validation splits).
In the open-world validation protocol, we employed samples of the first  subjects for training and samples of the remaining  subjects to validate each fold.
The number of the generated pairwise comparison for all protocols are detailed in Table~\ref{tab:protocol}.
The files determining all splits and setups detailed in this section are available along with the UFPR-Periocular~dataset. 
\section{Benchmark}
\label{sec:benchmark}

To carry out an extensive benchmark, we employ different models and strategies based on deep learning that achieved promising results in the ImageNet dataset/contest~\cite{deng2009imagenet} and were applied in recent works of ocular recognition~\cite{zanlorensi2018impact, silva2018multimodal, Luz2018, wang2019cross, zanlorensi2020deep}.
These methods differ from each other in network architecture, loss function, and training strategies.
We employed the following \gls{cnn} models: Multi-class classification, Multi-task learning, Siamese networks, and Pairwise filters networks.
In the following subsections, we describe and detail each one of~them.


\subsection{Multi-class Classification}
\label{sec:multiclass}

Multi-class classification is the task of classifying instances into three or more classes, where each sample must have a single unique class/label.
Several techniques~\cite{platt1999dags, hastie2000adaboost, huang2012extreme} have been proposed combining multiple binary classifiers to solve multi-class classification problems.
Deep learning-based approaches usually address this problem through \gls{cnn} models with softmax cross-entropy loss.
Therefore, we start by evaluating several \gls{cnn} architectures that achieved expressive results in the ImageNet dataset/contest~\cite{deng2009imagenet}.
In summary, the architecture of these models has several convolutional, pooling, activation, and fully-connected layers, as shown in~Fig.~\ref{fig:multiclass}.

\begin{figure}[!ht]
\centering

   	\includegraphics[width=\columnwidth]{figs/multiclass.pdf}

\vspace{-0.5mm}

\caption{Multi-class classification \gls{cnn} architecture.}
\label{fig:multiclass}
\end{figure}

In the training stage, a batch of images and their labels feed these models.
The model extracts the image features through convolutional, pooling, and fully connected (dense) layers.
The last layer is composed of a fully connected layer using the softmax cross-entropy as a loss function.
Below we describe the main characteristics of each model.

\subsubsection{\textbf{VGG}}

The VGG model, proposed by Simonyan and Zisserman~\cite{simonyan2015vgg}, consists of a \gls{cnn} using small convolution filters () with a fixed stride of  pixel.
The spatial polling is computed by  max-pooling layers over a  pixel window.
Two models were proposed varying the number of convolutional layers: VGG16 and VGG19.
Both models have two fully connected layers at the top with  channels each --~these architectures achieved the first and second places in the localization and classification tracks on the ImageNet Challenge 2014.
The authors also stated that it is possible to improve prior-art configurations by increasing the depth of the models.
Parkhi et al.~\cite{parkhi2015vggface} applied these models (called VGG16-Face) on the face recognition problem, showing that a deep \gls{cnn} with a simpler network architecture can achieve results comparable to the state of the~art.
Furthermore, recent approaches for ocular (iris/periocular) biometrics employing VGG models have demonstrated the ability to produce discriminant features~\cite{zanlorensi2018impact, silva2018multimodal, Luz2018, wang2019cross, zhao2019iriscapsule, zanlorensi2020deep, behera2020twindeep}.
In this work, we employed the VGG16 and VGG16-Face to perform the benchmark.

\subsubsection{\textbf{ResNet}}

The Residual Network (ResNet) was introduced by He et al.~\cite{he2016resnet} and applied to biometrics for face recognition~\cite{cao2017resnetface}, iris recognition~\cite{zanlorensi2018impact, boyd2019deep, zanlorensi2020deep, wang2019cross, zhao2019iriscapsule} and periocular recognition~\cite{zanlorensi2020deep, hern2020crossspectral, behera2020twindeep, boutros2020fusing}.
The authors addressed the degradation (vanishing gradient) problem caused by deeper network architectures proposing a deep residual learning framework.
They added shortcut connections between residual blocks to insert residual information.
These residual blocks are composed of a weighted layer followed by batch normalization, an activation function, another weighted layer, and batch normalization.
Let  be a residual block, and  the input of this block (identity map), the residual information consists of adding  to , i.e., , and using it as input to the next residual block.
Different architectures were proposed and evaluated, varying the depth of the models: ResNet50, ResNet101, and ResNet152.
These models achieved promising results on the ImageNet dataset~\cite{deng2009imagenet}.
In~\cite{he2016resnetv2}, He et al. proposed the ResNetV2 by changing the residual block by adding a pre-activation into it.
Empirical experiments showed that the proposed method improved the network generalization ability, reporting better results than ResNetV1~on ImageNet.

\subsubsection{\textbf{InceptionResNet}}

The InceptionResNet model~\cite{szegedy2016inceptionresnet}\st{,} combines the residual connections~\cite{he2016resnet} and the inception architecture~\cite{szegedy2016inception}.
The first inception model~\cite{szegedy2015inception}, known as GoogLeNet, introduced the Inception module aiming to increase the network depth while keeping a relatively low computational cost.
The main idea of inception is to approximate a sparse \gls{cnn} with a normal dense construction.
The inception module consists of several convolutional layers, where their output filter banks are concatenated and used as the input to the next module.
The model version difference is based on the organization inside its inception module.
Combining the residual connections with the InceptionV3 and InceptionV4 models, the author developed InceptionResNetV1 and InceptionResNetV2, respectively.
Experiments performed on the ImageNet dataset showed that the InceptionResNet models trained faster and reached slightly better results than the inception architecture~\cite{szegedy2016inceptionresnet}.
In our experiments, we employed the InceptionResNetV2 model since it achieved the best results on~ImageNet.

\subsubsection{\textbf{MobileNet}}

The first version of the MobileNet model (MobileNetV1)~\cite{howard2017mobilenet} was developed focusing on mobile and embedded vision applications, in which it is desirable that the \gls{cnn} model has a small size and high computational efficiency.
This model is based on depthwise separable filters, which are composed of depthwise and pointwise convolutions. 
As described in~\cite{howard2017mobilenet},  depthwise convolutions apply a single filter for each input channel, and pointwise convolutions use a  convolution to compute a linear combination of the depthwise output.
Both layers use batch normalization and ReLU activation.
MobileNetV1 achieved promising results in both terms of performance and accuracy on several tasks such as fine-grained recognition, large scale geolocation, face attributes classification, object detection, and face recognition~\cite{howard2017mobilenet}. 
MobileNetV2~\cite{sandler2018mobilenetv2} combines the first version architecture with an inverted ResNet~\cite{he2016resnet} structure, which has shortcut connections between the bottleneck layers.
Experiments performed in different tasks such as image classification, object detection, and image segmentation showed that the MobileNetV2 can achieve high accuracy with low computation costs compared to state-of-the-art~methods~\cite{sandler2018mobilenetv2}.


\subsubsection{\textbf{DenseNet}}

The Dense Convolutional Network~(DenseNet) model~\cite{huang2017densenet} consists of a \gls{cnn} architecture where each layer is connected to every other layer in a feed-forward way.
Thus, let  be the number of layers from a network, a DenseNet layer has  direct connections with subsequent layers --~instead of  as a traditional \gls{cnn} model.
As in the ResNet models~\cite{he2016resnet, he2016resnetv2}, these connections can handle the vanishing-gradient problem and ensure maximum information flow between layers.
The feed-forward is preserved, passing the output from all layers as an additional input to the subsequent ones in a channel-wise concatenation.
The DenseNet models achieved state-of-the-art accuracies in image classification on the CIFAR10/100 and ImageNet datasets~\cite{deng2009imagenet, huang2017densenet}. 
The authors proposed different models varying the depth of the network.
In our experiments, we employed DenseNet121~(the shallowest~one).

\subsubsection{\textbf{Xception}}

Inception modules inspired the creation of the Xception model, which can be defined as an intermediate step between regular convolution and the depthwise separable convolution operation~\cite{chollet2017xception}.
The proposed architecture replaces the standard inception modules with depthwise separable convolutions, and also have residual connections.
The Xception architecture has the same number of parameters as InceptionV3 but outperforms it on the ImageNet~dataset~\cite{deng2009imagenet}.


\subsection{Multi-task Learning}
\label{sec:multitask}

Multi-task learning uses the domain information of related tasks as an inductive bias to improve generalization~\cite{caruana1997multitask}.
A Multi-task network can learn several tasks using a shared \gls{cnn} model, where each task can help the generalization for other tasks.
Caruana~\cite{caruana1997multitask} introduced the Multi-task learning concept and evaluated it in different domains, demonstrating that this method can achieve better results than single-task learning models for related tasks.
In deep neural networks, multi-task learning can be performed by using hard or soft parameter sharing~\cite{ruder2017multideep}.
The most common one is the hard parameter sharing, where all the hidden (convolutional) layers weights are shared, i.e., the model learns a single representation for all tasks.
Then, different tasks use these shared features by adding some layers for each specific task.
On the other hand, in soft parameter sharing one model is employed for each task.
Then, the parameters of these models are regularized to encourage similarities among them.

As shown in Fig.~\ref{fig:multiclass}, our Multi-task network shares all convolutional layers and some dense layers.
The model has exclusive dense layers for each task, followed by the prediction layers, using the softmax cross-entropy as function~loss.

\begin{figure}[!ht]
\centering

   	\includegraphics[width=\columnwidth]{figs/multitask.pdf}

\caption{Multi-task \gls{cnn} architecture. In this model, each task has its own output and all tasks share the convolutional layers. The loss of all tasks is used to update the weights of the convolutional layers.}
\label{fig:multitask}
\end{figure}

In this work, based on the results of multi-class classification, we employ  MobileNetV2 as the base model on our multi-task approach.
Furthermore, as detailed in Table~\ref{tab:multitask-architecture}, we build our multi-task model with hard parameter sharing for the following  tasks: (i)~class prediction, (ii)~age rate, (iii)~gender, (iv)~eye side, and (v)~smartphone model.

\begin{table}[!htb]
\centering
\caption{Multi-task architecture in the closed-world protocol.}
\label{tab:multitask-architecture}

\vspace{-1.5mm}

\resizebox{\columnwidth}{!}{ \begin{tabular}{@{}clccc@{}}
\toprule
\textbf{\#} & \textbf{Layer}      & \textbf{Connected to}  & \textbf{Input}             & \textbf{Output} \\ \midrule
  & MobileNetV2 ( layers)   & --                     &   &  \\
  & dense (classes)             &                   &                      &  \\
  & dense (age)                 &                   &                      &  \\
  & dense (gender)              &                   &                      &  \\
  & dense (eye side)            &                   &                      &  \\
  & dense (smartphone model)    &                   &                      &  \\
  & predict (classes)           &                   &                       &  \\
  & predict (age)               &                   &                       &  \\
  & predict (gender)            &                   &                       &  \\
  & predict (eye side)          &                   &                       &  \\
 & predict (smartphone model)  &                   &                       &  \\ \bottomrule


\end{tabular}} \end{table}

For the age estimation task, we generate the classes by grouping ages into the following  ranges: -, -, -, -, -, -, -, -, -, and -.
The gender and eye side prediction tasks have only  classes, while the smartphone model prediction has ~classes.
Note that Multi-task learning networks can use weighted loss for the tasks, penalizing the wrong classification of some tasks more than others.
For simplicity, in this work, we do not use weighted losses in our experiments, giving equal importance to all tasks.

\subsection{Pairwise Filters Network}
\label{sec:pairwise}

Inspired by~\cite{liu2016deepiris}, which is one of the first works applying deep learning for iris verification, we also evaluate the performance of the pairwise filters network.
This kind of model directly learns the similarity between a pair of images through pairwise filters.
The Pairwise Filters Network is a Multi-class classification model that contains one or two outputs informing whether the input pairs are from the same class or from different classes.
The difference is that the network input is a pair of images instead of a single image.
Thus, the network architecture consists of convolutional, pooling, activation, and fully connected layers, as shown in Fig.~\ref{fig:pairwise}.


\begin{figure}[!ht]
\centering

   	\includegraphics[width=\columnwidth]{figs/pairwise.pdf}

\vspace{-0.5mm}

\caption{Pairwise filters \gls{cnn} architecture. This model contains filters that directly learn the similarity between a pair of images. The output informs whether the images are of the same person or not.}
\label{fig:pairwise}
\end{figure}

As this model requires a pair of images as input, different concatenation strategies can be employed.
Following Liu et al.~\cite{liu2016deepiris}, in this work, we generate the input pairs by concatenating the images at the depth level.
Let two RGB images with shapes of , concatenating both images by its channels; the resulting input image will have a shape of .
The output of our model has two neurons and uses a softmax cross-entropy loss.
As the verification problem has only two classes, this model' output can also have only one neuron using a binary cross-entropy loss function.
As in the Multi-task network, we employ MobileNetV2 as the base model for our Pairwise Filters~Network. 


\subsection{Siamese Network}
\label{sec:siamese}

Introduced by Bromley et al.~\cite{bromley1993siamese} for signature verification, Siamese networks consist of twin branches sharing their parameters (trainable parameters).
Such models learn similarities/distances between a pair of inputs, being used mainly for verification tasks.
As illustrated in Fig.~\ref{fig:siamese}, each branch of the Siamese structure is composed of a \gls{cnn} model followed by some dense layers.
These models can also have shared and non-shared dense layers at the top. 

\begin{figure}[!ht]
\centering
   	\includegraphics[width=\columnwidth]{figs/siamese.pdf}

\vspace{-0.5mm}

\caption{Siamese \gls{cnn} architecture. This model is composed of two twin branches of convolutional layers sharing their trainable parameters. The output computes a distance between the input image~pairs.}
\label{fig:siamese}
\end{figure}


As detailed in Table~\ref{tab:siamese-architecture}, we employ MobileNetV2 as the base model for each branch of the Siamese network.
We use the contrastive loss~\cite{chopra2005contrastive, hadsell2006contrastive} in the training stage to compute the similarity between the input pair~images. 

\begin{table}[!htb]
\centering
\caption{Siamese network architecture description.}
\label{tab:siamese-architecture}

\vspace{-1.5mm}

\resizebox{\columnwidth}{!}{ \begin{tabular}{@{}llccc@{}}
\toprule
\textbf{\#} & \textbf{Layer}  & \textbf{Connected to} & \textbf{Input}            & \textbf{Output} \\ \midrule
 & branch\_a (MobileNetV2 ( layers)) & --                    &  &  \\
 & branch\_b (MobileNetV2 ( layers)) & --                    &  &  \\
 & dense              & \#0 and \#1           &                      &  \\
 & Euclidean dist. / Contrastive loss        & \#2                   &                      &  \\ \bottomrule


\end{tabular}} \end{table}

As described in~\cite{hadsell2006contrastive}, let  be the Euclidean distance between two input vectors, the contrastive loss can be written as follows:



\noindent where



\noindent and  is the number of training pairs,  corresponds to the -th label () of the sample pair , and  and  are partial losses for a pair of similar and dissimilar points, respectively.
The objective of this function is to minimize  for  and  by computing low and high values of  for similar and dissimilar pairs, respectively.  

The contrastive loss was proposed and applied to face verification~\cite{chopra2005contrastive, hadsell2006contrastive} and has been employed for periocular recognition~\cite{zhao2018improving, behera2020twin} and iris recognition~\cite{wang2019cross}.
 
\section{Results and Discussion}
\label{sec:results}

This section presents the benchmark results for the identification and verification tasks.
We first describe the experimental setup used to perform the benchmark.
Then, we report and discuss the results achieved by each~approach.

\subsection{Experimental Setup}

Inspired by several recent works~\cite{Luz2018, zanlorensi2018impact, reddy2018comparison, wang2019cross, boyd2019fine, zanlorensi2020deep, boutros2020fusing, diaz2020spectrum, hern2020crossspectral}, we perform the benchmark employing pre-trained models on ImageNet and also for face recognition~(VGG16-Face and ResNet50-Face).
Afterward, we fine-tuned these models using the UFPR-Periocular dataset.
Similar to recent works on ocular recognition~\cite{Luz2018, zanlorensi2018impact, silva2018multimodal, zanlorensi2019ocular}, we modify all models by adding a fully convolutional layer before the last layer (softmax) to generate a feature vector with a size of  for each image.
The default input size of the models is , except for the InceptionResNet and Xception models, which have an input size of~. 
Note that the input dimensions are different because we are using pre-trained models and our fine-tuning process should respect the input size of the original architectures.

For all methods, the training was performed during  epochs with a learning rate of  for the first  epochs and  for the remaining epochs using the Stochastic Gradient Descent~(SGD) optimizer.
Then, we used the weights from the epoch that achieves the lower loss in the validation set to perform the~evaluation.

We employ Rank  and Rank  accuracy for the identification task, and the Area Under the Curve~(AUC), Equal Error Rate~(EER), and Decidability~(DEC) metrics for verification.
Furthermore, to generate the verification scores, we compute the cosine distance between the deep representations generated by each \gls{cnn} model.
As described and applied in several works with state-of-the-art results~\cite{Luz2018, zanlorensi2018impact, zanlorensi2020deep, zanlorensi2020attnormalization}, the cosine distance is computed by the cosine angle between two vectors, being invariant to scalar transformation.
This measure gives more attention to the orientation than to the coefficient of magnitude of the representations, being an interesting metric to compute the similarity between two vectors.
The cosine metric distance is given by:

\noindent where  and  stand for the feature vectors.

Regarding the models explicitly developed for the verification tasks, i.e., the Siamese network and the Pairwise Filters network, as this task has unbalanced samples of genuine and impostors pairs, selecting the best samples to perform the training is challenging.
Thus, trying to fit the models by feeding them as diverse samples as possible, we employed all genuine pairs and randomly selected the same number from the impostor pairs for each epoch.
Hence, each epoch may have different impostor samples.
However, for a fair comparison, we generated the random impostor pairs only once for each epoch and fold, and used the same samples for training both~models.

The reported results are from  repetitions for each fold, except for the Siamese and Pairwise filter networks, in which we ran only  repetitions due to the high computational cost.
All experiments were performed on a computer with an AMD Ryzen Threadripper X GHz (GHz Turbo)~CPU, ~GB of RAM and an NVIDIA Titan V~GPU.
All~\gls{cnn} models were implemented in python using the Tensorflow\footnote{\url{https://www.tensorflow.org/}} and Keras\footnote{\url{https://keras.io/}} frameworks.


\subsection{Benchmark results}

This section presents the results obtained by each approach in the closed-world and open-world protocols.
We also perform an ablation study on the Multi-task learning network to evaluate each task's influence in the identification mode.
First, we show in Table~\ref{tab:modelstats} the size and the number of trainable parameters of each \gls{cnn} model used as a benchmark.
This information is from the models that we used on the closed-world protocol since they have more neurons on the last layer than the open-world protocol models.

\begin{table}[!ht]
\centering
\caption{Size~(MB) and number of trainable parameters of the \gls{cnn}~models used in the benchmark.}
\label{tab:modelstats}

\vspace{-1.5mm}

\begin{tabular}{@{}lrr@{}}
\toprule

Model              & Size (MB)       & Trainable parameters   \\


\midrule
VGG16              &   &   \\
VGG16-Face         &   &   \\ 
InceptionResNet    &    &   \\
ResNet50V2         &    &   \\
ResNet50           &    &   \\
ResNet50-Face      &    &   \\
Xception           &    &   \\
DenseNet121        &     &   \\
MobileNetV2        &     &   \\
\midrule
Multi-task         &     &   \\
\midrule
Siamese            &     &   \\
Pairwise           &     &   \\
\bottomrule
\end{tabular}
\end{table}

As can be seen, the benchmark has a great diversity of models with different sizes and parameters due to their difference in structure, depth, concept, and~architectures. 

\subsubsection{Closed-world protocol}
\label{sec:closed}
In the closed-world protocol, we perform the benchmark for both the identification and verification tasks.
All results are presented in Table~\ref{tab:benchclosed}.
As can be seen, although MobileNetV2 is the smallest model in terms of size and trainable parameters, it achieved the best results for both identification and verification tasks.
Hence, we used MobileNetV2 as the base model for the Multi-task, Siamese, and Pairwise Filters networks.

\begin{table*}[!ht]
\centering
\caption{Benchmark results in the closed-world protocol for the identification and verification tasks.}
\label{tab:benchclosed}

\vspace{-1.5mm}

\begin{tabular}{@{}lccccc@{}}
\toprule

\centering \multirow{2}{*}{Model} & \multicolumn{2}{c}{Identification (:)} & \multicolumn{3}{c}{Verification (:)}  \\

\cmidrule{2-6}

                   & Rank 1 (\%)     & Rank 5 (\%)     & AUC (\%)           & EER (\%)        & Decidability       \\


\midrule
VGG16              &   &   &   &    &  \\
VGG16-Face         &   &   &   &    &  \\
Xception           &   &   &   &    &  \\
ResNet50V2         &   &   &   &    &  \\
InceptionResNet    &   &   &   &    &  \\
ResNet50           &   &   &   &    &  \\
ResNet50-Face      &   &   &   &    &  \\
DenseNet121        &   &   &   &    &  \\
MobileNetV2        &   &   &   &    &  \\
\midrule
\textbf{Multi-task}& \boldmath{}  & \boldmath{}  & \boldmath{}  & \boldmath{}   &  \\

\midrule
Siamese            &              &              &   &   &   \\
Pairwise           &              &              &   &   & \boldmath{}  \\
\bottomrule
\end{tabular}
\end{table*}

In general, the Multi-task model achieved the best results in terms of Rank~, Rank~, AUC, and~EER.
We highlight that we only explored the other tasks --~age, gender, eye side, and mobile device model~-- at the training stage of this model.
For the evaluation, we extracted the representations for the classification task and used it for the identification (using the softmax layer) and verification (using the cosine distance) tasks.
The Siamese network obtained the worst results in the benchmark, while the Pairwise Filters network reached the higher Decidability index, indicating that it was the best at separating genuine and impostors distributions.
However, it did not achieve the best results in terms of AUC and~EER.

As stated in some previous works~\cite{Luz2018, boyd2019fine}, the models pre-trained for face recognition generally achieve best results than those pre-trained on the ImageNet~dataset.


\subsubsection{Open-world protocol}
\label{sec:open}

The main idea of the open-world protocol is to evaluate the capability of the methods to extract discriminant features from samples of classes that are not present in the training stage.
Thus, for this protocol, we perform a benchmark only for the verification task.
The results are shown in Table~\ref{tab:benchopen}.


\begin{table*}[!ht]
\centering
\caption{Benchmark results in the open-world protocol for the verification task.}
\label{tab:benchopen}

\vspace{-1.5mm}

\begin{tabular}{@{}lcccc@{}}
\toprule

\centering \multirow{2}{*}{Model} & \centering \multirow{2}{*}{Validation} & \multicolumn{3}{c}{Verification (1:1)} \\

\cmidrule{3-5}

                   &               & AUC (\%)            & EER (\%)        & Decidability       \\


\midrule
VGG16              & Closed-World  &   &    &  \\
VGG16-Face         & Closed-World  &   &    &  \\
ResNet50           & Closed-World  &   &    &  \\
ResNet50V2         & Closed-World  &   &    &  \\
Xception           & Closed-World  &   &    &  \\
InceptionResNet    & Closed-World  &   &    &  \\
ResNet50-Face      & Closed-World  &   &    &  \\
DenseNet121        & Closed-World  &   &    &  \\
MobileNet          & Closed-World  &   &    &  \\
\midrule
\textbf{Multi-task }& \textbf{Closed-World}  & \boldmath{}  & \boldmath{}   &  \\
\midrule
Siamese            & Closed-World  &   &    &  \\
Pairwise           & Closed-World  &   &    & \boldmath{} \\
\midrule
Siamese            & Open-World    &   &    &  \\
Pairwise           & Open-World    &   &    &  \\
\bottomrule
\end{tabular}
\end{table*}


As in the closed-world protocol, the Multi-task model achieved the best results in Rank~, Rank~, AUC, and EER, and the Pairwise network achieved the best Decidability index.
The Siamese and Pairwise Filters networks trained using the closed-world validation split reached better results than when trained using the open-world validation split.
We believe this occurred due to the fact that there are fewer classes in the training set in the open-world validation split than in the closed-world validation split.
Although the open-world validation split corresponds to a more realistic scenario regarding the test set, the networks trained with samples from a larger number of classes can reach a higher capability of generalization, producing discriminative representations even for samples from classes not present in the training stage.

\subsubsection{Multi-task Learning}
\label{sec:multi}

The Multi-task model achieved the best results both in the closed- and open-world protocols.
As this network simultaneously learns different tasks, we perform an ablation study by running some experiments with  new models created by removing one of the tasks at a time.
The experiments were carried out in the closed-world protocol to evaluate the performance of both identification and verification.
We also evaluated the results achieved by all models in each~task.

\begin{table*}[!htb]
\centering
\caption{Results (\%) from several Multi-task models trained to predict different tasks.}
\label{tab:multitask}

\vspace{-1.5mm}

\begin{tabular}{@{}lccccccc@{}}
\toprule

Model                   & Rank 1     & Rank 5      & Device Model  & Age             & Gender          & Eye Side        \\


\midrule
Multi-task (no model)   &   &   &                &   &   & \boldmath{}  \\
Multi-task (no age)     &   &   &     &              &   & \boldmath{}  \\
Multi-task (no gender)  &   &   &     &   &              & \boldmath{}  \\
Multi-task (no side)    &   &   &     &   &   &              \\
\textbf{Multi-task}     & \boldmath{}  & \boldmath{}  & \boldmath{}    & \boldmath{}  & \boldmath{}  &   \\



\bottomrule
\end{tabular}
\end{table*}

According to Table~\ref{tab:multitask}, the Multi-task network without the prediction of the mobile device model was the most penalized for the identification task, followed by the network variations without age, gender, and eye side estimation, respectively.
The gender and eye side classification tasks were handled well by all models, while the device model and age range classification tasks proved to be more challenging.
One problem in the device model and age range classification is the unbalanced number of samples per class, which can generate a bias during the training stage.

Note that in both closed-world and open-world protocols, we only explored the class prediction for the matching.
However, as shown in Table~\ref{tab:multitask}, the multi-task architecture also achieved promising results in the other tasks.
In this sense, it may be possible to further improve the recognition results by adopting heuristic rules based on the scores of the other~tasks.

\subsubsection{Subjective evaluation}
\label{sec:subjective}

In this section, we perform a subjective evaluation through visual inspection on the pairs of images erroneously classified by the Multi-task model, which achieved the best result in the verification task in the closed-world protocol.
The best impostors (impostors classified as genuine) and the worst genuines (genuine classified as impostors) pairs are presented in Fig.~\ref{fig:pairserror}.

\begin{figure}[!tb]
\centering
\setlength{\tabcolsep}{1.5pt}
\begin{tabular}{cccc}

    \multicolumn{4}{c}{Wrong Genuines (Best Impostors)} \\
    
    \scriptsize  & \scriptsize  & \scriptsize  & \scriptsize \\

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0022S3I13R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0023S3I13R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0486S3I13L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0487S3I13L.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0199S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0958S3I11R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0478S3I14R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C1045S3I14R.jpg}}\\

    \scriptsize  & \scriptsize & \scriptsize  & \scriptsize \\

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0271S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0281S3I12R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0854S3I14R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C1118S3I14R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0653S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0698S3I12R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0647S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C1026S3I12R.jpg}}\\
    
     \scriptsize  & \scriptsize  & \scriptsize  & \scriptsize\\

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0658S3I14L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0659S3I14L.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0876S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C1056S3I11R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0063S3I13R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0525S3I13R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0121S3I14R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/pairs/C0286S3I14R.jpg}}\1.75ex] 
    
    \multicolumn{4}{c}{Wrong Impostors (Worst Genuines)} \\
    
    \scriptsize  & \scriptsize  & \scriptsize  & \scriptsize \\

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0981S3I11L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0981S3I12L.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0110S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0110S3I15R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0964S3I11L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0964S3I15L.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0865S3I12L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0865S3I14L.jpg}}\\

    \scriptsize  & \scriptsize  & \scriptsize  & \scriptsize \\

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0714S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0714S3I13R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0393S3I12R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0393S3I14R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0120S3I13R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0120S3I14R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0160S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0160S3I12R.jpg}}\\
    
    \scriptsize  & \scriptsize  & \scriptsize  & \scriptsize \\

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0998S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0998S3I15R.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0526S3I12L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0526S3I15L.jpg}}&

    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0521S3I11R.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0521S3I14R.jpg}}&
    
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0694S3I13L.jpg}}
    {\includegraphics[width=0.11\columnwidth]{figs/worstgen/C0694S3I14L.jpg}}\\

\end{tabular}
\caption{Pairwise images wrongly classified by the model that obtained the best result in the verification task in the open-world protocol. Higher scores mean that the pair of periocular images is more likely to be~genuine.}
\label{fig:pairserror}
\end{figure}

Performing a visual analysis of all pairwise errors, it is clear that hair occlusion, age, eyeglasses, and eye shape were the most influential factors that led the model to the wrong classification of genuine pairs (intra-class comparison).
In pairs wrongly classified as impostors (inter-class comparison), we saw that lighting, blur, eyeglasses, off-angle, eye-gaze, reflection, and facial expression caused the main difference between the images.
We hypothesize that some errors caused by lightning, blur, reflection, and occlusion can be reduced by employing some data augmentation techniques in the training stage.
Attribute normalization~\cite{zanlorensi2020attnormalization} can also reduce the errors caused by attributes present in the periocular region such as eyeglasses, eye gaze, makeup, and some types of occlusion.
Although some methods can be applied to reduce the matching errors, there are still several characteristics in these images that make the mobile periocular recognition a challenging~task, mainly to the high intra-class variations. 
\section{Conclusion}
\label{sec:conclusion}

This article introduces a new periocular dataset that contains images captured in unconstrained environments on different sessions using several mobile device models.
The main idea was to create a dataset with real-world images regarding lighting, noises, and attributes in the periocular region.
To the best of our knowledge, in the literature, this is the first periocular dataset with more than  subject samples and the largest one in the number of different sensors~().

We presented an extensive benchmark with several~\gls{cnn} models and architectures employed in recent works for ocular recognition.
These architectures consist of models for Multi-class classification and Multi-task Learning, in addition to Siamese and Pairwise Filters networks.
We evaluated the methods in the closed-world and open-world protocols, as well as for the identification and verification tasks.
For both protocols and tasks, the Multi-task model achieved the best results.
Thus, we conducted an ablation study on this model to understand which tasks had the most significant influence on the results.
We stated that the mobile device model identification task was the most important one, followed by age range, gender, and eye side classification.
The model trained using all these tasks reported the best result for the identification and verification in the closed- and open-world protocols. 

In a complementary way, we performed a subjective analysis of the best/worst false genuine and true impostors image pairwise comparisons using the Multi-task model, which achieved the best performance for the verification task. 
We observed that lighting, occlusion, and image resolution were the most critical factors that led the model to wrong~verification.

We believe that the UFPR-Periocular dataset will be of great relevance to assist in evolving ocular biometric systems using images obtained by mobile devices in unconstrained scenarios.  
This dataset is the most extensive in terms of the number of subjects in the literature and has natural within-class variability due to samples captured in different sessions.

The Multi-task network using the MobileNetV2 as baseline model achieved the best benchmark results for the identification and verification tasks, reaching a rank  of \% and an EER of \% in the closed-world protocol, and an EER of \% in the open-world protocol.
Therefore, there is still room for improvement in both identification and verification tasks.
 

\section*{Acknowledgment}
This work was supported by grants from the National Council for Scientific and Technological Development~(CNPq) (\#~313423/2017-2 and \#~428333/2016-8) and the Coordination for the Improvement of Higher Education Personnel~(CAPES).
We acknowledge the support of NVIDIA Corporation with the donation of the Titan V GPU used for this~research.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}

\bibliography{references.bib}

\end{document}
