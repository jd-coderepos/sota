\section{Results}

The best supervised model trained only on train-clean-100 () achieves a (no LM) WER of 14.00\%/37.02\% on dev-clean/dev-other, respectively.
Unless otherwise stated, we use train-clean-360 as the unpaired speech dataset,  for length filtering with reference lengths obtained from ASR-only greedy decoding.

\subsection{Beam Size, Mixing Ratio, and LPM Weights}

\begin{table*}[ht]
    \small
    \caption{We vary the mixing ratio  and the beam size . An LPM weight  is used.}
    \label{tab:beam}
    \begin{center}
    \begin{tabular}{llccccc}
        \toprule
        \multirow{2}{*}{} & \multirow{2}{*}{} & 
        \multicolumn{5}{c}{dev-clean / dev-other WER (\%)} \\
        & &  &  &  &  &  \\ 
        \midrule
        \multirow{4}{*}{360hr}
        &  & 10.75 / 31.62 & 10.60 / 30.96 & 10.25 / 30.67 & 10.14 / 30.17 & 10.09 / 29.99 \\
        &  & 10.43 / 29.76 &  9.56 / 28.83 &  9.37 / 28.10 &  9.06 / 27.35 &  8.88 / 27.25 \\
        &  & 11.09 / 29.89 &  9.34 / 27.45 &  \textbf{9.00 / 26.47} &  9.15 / 26.52 &  9.36 / 27.00 \\
        &  & 12.11 / 30.89 & 10.11 / 27.71 &  9.76 / 27.08 & 10.17 / 27.41 & 10.30 / 27.62 \\
        \midrule
        860hr &  & 10.59 / 26.05 & 9.37 / 23.85 & 8.68 / 22.53 & 8.37 / 21.56 & \textbf{8.37} / \textbf{21.33} \\
        \bottomrule
    \end{tabular}
    \end{center}
\end{table*}

Table~\ref{tab:beam} shows how the WER varies with the beam size and the mixing ratio. 
For all mixing ratios, the model improves the most from a beam size of  to , showing the benefit of considering multiple hypotheses.
The improvement is greater when a higher mixing ratio of unpaired-to-paired speech is used.
In addition, we note that the LM is effectively unused when  because the LM probability assigned to each hypothesis is normalized within the beam. If there is only one hypothesis, it will be assigned an approximate posterior probability of one.
The amount of improvement diminishes with larger beam sizes, and the performance even starts to degrade beyond  when using a higher mixing ratio.
This may result from the inclusion of worse hypotheses which have a better score under the LM.
We use the best setting for the following experiments with a mixing ratio , a beam size , and an LPM weight .
We present detailed results varying the LPM weight in the Supplementary Material.

\subsection{Proposal Model Update and Model Initialization}\label{sec:res_init_prop}

In addition to the two update strategies proposed in Section~\ref{sec:offpolicy}, termed \emph{On} and \emph{Off (better)}, we experiment with two additional strategies.
The first, \emph{Off (never)}, uses a fixed proposal model throughout training.
The second, \emph{Off (always)}, updates the proposal model with the online model every  steps (i.e., set ) regardless of performance.

The full results are shown in Table~\ref{tab:init_update}. 
Four key takeaways are as follows:
\begin{enumerate*}[label=(\arabic*)]
    \item For all combinations of  initialization, off-policy (never) is the worst. This demonstrates the importance of updating the proposal model to generate better hypotheses during training.
    
    \item Off-policy (always) consistently outperforms on-policy. We observe that training is significantly stabilized by reducing the proposal model update frequency from every step to every 1,000 steps.
    The effect is particularly prominent when initializing  and  from an earlier checkpoint (9.62\% vs 20.02\% on dev-clean, and 27.51\% vs 45.62\% on dev-other).
    
    \item Off-policy (better) achieves the best WER in all settings and outperforms off-policy (always) by a larger margin when initializing from an earlier checkpoint.
    
    \item Unlike the other strategies, off-policy (better) demonstrates consistent improvement when using a less-trained initial online model.
\end{enumerate*}
In the following experiments, we initialize models with  and  unless otherwise specified.

\begin{table}[ht]
    \small
    \caption{We vary the proposal update strategies and the initial model weights for both  and .}
    \label{tab:init_update}
    \begin{center}
    \resizebox{\linewidth}{!}{\begin{tabular}{llccc}
        \toprule
        \multirow{2}{*}{Init } & 
        \multirow{2}{*}{ update} & 
        \multicolumn{3}{c}{dev-clean / dev-other WERs} \\
        & & Init  & Init  & Init  \\
        \midrule
        \multirow{4}{*}{}
        & On             &  9.50 / 28.29 & N/A & N/A \\
        & Off (never)    & 11.19 / 31.74 & 11.14 / 31.69 & 11.24 / 31.53 \\
        & Off (always)   &  9.40 / 27.79 &  9.27 / 27.33 &  9.52 / 27.34 \\
        & Off (better)   &  9.20 / 27.42 &  9.14 / 26.80 &  \textbf{9.00} / \textbf{26.47} \\
        \midrule
        \multirow{4}{*}{}
        & On             & N/A & 10.17 / 28.35 & N/A\\
        & Off (never)    & 13.61 / 35.39 & 13.95 / 35.43 & 13.56 / 35.62 \\
        & Off (always)   &  9.50 / 27.81 &  9.56 / 27.58 &  9.79 / 27.44 \\
        & Off (better)   &  9.30 / 27.34 &  9.26 / 27.01 &  \textbf{9.15} / \textbf{26.63} \\
        \midrule
        \multirow{4}{*}{}
        & On             & N/A & N/A & 20.20 / 45.62 \\
        & Off (never)    & 20.59 / 44.09 & 22.95 / 46.43 & 23.42 / 46.89 \\
        & Off (always)   &  9.52 / 27.89 &  9.46 / 27.35 &  9.62 / 27.51 \\
        & Off (better)   &  9.44 / 27.34 &  \textbf{9.31} / 27.26 &  9.43 / \textbf{27.19} \\
        \bottomrule
    \end{tabular}
    }
    \end{center}
\end{table}


\subsection{Length Filtering}

Table~\ref{tab:filter} shows the impact of length filtering with different proposal model update strategies, where both the proposal model and the online model are initialized with .
As discussed in Section~\ref{sec:filter}, on-policy suffers more than off-policy without length filtering.
If the proposal model is never updated, then length filtering does not affect the final WER. We hypothesize that length filtering keeps the proposal model stable during training.
In addition, we also compare three reference lengths: the oracle length, the predicted length from ASR + LM decoding, and that from ASR-only decoding. We observe that the WER does not differ much when using different reference lengths for filtering. Detailed results are shown in the Supplementary Material.

\begin{table}[ht]
    \small
    \caption{A comparison of WER with and without length filtering.}
    \label{tab:filter}
    \begin{center}
    \begin{tabular}{lcc}
        \toprule
        \multirow{2}{*}{ update} &  
        \multicolumn{2}{c}{dev-clean / dev-other WER} \\
        & No filtering & With filtering \\ 
        \midrule
        On-Policy             & 26.65 / 59.07 &  9.50 / 28.29 \\
        Off-Policy (never)    & 11.18 / 31.83 & 11.19 / 31.74 \\
        Off-Policy (always)   & 13.99 / 35.52 &  9.40 / 27.79 \\
        Off-Policy (better)   & 11.42 / 31.56 &  \textbf{9.20} / \textbf{27.42} \\
        \bottomrule
    \end{tabular}
    \end{center}
    \caption{Comparing  with different token perplexity.}
    \label{tab:lmppl}
    \begin{center}
    \begin{tabular}{lc}
        \toprule
         PPL & dev-clean / dev-other WER \\
        \midrule
        34.24 &  \textbf{9.00} / \textbf{26.47} \\
        64.22 & 10.08 / 26.92 \\
        97.87 & 10.90 / 27.97 \\
        142.12 & 11.53 / 28.74 \\
        180.71 & 13.18 / 30.74 \\
        \bottomrule
    \end{tabular}
    \end{center}
\end{table}


\subsection{Choice of Language Models}

We study how the quality of the LM affects the results using the same ConvLM but trained for a varying number of steps.
We quantify LM quality with the token perplexity (PPL) on the development set.
Table~\ref{tab:lmppl} shows a clear positive correlation between the LM quality and the final WER. This is expected given that the better LM results in a more accurate posterior approximation, .

\subsection{Comparison with Knowledge Distillation}
We next study different weak KD strategies and compare them with LPM. When multiple hypotheses () are used for KD, the student model matches against a uniform target distribution as done in~\citet{kahn2019self}. In addition, while KD typically considers a fixed teacher, we include another variant that adopts the same off-line teacher update strategy as LPM to disentangle the effect.
Results are shown in Table~\ref{tab:kd} with three key takeaways.
\begin{enumerate*}[label=(\arabic*)]
    \item Using an improving teacher leads to better performance.
    \item Distilling from a posterior that combines the ASR and LM models achieves better results; however, this hinders the use of an improving model as discussed in Section~\ref{sec:related}.
    \item Incorporating multiple hypotheses can slightly improve the performance even with a uniform target. Nonetheless, the gain is noticeably smaller than matching with our proposed local prior, which can be seen by comparing the ``(ASR, Imp, 4)'' rows, where the only difference is the target distribution to match.
\end{enumerate*}

\begin{table}[ht]
    \small
    \caption{Comparison with various knowledge distillation configurations. Each configuration is parameterized by (ASR/ASR+LM, Fix/Imp, ), where \emph{ASR/ASR+LM} indicates whether hypotheses are generated from ASR-only decoding or ASR+LM shallow fusion decoding, \emph{Fix/Imp} indicates whether the teacher model is fixed or improving (using our proposed off-policy update strategy), and  indicates the number of hypotheses used for each utterance.}
    \label{tab:kd}
    \begin{center}
    \begin{tabular}{llc}
      \toprule
      Method & Param & dev-clean / dev-other WER \\
      \midrule
      \multirow{9}{*}{KD}
      & (ASR+LM,Fix,1) &  9.60 / 29.00 \\
      \cmidrule{2-3}
& (ASR,Fix,1)   &  12.24 / 33.25 \\
      & (ASR,Fix,2)   &  11.94 / 32.19 \\
      & (ASR,Fix,4)   &  11.60 / 32.26 \\
      & (ASR,Fix,8)   &  11.77 / 32.07 \\
      \cmidrule{2-3}
& (ASR,Imp,1)   &  11.79 / 30.09 \\
      & (ASR,Imp,2)   &  11.79 / 29.89 \\
      & (ASR,Imp,4)   &  12.09 / 30.21 \\
      & (ASR,Imp,8)   &  12.19 / 29.88 \\
      \midrule 
      LPM & (ASR,Imp,4) &   9.00 / 26.47\\
      \bottomrule
    \end{tabular}
    \end{center}
\end{table}



\subsection{Final Results and Comparison to Prior Work}
The best performing model is trained for 3.2M steps, with a learning rate annealed by a factor of two every 1.28M steps when using 360 hours of unpaired speech, and every 0.64M steps when using 860 hours of unpaired speech.
Reference lengths for filtering are obtained from ASR+LM beam search decoding.
We compare LPM to fully supervised models and a number of semi-supervised ASR techniques in Table~\ref{tab:main}.
Among the listed studies, pseudo labeling (PL) is the most comparable alternative as we follow the same experimental setup, and, more importantly, it achieved the previous state-of-the-art results on LibriSpeech when using train-clean-100 as paired data and train-other-360 as unpaired speech. 
We give a more detailed table comparing to prior work in the Supplemental Material.

The upper half of Table~\ref{tab:main} shows greedy decoding results without an LM. 
For the fully supervised model, when removing train-clean-360 the WER increases by 6.86\% on test-clean and 13.36\% on test-other.
Using train-clean-360 speech without transcripts, LPM reduces the absolute WER by 5.64\% and 12.21\% on the two test sets, which recovers 82\% and 91\%, respectively, of the WER drop from removing the labels.
Adding noisier train-other-500 to the unpaired set (total 860hr ) further reduces the WER, and LPM achieves a better WER on the noisy sets (dev-other and test-other) compared to the supervised model trained on 460 hours of clean paired data.
In addition, LPM outperforms PL in all settings.
This trend is consistent even when decoding with a strong ConvLM.

The last row in the upper and lower halves of Table~\ref{tab:main} show the results of using LPM on the 60k hours of unlabelled speech from Libri-Light. We see that the WER improves by another 15\% and 9\% relative over using the 860hr dataset on the clean and other test sets respectively. When training on the 60k hours we use a batch size of 128, a beam size of  and no learning rate decay. Reference lengths are from ASR-greedy decoding and we filter empty transcriptions, yielding 55.8k hours of training data. We also use a larger TDS model with the same architecture as~\citet{hannun2019sequence} (11 TDS blocks instead of 9). To disentangle the effect of the larger model from more unlabelled data with LPM, we also trained the larger model on the 860hr dataset. In this case, we did not see a gain in WER, suggesting that the improvement is due to LPM with more unlabelled data.

\begin{table*}[ht]
    \small
    \caption{Results of baselines and the proposed methods. Word error rate recovered (WERR) measures the improvment relative to the gap between supervised model trained on 100hr and 460hr of data, computed as . "-" means the number is not available, or the supervised results are not provided and hence WERR cannot be computed.}
    \label{tab:main}
    \begin{center}
    \resizebox{\linewidth}{!}{\begin{tabular}{llllcccccc}
        \toprule
        & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{LM}
        & \multicolumn{2}{c}{dev WER (\%)} & \multicolumn{2}{c}{test WER (\%)}
        & \multicolumn{2}{c}{test WERR (\%)}\\
        & & & & clean & other & clean & other \\
        \midrule
BT~\citep{hayashi2018back} & 100hr & 360hr (T) & None & 23.5 & - & 23.6 & - & 11.9 & - \\
Crit-LM~\citep{liu2019adversarial} & 100hr & 360hr (T) & None & 19.1 & - & 19.2 & - & - & - \\
Cycle-TTE~\citep{hori2019cycle} & 100hr & 360hr (S) & None & 21.5 & - & 21.5 & - & 27.6 & - \\
Cycle-TTS~\citep{baskar2019self} & 100hr & 360hr (S) & None & - & - & 17.9 & - & - & - \\
Cycle-TTS~\citep{baskar2019self} & 100hr & 360hr (S+T) & None & - & - & 17.5 & - & - & - \\
PL (ASR)~\citep{kahn2019self} & 100hr & 360hr (S)  & None & 12.27 & 33.42 & 12.57 & 35.36 & 33.24 & 34.36 \\
PL (ASR+LM)~\citep{kahn2019self} & 100hr & 360hr (S) + All (T) & None &  9.30 & 28.79 & 9.84 & 30.15 & 73.03 & 73.35 \\
PL (ASR+LM)~\citep{kahn2019self} & 100hr & 860hr (S) + All (T) & None &  9.03 & 26.03 & 9.44 & 27.25 & 78.86 & 95.06\\ 
\cmidrule{2-10}
Supervised & 100hr & N/A & None & 14.00 & 37.02 & 14.85 & 39.95 & 0.00 & 0.00 \\
Supervised & 460hr & N/A & None &  7.20 & 25.32 &  7.99 & 26.59 & 100.00 & 100.00 \\
Local Prior Matching & 100hr & 360hr (S) + All (T) & None &  8.85 & 26.33 & 9.21 & 27.74 & 82.22 & 91.39 \\
Local Prior Matching & 100hr & 860hr (S) + All (T) & None &  \hl{8.08} & \hl{21.52} & \hl{8.37} & \hl{22.89} & \hl{94.45} & \hl{132.19} \\
Local Prior Matching (large model)  & 100hr & 60,000hr (S) + All (T) & None & \bf{6.87} & \bf{19.92} & \bf{7.19} & \bf{20.84} & \bf{111.66} & \bf{143.04} \\
\midrule
PL (ASR)~\citep{kahn2019self} & 100hr & 360hr (S) & ConvLM & 6.19 & 23.53 & 6.81 & 24.99 & 32.64 & 41.66 \\
PL (ASR+LM)~\citep{kahn2019self}  & 100hr & 360hr (S) + All (T) & ConvLM & 5.73 & 22.54 & 6.35 & 24.13 & 44.65 & 48.24 \\
PL (ASR+LM)~\citep{kahn2019self}  & 100hr & 860hr (S) + All (T) & ConvLM & 6.31 & 21.87 & 6.84 & 23.29 & 31.85 & 54.66 \\
\cmidrule{2-10}
Supervised & 100hr & N/A & ConvLM & 7.78 & 28.15 & 8.06 & 30.44 & 0.00 & 0.00 \\
Supervised & 460hr & N/A & ConvLM & 3.98 & 17.00 & 4.23 & 17.36 & 100.00 & 100.00 \\
Local Prior Matching & 100hr & 360hr (S) + All (T) & ConvLM &  5.69 & 20.22 & 5.99 & 20.93 & 54.05 & 72.71 \\
Local Prior Matching & 100hr & 860hr (S) + All (T) & ConvLM & \hl{5.39} & \hl{14.89} & \hl{5.78} & \hl{16.27} &
\hl{59.53} & \hl{108.33} \\ 
Local Prior Matching (large model)  & 100hr & 60,000hr (S) + All (T) & ConvLM & \bf{4.87} & \bf{13.84} & \bf{4.88} & \bf{15.28} & \bf{83.03} & \bf{115.90} \\
        \bottomrule
    \end{tabular}
    }
    \end{center}
\end{table*}