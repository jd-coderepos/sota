





\RequirePackage{fix-cm}
\documentclass[smallextended]{svjour3}
\smartqed  \journalname{Cognitive Computation}

\usepackage[immediate]{silence}
\WarningFilter{caption}{Unknown document class (or package),}
\hfuzz=5pt
\vfuzz=3.5pt \hbadness=8000
\vbadness=8000

\usepackage{hyperref}
\usepackage{natbib}\let\cite\citep
\usepackage{gb4e}
\noautomath
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

\newcounter{chapter} \usepackage[capitalize]{cleveref}

\usepackage{amssymb}
\usepackage{multirow,multicol}
\usepackage{graphicx}
\graphicspath{{figs/}}
\providecommand{\UrlFont}{\ttfamily\small}
\usepackage{array}
\usepackage{soul,color}
    
    \usepackage{booktabs}


\setlength\marginparwidth{2cm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{tipa}
\usepackage{stackengine}
\usepackage{rotating}
\usepackage{tabularx}




\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}
\DeclareMathOperator*{\argmax}{\text{argmax}}
\newcommand*\rot{\rotatebox{90}}


\definecolor{gelbukh}{rgb}{.9,1,.9} 
\newcommand\emo[1]{\textsc{#1}}
\newcommand\code[1]{\texttt{#1}}
\newcommand\ToDeleteIfNeedSpace{}

\newcommand\RECCON{recognizing emotion cause in conversations} \newcommand\ECRIC{ECRIC}
\newcommand\RECCONDA{RECCON}
\newcommand\RECCONDADD{RECCON-DD}
\newcommand\RECCONDAIE{RECCON-IE}
\newcommand\DailyDialog{Daily\-Dialog}
\newcommand\0{\hphantom{0}}

\begin{document}

\title{Recognizing Emotion Cause in Conversations\thanks{S. Poria, N. Majumder, D. Ghosal, R. Bhardwaj, S. Yu Bai Jian, and P. Hong have received support from the  A*STAR under its RIE 2020 Advanced Manufacturing and Engineering programmatic grant, Award No.~A19E2b0098.
A. Gelbukh has received support from the Mexican Government through the grant A1-S-47854 of the CONACYT, Mexico, and grants 20211784, 20211884, and 20211178 of the Secretar\'{\i}a de Investigaci\'on y Posgrado of the Instituto Polit\'ecnico Nacional, Mexico.
}}


\author{Soujanya Poria
\and Navonil Majumder
\and Devamanyu Hazarika
\and Deepanway Ghosal
\and Rishabh Bhardwaj
\and Samson Yu Bai Jian
\and Pengfei Hong
\and Romila Ghosh
\and Abhinaba Roy
\and Niyati Chhaya
\and Alexander Gelbukh\thanks{Corresponding author: Alexander Gelbukh} 
\and Rada Mihalcea}



\institute{S. Poria, N. Majumder, D. Ghosal, R. Bhardwaj, S. Yu Bai Jian, and P. Hong \at
Singapore University of Technology and Design, Singapore \\
\email{sporia@sutd.edu.sg, navonil\_majumder@sutd.edu.sg, deepanway\_ghosal@mymail.sutd.edu.sg, rishabh\_bhardwaj@mymail.sutd.edu.sg, samson\_yu@sutd.edu.sg, pengfei\_hong@mymail.sutd.edu.sg}
\and
D. Hazarika \at
National University of Singapore, Singapore \\
\email{hazarika@comp.nus.edu.sg}
\and
R. Ghosh \at
Independent researcher, India \\
\email{romila.ghosh93@gmail.com}
\and
A. Roy \at
Nanyang Technological University, Singapore \\
\email{abhinaba.roy@ntu.edu.sg}
\and
N. Chhaya \at
Adobe Research, India \\
\email{nchhaya@adobe.com}
\and
A. Gelbukh (corresponding author) \at
CIC, Instituto Polit\'ecnico Nacional, Mexico \\
\email{gelbukh@gelbukh.com}
\and
R. Mihalcea \at
University of Michigan, USA \\
\email{mihalcea@umich.edu}
}









\maketitle



\begin{abstract}
We address the problem of recognizing emotion cause in conversations, define two novel sub-tasks of this problem, and provide a corresponding dialogue-level dataset, along with strong Transformer-based baselines.
The dataset is available at \url{https://github.com/declare-lab/RECCON}.

\paragraph{Introduction}
Recognizing the cause behind emotions in text is a fundamental yet under-explored area of research in NLP. Advances in this area hold the potential to improve interpretability and performance in affect-based models. Identifying emotion causes at the utterance level in conversations is particularly challenging due to the intermingling dynamics among the interlocutors. 

\paragraph{Method}
We introduce the task of Recognizing Emotion Cause in CONversations with an accompanying dataset named \RECCONDA, containing over 1,000 dialogues and 10,000 utterance cause-effect pairs. Furthermore, we define different cause types based on the source of the causes, and establish strong Transformer-based baselines to address two different sub-tasks on this dataset: causal span extraction and causal emotion entailment.

\paragraph{Result}
Our Transformer-based baselines, which leverage contextual pre-trained embeddings, such as RoBERTa, 
outperform
the state-of-the-art emotion cause extraction approaches
on our dataset.

\paragraph{Conclusion}
We introduce a new task highly relevant for (explainable) emotion-aware artificial intelligence: recognizing emotion cause in conversations, provide a new highly challenging publicly available dialogue-level dataset for this task, and give strong baseline results on this dataset.
\end{abstract}


\section{Introduction}











Emotions are intrinsic to humans; consequently,
emotion understanding is a key part of human-like artificial
intelligence (AI). Language is
often indicative of one's emotions. Hence, emotion recognition has attracted much attention 
in the field of natural language processing (NLP)~\citep{kratzwald2018decision, colneric2018emotion} due to its wide range of applications in
opinion mining, recommender systems, healthcare, and other areas.

In particular, emotions are an integral part of human cognition; thus understanding human emotions and reasoning about them is one the key issues in computational modeling of human cognitive processes~\citep{Izard1992}. Among different settings where human emotions play important cognitive role is human-human and human-computer conversations. Similarly, among different issues in automatic reasoning about human emotions is identifying the causal root of the expressed emotions in the discourse of such a conversation.
During a dialog, cognitive and affective processes can be triggered by non-verbal external events or sensory input. Sometimes such affective processes can happen even before the corresponding cognitive processing by the person---a phenomenon called \emph{affective primacy}~\citep{Zajonc80feelingand}. On the other hand, complex cognitive processing, which would lead to updating the computational model's speaker state, can also happen before or after the affective reaction of the participant of the conversation. 


Substantial progress has been made in the detection and classification of emotions, expressed in text or videos, according to emotion taxonomies~\cite{ekman1993facial,plutchik}. However, further reasoning about emotions, such as  
understanding the cause of an emotion expressed by a speaker,
has been less explored so far.
For example, 
understanding
the following review of a smartphone, ``\textit{I hate the touchscreen as it freezes after 2-3 touches}'',
implies not only detecting the expressed negative emotion, specifically \emo{disgust}, but also spotting its cause~\citep{liu2012sentiment}---in this case, ``\textit{it freezes after 2-3 touches}.''






Of a wide spectrum of emotion-reasoning tasks~\cite{ellsworth2003appraisal},
in this work we focus on identifying the causes (also called antecedents, triggers, or stimuli) of emotions expressed specifically in conversations. In particular, we look for events, situations, opinions, or experiences in the conversational context that are primarily responsible for an elicited emotion in the target utterance. Apart from event mentions, the cause could also be a speaker's counterpart reacting towards an event cared for by the speaker (inter-personal emotional influence). 

We introduce the task of \textbf{\underline{r}ecognizing \underline{e}motion \underline{c}ause in \underline{con}\-ver\-sa\-tions} (RECCON), which refers to the extraction of such stimuli behind an emotion in a conversational utterance. The cause could be present in the same or contextual utterances.
We formally define this task in~\Cref{sec:annot}.

\begin{figure}[t] 
    \centering 
    \includegraphics[width=60ex]{examples.png}
    \caption{Emotion causes in conversations.}
    \label{fig:examples}
\end{figure}

In~\cref{fig:examples} we exemplify this task. In the first example, we want to know the cause of person B's () emotion (\emo{happy}). It can be seen that  is happy due to the event ``\textit{getting married}'' and similarly  also reacts positively to this event. Here, we can infer that 's emotion is caused either by the reference of the first utterance to the event of getting married or by the fact that  is happy about getting married---both of which can be considered as stimulus for 's emotion. In the second example, the cause of 's emotion is the event ``\textit{football match}" and a negative emotion \emo{disgust} indicates that  is unsatisfied 
with
the match. In contrast,  
likes
the match---sharing the same cause with ---with \emo{happiness} emotion. These examples demonstrate the challenging problem of recognizing emotion causes in conversations, which to the best of our knowledge, is one of the first attempts in this area of research.
































We can summarize our contributions as follows:



\begin{enumerate}

    \item We introduce a new task, \textbf{\RECCON{}}, and dive into many unique characteristics of this task that is peculiar to conversations. 
In particular, we define the relevant types of emotion causes (\cref{sec:types}). 
     
     \item 
     Further, 
     we describe a new annotated dataset for this task, \RECCONDA{}\footnote{pronounced as \textit{reckon}.}, including both acted and real-world dyadic conversations (\cref{sec:dataset}). To the best of our knowledge, there is no other dataset for the task of emotion cause recognition \emph{in conversations}.
     
     \item 
Finally, 
     we introduce two challenging sub-tasks that demand complex reasoning, and 
setup strong baselines to solve the sub-tasks (\cref{sec:experiments}). These baselines surpass the performance of several newly introduced complex neural approaches, e.g., ECPE-MLL~\cite{DBLP:conf/emnlp/DingXY20}, RankCP~\cite{wei-etal-2020-effective},  and ECPE-2D~\cite{DBLP:conf/acl/DingXY20}.
\end{enumerate}

\section{Related Work} \label{sec:related_works}

Initial works on emotion analysis were applied to the opinion mining task, exploring different aspects of affect beyond polarity prediction, such as identifying the opinion~/~emotion feeler (holder, source)~\cite{das-bandyopadhyay-2010-finding,DBLP:conf/naacl/ChoiCRP05}.
More recently, sentiment analysis research has been used in a wider context of natural language understanding and reasoning~\citep{OntoSenticNet2}.
New methods for multi-label emotion classification are developed~\cite{Iqra} and new corpora for emotion detection are compiled for languages other than English~\cite{LiSSS}.

The task of emotion cause extraction was 
studied initially by~\citet{lee-etal-2010-text}. 
The early works 
used 
rule-based
approaches~\cite{chen-etal-2010-emotion}. \citet{gui2016event} constructed an emotion cause extraction dataset by identifying events that trigger emotions. They used news articles as their source for the corpus to avoid the latent emotions and implicit emotion causes associated with the informal text, thus reducing reasoning complexity for the annotators while extracting emotion causes. Other notable works on emotion cause extraction (ECE) are \citep{DBLP:conf/cicling/GhaziIS15} and \citep{gao2017overview}.





As a modification of the ECE task, \citet{DBLP:conf/acl/XiaD19} proposed emotion-cause pair extraction (ECPE) that jointly identifies both emotions and their corresponding causes~\cite{DBLP:conf/emnlp/ChenHCL18}. Further, \citet{chen-etal-2020-conditional} recently proposed the conditional emotion cause pair (ECP) identification task, where they highlighted the causal relationship to be valid only in particular contexts. We incorporate this property in our dataset construction, as we annotate multiple spans in the conversational history that \textit{sufficiently} indicate the cause. Similar to~\citet{chen-etal-2020-conditional}, we also provide negative examples of context that does not contain the causal span. 

Our work is a natural extension of those works. We propose a new dataset on dyadic conversations, which is more difficult to annotate. Additionally, the associated task of recognizing emotion cause in conversations poses a greater hitch to solve due to numerous challenges. For example, \textit{}~expressed emotions are not always explicit in the conversations; \textit{(}~conversations can be very informal where the phrase connecting emotion with its cause can often be implicit and thus needs to be inferred; \textit{}~the stimuli of the elicited emotions can be located far from the target utterance in the conversation history, so that detecting it requires complex reasoning and co-reference, often using commonsense.






























\section{Definition of the Task}
\label{sec:terminology}

We distinguish between emotion \textbf{evidence} and emotion \textbf{cause}:
\begin{itemize}[itemsep=0ex,leftmargin=*]
    \item \textit{Emotion evidence} is a part of the text that indicates the presence of an emotion in the speaker's emotional state. It acts in the real world between the text and the reader. 
Identifying and interpreting the emotion evidence is the underlying process of the well-known emotion detection task. 
    
    \item \textit{Emotion cause} is a part of the text expressing the reason for the speaker to feel the emotion given by the emotion evidence. It acts in the 
described world
    between the (described) circumstances and the (described) speaker's emotional state. Identifying the emotion cause constitutes the task we consider in this~paper.
\end{itemize}
For instance, in \cref{fig:examples}, 's turn contains evidence of 's emotion, while 's turn contains its cause.
The same text span can be both emotion evidence and cause, but generally this is not the~case.

Defining the notion of emotion cause is, in a way, the main goal of this paper. However, short of a formal definition, we will explain this notion on numerous examples and, in computational terms, via the labeled dataset. 


We use the following terminology throughout the paper. 
The \textbf{target utterance}  is the  utterance of a conversation, whose emotion label  is known and whose emotion cause we want to identify.
The \textbf{conversational history}  of the utterance  is the set of all utterances from the beginning of the conversation till the utterance , including .
A \textbf{causal span} for an utterance  is a maximal sub-string, of an utterance from , that is a part of 's emotion cause; we will denote the set of the causal spans for an utterance  by .
A \textbf{causal utterance} is an utterance containing a causal span; we denote the set of all causal utterances for  by .
An \textbf{\underline utterance--\underline causal \underline span (UCS) pair} is a pair , where  is an utterance and .

Thus, \textbf{recognizing emotion cause} is the task of identifying all (correct) UCS pairs in a given~text.

In the context of our training procedure, we will refer to (correct) UCS pairs as \textbf{positive 
examples}, whereas pairs  with  are  \textbf{negative
examples}.
In \cref{sec:neg}, we describe the sampling strategies for negative examples.







\section{Building the \RECCONDA{} dataset}\label{sec:dataset}

\subsection{Emotional Dialogue Sources}
We consider two popular conversation datasets \textbf{IEMOCAP}~\cite{iemocap} and \textbf{\DailyDialog{}}~\cite{li2017DailyDialog}, both equipped with utterance-level emotion labels:



\begin{description}
\item[\textbf{IEMOCAP}] is a dataset of two-person conversations in English annotated with six emotion classes: \emo{anger}, \emo{excited}, \emo{frustrated}, \emo{happy}, \emo{neutral}, 
\emo{sad}. The dialogues in this dataset span across sixteen 
conversational situations. To avoid redundancy, we handpicked only one dialogue from each of these situations. We denote the subset of our 
dataset comprising these dialogues as 
\RECCONDAIE{}.

\item[\textbf{\DailyDialog{}}] is an English-language natural human communication dataset covering various topics on our daily lives. All utterances are labeled with emotion categories: \emo{anger}, \emo{disgust}, \emo{fear}, \emo{happy}, \emo{neutral}, \emo{sad}, 
\emo{surprise}. Since the dataset is skewed 
( \emo{neutral} labels),
we randomly selected dialogues 
with
at least four non-\emo{neutral} utterances. We denote the subset of \RECCONDA{} comprising these dialogues from \DailyDialog{} as \RECCONDADD. Some statistics about the annotated dataset is shown in \cref{tab:stat}.
\end{description}
Thus our RECCON dataset consists of two parts, \RECCONDAIE{} and \RECCONDADD{}. In particular, the label sets are slightly different in these two parts, as explained above.
\paragraph{Why
sampling from two 
datasets} 
\label{sec:dataset_diffs}
Although both IEMOCAP and \DailyDialog{} are annotated with utterance-level emotions, they differ in many aspects. First, 
IEMOCAP has more than  utterances per dialogue on average,
whereas \DailyDialog{} has 
only~ on average. Second, the shifts between non-neutral emotions (e.g., \emo{sad} to \emo{anger}, \emo{happy} to \emo{excited}) are more frequent in IEMOCAP than in \DailyDialog{}; see \citep{ghosal2020utterancelevel}. Consequently, both cause detection and causal reasoning in IEMOCAP are more interesting as well as difficult. Lastly, in \cref{tab:stat}, we can see that in our annotated IEMOCAP split, almost 40.5\% of utterances have their emotion cause in utterances at least  timestamps distant in the contextual history. 
In contrast,
this percentage is just  in our annotated \DailyDialog{} dataset. 

\subsection{Annotation Process}\label{sec:annot}


\paragraph{Annotators}
The annotators were undergraduate and graduate computer science students. They had adequate knowledge about the problem of emotion cause recognition; in particular, we organized a special workshop to instruct them on the topic. Their annotations were first verified on a trial dataset, and feedback was provided to them to correct their mistakes. Once they achieved satisfactory performance on the trial dataset, they were qualified for the main dataset annotation. While the annotators were not native English speakers, they communicate in English in their daily life, and their medium of instruction in their study was English.


\paragraph{Annotation guidelines}
Given an utterance  labeled with an emotion , the annotators were asked to extract the set of causal spans  
that sufficiently represent the causes of the emotion~. If the cause of  was latent, i.e., there was no explicit causal span in the dialog,
the annotators wrote down the assumed causes that they inferred from the text. Each utterance was annotated by two human experts---graduate students with reasonable knowledge of the task.

In fact, the annotators were asked to look for the casual spans of  in the whole dialog and not only in the past history . We show 
a
case in \cref{fig:latent_cause} where the causal span of the emotion \emo{fear} in utterance~1 is recognized in utterance~3: ``someone is stalking me''.
However, 
there were
only seven instances of the utterances with explicit emotion causal spans 
in the conversational future with respect to  in the whole dataset.
So
we discarded those spans and decided to consider only causal spans in ; hence the definition in~\cref{sec:terminology}.

\paragraph{Emotional expression}
An utterance can contain \textit{}~a description of the triggers or stimuli of the expressed emotion, and~/~or \textit{}~a reactionary emotional expression. 
In our setup, by following the discrimination among emotion evidence and cause as explained in~\cref{sec:terminology}, we instructed the annotators to look beyond just emotional expressions and 
identify the 
emotion cause. We can illustrate this with
\cref{fig:causevsexpression}, where  explains the cause for \emo{happiness}; the same cause evokes the emotion \emo{excited} in . Meanwhile, the utterance  by  is merely an emotional expression (evidence).


\ToDeleteIfNeedSpace Emotion cause can also corroborate in generating an emotional expression, e.g., in \cref{fig:causevsexpression}, the event ``\textit{winning the prize}'' causes \emo{excited} emotion in  which directs  to utter the expression ``\textit{Wow! Incredible}''. This type of generative reasoning will be very important in our future work.

\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{60ex}
         \centering
         \includegraphics[width=\textwidth]{target_utt_cause.png}
         \caption{}
         \label{fig:emotion_in_target}
     \end{subfigure}
\3ex]
     \begin{subfigure}[b]{60ex}
         \centering
         \includegraphics[width=\textwidth]{causevsexpression.png}
         \caption{}
         \label{fig:causevsexpression}
     \end{subfigure}
     \caption{{() No context. {()} Unmentioned latent cause. {(}) Distinguishing emotion cause from emotional expressions}.}
\end{figure*}



\paragraph{Why span detection?}
First, emotion-cause extraction has historically been defined as an information extraction task of identifying spans within the emotion-bearing sentences~\cite{DBLP:conf/acl/XiaD19,DBLP:conf/cicling/GhaziIS15}. The core assumption is that such spans are good descriptors of the underlying causes of the generated emotions~\cite{talmy2000toward}. We extend this popular formalism into a multi-span framework.
Second, while recognizing emotion cause is driven by multiple controlling variables such as goal, intent, personality, we adopt this setup as these spans can often represent or allude to these controlling variables. A more elaborate setup would require explaining how the spans can be combined to form the trigger and consequently evoke the emotion (see \cref{fig:csk_exx}); we leave such emotion causal reasoning in conversations to future work.


\subsubsection{Annotation Aggregation}
Following \citet{gui2016event}, we aggregate the annotations in two stages: 
at utterance and span level.


\paragraph{Stage 1: Utterance-level aggregation}
Here, we decide whether an utterance is causal by majority voting: a third expert annotator is brought in as the tie breaker.

\begin{table}[t!]
\centering
\resizebox{1\linewidth}{!}
{\begin{tabular}{@{}l@{}c@{~~~}c@{~~~}cc@{}}
\toprule
\textbf{Dataset} & \textbf{Language} & \textbf{Source} & \textbf{Size} & \textbf{Format}\\
\midrule
\citet{DBLP:conf/ijcnlp/NeviarouskayaA13} & English    & ABBYY Lingvo dictionary  & 532& sentences\\
\citet{DBLP:conf/nlpcc/GuiYXLLZ14} & Chinese   & Chinese Weibo & 1333& sentences\\
\citet{DBLP:conf/cicling/GhaziIS15} & English    & FrameNet  & 1519  & sentences\\
\citet{gui2016event} & Chinese  & SINA city news & 2167  & clauses\\
\citet{gao2017overview} & Chinese / Eng. & SINA city news / English novel & 4054 / 4858 & clauses \\
\midrule
\multirow2*{\RECCONDA{} (our)} & \multirow2*{English} & \multirow2*{\DailyDialog{} / IEMOCAP} & 5861 / 494   & utterances \\
&& & 1106 / 16& dialogues\\
\bottomrule
\end{tabular}}
	\caption{{Datasets for emotion cause extraction and related tasks. Datasets in \cite{DBLP:conf/acl/XiaD19,chen-etal-2020-conditional} are derived from \cite{gui2016event}.}}
\label{tab:related_datasets}
\end{table}

\paragraph{Stage 2: Span-level aggregation}
Within each causal utterance selected at 
stage 1,
we took the union of the candidate spans from different annotators as the final causal span only when the size of their intersection is at least 50\% of the size of the shortest candidate span. 
Otherwise,
a third annotator was brought in to determine the final span from the existing spans. This third annotator was also instructed to prefer the shorter spans over the longer ones when they can sufficiently represent the cause without losing any information. The threshold of 50\% of the shortest span was chosen empirically by examining a small subset of the dialogues.
The third annotator could not break the tie for~34 causal utterances, which we discarded from the dataset.





\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{60ex}
         \centering
         \includegraphics[width=\textwidth]{self-contagion.png}
         \caption{Mood Setting}
         \label{fig:self-contagion}
     \end{subfigure}
\3ex]
     \begin{subfigure}[b]{60ex}
         \centering
         \includegraphics[width=\textwidth]{hybrid.png}
         \caption{Hybrid}
         \label{fig:hybrid}
     \end{subfigure}
     \caption{{(), () \textit{Self-contagion:} The cause of the emotion is primarily due to a stable mood of the speaker that was induced in the previous dialogue turns; () \textit{Hybrid:} The hybrid type 
with both
     inter-personal emotional influence and self-contagion.
}}
\end{figure*}

\begin{table}[t!]
\centering
{\begin{tabular}{lr@{~~~}r@{~~~~}r}
		\toprule
		\textbf{Number of items} & \multicolumn1c{\textbf{DD}}  & \multicolumn1c{\textbf{IE}} & \textbf{Total}\\
		\midrule
		Dialogues &  &  & 1122\\
		Utterances &  &  & 11769\\
		Utterances annotated with emotion cause  &  &  & 6355\\
Utterances that cater to background 
cause &  &  & 465\\
		Utterances where cause solely lies in the same utterance &  &  & 1601\\
		Utterances where cause solely lies in the contextual utterances &  &  & 1141\\
		Utterances where cause lies both in same and context utterances 
&  &  & 3613\\
\midrule
		UCS pairs &  &  &11069\\
		Utterances having single cause &  &  & 54\%\\
		Utterances having two causes &  &  & 31\%\\
		Utterances having three causes &  &  & 9\%\\
		Utterances having more than three causes &  &  & 6\%\\
		Causes per utterance (average) &  &  &1.73\\
\bottomrule
	\multicolumn4{@{}c@{}}{\begin{tabular}{@{}c@{}}
	    \begin{tabular}[t]{lr@{~~~}r@{~~~~}r}
	    \toprule
		\textbf{Utterances with} & \multicolumn1c{\textbf{DD}}  & \multicolumn1c{\textbf{IE}} & \textbf{Total}\\
		\midrule
		 \emo{Anger}       & 451 & 89 & 540\\
         \emo{Fear}        & 74 & -- &74\\
         \emo{Disgust}     & 140 & -- &140\\
         \emo{Frustration} &  -- & 109 &109\\
         \emo{Happy}       & 4361 & 58 &4419\\
         \emo{Sad}         & 351 & 70 &4419\\
         \emo{Surprise}    & 484 & -- &484\\
         \emo{Excited}     & -- & 197 &197\\
         \emo{Neutral}     & 5243 & 142 &5385\\  
        \bottomrule
	    \end{tabular} ~~~~ \begin{tabular}[t]{lr@{~~~}r@{~~~~}r}
	    \toprule
		\textbf{Utterances  with} & \multicolumn1c{\textbf{DD}}  & \multicolumn1c{\textbf{IE}} & \textbf{Total}\\
		\midrule
		No context &  &  &43\%\\
		Inter-personal &  &  &31\%\\
		Self-contagion &  &  &10\%\\
		Hybrid &  &  &11\%\\
		Latent &  &  &5\%\\
		\midrule
		Cause at  &  &  &3034\\
		Cause at  &  &  & 1306\\
		Cause at  &  &  &672\\
		Cause at  &  &  &969\\
		\bottomrule
	    \end{tabular}
	\end{tabular}}
	\end{tabular}}
	\caption{{Statistics of the \RECCONDA{} annotated dataset. DD stands for \RECCONDADD{}, IE for \RECCONDAIE{}.}}
	\label{tab:stat}
\end{table}

\subsection{Dataset Statistics}
In \cref{tab:related_datasets}, we compare our dataset with the existing datasets in terms of size, data sources, and language. The remaining statistics of \RECCONDA{} are consolidated in \cref{tab:stat}.

We measured 
inter-annotator agreement (IAA) at the level of ~utterance and ~span.
At the utterance level, we measured IAA following \citet{gui2016event}, which gave a kappa of 0.7928. 
However, as pointed out by \citet{brandsen-etal-2020-creating}, macro F1 score is 
more appropriate 
for span extraction-type tasks. Hence, at the utterance level, we also compute the pairwise macro F1 score between all possible pairs of annotators and then average them, which 
gives
a 0.8839 macro F1 score. \citet{brandsen-etal-2020-creating} also suggest the removing negative examples---in our case, the utterances in the conversational history containing no causal span for the emotion of the target utterance---for macro F1 calculation, since such examples are usually very frequent, which may lead to a skewed F1 score. As expected, 
this yields a lower F1 score of 0.8201. At span level, the F1 score, as explained in \citet{rajpurkar2016squad}, is calculated for all possible pairs of annotators followed by taking their average. Overall, we obtain an F1 score of 0.8035 at span level.

\section{Types of Emotion Causes}
\label{sec:types}
In our dataset, \RECCONDA{}, we observe five predominant types of emotion causes that are based on the source of the stimuli (events / situations / acts) in the conversational context, responsible for the target emotion. 
The annotators were asked to flag the utterances with latent emotion cause or emotion cause of type 
shown in \cref{fig:latent_cause} (unmentioned latent cause),
as explained below.
The distribution of these cause types is given in \cref{tab:stat}.



\paragraph{Type 1: No Context} The cause is present within the target utterance itself. The speaker feeling the emotion explicitly mentions its cause in the target utterance (see~\cref{fig:emotion_in_target}).
 


\paragraph{Type 2: Inter-Personal Emotional Influence}
The emotion cause is present in the other speaker's utterances (see \cref{fig:examples}).
We observe two possible sub-types of such influences:
\begin{enumerate}[itemsep=0ex, leftmargin=*, label=2\alph*)]
    \item \textbf{Trigger Events / Situations.} The emotion cause lies within an event or concept mentioned by the other speaker.
    \item \textbf{Emotional Dependency.} The emotion of the target speaker is induced from the emotion of the other speaker over some event / situation.
\end{enumerate}

\paragraph{Type 3: Self-Contagion}
In many cases, 
the cause of the emotion is primarily due to a stable mood of the speaker that was induced in 
previous dialogue turns. 
E.g.,
in a dialogue involving cordial greetings, there is a tendency for a \emo{happy} mood to persist across several turns for a speaker. \cref{fig:self-contagion} presents an example where such self-influences can be observed. Utterance  establishes that  likes winter. This concept triggers a \emo{happy} mood for the future utterances, as observed in utterances~3 and~5. In \cref{fig:generic_cause}, similarly, the trigger of emotion \emo{excited} in utterance~3 is mentioned by the same speaker in his or her previous utterance.

\paragraph{Type 4: Hybrid} Emotion causes of type~2 and~3 can jointly cause the emotion of an utterance, as illustrated by \cref{fig:hybrid}.




\paragraph{Type 5: Unmentioned Latent Cause} There are instances in the dataset where no explicit span in the target utterance or the conversational history can be identified as the emotion cause. \cref{fig:latent_cause} shows such a case. Here, in the first utterance,  speaks of being terrified and fearful without indicating the cause. We annotate such cases as latent causes. Sometimes the cause is revealed in future utterances, e.g., ``\textit{someone is stalking me}'' as the reason of being fearful. However, as online settings would not have access to the future turns, we refrain from treating future spans as 
causes.


\section{Experiments}
\label{sec:experiments}

We formulate two distinct subtasks of \RECCON{}: \textit{}~causal span extraction and \textit{}~causal emotion entailment. However, 
note that
the main purposes of this work are to present a dataset and setup the strong baselines. 


\subsection{Compiling Dataset Splits}
\label{sec:dataprep}

\RECCONDADD{} is the subset of our dataset that contains dialogues from \DailyDialog{}. For this subset, we created the training, validation, and testing examples based on the original splits in~\cite{li2017DailyDialog}. However, this resulted in the validation and testing sets to be quite small, so we moved some dialogues to them from the original training set. 

The subset \RECCONDAIE{} consists of dialogues from the IEMOCAP dataset. This subset is quite small as it contains only sixteen unique dialogues (situations). So, we consider the entire \RECCONDAIE{}  as another testing set, emulating an out-of-distribution generalization test. We report results on this dataset based on models trained on \RECCONDADD{}. In our experiments, we ignore the utterances with only latent emotion causes. 

\subsubsection{Generating Negative Examples}
\label{sec:neg}
The annotated dataset, \RECCONDA{} (consisting of subsets \RECCONDADD{} and \RECCONDAIE{}) only contains positive examples, where an emotion-containing target utterance is annotated with a causal span extracted from its conversational historical context. However, to train a model for the \RECCON{} task, we need negative examples, i.e., the instances which are not cause of the utterance. In the sequel, we use the terminology introduced in \cref{sec:terminology}; the reader should 
refer to that section for clearer understanding.

We use three different strategies to create the negative examples.
In this section, we will discuss in detail Fold 1. Then, in \cref{sec:analysisx}, to further analyze the performance of our models, besides Fold 1, we will adopt two more strategies, Fold 2 and Fold 3, to create the negative examples:
\begin{description}
\item[\textbf{Fold 1:}] Consider a dialogue~ and a target utterance~ in~.
We construct the complete set of negative examples as 
,
where~ is the conversational history and~ is the set of causal utterances for~.
\item[\textbf{Fold 2:}] In this scheme, we randomly sample the non-causal utterance~ along with the corresponding historical conversational context~ from another dialogue in the dataset to create a negative example.
\item[\textbf{Fold 3:}] This is similar to Fold~2 with a constraint. In this case, a non-causal utterance~ along with its historical conversational context~ from the other dialogue is only sampled when its emotion matches the emotion of the target utterance~ to construct a negative example.
\end{description}
Note that unlike Fold 1, a negative example in Fold 2 and 3 comprising a non-causal utterance  and a target utterance  belong to different dialogues. For the cases where the causal spans do not lie in the target utterance, we remove the target utterance from its historical context when creating a positive example in Fold 2 and 3. As a result, it helps to prevent the models from learning any trivial patterns. The statistics for the three folds are shown in \cref{tab:finalstatx}.












\begin{table}[ht!]
\centering
{
\begin{tabular}{llrrrr}
\toprule
& &  \multicolumn1c{Data} & \multicolumn1c{Train} & \multicolumn1c{Val} & \multicolumn1c{Test} \\
\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & Positive UCS pairs & 7269 & 347 & 1894 \\
&& Negative UCS pairs & 20646 & 838 & 5330 \\
\cmidrule{2-6}
&\multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & Positive UCS pairs & \multicolumn1c{--} & \multicolumn1c{--} & 1080 \\
&& Negative UCS pairs & \multicolumn1c{--} & \multicolumn1c{--} & 11305 \\

\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 2}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & Positive UCS pairs & 7269 & 347 & 1894 \\
&& Negative UCS pairs & 18428 & 800 & 4396 \\
\cmidrule{2-6}
&\multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & Positive UCS pairs & \multicolumn1c{--} & \multicolumn1c{--} & 1080 \\
&& Negative UCS pairs & \multicolumn1c{--} & \multicolumn1c{--} & 7410 \\

\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 3}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & Positive UCS pairs & 7269 & 347 & 1894 \\
&& Negative UCS pairs & 18428 & 800 & 4396 \\
\cmidrule{2-6}
&\multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & Positive UCS pairs & \multicolumn1c{--} & \multicolumn1c{--} & 1080 \\
&& Negative UCS pairs & \multicolumn1c{--} & \multicolumn1c{--} & 7410 \\
\bottomrule
\end{tabular}
}
\caption{{The statistics of \RECCONDA{} comprising both positive (valid) and negative (invalid) UCS pairs. DD stands for \RECCONDADD{}, IE for \RECCONDAIE{}. Utterances with only latent emotion causes are ignored in our experiments.}}
\label{tab:finalstatx}
\end{table}


\subsection{Subtask 1: Causal Span Extraction}
\label{sec:cse}
\textit{Causal Span Extraction} is the task of identifying the causal span (emotion cause) for a target non-neutral utterance. In our experimental setup, we formulate \textit{Causal Span Extraction} as a Machine Reading Comprehension (MRC) task similar to the task in Stanford Question Answering Dataset~\citep{rajpurkar2016squad}. Similar MRC techniques have been used in literature for various NLP tasks such as named entity recognition~\citep{li-etal-2020-unified} and zero shot relation extraction~\citep{levy-etal-2017-zero}. In this work, we propose two different span extraction settings: \textit{}~with conversational context and \textit{}~without conversational context.


\subsubsection{Subtask Description}









\paragraph{With Conversational Context (w/ CC)}
We 
believe
that the presence of conversational context would be key to the span extraction algorithms. To evaluate this hypothesis, we design this subtask, where the conversational history is available to the model. In this setup, for a target utterance , the causal utterance , and a causal span  from , we construct the context, question, and answer as follows:\footnote{By ``causal span from evidence in the context'' we mean a causal span from the conversation history~.}
\begin{description}
\item[\textbf{Context:}] The context of a target utterance  is the conversational history, i.e., a concatenation of all utterances from .
Similarly, for a negative example , where , conversational history of  is used as context.\\
\item[\textbf{Question:}] The question is framed as follows: ``\textit{The target utterance is . The evidence utterance is . What is the causal span from evidence in the context that is relevant to the target utterance's emotion ?}".
\item[\textbf{Answer:}] The causal span  appearing in  if . For negative examples,  is assigned an empty string. 
\end{description}

If a target utterance has multiple causal utterances and causal spans, then we create separate (Context, Question, Answer) instances for them. Unanswerable questions are also created from invalid (cause, utterance) pairs following the same approaches explained in \cref{sec:dataprep}. 

\paragraph{Without Conversational Context (w/o CC)}
In this formulation, we intend to identify whether the \textit{Causal Span Extraction} task is feasible when we only have information about the target utterance and the causal utterance. Given a target utterance  with emotion label , its causal utterance 
, and the causal span , the question is framed as framed as follows: ``\textit{The target utterance is . What is the causal span from context that is relevant to the target utterance's emotion ?}''. The task is to extract answer  from context . For negative examples,  is assigned an empty string.

\subsubsection{Models}
We use 
two pretrained Transformer-based models to benchmark the \textit{Causal Span Extraction} task.


\paragraph{{RoBERTa Base}} We use the \code{roberta-base}
model~\cite{liu2019roberta} and add a linear layer on top of the hidden-states output to compute span start and end logits. Scores of candidate spans are computed following~\citet{devlin2018bert}, and the span with maximum score is selected as the answer.


\paragraph{{SpanBERT Fine-tuned on SQuAD}} We use SpanBERT~\citep{joshi2020spanbert} as the second baseline model. 
SpanBERT follows a different pre-training objective compared to RoBERTa (e.g. predicting masked contiguous spans instead of tokens) and performs better on question answering tasks. 
In this work we are using the SpanBERT base model fine-tuned on SQuAD 2.0 dataset.




\subsubsection{Evaluation Metrics}
\label{sec:metric}


We use the following evaluation metrics.
\textbf{EM (Exact Match):} EM represents, with respect to the gold standard data, how many causal spans are exactly extracted by the model.
\textbf{F1}:~This is the F1 score introduced by~\citet{rajpurkar2016squad} to evaluate predictions of extractive QA models and calculated over positive examples in the data.
\textbf{F1}:~Negative F1 represents the F1 score of detecting negative examples with respect to the gold standard data. Here, for a target utterance , the ground truth are empty spans.
\textbf{F}:~This metric is similar to F1 but calculated for every positive and negative example followed by an average over them.


\begin{table}[t]
  \centering
{\setlength{\tabcolsep}{1ex}\begin{tabular}{lllccccccccc}
    \toprule
   \multicolumn3c{\multirow3*{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{w/o CC}} && \multicolumn{4}{c}{\textbf{w/ CC}}\\
    \cmidrule{4-7}
    \cmidrule{9-12}
   & & & EM & F1 & F1 &  && EM & F1  & F1 &  \\
    \midrule
   \multirow{4}{*}{\rotatebox{90}{\textbf{{Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & RoBERTa  & 26.82 & 45.99 & \textbf{84.55} & \textbf{73.82} && 32.63 & 58.17 & 85.85 & 75.45\\
   
  &  & SpanBERT & \textbf{33.26} & \textbf{57.03} & 80.03 & 69.78 && \textbf{34.64} & \textbf{60.00} & \textbf{86.02} & \textbf{75.71} \\
    \cmidrule{2-12}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & RoBERTa  & \09.81 & 18.59 & \textbf{93.45} & \textbf{87.60} && 10.19 & 26.88 & \textbf{91.68} & \textbf{84.52}\\
    
  &  & SpanBERT & \textbf{16.20} & \textbf{30.22} & 87.15 & 77.45 && \textbf{22.41}  & \textbf{37.80} & 90.54 & 82.86 \\
 
      \midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{{}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & RoBERTa  & 37.76 & 63.87 & -- & -- && 39.02 & 69.13 & -- & --\\
   
  &  & SpanBERT & 41.96 & 72.01 & -- & -- && 42.24 & 71.91 & -- & --\\
    \cmidrule{2-12}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & RoBERTa  & 22.49 & 45.01 & -- & -- && 17.27 & 42.15 & -- & --\\
    
  &  & SpanBERT & 26.91 & 52.22 & -- & -- && 31.33 & 60.14 & -- & --\\








\bottomrule
   \end{tabular}}
\caption{{Results for Causal Span Extraction task on the test sets of \RECCONDADD{} and \RECCONDAIE{}. All scores are in percentage and are reported at best validation F1 scores. DD 
stands for
  \RECCONDADD{}, IE 
for \RECCONDAIE{}, 
  RoBERTa 
for
  RoBERTa Base. For definition of Fold 1, see \cref{sec:neg}.}}
  \label{tab:cse}
\end{table}

While all 
these
metrics are important for evaluation, we stress that future works should 
particularly consider performances for EM, F1, and F.






\begin{table}[t]
  \centering
{
\begin{tabular}{@{}lllccccccc@{}}
    \toprule
      \multicolumn{3}{c}{\multirow3*{\textbf{Model}}} & \multicolumn{3}{c}{\textbf{w/o CC}} && \multicolumn{3}{c}{\textbf{w/ CC}}\\
    \cmidrule{4-6}
    \cmidrule{8-10}
    && & Pos. F1 & Neg. F1 & macro F1 && Pos. F1 & Neg. F1 & macro F1\\
    
    
    \midrule
    \multirow{10}{*}{\rotatebox{90}{\textbf{{Fold 1}}}} & \multirow{5}{*}{\rotatebox{90}{\textbf{{DD}}}} &  Base & \textbf{56.64} & 85.13 & \textbf{70.88} && 64.28 & 88.74 & 76.51 \\
   & &  Large & 50.48 & \textbf{87.35} & 68.91 && \textbf{66.23} & 87.89 & \textbf{77.06} \\
   & &  ECPE-MLL & -- & -- & -- && 48.48 & 94.68 & 71.59 \\
   & &  ECPE-2D & -- & -- & -- && 55.50 & 94.96 & 75.23 \\
   & &  RankCP & -- & -- & -- && 33.00 & \textbf{97.30} & 65.15 \\
\cmidrule{2-10}
   & \multirow{5}{*}{\rotatebox{90}{\textbf{{IE}}}} &  Base & 25.98 & 90.73 & 58.36 && 28.02 & 95.67 & 61.85\\
  &  &  Large & \textbf{32.34} & \textbf{95.61} & \textbf{63.97} && \textbf{40.83} & \textbf{95.68} & \textbf{68.26} \\
     & &  ECPE-MLL & -- & -- & -- && 20.23 & 93.55 & 57.65 \\
   & &  ECPE-2D & -- & -- & -- && 28.67 & 97.39 & 63.03 \\
    & &  RankCP & -- & -- & -- && 15.12 & 92.24 & 54.75 \\
  
    \midrule
    \multirow{10}{*}{\rotatebox{90}{\textbf{{}}}} & \multirow{5}{*}{\rotatebox{90}{\textbf{{DD}}}} &  Base & 93.12 & -- & -- && 92.64 & -- & -- \\
   & &  Large & \textbf{98.87} & -- & -- && \textbf{97.78} & -- & -- \\
& &  ECPE-MLL & -- & -- & -- && 84.50 & -- & -- \\
   & &  ECPE-2D & -- & -- & -- && 88.13 & -- & -- \\
   & &  RankCP & -- & -- & -- && 85.67 & -- & -- \\
    \cmidrule{2-10}
   & \multirow{5}{*}{\rotatebox{90}{\textbf{{IE}}}} &  Base & 71.98 & -- & -- && 58.52 & -- & -- \\
  &  &  Large & \textbf{73.92} & -- & -- && \textbf{74.56} & -- & -- \\
     & &  ECPE-MLL & -- & -- & -- && 66.45 & -- & -- \\
   & &  ECPE-2D & -- & -- & -- && 64.33 & -- & -- \\
   & &  RankCP & -- & -- & -- && 70.21 & -- & -- \\


    
\bottomrule
   \end{tabular}}
  \caption{{Results for Causal Emotion Entailment task on the test sets of \RECCONDADD{} and \RECCONDAIE{}. Class-wise F1 score and the overall macro F1 scores are reported. All scores reported at best macro F1 scores. All models are RoBERTa-based. The cause-pivot emotion extraction setting was used for ECPE-MLL. DD stands for RECCON-DD, IE for RECCON-IE.}}
  \label{tab:cus}
\end{table}



\subsection{Subtask 2: Causal Emotion Entailment}
\label{sec:cus}

The \textit{Causal Emotion Entailment} is a simpler version of the span extraction task. In this task, given a target non-neutral utterance (), the goal is to predict which particular utterances in the conversation history  are responsible for the non-neutral emotion in the target utterance. Following the earlier setup, we formulate this task with and without historical conversational context.

\subsubsection{Subtask Description}
We consider the following two subtasks:
\paragraph{With Conversational Context (w/ CC)}
We consider the historical conversational context  of the target utterance  and posit the problem as a triplet classification task: 
the tuple  is aimed to be classified as positive, . For negative examples, the tuple  should be classified as negative for .

\paragraph{Without Conversational Context (w/o CC)}
We posit this problem as a binary sentence pair classification task, where (, ) should be classified as positive as . For the negative example (, ) where , the classification output should be negative.



\subsubsection{Models}
In this paper we consider the following models.
\paragraph{RoBERTa Base and Large}
Similar to subtask 1, we use Transformer-based models to benchmark this task. We use a \code{<CLS>} token and the emotion label  of the target utterance  in front, and join the pair or triplet elements with \code{<SEP>} in between to create the input. 
The classification is performed from the corresponding final layer vector of the \code{<CLS>} token. 
We use the \code{roberta-base/-large} models from~\citep{liu2019roberta} as the baselines.


\paragraph{ECPE-2D}
\citet{DBLP:conf/acl/DingXY20} proposed an end-to-end approach for emotion cause pair extraction. They use a 2D Transformer network to improve interaction among the utterances.

\paragraph{ECPE-MLL}
\citet{DBLP:conf/emnlp/DingXY20} introduced a joint multi-label approach for emotion cause pair extraction. Specifically, the joint framework comprises two modules:  extraction of causal utterances for the target emotion utterance,  extraction of emotion utterance for a causal utterance. Both these modules were trained using a multi-label training scheme. 

\paragraph{RankCP}
\citet{wei-etal-2020-effective} proposed an end-to-end emotion cause pair extraction where first the utterance pairs are ranked and then a one-stage neural approach is applied for inter-utterance correlation modeling that enhances the emotion cause extraction. Specifically, they apply graph attentions to model the interrelations between the
utterances in a dialogue.
ECPE-2D, ECPE-MLL, and RankCP use RoBERTa-base as a sentence encoder in our implementation to facilitate a fair comparison.

\subsubsection{Evaluation Metrics}
We use F1 score for both positive and negative examples, denoted as Pos. F1 and Neg. F1 respectively. We also report the overall macro F1.

\subsection{Results and Discussions}
\label{sec:results}
\cref{tab:cse} shows the 
results of the causal span extraction task where SpanBERT obtains the best performance in both \RECCONDADD{} and \RECCONDAIE{}. SpanBERT outperforms RoBERTa Base in EM, and F1 metrics. However, the performance of SpanBERT is worse for negative examples, which consequently results in a lower F1 score compared to RoBERTa Base model in both the datasets under ``w/o CC" setting. Contrary to this, the performance of the SpanBERT in the presence of context (w/ CC) is consistently higher than RoBERTa Base with respect to all the metrics in \RECCONDADD{}.


In \cref{tab:cus}, we report the performance of the Causal Emotion Entailment task. Under the ``w/o CC'' setting, in Fold , RoBERTa Base outperforms RoBERTa Large by \% in \RECCONDADD{}. In contrast to this, in \RECCONDAIE{}, RoBERTa Large performs better and beats RoBERTa Base by \% in Fold . On the other hand, RoBERTa Large outperforms RoBERTa Base in both \RECCONDADD{} and \RECCONDAIE{} under the ``w/ CC'' setting. The performance in \RECCONDAIE{} is consistently worse than in \RECCONDADD{} under various settings in both subtask 1 and 2. We reckon this can be due to multiple reasons mentioned in~\cref{sec:dataset_diffs}, making the task harder on the IEMOCAP split.



We have also analyzed the performance of the baseline models on the utterances having one or multiple causes. The models consistently perform better for the utterances having only one causal span compared to the ones having multiple causes (\% on an average calculated over all the settings and models). In the test data of Fold , approximately 38\% of the UCS pairs (which we call as  ) have their causal spans lie within the target utterances. In \cref{tab:cse} and \ref{tab:cus}, we report the results on . According to these results, the models perform significantly better on such UCS pairs under all the settings in both the subtasks. The models leverage contextual information for both the subtasks in the ``w/ CC'' setting which substantially improves the performance of the non-contextual (refer to the ``w/o CC'' setting) counterpart. In this setting, SpanBERT obtains the best performance for positive examples in both \RECCONDADD{}, and \RECCONDAIE{}. On the other hand, in the same setting, RoBERTa Large outperforms RoBERTa Base and achieves the best performance in subtask 2.


The low scores of the models in 
subtasks  and  
show
the difficulty of the tasks. 
This implies
significant room for model improvement in these 
subtasks of \RECCON{}.
\cref{tab:cus} shows
that all the complex neural baselines, i.e., ECPE-MLL, ECPE-2D, and RankCP fail to outperform the very simple RoBERTa baselines introduced in this paper. This corroborates the usefulness and importance of these strong baselines, one of the major contributions of this paper.























    


  





































































\section{Further Analysis and Discussion} \label{sec:analysisx}





















For further insights into the performance of our models, we analyzed more strategies to create the negative examples: Folds~2 and~3; see \cref{sec:neg} for their description.


\begin{table}[t!]
  \centering
{
\begin{tabular}{@{}lll@{\hspace{5ex}}cccc@{}c@{\hspace{5ex}}cccc@{}}
    \toprule
   \multicolumn{3}{c}{\multirow3*{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{w/o CC}} && \multicolumn{4}{c}{\textbf{w/ CC}}\\
   \cmidrule{4-7}\cmidrule{9-12}
& & & EM & F1 & F1 & F && EM & F1  & F1 & F \\
\midrule
   \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 1  Fold 1}~~}}} 
   & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} 
   & RoBERTa  & 26.82 & 45.99 & \textbf{84.55} & \textbf{73.82} && 32.63 & 58.17 & 85.85 & 75.45\\
   
  &  & SpanBERT & \textbf{33.26} & \textbf{57.03} & 80.03 & 69.78 && \textbf{34.64} & \textbf{60.00} & \textbf{86.02} & \textbf{75.71} \\
    \cmidrule{2-12}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & RoBERTa  & \09.81 & 18.59 & \textbf{93.45} & \textbf{87.60} && 10.19 & 26.88 & \textbf{91.68} & \textbf{84.52}\\
    
  &  & SpanBERT & \textbf{16.20} & \textbf{30.22} & 87.15 & 77.45 && \textbf{22.41}  & \textbf{37.80} & 90.54 & 82.86 \\


      \midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 1  Fold 2}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & RoBERTa  & 26.82 & 45.99 & 83.52 & 72.66 && \textbf{32.95} & \textbf{59.02} & \textbf{95.36} & \textbf{87.63} \\
    &  & SpanBERT & \textbf{33.26} & \textbf{57.03} & \textbf{84.02} & \textbf{74.80} && 32.37 & 57.04 & 95.01 & 87.00 \\
    \cmidrule{2-12}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & RoBERTa  & \09.81 & 18.59 & \textbf{92.18} & \textbf{85.41}  && 10.93 & 28.26 & 95.49 & 90.85 \\
  &  & SpanBERT & \textbf{16.20} & \textbf{30.22} & 88.63 & 79.80 && \textbf{24.07} & \textbf{40.57} & \textbf{96.28} & \textbf{92.41} \\
\midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 1  Fold 3}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & RoBERTa  & 26.82 & 45.99 & \textbf{81.50} & \textbf{70.26} && \textbf{32.95} & \textbf{59.02} & \textbf{95.37} & \textbf{87.65} \\
  &  & SpanBERT & \textbf{33.26} & \textbf{57.03} & 79.65 & 69.83 && 32.31 & 56.99 & 94.92 & 86.87 \\
    \cmidrule{2-12}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & RoBERTa  & \09.81 & 18.59 & \textbf{91.82} & \textbf{84.83} && 10.93 & 28.26 & 95.47 & 90.81 \\
  &  & SpanBERT & \textbf{16.20} & \textbf{30.22} & 86.95 & 77.25 && \textbf{24.07} & \textbf{40.57} & \textbf{96.28}  & \textbf{92.41}  \\
\midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 2  Fold 2}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & RoBERTa  & \textbf{33.26} & 58.44 & 90.14 & 82.19 && 41.61 & 73.57 & \textbf{99.98} & 92.04 \\
  &  & SpanBERT & 32.31 & \textbf{58.61} & \textbf{90.20} & \textbf{82.29} && \textbf{41.97} & \textbf{74.85} & 99.94 & \textbf{92.43} \\
    \cmidrule{2-12}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & RoBERTa  & 15.93 & 31.74 & \textbf{92.93} & \textbf{86.50} && 30.28 & 59.14 & \textbf{99.43} & 94.58 \\
  &  & SpanBERT & \textbf{22.13} & \textbf{38.84} & 90.37 & 82.49 && \textbf{32.50} & \textbf{65.45} & 98.37 & \textbf{95.50} \\
  


      \midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 2  Fold 1}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & RoBERTa  & \textbf{33.26} & 58.44 & 71.29 & 60.45 && \textbf{36.06} & \textbf{65.04} & \00.19 & \textbf{17.12} \\
    &  & SpanBERT & 32.31 & \textbf{58.61} & \textbf{72.52} & \textbf{61.70} && 31.52 & 60.81 & \textbf{\00.67} & 16.19 \\
    \cmidrule{2-12}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & RoBERTa  & 15.93 & 31.74 & \textbf{90.70} & \textbf{82.91} && \textbf{22.96} & 46.87 & \04.66 & \06.35 \\
  &  & SpanBERT & \textbf{22.13} & \textbf{38.84} & 85.03 & 74.34 && 21.85 & \textbf{49.18} & \textbf{\06.36} & \textbf{\07.40} \\
  
\midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 3  Fold 3}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & RoBERTa  & 28.72 & 51.32 & \textbf{90.06} & \textbf{82.11} && 41.29 & 74.95 & \textbf{99.94} & 92.44 \\
  &  & SpanBERT & \textbf{30.62} & \textbf{54.96} & 89.41 & 81.21 && \textbf{42.61} & \textbf{75.36} & 99.93 & \textbf{92.46} \\
    \cmidrule{2-12}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & RoBERTa  & 14.54 & 26.51 & \textbf{93.68} & \textbf{87.79} && 24.35 & 53.46 & 97.84 & 94.08 \\
  &  & SpanBERT & \textbf{17.41} & \textbf{31.75} & 91.85 & 84.86 && \textbf{32.87} & \textbf{62.70} & \textbf{99.54} & \textbf{95.11} \\
  
\midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 3  Fold 1}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} & RoBERTa  & 28.72 & 51.32 & \textbf{75.55} & 64.31 && \textbf{37.22} & \textbf{69.64} & \textbf{\00.90} & \textbf{18.59} \\
  &  & SpanBERT & \textbf{30.62} & \textbf{54.96} & 75.49 & \textbf{64.46} && 31.94 & 60.81 & \00.15 & 16.00\\
    \cmidrule{2-12}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} & RoBERTa  & 14.54 & 26.51 & \textbf{92.33} & \textbf{85.61} && 21.20 & \textbf{48.34} & \textbf{11.42} & \textbf{\09.76}\\
  &  & SpanBERT & \textbf{17.41} & \textbf{31.75} & 89.41 & 80.94 && \textbf{21.48} & 45.49 & \04.01 & \05.84 \\
    \bottomrule
   \end{tabular}
  }
\caption{{Results for Causal Span Extraction task on the test sets of \RECCONDADD{} and \RECCONDAIE{}. All scores are in percentage and are reported at best validation F1 scores. RoBERTa stands for RoBERTa Base, DD for \RECCONDADD{}, IE for \RECCONDAIE{}. Fold   Fold  means trained on Fold , tested on Fold .}}
  \label{tab:cse2x}
\end{table}



    




    


The use of context (w/ CC) in the baseline models improves the results (see Tables~\ref{tab:cse2x} and~\ref{tab:cus2x}) in Folds~2 and~3 as it highlights the contextual discrepancy or coherence between the target utterance and context which should strongly aid in identifying randomly generated negative samples from the rest. For the positive examples, we achieve a much better score in Folds~2 and~3 as compared with Fold~1 (see Tables~\ref{tab:cse} and~\ref{tab:cus}) for both ``w/o CC" and ``w/ CC" constraints. However, this does not validate Folds~2 and~3 as better training datasets than Fold~1. We confirm this by training the models on Folds~2 and~3 and evaluating them on Fold~1. These two experiments are denoted as {Fold~2  Fold~1} and {Fold~3  Fold~1}, respectively, and the corresponding results are reported in Tables~\ref{tab:cse2x} and~\ref{tab:cus2x}. The outcomes of these experiments, as shown in Tables~\ref{tab:cse2x} and~\ref{tab:cus2x}, show abysmal performance by the baseline models on the negative examples in Fold~1.

This may be ascribed to the fundamental difference between Fold~1 and Folds~2 and~3. Negative samples in Folds~2 and~3 are easily identifiable, as compared to Fold~1, as all the model needs to do to judge the absence of a causal span in the context is to detect the contextual incoherence of the target utterance with the context. Models fine-tuned on BERT and SpanBERT are expected to perform well at deciding contextual incoherence. Identifying negative samples in Fold~1, however, requires more sophisticated and non-trivial approach as the target utterances are, just as the positive examples, contextually coherent with the context. As such, a model that correlates contextual incoherence with negative samples naturally performs poorly on Fold~1.

The  scores for {Fold~2  Fold~1}, and {Fold~3  Fold~1} modes under both ``w/o CC" and ``w/ CC" settings are adversely affected by the low precision of the models in both the subtasks. In other words, the baseline models in these two modes perform poor in extracting empty spans from the ground truth negative examples in subtask~1 and also classify most of the negative examples as positive in subtask~2.
\begin{table}[t!]
  \centering
{
\begin{tabular}{@{}lll@{\hspace{7ex}}ccc@{}c@{\hspace{6ex}}ccc@{}}
    \toprule
      \multicolumn{3}{c}{\multirow3*{\textbf{Model}}} & \multicolumn{3}{c}{\textbf{w/o CC}} && \multicolumn{3}{c}{\textbf{w/ CC}}\\
      \cmidrule{4-6}\cmidrule{8-10}
    && & Pos. F1 & Neg. F1 & macro F1 && Pos. F1 & Neg. F1 & macro F1\\
    
    
\midrule
    \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 1  Fold 1}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} &  Base & \textbf{56.64} & 85.13 & \textbf{70.88} && 64.28 & \textbf{88.74} & 76.51 \\
   & &  Large & 50.48 & \textbf{87.35} & 68.91 && \textbf{66.23} & 87.89 & \textbf{77.06} \\
\cmidrule{2-10}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} &  Base & 25.98 & 90.73 & 58.36 && 28.02 & \textbf{95.67} & 61.85\\
  &  &  Large & \textbf{32.34} & \textbf{95.61} & \textbf{63.97} && \textbf{40.83} & \textbf{95.68} & \textbf{68.26} \\
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 1  Fold 2}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} &  Base & \textbf{57.50}  & 82.71  & 70.11  && 59.06  & 86.91  & 72.98 \\
   & &  Large & 56.13  & \textbf{88.33}  & \textbf{72.23}  && \textbf{60.09} & \textbf{88.00} & \textbf{74.04} \\
    \cmidrule{2-10}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} &  Base & 32.60  & 89.99  & 61.30  && 27.14  & 94.16  & 60.65 \\
  &  &  Large & \textbf{36.61}  & \textbf{94.60} & \textbf{65.60}  && \textbf{37.59}  & \textbf{94.63}  & \textbf{66.11} \\  
\midrule
    \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 1  Fold 3}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} &  Base  & 57.52 & 82.72  & 70.12  && 49.30  & 79.27  & 64.29 \\
   & &  Large & \textbf{56.04}  & \textbf{88.28}  & \textbf{72.16}  && \textbf{60.63}  & \textbf{88.30}  & \textbf{74.46} \\
    \cmidrule{2-10}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} &  Base & 33.24  & 90.30  & 61.77  && 23.83  & 92.97  & 58.40 \\
  &  &  Large & \textbf{36.55}  & \textbf{94.59}  & \textbf{65.57}  && \textbf{37.87}  & \textbf{94.69}  & \textbf{66.28} \\  
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 2  Fold 2}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} &  Base & 76.21 & 91.23 & 83.72 && 89.37 & 95.21 & 92.32 \\
   & &  Large & \textbf{79.52} & \textbf{91.27} & \textbf{85.40} && \textbf{93.05} & \textbf{97.22} & \textbf{95.13} \\
    \cmidrule{2-10}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} &  Base & 46.12 & \textbf{93.80} & 69.96 && \textbf{65.09} & \textbf{95.60} & \textbf{80.35} \\
  &  &  Large & \textbf{48.36} & 92.06 & \textbf{70.21} && 61.12 & 95.59 & 78.35 \\
  
  
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 2  Fold 1}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} &  Base  & \textbf{52.52} & \textbf{75.51}  & \textbf{64.02}  && 41.86 & \03.25  & 22.55 \\
   & &  Large & 51.57 & 67.58  & 59.57  && \textbf{43.25}  & \textbf{19.95} & \textbf{31.60} \\
    \cmidrule{2-10}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} &  Base & \textbf{31.51}  & \textbf{92.09}  & \textbf{61.80}  && 25.22  & 74.69  & 49.96 \\
  &  &  Large & 29.64  & 87.68  & 58.66  && \textbf{26.30} & \textbf{76.44}  & \textbf{51.37} \\
  
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 3  Fold 3}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} &  Base & 74.73 & \textbf{90.33} & \textbf{82.53} && 92.64 & 96.99 & 94.81 \\
   & &  Large & \textbf{75.79} & 88.43 & 82.11 && \textbf{93.34} & \textbf{97.23} & \textbf{95.29} \\
    \cmidrule{2-10}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} &  Base & \textbf{51.23} & \textbf{93.70} & \textbf{72.46} && \textbf{63.91} & \textbf{94.55} & \textbf{79.23} \\
  &  &  Large & 43.00 & 88.47 & 65.74 && 59.03 & 92.21 & 75.62 \\
  
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\tiny{Fold 3  Fold 1}~~}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{{DD}}}} &  Base  & \textbf{52.02} & \textbf{74.59}  & \textbf{63.31}  && 41.64 & \02.99  & 22.31 \\
   & &  Large & 51.53  & 65.76  & 58.65  && \textbf{41.86} & \textbf{\04.89} & \textbf{23.38} \\
    \cmidrule{2-10}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{{IE}}}} &  Base & \textbf{34.74}  & \textbf{91.46}  & \textbf{63.10}  && \textbf{19.13}  & \textbf{54.25}  & \textbf{36.69} \\
  &  &  Large & 27.58  & 84.13  & 55.86  && 18.33 &  48.01  & 33.17 \\
    \bottomrule
   \end{tabular}
  }
  \caption{{Results for Causal Emotion Entailment task on the test sets of \RECCONDADD{} and \RECCONDAIE{}. Class wise F1 scores and the overall macro F1 scores are reported. All scores reported at best macro F1 scores. DD stands for \RECCONDADD{}, IE for \RECCONDAIE{}. All models are RoBERTa-based models. Fold   Fold  means trained on Fold , tested on Fold .}}
  \label{tab:cus2x}
\end{table}

On the other hand, we do not observe any significant performance drop for either negative or positive examples when the models trained in Fold 1 are evaluated in Folds~2 and~3. This affirms the superiority of Fold~1 as a training dataset. Besides, note that Fold~1 is a more challenging and practical choice than the rest of the two folds as in real scenarios, we need to identify causes of emotions within a single dialogue by reasoning over the utterances in it.

\section{Challenges of the Task} 
\label{sec:challengesx}

This section identifies several examples that indicate the need for \textbf{complex reasoning} to solve the causal span extraction task. Abilities to accurately reason will help validate if a candidate span is causally linked to the target emotion. We believe these pointers would help further research on this dataset and solving the task in general.

\paragraph{Amount of Spans} 
One of the primary challenges of this task is determining the set of spans that can sufficiently be treated as the cause for a target emotion. The spans should have coverage to be able to formulate logical reasoning steps (performed implicitly by annotators) that include skills such as numerical reasoning (see \cref{fig:numerical_reasoning}), among others.

\paragraph{Emotional Dynamics} Understanding emotional dynamics in conversations is closely tied with emotion cause identification. As shown in our previous sections, many causal phrases in the dataset depend on the inter-personal event/concept mentions, emotions, and self-influences (sharing causes). We also observe that emotion causes may be present across multiple turns, thus requiring the ability to model long-term information. Emotions of the contextual utterances help in this modeling. In fact, without the emotional information of the contextual utterances, our annotators found it difficult to annotate emotion causes in the dataset. Understanding cordial greetings, conflicts, agreements, and empathy are some of the many scenarios where contextual emotional dynamics play a significant role. 








\begin{figure}[t!]
    \centering
    \includegraphics[width=60ex]{numerical_reasoning.png}
    \caption{{In this example, , in utterance , is sad because of failing to negotiate the desired amount to sell a TV. While ``\textit{the price is final}" is a valid causal span, one also needs to identify the discussion where  is ready to pay only \2500.}}
    \label{fig:numerical_reasoning}
\end{figure}

\paragraph{Commonsense Knowledge} Extracting emotion causes in conversations comprises complex reasoning steps, and commonsense knowledge is an integral part of this process. The role of commonsense reasoning in emotion cause recognition is more evident when the underlying emotion the cause is latent. Consider the example below:
\begin{exe}
\ex { (\emo{happy})}: \textit{Hello, thanks for calling 123 Tech Help, I'm Todd. How can I help you?}\\
{ (\emo{fear})}: \textit{Hello ? Can you help me ? My computer ! Oh man ...}
\label{ex:latent}
\end{exe}
In this case,  is happily offering help to . The cause of happiness in this example is due to the event ``\textit{greeting}" or intention to offer help. On the other hand,  is fearful because of his/her \textit{broken computer}. The causes of elicited emotions by both the speakers can only be inferred using commonsense knowledge.

\paragraph{Complex Co-Reference} While in narratives, co-references are accurately used and often explicit, it is not the case in dialogues (see~\cref{fig:pronoun_mismatch}).  

\begin{figure}[t!]
    \centering
    \includegraphics[width=60ex]{pronoun_mismatch.png}
    \caption{{In this example, the emotion cause for utterance 2 may lie in phrases spoken by (and for) the counterpart () and not the target speaker () i.e., ``\textit{flashy red lines}'' in 's utterance points to the property of the ``\textit{watch}'' that  bought. One needs to infer such co-referential links to extract the correct causal spans.}}
    \label{fig:pronoun_mismatch}
\end{figure}


\paragraph{Exact vs. Perceived Cause} At times, the complex and informal nature of conversations prohibits the extraction of exact causes. In such cases, our annotators extract the spans that can be perceived as the respective cause. These causal spans can be rephrased to represent the exact cause for the expressed emotion. For example,

\begin{exe}
\ex { (\emo{neutral})}: \textit{How can I help you Sir?.}\\
{ (\emo{frustrated})}: \textit{I just want my flip phone to work----that's all I need.}
\label{ex:exact1}
\end{exe}
In this example, the cause lies in the sentence ``\textit{I just want my flip phone to work}", with the exact cause meaning of ``\textit{My flip phone is not working}". Special dialogue-act labels such as \emph{goal achieved} and \emph{goal not-achieved} can also be adopted to describe such causes.

\paragraph{From Cause Extraction to Causal Reasoning}

\begin{figure}[t!]
    \centering
    \includegraphics[width=60ex]{temporal_reasoning.png}
    \caption{{In this example, the cause for the happy state of  (utterance 6) is corroborated by three indicated spans. First,  gets happy over receiving a ``\textit{birthday present}" (utterance 3) which is a ``\textit{gold watch}" (utterance 4). Then, the emotion evoked by the 4 utterance is propagated into 's next utterance where it is confirmed that  loves the gift (``\textit{I love it!}"). Performing temporal reasoning over these three spans helps understand that  is happy because of liking a present received as a birthday gift.}}
    \label{fig:temporal_reasoning}
\end{figure}

Extracting causes of utterances involve reasoning steps. In this work, we do not ask our annotators to explain the reasoning steps pertaining to the extracted causes. However, one can still sort the extracted causes of an utterance according to their temporal order of occurrence in the dialogue. The resulting sequence of causes can be treated as a participating subset of the reasoning process as shown in \cref{fig:temporal_reasoning}. In the future, this dataset can be extended by including reasoning procedures. However, coming up with an optimal set of instructions for the annotators to code the reasoning steps is one of the major obstacles. \cref{fig:csk_exx} also demonstrates the process of reasoning where utterance  and  are the triggers of \emo{happy} emotion in the utterance . However, the reasoning steps that are involved to extract these causes can be defined as:  is happy because his/her goal to participate in the \textit{house open party} is achieved after the confirmation of  who will organize the \textit{house open party}. This reasoning includes understanding discourse~\cite{chakrabarty-etal-2019-ampersand}, logic and leveraging commonsense knowledge.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=60ex]{csk.png}
    \caption{{An example of emotional reasoning where the \emph{happiness} in utterance~3 is caused by the triggers in utterances~1 and~2.}}
    \label{fig:csk_exx}
\end{figure}

More generally, 
\textbf{emotion causal reasoning in conversations}
extends the task of identifying emotion cause to determining the \textbf{function} and \textbf{explanation} of why the stimuli or triggers evoke the emotion in the target utterance. 
\textbf{Evidence utterance ():} An utterance containing a span that is the target utterance's emotion cause. As there can be multiple evidence utterances of , we represent the set all evidence utterances as  and . 












\section{Connection to Interpretability of the Contextual Models}
One of the advantages of identifying the causes of emotions in conversations is its role in interpreting a model's predictions. We reckon two situations where emotion cause identification can be useful to verify the interpretability of the contextual emotion recognition models that rely on attention mechanisms to count on the context:

\begin{itemize}[leftmargin=*]
    \item In conversations, utterances may not contain any explicit emotion bearing words or sound neutral on the surface but still carry emotions that can only be inferred from the context. In these cases, one can probe contextual models by dropping the causal utterances that contribute significantly to evoke emotion in the target utterance. It would be interesting to observe whether the family of deep networks that rely on attention mechanisms for context modeling e.g., transformer assign higher probability scores to causal contextual utterances in order to make correct predictions.
    \item As discussed in Section 5, the cause can be present in the target utterance and the model may not need to cater contextual information to predict the emotion. In such cases, it would be worth checking whether attention-based models assign high probability scores to the spans in the target utterance that contribute to the causes of its emotion. 
\end{itemize}

One should also note that a model does not always need to identify the cause of emotions to make correct predictions. For example, 
\begin{exe}
\ex { (\emo{happy})}: \textit{Germany won the match!}\\
{ (\emo{happy})}: \textit{That's great!}
\end{exe}
Here, a model can predict the emotion of  by just leveraging the cues present in the corresponding utterance. However, the utterance by  is just an expression and the cause of the emotion is an event ``\textit{Germany won the match}". Nonetheless, identifying the causes of emotions expressed in a conversation makes the model trustworthy, interpretable, and explainable. 
























\section{Conclusion}
We have addressed the problem of \textbf{R}ecognizing \textbf{E}motion \textbf{C}ause in \textbf{CON}versations and introduced a new dialogue-level dataset, \RECCONDA{}, 
containing more than 1,126 dialogues (dyadic conversations) and 10,600 utterance causal span pairs. We identified various emotion types and key challenges that make the task 
extremely challenging. Further, we 
proposed two subtasks and formulated Transformer-based strong baselines to address these subtasks.


Future work will target the analysis of emotion cause in multi-party settings. We also plan to annotate the reasoning steps involved in identifying causal spans of elicited emotions in conversations.
Another direction of future work is to extend the approach to multi-modal setting, both in terms of transferring our annotation to the multi-modal data where such data are available (the part of our dataset extracted from IEMOCAP) and in terms of the benchmark algorithms.



\section*{Conflict of interest}
The authors declare that they have no conflict of interest.
\section*{Compliance with Ethical Standards}
\begin{itemize}
\item This article does not contain any studies with human participants or animals performed by any of the authors.
    \item All authors certify that they have no affiliations with or involvement in any organization or entity with any financial interest or non-financial interest in the subject matter or materials discussed in this manuscript.
\end{itemize}

\bibliographystyle{spbasic}      \bibliography{refs}

\end{document}
