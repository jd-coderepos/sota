\documentclass{article} \usepackage{acl2017,times}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx,subfigure}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{color}
\usepackage{wrapfig,booktabs}

\usepackage{enumitem}

\usepackage{url}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\title{All-but-the-Top: Simple and Effective Postprocessing for Word Representations}



\author{Jiaqi Mu,  Pramod Viswanath \\
University of Illinois at Urbana Champaign\\
{\tt \{jiaqimu2, pramodv\}@illinois.edu}}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 

\begin{document}


\maketitle

\begin{abstract}
Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a {\em very simple}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations {\em even stronger}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and { text classification}) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones. 
\end{abstract}

\section{Introduction}

Words and their interactions (as sentences) are the basic units of natural language.  Although words are readily modeled as discrete atomic units, this is unable to capture the relation between the words.  Recent distributional real-valued representations of words  (examples: word2vec, GloVe) have transformed the landscape of NLP applications -- for instance, text classification \citep{socher2013recursive,maas2011learning,kim2014convolutional}, machine translation \citep{sutskever2014sequence,bahdanau2014neural} and knowledge base completion \citep{bordes2013translating,socher2013reasoning}.  The success  comes from the geometry of the representations that efficiently captures  linguistic regularities: the semantic similarity of words is   well captured by the similarity of the corresponding vector representations. 

A variety of approaches have been proposed in recent years to learn the word representations:  \citet{collobert2011natural,turian2010word} learn the representations via semi-supervised learning by jointly training the language model and  downstream applications;  \citet{bengio2003neural,mikolov2010recurrent,huang2012improving} do so by fitting the data into a neural network language model; \citet{mikolov2013efficient,mnih2007three} by log-linear models; and \citet{dhillon2012two,pennington2014glove,levy2014neural,stratos2015model,arora2015rand} by producing a low-dimensional representation of the cooccurrence statistics. Despite the  wide disparity of algorithms to induce word representations, the performance of several of the recent methods  is roughly similar on a  variety of intrinsic and extrinsic evaluation testbeds. 

In this paper, we find that  a {\em simple} processing renders the off-the-shelf existing  representations {\em even stronger}. The proposed  algorithm is motivated by the following observation.

\paragraph{Observation}   {\em Every} representation we tested, in many languages,  has the following properties: 
\begin{itemize}\item The word representations have {\em non-zero mean} -- indeed, word vectors share a large  common vector (with norm up to a half of the average norm of word vector). 
\item After removing the common mean vector, the representations are {\em far from} isotropic -- indeed,  much of the energy of most word vectors is contained in a very low dimensional subspace (say, 8 dimensions out  of 300). 
\end{itemize} 


\paragraph{Implication} Since all words share the same common vector and have the same dominating directions, and such vector and directions strongly influence the word representations in the same way, we propose to eliminate them by: (a) removing the nonzero mean vector from all word vectors, effectively reducing the energy;  (b) projecting the representations {\em away} from the dominating $D$ directions, effectively reducing the dimension. Experiments suggest that $D$ depends on the representations (for example, the dimension of the representation, the training methods and their specific hyperparameters, the training corpus) and also depends on the downstream applications. Nevertheless, a rule of thumb of choosing $D$ around $d/100$, where $d$ is the dimension of the word representations, works uniformly well across multiple languages and multiple representations and multiple test scenarios. 

We emphasize that the proposed postprocessing is {\em counter intuitive} -- typically  denoising by dimensionality reduction is done by eliminating the {\em weakest} directions (in a singular value decomposition of the stacked word vectors), and {\em not} the  dominating ones. Yet, such postprocessing yields a ``purified'' and  more ``isotropic'' word representation as seen in our elaborate experiments. 


\paragraph{Experiments} By postprocessing the word representation by eliminating the common parts, we find the processed word representations to capture stronger linguistic regularities. We demonstrate this  quantitatively, by comparing the performance of both the original word representations and the processed ones on three canonical lexical-level tasks: 
\begin{itemize}\item {\em word similarity} task tests the extent to which the representations capture the similarity between two words -- the processed representations are consistently better on seven different datasets, on average by 1.7\%; 
\item {\em concept categorization} task  tests the extent to which the clusters of word representations capture the word semantics -- the processed representations are consistently better on three different datasets, by 2.8\%, 4.5\% and 4.3\%; 
\item {\em word analogy} task  tests the extent to which the difference of two representations captures a latent linguistic relation -- again, the performance is consistently improved (by 0.5\% on semantic analogies, 0.2\% on  syntactic analogies and 0.4\% in total). Since part of the dominant components are inherently canceled due to the subtraction operation while solving the analogy, we posit that the performance improvement is not as pronounced as earlier. 
\end{itemize}

Extrinsic evaluations provide a way to test the goodness of representations in specific downstream tasks. We evaluate the effect of postprocessing on a standardized and important extrinsic evaluation task on  sentence modeling: 
 {\em semantic textual similarity} task -- where we represent a sentence by its averaged word vectors and score the similarity between a pair of sentences by the cosine similarity between the corresponding sentence representation.  Postprocessing improves the performance consistently and significantly  over 21 different datasets (average  improvement of 4\%). 
 
 
Word representations have been particularly successful in NLP applications involving supervised-learning, especially in conjunction with neural network architecture. Indeed, we see the power of postprocessing in an experiment on a standard {\em text classification} task using a well established convoluntional neural network (CNN) classifier \citep{kim2014convolutional} and three RNN classifiers (with vanilla RNN, GRU \citep{chung2015gated} and LSTM \cite{greff2016lstm}  as recurrent units).  Across two different pre-trained word vectors, five datasets and four different architectures, the performance with processing improves on a majority of instances (34 out of 40) by a good margin (2.85\% on average), and the two performances with and without processing are comparable in the remaining ones. 


\paragraph{Related Work.} Our work is directly related to  word representation algorithms, most of which have been elaborately cited. 

 Aspects similar to  our  postprocessing algorithm have appeared in specific NLP contexts very recently in \citep{SAHLGREN16.102} (centering the mean) and \citep{arora2017simple} (nulling away only the first principal component). Although there is a superficial similarity between our work and (Arora et al. 2017), the  nulling directions we take and the one they take are fundamentally different. Specifically, in Arora et al. (2017), the first dominating vector  is *dataset-specific*, i.e., they first compute the sentence representation for the entire semantic textual similarity dataset, then extract the top direction from those sentence representations and finally project the sentence representation away from it. By doing so, the top direction will inherently encode the common information across the entire dataset, the top direction for the "headlines" dataset may encode common information about news articles while the top direction for "Twitter'15" may encode the common information about tweets. In contrast, our dominating vectors are over the entire vocabulary of the language.
 
 More generally, the idea of removing the top principal components has been studied in the context of {\em positive-valued, high-dimensional} data matrix analysis \citep{bullinaria2012extracting,price2006principal}. \citet{bullinaria2012extracting}  posits that the highest variance components of the cooccurrence matrix are corrupted by information other than lexical semantics, thus heuristically justifying the removal of the top  principal components.  A similar idea appears in the context of population matrix analysis  \citep{price2006principal}, where the entries are also all positive. Our  postprocessing operation is on dense low-dimensional representations (with both positive and negative entries). 
 
 We posit that the  postprocessing operation  makes the representations more ``isotropic'' with  stronger self-normalization  properties -- discussed in detail in Section ~\ref{sec:postprocessing} and Appendix~\ref{app:angular}.  Our main point is that this isotropy condition can be explicitly enforced to come up with new embedding algorithms (of which our proposed post-processing is a simple and practical version).


\section{Postprocessing}
\label{sec:postprocessing}
We test our observations on various word representations: four publicly available word representations (WORD2VEC\footnote{\url{https://code.google.com/archive/p/word2vec/}} \citep{mikolov2013efficient} trained using Google News, GLOVE\footnote{\url{https://github.com/stanfordnlp/GloVe}} \citep{pennington2014glove} trained using Common Crawl, RAND-WALK \citep{arora2015rand} trained using Wikipedia and TSCCA\footnote{\url{http://www.pdhillon.com/code.html}} trained using English Gigaword) and two  self-trained word representations using CBOW and Skip-gram \citep{mikolov2013efficient} on the 2010 Wikipedia corpus from \citep{al2013polyglot}. The detailed statistics for all representations are listed in Table~\ref{tb:embedding}. For completeness, we also consider the representations on other languages: a detailed study is provided in Appendix~\ref{app:multilingual}. 

\begin{table*}[!h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
 &  \bf Language &  \bf Corpus &  \bf dim &  \bf vocab size &  \bf avg. $\|v(w)\|_2$ &  $\bf \|\mu\|_2$\\
\hline
WORD2VEC & English & Google News & 300 & 3,000,000 & 2.04 & 0.69 \\
GLOVE & English & Common Crawl & 300 & 2,196,017 & 8.30 & 3.15\\
RAND-WALK & English & Wikipedia & 300 & 68, 430 & 2.27 & 0.70\\
CBOW & English & Wikipedia & 300 & 1,028,961 & 1.14 & 0.29 \\
Skip-Gram & English & Wikipedia & 300 & 1,028,961 & 2.32 & 1.25 \\
\hline
\end{tabular}
\end{center}
\caption{A detailed description for the embeddings in this paper.}
\label{tb:embedding}
\end{table*}


\begin{wrapfigure}{l}{0.3\textwidth}
  \centering
{
  \includegraphics[width = 0.3\textwidth]{figures/decay.pdf}
  }
\caption{The decay of the normalized singular values of word representation.}
  \vspace{-10pt}
  \label{fig:decay}
\end{wrapfigure}

Let $v(w) \in \mathbb{R}^d $ be a word representation for a given word $w$ in the vocabulary $\mathcal{V}$. We observe the following two phenomena in each of the word representations listed above:
\begin{itemize}[leftmargin=*,,topsep=0pt]
\item $\{v(w): w\in \mathcal{\mathcal{V}}\}$ are not of zero-mean: i.e., all $v(w)$ share a non-zero common vector, 
$ v(w) = \tilde{v}(w) + \mu,$ 
where $\mu$ is the average of all $v(w)$'s, i.e., $\mu = {1}/{|\mathcal{V}|} \sum_{w\in \mathcal{V}} v(w)$. The norm of $\mu$ is approximately 1/6 to 1/2 of the  average norm of all $v(w)$ (cf.\ Table~\ref{tb:embedding}).
\item $\{\tilde{v}(w): w\in \mathcal{V}\}$ are not isotropic: Let $u_1,..., u_d$ be the first to the last components recovered by the principal component analysis (PCA) of $\{\tilde{v}(w): w\in \mathcal{V}\}$, and $\sigma_1,...,\sigma_d$ be the corresponding normalized variance ratio. Each $\tilde{v}(w)$ can be written as a linear combinations of $u$:
$ \tilde{v}(w) = \sum_{i=1}^d \alpha_i(w) u_i.$ 
As shown in Figure~\ref{fig:decay}, we observe that $\sigma_i$ decays near exponentially for  small values of $i$ and  remains roughly constant over the later ones. This suggests there exists $D$ such that $\alpha_i \gg \alpha_j$ for all $i \le D$ and $j \gg D$;  from Figure~\ref{fig:decay} one observes that $D$ is roughly  10 with  dimension $d=300$. 
\end{itemize} 

\paragraph{Angular Asymmetry of Representations} A modern understanding of word representations involves either PMI-based  (including word2vec \citep{mikolov2010recurrent,levy2014neural} and GloVe \citep{pennington2014glove}) or CCA-based  spectral factorization approaches. While CCA-based spectral factorization methods have long been understood from  a probabilistic (i.e., generative model) view point  \citep{browne1979maximum,hotelling1936relations} and   recently in the NLP context  \citep{stratos2015model},  a  corresponding  effort for the PMI-based methods has only recently been conducted in an inspired work   \citep{arora2015rand}. 

\citet{arora2015rand}  propose a generative model (named RAND-WALK) of sentences, where  every word is parameterized by a $d$-dimensional vector. With a key postulate that the word vectors are angularly uniform   (``isotropic"), the family of PMI-based word representations can be explained under the RAND-WALK model in terms of the maximum likelihood rule.  
Our observation that word vectors learnt through PMI-based approaches are not of zero-mean and are not isotropic (c.f.\ Section 2) contradicts with this postulate. The isotropy conditions  are  relaxed in Section~2.2  of \citep{arora2015rand}, but the match with the spectral properties observed in Figure~\ref{fig:decay} is not immediate.  

This contradiction is  explicitly resloved by relaxing the constraints on the word vectors to directly fit the observed spectral properties.  The relaxed conditions are: the word vectors should be isotropic around a point (whose distance to the origin is a  small fraction of the average norm of word vectors) lying on a low dimensional subspace. Our main result is to show that even with this enlarged parameter-space, the maximum likelihood rule continues to be close to the PMI-based spectral factorization methods.  A brief summary of RAND-WALK, and the mathematical connection between our work and theirs, are explored in detail in Appendix A.





\subsection{Algorithm}
\label{sec:algorithm} Since all word representations share the same common vector $\mu$ and have the same dominating directions  and such vector and directions strongly influence the word representations in the same way, we propose to eliminate them, as formally achieved as Algorithm \ref{algo:representation}.

\begin{algorithm}[!h]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Word representations $\{v(w), w\in \mathcal{V}\}$, a threshold parameter $D$, }
Compute the mean of $\{v(w), w\in \mathcal{V}\}$,
$ \mu \leftarrow \frac{1}{|\mathcal{V}|} \sum_{w\in \mathcal{V}} v(w), \tilde{v}(w) \leftarrow v(w) - \mu$  \\
Compute the PCA components:
$u_1,...,u_d \leftarrow {\rm PCA}(\{\tilde{v}(w), w\in \mathcal{V}\}).$ \\
Preprocess the representations:
$v'(w) \leftarrow \tilde{v}(w) - \sum_{i=1}^D \left(u_i^{\top} v(w)\right) u_i$ \\
\Output{Processed representations $v'(w)$.}
\caption{Postprocessing algorithm on word representations.}
\label{algo:representation}
\end{algorithm}



\paragraph{Significance of Nulled Vectors}
Consider the representation of the words as viewed in terms of the top $D$ PCA  coefficients $\alpha_{\ell}(w)$, for $1\leq \ell \leq D$.    
We  find that  these few coefficients   encode the {\em frequency} of the  word  to a  significant degree;  Figure~\ref{fig:frequency} illustrates the relation between the ($\alpha_{1}(w),\alpha_2(w))$ and the unigram  probabilty $p(w)$, where the correlation is geometrically visible.


\begin{figure*}[!h]
  \centering
  \vspace{-10pt}
  \includegraphics[width=1.1\textwidth]{./figures/freq.png}
  \vspace{-20pt}
  \caption{The top two PCA directions (i.e, $\alpha_1(w)$ and $\alpha_2(w)$) encode   frequency.}
  \vspace{-10pt}
  \label{fig:frequency}
\end{figure*}

\paragraph{Discussion} In our proposed processing algorithm, the number of components to be nulled,  $D$, is the only hyperparameter that needs to be tuned. We find that a good rule of thumb is to choose $D$ approximately to be $d/100$, where $d$ is the dimension of a word representation. This is empirically justified in the experiments of the following  section where $d=300$ is standard for published word representations. We trained word representations for higher  values of $d$ using the WORD2VEC and GLOVE algorithms and  
 repeated these experiments; we see corresponding consistent improvements due to postprocessing in   Appendix \ref{app:extra}.


{
\subsection{Postprocessing as a ``Rounding'' towards Isotropy}
\label{app:rounding}

The idea of isotropy comes from the partition function defined in \citep{arora2015rand}, 
\begin{align*}
    Z(c) = \sum_{w\in \mathcal{V}}\exp\left(c^{\top}v(w)\right),
\end{align*}
where $Z(c)$ should approximately be a constant with any unit vector $c$ (c.f. Lemma 2.1 in \citep{arora2015rand}). Hence, we mathematically define a measure of isotropy as follows,
\begin{align}
    I(\{v(w)\}) = \frac{\min_{\|c\| = 1} Z(c)}{\max_{\|c\|=1} Z(c)}, \label{eq:iso_def}
\end{align}
where $I(\{v(w)\})$ ranges from 0 to 1, and $I(\{v(w)\})$ closer to 1 indicates that $\{v(w)\}$ is more isotropic. The intuition behind our postprocessing algorithm can also be motivated by letting $I(\{v(w)\}) \to 1$. 


Let $V$ be the matrix stacked by all word vectors, where the rows correspond to word vectors, and $1_{|\mathcal{V}|}$ be the  $|\mathcal{V}|$-dimensional vectors with all entries equal to one, $Z(c)$ can be equivalently defined as follows,
\begin{align*}
    Z(c) 
&= |\mathcal{\mathcal{V}}| + 1_|\mathcal{V}|^{\top} V c + \frac{1}{2} c^{\top} V^{\top} Vc + \sum_{k=3}^{\infty} \frac{1}{k!} \sum_{w\in \mathcal{V}} (c^{\top} v(w))^k.
\end{align*}
$I(\{v(w)\})$ is, therefore, can be {\em very coarsely} approximated by,
\begin{itemize}
\item {\bf A first order approximation}:
\begin{align*}
    I(\{v(w)\}) &\approx \frac{|\mathcal{V}| + \min_{\|c\|=1}1_{|\mathcal{V}|}^{\top} Vc}{|\mathcal{V}| + \max_{\|c\|=1}1_{|\mathcal{V}|}^{\top} Vc} =  \frac{|\mathcal{V}| -  \|1_{|\mathcal{V}|}^{\top}V\|}{|\mathcal{V}| + \|1_{|\mathcal{V}|}^{\top}V\|}.
\end{align*}
Letting $I(\{v(w)\})=1$ yields $ \|1_{|\mathcal{V}|}^{\top}V\| = 0$, which is equivalent to $\sum_{w\in\mathcal{V}} v(w) = 0$. The intuition behind the first order approximation matches with the first step of the proposed algorithm, where we enforce $v(w)$ to have a  zero mean.
\item {\bf A second order approximation}:
\begin{align*}
    I(\{v(w)\}) 
    &\approx \frac{|\mathcal{V}| + \min_{\|c\|=1}1_{|\mathcal{V}|}^{\top} Vc + \min_{\|c\|=1}\frac{1}{2}c^{\top} V^{\top} Vc}{|\mathcal{V}| + \max_{\|c\|=1}1_{|\mathcal{V}|}^{\top} Vc + \max_{\|c\|=1}\frac{1}{2}c^{\top} V^{\top} Vc} =  \frac{|\mathcal{V}| - \|1_{|\mathcal{V}|}^{\top}V\| + \frac{1}{2}\sigma_{\min}^2}{ |\mathcal{V}| + \|1_{|\mathcal{V}|}^{\top}V\| + \frac{1}{2}\sigma_{\max}^2},
\end{align*}
where $\sigma_{\min}$ and $\sigma_{\max}$ are the smallest and largest singular value of $V$, respectively. Letting $I(\{v(w)\}) = 1$ yields $ \|1_{|\mathcal{V}|}^{\top}V\| = 0$ and $\sigma_{\min} = \sigma_{\max}$. The fact that $\sigma_{\min} = \sigma_{\max}$ suggests the spectrum of $v(w)$'s should be flat. The second step of the proposed algorithm removes the highest singular values, and therefore explicitly flatten the spectrum of $V$. 
\end{itemize}


\paragraph{Empirical Verification} Indeed, we empirically validate the effect of postprocessing of on $I(\{v(w)\})$. Since there is no closed-form solution for $\arg\max_{\|c\|=1}Z(c)$ or $\arg\min_{\|c\|=1} Z(c)$, and it is impossible to enumerate all $c$'s, we estimate the measure by,
\begin{align*}
    I(\{v(w)\}) \approx \frac{\min_{c \in C} Z(c)}{\max_{c \in C} Z(c)},
\end{align*}
where $C$ is the set of eigenvectors of $V^{\top} V$. The value of $I(\{v(w)\})$ for the original vectors and processed ones are reported in Table~\ref{tb:app:isotropy}, where we can observe that the degree of isotropy vastly increases in terms of this measure.

\begin{wraptable}{l}{0.48\textwidth}
\centering
\begin{tabular}{|c|c|c|}
\hline
         & before & after         \\ \hline
WORD2VEC & 0.7    & \textbf{0.95} \\ \hline
GLOVE    & 0.065  & \textbf{0.6}  \\ \hline
\end{tabular}
\caption{Before-After on the measure of isotropy.}
\label{tb:app:isotropy}
\end{wraptable}


A formal way to verify the isotropy property is to directly check if the ``self-normalization" property (i.e., $Z(c)$ is a constant, independent of $c$ \citep{andreas2015and}) holds more strongly. Such a validation is seen diagrammatically in Figure~\ref{app:fig:partition} where we randomly sampled 1,000 $c$'s as \citep{arora2015rand}.  

}

\begin{figure}[!h]
\centering
\vspace{-20pt}
\subfigure[word2vec]
{
\includegraphics[width = 0.4\textwidth]{./figures/word2vec-hist.pdf}
}
\subfigure[GloVe]
{
\includegraphics[width = 0.4\textwidth]{./figures/glove-hist.pdf}
}
\caption{The histogram of $Z(c)$ for 1,000 randomly sampled vectors $c$ of unit norm, where  $x$-axis is normalized by the mean of all values and $D = 2$ for GLOVE and $D=3$ for  WORD2VEC.}
\label{app:fig:partition}
\end{figure}






\section{Experiments}
\label{sec:experiments}
Given the popularity and widespread use of WORD2VEC \citep{mikolov2013efficient} and GLOVE \citep{pennington2014glove}, we use their publicly available pre-trained reprepsentations in the following experiments. We choose $D=3$ for WORD2VEC and $D=2$ for GLOVE. 
The key underlying principle behind word representations is that similar words should have similar representations. Following the tradition of evaluating word representations \citep{schnabel2015evaluation,baroni2014don}, we perform three canonical {\em lexical-level} tasks: (a) word similarity; (b) concept categorization; (c) word analogy; and one {\rm sentence-level} task: (d) semantic textual similarity. The processed representations consistently improve  performance on all three of them, and  especially strongly on the first two. 


\begin{wraptable}{l}{0.48\textwidth}
\vspace{-10pt}
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c||}{WORD2VEC} & \multicolumn{2}{c|}{GLOVE} \\ \cline{2-5} 
                  & orig.      & proc.     & orig.    & proc.    \\ \hline
RG65  &     76.08 &     \bf  78.34  &  \bf 76.96 &  74.36       \\ \hline
WS    &     68.29 &     \bf  69.05 &   73.79  &  \bf 76.79 \\ \hline
RW    &     53.74 &     \bf 54.33 &  46.41   &  \bf 52.04    \\ \hline
MEN   &     78.20 &     \bf 79.08 &  80.49   &  \bf 81.78     \\ \hline
MTurk &     68.23 &     \bf 69.35 &  69.29  &   \bf 70.85\\ \hline
SimLex  &   44.20 &   \bf 45.10   &   40
83 & \bf 44.97      \\ \hline
SimVerb &   36.35 &  \bf 36.50   &  28.33   &  \bf 32.23      \\ \hline
\end{tabular}
\caption{Before-After results (x100) on word similarity task on seven datasets.}
\label{tb:similarity}
\vspace{-10pt}
\end{wraptable}

\paragraph{Word Similarity} 

The  word similarity task is as follows: given a pair of words, the algorithm assigns a ``similarity" score -- if the pair of words are highly related then the score should also be high and vice versa. The algorithm is evaluated in terms of Spearman's rank correlation compared to (a gold set of) human judgements.

For this experiment, we use seven standard datasets: the first published RG65 dataset \citep{rubenstein1965contextual}; the widely used WordSim-353 (WS) dataset \citep{finkelstein2001placing} which contains 353 pairs of commonly used verbs and nouns; the rare-words (RW) dataset \citep{luong2013better} composed of rarely used words; the MEN dataset \citep{bruni2014multimodal} where the 3000 pairs of words are rated by crowdsourced participants; the MTurk dataset \citep{radinsky2011word} where the 287 pairs of words are rated in terms of relatedness;  the SimLex-999 (SimLex) dataset \citep{hill2016simlex} where the score measures ``genuine" similarity; and lastly the SimVerb-3500 (SimVerb) dataset \citep{gerz2016simverb}, a newly released large dataset focusing on similarity of verbs.



In our experiment, the algorithm scores the similarity between two words by the cosine similarity between the two corresponding word vectors (${\rm CosSim}(v_1,v_2) = v_1^{\top}v_2 / \|v_1\|\|v_2\|$).  
The detailed performance on the seven datasets is reported in Table~\ref{tb:similarity}, where we see   a consistent and significant performance improvement due to postprocessing, across all seven datasets. These statistics (average improvement of {\bf 2.3}\%) suggest that by removing the common parts, the remaining word representations are able to capture stronger semantic relatedness/similarity between words. 



\begin{wraptable}{l}{0.45\textwidth}
\vspace{-10pt}
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c||}{WORD2VEC} & \multicolumn{2}{c|}{GLOVE} \\ \cline{2-5} 
                  & orig.      & proc.     & orig.    & proc.    \\ \hline
ap     &     54.43 &    \bf  57.72 &    64
.18 & \bf 65.42  \\ \hline
esslli &     75.00 &     \bf 84.09 &    81.82 &  81.82    \\ \hline
battig &     71.97 &    \bf  81.71 &    86.59 &  86.59       \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the categorization task.}
\label{tb:categorization}
\vspace{-10pt}
\end{wraptable}

\paragraph{Concept Categorization}

This task is an indirect evaluation of the similarity principle: given a set of concepts, the algorithm needs to group them into different categories (for example, ``bear'' and ``cat'' are both animals and ``city'' and ``country'' are both related to districts). The clustering performance is then evaluated in terms of purity \citep{manning2008introduction} -- the fraction of the total number of the objects that were classified correctly.  


We conduct this task on three different datasets: the Almuhareb-Poesio (ap) dataset \citep{almuhareb2006attributes} contains 402 concepts which fall into 21 categories; the ESSLLI 2008 Distributional Semantic Workshop shared-task dataset \citep{baroni2008bridging} that contains 44 concepts in 6 categories; and the Battig test set \citep{baroni2010distributional} that contains 83 words in 10 categories. 

Here we follow the setting and the proposed algorithm in \citep{baroni2014don,schnabel2015evaluation} --   we cluster words (via their representations)  using the classical $k$-Means algorithm (with fixed $k$).
Again, the processed vectors perform consistently better on all three datasets (with average improvement of 2.5\%); the full details are in   Table~\ref{tb:categorization}.

\begin{wraptable}{l}{0.48\textwidth}
\vspace{-10pt}
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c||}{WORD2VEC} & \multicolumn{2}{c|}{GLOVE} \\ \cline{2-5} 
                  & orig.      & proc.     & orig.    & proc.    \\ \hline
syntax &     73.46 &      \bf 73.50 &  74.95   & \bf 75.40   \\ \hline
semantics &     72.28 &    \bf  73.36 &   79.22  & \bf 79.25  \\ \hline
all  &     72.93 &    \bf 73.44 &   76.89  & \bf 77.15  \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the word analogy task.}
\label{tb:analogy}
\vspace{-10pt}
\end{wraptable}

\paragraph{Word Analogy}

The analogy task  tests to what extent the word representations can encode  latent linguistic relations between a pair of words. Given three words $w_1$, $w_2$, and $w_3$, the analogy task requires the algorithm to find the word $w_4$ such that $w_4$ is to $w_3$ as $w_2$ is to $w_1$. 

We use the analogy dataset introduced in \citep{mikolov2013efficient}. The dataset can be divided into two parts: (a) the {\em semantic} part containing around 9k questions, focusing on  the latent semantic relation between pairs of words (for example, what is to {\rm Chicago} as {\rm Texas} is to {\rm Houston}); and (b) the {\em syntatic}  one containing roughly 10.5k questions, focusing on the latent syntatic relation between pairs of words (for example, what is to ``amazing'' as ``apprently'' is to ``apparent''). 

In our setting, we use the original algorithm introduced in \citep{mikolov2013efficient} to solve this problem, i.e., $w_4$ is the word that maximize the cosine similarity between $v(w_4)$ and $v(w_2) - v(w_1) + v(w_3)$. 
The average performance on the analogy task is provided in Table~\ref{tb:analogy} (with a detailed performance provided in Table~\ref{tb:analogy:detail} in Appendix~\ref{app:analogy}). It can be noticed that while postprocessing continues to improve the performance, the improvement is not as pronounced as earlier. We hypothesize that this is because the mean and some dominant components get canceled  during  the subtraction of $v(w_2)$ from $v(w_1)$, and therefore the effect of postprocessing is less relevant.



\begin{wraptable}{l}{0.45\textwidth}
\vspace{-10pt}
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c||}{WORD2VEC} & \multicolumn{2}{c|}{GLOVE} \\ \cline{2-5} 
                  & orig.      & proc.     & orig.    & proc.    \\ \hline
2012 &     57.22 &      \bf 57.67 &  48.27   &  \bf 54.06    \\ \hline
2013 &     56.81 &      \bf 57.98 &  44.83   & \bf 57.71     \\ \hline
2014 &     62.89 &      \bf 63.30 &   51.11  &  \bf 59.23    \\ \hline
2015 &     62.74 &      \bf 63.35&   47.23   &  \bf 57.29     \\ \hline
SICK &     70.10 &      \bf 70
20 &  65.14   &  \bf 67.85    \\ \hline
all  &     60.88 &      \bf 61.45 &  49.19   &    \bf 56.76  \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the semantic textual similarity tasks.}
\label{tb:sts}
\end{wraptable}
\paragraph{Semantic Textual Similarity}

Extrinsic evaluations measure the contribution of a word representation to specific downstream tasks; below, we study the effect of postprocessing on a standard sentence modeling task --   {\em semantic textual similarity} (STS) which aims at testing the degree to which the algorithm can capture the semantic equivalence between two sentences. For each pair of sentences, the algorithm needs to measure how similar the two sentences are. The degree to which the measure matches with  human judgment (in terms of Pearson correlation) is an index of the algorithm's performance.  
We test the word representations on  20 textual similarity datasets from the 2012-2015 SemEval STS tasks \citep{agirre2012semeval,agirre2013sem,agirre2014semeval,agirrea2015semeval}, and the 2012 SemEval Semantic Related task (SICK) \citep{marelli2014sick}. 

Representing  sentences by the average of their constituent word representations is surprisingly effective in encoding the semantic information of sentences
\citep{wieting2015paraphrase,adi2016fine} and close to the state-of-the-art in these datasets. We follow this rubric and represent a sentence $s$ based on its averaged word representation, i.e., $v(s) = \frac{1}{|s|}\sum_{w\in s} v(w)$, and then compute the similarity between two sentences via the cosine similarity between the two representations.  
The average performance of the original and processed representations is itemized in Table~\ref{tb:sts} (with a detailed performance in Table~\ref{tb:sts:detail} in Appendix~\ref{app:sts}) -- we see a consistent and significant improvement in performance because of postprocessing (on average {\bf 4}\% improvement).

\section{Postprocessing and Supervised Classification}
Supervised downstream NLP applications have greatly improved their performances  in recent years by combining the discriminative learning powers of neural networks in conjunction with the word  representations. We evaluate the performance of a variety of neural network architectures on a standard and important NLP application: {\em text classification}, with 
 sentiment analysis being a particularly important and popular example. The task is defined as follows: given a sentence, the algorithm needs to decide which category it falls into. The categories can be either binary (e.g., positive/negative) or can be more fine-grained (e.g. very positive, positive, neutral, negative, and very negative). 

We evaluate the word representations (with and without postprocessing) using  four different neural network architectures (CNN, vanilla-RNN, GRU-RNN and LSTM-RNN) on  five benchmarks: (a) the movie review (MR) dataset \citep{pang2005seeing}; (b) the subjectivity (SUBJ) dataset \citep{pang2004sentimental}; (c) the TREC question dataset \citep{li2002learning}; (d) the IMDb dataset \citep{maas2011learning}; (e) the stanford sentiment treebank (SST) dataset \citep{socher2013reasoning}.  A detailed description of these standard datasets, their training/test parameters and the cross validation methods adopted is in Appendix~\ref{app:sentiment}. { Specifically, we allow the parameter $D$ (i.e., the number of nulled components) to vary between 0 and 4, and the best performance of the  four  neural network architectures with the now-standard CNN-based text classification algorithm \citep{kim2014convolutional} (implemented using tensorflow\footnote{\url{https://github.com/dennybritz/cnn-text-classification-tf}})  is itemized in Table~\ref{tb:sentiment-analysis}.  The key observation is that the performance of postprocessing is better in a majority (34 out of 40) of the instances by 2.32\% on average, and in the rest the instances the two performances (with and without postprocessing) are comparable. }

\begin{table*}[!h]
\centering
\resizebox{1\textwidth}{!}{
\begin{tabular}{|r|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{3}{*}{} & \multicolumn{4}{c|}{CNN}                                   & \multicolumn{4}{c|}{vanilla-RNN}                             & \multicolumn{4}{c|}{GRU-RNN}                               & \multicolumn{4}{c|}{LSTM-RNN}                                     \\ \cline{2-17} 
                  & \multicolumn{2}{c|}{WORD2VEC} & \multicolumn{2}{c|}{GLOVE} & \multicolumn{2}{c|}{WORD2VEC}   & \multicolumn{2}{c|}{GLOVE} & \multicolumn{2}{c|}{WORD2VEC} & \multicolumn{2}{c|}{GLOVE} & \multicolumn{2}{c|}{WORD2VEC}   & \multicolumn{2}{c|}{GLOVE}      \\ \cline{2-17} 
                  & orig.         & proc.         & orig.        & proc.       & orig.          & proc.          & orig.   & proc.            & orig.     & proc.             & orig.   & proc.            & orig.          & proc.          & orig.          & proc.          \\ \hline
MR                & 70.80         & \bf 71.27     & 71.01        & \bf 71.11   & \textbf{74.95} & 74.01          & 71.14   & \textbf{72.56}   & 77.86     & \textbf{78.26}    & 74.98   & \textbf{75.13}   & 75.69          & \textbf{77.34} & \textbf{72.02} & 71.84          \\ \hline
SUBJ              & 87.14         & \bf 87.33     & 86.98        & \bf 87.25   & 82.85          & \textbf{87.60} & 81.45   & \textbf{87.37}   & 90.96     & \textbf{91.10}    & 91.16   & \textbf{91.85}   & 90.23          & \textbf{90.54} & 90.74          & \textbf{90.82} \\ \hline
TREC              & 87.80         & \bf 89.00     & 87.60        & \bf 89.00   & 80.60          & \textbf{89.20} & 85.20   & \textbf{89.00}   & 91.60     & \textbf{92.40}    & 91.60   & \textbf{93.00}   & 88.00          & \textbf{91.20} & 85.80          & \textbf{91.20} \\ \hline
SST               & \bf 38.46     & 38.33         & \bf 38.82    & 37.83       & \textbf{42.08} & 39.91          & 41.45   & \textbf{41.90}   & 41.86     & \textbf{45.02}    & 36.52   & \textbf{37.69}   & \textbf{43.08} & 42.08          & 37.51          & \textbf{38.05} \\ \hline
IMDb              & 86.68         & \bf 87.12     & \bf 87.27    & 87.10       & 50.15          & \textbf{53.14} & 52.76   & \textbf{76.07}   & 82.96     & \textbf{83.47}    & 81.50   & \textbf{82.44}   & 81.29          & \textbf{82.60} & 79.10          & \textbf{81.33} \\ \hline
\end{tabular}}
\caption{Before-After results (x100) on the text classification task using CNN \citep{kim2014convolutional} and vanilla RNN, GRU-RNN and LSTM-RNN. }
\label{tb:sentiment-analysis}
\end{table*} 


A further  validation of  the postprocessing operation in a variety of downstream applications (eg: named entity recognition, syntactic parsers, machine translation) and classification methods (eg: random forests, neural network architectures) is  of active research interest. Of particular  interest is the  impact of the postprocessing on the rate of convergence and generalization capabilities of the classifiers. Such a systematic study would entail a concerted and large-scale effort by the research community and is left to future research. 


{\paragraph{Discussion} All neural network architectures, ranging from  feedforward  to recurrent  (either  vanilla or GRU or LSTM),  implement at least  linear processing of  hidden/input state vectors at each of their nodes; thus  the postprocessing  operation  suggested in this paper  can {\em in principle} be  automatically ``learnt'' by the neural network, if such internal learning is in-line with the end-to-end training examples.  Yet, in practice this is complicated due to limitations of optimization procedures (SGD) and sample noise. We conduct a preliminary experiment in Appendix \ref{app:neuralnet} and show that subtracting the mean (i.e., the first step of postprocessing) is ``effectively learnt" by neural networks  within their nodes. 
}

\section{Conclusion}

We present a simple postprocessing operation that
renders word representations even stronger, by eliminating the top principal components of all words. Such an simple operation could be used for word embeddings in downstream tasks or as intializations for training task-specific embeddings.  
Due to their popularity, we have used the published representations of WORD2VEC and GLOVE in English in the main text of this paper; postprocessing continues to be successful for other representations and in multilingual settings -- the detailed empirical results are tabulated in Appendix \ref{app:extra}.




\bibliography{preprocessing}
\bibliographystyle{acl_natbib}

\newpage
\onecolumn
\begin{center}
  \Large Appendix: All-but-the-Top: Simple and Effective postprocessing for Word Representations
\end{center}

\appendix


\section{Angular Asymmetry of Representations} 
\label{app:angular}
A modern understanding of word representations involves either PMI-based  (including word2vec \citep{mikolov2010recurrent,levy2014neural} and GloVe \citep{pennington2014glove}) or CCA-based  spectral factorization approaches. While CCA-based spectral factorization methods have long been understood from  a probabilistic (i.e., generative model) view point  \citep{browne1979maximum,hotelling1936relations} and   recently in the NLP context  \citep{stratos2015model},  a  corresponding  effort for the PMI-based methods has only recently been conducted in an inspired work   \citep{arora2015rand}. 

\citep{arora2015rand}  propose a generative model (named RAND-WALK) of sentences, where  every word is parameterized by a $d$-dimensional vector. With a key postulate that the word vectors are angularly uniform   (``isotropic"), the family of PMI-based word representations can be explained under the RAND-WALK model in terms of the maximum likelihood rule.  
Our observation that word vectors learnt through PMI-based approaches are not of zero-mean and are not isotropic (c.f.\ Section 2) contradicts with this postulate. The isotropy conditions  are  relaxed in Section~2.2  of \citep{arora2015rand}, but the match with the spectral properties observed in Figure~\ref{fig:decay} is not immediate.  

In this section, we  resolve this by explicitly  relaxing the constraints on the word vectors to directly fit the observed spectral properties.  The relaxed conditions are: the word vectors should be isotropic around a point (whose distance to the origin is a  small fraction of the average norm of word vectors) lying on a low dimensional subspace. Our main result is to show that even with this enlarged parameter-space, the maximum likelihood rule continues to be close to the PMI-based spectral factorization methods.  
Formally,  the model, the original constraints of \citep{arora2015rand} and the enlarged constraints on the word vectors are listed  below: 
\begin{itemize}\item {\bf A generative model of sentences}: the word at time $t$, denoted by $w_t$, is generated via a log-linear model with a latent discourse variable $c_t$ \citep{arora2015rand}, i.e.,
    \begin{align}
      p(w_t|c_t) = \frac{1}{Z(c_t)} \exp\left(c_t^{\top}v(w_t)\right), \label{eq:model}
    \end{align}
    where $v(w) \in \mathbb{R}^d$ is the vector representation for a word $w$ in the vocabulary $V$, $c_t$ is the latent variable which forms a ``slowly moving" random walk, and  the partition function:  $Z(c) = \sum_{w\in \mathcal{V}}\exp\left(c^{\top}v(w)\right)$.
    \item {\bf Constraints on the word vectors}: \citep{arora2015rand} suppose that there is a Bayesian priori on the word vectors: 
    \begin{quote}
      The ensemble of word vectors consists of i.i.d.\ draws generated by $v = s\cdot \hat{v}$, where $\hat{v}$ is from the spherical Gaussian distribution, and $s$ is a scalar random variable.
    \end{quote}
A deterministic version of this  prior is  discussed in Section~2.2 of \citep{arora2015rand}, but part of these (relaxed) conditions on the word vectors are specifically meant for Theorem~4.1 and not the main theorem (Theorem 2.2).  The geometry of the word representations is only evaluated via the ratio of the  quadratic {\em mean} of the singular values to the smallest one being small enough. This meets the relaxed conditions, but not sufficient to validate the proof approach of the main result  (Theorem~2.2); what would be needed is that the ratio  of the {\em largest} singular value to the smallest one  be small. 

\item{\bf Revised conditions}: We revise the Bayesian prior postulate (and in a deterministic fashion) formally as follows:  there is a mean vector $\mu$, $D$ {\em orthonormal} vectors $u_1,\ldots ,u_D$ (that are orthogonal and of unit norm), such that  every word vector $v(w)$ can be represented by,
    \begin{align}
      v(w) = \mu + \sum_{i=1}^D \alpha_i (w) u_i  + \tilde{v}(w), \label{eq:word-vec}
    \end{align}
    where $\mu$ is bounded, $\alpha_i$ is bounded by $A$, $D$ is bounded by $DA^2 = o(d)$, $\tilde{v}(w)$ are statistically isotropic. By statistical isotropy, we mean: for  high-dimensional rectangles $R$, $\frac{1}{|\mathcal{V}|}\sum_{w\in \mathcal{V}} \mathbf{1}(\tilde{v}(w) \in  R) \to \int_R f(\tilde{v}) d\tilde{v}$, as $|\mathcal{V}|\to \infty$, where $f$ is an angle-independent pdf, i.e., $f(\tilde{v})$ is a function of $\|\tilde{v}\|$. 
\end{itemize}

The revised postulate differs from the original one in two ways: (a) it imposes a formal deterministic constraint on the word vectors;   (b) 
the revised postulate allows the word vectors to be angularly asymmetric: as long as the energy in the direction of $u_1$,\ldots,$u_D$ is bounded, there is no constraint on the coefficients. Indeed, note that  there is no constraint on  $\tilde{v}(w)$ to be orthogonal to  $u_1,\ldots u_D$.  


\paragraph{Empirical Validation}  We can verify  that  the enlarged  conditions are  met by  the  existing word representations.  Specifically, the natural choice for $\mu$ is the mean of the word representations and $u_1 \ldots u_D$ are the singular vectors associated with the top $D$ singular values of the matrix of word vectors. We pick $D=20$ for WORD2VEC and $D = 10$ for GLOVE, and the corresponding value of $DA^2$ for WORD2VEC and GLOVE vectors are both roughly  40, respectively; both values are small compared to  $d = 300$. 


\begin{figure}[!h]
\centering
\includegraphics[width = 0.35\textwidth]{./figures/flat-decay.pdf}
\caption{Spectrum of the published WORD2VEC and GLOVE and  random Gaussian matrices, ignoring the top $D$ components; $D = 10$ for GLOVE and $D=20$ for  WORD2VEC.}
\label{app:fig:decay}
\end{figure}

This  leaves us  to check the statistical isotropy of the 
``remaining" vectors $\tilde{v}(w)$ for words $w$ in the vocabulary.  We do this by  plotting the  remaining spectrum (i.e. the $(D+1)$-th, ..., 300th singular values) for the published  WORD2VEC and GLOVE  vectors in Figure~\ref{app:fig:decay}. As a comparison, the empirical spectrum of  a  random Gaussian matrix  is also  plotted in Figure~\ref{app:fig:decay}.  We see that both spectra are  flat (since the vocabulary size is much larger than the dimension $d = 300$).  Thus the postprocessing operation can also be viewed as a way of making the vectors ``more isotropic''.


\paragraph{Mathematical Contribution} Under the revised postulate, we show that the main theorem in \citep{arora2015rand} (c.f.\ Theorem 2.2) still holds. Formally:  
\begin{theorem}
\label{thm:iso}
Suppose the word vectors satisfy the constraints. Then 
\begin{align}
  {\rm PMI}(w_1, w_2) \stackrel{{\rm def}}{=} \log \frac{p(w_1,w_2)}{p(w_1)p(w_2)}  \to \frac{v(w_1)^{\top}v(w_2)}{d},\quad  \textrm{ as $|\mathcal{V}|\to \infty$}, \label{eq:iso}
\end{align}
where $p(w)$ is the unigram distribution induced from the model \eqref{eq:model}, and $p(w_1,w_2)$ is the probability that two words $w_1$ and $w_2$ occur with each other within distance $q$.
\end{theorem}

The proof is in Appendix~\ref{app:iso}. Theorem~\ref{thm:iso} suggests that the RAND-WALK generative model and its properties proposed by \citep{arora2015rand} can be generalized to a broader setting (with a relaxed restriction on the geometry of word representations) -- relevantly, this relaxation on the geometry of word representations is empirically satisfied by the vectors learnt as part of the maximum likelihood rule. 



{
\section{Neural Networks Learn to Postprocess}
\label{app:neuralnet}


Every neural network family  posseses the ability to  conduct linear processing inside their nodes; this includes feedforward and recurrent and  convolutional neural network models. Thus, in principle, the  postprocessing operation  can be  ``learnt and implemented"  
within  the parameters of the neural network. On the other hand, due to the large number of parameters within the neural network, it is unclear how to verify such a process, even if it were learnt (only one of the layers might be implementing the postprocessing operation or via a combination of multiple effects). 


To address this issue, we have adopted a {\em comparative} approach  in the rest of this section. The comparative approach involves adding an {\em extra layer} interposed in between the inputs (which are word vectors) and the  rest of the neural network. This extra layer involves only linear processing. Next we compare the results of the  final  
parameters  of the extra layer  (trained jointly  with the rest of tne neural network parameters, using the end-to-end training examples) with and without  preprocessing of the  word vectors.  Such a comparative approach  allows us to separate the effect of the  postprocessing operation on the  word vectors  from the complicated ``semantics'' of the neural network parameters. 
 


\begin{figure}[htbp]
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=0.7\textwidth]{./figures/rnn.pdf}
\caption{Time-expanded RNN architecture with an appended layer involving linear bias. \label{fig:rnn}}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\centering
\resizebox{1\textwidth}{!}{
\begin{tabular}{|r|c|c|c|c|c|c|}
\hline
     & \multicolumn{2}{c|}{vanilla} & \multicolumn{2}{c|}{GRU} & \multicolumn{2}{c|}{LSTM} \\ \hline
     & W.            & G.           & W.          & G.         & W.          & G.          \\ \hline
MR   & 82.07         & 49.23        & 81.35       & 47.63      & 77.95       & 44.92       \\ \hline
SUBJ & 84.02         & 49.94        & 83.15       & 50.60      & 83.39       & 48.95       \\ \hline
TREC & 81.68         & 52.99        & 82.68       & 50.42      & 80.80       & 46.77       \\ \hline
SST  & 79.64         & 46.59        & 78.06       & 43.21      & 77.72       & 42.82       \\ \hline
IMDb & 93.48         & 66.37        & 94.49       & 55.24      & 87.27       & 46.74       \\ \hline
\end{tabular}}
\caption{The cosine similarity (x100) between $b_{\rm proc.} + \mu$ and  $b_{\rm orig.}$, where W. and G. stand for WORD2VEC and GLOVE respectively.}
\label{tb:biases}
\end{minipage}
\end{figure}


\paragraph{Experiment} We construct a modified neural network by explicitly adding a ``postprocessing unit" as the first layer of the RNN architecture (as in Figure~\ref{fig:rnn}, where the appended layer is used to test the first step (i.e., remove  the mean vector)) of the  postprocessing algorithm. 

In the modified neural network, the input word vectors are now $v(w) - b$ instead of $v(w)$. Here  $b$ is a bias vector trained {\em jointly} with the rest of the neural network parameters. Note that this is only a relabeling of the parameters from the perspective of the RNN architecture:   the nonlinear activation function of the node is now operated on $A(v(w)-b) + b' = Av(w) + (b'-Ab)$ instead of the previous $Av(w) + b'$. Let $b_{\rm proc.}$ and $b_{\rm orig.}$ be the inferred biases when using  the processed and original word representations, respectively. 

We itemize the cosine similarity between  $b_{\rm proc.} + \mu$ and  $b_{\rm orig.}$ in Table~\ref{tb:biases} for the 5 different datasets and 3 different neural network architectures. In each case,  the cosine similarity is remarkably large (on average 0.66, in 300 dimensions) -- in other words, trained neural networks   implicitly postprocess the word vectors nearly exactly as we proposed.
This  agenda is successfully implemented  in the  context of verifying the  removal of the mean vector.  

The second step of our postprocessing algorithm (i.e., nulling away from the top principal components) is equivalent to applying a projection matrix $P = I - \sum_{i=1}^D u_iu_i^{\top}$ on  the word vectors, where $u_i$ is the $i$-th principal component and $D$ is the number of the removed directions. 
 A comparative analysis effort for the second step  (nulling the dominant PCA directions) is the following. Instead of applying a bias term $b$, we multiply by a matrix $Q$ to simulate the projection operation. The input word vectors are now $Q_{\rm orig.}v(w)$ instead of $v(w)$ for the original word vectors, and $Q_{\rm proc.}Pv(w)$ instead of $Pv(w)$ for the processed vectors. Testing the similarity between $Q_{\rm orig.}P$ and $Q_{\rm proc.}$, allows us to verify if the neural network learns to conduct the projection operation as proposed. 
 
 In our experiment, we found that such a  result cannot be inferred. One possibility is that there are too many parameters in both $Q_{\rm proc.}$ and $Q_{\rm orig.}$, which adds  randomness to the experiment. Alternatively, the neural network weights may not be able to learn the second step of the postprocessing operation (indeed, in our experiments postprocessing significantly boosted end-performance of neural network architectures).  A more careful experimental setup  to test whether the second step of the postprocessing operation  is learnt is left  as a future research direction. 
 }
 
 \section{Experiments on Various Representations}
\label{app:extra}
In the main text, we have reported empirical results for two published word representations:    WORD2VEC and GLOVE, each in 300 dimensions. In this section, we report the results of the same  experiments in a variety of other settings to show the generalization capability of the postprocessing operation: representations trained via WORD2VEC and GLOVE algorithms in  dimensions other than 300, other representations algorithms (specifically TSCCA and RAND-WALK) and in  multiple languages.

\subsection{Statistics of Multilingual Word Representations}
We use the publicly available TSCCA representations \citep{dhillon2012two} on German, French, Spanish, Italian, Dutch and Chinese. The detailed statistics can be found in Table~\ref{tb:tscca} and the decay of their singular values are plotted in Figure~\ref{fig:tscca-decay}.

\begin{table*}[!h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
 &  \bf Language &  \bf Corpus &  \bf dim &  \bf vocab size &  \bf avg. $\|v(w)\|_2$ &  $\bf \|\mu\|_2$\\
\hline
TSCCA-En & English & Gigawords & 200 & 300,000 & 4.38 & 0.78 \\
TSCCA-De & German & Newswire & 200 & 300,000 & 4.52 & 0.79 \\
TSCCA-Fr & French & Gigaword & 200 & 300,000 & 4.34 & 0.81 \\
TSCCA-Es & Spanish & Gigaword & 200 & 300,000 & 4.17 & 0.79 \\
TSCCA-It & Italian & Newswire+Wiki & 200 & 300,000 & 4.34 & 0.79 \\
TSCCA-Nl & Dutch & Newswire+Wiki & 200 & 300,000 & 4.46 & 0.72 \\
TSCCA-Zh & Chinese & Gigaword & 200 & 300,000 & 4.51 & 0.89 \\
\hline 
\end{tabular}
\end{center}
\caption{A detailed description for the TSCCA embeddings in this paper.}
\label{tb:tscca}
\end{table*}


\begin{figure}[!h]
  \centering
  {
  \includegraphics[width = 0.4\textwidth]{figures/TSCCA-decay.pdf}
  }
  \caption{The decay of the normalized singular values of multilingual word representation.}
  \label{fig:tscca-decay}
\end{figure}


\subsection{Multilingual Generalization }
\label{app:multilingual}

In this section, we perform the word similarity task with the original and the processed TSCCA word representations in German and Spanish on three German similarity datasets (GUR65 -- a German version of the RG65 dataset, GUR350, and ZG222 in terms of relatedness) \citep{zesch2006automatically} and the Spanish version of  RG65 dataset \citep{camacho2015framework}.  The choice of $D$ is 2 for both German and Spanish.

The experiment setup and the similarity scoring algorithm are the same as those in Section~\ref{sec:experiments}. The detailed experiment results are provided in Table~\ref{tb:similarity-multilingual}, from which we observe that the processed representations are consistently better than the original ones. This provides evidence to the generalization capabilities of the  postprocessing operation  to  multiple languages (similarity datasets in Spanish and German were the only ones we could locate).  

\begin{table}[!h]
\centering
\begin{tabular}{|r|l|l|l|}
\hline
\multirow{2}{*}{} & \multirow{2}{*}{language}   & \multicolumn{2}{l|}{TSCCA} \\ \cline{3-4} 
                  &                             & orig.        & proc.       \\ \hline
RG65              & Spanish                     & 60.33             &    \bf 60.37         \\ \hline
GUR65             & \multicolumn{1}{c|}{German} & 61.75             &    \bf 64.39         \\ \hline
GUR350            & German                      & 44.91             &    \bf 46.59         \\ \hline
ZG222             & German                      & 30.37             &    \bf 32.92         \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the word similarity task in multiple languages.}
\label{tb:similarity-multilingual}
\end{table}

\subsection{Generalization to Different Representation Algorithms}
Given the popularity and widespread use of WORD2VEC \citep{mikolov2013efficient} and GLOVE \citep{pennington2014glove}, the main text has solely focused on their published publicly avalable 300-dimension representations. In this section, we show that the proposed postprocessing algorithm generalizes to other representation methods. Specifically, we demonstrate this on RAND-WALK (obtained via personal communication) and TSCCA (publicly available) on all the experiments of  Section 3. The choice of $D$ is 2 for both RAND-WALK and  TSCCA.

In summary, the performance improvements on the similarity task, the concept categorization task, the analogy task, and the semantic textual similarity dataset are on average 2.23\%, 2.39\%, 0.11\% and 0.61\%, respectively. The detailed statistics are provided in Table~\ref{tb:similarity-other}, Table~\ref{tb:categorization-other}, Table~\ref{tb:analogy-other} and Table~\ref{tb:sts-other}, respectively. These  results are a testament to the generalization capabilities of the postprocessing algorithm to other representation algorithms.  

\begin{table}[!h]
\centering
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c||}{RAND-WALK} & \multicolumn{2}{c|}{TSCCA} \\ \cline{2-5} 
                  & orig.      & proc.     & orig.    & proc.    \\ \hline
RG65  &     80.66 &     \bf  82.96 &   47.53 &   \bf 47.67       \\ \hline
WS    &     65.89 &     \bf  74.37 &   54.21  &  \bf 54.35 \\ \hline
RW    &     45.11 &     \bf 51.23 &  \bf 43.96   & 43.72   \\ \hline
MEN   &     73.56 &     \bf 77.22 &  65.48   &  \bf65.62     \\ \hline
MTurk &     64.35 &     \bf 66.11 &  59.65  &   \bf 60.03\\ \hline
SimLex  &   34.05 &   \bf 36.55   &   34.86 & \bf 34.91      \\ \hline
SimVerb &   16.05 &  \bf 21.84   &  23.79   &  \bf 23.83     \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the word similarity task on seven datasets.}
\label{tb:similarity-other}
\end{table}

\begin{table}[!h]
\centering
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c||}{RAND-WALK} & \multicolumn{2}{c|}{TSCCA} \\ \cline{2-5} 
                  & orig.      & proc.     & orig.    & proc.    \\ \hline
ap     &     59.83 &    \bf  62.36 &  60.00 & {\bf 63.42}    \\ \hline
esslli &     72.73 &     72.73  &     68.18 & \bf 70.45\\ \hline
battig &     75.73 &    \bf  81.82 &  70.73 & 70.73      \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the categorization task.}
\label{tb:categorization-other}
\end{table}

\begin{table}[!h]
\centering
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c||}{RAND-WALK} & \multicolumn{2}{c|}{TSCCA} \\ \cline{2-5} 
                  & orig.      & proc.     & orig.    & proc.    \\ \hline
syn. &     60.39 &      \bf 60.48 &  37.72   & \bf 37.80   \\ \hline
sem. &     83.55 &    \bf  83.82 &   14.54  & \bf 14.55  \\ \hline
all  &     70.50 &    \bf 70.67 &   27.30  & \bf 27.35  \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the word analogy task.}
\label{tb:analogy-other}
\end{table}


\begin{table}[!h]
\centering
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c||}{RAND-WALK} & \multicolumn{2}{c|}{TSCCA} \\ \cline{2-5} 
                  & orig.      & proc.     & orig.    & proc.    \\ \hline
2012 &     \bf 38.03 &      37.66 &  44.51   &  \bf 44.63    \\ \hline
2013 &     \bf 37.47 &      36.85 &  \bf 43.21   & \bf 42.74    \\ \hline
2014 &     46.06 &      \bf 48.32 &  52.85  &  \bf 52.87    \\ \hline
2015 &     47.82 &      \bf 51.76 &   \bf 56.22   &  56.14     \\ \hline
SICK &     51.58 &      \bf  51.76 &  \bf 56.15  &  56.11    \\ \hline
all  &     43.48 &      \bf 44.67 &  50.01   &    \bf 50.23  \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the semantic textual similarity tasks.}
\label{tb:sts-other}
\end{table}

\subsection{Role of Dimensions}
The main text has focused on the dimension choice of $d=300$, due to its popularity.  In this section we explore the role of the dimension in terms of both choice of $D$ and  the performance of the postprocessing operation -- we do this by using skip-gram model on the 2010 snapshot of Wikipedia corpus \citep{al2013polyglot} to train word representations. We first observe that the two phenomena of Section 2  continue to hold: 
\begin{itemize}
\item From Table~\ref{tb:high-dim-stat} we  observe that the ratio between the norm of $\mu$ and the norm average of all $v(w)$ spans from 1/3 to 1/4;
\begin{table}[!h]
\centering
\begin{tabular}{|l||l|l|l|l|l|l|l|l|}
\hline
 dim               & 300  & 400  & 500  & 600  & 700  & 800  & 900  & 1000 \\ \hline
 avg. $\|v(w)\|_2$ & 4.51 & 5.17 & 5.91 & 6.22 & 6.49 & 6.73 & 6.95 & 7.15 \\ \hline
$\|\mu\|_2$           & 1.74 & 1.76 & 1.77 & 1.78 & 1.79 & 1.80 & 1.81 & 1.83 \\ \hline
\end{tabular}
\caption{Statistics on word representation of dimensions 300, 400, ..., and 1000 using the skip-gram model.}
\label{tb:high-dim-stat}
\end{table}
\item From Figure~\ref{fig:high-dim-decay} we  observe that the decay of the variance ratios $\sigma_i$ is near exponential for small values of $i$ and remains roughly constant over the later ones. 
\begin{figure}[!h]
\centering
\includegraphics[width=0.45\textwidth]{./figures/high-dim-decay.pdf}
\caption{The decay of the normalized singular values of word representations.}
\label{fig:high-dim-decay}
\end{figure}
\end{itemize}

A rule of thumb choice of $D$ is around $d/100$. We validate this claim empirically by performing the tasks in Section 3 on word representations of higher dimensions, ranging from 300 to 1000, where we set the parameter $D = d/100$.  
In summary, the performance improvement on the four itemized tasks of Section~3 are 2.27\%, 3.37\%, 0.01 and 1.92\% respectively;  the detailed results can be found in  Table~\ref{tb:high-dim-similarity}, Table~\ref{tb:high-dim-categorization}, Table~\ref{tb:high-dim-analogy}, and Table~\ref{tb:high-dim-sts}. Again, note that the improvement for analogy tasks is marginal.  These experimental results justify the rule-of-thumb setting of $D = d/100$, although we emphasize that the  improvements can be further accentuated by tuning the choice of $D$ based on the specific setting. 

\begin{table}[!h]
\centering
\begin{tabular}{|r|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\bf Dim} & \multicolumn{2}{c|}{300} & \multicolumn{2}{c|}{400} & \multicolumn{2}{c|}{500} & \multicolumn{2}{c|}{600}  \\ \cline{2-9} 
                         & orig.  & proc.           & orig.  & proc.           & orig.  & proc.           & orig.   & proc.           \\ \hline
RG65                     & 73.57  & \textbf{74.72}  & 75.64  & \textbf{79.87}  & 77.72  & \textbf{81.97}  & 77.59   & \textbf{80.7}   \\ \hline
WS                       & 70.25  & \textbf{71.95}  & 70.8   & \textbf{72.88}  & 70.39  & \textbf{72.73}  & 71.64   & \textbf{74.04}  \\ \hline
RW                       & 46.25  & \textbf{49.11}  & 45.97  & \textbf{47.63}  & 46.6   & \textbf{48.59}  & 45.7    & \textbf{47.81}  \\ \hline
MEN                      & 75.66  & \textbf{77.59}  & 76.07  & \textbf{77.89}  & 75.9   & \textbf{78.15}  & 75.88   & \textbf{78.15}  \\ \hline
Mturk                    & 75.66  & \textbf{77.59}  & 67.68  & \textbf{68.11}  & 66.89  & \textbf{68.25}  & 67.6    & \textbf{67.87}  \\ \hline
SimLex                   & 34.02  & \textbf{36.19}  & 35.17  & \textbf{37.1}   & 35.73  & \textbf{37.65}  & 35.76   & \textbf{38.04}  \\ \hline
SimVerb                  & 22.22  & \textbf{24.98}  & 22.91  & \textbf{25.32}  & 23.03  & \textbf{25.82}  & 23.35   & \textbf{25.97}  \\ \hline
\multirow{2}{*}{\bf Dim} & \multicolumn{2}{c|}{700} & \multicolumn{2}{c|}{800} & \multicolumn{2}{c|}{900} & \multicolumn{2}{c|}{1000} \\ \cline{2-9} 
                         & orig.  & proc.           & orig.  & proc.           & orig.  & proc.           & orig.   & proc.           \\ \hline
RG65                     & 77.3   & \textbf{81.07}  & 77.52  & \textbf{81.07}  & 79.75  & \textbf{82.34}  & 78.18   & \textbf{79.07}  \\ \hline
WS                       & 70.31  & \textbf{73.02}  & 71.52  & \textbf{74.65}  & 71.19  & \textbf{73.06}  & 71.5    & \textbf{74.78}  \\ \hline
RW                       & 45.86  & \textbf{48.4}   & 44.96  & \textbf{49}     & 44.44  & \textbf{49.22}  & 44.5    & \textbf{49.03}  \\ \hline
MEN                      & 75.84  & \textbf{78.21}  & 75.84  & \textbf{77.96}  & 76.16  & \textbf{78.35}  & 76.72   & \textbf{78.1}   \\ \hline
Mturk                    & 67.47  & \textbf{67.79}  & 67.67  & \textbf{68}     & 67.98  & \textbf{68.87}  & 68.34   & \textbf{69.44}  \\ \hline
SimLex                   & 35.3   & \textbf{37.59}  & 36.54  & \textbf{37.85}  & 36.62  & \textbf{38.44}  & 36.67   & \textbf{38.58}  \\ \hline
SimVerb                  & 22.81  & \textbf{25.6}   & 23.48  & \textbf{25.57}  & 23.68  & \textbf{25.76}  & 23.24   & \textbf{26.58}  \\ \hline
\end{tabular}
\caption{Before-After results (x100) on word similarity task on seven datasets.}
\label{tb:high-dim-similarity}
\end{table}

\begin{table}[!h]
\centering
\begin{tabular}{|r|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\bf Dim} & \multicolumn{2}{c|}{300} & \multicolumn{2}{c|}{400}        & \multicolumn{2}{c|}{500}        & \multicolumn{2}{c|}{600}        \\ \cline{2-9} 
                         & orig.  & proc.           & orig.          & proc.          & orig.          & proc.          & orig.          & proc.          \\ \hline
ap                       & 46.1   & \textbf{48.61}  & 42.57          & \textbf{45.34} & 46.85          & \textbf{50.88} & 40.3           & \textbf{45.84} \\ \hline
esslli                   & 68.18  & \textbf{72.73}  & 64.2           & \textbf{82.72} & 64.2           & \textbf{65.43} & 65.91          & \textbf{72.73} \\ \hline
battig                   & 71.6   & \textbf{77.78}  & 68.18          & \textbf{75}    & 68.18          & \textbf{70.45} & 46.91          & \textbf{66.67} \\ \hline
\multirow{2}{*}{\bf Dim} & \multicolumn{2}{c|}{700} & \multicolumn{2}{c|}{800}        & \multicolumn{2}{c|}{900}        & \multicolumn{2}{c|}{1000}       \\ \cline{2-9} 
                         & orig.  & proc.           & orig.          & proc.          & orig.          & proc.          & orig.          & proc.          \\ \hline
ap                       & 38.04  & \textbf{41.31}  & 34.76          & \textbf{39.8}  & \textbf{34.76} & 27.46          & 27.96          & \textbf{28.21} \\ \hline
esslli                   & 54.55  & 54.55           & \textbf{68.18} & 56.82          & 72.73          & 72.73          & 52.27          & 52.27          \\ \hline
battig                   & 62.96  & \textbf{66.67}  & 67.9           & \textbf{69.14} & 49.38          & \textbf{59.26} & \textbf{51.85} & 46.91          \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the categorization task.}
\label{tb:high-dim-categorization}
\end{table}

\begin{table}[!h]
\centering
\begin{tabular}{|r|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\bf Dim} & \multicolumn{2}{c|}{300} & \multicolumn{2}{c|}{400}        & \multicolumn{2}{c|}{500}        & \multicolumn{2}{c|}{600}        \\ \cline{2-9} 
                         & orig.  & proc.           & orig.          & proc.          & orig.          & proc.          & orig.          & proc.          \\ \hline
syn.                     & 60.48  & \textbf{60.52}  & \textbf{61.61} & 61.45          & \textbf{60.93} & 60.84          & \textbf{61.66} & 61.57          \\ \hline
sem.                     & 74.51  & \textbf{74.54}  & 77.11          & \textbf{77.36} & 76.39          & \textbf{76.89} & 77.28          & \textbf{77.61} \\ \hline
all.                     & 66.86  & \textbf{66.87}  & 68.66          & \textbf{68.69} & 67.88          & 68.11          & 68.77          & \textbf{68.81} \\ \hline
\multirow{2}{*}{\bf Dim} & \multicolumn{2}{c|}{700} & \multicolumn{2}{c|}{800}        & \multicolumn{2}{c|}{900}        & \multicolumn{2}{c|}{1000}       \\ \cline{2-9} 
                         & orig.  & proc.           & orig.          & proc.          & orig.          & proc.          & orig.          & proc.          \\ \hline
syn.                     & 60.94  & \textbf{61.02}  & \textbf{68.38} & 68.34          & 60.47          & \textbf{60.30} & \textbf{67.56} & 67.30          \\ \hline
sem.                     & 77.24  & \textbf{77.26}  & 77.24          & \textbf{77.35} & 76.76          & \textbf{76.90} & \textbf{76.71} & 76.51          \\ \hline
all.                     & 68.36  & \textbf{68.41}  & 68.38          & \textbf{68.50} & \textbf{67.91} & 67.67          & \textbf{67.56} & 67.30          \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the word analogy task.}
\label{tb:high-dim-analogy}
\end{table}


\begin{table}[!h]
\centering
\begin{tabular}{|r|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\bf Dim} & \multicolumn{2}{c|}{300} & \multicolumn{2}{c|}{400} & \multicolumn{2}{c|}{500} & \multicolumn{2}{c|}{600}  \\ \cline{2-9} 
                         & orig.  & proc.           & orig.  & proc.           & orig.  & proc.           & orig.   & proc.           \\ \hline
2012                     & 54.51  & \textbf{54.95}  & 54.31  & \textbf{54.57}  & 55.13  & \textbf{56.23}  & 55.35   & \textbf{56.03}  \\ \hline
2013                     & 56.58  & \textbf{57.89}  & 56.35  & \textbf{57.35}  & 57.55  & \textbf{59.38}  & 57.43   & \textbf{59.00}  \\ \hline
2014                     & 59.6   & \textbf{61.92}  & 59.57  & \textbf{61.62}  & 61.19  & \textbf{64.38}  & 61.10   & \textbf{63.86}  \\ \hline
2015                     & 59.65  & \textbf{61.48}  & 59.69  & \textbf{61.19}  & 61.63  & \textbf{64.77}  & 61.42   & \textbf{64.04}  \\ \hline
SICK                     & 68.89  & \textbf{70.79}  & 60.6   & \textbf{70.27}  & 68.63  & \textbf{71.00}  & 68.58   & \textbf{70.57}  \\ \hline
all                      & 58.32  & \textbf{59.91}  & 58.25  & \textbf{59.55}  & 59.61  & \textbf{62.02}  & 59.57   & \textbf{61.55}  \\ \hline
\multirow{2}{*}{\bf Dim} & \multicolumn{2}{c|}{700} & \multicolumn{2}{c|}{800} & \multicolumn{2}{c|}{900} & \multicolumn{2}{c|}{1000} \\ \cline{2-9} 
                         & orig.  & proc.           & orig.  & proc.           & orig.  & proc.           & orig.   & proc.           \\ \hline
2012                     & 55.52  & \textbf{56.49}  & 54.47  & \textbf{54.85}  & 54.69  & \textbf{55.18}  & 54.34   & \textbf{54.78}  \\ \hline
2013                     & 57.61  & \textbf{59.31}  & 56.75  & \textbf{57.62}  & 56.98  & \textbf{58.26}  & 56.78   & \textbf{57.73}  \\ \hline
2014                     & 61.57  & \textbf{64.77}  & 60.51  & \textbf{62.83}  & 60.89  & \textbf{63.34}  & 60.78   & \textbf{63.03}  \\ \hline
2015                     & 62.05  & \textbf{65.45}  & 60.74  & \textbf{62.84}  & 61.09  & \textbf{63.48}  & 60.92   & \textbf{63.03}  \\ \hline
SICK                     & 68.38  & \textbf{70.63}  & 67.94  & \textbf{69.59}  & 67.86  & \textbf{69.5}   & 67.58   & \textbf{69.16}  \\ \hline
all                      & 59.96  & \textbf{62.34}  & 58.87  & \textbf{60.39}  & 59.16  & \textbf{60.88}  & 58.94   & \textbf{60.48}  \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the semantic textual similarity tasks.}
\label{tb:high-dim-sts}
\end{table}


 
 \section{Experiments on Word Analogy Task}
\label{app:analogy}

The detailed performance on the analogy task is provided in Table~\ref{tb:analogy:detail}.

\begin{table}[thbp]
\centering
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{}           & \multicolumn{2}{c||}{WORD2VEC}       & \multicolumn{2}{c|}{GLOVE}      \\ \cline{2-5} 
                            & orig.            & proc.            & orig.          & proc.          \\ \hline
capital-common-countries    & 82.01            & \textbf{83.60}   & 95.06          & 95.96          \\ \hline
capital-world               & 78.38            & \textbf{80.08}   & 91.89          & \textbf{92.31} \\ \hline
city-in-state               & 69.56            & \textbf{69.88}   & 69.56          & \textbf{70.45} \\ \hline
currency                    & 32.43            & \textbf{32.92} & \textbf{21.59} & 21.36          \\ \hline
family                      & \textbf{84.98} & 84.59            & \textbf{95.84} & 95.65          \\ \hline
gram1-adjective-to-adverb   & \textbf{28.02} & 27.72            & \textbf{40.42} & 39.21          \\ \hline
gram2-opposite              & 40.14            & \textbf{40.51} & \textbf{31.65} & 30.91          \\ \hline
gram3-comparative           & 89.19            & \textbf{89.26} & 86.93          & \textbf{87.09} \\ \hline
gram4-superlative           & 82.71            & \textbf{83.33} & 90.46          & \textbf{90.59} \\ \hline
gram5-present-participle    & 79.36            & \textbf{79.64} & \textbf{82.95} & 82.76          \\ \hline
gram6-nationality-adjective & 90.24            & \textbf{90.36} & 90.24          & 90.24          \\ \hline
gram7-past-tense            & 66.03            & \textbf{66.53} & 63.91          & \textbf{64.87} \\ \hline
gram8-plural                & 91.07            & \textbf{90.61} & 95.27          & \textbf{95.36} \\ \hline
gram9-plural-verbs          & \textbf{68.74} & 67.58            & 67.24          & \textbf{68.05} \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the word analogy task.}
\label{tb:analogy:detail}
\end{table}

\section{Experiments on Semantic Textual Similarity Task}
\label{app:sts}

The detailed performance on the semantic textual similarity is provided in Table~\ref{tb:sts:detail}.



\begin{table}[!h]
\centering
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{}     & \multicolumn{2}{c||}{WORD2VEC}                                    & \multicolumn{2}{c|}{GLOVE}                                       \\ \cline{2-5} 
                      & orig.                      & proc.                               & orig.                      & proc.                               \\ \hline
2012.MSRpar           & 42.12                      & \textbf{43.85}                      & \textbf{44.54}             & 44.09                               \\ \hline
2012.MSRvid           & 72.07                      & \textbf{72.16}                      & 64.47                      & \textbf{68.05}                      \\ \hline
2012.OnWN             & 69.38                      & \textbf{69.48}                      & 53.07                      & \textbf{65.67}                      \\ \hline
2012.SMTeuroparl      & 53.15                      & \textbf{54.32}                      & 41.74                      & \textbf{45.28}                      \\ \hline
2012.SMTnews          & \textbf{49.37}             & 48.53                               & 37.54                      & \textbf{47.22}                      \\ \hline
2013.FNWN             & 40.70                      & \textbf{41.96}                      & 37.54                      & \textbf{39.34}                      \\ \hline
2013.OnWN             & 67.87                      & \textbf{68.17}                      & 47.22                      & \textbf{58.60}                      \\ \hline
2013.headlines        & \multicolumn{1}{l|}{61.88} & \multicolumn{1}{l|}{\textbf{63.81}} & \multicolumn{1}{l|}{49.73} & \multicolumn{1}{l|}{\textbf{57.20}} \\ \hline
2014.OnWN             & 74.61                      & \textbf{74.78}                      & 57.41                      & \textbf{67.56}                      \\ \hline
2014.deft-forum       & 32.19                      & \textbf{33.26}                      & 21.55                      & \textbf{29.39}                      \\ \hline
2014.deft-news        & \textbf{66.83}             & 65.96                               & 65.14                      & \textbf{71.45}                      \\ \hline
2014.headlines        & 58.01                      & \textbf{59.58}                      & 47.05                      & \textbf{52.60}                      \\ \hline
2014.images           & 73.75                      & \textbf{74.17}                      & 57.22                      & \textbf{68.28}                      \\ \hline
2014.tweet-news       & 71.92                      & \textbf{72.07}                      & 58.32                      & \textbf{66.13}                      \\ \hline
2015.answers-forum    & 46.35                      & \textbf{46.80}                      & 30.02                      & \textbf{39.86}                      \\ \hline
2015.answers-students & \textbf{68.07}             & 67.99                               & 49.20                      & \textbf{62.38}                      \\ \hline
2015.belief           & 59.72                      & \textbf{60.42}                      & 44.05                      & \textbf{57.68}                      \\ \hline
2015.headlines        & 61.47                      & \textbf{63.45}                      & 46.22                      & \textbf{53.31}                      \\ \hline
2015.images           & 78.09                      & \textbf{78.08}                      & 66.63                      & \textbf{73.20}                      \\ \hline
SICK                  & 70.10                      & \textbf{70.20}                      & 65.14                      & \textbf{67.85}                      \\ \hline
\end{tabular}
\caption{Before-After results (x100) on the semantic textual similarity tasks.}
\label{tb:sts:detail}
\end{table}



\section{Statistics of Text Classification Datasets}
\label{app:sentiment}
We evaluate the word representations (with and without postprocessing) using  four different neural network architectures (CNN, vanilla-RNN, GRU-RNN and LSTM-RNN) on  five benchmarks: 
\begin{itemize}
\item the movie review (MR) dataset \citep{pang2005seeing} where each review is composed by only one sentence; 
\item the subjectivity (SUBJ) dataset \citep{pang2004sentimental}  where the algorithm needs to decide whether a sentence is subjective or objective; 
\item  the TREC question dataset \citep{li2002learning} where all the questions in this dataset has to be partitioned into six categories; 
\item  the IMDb dataset \citep{maas2011learning} -- each review consists of several sentences; 
\item the Stanford sentiment treebank (SST) dataset \citep{socher2013reasoning}, where we only use the full sentences as the training data. 
\end{itemize}

In TREC, SST and IMDb, the datasets have already been split into train/test sets. Otherwise we use 10-fold cross validation in the remaining datasets (i.e., MR and SUBJ). Detailed statistics of various features of each of the datasets are provided in Table~\ref{tb:sentiment-analysis-dataset}.

\begin{table}[!h]
\centering
\begin{tabular}{|r|l|l|l|l|}
\hline
     & $c$ & $l$  & Train    & Test   \\ \hline
MR   & 2   & 20   & 10,662 & 10-fold cross validation     \\ \hline
SUBJ & 2   & 23   & 10,000 & 10-fold cross validation     \\ \hline
TREC & 6   & 10   & 5,952  & 500    \\ \hline
SST  & 5   & 18   & 11,855 & 2,210  \\ \hline
IMDb & 2   & 100 & 25,000 & 25,000 \\ \hline
\end{tabular}
\caption{Statistics for the five datasets after tokenization: $c$ represents the number of classes; $l$ represents the average sentence length; Train represents the size of the training set; and Test represent the size of the test set.}
\label{tb:sentiment-analysis-dataset}
\end{table}


\section{Proof of Theorem~\ref{thm:iso}}
\label{app:iso}


Given the similarity between the setup in Theorem 2.2 in \citep{arora2015rand} and Theorem~\ref{thm:iso}, many parts of the original proof can be reused except one key aspect -- the concentration of $Z(c)$. We summarize this part in the following lemma:
\begin{lemma}
\label{lemma}
Let $c$ be a random variable uniformly distributed over the unit sphere, we prove that with high probability, $Z(c) / |\mathcal{V}|$ converges to a constant $Z$:
    \begin{align*}
    p((1-\epsilon_z)Z \le Z(c) \le (1+\epsilon_z) Z) \ge 1-\delta,
\end{align*}
where $\epsilon_z = \Omega((D+1)/|\mathcal{V}|)$ and $\delta = \Omega((DA^2 + \|\mu\|^2)/d)$.
\end{lemma}
Our proof differs from the one in \citep{arora2015rand} in two ways: (a) we treat  $v(w)$ as deterministic parameters instead of random variables and prove the Lemma by showing a certain concentration of measure;  (b) the asymmetric parts $\mu$ and $u_1$,...,$u_D$, (which did not exist in the original proof),   need to be carefully addressed to complete the proof.

\subsection{Proof of Lemma \ref{lemma}}
Given the constraints on the word vectors \eqref{eq:word-vec}, the partition function $Z(c)$ can be rewritten as,
\begin{align*}
    Z(c) &= \sum_{v\in \mathcal{V}} \exp(c^{\top} v(w)) \\
    &= \sum_{v\in \mathcal{V}} \exp\left(c^{\top} \left(\mu  + \sum_{i=1}^D \alpha_i(w) u_i + \tilde{v}(w) \right)\right) \\
    &=  \sum_{v\in \mathcal{V}} \exp(c^{\top} \mu) \left[ \prod_{i=1}^D \exp(\alpha_i(w) c^{\top} u_i) \right] \exp\left(c^{\top} \tilde{v}(w)\right).
\end{align*}
The equation above suggests that we can divide the proof into five parts.

\paragraph{Step 1:} for every unit vector $c$, one has,
\begin{align}
    \frac{1}{|\mathcal{V}|}\sum_{w\in \mathcal{V}} \exp\left(c^{\top} \tilde{v}(w)\right) \to \mathbb{E}_{f} \left(\exp\left(c^{\top} \tilde{v} \right)\right), \textrm{ as } |\mathcal{V}| \to \infty. \label{eq:converge}
\end{align}
\begin{proof}
    Let $M, N$ be a positive integer, and let $A_M\subset \mathbb{R}^d$ such that,
    \begin{align*}
        A_{M, N} = \left\{\tilde{v} \in \mathbb{R}^d: \frac{M-1}{N}<\exp(c^{\top} \tilde{v}) \le \frac{M}{N}\right\}.
    \end{align*}
    Since $A_{M,N}$ can be represented by a union of countable disjoint rectangles, we know that for every $M, N \in \mathbb{N}_+$,
    \begin{align*}
        \frac{1}{|\mathcal{V}|}\sum_{w\in \mathcal{V}} \mathbf{1}(\tilde{v}(w) \in A_{M,N}) = \int_{A_{M,N}} f(\tilde{v}) d\tilde{v}. 
    \end{align*}
    Further, since $A_{M,N}$ are disjoint for different $M$'s and $\mathbb{R}^d = \cup_{M=1}^{\infty} A_{M,N}$, one has,
    \begin{align*}
        \frac{1}{|\mathcal{V}|}\sum_{w\in \mathcal{V}} \exp\left(c^{\top} \tilde{v}(w)\right) =& \sum_{M=1}^{\infty} \frac{1}{|\mathcal{V}|}\sum_{w\in \mathcal{V}} \mathbf{1}(\tilde{v}(w)\in A_{M,N}) \exp(c^{\top}  \tilde{v}(w)) \\
        \le & \sum_{M=1}^{\infty} \frac{1}{|\mathcal{V}|}\sum_{w\in \mathcal{V}} \mathbf{1}(\tilde{v}(w)\in A_{M,N}) \frac{M}{N} \\
        \to & \sum_{M=1}^{\infty} \frac{M}{N} \int_{A_{M,N}} f(\tilde{v})d\tilde{v}.
   \end{align*}
   The above statement holds for every $N$. Let $N\to \infty$, by definition of integration, one has,
   \begin{align*}
        \lim_{N\to\infty} \sum_{M=1}^{\infty} \frac{M}{N} \int_{A_{M,N}} f(\tilde{v})d\tilde{v} = \mathbb{E}_{f} \left(\exp\left(c^{\top} \tilde{v} \right)\right), 
   \end{align*}
    which yields,
    \begin{align}
        \frac{1}{|\mathcal{V}|}\sum_{w\in \mathcal{V}} \exp\left(c^{\top} \tilde{v}(w)\right) \le \mathbb{E}_{f} \left(\exp\left(c^{\top} \tilde{v} \right)\right), \textrm{ as }|\mathcal{V}| \to \infty.\label{eq:converge-rhs}
    \end{align}
    Similarly, one has,
    \begin{align}
        \frac{1}{|\mathcal{V}|}\sum_{w\in \mathcal{V}} \exp\left(c^{\top} \tilde{v}(w)\right) &\ge \lim_{N\to\infty} \sum_{M=1}^{\infty} \frac{M-1}{N} \int_{A_{M,N}} f(\tilde{v})d\tilde{v} \nonumber \\
        &= \mathbb{E}_{f} \left(\exp\left(c^{\top} \tilde{v} \right)\right), \textrm{ as }|\mathcal{V}| \to \infty. \label{eq:converge-lhs}
    \end{align}
    Putting \eqref{eq:converge-rhs} and \eqref{eq:converge-lhs} proves \eqref{eq:converge}.
\end{proof}
\paragraph{Step 2:} the expected value, $\mathbb{E}_{f} \left(\exp\left(c^{\top} \tilde{v} \right)\right)$ is a constant independent of $c$:  
\begin{align}
    \mathbb{E}_{f} \left(\exp\left(c^{\top} \tilde{v} \right)\right) = Z_0. \label{eq:constant}
\end{align}
\begin{proof}
    Let $Q\in \mathbb{R}^{d\times d}$ be a orthonormal matrix such that $Q^{\top}c_0 = c$ where $c_0 = (1,0,...,0)^{\top}$ and $\det(Q) = 1$, then we have $f(\tilde{v}) = f(Q\tilde{v})$, and,
    \begin{align*}
        \mathbb{E}_{f} \left(\exp\left(c_0^{\top} \tilde{v} \right)\right) &= \int_{\tilde{v}} f(\tilde{v}) \exp\left(c_0^{\top} \tilde{v} \right) d\tilde{v} \\
        &= \int_{\tilde{v}} f(Q\tilde{v}) \exp\left(c^{\top} Q\tilde{v} \right) \det(Q) d\tilde{v} \\
        &= \int_{\tilde{v}'} f(\tilde{v}') \exp\left(c^{\top}\tilde{v}' \right) d\tilde{v}' = \mathbb{E}_{f} \left(\exp\left(c^{\top} \tilde{v} \right)\right),
    \end{align*}
    which proves \eqref{eq:constant}.
\end{proof}
\paragraph{Step 3:} for any vector $\mu$, one has the following concentration property,
\begin{align}
    p\left(|\exp\left(c^{\top}\mu\right) - 1| > k\right) \le 2\left[\exp\left(-\frac{1}{4}\right) + \frac{\|\mu\|^2}{d-1}\frac{1}{\log^2(1-k)}\right] \label{eq:concentrate}
\end{align}
\begin{proof}
    Let $c_1$,...,$c_d$ be i.i.d.\ $\mathcal{N}(0, 1)$, and let $C = \sum_{i=1}^d c_i^2$, then $c = (c_1,...,c_d) / \sqrt{C}$ is uniform over unit sphere. Since $c$ is uniform, then without loss of generality we can consider  $\mu = (\|\mu\|, 0, ..., 0)$. Thus it suffices to bound $\exp\left(\|\mu\|c_1 / \sqrt{C}\right)$. We divide the proof into the following steps:
\begin{itemize}
\item $C$ follows chi-square distribution with the degree of freedom of $d$, thus $C$ can be bounded by \citep{laurent2000adaptive},
    \begin{align}
        p(C \ge d + 2\sqrt{dx} + 2x) \le  \exp(-x), \forall x > 0. \label{eq:c-bound-upper} \\
        p(C \le d - 2\sqrt{dx}) \le \exp(-x), \forall x>0. \label{eq:c-bound-lower}
    \end{align}
    \item Therefore for any $x>0$, one has,
    \begin{align*}
        p\left(|C - d| \ge 2\sqrt{dx} \right) \le \exp(-x)
    \end{align*}
   Let $x = 1/4d$, one has,
   \begin{align*}
    p(C > d+1) \le \exp\left(-\frac{1}{4d}\right), \\
    p(C < d-1) \le \exp\left(-\frac{1}{4d}\right).
   \end{align*}
    \item Since $c_1$ is a Gaussian random variable with variance $1$, by Chebyshev's inequality, one has,
    \begin{align*}
        p\left(yc_i \ge k\right) \le \frac{y^2}{k^2}, \qquad p\left(yc_i \le -k\right) \le \frac{y^2}{k^2}, \forall k > 0
    \end{align*}
    and therefore thus,
    \begin{align*}
        p(\exp(yc_i) -1 > k) &\le \frac{y^2}{\log^2(1+k)}, \\
        p(\exp(yc_i) -1 < -k) &\le \frac{y^2}{\log (1-k)^2}, \ \forall k > 0.
    \end{align*}
    \item Therefore we can bound $\exp\left(\|\mu\|c_1 / \sqrt{C}\right)$ by,
    \begin{align*}
        p\left(\exp\left(\frac{\|\mu\|c_1}{ \sqrt{C}}\right) -1 > k \right) \le & p\left(C > d+1\right) \\
        & + p\left(\left.\exp\left(\frac{\|\mu\|c_1}{ \sqrt{C}}\right) -1 > k \right| C < d+1\right)p\left(C < d+1\right) \\
        \le & \exp\left(-\frac{1}{4d}\right) + p\left(\exp\left(\frac{\|\mu\|c_1}{ \sqrt{d+1}}\right) -1 > k \right) \\
        =& \exp\left(-\frac{1}{4d}\right) + \frac{\|\mu\|^2}{d+1}\frac{1}{\log(1-k)^2}. \\
        p\left(\exp\left(\frac{\|\mu\|c_1}{ \sqrt{C}}\right) -1 < -k \right) \le & \exp\left(-\frac{1}{4d}\right) + \frac{\|\mu\|^2}{d-1} \frac{1}{\log^2(1+k)}.
    \end{align*}
    Combining the two inequalities above, one has \eqref{eq:concentrate} proved.
\end{itemize}
\paragraph{Step 4:} We are now ready to prove convergence of $Z(c)$. With \eqref{eq:concentrate}, let $\mathcal{C}\subset\mathbb{R}^d$ such that,
\begin{align*}
    \mathcal{C} = \left\{c:\left|\exp(c^{\top} \mu) - 1 \right| < k, \left| \exp(A c^{\top} u_i) - 1\right| < k , \left| \exp(-A c^{\top} u_i) - 1\right| < k\ \forall i=1,...,D \right\}
\end{align*}
Then we can bound the probability on $\mathcal{C}$ by,
\begin{align*}
  p(\mathcal{C}) 
    &\ge p\left(\left|\exp(c^{\top} \mu) - 1 \right| < k\right)  + \sum_{i=1}^D p(\left| \exp(A c^{\top} u_i) - 1\right| < k) - 2D \\
    &\ge 1 - (2D+1) \exp\left(-\frac{1}{4d}\right) - \frac{2DA^2}{d-1}\frac{1}{\log^2(1-k)} - \frac{\|\mu\|^2}{d-1}\frac{1}{\log^2(1-k)}.
\end{align*}
Next, we need to show that for every $w$, the corresponding $\mathcal{C}(w)$, i.e.,
\begin{align*}
    \mathcal{C}(w) = \left\{c:\left|\exp(c^{\top} \mu) - 1 \right| < k, \left| \exp(\alpha_i(w) c^{\top} u_i) - 1\right| < k , \ \forall i=1,...,D \right\}
\end{align*}
We observe that $\alpha_i(w)$ is bounded by $A$, therefore for any $c$ that,
\begin{align*}
    \min(\exp(-Ac^{\top}u_i), \exp(A c^{\top}u_i)) \le \exp(\alpha_i c^{\top}u_i) \le \max(\exp(-Ac^{\top}u_i), \exp(A c^{\top}u_i)),
\end{align*}
and thus,
\begin{align*}
    \min(\exp(-Ac^{\top}u_i), \exp(A c^{\top}u_i)) - 1\le \exp(\alpha_i c^{\top}u_i) - 1 \le \max(\exp(-Ac^{\top}u_i), \exp(A c^{\top}u_i)) - 1,
\end{align*}
which yields,
\begin{align*}
  |\exp(\alpha_i c^{\top}u_i) - 1| \le \max(|\exp(-Ac^{\top}u_i)-1|, |\exp(A c^{\top}u_i)- 1|) < k.
\end{align*}
Therefore we prove $\mathcal{C}(w) \supset \mathcal{C}$. Assembling everything together, one has,
\begin{align*}
p&\left(\left|\exp(c^{\top} \mu)\prod_{i=1}^D \exp(\alpha_i(w) c^{\top} u_i) - 1\right| > (D+1)k , \ \forall i=1,...,D, \forall w \in \mathcal{V}\right) \\
& \le p(\bar{\mathcal{C}}) \\
& \le (2D+1)\exp(-\frac{1}{4d}) + \frac{2DA^2}{d-1}\frac{1}{\log^2(1-k)} + \frac{\|\mu\|^2}{d-1}\frac{1}{\log^2(1-k)}
\end{align*}
For every $c\in \mathcal{C}$, one has, 
\begin{align*}
    \frac{1}{|\mathcal{V}|}\left|Z(c) - Z_0\right| \le \frac{(D+1)k}{|\mathcal{V}|}Z_0.
\end{align*}
Let $Z = |\mathcal{V}|Z_0$, one can conclude that,
\begin{align*}
    p((1-\epsilon_z)Z \le Z(c) \le (1+\epsilon_z) Z) \ge 1-\delta,
\end{align*}
where $\epsilon_z = \Omega((D+1)/|\mathcal{V}|)$ and $\delta = \Omega(DA^2/d)$.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:iso}}

Having Lemma~\ref{lemma} ready, we can follow the same proof as in \citep{arora2015rand} that both $p(w)$ and $p(w, w')$ are correlated with $\|v(w)\|$, formally
    \begin{align}
        \log p(w) &\to \frac{\|v(w)\|^2}{2d} - \log Z, \textrm { as $|\mathcal{V}|\to \infty$},  \label{eq:unigram} \\
        \log p(w, w') &\to \frac{\|v(w) + v(w')\|^2}{2d} - \log Z, \textrm  { as $|\mathcal{V}|\to \infty$}.  \label{eq:bigram}
    \end{align}
Therefore, the inference presented in \citep{arora2015rand} (i.e., \eqref{eq:iso}) is obvious by assembling \eqref{eq:unigram} and \eqref{eq:bigram} together:
\begin{align*}
    {\rm PMI}(w, w') \to \frac{v(w)^{\top}v(w')}{d}, \textrm{ as $|\mathcal{V}|\to\infty$}.
\end{align*}


\end{document}
