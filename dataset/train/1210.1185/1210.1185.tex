\documentclass[journal]{IEEEtran}

\usepackage{amsmath}
\usepackage{cite}
\usepackage{mdwtab}
\usepackage{subfigure}
\usepackage{placeins}
\usepackage{psfrag}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage[latin1]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{comment}

\hyphenation{op-tical net-works semi-conduc-tor}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\theoremstyle{remark}
\newtheorem{lemma}{Lemma}

\IEEEoverridecommandlockouts 

\begin{document}

\title{Scaling Laws of the Throughput Capacity and Latency in Information-Centric Networks}

\author{\IEEEauthorblockN{Bita Azimdoost\thanks{Bita Azimdoost was with Huawei Innovation Center, Santa Clara, CA 95050, USA, as an intern while working on this paper.}, Cedric Westphal , and Hamid R. Sadjadpour}\\
\IEEEauthorblockA{Department of Electrical Engineering and Computer Engineering\\
University of California Santa Cruz, Santa Cruz, CA 95064, USA\\
\{bazimdoost,cedric,hamid\}@soe.ucsc.edu\\
 Huawei Innovation Center, Santa Clara, CA 95050, USA\\
cedric.westphal@huawei.com}}


\maketitle

\begin{abstract}
Wireless information-centric networks consider storage as one of the network primitives, and propose to cache data within the network in order to improve latency and reduce bandwidth consumption. We study the throughput capacity and delay in an information-centric network when the data cached in each node has a limited lifetime. The results show that with some fixed request and cache expiration rates, the order of the data access time does not change with network growth, and the maximum throughput order is inversely proportional to the square root and logarithm of the network size  in cases of grid and random networks, respectively. Comparing these values with the corresponding throughput and latency with no cache capability (throughput inversely proportional to the network size, and latency of order  and  in grid and random networks, respectively), we can actually quantify the asymptotic advantage of caching. Moreover, we compare these scaling laws for different content discovery mechanisms and illustrate that not much gain is lost when a simple path search is used. 
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

In today's networking situations, users are mostly interested in accessing content regardless of which host is providing this content. They are looking for a fast and secure access to data in a whole range of situations: wired or wireless; heterogeneous technologies; in a fixed location or when moving. The dynamic characteristics of the network users makes the host-centric networking paradigm inefficient. Information-centric networking (ICN) is a new networking architecture where content is accessed based upon its name, and independently of the location of the hosts \cite{Zhang2010Named,Pursuit,Ahlgren2012Survey,Jacobson2009Networking}. In most ICN architectures, data is allowed to be stored in the nodes and routers within the network in addition to the content publisher's servers. This reduces the burden on the servers and on the network operator, and shortens the access time to the desired content.

Combining content routing with in-network-storage for the information is intuitively attractive, but there has been few works considering the impact of such architecture on the capacity of the network in a formal or analytical manner. In this work we study a wireless information-centric network where nodes can both route and cache content\footnote{A preliminary version of this paper has appeared at ITC25 \cite{Azimdoost2013Throughput}}. We also assume that a node will keep a copy of the content only for a finite period of time, that is until it runs out of memory space in its cache and has to rotate content, or until it ceases to serve a specific content.

The nodes issue some queries for content that is not locally available. We suppose that there exists a server which permanently keeps all the contents. This means that the content is always provided at least by its publisher, in addition to the potential copies distributed throughout the network. Therefore, at least one replica of each content always exists in the network and if a node requests a piece of information, this data will be provided either by its original server or by a cache containing the desired data. When the customer receives the content, it will store the content and share it with the other nodes if needed.

The present paper thus investigates the access time and throughput capacity in such content-centric networks and addresses the following questions:

\begin{enumerate}
	\item Looking at the throughput capacity and latency, can we quantify the performance improvement brought about by a  content-centric network architecture over networks with no content sharing capability?
	\item How does the content discovery mechanism affect the performance? More specifically, does selecting the nearest copy of the content improve the scaling of the capacity and access time compared to selecting the nearest copy in the direction of original server?  
	\item How does the caching policy, and in particular, the length of time each piece of content spends in the cache's memory, affect the performance?
\end{enumerate}

We state our results in three Theorems; Theorem \ref{thm:01} formulates the throughput capacity in a grid network which uses the shortest path to the server content discovery mechanism considering different content availability in different caches, and Theorem \ref{thm:02} and \ref{thm:03} will answer the above questions studying two different network models (grid and random network) and two content discovery scenarios (shortest path to the server and shortest path to the closest copy of the content) when the information exists in all caches with the same probability. These Theorems demonstrate that adding the content sharing capability to the nodes can significantly increase the capacity.

The rest of the paper is organized as follows. After a brief review of the related work in Section \ref{sec:related}, the network models, the content discovery algorithms used in the current work, and the content distribution in steady-state are introduced in Section \ref{sec:netmodel}. The main Theorems are stated and proved in Section \ref{sec:theorems}. We will discuss the results and study some simple examples in Section \ref{sec:discussion}. Finally the paper is concluded and some possible directions for the future work will be introduced in section \ref{sec:conclusion}.

\section{Related Work}
\label{sec:related}

Information Centric Networks have recently received considerable attention. While our work presents an analytical abstraction, it is based upon the principles described in some ICN architectures, such as CCN~\cite{Jacobson2009Networking}, NetInf~\cite{Ahlgren2008Design}, PURSUIT~\cite{Pursuit}, or DONA~\cite{Koponen2007Dataoriented}, where nodes can cache content, and requests for content can be routed to the nearest copy. Papers surveying the landscape of ICN~\cite{Ahlgren2012Survey}\cite{Ghodsi2011InformationCentric} show the dearth of theoretical results underlying these architectures. 

Caching, one of the main concepts in ICN networks, has been studied in prior works~\cite{Ahlgren2012Survey}. \cite{Olmos2014Catalog} computes the performance of a LRU cache taking into account the dynamical nature of the content catalog. Some performance metrics like miss ratio in the cache, or the average number of hops each request travels to locate the content have been studied in \cite{InfoCom01:Che,InfoCom10:Rosensweig}, and the benefit of cooperative caching has been investigated in \cite{Wolman1999Scale}. 

Optimal cache locations \cite{Rosensweig2009Breadcrumbs} and cache replacement techniques \cite{IEEEMob05:Yin} are two other aspects most commonly investigated. And an analytical framework for investigating properties of these networks like fairness of cache usage is proposed in \cite{Tortelli2011Fairness}. \cite{Westphal2005Maximizing} considered information being cached for a limited amount of time at each node, as we do here, but focused on flooding mechanism to locate the content, not on the capacity of the network. \cite{Dehghan2014Complexity} investigates the routing in such networks in order to minimize the average access delay.

However, to the best of our knowledge, there are just a few works focusing on the achievable data rates in such networks. Calculating the asymptotic throughput capacity of wireless networks with no cache has been solved in~\cite{Gupta2000Capacity} and many subsequent works~\cite{Li2001Capacity}\cite{Niesen2009Capacity}. Some work has studied the capacity of wireless networks with caching \cite{Grossglauser2002Mobility}\cite{Herdtner2005Throughput}\cite{AlfanoContentCentric} . There, caching is used to buffer data at a relay node which will physically move to deliver the content to its destination, whereas we follow the ICN assumption that caching is triggered by the node requesting the content. 
\cite{ICCW09:Liu} uses a network simulation model and evaluates the performance (file transfer delay) in a cache-and-forward system with no request for the data. \cite{Carofiglio2011Modeling} proposes an analytical model for single cache miss probability and stationary throughput in cascade and binary tree topologies. \cite{IEEEIT11:Niesen} considers a general problem of delivering content cached in a wireless network and provides some bounds on the caching capacity region from an information-theoretic point of view. Some scaling regimes for the required link capacity is computed in \cite{Gitzenis2013Asymptotic} for a static cache placement in a multihop wireless network. 


\section{Preliminaries}
\label{sec:netmodel}

\subsection{Network Model}
Two network models are studied in this work.
\subsubsection{Grid Network}
Assume that the network consists of  nodes  each with a local cache of size  located on a grid (Figure \ref{fig:gridnet}). The distance between two adjacent nodes equals to the transmission range of each node, so the packets sent from a node are only received by four adjacent nodes. There are  different contents,  with sizes , for which each node  may issue a query. Based on the content discovery algorithms which will be explained later in this section, the query will be transmitted in the network to discover a node containing the desired content locally.  then downloads  bits of data with rate  in a hop-by-hop manner through the path  from either a node () containing it locally () or the server (). When the download is completed, the data is cached and shared with other nodes either by all the nodes on the delivery path, or only by the end node. In the paper we consider both options. 

 denotes the nodes on the path from  to server. Without loss of generality, we assume that the server is attached to the node located at the middle of the network, as changing the location of the server does not affect the scaling laws. Using the protocol model and according to \cite{book06:Xue},  the transport capacity in such network is upper bounded by . This is the model studied in \ref{thm:01} and the first two scenarios of Theorem \ref{thm:02}.
\begin{figure}[http]
    \center
      \includegraphics[scale=0.4,angle=0]{gridnet.pdf}\\
      \caption{The transmission range of node  contains four surrounding nodes. The black vertices contain the content in their  local caches. The arrow lines demonstrate a possible discovery and receive path in scenario , where node  downloads the required information from . In scenario ,  will download the data from  instead.}
    \label{fig:gridnet}
\end{figure}


\subsubsection{Random Network}

The next network studied in Theorem \ref{thm:02} is a more general network model where the nodes are randomly distributed over a unit square area according to a uniform distribution. We use the same model used in \cite{book06:Xue} (section ) and divide the network area into square cells each with side-length proportional to the transmission range , which is selected to be at least in the order of  to guarantee the connectivity of the network \cite{Applied97:Penrose}. According to the protocol model \cite{book06:Xue}, if the cells are far enough they can transmit data at the same time with no interference; we assume that there are  non-interfering groups which take turn to transmit at the corresponding time-slot in a round robin fashion. Again, without loss of generality the server is assumed to be located at the middle of the network. In this model the maximum number of simultaneous feasible transmissions  will be in the order of  as each transmission consumes an area proportional to .

All other assumptions are similar to the grid network.

\subsection{Content Discovery Algorithm}

\subsubsection{Path-wise Discovery}

To discover the location of the desired content, the request is sent through the shortest path toward the server containing the requested content. If an intermediate node has the data in its local cache, it does not forward the request toward the server anymore and the requester will start downloading from the discovered cache. Otherwise, the request will go all the way toward the server and the content is obtained from the main source. In case of the random network when a node needs a piece of information, it will send a request to its neighbors toward the server, i.e. the nodes in the same cell and one adjacent cell in the path toward the server, if any copy of the data is found it will be downloaded. If not, just one node in the adjacent cell will forward the request to the next cell toward the server.
	
\subsubsection{Expanding Ring Search}
In this algorithm the request for the information is sent to all the nodes in the transmission range of the requester. If a node receiving the request contains the required data in its local cache, it notifies the requester and then downloading from the discovered cache is started. Otherwise, all the nodes that receive the request will broadcast the request to their own neighbors. This process continues until the content is discovered in a cache and the downloading follows after that. This will return the nearest copy from the requester.

\subsection{Content Distribution in Steady-State}
\label{SSanalysis}

The time diagram of data access process in a cache is illustrated in Figure \ref{fig:Tdgrm1}.
When a query for content  is initiated, the content is available at the requester's cache after a wait time () which is a function of the distance between the user and the data source (server or an intermediate cache), the data size, and the download speed. An expiration timer will be set upon receiving the data, and this data will be finally dropped after a holding time () with distribution  and mean . During this time, the cached data can be shared with the other users if needed. The same user may re-issue a query for that data after some random time () with distribution  and mean . Note that a node will send out a request for a content only if it does not have it in its local cache, otherwise, its request will be served locally and no request is sent to the other nodes. The solid lines in this diagram denote the portions of time that the data is available at local cache.

\begin{figure}[http]
    \center
      \includegraphics[scale=0.25,angle=0]{Tdgram}\\
      \caption{Data access process time diagram in a cache network}
    \label{fig:Tdgrm1}
\end{figure}

In this work we assume identical content sizes , and assume all the contents have the same popularity leading to similar request rates , and the same holding times . As the requests for different contents are supposed to be independent and holding times are set for each content independent of the others, we can do the calculations for one single content. If the total number of contents is not a function of the network size, this will not change the capacity order.
Suppose that  is much larger than the request packet size, so we ignore the overhead of the discovery phase in our calculations. Furthermore, if the information sizes are the same and the download rates are also the same, the download time will be a function of the number of hops () between the source and the customer; . In the steady-state analysis, we ignore this constant time.

The average portion of time that each node contains a content in its local cache is

which is the average probability that a node contains the data at steady-state.  is the rate of requests for a data from each user in case of the data not being  available, and  is the rate of the data being expunged from the cache. Both these parameters are strongly dependent on the total number of users, or the topology and configuration of the network or the cache characteristics like size and replacement policy. 


\section{Theorem Statements and Proofs}
\label{sec:theorems}

\begin{theorem} \label{thm:01}
Consider a grid wireless network consisting of  nodes. Each node can transmit over a common wireless channel, with bandwidth  bits per second, shared by all nodes. Assume that there is a server which contains all the information. Without loss of generality we assume that this server is located in the middle of the network. Each node contains some information in its local cache. Assume that the probability of the information being in all the caches with the same distance ( hops) from the server is the same (). The maximum achievable throughput capacity order\footnote{ or  if .  or  if .  or  if both  and .  or  if .  or  if .} () in such network when the nodes use the nearest copy of the required content on the shortest path toward the server is given by


	
where , which means that the server always contains the information.

\end{theorem}

\begin{IEEEproof}
A request initiated by a user  in -hop distance from the server (located in level ) is served by cache  located in level  on the shortest path from  to the server if no caches before , including , on this path contains the required information, and  contains it. This request is served by the server if no copy of it is available on the path. Assuming that the availability of the information in each cache is independent of the contents in the other caches, this probability denoted by  is given by



where  is the probability of the information being available in a cache in level , and  shows the server and . Thus a content requested by  is traveling  hops with probability . There are  nodes in level  so the average number of hops () traveled by each piece of data from the serving cache (or the original server) to the requester is




Assume that each user is receiving data with rate . The transport capacity in this network, which equals to , is upper bounded by . So  and the Theorem is proved.
\end{IEEEproof}


\begin{theorem} \label{thm:02}
	Consider a wireless network consisting of  nodes, with each node containing the information in its local cache with common probability .\footnote{Note that for , the request is served locally and no data is transferred between the nodes.} Assume that the request process and cache look up time in each node is not a function of the number of nodes, then
	
	\begin{itemize}
		\item Scenario - If the nodes are located on a grid and search for the contents just on the shortest path toward the server, the average delay order is
		
		
		
		\item Scenario - If the nodes are located on a grid and use ring expansion as their content search algorithm, the average delay order is
				
		
		\item Scenario - If the nodes are randomly distributed over a unit square area and use path-wise content discovery algorithm, the average delay order is 	
		
		
		\end{itemize}
\end{theorem}

Here we prove Theorem \ref{thm:02} by utilizing some Lemmas.

\begin{lemma} \label{lem:01}
	Consider the wireless networks described in Theorem \ref{thm:02}. The average number of hops between the customer and the serving node (a cache or original server) is 
	
		\begin{itemize}
		\item Scenario 
			

		\item Scenario 
		
		
		
		\item Scenario 
		
		
	
		\end{itemize}
	\end{lemma}
	
\begin{IEEEproof}
Let , , and  denote the number of hops between the customer and the serving node (cache or original server), the number of hops between the customer and the original server, and the maximum value of , respectively. The average number of hops between the customer and the serving node () is given by



Scenario - This case can be considered as a special case of the network studied in theorem \ref{thm:01}, where  is the same for all . Thus we can drop the index  and let  denote the common value of this probability. Using equation \ref{eq:barh} we will have



The constant factor  does not have any affect on the scaling order, so it can be dropped. Using variable  then proves the Lemma.



Scenario  -  in this network is , and there are  nodes at distance of  hops from the original server. 


Each customer may have the required item in its local cache with probability . If the requester is one hop away from the original server, it gets the required item from the server with probability . The customers at two hops distance from the server ( such customers) download the required item from the original server (traveling  hops) if no cache in a diamond of two hops diagonals contains it (probability ), and gets it from a cache at distance one hop if one of those caches has the item (probability ). Using similar reasoning, the customers at distance  from the server get the item from the server (distance  hops) with probability , and from a cache at distance  with probability  as there are  nodes at distance of  hops. Therefore, using equations (\ref{eq:hbar}) and (\ref{eq:barh})





Scenario  -  Each hop is one cell containing  caches.  in this network is of the order of  and
.


Each customer may have the required item in its local cache with probability .  If the requester is one hop away from the original server ( nodes), it gets the required item from the server with probability . The customers at two hops distance from the server ( such customers) download the required item from the original server (traveling  hops) if no cache in the cell at one hop distance contains it (probability ), and gets it from a cache at distance one hop if one of those caches has the item (probability ). Using similar reasoning the customers at distance  from the server get the item from the server (distance  hops) with probability , and from a cache at distance  with probability . Therefore, according to equation (\ref{eq:hbar})




	Noting that  is always less than one, and tends to zero for sufficiently large , the Lemma is proved.
	\end{IEEEproof}
	
	\begin{lemma}\label{lem:02}
	Consider the wireless networks described in Theorem \ref{thm:02}. For sufficiently large networks, the average number of hops between the customer and the serving node (a cache or the original server) is 
	
	\begin{itemize}
		\item Scenario 
			

		\item Scenario 
		
	
		\item Scenario 
		
	
		\end{itemize}
\end{lemma}

\begin{IEEEproof}
To prove this Lemma we use the following equation which is true for every  and .


Scenario  - Let's define 


Thus equation (\ref{eq:EXi}) is written as .
First we investigate the value of  for different ranges of . The summation for  can be decomposed into two  summations.



Assume , then using first and second region of equation (\ref{eq:e_xn}) we have


Moreover it can easily be seen that  is a decreasing function of , so for  with order less than  it is more than . Since , we can say  for .

Now we expand the summation to obtain 




when  then using third region in equation \ref{eq:e_xn},  is going to zero exponentially, so . Thus, .



According to equation (\ref{eq:Esi}) and since , when  (for ) which is the maximum possible order for , then adding  to  cannot increase its order beyond the maximum possible value.
Now to derive the order of  for other values of , we decompose the equation of  to the following summations and investigate their behaviors when .



The number of  is in the order of . Therefore using the following series  , we have 

which is equivalent to  when .

Utilizing the same series, the first summation in  is in the order of . Hence we arrive at







Since  ,  is the dominant factor in , and also it is dominant factor in . Thus,



Scenario  - Let's define 




Assume that , then 

Since  is increasing when   is decreasing and its maximum possible order is , then  for all .

For , we approximate the summation with the integral.




where  is the error function which is always limited by  and is zero at zero.
If , then it is obvious that . For other values of  we use the third approximation in equation \eqref{eq:e_xn}, and also\footnote{This is true when  tends to zero while n approaches infinity.}  for  and  for  to obtain


Since for  the  reaches the maximum , therefore  cannot increase the scaling value of  anymore.
For   we have


Thus it can easily be verified that




Scenario  - Let's define




	
	First we check the behavior of  when . Using second region in equation \eqref{eq:e_xn} we will have .  is increasing when  is decreasing and the maximum possible value for the number of hops is , then  for all .
	
	By approximating the summation with integral, we arrive at
	

	

	If , using equation \eqref{eq:e_xn} and the fact , we will have
		
	When , equation \eqref{eq33} tends to zero.
	

	Using the previous approximations along with  for  and  for , we can approximate  as its dominant terms.
	
	
	
	When , the dominant term is . Thus,
	
	
	
	It can be seen that for large enough  the average number of hops between the nearest content location and the customer is just  hops. This is the result of having  caches in one hop distance for  every requester. Each one of these caches can be  a potential source for the content. When the network grows, this number will increase and if  is large enough () the probability that at least one of these nodes contain the required data will approach 1, i.e., .
	
	\end{IEEEproof}
	
	Theorem \ref{thm:02} is now simply proved using the above Lemmas.

\begin{IEEEproof}
Assuming that the delay of the request process and cache look up in each node is not increasing when the network size (the number of nodes) increases, and the there is enough bandwidth to avoid congestion, then the delay of getting the data is directly proportional to the average number of hops between the serving node and the customer. Thus, the delay and the average number of hops the data is traveling to reach the customer are of the same order and \textit{Theorem \ref{thm:02}} is proved.
\end{IEEEproof}

\begin{theorem}\label{thm:03}
Consider the networks of Theorem \ref{thm:02}, and assume each node can transmit over a common wireless channel, with  bits per second bandwidth, shared by all nodes. The maximum achievable throughput capacity order   in the three discussed scenarios are
	
	\begin{itemize}
		\item Scenario - 
		
		
		
		\item Scenario - 
				
		
		\item Scenario -	
		
		
		\end{itemize}
\end{theorem}

To prove Theorem \ref{thm:03} we use Lemma \ref{lem:02}, and the following two Lemmas.

\begin{lemma}\label{lem:03}
	Consider the wireless networks described in Theorem \ref{thm:02}. In order not to have interference, the maximum throughput capacity is upper limited by  
	
	\begin{itemize}
		\item Scenario - 
		
		
		
		\item Scenario - 
			
		
		\item Scenario -	
		
		
		\end{itemize}
\end{lemma}	
		
\begin{IEEEproof}
Assume that each content is retrieved with rate  bits/sec. The traffic generated because of one download from a cache (or server) at average distance of  hops from the requester node is . The total number of requests for a content in the network at any given time is limited by the number of nodes . Thus the maximum total bandwidth needed to accomplish these downloads will be , which is upper limited by () in scenarios , , and () in scenario . Thus,



in scenarios , , and



in scenarios . Therefore the maximum download rate is easily derived using the results of Lemma \ref{lem:02}.
\end{IEEEproof}

In the previous Lemma, the maximum throughput capacity in a wireless network utilizing caches has been calculated such that no interference occurs. Now it is important to verify if this throughput can be supported by each node (cell), i.e. the traffic carried by each node (cell) is not more than what it can support ().

\begin{lemma} \label{lem:04}

	The throughput capacities of Lemma \ref{lem:03} are supported for all values of  in grid topology. The random network can support the obtained throughput capacities just when . For smaller values of  the maximum supportable throughput capacities are as follows.
	
\end{lemma}

\begin{IEEEproof}
Each link between two nodes in scenarios  and , or two cells in scenario  can carry at most  bits per second. Here we calculate the maximum traffic passing through a link considering the throughput capacities derived in previous Theorems, and check if any link can be a bottleneck.

Scenario -  Each one of the four links connected to the server will carry all the traffic related to the items not found in the on-path caches. Thus, the total traffic carried by each of those links is
.

When , we have  for all . So this traffic is equal to


When , using equation \ref{eq:e_xn} the above summation can be written as




Therefore, the links directly connected to the server will never be a bottleneck. On the other hand, the traffic carried by a node to cache content in level  is , so the server links carry the maximum load, and thus the derived capacity is supportable in every link. 

Scenario - Each one of the four links connected to the server will carry all the traffic related to the items not found in any caches closer to the requester. Thus, the total traffic carried by each of those links is

 

If , then  for all . Thus the above traffic will be .

If  and , then using  the above equation is equivalent to , which is less than  in order.

Finally, if , then the traffic is equivalent to , which is less than .
 So server links will not be a bottleneck. Using similar reasoning as in scenario  other links carry less traffic, so the derived capacities are supportable.


Scenario - The traffic load between the server cell and each of the four neighbor cells is given by



If , then  for , thus the traffic load equals to . Therefore, the obtained capacity is not supported for very small  (). The maximum supportable throughput capacity in this case is .  

If , then the maximum traffic load on a link is 



The maximum throughput capacity obtained for this region is , which will lead to a traffic load of , which means that this bit rate is not supportable. The maximum supportable rate in this region is then , which is much less than .

If  and , then equation \eqref{eq44}  is equivalent to , which is supportable. If  and , then the maximum traffic is , which is less than , and supportable.

Note that if there were no cache in the system, or  is very low, less than the stated threshold values, almost all the requests would be served by the server, and the maximum download rate would be  in case ,  and  in case .

\end{IEEEproof}

The maximum throughput capacity is the value which can be supported by all the nodes while no interference is occurred. Thus combining Lemmas \ref{lem:03} and \ref{lem:04}, Theorem \ref{thm:03} is proved.

\section{Discussion}
\label{sec:discussion}

In this section, we discuss our results based on two examples. The first example is that of a grid wireless network with  caches, and one server, which contains all the items located in the middle of the network. The requesters use the path search to locate the contents.  In the second example we study the impact of caching on the maximum capacity order in the grid and random networks where all the caches have the same probability of having each item at any given time. The networks where the received data is stored only at the receivers and then shared with the other nodes as long as the node keeps the content can be considered as an example of such networks. 

 
\subsubsection{Example 1}
\label{ex:01}

Assume that each cache in level  (nodes at  hops away from the server) in a grid network receives requests for a specific document according to a Poisson distribution with rate  from the local user, and with rate  from all the other nodes. Note that rate  is a function of the individual request rate of users () and also the location of the cache in the network. The content discovery mechanism is path-wise discovery, and whenever a copy of the required data is found (in a cache or server), it will be downloaded through the reverse path, and all the nodes on the download path store it in their local caches. Moreover, we assume that receiving the data and also any request for the available cached data by a node in level  refreshes a time-out timer with fixed duration . According to \cite{Che2002Hierarchical},  this is a good approximation for caches with Least Recently Used (LRU) replacement policy when the cache size and the total number of documents are reasonably large. We will calculate the average probability of the data being in a cache in level  () based on these assumptions and then use Theorem \ref{thm:01} to obtain the throughput capacity.

Let random variable  denote the total time of the data being available in a cache during constant time . Assume that  requests are received by each node  in level  ( hop distance from the server). The data available time between any two successive requests (internal and external) is  if the timer set by the first request is expired before the second one comes, or is equal to the time between these two requests. Let  denote the time between receiving two successive requests. This process has an exponential distribution with parameter . So the total time of data availability in a level  cache is 

 
and the average value of this time is 


According to the Poisson arrivals of requests with parameter , . 

 can be easily calculated and equals to . Therefore,


 
And finally the probability of an item being available in a level  cache is . 
Note that  so that .

Now we need to calculate the rate of requests received by each node in level . We assume that the shortest path from the requester to the server is selected such that all the nodes in level  receive the requests with the same rate. There are  nodes in level  and  nodes in level . So the request initiated or forwarded from a node in level  will be received by a specific node in level  with probability  if it is not locally available in that node, so  can be expressed as

 

Combining equation \ref{eq:beta}, the relationship between  and , and the fact that there is no external request coming to the nodes at the edge boundary of the network (), together with the result of Theorem \ref{thm:01} we can obtain the capacity () in the grid network with path-wise content discovery and on-path storing scheme which is given by

  

Figure \ref{fig:onpath} (a) illustrates that the maximum throughput capacity changes with the network size () when  is the same for all nodes. It can be seen that the this capacity is inversely proportional to , just like the throughput capacity when no timer refreshing is available and the downloaded data is stored just in the end user's cache. 

Figure \ref{fig:onpath} (b) shows the capacity versus different values for  assuming  and same timer expiration time for all nodes. It can be seen that the maximum capacity is very close to . For large  products the probability of the content being available in each and every cache will tend to be one, so all the contents are downloaded from the local cache and no data transfer is needed to be done, therefore the calculated throughput capacity will be very large which means that the all the links are available with their maximum bandwidth.


\begin{figure}[http]
    \center
       \includegraphics[scale=0.23,angle=0]{onpathrefresh1}\\
			 \includegraphics[scale=0.23,angle=0]{onpathrefresh2}\\
      \caption{Maximum throughput capacity () versus (a) network size (), (b) Timeout-request rate product ().}
    \label{fig:onpath}
\end{figure}

\begin{comment}

since the time of that data being received and cached till the time of expiration, and  denote the total number of incoming requests (internal and external) for a cached data before its expiration. If  requests are received during the available data time, average value of  is . The average value of  then is given by

 

Denoting the  time-out time and inter request time by  and , respectively, the probability that no request is received before the first timer expiration time is the probability that . The probability that exactly  requests are received is the probability that  for  and . Assuming independence between the expiration times and also the request inter-arrival times, 



Where the index  and  is omitted in the second equation because of identical distributions of expiration times and also request inter-arrival times.



Thus, , and

 

Therefore  and the probability of the data being in a cache is .
\end{comment}


\subsubsection{Example 2}
\label{ex:02}

As a possible example leading to equal probability of all the caches containing a piece of data, which is the basic assumption of Theorems \ref{thm:02} and \ref{thm:03}, assume that receiving a data in the local cache of the requesting user sets a time-out timer with exponentially distributed duration with parameter  and no other event will change the timer until it times-out, meaning that . Considering the request process for each content from each user being a Poisson process with rate , and using the memoryless property of exponential distribution (internal request inter-arrival times), and assuming that the received data is stored only in the end user's cache (the caches on the download path don't store the downloading data), it can be proved that . Thus we can write the presence probability of each content in each cache as . 

Figures \ref{fig:traffic_lambda} (a),(b) respectively illustrate the total request rate and the total traffic generated in a fixed size network in scenario  for different request rates when the time-out rate is fixed. The total request rate in the network is the product of the number of requesting nodes and the rate at which each node is sending the request (). The total traffic is the product of the total request rate and the number of hops between source and destination and the content size ().
Small  means that each node is sending requests with low rate, so fewer caches have the content, and consequently more nodes are sending requests with this low rate. In this case most of the requests are served by the server. The total request rate will increase by increasing the per node request rate. High  shows that each node is requesting the content with higher rate, so the number of cached content in the network is high, thus fewer nodes are requesting the content with this high rate externally. Here most of the requests are served by the caches. The total request rate then is determined by the content drop rate. So for very large , the total request rate is the total number of nodes in the network times the drop rate () and the total traffic is . As can be seen there is some request rate at which the traffic reaches its maximum; this happens when there is a balance between the requests served by the server and by the caches, for smaller request rates, most of the requests are served by the server and increasing  increases the total traffic; for larger , on the other hand, most of the requests are served by the caches and increasing the request rate will decrease the distance to the nearest content and decrease total traffic.

Figures \ref{fig:traffic_mu} (a),(b) respectively illustrate the total request rate and the total traffic generated in a fixed size network in scenario  for different time-out rates when the request rate is fixed. Low  means high time-out rates or small lifetimes, which means most of the requests are served by the server and caching is not used at all. For large time-out times, all the requests are served by the caches, and the only parameter in determining the total request rate is the time-out rate.

However, when the network grows the traffic in the network will increase and the download rate will decrease. If we assume that the new requests are not issued in the middle of the previous download, the request rate will decrease with network growth. If the holding time of the contents in a cache increases accordingly the total traffic will not change, i.e. if by increasing the network size the requests are issued not as fast as before, and the contents are kept in the caches for longer times, the network will perform similarly.


\begin{figure}[http]
    \center
			\includegraphics[scale=0.23,angle=0]{traffic_fixmu}\\
      \caption{(a) Total request rate in the network (), (b) Total traffic in the network () vs. the request rate () with fixed time-out rate ().}
    \label{fig:traffic_lambda}
\end{figure}

\begin{figure}[http]
    \center
			\includegraphics[scale=0.23,angle=0]{traffic_fixlambda}\\
      \caption{(a) Total request rate in the network (), (b) Total traffic in the network () vs. the inverse of the time-out rate () with fixed request ratio ().}
    \label{fig:traffic_mu}
\end{figure}

In Figure \ref{fig:capacity} (a) we assume that the request rate is roughly  times the drop rate, so , and show the maximum throughput order as a function of the network size. According to Theorem \ref{thm:02} and as can be observed from this figure, the maximum throughput capacity of the network in a grid network with the described characteristics is inversely proportional to the square root of the network size if the probability of each item being in each cache is fixed, while in a network with no cache this capacity will be inversely proportional to the network size. Similarly in the random network the maximum throughput is inversely proportional to the logarithm of the network size. 


Moreover, comparing scenario  with , we observe that the throughput capacity in both cases are almost the same; meaning that using the path discovery scheme will lead to almost the same throughput capacity as the expanding ring discovery. Thus, we can conclude that just by knowing the address of a server containing the required data and forwarding the requests through the shortest path toward that server we can achieve the best performance, and increasing the complexity and control traffic to discover the closest copy of the required content does not add much to the capacity. 

On the other hand with a fixed network size, if the probability of an item being in each cache is greater than a threshold (, , and  in cases  and , respectively), most of the requests will be served by the caches and not the server, so increasing the probability of an intermediate cache having the content reduces the number of hops needed to forward the content to the customer, and consequently increases the throughput (Figure \ref{fig:capacity} (b), ). For content presence probability orders less than these thresholds ( in cases ) most of the requests are served by the main server, so the maximum possible number of hops will be traveled by each content to reach the requester and the minimum throughput capacity () will be achieved. Note that in random network, the maximum throughput is limited by the maximum supportable load on each link.

\begin{figure}[http]
    \center
\includegraphics[scale=0.23,angle=0]{capacity_journal_new}\\
      \caption{Maximum download rate () vs. (a) the number of nodes (), (b) the content presence probability().}
    \label{fig:capacity}
\end{figure}

\begin{comment}
Furthermore, if the content availability increases with network growth, higher throughput capacities may be achievable. For example in scenario  if , then the resulting throughput will be  which is much higher than . However, noting that according to Theorem \ref{thm:02},  is upper bounded by some values, the achievable capacity will be upper bounded by  () and  (). 
\end{comment}

As may have been expected and according to our results, the obtained throughput is a function of the probability of each content being available in each cache, which in turn is strongly dependent on the network configuration and cache management policy. 


\section{Conclusion And Future Work}
\label{sec:conclusion}

We studied the asymptotic throughput capacity and latency of ICNs with limited lifetime cached data at each node. The grid and random networks are two network models we investigated in this work. The results show that with fixed content presence probability in each cache, the network can have the maximum throughput order of  and  in cases of grid and random networks, respectively, and the number of hops travelled by each data to reach the customer (or latency of obtaining data), can be as small as one hop. 



Moreover, we studied the impact of the content discovery mechanism on the performance. It can be observed that looking for the closest cache containing the content will not have much asymptotic advantage over the simple path-wise discovery. Consequently, downloading the nearest available copy on the path toward the server will have the same performance as downloading from the nearest copy. A practical consequence of this result is that routing may not need to be updated with knowledge of local copies, just getting to the source and finding the content opportunistically will yield the same benefit. 

Another interesting finding is that whether all the caches on the download path keep the data or just the end user does it, the maximum throughput capacity scale does not change. 

In this work, we have made several assumptions to simplify the analysis. For example, we assumed all the contents have the same characteristics (size, popularity). This assumption should be relaxed in future work. We also assumed that the requester downloads the data completely from one content location. However, if the node that needs the data can download each part of it from different nodes and makes a complete content out of the collected parts, achievable capacities may be different. Proposing a caching and downloading scheme that can improve the capacity order is part of our  future work.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,ToN2015}

\end{document} 