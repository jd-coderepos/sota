

\documentclass[twoside]{article}
\usepackage[a4paper]{geometry}
\usepackage[T1]{fontenc} \usepackage{RR}
\usepackage{hyperref}
\usepackage{color}
\usepackage{amsfonts,amssymb}
\usepackage{url}
\usepackage{algpseudocode}
\usepackage{rotating}
\usepackage{multirow}

\usepackage{footnote}
\makesavenoteenv{eqnarray}

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ud}{\textrm{d}}

\usepackage[utf8x]{inputenc}
\usepackage[francais]{babel}

\usepackage[usenames,svgnames]{xcolor}

\newcommand{\paulo}[2]{#2}
\newcommand{\roy}[2]{#2}
\RRNo{8072}
\RRdate{September 2012}
\RRauthor{Shubhabrata Roy\and
Thomas Begin\and
Patrick Loiseau\and Paulo Gonc\c alves}
\authorhead{Roy \& Begin \& Loiseau \& Gonc\c alves }
\RRetitle{A Versatile  Model for VoD Buzz Workload: Identification, Numerical Validation and Applications in Dynamic Resource Management}
\RRtitle{Un mod\`{e}le de trafic adapté à la volatilité de charge d'un service de vidéo à la demande: Identification, validation et application à la gestion dynamique de ressources.}
\titlehead{VoD Buzz Workload and its application in Dynamic Resource Management}
\RRnote{Work described in this report has been supported by the EU FP7 project SAIL}
\RRresume{La gestion dynamique de ressources est un élément clé du paradigme de {\it cloud computing} et plus récemment de celui de {\ cloud networking}. Dans ce contexte d'infrastructures virtualisées, la réduction des coûts associés à l'utilisation et à la ré-allocation des ressources contraint les opérateurs et les utilisateurs de clouds à une gestion rationnelle de celles-ci. 
Dans ce travail nous proposons une description probabiliste des besoins liée à la volatilité de la charge d'un service de distribution de vidéos à la demande. Cette description peut alors servir de consigne (input) à la provision et à l'allocation dynamique des ressources nécessaires. Notre approche repose sur la construction d'un modèle stochastique inspiré des modèles de Markov standards de propagation épidémiologique, capable de reproduire des variations soudaines et intenses d'activité ({\it buzz}). Nous proposons alors une procédure heuristique d'identification du modèle à partir de séries temporelles du nombre d'utilisateurs connectés au serveur. Les performances d'estimation de chacun des paramètres du modèle sont évaluées numériquement, et nous vérifions l'adéquation du modèle aux données en comparant les distributions des états stationnaires ainsi que les fonctions d'auto-corrélation des processus. 
\newline
Les propriétés markoviennes de notre modèle garantissent qu'il vérifie un principe de grandes déviations permettant de caractériser statistiquement l'ampleur et la durée d'évènements extrêmes et rares tels que ceux produits par les {\it buzzs}.  C'est cette propriété que nous exploitons pour dimensionner le volume de ressources (e.g. bande-passante, nombre de serveurs, taille de buffers) à prévoir pour réaliser un bon compromis entre coût de re-déploiement des infrastructures et qualité de service. Cette approche probabiliste de la gestion des ressources ouvre des perspectives sur les  politiques de  {\it Service Level Agreement} adaptées aux {\it clouds} et servant au mieux les intérêts des opérateurs de réseaux, de services et de leurs clients.
}
\RRabstract{
Dynamic resource management has become an active area of research in the Cloud Computing paradigm. Cost of resources varies significantly depending on configuration for using them. Hence efficient management of resources is of prime interest to both Cloud Providers and Cloud Users. In this report we suggest a probabilistic resource provisioning approach that can be exploited as the input of a dynamic resource management scheme. Using a Video on Demand use case to justify our claims, we propose an analytical model inspired from  standard models developed for epidemiology spreading, to represent sudden and intense workload variations. As an essential step we also derive a heuristic identification procedure to calibrate all the model parameters and evaluate the performance of our estimator on synthetic time series. We show how good can our model fit to real workload traces with respect to the stationary case in terms of steady-state probability and autocorrelation structure. We find that the resulting model verifies a Large Deviation Principle that  statistically characterizes extreme rare events, such as the ones produced by ``buzz effects" that may cause workload overflow in the VoD context.\newline
This analysis provides valuable insight on expectable abnormal behaviors of  systems. We exploit the information obtained using the Large Deviation Principle for the proposed Video on Demand use-case for defining policies (Service Level Agreements). We believe these policies for elastic resource provisioning and usage may be of some interest to all stakeholders in the emerging context of cloud networking.}
\RRmotcle{R\'{e}seaux, Cloud, Gestion probabiliste des Ressources, Modèles Epidémiques , Générateur de Charge, Estimation Statistique, Principe de Grandes Déviations, {\it Service Level Agreement}, Vidéo à la Demande, Buzz}
\RRkeyword{Cloud Networking,Probabilistic Resource Management, Epidemic Model, Workload Generator, Statistical Estimation, Large Deviation Principle, Service Level Agreements, Video on Demand, Buzz}
\RRprojet{RESO}
\RCGrenoble 

\begin{document}
\makeRR   \tableofcontents
\section{Introduction}
\label{sec:intro}

In recent trend of data-intensive applications with pay-as-you-go execution in a cloud environment, there are new challenges in system management and design to optimize the resource utilization. Types of the application, deployed in a cloud, can be very diverse. \roy{There are some applications that need to be rapidly cloned or re-allocated, like a pre-production environment.}{} Some applications exhibit highly varying demand in resources. In this paper we consider a Video on Demand (VoD) system as a relevant example of a data-intensive application where bandwidth usage varies rapidly over time. \newline
A VoD service delivers video contents to consumers on request. According to Internet usage trends, users are increasingly getting more involved in the VoD and this enthusiasm is likely to grow. According to 2010 statistics a popular VoD provider like Netflix accounts for around 30 percent of  the peak downstream traffic in the North America and is the ``largest source of Internet traffic overall" \cite{website:sandvine}. Since VoD has stringent streaming rate requirements, each VoD provider needs to reserve a sufficient amount of server outgoing bandwidth to sustain continuous media delivery (we are not considering IP multicast here). However, resource reservation is very challenging in a situation, when a video becomes popular very quickly leading to a \emph{flood} of user requests on the VoD servers. This situation, also known as a ``buzz", demands an adaptive resource allocation strategy to cope with the sudden (and significant) variation of workload. Following is one example of ``buzz" (see Figure~\ref{fig:viral}) where interest over a video ``Star Wars Kid" \cite{website:waxy} grew very quickly within a short timespan. According to \cite{website:bbc} it was viewed more than 900 millions times within a short interval of time making it one of the top viral videos.
\begin{figure}[h]
\centering
\hspace*{-3mm}
\begin{tabular}{cc}
\begin{turn}{90}{\hspace*{5mm} Number of downloads / day} \end{turn} &
\hspace*{-4.5mm}\includegraphics[width=0.7\columnwidth]{Viral_Video.eps}\
\mathbb{P}_{S\to C} = {(l+ (N_I(t)+N_R(t)) \, \beta) {\rm d}t} + o({\rm d}t)
\label{eq:transition-prob}

\lambda(t) = l+ (N_I(t)+N_R(t))\beta.
\label{eq:infinity}

\lefteqn{\mathbb{P}(i', r' | i, r) } \\
&=& (l+(i+r) \beta){\rm d}t + o({\rm d}t) \hspace*{5mm} \mbox{for } (i'=i+1,r'=r), \nonumber \footnotemark\footnotetext{In a closed system, where the total number of viewers (susceptible, current and past) is constant, say , the transition probability for (+1,) needs to be modified, since it would then depend on the number of susceptible viewers as well,, i.e (). The transition probability in this case would be . Therefore, Eq. \ref{eq:i_mean} and \ref{eq:stability} need to be modified accordingly.}\\
&=& (\gamma i){\rm d}t + o({\rm d}t)  \hspace*{12mm}\mbox{for } (r'=r+1,i'=i-1), \nonumber \\
&=& (\mu r){\rm d}t + o({\rm d}t) \hspace*{17mm} \mbox{for } (r'=r-1,i'=i), \nonumber \\
&=& o({\rm d}t)  \hspace*{45mm} \mbox{otherwise.} \nonumber
 \label{eq:MC2-transition}

\mathbb{E}(i) = {\mu l \over {\mu \gamma - \mu \beta - \gamma \beta}},
\label{eq:i_mean}

{\beta}^{-1} > {\mu}^{-1} + {\gamma}^{-1}. 
\label{eq:stability}

\bar{\beta} =  {{\beta_{1} a_2} \over {a_1 + a_2}} + {{\beta_{2} a_1} \over {a_1 + a_2}}.
1mm] \hline
 &  &  &  \\ 
Emp. mean  &  &  &  \\
\hline
\end{tabular}
\label{table1}
\end{table}
\paulo{}{While the synthetic traces corresponding to  cases (b) and (c) reproduce distinct and easily identifiable buzz regimes, the parameter set of case (a) leads to a workload variation distinct from the typical shape of Figure \ref{fig:viral}. Nonetheless, for all 3 configurations, the empirical means estimated from the  samples of the traces are in  good agreement with the expected values of \roy{relation}{Eq.} (\ref{eq:i_mean}).
}
\newline
\paulo{It is to be noted that even though we consider exponential distribution in our model for simplicity, any other distribution can be used here and the equations for the mean workload and stability condition (Eq. \ref{eq:i_mean} and \ref{eq:stability}) would remain unchanged. However, our parameter identification procedure is based on exponential assumptions and needs to be adapted accordingly, if other types of distribution were to be used.}
{Finally, let us notice that even though we consider exponentially distributed random variables in our model, any other distributions could be used, which, according to the same balance principle, would lead to a mean workload  and to a stability condition of the same kind as  (\ref{eq:i_mean}) and (\ref{eq:stability}). However, the estimation procedure we derive in the next section strongly relies on the exponential assumption and it would need to be thoroughly reworked to adapt to different hypotheses.
}
\begin{figure}[t]
\centering
\begin{tabular}{ccc}
\hspace*{-8mm}case (a) & case (b) & case (c) \\
\hspace*{-8mm}\includegraphics[width=0.40\columnwidth]{trace_case_a.eps} &
\hspace*{-8mm}\includegraphics[width=0.40\columnwidth]{trace_case_b.eps} &
\hspace*{-8mm}\includegraphics[width=0.40\columnwidth]{trace_case_c.eps}
\end{tabular}
\caption{\small Illustration of our model ability at generating different dynamics of workload . See Table \ref{table1} for the parameter values corresponding to each of these three cases. The axis corresponds to time (in hours unit) while the axis indicates the number of active viewers.}
\label{fig:traces}
\end{figure}


\section{Estimation procedure}
\label{sec:estimation}
In this section, we address the identifiability of our model and design a calibration algorithm to fit workload data.
We start constructing empirical estimators for each parameter of the model and we numerically evaluate their performance on synthetic traces. 


\subsection{Parameters estimation}
\label{sec:est_method}

Considering a standard epidemic process  with propagation rate , the maximum likelihood estimate  is derived in \cite{BarDynaPro2008}, \cite{AndStoEpi2000} and reads:

where  is the number of contaminations (i.e. number of increments of ) occurring within the time interval .

Very often, maximum likelihood approach yields optimal results (in terms of estimate variance and or bias) but it is not always possible to get a closed-form expression for the estimated parameters. This can either be due to the likelihood function that is impossible to derive analytically, or to missing data that preclude straightforward application of the maximum likelihood principle. Nonetheless, solutions, such as the Expectation-Maximization (EM) or the Monte Carlo Markov Chain (MCMC) algorithms exist, which in some cases can approximate maximum likelihood estimators. 

Returning to our model depicted in Figure \ref{fig:Markov}, each parameter needs to be empirically estimated, assuming that the instantaneous workload time series is the only available observation.\\

\noindent{\it Watching parameter .} As  is the departure rate of users that leave the infected state after they finished watching a video, it can directly be inferred from the number  of decrements of the observable process . Therefore,  the MLE  of \roy{equation}{Eq.} (\ref{eq:mle}) straightforwardly applies and leads to:

 
\noindent{\it Memory parameter .} This rate at which past viewers leave the recovery compartment and stop propagating the virus (gossip), relates to the decrement density of the non-observed process . It is thus impossible to simply apply the MLE of Eq. (\ref{eq:mle}) unless we first construct a substitute  to the missing data from the observable data set . Let us recall that in our model, all current viewers turn and remain contagious for a mean period of time . Then, in first approximation, we can consider that  derives from the finite memory cumulative process: 

which itself, depends on the parameter to be estimated . 
We propose an estimation procedure based on the inherent exponential property of the model. From the Poisson assumption, the inter-arrival time  between the consecutive arrivals of two new viewers is an exponentially distributed random variable such that  . 
\paulo{Our rationale is based on the fact that for a given value of , we can evaluate its likelihood with regard to the trace. First, we consider a value of  and derive  from Eq. (\ref{eq:estimate-R}). For each value of the sum , we obtain a set of inter-arrival times. We normalize them by dividing the samples of each set by their corresponding mean. Then we merge all sets to build a single set of inter-arrival times. Clearly this set contains samples  of a random variable , that are exponentially and identically distributed, i.e. they follow the same exponential distribution. We rearrange this set in an ascending order with  as the minimum value followed by . We then perform the ``normalized spacings" \cite{Jammalamadaka03} and generate the transformed variable , such that .  is also exponentially and identically distributed, provided that the original variable () also holds the same property \cite{Seshadri69}. \newline
Following \cite{Jammalamadaka03} we compare the cumulative empirical distribution function (cedf) of the original variable  with that of the transformed one, . Let  and  denote the empirical distribution functions	of	 and  respectively. We then construct test of exponentiality by measuring the distance between these two cedf’s, using the classical Kolmogorov-Smirnov type distances. For each value of , we obtain a test statistics () that can be expressed as follows:

 somehow expresses the likelihood of  as being the right value that generates the workload trace.
For  (Typically we choose  in the order of  with steps of ) we roughly reconstruct  and compute  following Eq. (\ref{eq:estimate-mu}). We choose our estimated value of , i.e.  as the one with the lowest value of . In Figure \ref{fig:KS}  we show how the value of  varies with  for the three cases. It clearly indicates a minimum value of  which is around the actual value of .}
{
Its means that, for  fixed, the normalized random variable  is exponentially distributed with unitary parameter and becomes independent of . Ideally then, for each value of ,  all the sub-series  , after normalization by their own empirical mean, yield independent and identically distributed realizations  of a unitary exponential random variable. In practice though, as  is not observable, only if   is accurately estimated, should this unitary exponential i.i.d. assumption hold true. From there, we propose the following algorithm: for different values of  spanning a {\it reasonable} interval, we use  estimated from \roy{}{Eq.} (\ref{eq:estimate-R}) to build the normalized series . A statistical test applied to each  allows for assessing the exponential i.i.d. hypothesis and then to select the value of  that yield the best score. \\
More concretely, we apply to  the statistical exponentially test derived in \cite{Jammalamadaka03}: Form the {\it normalized spacings}  where  stands for  rearranged in ascending order. Let  and  denote the cumulative distribution functions of  and   respectively, and compute the  classical Kolmogorov-Smirnov distance:

As  and  are identical for an exponentially i.i.d. random series, we then expect  to reach its minimum  for the value of  that gives the best estimate  of :
1mm]
\widehat{R} = \widehat{R}_{\widehat{\mu}}.
\end{array}\right.
\label{eq:mu-estimate}
-1mm]
&  ({\em logarithmic scale}) \\
\end{tabular}
\caption{\small Evolution of the exponential test statistics (\ref{eq:KS}) applied to the traces of Figure \ref{fig:traces}. Dotted vertical lines locate the actual value of  for each case;  dot markers on each curve indicate the estimated value  corresponding to the minimum point of the statistical test . }
\label{fig:KS}
\end{figure}

\begin{figure}[h]
\centering
\hspace*{-2mm}\begin{tabular}{cc}
\begin{turn}{90} \hspace*{8mm} ,  \end{turn} & 
\hspace*{-3mm}\includegraphics[width=.55\columnwidth]{compare_R} \
{\Omega}(x) = \frac{1}{|\mathcal{I}(x)|}\,\sum_{t_n\in\mathcal{I}(x)} w_{n}~:~\mathcal{I}(x)=\{t_n:I(t_n)+\widehat{R}(t_n)=x\}.
\label{eq:linear-regression}
-1mm]
&  \\
\end{tabular}
\caption{\small Weighted linear regression of  vs  corresponding to the three traces of Figure \ref{fig:traces}. Superimposed are the  linear trends fitted on the respective data.}
\label{fig:regression}
\end{figure}


Formally, we can apply the exact same procedure to estimate , but considering opposite weights to favor 
the large values of 's. However, due to the large fluctuations of  in the corresponding region, the  slope   is subject to a very poor estimation variance. Instead, we propose to apply the ML estimator \roy{}{described in Eq.} (\ref{eq:mle}) on the restriction of  to the buzz  periods only. Strictly speaking, we should consider  as well, but since a buzz event normally occurs on very small interval of time, we assume that   (resp. ) remains constant in the meanwhile (flash crowd viewers will enter in  R compartment only after the visualization time). In practice, to automatically identify the buzz periods, we threshold  and consider only the persistent increasing parts that remain above the threshold.\\


\noindent{\it Transition rates  and .} As we already said, at time , the inter-arrival time  separating to new incomers is a random variable drawn from an exponential law of parameter , where  and  is either equal to  or to . We denote  and  the corresponding densities built upon the reconstructed process  and  the estimated parameters  and  respectively.  For a given inter-arrival time  observed at time , we form the likelihood ratio  to determine whether the system is in buzz or in buzz-free state. Moreover, in order to avoid non-significant state transitions we resort to a restoration method inspired by the Viterbi algorithm \cite{Kleinberg2002}.
Once we have identified the hidden states of the process, we estimate the transitions rates  and  from the average times spent in each state.




\subsection{Numerical Validation}
\label{sec:est_results}

\paulo{We validate our estimation procedure against the synthetic traces with three different workloads as shown in Figure \ref{fig:traces}. Each trace contains  events and we conducted  independent realizations to produce  independent traces for each case. Then we apply our estimation procedure on each of trace. For each parameter we obtain the so called ``descriptive statistics", i.e. the smallest observation (sample minimum), lower quartile, median, upper quartile, and largest observation (sample maximum) from the box-and-whisker plot. \newline
From Figure \ref{fig:box-plot} we can infer that estimation of  is almost unbiased with an interquartile range (IQR) less than  for all three cases.  is estimated with little bias but  is harder to estimate and the IQR ranges around . As we discussed in the previous section, estimation of  is not trivial. We find that this estimation is negatively biased for the three cases with IQR around . Estimation of  and  are also negatively biased with IQR less than .}
{
To evaluate the statistical performance of our estimation procedure, we resort to numerical experiments to empirically get the first and the second order moments of each parameter estimator. Owing to  the versatility of our model, we must ensure that the proposed calibration algorithm performs well for a variety of workload dynamics. To this end, we systematically reproduce the experiments considering the 3 sets of parameters reported in Table \ref{table1}. For each configuration, we generate 10 independent  realizations of processes similar to the ones depicted in Figure \ref{fig:traces}, and use these to derive  descriptive statistics. 

The box-and-whisker plots of Figure \ref{fig:box-plot} indicate for each estimated parameter (centered and normalized by the corresponding actual value) the sample median (red line), the inter-quartile range (blue box height) along with  the extreme samples (whiskers) obtained from time series of length  points each. As expected (owing to the maximum likelihood procedure), estimation of  shows to be the most accurate, both in terms of bias and variance. But more surprisingly though, although the estimation  derives from a heuristic procedure that  itself depends on the raw approximation  of Eq. (\ref{eq:estimate-R}), the resulting performance is remarkably good: bias is always negligible (less than 5\% in the worst case (c))  and the variance always confines to 10\% of the actual value of . Notice also that the estimation of  goes from a slight underestimation in case (a) to a slight overestimation in case (c), as the buzz effect,  i.e. the value of , grows from traces (a) to (b). Compared to , the estimation of  behaves more poorly and proves to be the most difficult parameter to estimate. But we have to keep in mind that this latter is only based on buzz periods which represent only a small fraction of the entire time series. Regarding the parameter , its estimation remains within a 20\% inter-quartile range but cases (a) and (c) show a systematic bias (median hits the lower quartile bound). Let us then recall that the procedure\roy{}{, described by Eq.} (\ref{eq:mu-estimate}) to determine  selects within some discretized interval, the value of  that yields the best  score. It is then very likely that the true value does not coincide with any sampled point of the interval and therefore, the procedure picks the closest one that systematically lies beneath or above. Finally,  estimation of the transition parameters  and  between the two hidden states relies on all other parameters estimation, cumulating so all relative inaccuracies. Nonetheless and despite a systematic underestimating trend, precision remains within a very acceptable confidence interval. 
}
\begin{figure}[h]
\centering
\hspace*{-5mm}\begin{tabular}{ccc}
\hspace*{5mm}case (a) & \hspace*{5mm}case (b) & \hspace*{5mm}case (c)\-1mm]
& time ({\em hrs}) \\
\end{tabular}
\caption{\small Real workload time series corresponding to a VoD server demand from \cite{website:grnet}. Initial trace was scaled up by a factor of  to increase the mean workload. Trace is chopped into two separate processes (Trace I and II) corresponding to different activity levels.}
\label{fig:real_trace}
\end{figure}
\begin{table}[h]
\caption{Estimated Parameters from traces I and II separately.}
\begin{tabular*}{0.83\textwidth}{@{\extracolsep{\fill}} c c  c  c  c  c  c  c  c  c }
  \hline\0.5mm]
  \hline
   I &
  0.0013  & 0.0084  & 0.0039  &  0.0028 & 0.0032 &   & 0.022\\
  II &
  0.0049  & 0.0183  & 0.0118  &  0.0095 & 0.0005 &   & 0.041\\ \hline
\end{tabular*} 
\label{table2}
\end{table}
\begin{figure}[h]
\centering
\begin{tabular}{cc}
Steady-state distribution & Autocorrelation function \\
\includegraphics[width=.4\columnwidth]{ssd1} &
\includegraphics[width=.4\columnwidth]{autocorrelation1} \\
\includegraphics[width=.4\columnwidth]{ssd2} &
\includegraphics[width=.4\columnwidth]{autocorrelation2}\\
Number of current viewers & time lag  ({\em hrs})\\
\end{tabular}
\caption{\small Comparison of the empirical steady-state distribution and of the autocorrelation function of the real (blue curves) and the fitted (red curves) traces. Top two plots correspond to trace~I and bottom plots correspond two trace~II.}
\label{fig:SS&AC}
\end{figure} \newpage
\section{Large Deviation Principle and its interpretation}
\label{sec:ldp}
Consider a continuous-time Markov process , taking values in a finite state space , of rate matrix .  In our case  is a vectorial process , and . If the rate matrix  is irreducible, then the process  admits a unique steady-state distribution  satisfying . Moreover, by Birkhoff ergodic theorem, it is known that for any mapping , the sample mean of  at scale , i.e.   converges almost-surely towards the mean of  under the steady-state distribution, as  tends to infinity. The function  is often called the \emph{observable}. In our case, as we are interested in the variations of the current number of users ,  will simply be the function that selects the first component: . 
The large deviations principle (LDP), which holds for irreducible Markov processes on a finite state space \cite{Varadhan08a}, gives a efficient way to estimate the probability for the sample mean calculated over a large period of time  to be around a value  that deviates from the almost-sure mean:

The mapping  is called the large deviations spectrum (or the rate function). For a given function , it is possible to compute the theoretical large deviations spectrum from the rate matrix  as follows. One first computes, for each values of , the quantity  defined as the principal eigenvalue (\emph{i.e.,} the largest) of the matrix with elements  ( if  and 0 otherwise). Then the large deviations spectrum can be computed as the Legendre transform of : 


As described in Equation(\ref{eq:LD-proba}),   corresponds in our study case, to the mean number of users  observable over a period of time of length  and  relates to the probability of its occurrence as follows:


Interestingly also, if the process is strictly stationary (\emph{i.e.} the initial distribution is invariant) the same large deviation spectrum  can be estimated from a single trace, provided that it is "long enough'' \cite{Barral11a}. We proceed as follows: At a scale , the trace is chopped into  intervals  of length  and we have (almost-surely), for all :
4mm]
 \mbox{and }\displaystyle{ \lim_{\tau\to\infty} f_{\tau}(\alpha, \epsilon_{\tau}) =  f(\alpha).}
 \end{array}
\label{eq:falpha}

\alpha_{\tau}  = \Lambda_{\tau}^{\prime} (q) \quad \textrm{ and }  \quad \epsilon_{\tau} = \sqrt{ \frac{-\Lambda_{\tau}^{\prime\prime} (q)}{\tau} }.
\label{eq:alpha}
-26mm]
\hspace*{-72mm}\begin{turn}{90}\end{turn} 
& 
\hspace*{-56mm}\begin{turn}{90}\end{turn} 
&
\hspace*{-56mm}\begin{turn}{90}\end{turn}  \
\tau^* = \max\{\tau : \mathbb{P}{\{ \langle i \rangle_{\tau} \geq \alpha^* \} }  =  \int^\infty_{\alpha^*} e^{\tau f_{\tau}(\alpha)}\,d{\alpha}  \geq \sigma^*\},
\label{eq:meanusr}

C_0 & \:{\rm :} &  \int_{\alpha_{\rm a.s.}+C_0}^{\infty} \int_{\tau_{min}}^{\tau_{max}} e^{\tau\cdot f(\alpha)}\,{\rm d}\tau\,{\rm d}\alpha~ \leq ~p_{\rm loss} \nonumber \\\label{eq:ploss-1}
& \:{\rm :} &\int_{\alpha_{\rm a.s.}+C_0}^{\infty} \frac{e^{\tau_{max}\cdot f(\alpha)}-e^{\tau_{min}\cdot f(\alpha)}}{f(\alpha)} \,{\rm d}\alpha ~\leq ~p_{\rm loss}

\tau_{\rm min} = \frac{Q}{\alpha-(\alpha_{a.s.}+C_0)},
\label{eq:tau_min}

\begin{array}{c}
\displaystyle{C_0 = C-\alpha_{a.s.} \:{\rm :} \int_{C}^{\infty} \frac{-1} {f(\alpha)}\,e^{\frac{Q}{\alpha-C} f(\alpha)}\,{\rm d}\alpha ~ \leq ~ p_{\rm loss}},
\
a decreasing function of , which can be solved using a simple bisection technique. \\
As long as the server workload remains below ,  this resource dimensioning  guarantees that no loss occurs. All overrun above this value will produce losses, but we ensure that the frequency (probability) and duration of these overruns are such that the loss rate remains  conformed to the SLA. 
The proposed approach clearly contrasts  with resource over-provisioning  that does not seek at optimizing the {\sc capex} to comply with the loss probability tolerated in the SLA. 


The same provisioning scheme can straightforwardly be generalized to the case of several applications sharing a common set of resources. To fix the idea, let us consider an infrastructure provider that wants to  host  VoD servers over the same shared link. A corollary question is then to determine how many servers  can the fixed link capacity  support, while guaranteeing  a prescribed level of losses. If the servers are independent, the probability for two of them to undergo a flash crowd simultaneously is negligible. For ease and without loss of generality, we moreover suppose that they are identically distributed and modeled by the same LD spectrum  with the same nominal workload . 
Then, following the same reasoning as in the previous case of a single server, the maximum number  of servers reads:


where the safety margin  is defined as in expression (\ref{eq:ploss-2}).

Then, depending on the agreed {\it Service Level Agreements}, the infrastructure provider can easily offer different levels of probability losses (QoS) to its VoD clients, and adapt  the number of hosted servers, accordingly.
\begin{figure} [h]
\centering
\begin{tabular}{c}
\includegraphics[width=0.4\columnwidth]{cap_plan.eps}\\
{\normalsize Time}
\end{tabular}
\caption{\small Dimensioning , the number of hosted servers sharing a fixed capacity link . The safety margin  is determined according to the probabilistic loss rate negotiated in the {\it Service Level Agreement} between the infrastructure provider and the VoD service provider.}
\label{fig:caplan}
\end{figure}
\newpage

\section{Conclusion}
\label{sec:conclusion}
Many applications deployed on a cloud infrastructure, such as a Video on Demand service, are well known for undergoing highly volatile demand, making their workload hard to qualitatively and quantitatively characterize.
Adopting a constructive approach to capture the VoD users' behavior, in this report we  proposed a simple, concise and versatile model for generating the workload variations in such context.
We also devised an heuristic identification procedure that aims at estimating the parameters values of the model from a single collected trace. 
First, we numerically evaluated the accuracy of this procedure using several synthetic traces. 
Our experiments show that the procedure introduces little bias and typically recovers the actual parameters value with a relative error of about 10\%. 
Second, we apply this same procedure against two real video server workload traces. 
Obtained results demonstrate that, once the model has been calibrated, it succeeds to reproduce the statistical behavior of the real trace (in terms of both the steady-state probabilities and the autocorrelations for the workload time series).
Moreover, owing to the constructive nature of our model, the estimated values of the parameters provide valuable insight on the application that it would be difficult, or even impossible, to infer  from the raw traces. The captured information may answer questions of practical interest to cloud oriented service providers, like: is the application workload mostly driven by spontaneous behaviors, or is it rather subject to a  gossip phenomenon?

Furthermore, a key-point of this model is that it permits to reproduce the workload time series with a Markovian process, which is known to verify a Large Deviation Principle (LDP). This particularly interesting property yields a large deviation spectrum whose interpretation enriches the information conveyed by the standard steady state distribution: For a given observation (workload trace), LDP allows to infer (theoretically and empirically) the probability that the time average workload, calculated at an arbitrary aggregation scale, deviates from its nominal value (i.e. almost sure value). 

We leveraged this multiresolution probabilistic description to conceptualize two different management schemes for dynamic resource provisioning. As explained, the rationale is to use large deviation information to help network and service providers together to agree on the best {\sc capex}-{\sc opex} trade-off. Two major stakes of this negotiation are: {\it (i)} to determine the largest reconfiguration time scale adapted to the workload elasticity and {\it (ii)} to dimension VoD server so as to guarantee with upmost probability the Quality of Service imposed by the negotiated  Service Level Agreement.
\newline
More generally though, the same LDP based concepts can benefit any other ``Service on Demand" scenarii to be deployed on dynamic cloud environments.  \newpage

\bibliographystyle{IEEEbib}
\bibliography{rr_reso}

\end{document}
