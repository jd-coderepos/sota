\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{acl} 

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}



\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{url}
\usepackage{soul,xcolor}
\usepackage{graphicx} \usepackage{amsmath}
\usepackage{multicol,multirow}
\usepackage{array}
\usepackage[capitalize]{cleveref}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{changepage}
\usepackage{amssymb}
\crefformat{section}{\S#2#1#3} \crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\usepackage{pifont}
\usepackage{mdframed}
\let\realcite\cite
\renewcommand{\cite}[1]{\ifx.#1.\hl{[?]}\else\realcite{#1}\fi}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\dataset}{\texttt{MusicBench}}
\newcommand{\datasetFMA}{\texttt{FMACaps}}
\newcommand{\model}{\texttt{Mustango}}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}



\newcommand{\modelfig}{\raisebox{-\ht\strutbox}{\includegraphics[width=0.06\textwidth]{Sections/Figure/mustango2.png}}
  \parbox{0.08\textwidth}{
   \model{}
  }}
\newcommand{\modelemoji}{\model{}\includegraphics[height=1.4\fontcharht\font0]{Sections/Figure/mustango2.png}}
\newcommand{\greencheck}{{\color{green}\cmark}}
\newcommand{\redcross}{{\color{red}\xmark}}


\newcommand{\navo}[1]{\textcolor{blue}{\bf [#1]}}
\newcommand{\dhs}[1]{\textcolor{teal}{\bf [#1]}}



\title{Towards Controllable Text-to-Music via Music Feature Conditioned Latent Diffusion}

\title{\model{}: Toward Controllable Text-to-Music Generation}





\author{Jan Melechovsky\thanks{\hspace{0.2cm} Co-first authors. Both authors contributed equally.} ,
  Zixun Guo\footnotemark[1] ,
  Deepanway Ghosal, \\
  \textbf{Navonil Majumder,
  Dorien Herremans\thanks{\hspace{0.2cm} Both authors contributed equally and led this project.},
  Soujanya Poria\footnotemark[2]}
  \\
   Singapore University of Technology and Design, Singapore\\
   Queen Mary University of London, UK
}
  
\begin{document}
\maketitle
\begin{tikzpicture}[remember picture,overlay,shift={(current page.north west)}]
\node[anchor=north west,xshift=3.1cm,yshift=-2.2cm]{\scalebox{1}[1]{\includegraphics[width=1cm]{Sections/Figure/mustango2.png}}};
\end{tikzpicture}
\begin{abstract}


With recent advancements in text-to-audio and text-to-music based on latent diffusion models, the quality of generated content has been reaching new heights. The controllability of musical aspects, however, has not been explicitly explored in text-to-music systems yet.
In this paper, we present \model{}, a music-domain-knowledge-inspired text-to-music system based on diffusion, that expands the \texttt{Tango} text-to-audio model. \model{} aims to control the generated music, not only with general text captions, but from more rich captions that could include specific instructions related to chords, beats, tempo, and key. 
As part of \model{}, we propose MuNet, a Music-Domain-Knowledge-Informed UNet sub-module to integrate these music-specific features, which we predict from the text prompt, as well as the general text embedding, into the diffusion denoising process. 
To overcome the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract the music features which will then be appended to the existing descriptions in text format. We release the resulting \dataset{} dataset which contains over 52K instances and includes music-theory-based descriptions in the caption text. 
Through extensive experiments, we show that the quality of the music generated by \model{} is state-of-the-art, and the controllability through music-specific text prompts greatly outperforms other models in terms of desired chords, beat, key, and tempo, on multiple datasets. 
\end{abstract}

\section{Introduction}
\label{sec:intro}







In recent years, diffusion models~\cite{popov2021diffusion} have shown prowess in image~\cite{dalle2} and audio~\cite{liu2023audioldm,ghosal2023tango, borsos2023audiolm} generation tasks. Several attempts have also been made to generate music~\cite{huang2023noise2music, mousai} using diffusion models. Within the realm of audio, music occupies its own unique space, characterized by its rhythmic intricacies and distinctive harmonic or melodic structures. As such, this paper aims to harness the power of diffusion models, equipped with music-domain knowledge, to generate audio music fragments directly from text prompts.

Generating music directly from a diffusion model presents new challenges due to the unique nature of music. Firstly, achieving a balance between alignment with the conditional text and musicality in the generated music is not trivial. Recently, \citet{agostinelli2023musiclm} proposed \texttt{MusicLM} to ensure the music generated matches the input texts (e.g., correct instrumentation, music vibe). However, the matter of musicality, such as musically meaningful harmonies and consistent performance attributes (e.g., tempo), remains only partially addressed. Secondly, the availability of paired music and textual description datasets is limited~\cite{agostinelli2023musiclm, huang2023noise2music}. Although the textual descriptions in the existing datasets include details like instrumentation or vibe, the more representational description that captures the structural, melodic, and harmonic aspects of music is missing from the existing datasets. We thus argue that including this information during generation may improve the current text-to-music systems in terms of musicality---following metrical structure, chord progressions---and controllability. Beyond existing text-to-music systems' capability (e.g., setting correct instrumentation), our proposed \model{} model enables musicians, producers and sound designers to create music clips with specific conditions like following a chord progression, setting tempo, and key selection.


In this paper, we propose \modelemoji{} to address these challenges through our novel data augmentation pipeline, a Music-Domain-Knowledge-Informed UNet module, \texttt{MuNet}, to replace the traditional UNet during diffusion. Our data augmentation method has two major components: \emph{description enrichment} and \emph{music diversification}. The aim of \emph{description enrichment} is to augment the existing text descriptions with beats and downbeats location, underlying chord progression, key, and tempo as control information. During inference, these additional descriptive text could successfully steer the music generation towards user-specified music quality. We use state-of-the-art music information retrieval (MIR) methods~\cite{chordino,heydari2021beatnet,bogdanov2013essentia} to extract such control information from our training data. Subsequently, we append these information (in text format) to the existing text descriptions and use ChatGPT to rephrase them into a coherent and descriptive text. 
Furthermore, to diversify the music samples in the training set, we augment this dataset with variants of the existing music, altered along three aspects---tempo, pitch\footnote{\url{https://github.com/bmcfee/pyrubberband}}, and volume---that essentially determine the rhythmic, harmonic, and interpretive aspect of music. The text descriptions are also altered accordingly. Consequently, we increase the size of the original dataset by 11-fold, including a larger variety of musical qualities as well as text descriptions. We call this augmented dataset \dataset{}.

Our proposed diffusion-based generative model incorporates a novel MuNet as compared to a normal UNet. With the proposed MuNet, extracted music domain information: chords, beats, key, tempo in non-text format along with text conditions can be leveraged to guide the music generation during the reverse-diffusion towards user-specified music quality. The results in \cref{sec:experiment} suggest that this results in more musically-meaningful generation and improved controllability through user input (e.g., changing chords).






The overall contributions of this paper are summarized as follows:
\begin{enumerate}[itemsep=0pt, leftmargin=*, wide, labelwidth=0pt, labelindent=0pt, parsep=0pt]
\item We develop \modelemoji{}, a text-to-music diffusion model that can understand music-specific text captions, such as those containing chord information. \model{} includes a dedicated MuNet module to explicitly guide the music generation with beat, chord, as well as general text information during reverse diffusion. 
  \item We release an open, large dataset, \dataset{}, with music audio as well as text captions that contain music specific descriptions (e.g. about chords, key, beats, etc.). This is based on a novel augmentation method that can alter music audio in terms of harmony, tempo, and volume, as well as perform a music-specific text-augmentation that allows us to add additional control information to text captions such as beats and downbeats location, underlying chord progression, key, and tempo. Training on such enriched data allows for more robust and controllable music generation. 
\item We verify in extensive experiments that our final model, \model{}, is able to generate high quality music based on text captions. We also verified that \model{} enables more powerful textual control---with respect to chords and beats---of the generated music as opposed to text-only guidance.
\end{enumerate}

Our newly released \dataset{} dataset and \modelemoji{} model implementation are available online \footnote{\url{https://github.com/amaai-lab/mustango}}. In what follows we will first discuss existing state-of-the-art text-to-music models, followed by a section on the new dataset \dataset{}. In \Cref{sec:method} we will describe the details of our proposed \model{} model. This is followed by extensive experiments, results, and finally a conclusion.  































































%
 \section{Related Work}\label{sec:relw}






In this section, we begin by describing existing state-of-the-art research on text-to-audio generation, followed by the more specific domain of text-to-music generation.  


In the field of sound generation, the AudioLM model~\cite{borsos2023audiolm} leverages the state-of-the-art semantic modeling model w2v-Bert~\cite{DBLP:conf/asru/ChungZHCQPW21} and the acoustic model SoundStream~\cite{DBLP:journals/taslp/ZeghidourLOST22} to generate audio from audio prompts in a hierarchical approach. Semantic tokens are first generated by w2v-Bert, and are then used to condition the generation of acoustic tokens which will be decoded using SoundStream. 

AudioLDM~\cite{liu2023audioldm} is a text-to-audio framework that leverages CLAP \cite{wu2023large}, a joint audio-text representation model, and a latent diffusion model (LDM). More specifically, an LDM is trained to generate latent representations of a melspectrogram which are obtained using a VAE. During diffusion, the CLAP embeddings are utilized to guide the generation. \texttt{Tango} \cite{ghosal2023tango} leverages the pre-trained VAE from AudioLDM and replaces the CLAP model with an instruction fine-tuned large language model: FLAN-T5 to achieve comparable or better results while training with a much smaller dataset.








In the field of music generation, there is a long history of generated MIDI music \cite{herremans2017functional}. Using MIDI may be useful for producers to work with in Digital Audio Workstations, yet it has the disadvantage that datasets are extremely limited. In the last year, however, models that directly generate \textit{audio} music from text captions have emerged, such as MusicLM~\cite{agostinelli2023musiclm}. This model uses two pre-trained models, MuLan~\cite{DBLP:conf/ismir/HuangJLGLE22}, a joint text-music embedding model, and w2v-Bert~\cite{DBLP:conf/asru/ChungZHCQPW21}, a masked language model to address the challenge of maintaining both synthesizing quality and coherence during music generation. These two pre-trained models are then utilized to condition the acoustic model SoundStream~\cite{DBLP:journals/taslp/ZeghidourLOST22} which in turn can generate acoustic tokens autoregressively. These acoustic tokens are then decoded by SoundStream to become the final audio output. MusicLM outperforms two existing commercially available text-to-music software: Mubert\footnote{\url{https://github.com/MubertAI/Mubert-Text-to-Music}} and Riffusion\footnote{\url{https://www.riffusion.com/}} in terms of Frechet Audio Distance, Faithfulness to the text description, KL divergence, and Mulan Cycle Consistency. Since no publications are linked to these latter two systems, the model details are not available. 




Another text-to-music model is Noise2Music~\cite{huang2023noise2music}. To obtain training data for the model, the authors propose a method to obtain a large amount of paired music and text data in which LaMDA-LF~\cite{DBLP:journals/corr/abs-2201-08239}, a large language model, is used to generate multiple generic candidate text descriptions. The aforementioned joint text-music embedding MuLan is then utilized to select the best candidates for existing music data. The obtained music and text pairs are then used to train a two-stage diffusion model, where the first diffusion model generates an intermediate representation and the second generates the final audio output. 

In recent months, a number of text-to-music models have come out. \citet{mousai} proposes a 2-stage diffusion model in which the first diffusion magnitude autoencoder (DMAE) learns a meaningful latent representation of music (64 times smaller than the input), while in the second diffusion model, text condition along with the latent acquired at the first stage is included to guide the final music generation.  MusicGen \cite{copet2023simple} utilizes a single-stage transformer LM with efficient token interleaving patterns to achieve high quality generation and better controlabillity over the output. MusicGen can be conditioned by a text prompt, or by an audio fragment in form of a chromagram. The system was trained with a licensed dataset. The JEN-1 model \cite{li2023jen} is an omnidirectional diffusion model designed to perform various tasks such as text-guided music generation, music inpainting, and continuation. 
Another interesting recent model is that of \citet{su2023v2meow}, which focuses on generating music pieces to complement video, conditioned on both video and text inputs. Unlike text, video conditioning can contain a lot of temporal information, such as beats and emotions, which are important for music. 

When it comes to controllability of text-to-music systems in musical terms like chords, tempo, key, and time signature, there has been a lack of research. Additionally, most of the research in text-to-music uses internal datasets and do not release their codes. In this work, we lay the foundations for filling this research gap by targetting musical controllability specifically and making all our contributions public.










































%
 \section{Dataset Creation}\label{sec:method}


In this section, we describe the creation of our \dataset{} dataset. First, the methods of music feature extraction and data augmentation are introduced, then the details of our dataset and how we applied these methods to it are discussed.

\subsection{Feature Extraction and Description Enrichment}\label{sec:method_feature_extract}

We extract four common music features---beats and downbeats, chords, keys, and tempo---that guide music generation and enhance the original text prompt. 




We utilize BeatNet~\cite{heydari2021beatnet} to extract the beats and downbeats features:  where the first dimension represents the type of beat according to the meter (e.g., 1, 2, 3) and the second represents the timing of each corresponding beat in seconds. The second feature, Tempo in Beats Per Minute (BPM), is estimated by averaging the reciprocal of the time interval between beats. Chordino~\cite{chordino} is used to extract chords features:  where the first dimension represents the roots of the chord sequence, the second represents the chord type (e.g., major, minor, maj7, etc.) and the third represents whether the chords are inverted. Finally, Essentia's \cite{bogdanov2013essentia} KeyExtractor algorithm\footnote{\url{https://essentia.upf.edu/reference/std_KeyExtractor.html}} is used to extract the key. These extracted features will be used to both enrich the text description and guide the reverse diffusion process. 






These features are then expressed in text format following several text templates (e.g., 'The song is in the key of A minor. The tempo of this song is Adagio. The beat counts to 4. The chord progression is Am, Cmaj7, G.'). We refer to these as control sentences and they will be appended to the original text prompt to form the enhanced prompts. A full list of the different control sentence templates can be found in the Appendix.




\subsection{Augmentation and Music Diversification}\label{sec:method_data_aug}

In this section, we introduce our dataset augmentation for both music audio and text prompts which boosts the total amount of training data by 11 fold, in order to increase both the audio quality and controllability of the model. Standard text-to-audio augmentations might not suit the nature of music audio. For example, the augmentation utilized in Tango \cite{ghosal2023tango}, whereby two audio samples of similar audio pressure levels are overlapped and their prompts concatenated, would not work for music by introducing two overlapping rhythms, dissonance in harmony, and overall musical concept mismatch.





Therefore, we augment single music \textit{audio} samples from either one of the three perspectives: pitch, speed, and volume which determines the melodic, rhythmic, and dynamic aspects of music. We use PyRubberband\footnote{\url{https://github.com/bmcfee/pyrubberband}} to shift the pitch of the music audio within a range of 3 semitones following a uniform distribution. We change the speed of the music audio by (5 to 25)\%, drawn from uniform distribution as well. Finally, we alter the volume of the audio by introducing a gradual volume change (both crescendo and decrescendo) with the minimum volume drawn from a uniform distribution from 0.1 to 0.5 times the original track's amplitude, with the maximum kept untouched. We notice a similar augmentation approach in concurrent research~\cite{gardner2023llark}. 

The corresponding text description will be changed as follows. We changed the enhanced prompts in the previous section accordingly based on the changes we have committed. To enhance the robustness of the model, we randomly discard one to four sentences from the prompt which describes the aforementioned four music features. More details are illustrated in the Appendix. Finally, we used ChatGPT to rephrase the text prompt to add variety to the text prompts. 


\subsection{MusicBench}
In this study, we make use of the MusicCaps~\cite{agostinelli2023musiclm} dataset, which comprises a collection of 5,521 audio clips featuring music. Each clip is 10 seconds long and is sourced from the train and evaluation splits of the AudioSet~\cite{gemmeke2017audio} dataset. These audio clips are accompanied by on average four-sentence-long texts that describe the music. However, due to the inaccessibility of some audio files, our dataset consists of 5,479 samples.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Sections/Figure/dataflow4.png}
    \caption{Composition of the \dataset{} dataset.}
    \label{fig:data}
\end{figure}


We split our dataset as shown in \Cref{fig:data}. First, we extract music features (for later use) from all the samples and split the data into TrainA and TestA sets. 
By concatenating all four control sentences to the original prompts, we obtain the TrainB and TestB sets. Then, by instructing ChatGPT to rephrase the TrainB text prompts, we get the final TrainC set. 


In addition, before performing audio augmentation, we filter out samples that mention `low quality' and similar terms in the captions of TrainA set, to get 3,413 instances. These higher quality samples are audio-augmented (see \Cref{sec:method_data_aug}) to to form a set of 37k samples. 
Following this, we randomly select control prompts to be concatenated with the original captions. We pick  prompts with a probability of  respectively. We then rephrase all of the captions using ChatGPT. In our final training dataset, we use both of the rephrased and non-rephrased prompts with a probability of  respectively. Finally, we take this augmented set and concatenate it with sets TrainA, TrainB, and TrainC to get our final training set consisting of 52,768 samples, further referred to as \dataset{}.










































\section{\modelfig{}}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Sections/Figure/mustango.pdf}
    \caption{Depiction of our proposed \model{} model. Beats and chords are inferred from the caption when they are not provided as input.}
    \label{fig:model}
\end{figure*}









\model{} consists of two components: 1) \texttt{Latent Diffusion Model}; 2) \texttt{MuNet}.







\subsection{Latent Diffusion Model (LDM)}

Inspired by \texttt{Tango}~\cite{ghosal2023tango} and \texttt{AudioLDM}~\cite{Liu2023AudioLDMTG}, we leverage the latent diffusion model (LDM) to reduce computational complexity meanwhile maintaining the expressiveness of the diffusion model. More specifically, we aim to construct the latent audio prior  extracted using an extra variational autoencoder (VAE) with condition  , which in our case refers to a joint music and text condition. Similar to \texttt{Tango}, we leverage the pre-trained VAE from AudioLDM~\cite{liu2023audioldm} to obtain the latent code of the audio.

Through the forward-diffusion process (Markovian Hierarchical VAE), the latent audio prior  turns into a standard gaussian noise , as shown in \cref{eq:forward_diff} where a pre-scheduled gaussian noise () is gradually added at each forward step:


In the reverse process of reconstructing  from Gaussian noise , we propose \texttt{MuNet}, which is able to steer the generated music towards the given condition . We will elaborate the details of \texttt{MuNet} in the next Section. Intuitively, the backward diffusion aims to reconstruct latent audio prior  from the prior step  until  is reconstructed as shown in \Cref{eq:munet}-\Cref{eq:guidance-scaling}:  


 is the guidance scale in \cref{eq:guidance-scaling} used during inference. During training however,  is directly used for noise estimation where the conditions  are randomly dropped as specified in \cref{sec:trainsetup}.

This reconstruction is trained using a noise-estimation loss, as defined in \cref{eq:loss}, where  is the estimated noise and  is the weight of reverse step :









\subsection{\texttt{MuNet}}

The reverse-diffusion process, described in \crefrange{eq:munet}{eq:guidance-scaling}, is conditioned on both music (beat  and chord ) and text  (). This is realized through the Music-Domain-Knowledge-Informed UNet (MuNet) denoiser whose noise estimator is defined as 

where MHA is multi-headed attention used for cross attention, where  and  are query, key, and value, respectively. 

MuNet follows a structure similar to UNet~\cite{UNET}, consisting of multiple downsampling, middle, and upsampling blocks, and the conditions are incorporated via cross attention. In the MuNet, we propose two encoders  and  to encode the beat and chord features which leverage the state-of-the-art Fundamental Music Embedding (FME) and Music Positional Encoding (MPE)~\cite{FME} to ensure the musical features are properly captured and several fundamental music properties (e.g., translational invariance) are preserved~\cite{FME}. 

We hereby introduce the details of the two encoders:  and  that extract the beat and chord embeddings from the raw input. In the beat encoder  formulated in \Cref{eq:beat_enc}, we employ a One-Hot Encoding () to encode the beat type:  and a Music Positional Embedding ()~\cite{FME} to capture the timing of the beats: . By concatenating these beat types and timing embeddings and passing them through a trainable linear layer (), we obtain the final embedded beat feature. 







In the chord encoder in \Cref{eq:chord_enc}, we obtain the chord embeddings by first concatenating 1. FME-embedded~\cite{FME} chord roots (); 2. One-Hot encoded chord type (); 3. One-Hot encoded chord inversions () and; 4. MPE-embedded~\cite{FME} timing of the chords (). Subsequently, this concatenated representation is passed through a trainable linear layer (). Notably, we incorporate a music-domain-knowledge informed music embedding through the use of the Fundamental Music Embedding ()~\cite{FME}, which effectively captures the translational invariant property of pitches and intervals, resulting in a more musically meaningful representation of the chord. 

 

After obtaining the encoded beat and chord embeddings, we use two additional cross-attention layers to integrate these music conditions during the denoising process, as compared to \texttt{TANGO}~\cite{ghosal2023tango} which only use one cross-attention layer to incoporate text conditions (see \cref{eqn:noise-estimator}). This enables MuNet to leverage music features as well as text features during the denoising process, resulting in more controllable and musically meaningful music generation.




\subsection{Inference}
During the training phase, we use teacher forcing and hence utilize the ground truth beats and chord features to condition the music generation process. However, during inference, we adopt a different approach. We employ two transformer-based text-to-music-feature generators that have been trained independently to predict the beat and chord features as follows: 


\textbf{Beats}: We use the DeBERTa Large model ~\cite{he2022debertav3} as the beats predictor. The model takes the text caption as input and predicts the following: i) the maximum beat of corresponding music, and ii) the sequence of interval duration between the beats. We predict them from the token-level representations of the final layer of the DeBERTa model. The maximum beat takes an integer value between 1 and 4 for the music instances in our training dataset. Hence, we predict the maximum beat using a four-class classification setup from the first token of the DeBERTa output layer. The interval durations are predicted as a float value from the second token onwards. As an example, if the maximum beat is predicted as  and the interval durations are predicted as , then the predicted beats are as follows:  at ,  at ,  at , etc. We keep the predicted beats time up to 10 seconds and ignore predicted timestamps beyond that. 

\textbf{Chords}: We use the sequence to sequence FLAN-T5 Large model ~\cite{chung2022scaling} as the chords predictor. The model takes the concatenation of the text caption and the verbalized beats as input. The verbalized beats are prepared for the example we illustrated earlier as follows: \textit{Timestamps: , , , Max Beat: }. The model is trained to generate the verbalized chords sequence with timestamps, which would look like something as follows: \textit{Am at 1.11; E at 4.14; C\#maj7 at 7.18}. We again keep the predicted chord time up to 10 seconds and ignore timestamps predicted beyond that. 




















 \section{Experiments}
\label{sec:experiment}
In this section, we describe our thorough experiments that aim to answer the following research questions:
\begin{itemize}[itemsep=0pt, leftmargin=*, wide, labelwidth=0pt, labelindent=0pt, parsep=0pt]
    \item How good is the audio quality of the music generated by \modelemoji{}. Is it comparable or better than the output by Tango \cite{ghosal2023tango}?
    \item Does \modelemoji{} achieve better musical quality than Tango?
    \item Is \modelemoji{} more controllable in term of music-specific instructions, compared to Tango?
    \item Is our data augmentation approach effective in enhancing performance, and can models trained on only this dataset compete with large-scale audio pre-trained models?
\end{itemize}
To answer these questions, we deploy extensive objective and subjective evaluations. 


















\subsection{Baselines and \modelemoji{} Variants}




We primarily compare \modelemoji{} with \texttt{Tango}~\cite{ghosal2023tango}, a latent diffusion model for audio generation that shares the same architecture with \modelemoji{}, except the lack of the extra conditioning on beats and chords of MuNet.
To judge the efficacy of \modelemoji{}, we train the following three models from scratch:
1) Tango trained on MusicCaps TrainA, 2) Tango trained on \dataset{}, 3) \modelemoji{} trained on \dataset{}. Additionally, we finetune Tango and \modelemoji{} from pre-trained Tango checkpoints: 4) \texttt{Tango} trained on TangoPromptBank \cite{ghosal2023tango} and fine-tuned on AudioCaps, and MusicCaps~\footnote{\url{https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps}}, 5) \texttt{Tango} checkpoint fine-tuned on AudioCaps, now finetuned on \dataset{}~\footnote{\url{https://huggingface.co/declare-lab/tango-full-ft-audiocaps}}, 6) \modelemoji{} initialized from pre-trained \texttt{Tango} checkpoint and finetuned on \dataset{}. 

\subsection{Training}
\label{sec:trainsetup}
All the models were trained at a learning rate of  using the AdamW~\cite{loshchilov2017decoupled} optimizer until convergence.
Our Beat and Chord predictors are also trained on \dataset{}.

To further improve the robustness of \modelemoji{}, we use these three dropouts during training-data loading:
\begin{enumerate}[itemsep=0pt, leftmargin=*, wide, labelwidth=0pt, labelindent=0pt, parsep=0pt]
    \item With 5\% probability, drop all the inputs (text, beats, and chords);
    \item With 5\% probability, drop an input feature (applied to each of the inputs separately);
    \item We determine the probability of masking a prompt as \%, where  represents the number of sentences in the current prompt, and  is the average number of sentences per prompt. Once a prompt is chosen for masking, we randomly draw an integer  from a uniform distribution in the range [20, 50] and proceed to remove \% of the input sentences in the prompt.
\end{enumerate}


The idea behind the first two dropouts is to enable the model to work with incomplete, faulty, or missing input information. The third dropout is aimed at improving robustness for short text inputs. We apply these dropouts to Tango as well, with a small modification: since Tango does not use music feature inputs, we replace the first two dropouts with a single 10 \% probability of dropping all text.







\subsection{Objective Evaluation}
\label{sec:objeval}
The quality of the generated audio samples is evaluated concerning three objective metrics: Fréchet Distance (FD), Fréchet Audio Distance (FAD) \cite{kilgour2019frechet}, and Kullback-Leibler divergence (KL).
To measure the distance between the distributions of generated audio and ground truth reference, KL divergence is used. This metric is applied to the labels generated by a pre-trained classifier, namely PANNs \cite{kong2020panns}, large pre-trained audio networks for audio pattern recognition. As in \cite{liu2023audioldm,ghosal2023tango} we also utilize PANNs to calculate FD, which is a similarity metric between two curves.
FAD is an evaluation metric similar to FID (Fréchet Inception Distance) from the image domain; which was specifically designed for the audio domain. It is based on human perception and it is computed through the use of a VGGish classifier.

\subsubsection{Out-of-Distribution Evaluation}
Given that the fine-tuned models used in our experiments were exposed to the entire MusicCaps dataset when training the initial pre-trained checkpoint, we can only fairly evaluate those models on our independently-created evaluation set, which we refer to as \datasetFMA{}. We source the new music files from the Free Music Archive (FMA) \cite{defferrard2016fma}, a large dataset of popular songs. In particular, we took 1,000 random samples from FMA-large and clipped out a random 10-second fragment from each of them. Then, we used Essentia's tagging models~\cite{bogdanov2013essentia} to assign tags to audio. Specifically, we used the models for general auto-tagging, mood, genre, instrumentation, voice, and voice gender which provide us with a rich set of tags along with their probabilities. Then, a music expert wrote text descriptions for 25 of the samples based on the audio as well as the extracted tags. Next, we instructed ChatGPT to perform an in-context learning task to get pseudo-prompts from tags for the rest of the dataset. Finally, we added relevant control sentences to the prompts after extracting relevant music features, as described in \cref{sec:method_feature_extract}. Similar to our training set, we added 0/1/2/3/4 control sentences with a probability of 25/30/20/15/10\% respectively. We refer to this evaluation set as \datasetFMA{}.

\subsubsection{Evaluation of Controllability}
\label{sec:ctrl_eval}
To evaluate the models in terms of controllability, we utilize TestB as depicted in \cref{fig:data}, as well as a modified version of \datasetFMA{} that has all the control sentences for each sample in the prompt. We generated music based on the input text prompts from the test sets. From these generated music, we extracted several musical features (see \cref{sec:method_feature_extract}) so that we could compare them to the features specified in the input text prompts. To quantify this comparison, we developed a number of control metrics, all of which are represented in percentage, hence ranging from 0 to 100. In case a metric is binary, it will be represented as 100 for True, and 0 for False. The metrics are defined as: \begin{itemize}[itemsep=0pt, leftmargin=*, wide, labelwidth=0pt, labelindent=0pt, parsep=0pt]
    \item \textbf{Tempo bin (TB)} --- the predicted beats per minute (bpm) fall into the ground truth tempo bin.
    \item \textbf{Tempo bin with tolerance (TBT)} --- the predicted bpm falls into the ground truth tempo bin or a neighboring one.
    \item \textbf{Correct key (CK)} --- the predicted key matches the ground truth key. 
    \item \textbf{Correct key with duplicates (CKD)} --- the predicted key matches the ground truth key or an equivalent key. We consider major and its equivalent minor as duplicates.  (e.g., C major and A minor). 
    \item \textbf{Perfect chord match (PCM)} --- the predicted chord sequence perfectly matches ground truth in terms of length, order, chord root, and chord type. 
    \item \textbf{Exact chord match (ECM)} --- the predicted chord sequence matches the ground truth exactly in terms of order, chord root, and chord type, with tolerance for missing and excess chord instances. 
    \item \textbf{Chord match in any order (CMAO)} --- the portion of predicted chord sequence that matches the ground truth in both chord root and chord type, in any order. 
    \item \textbf{Chord match in any order major/minor type (CMAOMM)} --- the portion of predicted chord sequence that matches the ground truth in terms of chord root and binary major/minor chord type, in any order (e.g., D, D6, D7, Dmaj7 are all considered major). 
    \item \textbf{Beat count prediction (BC)} --- the percentage of predicted beat counts that match the ground truth. 
\end{itemize}






\begin{table*}[ht!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Datasets}} & \multirow{2}{*}{\textbf{Pre-trained}} & \multirow{2}{*}{\textbf{\#Params}} & \multicolumn{3}{c}{\textbf{TestA}} & \multicolumn{3}{|c}{\textbf{TestB}} & \multicolumn{3}{|c}{\textbf{\datasetFMA{}}} \\ & & & & FD~ & FAD~ & KL~ & FD~ & FAD~ & KL~ & FD~ & FAD~ & KL~ \\
\midrule
\texttt{Tango} & \texttt{MusicCaps} & 
\redcross & M & 30.80 & 2.84 & 1.34 & 30.39 & 2.92 & 1.33 & 28.32 & 3.75 & 1.22 \\
\texttt{Tango} & \texttt{MusicCaps} & 
\greencheck & M & 34.87 & 4.05 & 1.25 & 37.85 & 4.52 & 1.32 & 28.81 & 2.92 & 1.21 \\
\texttt{Tango} & \dataset{} & 
\redcross & M & 28.50 & 2.29 & 1.33 & 28.27 & 2.17 & 1.32 & 26.31 & 2.31 & 1.16 \\
\texttt{Tango} & \dataset{} & 
\greencheck & M & 25.38 & 1.91 & 1.19 & 24.60 & 1.77 & 1.13 & 24.48 & 2.96 & 1.15 \\
\modelemoji{} & \dataset{} & 
\redcross & B & 26.58 & 2.09 & 1.21 & 25.24 & 1.57 & 1.18 & 24.24 & 2.94 & 1.16 \\
\modelemoji{} & \dataset{} & 
\greencheck & B & 26.35 & 1.46 & 1.21 & 25.97 & 1.67 & 1.12 & 25.18 & 2.34 & 1.16 \\
\bottomrule
\end{tabular}
}
\caption{Objective evaluation results of the models on all the test datasets. The columns show the average value for each of the metrics per model. }
\label{tab:obj_eval}
\end{table*}





\begin{table*}[ht!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccc|cc|cc|cccc|c|cc|cc|cccc|c}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multirow{3}{*}{\textbf{Datasets}} & \multirow{3}{*}{\textbf{Pre-trained}} & \multicolumn{9}{c}{\textbf{TestB}} & \multicolumn{9}{|c}{\textbf{\datasetFMA{}}} \\ \hline
& & & \multicolumn{2}{c}{\textbf{Tempo}} & \multicolumn{2}{|c}{\textbf{Key}} & \multicolumn{4}{|c}{\textbf{Chord}} & \multicolumn{1}{|c}{\textbf{Beat}} & \multicolumn{2}{|c}{\textbf{Tempo}} & \multicolumn{2}{|c}{\textbf{Key}} & \multicolumn{4}{|c}{\textbf{Chord}} & \multicolumn{1}{|c}{\textbf{Beat}} \\
& & & TB & TBT & CK & CKD & PCM & ECM & CMAO & CMAOMM & BC & TB & TBT & CK & CKD & PCM & ECM & CMAO & CMAOMM & BC \\
\midrule
\texttt{Tango} & \texttt{MusicCaps} & 
\redcross & 26.00 & 55.25 & 4.00 & 7.00 & 0.53 & 2.09 & 4.30 & 11.13 & 41.00 & 22.50 & 49.60 & 3.60 & 8.60 & 0.64 & 1.43 & 4.03 & 10.82 & 41.10\\
\texttt{Tango} & \texttt{MusicCaps} & 
\greencheck & 27.50 & 52.00 & 7.75 & 11.25 & 1.06 & 3.07 & 6.72 & 13.99 & 36.75 & 24.20 & 48.60 & 5.90 & 8.60 & 1.17 & 2.74 & 5.17 & 12.69 & 35.40  \\
\texttt{Tango} & \dataset{} & 
\redcross & 24.75 & 50.75 & 34.25 & 34.50 & 5.56 & 12.03 & 21.54& 32.21& 34.25 & 25.50 & 51.00 & 38.10 & 38.40 & 6.60 & 13.45 & 21.18 & 41.49 & 36.40 \\
\texttt{Tango} & \dataset{} & 
\greencheck & 26.00 & 48.75 & 30.25 & 31.00 & 6.61 & 13.33 & 22.53 & 39.31 & 38.50 & 22.80 & 45.60 & 30.60 & 31.70 & 7.55 & 14.72 & 22.35 & 44.46 & 36.00\\
\modelemoji{} & \dataset{} & 
\redcross & 25.50 & 52.00 & 41.75 & 42.50 & 17.99 & 32.61 & 48.74 & 68.46 & 42.00 & 24.10 & 50.90 & 36.80 & 37.30 & 23.94 & 35.43 & 49.59 & 75.83 & 42.60\\
\modelemoji{} & \dataset{} & 
\greencheck & 21.25 & 48.25 & 34.50 & 35.50 & 11.64 & 20.82 & 32.93 & 50.56 & 34.75 & 26.20 & 52.20 & 33.90 & 34.70 & 15.21 & 25.48 & 37.50 & 61.55 & 39.10\\
\bottomrule
\end{tabular}
}
\caption{Controllability evaluation results for \modelemoji{} and Tango models on both TestB and full-control variant of \datasetFMA{}. The columns show the average value for each of the metrics per model. Higher numbers indicate better controllability.  }
\label{tab:control_eval}
\end{table*}

\subsection{Subjective Evaluation}
\label{sec:subjeval}
In addition to objective evaluation, we also performed subjective evaluation in the form of two listening tests: a general listening test and an expert listening test that focuses on controllability. 

For the general listening test, subjects listened to ten generated music samples for each of the four models and were provided with the input text caption. The four models included \modelemoji{}, both pre-trained as well as trained from scratch. The ten text prompts were custom-made by music experts in the style of musicCaps, and are shown in \Cref{tab:controlprompts} in the Appendix. The participants were asked to rate the 1) Audio quality (AQ), 2) Relevance of the audio to the input text prompt (REL), 3) Overall musicality (OM), 4) Rhythm presence and stability (RP), 5) Harmony and consonance of music (HC). All the aspects were rated on a 7-point Likert scale using the PsyToolkit interface \cite{stoet2010psytoolkit}. The full questions and interface used are shown in the Appendix.  

For the expert listening test, we found expert raters with at least 5 years of formal musical training who are able to recognize chords from music audio. They were presented with 80 samples to rate generated using 20 text prompts for each of the four models as shown in \Cref{tab:controlprompts}. The text prompts were custom made by music experts and consisted of ten `contrasting' pairs. Care was given to make sure that they were realistic and that there were no contradicting elements in the prompts. For instance, caption 1 in Table contrasts with caption 2. The main text is the dame: ``An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist’s strumming keeps the rhythm steady.''. But the control sentences are different: ``The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute.'' versus ``The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute.'' Both chord sequences come from blues progressions, but they belong to a different key/mode. The tempo of caption 2 is significantly slower. Such captions are ideally suited to test if the control sentences influence the generated music. In addition to the five aspects to rate from the general listening study, we added 2 music control-specific aspects to rate. These include the degree to which the chords from the generated music match those specified in the text-prompt (Chord Match or MCM), and the degree to which the tempo of the generated music matched the tempo specified in the text-prompt (Tempo match or MTM).



\subsection{Experimental Results}
\subsubsection{Objective Evaluation}
Results of objective evaluation for TestA, TestB, and FMA eval set are depicted in \Cref{tab:obj_eval}. Both Tango variants trained on MusicCaps are inferior to the other 4 models, \textbf{which depicts the efficacy of our augmentation strategy}. Pre-trained Tango fine-tuned on \dataset{} and \modelemoji{} pre-trained seem to perform very similarly in FD and KL, but \modelemoji{} pre-trained shows a big improvement in FAD, which suggests better-perceived quality and musicality as FAD is a human perception-inspired metric. Last but not least, the performance of \modelemoji{} trained from scratch is comparable in FD and KL to both pre-trained versions of \modelemoji{} and Tango trained on \dataset{}, which shows that training with our augmented dataset can be an alternative to large-scale audio pre-training for music generation.

The evaluation of controllability results are shown in \Cref{tab:control_eval}. On TestB, in Tempo metrics, all the models perform comparably. In Key metrics, we can observe that models trained on \dataset{} perform significantly better than the ones trained on MusicCaps. Additionally, \modelemoji{} outperforms all the other models on TestB and placed second on \datasetFMA{}. In Chord controllability, \modelemoji{} outperforms all the Tango models by a big margin. Finally, in Beat metrics, the models seem to perform similarly to each other, but \modelemoji{} shows the best result. On \datasetFMA{}, we further see that the Chord metrics are even better for \modelemoji{} with CMAOMM reaching 75.83. Overall, the results gathered from both TestB and modified \datasetFMA{} correlate in most aspects.



\subsubsection{Subjective Evaluation}

A total of 48 participants participated in the general listening test, of which 26 had more than 5 years of formal musical training. The results in \Cref{tab:subj_eval} show the average ratings for each of the metrics defined above. We can clearly see that the Tango baseline model is outperformed in all metrics by the models trained on \dataset{}. Interestingly, \modelemoji{} trained from scratch performs the best in terms of audio quality, rhythm presence, and harmony. The differences in ratings are minimal between the three top models, clearly confirming that our augmentation method is effective in furthering the output quality, and that \modelemoji{} is able to reach state-of-the-art quality. 


A total of 4 experts participated in the controllability listening study. The results of the expert listening study in \Cref{tab:expert_eval} further confirm that both \modelemoji{} models outperform  the Tango baselines in all metrics, especially in term of the chords of the generated music matching with the input text caption (Chord Match or MCM). This further supports the controllability results presented in \Cref{tab:control_eval} and shows that our proposed \modelemoji{} model can indeed understand music-specific text prompts. 


\begin{table}[h!]
    \centering
    \begin{adjustbox}{width=1\linewidth,center}
    \begin{tabular}{lccccccc}
    \toprule
      Model & Dataset & Pre-trained & REL & AQ & OM & RP & HC  \\
    \midrule
     \texttt{Tango} & \texttt{MusicCaps} & \greencheck & 4.09 & 3.68 & 3.55 & 3.91 & 3.80   \\
     \texttt{Tango} & \dataset{} & \greencheck & 4.96 & 4.26 & 4.40 & 4.49 & 4.61  \\
     \modelemoji{} & \dataset{} & \greencheck & 4.85 & 4.10 & 4.02 & 4.24 & 4.43  \\
     \modelemoji{} & \dataset{} & \redcross & 4.79 & 4.20 & 4.23 & 4.51 & 4.63  \\
    \bottomrule
    \end{tabular}
        \end{adjustbox}
    \caption{Average ratings for each metric in the general listening study. 
}
    \label{tab:subj_eval}
\end{table}


\begin{table}[h!]
    \centering
    \begin{adjustbox}{width=1\linewidth,center}
    \begin{tabular}{lccccccccc}
    \toprule
      Model & Dataset & Pre-trained & REL & MCM & MTM & AQ & OM & RP & HC  \\
    \midrule
     \texttt{Tango} & \texttt{MusicCaps} & \greencheck &  4.35 & 2.75 & 3.88 & 3.35 & 2.83 & 3.95 & 3.84  \\
     \texttt{Tango} & \dataset{} & \greencheck & 4.91 & 3.61 & 3.86 & 3.88 & 3.54 & 4.01 & 4.34  \\
     \modelemoji{} & \dataset{} & \greencheck & 5.49 & 5.76 & 4.98 & 4.30 & 4.28 & 4.65 & 5.18  \\
    \modelemoji{} & \dataset{} & \redcross & 5.75 & 6.06 & 5.11 & 4.80 & 4.80 & 4.75 & 5.59  \\
    \bottomrule
    \end{tabular}
        \end{adjustbox}
    \caption{Average ratings for each metric in the expert listening study. 
}
    \label{tab:expert_eval}
\end{table}

\subsection{Ablation Study}

The natural ablations would be to discard one control at a time in \modelemoji{}. However, due to resource constraints, we were unable to conduct these experiments.

On the other hand, we address the following two research questions through ablations:


\paragraph{Is Pre-training \modelemoji{} Necessary?.}
In certain experimental setups, we initialized \modelemoji{} with a \texttt{Tango} checkpoint pre-trained on \texttt{TangoPromptBank} and subsequently fine-tuned through instruction tuning on the \texttt{AudioCaps} dataset. These checkpoints encapsulate a broad understanding of general audio and sound, such as ``the sound of an elephant''. However, we observed that this general audio knowledge did not prove beneficial for music generation (See \Cref{tab:obj_eval,tab:subj_eval,tab:expert_eval,tab:control_eval}). Nevertheless, these checkpoints may find utility in composing music with diverse sounds, such as "African hip-hop music with the accompaniment of a lion's roar."


\paragraph{Is \texttt{MuNet} helpful?.}
The role of \texttt{MuNet} in \modelemoji{} is to control the music generation process.
As elaborated in \Cref{sec:objeval}, the inclusion of \texttt{MuNet} significantly enhances the performance of \modelemoji{} on both TestB and \datasetFMA{} under both objective and subjective evaluations, directly measuring how well the model adheres to control instructions in the provided prompt. Importantly, \texttt{MuNet} does not compromise the overall performance of general music generation in the absence of control sentences in the prompts. In fact, several objective metrics (such as FD, FAD, and KL-divergence), not explicitly focused on evaluating control instruction adherence, consistently show performance improvements when \texttt{MuNet} is incorporated.







\subsection{Discussions}














\subsubsection{Performance of the Predictors}
During the inference phase, we utilize pre-trained predictors for chord and beat predictions based on textual prompts. These predictors exhibit exceptional performance when the prompts explicitly contain chord and beat information, achieving accuracy of 94.5 \% on the TestB dataset. However, our interest extends to evaluating their performance in scenarios where control sentences are absent from the prompt—essentially, do these predictors generate noisy chords and beats? The concern is that such noise might propagate from the predictors to \modelemoji{}, significantly impacting the overall quality of the generated music.

In our experiments, TestA serves as a scenario where control sentences are not included in the textual prompts. Upon comparing the performance (\Cref{tab:obj_eval}) of \texttt{Tango} and \modelemoji{} on TestA, we observe that the latter outperforms the former across most metrics. This observation indicates that the control predictors do not compromise the performance of \modelemoji{} relative to \texttt{Tango}. The adaptability of these predictors to specific themes or styles in the absence of control sentences remains a potential avenue for future exploration, a topic we briefly touch upon below.

First, we investigate the effect of the Chord predictor on the generated output in a little comparison experiment. We take both TestA and TestB samples synthesized by \modelemoji{} and extract features from them. Then, we evaluate the chord control metrics of PCM, ECM, CMAO, and CMAOMM using chords predicted by chord predictor vs chords detected in the audio from feature extraction.
The metrics on TestA are PCM - 16.15, ECM - 33.95, CMAO - 39.81, and CMAOMM - 47.82.
The metrics on TestB are PCM - 17.75, ECM - 32.07, CMAO - 47.36, and CMAOMM - 66.80.
These results show that \modelemoji{} tends to follow the chords predicted by the chord predictor quite often. While the results on TestA are a bit lower than on TestB, they are still higher than Tango results on TestB as shown in \Cref{tab:control_eval}.


Second, we take a look at some specific examples:


\begin{mdframed}[backgroundcolor=blue!10] 
\textbf{Prompt:} ``This folk song features a female voice singing the main melody. This is accompanied by a tabla playing the percussion. A guitar strums chords. For most parts of the song, only one chord is played. At the last bar, a different chord is played. This song has minimal instruments. This song has a story-telling mood. This song can be played in a village scene in an Indian movie. \textit{The chord sequence is Bbm, Ab. The beat is 3. The tempo of this song is Allegro. The key of this song is Bb minor.}''

Without control sentences in italics (TestA):
\textbf{chords predicted}: ["G", "C", "G", "C", "G", "C"], \textbf{chords predicted time}: [0.46, 1.21, 3.25, 5.48, 7.24, 8.92].
\textbf{chords extracted from audio}: ["G6", "C", "G", "C", "G", "Cmaj7"], \textbf{chords time extracted from audio}: [0.46, 1.58, 3.07, 5.94, 7.62, 9.66]

With control sentences in italics (TestB):
\textbf{chords predicted}: ["Bbm", "Ab"], \textbf{chords predicted time}: [0.46, 7.24],
\textbf{chords extracted from audio}: ["F\#maj7", "Ab"], \textbf{chords time extracted from audio}: [0.46, 7.43].

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!10] 
\textbf{Prompt:} ``A female singer sings this bluesy melody. The song is medium tempo with minimal guitar accompaniment and no other instrumentation. The song's medium tempo is very emotional and passionate. The song is a modern pop hit but with poor audio quality. \textit{The key of this song is G minor. The time signature is 3/4. This song goes at 168.0 beats per minute. The chord progression in this song is Am7, G7, Cm, G, A7.}''

Without control sentences in italics (TestA):
\textbf{chords predicted}: ["C\#m7", "C\#m7", "C\#m7", "C\#m7", "C\#m7"], \textbf{chords predicted time}: [0.46, 3.25, 6.32, 8.17, 9.29],
\textbf{chords extracted from audio}: ["F\#", "C\#m", "F\#m", "C\#m7"], \textbf{chords time extracted from audio}: [0.46, 1.21, 4.55, 5.39]

With control sentences in italics (TestB):
\textbf{chords predicted}: ["Am7", "G7", "Cm", "G", "A7"], \textbf{chords predicted time}: [0.46, 1.67, 3.53, 5.48, 8.92],
\textbf{chords extracted from audio}: ["Am", "G", "C", "Gmaj7", "A6", "Gmaj7"], \textbf{chords time extracted from audio}: [0.46, 1.67, 3.72, 5.94, 8.73, 9.85]
\end{mdframed}




The two depicted samples give us some specific insights into the predicted chords and chords detected in the generated audio. Most of the time, \modelemoji{} follows the chords provided by the chord predictor in most cases. We can observe some substitutions in the actual chords detected from the audio compared to the predicted chords, e.g., G became G6, C became Cmaj7, and C\#m7 became C\#m. These chord substitutions are very close musically and could even be a consequence of the feature extraction system not being 100\% accurate. The substitution of Bbm for F\#maj7 is more of a change at first glance, but given that 2 out of 3 notes in Bbm are also contained in the 
4-note F\#maj \modelemoji{}, we see this substitution as understandable too. However, we note that this substitution would not be considered a valid one in any of our proposed chord control metrics.

Last but not least, in the absence of explicit control sentences in the prompt, we observe that the chords predicted by the chord predictor usually follow specific patterns. The generated samples follow a pattern of two chords that alternate (A, B, A, B, A, B). Another type of an observed pattern is one chord repeated (A, A, A, A, A, A). A more elaborate study on the Chord predictor behavior should be a topic for future work.






























\subsubsection{Insights from the Human Annotation}
Here, we take a look at some generated examples from the expert listening test, specifically a blues sample with the following prompt: \texttt{``An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute.''}

In \Cref{fig:sample_tango} we can see the mel-spectrogram generated by pre-trained \texttt{Tango} finetuned on \dataset{}. As is clear from the spectrogram and the waveform attached, the music appears a bit abruptly in contrast to the sample generated by \modelemoji{} depicted in \Cref{fig:sample_mustango} where the rhythm is very consistent. This seems to reflect the results of our expert listening study from \Cref{tab:expert_eval}. The predicted beat timestamps by our Beat predictor that condition the diffusion process are as follows:
\textbf{beats predicted}: [[0.26, 0.87, 1.52, 2.09, 2.76, 3.41, 4.0, 4.57, 5.1, 5.65, 6.22, 6.79, 7.36, 7.79, 8.3, 8.8, 9.3, 9.75], 3].
\textbf{These predicted beat timestamps show that there is a beat roughly every 0.6 seconds, which corresponds to 100 beats per minute tempo. This is the tempo ordered and properly predicted to condition the model.}

When it comes to chords, Tango would sometimes not follow the chords, make them sound unclear, or not give them enough time to sound through. On the other hand, \modelemoji{} seems to follow the predicted chords as well as their starting time. We take a look at the same blues example. The predicted chord condition from the Chord predictor is as follows:
\textbf{chords predicted}: ["G7", "F7", "C7", "G7"], \textbf{chords predicted time}: [0.46, 2.04, 4.37, 8.17].
We can see that the chord onset time is nicely spread in time. This is also clear from listening to the sample and seeing the spectrogram with perceived chord starts in \Cref{fig:sample_mustango}. To confirm this, we extracted the chord features from the generated audio to compare.
The chord feature extracted from the audio sample generated by \modelemoji{} is:
\textbf{chords}: ["G7", "F7", "C", "G7"], \textbf{chords time}: [0.46, 1.76, 4.74, 8.45]
Interestingly, the match of timing and chord sequence is very clear here. The substitution of the C7 chord for C can be a minor mistake either on the generation part or the feature extraction part. If we consider the chord metrics from \cref{sec:ctrl_eval}, this would yield a score of 100 for CMAOMM and a score of 75 for CMAO and ECM.
In contrast, the sample generated by pre-trained Tango finetuned on \dataset{} sounds more unstable and does not give enough time to chords to sound through.
The chord feature extracted from the audio sample generated by pre-trained Tango finetuned on \dataset{} is:
\textbf{chords}: ["Fm6", "G", "Dm", "G", "C", "Gm"], \textbf{chords time}: [0.46, 2.69, 3.53, 5.76, 6.69, 9.66]. We can see that there are 6 chords extracted from the audio sample instead of the ordered 4, and they do not match too well, as we see a minor type of F chord instead of a major; G also appears in a minor variant once; and there is an additional Dm chord too. This would yield a CMAOMM score of 75, but CMAO and ECM scores of 0. The perceived chord starts can be seen in \Cref{fig:sample_tango}.






\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Sections/Figure/tangoaug_sample0b.png}
    \caption{Mel-spectrogram of a blues sample generated by \texttt{Tango} trained on \dataset{}.}
    \label{fig:sample_tango}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Sections/Figure/mustango_sample0b.png}
    \caption{Mel-spectrogram of a blues sample generated by \modelemoji{} with vertical lines showing perceived chord starts.}
    \label{fig:sample_mustango}
\end{figure} \section{Conclusion}
\label{sec:conclusion}


In conclusion, this paper introduces a significant advancement in the field of text-to-music synthesis with the development of \modelemoji{}, a novel diffusion-based system inspired by music-domain knowledge. By extending the capabilities of the \texttt{Tango} text-to-audio model, \modelemoji{} not only achieves state-of-the-art music quality but also addresses a crucial gap in controllability within text-to-music systems. The integration of a variance adaptor and MuNet module, specifically designed for predicting musical features such as chords and beats, enhances the system's ability to respond to user-provided inputs, including chords, beats, tempo, and key music features.

The introduction of MuNet, a Music-Domain-Knowledge-Informed UNet sub-module, plays a pivotal role in seamlessly integrating both text and music features into the diffusion denoising process. To overcome the challenge of limited datasets, we developed \dataset{}, an open dataset of music with music-specific text captions, created by augmenting the \texttt{MusicCaps} dataset, providing a valuable resource for future research in this domain. 

Through a series of comprehensive experiments, \modelemoji{} demonstrates its prowess by achieving state-of-the-art music quality. Furthermore, the controllability offered by \modelemoji{} through music-specific text prompts surpasses other models, showcasing superior performance in capturing desired chords, beats, keys, and tempo across multiple datasets. This research not only pushes the boundaries of text-to-music synthesis but also sets a new standard for controllability in music generation systems. The \modelemoji{} model code, \dataset{} and \datasetFMA{} datasets, as well as a live demo of \modelemoji{} are available online\footnote{\url{https://github.com/amaai-lab/mustango}}. 

%
 










































































\bibliography{anthology,tango}

\appendix



\clearpage
\onecolumn
\FloatBarrier
\section{User Interface and Questions Used for Listening Studies}
\FloatBarrier
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{appendix/general_study_example.png}
    \caption{Question interface used for the general listening test. }
    \label{appfig:general_study}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{appendix/expert_study_example.png}
    \caption{Question interface used for the controllability listening test with music experts. }
    \label{appfig:expert_study}
\end{figure}



\clearpage
\FloatBarrier
\section{Control Sentence Templates to Enhance Prompts}
\label{sec:controlcaptions}

\begin{table}[h] \footnotesize
\caption{Rules used to create text sentences from input parameters detected from the data (key, chords, beats, tempo), and those used to augment the data (crescendo, etc.). }
\begin{tabular}{@{}lll@{}}
\toprule
Feature & Input   & Output sentences  \\
\midrule
Tempo & int  & \tabitem The bpm is . \\
& & \tabitem The tempo of this song is  beats per minute.\\
& & \tabitem This song goes at  beats per minute.\\
\midrule
Tempo & string  [`Grave', `Largo', `Adagio', & \tabitem This song is in .\\
& `Andante', `Moderato', `Allegro',  & \tabitem The tempo of this song is .\\
& `Vivace', `Presto', `Prestissimo']& \tabitem This song is played in .\\
& & \tabitem The song is played at the pace of .\\
\midrule
Beat count& int  & \tabitem The time signature is .\\
& & \tabitem The beat is .\\
& & \tabitem The beat counts to .\\
\midrule
Chords&  text list of chords  & \tabitem The chord sequence is . \\
& & \tabitem The chord progression in this song is .\\
\midrule
Key& string   & \tabitem The key is  \\
& string  [`major', `minor'] & \tabitem The key of this song is  .\\
& & \tabitem This song is in the key of  \\
\midrule
Volume change& float  indicating start/end time  & \tabitem There is a  from start until  seconds \\
& of crescendo/decrescendo, & \tabitem The song starts with a .\\
& string   ['crescendo', 'decrescendo'], & \tabitem  the volume progressively!\\
& and  ['increase', 'decrease'] & \tabitem There is a  from  seconds on.\\
& & \tabitem At seconds , the song starts to gradually  in volume.\\
& & \tabitem Midway through the song, a  starts.\\
\bottomrule
\end{tabular}

Note that tempo strings  were assigned based on music theory binning based on bpm: Grave {(0, 40]}, Largo {(40,60]}, \\ Adagio {(60, 70]}, Andante {(70,90]}, 
Moderato {(90,110]}, 
Allegro {(110,140]}, 
Presto {(140,160]}, 
Prestissimo {(160,inf)}. 
\end{table}





\clearpage
\FloatBarrier
\section{Custom Captions Used for Listening Studies}
\label{sec:captions}
\FloatBarrier


\begin{table*}[h]
\caption{Custom captions used for the general listening test. }
\footnotesize
\begin{tabular}{p{0.4cm} p{15cm}} 
\toprule
1& This piece is an instrumental reggae song that is very chill and slow. There is no singer. It is relaxing to hear the groove with the bass guitar. The song includes reggae electric guitar, horn, and percussion like bongos. The keyboard provides lush chords. The time signature is 4/4. The chord progression is G, F, C.\\ 
2& This instrumental blues song goes very slow at a bpm of 50. You can hear the bass, harmonica and guitar grooving. The harmonica plays a solo over the harmonious guitar and bass.  \\
3 & This classical piece is a waltz played by a string quartet. It includes two violins, a viola, and a cello, the beat counts to 3. It sounds elegant, and has a strong first beat. It has a natural and danceable rhythm. The mood is romantic. The chord progression is Em, Am, D, G. \\
4& African drums are playing a complex rhythm while a male vocalist chants a ritual. The atmosphere is mesmerizing. The complex drumming pattern is a mesmerizing blend of syncopation, polyrhythms, and intricate patterns. It takes place somewhere in the wilderness, or in an indigenous village. \\
5& This rock piece with guitars and drums is loud but fades out later on and becomes softer. It sounds powerful yet melancholic. It is instrumental only. A bass guitar provides a steady beat, enhancing the groove and energy of the song.    \\
6& A single bass instrument is playing a running baseline. It has a jazzy feeling to it and sounds mellow. This could be played in a jazz club. The tempo is 120 bpm.    \\
7& This is a hip hop song. It has two rappers taking turns, one female and one male. An electronic synth melody sample in the background keeps on looping. We can hear electronic beats and sometimes record-scratching sound effects.     \\
8& A smooth jazz song with saxophone, drums and guitar with a chord progression of Dm7, G7, Cmaj7. The song is relaxed and slow. There are no vocals, it is instrumental only. The saxophone produces a velvety tone that delivers an emotive melody. \\
9& A piano plays a soothing popular instrumental song that could serve as background music in a restaurant. There is only piano playing, no other instruments. There is a piano melody with background piano chords of Am, Fmaj7, Cmaj7, and G. The tempo is unhurried. The melody is gentle and soothing, evoking a sense of nostalgia and comfort.    \\
10& Indian folk music with a sitar and female vocals. It evokes a sense of zen and elevation. A sitar player begins with a gentle and melodic introduction, plucking the strings with precision and emotion. There are rhythmic beats of traditional hand percussion instruments, such as the tabla. It could be played at a cultural festival to showcase Indian culture.    \\
  \\ \bottomrule
\end{tabular}
\end{table*}



\begin{table*}[h]
\caption{Custom opposing captions created for the control experiment. }
\label{tab:controlprompts}
\footnotesize
\begin{tabular}{p{0.4cm} p{15cm}} 
\toprule
1 & An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute.       \\ 
2 & An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute.                   \\
3& A piano plays a popular melody over the chords of Am, Fmaj7, Cmaj7, G. There is only piano playing, no other instruments or voice. The tempo is Adagio.    \\
4& A piano plays a popular melody over the chords of Gm, Bb, Eb. There is only piano playing, no other instruments or voice. The tempo is Vivace.                                                                                                        \\
5& This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a pounding rhythm. A guitar solo melody emerges from the chaotic background of the chords. The chord progression is A, D, E. The tempo of the song is 160 bpm.           \\
6& This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a pounding rhythm. A guitar solo melody emerges from the chaotic background of the chords.The chord progression is C, B, A, G. The tempo of the song is 100 bpm.          \\
7& A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of Em7b5, A7, Dm7. The pianist produces delicate harmonies and subtle embellishments. The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 80 beats per minute.  \\
8& A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of B7, G7, E7, C7. The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 115 beats per minute.  \\
9& This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 120 bpm. The chords played by the synth are Am, Cm, Dm, Gm.    \\
10& This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 160 bpm. The chords played by the synth are C, F, G.          \\
11& A horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Adagio. An electric keyboard plays the chords Am, Dm, G, C.                    \\
12& A horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Moderato. An electric keyboard plays the chords E, B, A.                                                \\
13& This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of Em, C, G, D. The tempo is 120 bpm.   \\
14& This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of A, F\#m, D, E. The tempo is 170 bpm.  \\
15& A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is G, C, D, G. The tempo is 100 beats per minute.                                          \\
16& A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is Am, Em, Dm, Am. The tempo is 70 beats per minute.                                    \\
17& This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello's soulful and melodic contributions add depth and gravitas to the performance. The time signature is ¾. The tempo of this song is Presto. The chord sequence is E, C\#m, A, B.           \\
18& This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello's soulful and melodic contributions add depth and gravitas to the performance. The time signature is 4/4. The tempo of this song is Andante. The chord sequence is Am, Dm, E7, Am.   \\
19& This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song's electronic foundation, creating a rich and layered sonic landscape. The charismatic female singer has a dynamic and emotive voice. The tempo is Moderato. The chord sequence is                                                                                                               C, G, Am, F. \\
20& This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song's electronic foundation, creating a rich and layered sonic landscape. The charismatic female singer has a dynamic and emotive voice. The tempo is Presto. The key is A minor and the chord sequences are Am, Dm, E.   \\ \bottomrule
\end{tabular}
\end{table*}








\clearpage
\FloatBarrier
\section{Additional Examples of Generated Music. }
\label{app:examples}
\FloatBarrier
Here we show additional samples generated from pre-trained \texttt{Tango} fine-tuned on MusicCaps, \texttt{Tango} finetuned on \dataset{}, and \modelemoji{}, all generated from the same prompts.\\

\textbf{Prompt:} A horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Adagio. An electric keyboard plays the chords Am, Dm, G, and C.




\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Sections/Figure/tmchf_sample10b.png}
    \caption{Mel-spectrogram of a reggae sample generated by pre-trained \texttt{Tango} fine-tuned on MusicCaps with vertical lines showing perceived chord starts. The blue box shows an area of dissonance in the music. Overall, the audio is a bit noisy.}
    \label{fig:sample_tango1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Sections/Figure/tangoaug_sample10b.png}
    \caption{Mel-spectrogram of a reggae sample generated by pre-trained \texttt{Tango} fine-tuned on \dataset{} with vertical lines showing perceived chord starts. There are too many chords here.}
    \label{fig:sample_tangoaug1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Sections/Figure/mustango_sample10b.png}
    \caption{Mel-spectrogram of a reggae sample generated by \model{} with vertical lines showing perceived chord starts. The chords match the prompt. }
    \label{fig:sample_mustango1}
\end{figure}


\clearpage
\FloatBarrier

\textbf{Prompt:} This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of Em, C, G, D. The tempo is 120 bpm.




\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Sections/Figure/tmchf_sample12.png}
    \caption{Mel-spectrogram of a metal song sample generated by pre-trained \texttt{Tango} fine-tuned on MusicCaps. It is very noisy from the very start.}
    \label{fig:sample_tango2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Sections/Figure/tangoaug_sample12.png}
    \caption{Mel-spectrogram of a metal song sample generated by pre-trained \texttt{Tango} fine-tuned on \dataset{}. The song starts with 4 beats from the drummer, but there is a bit of noise from the start.}
    \label{fig:sample_tangoaug2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Sections/Figure/mustango_pretrained_sample12.png}
    \caption{Mel-spectrogram of a metal sample generated by \model{}. The song starts with 4 distinguishable beats from the drummer, then the guitars join.}
    \label{fig:sample_mustango2}
\end{figure}



\end{document}
