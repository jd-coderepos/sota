
\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{newunicodechar}

\usepackage{subcaption}
\usepackage{stfloats} \usepackage{lipsum}
\title{AnoSeg: Anomaly Segmentation Network Using Self-Supervised Learning}





\author{Jou Won Song{}\thanks{*equal contribution} , Kyeongbo Kong{}, Ye-In Park{}, Seong-Gyun Kim{}, Suk-Ju Kang{} \\
{}Department of Electronic Engineering, Sogang University, Seoul, Korea\\
{}Department of Media communication, Pukyong National University, Busan, Korea\\
{}LG Display, Seoul, South Korea\\
\texttt{\{wn5649,yipark,sjkang\}@sogang.ac.kr}{} \\
\texttt{\{kbkong\}@pknu.ac.kr}{} \\
\texttt{\{ksglcd\}@lgdisplay.com}{} \\


}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
Anomaly segmentation, which localizes defective areas, is an important component in large-scale industrial manufacturing. However, most recent researches have focused on anomaly detection. This paper proposes a novel anomaly segmentation network (AnoSeg) that can directly generate an accurate anomaly map using self-supervised learning. For highly accurate anomaly segmentation, the proposed AnoSeg considers three novel techniques: Anomaly data generation based on hard augmentation, self-supervised learning with pixel-wise and adversarial losses, and coordinate channel concatenation. First, to generate synthetic anomaly images and reference masks for normal data, the proposed method uses hard augmentation to change the normal sample distribution. Then, the proposed AnoSeg is trained in a self-supervised learning manner from the synthetic anomaly data and normal data. Finally, the coordinate channel, which represents the pixel location information, is concatenated to an input of AnoSeg to consider the positional relationship of each pixel in the image. The estimated anomaly map can also be utilized to improve the performance of anomaly detection. Our experiments show that the proposed method outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for the MVTec AD dataset. In addition, we compared the proposed method with the existing methods through the intersection over union (IoU) metric commonly used in segmentation tasks and demonstrated the superiority of our method for anomaly segmentation.
\end{abstract}

\section{Introduction}



Anomaly segmentation is the process that localizes anomaly regions. In the real world, since the number of anomaly data is very limited, conventional anomaly segmentation methods are trained using only normal data. Typically, many anomaly segmentation methods are based on anomaly detection techniques because the real dataset includes few anomaly images without the ground truth (GT) mask. Therefore, these methods are not trained directly on pixel-level segmentation and they are difficult to generate anomaly maps similar to GT masks.

Specifically, existing reconstruction-based methods using autoencoder (AE) (\cite{re8,re9,re12,re10, mvtec}) and generative adversarial network (GAN) (\cite{re7,re11,anog,re14}) are trained to learn reconstruction of normal images and determine anomaly if the test sample has the high reconstruction error for an abnormal region. However, reconstruction-based methods often restore even non-complex anomaly regions, which degrade the performance on both anomaly detection and segmentation. Therefore, the anomaly map in Fig. \ref{fig1}(b) greatly differs from the corresponding GT mask. Alternative methods have been recently studied by using the high-level learned representation for anomaly detection and segmentation. These methods use a pretrained model to extract a holistic representation of a given image and compare it to the representation of a normal image. Also, several existing methods use patches, splitting a given image to perform anomaly segmentation. By extracting representations from an image patch, these methods compute the scores of the image patches and combine them to generate the final anomaly map. Therefore, the quality of anomaly maps is highly correlated with the patch size. The uninformed students (US) (\cite{stu}) in Figs. \ref{fig1}(c) and (d) are trained using a small patch size (17 x 17) and a large patch size (65 x 65), respectively. Therefore, as shown in Fig. \ref{fig1}(d), US\textsubscript{65 x 65} is difficult to detect small anomaly regions. Patch SVDD (\cite{patch}) and SPADE (\cite{spa}) use feature maps of multiple scales to detect anomaly regions with various sizes. However, as shown in Figs. \ref{fig1}(e) and (f), these methods approximately detect anomaly regions. In addition, in GradCAM-based methods, GradCAM (\cite{grad}) is used to generate anomaly maps to detect regions that influence the decision of the trained model (\cite {att,eatt}). CutPaste (\cite{cut}) introduces a self-supervised framework using a simple effective augmentation that encourages the model to find local irregularities. CutPaste also performs anomaly localization through GradCAM by extending the model to use patch images after training the classifier. However, these methods are not aimed at anomaly segmentation and detect anomaly regions using a modified anomaly detection method. Generally, to improve the segmentation performance, a methodology that can be learned pixel-wise should be considered. Therefore, existing methods cannot clearly detect anomalies because it is difficult that directly use the pixel-wise loss such as a mean squared error typically used in the segmentation task.

To handle this problem, this paper proposes a new methodology that can directly learn the segmentation task. The proposed anomaly segmentation network (AnoSeg) can generate an anomaly map to segment the anomaly region that is unrelated to the normal class. The goal of AnoSeg is to generate an anomaly map that represents the normal class region within a given image for anomaly segmentation, unlike the existing methods to indirectly extract anomaly maps. For this goal, our AnoSeg proposes three following approaches. First, as shown in Fig. 2, AnoSeg uses the segmentation loss directly using the synthesized data generated through hard augmentation, which generates data shifted away from the input data distribution. Second, AnoSeg learns to generate the anomaly map and reconstruct normal images. 

Also, an adversarial loss is applied by using a generated anomaly map and an input image. Unlike the existing GAN, the discriminator of AnoSeg determines whether the image is a normal class and whether the anomaly map is focused on the normal region. Since the anomaly map learns the normal sample distribution, AnoSeg has high generalization for unseen normal and anomaly regions even with a small number of normal samples. 

Third, we propose the coordinate channel concatenation using a coordinate vector based on coordconv (\cite{coord}). Anomaly regions in a particular category often depend on the location information of a given image. Therefore, the proposed coordinate vector helps to understand the positional relationship of normal and anomaly regions in the input image. As a result, Fig. \ref{fig1}(h) shows that the anomaly map of AnoSeg is very similar to GT even without thresholding. Moreover, we describe how to perform the anomaly detection using the generated anomaly map. By simply extending the model using an anomaly map to the existing GAN-based method (\cite{alocc}), we could achieve 96.4 area under ROC curve (AUROC) for image-level localization, which is a significant improvement over conventional state-of-the-art (SOTA) methods. As a result, the proposed method achieves SOTA performance on the MVTec Anomaly Detection (MVTec AD) dataset for anomaly detection and segmentation compared to conventional methods without using a pretrained model. The main contributions of this study are summarized as follows:

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{11.png} 
\end{center}
\vspace{-0.3cm}
   \caption{Comparison of anomaly maps (before thresholding) of the proposed method with the SOTA methods in the MVTec-AD dataset. Except for the proposed method, anomaly maps of existing methods are normalized to [0, 1].}
\label{fig1}
\vspace{-0.4cm}
\end{figure*}

\begin{itemize}
\item We propose a novel anomaly segmentation network (AnoSeg) to directly generate an anomaly map. AnoSeg generates detailed anomaly maps using the holistic approaches to maximize segmentation performance.
\item The proposed anomaly map can also be used in existing anomaly detection methods to improve the anomaly detection performance. 
\item In anomaly segmentation and detection, AnoSeg outperforms SOTA methods on the MVTec AD dataset in terms of intersection over union (IoU) and AUROC. Additional experiments using IoU metric also show that AnoSeg is robust for thresholding.
\end{itemize}




\section{Related Works}
Anomaly detection is a research topic that has received considerable attention. Anomaly detection and segmentation are usually performed via unsupervised methods using the generative model for learning the distribution of a certain class. In these methods, GAN (\cite{gan}) or VAE (\cite{vae}) learned the distribution of a certain class and used the difference between a reconstructed image and an input for anomaly detection (\cite{re8,re10, re12,alocc}). In addition, initial deep learning-based anomaly segmentation methods focused on generative models such as GAN (\cite{anog}) and AE (\cite{mvtec}). However, these approaches could have high reconstruction performance for simple anomaly regions. Recently, methods using a representation of an image patch have shown great effectiveness in anomaly detection (\cite{patch, spa}). In \cite{stu}, US was trained to mimic a pretrained teacher by dividing an image into patches. In recent studies (\cite{cut}), an activation map that visualizes the region of interest through GradCAM (\cite{grad}) was applied to anomaly detection. \cite{att} generated an activation map using GradCAM to focus only on the reconstruction loss of the ROI. \cite{eatt} improved the detection performance using an activation map in the training process. \cite{fcdd} apply one-class classification on features extracted from a fully convolutional network and use receptive field upsampling with Gaussian smoothing to extract anomaly map. However, in these existing methods, it is difficult to apply the loss related to anomaly segmentation because the model does not directly generate an anomaly map by using the modified anomaly detection method. Our method is different from the conventional methods which use GradCAM to indirectly extract the activation map. Instead, the proposed method directly extracts and supervises the anomaly map. Therefore, the proposed method discriminates between anomaly and normal regions more accurately compared to previous methods.


\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{22.png} 
\end{center}
   \caption{Overview of the training process of the proposed AnoSeg. AnoSeg generates reconstructed images and anomaly maps. To directly generate anomaly maps, AnoSeg applies three novel techniques: hard augmentation, adversarial learning, and coordinate channel concatenation.}
\label{fig2}
\vspace{-0.4cm}
\end{figure}



\section{Proposed Method: AnoSeg}
The proposed AnoSeg is a ``holistic'' approach which incorporates three techniques: self-supervised learning using hard augmentation, adversarial learning, and coordinate channel concatenation. The details are explained in the following sub-sections.



\subsection{Self-supervised Learning Using Hard Augmentation}
To train anomaly segmentation directly, an image with an anomaly region and its corresponding GT mask corresponding to the image are required. However, it is difficult to obtain these images and GT masks in the real case. Therefore, the proposed method uses hard augmentation (\cite{csi}) and Cutpaste (\cite{cut}) to generate synthetic anomaly data and GT masks. Hard augmentation refers to generating samples shifted away from the original sample distribution. As confirmed in \cite{csi}, the hard augmented samples can be used as a negative samples. Therefore, as shown in Fig. 3, we use three types of hard augmentation: rotation, perm, and color jitter. Each augmentation is applied with a 50\% chance. Then, like Cutpaste (\cite{cut}), the augmented data is pasted into a random region of normal data to generate the synthetic anomaly data and corresponding masks for segmentation. Finally, the anomaly segmentation dataset is composed as follows:

where  is a set of normal and synthetic anomaly images, in which  and  are normal images and synthetic anomaly images, respectively.  is a set of normal and synthetic anomaly masks, in which  and  are normal masks with all inner values set to one and synthetic anomaly masks, respectively.

Using the anomaly segmentation dataset with a pixel-level loss, we can directly train our AnoSeg. The anomaly segmentation loss  is as follows:

where  indicates the generated anomaly map (normal and anomaly classes). The generated anomaly map has the same size as an input image and outputs a value in the range of [0, 1] for each pixel depending on the importance of the pixel of the input image. However, since the synthetic anomaly data are only subset of various anomaly data, it is difficult to generate a real anomaly maps that are unseen in training phase.





\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{33.png} 

\end{center}
\vspace{-0.2cm}
\caption{Our synthetic anomaly data augmentation. The synthetic anomaly data is generated by several hard augmentations and Cutpaste (\cite{cut}). Synthetic anomaly data is generated by applying a rotation, perm, color jitter, and Cutpaste for each step. Hard augmentations are applied with a 50\% chance.}
\vspace{-0.2cm}
\label{fig3}
\end{figure}
\subsection{Adversarial Learning with Reconstruction}
To improve the generality for various anomaly data, it is important to train normal region distribution accurately. Therefore, AnoSeg utilizes masked reconstruction loss that uses reconstruction loss only in normal regions to learn only the distribution of normal regions and avoid bias of the distribution of synthetic anomaly regions. Also, since the discriminator inputs a pair for an input image and its GT masks, the discriminator and generator can focus on normal region distribution. Thus, anomaly region cannot be reconstructed well and the detail of the anomaly map can also be improved. Loss functions for adversarial learning are as follows:


where , , and  are a discriminator, a generator, and a concatenation operation, respectively. In Section 5, we demonstrated the effectiveness of adversarial loss.

















\begin{wrapfigure}{H}{0.5\textwidth}
\hspace{-10pt}
 \begin{center}
 \vspace{-12pt}
 \centerline{\includegraphics[width=0.5\columnwidth]{44.png}}
 \end{center}
 \vspace{-20pt}
 \caption{Overall process of the coordinate channel concatenation.}
\label{fig4}
 \vspace{-10pt}
\end{wrapfigure}



\subsection{Coordinate Channel Concatenation}
In the typical segmentation task, the location information is the most important information because normal and anomaly regions can be changed depending on where they are located. To provide additional location information, we use a coordinate vector inspired by CoordConv (\cite{coord}). We first generate rank 1 matrices that are normalized to [-1, 1]. Then, we concatenate these matrices with the input image as channels (Fig. \ref{fig4}). As a result, AnoSeg extracts features by considering the positional relationship of the input image. In ablation study, we demonstrated the effectiveness of coordinate channel concatenation.


\begin{wrapfigure}{H}{0.5\textwidth}
\hspace{-10pt}
 \begin{center}
 \vspace{-20pt}
 \centerline{\includegraphics[width=0.5\columnwidth]{55.png}}
 \end{center}
 \vspace{-20pt}
 \caption{An overview of the proposed anomaly detection method. To obtain anomaly score, the pair of images reconstructed from the anomaly map and the anomaly detector (fake pair) are compared with the pair of the normal mask and the input image (real pair) using a discriminator.}
\label{fig5}
 \vspace{-10pt}
\end{wrapfigure}

\subsection{Anomaly Detection Using Proposed Anomaly Map}
In this section, we design a simple anomaly detector that adds the proposed anomaly map to the existing GAN-based detection method (\cite{alocc}). The proposed anomaly detector performs anomaly detection by learning only normal data distribution. We simply concatenate the input image and anomaly map to use them as inputs of detector, and apply both an adversarial loss and a reconstruction loss. Then, we use the feature matching loss introduced in (\cite{imp}) to stabilize the learning of the discriminator and extract the anomaly score. We include a detailed description of the training process for anomaly detection in Appendix A. 

In the test process (Fig. \ref{fig5}), the proposed anomaly detector obtains anomaly scores using the discriminator that has learned the normal data distribution. We first assume that the input image is normal, so the mask  with all inner values set to one is used in pairs with the input image. When the input image is really normal, a fake pair (anomaly map and reconstructed image) is similar to the real pair (normal mask and input image), so the anomaly detector has a low anomaly score. On the other hand, when the input image is abnormal, the fake pair is significantly different to the real pair, so it has a high anomaly score. To compare the real and fake pair, the reconstruction loss and the feature matching loss are used as follows:

where  and  are 1 and 0.1, respectively.  and  represent a normal GT mask and the mean squared error, respectively.


\begin{table*}
\begin{center}
\label{table:headings}
\caption{Performance comparison of anomaly segmentation and detection in terms of pixel-level AUROC and image-level AUROC with the proposed method and conventional SOTA methods on the MVTec AD dataset (\cite{mvtec}). Full results for anomaly detection are added in Table 4 of Appendix A.3.}
\makeatletter
\def\hlinewd#1{
\noalign{\ifnum0=‘}\fi\hrule \@height #1 \futurelet
\reserved@a\@xhline}

\newcommand{\hthickline}{\hlinewd{1pt}}
\newcommand{\hthinline}{\hlinewd{.2pt}}
\makeatother
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\begin{tabularx}{\linewidth}{c||Z|Z|Z|Z|Z|Z|Z|Z}
\hthickline
  &\multicolumn{8}{c}{Anomaly Segmentation (Pixel-level AUROC)}\\\hline
\multirow{2}{*}{Method} &\multirow{2}{*}{AE} &\text{\!\multirow{2}{*}{CAVGA}} &\multirow{2}{*}{US} &\multirow{2}{*}{FCDD} &Patch SVDD &\multirow{2}{*}{SPADE}  &\text{\!\!\multirow{2}{*}{Cutpaste} } &\text{\multirow{2}{*}{\!\!Proposed}}\\
\hline\noalign{\smallskip}
\hline
Bottle     & 0.86 & 0.89 & 0.94   & 0.97 & 0.98 & 0.98 & 0.98 & \textbf{0.99} \\\hline
Cable      & 0.86 & 0.85 &  0.91  & 0.90 & 0.97 & 0.97 & 0.90 & \textbf{0.99} \\\hline
Capsule    & 0.88 & 0.95 &  0.92  & 0.93 & 0.96 & \textbf{0.99} & 0.97 & 0.90 \\\hline
Carpet     & 0.59 & 0.88 &  0.72  & 0.96 & 0.93 & 0.98 & 0.98 & \textbf{0.99} \\\hline
Grid       & 0.90 & 0.95 &  0.85  & 0.91 & 0.96 & 0.94 & 0.98 & \textbf{0.99} \\\hline
Hazelnut   & 0.95 & 0.96 &  0.95  & 0.95 & 0.98 & \textbf{0.99} & 0.97 & \textbf{0.99} \\\hline
Leather    & 0.75 & 0.94 &   0.84 & 0.98 & 0.97 & 0.98 & \textbf{0.99} & 0.98 \\\hline
Metal\_nut  & 0.86 & 0.85 &   0.92 & 0.94 & 0.98 & 0.98 & 0.93 & \textbf{0.99} \\\hline
Pill       & 0.85 & 0.94 &   0.91 & 0.81 & 0.95 & \textbf{0.96} & \textbf{0.96} & 0.94 \\\hline
Screw      & 0.96 & 0.85 &  0.92  & 0.86 & 0.96 & \textbf{0.99} & 0.97 & 0.91 \\\hline
Tile       & 0.51 & 0.80 &  0.91  & 0.91 & 0.91 & 0.87 & 0.90 & \textbf{0.98} \\\hline
Toothbrush & 0.93 & 0.91 &  0.88  & 0.94 & \textbf{0.98} & \textbf{0.98} & \textbf{0.98} & 0.96 \\\hline
Transistor & 0.86 & 0.85 &  0.73  & 0.88 & \textbf{0.97} & 0.94 & 0.93 & 0.96 \\\hline
Wood       & 0.73 & 0.86 &  0.85  & 0.88 & 0.91 & 0.89 & 0.96 & \textbf{0.98} \\\hline
Zipper     & 0.77 & 0.94 & 0.91   & 0.92 & 0.95 & 0.97 & \textbf{0.99} & 0.98 \\\hline\hline
Mean       & 0.82 & 0.89 &  0.88  & 0.92 & 0.96 & 0.96 & 0.96 & \textbf{0.97}\\\hline
  &\multicolumn{8}{c}{Anomaly Detection (Image-level AUROC)}\\\hline
Mean &0.71 &0.82 &0.84 &- &0.92 &0.86 &0.95 &\textbf{0.96} \\\hline
\hthickline 
\end{tabularx}
\end{center}
\vspace{-0.3cm}
\end{table*}

\section{Experimental Results}
\subsection{Evaluation Datasets and Metrics}
To verify the anomaly segmentation and detection performance of the proposed method, several evaluations were performed on the MVTec AD dataset (\cite{mvtec}). For the MVTec AD dataset, we resized both training and testing images to the size of 256 × 256, and each training batch contains 16 images. Following the previous works (\cite{mvtec,eatt, super}), we adopted the pixel-level and image-level AUROCs to quantitatively evaluate the performance of different methods for anomaly segmentation and detection, respectively. In addition, we used IoU to evaluate anomaly segmentation. For the measurement of IoU, a threshold, which maximizes IoU, was applied in each method.
 
\subsection{Implementation Details}
The encoder of AnoSeg consists of the convolution layers of ResNet-18 (\cite{res}). The up-sampling layer of decoders consists of one transposed convolution layer and convolution layers. Two decoders of the AnoSeg are composed of five up-sampling layers and two convolution layer to generate an anomaly map and a reconstructed image. The structure of the anomaly detector is the same as the AnoSeg structure except for the decoder that generates the anomaly map. Detailed information on training process and the network architecture is described in Appendix B.









\subsection{Experiments on the MVTec AD Dataset}

\subsubsection{Compared Methods}
We compared the reconstruction-based method with the proposed method using autoencoder-L2 (). GradCAM-based methods (CAVGA (\cite{eatt}) and Cutpaste (\cite{cut})) were also compared with the proposed method. Also, we compared the proposed method with the US \cite{stu} using the representation of patch images. In our experiment, we compared the US trained with a patch size of . The proposed method is also compared with FCDD (\cite{fcdd}) using receptive field upsampling. Finally, among the embedding similarity-based methods, the patch SVDD (\cite{patch}) and SPADE (\cite{spa}) were also used for the performance comparison.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{99.png} 
\end{center}
\vspace{-0.3cm}
  \caption{(a) Comparison on AUROC and IoU using Anomaly map and (b) mean IoU change according to the threshold for each category. The x-axis and y-axis represent a threshold and IoU, respectively.}
  \vspace{-0.3cm}
\label{fig6}

\end{figure}



\begin{table*}
\begin{center}
\label{table:headings}
\caption{Performance comparison of anomaly segmentation in term of mean IoU with the proposed and conventional SOTA methods on the MVTec AD dataset.}
\makeatletter
\def\hlinewd#1{\noalign{\ifnum0=‘}\fi\hrule \@height #1 \futurelet
\reserved@a\@xhline}
\newcommand{\hthickline}{\hlinewd{1pt}}
\newcommand{\hthinline}{\hlinewd{.2pt}}
\makeatother
\newcolumntype{Z}{>{\centering\arraybackslash}X}
{\footnotesize
\begin{tabularx}{\linewidth}{c||Z|Z|Z|Z|Z|Z}
\hthickline
  &\multicolumn{5}{c}{Anomaly Segmentation (IoU)}\\\hline
Method &CAVGA &US &Patch SVDD &SPADE &Proposed \\
\hline Mean &0.470 &0.244 &0.427 &0.483 &\textbf{0.542} \\\hline
\hthickline 
\end{tabularx}
}
\vspace{-0.3cm}
\end{center}
\end{table*}

\subsubsection{Quantitative Results}
We evaluated the anomaly segmentation performance between the proposed method and the existing SOTA methods mentioned in section 4.3.1 using the MVTec AD dataset. As shown in Table 1, the proposed method consistently outperformed all other existing methods evaluated in AUROC. The reconstruction-based methods such as  used the reconstruction loss as the anomaly score.  had lower performance (0.82 AUROC) compared to the proposed method. CAVGA (\cite{eatt}) and Cutpaste (\cite{cut}) obtained anomaly maps using GradCAM (\cite{grad}), but these anomaly maps highly depend on the classification loss. In addition, compared to the methods using patch image representation such as US, the proposed method achieved higher performance. As a result, AnoSeg outperformed the conventional SOTA, such as Patch SVDD, SPADE, and Cutpaste, by  AUROC in anomaly segmentation.

In addition, we evaluated IoU, which is typically used as a metric for segmentation. Table 2 shows the quantitative comparison on IoU. AnoSeg achieved the highest performance compared to other methods in IoU. In particular, Patch SVDD and SPADE achieved 0.96 AUROC similar to AnoSeg in the evaluation of AUROC, but had lower IoU than the proposed method. This is because, unlike the existing method, the proposed method was directly trained for segmentation.

Additionally, we compared the AUROC and IoU metrics for the generated anomaly map in Fig. \ref{fig6}(a). In general, AUROC is affected by the detection performance of the anomaly regions. False positives for normal regions have relatively no impact on AUROC. In the Patch SVDD of Fig. \ref{fig6}(a), there were abnormal regions that cannot be detected. Therefore, the anomaly map of Patch SVDD had lower AUROC compared to other methods. Although the anomaly maps of AnoSeg and SPADE visually show different anomaly maps, the same AUROC was calculated because most anomaly regions are detected in anomaly maps of AnoSeg and SPADE. However, IoU was affected by false positives in normal regions. Therefore, IoU of SPADE had lower performance compared to AUROC. The proposed AnoSeg achieved the highest performance for both IoU and AUROC. These results shows that the proposed method is superior in various aspects of anomaly segmentation. 

We compared the anomaly detection performance between the proposed and existing methods using the method introduced in section 4.3.1. As shown in Table 1, the proposed method achieved similar AUROC to existing SOTA methods (Full results are in Appendix A.3). Discriminator of anomaly detector learned representations of images and anomaly maps together. Therefore, with a simple anomaly detection method using the generated anomaly map, we achieve anomaly detection performance similar to that of the existing SOTA.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{66.png} 
\end{center}
   \caption{Qualitative results on the MVTec AD dataset for (first row) input image, (second row) GT mask, and (third row) proposed anomaly map.}
\label{fig7}
\end{figure}

\begin{table*}
\begin{center}
\label{table:headings}
\caption{Performance of various configurations on the MVTec AD dataset.}
\makeatletter
\def\hlinewd#1{\noalign{\ifnum0=‘}\fi\hrule \@height #1 \futurelet
\reserved@a\@xhline}
\newcommand{\hthickline}{\hlinewd{1pt}}
\newcommand{\hthinline}{\hlinewd{.2pt}}
\makeatother
\newcolumntype{Z}{>{\centering\arraybackslash}X}
{\footnotesize
\begin{tabularx}{\linewidth}{c||Z|Z|Z|Z}
\hthickline
  &\multicolumn{4}{c}{Ablation study (AUROC / IoU)}\\\hline
Method &Base model (Cutpaste only) & + Hard augmentation & + Adversarial learning & + Coordinate channel \\
\hline Mean &0.923 / 0.492 &0.942 / 0.503 &0.951 / 0.527 &0.970 / 0.542\\\hline
\hthickline 
\end{tabularx}
}
\vspace{-0.3cm}
\end{center}
\end{table*}

\subsubsection{Qualitative Results}

For the evaluation with existing methods, we visualized anomaly maps of existing and proposed methods in Fig. \ref{fig1}. The output image of  (\cite{mvtec}) was restored up to the anomaly image region and it was difficult to restore high-frequency regions of the normal image. Also,  could detect large defects, but had poor detection performance for small defects. These results show that patch representations based methods are difficult to accurately localize defects for various sizes. Patch SVDD and SPADE extracted anomaly maps using feature extractions for different sizes to consider defects with various sizes. Therefore, the defects with different sizes could be detected, as shown in Fig. \ref{fig1}. However, these anomaly maps had many false positives for normal regions and approximately detected anomaly regions. In contrast, as shown in Fig. \ref{fig7}, the proposed AnoSeg was trained to generate anomaly maps directly for anomaly segmentation using the segmentation loss. Therefore, the proposed method generated an anomaly map more similar to GT than the results of the existing methods as shown in Fig. 6. More comprehensive results on defect segmentation are given in Appendix C.

\subsubsection{Analysis of Threshold Sensitivity}
In this section, Patch SVDD and our AnoSeg were compared to verify the performance variation depending on the threshold of the proposed method. IoU was measured by dividing the anomaly score by 10000 units. Fig. \ref{fig6}(b) shows the performance change of AnoSeg, SPADE and Patch SVDD according to a threshold. As shown in Fig. \ref{fig6}(b), the performance of AnoSeg did not significantly change significantly for different thresholds. Therefore, the anomaly map is shown similar to the GT mask even though thresholding was not applied in Fig. \ref{fig6}. On the other hand, Fig. \ref{fig6}(b) shows that Patch SVDD and SPADE had a significant change in performance when the threshold is changed around the threshold with the highest IoU. The result shows that our model is robust against thresholding. By setting the threshold between 0.2 and 0.8, AnoSeg could always achieve better results consistently than other SOTA solutions listed in Table 2.


\section{Ablation Study} 
We modified the generator structure (Section 4.2) to generate the only anomaly map and construct the base model with only Cutpaste applied. Then, we added modules incrementally on the base model, and evaluated with IoU and AUROC scores. The overall results show that the method using all modules improved by 5.4\% and 10.2\% for AUROC and IoU, respectively, compared to the base model. The effectiveness of each module is described below.\\


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{88.png} 

\end{center}
\vspace{-0.2cm}
\caption{Qualitative results of the ablation study to illustrate the performance of the anomaly segmentation on the MVtec AD dataset.}
\vspace{-0.2cm}
\label{fig3}
\end{figure}

\textbf{Hard augmentation} \quad We used images with several hard augmentations applied to train AnoSeg on anomaly regions. Hard augmentations generate samples away from the normal data distribution. Intuitively, synthetic anomaly data applied with hard augmentation can generate more diverse anomaly regions than Cutpaste. Therefore, AnoSeg detected more anomaly regions than the base model. As a result, AUROC and IOU were improved by 2.1\% and 1.9\% respectively.

\textbf{Adversarial learning with reconstruction loss} \quad The proposed AnoSeg learns the normal region distribution through adversarial learning. We also use masked reconstruction loss in AnoSeg to apply reconstruction loss only for normal regions to avoid biasing synthetic anomaly regions. As shown in a of Fig. 8(a), the base model is difficult to learn the normal data distribution. Therefore, the reconstructed image of base model partially restores the anomaly regions, and the base model detects anomaly regions as normal regions. In contrast, a model using adversarial learning learns the normal data distribution and can segment between normal and abnormal regions. Therefore, AnoSeg can generate detailed anomaly maps.

\textbf{Coordinate channel concatenation} \quad To consider the additional location information while performing anomaly segmentation, we concatenated coordinate channels. In Fig. 8(b), the effectiveness of coordinate channel concatenation is confirmed. The yellow cable in the input image changes the class property depending on the location. Therefore, these anomaly regions can be determine as normal if location information is insufficient. Because the base model that does not use the coordinate channel lacks location information, the yellow cable, which is an abnormal area, is reconstructed and determined as a normal area. AnoSeg provides additional location information by connecting the coordinate channel to the input image. As a result, as shown in Fig.8(b) anomaly regions that depend on location information were additionally detected, and AUROC and IOU were improved by 1.9\% and 2.8\% respectively.

\section{Conclusion}
This paper presented a novel anomaly segmentation network to directly generate an anomaly map. We proposed AnoSeg, a segmentation model using adversarial learning, and the proposed AnoSeg was directly trained for anomaly segmentation using synthetic anomaly data generated through hard augmentation. In addition, anomaly regions sensitive to positional relationships were more easily detected through coordinate vectors representing the pixel position information. Hence, our approach enabled AnoSeg to be trained to generate anomaly maps with direct supervision. We also applied this anomaly maps to existing methods to improve the performance of anomaly detection. Experimental results on the MVTec AD dataset using AUROC and IoU demonstrated that the proposed method is a specialized network for anomaly segmentation compared to the existing methods.



\bibliography{iclr2022_conference.bbl}
\bibliographystyle{iclr2022_conference}

\appendix
\section{Anomaly Detection Using Proposed Anomaly Map}
Here we provide detailed information for the training and loss functions of anomaly detector using the proposed anomaly map from Section 3.4.
\subsection{Training Process of Anomaly Detection Method}
The proposed anomaly detection method uses an anomaly map generated from the AnoSeg along with the input image to learn the distribution of the normal image and the anomaly map. Therefore, the anomaly detector determines whether the anomaly map is focusing on the normal region of the input image while determining whether the input image is a normal image. Unlike AnoSeg, the proposed anomaly detection method does not use the synthetic anomaly  as a real class in an adversarial loss because discriminator of anomaly detector only needs to learn the normal data distribution for anomaly detection. The loss function for learning the discriminator of the anomaly detector () is as follows:

where , , , and  represent reconstructed a normal image, a anomaly map of AnoSeg, a normal image, and a normal mask, repectively.

Also, to help estimate the normal data distribution, we propose a synthetic anomaly classification loss that discriminates between synthetic data and normal data. As confirmed in (\cite{semi}), the proposed synthetic anomaly classification loss improves the anomaly performance of the discriminator. This synthetic anomaly classification loss is defined as:


Then, we use the feature matching loss introduced in (\cite{imp}) to stabilize the learning of the discriminator and extract the anomaly score. The high-level representations of the normal and reconstructed samples are expected to be identical. This loss is given as follows:

where  is the second to the last layer of the discriminator. Fig. 9 shows an overview of the overall training process.
\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{110.png} 
\end{center}
\vspace{-0.2cm}
  \caption{Overview of the training process of the proposed anomaly detection method.}
\label{fig2}
\end{figure}

\subsection{Quantitative Evaluation of Anomaly Detection in the MVTec AD dataset.}

We describe the performance evaluation setting of the existing method that was not included in the main paper due to the length limitation. For performance comparison with existing methods, we used the results from existing literature, excluding the uninformed students method (US) (\cite{stu}). US method is only evaluated with PRO scores for anomaly segmentation without the provision of the AUROC for the anomaly segmentation and detection. Therefore, we re-implemented the large patch size (patch size is ) version of the Student method and evaluated it on anomaly detection and segmentation. Tables 4 also shows the class-wise anomaly detection performances for the MVTec AD (AUROC) dataset.


\begin{table*}
\begin{center}
\label{table:headings}
\caption{Performance comparison of anomaly detection in terms of image-level AUROC with the proposed method and conventional SOTA methods on the MVTec AD dataset (\cite{mvtec}).}
\makeatletter
\def\hlinewd#1{
\noalign{\ifnum0=‘}\fi\hrule \@height #1 \futurelet
\reserved@a\@xhline}
\newcommand{\hthickline}{\hlinewd{1pt}}
\newcommand{\hthinline}{\hlinewd{.2pt}}
\makeatother
\newcolumntype{Z}{>{\centering\arraybackslash}X}
{\footnotesize
\begin{tabularx}{\linewidth}{c||Z|Z|Z|Z|Z|Z|Z|Z}
\hthickline
  &\multicolumn{7}{c}{Anomaly Detection (Image-level AUROC)}\\\hline
\multirow{2}{*}{Method} &\multirow{2}{*}{AE} &\!\multirow{2}{*}{CAVGA} &\multirow{2}{*}{US}  &Patch SVDD &\multirow{2}{*}{SPADE}  &\!\!\multirow{2}{*}{Cutpaste}  &\!\!\multirow{2}{*}{Proposed} \\
\hline\noalign{\smallskip}
\hline
bottle     & 0.80 & 0.91 &  0.85   & \textbf{0.99} & - &0.98 &0.98 \\\hline
Cable      & 0.56 & 0.67 &  0.90   & 0.90 & - & 0.81 & \textbf{0.98} \\\hline
Capsule    & 0.62 & 0.87 &  0.82   & 0.77 & - & \textbf{0.96} & 0.84 \\\hline
Carpet     & 0.50 & 0.78 &  0.86   & 0.93 & - & 0.93 & \textbf{0.96} \\\hline
Grid       & 0.78 & 0.78 &  0.60   & 0.95 & - &\textbf{0.99} & \textbf{0.99} \\\hline
Hazelnut   & 0.88 & 0.87 &  0.91   & 0.92 & - & 0.97 & \textbf{0.98} \\\hline
Leather    & 0.44 & 0.75 &  0.73   & 0.91 & - &\textbf{1.00} & 0.99 \\\hline
Metal\_nut & 0.73 & 0.71 &  0.58   & 0.94 & - & \textbf{0.99} & 0.95 \\\hline
Pill       & 0.62 & 0.91 &  0.90   & 0.86 &- & \textbf{0.92} & 0.87 \\\hline
Screw      & 0.69 & 0.78 &  0.90   & 0.81 & - & 0.86 & \textbf{0.97} \\\hline
Tile       & 0.77 & 0.72 &  0.87   & \textbf{0.98} & - & 0.93 & \textbf{0.98} \\\hline
Toothbrush & 0.98 & 0.97 &  0.81   & \textbf{1.00} & - & 0.98 & 0.99 \\\hline
Transistor & 0.71 & 0.75 &  0.85   & 0.92 & - & \textbf{0.96} & \textbf{0.96} \\\hline
Wood       & 0.74 & 0.88 &  0.68   & 0.92 & - &\textbf{0.99} & \textbf{0.99} \\\hline
Zipper     & 0.80 & 0.94 &  0.90   & 0.98 & - & \textbf{0.99} & \textbf{0.99} \\\hline\hline
Mean       & 0.71 & 0.82 &  0.84   & 0.92 & 0.86 & 0.95 & \textbf{0.96}\\\hline

\hthickline 
\end{tabularx}
}
\end{center}
\vspace{-0.2cm}
\end{table*}


\subsection{Ablation study of Anomaly Detection Method}
We evaluated the effectiveness of the individual components in the proposed anomaly detection method on the MVTec AD dataset, as shown in Table 5. The base model used the same structure as the proposed model, and only the input images were fed except for the mask. The base model compared the features of the input image and the reconstructed image to calculate an anomaly score. However, since the reconstructed image often had anomaly regions restored, the base model has the low performance. The model that the feature matching loss is applied had slightly improved AUROC than the base model. The proposed anomaly detection method performed anomaly detection using input images and anomaly maps. Image-level AUROC was significantly increased by up to 15\%. Hence, the model using an anomaly map as an input performed anomaly detection more sensitive than the conventional method using only an input image. Finally, to enhance the estimation of the normal data distribution, we added an anomaly classification loss. This loss helps in estimating the boundaries of the normal data distribution where synthetic anomaly data are separated. 

\begin{table*}
\begin{center}
\label{table:headings}
\caption{Anomaly detection performance of various configurations on the MVTec AD dataset.}
\makeatletter
\def\hlinewd#1{\noalign{\ifnum0=‘}\fi\hrule \@height #1 \futurelet
\reserved@a\@xhline}
\newcommand{\hthickline}{\hlinewd{1pt}}
\newcommand{\hthinline}{\hlinewd{.2pt}}
\makeatother
\newcolumntype{Z}{>{\centering\arraybackslash}X}
{\footnotesize
\begin{tabularx}{\linewidth}{c||Z|Z|Z|Z}
\hthickline
  &\multicolumn{4}{c}{Ablation study (Image-level AUROC)}\\\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Base model} & + Feature matching loss & + Input anomaly map & + Anomaly classification loss\\
\hline Mean &0.812 &0.842 &0.943 &0.961\\\hline
\hthickline 
\end{tabularx}
}
\end{center}
\end{table*}

\section{Details on the Network Architectures}

Table 6 shows the network structure of the proposed method. Each network is described by a list of layers including an output shape, a kernel size, a padding size, and a stride. In addition, batch normalization (BN) and activation function define whether BN is applied and which activation function is applied, respectively. The decoder used for image reconstruction has the same structure as the decoder for generating anomaly map, and AnoSeg uses two decoders. The structure of the proposed anomaly detector also has the same structure as that of AnoSeg. The structure of the AnnoSeg is also available in our code added in the supplementary material. The provided code contains pre-trained weight.


\begin{table*}
\begin{center}
\label{table:headings}
\renewcommand{\tabcolsep}{4pt}
\makeatletter
\def\hlinewd#1{\noalign{\ifnum0=‘}\fi\hrule \@height #1 \futurelet
\reserved@a\@xhline}
\newcommand{\hthickline}{\hlinewd{1pt}}
\newcommand{\hthinline}{\hlinewd{.2pt}}
\makeatother
\newcolumntype{Z}{>{\centering\arraybackslash}X}
{\small
\begin{tabularx}{\linewidth}{Z||Z|Z|c|c|c}
\hthickline
Network &Layer (BN, activation function) &Output size &Kernel &Stride &Pad\\
\hline\noalign{\smallskip}
\hline
\multirow{1}{*}{Encoder}  &Resnet-18 &8 x 8 x 512 & - & - & - \\
\hline

\multirow{12}{*}{Decoder}  
&Conv 1 (BN, ReLU) &8 x 8 x 512 &3 x 3 &1 &1\\
&ConvTr 1 (BN, ReLU) &16 x 16 x 512 &4 x 4 &2 &1\\
&Conv 2 (BN, ReLU) &16 x 16 x 256 &3 x 3 &1 &1\\
&ConvTr 2 (BN, ReLU) &32 x 32 x 256 &4 x 4 &2 &1\\
&Conv 3 (BN, ReLU) &32 x 32 x 128 &3 x 3 &1 &1\\
&ConvTr 3 (BN, ReLU) &64 x 64 x 128 &4 x 4 &2 &1\\
&Conv 4 (BN, ReLU) &64 x 64 x 128 &3 x 3 &1 &1\\
&ConvTr 4 (BN, ReLU) &128 x 128 x 128 &4 x 4 &2 &1\\
&Conv 5 (BN, ReLU) &128 x 128 x 128 &3 x 3 &1 &1\\
&ConvTr 5 (BN, ReLU) &256 x 256 x 128 &4 x 4 &2 &1\\
&Conv 6 (BN, ReLU) &256 x 256 x 128 &3 x 3 &1 &1\\
&Conv 7 (-, Sigmoid) &256 x 256 x 3 &3 x 3 &1 &1\\

\hline
\multirow{8}{*}{Discriminator}  
&Conv 1 (-, LeakyReLU) &128 x 128 x 64 &4 x 4 &2 &1\\
&Conv 2 (BN, LeakyReLU) &64 x 64 x 128 &4 x 4 &2 &1\\
&Conv 3 (BN, LeakyReLU) &32 x 32 x 256 &4 x 4 &2 &1\\
&Conv 4 (BN, LeakyReLU) &16 x 16 x 512 &4 x 4 &2 &1\\
&Conv 5 (BN, LeakyReLU) &8 x 8 x 512 &4 x 4 &2 &1\\
&Conv 6 (BN, LeakyReLU) &4 x 4 x 512 &4 x 4 &2 &1\\
&Conv 7 (BN, LeakyReLU) &2 x 2 x 128 &4 x 4 &2 &1\\
&Conv 8 (-, Sigmoid) &1 x 1 x 1 &4 x 4 &2 &1\\
\hline
\end{tabularx}}
\end{center}
\caption{Architectural details of the proposed method. ConvTr denotes a transposed convolution layer and Conv denotes a convolution layer.}
\end{table*}



\section{Analysis of Threshold Sensitivity}

In this section, we show the IoU results according to threshold changes for each category in the MVTec AD dataset. As shown in Figs. 10, 11, and 12, compared to SPADE and Patch SVDD, which are comparative methods, the performance difference of the proposed AnoSeg is not large according to the change in the threshold.


\begin{figure}[b]
\begin{center}
\includegraphics[width=1.0\linewidth]{d1.png} 
\end{center}

  \caption{IoU results for each category in the MVTec AD dataset according to the threshold change. (Green: AnoSeg, Orange: SPADE, Blue: Patch SVDD)}
\label{fig9}
\end{figure}


\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{d2.png} 
\end{center}

  \caption{IoU results for each category in the MVTec AD dataset according to the threshold change. (Green: AnoSeg, Orange: SPADE, Blue: Patch SVDD)}
\label{fig10}
\end{figure}


\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{d3.png} 
\end{center}

  \caption{IoU results for each category in the MVTec AD dataset according to the threshold change. (Green: AnoSeg, Orange: SPADE, Blue: Patch SVDD)}
\label{fig11}
\end{figure}




\section{Qualitative results on the MVTec AD dataset}

We provided additional qualitative results of our method on the MVTec AD dataset in Figs. 13, 14, 15, 16, and 17. For each class, an Input image, a proposed anomaly map, and a GT mask are provided. The proposed AnoSeg had the highest performance even for anomaly regions with various sizes.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{a1.png} 
\end{center}

  \caption{Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.}
\label{fig12}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{a2.png} 
\end{center}

  \caption{Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.}
\label{fig13}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{a3.png} 
\end{center}

  \caption{Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.}
\label{fig14}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{a4.png} 
\end{center}
  \caption{Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.}
\label{fig15}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{a5.png} 
\end{center}

  \caption{Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.}
\label{fig16}
\end{figure}


\end{document}
