\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

\usepackage{amsmath,graphicx}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{tikz}
\usepackage{url}
\newcommand{\displaycomments}

\ifdefined\displaycomments
\newcommand{\note}[1]{\textcolor{red}{\emph{#1}}}
\else
\newcommand{\note}[1]{}
\fi

\usepackage{xspace}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}      \usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{cite}
\usetikzlibrary{arrows}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\subj}{subject \  to}
\DeclareMathOperator*{\var}{Var}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{cor}{Corollary}
\newtheorem{mydef}{Definition}
\newtheorem{lem}{Lemma}
\newtheorem{theo}{Theorem}
\newtheorem{prop}{Proposition}

\renewcommand{\d}{\:{\rm{d}}} 
\newcommand{\E}{\mathop{\mathbb E}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\NormalGamma}{\operatorname{NormalGamma}}
\newcommand{\GaussianGamma}{\operatorname{GaussianGamma}}
\newcommand{\GIW}{\operatorname{GIW}}
\newcommand{\B}{\operatorname{B}}
\newcommand{\Bern}{\operatorname{Bern}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Weibull}{\operatorname{Weibull}}
\newcommand{\Rayleigh}{\operatorname{Rayleigh}}
\newcommand{\Laplace}{\operatorname{Laplace}}
\newcommand{\Gam}{\operatorname{Gamma}}
\newcommand{\Dir}{\operatorname{Dir}}
\newcommand{\IGam}{\operatorname{IGamma}}
\newcommand{\ISE}{\operatorname{ISE}}
\DeclareMathOperator*{\round}{round}
\renewcommand{\t}[1]{\mathrm{T}#1}
\newcommand{\N}{\mathcal{N}}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\aka}{a.k.a.,\xspace}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\cf}{c.f.,\xspace}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\diag}{\operatorname{Diag}}
\renewcommand{\baselinestretch}{0.9}
\usepackage{cite} \usepackage{stfloats}
\usepackage{amsmath}

\def\x{{\mathbf x}}
\def\L{{\cal L}}



\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{Maximum Entropy property of discrete-time  stable spline kernel}

\name{Tohid Ardeshiri and Tianshi Chen\thanks{This research has been partially supported by a research grant for junior researchers, No. 2014-5894 and the frame project grant ETT (621-2010-4301), both funded by the Swedish Research Council, and the ERC advanced grant LEARN, no. 267381, funded by the European Research Council.}}
\address{Division of Automatic Control, Department of Electrical Engineering,\\
Link\"{o}ping University, 581 83 Link\"{o}ping, Sweden,\\
email: \{tohid,tschen\}@isy.liu.se}


\begin{document}

\maketitle
\begin{abstract}
In this paper, the maximum entropy property of the discrete-time
first-order stable spline kernel is studied. The advantages of
studying this property in discrete-time domain instead of
continuous-time domain are outlined. One of such advantages is that
the differential entropy rate is well-defined for discrete-time
stochastic processes. By formulating the maximum entropy problem for
discrete-time stochastic processes we provide a simple and
self-contained proof to show what maximum entropy property
the discrete-time first-order stable spline kernel has.
\end{abstract}
\begin{keywords}
Machine learning, Gaussian process, impulse response estimation,
maximum entropy (MaxEnt).\end{keywords}
\section{Introduction}
\label{sec:intro} System identification is about how to construct
mathematical models based on observed data, see e.g.,
\cite{Ljung:99}. For linear time-invariant (LTI) and causal systems,
the identification problem can be stated as follows. Consider

where  are the time instants at which the
measured input  and output  are collected,  is the
disturbance,  is the impulse response with  for continuous-time systems and
 for discrete-time systems, and  is
the convolution of  and  evaluated at .
The goal is to estimate  as good as possible.



Recently, there have been increasing interests in system
identification community to study system identification problems
with machine learning methods, see e.g., \cite{LHO:11},
\cite{PDCDL14}. An emerging trend among others is to apply Gaussian
process regression methods for LTI, stable and causal system
identification problems, see \cite{PN10a} and its follow up papers
\cite{PCN11}, \cite{COL12a}, \cite{CACLP14}. Its idea is to model
the impulse response  with a suitably defined Gaussian process
which is characterized by where  is the mean function and is often set to be
zero, and  is the covariance function, also called the
kernel function in machine learning and statistics, see e.g.,
\cite{RasmussenW:06}.


The kernel  is parametrized by a hyper-parameter 
and further written as . The key issue is to design a
suitable parametrization of , or in other words, the
structure of , because it reflects our prior knowledge
about the system to be identified. Several kernel structures  have
been proposed  in the literature, e.g., the
stable spline (SS) kernel in \cite{PN10a} and the diagonal and
correlated (DC) kernel in \cite{COL12a}.

Our prior knowledge is however never complete and it is thus worth
to note Jaynes's maximum entropy rationale \cite{jaynes} to derive
complete statistical prior distributions from incomplete a priori
information.  By maximizing the entropy rate of a stochastic process
subject to constraints imposed by prior knowledge, the stochastic
process which encompasses the least assumptions about the data can
be obtained.



Interestingly, \cite{pillonetto2011} shows based on a result in
\cite{DeTL98} that for continuous-time systems, the continuous-time
first-order SS kernel (also derived by  deterministic arguments in
\cite{COL12a} and called Tuned Correlated (TC) kernel):

has a certain maximum entropy property.

In Section \ref{sec:CTmaxEnt} the maximum entropy
property of the continuous-time kernel (\ref{eq:TC}) is briefly presented.
Then, we explain why it is worthwhile to study the maximum entropy
property for the discrete-time first order SS kernel
 and we will further elaborate our results in Section~\ref{sec:DTmaxEnt}.




\subsection{Maximum entropy property of continuous-time SS kernel}
\label{sec:CTmaxEnt}

In \cite{pillonetto2011}, the maximum differential entropy rate
\emph{continuous-time} stochastic process subject to constraints on
smoothness and bounded-input bounded-output (BIBO) stability is
sought. The definition of the differential entropy rate of a stationary \emph{continuous-time
Gaussian} process  with power spectrum  is adopted from \cite{DeTL98} in \cite{pillonetto2011}:

 We describe how smoothness and stability constraints are expressed in \cite{pillonetto2011} in separate items.

\vspace{0.2cm}
\noindent\textbf{1) Smoothness:} The smoothness constraint on the impulse responses
is addressed by using \cite[Theorem 1]{DeTL98} which suggests that the
smoothness of a signal (with some of its derivatives continuous and
bounded) can be imposed by assuming that the variances of these
derivatives are finite.
The main result in \cite[Theorem 1]{DeTL98} is given in Proposition~\ref{prop:DeNicalao} for the sake of completeness.

\begin{prop}
\label{prop:DeNicalao}\cite[Theroem 1]{DeTL98}  Let  be a
zero-mean bandlimited stationary Gaussian process with power
spectrum   for . Given finite
, , assume that there exist real
numbers ,  such that
, . Under this assumption, if
there exists  that maximizes  in (\ref{eq:def_De})
subject to constraints , , then  the spectrum is
given by . In
particular, if there is no constraints on the first  order
derivatives, then the spectrum becomes .
\end{prop}
It is further claimed in  \cite{pillonetto2011} that as , the Wiener
process is the maximum differential entropy rate process among all
\emph{Gaussian} processes whose 1st-order derivatives are stationary
Gaussian processes with finite variance.

\vspace{0.2cm}
\noindent
\textbf{2) Stability:} The BIBO stability constraint on the impulse
response  is imposed by using a \emph{stable} time
transformation:  where  is the Wiener
process defined on  and . Adding this
on top of \cite[Theorem 1]{DeTL98} leads to the maximum differential
entropy rate result in \cite[Proposition 2]{pillonetto2011}
regarding the SS kernel (\ref{eq:TC}).




Deriving the maximum entropy process in continuous-time in \cite{DeTL98} and \cite{pillonetto2011} is quite involved, due to the infinite-dimensional nature of of the problem and absence of a well-defined differential entropy rate for a generic continuous-time stochastic process.

\subsection{Our contributions}
In this paper, we focus on discrete-time impulse responses
(stochastic processes), and provide a simple and self-contained
proof to show  the maximum entropy property of the discrete-time
first-order SS kernel (\ref{eq:TC_dt}). The advantages of working in
discrete-time domain include
\begin{enumerate}
\item The differential entropy rate is well-defined for discrete-time stochastic process.\vspace{-3mm}
\item Given a stochastic process, its finite difference process can be well-defined in discrete-time domain.\vspace{-3mm}
\item It is possible to show what maximum entropy property a zero-mean discrete-time Gaussian process with covariance function (\ref{eq:TC_dt}) has.\vspace{-2mm}
\end{enumerate}
Also, we define the discrete-time Wiener process and prove its
maximum entropy property.

\section{The Discrete-time stable spline kernel}
\label{sec:DTmaxEnt} Before deriving the maximum entropy kernels for
discrete-time  processes, we give some definitions. In the
rest of the paper the ordered index set  is defined as
 also,
the points  in the index set  do not have to be
equidistant.
\begin{mydef}
The differential entropy  of a continuous random variable  with density  is defined as

where,  is the support set of the random variable \cite{Cover}. \hfill 
\end{mydef}
\begin{mydef}
The differential entropy rate of a real-valued discrete-time
stochastic process 
is defined as

if the limit exists \cite{Cover}.    \hfill 
\end{mydef}


\begin{mydef}
The discrete-time Gaussian white noise process is a
discrete-time Gaussian process whose covariance function is
 \cite{Vantrees}  where  is
equal to  for  and  otherwise. \hfill 


\end{mydef}

In Lemma \ref{lem:WGN} we show that Gaussian white noise
process is the maximum differential entropy rate stochastic process
with constant and finite variance. The proof is an adaptation of
proof of Burg's maximum entropy theorem in \cite{Cover}.
\begin{lem}
\label{lem:WGN} The discrete-time Gaussian white noise
process is the maximum differential entropy rate stochastic process
on  with constant and finite variance.
\begin{proof}
First, let us formulate the  maximum differential entropy rate
problem.

where,  is the variance operator. In the
following, we show that  is a Gaussian white noise process
with variance .

Let  be any stochastic process that  satisfies the constraint .

Also, let  be a Gaussian process
with the same covariance matrix as  \footnote{Note that we are not making any assumptions
regarding the off-diagonal elements of the covariance matrix}. The
multivariate Gaussian distribution maximizes the entropy over all
dimensional  vector valued random variables under a covariance
constraint \cite{Cover}, therefore

using the  the chain rule and owing to the fact that conditioning reduces the entropy we obtain

Since  obeys a multivariate Gaussian distribution and all the diagonal entries  of the covariance matrix are equal to , all  are distributed according to a uni-variate Gaussian distribution with variance . Also, it is known that the entropy of a uni-variate Gaussian distribution depends only on its variance.  Hence, .

Now define  as a Gaussian white noise process where  are identically distributed as . Since

we obtain

Dividing by  and taking the limit, we obtain

where  and is the differential entropy rate of the
Gaussian white noise process. Hence, the maximum differential
entropy rate stochastic process with constant and finite variance
 is the Gaussian white noise process with variance
.
\end{proof}
\end{lem}

The Wiener process  is a continuous-time stochastic process
which can be defined as the definite integral of continuous-time
zero-mean Gaussian white noise, has many applications in applied mathematics
and signal processing \cite{arnold}. The Wiener process can be
characterized by these properties~\cite{durrett2010}
\begin{enumerate}
\item The initial condition .\vspace{-2mm}
\item The function  is almost surely continuous everywhere.\vspace{-2mm}
\item  has independent increments with   for  , where  denotes the Gaussian distribution with mean  and variance .\vspace{-2mm}
\end{enumerate}

In the following we will define a stochastic process, we refer to as
discrete-time Wiener process and show two of its properties in
Lemmas~\ref{lem:Wiener} and~\ref{lem:covWiener} and its maximum
differential entropy rate property in
Proposition~\ref{prop:Wiener}.
\begin{mydef}
The discrete-time Wiener process  is characterized by these properties
\begin{enumerate}
\item ,
\item  has independent increments with   for .\hfill \end{enumerate}
\end{mydef}
\begin{lem}
\label{lem:Wiener} The discrete-time stochastic process   on
 is  a discrete-time Wiener process if and only if
 and

where  is the zero-mean Gaussian white noise process.\begin{proof}
First, we prove the necessary part. That is, we show if  is a
discrete-time Wiener process, it can be expressed in the form of
(\ref{eq:wiener}). Let  for
. Since  we have

Also, since  has independent increments it follows that  is a
discrete-time zero-mean Gaussian white noise process. Also, from the
definition of  we have 
\small{}\normalsize
Now we prove the sufficient part, i.e., the stochastic process
(\ref{eq:wiener}) is a discrete-time Wiener process. Since Gaussian
processes are closed under linear operations \cite{RasmussenW:06},
 is a Gaussian process. Also .
Furthermore, let ,

Therefore,   for . Since  is a Gaussian white
noise process, the increments of  are independent and the proof
follows.
\end{proof}

\end{lem}
\begin{lem}
\label{lem:covWiener}
The covariance of the discrete-time Wiener process is given by

where,  is the variance of the underlying Gaussian white noise process.
\begin{proof} Since , then 

\end{proof}
\end{lem}
In Proposition \ref{prop:Wiener} the maximum differential entropy
rate property of the discrete-time Wiener process when the index set is unbounded from above is studied. For
the continuous-time case, the smoothness constraint on the
stochastic process is expressed by constraining  the variance of its
first-order derivative  to be constant and finite, see Proposition
\ref{prop:DeNicalao}. Due to the absence of derivative for the
Wiener process we use the variance of the finite difference of the
discrete-time stochastic process defined below.
\begin{mydef} The finite difference of a discrete-time stochastic process  on , at  is an expression of the form

\hfill 
\end{mydef}
\begin{prop}
\label{prop:Wiener} The discrete-time Wiener process is the maximum
differential entropy rate stochastic process on  with
 and   such that its value at origin is zero and is
zero-mean and variance of its finite difference at all
 is proportional to the time increment
 and  is bounded from below by a
positive number. That is, the discrete-time Wiener process is the
optimal solution to the problem:


\begin{proof}
Let   be any discrete-time stochastic process on
. Now we define the stochastic process  as

Therefore,  and the variance of the finite difference of  obeys, 
So the third constraint in the maximization problem
\eqref{eq:maxEntWiener} can be written as
. Also, from \eqref{eq:defw} we
have
Let  and
. We have
 where  is a lower triangular
non-singular matrix independent of  and .
Using~\eqref{eq:defH} and \cite[Corollary to Theorem 8.6.4]{Cover}
we obtain

Therefore, it is sufficient to maximize the differential entropy
rate of the underlying stochastic process  such that the
variance of  is constant and . Using Lemma~\ref{lem:WGN} 
turns out to be a zero-mean Gaussian white noise process. Consequently, using
Lemma~\ref{lem:Wiener} the maximum differential entropy rate
stochastic process  turns out to be the discrete-time Wiener
process.
\end{proof}
\end{prop}

\noindent\textbf{Remark 1.} The assumption  is not restrictive. 
For example, the assumption is trivially satisfied for uniform sampling where  , . 
When the time increment 
is not bounded from below,  becomes infinite and thus the differential entropy rate is
not defined. In this case, the discrete-time Wiener process is the
maximum differential entropy stochastic process on the finite
segment  in the following
sense: for any , it optimizes the maximum
differential entropy problem




Before proceeding to the maximum differential entropy  property of
the discrete-time stable spline kernel (\ref{eq:TC_dt}) in
Proposition \ref{prop:DTstablespline}, we introduce the concept of
reverse ordered index set of 
which is defined as the ordered index set 
and denoted by .

\begin{prop}
\label{prop:DTstablespline} Let  denote a zero-mean
discrete-time stochastic process defined on an ordered index set
. Now consider a finite segment of  with index set
. Then for any , the zero-mean Gaussian process with
covariance function (\ref{eq:TC_dt}) is the solution to the maximum
differential entropy problem:

\begin{proof} Note that 
which implies that (\ref{eq:maxEntSS_dt}) is equivalent to the
maximum entropy problem (\ref{eq:maxEntWiener_reduced}). From
Proposition \ref{prop:Wiener} and Remark 1, the optimal solution to
(\ref{eq:maxEntWiener}) is the discrete-time Wiener process 
which is zero-mean Gaussian and has covariance function
,  (from Lemma
\ref{lem:covWiener}). As a result, the optimal solution to
(\ref{eq:maxEntSS_dt}) is the zero-mean Gaussian process induced by
 which has the covariance function
.
\end{proof}
\end{prop}


\section{conclusion}
The maximum entropy property of the first-order discrete-time stable spline
kernel for identification of LTI stable and causal systems are
studied. By formulating the maximum entropy problem for
discrete-time stochastic processes we provide a simple and
self-contained proof to show  the maximum entropy property of the
discrete-time first-order stable spline kernel. Also, we define the
discrete-time Wiener process and prove its maximum entropy property.

\bibliographystyle{IEEEbib}
\bibliography{taref2014}
\end{document}
