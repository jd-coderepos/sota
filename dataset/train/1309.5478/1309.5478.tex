

\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}

\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{subfigure}
\usepackage{cite}

\usepackage{color} 




\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}



\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



\date{}

\pagestyle{myheadings}







\begin{document}
\begin{flushleft}
{\Large
\textbf{Fast -NNG construction with GPU-based quick multi-select}
}
\\
Ivan Komarov, Ali Dashti,  
Roshan M. D'Souza, 
\\
 Dept. of Mechanical Engineering, Complex Systems Simulation Lab, University of Wisconsin-Milwaukee, Milwaukee, WI, USA
\\
 E-mail: dsouza@uwm.edu
\end{flushleft}
\begin{abstract}
In this paper we describe a new brute force algorithm for building the -Nearest Neighbor Graph (-NNG). The -NNG algorithm has many applications in areas such as machine learning, bio-informatics, and clustering analysis. While there are very efficient algorithms for data of low dimensions, for high dimensional data the brute force search is the best algorithm.  There are two main parts to the algorithm: the first part is finding the distances between the input vectors which may be formulated as a matrix multiplication problem. The second is the selection of the -NNs for each of the query vectors. For the second part, we describe a novel graphics processing unit (GPU) -based multi-select algorithm based on quick sort.  Our optimization makes clever use of warp voting functions available on the latest GPUs along with use-controlled cache. Benchmarks show significant improvement over state-of-the-art implementations of the -NN search on GPUs. 

\end{abstract}

\section{Introduction}
The -nearest neigbhor (-NN) and the related -nearest neighbor graph (-NNG) are important algorithms for data classification with a wide variety of applications in areas such as bioinformatics \cite{Roberts:2007, Weston:2004}, data mining\cite{Zaki:2000}, machine learning \cite{Maier:2009,Liu:2010,Tenenbaum:2000}, cluster analysis\cite{Franti:2006}, and pattern recognition\cite{Duda:2001}. Given a initial set of data points called corpus/training set, and a set of query points, -NN find the nearest  neighbors in the corpus set for each member of the query set. For low dimensional data-sets, there are a variety of indexing data structures such as  kd-trees \cite{Jones:2011}, BBD-tree \cite{Arya:1994}, random-projection trees (rp-trees) \cite{Dasgupta:2008}, and hashing based on locally sensitive hash \cite{Datar:2004} that can be built from the set of corpus/training data points. Next the -NN search can be accomplished very efficiently for each data point in the query set using these data structures.  In the case of -NNG, every data point in the corpus is also a member of the query set. \\

Direct construction of exact -NNG has been also been investigated by many researchers \cite{Bentley:1980, Clarkson:1983,Vaidya:1989,ParedesCFN:2006,Chan:1998,ConnorK:2010}. All these methods have time complexity that increases exponentially with data dimension.  Approximate methods that can handle high-dimensional data with a trade-off between speed and accuracy have also been investigated. These techniques are based on hybrid of spatial subdivision and small scale brute force refinement  \cite{ChenFS:2009,Wang:2012, DongCL:2011}.  The spatial subdivision becomes grossly inaccurate for data sets that have dimension exceed .\\

For high dimensional data, the most efficient method for finding -NNG is in fact the brute force method \cite{Indyk:2004}. The brute force algorithm consists of two fundamental steps. The first step finds the distances between the query point based on an user-defined metric. This process result in a distance matrix of size  where  is the number of query points and  is the number of corpus data points. The -NNs in the corpus data points for each query point is then found by sorting each row of the matrix and finding the  indices of the  smallest distances. Recent efforts have accelerated the brute force -NN and -NNG methods by parallelizing on graphics processing units. These methods differ in the manner in which the  nearest indices are found. In \cite{Garcia2010, Arefin2012} each row of the distance matrix is processed by one thread. They use a modified insertion sort algorithm to select the  nearest neighbors. The data structure is stored in global memory, which in turn leads to un-coalseced memory access patterns. Moreover, each thread has divergent paths to find the  nearest neighbors which leads to loss in computational efficiency. In \cite{Barrientos2011}, every row in the distance matrix is processed by one thread block. A single heap in global memory maintains the current  nearest elements. Each thread strides through the row of the matrix and inserts an element into a buffer if it is smaller that the largest element in the heap. When the buffer fills up, all threads synchronize and push their elements into the heap serially.   In \cite{Kato2009}, a thread block is used to process a single row. Each thread in the thread block strides through the array storing the  smallest elements in a local heap maintained in global memory. Next all the thread heaps are merged into 32 heaps in shared memory threads in a single thread warp. Finally a single thread merges the 32 heaps in shared memory to find the  nearest neighbors. Both methods dramatically slow down if  grows large because of branch divergence and un-coalesced global memory access. In \cite{Kuang:2009}, radix sort is used for finding  nearest neigbhors. Each row is processed in a separate kernel call and for  that fit into GPU memory, this process underutilizes resources.\\

\subsection*{Graphics Processing Units: Architecture and Execution Model}
Graphics Processing Units (GPUs) were originally developed to handle computations related to computer graphics tasks such as transformation and shading. Computational scientists used this functionality to accelerate scientific computing by posing such computations in terms of shading operations \cite{Owens:2007}. Subsequently, GPU vendors developed specialized C language extensions to facilitate direct access to the computing hardware for scientific computing. Our implementation uses CUDA \cite{Sanders:2010}, a C language extension developed by NVIDIA.\\

Architecturally, a NVIDIA GPU consists of several multiprocessors (MPs). Each MP has several serial processors. In addition to registers, each MP has access to on-chip user-controlled cache called shared memory. Logically, threads are organized into thread blocks (TBs). All threads in a TB are executed on a single MP. Threads in a TB can be synchronized and can communicate through shared memory. Depending on the amount of shared memory and registers required, there could be several TBs assigned to be executed on a MP. All threads can communicate through global main memory. Registers have the highest access speed, followed by the shared memory, and then global memory. Of course, data in the registers cannot be shared among threads. Shared memory locations are accessed through specific memory banks.\\

At the hardware level, threads in a TB are divided into groups of 32 threads called thread warps. Thread warps are analogous the threads in a typical symmetric multi-processor architecture.  All threads in a thread warp execute in lockstep. Any branching between threads in a thread warp will cause serialization (different types of serialization depending on the complexity of the branching). Therefore it is advisable to avoid branching between threads of the same warp. During global memory access, if the threads in a warp access locations outside of contiguous 128 byte segments, the memory access will be serialized. Also, if two or more threads try to access shared memory affiliated to the same memory bank, this memory access will be serialized. An exception is when all threads in a warp access the same shared memory location. In such cases, a broadcast mechanism is automatically engaged. Therefore, implementations of algorithms for GPU acceleration require careful consideration of architectural limitations as well as advantages. \\

In our implementation we use an NVIDIA Tesla C2050 GPU. It has 14 multiprocessors each with 32 cores for a total of 448 cores. The single precision peak performance is 1.0TFlops. Each MP has 64KB of shared memory and a 128KB large register file. Main memory is 3GB GDDR5. 


\section*{Methods}
In this paper, we present a hybrid parallelization approach for brute force computation of multiple -NN queries. The first part of the brute force -NN, namely, finding the distance metric matrix is formulated as a matrix multiplication problem. Therefore efficient implementations of dense matrix operations can be used. The second part is finding the -NN for each query data point.  For this part of brute force -NN, we have develop a two level parallel implementation of the quick select algorithm. At the top or coarse level, multiple queries are processed independently and in parallel. At the fine level of parallelism, every query is parallelized over thread in a thread block or thread warp. \\





\subsection*{Related Work}
Recently, there are quite a few works that are focussed on parallel -NN queries on GPUs. In \cite{Sismanis:2012}, a truncated bitonic sort based selection algorithm has been described. The results show a dramatic drop off in performance with increase in both  the number of data points  and number of nearest neighbors . For example, the speedup over straight sort and selection using the CUDA \texttt{thurst::sort} for , is barely 4X. In \cite{Baxter:2011}, a radix select implementation on the GPU is presented for selecting the  element. This implementation is very efficient for large values of  and runs upto 190X faster than a single core CPU implementation of the serial n-select algorithm. In \cite{Monroe:2011} a randomized selection algorithm is presented. This method however shows no improvement over CUDA \texttt{thurst::sort} for data sizes of upto . On the other hand the implementation is \cite{Baxter:2011} shows a 6.3X speedup. Both methods are however only suited for single query operations on ultra-large data sets. A similar implementation has been presented by  Alabi et al. \cite{Alabi:2012}.  The -NN problem is slightly different from the method for selecting the  element because extra work is necessary to track all the  smallest elements. \\

In \cite{Cederman:2010} a data-parallel quick-sort algorithm for GPUs has been presented. They describe a two stage process. In the first stage, all thread-blocks cooperatively work on partitioning the original list to a point where the size of each partition is small enough to be processed by a single thread block. The first stage needs inter-thread block synchronization since the results of two or more thread blocks needs to be merged to build the partitions. This can either be done through atomic operations or through default synchronization by ending kernel execution. The latter needs a new kernel launch for each new phase of partitioning. The second stage is very similar to the first stage but each partition in this case is handled by a single thread block.\\

 The partitioning process is a two step processes and uses an auxiliary output array (i.e. it is not an in-place write operation). In the first step, a pivot element is randomly selected. Next, each thread strides through the input array counting the number of elements that are greater than equal to and less than the pivot. These two counts are stored in two arrays in shared memory. A pre-fix sum on these array in shared memory gives the location in global memory where each thread needs to write into the auxiliary the elements in the partitioning process. At the end of each partitioning task, the input and auxiliary arrays are swapped. When the partition size falls below a pre-defined threshold, a direct sort algorithm such as bitonic sort is used.\\

\subsection*{GPU-based quick multi-select}
Our approach while similar to approach in \cite{Cederman:2010}, address some of the major drawbacks as related to the -NN application. For one, the quick-sort algorithm in \cite{Cederman:2010} is only suitable for sorting a single input array at a time. Secondly, inter thread block synchronization and synchronization between threads in the same thread block affects performance. Thirdly, the pre-fix sum operation that is used during the partition process is quite an expensive operation. Fourthly, the two step process of partitioning involves reading the same data from global memory twice (once for counting, once for writing). Finally, the write process into the auxiliary array is un-coalesced. Our approach operates on multiple arrays simultaneously. Each array is handled by a single thread warp. Threads in a warp are executed simultaneously on a single multi-processor and therefore are synchronization by default.  Partitioning of an array is done incrementally in 32 element wide segments. We use shared memory to ensure coalesced memory writes of the results of partitioning into the auxiliary array in global memory.  Our approach uses the warp voting function \texttt{\_ballot()} to partition the input without reading the input array twice and without executing the parallel-prefix sum. The ballot function \texttt{\_ballot()} fills a 32 bit unsigned integer, one bit per thread in the warp, based on the evaluation of the predicate .\\
 
Each thread in the warp first reads in an element into its register from global memory. Elements are then written to a 32 element wide shared memory array with elements greater than equal to the pivot being written from the right end and elements less than the pivot being written from the left end. To do this, each thread needs to know where in shared memory to off load its element.  We execute the warp voting function based on a predicate which checks if the element in the register is greater than equal to the pivot or smaller than the pivot.  Threads which have an element greater than or equal to the pivot set the corresponding bit to '1' and to '0' if the element is less than the pivot (Figure 1). If the element in the register is less than the pivot, then the thread needs to find how many of threads before it have elements less than the pivot and vice-versa. We use a combination of bit shift operations and the \texttt{\_popc(x)} function on the integer result of the warp voting operation to accomplish this. The \texttt{\_popc(x)} functions counts the number of bits set to '1' in the input integer ''. For example, if the warp vote integer in binary is , then it is clear that threads 0,2  have elements less than the pivot and threads 1,3 have elements greater than equal to the pivot. Each thread  within the warp computes the result .  When the  \texttt{\_popc()} function is applied to the result of this step, it will indicate the position in shared memory at which thread  will off load its element that happens to be less than the pivot. Similarly,  will indicate the position from the right side at which which thread  will off load its element that happens to be greater than or equal to the pivot. In the example, thread 3 will bit shift  to the right by 29 bits and the result will be . Then \texttt{\_popc()=2}. Therefore thread 3 will off-load its element at the second location from the left in shared memory (Figure 2).  Note that the total number of elements in share memory that are less than the pivot is given by \texttt{\_popc()}. Two global counters,  and , keep track of the total number of elements less than the pivot and total number of elements greater than equal to the pivot, respectively, are also maintained. These two counters indicate location in the auxiliary array at which the warp writes the incremental results of pivoting from shared memory.  Next the threads write the contents of the shared memory into global auxiliary array with threads whose id is less than \texttt{\_popc()} (number of elements that are greater than the pivot) writing from the left side and other threads writing from the right side. This write process requires two coalesced writes, one for elements smaller than the pivot and one for elements greater than or equal to the pivot, into the auxiliary array (Figure 3). \\

With one warp per query, we must have a minimum of  queries to fully utilize GPU resources. If we use one thread block per query, we can reduce the minimum number of queries for full GPU utilization to . When processing each row of the distance matrix using a thread block, each thread block will have several thread warps (depending on the thread count) that will cooperatively pivot elements. While writing the elements into shared memory, each thread has to determine its relative write position in shared memory w.r.t. all thread block threads. As a first step, each thread finds its write position w.r.t. threads in its own warp as described in the previous paragraph. Subsequently,  all threads in the thread block are synchronized. A inter warp pre-fix sum step determines the write position of each thread with respect to the entire thread block. The maximum number of thread warps per block is 16 (max. thread count in a thread block is 512 and the number of threads in a warp is 32). Because of the small number of elements, we use a single thread per thread block to conduct the inter warp prefix sum. This is much more economical than a multi-threaded pre-fix sum. \\

Once a partition is complete, the output array (auxiliary array) has a left side of length  elements each of whom is less than the pivot. The right side is of length  elements each of whom is greater than equal to the pivot. Suppose we need  nearest neighbours, and , then we need to process only the left hand side. Suppose , then we keep the left hand side as is, and partition the right hand side to find  elements. Since the input and auxiliary arrays are swapped at the end of the partition process, in the second case (), we would have to copy the left hand side from the auxiliary array to the input array. We can avoid copying the data by instead storing a stack of references which indicate the start and end indices and the arrays (auxiliary or input) that the partitions that form the  nearest neighbors are to be found. This significantly reduces the memory transactions needed for the operations.\\

\subsection*{Distance calculation}
Our implementation supports three distance metrics, namely, Euclidian, Cosine, and Pearson. The Euclidian distance between two vectors  is given by

The Cosine distance metric is given by: 

Finally, the Pearson distance is given by:

where,  and  is the mean of the entries in . Note that the Pearson distance coefficient is essentially the Cosine distance of the centered data sets.\\

The process of finding either of these distances requires operations for finding vector means, vector magnitudes and dot products. For two data sets , where the columns of  and  represent the data vectors  and  respectively, the  dot products 

are easily formulated as a matrix product . There are very efficient libraries for dense matrix multiplication which can be used for this purpose \cite{Volkov:2008}. The process of finding vector means and vector magnitudes are essentially vector reductions. The \texttt{thrust} library has optimized \texttt{reduction\_by\_key} which we have used to simultaneously find the vector means and magnitudes of all vectors in .\\

Finding the Eulcidian and Cosine distances require the same inputs (dot products and norms of ). In the first parallel kernel operation, we compute the vector norms of the vectors in  using a combination of a transform iterator and \texttt{reduction\_by\_key} function. The transform iterator generates the square of individual elements and feeds the result into the \texttt{reduction\_by\_key} function which in turn computes the square of the vector norms.  The next step is to find the distance. In case of the Euclidian distance,  for finding the  nearest vectors to  from  the squared distance formula is given by:
 
The comparison to find the  nearest neighbhors of  is between the numbers . Furthermore, all of the  have a common addition factor . Therefore, it is sufficient to compare the distance metric . This saves an addition operation. For finding the  nearest vectors to  from ,  we follow the same process but find the matrix of dot products  by simply taking a transpose of the matrix  which we computed earlier. The same analysis is valid for Cosine distances as well.\\

For finding the Pearson distance, we need to pre-process the raw data to center it. We use the \texttt{reduction\_by\_key} to find the vector means.  We then center each vector using one thread block per vector. Once the data is centered, the process of finding the distance metric is the same as that of the Cosine distance. \\

\section*{Results}
Our algorithms were written in NVIDIA's CUDA \cite{Sanders:2010} and executed on a Tesla C2050 compute card. We used gcc4.2 with appropriate optimization flags. This card has 448 serial processors and a peak single precision performance of 1.03 TFlops.  The RAM memory is 3GB with a global memory bandwidth of 144 GB/s. We used CUDA 4.2 drivers for our implementation. We performed a comprehensive set of tests against implementations by Garcia et al.\cite{Garcia2010}, and Sismanis et al.\cite{Sismanis:2012}. Both these algorithms are for -NN. We also benchmarked our algorithm against the  \texttt{nth\_element} from the standard template library and the GPU-based radix/bucket select implementation of -element algorithm by Alabi et al.  \cite{Alabi:2012}. The work by Alabi et al., as indicated earlier, is slightly different since they just return the  largest or smallest element and not -NNs. This means that they do not keep track of the -NN elements and therefore perform less work than our algorithm and the implementations by Garcia and Sismanis.  For accurate comparison, we downloaded the all GPU codes (Sismanis, Garcia, Alabi) and recompiled it using the same compiler and CUDA drivers as the ones we used to compile our code. The test data was a matrix (,  is the number of queries, and  is the size of the corpus data set) of uniformly random floating point numbers with indices. In some tests, indices were left out.\\

For large data sets, the code developed by Garcia et al. \cite{Garcia2010}., batches execution of multiple queries. For large , the size of the batches (i.e., the number of queries) can cause underutilization of GPU resources. We chose parameters in our data sets  in a way that the algorithm by Garcia is operating at its full potential. Figure 4 shows the results of the benchmarks where kept  constant and varied the parameters . We set the number of simultaneous queries . We varied the number of closest neighbours as . Also, we varied the size of the corpus data as . We only benchmarked the search part of the algorithm since the method for distance calculation is virtually identical between the two methods.  Figure 4 the performance advantage of our algorithm. There is dramatic gain in speedup when  grow. As   grows, Garcia's algorithm shifts it's -NN stack from shared memory to global memory and is saddled with all the associated pitfalls such as un-coalesced memory access.  In figure 5,  we illustrate the effect of changing the number of queries. In this example, we kept the size of the corpus at  and changed the number of queries as . Simultaneously, we changed the number of selections as . For small queries, Garcia's method underutilizes the GPU.  As we increase the number of queries, the GPU utilization increases and therefore, we see a decrease in speedup from the peak. For the largest sizes of  and  that fit in our GPU memory, our method is  faster. \\


For the benchmarks against work by Sismanis (Figure 6), we compared the selection method alone. We kept the product  constant, where  is the number data corpus elements (columns in the distance matrix) and  is the number of query elements (rows in the distance matrix). We plotted our speedup again the ratio .  We ran a thousand trials for each evaluation and averaged the results. As seen in the figure 6, the performance gains for various  rapidly rise with  , reaches a saturation level, and then falls for large values of  . The degradation with performance occurs at  . This is point corresponds to , i.e., where the number of thread blocks falls below . At this stage, our algorithm is not able to fully utilize GPU resources. We show in a subsequent test that our algorithm saturates the GPU only when the thread block count exceeds . The performance advantage grows with . We were able to benchmark only upto  as the Sismaniss implementation cannot handle .  With increased GPU RAM, we can handle larger distance matrices and therefore, have more queries  for large values of . For example, if we had a GPU with 18 GB of available RAM instead of the current 2GB, we would be easily able to handle distance matrices of sizes . This would push the point of performance degradation to a point given by .  Consequently we envision our algorithm to perform even better on fused GPU-CPU architectures which have unified memory access to main CPU RAM. Such processors will become mainstream in the next 2-3 years.\\

 
Our third set of benchmarks were against the radix/bucket select-based  statistic algorithm implemented by \cite{Alabi:2012}. This implementation takes one query at a time. Therefore, our comparison tests involved initially loading all the entire distance matrix into GPU RAM and then invoking the radix select algorithm one query at a time. This approach adds kernel launch overhead to the time for calculation. However, the typical kernel launch overhead for the GPU  in our tests is 5.7 micro seconds and therefore nearly two orders of magnitude smaller than the time required for task execution. Figure 7 shows the results of our benchmarks. For this battery of test, we kept the product  constant. Once again, we ran a thousand trials for each evaluation and averaged the results.  For low values of , our approach completely dominates. This is because in this regime, the implementation by \cite{Alabi:2012} underutilizes GPU resources. As the ratio  increases, our performance advantage decreases. At , our algorithm starts underutilizing GPU resources because the number of thread blocks in operation falls below . The method of \cite{Alabi:2012} achieves GPU saturation only at  (i.e., ). Even at this point, our algorithm is slightly faster (), even though it is underutilizing GPU resources and is performing significantly larger amount of work keeping track of -NNs instead of finding just the  statistic.\\

Figure 8 illustrates our tests to investigate GPU saturation. In these tests we kept  constant and varied , the number of queries from  to . For various values of  it can be seen that the time per query drops with the  and flattens out at . Since we are utilizing one thread block per query, we should have at least  queries to fully utilize GPU resources.  \\

Finally, we benchmarked our selection algorithm against the Standard Template Library \texttt{nth\_select}. Just as in \cite{Alabi:2012}, the \texttt{nth\_select} algorithm works on a single query. We compared our implementation with a single threaded \texttt{nth\_select} working one query at time. The data was loaded into memory ahead of time and queries were processed in a \texttt{for} loop.  Figure 9 shows the results of the benchmark. In this test we varied the parameter  as  and the parameter  as  .  The speedup increases with  and as can be seen in the figure does not reach saturation even at . There is slight degradation with increase in  which is obvious since the larger  entails keep track of a larger list of elements. In our tests, we achieved around  speedup for the largest  with the largest data sets that could be accommodated in our GPU RAM. 

\section*{Discussion} We have successfully developed and implemented a multi-query -NN algorithm on the GPU. Our algorithm is an implementation of the Quick Select method and uses the architectural advantages of modern GPUs. For corpus data that fit in GPU memory, based on our benchmarks, our algorithm outperforms all current state-of-the art methods. There are however many limitations to our method. To achieve full GPU saturation, we cannot have the number of queries ,i.e., we need to have at least 128 thread blocks running simultaneously. Given the roughly 2GB available memory, this restricts the number of elements in the corpus data to .  With the newer fused GPU-CPU processors, with unified RAM (upto 64 GB), we will be able to handle much larger data sets.  We are also looking at batch execution with data partitioning for large corpus data that do not fit into GPU memory. We may be able to overlap computation with data transfer to and from the CPU RAM to reduce overall execution time.  Batch execution will obviously require merging of results between executions and additional memory to store intermediate results. We believe there will significant opportunities for optimization based on the number of queries, corpus data size, and the dimension of vectors in the corpus data. This research will be explored in subsequent publications.

\section*{Figures}
\noindent {\bf  Figure 1: Read in process.}  Read in of the array is done incrementally in sets of 32 elements. As illustrated the memory access is coalesced. The value if stored in a register. Simultaneously, the invocation of the warp voting function fills the bit array  based on the evaluation of the predicate which indicates if value in the register is greater than equal or less than the pivot.

\noindent{\bf Figure 2: Pivot process.} The pivot process is accomplished in shared memory. Each thread determines where in the shared memory the value has to be written. Values less than the pivot are accumulated on the left hand side and values greater than or equal to the pivot are accumulated on the right hand side. Since all threads write to different locations, there are no bank conflicts.

\noindent{\bf Figure 3: Write out process:} The thread id indicates (based on the computation \texttt{\_popc(B)} whether a given thread is writing out an element less than the pivot or greater than or equal to the pivot. The values  and  which are maintained in shared memory and updated incrementally indicate the location in the global array the location of the last element that is less than the pivot and greater than or equal to the pivot. This operation involves at most two coalesced memory writes.

\noindent{\bf Figure 4: Benchmark against GPU -NN by Garcia  for selection alone with varying size of corpus data ()\cite{Garcia2010}.}  We set the number of queries . This ensured full GPU utilization. The parameter  was varied from . The number of nearest neighbors  was varied as . Except for , the speedup remains flat w.r.t. . The dramatic gain in performance with  is because the Garcia algorithm shifts its -NN stack from shared memory to global memory as  increases. This increases inefficiencies due to uncoalesced memory access. We achieved a speedup  for  and .

\noindent{\bf Figure 5: Benchmark of selection alone against GPU -NN by Garcia for different query sizes ().} We set . We varied the number of queries as . Simultaneously, we varied the number of nearest neighbors as .  Note that speedup falls as the number of queries  increases. This is because the Garcia algorithm increases GPU utilization as the number of queries is increased. For the largest data sets that fit into GPU memory, corresponding to the level of full GPU utilization by Garcia () and for the largest number of nearest neighbors (), our algorithm is  faster.

\noindent{\bf Figure 6: Benchmark against work by Sismanis et al. \cite{Sismanis:2012}}. These tests were performed for elements that contained both values and indices. In this benchmark we varied  as . We kept the product . We simultaneously varied  as . Performance peaks at   stays flat till   and starts falling at  . The decrease in performance corresponds to the number of queries  falling below 128 where our method underutilizes GPU resources. 

\noindent{\bf Figure 7: Benchmark against work by Alabi et al. \cite{Alabi:2012}}. Here the benchmark is comparing the speedup of selecting  element vs selection of -NN using our algorithm. These tests were performed for elements with values alone. We kept the product . We simultaneously varied  as . Simultaneously, we varied  as . There is a dramatic fall in performance gain because the method by Alabi increases GPU utilization as  increases. It reaches saturation at   \cite{Alabi:2012}. Our algorithm starts underutilizing GPU resources at . Even when Alabi et al. is saturated and our algorithm is significantly underutilizing GPU resources, we are .  Once again, with a larger GPU RAM, our algorithm would perform significantly better.

\noindent{\bf Figure 8: Effect of varying the number of queries }. In this benchmark we kept  and varied the number of queries as .  As can be seen from the graph for various values of , the time per query decreases with increase in  and roughly flattens our at .


\noindent{\bf Figure 9: Benchmark against  element}.  This benchmark is against a single core of the CPU. As can be seen, the performance degrades slightly with larger values of . The number of queries were set to . We ran up to . We achieve around  speedup. As can be seen from the figure, the speedup graph has not saturated for the largest values of  that can be accommodated on our GPU. 

\newpage
\begin{figure}[!h]
\label{readin}
\begin{center}
\includegraphics[scale=0.5]{Figure1}
\end{center}
\caption{}
\end{figure}

\newpage
\begin{figure}[!h]
\label{pivot}
\begin{center}
\includegraphics[scale=0.5]{Figure2}
\end{center}
\caption{}
\end{figure}



\newpage
\begin{figure}[!h]
\label{writeout}
\begin{center}
\includegraphics[scale=0.5]{Figure3}
\end{center}
\caption{}
\end{figure}


\newpage
\begin{figure}[!h]
\label{garcia}
\begin{center}
\includegraphics[scale=0.6]{Figure4}\\
\end{center}
\caption{}
\end{figure}

\newpage
\begin{figure}[!h]
\label{MGPU}
\begin{center}
\includegraphics[scale=0.6]{Figure5}
\end{center}
\caption{}
\end{figure}

\newpage
\begin{figure}[!h]
\label{MGPU}
\begin{center}
\includegraphics[scale=0.6]{Figure6}
\end{center}
\caption{}
\end{figure}

\newpage
\begin{figure}[!h]
\label{MGPU}
\begin{center}
\includegraphics[scale=0.6]{Figure7}
\end{center}
\caption{}
\end{figure}

\newpage
\begin{figure}[!h]
\label{MGPU}
\begin{center}
\includegraphics[scale=0.6]{Figure8}
\end{center}
\caption{}
\end{figure}

\newpage
\begin{figure}[!h]
\label{MGPU}
\begin{center}
\includegraphics[scale=0.6]{Figure9}
\end{center}
\caption{}
\end{figure}

\bibliographystyle{plos2009}
\bibliography{GPU-kNN.bib} 




 
\end{document}
