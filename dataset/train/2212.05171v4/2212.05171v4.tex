

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[accsupp]{axessibility}  

\usepackage{cvpr}      

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{url}
\usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage[accsupp]{axessibility}  

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\newcommand{\jw}[1]{\textcolor{Bittersweet}{[Jiajun\@: #1]}}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{4814} \def\confName{CVPR}
\def\confYear{2023}

\definecolor{DarkGreen}{rgb}{0.0, 0.5, 0.0}


\begin{document}

\title{ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding}
\author{Le Xue\thanks{\ Contact: lxue@salesforce.com},
Mingfei Gao,
Chen Xing,
Roberto Martín-Martín,
Jiajun Wu,
Caiming Xiong,
\\
Ran Xu,
Juan Carlos Niebles, and
Silvio Savarese
\\
\\
 Salesforce AI, Palo Alto, USA \\
 Stanford University, Stanford, USA \quad  UT Austin, Texas, USA \\
{\tt\small Project Website: \href{https://tycho-xue.github.io/ULIP/}{https://tycho-xue.github.io/ULIP/}}}

\maketitle

\begin{abstract}
The recognition capabilities of current state-of-the-art 3D models are limited by datasets with a small number of annotated data and a pre-defined set of categories.
In its 2D counterpart, recent advances have shown that similar problems can be significantly alleviated by employing knowledge from other modalities, such as language.
Inspired by this, leveraging multimodal information for 3D modality could be promising to improve 3D understanding under the restricted data regime, but this line of research is not well studied.
Therefore, we introduce ULIP to learn a unified representation of images, texts, and 3D point clouds by pre-training with object triplets from the three modalities.
To overcome the shortage of training triplets, ULIP leverages a pre-trained vision-language model that has already learned a common visual and textual space by training with massive image-text pairs. Then, ULIP learns a 3D representation space aligned with the common image-text space, using a small number of automatically synthesized triplets.
ULIP is agnostic to 3D backbone networks and can easily be integrated into any 3D architecture.
Experiments show that ULIP effectively improves the performance of multiple recent 3D backbones by simply pre-training them on ShapeNet55 using our framework, achieving state-of-the-art performance in both standard 3D classification and zero-shot 3D classification on ModelNet40 and ScanObjectNN. ULIP also improves the performance of PointMLP by around 3\% in 3D classification on ScanObjectNN, and outperforms PointCLIP by 28.8\% on top-1 accuracy for zero-shot 3D classification on ModelNet40. Our code and pre-trained models are released at \href{https://github.com/salesforce/ULIP}{https://github.com/salesforce/ULIP}.
\end{abstract}




\section{Introduction}
\label{sec:intro}



\begin{figure}[hbt]
    \centering
    \includegraphics[width=1.0\linewidth]{figure1_san.pdf}
    \caption{Illustration of ULIP. ULIP improves 3D understanding by aligning features from images, texts, and point clouds in the same space. To reduce the demand of 3D data, ULIP leverages image and text encoders that are pre-trained with large-scale image-text pairs, and aligns 3D representation to the pre-aligned image-text feature space using a small scale of training triplets. }
    \label{fig:alignment}
\end{figure}

3D visual understanding research~\cite{landrieu2018large, hu2020randla, liu2019densepoint, li2021lidar, graham20183d, choy20163d} is drawing significant attention in recent years due to the increasing demand of real-world applications such as augmented/virtual reality~\cite{vu2022softgroup, liu2021group, misra2021end, armeni20163d}, autonomous driving~\cite{yin2021center, li2022deepfusion} and robotics~\cite{wojek2011monocular, cadena2016multi}.
However, compared to their 2D counterpart, 3D visual recognition research is still limited by datasets with a small number of samples and a small set of pre-determined categories~\cite{Uy_2019_ICCV, wu20153d}.
For example, ShapeNet55~\cite{chang2015shapenet}, one of the largest publicly available 3D datasets, only contains around 52.5k samples of 3D objects with 55 category labels. 
That is in contrast to the 2D domain, where ImageNet~\cite{deng2009imagenet} contains millions of images that cover thousands of categories.
This scale limit of 3D data, caused by the high cost of 3D data collection and annotation~\cite{yu2022point, chang2015shapenet, goyal2021revisiting, wu20153d}, has been hindering the generalization of 3D recognition models and their real-world applications.  


To tackle the shortage of annotated data,  existing work in other domains shows that employing knowledge from different modalities can significantly help the concept understanding in the original modality\cite{radford2021learning,xing2019adaptive}. 
Among such work, CLIP~\cite{radford2021learning} pioneered alignment between visual and textual features by pre-training on large-scale image-text pairs. It improves state-of-the-art visual concept recognition and enables zero-shot classification of unseen objects.
However, multimodal learning that involves 3D modality, and whether it can help 3D recognition tasks are still not well studied.

In this paper, we propose Learning a \textbf{U}nified Representation of \textbf{L}anguage, \textbf{I}mages, and \textbf{P}oint Clouds (ULIP). 
An illustration of our framework is shown in Figure \ref{fig:alignment}. 
Obtaining a unified representation space of all three modalities requires large-scale triplets of image, text, and point cloud as training data.
However, such triplets remain hard to collect compared to the large-scale image-text pairs available.
To circumvent the lack of triplet data, we take advantage of a vision-language model pretrained on massive image-text pairs, and align the feature space of a 3D point cloud encoder to the pre-aligned vision/language feature space. 
When training the 3D encoder for space alignments, we use a small number of automatically synthesized triplets from ShapeNet55~\cite{chang2015shapenet} without requiring manual annotations.
Making use of a pretrained vision-language model lets us leverage the abundant semantics captured in the image-text feature space for 3D understanding.
Our framework uses CLIP as the vision and language model because of its excellent generalization performance. During pre-training, we keep the CLIP model frozen and train the 3D encoder by aligning the 3D feature of an object with its corresponding textual and visual features from CLIP using contrastive learning.
The pre-trained 3D backbone model can be further fine-tuned for different downstream tasks.




ULIP has three major advantages. First, ULIP can substantially improve the recognition ability of 3D backbone models. Second, ULIP is agnostic to the architecture of 3D models; therefore, we can easily plug in any 3D backbones and improve them with ULIP. Third, aligning three modalities in the same feature space can potentially enable more cross-domain downstream tasks, including zero-shot 3D classification and image-to-3D retrieval.

We quantitatively evaluate ULIP on two fundamental 3D tasks: standard 3D classification and zero-shot 3D classification. We experiment with recent 3D networks including PointNet++ ~\cite{qi2017pointnet++}, PointMLP~\cite{ma2022rethinking} and PointBERT~\cite{yu2022point}. 
Experimental results show that ULIP achieves state-of-the-art (SOTA) performance for both standard 3D classification and zero-shot 3D classification on ModelNet40 and ScanObjectNN. Specifically, ULIP surpasses PointMLP by around 3\% in standard 3D classification on ScanObjectNN~\cite{Uy_2019_ICCV}. ULIP also outperforms PointCLIP~\cite{zhang2022pointclip} (the previous SOTA) by around 28.8\% top-1 accuracy in zero-shot 3D classification on ModelNet40. Moreover, we showcase the potential of applying ULIP on the image to point cloud retrieval task. Qualitative evaluation demonstrate our promising potential for cross-modal applications.






\section{Related Work}
\label{sec:related work}
\noindent\textbf{Multi-modal Representation Learning.}
Most existing multimodal approaches are about image and text modalities. 
Among these methods, one line of research focuses on learning interaction between image regions and caption words \cite{tan2019lxmert,chen2020uniter,li2020oscar,lu2019vilbert,li2019visualbert,li2021align} using transformer-based architectures. These methods show great predictive capability while being costly to train. 
The other line of research, such as CLIP \cite{radford2021learning}, uses image and text encoders to output a single image/text representation for each image-text pair, and then aligns the representations from both modalities. This simple architecture makes training with massive noisy web data efficient, facilitating its zero-shot generalization capability.

The success of CLIP has promoted many image-text related research directions, including text-based image manipulation \cite{patashnik2021styleclip}, open vocabulary object detection \cite{gu2021open,gao2021towards} and language grounding \cite{li2022grounded}. Some recent works explore how multi-modal information can help 3D understanding and show promising results \cite{yan2022let, chen2021multimodal}. The most related method to our work is PointCLIP \cite{zhang2022pointclip}. It first converts the 3D point cloud into a set of depth maps and then leverages CLIP directly for zero-shot 3D classification. Unlike PointCLIP, which targets reshaping the task of point cloud and text matching to image and text alignment, our method learns a unified representation among images, texts, and point clouds that substantially improves 3D understanding.

\noindent\textbf{3D Point Cloud Understanding.}
There are mainly two streams of research lines for point cloud modeling. One is projecting a point cloud into 3D voxels \cite{maturana2015voxnet,shi2020pv} and then using 2D/3D convolutions for feature extraction. PointNet \cite{qi2017pointnet} explores ingesting 3D point clouds directly. It extracts permutation-invariant feature from the point cloud that significantly impacts point-based 3D networks. PointNet++ \cite{qi2017pointnet++} proposes a hierarchical neural network that extracts local features with increasing contextual scales. Recently, PointMLP \cite{ma2022rethinking} proposes a pure residual MLP network and achieves competitive results without integrating sophisticated local geometrical extractors. 
Moreover, self-supervised learning for 3D point clouds has also shown promising performance in 3D understanding field. PointBERT \cite{yu2022point} adopts mask language modeling from BERT \cite{devlin2018bert} to the 3D field, where it tokenizes 3D patches using an external model, randomly masks out 3D tokens, and predicts them back during pre-training. A more recent work, PointMAE \cite{pang2022masked}, directly operates the point cloud by masking out 3D patches and predicting them back using L2 loss. Our method is orthogonal to the above 3D encoders. Their performance on 3D recognition can be potentially improved by ULIP with no/minor modification.

\section{Learning a Unified Representation of Language,
Images, and Point Clouds}
\label{sec:method}


ULIP learns a unified representation space of language, images, and 3D point clouds via pre-training on triplets from these three modalities. 
In this section, we first introduce how we create such triplets for pre-training. Then, we present our pre-training framework.

\subsection{Creating Training Triplets for ULIP}
\label{sec:create triplet}


We build our dataset of triplets from ShapeNet55~\cite{chang2015shapenet}, which is one of the most extensive public 3D CAD datasets. 
ShapeNet55 is the publicly-available subset of ShapeNet. 
It contains around 52.5K CAD models, each of which is associated with metadata that textually describes the semantic information of the CAD model. 
For each CAD model  in the dataset, we create a triplet  of image , text description  and point cloud . ULIP will then use these triplets for pre-training. 

\noindent\textbf{Point Cloud Generation}. We directly use the generated point cloud of each CAD model in ShapeNet55. We uniformly sample  points from the original point cloud. During pre-training, standard data augmentation techniques of 3D point clouds are performed, including random point drop, random scaling point cloud, shift point cloud and rotate perturbation.
Then a 3D encoder takes the augmented point cloud  as input and outputs its 3D representation  via

where  represents the 3D backbone encoder.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figure2_san.pdf}
\caption{Illustration of our method. The inputs of multimodal pre-training (\textbf{Left}) are a batch of objects represented as triplets (image, text, point cloud). Image and text features are extracted from a pre-trained (frozen) vision and language model such as CLIP, and 3D features are extracted from a 3D encoder. Contrastive losses are applied to align the 3D feature of an object to its image and text features during pre-training. The pre-trained 3D encoders are further fine-tuned in downstream tasks, including standard 3D classification (\textbf{Top Right}) and zero-shot 3D classification (\textbf{Bottom Right}).}
    \label{fig:method}
\end{figure*}

\noindent\textbf{Multi-view Image Rendering}. ShapeNet55 CAD models do not come with images. 
To obtain images that semantically align well with each CAD model, we synthesize multi-view images of each CAD model by placing virtual cameras around each object and rendering the corresponding RGB images and depth maps from each viewpoint.\footnote{We utilize the following repository with their default settings in practice. \newline https://github.com/panmari/stanford-shapenet-renderer} Specifically, we render an RGB image with a depth map for every 12 degrees. Therefore, we get 30 RGB images and 30 depth maps for each object, 60 image candidates in total. During each iteration of pre-training, we randomly select one image or depth map from each CAD model's 60 renderred candidates as  and take  as input of the image encoder  to extract the image feature ,




\noindent\textbf{Text Generation}. We leverage the metadata that comes with each CAD model as the corresponding text description. The metadata includes a synset of taxonomy as a textual description of each CAD model. For each word in the metadata, we adopt simple prompts to construct meaningful sentences that will be utilized during pre-training. We follow prior works~\cite{gu2021open,gao2021towards} that use 63 prompts such as "a picture of [WORD]" in image-text pre-training tasks and additionally add a dedicated prompt ``a point cloud model of [WORD]" to accommodate the 3D modality. 
In each training iteration, we randomly choose a word from the metadata and apply the 64 templates on the word to build a set of text descriptions, .
Then we input  into our text encoder  and get a set of representations, respectively. Finally, we conduct average pooling over the set of outputs as the text-domain representation  of object ,



\subsection{Aligning Representations of Three Modalities}
\label{sec:align representation}
With the created triplets of image, text, and point cloud, ULIP conducts pre-training to align representations of all three modalities into the same feature space.
Specifically, we take advantage of pre-trained vision-language models, i.e., CLIP, and train a 3D encoder by aligning the 3D feature with the features of image and text encoders ( and  ) of CLIP.
By doing so, we hope that the abundant semantics already captured and aligned by CLIP's encoders can be employed for better 3D understanding.
The resulting unified feature space enables numerous cross-modal applications among these three modalities and potentially improves the 3D recognition performance of the underlying 3D backbone encoder .




\noindent\textbf{Cross-modal Contrastive Learning.}
As shown in Figure \ref{fig:method}, for an object , features ,  and  are extracted from image, text, and 3D point cloud encoders. Then contrastive loss among each pair of modalities is computed as follows, 

where  and  represent two modalities and  indicates a positive pair in each training batch. We use a learnable temperature parameter  as well, similar to CLIP \cite{radford2021learning}.

Finally, we minimize  for all modality pairs with different coefficients, 

By default,  is set to be constant 0,  and  are set to be 1 equally; because during pre-training, we find that if we update CLIP's image and text encoders, catastrophic forgetting will emerge due to our limited data size. This will lead to a significant performance drop when applying ULIP to downstream tasks. Therefore we freeze the weights of  and  during the entire pre-training and only update  with .


\section{Experiments}
\label{sec:experiments}
To demonstrate the benefits of pre-training 3D backbone networks using ULIP, we conduct experiments on two 3D tasks: a standard 3D classification task that involves a single modality and a zero-shot 3D classification task that involves multimodal inputs.
In this section, we first present experimental settings, including our experimenting 3D backbones, downstream datasets, and implementation details.
Then we present the quantitative results of standard 3D classification and zero-shot 3D classification, respectively. Lastly, we include analyses of our model and show results on cross-modal retrieval.

\subsection{3D Backbone Networks}
We experiment with the following 3D backbone networks under our framework.

\noindent\textbf{PointNet++} \cite{qi2017pointnet++} is an advanced version of PointNet \cite{qi2017pointnet}. It uses a hierarchical structure to better capture the local geometry of the point cloud, and becomes the cornerstone of many point cloud applications.


\noindent\textbf{PointBERT}~\cite{yu2022point} utilizes a transformer architecture for point cloud feature extraction. It improves its recognition ability by conducting self-supervised pre-training on ShapeNet55.

\noindent\textbf{PointMLP}~\cite{ma2022rethinking} is the SOTA method on standard 3D classification task. It uses a residual MLP network with a lightweight geometric affine module to better capture local geometric features.

\begin{table}[htb]
    \small
    \centering
    \begin{tabular}{lcc}
    \toprule
         Model& Overall Acc & Class-mean Acc \\
         \midrule
         PointNet \cite{qi2017pointnet} &  68.2 & 63.4 \\
PointNet++ \cite{qi2017pointnet++} &  77.9 & 75.4 \\
DGCNN \cite{wu2018dgcnn} &  78.1 & 73.6 \\
MVTN \cite{hamdi2021mvtn} &  82.8 &  --\\
PointBERT \cite{yu2022point} &  83.1 &  --\\
RepSurf-U \cite{ran2022surface} & 84.6 &  --\\
PointMAE \cite{pang2022masked} & 85.2 &  --\\
RepSurf-U (2x) \cite{ran2022surface} &  86.0 &  --\\
         \midrule
PointBERT \cite{yu2022point} &  83.1 &  --\\
PointBERT + ULIP &  \textbf{86.4} \textcolor{DarkGreen}{\small ()} & --\\
\midrule
PointMLP \cite{ma2022rethinking} &  85.7 & 84.4 \\
PointMLP+ ULIP &  \textbf{88.8} \textcolor{DarkGreen}{\small ()} & \textbf{87.8} \textcolor{DarkGreen}{\small ()} \\
\midrule
         PointMLP \dag &  86.5 & 85.1 \\
PointMLP \dag+ ULIP &  \textbf{89.4} \textcolor{DarkGreen}{\small ()} & \textbf{88.5} \textcolor{DarkGreen}{\small ()} \\
\bottomrule
    \end{tabular}
    \caption{3D classification results on ScanObjectNN. ULIP significantly improves our baselines. Our best result outperforms SOTA largely by around 3\% on Overall Acc. \dag  indicates a model uses 2K sampled points and all others use 1K sampled points.}
    \label{tab:fintune-scan}
    \vspace{-4mm}
\end{table}


\subsection{Downstream Datasets}
We use the following two datasets for both standard and zero-shot 3D classification.

\noindent\textbf{ModelNet40} is a synthetic dataset of 3D CAD models. It contains 9,843 training samples and 2,468 testing samples, covering 40 categories.\footnote{For each CAD model, we utilized preprocessed point cloud from \cite{ma2022rethinking}.}

\noindent\textbf{ScanObjectNN} is a dataset of scanned 3D objects from the real world.
It contains 2,902 objects that are categorized into 15 categories. It has three variants: \emph{OBJ\_ONLY} includes ground truth segmented objects extracted from the scene meshes datasets; \emph{OBJ\_BJ} has objects attached with background noises and \emph{Hardest} introduces perturbations such as translation, rotation, and scaling to the dataset\cite{Uy_2019_ICCV}.\footnote{We used the variants provided by \cite{yu2022point} in our experiments.}

\subsection{Implementation Details}

\paragraph{Pre-training.} For the 3D input, we uniformly sample  1024, 2048, or 8192 points for accommodating the requirements of different backbones. The inputs of image and text modalities are generated as described in Section \ref{sec:create triplet}. During pre-training, we utilize an advanced version of CLIP, namely SLIP~\cite{mu2022slip}, that shows superior performance as our image-text encoders. 
As mentioned in Section \ref{sec:align representation}, we freeze the image and text encoders and only update the 3D encoder's parameters during pre-training. 
ULIP is trained for 250 epochs.
We use  as the batch size,  as the learning rate, and AdamW as the optimizer. 

\vspace{-10pt}
\paragraph{Standard 3D Classification.} On ModelNet40, we use the learning rate as 0.00015 and fine-tune our model for 200 epochs, with the batch size as 24 for PointNet++. 
For PointMLP, we set the learning rate as 0.1 and fine-tune the model for 300 epochs, with the batch size as 32. 


On ScanObjectNN, we use the learning rate of 0.03 and finetune for 350 epochs with batch size 32 for PointMLP. 
For PointBERT, we use the learning rate of 0.0002 and finetune for 300 epochs with batch size 32.

\vspace{-10pt}
\paragraph{Zero-Shot 3D Classification.} Following \cite{zhang2022pointclip}, zero-shot 3D classification is conducted by measuring distances between the 3D features of an object and the text features of category candidates. The category that introduces the smallest distance is selected as the predicted category, as shown in Figure \ref{fig:method}. We use our pre-trained models as they are when performing zero-shot classification. There is no finetuning stage involved. We keep using the same prompt strategy as it is during pre-training when constructing text features for each category candidate in this task.

All of our experiments are conducted using PyTorch. Pre-training and finetuning experiments use 8 and 1 A100 GPUs, respectively.

\subsection{Standard 3D Classification}
We demonstrate the effectiveness of ULIP by improving different 3D classification baselines. We follow the original settings of the baselines in our experiments. When applying ULIP, the only difference is that we pre-train the 3D networks under our framework before finetuning them with the labeled point cloud. Since the structure of the 3D backbone is unchanged, our framework does not introduce extra latency during inference time.
For all experiments, we follow the community practice of using OA (Overall Accuracy) and mAcc (Class Average Accuracy) as our evaluation metrics.







\begin{table}[htb]
    \small
    \centering
    \begin{tabular}{lcc}
    \toprule
         Model  &  Overall Acc & Class-mean Acc\\
         \midrule
         PointNet \cite{qi2017pointnet}  & 89.2 & 86.0 \\
PointCNN \cite{li2018pointcnn}  & 92.2 & -\\
SpiderCNN \cite{xu2018spidercnn} & 92.4 & -\\
PointConv \cite{wu2019pointconv} & 92.5 & -\\
Point Transformer \cite{zhao2021point} & 92.8 & -\\
KPConv \cite{thomas2019kpconv} & 92.9 & -\\
DGCNN \cite{wang2019dynamic}  & 92.9 & 90.2 \\
PCT \cite{guo2021pct}  & 93.2 & -\\
RS-CNN* \cite{liu2019relation}  & 93.6 & -\\
GDANet \cite{xu2021learning}  & 93.8 & -\\
GBNet \cite{qiu2021geometric}  & 93.8 & 91.0 \\
MTVN \cite{hamdi2021mvtn}  & 93.8 & 92.0 \\
RPNet \cite{ran2021learning} & 94.1 & -\\
CurveNet \cite{xiang2021walk} & 94.2 &- \\
\midrule
         PointNet++(ssg) & 90.7 & -\\
PointNet++(ssg) + ULIP & \textbf{93.4} \textcolor{DarkGreen}{\small ()} & 91.2 \\
\midrule
         PointBERT  & 93.2 &  -\\
PointBERT + ULIP & \textbf{94.1} \textcolor{DarkGreen}{\small ()} & - \\
\midrule
         PointMLP  & 94.1 & 91.3 \\
PointMLP + ULIP  &  \textbf{94.3} \textcolor{DarkGreen}{\small ()} & \textbf{92.3} \textcolor{DarkGreen}{\small ()} \\
\midrule
         PointMLP*  &  94.5 & 91.4 \\
PointMLP* + ULIP  &  \textbf{94.7} \textcolor{DarkGreen}{\small ()} & \textbf{92.4} \textcolor{DarkGreen}{\small ()}\\
\bottomrule
\end{tabular}
    \caption{Standard 3D classification results on ModelNet40. ULIP significantly improves our baselines. Our best number achieves new SOTA. * means a voting technique is applied to the method to boost performance.}
    \label{tab:fintune-modelnet}
    \vspace{-4mm}
\end{table}

\begin{table*}[htbp]
    \small
    \vspace{-4mm}
    \centering
    \begin{tabular}{lccccccc}
         \toprule
         \multirow{2}*{Model} & \multirow{2}*{Modalitiy aligned} & \multicolumn{2}{c}{ALL} & \multicolumn{2}{c}{Medium} & \multicolumn{2}{c}{Hard}
         \\
         \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
         ~ & ~ & top1 & top5 & top1 & top5 & top1 & top5
         \\
         \midrule
         PointNet++ ssg + ULIP & P+T & 44.9 & 70.3 & 17.2 & 55.0 & 20.3 & 50.1\\
PointNet++ ssg + ULIP & P+I & 35.3 & 67.2 & 33.6 & 62.4 & 30.1 & 55.1\\
PointNet++ ssg + ULIP & P+I+T & \textbf{55.7} & \textbf{75.7} & \textbf{35.6} & \textbf{64.4} & \textbf{33.7} & \textbf{55.8}\\
\midrule
         PointNet++ msg + ULIP & P+T & 48.0 & 63.8 & 17.8 & 42.3 & 21.3 & 40.7\\
PointNet++ msg + ULIP & P+I & 36.4 & 64.4 & 34.7 & 59.0 & 31.0 & 52.0\\
PointNet++ msg + ULIP & P+I+T & \textbf{58.4} & \textbf{78.2} & \textbf{36.9} & \textbf{67.2} & \textbf{33.9} & \textbf{59.6}\\
         \midrule
PointMLP + ULIP & P+T & 52.2 & 73.0 & 23.3 & 60.8 & 18.1 & 52.2\\
PointMLP + ULIP & P+I & 34.6 & 64.3 & 31.3 & 61.7 & 27.0 & 53.7\\
PointMLP + ULIP & P+I+T & \textbf{61.5} & \textbf{80.7} & \textbf{43.2} & \textbf{72.0} & \textbf{36.3} & \textbf{65.0}\\
         \midrule
PointBERT + ULIP & P+T & 44.7 & 66.0 & 19.4 & 49.3 & 14.7 & 39.3\\
PointBERT + ULIP & P+I & 35.5 & 66.9 & 35.0 & 64.4 & 34.1 & 59.1\\
PointBERT + ULIP & P+I+T & \textbf{60.4} & \textbf{84.0} & \textbf{40.4} & \textbf{72.1} & \textbf{37.1} & \textbf{66.3}\\
         \bottomrule
    \end{tabular}
    \caption{Analysis of aligning three vs. two modalities on zero-shot 3D classification on ModelNet40. Results show that aligning representations of three modalities always produces better results than two modalities.}
    \label{tab:ablation-modelnet}
\end{table*}


\begin{table*}[htb]
    \small
\centering
    \begin{tabular}{lcccccc}
         \toprule
         \multirow{2}*{Model}  & \multicolumn{2}{c}{ ALL} & \multicolumn{2}{c}{Medium} & \multicolumn{2}{c}{Hard}
         \\
         \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
         ~  & top-1 & top5 & top-1 & top-5 & top-1 & top-5
         \\
         \midrule
PointCLIP  & 20.2 & -- & 10.4 & --& 8.3 &-- \\
\midrule
         PointNet++(ssg) + ULIP& 55.7 & 75.7 & 35.6 & 64.4 & 33.7 & 55.8\\
PointNet++(msg) + ULIP & 58.4 & 78.2 & 36.9 & 67.2 & 33.9 & 59.6\\
PointMLP + ULIP & 61.5 & 80.7 & 43.2 & 72.0 & 36.3 & 65.0\\
PointBERT + ULIP& \textbf{60.4} \textcolor{DarkGreen}{\small ()} & 84.0 & \textbf{40.4} \textcolor{DarkGreen}{\small ()} & 72.1 & \textbf{37.1} \textcolor{DarkGreen}{\small ()} & 66.3\\
         \bottomrule
    \end{tabular}
    \caption{Zero-shot 3D classification on ModelNet40. ULIP-based methods outperform the previous SOTA (PointCLIP) by a very large margin in different evaluation sets.}
    \label{tab:zero-shot-modelnet}
\end{table*}

\begin{table}[htb]
    \small
    \centering
    \begin{tabular}{lcc}
         \toprule
         \multirow{2}*{Model}  & \multicolumn{2}{c}{ALL}
         \\
         \cmidrule{2-3}
         ~  & top-1 & top-5 
         \\
         \midrule
         PointCLIP & 15.4&-- \\
\midrule
         PointMLP + ULIP &  44.6 & 82.3\\
         PointNet++(ssg)  + ULIP& 45.6 & 73.8\\
         PointBERT + ULIP  & 48.5 & 79.9\\
PointNet++(msg) + ULIP & \textbf{49.9} \textcolor{DarkGreen}{\small ()} & 78.8\\




         \bottomrule
    \end{tabular}
    \caption{Zero-shot 3D classification on ScanObjectNN. ULIP-based methods outperform the previous SOTA (PointCLIP) by a very large margin (at least 29.2\% on top-1 accuracy).}\label{tab:zero-shot-scan}
    \vspace{-4mm}
\end{table}






\vspace{-10pt}
\paragraph{Experimental Results.}
We present the standard 3D classification performances of our baselines and our methods on ScanObjectNN in Table~\ref{tab:fintune-scan}. As shown, the performances of our baselines are significantly improved by ULIP. Specifically, our framework improves PointBERT and PointMLP significantly by around 3\%.
When we apply ULIP on the strongest backbone, PointMLP, ULIP+PointMLP achieves the new SOTA performance, and outperforms previous SOTA, RepSurf-U(2), by 3.4\% Overall Accuracy. 

In Table~\ref{tab:fintune-modelnet}, we show the standard 3D classification results on ModelNet40.
Unlike ScanObjectNN, which contains scans of real objects, ModelNet40 is a synthetic dataset thus it is easier for classification. The Overall Accuracy of recent methods is already saturated around  on this dataset.
Even in such a scenario, from Table~\ref{tab:fintune-modelnet} we can see that ULIP is still able to improve the Overall Accuracy of all of our baselines. Among them, ULIP+PointMLP* achieves a new SOTA.
For the class-mean accuracy metric, we also observe decent performance improvement when using ULIP and achieves a new SOTA as well.


\subsection{Zero-Shot 3D Classification}
By aligning the 3D representation with text and image representations, ULIP also enables the 3D backbone networks to conduct tasks involving multiple modalities. We evaluate zero-shot 3D classification in this section.

PointCLIP is the first work and the current SOTA for zero-shot 3D classification. It conducts zero-shot 3D classification by first converting a 3D point cloud into 6 orthogonal depth maps, then using CLIP's image encoder to get ensembled depth map features, and finally using CLIP to match text and depth map features for zero-shot classification. We use it as our major baseline and follow its evaluation protocol in this task. For all experiments, we report top-1 and top-5 OA (Overall Accuracy).

\begin{table}[htb]
    \small
    \centering
    \begin{tabular}{lccc}
         \toprule
         \multirow{2}*{Model} & \multirow{2}*{Modality} & \multicolumn{2}{c}{ALL}
         \\
         \cmidrule{3-4}
         ~ & ~ & top-1 & top-5 
         \\
         \midrule PointNet++ ssg + ULIP & P+T & 33.4 & 73.0 \\
PointNet++ ssg + ULIP & P+I & 35.3 & 72.8\\
PointNet++ ssg + ULIP & P+I+T & \textbf{45.6} & \textbf{73.8}\\
\midrule
         PointNet++ msg + ULIP & P+T & 39.2 & 70.4\\
PointNet++ msg + ULIP & P+I & 34.9 & 71.3 \\
PointNet++ msg + ULIP & P+I+T & \textbf{49.9} & \textbf{78.8}\\
\midrule
         PointMLP + ULIP & P+T & 41.3 & 76.1 \\
PointMLP + ULIP & P+I & 33.6 & 74.7 \\
PointMLP + ULIP & P+I+T & \textbf{44.6} & \textbf{82.3}\\
\midrule
         PointBERT + ULIP & P+T & 31.0 & 69.0\\
PointBERT + ULIP & P+I & 36.3 & 71.3\\
PointBERT + ULIP & P+I+T & \textbf{48.5} & \textbf{79.9}\\
         \bottomrule
    \end{tabular}
    \caption{Analysis of aligning three vs. two modalities on zero-shot 3D classification on ScanObjectNN. Results show that aligning representations of three modalities always produces better results than two modalities.}
\label{tab:ablation-scan}
    \vspace{-4mm}
\end{table}


\vspace{-10pt}
\paragraph{Evaluation Sets.}


To perform a fair comparison with PointCLIP, we evaluate zero-shot 3D classification on the entire test sets of both ModelNet40 and ScanObjectNN. We refer to this set as \emph{ALL}.

Besides, we notice that there are some common classes between our pre-train dataset, ShapeNet55, and ModelNet40. Evaluations on these common classes might introduce an unfair comparison of zero-shot performance.
To deal with this issue, we create two more sets in ModelNet40, referred to as \emph{Medium} and \emph{Hard} for evaluation.

\noindent\textit{Medium set}: We remove the ModelNet40 categories whose exact category names exist in our pre-training category list.

\noindent\textit{Hard set}: In the ``Medium'' category list, there are still some category names that are synonyms to the pre-training categories, 
such as 'cup' vs. 'mug' and 'chair' vs. 'stool.' Therefore, for the ``Hard'' ModelNet40 category list, we remove the categories from the ``Medium'' list with semantically similar counterparts in pre-training categories. 


\begin{figure}[htb]
    \centering
    \includegraphics[width=1.0\linewidth]{pointbert_pointmlp_data_efficiency.pdf}
\caption{Data efficiency comparison. The X axis indicates the percentage of samples used for training and Y axis denotes the overall accuracy. Both PointMLP and PointBERT are significantly improved when pre-training with ULIP. }
    \label{fig:data-efficiency}
\end{figure}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=1.0\linewidth]{figure3_san.pdf}
\caption{Qualitative results of real image to point cloud retrieval. Query images are from Caltech101, and point clouds are from ModelNet40. We show the top-5 retrieved point cloud models, ranked in order. The results demonstrate the retrieval capability of our model.}
    \label{fig:image_pc_retrieval}
\end{figure*}
\paragraph{Experimental Results.}
We present the zero-shot 3D classification results on ModelNet40 in Table~\ref{tab:zero-shot-modelnet} and the results on ScanObjectNN in Table~\ref{tab:zero-shot-scan}. We can see that all ULIP-based methods significantly outperform our major baseline, PointCLIP, by a large margin in every evaluation set.
Specifically, on the \emph{Hard set}, our best performing method, ULIP + PointBERT, outperforms PointCLIP by around 29\% in top-1 accuracy. It also indicates that the superior performance of ULIP-based methods is not caused by pre-training the model on exact/similar categories as the target categories.
Instead, it suggests that aligning the representations of different modalities can benefit the recognition of rare categories in general. Results in Table \ref{tab:zero-shot-scan} demonstrate that ULIP-based methods consistently surpass PointCLIP on real scanned objects. Furthermore, all of the 3D backbones outperform the SOTA zero-shot method, PointCLIP, by 30\% with the help of our ULIP framework.

























\subsection{Analyses}
\label{sec:more analyses}

\paragraph{Align Representations, Three Modalities or Two?}
As described in Eq. \ref{eq:final loss}, ULIP by default aligns the 3D representation with both the text and image representations during pre-training.
We wonder to what extent ULIP will still work if we align the 3D representation to only the text feature or image feature alone.
In this section,
we conduct an ablation study for ULIP by aligning two rather than three modalities in zero-shot settings.
Results are shown in Table~\ref{tab:ablation-scan} and Table~\ref{tab:ablation-modelnet} for ScanObjectNN and ModelNet40 datasets, respectively. As we can see in both tables, aligning the 3D modality with both text and image modalities consistently achieves the best performance compared to aligning with either image or text modality in every scenario with each baseline.

\vspace{-10pt}
\paragraph{Data Efficiency.}
Model pre-training could potentially reduce the demand for labeled data during fine-tuning in downstream tasks. We validate the data efficiency of ULIP by comparing it with baselines under a varying number of fine-tuning samples. The comparison results are shown in Figure \ref{fig:data-efficiency}. As shown in Figure \ref{fig:data-efficiency} (left), PointMLP's performance is largely improved in the low data regime when pre-trained under the ULIP framework. When we compare PointBERT with PointMLP baselines (two red lines in the two sub-figures), we observe that PointBERT performs better than PointMLP when using less than 20\% training data. This is because of that the PointBERT model itself is pre-trained on ShapeNet55. Although both ULIP and PointBERT are pretrained on ShapeNet55, ULIP still improves PointBERT by a clear margin, as shown in Figure \ref{fig:data-efficiency} (right).








\subsection{Cross-Modal Retrieval}
As mentioned in Section~\ref{sec:intro}, one of the benefits of ULIP is that it enables more cross-modal downstream tasks. Here, we qualitatively show the potential of using ULIP to conduct real image to point cloud retrieval. 

We use our pre-trained ULIP with PointBERT as the 3D encoder directly. We conduct a small-scale experiment with real images from Caltech101 \cite{fei2004learning} and use the images to retrieve 3D point clouds from around 2.5k samples over 40 categories in ModelNet40. In Figure \ref{fig:image_pc_retrieval}, we show the top-5 retrieved 3D point cloud models (ranked in order) using image examples from categories of \emph{chair}, \emph{airplane}, \emph{laptop} and \emph{lamp}. The results show encouraging signs that our pre-trained model has learned meaningful features across image and 3D point cloud modalities. Surprisingly, the top-1 retrieved 3D models have the closest appearance to the query images compared to other retrieved 3D models. For example, when we use images from different aircraft types (fight and airliner) for retrieval (2nd and 3rd rows), the retrieved top-1 point clouds maintain the subtle difference of the query images.


\section{Conclusions}
\label{sec:conclusion}
We propose ULIP, a pre-training framework that aligns multiple modalities of images, language, and point clouds in the same feature space. We take advantage of the pre-trained text and image encoders and improve different 3D encoders using our framework. Experiments results show that ULIP can effectively improve representations of 3D backbones. Our method achieves state-of-the-art performance in both zero-shot and standard 3D classification tasks, and our qualitative results show that ULIP has promising potential for cross-modal retrieval applications.













{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\clearpage

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{PointNeXt Backbone Experiments}
\noindent\textbf{PointNeXt}\cite{qian2022pointnext} is a concurrent work which proposes a lightweight backbone based on PointNet++ and in particularly it gives promising results on the ScanObjectNN benchmark. In order to demonstrate the effectiveness of our ULIP on this most recent backbone, we pre-train PointNeXt using ULIP, and use the pre-trained weights to finetune on the ScanObjectNN dataset.

As shown in Table \ref{tab:fintune-scan}, ULIP significantly improves PointNeXt in both Overall Accuracy and Class-mean Accuracy.

\begin{table}[htb]
\centering
    \begin{tabular}{lcc}
    \toprule
         Model& Overall Acc & Class-mean Acc \\
         \midrule
PointNeXt* \cite{qian2022pointnext} &  87.4 & 85.8 \\
PointNeXt + ULIP &  \textbf{89.2} \textcolor{DarkGreen}{\small ()} & \textbf{88.0} \textcolor{DarkGreen}{\small ()} \\
\midrule
         PointNeXt \dag * & 87.5  & 85.9 \\
PointNeXt \dag + ULIP &  \textbf{89.7} \textcolor{DarkGreen}{\small ()} & \textbf{88.6} \textcolor{DarkGreen}{\small ()} \\
\bottomrule
    \end{tabular}
    \caption{3D classification results on ScanObjectNN for PointNeXt. \dag indicates a model uses 2K sampled points and all others use 1K sampled points. * indicates it's reproduced result.}
    \label{tab:fintune-scan}
\end{table}

\subsection{Details of Evaluation Sets in  Zero Shot Classification}
\noindent When evaluating zeroshot classification, we notice that there are some common classes between our pre-train dataset, ShapeNet55, and ModelNet40. Evaluations on these common classes might introduce an unfair comparison of zeroshot performance. Therefore, we introduced three different validation sets for evaluating our models and our baselines on ModelNet40.

\noindent\textbf{All Set}: Includes all the categories in ModelNet40 as shown in Table \ref{tab:ModelNet40-All-Set}.

\begin{table}[htb]
    \small
    \begin{tabular}{ccccc}
        \toprule
        airplane & bathtub & bed & bench & bookshelf \\
        \midrule
        bottle & bowl & car & chair & cone \\
        \midrule
        cup& curtain& desk& door& dresser \\
        \midrule
        flower\_pot& glass\_box& guitar& keyboard& lamp \\
        \midrule
        laptop& mantel& monitor& night\_stand& person \\
        \midrule
        piano& plant& radio& range\_hood& sink \\
        \midrule
        sofa& stairs& stool& table& tent \\
        \midrule
        toilet& tv\_stand& vase& wardrobe& xbox \\
        \bottomrule
    \end{tabular}
    \caption{ModelNet40 All Set.}
    \label{tab:ModelNet40-All-Set}
\end{table}

\noindent\textbf{Medium Set}: We remove categories whose
exact category names exist in our pre-training dataset. The resulting categories in this set is shown in Table \ref{tab:ModelNet40-Medium-Set}.

\begin{table}[htb]
    \small
    \begin{tabular}{ccccc}
        \toprule
        cone& cup& curtain& door& dresser \\
        \midrule
        glass\_box& mantel& monitor& night\_stand& person \\
        \midrule
        plant& radio& range\_hood& sink& stairs \\
        \midrule
        stool& tent& toilet& tv\_stand& vase \\
        \midrule
        wardrobe& xbox \\
        \bottomrule
    \end{tabular}
    \caption{ModelNet40 Medium Set.}
    \label{tab:ModelNet40-Medium-Set}
\end{table}

\noindent\textbf{Hard Set}: We remove both extract category names and their synonyms in our pre-training dataset. The final \emph{Hard Set} is shown in Table \ref{tab:ModelNet40-Hard-Set}

\begin{table}[htb]
    \small
    \begin{tabular}{ccccc}
        \toprule
        cone& curtain& door& dresser& glass\_box \\
        \midrule
        mantel& night\_stand& person& plant& radio \\
        \midrule
        range\_hood& sink& stairs& tent& toilet \\
        \midrule
        tv\_stand& xbox \\
        \bottomrule
    \end{tabular}
    \caption{ModelNet40 Hard Set.}
    \label{tab:ModelNet40-Hard-Set}
\end{table}


\subsection{Indoor 3D Detection Experiments}
In order to show our potential on 3D scene applications, we conduct experiments on ScanNet-v2 dataset and benchmark 3D detection performance based on one of SOTA 3D detection frameworks, Group-Free-3D \cite{liu2021group}. In our setting, we use the Group-Free-3D basic model and observe significant improvements as shown in Table \ref{tab:GroupFree3D experiments}.

\begin{table}[htb]
    \small
\centering
    \begin{tabular}{lcc}
         \toprule
~ & mAP@0.5 & mAP@0.5 Averaged
         \\
         \midrule
         Group-Free-3D & 48.9 & 48.4\\
\midrule
         Group-Free-3D + ULIP & 50.2 \textcolor{DarkGreen}{\small ()} & 49.6 \textcolor{DarkGreen}{\small ()}\\
         \bottomrule
    \end{tabular}
\caption{Experiments on indoor 3D Detection. We use Group-Free-3D basic model as our detection framework, and we follow the same metric computation as in \cite{liu2021group}.}
    \label{tab:GroupFree3D experiments}
\end{table}












\end{document}
