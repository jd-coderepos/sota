

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{xspace}
\usepackage[accsupp]{axessibility}  




\newcommand{\marcus}[1]{{\textcolor{orange}{(Marcus: #1)}}}
 \newcommand{\marcusAdd}[1]{{\textcolor{orange}{#1\xspace}}}



\newcommand{\ls}[1]{}
\newcommand{\sh}[1]{}


\newcommand{\myparagraph}[1]{\noindent\textbf{#1}\xspace}

\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}








\newcommand{\Method}{Learn2Augment\xspace}
\newcommand{\MethodShort}{L2A\xspace}


\newcommand{\SemanticMatch}{Semantic Matching\xspace}
\newcommand{\SemanticMatchShort}{SM\xspace}




\newcommand{\VideoMix}{Video Compositing\xspace}
\newcommand{\VideoMixShort}{VC\xspace}

\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{5234}  

\title{Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition} 

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Learn2Augment}
\author{Shreyank N Gowda\inst{1} \and
Marcus Rohrbach\inst{2} \and \\
Frank Keller\inst{1}  \and
Laura Sevilla-Lara\inst{1}}
\authorrunning{Shreyank N Gowda et al.}
\institute{University of Edinburgh \and
Meta AI\\
}


\maketitle



\begin{abstract}
We address the problem of data augmentation for video action recognition. Standard augmentation strategies in video are hand-designed and sample the space of possible augmented data points either at random, without knowing which augmented points will be better, or through heuristics. We propose to learn what makes a ``good'' video for action recognition and select only high-quality samples for augmentation. In particular, we choose video compositing of a foreground and a background video as the data augmentation process, which results in diverse and realistic new samples. We learn which pairs of videos to augment {\em without} having to actually composite them. This reduces the space of possible augmentations, which has two advantages: it saves computational cost and increases the accuracy of the final trained classifier, as the augmented pairs are of higher quality than average. We present experimental results on the entire spectrum of training settings: few-shot, semi-supervised and fully supervised. We observe consistent improvements across all of them over prior work and baselines on Kinetics, UCF101, HMDB51, and achieve a new state-of-the-art on settings with limited data. We see improvements of up to 8.6\% in the semi-supervised setting. Project Page: \url{https://sites.google.com/view/learn2augment/home}
\end{abstract}

\begin{figure}
\begin{center}
    \centering
\includegraphics[width=0.99\textwidth]{./figures/teaser_final.png}
    \caption{Standard video augmentation techniques generate data using hand-designed heuristics (left). We propose to learn to select videos for augmentation, based on how effective they will be for learning to classify (middle). Our approach, \Method, improves classification across datasets and settings, including UCF101 (right). }\end{center}
\end{figure}

\section{Introduction}
\label{sec:intro}


Large-scale datasets have played a key role in the progress of research across AI problems. In computer vision, neural networks have existed for decades, but one of the enabling factors for the current revolution was the development of the large ImageNet~\cite{deng2009imagenet}.  
In the video domain, manually collecting and annotating data can be a prohibitively expensive process. In video action recognition, for example, collecting data requires an immense amount of manual labor, as it involves finding suitable videos, trimming them and classifying them. 



Recent efforts in video focus on relieving the strong dependency of current methods to the size of labeled datasets. Some of these efforts \cite{sun2021autoflow,actorcut} involve increasing the number of data samples through data augmentation. This strategy aims to create new videos in the training set by performing transformations on the original annotated videos, where labels are known. This process adds diversity to the training data, while new videos are still realistic and plausible. In the simplest version of data augmentation in video, new data samples are generated by flipping the input video horizontally, or by cropping a subsection of the video. New methods~\cite{actorcut,videomix} propose more sophisticated processes like combining two videos. VideoMix~\cite{videomix}
randomly crops regions of one video and pastes them onto another. ActorCut~\cite{actorcut} goes one step further and uses the bounding box detections of humans on one video to paste them onto the background of another video. This increases the diversity of the new videos, and despite the lack of visual realism of the resulting videos, this strategy helps.   

However, as datasets become larger, such data augmentation strategies become computationally expensive. The search space of possible video pairs and transformations is enormous and difficult to explore. The solution is often to sample the space randomly, or to manually design augmentation heuristics. Any exploration process is particularly burdening in the context of video data, where the augmentation process needs to be repeated in every frame, which may be orders of magnitude more expensive than for images. 

In this paper we address the problem of sampling for data augmentation, and propose to learn to select pairs of videos. We show that this reduces the search space of augmented data points by orders of magnitude and improves the final accuracy of the classifier significantly. We leverage two observations. First, not all data points are as useful for classification. This idea has been exploited in the context of frame or clip selection~\cite{smart,korbar2019scsampler,whatmakes}. Second, we can learn to predict which data points will be useful without actually generating them. This is essential, as the space of transformations is huge, and if we needed to create each candidate augmented video, the process would be prohibitively expensive. 






More concretely, we propose a data augmentation method which we call \Method. The proposed method contains a ``Selector'' network, which predicts a score of how useful a combination of two videos will be, without having to actually composite them. The Selector is trained using the accuracy of the classification as the cue. Since this metric depends on the classifier, it is not differentiable with respect to the Selector's parameters. Therefore we optimize the network using reinforcement learning. Once the Selector network is trained, we use it to choose good pairs of videos, composite them, and train a classification network. In our experiments, for example in the case of the UCF101 dataset, using the Selector reduces the number of augmented videos by 92\% while increasing the classification accuracy. 

In the proposed method, each augmented video is created from a pair of videos using a composition of the segmented foreground of one video, including actor and objects, onto the background of the other video. This process yields diverse and realistic new data samples, which we demonstrate is important for learning. More concretely, results show an improvement of 4.4\% over using a simpler transformation. 



The Selector is indeed useful to reduce the number of videos for training the classifier. However, we also need to reduce the space of possible pairs for training the Selector network itself. For example, the number of possible pairs of videos in video datasets can be in the order of millions for small datasets or billions for large datasets.
For this, we leverage the natural correlation between the occurrence of foreground activities and background scenes~\cite{Choi-NeurIPS-2019}. This is, it is more likely to find someone playing football in a football field than at a restaurant. Instead of sampling at random the pairs of videos to train the Selector on, we sample pairs from classes that are semantically similar. In particular, we use the class names to obtain a semantic embedding, and match each class to their nearest neighbor in this space. Experiments show that this extremely simple design choice of Semantic Matching reduces the space of possible pairs of videos by several orders of magnitude (from quadratic to linear on the number of videos). This yields better results than choosing pairs at random, which may result in non-plausible scenarios, or choosing pairs from the same class, which may not add as much diversity. 











In summary, the proposed \textbf{\Method} contains three core components: a Selector that learns to choose good videos to augment, a \SemanticMatch method that improves optimization, and a \VideoMix that composites video pairs for augmentation. Experimental results show that all components contribute to the performance of the system in different ways, and the overall method obtains state-of-the-art in all datasets, and in all settings that involve limited training data. In addition, in the setting which considers the full training set, the proposed data augmentation technique improves upon the baseline on all datasets, including UCF101, HMDB51, and the large-scale Kinetics-400. 





\section{Related Work}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{./figures/overview_final.png}
\caption{Overview of the proposed \Method. Given a pair of videos and their labels, a Selector network gives a score  of the quality of the potential composited video. At training time, the Selector is trained with the validation loss of the classification network. Once the Selector is trained, pairs of videos are sampled, and only the promising combinations with high score  are composited and used for training the classifier. }
    \label{fig:overview}
\end{figure*}
\paragraph{Data Augmentation for Video Action Recognition.}
Standard data augmentation techniques in action recognition include horizontal flip and cropping, where new videos are created by selecting a box at each frame, and then resizing the resulting video to have the same size as the original one. While this strategy helps, generated videos do not add much diversity to the training set. 
Recent efforts such as ActorCut~\cite{actorcut} and VideoMix~\cite{videomix} increase the diversity of new video samples by cutting and pasting the foreground of one video onto another. This general technique of combining two data samples has proven to be quite effective, even in the image domain~\cite{yun2019cutmix}. However, the resulting videos are not very realistic, and are used for training regardless of their quality. Zhang et al.~\cite{aug_gans} go one step further and synthesize new samples using GANs, and use ``self-paced selection'' to train, starting with easy samples and progressively choosing harder samples. Instead, we propose to create realistic data samples by segmenting, inpainting and blending the foreground of one video onto the background of another. Crucially, we learn to discard novel video samples that are not expected to be useful for classification, overall producing a more accurate data augmentation strategy. 








\paragraph{Learning to Augment Data.} 
The idea of learning to augment data has been used in other computer vision problems. In the image classification domain, this strategy has been done using the final classification loss as the training criterion~\cite{Lemley2017SmartAL}, augmenting in feature space~\cite{devries2017dataset}, and learning data augmentation policies~\cite{Cubuk_2019_CVPR}. As in this paper, in the image domain it has been noted that the search space for data samples can be  large and thus expensive~\cite{NEURIPS2020_auto}. 

Other computer vision domains like low level vision, also struggle with data dependency, as creating ground truth is particularly hard. In optical flow, AutoFlow~\cite{sun2021autoflow} recently introduced the strategy of learning to generate good training data for a target dataset. 
 
 

\paragraph{Semi-supervised Video Action Recognition.} Semi-supervised learning (SSL) also aims to reduce data dependence by learning from large sets of unlabeled samples and a small set of labeled ones. SSL in images has been widely explored.
For example, some strategies include giving pseudo-labels \cite{arazo2020pseudo,pseudo} to samples where the classifier has high confidence, and adding these to the labeled training data. Other common approaches use consistency regularization \cite{conreg1,conreg2,meanteacher}. Approaches that combine consistency regularization and entropy minimization ~\cite{grandvalet2005semi} have shown to be very effective in tackling the SSL task in images such as MixMatch \cite{berthelot2019mixmatch} and RemixMatch \cite{berthelot2019remixmatch}.

SSL in videos however, has not been explored as much. One of the early works used extreme learning machines \cite{iosifidis2014semi} to perform SSL on videos. Recently, VideoSSL \cite{Jing_2021_WACV} and Temporal Contrastive Learning (TCL) \cite{tcl} leverage SSL in videos. VideoSSL \cite{Jing_2021_WACV} uses pseudo-labels and object cues from unlabeled samples to guide the learning process. TCL \cite{tcl} use a two-pathway contrastive learning model using unlabeled videos at two different speeds with the intuition that changing video speeds do not change the action being performed.

Data augmentation and SSL are two different families of techniques to relieve the dependence on labeled data, and in this paper we experiment with the combination of both, showing that they are actually complementary. 







\paragraph{Sample Selection.}
Recent work~\cite{whatmakes} has shown that not all data samples are as useful. Selecting a subset of high quality frames or clips at test time shows better results than using the entire video for action recognition. In this spirit, SMART~\cite{smart} uses an attention and relation network to learn scores for each frame in a video and then select only the high ranked ones for inference. Similarly, SCSampler~\cite{korbar2019scsampler} uses a lightweight clip sampling model to select the salient clips in a video and use only those. Unlike the proposed method, these learn to choose single videos, which are already available, while we learn to choose pairs of videos to be composited, which are not already combined. 

The most relevant work to ours is data valuation in the image domain, using RL \cite{yoon2020data}, in the image domain where each sample is given a score of how effective the sample is, and at training time the sample is multiplied by this score. In our work, instead of learning the effectiveness of the training set, we leverage that knowledge for augmentation. 


\section{\Method}

\begin{figure*}[t]
\centering
    \includegraphics[width=0.7\textwidth]{figures/mix_fin.png}
    \caption{Pipeline for compositing a single frame. The foreground is from the class ``soccer juggling'' and the background from the class ``soccer penalty'', which are semantic class neighbors. We can see objects such as `person' and `ball' are detected as objects of interest.}
    \label{fig:mix}
\end{figure*}



In this section we describe in detail the architecture of the proposed \Method. In a nutshell, the goal is to learn to augment novel data points which are realistic and diverse, such that we can train a better classifier with them. For this, we train a Selector network, which predicts a score of how useful a given pair of videos is for augmentation. We pick pairs that have a high score to be augmented. The transformation we use for augmentation is \VideoMix. Training the Selector using the entire dataset is infeasible, and sampling pairs of videos at random will yield unlikely pairs. Thus we sample pairs of videos using \SemanticMatch. Figure~\ref{fig:overview} shows an overview of the proposed method and in Sec.~\ref{sec:optimization} we describe how we train our approach.







\subsection{Selector}
\label{subsec:choosing} 




Given two input videos  and , the goal of the Selector is to predict a weight , rating the quality of the potential composited video. Note that the input to the Selector is two putative videos instead of the composited one. This means that at test time, we can predict how useful the composited video will be without having to actually create it. 

The architecture of the Selector includes a standard video classification network to extract video features, which is ResNet3D-18~\cite{resnet} followed by a simple multi-layer perceptron (MLP) with 3 hidden layers of sizes 2048, 1024 and 512. Two videos are input to the Selector at a time, and their features and labels are concatenated and input to the MLP. 

Since there is no ground truth of how ``good'' a video sample is for learning, we train the Selector using the change in validation loss of the classifier. This is, we argue that a ``good'' training sample is one which, if used for training, improves the validation loss of the classification network. In other words, if we take one optimization step training the classifier, after updating the weights, the validation loss will go down. Section~\ref{subsec:RL} describes the training process in detail. 


At test time, we use the Selector by sampling pairs of videos, choosing those pairs with high score , and input to the \VideoMix module, which we describe in Sec.~\ref{subsec:video_mixing}. The resulting video is finally used to augment the training set for the classification network. 






\subsection{\SemanticMatch (SM)}



The number of pairs in the full dataset can be very large, as it grows with the square of the number of videos. For Kinetics~\cite{carreira2017quo}, for example, we would encounter 360 billion pairs. Training the classifier using these is clearly infeasible, and thus we use the Selector. But training the Selector itself with all these samples is infeasible too. Sampling uniformly is a reasonable solution, but many video pairs may not be useful for learning. We leverage the observation that all combinations of actions and backgrounds are not equally likely~\cite{Choi-NeurIPS-2019}. This natural correlation between actions and backgrounds helps to prune unlikely class combinations. 

For this, we make the assumption that classes that are semantically similar are more likely to contain a foreground and a background that are plausible in the real world, and therefore more realistic for our data augmentation purposes. Thus, we use the class names to extract a language embedding using sen2vec \cite{sen2vec}, and use these embeddings to match each class to its nearest neighbor. We sample videos  and  from class  and its closest neighbor  respectively. This simple decision reduces the number of pairs to grow linearly with the size of the dataset, and furthermore increases the accuracy significantly with respect to sampling video pairs at random. More details on the numerical impact can be found in Sec.~\ref{subsec:ablation}. Semantic class pairs and additional experiments using intra-class augmentation can be found in the supplementary material. 


\subsection{\VideoMix (VC)}
\label{subsec:video_mixing}

The goal of the augmentation process is to composite two videos, to produce realistic, plausible and diverse new videos, that will improve the classification. 
Figure~\ref{fig:mix} shows the overall pipeline for compositing a single frame. 

Given two videos which will be used for foreground  and background , we use a standard object segmentation network (MaskRCNN~\cite{maskrcnn}) to segment out people and objects in every frame of both videos. Objects categories in action datasets are not completely contained in the image dataset COCO~\cite{coco}, which is used for training MaskRCNN. However, we observe that object detections with high confidence tend to correspond to actual objects, even if the category is not correct (boxing bag is often classified as fire hydrant), and therefore are useful to our purpose. We could also have selected only the humans in the video, as action categories tend to be focused on humans. However, we find that the presence of specific objects is highly correlated with action categories (musical instruments in the classes ``playing guitar" or ``playing violin"). Therefore removing the original objects from the background and adding the ones from the foreground is essential for recognition. See numerical results of the impact of these decisions in the ablation study of Sec.~\ref{subsec:ablation}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.13\textwidth]{figures/sample1.png}
    \includegraphics[width=0.13\textwidth]{figures/sample2.png}
    \includegraphics[width=0.13\textwidth]{figures/sample3.png}
    \includegraphics[width=0.13\textwidth]{figures/sample4.png}
    \includegraphics[width=0.13\textwidth]{figures/sample5.png}
    \includegraphics[width=0.13\textwidth]{figures/sample6.png}
    \includegraphics[width=0.13\textwidth]{figures/sample7.png}
    \caption{Sample frames of rendered videos. While the segmentation contains errors, such as missing limbs or portions of the object, the action category remains clear.}
    \label{fig:sample_videos}
\end{figure*}

We remove the segmented objects from the background video and fill in the holes using image inpainting~\cite{inpaint}, to obtain a clean background video . Finally, we combine the foreground objects and the background at each frame by simple composition, as in: 

where  is the resulting composited frame at time ,  and  are frames of the foreground and background videos respectively,  is the binary mask with the union of all detected objects, and  is the element-wise multiplication. Figure~\ref{fig:sample_videos} shows sample frames of the resulting videos. 











\section{Optimization of \Method}
\label{sec:optimization}




The optimization of the proposed \Method method has two stages. In the first stage, we train the Selector network using RL, as described in Sec.~\ref{subsec:RL}. Once the Selector network is trained, in the second stage, we perform data augmentation to train the classifier. That is, we sample pairs of videos, pass them through the trained Selector, choose the pairs with high score, create new videos with these pairs through \VideoMix, and add them to the training set. We now describe the details of these two training stages.   




\subsection{Training the Selector}
\label{subsec:RL}

As mentioned before, there is no ground truth to tell us how good an augmented data sample is. Instead, we use the validation loss of the classification network to train the Selector network. This function is not differentiable with respect to the parameters of the Selector. A common solution to dealing with this is to use RL \cite{yoon2020data}. 

Specifically, the state  at time  is the batch of video pairs sampled using \SemanticMatchShort. The action  is the subset of these video pairs selected for compositing and is represented as a vector of values between 0 and 1.
The environment is the classification network and the validation process. This environment is used to compute a reward  for choosing a particular action, where  are the parameters of the Selector. 

We calculate the reward in a single step, as the difference between the loss in the current batch and the moving average of losses in the previous  steps (where ) denoted as , as in Eq.~\ref{eq:reward}: 

where  is the classification cross-entropy loss,  is the classifier network of parameters ,  and  are an input video and its label respectively,  is the validation set and  is the number of samples in . The objective function that we want to maximize is the expected value of the reward: 

To find the optimal policy, we would typically differentiate the objective function with respect to the parameters . However, the reward function is dependent on the validation loss, calculated with the classifier network, which does not involve . Instead, using REINFORCE~\cite{reinforce}, we approximate the objective function as: 


where,  is the  state-action trajectory under the policy ,  is the number of sample trajectories and  is the number of actions performed in a trajectory. Note that as we have single-step episodes, we can make several simplifications as , as , and as there is only one trajectory , and thus  is just .
With these simplifications and substituting Eq.~\ref{eq:objective} in  Eq.~\ref{eq:approx}, we obtain: 

where  corresponds to the subset of pairs of samples to composite and  to all the pairs of samples in the batch. The Selector is updated by  where  is the learning rate and  is updated with the last calculated loss as seen in Eq~\ref{delta}.




Note that this training process does involve generating the composited videos for pairs in , to input to the classifier and compute the loss. However, crucially, during training this is a small portion (one order of magnitude smaller) of how many videos would need to be generated if we were to composite all pairs of videos. 



Once the Selector is trained, we use it for actually filtering good pairs. At that point, given two videos and their labels, the Selector network predicts a policy  of how likely it is to select the pair. The score  is the value of  for each pair. We use a threshold on that score to select the pairs of videos to augment. In our experiments, we first determine a budget on the number of videos that we want to augment, and then pick the threshold to select the top-ranked video pairs. We use these selected pairs of videos as input to \VideoMix, add them to the training set, and use them to train the classifier. 

\subsection{Training the Classifier}



Similar to previous work which combines multiple samples for augmentation~\cite{yun2019cutmix,actorcut}, composited/mixed samples should include mixed labels. We adopt the strategy of Cutmix~\cite{yun2019cutmix}, where the foreground label  and the background label  are combined using a ratio , as:

to obtain the mixed label . A simple way to choose  is to use the ratio of the foreground mask with respect to the overall video. Given the foreground video  of dimensions , and mask at each frame , the foreground ratio would be .
Instead of choosing  to be directly proportional to the foreground ratio , we give slightly more weight to the foreground~\cite{actorcut}, as in Eq.~\ref{eq:scaling}, where .

We add composited videos , and their mixed labels  to the training set, and train the classifier network using a standard cross-entropy loss, with stochastic gradient descent. 

The choice of classifier is not tied to our method. In our experiments, we choose the widely used 3D ResNet-18 architecture, which allows us to compare directly to other approaches.





\section{Experiments}
\label{sec:experiments}

We experiment extensively with \Method using three data settings, four datasets, and two splits. We also present ablation studies. In this section we first describe the details of the experiments and then discuss our results.


\subsection{Experimental Details}

\paragraph{Datasets.} In order to provide comparison to prior work (e.g. \cite{actorcut,tcl}), we use standard datasets for evaluation in action recognition, including HMDB51~\cite{hmdb}, UCF101~\cite{ucf}, Kinetics-400~\cite{carreira2017quo}, and Kinetics-100, which includes the 100 classes with the largest amount of samples in Kinetics, as it is used in prior work~\cite{Jing_2021_WACV} and helps us compare directly. For experiments on the effect of pre-training the Selector, we use Kinetics-400. For the semi-supervised setting, we split the datasets following the protocol of  VideoSSL~\cite{Jing_2021_WACV} and ActorCut~\cite{actorcut}. For few-shot we use the standard split \cite{arn} and the Truze split \cite{truze} which ensures no overlap of novel classes with Kinetics-400.

\paragraph{Problem Settings.} We test the proposed method in three different settings. In the \emph{semi-supervised} setting, a portion of the training set is artificially held out, and the rest of the training data is assumed to be available, but unlabeled. Tests are performed on different percentages of held out data. In the \emph{few-shot} setting, some classes (novel classes) are assumed to have a very small number of training samples (one to five instances), while other classes have the full number of samples (seen classes). We effectively change the -shot learning problem to a -shot problem where  is the number of augmented samples. 
Finally, in the standard \emph{full set} setting, all training data is available. 






\paragraph{Training Settings.} We use mini-batch stochastic gradient descent, with momentum of 0.9 and weight decay 0.001. For each video, we use an 8-frame clip, where the frames are uniformly sampled. We use batch size of 8. For UCF101 and Kinetics100 in the SSL setting, we train the model for 400 epochs and for HMDB51, we train for 500 epochs. The initial learning rate is set to 0.1 and then decayed using cosine annealing policy. For the SSL setting, we use the data split proposed in VideoSSL \cite{Jing_2021_WACV}. For the few-shot setting, we use the default hyperparameters of TRX~\cite{trx}, ARN~\cite{arn} and C3D-PN~\cite{pn}, respectively. In the fully supervised setting, we train R(2+1)D for 100 epochs on UCF101, HMDB51 and 50 epochs on Kinetics-400.













\subsection{Architectural Changes for Different Settings}

We briefly explain the structural adaptations of our approach for each of the settings.

\paragraph{Semi-supervised Learning.}
Similar to VideoSSL \cite{Jing_2021_WACV}, we first train the classifier on the available labeled data using the categorical cross-entropy loss. Once this network is trained, we do a forward pass of the unlabeled examples and assign pseudo-labels to those samples with high confidence. We use these pseudo-labels as additional data for augmentation. We also add a knowledge distillation loss inspired by VideoSSL \cite{Jing_2021_WACV}. Details can be found in the supplementary material.



\paragraph{Few-shot Learning.}
We only augment the novel classes using \Method. We also do not perform label mixing and simply use the foreground label for the augmented sample. This incorporates our composited samples seamlessly into the meta-learning framework typically followed. 
We show results on the standard split, as on the recently proposed TruZe \cite{truze}. TruZe ensures that the novel classes do not overlap with Kinetics-400.

\paragraph{Fully-supervised Learning.} This is the simplest setting, where the Selector is trained on the full training set, and used for data augmentation to train the classifier. We explore two scenarios: training the classifier from scratch and using a model pre-trained on Sports1M \cite{sports1M}.






\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\centering
\begin{tabular}{c c c c c}
Pairs  & Video  & Semantic  & Accuracy & \#Videos  \\
 Selector &  Compositing &  Matching & in \% &  (S) \\
\noalign{\smallskip}
\hline
\checkmark & \checkmark & \checkmark & {\bf 58.9} & 12K  \\
 & \checkmark & \checkmark & 55.8 & 99K  \\
\checkmark &   & \checkmark & 54.5 & 12K  \\
\checkmark & \checkmark &   & 55.2 & (1.2M) \\
\checkmark &  &  & 52.9 & (1.2M) \\
 & \checkmark &  & 48.6 & (10.4M) \\
 &  & \checkmark & 50.8 & 99K \\
  &   &   & 45.5 & (10.4M) \\ \noalign{\smallskip}
\end{tabular}
\caption{Ablation study to explore the impact of each proposed component. All settings use the same number of samples for training, so that they can be compared fairly. The \# Videos (S) corresponds to the search space in each scenario. As we can see, we obtain the best accuracy using just 12K instead of the standard scenario which would have had 10.4M i.e. a reduction of over 1000x. }

\label{table:ablation}
\end{table}

\subsection{Ablation Study}
\label{subsec:ablation}

Table~\ref{table:ablation} shows the ablation study of \Method, which illustrates the impact of each of the proposed elements in the design. The experiment is done on the UCF101 dataset, using 20\% of the data i.e. in a semi-supervised setting. All three contributions (Selector, \SemanticMatch and \VideoMix) improve accuracy. Crucially, \SemanticMatch and the Selector also reduce greatly the number of possible video combinations, and the overall reduction is around three orders of magnitude. We see that \Method obtains a 13.4\% improvement over the baseline. While there are improvements of up to 7.4\% for each component, the combination of all three gives the best results. Further analysis can be found in the supplementary material.



The \VideoMix module also has multiple components. In Table~\ref{table:ablation_mix}, we ablate these components and observe that removing objects is actually essential, and has the most significant impact, followed by using segmentation instead of a bounding box, and finally inpainting. 

Although the compositing process is more computationally expensive than previous simpler mixing strategies, it is important to note that 1) the overall accuracy indeed improves, 2) the actual composition for training the classifier is done on a small subset of pairs of videos and 3) the Selector can be trained on a large dataset (e.g.: Kinetics) just once and can be reused for the smaller datasets without the need of fine-tuning (see Table~\ref{tab:semi}). 

\begin{table}[t]
\centering
\begin{tabular}{lc}
Method                  & Accuracy \\
\hline
\MethodShort                    & 58.9     \\
\MethodShort w/o Inpaint        & 57.6     \\
\MethodShort w/o Segmentation       & 56.8     \\
\MethodShort w/o Objects & 55.7    \\
\MethodShort w/o All & 54.5 \\
\end{tabular}
\caption{Ablation study of compositing components. The version ``w/o Inpaint" refers to pasting the foreground without first filling in the holes of removed objects in the background. The version ``w/o Segmentation" refers to using bounding boxes instead of object segmentations. ``w/o Objects" refers to copying and pasting only the humans in the scene, leaving the objects. }
\vspace{-0.5cm}
\label{table:ablation_mix}
\end{table}





\begin{table}[]
\setlength{\tabcolsep}{5pt}
\centering
\resizebox{\columnwidth}{!}
{\begin{tabular}{l@{\ }cccccccccccc}
     &  & \multicolumn{4}{c}{Kinetics 100} & \multicolumn{4}{c}{UCF101} & \multicolumn{3}{c}{HMDB51} \\
Method & Conference & 50\%   & 20\%   & 10\%   & 5\%   & 50\%  & 20\%  & 10\% & 5\% & 60\%    & 50\%    & 40\%   \\
\hline
CutMix~\cite{yun2019cutmix} & ICCV19 & 53.7 & 46.1 & 43.2 & 39.9 & 46.1 & 36.5 & 34.6 & 25.8 & 33.9 & 30.8 & 27.8 \\
MixUp~\cite{mixup} & ICLR18 & 53.4 & 45.5 & 43.0 & 39.6 & 45.8 & 36.1 & 34.2 & 25.5 & 33.7 & 31.0 & 27.5 \\
CutOut~\cite{cutout}& Arxiv17 & 52.8 & 45.1 & 42.3 & 38.8 & 45.2 & 35.6 & 33.9 & 24.6 & 33.0 & 30.5 & 27.1 \\
ST-VideoMix~\cite{videomix} &Arxiv21 & 55.3 & 46.6 & 43.9 & 40.4 & 46.4 & 36.4 & 35.2 & 25.9 & 34.8 & 31.3 & 28.7 \\
\hline
PseudoLabel~\cite{pseudo} &ICMLW13  & 59.0 & 48.0 & 38.9 & 27.9 & 47.5  & 37.0  & 24.7 & 17.6 & 33.5    & 32.4    & 27.3    \\  
MeanTeacher~\cite{meanteacher}& Neurips17 & 59.3 & 47.1 & 36.4 & 27.8  & 45.8  & 36.3  & 25.6 & 17.5 & 32.2    & 30.4    & 27.2    \\
S4L~\cite{s4l}& ICCV19 & 54.6 & 51.1 & 43.3 & 33.0 & 47.9  & 37.7  & 29.1 & 22.7 & 35.6    & 31.0    & 29.8    \\
VideoSSL~\cite{Jing_2021_WACV}& WACV21 & 65.0 & 57.7 & 52.6 & 47.6  & 54.3  & 48.7  & 42.0 & 32.4 & 37.0    & 36.2    & 32.7    \\
ActorCut~\cite{actorcut} &Arxiv21 & 68.7 & 61.2 & 56.8 & 52.7  & 59.9  & 51.7  & 40.2 & 27.0 & 38.9    & 38.2    & 32.9    \\
ActorCut+ID~\cite{actorcut}& Arxiv21 & 72.2 & 68.7 & 63.9 & 59.1 & 64.7 & 57.4 & 53.0 & 45.1 & 40.8 & 39.5 & 35.7    \\
TCL~\cite{tcl}& ICCV21 & 70.4 & 64.7 & 61.1 & 58.2 & 62.1 & 55.4 & 52.1 & 42.8 & 41.2 & 40.4 & 34.8\\
\MethodShort& & \textbf{75.9} & \textbf{72.1} & \textbf{67.5} & \textbf{63.7} & 72.1 & 60.3 & 56.1 & 48.0 & 44.5 & 43.2 & 37.9 \\ 
\hline
\MethodShort+Pre-training& & - & - & - & - & \textbf{73.3} & \textbf{64.8} & \textbf{60.1} & \textbf{50.9} & \textbf{47.1} & \textbf{46.3} & \textbf{42.1} \\
\end{tabular}}
\caption{Results on the semi-supervised setting. Results for TCL and ActorCut are obtained by us running the author's code. All methods are run with a 3D ResNet-18 backbone for fair comparison. \MethodShort+Pre-training refers to pre-training the selector and fixing it.}

\label{tab:semi}
\end{table}

\subsection{Augmenting in the Semi-supervised Setting}

In this setting we artificially hold out a portion of the training set, with the goal of observing the behavior of different methods as the size of the training set changes. In this setting, we use the remaining part of the dataset by producing pseudo-labels, similar to VideoSSL~\cite{Jing_2021_WACV}. Table~\ref{tab:semi} shows results in this semi-supervised setting. The \MethodShort version of the method uses a Selector and a classifier trained only on the target dataset (in this case UCF101, HMDB51 or Kinetics-100). We observe that \Method improves on all settings over all previous methods. 

The ``\MethodShort+Pre-training'' row refers to \Method where the Selector has been pre-trained on Kinetics-400, without fine-tuning on the target dataset. 
We make two observations: First that pre-training on a large dataset helps, as the results from the pre-trained model are higher for all datasets and settings. Second that the Selector trained on Kinetics generalizes quite well to the smaller datasets without the need for fine-tuning. We do not test on Kinetics-100 with the pre-trained model, as this would mix training and testing sets. 


\subsection{Augmenting in the Few-shot Setting}

We also explore the impact of the proposed method on the more extreme few-shot setting, where there are only a few examples per class. This is interesting because few-shot methods are already designed to address data scarcity. 

We compare with the current state of the art in this setting, including CD3-PN~\cite{pn}, ARN~\cite{arn} and TRX~\cite{trx}, on the UCF101 and HMDB51 datasets. We observe that the proposed \Method method improves upon all existing approaches, suggesting data augmentation is complementary to few-shot methods. Table~\ref{tab:fslucf} shows the results of the experiments. 
 
\begin{table}
\centering
\resizebox{\columnwidth}{!}
{
\begin{tabular}{l@{\ }ccccccccccc}
     &  & \multicolumn{5}{c}{UCF101} & \multicolumn{5}{c}{HMDB51}\\
Method & Split & 1 & 2 & 3 & 4 & 5 & 1 & 2 & 3 & 4 & 5\\
\hline
C3D-PN \cite{pn} & S & 57.1 & 66.4 & 71.7 & 75.5 & 78.2 & 38.1 & 47.5 & 50.3 & 55.6 & 57.4 \\
C3D-PN + \MethodShort & S & \textbf{60.8} & \textbf{68.9} & \textbf{73.3} & \textbf{76.6} & \textbf{79.1}& \textbf{39.8} & \textbf{48.9} & \textbf{51.5} & \textbf{57.3} & \textbf{58.2} \\
\hline
ARN \cite{arn} & S & 66.3 & 73.1 & 77.9 & 80.4 & 83.1 & 45.5 & 50.1 & 54.2 & 58.7 & 60.6\\
ARN + \MethodShort & S & \textbf{67.7} & \textbf{74.2} & \textbf{79.6} & \textbf{81.1} & \textbf{84.4} & \textbf{47.3} & \textbf{51.7} & \textbf{55.5} & \textbf{60.1} & \textbf{61.8}\\
\hline
TRX \cite{trx} & S & 77.5 & 88.8 & 92.8 & 94.7 & 96.1 & 50.5 & 62.7 & 66.9 & 73.5 & 75.6 \\
TRX + \MethodShort & S & \textbf{79.2} & \textbf{89.2} & \textbf{93.2} & \textbf{95.0} & \textbf{96.3} & \textbf{51.9} & \textbf{63.8} & \textbf{68.2} & \textbf{74.4} & \textbf{77.0} \\
\hline
C3D-PN \cite{pn} & T & 50.9 & 61.9 & 67.5 & 72.9 & 75.4 & 28.8 & 38.5 & 43.4 & 46.7 & 49.1\\
C3D-PN + \MethodShort & T & \textbf{52.5} & \textbf{63.8} & \textbf{70.1} & \textbf{75.2} & \textbf{78.2} & \textbf{29.9} & \textbf{40.1} & \textbf{44.5} & \textbf{47.7} & {50.8}  \\
\hline 
ARN \cite{arn} & T & 61.2 & 70.7 & 75.2 & 78.8 & 80.2 & 31.9 & 42.3 & 46.5 & 49.8 & 53.2\\
ARN + \MethodShort & T & \textbf{63.9} & \textbf{73.1} & \textbf{77.4} & \textbf{80.4} & \textbf{81.3} & \textbf{33.6} & \textbf{43.7} & \textbf{48.0} & \textbf{51.1} & \textbf{53.8} \\
\hline
TRX \cite{trx} & T & 75.2 & 88.1 & 91.5 & 93.1 & 93.5 & 33.5 & 46.7 & 49.8 & 57.9 & 61.5\\
TRX + \MethodShort & T & \textbf{76.8} & \textbf{88.9} & \textbf{92.7} & \textbf{93.8} & \textbf{94.1} & \textbf{35.0} & \textbf{48.1} & \textbf{51.1} & \textbf{59.2} & \textbf{62.1}\\
\end{tabular}}
\caption{Results on UCF101 for the Few-Shot Learning setting, with different splits. Accuracies are reported for 5-way, 1, 2, 3, 4, 5-shot classification. S corresponds to the split used in \cite{arn,trx} and T is the TruZe split \cite{truze}, which avoids overlapping classes with Kinetics.}
\label{tab:fslucf}
\end{table}



\subsection{Augmenting the Full Training Set}

We finally explore the effect of augmenting the full dataset, both for smaller datasets, and the large-scale Kinetics. 
Results can be found on Table~\ref{table:fullset}. Again, \Method improves the performance on all datasets even for a pre-trained model. 

\begin{table}[t]
\centering

\begin{tabular}{l l l c}
Augmentation & Dataset & Pretrained & Top-1 \\
\hline 
Standard & UCF101 & No Pretraining & 55.7 \\
ActorCut \cite{actorcut} & UCF101 & No Pretraining & 68.3 \\
\MethodShort & UCF101 & No Pretraining & {\bf 73.1} \\
\hline 
Standard & HMDB51 & No Pretraining & 40.8 \\
ActorCut \cite{actorcut} & HMDB51 & No Pretraining & 44.5 \\
\MethodShort & HMDB51 & No
Pretraining & {\bf 46.4} \\
\hline 
Standard & UCF101 & Sports1M &  93.6 \\
\MethodShort & UCF101 & Sports1M & {\bf 95.3} \\
\hline 
Standard & HMDB51 & Sports1M & 66.6 \\
\MethodShort & HMDB51 & Sports1M &  {\bf 68.4} \\
\hline
Standard & Kinetics & Sports1M & 75.4 \\
\MethodShort & Kinetics & Sports1M &  {\bf 76.3}\\
\end{tabular}
\caption{Augmenting standard datasets improves classification even with a model pre-trained on the largest existing dataset (Sports1M). }
\label{table:fullset}
\vspace{-0.5cm}
\end{table}












\iffalse
\subsection{Why not intra-class augmentation?}

One other possibility we explored is intra-class augmentation instead of using semantic classes. However, when we followed the same procedure on 20\% labeled data of UCF101 we obtain an accuracy of 41.4\% in comparison to 58.9\% when using semantically similar classes. We believe there to be two main concerns in intra-class augmentation. The first is that Cutmix \cite{yun2019cutmix} has been shown to be an excellent regularization technique. This is aided by having samples that have soft labels (since they are a ratio of samples from different classes). However, using intra-class augmentation would force the labels to be the same as the ground truth class. The second reason is that samples of a particular class are clips that were part of the same video. This is the case in both HMDB51 and UCF101. If we cut the background from one sample and paste the foreground onto this, it results in an identical sample to the original foreground sample. This is because the background is the same in both cases. All we end up doing then is training the model on multiple instances of the same data which leads to overfitting and hence a poor accuracy at test time. \marcus{I would consider putting this section in supplement}
\fi

\section{Why Not Intra-class Augmentation?}
\label{intra-class}
One other possibility we explored is intra-class augmentation instead of using semantic classes. However, when we followed the same procedure on 20\% labeled data of UCF101 we obtain an accuracy of 41.4\% in comparison to 58.9\% when using semantically similar classes. Similarly, in Kinetics100 we obtain an accuracy of 50.1\% and 54.4\% using 5\% and 10\% labeled data respectively. That is 9.4\% and 8.9\% lower than the results using semantic neighbors. We believe there to be two main concerns in intra-class augmentation. The first is that Cutmix \cite{yun2019cutmix} has been shown to be an excellent regularization technique. This is aided by having samples that have soft labels (since they are a ratio of samples from different classes). However, using intra-class augmentation would force the labels to be the same as the ground truth class. The second reason is that samples of a particular class are clips that were part of the same video.
This is the case in both HMDB51 and UCF101 and not so in Kinetics100. If we cut the background from one sample and paste the foreground onto this, it results in an identical sample to the original foreground sample. This is because the background is the same in both cases. All we end up doing then is training the model on multiple instances of the same data which leads to overfitting and hence a poor accuracy at test time. However, since the results are much worse for Kinetics100 as well, we believe that this could be a smaller contributing factor.


\section{Distillation Loss for Semi-Supervised Learning (SSL)}
\label{dist-loss}
Given frame  from video , to distill appearance information of objects of interest, we use the softmax predictions of a ResNet \cite{resnet} image classifier. This network is pre-trained on Imagenet and not modified during training. Let the output of the ResNet be denoted as  where  = 1000 which is the number of classes in Imagenet.  We randomly select a frame from all videos (labeled, unlabeled and augmented) for training. The classifier model in our architecture, produces an embedding  which is of the same dimensions and space of .
We train  to match the output of  by using a soft cross-entropy loss that treats the ResNet outputs as soft labels. This loss  can be seen in Eq.~\ref{eq:dist}. Our final loss function is a combination of  and  (categorical cross-entropy loss for video samples). This is done following the work in VideoSSL \cite{Jing_2021_WACV}.



\section{Analysis of Number of Augmented Samples}
\label{number_of_samples}
We see a common pattern when adding augmented samples to the different SSL settings. This basically refers to increasing the number of augmented samples in the training set.
We see that the accuracy increases initially, reaches a peak performance and then starts dropping slowly as can be seen in Figure~\ref{fig:comparison}.
This makes sense as we don't expect every mixed example to be helpful for training. In fact, this helps us to define  for the selector.
We can see Figure~\ref{fig:comparison} for the results from 0 augmentations to 5000 for 10\% and 20\% labeled data on UCF101. The sweet spot for the 10\% labeled data is around 1200 augmentations and for the 20\% labeled data is around 2000 augmentations. Both of which are obtained using . We decide the value of  based on these and results and use the same for HMDB51 and Kinetics100 for all settings. If we increase the value of  we obtain fewer samples and decreasing the value of  results in more number of samples for training. The value of  thus determines the number of augmented samples and also their quality.

\begin{figure}[]
    \centering
    \includegraphics[width=0.45\textwidth]{./figures/compare.png}
\caption{Comparison of performance with increasing number of augmented samples. Results are for 10\% and 20\% of labeled data UCF101. We see that the performance increases initially, reaches a peak and slowly starts dropping.
    }
    \label{fig:comparison}
\end{figure}

\section{Other Selector Choices}
\label{selector}

The design of the selector is a crucial aspect of our model. We want the selector to be able to learn what makes a good pair of videos for mixing without actually having to mix every single pair.
However, for lower percentages of labeled data, we can generate all possible samples of semantic
classes and convert a state-of-the art frame selection model
(SMART) \cite{smart} to do sample importance instead of frame importance.
We also consider a simple baseline of using a discriminator network to pick only realistic samples. We report the results in Table~\ref{tab:selector}. Another approach was to randomly pick a certain amount of samples to train the classifier network. 

We not only outperform all alternative approaches, we also do this by saving on both memory and computation cost. For example, in the 20 percent setting, SMART sees
99K videos and these 99k videos have to be precomputed
and stored before training SMART. However, the proposed
approach only needs 12K videos and outperforms SMART
by up to 1.4\%. This analysis is only to show a comparison to possible alternatives when storing data is feasible. The idea of trying these alternatives is only
feasible in low percentage labeled data of small datasets like
UCF101 and HMDB51. Even 50\% labeled data in UCF101, results in having to mix over 400k videos while large scale datasets like Kinetics400 would lead to millions of mixes being needed making it practically unfeasible.  


\begin{table}[htb]
\begin{center}
    

\begin{tabular}{lllllllll}
              & \multicolumn{2}{c}{50\%}     & \multicolumn{2}{c}{20\%}     & \multicolumn{2}{c}{10\%}     & \multicolumn{2}{c}{5\%}      \\
Method        & Acc     & SS & Acc     & SS & Acc      & SS & Acc     & SS \\ 
\hline
Random        & 61.9          & 430K         & 56.2         & 99K          & 51.8          & 44K          & 42.3          & 9.7K         \\
Discriminator & 62.8          & 430K         & 57.3         & 99K          & 52.2          & 44K          & 41.1          & 9.7K         \\
SMART \cite{smart} & 68.9 & 430K & 58.9 & 99K & 57.8 & 44K & 46.5 & 9.7K \\
Proposed      & \textbf{72.1} & 39K          & \textbf{60.3} & 12K          & \textbf{56.1} & 5.2K         & \textbf{48.0} & 1.2K     \\   
\end{tabular}
\end{center}
\caption{Comparison of approaches for the use of Selector. All results are reported on UCF101. 'Acc' corresponds to accuracy and 'SS' corresponds to the number of mixed videos that the Selector looks at. All results are on different percentage of labeled data in UCF101.}
\label{tab:selector}
\end{table}

\section{Why Re-train the Classifier Network?}
\label{re-train}
Here, we are talking about the classifier network in our proposed architecture that the selector learns from (based on the validation loss).
Training the Selector and the Classifier together is also possible. But we decide against this for 2 reasons. First, and the most important reason is that we want to save out on computational cost needed to generate an augmented sample. We showed that the selector network looks only at a fraction of samples before it understands what makes a good pair. Hence, we first train the selector by generating augmented samples taken from random samples of semantically similar classes. Once the selector is trained, we don't need to generate the mixed sample for all possible pairs and only generate the mixed samples for good pairs (the selector need not have seen these pairs before). We then augment the original dataset by samples that the selector believes will improve the classifiers performance We compare the performance of the joint training and re-training of the classifier network in Table~\ref{tab:retrain}. We see that re-training the classifier network always yields the best performance.

\begin{table}[htb]
\begin{center}
\begin{tabular}{lllll}
Method        & 50\% & 20\% & 10\% & 5\%  \\
\hline
Jointly trained & 66.5 & 57.4 & 53.1 & 44.7 \\
Retrained  & \textbf{72.1} & \textbf{60.3} & \textbf{56.1} & \textbf{48.0} \\
\end{tabular}
\end{center}
\caption{Comparison of jointly training classifier and re-training it. We see that there is a consistent large improvement in re-training the classifier.}
\label{tab:retrain}
\end{table}




\section{Examples of Selected and Discarded Samples}
\label{examples}

To understand what made a good sample we visualize a few samples that were selected by the selector model and a few samples that were discarded. These can be seen in Figure~\ref{fig:select}. The samples are displayed as 4 frames for better visualization. Based on the small subset of examples seen, we believe that for good pairs to be selected some of the criteria could be coherent inpainting, similar camera movement, not too drastic a background change.

\begin{figure*}[]
    \centering
    \includegraphics[width=\textwidth]{./figures/selected_blur.png}
\caption{Visualizing selected examples. From top to bottom as (foreground, background) pairs: (flic-flac, cartwheel), (smile, laugh), (playing violin, playing cello), (front crawl, swimming backstroke). The first two are examples from HMDB51 and the last two from UCF101.
    }
    \label{fig:select}
\end{figure*}

We see some samples of discarded examples in Figure~\ref{fig:discarded}. Based on the small subset of examples seen we think possible bad pairs are due to bad video compositing (example 2 in Figure~\ref{fig:discarded}), varying camera movements (example 3 in Figure~\ref{fig:discarded}) or a drastic change in background (example 1 in Figure~\ref{fig:discarded}). These are however based on the few examples we see.

\begin{figure*}[]
    \centering
    \includegraphics[width=\textwidth]{./figures/discarded_blur.png}
\caption{Visualizing discarded examples. From top to bottom as (foreground, background) pairs: (somersault, diving), (climbing stairs, falling floor), (baby crawling, walking dog), (hammering, hammer throw). 
    }
    \label{fig:discarded}
\end{figure*}


\section{Effect of Semantic Match in generalization ability.}
\label{gen_ability}
 We test the generalization ability of the semantic matching by comparing it with random matching which would correspond to row 4 of Table 1 in the main paper. We observe that the performance does decrease. To strengthen this test, we tried the same experiment in the FSL setting, which is an extreme case for generalization. We augment data for two different methods, using the proposed L2A, using both semantic and random matching of classes. We observe that even in this setting, which is the most susceptible to overfitting, the semantic matching outperforms random matching. We will add this to the final version. 

\begin{table}[htb]
\centering
\resizebox{0.6\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|}
\hline
Method                        & Class Matching & 1-shot & 3-shot & 5-shot \\ \hline
C3D-PN & Random   &    28.1    &   42.9     &    47.7    \\ \hline
C3D-PN & Semantic &   29.9 &   44.5 &   50.8 \\ \hline
TRX & Random       &   33.5     &   49.9     &   60.3     \\ \hline
TRX & Semantic &    35.0 &  51.1 &  62.1   \\ \hline
\end{tabular}}
\caption{Results on FSL using the proposed Semantic Matching vs random matching using the TruZe \cite{truze} split.}
\end{table}

\section{Limitations and Future Work}

The main area of improvement is the time needed for training. Optimizing the Selector with RL is time-consuming, and so is compositing the initial samples for training it. Future work could address this by parameterizing the composition process and learn these parameters instead of compositing the pairs directly. It could also learn to select particular frames in a video, and avoid the computational cost of temporal redundancy. Finally, another possible direction is to learn what samples to discard from the initial dataset itself.








\section{Conclusion}

While standard data augmentation strategies in action recognition are hand-crafted, we propose to learn which pairs of videos are good to composite. In order to do this, our approach leverages three components. We train a Selector optimized with RL to choose which pairs of videos are good to composite. We reduce the search space by using samples from semantically similar classes. We perform a clean segmentation for mixing samples and remove actors as well as objects from foreground and background samples. With this, we obtain state-of-the-art results in semi-supervised and few-shot action recognition settings, and improve in the fully supervised setting. In particular, we see gains of up to 8.6\% and 3.7\% in the semi-supervised and few-shot settings. We also see an improvement of up to 17.4\% when compared to standard augmentation in the fully supervised setting when training from scratch.
\bibliographystyle{splncs04}
\bibliography{eccv}
\end{document}
