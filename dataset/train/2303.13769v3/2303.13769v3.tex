\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm, algorithmic}
\usepackage{multirow}
\usepackage{wasysym}
\usepackage{pifont}
\usepackage{threeparttable}
\usepackage{color}

\usepackage{tablefootnote}

\usepackage{sidecap}
\sidecaptionvpos{figure}{c}

\usepackage{paralist}
\let\itemize\compactitem
\let\enditemize\endcompactitem

\definecolor{xf}{RGB}{69,137,148}
\definecolor{yellow2}{RGB}{235,213,52}
\newcommand{\xf}[1]{{\color{black} #1}}
\newcommand{\xff}[1]{{\color{black} #1}}
\newcommand{\xfnew}[1]{{\color{black} #1}}
\newcommand{\que}[1]{{\color{black} #1}}
\newcommand{\lwt}[1]{{\color{black} #1}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{9294}
\def\confName{CVPR}
\def\confYear{2023}

\newcommand{\net}{UnSniffer\xspace}
\newcommand{\MyScore}{Generalized Object Confidence Score\xspace}
\newcommand{\myscore}{generalized object confidence score\xspace}
\newcommand{\score}{GOC scores\xspace}
\newcommand{\loss}{GOC-loss\xspace}
\newcommand{\ood}{unknown objects\xspace}


\begin{document}

\title{Unknown Sniffer for Object Detection: Don't Turn a Blind Eye\\ to Unknown Objects}

\author{Wenteng Liang, Feng Xue, Yihao Liu, Guofeng Zhong, Anlong Ming\\
Beijing University of Posts and Telecommunications, China\\
Chongqing University of Posts and Telecommunications, China\\
{\tt\small \{liangwenteng,xuefeng,l1h,mal\}@bupt.edu.cn}}

\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Equal Contribution}
\footnotetext[1]{Corresponding Author}

\footnotetext{This work was supported by the national key R \& D program intergovernmental international science and technology innovation cooperation project 2021YFE0101600.}

\begin{abstract}
The recently proposed open-world object and open-set detection \xf{have achieved} a breakthrough in finding never-seen-before objects and distinguishing them from \xf{known ones}.
However, their studies on knowledge transfer from known classes to unknown ones \xf{are not deep enough},
\xf{resulting in} the scanty capability for detecting unknowns hidden in the background.
In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects.
Firstly, the generalized object confidence (GOC) score is introduced,
which only uses \xf{known} samples for supervision and avoids improper suppression of unknowns in the background.
Significantly, such confidence score learned from \xf{known} objects can be generalized to unknown ones.
Additionally, we propose a negative energy suppression loss to further \xf{suppress} the non-object samples in the background.
Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training.
To solve this issue,
we introduce a graph-based determination scheme to replace hand-designed non-maximum suppression (NMS) post-processing.
Finally, we present the Unknown Object Detection Benchmark,
the first publicly benchmark that encompasses precision evaluation for \xf{unknown detection} to our knowledge.
Experiments show that our method is far better than the existing state-of-the-art methods.
Code is available at: \url{https://github.com/Went-Liang/UnSniffer}.


\end{abstract}

\section{Introduction}
\label{sec:intro}

Detecting objects with a limited number of classes in the closed-world setting \cite{ren2015faster,redmon2016you, liu2016ssd, he2017mask, lin2017feature, lin2017focal, redmon2017yolo9000, cai2018cascade, carion2020end, zhu2020deformable} has been the \xff{norm for} years.
Recently,
the popularity of autonomous driving \cite{pinggera2016lost, ramos2017detecting, dosovitskiy2017carla, 8126154, 9210191,8794279, janai2020computer, YuZhou-IJCV2016-SFVT, NIPS2012_3e313b9b, ZheLiu-AAAI2019-TANet, 2014ONLINE} has \xff{raised the bar} for object detection.
\xf{That is, the detector should \xff{detect} both known and unknown objects.
`\textbf{Known Objects}' are those that belong to pre-defined categories,
\xff{while `\textbf{Unknown Objects}'} are those that the detector has never seen during training.
Detecting unknown objects \xff{is crucial in coping} with more challenging environments,
such as autonomous driving scenes with potential hazards.}


\begin{figure}
\centering\includegraphics[width=0.97\linewidth]{./first_page_v16.pdf}
\caption{
(a)-(c) the predicted unknown (\textcolor{blue}{blue}), known (\textcolor{yellow2}{yellow}), and missed (\textcolor{red}{red}) objects of VOS \cite{vos}, ORE \cite{owod}, and our model.
(d) t-SNE visualization of various classes' hidden vectors.
(e) score for objects and non-object (generalized object confidence).
(f) score for unknown, known, and non-object (negative energy).
}
\vspace{-10pt}
\label{fig:first_page}
\end{figure}


Since unknown objects \xf{do not have labels} in the training set,
how to learn knowledge that can be generalized to unknown classes from finite pre-defined categories is the key issue in detecting unknown objects.
In recent years,
a series of groundbreaking works have been impressive on open-set detection (OSD) \cite{vos,gal2016dropout, miller2018dropout, du2022unknown} and open-world object detection (OWOD) \cite{owod, OWDETR, yang2021objects, wu2022uc}.
Several OSD \xf{methods} have \xf{used} uncertainty measures to distinguish unknown objects from known ones.
However, they primarily focus on improving the \xf{discriminatory} power of uncertainty and tend to suppress non-objects along with many potential unknowns in the training phase.
As a result, these methods \xf{miss many} unknown objects.
Fig. \ref{fig:first_page} (a) shows that VOS \cite{vos} misses many unknown objects, such as bags, stalls and surfboards.
Furthermore,
OWOD requires generating high-quality boxes for both known and unknown objects.
ORE \cite{owod} and OW-DETR \cite{OWDETR} collect the pseudo-unknown samples by an auto-labelling step for supervision
and perform knowledge transfer from the known to the unknown by contrastive learning or foreground objectness.
\xf{But} 
the pseudo-unknown samples are unrepresentative of the unknown objects, thus limiting the model's ability to describe unknowns.
Fig. \ref{fig:first_page} (b) shows that ORE \cite{owod} mis-detects many unknown objects,
even though some are apparent.




In philosophy,
there is a concept called `\emph{Analogy}' \cite{Ribeiro2014},
which describes unfamiliar things with familiar ones.
We argue that \textit{despite being ever-changing in appearance,
the unknown objects are often visually similar to the objects \xf{of} pre-defined classes},
as observed in Fig. \ref{fig:first_page} (d).
The t-SNE visualization shows that the unknown objects tend to be \xf{among} several pre-defined classes,
while the non-objects are far away from them.
\xf{This inspires} us to express a unified concept of `object' by the proposed generalized object confidence (GOC) score learned from the known objects only.
To this end,
we first discard the background bounding boxes and only collect the object-intersected boxes for training to prevent potential unknown objects from being \xf{classified} as backgrounds.
Then, a combined loss function is designed to enforce the detector to assign relatively higher scores to boxes tightly enclosing objects.
\xf{Unlike} `objectness',
non-object boxes are not used as the negative samples for supervision.
Fig. \ref{fig:first_page} (e) shows that the GOC score distinctly separates \xf{non-objects} and \xf{`objects'}.
In addition,
we design a negative energy suppression loss on top of VOS's energy calculation \cite{vos} to further widen the gap between the non-object and the `object'.
Fig. \ref{fig:first_page} (f) shows three distinct peaks for the \xf{knowns, unknowns and non-objects.}
Next,
due to the absence of the unknown's semantic information in training,
the detector hardly determines the best bounding box by a constant threshold when the number of objects cannot be \xf{predicted} ahead of time.
In our model,
the best box determination is modelled as a graph partitioning problem,
which adaptively clusters high-score proposals into several groups and selects one from each group as the best box.



As far as we know,
the existing methods are evaluated on the COCO \cite{lin2014microsoft} and Pascal VOC benchmarks \cite{voc} that do not thoroughly label unknown objects.
Therefore, the \xf{accuracy} of unknown object detection cannot be evaluated.
Motivated by this practical need,
we propose the Unknown Object Detection Benchmark (UOD-Benchmark),
which takes the VOC's training set as the training data and contains two test sets.
(1) COCO-OOD containing objects with the unknown class only;
(2) COCO-Mix with both unknown and known objects.
They are collected from the original COCO dataset \cite{lin2014microsoft} and annotated \xf{according to} the COCO's instance labeling standard.
In addition,
the Pascal VOC testing set is employed for evaluating known object detection.


Our key contributions \xf{can be} summarized as follows:

\begin{itemize}
\item To better separate non-object and `object',
we propose the GOC score learned from known objects to express unknown objects and design the negative energy suppression to further limit non-object.

\item The graph-based box determination is designed to adaptively select the best bounding box for each object \xf{during} inference for higher unknown detection precision.

\item We propose the UOD-Benchmark containing annotation of both known and unknown objects,
enabling us to evaluate the precision of unknown detection.
We comprehensively evaluate our method on this benchmark which facilitates future use of unknown detection in real-world settings.
\end{itemize}


\section{Related Work}
\label{sec:relatedwork}

\noindent
\textbf{Open Set Classification and Detection} aim to deal with unknown samples encountered in classification or detection tasks.
Many uncertainties measuring the feature difference between unknown and known objects have been proposed,
such as OpenMax \cite{bendale2016towards}, MSP \cite{hendrycks2016baseline}, ODIN \cite{liang2017enhancing}, Mahalanobis distance \cite{denouden2018improving} and Energy \cite{liu2020energy}.
For detection, some works \cite{gal2016dropout, miller2018dropout, miller2019evaluating} used Monte Carlo dropout to generate uncertainty scores.
David \emph{et al}. \cite{hall2020probabilistic} proposed probabilistic detection quality to measure spatial and semantic uncertainty. Du \emph{et al}. \cite{vos, du2022unknown} synthesized virtual outliers to shape the decision boundary of networks and used energy as an uncertainty measure.
However,
to ensure the accuracy of detecting known objects,
they suppress both unknowns and non-objects in training,
leading to a low recall of unknowns.
In contrast,
our method aims to detect all unknown objects.


\begin{figure*}
\centering
\includegraphics[width=1\linewidth]{./pipeline_v17.pdf}
\vspace{-16pt}
\caption{
\textbf{The framework of \net}
contains a feature extractor, a known object detector and a generalized object detector.
(a) is the input RGB image.
(b) visualizes several channels of deep features encoding the known and unknown objects at the same time,
and the red circles mark the position of the objects.
(c) shows the GOC score's distribution before and after training the GOC.
(d) shows the negative energy's distribution before and after using negative energy suppression.
(e) is the result.
}
\vspace{-10pt}
\label{fig:pipeline}
\end{figure*}

\noindent
\textbf{Open-world object detection (OWOD)} is proposed by ORE \cite{owod}.
It detects both known and unknown objects by training pseudo-labeled unknown objects and incrementally learns updated annotations of new classes.
OW-DETR \cite{OWDETR} improves performance with multi-scale self-attention and deformable receptive fields. Yang \emph{et al}. \cite{yang2021objects} introduced semantic topology to ensure that the feature representations are discriminative and consistent.
UC-OWOD \cite{wu2022uc} also classifies unknown objects to achieve better results than ORE on measures about unknown classes.
Zhao et al. \cite{rowod} correct the auto-labeled proposals by Selective Search and calibrate the over-confident activation boundary by a class-specific expelling function.
However, the auto-labeling step generates many pseudo-unknown samples that are unrepresentative of the unknowns in fact,
limiting their ability to transfer knowledge from the known to the unknown.
Thus, during inference, many non-objects are mis-detected as unknown,
leading to the low precision of unknown.
This paper aims to reduce the false positives by the proposed GOC score and a graph-based box determination scheme.


\section{Problem Formulation}


Referring to \cite{vos},
the problem of unknown detection in the setting of object detection is formulated as follows.
We have a known class set  and an unknown class .
The  input RGB images are denoted as ,
with corresponding labels .
Each  contains a set of object instances with ,
where  is the class label for a bounding box represented by .
If  encloses a known object, , otherwise .

The model is trained on the data containing known-class objects only ,
but tested on the data including unknown objects ,
where  is the image number of the training set,
 for that of the test set,
and .


\section{Method}
\label{sec:method}

We propose the unknown sniffer (UnSniffer) to find both the known and unknown objects.
The pipeline is shown in Fig. \ref{fig:pipeline}.
The RGB image 
is fed into a feature extractor \cite{ren2015faster} that captures numerous object proposals  and their feature vectors .
Taking the feature  as input,
we use two detectors for known and unknown objects. 

Firstly,
the generalized object detector learns the proposed generalized object confidence (GOC) score to determine whether proposal  contains an object (See Sec. \ref{sec:MyScore}).
Then, the graph-based box determination scheme is used to cluster the high-score proposals into several groups (See Sec. \ref{sec:Ncut}).
We select the one with the highest GOC score in each group \xf{as} a set of unknown predictions.

The second one, i.e., a known object detector,
computes the class-specific probabilities
and the negative energy score \cite{vos} for .
In addition to the classification head and box regression head commonly used in two-stage object detectors \cite{ren2015faster,he2017mask, lin2017feature,cai2018cascade},
we employ the virtual outliers synthesis \cite{vos} to learn energy scores
and remove the low-negative-energy proposals \xf{during} inference.
Unlike \cite{vos},
we employ a negative energy suppression loss to enforce the negative energy scores of non-object boxes less than zero (See Sec. \ref{sec:Energy_Suppression}).
It \xf{lowers} the feature response inside non-object boxes and boosts the discriminative power of both detectors.

Finally,
the first detector outputs the bounding box predictions of unknown class,
and the second detector gives that of known class.
We directly concatenate the two results and remove the unknown-class predictions whose IoU with any known-class prediction exceeds a constant threshold .
Fig. \ref{fig:pipeline} (e) shows the merged result of image .

Note that the UnSniffer has two training stages,
which \xf{are} consistent with VOS \cite{vos}.
In the first stage,
we employ the training process of Faster-RCNN \cite{ren2015faster}
(the red dot arrows in Fig. \ref{fig:pipeline}),
where  and  are the losses for classification and bounding box regression, respectively.
And the second stage additionally employs the losses proposed in this paper (the green dot arrows in Fig. \ref{fig:pipeline}).



\subsection{\MyScore}
\label{sec:MyScore}

\noindent\textbf{Uncertainty \xf{Scores}} \xf{are} usually modeled as either the maximum known-class conﬁdence \xf{scores} \cite{hendrycks2016baseline,liang2017enhancing} or the entropy of the classification results \cite{liu2020energy, vos, du2022unknown}.
It can be used to distinguish unknowns from known objects according to the high uncertainty \xf{scores} of the unknown objects.
However,
the uncertainty's training phase suppresses both unknowns and non-objects,
causing the inadequate detection of unknowns.



\noindent\textbf{Objectness \xf{Scores}} \xf{are} usually used to judge whether a bounding box containing an object \cite{OLP,edgeboxes,ren2015faster},
which naturally meets the requirement of unknown object detection,
such as the foreground objectness learning in OW-DETR \cite{OWDETR} implemented by a binary classification.
However,
learning-based object proposal methods cannot avoid \xf{a} misuse of unknown samples as negative samples,
leading to low discriminative power between non-objects and unknowns.

\noindent\textbf{Generalized Object Confidence Score and Losses.}
We propose the generalized object confidence (GOC) score.
\xf{It can be used to judge} whether a proposal contains an object (including unknown and known classes),
while this capability stems from the fact that many unknowns are actually encoded by the pre-trained backbone,
as the `flag' shown in Fig. \ref{fig:pipeline}(b).

Different from uncertainty and objectness,
the GOC score is trained using only known objects and can be generalized to unknown objects.
Specifically,
the GOC regression head that is composed of a linear transformation, denoted as , is used to compute the GOC score  for a given proposal's feature .
In the training phase,
given an image-label pair ,
the region proposal network is firstly used to extract numerous proposals  from image .
And we define the intersection over the predicted bounding box () and \xf{the} intersection over the correct bounding box () for collecting training samples from  as follows:
\vspace{-6pt}

where  is the -th instance's bounding box in .
Subsequently, for each proposal  in ,
we find the object instance \xf{that has} the maximum IoU with .
And the proposals enclosing the same object are assigned to the same group,
obtaining  groups of proposals: .
Then,
we divide the proposals of  into \textbf{c}omplete-object, \textbf{p}artial-object, \textbf{o}versized, and \textbf{n}on-object according to IoU, IoP, and IoC,
as shown in Fig. \ref{fig:box}:

where  are the constant thresholds,
as shown in Fig. \ref{fig:box}.
In order to prevent the potential \ood from being \xf{treated} as background during training,
we only use the first three groups in Eq. \ref{eq:S_plsn} to train the module  \xf{with} three losses.
In the first loss,
the GOC scores of complete-object bounding boxes are pushed towards one:


\begin{figure}
\centering
\includegraphics[width=1\linewidth]{./box.pdf}
\caption{The sample definition in GOC supervision.}
\vspace{-10pt}
\label{fig:box}
\end{figure}

Then, due to the lack of clear criteria measuring GOC scores of partial-object or oversized boxes,
we suppress their GOC scores to below a constant :
\begin{figure*}
\centering
\includegraphics[width=0.95\linewidth]{./NCut_v3.pdf}
\vspace{-5pt}
\caption{The illustration and more examples for the graph-based top-scoring box determination.
The white rectangles denote the proposals with top GOC scores in the image.
For other rectangles,
each group of proposals is represented by the same color.}
\vspace{-10pt}
\label{fig:ncut}
\end{figure*}

{\setlength\abovedisplayskip{0.18cm}
\setlength\belowdisplayskip{0.18cm}
}where .
Next,
we improve the model's ability to capture a box enclosing an object more entirely by a contrastive loss,
which compares two boxes in :
{\setlength\abovedisplayskip{0.18cm}
\setlength\belowdisplayskip{0.18cm}
}where  when ,
otherwise .
 is a tiny constant that is set to 0.01,
and .
Finally, the total GOC loss is formulated as:
{\setlength\abovedisplayskip{0.18cm}
\setlength\belowdisplayskip{0.18cm}
}

Since our training process does not utilize the sample of the background area,
the GOC scores of non-object bounding boxes would not be affected greatly.
On the contrary, the GOC scores of both unknown and known objects are pushed to a high score.
As shown in Fig. \ref{fig:pipeline} (c),
when the GOC is not supervised,
unknown boxes have the same output as non-object boxes,
but it changes dramatically when the GOC regression head is supervised by .



\subsection{Graph-based Top-scoring Box Determination}
\label{sec:Ncut}
By using the GOC score for ranking proposals during inference,
we obtain the proposals where the objects are most likely to be.
However,
the traditional post-processing mechanism, i.e. using NMS and outputting top- highest results, is inappropriate to determine the unknown prediction,
as the number of objects cannot be prophesied at ahead of time.

To address this issue,
we perform the top-scoring box determination as a graph partitioning problem,
which adaptively finds the best bounding box for each object,
as shown in Fig. \ref{fig:ncut}(a).
Specifically,
during the inference,
given a set of proposals  and their GOC scores  for image ,
we construct a weighted undirected graph .
Each node in set  represents an object proposal ,
and each edge in set  is formed by the IoU between both ends of this edge, i.e. .
As shown in Fig. \ref{fig:ncut}(a),
considering that some of the proposals may only cover part of the objects,
we employ the recursive two-way normalized cut algorithm \cite{ncut} to decompose the entire graph  into several sub-graphs iteratively,
which is terminated until the NCut value \cite{ncut} of a sub-graph is lower than a threshold ,
where  is determined by a threshold selection method in Sec. \ref{sec:imp}.
Finally, the top- GOC proposal of each sub-graph is taken as the prediction.

It can be seen from Fig. \ref{fig:ncut}(b),
even if only IoU is used as the measurement of edge in graph ,
our model still performs considerable proposal clustering and avoids outlier proposals as an independent group.
Especially in the upper right prediction of Fig. \ref{fig:ncut}(b),
almost every elephant gets an independent group of proposals.



\subsection{Negative Energy Suppression for Non-object}
\label{sec:Energy_Suppression}

Referring to VOS \cite{vos},
the energy score is employed to distinguish unknown objects from known ones,
as shown in Fig. \ref{fig:pipeline}.
For proposal ,
the energy score is formulated as the negative of the weighted sum of this proposal's logit output in exponential space:

where  is the logit output for class  in the classification head,
 is the number of the know classes,
and  is the learnable parameter for alleviating the class imbalance.
The proposals with higher negative energy scores are treated as known predictions,
\xf{whereas} others are unknown predictions.
However, due to insufficient training,
some non-object proposals gain such high negative energy scores that they are indistinguishable from objects, as shown in the left plot of Fig. \ref{fig:pipeline}(d).
To address this issue,
we propose negative energy suppression to further reduce the negative energy scores of non-object proposals.
Specifically,
we observe that most non-object boxes have lower negative energy scores than \xf{those} of the known and unknown classes,
which motivates us to design a suppression loss to constraint  proposals with the lowest negative energy scores:
\vspace{-5pt}


The overall energy loss consists of our proposed  and  defined by VOS \cite{vos}:
\vspace{-5pt}


As shown in the right plot of Fig. \ref{fig:pipeline}(d),
after training with loss ,
the negative energy distribution of the non-object is significantly different from that of the unknown,
indicating that the non-objects are indeed suppressed.
In addition,
by using Eq. \ref{eq:loss_suppression},
the feature responses of non-object bounding boxes are reduced simultaneously,
which further widens the GOC difference between non-object and object.
It can be proved by the fact that the high GOC scores of unknowns in Fig. \ref{fig:first_page}(e) (with ) are more than that in the right plot of Fig. \ref{fig:pipeline}(c) (without ).
In addition,
the ablation studies of Sec. \ref{sec:Analysis} demonstrate that this approach improves the detection precision of the model.

\section{Unknown Object Detection Benchmark}
\label{sec:experimental}

\subsection{Datasets}
In the proposed UOD-Benchmark,
we refer to \cite{vos,owod} 
\xf{and use} the Pascal VOC dataset \cite{voc} as the training data that contains annotations of 20 object categories.
For testing,
since the MS-COCO dataset \cite{lin2014microsoft} extends the PASCAL VOC categories to 80 object categories,
we naturally employ MS-COCO \xf{to evaluate} unknown objects.
However, MS-COCO does not thoroughly label potential unknown objects in images.
To address this issue,
we propose two datasets, i.e., COCO-OOD and COCO-Mixed, which fully \xf{label} the unknown objects.
Firstly,
according to the definition of objects in COCO \cite{lin2014microsoft},
i.e. ``objects are individual instances that can be easily labelled (person, chair, car)'',
we hand-pick more than a thousand images that have no area confused with this definition.
Secondly,
several master students are asked to mark the object regions they got at first glance by drawing polygons,
referring to the object definition above.
As shown in Fig. \ref{fig:dataset},
we label almost every object in the selected images with fine-grained annotation.
Finally, we have two datasets both for testing as follows:

\noindent\textbf{COCO-OOD} dataset \xf{contains only} unknown categories,
consisting of 504 images with ﬁne-grained annotations of 1655 unknown objects.
All annotations consist of original annotations in COCO and the augmented annotations on the basis of the COCO definition.



\noindent\textbf{COCO-Mixed} dataset includes 897 images with annotations of both known and unknown categories. 
It contains 2533 unknown objects and 2658 known objects, with original COCO annotations used as labels for known objects. 
Unambiguous unlabeled objects are also annotated. 
The dataset is more challenging to evaluate due to the images containing more object instances with complex categories and concentrated locations.


\begin{table}[t!]
\centering
\begin{tabular}{c|c|c c}
\toprule
Datasets               & Images & Known        & Unknown\\ \midrule
VOC-Pretest            & 200    & 5.09         & 0      \\
VOC-Test               & 4952   & 3.02         & 0      \\
COCO-OOD   & 504    & 0            & 3.28   \\ 
COCO-Mixed & 897    & 2.96         & 2.82   \\\bottomrule
\end{tabular}
\caption{\textbf{The Statistics of datasets} \xf{that include} the number of images, the average number of known and unknown instances per image.
 denotes the augmented datasets.}
\vspace{-7pt}
\label{tab:datasets}
\end{table}



In addition, we employ the test set of the Pascal VOC dataset to evaluate the accuracy of known object detection.
The statistics of these test datasets are placed in Table \ref{tab:datasets}. Fig. \ref{fig:dataset} shows some fully annotated images in COCO-OOD and COCO-Mixed.
Note that the VOC-Pretest is used to set a threshold,
mentioned in Sec .\ref{sec:imp}.

\subsection{Evaluation Metrics}


To evaluate the performance of known object detection,
we employ a prevalent metric,
i.e., mean Average Precision (mAP) \cite{vos}.
As for the unknown detection performance,
assuming that  denotes the true positive proposals of unknown classes,
 for false negative proposals,
and  for false positive proposals,
five metrics are employed:
\begin{itemize}
    \item The Unknown Average Precision (U-AP) is used with reference to the conventional object detection \cite{voc}.
    \item The Recall Rate (U-REC) and Precision Rate of Unknown (U-PRE) are defined \xf{as follows} .
    \item For a comprehensive comparison,
    we report the Unknown F1-Score defined as the harmonic mean of U-PRE and U-REC: .
    \item The Absolute Open-Set Error (A-OSE) \cite{miller2018dropout} is also employed to report the number count of unknown objects that are wrongly classiﬁed as any known classes.
    \item Wilderness Impact (WI) \cite{dhamija2020overlooked} are defined as  to characterize the case that unknown objects are confused with the known.
\end{itemize}

\xf{Note} that we measure mAP over different IoU thresholds from 0.5 to 0.95.
Other metrics, such as U-REC, U-PRE, etc., are measured at the IoU threshold of 0.5. WI is measured at a recall rate of 0.8.


\section{Experiment}
\subsection{Implementation Details}
\label{sec:imp}
We use the Detectron2 \cite{wu2019detectron2} library and employ a ResNet-50 \cite{he2016deep} backbone.
 in Eq. \ref{eq:negloss} is empirically set to 0.5.
 is set to 0.98.
We set the thresholds  in Eq. \ref{eq:S_plsn} to 0.0,
and  to 0.5 by parameter experiments.
The proposal number  is set to 100 in Eq.\ref{eq:loss_suppression}.
We train the model for a total of 18,000 iterations.
The starting iteration of our second stage is 12000,
which is consistent with VOS \cite{vos}.

\begin{figure}[t!]
\includegraphics[width=1\linewidth]{./dataset.pdf}
\caption{Annotated samples in COCO-OOD and COCO-Mix.}
\vspace{-8pt}
\label{fig:dataset}
\end{figure}



\begin{table*}
\centering
\begin{threeparttable}
\setlength{\tabcolsep}{1.3mm}{
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|c|cccc|ccccccc}
\toprule
\multirow{2}{1cm}{Groups}&\multirow{2}{2cm}{Methods} & VOC-Test & \multicolumn{4}{c|}{COCO-OOD} & \multicolumn{7}{c}{COCO-Mix} \\ 
\cline{3-14} 
& & mAP & U-AP & U-F1 & U-PRE & U-REC & mAP & U-AP & U-F1 & U-PRE & U-REC & AOSE & WI \\
\hline
\ding{172} &Faster-RCNN \cite{ren2015faster} & 0.483 & - & - & - & - & - & - & - & - & - & - & -\\
 \hline
\multirow{3}{1cm}{\centering\ding{173}} & MSP\cite{hendrycks2016baseline} & 0.470 & 0.213 & 0.314 & 0.279 & 0.359 & 0.364 & 0.055 & 0.169 & 0.190 & 0.153 & 588 & 0.135\\
&Mahalanobis \cite{denouden2018improving} &0.447 &0.129 &0.271  & 0.309 &0.241 & 0.351 & 0.051 & 0.149 & 0.207 & 0.116 & 604 & 0.165 \\
&Energy score \cite{liu2020energy} &\underline{0.474} &0.213 &0.308  & 0.260 &0.377 & 0.364 & 0.048 & 0.169 & 0.167 & 0.171 & 470 & 0.137\\
\hline
\multirow{2}{1cm}{\centering\ding{174}} &OW-DETR \cite{OWDETR} &0.420 &0.033  &0.056 &0.030 &0.380 &\textbf{0.414}  &0.007  &0.025 &0.014  &0.161  &569 & \textbf{0.086}\\
&ORE \cite{owod} & 0.243 & \underline{0.214} &0.255 & 0.153 & \textbf{0.782} & 0.213 & \underline{0.140} & \underline{0.175} & 0.103 & \textbf{0.592} & 485 &\underline{0.089}\\
\hline
\multirow{2}{1cm}{\centering\ding{175}} &VOS \cite{vos} & \textbf{0.485} & 0.135 &0.196 & \underline{0.342} & 0.137 & \underline{0.377} & 0.040 &0.101 & \textbf{0.262} & 0.062 & 640 & 0.152\\
&VOS \cite{vos} & 0.469 & 0.205 &\underline{0.317} & 0.291 & 0.348 & 0.364 & 0.051 & 0.172 & 0.184 & 0.163 & \underline{409} &0.124 \\
\hline
\ding{176}&Ours & 0.464 & \textbf{0.454} &\textbf{0.479} & \textbf{0.433} & \underline{0.535} & 0.359 & \textbf{0.150} &\textbf{0.287} & \underline{0.222} & \underline{0.409} & \textbf{398} & 0.175\\
\bottomrule
\end{tabular}}}
\vspace{-0.5em}
\caption{Comparisons with the traditional detector \ding{172} and detectors using open-set classification \ding{173},
open-world object detection \ding{174},
and open-set detection \ding{175} methods. 
VOS\tnote{1} means using the threshold in the official repository,
calculated on the BDD100K dataset \cite{yu2020bdd100k}.
VOS\tnote{2} means using the threshold computed on the COCO-OOD dataset by the official code.
Best results are in bold,
second best are underlined.}
\begin{tablenotes}
\scriptsize
\item[1] https://github.com/deeplearning-wisc/vos/issues/26
\item[2] https://github.com/deeplearning-wisc/vos/issues/13
\end{tablenotes}
\end{threeparttable}
\label{tab:all_result}
\vspace{-0.5em}
\end{table*}

\noindent\textbf{Determining inference thresholds in pretest mode.}
Both VOS \cite{vos} and OWOD \cite{owod} determine \xf{the} threshold before inference,
but they bring in unknown data in an implicit or explicit way when computing the thresholds.
Therefore, we introduce a pretest operation before inference,
which \xf{selects 200 images from} the training set that do not contain any potential unknown objects for threshold determination.
The first row of Table \ref{tab:datasets} shows the statistics of the pretest dataset.
In the pretest mode,
we firstly obtain the negative energy score of the proposals predicted from the pretest dataset,
and set the threshold  such that 95\% of predicted proposals have a negative energy score greater than it.
Then,
for the graph-based box determination,
we set 10 thresholds of NCut value equally spaced from 0 to 1,
and choose the threshold when the AP of known objects is the largest as .


\begin{table}
\small
\centering
\setlength{\tabcolsep}{1.2mm}{
\begin{tabular}{c|ccc|cccc}
\toprule
Row & GOC & NES & GBD &U-AP &U-F1 & U-PRE & U-REC \\
\midrule
1 &  &   &  &0.066 &0.050 &0.026 &0.808 \\
2 &  &   & \checked &0.250 &0.434 &0.395 &0.481 \\
3 &  & \checked  &  &0.442 &0.054 &0.028 &0.861 \\
4 & \checked &   &  &0.479 &0.323 &0.215 &0.646 \\
\midrule
5 &  & \checked  & \checked &0.409 &\underline{0.467}&\textbf{0.437} &0.502 \\
6 & \checked &   & \checked &\underline{0.455} &0.454 &0.399 &0.528 \\
7 & \checked & \checked  &  & \textbf{0.474} & 0.342 & 0.234 & \textbf{0.632} \\
8 & \checked & \checked  & \checked &0.454 &\textbf{0.479} &\underline{0.433}  &\underline{0.535} \\
\bottomrule
\end{tabular}
}
\vspace{-0.5em}
\caption{
\textbf{Ablation studies on COCO-OOD}.
GOC, NES and GBD refer to `generalized object confidence', `negative energy suppression' and `graph-based box determination', respectively.
If `GBD' is , we use NMS+top- with \xf{the} threshold of the known detector.}
\vspace{-1em}
\label{tab:Ablation}
\end{table}


\subsection{Results}

\noindent\textbf{Quantitative Analysis.}
In Table \ref{tab:all_result},
we show \net's result on the UOD-Benchmark,
along with the results of MSP \cite{hendrycks2016baseline}, Mahalanobis \cite{denouden2018improving}, Energy score \cite{liu2020energy}, ORE \cite{owod}, OW-DETR \cite{OWDETR} and VOS \cite{vos}.
Note that OW-DETR is based on Deformable DETR \cite{zhu2020deformable} with a stronger discriminative power,
while other methods use Faster-RCNN.
Since U-PRE or U-REC cannot independently reflect the model's performance,
we mainly employ U-AP, U-F1, AOSE and WI.
Observably,
on the COCO-OOD dataset, the U-AP of UnSniffer outperforms the 2 result by more than twice, and our U-F1 is 16.2\% higher than the 2 result,
at the cost of a 1.9\% drop in mAP on VOC compared to Faster-RCNN \cite{ren2015faster}.
On the COCO-Mix dataset,
the UnSniffer still holds the lead in both U-AP and U-F1,
which are 1\% and 11.2\% higher than the 2 results, respectively.
Those comparisons demonstrate that UnSniffer outperforms the existing methods in unknown object detection,
which \xf{is owed to} to our GOC learning the overall confidence of objects from finite known objects.
Furthermore, UnSniffer has the smallest AOSE (398) but the largest WI (0.175),
which can be explained by the inverse relationship between WI and the count of known objects misclassified as an incorrect class.
More details are illustrated in the supplementary material.



\begin{figure}
\centering
\includegraphics[width=1\linewidth]{./sampler_param.pdf}
\vspace{-2em}
\caption{
\textbf{Sensitivity analysis on (a) thresholds , and (b) threshold }.
 indicates the failed training outside this threshold.
}
\vspace{-0.6em}
\label{fig:samplingandstrategies}
\end{figure}

\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\textwidth]{./vis_main_v7.pdf}
\vspace{-1.5em}
\caption{\textbf{Example results on COCO-Mix (first two rows) and COCO-OOD datasets (last three rows)}.
1 column: ground truth;
2-8 columns:
MSP \cite{hendrycks2016baseline}, Mahalanobis \cite{denouden2018improving}, Energy score \cite{liu2020energy}, OW-DETR \cite{OWDETR}, ORE \cite{owod}, VOS \cite{vos} (with threshold computed on COCO-OOD dataset), and our method.
The detections are overlaid on the known (\textcolor{yellow2}{yellow}) and unknown (\textcolor{blue}{blue}) class objects.
Since ORE and OW-DETR generate too many results,
we only draw the top- boxes for each image \xf{, and} other methods draw all predicted boxes.}
\label{fig:Qualitative}
\vspace{-1.1em}
\end{figure*}


\noindent\textbf{Qualitative Analysis.}
\label{sec:Qualitative}
Fig. \ref{fig:Qualitative} visualizes the results of different methods on example images of the COCO-Mix (first two rows) and COCO-OOD dataset (last three rows).
It can be seen that VOS \cite{vos}, MSP \cite{hendrycks2016baseline}, Mahalanobis distance \cite{denouden2018improving}, and Energy score \cite{liu2020energy} miss many objects of the unknown class,
such as the surfboards in the  image,
the keyboard and water cup in the  image,
the CD case in the  image.
Their failure is due to the suppression of unknown objects in training.
For the OWOD methods,
ORE and OW-DETR generate too many predicted object boxes on almost all images.
Most predictions are false positives.
In contrast,
UnSniffer does not miss any unknown objects because we give reasonable GOC scores to all unknown objects.
Moreover, using graph-based box determination,
UnSniffer reliably predicts a single bounding box for each object even if two or more objects overlap (See the 4 row).
More results are available in the supplementary material.



\begin{figure}
\centering
\includegraphics[width=1\linewidth]{./ncut_energy_param_v5.pdf}
\vspace{-2em}
\caption{
(a) Comparison between the thresholds  determined in the pretest set and the COCO-OOD dataset. 
(b) Validation of the threshold  computed in pretest mode on the COCO-Mix dataset.
}
\vspace{-14pt}
\label{fig:pretest}
\end{figure}

\subsection{Discussions and Analysis}
\label{sec:Analysis}
\noindent\textbf{Ablation Study.}
To investigate the contribution of each component in \net,
we design ablation experiments in Table \ref{tab:Ablation}.
Since the softmax in known object detector fails to provide reasonable confidences for unknowns,
when the 'GOC' is ,
we employ the negative energy score for ranking the unknown proposals.
Comparing the 1 and 2 rows,
it can be seen that the GBD outperforms NMS,
and GBD plays a key role in improving detection precision.
Comparing the 1 and 4 rows,
obviously,
GOC helps to increase U-AP by 41.3\%,
which shows that the GOC score well measures the probability that a proposal belongs to an object.
Using both NES and GBD (5 row) results in good U-PRE performance,
indicating the effectiveness of NES in reducing false-positive unknown objects.
Finally, using all modules achieves high U-PRE and U-REC simultaneously.


\noindent\textbf{Parameter sensitivity analysis of GOC sampling.}
We adjust the thresholds  in Eq. \ref{eq:S_plsn}, respectively.
And the results on the COCO-OOD dataset are shown in Fig. \ref{fig:samplingandstrategies}.
The best result is achieved when , , and .
Note that, when  exceeds 0.4,
the network cannot be successfully trained.
When  is lower than 0.2,
the training cannot converge,
which means that the contrastive loss  is unsuitable for supervising too many samples.



\noindent\textbf{Effectiveness of pretest-based threshold determination.}
As shown in Fig. \ref{fig:pretest} (a),
the optimal threshold of  determined by pretest data is almost the same as that determined by traversing the COCO-OOD dataset.
Fig. \ref{fig:pretest} (b) shows the curve of mAP and AOSE when using different .
Note that  is determined to be 4.5 in the pretest data.
When  is equal to 4.5,
the mAP loss of known objects is small,
but the AOSE is largely reduced.
It shows that UnSniffer largely retains the ability to detect known objects,
meanwhile effectively alleviating the false detection of the unknown objects as a known class.
These comparisons prove that the pretest mode is suitable for determining the threshold of an unknown object detector.
And it only uses part of the training data without any risk of leaking test data.



\section{Conclusion}
\label{sec:conclusion}
To meet the real-world requirements for perceiving known and unknown objects,
the \net is designed.
Firstly,
we design the GOC score that reliably measures the probability of a box that contains an object.
Then, we model the top-scoring box determination as graph partitioning to obtain the best box for each object.
Thirdly,
the proposed negative energy suppression further limits the non-object boxes.
Finally, we introduce the UOD-Benchmark to more comprehensively evaluate the real-world usability of the model.
We hope our work inspires future research on unknown object detection in real-world settings.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{UnSniffer_CameraReady_v2}
}

\end{document}
