

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}



\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{16} \def\confName{EarthVision}
\def\confYear{2023}


\begin{document}

\title{DINO-MC: Self-supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops}

\author{Xinye Wanyan\\
{\tt\small xwanyan@student.unimelb.edu.au}
\and
Sachith Seneviratne\\
{\tt\small sachith.seneviratne@unimelb.edu.au}
\and
Shuchang Shen \\
{\tt\small chuchangs@student.unimelb.edu.au}
\and
Michael Kirley \\
{\tt\small mkirley@unimelb.edu.au}
}
\maketitle

\begin{abstract}
Due to the costly nature of remote sensing image labeling and the large volume of available unlabeled imagery, self-supervised methods that can learn feature representations without manual annotation have received great attention.
While prior works have explored self-supervised learning in remote sensing tasks, pretext tasks based on local-global view alignment remain underexplored.
Inspired by DINO \cite{caron2021emerging}, which employs an effective representation learning structure with knowledge distillation based on global-local view alignment, we formulate two pretext tasks for use in self-supervised learning on remote sensing imagery (SSLRS).
Using these tasks, we explore the effectiveness of positive temporal contrast as well as multi-sized views on SSLRS. 
Moreover, we extend DINO and propose DINO-MC which uses local views of various sized crops instead of a single fixed size.
Our experiments demonstrate that even when pre-trained on only 10\% of the dataset, DINO-MC performs on par or better than existing state of the art SSLRS methods on multiple remote sensing tasks, while using less computational resources.
All codes, models and results are available at 
\url{https://github.com/WennyXY/DINO-MC}.


\end{abstract}













\section{Introduction}
\label{sec:intro}
Computer vision models have been widely used in remote sensing to solve numerous real-world challenges, including disaster prevention \cite{2000remote}, forestry \cite{2021Review}, agriculture \cite{Mulla2013Twenty}, land surface change \cite{2021A}, biodiversity \cite{turner2003remote}.
Generally, the models with deeper and more complex structures are able to extract more useful features from images, leading to superior performance in a wide range of applications.
However, training large-scale computer vision models in a traditional supervised manner always requires large labeled datasets, which are costly and error-prone, especially in remote sensing domain \cite{stojnic2021self}.
Therefore, reducing the reliance of the model on labeled images is crucial for resolving specific downstream tasks.
The self-supervised learning (SSL) paradigm is a common solution which is able to train a general feature extraction model on unlabeled datasets.
SSL is conducted in two phases: first, the model is pre-trained to learn general latent representations by solving complex pretext tasks. Then, the pre-trained model is fine-tuned on downstream tasks.
Since different pretext tasks make the model learn different aspects of features, designing a suitable task whose labels can be automatically obtained from the data is one of the keys to SSL.

According to the pretext tasks, SSL models can be classified into three categories.
Generative tasks allow the model learn feature representations by reconstructing or generating original images.
For example, the image inpainting \cite{pathak2016context} uses the original data as labels to train the model to recover several masked parts of the original image.
The discriminative task \cite{chen2020simple,dosovitskiy2014discriminative} trains the network to distinguish a set of categories.
For example, \cite{doersch2015unsupervised} trains the model to predict the relative position of a patch to its neighbors.
It is an eight-label classification task in which each image patch is defined to have up to eight adjacent patches.
However, the feature representations learned by generative and discriminative methods are highly dependent on the used pretext task, and an ineffective pretext task might reduce the transfer-ability of a pre-trained model \cite{wang2022selfreview}.
In stead of solving a single pretext task, contrastive approaches train models by maximising the similarity between the feature representations of two positive samples, e.g., two augmented views of the same instance. 
However, simply following this approach will easily generate an identity map for each pair of positive samples, i.e., model collapse \cite{wang2022selfreview}.

DINO \cite{caron2021emerging}, a state-of-the-art contrastive self-supervised model, utilizes knowledge distillation and centering
and sharpening of the teacher network \cite{hinton2015distilling} to handle this issue.
DINO has shown impressive performance in numerous computer vision tasks, including image retrieval, copy detection and video instance segmentation. 
While some previous work has applied DINO to remote sensing domain \cite{wang2022last,seneviratne2021self,wang2022selfSAR}, a thorough evaluation and extension of the self-supervised objective for remote sensing imagery has not attempted. 
In particular, the size of instances observed during pre-training on traditional natural scenes shows wider variation than seen in remote sensing imagery. 
Hence, the alignment of the latent representation of the multi-sized local augmented crops against the global augmented views is of interest in remote sensing imagery as it leads to a more challening pretext task.


In SeCo \cite{manas2021seasonal}, the temporal information plays a crucial role in learning transferable features from remote sensing imagery, building primarily on the contrastive model MoCo-V2 \cite{chen2020improved}. 
In this work, we explore whether using temporal views as positive instances can further improve the performance of DINO in remote sensing domain.
Furthermore, inspired by the inherent characteristics of the size variation of semantic content observed between traditional natural scenes and remote sensing imagery, we propose DINO-MC which uses size variation in local crops to drive better representation learning of the semantic content of remote sensing imagery.
We evaluate our representations on different backbones including two transformer networks and two convnets.
While most existing self-supervised methods for remote sensing employ ResNet and Vision Transformers (ViTs) as backbone models, the self-supervised feature extraction potential of Wide ResNets (WRN) and Swin Transformers is of particular interest in remote sensing, and we thus include them in our analysis.
In the linear probing evaluation, the findings demonstrate that DINO-MC has great transfer-ability, as it achieves 2.56\% higher accuracy with a smaller pre-trained dataset than SeCo.
Besides, DINO-MC outperforms DINO and SeCo when fine-tuned on two remote sensing classification tasks as well as on change detection (a segmentation task).

In conclusion, our main contributions are summarized as follows:
\begin{itemize}
\item[] We apply temporal views as positive instances to recent contrastive self-supervised models (DINO-TP). We analyze different backbone networks to explore their effectiveness on different remote sensing tasks when pretrained under this setting.
\item[] We combine a new multi-sized local cropping strategy with DINO and propose DINO-MC. We pre-train DINO-MC with different backbones on a satellite imagery dataset SeCo-100K to learn a general representations.
\item[] DINO-MC outperforms SeCo with only 10\% pre-training dataset, and achieves state-of-the-art results on BigEarthNet multi-label and EuroSAT multi-class land use classification, as well as OSCD change detection task.
\end{itemize}

\begin{figure*}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/DINO.png}
  \caption{
  \textit{DINO}: the self-supervised contrastive algorithm with knowledge distillation.
  It is the basic structure of both DINO-TP and DINO-MC.
  For DINO-TP, we use three temporal views to generate global crops and multi-sizes local crops as the input to do positive contrastive representation learning.
  For DINO-MC, we generate global and local crops from one imagery, then apply two different augmentations to global views and multi-sizes local views, respectively, to get the input of teacher and student network.
  }
  \label{fig:dino}
\end{figure*}

\section{Related Work}
\subsection{Contrastive Self-supervised Learning.}
The self-supervised pre-training aims to learn a general features which can generalise well to different downstream tasks.
As a branch of self-supervised methods, contrastive learning has recently gained attention and shown promise in performance. It learns feature representations by maximizing the similarity of two positive instances and the distance between positive and negative instances.
In generative and discriminative models, the encoder is trained to learn feature representations by optimizing the loss function calculated on the ground truth and the prediction, while the loss of contrastive model is calculated in the latent space \cite{wang2022selfreview}.

\textbf{Instance discrimination} \cite{dosovitskiy2014discriminative} is a simple but effective pretext task.
It is widely used in contrastive learning which sets augmented views of the same input as positive samples and different instances as negative samples, and then trains the model to keep positive pairs close and negative pairs far in the representation space \cite{dosovitskiy2014discriminative,wu2018unsupervised, tian2020makes,seneviratne2021self}.
\cite{he2020momentum} proposes Momentum Contrast model (MoCo-V1) with a momentum encoder to effectively maintain multiple negative samples for representation learning.
SimCLR \cite{chen2020simple} employs very large batch sizes instead of momentum encoders, and provides experimental results on  forms of data augmentation.
Inspired by SimCLR, MoCo-V1 is extended to MoCo-V2 \cite{chen2020improved}, and then MoCo-V3 \cite{chen2021empirical} is proposed by applying ViT as the backbone.

\textbf{Clustering method} is another line of contrastive learning, which demands no precise indication from the inputs \cite{caron2018deep}. 
\cite{caron2020unsupervised} introduces a contrasting cluster assignment that learns features by Swapping Assignments between multiple Views of the same image (SwAV).
SwAV is easily applicable to different sizes of datasets since it employs clustering instead of pairwise comparisons.
Furthermore, \cite{caron2020unsupervised} proposes a new data augmentation named multi-crop to increase the instances without drastically additional memory and computation.  
Specifically, when doing multi-crop, images will be cropped to a collection of views with lower resolutions instead of the full-resolution views.
Multi-crop shows both efficiency and effectiveness since several self-supervised models \cite{caron2018deep,asano2019self,chen2020simple} perform well with it.



\textbf{Knowledge distillation} can be used in contrastive learning for feature extraction without separating between images \cite{caron2021emerging}.
\cite{grill2020bootstrap} proposes Bootstrap Your Own Latent (BYOL) based on the online and target networks whose weights are updated with each other.
BYOL experiments with ResNet of different sizes.
Inspired by BYOL, \cite{caron2021emerging} explores the further synergy between SSL and different backbones, especially ViTs, and proposes a simple form of self-distillation with no labels (DINO).
DINO uses the same two networks architecture as BYOL but with different loss function and backbone models.
When pre-trained on ImageNet \cite{russakovsky2015imagenet}, DINO achieves better linear and KNN probing evaluation results than other self-supervised methods with fewer computation resources \cite{grill2020bootstrap,caron2020unsupervised,caron2021emerging}.


\subsection{Self-supervised Learning in Remote Sensing.} 
\cite{tao2020remote} experiments with self-supervised models on different pretext tasks, including image inpainting \cite{pathak2016context}, context prediction \cite{doersch2015unsupervised}, and instance discrimination \cite{wu2018unsupervised} on remote sensing imagery tasks. 
\cite{vincenzi2021color} proposes to learn practical representations from satellite imagery by reconstructing the visible colors (RGB) from its high-dimensionality spectral bands (Spectral).
\cite{ayush2021geography} explores the application of contrastive representation learning to large-scale satellite image datasets.
\cite{manas2021seasonal} proposes the seasonal contrast (SeCo) method, which significantly improves the performance of MoCo-V2 on three remote sensing tasks.
Similar to \cite{ayush2021geography}, SeCo utilizes temporal information from remote sensing images to set two kinds of experiments MoCo-V2 with TP and SeCo.
MoCo-V2 with TP is a positive temporal contrast, which regards the temporal views as the positive instances and trains models match their representations allowing the model to learn essential features that do not change over time.
While SeCo is a negative temporal contrastive model regarding temporal views as the negative instances to capture the changes or differences because of the time changing.
\cite{wang2022last} uses Swin Transformer as the backbone of DINO and applies it to remote sensing imagery tasks.
DINO-MM \cite{wang2022selfSAR} extends DINO by combining synthetic-aperture radar (SAR) and multispectral (optical) images and is applied to BigEarthNet land use classification task.

Existing studies have demonstrated the value and feasibility of self-supervised models for practical applications in remote sensing image tasks.
However, the potential of SSL in remote sensing has not been fully unlocked.
Our work targets to bridge this gap and extend existing self-supervised model by generating more effective contrastive instances.


\section{Method}
Our work is mainly based on a contrastive self-supervised model DINO, which has been applied to both natural and remote sensing imagery \cite{caron2021emerging,wang2022selfSAR,wang2022ssl4eo}.
We aim to learn useful, transferable features for remote sensing imagery by exploring positive temporal contrastive self-supervised model DINO-TP (\cref{section:dino-tp}) and introducing a self-supervised learning method DINO-MC (\cref{section:dino-mc}).
In addition, we also experiment with the feature extraction ability of different backbones in SSL.


\subsection{DINO-TP}
\label{section:dino-tp}
\textbf{Architecture }
\cref{fig:dino} shows the model structure.
The student and teacher networks in DINO are two neural networks with the same architecture  but different weights  and .
Two sets of augmented views  and  are generated from the same image.
The student network receives both global and local crops as inputs, whereas the teacher network only receives global crops.
Specifically, the global view covers the majority of the initial image and the local view only contains a small portion of it.
In this way, the model is trained to match the individual local views to global views in the feature space.

Rather than a pre-trained and frozen teacher network used in previous study \cite{fang2021seed,chen2020big}, DINO dynamically updates the teacher weights by using EMA on student weights  

, where  is the current weight of the teacher network and  is the current weight of the student network.
The  values adhere to a cosine schedule between  and , indicating that the teacher network is less dependent on the present student and more dependent on the integration of the student network in each round; hence, the weights are updated slowly.

DINO uses centering and sharpening to prevent model collapse.
Centering is adding a bias term  to the features of the output of the teacher model, i.e., 

As \cref{eq:c_update} shown, the bias term  is dynamically updated by the EMA, where  denotes the update rate and  is the batch size.

DINO achieves sharpening by performing softmax normalization using low temperature in the teacher network to avoid consistent distribution.
The teacher network in DINO consistently performs better than the student network during pre-training, so it is used as the feature extractor in downstream tasks after pre-training \cite{caron2021emerging}.

\begin{figure*}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/augmentation_process.png}
  \caption{
  The process of handling temporal views of contrastive learning in DINO-TP.
  We randomly select three temporal views of the same location and augment them to obtain global and local crops, which is the input of the teacher and student network.
  Different temporal views of the same location in DINO-TP are considered as positive examples, and we train the model to match their representations in the feature space.
  }
  \label{fig:temporal_aug}
\end{figure*}

\textbf{Temporal Contrastive } 
This work performs self-supervised pre-training on SeCo-100K \cite{manas2021seasonal} to exploit temporal information in representation learning.
SeCo-100K is an unlabeled remote sensing imagery dataset with 100K instances, and each instance is composed of five different temporal views of the same location.
In DINO-TP, we regard the temporal views of the same location as the positive instances, i.e., we pre-train the teacher and student networks to match temporal views in the feature space.
The exact process is as follows.
In the pre-training phase, we randomly select three temporal views and name them as .
As shown in \cref{fig:temporal_aug},  can be regarded as the temporal augmentation views of .
The temporal image  is directly used as  to generate  local crops of various sizes, including  and .
After applying color jittering, grayscale, and Gaussian blur to each of the three temporal perspectives, we acquire  views.
We scale  within a specific range and resize them into  to get three global crops, which are used as the input of the teacher network.
In prior work, different techniques are paired with different crop scaling ranges.
SwAV uses  \cite{caron2020unsupervised} and DINO chooses  \cite{caron2021emerging}.
This study follows DINO and uses  as the scaling range for local crops and global crops.
Following this method, DINO-TP not only learns the relationship between the whole and the pieces of the image, but also discovers the correlation between various temporal perspectives.

\subsection{DINO-MC}
\label{section:dino-mc}
The structure of DINO-MC is nearly identical to DINO-TP.
Intuitively, DINO-MC can be regarded as DINO with different multi-crop and color transformation augmentations, and as DINO-TP without temporal views.
In our work, we change the cropping strategy for local crops in DINO to create a more challenging pretext task.
We use multi-sized crops instead of the fixed-size local crops used in DINO. 
Keeping the number of global and local crops same as DINO, DINO-MC outperforms DINO on both linear and KNN probing as well as end-to-end evaluation on multiple downstream tasks.
Color-related pretext tasks have been proven to be effective in many image tasks in the field of remote sensing \cite{zhang2016colorful, vincenzi2021color, assran2022masked}.
This is due to the strong connection between color and semantics.
Therefore, we set two strategies of color transformations for global and local crops respectively.
The global views are augmented by random color jittering and GaussianBlur.
The local views are augmented by random color jittering, random grayscale shifting, and random Gaussian blur all together.
Random color jittering can change the brightness, contrast, saturation and hue of the image with a specific probability within a certain range.
It is a commonly used color transformation method because it can simulate the effect of shooting in different lighting environments and other real-world shooting situations.
Random grayscale converts an image into a grayscale image randomly.
This enhancement method can reduce the impact of color, which is beneficial for some specific application scenarios, and can learn aspects other than color properties.
We use different cropping strategy and different settings for color transformation.
Our strategy is simple but effective, and the experiments verify that DINO-MC outperforms DINO on both linear probing and end-to-end fine-tuning on three downstream tasks (classification and change detection).

\section{Experiments}
In this study, we evaluate the features learned from the self-supervised pre-training on two downstream tasks: a land use classification on EuroSAT \cite{helber2019eurosat} and a change detection task on OSCD \cite{daudt2018urban}.

\textbf{Self-supervised Pre-training } 
We adopt and extend DINO for doing experiments since it is not only flexible and effective but also greatly declines the computational requirements.
The models are pre-trained on SeCo-100K dataset \cite{manas2021seasonal}, which is collected from Sentinel-2 \cite{drusch2012sentinel} for self-supervised representation learning.
It is an unlabeled remote sensing dataset containing 100K instances from different locations around the world, each consisting of five temporal views taken from the same location.
In the pre-training phase, two distinct sets of temporal views of the same region are supplied to the student and teacher networks, respectively, then the self-supervised model matches these views in feature space to generate representations that do not vary over time.
In DINO-MC and DINO-TP, the scaling range of multi-crop is  for local crops and  for global crops.
Our self-supervised models, pretrained on 100k images over 300 epochs, is compared against several baselines, including DINO, MoCo-V2, and SeCo, on three different remote sensing downstream tasks, while using less overall computation than the current state of the art (SeCo-1M).


\subsection{DINO with Different Backbones}
Being the backbone of DINO, ViT is superior to ResNet \cite{caron2021emerging}.
In this work, there are four different networks applied as the backbone of DINO, DINO-TP, and DINO-MC, including ViT, Swin Transformer, ResNet, and WRN.

ViT preserves the Transformer structure used in NLP as much as possible and performs very well in computer vision tasks \cite{dosovitskiy2020image}.
The input of transformer encoder is the embedding formed by adding the patch embedding with the position encoding.
The transformer encoder of ViT consists mainly of layer normalization (LN) which is applied before each block, multi-head attention, and multi-layer perceptron block (MLP).

Inspired by ViT, Swin Transformer is proposed with a hierarchical transformer, which computes representation with both regular and shifted windows to capture features from different levels and resolutions. 


A deep residual learning framework (ResNet) is proposed to overcome the degradation issue by \cite{he2016deep}.
They add shortcut connections to transmit the information of shallow layers directly to the deeper layers of the neural network and require no additional computation.
With this mechanism, the residual net is deepened to  layers and achieves state-of-the-art results in multiple computer vision tasks. 
Furthermore, with the development of self-supervised representation learning, ResNet also becomes an effective backbone widely used in multiple self-supervised architectures \cite{chen2020simple, grill2020bootstrap, oord2018representation, tian2020contrastive, misra2020self}. 

WRN \cite{zagoruyko2016wide} is proposed to improve the performance of ResNet by increase the width of the residual block, i.e., widening the convolutional layers.
When the same number of parameters are utilized, wide residual block can get superior outcomes with less training time compared to the original residual block.
WRN adds a widening factor to a block denoted by , and the initial residual block can be represented as . 
Although the parameters numbers and computational complexity are quadratic in , this method is more effective than expanding the number of layers in ResNet since large tensors are able to make better use of the parallel-computing ability of GPUs \cite{zagoruyko2016wide}.
WRN is proved to achieve new state-of-the-art results in a supervised manner in terms of classification F1-Score and training efficiency \cite{papoutsis2021efficient}.
Therefore, this project intends to employ it with a self-supervised training framework.

\textbf{Implementation Details }
There are different sizes of ViT models, and we use ViT-small as the backbone model and implement it following DINO.
In the experiments, we load different backbones as the teacher and student network without pre-trained weights.
WRN-50-2 has similar structure to ResNet, with the exception of the bottleneck number of channels, which is twice as large in each block.
We follow the recommendation from DINO to use AdamW optimizer.
We use cross-entropy as the loss function to calculate the distance between feature representations output by two networks.
We evaluate the learned representations by applying KNN and linear probing on EuroSAT land use classification task.


\textbf{Quantitative Results}
In order to evaluate the representations independently, the feature extraction model is frozen and only linear and KNN classifiers are trained on EuroSAT land use classification task.
The results are shown in \cref{table:DINO-Backbones}.
From the table, DINO-MC performs better than both DINO and DINO-TP when using ViT-samll, WRN-50-2, and ResNet-50 backbones.
DINO-MC with WRN-50-2 pre-trained on 100K data is even 2.56\% higher than the linear probing accuracy of SeCo pre-trained on 1 million data.
Besides, DINO-MC with ViT-samll has 2.59\% higher accuracy than DINO with ViT-samll which indicates the effectiveness of our strategy.
In comparison to the other three backbones, Swin-tiny performs less well.
One possible reason could be that the model size of Swin-tiny is much smaller than the other three models.
Another interesting observation is that the DINO-TP performs worse with two convnets than the DINO, but better with two transformer models.
Among the different self-supervised models, ViT-small and Swin-tiny performed more consistently than ResNet-50 and WRN-50-2.
Overall, DINO-MC is particularly effective.


\begin{table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    Model & Arch & \#images & KNN & Linear \\
    \midrule
    MoCo-V2 & ResNet-50 & 1M & - & 83.72 \\
    SeCo-1M & ResNet-50 & 1M & - & 93.14 \\
    \midrule
    DINO & ResNet-50 & 100K & 90.09 & 89.65  \\
    DINO-MC & ResNet-50 & 100K & 93.94 & 95.59  \\
    DINO-TP & ResNet-50 & 100K & 79.05 & 86.70  \\
    \midrule
    DINO & WRN-50-2 & 100K & 92.74 & 91.65 \\
    DINO-MC & WRN-50-2 & 100K & \textbf{94.65} & \textbf{95.70} \\
    DINO-TP & WRN-50-2 & 100K & 86.37 & 88.15 \\
    \midrule
    DINO & ViT-small & 100K & 93.35 & 91.50 \\
    DINO-MC & ViT-small & 100K & 93.41 & 94.09 \\
    DINO-TP & ViT-small & 100K & 93.15 & 93.89 \\
    \midrule
    DINO & Swin-tiny & 100K & 92.15 & 86.87 \\
    DINO-MC & Swin-tiny & 100K & 93.22 & 90.54 \\
    DINO-TP & Swin-tiny & 100K & 92.83 & 91.94 \\
    \bottomrule
  \end{tabular}
  \caption{Linear and KNN probing classification on EuroSAT. 
  We evaluate our models with different backbones on EuroSAT and record the top-1 accuracy of KNN and linear probing on the validation set.
  MoCo-V2 \cite{chen2020improved} and SeCo-1M \cite{manas2021seasonal} are pre-trained on SeCo-1M dataset, and their linear probing results on EuroSAT are from SeCo \cite{manas2021seasonal}.
  Other listed models are pre-trained on SeCo-100K dataset with only 10\% images of SeCo-1M.
  }
  \label{table:DINO-Backbones}
\end{table}


\subsection{Land Use Classification on EuroSAT}
EuroSAT is a widely used benchmark dataset for remote sensing land use classification tasks.
It is used for representation evaluation in this study, allowing the results of models based on it to be easily compared to those of other models.
The dataset, which collects 27,000 remote sensing images from the Sentinel-2 satellite, is divided into 21,600 and 5,400 images for training and evaluation in the experiment, respectively.

\textbf{Implementation Details }
The pre-trained backbone models in self-supervised learning are evaluated as feature extractors in this land use classification downstream task.
Based on the pre-trained backbone models, we add a fully-connected layer as the classifier to output the classification results.
Both the feature extractor and the classifier are fine-tuning on this supervised classification task.
We train the models for around 200 epochs with a batch size of 32.
The learning rate is  or  for different models.
We use the \textit{CosineAnnealingLR} scheduler to update the learning rate.
Same as SeCo \cite{manas2021seasonal}, we use SGD optimizer without weight decay to update weights.

\textbf{Quantitative Results}
\cref{table:EuroSAT_results} compares our pre-trained models against three supervised baseline models on EuroSAT land use classification task.
DINO-MC with WRN-50-2 achieves similar results to the three supervised models, in particular, it was pre-trained on only 100K images, while the three supervised models were pre-trained on 1M images.
This further confirms the effectiveness of the representations learned by DINO-MC.

\begin{table}
  \centering
  \begin{tabular}{lcc}
    \toprule
    Model & Backbone & Accuracy \\
    \midrule
    Supervised & WRN-50-2 & 98.72 \\
    Supervised & ResNet-50 & 98.78 \\
    Supervised & ViT-base & 98.83 \\
    \midrule
DINO & ViT-small & 97.98 \\
DINO-MC  & ViT-small & 98.15 \\
DINO-MC  & Swin-tiny & 98.43 \\
    DINO-MC  & ResNet-50 & 98.69 \\
    DINO-MC  & WRN-50-2 & \textbf{98.78} \\
    \bottomrule
  \end{tabular}
  \caption{
  End-to-end accuracy results on EuroSAT land use classification task.
  The three supervised models listed are pre-trained on ImageNet-1K \cite{russakovsky2015imagenet}, and we load and fine-tune them on EuroSAT.
  While DINO and DINO-MC are only pre-trained on SeCo-100K dataset, which has only 10\% the number of images of ImageNet-1K.
  }
  \label{table:EuroSAT_results}
\end{table}


\subsection{Land Use Classification on BigEarthNet}
BigEarthNet \cite{sumbul2019bigearthnet} is a widely used benchmark dataset for land use classification task, containing a total of 590,326 images.
This paper uses BigEarthNet-S2 \cite{sumbul2019bigearthnet}, which collects remote sensing images from Sentinel-2 only, and each image is annotated by multiple land use categories.
The dataset provides 12 spectral bands for each image and a JSON file with its multi-labels and metadata information.
Following SeCo \cite{manas2021seasonal}, we employ a new nomenclature of 19 classes introduced in \cite{sumbul2020bigearthnet}, and around  of the patches that are totally masked by seasonal snow, clouds, or cloud shadows are eliminated in this experiment.
We used the training/validation splitting strategy suggested by \cite{neumann2019domain} with 311,667 instances for training and 103,944 images for validation.

\textbf{Implementation Details }
We add a linear classification layer on top of the backbone model as the output layer and then fine-tune it on 10\% and 100\% BigEarthNet, respectively, to evaluate the features learned by the self-supervised models.
In this experiment, the Adam and AdamW optimizer with default hyper-parameters are used to update the weights of the models. 
Identical to SeCo, we set the learning rate to  and scale it down by ten in epochs of 60\% and 80\%, respectively.
We use the MultiLabelSoftMarginLoss as the loss function, which allows assigning a different number of target classes to each sample.

\textbf{Quantitative Results}
\cref{table:BigEarthNet_results} provides the results of each pre-trained model in the end-to-end BigEarthNet classification task.
We measure the performance of each model by mean average precision (MAP).
When fine-tuned on the 10\% BigEarthNet, DINO-MC outperforms SeCo-100K with the same backbone.
Besides, DINO-MC with ViT-small achieves 1.65\% higher MAP than with ResNet-50, 2.48\% higher MAP than SeCo-100K, and even 1.58\% higher MAP than SeCo-1M.
The performance of DINO-MC with each of the four backbone models exceeds that of SeCo-100K, and even outperforms SeCo-1M except for ResNet-50.

When fine-tuned on the whole BigEarthNet, DINO-MC achieves comparable result to SeCo-100K with the same backbone.
Interestingly, DINO-MC with Swin-tiny achieves 1.89\% higher MAP than with ResNet-50, 1.63\% higher MAP than SeCo-100K, and 0.94\% higher than SeCo-1M.
The results demonstrate that the representations learned by DINO-MC can generalize well on the multi-label classification task on BigEarthNet.

\begin{table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    Model & Backbone & Param. & 10\% & 100\% \\
    \midrule
    SeCo-100K & ResNet-50 & 23M & 81.72 & 87.12 \\
    SeCo-1M & ResNet-50 & 23M & 82.62 & 87.81 \\
    \midrule
    DINO & ResNet-50 & 23M & 79.67 & 85.38 \\
    DINO-TP  & ResNet-50 & 23M & 80.10 & 85.20 \\
    DINO-MC  & ResNet-50 & 23M & 82.55 & 86.86 \\
    DINO-MC  & WRN-50-2 & 69M & 82.67 & 87.22 \\
    DINO-MC & Swin-tiny & 28M & 83.84 & \textbf{88.75} \\
    DINO-MC  & ViT-small & 21M & \textbf{84.20} & 88.69 \\
    \bottomrule
  \end{tabular}
  \caption{Mean average precision (MAP) results on BigEarthNet-S2 land use classification. 
  We use the same train/validation splits as SeCo \cite{manas2021seasonal}.
  The comparison is done by fine-tuning on 10\% and 100\% training data and evaluating on all validation dataset with different self-supervised models and backbones.
  SeCo-100K and SeCo-1M represent SeCo pre-trained on SeCo-100K and SeCo-1M, respectively, and their results listed are from \cite{manas2021seasonal}.
  DINO, DINO-MC, and DINO-TP are pre-trained on SeCo-100K only.
  }
  \label{table:BigEarthNet_results}
\end{table}

\subsection{Change Detection on OSCD}

\begin{table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    Model & Backbone & Pre. & Rec. & F1 \\
    \midrule
    Supervised & ResNet-50 & 56.49 & 43.63 & 48.61 \\
    Supervised & WRN-50-2 & 53.76 & 47.11 & 49.72 \\
    \multicolumn{2}{l}{PatchSSL (21M param.)} & 40.44 & 69.10 & 51.00 \\
MoCo-V2 & ResNet-50 & 64.49 & 30.94 & 40.71 \\
    SeCo-1M & ResNet-50 & 65.47 & 38.06 & 46.94 \\
    \midrule
    DINO & ResNet-50 & 57.37 & 44.21 & 49.53 \\
    DINO-MC & ResNet-50 & 51.94 & 54.04 & 52.46 \\
    DINO-TP & ResNet-50 & 51.10 & 49.03 & 49.74 \\
    \midrule
    DINO & WRN-50-2 & 53.58 & 52.28 & 52.41 \\
    DINO-MC & WRN-50-2 & 49.99 & 56.81 & \textbf{52.70} \\
    DINO-TP & WRN-50-2 & 55.77 & 47.30 & 50.61 \\
    \bottomrule
  \end{tabular}
  \caption{Fine-tuning accuracy results on OSCD change detection task.
  We adopt the same train/validation splits as SeCo \cite{manas2021seasonal}.
  Following SeCo, we freeze the pre-trained backbone and only update the weights of U-net \cite{ronneberger2015u}.
  Our WRN-50-2 and ResNet-50 models are pre-trained on 100K satellite images.
  The result of PatchSSL is from \cite{chen2021self}.
The listed MoCo-V2 \cite{chen2020improved} and SeCo-1M are pre-trained on 1M satellite images and are results provided by \cite{manas2021seasonal}.
  }
  \label{table:OSCD_results}
\end{table}


\begin{figure*}
  \centering
  
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/lasvegas/lasv_img1.png}
    \subcaption*{
    Image 1 \protect \\
    (Lasvegas)
    }
\end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/lasvegas/lasv_img2.png}
    \subcaption*{Image 2 \\
    (Lasvegas)}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/lasvegas/lasv_mask.png}
    \subcaption*{Mask \\
    (Ground truth)}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/lasvegas/lasv_out_seco_84.25.png}
    \subcaption*{SeCo-1M \\
    F1=84.25}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/lasvegas/lasv_out_wr_base8683.png}
    \subcaption*{DINO \\
    F1=86.83}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/lasvegas/lasv_out_wr_seco8860.png}
    \subcaption*{DINO-TP \\
    F1=88.60}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/lasvegas/lasv_out_wr_mc86.69.png}
    \subcaption*{
    DINO-MC \\
    F1=86.69
    }
  \end{subfigure}

\hspace{5mm}

    \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/dubai/dubai_img1.png}
    \subcaption*{Image 1 \\
    (Dubai)}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/dubai/dubai_img2.png}
    \subcaption*{Image 2 \\
    (Dubai)}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/dubai/dubai_mask.png}
    \subcaption*{Mask \\
    (Ground truth)}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/dubai/dubai_out_seco_61.89.png}
    \subcaption*{SeCo-1M \\ 
    F1=61.89}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/dubai/dubai_out_wr_base6540.png}
    \subcaption*{DINO \\ 
    F1=65.40}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/dubai/dubai_out_wr_seco5964.png}
    \subcaption*{DINO-TP \\ 
    F1=59.64}
  \end{subfigure}
  \begin{subfigure}{0.138\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/cd/dubai/dubai_out_wr_mc6842.png}
    \subcaption*{ DINO-MC\\
     F1=68.42}
  \end{subfigure}
  \caption{
  Same as SeCo \cite{manas2021seasonal}, our visualization is based on two instances of 'Losvegas' and 'Dubai' from the OSCD change detection dataset. SeCo-1M is SeCo model pre-trained on SeCo-1M dataset, while DINO, DINO-TP, and DINO-MC are pre-trained on only SeCo-100K.
  We visualize the outputs of DINO-TP and DINO-MC for comparison with DINO and SeCo.  
  We present results on the same images as SeCo for comparison and the two outputs of SeCo is from \cite{manas2021seasonal}.
  We also provide F1 score for each output on the bottom of output image.}
  \label{Figure:OSCD_visualization}
\end{figure*}

The Onera Satellite Change Detection (OSCD) is a benchmark dataset of which the images are collected from Sentinel-2 satellites. 
It focuses primarily on urban growth and disregards natural changes \cite{daudt2018urban}. 
Change detection is a fundamental problem in the field of earth observation image analysis.
The input is a pair of images captured at the same location, while the label is a mask map highlighting the change parts.
It is a binary classification task in which labels are assigned to each pixel based on a series of images sampled at different times: change (positive) or no change (negative).
The results are measured by F1 Score, precision and recall.

\textbf{Implementation Details }
The OSCD dataset is divided into fourteen and ten pairs for training and validation separately, as recommended by prior research \cite{daudt2018urban, manas2021seasonal}.
In addition, the categories of change and unchanged in this dataset are unbalanced due to the property of the task.
We use the same U-net architecture as SeCo for the change detection task, which employ the pre-trained backbone models to extract features as the encoder in the U-net.
We apply the pre-trained WRN-50-2 and ResNet-50 backbone models as the encoder and select the first convolution layer, and Layer 1 to 4 as the copy and cropping layers.
As for the decoder module, it is  constructed  to rebuild an image with the same width and height as the input.
And the input and output sizes of its layers are set according to the input and output of the specific encoder layers, in order to concatenate them in the direction of the channel.
Upsampling here is achieved by interpolate function, which can be understood simply as a technology to increase the resolution of the output.
The last layer of the U-net is a  convolutional layer, which is used to map the channel of the feature vector to the number of output classes.
During fine-tuning, in order to avoid overfitting, only the U-net weights are updated.
In this task, the pre-trained WRN are fine-tuned with a batch size of  and learning rate of .
The loss function used is 

\textbf{Quantitative Results}
\cref{table:OSCD_results} provides the results of DINO, DINO-MC, and DINO-TC with ResNet-50 and WRN-50-2 respectively, compared against some supervised and self-supervised baselines on OSCD dataset.
The F1 score of DINO-MC with ResNet-50 is  higher than that of SeCo and almost  higher than that of DINO.
When using WRN-50-2 as the backbone, the F1 score of DINO-MC is  higher than that of SeCo and similar with DINO.
It is no surprise that DINO-TP does not perform as well as DINO and DINO-MC, because DINO-TP receives images of the same location taken at different times as positive instances, so it aims to learn features that do not change over time, which is not very suitable for change detection task \cite{manas2021seasonal}.
In the results of OSCD task, the backbones pre-trained in DINO-MC can capture the subtle differences of image changes over time after a simple fine-tuning on the change detection dataset, which indicates that DINO-MC is able to learn general and effective features using only very simple data augmentation methods.

\textbf{Qualitative Results}
\cref{Figure:OSCD_visualization} gives the visualization masks of DINO-MC on OSCD task.
To do comparison to SeCo, we select two identical examples from the OSCD validation set.
The masks generated by DINO-MC outperform SeCo-1M since they cover more changed pixels without excessive false predictions.
We observe that the performance of DINO-TP on OSCD task is unstable since it achieves particularly high F1 score on the first instance, which is  higher than SeCo-1M, but much lower in the second one, which is  lower than SeCo-1M. 
Although positive temporal contrast used in DINO-TP has been proved to be undesirable for change detection task \cite{manas2021seasonal}, the performance of DINO-TP is comparable to SeCo-1M when evaluated on the whole validation set.




\section{Conclusions}
In this work, we introduce DINO-TP and DINO-MC, which extend DINO in two ways: (1) DINO-TP employs a positive temporal contrast strategy and (2) DINO-MC utilizes a new cropping strategy and color transformations for local views.
Experimental results of KNN and linear probing evaluation on EuroSAT demonstrate the effectiveness of our cropping strategy, as well as the unstable performance of the positive temporal contrast on remote sensing imagery.
We evaluate and compare our models with some self-supervised and supervised baselines on three remote sensing tasks.
The results of three end-to-end tasks indicate the superiority and efficiency of DINO-MC over the existing state-of-the-art models.




\textbf{Acknowledgement}
This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
