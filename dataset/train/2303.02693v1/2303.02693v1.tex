
\documentclass{article} \usepackage{iclr2023_conference,times}
\iclrfinalcopy


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{caption}
\usepackage{floatrow}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{latexsym}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{tabulary,multirow,overpic,xcolor}
\usepackage{algorithmic}
\usepackage{wrapfig}

\captionsetup[figure]{font=small}
\captionsetup[table]{font=small}

\def\etal{\emph{et al. }}
\def\ie{\emph{i.e., }}
\def\eg{\emph{e.g., }}

\newtheorem{theorem}{Theorem}

\title{Maximizing Spatio-Temporal Entropy of Deep 3D CNNs for Efficient Video Recognition}




\author{Junyan Wang, Zhenhong Sun, Yichen Qian, Dong Gong, Xiuyu Sun\thanks{Corresponding author, equal contribution, work done in Alibaba.}, Ming Lin, \\ \textbf{Maurice Pagnucco, Yang Song}\\
University of New South Wales \\
DAMO Academy, Alibaba Group \\
Amazon \\
\texttt{\{junyan.wang, dong.gong, yang.song1\}@unsw.edu.au}\\
\texttt{\{zhenhong.szh, yichen.qyc, xiuyu.sxy\}@alibaba-inc.com}\\
\texttt{\{morri\}@cse.unsw.edu.au}, \texttt{\{minglamz\}@amazon.com}}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
3D convolution neural networks (CNNs) have been the prevailing option for video recognition. To capture the temporal information, 3D convolutions are computed along the sequences, leading to cubically growing and expensive computations. To reduce the computational cost, previous methods resort to manually designed 3D/2D CNN structures with approximations or automatic search, which sacrifice the modeling ability or make training time-consuming. In this work, we propose to automatically design efficient 3D CNN architectures via a novel training-free neural architecture search approach tailored for 3D CNNs considering the model complexity. To measure the expressiveness of 3D CNNs efficiently, we formulate a 3D CNN as an information system and derive an analytic entropy score, based on the Maximum Entropy Principle. Specifically, we propose a spatio-temporal entropy score (STEntr-Score) with a refinement factor to handle the discrepancy of visual information in spatial and temporal dimensions, 
through dynamically leveraging the correlation between the feature map size and kernel size depth-wisely.
Highly efficient and expressive 3D CNN architectures, \ie entropy-based 3D CNNs (E3D family),  can then be efficiently searched by maximizing the STEntr-Score under a given computational budget, via an evolutionary algorithm without training the network parameters. Extensive experiments on Something-Something V1\&V2 and Kinetics400 demonstrate that the E3D family achieves state-of-the-art performance with higher computational efficiency.
Code is available at \url{https://github.com/alibaba/lightweight-neural-architecture-search}.











\end{abstract}
\vspace{-0.5em}
\section{Introduction}
\vspace{-0.5em}
Video recognition is a fundamental task for video understanding. 
To capture the visual information in both temporal and spatial domains from  high-quality large-scale videos, most works have been focusing on proposing highly expressive models which, however, lead to higher computational costs \cite{kondratyuk2021movinets,zhang2022revisiting,liuniformer}.
Recent research shows that 3D CNNs achieve excellent performance on large-scale benchmarks \citep{hara3dcnns} with unified computations to capture spatio-temporal features jointly. 
However, the computational cost grows cubically in standard 3D convolution, making it prohibitive for high-resolution long-duration videos.
Previous works propose to improve the efficiency of 3D CNNs via 2D decomposition or approximation manually \citep{carreira2017i3d,tran2018R21d,feichtenhofer2020x3d}.
Some practices have also been conducted to manually design efficient 3D CNNs relying on heuristics or experiences \citep{hara3dcnns,feichtenhofer2020x3d}. The manually designed 3D or 2D CNN structures cost massive efforts and time in strengthening the modeling ability.
Neural Architecture Search (NAS) approaches \citep{kondratyuk2021movinets,wang2020pv} can automatically generate 3D CNN architectures with higher modeling ability. 
However, searching for a single 3D architecture requires days on multiple GPUs or TPUs, as training and evaluation of the accuracy indicator are required in the process, making the automatic 3D CNN design process time-consuming and/or hardware-dependent.




To tackle the above issues, we study how to automatically generate (or design) efficient and expressive 3D CNNs with limited computations.
Recently, training-free technologies have been introduced by some approaches \citep{ntk,lin2021zen,sun2022mae}, in which kernel spectrum analysis or forward inference are adopted to measure the expressiveness of spatial 2D CNNs. 
Inspired by the training-free concept and information theory, we suggest that a deep network can be regarded as an information system, and measuring the expressiveness of the network can be considered equivalent to analyzing how much information it can capture.
Therefore, based on the \textbf{Maximum Entropy Principle} \citep{jaynes1957information}, the probability distribution of the system that best represents the current state of knowledge is the one with the highest entropy.
However, as discussed in~\citep{xie2018s3d}, the information in spatial and temporal domains is different in natural video data.
The spatial dimension is usually limited to some local properties, like connectivity \citep{claramunt2012towards}, while the temporal dimension usually contains more drastic variations with more complex information.
To address the spatio-temporal discrepancy in video data, we conduct a kernel selection experiment and observe that different 3D kernel selections in different stages have different effects on performance, and the focus of 3D CNNs changes from spatial information to spatio-temporal information, as the network depth increases.
We thus consider that the design of 3D CNN architecture should focus on spatial-temporal aggregation depth-wisely.



The above analysis has motivated us to propose a training-free NAS approach to obtain optimal architectures, \ie entropy-based 3D CNNs (\textbf{E3D} family).
Concretely, we first formulate a 3D CNN-based architecture as an information system whose expressiveness can be measured by the value of its differential entropy.
We then derive the upper bound of the differential entropy using an analytic formulation, named \textbf{Spatio-Temporal Entropy Score} (STEntr-Score), conditioned on spatio-temporal aggregation by dynamically measuring the correlation between feature map size and kernel size depth-wisely.
Finally, an evolutionary algorithm is employed to identify the optimal architecture utilizing the STEntr-Score without training network parameters during searching. 
In summary, the key contributions of our work are as follows:\\
 We present a novel training-free neural architecture search approach to design efficient 3D CNN architectures. Instead of using forward inference estimation, we calculate the differential entropy of a 3D CNN by an analytic formulation under Maximum Entropy Principle.\\
 We investigate the video data characteristics in spatial and temporal domains  and correlation between feature map with kernel selection, then propose the corresponding spatio-temporal entropy score to estimate the spatio-temporal aggregation dynamically, with a spatio-temporal refinement mechanism to handle the information discrepancy.  \\
 Each model of E3D family can be searched within three hours on a desktop CPU, and the models demonstrate state-of-the-art performance on various video recognition datasets. 





\section{Related Work}



\noindent\textbf{Action recognition.}
2D CNNs lack temporal modeling for video sequences, and many approaches \citep{wang2016tsn,lin2019tsm,li2020tea,wang2021tdn,wang2021actionnet,huang2021tada} focused on designing an extended module for temporal information learning.
Meanwhile, 3D CNN-based frameworks have a spatio-temporal modeling capability, which improves model performance for video action recognition \citep{tran2015c3d,carreira2017i3d,feichtenhofer2020x3d,kondratyuk2021movinets}.
Some attempts \citep{feichtenhofer2020x3d,fan2020rubiksnet,kondratyuk2021movinets} focused on designing efficient 3D CNN-based architectures.
For example, X3D \citep{feichtenhofer2020x3d} progressively expands a tiny 2D image classification architecture along multiple network axes, in space, time, width and depth.
Our work also focuses on designing efficient 3D CNN-based architectures, but in a deterministic manner with entropy-based information criterion analysis.






\noindent\textbf{Maximum Entropy Principle}.
The Principle of Maximum Entropy is one of the fundamental principles in Physics and Information Theory~\citep{shannon1948,theory1,theory2,theory3}. Accompanied by the widespread applications of deep learning, many theoretical studies \citep{saxe2019information,chan2021redunet,yu2020learning,sun2022mae} try to understand the success of deep learning based on the Maximum Entropy Principle.
Our work focuses on video recognition and explores the aggregation of spatio-temporal information under the Maximum Entropy Principle. 



\noindent\textbf{Training-Free NAS}.
To reduce the search time of NAS, recent attempts \citep{naswot,ntk,syn,lin2021zen,sunentropy,sun2022mae,zhou2022training,chen2022nasbenchzero,DBLP:journals/corr/abs-2006-14090} proposed training-free strategies for architecture searching, which construct an alternative score to rank the initialized networks without training.
For example, the work of \citep{sun2022mae} maximizes the differential entropy of detection backbones, leading to a better feature extractor for object detection under the given computational budgets.
However, these methods construct scores on spatial 2D CNNs, and cannot handle the discrepancy of visual information in spatial and temporal dimensions of 3D CNNs.
In order to address the above issues, our work aims to optimize the network architecture by considering spatio-temporal dimensions aggregation.











\section{The Proposed Approach}
In this section, we first present a detailed technical description of the derivation process of an analytical solution and propose the STEntr-Score with a refinement factor to handle the discrepancy of visual information in spatial and temporal dimensions.
Then we give an overview of the search strategy for the E3D family, via an evolutionary algorithm without training the network parameters.



\subsection{Preliminary}




In \textit{Information Theory}, differential entropy is employed to represent the information capacity of an information system by measuring the output of the system~\citep{jaynes1957information,theory1,theory2,theory3,entropy}.
Generally, the output of a system is a high-dimensional continuous variable with a complex probability distribution, making it difficult to compute the precise value of its entropy directly.
Based on the \textbf{Maximum Entropy Principle} \citep{jaynes1957information}, a common alternative approach is to estimate the upper bound of the entropy \citep{Elements2012}, as:
\begin{theorem}
For any continuous distribution  of mean  and variance , its differential entropy is maximized when  is a Gaussian distribution .
\label{trm:gaussian}
\end{theorem}
Thus, the differential entropy of any distribution is upper bound by the Gaussian distribution with the same mean and variance.
Suppose  is sampled from Gaussian distribution , the differential entropy~\citep{entropy} of  is then:

where  represents the probability density function of .
Note that the entropy of the Gaussian distribution depends only on the variance , and a simple proof is included in the \textbf{Appendix \ref{ssec:proof entropy}}.

According to successful deep learning applications~\citep{saxe2019information,chan2021redunet,yu2020learning,sun2022mae,sunentropy} of Maximum Entropy Principle, a deep neural network can be regarded as an information system, and the differential entropy of the last output feature map represents the expressiveness of the system.
Recent method \citep{sun2022mae} estimates the entropy of 2D CNNs by simply computing the feature map variance via sampling input data and initializing network parameters from a random standard Gaussian distribution.
However, when migrating to 3D CNNs, 
how to efficiently reduce the random sampling noise due to the random initialization, and how to estimate the entropy after aggregating spatial and temporal dimensions in 3D CNNs design, still remain open questions. 
We then propose our method to address these problems.








 






















\subsection{Statistical Analysis of Entropy in Deep 3D CNNs}





\noindent\textbf{Simple Network Space}.
Following the idea that \textit{simpler is better} \citep{lin2021zen,sun2022mae}, we apply vanilla 3D CNNs without considering auxiliary modules (\eg BN \citep{ioffe2015batch}, Reslink \citep{he2016deep}, SE block \citep{hu2018squeeze} and so on) to conduct analysis of network architectures.
Formally, given a convolutional network with  layers of weights ,  ,  , the forward inference with a  simple network space is given by:

where  denotes the  layer feature map. For holistic analysis, the bias of the convolutional layer is set to zero and the activation function is omitted in the network for simplification.
Auxiliary modules are ignored during entropy calculation and plugged into the backbone without special modification during training.
A detailed discussion about these rules is included in \textbf{Appendix \ref{sec:simple network}}.


Since the input data and network parameters are randomly sampled from Gaussian distributions, the forward entropy calculation will be inconsistent, which might lead to random sampling noise.
To obtain a valid entropy value, computing an average value from multiple sampling iterations and increasing the value of batch size or resolution can be adopted to reduce the noise. 
These operations are however time-consuming and cost higher computational resources.
To this end, we propose to explore the statistical characteristics of the forward inference, to provide an efficient solution. 




\noindent\textbf{Maximum Entropy of 3D CNNs}.
We first consider the \textit{product law of expectation} \citep{mood1950introduction} and the \textit{Bienaymé's identity} in probability theory \citep{loeve2017probability}, as follows:
\begin{theorem}
Given two independent random variables , , the expectation of their product  is: .
\label{trm:product}
\end{theorem}
\begin{theorem}
Given  random variables  which are pairwise independent integrable, the sums of their expectations and variances are: , and .
\label{trm:sum}
\end{theorem}
We can thus compute the expectation and variance of  layer feature map element  as:
 
where  represents the kernel size of the  layer in the 3D CNN, and  denotes its input channels size. Note that  is equal to 1 when the layer is a depth-wise convolution.
Besides,  denote the temporal, height, and width positions, respectively.
A simple proof is included in \textbf{Appendix \ref{ssec:proof variance}}.








The input  is initialized from a standard Gaussian distribution, which means that its expectation  and variance .
From the perspective of statistics, we can regard  when sampling sufficient times.
Also, suppose that all parameters are initialized from a zero-mean Gaussian distribution, and thus the variance of the last layer  can be computed by propagating the variances from previous layers as:

Finally, by combining Eq. (\ref{eq:x_d}) and Eq. (\ref{eq:gaussian}), we derive that the upper bound entropy is numerically proportional to:

where detailed proof is included in \textbf{Appendix \ref{ssec:proof 3d cnn}}. 
By assuming that the parameters of each layer are initialized with a standard Gaussian distribution with , the entropy score defined in Eq. (\ref{eq:maxe_score}) can be written as  . It measures the influence of kernel size and channel dimension on the entropy score in a homogeneous way, named \textbf{HomoEntr-Score}. 
This analytic formulation does not require random sampling, thus no random sampling noise exists.












\subsection{Spatio-temporal Entropy Score}
The HomoEntr-Score is derived from the analysis with an independent and identical assumption on the input elements (and the corresponding intermediate features). Although it can generally represent the expressiveness characteristics of a neural network, there is a gap between HomoEntr-Score and reality on 3D CNNs. 
When directly applying it on 3D CNNs for handling video sequences, we realize the HomoEntr-Score with the independent and identical assumption cannot capture the discrepancy of the visual information in the spatial and temporal domain, as the information between spatial and temporal dimensions in video data is different in video recognition. 
The gap 
leads to some issues with HomoEntr-Score for modeling video data with 3D CNNs. The observations will be discussed and analyzed in the following. 
Note that HomoEntr-Score (and similar approaches \citep{sun2022mae}) can work well for modeling the expressiveness of 2D CNNs since there is no (obvious) discrepancy on the information of the two directions in 2D images statistically. 
Based on the analyses, we propose a Spatio-Temporal Entropy Score (STEntr-Score) for 3D CNNs on video data, where a Spatio-temporal refinement factor is introduced to handle the information discrepancy. 





\begin{table}[h]
\caption{Results of different kernel positions on the Sth-Sth V1 validation dataset. All model structures are based on X3D-S \citep{feichtenhofer2020x3d}. ``S-N" models mean only stage N selects 155 kernel, and others select 333. ``T-N" models mean only stage N selects 333 kernel, and others select 155. Note that we divide stage 4 of X3D with 11 layers into two stages (5 and 6 layers).}
\begin{floatrow}
\resizebox{0.45\textwidth}{!}{
 \begin{tabular}{cccccc}
    	    \toprule
    		 Model &  Top1  &  \begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular} & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (G)\end{tabular} & \begin{tabular}[c]{@{}c@{}}HomoEntr\\ Score\end{tabular}\\
    		\midrule
S-2  &  \textbf{45.15\%} &  3.33 & 1.93 & 178.5\\
		    S-3  & 44.87\%  & 3.33 & 1.93 & 178.4\\
		    S-4  & 44.35\%  & 3.33 & 1.94 & 178.4\\
		    S-5  & 43.85\%  & 3.33 & 1.94 &178.4\\
		    S-6  & 43.83\%  & 3.33 & 1.94 &178.4\\
    		\bottomrule
    		
        \end{tabular}
}
\resizebox{0.45\textwidth}{!}{
 \begin{tabular}{ccccc}
    	    \toprule
    		 Model &  Top1  &  \begin{tabular}[c]{@{}c@{}}Params\\ (M)\end{tabular} & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (G)\end{tabular} & \begin{tabular}[c]{@{}c@{}}HomoEntr\\ Score\end{tabular}\\
    		\midrule
T-2  & 41.59\%  & 3.32 & 1.93 & 177.7\\
    		T-3  & 42.93\%  & 3.32 & 1.93 & 177.8\\
    		T-4  & 43.17\%  & 3.32 & 1.92 & 177.8\\
    		T-5  & \textbf{43.43\%} &  3.32 & 1.92 & 177.9\\
    		T-6  & 43.35\%  & 3.32 & 1.92 & 177.9\\
    		\bottomrule
    		
        \end{tabular}
}
\end{floatrow}
\label{tab:kernel}
\end{table}








\noindent\textbf{Kernel Selection Observations}.
We conduct an experiment to explore how different 3D convolutional kernel sizes at different stages (\ie layer blocks at different positions in the network) impact the performance, as shown in Table \ref{tab:kernel}.
All models are based on X3D-S but with different kernels in different stages. We set 155 and 333 kernels at the different stages in the 3D CNNs, which are typical 3D convolutional kernels for learning spatio-temporal information. 
These two different  choices enable a layer to aggregate the visual information focusing on different spatial and temporal dimensions, 
with the receptive field of CNN in the most pertinent directions.
In Table \ref{tab:kernel}, the performances of S-2 and S-3 models are higher than X3D-S with only 333 kernels (44.6\% in Table \ref{tb:ss}), and S-series outperform T-series, which show that kernel selection at different stages influences the performance significantly, and that different stages may prefer different kernel sizes, respectively. 
Although the kernel selections (with different spatio-temporal dimensions) at different stages lead to different effects on performance, the corresponding 3D CNNs have similar HomoEntr-Score. 



\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{pics/resolution.pdf}
\caption{Input feature map size and kernel sizes of S-2, T-5 and X3D model in each stage.}
    \label{fig:resolution}
\end{figure}



According to the downsampling strategy of the 3D CNNs, spatial resolutions become smaller from the large input as the depth increases, while the temporal frame size remains a certain value, as shown in Figure \ref{fig:resolution}a.
Through analyzing the results in Table \ref{tab:kernel}, we can infer that spatial kernels (like 155) can obtain spatial information more effectively at low-level stages, and spatio-temporal kernels (like 333) are more stable to obtain spatio-temporal information at high-level stages. 
Meanwhile, the similarity between the input feature map and the kernel size of the S-2 model at each stage is higher than that of the T-5 model or X3D, according to Figure \ref{fig:resolution}.
We thus consider that with the higher correlation between the feature map size and kernel size depth-wisely, the model can obtain higher expressiveness of spatial and temporal information.
















\noindent\textbf{Spatio-Temporal Refinement}.
To estimate the correlation between feature map and kernel size in different depths, we first define two vectors: the input feature map size  and the 3D kernel size  in a convolutional layer, where  represent frame, height and width dimension size.
We compute the distance  based on commonly used cosine distance as:

where  represents the \textit{Cosine Distance} function, and we expand the diversity of the cosine distance by using .
We thus utilize the distance  between  and  in each layer to define the variance of weight dynamically.
Finally, we refine the upper bound differential entropy as:

\begin{wrapfigure}{r}{6.2cm}
\captionsetup{font={small}}
    \centering
    \includegraphics[width=0.95\linewidth]{pics/model_all_new.pdf}
\caption{Top-1 accuracy vs. STEntr-Score and HomoEntr-Score.}
    \label{fig:after calibration}
\end{wrapfigure}
We name this analytic formulation of Eq. (\ref{eq:st proxy}) as \textbf{Spatio-Temporal Entropy Score} (STEntr-Score) to measure the aggregation of spatio-temporal dimensions.
After spatio-temporal refinement, we re-calculate the entropy of each model by STEntr-Score in Table \ref{tab:kernel}, and present the relationship between accuracy with STEntr-Score and HomoEntr-Score in Figure \ref{fig:after calibration}.
According to this figure, STEntr-Score is positively correlated with Top1 accuracy which indicates that the proposed spatio-temporal refinement can handle the discrepancy of visual information in spatial and temporal dimensions.





\subsection{3D CNN Searching Strategy}
Utilizing STEntr-Score, we apply the basic \textbf{Evolutionary Algorithm} (EA) to find the optimal 3D CNN architectures, which is similar to \citep{lin2021zen,sun2022mae}.
We initialize a population of candidates randomly under a small budget and define the 3D kernel search space within each layer with two options: \{, \}, then randomly select two stages from the candidates and mutate them at each iteration step.
We calculate its STEntr-Score to navigate the evolution process instead of evaluating the accuracy after mutation, if the inference cost of the mutated structure does not exceed the budget.
The population will be maintained to a certain size during iterations, by discarding the worst candidate of the smallest STEntr-Score.
After all iterations, the target network is achieved with the largest STEntr-Score under the given budget (\eg FLOPs, parameters, and latency).
Since the latency budget requires a forward process on GPU which will diminish the efficiency of our STEntr-Score search, we choose FLOPs as the target budget. Another reason for applying FLOPs budget is to fairly compare with X3D \citep{feichtenhofer2020x3d} and MoViNet \citep{kondratyuk2021movinets}, which only report FLOPs rather than latency.
Thus, we obtain the spatio-temporal entropy 3D CNNs family (E3D family) under certain FLOPs,
All models are searched separately with different FLOPs bugdet (1.9G, 4.7G, and 18.4G) for a fair comparison with X3D-S/M/L as the baseline, and the detailed algorithm is described in \textbf{Appendix \ref{sec:algorithm}}







\section{Experiments}
Our E3D family consists of E3D-S (1.9G FLOPs), E3D-M (4.7G FLOPs), and E3D-L (18.3G FLOPs).
The detailed structures of the E3D family are described in \textbf{Appendix \ref{sec:architecture}}.
We compare our approach with other state-of-the-art methods and in-depth analysis to better understand our method.
More results are presented in \textbf{Appendix \ref{sec:more results}}.


\subsection{Experiment Settings}
The E3D family includes a search stage without training and a training \& inference stage for video recognition on a specific dataset. Detailed settings in each stage are described as follows:




\noindent\textbf{Search Settings}. 
Following X3D \citep{feichtenhofer2020x3d}, we also apply a MobileNet-like network basis, in which the core concept is 3D depth-wise separable convolution for efficiency.
The initial structure is composed of 5 stages with small and narrow blocks to meet the reasonable budget, which is usually below 1/3 of the target FLOPs budget.
The population size and total iterations of EA are set as 512 and , respectively.
During mutation stages from the candidates,
we randomly select 3D kernels from \{133, 155, 333\} to replace the current one; interchange the expansion ratio of bottleneck from (); scale the output channels with the ratios ; or increases or decreases depth with 1 or 2. 
Note that the channel dimension of every layer is fixed within 8 to 640 with multiples of 8, which helps shrink homologous search space and accelerate search speed.



\noindent\textbf{Training \& Inference}.
Our experiments are conducted on three large-scale datasets, Something-Something (Sth-Sth) V1\&V2 \citep{goyal2017something}, and Kinetics400 \citep{kay2017kinetics}.
All models are trained by using Stochastic Gradient Descent (SGD).
The cosine learning rate schedule \citep{loshchilov2016sgdr} is employed, and total epochs are set to 100 for Sth-Sth V1\&V2 datasets, and 150 for Kinetics400 dataset, with synchronized Batch-Norm instead of common Batch-Norm. 
Random scaling, cropping, and flipping are applied as data augmentation on all datasets.
To be comparable with previous work and evaluate accuracy and complexity trade-offs, we apply two testing strategies:
1) \textit{K-Center}: temporally, uniformly sampling of K clips (\eg K=10) from a video and taking a center crop.
2) \textit{K-LeftCenterRight}: also uniformly sampling K clips temporally, but taking multiple crops to cover the longer spatial axis, as an approximation of fully-convolutional testing.
For all methods, we follow prior studies by reporting Top1 and Top5 recognition accuracy, and FLOPs to indicate the model complexity.
More experiment setting details can be seen in \textbf{Appendix \ref{sec:setting}}.





\begin{table}[]
\captionsetup{font={small}}
\caption{Comparison with state-of-the-art methods on Sth-Sth V1 and V2 validation datasets. The models only take RGB frames as inputs. To be consistent with compared approaches, we present most results of 2D CNN-based methods with ResNet50. ``MN-V2"" denotes MobileNet-V2. kk denotes temporal clip with spatial crop evaluation. “-” indicates the results are not available for us, and  denotes our reproduced models.}
\label{tb:fps}
    \centering
\resizebox{\textwidth}{!}{
        \begin{tabular}{l  c  c  c  c  c  c  c  c}
        \toprule
        \multirow{2}{*}{Method} &
        \multirow{2}{*}{Backbone} &
        \multirow{2}{*}{Pretrain} &
        \multirow{2}{*}{Resolution} &
        \multirow{2}{*}{GFLOPs} &
        \multicolumn{2}{c}{11 V1-Val}    &
\multicolumn{2}{c}{23 V2-Val}     \\
        \cmidrule(lrr){6-7}
        \cmidrule(lrr){8-9}


        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{Top1} &
        \multicolumn{1}{c}{Top5}   &
\multicolumn{1}{c}{Top1}   & 
        \multicolumn{1}{c}{Top5}    \\ 
\midrule
        TSN \citep{wang2016tsn} & ResNet50 & ImageNet & 8  256 & 16  & 19.5 & -  & - &  -\\
TSM \citep{lin2019tsm} & ResNet50 & ImageNet & 16  256 & 65  & 47.2 & 77.1  &  63.4 & 88.5\\


        TANet \citep{liu2021tam} & ResNet50 & ImageNet & 16  256 & 66 & 47.6 & 77.7 & 64.6 & 89.5\\
ActionNet \citep{wang2021actionnet} & ResNet50 & ImageNet & 16  256 & 69.5  & - & -  & 64.0 & 89.3\\
        TAda \citep{huang2021tada} & ConvNeXt-T & ImageNet & 16  256 & 47 & - & - & 64.8 & 88.8 \\
\midrule
        I3D \citep{carreira2017i3d}  & InceptionV1 & ImageNet+K400 & 64  256 & 306  & 41.6 & 72.2  & - & -\\
        NL I3D \citep{carreira2017i3d}  & InceptionV1 & ImageNet+K400 & 64  256 & 334  & 44.4 & 76.0 & - & -\\
S3D-G \citep{xie2018s3d} & InceptionV1 & ImageNet & 64  256 & 71.4  & 48.2 & 78.7  & - & -\\
        X3D \citep{feichtenhofer2020x3d} & X3D-S & No pretrain & 13  160 & 2  & 44.6 & 74.4   & 60.1 & 85.9\\
        X3D \citep{feichtenhofer2020x3d} & X3D-M & No pretrain & 16  224 & 4.7  & 47.3 & 76.6   & 62.2 & 87.2\\
        X3D \citep{feichtenhofer2020x3d} & X3D-L & No pretrain & 16  312 & 18.4  & 49.4 & 77.9   &  & \\
        MoViNet \citep{kondratyuk2021movinets} & MoViNet-A0 & No pretrain & 50  172 & 2.7  & 46.9 & 75.0  & 61.9 & 87.2\\
        MoViNet \citep{kondratyuk2021movinets} & MoViNet-A1 & No pretrain & 50  172 & 6  & 49.3 & 77.1  & 64.5 & 89.1\\
\midrule
        E3D & E3D-S & No pretrain & 13  160  & 1.9 & 47.1 & 75.6   & 62.1 & 87.6 \\
        E3D & E3D-M & No pretrain & 16  224  & 4.7 & 49.4 & 78.1  & 64.7 & 89.6 \\
        E3D & E3D-L & No pretrain & 16  312  & 18.3 & \textbf{51.1} & \textbf{78.7}  & \textbf{65.7} & \textbf{89.8} \\
        \bottomrule
        \end{tabular}
        }
        \label{tb:ss}
\end{table}

\begin{table}[]
\captionsetup{font={small}}
\caption{Comparison with state-of-the-art methods on the validation set of Kinetics400. We report the inference cost with a single “view" (temporal clip with spatial crop) × the number of such views used (GFLOPsviews). “N/A” and “-” indicate the numbers are not available for us.}
\label{tb:fps}
    \centering
\resizebox{\textwidth}{!}{
        \begin{tabular}{l c c  c  c  c  c  c c}
        \toprule[1pt]
        \multirow{2}{*}{Method} &
        \multirow{2}{*}{Backbone} &
        \multirow{2}{*}{Pretrain} &
        \multirow{2}{*}{Frame} &
\multirow{2}{*}{\#Param.} &
        \multirow{2}{*}{GFLOPs  Views} &
        \multicolumn{2}{c}{Val}    \\
        \cmidrule(lrr){7-8}
        
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{Top1}   & 
        \multicolumn{1}{c}{Top5}    \\ 
\midrule[1pt]
TSN \citep{wang2016tsn} & ResNet50 & ImageNet & 25  & 24.3M & 80110 & 72.5 & 90.2  \\
TSM \citep{lin2019tsm} & ResNet50 & ImageNet & 16  & 24.3M & 65310 & 74.7 & 91.4  \\
TEA \citep{li2020tea}& ResNet50 & ImageNet & 16  & - & 70310 & 76.1 & 92.5  \\
TANet \citep{liu2021tam} & ResNet50 & ImageNet & 16  & 26M & 86310 & 76.9 & 92.9  \\
        TDN \citep{wang2021tdn} & ResNet50 & ImageNet & 16+64  & - & 72310 & 77.5 & 93.2  \\
R(2+1)D \citep{tran2018R21d} & ResNet50 & ImageNet & 16  & 63.6M & 67310 & 73.7 & 91.6  \\
SlowOnly \citep{feichtenhofer2019slowfast} & ResNet50 & ImageNet & 8  & - & 42310 & 74.8 & 91.6\\
        SlowFast \citep{feichtenhofer2019slowfast} & ResNet50 & ImageNet & 8+32  & 34.4M & 65.7310 & 77.0 & 92.6\\
\midrule[1pt]
        I3D \citep{carreira2017i3d} & InceptionV1 & ImageNet & 64  & 12M & 108N/A & 72.1 & 90.3  \\ 
        Two-Stream I3D \citep{carreira2017i3d} & InceptionV1 & ImageNet & 64  & 25M & 216N/A & 75.7 & 92.0  \\
S3D-G \citep{xie2018s3d} & InceptionV1 & ImageNet & 64  & - & 71.4310 & 74.7 & 93.4  \\ 
        X3D \citep{feichtenhofer2020x3d} & X3D-M & No pretrain & 16  & 3.8M & 6.2310 & 76.0 & 92.3  \\
        X3D \citep{feichtenhofer2020x3d} & X3D-L & No pretrain & 16  & 3.8M & 24.8310 & 77.5 & 92.9  \\
        MoViNet \citep{kondratyuk2021movinets} & MoViNet-A2 & No pretrain & 50 & 4.6M & 10.311 & 75.0 & 92.3 \\
\midrule[1pt]
        TimeSformer-S \citep{bertasius2021timesformer} & ViT-B & ImageNet & 8  & 121.4M & 590310 & 78.0 & 93.7  \\
Swin \citep{liu2022video} & Swin-T & ImageNet & 32 & 28.2M & 8834 & 78.8 & 93.6 \\
        \midrule[1pt]
E3D & E3D-M & No pretrain & 16  & 3.4M & 4.7310 & 76.4 & 92.5 \\
        E3D & E3D-L & No pretrain & 16  & 5.8M & 18.3310 & 77.6 & 92.9 \\
        \bottomrule[1pt]
        \end{tabular}
        }
        \label{tb:k400}
\end{table}

\subsection{Main Results}


\noindent\textbf{Sth-Sth V1\&V2}.
Tabel \ref{tb:ss} shows the comparison between E3D family and state-of-the-art methods.
It can be seen that our proposed E3D family achieves competitive performance with more efficient FLOPs-level, which indicates that the E3D models can recognize actions effectively and efficiently.
1) Compared to 2D CNN-based methods, E3D outperforms most previous approaches on the same FLOPs-level.
Even compared to many methods with similar performance, our model requires much lower computational costs.
Note that our E3D family does not need to be pretrained on other datasets, and the performance of these 2D CNN-based methods is based on ResNet50 or a stronger backbone that is not suitable for low-level computation.
2) The E3D family also achieves higher performance compared to 3D CNN-based methods, which indicates that the architecture of E3D can      handle the discrepancy of visual information in spatial and temporal dimensions
Compared to the NAS-based method \citep{kondratyuk2021movinets}, our proposed E3D can still achieve a remarkable result which thus verifies the effectiveness of the STEntr-Score for searching the architecture.




\noindent\textbf{Kinetics400}.
Table \ref{tb:k400} shows that E3D achieves state-of-the-art performance compared to most 2D and 3D methods, but uses much less computational resources.
1) Most methods apply ImageNet pretrained backbones on the Kinetics400 dataset. However, our model can still achieve excellent results without using pretrained models, which indicates that our searched architecture by STEntr-Score can effectively learn spatio-temporal information.
2) E3D outperforms other 3D CNN-based models \citep{carreira2017i3d,xie2018s3d,feichtenhofer2020x3d} which only employ 333 kernel. It means that kernel selection is important for action recognition, and STEntr-Score can benefit 3D CNN architecture design. 
3) Even though the performance of Transformer-based models \citep{bertasius2021timesformer,neimark2021video,liu2022video} is competitive, our model still provides remarkable results by using much lower computational resources (FLOPs) and parameters, which means our model is more suitable in efficient scenarios.




\subsection{Correlation Study}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{pics/random_models_new.pdf}
    \caption{Correlations between Top1 and STEntr-Score, HomoEntr-Score, FLOPs, and Parameters. Points represent different sampled models, which have different channel numbers and layer configurations.}
    \label{fig:corre}
\end{figure}



To verify the importance of STEntr-Score in the design of video understanding models, we randomly construct 60 different models (0.2 to 5 GFLOPs) with different channel dimensions and layer numbers to investigate the correlations between STEntr-Score, HomoEntr-Score, FLOPs and parameters. For a fair comparison, all networks are trained on the Sth-Sth V1 dataset with batch size of 256 and 50 epochs.
We also provide the performance of E3D-S and X3D-S under the same training setting. According to results in Figure~\ref{fig:corre}, we can observe that:
(1) The proposed STEntr-Score is more positively correlated with Top1 accuracy than other metrics, which proves the effectiveness of our proposed STEntr-Score in evaluating network architecture.
(2) Although HomoEntr-Score is discriminative on different FLOPs levels, the ability to capture the discrepancy of the visual information in the spatial and temporal domain is not as good as STEntr-Score on the same FLOPs level.
(3) Benefiting from STEntr-Score, EA can help us obtain 3D CNN architectures with higher expressiveness as measured by STEntr-Score on the same FLOPs or parameters level.





\subsection{Discussion}
\begin{figure}[h]
	\centering
    \begin{subfigure}[b]{0.47\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{pics/ch_entropy.pdf}
\caption{Consistency comparison.}
  \label{sfig:score}
    \end{subfigure}
  \begin{subfigure}[b]{0.47\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{pics/ch_time.pdf}
\caption{Efficiency comparison.}
  \label{sfig:time}
    \end{subfigure}
	\caption{Comparisons between HomoEntr-Score and “Forward" calculations. “Forward" represents using the forward inference. The calculations are conducted on an AMD Ryzen 5 5600X 6-core CPU.}
	\label{fig:simulation}	
\end{figure}

\noindent\textbf{Comparison with forward inference}.
For a fair comparison with the realization of forward inference in~\citep{sun2022mae}, we use HomoEntr-Score and conduct a simulation in a three-layer 3D network.
The shape of the input feature is , kernel sizes are set to ,  and  with a stride of , and channels are all set to . 
The entropy of each network is calculated  times with either forward inference via Eq. (\ref{eq:gaussian}) or direct computation of HomoEntr-Score. 
When performing the forward inference of the network, convolution blocks are re-initialized based on a Gaussian distribution during each iteration.
The filled “Forward" range in Figure~\ref{sfig:score} demonstrates there exists variance between different random samples, which also emphasizes the stability of the analytic formulation. In Figure~\ref{sfig:time}, regardless of how channels change, the speed of  times formulaic calculation of value remains constant, while the speed reduces almost linearly when performing forward inference. More comparison analysis of training-free scores is included in \textbf{Appendix \ref{sec:entropy comparison}}







\begin{wraptable}{r}{7.1cm}
    \caption{Searching cost comparison on the Sth-Sth V1 dataset. : 64 Google TPUv3, Power 450W per TPUv3; : 1 AMD Ryzen 5 5600X 6-Core CPU, Power 65W;}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccc}
    	     \toprule
    		 Method & \begin{tabular}[c]{@{}c@{}}Search \\ Devices\end{tabular} & \begin{tabular}[c]{@{}c@{}}Search \\Time \end{tabular} & \begin{tabular}[c]{@{}c@{}}Power \\Consumption\end{tabular}& GFLOPs &  TOP-1\\
    		\midrule
    		MoViNet-A1 & TPUs  & 24h  & 691.2kWh & 6 & 49.3 \\
		    E3D-M & CPU  & 3h  & 0.195kWh & 4.7 & 49.4 \\
    		\bottomrule
    		
        \end{tabular}}
    
    \label{tab:time}
\end{wraptable}
\noindent\textbf{Searching cost comparison}.
Since we apply analytic formulation rather than inference, the calculation of our STEntr-Score has lower hardware requirements, which means that CPU resources can meet it instead of GPU or TPU.
From Table~\ref{tab:time}, our method only takes three hours of searching time with a desktop CPU, while MoViNet consumes 24 hours with 64 commercial TPUs.
Extremely low time and power consumption demonstrate the searching efficiency of our analytic entropy formulation.








\section{Conclusion}

In this paper, we propose to automatically design efficient 3D CNN architectures via an entropy-based training-free neural architecture search approach, to address the problem of efficient action recognition.
In particular, we first formulate the 3D CNN architecture as an information system and propose the STEntr-Score to measure the expressiveness of the system.
Then we obtain the E3D family by an evolutionary algorithm, with the help of STEntr-Score.
Extensive results show that our searched E3D family achieves higher accuracy and better efficiency compared to many state-of-the-art action recognition models, within three desktop CPU hours searching.

\subsubsection*{Acknowledgments}


This research was supported by Alibaba Group through Alibaba Research Intern Program, and ARC DECRA Fellowship DE230101591 to D. Gong.

\bibliography{maxe3d}
\bibliographystyle{iclr2023_conference}


\newpage
\appendix
\section*{Appendix}



In the appendix, we provide a detailed description of notations in this paper (Appendix A), detailed proof of equations (Appendix B), a comparison between different training-free scores on the ImageNet dataset (Appendix C),  a discussion of simple network space in the entropy mechanism (Appendix D), STEntr-Score for maximizing expressiveness (Appendix E), E3D family structure details (Appendix F), experimental setting details (Appendix G), additional result analysis (Appendix H), and future work discussion (Appendix I). 



\section{Meaning of Notations}
\label{sec:notation}


\begin{table}[h]
    \centering
    \caption{The meaning of all notations appeared in this paper.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccl}
    \toprule
       Notation  &  Size &  Meaning \\
    \midrule
         & - & The function of computing the entropy of the given \\
         & - & The distribution of the given \\
         & Constant & The value of expectation\\
         & - & The function of computing expectation\\
         & Constant & The value of variance\\
         & - & The function of computing variance\\
         & Constant & Temporal dimension size of 3D CNN kernel\\
         & Constant & Height dimension size of 3D CNN kernel\\
         & Constant & Width dimension size of 3D CNN kernel\\
         & Constant & Channel dimension size\\
         &  & A 3D CNN kernel size\\
         &  & The weight matrix of the CNN layer\\
         &  & The input feature map size of a given depth (time  height  width)\\
         & - & The cosine similarity distance function \\
         & - & The expanded diversity of cosine similarity distance function\\
        
    \bottomrule
    \end{tabular}
    }
    \label{tab:notation}
\end{table}


\section{Proof of Spatio-temporal Entropy Score}


\subsection{Derivation Process of Differential Entropy}
\label{ssec:proof entropy}

Suppose  is sampled from Gaussian distribution , and we know about the probability density function of :


We can then derive the differential entropy with  as:



\subsection{Expectation and Variance of Feature Map }
\label{ssec:proof variance}

According to \textbf{Theorem 2} and \textbf{Theorem 3}, we can compute the expectation of  layer feature map element , as:



Given two independent random variables  and , based on  and \textbf{Theorem 2}, we can then calculate the variance of the product of these variables as:

We can then derive the variance of , based on \textbf{Theorem 2} and \textbf{Theorem 3}, as:



\subsection{Proof of 3D CNNs Entropy}
\label{ssec:proof 3d cnn}




As the input  is initialized from a standard Gaussian distribution , and all parameters initialized from Gaussian Distribution  , we can formulate Eq. (\ref{eq:expectation}) and Eq. (\ref{eq:variance}) as:

Subsequently, the expectation  and variance  of the last layer can be derived as:

Therefore, the variance can be computed by propagating the variances from previous layers as:

According to Eq. (\ref{eq:gaussian}), the upper bound entropy is proportional to the variance of last feature map. Then we can derive Eq. (\ref{eq:gaussian}) as:



\newpage
\section{Discussion of Simple Network Space}
\label{sec:simple network}

The bias of a convolutional layer is zero, and the activation function in the network is omitted in the search for simplification,  following the work of ZenNAS \citep{lin2021zen} and MAE-DET \citep{sun2022mae}, which has been shown to have no influence on the expressiveness of the network.    
The training of CNN models has been well studied, and some components can be integrated to boost performance. We deliberately avoid using these components to keep our design simple and universal. Nevertheless, these auxiliary components can easily be plugged into the architecture without any special modifications.
Moreover, we provide a discussion of auxiliary components with the entropy calculation, which is listed below.

\noindent\textbf{Batch Normalization (BN)}.
BN is a widely used method to re-center and re-scale the features to make the network converge faster and more stable. BN normalizes entropies adaptively to the network width (which can be related to output variance). When BN is used, networks of different widths will have the same entropy value. Hence, BN has to be removed when calculating entropy.

\noindent\textbf{Activation Function}. 
Activation functions increase the non-linearity of training, which has different effects on entropy. For example, ReLUs, half the variance of the output, decrease the entropy with a constant factor in each layer, having a less positive effect on entropy. Meanwhile, if we formulate each kind of activation for our system, it introduces redundancy and becomes complicated, so we give them a uniform form to omit them in search of concise expressiveness calculation. 



\noindent\textbf{Residual Link}.
If the input and all parameters are initialized from standard Gaussian distribution, the variances with or without residual links are less than 2\% different in entropy score, which means it affects the entropy value slightly. Meanwhile, the residual link has a significant impact on convergence in training.


\noindent\textbf{Squeeze-and-Excitation Module (SE)}.
SE modules are used to adaptively recalibrate channel-wise feature responses by explicitly modeling interdependency between channels. When the input is initialized from a Gaussian distribution, the output after global pooling in SE block is equal to 0 and the final output becomes 0.5, which will lose the ability to model interdependency between channels.


\section{Comparison on Training-Free Scores}
\label{sec:entropy comparison}


\subsection{Comparison on the ImageNet-1K dataset.}
\begin{table}[h]
\captionsetup{font={small}}
    \centering
\begin{tabular}{ccccc}
    \toprule
       Training-free method   & FLOPs & \begin{tabular}[c]{@{}c@{}}Search \\ Devices\end{tabular} & \begin{tabular}[c]{@{}c@{}}Design Cost\\ (hours)\end{tabular} & Top-1 \\
    \midrule
       ResNet-50  & 4.1G & - & - & 78.0 \\
       \midrule
       Zen-score \citep{lin2021zen}  & 4.4G & GPU & 24 & 78.9 \\
       MAE-DET score \citep{sun2022mae}  & 4.4G &  GPU & 14  & 79.1\\
       \midrule
       HomoEntr-Score (w/o K) & 4.3G & CPU & 3  & 79.0 \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of different training-free methods on ImageNet-1K dataset. : Nvidia Tesla V100 16G GPU, : AMD Ryzen 5 5600X 6-Core CPU.}
    \label{tab:training-free comparison}
\end{table}

We conduct comparison experiments using 2D CNNs on ImageNet with the same evolutionary strategies (ResNet design space), as shown in Table \ref{tab:training-free comparison}. 
Compared with the result of ResNet-50, the model searched by HomoEntr-Score improves 1.0\% of accuracy, which indicates that the entropy-based analytic formulation can also measure the information capacity of 2D CNNs.
Compared with Zen-score and MAE-DET, the performance of our proposed formulaic metric can also achieve comparable performance. It means that HomoEntr-Score can work well for modeling the information capacity of 2D CNNs, since there is no (obvious) discrepancy in the information of the two directions in 2D images statistically.



\subsection{Comparison on the Sth-Sth V1 dataset.}
Since there is no existing code available for training-free NAS methods for 3D CNNs, we then refine their implementations for the video recognition task. The results are shown in Table \ref{tab:training-free comparison 3D}.
    \begin{table}[h]
    \centering
\begin{tabular}{cccccc}
    \toprule
       Training-free method   & GFLOPs & \begin{tabular}[c]{@{}c@{}}Search \\ Devices\end{tabular} & \begin{tabular}[c]{@{}c@{}}Design Cost\\ (hours)\end{tabular} & Top-1 & Top-5\\
    \midrule
          X3D-S \citep{feichtenhofer2020x3d} & 2G & - & - & 44.6 & 74.4 \\
       \midrule
       Zen-score \citep{lin2021zen} & 1.9G & GPU & 26 & 45.5 & 74.6\\
       MAE-DET score \citep{sun2022mae}  & 1.9G &  GPU& 15  & 45.8 & 74.7\\
      \midrule
       E3D-S & 1.9G & CPU & 3  & 47.1 & 75.6 \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of different training-free methods on the Sth-Sth V1 dataset. : Nvidia Tesla V100 16G GPU, : AMD Ryzen 5 5600X 6-Core CPU.}
    \label{tab:training-free comparison 3D}
\end{table}

According to the results in Table \ref{tab:training-free comparison 3D}, the performances of other training-free NAS methods are better than X3D-S, but the performance of our searched model is higher.
It indicates directly applying training-free NAS methods can be effective in the video recognition task, but it still needs spatio-temporal refinement on video understanding tasks, which our work mainly focuses on.





\section{Detailed Searching Algorithm and Settings}
\label{sec:algorithm}
To obtain highly expressive 3D CNNs of maximized entropy, we use a customized Evolutionary Algorithm. The step-by-step description of EA is given in Algorithm \ref{al:ea}, as the architecture generator.
We only apply the STEntr-Score to guide the evolution process, not accuracy, which therefore does not need training on the dataset.
We choose EA due to its simplicity, and it is possible to choose other methods, such as reinforcement learning or even greedy selection.
According to our kernel selection observations, we define the 3D kernel size search space within each layer, , , to be chosen as one of the following: \{133, 155, 333\}. These choices enable a layer to focus on and aggregate different dimensional representations efficiently, expanding the network’s receptive field in the most pertinent directions, while reducing FLOPs along other dimensions \citep{kondratyuk2021movinets}.


\subsection{Initial Architecture}
\begin{table}[ht]
\centering
    \caption{E3D-S initial searching. ``stage{}" is a super structure which contains ``layers"-layer 3D inverted bottleneck block. Channels means the output channels of the corresponding convolution.}
        \renewcommand\arraystretch{1.05}
        \setlength\tabcolsep{6pt}
\scalebox{1}{
        \begin{tabular}{ccccc}
    	    \toprule
    		Stage & Kernels & Channels & Layers & \\
    		\midrule
    		\multirow{1}{*}{data} & \multirow{1}{*}{stride 6, 1} & 3 & 1 &     \\
\midrule
    		\multirow{1}{*}{conv} & \multirow{1}{*}{, {24}}  & 24 & 1 &   \\
\midrule
    		{stage}  & [11, 33, 11] & [48,\ 48,\ 24] & 1 & {}  \\
{stage}  & [11, 33, 11] & [96,\ 96,\ 48] & 1 & {} \\
{stage}  & [11, 33, 11] & [192,\ 192,\ 96] & 1 & {} \\
{stage}  & [11, 33, 11] & [192,\ 192,\ 96] & 1 & {} \\
{stage}  & [11, 33, 11] & [384,\ 384,\ 192] & 1 & {}  \\
    		\midrule
    		\multirow{1}{*}{conv} & \multirow{1}{*}{}  & 512 & 1 &    \\
\multirow{1}{*}{pool} &  & 512 &    1 &     \\
\multirow{1}{*}{conv} & \multirow{1}{*}{, }   & [2048,\  \#classes] & 1 &     \\
\bottomrule
        \end{tabular}}
        \label{tab:init}
\end{table}

Firstly, we set up the initial architecture in a MobileNet-styled network, as shown in Table \ref{tab:init}, which consists of five stages with only one layer that can be easily evolved during the algorithm. The initial architecture is inspired by the structure of X3D-S \citep{feichtenhofer2020x3d} because inheriting good prior design can reduce the uncertainty of search space.
Then, based on the initial architecture, applied EA helps us mutate channel dimension, kernel selection, bottleneck expansion ratio, and layer arrangement by randomly selecting the stage. Note that the channel dimension in conv and conv also participate in the mutation process.


\begin{algorithm}[ht]
 \caption{Maximum Entropy Evolutionary Algorithm}
 \label{ag:evolution}
 	\begin{algorithmic}[1]
		
		\REQUIRE Search space . Inference budget , maximal depth , total number of iterations , evolutionary population size , initial structure .
		
		\ENSURE Designed E3D backbone .
		
		\STATE Initialize population .
		\FOR{}
		\STATE Randomly select  and select two stages .
		
		\FOR{}
		\STATE {\bf Switch}\ {Randomly select one target of \{{\bf Kernel} size, {\bf Output} channels, {\bf Bottleneck} channels, {\bf Layers}\} from }\ {\bf do}
		\STATE {\bf Case kernel:}\ {Mutate kernel from 3D kernel search space.}
		\STATE {\bf Case Output:}\ {Mutate output channels with multiplier space.}
		\STATE {\bf Case Bottleneck:}\ {Mutate bottleneck channels with expansion ratio space.}
		\STATE {\bf Case Layers:}\ {Mutate block layers with addend from .}
		\ENDFOR
		\STATE Get mutated network  with two mutated stages .		
		\IF{ is within inference budget  and has no more than  layers}
		\STATE Get STEntr-Score of  and append  to .
		\ENDIF
		\STATE Remove networks of the smallest STEntr-Score if the size of  exceeds .
		\ENDFOR
		\STATE Return , the network of the highest STEntr-Score in .
		
	\end{algorithmic}
	\label{al:ea}
\end{algorithm}



\subsection{Evolutionary Algorithm}

In Algorithm \ref{al:ea}, we randomly initialize a population of candidates from the initial structure, under a computational budget. The population size and total iterations of EA are set to 512 and , respectively.
At each iteration step , we randomly select two stages from the candidates and mutate them. Next, we will randomly select a mutation strategy from 4 strategies for each stage.
Specific mutation strategies for our E3D family are described as follows. We randomly select 3D kernels from \{133, 155, 333\} to replace the current one; interchange the expansion ratio of bottleneck from (); scale the output channels with the ratios ; or increases or decreases depth with 1 or 2. 
Note that the channel dimension of every layer is fixed within from 8 to 640 with multiples of 8, which will help shrink homologous search space and accelerate the search speed.
The mutated structure  is appended to the population if its inference cost does not exceed the budget.
Finally, we maintain the population size by removing networks with the smallest STEntr-Score.
After  iterations, the target network with the largest STEntr-Score is obtained, namely E3D.







\section{E3D Family Architecture Details}
\label{sec:architecture}

Table \ref{tab:family} shows three instantiations of E3D with varying complexity, including E3D-S (1.9G FLOPs), E3D-M (4.7G FLOPs), and E3D-L (18.3G FLOPs).
All models are searched separately with different FLOPs budget (1.9G, 4.7G, and 18.4G) for a fair comparison with X3D-S/M/L as the baseline.
Meanwhile, SE block and ReLU activation function will be added into these architectures for training.
For both training and inference, the input size remains the same: 160 for E3D-S, 224 for E3D-M, and 312 for E3D-L. 
All channel dimensions and layer arrangements are searched by evolutionary algorithm under different given budgets.

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c cc cc cc}
    \toprule
    \multirow{2}{*}{Stage} &
    \multicolumn{2}{c}{E3D-S}&
    \multicolumn{2}{c}{E3D-M}&
    \multicolumn{2}{c}{E3D-L} \\
\multicolumn{1}{c}{} &
    \multicolumn{1}{c}{filters} &
    \multicolumn{1}{c}{output size}   &
    \multicolumn{1}{c}{filters} &
    \multicolumn{1}{c}{output size}   &
    \multicolumn{1}{c}{filters} &
    \multicolumn{1}{c}{output size}  \\
    \midrule
      data   & stride 6, 1 &  & stride 5, 1&  & stride 5, 1 &  \\
    \midrule
       conv  & , 24 &  & , 24 &  & , 24 & \\ 
       \midrule
       stage & 3 &  &3 &  & 3 &  \\
       
       stage & 6 & 
       & 6  &  & 13 & \\
       
       stage & 6 &  &  6  &  & 13& \\
       
       stage & 6 &  &  6  &  & 13& \\
       
       stage & 6 &  & 6  &  & 
       13 &  \\
       \midrule
       conv & ,  &  & , 464 &  & , 480 & \\
       pool &  &  & &  &  & \\
       conv & [2048, \#classes] & &[2048, \#classes] & & [2048, \#classes]& \\
    \bottomrule
    \end{tabular}}
    \caption{Three instantiations of E3D with varying complexity. E3D-S with 1.9G FLOPs, E3D-M with 4.7G FLOPs, and E3D-L with 18.4G FLOPs. The size of output is .}
    \label{tab:family}
\end{table}



\section{Experiment Setting Details}
\label{sec:setting}





\subsection{Datasets}
Our experiments are conducted on three large-scale datasets: Something-Something (Sth-Sth) V1\&V2 \citep{goyal2017something}, and Kinetics400 \citep{kay2017kinetics}. More dataset details can be seen in the supplementary materials.
1) The Sth-Sth datasets are more focused on fine-grained and motion-dominated actions, which contain pre-defined basic actions involving different interacting objects. 
Sth-Sth V1 comprises 86k video clips in the training set and 12k video clips in the validation set. Sth-Sth V2 is an updated version of Sth-Sth V1, which contains 169k video clips in the training set and 25k video clips in the validation set. They both have 174 action categories.
2) The Kinetics dataset contains activities in daily life and some categories are highly correlated with interacting objects or scene context.
Kinetics400 contains over 200k training videos and 20k validation videos divided into 400 categories, covering a wide range of human activities.

\subsection{Implementation Details}
Detailed implementation settings of \textbf{training \& inference stage} on Sth-Sth V1\&V2 and Kinetics400 datasets are listed in Table \ref{tab:implementation}.
All experiments are performed on 8Nvidia Tesla A100 GPUs.

\begin{table}[ht]
    \centering
    \begin{tabular}{l c c}
    \toprule
    Hyperparameter & Sth-Sth V1\&V2 & Kinetics400 \\
    \midrule
       Epoch  & 128 & 256 \\
    Batch Size per GPU & 32 & 16\\
    Optimizer     & SGD & SGD\\
    Learning Rate & 0.8 & 0.4 \\
    Learning Rate Policy & cosine & cosine\\
    Momentum & 0.9 & 0.9 \\
    Weight Decay & 5e & 5e\\
    Warm-up Epoch & 10 & 15 \\
    Synchronized Batch Normalization & True & True \\
    Training from scratch & True & True \\
    \bottomrule
    \end{tabular}
    \caption{List of hyperparameters used on Sth-Sth V1\&V2 and Kinetics400 datasets.}
    \label{tab:implementation}
\end{table}

\section{Additional Results}
\label{sec:more results}

\subsection{Accuracy vs. Complexity}
\begin{wrapfigure}{r}{6.5cm}
    \centering
\includegraphics[width=0.95\linewidth]{pics/tradeoff.pdf}
    \caption{Accuracy/complexity trade-off on the Sth-Sth V2 dataset.}
    \label{fig:tradoff}
\end{wrapfigure}
Figure \ref{fig:tradoff} shows the trade-off between accuracy and complexity (FLOPs). 
Compared to 2D CNN-based methods, E3D requires much lower computational resources. 
Although the performance of our method is similar to Tada-R50, the FLOPs of Tada-R50 are 4.7 times more than E3D-L.
Compared to 3D CNN-based methods, we observe that both E3D and MoViNet can achieve large improvement, which indicates that searched methods have higher efficiency in utilizing computing resources. Also, our method achieves comparable performance compared with MoViNet, which indicates that the proposed training-free STEntr-Score can effectively evaluate the expressiveness of a 3D architecture.



\subsection{HomoEntr-Score vs. STEntr-Score}
Table \ref{tab:hescore vs stescore} reports E3D results searched by HomoEntr-Score and STEntr-Score, under the same search settings.
The results show substantial improvement when using STEntr-Score instead of HomoEntr-Score, which indicates the effectiveness of STEntr-Score to handle the discrepancy of visual information in spatial and temporal dimensions.
Even though without refinement factor, the performance of HomoEntr-Score searched E3D still outperforms X3D, which means the entropy-based search strategy can also 
measure the expressiveness of 3D CNN architectures.

\begin{table}[h]
\captionsetup{font={small}}
    \centering
\begin{tabular}{lcccc}
    \toprule
       Model   & Resolution & GFLOPs & Top-1 & Top-5\\
    \midrule
      X3D-S* \citep{feichtenhofer2020x3d} & 13160 & 2 & 44.6 & 74.4\\
      MoViNet-A0* \citep{kondratyuk2021movinets} & 50172 & 2.7 &  46.9 & 75.0 \\
    \midrule
      E3D (HomoEntr-Score) & 13160 & 1.9 &  45.8 & 74.8\\
      E3D (STEntr-Score) & 13160 & 1.9 &  47.1 & 75.6\\
    \bottomrule
    \end{tabular}
    \caption{Comparison of different entropy scores on the Sth-Sth V1 dataset. * denotes our reproduced models.}
    \label{tab:hescore vs stescore}
\end{table}

\subsection{3D Kernel Search Space}
To analyze the impact of kernel search space, we expand the 3D kernel search space and conduct experiments, as shown in Table \ref{tab:search space}.
The results indicate that larger search spaces actually benefit the performance. However, compared to the results between E3D (HomoEntr-Score) with E3D (STEntr-Score)) in Table \ref{tab:hescore vs stescore}, the STEntr-Score based searching can boost the performance (+1.3\%) more than a large search space did (+0.2\%). 
It also verified the effectiveness of our proposed STEntr-Score in evaluating the expressiveness of 3D CNNs.

\begin{table}[h]
\captionsetup{font={small}}
    \centering
\begin{tabular}{cccc}
    \toprule
       Kernel Search Space   & FLOPs & Top-1 & STEntr-Score\\
    \midrule
      133, 333  & 1.9G &  46.3 & 198.55\\
      133, 155, 333  & 1.9G &  47.1 & 202.86\\
      133, 155, 333, 311  & 1.9G &  47.1 & 202.74\\
      133, 155, 333, 533  & 1.9G &  47.2 & 203.13\\
\bottomrule
    \end{tabular}
    \caption{Comparison of different 3D kernal search space on the Sth-Sth V1 dataset.}
    \label{tab:search space}
\end{table}
\newpage
\subsection{Inference time comparison}
We report the inference time comparison with some state-of-the-art methods in Table \ref{tab:time}.
All models are trained and tested on the Sth-Sth V1 dataset, and the batch size is set to 16.
Compared to X3D, our E3D performs better not only on accuracy but also costs lower inference time. It indicates that the searched architecture by our proposed STEntr-Score is more effective and efficient for video understanding.
Compared to MoViNet, even though Top-1 accuracies are similar, both latency and throughput of E3D are performing better. Due to MoViNet applies a causal convolutional network and contains more parameters.
Compared to 2D CNN-based methods, E3D performs better on both accuracy and running time and requires much lower computational resources.
Overall, we believe that our proposed E3D family is more efficient and practical for real-world applications. 
 

\begin{table}[ht]
    \caption{Inference comparison using a Tesla V100 on the Sth-Sth V1 dataset.}
    \centering
    \setlength\tabcolsep{3pt}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccc}
    	     \toprule
    		 Method & Resolution & Frame  & GFLOPs & \#Param & Top1 & Latency (ms/video) & Throughput(video/s)\\
    		\midrule
    		TSM \citep{lin2019tsm} & 256 & 16 & 65 & 23.9M & 47.2 & 23.0 & 43.5\\
    		TANet \citep{liu2021tam} & 256 & 16  & 66 & 26M & 47.6 & 14.7 & 68.0\\
    		\midrule
X3D-M \citep{feichtenhofer2020x3d} & 224 & 16  & 4.7 & 3.7M & 47.3 & 13.5 & 74.1\\
    		MoViNet-A1 \citep{kondratyuk2021movinets} & 172 & 50  & 6 & 4.6M & 49.3 & 21.9 & 45.7\\
    		
    		\midrule
E3D-M & 224 & 16  & 4.7 & 3.4M & 49.4 & 11.4 & 87.7\\
    		\bottomrule[1pt]
    		
        \end{tabular}}
    
    \label{tab:time}
\end{table}

















\section{Future Direction}
\label{sec:future}



\noindent\textbf{Data-driven design}.
The design of STEntr-Score search correlates with parameter initialization and kernel selection, with standard Gaussian initialization input.
If we replace the Gaussian input directly with target data, the output after a convolution will be random due to the Gaussian initialized weights, as the process of STEntr-Score based searching is contained without data training. 
The aim of our work is therefore to provide a training-free approach to 3D CNN architecture design according to the maximum entropy principle under the given budgets. We believe that the training-free method, combined with target data without training, could be a future direction for research.

\noindent\textbf{Transformer model}.
We believe that the principle of maximum entropy is theoretically applicable to transformers. However, there exist some challenges to overcome. For example, Transformer has more complex components than CNN, such as `Q' and `K' kernel operation and multi-head attention, which is difficult to calculate the maximum entropy. In addition, the discrepancy of visual information in spatial and temporal dimensions by Transformer still remains a challenge. Although these challenges are difficult to overcome, this would be a fascinating task for us in the future.


\end{document}
