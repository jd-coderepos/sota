\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{float}
\usepackage{graphicx}
\usepackage{graphics}

\newcommand{\tm}[1]{\textcolor{blue}{[TM: #1]}}



\title{Improving Long Tailed Document-Level Relation Extraction via Easy Relation Augmentation and Contrastive Learning}

\author{Yangkai Du\textsuperscript{1}, Tengfei Ma\textsuperscript{2}, Lingfei Wu\textsuperscript{3}, Yiming Wu\textsuperscript{1}, Xuhong Zhang\textsuperscript{1} \\\bf{Bo Long}\textsuperscript{3}, \bf{Shouling Ji}\textsuperscript{1} \\
        \textsuperscript{1}Zhejiang University; \textsuperscript{2}IBM Research; 
        \textsuperscript{3}JD.COM\\
        \texttt{\{{yangkaidu,wuyiming,zhangxuhong,sji}\}@zju.edu.cn} \\
        \texttt{tengfei.ma1@ibm.com, \{lingfei.wu,bo.long\}@jd.com}
        }



\begin{document}
\maketitle
\begin{abstract}
Towards real-world information extraction scenario, research of relation extraction is advancing to document-level relation extraction(DocRE). Existing approaches for DocRE aim to extract relation by encoding various information sources in the long context by novel model architectures. However, the inherent long-tailed distribution problem of DocRE is overlooked by prior work. We argue that mitigating the long-tailed distribution problem is crucial for DocRE in the real-world scenario. Motivated by the long-tailed distribution problem, we propose an Easy Relation Augmentation(ERA) method for improving DocRE by enhancing the performance of tailed relations. In addition, we further propose a novel contrastive learning framework based on our ERA, i.e., ERACL, which can further improve the model performance on tailed relations and achieve competitive overall DocRE performance compared to the state-of-arts. 

\end{abstract}

\section{Introduction}
Relation extraction plays an essential role in information extraction, which aims to predict relations of entities in texts. Early work on relation extraction mainly focuses on sentence-level relation extraction, i.e., predicting relation from a single sentence, and has achieved promising results. Recently, the research of relation extraction has advanced to document-level relation extraction, a scenario more practical than sentence-level relation extraction and more challenging.

The relation pattern between entity pairs across different sentences is often more complex, and the distance of these entity pairs is relatively long. Therefore, DocRE requires models to figure out the relevant context and conduct reasoning across sentences instead of memorizing the simple relation pattern in a single sentence. Moreover, multiple entity pairs co-exist in one document, and each entity may have more than one mention appearing across sentences. 
Thus, DocRE also requires the model to extract relations of multiple entity pairs from a single document at once. In other words, DocRE is a one-example-multi-instances task while sentence-level RE is a one-example-one-instance task.

Another unique challenge of DocRE that cannot be overlooked is long-tailed distribution. Long-tailed distribution is a common phenomenon in real-world data. In DocRE, we also observe the long-tailed distribution. Figure \ref{fig:docred-long-tail-distribution} presents the relation distribution of DocRED\citep{yao-etal-2019-docred}, a widely-used DocRE dataset:  most frequent relations from  relations takes up  of total relation triples; while the frequencies of  relations are only less than . Vanilla training on long-tailed data will cause the model to achieve overwhelming performance on head relations but underfitting on tailed relations. Although the overall DocRE performance is largely dependent on performance on head relations since they are the majority, model failure on tailed relations is a big concern in real-world DocRE scenarios. 
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{relation_distribution.pdf}
    \caption{Relation Distribution of DocRED Train set. Relation index are sorted by frequency count from high to low.}
    \label{fig:docred-long-tail-distribution}
\end{figure}

Data augmentation is a commonly used strategy for addressing the long-tailed problem. Nonetheless, applying data augmentation efficiently on DocRE is non-trivial. Ordinary data augmentation operation on the document, including text random-dropping or replacing\citep{wei-zou-2019-eda} would require the DocRE model for extra encoding process of the entire document, which is computation in-efficient on DocRE since the document may contain numerous sentences. Besides, DocRE is a one-example-multi-instances task, so tailed relations and head relations presumably co-exist in one document. As a result, the head relations would also be augmented if we augment the tailed relations by aforementioned trivial augmentation methods on text, which is unexpected and may lead to over-fitting on head relations.

In this paper, we propose a novel data augmentation mechanism for DocRE, named ERA, for improving the document-level relation extraction by mitigating the long-tailed problem. The proposed ERA method applies augmentation on relation representations rather than texts, so it can augment tail relations without another encoding operation of the long document, which is computation-efficient and also effective for improving performance on tailed relations.

In addition, we propose a contrastive learning framework based on our ERA method, i.e., ERACL, for pre-training on the distantly-supervised data. The proposed ERACL framework can further enhance the model performance on tailed relations and achieve comparable overall DocRE performance compared to the state-of-art methods on DocRED.

\section{Background and Related Works}
\subsection{Problem Formulation}
Given a document  with  words, a set of  entities  are identified by human annotation or external tools. For each entity ,  mentions of  denoted as  are also annotated by providing the start position and end position in . In addition, the relation scheme  is also defined.

The objective of DocRE is to extract the relation triple set  from all possible relation triples, where each realtion triple  extracted by the model can be interpreted as relation  holds between head entity  and tail entity . For future simplicity, we denote tail relations as  and head relations as .
\subsection{Document-Level Relation Extraction}
\label{sec:docre-background}


To address the prior challenges in DocRE, one main branch of DocRE works use \textbf{Graph-based Methods}\citep{sahu_inter-sentence_2019,christopoulou_connecting_2019,wang_global--local_2020,zeng_double_2020,nan_reasoning_2020,li_graph_2020,xu_discriminative_2021}. The general idea of graph-based methods is to conduct multi-hop reasoning across entities, mentions and sentences in a document by graph neural networks. First a document is converted to a document graph by human designed heuristics, attention mechanism or dependency parser. Then the document graph is encoded by graph neural networks\cite{DBLP:conf/iclr/KipfW17,NEURIPS2018_182be0c5,wu2021graph} to conduct multi-hop reasoning across graph nodes and edges. Another branch of DocRE methods adopt \textbf{Transformer-based Methods}\citep{wang_fine-tune_2019,ye_coreferential_2020, DBLP:conf/aaai/XuWLZM21, zhou_atlop_2021, zhang_docunet_2021}. Transformer-based methods rely on the strong long-context representation capability of pre-trained transformers\citep{devlin-etal-2019-bert,liu_roberta_2019}. In addition, self-attention mechanism in transformer architecture can implicitly model the dependency between entities, mentions and contexts, which can be utilized for relation reasoning\citep{zhou_atlop_2021}. 





Different from previous works, in this paper we focus more on addressing the challenges of long-tailed distribution in DocRE. 
\subsection{Contrastive Learning}
\label{sec:contrastive-learning-background}
Contrastive learning is proved to be a promising self-supervised pre-training task for image recognition\citep{pmlr-v119-chen20j,he_momentum_2020}.
The principle of contrastive learning is to increase the representation similarity of anchor example  and positive examples  while decreasing the representation similarity of anchor example  and negative examples  by INFONCE loss\citep{Oord_INFONCE_2018}. 


Under the self-supervised setting, positive samples  are constructed by data augmentation operation, including image cropping, resizing on anchor samples. The motivation of creating  via data augmentation is that augmented samples are still similar or even the same in semantic space, then it can provide training signals for self-supervised pre-training. Therefore, models pre-trained by self-supervised contrastive learning can learn task-agnostic and robust representation for down-streaming tasks, which also can capture the semantic information of input samples.

The general contrastive learning framework has been applied in language tasks and achieved competitive performance.  \citet{fang_cert_2020} adapted the contrastive learning framework for self-supervised pre-training on transformers and achieved superior performance compared to BERT\cite{devlin-etal-2019-bert}. \citet{sclnlp_gunel_2021} proposed to use supervised contrastive learning for more robust fine-tuning on pre-trained transformers. In relation extraction, \citet{peng-etal-2020-learning} and \citet{qin-etal-2021-erica} adopted contrastive learning as one pre-training task to improve the relation understanding capability of BERT. It has been demonstrated that proper adaptation of contrastive learning framework can encourage the model to learn more robust task-agnostic or task-related representations, especially when the training data is limited. The problem of long-tail relation distribution is essentially lack of training samples of certain relation types. Considering this, we proposed a new contrastive learning framework based on a new data augmentation method in DocRE, called Easy Relation Augmentation(ERA), which can learn more robust relation representations for DocRE, especially for the tailed relations.

\section{Easy Relation Augmentation}
\subsection{Overview}
We summarize the main components of ERA framework as follow: ERA takes a document  as input, then the Document Encoding and Relation Encoding modules will encode each entity pair  from two aspects: contextualized entity representation and pooled context representation via self-attention mechanism of Pre-trained Transformers\citep{zhou_atlop_2021}. Afterwards, we proposed a novel Easy Relation Augmentation(ERA) mechanism to enhance the entity pair representation by applying a random mask on pooled context representation. The proposed ERA mechanism can augment the tail relations  without another Relation Encoding and Document Encoding, which is computation-efficient and also effective. Finally, we train the relation prediction module on the augmented relation representations.
\subsection{Document Encoding}
    In light of the promising capability of Pre-trained Transformers\citep{devlin-etal-2019-bert,liu_roberta_2019} for modeling the long-range text dependency, We resort to pre-trained transformers for document encoding. We add a special entity marker "*"\citep{zhang-etal-2017-position} at the start and end position of each mention , and "*" can be replaced with other special tokens. Entity markers can spotlight the mention words and also provide entity positional information for Pre-trained Transformers, which proves to be effective in DocRE\citep{zhou_atlop_2021}. Feeding the document  to the pre-trained transformers, we can get the contextualized representation  of all words and vanilla multi-head self-attention  from the last block of Pre-trained Transformers().
    
    Where , .  is the model dimension of the Pre-trained Transformers and  is the number of self-attention heads of Pre-trained transformers.
\subsection{Relation Encoding}
    Given the contextualized representation  and self-attention  of the document , the goal of Relation Encoding module is to encode each entity pair  by aggregating the contextualized entity representation and pooled context representation, which are crucial for relation understanding and reasoning across the long document.
    
    Contextualized entity representation can provide the contextualized entity naming and entity typing information for relation inference. For entity , we obtain the contextualized mention representation by collecting the Pre-trained transformer last layer output of "*" marker at the start of mention , denoted as . Subsequently, we can get the final contextualized entity representation  by logsumexp pooling \citep{jia-etal-2019-document}, which can achieve better results compared to max pooling and average pooling on DocRE\citep{zhou_atlop_2021}.
    
    
    As mentioned in Section \ref{sec:docre-background}, DocRE requires the model to capture the dependencies among entities, mentions, and context words, and also filter out the unnecessary context information from the long document. We named the aforementioned information as pooled context information. The self-attention matrix  obtained from Pre-trained transformers have already implicitly modeled the dependency among entities, mentions, and context words, which can be utilized for getting meaningful pooled context representation\citep{zhou_atlop_2021}. We follow \citet{zhou_atlop_2021} to obtain the pooled context information by utilizing the self-attention matrix .
    
    Given a entity pair , one can get the pooled context representation  by Equation \ref{eq:pooled-context-representation} and \ref{eq:attention-dot-product}.
    
    
    
    Where , and .  is the attention score of entity  to all words in , which is obtained by averaging the attention score of all entity mentions , denoted as . Similar to contextualized mention representation , we obtain the mention attention score  by indexing the vanila self-attention matrix  with position of starting "*" marker. In addition, note that the vanila self-attention matrix is first averaged over all attention heads before performing the indexing.  is also calculated following the same procedure.
    
    In the end, for the entity pair , we can form a triple represention .  contains all the information for relation prediction and form the basis for our Easy Relation Augmentation and Contrastive Leaning framework.
    
\subsection{Relation Representation Augmentation}
    \label{sec:era-relation-aug}
     To address the long-tailed problem residing in the DocRE, we propose a novel Easy Relation Augmentation(ERA) mechanism to increase the frequency of tailed relations and enhance the entity pair representation.
    


    Denote the set of triple representation of all entity pairs as . In addition, we can manually select the set of relations need to be augmented, i.e., . 
    
    Given a entity pair  whose relation , we first retrieve the original triple representation  from . Recall that the pooled context representation  encodes the unique context information for relation inference, and a slight perturbation on the context should not affect the relation prediction. Established on this intuition, we add a small perturbation on .
    
    We first apply a random mask on  described in Equation \ref{eq:pooled-context-representation} by multiplying  with a randomly generated mask vector . Each dimension of  is in  and generated by a Bernoulli distribution with parameter .
    
    Applying the random mask on attention score  can be interpreted as randomly filter out some context information since the attention score for them are set to . In addition, the degree of perturbation can be controlled by setting proper .
    Then we can get the perturbed pooled context representation  in Equation \ref{eq:pertubed-context-representation}.
    
    
    For all the entity pairs  whose relation  in , we apply the prior steps to get  distinct pertubed context representations  by using  random mask, where  is a hyper-parameter for controlling the number of ERA operations. Eventually, we can get the augmented triple representation set , which can be formulated in Equation \ref{eq:augset-formulaiton}.
    
    
    Combining the original triple representation set  and , we can get the total tripe representation set  for relation prediction and our Contrastive Learning framework.
    
    
\subsection{Relation Prediction}
    Based on the triple representation of all entity pairs, the relation prediction module finally predict the relations hold between each pair. For a triple representation , we first apply two linear transformations with Tanh activation to fuse the pooled context representation  with  and .
    
    
    Where , which are trainable parameters of the model. Following \citet{zhou_atlop_2021}, then we used a grouped bi-linear layer to calculate a score for relation , which splits the vector representation to  groups and performs bilinear within group.
    
    Where  is the bilinear parameter of group . During training stage, we apply the adaptive thresholding loss \citep{zhou_atlop_2021} to dynamically learn a threshold  for each entity pair by introducing a threshold class .
    
     is the set of all valid relations that hold between entity pair , and it is empty when no relation hold between the pair. In addition, . In the inference stage, the threshold  for valid relation scores is set to .
   


\section{Contrastive Learning for relation pre-training}
\subsection{Overview}
   We propose a contrastive learning(CL) framework for unifying the augmented relation representations and improving the robustness of learned relation representations, especially for tailed relations. Specifically, we use the CL framework for pre-training on the distantly-supervised DocRE dataset\citep{yao-etal-2019-docred}, which is annotated by querying the knowledge graph but is noised. Considering that the model will be fine-tuned on the human-annotated dataset after the representation learning stage, the noise in the distantly supervised dataset is acceptable and correctable.
    
    Under DocRE setting, we claim that the semantically-similar samples should be the entity pairs that have the same relation , including both of the original pairs and augmented pairs by ERA. However, only a few entity pairs have the same relation within one document, especially for the tailed relation . 
    
    Increasing the mini-batch size can partially mitigate the problem, but it requires large GPU memory for training which may not be accessible. Thus, we adapted the MOCO framework \citep{he_momentum_2020} to the DocRE setting, named MoCo-DocRE. The moCo-DocRE framework can conduct the CL without using large batch-size by keeping a relation representation queue  holding  relation representations from the previous mini-batch for each relation . This allows us to reuse the encoded positive and negative relation representations in prior mini-batches. We summarize our contrastive learning framework in Figure \ref{fig:cl_overview}.
    \begin{figure}[t]
        \centering
        \includegraphics[width=8cm]{cl_overview.pdf}
        \caption{Overview of the proposed MoCo-DocRE framwork.}
        \label{fig:cl_overview}
    \end{figure}
\subsection{Anchor relation encoding}
    For document  in pre-training dataset, we first conduct the aforementioned document encoding, relation encoding and Easy Relation Augmentation(ERA) and obtain the triple representation set  of all entity pairs. For a triple representation , we use two linear transformations to fuse the triple representation, which are same as Equation \ref{eq:h_fusing} and \ref{eq:t_fusing}. Next, we use a MLP layer with ReLU activation for final relation representation:
    
    Where  denotes the vector concatenation operation,  and  are trainable model parameters in pre-training stage, and  is the dimension of final relation representation . After contrastive pre-training, the MLP layer will not be used for relation prediction in fine-tuning.
\subsection{MoCo-DocRE}
    To keep the consistency of relation representation in , we also use a momentum updated model to encode the positive and negative samples in contrastive learning\citep{he_momentum_2020}. The original model  is updated via back-propagation, and the momentum-updated model  is updated by Equation \ref{eq:momentum-updated}. 
    
    Where  is the momentum hyper-parameter, which can control the evolving speed of . Next we feed the document  to  for  by following the same procedure as getting anchor relation representation . Then we push  to  relation representation queues according to their relation labels. If relation  holds between , then  will be pushed to . Eventually, we can get the set of positive and negative relation representations of  from queues, i.e,  and .
    
    For anchor relation representation , now we can formalize the INFONCE loss \citep{Oord_INFONCE_2018} under our MoCo-DocRE in Equation \ref{eq:info-nce}.
    
    Where  is the temperature hyperparameter. In addition  in Equation \ref{eq:info-nce} are \textit{l2}-normalized.


\begin{table*}[ht]
    \centering
\begin{tabular}{lcccc}
        \toprule
        \multirow{2}*{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Dev}} & \multicolumn{2}{c}{\textbf{Test}}  \\ 
         & Ign  &  & Ign  &  \\
        \midrule
        \textit{Graph-based Methods}  & & & &\\     
        GLRE\citep{wang_global--local_2020} & -- & -- & 55.40 & 57.40 \\
        LSR\citep{nan_reasoning_2020} &52.43&  59.00& 56.96& 59.05 \\
        GAIN\citep{zeng_double_2020} &59.14& 61.22 & 59.00& 61.24\\
        DRN\citep{xu_discriminative_2021} &59.33& 61.39& 59.15& 61.37 \\
\midrule
        \textit{Transformer-based Methods} & & & &\\ 
SSAN\citep{DBLP:conf/aaai/XuWLZM21} &57.04& 59.19& 56.06& 58.41 \\
        ATLOP\citep{zhou_atlop_2021} & 59.22  & 61.09  &59.31 &61.30 \\
        DocuNet\citep{zhang_docunet_2021} &59.86 &61.83& 59.93& 61.86 \\
        AFLKD\citep{tan-etal-2022-document} &60.08 &62.03 &60.04 &62.08 \\
        
        
        \midrule
        \textit{Our Methods} & & & &\\
        ERA & 59.30  0.09 & 61.30  0.08 &58.71 &60.97 \\
        ERACL &59.72  0.19 & 61.80  0.20&59.08 &61.36  \\
        \toprule
    \end{tabular}
    \caption{Overall DocRE performance evaluated on DocRED benchmark. We report the mean and standard deviation of 3 runs with different random seeds on the development set. The official test results are reported by the best checkpoint on the development set. Results of all other models are reported in their original paper and use BERT-base-cased as backbone encoder.}
    \label{tb:main-results}
\end{table*}

\section{Experiment}
\subsection{Experiment Setup and Dataset}
   \begin{table}[h]
        \centering
        \begin{tabular}{lcc}
            \toprule

            Statistics & DocRED & HacRED \\
            \midrule
            \# Train & 3053 &6231 \\
            \# Dev &1000 &1500 \\
            \# Test &1000 &1500 \\
            \# Relations &96 &26 \\
            \toprule
        \end{tabular}
        \caption{Dataset Statistics of DocRED and HacRED}
          \label{tb:data-stat}
    \end{table}
    
    \textbf{Dataset:}
   We evaluate the proposed ERA and contrastive learning framework on two popular DocRE datasets, DocRED\citep{yao-etal-2019-docred} and HacRED\citep{cheng-etal-2021-hacred}. DocRED contains 5053 English documents extracted from Wikipedia and 96 relations, which are human-annotated. Besides, DocRED also provide a distantly-supervised dataset with 101873 documents, and the relation of entitity pairs are annotated by querying Wikidata. HacRED is a human annotated Chinese dataset with 26 relations. Statistics of Datasets are listied on Table \ref{tb:data-stat}.  \\
    \textbf{Implementation Details:} We use the pre-trained BERT-base-cased\citep{devlin-etal-2019-bert} as our backbone for DocRED dataset. All the hyperparameters are tuned on the development set. Specifically, we set the random mask probability  to 0.1 and the number of augmentation  to . In addition, the number of grouped bilinear  is set to . The temperature parameter  is set to  and the size of , i.e.,  is set to , and the momentum  is set to 0.99. The learning rate is set to  for pre-training on our CL framework. In the fine-tune on human-annotated data, we set the learning rate to  for parameters of BERT and  for other parameters. We use AdamW\citep{losch_adamw_2019} for optimization of all parameters and a linear-decayed scheduler with a warmup ratio . Gradients whose norm is larger than  are clipped. For HacRED dataset, we use XLM-Roberta-base\citep{conneau-etal-2020-unsupervised} as backbone. Under HacRED scenario, we set the random mask probability  to 0.05 and the number of augmentation  to . All the other parameters are same as the DocRED scenario. 
    
\subsection{Evaluation Metric}
    DocRED benchmark provide two evaluation metrics, i.e.  Ign .  is the minor  value for all predicted relations in test/development dataset, which can reflect the overall performance of DocRE. Compared to , Ign  excludes the entity pairs which appear both on training and test/dev data.
    To demonstrate how ERA and contrastive learning can improve the performance of tailed relations, we propose to use the following evaluation metrics: \\
    : it computes the  value by first calculating  for each relation separately and then getting the average of all relation classes. Compared to minor , macro  treat all relation classes equally,  of tailed relations will have equal impact compared to head relations.\\
    ,,: Those metrics target at tailed relations whose frequency count in train dataset is less than 500,200,100 respectively. Values are computed by averaging the F1 value of the targeted relations.

\begin{table*}[h]
\centering
    \begin{tabular}{lcccc}
    \toprule
    Methods &  &  &  &   \\
    \midrule
    DocuNet &40.69 & 36.54 & 28.95 & \bf{22.37}          \\
    ATLOP   & 39.54 & 35.20 & 26.82 & 18.76         \\
    \hline
    --Deletion &39.22 &34.88 &26.62 &18.78 \\
    --Mask &38.31 &33.80 &25.30 &17.35 \\
    --AFL &40.04 &35.78 &27.78 &20.52 \\
    \hline
    ERA     & 40.55 & 36.21 & 28.51 & 20.50 \\
    ERACL   & \bf{41.34} & \bf{37.13} & \bf{29.43} & 22.31     \\
    \toprule
\end{tabular}
    \caption{Evaluation on tailed relations. All the results are averaged on 3 runs with different random seeds on development set. Relation labels of test set are not accessible, so the results on test set cannot be reported.}
    \label{tb:tail-results}
\end{table*}

\subsection{Main Results}
We compare the proposed ERA and ERACL methods to graph-based and transformer-based models on the DocRED benchmark by using  and Ign  metrics on the dev/test dataset. Results are reported in Table \ref{tb:main-results}. The proposed ERACL method, which first conducts contrastive learning under our MoCo-DocRE framework on the distantly supervised dataset and then conducts ERA fine-tuning on the training set, can achieve competing  and Ign  value, compared to state-of-art graph-based methods and transformer-based methods. Besides, compared to ATLOP\citep{zhou_atlop_2021} which is the baseline of ERACL, ERACL can improve the minor  on the development set by  and ERA can improve the minor  by , which demonstrate the effectiveness of the proposed ERA method and contrastive learning pretraining.



\subsection{Results on tailed relation}
    To demonstrate the effectiveness of our ERA and ERACL on improving model performance on tailed relations, we evaluated ERA and ERACL using Macro, Macro@500, Macro@200, and Macro@100 metrics. Besides, we compare ERA and ERACL with three baseline methods which are used for addressing long-tailed distribution on DocRED. 
    \textbf{Text Random Deletion/Mask} are commonly used data augmentation techniques for NLP tasks. We apply the Text Random Deletion and masking as data augmentation for documents which contain  tailed relations. 
    \textbf{Adaptive Focal Loss} proposed by \citet{tan-etal-2022-document} is a adaptation of Focal Loss\citep{Lin_2017_ICCV} on DocRE scenario. AFL proved to be effective on tail relations. We implement those three methods based on ATLOP. Results are listed on Table \ref{tb:tail-results}. The proposed ERA and ERACL method can outperform those three baselines on tailed relations, which demonstrates the effectiveness and necessities of our ERA and ERACL for addressing the long-tailed distribution on DocRE scenario.
    
    Moreover, the proposed ERA method can improve the Macro over ATLOP by ,  on Macro@500,  on Macro@200, and  on Macro@100. We observe that the improvements are more significant on relations that appear less frequently. In addition, the proposed ERACL method can further gain improvements over ERA: 0.79 on Macro, 0.92 on Macro@500, 0.92 on Macro@200, 1.81 on Macro@100, which also show similar trends as ERA over ATLOP.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=7cm]{long-tail-demo2.pdf}
        \caption{Comparison of F1 across 10 relation clusters. All results are averaged by 3 runs with different random seeds.}
        \label{fig:relationF1-demo}
    \end{figure}
    
      \begin{table*}[h]
        \centering
        \begin{tabular}{lccccc}
        \toprule
        Methods & F1 &  &  &  &  \\
        \midrule
        ERACL   & 61.80 & 41.34 & 37.13     & 29.43     & 22.31     \\
        \hline
        -- ERA   & 61.36 & 40.49 & 36.22     & 28.61     & 21.52     \\
        -- CL    & 61.30 & 40.55 & 36.21     & 28.51     & 20.50     \\
        -- both  & 60.97 & 39.54 & 35.20     & 26.82     & 18.76    \\
        \toprule
        \end{tabular}
        \caption{Ablation Study on development set.}
        \label{tb:ablation-study}
    \end{table*}
    
    To better illustrate the performance gain on the tailed relations, we sort 96 relations according to their frequency count in the DocRED train set from high to low, then slice 96 relations to 10 relation clusters equally for more clear visualization. For each cluster, we calculate the cluster F1 by averaging the F1 of relation within the cluster. The results are demonstrated in Figure \ref{fig:relationF1-demo}. We observe that the proposed ERA method gain improvements compared to ATLOP on relation clusters 4-10, which correspond to the tailed relation in DocRED, and also achieve competing performance on clusters 1-3, which correspond to the head relations. Those findings show that our ERA methods are effective for improving the DocRE performance on tailed relations while keeping the performance on head relations. In addition, similar performance gain is also achieved by the proposed ERACL method, and ERACL can further improve the tailed relations compared to ERA and achieve competing performance on head relations. 
    
    In addition, we conduct another set of experiments by manually reducing the percentage of training data in order to explore the performance of the proposed ERA methods and ERACL methods under a limited-data scenario. The results are listed in Table \ref{tb:with-few-data}. Compared to the setting that uses all of the train data, we observe that the performance gain of the proposed ERA and ERACL under 10\% and 5\% settings are more significant, which also indicate that the proposed ERA and ERACL can improve the DocRE performance by mitigating the long-tailed problem and are especially effective when training data is limited.
    \begin{table}[!h]
        \centering
        \begin{tabular}{lccc}
            \toprule
            Methods & F1 &  & \\
            \midrule
            \textit{10\%} &  &  &     \\
            ATLOP &51.92   &23.68  &7.97  \\
            ERA    &52.46   &24.80  &11.20   \\
            ERACL    &\bf{53.73}   &\bf{27.88}  &\bf{14.89}   \\
            \hline
            \textit{5\%} &  &  &     \\
            ATLOP    &45.17   &16.17   &5.59    \\
            ERA     &47.18  &17.27    &6.22    \\
            ERACL     &\bf{49.40}   &\bf{23.47}   &\bf{11.84}    \\
            \toprule
        \end{tabular}
        \caption{Results on the development set under the limited-data setting. 10\% refers to only using 10\% of training data, and 5\% refers to only using 5\% of training data. All results are reported by averaging 3 runs with different random seeds.}
        \label{tb:with-few-data}
    \end{table}
    
    Besides, we also conduct experiments on HacRED to investigate whether our ERA can generalize well on other long-tailed DocRE datasets. Results are shown on Table \ref{tb:hacred-results}. We observe that ERA can still outperform the ATLOP on tailed relations.
    
     \begin{table}[!h]
        \centering
        \begin{tabular}{lccc}
            \toprule
            Methods & F1 &  & \\
            \midrule
            ATLOP &77.84   &70.99  &55.11  \\
            ERA    &78.27   &71.73  &57.13   \\
            \hline
            \toprule
        \end{tabular}
        \caption{Results on HacRED. Since HacRED do not have distant-labeled data, we can only evaluate ERA on HacRED. ATLOP results are implemented by us. All experiments use XLM-Roberta-base as the backbone encoder.}
        \label{tb:hacred-results}
    \end{table}
    
    
    
    
    
    
\subsection{Ablation Study}
    To evaluate the contribution of the ERA and contrastive learning(CL) framework separately, we conduct an ablation study on the development set by reducing one component at a time. The results are shown in Table \ref{tb:ablation-study}. All of the results are tuned on the development set for best performance. 
    
    Note that reducing ERA refers to turning off the relation representation augmentation operation described in Section \ref{sec:era-relation-aug} and only keeping the original relation representations. In addition, reducing CL means without conducting contrastive learning on distantly supervised data. We observe that the ERA component and contrastive learning(CL) framework are almost equally important, which lead to  and  performance drop on F1 metric,  and  performance drop on Macro F1.
    
\section{Conclusion}
   We propose a novel Easy Relation Augmentation(ERA) method for the Document-level Relation Extraction task, which improves the DocRE performance by addressing the long-tailed problem residing in DocRE by augmentation on relation representations. In addition, we propose a novel contrastive learning framework based on ERA, i.e., MoCo-DocRE, for unifying the augmented relation representations and improving the robustness of learned relation representations, especially for tailed relations. Experiments on the DocRED dataset demonstrate that the proposed ERA and ERACL can achieve competing performance compared to state-of-arts models, and we demonstrate that the performance gain of ERA and ERACL are mainly from the tailed relations.
    
    Nonetheless, addressing the long-tailed problem is still challenging for DocRE. One limitation of our method is it still relies on large amount of annotated data to achieve overwhelming performance. We hope it can be mitigated in future research.








































\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}







\end{document}
