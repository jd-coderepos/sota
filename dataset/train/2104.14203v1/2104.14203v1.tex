\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{atbegshi}

\usepackage{etoolbox}

\newbool{draftmode}

\boolfalse{draftmode}

\DeclareOption{draft}{
    \booltrue{draftmode}
}

\ProcessOptions\relax

\ifbool{draftmode}{

    \PackageWarning{customization/core.sty}{Draft mode is on.}

}{
}

\usepackage{algorithm}

\usepackage[
]{algorithmic}

\usepackage{fixme}

\ifbool{draftmode}{

    \ifdef{\fxsetup}{

        \fxsetup{
            status=draft,
            layout=inline,
            author=,
            theme=color
        }

    }{
}

}{

    \ifdef{\fxsetup}{

        \fxsetup{
            status=final
        }

    }{
}

}

\usepackage{appendix}

\usepackage[
    square,
    comma,
    numbers,
    sort&compress
]{natbib}

\ifdef{\addbibresource}{

\addbibresource{references.bib}
\ExecuteBibliographyOptions{
        sorting=6
    }

}{
}

\ifdef{\setcitestyle}{

    \setcitestyle{}

}{
}

\usepackage{microtype}

\usepackage{siunitx}

\usepackage{textcomp}

\usepackage{textgreek}

\usepackage[user,xr]{zref}

\ifdef{\zxrsetup}{

\ifbool{draftmode}{
}{

        \IfFileExists{main.tex}{

            \IfFileExists{sup.tex}{

\zexternaldocument*[main:]{main}

\zexternaldocument*[sup:]{sup}

            }{
}

        }{
}

    }

}{
}

\usepackage{afterpage}

\usepackage{graphicx}

\usepackage{caption}
\usepackage[
    labelformat=simple
]{subcaption}

\ifdef{\captionsetup}{

\captionsetup[figure]{
font=small,
skip=0.5ex
    }

\captionsetup[table]{
font=small,
skip=0.5ex
    }

}{
}

\ifdef{\thesubfigure}{

\renewcommand{\thesubfigure}{~(\alph{subfigure})}
    \renewcommand{\thesubtable}{~(\alph{subtable})}

}{
}

\ifdef{\setlist}{

    \setlist{
topsep=0ex,
partopsep=0ex,
parsep=0ex,
itemsep=0.5ex
    }

}{
}

\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{amssymb}

\usepackage{amsthm}

\usepackage{bbm}

\usepackage{mathrsfs}

\usepackage{mathtools}

\usepackage{xfrac}

\usepackage{titlesec}

\ifdef{\titlespacing}{

\titlespacing{\section}{0pt}{*.05 }{*.05 }
\titlespacing{\subsection}{0pt}{*.05 }{*.05 }
\titlespacing{\subsubsection}{0pt}{*.05 }{*.05 }
\titlespacing{\paragraph}{0pt}{*.05 }{*.05 }
\titlespacing{\subparagraph}{0pt}{*.05 }{*.05 }

}{
}

\usepackage{setspace}

\setlength{\floatsep}{1ex} \setlength{\textfloatsep}{1ex}
\setlength{\dblfloatsep}{1.5ex} \setlength{\dbltextfloatsep}{2ex} 

\setlength{\parskip}{1ex} 

\usepackage{array}

\usepackage{booktabs}

\usepackage{multirow}

\usepackage{tabu}

\usepackage{tabularx}

\usepackage{tabulary}

\ifdef{\heavyrulewidth}{

\setlength{\heavyrulewidth}{.16em}
\setlength{\lightrulewidth}{.16em}
\setlength{\cmidrulewidth}{.16em}
\setlength{\aboverulesep}{0.1ex}
\setlength{\belowrulesep}{0.1ex}

}{
}

\ifcsdef{tabu}{

\setlength{\tabulinesep}{3pt}

}{
}

\newcommand{\teacherset}{\mathcal{T}}
\newcommand{\studentset}{\mathcal{S}}
\newcommand{\classset}{\mathcal{C}}
\newcommand{\classsetall}{\mathcal{C}_0}
\newcommand{\areaset}{\mathcal{A}}
\newcommand{\imageset}{\mathcal{I}}

\newcommand{\indicatorfunction}{\Omega}
\newcommand{\indicator}{\omega}
\newcommand{\ratio}{\rho}

\newcommand{\iou}{\Phi}
\newcommand{\policy}{\pi}
\newcommand{\fusion}{f}
\newcommand{\overlap}{\varepsilon}
\newcommand{\unlabeled}{c_0}

\newcommand{\softprediction}{\hat{s}}
\newcommand{\hardprediction}{\hat{y}}

\newcommand{\area}[2]{A_{#1}^{#2}}
\newcommand{\fusedarea}[2]{A_{#1}^{#2}}

\newcommand{\xsrc}{x_{src}}
\newcommand{\ysrc}{y_{src}}
\newcommand{\dataset}{\mathcal{D}}
\newcommand{\xtgt}{x_{tgt}}
\newcommand{\ytgt}{y_{tgt}}

\newcommand{\yfused}{\tilde{y}}
\newcommand{\sfused}{\tilde{s}}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{cancel}
\usepackage{makecell}
\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\usepackage{bbm}
\usepackage{pdfpages}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{20} \def\confYear{CVPR 2021}

\begin{document}

\title{Rethinking Ensemble-Distillation for \\Semantic Segmentation Based Unsupervised Domain Adaptation}

\author{Chen-Hao Chao, Bo-Wun Cheng, and Chun-Yi Lee \\
Elsa Lab, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan\\
{\tt\small \{lancechao, bobcheng15, cylee\}@gapp.nthu.edu.tw}
}

\maketitle

\begin{abstract}
Recent researches on unsupervised domain adaptation (UDA) have demonstrated that end-to-end ensemble learning frameworks serve as a compelling option for UDA tasks. Nevertheless, these end-to-end ensemble learning methods often lack flexibility as any modification to the ensemble requires retraining of their frameworks. To address this problem, we propose a flexible ensemble-distillation framework for performing semantic segmentation based UDA, allowing any arbitrary composition of the members in the ensemble while still maintaining its superior performance. To achieve such flexibility, our framework is designed to be robust against the output inconsistency and the performance variation of the members within the ensemble. To examine the effectiveness and the robustness of our method, we perform an extensive set of experiments on both GTA5Cityscapes and SYNTHIACityscapes benchmarks to quantitatively inspect the improvements achievable by our method. We further provide detailed analyses to validate that our design choices are practical and beneficial. The experimental evidence validates that the proposed method indeed offer superior performance, robustness and flexibility in semantic segmentation based UDA tasks against contemporary baseline methods.
\end{abstract}
 \vspace{-1.5em}
\begin{figure}[t]
    \includegraphics[width=\linewidth]{images/introduction/issue.pdf}
    \caption{An illustrative example of (1) the inconsistency issue and (2) the performance variation issue mentioned in Section~\ref{sec:introduction}. For (1), the models  are trained with ADA methods, which produce relatively low-certainty outputs. In contrast, the model  is trained with self-training method and outputs high-certainty predictions. After averaging, the ensemble predicts class \texttt{A} as its final prediction instead of the majority consensus, i.e., class \texttt{C}. For (2), the models  are assumed to be high-performing members, while  is an under-performing one. After averaging, the high-certainty predictions from  could dominate the output of the ensemble, and cause the overall performance to degrade.} 
    \label{fig:issue}
\end{figure} \section{Introduction}
\label{sec:introduction}
In the past few years, semantic segmentation has been attracting the attention of computer vision researchers. Many supervised semantic segmentation methods have been proposed and achieved remarkable performance~\cite{yu2017dilated, lin2017refinenet, yu2015multi, badrinarayanan2017segnet, long2015fully, yuan2019object, wu2019wider, sandler2018mobilenetv2, chen2014semantic, chen2017deeplab, chen2017rethinking, chen2018encoder, zhao2017pyramid}. Typically, those supervised semantic segmentation methods require abundant labeled training data, which are usually expensive to annotate and are commonly unavailable in most real-world scenarios. To resolve this problem, semantic segmentation based unsupervised domain adaptation (UDA) methods~\cite{hoffman2016fcns, hoffman2018cycada, luo2019significance, gong2019dlow, wu2018dcan, tsai2018learning, yang2020adversarial, tsai2019domain, vu2019advent, zhang2018fully, chen2017no, zheng2019unsupervised, choi2019self, zou2018unsupervised, zou2019confidence, zheng2020rectifying, tranheden2020dacs, luo2019taking, lee2018spigan, chen2019learning, vu2019dada, lv2020cross, yang2020fda} have been introduced to bridge different domains. These semantic segmentation based UDA models learn to generalize to a target domain by training with the annotated data from a source domain and the unlabeled data from a target domain. Among these works, the authors in~\cite{luo2019taking, hoffman2016fcns, hoffman2018cycada, luo2019significance, gong2019dlow, wu2018dcan, tsai2018learning, yang2020adversarial, tsai2019domain, vu2019advent, zhang2018fully, chen2017no, zheng2019unsupervised, chen2019crdoco, li2019bidirectional, du2019ssf} resorted to adversarial domain adaptation (ADA) methods, through which the domain discrepancy is minimized by using their adversarial training schemes. Another branch of works has opted for self-training frameworks~\cite{choi2019self, zou2018unsupervised, zou2019confidence, zheng2020rectifying, tranheden2020dacs}, which aim to improve the stability of their models during deployment by minimizing the entropy of the models' predictions in a target domain. These ADA and self-training methods have demonstrated how a single model is able to learn to generalize to an annotation-less target domain. However, they only learn from a single distribution, leaving space for further improvements. Recently, in light of the potential benefits of combining multiple UDA models, a number of works~\cite{nguyen2021unsupervised, kang2019contrastive} have attempted to borrow the concepts from ensemble learning. These works demonstrated how a group of UDA models can be trained simultaneously in an end-to-end fashion to learn different distributions of semantic information, and meanwhile transferring the knowledge to a compact student model. Despite their successes in bridging the domain gaps with multiple learners, these ensemble learning methods often lack flexibility as any modification to the teacher ensemble requires complete retraining of the whole framework.
\vspace{-0.2em}

To address such a problem, the concept of ensemble-distillation~\cite{bucilua2006model, hinton2015distilling, cho2019efficacy, furlanello2018born, balan2015bayesian, nguyen2020joint, orbes2019knowledge, liu2019structured, malinin2019ensemble} can be leveraged since its focus is on designing an effective distillation process instead of a costly end-to-end ensemble learning framework. Typically, these ensemble distillation frameworks view the members in an ensemble as probabilistic models, and transfer the knowledge using expected certainty outputs. Nonetheless, the robustness of these methods is not guaranteed as they do not carefully take into account the followings:
(1) the inconsistency in the scale of the output certainty values among the members in an ensemble (abbreviated as the `\textbf{inconsistency issue}' hereafter), and (2) the performance variations across the members in an ensemble (abbreviated as the `\textbf{performance variation issue}' hereafter). An example of these two issues is illustrated in Fig.~\ref{fig:issue}. For the former, since each teacher model in the ensemble can be trained independently using different methods (e.g., ADA, self-training, data augmentation, or compound usage of them), the scale of the output certainty values may not be consistent across the ensemble. This may result in a situation that few members' decisions with high certainty values dominate the entire ensemble's output. As a result, the outputs from the members in an ensemble should be treated in an equal manner, as the inconsistency in their certainty values may come from their different training objectives instead of the real data distribution in the target domain. For the latter, since the performance (either per-class or average accuracy) of each teacher model in the ensemble may vary substantially, few under-performing members in the ensemble may cause the quality of the combined prediction to degrade significantly. This problem is especially severe under the context of UDA, since the ground truth labels in the target domain are unavailable, and the performance of the ensemble in the target domain is actually unknown.  The above observations suggest that an effective mechanism is necessary to deal with these two issues and prevent them from influencing the quality of the combined predictions.

Being aware of these problems, we introduce a novel ensemble-distillation framework to avoid the aforementioned pitfalls. First, to tackle the certainty inconsistency issue, we introduce an output unification method in the framework to reduce the impact of the inconsistent scales of the certainty outputs. Next, we embrace a new category of fusion function in our framework, named channel-wise fusion, to resolve the performance variation issue. Moreover, we design a method to determine the fusion policy of the proposed channel-wise fusion function to further enhance its effectiveness. To validate our designs, we evaluate the proposed framework with two commonly-adopted metrics, GTA5~\cite{richter2016playing}Cityscapes~\cite{Cordts2016Cityscapes} and SYNTHIA~\cite{ros2016synthia}Cityscapes, to demonstrate the effectiveness and robustness of our framework against a number of baselines. The contributions are summarized as follows:
\vspace{-0.5ex}
\begin{itemize}
\setlength\itemsep{-0.7ex}
  \item We introduce a flexible UDA ensemble-distillation framework which is robust against the inconsistency in the scale of the output certainty values and the performance variations among the members in an ensemble.
  \item We propose a new category of fusion function, called channel-wise fusion, along with a fusion policy selection strategy as well as a conflict resolving mechanism to enhance its effectiveness. \item We evaluate our framework under various configurations, and demonstrate that it is able to outperform the baselines in terms of its robustness and effectiveness.
\end{itemize}
\vspace{-0.5ex} \section{Related Works}
\label{sec:related_works}

\paragraph{Unsupervised Domain Adaptation:}~A number of methods have been proposed to bridge the discrepancy between different domains. One branch of these works adopted ADA frameworks to learn representations of their target domains~\cite{luo2019taking, hoffman2016fcns, hoffman2018cycada, luo2019significance, gong2019dlow, wu2018dcan, tsai2018learning, yang2020adversarial, tsai2019domain, vu2019advent, zhang2018fully, chen2017no, zheng2019unsupervised, chen2019crdoco, li2019bidirectional, du2019ssf}. These approaches typically employ a generator and a discriminator trained against each other to minimize the domain gap, and have shown significant improvements over those trained directly in the source domains. Another line of works has turned their attention to self-training and data augmentation measures to tackle UDA problems. For those works utilizing self-training, the concentration was mainly on preventing overfitting by using regularization~\cite{zou2019confidence, zheng2020rectifying} or class-balancing~\cite{zou2018unsupervised} when minimizing the uncertainty in their target domains. The authors in~\cite{tranheden2020dacs} extended the concept of self-training and proposed a data augmentation technique. Their proposed method fine-tunes a model with mixed labels generated by combining ground truth annotations from a source domain and pseudo labels from a target domain. Recent researchers employed ensemble learning frameworks to resolve UDA problems~\cite{nguyen2021unsupervised, kang2019contrastive}. The authors in~\cite{kang2019contrastive} proposed an end-to-end ensemble framework to solve UDA classification problems. The authors in~\cite{nguyen2021unsupervised} extended the idea of ensemble learning and proposed a joint learning ensemble framework to solve person re-identification UDA problems. These works showed how the ensemble learning frameworks can be integrated into UDA.

\paragraph{Pseudo Labeling:}~Pseudo labeling is a self-training method originally proposed to improve the performance of classification networks~\cite{lee2013pseudo}, and is usually accomplished by minimizing the entropy of a model's predictions on unseen data. Pseudo labeling enables a better decision boundary to be achieved as the certainty of a model's prediction increases~\cite{lee2013pseudo}. This concept has been further extended to the field of semantic segmentation, and has gained success by incorporating the information of unlabeled data. Since self-training via pseudo labeling and UDA share many similar characteristics in terms of their problem formulations, it has recently been used to solve UDA problems~\cite{zou2019confidence, zheng2020rectifying, tranheden2020dacs}.

\paragraph{Ensemble-Distillation Method:}~Ensemble-distillation is an extension of knowledge distillation. The authors in~\cite{hinton2015distilling,balan2015bayesian} studied how the knowledge of a teacher model ensemble can be transferred to a student by training it with the soft predictions of the ensemble. They adopted averaging operation for combining the predictions and used KL-divergence as the loss function to transfer the knowledge. The authors in~\cite{malinin2019ensemble} aimed at resolving the diversity collapse issue in the ensemble-distillation problem. They argued that the averaging operation harms the diversity of the models in an ensemble and proposed to use a prior network~\cite{malinin2018predictive} to estimate the distributions of their output uncertainties. These works have demonstrated their effectiveness under supervised training settings. However, the existing ensemble-distillation methods are not designed to handle unsupervised tasks, and are susceptible to the issues introduced in Section~\ref{sec:introduction}. 
\section{Preliminary}
\label{sec:preliminary}
\paragraph{Problem Definition:}~For semantic segmentation based UDA problems, a model has access to the image-label pairs, , from a source domain dataset , but only the images  from a target domain dataset . The training objective is to train the model such that its predictions can best estimate the ground truth labels  in the target domain. In other words, the mean intersection-over-union (mIoU) between the predictions of the model and  should be maximized. In the problem formulation concerned by this paper, a pretrained model ensemble  is given, where each member in  is separately trained using any arbitrary semantic segmentation based UDA method. The goal is to develop an ensemble-distillation strategy that can effectively integrate the knowledge from  and distill it into a single student model, in a way that the mIoU of the student's predictions for the instances in  is maximized.

\paragraph{Previous Ensemble-Distillation Method:}~In this section, we explain how the concepts of the previous ensemble-distillation works~\cite{bucilua2006model, hinton2015distilling, balan2015bayesian, cho2019efficacy, furlanello2018born, nguyen2020joint, orbes2019knowledge, liu2019structured} can be borrowed to perform semantic segmentation based UDA ensemble-distillation tasks. Typically, these works view  as a set of probabilistic models, and complete the ensemble-distillation process through minimizing the negative log-likelihood loss  between the expected outputs from the ensemble and the student model, as depicted in Fig.~\ref{fig:distillation}~(b). The distillation process of these methods under the settings of semantic segmentation based UDA can be formulated as:
 where  is a set of pixels in an image,  is a given set of semantic classes.  is the student's certainty output, and  represents the expected probabilistic prediction for class  at pixel . A common way to capture  is through averaging, expressed as follows:

 where  is the probabilistic prediction from  for class  at pixel  on target instances. As a result, the knowledge of the teacher ensemble can be transferred through minimizing  in a target domain.

\paragraph{Pitfalls:}~As discussed in Section~\ref{sec:introduction}, directly adopting previous methods to solve semantic segmentation based UDA problems is problematic because of the inconsistency issue and the performance variation issue. 
For the former, the scale of  may vary across the models in  under our problem formulation. This suggests that a direct operation, such as the averaging operation in Eq.~(\ref{eq:certainty}), is inappropriate as a few  with high certainty values may dominate the ensemble's output decision. For the performance variation issue, the per-class or the average performance of each member in  can vary substantially under our problem formulation. Since the fusion function formulated in Eq.~(\ref{eq:certainty}) fuses the predictions of \textit{all} teacher models, the under-performing members in  can influence the quality of the fused results. Therefore, the adoption of such a fusion function is inappropriate as it may be sensitive to the performance variations within .

\begin{figure}[t]
\includegraphics[width=\linewidth]{images/methodology/distillation.pdf}
    \caption{A comparison between (a) the proposed ensemble-distillation framework and (b) the baseline framework.}
    \label{fig:distillation}
\end{figure} \section{Methodology}
\label{sec:methodology}
To address the aforementioned problems, we introduce a new ensemble-distillation framework, and illustrate it in Fig.~\ref{fig:distillation}~(a). The main difference between the proposed method and the previous ones lies in two aspects: \textit{Output Unification} and \textit{Fusion Function}. In Sections~\ref{sec:method:output_unification} and~\ref{sec:method:fusion_function}, we walk through the designs of the output unification operation as well as the fusion function, and discuss how they may contribute to resolving the aforementioned issues. Finally, in Section~\ref{sec:method:ensemble_distillation}, we formulate and summarize the proposed ensemble-distillation framework.

\subsection{Output Unification}
\label{sec:method:output_unification}
To resolve the inconsistency issue, we argue that the soft predictions  in Eq.~(\ref{eq:certainty}) should be unified first in the target domain, as illustrated in Fig.~\ref{fig:distillation}.  This additional unification operation ensures that the raw output certainty values from the models in  do not directly influence the subsequent fusion results. To accomplish this, we unify the soft predictions by converting them to pseudo labels, so as to make them all bear the same scale, i.e., representing the final decisions of the models in . The unification operation is formulated as:
 where  is the unified output prediction from  for class  at pixel  in the target domain. This operation ensures that the subsequent fusion function can operate on items with a consistent scale, and thus eliminates the impact of the inconsistency in the original certainty outputs.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{images/methodology/fusion.pdf}
    \caption{A illustration of the pixel-wise and channel-wise fusions.}
    \label{fig:channel_wise}
\end{figure}
 \subsection{Fusion Function}
\label{sec:method:fusion_function}
We next move on to focus on investigating a fusion function that can take advantage of the unified predictions to achieve  robustness against the performance variations issue. We compare two categories of fusion functions: \textit{pixel-wise fusion} and \textit{channel-wise fusion}. The former is a direct conversion from Eq.~(\ref{eq:certainty}) and is used as our baseline method. The latter is the proposed method and is adopted to address the performance variation issue. Both pixel-wise fusion and channel-wise fusion are mapping functions  that assign a class label  for the pixel  in the fusion output based on , where  is a set that includes all  as well as the unlabeled symbol .

\subsubsection{Pixel-Wise Fusion}
Pixel-wise fusion () adopts a statistical view on , and is designed to capture the average behavior of the ensemble. As depicted in Fig.~\ref{fig:channel_wise}, pixel-wise fusion treats each pixel in a semantic segmentation map as the basic unit of the fusion operation. Specifically, the fused result of each pixel is determined by taking majority voting among the predictions from , and is implemented as the following:

 where  is the unified output generated according to Eq.~(\ref{eq:unification}).

\begin{figure}[t]
    \includegraphics[width=\linewidth]{images/methodology/set.pdf}
    \caption{An illustrative example of the three scenarios in Eq.~(\ref{eq:channel_wise}).}
    \label{fig:set}
\end{figure}
 \begin{figure}[t]
    \includegraphics[width=\linewidth]{images/methodology/new_policy.pdf}
    \caption{An illustrative example of  used in .}
    \label{fig:policy}
\end{figure}

 \subsubsection{Channel-Wise Fusion}
Based on a different perspective, channel-wise fusion () treats each class channel as the basis for fusion, as depicted in Fig.~\ref{fig:channel_wise}. Instead of fusing the outputs of all , channel-wise fusion relies on a fusion policy , which is a mapping function for recombining the unified outputs from different teacher models. More specifically, for each class , the fusion policy  selects that class channel from the unified output  of a teacher model , as illustrated in the example shown in Fig.~\ref{fig:policy}.
Given such a , the channel-wise fusion function is formulated as:

where (i) is the condition that  is labeled by multiple teachers, (ii) is the condition that  labeled as a certain class by a single teacher, and (iii) is the condition that  is unlabeled, as illustrated in Fig.~\ref{fig:set}. In Eq.~(\ref{eq:channel_wise}),  denotes a class label to be assigned in scenario (i),  is a class in  in scenario (ii), and  denotes the unlabeled symbol in scenario (iii).  is a set of pixels comprising   for a given class  generated by a teacher  selected according to the given fusion policy . Since different  may be produced by different  for different , the pixels they cover are not necessarily mutually exclusive. Therefore,  is defined to represent the set of pixels labeled by multiple teachers, expressed as follows:
\begin{spacing}{0.7}

\end{spacing} The mechanism that assigns the value of  for all  is referred as the conflict-resolving mechanism. In this work, we employ a spatially-aware conflict-resolving mechanism that assigns a class label for each pixel in  using majority voting on a kernel. The size of the kernel is denoted as , and the set of pixels covered by the kernel centered at  is referred to as . The mechanism is formulated as follows:
\vspace{-0.15em}

where  represents a set of class(es) assigned to a pixel  under .

\subsubsection{Theoretical Properties of Channel-Wise Fusion}
Let  and  be the per-class IoU's w.r.t.  for the pseudo labels generated by  and the fused pseudo labels generated by , respectively. Channel-wise fusion conforms to the following properties:

\begin{prop}
Consider an arbitrary fusion policy . Given a constant  and classes . If  and , we have:
\vspace{-0.2em}

\end{prop}

\vspace{-0.9em}

\begin{prop}
Consider an optimal fusion policy . Assume , we have:
\vspace{-0.2em}

\end{prop}

For the detailed elaborations and proofs with regard to these properties, please refer to the supplementary materials. \textbf{Proposition 1} states the condition when the mIoU lower bound can be ensured. On the other hand, \textbf{Proposition 2} describes how the effectiveness of channel-wise fusion can be maximized. In the next section, we discuss how an effective  can be determined.

\begin{figure}[t]
\includegraphics[width=\linewidth]{images/methodology/compare.pdf}
    \caption{An illustration of how the quality of a teacher model's pseudo labels can affect the output certainty values of the student model. If the student model is trained with high-quality pseudo labels (i.e., pseudo labels with high IoU's w.r.t. ), it can learn a mapping from input features to the segmentation mask effectively, and generates high-certainty predictions. In contrast, if the student model is trained with low-quality pseudo labels (i.e., pseudo labels with low IoU's w.r.t ) that are mismatched with the input features, the student's output certainty values are likely to degrade.}
    \label{fig:compare}
\end{figure}
 \begin{figure}[t]
    \includegraphics[width=\linewidth]{images/experiments/similarity.pdf}
    \caption{The cosine similarity between the per-class IoU of each  and the per-class certainty values of the student evaluated on the training set of Cityscapes, i.e., the cosine similarity between  in Proposition 2 and  in Eq.~(\ref{eq:policy}). The experimental results reveal that the two variables are positively correlated to each other, as the cosine similarity values for all  are greater than 0 and close to 1. For the detailed settings, please refer to Section~\ref{sec:setup}.}
    \label{fig:similarity}
\end{figure} \subsubsection{Certainty-Aware Policy Selection Strategy}
\label{subsubsec::certainty-aware-policy}
Since the fusion policy  determines which teacher model is allowed to involve in , choosing an appropriate  is therefore crucial to the robustness of our framework. In order to achieve this objective, a suitable measure is necessary for evaluating the quality of each teacher's predictions in the target domain without using any target domain ground truth. The experimental clue illustrated and explained in Fig.~\ref{fig:compare} and \ref{fig:similarity} offers an empirical manner for the above purpose. From Fig.~\ref{fig:compare}, it is observed that the quality of the unified outputs , i.e., the pseudo labels, from  are positively correlated to the output certainty values of distilled student model. This correlation suggests that any low-quality , which might be generated by some under-performing teacher models, can confuse the student model and causes its output certainty values to degrade. This correlation between the quality of the pseudo labels from  and the student's output certainty values thus shed light on the development of the measure for approximating a teacher model's performance without using target domain ground truth. In practice, an offline fusion policy selection strategy is adopted. Our framework first performs knowledge distillation on all  and transfers their knowledge to  identical student models using the unified outputs. Then, their output certainty values are measured to obtain the approximated performance of their corresponding teacher models. Finally, for each , a  that maximizes the student model's output certainty values is selected. The fusion policy  is written as follows:

where  refers to the average certainty outputs of class  from the student model trained with the unified outputs generated by teacher .

\subsection{The Proposed Framework}
\label{sec:method:ensemble_distillation}
Based on the formulations of output unification and fusion function, the loss function  for performing the ensemble-distillation in our framework is defined as follows:
 where  is the fused results, defined as:
 The pseudo code of the ensemble-distillation method in our framework is summarized in the supplementary materials. 

\section{Experimental Setup}
\label{sec:setup}
\paragraph{Baselines and Evaluation Methods:}~In this work, we evaluate and compare the experimental results in terms of the effectiveness and the robustness. To examine the effectiveness of the proposed framework, we compare our method against two ensemble-distillation schemes and a number of UDA baselines. The ensemble-distillation schemes include EnD~\cite{hinton2015distilling} and its recent revision EnD~\cite{malinin2019ensemble}. The semantic segmentation based UDA baselines cover APODA~\cite{yang2020adversarial}, PatchAlign~\cite{tsai2019domain}, AdvEnt~\cite{vu2019advent}, FDA-MBT~\cite{yang2020fda}, PIT~\cite{lv2020cross}, CBST~\cite{zou2018unsupervised}, MRKLD~\cite{zou2019confidence}, R-MRNet~\cite{zheng2020rectifying}, and DACS~\cite{tranheden2020dacs}. To examine the robustness of our framework, we select the members for  based on two criteria: (1) each member in  is trained with different UDA methods; and (2) there exists large per-class and average performance variations among the members in . According to these criteria, we select DACS~\cite{tranheden2020dacs} (data augmentation), R-MRNet~\cite{zheng2020rectifying} (adversarial training), MRKLD~\cite{zou2019confidence} (self-training), and CBST~\cite{zou2018unsupervised} (self-training) to form  in our experiments. We evaluate the proposed framework and the baselines on two commonly adopted benchmarks: GTA5~\cite{richter2016playing}Cityscapes~\cite{Cordts2016Cityscapes} and SYNTHIA~\cite{ros2016synthia}Cityscapes. For the former, the models have access to 24,966 image-label pairs from the training set of GTA5, and 2,975 images from the training set of Cityscapes. We evaluate the student model's per-class IoU's of the 19 semantic classes as well as the its mIoU's on the validation set of Cityscapes. For the latter, the models have access to 9,400 image-label pairs from the training set of SYNTHIA, and 2,975 images from the training set of Cityscapes. In a similar fashion, we evaluate the student model's per-class IoU's of 13 and 16 semantic classes as well as its mIoU's on the validation set of Cityscapes.

\paragraph{Implementation Details:}~For the student model, we adopt Deeplabv3+~\cite{chen2018encoder} architecture with DRN-D-54~\cite{yu2017dilated} as our backbone, which is trained using SGD with a learning rate initialized to  and decreased with a factor of . The weight decay is set to , the momentum is set to 0.9, and the batch size is set to 10 for 100K iterations with early stopping. The value of  in  is set to . During the process of certainty-aware policy selection strategy, 500 images from the training set of Cityscapes are used for measuring , while the other 2475 images are used for training the student. The student model is pre-trained in the source domain, and fine-tuned with  and  during the distillation process.
 \begin{table*}[t]
\centering

\resizebox{\textwidth}{!}{\renewcommand{\arraystretch}{1.11}
\newcommand{\mytoprule}{\toprule[1.5pt]}
\footnotesize
\begin{tabular}{clllllllllllllllllllll}
\mytoprule
\multicolumn{21}{c}{GTA5  Cityscapes} &  \\ \mytoprule
\multicolumn{1}{c|}{Method} & Road & SideW & Build & Wall & Fence & Pole & Light & Sign & Veg & Terrain & Sky & Person & Rider & Car & Truck & Bus & Train & Motor & \multicolumn{1}{l|}{Bike} & mIoU &  \\ \hline
\multicolumn{1}{c|}{APODA~\cite{yang2020adversarial}} & 85.6 & 32.8 & 79.0 & 29.5 & 25.5 & 26.8 & 34.6 & 19.9 & 83.7 & 40.6 & 77.9 & 59.2 & 28.3 & 84.6 & 34.6 & 49.2 & 8.0 & 32.6 & \multicolumn{1}{l|}{39.6} & 45.9 &  \\
\multicolumn{1}{c|}{PatchAlign~\cite{tsai2019domain}} & 92.3 & 51.9 & 82.1 & 29.2 & 25.1 & 24.5 & 33.8 & 33.0 & 82.4 & 32.8 & 82.2 & 58.6 & 27.2 & 84.3 & 33.4 & 46.3 & 2.2 & 29.5 & \multicolumn{1}{l|}{32.3} & 46.5 &  \\
\multicolumn{1}{c|}{AdvEnt~\cite{vu2019advent}} & 89.4 & 33.1 & 81.0 & 26.6 & 26.8 & 27.2 & 33.5 & 24.7 & 83.9 & 36.7 & 78.8 & 58.7 & 30.5 & 84.8 & 38.5 & 44.5 & 1.7 & 31.6 & \multicolumn{1}{l|}{32.4} & 45.5 &  \\
\multicolumn{1}{c|}{FDA-MBT~\cite{yang2020fda}} & 92.5 & 53.3 & 82.4 & 26.5 & 27.6 & 36.4 & 40.6 & 38.9 & 82.3 & 39.8 & 78.0 & 62.6 & 34.4 & 84.9 & 34.1 & 53.1 & 16.9 & 27.7 & \multicolumn{1}{l|}{46.4} & 50.5 &  \\
\multicolumn{1}{c|}{PIT~\cite{lv2020cross}} & 87.5 & 43.4 & 78.8 & 31.2 & 30.2 & 36.3 & 39.9 & 42.0 & 79.2 & 37.1 & 79.3 & 65.4 & 37.5 & 83.2 & 46.0 & 45.6 & 25.7 & 23.5 & \multicolumn{1}{l|}{49.9} & 50.6 &  \\ \multicolumn{1}{c|}{\textbf{\textcolor{blue}{CBST}}~\cite{zou2018unsupervised}} & 91.8 & 53.5 & 80.5 & 32.7 & 21.0 & 34.0 & 28.9 & 20.4 & 83.9 & 34.2 & 80.9 & 53.1 & 24.0 & 82.7 & 30.3 & 35.9 & 16.0 & 25.9 & \multicolumn{1}{l|}{42.8} & 45.9 &  \\
\multicolumn{1}{c|}{\textbf{\textcolor{blue}{MRKLD}}~\cite{zou2019confidence}} & 91.0 & 55.4 & 80.0 & 33.7 & 21.4 & 37.3 & 32.9 & 24.5 & 85.0 & 34.1 & 80.8 & 57.7 & 24.6 & 84.1 & 27.8 & 30.1 & \textbf{26.9} & 26.0 & \multicolumn{1}{l|}{42.3} & 47.1 &  \\
\multicolumn{1}{c|}{\textbf{\textcolor{blue}{R-MRNet}}~\cite{zheng2020rectifying}} & 90.4 & 31.2 & 85.1 & 36.9 & 25.6 & 37.5 & 48.8 & 48.5 & 85.3 & 34.8 & 81.1 & 64.4 & 36.8 & 86.3 & 34.9 & 52.2 & 1.7 & 29.0 & \multicolumn{1}{l|}{44.6} & 50.3 &  \\ 
\multicolumn{1}{c|}{\textbf{\textcolor{blue}{DACS}}~\cite{tranheden2020dacs}} & 89.90 & 39.66 & 87.87 & 30.71 & 39.52 & 38.52 & 46.43 & 52.79 & 87.98 & 43.96 & \textbf{88.76} & 67.20 & 35.78 & 84.45 & 45.73 & 50.19 & 0.00 & 27.25 & \multicolumn{1}{l|}{33.96} & 52.14 &  \\ 
\hline \hline
\multicolumn{1}{c|}{Source Only} & 57.40 & 21.43 & 56.80 & 8.93 & 22.14 & 32.38 & 34.62 & 24.90 & 78.98 & 15.92 & 63.71 & 55.55 & 13.83 & 58.11 & 21.99 & 29.78 & 2.36 & 28.41 & \multicolumn{1}{l|}{33.98} & 34.80 &  \\
\multicolumn{1}{c|}{EnD~\cite{hinton2015distilling}} & 92.17 & 53.12 & 84.85 & 24.77 & 29.76 & 40.38 & 40.98 & 49.35 & 86.21 & 42.85 & 79.74 & 62.79 & 35.98 & 85.72 & 42.10 & 44.45 & 0.26 & 28.27 & \multicolumn{1}{l|}{51.80} & 51.34 &  \\ 
\multicolumn{1}{c|}{EnD~\cite{malinin2019ensemble}} & 92.39 & 53.84 & 85.34 & 24.51 & 30.53 & 40.28 & 42.40 & 50.28 & 86.19 & 43.39 & 80.55 & 63.26 & 36.75 & 86.15 & 43.95 & 43.91 & 0.20 & 30.17 & \multicolumn{1}{l|}{53.22} & 51.96 &  \\
\multicolumn{1}{c|}{Ours (Pixel)} & 92.29 & 57.34 & 84.09 & 36.75 & 29.17 & 41.37 & 48.96 & 42.26 & 86.91 & 39.95 & 82.81 & 66.29 & 37.42 & 86.94 & 35.21 & 48.82 & 1.48 & 40.78 & \multicolumn{1}{l|}{53.02} & 53.26 &  \\
\multicolumn{1}{c|}{Ours (Channel)} & \textbf{94.43} & \textbf{60.90} & \textbf{88.07} & \textbf{39.46} & \textbf{41.80} & \textbf{43.24} & \textbf{49.08} & \textbf{56.00} & \textbf{88.01} & \textbf{45.83} & 87.79 & \textbf{67.58} & \textbf{38.05} & \textbf{90.08} & \textbf{57.64} & \textbf{51.90} & 0.00 & \textbf{46.57} & \multicolumn{1}{l|}{\textbf{55.28}} & \textbf{57.98} &  \\ \mytoprule
\multicolumn{21}{c}{SYNTHIA  Cityscapes} &  \\ \mytoprule
\multicolumn{1}{c|}{Method} & Road & SideW & Build & Wall* & Fence* & Pole* & Light & Sign & Veg & Terrain & Sky & Person & Rider & Car & Truck & Bus & Train & Motor & \multicolumn{1}{l|}{Bike} & mIoU & mIoU* \\ \hline
\multicolumn{1}{c|}{APODA~\cite{yang2020adversarial}} & 86.4 & 41.3 & 79.3 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 22.6 & 17.3 & 80.3 & \multicolumn{1}{c}{-} & 81.6 & 56.9 & 21.0 & 84.1 & \multicolumn{1}{c}{-} & 49.1 & \multicolumn{1}{c}{-} & 24.6 & \multicolumn{1}{l|}{45.7} & \multicolumn{1}{c}{-} & 53.1 \\
\multicolumn{1}{c|}{PatchAlign~\cite{tsai2019domain}} & 82.4 & 38.0 & 78.6 & 8.7 & 0.6 & 26.0 & 3.9 & 11.1 & 75.5 & \multicolumn{1}{c}{-} & 84.6 & 53.5 & 21.6 & 71.4 & \multicolumn{1}{c}{-} & 32.6 & \multicolumn{1}{c}{-} & 19.3 & \multicolumn{1}{l|}{31.7} & 40.0 & 46.5 \\
\multicolumn{1}{c|}{AdvEnt~\cite{vu2019advent}} & 85.6 & 42.2 & 79.7 & 8.7 & 0.4 & 25.9 & 5.4 & 8.1 & 80.4 & \multicolumn{1}{c}{-} & 84.1 & 57.9 & 23.8 & 73.3 & \multicolumn{1}{c}{-} & 36.4 & \multicolumn{1}{c}{-} & 14.2 & \multicolumn{1}{l|}{33.0} & 41.2 & 48.0 \\
\multicolumn{1}{c|}{FDA-MBT~\cite{yang2020fda}} & 79.3 & 35.0 & 73.2 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 19.9 & 24.0 & 61.7 & \multicolumn{1}{c}{-} & 82.6 & 61.4 & 31.1 & 83.9 & \multicolumn{1}{c}{-} & 40.8 & \multicolumn{1}{c}{-} & \textbf{38.4} & \multicolumn{1}{l|}{51.1} & \multicolumn{1}{c}{-} & 52.5 \\
\multicolumn{1}{c|}{PIT~\cite{lv2020cross}} & 83.1 & 27.6 & 81.5 & 8.9 & 0.3 & 21.8 & 26.4 & 33.8 & 76.4 & \multicolumn{1}{c}{-} & 78.8 & 64.2 & 27.6 & 79.6 & \multicolumn{1}{c}{-} & 31.2 & \multicolumn{1}{c}{-} & 31.0 & \multicolumn{1}{l|}{31.3} & 44.0 & 51.8 \\
\multicolumn{1}{c|}{\textbf{\textcolor{blue}{CBST}}~\cite{zou2018unsupervised}} & 68.0 & 29.9 & 76.3 & 10.8 & 1.4 & 33.9 & 22.8 & 29.5 & 77.6 & \multicolumn{1}{c}{-} & 78.3 & 60.6 & 28.3 & 81.6 & \multicolumn{1}{c}{-} & 23.5 & \multicolumn{1}{c}{-} & 18.8 & \multicolumn{1}{l|}{39.8} & 42.6 & 48.9 \\
\multicolumn{1}{c|}{\textbf{\textcolor{blue}{MRKLD}}~\cite{zou2019confidence}} & 67.7 & 32.2 & 73.9 & 10.7 & 1.6 & \textbf{37.4} & 22.2 & 31.2 & 80.8 & \multicolumn{1}{c}{-} & 80.5 & 60.8 & 29.1 & 82.8 & \multicolumn{1}{c}{-} & 25.0 & \multicolumn{1}{c}{-} & 19.4 & \multicolumn{1}{l|}{45.3} & 43.8 & 50.1 \\
\multicolumn{1}{c|}{\textbf{\textcolor{blue}{R-MRNet}}~\cite{zheng2020rectifying}} & 87.6 & 41.9 & 83.1 & 14.7 & 1.7 & 36.2 & 31.3 & 19.9 & 81.6 & \multicolumn{1}{c}{-} & 80.6 & 63.0 & 21.8 & 86.2 & \multicolumn{1}{c}{-} & 40.7 & \multicolumn{1}{c}{-} & 23.6 & \multicolumn{1}{l|}{53.1} & 47.9 & 54.9 \\
\multicolumn{1}{c|}{\textbf{\textcolor{blue}{DACS}}~\cite{tranheden2020dacs}} & 80.56 & 25.12 & 81.90 & 21.46 & 2.85 & 37.20 & 22.67 & 23.99 & \textbf{83.69} & \multicolumn{1}{c}{-} & \textbf{90.77} & \textbf{67.61} & \textbf{38.33} & 82.92 & \multicolumn{1}{c}{-} & 38.90 & \multicolumn{1}{c}{-} & 28.49 & \multicolumn{1}{l|}{47.58} & 48.34 & 54.81 \\
\hline \hline
\multicolumn{1}{c|}{Source Only} & 25.40 & 15.55 & 59.70 & 18.07 & 0.66 & 26.35 & 19.36 & 30.22 & 72.50 & \multicolumn{1}{c}{-} & 74.28 & 48.11 & 13.67 & 74.62 & \multicolumn{1}{c}{-} & 36.94 & \multicolumn{1}{c}{-} & 13.92 & \multicolumn{1}{l|}{36.45} & 35.36 & 40.06 \\
\multicolumn{1}{c|}{EnD~\cite{hinton2015distilling}} & 85.29 & 25.47 & 81.52 & 15.66 & 3.94 & 34.87 & 30.08 & 35.41 & 80.18 & \multicolumn{1}{c}{-} & 85.86 & 59.94 & 22.78 & 83.53 & \multicolumn{1}{c}{-} & 36.58 & \multicolumn{1}{c}{-} & 16.88 & \multicolumn{1}{l|}{52.42} & 46.90 & 53.53 \\
\multicolumn{1}{c|}{EnD~\cite{malinin2019ensemble}} & 83.88 & 39.08 & 81.14 & 12.65 & 1.01 & 41.16 & 22.91 & 28.26 & 82.83 & \multicolumn{1}{c}{-} & 84.17 & 69.54 & 23.87 & 87.65 & \multicolumn{1}{c}{-} & 41.59 & \multicolumn{1}{c}{-} & 21.68 & \multicolumn{1}{l|}{53.26} & 48.42 & 55.38 \\ 
\multicolumn{1}{c|}{Ours (Pixel)} & 86.98 & 44.18 & 80.95 & 19.38 & 1.52 & 30.47 & 25.64 & 30.39 & 79.92 & \multicolumn{1}{c}{-} & 78.84 & 56.55 & 27.14 & 84.47 & \multicolumn{1}{c}{-} & 44.52 & \multicolumn{1}{c}{-} & 26.03 & \multicolumn{1}{l|}{55.08} & 48.25 & 55.43 \\
\multicolumn{1}{c|}{Ours (Channel)} & \textbf{88.65} & \textbf{46.69} & \textbf{83.79} & \textbf{22.66} &  \textbf{4.14} & 35.01 & \textbf{35.93} & \textbf{36.16} & 82.80 & \multicolumn{1}{c}{-} & 81.35 & 61.61 & 32.13 & \textbf{87.93} & \multicolumn{1}{c}{-} & \textbf{52.79} & \multicolumn{1}{c}{-} & 31.95 & \multicolumn{1}{l|}{\textbf{57.65}} & \textbf{52.58} & \textbf{59.95} \\ \mytoprule
\end{tabular}}
\caption{The quantitative results evaluated on the GTA5Cityscapes and SYNTHIACityscapes benchmarks. The numbers presented in the middle and the last two columns correspond to per-class IoUs, mIoU, and mIoU*, respectively. mIoU* represents the mean IoU over all the semantic classes excluding those with superscript *, and is adopted by a few baseline methods in their original papers. The models used in our semantic segmentation based UDA model ensemble  are highlighted in \textbf{\textcolor{blue}{blue}}. The setting `\textit{Source Only}' indicates that the student model is trained only with the source domain ground truth annotations. The evaluation results of EnD~\cite{hinton2015distilling} and EnD~\cite{malinin2019ensemble} are obtained from our self-implemented models, while those of the remaining baselines are directly obtained from their original papers.}
\label{tab:benchmark}
\end{table*} 
\section{Experimental Results}
\label{sec:experiments}
In this section, we present a number of experiments to validate the design of our framework. First, we compare our framework against a number of baselines and demonstrate its superior performance. Next, according to the experimental results, we show that the output unification operation can provide robustness against the inconsistency issue. Then, we present another experiment, in which under-performing members are added to  to create performance variations, to demonstrate the robustness of our framework with  against the performance variation issue. In addition, we perform experiments to validate the effectiveness of the fusion policy selection strategy and analyze how the value of  in the conflict resolving mechanism can impact the performance. Finally, we explore the flexibility of our framework by adding additional teacher models in an iterative manner, and show that our framework is able to evolve with time.

\subsection{Quantitative Results on the Benchmarks}
\label{sec:experiments:benchmark}
Table~\ref{tab:benchmark} first demonstrates the quantitative results of the proposed framework against a number of baselines mentioned in Section~\ref{sec:setup} on the two benchmarks: GTA5Cityscapes and SYNTHIACityscapes. It is observed that the student models trained under our proposed framework with  (i.e., `Ours (Channel)') is able to outperform the previous ensemble-distillation baselines, i.e., EnD~\cite{hinton2015distilling} and EnD~\cite{malinin2019ensemble}, by a margin of 6.64\% mIoU and 6.02\% mIoU on GTA5Cityscapes, and 6.41\% mIoU and 4.57\% mIoU on SYNTHIACityscapes, respectively. In addition, it is also observed that the student model trained with the proposed framework with  (i.e., `Ours (Channel)') is able to outperform that with the baseline  (i.e., `Ours (Pixel)') by a margin of 4.72\% mIoU on GTA5Cityscapes, and 4.52\% mIoU on SYNTHIACityscapes. 

\subsection{Robustness of the Proposed Framework}
\label{sec:experiments:robustness}
In this section, we validate the robustness of the proposed framework against the two issues discussed in Section~\ref{sec:introduction}. First, to verify that the proposed output unification operation can provide robustness against the inconsistency issue, we leverage the insights from an experiment conducted on , with the members of   bearing substantial output certainty scale variations, as shown in Fig.~\ref{fig:certainty}. The results from Table~\ref{tab:benchmark} reveal that the performance of the student models trained with `Ours~(Pixel)', which is basically the ensemble-distillation baseline EnD~\cite{hinton2015distilling} equipped with the proposed output unification method, is able to outperform EnD by a noticeable margin on both benchmarks. This implies that the adoption of the output unification method is able to provide robustness against the inconsistency issue. Nevertheless, as discussed in Section~\ref{sec:method:fusion_function}, `Ours~(Pixel)' may still be vulnerable to the performance variations of the members in .
\begin{figure}[t]\includegraphics[width=\linewidth]{images/experiments/certainty.pdf}
    \caption{The distribution of the pixel output certainty values from the models in , which are trained on GTA5Cityscapes, with their output certainty values normalized to the range  using softmax operation and evaluated on the training set of Cityscapes.}
    \label{fig:certainty}
\end{figure}

\begin{figure}[t]\includegraphics[width=\linewidth]{images/experiments/robustness.pdf}
    \caption{The performance comparison of our framework with  (`Ours~(Channel)'), our framework with  (`Ours~(Pixel)'), and the baseline EnD and EnD methods, with under-performing members added to . In this experiment, the members in  are evaluated on the GTA5Cityscapes benchmark.}
    \label{fig:robustness}
\end{figure}
\vspace{-1.5em} 
To inspect the robustness of the proposed framework with channel-wise fusion (`Ours~(Channel)') against the performance variation issue, we next look into an experiment that introduces performance variations by adding under-performing members into . Specifically, we use the models trained using only source domain instances, i.e., `\textit{Source Only}' in Table~\ref{tab:benchmark}, as the under-performing members. The analysis is presented in Fig.~\ref{fig:robustness}. It is observed that the performance of the students trained with `Ours~(Pixel)', EnD, and EnD all degrade when the number of under-performing members increases. In contrast, the students trained using `Ours~(Channel)' is able to maintain their performance despite the inclusion of the under-performing members in  . These results demonstrate the significance of preventing clueless incorporation of information from all , and highlight the effectiveness of  in providing robustness against the unfavorable performance variation issue.

\subsection{Analysis on Channel-Wise Fusion}
\label{sec:experiments:analysis}
\paragraph{Fusion Policy: }~To examine the effectiveness of  selected according to the strategy described in Section~\ref{subsubsec::certainty-aware-policy}, we design two additional fusion policies,  and , for comparison purposes.  designates a  for each  randomly, while  carries out this designation greedily according to the oracle performance of the models in  in the target domain (i.e.,  described in Proposition 2). As shown in Table~\ref{tab:policy}, the mIoU of the fused pseudo labels generated based on the  selected according to the proposed strategy is 7.09\% higher than that with . It is also observed that the mIoU of the fused pseudo labels generated based on this  is very close to the mIoU obtained from . These pieces of evidence validate that the proposed policy selection strategy is able to generate a fusion policy  that is very close to the optimal policy  without leveraging any sort of unavailable target domain ground truth annotations.

\begin{table}[t]

\resizebox{\linewidth}{!}{\newcommand{\mytoprule}{\toprule[1pt]}
\renewcommand{\arraystretch}{1.05}
\centering
\footnotesize
\begin{tabular}{cccccccc}
\mytoprule
 & 1 & 5 & 7 & 13 & 21 & 27 \\ \hline
mIoU Gains & +0.00 & +0.15 & +0.37 & +0.73 & +0.31 & +0.09\\
\mytoprule
\end{tabular}}
\caption{The mIoU gains of  with  for different .}
\label{tab:kernel}
\end{table}
 \begin{figure}[t]
    \includegraphics[width=\linewidth]{images/experiments/kernel.pdf}
    \caption{The visualized results of the pseudo labels generated by the proposed channel-wise fusion with different choices of .} 
    \label{fig:kernel}
\end{figure}
 \begin{table}[t]
\newcommand{\mytoprule}{\toprule[1.08pt]}
\renewcommand{\arraystretch}{1.1}
\centering
\footnotesize
\begin{tabular}{ccccccc}
\mytoprule
Policy &  (Random) &  (Ours) &  (Oracle) \\ \hline 
mIoU & 49.22 & 56.31 & 56.48\\ \hline \hline
Overlapped Area &  & & \\ \hline
Ratio & 4.6\% & 3.7\% & 2.6\% \\
mIoU Gains & +0.73 & +1.13 & +1.21 \\\mytoprule
\end{tabular}\caption{The mIoU's of the fused pseudo labels generated by channel-wise fusion  under different fusion policies w.r.t. the ground truth annotations of the training set of Cityscapes. , , and  refer to the fusion policies mentioned in Section~\ref{sec:experiments:analysis}. `Ratio' refers to the average proportion of the overlapped area in an image (i.e., ). The `mIoU gains' represents the mIoU gains from the adoption of the proposed conflict-resolving mechanism.}
\label{tab:policy}
\end{table}
 \paragraph{Conflict-Resolving Mechanism:}~To investigate the effectiveness of the conflict-resolving mechanism described in Section~\ref{sec:method:fusion_function} and Eq.~(\ref{eq:fused_area}), we perform parameter analysis on  with different choices of . In this analysis, the fusion policy adopted is  to ensure that results are independent of the design of . Fig.~\ref{fig:kernel} shows the visualized results of the fused pseudo labels with different . Table~\ref{tab:kernel} reports the mIoU gained for various  with respect to the condition . It is observed that pixel predictions in the overlapped area can be better determined if a moderate collection of predictions from the neighboring pixels are taken into account. However, if  becomes too large, an excessive amount of unrelated semantic information is included in the conflict resolving mechanism, and degrades the quality of the fused pseudo labels. Table~\ref{tab:policy} shows that different policies, e.g.,  and , can also benefit from the conflict-resolving mechanism, thus justifying it from a different perspective.

\begin{figure}[t]\vspace{-1.2ex}
    \includegraphics[width=\linewidth]{images/experiments/flexibility.pdf}
    \caption{The performance of the student models trained with the proposed framework using  with different compositions of . The experiment is performed on GTA5Cityscapes. For settings `S4', `S5' and `S6', the student models trained under the previous settings, i.e., `S3', `S4', and `S5', are added back to .}
    \label{fig:flexibility}
\end{figure}
 \subsection{Flexibility of the Proposed Framework}
\label{sec:experiments:flexibility}
In comparison to end-to-end UDA ensemble learning methods, our framework is more flexible as the proposed framework can operate on any arbitrary compositions of . Such flexibility enables our framework to evolve with time as (1) a model trained with any newly developed UDA method can be integrated into our framework, and (2) the student models can be added back to  and further improve the overall performance. To demonstrate these two merits of our framework, we evaluate our framework with different composition of , as shown in Fig.~\ref{fig:flexibility}. It is observed that the student models trained under the settings `S1', `S2', and `S3', which simulate the addition of the members trained with newly developed UDA methods, can outperform their corresponding teacher models in  (whose mIoU's are reported in Table~\ref{tab:benchmark}). This implies that our framework offers the potential to evolve with time. In addition, `S4', `S5', and `S6' in Fig.~\ref{fig:flexibility} showcase that the performance of our framework can be further improved if the students are added back to , indicating that our framework can be applied in an iterative manner to produce better results. These results highlight the flexibility of our framework as any UDA methods can be incorporated and potentially enhance its performance.  \section{Conclusion}
\label{sec:conclusion}
In this paper, we presented a flexible ensemble-distillation framework to address the common pitfalls, i.e., the lack of robustness, of previous methods. We incorporated an output unification operation into the proposed framework to ensure that the fused outputs of the ensemble are free of the influence from the certainty inconsistency among the models in the ensemble. In addition, to tackle the performance variation issue, we proposed a channel-wise fusion function that is robust against this issue. As our framework is able to integrate different types of UDA methods while maintaining its robustness, it therefore pioneers a new direction for future semantic segmentation based UDA researches.

\paragraph{Acknowledgement: }~The authors acknowledge the support from the Ministry of Science and Technology (MOST) in Taiwan under grant nos. MOST 110-2636-E-007-010 and MOST 110-2634-F-007-019, as well as the support from MediaTek Inc., Taiwan. The authors thank the donation of the GPUs from NVIDIA Corporation and NVIDIA AI Technology Center, and the National Center for High-performance Computing of National Applied Research Laboratories in Taiwan for providing computational and storage resources.   \clearpage

{\small
\bibliographystyle{unsrtnat}
\bibliography{egbib}
}

\clearpage

\includepdf[pages=-]{sup.pdf}

\end{document}
