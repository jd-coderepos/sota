\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{mathtools}  
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\usepackage{float}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{amsthm}










\title{GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling}



\author{Tobias Katsch \\
Artificial Intelligence Program \\
Johannes Kepler University \\
Linz, 4040, Austria \\
\texttt{tobias.katsch42@gmail.com} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. 
Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. 
Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. 
While many existing models solely rely on data-controlled cumulative sums for context aggregation, our findings suggest that incorporating data-controlled complex cumulative products may be a crucial step towards more powerful sequence models.
\end{abstract}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.5\textwidth]{gate_loop.png}
\end{center}
\caption{The GateLoop framework takes input-dependent values $V$, keys $K$, queries $Q$ and state-transitions $A$. At each step of the recurrence, the loop's input, hidden state and output is gated. While S4, S5, LRU or RetNet forget at a fixed decay rate, the fully data-controlled approach allows for input-dependent incorporation of new information, retention of memories and forgetting. }

\end{figure}

\section{Introduction}

Modeling sequences across different modalities containing long-range dependencies is a central challenge in machine learning. Historically, Recurrent Neural Networks (RNNs) have been the natural choice for this task and led to early breakthroughs in the field. However, RNNs suffer from the vanishing and exploding gradient problem, often making them unstable to train on long sequences (\cite{hochreiter1997lstm}). Gated variants such as LSTM and GRU were developed to address this issue but are still inherently inefficient to train due to their non-linear recurrent nature. Furthermore, their sequential nature  leads to an inductive bias towards recent inputs, limiting their practical ability to draw long-range dependencies. This inspired the attention mechanism (\cite{garg2019jointly}), which was first introduced as an addition to RNN for language translation, allowing the model to draw pairwise global dependencies between input data points. 

\cite{vaswani2023attention} took this further with Transformer, which completely gets rid of recurrence and just relies on attention. The main advantages of Transformers are their efficient parallelizable training on modern hardware and their ability to draw global pairwise dependencies. The latter property comes at the price of quadratic complexity $O(l^2)$ compared to the linear complexity $O(l)$ of RNNs. This poses a practical bottleneck for many applications, for instance limiting the document length a transformer based language model can perform reasoning on. Therefore, much effort has been put into finding attention replacements with improved complexity. While these variants such as Reformer, Linformer and Performer offer a reduced complexity of $O(l \log l)$ or $O(l)$ the original transformer with only minor adjustments prevailed due to its stronger practical performance.
Furthermore, the departure from recurrence eliminated the locality bias of the model to pay more attention the recent inputs. While the absence of this bias is advantageous for some tasks, it has proven to be disadvantageous for others. This led to a line of work dedicated to injecting locality bias into Transformer (\cite{ma2023mega}, \cite{huang2023encoding}). 

Meanwhile, the works of \cite{gu2022efficiently} on the initialization of discretized State Space Models (SSMs) lead to a resurgence of linear RNNs for modeling long sequences. The most prominent model of this class S4 and its simplified diagonal variant S4D, achieve remarkable results on the long-range Arena (LRA) (\cite{tay2020long}), a benchmark designed to test a models ability to model long-range dependencies. SSMs can be trained efficiently by exploiting their linear and time-invariant nature. By rewriting the linear recurrence as a long convolution, it can be computed through the Fourier domain in $O(l \log l)$ time complexity. \cite{smith2023simplified} introduced S5, which further simplifies the application of SSMs and popularized the use of associative scan implementations for fast parallelized training.

Still, SSMs are heavily dependent on involved initialization schemes. Motivated by the question whether such tedious initialization is really necessary, \cite{orvieto2023resurrecting} developed the Linear Recurrent Unit (LRU) which is on par with S4, S4D and S5 while only requiring much simpler initialization. 

\textbf{Our contributions to this line of work are three-fold:}
\vspace{0.2cm}
\begin{itemize}[noitemsep, itemsep=0.2cm, topsep=0pt]
  \item We show that existing models only utilize a special case of linear recurrence. Motivated by this observation, we develop GateLoop, a foundational sequence model that generalizes existing linear recurrent models by utilizing data-controlled gating of inputs, hidden states and outputs. GateLoop can be trained efficiently in $O(l \log l)$ making use of highly optimized associative scan implementations. 
  \item Furthermore, we derive an equivalent $O(l^2)$ mode which links GateLoop to Transformer and prove that our approach can be interpreted as providing data-controlled relative-positional information to attention. 
  \item Finally, we demonstrate the empirical effectiveness of our approach. Specifically, our results show that GateLoop outperforms the state of the art models Transformer, Hyena (\cite{poli2023hyena}) and S5-Hyena (\cite{smith2023github}) on the WikiText103 benchmark for auto-regressive language modeling.
\end{itemize}

\section{Preliminaries}
We consider the task of approximating sequence-to-sequence mappings. The model takes a multi-channel input sequence \( x = \{ x_1, \ldots, x_l \} \) packed as a matrix \( X \in \mathbb{R}^{l \times d_{x}} \) and outputs \( Y \in \mathbb{R}^{l \times d_{y}} \). A common assumption in this context is causality, implying that for modeling \( y_n \), only information from all \( x_m \) with \( m \leq n \) may be used. This enables efficient training strategies such as auto-regressive language modeling. 

\newpage

\subsection{Recurrent Neural Network}
A Recurrent Neural Network (RNN) layer approximates a sequence-to-sequence mapping through the following recurrence relation involving learnable parameters $A \in \mathbb{R}^{d_h \times d_h}$, $B \in \mathbb{R}^{d_h \times d_x}$, $C \in \mathbb{R}^{d_y \times d_h}$ and an activation function $\sigma$.\footnote{For clarity, we omit the potential use of biases and skip connections throughout this paper. Furthermore, we consider $h_0$ to be 0.} 
\begin{equation}\label{eq:rnn}
h_n = \sigma(A h_{n-1} + B x_n), \quad y_n = C h_n
\end{equation}

Common choices for $\sigma$ are tanh or sigmoid. If we chose $\sigma$ to be the identity function, the RNN layer becomes linear. 

\subsection{State Space Model}
The continuous state space model (SSM) is characterized by the  differential equation \ref{eq:ssm}. Here, $\tilde{A} \in \mathbb{C}^{d_h \times d_h}$, $\tilde{B} \in \mathbb{C}^{d_h \times d_x}$, $\tilde{C} \in \mathbb{C}^{d_y \times d_h}$ are complex valued, the function $\Re(.)$ extracts the real part and $\Bar{h}(0)$ is defined to be $0$. 
\begin{equation}\label{eq:ssm}
\frac{d \tilde{h}(t)}{dt} = \tilde{A} \tilde{h}(t) + \tilde{B} x(t), \quad y(t) = \Re(\tilde{C} \tilde{h}(t))
\end{equation}

Moreover, \( \tilde{A} \) can be diagonalized through its eigenvalue decomposition \( \tilde{A} = V \Lambda V^{-1} \). In this representation, \( \Lambda \) is a diagonal matrix of eigenvalues, and \( V \) is the matrix of corresponding eigenvectors. Now, by absorbing \( V \) and \( V^{-1} \) into \( \tilde{C} \) and \( \tilde{B} \), respectively, we obtain the diagonalized SSM. For more details on this procedure, please see \cite{smith2023simplified}.
\begin{subequations}\label{eq:ssm_diag}
\begin{equation}
\Bar{B} = V^{-1} \tilde{B}, \quad \Bar{C} = \tilde{C} V, \quad \Bar{h}(t) = V^{-1} \tilde{h}(t)
\end{equation}
\begin{equation}
\frac{d \Bar{h}(t)}{dt} = \Lambda \Bar{h}(t) + \Bar{B} x(t), \quad y(t) = \Re(\Bar{C} \Bar{h}(t))
\end{equation}
\end{subequations}

In order to utilize the SSMs practically for sequence modeling, they can be discretized, e.g., through the zero-order hold (ZOH), bilinear, or Euler method. Given a fixed discretization step-size \( \Delta \in \mathbb{R_{+}} \), the ZOH method yields the linear recurrence relation
\begin{equation}\label{eq:ssm_discrete}
h_n = A h_{n-1} + B x_n, \quad y_n = \Re(C h_n)
\end{equation}
with the parameterization:
\begin{equation}
A = \exp(\Delta \Lambda), \quad B = \Lambda^{-1}(A-I)\Bar{B}, \quad C = \Bar{C}
\end{equation}
Discretizing the state space model (\ref{eq:ssm_discrete}) gives a linear RNN layer (\ref{eq:rnn}) involving special reparameterizations of its weights. While this result is simply the solution of the ZOH method application, it is worth paying attention to its interpretability. Specifically, consider the influence of the discretization step size:
\begin{equation}
\lim_{{\Delta \to 0}}(A, B) = (I, 0)
\end{equation}
In the limit $\Delta \to 0$, no new information enters the state space model and the hidden state remains constant. A small $\Delta$ leads to a sequence-to-sequence mapping with small rates of change, while a large $\Delta$ leads to large rates of change. It becomes clear, that the step-size has vital impact on the model's retain/forget properties. For S5, \cite{smith2023simplified} define $\Delta$ as a learnable parameter vector, where the default values for initialization are logarithmically spaced from $0.001$ up to $0.1$. This is done in order to facilitate the learning of dependencies across different time scales. 

\cite{gu2022efficiently} observe that training SSMs with naive parameter initialization for the state transition $\Bar{A}$ is not effective in practice. Grounded in theoretical memory compression results, they develop the HiPPO framework, which they utilize to find suitable initializations. Models of this class include S4, DSS, S4D and S5. Other initializations, which do not rely on HiPPO theory, nor on the correspondence to the continuous SSM representation have been proposed such as for LRU (\cite{orvieto2023resurrecting}) and RetNet (\cite{sun2023retentive}). 

\textbf{S4D:} The deterministic S4D-Lin initialization defines the diagonal state transition $\Bar{a}$ at channel dimension $k$ to be \( \bar{a}_k = -\frac{1}{2} + i \pi k\). Alternatively, the S4D-Inv initialization is $\bar{a}_k = -\frac{1}{2} + i\frac{l}{\pi} (\frac{l}{k+1} + 1)$. Here, $\Bar{a}$ is parameterized in continuous space. Through its ZOH discretization, $a$ is obtained.

\textbf{LRU:} The stable exponential initialization is defined as \( a = \exp(-\exp(\alpha) + i\exp(\theta)) \), where \( \alpha \) and \( \theta \) are learnable parameters. 

\textbf{RetNet:} \cite{sun2023retentive} applies a fixed state transition formulation closely linked to the xPos positional embedding for transformers (\cite{sun2022lengthextrapolatable}). For this model, we have \( a = \gamma \exp(i \theta) \) with the magnitude initialization \( \gamma = 1 - 2^{-5-c} \), where \( c \) is some positive constant.

\section{Data Controlled Linear Recurrence}
Incorporating data-control into deep learning models has proven to be highly successful for developing performant sequence models. Transformer, in its core, is built on the data-controlled linear operator implemented by attention (\cite{massaroli2021dissecting}). Furthermore, \cite{fu2023hungry} show, that SSMs lack the data-control required for modeling language adequately. Based on this observation, they develop H3 which employs SSMs in conjunction with data-controlled element-wise gating. With this addition, they decrease the expressivity gap between Transformer and SSM-based-models for language modeling tasks. Inspired by these findings, we take the data-control paradigm further. 

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.9\textwidth]{generalization.png}
\end{center}
\caption{Omitting $B$, $C$ and application of $\Re(.)$ for clarity. First, we define the input and output gates $k_n, q_n \in \mathbb{C}^{d_h}$ following \cite{sun2023retentive}. Next, as our core contribution, we replace the static state transition with time-varying state transitions $a_n \in \mathbb{C}^{d_h}$. This allows for content aware control over the forget- and retention behaviour. While $q_n$ and $k_n$ act as input and output gates respectively, $a_n$ can be interpreted as a forget- and retain gate. Putting everything together, we obtain GateLoop, characterized by the the linear recurrence relation \ref{eq:glr}. We hypothesize, that allowing for time-varying control over the forget/retain behaviour can enable sequence models to keep important memories longer and discard unimportant memories faster compared to only relying on static gates. In section \ref{sec:experiments} we present experimental results which confirm this hypothesis.}
\label{fig:generalization}
\vspace{-2em}
\end{figure}
\begin{align}\label{eq:glr}
h_n &= h_{n-1} a_n + k_n^{\top} v_n \\
y_n &= q_n h_n
\end{align}
\vspace{0.3em}
Note, that for generality we define an outer product $k_n^{\top} v_n$ entering the gate loop. Therefore, $k_n^{\top} v_n$ and $h_n$ are of shape $\mathbb{C}^{d_h \times d_h}$. Choosing a max-headed variant, that is $d_h = 1$, we obtain the \textit{SISO} case which coincides with previous definitions and element-wise gating when parallelized across multiple channels. 

Unfolding the recurrence relation yields equation \ref{eq:unfolded}, which involves a cumulative sum over preceding time steps discounted by a cumulative product of state transitions. 
\vspace{0.3em}
\begin{align} \label{eq:unfolded}
y_n &= q_n \sum_{m=1}^{n} k_m^\top v_m \prod_{j=m+1}^{n} a_j
\end{align}

\newpage

\subsection{Relation to other Models}

\textbf{S4, S4D, LRU:} These models are obtained as a special case of GateLoop. Here, no content aware gating nor data-controlled state transitions are applied. Their defining linear recurrence relation can be unfolded into an expression which  is equivalent to convolving $v$ with a structured filter. In contrast, GateLoop cannot be computed through convolution and instead we resort to associative scans for efficient computation. This is outlined in subsection \ref{sec:scan}.
\begin{equation} \label{eq:s4}
y_n = \sum_{m=1}^{n} v_m A^{n-m} = (V * (I, A, \dots, A^{l-1}))_n
\end{equation}

\textbf{Hyena:} \cite{poli2023hyena} obtain a Hyena as generalization of the SSM based H3 by considering arbitrarily defined long implicit convolutions of the form $y_n = v * (K_1, \dots, K_l)$. Therefore, both GateLoop and Hyena are mutually exclusive generalizations of the linear RNN layer. 

\textbf{RetNet:} Our method degenerates to RetNet when keeping data-controlled input and output gates but fixing the state transition gate. 
\begin{align} \label{eq:retnet}
y_n &= q_n \sum_{m=1}^{n} k_m^\top v_m A^{n-m}
\end{align}





\subsection{Efficient Associative Scan Computation}\label{sec:scan}
\cite{smith2023simplified} popularized the use of associative scan implementations for efficient parallelized computation of linear recurrence. In this subsection, we generalize their approach to derive an efficient method for computing the recurrence relation \ref{eq:glr} for $n = 1 \dots l$ parallelized in $O(l \log_2 l)$ time complexity. Given an arbitrary associative operator $\bullet$, and a sequence of elements $\{ x_n \}_{n=1}^{l}$, an associative scan computes their all-prefix sum $\Sigma$. 
\begin{align}
\Sigma(\{ x_n \}_{n=1}^{l}) &= ((x_1), (x_1 \bullet x_2), (x_1 \bullet x_2 \bullet x_3), \ldots, (x_1 \bullet x_2 \bullet \ldots \bullet x_l))
\end{align}

The recurrence relation in \ref{eq:glr} satisfies this form when arranging the elements $a_n$ and $k_n^\top v_n$ as the tuple leaf elements $\{ x_n \}_{n=1}^{l} = \{(a_n, k_n^\top v_n)\}_{n=1}^{l}$ and defining $\bullet$ as the following. 
\begin{equation}\label{eq:operator}
p \bullet q = (p_1, p_2) \bullet (q_1, q_2) = (p_1 \odot q_1, q_1 \odot p_2 + q_2)
\end{equation}


For more detailed information on prefix sum algorithms we refer to \cite{beloch90}. The associative scan computes the prefix-sum efficiently in parallel through application of the binary operator on a computational tree graph. In the following, we provide a simple python JAX implementation of the GateLoop operator. For the proof of the involved binary operator's associativity, we refer to the appendix \ref{sec:parallel_scan_details}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.\textwidth]{code1.png}
\end{figure}







\newpage

\subsection{Surrogate Attention Representation}
In this subsection, we derive an mathematically equivalent surrogate attention mode for computing the recurrence in $O(l^2)$. For this, we first rewrite the cumulative product of state transitions in order to separate the variables $n$ and $m$. 
\begin{align}
y_n &= q_n \sum_{m=1}^{n} k_m^\top v_m \left( \prod_{j=1}^{n} a_j \right) \left( \prod_{j=1}^{m} a_j^{-1} \right) \\
&= \sum_{m=1}^{n} \left( q_n \prod_{j=1}^{n} a_j \right) \left( k_m \prod_{j=1}^{m} a_j^{-1} \right)^\top v_m \\
\intertext{Using this arrangement, we can conveniently pre-compute the prefix-cumulative-product $\pi_n$ of the state transitions.}
\pi_n &= \prod_{j=1}^{n} a_j \\
y_n &= \sum_{m=1}^{n} \left( q_n \pi_n \right) \left( k_m \pi_m^{-1} \right)^\top v_m
\end{align}
From this, the parallel $O(l^2)$ surrogate attention formulation can be obtained by packing the prefix-cumulative-product in a matrix $\Pi(A) \in \mathbb{C}^{l \times d}$ and by applying a causal mask $M \in \mathbf{R}^{l \times l}$ to the resulting surrogate attention matrix.
\begin{figure}[H]
  \begin{minipage}{0.6\textwidth}
    \begin{align}
    \overline{Q} &= Q \odot \Pi(A) \\[3pt]
    \overline{K} &= K \odot \Pi(A)^{-1} \\[18pt]
    M_{nm} &= 
    \begin{cases} 
    1 & n \geq m \\
    0 & n < m
    \end{cases} \\[18pt]
    Y &= (\overline{Q} \overline{K}^\top \odot M) V
    \end{align}
  \end{minipage}
  \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=0.6\linewidth]{quadratic_gate_loop.png}
  \end{minipage}
  \caption{Considering this alternative formulation, our approach can be interpreted as providing data-controlled relative-positional information to Attention.}
\end{figure}
\subsection{Generalizing Softmax-Attention}
The $O(l^2)$ representation furthermore gives the opportunity of generalization for other forms of (non-linear) attention. For softmax attention this can be achieved by simply masking out the upper triangular matrix of the relative-positional-information infused attention scores with $-\infty$ and then applying softmax. The softmax sets the $-\inf$ entries to 0 resulting in the desired re-weighting of attention scores.

\begin{align}
M_{-\infty}(X) &= \begin{cases} 
  X_{ij} & i \geq j, \\
  -\infty & i < j 
\end{cases} \\[3pt]
Y &= \text{Softmax}(M_{-\infty}(\overline{Q} \overline{K}^\top))V
\end{align}

\section{Practical Implementation}\label{sec:practical}

For utilizing the GateLoop framework practically, we define a simple yet powerful model. To obtain values $v_n$, keys $k_n$, and queries $q_n$, we apply linear projections to the input $x_n$, following to \cite{vaswani2023attention}. As suggested by \cite{orvieto2023resurrecting} and \cite{sun2023retentive}, we control the magnitude $\gamma$ and phase $\theta$ of the state transitions separately. For some magnitude activation $f$ and a phase activation $g$, we define the state transition in polar form:
\begin{align}
    a_n &= \gamma_n \exp(i \theta_n) = f(\alpha_n) \exp(ig(\beta_n))
\end{align}

Inspired by the discretization of the state space model, \cite{orvieto2023resurrecting} utilizes the non-data-controlled parameterization $ \gamma = \exp(-\exp(\alpha)), \quad \theta = \exp(\beta)$. This restricts $\gamma$ to the interval (0, 1) which prevents a blow-up of $A^{n-m}$ for $n \to \infty$. 

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.45\textwidth]{amplitude_activations.png}
    \end{center}
    \caption{The stable exponential amplitude activation implemented by LRU is biased towards amplitudes close to 1. This bias is evident when plotting the (centered) stable-exponential amplitude activation function. In contrast, the sigmoid function does not have this bias. For our experiments, we chose $\gamma_n = \sigmoid(\alpha_n)$ as the magnitude activation. Because the imaginary part of an individual state transition is not strictly required to be restricted to a specific interval, we omit the phase activation. For the model details, we refer to appendix \ref{sec:model}.}
\end{figure}


\section{Experimental Results}\label{sec:experiments}

In this section, we report experimental results validating our hypothesis that data-controlled state transitions yield empirical benefits in sequence modeling. First we design a synthetic language modeling task that offers interpretable insights to our method. Moreover, we assess the performance of our method for autoregressive natural language modeling. For this we conduct experiments on the widely recognized WikiText-103 benchmark.

\subsection{Memory Horizon}

Synthetic datasets are have played an important role for guiding model development, highlighting specific model advantages and weaknesses and to improve model interpretability. (\cite{olsson2022incontext}, \cite{fu2023hungry}). We define our own synthetic task, specifically designed to validate the empirical advantage of data-controlled over non-data-controlled state transitions. The Memory Horizon Dataset for autoregressive synthetic language modeling is specified through an input number range, a reset token, sequence length and the number of randomized resets per sample. In order to solve this task successfully, at each time step, the past input information back to last preceding reset token needs to be memorized. We refer to appendix \ref{sec:memory_horizon_details} for details on the underlying target compression function and dataset construction parameters. The task is designed for favoring models that can forget memories preceding an encountered reset token. Although this is a synthetic language, we hypothesize and subsequently demonstrate in section \ref{subsec:WikiText103}, that the fundamental capability to forget memories based on input is crucial for effectively modeling sequences from more practical modalities.


\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.9\textwidth]{memory_horizon.png}
    \end{center}
    \caption{We visualize the applied state transition magnitudes of the trained fully data-controlled linear recurrent model, using a example sequence from the Memory Horizon dataset. Dataset details and hyperparameters can be found in appendix \ref{sec:memory_horizon_details} and \ref{subsec:memory_horizon_hyperparams} respectively. For all models layers and channels (vertically), the magnitude activations are plotted along the sequence length (horizontally). Moreover, the magnitude activation averages across channels and layers are shown. As hypothesized, through data-controlled linear recurrence, this model can learn to forget memories input-dependently by applying a (close to) zero state transition at the ideal reset positions, effectively vacating its hidden state for new relevant information.}
\end{figure}



\begin{figure}[H]
    \centering
    \begin{tabular}{cc}
        \toprule
        State transition type & Test Accuracy \\ 
        \midrule
        Data-Controlled & \textbf{0.43} \\ 
        Fixed & 0.25 \\ 
        \bottomrule
    \end{tabular}
    \caption{We compare the test accuracy of the GateLoop model instance with that of a second trained linear recurrent model, which differs only in its use of a fixed state transition. The results show that making the forget/retain mechanism input dependent improves the test accuracy significantly.}
    \label{fig:test_accuracy_table}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{vs.png}
    \caption{We plot the test accuracy over the required memory span. Not surprisingly, predicting the correct token becomes more difficult as the necessary memory capacity increases. For all required memory spans, the fully data-controlled variant performs better than the 'fixed' variant. While the performance of the latter model variant falls of rapidly after the required memory span exceeds 50, the former model variant maintains comparable performance for twice as long. Concluding, this simple synthetic language modeling task confirms that data-dependent control over the forget/retain properties can improve sequence modeling capabilities in practise.}
    \label{fig:accuracy_memory_span}
\end{figure}

\subsection{WikiText103}\label{subsec:WikiText103}

The WikiText103 dataset for autoregressive natural language modeling comprises over 100 million tokens extracted from verified Wikipedia articles. We test our fully data-controlled linear recurrent model against the state of the art competition. The model details are reported in section \ref{sec:model}. 

\begin{table}[H]
    \caption{Comparison of WikiText103 test perplexity (lower is better) of different models. All models use the same tokenizer. The results for the other models are taken from \cite{poli2023hyena} and \cite{smith2023github}}.
    \centering
    \begin{tabular}{ccc}
        \toprule
        Model & Parameters & Test Perplexity \\ 
        \midrule
        Transformer & 125M & 18.6 \\ 
        Hybrid H3 & 125M & 18.5 \\ 
        Performer & 125M & 26.8 \\ 
        Reformer & 125M & 26.0 \\ 
        Linear Attention & 125M & 25.6 \\ 
        Transformer-XL & 258M & 18.4 \\ 
        Hyena & 125M & 18.5 \\
        S5-Hyena & 125M & 18.3 \\
        GateLoop & 125M & \textbf{13.4} \\
        \bottomrule
    \end{tabular}
    \label{table:2}
\end{table}

GateLoop takes a significant performance leap forward over existing models.  while offering advantages such as avoiding softmax-attention layers (unlike Transformer and Hybrid H3), eliminating the need for tedious initialization (unlike State Space Models), and not requiring long implicit convolutions (unlike Hyena). 

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{state_transitions.png}
    \end{center}
    \caption{We plot the state transitions of the trained model for a random test input batch at layers 0 and 8. We observe structured patterns in the data-controlled state transition. While we leave interpretability for future work, we point out that these patterns indicate that the trained model deliberately utilizes the data-controlled gating of the state transition (and thus forgetting and retention of memories) by applying large varieties of magnitudes and phases.}
\end{figure}

\section{Future Work}

While our primary focus in this paper is to establish the groundwork for constructing fully data-controlled linear RNNs, we recognize the multitude of opportunities for future research. One avenue involves exploring the effects of different initialization strategies, amplitude- and phase-activations. Moreover, we suggest that future work should pay focus to the interpretability of the learned state transitions for gaining deeper insights into the model's inner workings.

\section{Conclusion}

We introduce GateLoop, a fully data-controlled linear RNN which generalizes existing linear recurrent models by leveraging data controlled gating of inputs and outputs and state transitions. While our method comes with linear runtime complexity $O(l)$,  we derive an efficient parallelizable $O(l \log l)$ training strategy utilizing parallel scans. Furthermore, GateLoop can be reformulated in an equivalent $O(l^2)$ surrogate attention mode which reveals, that its mechanism can be interpreted as providing relative positional information to Attention. Finally we validate empirically, that fully data-controlled linear recurrence is highly performant for autoregressive language modeling.  

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\appendix

\section{Memory Horizon dataset details}\label{sec:memory_horizon_details}

In this section, we describe the details of the Memory Horizon Dataset for synthetic language modeling. The goal of this dataset is to highlight the advantage of data-controlled over non-data-controlled state transitions for linear recurrent models.

\begin{table}[H]
    \caption{This table lists the parameters we use for constructing the Memory Horizon Dataset. The input vocabulary consists of a reset token and the number tokens for all numbers within the input number range. The output vocabulary consists of the number tokens from 0 up to the maximal output number.}
    \centering
    \begin{tabular}{lcc}
        \toprule
        Parameter & Value \\
        \midrule
        Input numbers range & $[0, 4]$ \\
        Sequence length & 1024 \\
        Resets per sample & 3 \\
        Max output & $50$ \\
        Number of samples & 2000 \\
        \bottomrule
    \end{tabular}
    \label{table:general_hparams}
\end{table}

Furthermore, we apply a memory compression function that computes the target token based on a list of input number tokens. This list extends from the most recent reset token to the end of the input sequence, or if no reset token is present, from the start of the sequence. The function calculates an alternating sum of products by multiplying pairs of numbers from opposite ends of the list. The operation alternates between addition and subtraction for each pair. In cases where the list has an odd number of elements, the middle element is either added or subtracted, depending on the current operation. Finally, the result is taken modulo a specified number to compress the memory value.




\begin{figure}[H]
    \centering
    \includegraphics[width=1.\textwidth]{code2.png}
\end{figure}

\section{Parallel Scan}\label{sec:parallel_scan_details}

For completeness, we show the associativity of the utilized binary operator. 

\begin{proof}
\begin{align*}
(a \bullet b) \bullet c &= (a_1 \odot b_1, a_2 + b_2) \bullet (c_1, c_2) \\
&= (a_1 \odot b_1 \odot c_1, c_1 \odot (a_2 + b_2) + c_2) \\
&= (a_1 \odot b_1 \odot c_1, c_1 \odot a_2 + c_1 \odot b_2 + c_2) \\
a \bullet (b \bullet c) &= a \bullet (b_1 \odot c_1, b_2 + c_2) \\
&= (a_1 \odot b_1 \odot c_1, c_1 \odot a_2 + c_1 \odot b_2 + c_2) \\
&= (a_1 \odot b_1 \odot c_1, c_1 \odot a_2 + c_1 \odot b_2 + c_2)
\end{align*}
\end{proof}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.74\textwidth]{associative_scan.png}
\end{center}
\caption{For the defined operator and leaf elements, the associative scan procedure is illustrated for $1 \leq n \leq 4$. The right tuple elements act as a \textit{kv-caches} of dimension $\mathbb{C}^{d_h \times d_h}$ for the compressed memories of the respective sub-graph.}
\label{fig:associative_scan}
\end{figure}

\newpage


\section{Model Details}\label{sec:model}

\textbf{Each model layer is composed of:}
\begin{itemize}
\item A Time-Mixing block that aggregates information across the temporal dimension. In this case, this is the GateLoop operator with the defined content aware inputs. We use real-valued weights for the involved linear projection and return only the real part of the GateLoop output. 
\item A Channel-Mixing block designed to approximate functions along the channel dimension. In this experiment, a simple FNN is applied point-wise to the sequence vectors.
\item Skip-Connections and Layer Normalization, which are recommended to allow information to skip channel/time mixing and stabilize training.
\end{itemize}
\textbf{The models consist of:}
\begin{itemize}
\item An learned input token embedding.
\item A stack of $L$ model layers, with the specific number depending on the model type.
\item A language head, which is a linear projection that maps the output of the last layer to a probability distribution (actually the logits) over the vocabulary. The model is trained to model the probability distribution over the possible output tokens given the current input context.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{model}
\caption{Visualization of the full model architecture.}
\end{figure}


\subsection{MemoryHorizon hyperparameters}\label{subsec:memory_horizon_hyperparams}

\begin{table}[H]
    \caption{Model hyperparmeters used for the MemoryHorizon experiment.}
    \centering
    \begin{tabular}{lcc}
        \toprule
        Hyperparameter & Value \\
        \midrule
        Number of epochs & 300 \\
        Batch size & 32 \\
        Learning rate & 0.0025 \\
        Optimizer & AdamW \\
        Optimizer momentum ($\beta_1, \beta_2$) & 0.9, 0.98 \\
        Weight decay & 0.05 \\
        Learning rate schedule & cosine decay (linear warm-up) \\
        Number of warmup steps & 10000 \\
        \midrule
        n\_layer & 4 \\
        d\_channel\_mixing & 128 \\
        d\_model & 64 \\
        d\_qk & 64 \\
        d\_v & 64 \\
        n\_head & 64 \\
        magnitude\_activation & sigmoid \\
        phase\_activation & identity \\
        \bottomrule
    \end{tabular}
    \label{table:general_hparams}
\end{table}

\subsection{WikiText103 hyperparameters}\label{sec:wikitext103_hyperparameters}

\begin{table}[H]
    \caption{Hyperparmeters used for the WikiText103 experiment. We apply a smaller learning to the projections which control the state transition. Moreover, no weight decay is applied to these parameters.}
    \centering
    \begin{tabular}{lcc}
        \toprule
        Hyperparameter & Value \\
        \midrule
        Number of epochs & 100 \\
        Batch size & 16 \\
        Base learning rate & 0.000125 \\
        State transition learning rate & 0.0001 \\
        Optimizer & AdamW \\
        Optimizer momentum ($\beta_1, \beta_2$) & 0.9, 0.98 \\
        Weight decay & 0.25 \\
        Learning rate schedule & cosine decay (linear warm-up) \\
        Number of warmup steps & 5000 \\
        \midrule
        n\_layer & 12 \\
        d\_channel\_mixing & 1872 \\
        d\_model & 624 \\
        d\_qk & 624 \\
        d\_v & 624 \\
        n\_head & 624 \\
        magnitude\_activation & sigmoid \\
        phase\_activation & identity \\
        \bottomrule
    \end{tabular}
    \label{table:general_hparams}
\end{table}






\end{document}
