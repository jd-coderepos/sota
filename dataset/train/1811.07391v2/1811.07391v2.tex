\section{Experiments}

We evaluated our online action detector against multiple
state-of-the-art and baseline methods on three publicly-available
datasets: HDD~\cite{RamanishkaCVPR2018}, TVSeries~\cite{de2016online},
and THUMOS'14~\cite{THUMOS14}. 
We chose these datasets because they
include long, untrimmed videos from diverse
perspectives and applications:
HDD consists of on-road driving from a first-person
(egocentric) view recorded by a front-facing dashboard camera, TVSeries
was recorded from television and contains a variety of
everyday activities, and THUMOS'14 is a popular dataset of
sports-related actions.

\subsection{Datasets}

\vspace{-5pt} \xhdr{HDD}\cite{RamanishkaCVPR2018} includes nearly 104
hours of 137 driving sessions in the San Francisco Bay Area. The
dataset was collected from a vehicle with a front-facing
camera, and includes frame-level annotations of 11 goal-oriented
actions (\eg, intersection passing, left turn,
right turn, \etc). The dataset also includes readings from a
variety of non-visual sensors collected by the instrumented vehicle's
Controller Area Network (CAN bus). We followed prior
work~\cite{RamanishkaCVPR2018} and used 100 sessions for training and
37 sessions for testing.

\xhdr{TVSeries}~\cite{de2016online} contains 27 episodes of 6 popular
TV series, totaling 16 hours of video. The dataset is temporally
annotated at the frame level with 30 realistic, everyday actions (\eg,
pick up, open door, drink, \etc). The dataset is challenging with
diverse actions, multiple actors,
unconstrained viewpoints, heavy occlusions, and a large proportion of
non-action frames.

\xhdr{THUMOS'14}~\cite{THUMOS14} includes over 20 hours of video
of sports annotated with 20 actions. The training set contains only trimmed videos
that cannot be used to train temporal action detection models, so
we followed prior work~\cite{gao2017red}
and train on the validation set (200 untrimmed videos) and evaluate on the test set (213 untrimmed videos).

\subsection{Implementation Details}
We implemented our proposed Temporal Recurrent Network (TRN) in 
PyTorch~\cite{pytorch}, and performed all
experiments on a system with Nvidia Quadro P6000 graphics
cards.\footnote{The code will be made publicly available upon
publication.} To learn the network weights, we used the
Adam~\cite{kingma2014adam} optimizer with default parameters, learning
rate , and weight decay . For data augmentation,
we randomly chopped off  frames from the 
beginning for \textit{each} epoch, and discretized the
video of length  into  non-overlapping training
samples, each with  consecutive frames.
Our models were optimized in an end-to-end manner using a batch size
of , each with  input sequence length.
The constant  in Eq.~(\ref{eq:loss})
was set to .

\subsection{Settings}
To permit fair comparisons with the
state-of-the-art~\cite{RamanishkaCVPR2018,de2016online,gao2017red},
we follow their experimental settings, including input features and
hyperparameters.

\xhdr{HDD.}
We use the same setting as in~\cite{RamanishkaCVPR2018}.
Video frames and values from CAN bus sensors are first sampled at
 frames per second (fps). The outputs of the \texttt{Conv2d\_7b\_1x1} layer in
InceptionResnet-V2~\cite{szegedy2016inception} pretrained on
ImageNet~\cite{deng2009imagenet} are extracted as the visual
feature for each frame. To preserve spatial information, we
apply an additional  convolution to reduce the extracted
frame features from  to
, and flatten them into -dimensional vectors.
Raw sensor values are passed into a fully-connected layer with
-dimensional outputs. These visual and sensor features are then
concatenated as a \textit{multimodal} representation for each video
frame.  We follow~\cite{RamanishkaCVPR2018} and set the input sequence
length  to  . The number of decoder steps  is
treated as a hyperparameter that we cross-validate in 
experiments. The hidden units of both the temporal decoder and the STA
are set to  dimensions.

\xhdr{TVSeries and THUMOS'14.}
We use the same setting as in~\cite{gao2017red}. We extract video
frames at  fps and set the video chunk size to . Decisions are
made at the chunk level, and thus performance is evaluated every 
seconds. We use two different feature extractors,
VGG-16~\cite{simonyan2014very} and two-stream (TS)
CNN~\cite{xiong2016cuhk}. VGG-16 features are extracted at the
\texttt{fc6} layer from the central frame of each chunk. For the
two-stream features, the appearance features are extracted at the
\texttt{Flatten\_673} layer of ResNet-200~\cite{he2016deep} from the
central frame of each chunk, and the motion features are extracted at
the \texttt{global\_pool} layer of BN-Inception~\cite{ioffe2015batch}
from precomputed optical flow fields between   consecutive
frames. The appearance and motion features are then concatenated to
construct the two-stream features. The input sequence length 
is set to   due to GPU memory limitations. Following the
state-of-the-art~\cite{gao2017red}, the number of decoder steps
 is set to , corresponding to  seconds.
As with HDD, our experiments report results with
different decoder steps. The hidden units of both the temporal decoder
and the STA are set to  dimensions.

\begin{table*}[t]
    \small
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}
            {@{\quad}lc@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\quad\;\;\;\;}c@{\quad}}
\toprule
& & \multicolumn{11}{c}{Individual actions} & \\
\cmidrule(r{4\cmidrulekern}){3-13}
            \begin{tabular}{@{}c@{}} \\ Method \end{tabular} &
            \begin{tabular}{@{}c@{}} \\ Inputs \end{tabular} &
            \begin{tabular}{@{}c@{}}intersection \\ passing \end{tabular} &
            \begin{tabular}{@{}c@{}}\\ L  turn            \end{tabular} &
            \begin{tabular}{@{}c@{}}\\ R  turn           \end{tabular} &
            \begin{tabular}{@{}c@{}}L  lane \\ change  \end{tabular} &
            \begin{tabular}{@{}c@{}} R  lane \\ change \end{tabular} &
            \begin{tabular}{@{}c@{}} L  lane \\ branch  \end{tabular} &
            \begin{tabular}{@{}c@{}} R  lane \\ branch \end{tabular} &
            \begin{tabular}{@{}c@{}}crosswalk \\ passing    \end{tabular} &
            \begin{tabular}{@{}c@{}}railroad \\ passing     \end{tabular} &
            \begin{tabular}{@{}c@{}} \\merge                   \end{tabular} &
            \begin{tabular}{@{}c@{}} \\u-turn                  \end{tabular} &
            \begin{tabular}{@{}c@{}}Overall \\mAP                  \end{tabular} \\
            \midrule
            CNN & \multirow{4}{*}{Sensors} & 34.2 & 72.0 & 74.9 & 16.0 &  8.5 & 7.6 & 1.2 & 0.4 & 0.1 & 2.5 & 32.5 & 22.7 \\
            LSTM~\cite{RamanishkaCVPR2018} & & 36.4 & 66.2 & 74.2 & 26.1 & 13.3 & 8.0 & 0.2 & 0.3 & 0.0 & 3.5 & 33.5 & 23.8 \\
            ED & & 43.9 & 73.9 & 75.7 & 31.8 & 15.2 & 15.1 & 2.1 & 0.5 & 0.1 & 4.1 & 39.1 & 27.4 \\
            \textbf{TRN} & & 46.5 & 75.2 & 77.7 & 35.9 & 19.7 & 18.5 & 3.8 & 0.7 & 0.1 & 2.5 & 40.3 &\textbf{29.2} \\
            \cmidrule{1-14}
            CNN & \multirow{4}{*}{InceptionResNet-V2} & 53.4 & 47.3 & 39.4 & 23.8 & 17.9 & 25.2 & 2.9 & 4.8 & 1.6 & 4.3 & 7.2 & 20.7 \\
            LSTM~\cite{RamanishkaCVPR2018} & & 65.7 & 57.7 & 54.4 & 27.8 & 26.1 & 25.7 & 1.7 & 16.0 & 2.5 & 4.8 & 13.6 & 26.9 \\
            ED & & 63.1 & 54.2 & 55.1 &28.3 & 35.9 & 27.6 & 8.5 & 7.1 & 0.3 & 4.2 & 14.6 & 27.2 \\
            \textbf{TRN} & & 63.5 & 57.0 & 57.3 & 28.4 & 37.8 & 31.8 & 10.5 & 11.0 & 0.5 & 3.5 & 25.4 &\textbf{29.7} \\
            \cmidrule{1-14}
            CNN & \multirow{4}{*}{Multimodal} & 73.7 & 73.2 & 73.3 & 25.7 & 24.0 & 27.6 & 4.2 & 4.0 & 2.8 & 4.7 & 30.6 & 31.3 \\
            LSTM~\cite{RamanishkaCVPR2018} & & 76.6 & 76.1 & 77.4 & 41.9 & 23.0 & 25.4 &  1.0 & 11.8 & 3.3 & 4.9 & 17.6 & 32.7 \\
            ED & & 77.2 & 74.0 & 77.1 & 44.6 & 41.4 & 36.6 & 4.1 & 11.4 & 2.2 & 5.1 & 43.1 & 37.8 \\
            \textbf{TRN} & & 79.0 & 77.0 & 76.6 & 45.9 & 43.6 & 46.9 & 7.5 & 13.4 & 4.5 & 5.8 & 49.6 &\textbf{40.8} \\
\bottomrule
        \end{tabular}
    }
    \vspace{-6pt}
    \caption{\textit{Results of online action detection
    on HDD,} comparing TRN and baselines using 
    mAP (\%).}
    \vspace{-6pt}
    \label{table:hdd_detection}
\end{table*}
 

\subsection{Evaluation Protocols}
\label{sec:protocols}

We follow most existing work and use per-frame \textbf{mean average
precision (mAP)} to evaluate the performance of online action
detection. We also use per-frame \textbf{calibrated average
precision (cAP)}, which was proposed in~\cite{de2016online} to better
evaluate online action detection on TVSeries,

where calibrated precision ,
 is  if frame  is a true
positive,  denotes the total number of true positives, and  is
the ratio between negative and positive frames. The advantage of cAP
is that it corrects for class imbalance between positive
and negative samples.

Another important goal of online
action detection is to recognize actions as early as
possible; \ie, an approach should be rewarded if it
produces high scores for target actions at their early stages (the
earlier the better).
To investigate our performance at different time stages, we
follow~\cite{de2016online} and compute mAP or cAP for each decile
(ten-percent interval) of the video frames separately. For example,
we compute mAP or cAP on the first  of the frames of the action,
then the next , and so on.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}
        {@{\quad}l@{\quad}@{\quad}c@{\quad}@{\quad}c@{\quad}}
        \toprule
        Method & Inputs & mcAP \\
        \midrule
        {CNN}~\cite{de2016online} & \multirow{7}{*}{VGG} & 60.8 \\
        {LSTM}~\cite{de2016online} & & 64.1 \\
        {RED}~\cite{gao2017red} & & 71.2 \\
        {Stacked LSTM}~\cite{de2018modeling} & & 71.4 \\
        {2S-FN}~\cite{de2018modeling} & & 72.4\\
        \textbf{TRN} & &\textbf{75.4} \\
        \midrule
        {SVM}~\cite{de2016online} & FV & 74.3 \\
        \midrule
        {RED}~\cite{gao2017red} & \multirow{2}{*}{TS} & 79.2 \\
        \textbf{TRN} & &\textbf{83.7} \\
        \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{\textit{Results of online action detection on TVSeries,}
    comparing TRN and the state-of-the-art using 
    cAP (\%).
    }
    \vspace{-5pt}
    \label{table:tv_detection}
\end{table}

\begin{table}[t]
    \centering
    \small
    \begin{tabular}
        {@{\quad}l@{\quad}@{\quad\;\;\;\;\;\;\;\;\;\;\;}c@{\quad}}
        \toprule
        Method & mAP \\
        \midrule
        {Single-frame CNN}~\cite{simonyan2014very} &34.7 \\
        {Two-stream CNN}~\cite{simonyan2014two} &36.2 \\
        {C3D + LinearInterp}~\cite{shou2017cdc} &37.0 \\
        {Predictive-corrective}~\cite{dave2017predictive} &38.9 \\
        {LSTM}~\cite{donahue2015long} &39.3 \\
        {MultiLSTM}~\cite{yeung2018every} &41.3\\
        {Conv \& De-conv}~\cite{shou2017cdc} &41.7\\
        {CDC}~\cite{shou2017cdc} &44.4 \\
        {RED~\cite{gao2017red}} &45.3 \\
        \midrule
        \textbf{TRN} &\textbf{47.2} \\
        \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{\textit{Results of online action detection on THUMOS'14,}
    comparing TRN and the state-of-the-art using mAP (\%).}
    \vspace{-10pt}
    \label{table:th_detection}
\end{table}

 

\subsection{Baselines}

We compared against multiple baselines to confirm the effectiveness
of our approach.

\xhdr{CNN} baseline models~\cite{simonyan2014very,simonyan2014two} consider online action detection as a
general image classification problem. These
baselines identify the action
in each individual video frame without modeling
temporal information.
For TVSeries and THUMOS'14, we reprint
the results of
CNN-based methods from De Geest \etal~\cite{de2016online}
and Shou \etal~\cite{shou2017cdc}. For HDD, we follow
Ramanishka \etal~\cite{RamanishkaCVPR2018} and use
InceptionResnet-V2~\cite{szegedy2016inception} pretrained on ImageNet
as the backbone and finetune the last fully-connected layer
with softmax to estimate class probabilities.

\xhdr{LSTM} and variants have been widely
used in action detection~\cite{RamanishkaCVPR2018,yeung2018every}.
LSTM networks model the dependencies between consecutive 
frames and jointly
capture spatial and temporal information of the video sequence. For
each frame, the LSTM receives the image features and the previous
hidden state as inputs, and outputs a probability distribution over
candidate actions.

\xhdr{Encoder-Decoder (ED)} architectures~\cite{cho2014learning}
also model temporal dependencies.
The encoder is similar to a general LSTM and summarizes  historical visual information into a feature vector.
The decoder is also an LSTM that produces
predicted representations for the future sequence based only on these
encoded features. Since there are no published results of ED-based
methods on HDD, we implemented a baseline with the same experimental
settings as TRN, including input features, hyperparameters,
loss function, \etc

\xhdr{Stronger Baselines.}
In addition to the above basic baselines, we tested three
types of stronger baselines that were designed for online action 
detection on TVSeries and THUMOS'14.
\textbf{Convolutional-De-Convolutional (CDC)}~\cite{shou2017cdc}
places CDC filters on top of a 3D CNN and integrates two reverse
operations, spatial downsampling and temporal upsampling, to
precisely predict actions at a frame-level.
\textbf{Two-Stream Feedback Network (2S-FN)}~\cite{de2018modeling}
is built on an LSTM with two recurrent units, where one stream focuses
on the input interpretation and the other models temporal dependencies
between actions.
\textbf{Reinforced Encoder-Decoder (RED)}~\cite{gao2017red} with
a dedicated reinforcement loss is an advanced version of ED,
and currently performs the best among all the baselines for online
action detection.

\subsection{Results}

\subsubsection{Evaluation of Online Action Detection}
\vspace{-5pt} Table~\ref{table:hdd_detection} presents evaluation
results on HDD dataset. TRN significantly outperforms the
state-of-the-art, Ramanishka \etal~\cite{RamanishkaCVPR2018}, by
, , and  in terms of mAP with sensor data,
InceptionResnet-v2, and multimodal features as inputs, respectively.
Interestingly, the performance gaps between TRN
and~\cite{RamanishkaCVPR2018} are much
larger when the input contains sensor data. Driving
behaviors are highly related to CAN bus signals, such as steering
angle, yaw rate, velocity, \etc, and this result suggests that TRN can better take
advantage of these useful input cues.
Table~\ref{table:tv_detection} presents comparisons between TRN and
baselines on TVSeries. TRN
significantly outperforms the state-of-the-art using VGG (mcAP of  over
2S-FN~\cite{de2018modeling})  and 
two-stream input features (mcAP of  over
RED~\cite{gao2017red}). We also
evaluated TRN on THUMOS'14 in
Table~\ref{table:th_detection}. The results show that TRN outperforms
all the baseline models (mAP of  over RED~\cite{gao2017red} and
 over CDC~\cite{shou2017cdc}).

\xhdr{Qualitative Results} are shown in Fig.~\ref{fig:results}.
In Fig.~\ref{fig:exp_hdd}, we visualize and compare the results of TRN,
and compare with Ramanishka \etal~\cite{RamanishkaCVPR2018} on HDD. As shown,
\textit{u-turn} is difficult to classify from a first-person perspective
because the early stage is nearly indistinguishable from
\textit{left turn}. With the help of the learned better representation
and predicted future information, TRN differentiates from
subtle differences and ``look ahead'' to reduce this
ambiguity. As shown in Table~\ref{table:hdd_detection}, TRN beats the
baseline models on most of the actions using \textit{multimodal}
inputs, but much more significantly on these ``difficult'' classes, such as
\textit{lane branch} and \textit{u-turn}.
Qualitative
results also clearly demonstrate that TRN produces not only the correct
action label, but also better boundaries. Fig.~\ref{fig:exp_tv}
and~\ref{fig:exp_th} show promising results on
TVSeries and THUMOS'14. Note that TVSeries is very
challenging; for example, the drinking action in Fig.~\ref{fig:exp_tv} 
by the person in the background in the upper left of the frame
is barely visible.

\begin{figure*}
    \center
    \begin{subfigure}[t]{1.0\textwidth}
        \center
        \includegraphics[width=0.99\linewidth]{figures/example_hdd.pdf}
        \vspace{-6pt}
        \caption{
            Qualitative comparison between our approach (3rd row)
            and~\cite{RamanishkaCVPR2018} (4th row) on HDD dataset.
            \textit{U-Turn} is shown in \textbf{\textcolor{mypurple}{purple}},
            \textit{Left Turn} is shown in \textbf{\textcolor{mygreen}{green}},
            and \textit{Background} is shown in \textbf{\textcolor{mygray}{gray}}.
        }
        \vspace{4pt}
        \label{fig:exp_hdd}
    \end{subfigure}
    \begin{subfigure}[t]{1.0\textwidth}
        \center
        \includegraphics[width=0.99\linewidth]{figures/example_tv.pdf}
        \vspace{-6pt}
        \caption{
            Qualitative result of our approach (3rd row) on TVSeries
            dataset. \textit{Drink} is shown in \textbf{\textcolor{mypink}{pink}} and
            \textit{Background} is shown in \textbf{\textcolor{mygray}{gray}}.
        }
        \vspace{4pt}
        \label{fig:exp_tv}
    \end{subfigure}
    \begin{subfigure}[t]{1.0\textwidth}
        \center
        \includegraphics[width=0.99\linewidth]{figures/example_th.pdf}
        \vspace{-6pt}
        \caption{
            Qualitative result of our approach (3rd row) on THUMOS'14
            dataset. \textit{Pole Vault} is shown in \textbf{\textcolor{myyellow}{yellow}}
            and \textit{Background} is shown in \textbf{\textcolor{mygray}{gray}}.
        }
        \vspace{4pt}
        \label{fig:exp_th}
    \end{subfigure}
    \vspace{-10pt}
    \caption{
        \textit{Qualitative results of our approach and baselines on HDD, TVSeries, and
        THUMOS'14 datasets.} 
The vertical bars
        indicate the scores of the predicted class. (Best viewed in color.) 
    }
    \label{fig:results}
    \vspace{-10pt}
\end{figure*}
 

\vspace{-10pt}
\subsubsection{Ablation Studies}

\vspace{-5pt} \xhdr{Importance of Temporal Context.}
By directly comparing evaluation results of TRN with CNN and LSTM
baselines, we demonstrate the importance of explicitly modeling temporal
context  for online action detection. LSTMs capture long- and
short-term temporal patterns in the video  by
receiving accumulated
historical observations as inputs. Comparing TRN and LSTM measures the
benefit of incorporating predicted action features as future context.
CNN-based methods conduct online action detection by only considering
the image features at each time step. Simonyan \etal~\cite{simonyan2014two} build a
two-stream network and incorporate motion features
between adjacent video frames by using optical flow as inputs.
Table~\ref{table:th_detection} shows that this motion information
provides  improvements. \textit{TRN-TS}
also takes optical flow as inputs and we can clearly see
a significant improvement ( vs. ) on TVSeries.

\xhdr{Future Context: An ``Oracle'' Study.}  To
demonstrate the importance of using predictions of the future context,
we implemented an ``oracle'' baseline,
\textit{RNN-offline}. RNN-offline shares the same architecture as RNN
but uses the features extracted from both the current and future
frames as inputs. Note that RNN-offline uses future information and
thus is not an online model; the goal of this experiment is to show:
(1) the effectiveness of incorporating future information in action
detection, given access to future information
instead of predicting it; (2) and the performance gap between
the estimated future information of TRN and
the ``real'' future information of RNN-offline.
To permit a fair comparison, the input to RNN-offline is
the concatenation of the feature extracted from the current frame and
the average-pooled features of the next 
frames (where  is the same as the number of
decoder steps of TRN).

\begin{table*}[t]
    \footnotesize
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{@{\quad}l@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}}
            \toprule
            & & \multicolumn{10}{c}{Portion of video} \\
            \cmidrule{3-12} 
            Method & Inputs & 0\%-10\% & 10\%-20\% & 20\%-30\% & 30\%-40\% & 40\%-50\% & 50\%-60\% & 60\%-70\% & 70\%-80\% & 80\%-90\% & 90\%-100\% \\
            \midrule
            {CNN~\cite{de2016online}} & \multirow{3}{*}{VGG} &61.0 &61.0 &61.2 &61.1 &61.2 &61.2 &61.3 &61.5 &61.4 &61.5 \\
            {LSTM~\cite{de2016online}} & &63.3 &64.5 &64.5 &64.3 &65.0 &64.7 &64.4 &64.3 &64.4 &64.3  \\
            \textbf{TRN} & &73.9 &74.3 &74.7 &74.7 &75.1 &75.1 &75.3 &75.2 &75.2 &75.3  \\
            \midrule
            {SVM~\cite{de2016online}} & FV &67.0 &68.4 &69.9 &71.3 &73.0 &74.0 &75.0 &76.4 &76.5 &76.8 \\
            \midrule
            \textbf{TRN} & TS &78.8 &79.6 &80.4 &81.0 &81.6 &81.9 &82.3 &82.7 &82.9 &83.3 \\
            \bottomrule
        \end{tabular}
        }
        \vspace{-5pt}
        \caption{
            \textit{Online action detection results 
            when only portions of action sequences are considered,}
            in terms of cAP (\%). 
            For example, 20\%-30\%  means that the first 30\% of frames
            of the action were seen and classified, but only frames
            in the 20\%-30\% time range
            were used to compute cAP.
        }
        \label{table:early}
        \vspace{-5pt}
\end{table*}

 

The results of RNN-offline are , ,  on
HDD, TVSeries, and THUMOS'14 datasets, respectively. Comparing
RNN-offline with the RNN baseline, we can see that the ``ground-truth"
future information significantly improves detection performance. We
also observe that the performance of TRN and RNN-offline are
comparable, \textit{even though TRN uses predicted rather than actual
future information}. This may be because TRN improves its
representation during learning by jointly optimizing current
and future action
recognition, while RNN-offline does not. We also evaluated TRN against
ED-based networks, by observing that ED can also improve its
representation by
jointly conducting action detection and anticipation. Thus,
comparisons between TRN with ED and its advanced
version~\cite{gao2017red} measure how much benefit comes purely from
explicitly incorporating anticipated future information.

\begin{table}
    \footnotesize
    \centering
    \begin{subtable}[t]{\linewidth}
        \centering
        \begin{tabular}
            {l@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\,}r@{\;\;\;}}
            \toprule
            & \multicolumn{8}{c}{Time predicted into the future (seconds)} & \\
            \cmidrule{2-9}            Method & 0.25s & 0.5s & 0.75s & 1.0s & 1.25s & 1.5s & 1.75s & 2.0s & \;\;\;\;Avg \\
            \midrule
            {ED~\cite{gao2017red}} & 78.5 & 78.0 & 76.3 & 74.6 & 73.7 & 72.7 & 71.7 & 71.0 & 74.5 \\
            {RED~\cite{gao2017red}} & 79.2 & 78.7 & 77.1 & 75.5 & 74.2 & 73.0 & 72.0 & 71.2 & 75.1 \\
            \textbf{TRN} & 79.9 & 78.4 & 77.1 & 75.9 & 74.9 & 73.9 & 73.0 & 72.3 &\textbf{75.7} \\
            \bottomrule
        \end{tabular}
        \vspace{-3pt}
        \caption{
            Results on TVSeries dataset in terms of \textit{cAP} (\%).
        }
        \vspace{3pt}
    \end{subtable}
    \begin{subtable}[t]{\linewidth}
        \centering
        \begin{tabular}
            {l@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\;}c@{\;}@{\,}r@{\;\;\;}}
            \toprule
            & \multicolumn{8}{c}{Time predicted into the future (seconds)} & \\
            \cmidrule{2-9}            Method & 0.25s & 0.5s & 0.75s & 1.0s & 1.25s & 1.5s & 1.75s & 2.0s & \;\;\;\;Avg \\
            \midrule
            {ED~\cite{gao2017red}} & 43.8 & 40.9 & 38.7 & 36.8 & 34.6 & 33.9 & 32.5 & 31.6 & 36.6 \\
            {RED~\cite{gao2017red}} & 45.3 & 42.1 & 39.6 & 37.5 & 35.8 & 34.4 & 33.2 & 32.1 & 37.5 \\
            \textbf{TRN} & 45.1 & 42.4 & 40.7 & 39.1 & 37.7 & 36.4 & 35.3 & 34.3 &\textbf{38.9} \\
            \bottomrule
        \end{tabular}
        \vspace{-3pt}
        \caption{
            Results on THUMOS'14 dataset in terms of \textit{mAP} (\%).
        }        
        \vspace{3pt}
    \end{subtable}
    \vspace{-10pt}
    \caption{
        Action anticipation results of TRN compared to state-of-the-art
        methods using two-stream features.
    }
    \vspace{-5pt} 
    \label{table:anticipation}
\end{table}

 \begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}
        {@{\;\;}l@{\;\;}@{\;\;}l@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}@{\;\;}c@{\;\;}}
        \toprule
        & & \multicolumn{4}{c}{Decoder steps ()}\\
        \cmidrule(r{2\cmidrulekern}){3-6}
        Dataset & Task & 4 & 6 & 8 & 10 \\
        \midrule
        \multirow{2}{*}{HDD}
        & {Online Action Detection}   & 39.9 & 40.8 & 40.1 & 39.6 \\
        & {Action Anticipation}       & 34.3 & 32.2 & 28.8 & 25.4 \\
        \midrule
        \multirow{2}{*}{TVSeries}
        & {Online Action Detection}   & 83.5 & 83.4 & 83.7 & 83.5 \\
        & {Action Anticipation}       & 77.7 & 76.4 & 75.7 & 74.1 \\
        \midrule
        \multirow{2}{*}{THUMOS'14}
        & {Online Action Detection}   & 46.0 & 45.4 & 47.2 & 46.4 \\
        & {Action Anticipation}       & 42.6 & 39.4 & 38.9 & 35.0 \\
        \bottomrule

    \end{tabular}
    \vspace{-5pt}
    \caption{
        Online action detection and action anticipation results of TRN
        with decoder steps .
    }
    \vspace{-12pt}
    \label{table:decoder}
\end{table}

 

\xhdr{Different Decoder Steps.} Finally, we evaluated the
effectiveness of different numbers of decoder steps . In
particular, we tested , as shown in
Table~\ref{table:decoder}, where the performance of action
anticipation is averaged over the decoder steps. The results show that
a larger number of decoder steps does not guarantee better
performance. This is because anticipation accuracy usually
decreases for longer future sequences, and
thus creates more  noise in the input features of STA.  To be
clear, we follow the state-of-the-art~\cite{gao2017red} and set
 to 2 video seconds (6 frames in HDD,  8 frames in
TVSeries and THUMOS'14) when comparing
with baseline methods of online action detection in
Tables~\ref{table:hdd_detection},~\ref{table:tv_detection},
and~\ref{table:th_detection}.

\subsubsection{Evaluation of Different Stages of Action.}

We evaluated TRN when only a fraction of each action is
considered, and compared with published
results~\cite{de2016online} on TVSeries. Table~\ref{table:early}
shows that TRN significantly outperforms existing methods at every
time stage. Specifically, when we compare \textit{TRN-TS} with the
best baseline \textit{SVM-FV}, the performance gaps between these
two methods are roughly in ascending order as less and less of the
actions are observed
(the gaps are , , , , , ,
, ,  and  from actions at 
observed to those are  observed). This indicates the advantage of
our approach at earlier stages of actions.

\subsubsection{Evaluation of Action Anticipation.}

We also evaluated TRN on predicting actions for up to 2 seconds into
the future, and  compare our approach with the
state-of-the-art~\cite{gao2017red} in
Table~\ref{table:anticipation}. The results show that TRN
performs better than RED and ED baselines (mcAP of
 vs.~ vs.~ on TVSeries and mAP of 
vs.~ vs.~ on THUMOS'14). The average of anticipation
results over the next  seconds on HDD dataset is  in terms
of per-frame mAP.
