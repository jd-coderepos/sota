\documentclass[12pt]{article}

\usepackage{hyperref,subfig,booktabs,amsthm,
  natbib, amssymb, graphicx, amsmath,color, lineno}



  \oddsidemargin 0.0in 
  \evensidemargin 0.0in 
  \textwidth 7.0in
  \headheight 0.0in 
  \topmargin 0.0in 
  \textheight=9.0in
 
\hypersetup{backref, colorlinks=false, filecolor=white}

\usepackage{Sweave}
\begin{document} 
\title{Employer Expectations, Peer Effects and Productivity:
  \\ Evidence from a Series of Field Experiments} \date{August 3, 2010}

\author{John J. Horton \\ Harvard University\footnote{ Email:
    john.joseph.horton@gmail.com.  Thanks to the NSF-IGERT
    Multidisciplinary Program in Inequality \& Social Policy (Grant
    No. 0333403), the University of Notre Dame and the John Templeton
    Foundation's Science of Generosity Initiative for generous
    financial support. Thanks to the Centre for Economic Performance
    at the London School of Economics and Political Science for
    hosting me while I worked on this project. Thanks to Richard
    Zeckhauser, Robin Yerkes Horton, Larry Katz, Nicholas Christakis,
    Rob Miller, Malcolm-Wiley Floyd, Renata Lemos and Lydia Chilton
    for helpful comments and suggestions. Thanks to John Comeau and
    Alex Breinen for research assistance. All datasets, code and
    auxiliary regression results are currently or will be available at
    the author's website:
    \href{http://sites.google.com/site/johnjosephhorton/}{http://sites.google.com/site/johnjosephhorton/}.}}

\maketitle

\begin{abstract} 
This paper reports the results of a series of field experiments
designed to investigate how peer effects operate in a real work
setting. Workers were hired from an online labor market to perform an
image-labeling task and, in some cases, to evaluate the work product
of other workers. These evaluations had financial consequences for
both the evaluating worker and the evaluated worker. The experiments
showed that on average, evaluating high-output work raised an
evaluator's subsequent productivity, with larger effects for
evaluators that are themselves highly productive. The content of the
subject evaluations themselves suggest one mechanism for peer effects:
workers readily punished other workers whose work product exhibited
low output/effort. However, non-compliance with employer expectations
did not, by itself, trigger punishment: workers would not punish
non-complying workers so long as the evaluated worker still exhibited
high effort. A worker's willingness to punish was strongly correlated
with their own productivity, yet this relationship was not the result
of innate differences---productivity-reducing manipulations also
resulted in reduced punishment. Peer effects proved hard to stamp out:
although most workers complied with clearly communicated maximum
expectations for output, some workers still raised their production
beyond the output ceiling after evaluating highly productive yet
non-complying work products.
\newline

JEL J01, J24, J3
\end{abstract}



\newcommand{\imgW}{.65}
\newcommand{\imgWs}{.3}
\setkeys{Gin}{width=\imgW \textwidth}

\section{Introduction} 
A perennial question of interest to both economists and firm managers
alike is why employees work hard despite facing weak incentives and
light monitoring. Many theories have been proposed: firms might obtain
high effort via explicit contracts \citep{holmstrom1982moral},
relational contracts \citep{levin2003relational} or efficiency wages
\citep{katz1986efficiency}. In each of these theories, the explanation
hinges upon the relationship between the firm and the individual
worker---a worker's co-workers or ``peers,'' if they matter at all,
are relevant only to the extent that they influence the incentives
offered by the firm (e.g., by influencing payoffs in a relative
performance scheme). However, recent empirical research has
highlighted the direct effects that co-workers can have on each others
productivity without intermediation by the firm.\footnote{See, for
  example, \cite{bandiera2010social}, \cite{mas2009peers},
  \cite{falk2006clean}, and \cite{guryan2009peer}.}  There are several
potential channels through which these workplace peer effects could
flow: peers could offer instruction about how to be more productive,
threaten punishment, promise rewards, offer examples of relevant norms
or spur competition. The purpose of this paper is to help clarify
these potential mechanisms through experimentation in a real work
setting.

\subsection{Overview of experiments and findings} 
This paper reports the results of five closely related field
experiments designed to explore the relationships among a worker's
peers, the policies and statements of the firm and a worker's
productivity. Table \ref{overview} provides an overview of each
experimental design and the results. In each experiment, workers from
an online labor market were hired to produce descriptive labels for
photographic images.\footnote{For example, a photograph of a breakfast
  scene might generate the labels ``juice, toast, cereal.''} In each
experiment, all subjects labeled the exact same image which makes
output comparisons across experimental groups meaningful. Before
joining the experiment, would-be workers read a description of the
task, learned the payment and viewed a work sample (i.e., a screen
shot of the image-labeling interface with some number of labels
completed).  If they chose to participate, they labeled one or more
images, depending on the details of the experiment. A worker's output
was simply the number of labels they produced. Because subjects were
not informed they were participating in experiments, the experiments
were ``natural'' field experiments in the \cite{harrison2004field}
taxonomy.

In Experiment A, subjects were randomly assigned to view either a
high-output work sample (with many labels for an image) or a
low-output work sample (with only a few labels for that same image).
All subjects then performed an image-labeling task. All subjects
labeled the same image, making cross-group output comparisons
meaningful. Exposure to the high-output work sample lowered labor
supply on the extensive margin but raised it on the intensive
margin. These two results are important for follow-on experiments,
because they imply that (1) subjects regarded effort as costly and (2)
subjects held the work sample as informative about employer
expectations.

In Experiment B, all subjects viewed the same low-output work sample
from Experiment A and then completed an image-labeling task.  After
completing this task, subjects evaluated the work of another
worker. The evaluated work displayed either high- or low-output, and
subjects were randomly assigned to the work they evaluated. Evaluation
had two parts: each subject was asked to recommend whether or not the
firm should ``approve'' the evaluated work as well as how to split a
bonus with the evaluated worker. The bonus split question created a
contextualized dictator game. The ``approve'' question has a technical
and consequential meaning in the market---when work is not approved,
the worker submitting that work does not get paid and their reputation
suffers.\footnote{A worker's reputation in this market is simply the
  percentage of their submissions that get approved. Buyers can put
  approval percentage screening criteria on their tasks, e.g., only
  allow workers with a 95\% approval rate to complete this task.}
Evaluating low-output work led to greater punishment: subjects viewing
low-output work were far more likely to recommend rejection and
granted smaller bonuses. Regardless of group assignment, highly
productive subjects (as measured by their output on the initial task)
were harsher judges; compared to their low-productivity peers, they
were more likely to recommend rejection and they granted smaller
bonuses.

All subjects in Experiment C were first shown a low productivity work
sample. Subjects then performed an image-labeling task. After
completing the task, subjects next evaluated the work product of
another worker. Unlike in Experiment B, all subjects in C evaluated
the same work. The only experimental manipulation in Experiment C
occurred during the image-labeling phase: subjects were randomly
assigned to either a normal image-labeling interface or to a special
image-labeling interface that generated a pop-up notice after subjects
added their second label. This pop-up notice was designed to modify
subjects' beliefs about employer expectations. The purpose of the
notice was to reduce output without inducing a change in extensive
labor supply.\footnote{Because the pop-up notice appeared after a
  subject had already decided to provide a positive amount of labor,
  it had no effect on the extensive margin.} By changing output
without changing the composition of subjects (i.e., no supply effects
on the extensive margin), it was possible to test the ``innate types''
explanation for the strong relationship between productivity and
punishment found in Experiment B. In this experiment, I found no
evidence that highly productive workers are simply more punishment
prone: workers receiving the pop-up notice reduced their output,
decreased their rejection recommendations and increased their bonuses.

In Experiment D, all subjects were first shown a low productivity work
sample. Next, they performed an initial image labeling task and then
were randomly assigned to evaluate either high- or low-output
work. After this evaluating, subjects performed an additional
image-labeling task. On average, workers that evaluated highly
productive work produced more labels in the follow-on image-labeling
task than workers that evaluated less productive work. These effects
on productivity were strong and easily detectable, but they were not
homogeneous: less productive workers were far less susceptible to the
effects, contra to some findings that peer effects raise the output of
low productivity workers \citep{falk2006clean,mas2009peers}.

In Experiment E all subjects were shown a work sample with exactly 
labels and were told that they should produce only  labels. After
performing an initial image-labeling task, subjects evaluated work
that contained either  or  labels. Workers did not treat the
high-effort but non-complying work as worthy of punishment: subjects
were just as likely to recommend approval of the  label work and
granted slightly larger bonus payments. Despite the explicit
statements of employer expectations, exposure to the
high-output/non-complying work had the same effect as exposure to
high-output work in Experiment D: exposed workers raised output, in
many cases beyond the clearly communicated ceiling. However, most
workers complied with the standard initially, and exposure to
complying work seemed to further increase compliance.

\subsection{Implications} 
One explanation for the results across the five experiments is that
workers were uncertain about what constituted an appropriate amount of
output and they use observations from employer-provided work samples
and the output of peers to determine that amount. Because workers find
labeling costly, these beliefs about employer expectations serve as a
constraint in the implicit optimization problem faced by workers.

Learning about employer expectations results in changes not only in a
worker's labor supply, but also in their willingness to punish or
reward their peers. As this learning can come from multiple sources,
perceived employer expectations do not fall wholly under any single
entity's control and can evolve as workers work, observe, and are
observed.

Punishment seems to come easily to many workers. The reasons why
workers punish is unclear, but there are several possible theoretical
explanations. Perhaps the simplest is that workers view themselves as
a monitored agent of the employer, and they make decisions about
acceptance and bonuses according to what they believe will satisfy the
principal. However, workers do not appear to be general-purpose
enforcers of employers' requests---workers punish low
effort. Non-complying but high effort work is treated no differently
vis-\`{a}-vis punishment than complying work. The fact that workers
only appear willing to punish low effort places a constraint on how
firms can make use of worker-driven norm enforcement. For example, it
may be difficult to get workers to substitute easy, correct procedures
for difficult, inefficient procedures. Ironically, the difficulty
itself might make an outdated procedure harder to replace, as workers
who adopt the easier method might be perceived to be shirking.

The finding that exposure to low-output work lowers output, combined
with the finding that low-productivity reduces willingness to punish,
suggests the possibility of an organizational vicious cycle: after
observing idiosyncratically bad work, workers may lower their own
output and punish less in response, in turn reducing other workers'
incentives to be highly productive. This may explain why
organizational leaders often use the language of contagion to describe
morale and so much of management theory focuses on understanding and
influencing organizational culture \citep{schein2004organizational},
rather than, for example, trying to write perfectly complete
employment contracts.

\subsection{Related work} 
Several recent papers examine the effects of peers on workplace
productivity. Perhaps the most illuminating observational evidence
comes from Mas and Moretti (\citeyear{mas2009peers}), who showed that
less productive grocery clerks exhibited greater productivity when
working near highly productive clerks, but only when they were in the
direct view of the highly productive clerks. This finding suggests
that the threat of punishment might partially explain workplace peer
effects. There is much laboratory literature supporting this
punishment-as-peer-effect view, with several studies showing that
workers will readily bear costs and altruistically punish peers that
free-ride in public goods games \citep{fehr2002altruistic,
  fehr2000cooperation}. This ``strong reciprocity''
\citep{carpenter2009strong} is a powerful peer effect, and although
firms are not perfectly analogous to public goods games, the notion of
worker-enforced productivity norms offers a very general potential
solution to the incentive problem of team production.

Guryan, et al. (\citeyear{guryan2009peer}) also use evidence from a
real workplace, albeit an unusual one: they exploited the random
assignment of professional golfers to tournament foursomes to estimate
the effects of each player's peers on the player's own performance. In
contrast to Mas and Moretti, Guryan, et al. found no evidence of peer
effects, providing a useful corrective to hasty or overly broad
generalizations. However, professional golf tournaments are unusual
work environments, in that two common channels for peer effects are
foreclosed: professional golfers know what constitutes good
performance and are unlikely to raise their quality of play in the
``shadow'' of punishment that might be meted out for non-compliance
with productivity norms. In marked contrast with the Mas and Moretti
setting, shirking by a professional golfer imposes a positive
externality on ``co-workers.''

Using a field experiment, Falk and Ichino (\citeyear{falk2006clean})
showed that workers stuffing envelopes in pairs had less variation in
their output levels than synthetically ``paired'' workers constructed
from an experimental group whose members worked alone. While they
cannot estimate the direction of peer effects (i.e., low productivity
affects or is affected by high productivity, or some amalgamation of
effects), their analysis of the output distribution led them to
conclude that it was more likely that less productive workers were
made more productive by working in pairs.

The difference between the Guryan, et al. setting, the Falk and Ichino
setting and that of Mas and Moretti---and the resultant difference in
findings---serves as a justification for the present study, which has
an unusual but highly controllable work context that preserves some of
the common features of work environments, including uncertainty about
norms, costly effort and a task unlikely to inspire much intrinsic
motivation. Unlike the Mas and Moretti setting, however, there are no
overt free-riding externalities (in the check-out line, slacking by
one clerk increases the work load of other clerks). The absence of
direct negative externalities is important, as evidence from such a
setting can provide some sense of how general punishment might operate
in the workplace.  

\subsection{Contribution} 
This paper contributes to the emerging literature on workplace peer
effects. It provides credible evidence of the existence and operation
of peer effects on productivity, which is especially useful given the
lack of concordance between some of the major results in the field and
the inherent difficulty of estimating these kinds of effects
\citep{manski1993}. This evidence is particularly useful because the
scope of possible interpretations is limited, due to the narrow
channel through which peer effects could operate. Observation of work
output was the only ``interaction'' and the payment scheme was not
relative. The punishment component provides additional insight into
the shadow cast by peer-based norm enforcement.

One methodological advance of this paper is that productivity is
measured both before and after exposure to peers. By base-lining prior
output, the conditional nature of peer effects becomes apparent.  For
example, I find that the conditional treatment effects of exposure to
low quality peers differ from the effects found by Falk and Ichino.
They found that ``bad apples far from damaging good apples seem
instead to gain in quality when paired with the latter.''  Mas and
Moretti find a similar result. In the setting examined here, the
traditional bad apples metaphor applied---the bad apples ruined the
good apples, and the good apples did nothing for the bad.


\section{Methods and Materials} 
Before describing the experiment results, I first describe the
marketplace where the experiments were conducted, the methodological
issues involved in online experimentation and the actual task
completed by workers and interface used. The experiments were
conducted on Amazon's Mechanical Turk (MTurk), an online labor market
where workers are available to complete small tasks for
payment. Background information on MTurk closely follows
\cite{horton2010labor}. MTurk is one of several online labor markets
that have emerged in recent years \citep{frei2009}. At present, it is
the most amenable to online experimentation.
\begin{table}
\caption{Details of the Experiments \label{overview}}
\begin{center}
\begin{tiny}
    \begin{tabular}{ l | p{3cm} | p{5cm} | p{2cm} | p{2cm} | p{3cm} }
     \small{Exp}. 
     & \small{Question} 
     & \small{Set-up} 
     & \small{Treatment} 
     & \small{Control} 
     & \small{Result} \\ 
    \hline A 
      & Can employers convey productivity expectations? 
      & Subjects viewed an employer-provided work sample, 
        then chose how many labels to produce (if any). Work 
        samples differed by experimental group. 
      & HIGH:Subjects viewed high-output work sample (many labels)
      & LOW: Subjects viewed low-output work sample (few labels)
      & HIGH increased labor supply on intensive margin, 
        but decreased it on extensive margin \\
    \hline  B 
      & Do workers punish workers that exhibit low productivity? 
      & Subjects viewed an employer-provided work sample, 
        then chose how many labels to produce. Subjects then 
        evaluated another worker's work product.  
      & GOOD: Subjects evaluated a high-output work sample
      & BAD:  Subjects evaluated a low-output work sample
      & GOOD increased approval recommendations and bonus amounts. 
        Highly productive workers punished more with their evaluations. \\
     \hline C
      & Is the relationship between own-productivity and punishment causal?
      & Subjects viewed an employer-provided work sample, 
        then chose how many labels to produce. Subjects then 
        evaluated another worker's work product.       
      & CURB: Subjects received a notice after two labels 
        saying that 3 labels was probably enough output
      & NONE: Subjects received no notice. 
      & Greatly reduced output in CURB;
        those in CURB more likely to recommend approval 
        and grant larger bonuses \\
      \hline D
      & Does exposure to low-output work affect a worker's productivity?
      & Subjects viewed an employer-provided work sample, 
        then chose how many labels to produce. Subjects then 
        evaluated another worker's work product. Then they labeled
        a second image. 
      & GOOD: Subjects evaluated a high-output work sample
      & BAD:  Subjects evaluated a low-output work sample
      & GOOD raised output on second task;
        effects were stronger for more productive 
        subjects (measured by first task output). \\
      \hline E
      & Are workers susceptible to peer effects 
        in presence of strongly-stated employer expectations? 
        Do they punish high-effort but non-complying work? 
      & Subjects viewed an employer-provided work sample with
        2 labels, then chose how many labels to produce. Subjects
        were told that 2 and only 2 labels should be produced. 
        Subjects then evaluated another worker's work product, 
        then labeled a second image.  
      & OVER: Subjects evaluated a worker producing too many labels
      & OK: Subjects evaluated a worker producing the required 
        number of labels 
      & OVER increased subsequent output beyond ceiling, but did not cause
        more punishment.  
\end{tabular}
\end{tiny}
\end{center}
\end{table}

\subsection{Online experimentation}
In the past few years, researchers in a number of disciplines---with
computer science leading the way---have begun running experiments
online using online labor markets.\footnote{For an overview of online
  labor markets, see \cite{hortonOLM2010}.} Some examples in economics
include \cite{mason2009fip}, \cite{chandler2010} and
\cite{horton2010labor}. Horton, Rand and Zeckhauser
(\citeyear{hortonZeck2010}) argue that online experiments can offer a
high degree of both internal and external validity. Despite their
advantages online experiments can also be harder to control compared
to conventional laboratory experiments. However, they are generally
easier to control than conventional field experiments. Because
subjects may quit at any time, the biggest threat to valid inference
is non-random attrition. In Experiment A, quitting was actually a
useful outcome to observe, as the experiment focused on labor supply
on both the intensive and extensive margins. In the other four
experiments, by design, essentially all attrition occurred before
subjects experienced any treatment-specific differences. For
Experiments B-E, only subjects that completed the initial
image-labeling task were included in the sample (with others dropped),
but this creates no sampling bias, since all subjects made their
initial output decisions before being exposed to any experimental
group-specific treatments.

\subsection{Amazon's Mechanical Turk} 
Amazon's Mechanical Turk is an online labor market where workers are
available to perform small jobs called ``Human Intelligence Tasks''
(HITs) for buyers, who, in the parlance of MTurk, are called
``requesters.''  HITs vary, but most are small, simple tasks that are
difficult for computers but relatively easy for humans to
perform. Common tasks include transcribing audio clips, classifying
and tagging images, reviewing documents and checking websites for
pornographic content. When posting a HIT, a requester describes the
task, creates a user interface, establishes a piece-rate payment,
specifies worker qualifications, and sets the number of times each HIT
may be performed.

In order to become an MTurk worker, a person must create an MTurk
account and provide a bank account number to Amazon. Workers are only
allowed to have one account, and Amazon uses several technical and
legal means to enforce this restriction. Once they are members,
workers are able to observe the collection of HITs available to them
and, in most cases, view a sample of the required work.  They can work
on any task for which they are qualified and can begin work
immediately after accepting a HIT.

Once a worker completes a HIT, the work product is submitted to the
requester for review. The requester decides whether or not to
``approve'' it. If approved, the worker is paid the piece rate. The
worker is also paid if the requester does not review and approve the
work within a specified amount of time. Solely at their discretion,
requesters may ``reject'' work, in which case the worker is not
paid. The ability of requesters to reject work creates consequences
for providing work that does not meet employer expectations---a
feature critical to the experiments conducted. Requesters may also
elect to pay bonuses, which makes it easy to tailor payments to
individual workers based on their performance within a nominally
piece-rate HIT.

MTurk workers appear to be split approximately evenly between the US
and India. Most report that they participate to earn money and
generally view employers online as having the same level of
trustworthiness as offline, traditional employers
\citep{horton2010condition}. For the demographics of the MTurk
population, see \cite{ipeirotis2010}.

\subsection{Task and interface}
In each of the experiments, subjects were asked to label images. The
images themselves were selected from the photo sharing website
Flickr.\footnote{The images each had a Creative Commons license and
  were chosen because they were conducive to labeling (e.g., photos
  depicting elaborate meals with many easily recognizable food
  types).} Image-labeling is a very common ``human computation'' task
because labels are needed to make images searchable, but computers do
a poor job of identifying objects in images \citep{von2004labeling,
  huang2010toward}. The interface itself was created in Limesurvey, an
open-source survey platform.\footnote{The interface for adding labels
  was written in JavaScript as an add-on module to Limesurvey.} In
order to add labels, subjects had to click a button labeled ``Add a
label.''  A screen shot of the interface can be seen in Figure
\ref{fig:work.samples}.  Clicking the button caused a new blank text
field to be added to the survey. When they finished adding labels
subjects clicked a button labeled ``Submit labels,'' which saved
within the survey all of the labels generated and the time spent
adding labels. No attempt was made to adjudicate the quality of the
labels---if the subject started to add another label, this was
recorded as an additional unit of output.

For the evaluation task, each subject viewed a screen shot of another
worker's work product. As with the initial image-labeling task, the
evaluation task (and the potential bonus) was likely perceived as
unextraordinary. On MTurk, it is very common to have workers evaluate
the work of other MTurk workers. A frequently used solution to the
problem of spam submissions (which occur often when buyers post many
tasks---see \cite{ipeirotis2010quality}) is to have workers vote on
the work product of other workers. It is also very common to use
bonuses to motivate performance.

It is important to note that all the subjects in an experiment labeled
the same image, regardless of their group assignment. For example, in
Experiment A, all subjects labeled an image showing a collection of
mechanics tools. What varied across experimental groups were factors
like the provided work sample, work instructions or the demonstrated
productivity of the worker they were asked to evaluate. Because the
images were the same across groups, output levels are comparable, and
differences in output can be attributed causally to whatever factor
was manipulated in that experiment.

\subsection{Demographic survey}
In each of the five experiments, subjects answered a short demographic
survey before beginning work. The survey was identical in each
experiment. Subjects were asked to report their gender, country
(choices were US, India or some other country) and whether they use
MTurk primarily in order to make money, learn new skills or have
fun. There were small differences in the reported covariates across
experiments, with most of the difference probably driven by
differences in when experiments were launched. 

Although one might think the survey would rise suspicions that the
task was an experiment, I view this as unlikely. Asking workers for basic
demographic information is fairly common in the market, as requesters
frequently use location and formal qualifications to screen workers. In
fact, place- and reputation-based screening is built into the system,
making it even more likely that workers view the demographic questions
as an employer work-around to further expand the ability to screen or
algorithmically adjudicate response quality. With some noted
exceptions, the demographic information had little predictive power
and only marginally improved the precision in the regressions, so they
were not included.




\section{Experiment A: Perceived employer expectations} 
Experiment A investigates whether a ``firm'' in this market can
influence employee expectations about productivity. For the image
labeling task, a simple way to convey expectations is to show a work
sample. In this experiment, workers were assigned to one of two
experimental groups: , in which the work sample showed 
labels, and , in which the work sample showed 
labels.\footnote{Throughout the paper, the experimental group names
  will be treated as indicator variables, i.e.,  is synonymous
  with worker  being assigned to group .}  The work samples
are shown in Figure \ref{fig:work.samples}.  After subjects viewed
their assigned work sample, they chose to either label an image or
exit the experiment, forfeiting payment.



Table \ref{tab:ExpA.ss} shows the summary statistics for the
experiment. The job posting explained that workers would be asked to
do a simple image-labeling task and would be paid 30 cents. The
planned sample size was 100. Subjects not completing the demographic
survey (which occurred prior to group assignment) were dropped from
the sample. In this experiment and all subsequent experiments,
subjects were assigned to groups by stratifying on arrival time (e.g.,
subject 1 was assigned to , subject 2 to , subject 3 to
 and so on).

\setkeys{Gin}{width=\imgWs \textwidth}
\begin{figure}
  \centering 
   \subfloat[][ ()]{  \includegraphics[scale=.25]{breakfast_sample_high}}
   \subfloat[][ ()]{  \includegraphics[scale=.25]{breakfast_sample_low}}
  \caption{Work samples shown to workers prior to task acceptance in
    Experiment A.\label{fig:work.samples}}
\end{figure} 
\setkeys{Gin}{width=\imgW\textwidth}


\begin{table}[h!]
  \begin{center} 
   \caption{Experiment A summary statistics () 
     \label{tab:ExpA.ss}}
\begin{tabular}{lcccccc}
  \toprule 
  \underline{Administrative} \\
    \multicolumn{4}{l}{\hspace{10pt} Launch: Fri Apr 09 21:10:31 GMT 2010} \\
    \multicolumn{4}{l}{\hspace{10pt} Finish:  Sun Apr 11 10:04:40 GMT 2010} \5pt]
  \underline{Treatment Assignment} \\ 
  \hspace{10pt}                 & 47 & 46 & 49.5  \
  1\{y > 0\}  = \underbrace{-0.177}_{[0.085]}\cdot HIGH +  
\underbrace{0.872}_{[0.050]}
y = \underbrace{1.970}_{[0.956]}\cdot HIGH +  
\underbrace{2.638}_{[0.430]} 5pt]
  \underline{Survey}    & \underline{FALSE} & \underline{TRUE} & \underline{\% TRUE}   \\
  \hspace{10pt} male                  & 70 & 97 & 58.1  \\
  \hspace{10pt} from India                 & 92 & 75 & 44.9  \\
  \hspace{10pt} from US                    & 118 & 49 & 29.3  \\
  \hspace{10pt} motivated by money                 & 41 & 126 & 75.4  \5pt]
 
 \underline{Recommended firm approve work}   \\
  \hspace{10pt} in GOOD           & 7 & 78 & 91.8\\
  \hspace{10pt} in BAD           & 41 & 41 & 50\
  approve = \underbrace{0.418}_{[0.064]}\cdot GOOD + 
\underbrace{0.500}_{[0.056]}

  bonus =  \underbrace{1.442}_{[0.393]}\cdot GOOD + 
  \underbrace{3.488}_{[0.298]} 
bonus = \underbrace{-0.259}_{[0.066]} \cdot y + 
  \underbrace{5.376}_{[0.365]} 
  approve = \underbrace{-0.055}_{[0.011]} \cdot y + 
  \underbrace{0.960}_{[0.049]} 5pt]
  \underline{Survey}    & \underline{FALSE} & \underline{TRUE} & \underline{\% TRUE}   \\
  \hspace{10pt} male                  & 117 & 156 & 57.1  \\
  \hspace{10pt} from India                 & 175 & 98 & 35.9  \\
  \hspace{10pt} from US                    & 162 & 111 & 40.7  \\
  \hspace{10pt} motivated by money                 & 80 & 193 & 70.7  \5pt]
 
 \underline{Recommended we approve work?}   \\
  \hspace{10pt} in CURB           & 46 & 94 & 67.1\\
  \hspace{10pt} in NONE          & 57 & 76 & 57.1\
  bonus = \underbrace{-0.207}_{[0.039]} \cdot y +
  \underbrace{4.418}_{[0.223]} 
  y = \underbrace{-3.172}_{[0.490]}\cdot CURB 
  +  \underbrace{6.150}_{[0.470]} 

  bonus = \underbrace{-0.251}_{[0.090]} \cdot y +
  \underbrace{4.619}_{[0.433]} 
  approve = \underbrace{-0.042}_{[0.005]}\cdot y + 
\underbrace{0.811}_{[0.037]}

  approve = \underbrace{-0.032}_{[0.017]}\cdot y + 
\underbrace{0.765}_{[0.083]}
5pt]   
  \underline{Survey}    & \underline{FALSE} & \underline{TRUE} & \underline{\% TRUE}   \\
  \hspace{10pt} male                  & 131 & 144 & 52.4  \\
  \hspace{10pt} from India                 & 177 & 98 & 35.6  \\
  \hspace{10pt} from US                    & 154 & 121 & 44  \\
  \hspace{10pt} motivated by money                 & 76 & 199 & 72.4  \5pt]
  
  \underline{Labels produced} &  \underline{Min} & \underline{.25} & \underline{Med.} & \underline{Mean} & \underline{.75} & \underline{Max}\\ 
  Initial output, before evaluation () \\
  \hspace{10pt} in GOOD     & 1 & 2 & 5 & 4.759 & 7 & 14  \\
  \hspace{10pt} in BAD      & 1 & 2 & 5 & 4.768 & 7 & 15  \\
  Follow-on output, after evaluation () \\
  \hspace{10pt} in GOOD           & 0 & 4 & 7 & 7.368 & 10 & 23\\
  \hspace{10pt} in BAD            & 0 & 1.25 & 5 & 5.014 & 7 & 16\\
  
  \bottomrule 
  \end{tabular}
\end{center} 
\emph{Notes:} Overlap in subjects across experiments was , . Subjects in 
evaluated high-output work, while subjects in  evaluated
low-output work. The key finding from this experiment was the effect
exposure had on subsequent output. We can see that there were no
differences in output means pre-exposure (``Initial output, before
evaluation'' rows) but a large difference after evaluation
(``Follow-on output, after evaluation'' rows).
\end{table} 



\subsection{Results} 
Exposure to the work of a peer strongly affected a subject's
subsequent output. Output following exposure to highly productive
peers was higher than output following exposure to less productive
peers.  The treatment effect is heterogeneous across productivity
distribution for the first task: more productive workers are more
strongly affected by the exposure to peers.

Most of the results can be readily seen in Figure
\ref{fig:ExpD.output}, which shows scatter plots of final output,
, against initial output, . Observations from  are
represented in the plot by a ``+'' symbol and those in  by a
``o'' symbol. The two panels contain the scatter plot and regression
line for the respective experimental group, as well as the points from
the other group, lightly plotted.  Because only integer-level outputs
were possible, all points are randomly perturbed to prevent
over-plotting. In the figure, the line for  is both above and
steeper than the regression line for , indicating non-constant
effects.


\subsubsection{Exposure to highly productive peers increased productivity}
Subjects that evaluated highly productive work produced considerably more
labels in the follow-on task: 

with  and . In
addition to the treatment effect, we can see that initial output was
highly correlated with subsequent output. The effect of  was
not, however, constant across the initial output distribution:
 
with  and ; as  increases, the
positive effect of assignment to  on output grows larger.

\begin{figure} 
\centering 
\includegraphics{peer_effects-024}
\caption{Subsequent output () versus initial output (), by
  treatment group. All subjects did an identical initial task and
  chose some number of labels to provide (shown on the
  x-axis). Subjects then evaluated another subject's work that
  demonstrated either low productivity (, left panel) or high
  productivity (, right panel). All points from each group are
  shown in each panel (as well as the regression line), but the points
  and lines are either black or gray depending on whether they came
  from the experiment group shown in the panel. All output levels are
  randomly perturbed by  to prevent
  over-plotting (which would occur because only integer output levels
  were possible).
   \label{fig:ExpD.output}}
\end{figure} 



\subsection{Discussion}
The peer effects detected in Experiment D strongly depend upon a
worker's initial output. There is no immediately obvious reason why
this should be the case, however, studies from other domains have also
found that different types of workers respond differently to
peers. One possible explanation for the pattern is that initially less
productive workers might already have strong beliefs that low
productivity is acceptable. Recall that subjects were exposed to the
 work sample from Experiment A prior to accepting the task, and
yet still chose to produce only  or  labels. Although being in
 and observing highly productive work might still have some
effect on their beliefs, these less productive workers might have
fairly stiff priors regarding what constitutes acceptable work.

Although Experiment D demonstrated that exposure to peer output
affects a worker's own output, it did not explain \emph{why} workers
are influenced by peers.  There are several possible explanations for
why peer effects exist in this setting including fear of punishment,
learning about relevant employer standards and perhaps even an innate
desire to match the performance of peers, regardless of the direct
material payoff.

\section{Experiment E: Peer effects after explicit employer instructions} 
Experiment D demonstrated the existence of productivity peer
effects---even with the minimal ``interaction'' created by evaluation,
yet it did not explain why workers are affected by peers. If peer
effects reflect learning about employer standards, then clear,
strongly stated production standards should ``inoculate'' workers
from learning-driven peer effects. However, if workers fear punishment
by fellow workers or if they have some innate desire to produce as
much as peers, then we should detect peer effects even when standards
are clearly communicated.

In Experiment E, these ideas were tested by providing subjects with
very explicit instructions about productivity expectations and then
exposing subjects to peers. The set-up was almost identical to that of
Experiment D, except that workers were told that they should produce 2
and only 2 labels per image. The requirement of 2 labels was stated
before workers began the task, and was repeated again with each of the
two image-labeling tasks, directly above the image. After performing
the initial task, workers were assigned to one of two groups: , in
which subjects evaluated a work sample showing , and , in
which subjects evaluated a work sample showing . After
evaluating the work, subjects performed an additional image-labeling
task. Table \ref{tab:ExpE.ss} shows the summary statistics for
the experiment. The requested sample size was  and the payment
was 40 cents.




\begin{figure} 
\centering 
\includegraphics{peer_effects-028}
\caption{Mosaic plots showing the relationship between output in the
  initial task () to output in the follow-on task (). In
  Experiment E, all subjects were told to produce  and only 
  labels. Subjects in  then evaluated work showing  labels
  (hence complying with employer expectations), while subjects in
   evaluated work with  labels. In both plots, the unit
  square is split vertically in thirds, in proportion to the number of
  subjects that selected ,  and ,
  respectively. Both plots are also split horizontally, in proportion
  to the number of subjects choosing ,  and ,
  respectively.  The  block in  is taller than
  the corresponding block in , indicating that exposure to the
  complying work sample (in ) made initially complying workers
  (i.e., those choosing ) more likely to comply in the follow
  on task (i.e., choosing ).
  \label{fig:ExpE.trans}}
\end{figure} 

 
 \begin{table}[h!]
  \begin{center}
   \caption{Experiment E summary statistics ()\label{tab:ExpE.ss}}
\begin{tabular}{lcccccc}
  \toprule 
 \underline{Administrative} \\
  \multicolumn{4}{l}{\hspace{10pt} Launch:  Wed May 19 21:19:42 GMT 2010 } \\
  \multicolumn{4}{l}{\hspace{10pt} Finish:   Sun May 23 15:26:30 GMT 2010} \5pt]
  \underline{Treatment Assignment} \\ 
  \hspace{10pt}  & 141 & 131 & 48.2 \
  1\{y_2=2\} = \underbrace{-0.228}_{[0.059]}\cdot INDIA + 
\underbrace{0.691}_{[0.040]}
 
  1\{y_2=2\} = \underbrace{0.161}_{[0.059]}\cdot OK + 
\underbrace{0.504}_{[0.042]}1\{y_2=2\} = \underbrace{0.022}_{[0.083]}\cdot OK + 
\underbrace{0.205}_{[0.102]}\cdot \left[OK\times 
1\{y_1=2\} \right] +
\underbrace{0.467}_{[0.076]}\cdot
1\{y_1=2\} +
\underbrace{0.242}_{[0.055]} \label{eq:E.approval}
  approve = \underbrace{-0.002}_{[0.041]}\cdot OK + 
\underbrace{0.872}_{[0.028]}
  bonus =
  \underbrace{-0.397}_{[0.263]}\cdot OK +
  \underbrace{5.305}_{[0.182]} bonus = \underbrace{0.720}_{[0.502]}\cdot OK + 
 \underbrace{0.020}_{[0.152]}\cdot y_1 + 
 \underbrace{-0.547}_{[0.204]} \cdot y_1 \times OK + 
\underbrace{5.264}_{[0.383]} y_2 = \underbrace{0.658}_{[0.120]} \cdot y_1 + 
 \underbrace{-0.659}_{[0.183]} \cdot OK + 
  \underbrace{1.294}_{[0.299]}  
 
with  and . However, unlike
in Experiment D, this effect was not conditional upon initial
output. When the regression above is augmented with an 
interaction term (not shown), the coefficient on the interaction is
small and insignificant, which is consistent with there being little
variation in initial output (i.e., the distribution of  is
heavily concentrated at ).

\subsection{Discussion} 
It is clear that many workers take explicit requests from employers as
informative and worth complying with. Yet a substantial number of
workers remained susceptible to peer effects that could, in principle,
have led them to have their work rejected for not following the letter
of the instructions. There are several possible explanations.

One possibility is mistake. Workers might believe that they
misinterpreted the instructions or that other workers have some inside
knowledge. Workers might reasonably believe that we had free disposal
of extra labels and added a few more to provide a margin of safety.
However, the requirement was clearly stated at least three times to
workers, and many initially complying workers were still pulled upward
by highly productive peers.

Another possibility is that workers want to produce an amount
comparable to that of their peers, regardless of employer
instructions. Given the propensity of peers to punish low effort,
matching the output of one's peers is a good idea if there is a chance
one will be evaluated by those peers. Because subjects are asked to
evaluate workers, it seems likely that they infer they will also be
evaluated by other workers. Given that other workers are likely to
reward or punish based on apparent effort, not necessarily on
compliance with the stated standards, above-standard output could be
rational even if it is technically non-complying.

\section{Conclusion} 
This paper reports a number of results on punishment, productivity and
peer effects: (1) workers readily punish low-effort work; (2) workers
are susceptible to peer effects, but the effects are conditional upon
a worker's productivity; (3) a worker's willing to punish is mediated
by their own productivity, which is in turn malleable; (4) workers
punish low effort, not failure to comply with an employer's
instructions. Some of these findings contradict or at least complicate
results from other workplace settings. Future research should
investigate the generalizability of these findings and which should
clarify which findings are general features of how humans think about
work and which are context-specific.

If strong reciprocity is a general feature of human organizations,
then a natural question for managers is whether they should encourage
this phenomenon among workers. Here, context seems to matter
greatly. Giving workers thicker sticks or juicier carrots to use on
their peers may backfire if workers are enforcing norms contrary to
the best interests of the firm. Further, other research has shown that
workers will enforce norms that are directly counter productive (e.g.,
Roy's \citeyear{roy1952quota} work on machine shops). Creating the
tools for norm enforcement is risky when knowledge of what will be
enforced is murky.

A key theme of these experiments is the apparent pliability of
productivity, which highlights the danger of what might be called
``organizational alchemy,'' i.e., the attempt to harness the power of
peer effects without knowing how they work in the relevant
context. For example, in contrast to several other findings, ``bad''
workers did not improve after evaluating good work.  At least in the
context examined, it would be a mistake to mix workers of different
abilities with the hope that the good would raise the bad.

\bibliographystyle{aer}
\bibliography{peer_effects.bib}

\end{document} 
