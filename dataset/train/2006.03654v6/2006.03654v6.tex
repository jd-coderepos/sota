\section{Experiment}
This section reports {\ModelName} results on various NLU tasks. 




\subsection{Main Results on NLU tasks}
\label{subsec:main}
Following previous studies of PLMs,
we report results using large and base models.
\subsubsection{Performance on Large Models}
\begin{table*}[htb!]
    \centering
    \begin{tabular}{@{\hskip3pt}l@{\hskip2pt}|@{\hskip2pt} c@{\hskip2pt}| @{\hskip2pt}c@{\hskip2pt}|c@{\hskip2pt}|c@{\hskip2pt}|c@{\hskip2pt}|c@{\hskip2pt}|c@{\hskip2pt}|c@{\hskip2pt}|c@{\hskip2pt}}
        \toprule
        \multirow{2}{*}{\bf Model} & {CoLA} &{QQP} &{MNLI-m/mm} &SST-2 &STS-B&QNLI&RTE&MRPC& Avg.\\ 
        & Mcc & Acc & Acc & Acc &Corr&Acc&Acc&Acc\\
        \midrule
        BERT &60.6 &91.3  & 86.6/- & 93.2 &90.0&92.3&70.4&88.0 &84.05 \\ \hline
        RoBERTa &68.0 & 92.2  & 90.2/90.2 & 96.4 &92.4&93.9&86.6&90.9& 88.82 \\ \hline
        XLNet& 69.0 & 92.3  & 90.8/90.8 & \textbf{97.0} &92.5&94.9&85.9&90.8 & 89.15 \\ \hline


ELECTRA\textsubscript{large}&69.1 & \textbf{92.4} &90.9/- &96.9& 92.6&95.0 & 88.0&90.8 &89.46 \\ \hline
        {\ModelName} &\textbf{70.5} & 92.3 & \textbf{91.1/91.1} &96.8 & \textbf{92.8} &\textbf{95.3}&\textbf{88.3}& \textbf{91.9} &\textbf{90.00}\\
        \bottomrule
        \end{tabular}
    \caption{
    Comparison results on the GLUE development set. 
    }
    \label{tab:glue}
    \vspace{-2mm}
\end{table*}






We pre-train our large models following the setting of BERT~\citep{devlin2018bert}, except that we use the BPE vocabulary of ~\cite{radford2019language,liu2019roberta}.
For training data, we use Wikipedia (English Wikipedia dump\footnote{https://dumps.wikimedia.org/enwiki/}; 12GB), BookCorpus~\citep{bookcorpus} (6GB), OPENWEBTEXT (public Reddit content~\citep{Gokaslan2019OpenWeb}; 38GB), and STORIES (a subset of CommonCrawl~\citep{trinh2018simple}; 31GB). 
The total data size after data deduplication~\citep{shoeybi2019megatron} is about 78G. 
Refer to Appendix A.2 for a detailed description of the pre-training dataset.

We use 6 DGX-2 machines (96 V100 GPUs) to train the models. 
A single model trained with 2K batch size and 1M steps takes about 20 days.
Refer to Appendix \ref{sec:appendix} for the detailed hyperparamters. 


We summarize the results on eight NLU tasks of GLUE~\citep{wang2018glue} in Table~\ref{tab:glue}, where DeBERTa is compared {\ModelName} with previous Transform-based PLMs of similar structures (i.e. 24 layers with hidden size of 1024) including BERT, RoBERTa, XLNet, ALBERT and ELECTRA.
Note that RoBERTa, XLNet and ELECTRA are pre-trained on 160G training data while {\ModelName} is pre-trained on 78G training data. RoBERTa and XLNet are pre-trained for 500K steps with 8K samples in a step, which amounts to four billion training samples. 
{\ModelName} is pre-trained for one million steps with 2K samples in each step.  
This amounts to two billion training samples, approximately half of either RoBERTa or XLNet.  
Table~\ref{tab:glue} shows that compared to BERT and RoBERTa, {\ModelName} performs consistently better across all the tasks. 
Meanwhile, {\ModelName} outperforms XLNet in six out of eight tasks. 
Particularly, the improvements on MRPC (1.1\% over XLNet and 1.0\% over RoBERTa), RTE (2.4\% over XLNet and 1.7\% over RoBERTa) and CoLA (1.5\% over XLNet and 2.5\% over RoBERTa) are significant. 
{\ModelName} also outperforms other SOTA PLMs, i.e., ELECTRA\textsubscript{large} and XLNet\textsubscript{large}, in terms of average GLUE score.  


Among all GLUE tasks, MNLI is most often used as an indicative task to monitor the research progress of PLMs. 
{\ModelName} significantly outperforms all existing PLMs of similar size on MNLI and creates a new state of the art. \begin{table*}[htb!]
    \centering
    \begin{tabular}{@{\hskip2pt}l@{\hskip3pt}|@{\hskip2pt} c@{\hskip2pt}|| @{\hskip2pt}c@{\hskip2pt}@{\hskip2pt}c@{\hskip2pt}|@{\hskip2pt}c@{\hskip2pt}|@{\hskip2pt}c@{\hskip2pt}||@{\hskip2pt}c@{\hskip2pt}||@{\hskip2pt}c@{\hskip2pt}}
        \toprule
        \multirow{2}{*}{\bf Model} &{MNLI-m/mm} & {SQuAD v1.1} &{SQuAD v2.0} &RACE &ReCoRD  &SWAG & NER\\ 
        & Acc & F1/EM & F1/EM &Acc&F1/EM& {Acc}   & F1\\
        \midrule
        BERT & 86.6/-& 90.9/84.1 &81.8/79.0  &72.0 &-   &86.6 &92.8\\ \hline
        ALBERT & 86.5/-& 91.8/85.2 & 84.9/81.8 &75.2 &-  &-&- \\ \hline
        RoBERTa & 90.2/90.2& 94.6/88.9 & 89.4/86.5 &83.2 &90.6/90.0   &89.9 &93.4\\ \hline
        XLNet & 90.8/90.8& 95.1/89.7 & 90.6/87.9 &85.4 &-  &-&- \\ \hline
        Megatron\textsubscript{336M} & 89.7/90.0 & 94.2/88.0 & 88.1/84.8 &83.0 &-   &-&- \\ \hline
        {\ModelName} & \textbf{91.1/91.1} &\textbf{95.5/90.1} & \textbf{90.7/88.0} & \textbf{86.8} &\textbf{91.4/91.0}   &\textbf{90.8} &\textbf{93.8}\\ \hline \hline
        ALBERT & 90.8/-& 94.8/89.3 & 90.2/87.4 &86.5 &-  &-&- \\ \hline
        Megatron\textsubscript{1.3B} & 90.9/91.0 & 94.9/89.1 & 90.2/87.1 & 87.3 &- &-&- \\ \hline
        Megatron\textsubscript{3.9B} & 91.4/91.4 & 95.5/90.0 & 91.2/88.5 &89.5&-  &-&- \\      
        \bottomrule
        \end{tabular}
    \caption{
    Results on MNLI in/out-domain,  SQuAD v1.1, SQuAD v2.0, RACE, ReCoRD, SWAG, CoNLL 2003 NER development set. Note that missing results in literature are signified by ``-''.}
    \label{tab:large}
    \vspace{-4mm}
\end{table*}



In addition to GLUE, DeBERTa is evaluated on three categories of NLU benchmarks: 
(1) Question Answering: SQuAD v1.1 \citep{squad1}, SQuAD v2.0 \citep{squad2}, RACE \citep{lai2017race}, ReCoRD \citep{zhang2018record} and SWAG \citep{zellers2018swag}; 
(2) Natural Language Inference: MNLI \citep{mnli2018}; and 
(3) NER: CoNLL-2003.
For comparison, we include 
ALBERT\textsubscript{xxlarge}~\citep{lan2019albert}
\footnote{ The hidden dimension of ALBERT\textsubscript{xxlarge} is 4 times of DeBERTa and the computation cost is about 4 times of DeBERTa.} and 
Megatron~\citep{shoeybi2019megatron} with three different model sizes, denoted as Megatron\textsubscript{336M}, Megatron\textsubscript{1.3B} and Megatron\textsubscript{3.9B}, respectively, which are trained using the same dataset as RoBERTa. 
Note that Megatron\textsubscript{336M} has a similar model size as other models mentioned above\footnote{T5~\citep{raffel2019t5} has more parameters (11B). \cite{raffel2019t5} only report the test results of T5 which are not comparable with other models.}. 

We summarize the results in Table~{\ref{tab:large}}. 
Compared to the previous SOTA PLMs with a similar model size (i.e., BERT, RoBERTa, XLNet,  ALBERT\textsubscript{large}, and Megatron\textsubscript{336M}),  {\ModelName} shows superior performance in all seven tasks. 
Taking the RACE benchmark as an example, {\ModelName} significantly outperforms XLNet by +1.4\% (86.8\% vs. 85.4\%). 
Although Megatron\textsubscript{1.3B} is three times larger than {\ModelName}, {\ModelName} outperforms it in three of the four benchmarks. We further report {\ModelName} on text generation tasks in Appendix~\ref{subsec:generation}.


\subsubsection{Performance on Base Models}
Our setting for base model pre-training is similar to that for large models. 
The base model structure follows that of the BERT base model, i.e., . 
We use 4 DGX-2 with 64 V100 GPUs to train the base model. 
It takes 10 days to finish a single pre-training of 1M training steps with batch size 2048. 
We train {\ModelName} using the same 78G dataset, and compare it to RoBERTa and XLNet trained on 160G text data. 


We summarize the base model results in Table~\ref{tab:base-dev}. 
Across all three tasks, {\ModelName} consistently outperforms RoBERTa and XLNet by a larger margin than that in large models.  
For example, on MNLI-m, {\ModelName\textsubscript{base}} obtains +1.2\%  (88.8\% vs. 87.6\%) over RoBERTa\textsubscript{base}, and +2\% (88.8\% vs. 86.8\%) over XLNet\textsubscript{base}.
\begin{table*}[htb!]
    \centering
    \begin{tabular}{@{\hskip2pt}l| c | c | c @{\hskip2pt}}
        \toprule
{\bf Model} &{MNLI-m/mm (Acc)} & {SQuAD v1.1 (F1/EM)} &{SQuAD v2.0 (F1/EM)}  \\ 
\midrule
RoBERTa & 87.6/- & 91.5/84.6 & 83.7/80.5  \\ \hline
        XLNet & 86.8/- & -/- & -/80.2  \\ \hline
        {\ModelName} & \textbf{88.8/88.5}  &\textbf{93.1/87.2} & \textbf{86.2/83.1}  \\
        \bottomrule
        \end{tabular}
    \caption{
    Results on MNLI in/out-domain (m/mm), SQuAD v1.1 and v2.0   development set. 
    }
    \label{tab:base-dev}
    \vspace{-3mm}
\end{table*}






\subsection{Model Analysis}
In this section, we first present an ablation study to quantify the relative contributions of different components introduced in {\ModelName}. 
Then, we study the convergence property to characterize the model training efficiency. 
We run experiments for analysis using the base model setting: a model is pre-trained using 
the Wikipedia + Bookcorpus dataset 
for 1M steps with batch size 256 
in 7 days on a DGX-2 machine with 16 V-100 GPUs.
Due to space limit, we visualize the different attention patterns of {\ModelName} and RoBERTa in Appendix A.7. 







\subsubsection{Ablation study}
\label{subsec:abs}
To verify our experimental setting, we pre-train the RoBERTa base model from scratch.  
The re-pre-trained RoBERTa model is denoted as RoBERTa-ReImp\textsubscript{base}. 
To investigate the relative contributions of different components in {\ModelName}, we develop three variations:
\begin{itemize}
\item {-\DecoderName} is the {\ModelName} base model without \DecoderName.
    \item {-C2P} is the  {\ModelName} base model without the content-to-position term ((c) in Eq.~\ref{dis-att}).
    \item {-P2C} is the {\ModelName} base model without the position-to-content term ((b) in Eq.~\ref{dis-att}). As XLNet also uses the relative position bias, this model is close to XLNet plus \DecoderName.
\end{itemize}
\begin{table*}[htb!]
    \centering
    \begin{tabular}{@{\hskip1pt}l| c| cc|c@{\hskip1pt}}
        \toprule
        \multirow{2}{*}{\bf Model} &{MNLI-m/mm}& {SQuAD v1.1} &{SQuAD v2.0} &RACE\\ 
         & { Acc}& F1/EM & F1/EM  & Acc\\
        \midrule
        BERT \cite{devlin2018bert} & 84.3/84.7 & 88.5/81.0 &76.3/73.7   & 65.0 \\ RoBERTa \cite{liu2019roberta} & 84.7/- & 90.6/- &79.7/-   & 65.6\\ XLNet \cite{yang2019xlnet}& 85.8/85.4 & -/- & 81.3/78.5  & 66.7\\ \hline
        RoBERTa-ReImp & 84.9/85.1 & 91.1/84.8 &79.5/76.0   & 66.8\\ \hline {\ModelName} & \textbf{86.3/86.2} &\textbf{92.1/86.1} & \textbf{82.5/79.3}  &\textbf{ 71.7}\\
        -EMD & 86.1/86.1& 91.8/85.8 &81.3/78.0   & 70.3\\ -C2P  & 85.9/85.7 & 91.6/85.8 &81.3/78.3   & 69.3\\ -P2C  & 86.0/85.8 & 91.7/85.7 &80.8/77.6   & 69.6\\ -(EMD+C2P) & 85.8/85.9 & 91.5/85.3 &80.3/77.2   & 68.1\\ -(EMD+P2C)  & 85.8/85.8& 91.3/85.1 &80.2/77.1    & 68.5\\ \bottomrule
        \end{tabular}
    \caption{
    Ablation study of the DeBERTa base model.
    }
    \label{tab:ablation}
\end{table*}
Table~\ref{tab:ablation} summarizes the results on four benchmark datasets. 
First, RoBERTa-ReImp performs similarly to RoBERTa across all  benchmark datasets, verfiying that our setting is reasonable. 
Second, we see that removing any one component in {\ModelName} results in a sheer performance drop. 
For instance, removing EMD (-EMD) results in a loss of 1.4\% (71.7\% vs. 70.3\%) on RACE, 0.3\% (92.1\% vs. 91.8\%) on SQuAD v1.1, 1.2\% (82.5\% vs. 81.3\%) on SQuAD v2.0, 0.2\% (86.3\% vs. 86.1\%) and 0.1\% (86.2\% vs. 86.1\%) on MNLI-m/mm, respectively. 
Similarly, removing either \textit{content-to-position} or  \textit{position-to-content} leads to inferior performance in all the benchmarks.
As expected, removing two components results in even more substantial loss in performance. 










































\subsection{Scale up to 1.5 billion parameters}
Larger pre-trained models have shown better generalization results \citep{raffel2019t5,brown2020language, shoeybi2019megatron}. 
Thus, we have built a larger version of DeBERTa with 1.5 billion parameters, denoted as DeBERTa.
The model consists of 48 layers with a hidden size of 1,536 and 24 attention heads
\footnote{See Table~\ref{tbl:hyper-pre-train} in Appendix for the model hyperparameters.}.
DeBERTa is trained on a pre-training dataset amounting to 160G, similar to that in~\cite{liu2019roberta}, with a new vocabulary of size 128K constructed using the dataset.


To train DeBERTa, we optimize the model architecture as follows. 
First, we share the projection matrices of relative position embedding  with , respectively, in all attention layers to reduce the number of model parameters. Our ablation study in Table \ref{tab:v2} on base models shows that the projection matrix sharing reduces the model size while retaining the model performance.
Second, a convolution layer is added aside the first Transformer layer to induce n-gram knowledge of sub-word encodings and their outputs are summed up before feeding to the next Transformer layer~\footnote{Please refer to Table~\ref{tab:xxlarge} in Appendix \ref{subsec:scale} for the ablation study of different model sizes, and Table \ref{tab:v2} in Appendix \ref{subsec:scale} for the ablation study of new modifications.}.



Table~\ref{tab:superglue} reports the test results of SuperGLUE \citep{wang2019superglue} which is one of the most popular NLU benchmarks. 
SuperGLUE consists of a wide of NLU tasks, including  
Question Answering \citep{clark2019boolq, khashabi2018multirc, zhang2018record}, 
Natural Language Inference \citep{rte1,rte2,rte3, rte5}, Word Sense Disambiguation \citep{pilehvar2019wic}, and Reasoning \citep{levesque2011winograd,roemmele2011choice}. 
Since its release in 2019, top research teams around the world have been developing large-scale PLMs that have driven striking performance improvement on SuperGLUE. 

The significant performance boost due to scaling DeBERTa to a larger model makes the single DeBERTa surpass the human performance on SuperGLUE for the first time in terms of macro-average score (89.9 versus 89.8) as of December 29, 2020, and the ensemble DeBERTa model (DeBERTa) sits atop the SuperGLUE benchmark rankings as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus 89.8). 
Compared to T5, which consists of 11 billion parameters, the 1.5-billion-parameter DeBERTa is much more energy efficient to train and maintain, and it is easier to compress and deploy to apps of various settings.



\begin{table*}[htb!]
    \centering
    \begin{tabular}{@{\hskip2pt}l@{\hskip2pt}|@{\hskip2pt} @{\hskip1pt}c@{\hskip1pt}|@{\hskip2pt}c@{\hskip1pt}|@{\hskip2pt}c@{\hskip1pt}|c@{\hskip1pt}|c@{\hskip1pt}|c@{\hskip1pt}|c@{\hskip1pt}|c@{\hskip1pt}|@{\hskip2pt}c@{\hskip1pt}}
        \toprule
        \multirow{2}{*}{\bf Model} & BoolQ &CB&COPA&MultiRC&ReCoRD&RTE&WiC&WSC& Average\\ 
         & Acc & F1/Acc & Acc &F1a/EM&F1/EM&Acc&Acc&Acc& Score\\
        \midrule
RoBERTa\textsubscript{large} & 87.1  & 90.5/95.2 & 90.6 &84.4/52.5&90.6/90.0&88.2&69.9 &89.0 &84.6\\  \hline
        NEXHA-Plus                  & 87.8  & 94.4/96.0 & 93.6 &84.6/55.1&90.1/89.6&89.1&74.6 &93.2 &86.7\\  \hline
        T5 &91.2  & 93.9/96.8 & 94.8 &88.1/63.3&94.1/93.4&92.5&76.9 &93.8 &89.3\\ 
        \hline
        T5+Meena &\textbf{91.3}  & \textbf{95.8/97.6} & 97.4 &88.3/63.0&94.2/93.5&92.7&\textbf{77.9} &95.9 &90.2\\ 
        \hline \hline
        Human  &89.0 & 95.8/98.9 & 100.0 &81.8/51.9&91.7/91.3&93.6&80.0 &100.0 &89.8\\  \hline \hline
        DeBERTa+SiFT &90.4  & 94.9/97.2 & 96.8&\textbf{88.2/63.7}&\textbf{94.5/94.1}&\textbf{93.2}&76.4 &	\textbf{95.9} & 89.9\\  
        DeBERTa &90.4  & 95.7/97.6 & \textbf{98.4}&88.2/63.7&94.5/94.1&93.2&77.5 &	95.9 & \textbf{90.3}\\
        \bottomrule
        \end{tabular}
    \caption{
SuperGLUE test set results scored using the SuperGLUE evaluation server. All the results are obtained from \href{https://super.gluebenchmark.com}{https://super.gluebenchmark.com} on January 6, 2021.
    }
    \label{tab:superglue}
    
\end{table*}
