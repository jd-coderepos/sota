[{'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy', 'Score': '86.8'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'F1', 'Score': '88.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'EM', 'Score': '63.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Quora Question Pairs', 'Metric': 'Accuracy', 'Score': '92.3%'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '90.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0', 'Metric': 'EM', 'Score': '88.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0', 'Metric': 'F1', 'Score': '90.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'COPA', 'Metric': 'Accuracy', 'Score': '98.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'COPA', 'Metric': 'Accuracy', 'Score': '96.8'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'SWAG', 'Metric': 'Test', 'Score': '90.8'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'F1', 'Score': '94.5'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'EM', 'Score': '94.1'}}, {'LEADERBOARD': {'Task': 'Word Sense Disambiguation', 'Dataset': 'Words in Context', 'Metric': 'Accuracy', 'Score': '77.5'}}, {'LEADERBOARD': {'Task': 'Word Sense Disambiguation', 'Dataset': 'Words in Context', 'Metric': 'Accuracy', 'Score': '76.4'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'F1', 'Score': '94.9'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'Accuracy', 'Score': '97.2'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'QNLI', 'Metric': 'Accuracy', 'Score': '95.3%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'WNLI', 'Metric': 'Accuracy', 'Score': '94.5%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '93.2%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Matched', 'Score': '91.1'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Mismatched', 'Score': '91.1'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'STS Benchmark', 'Metric': 'Accuracy', 'Score': '92.5'}}, {'LEADERBOARD': {'Task': 'Sentiment Analysis', 'Dataset': 'SST-2 Binary classification', 'Metric': 'Accuracy', 'Score': '96.5'}}, {'LEADERBOARD': {'Task': 'Coreference Resolution', 'Dataset': 'Winograd Schema Challenge', 'Metric': 'Accuracy', 'Score': '95.9'}}, {'LEADERBOARD': {'Task': 'Linguistic Acceptability', 'Dataset': 'CoLA Dev', 'Metric': 'Accuracy', 'Score': '69.5'}}]
