\section{Results}
\label{Sec:results}

\subsection{Dataset and training details} 
For training and testing we used the PCPNet shape dataset \cite{guerrero2018pcpnet}. The training set consists of eight shapes: four CAD objects (fandisk, boxunion, flower, cup) and four high quality scans of figurines (bunny, armadillo, dragon and turtle). All shapes are given as triangle meshes and densely sampled with 100k points. The data is augmented by introducing i.i.d. Gaussian noise for each point's spacial location with a standard deviation of  0.012, 0.006, 0.00125 w.r.t the bounding box size. This yields a set with 3.2M training examples. The test set consists of 22 shapes, including figurines, CAD objects, and analytic shapes. For evaluation we use the same 5000 point subset per shape as in Guerrero et al.~\cite{guerrero2018pcpnet}.

All variations of our method were trained using 32,768 (1024 samples by 32 shapes) random subsets of the 3.2M training samples at each epoch. We used a batch size of , the Adam optimizer and a learning rate of . The implementation was done in PyTorch and trained on a single Nvidia RTX 2080 GPU.

\subsection{Normal estimation performance}
\label{SubSec:results:baseline_n_est}

 We use the RMSE metric for comparing the proposed DeepFit to other deep learning based methods \cite{guerrero2018pcpnet,ben2019nesti,lenssen2019differentiable} and classical geometric methods \cite{hoppe1992surface,cazals2005estimating}. 
 Additionally, we analyze robustness for two types of data corruption: 
\begin{itemize}
    \item  Point density---applying two sampling regimes for point subset selection: gradient, simulating effects of distance from the sensor, and stripes, simulating local occlusions.    
    \item Point perturbations--adding Gaussian noise to the points coordinates with three levels of magnitude specified by , given as a percentage of the bounding box.
\end{itemize}
For the geometric methods, we show results for three different scales: small, medium and large, which correspond to 18, 112, 450 nearest neighbors. For the deep learning based methods we show the results for the single-scale (ss) and multi-scale (ms) versions.

Table \ref{table:results:baselines} shows the unoriented normal RMSE results for the methods detailed above. It can be seen that our method slightly outperforms all other methods for low, medium and no noise augmentation and for gradient density augmentation. For high noise, and striped occlusion augmentation we are a close second to the contemporary work  of Lenssen et al. \cite{lenssen2019differentiable} which only estimates the normal vectors while DeepFit also estimates other geometric properties, e.g., principal curvatures. 
The results also show that all method's performance deteriorate as the noise level rises. In this context, both PCA and Jet perform well for specific noise-scale pairs. In addition, for PCPNet, using a multiple scales only mildly improves performance. Nesti-Net's mixture of experts mitigate the scale-accuracy tradeoff well at the cost of computational complexity. DeepFit's soft point selection process overcomes this tradeoff. In the supplemental materials we perform additional evaluation using the percentage of good points (PGP) metric.

\begin{table*} 
	\centering	
		\tabcolsep = 0.0025\textwidth
		\begin{tabular}{| M{0.12\textwidth} | M{0.10\textwidth} | M{0.06\textwidth} | M{0.06\textwidth}| M{0.06\textwidth} | M{0.06\textwidth}|
		M{0.06\textwidth}| M{0.06\textwidth} | M{0.06\textwidth}|  M{0.07\textwidth} | M{0.08\textwidth} | M{0.12\textwidth} |} 
			\hline
			\centering\textbf{Aug.}  & \centering\textbf{Our DeepFit}  & \multicolumn{3}{c|}{\makecell[{{M{0.18\textwidth}}}]{\textbf{PCA} \\ \cite{hoppe1992surface}} }  &
			\multicolumn{3}{c|}{\makecell[{{M{0.18\textwidth}}}]{\textbf{Jet} \\ \cite{cazals2005estimating}} } & 	\multicolumn{2}{c|}{\makecell[{{M{0.14\textwidth}}}]{\textbf{PCPNet} \\ \cite{guerrero2018pcpnet}} } & \centering\textbf{Len-ssen et. al} \\ \cite{lenssen2019differentiable} & \centering\textbf{Nesti-Net}
 			\tabularnewline
 			\hline
 			scale &ss&small&med&large&small&med&large&ss&ms&ss&ms (MoE)\\
 			\hlineB{2}
 		    None               &\textbf{6.51}&8.31&12.29&16.77&7.60&12.35&17.35&9.68&9.62&6.72&6.99\\
 		    \hline
 			\textbf{Noise }     &&&&&&&&&&& \\
 			 &\textbf{9.21}&12.00&12.87&16.87&12.36&12.84&17.42&11.46&11.37&9.95&10.11\\
 			   &\textbf{16.72}&40.36&18.38&18.94&41.39&18.33&18.85&18.26&18.87&17.18&17.63\\
 			   &23.12&52.63&27.5 &23.5 &53.21&27.68&23.41&22.8&23.28&\textbf{21.96}&22.28\\
		    \hline
		    \textbf{Density}   &&&&&&&&&&& \\
		    Gradient           &\textbf{7.31}&9.14&12.81&17.26&8.49&13.13&17.8&13.42&11.7 &7.73&9.00\\
		    Stripes            &7.92&9.42&13.66&19.87&8.61&13.39&19.29&11.74&11.16&\textbf{7.51}&8.47\\
		    \hline
		    \textbf{average}   &\textbf{11.8}&21.97&16.25&18.87&21.95&16.29&19.02&14.56&14.34&11.84&12.41 \\
		    \hline
		\end{tabular}
	\caption{Comparison of the RMSE angle error for unoriented normal vector estimation of our DeepFit method to classical geometric methods (PCA \cite{hoppe1992surface} and Jet \cite{cazals2005estimating} - for three scales small, med, and large corresponding to ), and deep learning methods (PCPNet \cite{guerrero2018pcpnet}, Lenssen et. al \cite{lenssen2019differentiable}, and Nesti-Net \cite{ben2019nesti})}
	\label{table:results:baselines}
\end{table*} 


Figure \ref{fig:results_normals_visualiztion:a} depicts a visualization of DeepFit's results on three point clouds. Here the normal vectors are mapped to the RGB cube. 
It shows that for complex shapes (pillar, liberty) with high noise levels, the  general direction of the normal vector is predicted correctly, but, the fine details and exact normal vector are not obtained. For a basic shape (Boxysmooth) the added noise does not affect the results substantially. Most notably, DeepFit shows robustness to point density corruptions.
Figure \ref{fig:results_normals_visualiztion:b} depicts a visualization of the angular error in each point for the different methods using a heat map. For the Jet method~\cite{cazals2005estimating} we display the results for medium scale. For all methods, it can be seen that more errors occur in regions with small details, high curvature e.g. edges and corners, and complex geometry. DeepFit suffers the least from this effect due to its point-wise weight estimation, which allows it to adapt to the different local geometryand disregard irrelevant points in the fitting process. 

\begin{figure}
\centering
    \begin{subfigure}{.48\textwidth}
        \centering
    	\includegraphics[width=0.98\linewidth]{normal_visualization.pdf}
    	\caption{}
    	\label{fig:results_normals_visualiztion:a}
    \end{subfigure}
    \unskip\ \vrule\ 
    \begin{subfigure}{.48\textwidth}
        \centering
    	\includegraphics[width=0.98\linewidth]{"error_comparison".pdf}
        	\caption{}
        	\label{fig:results_normals_visualiztion:b}
	\end{subfigure}
	\caption{(a) DeepFit's normal estimation results for different noise levels (columns 1-4), and density distortions (columns 5-6). The colors of the points are normal vectors mapped to RGB. (b) Normal estimation error visualization results of DeepFit compared to other methods for three types of point clouds without noise. The colors of the points correspond to angular difference, mapped to a heatmap ranging from 0-60 degrees.}
\end{figure}


Figure \ref{figure:results_weights_visualiztion} qualitatively visualizes the performance of DeepFit's point-wise weight prediction network. The colors of the points correspond to weight magnitude, mapped to a heatmap ranging from 0 to 1 i.e. red points highly affect the fit while blue points have low influence. It shows that the network learns to adapt well to corner regions (column ), assigning high weights to points on one plane and excluding points on the perpendicular one. Additionally, it shows how the network adapted the weight to achieve a good fit for complex geometries (column ). 

\begin{figure}
\centering
	\includegraphics[width=0.98\linewidth]{point_weight_visualization.pdf}
	\caption{DeepFit point-wise weight prediction. Three views of different -jet surface fits. The colors of the points correspond to weight magnitude , mapped to a heatmap ranging from 0 to 1; see color bar on the right i.e. red points highly affect the fit while blue points have low influence.}.
	\label{figure:results_weights_visualiztion}
\end{figure}

Fig. \ref{fig:results:ablation} shows the unoriented normal RMSE results for different parameter choices of our method. We explore different Jet orders , and a different number of neighboring points , It shows that using a large neighborhood size highly improves the performance in high noise cases while only minimally affecting the performance in low noise. It also shows that all jet orders are comparable with a small advantage for order 1-jet (plane) and order 3-jet which is an indication for a bias in the dataset towards low curvature geometry. Additional ablation results, including more augmentations and the PGP metric are provided in the supplemental material.

Timing and efficiency performance are provided in the supplemental material. DeepFit is faster and has fewer parameters than PCPNet and Nesti-Net and has the potential of only being slightly slower than CGAL implementation of Jet fitting because the forward pass for weight estimation is linear with respect to the number of points and the network weights.

\begin{figure}[t]
    \centering
    	\begin{subfigure}{.98\textwidth}
    \centering
\includegraphics[width=0.4\linewidth]{ablations_legend_rmse.pdf}
    	\label{fig:results:ablations:b}
	\end{subfigure}

    \begin{subfigure}{.49\textwidth}
        \centering
    	\includegraphics[width=0.98\linewidth]{ablations_angle_rmse_results_no_noise.pdf}
    	\caption{}
        \label{fig:results:ablations:a}
    \end{subfigure}
	\begin{subfigure}{.49\textwidth}
    \centering
\includegraphics[width=0.98\linewidth]{ablations_angle_rmse_results_high_noise.pdf}
    	\caption{}
    	\label{fig:results:ablations:b}
	\end{subfigure}
	    \caption{Normal estimation RMSE results for DeepFit ablations for (a) no noise and (b) high noise augmentations. Comparing the effect of number of neighboring points and jet order.}
    
    \label{fig:results:ablation}
\end{figure}

\subsection{Principal curvature estimation performance}
\label{SubSec:results:baseline_c_est}
Figure \ref{fig:curvature_result_visualization} qualitatively depicts DeepFit's results on five point clouds. For visualization, the principal curvatures are mapped to RGB values according to the commonly used mapping given in its bottom right corner i.e. both positive (dome) are red, both negative (bowl) are blue, one positive and one negative (saddle) are green, both zero (plane) are white, and one zero and one positive/negative (cylinder) are yellow/cyan. For consistency in color saturation we map each model differently according to the mean and standard deviation of the principal curvatures.  Note that the curvature sign is determined by the ground truth normal orientation. 

\begin{figure}
\centering
    	\includegraphics[width=0.80\linewidth]{curvature_visualization.pdf}
	\caption{Curvature estimation results visualization.  The colors of the points corresponds to the mapping of   to the color map given in the bottom right. Values in the range .}
	\label{fig:curvature_result_visualization}
\end{figure}

For quantitative evaluation we use the normalized RMSE metric curvature estimation evaluation proposed in Guerrero et. al.~\cite{guerrero2018pcpnet} and given in Eq. \ref{eq:D_k}, for comparing the proposed method to other deep learning based  \cite{guerrero2018pcpnet} and geometric methods \cite{cazals2005estimating}. Table \ref{table:results:curvature_baselines} summarizes the results and shows an average error reduction of 35\% and 13.7\% for maximum and minimum curvatures respectively. We analyze robustness for the same types of data corruptions as in normal estimation i.e. point perturbation and density. DeepFit significantly outperforms all other methods for maximum principal curvature . For the minimum principal curvature  DeepFit outperforms all methods for low and no noise augmentation in addition to gradient and striped density augmentation,  however PCPNet has a small advantage for medium and high noise levels. The results for the minimum curvature are very sensitive since most values are close to zero. 


    \begin{table} 

	\parbox{.48\linewidth}{
		\centering	
\begin{tabular}{| M{0.1\textwidth} | M{0.08\textwidth}| M{0.08\textwidth} | M{0.06\textwidth}|
		M{0.06\textwidth}|	M{0.06\textwidth}|} 
		\hline
		\centering\textbf{Aug.} &\textbf{Our DeepFit}  &
				\centering\textbf{PCP-Net} \cite{guerrero2018pcpnet} & \multicolumn{3}{c|}{\makecell[{{M{0.18\textwidth}}}]{\textbf{Jet} \\ \cite{cazals2005estimating}} }  	
		\tabularnewline
 			\hline
 			output & +n &  +n &  &  &  \\ 
 			scale & ss & ms & small & med. & large \\
            \hlineB{2}
            None                & \textbf{1.00} & 1.36 &  2.19 &  6.55 & 2.97 \\
            \textbf{Noise }      &  & & & & \\
              & \textbf{1.00} & 1.48 & 57.35 &  6.68 & 2.90 \\
                & \textbf{0.98} & 1.46 & 60.91 &  9.86 & 3.30 \\
                & \textbf{1.21}  & 1.59 & 49.40 & 10.78 & 3.58 \\
            \textbf{Density}    & & & & & \\
            Gradient            &\textbf{ 0.59 }& 1.32 &  2.07 &  1.40 & 1.53 \\
            Stripes             & \textbf{ 0.6 }& 1.09 &  2.04 &  1.54 & 1.89 \\
            \hline
		    \textbf{average}    & \textbf{0.89} & 1.38 & 28.99 &  6.13 &  2.69\\
		    \textbf{reduc.}    & \textbf{35.5\%} & &  &  & \\
		    \hline
		\end{tabular}
}
\hfill
\parbox{.48\linewidth}{
	\centering	
\begin{tabular}{| M{0.1\textwidth} | M{0.08\textwidth}| M{0.08\textwidth} | M{0.06\textwidth}|
		M{0.06\textwidth}|	M{0.06\textwidth}|} 
		\hline
		\centering\textbf{Aug.} &\textbf{Our DeepFit}   &
				\centering\textbf{PCP-Net} \cite{guerrero2018pcpnet} & \multicolumn{3}{c|}{\makecell*[{{M{0.15\textwidth}}}]{\textbf{Jet} \\ \cite{cazals2005estimating} }}  	
		\tabularnewline
 			\hline
 			output & +n  & +n &  &  &  \\ 
 			scale & ss & ms & small & med. & large \\
            \hlineB{2}
            None                & 0\textbf{.46} & 0.54 &  1.61 & 2.91 & 1.59  \\
            \textbf{Noise }      & & & & &  \\

              & \textbf{0.47} & 0.53 & 25.83 & 2.98 & 1.53  \\
                & 0.57 & \textbf{0.51} & 22.27 & 4.88 & 1.73  \\
                & 0.68 & \textbf{ 0.53} & 18.17 & 5.22 & 1.84 \\
            \textbf{Density}    & & & & & \\
            Gradient            & \textbf{0.31} & 0.61 &  2.04 & 0.79 & 0.83 \\
            Stripes             & \textbf{0.31 }& 0.55 &  1.92 & 0.89 & 1.09 \\
            \hline
		    \textbf{average}    & \textbf{0.466} & 0.54 & 11.97 & 2.94 & 1.43\\
		    \textbf{reduc.}    & \textbf{13.7\%} & &  &  & \\
		    \hline
		\end{tabular}
}
	\caption{Comparison of normalized RMSE for (left) maximal ()  and (right) minimal () principal curvature  estimation of our DeepFit method to the classic Jet \cite{cazals2005estimating} with three scales, and PCPNet \cite{guerrero2018pcpnet}}
	\label{table:results:curvature_baselines}
\end{table}
 
The normalized RMSE metric is visualized in Fig. \ref{fig:results_curvature_error_comparison} for DeepFit and PCPNet as the magnitude of the error vector mapped to a heatmap.  It can be seen that more errors occur near edges, corners and small regions with a lot of detail and high curvature.
These figures show that for both simple and complex geometric shapes DeepFit is able to predict the principal curvatures reliably. 

\begin{figure}
\centering
	\includegraphics[width=0.98\linewidth]{curvature_error_comparison.pdf}
	\caption{Curvature estimation error results for DeepFit compared PCPNet. The numbers under each point cloud are its normalized RMSE errors in the format (, ). The color corresponds to the L2 norm of the error vector mapped to a heatmap ranging from 0-5.}
	\label{fig:results_curvature_error_comparison} 
\end{figure}

\subsection{Surface reconstruction and noise removal}
We further investigate the effectiveness of our surface fitting in the context of two subsequent applications---Poisson surface reconstruction~\cite{kazhdan2006poisson} and noise removal.

\subsubsection{Surface reconstruction.}
Fig. \ref{fig:results:poisson_recon} shows the results for the classical Jet fitting and our DeepFit approach. Since the reconstruction requires oriented normals, we orient the normals, in both methods, according to the ground truth normal. It shows that using DeepFit, the poisson reconstruction is moderately more satisfactory by being smoother overall, and crispier near corners. It also retains small details (liberty crown, cup rim).  



\subsubsection{Noise removal.}
The point-wise weight prediction network enables a better fit by reducing the influence of neighboring points. This weight can also be interpreted as the network's confidence of that point to lie on the object's surface. Therefore, we can use the weight to remove points with low confidence.   We first aggregate the weights by summing all of its weight prediction from all of its neighbors. Then we compute the mean and standard deviation of the aggregateed weights and remove points under a threshold of .  The output point cloud contains less points than the original one and the removed points are mostly attributed to outliers or noise. The results are depicted in Fig. \ref{fig:results:noise_removal}.

\begin{figure}[t]
\centering
    \begin{subfigure}{.48\textwidth}
        \centering
    	\includegraphics[width=0.98\linewidth]{poisson.pdf}
    	\caption{}
    	\label{fig:results:poisson_recon} 
\end{subfigure}
    \unskip\ \vrule\ 
\begin{subfigure}{.48\textwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{noise_Removal.pdf}
    \caption{}
    \label{fig:results:noise_removal}
    \end{subfigure}
    \caption{DeepFit performance in two subsequent application pipelines: (a) Poisson surface reconstruction using estimated normal vectors from the classical Jet fitting and the proposed DeepFit. (b) Noise removal results using DeepFit predicted weights.}
\end{figure}