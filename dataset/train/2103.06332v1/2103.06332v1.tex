\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{naacl2021}

\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{color,soul}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}



\title{Hurdles to Progress in Long-form Question Answering}



\author{Kalpesh Krishna \And Aurko Roy \\\\ University of Massachusetts Amherst, Google Research \\ \texttt{\{kalpesh,miyyer\}@cs.umass.edu}\\ \texttt{aurkor@google.com} \And Mohit Iyyer}

 \newcommand{\namedref}[2]{\hyperref[#2]{#1~\ref*{#2}}}

\newcommand{\sectionref}[1]{\namedref{Section}{#1}}
\newcommand{\tableref}[1]{\namedref{Table}{#1}}
\newcommand{\figureref}[1]{\namedref{Figure}{#1}}
\newcommand{\appendixref}[1]{\namedref{Appendix}{#1}}

\newcommand{\micomment}[1]{\textcolor{red}{\bf \small [ #1 --MI]}}
\newcommand{\kkcomment}[1]{\textcolor{blue}{\bf \small [ #1 --KK]}}
\newcommand{\arcomment}[1]{\textcolor{purple}{\bf \small [ #1 --AR]}}
\newcommand{\retriever}[1]{\textsc{c-REALM}}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}





\begin{document}
\maketitle
\begin{abstract}

The task of \emph{long-form question answering} (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and
contrastive retriever learning to achieve state-of-the-art performance on the \emph{ELI5} LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system's generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / test overlap, as at least 81\% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We provide suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.\blfootnote{* Work done during an internship at Google Research.}

\end{abstract} \section{Introduction}

Long-form question answering (LFQA) integrates the \emph{retrieval} component of open-domain QA, which involves searching a large external knowledge source for documents relevant to a given question, with a text \emph{generation} component to produce paragraph-length answers. Significant progress has been made on open-domain QA datasets such as Natural Questions~\citep{kwiatkowski2019natural}, whose questions are answerable with short phrases and entities, by leveraging dense retrieval techniques like ORQA~\citep{lee-etal-2019-latent}, REALM~\citep{guu2020realm}, and DPR~\citep{karpukhin2020dense,lewis2020retrieval,izacard2020leveraging}. Methods inspired by these results have recently been combined with pretrained language models~\citep{lewis2019bart,petroni2020kilt} and applied to the Reddit-derived ``Explain Like I'm Five'' (ELI5) dataset~\citep{fan-etal-2019-eli5}, which is the only publicly-available large-scale LFQA dataset.

The recently proposed KILT benchmark~\citep{petroni2020kilt}, which compares retrieval-augmented models across a variety of knowledge-intensive tasks including ELI5, automatically evaluates LFQA models by the quality of both generated answers (ROUGE-L against reference answers) and retrieved documents (R-precision against human-annotated relevant documents). In this paper, we build a state-of-the-art system\footnote{State-of-the-art as of March 10, 2021 --- the ``Google Research \& UMass Amherst'' team entry on \small{\url{https://evalai.cloudcv.org/web/challenges/challenge-page/689/leaderboard/1908}}} for ELI5 by using a sparse Transformer variant~\citep{roy2020efficient} to condition over Wikipedia paragraphs returned by a REALM-style retriever~\citep{guu2020realm}.

However, despite its success on the KILT leaderboard, our system does not actually \emph{use} the documents that it retrieves! To measure the effect of retrieval on generation quality, we design a control experiment in which retrieved documents are replaced with randomly-sampled documents at inference time. Results from both human A/B tests and automatic metrics like ROUGE-L demonstrate that conditioning on random documents has almost no effect on generated answer quality (\figureref{fig:main_diagram}c). We recommend that future LFQA research report the results of such control experiments in addition to reporting generation and retrieval quality.

\begin{figure*}[t]
\includegraphics[width=1\linewidth]{figures/diagram}
\caption{A summary of the major hurdles (a-d) to progress in long-form question answering with ELI5.}
\label{fig:main_diagram}
\end{figure*}

How can a system using random retrieval perform well on ELI5? Our analysis reveals that this result is partially due to significant train / validation overlap in the ELI5 dataset (\figureref{fig:main_diagram}a), which eliminates the need for external retrieval. A human study shows that at least 81\% of validation questions have a paraphrase in the training set, and almost all validation questions are topically similar to a training set question. While~\citet{fan-etal-2019-eli5} attempted to identify and remove question overlap using TF-IDF similarity, more complex semantic matching methods \& human verification is needed to address this issue in future LFQA datasets.

Digging deeper, we identify fundamental issues with using ROUGE-L to evaluate generated answer quality (\figureref{fig:main_diagram}b). Simple baselines such as just repeatedly copying the question, or choosing a random training set answer, can outperform  LFQA systems such as RAG~\citep{lewis2020retrieval} in terms of ROUGE-L. On the other hand, our system achieves \emph{higher} ROUGE-L than reference human-written answers, which is misleading since human A/B testers strongly prefer reference answers to our system's. We conclude that ROUGE-L is not a reliable metric to evaluate LFQA due to its large and relatively unconstrained output space (e.g., compared to translation or summarization), and we offer suggestions for better automatic \& human evaluations to enable meaningful progress on this task.
 \section{A state-of-the-art LFQA system}\label{sec:model}
The ELI5 task~\citep{fan-etal-2019-eli5} asks models to generate paragraph-length answers to open-ended questions that often rely on world knowledge (e.g., \textit{how do jellyfish function without brains or nervous systems?}). LFQA systems thus benefit from conditioning answer generation on relevant documents from the web (such as the Wikipedia article about \textit{jellyfish}). While large-scale pretrained language models store surprising amounts of world knowledge within their parameters~\citep{petroni2019language, roberts2020much}, external document retrieval not only augments this intrinsic knowledge but also grounds model outputs in a knowledge source, which provides interpretability.

In this section, we describe our proposed LFQA system, which conditions answer generation on Wikipedia articles identified by a pretrained retriever. We use a dense retriever trained by scaling up a distantly supervised algorithm from~\citet{jernite2020}. Since retrieved articles can be quite long and often exceed the maximum sequence length of pretrained models like BERT~\citep{devlin-etal-2019-bert}, we use a sparse-attention variant of the Transformer to allow modeling over longer sequences. Our system sets a new state of the art on ELI5, although we question the significance of this result in \sectionref{sec:analysis_main}.

\subsection{Retriever}
We begin by specifying our dense retriever (``contrastive REALM'' or~\retriever~), which returns documents related to an input question.
Consider a corpus of long-form questions and answers, represented by . Our retriever uses  as a query to retrieve  documents  from a knowledge corpus (Wikipedia), which is enabled by an encoder network that projects both questions and candidate documents to a 128- shared embedding space. Like REALM~\citep{guu2020realm}, our encoder is a BERT-base Transformer~\citep{devlin-etal-2019-bert} with a final projection layer. 

Since the ELI5 dataset does not include gold retrievals, we train our retriever by scaling up a method recently introduced by~\citet{jernite2020} that uses gold answers for distant supervision. The key idea is to push the encoded vector for a question close to a vector representation of its ground-truth answer(s), but away from all other answer vectors in the mini-batch (negative examples). Intuitively, this method works because both ELI5 answers and external documents are of paragraph length (documents are paragraph-length chunks from Wikipedia). Concretely, we optimize the loss,



where  is the mini-batch and ,  are the encoded vector representations for . This objective is based on contrastive learning, a method that has been used effectively for semi-supervised learning~\citep{chen2020simple} and dense retriever training~\citep{karpukhin2020dense}. Scaling up from~\citet{jernite2020}, we use large mini-batches of size 12K, and initialize our retriever with the REALM CCNews checkpoint~\citep{guu2020realm}.\footnote{\citet{jernite2020} use batch size 512, initialize with BERT.} These design decisions greatly improve retriever quality, as we observe in an ablation study (see \appendixref{appendix:ablation_batch_size}). During inference, a MIPS search is conducted with the ScaNN library~\citep{avq_2020} to efficiently find the top  documents; we use  in all experiments.

\subsection{Generator}

We next describe our generator model, which conditions its generated answers on retrieved documents returned by \retriever~. We use the Routing Transformer (RT) from~\citet{roy2020efficient}, which is the current
state of the art in long-form language modeling. The RT is a sparse attention model that employs 
local attention as well as mini-batch -means clustering to better model long-range 
dependencies in sequences (attention maps in \appendixref{appendix:training_model}). Long-form language models such as RT are well-suited to ELI5 as the task requires conditioning answer generation not only on a short question but also many lengthy retrieved documents. 

We pretrain our RT model on PG-19, a long-form language
modeling benchmark~\citep{rae2020compressive} created from approximately 28,000 Project Gutenberg books published before 1919. PG-19 has 1.9B tokens and an average context size of 69K words. While this data is out-of-domain for ELI5, we choose it to encourage long \& coherent generation. Our RT is a 22-layer model with 1032 hidden units (486M parameters), maximum sequence length of  tokens, and a vocabulary of 98K subwords.\footnote{Our hyperparameters have been chosen manually with minimal tuning. See \appendixref{appendix:training_model} for details.}
We fine-tune our model in a decoder-only 
fashion~\citep{liu2018generating, wolf2019transfertransfo} by concatenating the top  retrieved documents to the question
 and training the model to predict tokens of the answer . We do not backpropagate gradients through the 
retriever.\footnote{We tried training the retriever jointly with RT using the attention bias scheme proposed in MARGE~\citep{lewis2020pre}. This 
improved perplexity only in \textit{autoencoding} settings where the gold answer itself is used as 
a retrieval query (like the setup in~\citealp{lewis2020pre}), which is not valid in LFQA.} Retrievals slightly improve perplexity (18.1 vs 17.8) as seen in~\citet{wang2020fly}, but do not improve generations (\S \ref{sec:grounding}).


 \subsection{Main Experiments}
\label{sec:experiments}


\noindent \textbf{Dataset \& Evaluation}: We evaluate our model on the KILT validation \& test subsets of ELI5~\citep{petroni2020kilt}, since the original ELI5 dataset does not have human annotations to measure retriever performance (split sizes in \appendixref{appendix:dataset_stats}). 
Answer
quality is measured by the maximum overlap of generations with a set of gold answers in 
terms of unigram F1 score and ROUGE-L~\citep{lin-2004-rouge}.~\citet{petroni2020kilt} collected human annotations of Wikipedia articles which support 
ELI5 gold answers, which enables measuring retrieval quality by computing R-precision (if
the top-1 retrieval matches the annotation) and Recall@5 using the top-5 
retrievals. Finally, the KILT benchmark combines R-prec. and 
ROUGE-L to measure the overall performance of the system by ``KILT ROUGE-L''. This metric is similar to ROUGE-L, but assigns a score of 0 whenever the top-1 retrieval does not match the gold annotation.\\


\noindent \textbf{Baselines}: We compare our model with the other entries on the ELI5 KILT leaderboard which are either generation-only, like T5-base~\citep{raffel2019exploring} and BART~\citep{lewis2019bart}, or variants of BART using retrieval such as RAG~\citep{lewis2020retrieval} and BART + DPR~\citep{petroni2020kilt}. These systems are based on massive pretrained language models, with similar number of parameters as our model (details in \appendixref{appendix:number_parameters}).\\


\begin{table}[t]
\small
\begin{center}
\begin{tabular}{ lrrrrr } 
 \toprule
 & \multicolumn{2}{c}{\emph{Retrieval}} & \multicolumn{2}{c}{\emph{Generation}}\\
Model & RPr. & R@5 & F1 & R-L & KRL \\
\midrule
T5-base & 0.0 & 0.0 & 16.1 & 19.1 & 0.0 \\
BART & 0.0 & 0.0 & 19.2 & 20.6 & 0.0 \\
RAG & \textbf{11.0} & 22.9 & 14.5 & 14.1 & 1.7 \\
BART + DPR & 10.7 & \textbf{26.9} & 17.9 & 17.4 & 1.9 \\
\midrule
 \\
RT + REALM & 6.7 & 15.5 & 25.1 & 21.5 & 1.4 \\ RT + \retriever & 10.2 & 24.4 & \textbf{25.4} & 21.5 & 2.1\\\\

 \\
RT + REALM & 6.7 & 15.7 & 23.1 & \textbf{23.4} & 1.5\\ RT + \retriever & 10.7 & 24.6 & 22.9 & 23.2 & \textbf{2.4} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Results on the KILT test set for ELI5 for (1) retrieval performance, using R-precision and Recall@5 (RPrec, R@5), and (2) generation quality, using ROUGE-L (R-L). These scores are combined to produce the final metric KILT R-L (KRL). We outperform prior work on both generation \& combined scores.}
\label{tab:kilt_test_main_results}
\end{table}

\noindent \textbf{Results}:  \tableref{tab:kilt_test_main_results} contains our results on the test set of the ELI5 (also on the public KILT leaderboard). We present four variants of our system, using a different retriever during inference (REALM or~\retriever~), and different nucleus sampling  values~\citep{holtzman2020curious}. All variants outperform prior work in generation quality,
with lower-entropy models () performing best.\footnote{As in~\citet{holtzman2020curious}, a human study reveals that higher entropy () answers are slightly more coherent and sensible, but lower entropy answers () are more relevant to the question (details in \appendixref{appendix:human_evaluation}).} ~\retriever~ performs competitively to RAG and DPR despite being only distantly supervised, and outperforms REALM. Our proposed RT+\retriever~ system achieves a new state-of-the-art on combined performance (KILT R-L). Generations from our model are provided in \figureref{fig:generations_main} and \appendixref{appendix:generations}.


\begin{figure}[t]
    \footnotesize
    \centering
\begin{tabular}{p{0.95\columnwidth}}
    \toprule
  Q: Why are almost all boats white?\\
  \midrule
  A: Boats are generally painted white for obvious reasons, as they are inexpensive and easy to clean. Ships are also often painted to provide a visible visual contrast to the sea. Boats also often have high visibility ports to allow for easy international trade. In addition, many boats have an air-breathing function that makes them easier to spot and stay afloat, which is a good thing if you're a pirate or trying to get into some country's waters. Lastly, many ships are built in the last few decades, so they are not as exotic looking as they were thirty years ago. And lastly, many ships are made out of materials that are easy to work with, and so many of them are painted white as a practical matter. \hl{There are many, many more reasons for why ships are white, but those are the ones I know of. I'm sure there are many other reasons, but those are the ones I know of.} Edit: I forgot to mention that the other answers about visibility and easy to see ports are also true. Ships are painted to appear to be larger, and more expensive. That makes them appear more impressive, and so they are more likely to be bought by people who want to show off their wealth and status.\\
  \bottomrule
       \end{tabular}
\caption{Example generation from our LFQA system with . Generations are long \& coherent, but suffer from \hl{repetition} towards the end. (more in \appendixref{appendix:generations} and attached data supplementary material).}
    \label{fig:generations_main}
\end{figure}
 \section{Analysis}
\label{sec:analysis_main}

In this section, we conduct a thorough analysis of our model's usage of retrievals (\sectionref{sec:grounding}), the impact of overlap in ELI5's train / validation / test folds (\sectionref{sec:train_valid_overlap}), issues with ROUGE-L and performance bounds (\sectionref{sec:rougel_bounds}), and the difficulty in human evaluation for this task (\sectionref{sec:human_eval_difficulty}).
\subsection{Are generations grounded in retrieval?}
\label{sec:grounding}

While our retrieval-augmented system achieves state-of-the-art performance, we find little evidence that it is actually \emph{using} the retrieved documents. To measure this, we run an ablation study where at \emph{inference time} we replace retrieved paragraphs with randomly sampled paragraphs from Wikipedia. We compare this \emph{Random} baseline with our original system (\emph{Predicted}) in terms of generation quality as well as the -gram overlap between the generation and the retrieved paragraphs.\\

\begin{table}[t]
\small
\begin{center}
\begin{tabular}{ lrrrrrr } 
 \toprule
  &  & \multicolumn{2}{r}{vs predicted retr.} & \multicolumn{2}{c}{vs random retr.} \\
 & R-L & ~~~~~~~~~1-g & 2-g & ~~~~~~~1-g & 2-g \\
\midrule
\emph{Predicted}  & 24.42 & 52.3 & 9.0 & 38.8 & 3.9 \\
\emph{Random} & 24.20 & 51.2 & 8.5 & 38.5 & 3.9 \\
\midrule
\emph{Gold Ans} & - & 54.1 & 9.1 & 40.2 & 3.8 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Comparison of generations (with ) conditioned on predicted retrievals (\emph{Predicted}) and randomly chosen retrievals (\emph{Random}). Notice small differences in: (1) ROUGE-L vs gold answers (R-L); (2) -gram overlap (-g) with predicted retrievals (vs predicted retr.). Gold answers also have a similar overlap with predicted retrievals. To control for stopwords, we show overlaps with the random retrievals.}
\label{tab:pred_vs_random_all}
\end{table}

\begin{table}[t]
\small
\begin{center}
\begin{tabular}{ lllrrr } 
 \toprule
A & B &  Prefer A & Prefer B & Tie \\
\midrule
\multicolumn{5}{l}{For }\\
 pred. & random & 40\% (78) & 33\% (~~64) & 27\% (51) \\
 pred. & gold ans. & 14\% (29) & \textbf{68}\% (138) & 18\% (36) \\
 \midrule
\multicolumn{5}{l}{For }\\
  pred. & random & 31\% (52) & 37\% (~~63) & 32\% (54)\\
 pred. & gold ans. & 17\% (49) & \textbf{72}\% (203) & 11\% (31) \\
\bottomrule
\end{tabular}
\end{center}
\caption{Human evaluation results with exact number of ratings shown in (). Annotators are shown a question along with two answers (A, B) in random order and ask them to choose one (details in \appendixref{appendix:human_evaluation}). For both model variants (), we see (1) little difference between generations conditioned on predicted (pred.) or random (rand.) retrievals; (2) strong preference for gold answers over generations. }
\vspace{-0.1in}
\label{tab:human_evaluation}
\end{table}

\begin{table}[t]
\small
\begin{center}
\begin{tabular}{ lrrr } 
 \toprule
  & vs qn. & vs predicted retr. & vs random retr. \\
 & & but not in qn. & but not in qn. \\
\midrule
\multicolumn{4}{l}{\emph{lemmatized nouns, proper nouns, numbers only}} \\\\
Predicted & 13.4\% & 34.4\% & 11.9\% \\
Random & 13.7\% & 31.7\% & 12.1\% \\
\midrule
Gold Ans & 8.3\% & 28.8\% & 15.1\% \\
\bottomrule
\end{tabular}
\end{center}
\caption{A fine-grained version of \tableref{tab:pred_vs_random_all} measuring the unigram overlap of nouns/numbers in the generations with the input question (vs qn.), retrievals predicted by \retriever~ (vs predicted retr.) and randomly sampled retrievals (vs random retr.). Similar to \tableref{tab:pred_vs_random_all}, notice very little difference with and without retrieval.}
\label{tab:pred_vs_random_all_nouns}
\end{table}


\noindent \textbf{Generations are similar irrespective of type of retrievals}: We present our results in \tableref{tab:pred_vs_random_all}. Despite not being conditioned on any meaningful retrievals, the \emph{Random} retrieval model has similar ROUGE-L scores as our \emph{Predicted} system. Moreover, generations from the \emph{Random} and \emph{Predicted} models have similar amounts of 1-gram and 2-gram overlap with the paragraphs retrieved by \retriever~, despite the fact that the \emph{Random} model does not actually see the retrieved paragraphs.\footnote{Corresponding experiments with the  variant of our model are presented in \appendixref{appendix:grounding_correct_retrievals}.}

The -gram overlaps are possibly overestimates due to stopwords (e.g., prepositions, punctuation) and entities which are copied from the question. To tackle this issue, in \tableref{tab:pred_vs_random_all_nouns} we measure the fractions of lemmatized nouns, proper nouns and numbers in the generated answer which are present in the predicted retrievals but not in the question. We notice similar trends as before, with only small differences between the two systems.
Finally, there is almost no correlation (Spearman ) between the \emph{Predicted} model's generation quality and the amount of unigram overlap between its outputs and the retrieved documents (scatter plots in \appendixref{appendix:grounding_correct_retrievals}), strengthening our hypothesis that generations are not grounded in retrievals.\footnote{All these trends persist even on questions for which our retriever predicts the ground-truth document (\appendixref{appendix:grounding_correct_retrievals})}\\

\noindent \textbf{Human evaluation validates our findings}: As ROUGE-L and -gram overlap have major limitations for LFQA (\sectionref{sec:rougel_bounds}), we perform additional human A/B testing on the output of \emph{Random} and \emph{Predicted}. Specifically, we ask human volunteers\footnote{Details of our experimental setup in \appendixref{appendix:human_evaluation}.} to choose between answers generated by the two systems (presented in random order). As seen in \tableref{tab:human_evaluation}, humans struggle to choose which of the two answers is more relevant to the question. For both model variants (), there is a less than 7\% preference for a particular answer type, with humans preferring answers (by 6\%) from the \emph{Random} model for !\\


\noindent \textbf{Other systems also have this issue, possibly due to source-reference divergence and train-validation overlap}: We note that this issue is not unique to our system --- other systems on the KILT leaderboard like BART + DPR and RAG actually perform \emph{worse} than their no-retrieval counterpart (BART) in generation quality, as shown in \tableref{tab:kilt_test_main_results}. Qualitatively, we found no evidence of retrieval usage in a publicly hosted ELI5 model demo by~\citet{jernite2020}.\footnote{\url{https://huggingface.co/qa}} A possible explanation for this issue is high source-reference divergence, a common problem in table-to-text generation~\citep{wiseman2017challenges, tian2019sticking}. In \tableref{tab:pred_vs_random_all} and \tableref{tab:pred_vs_random_all_nouns}, we measure the -gram overlap of top-ranked gold validation answers (\emph{Gold Ans}) with predicted retrievals. This overlap is low and similar to that of our generations, which we suspect encourages our model to ignore retrievals. A second explanation is the large amount of train-validation overlap (\sectionref{sec:train_valid_overlap}), which eliminates the need for retrieval.\\


\noindent \textbf{Why does our model do well compared to other systems despite not using retrievals?} While our model has similar capacity as the BART/RAG baselines (comparison in \appendixref{appendix:number_parameters}), we hypothesize that our improvements in ROUGE-L are due to a different pretraining objective. BART is pretrained on a masked infilling task on short sequences. Instead, we pretrain our model to perform next-word prediction on long sequences from Project Gutenberg,  which encourages long \& fluent generations. To illustrate this length effect, in \appendixref{appendix:length_effect} we show that truncated outputs from our model get lower ROUGE-L scores on ELI5.\footnote{While we do not have access to generations from baselines on the KILT leaderboard, example generations from the demo of the BART model in ~\citet{jernite2020} are significantly shorter (59 words avg.) than our generations (187 words avg.).} Prior summarization literature~\citep{sun2019compare} has also shown that ROUGE scores vary heavily by length. To compare the same systems on shorter length outputs, we also tried finetuning the pretrained model on Wizard of Wikipedia~\citep{dinan2019wizard}, an unconstrained dialogue generation task with \textbf{single sentence} dialogues (much shorter than ELI5). As seen on the public KILT leaderboard,\footnote{\url{https://eval.ai/web/challenges/challenge-page/689/leaderboard/1909}} our system has \emph{lower} ROUGE-L scores than the BART / RAG baselines. Another possible explanation is issues with ROUGE-L itself, as discussed in \sectionref{sec:rougel_bounds}.\\


\noindent \textbf{Takeaway (better evaluation of grounding)}: For evaluating LFQA, it is important to run control experiments with random retrievals \& measure grounding of generations in retrieval. While the KILT benchmark does attempt to measure the combined retrieval + generation performance via KILT RL, it does not check whether the generations actually \emph{used} the retrievals. In other words, one can submit independent retrieval \& generation systems, but still perform well on the combined score. This may not be an issue for short-form QA tasks like Natural Questions, since the gold answer is often exactly contained as a span in the gold retrieval. Also, as retrieval might be less important for large language models with parametric knowledge~\citep{roberts2020much}, the KILT-RL strategy of simply aggregating top-1 retrieval score with ROUGE-L unfairly penalizes systems not relying on retrieval.\footnote{Another issue of KILT-RL is ignoring non top-1 retrievals, penalizing models using multiple retrievals together in context.}


\subsection{Training / Validation Overlap}
\label{sec:train_valid_overlap}

Our experiments in \sectionref{sec:grounding} show that model performance is mostly unchanged by 
conditioning generation on randomly sampled retrievals instead of predictions from~\retriever~. 
Despite not using retrievals, we observe qualitatively that our model displays a large amount of parametric knowledge (``Faraday Cage'' in \figureref{fig:main_diagram}c), which is surprising since 
it was pretrained on novels from Project Gutenberg (not Wikipedia). In this section, 
we discover that a major reason for ignoring retrievals is the large amount of train / validation overlap in ELI5. While ~\citet{fan-etal-2019-eli5} attempted to fix this issue through TF-IDF overlap, this method is insufficient to identify all question paraphrases, as we find significant overlap between the training set and the KILT validation set of ELI5.\footnote{The ELI5 demo from~\citet{jernite2020} also retrieves the top-1 similar training set question. Qualitatively, we found many validation examples had near-identical train paraphrases.} ELI5 is not the only dataset with substantial train / test overlap: ~\citet{lewis2020question} identify similar issues with short-form QA datasets like Natural 
Questions.\\

\noindent \textbf{Finding similar questions \& measuring overlap}: We use our retriever~\retriever~~to \emph{retrieve} similar questions from the training set, since it has learned to map questions to a feature-rich embedding space. For each validation question, we retrieve the 7 most similar training set questions. We use both human and automatic evaluation to calculate the amount of overlap. For human evaluation, we show annotators on Amazon Mechanical Turk\footnote{We pay workers 4 cents per question pair (\\kappaK(\downarrow)(\downarrow)(\uparrow)(\uparrow)(\uparrow)(\uparrow)(\uparrow)(\downarrow)p=0.6np=0.61/ff1/fp=0.6f1/f1/f\rho = 0.09\rho = 0.13p=0.6p = 0.6p = 0.9nnnp=0.6\rho = 0.13p=0.9p=0.9nnnpp=0.6p=0.9p=0.6p=0.6p=0.9p=0.9p=0.6p=0.9\cdot$.}
    \label{tab:generations_extra}
\end{table*}


 
\end{document}
