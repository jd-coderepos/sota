
\documentclass[10pt]{article} \usepackage[accepted]{tmlr}




\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{supertabular}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{tabularx, booktabs, multirow}

\title {INR-V: A Continuous Representation Space for Video-based Generative Tasks}



\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\author{\name Bipasha Sen$^*$ \email bipasha.sen@research.iiit.ac.in \\
      \addr IIIT Hyderabad
      \AND
      \name Aditya Agarwal$^*$ \email aditya.ag@research.iiit.ac.in \\
      \addr IIIT Hyderabad
      \AND
      \name Vinay P Namboodiri \email vpn22@bath.ac.uk \\
      \addr University of Bath
      \AND
      \name C. V. Jawahar \email
      jawahar@iiit.ac.in\\
      \addr IIIT Hyderabad}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{10}  \def\year{2022} \def\openreview{\url{https://openreview.net/forum?id=aIoEkwc2oB}} 

\begin{document}


\maketitle

\def\thefootnote{*}\footnotetext{Equal contribution.}

\begin{abstract}
Generating videos is a complex task that is accomplished by generating a set of temporally coherent images frame-by-frame. This limits the expressivity of videos to only image-based operations on the individual video frames needing network designs to obtain temporally coherent trajectories in the underlying image space. We propose INR-V, a video representation network that learns a continuous space for video-based generative tasks. INR-V parameterizes videos using implicit neural representations (INRs), a multi-layered perceptron that predicts an RGB value for each input pixel location of the video. The INR is predicted using a meta-network which is a hypernetwork trained on neural representations of multiple video instances. Later, the meta-network can be sampled to generate diverse novel videos enabling many downstream video-based generative tasks. Interestingly, we find that conditional regularization and progressive weight initialization play a crucial role in obtaining INR-V. The representation space learned by INR-V is more expressive than an image space showcasing many interesting properties not possible with the existing works. For instance, INR-V can smoothly interpolate intermediate videos between known video instances (such as intermediate identities, expressions, and poses in face videos). It can also in-paint missing portions in videos to recover temporally coherent full videos. In this work, we evaluate the space learned by INR-V on diverse generative tasks such as video interpolation, novel video generation, video inversion, and video inpainting against the existing baselines. INR-V significantly outperforms the baselines on several of these demonstrated tasks, clearly showcasing the potential of the proposed representation space.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Learning to generate complex spatio-temporal videos from simple distributions is a challenging problem in computer vision that has been recently addressed in various ways~\cite{mocogan-hd, mocogan, dvd-gan, stylegan-v, tgan, digan, videogpt}. State-of-the-art (SOTA) works~\cite{stylegan-v, mocogan-hd, digan} treat video generation as a task of generating a sequence of temporally coherent frames. Although such networks have advanced the SOTA to generate high-quality frames (such as carefully crafted eyes, nose, and mouth for talking-head videos), they come with a major limitation: They rely on an image space. This limits the application of the learned space to image-based operations such as animating images and editing on frames. Direct operations on videos, such as interpolating intermediate videos between two videos and generating future segment of a video, become difficult. 
This is because such operations require learning the set of frame and motion constraints and ensuring that they are coherently learned.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{images/banner._V18.pdf}
    \caption{Demonstrating the continuity of the video space learned by INR-V by interpolating novel videos between two real videos $V_1$ and $V_2$. Note that content (identity, hair) and motion (pose, expressions) gradually transition as we traverse the trajectory in the latent space between $V_1$ and $V_2$'s latents.}
  \label{fig:banner-interpolation}
\end{figure}

\begin{wrapfigure}[23]{R}{0.4\textwidth}
\centering
\vspace{-21pt}
\includegraphics[width=\linewidth]{images/smaller-banner._V28.pdf}
\caption{\textbf{Overview of INR-V:}  INR-V learns a continuous video space by first parameterizing videos as implicit neural representations denoted by $f_{\theta_z}$, where $z$ denotes a unique video instance $V_z$. Next, a meta-network based on hypernetworks denoted by $d_\Omega$ is used to learn a continuous representation over the neural representations. $d_\Omega$ is conditioned by an underlying continuous video space where each point denotes the condition for a complete video.}
\label{fig:arch-teaser}
\end{wrapfigure}

We propose that videos can be represented as a single unit instead of being broken into a sequence of images. 
One can learn a latent space where each latent point represents a complete video. 
However, with existing video generator architectures, 
such representations are difficult. Firstly, such a video generator would be made of several $3$D convolution operations. As the dimension and length of the video increase, such an architecture would become drastically computationally expensive (a GPU with limited memory can only fit a video of limited dimension). Secondly, videos are high-dimensional signals spanning both spatial and temporal directions. Representing such a highly expressive signal by a single latent point would require complicated generator architectures and a very high-dimensional latent space. Instead, videos can be parameterized as a function of space and time using implicit neural representations (INRs). Any point in a video $V_{hwt}$ can be represented by a function $f_\theta(h, w, t) \rightarrow RGB_{hwt}$ where $t$ denotes the $t^{th}$ frame in the video and $h$, $w$ denote the spatial location in the frame and $RGB$ denotes the color at the pixel position $\{h,w,t\}$. Here, the dynamic dimension of videos (a few million pixels) is reduced to a constant number of weights $\theta$ (a few thousand) required for the parameterization. A network can then be used to learn a prior over videos in this parameterized space. This can be obtained through a meta-network that learns a function to map from a latent space to a reduced parameter space that maps to a video. A complete video is thus represented as a single latent point.

We propose INR-V, a video generator network with a continuous video representation space based on learning an implicit neural representation for videos. It is illustrated in Fig.~\ref{fig:arch-teaser}. INR-V is made of key elements that, when combined, makes it ideal for video representation: (1) 
Its INR is free of expensive convolutional layers (millions of parameters) such as in the existing architectures \cite{mocogan-hd, stylegan-v} and relies on a few layers of traditional multi-layered perceptrons (MLPs), leading to a very few parameters (a few thousand).
(2) Having very few parameters, INR's weights can be populated using a secondary meta-network called hypernetwork~\cite{hypernetwork}
that learns a continuous function over the INRs by getting trained on multiple video instances.
(3) It is trained on a deterministic distance loss, such as Euclidean or Manhattan distance. This allows INR-V to learn the exact requirements of a coherent video directly from the ground truth video instances.

Hypernetworks have seen wide applications in graphics~\cite{siren, lfns, 3dhypernet1, 3dhypernet2}; however, they have been seldom used for videos. 
Hypernetworks are notoriously unstable \cite{} to train, especially on the parameterizations of highly expressive signals like videos. Thus, we propose a key prior regularization and a progressive weight initialization scheme to stabilize the hypernetwork training allowing it to scale quickly to more than $30$,$000$ videos. As we show in the experimental section, INR-V demonstrates an expressive and continuous video space by getting trained on these datapoints. 
The learned prior enables several downstream tasks such as novel video generation, video inversion, future segment prediction, and video inpainting directly at the video level. As shown in Fig.~\ref{fig:banner-interpolation}, INR-V also showcases smooth interpolation of novel videos between two videos by traversing the path between their latent points. Interpolation morphs different identities and motions and generates coherent videos. Interestingly, the properties demonstrated in this work are not enforced at training but are natural outcomes of the continuous video space. 
To summarize, our contributions in this work are as follows: 
\begin{enumerate}
    \item We propose considering videos as a single unit and learning a continuous latent space for videos where each latent point represents a complete video. 
    \item We propose INR-V, a video representation technique that parameterizes videos using INRs, bringing down the dimension of a video from a dynamic few million to a constant few thousand. INR-V uses a hypernetwork as a meta-network to learn a continuous space over these parameterizations.
    \item We demonstrate the benefit of a key regularization and progressive weight initialization scheme to stabilize the hypernetwork training. We scale the hypernetworks to more than $30$,$000$ video points enabling it to learn a continuous meaningful latent space over the INRs. 
    \item Lastly, we demonstrate key properties of the learned video space, such as video interpolation, video inversion, and so on, by conducting several experiments and evaluations. 
\end{enumerate}

\section{Related Work}
\label{related_work}

\textbf{Video Generation.} Video generation aims to produce novel videos from scratch. It falls under the paradigm of `video synthesis' that encompasses several categories, including (1) Video prediction~\cite{transformersynth, ccvs, vqvaevideopred}: that predicts the next set of frames given the current frames, (2) Frame interpolation~\cite{videointerpolationassymetric, videointerpolationsoftmaxsplatting, videointerpolationadaptiveseperableconvolution,
videointerpolationwithouttemporalpriors}: that interpolates frames between given frames of a video. These tasks generate the unseen portion of the video based on the context of the seen portion. 
On the other hand, video generation produces videos without any expressive prior context, making the task more challenging. The complexity of the problem has led to a plethora of works in this area~\cite{mocogan-hd, mocogan, stylegan-v, dvd-gan, tgan, digan}. VideoGPT~\cite{videogpt} tackled this challenge by first reducing the raw videos of up to $128 \times 128$ dimension to a quantized space. It then trained a transformer architecture to model a prior over the quantized space. Our architecture is conceptually similar to VideoGPT, which used a likelihood-based generative model to learn a video prior. However, VideoGPT operates on a quantized space that is discontinuous, making the prior less expressive. INR-V, on the other hand, models a continuous video space. VideoGPT also consists of $3$D convolution layers making the model computationally expensive for larger videos. INR-V is a simple MLP, based on a continuous parameterization scheme of INRs, making it agnostic to the video dimension. This allows scaling to multiple resolutions ($64 \times 64$ or $256 \times 256$) at inference without any architectural changes or finetuning. 
More recent works StyleGAN-V~\cite{stylegan-v}, DIGAN~\cite{digan}, and MoCoGAN-HD~\cite{mocogan-hd} are a GAN-based setup that model videos as a temporally coherent trajectory over an image space. 
Use of a continuous representation space for videos has been considered before in \cite{continuous_action_classification, continuous_action_classification_1} for the task of action classification. However, in this work, we focus on learning a representation space for generative tasks.

\textbf{Hypernetworks.} Hypernetworks~\cite{hypernetwork} were introduced as a metafunction that initializes the weights for a different network called the primary network. Hypernetworks have been widely used for several purposes, starting from representation learning for continuous signals~\cite{deepsdf, lfns, metasdf, siren, occunet, 3dhypernet2}, compression~\cite{hncompress1, hncompress2}, few-shot learning~\cite{hnfewshot1, hnfewshot2}, continual learning~\cite{continualhypernet}, language modeling~\cite{hnlangmodel}.
We use hypernetworks to populate our primary video generation network, an MLP parameterizing different video instances. 

\textbf{Implicit Neural Representations.} In this paradigm, a continuous signal is represented by a neural network. 
INRs have had wide adaptations in 3D Computer Vision~\cite{deepsdf, inr1, inr2, occunet, lfns, nerf} and Computer Graphics~\cite{adnerf, dfanerf}. Recently, INR was adopted for images~\cite{inr-gan} and videos~\cite{nerv, siren, digan, videoinr}. INR-GAN~\cite{inr-gan} first showed the application of INRs in generating high-quality images by replacing the generator component of StyleGAN2~\cite{stylegan2} with an MLP-based INR. It then used a hypernetwork to populate the INR. Unlike INR-GAN, which is trained using a stochastic discriminator, INR-V relies on a deterministic distance-based loss to train the hypernetwork. SIREN~\cite{siren} proposed periodic activation functions for INRs
as a replacement for ReLU activation to parameterize many different data types like images, videos, sounds, and $3$D shapes, with fine details. NeRV~\cite{nerv} designed an implicit function as a continuous function of time and used convolution blocks at each time step to parameterize discrete frames showcasing an improved frame quality over SIREN. Recently, VideoINR~\cite{videoinr} was proposed that used INRs for video superresolution. DIGAN~\cite{digan} incorporated INRs made of MLP layers for video generation. It consisted of two separate networks that generated spatial and temporal codes for generating videos in a frame-wise fashion. 
StyleGAN-V~\cite{stylegan-v} also incorporated INRs and relied on continuous non-periodic positional encodings for each timestep of a video.
Like NeRV, StyleGAN-V used traditional convolution operations for frame-by-frame video generation. Both DIGAN and StyleGAN-V used a GAN setup to train the video generators. INR-V is based on MLPs with ReLU activation trained in a fashion similar to Light Field Networks (LFNs)~\cite{lfns}. LFNs proposed a novel neural scene representation for novel view synthesis and trained a hypernetwork over multiple object instances using distance-based losses like Euclidean or Manhattan distance. Like LFNs and INR-GAN, INR-V parameterizes the entire signal (a video) using INRs and relies on a single hypernetwork to generate the INRs. 
However, unlike LFNs and INR-GAN, INR-V encodes a denser representation of a volumetric $3$D signal $\in \mathbb{R}^3$ data making hypernetwork training more challenging.

\section{INR-V: Implicit Neural Representation for Video Synthesis}

\label{method}
Each video instance $V_n$ consists of pixels at locations $(h,w)$ at $t^{th}$ frame. We have a particular parameter vector $\theta_n$ that is used by a network $f$ to generate the value of the color $RGB_{hwt}$ for that pixel location $(h,w,t)$. We need to learn a network $d$ with parameters $\Omega$ that predicts the parameters $\theta_n$ for a particular video $V_n$. Here, $d$ is a hyper-network. The overall approach to train the network is illustrated in Fig.~\ref{fig:arch}.

\subsection{Hypernetwork for Modeling Multiple Video Instances}

As  $f_\theta$ implicitly stores a single video signal, any new video would need its own implicit function. 
Let $f_{\theta_n}$ denote the implicit function for a given video $\{V_n\}_{n=1}^{N}$ where $N$ is the total number of available videos in the training dataset $\mathcal{D}$. Each of these implicit functions, $f_{\theta_n}$ can be modeled using a neural network trained on each pixel value of the video $V_n$. Thus, implicit functions minimize the following objective: 
\begin{equation}
    L(\theta_n) = \frac{1}{T}\frac{1}{W}\frac{1}{H}\sum_{t=1}^{T}\sum_{w=1}^{W}\sum_{h=1}^{H} (f_{\theta_n}(h, w, t) - RGB_{hwt})^2
\label{eqn:single_ftheta}
\end{equation}
Generating a novel video $V_z$ translates to generating a novel implicit function $f_{\theta_z}$ that represents the video meaningfully. Let us consider $f_{\theta_z}$, an unseen sample from an underlying distribution $\Phi$. Each point in the distribution $\Phi$ denotes an implicit function of a meaningful video. To randomly sample $f_{\theta_z}$, we make use of a meta-network to learn the distribution $\Phi$. 

We use a hypernetwork $d_\Omega$ as a meta-network to parameterize $f_\theta$, such that $d_\Omega(m_n) = \theta_n$ for video instance $V_n$. Here $m_n$ is a a $d$-dimensional point in the latent space, say $\tau$, and serves as an instance code for $V_n$.
Given enough number of samples $N$, $d_\Omega$ learns to map the latent codes sampled from $\tau$ to their corresponding parameterized space $\Phi$, as shown in Fig.~\ref{fig:arch}. 
The parameters $\theta_n$ are then used to initialize $f$ to generate $V_n$.

Let us consider $\tau$ as a meta-distribution such that $\{m\}_{\mathcal{D}} \in \tau$. At the time of inference, $m_z$ can be sampled from $\tau$. As $d_\Omega$ has learned a valid representation over $\Phi$, $m_z$ enables $d_\Omega$ to generate a meaningful implicit function $f_{\theta_z} \in \Phi$. Sampling from $\tau$ can be made straight forward by making sure $\tau$ is regularized during training. 
At the time of training, $\Omega$ and $\{m_n\}_{n=1}^{N}$ are optimized together. 
$\theta$ is a non-learnable parameter and $f$ is initialized as the output of $d_\Omega$. 
The following objective is optimized:
\begin{equation}
    L(\Omega, m) = \frac{1}{N}\frac{1}{T}\frac{1}{W}\frac{1}{H}\sum_{n=1}^{N}\left(\sum_{t=1}^{T}\sum_{w=1}^{W}\sum_{h=1}^{H} (f_{\theta_n}(h, w, t) - RGB_{ijk})^2\right)
    \label{eq:main_equation} \quad \textrm{and} \quad
    \theta_n = h_\Omega(m_n)
\end{equation}

\subsection{Regularizing $\tau$ for Hypernetwork Conditioning}
\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{images/arch-29.pdf}
  \caption{\textbf{Architecture of INR-V: }Any video instance $V_n$ is represented by its corresponding implicit neural representation, an MLP, $f_{\theta_n}$. $f_{\theta_n}$ takes a grid as input denoting the pixel positions of the video encoded using periodic positional encodings. It then generates the pixel values for all the positions. $f_{\theta_n}$ is initialized by a meta-network called hypernetworks denoted as $d_\Omega$ composed of a set of MLPs. $d_\Omega$ is conditioned by an instance code $m_n$ unique to every video instance $V_n$. $m_n$ is trained by combining (1) auto-decoding framework to regress to a code $c_n$ and (2) encoding-framework to regularize the space using CLIP embedding that generates  $V_n$'s semantic code $g_n$. At the time of inference, $m_n$ is randomly sampled from an underlying learned distribution $\tau$.}
  \label{fig:arch}
\end{figure}

\begin{wrapfigure}[17]{R}{0.35\textwidth}
\centering
\vspace{-15pt}
\includegraphics[width=\linewidth]{images/video-clip._V5.pdf}
\caption{\textbf{Video-CLIP}: Encoding a video $V_n$ to a latent vector $g_n$ by using image-level CLIP encodings.}
\label{fig:videoclip}
\end{wrapfigure}
To generate a novel video, a random latent $m_z$ is sampled from the latent space $\tau$. $d_\Omega$ is then conditioned on $m_z$ generating an implicit function $f_{\theta_z} \in \Phi$. 
In a standard hypernetwork training~\cite{3dhypernet2, lfns, deepsdf, siren}, $m_n$ is optimized in an auto-decoding framework as given in Eqn.~\ref{eq:main_equation}. However, given the complexity of the signal $V$ (a $3$D volumetric representation) that $d_\Omega$ has to model, $\{m\}_{\mathcal{D}}$ can collapse to a single point if $\tau$ is not regularized at the time of training, bringing the expressiveness of $d_\Omega$ down to a single implicit function. 
We regularize $\tau$ by 
leveraging pretrained CLIP~\cite{clip} designed for generating semantically meaningful embeddings for images. We design Video-CLIP that encodes an entire video $V_n$ to a vector $g_n$.
As shown in Fig.~\ref{fig:videoclip}, Video-CLIP first generates the image-level CLIP embeddings. These embeddings are then passed through a bi-directional GRU. The mean of the hidden state outputs of the final layer produces $g_n$. As shown in Fig.~\ref{fig:arch}, the regularized instance code $m_n$ is now:
\begin{equation}
    m_n = \phi(c_n, g_n) 
\end{equation}
where $c_n$ is the instance code of $V_n$ optimized in an auto-decoding fashion at the time of training, and $\phi$ is a neural network. The pretrained CLIP embeddings are kept frozen during training and the learnable parameters are the instance codes $c_n$ that are regularized by $g_n$. CLIP regularization encourages the latent codes to be spaced sufficiently apart by leveraging predefined semantic encoding. This helps avoid mode collapse during the initial stages of training. 
Please find the ablation on CLIP regularization in Appendix~\ref{sec:ablation-appendix}.

\subsection{Progressive Training}
\label{sec:progressive-training}

A video is a dense $3$D volume mandating its neural representation to model every single point in the volume. Although implicit representations have a constant number of parameters made of only a few layers of MLPs in our case, learning a meta-function using a hypernetwork over such dense representations is challenging. As a result, if not appropriately initialized, the hypernetworks can easily collapse to a single representation despite CLIP regularization. Moreover, a sub-optimal hypernetwork initialization could result in a significantly longer convergence period. To tackle this challenge, we adopt a progressive initialization scheme. Firstly, the training is divided into multiple stages. Each stage, denoted by $\{l\}_{l=1}^{\mathcal{K}}$ where $\mathcal{K}$ is the total number of stages, is made of a subset of the training dataset $\mathcal{D}$. The number of samples $N_{l}$ in each stage $l$ is given as:
\begin{equation}
    \begin{split}
    N_{l} = 
    \begin{cases}
      \; N_{l-1} + \epsilon_{l} & l > 1\\
      \; \mathcal{C} & l = 1\\
    \end{cases}  
    \end{split}
\end{equation}
where $\mathcal{C}$ is a constant and $\epsilon_{l}$ denotes the number of additional samples for $l$\textsuperscript{th} stage. Each step $l$ consists of $\{V_n\}_{n=1}^{N_{l}}$ datapoints that is computed as:
\begin{equation}
     \{V_n\}_{n=1}^{N_{l}} = \{V_i\}_{i=1}^{N_{l-1}} + \{V_j\}_{j=N_{l-1}}^{N_{l-1} + \epsilon_l}
\end{equation}
where the order of set $\{V\}$ is maintained across the training stages. At the start of the training, the model is trained on $\mathcal{C} < 10$ examples. This allows the hypernetwork to quickly adapt to the handful of examples and initialize the weights. However, jumping from $\mathcal{C}$ to $\sim 30$,$000$ samples causes the network to collapse again. Thus, we adapt the network progressively to the given examples. Each stage of the progressive training is a full training of the model, with the weights in the current stage initialized with the weights learned from the previous stage. 
This includes reusing the instance codes $c_n$ learned at a previous stage $l-1$ in the current stage $l$. This step is crucial, as without this, the hypernetwork is pushed to re-learn all the instance codes. 
The new instance codes added in the current stage are initialized from a Gaussian distribution.

\section{Experiments}

\textbf{Experimental Setup: }
\label{exp_setup}
We perform our experiments on (1) How2Sign-Faces~\cite{how2sign}, (2) SkyTimelapse~\cite{skytimelapse}, (3) Moving-MNIST~\cite{moving-mnist}, and (4) RainbowJelly~\cite{stylegan-v}. Real video samples of each dataset are visualized in Appendix Fig.~\ref{fig:realexamples}. How2Sign~\cite{how2sign} is a full-body sign-language dataset consisting of $11$ signers. The signers have elaborate facial expressions, mouth, and head movements. We modify How2Sign to How2Sign-Faces by cropping the face region out of all the videos and randomly sample $10$,$000$ talking head videos, each of at least $25$ frames, of dimension $128 \times 128$. SkyTimelapse~\cite{skytimelapse} consists of scenic videos of sky changes. It is made of $1803$ videos, each at least $25$ frames. The videos are first center-cropped to $360 \times 360$ from an original dimension of $360 \times 620$ and then resized to $128 \times 128$ for training. Moving-MNIST~\cite{moving-mnist} is a video dataset of moving MNIST digits containing a total of $10$,$000$ datapoints. Each video is $20$ frames long.  
RainbowJelly is a single underwater video capturing colorful jellyfishes. The video is first extracted into frames which are then divided into videos of $25$ frames each, making a total of $34$,$526$ videos.
Similar to SkyTimelapse, the videos are first center cropped to $360 \times 360$ and then resized to $128 \times 128$.

All experiments are performed on 2 NVIDIA-GTX 2080-ti GPUs with 12 GB memory each. All models, except INR-V, are trained at a resolution of $128 \times 128$. To make training computationally efficient, INR-V is trained on a lower resolution of $100 \times 100$ videos. Based on INRs, INR-V can infer directly at multiple resolutions (please refer section~\ref{sec:multi-res-section}). For evaluations and comparisons, INR-V is inferred at $128 \times 128$ like the other models.  
The training setup and model architecture are explained in Appendix~\ref{sec:appendix-archdets}\def\thefootnote{$^1$}\footnote{The codebase, dataset, and pretrained models can be found at \textcolor{blue}{\href{https://skymanaditya1.github.io/INRV}{https://skymanaditya1.github.io/INRV}}}.


\subsection{Comparing INR-V with Single-INR}
\begin{table}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{r|ccc|ccc|cc}
   \toprule
   & \multicolumn{3}{c|}{Single-INR} & \multicolumn{5}{c}{INR-V} \\
   Dataset &  $\mathcal{E}$ $\downarrow$ & PSNR$_{50}$ $\uparrow$ & SSIM$_{50}$ $\uparrow$ &  $\mathcal{E}_{50}$ $\downarrow$ & PSNR$_{50}$ $\uparrow$ & SSIM$_{50}$ $\uparrow$ & PSNR$_{\textrm{FULL}}$ $\uparrow$ & SSIM$_{\textrm{FULL}}$ $\uparrow$ \\
   \midrule
   How2Sign-Faces & $4.83$ & $29.72$ & $0.925$ & $8.29$ & $25.69$ & 0.850 & $25.84$ & $0.869$ \\ 
   SkyTimelapse & $4.69$ & $36.19$ & $0.943$ & $5.87$ & $33.69$ & $0.931$ & $33.94$ & 0.924 \\
   Moving-MNIST & $3.57$ & $37.26$ & $0.978$ & $6.06$ & $29.81$ & $0.949$ & $29.54$ & $0.975$ \\
   RainbowJelly & $4.17$ & $35.93$ & $0.918$ & $5.02$ & $33.34$ & $0.937$ & $33.57$ & $0.938$\\
   \bottomrule
\end{tabular}\end{adjustbox}
  \caption{\small Quantitative metrics on reconstruction quality. Comparison set is made of $50$ videos per training dataset. INRs trained individually for each video is denoted as Single-INR. INR-V trains a single hypernetwork $d_\Omega$ to populate the INRs of all the videos in the training dataset. PSNR$_{50}$ and SSIM$_{50}$ are computed on the comparison set, PSNR\textsubscript{FULL} and SSIM\textsubscript{FULL} are computed on the entire training set. $\mathcal{E}$ is computed on videos with pixel range $[0, 255]$. INR-V performs comparably with Single-INR despite getting trained on huge datasets of more than $30$,$000$ videos.}
  \label{tab:inrvsinrv}
\end{table}

\begingroup
\label{tab:metrics}
\begin{table*}[t]
\centering
\begin{tabularx}{0.83\textwidth}{r|cccc}
  \toprule
    Method & How2Sign-Faces & SkyTimelapse & Moving-MNIST & RainbowJelly \\
    \midrule
    MoCoGAN-HD & 396.53 & 321.44 & 296.95 & 1856.21 
    \\
    DIGAN & 165.89 & 135.60 & 144.97 & 408.19
    \\
    StyleGAN-V & 94.64 & \textbf{85.05} & 109.85 & 1227.70
    \\
    \midrule
    INR-V & 161.68 & 153.42 & 103.24 & \textbf{260.72} 
    \\
    $+$ Denoising & \textbf{87.22} & - & \textbf{47.28} & -
    \\
    \bottomrule
  \end{tabularx}
  \caption{ FVD$_{16}$ metrics computed on random videos generated by the respective models. }
  \label{tab:main_results}
\end{table*}
\endgroup

INR-V uses hypernetworks to learn a distribution over the INRs of videos. A single hypernetwork $d_\Omega$ can initialize the INRs for multiple videos $\{V_n\}$ based on their respective instance codes $m_n$. Thus, measuring if $d_\Omega$ generates the INR functions $f_\theta$ accurately is crucial.
We evaluate this using a set of $50$ randomly sampled videos from the training dataset.  
Each video is first trained to fit a single INR function $f_{\theta_n}$ using Eqn.~\ref{eqn:single_ftheta} denoted as Single-INR. Next, the INRs of these $50$ videos are populated using a pretrained hypernetwork $d_\Omega$ trained on the entire dataset. We measure the reconstruction quality with PSNR (Peak Signal to Noise Ratio), SSIM (Structural SIMilarity), and the error as:
\begin{equation}
     \mathcal{E} = \left( \frac{1}{50}\sum_{n=1}^{50}\frac{1}{HWT}(V_n^{'} - V_n)^2\right)^\frac{1}{2}
    \label{eqn:error-inr-inrv}
\end{equation}
where $V_n^{'}$ denotes the video generated using the implicit function $f_\theta$. Single-INR was optimized for $750$ steps using Eqn.~\ref{eqn:single_ftheta} taking $\sim5.56$ minutes for each video ($\sim4.63$ hours for $50$ videos).
Table.~\ref{tab:inrvsinrv} presents quantitative metrics on the videos reconstructed using Single-INR and INR-V. PSNR\textsubscript{FULL} computes the PSNR on the entire training dataset, PSNR$_{50}$ computes the metric on the selected $50$ videos for comparison. 
As can be seen, 
although hypernetwork $d_\Omega$ is trained on huge datasets, it performs comparably with Single-INR. For RainbowJelly, it even outperforms Single-INR in SSIM metric and performs at par on SkyTimelapse. 
This indicates that $d_\Omega$ has learned to accurately generate INRs for complex spatio-temporal signals. Thus, INR-V can be used as a compression technique to compress $1000$s of videos with minimal loss in perceptual quality. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{images/examples._V11.pdf}
  \caption{\small Examples of random videos generated on, from top to bottom, How2Sign-Faces~\cite{how2sign}, SkyTimelapse~\cite{skytimelapse}, RainbowJelly, and Moving-MNIST~\cite{moving-mnist}. For Moving-MNIST, every $2$\textsuperscript{nd} frame of $20$ frames long videos and for other datasets, every $3$\textsuperscript{rd} frame of $25$ frames long generated are shown. Moving-MNIST and How2Sign-Faces are passed through VQVAE2 denoising network as described in Section~\ref{sec:video-gen-compare}}
  \label{fig:main_results}
\end{figure}
 
\subsection{Comparing INR-V with SOTA video generation networks}
\label{sec:video-gen-compare}

\textbf{Overview: } Fig.~\ref{fig:main_results} and Table~\ref{tab:main_results} present qualitative and quantitative comparisons respectively between MocoGAN-HD~\cite{mocogan-hd}, DIGAN~\cite{digan},  StyleGAN-V~\cite{stylegan-v}, and INR-V. 
All models were trained from scratch. 
As we train the models on smaller datasets of $\sim10$,$000$ datapoints, MoCoGAN-HD is trained on StyleGAN2-ADA~\cite{stylegan2-ada} image-generator backend. For each model, the best-performing checkpoint is selected for comparison. 

\textbf{Evaluation:} As can be seen in Fig.~\ref{fig:main_results}, INR-V generates novel videos with coherent content and motion. MoCoGAN-HD fails to maintain the identity in a single video instance. 
For quantitative evaluation, we use the Frechet Video Distance (FVD) metric as implemented by StyleGAN-V. FVD{$_{16}$} is computed on $2048$ videos of $16$ frames sampled at a resolution of $128 \times 128$. As can be seen in Table~\ref{tab:main_results}, INR-V outperforms the existing networks on Moving-MNIST and RainbowJelly and performs comparably on the remaining datasets. 

\textbf{Enhancing INR-V's Visual Quality} 
Enhancing image and video quality has been an area of extensive research~\cite{gpen, tecogan, vrt, iseebetter} with many breakthroughs.
We propose that video generation can be partitioned into two stages (1) generating coherent content and motion (2) enhancing the visual quality. Note that, in the current work, our effort has been (1) to propose a novel continuous representation space for videos. 
We demonstrate (2) by developing a simple denoising network using a standard VQVAE2~\cite{vqvae2}. We train VQVAE2 as a frame-by-frame denoising autoencoder making one minor change: Instead of reconstructing the given low-quality input, we use the high-quality frame for computing the error. The low-quality inputs are the intermediate video instances reconstructed by INR-V during training. We train denoising VQVAE2 on How2Sign-Faces and Moving-MNIST. Appendix Fig.~\ref{fig:deblurnet} demonstrates the results of the denoising network on blurry instances generated by INR-V. 
As can be seen from the quantitative metrics in Table.~\ref{tab:main_results}, using an additional denoising network improves the network's performance by $\sim 2 \times$. 

\section{Applications of the continuous video space learned by INR-V }

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{images/interpolation.V10.pdf}
    \caption{\textbf{Each cell displays $12$\textsuperscript{th} frame of $25$ frames long generated videos}. The videos demonstrate interpolation between two given videos (in red boxes) by traversing along a trajectory in the latent space connecting the latent points of the given videos. Here, MoCoGAN$+$ and StyleGAN$+$ denote MoCoGAN-HD and StyleGAN-V. White boxes indicate a sudden transition in content (e.g. identity) or motion (e.g. pose).
    }
  \label{fig:interpolation}
\end{figure}

INR-V learns a continuous latent representation for videos allowing complex spatio-temporal video signals to be represented using a single latent point. In this section, we showcase the advantage of such a latent space through several demonstrated properties and comparisons. We also benchmark several tasks based on the inversion of $256$ videos on How2Sign-Faces using full and incomplete video context. 

\subsection{Video Interpolation}
\label{sec:video-interpolation}

\begin{wraptable}[8]{R}{5.5cm}
\vspace{-5pt}
\centering
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{ccc}
\toprule  
MoCoGAN-HD & DIGAN & StyleGAN-V
\\ \midrule
100.00 & 89.43 & 95.24\\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\small Video interpolation user study: \% of times INR-V interpolation was preferred over existing models.}
\label{tab:user-study-interpolation}

\end{wraptable}

Given two videos $V_1$ and $V_2$, a continuous video space should be able to make a gradual transition between the two videos such that every point along the trajectory between the two (1) produces a meaningful video and (2) shares content and motion properties from $V_1$ and $V_2$. We demonstrate this property in Fig.~\ref{fig:banner-interpolation} and Fig.~\ref{fig:interpolation} with Spherical Linear Interpolation (Slerp)\def\thefootnote{$^2$}\footnote{\href{https://splines.readthedocs.io/en/latest/rotation/slerp.html}{https://splines.readthedocs.io/en/latest/rotation/slerp.html}}. Each cell in Fig.~\ref{fig:interpolation} demonstrate the $12$\textsuperscript{th} frame of the $25$ frames long videos. 
As can be seen, INR-V observed a gradual change in motion (pose, mouth movements, expressions, cloud shift) and content (identity, visibility of sun). The interpolated videos are spatio-temporally coherent (best seen in the supplementary video). Appendix Fig.~\ref{fig:interpolation-h2g-appendix} and Fig.~\ref{fig:interpolation-sky-appendix} demonstrate the spatio-temporal transition on How2Sign-Faces and SkyTimelapse. As we represent an entire video in a single point in the continuous video space, interpolation is a natural operation that can be performed with INR-V. 

\textbf{Comparisons: } Existing models have different motion and content codes; thus, to interpolate videos, intermediate content codes were interpolated between two videos by Slerp interpolation. INR-V does not have separate motion and content vectors; thus, videos can be interpolated directly using given video's latent points. As shown in Fig.~\ref{fig:interpolation}, INR-V has a gradual transition in motion and content. For How2Sign-Faces, StyleGAN-V abruptly changes motion (cell 5-7), and DIGAN abruptly switches identity (cell 1-2, cell 5-6). This effect is highlighted in white boxes. This is expected as both of these architectures operate in the image space, and thus a gradual spatio-temporal transition is harder to achieve. We performed a user study on $30$ users to qualitatively evaluate the interpolation quality of INR-V against the SOTA models and report the metrics in Table.~\ref{tab:user-study-interpolation}. 
INR-V interpolation was randomly shown against either of the other three models. 
The users provided their preference on which interpolation looked smoother in terms of transition in content and motion. INR-V was preferred at least 85\% more than all the SOTA networks. This demonstrates the continuous nature of the video space learned by INR-V. 

\subsection{Multi-Resolution and Multi-Length Inference}
\label{sec:multi-res-section}
\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{images/multi-resolution._V6.pdf}
  \caption{\textbf{INR-V direct inference on multiple resolutions and frame length}.  INR-V trained on only $25$ frames long $100 \times 100$ videos. Novel videos of multiple resolutions ($64 \times 64$, $128 \times 128$, $256 \times 256$) and video length ($50$) are directly generated on the trained model without any architectural change or finetuning. The images are not upto scale, please refer Appendix Fig.~\ref{fig:multi-resolution-appendix} for scaled representation.}
  \label{fig:multi-scale}
\end{figure}















In Fig.~\ref{fig:multi-scale} we show INR-V trained on videos of only $100 \times 100$ resolution with $25$ frames per video, generating novel videos of multiple resolutions and lengths, maintaining the content and motion quality of the output. An underlying property of INRs is a continuous representation of the signal given as $f_\theta(h, w, t) \rightarrow$ RGB. This enables the model to understand a continuous property of the signal making it agnostic to the dimension. We show quantitative metrics on INR-V inferred at multiple resolutions and compare INR-V with existing SOTA superresolution techniques~\cite{videoinr} in Appendix~\ref{sec:appendix-superresolution}.  



\subsection{Video Inversion and its applications}
Inversion has been widely adopted in many applications prominently for images. StyleGAN2~\cite{stylegan2} is extensively used for image inversion enabling many downstream image editing tasks such as changing the emotion, age, or gender of a given face. 
In video inversion, we aim to invert a given video back into the latent space of  a pretrained video generation network. Existing methods perform frame-by-frame inversion to individually invert the context code for each frame and the motion code for the video. 
In INR-V, we only need to invert to a single latent code that can be achieved through a simple optimization objective:
\begin{equation}
    \begin{gathered}
        \underset{m_z}{\mathrm{argmin}} \frac{1}{T}\frac{1}{W}\frac{1}{H}\sum_{T}^{t=1}\sum_{W}^{w=1}\sum_{H}^{h=1} (f_{\theta_z}(h, w, t) - {RGB}_s)^2 \quad \textrm{where} \quad \theta_z = h_\Omega(m_z)
    \end{gathered}
    \label{eqn:video-inversion}
\end{equation}
where $m_z$ is the latent point for a video instance $V_z$. Fig~\ref{fig:videocompletion} shows the qualitative demonstration of INR-V inversion trained on How2Sign-Faces for a video outside of the training dataset $\mathcal{D}$. 

\begin{wrapfigure}[36]{r}{0.49\textwidth}
\centering
\vspace{-12pt}
\includegraphics[width=\linewidth]{images/video-completion._V5.pdf}
\caption{\small \textbf{Video Inversion and it's applications.}
INR-V can be directly used for several tasks by simply inverting a video to its latent point based on the given context. We demonstrate some qualitative results.}
\label{fig:videocompletion}
\end{wrapfigure}

\textbf{Video Completion:} Key categories of `video synthesis' include future frames prediction (future prediction), completing the video between frames (frame interpolation), and predicting the missing part of the video (video inpainting). In INR-V, a video $V_z$, represented by a single latent code $m_z$ can be generated without any additional knowledge. Thus, all the above operations can be performed using a modified optimization operation based on Eqn~\ref{eqn:video-inversion} on the seen part of the video given as:
\begin{equation}
    \begin{gathered}
        \underset{m_z}{\mathrm{argmin}} \frac{1}{S} (f_{\theta_z}(h_s, w_s, t_s) - {RGB}_s)^2\\  \textrm{and} \quad \theta_z = h_\Omega(m_z)
    \end{gathered}
    \label{eqn:half-inversion}
\end{equation}
where $S$ is the number of context points, $h_s$, $w_s$, and $t_s$ are the context points of $V_z$ seen at the time of optimization. 
With the optimized $m_z$, the full video can simply be generated back with INR-V. Fig.~\ref{fig:videocompletion} demonstrates the results for the various operation on a video outside of $\mathcal{D}$ with $\sim2.5$ minutes of optimization on a single $12$ GB NVIDIA GTX 2080ti GPU. As can be seen, the network is able to regress to a latent corresponding to the given identity while preserving finer details like spectacles, mouth shape, pose, etc. In the case of `Video Inpainting', the network understands the person's pose.
For `Frame Prediction', although the pose does not match the ground truth, the overall video is coherent. In `Frame Interpolation', the model is able to generate a coherent context between two frames, including the pose, expressions, identity, and mouth movements. In `Sparse Inpainting', 
we randomly set $25\%$ of all the video pixels as the context points for optimization. Even with very sparse context, INR-V is able to regress to the correct specifications including the finer content details.


\textbf{Video Superresolution through inversion:} Video Superresolution is the task of enhancing the resolution of a given video.
Recent works such as ~\cite{tecogan, vrt, framerecvideosr, iseebetter, edvr, videoinr} have made significant progress in video superresolution, showcasing $4\times$ enhancement. INR-V can directly superresolve seen video instances as showcased in Appendix Table.~\ref{tab:superresolution}. For unseen instances, combining the capability of video inversion and multi-resolution video generation, INR-V can superresolve a given video $V_z$ of a lower resolution (say $32 \times 32$) simply as following: (1) Invert $V_z$ at the smaller resolution to obtain $m_z$. (2) Render $V_z$ from $m_z$ directly at a higher resolution (say $256 \times 256$). In Fig.~\ref{fig:videocompletion}, we demonstrate the qualitative results on a video outside the training dataset. The video was optimized at $32 \times 32$ for $\sim2.5$ minutes.
The inverted video was then superresolved at a scale factor of $8\times$ to $256 \times 256$. Additional details are present in Appendix~\ref{sec:appendix-superresolution}.


\begin{wraptable}[21]{R}{12cm}

\centering
\vspace{5pt}

\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{l | l | cccccc| c}
\toprule  
Task & Method & GT-ID $\uparrow$ & TL-ID $\uparrow$ & TG-ID $\uparrow$ & Context-L1 $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & Cost $\downarrow$ \\ \midrule
\multirow{3}{*}{Inv.} & DIGAN & 0.652 & 0.953 & 0.9599 & 45.08 & 19.59 & 0.653 & $\sim$ 4.25\\
 & Style-V & \textbf{0.804} &\textbf{ 0.985} & \textbf{0.998} & 42.16 & 19.65 & 0.665 & $\sim$ 3.25 \\
 & INR-V & 0.770 & 0.950 & 0.950 & \textbf{5.25} & \textbf{21.21} & \textbf{0.773} & $\sim$ \textbf{2.75}\\
 \midrule
 
\multirow{2}{*}{Inp.} & DIGAN & 0.628 & \textbf{0.960} & \textbf{0.969} & 45.80 & - & - & $\sim$ 4.25 \\
 & INR-V & \textbf{0.758} & 0.948 & 0.939 & \textbf{4.83} & - & - & $\sim$ \textbf{2.75}\\
 \midrule
 
\multirow{2}{*}{Pre.} & DIGAN & 0.603 & 0.940 & 0.928 & 40.26 & - & - & $\sim$ 4.25 \\
& INR-V & \textbf{0.703} &\textbf{ 0.946} &\textbf{ 0.932} & \textbf{4.72} & - & - & $\sim$ \textbf{2.75} \\
\midrule
\multirow{2}{*}{Int.} & DIGAN & 0.653 & 0.925 & \textbf{0.921} & 48.66 & - & - & $\sim$ 4.25 \\
& INR-V & \textbf{0.702} &\textbf{ 0.928} & 0.905 & \textbf{7.46} & - & - & $\sim$ \textbf{2.75} \\
\midrule
\multirow{2}{*}{Spr.} & DIGAN & 0.718 & 0.961 & 0.967 & 46.24 & 19.74 & 0.671 & $\sim$ 4.25 
\\
& INR-V & \textbf{0.768} & \textbf{0.968} & \textbf{0.974} & \textbf{5.29} & \textbf{22.35} & \textbf{0.774} & $\sim$ \textbf{2.75}  \\
\midrule
\multirow{3}{*}{\makecell{Sup.\\$4\times$}} & Bicubic & 0.808 & 0.923 & 0.903 & - & 28.36 & 0.906 & -  \\
& VideoINR & \textbf{0.939} & \textbf{0.982} & \textbf{0.974} & - &\textbf{ 32.86} & \textbf{0.957} & - \\
& INR-V & 0.734 & 0.911 & 0.903 & 4.92 & 21.94 & 0.742 & $\sim$ \textbf{2.75} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\small Comparison of INR-V on various video inversion tasks: Video Inversion (Inv.), Video Inpainting (Inp.), Frame Prediction (Pre.), Frame Interpolation (Int.), Sparse Interpolation (Spr.), and Superresolution (Sup.). Comparison set is made of $256$ videos outside of the training dataset. Metrics used for evaluation is explained in Sec.~\ref{sec:video-inversion-quant}. Cost denotes the time to optimize a single video instance in minutes.}
\label{tab:video_inversion}
\end{wraptable} 


\textbf{Quantitative Evaluation: }
\label{sec:video-inversion-quant}
To quantify the performance of INR-V, we prepare a comparison set by randomly sampling $256$ videos outside of the training set. 
We compare against DIGAN on the tasks of Video Inversion, Video Inpainting, Frame Prediction, Frame Interpolation, and Sparse Interpolation and against StyleGAN-V on the task of Video Inversion. Since DIGAN is based on INRs, it can invert incomplete frames, however, StyleGAN-V expects a full frame for backpropagation. Thus we do not compare with StyleGAN-V on the other tasks.
For the task of Superresolution, we compare against Bicubic Upsampling and VideoINR at a scale factor of $4\times$ from $32\times 32$ to $128\times 128$.

We evaluate on the following metrics: (1) PSNR, (2) SSIM, (3) Temporally Locally (TL-ID) and Temporally Globally (TG-ID) Identity Preservation, (4) Context-L1, and (5) Ground Truth Identity (GT-ID) Match. TL-ID and TG-ID were proposed in ~\cite{stit}. They evaluate a video's identity consistency at a local and global level. For both metrics, a score of 1 would indicate that the method successfully maintains the identity consistency of the original video. Context-L1 computes the L1 error on the inverted videos at the given context points. An error of $0$ would indicate that the inversion is perfect. GT-ID measures the match in identity between the ground truth and the inverted video. DeepFace\def\thefootnote{$^3$}\footnote{\href{https://github.com/serengil/deepface}{https://github.com/serengil/deepface}} face features are extracted for both the videos, and the cosine similarity is computed between the extracted features.
Since there is no single correct prediction for tasks like `Future Frame Prediction', `Frame Interpolation', and `Video Inpainting', we do not evaluate these tasks on PSNR and SSIM.

As can be seen, INR-V outperforms all the existing networks in most of the metrics on video inversion and the proposed inversion tasks, except `Superresolution', indicating the advantage and robustness of the proposed space. 
For the task of Superresolution, INR-V performs comparably with Bicubic and VideoINR. However, unlike these works that directly superresolve a video, INR-V first inverts the low resolution video to generate a high resolution video. 
Such a mechanism opens several possibilities, such as inverting a low resolution incomplete video (missing frames due to corruption) to a high resolution video with full context.

\section{Conclusion}
\label{conclusion_future_work}

We present INR-V, a continuous video representation network.
Unlike existing architectures that extend superior image generation networks for generating videos one frame at a time, we use implicit neural representations to parameterize videos as complete signals allowing a meta-network to encode it to a single latent point. Given enough examples, the meta-network learns a continuous video space as demonstrated through video interpolation and inversion tasks. INR-V generates diverse coherent videos outperforming many existing video generation networks.
INR-V opens the door to a multitude of video-based tasks and removes the dependency on an image generator. 
To showcase this, we propose several downstream tasks and observe that INR-V outperforms the existing works on a majority of these tasks. This demonstrates the advantages and potential of a continuous video space and we hope to encourage research in this direction. 

\bibliography{tmlr}
\bibliographystyle{tmlr}

\appendix
\section{Appendix}

\subsection{Ablation}

\subsubsection{Effect of Regularization}
\label{sec:ablation-appendix}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{images/clip,noclip._V5.pdf}
  \caption{Qualitative results of CLIP regularization. Intermediate results are shown after $30$ hours of training on 2 NVIDIA GTX 2080ti GPUs. (a) Video reconstruction quality. CLIP regularization enables the meta-network to model the INRs with finer details. (b) Videos generated by random sampling. CLIP regularization improves the quality of the sampled videos and encourages variation in the implicit representations.}
  \label{fig:clipnoclip}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/mode_collapse_combined._V3.pdf}
  \caption {Convergence rate for different regularization schemes on RainbowJelly (left) and SkyTimelapse (right) datasets. RainbowJelly consists of $\sim34$K datapoints and SkyTimelapse consists of $\sim2$K datapoints. As can be observed, SkyTimelapse being a relatively smaller dataset performs equally well on all the regularization schemes. RainbowJelly performs worst without any regularization facing mode-collapse for about the first $\sim75$ hours. Progressive training and CLIP regularization help INR-V converge the fastest.}
  \label{fig:skyrainbowcompletegraph}
\end{figure}

\begin{wrapfigure}[52]{r}{0.45\textwidth}
    \centering
    \vspace{2pt}
    \includegraphics[width=\linewidth]{images/rainbow_regularization_ablation._V1.png}
    \caption{\small Convergence rate for different regularization schemes on RainbowJelly dataset made of $\sim34$K instances. All the models are trained progressively. CLIP Regularization leads to the fastest convergence. }
    \label{fig:rainbowgraph}
    


\vspace{7pt}
    \includegraphics[width=\linewidth]{images/fvd_convergence_codebook_cropped.png}
    \caption{Performance of INR-V when trained on varying number of video instances (codebook size). FVD is computed against the full dataset.}
    \label{fig:codebooksize-rainbow}


\vspace{7pt}
    \includegraphics[width=\linewidth]{images/different_initialization.png}
    \caption{INR-V on progressive training with different initializations: the order of video selection for each stage differ across the initialization.}
    \label{fig:diff-progtraining}
\end{wrapfigure}
In this section, we compare the training time and the performance of INR-V (1) with/without CLIP regularization and (2) with/without progressive training. Fig.~\ref{fig:clipnoclip} presents the qualitative results on INR-V after $30$ hours of training on 2 NVIDIA GTX 2080ti GPUs. Fig.~\ref{fig:skyrainbowcompletegraph} plots the rate of convergence on RainbowJelly (left) and SkyTimelapse (right) datasets on the same training setup. We also show Gaussian regularization with INR-V by adding the following additional loss term to the overall loss term in Eqn.~\ref{eq:main_equation}: 
\begin{equation}
    \delta D_{KL}(\; \mathcal{N}(\mu, \sigma)\; ||\; \mathcal{N}(0, 1) \;)
\end{equation}
where $\mu$ and $\sigma$ denote the mean and standard deviation over the latent codes $\{m_n\}_{n=1}^{N}$ and $\delta$ is a hyperparameter. In our experiments, $\delta = 1.0$.

As can be observed from Fig.~\ref{fig:clipnoclip}, reconstruction quality is much worse without CLIP. This is expected as Video-CLIP (see Fig.~\ref{fig:videoclip}) assigns semantic meaning to the initialized codes for each video instance. As we observe the 
novel video instances generated using this model (Fig.~\ref{fig:clipnoclip}, right), we already see a motion emerging with expressive faces. 
As can be observed from Fig.~\ref{fig:skyrainbowcompletegraph}, on RainbowJelly dataset (made of $34526$ instances), INR-V takes more than $250$ hours ($\sim$ 11 days) to converge without any regularization scheme or progressive training. With progressive training, the convergence time is drastically reduced to less than $180$ hours ($\sim$ 7.5 days). The best performance is achieved when progressive training is done along with CLIP regularization where the convergence occurs in less than $120$ hours ($\sim$ 5 days) on 2 NVIDIA GTX 2080ti GPUs. On SkyTimelapse dataset (made of $1803$ instances), INR-V converges equally on either of the regularization schemes. With a Gaussian prior, we observed a slight advantage in terms of convergence time. Fig.~\ref{fig:rainbowgraph} plots an additional comparison on RainbowJelly with progressive training on three different regularization methods: Gaussian, CLIP, and no regularization.

The graphs indicate that CLIP regularization is more suitable for a larger dataset like RainbowJelly, however for a smaller dataset like SkyTimelapse (Fig~\ref{fig:skyrainbowcompletegraph}, right), Gaussian regularization is more effective. 
INR-V performs equally well on either of the training schemes, given enough time to train. This indicates that the generation capabilities is inherent to the proposed architecture, whereas the different training schemes help in stabilizing the training and thus, lead to a faster convergence. Additional insights are provided in Appendix~\ref{sec:insights-appendix}.

\subsubsection{Effect of the size of the codebook}

Fig~\ref{fig:codebooksize-rainbow} presents a comparison between the FVDs of INR-V when trained on varying number of video instances on the RainbowJelly dataset. FVD is computed against the entire dataset made of $34,526$ samples
As can be seen from the graph, the performance of INR-V deteriorates as the number of video instances reduce. However, the effect of the added instances is marginal as the codebook size increases; the FVD improving by only 12\% when going from 10K to 34K (24K additional instances) video instances. However, the FVD improves by 20\% when the codebook size increases from 500 to 1000 (500 additional instances) video instances.

\subsubsection{Progressive Training with different Initializations}

To stabilize the hypernetwork training, we train our network progressively as explained in Sec.~\ref{sec:progressive-training}. In this section, we compare the performance of INR-V on training with different video intializations. We randomly assign videos for each stage of the training and the random assignments differ across the different initializations. As shown in Fig.~\ref{sec:progressive-training}, INR-V takes about the same time to converge for all of them.


\subsection{INR-V Implementation Details}
\label{sec:appendix-archdets}

The implicit neural representation $f_\theta$ is an MLP with three $256$-dimensional hidden layers. 
Each hidden layer is passed through ReLU activations. The hypernetwork $d_\Omega$ is a set of MLPs. Each MLP predicts the weights for a single hidden layer and the output layer of $f_\theta$. Each MLP has three $256$-dimensional hidden layers. CLIP embeddings are $512$-dimensional vectors, Video-CLIP encodes the CLIP embeddings of each frame through three $512$ dimensional, GRU layers. As shown in Fig.~\ref{fig:videoclip}, Video-CLIP produces $512$-dimensional video-level embedding $g_n$. $c_n$ is a $512$-dimensional context vector that is regressed in an auto-decoding fashion during training. $\phi$ is made of $3$-hidden layers that takes a $1024$-dimensional vector as input (concatenation of $g_n$ and $c_n$) and produces $m_n$, a $128$-dimensional instance code of $V_n$, as the input for $d_\Omega$. The input to $f_\theta$ is a periodic positional encoding of $(\{h\}_{h=1}^{H},\{w\}_{w=1}^{W},\{t\}_{t=1}^{T})$ 
as implemented in \cite{lfns}. Adam optimizer is used with a learning rate of $1e-4$ during training and $1e-2$ during inversion tasks. No scheduler is used. Progressive training is done at a power of $10$ where $i$\textsuperscript{th} stage is made of min$(10^{i}, N)$ examples. $i = 0\dots\mathcal{K}$ such that $10^{\mathcal{K}+1} < N+1$, where $N$ is the total number of training samples. Each stage except the last stage is trained until the reconstruction error reaches a threshold of $1e-3$.

\subsection{Comparison of Computational Complexity of INR-V against 3D Convolutional models}


\begin{wraptable}[]{R}{9.5cm}
\vspace{-5pt}
\centering
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{r|cccccc}
\toprule  
& 128 $\times$ 128 & 256 $\times$ 256 & 512 $\times$ 512 & 1024 $\times$ 1024 
\\ \midrule
INR-V & 1.68 & 6.71 & 26.84 & 107.36 \\
3DConv & 252.71 & 691.76 & 3860.77 & OOM \\
\toprule
& 25 frames & 50 frames & 75 frames & 100 frames \\
\midrule
INR-V & 6.71 & 13.42 & 20.13 & 26.84 &  \\
3DConv & 691.76 & 2036.05 & 3719.98 & 6673.27 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Comparison of computational complexity of INR-V against 3D convolutional-based video generative models for different spatial and temporal dimensions. The computational complexity is the total number of multiply-add Giga operations denoted by GMAC (Multiply-Add Cumulation).}
\label{tab:computation_complexity}

\end{wraptable}

We compare the computational complexity of INR-V against standard implementations of 3D convolution-based video generation models of varying spatial and temporal dimensions. We call these models as ``3DConv". 3DConvs do not generate any meaningful output and are used solely to compare the computation costs against INR-V. They comprise of several transpose 3D convolution-based upsampling layers and take a fixed 128-dimensional latent vector as input. For comparison against INR-V, we gradually vary their spatial dimension from 128 $\times$ 128 to 1024 $\times$ 1024 by keeping the temporal dimension fixed to 25 frames. To generate a video of resolution 128 $\times$ 128, three 3D convolution-based upsampling layers are used.
An additional upsampling layer is added for every subsequent jump in the spatial dimension. Next, we vary the temporal dimension for 25, 50, 75, and 100 frames by keeping the spatial extent and the number of layers fixed to 256 $\times$ 256 and four, respectively, and adjusting the kernel size corresponding to the temporal dimension. The stride, kernel size, and padding are appropriately adjusted for all the models. The batch size for comparison is fixed to 1 for both 3DConv and INR-V. As can be seen in Table.~\ref{tab:computation_complexity}, the number of operations (MAC) increases drastically as the spatial dimension increases for the 3DConvs. It becomes prohibitively expensive to generate videos of higher spatial dimensions. For example, generating a single video of 25 frames of dimension 1024 $\times$ 1024 results in out of memory (OOM) on a single NVIDIA GTX 2080 ti GPU with a memory of 12 GB. In summary, MAC remains hundreds of orders of magnitude lower for INR-V compared to its 3D convolution-based counterparts as it mainly comprises inexpensive MLPs.


\begin{figure}[]
  \centering
  \includegraphics[width=0.9\textwidth]{images/tsne_visualizations._V6.pdf}
  \caption{t-SNE visualization of the latent codes ($m_n$) learned by INR-V with and without CLIP regularization on RainbowJelly. In Epoch 1, the latent codes are bounded within a very small region when INR-V is trained without any regularization (top-left). With CLIP regularization, the latents are more spread out (bottom-left). In the final epochs (top \& bottom right), both latent representations have spread out farther. INR-V without regularization forms a denser space; and with regularization converges faster. Both spaces are continuous, form meaningful interpolations, and support inversion.}
  \label{fig:tsnevisualization}
\end{figure}

\begin{figure}[]
  \centering
  \includegraphics[width=\textwidth]{images/toy-example._V3.pdf}
  \caption{\small We demonstrate the ability of INR-V to learn the underlying structure of a dataset. To do so, we create a toy video dataset called BouncingBall consisting of a blue ball bouncing horizontally at different heights.
  Given 50 instances of such videos, INR-V learns the structure and we demonstrate the learned structure through interpolation. In this example, we show the $16$\textsuperscript{th} frame of each video with the novel interpolated videos in gray boxes. The videos in red boxes are seen during training. We can observe a correct interpolation with smooth transition in motion: the height and the horizontal position of the ball gradually shifts from bottom to top and left to right respectively.}
  \label{fig:toyexampleinterpolation}
\end{figure}


\subsection{Insights on the learned latent space}
\label{sec:insights-appendix}



\begin{wrapfigure}[29]{R}{0.32\textwidth}
    \centering
    \vspace{-10pt}
    \includegraphics[width=\linewidth]{images/BouncingBallDataset._V7.pdf}
    \caption{BouncingBall dataset with an infused structure. Each video instance is $100 \times 100$ and has a ball bouncing horizontally at a specific height. Red lines are added to show the height of the ball and is not a part of the videos.}
    \label{fig:bouncingballsdatset}
    \vspace{1.2em}    \includegraphics[width=\linewidth]{images/sampling_from_latent._V8.pdf}
    \caption{Videos generated by INR-V on BouncingBall. Novel video is generated at an unseen height. Red lines are for demonstration and not generated.}
    \label{fig:bouncingballs_sample}
  
\end{wrapfigure}

Unlike the existing video generation networks that are conformed to a predefined latent space (Gaussian or Uniform), INR-V learns a space that best fits a given distribution. The experiments (Sec.~\ref{sec:video-interpolation} and Sec.~\ref{sec:video-inversion-quant}) and our observations indicate that the learned space is continuous, supports inversion, and smooth video interpolations. Thus, such a space learns a structure in the dataset. For instance, we observe a smooth transition across different poses, expressions, mouth movements, and identity on How2Sign-Faces (best viewed in the Supplementary Video). Therefore, novel instances are observed as we traverse the path between seen latent points A and B. We use this property to generate novel videos by sampling latent points through Slerp interpolation. In this section, we aim to validate the space learned by INR-V and answer the following question: what does INR-V learn? We analyse this in two ways: (1) by visualizing the latent codes learned by INR-V through t-SNE visualization and (2) by training INR-V on a structured toy dataset to see if it learns the underlying structure.

\subsubsection{t-SNE visualization}

Fig.~\ref{fig:tsnevisualization} plots t-SNE\def\thefootnote{$^4$}\footnote{\href{https://lvdmaaten.github.io/tsne/}{https://lvdmaaten.github.io/tsne/}} on the latent codes learned by INR-V on the RainbowJelly dataset. We see a clear pattern of "interpolation" occurring in the learned latent space in both cases (with and without CLIP regularization). No progressive training is used in this visualization. At the end of the first epoch, the latent codes are tightly bounded when trained without CLIP regularization. As the model is trained with CLIP, the latent vectors are more spread out possibly in a semantically meaningful manner. In both the cases, by the final epoch, we observe patterns of interpolation evolve.

\subsubsection{Learning Bouncing Ball}

In this section, we want to analyze if INR-V can learn the structure of a dataset. To do so, we generate a toy dataset, BouncingBall, with an artificially infused structure. The dataset is made of 50 video instances where each video instance consists of a blue ball bouncing horizontally at different heights as shown in Fig.~\ref{fig:bouncingballsdatset}. The videos are of dimension $100 \times 100$ and are $25$ seconds long each.

As shown in Fig.~\ref{fig:toyexampleinterpolation}, INR-V is able to learn the infused structure when trained on BouncingBall. We demonstrate interpolated videos by sampling intermediate points through Slerp interpolation. The intermediate videos (grey boxes) demonstrate a smooth interpolation, and have heights and horizontal displacements gradually varying from Video A to B (red boxes). An example of an intermediate video shown in Fig.~\ref{fig:bouncingballs_sample}. 


\subsection{Inferring at Multiple Resolutions and Multiple Lengths}
\label{sec:appendix-superresolution}



An underlying property of INRs is a continuous representation of the signal. This allows inferring INR-V on multiple spatial and temporal resolutions directly without changing the model's architecture or additional finetuning. To generate a video of an arbitrary dimension $\hat{H} \times \hat{W} \times \hat{T},$ those many number of equally spaced points are sampled between $[-1, 1]$. In Table~\ref{tab:multi-resolution}, we report FVD$_{16}$ scores on random videos generated on INR-V with varying spatial dimensions on $25$ frames. To infer at multiple resolutions, INR-V pretrained on $100\times100$ dimensional videos of $25$ frames was used. Note that the FVD scores do not degrade even at higher spatial resolutions. Additionally, we compare INR-V with existing SOTA superresolution techniques~\cite{videoinr} in Table~\ref{tab:superresolution} on $2048$ videos randomly sampled from the RainbowJelly dataset. 
As can be seen, INR-V performs comparably with methods on the task of superresolution. Moreover, INR-V can be superresolved to any arbitrary resolution ($3.6\times$) and aspect ratio. Please \underline{note} that we do not solve the task of superresolution but rather show superresolution as a potential application of our work. 








\begin{wraptable}[21]{R}{8cm}
\centering

\vspace{-0.7em}

\begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{r | ccc}
         \toprule
& $128 \times 128 $ & $256 \times 256$ & $360 \times 360$ \\
         \midrule
INR-V (Ours) & 260.72 & \textbf{232.43} & 251.14 
         \\
         \bottomrule
    \end{tabular}
\end{adjustbox}
\caption{\small  
FVD$_{16}$ ($\downarrow$) metrics on random video generation at multi-resolution on INR-V. Training was done on only $100 \times 100$ dimensional videos of $25$ frames. Inference was taken directly on multiple resolutions without any finetuning or architectural changes. }
\label{tab:multi-resolution}

\vspace{1em}

\begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{r | cc cc cc}
         \toprule
         & \multicolumn{2}{c}{$2\times$} & \multicolumn{2}{c}{$3\times$} & \multicolumn{2}{c}{$3.6\times$} \\
         & \multicolumn{2}{c}{$200 \times 200 $} & \multicolumn{2}{c}{$300 \times 300$} & \multicolumn{2}{c}{$360 \times 360$} \\
         & PSNR $\uparrow$ & SSIM $\uparrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & PSNR $\uparrow$ & SSIM $\uparrow$\\ 
         \midrule
Bicubic & 31.53 & 0.884 & 32.13 & \textbf{0.915} & \textbf{32.31} & \textbf{0.920} \\
         VideoINR & \textbf{31.59} & 0.883 & \textbf{33.01} & $0.913$ & - & - \\
         INR-V & 28.62 & \textbf{0.892} & 29.17 & 0.894 & 29.05 & 0.896
         \\
         \bottomrule
    \end{tabular}
\end{adjustbox}
\caption{\small Quantitative metrics on video superresolution using INR-V and SOTA superresolution networks on video instance seen at the time of training. INR-V was trained at $100\times100$ video resolution. INR-V performs comparably with the SOTA superresolution networks. }
\label{tab:superresolution}
\end{wraptable}




\subsection{Discussion}

\textbf{Limitations. }Although INR-V has learned a powerful video space demonstrating several intriguing properties, the videos generated by the model are sometimes blurry. This is prominent when moving away to unseen points in the video space far from the seen instances. Fig.~\ref{fig:deblurnet} demonstrates the enhancement on one such blurry sample. This is done by training a standard VQVAE2 network in a denoising fashion (please refer to Sec.~\ref{sec:video-gen-compare}). However, the entire process is broken into generating a relatively lower quality output and relying on a second network to improve its quality. A single end-to-end network capable of retaining the demonstrated powerful properties while generating high-quality videos is a potential future work.

Another limitation of INR-V is generating infinitely long videos. Although coupling the content and time into a single latent has clear advantages, it removes the network's ability to leverage the temporal dimension separately and find infinitely long temporally coherent paths in the image space. This can be tackled by training INR-V to encode video segments of multiple lengths in a single space  ($1$ to $50$ or more frames long video segments). A temporally and semantically coherent trajectory between these video segments can then be learned. Such a generation technique would directly leverage video segments and potentially remove repetitions in the long videos. We believe that leveraging a video space for generating infinitely long videos at multiple resolutions presents an interesting and exciting direction for future research.

Lastly, we observed that INR-V does not learn a meaningful representation space when trained on datasets like UCF-101 that have extreme diversity and limited structure in motion. A similar issue was observed when training the baseline models on such datasets.
However, INR-V can be trained on a single action class of UCF-101 (such as JumpRope) to learn a meaningful representation space even with significant camera motion and in-video subject movement. A single action class limits the visual and motion diversity in the dataset. 



\begin{wrapfigure}[]{R}{0.45\textwidth}
    \centering
    \vspace{-10pt}
    \includegraphics[width=\linewidth]{images/deblur-small._V3.pdf}
    \caption{\small Denoising VQVAE2 reconstructions to enhance the visual quality of relatively blurry videos generated by INR-V. 
    Please refer Sec.~\ref{sec:video-gen-compare}.}
    \label{fig:deblurnet}
  
\end{wrapfigure}

\textbf{Broader Impact. }
\label{broader_impact}
The potential negative impact of our work is similar to existing image-based and video-based GANs: creating "photorealistic-deepfakes" and using them for malicious purposes. Our simple training strategy makes it easier to train a model which produces realistic-looking videos. However, this is partly addressed for the following reasons: (1) Even though our network produces diverse novel videos, the perceptual quality of our generated videos falls short of the existing state-of-the-art image-based generators that produce high-resolution images. (2) The availability of high-quality video datasets limits the intended malicious use of this codebase. 
Despite these limitations, we believe that the potential of our work far outweighs its limitations. A continuous video representation space offers tremendous applications in areas requiring video prediction, interpolation, and conditional video generation. E.g. pedestrian trajectory prediction is an important area of research for self-driving cars. Pedestrian trajectory prediction through future frame generation can serve to reduce accidents in fully-autonomous vehicles in the future. Similarly, conditional video generation can be used for synthesizing novel sign language videos that can be integrated into schools and universities to encourage and enable hard-of-hearing students.


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{images/real_examples.pdf}
  \caption{Examples of real videos instances of How2Sign-Faces (H2S)~\cite{how2sign}, Moving-MNIST (MNIST)~\cite{moving-mnist}, RainbowJelly (Jelly), and SkyTimelapse (Sky)~\cite{skytimelapse} datasets.}
  \label{fig:realexamples}
\end{figure}

\subsection{Additional Qualitative Results}

We encourage our readers to view the supplementary video results of INR-V. Fig.~\ref{fig:realexamples} presents the real video instances in the training set. Fig.~\ref{fig:rec-1-appendix} and Fig.~\ref{fig:rec-2-appendix} presents qualitative results on the reconstruction of video instances from different training datasets. Fig~\ref{fig:random-samples-appendix} presents random videos generated by INR-V on different datasets. Fig~\ref{fig:interpolation-h2g-appendix} and Fig.~\ref{fig:interpolation-sky-appendix} present spatio-temporal view of video interpolations on How2Sign-Faces and SkyTimelapse respectively. Fig.~\ref{fig:multi-resolution-appendix} presents the random generation of INR-V on multiple resolutions starting from $32\times32$ to $256\times256$ jumping a scale factor of $8\times$. The visualization is up to scale, and one can see the scale jump. INR-V can also be inferred at multiple frame rates. The supplementary videos include inferences at $50$ frames. Fig.~\ref{fig:plain-inversion-appendix} - Fig.~\ref{fig:inversion-superresolution-appendix} present the qualitative results and comparisons on the proposed inversion tasks. Fig.~\ref{fig:multimodalframe-prediction-appendix} presents an example of multi-modal future segment prediction. Additional results on several inversion tasks can also be found in the supplementary video.

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/reconstruction.example.V5.pdf}
  \caption{Examples of video instances in the training set reconstructed by INR-V.}
  \label{fig:rec-1-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/reconstruction.example-2.V2.pdf}
  \caption{Examples of video instances in the training set reconstructed by INR-V.}
  \label{fig:rec-2-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/random_samples.V1.pdf}
  \caption{Examples of random videos generated by INR-V.}
  \label{fig:random-samples-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/interpolation-h2g.V1.pdf}
  \caption{Examples of video interpolation in INR-V on How2Sign-Faces. Two latent points are sampled from the training dataset. Intermediate videos are then generated by sampling intermediate latent points using Slerp interpolation technique. We urge the readers to view the supplementary video for best experience. }
  \label{fig:interpolation-h2g-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/interpolation-h2g.V2.pdf}
  \caption{Examples of video interpolation in INR-V on SkyTimelapse. Two latent points are sampled from the training dataset. Intermediate videos are then generated by sampling intermediate latent points using Slerp interpolation technique. We urge the readers to view the supplementary video for best experience. }
  \label{fig:interpolation-sky-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/multi-resolution.V2.pdf}
  \caption{Examples of random videos generated by INR-V at multiple resolutions of $32 \times 32$, $64 \times 64$, $128 \times 128$, and $256 \times 256$ on How2Sign-Faces (top) and RainbowJelly (bottom). The videos are $25$ frames long each. The videos are upto scale. INR-V was trained on videos of only $100 \times 100$ resolution. Please refer the supplementary videos for better experience and additional results on $50$ frames long video generation. }
  \label{fig:multi-resolution-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/plain_inversion._V2.pdf}
  \caption{Comparison of video inversion. Red boxes highlight the differences and matches between the ground truth (GT) and the various methods. To note, INR-V is able to preserve the finer mouth movements.}
  \label{fig:plain-inversion-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/inpainting.pdf}
  \caption{Comparison of half-context inversion in an inpainting setting. At the time of optimization, the model only sees the top half of the video. It then generates the full video back. There can be multiple correct predictions, we showcase one such prediction.}
  \label{fig:inpainting-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/sparse-inversion.pdf}
  \caption{Comparison of half-context inversion in a sparse context setting. At the time of optimization, the model only sees $25\%$ of the full video. INR-V preserve the identity including finer content details like earrings. It also preserves motion like pose and mouth movements.}
  \label{fig:sparse-inpainting-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/prediction._V1.pdf}
  \caption{Comparison of half-context inversion in a future frame prediction setting. At the time of optimization, the model only sees the first $4$ frames of the video. There can be multiple correct predictions given the identity is preserved across the video. We show one such example.}
  \label{fig:frame-prediction-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/multimodal-video-prediction._V3.pdf}
  \caption{Multimodal Future Frame Prediction: Given the first few frames of a video, we want to predict multiple different outcomes. This can be achieved using INR-V by conditionally optimizing the video's latent vector on half-context. In this example, we condition the latent by varying the number of frames used for inversion or by adding Gaussian noise to the optimized latent.
  As can be seen, the inversion preserves the seen context; and the future predictions have varying mouth movements, pose, and eye gazes.}
  \label{fig:multimodalframe-prediction-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/interpolation.pdf}
  \caption{Comparison of half-context inversion in a frame interpolation setting. At the time of optimization, the model only sees the first and last frames of the video. As can be seen, the first and the last frame generated by INR-V match the context (pose, identity, mouth movements), whereas the intermediate frames are very different from the ground truth.}
  \label{fig:frame-interpolation-appendix}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.9\textwidth]{images/superresolution._V1.pdf}
  \caption{Comparison of video superresolution. A video of $32\times32$ is given as input to INR-V for optimization. Once the video is optimized, INR-V regenerates the video at a higher resolution of $128\times128$. VideoINR and Bicubic directly see the $32\times32$ video and superresolves it to $128\times128$. Here, INR-V is not influenced by the glaze on the spectacles and superresolves to a higher dimension closer to the ground truth.}
  \label{fig:inversion-superresolution-appendix}
\end{figure}
\end{document}