

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{eacl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{amsmath, amssymb}
\usepackage{breqn}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{booktabs, makecell}
\usepackage{footnote}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{threeparttable}
\usepackage{subcaption,siunitx,booktabs}
\usepackage{mathtools}
\usepackage{nidanfloat}


\usepackage{microtype}

\aclfinalcopy \def\aclpaperid{4} 

\setlength\titlebox{5cm}


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Text Simplification by Tagging}

\author{Kostiantyn Omelianchuk\thanks{\hspace{4pt}Authors contributed equally to this work; names are given in alphabetical order.} \\ \And
  Vipul Raheja\footnotemark[1] \\
  Grammarly \\
  \texttt{firstname.lastname@grammarly.com} \\\And
  Oleksandr Skurzhanskyi\footnotemark[1]}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Edit-based approaches have recently shown promising results on multiple monolingual sequence transduction tasks. In contrast to conventional sequence-to-sequence (Seq2Seq) models, which learn to generate text from scratch as they are trained on parallel corpora, these methods have proven to be much more effective since they are able to learn to make fast and accurate transformations while leveraging powerful pre-trained language models. Inspired by these ideas, we present TST, a simple and efficient \textbf{T}ext \textbf{S}implification system based on sequence \textbf{T}agging, leveraging pre-trained Transformer-based encoders. Our system makes simplistic data augmentations and tweaks in training and inference on a pre-existing system, which makes it less reliant on large amounts of parallel training data, provides more control over the outputs and enables faster inference speeds. Our best model achieves near state-of-the-art performance on benchmark test datasets for the task. Since it is fully non-autoregressive, it achieves faster inference speeds by over 11 times than the current state-of-the-art text simplification system. 
\end{abstract}

\section{Introduction}
Text Simplification is the task of rewriting text into a form that is easier to read and understand while preserving its underlying meaning and information. It has been shown to be valuable in providing assistance in terms of readability and understandability to children \cite{Belder2010TextSF, kajiwara-etal-2013-selecting}, people with language disabilities like aphasia \cite{carroll1998practical, carroll-etal-1999-simplifying, 10.1145/1168987.1169027}, dyslexia \cite{10.1145/2461121.2461126, 10.1007/978-3-642-37256-8_41}, or autism \cite{evans-etal-2014-evaluation}; non-native English speakers \cite{Petersen07textsimplification, paetzold-2015-reliable, paetzold-specia-2016-understanding, 10.5555/3016387.3016433, pellow-eskenazi-2014-open}, and people with low literacy skills or reading ages \cite{10.1007/11671299_59, 10.1145/1456536.1456540, Gasperin2009NaturalLP, 10.1145/1621995.1622002}.
Moreover, it has also been successfully leveraged as a pre-processing step to improve the performance of various NLP tasks such as parsing \cite{chandrasekar-etal-1996-motivations}, summarization \cite{10.1007/978-3-540-30468-5_47, Silveira12enhancingmulti-document}, semantic role labeling \cite{vickrey-koller-2008-sentence, 10.5555/3171837.3172021} and machine translation \cite{10.1007/3-540-49478-2_40, stajner-popovic-2016-text, HASLER2017221}.

Evolving from the approaches ranging from building hand-crafted rules \cite{chandrasekar-etal-1996-motivations, Siddharthan2006} to syntactic and lexical simplification via synonyms and paraphrases \cite{oro58886, kaji-etal-2002-verb, horn-etal-2014-learning, glavas-stajner-2015-simplifying}, the task has gained popularity as a monolingual Machine Translation (MT) problem, where the system learns to ``translate" a given complex sentence to its simplified form. Initially, Statistical phrase-based (SMT) and Syntactic-based Machine Translation (SBMT) techniques  \cite{zhu-etal-2010-monolingual, 10.1007/978-3-642-12320-7_5, coster-kauchak-2011-learning, wubben-etal-2012-sentence, narayan-gardent-2014-hybrid, stajner-etal-2015-deeper, xu-etal-2016-optimizing} were successfully applied as a way to learn simplification rewrites implicitly from complex-simple sentence pairs, often in combination with hand-crafted rules or features. More recently, several Neural Machine Translation-based (NMT) systems have been developed with promising results \cite{NIPS2014_5346, cho-etal-2014-learning, DBLP:journals/corr/BahdanauCB14}, and their successful application to text simplification, either in combination with SMT or other data-driven approaches \cite{Zhang2017ACS, zhao-etal-2018-integrating}; or strictly as neural models \cite{AAAI1611944, nisioi-etal-2017-exploring, zhang-lapata-2017-sentence, stajner-nisioi-2018-detailed, guo-etal-2018-dynamic, vu-etal-2018-sentence, 10.1007/978-3-030-04221-9_48, kriz-etal-2019-complexity, surya-etal-2019-unsupervised, DBLP:conf/aaai/ZhaoCCY20}, has emerged as the state-of-the-art. 

Human editors perform several rewriting transformations in order to simplify a sentence, such as lexical paraphrasing, changing the syntactic structure, or removing superfluous information from the sentence \cite{Petersen07textsimplification, Alusio2008ACA, mallinson2020felix}. Therefore, even though NMT-based sequence-to-sequence (Seq2Seq) approaches offer a generic framework for modeling almost any kind of sequence transduction, target texts in these approaches are typically generated from scratch - a process which can be unnecessary for monolingual editing tasks such as text simplification, owing to these aforementioned transformations. Moreover, these approaches have a few shortcomings that make them inconvenient for real-world deployment. First, they give limited insight into the simplification operations and provide little control or adaptability to different aspects of simplification (e.g., lexical vs. syntactical simplification). This inhibits interpretability and explainability, which is crucial for real-world settings. Second, they are not sample-efficient and require a large number of complex-simple aligned sentence pairs for training, which requires considerable human effort to obtain. Third, these models typically employ an autoregressive decoder, i.e., output texts are generated in a sequential, non-parallel fashion, and hence, are generally characterized by slow inference speeds.

Based on the aforementioned observations and issues, text-editing approaches have recently re-gained significant interest \cite{NIPS2019_9297, dong-etal-2019-editnts, awasthi-etal-2019-parallel,  malmi-etal-2019-encode, omelianchuk-etal-2020-gector, mallinson2020felix}. Typically, the set of edit operations in such tasks is fixed and predefined ahead of time, which on one hand limits the flexibility of the model to reconstruct arbitrary output texts from their inputs, but on the other, leads to higher sample-efficiency as the limited set of allowed operations significantly reduces the search space \cite{mallinson2020felix}. This pattern is especially true for monolingual settings where input and output texts have relatively high degrees of overlap. In such cases, a natural approach is to cast the task of conditional text generation into a text-editing task, where the model learns to reconstruct target texts by applying a set of edit operations to the inputs. We leverage this insight in our work, and simplify the task from sequence generation or editing, going a step further, to formulate it as a  sequence tagging task. In addition to being sample efficient, thanks to the separation of various edit operations in the form of tags, the system has better interpretability and explainability. Finally, since for sequence tagging we don’t need to predict tokens one-by-one as in autoregressive decoders, the inference is naturally parallelizable and therefore runs many times faster.

Following from the success of the aforementioned monolingual edit-tag based systems, we propose to leverage the current state-of-the-art model for Grammatical Error Correction by \citet{omelianchuk-etal-2020-gector} (GECToR) and adapt it to the task of Text Simplification. 
In summary, we make the following contributions:
\begin{itemize}
    \itemsep0pt
    \item We develop a Text Simplification system by adapting the GECToR model to Text Simplification, leveraging Transformer-based encoders trained on large amounts of human-annotated and synthetic data.\footnote{Available at \url{https://github.com/grammarly/gector\#text-simplification}} Empirical results demonstrate that our system achieves near state-of-the-art performance on benchmark test datasets in terms of readability and simplification metrics.
    \item We propose crucial data augmentations and tweaks in training and inference and show their significant impact on the task: enabling the model to learn to edit the sentences more effectively, rather than relying heavily on copying the source sentences, leading to a higher quality of simplifications.
    \item Since our model is a non-autoregressive sequence tagging model, it achieves over 11 times speedup in inference time, compared to the state-of-the-art for Text Simplification.
\end{itemize}

\section{Related Work}
Recent text editing works have shown promising results of reformulating multiple monolingual sequence transduction tasks into sequence tagging tasks compared to the conventional Seq2Seq sequence generation formulation. This observation is especially true for tasks where input and output sequences have a large overlap. 
Generally, these works try to simplify monolingual sequence transduction by explicitly modeling edit operations such as \textsc{keep}, \textsc{add}/\textsc{insert} and \textsc{delete}. \citet{alva-manchego-etal-2017-learning} proposed the first such formulation, employing a BiLSTM to predict edit labels sequentially. Our model for sentence simplification does not rely on external simplification rules nor alignment tools. \citet{ribeiro-etal-2018-local} proposed an approach applied only to character deletion and insertion and was based on simple patterns. LaserTagger \cite{malmi-etal-2019-encode} combines a BERT encoder with an autoregressive Transformer decoder to similarly predict the aforementioned three main edit operations for several text editing tasks. In contrast, in our system, the decoder is a softmax layer. Similarly, EditNTS \cite{dong-etal-2019-editnts} and PIE \cite{awasthi-etal-2019-parallel} predict edit labels, developed specifically for text simplification and GEC, respectively. While EditNTS employs an autoregressive encoder-decoder based neural programmer-interpreter model, PIE differs from our work because of our custom edit transformations and incorporation of a pre-trained Transformer encoder for sequence tagging. Levenshtein Transformer \cite{NIPS2019_9297}, an autoregressive model that performs text editing by executing a sequence of deletion and insertion actions, is another recent work along similar lines. More recently, \citet{mallinson2020felix} proposed Felix - a text-editing-based system for multiple generation tasks, splitting the text-editing task into two sub-tasks: tagging and insertion. Their tagging model employs a Pointer mechanism, while the insertion model is based on a Masked Language Model. 


\section{System Description}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{TST-Horizontal2.pdf}
    \caption{\textbf{T}ext \textbf{S}implification by \textbf{T}agging (TST): A given sentence undergoes multiple iterations of tag-and-edit transformations, where, in each iteration, it is tagged using custom token-level edit-tags, and the sequence of tags is converted back to text by applying those edits, iteratively making simplifying edits.}
    \label{fig:framework}
\end{figure}

Following recent works such as \citet{malmi-etal-2019-encode, awasthi-etal-2019-parallel, omelianchuk-etal-2020-gector}, who leveraged similar frameworks for different text editing problems such as GEC, Sentence Fusion, and Abstractive Summarization, we formulate the task of Text Simplification as a tagging problem.

Specifically, our system is based on GECToR \cite{omelianchuk-etal-2020-gector}, an iterative sequence-tagging system that works by predicting token-level edit operations, originally developed for Grammatical Error Correction (GEC). 
We adapt the GECToR framework for the task of Text Simplification, with minimal modifications to the original architecture. Our system consists of three main parts: (a) defining the custom transformations (token-level edit-tags), (b) performing iterative sequence tagging to convert target sequences to tag sequences, (c) fine-tuning of pre-trained Transformers to predict the tag sequences. Each of these components are described below.

\subsection{Edit Transformations}
\label{section:transformations}
In order to formulate the task as a tagging problem, building on the aforementioned edit-tagging-based approaches, we use custom token-level edit operations (also referred to as \textbf{edit-tags} or \textbf{transformations}) to perform text simplification. Formally, given a sentence : , and its simplified form :  as the target sentence, we aim to predict an edit tag  ( denoting the edit-tag vocabulary) for each token  in , generating a sequence of edit-tags of the same length  as the input sequence , such that : applying the edit operation represented by the edit-tag  to the input token  at each position , reconstructs the target sequence , even though .

We reuse the edit transformations in GECToR, which were developed for GEC. We chose to do so because we found a significantly high overlap of 92.64\% in the tag distributions between the GEC and Text Simplification domains. This was done by building the edit-tag vocabularies independently on both (GEC and Text Simplification) datasets and comparing the tag distributions represented by the two vocabularies. This was not surprising since these edit-tags have been obtained from huge amounts of synthetic GEC data, they are expected to have good coverage with many standard monolingual text editing problems. Additionally, using the same edit-tags is a necessary pre-requisite to leverage GEC initialization in the model (Section \ref{ssec:pre-training}), which we later show to be quite impactful for our text simplification system (Section \ref{ssec:ablation_gec}). Consequently, the edit space  is of size 5000, out of which 4971 are basic edit-tags and 29 are token-independent GEC-specific edit-tags (such as \texttt{\APPEND_just}, which appends the word ``just" to the given word) and 3802 token-dependent \textsc{replace} tags (such as \texttt{\t_ix_ix_{\textsc{BASE}}_{\textsc{BASE}}\epsilonn\in{1, 2, 3, 4}\uparrow\uparrow\uparrow\uparrow\downarrow\ddagger_{\pm 0.36}_{\pm 1.06}_{\pm 0.72}_{\pm 0.60}_{\pm 1.35}_{\pm 1.46}_{\pm 0.19}_{\pm 0.77}_{\pm 0.41}_{\pm 3.14}_{\pm 1.45}_{\pm 0.31}_{\pm 0.32}_{\pm 0.44}_{\pm 0.75}_{\pm 1.19}_{\pm 0.19}\ddagger\uparrow\uparrow\uparrow\uparrow\downarrow_{\pm 0.6}_{\pm 1.06}_{\pm 0.90}_{\pm 1.20}_{\pm 2.24}_{\pm 2.03}_{\pm 0.42}_{\pm 1.62}_{\pm 0.59}_{\pm 4.5}_{\pm 0.52}_{\pm 0.31}_{\pm 0.3}_{\pm 0.29}_{\pm 1.22}_{\pm 1.68}_{\pm 0.27}\uparrow\uparrow\uparrow\uparrow\downarrow\ddagger_{\pm 1.87}_{\pm 1.31}_{\pm 4.73}_{\pm 2.75}_{\pm 1.01}_{\pm 1.26}_{\pm 0.92}_{\pm 2.09}_{\pm 1.76}_{\pm 0.9}\ddagger\uparrow\downarrow\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm_{\textsc{LARGE}}$), ensembles, or external data will likely lead to better SARI scores at the cost of speed and system complexity: ideas we plan to explore in future work. 




\bibliography{anthology,eacl2021}
\bibliographystyle{acl_natbib}

\clearpage

\appendix

\section{Model Configurations}
\label{sec:appendix1}
Table \ref{tab:hyperparameters} describes the list of hyper-parameters used for \textsc{TST-Final} model. In Table \ref{tab:inference_tweaks}, we list the inference tweaks hyper-parameters found by Bayesian Search on TurkCorpus and ASSET datasets. 



\begin{table}[H]
\begin{center}
\small
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameter name} & \textbf{Value} \\
\toprule
learning\_rate & 1e-5\\
transformer\_model & roberta-base \\
accumulation\_size & 2 \\
batch\_size & 128 \\
cold\_steps\_count & 2 \\
n\_epoch & 50 \\
patience & 3 \\
vocab\_size & 5000 \\
max\_len & 50 \\
min\_len & 3 \\
pieces\_per\_token & 5 \\
filter\_brackets & 0/1 \\
seed & 1/2/3/11 \\
normalize & 1 \\
\bottomrule
\end{tabular}
\caption{The list of hyper-parameters used during training}
\label{tab:hyperparameters}
\end{center}
\end{table}


\begin{table*}[b]
\begin{center}
\small
\begin{tabular}{ccccccc}
\toprule
\textbf{Model description} & \textbf{Tuned dataset} & \textbf{Seed} & \textbf{Del conf} &	\textbf{Keep conf} & \textbf{Iterations} &	\textbf{Min error probability}  \\
\toprule
TST w/o InfTweaks & - & - & 0 & 0 & 5 & 0 \\
TST with InfTweaks & Turk & 1 & -0.84 &	-0.66 &	2 & 0.04 \\
TST with InfTweaks & Turk & 2 &	-0.93 &	-0.51 &	3 &	0.02 \\
TST with InfTweaks & Turk & 3	&-0.86 &	-0.68 &	2 &	0.03 \\
TST with InfTweaks & Turk & 11	& -0.88 & -0.61 &	2 &	0.03 \\
TST with InfTweaks & ASSET & 1 & -0.66 &	-0.9 &	3 & 0.02 \\
TST with InfTweaks & ASSET & 2 & -0.72 &	-0.88 &	3 &	0.02 \\
TST with InfTweaks & ASSET & 3	& -0.52 &	-0.89 &	3 & 0.04 \\
TST with InfTweaks & ASSET & 11	& -0.72 &	-0.91 &	3 &	0.02 \\


\bottomrule
\end{tabular}
\caption{Inference hyper-parameters found by the Bayesian Search on TurkCorpus and ASSET development sets}
\label{tab:inference_tweaks}
\end{center}
\end{table*}

\newpage

\section{Simplification Examples}

Various examples from our system are shown in Table \ref{tab:examples}. Examining the simplifications, we see reduced sentence length, sentence splitting of a complex sentence into multiple shorter sentences, and the use of simpler vocabulary. Manual comparison between \textsc{TST-base} and \textsc{TST-Final} shows that the first system tends to delete some complex words from the text. For example, ``theoretically possible” gets shortened to just ``possible,” and ``administrative district” to ``district”. \textsc{TST-Final} model tends to be more creative and changes phrases to simpler versions like “is theoretically possible” to ``might be” or ``an administrative district” to ``a part of.” However, this aggressive and creative strategy sometimes also generates ungrammatical output like in the last example in Table \ref{tab:examples}. While it rarely happens, but the model might also change the meaning of the original sentence. For example, replacing ``the five” to ``the three.” It is worth noticing that the same problem of meaning change is present in the reference sentences as well: where ``the five” got replaced with ``one of four”.




\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{p{1.9cm}|p{18cm}}
        \toprule
        Original & he also \textbf{completed two collections} of short stories \textbf{entitled} the ribbajack \& other curious yarns and seven strange and ghostly tales . \\
        Reference & he also \textbf{wrote two books} of short stories \textbf{called ,} the ribbajack \& other curious yarns and seven strange and ghostly tales . \\
        \textsc{TST-Base} & he also  \textbf{wrote} two collections of short stories  \textbf{called} the ribbajack \& other curious yarns and seven strange and ghostly tales . \\
        \textsc{TST-Final} & he also \textbf{wrote a series} of short stories \textbf{called} the ribbajack \& other curious yarns and seven strange and ghostly tales . \\
        \midrule
        Original & it \textbf{is theoretically possible} that the other editors who may have \textbf{reported} you , and the administrator who blocked you , are part of a conspiracy against someone half a world away they 've never met in person . \\
        Reference & it is theoretically possible that the other editors who may \textbf{have written about} you, and the \textbf{officer} who blocked you, are part of \textbf{a bad plan} against someone \textbf{miles} away, they 've never met \textbf{face to face} . \\
        \textsc{TST-Base} & it is possible that the other editors who may have reported you , and the administrator who blocked you , are part of a conspiracy against someone half a world away they ' ve never met in person . \\
        \textsc{TST-Final} & it \textbf{might be} that the other editors who may have \textbf{sent} you , and the administrator who blocked you , are part of a conspiracy against someone half a world away where they 've never met in person . \\
        \midrule
        Original & as a result , although many mosques will not enforce violations , both men and women when attending a mosque must \textbf{adhere to these guidelines} . \\
        Reference & as a result , both men and women must follow this rule when they attend a mosque , even though many mosques do not enforce these rules  \\
        \textsc{TST-Base} & both men and women when \textbf{going} a mosque must \textbf{follow these rules} . \\
        \textsc{TST-Final} & both men \textbf{,} and women \textbf{that attend} mosque \textbf{,} must \textbf{follow the law} . \\
        \midrule
        Original & hinterrhein is \textbf{an administrative district in} the canton of graubünden , switzerland . \\
        Reference & hinterrhein is a district \textbf{of} the canton of graubünden , switzerland . \\
        \textsc{TST-Base} & hinterrhein is a district \textbf{of} the canton of graubünden , switzerland . \\
        \textsc{TST-Final} & hinterrhein is \textbf{a part of} the canton of graubünden in the switzerland . \\
        \midrule        
        Original & a majority of south indians speak one of the five dravidian languages — kannada , malayalam , tamil , telugu and tulu . \\
        Reference & \textbf{many} of the south indians \textbf{are dravidians and they speak one of four} dravidian languages — kannada , malayalam , tamil \textbf{or} telugu . \\
        \textsc{TST-Base} & \textbf{most} of south indians speak one of the five dravidian languages — kannada , malayalam , tamil , telugu and tulu . \\
        \textsc{TST-Final} & \textbf{most of the people} speak \textbf{speakers from the three} dravidian languages \textbf{spoken are ,} kannada , malayalam , tamil , telugu \textbf{,} and tulu . \\
        
        \bottomrule
    \end{tabular}
}
\caption{Examples of simplifications by TST}
\label{tab:examples}
\end{table*}













\end{document}
