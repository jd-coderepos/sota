

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{mathtools}

\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{bbm}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

\usepackage{float}

\usepackage{lipsum}
\usepackage{stfloats}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{bm}
\usepackage{etoolbox}
\usepackage{icomma}
\usepackage{array}
\usepackage{tabulary}
\usepackage[table]{xcolor}
\usepackage{paralist}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{caption}
\captionsetup[table]{format=plain,labelformat=simple,labelsep=period}\usepackage{subcaption}
\usepackage{arydshln}
\newcommand\ver[1]{\rotatebox[origin=c]{90}{#1}}
\newcommand{\fl}[1]{\multicolumn{1}{c}{#1}}
\definecolor{gray}{rgb}{0.3,0.3,0.3}
\definecolor{blue}{rgb}{0,0.5,1}
\definecolor{mask_red}{rgb}{1,0,0.8}
\definecolor{green}{rgb}{0.2,1,0.2}
\definecolor{rblue}{rgb}{0,0,1}
\definecolor{lightblue}{HTML}{6495ed}
\definecolor{lightred}{HTML}{F19C99}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
\newcommand{\lightblue}[1]{\textcolor{lightblue}{#1}}
\newcommand{\fn}[1]{\footnotesize{#1}}
\newcommand{\gbf}[1]{\green{\bf{\fn{(#1)}}}}
\newcommand{\bbf}[1]{\lightblue{\bf{\fn{(#1)}}}}
\newcommand{\rbf}[1]{\gray{\bf{\fn{(#1)}}}}
\newcommand{\obf}[1]{\textcolor{orange}{\bf{\fn{(#1)}}}}
\definecolor{graytablerow}{gray}{0.6}
\newcommand{\grow}[1]{\textcolor{graytablerow}{#1}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\hypersetup{colorlinks, citecolor=blue}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\begin{document}

\title{Delivering Arbitrary-Modal Semantic Segmentation}

\author{Jiaming Zhang\thanks{Equal contribution.},
~~Ruiping Liu,
~~Hao Shi,
~~Kailun Yang\thanks{Corresponding author (e-mail: {\tt kailun.yang@hnu.edu.cn}).},
~~Simon Rei√ü,\\
~~Kunyu Peng,
~~Haodong Fu,
~~Kaiwei Wang,
~~Rainer Stiefelhagen\\
\normalsize
Karlsruhe Institute of Technology,
\normalsize
~Hunan University,
\normalsize
~Zhejiang University,
\normalsize
~Beihang University
}
\maketitle

\begin{abstract}
Multimodal fusion can make semantic segmentation more robust. However, fusing an arbitrary number of modalities remains underexplored. To delve into this problem, we create the \textsc{DeLiVER} arbitrary-modal segmentation benchmark, covering \underline{De}pth, \underline{Li}DAR, multiple \underline{V}iews, \underline{E}vents, and \underline{R}GB. Aside from this, we provide this dataset in four severe weather conditions as well as five sensor failure cases to exploit modal complementarity and resolve partial outages. To make this possible, we present the arbitrary cross-modal segmentation model \textsc{CMNeXt}. It encompasses a \emph{Self-Query Hub~(SQ-Hub)} designed to extract effective information from any modality for subsequent fusion with the RGB representation and adds only negligible amounts of parameters () per additional modality. On top, to efficiently and flexibly harvest discriminative cues from the auxiliary modalities, we introduce the simple \emph{Parallel Pooling Mixer (PPX)}. With extensive experiments on a total of six benchmarks, our \textsc{CMNeXt} achieves state-of-the-art performance on the \textsc{DeLiVER}, KITTI-360, MFNet, NYU Depth V2, UrbanLF, and MCubeS datasets, allowing to scale from  to  modalities. On the freshly collected \textsc{DeLiVER}, the quad-modal \textsc{CMNeXt} reaches up to  in mIoU with a  gain as compared to the mono-modal baseline.\footnote{The \textsc{DeLiVER} dataset and our code will be made publicly available at: \url{https://jamycheung.github.io/DELIVER.html}.} \end{abstract}

\section{Introduction}
\label{sec:intro}

With the explosion of modular sensors, multimodal fusion for semantic segmentation has progressed rapidly recently~\cite{cao2021shapeconv,chen2020sagate,liu2022cmx} and in turn has stirred growing interest to assemble more and more sensors to reach higher and higher segmentation accuracy aside from more robust scene understanding.
However, most works~\cite{hu2019acnet,wu2020depth_adapted,zhuang2021pmf} and multimodal benchmarks~\cite{ha2017mfnet,silberman2012nyuv2,zhang2021issafe} focus on specific sensor pairs, which lack behind the current trend of fusing more and more modalities~\cite{wang2022tokenfusion,broedermann2022hrfuser}, \ie, progressing towards Arbitrary-Modal Semantic Segmentation (AMSS).

\begin{figure}[!t]
	\centering
    \begin{minipage}[t]{.33\columnwidth}
        \includegraphics[width=0.9\columnwidth]{Figures/fig1_rgbdel.pdf}
\subcaption{RGB-D-E-L fusion.}\label{fig1:rgbdel}
    \end{minipage}\begin{minipage}[t]{.33\columnwidth}
        \includegraphics[width=0.9\columnwidth]{Figures/fig1_rgbadn.pdf}
\subcaption{RGB-A-D-N fusion.}\label{fig1:rgbadn}
    \end{minipage}\begin{minipage}[t]{.33\columnwidth}
        \includegraphics[width=0.9\columnwidth]{Figures/fig1_rgblf.pdf}
\subcaption{RGB-Light Field.}\label{fig1:rgblf}
    \end{minipage}\vskip -1ex
	\caption{Arbitrary-modal segmentation results of CMNeXt using: \
\begin{split}\label{eq:sq-hub1}
    \boldsymbol{\hat{f}}^m &= \text{DW-Conv}_{3{\times}3}{(C, C)}(\boldsymbol{f}^m),
\end{split}\\
\begin{split}\label{eq:sq-hub2}
    Q^m &= \text{Sigmoid}(\text{Conv}(C, 1)(\boldsymbol{\hat{f}}^m)),
\end{split}
\label{eq:sq-hub3}
\begin{split}
    \boldsymbol{f}^q &=\{p^q|p^q{\in}{H{\times}W}\} \\
     &= \phi(\{\boldsymbol{f}^m{+}Q^m{\cdot}\boldsymbol{\hat{f}}^m|m{\in}[1,M]\}) \\
     &= \phi(\{p^m|p^m{\in}{H{\times}W},m{\in}[1,M]\}), 
\end{split}
\label{eq:ppx1}
\begin{split}
    \hat{\boldsymbol{f}^q}&=\text{DW-Conv}_{7{\times}7}(C, C)(\boldsymbol{f}^q), 
\end{split}\\
\begin{split}
    \hat{\boldsymbol{f}^q}&\coloneqq\sum_{k{\in}\{3,7,11\}}\text{Pool}_{k{\times}k}(\hat{\boldsymbol{f}^q}) + \hat{\boldsymbol{f}^q}, 
\end{split}\\
\begin{split}
    \boldsymbol{w}&=\text{Sigmoid}(\text{Conv}_{1{\times}1}(C, C)(\hat{\boldsymbol{f}^q})), 
\end{split}\\
\begin{split}
    \boldsymbol{f}^w&=\boldsymbol{w}{\cdot}\boldsymbol{f}^q+\boldsymbol{f}^q. 
\end{split}
\label{eq:ppx2}
\begin{aligned}
    \hat{\boldsymbol{f}^w}&=\text{FFN}(C, C)(\boldsymbol{f}^w)+\text{SE}(\boldsymbol{f}^w). \\
\end{aligned}

    f_x{=}H/(2{\times}tan(FoV{\times}\pi/360)),\\
    f_y{=}W/(2{\times}tan(FoV{\times}\pi/360)).

\begin{bmatrix}
u \\
v \\
1 \\
\end{bmatrix} = 
\begin{bmatrix}
f_x & 0 & u_0 \\
0 & f_y & v_0 \\
0 & 0 & 1 \\
\end{bmatrix} 
\begin{bmatrix}
\boldsymbol{R} & \boldsymbol{t} \\
\boldsymbol{0}^T_{3{\times}1} & 1 \\
\end{bmatrix}
\begin{bmatrix}
X \\
Y \\
Z \\
1 \\
\end{bmatrix},

where  is the LiDAR point,  is the 2D image pixel, and the rotation () and the translation () matrices are set as the unit matrix in the CARLA simulator~\cite{dosovitskiy2017carla}. 


\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{Figures/supp_deliver2.pdf}
\caption{Data structure of the \textsc{DeLiVER} dataset. The columns from left to right are respective conditions, cases, multiple views, modalities and annotations. \textbf{MB}: Motion Blur; \textbf{OE}: Over-Exposure; \textbf{UE}: Under-Exposure; \textbf{LJ}: LiDAR-Jitter; and \textbf{EL}: Event Low-resolution.}
    \label{fig:deliver_struc}
\end{figure*}
\subsection{Dataset structure}
\textsc{DeLiVER} contains Depth, LiDAR, Event, and RGB modalities.
As shown in Fig.~\ref{fig:deliver_struc}, four adverse road scene conditions of \emph{rainy}, \emph{sunny}, \emph{foggy}, and \emph{night} are included in our dataset.
There are five sensor failure cases including Motion Blur (\textbf{MB}), Over-Exposure (\textbf{OE}), Under-Exposure (\textbf{UE}), LiDAR-Jitter (\textbf{LJ}), and Event Low-resolution (\textbf{EL}) to verify that the performance of model is robust and stable in the presence of sensor failures. The sensors are mounted at different locations on the ego car to provide multiple views including \emph{front}, \emph{rear}, \emph{left}, \emph{right}, \emph{up}, and \emph{down}. Each sample is annotated with semantic and instance labels. In this work, we focus on the front-view semantic segmentation.

The  semantic classes in \textsc{DeLiVER} dataset are: \emph{Building, Fence, Other, Pedestrian, Pole, RoadLine, Road, SideWalk, Vegetation, Cars, Wall, TrafficSign, Sky, Ground, Bridge, RailTrack, GroundRail, TrafficLight, Static, Dynamic, Water, Terrain, TwoWheeler, Bus, Truck}.

\subsection{Dataset statistics}
\begin{table*}
\centering
\caption{Data statistic of DeLiVER dataset. It includes four adverse conditions (\emph{cloudy}, \emph{foggy}, \emph{rainy}, and \emph{night}), and each condition has five failure cases (\textbf{MB}: Motion Blur; \textbf{OE}: Over-Exposure; \textbf{UE}: Under-Exposure; \textbf{LJ}: LiDAR-Jitter; and \textbf{EL}: Event Low-resolution).}
\label{tab:sup_deliver_stat}
\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{l|rrrrr:r||rrrrrr:r} 
\toprule
\textbf{Split} & \textbf{Cloudy} & \textbf{Foggy} & \textbf{Night} & \textbf{Rainny} & \textbf{Sunny} & \textbf{Total} & \textbf{Normal} & \textbf{MB} & \textbf{OE} & \textbf{UE} & \textbf{LJ} & \textbf{EL} & \textbf{Total}  \\\midrule\midrule
Train & 794 & 795 & 797 & 799 & 798 & 3983  & 2585 & 600 & 200 & 199 & 199 & 200 & 3983 \\
Val & 398 & 400 & 410 & 398 & 399 & 2005 & 1298 & 299 & 100 & 99 & 100 & 109 & 2005 \\
Test & 379 & 379 & 379 & 380 & 380 & 1897 & 1198 & 300 & 100 & 100 & 99 & 100 & 1897 \\\hline
Front-view & 1571 & 1574 & 1586 & 1577 & 1577 & 7885 & 5081 & 1199 & 400 & 398 & 398 & 409 & 7885 \\\hline
All six views & 9426 & 9444 & 9516 & 9462 & 9462 & 47310 & 30486 & 7194 & 2400 & 2388 & 2388 & 2454 & 47310 \\
\bottomrule
\end{tabular}
}
}
\end{table*} We present statistics of the \textsc{DeLiVER} dataset in Table~\ref{tab:sup_deliver_stat}. We discuss data partitioning in two groups, one according to the conditions and the other according to the sensor failures. Note that, the two groups are mutually inclusive. The five cases from the second group are included in each of five conditions from the first group. For example, cases of \textbf{MB}, \textbf{OE}, \textbf{UE}, \textbf{LJ}, and \textbf{EL} are included in \textit{cloudy}, \textit{foggy}, \textit{night}, \textit{rainy}, and \textit{sunny} conditions, but with different samples. To investigate the robustness under sensor failures, we collect , , , , and  frames on respective cases.

\subsection{Dataset comparison}
\begin{table*}[!t]
\centering
\caption{
Comparison between multimodal datasets. D:Day; S:Sunset; N:Night; *:random; Sem.:Semantic; Ins.:Instance.
}
\label{tab:sup_dataset_comp}
\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{c|c:cccc:c:c:ccc:c:cc}
\toprule
    \multicolumn{1}{c|}{\multirow{2}{*}{Dataset}}&\multirow{2}{*}{Type}&\multicolumn{4}{c:}{Sensors}&\multicolumn{1}{c:}{Sensor}&\multicolumn{1}{c:}{RGB}&\multicolumn{3}{c:}{Diversity} &\multirow{2}{*}{Classes}&\multicolumn{2}{c}{Labels}\\
    &&Camera&Depth&Event&LiDAR&Failures&Failures&Weathers&Daytime&Views&&Sem.&Ins.\\
\midrule
\midrule
WildDash~\cite{Zendel2018WildDashC}&Real&1&0&0&0&0&15&*&*&*&19&&\\
Waymo~\cite{Sun2020waymo}&Real&5&0&0&5&0&0&2&DN&5&28&&\\
SELMA~\cite{testolina2022selma}&Synthetic&7&7&0&3&0&6&9&DSN&7&19&&\\
SynWoodScape~\cite{sekkat2022synwoodscape}&Synthetic&5&5&5&1&0&0&4&DS&5&25&&\\
SynPASS~\cite{zhang2022trans4pass}&Synthetic&6&0&0&0&0&0&4&DN&1&22&&\\
\rowcolor{gray!15}DeLiVER (ours)&Synthetic&6&6&6&1&5&3&4&DN&6&25&&\\
\bottomrule

\end{tabular}
}
}
\end{table*} As shown in Table~\ref{tab:sup_dataset_comp}, we compare several datasets with adverse conditions and cases.
All the datasets cover the whole daytime.
The real-scene datasets, \eg, WildDash~\cite{Zendel2018WildDashC} and Waymo\cite{Sun2020waymo}, capture data by using only one or a few sensors, which results a lack of data diversity. In contrast, our \textsc{DeLiVER} dataset has four different modalities, including \textit{RGB}, \textit{Depth}, \textit{Event} and \textit{LiDAR}, which enables the multimodal semantic segmentation task to involve up to  modalities. 
Compared to previous synthetic datasets, \eg, SELMA~\cite{testolina2022selma}, SynWoodScape~\cite{sekkat2022synwoodscape}, SynPASS~\cite{zhang2022trans4pass}, our \textsc{DeLiVER} additionally includes  types of sensor failure.
Each sample has semantic and instance annotations, so semantic, instance and panoptic segmentation tasks can be conducted on our \textsc{DeLiVER} dataset. 

\section{Implementation Details}
We conduct our experiments with PyTorch . All models are trained on a node with 4 A100 GPUs. Below we describe the specific implementation details for six datasets. 

\noindent\textbf{Data representation.}
For depth images, we follow SA-Gate~\cite{chen2020sagate} and CMX~\cite{liu2022cmx} to preprocess the one-channel depth images to HHA-encoded representations~\cite{gupta2014learning}, where HHA includes horizontal disparity, height above ground, and norm angle. The 3D LiDAR and Event data of \textsc{DeLiVER} dataset are transformed to the aforementioned frame format. Then, both LiDAR- and Event-based data are preprocessed as 2D range views~\cite{zhuang2021pmf} and 3-channel representations~\cite{zhang2021issafe}, respectively. 

\noindent\textbf{\textsc{DeLiVER} dataset.} 
We train our models for  epochs on the \textsc{DeLiVER} dataset. The batch size is  on each of four GPUs. The resolution of all modalities is set as  for training and inference. In the Event Low-resolution cases, the Event-based images with the original size of  are upsampled to . During evaluation, we only apply the single-scale test strategy. The backbone of CMNeXt is based on MiT-B2~\cite{xie2021segformer}. To verify the effectiveness of our method under convolutional networks, the CNN-based SegNeXt-Base~\cite{guo2022segnext} is selected as the backbone, when compared to the MiT-B2 one.

\noindent\textbf{KITTI-360 dataset.} As there are more than  training data on KITTI-360 dataset, the models are trained for  epochs. The image resolution is set as  and the batch size is  on each of four GPUs. The backbone of CMNeXt is based on MiT-B2~\cite{xie2021segformer}. 

\noindent\textbf{NYU Depth V2 dataset.} Following CMX~\cite{liu2022cmx}, the number of training epochs is set as  for a fair comparison. The resolution of RGB and Depth images is set as . The training batch size is  on each of four GPUs. The backbone of CMNeXt is based on MiT-B4~\cite{xie2021segformer}. We apply the multi-scale flip test strategy for a fair comparison.

\noindent\textbf{MFNet dataset.} We train our CMNeXt models with the MiT-B4 backbone for  epochs on the MFNet dataset. The resolution of RGB and Thermal images is set as  and the batch size is  on each of four GPUs. We apply the multi-scale flip test strategy for a fair comparison.

\noindent\textbf{MCubeS dataset.} To compare with MCubeSNet~\cite{liang2022mcubesnet}, we build CMNeXt with MiT-B2 and train the model for  epochs. Following MCubeSNet~\cite{liang2022mcubesnet}, the image size is set as  during training and  during evaluation. The batch size is set as  on each of four GPUs. 

\noindent\textbf{UrbanLF dataset.} To perform comparison with the OCR-LF model~\cite{sheng2022urbanlf}, we build CMNeXt with MiT-B4. The image size on the real and synthetic sets is . The angular resolution of  sub-aperture images of the UrbanLF dataset is . To conduct arbitrary-modal segmentation, the center-aperture image is selected as the primary modality, while the other apertures are as additional modalities. We sample respective , , and  light field images as the supplementary modalities, \ie, LF, LF, and LF for short. The  images are from the center horizontal direction, while the  images are from the four directions of horizontal, vertical, , and , following UrbanLF~\cite{sheng2022urbanlf}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{Figures/supp_vis_deliver.pdf}
    \vskip -1ex
    \caption{More visualization results on \textsc{DeLiVER} dataset. From left to right are the respective \textit{cloudy}, \textit{foggy}, \textit{night} and \textit{rainy} scene.}
    \label{fig:sup_qualitative_vis}
    \vskip -2ex
\end{figure}
\section{More visualizations on \textsc{DeLiVER}}
As shown in Fig.~\ref{fig:sup_qualitative_vis}, in the four adverse weather conditions, RGB-D fusion-based methods greatly improve the performance, particularly for distant elements in \textit{foggy} and \textit{nighttime} scenes.
Our RGB-D solution is more accurate than CMX (RGB-D),
and the full quad-modal RGB-D-E-L CMNeXt model further enhances the segmentation. A failure case is shown on the right column (\ie, the \textit{rainy} scene) of Fig.~\ref{fig:sup_qualitative_vis}, in which the RGB-only model has a better segmentation on the \textit{sidewalk} class. However, our quad-modal CMNeXt has a higher accuracy score with . 


\section{Acknowledgments}
This work was supported in part by Helmholtz Association of German Research Centers, in part by the Federal Ministry of Labor and Social Affairs (BMAS) through the AccessibleMaps project under Grant 01KM151112, in part by the University of Excellence through the ``KIT Future Fields'' project, and in part by Hangzhou SurImage Technology Company Ltd. This work was partially performed on the HoreKa supercomputer funded by the Ministry of Science, Research and the Arts Baden-W√ºrttemberg and by the Federal Ministry of Education and Research.

 
\end{document}
