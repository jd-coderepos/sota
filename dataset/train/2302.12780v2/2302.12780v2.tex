
\documentclass{article} \usepackage{iclr2023/iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}
\usepackage{amssymb,amsthm,mathtools}
\usepackage{soul}

\allowdisplaybreaks
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\def\err{{\mathrm{err}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}


\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}
\DeclareMathOperator*{\subopt}{SubOpt}
\DeclareMathOperator*{\gap}{gap}
\DeclareMathOperator*{\vect}{vec}
\DeclareMathOperator*{\logdet}{logdet}
\DeclareMathOperator*{\tr}{tr}

\usepackage{wrapfig}
\usepackage{bbm}
\newcommand{\thanh}[1]{\textcolor{blue}{\emph{[\textbf{Thanh}: #1]}}}
\newcommand{\raman}[1]{\textcolor{red}{\emph{[\textbf{Raman}: #1]}}}
\newcommand{\cmt}[1]{\textcolor{blue}{#1}}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{subfigure}


\newtheorem{defn}{Definition}
\newtheorem{conj}{Conjecture}[section]
\newtheorem{example}{Example}[section]
\newtheorem{assumption}{Assumption}[section]

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{prop}{Proposition}
\newtheorem{observation}{Observation}
\newtheorem{corollary}{Corollary}

\newtheorem{remark}{Remark}

\usepackage[linewidth=1pt]{mdframed}

\newenvironment{hint}
{
\begin{mdframed}[backgroundcolor=orange!5, linecolor=white] 
\textbf{Hint}. 
}
{
\end{mdframed}
}
\usepackage{float}
\usepackage{multirow}



%
 

\usepackage{hyperref}
\usepackage{url}

\title{VIPeR: Provably Efficient Algorithm for Offline RL with Neural Function Approximation}







\author{Thanh Nguyen-Tang \\
Department of Computer Science\\
Johns Hopkins University\\
Baltimore, MD 21218, USA \\
\texttt{nguyent@cs.jhu.edu} \\
\And
Raman Arora \\
Department of Computer Science\\
Johns Hopkins University\\
Baltimore, MD 21218, USA \\
\texttt{arora@cs.jhu.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
We propose a novel algorithm for offline reinforcement learning called Value Iteration with Perturbed Rewards (VIPeR), which amalgamates the pessimism principle with random perturbations of the value function. Most current offline RL algorithms explicitly construct statistical confidence regions to obtain pessimism via lower confidence bounds (LCB), which cannot easily scale to complex problems where a neural network is used to estimate the value functions. Instead, VIPeR implicitly obtains pessimism by simply perturbing the offline data multiple times with carefully-designed i.i.d. Gaussian noises to learn an ensemble of estimated state-action {value functions} and acting greedily with respect to the minimum of the ensemble. The estimated state-action values are obtained by fitting a parametric model (e.g., neural networks) to the perturbed datasets using gradient descent. As a result, VIPeR only needs  time complexity for action selection, while LCB-based algorithms require at least , where  is the total number of trajectories in the offline data. We also propose a novel data-splitting technique that helps remove a factor involving the log of the covering number in our bound. We prove that VIPeR yields a provable uncertainty quantifier with overparameterized neural networks and enjoys a bound on sub-optimality of , where  is the effective dimension,  is the horizon length and  measures the distributional shift. We corroborate the statistical and computational efficiency of VIPeR with an empirical evaluation on a wide set of synthetic and real-world datasets. To the best of our knowledge, VIPeR is the first algorithm for offline RL that is provably efficient for general Markov decision processes (MDPs) with neural network function approximation. 



%
 \end{abstract}

\section{Introduction}
Offline reinforcement learning (offline RL)~\citep{lange2012batch,levine2020offline} is a practical paradigm of RL for domains where active exploration is not permissible. Instead, the learner can access a fixed dataset of previous experiences available a priori. Offline RL finds applications in several critical domains where exploration is prohibitively expensive or even implausible, including healthcare~\citep{gottesman2019guidelines, nie2021learning}, recommendation systems~\citep{strehl2010learning, thomasAAAI17}, and  econometrics~\citep{Kitagawa18, athey2021policy}, among others. The recent surge of interest in this area and renewed research efforts have yielded several important empirical successes~\citep{chen2021decision, wang2022diffusion, wang2022bootstrapped, meng2021offline}. 



A key challenge in offline RL is to efficiently exploit the given offline dataset to learn an optimal policy in the absence of any further exploration. The dominant approaches to offline RL address this challenge by incorporating uncertainty from the offline dataset into  decision-making~\citep{buckman2020importance, jin2021pessimism, xiao2021optimality, nguyen2021offline, ghasemipour2022so, an2021uncertainty, bai2022pessimistic}. The main component of these uncertainty-aware approaches to offline RL is the \emph{pessimism} principle, which constrains the learned policy to the offline data and leads to various lower confidence bound (LCB)-based algorithms. However, these methods are not easily extended or scaled to complex problems where neural function approximation is used to estimate the value functions. In particular, it is costly to explicitly compute the statistical confidence regions of the model or value functions if the class of function approximator is given by overparameterized neural networks. For example, constructing the LCB for neural offline contextual bandits~\citep{nguyen2021offline} and RL \citep{xu2022provably} requires computing the inverse of a large covariance matrix whose size scales with the number of parameters in the neural network. This computational cost hinders the practical application of these provably efficient offline RL algorithms. Therefore, a largely open question is \emph{how to design provably  computationally efficient algorithms for offline RL with neural network function approximation.} 



In this work, we present a solution based on a computational approach that combines the pessimism principle with randomizing the value function~\citep{osband2016generalization, ishfaq2021randomized}. The algorithm is strikingly simple: we randomly perturb the offline rewards several times and act greedily with respect to the minimum of the estimated state-action values. The intuition is that taking the minimum from an ensemble of randomized state-action values can efficiently achieve pessimism with high probability while avoiding explicit computation of statistical confidence regions. We learn the state-action value function by training a neural network using gradient descent (GD). Further, we consider a novel data-splitting technique that helps remove the dependence on the potentially large log covering number in the learning bound. We show that the proposed algorithm yields a provable uncertainty quantifier with overparameterized neural network function approximation and achieves a sub-optimality bound of  , where  is the total number of episodes in the offline data,  is the effective dimension,  is the horizon length, and  measures the distributional shift. We achieve computational efficiency since the proposed algorithm only needs  time complexity for action selection, while LCB-based algorithms require   time complexity. We empirically corroborate the statistical and computational efficiency of our proposed algorithm on a wide set of synthetic and real-world datasets. The experimental results show that the proposed algorithm has a strong advantage in computational efficiency while outperforming LCB-based neural algorithms. To the best of our knowledge, ours is the first offline RL algorithm that is both provably and computationally efficient in general MDPs with neural network function approximation. 


















%
 \section{Related Work}


\paragraph{Randomized value functions for RL.} 
For online RL, \citet{osband2016generalization,osband2019deep} were the first to explore randomization of estimates of the value function for exploration. Their approach was inspired by posterior sampling for RL~\citep{osband2013more}, which samples a value function from a posterior distribution and acts greedily with respect to the sampled function. Concretely, \citet{osband2016generalization,osband2019deep} generate randomized value functions by injecting Gaussian noise into the training data and fitting a model on the perturbed data. \citet{jia2022learning} extended the idea of perturbing rewards to online contextual bandits with neural function approximation. \citet{ishfaq2021randomized} obtained a provably efficient method for online RL with general function approximation using the perturbed rewards. 
While randomizing the value function is an intuitive approach to obtaining optimism in online RL, obtaining pessimism from the randomized value functions can be tricky in offline RL. Indeed, \citet{ghasemipour2022so} point out a critical flaw in several popular existing methods for offline RL that update an ensemble of randomized Q-networks toward a \emph{shared} pessimistic temporal difference target. In this paper, we propose a simple fix to obtain pessimism properly by updating each randomized value function independently and taking the minimum over an ensemble of randomized value functions to form a pessimistic value function.








\paragraph{Offline RL with function approximation.} 
Provably efficient offline RL has been studied extensively for linear function approximation. \citet{jin2021pessimism} were the first to show that pessimistic value iteration is provably efficient for offline linear MDPs. \citet{Xiong2022NearlyMO,yinnear} improved upon \citet{jin2021pessimism} by leveraging variance reduction. \citet{xie2021bellman} proposed a Bellman-consistency assumption with general function approximation, which improves the bound of \citet{jin2021pessimism} by a factor of  when realized to finite action space and linear MDPs. \citet{wang2020statistical,zanette2021exponential} studied the statistical hardness of offline RL with linear function approximation via exponential lower bound, and \citet{foster2021offline} suggested that only realizability and strong uniform data coverage are not sufficient for sample-efficient offline RL. Beyond linearity, some works study offline RL for general function approximation, both parametric and nonparametric. These approaches are either based on Fitted-Q Iteration (FQI)~\citep{DBLP:journals/jmlr/MunosS08, DBLP:conf/icml/0002VY19, chen2019information, duan2021risk, duan2021optimal, hu2021fast, nguyentang2021sample} or the pessimism principle~\citep{uehara2021pessimistic,nguyen2021offline,jin2021pessimism}. While pessimism-based algorithms avoid the strong assumptions of data coverage used by FQI-based algorithms, they require an explicit computation of valid confidence regions and possibly the inverse of a large covariance matrix which is computationally prohibitive and does not scale to complex function approximation setting. This limits the applicability of pessimism-based, provably efficient offline RL to practical settings. A very recent work \cite{bai2022pessimistic} estimates the uncertainty for constructing LCB via the disagreement of bootstrapped Q-functions. However, the uncertainty quantifier is only guaranteed in linear MDPs and must be computed explicitly. 

We provide a more detailed discussion of our technical contribution in the context of existing literature in Section \ref{subsection: technical review}.










%
 \section{Preliminaries}
\label{section: preliminary}
In this section, we provide basic background on offline RL and overparameterized neural networks. 

\subsection{Episodic time-inhomogenous Markov decision processes (MDPs)}
A finite-horizon Markov decision process (MDP) is denoted as the tuple , where  is an arbitrary state space,  an arbitrary action space,  the episode length, and  the initial state distribution. We assume that  is finite but arbitrarily large, e.g.,  it can be as large as the total number of atoms in the observable universe . Let  denote the set of probability measures over . A time-inhomogeneous transition kernel , where  maps each state-action pair  to a probability distribution . Let  where  is the mean reward function at step . A policy  assigns each state  to a probability distribution, , over the action space  and induces a random trajectory  where , , . We define the state value function  and the action-state value function  at each timestep  as 
, and ,
where the expectation  is taken with respect to the randomness of the trajectory induced by . Let  denote the transition operator defined as . For any , we define the Bellman operator at timestep  as . The Bellman equations are given as follows. For any ,

where , and  denotes the summation over all . 
We define an optimal policy  as any policy that yields the optimal value function, i.e.  for any . For simplicity, we denote  and  as  and , respectively. The Bellman optimality equation can be written as

Define the occupancy density as   which is the probability that we visit state  and take action  at timestep  if we follow the policy   
We denote  by . 

\paragraph{Offline regime.} In the offline regime, the learner has access to a fixed dataset  generated a priori by some unknown behaviour policy . Here,  is the total number of trajectories, and  for any . Note that we allow the trajectory at any time  to depend on the trajectories at previous times. The goal of offline RL is to learn a policy , based on (historical data) ,
such that  achieves small sub-optimality, which we define as




\paragraph{Notation.} For simplicity, we write  and . We write  to hide logarithmic factors of the problem parameters  in the standard Big-Oh notation. We use  as the standard Omega notation. We write  if  and write  if . We write  iff  is a positive definite matrix.   denotes the  identity matrix. 



\vspace{-2pt}
\subsection{Overparameterized Neural Networks}
\label{subsetion: overparameterized nn}
In this paper, we consider neural function approximation setting where the state-action value function is approximated by a two-layer neural network. For simplicity, we denote  and view it as a subset of . Without loss of generality, we assume . We consider a standard two-layer neural network: ,  
where  is an even number,  is the ReLU activation function \citep{aroraunderstanding}, and . During the training, we initialize  via the symmetric initialization scheme \citep{gao2019convergence} as follows: For any , , and .\footnote{This symmetric initialization scheme makes  and  for any .}  
During the training, we optimize over  while the  are kept fixed, thus we write  as . Denote , and let  be the initial parameters of . We assume that the neural network is overparameterized, i.e, the width  is sufficiently larger than the number of samples . Overparameterization has been shown to be effective in studying the convergence and the interpolation behaviour of neural networks \citep{arora2019exact,allen2019convergence,hanin2019finite,cao2019generalization,belkin2021fit}. Under such an overparameterization regime, the dynamics of the training of the neural network can be captured using the framework of the neural tangent kernel (NTK) \citep{jacot2018neural}. 









 \section{Algorithm}





\begin{algorithm}[t]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data , a parametric function family  (e.g. neural networks), perturbed variances , number of bootstraps , regularization parameter , step size , number of gradient descent steps , and cutoff margin , split indices  where 
\State Initialize  and initialize  with initial parameter 
\For{}
\For{}
\State Sample  and 
\label{PERVI: sample noises}


\State Perturb the dataset  \Comment{\textit{Perturbation}}~
\label{PERVI: perturb data}
\State Let  (Algorithm \ref{algo:GD}) \Comment{\textit{Optimization}}
\label{PERVI: GD}

\EndFor 

\State Compute   \Comment{\textit{Pessimism}}~~~~
\label{PERVI: minimum of ensemble}






\State  and  \Comment{\textit{Greedy}}\qquad~
\label{PERVI: greedy policy}
\EndFor
\State \textbf{Output}: .
\end{algorithmic}
\caption{{Value Iteration with Perturbed Rewards (VIPeR)}}
\label{algorithm: PERVI}
\end{algorithm}

\begin{wrapfigure}{R}{0.5\textwidth}
\vspace{-20pt}
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\State \textbf{Input:} Regularization parameter , step size , number of gradient descent steps , perturbed dataset , regularization perturber , initial parameter  
\State  + 
\For{}
\State 
\EndFor 
\State \textbf{Output:} . 
\caption{GradientDescent}
\label{algo:GD}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{wrapfigure}

In this section, we present the proposed algorithm called Value Iteration with Perturbed Rewards, or VIPeR; see Algorithm~\ref{algorithm: PERVI} for the pseudocode. The key idea underlying VIPeR is to train a parametric model (e.g., a neural network) on a perturbed-reward dataset several times and act pessimistically by picking the minimum over an ensemble of estimated state-action value functions. 
In particular, at each timestep , we draw  independent samples of zero-mean Gaussian noise with variance . We use these samples to perturb the sum of the observed rewards, , and the estimated value function with a one-step lookahead, i.e.,   (see Line \ref{PERVI: perturb data} of Algorithm~\ref{algorithm: PERVI}). The weights  are then updated by minimizing the perturbed regularized squared loss on  using gradient descent (Line \ref{PERVI: GD}). We pick the value function pessimistically by selecting the minimum over the finite ensemble. The chosen value function is truncated at  (see Line \ref{PERVI: minimum of ensemble}), where  is a small cutoff margin (more on this when we discuss the theoretical analysis). The returned policy is greedy with respect to the truncated pessimistic value function (see Line \ref{PERVI: greedy policy}).

It is important to note that we split the trajectory indices  evenly into  disjoint buckets  , where  for \footnote{Without loss of generality, we assume .}, as illustrated in Figure \ref{fig: data split}. The estimated  is thus obtained only from the offline data with (trajectory) indices from  along with . This novel design removes the data dependence structure in offline RL with function approximation \citep{nguyentang2021sample} and avoids a factor involving the log of the covering number in the bound on the sub-optimality of Algorithm~\ref{algorithm: PERVI}, as we show in Section \ref{lemma: bound ERM with the Bellman target}. 























To deal with the non-linearity of the underlying MDP, we use a two-layer fully connected neural network as the parametric function family  in Algorithm \ref{algorithm: PERVI}. In other words, we approximate the state-action values: , as described in Section \ref{subsetion: overparameterized nn}. We use two-layer neural networks to simplify the computational analysis. We utilize gradient descent to train the state-action value functions , on perturbed rewards. The use of gradient descent is for the convenience of computational analysis, and our results can be extended to stochastic gradient descent by leveraging recent advances in the theory of deep learning~\citep{allen2019convergence,cao2019generalization}, albeit with a more involved analysis. 



Existing offline RL algorithms utilize estimates of statistical confidence regions to achieve pessimism in the offline setting. Explicitly constructing these confidence bounds is computationally expensive in complex problems where a neural network is used for function approximation. For example, the lower-confidence-bound-based algorithms in neural offline contextual bandits \citep{nguyen2021offline} and RL \citep{xu2022provably} require computing the inverse of a large covariance matrix with the size scaling with the number of network parameters. This is computationally prohibitive in most practical settings. Algorithm~\ref{algorithm: PERVI} (VIPeR) avoids such expensive computations while still obtaining provable pessimism and guaranteeing a  rate of  on the sub-optimality, as we show in the next section. 









\begin{wrapfigure}{r}{0.24\textwidth}
    \vspace{-45pt}
    \centering
    \includegraphics[scale=0.45]{iclr2023/figs/data_split.pdf}
    \vspace{-10pt}
    \caption{Data splitting.}
    \label{fig: data split}
    \vspace{-14pt}
\end{wrapfigure}






































%
 \section{Sub-optimality Analysis}
\label{section: subopt analysis}


Next, we provide a theoretical guarantee on the sub-optimality of VIPeR for the function approximation class, , represented by (overparameterized) neural networks. Our analysis builds on the recent advances in generalization and optimization of deep neural networks \citep{arora2019exact, allen2019convergence, hanin2019finite, cao2019generalization, belkin2021fit} that leverage the observation that the dynamics of the neural parameters learned by (stochastic) gradient descent can be captured by the corresponding neural tangent kernel (NTK) space~\citep{jacot2018neural} when the network is overparameterized. 


Next, we recall some definitions and state our key assumptions, formally. 






\begin{defn}[NTK \citep{jacot2018neural}]
The NTK kernel  is defined as

{where .}
\label{definition: ntk}
\end{defn}
Let  denote the reproducing kernel Hilbert space (RKHS) induced by the NTK, . {Since is a universal kernel \citep{DBLP:conf/iclr/JiTX20}, we have that  is dense in the space of continuous functions on (a compact set)  \citep{Rahimi08}.} 

\begin{defn}[Effective dimension]
For any , the effective dimension of the NTK matrix on data  is defined as 

where  is the Gram matrix of  on the data . We further define . 
\label{definition: effective dimension}
\end{defn}
\begin{remark}
Intuitively, the effective dimension  measures the number of principal dimensions over which the projection of the data  in the RKHS  is spread. It was first introduced by \citet{valko2013finite} for kernelized contextual bandits and was subsequently adopted by \citet{yang2020reinforcement} and \citet{zhou2020neural} for kernelized RL and neural contextual bandits, respectively. The effective dimension is data-dependent and can be bounded by  in the worst case (see Section \ref{section: extended discussion} for more details).\footnote{Note that this is the worst-case bound, and the effective dimension can be significantly smaller in practice.} 
\end{remark}



{
\begin{defn}[RKHS of the infinite-width NTK] 
Define  

where  is any function,  is the probability density function of , and  is some positive constant.
\label{eq: target function class}
\end{defn}
}



We make the following assumption about the regularity of the underlying MDP under function approximation. 
\begin{assumption}[Completeness]
For any  and any , .\footnote{We consider  instead of  due to the cutoff margin  in Algorithm \ref{algorithm: PERVI}.}
\label{assumption: completeness}
\end{assumption}
Assumption \ref{assumption: completeness} ensures that the Bellman operator  can be captured by an infinite-width neural network. This assumption is mild as   is a dense subset of ~\citep[Lemma~C.1]{gao2019convergence} when , thus  is an expressive function class when  is sufficiently large. Moreover, similar assumptions have been used in many prior works on provably efficient RL with function approximation \citep{cai2019neural,wang2020reinforcement,yang2020function,nguyentang2021sample}.

Next, we present a bound on the suboptimality of the policy  returned by Algorithm~\ref{algorithm: PERVI}. Recall that we use the initialization scheme described in Section~\ref{subsetion: overparameterized nn}. Fix any . \
\begin{theorem}
Let .  
Let { be some high-order polynomial of the problem parameters}, , , ,
, 
and , where  is the cumulative distribution function of the standard normal distribution. Then, under Assumption \ref{assumption: completeness}, with probability at least , for any , we have that 

where .







\label{theorem: main theorem}
\end{theorem}

\begin{remark}
Theorem \ref{theorem: main theorem} shows that the randomized design in our proposed algorithm yields a provable uncertainty quantifier even though we do not explicitly maintain any confidence regions in the algorithm. {The implicit pessimism via perturbed rewards introduces an extra factor of  into the confidence parameter .} 
\label{remark: simplified params}
\end{remark}


We build upon Theorem \ref{theorem: main theorem} to obtain an explicit bound using the following data coverage assumption. 
\begin{assumption}[Optimal-Policy Concentrability]
,  . 


\label{assumption: OPC}
\end{assumption}
Assumption \ref{assumption: OPC} requires any positive-probability trajectory induced by the optimal policy to be covered by the behavior policy. This data coverage assumption is significantly milder than the uniform coverage assumptions in many FQI-based offline RL algorithms \citep{DBLP:journals/jmlr/MunosS08,chen2019information,nguyentang2021sample} and is common in pessimism-based algorithms \citep{rashidinejad2021bridging,nguyen2021offline,Chen2022OfflineRL,zhan2022offline}. 
\begin{theorem}
For the same parameter settings and the same assumption as in Theorem \ref{theorem: main theorem}, we have that 
with probability at least ,

where . 
\label{theorem: explicit bound}
\end{theorem}

\begin{remark}
Theorem \ref{theorem: explicit bound} shows that with appropriate parameter choice, VIPeR achieves a sub-optimality of  .
Compared to \cite{yang2020function}, we improve by a factor of  for some  at the expense of . When realized to a linear MDP in ,  and our bound reduces into  which improves the bound  of PEVI \citep[Corollary~4.6]{jin2021pessimism} by a factor of . We provide the result summary and comparison in Table \ref{tab:my_label} and give a more detailed discussion in Subsection \ref{subsection: comparison with other works in details}. 
\end{remark}



\begin{table}[]
\centering
    \def\arraystretch{1.9}\resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
work & bound & i.i.d? & explorative data? & finite spectrum? & matrix inverse? & opt \\
       \hline 
       \hline
\cite{jin2021pessimism}   &  & no & yes & yes  & yes & analytical \\ 
      \hline 
      \cite{yang2020function} &  & no & -- & no & yes & oracle \\ 
      \hline
      \cite{xu2022provably} &  & yes & yes & yes  & yes & oracle \\ 
      \hline 
      \textbf{This work} & \textbf{} & \textbf{no} & \textbf{no} & \textbf{no} & \textbf{no} & \textbf{GD} \\
      \hline
    \end{tabular}
    }
    \caption{State-of-the-art results for offline RL with function approximation. The third and the fourth columns ask if the corresponding result needs the data to be i.i.d, and well-explored, respectively; the fifth column asks if the induced RKHS needs to have a finite spectrum; the sixth column asks if the algorithm needs to invert a covariance matrix and the last column presents the optimizer being used. Here  is the log of the covering number.}
    \label{tab:my_label}
\end{table}

















































































































%
 \vspace{-5pt}
\section{Experiments}
In this section, we empirically evaluate the proposed algorithm VIPeR against several state-of-the-art baselines, including (a) PEVI~\citep{jin2021pessimism}, which explicitly constructs lower confidence bound (LCB) for pessimism in a linear model (thus, we rename this algorithm as LinLCB for convenience in our experiments); (b) NeuraLCB~\citep{nguyen2021offline} which explicitly constructs an LCB using neural network gradients; (c) NeuraLCB (Diag), which is NeuraLCB with a diagonal approximation for estimating the confidence set as suggested in NeuraLCB \citep{nguyen2021offline}; (d) Lin-VIPeR which is VIPeR realized to the linear function approximation instead of neural network function approximation; (e) NeuralGreedy (LinGreedy, respectively) which uses neural networks (linear models, respectively) to fit the offline data and act greedily with respect to the estimated state-action value functions without any pessimism. Note that when the parametric class, , in Algorithm \ref{algorithm: PERVI} is that of neural networks, we refer to VIPeR as Neural-VIPeR. We do not utilize data splitting in the experiments. We provide further algorithmic details of the baselines in Section~\ref{section: baseline algorithms}. 


We evaluate all algorithms in two problem settings: (1) the underlying MDP is a linear MDP whose reward functions and transition kernels are linear in some known feature map~\citep{jin2020provably}, and (2) the underlying MDP is non-linear with horizon length  (i.e., non-linear contextual bandits)~\citep{zhou2020neural}, where the reward function is either synthetic or constructed from MNIST dataset~\citep{lecun1998gradient}. We also evaluate (a variant of) our algorithm and show its strong performance advantage in the D4RL benchmark~\citep{DBLP:journals/corr/abs-2004-07219} in Section~\ref{subsection: d4rl}. We implemented all algorithms in Pytorch~\citep{paszke2019pytorch} on a server with Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 755G RAM, and one NVIDIA Tesla V100 Volta GPU Accelerator 32GB Graphics Card.\footnote{Our code is available here: \url{https://github.com/thanhnguyentang/neural-offline-rl}.}















\subsection{Linear MDPs}
\label{subsection: linear mdp}


We first test the effectiveness of pessimism implicit in VIPeR (Algorithm~\ref{algorithm: PERVI}). To that end, we construct a hard instance of linear MDPs~\citep{yinnear,min2021variance}; due to page limitation, we defer the details of our construction to Section~\ref{subsection: description of linear mdp}. We test for different values of  and report the sub-optimality of LinLCB, Lin-VIPeR, and LinGreedy, averaged over  runs, in Figure \ref{fig: linear mdp}. 
We find that LinGreedy, which is uncertainty-agnostic, fails to learn from offline data and has poor performance in terms of sub-optimality when compared to pessimism-based algorithms LinLCB and Lin-VIPeR. Further, LinLCB outperforms Lin-VIPeR when  is smaller than , but the performance of the two algorithms matches for larger sample sizes. Unlike LinLCB, Lin-VIPeR does not construct any confidence regions or require computing and inverting large (covariance) matrices. The Y-axis is in log scale; thus, Lin-VIPeR already has small sub-optimality in the first  samples. These show the effectiveness of the randomized design for pessimism implicit in Algorithm \ref{algorithm: PERVI}. 















\begin{figure}
\centering
    \includegraphics[scale=0.27]{iclr2023/figs/VIPeR_linmdp.png}
    \caption{Empirical results of sub-optimality (in log scale) on linear MDPs.}
    \label{fig: linear mdp}
    \vspace{-10pt}
\end{figure}




\subsection{Neural Contextual Bandits}
\label{subsection: main neural contextual bandits}
\begin{figure}[h!]
    \centering
    \vspace{-10pt}
\subfigure[]{\includegraphics[width=0.322\textwidth]{iclr2023/figs/cosine_regret_VIPeR.png}}
    \subfigure[]{\includegraphics[width=0.322\textwidth]{iclr2023/figs/exp_regret_VIPeR.png}}
    \subfigure[]{\includegraphics[width=0.322\textwidth]{iclr2023/figs/mnist_regret_VIPeR.png}}
    \vspace{-15pt}
    \caption{Sub-optimality (on log-scale) vs. sample size (K) for neural contextual bandits with following reward functions: (a) , (b) , and (c)~MNIST. }
\label{fig: neural contextuable bandits}
\end{figure}

Next, we compare the performance and computational efficiency of various algorithms against VIPeR when neural networks are employed. For simplicity, we consider contextual bandits, a special case of MDPs with horizon . 
Following~\cite{zhou2020neural, nguyen2021offline}, we use the bandit problems specified by the following reward functions: (a) ; (b) , where  and  are generated uniformly at random from the unit sphere  with  and ; (c) MNIST, where  if  is the true label of the input image  and , otherwise. To predict the value of different actions from the same state  using neural networks, we transform a state  into -dimensional vectors  and train the network to map  to  given a pair of data . For Neural-VIPeR, NeuralGreedy, NeuraLCB, and NeuraLCB (Diag), we use the same neural network architecture with two hidden layers of width  and train the network with Adam optimizer \citep{kingma2014adam}. Due to page limitations, we defer other experimental details and hyperparameter setting to Section~\ref{subsection: extended page for neural contextual bandit experiment}. We report the sub-optimality averaged over  runs in Figure~\ref{fig: neural contextuable bandits}. We see that algorithms that use a linear model, i.e., LinLCB and Lin-VIPeR significantly underperform neural-based algorithms, i.e., NeuralGreedy, NeuraLCB, NeuraLCB (Diag) and Neural-VIPeR, attesting to the crucial role neural representations play in RL for non-linear problems. It is also interesting to observe from the experimental results that NeuraLCB does not always outperform its diagonal approximation, NeuraLCB (Diag) (e.g., in Figure~\ref{fig: neural contextuable bandits}(b)), putting a question mark on the empirical effectiveness of NTK-based uncertainty for offline RL. Finally, Neural-VIPeR outperforms all algorithms in the tested benchmarks, suggesting the effectiveness of our randomized design with neural function approximation. 





\begin{figure}[h]
    \vspace{-10pt}
    \centering
    \subfigure[]{\includegraphics[width=0.44\textwidth]{iclr2023/figs/quadratic_VIPeR_time.png}} 
    \subfigure[]{\includegraphics[width=0.42\textwidth]{iclr2023/figs/quadratic_elapsed_time_vs_m_VIPeR.png}}
    \vspace{-10pt}
    \caption{Elapsed time (in seconds) for action selection in the contextual bandits problem with : (a) Runtime of action selection versus the number of (offline) data points , and (b) runtime of action selection versus the network width  (for ).}
    \label{fig: time for action selection}
    \vspace{-8pt}
\end{figure}

\begin{wrapfigure}{r}{0.37\textwidth}
\vspace{-10pt}
    \centering
    \includegraphics[scale=0.43]{iclr2023/figs/subopt_vs_M.png}
\caption{Sub-optimality of Neural-VIPeR versus different values of .}
    \label{fig: subopt vs M}
    \vspace{-5pt}
\end{wrapfigure}

Figure~\ref{fig: time for action selection} shows the average runtime for action selection of neural-based algorithms NeuraLCB, NeuraLCB (Diag), and Neural-VIPeR. We observe that algorithms that use explicit confidence regions, i.e., NeuraLCB and NeuraLCB (Diag), take significant time selecting an action when either the number of offline samples  or the network width  increases. This is perhaps not surprising because NeuraLCB and NeuraLCB (Diag) need to compute the inverse of a large covariance matrix to sample an action and maintain the confidence region for each action per state. The diagonal approximation significantly reduces the runtime of NeuraLCB, but the runtime still scales with the number of samples and the network width. In comparison, the runtime for action selection for Neural-VIPeR is constant. Since NeuraLCB, NeuraLCB (Diag), and Neural-VIPeR use the same neural network architecture, the runtime spent training one model is similar. The only difference is that Neural-VIPeR trains  models while NeuraLCB and NeuraLCB (Diag) train a single model. However, as the perturbed data in Algorithm~\ref{algorithm: PERVI} are independent, training  models in Neural-VIPeR is embarrassingly parallelizable. 

















Finally, in Figure~\ref{fig: subopt vs M}, we study the effect of the ensemble size on the performance of Neural-VIPeR. We use  different values of  for sample size . We find that the sub-optimality of Neural-VIPeR decreases graciously as  increases. Indeed, the grid search from the previous experiment in Figure~\ref{fig: neural contextuable bandits} also yields  and  from the search space  as the best result. This suggests that the ensemble size can also play an important role as a hyperparameter that can determine the amount of pessimism needed in a practical setting.






%
 \section{Conclusion}
We propose a novel algorithmic approach for offline RL that involves randomly perturbing value functions and pessimism. Our algorithm eliminates the computational overhead of explicitly maintaining a valid confidence region and computing the inverse of a large covariance matrix for pessimism. We bound the suboptimality of the proposed algorithm as . We support our theoretical claims of computational efficiency and the effectiveness of our algorithm with extensive experiments. 






\section*{Acknowledgements}
This research was supported, in part, by  DARPA~GARD award HR00112020004, NSF CAREER award IIS-1943251, an award from the Institute of Assured Autonomy, and Spring 2022 workshop on ``Learning and Games'' at the Simons Institute for the Theory of Computing. \bibliography{main}
\bibliographystyle{iclr2023/iclr2023_conference}
\newpage
\appendix
\section{Experiment Details}
\subsection{Linear MDPs}
\label{subsection: description of linear mdp}
In this subsection, we provide further details to the experiment setup used in Subsection \ref{subsection: linear mdp}. We describe in detail a variant of the hard instance of linear MDPs \citep{yinnear} used in our experiment. The linear MDP has , , and the feature dimension . Each action  is represented by its binary encoding vector  with entry being either  or . The feature mapping  is given by , where  if  and  otherwise. The true measure  is given by   where  are generated uniformly at random and  is the XOR operator. We define  where . Recall that the transition follows  and the mean reward . We generated a priori  trajectories using the behavior policy , where for any  we set , where we set .

We run over  and . We set  for all algorithms. For Lin-VIPeR, we grid searched  and . For LinLCB, we grid searched its uncertainty multiplier . The sub-optimality metric is used to compare algorithms. For each , each algorithm was executed for  times and the averaged results (with std) are reported in Figure \ref{fig: linear mdp}. 

\subsection{Neural Contextual Bandits}
\label{subsection: extended page for neural contextual bandit experiment}
In this subsection, we provide in detail the experimental and hyperparameter setup in our experiment in Subsection \ref{subsection: main neural contextual bandits}. For Neural-VIPeR, NeuralGreedy, NeuraLCB and NeuraLCB (Diag), we use the same neural network architecture with two hidden layers whose width , train the network with Adam optimizer \citep{kingma2014adam} with learning rate being grid-searched over  and batch size of . For NeuraLCB, NeuraLCB (Diag), and LinLCB, we grid-searched  over . For Neural-VIPeR and Lin-VIPeR, we grid-searched  over  and  over . We did not run NeuraLCB in MNIST as the inverse of a full covariance matrix in this case is extremely expensive. We fixed the regularization parameter  for all algorithms. Offline data is generated by the -optimal policy which generates non-optimal actions with probability  and optimal actions with probability . We set  in our experiments. To estimate the expected sub-optimality, we randomly obtain  novel samples (i.e. not used in training) to compute the average sub-optimality and keep these same samples for all algorithms. 

\subsection{Experiment in D4RL Benchmark}
\label{subsection: d4rl}
In this subsection, we evaluate the effectiveness of the reward perturbing design of VIPeR in the Gym domain in the D4RL benchmark \citep{DBLP:journals/corr/abs-2004-07219}. The Gym domain has three environments (HalfCheetah, Hopper, and Walker2d) with five datasets (random, medium, medium-replay, medium-expert, and expert), making up 15 different settings.


\paragraph{Design.} To adapt the design of VIPeR to continuous control, we use the actor-critic framework. Specifically, we have  critics  and one actor , where  and  are the learnable parameters for the critics and actor, respectively. Note that in the continuous domain, we consider discounted MDP with discount factor , instead of finite-time episode MDP as we initially considered in our setting in the main paper. In the presence of the actor , there are two modifications to Algorithm \ref{algorithm: PERVI}. The first modification is that when training the critics , we augment the training loss in Algorithm \ref{algo:GD} with a new penalization term. Specifically, the critic loss for  on a training sample  (sampled from the offline data ) is 

where  has the same value of the current  but is kept fixed,  and  is Gaussian noise, and  is a penalization parameter (note that  here is totally different from the  in Theorem \ref{theorem: main theorem}). The penalization term  discourages overestimation in the value function estimate  for out-of-distribution (OOD) actions . Our design of  is initially inspired by the OOD penalization in \cite{bai2022pessimistic} that creates a pessimistic pseudo target for the values at OOD actions. Note that we do not need any penalization for OOD actions in our experiment for contextual bandits in Section \ref{subsection: main neural contextual bandits}. This is because in the contextual bandit setting in Section \ref{subsection: main neural contextual bandits} the action space is finite and not large, thus the offline data often sufficiently cover all good actions. In the continuous domain such as the Gym domain of D4RL, however, it is almost certain that there are actions that are not covered by the offline data since the action space is continuous. We also note that the inclusion of the OOD action penalization term  in this experiment does not contradict our guarantee in Theorem \ref{theorem: main theorem} since in the theorem we consider finite action space while in this experiment we consider continuous action space.  We argue that the inclusion of some regularization for OOD actions (e.g., ) is necessary for the continuous domain. \footnote{In our experiment, we also observe that without this penalization term, the method struggles to learn any good policy. However, using only the penalization term without the first term in Eq. (\ref{eq: new critic loss for continuous domain}), we observe that the method cannot learn either.}



The second modification to Algorithm \ref{algorithm: PERVI} for the continuous domain is the actor training, which is the implementation of policy extraction in line \ref{PERVI: greedy policy}  of Algorithm \ref{algorithm: PERVI}. Specifically, to train the actor  given the ensemble of critics ,  we use soft actor update in \cite{haarnoja2018soft} via

which is trained using gradient ascent in practice. Note that in the discrete action domain, we do not need such actor training as we can efficiently extract the greedy policy with respect to the estimated action-value functions when the action space is finite. Also note that we do not use data splitting and value truncation as in the original design of Algorithm \ref{algorithm: PERVI}. 

\paragraph{Hyperparameters.} For the hyper-parameters of our training, we set  and the noise variance . For , we decrease it from  to  by linear decay for the first 50K steps and exponential decay for the remaining steps. For the other hyperparameters of actor-critic training, we fix them the same as in \cite{bai2022pessimistic}. Specifically, the Q-network is the fully connected neural network with three hidden layers all of which has  neurons. The learning rate for the actor and the critic are  and , respectively. The optimizer is Adam. 

\paragraph{Results.} We compare VIPeR with several state-of-the-art algorithms, including (i) BEAR \citep{DBLP:conf/nips/KumarFSTL19} that use MMD distance to constraint policy to the offline data, (ii) UWAC \citep{DBLP:conf/icml/0001ZSSZSG21} that improves BEAR using dropout uncertainty, (iii) CQL \citep{DBLP:conf/nips/KumarZTL20} that minimizes Q-values of OOD actions, (iv) MOPO \citep{DBLP:conf/nips/YuTYEZLFM20} that uses model-based uncertainty via ensemble dynamics, (v) TD3-BC \citep{DBLP:conf/nips/FujimotoG21} that uses adaptive behavior cloning, and (vi) PBRL \citep{bai2022pessimistic} that use uncertainty quantification via disagreement of bootstrapped Q-functions. We follow the evaluation protocol in \cite{bai2022pessimistic}. We run our algorithm for five seeds and report the average final evaluation scores with standard deviation. We report the scores of our method and the baselines in Table \ref{tab: d4rl result}. We can see that our method has a strong advantage of good performance (highest scores) in 11 out of 15 settings, and has good stability (small std) in all settings. Overall, we also have the strongest average scores aggregated over all settings. 

\begin{table}[]
    \centering
    \def\arraystretch{1.3}\resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllll}
        \hline 
        & & BEAR & UWAC & CQL & MOPO & TD3-BC & PBRL & VIPeR \\
        \hline 
        \multirow{3}{*}{\rotatebox{90}{Random}} 
        & HalfCheetah & 2.3 ±0.0 & 2.3 ±0.0 & 17.5 ±1.5 & \textbf{35.9} ±2.9 & 11.0 ±1.1 & 11.0 ±5.8 &14.5	±2.1\\
        & Hopper & 3.9 ±2.3 &2.7 ±0.3 &7.9 ±0.4 &16.7 ±12.2 &8.5 ±0.6 &26.8 ±9.3 &\textbf{31.4}	±0.0\\ 
       & Walker2d & 12.8 ±10.2 &2.0 ±0.4 &5.1 ±1.3 &4.2 ±5.7 &1.6 ±1.7 &8.1 ±4.4 &\textbf{20.5}	±0.5\\
       \hline 
        \multirow{3}{*}{\rotatebox{90}{Medium}} 
        & HalfCheetah & 43.0 ±0.2 &42.2 ±0.4 &47.0 ±0.5 &\textbf{73.1} ±2.4 &48.3 ±0.3 &57.9 ±1.5 &58.5	±1.1\\
         & Hopper & 51.8 ±4.0 &50.9 ±4.4 &53.0 ±28.5 &38.3 ±34.9 &59.3 ±4.2 &75.3 ±31.2 &\textbf{99.4}	±6.2\\ 
       & Walker2d & -0.2 ±0.1 &75.4 ±3.0 &73.3 ±17.7 &41.2 ±30.8 &83.7 ±2.1 &\textbf{89.6} ±0.7 &\textbf{89.6}	±1.2\\
       \hline 
       \multirow{3}{*}{\rotatebox{90}{Medium}} \multirow{3}{*}{\rotatebox{90}{Replay}} 
       & HalfCheetah & 36.3 ±3.1 &35.9 ±3.7 &45.5 ±0.7 &\textbf{69.2} ±1.1 &44.6 ±0.5 &45.1 ±8.0 &45.0	±8.6\\
         & Hopper & 52.2 ±19.3 &25.3 ±1.7 &88.7 ±12.9 &32.7 ±9.4 &60.9 ±18.8 &\textbf{100.6} ±1.0 &\textbf{100.2}	±1.0\\ 
       & Walker2d & 7.0 ±7.8 &23.6 ±6.9 &81.8 ±2.7 &73.7 ±9.4 &81.8 ±5.5 &77.7 ±14.5 &\textbf{83.1}	±4.2\\
        \hline 
       \multirow{3}{*}{\rotatebox{90}{Medium}} \multirow{3}{*}{\rotatebox{90}{Expert}} 
       & HalfCheetah & 46.0 ±4.7 &42.7 ±0.3 &75.6 ±25.7 &70.3 ±21.9 &90.7 ±4.3 &92.3 ±1.1 &\textbf{94.2}	±1.2\\
         & Hopper & 50.6 ±25.3 &44.9 ±8.1 &105.6 ±12.9 &60.6 ±32.5 &98.0 ±9.4 &\textbf{110.8} ±0.8 &\textbf{110.6}	±1.0\\ 
       & Walker2d & 22.1 ±44.9 &96.5 ±9.1 &107.9 ±1.6 &77.4 ±27.9 &\textbf{110.1} ±0.5 &\textbf{110.1} ±0.3 &\textbf{109.8}	±0.5\\
       \hline
        \multirow{3}{*}{\rotatebox{90}{Expert}} 
        & HalfCheetah & 92.7 ±0.6 &92.9 ±0.6 &96.3 ±1.3 &81.3 ±21.8 &96.7 ±1.1 &92.4 ±1.7 &\textbf{97.4}	±0.9\\
        & Hopper & 54.6 ±21.0 &110.5 ±0.5 &96.5 ±28.0 &62.5 ±29.0 &107.8 ±7 &\textbf{110.5} ±0.4 &\textbf{110.8}	±0.4\\ 
       & Walker2d & 106.6 ±6.8 &108.4 ±0.4 &108.5 ±0.5 &62.4 ±3.2 &\textbf{110.2} ±0.3 &108.3 ±0.3 &108.3	±0.2\\
       \hline 
       & Average & 38.78 ±10.0 &50.41 ±2.7 &67.35 ±9.1 &53.3 ±16.3 &67.55 ±3.8 &74.37 ±5.3 &\textbf{78.2}	±1.9\\
       \hline
    
    \end{tabular}
    }
    \caption{{Average normalized score and standard deviation of all algorithms over five seeds in the Gym domain in the ``v2'' dataset of D4RL \citep{DBLP:journals/corr/abs-2004-07219}. The scores for all the baselines are from Table 1 of \cite{bai2022pessimistic}. The highest scores are highlighted.}}
    \label{tab: d4rl result}
\end{table}
\color{black} \section{Extended Discussion}
\label{section: extended discussion}
Here we provide extended discussion of our result. 
\subsection{Comparison with other works and discussion}
\label{subsection: comparison with other works in details}

We provide further discussion regarding comparison with other works in the literature.  
\paragraph{Comparing to \cite{jin2021pessimism}.}
When the underlying MDP reduces into a linear MDP, if we use the linear model as the plug-in parametric model in Algorithm \ref{algorithm: PERVI},  our bound reduces into  which improves the bound  of PEVI \citep[Corollary~4.6]{jin2021pessimism} by a factor of  and worsen by a factor of  due to the data splitting. Thus, our bound is more favorable in the linear MDPs with high-dimensional features. Moreover, our bound is guaranteed in more practical scenarios where the offline data can have been adaptively generated and is not required to uniformly cover the state-action space. The explicit bound  of PEVI \citep[Corollary~4.6]{jin2021pessimism} is obtained under the assumption that the offline data have uniform coverage and are generated independently on the episode basis. 

\paragraph{Comparing to \cite{yang2020function}.} Though \citet{yang2020function} work in the online regime, it shares some part of the literature with our work in function approximation for RL. Besides different learning regimes (offline versus online), we offer three key distinctions which can potentially be used in the online regime as well: (i) perturbed rewards, (ii) optimization, and (iii) data split. Regarding (i), our perturbed reward design can be applied to online RL with function approximation to obtain a provably efficient online RL that is computationally efficient and thus remove the need of maintaining explicit confidence regions and performing the inverse of a large covariance matrix. Regarding (ii), we incorporate the optimization analysis into our algorithm which makes our algorithm and analysis more practical. We also note that unlike \citep{yang2020function}, we do not make any assumption on the eigenvalue decay rate of the empirical NTK kernel as the empirical NTK kernel is data-dependent. Regarding (iii), our data split technique completely removes the factor  in the bound at the expense of increasing the bound by a factor of . In complex models, such log covering number can be excessively larger than the horizon , making the algorithm too optimistic in the online regime (optimistic in the offline regime, respectively). For example, the target function class is RKHS with a -polynomial decay, the log covering number scales as \citep[Lemma~D1]{yang2020function},

for some . In the case of two-layer ReLU NTK,  \citep{bietti2019inductive}, thus  which is much larger than  when the size of dataset is large. Note that our data-splitting technique is general that can be used in the online regime as well. 

\paragraph{Comparing to \cite{xu2022provably}.} \citet{xu2022provably} consider a different setting where per-timestep rewards are not available and only the total reward of the whole trajectory is given. Used with neural function approximation, they obtain  where  is their effective dimension. Note that \citet{xu2022provably} do not use data splitting and still achieve the same order of  as our result with data splitting. It at first might appear that our bound is inferior to their bound as we pay the cost of  due to data splitting. However, to obtain that bound, they make three critical assumptions: (i) the offline data trajectories are independently and identically distributed (i.i.d.) (see their Assumption 3), (ii) the offline data is uniformly explorative over all dimensions of the feature space (also see their Assumption 3), and (iii) the eigenfunctions of the induced NTK RKHS has finite spectrum (see their Assumption 4). The i.i.d. assumption under the RKHS space with finite dimensions (due to the finite spectrum assumption) and the well-explored dataset is critical in their proof to use a matrix concentration that does not incur an extra factor of  as it would normally do without these assumptions (see Section E, the proof of their Lemma 2). Note that the celebrated ReLU NTK does not satisfy the finite spectrum assumption \citep{bietti2019inductive}. Moreover, we do not make any of these three assumptions above for our bound to hold. That suggests that our bound is much more general. In addition, we do not need to compute any confidence regions nor perform the inverse of a large covariance matrix. 

\paragraph{Comparing to \cite{yin2022offline}.} During the submission of our work, a concurrent work of \cite{yin2022offline} appeared online. \citet{yin2022offline} study provably efficient offline RL with a general parametric function approximation that unifies the guarantees of offline RL in linear and generalized linear MDPs, and beyond with potential applications to other classes of functions in practice. We remark that the result in \cite{yin2022offline} is orthogonal/complementary to our paper since they consider the parametric class with third-time differentiability which cannot apply to neural networks (not necessarily overparameterized) with non-smooth activation such as ReLU. In addition, they do not consider reward perturbing in their algorithmic design or optimization errors in their analysis. 

\color{black}




\subsection{Worse-Case Rate of Effective Dimension}
In the main paper, we prove an  sub-optimality bound which depends on the notion of effective dimension defined in Definition \ref{definition: effective dimension}. Here we give a worst-case rate of the effective dimension  for the two-layer ReLU NTK. We first briefly review the background of RKHS. 


Let  be an RKHS defined on  with kernel function . Let  and  be the inner product and the RKSH norm on . By the reproducing kernel property of , there exists a feature mapping  such that  and . We assume that the kernel function  is uniformly bounded, i.e. . Let  be the space of square-integral functions on  with respect to the Lebesgue measure and let  be the inner product on . The kernel function  induces an integral operator  defined as 


By Mercer's theorem \citep{steinwart2008support},  has countable and positive eigenvalues  and eigenfunctions . The kernel function and  can be expressed as 


Now consider the NTK defined in Definition \ref{definition: ntk}: 


It follows from \cite[Proposition~1]{bietti2019inductive} that . Thus, by \cite[Theorem~5]{srinivas2009gaussian}, the data-dependent effective dimension of  can be bounded in the worst case by  


We remark that this is the worst-case bound that considers uniformly over all possible realizable of training data. The effective dimension  is on the other hand data-dependent, i.e. its value depends on the specific training data at hand thus  can be actually much smaller than the worst-case rate. 







 \section{Proof of Theorem \ref{theorem: main theorem} and Theorem \ref{theorem: explicit bound}}
\begin{table}
    \centering
    \def\arraystretch{2.2}\resizebox{\textwidth}{!}{
    \begin{tabular}{cc}
        \hline
        Parameters & Meaning/Expression \\ 
        \hline
        \hline
         & Network width \\
         & Regularization parameter \\ 
         & Learning rate \\ 
         & Number of bootstraps \\ 
         & Noise variances \\ 
         & Number of GD steps \\
         & Cutoff margin \\
         & Number of offline episodes \\
         & Radius parameter \\
         & Failure level \\ 
         & bucket size,  \\ 
         & index buckets,  \\ 
         & Parameter radius of the Bellman operator \\
        \hline
         &   \\
         &  \\ 
 &   \\
 &  \\
 &   \\
 &   \\ 
         &  \\
 &  \\
 &  \\ 
        \multirow{ 2}{*}{} &  \\
        &  \\
\end{tabular}
    }
    \caption{The problem parameters and the additional parameters that we introduce for our proofs. Here , , and  are some absolute constants independent of the problem parameters.}
    \label{tab: key parameters in the proofs}
\end{table}



In this section, we provide both the outline and detailed proofs of Theorem \ref{theorem: main theorem} and Theorem \ref{theorem: explicit bound}. 

\subsection{Technical Review and Proof Overview}
\label{subsection: technical review}


\paragraph{Technical Review.} In what follows, we provide more detailed discussion when placing our technical contribution in the context of the related literature. Our technical result starts with the value difference lemma in \cite{jin2021pessimism} to connect bounding the suboptimality of an offline algorithm to controlling the uncertainty quantification in the value estimates. Thus, our key technical contribution is to provably quantify the uncertainty of the perturbed value function estimates which were obtained via reward perturbing and gradient descent. This problem setting is largely different from the current analysis of overparameterized neural networks for supervised learning which does not require uncertainty quantification. 

Our work is not the first to consider uncertainty quantification with overparameterized neural networks, since it has been studied in \cite{zhou2020neural,nguyen2021offline,jia2022learning}. However, there are significant technical differences between our work and these works. The work in \cite{zhou2020neural,nguyen2021offline} considers contextual bandits with overparameterized neural networks trained by (S)GD and quantifies the uncertainty of the value function with explicit empirical covariance matrices. We consider general MDP and use reward perturbing to implicitly obtain uncertainty, thus requiring different proof techniques.

\citet{jia2022learning} is more related to our work since they consider reward perturbing with overparameterized neural networks (but they consider contextual bandits). However, our reward perturbing strategy is largely different from that in \cite{jia2022learning}. Specifically, \citet{jia2022learning} perturbs each reward only once while we perturb each reward multiple times, where the number of perturbing times is crucial in our work and needs to be controlled carefully. We show in Theorem \ref{theorem: main theorem} that our reward perturbing strategy is effective in enforcing sufficient pessimism for offline learning in general MDP and the empirical results in Figure \ref{fig: linear mdp}, Figure \ref{fig: neural contextuable bandits}, Figure \ref{fig: subopt vs M}, and Table \ref{tab: d4rl result} are strongly consistent with our theoretical suggestion. Thus, our technical proofs are largely different from those of \citet{jia2022learning}. 

Finally, the idea of perturbing rewards multiple times in our algorithm is inspired by \cite{ishfaq2021randomized}. However, \citet{ishfaq2021randomized} consider reward perturbing for obtaining optimism in online RL. While perturbing rewards are intuitive to obtain optimism for online RL, for offline RL, under distributional shift, it can be paradoxically difficult to properly obtain pessimism with randomization and ensemble \citep{ghasemipour2022so}, especially with neural function approximation. We show affirmatively in our work that simply taking the minimum of the randomized value functions after perturbing rewards multiple times is sufficient to obtain provable pessimism for offline RL. In addition, \citet{ishfaq2021randomized} do not consider neural network function approximation and optimization. Controlling the uncertainty of randomization (via reward perturbing) under neural networks with extra optimization errors induced by gradient descent sets our technical proof significantly apart from that of \cite{ishfaq2021randomized}. 

Besides all these differences, in this work, we propose an intricately-designed data splitting technique that avoids the uniform convergence argument and could be of independent interest for studying sample-efficient RL with complex function approximation.

\paragraph{Proof Overview.} The key steps for proving Theorem \ref{theorem: main theorem} and Theorem \ref{theorem: explicit bound} are highlighted in Subsection \ref{subsection: proof of theorem 1} and Subsection \ref{susbection: proof of theorem 2}, respectively. Here, we discuss an overview of our proof strategy. The key technical challenge in our proof is to quantify the uncertainty of the perturbed value function estimates. To deal with this, we carefully control both the near-linearity of neural networks in the NTK regime and the estimation error induced by reward perturbing. A key result that we use to control the linear approximation to the value function estimates is Lemma \ref{lemma: linear approximation of neural functions}. The technical challenge in establishing Lemma \ref{lemma: linear approximation of neural functions} is how to carefully control and propagate the optimization error incurred by gradient descent. The complete proof of Lemma \ref{lemma: linear approximation of neural functions} is provided in Section \ref{subsection: proof of linear approximation lemma}. 

The implicit uncertainty quantifier induced by the reward perturbing is established in Lemma \ref{lemma: bound ERM with the Bellman target} and Lemma \ref{Lemma: anti-concentration of tilde Q}, where we carefully design a series of intricate auxiliary loss functions and establish the anti-concentrability of the perturbed value function estimates. This requires a careful design of the variance of the noises injected into the rewards. 

To deal with removing a potentially large covering number when we quantify the implicit uncertainty, we propose our data splitting technique which is validated in the proof of Lemma \ref{lemma: bound ERM with the Bellman target} in Section \ref{subsection: proof of lemma D1}.  Moreover, establishing Lemma \ref{lemma: bound ERM with the Bellman target} in the overparameterization regime induces an additional challenge since a standard analysis would result in a vacuous bound that scales with the overparameterization. We avoid this issue by carefully incorporating the use of the effective dimension in Lemma \ref{lemma: bound ERM with the Bellman target}.  

 






\color{black}

\subsection{Proof of Theorem \ref{theorem: main theorem}}
\label{subsection: proof of theorem 1}
In this subsection, we present the proof of Theorem \ref{theorem: main theorem}. We first decompose the suboptimality  and present the main lemmas to bound the evaluation error and the summation of the implicit confidence terms, respectively. The detailed proof of these lemmas are deferred to Section \ref{section: proof of main lemma about evaluation error}. For proof convenience,  we first provide the key parameters that we use consistently throughout our proofs in Table \ref{tab: key parameters in the proofs}. 


We define the model evaluation error at any  as 

where  is the Bellman operator defined in Section \ref{section: preliminary}, and  and  are the estimated (action-) state value functions returned by Algorithm \ref{algorithm: PERVI}. Using the standard suboptimality decomposition \citep[Lemma~3.1]{jin2021pessimism}, for any ,  




where the third term is non-positive as  is greedy with respect to . Thus, for any , we have 


In the following main lemma, we bound the evaluation error . In the rest of the proof, we consider an additional parameter  and fix any . 
\begin{lemma}
Let 

where , , , , and  are defined in Table \ref{tab: key parameters in the proofs},  is a absolute constant given in Lemma \ref{lemma: NTK approximation error}, and  is an additional parameter.
Let  where  is the cumulative distribution function of the standard normal distribution. With probability at least , for any , we have 

where . 
\label{lemma: bound of the evalulation error - main lemma}
\end{lemma}


Now we can prove Theorem \ref{theorem: main theorem}. 
\begin{proof}[Proof of Theorem \ref{theorem: main theorem}]
Theorem \ref{theorem: main theorem} can directly follow from substituting Lemma \ref{lemma: bound of the evalulation error - main lemma} into Equation (\ref{eq: decompose subopt simplified}). We now only need to simplify the conditions in Equation (\ref{equation: conditions for R and eta final version and condition for psi and sigma}). To satisfy Equation (\ref{equation: conditions for R and eta final version and condition for psi and sigma}), it suffices to set


Combining with Equation \ref{equation: conditions for R and eta final version and condition for psi and sigma}, we have


Note that with the above choice of , we have 
We further set that , we have

Thus, 

for  large enough. Therefore, there exists  that satisfies Equation (\ref{eq: final parameter conditions simplified}). We now only need to verify . We have 


if 

Note that 

Thus, Equation (\ref{eq: constraint iota 1}) is satisfied if 

Finally note that . Rearranging the derived conditions here gives the complete parameter conditions in Theorem \ref{theorem: main theorem}. Specifically, the polynomial form of  is 
, , .







\end{proof}

\subsection{Proof of Theorem \ref{theorem: explicit bound}}
\label{susbection: proof of theorem 2}
In this subsection, we give a detailed proof of Theorem \ref{theorem: explicit bound}. We first present intermediate lemmas whose proofs are deferred to Section \ref{section: proof of main lemma about evaluation error}. For any  and , we define the filtration 

Let





In the following lemma, we connect the expected sub-optimality of  to the summation of the uncertainty quantifier at empirical data. 
\begin{lemma}
Suppose that the conditions in Theorem \ref{theorem: main theorem} all hold. With probability at least ,

\label{lemma: expected subopt to empirical confidence quantifiers}
\end{lemma}

\begin{lemma}
Under Assumption \ref{assumption: OPC}, for any  and fixed , with probability at least ,

\label{lemma:sum_sample_subopt}
\end{lemma}

\begin{lemma}
If  and , then with probability at least , for any , we have 

where  is the effective dimension defined in Definition \ref{definition: effective dimension}. 
\label{lemma: bound the summation of the uncertainty quantifier}
\end{lemma}




\begin{proof}[Proof of Theorem \ref{theorem: explicit bound}]
Theorem \ref{theorem: explicit bound} directly follows from Lemma \ref{lemma: expected subopt to empirical confidence quantifiers}-\ref{lemma:sum_sample_subopt}-\ref{lemma: bound the summation of the uncertainty quantifier} using the union bound. 
\end{proof}
 \section{Proof of Lemma \ref{lemma: bound of the evalulation error - main lemma}}
\label{section: proof of main lemma about evaluation error}

In this section, we provide the proof for Lemma \ref{lemma: bound of the evalulation error - main lemma}. We set up preparation for all the results in the rest of the paper and provide intermediate lemmas that we use to prove Lemma \ref{lemma: bound of the evalulation error - main lemma}. The detailed proofs of these intermediate lemmas are deferred to Section \ref{section: proofs of intermediate lemmas for the main lemmas}. 

\subsection{Preparation}
To prepare for the lemmas and proofs in the rest of the paper, we define the following quantities. Recall that we use abbreviation  and . For any  and , we define the perturbed loss function

where

 is computed by Algorithm \ref{algorithm: PERVI} at Line \ref{PERVI: greedy policy} for timestep , and  and  are the Gaussian noises obtained at Line \ref{PERVI: sample noises} of Algorithm \ref{algorithm: PERVI}. 

Here the subscript  and the superscript  in  emphasize the dependence on the ensemble sample  and timestep . The gradient descent update rule of  is

where  is the initialization parameters. Note that 

where  is returned by Line \ref{PERVI: GD} of Algorithm \ref{algorithm: PERVI}. We consider a non-perturbed auxiliary loss function 

where 

Note that  is simply a non-perturbed version of  where we drop all the noises  and . We consider the gradient update rule for  as follows

where  is the initialization parameters. To correspond with , we denote 


We also define the auxiliary loss functions for both non-perturbed and perturbed data in the linear model with feature  as follows




We consider the auxiliary gradient updates for  as 


where  for all . Finally, we  define the least-square solutions to the auxiliary perturbed and non-perturbed loss functions for the linear model as follows  



For any , we define the auxiliary covariance matrix  as follows 


It is worth remarking that Algorithm \ref{algorithm: PERVI} only uses Equation (\ref{eq: perturbed loss function}) and (\ref{eq: GD update in non-linear case}) thus it does not actually require any of the auxiliary quantities defined in this subsection during its run time. The auxiliary quantities here are only for our theoretical analysis. 

\subsection{Proof of Lemma \ref{lemma: bound of the evalulation error - main lemma}}
In this subsection, we give detailed proof of Lemma \ref{lemma: bound of the evalulation error - main lemma}. To prepare for proving Lemma \ref{lemma: bound of the evalulation error - main lemma}, we first provide the following intermediate lemmas. The detailed proofs of these intermediate lemmas are deferred to Section \ref{section: proofs of intermediate lemmas for the main lemmas}. 


In the following lemma, we bound the uncertainty  in estimating the Bellman operator at the estimated state-value function . 
\begin{lemma}
Let 




 With probability at least , for any , and any ,
 
where  is computed by Algorithm \ref{algorithm: PERVI} for timestep ,  is defined in Equation (\ref{eq: hat W_h}), and
,  and  are defined in Table \ref{tab: key parameters in the proofs}. 
\label{lemma: bound ERM with the Bellman target}
\end{lemma}

In the following lemma, we establish the anti-concentration of . 
\begin{lemma}
Let 

where , ,  and  are defined in Table \ref{tab: key parameters in the proofs}, and  is a constant given in Lemma \ref{lemma: NTK approximation error}. Let  where  is the cumulative distribution function of the standard normal distribution and  is the number of bootstrapped samples in Algorithm \ref{algorithm: PERVI}. Then with probability , for any  and , 
where  is defined in Equation (\ref{eq: hat W_h lin}),  is computed by Line \ref{PERVI: minimum of ensemble} of Algorithm \ref{algorithm: PERVI}, and  and  are defined in Table \ref{tab: key parameters in the proofs}. 
\label{Lemma: anti-concentration of tilde Q}
\end{lemma}


We prove the following linear approximation error lemma. 

\begin{lemma}
Let 

where , ,  and  are defined in Table \ref{tab: key parameters in the proofs}, and  is a constant given in Lemma \ref{lemma: NTK approximation error}. 
With probability at least , for any , 

where , , and  are defined in Equation (\ref{eq: GD update in non-linear case}), Equation (\ref{eq: tilde W i lin h}), and Table \ref{tab: key parameters in the proofs}, respectively. 

In addition, with probability at least , for any for any ,

where , , and  are defined in Equation (\ref{eq: GD update in non-linear case for non-perturbed loss}), Equation (\ref{eq: hat W_h lin}), and Table \ref{tab: key parameters in the proofs}, respectively. 
\label{lemma: linear approximation of neural functions}
\end{lemma}

We now can prove Lemma \ref{lemma: bound of the evalulation error - main lemma}. 
\begin{proof}[Proof of Lemma \ref{lemma: bound of the evalulation error - main lemma}]
Note that the first fourth conditions in Equation (\ref{equation: conditions for R and eta final version and condition for psi and sigma}) of Lemma \ref{lemma: bound of the evalulation error - main lemma} satisfy Equation (\ref{equation: conditions for R and eta final version}). Moreover, the event in which the inequality in Lemma \ref{lemma: linear approximation of neural functions} holds already implies the event in which the inequality in Lemma \ref{lemma: bound ERM with the Bellman target} holds (see the proofs of Lemma \ref{lemma: linear approximation of neural functions} and Lemma \ref{lemma: bound ERM with the Bellman target} in Section \ref{section: proof of main lemma about evaluation error}). Now in the rest of the proof, we consider the joint event in which both the inequality of Lemma \ref{lemma: linear approximation of neural functions} and that of Lemma \ref{lemma: bound ERM with the Bellman target} hold. Then, we also have the inequality in Lemma \ref{lemma: bound ERM with the Bellman target}. Consider any . 



It follows from Lemma \ref{lemma: bound ERM with the Bellman target} that


It follows from Lemma \ref{Lemma: anti-concentration of tilde Q} that 


Note that . If , Equation (\ref{eq: bound tilde Q with max 0 and estimate}) implies that  and thus 


Otherwise, if , 
Equation (\ref{eq: bound tilde Q with max 0 and estimate}) implies that

Thus, combining Equation (\ref{eq: lower bound bellman op}), (\ref{eq: anti-concentration of Q tilde to linear})
and Lemma \ref{lemma: linear approximation of neural functions}, with the choice , we have


As , in either case, we have 


Note that due to Equation (\ref{equation: estimated Q upper bounded by Bellman operator}), we have

where the last inequality holds due to the choice . Thus, we have 


Substituting Equation (\ref{eq: simplify the bellman target by removing the max value}) into the definition of , we have

where the first inequality holds due to Equation (\ref{eq: simplify the bellman target by removing the max value}), the second inequality holds due to Lemma \ref{lemma: bound ERM with the Bellman target}, the third inequality holds due to Lemma \ref{lemma: linear approximation of neural functions}, and the last inequality holds due to Lemma \ref{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian} and Lemma \ref{lemma: Concentration of multivariate Gaussian} via the union bound.

\end{proof}

\subsection{Proof of Lemma \ref{lemma: expected subopt to empirical confidence quantifiers}}
\begin{proof}[Proof of Lemma \ref{lemma: expected subopt to empirical confidence quantifiers}]
Let  where  is the indicator function. Under the event in which the inequality in Theorem \ref{theorem: main theorem} holds, we have 

where the first inequality holds due to Theorem \ref{theorem: main theorem} and that , the second inequality holds due to , the third inequality holds due to that , the fourth inequality holds due to Jensen's inequality for the convex function . It follows from Lemma \ref{lemma:improved_online_to_batch} that with probability at least , 

Substituting Equation (\ref{eq: apply improved online to batch to the uncertainty quantifier}) into Equation (\ref{eq: bound subopt with the empirical summation}) and using the union bound complete the proof.

\end{proof}


\subsection{Proof of Lemma \ref{lemma:sum_sample_subopt}}
\begin{proof}[Proof of Lemma \ref{lemma:sum_sample_subopt}]
Let . We have  is -measurable, and by Assumption \ref{assumption: OPC}, we have, 

Thus, by Lemma \ref{lemma:azuma}, for any , with probability at least , we have: 

\end{proof}

\subsection{Proof of Lemma \ref{lemma: bound the summation of the uncertainty quantifier}}

\begin{proof}[Proof of Lemma \ref{lemma: bound the summation of the uncertainty quantifier}]
For any fixed , let  

By the union bound, with probability at least , for any , we have 

where the first inequality holds due to  and \cite[Lemma~11]{NIPS2011_e1d5be1c}, the third equality holds due to that , the second inequality holds due to that  as the result of the convexity of , the third inequality holds due to that , the fourth inequality holds due to  by the choice of , Lemma \ref{lemma: ntk gram matrix versus grad gram matrix} and the union bound, and the last equality holds due to the definition of . 
\end{proof}

 \section{Proofs of Lemmas in Section \ref{section: proof of main lemma about evaluation error}}
\label{section: proofs of intermediate lemmas for the main lemmas}


\subsection{Proof of Lemma \ref{lemma: bound ERM with the Bellman target}}
\label{subsection: proof of lemma D1}

In this subsection, we give detailed proof of Lemma \ref{lemma: bound ERM with the Bellman target}. For this, we first provide a lemma about the linear approximation of the Bellman operator. In the following lemma, we show that  can be well approximated by the class of linear functions with features  with respect to -norm. 
\begin{lemma}
Under Assumption \ref{assumption: completeness}, with probability at least  over  drawn i.i.d. from , for any , there exist  where  and  such that 


Moreover,  can be re-written as 

\label{lemma: linear approximation for the Bellman target}
\end{lemma}

We now can prove Lemma \ref{lemma: bound ERM with the Bellman target}. 
\begin{proof}[Proof of Lemma \ref{lemma: bound ERM with the Bellman target}]

We first bound the difference :


For bounding , it follows from Lemma \ref{lemma: linear approximation for the Bellman target} that with probability at least , for any for any  and any , 

where  is defined in Table \ref{tab: key parameters in the proofs}.
where  is defined in Lemma \ref{lemma: linear approximation for the Bellman target}. 
Thus, with probability at least , for any for any  and any , 

where the first equation holds due to the definition of , and the last inequality holds due to Step I with . 

For bounding , we have



If we directly apply the result of \cite{jin2021pessimism} in linear MDP, we would get

which gives a vacuous bound as  is sufficiently larger than  in our problem. Instead, in the following, we present an alternate proof that avoids such vacuous bound. 

For notational simplicity, we write 

We denote  as the Gram matrix of the empirical NTK kernel on the data . We denote 

Recall the definition of the Gram matrix  of the NTK kernel on the data . It follows from Lemma \ref{lemma: ntk gram matrix versus grad gram matrix} and the union bound that if  with probability at least , for any ,




We now can bound . We have 



We bound each  and  separately. For bounding , applying Lemma \ref{lemma: NTK approximation error}, with , for any , 

where the first inequality holds due to the triangle inequality, the second inequality holds due to the triangle inequality, Lemma \ref{lemma: difference of two inverse matrices}, and , the third inequality holds due to  due to Lemma \ref{lemma: NTK approximation error}, the fourth inequality holds due to , , and . 

Substituting Equation (\ref{eq: cov ntk - cov init}) in Equation (\ref{eq: bound approx error of self-normalized process}) using the union bound, with probability , for any , 

where the last inequality holds due to the choice of  and thus 




For bounding , as , we have 

Let  be the -algebra induced by the set of random variables. For any  and , we define the filtration 

which is simply all the data up to episode  and timestep  but right before  and  are generated (in the offline data). \footnote{To be more precise, we need to include into the filtration the randomness from the generated noises  and  but since these noises are independent of any other randomness, they do not affect any derivations here but only complicate the notations and representations.} Note that for any , we have , and

Thus, for any , we have 

The key property in our data split design is that we nicely have that

Thus, conditioned on ,  becomes deterministic. This implies that 

Note that this is only possible with our data splitting technique. Otherwise,  is not zero-mean due to the data dependence structure induced in offline RL with function approximation \citep{nguyentang2021sample}. Our data split technique is a key to avoid the uniform convergence argument with the log covering number that is often used to bound this term in \cite{jin2021pessimism}, which is often large for complex models. For example, in a two-layer ReLU NTK, the eigenvalues of the induced RKHS has -polynomial decay \citep{bietti2019inductive}, thus its log covering number roughly follows, by \citep[Lemma~D1]{yang2020function},

for some . 

Therefore, for any ,  is adapted to the filtration . Applying Lemma \ref{lemma: self-normalized concentration process in RKHS} with , , , for any , with probability at least , for any , 


Substituting Equation (\ref{eq: apply self-normalized concentration of RKHS}) into Equation (\ref{eq: transform the form of the self-normalized process}), we have 

where the last equation holds due to the definition of the effective dimension. 



Combining Equations (\ref{eq: bound of self-normalized process with NTK kernel}), (\ref{eq: bound I4 in self-normalized process proof}), (\ref{eq: I3 in terms of I4 and I5}), (\ref{eq: bound I2 in terms of I3}), and (\ref{eq: bound I1}) via the union bound, with probability at least , for any  and any ,

where 

Combing with Lemma \ref{lemma: linear approximation of neural functions} using the union bound, with probability at least , for any , and any ,

where , and  are defined in Table \ref{tab: key parameters in the proofs}.


Similarly, it is easy to show that  


\end{proof} 

\subsection{Proof of Lemma \ref{Lemma: anti-concentration of tilde Q}}

Before proving Lemma \ref{Lemma: anti-concentration of tilde Q}, we prove the following intermediate lemmas. The detailed proofs of these intermediate lemmas are deferred to Section \ref{section: proof of intermediate lemmas of intermediate lemmas}. 

\begin{lemma}
Conditioned on all the randomness except  and , for any ,

\label{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian}
\end{lemma}



\begin{lemma}
If we set  where  is the cumulative distribution function of the standard normal distribution, then with probability at least , for any ,  

\label{lemma: anti-concentration for proxy linear MDP}
\end{lemma}


We are now ready to prove Lemma \ref{Lemma: anti-concentration of tilde Q}. 
\begin{proof}[Proof of Lemma \ref{Lemma: anti-concentration of tilde Q}]
Note that the parameter condition in Equation (\ref{equation: conditions for R and eta final version repeated}) of Lemma \ref{Lemma: anti-concentration of tilde Q} satisfies Equation (\ref{equation: conditions for R and eta final version}) of Lemma \ref{lemma: linear approximation of neural functions}, thus given the parameter condition Lemma \ref{Lemma: anti-concentration of tilde Q}, Lemma \ref{lemma: linear approximation of neural functions} holds. For the rest of the proof, we consider under the joint event in which both the inequality of Lemma \ref{lemma: linear approximation of neural functions} and that of Lemma \ref{lemma: anti-concentration for proxy linear MDP} hold. By the union bound, probability that this joint event holds is at least . Thus, for any , , and , 


where the first inequality holds due to Lemma \ref{lemma: linear approximation of neural functions}, and the second inequality holds due to Lemma \ref{lemma: anti-concentration for proxy linear MDP}. Thus, we have 




\end{proof}

\subsection{Proof of Lemma \ref{lemma: linear approximation of neural functions}}
\label{subsection: proof of linear approximation lemma}
In this subsection, we provide a detailed proof of Lemma \ref{lemma: linear approximation of neural functions}. We first provide intermediate lemmas that we use for proving Lemma \ref{lemma: linear approximation of neural functions}. The detailed proofs of these intermediate lemmas are deferred to Section \ref{section: proof of intermediate lemmas of intermediate lemmas}. 







The following lemma bounds the the gradient descent weight of the perturbed loss function around the linear weight counterpart. 



\begin{lemma}
Let 

where , ,  and  are defined in Table \ref{tab: key parameters in the proofs} and  is a constant given in Lemma \ref{lemma: NTK approximation error}. 
With probability at least , for any , we have 
\begin{itemize}
    \item , 
    \item 
\end{itemize}
\label{lemma: improve bounding GD weight and that of linear auxilary}
\end{lemma}

Similar to Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}, we obtain the following lemma for the gradient descent weights of the non-perturbed loss function. 

\begin{lemma}
Let 

where  , ,  and  are defined in Table \ref{tab: key parameters in the proofs} and  is a constant given in Lemma \ref{lemma: NTK approximation error}. 
With probability at least , for any , we have 
\begin{itemize}
    \item , 
    \item 
\end{itemize}
\label{lemma: improved bounding GD weight and that of linear auxilary for non-perturbed loss}
\end{lemma}







We now can prove Lemma \ref{lemma: linear approximation of neural functions}.
\begin{proof}[Proof of Lemma \ref{lemma: linear approximation of neural functions}]
Note that Equation (\ref{equation: conditions for R and eta final version}) implies both Equation (\ref{equation: improved conditions for R and eta comprehensive list}) of Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary} and Equation (\ref{equation: improved conditions for R and eta comprehensive list for non-perturbed loss}) of Lemma \ref{lemma: improved bounding GD weight and that of linear auxilary for non-perturbed loss}, thus both  Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary} and Lemma \ref{lemma: improved bounding GD weight and that of linear auxilary for non-perturbed loss} holds under Equation (\ref{equation: conditions for R and eta final version}). Thus, by the union bound, with probability at least , for any , and ,

where the first inequality holds due to the triangle inequality, the second inequality holds due to Cauchy-Schwarz inequality, Lemma \ref{lemma: NTK approximation error}, and Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}. 

Similarly, by the union bound, with probability at least , for any , and ,

where the first inequality holds due to the triangle inequality, the second inequality holds due to Cauchy-Schwarz inequality, Lemma \ref{lemma: improved bounding GD weight and that of linear auxilary for non-perturbed loss}, and Lemma \ref{lemma: NTK approximation error}. 
\end{proof}




 \section{Proofs of Lemmas in Section \ref{section: proofs of intermediate lemmas for the main lemmas}}
\label{section: proof of intermediate lemmas of intermediate lemmas}
In this section, we provide the detailed proofs of Lemmas in Section \ref{section: proofs of intermediate lemmas for the main lemmas}. 

\subsection{Proof of Lemma \ref{lemma: linear approximation for the Bellman target}}
\begin{proof}[Proof of Lemma \ref{lemma: linear approximation for the Bellman target}]
As  by Assumption \ref{assumption: completeness}, where  is defined in Section \ref{section: subopt analysis},
we have 

for some  such that . The lemma then directly follows from approximation by finite sum \citep{gao2019convergence}. 
\end{proof}

\subsection{Proof of Lemma \ref{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian}}

\begin{proof}[Proof of Lemma \ref{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian}]
Let  and 

where  . 
We have 
and  as both  and  are convex. Using the regularized least-squares solution,  

Thus, we have

By direct computation, it is easy to see that 


\end{proof}

\subsection{Proof of Lemma \ref{lemma: anti-concentration for proxy linear MDP}}

In this subsection, we provide a proof for \ref{lemma: anti-concentration for proxy linear MDP}. We first provide a bound for the perturbed noises used in Algorithm \ref{algorithm: PERVI} in the following lemma.
\begin{lemma}
There exist absolute constants  such that for any , event  holds with probability at least , for any , 

\label{Lemma: good events where the noises are bounded}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{Lemma: good events where the noises are bounded}]
It directly follows from the Gaussian concentration inequality in Lemma \ref{lemma: Concentration of multivariate Gaussian} and the union bound.
\end{proof}





We now can prove Lemma \ref{lemma: anti-concentration for proxy linear MDP}. 
\begin{proof}[Proof of Lemma \ref{lemma: anti-concentration for proxy linear MDP}]
By Lemma \ref{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian}, 

Using the anti-concentration of Gaussian distribution, for any  and any , 

As  are independent, using the union bound, with probability at least , for any , and ,

Setting  completes the proof.
\end{proof}





\subsection{Proof of Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}}
In this subsection, we provide a detailed proof of Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}. We first prove the following intermediate lemma whose proof is deferred to Subsection \ref{subsection: proof of lemma f_j - y}. 

\begin{lemma}
Let 

 and additionally let 

Then with probability at least , for any , if  for any , then 

\label{lemmaq: improved bound f_j - y}
\end{lemma}


We now can prove Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}. 
\begin{proof}[Proof of Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}]
To simplify the notations, we define 

The gradient descent update rule for  in Equation (\ref{eq: GD update in non-linear case}) can be written as: 


The auxiliary updates in Equation (\ref{eq: GD update in linear case}) can be written as:


\paragraph{Step 1: Proving  for all .} In the first step, we prove by induction that with probability at least , for any , we have 


In the rest of the proof, we consider under the event that Lemma \ref{lemmaq: improved bound f_j - y} holds. Note that the condition in Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary} satisfies that of Lemma \ref{lemmaq: improved bound f_j - y} and under the above event of Lemma \ref{lemmaq: improved bound f_j - y}, Lemma \ref{lemma: NTK approximation error} and Lemma \ref{Lemma: good events where the noises are bounded} both hold. It is trivial that 

For any fix , we assume that


We will prove that . We have

We bound ,  and  separately. 




\paragraph{Bounding .} For bounding ,

where the first inequality holds due to the spectral norm inequality, the second inequality holds due to 

where the first inequality holds due to that , the second inequality holds due to that  due to Lemma \ref{lemma: NTK approximation error}, and the last inequality holds due to the choice of  in Equation (\ref{equation: improved conditions for R and eta comprehensive list}). 

\paragraph{Bounding .} For bounding ,

where the first inequality holds due to Cauchy-Schwarz inequality, the second inequality holds due to the induction assumption in Equation (\ref{eq: induction step for proving tilde W h i j near W 0}) and Lemma \ref{lemma: NTK approximation error}, and the third inequality holds due to Lemma \ref{lemmaq: improved bound f_j - y} and the induction assumption in Equation (\ref{eq: induction step for proving tilde W h i j near W 0}). 
\paragraph{Bounding .} For bounding ,

where the first inequality holds due to Cauchy-Schwarz inequality and due to that  and the second inequality holds due to the induction assumption in Equation (\ref{eq: induction step for proving tilde W h i j near W 0}) and Lemma \ref{lemma: NTK approximation error}. 

Combining the bounds of  above, we have 

Recursively applying the inequality above for all , we have 

where the second inequality holds due the choice specified in Equation (\ref{equation: improved conditions for R and eta comprehensive list}). We also have 

where the first inequality holds due the the definition of , the second inequality holds due to the monotonicity of  on the gradient descent updates  for the squared loss on a linear model, the third equality holds due to  from the symmetric initialization scheme, and the last inequality holds due to Lemma \ref{Lemma: good events where the noises are bounded}. Thus, we have

where the first inequality holds due to Cauchy-Schwarz inequality, the second inequality holds due to Equation (\ref{eq: bound tilde W i lin j+1 + zeta i h - W 0}) and Lemma \ref{Lemma: good events where the noises are bounded}, and the last inequality holds due to the choice specified in Equation (\ref{equation: improved conditions for R and eta comprehensive list}). 

Combining Equation (\ref{eq: bound Delta j by R/2}) and Equation (\ref{eq: tilde W i lin j+1 - W0}), we have 

where the first inequality holds due to the triangle inequality. 
\paragraph{Step 2: Bounding .} By the standard result of gradient descent on ridge linear regression,  converges to  with the convergence rate, 

Thus, for any , we have 

where the first inequality holds due to the triangle inequality, the second inequality holds due to Equation (\ref{eq: bound Delta j by R/2}) and Equation (\ref{eq: tilde W i lin j 0 tilde W i lin}). 
\end{proof}



























\subsection{Proof of Lemma \ref{lemmaq: improved bound f_j - y}} 
\label{subsection: proof of lemma f_j - y}
\begin{proof}[Proof of Lemma \ref{lemmaq: improved bound f_j - y}]
We bound this term following the proof flow of \cite[Lemma~C.3]{zhou2020neural} with modifications for different neural parameterization and noisy targets. Suppose that for some fixed , 

Let us define 

To further simplify the notations in this proof, we drop  in  defined in Equation (\ref{eq: perturbed loss function}) to write  as  and write , where

Suppose that . By that  is -smooth, 

For bounding , 

where the first inequality holds due to Cauchy-Schwarz inequality,  and Lemma \ref{lemma: NTK approximation error}. Substituting Equation (\ref{eq: bounding I1 in  L W' - L W}) into Equation (\ref{eq: bounding L W' - L W}) with ,


By the -strong convexity of , for any ,

where the last inequality holds due to Cauchy-Schwarz inequality. 

Substituting Equation (\ref{eq: bound L W' L W by strong convexity}) into Equation (\ref{eq: bounding L W' - L W more explicit}), for any ,

where the second inequality holds due to Cauchy-Schwarz inequality for any , and the third inequality holds due to  . 

Rearranging terms in Equation (\ref{eq: bound L W' - L W final stage}) and setting , , , , 


where , the last inequality holds due to Equation (\ref{eq: induction condition that tilde W i j' in the ball}) and Lemma \ref{lemma: NTK approximation error}. Applying Equation (\ref{eq: recursion of L W_j - L W_0}), we have

Rearranging the above inequality, 

where the last inequality holds due to the choice of . 
Finally, we have

and  due to Lemma \ref{Lemma: good events where the noises are bounded}.
\end{proof}
 \section{Support Lemmas}
\begin{lemma}


Let  and . With probability at least  with respect to the random initialization, it holds for any  and  that

where  is a constant independent of  and . Moreover, without loss of generality, we assume .  
\label{lemma: NTK approximation error}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma: NTK approximation error}]
Due to \cite[Lemma~C.2]{yang2020function} and \cite[Lemma~F.1, F.2]{cai2019neural}, we have the first two inequalities and the following: 

For any , 

Thus, 

\end{proof}

\begin{lemma}[{\cite[Theorem~3]{arora2019exact}}]
\label{lemma: ntk gram matrix versus grad gram matrix}
If , then for any , with probability at least , 

\end{lemma}


\begin{lemma}
Let  be a -dimensional normal variable where  is a scalar. There exists an absolute constant  such that for any , with probability at least , 

For , . 

\label{lemma: Concentration of multivariate Gaussian}
\end{lemma}

\begin{lemma}[A variant of Hoeffding-Azuma inequality]
Suppose  is a real-valued stochastic process with corresponding filtration , i.e. ,  is -measurable. Suppose that for any ,  and  almost surely. Then for all positive  and , we have: 

\label{lemma:azuma}
\end{lemma}

\begin{lemma}[{\citep[Theorem~1]{chowdhury2017kernelized}}]
Let  be an RKHS defined over . Let  be a discrete time stochastic process adapted to filtration . Let  be a real-valued stochastic process such that , and  is zero-mean and -sub Gaussian conditioned on . Let  and  be the Gram matrix of  defined on . For any  and , with probability at least , 

\label{lemma: self-normalized concentration process in RKHS}
\end{lemma}


\begin{lemma}
For any matrices  and  where  is invertible, 

\end{lemma}

\begin{lemma}
For any invertible matrices , 

\label{lemma: difference of two inverse matrices}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma: difference of two inverse matrices}]
We have: 

\end{proof}




\begin{lemma}[Freedman's inequality \citep{tropp2011freedman}]
Let  be a real-valued martingale difference sequence with the corresponding filtration , i.e.  is -measurable and . Suppose for any ,  almost surely and define . For any , we have:

In an alternative form, for any , we have: 

\label{lemma:freedman}
\end{lemma}

\begin{lemma}[Improved online-to-batch argument \cite{nguyen2022instance}]
Let  be any real-valued stochastic process adapted to the filtration , i.e.  is -measurable. Suppose that for any ,  almost surely for some . For any , with probability at least , we have:

\label{lemma:improved_online_to_batch}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:improved_online_to_batch} ]
Let  and . We have  is a real-valued difference martingale with the corresponding filtration  and that

Note that   and  and let .
Also note that . Thus if , we have . For any , leveraging the peeling technique \citep{bartlett2005local}, we have:

where the first equation is by that  thus , the second inequality is by that , and the last inequality is by Lemma \ref{lemma:freedman}. Thus, with probability at least , we have:

The above inequality implies that , due to the simple inequality: if , . Then setting  completes the proof. 
\end{proof}
 \section{Baseline algorithms}
\label{section: baseline algorithms}
For completeness, we give the definition of linear MDPs as follows.
\begin{defn}[Linear MDPs \citep{yang2019sample,jin2020provably}]
An MDP has a linear structure if for any , we have: 

where  is a known feature map,  is an unknown vector, and  are unknown signed measures. 
\label{definition:linear_mdp}
\end{defn}

We also give the details of the baseline algorithms: LinLCB in Algorithm \ref{algorithm: LinLCB}, LinGreedy in Algorithm \ref{algorithm: LinGreedy}, Lin-VIPeR in Algorithm \ref{algorithm: LinPER}, NeuraLCB in Algorithm \ref{algorithm: NeuraLCB} and NeuralGreedy in Algorithm \ref{algorithm: NeuralGreedy}. For simplicity, we do not use data split in these algorithms presented here.  

\begin{algorithm}[h!]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data , uncertainty multiplier , regularization parameter . 
\State Initialize 
\For{}
\State 


\State 

 \State . 
\label{bpvi:lcb}
\State . 
\State  and .





\EndFor
\State \textbf{Output}: 
\end{algorithmic}
\caption{LinLCB \citep{jin2021pessimism}}
\label{algorithm: LinLCB}
\end{algorithm}

\begin{algorithm}[h!]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data , perturbed variances , number of bootstraps , regularization parameter . 
\State Initialize 
\For{}


\State 

\label{bpvi:lcb}
\State . 
\State  and .





\EndFor
\State \textbf{Output}: 
\end{algorithmic}
\caption{LinGreedy}
\label{algorithm: LinGreedy}
\end{algorithm}


\begin{algorithm}[h!]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data , perturbed variances , number of bootstraps , regularization parameter . 
\State Initialize 
\For{}
\State 
\For{}
\State Sample  and 


\State Solve the perturbed regularized least-squares regression: 
\label{RAVI-Lin: perturbed ERM}


\EndFor 

\State Compute 
\State  and 
\EndFor
\State \textbf{Output}: 
\end{algorithmic}
\caption{Lin-VIPeR}
\label{algorithm: LinPER}
\end{algorithm}


\begin{algorithm}[h]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data , neural networks , uncertainty multiplier , regularization parameter , step size , number of gradient descent steps   
\State Initialize  and initialize  with initial parameter 
\For{}


\State  (Algorithm \ref{algo:GD})


\State 



\State Compute 






\State  and 
\EndFor
\State \textbf{Output}: .
\end{algorithmic}
\caption{NeuraLCB (a modification of \citep{nguyen2021offline})}
\label{algorithm: NeuraLCB}
\end{algorithm}


\begin{algorithm}
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data , neural networks , uncertainty multiplier , step size , number of gradient descent steps   
\State Initialize  and initialize  with initial parameter 
\For{}


\State  (Algorithm \ref{algo:GD})






\State Compute 






\State  and 
\EndFor
\State \textbf{Output}: .
\end{algorithmic}
\caption{NeuralGreedy}
\label{algorithm: NeuralGreedy}
\end{algorithm}

 
\end{document}
