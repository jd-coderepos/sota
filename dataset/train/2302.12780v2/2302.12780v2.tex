
\documentclass{article} \usepackage{iclr2023/iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}
\usepackage{amssymb,amsthm,mathtools}
\usepackage{soul}

\allowdisplaybreaks
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\def\err{{\mathrm{err}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}


\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}
\DeclareMathOperator*{\subopt}{SubOpt}
\DeclareMathOperator*{\gap}{gap}
\DeclareMathOperator*{\vect}{vec}
\DeclareMathOperator*{\logdet}{logdet}
\DeclareMathOperator*{\tr}{tr}

\usepackage{wrapfig}
\usepackage{bbm}
\newcommand{\thanh}[1]{\textcolor{blue}{\emph{[\textbf{Thanh}: #1]}}}
\newcommand{\raman}[1]{\textcolor{red}{\emph{[\textbf{Raman}: #1]}}}
\newcommand{\cmt}[1]{\textcolor{blue}{#1}}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{subfigure}


\newtheorem{defn}{Definition}
\newtheorem{conj}{Conjecture}[section]
\newtheorem{example}{Example}[section]
\newtheorem{assumption}{Assumption}[section]

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{prop}{Proposition}
\newtheorem{observation}{Observation}
\newtheorem{corollary}{Corollary}

\newtheorem{remark}{Remark}

\usepackage[linewidth=1pt]{mdframed}

\newenvironment{hint}
{
\begin{mdframed}[backgroundcolor=orange!5, linecolor=white] 
\textbf{Hint}. 
}
{
\end{mdframed}
}
\usepackage{float}
\usepackage{multirow}



%
 

\usepackage{hyperref}
\usepackage{url}

\title{VIPeR: Provably Efficient Algorithm for Offline RL with Neural Function Approximation}







\author{Thanh Nguyen-Tang \\
Department of Computer Science\\
Johns Hopkins University\\
Baltimore, MD 21218, USA \\
\texttt{nguyent@cs.jhu.edu} \\
\And
Raman Arora \\
Department of Computer Science\\
Johns Hopkins University\\
Baltimore, MD 21218, USA \\
\texttt{arora@cs.jhu.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
We propose a novel algorithm for offline reinforcement learning called Value Iteration with Perturbed Rewards (VIPeR), which amalgamates the pessimism principle with random perturbations of the value function. Most current offline RL algorithms explicitly construct statistical confidence regions to obtain pessimism via lower confidence bounds (LCB), which cannot easily scale to complex problems where a neural network is used to estimate the value functions. Instead, VIPeR implicitly obtains pessimism by simply perturbing the offline data multiple times with carefully-designed i.i.d. Gaussian noises to learn an ensemble of estimated state-action {value functions} and acting greedily with respect to the minimum of the ensemble. The estimated state-action values are obtained by fitting a parametric model (e.g., neural networks) to the perturbed datasets using gradient descent. As a result, VIPeR only needs $\mathcal{O}(1)$ time complexity for action selection, while LCB-based algorithms require at least $\Omega(K^2)$, where $K$ is the total number of trajectories in the offline data. We also propose a novel data-splitting technique that helps remove a factor involving the log of the covering number in our bound. We prove that VIPeR yields a provable uncertainty quantifier with overparameterized neural networks and enjoys a bound on sub-optimality of $\tilde{\mathcal{O}}(  { \kappa H^{5/2}  \tilde{d} }/{\sqrt{K}})$, where $\tilde{d}$ is the effective dimension, $H$ is the horizon length and $\kappa$ measures the distributional shift. We corroborate the statistical and computational efficiency of VIPeR with an empirical evaluation on a wide set of synthetic and real-world datasets. To the best of our knowledge, VIPeR is the first algorithm for offline RL that is provably efficient for general Markov decision processes (MDPs) with neural network function approximation. 



%
 \end{abstract}

\section{Introduction}
Offline reinforcement learning (offline RL)~\citep{lange2012batch,levine2020offline} is a practical paradigm of RL for domains where active exploration is not permissible. Instead, the learner can access a fixed dataset of previous experiences available a priori. Offline RL finds applications in several critical domains where exploration is prohibitively expensive or even implausible, including healthcare~\citep{gottesman2019guidelines, nie2021learning}, recommendation systems~\citep{strehl2010learning, thomasAAAI17}, and  econometrics~\citep{Kitagawa18, athey2021policy}, among others. The recent surge of interest in this area and renewed research efforts have yielded several important empirical successes~\citep{chen2021decision, wang2022diffusion, wang2022bootstrapped, meng2021offline}. 



A key challenge in offline RL is to efficiently exploit the given offline dataset to learn an optimal policy in the absence of any further exploration. The dominant approaches to offline RL address this challenge by incorporating uncertainty from the offline dataset into  decision-making~\citep{buckman2020importance, jin2021pessimism, xiao2021optimality, nguyen2021offline, ghasemipour2022so, an2021uncertainty, bai2022pessimistic}. The main component of these uncertainty-aware approaches to offline RL is the \emph{pessimism} principle, which constrains the learned policy to the offline data and leads to various lower confidence bound (LCB)-based algorithms. However, these methods are not easily extended or scaled to complex problems where neural function approximation is used to estimate the value functions. In particular, it is costly to explicitly compute the statistical confidence regions of the model or value functions if the class of function approximator is given by overparameterized neural networks. For example, constructing the LCB for neural offline contextual bandits~\citep{nguyen2021offline} and RL \citep{xu2022provably} requires computing the inverse of a large covariance matrix whose size scales with the number of parameters in the neural network. This computational cost hinders the practical application of these provably efficient offline RL algorithms. Therefore, a largely open question is \emph{how to design provably  computationally efficient algorithms for offline RL with neural network function approximation.} 



In this work, we present a solution based on a computational approach that combines the pessimism principle with randomizing the value function~\citep{osband2016generalization, ishfaq2021randomized}. The algorithm is strikingly simple: we randomly perturb the offline rewards several times and act greedily with respect to the minimum of the estimated state-action values. The intuition is that taking the minimum from an ensemble of randomized state-action values can efficiently achieve pessimism with high probability while avoiding explicit computation of statistical confidence regions. We learn the state-action value function by training a neural network using gradient descent (GD). Further, we consider a novel data-splitting technique that helps remove the dependence on the potentially large log covering number in the learning bound. We show that the proposed algorithm yields a provable uncertainty quantifier with overparameterized neural network function approximation and achieves a sub-optimality bound of  $\tilde{\mathcal{O}}(  \kappa H^{5/2}  \tilde{d}/{\sqrt{K}} )$, where $K$ is the total number of episodes in the offline data, $\tilde{d}$ is the effective dimension, $H$ is the horizon length, and $\kappa$ measures the distributional shift. We achieve computational efficiency since the proposed algorithm only needs $\mathcal{O}(1)$ time complexity for action selection, while LCB-based algorithms require  $\mathcal{O}(K^2)$ time complexity. We empirically corroborate the statistical and computational efficiency of our proposed algorithm on a wide set of synthetic and real-world datasets. The experimental results show that the proposed algorithm has a strong advantage in computational efficiency while outperforming LCB-based neural algorithms. To the best of our knowledge, ours is the first offline RL algorithm that is both provably and computationally efficient in general MDPs with neural network function approximation. 


















%
 \section{Related Work}


\paragraph{Randomized value functions for RL.} 
For online RL, \citet{osband2016generalization,osband2019deep} were the first to explore randomization of estimates of the value function for exploration. Their approach was inspired by posterior sampling for RL~\citep{osband2013more}, which samples a value function from a posterior distribution and acts greedily with respect to the sampled function. Concretely, \citet{osband2016generalization,osband2019deep} generate randomized value functions by injecting Gaussian noise into the training data and fitting a model on the perturbed data. \citet{jia2022learning} extended the idea of perturbing rewards to online contextual bandits with neural function approximation. \citet{ishfaq2021randomized} obtained a provably efficient method for online RL with general function approximation using the perturbed rewards. 
While randomizing the value function is an intuitive approach to obtaining optimism in online RL, obtaining pessimism from the randomized value functions can be tricky in offline RL. Indeed, \citet{ghasemipour2022so} point out a critical flaw in several popular existing methods for offline RL that update an ensemble of randomized Q-networks toward a \emph{shared} pessimistic temporal difference target. In this paper, we propose a simple fix to obtain pessimism properly by updating each randomized value function independently and taking the minimum over an ensemble of randomized value functions to form a pessimistic value function.








\paragraph{Offline RL with function approximation.} 
Provably efficient offline RL has been studied extensively for linear function approximation. \citet{jin2021pessimism} were the first to show that pessimistic value iteration is provably efficient for offline linear MDPs. \citet{Xiong2022NearlyMO,yinnear} improved upon \citet{jin2021pessimism} by leveraging variance reduction. \citet{xie2021bellman} proposed a Bellman-consistency assumption with general function approximation, which improves the bound of \citet{jin2021pessimism} by a factor of $\sqrt{d}$ when realized to finite action space and linear MDPs. \citet{wang2020statistical,zanette2021exponential} studied the statistical hardness of offline RL with linear function approximation via exponential lower bound, and \citet{foster2021offline} suggested that only realizability and strong uniform data coverage are not sufficient for sample-efficient offline RL. Beyond linearity, some works study offline RL for general function approximation, both parametric and nonparametric. These approaches are either based on Fitted-Q Iteration (FQI)~\citep{DBLP:journals/jmlr/MunosS08, DBLP:conf/icml/0002VY19, chen2019information, duan2021risk, duan2021optimal, hu2021fast, nguyentang2021sample} or the pessimism principle~\citep{uehara2021pessimistic,nguyen2021offline,jin2021pessimism}. While pessimism-based algorithms avoid the strong assumptions of data coverage used by FQI-based algorithms, they require an explicit computation of valid confidence regions and possibly the inverse of a large covariance matrix which is computationally prohibitive and does not scale to complex function approximation setting. This limits the applicability of pessimism-based, provably efficient offline RL to practical settings. A very recent work \cite{bai2022pessimistic} estimates the uncertainty for constructing LCB via the disagreement of bootstrapped Q-functions. However, the uncertainty quantifier is only guaranteed in linear MDPs and must be computed explicitly. 

We provide a more detailed discussion of our technical contribution in the context of existing literature in Section \ref{subsection: technical review}.










%
 \section{Preliminaries}
\label{section: preliminary}
In this section, we provide basic background on offline RL and overparameterized neural networks. 

\subsection{Episodic time-inhomogenous Markov decision processes (MDPs)}
A finite-horizon Markov decision process (MDP) is denoted as the tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathbb{P},r, H, d_1)$, where $\mathcal{S}$ is an arbitrary state space, $\mathcal{A}$ an arbitrary action space, $H$ the episode length, and $d_1$ the initial state distribution. We assume that $S A := | \mathcal{S}| |\mathcal{A} |$ is finite but arbitrarily large, e.g.,  it can be as large as the total number of atoms in the observable universe $\approx 10^{82}$. Let $\mathcal{P}(\mathcal{S})$ denote the set of probability measures over $\mathcal{S}$. A time-inhomogeneous transition kernel $\mathbb{P} = \{\mathbb{P}_h\}_{h=1}^H$, where $\mathbb{P}_h: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(\mathcal{S})$ maps each state-action pair $(s_h, a_h)$ to a probability distribution $\mathbb{P}_h(\cdot|s_h, a_h)$. Let $r = \{r_h\}_{h=1}^H$ where $r_h: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$ is the mean reward function at step $h$. A policy $\pi = \{\pi_h\}_{h=1}^H$ assigns each state $s_h \in \mathcal{S}$ to a probability distribution, $\pi_h(\cdot|s_h)$, over the action space  and induces a random trajectory $s_1, a_1, r_1, \ldots, s_H, a_H, r_H, s_{H+1}$ where $s_1 \sim d_1$, $a_h \sim \pi_h(\cdot|s_h)$, $s_{h+1} \sim \mathbb{P}_h(\cdot | s_h, a_h)$. We define the state value function $V^{\pi}_h \in \mathbb{R}^{\mathcal{S}}$ and the action-state value function $Q^{\pi}_h \in \mathbb{R}^{\mathcal{S} \times \mathcal{A}}$ at each timestep $h$ as 
$Q^{\pi}_h(s,a) = \mathbb{E}_{\pi} [\sum_{t=h}^H r_t | s_h=s, a_h=a ]$, and $
    V^{\pi}_h(s) = \mathbb{E}_{a \sim \pi(\cdot |s)} \left[Q^{\pi}_h(s,a) \right]$,
where the expectation $\mathbb{E}_{\pi}$ is taken with respect to the randomness of the trajectory induced by $\pi$. Let $\mathbb{P}_h$ denote the transition operator defined as $(\mathbb{P}_h V)(s,a) := \mathbb{E}_{s' \sim \mathbb{P}_h(\cdot| s,a)} [V(s')]$. For any $V: \mathcal{S} \rightarrow \mathbb{R}$, we define the Bellman operator at timestep $h$ as $(\mathbb{B}_h V)(s,a) := r_h(s,a) + (\mathbb{P}_h V)(s,a)$. The Bellman equations are given as follows. For any $(s,a,h) \in \mathcal{S} \times \mathcal{A} \times [H]$,
\begin{align*}
    Q^{\pi}_h(s,a) = (\mathbb{B}_h V^{\pi}_{h+1})(s,a), \hspace{5pt} V^{\pi}_h(s) = \langle Q^{\pi}_h(s, \cdot), \pi_h(\cdot|s) \rangle_{\mathcal{A}}, \hspace{5pt} V^{\pi}_{H+1}(s) = 0,
\end{align*}
where $[H] := \{1, 2, \ldots, H\}$, and $\langle \cdot, \cdot \rangle_{\mathcal{A}}$ denotes the summation over all $a \in \mathcal{A}$. 
We define an optimal policy $\pi^*$ as any policy that yields the optimal value function, i.e. $V^{\pi^*}_h(s) = \sup_{\pi} V_h^{\pi}(s)$ for any $(s,h) \in \mathcal{S} \times [H]$. For simplicity, we denote $V^{\pi^*}_h$ and $Q^{\pi^*}_h$ as $V^*_h$ and $Q^*_h$, respectively. The Bellman optimality equation can be written as
\begin{align*}
    Q^*_h(s,a) = (\mathbb{B}_h V^*_{h+1})(s,a), \hspace{5pt} V^*_h(s) = \max_{a \in \mathcal{A}} Q^*_h(s,a), \hspace{5pt} V^*_{H+1}(s) = 0.
\end{align*}
Define the occupancy density as  $d^{\pi}_h(s,a) := \mathbb{P}((s_h,a_h) = (s,a) | \pi)$ which is the probability that we visit state $s$ and take action $a$ at timestep $h$ if we follow the policy $\pi.$  
We denote $d^{\pi^*}_h$ by $d^*_h$. 

\paragraph{Offline regime.} In the offline regime, the learner has access to a fixed dataset $\mathcal{D} = \{(s^t_h, a^t_h, r^t_h, s^t_{h+1})\}^{t \in [K]}_{h \in [H]}$ generated a priori by some unknown behaviour policy $\mu = \{\mu_h\}_{h \in [H]}$. Here, $K$ is the total number of trajectories, and $a^t_h \sim \mu_h(\cdot|s^t_h), s^t_{h+1} \sim \mathbb{P}_h(\cdot| s^t_h, a^t_h)$ for any $(t,h) \in [K] \times [H]$. Note that we allow the trajectory at any time $t \in [K]$ to depend on the trajectories at previous times. The goal of offline RL is to learn a policy $\hat{\pi}$, based on (historical data) $\mathcal{D}$,
such that $\hat{\pi}$ achieves small sub-optimality, which we define as
\begin{align*}
    \subopt( \hat{\pi} ) := \mathbb{E}_{s_1 \sim d_1} \left[ \subopt(\hat{\pi}; s_1) \right], \text{ where }
    \subopt(\hat{\pi}; s_1) := V^{\pi^*}_1(s_1) - V^{\hat{\pi}}_1(s_1).
\end{align*}



\paragraph{Notation.} For simplicity, we write $x^t_h = (s^t_h, a^t_h)$ and $x = (s,a)$. We write $\tilde{\mathcal{O}}(\cdot)$ to hide logarithmic factors of the problem parameters $(d,H,K,m, 1/\delta)$ in the standard Big-Oh notation. We use $\Omega(\cdot)$ as the standard Omega notation. We write $u \lesssim v$ if $u = {\mathcal{O}}(v)$ and write $u \gtrsim v$ if $v \lesssim u$. We write $A \preceq B$ iff $B - A$ is a positive definite matrix. $I_d$  denotes the $d \times d$ identity matrix. 



\vspace{-2pt}
\subsection{Overparameterized Neural Networks}
\label{subsetion: overparameterized nn}
In this paper, we consider neural function approximation setting where the state-action value function is approximated by a two-layer neural network. For simplicity, we denote $\mathcal{X} := \mathcal{S} \times \mathcal{A}$ and view it as a subset of $\mathbb{R}^d$. Without loss of generality, we assume $\mathcal{X} \subset \mathbb{S}_{d-1}:= \{x \in \mathbb{R}^d: \|x\|_2 = 1\}$. We consider a standard two-layer neural network: $f(x; W, b) = \frac{1}{\sqrt{m}}\sum_{i=1}^{m} b_i \sigma(w_i^T x)$,  
where $m$ is an even number, $\sigma(\cdot) = \max \{ \cdot,0\}$ is the ReLU activation function \citep{aroraunderstanding}, and $W = (w_1^T, \ldots, w_m^T)^T \in \mathbb{R}^{md}$. During the training, we initialize $(W,b)$ via the symmetric initialization scheme \citep{gao2019convergence} as follows: For any $i \leq \frac{m}{2}$, $ w_i = w_{\frac{m}{2} + i} \sim \mathcal{N}(0, I_d /d)$, and $b_{\frac{m}{2} + i} = - b_i \sim \textrm{Unif}(\{-1,1\})$.\footnote{This symmetric initialization scheme makes $f(x; W_0) = 0$ and $\langle g(x;W_0), W_0\rangle = 0$ for any $x$.}  
During the training, we optimize over $W$ while the $b_i$ are kept fixed, thus we write $f(x; W, b)$ as $f(x;W)$. Denote $g(x; W) = \nabla_W f(x; W) \in \mathbb{R}^ {md}$, and let $W_0$ be the initial parameters of $W$. We assume that the neural network is overparameterized, i.e, the width $m$ is sufficiently larger than the number of samples $K$. Overparameterization has been shown to be effective in studying the convergence and the interpolation behaviour of neural networks \citep{arora2019exact,allen2019convergence,hanin2019finite,cao2019generalization,belkin2021fit}. Under such an overparameterization regime, the dynamics of the training of the neural network can be captured using the framework of the neural tangent kernel (NTK) \citep{jacot2018neural}. 









 \section{Algorithm}





\begin{algorithm}[t]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data $ \mathcal{D} = \{(s_h^k, a_h^k, r_h^k)\}_{h \in [H]}^{k \in [K]} $, a parametric function family $\mathcal{F} = \{f(\cdot, \cdot; W): W \in \mathcal{W}\} \subset \{\mathcal{X} \rightarrow \mathbb{R}\}$ (e.g. neural networks), perturbed variances $\{\sigma_h\}_{h \in [H]}$, number of bootstraps $M$, regularization parameter $\lambda$, step size $\eta$, number of gradient descent steps $J$, and cutoff margin $\psi$, split indices $\{\mathcal{I}_h\}_{h \in [H]}$ where $\mathcal{I}_{h} := [(H-h) K' + 1, \ldots, (H - h+1) K']$
\State Initialize $\tilde{V}_{H+1}(\cdot) \leftarrow 0$ and initialize $f(\cdot, \cdot; W)$ with initial parameter $W_0$
\For{$h = H, \ldots, 1$}
\For{$i = 1, \ldots, M$}
\State Sample $\{\xi^{k, i}_h\}_{k \in \mathcal{I}_h} \sim \mathcal{N}(0, \sigma^2_h)$ and $\zeta^i_h = \{\zeta^{j, i}_h\}_{j \in [d]} \sim \mathcal{N}(0, \sigma^2_h I_d)$
\label{PERVI: sample noises}


\State Perturb the dataset $\tilde{\mathcal{D}}^i_h \leftarrow \{s^k_h, a^k_h, r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{k, i}_h\}_{k \in \mathcal{I}_h}$ \Comment{\textit{Perturbation}}~
\label{PERVI: perturb data}
\State Let $\tilde{W}^i_h \leftarrow \textrm{GradientDescent}(\lambda, \eta, J, \tilde{\mathcal{D}}^i_h, \zeta^i_h, W_0)$ (Algorithm \ref{algo:GD}) \Comment{\textit{Optimization}}
\label{PERVI: GD}

\EndFor 

\State Compute $\tilde{Q}_h(\cdot, \cdot) \leftarrow \min \{ \textcolor{blue}{\min_{i \in [M]}} f(\cdot, \cdot; \tilde{W}^i_h), (H - h +1)(1 + \psi) \}^{+}$  \Comment{\textit{Pessimism}}~~~~
\label{PERVI: minimum of ensemble}






\State $\tilde{\pi}_{h} \leftarrow \argmax_{\pi_{h}}\langle \tilde{Q}_{h}, \pi_{h} \rangle$ and $\tilde{V}_h \leftarrow \langle \tilde{Q}_{h}, \tilde{\pi}_{h} \rangle$ \Comment{\textit{Greedy}}\qquad~
\label{PERVI: greedy policy}
\EndFor
\State \textbf{Output}: $\tilde{\pi} = \{ \tilde{\pi}_h \}_{h \in [H]}$.
\end{algorithmic}
\caption{{Value Iteration with Perturbed Rewards (VIPeR)}}
\label{algorithm: PERVI}
\end{algorithm}

\begin{wrapfigure}{R}{0.5\textwidth}
\vspace{-20pt}
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\State \textbf{Input:} Regularization parameter $\lambda$, step size $\eta$, number of gradient descent steps $J$, perturbed dataset $\tilde{\mathcal{D}}^i_h = \{s^k_h,a^k_h, r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{t, i}_h\}_{k \in \mathcal{I}_h}$, regularization perturber $\zeta^i_h$, initial parameter $W_0$ 
\State $L(W) := \frac{1}{2}\sum_{k \in \mathcal{I}_h} ( f(s^k_h, a^k_h; W) - (r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{k, i}_h) )^2 $ + $\frac{\lambda}{2} \| W + \zeta^i_h - W_0 \|_2^2$
\For{$j = 0, \ldots, J-1$}
\State $W_{j+1} \leftarrow W_j - \eta \nabla L(W_j)$
\EndFor 
\State \textbf{Output:} $W_J$. 
\caption{GradientDescent$(\lambda, \eta, J, \tilde{\mathcal{D}}^i_h, \zeta^i_h, W_0)$}
\label{algo:GD}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{wrapfigure}

In this section, we present the proposed algorithm called Value Iteration with Perturbed Rewards, or VIPeR; see Algorithm~\ref{algorithm: PERVI} for the pseudocode. The key idea underlying VIPeR is to train a parametric model (e.g., a neural network) on a perturbed-reward dataset several times and act pessimistically by picking the minimum over an ensemble of estimated state-action value functions. 
In particular, at each timestep $h \in [H]$, we draw $M$ independent samples of zero-mean Gaussian noise with variance $\sigma_h$. We use these samples to perturb the sum of the observed rewards, $r^k_h$, and the estimated value function with a one-step lookahead, i.e.,  $\tilde{V}_{h+1}(s^k_{h+1})$ (see Line \ref{PERVI: perturb data} of Algorithm~\ref{algorithm: PERVI}). The weights $\tilde{W}^i_h$ are then updated by minimizing the perturbed regularized squared loss on $\{\tilde{D}^i_h \}_{i \in [M]}$ using gradient descent (Line \ref{PERVI: GD}). We pick the value function pessimistically by selecting the minimum over the finite ensemble. The chosen value function is truncated at $(H - h +1)(1 + \psi)$ (see Line \ref{PERVI: minimum of ensemble}), where $\psi \geq 0$ is a small cutoff margin (more on this when we discuss the theoretical analysis). The returned policy is greedy with respect to the truncated pessimistic value function (see Line \ref{PERVI: greedy policy}).

It is important to note that we split the trajectory indices $[K]$ evenly into $H$ disjoint buckets  $[K] = \cup_{h \in [H]} \mathcal{I}_h $, where $\mathcal{I}_h = [(H-h) K' + 1, \ldots, (H-h + 1) K']$ for $K' := \lfloor K/H \rfloor$\footnote{Without loss of generality, we assume $K / H \in \mathbb{N}$.}, as illustrated in Figure \ref{fig: data split}. The estimated $\tilde{Q}_h$ is thus obtained only from the offline data with (trajectory) indices from $\mathcal{I}_h$ along with $\tilde{V}_{h+1}$. This novel design removes the data dependence structure in offline RL with function approximation \citep{nguyentang2021sample} and avoids a factor involving the log of the covering number in the bound on the sub-optimality of Algorithm~\ref{algorithm: PERVI}, as we show in Section \ref{lemma: bound ERM with the Bellman target}. 























To deal with the non-linearity of the underlying MDP, we use a two-layer fully connected neural network as the parametric function family $\mathcal{F}$ in Algorithm \ref{algorithm: PERVI}. In other words, we approximate the state-action values: $f(x; W) = \frac{1}{\sqrt{m}}\sum_{i=1}^{m} b_i \sigma(w_i^T x)$, as described in Section \ref{subsetion: overparameterized nn}. We use two-layer neural networks to simplify the computational analysis. We utilize gradient descent to train the state-action value functions $\{f(\cdot, \cdot; \tilde{W}^i_h)\}_{i \in [M]}$, on perturbed rewards. The use of gradient descent is for the convenience of computational analysis, and our results can be extended to stochastic gradient descent by leveraging recent advances in the theory of deep learning~\citep{allen2019convergence,cao2019generalization}, albeit with a more involved analysis. 



Existing offline RL algorithms utilize estimates of statistical confidence regions to achieve pessimism in the offline setting. Explicitly constructing these confidence bounds is computationally expensive in complex problems where a neural network is used for function approximation. For example, the lower-confidence-bound-based algorithms in neural offline contextual bandits \citep{nguyen2021offline} and RL \citep{xu2022provably} require computing the inverse of a large covariance matrix with the size scaling with the number of network parameters. This is computationally prohibitive in most practical settings. Algorithm~\ref{algorithm: PERVI} (VIPeR) avoids such expensive computations while still obtaining provable pessimism and guaranteeing a  rate of $\tilde{\mathcal{O}}(\frac{1}{\sqrt{K}})$ on the sub-optimality, as we show in the next section. 









\begin{wrapfigure}{r}{0.24\textwidth}
    \vspace{-45pt}
    \centering
    \includegraphics[scale=0.45]{iclr2023/figs/data_split.pdf}
    \vspace{-10pt}
    \caption{Data splitting.}
    \label{fig: data split}
    \vspace{-14pt}
\end{wrapfigure}






































%
 \section{Sub-optimality Analysis}
\label{section: subopt analysis}


Next, we provide a theoretical guarantee on the sub-optimality of VIPeR for the function approximation class, $\mathcal{F}$, represented by (overparameterized) neural networks. Our analysis builds on the recent advances in generalization and optimization of deep neural networks \citep{arora2019exact, allen2019convergence, hanin2019finite, cao2019generalization, belkin2021fit} that leverage the observation that the dynamics of the neural parameters learned by (stochastic) gradient descent can be captured by the corresponding neural tangent kernel (NTK) space~\citep{jacot2018neural} when the network is overparameterized. 


Next, we recall some definitions and state our key assumptions, formally. 






\begin{defn}[NTK \citep{jacot2018neural}]
The NTK kernel $K_{ntk}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is defined as
\begin{align*}
    K_{ntk}(x,x') = \mathbb{E}_{w \sim \mathcal{N}(0, I_d/d)} \langle x \sigma'(w^T x), x' \sigma'(w^T x') \rangle,
\end{align*}
{where $\sigma'(u) = \mathbbm{1}\{u \geq 0\}$.}
\label{definition: ntk}
\end{defn}
Let $\mathcal{H}_{ntk}$ denote the reproducing kernel Hilbert space (RKHS) induced by the NTK, $K_{ntk}$. {Since$K_{ntk}$ is a universal kernel \citep{DBLP:conf/iclr/JiTX20}, we have that $\mathcal{H}_{ntk}$ is dense in the space of continuous functions on (a compact set) $\gX = \gS \times \gA$ \citep{Rahimi08}.} 

\begin{defn}[Effective dimension]
For any $h \in [H]$, the effective dimension of the NTK matrix on data $\{x^k_h\}_{k \in \mathcal{I}_h}$ is defined as 
\begin{align*}
    \tilde{d}_h := \frac{\logdet(I_{K'} + \mathcal{K}_h/\lambda)}{\log(1 + K' / \lambda)},
\end{align*}
where $\mathcal{K}_h := [K_{ntk}(x^i_h, x^j_h)]_{i,j \in \mathcal{I}_h}$ is the Gram matrix of $K_{ntk}$ on the data $\{x^k_h\}_{k \in \mathcal{I}_h}$. We further define $\tilde{d} := \max_{h \in [H]} \tilde{d}_h$. 
\label{definition: effective dimension}
\end{defn}
\begin{remark}
Intuitively, the effective dimension $\tilde{d}_h$ measures the number of principal dimensions over which the projection of the data $\{x^k_h\}_{k \in \mathcal{I}_h}$ in the RKHS $\mathcal{H}_{ntk}$ is spread. It was first introduced by \citet{valko2013finite} for kernelized contextual bandits and was subsequently adopted by \citet{yang2020reinforcement} and \citet{zhou2020neural} for kernelized RL and neural contextual bandits, respectively. The effective dimension is data-dependent and can be bounded by $\tilde{d} \lesssim K'^{(d+1) / (2d)}$ in the worst case (see Section \ref{section: extended discussion} for more details).\footnote{Note that this is the worst-case bound, and the effective dimension can be significantly smaller in practice.} 
\end{remark}



{
\begin{defn}[RKHS of the infinite-width NTK] 
Define  
$\mathcal{Q}^* := \{ f(x) = \int_{\mathbb{R}^d} c(w)^T x \sigma'(w^T x) d w: \sup_{w} \frac{\| c(w) \|_2}{p_0(w)} < B \}, 
    \label{eq: function class}$
where $c: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is any function, $p_0$ is the probability density function of $\mathcal{N}(0, I_d/d)$, and $B$ is some positive constant.
\label{eq: target function class}
\end{defn}
}



We make the following assumption about the regularity of the underlying MDP under function approximation. 
\begin{assumption}[Completeness]
For any $V: \mathcal{S} \rightarrow [0,H + 1]$ and any $h \in [H]$, $\mathbb{B}_h V \in \mathcal{Q}^*$.\footnote{We consider $V: \mathcal{S} \rightarrow [0,H + 1]$ instead of $V: \mathcal{S} \rightarrow [0,H]$ due to the cutoff margin $\psi$ in Algorithm \ref{algorithm: PERVI}.}
\label{assumption: completeness}
\end{assumption}
Assumption \ref{assumption: completeness} ensures that the Bellman operator $\mathbb{B}_h$ can be captured by an infinite-width neural network. This assumption is mild as  $\mathcal{Q}^*$ is a dense subset of $\mathcal{H}_{ntk}$~\citep[Lemma~C.1]{gao2019convergence} when $B = \infty$, thus $\mathcal{Q}^*$ is an expressive function class when $B$ is sufficiently large. Moreover, similar assumptions have been used in many prior works on provably efficient RL with function approximation \citep{cai2019neural,wang2020reinforcement,yang2020function,nguyentang2021sample}.

Next, we present a bound on the suboptimality of the policy $\tilde{\pi}$ returned by Algorithm~\ref{algorithm: PERVI}. Recall that we use the initialization scheme described in Section~\ref{subsetion: overparameterized nn}. Fix any $\delta \in (0,1)$. \
\begin{theorem}
Let $\sigma_h = \sigma:= 1 + \lambda^{\frac{1}{2}}  B + (H + 1) \big[ \textstyle {\tilde{d} \log (1 + K' / \lambda) + 2 + 2 \log (3 H / \delta)} \big]^{\frac{1}{2}}$.  
Let {$m = \textrm{poly}( K', H, d, B, \tilde{d}, \lambda, \delta)$ be some high-order polynomial of the problem parameters}, $\lambda = 1 + \frac{H}{K}$, $\eta \lesssim (\lambda + K')^{-1}$, $J \gtrsim K' \log (K'(H \sqrt{\tilde{d}} + B))$,
$\psi = 1$, 
and $M = \log \frac{HSA}{\delta} / \log \frac{1}{1 - \Phi(-1)}$, where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal distribution. Then, under Assumption \ref{assumption: completeness}, with probability at least $1 -  MH m^{-2} - 2 \delta$, for any $s_1 \in \mathcal{S}$, we have that 
\begin{align*}
   \subopt(\tilde{\pi}; s_1) \leq \sigma (1 +  \sqrt{2 \log (MSAH / \delta)}  ) \cdot  \mathbb{E}_{\pi^*} \left[ \sum_{h=1}^H \| g(s_h, a_h; W_0) \|_{\Lambda_h^{-1}} \right] + \tilde{\mathcal{O}}(\frac{1}{K'})
\end{align*}
where $\Lambda_h := \lambda I_{md} + \sum_{k \in \mathcal{I}_h} g(s^k_h, a^k_h; W_0) g(s^k_h, a^k_h; W_0)^T \in \mathbb{R}^{md \times md }$.







\label{theorem: main theorem}
\end{theorem}

\begin{remark}
Theorem \ref{theorem: main theorem} shows that the randomized design in our proposed algorithm yields a provable uncertainty quantifier even though we do not explicitly maintain any confidence regions in the algorithm. {The implicit pessimism via perturbed rewards introduces an extra factor of $1 +  \sqrt{2 \log (MSAH / \delta)}$ into the confidence parameter $\beta$.} 
\label{remark: simplified params}
\end{remark}


We build upon Theorem \ref{theorem: main theorem} to obtain an explicit bound using the following data coverage assumption. 
\begin{assumption}[Optimal-Policy Concentrability]
$\exists \kappa < \infty$,  $\sup_{(h,s_h, a_h)} \frac{d^{*}_h(s_h,a_h)}{d^{\mu}_h(s_h,a_h)} \leq \kappa$. 


\label{assumption: OPC}
\end{assumption}
Assumption \ref{assumption: OPC} requires any positive-probability trajectory induced by the optimal policy to be covered by the behavior policy. This data coverage assumption is significantly milder than the uniform coverage assumptions in many FQI-based offline RL algorithms \citep{DBLP:journals/jmlr/MunosS08,chen2019information,nguyentang2021sample} and is common in pessimism-based algorithms \citep{rashidinejad2021bridging,nguyen2021offline,Chen2022OfflineRL,zhan2022offline}. 
\begin{theorem}
For the same parameter settings and the same assumption as in Theorem \ref{theorem: main theorem}, we have that 
with probability at least $1 - MH m^{-2} - 5 \delta$,
\begin{align*}
    \subopt(\tilde{\pi}) &\leq \frac{2 \tilde{\sigma} \kappa H}{\sqrt{K'}} \left( \sqrt{2\tilde{d} \log(1 + K' / \lambda) + 1} +   \sqrt{\frac{\log \frac{H}{\delta}}{\lambda}} \right) + \frac{16 H}{3 K'} \log \frac{\log_2(K' H)}{\delta} + \tilde{\mathcal{O}}(\frac{1}{K'}),
\end{align*}
where $\tilde{\sigma} := \sigma (1 +  \sqrt{2 \log (SAH / \delta)}  )$. 
\label{theorem: explicit bound}
\end{theorem}

\begin{remark}
Theorem \ref{theorem: explicit bound} shows that with appropriate parameter choice, VIPeR achieves a sub-optimality of  $\tilde{\mathcal{O}}\left(  \frac{ \kappa H^{3/2} \sqrt{\tilde{d}} \cdot \max\{ B, H \sqrt{ \tilde{d} } \} }{\sqrt{K}} \right)$.
Compared to \cite{yang2020function}, we improve by a factor of $K^{\frac{2}{d \gamma - 1}}$ for some $\gamma \in (0,1)$ at the expense of $\sqrt{H}$. When realized to a linear MDP in $\mathbb{R}^{d_{lin}}$, $\tilde{d} = d_{lin}$ and our bound reduces into $\tilde{\mathcal{O}}\left(  \frac{\kappa H^{5/2}  d_{lin} }{\sqrt{K}} \right)$ which improves the bound $\tilde{\mathcal{O}}(d^{3/2}_{lin} H^2 / \sqrt{K})$ of PEVI \citep[Corollary~4.6]{jin2021pessimism} by a factor of $\sqrt{d_{lin}}$. We provide the result summary and comparison in Table \ref{tab:my_label} and give a more detailed discussion in Subsection \ref{subsection: comparison with other works in details}. 
\end{remark}



\begin{table}[]
\centering
    \def\arraystretch{1.9}\resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
work & bound & i.i.d? & explorative data? & finite spectrum? & matrix inverse? & opt \\
       \hline 
       \hline
\cite{jin2021pessimism}   & $\tilde{\mathcal{O}}\left( \frac{d^{3/2}_{lin} H^2 } { \sqrt{K}}  \right)$ & no & yes & yes  & yes & analytical \\ 
      \hline 
      \cite{yang2020function} & $\tilde{\mathcal{O}} \left( \frac{H^2 \sqrt{\tilde{d}^2 + \tilde{d} \tilde{n} }}{\sqrt{K}} \right)$ & no & -- & no & yes & oracle \\ 
      \hline
      \cite{xu2022provably} & $\tilde{\mathcal{O}} \left( \frac{\tilde{d} H^2}{ \sqrt{K}} \right)$ & yes & yes & yes  & yes & oracle \\ 
      \hline 
      \textbf{This work} & \textbf{$\tilde{\mathcal{O}}\left(  \frac{ \kappa H^{5/2}  \tilde{d} }{\sqrt{K}} \right)$} & \textbf{no} & \textbf{no} & \textbf{no} & \textbf{no} & \textbf{GD} \\
      \hline
    \end{tabular}
    }
    \caption{State-of-the-art results for offline RL with function approximation. The third and the fourth columns ask if the corresponding result needs the data to be i.i.d, and well-explored, respectively; the fifth column asks if the induced RKHS needs to have a finite spectrum; the sixth column asks if the algorithm needs to invert a covariance matrix and the last column presents the optimizer being used. Here $\tilde{n}$ is the log of the covering number.}
    \label{tab:my_label}
\end{table}

















































































































%
 \vspace{-5pt}
\section{Experiments}
In this section, we empirically evaluate the proposed algorithm VIPeR against several state-of-the-art baselines, including (a) PEVI~\citep{jin2021pessimism}, which explicitly constructs lower confidence bound (LCB) for pessimism in a linear model (thus, we rename this algorithm as LinLCB for convenience in our experiments); (b) NeuraLCB~\citep{nguyen2021offline} which explicitly constructs an LCB using neural network gradients; (c) NeuraLCB (Diag), which is NeuraLCB with a diagonal approximation for estimating the confidence set as suggested in NeuraLCB \citep{nguyen2021offline}; (d) Lin-VIPeR which is VIPeR realized to the linear function approximation instead of neural network function approximation; (e) NeuralGreedy (LinGreedy, respectively) which uses neural networks (linear models, respectively) to fit the offline data and act greedily with respect to the estimated state-action value functions without any pessimism. Note that when the parametric class, $\mathcal{F}$, in Algorithm \ref{algorithm: PERVI} is that of neural networks, we refer to VIPeR as Neural-VIPeR. We do not utilize data splitting in the experiments. We provide further algorithmic details of the baselines in Section~\ref{section: baseline algorithms}. 


We evaluate all algorithms in two problem settings: (1) the underlying MDP is a linear MDP whose reward functions and transition kernels are linear in some known feature map~\citep{jin2020provably}, and (2) the underlying MDP is non-linear with horizon length $H = 1$ (i.e., non-linear contextual bandits)~\citep{zhou2020neural}, where the reward function is either synthetic or constructed from MNIST dataset~\citep{lecun1998gradient}. We also evaluate (a variant of) our algorithm and show its strong performance advantage in the D4RL benchmark~\citep{DBLP:journals/corr/abs-2004-07219} in Section~\ref{subsection: d4rl}. We implemented all algorithms in Pytorch~\citep{paszke2019pytorch} on a server with Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 755G RAM, and one NVIDIA Tesla V100 Volta GPU Accelerator 32GB Graphics Card.\footnote{Our code is available here: \url{https://github.com/thanhnguyentang/neural-offline-rl}.}















\subsection{Linear MDPs}
\label{subsection: linear mdp}


We first test the effectiveness of pessimism implicit in VIPeR (Algorithm~\ref{algorithm: PERVI}). To that end, we construct a hard instance of linear MDPs~\citep{yinnear,min2021variance}; due to page limitation, we defer the details of our construction to Section~\ref{subsection: description of linear mdp}. We test for different values of $H \in \{20,30,50,80\}$ and report the sub-optimality of LinLCB, Lin-VIPeR, and LinGreedy, averaged over $30$ runs, in Figure \ref{fig: linear mdp}. 
We find that LinGreedy, which is uncertainty-agnostic, fails to learn from offline data and has poor performance in terms of sub-optimality when compared to pessimism-based algorithms LinLCB and Lin-VIPeR. Further, LinLCB outperforms Lin-VIPeR when $K$ is smaller than $400$, but the performance of the two algorithms matches for larger sample sizes. Unlike LinLCB, Lin-VIPeR does not construct any confidence regions or require computing and inverting large (covariance) matrices. The Y-axis is in log scale; thus, Lin-VIPeR already has small sub-optimality in the first $K \approx 400$ samples. These show the effectiveness of the randomized design for pessimism implicit in Algorithm \ref{algorithm: PERVI}. 















\begin{figure}
\centering
    \includegraphics[scale=0.27]{iclr2023/figs/VIPeR_linmdp.png}
    \caption{Empirical results of sub-optimality (in log scale) on linear MDPs.}
    \label{fig: linear mdp}
    \vspace{-10pt}
\end{figure}




\subsection{Neural Contextual Bandits}
\label{subsection: main neural contextual bandits}
\begin{figure}[h!]
    \centering
    \vspace{-10pt}
\subfigure[]{\includegraphics[width=0.322\textwidth]{iclr2023/figs/cosine_regret_VIPeR.png}}
    \subfigure[]{\includegraphics[width=0.322\textwidth]{iclr2023/figs/exp_regret_VIPeR.png}}
    \subfigure[]{\includegraphics[width=0.322\textwidth]{iclr2023/figs/mnist_regret_VIPeR.png}}
    \vspace{-15pt}
    \caption{Sub-optimality (on log-scale) vs. sample size (K) for neural contextual bandits with following reward functions: (a) $r(s,a)\!=\!\cos (3 s^T \theta_a)$, (b) $r(s,a)\!=\!\exp (-10 (s^T \theta_a)^2)$, and (c)~MNIST. }
\label{fig: neural contextuable bandits}
\end{figure}

Next, we compare the performance and computational efficiency of various algorithms against VIPeR when neural networks are employed. For simplicity, we consider contextual bandits, a special case of MDPs with horizon $H = 1$. 
Following~\cite{zhou2020neural, nguyen2021offline}, we use the bandit problems specified by the following reward functions: (a) $r(s, a) = \cos(3 s^T \theta_a)$; (b) $r(s, a) = \exp(-10 (s^T \theta_a)^2)$, where $s$ and $\theta_a$ are generated uniformly at random from the unit sphere $\mathbb{S}_{d-1}$ with $d = 16$ and $A = 10$; (c) MNIST, where $r(s, a) = 1$ if $a$ is the true label of the input image $s$ and $r(s, a) = 0$, otherwise. To predict the value of different actions from the same state $s$ using neural networks, we transform a state $s \in \mathbb{R}^d$ into $d A$-dimensional vectors $s^{(1)} = (s, 0, \ldots, 0), s^{(2)} = (0, s, 0, \ldots, 0), \ldots, s^{(A)} = (0, \ldots, 0, s)$ and train the network to map $s^{(a)}$ to $r(s, a)$ given a pair of data $(s, a)$. For Neural-VIPeR, NeuralGreedy, NeuraLCB, and NeuraLCB (Diag), we use the same neural network architecture with two hidden layers of width $m = 64$ and train the network with Adam optimizer \citep{kingma2014adam}. Due to page limitations, we defer other experimental details and hyperparameter setting to Section~\ref{subsection: extended page for neural contextual bandit experiment}. We report the sub-optimality averaged over $5$ runs in Figure~\ref{fig: neural contextuable bandits}. We see that algorithms that use a linear model, i.e., LinLCB and Lin-VIPeR significantly underperform neural-based algorithms, i.e., NeuralGreedy, NeuraLCB, NeuraLCB (Diag) and Neural-VIPeR, attesting to the crucial role neural representations play in RL for non-linear problems. It is also interesting to observe from the experimental results that NeuraLCB does not always outperform its diagonal approximation, NeuraLCB (Diag) (e.g., in Figure~\ref{fig: neural contextuable bandits}(b)), putting a question mark on the empirical effectiveness of NTK-based uncertainty for offline RL. Finally, Neural-VIPeR outperforms all algorithms in the tested benchmarks, suggesting the effectiveness of our randomized design with neural function approximation. 





\begin{figure}[h]
    \vspace{-10pt}
    \centering
    \subfigure[]{\includegraphics[width=0.44\textwidth]{iclr2023/figs/quadratic_VIPeR_time.png}} 
    \subfigure[]{\includegraphics[width=0.42\textwidth]{iclr2023/figs/quadratic_elapsed_time_vs_m_VIPeR.png}}
    \vspace{-10pt}
    \caption{Elapsed time (in seconds) for action selection in the contextual bandits problem with $r(s,a) = 10 (s^T \theta_a)^2$: (a) Runtime of action selection versus the number of (offline) data points $K$, and (b) runtime of action selection versus the network width $m$ (for $K = 500$).}
    \label{fig: time for action selection}
    \vspace{-8pt}
\end{figure}

\begin{wrapfigure}{r}{0.37\textwidth}
\vspace{-10pt}
    \centering
    \includegraphics[scale=0.43]{iclr2023/figs/subopt_vs_M.png}
\caption{Sub-optimality of Neural-VIPeR versus different values of $M$.}
    \label{fig: subopt vs M}
    \vspace{-5pt}
\end{wrapfigure}

Figure~\ref{fig: time for action selection} shows the average runtime for action selection of neural-based algorithms NeuraLCB, NeuraLCB (Diag), and Neural-VIPeR. We observe that algorithms that use explicit confidence regions, i.e., NeuraLCB and NeuraLCB (Diag), take significant time selecting an action when either the number of offline samples $K$ or the network width $m$ increases. This is perhaps not surprising because NeuraLCB and NeuraLCB (Diag) need to compute the inverse of a large covariance matrix to sample an action and maintain the confidence region for each action per state. The diagonal approximation significantly reduces the runtime of NeuraLCB, but the runtime still scales with the number of samples and the network width. In comparison, the runtime for action selection for Neural-VIPeR is constant. Since NeuraLCB, NeuraLCB (Diag), and Neural-VIPeR use the same neural network architecture, the runtime spent training one model is similar. The only difference is that Neural-VIPeR trains $M$ models while NeuraLCB and NeuraLCB (Diag) train a single model. However, as the perturbed data in Algorithm~\ref{algorithm: PERVI} are independent, training $M$ models in Neural-VIPeR is embarrassingly parallelizable. 

















Finally, in Figure~\ref{fig: subopt vs M}, we study the effect of the ensemble size on the performance of Neural-VIPeR. We use  different values of $M \in \{1,2,5,10,20,30,50,100,200\}$ for sample size $K = 1000$. We find that the sub-optimality of Neural-VIPeR decreases graciously as $M$ increases. Indeed, the grid search from the previous experiment in Figure~\ref{fig: neural contextuable bandits} also yields $M = 10$ and $M = 20$ from the search space $M \in \{1, 10, 20\}$ as the best result. This suggests that the ensemble size can also play an important role as a hyperparameter that can determine the amount of pessimism needed in a practical setting.






%
 \section{Conclusion}
We propose a novel algorithmic approach for offline RL that involves randomly perturbing value functions and pessimism. Our algorithm eliminates the computational overhead of explicitly maintaining a valid confidence region and computing the inverse of a large covariance matrix for pessimism. We bound the suboptimality of the proposed algorithm as $\tilde{\mathcal{O}}\big(  { \kappa H^{5/2}  \tilde{d} }/{\sqrt{K}} \big)$. We support our theoretical claims of computational efficiency and the effectiveness of our algorithm with extensive experiments. 






\section*{Acknowledgements}
This research was supported, in part, by  DARPA~GARD award HR00112020004, NSF CAREER award IIS-1943251, an award from the Institute of Assured Autonomy, and Spring 2022 workshop on ``Learning and Games'' at the Simons Institute for the Theory of Computing. \bibliography{main}
\bibliographystyle{iclr2023/iclr2023_conference}
\newpage
\appendix
\section{Experiment Details}
\subsection{Linear MDPs}
\label{subsection: description of linear mdp}
In this subsection, we provide further details to the experiment setup used in Subsection \ref{subsection: linear mdp}. We describe in detail a variant of the hard instance of linear MDPs \citep{yinnear} used in our experiment. The linear MDP has $\mathcal{S} = \{0,1\}$, $\mathcal{A} = \{0,1, \cdots, 99\}$, and the feature dimension $d = 10$. Each action $a \in [99] = \{1, \ldots, 99\}$ is represented by its binary encoding vector $u_a \in \mathbb{R}^8$ with entry being either $-1$ or $1$. The feature mapping $\phi(s,a)$ is given by $\phi(s,a) = [u_a^T, \delta(s,a), 1 - \delta(s,a)]^T \in \mathbb{R}^{10}$, where $\delta(s,a) =1 $ if $(s,a)=(0,0)$ and $\delta(s,a) =0$ otherwise. The true measure $\nu_h(s)$ is given by $\nu_h(s) = [0,\cdots, 0, (1 - s) \oplus \alpha_h, s \oplus \alpha_h]$  where $\{\alpha_h\}_{h \in [H]} \in \{0,1\}^{H}$ are generated uniformly at random and $\oplus$ is the XOR operator. We define $\theta_h = [0, \cdots, 0, r, 1 - r]^T \in \mathbb{R}^{10}$ where $r = 0.99$. Recall that the transition follows $\mathbb{P}_h(s'|s,a) = \langle \phi(s,a), \nu_h(s') \rangle$ and the mean reward $r_h(s,a) = \langle \phi(s,a), \theta_h \rangle$. We generated a priori $K \in \{1, \ldots, 1000\}$ trajectories using the behavior policy $\mu$, where for any $h \in [H]$ we set $\mu_h(0|0) = p, \mu_h(1|0) = 1 - p, \mu_h(a|0) = 0 ,\forall a >1; \mu_h(0|1) =  p, \mu_h(a|1) = (1 - p)/99, \forall a > 0 $, where we set $p = 0.6$.

We run over $K \in \{1, \ldots, 1000\}$ and $H \in \{20, 30, 50, 80\}$. We set $\lambda = 0.01$ for all algorithms. For Lin-VIPeR, we grid searched $\sigma_h = \sigma \in \{0.0, 0.1, 0.5, 1.0, 2.0\}$ and $M \in \{1,2,10,20\}$. For LinLCB, we grid searched its uncertainty multiplier $\beta \in \{0.1, 0.5, 1, 2\}$. The sub-optimality metric is used to compare algorithms. For each $H \in \{20, 30, 50, 80\}$, each algorithm was executed for $30$ times and the averaged results (with std) are reported in Figure \ref{fig: linear mdp}. 

\subsection{Neural Contextual Bandits}
\label{subsection: extended page for neural contextual bandit experiment}
In this subsection, we provide in detail the experimental and hyperparameter setup in our experiment in Subsection \ref{subsection: main neural contextual bandits}. For Neural-VIPeR, NeuralGreedy, NeuraLCB and NeuraLCB (Diag), we use the same neural network architecture with two hidden layers whose width $m = 64$, train the network with Adam optimizer \citep{kingma2014adam} with learning rate being grid-searched over $\{0.0001, 0.001, 0.01\}$ and batch size of $64$. For NeuraLCB, NeuraLCB (Diag), and LinLCB, we grid-searched $\beta$ over $\{0.001, 0.01, 0.1, 1, 5, 10\}$. For Neural-VIPeR and Lin-VIPeR, we grid-searched $\sigma_h = \sigma$ over $\{0.001, 0.01, 0.1, 1, 5, 10\}$ and $M$ over $\{1, 10, 20\}$. We did not run NeuraLCB in MNIST as the inverse of a full covariance matrix in this case is extremely expensive. We fixed the regularization parameter $\lambda = 0.01$ for all algorithms. Offline data is generated by the $(1-\epsilon)$-optimal policy which generates non-optimal actions with probability $\epsilon$ and optimal actions with probability $1 - \epsilon$. We set $\epsilon = 0.5$ in our experiments. To estimate the expected sub-optimality, we randomly obtain $1,000$ novel samples (i.e. not used in training) to compute the average sub-optimality and keep these same samples for all algorithms. 

\subsection{Experiment in D4RL Benchmark}
\label{subsection: d4rl}
In this subsection, we evaluate the effectiveness of the reward perturbing design of VIPeR in the Gym domain in the D4RL benchmark \citep{DBLP:journals/corr/abs-2004-07219}. The Gym domain has three environments (HalfCheetah, Hopper, and Walker2d) with five datasets (random, medium, medium-replay, medium-expert, and expert), making up 15 different settings.


\paragraph{Design.} To adapt the design of VIPeR to continuous control, we use the actor-critic framework. Specifically, we have $M$ critics $\{{Q}_{\theta^i}\}_{i \in [M]}$ and one actor $\pi_{\phi}$, where $\{\theta^i\}_{i\in [M]}$ and $\phi$ are the learnable parameters for the critics and actor, respectively. Note that in the continuous domain, we consider discounted MDP with discount factor $\gamma$, instead of finite-time episode MDP as we initially considered in our setting in the main paper. In the presence of the actor $\pi_{\phi}$, there are two modifications to Algorithm \ref{algorithm: PERVI}. The first modification is that when training the critics $\{{Q}_{\theta}^i\}_{i \in [M]}$, we augment the training loss in Algorithm \ref{algo:GD} with a new penalization term. Specifically, the critic loss for $Q_{\theta^i}$ on a training sample $\tau := (s,a,r,s')$ (sampled from the offline data $\gD$) is 
\begin{align}
    \gL(\theta^i; \tau) = \left(Q_{\theta^i}(s, a) - (r + \gamma  Q_{\bar{\theta}^i}(s') + \xi ) \right)^2 +  \beta \underbrace{\mathbb{E}_{a' \sim \pi_{\phi}
(\cdot| s)} \left[ (Q_{\theta^i}(s, a') - \bar{Q}(s, a') )^2 \right]}_{\text{penalization term } R(\theta^i; s, \phi) },
\label{eq: new critic loss for continuous domain}
\end{align}
where $\bar{\theta}^i$ has the same value of the current $\theta^i$ but is kept fixed, $\bar{Q} = \frac{1}{M} \sum_{i=1}^M Q_{\theta^i}$ and $\xi \sim \gN(0, \sigma^2)$ is Gaussian noise, and $\beta$ is a penalization parameter (note that $\beta$ here is totally different from the $\beta$ in Theorem \ref{theorem: main theorem}). The penalization term $R(\theta^i; s, \phi)$ discourages overestimation in the value function estimate $Q_{\theta^i}$ for out-of-distribution (OOD) actions $a' \sim \pi_{\phi}(\cdot|s)$. Our design of $R(\theta^i; s, \phi)$ is initially inspired by the OOD penalization in \cite{bai2022pessimistic} that creates a pessimistic pseudo target for the values at OOD actions. Note that we do not need any penalization for OOD actions in our experiment for contextual bandits in Section \ref{subsection: main neural contextual bandits}. This is because in the contextual bandit setting in Section \ref{subsection: main neural contextual bandits} the action space is finite and not large, thus the offline data often sufficiently cover all good actions. In the continuous domain such as the Gym domain of D4RL, however, it is almost certain that there are actions that are not covered by the offline data since the action space is continuous. We also note that the inclusion of the OOD action penalization term $R(\theta^i; s, \phi)$ in this experiment does not contradict our guarantee in Theorem \ref{theorem: main theorem} since in the theorem we consider finite action space while in this experiment we consider continuous action space.  We argue that the inclusion of some regularization for OOD actions (e.g., $R(\theta^i; s, \phi)$) is necessary for the continuous domain. \footnote{In our experiment, we also observe that without this penalization term, the method struggles to learn any good policy. However, using only the penalization term without the first term in Eq. (\ref{eq: new critic loss for continuous domain}), we observe that the method cannot learn either.}



The second modification to Algorithm \ref{algorithm: PERVI} for the continuous domain is the actor training, which is the implementation of policy extraction in line \ref{PERVI: greedy policy}  of Algorithm \ref{algorithm: PERVI}. Specifically, to train the actor $\pi_{\phi}$ given the ensemble of critics $\{{Q}_{\theta}^i\}_{i \in [M]}$,  we use soft actor update in \cite{haarnoja2018soft} via
\begin{align}
    \max_{\phi} \left\{ \mathbb{E}_{s \sim \gD, a' \sim \pi_{\phi}(\cdot|s)} \left[ \min_{i \in [M]} Q_{\theta^i}(s,a') - \log \pi_{\phi}(a'|s) \right] \right\},
\end{align}
which is trained using gradient ascent in practice. Note that in the discrete action domain, we do not need such actor training as we can efficiently extract the greedy policy with respect to the estimated action-value functions when the action space is finite. Also note that we do not use data splitting and value truncation as in the original design of Algorithm \ref{algorithm: PERVI}. 

\paragraph{Hyperparameters.} For the hyper-parameters of our training, we set $M = 10$ and the noise variance $\sigma=0.01$. For $\beta$, we decrease it from $0.5$ to $0.2$ by linear decay for the first 50K steps and exponential decay for the remaining steps. For the other hyperparameters of actor-critic training, we fix them the same as in \cite{bai2022pessimistic}. Specifically, the Q-network is the fully connected neural network with three hidden layers all of which has $256$ neurons. The learning rate for the actor and the critic are $10^{-4}$ and $3 \times 10^{-4}$, respectively. The optimizer is Adam. 

\paragraph{Results.} We compare VIPeR with several state-of-the-art algorithms, including (i) BEAR \citep{DBLP:conf/nips/KumarFSTL19} that use MMD distance to constraint policy to the offline data, (ii) UWAC \citep{DBLP:conf/icml/0001ZSSZSG21} that improves BEAR using dropout uncertainty, (iii) CQL \citep{DBLP:conf/nips/KumarZTL20} that minimizes Q-values of OOD actions, (iv) MOPO \citep{DBLP:conf/nips/YuTYEZLFM20} that uses model-based uncertainty via ensemble dynamics, (v) TD3-BC \citep{DBLP:conf/nips/FujimotoG21} that uses adaptive behavior cloning, and (vi) PBRL \citep{bai2022pessimistic} that use uncertainty quantification via disagreement of bootstrapped Q-functions. We follow the evaluation protocol in \cite{bai2022pessimistic}. We run our algorithm for five seeds and report the average final evaluation scores with standard deviation. We report the scores of our method and the baselines in Table \ref{tab: d4rl result}. We can see that our method has a strong advantage of good performance (highest scores) in 11 out of 15 settings, and has good stability (small std) in all settings. Overall, we also have the strongest average scores aggregated over all settings. 

\begin{table}[]
    \centering
    \def\arraystretch{1.3}\resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllll}
        \hline 
        & & BEAR & UWAC & CQL & MOPO & TD3-BC & PBRL & VIPeR \\
        \hline 
        \multirow{3}{*}{\rotatebox{90}{Random}} 
        & HalfCheetah & 2.3 0.0 & 2.3 0.0 & 17.5 1.5 & \textbf{35.9} 2.9 & 11.0 1.1 & 11.0 5.8 &14.5	2.1\\
        & Hopper & 3.9 2.3 &2.7 0.3 &7.9 0.4 &16.7 12.2 &8.5 0.6 &26.8 9.3 &\textbf{31.4}	0.0\\ 
       & Walker2d & 12.8 10.2 &2.0 0.4 &5.1 1.3 &4.2 5.7 &1.6 1.7 &8.1 4.4 &\textbf{20.5}	0.5\\
       \hline 
        \multirow{3}{*}{\rotatebox{90}{Medium}} 
        & HalfCheetah & 43.0 0.2 &42.2 0.4 &47.0 0.5 &\textbf{73.1} 2.4 &48.3 0.3 &57.9 1.5 &58.5	1.1\\
         & Hopper & 51.8 4.0 &50.9 4.4 &53.0 28.5 &38.3 34.9 &59.3 4.2 &75.3 31.2 &\textbf{99.4}	6.2\\ 
       & Walker2d & -0.2 0.1 &75.4 3.0 &73.3 17.7 &41.2 30.8 &83.7 2.1 &\textbf{89.6} 0.7 &\textbf{89.6}	1.2\\
       \hline 
       \multirow{3}{*}{\rotatebox{90}{Medium}} \multirow{3}{*}{\rotatebox{90}{Replay}} 
       & HalfCheetah & 36.3 3.1 &35.9 3.7 &45.5 0.7 &\textbf{69.2} 1.1 &44.6 0.5 &45.1 8.0 &45.0	8.6\\
         & Hopper & 52.2 19.3 &25.3 1.7 &88.7 12.9 &32.7 9.4 &60.9 18.8 &\textbf{100.6} 1.0 &\textbf{100.2}	1.0\\ 
       & Walker2d & 7.0 7.8 &23.6 6.9 &81.8 2.7 &73.7 9.4 &81.8 5.5 &77.7 14.5 &\textbf{83.1}	4.2\\
        \hline 
       \multirow{3}{*}{\rotatebox{90}{Medium}} \multirow{3}{*}{\rotatebox{90}{Expert}} 
       & HalfCheetah & 46.0 4.7 &42.7 0.3 &75.6 25.7 &70.3 21.9 &90.7 4.3 &92.3 1.1 &\textbf{94.2}	1.2\\
         & Hopper & 50.6 25.3 &44.9 8.1 &105.6 12.9 &60.6 32.5 &98.0 9.4 &\textbf{110.8} 0.8 &\textbf{110.6}	1.0\\ 
       & Walker2d & 22.1 44.9 &96.5 9.1 &107.9 1.6 &77.4 27.9 &\textbf{110.1} 0.5 &\textbf{110.1} 0.3 &\textbf{109.8}	0.5\\
       \hline
        \multirow{3}{*}{\rotatebox{90}{Expert}} 
        & HalfCheetah & 92.7 0.6 &92.9 0.6 &96.3 1.3 &81.3 21.8 &96.7 1.1 &92.4 1.7 &\textbf{97.4}	0.9\\
        & Hopper & 54.6 21.0 &110.5 0.5 &96.5 28.0 &62.5 29.0 &107.8 7 &\textbf{110.5} 0.4 &\textbf{110.8}	0.4\\ 
       & Walker2d & 106.6 6.8 &108.4 0.4 &108.5 0.5 &62.4 3.2 &\textbf{110.2} 0.3 &108.3 0.3 &108.3	0.2\\
       \hline 
       & Average & 38.78 10.0 &50.41 2.7 &67.35 9.1 &53.3 16.3 &67.55 3.8 &74.37 5.3 &\textbf{78.2}	1.9\\
       \hline
    
    \end{tabular}
    }
    \caption{{Average normalized score and standard deviation of all algorithms over five seeds in the Gym domain in the ``v2'' dataset of D4RL \citep{DBLP:journals/corr/abs-2004-07219}. The scores for all the baselines are from Table 1 of \cite{bai2022pessimistic}. The highest scores are highlighted.}}
    \label{tab: d4rl result}
\end{table}
\color{black} \section{Extended Discussion}
\label{section: extended discussion}
Here we provide extended discussion of our result. 
\subsection{Comparison with other works and discussion}
\label{subsection: comparison with other works in details}

We provide further discussion regarding comparison with other works in the literature.  
\paragraph{Comparing to \cite{jin2021pessimism}.}
When the underlying MDP reduces into a linear MDP, if we use the linear model as the plug-in parametric model in Algorithm \ref{algorithm: PERVI},  our bound reduces into $\tilde{\mathcal{O}}\left(  \frac{\kappa H^{5/2}  d_{lin} }{\sqrt{K}} \right)$ which improves the bound $\tilde{\mathcal{O}}(d^{3/2}_{lin} H^2 / \sqrt{K})$ of PEVI \citep[Corollary~4.6]{jin2021pessimism} by a factor of $\sqrt{d_{lin}}$ and worsen by a factor of $\sqrt{H}$ due to the data splitting. Thus, our bound is more favorable in the linear MDPs with high-dimensional features. Moreover, our bound is guaranteed in more practical scenarios where the offline data can have been adaptively generated and is not required to uniformly cover the state-action space. The explicit bound $\tilde{\mathcal{O}}(d^{3/2}_{lin} H^2 / \sqrt{K})$ of PEVI \citep[Corollary~4.6]{jin2021pessimism} is obtained under the assumption that the offline data have uniform coverage and are generated independently on the episode basis. 

\paragraph{Comparing to \cite{yang2020function}.} Though \citet{yang2020function} work in the online regime, it shares some part of the literature with our work in function approximation for RL. Besides different learning regimes (offline versus online), we offer three key distinctions which can potentially be used in the online regime as well: (i) perturbed rewards, (ii) optimization, and (iii) data split. Regarding (i), our perturbed reward design can be applied to online RL with function approximation to obtain a provably efficient online RL that is computationally efficient and thus remove the need of maintaining explicit confidence regions and performing the inverse of a large covariance matrix. Regarding (ii), we incorporate the optimization analysis into our algorithm which makes our algorithm and analysis more practical. We also note that unlike \citep{yang2020function}, we do not make any assumption on the eigenvalue decay rate of the empirical NTK kernel as the empirical NTK kernel is data-dependent. Regarding (iii), our data split technique completely removes the factor $\sqrt{ \log \mathcal{N}_{\infty}(\mathcal{H}, 1/K, B)}$ in the bound at the expense of increasing the bound by a factor of $\sqrt{H}$. In complex models, such log covering number can be excessively larger than the horizon $H$, making the algorithm too optimistic in the online regime (optimistic in the offline regime, respectively). For example, the target function class is RKHS with a $\gamma$-polynomial decay, the log covering number scales as \citep[Lemma~D1]{yang2020function},
\begin{align*}
    \sqrt{\log \mathcal{N}_{\infty}(\mathcal{H}, 1/K, B)} \lesssim  K ^{\frac{2}{\alpha \gamma -1}},
\end{align*}
for some $\alpha \in (0,1)$. In the case of two-layer ReLU NTK, $\gamma = d$ \citep{bietti2019inductive}, thus $\sqrt{\log \mathcal{N}_{\infty}(\mathcal{H}, 1/K, B)} \lesssim  K ^{\frac{2}{\alpha d -1}}$ which is much larger than $\sqrt{H}$ when the size of dataset is large. Note that our data-splitting technique is general that can be used in the online regime as well. 

\paragraph{Comparing to \cite{xu2022provably}.} \citet{xu2022provably} consider a different setting where per-timestep rewards are not available and only the total reward of the whole trajectory is given. Used with neural function approximation, they obtain $\tilde{\mathcal{O}}(D_{\textrm{eff}} H^2 / \sqrt{K})$ where $D_{\textrm{eff}}$ is their effective dimension. Note that \citet{xu2022provably} do not use data splitting and still achieve the same order of $D_{\textrm{eff}}$ as our result with data splitting. It at first might appear that our bound is inferior to their bound as we pay the cost of $\sqrt{H}$ due to data splitting. However, to obtain that bound, they make three critical assumptions: (i) the offline data trajectories are independently and identically distributed (i.i.d.) (see their Assumption 3), (ii) the offline data is uniformly explorative over all dimensions of the feature space (also see their Assumption 3), and (iii) the eigenfunctions of the induced NTK RKHS has finite spectrum (see their Assumption 4). The i.i.d. assumption under the RKHS space with finite dimensions (due to the finite spectrum assumption) and the well-explored dataset is critical in their proof to use a matrix concentration that does not incur an extra factor of $\sqrt{D_{\textrm{eff}}}$ as it would normally do without these assumptions (see Section E, the proof of their Lemma 2). Note that the celebrated ReLU NTK does not satisfy the finite spectrum assumption \citep{bietti2019inductive}. Moreover, we do not make any of these three assumptions above for our bound to hold. That suggests that our bound is much more general. In addition, we do not need to compute any confidence regions nor perform the inverse of a large covariance matrix. 

\paragraph{Comparing to \cite{yin2022offline}.} During the submission of our work, a concurrent work of \cite{yin2022offline} appeared online. \citet{yin2022offline} study provably efficient offline RL with a general parametric function approximation that unifies the guarantees of offline RL in linear and generalized linear MDPs, and beyond with potential applications to other classes of functions in practice. We remark that the result in \cite{yin2022offline} is orthogonal/complementary to our paper since they consider the parametric class with third-time differentiability which cannot apply to neural networks (not necessarily overparameterized) with non-smooth activation such as ReLU. In addition, they do not consider reward perturbing in their algorithmic design or optimization errors in their analysis. 

\color{black}




\subsection{Worse-Case Rate of Effective Dimension}
In the main paper, we prove an $\tilde{\mathcal{O}}\left(  \frac{ \kappa H^{5/2}  \tilde{d} }{\sqrt{K}} \right)$ sub-optimality bound which depends on the notion of effective dimension defined in Definition \ref{definition: effective dimension}. Here we give a worst-case rate of the effective dimension $\tilde{d}$ for the two-layer ReLU NTK. We first briefly review the background of RKHS. 


Let $\mathcal{H}$ be an RKHS defined on $\mathcal{X} \subseteq \mathbb{R}^d$ with kernel function $\rho: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$. Let $\langle \cdot, \cdot \rangle_{\mathcal{H}}: \mathcal{H} \times \mathcal{H} \rightarrow \mathbb{R}$ and $\| \cdot \|_{\mathcal{H}}: \mathcal{H} \rightarrow \mathbb{R}$ be the inner product and the RKSH norm on $\mathcal{H}$. By the reproducing kernel property of $\mathcal{H}$, there exists a feature mapping $\phi: \mathcal{X} \rightarrow \mathcal{H}$ such that $f(x) = \langle f, \phi(x) \rangle_{\mathcal{H}}$ and $\rho(x,x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}}$. We assume that the kernel function $\rho$ is uniformly bounded, i.e. $\sup_{x \in \mathcal{X}} \rho(x,x) < \infty$. Let $\mathcal{L}^2(\mathcal{X})$ be the space of square-integral functions on $\mathcal{X}$ with respect to the Lebesgue measure and let $\langle \cdot, \cdot \rangle_{\mathcal{L}^2}$ be the inner product on $\mathcal{L}^2(\mathcal{X})$. The kernel function $\rho$ induces an integral operator $T_{\rho}:\mathcal{L}^2(\mathcal{X}) \rightarrow \mathcal{L}^2(\mathcal{X}) $ defined as 
\begin{align*}
    T_{\rho} f(x) = \int_{\mathcal{X}} \rho(x,x') f(x') dx'. 
\end{align*}

By Mercer's theorem \citep{steinwart2008support}, $T_{\rho}$ has countable and positive eigenvalues $\{\lambda_i\}_{i \geq 1}$ and eigenfunctions $\{\nu_i\}_{i \geq 1}$. The kernel function and $\mathcal{H}$ can be expressed as 
\begin{align*}
    \rho(x,x') &= \sum_{i=1}^{\infty} \lambda_i \nu_i(x) \nu_i(x'), \\ 
    \mathcal{H} &= \{f \in \mathcal{L}^2(\mathcal{X}): \sum_{i=1}^{\infty} \frac{\langle f, \nu_i \rangle_{\mathcal{L}^2}}{ \lambda_i} < \infty\}. 
\end{align*}

Now consider the NTK defined in Definition \ref{definition: ntk}: 
\begin{align*}
    K_{ntk}(x,x') = \mathbb{E}_{w \sim \mathcal{N}(0, I_d/d)} \langle x \sigma'(w^T x), x' \sigma'(w^T x') \rangle. 
\end{align*}

It follows from \cite[Proposition~1]{bietti2019inductive} that $\lambda_i \asymp i^{-d} $. Thus, by \cite[Theorem~5]{srinivas2009gaussian}, the data-dependent effective dimension of $\mathcal{H}_{ntk}$ can be bounded in the worst case by  
\begin{align*}
    \tilde{d} \lesssim K'^{(d+1) / (2d)} . 
\end{align*}

We remark that this is the worst-case bound that considers uniformly over all possible realizable of training data. The effective dimension $\tilde{d}$ is on the other hand data-dependent, i.e. its value depends on the specific training data at hand thus $\tilde{d}$ can be actually much smaller than the worst-case rate. 







 \section{Proof of Theorem \ref{theorem: main theorem} and Theorem \ref{theorem: explicit bound}}
\begin{table}
    \centering
    \def\arraystretch{2.2}\resizebox{\textwidth}{!}{
    \begin{tabular}{cc}
        \hline
        Parameters & Meaning/Expression \\ 
        \hline
        \hline
        $m$ & Network width \\
        $\lambda$ & Regularization parameter \\ 
        $\eta$ & Learning rate \\ 
        $M $ & Number of bootstraps \\ 
        $\{\sigma_h\}_{h \in [H]}$ & Noise variances \\ 
        $J$ & Number of GD steps \\
        $\psi$ & Cutoff margin \\
        $K$ & Number of offline episodes \\
        $R$ & Radius parameter \\
        $\delta$ & Failure level \\ 
        $K'$ & bucket size, $K/H$ \\ 
        $\mathcal{I}_h$ & index buckets, $[(H-h)K' + 1, (H-h)K' +2, \ldots, (H - h + 1)K']$ \\ 
        $B$ & Parameter radius of the Bellman operator \\
        \hline
        $\gamma_{h,1}$ & $c_1 \sigma_h \sqrt{\log (KM / \delta)}$  \\
        $\gamma_{h,2}$ & $ c_2 \sigma_h \sqrt{d \log (d KM / \delta)}$ \\ 
$B_1$ &  $\lambda^{-1} \sqrt{   2K(H + \psi )^2  + 8 C_g R^{4/3} m^{-1/6} \sqrt{ \log m}} \sqrt{K} C_g R^{1/3} m^{-1/6} \sqrt{ \log m} $ \\
$\tilde{B}_1$ & $\lambda^{-1} \sqrt{   2K'(H + \psi + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2 + 8 C_g R^{4/3} m^{-1/6} \sqrt{ \log m}} \sqrt{K'} C_g R^{1/3} m^{-1/6} \sqrt{ \log m}$ \\
$\tilde{B}_2$ &  $ \lambda^{-1} K' C_g R^{4/3} m^{-1/6} \sqrt{\log m}$ \\
$\iota_0$ & $B m^{-1/2} (2 \sqrt{d} + \sqrt{2 \log (3H / \delta)}) $  \\ 
        $\iota_1$ & $ C_g R^{4/3} m^{-1/6} \sqrt{ \log m} + C_g \left( \tilde{B}_1 + \tilde{B}_2 + \lambda^{-1}(1 - \eta \lambda)^J \left( K'(H + \psi + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2\right) \right) $ \\
$\iota_2$ &  $C_g R^{4/3} m^{-1/6} \sqrt{ \log m} + C_g \left( B_1 + \tilde{B}_2 + \lambda^{-1}(1 - \eta \lambda)^J  K'(H + \psi  )^2 \right) $\\
$\iota$ & $\iota_0 + \iota_1 + 2 \iota_2$ \\ 
        \multirow{ 2}{*}{$\beta$} & $  \frac{B K' }{\sqrt{m}} (2 \sqrt{d} + \sqrt{2 \log ( 3H /\ \delta)}) \lambda^{-1/2} C_g + \lambda^{1/2}  B $ \\
        & $+ (H + \psi) \left[ \sqrt{\tilde{d}_h \log (1 + \frac{K'}  {\lambda }) + K' \log \lambda + 2 \log (3 H / \delta)} \right] $ \\
\end{tabular}
    }
    \caption{The problem parameters and the additional parameters that we introduce for our proofs. Here $c_1$, $c_2$, and $C_g$ are some absolute constants independent of the problem parameters.}
    \label{tab: key parameters in the proofs}
\end{table}



In this section, we provide both the outline and detailed proofs of Theorem \ref{theorem: main theorem} and Theorem \ref{theorem: explicit bound}. 

\subsection{Technical Review and Proof Overview}
\label{subsection: technical review}


\paragraph{Technical Review.} In what follows, we provide more detailed discussion when placing our technical contribution in the context of the related literature. Our technical result starts with the value difference lemma in \cite{jin2021pessimism} to connect bounding the suboptimality of an offline algorithm to controlling the uncertainty quantification in the value estimates. Thus, our key technical contribution is to provably quantify the uncertainty of the perturbed value function estimates which were obtained via reward perturbing and gradient descent. This problem setting is largely different from the current analysis of overparameterized neural networks for supervised learning which does not require uncertainty quantification. 

Our work is not the first to consider uncertainty quantification with overparameterized neural networks, since it has been studied in \cite{zhou2020neural,nguyen2021offline,jia2022learning}. However, there are significant technical differences between our work and these works. The work in \cite{zhou2020neural,nguyen2021offline} considers contextual bandits with overparameterized neural networks trained by (S)GD and quantifies the uncertainty of the value function with explicit empirical covariance matrices. We consider general MDP and use reward perturbing to implicitly obtain uncertainty, thus requiring different proof techniques.

\citet{jia2022learning} is more related to our work since they consider reward perturbing with overparameterized neural networks (but they consider contextual bandits). However, our reward perturbing strategy is largely different from that in \cite{jia2022learning}. Specifically, \citet{jia2022learning} perturbs each reward only once while we perturb each reward multiple times, where the number of perturbing times is crucial in our work and needs to be controlled carefully. We show in Theorem \ref{theorem: main theorem} that our reward perturbing strategy is effective in enforcing sufficient pessimism for offline learning in general MDP and the empirical results in Figure \ref{fig: linear mdp}, Figure \ref{fig: neural contextuable bandits}, Figure \ref{fig: subopt vs M}, and Table \ref{tab: d4rl result} are strongly consistent with our theoretical suggestion. Thus, our technical proofs are largely different from those of \citet{jia2022learning}. 

Finally, the idea of perturbing rewards multiple times in our algorithm is inspired by \cite{ishfaq2021randomized}. However, \citet{ishfaq2021randomized} consider reward perturbing for obtaining optimism in online RL. While perturbing rewards are intuitive to obtain optimism for online RL, for offline RL, under distributional shift, it can be paradoxically difficult to properly obtain pessimism with randomization and ensemble \citep{ghasemipour2022so}, especially with neural function approximation. We show affirmatively in our work that simply taking the minimum of the randomized value functions after perturbing rewards multiple times is sufficient to obtain provable pessimism for offline RL. In addition, \citet{ishfaq2021randomized} do not consider neural network function approximation and optimization. Controlling the uncertainty of randomization (via reward perturbing) under neural networks with extra optimization errors induced by gradient descent sets our technical proof significantly apart from that of \cite{ishfaq2021randomized}. 

Besides all these differences, in this work, we propose an intricately-designed data splitting technique that avoids the uniform convergence argument and could be of independent interest for studying sample-efficient RL with complex function approximation.

\paragraph{Proof Overview.} The key steps for proving Theorem \ref{theorem: main theorem} and Theorem \ref{theorem: explicit bound} are highlighted in Subsection \ref{subsection: proof of theorem 1} and Subsection \ref{susbection: proof of theorem 2}, respectively. Here, we discuss an overview of our proof strategy. The key technical challenge in our proof is to quantify the uncertainty of the perturbed value function estimates. To deal with this, we carefully control both the near-linearity of neural networks in the NTK regime and the estimation error induced by reward perturbing. A key result that we use to control the linear approximation to the value function estimates is Lemma \ref{lemma: linear approximation of neural functions}. The technical challenge in establishing Lemma \ref{lemma: linear approximation of neural functions} is how to carefully control and propagate the optimization error incurred by gradient descent. The complete proof of Lemma \ref{lemma: linear approximation of neural functions} is provided in Section \ref{subsection: proof of linear approximation lemma}. 

The implicit uncertainty quantifier induced by the reward perturbing is established in Lemma \ref{lemma: bound ERM with the Bellman target} and Lemma \ref{Lemma: anti-concentration of tilde Q}, where we carefully design a series of intricate auxiliary loss functions and establish the anti-concentrability of the perturbed value function estimates. This requires a careful design of the variance of the noises injected into the rewards. 

To deal with removing a potentially large covering number when we quantify the implicit uncertainty, we propose our data splitting technique which is validated in the proof of Lemma \ref{lemma: bound ERM with the Bellman target} in Section \ref{subsection: proof of lemma D1}.  Moreover, establishing Lemma \ref{lemma: bound ERM with the Bellman target} in the overparameterization regime induces an additional challenge since a standard analysis would result in a vacuous bound that scales with the overparameterization. We avoid this issue by carefully incorporating the use of the effective dimension in Lemma \ref{lemma: bound ERM with the Bellman target}.  

 






\color{black}

\subsection{Proof of Theorem \ref{theorem: main theorem}}
\label{subsection: proof of theorem 1}
In this subsection, we present the proof of Theorem \ref{theorem: main theorem}. We first decompose the suboptimality $\subopt(\tilde{\pi}; s)$ and present the main lemmas to bound the evaluation error and the summation of the implicit confidence terms, respectively. The detailed proof of these lemmas are deferred to Section \ref{section: proof of main lemma about evaluation error}. For proof convenience,  we first provide the key parameters that we use consistently throughout our proofs in Table \ref{tab: key parameters in the proofs}. 


We define the model evaluation error at any $(x,h) \in \mathcal{X} \times [H]$ as 
\begin{align}
    \err_h(x) = (\mathbb{B}_h \tilde{V}_{h+1} - \tilde{Q}_h)(x),
    \label{equation: model evaluation error}
\end{align}
where $\mathbb{B}_h$ is the Bellman operator defined in Section \ref{section: preliminary}, and $\tilde{V}_h$ and $\tilde{Q}_h$ are the estimated (action-) state value functions returned by Algorithm \ref{algorithm: PERVI}. Using the standard suboptimality decomposition \citep[Lemma~3.1]{jin2021pessimism}, for any $s_1 \in \mathcal{S}$,  



\begin{align*}
    \subopt(\tilde{\pi}; s_1) &= - \sum_{h=1}^H \mathbb{E}_{\tilde{\pi}} \left[ \err_h(s_h, a_h) \right] +  \sum_{h=1}^H \mathbb{E}_{\pi^*} \left[ \err_h(s_h, a_h) \right] \\
    &+ \sum_{h=1}^H \mathbb{E}_{\pi^*}  \underbrace{ \left[\langle \tilde{Q}_h(s_h, \cdot), \pi^*_h(\cdot | s_h) - \tilde{\pi}_h(\cdot | s_h)
    \rangle_{\mathcal{A}} \right]}_{\leq 0} ,
\end{align*}
where the third term is non-positive as $\tilde{\pi}_h$ is greedy with respect to $\tilde{Q}_h$. Thus, for any $s_1 \in \mathcal{S}$, we have 
\begin{align}
    \subopt(\tilde{\pi}; s_1) &\leq - \sum_{h=1}^H \mathbb{E}_{\tilde{\pi}} \left[ \err_h(s_h, a_h) \right] +  \sum_{h=1}^H \mathbb{E}_{\pi^*} \left[ \err_h(s_h, a_h) \right]. \label{eq: decompose subopt simplified}
\end{align}

In the following main lemma, we bound the evaluation error $err_h(s,a)$. In the rest of the proof, we consider an additional parameter $R$ and fix any $\delta \in (0,1)$. 
\begin{lemma}
Let 
\begin{align}
\begin{cases}
m = \Omega \left( d^{3/2}  R^{-1} \log^{3/2} (\sqrt{m} /R) \right) \\
    R = \mathcal{O} \left( m^{1/2} \log^{-3} m \right), \\ 
m = \Omega \left( K'^{10} (H + \psi)^2 \log(3 K'H/\delta)\right) \\ 
    \lambda > 1 \\ 
    K' C_g^2 \geq \lambda 
R \geq \max\{ 4 \tilde{B}_1, 4 \tilde{B}_2, 2 \sqrt{2\lambda^{-1}K' (H + \psi + \gamma_{h,1})^2 + 4 \gamma_{h,2}^2} \}, \\
      \eta \leq (\lambda + K' C_g^2)^{-1}, \\
     \psi > \iota, \\
     \sigma_h \geq \beta, \forall h \in [H],
\end{cases}
\label{equation: conditions for R and eta final version and condition for psi and sigma}
\end{align}
where $\tilde{B}_1$, $\tilde{B}_2$, $\gamma_{h,1}$, $\gamma_{h,2}$, and $\iota$ are defined in Table \ref{tab: key parameters in the proofs}, $C_g$ is a absolute constant given in Lemma \ref{lemma: NTK approximation error}, and $R$ is an additional parameter.
Let $M = \log \frac{HSA}{\delta} / \log \frac{1}{1 - \Phi(-1)}$ where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal distribution. With probability at least $1 -  MH m^{-2} - 2 \delta$, for any $(x,h) \in \mathcal{X} \times [H]$, we have 
\begin{align*}
    -\iota \leq \err_h(x) \leq \sigma_h(1 +  \sqrt{2 \log (M SAH / \delta)}  ) \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota 
\end{align*}
where $\Lambda_h := \lambda I_{md} + \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) g(x^k_h; W_0)^T \in \mathbb{R}^{md \times md }$. 
\label{lemma: bound of the evalulation error - main lemma}
\end{lemma}


Now we can prove Theorem \ref{theorem: main theorem}. 
\begin{proof}[Proof of Theorem \ref{theorem: main theorem}]
Theorem \ref{theorem: main theorem} can directly follow from substituting Lemma \ref{lemma: bound of the evalulation error - main lemma} into Equation (\ref{eq: decompose subopt simplified}). We now only need to simplify the conditions in Equation (\ref{equation: conditions for R and eta final version and condition for psi and sigma}). To satisfy Equation (\ref{equation: conditions for R and eta final version and condition for psi and sigma}), it suffices to set
\begin{align*}
    \begin{cases}
    \lambda = 1 + \frac{H}{K} \\ 
    \psi = 1 >  \iota \\ 
    \sigma_h = \beta \\
    8 C_g R^{4/3} m^{-1/6} \sqrt{ \log m} \leq 1 \\
    \lambda^{-1}K' H^2 \geq 2 \\ 
    \tilde{B}_1  \leq \sqrt{   2K'(H + \psi + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2 + 1} \sqrt{K'} C_g R^{1/3} m^{-1/6} \sqrt{ \log m} \leq 1 \\ 
    \tilde{B}_2 \leq K' C_g R^{4/3} m^{-1/6} \sqrt{\log m} \leq 1.
    \end{cases}
\end{align*}

Combining with Equation \ref{equation: conditions for R and eta final version and condition for psi and sigma}, we have
\begin{align}
\begin{cases}
    \lambda = 1 + \frac{H}{K} \\ 
    \psi = 1 > \iota \\ 
    \sigma_h = \beta \\
    \eta \lesssim (\lambda + K')^{-1} \\
    m \gtrsim \max\left\{ R^8 \log^3 m, K'^{10} (H + 1)^2 \log(3 K'H/\delta), d^{3/2}  R^{-1} \log^{3/2} (\sqrt{m} /R), K'^6  R^8 \log^3 m  \right\} \\ 
    m \gtrsim [2K'(H + 1 + \beta \sqrt{\log (K' M / \delta)} )^2 + \lambda \beta^2 d \log (d K' M / \delta) + 1]^3 K'^3 R \log^3 m \\ 
      4 \sqrt{K'} (H + 1 + \beta \sqrt{\log (K' M / \delta)}) +  4 \beta \sqrt{ d \log (d K' M / \delta)} \leq R \lesssim K'.  
\end{cases}
\label{eq: final parameter conditions simplified}
\end{align}

Note that with the above choice of $\lambda = 1 + \frac{H}{K}$, we have \begin{align*}
    K' \log \lambda = \log (1 + \frac{1}{K'})^{K'} \leq \log 3 < 2.
\end{align*}
We further set that $ m \gtrsim B^2 K'^2 d \log (3 H / \delta)$, we have
\begin{align*}
    \beta &= \frac{B K' }{\sqrt{m}} (2 \sqrt{d} + \sqrt{2 \log ( 3H /\ \delta)}) \lambda^{-1/2} C_g + \lambda^{1/2}  B \\
    &+ (H + \psi) \left[ \sqrt{\tilde{d}_h \log (1 + \frac{K'}  {\lambda }) + K' \log \lambda + 2 \log (3 H / \delta)} \right] \\ 
    &\leq 1 + \lambda^{1/2}  B + (H + 1) \left[ \sqrt{\tilde{d}_h \log (1 + \frac{K'}  {\lambda }) + 2 + 2 \log (3 H / \delta)} \right] = o(\sqrt{K'}).
\end{align*}
Thus, 
\begin{align*}
     4 \sqrt{K'} (H + 1 + \beta \sqrt{\log (K' M / \delta)}) +  4 \beta \sqrt{ d \log (d K' M / \delta)} << K'
\end{align*}
for $K'$ large enough. Therefore, there exists $R$ that satisfies Equation (\ref{eq: final parameter conditions simplified}). We now only need to verify $\iota < 1$. We have 
\begin{align*}
    \iota_0 = B m^{-1/2} (2 \sqrt{d} + \sqrt{2 \log (3H / \delta)}) \leq 1/3,
\end{align*}
\begin{align*}
    \iota_1 &= C_g R^{4/3} m^{-1/6} \sqrt{ \log m} + C_g \left( \tilde{B}_1 + \tilde{B}_2 + \lambda^{-1}(1 - \eta \lambda)^J \left( K'(H + 1 + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2\right) \right) \lesssim 1/3
\end{align*}
if 
\begin{align}
    (1 - \eta \lambda)^J  \left[ K'(H + 1 + \beta \sqrt{\log (K' M / \delta)} )^2 + \lambda  \beta^2 d \log (d K' M / \delta) \right] \lesssim 1. 
    \label{eq: constraint iota 1}
\end{align}
Note that 
\begin{align*}
    (1 - \eta \lambda)^J &\leq e^{-\eta \lambda J}, \\ 
    K'(H + 1 + \beta \sqrt{\log (K' M / \delta)} )^2 + \lambda  \beta^2 d \log (d K' M / \delta) &\lesssim K' H^2 \lambda  \beta^2 d \log (d K' M / \delta).
\end{align*}
Thus, Equation (\ref{eq: constraint iota 1}) is satisfied if 
\begin{align*}
    J \gtrsim \eta \lambda \log\left( K' H^2 \lambda  \beta^2 d \log (d K' M / \delta) \right). 
\end{align*}
Finally note that $\iota_2 \leq \iota_1$. Rearranging the derived conditions here gives the complete parameter conditions in Theorem \ref{theorem: main theorem}. Specifically, the polynomial form of $m$ is 
$m \gtrsim  \max\{R^8 \log^3 m, K'^{10} (H + 1)^2 \log(3 K'H/\delta), d^{3/2}  R^{-1} \log^{3/2} (\sqrt{m} /R),  K'^6  R^8 \log^3 m$, $B^2 K'^2 d \log (3 H / \delta) \} $, $ m \gtrsim  [2K'(H + 1 + \beta \sqrt{\log (K' M / \delta)} )^2 + \lambda \beta^2 d \log (d K' M / \delta) + 1]^3 K'^3 R \log^3 m$.







\end{proof}

\subsection{Proof of Theorem \ref{theorem: explicit bound}}
\label{susbection: proof of theorem 2}
In this subsection, we give a detailed proof of Theorem \ref{theorem: explicit bound}. We first present intermediate lemmas whose proofs are deferred to Section \ref{section: proof of main lemma about evaluation error}. For any $h \in [H]$ and $k  \in \mathcal{I}_h = [(H - h) K' + 1, \ldots, (H - h + 1) K']$, we define the filtration 
\begin{align*}
    \mathcal{F}^k_h = \sigma \left(\{(s^t_{h'}, a^t_{h'}, r^t_{h'})\}_{h' \in [H]}^{t \leq k} \cup \{(s^{k+1}_{h'}, a^{k+1}_{h'}, r^{k+1}_{h'})\}_{h' \leq h-1} \cup \{(s^{k+1}_h, a^{k+1}_h)\} \right).
\end{align*}
Let
\begin{align*}
    \Lambda_h^k &:= \lambda I + \sum_{t \in \mathcal{I}_k, t \leq k} g(x^t_h; W_0) g(x^t_h; W_0)^T, \\
    \tilde{\beta} &:= \beta (1 + 2 \sqrt{\log (SAH / \delta)}).
\end{align*}




In the following lemma, we connect the expected sub-optimality of $\tilde{\pi}$ to the summation of the uncertainty quantifier at empirical data. 
\begin{lemma}
Suppose that the conditions in Theorem \ref{theorem: main theorem} all hold. With probability at least $1 - MH m^{-2} - 3 \delta$,
\begin{align*}
   \subopt(\tilde{\pi}) &\leq \frac{2 \tilde{\beta} }{K'}    \sum_{h=1}^H \sum_{k \in \mathcal{I}_h} \mathbb{E}_{\pi^*}  \left[ \| g(x_h; W_0) \|_{(\Lambda_h^k)^{-1}} \bigg| \mathcal{F}^{k-1}_h, s_1^k  \right]  + \frac{16}{3 K'} H \log (\log_2(K'H)/\delta) \\
   &+ \frac{2}{K'} +  2\iota,
\end{align*}
\label{lemma: expected subopt to empirical confidence quantifiers}
\end{lemma}

\begin{lemma}
Under Assumption \ref{assumption: OPC}, for any $h \in [H]$ and fixed $W_0$, with probability at least $1 - \delta$,
\begin{align*}
    \sum_{k \in \mathcal{I}_h} \mathbb{E}_{\pi^*} \left[ \| g(x_h; W_0) \|_{(\Lambda_h^k)^{-1}} \bigg| \mathcal{F}_{k-1}, s_1^k  \right] &\leq \sum_{k \in \mathcal{I}_h} \kappa \| g(x_h; W_0)  \|_{(\Lambda^k_h)^{-1}} + 
    \kappa \sqrt{\frac{K' \log(1/\delta)}{\lambda}}.
\end{align*}
\label{lemma:sum_sample_subopt}
\end{lemma}

\begin{lemma}
If $\lambda \geq C_g^2$ and $m = \Omega(K'^4 \log (K'H /\delta))$, then with probability at least $1 - \delta$, for any $h \in [H]$, we have 
\begin{align*}
     \sum_{k \in \mathcal{I}_h} \| g(x_h; W_0)  \|_{(\Lambda^k_h)^{-1}}^2 &\leq 2\tilde{d}_h \log(1 + K' / \lambda) + 1. 
\end{align*}
where $\tilde{d}_h$ is the effective dimension defined in Definition \ref{definition: effective dimension}. 
\label{lemma: bound the summation of the uncertainty quantifier}
\end{lemma}




\begin{proof}[Proof of Theorem \ref{theorem: explicit bound}]
Theorem \ref{theorem: explicit bound} directly follows from Lemma \ref{lemma: expected subopt to empirical confidence quantifiers}-\ref{lemma:sum_sample_subopt}-\ref{lemma: bound the summation of the uncertainty quantifier} using the union bound. 
\end{proof}
 \section{Proof of Lemma \ref{lemma: bound of the evalulation error - main lemma}}
\label{section: proof of main lemma about evaluation error}

In this section, we provide the proof for Lemma \ref{lemma: bound of the evalulation error - main lemma}. We set up preparation for all the results in the rest of the paper and provide intermediate lemmas that we use to prove Lemma \ref{lemma: bound of the evalulation error - main lemma}. The detailed proofs of these intermediate lemmas are deferred to Section \ref{section: proofs of intermediate lemmas for the main lemmas}. 

\subsection{Preparation}
To prepare for the lemmas and proofs in the rest of the paper, we define the following quantities. Recall that we use abbreviation $x = (s,a) \in \mathcal{X} \subset \mathbb{S}_{d-1}$ and $x^k_h = (s^k_h, a^k_h) \in \mathcal{X} \subset \mathbb{S}_{d-1}$. For any $h \in [H]$ and $i \in [M]$, we define the perturbed loss function
\begin{align}
    \tilde{L}_h^i(W) &:= \frac{1}{2}\sum_{k \in \mathcal{I}_h} \left( f(x^k_h; W) - \tilde{y}^{i,k}_h ) \right)^2 + \frac{\lambda}{2} \| W + \zeta^i_h - W_0 \|_2^2, 
    \label{eq: perturbed loss function}
\end{align}
where
\begin{align*}
    \tilde{y}^{i,k}_h &:= r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{i,k}_h,
\end{align*}
$\tilde{V}_{h+1}$ is computed by Algorithm \ref{algorithm: PERVI} at Line \ref{PERVI: greedy policy} for timestep $h+1$, and $\{\xi^{i,k}_h\}$ and $\zeta^i_h$ are the Gaussian noises obtained at Line \ref{PERVI: sample noises} of Algorithm \ref{algorithm: PERVI}. 

Here the subscript $h$ and the superscript $i$ in $\tilde{L}_h^i(W)$ emphasize the dependence on the ensemble sample $i$ and timestep $h$. The gradient descent update rule of $\tilde{L}^i_h(W)$ is
\begin{align}
    \tilde{W}^{i,(j+1)}_h = \tilde{W}^{i,(j)}_h - \eta \nabla \tilde{L}^i_h(W),
    \label{eq: GD update in non-linear case}
\end{align}
where $\tilde{W}^{i,(0)}_h = W_0$ is the initialization parameters. Note that 
\begin{align*}
    \tilde{W}^i_h = \textrm{GradientDescent}(\lambda, \eta, J, \tilde{\mathcal{D}}^i_h, \zeta^i_h, W_0) = \tilde{W}^{i,(J)}_h,
\end{align*}
where $\tilde{W}^i_h$ is returned by Line \ref{PERVI: GD} of Algorithm \ref{algorithm: PERVI}. We consider a non-perturbed auxiliary loss function 
\begin{align}
    L_h(W) &:= \frac{1}{2} \sum_{k \in \mathcal{I}_h} \left( f(x^k_h; W) - y^k_h ) \right)^2 + \frac{\lambda}{2} \| W - W_0  \|_2^2,
    \label{eq: non=perturbed loss function}
\end{align}
where 
\begin{align*}
y^k_h &:= r^k_h + \tilde{V}_{h+1}(s^k_{h+1}). 
\end{align*}
Note that $L_h(W)$ is simply a non-perturbed version of $\tilde{L}_h^i(W)$ where we drop all the noises $ \{\xi^{i,k}_h \}$ and $ \{ \zeta^i_h \}$. We consider the gradient update rule for $L_h(W)$ as follows
\begin{align}
    \hat{W}^{(j+1)}_h = \hat{W}^{(j)}_h - \eta \nabla L_h(W),
    \label{eq: GD update in non-linear case for non-perturbed loss}
\end{align}
where $\hat{W}^{(0)}_h = W_0$ is the initialization parameters. To correspond with $\tilde{W}^i_h$, we denote 
\begin{align}
    \hat{W}_h := \hat{W}_h^{(J)}. 
    \label{eq: hat W_h}
\end{align}

We also define the auxiliary loss functions for both non-perturbed and perturbed data in the linear model with feature $g(\cdot; W_0)$ as follows
\begin{align}
    \tilde{L}_h^{i, lin}(W) &:= \frac{1}{2} \sum_{k \in \mathcal{I}_h} \left( \langle g(x^k_h; W_0), W  \rangle -  \tilde{y}^{i,k}_h \right)^2 + \frac{\lambda}{2} \| W + \zeta^i_h - W_0 \|_2^2,
\end{align}
\begin{align}
    L_h^{lin}(W) &:= \frac{1}{2}\sum_{k \in \mathcal{I}_h} \left( \langle g(x^k_h; W_0), W  \rangle - y^k_h \right)^2 + \frac{\lambda}{2} \| W - W_0  \|_2^2. 
\end{align}


We consider the auxiliary gradient updates for $\tilde{L}^{i,lin}_h(W)$ as 
\begin{align}
    \tilde{W}^{i,lin,(j+1)}_h = \tilde{W}^{i,lin,(j)}_h - \eta \nabla \tilde{L}^{i,lin}_h(W),
    \label{eq: GD update in linear case}
\end{align}
\begin{align}
    \hat{W}^{lin,(j+1)}_h = \hat{W}^{lin,(j)}_h - \eta \nabla \tilde{L}^{lin}_h(W),
    \label{eq: GD update in linear case for non-perturbed data}
\end{align}
where $\tilde{W}^{i,lin,(0)}_h = \hat{W}^{i,lin,(0)}_h = W_0$ for all $i,h$. Finally, we  define the least-square solutions to the auxiliary perturbed and non-perturbed loss functions for the linear model as follows  
\begin{align}
\tilde{W}^{i,lin}_h &= \argmin_{W \in \mathbb{R}^{md}} \tilde{L}^{i,lin}_h(W),
    \label{eq: tilde W i lin h}
\end{align}
\begin{align}
    \hat{W}_h^{lin} &= \argmin_{W \in \mathbb{R}^{md}} L_h^{lin}(W). 
    \label{eq: hat W_h lin}
\end{align}

For any $h \in [H]$, we define the auxiliary covariance matrix $\Lambda_h$ as follows 
\begin{align}
    \Lambda_h &:= \lambda I_{md} + \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) g(x^k_h; W_0)^T. 
\end{align}

It is worth remarking that Algorithm \ref{algorithm: PERVI} only uses Equation (\ref{eq: perturbed loss function}) and (\ref{eq: GD update in non-linear case}) thus it does not actually require any of the auxiliary quantities defined in this subsection during its run time. The auxiliary quantities here are only for our theoretical analysis. 

\subsection{Proof of Lemma \ref{lemma: bound of the evalulation error - main lemma}}
In this subsection, we give detailed proof of Lemma \ref{lemma: bound of the evalulation error - main lemma}. To prepare for proving Lemma \ref{lemma: bound of the evalulation error - main lemma}, we first provide the following intermediate lemmas. The detailed proofs of these intermediate lemmas are deferred to Section \ref{section: proofs of intermediate lemmas for the main lemmas}. 


In the following lemma, we bound the uncertainty $f(x; \hat{W}_h)$ in estimating the Bellman operator at the estimated state-value function $\mathbb{B}_h \tilde{V}_{h+1}$. 
\begin{lemma}
Let 
\begin{align*}
\begin{cases}
    m = \Omega \left( K'^{10} (H + \psi)^2 \log(3 K'H/\delta)\right) \\ 
    \lambda > 1 \\ 
    K' C_g^2 \geq \lambda 
\end{cases}
\end{align*}



 With probability at least $ 1 - H m^{-2} - 2 \delta$, for any $x \in \mathbb{S}_{d-1}$, and any $h \in [H]$,
 \begin{align*}
    |f(x; \hat{W}_h) - (\mathbb{B}_h \tilde{V}_{h+1})(x)| 
    \leq \beta  \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_2 + \iota_0, 
\end{align*}
where $\tilde{V}_{h+1}$ is computed by Algorithm \ref{algorithm: PERVI} for timestep $h+1$, $\hat{W}_h$ is defined in Equation (\ref{eq: hat W_h}), and
$\beta$, $\iota_2$ and $\iota_0$ are defined in Table \ref{tab: key parameters in the proofs}. 
\label{lemma: bound ERM with the Bellman target}
\end{lemma}

In the following lemma, we establish the anti-concentration of $\tilde{Q}_h$. 
\begin{lemma}
Let 
\begin{align}
\begin{cases}
m = \Omega \left( d^{3/2}  R^{-1} \log^{3/2} (\sqrt{m} / R) \right) \\
      R = \mathcal{O} \left( m^{1/2} \log^{-3} m \right), \\ 
\eta \leq (\lambda + K' C_g^2)^{-1}, \\
R \geq \max\{ 4 \tilde{B}_1, 4 \tilde{B}_2, 2 \sqrt{2\lambda^{-1}K' (H + \psi + \gamma_{h,1})^2 + 4 \gamma_{h,2}^2} \},
\end{cases}
\label{equation: conditions for R and eta final version repeated}
\end{align}
where $\tilde{B}_1$, $\tilde{B}_2$, $\gamma_{h,1}$ and $\gamma_{h,2}$ are defined in Table \ref{tab: key parameters in the proofs}, and $C_g$ is a constant given in Lemma \ref{lemma: NTK approximation error}. Let $M = \log \frac{HSA}{\delta}/ \log \frac{1}{1 - \Phi(-1)}$ where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal distribution and $M$ is the number of bootstrapped samples in Algorithm \ref{algorithm: PERVI}. Then with probability $1 - MH m^{-2} - \delta$, for any $x \in \mathbb{S}_{d-1}$ and $h \in [H]$, \begin{align*}
\tilde{Q}_h(x) \leq \max\{\langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle  -\sigma_h \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_1 + \iota_2, 0\},
\end{align*}
where $\hat{W}^{lin}_h$ is defined in Equation (\ref{eq: hat W_h lin}), $\tilde{Q}_h$ is computed by Line \ref{PERVI: minimum of ensemble} of Algorithm \ref{algorithm: PERVI}, and $\iota_1$ and $\iota_2$ are defined in Table \ref{tab: key parameters in the proofs}. 
\label{Lemma: anti-concentration of tilde Q}
\end{lemma}


We prove the following linear approximation error lemma. 

\begin{lemma}
Let 
\begin{align}
\begin{cases}
m = \Omega \left( d^{3/2}  R^{-1} \log^{3/2} (\sqrt{m} / R) \right) \\
      R = \mathcal{O} \left( m^{1/2} \log^{-3} m \right), \\ 
\eta \leq (\lambda + K' C_g^2)^{-1}, \\
R \geq \max\{ 4 \tilde{B}_1, 4 \tilde{B}_2, 2 \sqrt{2\lambda^{-1}K' (H + \psi + \gamma_{h,1})^2 + 4 \gamma_{h,2}^2} \},
\end{cases}
\label{equation: conditions for R and eta final version}
\end{align}
where $\tilde{B}_1$, $\tilde{B}_2$, $\gamma_{h,1}$ and $\gamma_{h,2}$ are defined in Table \ref{tab: key parameters in the proofs}, and $C_g$ is a constant given in Lemma \ref{lemma: NTK approximation error}. 
With probability at least $1 - M H m^{-2} - \delta$, for any $(x, i,j,h)  \in \mathbb{S}_{d-1} \times [M] \times [J] \times [H]$, 
\begin{align*}
    |f(x; \tilde{W}^{i,(j)}_h) - \langle g(x; W_0), \tilde{W}_h^{i,lin} - W_0 \rangle| \leq \iota_1,
\end{align*}
where $ \tilde{W}^{i,(j)}_h$, $\tilde{W}_h^{i,lin}$, and $\iota_1$ are defined in Equation (\ref{eq: GD update in non-linear case}), Equation (\ref{eq: tilde W i lin h}), and Table \ref{tab: key parameters in the proofs}, respectively. 

In addition, with probability at least $1 -  H m^{-2}$, for any for any $(x,j,h)  \in \mathbb{S}_{d-1} \times [J] \times [H]$,
\begin{align*}
    |f(x; \hat{W}_h^{(j)}) - \langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle| \leq \iota_2,
\end{align*}
where $ \hat{W}^{(j)}_h$, $\hat{W}_h^{lin}$, and $\iota_2$ are defined in Equation (\ref{eq: GD update in non-linear case for non-perturbed loss}), Equation (\ref{eq: hat W_h lin}), and Table \ref{tab: key parameters in the proofs}, respectively. 
\label{lemma: linear approximation of neural functions}
\end{lemma}

We now can prove Lemma \ref{lemma: bound of the evalulation error - main lemma}. 
\begin{proof}[Proof of Lemma \ref{lemma: bound of the evalulation error - main lemma}]
Note that the first fourth conditions in Equation (\ref{equation: conditions for R and eta final version and condition for psi and sigma}) of Lemma \ref{lemma: bound of the evalulation error - main lemma} satisfy Equation (\ref{equation: conditions for R and eta final version}). Moreover, the event in which the inequality in Lemma \ref{lemma: linear approximation of neural functions} holds already implies the event in which the inequality in Lemma \ref{lemma: bound ERM with the Bellman target} holds (see the proofs of Lemma \ref{lemma: linear approximation of neural functions} and Lemma \ref{lemma: bound ERM with the Bellman target} in Section \ref{section: proof of main lemma about evaluation error}). Now in the rest of the proof, we consider the joint event in which both the inequality of Lemma \ref{lemma: linear approximation of neural functions} and that of Lemma \ref{lemma: bound ERM with the Bellman target} hold. Then, we also have the inequality in Lemma \ref{lemma: bound ERM with the Bellman target}. Consider any $x \in \mathcal{X}, h \in [H]$. 



It follows from Lemma \ref{lemma: bound ERM with the Bellman target} that
\begin{align}
     (\mathbb{B}_h \tilde{V}_{h+1})(x) \geq  f(x; \hat{W}_h) - \beta  \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} - \iota_0 - \iota_2.
     \label{eq: lower bound bellman op}
\end{align}

It follows from Lemma \ref{Lemma: anti-concentration of tilde Q} that 
\begin{align}
   \tilde{Q}_h(x) \leq \max\{\langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle  -\sigma_h \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_1 + \iota_2, 0\}.
\label{eq: bound tilde Q with max 0 and estimate}
\end{align}

Note that $\tilde{Q}_h(x) \geq 0$. If $\langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle  -\sigma_h \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_1 + \iota_2 \leq 0$, Equation (\ref{eq: bound tilde Q with max 0 and estimate}) implies that $\tilde{Q}_h(x) = 0$ and thus 
\begin{align*}
    err_h(x) &= (\mathbb{B}_h \tilde{V}_{h+1})(x) -\tilde{Q}_h(x) \\ 
    &= (\mathbb{B}_h \tilde{V}_{h+1})(x) \geq 0. 
\end{align*}

Otherwise, if $\langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle  -\sigma_h \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_1 + \iota_2 > 0$, 
Equation (\ref{eq: bound tilde Q with max 0 and estimate}) implies that
\begin{align}
   \tilde{Q}_h(x) \leq \langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle  -\sigma_h \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_1 + \iota_2.
   \label{eq: anti-concentration of Q tilde to linear}
\end{align}
Thus, combining Equation (\ref{eq: lower bound bellman op}), (\ref{eq: anti-concentration of Q tilde to linear})
and Lemma \ref{lemma: linear approximation of neural functions}, with the choice $\sigma_h \geq \beta$, we have
\begin{align*}
err_h(x) := (\mathbb{B}_h \tilde{V}_{h+1})(x) -\tilde{Q}_h(x) \geq  -(\iota_0 + \iota_1 + 2 \iota_2 ) = -\iota.
\end{align*}

As $\iota \geq 0$, in either case, we have 
\begin{align}
err_h(x) := (\mathbb{B}_h \tilde{V}_{h+1})(x) -\tilde{Q}_h(x) \geq -\iota.
   \label{equation: estimated Q upper bounded by Bellman operator}
\end{align}

Note that due to Equation (\ref{equation: estimated Q upper bounded by Bellman operator}), we have
\begin{align*}
    \tilde{Q}_h(x) \leq (\mathbb{B}_h \tilde{V}_{h+1})(x)  + \iota \leq H - h + 1 + \iota < H - h + 1 + \psi,
\end{align*}
where the last inequality holds due to the choice $\psi > \iota$. Thus, we have 
\begin{align}
    \tilde{Q}_h(x) =  \min\{ \min_{i \in [M]}  f(x; \tilde{W}^i_h), H-h+1 + \psi \}^+ = \max\{ \min_{i \in [M]}  f(x; \tilde{W}^i_h), 0 \}. 
    \label{eq: simplify the bellman target by removing the max value}
\end{align}

Substituting Equation (\ref{eq: simplify the bellman target by removing the max value}) into the definition of $err_h(x)$, we have
\begin{align*}
    err_h(x) &= (\mathbb{B}_h \tilde{V}_{h+1})(x) -\tilde{Q}_h(x) \\ 
    &\leq (\mathbb{B}_h \tilde{V}_{h+1})(x) - \min_{i \in [M]} f(x; \tilde{W}^i_h) \\ 
    &= (\mathbb{B}_h \tilde{V}_{h+1})(x) - f(x; \hat{W}_h) + f(x; \hat{W}_h)  -\min_{i \in [M]} f(x; \tilde{W}^i_h) \\ 
    &\leq  \beta  \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_0 + \iota_2 + f(x; \hat{W}_h)  -\min_{i \in [M]} f(x; \tilde{W}^i_h) \\ 
    &\leq  \beta  \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_0 + \iota_2 + \langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle + \iota_2 \\
    &- \min_{i \in [M]}\langle g(x; W_0), \tilde{W}_h^{i,lin} - W_0 \rangle + \iota_1 \\ 
    &=  \beta  \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_0 + \iota_2 + \max_{i \in [M]}\langle g(x; W_0), \hat{W}_h^{lin} - \tilde{W}_h^{i,lin} \rangle  + \iota_1 + \iota_2 \\ 
    &\leq  \beta  \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_0 + \iota_2 + \sqrt{2 \log (MSAH / \delta)}  \sigma_h \| g(x; W_0)\|_{\Lambda^{-1}_h} +  \iota_1 + \iota_2
\end{align*}
where the first inequality holds due to Equation (\ref{eq: simplify the bellman target by removing the max value}), the second inequality holds due to Lemma \ref{lemma: bound ERM with the Bellman target}, the third inequality holds due to Lemma \ref{lemma: linear approximation of neural functions}, and the last inequality holds due to Lemma \ref{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian} and Lemma \ref{lemma: Concentration of multivariate Gaussian} via the union bound.

\end{proof}

\subsection{Proof of Lemma \ref{lemma: expected subopt to empirical confidence quantifiers}}
\begin{proof}[Proof of Lemma \ref{lemma: expected subopt to empirical confidence quantifiers}]
Let $Z_k := \tilde{\beta}  \sum_{h=1}^H \mathbb{E}_{\pi^*} \left[ \mathbbm{1}\{k \in \mathcal{I}_h\} \| g(x_h; W_0) \|_{(\Lambda_h^k)^{-1}} | s_1^k, \mathcal{F}_h^{k-1} \right]$ where $\mathbbm{1}\{\}$ is the indicator function. Under the event in which the inequality in Theorem \ref{theorem: main theorem} holds, we have 
\begin{align}
   \subopt(\tilde{\pi}) &\leq \min \left\{H, \tilde{\beta} \cdot  \mathbb{E}_{\pi^*} \left[ \sum_{h=1}^H \| g(x_h; W_0) \|_{\Lambda_h^{-1}} \right] + 2 \iota  \right\} \nonumber\\ 
   &\leq \min \left\{H, \tilde{\beta} \mathbb{E}_{\pi^*} \left[ \sum_{h=1}^H \| g(x_h; W_0) \|_{\Lambda_h^{-1}} \right] \right\} + 2 \iota \nonumber\\
   &= \frac{1}{K'} \sum_{k=1}^K \min \left\{H, \tilde{\beta} \mathbb{E}_{\pi^*} \left[ \sum_{h=1}^H \mathbbm{1}\{k \in \mathcal{I}_h\} \| g(x_h; W_0) \|_{\Lambda_h^{-1}} \right] \right\} + 2 \iota \nonumber\\
   &\leq \frac{1}{K'} \sum_{k=1}^{K}  \min \left\{H, \tilde{\beta} \mathbb{E}_{\pi^*} \left[ \sum_{h=1}^H \mathbbm{1}\{k \in \mathcal{I}_h\} \| g(x_h; W_0) \|_{(\Lambda_h^k)^{-1}} | \mathcal{F}_h^{k-1} \right] \right\} + 2 \iota \nonumber\\
   &= \frac{1}{K'} \sum_{k=1}^{K} \min \left\{ H, \mathbb{E}[Z_k|\mathcal{F}_h^{k-1}] \right\} + 2 \iota \nonumber\\
   &\leq \frac{1}{K'} \sum_{k=1}^{K} \mathbb{E} \left[ \min \{H, Z_k  \} | \mathcal{F}_h^{k-1} \right] + 2 \iota,
   \label{eq: bound subopt with the empirical summation}
\end{align}
where the first inequality holds due to Theorem \ref{theorem: main theorem} and that $\subopt(\tilde{\pi}; s_1) \leq H, \forall s_1 \in \mathcal{S}$, the second inequality holds due to $\min \{a, b + c\} \leq \min\{a,b\} + c$, the third inequality holds due to that $\Lambda_h^{-1} \preceq (\Lambda_h^k)^{-1}$, the fourth inequality holds due to Jensen's inequality for the convex function $f(x) = \min\{H,x\}$. It follows from Lemma \ref{lemma:improved_online_to_batch} that with probability at least $1-\delta$, 
\begin{align}
    \sum_{k=1}^K \mathbb{E} \left[ \min \{H, Z_k  \} | \mathcal{F}_{k-1} \right] \leq 2 \sum_{k=1}^K Z_k + \frac{16}{3} H \log(\log_2(KH)/\delta) + 2. 
    \label{eq: apply improved online to batch to the uncertainty quantifier}
\end{align}
Substituting Equation (\ref{eq: apply improved online to batch to the uncertainty quantifier}) into Equation (\ref{eq: bound subopt with the empirical summation}) and using the union bound complete the proof.

\end{proof}


\subsection{Proof of Lemma \ref{lemma:sum_sample_subopt}}
\begin{proof}[Proof of Lemma \ref{lemma:sum_sample_subopt}]
Let $Z^k_h := \mathbbm{1}\{k \in \mathcal{I}_h\} \frac{d^*_h(x^k_h)}{d^{\mu}_h(x^k_h)} \| g(x^k_h; W_0)  \|_{(\Lambda^k_h)^{-1}}$. We have $Z^k_h$ is $\mathcal{F}_h^k$-measurable, and by Assumption \ref{assumption: OPC}, we have, 
\begin{align*}
    |Z^k_h| &\leq \frac{d^*_h(x^k_h)}{d^{\mu}_h(x^k_h)} \| g(x^k_h; W_0))  \|_2 \sqrt{\| (\Lambda^k_h)^{-1} \|} \leq 1/\sqrt{\lambda} \frac{d^*_h(x^k_h)}{d^{\mu}_h(x^k_h)} < \infty, \\ 
\mathbb{E} \left[ Z^k_h | \mathcal{F}_h^{k-1}, s^k_1 \right] &= \mathbb{E}_{x_h \sim d^{\mu}_h} \left[ \mathbbm{1}\{k \in \mathcal{I}_h\} \frac{d^*_h(x_h)}{d^{\mu}_h(x_h)} \| g(x_h; W_0) \|_{(\Lambda_h^k)^{-1}} \bigg| \mathcal{F}_h^{k-1}, s_1^k  \right].
\end{align*}
Thus, by Lemma \ref{lemma:azuma}, for any $h \in [H]$, with probability at least $1 - \delta$, we have: 
\begin{align*}
    &\sum_{k=1}^K \mathbb{E}_{x \sim d^*_h} \left[ \mathbbm{1}\{k \in \mathcal{I}_h\} \| g(x_h; W_0) \|_{(\Lambda_h^k)^{-1}} \bigg| \mathcal{F}_h^{k-1}, s_1^k  \right] \nonumber\\
    &=\sum_{k=1}^K \mathbb{E}_{x_h \sim d^{\mu}_h)} \left[ \mathbbm{1}\{k \in \mathcal{I}_h\} \frac{d^*_h(x_h)}{d^{\mu}_h(x_h)} \| \phi_h(x_h) \|_{(\Lambda_h^k)^{-1}} \bigg| \mathcal{F}_h^{k-1}, s_1^k  \right] \nonumber \\
    &\leq \sum_{k=1}^K \mathbbm{1}\{k \in \mathcal{I}_h\} \frac{d^*_h(x^k_h)}{d^{\mu}_h(x^k_h)} \| g(x^k_h; W_0)  \|_{(\Lambda^k_h)^{-1}} +  \sqrt{\frac{1}{\lambda} \log(1/\delta)} \sqrt{\sum_{k=1}^K \mathbbm{1}\{k \in \mathcal{I}_h\} \left(\frac{d^*_h(x^k_h)}{d^{\mu}_h(x^k_h)} \right)^2 } \\
    &\leq \kappa \sum_{k=1}^K  \mathbbm{1}\{k \in \mathcal{I}_h\} \| g(x_h; W_0)  \|_{(\Lambda^k_h)^{-1}} + 
    \kappa \sqrt{\frac{K' \log(1/\delta)}{\lambda}} \\
    &= \kappa \sum_{k \in \mathcal{I}_h}  \| g(x_h; W_0)  \|_{(\Lambda^k_h)^{-1}} + 
    \kappa \sqrt{\frac{K' \log(1/\delta)}{\lambda}}
\end{align*}
\end{proof}

\subsection{Proof of Lemma \ref{lemma: bound the summation of the uncertainty quantifier}}

\begin{proof}[Proof of Lemma \ref{lemma: bound the summation of the uncertainty quantifier}]
For any fixed $h \in [H]$, let  
\begin{align*}
    U = [g(x^k_h; W_0)]_{k \in \mathcal{I}_h} \in \mathbb{R}^{md \times K'}. 
\end{align*}
By the union bound, with probability at least $1 - \delta$, for any $h \in [H]$, we have 
\begin{align*}
    \sum_{k \in \mathcal{I}_h} \| g(x_h; W_0)  \|_{(\Lambda^k_h)^{-1}}^2 &\leq 2 \log \frac{\det \Lambda_h}{\det (\lambda I)} \\
    &= 2 \logdet \left(I + \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) g(x^k_h; W_0)^T / \lambda \right) \\ 
    &= 2 \logdet(I + U U^T /\lambda) \\ 
    &= 2 \logdet(I + U^T U /\lambda) \\
    &= 2 \logdet(I + \mathcal{K}_h/ \lambda + (U^T U - \mathcal{K}_h)/ \lambda) \\ 
    &\leq 2 \logdet(I + \mathcal{K}_h/ \lambda) + 2 \tr\left( (I + \mathcal{K}_h/ \lambda)^{-1} (U^T U - \mathcal{K}_h)/ \lambda \right) \\ 
    &\leq 2 \logdet(I + \mathcal{K}_h/ \lambda) + 2  \| (I + \mathcal{K}_h/ \lambda)^{-1} \|_F \| U^T U - \mathcal{K}_h \|_F \\ 
    &\leq 2 \logdet(I + \mathcal{K}_h/ \lambda) + 2 \sqrt{K'} \| U^T U - \mathcal{K}_h \|_F \\ 
    &\leq 2 \logdet(I + \mathcal{K}_h/ \lambda) + 1 \\ 
    &= 2\tilde{d}_h \log(1 + K' / \lambda) + 1
\end{align*}
where the first inequality holds due to $\lambda \geq C_g^2$ and \cite[Lemma~11]{NIPS2011_e1d5be1c}, the third equality holds due to that $\logdet(I + AA^T) = \logdet(I + A^TA)$, the second inequality holds due to that $\logdet(A + B) \leq \logdet(A) + \tr(A^{-1} B)$ as the result of the convexity of $\logdet$, the third inequality holds due to that $\tr(A) \leq  \| A \|_F$, the fourth inequality holds due to $2 \sqrt{K'} \| U^T U - \mathcal{K}_h \|_F \leq 1$ by the choice of $m = \Omega(K'^4 \log (K'H /\delta))$, Lemma \ref{lemma: ntk gram matrix versus grad gram matrix} and the union bound, and the last equality holds due to the definition of $\tilde{d}_h$. 
\end{proof}

 \section{Proofs of Lemmas in Section \ref{section: proof of main lemma about evaluation error}}
\label{section: proofs of intermediate lemmas for the main lemmas}


\subsection{Proof of Lemma \ref{lemma: bound ERM with the Bellman target}}
\label{subsection: proof of lemma D1}

In this subsection, we give detailed proof of Lemma \ref{lemma: bound ERM with the Bellman target}. For this, we first provide a lemma about the linear approximation of the Bellman operator. In the following lemma, we show that $\mathbb{B}_h \tilde{V}_{h+1}$ can be well approximated by the class of linear functions with features $g(\cdot; W_0)$ with respect to $l_{\infty}$-norm. 
\begin{lemma}
Under Assumption \ref{assumption: completeness}, with probability at least $1 - \delta$ over $w_1, \ldots, w_m$ drawn i.i.d. from $\mathcal{N}(0, I_d)$, for any $h \in [H]$, there exist $c_1, \ldots, c_m$ where $c_i \in \mathbb{R}^d$ and $\|c_i \|_2 \leq \frac{B}{m} $ such that 
\begin{align*}
    \bar{Q}_h(x) &:= \sum_{i=1}^m c_i^T x \sigma'(w^T_i x), \\
    \| \mathbb{B}_h \tilde{V}_{h+1} - \bar{Q}_h \|_{\infty} &\leq \frac{B}{\sqrt{m}}  (2 \sqrt{d} + \sqrt{2 \log (H / \delta)})
\end{align*}

Moreover, $\bar{Q}_h(x)$ can be re-written as 
\begin{align}
     \bar{Q}_h(x) &= \langle g(x; W_0), \bar{W}_h \rangle, \nonumber\\
\bar{W}_h &:= \sqrt{m} [ a_1 c_1^T, \ldots, a_m c_m^T ]^T \in \mathbb{R}^{md}, \text{ and } \| \bar{W}_h \|_2 \leq B. 
    \label{eq: bar W h}
\end{align}
\label{lemma: linear approximation for the Bellman target}
\end{lemma}

We now can prove Lemma \ref{lemma: bound ERM with the Bellman target}. 
\begin{proof}[Proof of Lemma \ref{lemma: bound ERM with the Bellman target}]

We first bound the difference $\langle g(x; W_0), \bar{W}_h \rangle -  \langle g(x; W_0), \hat{W}^{lin}_h - W_0 \rangle$:
\begin{align*}
    &\langle g(x; W_0), \bar{W}_h \rangle -  \langle g(x; W_0), \hat{W}^{lin}_h - W_0 \rangle = g(x; W_0)^T \bar{W}_h - g(x; W_0)^T \Lambda_h^{-1} \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) y^k_h \\
    &= \underbrace{ g(x; W_0)^T \bar{W}_h - g(x; W_0)^T \Lambda_h^{-1} \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) \cdot (\mathbb{B}_h \tilde{V}_{h+1})(x^k_h) }_{I_1}\\ 
    &+ \underbrace{g(x; W_0)^T \Lambda_h^{-1} \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) \cdot \left[ (\mathbb{B}_h \tilde{V}_{h+1})(x^k_h)  - (r^k_h + \tilde{V}_{h+1}(s^k_{h+1}))\right] }_{I_2}. 
\end{align*}

For bounding $I_1$, it follows from Lemma \ref{lemma: linear approximation for the Bellman target} that with probability at least $1 - \delta / 3$, for any for any $x \in \mathbb{S}_{d-1}$ and any $h \in [H]$, 
\begin{align*}
    |(\mathbb{B}_h \tilde{V}_{h+1})(x) - \langle g(x; W_0), \bar{W}_h \rangle | \leq \iota_0, 
\end{align*}
where $\iota_0$ is defined in Table \ref{tab: key parameters in the proofs}.
where $\bar{W}_h$ is defined in Lemma \ref{lemma: linear approximation for the Bellman target}. 
Thus, with probability at least $1 - \delta/3$, for any for any $x \in \mathbb{S}_{d-1}$ and any $h \in [H]$, 
\begin{align}
    I_1 &= g(x; W_0)^T \bar{W}_h - g(x; W_0)^T \Lambda_h^{-1} \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) \cdot \left[ (\mathbb{B}_h \tilde{V}_{h+1})(x^k_h) - g(x^k_h; W_0)^T \bar{W}_h \right] \nonumber \\
    &- g(x; W_0)^T \bar{W}_h + \lambda g(x; W_0)^T \Lambda_h^{-1} \bar{W}_h \nonumber \\
    &\leq \| g(x; W_0)^T \|_{\Lambda_h^{-1} } \sum_{k \in \mathcal{I}_h} \iota_0 \| g(x^k_h; W_0)^T \|_{\Lambda_h^{-1} }  + \lambda  \| g(x; W_0) \|_{ \Lambda_h^{-1} } \| \bar{W}_h \|_{ \Lambda_h^{-1} } \nonumber \\ 
    &\leq  \| g(x; W_0) \|_{\Lambda_h^{-1} } \left[ K' \iota_0 \lambda^{-1/2} C_g + \lambda^{1/2}  B \right],
    \label{eq: bound I1}
\end{align}
where the first equation holds due to the definition of $\Lambda_h$, and the last inequality holds due to Step I with $\| \bar{W}_h \|_{\Lambda_h^{-1}} \leq \sqrt{ \| \Lambda_h^{-1}\|_2} \cdot \| \bar{W}_h \|_2 \leq \lambda^{-1/2} B $. 

For bounding $I_2$, we have
\begin{align}
    I_2 &\leq  \underbrace{\left\| \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) \left[ (\mathbb{B}_h \tilde{V}_{h+1})(x^k_h) - r^k_h - \tilde{V}_{h+1}(s^k_{h+1}) \right] \right\|_{\Lambda_h^{-1}} }_{I_3}  \| g(x; W_0) \|_{\Lambda_h^{-1}}.
    \label{eq: bound I2 in terms of I3}
\end{align}


If we directly apply the result of \cite{jin2021pessimism} in linear MDP, we would get
\begin{align*}
    I_2 \lesssim  dm H \sqrt{\log (2 dm K'H /\delta)} \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}},
\end{align*}
which gives a vacuous bound as $m$ is sufficiently larger than $K$ in our problem. Instead, in the following, we present an alternate proof that avoids such vacuous bound. 

For notational simplicity, we write 
\begin{align*}
    \epsilon^k_h &:= (\mathbb{B}_h \tilde{V}_{h+1})(x^k_h) - r^k_h - \tilde{V}_{h+1}(s^k_{h+1}), \\ 
    E_h &:= [(\epsilon^k_h)_{k \in \mathcal{I}_h}]^T  \in \mathbb{R}^{K'}. 
\end{align*}
We denote $\mathcal{K}^{init}_h := [\langle g(x^i_h; W_0), g(x^j_h; W_0) \rangle]_{i,j \in \mathcal{I}_h}$ as the Gram matrix of the empirical NTK kernel on the data $\{x^k_h\}_{k \in [K]}$. We denote 
\begin{align*}
    G_0 &:= \left( g(x^k_h; W_0) \right)_{k \in \mathcal{I}_h} \in \mathbb{R}^{md \times K' }, \\ 
    \mathcal{K}^{int}_h &:= G_0^T G_0 \in \mathcal{R}^{K' \times K'}. 
\end{align*}
Recall the definition of the Gram matrix $\mathcal{K}_h$ of the NTK kernel on the data $\{x^k_h\}_{k \in \mathcal{I}_h}$. It follows from Lemma \ref{lemma: ntk gram matrix versus grad gram matrix} and the union bound that if $m = \Omega(\epsilon^{-4} \log(3 K'H/\delta))$ with probability at least $1 - \delta/3$, for any $h \in [H]$,
\begin{align}
    \| \mathcal{K}_h - \mathcal{K}_h^{init} \|_F \leq \sqrt{K'} \epsilon.
    \label{eq: cov ntk - cov init}
\end{align}



We now can bound $I_3$. We have 
\begin{align}
    I_3^2 &= \left\| \sum_{k=\mathcal{I}_h} g(x^k_h; W_0) \epsilon^k_h \right\|_{\Lambda_h^{-1}}^2 \nonumber \\ 
    &= E_h^T G_0^T (\lambda I_{md} + G_0 G_0^T)^{-1} G_0 E_h \nonumber \\ 
    &= E_h^T G_0^T G_0 (\lambda I_{K'} + G_0^T G_0)^{-1} E_h \nonumber \\  
    &= E_h^T  \mathcal{K}^{init}_h ( \mathcal{K}^{init}_h + \lambda I_K)^{-1} E_h \nonumber \nonumber \\  
    &= \underbrace{E_h^T  \mathcal{K}_h ( \mathcal{K}_h + \lambda I_{K'})^{-1} E_h}_{I_5} +  \underbrace{ E_h^T \left( \mathcal{K}_h ( \mathcal{K}_h + \lambda I_{K'})^{-1} - \mathcal{K}^{init}_h ( \mathcal{K}^{int}_h + \lambda I_{K'})^{-1}  E_h \right)}_{I_4}.
    \label{eq: I3 in terms of I4 and I5}
\end{align}


We bound each $I_4$ and $I_5$ separately. For bounding $I_4$, applying Lemma \ref{lemma: NTK approximation error}, with $1 - H m^{-2}$, for any $h \in [H]$, 
\begin{align}
     I_4  &\leq \left\| \mathcal{K}_h ( \mathcal{K}_h + \lambda I_{K'})^{-1} - \mathcal{K}^{init}_h ( \mathcal{K}^{int}_h + \lambda I_{K'})^{-1} \right\|_2 \| E_h \|_2^2 \nonumber\\ 
    &= \left\| (\mathcal{K}_h - \mathcal{K}_h^{init}) ( \mathcal{K}_h + \lambda I_{K'})^{-1} + \mathcal{K}_h^{init} \left( ( \mathcal{K}_h + \lambda I_{K'})^{-1} -  (\mathcal{K}^{int}_h + \lambda I_{K'})^{-1}\right) \right\|_2 \| E_h \|_2^2 \nonumber\\
    &\leq  \| \mathcal{K}_h -  \mathcal{K}_h^{init} \|_2 / \lambda + \| \mathcal{K}_h^{init} \|_2 \cdot \| \mathcal{K}_h - \mathcal{K}_h^{init} \|_2 / \lambda^{2} \| E_h \|_2^2 \nonumber\\
    &\leq \frac{\lambda + K' C_g^2}{\lambda^2} \| \mathcal{K}_h - \mathcal{K}_h^{init} \|_2 \| E_h \|_2^2 \nonumber\\ 
&\leq 2 K' C_g^2  K' (H + \psi)^2  \| \mathcal{K}_h - \mathcal{K}_h^{init} \|_2,
    \label{eq: bound approx error of self-normalized process}
\end{align}
where the first inequality holds due to the triangle inequality, the second inequality holds due to the triangle inequality, Lemma \ref{lemma: difference of two inverse matrices}, and $\|( \mathcal{K}_h + \lambda I_{K'})^{-1} \|_2 \leq \lambda^{-1}$, the third inequality holds due to $\| \mathcal{K}_h^{init} \|_2 \leq \|G_0\|_2^2 \leq \| G_0 \|_F^2 \leq K' C_g^2$ due to Lemma \ref{lemma: NTK approximation error}, the fourth inequality holds due to $\|E_h \|_2 \leq \sqrt{K'} (H + \psi)$, $\lambda \geq 1$, and $K' C_g^2 \geq \lambda$. 

Substituting Equation (\ref{eq: cov ntk - cov init}) in Equation (\ref{eq: bound approx error of self-normalized process}) using the union bound, with probability $1 - H m^{-2} - \delta / 3$, for any $h \in [H]$, 
\begin{align}
    I_4 &\leq 2 K' C_g^2  K' (H + \psi)^2 \sqrt{K'} \epsilon \leq 1,
    \label{eq: bound I4 in self-normalized process proof}
\end{align}
where the last inequality holds due to the choice of $\epsilon = 1/2 K'^{-5/2} (H + \psi)^{-2} C_g^{-2} $ and thus 
\begin{align*}
    m = \Omega(\epsilon^{-4} \log(3 K'H/\delta)) = \Omega \left( K'^{10} (H + \psi)^2 \log(3 K'H/\delta)\right).
\end{align*}



For bounding $I_5$, as $\lambda > 1$, we have 
\begin{align}
    I_5  &= E_h^T  \mathcal{K}_h ( \mathcal{K}_h + \lambda I_{K'})^{-1} E_h \nonumber \\ 
    &\leq E_h^T  (\mathcal{K}_h + (\lambda-1) I_K) ( \mathcal{K}_h + \lambda I_{K'})^{-1} E_h \nonumber\\ 
    &=  E_h^T  \left[ (\mathcal{K}_h + (\lambda - 1) I_{K'})^{-1} + I_{K'} \right]^{-1} E_h. 
    \label{eq: transform the form of the self-normalized process}
\end{align}
Let $\sigma(\cdot)$ be the $\sigma$-algebra induced by the set of random variables. For any $h \in [H]$ and $k  \in \mathcal{I}_h = [(H - h) K' + 1, \ldots, (H - h + 1) K']$, we define the filtration 
\begin{align*}
    \mathcal{F}^k_h = \sigma \left(\{(s^t_{h'}, a^t_{h'}, r^t_{h'})\}_{h' \in [H]}^{t \leq k} \cup \{(s^{k+1}_{h'}, a^{k+1}_{h'}, r^{k+1}_{h'})\}_{h' \leq h-1} \cup \{(s^{k+1}_h, a^{k+1}_h)\} \right) 
\end{align*}
which is simply all the data up to episode $k+1$ and timestep $h$ but right before $r^{k+1}_h$ and $s^{k+1}_{h+1}$ are generated (in the offline data). \footnote{To be more precise, we need to include into the filtration the randomness from the generated noises $\{\xi^{k,i}_h\}$ and $\{\zeta^i_h\}$ but since these noises are independent of any other randomness, they do not affect any derivations here but only complicate the notations and representations.} Note that for any $k \in \mathcal{I}_h$, we have $(s^k_h, a^k_h, r^k_h, s^k_{h+1}) \in \mathcal{F}^k_h $, and
\begin{align*}
    \tilde{V}_{h+1} \in \sigma \left( \{(s^k_{h'}, a^k_{h'}, r^k_{h'})\}^{k \in \mathcal{I}_{h'}}_{h' \in [h+1, \ldots, H]}  \right) \subseteq \mathcal{F}^{k-1}_h \subseteq \mathcal{F}^k_h. 
\end{align*}
Thus, for any $k \in \mathcal{I}_h$, we have 
\begin{align*}
    \epsilon^k_h = (\mathbb{B}_h \tilde{V}_{h+1})(x^k_h) - r^k_h - \tilde{V}_{h+1}(s^k_{h+1}) \in \mathcal{F}^k_h. 
\end{align*}
The key property in our data split design is that we nicely have that
\begin{align*}
    \tilde{V}_{h+1} \in \sigma \left( \{(s^k_{h'}, a^k_{h'}, r^k_{h'})\}^{k \in \mathcal{I}_{h'}}_{h' \in [h+1, \ldots, H]}  \right) \subseteq \mathcal{F}^{k-1}_h.
\end{align*}
Thus, conditioned on $\mathcal{F}^{k-1}_h$, $\tilde{V}_{h+1}$ becomes deterministic. This implies that 
\begin{align*}
    \mathbb{E} \left[ \epsilon^k_h | \mathcal{F}^{k-1}_h \right] = \left[ (\mathbb{B}_h \tilde{V}_{h+1})(s^k_h, a^k_h) - r^k_h - \tilde{V}_{h+1}(s^k_{h+1}) | \mathcal{F}^{k-1}_h\right] = 0. 
\end{align*}
Note that this is only possible with our data splitting technique. Otherwise, $\epsilon^k_h$ is not zero-mean due to the data dependence structure induced in offline RL with function approximation \citep{nguyentang2021sample}. Our data split technique is a key to avoid the uniform convergence argument with the log covering number that is often used to bound this term in \cite{jin2021pessimism}, which is often large for complex models. For example, in a two-layer ReLU NTK, the eigenvalues of the induced RKHS has $d$-polynomial decay \citep{bietti2019inductive}, thus its log covering number roughly follows, by \citep[Lemma~D1]{yang2020function},
\begin{align*}
    \log \mathcal{N}_{\infty}(\mathcal{H}_{ntk}, \epsilon, B) \lesssim \left( \frac{1}{\epsilon} \right)^{\frac{4}{\alpha d -1}},
\end{align*}
for some $\alpha \in (0,1)$. 

Therefore, for any $h \in [H]$, $\{\epsilon^k_h\}_{k \in \mathcal{I}_h}$ is adapted to the filtration $\{\mathcal{F}_h^{k}\}_{k \in \mathcal{I}_h}$. Applying Lemma \ref{lemma: self-normalized concentration process in RKHS} with $Z_t = \epsilon_t^h \in [-(H + \psi), H + \psi]$, $\sigma^2 = (H + \psi)^2$, $\rho = \lambda - 1$, for any $\delta > 0$, with probability at least $1 - \delta / 3$, for any $h \in [H]$, 
\begin{align}
    E^T_h \left[(\mathcal{K}_h + (\lambda - 1) I_{K'})^{-1} + I \right]^{-1} E_h \leq (H + \psi)^2 \logdet \left(\lambda I_{K'} + \mathcal{K}_h \right) + 2 (H + \psi)^2 \log (3 H / \delta)
    \label{eq: apply self-normalized concentration of RKHS}
\end{align}

Substituting Equation (\ref{eq: apply self-normalized concentration of RKHS}) into Equation (\ref{eq: transform the form of the self-normalized process}), we have 
\begin{align}
    I_5 &\leq (H + \psi)^2 \logdet (\lambda I_{K'} + \mathcal{K}_h) + 2 (H + \psi)^2 \log (H / \delta) \nonumber \\ 
    &=  (H + \psi)^2 \logdet ( I_{K'} + \mathcal{K}_h /\lambda) + (H + \psi)^2 K' \log \lambda + 2 (H + \psi)^2 \log (H / \delta) \nonumber \\
    &= (H + \psi)^2 \tilde{d}_h \log (1 + K' / \lambda) + (H + \psi)^2 K' \log \lambda + 2 (H + \psi)^2 \log (H / \delta),
    \label{eq: bound of self-normalized process with NTK kernel}
\end{align}
where the last equation holds due to the definition of the effective dimension. 



Combining Equations (\ref{eq: bound of self-normalized process with NTK kernel}), (\ref{eq: bound I4 in self-normalized process proof}), (\ref{eq: I3 in terms of I4 and I5}), (\ref{eq: bound I2 in terms of I3}), and (\ref{eq: bound I1}) via the union bound, with probability at least $1 - H m^{-2} - \delta$, for any $x \in \mathbb{S}_{d-1}$ and any $h \in [H]$,
\begin{align*}
    |\langle g(x; W_0), \bar{W}_h \rangle -  \langle g(x; W_0), \hat{W}^{lin}_h - W_0 \rangle | \leq \beta \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}}, 
\end{align*}
where 
\begin{align}
    \beta := K' \iota_0 \lambda^{-1/2} C_g + \lambda^{1/2}  B + (H + \psi) \left[ \sqrt{\tilde{d}_h \log (1 + K' / \lambda) + K' \log \lambda + 2 \log (3 H / \delta)} \right]. 
    \label{equation: confidence multiplier beta}
\end{align}
Combing with Lemma \ref{lemma: linear approximation of neural functions} using the union bound, with probability at least $ 1 - H   m^{-2} - 2\delta$, for any $x \in \mathbb{S}_{d-1}$, and any $h \in [H]$,
\begin{align*}
    f(x; \hat{W}_h) - (\mathbb{B}_h \tilde{V}_{h+1})(x) &\leq \langle g(x; W_0), \hat{W}^{lin}_h - W_0 \rangle + \iota_2  -  \langle g(x; W_0), \bar{W}_h \rangle + \iota_0 \\
    &\leq \beta \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_2 + \iota_0 , \\
    &= \beta  \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_2 + \iota_0
\end{align*}
where $\iota_2$, and $\beta$ are defined in Table \ref{tab: key parameters in the proofs}.


Similarly, it is easy to show that  
\begin{align*}
    (\mathbb{B}_h \tilde{V}_{h+1})(x) - f(x; \hat{W}_h)
    &\leq \beta  \cdot \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_2 + \iota_0. 
\end{align*}

\end{proof} 

\subsection{Proof of Lemma \ref{Lemma: anti-concentration of tilde Q}}

Before proving Lemma \ref{Lemma: anti-concentration of tilde Q}, we prove the following intermediate lemmas. The detailed proofs of these intermediate lemmas are deferred to Section \ref{section: proof of intermediate lemmas of intermediate lemmas}. 

\begin{lemma}
Conditioned on all the randomness except $\{\xi^{k,i}_h\}$ and $\{\zeta^i_h \}$, for any $i \in [M]$,
\begin{align*}
    \tilde{W}^{i,lin}_h - \hat{W}^{lin}_h \sim \mathcal{N}(0, \sigma^2_h \Lambda_h^{-1}). 
\end{align*}
\label{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian}
\end{lemma}



\begin{lemma}
If we set $M = \log \frac{HSA}{\delta} / \log \frac{1}{1 - \Phi(-1)}$ where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal distribution, then with probability at least $1 - \delta$, for any $(x,h) \in \mathcal{X} \times [H]$,  
\begin{align*}
     \min_{i \in [M]} \langle g(x; W_0), \tilde{W}^{i,lin}_h \rangle \leq \langle  g(x; W_0), \hat{W}_h^{lin} \rangle - \sigma_h \|  g(x; W_0)\|_{\Lambda_h^{-1}}. 
\end{align*}
\label{lemma: anti-concentration for proxy linear MDP}
\end{lemma}


We are now ready to prove Lemma \ref{Lemma: anti-concentration of tilde Q}. 
\begin{proof}[Proof of Lemma \ref{Lemma: anti-concentration of tilde Q}]
Note that the parameter condition in Equation (\ref{equation: conditions for R and eta final version repeated}) of Lemma \ref{Lemma: anti-concentration of tilde Q} satisfies Equation (\ref{equation: conditions for R and eta final version}) of Lemma \ref{lemma: linear approximation of neural functions}, thus given the parameter condition Lemma \ref{Lemma: anti-concentration of tilde Q}, Lemma \ref{lemma: linear approximation of neural functions} holds. For the rest of the proof, we consider under the joint event in which both the inequality of Lemma \ref{lemma: linear approximation of neural functions} and that of Lemma \ref{lemma: anti-concentration for proxy linear MDP} hold. By the union bound, probability that this joint event holds is at least $1 - MH m^{-2} - \delta$. Thus, for any $x \in \mathbb{S}_{d-1}$, $h \in [H]$, and $i \in [M]$, 

\begin{align*}
    \min_{i \in [M]} f(x; \tilde{W}^{i}_h) - f(x; \hat{W}_h) &\leq \min_{i \in [M]} \langle g(x; W_0), \tilde{W}^{i,lin}_h - W_0 \rangle  - \langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle + \iota_1 + \iota_2 \\
    &\leq -\sigma_h \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_1 + \iota_2
\end{align*}
where the first inequality holds due to Lemma \ref{lemma: linear approximation of neural functions}, and the second inequality holds due to Lemma \ref{lemma: anti-concentration for proxy linear MDP}. Thus, we have 
\begin{align*}
    \tilde{Q}_h(x) &= \min \{\min_{i \in [M]} f(x; \tilde{W}^i_h), H - h +1 + \psi \}^{+} \leq \max\{\min_{i \in [M]} f(x; \tilde{W}^i_h), 0\} \\
    &\leq \max\{\langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle  -\sigma_h \| g(x; W_0) \|_{\Lambda_h^{-1}} + \iota_1 + \iota_2, 0\}. 
\end{align*}



\end{proof}

\subsection{Proof of Lemma \ref{lemma: linear approximation of neural functions}}
\label{subsection: proof of linear approximation lemma}
In this subsection, we provide a detailed proof of Lemma \ref{lemma: linear approximation of neural functions}. We first provide intermediate lemmas that we use for proving Lemma \ref{lemma: linear approximation of neural functions}. The detailed proofs of these intermediate lemmas are deferred to Section \ref{section: proof of intermediate lemmas of intermediate lemmas}. 







The following lemma bounds the the gradient descent weight of the perturbed loss function around the linear weight counterpart. 



\begin{lemma}
Let 
\begin{align}
\begin{cases}
m = \Omega \left( d^{3/2}  R^{-1} \log^{3/2} (\sqrt{m} / R) \right) \\
      R = \mathcal{O} \left( m^{1/2} \log^{-3} m \right), \\ 
\eta \leq (\lambda + K' C_g^2)^{-1}, \\
R \geq \max\{ 4 \tilde{B}_1, 4 \tilde{B}_2, 2 \sqrt{2\lambda^{-1}K' (H + \psi + \gamma_{h,1})^2 + 4 \gamma_{h,2}^2} \},
\end{cases}
\label{equation: improved conditions for R and eta comprehensive list}
\end{align}
where $\tilde{B}_1$, $\tilde{B}_2$, $\gamma_{h,1}$ and $\gamma_{h,2}$ are defined in Table \ref{tab: key parameters in the proofs} and $C_g$ is a constant given in Lemma \ref{lemma: NTK approximation error}. 
With probability at least $1 -  MH m^{-2} - \delta$, for any $(i,j,h) \in [M] \times [J] \times [H]$, we have 
\begin{itemize}
    \item $\tilde{W}_h^{i, (j)} \in \mathcal{B}(W_0; R)$, 
    \item $\| \tilde{W}_h^{i, (j)} - \tilde{W}_h^{i,lin} \|_2 \leq \tilde{B}_1 + \tilde{B}_2 + \lambda^{-1}(1 - \eta \lambda)^j \left( K'(H + \psi + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2\right)$
\end{itemize}
\label{lemma: improve bounding GD weight and that of linear auxilary}
\end{lemma}

Similar to Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}, we obtain the following lemma for the gradient descent weights of the non-perturbed loss function. 

\begin{lemma}
Let 
\begin{align}
\begin{cases}
m = \Omega \left( d^{3/2}  R^{-1} \log^{3/2} (\sqrt{m} / R) \right) \\
      R = \mathcal{O} \left( m^{1/2} \log^{-3} m \right), \\ 
\eta \leq (\lambda + K' C_g^2)^{-1}, \\
R \geq \max\{ 4 B_1, 4 \tilde{B}_2, 2 \sqrt{2\lambda^{-1}K'} (H + \psi) \},
\end{cases}
\label{equation: improved conditions for R and eta comprehensive list for non-perturbed loss}
\end{align}
where  $B_1$, $\tilde{B}_2$, $\gamma_{h,1}$ and $\gamma_{h,2}$ are defined in Table \ref{tab: key parameters in the proofs} and $C_g$ is a constant given in Lemma \ref{lemma: NTK approximation error}. 
With probability at least $1 -  MH m^{-2} - \delta$, for any $(i,j,h) \in [M] \times [J] \times [H]$, we have 
\begin{itemize}
    \item $\hat{W}_h^{ (j)} \in \mathcal{B}(W_0; R)$, 
    \item $\| \hat{W}_h^{(j)} - \hat{W}_h^{lin} \|_2 \leq B_1 + \tilde{B}_2 + \lambda^{-1}(1 - \eta \lambda)^j  K'(H + \psi  )^2 $
\end{itemize}
\label{lemma: improved bounding GD weight and that of linear auxilary for non-perturbed loss}
\end{lemma}







We now can prove Lemma \ref{lemma: linear approximation of neural functions}.
\begin{proof}[Proof of Lemma \ref{lemma: linear approximation of neural functions}]
Note that Equation (\ref{equation: conditions for R and eta final version}) implies both Equation (\ref{equation: improved conditions for R and eta comprehensive list}) of Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary} and Equation (\ref{equation: improved conditions for R and eta comprehensive list for non-perturbed loss}) of Lemma \ref{lemma: improved bounding GD weight and that of linear auxilary for non-perturbed loss}, thus both  Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary} and Lemma \ref{lemma: improved bounding GD weight and that of linear auxilary for non-perturbed loss} holds under Equation (\ref{equation: conditions for R and eta final version}). Thus, by the union bound, with probability at least $1 -  M H m^{-2} - \delta$, for any $(i,j,h)  \in [M] \times [J] \times [H]$, and $x \in \mathbb{S}_{d-1}$,
\begin{align*}
    &|f(x; \tilde{W}^{i,(j)}_h) - \langle g(x; W_0), \tilde{W}_h^{i,lin} - W_0 \rangle| \\
    &\leq |f(x; \tilde{W}^{i,(j)}_h) - \langle g(x; W_0), \tilde{W}_h^{i, (j)} - W_0 \rangle| + |\langle g(x; W_0), \tilde{W}^{i,(j)}_h - \tilde{W}_h^{i,lin} \rangle| \\ 
    &\leq C_g R^{4/3} m^{-1/6} \sqrt{ \log m} + C_g \left( \tilde{B}_1 + \tilde{B}_2 + \lambda^{-1}(1 - \eta \lambda)^j \left( K'(H + \psi + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2\right) \right)  = \iota_1, 
\end{align*}
where the first inequality holds due to the triangle inequality, the second inequality holds due to Cauchy-Schwarz inequality, Lemma \ref{lemma: NTK approximation error}, and Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}. 

Similarly, by the union bound, with probability at least $1 -  H m^{-2}$, for any $(i,j,h)  \in [M] \times [J] \times [H]$, and $x \in \mathbb{S}_{d-1}$,
\begin{align*}
    &|f(x; \hat{W}^{(j)}_h) - \langle g(x; W_0), \hat{W}_h^{lin} - W_0 \rangle| \leq |f(x; \hat{W}^{(j)}_h) - \langle g(x; W_0), \hat{W}_h^{(j)} - W_0 \rangle| \\
    &+ |\langle g(x; W_0), \hat{W}^{(j)}_h - \hat{W}_h^{lin} \rangle| \\ 
    &\leq C_g R^{4/3} m^{-1/6} \sqrt{ \log m} + C_g \left( B_1 + \tilde{B}_2 + \lambda^{-1}(1 - \eta \lambda)^j  K'(H + \psi  )^2 \right)  = \iota_2, 
\end{align*}
where the first inequality holds due to the triangle inequality, the second inequality holds due to Cauchy-Schwarz inequality, Lemma \ref{lemma: improved bounding GD weight and that of linear auxilary for non-perturbed loss}, and Lemma \ref{lemma: NTK approximation error}. 
\end{proof}




 \section{Proofs of Lemmas in Section \ref{section: proofs of intermediate lemmas for the main lemmas}}
\label{section: proof of intermediate lemmas of intermediate lemmas}
In this section, we provide the detailed proofs of Lemmas in Section \ref{section: proofs of intermediate lemmas for the main lemmas}. 

\subsection{Proof of Lemma \ref{lemma: linear approximation for the Bellman target}}
\begin{proof}[Proof of Lemma \ref{lemma: linear approximation for the Bellman target}]
As $\mathbb{B}_h \tilde{V}_{h+1} \in \mathcal{Q}^*$ by Assumption \ref{assumption: completeness}, where $\mathcal{Q}^*$ is defined in Section \ref{section: subopt analysis},
we have 
\begin{align*}
    \mathbb{B}_h \tilde{V}_{h+1} =  \int_{\mathbb{R}^d} c(w)^T x \sigma'(w^T x) d w,
\end{align*}
for some $c: \mathbb{R}^d \rightarrow \mathbb{R}^d$ such that $\sup_{w} \frac{\| c(w) \|_2}{p_0(w)} \leq B$. The lemma then directly follows from approximation by finite sum \citep{gao2019convergence}. 
\end{proof}

\subsection{Proof of Lemma \ref{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian}}

\begin{proof}[Proof of Lemma \ref{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian}]
Let $\bar{W} := W + \zeta^i_h$ and 
\begin{align*}
    \bar{L}^{i}_h(\bar{W}) := \sum_{k \in \mathcal{I}_h} \left(\langle g(x^k_h; W_0), \bar{W} \rangle - \bar{y}^{i,k}_h \right)^2 + \lambda \| \bar{W}  \|_2^2, 
\end{align*}
where  $\bar{y}^k_h = r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{k,i}_h + \langle g(x^k_h; W_0), \zeta^i_h \rangle$. 
We have $\tilde{L}_h^{i,lin}(W) = \bar{L}_h^{i}(\bar{W})$
and $\argmax_{W} \tilde{L}_h^{i,lin}(W)  = \argmax_{\bar{W}} \bar{L}^i_h(\bar{W}) - \zeta^i_h$ as both $\tilde{L}_h^{i,lin}(W)$ and $\bar{L}^i_h$ are convex. Using the regularized least-squares solution,  
\begin{align*}
     &\argmax_{\bar{W}} \bar{L}_h^i(\bar{W}) = \Lambda_h^{-1} \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) \bar{y}^k_h \\ 
     &= \Lambda_h^{-1} \left[ \sum_{k \in \mathcal{I}_h}  g(x^k_h; W_0) (r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{k,i}_h ) + \sum_{k \in \mathcal{I}_h}  g(x^k_h; W_0) \langle  g(x^k_h; W_0), \zeta^i_h \rangle \right] \\
     &= \Lambda_h^{-1} \left[ \sum_{k \in \mathcal{I}_h}  g(x^k_h; W_0) (r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{k,i}_h ) + \sum_{k \in \mathcal{I}_h}  g(x^k_h; W_0)  g(x^k_h; W_0)^T \zeta^i_h  \right] \\ 
     &= \Lambda_h^{-1} \left[ \sum_{k \in \mathcal{I}_h}  g(x^k_h; W_0) (r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{k,i}_h ) + (\Lambda_h - \lambda I_{md}) \zeta^i_h  \right]. 
\end{align*}
Thus, we have
\begin{align*}
    \tilde{W}^i_h &= \argmax_{W} \tilde{L}_h^{i,lin}(W) = \argmax_{\bar{W}} \bar{L}^i_h(\bar{W}) - \zeta^i_h \\
    &=  \Lambda_h^{-1} \left[ \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) (r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{k,i}_h ) + (\Lambda_h - \lambda I_{md}) \zeta^i_h  \right] - \zeta^i_h \\
    &= \Lambda_h^{-1} \left[ \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) (r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{k,i}_h )  - \lambda  \zeta^i_h  \right] \\
    &= \hat{W}_h + \Lambda^{-1}_h \left[ \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) \xi^{k,i}_h - \lambda \zeta^i_h \right]
\end{align*}
By direct computation, it is easy to see that 
\begin{align*}
    \tilde{W}^i_h - \hat{W}_h =  \Lambda^{-1}_h \left[ \sum_{k \in \mathcal{I}_h} g(x^k_h; W_0) \xi^{k,i}_h - \lambda \zeta^i_h \right] \sim \mathcal{N}(0, \sigma_h^2 \Lambda_h^{-1}). 
\end{align*}

\end{proof}

\subsection{Proof of Lemma \ref{lemma: anti-concentration for proxy linear MDP}}

In this subsection, we provide a proof for \ref{lemma: anti-concentration for proxy linear MDP}. We first provide a bound for the perturbed noises used in Algorithm \ref{algorithm: PERVI} in the following lemma.
\begin{lemma}
There exist absolute constants $c_1, c_2 > 0$ such that for any $\delta > 0$, event $\mathcal{E}(\delta)$ holds with probability at least $1 - \delta$, for any $(k,h,i) \in [K] \times [H] \times [M]$, 
\begin{align*}
     |\xi^{k,i}_h| &\leq c_1 \sigma_h \sqrt{\log (K'HM / \delta)} =: \gamma_{h,1}, \\
     \| \zeta^i_h \|_2 &\leq c_2 \sigma_h \sqrt{d \log (d K'HM / \delta)} =: \gamma_{h,2}. 
\end{align*}
\label{Lemma: good events where the noises are bounded}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{Lemma: good events where the noises are bounded}]
It directly follows from the Gaussian concentration inequality in Lemma \ref{lemma: Concentration of multivariate Gaussian} and the union bound.
\end{proof}





We now can prove Lemma \ref{lemma: anti-concentration for proxy linear MDP}. 
\begin{proof}[Proof of Lemma \ref{lemma: anti-concentration for proxy linear MDP}]
By Lemma \ref{Lemma: perturbed ERM minus non-perturbed ERM follows a Gaussian}, 
\begin{align*}
    \tilde{W}^{i,lin}_h - \hat{W}^{lin}_h \sim \mathcal{N}(0, \sigma^2_h \Lambda_h^{-1}). 
\end{align*}
Using the anti-concentration of Gaussian distribution, for any $x = (s,a) \in \mathcal{S} \times \mathcal{A}$ and any $i \in [M]$, 
\begin{align*}
    \mathbb{P} \left( \langle g(x; W_0), \tilde{W}^{i,lin}_h \rangle \leq \langle g(x; W_0), \hat{W}^{lin}_h \rangle - \sigma_h \| g(x; W_0) \|_{\Lambda_h^{-1}} \right) = \Phi(-1) \in (0,1). 
\end{align*}
As $\{\tilde{W}^{i,lin}_h \}_{i \in [M]}$ are independent, using the union bound, with probability at least $1 - SAH (1 - \Phi(-1))^M$, for any $x = (s,a) \in \mathcal{S} \times \mathcal{A}$, and $h \in [H]$,
\begin{align*}
    \min_{i \in [M]}\langle g(x; W_0), \tilde{W}^{i,lin}_h \rangle \leq \langle g(x; W_0), \hat{W}^{lin}_h \rangle - \sigma_h \| g(x; W_0) \|_{\Lambda_h^{-1}}. 
\end{align*}
Setting $\delta = SAH (1 - \Phi(-1))^M$ completes the proof.
\end{proof}





\subsection{Proof of Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}}
In this subsection, we provide a detailed proof of Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}. We first prove the following intermediate lemma whose proof is deferred to Subsection \ref{subsection: proof of lemma f_j - y}. 

\begin{lemma}
Let 
\begin{align*}
    \begin{cases}
        m = \Omega \left( d^{3/2}  R^{-1} \log^{3/2} (\sqrt{m} / R) \right) \\
      R = \mathcal{O} \left( m^{1/2} \log^{-3} m \right). 
    \end{cases}
\end{align*}
 and additionally let 
\begin{align*}
\begin{cases}
    \eta \leq (K' C_g^2 + \lambda /2)^{-1}, \\ 
    \eta \leq \frac{1}{2 \lambda}.
\end{cases}
\end{align*}
Then with probability at least $1 - MH m^{-2} - \delta$, for any $(i,j,h) \in [M] \times [J] \times [H]$, if $\tilde{W}_h^{i,(j)} \in \mathcal{B}(W_0; R)$ for any $j' \in [j]$, then 
\begin{align*}
    \|f_{j'} - \tilde{y} \|_2 \lesssim \sqrt{  K'(H + \psi + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2 + (\lambda \eta)^{-2} R^{4/3} m^{-1/6} \sqrt{ \log m}}. 
\end{align*}
\label{lemmaq: improved bound f_j - y}
\end{lemma}


We now can prove Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}. 
\begin{proof}[Proof of Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary}]
To simplify the notations, we define 
\begin{align*}
    \Delta_j &:= \tilde{W}_h^{i,(j)} - \tilde{W}_h^{i, lin,(j)} \in \mathbb{R}^{md}, \\
    G_j &:= \left( g(x^k_h; \tilde{W}_h^{i,(j)}) \right)_{k \in \mathcal{I}_h} \in \mathbb{R}^{md \times K' }, \\ 
    H_j &:= G_j G_j^T \in \mathbb{R}^{md \times md}, \\
    f_j &:= \left( f(x^k_h; \tilde{W}_h^{i,(j)})  \right)_{k \in \mathcal{I}_h} \in \mathbb{R}^{K'} \\ 
    \tilde{y} &:= \left( \tilde{y}^{i,k}_h \right)_{k \in \mathcal{I}_h} \in \mathbb{R}^{K'}.
\end{align*}
The gradient descent update rule for $\tilde{W}_h^{i,(j)}$ in Equation (\ref{eq: GD update in non-linear case}) can be written as: 
\begin{align*}
    \tilde{W}_h^{i,(j+1)} = \tilde{W}_h^{i,(j)} - \eta \left[ G_j (f_j - \tilde{y}) + \lambda (\tilde{W}_h^{i,(j)} + \zeta^i_h - W_0 ) \right]. 
\end{align*}

The auxiliary updates in Equation (\ref{eq: GD update in linear case}) can be written as:
\begin{align*}
    \tilde{W}_h^{i,lin,(j+1)} = \tilde{W}_h^{i,lin,(j)} - \eta \left[ G_0 \left( G_0^T ( \tilde{W}_h^{i,lin,(j)} - W_0) - \tilde{y} \right) + \lambda (\tilde{W}_h^{i,lin,(j)} + \zeta^i_h - W_0 ) \right]. 
\end{align*}

\paragraph{Step 1: Proving $\tilde{W}_h^{i, (j)} \in \mathcal{B}(W_0; R)$ for all $j$.} In the first step, we prove by induction that with probability at least $1 -  MH m^{-2} - \delta$, for any $(i,j,h) \in [M] \times [J] \times [H]$, we have 
\begin{align*}
    \tilde{W}_h^{i, (j)} &\in \mathcal{B}(W_0; R). 
\end{align*}

In the rest of the proof, we consider under the event that Lemma \ref{lemmaq: improved bound f_j - y} holds. Note that the condition in Lemma \ref{lemma: improve bounding GD weight and that of linear auxilary} satisfies that of Lemma \ref{lemmaq: improved bound f_j - y} and under the above event of Lemma \ref{lemmaq: improved bound f_j - y}, Lemma \ref{lemma: NTK approximation error} and Lemma \ref{Lemma: good events where the noises are bounded} both hold. It is trivial that 
\begin{align*}
    \tilde{W}_h^{i, (0)} = W_0 \in \mathcal{B}(W_0; R). 
\end{align*}
For any fix $j \geq 0$, we assume that
\begin{align}
    \tilde{W}_h^{i, (j')} \in \mathcal{B}(W_0; R), \forall j' \in [j]. 
    \label{eq: induction step for proving tilde W h i j near W 0}
\end{align}

We will prove that $\tilde{W}_h^{i, (j+1)} \in \mathcal{B}(W_0; R)$. We have
\begin{align*}
    &\| \Delta_{j+1} \|_2 \\
    &=  \bigg\| (1 - \eta \lambda) \Delta_j - \eta \left[ G_0 (f_j - G_0^T ( \tilde{W}_h^{i,(j)} - W_0)) + G_0 G_0^T (\tilde{W}_h^{i,(j)} - \tilde{W}_h^{i,lin,(j)}) + (f_j - \tilde{y}) (G_j - G_0) \right] \bigg\|_2 \\ 
    &\leq \underbrace{\|(I - \eta (\lambda I + H_0)) \Delta_j \|_2}_{I_1} + \underbrace{\eta \|f_j - \tilde{y} \|_2 \| G_j - G_0 \|_2}_{I_2}  + \underbrace{\eta \| G_0\|_2 \| f_j - G_0^T ( \tilde{W}_h^{i,(j)} - W_0) \|_2}_{I_3}.
\end{align*}
We bound $I_1$, $I_2$ and $I_3$ separately. 




\paragraph{Bounding $I_1$.} For bounding $I_1$,
\begin{align*}
    I_1 &= \|(I - \eta (\lambda I + H_0)) \Delta_j \|_2 \\
    &\leq \|I - \eta (\lambda I + H_0) \|_2 \| \Delta_j \|_2 \\
    &\leq (1 - \eta( \lambda + K' C_g^2)) \| \Delta_j \|_2 \\
    &\leq (1 - \eta \lambda ) \| \Delta_j \|_2
\end{align*}
where the first inequality holds due to the spectral norm inequality, the second inequality holds due to 
\begin{align*}
    \eta (\lambda I + H_0) \preceq \eta (\lambda + \|G_0\|^2) I \preceq  \eta (\lambda + K' C_g^2) I \preceq I,
\end{align*}
where the first inequality holds due to that $H_0 \preceq \|H_0\|_2 I \preceq \|G_0\|_2^2 I$, the second inequality holds due to that $\|G_0 \|_2 \leq \sqrt{K} C_g$ due to Lemma \ref{lemma: NTK approximation error}, and the last inequality holds due to the choice of $\eta$ in Equation (\ref{equation: improved conditions for R and eta comprehensive list}). 

\paragraph{Bounding $I_2$.} For bounding $I_2$,
\begin{align*}
    I_2 &= \eta \|f_j - \tilde{y} \|_2 \| G_j - G_0 \|_2 \\
    &\leq \eta \|f_j - \tilde{y} \|_2  \max_{k \in \mathcal{I}_h} \sqrt{K'} \|g(x^k_h; \tilde{W}_h^{i,(j)}) - g(x^k_h; W_0) \|_2 \\ 
    &\leq \eta \|f_j - \tilde{y} \|_2 \sqrt{K'} C_g R^{1/3} m^{-1/6} \sqrt{ \log m} \\
    &\leq \eta \sqrt{ 2 K'(H + \psi + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2 + 8 C_g R^{4/3} m^{-1/6} \sqrt{ \log m})} \sqrt{K'} C_g R^{1/3} m^{-1/6} \sqrt{ \log m}
\end{align*}
where the first inequality holds due to Cauchy-Schwarz inequality, the second inequality holds due to the induction assumption in Equation (\ref{eq: induction step for proving tilde W h i j near W 0}) and Lemma \ref{lemma: NTK approximation error}, and the third inequality holds due to Lemma \ref{lemmaq: improved bound f_j - y} and the induction assumption in Equation (\ref{eq: induction step for proving tilde W h i j near W 0}). 
\paragraph{Bounding $I_3$.} For bounding $I_3$,
\begin{align*}
I_3 &= \eta \| G_0\|_2 \| f_j - G_0^T ( \tilde{W}_h^{i,(j)} - W_0) \|_2 \\ 
&\leq \eta \sqrt{K'} C_g  \sqrt{K'} \max_{k \in \mathcal{I}_h}| f(x^k_h; \tilde{W}_h^{i, (j)}) - g(x^k_h; W_0)^T ( \tilde{W}_h^{i,(j)} - W_0) | \\ 
&\leq \eta K' C_g R^{4/3} m^{-1/6} \sqrt{\log m},
\end{align*}
where the first inequality holds due to Cauchy-Schwarz inequality and due to that $\| G_0 \|_2 \leq \sqrt{K'} C_g$ and the second inequality holds due to the induction assumption in Equation (\ref{eq: induction step for proving tilde W h i j near W 0}) and Lemma \ref{lemma: NTK approximation error}. 

Combining the bounds of $I_1, I_2, I_3$ above, we have 
\begin{align*}
    \| \Delta_{j+1} \|_2 \leq (1 - \eta \lambda) \| \Delta_j \|_2 + I_2 + I_3.
\end{align*}
Recursively applying the inequality above for all $j$, we have 
\begin{align}
    \| \Delta_j \|_2 \leq \frac{I_2 + I_3}{ \eta \lambda } \leq \frac{R}{4} + \frac{R}{4} = \frac{R}{2}, 
    \label{eq: bound Delta j by R/2}
\end{align}
where the second inequality holds due the choice specified in Equation (\ref{equation: improved conditions for R and eta comprehensive list}). We also have 
\begin{align}
    \lambda \| \tilde{W}^{i,lin,(j+1)}_h + \zeta^i_h - W_0 \|_2^2 &\leq 2 \tilde{L}_h^{i, lin}(\tilde{W}^{i,lin,(j+1)}_h) \nonumber\\ 
    &\leq  2 \tilde{L}_h^{i, lin}(\tilde{W}^{i,lin,(0)}_h) \nonumber\\
    &= 2 \tilde{L}_h^{i, lin}(W_0) \nonumber\\
    &=  \sum_{k \in \mathcal{I}_h} \left( \underbrace{\langle g(x^k_h; W_0), W_0  \rangle}_{=0} -  \tilde{y}^{i,k}_h \right)^2 +\lambda \|  \zeta^i_h  \|_2^2 \nonumber\\
    &=  \sum_{k \in \mathcal{I}_h}  (\tilde{y}^{i,k}_h)^2 +\lambda \|  \zeta^i_h  \|_2^2 \nonumber\\
    &\leq K' (H + \psi + \gamma_{h,1})^2 + \lambda \gamma_{h,2}^2,
    \label{eq: bound tilde W i lin j+1 + zeta i h - W 0}
\end{align}
where the first inequality holds due the the definition of $\tilde{L}_h^{i, lin}(\tilde{W}^{i,lin,(j+1)}_h)$, the second inequality holds due to the monotonicity of $\tilde{L}_h^{i, lin}(W)$ on the gradient descent updates $\{\tilde{W}^{i,lin,(j')}_h\}_{j'}$ for the squared loss on a linear model, the third equality holds due to $\langle g(x^k_h; W_0), W_0  \rangle = 0$ from the symmetric initialization scheme, and the last inequality holds due to Lemma \ref{Lemma: good events where the noises are bounded}. Thus, we have
\begin{align}
     \| \tilde{W}^{i,lin,(j+1)}_h  - W_0 \|_2 &\leq \sqrt{ 2\| \tilde{W}^{i,lin,(j+1)}_h + \zeta^i_h - W_0 \|_2^2 + \| \zeta^i_h \|_2^2 } \nonumber\\ 
     &\leq \sqrt{2\lambda^{-1}K' (H + \psi + \gamma_{h,1})^2 + 4 \gamma_{h,2}^2} \nonumber \\ 
     &\leq \frac{R}{2},
     \label{eq: tilde W i lin j+1 - W0}
\end{align}
where the first inequality holds due to Cauchy-Schwarz inequality, the second inequality holds due to Equation (\ref{eq: bound tilde W i lin j+1 + zeta i h - W 0}) and Lemma \ref{Lemma: good events where the noises are bounded}, and the last inequality holds due to the choice specified in Equation (\ref{equation: improved conditions for R and eta comprehensive list}). 

Combining Equation (\ref{eq: bound Delta j by R/2}) and Equation (\ref{eq: tilde W i lin j+1 - W0}), we have 
\begin{align*}
    \| \tilde{W}_h^{i, (j+1)} - W_0 \|_2 &\leq  \| \tilde{W}_h^{i, (j+1)} - \tilde{W}^{i,lin,(j+1)}_h \|_2 + \| \tilde{W}^{i,lin,(j+1)}_h - W_0 \|_2 \\ 
    &\leq \frac{R}{2} + \frac{R}{2} = R,
\end{align*}
where the first inequality holds due to the triangle inequality. 
\paragraph{Step 2: Bounding $\| \tilde{W}_h^{i, (j)} - \tilde{W}_h^{i,lin} \|_2$.} By the standard result of gradient descent on ridge linear regression, $\tilde{W}_h^{i, (j)}$ converges to $\tilde{W}_h^{i,lin}$ with the convergence rate, 
\begin{align*}
    \|\tilde{W}_h^{i, lin, (j)} - \tilde{W}_h^{i,lin} \|_2^2 &\leq (1 - \eta \lambda)^j \frac{2}{ \lambda} (\tilde{L}(W_0) - \tilde{L}(\tilde{W}_h^{i,lin})) \\ 
    &\leq (1 - \eta \lambda)^j \frac{2}{ \lambda} \tilde{L}(W_0) \\ 
    & \leq \lambda^{-1}(1 - \eta \lambda)^j \left( K'(H + \psi + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2\right).
\end{align*}
Thus, for any $j$, we have 
\begin{align}
    \| \tilde{W}_h^{i, (j)} - \tilde{W}_h^{i,lin}  \|_2 &\leq  \| \tilde{W}_h^{i, (j)} - \tilde{W}^{i,lin,(j)}_h \|_2 + \| \tilde{W}^{i,lin,(j)}_h - \tilde{W}_h^{i,lin}  \|_2 \nonumber \\ 
    &\leq (\eta \lambda)^{-1} (I_2 + I_3) + \lambda^{-1}(1 - \eta \lambda)^j \left( K'(H + \psi + \gamma_{h,1} )^2 + \lambda \gamma_{h,2}^2\right), 
    \label{eq: tilde W i lin j 0 tilde W i lin}
\end{align}
where the first inequality holds due to the triangle inequality, the second inequality holds due to Equation (\ref{eq: bound Delta j by R/2}) and Equation (\ref{eq: tilde W i lin j 0 tilde W i lin}). 
\end{proof}



























\subsection{Proof of Lemma \ref{lemmaq: improved bound f_j - y}} 
\label{subsection: proof of lemma f_j - y}
\begin{proof}[Proof of Lemma \ref{lemmaq: improved bound f_j - y}]
We bound this term following the proof flow of \cite[Lemma~C.3]{zhou2020neural} with modifications for different neural parameterization and noisy targets. Suppose that for some fixed $j$, 
\begin{align}
    \tilde{W}_h^{i,(j')} \in \mathcal{B}(W_0; R), \forall j' \in [j].
    \label{eq: induction condition that tilde W i j' in the ball}
\end{align}
Let us define 
\begin{align*}
G(W) &:= \left( g(x^k_h; W) \right)_{k \in \mathcal{I}_h} \in \mathbb{R}^{md \times K' }, \\ 
f(W) &:= \left( f(x^k_h; W)\right)_{k \in \mathcal{I}_h} \in \mathbb{R}^{K'}, \\ 
    e(W',W) &:= f(W') - f(W) - G(W)^T (W' - W) \in \mathbb{R}.
\end{align*}
To further simplify the notations in this proof, we drop $i,h$ in $\tilde{L}_h^i(W)$ defined in Equation (\ref{eq: perturbed loss function}) to write $\tilde{L}_h^i(W)$ as $\tilde{L}(W)$ and write $W_j = W_h^{i, (j)}$, where
\begin{align*}
    \tilde{L}(W) &= \frac{1}{2}\sum_{k \in \mathcal{I}_h} \left( f(x^k_h; W) - \tilde{y}^{i,k}_h ) \right)^2 + \frac{\lambda}{2} \| W + \zeta^i_h - W_0 \|_2^2 \\ 
    &= \frac{1}{2} \| f(W) - \tilde{y} \|_2^2 +   \frac{\lambda}{2} \| W + \zeta^i_h - W_0 \|_2^2. 
\end{align*}
Suppose that $W \in \mathbb{B}(W_0; R)$. By that $\| \cdot \|_2^2$ is $1$-smooth, 
\begin{align}
    &\tilde{L}(W') - \tilde{L}(W) \leq \langle f(W) - \tilde{y}, f(W') - f(W) \rangle + \frac{1}{2} \| f(W') - f(W) \|_2^2 \nonumber \\
    &+ \lambda \langle W + \zeta^i_h - W_0, W' - W \rangle + \frac{\lambda}{2} \| W' - W\|_2^2 \nonumber \\
    &= \langle f(W) - \tilde{y}, G(W)^T (W' - W) + e(W',W) \rangle + \frac{1}{2} \| G(W)^T (W' - W) + e(W',W) \|_2^2 \nonumber \\
    &+ \lambda \langle W + \zeta^i_h - W_0, W' - W \rangle + \frac{\lambda}{2} \| W' - W\|_2^2 \nonumber \\ 
    &= \langle \nabla \tilde{L}(W), W'-W \rangle  \nonumber\\ 
    &+ \underbrace{\langle f(W) - \tilde{y}, e(W',W) \rangle + \frac{1}{2} \| G(W)^T (W' - W) + e(W',W) \|_2^2 + \frac{\lambda}{2} \| W' - W\|_2^2}_{I_1}. 
    \label{eq: bounding L W' - L W}
\end{align}
For bounding $I_1$, 
\begin{align}
    I_1 &\leq \| f(W) - \tilde{y} \|_2 \| e(W',W) \|_2 +  K' C^2_g \|W' - W\|_2^2 + \|e(W',W) \|_2^2 + \frac{\lambda}{2} \| W' - W\|_2^2 \nonumber \\ 
    &= \| f(W) - \tilde{y} \|_2 \| e(W',W) \|_2 +  (K' C^2_g + \lambda/2) \|W' - W\|_2^2 + \|e(W',W) \|_2^2,
    \label{eq: bounding I1 in  L W' - L W}
\end{align}
where the first inequality holds due to Cauchy-Schwarz inequality, $W \in \mathcal{B}(W_0; R)$ and Lemma \ref{lemma: NTK approximation error}. Substituting Equation (\ref{eq: bounding I1 in  L W' - L W}) into Equation (\ref{eq: bounding L W' - L W}) with $W' = W - \eta \nabla \tilde{L}(W)$,
\begin{align}
    \tilde{L}(W') - \tilde{L}(W) &\leq -\eta (1 - (K C_g^2 + \lambda/2) \eta) \| \nabla \tilde{L}(W) \|_2^2 + \| f(W) - \tilde{y} \|_2 \| e(W',W) \|_2 \nonumber\\
    &+ \|e(W',W) \|_2^2. 
    \label{eq: bounding L W' - L W more explicit}
\end{align}

By the $1$-strong convexity of $\| \cdot\|_2^2$, for any $W'$,
\begin{align}
    &\tilde{L}(W') - \tilde{L}(W) \geq \langle f(W) - \tilde{y}, f(W') - f(W) \rangle  + \lambda \langle W + \zeta^i_h - W_0, W' - W \rangle + \frac{\lambda}{2} \| W' - W\|_2^2 \nonumber \\
    &= \langle f(W) - \tilde{y}, G(W)^T (W' - W) + e(W',W) \rangle + \lambda \langle W + \zeta^i_h - W_0, W' - W \rangle + \frac{\lambda}{2} \| W' - W\|_2^2 \nonumber \\ 
    &= \langle \nabla \tilde{L}(W), W'-W \rangle + \langle f(W) - \tilde{y}, e(W',W) \rangle  + \frac{\lambda}{2} \| W' - W\|_2^2 \nonumber \\
     &\geq - \frac{\| \nabla \tilde{L}(W) \|_2^2}{2 \lambda} - \| f(W) - \tilde{y} \|_2 \| e(W',W)\|_2, 
     \label{eq: bound L W' L W by strong convexity}
\end{align}
where the last inequality holds due to Cauchy-Schwarz inequality. 

Substituting Equation (\ref{eq: bound L W' L W by strong convexity}) into Equation (\ref{eq: bounding L W' - L W more explicit}), for any $W'$,
\begin{align}
    &\tilde{L}( W - \eta \nabla \tilde{L}(W)) - \tilde{L}(W) \nonumber\\
    &\leq \underbrace{2 \lambda \eta (1 - (K C_g^2 + \lambda/2) \eta)}_{\alpha} \left( \tilde{L}(W') - \tilde{L}(W) + \| f(W) - \tilde{y} \|_2 \| e(W',W)\|_2 \right) \nonumber\\
    &+ \| f(W) - \tilde{y} \|_2 \| e( W - \eta \nabla \tilde{L}(W),W) \|_2 + \|e( W - \eta \nabla \tilde{L}(W),W) \|_2^2 \nonumber\\
    &\leq \alpha \left( \tilde{L}(W') - \tilde{L}(W) + \frac{\gamma_1}{2} \| f(W) - \tilde{y} \|_2^2 + \frac{1}{ 2 \gamma_1} \| e(W',W) \|_2^2\right) \nonumber\\
    &+  \frac{\gamma_2}{2} \| f(W) - \tilde{y} \|_2^2 + \frac{1}{ 2 \gamma_2} \| e(W - \eta \nabla \tilde{L}(W),W) \|_2^2 + \|e(W - \eta \nabla \tilde{L}(W),W) \|_2^2 \nonumber\\
    &\leq \alpha \left( \tilde{L}(W') - \tilde{L}(W) + \gamma_1 \tilde{L}(W)  + \frac{1}{ 2 \gamma_1} \| e(W',W) \|_2^2\right) \nonumber\\
    &+  \gamma_2 \tilde{L}(W)  + \frac{1}{ 2 \gamma_2} \| e(W - \eta \nabla \tilde{L}(W),W) \|_2^2 + \|e(W - \eta \nabla \tilde{L}(W),W) \|_2^2,
    \label{eq: bound L W' - L W final stage}
\end{align}
where the second inequality holds due to Cauchy-Schwarz inequality for any $\gamma_1, \gamma_2 > 0$, and the third inequality holds due to  $\| f(W) - \tilde{y} \|_2^2 \leq 2 \tilde{L}(W)$. 

Rearranging terms in Equation (\ref{eq: bound L W' - L W final stage}) and setting $W = W_j$, $W' = W_0$, $\gamma_1 = \frac{1}{4}$, $\gamma_2 = \frac{\alpha}{4}$, 

\begin{align}
\tilde{L}(W_{j+1}) - \tilde{L}(W_0) &\leq (1 -\alpha + \alpha \gamma_1 + \gamma_2 )\tilde{L}(W_j) 
- (1 - \frac{\alpha}{2})  \tilde{L}(W_0) + \frac{\alpha}{2} \tilde{L}(W_0)  \nonumber \\
&+ \frac{\alpha}{ 2 \gamma_1} \| e(W_0,W_j) \|_2^2  + \frac{1}{ 2 \gamma_2} \| e(W_{j+1},W_j) \|_2^2 + \|e(W_{j+1},W_j) \|_2^2 \nonumber\\ 
&= (1 - \frac{\alpha}{2} ) \left(\tilde{L}(W_j) 
-   \tilde{L}(W_0) \right) + \frac{\alpha}{2} \tilde{L}(W_0)  + 2 \alpha \| e(W_0,W_j) \|_2^2  \nonumber\\
&+  (1 + \frac{2}{ \alpha}) \| e(W_{j+1},W_j) \|_2^2 \nonumber\\
&\leq (1 - \frac{\alpha}{2} ) \left(\tilde{L}(W_j) -\tilde{L}(W_0) \right)  + \frac{\alpha}{2} \tilde{L}(W_0)  + (1 + \frac{2}{\alpha} + 2 \alpha) e , 
\label{eq: recursion of L W_j - L W_0}
\end{align}
where $e := C_g R^{4/3} m^{-1/6} \sqrt{ \log m}$, the last inequality holds due to Equation (\ref{eq: induction condition that tilde W i j' in the ball}) and Lemma \ref{lemma: NTK approximation error}. Applying Equation (\ref{eq: recursion of L W_j - L W_0}), we have
\begin{align*}
    \tilde{L}(W_j) - \tilde{L}(W_0) \leq \frac{2}{\alpha} \left( \frac{\alpha}{2} \tilde{L}(W_0)  + (1 + \frac{2}{\alpha} + 2 \alpha) e \right). 
\end{align*}
Rearranging the above inequality, 
\begin{align*}
    \tilde{L}(W_j) &\leq 2 \tilde{L}(W_0) + (\frac{2}{\alpha} + \frac{4}{\alpha^2} + 4) e 
\end{align*}
where the last inequality holds due to the choice of $\eta$. 
Finally, we have
\begin{align*}
    \| f_j - \tilde{y}\|_2^2 \leq 2 \tilde{L}(W_j)  
\end{align*}
and $\tilde{L}(W_0) = \frac{1}{2} \| \tilde{y} \|_2^2 + \frac{\lambda}{2} \| \zeta^i_h \|_2^2 \leq  \frac{K'}{2}(H + \psi + \gamma_{h,1} )^2 + \frac{\lambda}{2} \gamma_{h,2}^2$ due to Lemma \ref{Lemma: good events where the noises are bounded}.
\end{proof}
 \section{Support Lemmas}
\begin{lemma}


Let $m = \Omega \left( d^{3/2}  R^{-1} \log^{3/2} (\sqrt{m} / R) \right)$ and $R = \mathcal{O} \left( m^{1/2} \log^{-3} m \right)$. With probability at least $1 - e^{-\Omega(\log^2 m)} \geq 1 - m^{-2}$ with respect to the random initialization, it holds for any $W, W' \in \mathcal{B}(W_0; R)$ and $x \in \mathbb{S}_{d-1}$ that
\begin{align*}
    \| g(x; W) \|_2 &\leq C_g, \\ 
    \| g(x; W) - g(x; W_0) \|_2 &\leq \mathcal{O} \left( C_g R^{1/3} m^{-1/6} \sqrt{\log m} \right), \\ 
    |f(x; W) - f(x; W') - \langle g(x; W'), W - W' \rangle| &\leq \mathcal{O} \left( C_g R^{4/3} m^{-1/6} \sqrt{\log m} \right),
\end{align*}
where $C_g = \mathcal{O}(1)$ is a constant independent of $d$ and $m$. Moreover, without loss of generality, we assume $C_g \leq 1$.  
\label{lemma: NTK approximation error}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma: NTK approximation error}]
Due to \cite[Lemma~C.2]{yang2020function} and \cite[Lemma~F.1, F.2]{cai2019neural}, we have the first two inequalities and the following: 
\begin{align*}
    |f(x; W) - \langle g(x; W_0), W - W_0 \rangle| &\leq \mathcal{O} \left( C_g R^{4/3} m^{-1/6} \sqrt{\log m} \right).
\end{align*}
For any $W, W' \in \mathcal{B}(W_0; R)$, 
\begin{align*}
    &f(x; W) - f(x; W') - \langle g(x; W'), W - W' \rangle \\
    &= f(x; W) - \langle g(x; W_0), W - W_0 \rangle - ( f(x; W') - \langle g(x; W_0), W' - W_0 \rangle ) \\
    &+ \langle g(x; W_0) - g(x; W'), W_0 - W' \rangle. 
\end{align*}
Thus, 
\begin{align*}
    &|f(x; W) - f(x; W') - \langle g(x; W'), W - W' \rangle | \\
    &\leq |f(x; W) - \langle g(x; W_0), W - W_0 \rangle| + | f(x; W') - \langle g(x; W_0), W' - W_0 \rangle | \\
    &+  \| g(x; W_0) - g(x; W')\|_2 \| W_0 - W' \|_2 \leq  \mathcal{O} \left( C_g R^{4/3} m^{-1/6} \sqrt{\log m} \right).
\end{align*}
\end{proof}

\begin{lemma}[{\cite[Theorem~3]{arora2019exact}}]
\label{lemma: ntk gram matrix versus grad gram matrix}
If $m = \Omega(\epsilon^{-4} \log(1/\delta))$, then for any $x, x' \in \mathcal{X} \subset \mathbb{S}_{d-1}$, with probability at least $1 - \delta$, 
\begin{align*}
    |\langle g(x; W_0), g(x', W_0) \rangle  - K_{ntk}(x,x') | \leq 2 \epsilon. 
\end{align*}
\end{lemma}


\begin{lemma}
Let $X \sim \mathcal{N}(0, a \Lambda^{-1})$ be a $d$-dimensional normal variable where $a$ is a scalar. There exists an absolute constant $c > 0$ such that for any $\delta > 0$, with probability at least $1 - \delta$, 
\begin{align*}
    \| X \|_{\Lambda} \leq c \sqrt{d a \log (d/\delta)}. 
\end{align*}
For $d = 1$, $c = \sqrt{2}$. 

\label{lemma: Concentration of multivariate Gaussian}
\end{lemma}

\begin{lemma}[A variant of Hoeffding-Azuma inequality]
Suppose $\{Z_k\}_{k = 0}^{\infty}$ is a real-valued stochastic process with corresponding filtration $\{\mathcal{F}_{k}\}_{k=0}^{\infty}$, i.e. $\forall k $, $Z_k$ is $\mathcal{F}_k$-measurable. Suppose that for any $k$, $\mathbb{E}[|Z_k|] < \infty$ and $|Z_k - \mathbb{E} \left[ Z_k | \mathcal{F}_{k-1} \right]| \leq c_k$ almost surely. Then for all positive $n$ and $t$, we have: 
\begin{align*}
    \mathbb{P}\left( \bigg|\sum_{k=1}^n Z_k  -  \sum_{k=1}^n  \mathbb{E} \left[ Z_k | \mathcal{F}_{k-1} \right] \bigg| \geq t \right) \leq 2 \exp \left( \frac{-t^2}{\sum_{i=1}^n c_i^2} \right).
\end{align*}
\label{lemma:azuma}
\end{lemma}

\begin{lemma}[{\citep[Theorem~1]{chowdhury2017kernelized}}]
Let $\mathcal{H}$ be an RKHS defined over $\mathcal{X} \subseteq \mathbb{R}^d$. Let $\{x_t\}_{t=1}^{\infty}$ be a discrete time stochastic process adapted to filtration $\{\mathcal{F}_t\}_{t=0}^{\infty}$. Let $\{Z_k\}_{k=1}^{\infty}$ be a real-valued stochastic process such that $Z_k \in \mathcal{F}_k$, and $Z_k$ is zero-mean and $\sigma$-sub Gaussian conditioned on $\mathcal{F}_{k-1}$. Let $E_k = (Z_1, \ldots, Z_{k-1})^T \in \mathbb{R}^{k-1}$ and $\mathcal{K}_k$ be the Gram matrix of $\mathcal{H}$ defined on $\{x_t\}_{t \leq k - 1}$. For any $\rho > 0$ and $\delta \in (0,1)$, with probability at least $1 - \delta$, 
\begin{align*}
    E_k^T \left[ (\mathcal{K}_k + \rho I)^{-1} + I \right]^{-1} E_k \leq \sigma^2 \logdet \left[ (1 + \rho) I + \mathcal{K}_k\right] + 2 \sigma^2 \log (1 / \delta). 
\end{align*}
\label{lemma: self-normalized concentration process in RKHS}
\end{lemma}


\begin{lemma}
For any matrices $A$ and $B$ where $A$ is invertible, 
\begin{align*}
    \logdet(A + B) \leq \logdet(A) + tr(A^{-1} B). 
\end{align*}
\end{lemma}

\begin{lemma}
For any invertible matrices $A, B$, 
\begin{align*}
    \| A^{-1} - B^{-1} \|_2 \leq \frac{\| A - B\|_2}{ \lambda_{\min}(A) \lambda_{\min}(B) }. 
\end{align*}
\label{lemma: difference of two inverse matrices}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma: difference of two inverse matrices}]
We have: 
\begin{align*}
    \|A^{-1} - B^{-1} \|_2 &= \| (AB)^{-1} (AB) (A^{-1} - B^{-1}) \|_2 \\
    &=\| (AB)^{-1} (ABA^{-1} - A) \|_2 \\ 
    &\leq \| (AB)^{-1} \|_2 \| ABA^{-1} - A \|_2 \\ 
    &= \| (AB)^{-1} \|_2 \| ABA^{-1} - AAA^{-1} \|_2 \\ 
    &= \| (AB)^{-1} \|_2 \| A(B - A)A^{-1} \|_2 \\ 
    &= \| (AB)^{-1} \|_2 \| B - A \|_2 \\ 
    &\leq \lambda_{\max}(A^{-1}) \lambda_{\max}(B^{-1}) \|_2 \| B - A \|_2.
\end{align*}
\end{proof}




\begin{lemma}[Freedman's inequality \citep{tropp2011freedman}]
Let $\{X_k\}_{k=1}^n$ be a real-valued martingale difference sequence with the corresponding filtration $\{\mathcal{F}_k\}_{k=1}^n$, i.e. $X_k$ is $\mathcal{F}_{k}$-measurable and $\mathbb{E}[X_k | \mathcal{F}_{k-1}] = 0$. Suppose for any $k$, $|X_k| \leq M$ almost surely and define $V:= \sum_{k=1}^n \mathbb{E}\left[ X_k^2 | \mathcal{F}_{k-1} \right]$. For any $a,b > 0$, we have:
\begin{align*}
    \mathbb{P}\left( \sum_{k=1}^n X_k \geq a, V \leq b \right) \leq \exp \left( \frac{-a^2}{2b + 2 a M/3} \right). 
\end{align*}
In an alternative form, for any $t > 0$, we have: 
\begin{align*}
    \mathbb{P}\left( \sum_{k=1}^n X_k \geq  \frac{2Mt}{3} + \sqrt{2bt}, V \leq b \right) \leq e^{-t} . 
\end{align*}
\label{lemma:freedman}
\end{lemma}

\begin{lemma}[Improved online-to-batch argument \cite{nguyen2022instance}]
Let $\{X_k\}$ be any real-valued stochastic process adapted to the filtration $\{\mathcal{F}_k\}$, i.e. $X_k$ is $\mathcal{F}_k$-measurable. Suppose that for any $k$, $X_k \in [0,H]$ almost surely for some $H > 0$. For any $K > 0$, with probability at least $1 - \delta$, we have:
\begin{align*}
    \sum_{k=1}^K \mathbb{E} \left[ X_k | \mathcal{F}_{k-1} \right] \leq 2 \sum_{k=1}^K X_k + \frac{16}{3} H \log(\log_2(KH)/\delta) + 2 . 
\end{align*}
\label{lemma:improved_online_to_batch}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:improved_online_to_batch} ]
Let $Z_k = X_k - \mathbb{E} \left[ X_k | \mathcal{F}_{k-1} \right]$ and $f(K) = \sum_{k=1}^K \mathbb{E} \left[ X_k | \mathcal{F}_{k-1} \right]$. We have $Z_k$ is a real-valued difference martingale with the corresponding filtration $\{\mathcal{F}_k\}$ and that
\begin{align*}
    V: = \sum_{k=1}^K \mathbb{E} \left[ Z_k^2 | \mathcal{F}_{k-1} \right] \leq \sum_{k=1}^K \mathbb{E} \left[ X_k^2 | \mathcal{F}_{k-1} \right] \leq H \sum_{k=1}^K \mathbb{E} \left[ X_k | \mathcal{F}_{k-1} \right]
    = H f(K). 
\end{align*}
Note that $|Z_k| \leq H$  and $f(K) \in [0, KH]$ and let $m = \log_2(KH)$.
Also note that $f(K) = \sum_{k=1}^K X_k - \sum_{k=1}^K Z_k \geq - \sum_{k=1}^K Z_k$. Thus if $\sum_{k=1}^K Z_k \leq -1$, we have $f(K) \geq 1$. For any $t > 0$, leveraging the peeling technique \citep{bartlett2005local}, we have:
\begin{align*}
    &\mathbb{P} \left( \sum_{k=1}^K Z_k \leq -\frac{2 H t}{3} - \sqrt{ 4 H f(K) t } - 1 \right) \\
    &= \mathbb{P} \left( \sum_{k=1}^K Z_k \leq -\frac{2 H t}{3} - \sqrt{ 4 H f(K) t  } - 1, f(K) \in [1, KH]\right) \\ 
    &\leq \sum_{i=1}^m \mathbb{P} \left( \sum_{k=1}^K Z_k \leq -\frac{ 2 H t}{3} - \sqrt{ 4 H f(K) t } - 1, f(K) \in [2^{i-1}, 2^i)\right) \\ 
    &\leq \sum_{i=1}^m \mathbb{P} \left( \sum_{k=1}^K Z_k \leq -\frac{2 H t}{3} - \sqrt{ 4 H 2^{i-1} t } - 1, V \leq H 2^i, f(K) \in [2^{i-1}, 2^i)\right) \\ 
    &\leq \sum_{i=1}^m \mathbb{P} \left( \sum_{k=1}^K Z_k \leq -\frac{2 H t}{3} - \sqrt{ 2 H 2^i t }, V \leq H 2^i \right) \\ 
&\leq \sum_{i=1}^m e^{-t} = m e^{-t},
\end{align*}
where the first equation is by that $\sum_{k=1}^K Z_k \leq -\frac{2 H t}{3} - \sqrt{ 4 H f(K) t } - 1 \leq -1$ thus $f(K) \geq 1$, the second inequality is by that $V \leq H f(K)$, and the last inequality is by Lemma \ref{lemma:freedman}. Thus, with probability at least $1 - m e^{-t}$, we have:
\begin{align*}
    \sum_{k=1}^K X_k - f(K) = \sum_{k=1}^K Z_k \geq -\frac{2 H t}{3} - \sqrt{ 4 H f(K) t } - 1.
    \label{eq:temp}
\end{align*}
The above inequality implies that $f(K) \leq 2 \sum_{k=1}^K X_k + 4Ht/3 + 2 + 4 Ht $, due to the simple inequality: if $ x \leq a \sqrt{x} + b$, $x \leq a^2 + 2b$. Then setting $t = \log(m/\delta)$ completes the proof. 
\end{proof}
 \section{Baseline algorithms}
\label{section: baseline algorithms}
For completeness, we give the definition of linear MDPs as follows.
\begin{defn}[Linear MDPs \citep{yang2019sample,jin2020provably}]
An MDP has a linear structure if for any $(s,a,s',h)$, we have: 
\begin{align*}
    r_h(s,a) = \phi_h(s,a)^T \theta_h, \mathbb{P}_h(s'|s,a) = \phi_h(s,a)^T \mu_h(s'),
\end{align*}
where $\phi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^{d_{lin}}$ is a known feature map, $\theta_h \in \mathbb{R}^{d_{lin}}$ is an unknown vector, and $\mu_h: \mathcal{S} \rightarrow \mathbb{R}^{d_{lin}}$ are unknown signed measures. 
\label{definition:linear_mdp}
\end{defn}

We also give the details of the baseline algorithms: LinLCB in Algorithm \ref{algorithm: LinLCB}, LinGreedy in Algorithm \ref{algorithm: LinGreedy}, Lin-VIPeR in Algorithm \ref{algorithm: LinPER}, NeuraLCB in Algorithm \ref{algorithm: NeuraLCB} and NeuralGreedy in Algorithm \ref{algorithm: NeuralGreedy}. For simplicity, we do not use data split in these algorithms presented here.  

\begin{algorithm}[h!]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data $ \mathcal{D} = \{(s_h^k, a_h^k, r_h^k)\}_{h \in [H]}^{k \in [K]} $, uncertainty multiplier $\beta$, regularization parameter $\lambda$. 
\State Initialize $\tilde{V}_{H+1}(\cdot) \leftarrow 0$
\For{$h = H, \ldots, 1$}
\State $\Lambda_h \leftarrow \sum_{k=1}^K \phi(s^k_h, a^k_h) \phi(s^k_h, a^k_h)^T + \lambda I$


\State $\hat{\theta}_h \leftarrow \Sigma_h^{-1} \sum_{k=1}^{K} \phi_h(s_h^k, a_h^k) \cdot (r^k_h + \hat{V}_{h+1}(s^k_{h+1}))$

 \State $b_h(\cdot,\cdot) \leftarrow \beta \cdot  \| \phi_h(\cdot, \cdot) \|_{\Sigma_h^{-1}}$. 
\label{bpvi:lcb}
\State $\hat{Q}_h(\cdot, \cdot) \leftarrow \min\{\langle \phi_h(\cdot, \cdot), \hat{\theta}_h \rangle - b_h(\cdot, \cdot), H - h +1\}^+$. 
\State $\hat{\pi}_h \leftarrow  \argmax_{\pi_h } \langle \hat{Q}_h, \pi_h \rangle$ and $\hat{V}_h^k \leftarrow \langle \hat{Q}_h^k, \pi_h^k \rangle$.





\EndFor
\State \textbf{Output}: $\hat{\pi} = \{ \hat{\pi}_h \}_{h \in [H]}$
\end{algorithmic}
\caption{LinLCB \citep{jin2021pessimism}}
\label{algorithm: LinLCB}
\end{algorithm}

\begin{algorithm}[h!]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data $ \mathcal{D} = \{(s_h^k, a_h^k, r_h^k)\}_{h \in [H]}^{k \in [K]} $, perturbed variances $\{\sigma_h\}_{h \in [H]}$, number of bootstraps $M$, regularization parameter $\lambda$. 
\State Initialize $\tilde{V}_{H+1}(\cdot) \leftarrow 0$
\For{$h = H, \ldots, 1$}


\State $\hat{\theta}_h \leftarrow \Sigma_h^{-1} \sum_{k=1}^{K} \phi_h(s_h^k, a_h^k) \cdot (r^k_h + \hat{V}_{h+1}(s^k_{h+1}))$

\label{bpvi:lcb}
\State $\hat{Q}_h(\cdot, \cdot) \leftarrow \min\{\langle \phi_h(\cdot, \cdot), \hat{\theta}_h \rangle, H - h +1\}^+$. 
\State $\hat{\pi}_h \leftarrow  \argmax_{\pi_h } \langle \hat{Q}_h, \pi_h \rangle$ and $\hat{V}_h^k \leftarrow \langle \hat{Q}_h^k, \pi_h^k \rangle$.





\EndFor
\State \textbf{Output}: $\hat{\pi} = \{ \hat{\pi}_h \}_{h \in [H]}$
\end{algorithmic}
\caption{LinGreedy}
\label{algorithm: LinGreedy}
\end{algorithm}


\begin{algorithm}[h!]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data $ \mathcal{D} = \{(s_h^k, a_h^k, r_h^k)\}_{h \in [H]}^{k \in [K]} $, perturbed variances $\{\sigma_h\}_{h \in [H]}$, number of bootstraps $M$, regularization parameter $\lambda$. 
\State Initialize $\tilde{V}_{H+1}(\cdot) \leftarrow 0$
\For{$h = H, \ldots, 1$}
\State $\Lambda_h \leftarrow \sum_{k=1}^K \phi(s^k_h, a^k_h) \phi(s^k_h, a^k_h)^T + \lambda I$
\For{$i = 1, \ldots, M$}
\State Sample $\{\xi^{\tau, i}_h\}_{\tau \in [K]} \sim \mathcal{N}(0, \sigma^2_h)$ and $\zeta^i_h = \{\zeta^{j, i}_h\}_{j \in [d]} \sim \mathcal{N}(0, \sigma^2_h I_d)$


\State Solve the perturbed regularized least-squares regression: 
\label{RAVI-Lin: perturbed ERM}
\begin{align*}
   \tilde{\theta}^i_h &\leftarrow \argmax_{\theta \in \mathbb{R}^d} \sum_{k=1}^K \left(\langle \phi(s^k_h, a^k_h), \theta \rangle - (r^k_h + \tilde{V}_{h+1}(s^k_{h+1}) + \xi^{k,i}_h ) \right)^2 + \lambda \| \theta + \zeta^i_h \|_2^2, 
\end{align*}

\EndFor 

\State Compute $\tilde{Q}_h(\cdot, \cdot) \leftarrow \min \{\min_{i \in [M]} \langle \phi(\cdot, \cdot), \tilde{\theta}^i_h \rangle, H - h +1 \}^{+}$
\State $\tilde{\pi}_{h} \leftarrow \argmax_{\pi_{h}}\langle \tilde{Q}_{h}, \pi_{h} \rangle$ and $\tilde{V}_h \leftarrow \langle \tilde{Q}_{h}, \tilde{\pi}_{h} \rangle$
\EndFor
\State \textbf{Output}: $\tilde{\pi} = \{ \tilde{\pi}_h \}_{h \in [H]}$
\end{algorithmic}
\caption{Lin-VIPeR}
\label{algorithm: LinPER}
\end{algorithm}


\begin{algorithm}[h]
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data $ \mathcal{D} = \{(s_h^k, a_h^k, r_h^k)\}_{h \in [H]}^{k \in [K]} $, neural networks $\mathcal{F} = \{f(\cdot, \cdot; W): W \in \mathcal{W}\} \subset \{\mathcal{X} \rightarrow \mathbb{R}\}$, uncertainty multiplier $\beta$, regularization parameter $\lambda$, step size $\eta$, number of gradient descent steps $J$  
\State Initialize $\tilde{V}_{H+1}(\cdot) \leftarrow 0$ and initialize $f(\cdot, \cdot; W)$ with initial parameter $W_0$
\For{$h = H, \ldots, 1$}


\State $\hat{W}_h \leftarrow \textrm{GradientDescent}(\lambda, \eta, J, \{(s_h^k, a_h^k, r_h^k)\}_{k \in [K]}, 0, W_0)$ (Algorithm \ref{algo:GD})


\State $\Lambda_h = \lambda I + \sum_{k=1}^K g(s^k_h, a^k_h; \hat{W}_h) g(x^k_h; \hat{W}_h)^T$



\State Compute $\hat{Q}_h(\cdot, \cdot) \leftarrow \min \{ f(\cdot, \cdot; \hat{W}_h) - \beta \| g(\cdot, \cdot; \hat{W}_h) \|_{\Lambda_h^{-1}}, H - h +1  \}^{+}$






\State $\hat{\pi}_{h} \leftarrow \argmax_{\pi_{h}}\langle \hat{Q}_{h}, \pi_{h} \rangle$ and $\hat{V}_h \leftarrow \langle \hat{Q}_{h}, \hat{\pi}_{h} \rangle$
\EndFor
\State \textbf{Output}: $\hat{\pi} = \{ \hat{\pi}_h \}_{h \in [H]}$.
\end{algorithmic}
\caption{NeuraLCB (a modification of \citep{nguyen2021offline})}
\label{algorithm: NeuraLCB}
\end{algorithm}


\begin{algorithm}
\begin{algorithmic}[1]
\State \textbf{Input}: Offline data $ \mathcal{D} = \{(s_h^k, a_h^k, r_h^k)\}_{h \in [H]}^{k \in [K]} $, neural networks $\mathcal{F} = \{f(\cdot, \cdot; W): W \in \mathcal{W}\} \subset \{\mathcal{X} \rightarrow \mathbb{R}\}$, uncertainty multiplier $\beta$, step size $\eta$, number of gradient descent steps $J$  
\State Initialize $\tilde{V}_{H+1}(\cdot) \leftarrow 0$ and initialize $f(\cdot, \cdot; W)$ with initial parameter $W_0$
\For{$h = H, \ldots, 1$}


\State $\hat{W}_h \leftarrow \textrm{GradientDescent}(\lambda, \eta, J, \{(s_h^k, a_h^k, r_h^k)\}_{k \in [K]}, 0, W_0)$ (Algorithm \ref{algo:GD})






\State Compute $\hat{Q}_h(\cdot, \cdot) \leftarrow \min \{ f(\cdot, \cdot; \hat{W}_h), H - h +1  \}^{+}$






\State $\hat{\pi}_{h} \leftarrow \argmax_{\pi_{h}}\langle \hat{Q}_{h}, \pi_{h} \rangle$ and $\hat{V}_h \leftarrow \langle \hat{Q}_{h}, \hat{\pi}_{h} \rangle$
\EndFor
\State \textbf{Output}: $\hat{\pi} = \{ \hat{\pi}_h \}_{h \in [H]}$.
\end{algorithmic}
\caption{NeuralGreedy}
\label{algorithm: NeuralGreedy}
\end{algorithm}

 
\end{document}
