\label{sec:experiments}

\subsection{Implementation Details}
\label{subsec:implementation}


\paragraph{\bf General Details}
The 2D backbone for estimating boxes and 2D pose heatmaps is ResNet-152 \cite{he2016deep}. The backbone for estimating depth map from heatmaps is ResNet-18 \cite{he2016deep}. The input image is resized to  and the size of the resulting heatmap is . We adopt Adam \cite{kingma2015adam} as optimizer, the learning rate is  and the batch size is . The 2D backbone network is trained for  epochs. The depth estimator, REN and PEN are trained end-to-end with synthetic heatmaps and their GT 3D poses for  epochs. 


\paragraph{\bf Training Data}
For the experiments on CMU Panoptic\cite{joo2015panoptic}, the backbone is trained on the combined COCO\cite{lin2014microsoft} and CMU Panoptic datasets. When synthesizing AGR training data, the absolute 3D poses and camera views are the same as the original training data in Panoptic dataset. For the MuPoTS-3D\cite{mehta2018single} experiment, the backbone is trained on the combined COCO and MuCo-3DHP\cite{mehta2018single} datasets. When synthesizing AGR training data, the relative 3D poses and camera views are from MuCo-3DHP, and we randomly place the poses in 3D space to enhance the diversity of the training data.


\begin{table}
    \centering
    \caption{Comparison to the state-of-the-art methods on the CMU Panoptic dataset. The metric is MPJPE (mm). \cite{wang2020hmor} uses the GT depth when estimating absolute 3D poses so it is not fairly comparable to other methods. Our method does not use paired images and poses data for training but it still achieves smaller errors than other methods.}
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{l|ccccc} 
    \thickhline 
    Method & Haggling &  Mafia & Ultimatum & Pizza & Mean  \\
    \hline
    \rowcolor{mygray}
    Li \etal (\textbf{HMOR}) \cite{wang2020hmor} & 50.9 & 50.5 & 50.7 & 68.2 & 55.1 \\
    \hline
    PoPa \etal \cite{popa2017deep} & 217.9 & 187.3 & 193.6 & 221.3 & 203.4 \\
    \rowcolor{mygray}
    Zanfir \etal \cite{zanfir2018monocular} &  140.0 & 165.9 & 150.7 & 156.0 & 153.4 \\
    Moon \etal (\textbf{RootNet}) \cite{moon2019camera} &  89.6 & 91.3 & 79.6 & 90.1 & 87.6 \\
    \rowcolor{mygray}
    Zanfir \etal \cite{zanfir2018deep} &  72.4 & 78.8 & 66.8 & 94.3 & 78.1 \\
    Zhen \etal (\textbf{SMAP}) \cite{zhen2020smap} & \underline{63.1} & \textbf{60.3} & \underline{56.6} & \underline{67.1} & \underline{61.8} \\
    \hline
    \rowcolor{mygray}
    Ours & \textbf{54.1} & \underline{61.6} & \textbf{54.6} & \textbf{65.4} & \textbf{58.9} \\
    \thickhline
    \end{tabular}}
    \label{tab:sota_panoptic}
\end{table}

\subsection{Comparison to the State-of-the-arts}
\label{subsec:comparision}

\paragraph{\bf CMU Panoptic}\cite{joo2015panoptic} Following \cite{zanfir2018monocular}, we use the video sequences of camera  and  as training and test data which consist of four activities, \ie Haggling, Mafia, Ultimatum, Pizza. We show the results in Table ~\ref{tab:sota_panoptic}. We achieve better results than previous methods except for HMOR \cite{wang2020hmor}  which uses the GT depth when estimating the absolute 3D pose. Besides, our method achieves the best performance in the Pizza sequence which is not included in the training data. This partially validates the generalization ability of our method.


\paragraph{\bf MuPoTS-3D}\cite{mehta2018single} \
Our method is trained on the synthetic data where the 3D poses are from the MuCo-3DHP dataset. The rest methods are trained end-to-end on paired images and 3D poses from the MuCo-3DHP dataset. Following the standard practice on this dataset, the metric of percentage of correct keypoints (3DPCK) is used to measure the estimation results.  The results are shown in Table \ref{tab:sota_mupots}.  measures the accuracy of absolute pose and  measures the accuracy of the root joint. We can see that our method achieves significantly better depth and pose estimation results than the state-of-the-arts. It validates that our absolute 3D depth estimation method has strong generalization capability. We hope to emphasize the importance of the results because it means the method has the potential to be applied in the wild environments.

\begin{table}[t]
    \centering
    \caption{Comparison to the state-of-the-art methods on MuPoTS-3D. \emph{Matched people} only computes accuracy for GT poses which are matched to predictions and \emph{All people} computes accuracy for all GT poses in the dataset. The methods are not strictly comparable because they may have different backbones.}
    \setlength{\tabcolsep}{3.8mm}{
    \begin{tabular}{l|cc|c} 
    \thickhline 
    \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Matched people} &  \multicolumn{1}{c}{All people} \\
    \cline{2-4}
    &  &  &  \\
    \hline
    \rowcolor{mygray}
    Moon \etal (\textbf{RootNet}) \cite{moon2019camera} & 31.8 & 31.0 & 31.5 \\
    Lin \etal (\textbf{HDNet}) \cite{lin2020hdnet} & 35.2 & 39.4 & - \\
    \rowcolor{mygray}
    Zhen \etal (\textbf{SMAP}) \cite{zhen2020smap} & 38.7 & 45.5 &  35.4 \\
    Veges \etal \cite{veges2020multi} & 39.6 & - & 37.3 \\
    \rowcolor{mygray}
    Sarandi \etal \cite{sarandi2020metrabs} & 40.5 & - & 38.4 \\
    Li \etal (\textbf{HMOR}) \cite{wang2020hmor} & - & - & 43.8 \\
    \rowcolor{mygray}
    Guo \etal \cite{guo2021monocular} & 39.6 & - & 39.2 \\
    
    \hline
    
    Ours & \textbf{47.0} & \textbf{53.5} & \textbf{44.0} \\

    \thickhline
    \end{tabular}}
    \label{tab:sota_mupots}
\end{table}




\subsection{Ablation Study}
\label{subsec:ablation}

In this section, we will evaluate the impact of our proposed modules and the training strategies to the estimation results of the root joint. 


\paragraph{\bf Root Estimation Network} We first introduce two baselines. Baseline (a) analytically computes the 3D root position based on the 2D position and the estimated depth map. The baseline (b) uses the 3D root estimator to estimate the 3D root position on top of (a). The results on the CMU Panoptic and MuPoTS-3D datasets are shown in Table \ref{tab:ablation_effect}. By comparing the results of (a) and (b), we can see that the root depth error has a significant reduction which validates the effectiveness of the 3D root estimator. 


\paragraph{\bf Pose Estimation Network}
By comparing the results of (b) and (c) in Table \ref{tab:ablation_effect}, we can see that depth estimation can be notably improved by PEN, which leverages the rest of the body joints to refine the root joint. Another reason for the improvement is that the quantization error is reduced by computing continuous root locations via the integral trick.


\paragraph{\bf Depth Based Feature Projection} As stated in Section \ref{subsec:root_estimation_network}, we construct the 3D feature volume by projecting 2D pose heatmaps based on the estimated depths and bounding boxes. We compare it to a baseline which naively projects the heatmaps to all voxels as in \cite{iskakov2019learnable, tu2020voxelpose} and find that  and  of our method are significantly better than the baseline (mm \emph{vs.} mm, and mm \emph{vs.} mm) on the MuPoTS-3D dataset.


\begin{table}
    \centering
    \caption{Ablation study on Root Estimation Network (REN) and Pose Estimation Network (PEN) in our method. We report the  and  (mm) on the test set of CMU Panoptic and MuPoTS-3D dataset. }
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{l|c|c|cc|cc} 
    \thickhline 
    \multirow{2}{*}{Method} & \multirow{2}{*}{REN} & \multirow{2}{*}{PEN} & \multicolumn{2}{c|}{CMU Panoptic} & \multicolumn{2}{c}{MuPoTS-3D} \\
    \cline{4-7}
    & & &  &  &  &   \\
    \hline
    \rowcolor{mygray}
    (a) DE & 2D & \textcolor{red}{\xmark} & 113.9  & 104.1 &  282.0 & 245.4  \\	
    (b) REN & 2D+3D & \textcolor{red}{\xmark} & 115.7 & 93.6  & 217.5 & 165.0   \\	\rowcolor{mygray}
    (c) Ours & 2D+3D & \textcolor{mygreen}{\cmark} &\textbf{97.0} & \textbf{86.0} &  \textbf{204.0} & \textbf{159.8}  \\
    \thickhline
    \end{tabular}}
    \label{tab:ablation_effect}
\end{table}



\paragraph{\bf Training Data Generation Strategies} 
We compare several training data generation strategies for different scenarios. First, when we have little knowledge about the testing camera view point, we can only uniformly sample camera views for the virtual camera to synthesize training data. The results are shown in Table \ref{tab:experiment_random_view}. We can see that by training a universal model, our method achieves smaller depth estimation errors than the model trained on a single camera.

Second, if we know the camera view point in the testing environment, we can generate training data specifically for it. If we train a model for camera 19, then the testing error on camera 19 will be decreased significantly to mm. This is helpful when the camera is fixed, \eg deployed at home for child or elderly care. In this case, we can get very accurate estimation results. We have similar observations for the pose factor. In particular, if we have the pose information for the people who are going to appear in the camera, then we can generate AGR using those poses. In that case, the testing accuracy will also be significantly improved. For example, if we train the model using the poses from the MuPoTS-3D dataset, then the  will be improved from  to . 










\begin{table}
    \centering
    \caption{Impact of training data generation strategies. The metric is  (mm). We can see that by training a universal model, our method achieves smaller depth estimation errors than the model trained on a single camera.}
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{l|c|c|c} 
    \thickhline 
     \multirow{2}{*}{Training Camera View} & \multicolumn{3}{c}{Test Camera view} \\
    \cline{2-4}
    & 14 & 16 & 19 \\
    \hline
    \rowcolor{mygray}
    14 & - & 170.3 & 294.6 \\
    16 & 454.6 & - & 198.6 \\
    \rowcolor{mygray}
    19 & 621.3 & 385.1 & - \\
    \hline
    Random & \textbf{273.8} & \textbf{134.0} & \textbf{155.2} \\
    \thickhline
    \end{tabular}}
    \label{tab:experiment_random_view}
\end{table}

\subsection{Qualitative Results}
\label{subsec:qualitative}

Fig. \ref{fig:experiment_quality}(a) shows some estimation results for images from the COCO dataset and MuPoTS-3D dataset. Since the camera parameters are not provided in the COCO dataset, we choose a general focal length (\ie 1400) and assume the pitch angle of the camera is zero. We can see that our approach not only obtains accurate 3D pose for each person in the image but also estimates their absolute depth correctly in the 3D space. In particular, the model is robust to pose and background variations. For instance, in the baseball example, we get correct depth estimate for the person in sitting posture. Fig. \ref{fig:experiment_quality}(b) and (c) show some typical failure cases. In Fig. \ref{fig:experiment_quality}(b), the man in the red circle is occluded and truncated, while in Fig. \ref{fig:experiment_quality}(c), the little girl is much shorter than the people in the training dataset. In the future, to address the second issue, we will study the possibility of adding another parallel branch in the 2D network to estimate a correction factor to refine the estimated depth of each person.



\begin{figure}[t]
	\centering
	\includegraphics[width=5in]{imgs/quality_results.pdf}
    \caption{(a) Some pose estimation results on the COCO (top) and MuPoTS-3D (bottom) datasets. (b) Typical failure cases due to occlusion and truncation. (c) Typical failure cases due to the person is shorter than those in the training dataset. As a result, the estimated depth is larger than GT.}
	\label{fig:experiment_quality}
\end{figure}




