\documentclass{article}









\usepackage[final,nonatbib]{neurips_2021}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \hypersetup{colorlinks}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
\setlist[itemize]{leftmargin=*}

\def\eg{\emph{e.g.}}
\def\ie{\emph{i.e.}}
\def\etc{\emph{etc}}
\def\vs{\emph{vs}~}
\newcommand{\ycnote}[1]{\textcolor{red}{\small [#1 --YC]}}
\newcommand{\zongxin}[1]{#1}
\newcommand{\pub}[1]{{\color{gray}{\tiny{[{#1}]}}}}



\title{Associating Objects with Transformers for \\ Video Object Segmentation}



\author{Zongxin Yang, Yunchao Wei, Yi Yang \\
  \\
  ~CCAI, College of Computer Science and Technology, Zhejiang University \hspace{1mm}~Baidu Research\\
  ~Institute of Information Science, Beijing Jiaotong University \\
  ~Beijing Key Laboratory of Advanced Information Science and Network \\
  \texttt{\{zongxinyang1996, wychao1987, yee.i.yang\}@gmail.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, \ie, YouTube-VOS (84.1\% ), DAVIS 2017 (84.9\%), and DAVIS 2016 (91.1\%), while keeping more than  faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked  in the 3rd Large-scale VOS Challenge.
\end{abstract} 
\section{Introduction}\label{sec:introduction}
Video Object Segmentation (VOS) is a fundamental task in video understanding with many potential applications, including augmented reality~\cite{ngan2011video} and self-driving cars~\cite{zhang2016instance}. The goal of semi-supervised VOS, the main task in this paper, is to track and segment object(s) across an entire video sequence based on the object mask(s) given at the first frame.

Thanks to the recent advance of deep neural networks, many deep learning based VOS algorithms have been proposed recently and achieved promising performance. STM~\cite{spacetime} and its following works~\cite{KMN,EGMN} leverage a memory network to store and read the target features of predicted past frames and apply a non-local attention mechanism to match the target in the current frame. FEELVOS~\cite{feelvos} and CFBI~\cite{cfbi,cfbip} utilize global and local matching mechanisms to match target pixels or patches from both the first and the previous frames to the current frame.

Even though the above methods have achieved significant progress, the above methods learn to decode scene features that contain a single positive object. Thus under a multi-object scenario, they have to match each object independently and ensemble all the single-object predictions into a multi-object segmentation, as shown in Fig.~\ref{fig:post_ensemble}. Such a post-ensemble manner eases network architectures' design since the networks are not required to adapt the parameters or structures for different object numbers. However, {modeling multiple objects independently, instead of uniformly, is inefficient in exploring multi-object contextual information to learn a more robust feature representation for VOS.} In addition, processing multiple objects separately yet in parallel requires multiple times the amount of GPU memory and computation for processing a single object. This problem restricts the training and application of VOS under multi-object scenarios, especially when computing resources are limited. 


To solve the problem, Fig.~\ref{fig:aot} demonstrates a feasible approach to associate and decode multiple objects uniformly in an end-to-end framework. Hence, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple targets uniformly. First, an identification mechanism is proposed to assign each target a unique identity and embed multiple targets into the same feature space. Hence, the network can learn the association or correlation among all the targets. Moreover, the multi-object segmentation can be directly decoded by utilizing assigned identity information. Second, a Long Short-Term Transformer (LSTT) is designed for constructing hierarchical object matching and propagation. Each LSTT block utilizes a long-term attention for matching with the first frame's embedding and a short-term attention for matching with several nearby frames' embeddings. Compared to the methods~\cite{spacetime,KMN} utilizing only one attention layer, we found hierarchical attention structures are more effective in associating multiple objects.

We conduct extensive experiments on two popular multi-object benchmarks for VOS, \ie, YouTube-VOS~\cite{youtubevos} and DAVIS 2017~\cite{davis2017}, to validate the effectiveness and efficiency of the proposed AOT. {Even using the light-weight Mobilenet-V2~\cite{sandler2018mobilenetv2} as the backbone encoder}, the AOT variant networks achieve superior performance on the validation 2018 \& 2019 splits of the large-scale YouTube-VOS (ours,  \textbf{82.6 84.5}\% \& \textbf{82.2 84.5}\%) while keeping more than  faster multi-object run-time (\textbf{27.1 9.3}FPS) compared to the state-of-the-art competitors (\eg, CFBI~\cite{cfbi}, 81.4\% \& 81.0\%, 3.4FPS). We also achieve new state-of-the-art performance on both the DAVIS-2017 validation (\textbf{85.4}\%) and testing (\textbf{81.2}\%) splits. Moreover, AOT is effective under single-object scenarios as well and outperforms previous methods on DAVIS 2016~\cite{davis2016} (\textbf{92.0}\%), a popular single-object benchmark. Besides, our smallest variant, AOT-T, can maintain real-time multi-object speed on all above benchmarks (\textbf{51.4}FPS on 480p videos). Particularly, AOT ranked  in the Track 1 (Video Object Segmentation) of the 3rd Large-scale Video Object Segmentation Challenge.

\begin{figure}[t!]
\begin{center}

\begin{subfigure}[b]{.4\textwidth}
			\centering
			\includegraphics[height=3cm]{Sources/post_ensemble.pdf}
			\caption{Post-ensemble}\label{fig:post_ensemble}
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
			\centering
			\includegraphics[height=2.85cm]{Sources/aot.pdf}
			\vspace{-3mm}
			\caption{Associating objects (ours)}\label{fig:aot}
\end{subfigure}
\begin{subfigure}[b]{.28\textwidth}
			\centering
			\includegraphics[height=3cm]{Sources/complexity.pdf}
			\caption{Comparison}\label{fig:complexity}
\end{subfigure}

\end{center}

\caption{VOS methods, \eg,~\cite{cfbi,KMN}, process multi-object scenarios in a post-ensemble manner (a). In contrast, our AOT associates all the objects uniformly (b), leading to better efficiency (c).} \label{fig:better}
\end{figure} 
Overall, our contributions are summarized as follows: 
\begin{itemize}
\vspace{-0.5em}
\item  We propose an identification mechanism to associate and decode multiple targets uniformly for VOS. For the first time, multi-object training and inference can be efficient as single-object ones, as demonstrated in Fig.~\ref{fig:complexity}.
\vspace{-0.5em}
\item  Based on the identification mechanism, we design a new efficient VOS framework, \ie, Long Short-Term Transformer (LSTT), for constructing hierarchical multi-object matching and propagation. LSTT achieves superior performance on VOS benchmarks~\cite{youtubevos,davis2017,davis2016} while maintaining better efficiency than previous state-of-the-art methods. {To the best of our knowledge, LSTT is the first hierarchical framework for object matching and propagation by applying transformers~\cite{transformer} to VOS.}

\end{itemize}



 
\section{Related Work}

\noindent\textbf{Semi-supervised Video Object Segmentation.}
Given one or more annotated frames (the first frame in general), semi-supervised VOS methods propagate the manual labeling to the entire video sequence. Traditional methods often solve an optimization problem with an energy defined over a graph structure~\cite{tradition1,tradition3,tradition2}. In recent years, VOS methods have been mainly developed based on deep neural networks (DNN), leading to better results.

Early DNN methods rely on fine-tuning the networks at test time to make segmentation networks focus on a specific object. Among them, OSVOS~\cite{osvos} and MoNet~\cite{xiao2018monet} fine-tune pre-trained networks on the first-frame ground-truth at test time. OnAVOS~\cite{onavos} extends the first-frame fine-tuning by introducing an online adaptation mechanism. Following these approaches, MaskTrack~\cite{masktrack} and PReM~\cite{premvos} 
utilize optical flow to help propagate the segmentation mask from one frame to the next. Despite achieving promising results, the test-time fine-tuning restricts the network efficiency.

Recent works aim to achieve a better run-time and avoid using online fine-tuning. OSMN~\cite{osmn} employs one convolutional network to extract object embedding and another one to guide segmentation predictions. PML~\cite{pml} learns pixel-wise embedding with a nearest neighbor classifier, and VideoMatch~\cite{videomatch} uses a soft matching layer that maps the pixels of the current frame to the first frame in a learned embedding space. Following PML and VideoMatch, FEELVOS~\cite{feelvos} and CFBI~\cite{cfbi,cfbip} extend the pixel-level matching mechanism by additionally matching between the current frame and the previous frame. RGMP~\cite{rgmp} also gathers guidance information from both the first frame and the previous frame but uses a siamese encoder with two shared streams. STM~\cite{spacetime} and its following works (\eg, EGMN~\cite{EGMN} and KMN~\cite{KMN}) leverage a memory network to embed past-frame predictions into memory and apply a non-local attention mechanism on the memory to decode the segmentation of the current frame. SST~\cite{sstvos} utilizes attention mechanisms in a different way, \ie, transformer blocks~\cite{transformer} are used to extract pixel-level affinity maps and spatial-temporal features. The features are target-agnostic, instead of target-aware like our LSTT, since the mask information in past frames is not propagated and aggregated in the blocks. Instead of using matching mechanisms, LWL~\cite{LWLVOS} proposes to use an online few-shot learner to learn to decode object segmentation.

The above methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources of single-object cases. The problem restricts the application and development of the VOS with multiple targets. Hence, we propose our AOT to associate and decode multiple targets uniformly and simultaneously, as efficiently as processing a single object.

\noindent\textbf{Visual Transformers.} Transformers~\cite{transformer} was proposed to build hierarchical attention-based networks for machine translation. Similar to Non-local Neural Networks~\cite{nonlocal}, transformer blocks compute correlation with all the input elements and aggregate their information by using attention mechanisms~\cite{att}. Compared to RNNs, transformer networks model global correlation or attention in parallel, leading to better memory efficiency, and thus have been widely used in natural language processing (NLP) tasks~\cite{devlin2018bert,radford2019language,synnaeve2019end}. Recently, transformer blocks were introduced
to many computer vision tasks, such as image classification~\cite{vit,vaswani2021scaling,swin}, object detection~\cite{detr}/segmentation~\cite{vistr}, and image generation~\cite{parmar2018image}, and have shown promising performance compared to CNN-based networks.

Many VOS methods~\cite{lin2019agss,spacetime,EGMN,KMN} have utilized attention mechanisms to match the object features and propagate the segmentation mask from past frames to the current frames. Nevertheless, these methods consider only one positive target in the attention processes, and how to build hierarchical attention-based propagation has been rarely studied. In this paper, we carefully design a long short-term transformer block, which can effectively construct multi-object matching and propagation within hierarchical structures for VOS.


 
\section{Revisit Previous Solutions for Video Object Segmentation} 
In VOS, many common video scenarios have multiple targets or objects required for tracking and segmenting. Benefit from deep networks, current state-of-the-art VOS methods~\cite{spacetime,cfbi} have achieved promising performance. Nevertheless, these methods focus on matching and decoding a single object. Under a multi-object scenario, they thus have to match each object independently and ensemble all the single-object predictions into a multi-object prediction, as demonstrated in Fig.~\ref{fig:post_ensemble}. Let  denotes a VOS network for predicting single-object segmentation, and  is an ensemble function such as  or the soft aggregation~\cite{spacetime}, the formula of such a post-ensemble manner for processing  objects is like,

where  and  denote the image of the current frame and memory frames respectively, and  are the memory masks (containing the given reference mask and past predicted masks) of all the  objects. This manner extends networks designed for single-object VOS into multi-object applications, so there is no need to adapt the network for different object numbers.

Although the above post-ensemble manner is prevalent and straightforward in the VOS field, processing multiple objects separately yet in parallel requires multiple times the amount of GPU memory and computation for matching a single object and decoding the segmentation. This problem restricts the training and application of VOS under multi-object scenarios when computing resources are limited.
To make the multi-object training and inference as efficient as single-object ones, {an expected solution should be capable of associating and decoding multiple objects uniformly instead of individually. To achieve such an objective, we propose an identification mechanism to embed the masks of any number (required to be smaller than a pre-defined large number) of targets into the same high-dimensional space. Based on the identification mechanism, a novel and efficient framework, \ie, Associating Objects with Transformers (AOT), is designed for propagating all the object embeddings uniformly and hierarchically, from memory frames to the current frame.}


As shown in Fig.~\ref{fig:aot}, our AOT associates and segments multiple objects within an end-to-end framework. For the first time, processing multiple objects can be as efficient as processing a single object (Fig.~\ref{fig:complexity}). Compared to previous methods, our training under multi-object scenarios is also more efficient since AOT can associate multiple object regions and learn contrastive feature embeddings among them uniformly. 
\begin{figure}[t!]
\begin{center}

\begin{subfigure}[b]{.393\textwidth}
			\centering
			\includegraphics[width=0.95\textwidth]{Sources/overview.pdf}
			\caption{Overview}\label{fig:overview}
\end{subfigure}
\begin{subfigure}[b]{.292\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Sources/id.pdf}

			\caption{Identity assignment}\label{fig:id}
\end{subfigure}
\begin{subfigure}[b]{.292\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Sources/lstt.pdf}
			\caption{-th LSTT block}\label{fig:lstt}
\end{subfigure}

\end{center}

\caption{(a) The overview of our Associating Objects with Transformers (AOT). The multi-object masks are embedded by using our Identification mechanism. Moreover, a -layer Long Short-Term Transformer is responsible for matching multiple objects uniformly and hierarchically. (b) An illustration of the IDentity assignment (ID) designed for transferring a -object mask into an identification embedding. (c) The structure of an LSTT block. LN: layer normalization~\cite{ln}.}
\end{figure} 
\section{Associating Objects with Transformers}
In this section, we introduce our identification mechanism proposed for efficient multi-object VOS. Then, we design a new VOS framework, \ie, long short-term transformer, based on the identification mechanism for constructing hierarchical multi-object matching and propagation.


\subsection{Identification Mechanism for Multi-object Association}
Many recent VOS methods~\cite{spacetime,EGMN,KMN} utilized attention mechanisms and achieved promising results.
To formulate, we define , , and  as the query embedding of the current frame, the key embedding of the memory frames, and the value embedding of the memory frames respectively, where , , ,  denote the temporal, height, width, and channel dimensions. The formula of a common attention-based matching and propagation is,

where a matching map is calculated by the correlation function , and then the value embedding, , will be propagated into each location of the current frame.

In the common single-object propagation~\cite{spacetime}, the binary mask information in memory frames is embedded into  with an additional memory encoder network and thus can also be propagated to the current frame by using Eq.~\ref{equ:att}. A convolutional decoder network following the propagated feature will decode the aggregated feature and predict the single-object probability logit of the current frame.

The main problem of propagating and decoding multi-object mask information in an end-to-end network is how to adapt the network to different target numbers. To overcome this problem, we propose an identification mechanism consisting of identification embedding and decoding based on attention mechanisms.

First, an \textbf{Identification Embedding} mechanism is proposed to embed the masks of multiple different targets into the same feature space for propagation. As seen in Fig.~\ref{fig:id}, we initialize an identity bank, , where  identification vectors with  dimensions are stored. For embedding multiple different target masks, each target will be randomly assigned a different identification vector. Assuming  () targets are in the video scenery, the formula of embedding the targets' one-hot mask, , into a identification embedding, , by randomly assigning identification vector from the bank  is,

where  is a random permutation matrix, satisfying that  is equal to a  unit matrix, for randomly selecting  identification embeddings. After the  assignment, different target has different identification embedding, and thus we can propagate all the target identification information from memory frames to the current frame by attaching the identification embedding  with the attention value , \ie,

where  aggregates all the multiple targets' embeddings from the propagation.

For \textbf{Identification Decoding}, \ie, predicting all the targets' probabilities from the aggregated feature , we firstly predict the probability logit for every identity in the bank  by employing a convolutional decoding network , and then select the assigned ones and calculate the probabilities, \ie,

where  is all the  identities' probability logits,  is the same as the selecting matrix used in the identity assignment (Eq.~\ref{equ:id}), and  is the probability prediction of all the  targets.

For training, common multi-class segmentation losses, such as cross-entropy loss, can be used to optimize the multi-object  regarding the ground-truth labels. The identity bank  is trainable and randomly initialized at the training beginning. To ensure that all the identification vectors have the same opportunity to compete with each other, we randomly reinitialize the identification selecting matrix  in each video sample and each optimization iteration.

\subsection{Long Short-Term Transformer for Hierarchical Matching and Propagation}
Previous methods~\cite{spacetime,KMN} always utilize only one layer of attention (Eq.~\ref{equ:att}) to aggregate single-object information. In our identification-based multi-object pipeline, we found that a single attention layer cannot fully model multi-object association, which naturally should be more complicated than single-object processes. Thus, we consider constructing hierarchical matching and propagation by using a series of attention layers. Recently, transformer blocks~\cite{transformer} have been demonstrated to be stable and promising in constructing hierarchical attention structures in visual tasks~\cite{detr,vit}. Based on transformer blocks, we carefully design a Long Short-Term Transformer (LSTT) block for multi-object VOS.

Following the common transformer blocks~\cite{transformer,devlin2018bert}, LSTT firstly employs a self-attention layer, which is responsible for learning the association or correlation among the targets within the current frame. Then, LSTT additionally introduces a long-term attention, for aggregating targets' information from long-term memory frames and a short-term attention, for learning temporal smoothness from nearby short-term frames. The final module is based on a common 2-layer feed-forward MLP with GELU~\cite{gelu} non-linearity in between. Fig.~\ref{fig:lstt} shows the structure of an LSTT block. Notably, all these attention modules are implemented in the form of the multi-head attention~\cite{transformer}, \ie, multiple attention modules followed by concatenation and a linear projection. Nevertheless, we only introduce their single-head formulas below for the sake of simplicity.

\noindent\textbf{Long-Term Attention} is responsible for aggregating targets' information from past memory frames, which contains the reference frame and stored predicted frames, to the current frame. Since the time intervals between the current frame and past frames are variable and can be long-term, the temporal smoothness is difficult to guarantee. Thus, the long-term attention employs non-local attention like Eq.~\ref{equ:att}. Let  denotes the input feature embedding at time  and in block , where  is the block index of LSTT, the formula of the long-term attention is,

where  and  are the input feature embeddings and target masks of memory frames with indices . Besides,  and  are trainable parameters of the space projections for matching and propagation, respectively. Instead of using different projections for  and , we found the training of LSTT is more stable with a siamese-like matching, \ie, matching between the features within the same embedding space (-th features with the same projection of ).

\noindent\textbf{Short-Term Attention} is employed for aggregating information in a spatial-temporal neighbourhood for each current-frame location. Intuitively, the image changes across several contiguous video frames are always smooth and continuous. Thus, the target matching and propagation in contiguous frames can be restricted in a small spatial-temporal neighborhood, leading to better efficiency than non-local processes. Considering  neighbouring frames with indices  are in the spatial-temporal neighbourhood, the features and masks of these frames are  and , and then the formula of the short-term attention at each spatial location  is,

where  is the feature of  at location ,  is a  spatial neighbourhood centered at location , and thus  and  are the features and masks of the spatial-temporal neighbourhood, respectively, with a shape of  or . 


When extracting features of the first frame , there is no memory frames or previous frames, and hence we use  to replace  and . In other words, the long-term attention and the short-term attention are changed into self-attentions without adjusting the network structures and parameters.
 
\section{Implementation Details}\label{sec:implementation}

\noindent\textbf{Network Details:} For sufficiently validating the effectiveness of our identification mechanism and LSTT, we mainly use light-weight backbone encoder, MobileNet-V2~\cite{sandler2018mobilenetv2}, and decoder, FPN~\cite{fpn} with Group Normalization~\cite{gn}. The spatial neighborhood size  is set to 15, and the number of identification vectors, , is set to 10, which is consistent with the maximum object number in the benchmarks~\cite{youtubevos,davis2017}. AOT performs well with PaddlePaddle~\cite{paddlepaddle} and PyTorch~\cite{pytorch}. 
More details can be found in the supplementary material.



\noindent\textbf{Architecture Variants:} We build several AOT variant networks with different LSTT layer number  or long-term memory size . The hyper-parameters of these variants are: \textbf{(1) AOT-Tiny}: , ; \textbf{(2) AOT-Small}: , ; \textbf{(3) AOT-Base}: , ; \textbf{(4) AOT-Large}: , . In the experiments, we also equip AOT-L with ResNet50 (R50)~\cite{resnet} or Swin-B~\cite{swin}.

AOT-S is a small model with only 2 layers of LSTT block. Compared to AOT-S, AOT-T utilizes only 1 layer of LSTT, and AOT-B/L uses 3 layers. In AOT-T/S/B, only the first frame is considered into long-term memory, which is similar to \cite{feelvos,cfbi}, leading to a smooth efficiency. In AOT-L, the predicted frames are stored into long-term memory per  frames, following the memory reading strategy~\cite{spacetime}. We set  to 2/5 for training/testing.

\noindent\textbf{Training Details:} Following~\cite{rgmp,spacetime,EGMN,KMN}, the training stage is divided into two phases: (1) pre-training on sythetic video sequence generated from static image datasets~\cite{voc,coco,cheng2014global,shi2015hierarchical,semantic} by randomly applying multiple image augmentations~\cite{rgmp}. (2) main training on the VOS benchmarks~\cite{youtubevos,davis2017} by randomly applying video augmentations~\cite{cfbi}. 
More details are in the supplementary material.





 
\begin{table}[t!]
	\centering
	\caption{The quantitative evaluation on multi-object benchmarks, YouTube-VOS~\cite{youtubevos} and DAVIS 2017~\cite{davis2017}. 
	\textbf{Y}: using YouTube-VOS for training. : using 600p instead of 480p videos in inference. : timing extrapolated from single-object speed assuming linear scaling in the number of objects.
	}\label{tab:comparisons}
	
\begin{subtable}[t]{.54\textwidth}
\centering
\caption{YouTube-VOS}\label{tab:youtubevos}
\setlength{\tabcolsep}{3pt}
\small
\vspace{-0.5mm}
\begin{tabular}{lcccccc}
	
\toprule[1.5pt]
            &   &  \multicolumn{2}{c}{Seen}  &    \multicolumn{2}{c}{Unseen} &  \\
\midrule[1pt]
 Methods &  &  &  &  &  & FPS \\
\midrule[1pt]
\multicolumn{7}{c}{\textit{Validation 2018 Split}} \\
\midrule[1pt]
STM\pub{ICCV19}~\cite{spacetime}   &  79.4  &  79.7  &  84.2  &  72.8  &  80.9 & - \\
KMN\pub{ECCV20}~\cite{KMN}  &  81.4  &  81.4  &  85.6  &  75.3  &  83.3 & - \\
CFBI\pub{ECCV20}~\cite{cfbi} &  81.4  &  81.1  & 85.8  & 75.3  & 83.4 & 3.4 \\
LWL\pub{ECCV20}~\cite{LWLVOS} & 81.5  &  80.4  &  84.9  &  76.4  &  84.4 & - \\
SST\pub{CVPR21}~\cite{sstvos} & 81.7  &  81.2  &  -  &  76.0  &  - & - \\
CFBI+\pub{TPAMI21}~\cite{cfbip} &  82.8  &  81.8  & 86.6  & 77.1  & 85.6 & 4.0 \\
\hline

AOT-T & 80.2  &  80.1  & 84.5  & 74.0  & 82.2 & \textbf{41.0} \\
AOT-S & 82.6  &  82.0  & 86.7  & 76.6  & 85.0 & {27.1} \\
AOT-B & 83.5  &  {82.6}  & 87.5  & {77.7}  & 86.0 & 20.5 \\
AOT-L & {83.8}  &  {82.9}  & {87.9}  & {77.7}  & \textbf{86.5} & 16.0 \\
R50-AOT-L & {84.1}  &  {83.7}  & {88.5}  & \textbf{78.1}  & {86.1} & 14.9 \\
SwinB-AOT-L & \textbf{84.5}  &  \textbf{84.3}  & \textbf{89.3}  & {77.9}  & {86.4} & 9.3 \\
\midrule[1pt]
\multicolumn{7}{c}{\textit{Validation 2019 Split}} \\
\midrule[1pt]
CFBI\pub{ECCV20}~\cite{cfbi}  &  81.0  &  {80.6}  & {85.1}  & {75.2}  & {83.0} & 3.4  \\
SST\pub{CVPR21}~\cite{sstvos} & 81.8  &  80.9  &  -  &  76.6  &  - & - \\
CFBI+\pub{TPAMI21}~\cite{cfbip} &  82.6  &  81.7  & 86.2  & 77.1  & 85.2 & 4.0 \\
\hline
AOT-T &  79.7  &  79.6  & 83.8  & 73.7  & 81.8 & \textbf{41.0} \\
AOT-S &  82.2 &  81.3  & 85.9  & 76.6  & 84.9 & {27.1} \\
AOT-B &  83.3 &  {82.4}  & {87.1}  & 77.8  & 86.0 & 20.5 \\
AOT-L &  {83.7} &  {82.8}  & {87.5}  & {78.0}  & \textbf{86.7} & 16.0 \\
R50-AOT-L & {84.1}  &  {83.5}  & {88.1}  & \textbf{78.4}  & {86.3} & 14.9 \\
SwinB-AOT-L & \textbf{84.5}  &  \textbf{84.0}  & \textbf{88.8}  & \textbf{78.4}  & \textbf{86.7} & 9.3 \\

\bottomrule[1.5pt]
\end{tabular}

\end{subtable}
\begin{subtable}[t]{.45\textwidth}










\caption{DAVIS 2017}\label{tab:davis}
\begin{center}
\small
\vspace{-2.3mm}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l c c c c}
\toprule[1.5pt]
 Methods  &  &  &  & FPS \\
\midrule[1pt]
\multicolumn{5}{c}{\textit{Validation 2017 Split}} \\
\midrule[1pt]


CFBI~\cite{cfbi} (\textbf{Y}) &  81.9  & 79.3  & 84.5 & 5.9 \\
SST~\cite{sstvos} (\textbf{Y}) &  82.5  & 79.9  & 85.1 & - \\
KMN~\cite{KMN}  &  76.0  & 74.2  & 77.8 & 4.2 \\
KMN~\cite{KMN} (\textbf{Y}) &  82.8  & 80.0  & 85.6 & 4.2 \\
\vspace{0.65mm}
CFBI+~\cite{cfbip} (\textbf{Y}) &  82.9  & 80.1  & 85.7 & 5.6 \\
\hline
AOT-T (\textbf{Y}) & 79.9 & 77.4 & 82.3 & \textbf{51.4} \\
AOT-S  &  79.2  & 76.4 & 82.0 & 40.0 \\
AOT-S (\textbf{Y}) & 81.3 & 78.7 & 83.9 & {40.0} \\
AOT-B (\textbf{Y}) & 82.5 & 79.7 & 85.2 & 29.6 \\
AOT-L (\textbf{Y}) & {83.8} & {81.1} & {86.4} & 18.7 \\
R50-AOT-L (\textbf{Y}) & {84.9} & {82.3} & {87.5} & 18.0 \\
SwinB-AOT-L (\textbf{Y}) & \textbf{85.4} & \textbf{82.4} & \textbf{88.4} & 12.1 \\
\midrule[1pt]
\multicolumn{5}{c}{\textit{Testing 2017 Split}} \\
\midrule[1pt]
CFBI~\cite{cfbi} (\textbf{Y}) &  75.0  & 71.4  & 78.7 & 5.3 \\
CFBI~\cite{cfbi} (\textbf{Y}) &  76.6  & 73.0  & 80.1 & 2.9 \\
KMN~\cite{KMN} (\textbf{Y})  &  77.2  & 74.1  & 80.3 & - \\
\vspace{0.65mm}
CFBI+~\cite{cfbip} (\textbf{Y}) &  78.0  & 74.4  & 81.6 & 3.4 \\

\hline

AOT-T (\textbf{Y}) & 72.0 & 68.3 & 75.7 & \textbf{51.4} \\
AOT-S (\textbf{Y}) &  73.9  & 70.3 & 77.5 & 40.0 \\
AOT-B (\textbf{Y})  &  75.5  & 71.6 & 79.3 & 29.6 \\
AOT-L (\textbf{Y})  &  78.3  & 74.3 & 82.3 & 18.7 \\
R50-AOT-L (\textbf{Y}) & {79.6} & {75.9} & {83.3} & 18.0 \\
SwinB-AOT-L (\textbf{Y}) & \textbf{81.2} & \textbf{77.3} & \textbf{85.1} & 12.1 \\

\bottomrule[1.5pt]

\end{tabular}
\end{center}

\end{subtable}

\end{table}





 \section{Experimental Results}\label{sec:experiments}
We evaluate AOT on popular multi-object benchmarks, YouTube-VOS~\cite{youtubevos} and DAVIS 2017~\cite{davis2017}, and single-object benchmark, DAVIS 2016~\cite{davis2016}. For YouTube-VOS experiments, we train our models on the YouTube-VOS 2019 training split. For DAVIS, we train on the DAVIS-2017 training split. When evaluating YouTube-VOS, we use the default 6FPS videos, and all the videos are restricted to be smaller than  resolution. As to DAVIS, the default 480p 24FPS videos are used.

The evaluation metric is the  score, calculated as the average Intersect over Union (IoU) score between the prediction and the ground truth mask, and the  score, calculated as an average boundary similarity measure between the boundary of the prediction and the ground truth, and their mean value, denoted as \&. We evaluate all the results on official evaluation servers or with official tools.
\subsection{Compare with the State-of-the-art Methods}


\noindent \textbf{YouTube-VOS}~\cite{youtubevos} is the latest large-scale benchmark for multi-object video segmentation and is about 37 times larger than DAVIS 2017 (120 videos). Specifically, YouTube-VOS contains 3471 videos in the training split with 65 categories and 474/507 videos in the validation 2018/2019 split with additional 26 unseen categories. The unseen categories do not exist in the training split to evaluate algorithms' generalization ability.

As shown in Table~\ref{tab:youtubevos}, AOT variants achieve superior performance on YouTube-VOS compared to the previous state-of-the-art methods. With our identification mechanism, AOT-S (82.6\% ) is comparable with CFBI+~\cite{cfbip} (82.8\%) while running about  faster (27.1 \vs 4.0FPS). By using more LSTT blocks, AOT-B improves the performance to 83.5\%. Moreover, AOT-L further improves both the seen and unseen scores by utilizing the memory reading strategy, and our R50-AOT-L (\textbf{84.1\%/84.1\%}) significantly outperforms the previous methods on the validation 2018/2019 split while maintaining an efficient speed (14.9FPS).

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{Sources/comparisons.pdf}

    \caption{\zongxin{Qualitative results. (top) Compared with CFBI~\cite{cfbi}, AOT performs better when segmenting multiple highly similar objects (\textit{carousels} and \textit{zebras}). (bottom) AOT fails to segment some tiny objects (\textit{ski poles} and \textit{watch}) since AOT has no specific design for processing rare tiny objects.}}
    \label{fig:comparisons}

\end{figure} 
\begin{wraptable}[13]{r}{0.45\textwidth}
\begin{center}
\vspace{-6.5mm}
\caption{The quantitative evaluation on the single-object DAVIS 2016~\cite{davis2016}.}\label{tab:davis2016}

\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{l c c c c}
\toprule[1.5pt]
 Methods  &  &  &  & FPS \\
\midrule[1pt]
CFBI+~\cite{cfbip} (\textbf{Y})  & 89.9  & 88.7 & 91.1  & 5.9 \\
KMN~\cite{KMN} (\textbf{Y})  &  90.5 & {89.5}  & {91.5}  & 8.3 \\
\hline
AOT-T (\textbf{Y}) &  86.8  & 86.1 & 87.4 & \textbf{51.4} \\
AOT-S (\textbf{Y}) &  89.4  & 88.6 & 90.2 & 40.0 \\
AOT-B (\textbf{Y}) &  89.9  & 88.7 & 91.1 & 29.6 \\
AOT-L (\textbf{Y}) &  {90.4}  & {89.6} & {91.1} & 18.7 \\
R50-AOT-L (\textbf{Y}) &  {91.1}  & {90.1} & {92.1} & 18.0 \\
SwinB-AOT-L (\textbf{Y}) &  \textbf{92.0}  & \textbf{90.7} & \textbf{93.3} & 12.1 \\
\bottomrule[1.5pt]

\end{tabular}
\end{center}
\end{wraptable} \noindent \textbf{DAVIS 2017}~\cite{davis2017} is a multi-object extension of DAVIS 2016. The validation split of DAVIS 2017 consists of 30 videos with 59 objects, and the training split contains 60 videos with 138 objects. Moreover, the testing split contains 30 more challenging videos with 89 objects in total.




Table~\ref{tab:davis} shows that our R50-AOT-L (\textbf{Y}) surpasses all the competitors on both the DAVIS-2017 validation (\textbf{84.9\%}) and testing (\textbf{79.6\%}) splits and maintains an efficient speed (18.0FPS). Notably, such a multi-object speed is the same as our single-object speed on DAVIS 2016. For the first time, processing multiple objects can be as efficient as processing a single object over the AOT framework. We also evaluate our method without training with YouTube-VOS, and AOT-S (79.2\%) performs much better than KMN~\cite{KMN} (76.0\%) by +3.2\%.


\noindent \textbf{DAVIS 2016}~\cite{davis2016} is a single-object benchmark containing 20 videos in the validation split. Although our AOT aims at improving multi-object video segmentation, we also achieve a new state-of-the-art performance on DAVIS 2016 (R50-AOT-L (\textbf{Y}), \textbf{91.1\%}). Under single-object scenarios, the multi-object superiority of AOT is limited, but R50-AOT-L still maintains an about  efficiency compared to KMN (18.0 \vs 8.3FPS). Furthermore, our smaller variant, AOT-B (89.9\%), achieves comparable performance with CFBI+ (89.9\%) while running 5 faster (29.6 \vs 5.9FPS).

Apart from the above results, replacing the AOT encoder from commonly used ResNet50 to SwinB can further boost our performance to higher level (Table~\ref{tab:youtubevos}, \ref{tab:davis}, and \ref{tab:davis2016}).


\zongxin{\noindent\textbf{Qualitative results:} Fig.~\ref{fig:comparisons} visualizes some qualitative results in comparison with CFBI~\cite{cfbi}, which only associates each object with its relative background. As demonstrated, CFBI is easier to confuse multiple highly similar objects. In contrast, our AOT tracks and segments all the targets accurately by associating all the objects uniformly. However, AOT fails to segment some tiny objects (\textit{ski poles} and \textit{watch}) since we do not make special designs for tiny objects.}

\begin{table}[t!]
	\centering
	\caption{Ablation study. The experiments are based on AOT-S and conducted on the validation 2018 split of YouTube-VOS~\cite{youtubevos} without pre-training on synthetic videos. Self: the position embedding type used in the self-attention. Rel: use relative positional embedding~\cite{relativepos} on the local attention.}\label{tab:ablation}
	
\begin{subtable}{.3\textwidth}
\center
\caption{Identity number}\label{tab:id_number}
\setlength{\tabcolsep}{2pt}
\small
\begin{tabular}{l c c c}
    \toprule[1.5pt]
          &  &  &  \\
    \midrule[1pt]
       10 & \textbf{80.3} & \textbf{80.6} & \textbf{73.7} \\
    \hline
       15 & 79.0 & 79.4 & 72.1 \\
       20 & 78.3 & 79.4 & 70.8 \\
       30 & 77.2 & 78.5 & 70.2 \\
    \bottomrule[1.5pt]
\end{tabular}
\end{subtable}
\begin{subtable}{.3\textwidth}
\center
\caption{Local window size}\label{tab:window_size}
\setlength{\tabcolsep}{2pt}
\small
\begin{tabular}{l c c c}
    \toprule[1.5pt]
          &  &  &  \\
    \midrule[1pt]
       15 & \textbf{80.3} & \textbf{80.6} & \textbf{73.7} \\
    \hline
      11 & 78.8 & 79.5 & 71.9 \\
       7 & 78.3 & 79.3 & 70.9 \\
       0 & 74.3 & 74.9 & 67.6 \\
    \bottomrule[1.5pt]
\end{tabular}
\end{subtable}
\begin{subtable}{.3\textwidth}
\center
\caption{Local frame number}\label{tab:local_frame}
\setlength{\tabcolsep}{2pt}
\small
\begin{tabular}{l c c c}
    \toprule[1.5pt]
          &  &  &  \\
    \midrule[1pt]
       1 & \textbf{80.3} & \textbf{80.6} & \textbf{73.7} \\
    \hline
       2 & 80.0 & 79.8 & 73.7 \\
       3 & 79.1 & 80.0 & 72.2 \\
       0 & 74.3 & 74.9 & 67.6 \\
    \bottomrule[1.5pt]
\end{tabular}
\end{subtable}

\begin{subtable}{.37\textwidth}
\center
\caption{LSTT block number}\label{tab:lstt_number}
\setlength{\tabcolsep}{2pt}
\small
\begin{tabular}{l c c c c c}
    \toprule[1.5pt]
          &  &  &  & FPS & Param \\
    \midrule[1pt]
       2 &  {80.3} & {80.6} & {73.7} & 27.1 & 7.0M \\
    \hline
       3 &  \textbf{80.9} & \textbf{81.1} & \textbf{74.0} & 20.5 & 8.3M \\
       1 & 77.9 & 78.8 & 71.0 & \textbf{41.0} & \textbf{5.7M} \\
    \bottomrule[1.5pt]
\end{tabular}
\end{subtable}
\begin{subtable}{.37\textwidth}
\center
\caption{Positional embedding}\label{tab:pos_emb}
\setlength{\tabcolsep}{2pt}
\small
\begin{tabular}{c c c c c}
    \toprule[1.5pt]
         Self & Rel &  &  &  \\
    \midrule[1pt]
       sine & \checkmark & \textbf{80.3} & \textbf{80.6} & \textbf{73.7} \\
    \hline
none & \checkmark & 80.1 & 80.4 & 73.5 \\
        sine & - & 79.7 & 80.1 & 72.9 \\
    \bottomrule[1.5pt]
\end{tabular}
\end{subtable}

\end{table} \subsection{Ablation Study}
In this section, we analyze the main components and hyper-parameters of AOT and evaluate their impact on the VOS performance in Table~\ref{tab:ablation}.

\textbf{Identity number:} The number of the identification vectors, , have to be larger than the object number in videos. Thus, we set  to 10 in default to be consistent with the maximum object number in the benchmarks~\cite{youtubevos,davis2017}. As seen in Table~\ref{tab:id_number},  larger than 10 leads to worse performance since (1) no training video contains so many objects; (2) embedding more than 10 objects into the space with only 256 dimensions is difficult.

\textbf{Local window size:} Table~\ref{tab:window_size} shows that larger local window size, , results in better performance. Without the local attention, , the performance of AOT significantly drops from 80.3\% to 74.3\%, which demonstrates the necessity of the local attention.

\textbf{Local frame number:} In Table~\ref{tab:local_frame}, we also try to employ more previous frames in the local attention, but using only the  frame (80.3\%) performs better than using 2/3 frames (80.0\%/79.1\%). A possible reason is that the longer the temporal interval, the more intense the motion between frames, so it is easier to introduce more errors in the local matching when using an earlier previous frame.

\textbf{LSTT block number:} As shown in Table~\ref{tab:lstt_number}, the AOT performance increases by using more LSTT blocks. Notably, the AOT with only one LSTT block (77.9\%) reaches a fast real-time speed (41.0FPS) on YouTube-VOS, although the performance is -2.4\% worse than AOT-S (80.3\%). By adjusting the LSTT block number, we can flexibly balance the accuracy and speed of AOT.

\textbf{Position embedding:} In our default setting, we apply fixed sine spatial positional embedding to the self-attention following~\cite{detr}, and our local attention is equipped with learned relative positional embedding~\cite{relativepos}. The ablation study is shown in Table~\ref{tab:pos_emb}, where removing the sine embedding decreases the performance to 80.1\% slightly. In contrast, the relative embedding is more important than the sine embedding. Without the relative embedding, the performance drops to 79.7\%, which means the motion relationship between adjacent frames is helpful for local attention. We also tried to apply learned positional embedding to self-attention modules, but no positive effect was observed.




 
\section{Conclusion}
This paper proposes a novel and efficient approach for video object segmentation by associating objects with transformers and achieves superior performance on three popular benchmarks. A simple yet effective identification mechanism is proposed to associate, match, and decode all the objects uniformly under multi-object scenarios. For the first time, processing multiple objects in VOS can be efficient as processing a single object by using the identification mechanism. In addition, a long short-term transformer is designed for constructing hierarchical object matching and propagation for VOS. The hierarchical structure allows us to flexibly balance AOT between real-time speed and state-of-the-art performance by adjusting the LSTT number. 
\zongxin{We hope the identification mechanism will help ease the future study of multi-object VOS and related tasks (\eg, video instance segmentation, interactive VOS, and multi-object tracking), and AOT will serve as a solid baseline.} 




{
\bibliographystyle{splncs04}
\bibliography{reference}
}

\newpage
\appendix

\section{Appendix}

\subsection{More Implementation Details}
\subsubsection{Network Details}
For MobileNet-V2 encoder, we increase the final resolution of the encoder to  by adding a dilation to the last stage and removing a stride from the first convolution of this stage. For ResNet-50 and SwinB encoders, we remove the last stage directly. The encoder features are flattened into sequences before LSTT. In LSTT, the input channel dimension is 256, and the head number is set to 8 for all the attention modules. To increase the receptive field of LSTT, we insert a depth-wise convolution layer with a kernel size of 5 between two layers of each feed-forward module. In our default setting of the short-term memory , only the previous () frame is considered, which is similar to the local matching strategy~\cite{feelvos,cfbi}. After LSTT, all the output features of LSTT blocks are reshaped into 2D shapes and will serve as the decoder input. Then, the FPN decoder progressively increases the feature resolution from  to  and decreases the channel dimension from 256 to 128 before the final output layer, which is used for identification decoding.

\noindent\textbf{Patch-wise Identity Bank:} Since the spatial size of LSTT features is only 1/16 of the input video, we can not directly assign identities to the pixels of high-resolution input mask to construct a low-resolution identification embedding. To overcome this problem, we further propose a strategy named patch-wise identity bank. In detail, we first separate the input mask into non-overlapping patches of 1616 pixels. The original identity bank with  identities is also expanded to a patch-wise identity bank, in which each identity has 1616 sub-identity vectors corresponding to 1616 positions in a patch. Hence, the pixels of an object region with different patch positions will have different sub-identity vectors under an assigned identity. By summing all the assigned sub-identities in each patch, we can directly construct a low-resolution identification embedding while keeping the shape information inside each patch. 

\subsubsection{Training Details}
All the videos are firstly down-sampled to 480p resolution, and the cropped window size is 465 465. For optimization, we adopt the AdamW~\cite{adamw} optimizer and the sequential training strategy~\cite{cfbi}, whose sequence length is set to 5. The loss function is a 0.5:0.5 combination of bootstrapped cross-entropy loss and soft Jaccard loss~\cite{nowozin2014optimal}. For stabilizing the training, the statistics of BN~\cite{bn} modules and the first two stages in the encoder are frozen, and Exponential Moving Average (EMA)~\cite{polyak1992acceleration} is used. Besides, we apply stochastic depth~\cite{huang2016deep} to the self-attention and the feed-forward modules in LSTT.

The batch size is 16 and distributed on 4 Tesla V100 GPUs. For pre-training, we use an initial learning rate of  and a weight decay of  for 100,000 steps. For main training, the initial learning rate is set to  and the weight decay is . In addition, the training steps are 100,000 for YouTube-VOS or 50,000 for DAVIS. To relieve over-fitting, the initial learning rate of encoders is reduced to a 0.1 scale of other network parts. All the learning rates gradually decay to  in a polynomial manner~\cite{cfbi}.

\begin{figure}[t!]
\begin{center}

\begin{subfigure}[t]{.24\textwidth}
			\centering
			\includegraphics[width=0.7\textwidth]{Sources/id_bank_10_nobg.png}
			\caption{ (default)}\label{fig:id_bank_10}
\end{subfigure}
\begin{subfigure}[t]{.24\textwidth}
			\centering
			\includegraphics[width=0.7\textwidth]{Sources/id_bank_15_nobg.png}
			\caption{}\label{fig:id_bank_15}
\end{subfigure}
\begin{subfigure}[t]{.24\textwidth}
			\centering
			\includegraphics[width=0.7\textwidth]{Sources/id_bank_20_nobg.png}
			\caption{}\label{fig:id_bank_20}
\end{subfigure}
\begin{subfigure}[t]{.24\textwidth}
			\centering
			\includegraphics[width=0.7\textwidth]{Sources/id_bank_30_nobg.png}
			\caption{}\label{fig:id_bank_30}
\end{subfigure}

\end{center}

\caption{Visualization of the cosine similarity between every two of  identification vectors in the identity bank. We use the form of a  symmetric matrix to visualize all the cosine similarities, and the values on the diagonal are all equal to 1. The darker the green color, the higher the similarity. In the case of , the similarities are stable and balanced. As the vector number  increases, The visualized matrix becomes less and less smooth, which means the similarities become unstable.}\label{fig:id_bank}
\end{figure} \begin{figure}[t!]
\begin{center}

\begin{subfigure}[b]{.45\textwidth}
			\centering
			\includegraphics[width=0.6\textwidth]{Sources/long_term.pdf}
			\caption{Long-term Attention}\label{fig:long_term}
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
			\centering
			\includegraphics[width=0.6\textwidth]{Sources/short_term.pdf}

			\caption{Short-term Attention}\label{fig:short_term}
\end{subfigure}


\end{center}

\caption{Illustrations of the long-term attention and the short-term attention. (a) The long-term attention employs a non-local manner to match all the locations in the long-term memory. (b) In contrast, the short-term attention only focus on a nearby spatial-temporal region with a shape of .}\label{fig:long_short_term}
\end{figure} 
\subsection{Visualization of Identity Bank}
In AOT, the identity bank is randomly initialized, and all the  identification vectors are learned by being randomly assigned to objects during the training phase. Intuitively, all the identification vectors should be equidistant away from each other in the feature space because their roles are equivalent. To validate our hypothesis, we visualize the similarity between every two identification vectors in Fig.~\ref{fig:id_bank}.

In our default setting,  (Fig.~\ref{fig:id_bank_10}), all the vectors are far away from each other, and the similarities remain almost the same. This phenomenon is consistent with our above hypothesis. In other words, the reliability and effectiveness of our identification mechanism are further verified. 

In the ablation study, using more identities leads to worse results. To analyze the reason, we also visualize the learned identity banks with more vectors. Fig.~\ref{fig:id_bank_15},~\ref{fig:id_bank_20}, and~\ref{fig:id_bank_30} demonstrate that maintaining equidistant between every two vectors becomes more difficult when the identity bank contains more vectors, especially when . There are two possible reasons for this phenomenon: (1) No training video contains enough objects to be assigned so many identities, and thus the network cannot learn to associate all the identities simultaneously; (2) the used space with only 256 dimensions is difficult for keeping more than 10 objects to be equidistant. 



\subsection{Illustration of Long Short-term Attention}
To facilitate understanding our long-term and short-term attention modules, we illustrate their processes in Fig.~\ref{fig:long_short_term}. Since the temporal smoothness between the current frame and long-term memory frames is difficult to guarantee, the long-term attention employs a non-local manner to match all the locations in the long-term memory. In contrast, short-term attention only focuses on a nearby spatial-temporal neighborhood of each current-frame location.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{Sources/visual_attention.pdf}

    \caption{Visualization of long-term and short-term attention maps during the inference of DAVIS 2017~\cite{davis2017}. There are three similar people, marked in different colors, in the video. To sufficiently verify the effect of long-term attention, only the first frame is considered into the long-term memory, and thus we AOT-B to conduct the experiment. For visualization, we propagate the colored multi-object masks in long-term or short-term memory to the current frame regarding the corresponding attention map. The brighter the color, the stronger the attention. , , and  denote the 3 LSTT layers of AOT-B in order.}
    \label{fig:attention}

\end{figure} \subsection{Visualization of Hierarchical Matching and Propagation}

In our AOT, we propose to construct a hierarchical framework, \ie, LSTT, for multi-object matching and propagation, and the ablation study indicates that using more LSTT layers (or blocks) results in better VOS performance. To further validate the effectiveness of LSTT and analyze the behavior of each LSTT layer, we visualize long-term and short-term attention maps in each layer during inference, as shown in Fig.~\ref{fig:attention} and~\ref{fig:occlusion}.


At the bottom of Fig.~\ref{fig:attention}, the attention maps become more accurate and sharper as the index of layers increases. In the first layer, \ie, , the current features have not aggregated the multi-object mask information from memory frames, and the long-term attention map is very vague and contains a lot of wrong matches among the objects and the background. Nevertheless, as the layer index increases, the mask information of all the objects is gradually aggregated so that the long-term attention becomes more and more accurate. Similarly, the quality, especially the boundary of objects, of the short-term attention improves as the layer index increases. Notably, the short-term attention performs well even in the first layer, , which is different from the long-term attention. The reason is that the neighborhood matching of short-term attention is easier than the long-term matching of long-term attention. However, long-term attention is still necessary because short-term attention will fail in some cases, such as occlusion (Fig.~\ref{fig:occlusion}).



In short, the visual analysis further proves the necessity and effectiveness of our hierarchical LSTT. The hierarchical matching is not simply a combination of multiple matching processes. Critically, the multi-object information will be gradually aggregated and associated as the LSTT structure goes deeper, leading to more accurate attention-based matching.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{Sources/occlusion.pdf}

    \caption{Visualization of long-term and short-term attention maps during the inference of DAVIS 2017~\cite{davis2017}. In this case, the red person is occluded in the  frame, and thus the short-term attention fails to match the person in the current frame. However, the long-term attention generates an accurate attention map with a clean background in the last layer, , resulting in a correct prediction.}
    \label{fig:occlusion}

\end{figure} 



\subsection{Compare with More Methods}

We compare our AOT with more VOS methods in Table~\ref{tab:full_comparisons} and~\ref{tab:full_davis2016}. To compare with latest real-time methods~\cite{realtimevos1,realtimevos2}, we introduce the real-time AOT variant, \ie, AOT-T. 

Even on the single-object DAVIS 2016~\cite{davis2016}, AOT-S (89.4\%, 40.0FPS) can achieve comparable speed with SAT~\cite{realtimevos1} (83.1\%, 39FPS) and comparable performance with CFBI~\cite{cfbi} (89.4\%, 6.3FPS). On the multi-object DAVIS 2017~\cite{davis2017}, AOT-T (79.9\%, 51.4FPS) significantly outperforms SAT (72.3\%, 19.5FPS) and GC (71.4\%, 12.5FPS). Particularly, on the large-scale multi-object YouTube-VOS~\cite{youtubevos}, AOT-T/S (80.2\%/82.6\%) achieves superior performance compared to previous real-time methods, SAT (63.6\%) and GC (73.2\%), while still maintaining a real-time speed (41.0FPS/27.1FPS).

\begin{table}[t!]
	\centering
	\caption{Additional quantitative comparison on multi-object benchmarks, YouTube-VOS~\cite{youtubevos} and DAVIS 2017~\cite{davis2017}. 
	}\label{tab:full_comparisons}
	
\begin{subtable}{.54\textwidth}
\centering
\caption{YouTube-VOS}\label{tab:full_youtubevos}
\setlength{\tabcolsep}{3pt}
\small
\vspace{-0.5mm}
\begin{tabular}{lcccccc}
	
\toprule[1.5pt]
            &   &  \multicolumn{2}{c}{Seen}  &    \multicolumn{2}{c}{Unseen} &  \\
\midrule[1pt]
 Methods &  &  &  &  &  & FPS \\
\midrule[1pt]
\multicolumn{7}{c}{\textit{Validation 2018 Split}} \\
\midrule[1pt]
SAT\pub{CVPR20}~\cite{realtimevos1}  &  63.6  &  67.1  & 70.2  &  55.3  & 61.7 & - \\
AG\pub{CVPR19}~\cite{agame}  &  66.1  &  67.8  &  -  &  60.8  &  - & - \\
PReM\pub{ACCV18}~\cite{premvos}    &  66.9  &  71.4  &  75.9  &  56.5  &  63.7 & 0.17 \\
BoLT\pub{arXiv19}~\cite{boltvos}   &  71.1  &  71.6  &  -  &  64.3  &  - & 0.74 \\

GC\pub{ECCV20}~\cite{realtimevos2}   & 73.2  & 72.6  & 75.6 & 68.9 & 75.7  & - \\
STM\pub{ICCV19}~\cite{spacetime}   &  79.4  &  79.7  &  84.2  &  72.8  &  80.9 & - \\
EGMN\pub{ECCV20}~\cite{EGMN}  &  80.2  &  80.7  &  85.1  &  74.0  &  80.9 & - \\
KMN\pub{ECCV20}~\cite{KMN}  &  81.4  &  81.4  &  85.6  &  75.3  &  83.3 & - \\
CFBI\pub{ECCV20}~\cite{cfbi} &  81.4  &  81.1  & 85.8  & 75.3  & 83.4 & 3.4 \\
LWL\pub{ECCV20}~\cite{LWLVOS} & 81.5  &  80.4  &  84.9  &  76.4  &  84.4 & - \\
SST\pub{CVPR21}~\cite{sstvos} & 81.7  &  81.2  &  -  &  76.0  &  - & - \\
CFBI+\pub{TPAMI21}~\cite{cfbip} &  82.8  &  81.8  & 86.6  & 77.1  & 85.6 & 4.0 \\
\hline

AOT-T & 80.2  &  80.1  & 84.5  & 74.0  & 82.2 & \textbf{41.0} \\
AOT-S & 82.6  &  82.0  & 86.7  & 76.6  & 85.0 & {27.1} \\
AOT-B & 83.5  &  {82.6}  & 87.5  & {77.7}  & 86.0 & 20.5 \\
AOT-L & {83.8}  &  {82.9}  & {87.9}  & {77.7}  & \textbf{86.5} & 16.0 \\
R50-AOT-L & {84.1}  &  {83.7}  & {88.5}  & \textbf{78.1}  & {86.1} & 14.9 \\
SwinB-AOT-L & \textbf{84.5}  &  \textbf{84.3}  & \textbf{89.3}  & {77.9}  & {86.4} & 9.3 \\
\midrule[1pt]
\multicolumn{7}{c}{\textit{Validation 2019 Split}} \\
\midrule[1pt]
CFBI\pub{ECCV20}~\cite{cfbi}  &  81.0  &  {80.6}  & {85.1}  & {75.2}  & {83.0} & 3.4  \\
SST\pub{CVPR21}~\cite{sstvos} & 81.8  &  80.9  &  -  &  76.6  &  - & - \\
CFBI+\pub{TPAMI21}~\cite{cfbip} &  82.6  &  81.7  & 86.2  & 77.1  & 85.2 & 4.0 \\
\hline
AOT-T &  79.7  &  79.6  & 83.8  & 73.7  & 81.8 & \textbf{41.0} \\
AOT-S &  82.2 &  81.3  & 85.9  & 76.6  & 84.9 & {27.1} \\
AOT-B &  83.3 &  {82.4}  & {87.1}  & 77.8  & 86.0 & 20.5 \\
AOT-L &  {83.7} &  {82.8}  & {87.5}  & {78.0}  & \textbf{86.7} & 16.0 \\
R50-AOT-L & {84.1}  &  {83.5}  & {88.1}  & \textbf{78.4}  & {86.3} & 14.9 \\
SwinB-AOT-L & \textbf{84.5}  &  \textbf{84.0}  & \textbf{88.8}  & \textbf{78.4}  & \textbf{86.7} & 9.3 \\

\bottomrule[1.5pt]
\end{tabular}

\end{subtable}
\begin{subtable}{.45\textwidth}










\caption{DAVIS 2017}\label{tab:full_davis}
\begin{center}
\small
\vspace{-2.3mm}
\setlength{\tabcolsep}{5pt}
\begin{tabular}{l c c c c}
\toprule[1.5pt]
 Methods  &  &  &  & FPS \\
\midrule[1pt]
\multicolumn{5}{c}{\textit{Validation 2017 Split}} \\
\midrule[1pt]
OSMN~\cite{osmn}  &  54.8  & 52.5  & 57.1 & 3.6 \\
VM~\cite{videomatch} & 62.4  & 56.5 & 68.2 & 2.9 \\
OnA~\cite{onavos}    &  63.6  & 61.0  & 66.1 & 0.04 \\
RGMP~\cite{rgmp}    & 66.7 & 64.8   & 68.6 & 3.6 \\
AG~\cite{agame} (\textbf{Y})    & 70.0  &  67.2  & 72.7 & 7.1 \\
GC~\cite{realtimevos2}   & 71.4  & 69.3  & 73.5  & 12.5 \\
FEEL~\cite{feelvos} (\textbf{Y})   &  71.5  &  69.1  & 74.0 & 2.0 \\
SAT~\cite{realtimevos1} (\textbf{Y})  &  72.3 & 68.6  & 76.0  & 19.5 \\
PReM~\cite{premvos}   &  77.8  &  73.9  & 81.7 & 0.03 \\
LWL~\cite{LWLVOS} (\textbf{Y}) &  81.6  & 79.1  & 84.1 & 2.5 \\
STM~\cite{spacetime} (\textbf{Y})  &  81.8  & 79.2  & 84.3 & 3.1 \\
CFBI~\cite{cfbi} (\textbf{Y}) &  81.9  & 79.3  & 84.5 & 5.9 \\
SST~\cite{sstvos} (\textbf{Y}) &  82.5  & 79.9  & 85.1 & - \\
EGMN~\cite{EGMN} (\textbf{Y})  &  82.8  & 80.2  & 85.2 & 2.5 \\
KMN~\cite{KMN}  &  76.0  & 74.2  & 77.8 & 4.2 \\
KMN~\cite{KMN} (\textbf{Y}) &  82.8  & 80.0  & 85.6 & 4.2 \\
CFBI+~\cite{cfbip} (\textbf{Y}) &  82.9  & 80.1  & 85.7 & 5.6 \\
\hline
AOT-T (\textbf{Y}) & 79.9 & 77.4 & 82.3 & \textbf{51.4} \\
AOT-S  &  79.2  & 76.4 & 82.0 & 40.0 \\
AOT-S (\textbf{Y}) & 81.3 & 78.7 & 83.9 & {40.0} \\
AOT-B (\textbf{Y}) & 82.5 & 79.7 & 85.2 & 29.6 \\
AOT-L (\textbf{Y}) & {83.8} & {81.1} & {86.4} & 18.7 \\
R50-AOT-L (\textbf{Y}) & {84.9} & {82.3} & {87.5} & 18.0 \\
SwinB-AOT-L (\textbf{Y}) & \textbf{85.4} & \textbf{82.4} & \textbf{88.4} & 12.1 \\
\midrule[1pt]
\multicolumn{5}{c}{\textit{Testing 2017 Split}} \\
\midrule[1pt]
OSMN~\cite{osmn}  &  41.3  & 37.7  & 44.9 & 2.4 \\
RGMP~\cite{rgmp}  & 52.9 & 51.3   & 54.4 & 2.4 \\
OnA~\cite{onavos}  &  56.5  & 53.4  & 59.6 & 0.03 \\
FEEL~\cite{feelvos} (\textbf{Y})  &  57.8  &  55.2  & 60.5 & 1.9 \\
PReM~\cite{premvos}   &  71.6  &  67.5  & 75.7 & 0.02 \\
STM~\cite{spacetime} (\textbf{Y}) &  72.2  & 69.3  & 75.2 & - \\
CFBI~\cite{cfbi} (\textbf{Y}) &  75.0  & 71.4  & 78.7 & 5.3 \\
CFBI~\cite{cfbi} (\textbf{Y}) &  76.6  & 73.0  & 80.1 & 2.9 \\
KMN~\cite{KMN} (\textbf{Y})  &  77.2  & 74.1  & 80.3 & - \\
CFBI+~\cite{cfbip} (\textbf{Y}) &  78.0  & 74.4  & 81.6 & 3.4 \\

\hline
AOT-T (\textbf{Y}) & 72.0 & 68.3 & 75.7 & \textbf{51.4} \\
AOT-S (\textbf{Y}) &  73.9  & 70.3 & 77.5 & 40.0 \\
AOT-B (\textbf{Y})  &  75.5  & 71.6 & 79.3 & 29.6 \\
AOT-L (\textbf{Y})  &  78.3  & 74.3 & 82.3 & 18.7 \\
R50-AOT-L (\textbf{Y}) & {79.6} & {75.9} & {83.3} & 18.0 \\
SwinB-AOT-L (\textbf{Y}) & \textbf{81.2} & \textbf{77.3} & \textbf{85.1} & 12.1 \\

\bottomrule[1.5pt]

\end{tabular}
\end{center}

\end{subtable}

\end{table}





 \begin{table}[t!]
\begin{center}
\vspace{-6.5mm}
\caption{Additional quantitative comparison on DAVIS 2016~\cite{davis2016}.}\label{tab:full_davis2016}

\setlength{\tabcolsep}{3pt}
\begin{tabular}{l c c c c}
\toprule[1.5pt]
 Methods  &  &  &  & FPS \\
\midrule[1pt]
OSMN~\cite{osmn}  &  - & 74.0  &   & 7.1 \\
PML~\cite{pml}  &  77.4 & 75.5  & 79.3  & 3.6 \\
VM~\cite{videomatch}  &   80.9  & 81.0  & 80.8  & 3.1 \\
FEEL~\cite{feelvos} (\textbf{Y})  & 81.7  &  81.1 &  82.2 & 2.2 \\
RGMP~\cite{rgmp}  & 81.8 &  81.5 & 82.0  & 7.1 \\
AG~\cite{agame} (\textbf{Y})  &  82.1 & 82.2  & 82.0  & 14.3 \\
SAT~\cite{realtimevos1} (\textbf{Y})  &  83.1 & 82.6  & 83.6  & 39 \\
OnA~\cite{onavos}   & 85.0  & 85.7  & 84.2  & 0.08 \\
GC~\cite{realtimevos2}   & 86.6  & 87.6  & 85.7  & 25 \\
PReM~\cite{premvos}   & 86.8  & 84.9  & 88.6  & 0.03 \\
STM~\cite{spacetime} (\textbf{Y})  &  89.3 & 88.7  & 89.9  & 6.3 \\
CFBI~\cite{cfbi} (\textbf{Y})  & 89.4  & 88.3 & 90.5  & 6.3 \\
CFBI+~\cite{cfbip} (\textbf{Y})  & 89.9  & 88.7 & 91.1  & 5.9 \\
KMN~\cite{KMN} (\textbf{Y})  &  90.5 & {89.5}  & {91.5}  & 8.3 \\
\hline
AOT-T (\textbf{Y}) &  86.8  & 86.1 & 87.4 & \textbf{51.4} \\
AOT-S (\textbf{Y}) &  89.4  & 88.6 & 90.2 & 40.0 \\
AOT-B (\textbf{Y}) &  89.9  & 88.7 & 91.1 & 29.6 \\
AOT-L (\textbf{Y}) &  {90.4}  & {89.6} & {91.1} & 18.7 \\
R50-AOT-L (\textbf{Y}) &  {91.1}  & {90.1} & {92.1} & 18.0 \\
SwinB-AOT-L (\textbf{Y}) &  \textbf{92.0}  & \textbf{90.7} & \textbf{93.3} & 12.1 \\
\bottomrule[1.5pt]

\end{tabular}
\end{center}
\end{table} 
\subsection{Additional Qualitative Results}
We supply more qualitative results under multi-object scenarios on the large-scale YouTube-VOS~\cite{youtubevos} and the small-scale DAVIS 2017~\cite{davis2017} in Fig.~\ref{fig:youtubevos} and~\ref{fig:davis2017}, respectively. As demonstrated, our AOT-L is robust to many challenging VOS cases, including similar objects, occlusion, fast motion, and motion blur, etc.

\subsection{Border Impact and Future Works}
The proposed AOT framework greatly simplifies the process of multi-object VOS and achieves a significant performance of effectiveness, robustness, and efficiency. Some AOT variants can achieve promising results while keeping real-time speed. In other words, AOT may promote the applications of VOS in real-time video systems, such as video conference, self-driving car, augmented reality, etc.

Nevertheless, the speed and accuracy of AOT can still be further improved. There is still a very large accuracy gap between the real-time AOT-T and the state-of-the-art SwinB-AOT-L. Moreover, AOT uses only a lightweight encoder and decoder. How to design stronger yet efficient encoders and decoders for VOS is still an open question.

As to related areas of VOS, the simple yet effective identification mechanism should also be promising for many tasks requiring multi-object matching, such as interactive video object segmentation~\cite{oh2019fast,miao2020memory}, video instance segmentation~\cite{vis,bertasius2020classifying,vist}, and multi-object tracking~\cite{milan2016mot16,pointtrack,wang2019towards}. Besides, the hierarchical LSTT may serve as a new solution for processing video representations in these tasks.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{Sources/ytb.pdf}

    \caption{Qualitative results on the validation 2019 split of YouTube-VOS~\cite{youtubevos}. Our AOT-L performs well under many challenging multi-object cases, including similar objects, occlusion, and fast motion, etc.}
    \label{fig:youtubevos}

\end{figure} \begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{Sources/davis.pdf}

    \caption{Qualitative results on DAVIS 2017~\cite{davis2017}.}
    \label{fig:davis2017}

\end{figure}  
\end{document}
