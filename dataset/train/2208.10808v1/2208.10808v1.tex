\section{Experiments}
\label{sec:experiments}

In this section, we perform extensive experiments to verify the effectiveness of the proposed method. All the
experiments are conducted on an NVIDIA v100 GPU. The models are implemented by PyTorch.
\subsection{Datasets}

We conduct experiments on a number of popular 2D face landmark detection datasets, including 300W, WFLW, COFW and
AFLW. 300W~\cite{300W2013} is collected from five facial datasets, consisting of  training images and  test
images. The test dataset is further divided into 2 subsets, \ie, common set with  images and challenging set
with  images. Each image is annotated with  landmarks.

WFLW~\cite{LAB} is collected from WIDER Face, which includes large variations in pose, expression and occlusion. Each
face is originally annotated by  landmarks, and re-annotated by  landmarks in~\cite{PIPNet2021}. There are  images for training and  for test. The test set is
further divided into 6 subsets for different scenarios. 

COFW~\cite{COFW2013} contains  training images and  test images under different occlusion conditions. Each image is annotated by  landmarks, and we also use  landmarks re-annotated by~\cite{COFWReanno} for the cross-domain setting.

AFLW~\cite{AFLW2011} contains  images for training and  images for test, providing  landmarks for each face.

CelebA~\cite{celeba2015} is a large-scale attributes
dataset with 202,599 face images in the wild. We only use the images without annotation for training in Section \ref{crossdataeva}.






\subsection{Implementation Details}

For all datasets, the faces are cropped according to the provided bounding boxes firstly, and then resized to . In order to retain more context information, the bounding boxes on 300W and WFLW are enlarged by 10 and 20, respectively, following previous work~\cite{PIPNet2021}.
Data augmentation is adopted involving translation, horizontal flipping, rotation, occlusion and blurring.
The whole model is trained end-to-end by Adam optimizer for 120 epochs in total. The learning rate is set to 1e-4 initially and then reduced to 1e-5 at th epoch, where the learning rate for backbone is 10 times smaller than the above. By default, we use 3 decoder layers, with a feature dimension of 256 and
8 heads. For each query, we sample 4 features for each head from each level of the feature maps. The configuration will be analyzed in ablation study. We train the model on 1 v100 GPU with a batch size of 16. The reported results are averaged over three runs.

\begin{figure*}[t]
    \centering
        \includegraphics[width=0.6\textwidth]{examples1.pdf}
        \vspace{-2mm}
    \caption{Visualization of typical landmark detection results.
\textcolor[rgb]{1,0,0}{Red} denotes the ground truth, and \textcolor[rgb]{0,1,1}{cyan} represents our predictions. Our model is able to detect landmarks accurately in various scenarios, such as blur, makeup, expression, occlusion, or even with big pose.}
\label{example}
\end{figure*}

\subsection{Evaluation Metric}

we adopt the most widely used metric, normalized mean error (NME), to evaluate our model for fair comparison with
previous work. It is calculated by,
\vspace{-1mm}


We employ the prediction from last decoder layer for evaluation.  is a normalization distance, and we use inter-ocular
distance for 300W, WFLW, COFW, and image size for AFLW, following common practice.
Failure rate (FR) is also reported which refers to the percentage of failed examples whose NMEs are larger than a certain threshold.

\begin{table*}[htbp]
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
        \setlength{\tabcolsep}{4mm}
        \scalebox{0.85}{
            \begin{tabular}{lcccccc}\hline
                Method & Year &  Backbone & NME() & Param.(M) & GFLOPs & FPS(GPU) \\
                \hline
                HRNet~\cite{HRNet2019}  & 2019 & HRNet-W18 &  &  &  &  \\
AWing~\cite{AWing}& 2019 & Hourglass &  &  &  &  \\
DeCaFa ~\cite{decafa} & 2019 & Cascaded U-Net &  &  &  & \\
                LUVLi~\cite{LUVLi}& 2020 & DU-Net  &  &  &  &  \\
                PIPNet-18~\cite{PIPNet2021} & 2020 & \textbf{ResNet-18} &  &  &  & 
                \\
PIPNet-101~\cite{PIPNet2021} & 2020 & ResNet-101 &  &  &  &  \\
BarrelNet-18~\cite{BarrelNet} & 2021 & ResNet-18 &  &  &  &   \\
BarrelNet-101~\cite{BarrelNet} & 2021 & ResNet-101 &  &  &  &  \\
                \hline
                \textbf{DTLD} & 2021 & \textbf{ResNet-18} &  &  & \textit{2.5} &  \\
                \textbf{DTLD+} & 2021 & \textbf{ResNet-18} &  &  & \textit{2.5} &  \\
                \hline
            \end{tabular}
        }
    \end{center}
\vspace{-5mm}
\caption{Comparison with other methods on Parameter size, GFLOPs and FPS. Our method achieves the highest accuracy with a small amount of GFLOPs and parameters. The FPS is lower than PIPNet-18, which leaves for future improving.}
\label{Tab:speed}
\vspace{-2mm}
\end{table*}



\subsection{Comparison with the SOTA}

As presented in Table~\ref{Tab:300w}, we firstly compare our model with SOTA methods on landmark detection accuracy using the four benchmarks. DTLD is the model with basic decoder, while DTLD-s has all model parameters trained from scratch. DTLD+ is equipped with the parallel decoder.
The models are trained and tested separately, with the default configuration.

The results show that our models consistently outperform all the other methods on all test datasets with a simple backbone. To be specific, our DTLD achieves the NME of ,  and  on 300W-Full, COFW and AFLW respectively. In addition, with the NME threshold of , the failure rates are ,  and  separately.
On WFLW-Full which contains various scenarios, DTLD obtains NME of , leading to a relative decrease of  compared to the second best ( NME), and  relative to BarrelNet-18 ( NME), the previous best method using the same backbone. The failure rates are  at the threshold of  and  at the threshold of . Comprehensive results on each WFLW subset are shown in supplementary.

Our model also benefits from the ImageNet pre-trained backbone. Without pre-training (referring to DTLD-s), NMEs increase a lot, but are still smaller than SOTA models with similar model size.
The use of the parallel decoder (DTLD+) improves the detection accuracy further, leading to NME of  on WFLW and  on COFW, averagely  lower than that obtained by DTLD.


Next, we compare the model size and running speed of our models with others. As presented in Table~\ref{Tab:speed}, DTLD has M parameters and only  GFLOPs, but achieves very competitive accuracy. The running speed is lower than PIPNet-18 and BarrelNet-18 because of the multiple refining process, but is still faster than others. DTLD+ achieves relatively higher accuracy at the sacrifice of running speed.



Some landmark detection results by DTLD are visualized in Figure~\ref{example}. Our model can accurately predict landmarks in the tough scenes for faces with blur, large posture changes, rich expressions, and partial occlusion.

\subsection{Ablation Studies on DTLD}
\label{utld_ex}

We conduct a series of ablation studies to analyze each part of the proposed model. The ablation experiments are performed on WFLW-Full as it includes comprehensive scenarios.

\begin{figure}[t]
    \centering
        \includegraphics[width=0.6\linewidth]{Q_effect1.pdf}
    \caption{Visualization of the effect of our query initialization. Fiducial landmark positions are produced by randomly initialized  (left), while our query initialization method sets up good starting points (right).}
    \vspace{-5mm}
\label{Qeffect}
\end{figure}



\noindent{\bf Effect of .} In DTLD, we use a well-calculated  as the initial
query features and calculate the initial reference points based on . Here, we perform experiments with
a randomly initialized learnt positional encodings as , as that used in~\cite{DETR2020,deformable2020}. Experimental result in Table~\ref{Tab:Abl1} shows a performance drop of NME on WFLW. We also visualize the effect in Figure~\ref{Qeffect}. As can be seen, our  will lead to image related initial reference points, instead of a fiducial landmark template.


\begin{figure}[!th]
    \centering
        \includegraphics[width=0.9\linewidth]{deformablepoints1.pdf}
        \vspace{-2mm}
    \caption{Visualizations of deformable attention on pyramid backbone features. The red cross denotes the ground-truth, while others dots show the sampling points with attention weights expressed by colors. The brighter the point, the greater the weight. We combine the sampling points from all heads for each feature map. Sampling points with attention weights lower than  are omitted.}
\vspace{-1mm}
\label{deformablepoints}
\end{figure}

\begin{table}[ht!]
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\begin{center}
		\scalebox{0.85}{
			\begin{tabular}{lccc}\hline
				Backbone &  & Self-Attn & NME() \\
				\hline
                R18  & FC &  \CheckmarkBold
                &  \\
                \hline
                R18 & Random Init. & \CheckmarkBold
                &  \\
R18 & FC & \XSolidBrush
                &   \\
R50 & FC & \CheckmarkBold
                &   \\
R101 & FC & \CheckmarkBold
                &   \\
				\hline
			\end{tabular}
		}
	\end{center}
\vspace{-5mm}
\caption{Ablation study on DTLD, with backbone,  initial strategy and self-attention analyzed. \emph{R18 / 50 / 101} represent ResNet-18 / 50 / 101 backbone pre-trained by ImageNet . \emph{FC} is our initialization compared with randomly initialized \emph{Random Init}.} \vspace{-2mm}
\label{Tab:Abl1}
\end{table}



\noindent{\bf Effect of self-attention.} Self-attention is employed in decoder layer so as to exploit the structural knowledge among landmark positions. Without self-attention, NME of DTLD reduces  ( to ) as indicated in Table~\ref{Tab:Abl1}.


\noindent{\bf Effect of backbone.} We experiment with different backbones as shown in Table~\ref{Tab:Abl1}. However,
the performance gain is not obvious (only NME improve) when using deeper backbone like ResNet-50 and ResNet-101, which means DTLD is not sensitive to much deeper backbone.


\noindent{\bf Effect of deformable attention.} We also visualize the multi-scale deformable attention as presented in Figure~\ref{deformablepoints}.
The visualization shows that the deformable module can extract most related image features around the landmark point for coordinate prediction. Moreover, the first decoder layer attends more on the rear feature maps like F3 and F4 that tend to high level global information, while the last decoder layer attends more on the frontal feature maps like F1 and F2 that are apt to capture low level local features for coordinate fine tuning.







\noindent{\bf Effect of model hyper-parameters.} Here we conduct experiments with different model hyper-parameters on DTLD, including the feature dimension  used in decoder
layers, the number of sampling points  used in deformable attention, and the head number used in all attention layers. As shown in Table~\ref{Tab:Abl2}, the higher the feature dimension, the better the performance, but  seems to be enough for feature encoding.  In our default configuration, for each query feature, we sample  points from each feature level for each head. We then test other numbers such as . However, the change has little impact on the final accuracy, indicating that our model can adaptively decouple the most critical information from redundant features.
We also run models with different head number in both self and deformable attention. More heads benefit final performance.
We visualize the deformable attention for each head in Supplementary. It illustrates intuitively that different heads will pay attention to different directions of image features.

\begin{table}[!t]
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\begin{center}
		\scalebox{0.85}{
			\begin{tabular}{lccc}\hline
				\tabincell{c}{\# Feature  \\ Dimension} & \tabincell{c}{\# Sampling  \\ Points} &  \tabincell{c}{\#
Attention \\ Head}  & NME ()  \\
                \hline
                256  & 4 & 8 &   \\
				\hline
                \textbf{64}  & 4 & 8 &   \\
\textbf{128}  & 4 & 8 &   \\
\textbf{512}  & 4 & 8 &   \\
				\hline
                256  & \textbf{2} & 8 &   \\
256  & \textbf{6} & 8 &   \\
                \hline
                256  & 4 & \textbf{4} &   \\
256  & 4 & \textbf{16} &   \\
				\hline
			\end{tabular}
		}
	\end{center}
\vspace{-5mm}
\caption{Ablation on DTLD model hyper-parameters including \emph{feature dimension}, \emph{sampling points}, and \emph{attention heads}. The effect of \emph{sampling points} is tiny, while that of the others is large.}
\label{Tab:Abl2}
\vspace{-2mm}
\end{table}

\begin{table*}[htb!]
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\begin{center}
		\scalebox{0.85}{
\begin{tabular}{lccccccc}\toprule
                \multirow{7}*{\makecell[c]{DTLD}} &
				\multirow{2}*{\makecell[c]{\# Encoder  \\ Layer}}
& \multicolumn{6}{c}{\# Decoder Layer}   \\
                \cline{3-8}
                & & 1 & 2 & 3 & 4 & 5 & 6  \\
				\cline{2-8}
				&0 & 4.417 / 12.1 / 165 & 4.187 / 12.7 / 123 & 4.076 / 13.3 / 100 & 4.068 / 14.0 / 82 & 4.044 / 14.6 /
69 & 4.064 / 15.3 / 60 \\
&1 & 4.369 / 12.6 / 105 & 4.178 / 13.2 / 86 & 4.066 / 13.9 / 71 & 4.026 / 14.5 / 64 & 4.050 / 15.1 /
56 & 4.051 / 15.7 / 51 \\&2 & 4.327 / 13.1 / 81 & 4.133 / 13.7 / 69 & 4.047 / 14.4 / 61 & 4.015 / 15.0 / 54 & 4.012 / 15.6 / 45
                & 4.000 / 16.2 / 43 \\	
&3 & 4.244 / 13.6 / 62 & 4.114 / 14.2 / 54 & 4.028 / 14.8 / 52 & 4.006 / 15.5 / 42 & 3.980 / 16.1 / 35
& 3.978 / 16.7 / 33 \\
&4 & 4.235 / 14.1 / 53 & 4.079 / 14.7 / 46 & 3.999 / 15.3 / 42 & 3.968 / 16.0 / 34 & 3.972 / 16.6 / 25
& 3.974 / 17.2 / 22 \\
                \hline
				DTLD+ & 0 & 4.417 / 12.1 / 115 & 4.185 / 12.7 / 94 & 4.054 / 13.3 / 78 & 4.022 / 14.0 / 69 & 4.016 / 14.6 /
63 & 3.996 / 15.3 / 55 \\
\bottomrule
			\end{tabular}
		}\end{center}
\vspace{-5mm}
\caption{Experimental results on WFLW by using varying encoder and decoder layers. More encoder or decoder layer contributes to higher performance. The last line shows the effect of our proposed parallel decoder. With similar parameters, DTLD+ achieves slightly higher accuracies. The results are demonstrated by NME(\%) / Model Parameter Size (M) /
FPS (on V100 GPU).}
\vspace{-2mm}
\label{Tab:unif}
\end{table*}



\noindent{\bf Effect of decoder.}
Encoder layers are commonly adopted to further encode the image features, \eg, ~\cite{DETR2020,deformable2020,PoseTrans,TFPose}.
Here we also conduct experiments by adding encoder in DTLD as in deformable DETR~\cite{deformable2020} and varying the number of layers in both encoder and decoder. Experimental results in Table~\ref{Tab:unif} show that the added encoder or decoder layers indeed contribute to reduce NME furthermore, even achieving NME smaller than  on WFLW. Results also show that the adding of decoder layers has relatively larger effect on accuracy than that of encoder layers. However, the added layer brings more parameters (0.5M for one encoder layer and 0.6M for one decoder layer) and degrades the speed.

To improve the accuracy without increasing model size, we propose the parallel decoder, where the image features are encoded along with the decoding process.
By sharing the deformable attention layers, the model size is almost the same as that without encoder layers (the little parameter increase comes from separate layer normalization), but NME further decreases. When using similar number of parameters, DTLD+ always obtains higher accuracy than DTLD counterparts. When using decoder layers , DTLD+ gets a higher accuracy compared to DTLD at similar speed.
It should be noted that when we use  parallel decoder layer, the model exactly becomes DTLD with  decoder layer and  encoder. The lower speed is caused by image feature updating which is not used anymore. However, it may provide a chance of inferring occluded face part based on the features so as to improve model performance further. We leave it as a future work.

Moreover, we attempt to remove the backbone and compute the pyramid features simply by image dividing and patch embedding as performed in~\cite{PVT}. The pyramid embeddings are fed into DTLD+ directly for feature encoding and landmark prediction. With 6 layers and only M parameters, our model achieves NME of  on WFLW.







\begin{table}[!t]\centering
    \scalebox{0.85}{
    \begin{tabular}{lccc}
        \toprule
        \multirow{2}*{Methods} & \multicolumn{3}{c}{Test Data}\\
        \cmidrule(){2-4}
& 300W &COFW68&WFLW68\\
        \midrule
        LAB~\cite{LAB} & 3.49 & 4.62 &  \\
        ODN~\cite{ODN} & 4.17 & 5.30 &  \\
        AVS w/SAN~\cite{AVS} & 3.86 & 4.43 &  \\
        DAG~\cite{DAG2020} & 3.04 & 4.22 & \\
        PIPNet(ST)~\cite{PIPNet2021} & 3.36 & 4.55 & 8.09 \\
        PIPNet(UDA)~\cite{PIPNet2021} & 3.35(-0.3) & 4.34(-4.6) & 7.45(-7.9) \\

        \midrule
        DTLD (ST) & 3.07 & 4.42 & 7.23 \\
DTLD (UDA) & \textbf{3.03(-1.3)} & \textbf{4.14(-6.3)} & \textbf{6.39(-11.6)} \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-3mm}
    \caption{Cross-dataset evaluation and comparison with others. \emph{ST} means supervised training only on 300W training data, but test on others. \emph{UDA} means unsupervised domain adaption by utilizing COFW and WFLW training images without annotation used.}
    \label{res:crossdataeva}
\end{table}

\begin{table}[!t]\centering
\scalebox{0.85}{
  \begin{tabular}{llccc}\toprule
    Methods &Unlabeled Data & 300W & WFLW\\
    \midrule
    \multirow{2}{*}{PIPNet~\cite{PIPNet2021}} &- & 3.36 &   \\
&CelebA & 3.27 (-2.7) &  \\
    \midrule
    \multirow{2}{*}{DTLD} &- & 3.07 & 4.08  \\
&CelebA & \textbf{2.94 (-4.2)} & \textbf{3.89 (-4.7)} \\
    \bottomrule
  \end{tabular}
  }
  \vspace{-2mm}
  \caption{Boost our model by using unlabeled images from other domain. Our model shows better scalability, which can be improved more by using unlabeled images. Note that it is the enlarged bounding boxes used in 300W that cause NME of , larger than  presented in Table~\ref{Tab:300w}.}
  \label{tab:WFLW_300W}
  \vspace{-1mm}
\end{table}


\subsection{Cross-dataset Evaluation}
\label{crossdataeva}


To verify the robustness and generalization ability of our model, we conduct cross-dataset evaluation on COFW and WFLW testsets, using DTLD trained on 300W training data. To maintain distribution consistency between different datasets, we follow the practice in~\cite{PIPNet2021}, enlarging the provided bounding boxes of 300W, COFW68, and WFLW68 by 30, 30 and 20 respectively. Experimental results in Table~\ref{res:crossdataeva} indicate the robustness of our model in cross dataset evaluation.

In addition, to analyze the model scalability, we perform an unsupervised domain adaption (UDA).  More precisely, we apply the classic self-training strategy and re-train the model using COFW and WFLW training images, without landmark annotation employed. The model trained on 300W is used as a teacher model to reason the pseudo-labels for unlabeled data. They are then combined with the original labeled data and re-train the model. After 3 times of re-training, we achieve NME of  on COFW68 and  on WFLW68, new SOTA accuracies on both testsets. Compared to PIPNet, the UDA improvement is more obvious, which demonstrates the good scalability of our method.



Motivated by the good scalability, we attempt to promote the model additionally by leveraging the numerous unlabeled face images from CelebA. With the same self-training paradigm, it is found that the detection accuracy can be further improved on 300W and WFLW-Full testsets.  As indicated in Table~\ref{tab:WFLW_300W}, although the unlabeled images are from a different domain compared with the test datasets, our model can still learn from them and leads to even more accurate landmark prediction.  It finally achieves NME of  on 300W and  on WFLW.

Another group of cross-dataset evaluation is performed following the experimental setting in~\cite{LUVLi}. To be specific, we train DTLD from scratch on the training data of 300W Split2, and evaluate it on 300W Split2 test data, Menpo frontal~\cite{Menpo,Menpo1,Menpo2} and COFW68. There are  images in 300W Split2 train
set and  images in test set.  The  near-frontal training images in Menpo 2D (denoted as Menpo frontal) are adopted here for evaluation, as well as the  test images in COFW68. Following~\cite{LUVLi}, here we adopt  and  as the evaluation metrics.  uses the geometric mean of the width and height of the ground-truth bounding box () as the normalization distance . AUC (Area Under Curve) is computed as the area under the cumulative distribution curve, up to a cutoff NME value. The cumulative distribution curve is plotted by the fraction of test images whose NME is less than or equal to the specific NME value on the horizontal axis. Here we use  and the cutoff value of . The lower the , the higher the  , the better the performance. Experimental results are presented in Table~\ref{crossdata2}. Our DTLD without any pretraining achieves the best detection accuracy on 300W Split2 test set and COFW68, even surpassing other methods pretrained on 300W-LP-2D~\cite{300WLP}. On Menpo frontal, our DTLD is still better than previous models without pretraining.

\begin{table}[!t]\centering
    \scalebox{0.75}{
    \begin{tabular}{lccc|ccc}
        \toprule
        \multirow{2}*{Methods} & \multicolumn{3}{c|}{ (\%) () }& \multicolumn{3}{c}{ (\%) ()}\\
        \cmidrule(){2-7}
& 300W & Menpo & COFW68  & 300W &Menpo&COFW68\\
        \midrule
        SAN*~\cite{SAN,SAN2} & 2.86 & 2.95 & 3.50 &59.7&61.9&51.9\\
        2D-FAN*~\cite{FAN} & 2.32 & 2.16 & 2.95 &66.5&69.0&57.5\\
        Softlabel*~\cite{SAN2} & 2.32 & 2.27 & 2.92&66.6&67.4&57.9\\
        KDN~\cite{KDN} & 2.49 & 2.26 & &67.3&68.2&  \\
        KDN*~\cite{KDN} & 2.21 & \textbf{2.01} & 2.73&68.3&71.1&60.1 \\
        LUVLi~\cite{LUVLi} & 2.24 & 2.18 & 2.75&68.3&70.1&60.8 \\
        LUVLi*~\cite{LUVLi} & 2.10 & 2.04 & 2.57&70.2&\textbf{71.9}&63.4 \\

        \midrule
        \textbf{DTLD-s}  & \textbf{2.05} & 2.10 & \textbf{2.47} & \textbf{70.9}& 71.8& \textbf{65.0} \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-3mm}
    \caption{Another group of cross-dataset evaluation. \textbf{DTLD-s} is our proposed DTLD model trained from scratch. The methods marked with * are pretrained on 300W-LP-2D. Our DTLD exceeds previous models without pretraining and is even better than some models pretrained on 300W-LP-2D. }
    \label{crossdata2}
\end{table}


\subsection{Failure Case Analysis}
\label{Sec:limit}

\begin{figure}[t!]
    \centering
        \includegraphics[width=0.95\linewidth]{failure.pdf}
    \caption{Visualizations of some typical failures. \textcolor[rgb]{1,0,0}{Red} represents the ground truth, and \textcolor[rgb]{0,1,1}{cyan} represents our predictions.}
\label{failure}
\end{figure}

Although our model shows strong superiority on facial landmark detection, it is still weak for face image with severe occlusions, especially obscured by other people, as illustrated in Figure~\ref{failure}. Specifically, 1) If the challenge (\ie, blurring, occlusion, \etc) causes a great uncertainty on face boundary inference, our model may fail. 2) If the face to be aligned is obscured by another face, our model has difficulty in distinguishing the target character, thus leading to large errors. 3) The ambiguity of landmark annotations may lead to poor performance, especially for landmarks on face
boundary.
For these weaknesses, a possible solution is to make better use of the connections between landmarks to infer the invisible part. We leave it as a future work.
