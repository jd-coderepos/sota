\appendix

\section{Theoretical results}
\subsection{Relation to Newton's method}
\label{appsec:relation-to-newton}

The main method presented in this paper is basically a realization of Newton's method in Banach space.
Finding the root of functional $\mathbf{q}(\mathbf{y}) = \mathbf{0}$ can be done by performing the Newton's method iteration $\mathbf{y}^{(i+1)} = \mathbf{y}^{(i)} - [D\mathbf{q}(\mathbf{y}^{(i)})]^{-1} \mathbf{q}(\mathbf{y}^{(i)})$ where $D\mathbf{q}$ is the Fr\'{e}chet derivative of $\mathbf{q}$.
In equation \ref{eq:general-delayed-differential-equation}, the function $\mathbf{q}(\mathbf{y})$ is given by $\mathbf{q}(\mathbf{y}(\mathbf{r})) = L[\mathbf{y}(\mathbf{r})] - \mathbf{f}(\mathbf{y}(\mathbf{r}-\mathbf{s}_1), ..., \mathbf{x}(\mathbf{r}), \theta)$.
Therefore, the Fr\'{e}chet derivative of functional $\mathbf{q}$ operated on a function $\mathbf{h}(\mathbf{r})$ is given by
\begin{align}
\label{eq:frechet-derivative}
    D\mathbf{q}(\mathbf{y})[\mathbf{h}(\mathbf{r})] = L[\mathbf{h}(\mathbf{r})] - \sum_{p=1}^P\partial_p{\mathbf{f}}\mathbf{h}(\mathbf{r} - \mathbf{s}_p).
\end{align}
Therefore, the Newton's method iteration to find the root of functional $\mathbf{q}(\mathbf{y}) = \mathbf{0}$ is
\begin{align}
    \mathbf{y}^{(i + 1)} = \mathbf{y}^{(i)} - \left(D\mathbf{q}(\mathbf{y})\right)^{-1}\left[L[\mathbf{y}^{(i)}] - \mathbf{f}(\mathbf{y}^{(i)}(\mathbf{r}-\mathbf{s}_1), ..., \mathbf{x}(\mathbf{r}), \theta)\right].
\end{align}
The inverse Fr\'{e}chet derivative terms can be computed by solving the equation below for $\mathbf{y}^{(i)} - \mathbf{y}^{(i + 1)}$,
\begin{equation}
    D\mathbf{q}( \mathbf{y})\left[\mathbf{y}^{(i)} - \mathbf{y}^{(i + 1)}\right] = L[\mathbf{y}^{(i)}] - \mathbf{f}(\mathbf{y}^{(i)}(\mathbf{r} - \mathbf{s}_1), ..., \mathbf{x}(\mathbf{r}), \theta)
\end{equation}
Expanding $D\mathbf{q}(\mathbf{y})$ from equation \ref{eq:frechet-derivative} on $\mathbf{y}^{(i)}$, we obtain
\begin{align}
    D\mathbf{q}(\mathbf{y})[\mathbf{y}^{(i+1)}] = \mathbf{f}(\mathbf{y}^{(i)}(\mathbf{r} - \mathbf{s}_1), ..., \mathbf{x}(\mathbf{r}), \theta) - \sum_{p=1}^P \partial_p{\mathbf{f}}\mathbf{y}^{(i)}(\mathbf{r} - \mathbf{s}_p)
\end{align}
Solving for $\mathbf{y}^{(i+1)}$ on the equation above will produce the equation \ref{eq:yr-iterative equation}, showing that equation \ref{eq:yr-iterative equation} is just a realization of Newton's method in solving equation \ref{eq:general-delayed-differential-equation}.

\subsection{Relation to Direct Multiple Shooting}
\label{appsubsec:relation-to-direct-multiple-shooting}

One popular way to parallelize an ODE solver is to perform direct multiple shooting (MS) method \citep{chartier1993parallel, massaroli2021differentiablemultipleshooting}.
The following parts will follow \cite{massaroli2021differentiablemultipleshooting} very closely.
Consider an initial value problem with $d\mathbf{y}/dt = \mathbf{f}(\mathbf{y}, t)$ for $t \in [0, T]$ with $\mathbf{y}(0) = \mathbf{y}_0$.
With MS, we first split the time horizon into $N$ multiple regions with $0 = t_0 < t_1 < t_2 < ... < t_N = T$, have a guess on initial values at each region, $\mathbf{y}(t_i) = \mathbf{b}_i$, then apply the ODE solver for each region.
After applying the ODE, the guessed values $\mathbf{b}_i$ can then be updated to match the results of ODE from the previous region.
Denote a function $\boldsymbol{\phi}(\mathbf{b}_i, t_i, t_{i+1})$ as the value of $\mathbf{y}$ at $t_{i+1}$ after solving the initial value problem with initial value $\mathbf{y}(t_i) = \mathbf{b}_i$.
Evaluating this function $\boldsymbol{\phi}$ requires solving an ODE from $t_i$ to $t_{i+1}$.
With the function above, we can write the constraints to be solved, $\mathbf{b}_{i+1} = \boldsymbol{\phi}(\mathbf{b}_i, t_i, t_{i+1})$ with $\mathbf{b}_0 = \mathbf{y}_0$.

The works by \cite{chartier1993parallel} and \cite{massaroli2021differentiablemultipleshooting} update $\mathbf{b}_i$ by applying Newton method.
Specifically, at $k$-th iteration with values of $\mathbf{b}_i^{(k)}$ are available for all $i$-s, the values at the next iteration can be determined by \citep{massaroli2021differentiablemultipleshooting}
\begin{align}
\label{appeq:newton-update-multi-shooting}
    \mathbf{b}_{i+1}^{(k+1)} = \boldsymbol{\phi}(\mathbf{b}_i^{(k)}, t_i, t_{i+1}) + \frac{\partial \boldsymbol{\phi}}{\partial \mathbf{b}}(\mathbf{b}_i^{(k)}, t_i, t_{i+1}) \left({\color{blue}\mathbf{b}_i^{(k+1)}} - \mathbf{b}_i^{(k)}\right);\ \ \mathbf{b}_0^{(k+1)} = \mathbf{y}_0.
\end{align}
The term ${\color{blue}\mathbf{b}_i^{(k+1)}}$ makes the equation above needs to be evaluated sequentially, in addition to computing the function $\boldsymbol{\phi}$ that requires solving an ODE.

If we assume there are large number of regions, $N \rightarrow \infty$ and $\Delta t_i = t_{i+1} - t_i \rightarrow 0$, then the MS method becomes equivalent to the DEER realization on solving the ODE.
In this limit, we can write
\begin{align}
\label{appeq:phi-approximations}
    \boldsymbol\phi(\mathbf{b}_i, t_i, t_{i+1}) = \mathbf{b}_i + \mathbf{f}(\mathbf{b}_i, t_i) \Delta t_i\ \ \mathrm{and}\ \ \frac{\partial \boldsymbol{\phi}}{\partial \mathbf{b}} = \mathbf{I} + \frac{\partial \mathbf{f}}{\partial \mathbf{b}}(\mathbf{b}_i, t_i),
\end{align}
with $\Delta t_i = t_{i+1} - t_i$ and $\mathbf{I}$ an identity matrix.
Substituting equation \ref{appeq:phi-approximations} to equation \ref{appeq:newton-update-multi-shooting}, we obtain
\begin{align}
\label{appeq:newton-update-multi-shooting-linear}
    \mathbf{b}_{i+1}^{(k+1)} = \left(\mathbf{I} + \frac{\partial \mathbf{f}}{\partial\mathbf{b}}(\mathbf{b}_i^{(k)}, t_i) \Delta t_i\right)\mathbf{b}_i^{(k+1)} + \left[\mathbf{f}(\mathbf{b}_i^{(k)}, t_i) - \frac{\partial \mathbf{f}}{\partial \mathbf{b}}(\mathbf{b}_i^{(k)}, t_i) \mathbf{b}_i^{(k)} \right] \Delta t_i.
\end{align}
By re-arranging the equation above, we get
\begin{align}
    \left(\frac{\mathbf{b}_{i+1}^{(k+1)} - \mathbf{b}_i^{(k+1)}}{\Delta t_i}\right) - \frac{\partial \mathbf{f}}{\partial\mathbf{b}}(\mathbf{b}_i^{(k)}, t_i) \mathbf{b}_i^{(k+1)} = \mathbf{f}(\mathbf{b}_i^{(k)}, t_i) - \frac{\partial \mathbf{f}}{\partial \mathbf{b}}(\mathbf{b}_i^{(k)}, t_i) \mathbf{b}_i^{(k)}.
\end{align}
With the limit $\Delta t_i \rightarrow 0$ and $N \rightarrow \infty$, the term $\mathbf{b}_i$ can be written as $\mathbf{y}(t_i)$ and $(\mathbf{b}_{i+1}^{(k+1)} - \mathbf{b}_i^{(k)}) / \Delta t_i$ equals to $d\mathbf{y}^{(k+1)} / dt$.
By rewriting the equation above in $\mathbf{y}$, we obtain
\begin{align}
    \frac{d\mathbf{y}^{(k+1)}}{dt} - \frac{\partial \mathbf{f}}{\partial \mathbf{y}}(\mathbf{y}^{(k)}, t) \mathbf{y}^{(k+1)} = \mathbf{f}(\mathbf{y}^{(k)}, t) - \frac{\partial \mathbf{f}}{\partial \mathbf{y}}(\mathbf{y}^{(k)}, t) \mathbf{y}^{(k)}.
\end{align}
With a little bit of algebra, one can show that the equation above is equivalent to equations \ref{eq:yr-iterative equation} and \ref{eq:lginv-ode}.
By having a large number of regions, we can approximate $\boldsymbol\phi$ as in equation \ref{appeq:phi-approximations} that can be evaluated very quickly.

\subsection{Proof of quadratic convergence of DEER iteration}
\label{appsec:proof-of-quadratic-convergence}

This proof is inspired by standard results on the order of convergence of Newton's method and follows the argument of \cite{kelley1995iterative}, \S\S4 and 5.1. Throughout, we will make repeated use of the Cauchy-Schwarz (CS) and triangle (T) inequalities. 

We employ the standard Euclidean norm on $\mathbb{R}^n$,
\begin{equation}
    \big \| \mathbf{y} \big \| := \sqrt{\mathbf{y} \cdot \mathbf{y}} = \sqrt{y_1^2 + \dots + y_n^2} \ , \ \mathbf{y} = (y_1, \dots, y_n)^T \in \mathbb{R}^n,
\end{equation}
and the induced operator norm on $\mathbb{R}^{n \times n}$,
\begin{equation}
    \big \| \mathbf{A} \big \| := \textrm{sup} \big \{ \big \| \mathbf{Ay} \big \|, \ \big \| \mathbf{y} \big \| = 1, \ \mathbf{A} \in \mathbb{R}^{n \times n}, \ \mathbf{y} \in \mathbb{R}^{n} \big \}.
\end{equation}
Note that the Fr\'{e}chet derivative $D\mathbf{q}(\mathbf{y}) \in \mathbb{R}^{n \times n}$.

We denote the value of $\mathbf{y}$ at the $i$th iteration as $\mathbf{y}^{(i)}(\mathbf{r}) = \mathbf{y}^*(\mathbf{r})+\delta\mathbf{y}^{(i)}(\mathbf{r})$, where $\mathbf{y}^*$ exactly satisfies equation \ref{eq:yr-iterative equation}. Following on from \ref{appsec:relation-to-newton}, we assume that $D\mathbf{q}(\mathbf{y})$ is Lipschitz continuous with Lipschitz constant $\gamma$, i.e.:
\begin{align}
\label{eq:lipschitz}
    \big \| D\mathbf{q}(\mathbf{y}) - D\mathbf{q}(\mathbf{z}) \big \| \le \gamma \big \| \mathbf{y} - \mathbf{z} \big \| \quad \forall \ \mathbf{y}, \mathbf{z} \in \mathbb{R}^n.
\end{align}
Consider the operators $\mathbf{A} = D\mathbf{q}(\mathbf{y})$ and $\mathbf{B} = D\mathbf{q}(\mathbf{y}^*)$. We assume that both are invertible and that $\big \| \mathbf{I} - \mathbf{B}^{-1}\mathbf{A} \big \|  < 1$:

\begin{equation}
\begin{split}
    \mathbf{A}^{-1}\mathbf{B} & = \left(\mathbf{I} - \left(\mathbf{I} - \mathbf{B}^{-1}\mathbf{A}\right)\right)^{-1} \\
    & = \sum_{n=0}^{\infty}\left(\mathbf{I} - \mathbf{B}^{-1}\mathbf{A}\right)^n \qquad \textrm{(Neumann series for matrix inverse)}.
\end{split}
\end{equation}
Post-multiplying by $\mathbf{B}^{-1}$ and taking norms:
\begin{equation}
\begin{split}
    \big \| \mathbf{A}^{-1} \big \| & = \Bigg \| \left(\sum_{n=0}^{\infty}\left(\mathbf{I} - \mathbf{B}^{-1}\mathbf{A}\right)^n\right)\mathbf{B}^{-1} \Bigg \| \\
    & \overset{(CS)}{\le} \Bigg \| \sum_{n=0}^{\infty}\left(\mathbf{I} - \mathbf{B}^{-1}\mathbf{A}\right)^n \Bigg \| \ \big \| \mathbf{B}^{-1} \big \| \\
    & \overset{(T)}{\le} \left(\sum_{n=0}^{\infty}\Big \| \left(\mathbf{I} - \mathbf{B}^{-1}\mathbf{A}\right)^n \Big \|\right) \big \| \mathbf{B}^{-1} \big \| \\
    & \overset{(CS)}{\le} \left(\sum_{n=0}^{\infty}\big \| \mathbf{I} - \mathbf{B}^{-1}\mathbf{A} \big \|^n\right) \big \| \mathbf{B}^{-1} \big \|  \\
    & = \frac{\big \|\mathbf{B}^{-1}\big \|}{1-\big \|\mathbf{I} - \mathbf{B}^{-1}\mathbf{A}\big \|} \\ 
    \big \| [D\mathbf{q}(\mathbf{y})]^{-1} \big \| & \le \frac{\big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \|}{1-\big \|\mathbf{I} - [D\mathbf{q}(\mathbf{y}^*)]^{-1}D\mathbf{q}(\mathbf{y})\big \|}.
\end{split}
\end{equation}
We denote the ball of radius $\epsilon$ as $\mathcal{B}(\epsilon) = \{\mathbf{y} \ | \ \|\delta \mathbf{y}\| < \epsilon \}$, where $\delta \mathbf{y}=\mathbf{y}-\mathbf{y}^*$. 
\begin{equation}
\begin{split}
    \big \|\mathbf{I} - [D\mathbf{q}(\mathbf{y}^*)]^{-1}D\mathbf{q}(\mathbf{y})\big \| & = \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\left(D\mathbf{q}(\mathbf{y}^*) - D\mathbf{q}(\mathbf{y})\right)\big \| \\
    & \overset{(CS)}{\le} \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \ \big \|D\mathbf{q}(\mathbf{y}^*) - D\mathbf{q}(\mathbf{y})\big \| \\
    & \overset{(\ref{eq:lipschitz})}{\le}  \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \ \gamma  \big \|\mathbf{y}^* - \mathbf{y}\big \| \\
    & =  \gamma\big \|\delta\mathbf{y}\big \| \ \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \|    
\end{split}
\end{equation}
Let $0 < \epsilon < \left(2\gamma \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \|\right)^{-1}$. Then $\forall \ \mathbf{y} \in \mathcal{B}(\epsilon)$: 
\begin{equation}
\begin{split}
    \big \|\mathbf{I} - [D\mathbf{q}(\mathbf{y}^*)]^{-1}D\mathbf{q}(\mathbf{y})\big \| & \le  \gamma \big \|\delta\mathbf{y}\big \| \ \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \\
    & \le  \gamma \epsilon \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \\
    & < \frac{1}{2}.
\end{split}
\end{equation}
This means that
\begin{align}
\label{eq:norm-relation}
    \big \| [D\mathbf{q}(\mathbf{y})]^{-1} \big \| \le 2\big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \|.
\end{align}
The fundamental theorem of calculus can be expressed as:
\begin{equation}
\begin{split}
    \mathbf{q}(\mathbf{y})-\mathbf{q}(\mathbf{y}^*) & = \int_0^1 D\mathbf{q} \left(\mathbf{y}^*+\lambda\left(\mathbf{y}-
    \mathbf{y}^*\right)\right) \left(\mathbf{y}-\mathbf{y}^*\right) d\lambda \\
    \mathbf{q}(\mathbf{y}) & = \int_0^1 D\mathbf{q} \left(\mathbf{y}^*+\lambda\delta \mathbf{y}\right) \delta \mathbf{y} d\lambda .
\end{split}
\end{equation}
Therefore,
\begin{equation}
\begin{split}
    \delta \mathbf{y}^{(i+1)} & = \delta \mathbf{y}^{(i)} - [D\mathbf{q}(\mathbf{y}^{(i)})]^{-1} \mathbf{q}(\mathbf{y}^{(i)}) \\
    & = \delta \mathbf{y}^{(i)} - [D\mathbf{q}(\mathbf{y}^{(i)})]^{-1}\int_0^1 D\mathbf{q} \left(\mathbf{y}^*+\lambda\delta \mathbf{y}^{(i)}\right) \delta \mathbf{y}^{(i)} d\lambda \\
    & = [D\mathbf{q}(\mathbf{y}^{(i)})]^{-1}\int_0^1 \left(D\mathbf{q}(\mathbf{y}^{(i)})-D\mathbf{q} \left(\mathbf{y}^*+\lambda\delta \mathbf{y}^{(i)}\right)\right) \delta \mathbf{y}^{(i)} d\lambda.
\end{split}
\end{equation}
Let $\mathbf{y}^{(i)} \in \mathcal{B}(\epsilon)$. Then:
\begin{equation}
\begin{split}
    \big\| \delta \mathbf{y}^{(i+1)} \big\|& = \bigg \|[D\mathbf{q}(\mathbf{y}^{(i)})]^{-1}\int_0^1 \left(D\mathbf{q}(\mathbf{y}^{(i)})-D\mathbf{q} \left(\mathbf{y}^*+\lambda\delta \mathbf{y}^{(i)}\right)\right) \delta \mathbf{y}^{(i)} d\lambda \bigg \|  \\
    & \overset{(CS)}{\le} \big \|[D\mathbf{q}(\mathbf{y}^{(i)})]^{-1}\big \|\bigg \|\int_0^1 \left(D\mathbf{q}(\mathbf{y}^{(i)})-D\mathbf{q} \left(\mathbf{y}^*+\lambda\delta \mathbf{y}^{(i)}\right)\right) \delta \mathbf{y}^{(i)} d\lambda \bigg \| \\ 
    & \overset{(\ref{eq:norm-relation})}{\le} 2\big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \|\bigg \|\int_0^1 \left(D\mathbf{q}(\mathbf{y}^{(i)})-D\mathbf{q} \left(\mathbf{y}^*+\lambda\delta \mathbf{y}^{(i)}\right)\right) \delta \mathbf{y}^{(i)} d\lambda \bigg \|  \\
    & \overset{(T)}{\le} 2\big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \int_0^1 \Big \|\left(D\mathbf{q}(\mathbf{y}^{(i)})-D\mathbf{q} \left(\mathbf{y}^*+\lambda\delta \mathbf{y}^{(i)}\right)\right) \delta \mathbf{y}^{(i)} \Big \|d\lambda    \\ 
    & \overset{(CS)}{\le} 2\big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \int_0^1 \Big \|D\mathbf{q}(\mathbf{y}^{(i)})-D\mathbf{q} \left(\mathbf{y}^*+\lambda\delta \mathbf{y}^{(i)}\right) \Big \|d\lambda \ \big \|\delta \mathbf{y}^{(i)}\big \|   \\ 
    & \overset{(\ref{eq:lipschitz})}{\le}  2\big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \int_0^1 \gamma \Big \| \mathbf{y}^{(i)}-\left(\mathbf{y}^*+\lambda\delta \mathbf{y}^{(i)}\right) \Big \|d\lambda \ \big \|\delta \mathbf{y}^{(i)}\big \| \\
    & =  2\big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \int_0^1 \gamma \Big \| (1-\lambda)\delta \mathbf{y}^{(i)} \Big \|d\lambda \ \big \|\delta \mathbf{y}^{(i)}\big \| \\
    & \overset{(CS)}{\le}  2\big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \int_0^1 \gamma \big \| 1-\lambda\big \|d\lambda \ \big \|\delta \mathbf{y}^{(i)}\big \|^2  \\
    & =  2\big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| \ \frac{\gamma}{2} \ \big \|\delta \mathbf{y}^{(i)}\big \|^2 \\
    & =  \gamma \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \|  \ \big \|\delta \mathbf{y}^{(i)}\big \|^2. 
\end{split}
\end{equation}
So $\big\| \delta \mathbf{y}^{(i+1)} \big\|$ is bounded by $\big\| \delta \mathbf{y}^{(i)} \big\|^2$ with constant $\gamma \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \|$. For quadratic \textit{convergence}, the norms of successive errors must decrease. If we shrink $\epsilon$ such that $\gamma \epsilon \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \| < 1$, then:
\begin{equation}
\begin{split}
    \big\| \delta \mathbf{y}^{(i+1)} \big\|
    & \le \gamma \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \|  \ \big \|\delta \mathbf{y}^{(i)}\big \|^2  \\
    & < \gamma \epsilon \big \|[D\mathbf{q}(\mathbf{y}^*)]^{-1}\big \|  \ \big \|\delta \mathbf{y}^{(i)}\big \| \\
    & < \big \|\delta \mathbf{y}^{(i)}\big \|
\end{split}
\end{equation}
and the sequence therefore converges quadratically to $\mathbf{y}^*$. \hfill $\square$



\subsection{Application to Partial Differential Equations (PDEs)}
\label{appsubsec:pde-example}

We have mentioned that equation \ref{eq:general-delayed-differential-equation} can be applied to Partial Differential Equations (PDEs).
As this paper is concerning about Ordinary Differential Equations (ODEs) and discrete difference equation, we only present in this appendix the example of equation \ref{eq:general-delayed-differential-equation} in representing a simple PDE.
Let's rewrite the equation \ref{eq:general-delayed-differential-equation} as a reminder,
\begin{align}
    L[\mathbf{y}(\mathbf{r})] = \mathbf{f}\left(\mathbf{y}(\mathbf{r} - \mathbf{s}_1), ..., \mathbf{y}(\mathbf{r} - \mathbf{s}_P), \mathbf{x}(\mathbf{r}), \theta\right),
\end{align}
where $L[\cdot]$ is the linear operator, $\mathbf{f}$ is the non-linear function, $\mathbf{y}$ is the signal of interest, $\mathbf{x}$ is the external signal, $\mathbf{r}$ is the coordinate, $\theta$ is the parameters of $\mathbf{f}$, and $\mathbf{s}_i$ is the shifted location.

We will take the Burgers' equation as an example.
Burgers' equation can be written as
\begin{align}
    \frac{\partial u}{\partial t} + \frac{1}{2}\frac{\partial (u^2)}{\partial x} - \nu \frac{\partial^2 u}{\partial x^2} = 0
\end{align}
where $u$ is a function of $x$ and $t$, $u(x, t)$.
If we define $w = u^2$, we can write the Burgers' equation to be
\begin{align}
    \left(\begin{matrix}
        \frac{\partial}{\partial t} - v \frac{\partial^2}{\partial x^2} & \frac{1}{2}\frac{\partial}{\partial x} \\ 0 & 1
    \end{matrix}\right) \left(\begin{matrix}
        u \\ w
    \end{matrix}\right) = \left(\begin{matrix}
        0 \\ u^2
    \end{matrix}\right).
\end{align}
From the equation above, we can define the signal of interest $\mathbf{y} = (u, w)^T$, the coordinates $\mathbf{r}=(x, t)^T$, the non-linear function $\mathbf{f} = (0, u^2)^T$, and the linear operator
\begin{align}
    L = \left(\begin{matrix}
        \frac{\partial}{\partial t} - v \frac{\partial^2}{\partial x^2} & \frac{1}{2}\frac{\partial}{\partial x} \\ 0 & 1
    \end{matrix}\right).
\end{align}

Given the relations between the Burgers' equation and equation \ref{eq:general-delayed-differential-equation}, we can analytically compute the $\mathbf{G}$ matrix and the argument of $L_\mathbf{G}^{-1}$,
\begin{align}
    \mathbf{G} =& -\frac{\partial \mathbf{f}}{\partial \mathbf{y}} = \left(\begin{matrix}
        0 & 0 \\
        -2u & 0
    \end{matrix}\right) \\
    \mathbf{h} =& \mathbf{f} + \mathbf{Gy} = \left(\begin{matrix}
        0 \\
        -u^2
    \end{matrix}\right).
\end{align}
Therefore, the linear operation that needs to be solved, $L_\mathbf{G}^{-1}[\mathbf{h}]$, is
\begin{align}
    \frac{\partial u}{\partial t} + \frac{1}{2}\frac{\partial w}{\partial x} - \nu \frac{\partial^2 u}{\partial x^2} =& 0 \\
    g_{21}(x, t) u + w =& h_2(x, t),
\end{align}
given $h_2(x, t) = -u^2$ and $g_{21}(x, t) = -2u$ with $u$ from the previous iteration.
The equations above can still be further simplified into
\begin{align}
    \left[\frac{\partial}{\partial t} - \frac{g_{21}(x, t)}{2}\frac{\partial}{\partial x} - \frac{1}{2}\frac{\partial g_{21}}{\partial x}(x, t) - \nu \frac{\partial^2}{\partial x^2}\right] u =& -\frac{1}{2}\frac{\partial h_2}{\partial x}(x, t).
\end{align}
The equation above can be repeatedly evaluated until the convergence is achieved.

\subsection{Error bound on taking the midpoints in ODE}
\label{appsec:error-bound-midpoints-ode}

The exact analytical solution to the ODE
\begin{equation}\label{eq:ode-appendix}
    \frac{d\mathbf{y}}{dt}(t) + \mathbf{G}(t)\mathbf{y}(t) = \mathbf{z}(t) 
\end{equation}
is 
\begin{equation}\label{eq:ode-solution}
    \mathbf{y}(t) = e^{-\mathbf{M}(t)}\left[\mathbf{y}(0) + \int_0^t e^{\mathbf{M}(\tau)}\mathbf{z}(\tau) d\tau \right],
\end{equation}
where 
\begin{equation}
    \mathbf{M}(t) = \int_0^t \mathbf{G}(\lambda) d\lambda.
\end{equation}
Equation \ref{eq:ode-solution} is the form that equation \ref{eq:yr-iterative equation} takes in the ODE case. The approximation we wish to take involves inserting $\mathbf{G}(t) = \frac{1}{2}(\mathbf{G}(t_i)+\mathbf{G}(t_{i+1}))$ and $\mathbf{z}(t) = \frac{1}{2}(\mathbf{z}(t_i)+\mathbf{z}(t_{i+1}))$ into \ref{eq:ode-solution}, which yields equation \ref{eq:ode-recursive-equation}. In order to find the local truncation error when applying this method, we need to find and compare the Taylor expansions of equations \ref{eq:ode-solution} and \ref{eq:ode-recursive-equation}. 

We define $\square_0^{(n)} = \left.\frac{d^{n}\square}{dt^n}\right|_{t=0}$ so that, for instance, $\mathbf{G}_0 = \mathbf{G}(0), \mathbf{G}_0' = \left. \frac{d\mathbf{G}}{dt}\right|_{t=0}, \mathbf{G}_0'' = \left. \frac{d^2\mathbf{G}}{dt^2}\right|_{t=0}$ and $\mathbf{G}_0^{(3)} = \left. \frac{d^3\mathbf{G}}{dt^3}\right|_{t=0}$. We can then write the Taylor expansion of $\mathbf{M}(t)$ as:
\begin{equation}
\begin{split}
    \mathbf{M}(t) & = \int_{0}^t \mathbf{G}(\lambda) d \lambda \\
    & = \int_{0}^t \left(\mathbf{G}_0 +\mathbf{G}_0'\lambda +\frac{1}{2}\mathbf{G}_0''\lambda^2 + \mathcal{O}(\lambda^3) \right)d \lambda \\
    & = \mathbf{G}_0t + \frac{1}{2}\mathbf{G}_0't^2+\frac{1}{6}\mathbf{G}_0''t^3 + \mathcal{O}(t^4),     
\end{split}
\end{equation}
whence we immediately get 
\begin{equation}
    \mathbf{M}_0^{(n)} = \mathbf{G}_0^{(n-1)},
\end{equation}

as expected from the Leibniz integral rule. Therefore:
\begin{equation}
\begin{split}
    e^{\pm \mathbf{M}(t)} & = \mathbf{I} \pm \mathbf{M}(t) + \frac{1}{2}\mathbf{M}(t)^2 \pm \frac{1}{6}\mathbf{M}(t)^3 + \mathcal{O}(\mathbf{M}(t)^4) \\
    & = \mathbf{I} \pm \mathbf{G}_0t+\frac{1}{2}\left(\mathbf{G}_0^2 \pm \mathbf{G}_0'\right)t^2 \pm \frac{1}{12}\left(2\mathbf{G}_0^3 \pm 3\mathbf{G}_0\mathbf{G}_0' \pm 3\mathbf{G}_0'\mathbf{G}_0 + 2\mathbf{G}_0''\right)t^3 + \mathcal{O}(t^4) \\
    & \equiv \mathbf{I} \pm \mathbf{G}_0t+ \mathbf{A}_\pm t^2 \pm \mathbf{B}_\pm t^3 + \mathcal{O}(t^4),
\end{split}
\end{equation}
where for ease of notation we have introduced the new variables $\mathbf{A}_\pm = \frac{1}{2}\left(\mathbf{G}_0^2 \pm \mathbf{G}_0'\right)t^2$ and $\mathbf{B}_\pm = \frac{1}{12}\left(2\mathbf{G}_0^3 \pm 3\mathbf{G}_0\mathbf{G}_0' \pm 3\mathbf{G}_0'\mathbf{G}_0 + 2\mathbf{G}_0''\right)$. As the ODE in section \ref{sec:nonlin-to-fpi} is originally cast in the form $\frac{d\mathbf{y}}{dt} = \mathbf{f}(\mathbf{y}(t),\mathbf{x}(t),\theta)$, the right hand side of \ref{eq:ode-appendix} is
\begin{equation}
    \mathbf{z}(t) = \mathbf{f}(\mathbf{y}(t), \mathbf{x}(t),\theta)+\mathbf{G}(t)\mathbf{y}(t),
\end{equation}

which can be expressed using the Taylor expansions of $\mathbf{f}$, $\mathbf{G}$, and $\mathbf{y}$ as:
\begin{equation}
    \mathbf{z}(t) = \mathbf{f}_0 + \mathbf{G}_0\mathbf{y}_0 + \left(\mathbf{f}_0'+\mathbf{G}_0\mathbf{y}_0'+\mathbf{G}_0'\mathbf{y}_0\right)t + \frac{1}{2}\left(\mathbf{f}_0''+\mathbf{G}_0\mathbf{y}_0'' +2\mathbf{G}_0'\mathbf{y}_0'+\mathbf{G}_0''\mathbf{y}_0\right)t^2 + \mathcal{O}(t^3).
\end{equation}
Then, 
\begin{equation}
\begin{split}
    \int_{0}^t e^{\mathbf{M}(\tau)} \mathbf{z}(\tau) d \tau & = \int_{0}^t\left(
    \mathbf{I} + \mathbf{G}_0\tau+\mathbf{A}_{+}\tau^2 + \mathcal{O}(\tau^3)\right)\biggl(\mathbf{f}_0+\mathbf{G}_0\mathbf{y}_0 + \left(\mathbf{f}_0'+\mathbf{G}_0\mathbf{y}_0'  +\mathbf{G}_0'\mathbf{y}_0\right)\tau \\
    & \qquad  +\frac{1}{2}\left(\mathbf{f}_0''+\mathbf{G}_0\mathbf{y}_0''+2\mathbf{G}_0'\mathbf{y}_0'+\mathbf{G}_0''\mathbf{y}_0\right)\tau^2 + \mathcal{O}(\tau^3)\biggr)d \tau \\
    & = \left(\mathbf{f}_0+\mathbf{G}_0\mathbf{y}_0\right)t+\frac{1}{2}\left(\mathbf{f}_0'+\mathbf{G}_0\mathbf{y}_0'+\mathbf{G}_0'\mathbf{y}_0+\mathbf{G}_0\mathbf{f}_0+\mathbf{G}_0^2\mathbf{y}_0\right)t^2 \\
    & \qquad +\frac{1}{6}\left(\mathbf{f}_0'' +\mathbf{G}_0\mathbf{y}_0'' +2\mathbf{G}_0'\mathbf{y}_0'+\mathbf{G}_0''\mathbf{y}_0 +2\mathbf{G}_0\mathbf{f}_0'+2\mathbf{G}_0^2\mathbf{y}_0'+2\mathbf{G}_0\mathbf{G}_0'\mathbf{y}_0 \right. \\
    & \qquad \left. +\mathbf{G}_0^2\mathbf{f}_0 +\mathbf{G}_0^3\mathbf{y}_0 +\mathbf{G}_0'\mathbf{f}_0+\mathbf{G}_0'\mathbf{G}_0\mathbf{y}_0\right)t^3 + \mathcal{O}(t^4) \\
    & \equiv \mathbf{C}t + \mathbf{D}t^2 + \mathbf{E}t^3 + \mathcal{O}(t^4),
\end{split}
\end{equation}
where we have introduced $\mathbf{C}, \mathbf{D},$ and $\mathbf{E}$ as the coefficients of $t, t^2,$ and $t^3$ respectively. Then,
\begin{equation}
\begin{split}
    e^{-\mathbf{M}(t)}\int_{0}^t e^{\mathbf{M}(\tau)} \mathbf{z}(\tau) d \tau & = \Bigl(\mathbf{I} - \mathbf{G}_0t+\mathbf{A}_{-}t^2 + \mathcal{O}(t^3)\Bigr) \Bigl(\mathbf{C}t + \mathbf{D}t^2 + \mathbf{E}t^3 + \mathcal{O}(t^4)\Bigr) \\
    & = \left(\mathbf{f}_0+\mathbf{G}_0\mathbf{y}_0\right)t+\frac{1}{2}\left(\mathbf{f}_0'+\mathbf{G}_0\mathbf{y}_0'+\mathbf{G}_0'\mathbf{y}_0  -\mathbf{G}_0\mathbf{f}_0-\mathbf{G}_0^2\mathbf{y}_0\right)t^2 \\
    & \qquad +\frac{1}{6}\left(\mathbf{f}_0''+\mathbf{G}_0\mathbf{y}_0''+2\mathbf{G}_0'\mathbf{y}_0'+\mathbf{G}_0''\mathbf{y}_0 -\mathbf{G}_0\mathbf{f}_0'-\mathbf{G}_0^2\mathbf{y}_0' \right. \\
    & \qquad \left. -\mathbf{G}_0\mathbf{G}_0'\mathbf{y}_0+\mathbf{G}_0^2\mathbf{f}_0 +\mathbf{G}_0^3\mathbf{y}_0-2\mathbf{G}_0'\mathbf{f}_0-2\mathbf{G}_0'\mathbf{G}_0\mathbf{y}_0\right)t^3 + \mathcal{O}(t^4) \\
    & \equiv \mathbf{F}t + \mathbf{H}t^2 + \mathbf{J}t^3 + \mathcal{O}(t^4),
\end{split}
\end{equation}
where again we have introduced new variables for the coefficients: $\mathbf{F}, \mathbf{H},$ and $\mathbf{J}$. We define $\square_i = \square(t_i)$, and take $t_i = 0$ and $t = t_{i+1}$.  This means that $\square_0=\square_i$ and we can substitute $t$ with $\Delta_i$. Therefore, treating equation \ref{eq:ode-solution} as a fixed-point iteration problem gives:
\begin{equation}
\begin{split}
    \mathbf{y}^{(i+1)}(\Delta_i) 
    & = e^{-\mathbf{M}(\Delta_i)}\left[\mathbf{y}(0)+\int_{0}^{\Delta_i} e^{\mathbf{M}(\tau)} \left(\mathbf{f}(\mathbf{y}^{(i)}(\tau), \mathbf{x}(\tau), \theta)+\mathbf{G}(\tau)\mathbf{y}^{(i)}(\tau) \right) d \tau\right] \\
    & = \left(\mathbf{I} - \mathbf{G}_i\Delta_i + \mathbf{A}_{-}\Delta_i^2 - \mathbf{B}_{-}\Delta_i^3 \right)\mathbf{y}_i + \mathbf{F}\Delta_i + \mathbf{H}\Delta_i^2 + \mathbf{J}\Delta_i^3 + \mathcal{O}(\Delta_i^4) \\
    & = \mathbf{y}_i + \mathbf{f}_i \Delta_i + \frac{1}{2}\left(\mathbf{f}_i'+\mathbf{G}_i\mathbf{y}_i'-\mathbf{G}_i\mathbf{f}_i\right)\Delta_i^2 + \frac{1}{6}\biggl(\mathbf{f}_i''+\mathbf{G}_i\mathbf{y}_i''+2\mathbf{G}_i'\mathbf{y}_i'-\mathbf{G}_i\mathbf{f}_i'-\mathbf{G}_i^2\mathbf{y}_i' \\
    & \qquad \qquad  + \frac{1}{2}\mathbf{G}_i\mathbf{G}_i'\mathbf{y}_i+\mathbf{G}_i^2\mathbf{f}_i - 2\mathbf{G}_i'\mathbf{f}_i-\frac{1}{2}\mathbf{G}_i'\mathbf{G}_i\mathbf{y}_i\biggr)\Delta_i^3 + \mathcal{O}(\Delta_i^4).
\end{split} 
\end{equation}
As $\mathbf{z}(t) = \mathbf{f}(\mathbf{y}(t), \mathbf{x}(t), \theta) + \mathbf{G}(t)\mathbf{y}(t)$, equation \ref{eq:ode-recursive-equation} is:
\begin{equation}
\begin{split}
    \mathbf{y}_{i+1} & = e^{-\mathbf{G}_c\Delta_i}\mathbf{y}_i+\mathbf{G}_c^{-1}\left(\mathbf{I}-e^{-\mathbf{G}_c\Delta_i}\right)\left(\mathbf{f}_c+\mathbf{G}_c\mathbf{y}_c\right) \\
    & = \left(\mathbf{I}-\mathbf{G}_c\Delta_i+\frac{1}{2}\mathbf{G}_c^2\Delta_i^2-\frac{1}{6}\mathbf{G}_c^3\Delta_i^3\right)\mathbf{y}_i + \mathbf{G}_c^{-1}\left(\mathbf{G}_c\Delta_i-\frac{1}{2}\mathbf{G}_c^2\Delta_i^2+\frac{1}{6}\mathbf{G}_c^3\Delta_i^3\right) \\
    & \qquad \times \left(\mathbf{f}_c +\mathbf{G}_c\mathbf{y}_c\right) + \mathcal{O}(\Delta_i^4) \\
    & = \mathbf{y}_i+\left(\mathbf{f}_c+\mathbf{G}_c\left(\mathbf{y}_c-\mathbf{y}_i\right)\right)\Delta_i-\frac{1}{2}\left(\mathbf{G}_c\mathbf{f}_c+\mathbf{G}_c^2\left(\mathbf{y}_c-\mathbf{y}_i\right)\right)\Delta_i^2+\frac{1}{6}\left(\mathbf{G}_c^2\mathbf{f}_c+\mathbf{G}_c^3\left(\mathbf{y}_c-\mathbf{y}_i\right)\right) \\
    & \qquad \Delta_i^3+\mathcal{O}(\Delta_i^4),
\end{split}
\end{equation}
with $\mathbf{f}_c = \frac{1}{2}\left(\mathbf{f}_i+\mathbf{f}_{i+1}\right)$, $\mathbf{G}_c = \frac{1}{2}\left(\mathbf{G}_i+\mathbf{G}_{i+1}\right)$, and $\mathbf{y}_c = \frac{1}{2}\left(\mathbf{y}_i+\mathbf{y}_{i+1}\right)$ for $t_i \le t < t_{i+1}$. 

The Taylor expansions of the midpoint approximations are:
\begin{equation}
\begin{split}
    \mathbf{f}_c & = \mathbf{f}_i + \frac{1}{2}\mathbf{f}_i'\Delta_i + \frac{1}{4} \mathbf{f}_i''\Delta_i^2 + \mathcal{O}(\Delta_i^3) \\
    \mathbf{G}_c & = \mathbf{G}_i + \frac{1}{2}\mathbf{G}_i'\Delta_i + \frac{1}{4} \mathbf{G}_i'' \Delta_i^2 + \mathcal{O}(\Delta_i^3) \\
    \mathbf{y}_c & = \mathbf{y}_i + \frac{1}{2}\mathbf{y}_i'\Delta_i + \frac{1}{4} \mathbf{y}_i'' \Delta_i^2 + \mathcal{O}(\Delta_i^3).
\end{split}
\end{equation}

Recall that $\mathbf{y}^{(i+1)}(\Delta_i)$ is the $(i+1)$th iterated guess of the form of $\mathbf{y}$, given $\mathbf{y}(0)$ and the previous guess $\mathbf{y}^{(i)}(t)$, evaluated at $t = t_{i+1} = \Delta_i$. Meanwhile, $\mathbf{y}_{i+1}$ is the value of the approximation to $\mathbf{y}^{(i+1)}$ at $t = t_{i+1} = \Delta_i$, given $\mathbf{y}(0)$ and the previous guess $\mathbf{y}^{(i)}(t)$.
This gives the local truncation error as: 
\begin{equation}
\begin{split}
    \textrm{LTE} & = \mathbf{y}^{(i+1)}(\Delta_i)-\mathbf{y}_{i+1} \\
    & = \left[\mathbf{f}_i-\mathbf{f}_c-\mathbf{G}_c\left(\mathbf{y}_c-\mathbf{y}_i\right)\right]\Delta_i + \frac{1}{2}\left[\mathbf{f}_i'+\mathbf{G}_i\mathbf{y}_i'-\mathbf{G}_i\mathbf{f}_i+\mathbf{G}_c\mathbf{f}_c+\mathbf{G}_c^2\left(\mathbf{y}_c-\mathbf{y}_i\right)\right]\Delta_i^2 \\
    &  \qquad + \frac{1}{6}\left[\mathbf{f}_i''+\mathbf{G}_i\mathbf{y}_i''+2\mathbf{G}_i'\mathbf{y}_i'-\mathbf{G}_i\mathbf{f}_i'-\mathbf{G}_i^2\mathbf{y}_i' + \frac{1}{2}\mathbf{G}_i\mathbf{G}_i'\mathbf{y}_i+\mathbf{G}_i^2\mathbf{f}_i - 2\mathbf{G}_i'\mathbf{f}_i \right. \\ 
    & \qquad \left. -\frac{1}{2}\mathbf{G}_i'\mathbf{G}_i\mathbf{y}_i-\mathbf{G}_c^2\mathbf{f}_c-\mathbf{G}_c^3\left(\mathbf{y}_c-\mathbf{y}_i\right)\right]\Delta_i^3 + \mathcal{O}(\Delta_i^4) \\ 
    & = \left[\mathbf{f}_i - \mathbf{f}_i - \frac{1}{2}\mathbf{f}_i'\Delta_i - \frac{1}{4} \mathbf{f}_i''\Delta_i^2 - \frac{1}{2}\mathbf{G}_i\mathbf{y}_i'\Delta_i - \frac{1}{4}\mathbf{G}_i\mathbf{y}_i''\Delta_i^2 - \frac{1}{4}\mathbf{G}_i'\mathbf{y}_i'\Delta_i^2\right]\Delta_i \\
    & \qquad + \frac{1}{2}\left[\mathbf{f}_i'+\mathbf{G}_i\mathbf{y}_i'-\mathbf{G}_i\mathbf{f}_i+\mathbf{G}_i\mathbf{f}_i + \frac{1}{2}\mathbf{G}_i\mathbf{f}_i'\Delta_i + \frac{1}{2}\mathbf{G}_i'\mathbf{f}_i\Delta_i + \frac{1}{2}\mathbf{G}_i^2\mathbf{y}_i'\Delta_i\right]\Delta_i^2 \\
    &  \qquad + \frac{1}{6}\left[\mathbf{f}_i''+\mathbf{G}_i\mathbf{y}_i''+2\mathbf{G}_i'\mathbf{y}_i'-\mathbf{G}_i\mathbf{f}_i'-\mathbf{G}_i^2\mathbf{y}_i' + \frac{1}{2}\mathbf{G}_i\mathbf{G}_i'\mathbf{y}_i+\mathbf{G}_i^2\mathbf{f}_i - 2\mathbf{G}_i'\mathbf{f}_i \right. \\ 
    & \qquad \left. -\frac{1}{2}\mathbf{G}_i'\mathbf{G}_i\mathbf{y}_i-\mathbf{G}_i^2\mathbf{f}_i\right]\Delta_i^3 + \mathcal{O}(\Delta_i^4) \\
    & = \frac{1}{12}\Bigl[-\mathbf{f}_i'' - \mathbf{G}_i\mathbf{y}_i'' + \mathbf{G}_i'\mathbf{y}_i' + \mathbf{G}_i\mathbf{f}_i' + \mathbf{G}_i^2\mathbf{y}_i' + \mathbf{G}_i\mathbf{G}_i'\mathbf{y}_i - \mathbf{G}_i'\mathbf{f}_i - \mathbf{G}_i'\mathbf{G}_i'\mathbf{y}_i\Bigr]\Delta_i^3 + \mathcal{O}(\Delta_i^4).
\end{split}
\end{equation}
Recalling from section \ref{sec:nonlin-to-fpi} that $\mathbf{G}(t) = -\frac{\partial \mathbf{f}}{\partial \mathbf{y}}$, we define $\mathbf{f}_i^{(m,n)} = \left.\frac{\partial ^{m+n}\mathbf{f}}{\partial \mathbf{y}^m \partial \mathbf{x}^n}\right|_{t=0}$ and expand the total derivatives:
\begin{equation}
\begin{split}
    \mathbf{f}_i' & = \mathbf{f}_i^{(1,0)}\mathbf{y}_i'+\mathbf{f}_i^{(0,1)}\mathbf{x}_i'\\
    \mathbf{f}_i'' & = \mathbf{f}_i^{(1,0)}\mathbf{y}_i'' + \left(\mathbf{f}_i^{(2,0)}\mathbf{y}_i'\right)\mathbf{y}_i'  + \left(\mathbf{f}_i^{(1,1)}\mathbf{y}_i'\right)\mathbf{x}_i' + \left(\mathbf{f}_i^{(1,1)}\mathbf{x}_i'\right)\mathbf{y}_i' + \left(\mathbf{f}_i^{(0,2)}\mathbf{x}_i'\right)\mathbf{x}_i' + \mathbf{f}_i^{(0,1)}\mathbf{x}_i''\\
    \mathbf{G}_i & = -\mathbf{f}_i^{(1,0)} \\
    \mathbf{G}_i' & =  - \mathbf{f}_i^{(2,0)}\mathbf{y}_i' - \mathbf{f}_i^{(1,1)}\mathbf{x}_i'.\\
\end{split}
\end{equation}
The local truncation error can therefore also be written:
\begin{equation}
\begin{split}
    \textrm{LTE} & = \frac{1}{12}\Bigl[-2\left(\mathbf{f}_i^{(2,0)}\mathbf{y}_i'\right)\mathbf{y}_i' - \left(\mathbf{f}_i^{(1,1)}\mathbf{y}_i'\right)\mathbf{x}_i' - 2\left(\mathbf{f}_i^{(1,1)}\mathbf{x}_i'\right)\mathbf{y}_i' - \left(\mathbf{f}_i^{(0,2)}\mathbf{x}_i'\right)\mathbf{x}_i' - \mathbf{f}_i^{(0,1)}\mathbf{x}_i'' \\ 
    & \qquad - \mathbf{f}_i^{(1,0)}\mathbf{f}_i^{(0,1)}\mathbf{x}_i' + \mathbf{f}_i^{(1,0)}\left(\mathbf{f}_i^{(2,0)}\mathbf{y}_i'\right)\mathbf{y}_i + \mathbf{f}_i^{(1,0)}\left(\mathbf{f}_i^{(1,1)}\mathbf{x}_i'\right)\mathbf{y}_i + \left(\mathbf{f}_i^{(2,0)}\mathbf{y}_i'\right)\mathbf{f}_i \\ 
    & \qquad  + \left(\mathbf{f}_i^{(1,1)}\mathbf{x}_i'\right)\mathbf{f}_i - \left(\mathbf{f}_i^{(2,0)}\mathbf{y}_i'\right)\mathbf{f}_i^{(1,0)}\mathbf{y}_i - \left(\mathbf{f}_i^{(1,1)}\mathbf{x}_i'\right)\mathbf{f}_i^{(1,0)}\mathbf{y}_i\Bigr]\Delta_i^3 + \mathcal{O}(\Delta_i^4). 
\end{split}
\end{equation} 

Accordingly, taking the midpoint values for the approximation enables us to obtain a cubic local truncation error, $\textrm{LTE} = K\Delta_i^3 + \mathcal{O}(\Delta_i^4)$. 

\section{Experimental details}

\subsection{Code in JAX}
\label{appsubsec:code-in-jax}



\lstinputlisting[language=Python]{code.py}

\subsection{Hamiltonian neural network training with NeuralODE}
\label{appsec:hnn-training-details}

Let's denote $\mathbf{s}$ as the states of the system.
In the two-body case, the states are positions and velocities of the two masses, $\mathbf{s} = (x_1, y_1, v_{x1}, v_{y1}, x_2, y_2, v_{x2}, v_{y2})^T$.
For typical works in learning physical systems, a neural network is designed to take the states $\mathbf{s}$ as the input and produces the states dynamics $\mathbf{\dot{s}}$ as the output.
In those works, a set of states and the states dynamics are given as the training data, $\{..., (\mathbf{s}_i, \mathbf{\dot{s}}_i), ...\}$, so the neural network can just be trained without solving an ODE.

In our set up, the training data is only given as the states as a function of time $\{..., \mathbf{s}_i(t), ...\}$.
To train the model with the given dataset, we need to roll out the states as a function of time $\mathbf{s}(t)$ using NeuralODE.
In our case, the dataset is generated by generating the initial condition randomly and the dynamics are governed by the gravitational force.
The initial conditions are chosen so the orbits of the two-body system does not diverge and the orbits are still close to a circle, to make the simulation numerically stable.
The states are rolled out from $t=0$ to $t=10$ with 10,000 sampled time.
Within the time period, the system makes about 2-4 rounds of orbit.
There are 1000 rows of dataset generated and split into 800 for the training, 100 for the validation, and 100 for the test.

\begin{figure}[t!]
\centering
\begin{tikzpicture}[block/.style={rectangle, draw, fill=blue!20, text width=6em, text centered, rounded corners, minimum height=4em}, addnormblock/.style={rectangle, draw, fill=blue!20, text width=6em, text centered, rounded corners, minimum height=2em}]

\node [block] (encoder) {Encoder};
\node [block, above of=encoder, node distance=2.5cm] (deer) {GRU};
\node [addnormblock, above of=deer, node distance=1.35cm] (addnorm1) {Add \& Norm};
\node [block, above of=addnorm1, node distance=2.5cm] (mlp) {MLP};
\node [addnormblock, above of=mlp, node distance=1.35cm] (addnorm2) {Add \& Norm};
\node [block, above of=addnorm2, node distance=2.5cm] (decoder) {Decoder};
\node [block, above of=decoder, node distance=2.5cm] (mean) {Sequence Mean};



\draw[->] ++(0,-1.5cm) node[below] {Input} -- (encoder.south);
\draw[->] (encoder) -- (deer);
\draw[-] (deer) -- (addnorm1);
\draw[->] (addnorm1) -- (mlp);
\draw[-] (mlp) -- (addnorm2);
\draw[->] (addnorm2) -- (decoder);
\draw[->] (decoder) -- (mean);
\draw[->] (mean.north) -- ++(0,.75cm) node[above] {output};

\draw[->, rounded corners] ($(deer.south)-(0,.3cm)$) -- ++(1.75cm,0) |- (addnorm1.east);
\draw[->, rounded corners] ($(mlp.south)-(0,.3cm)$) -- ++(1.75cm,0) |- (addnorm2.east);


\node[draw, thick, dashed, fit=(deer) (addnorm2), inner xsep=30pt, inner ysep=20pt, 
label={[anchor=north west,inner sep=5pt,font=\tiny]north west:{$\times$5}}] (box) {};

\end{tikzpicture}
\caption{Architecture for the EigenWorms experiments.}
\label{appfig:eigenworms}
\end{figure}



For the model, we are using Hamiltonian neural network \citep{greydanus2019hamiltonian} that consists of 6 linear layers with softplus activation except on the last linear layer.
The hidden layers have 64 elements each.
The input to the network has 8 elements (corresponding to the states $\mathbf{s}$) and the output of the network has only 1 element that corresponds to the Hamiltonian value.

For every training step during the training with DEER method, we save the predicted trajectory for every row of the dataset.
The saved trajectory will be used as the initial guess of the DEER method for the next training step.
The training was performed using ADAM optimizer with $10^{-3}$ learning rate.
To speed up the training, we start from 20 time points at the beginning of the training and increase the number of time points by 20 every 1 epoch (50 training steps) until it reaches the maximum 10k time points.
The loss function for the training is a simple mean squared error.

\subsection{Time series classification with GRU}
\label{appsec:gru-training-details}

The dataset used in this case is Eigenworms \citep{brown2013dictionaryeigenworms}. There are 259 worm samples with very long sequences (length $N = 17984$) in this dataset, which are then divided into train, validation and test sets using a 70\%, 15\%, 15\% split following \cite{morrill2021neuralrough}. Each sample is classified as either wild-type or one of four mutant types based on the representation using six base shapes.

The model used is shown in Figure \ref{appfig:eigenworms}. It consists of four main components: encoder, GRU, MLP and decoder. The encoder would project the input features to a higher dimension of 32, and the encoded input would then go through 5 layers of GRU and MLP pair, in that order. The number of hidden states for each GRU and MLP is set to 32. Then, the output of the GRU and MLP pair would go through a decoder to project it down to 5 classes. We then take the mean over the sequence length to obtain the final output. Residual connection followed by LayerNorm is applied to each GRU and MLP sublayer. Both the encoder and decoder are also simple MLPs, and all MLPs in the architecture have a depth of 1 with ReLU activation function in the hidden layer.

The training is done using cross entropy loss with the patience for early stopping set to 1000 epochs per validation accuracy. The optimization algorithm we used is ADAM optimizer with $3\times 10^{-5}$ learning rate and the gradient is clipped at 1.0 per global norm. The initial guesses for the DEER method are always initialized at zeros.














\section{Additional results}

\subsection{Benchmarks}
\label{appsec:benchmarks}

\begin{table}
\caption{
    The speed up of GRU calculated using DEER method (this paper) vs commonly-used sequential method on a V100 GPU with batch size of 16, 8, 4, and 2.
    The missing data for large number of dimensions and sequence lengths are due to insufficient memory in computing the DEER method.
    The numbers in the table represent the mean speed up over 5 different random seeds.}
\label{apptab:fwd-speedup}
\begin{center}
\begin{tabular}{|c|ccccccc|}
\hline
~ & ~ & \multicolumn{5}{c}{Sequence lengths (batch size = 16)} & ~ \\
\#dims & 1k & 3k & 10k & 30k & 100k & 300k & 1M \\ \hline
1 & 15.7 & 43.5 & 114 & 182 & 309 & 394 & 516 \\ 2 & 24.4 & 47.6 & 75.0 & 78.4 & 72.4 & 77.1 & 74.6 \\ 4 & 22.1 & 35.4 & 46.5 & 44.4 & 52.1 & 54.2 & 51.3 \\ 8 & 14.8 & 20.0 & 21.9 & 24.0 & 25.2 & 25.1 & \text{-} \\ 16 & 7.11 & 8.37 & 8.89 & 9.23 & 9.46 & \text{-} & \text{-} \\ 32 & 3.74 & 3.76 & 4.16 & 4.24 & \text{-} & \text{-} & \text{-} \\ 64 & 1.29 & 1.26 & 1.27 & \text{-} & \text{-} & \text{-} & \text{-} \\ \hline
\end{tabular}

\begin{tabular}{|c|ccccccc|}
\hline
~ & ~ & \multicolumn{5}{c}{Sequence lengths (batch size = 8)} & ~ \\
\#dims & 1k & 3k & 10k & 30k & 100k & 300k & 1M \\
\hline
1 & 16.9 & 44.9 & 131 & 264 & 521 & 604 & 945 \\
2 & 22.5 & 58.0 & 113 & 149 & 119 & 141 & 138 \\
4 & 24.1 & 50.7 & 79.0 & 95.9 & 101 & 103 & 104 \\
8 & 21.4 & 31.9 & 42.3 & 45.1 & 47.9 & 49.6 & 49.6 \\
16 & 11.7 & 15.3 & 16.8 & 18.5 & 18.6 & 18.7 & - \\
32 & 6.32 & 7.72 & 8.02 & 8.32 & 8.30 & - & - \\
64 & 2.64 & 2.67 & 2.78 & 2.54 & - & - & - \\
\hline
\end{tabular}

\begin{tabular}{|c|ccccccc|}
\hline
~ & ~ & \multicolumn{5}{c}{Sequence lengths (batch size = 4)} & ~ \\
\#dims & 1k & 3k & 10k & 30k & 100k & 300k & 1M \\
\hline
1 & 16.6 & 45.1 & 130 & 316 & 809 & 1110 & 1530 \\
2 & 23.1 & 62.7 & 162 & 256 & 301 & 294 & 306 \\
4 & 24 & 63.1 & 131 & 174 & 171 & 199 & 207 \\
8 & 23.1 & 48.1 & 73.9 & 89.3 & 95.7 & 98.4 & 100 \\
16 & 18.2 & 28.6 & 32.7 & 35.1 & 37.4 & 39.1 & 38 \\
32 & 11.8 & 14.9 & 15.9 & 17 & 17.3 & 15.2 & - \\
64 & 5.14 & 5.7 & 5.62 & 5.22 & - & - & - \\
\hline
\end{tabular}

\begin{tabular}{|c|ccccccc|}
\hline
~ & ~ & \multicolumn{5}{c}{Sequence lengths (batch size = 2)} & ~ \\
\#dims & 1k & 3k & 10k & 30k & 100k & 300k & 1M \\
\hline
1 & 17.2 & 43.1 & 128 & 357 & 1030 & 1860 & 2660 \\
2 & 24.8 & 66.7 & 187 & 382 & 589 & 516 & 610 \\
4 & 25.2 & 64 & 172 & 280 & 372 & 416 & 433 \\
8 & 24.4 & 60.5 & 114 & 162 & 178 & 181 & 202 \\
16 & 22.5 & 45.5 & 62.8 & 68.5 & 75.3 & 78.1 & 77.7 \\
32 & 16.7 & 25.7 & 32 & 31.4 & 34.3 & 35.1 & - \\
64 & 9.18 & 10.4 & 11.1 & 10.4 & 10.7 & - & - \\
\hline
\end{tabular}



\end{center}
\end{table}

Table \ref{apptab:fwd-speedup} shows the speed up for batch size 16 and 8.
It can be seen that the smaller batch size can achieve higher speed up on average.

Another interesting finding that we found can be seen in Figure \ref{appfig:V100-vs-A100-speed-up}.
The figure shows the speed up comparison when using V100 GPU and when using A100 GPU.
Although for smaller number of dimensions A100 can achieve larger speed up than V100, the speed up when using 32 dimensions on A100 suddenly drops to below 1.
This effect is not as severe in V100.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{fwd-speedup.png}
    \includegraphics[width=\textwidth]{fwd-speedup-a100.png}
    \caption{Speed up of DEER GRU over sequential method achieved in (top) V100 and (bottom) A100.}
    \label{appfig:V100-vs-A100-speed-up}
\end{figure}

Furthermore, we performed profiling to see where the bottleneck is.
On the code in appendix \ref{appsubsec:code-in-jax}, we have denoted three lines of interest with FUNCEVAL, GTMULT, and INVLIN.
Table \ref{apptab:fwd-speedup} shows the run time using GRU with various number of dimensions with batch size of 16.
From the table, we can see that the bottleneck is in solving $L_\mathbf{G}^{-1}$.

\begin{table}
    \centering
    \caption{The run time of one iteration of a GRU cell in a V100 GPU in nanoseconds.
    Numbers that are less than 30 $\mathrm{\mu s}$ are not present in our profiling tool. \TODO{wait for the correct numbers from Jason.}}
    \label{apptab:profiling}
    \begin{tabular}{|c|rrrrrr|}
        \hline
        ~ & ~ & \multicolumn{4}{c}{Number of dimensions} & ~ \\
        Line label & 1 & 2 & 4 & 8 & 16 & 32 \\
        \hline
        FUNCEVAL & 31,430 & 432,184 & 911,824 & 1,194,010 & 3,419,370 & 5,249,474 \\
        GTMULT & ~ & 88,256  &  248,562  &  428,030  & 1,357,965 & 4,724,395 \\
        INVLIN & 147,669 & 1,336,836 & 2,015,883 & 4,460,318 & 9,786,933 & 19,248,959 \\
        \hline
    \end{tabular}
\end{table}

We also did benchmark the GPU memory consumption as a function of number of dimensions with batch size equals to 16 in the GRU case.
The results can be seen in Table \ref{apptab:gpu-memory-consumption}.
From the table, it can be seen that the GPU memory consumption approximately grows quadratically, as expected from storing the matrices $\mathbf{G}$ explicitly.

\begin{table}[]
    \caption{GPU memory consumption in MiB for evaluating GRU using DEER method with batch size = 16 and for various number of dimensions.\\}
    \label{apptab:gpu-memory-consumption}
    \centering
    \begin{tabular}{c|cccccc}
        \hline
        Number of dimensions & 1 & 2 & 4 & 8 & 16 & 32 \\
        \hline
        GPU memory (MiB) & 18.32 & 73.25 & 161.14 & 380.87 & 1351.68 & 5038.08 \\
        \hline
    \end{tabular}
\end{table}