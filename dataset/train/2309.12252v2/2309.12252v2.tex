\appendix

\section{Theoretical results}
\subsection{Relation to Newton's method}
\label{appsec:relation-to-newton}

The main method presented in this paper is basically a realization of Newton's method in Banach space.
Finding the root of functional  can be done by performing the Newton's method iteration  where  is the Fr\'{e}chet derivative of .
In equation \ref{eq:general-delayed-differential-equation}, the function  is given by .
Therefore, the Fr\'{e}chet derivative of functional  operated on a function  is given by

Therefore, the Newton's method iteration to find the root of functional  is

The inverse Fr\'{e}chet derivative terms can be computed by solving the equation below for ,

Expanding  from equation \ref{eq:frechet-derivative} on , we obtain

Solving for  on the equation above will produce the equation \ref{eq:yr-iterative equation}, showing that equation \ref{eq:yr-iterative equation} is just a realization of Newton's method in solving equation \ref{eq:general-delayed-differential-equation}.

\subsection{Relation to Direct Multiple Shooting}
\label{appsubsec:relation-to-direct-multiple-shooting}

One popular way to parallelize an ODE solver is to perform direct multiple shooting (MS) method \citep{chartier1993parallel, massaroli2021differentiablemultipleshooting}.
The following parts will follow \cite{massaroli2021differentiablemultipleshooting} very closely.
Consider an initial value problem with  for  with .
With MS, we first split the time horizon into  multiple regions with , have a guess on initial values at each region, , then apply the ODE solver for each region.
After applying the ODE, the guessed values  can then be updated to match the results of ODE from the previous region.
Denote a function  as the value of  at  after solving the initial value problem with initial value .
Evaluating this function  requires solving an ODE from  to .
With the function above, we can write the constraints to be solved,  with .

The works by \cite{chartier1993parallel} and \cite{massaroli2021differentiablemultipleshooting} update  by applying Newton method.
Specifically, at -th iteration with values of  are available for all -s, the values at the next iteration can be determined by \citep{massaroli2021differentiablemultipleshooting}

The term  makes the equation above needs to be evaluated sequentially, in addition to computing the function  that requires solving an ODE.

If we assume there are large number of regions,  and , then the MS method becomes equivalent to the DEER realization on solving the ODE.
In this limit, we can write

with  and  an identity matrix.
Substituting equation \ref{appeq:phi-approximations} to equation \ref{appeq:newton-update-multi-shooting}, we obtain

By re-arranging the equation above, we get

With the limit  and , the term  can be written as  and  equals to .
By rewriting the equation above in , we obtain

With a little bit of algebra, one can show that the equation above is equivalent to equations \ref{eq:yr-iterative equation} and \ref{eq:lginv-ode}.
By having a large number of regions, we can approximate  as in equation \ref{appeq:phi-approximations} that can be evaluated very quickly.

\subsection{Proof of quadratic convergence of DEER iteration}
\label{appsec:proof-of-quadratic-convergence}

This proof is inspired by standard results on the order of convergence of Newton's method and follows the argument of \cite{kelley1995iterative}, \S\S4 and 5.1. Throughout, we will make repeated use of the Cauchy-Schwarz (CS) and triangle (T) inequalities. 

We employ the standard Euclidean norm on ,

and the induced operator norm on ,

Note that the Fr\'{e}chet derivative .

We denote the value of  at the th iteration as , where  exactly satisfies equation \ref{eq:yr-iterative equation}. Following on from \ref{appsec:relation-to-newton}, we assume that  is Lipschitz continuous with Lipschitz constant , i.e.:

Consider the operators  and . We assume that both are invertible and that :


Post-multiplying by  and taking norms:

We denote the ball of radius  as , where . 

Let . Then : 

This means that

The fundamental theorem of calculus can be expressed as:

Therefore,

Let . Then:

So  is bounded by  with constant . For quadratic \textit{convergence}, the norms of successive errors must decrease. If we shrink  such that , then:

and the sequence therefore converges quadratically to . \hfill 



\subsection{Application to Partial Differential Equations (PDEs)}
\label{appsubsec:pde-example}

We have mentioned that equation \ref{eq:general-delayed-differential-equation} can be applied to Partial Differential Equations (PDEs).
As this paper is concerning about Ordinary Differential Equations (ODEs) and discrete difference equation, we only present in this appendix the example of equation \ref{eq:general-delayed-differential-equation} in representing a simple PDE.
Let's rewrite the equation \ref{eq:general-delayed-differential-equation} as a reminder,

where  is the linear operator,  is the non-linear function,  is the signal of interest,  is the external signal,  is the coordinate,  is the parameters of , and  is the shifted location.

We will take the Burgers' equation as an example.
Burgers' equation can be written as

where  is a function of  and , .
If we define , we can write the Burgers' equation to be

From the equation above, we can define the signal of interest , the coordinates , the non-linear function , and the linear operator


Given the relations between the Burgers' equation and equation \ref{eq:general-delayed-differential-equation}, we can analytically compute the  matrix and the argument of ,

Therefore, the linear operation that needs to be solved, , is

given  and  with  from the previous iteration.
The equations above can still be further simplified into

The equation above can be repeatedly evaluated until the convergence is achieved.

\subsection{Error bound on taking the midpoints in ODE}
\label{appsec:error-bound-midpoints-ode}

The exact analytical solution to the ODE

is 

where 

Equation \ref{eq:ode-solution} is the form that equation \ref{eq:yr-iterative equation} takes in the ODE case. The approximation we wish to take involves inserting  and  into \ref{eq:ode-solution}, which yields equation \ref{eq:ode-recursive-equation}. In order to find the local truncation error when applying this method, we need to find and compare the Taylor expansions of equations \ref{eq:ode-solution} and \ref{eq:ode-recursive-equation}. 

We define  so that, for instance,  and . We can then write the Taylor expansion of  as:

whence we immediately get 


as expected from the Leibniz integral rule. Therefore:

where for ease of notation we have introduced the new variables  and . As the ODE in section \ref{sec:nonlin-to-fpi} is originally cast in the form , the right hand side of \ref{eq:ode-appendix} is


which can be expressed using the Taylor expansions of , , and  as:

Then, 

where we have introduced  and  as the coefficients of  and  respectively. Then,

where again we have introduced new variables for the coefficients:  and . We define , and take  and .  This means that  and we can substitute  with . Therefore, treating equation \ref{eq:ode-solution} as a fixed-point iteration problem gives:

As , equation \ref{eq:ode-recursive-equation} is:

with , , and  for . 

The Taylor expansions of the midpoint approximations are:


Recall that  is the th iterated guess of the form of , given  and the previous guess , evaluated at . Meanwhile,  is the value of the approximation to  at , given  and the previous guess .
This gives the local truncation error as: 

Recalling from section \ref{sec:nonlin-to-fpi} that , we define  and expand the total derivatives:

The local truncation error can therefore also be written:
 

Accordingly, taking the midpoint values for the approximation enables us to obtain a cubic local truncation error, . 

\section{Experimental details}

\subsection{Code in JAX}
\label{appsubsec:code-in-jax}



\lstinputlisting[language=Python]{code.py}

\subsection{Hamiltonian neural network training with NeuralODE}
\label{appsec:hnn-training-details}

Let's denote  as the states of the system.
In the two-body case, the states are positions and velocities of the two masses, .
For typical works in learning physical systems, a neural network is designed to take the states  as the input and produces the states dynamics  as the output.
In those works, a set of states and the states dynamics are given as the training data, , so the neural network can just be trained without solving an ODE.

In our set up, the training data is only given as the states as a function of time .
To train the model with the given dataset, we need to roll out the states as a function of time  using NeuralODE.
In our case, the dataset is generated by generating the initial condition randomly and the dynamics are governed by the gravitational force.
The initial conditions are chosen so the orbits of the two-body system does not diverge and the orbits are still close to a circle, to make the simulation numerically stable.
The states are rolled out from  to  with 10,000 sampled time.
Within the time period, the system makes about 2-4 rounds of orbit.
There are 1000 rows of dataset generated and split into 800 for the training, 100 for the validation, and 100 for the test.

\begin{figure}[t!]
\centering
\begin{tikzpicture}[block/.style={rectangle, draw, fill=blue!20, text width=6em, text centered, rounded corners, minimum height=4em}, addnormblock/.style={rectangle, draw, fill=blue!20, text width=6em, text centered, rounded corners, minimum height=2em}]

\node [block] (encoder) {Encoder};
\node [block, above of=encoder, node distance=2.5cm] (deer) {GRU};
\node [addnormblock, above of=deer, node distance=1.35cm] (addnorm1) {Add \& Norm};
\node [block, above of=addnorm1, node distance=2.5cm] (mlp) {MLP};
\node [addnormblock, above of=mlp, node distance=1.35cm] (addnorm2) {Add \& Norm};
\node [block, above of=addnorm2, node distance=2.5cm] (decoder) {Decoder};
\node [block, above of=decoder, node distance=2.5cm] (mean) {Sequence Mean};



\draw[->] ++(0,-1.5cm) node[below] {Input} -- (encoder.south);
\draw[->] (encoder) -- (deer);
\draw[-] (deer) -- (addnorm1);
\draw[->] (addnorm1) -- (mlp);
\draw[-] (mlp) -- (addnorm2);
\draw[->] (addnorm2) -- (decoder);
\draw[->] (decoder) -- (mean);
\draw[->] (mean.north) -- ++(0,.75cm) node[above] {output};

\draw[->, rounded corners] () -- ++(1.75cm,0) |- (addnorm1.east);
\draw[->, rounded corners] () -- ++(1.75cm,0) |- (addnorm2.east);


\node[draw, thick, dashed, fit=(deer) (addnorm2), inner xsep=30pt, inner ysep=20pt, 
label={[anchor=north west,inner sep=5pt,font=\tiny]north west:{5}}] (box) {};

\end{tikzpicture}
\caption{Architecture for the EigenWorms experiments.}
\label{appfig:eigenworms}
\end{figure}



For the model, we are using Hamiltonian neural network \citep{greydanus2019hamiltonian} that consists of 6 linear layers with softplus activation except on the last linear layer.
The hidden layers have 64 elements each.
The input to the network has 8 elements (corresponding to the states ) and the output of the network has only 1 element that corresponds to the Hamiltonian value.

For every training step during the training with DEER method, we save the predicted trajectory for every row of the dataset.
The saved trajectory will be used as the initial guess of the DEER method for the next training step.
The training was performed using ADAM optimizer with  learning rate.
To speed up the training, we start from 20 time points at the beginning of the training and increase the number of time points by 20 every 1 epoch (50 training steps) until it reaches the maximum 10k time points.
The loss function for the training is a simple mean squared error.

\subsection{Time series classification with GRU}
\label{appsec:gru-training-details}

The dataset used in this case is Eigenworms \citep{brown2013dictionaryeigenworms}. There are 259 worm samples with very long sequences (length ) in this dataset, which are then divided into train, validation and test sets using a 70\%, 15\%, 15\% split following \cite{morrill2021neuralrough}. Each sample is classified as either wild-type or one of four mutant types based on the representation using six base shapes.

The model used is shown in Figure \ref{appfig:eigenworms}. It consists of four main components: encoder, GRU, MLP and decoder. The encoder would project the input features to a higher dimension of 32, and the encoded input would then go through 5 layers of GRU and MLP pair, in that order. The number of hidden states for each GRU and MLP is set to 32. Then, the output of the GRU and MLP pair would go through a decoder to project it down to 5 classes. We then take the mean over the sequence length to obtain the final output. Residual connection followed by LayerNorm is applied to each GRU and MLP sublayer. Both the encoder and decoder are also simple MLPs, and all MLPs in the architecture have a depth of 1 with ReLU activation function in the hidden layer.

The training is done using cross entropy loss with the patience for early stopping set to 1000 epochs per validation accuracy. The optimization algorithm we used is ADAM optimizer with  learning rate and the gradient is clipped at 1.0 per global norm. The initial guesses for the DEER method are always initialized at zeros.














\section{Additional results}

\subsection{Benchmarks}
\label{appsec:benchmarks}

\begin{table}
\caption{
    The speed up of GRU calculated using DEER method (this paper) vs commonly-used sequential method on a V100 GPU with batch size of 16, 8, 4, and 2.
    The missing data for large number of dimensions and sequence lengths are due to insufficient memory in computing the DEER method.
    The numbers in the table represent the mean speed up over 5 different random seeds.}
\label{apptab:fwd-speedup}
\begin{center}
\begin{tabular}{|c|ccccccc|}
\hline
~ & ~ & \multicolumn{5}{c}{Sequence lengths (batch size = 16)} & ~ \\
\#dims & 1k & 3k & 10k & 30k & 100k & 300k & 1M \\ \hline
1 & 15.7 & 43.5 & 114 & 182 & 309 & 394 & 516 \\ 2 & 24.4 & 47.6 & 75.0 & 78.4 & 72.4 & 77.1 & 74.6 \\ 4 & 22.1 & 35.4 & 46.5 & 44.4 & 52.1 & 54.2 & 51.3 \\ 8 & 14.8 & 20.0 & 21.9 & 24.0 & 25.2 & 25.1 & \text{-} \\ 16 & 7.11 & 8.37 & 8.89 & 9.23 & 9.46 & \text{-} & \text{-} \\ 32 & 3.74 & 3.76 & 4.16 & 4.24 & \text{-} & \text{-} & \text{-} \\ 64 & 1.29 & 1.26 & 1.27 & \text{-} & \text{-} & \text{-} & \text{-} \\ \hline
\end{tabular}

\begin{tabular}{|c|ccccccc|}
\hline
~ & ~ & \multicolumn{5}{c}{Sequence lengths (batch size = 8)} & ~ \\
\#dims & 1k & 3k & 10k & 30k & 100k & 300k & 1M \\
\hline
1 & 16.9 & 44.9 & 131 & 264 & 521 & 604 & 945 \\
2 & 22.5 & 58.0 & 113 & 149 & 119 & 141 & 138 \\
4 & 24.1 & 50.7 & 79.0 & 95.9 & 101 & 103 & 104 \\
8 & 21.4 & 31.9 & 42.3 & 45.1 & 47.9 & 49.6 & 49.6 \\
16 & 11.7 & 15.3 & 16.8 & 18.5 & 18.6 & 18.7 & - \\
32 & 6.32 & 7.72 & 8.02 & 8.32 & 8.30 & - & - \\
64 & 2.64 & 2.67 & 2.78 & 2.54 & - & - & - \\
\hline
\end{tabular}

\begin{tabular}{|c|ccccccc|}
\hline
~ & ~ & \multicolumn{5}{c}{Sequence lengths (batch size = 4)} & ~ \\
\#dims & 1k & 3k & 10k & 30k & 100k & 300k & 1M \\
\hline
1 & 16.6 & 45.1 & 130 & 316 & 809 & 1110 & 1530 \\
2 & 23.1 & 62.7 & 162 & 256 & 301 & 294 & 306 \\
4 & 24 & 63.1 & 131 & 174 & 171 & 199 & 207 \\
8 & 23.1 & 48.1 & 73.9 & 89.3 & 95.7 & 98.4 & 100 \\
16 & 18.2 & 28.6 & 32.7 & 35.1 & 37.4 & 39.1 & 38 \\
32 & 11.8 & 14.9 & 15.9 & 17 & 17.3 & 15.2 & - \\
64 & 5.14 & 5.7 & 5.62 & 5.22 & - & - & - \\
\hline
\end{tabular}

\begin{tabular}{|c|ccccccc|}
\hline
~ & ~ & \multicolumn{5}{c}{Sequence lengths (batch size = 2)} & ~ \\
\#dims & 1k & 3k & 10k & 30k & 100k & 300k & 1M \\
\hline
1 & 17.2 & 43.1 & 128 & 357 & 1030 & 1860 & 2660 \\
2 & 24.8 & 66.7 & 187 & 382 & 589 & 516 & 610 \\
4 & 25.2 & 64 & 172 & 280 & 372 & 416 & 433 \\
8 & 24.4 & 60.5 & 114 & 162 & 178 & 181 & 202 \\
16 & 22.5 & 45.5 & 62.8 & 68.5 & 75.3 & 78.1 & 77.7 \\
32 & 16.7 & 25.7 & 32 & 31.4 & 34.3 & 35.1 & - \\
64 & 9.18 & 10.4 & 11.1 & 10.4 & 10.7 & - & - \\
\hline
\end{tabular}



\end{center}
\end{table}

Table \ref{apptab:fwd-speedup} shows the speed up for batch size 16 and 8.
It can be seen that the smaller batch size can achieve higher speed up on average.

Another interesting finding that we found can be seen in Figure \ref{appfig:V100-vs-A100-speed-up}.
The figure shows the speed up comparison when using V100 GPU and when using A100 GPU.
Although for smaller number of dimensions A100 can achieve larger speed up than V100, the speed up when using 32 dimensions on A100 suddenly drops to below 1.
This effect is not as severe in V100.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{fwd-speedup.png}
    \includegraphics[width=\textwidth]{fwd-speedup-a100.png}
    \caption{Speed up of DEER GRU over sequential method achieved in (top) V100 and (bottom) A100.}
    \label{appfig:V100-vs-A100-speed-up}
\end{figure}

Furthermore, we performed profiling to see where the bottleneck is.
On the code in appendix \ref{appsubsec:code-in-jax}, we have denoted three lines of interest with FUNCEVAL, GTMULT, and INVLIN.
Table \ref{apptab:fwd-speedup} shows the run time using GRU with various number of dimensions with batch size of 16.
From the table, we can see that the bottleneck is in solving .

\begin{table}
    \centering
    \caption{The run time of one iteration of a GRU cell in a V100 GPU in nanoseconds.
    Numbers that are less than 30  are not present in our profiling tool. \TODO{wait for the correct numbers from Jason.}}
    \label{apptab:profiling}
    \begin{tabular}{|c|rrrrrr|}
        \hline
        ~ & ~ & \multicolumn{4}{c}{Number of dimensions} & ~ \\
        Line label & 1 & 2 & 4 & 8 & 16 & 32 \\
        \hline
        FUNCEVAL & 31,430 & 432,184 & 911,824 & 1,194,010 & 3,419,370 & 5,249,474 \\
        GTMULT & ~ & 88,256  &  248,562  &  428,030  & 1,357,965 & 4,724,395 \\
        INVLIN & 147,669 & 1,336,836 & 2,015,883 & 4,460,318 & 9,786,933 & 19,248,959 \\
        \hline
    \end{tabular}
\end{table}

We also did benchmark the GPU memory consumption as a function of number of dimensions with batch size equals to 16 in the GRU case.
The results can be seen in Table \ref{apptab:gpu-memory-consumption}.
From the table, it can be seen that the GPU memory consumption approximately grows quadratically, as expected from storing the matrices  explicitly.

\begin{table}[]
    \caption{GPU memory consumption in MiB for evaluating GRU using DEER method with batch size = 16 and for various number of dimensions.\\}
    \label{apptab:gpu-memory-consumption}
    \centering
    \begin{tabular}{c|cccccc}
        \hline
        Number of dimensions & 1 & 2 & 4 & 8 & 16 & 32 \\
        \hline
        GPU memory (MiB) & 18.32 & 73.25 & 161.14 & 380.87 & 1351.68 & 5038.08 \\
        \hline
    \end{tabular}
\end{table}