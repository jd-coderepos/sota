\section{Experiments} \label{sec:experiments}
In this section, we present two application scenarios of the pseudo-antichain based symblicit algorithm of the previous section, one for the SSP problem and the other for the EMP problem. In both cases, we first show the reduction to monotonic MDPs that satisfy Assumptions~\ref{atwo} and~\ref{aone}, and we then present some experimental results. All our experiments have been done on a Linux platform with a GHz CPU (Intel Core i7) and GB of memory. Note that our implementations are single-threaded and thus use only one core. For all those experiments, the timeout is set to  hours and is denoted by \timeout. Finally, we restrict the memory usage to GB\footnote{Restricting the memory usage to GB is enough to have a good picture of the behavior of each implementation with respect to the memory consumption.} and when an execution runs out of memory, we denote it by \memout.

\subsection{Stochastic shortest path on STRIPSs}
We consider the following application of the pseudo-antichain based symblicit algorithm for the SSP problem. In the field of planning, a class of problems called \textit{planning from STRIPSs}~\cite{fikes1972strips} operate with states represented by valuations of propositional variables. Informally, a STRIPS is defined by an \textit{initial} state representing the initial configuration of the system and a set of \textit{operators} that transform a state into another state. The problem of planning from STRIPSs then asks, given a valuation of propositional variables representing a set of \textit{goal} states, to find a sequence of operators that lead from the initial state to a goal one. Let us first formally define the notion of STRIPS and show that each STRIPS can be made monotonic. We will then add stochastic aspects and show how to construct a monotonic MDP from a monotonic stochastic STRIPS.

\paragraph{STRIPSs.} A \textit{STRIPS}~\cite{fikes1972strips} is a tuple  where  is a finite set of \textit{conditions} (i.e. propositional variables),  is a subset of conditions that are initially true (all others are assumed to be false), , with  and , specifies which conditions are true and false, respectively, in order for a state to be considered a goal state, and  is a finite set of \textit{operators}. An operator  is a pair  such that  is the \textit{guard} of , that is,  (resp. ) is the set of conditions that must be true (resp. false) for  to be executable, and  is the \textit{effect} of , that is,  (resp. ) is the set of conditions that are made true (resp. false) by the execution of . For all , we have that  and .

\noindent From a STRIPS, we derive a transition system as follows. The set of states is , that is, a state is represented by the set of conditions that are true in it. The initial state is . The set of goal states are states  such that  and . There is a transition from state  to state  under operator  if ,  (the guard is satisfied) and  (the effect is applied). A standard problem is to ask whether or not there exists a path from the initial state to a goal state.

\paragraph{Monotonic STRIPSs.} 
A \textit{monotonic STRIPS (MS)} is a tuple  where  and  are defined as for STRIPSs,  specifies which conditions must be true in a goal state, and  is a finite set of operators. In the MS definition, an operator  is a pair  where  is the guard of , that is, the set of conditions that must be true for  to be executable, and  is the effect of  as in the STRIPS definition. 
MSs thus differ from STRIPS in the sense that guards only apply on conditions that are true in states, and goal states are only specified by true conditions. The monotonicity will appear more clearly when we will derive hereafter monotonic MDPs from MSs.

\noindent
Each STRIPS  can be made monotonic by duplicating the set of conditions, in the following way. We denote by  the set  containing a new condition  for each  such that  represents the negation of the propositional variable . We construct from  an MS  such that , ,  and . It is easy to check that  and  are equivalent (a state  in  has its counterpart  in ). In the following, we thus only consider MSs. 

\begin{example} \label{ex:ST}
To illustrate the notion of MS, let us consider the following example of the monkey trying to reach a bunch of bananas (cf. Example~\ref{ex:monkey}). Let  be an MS such that  \{\textit{box}, \textit{stick}, \textit{bananas}, , , and  where ,   and . In this MS, a condition  is true when the monkey possesses the item corresponding to . At the beginning, the monkey possesses no item, i.e.  is the empty set, and its goal is to get the \textit{bananas}, i.e. to reach a state . This can be done by first executing the operators \textit{takebox} and \textit{takestick} to respectively get the \textit{box} and the \textit{stick}, and then executing \textit{takebananas}, whose guard is .
\end{example}


\paragraph{Monotonic stochastic STRIPSs.} MSs can be extended with stochastic aspects as follows~\cite{blum2000probabilistic}. Each operator  now consists of a guard  as before, and an effect given as a probability distribution  on the set of pairs . An MS extended with such stochastic aspects is called a \textit{monotonic stochastic STRIPS (MSS)}. 

\noindent
Additionally, we associate with an MSS  a cost function  that associates a strictly positive cost with each operator. The problem of planning from MSSs is then to minimize the expected truncated sum up to the set of goal states from the initial state, i.e. this is a version of the SSP problem.

\begin{example}\label{ex:mst}
We extend the MS of Example~\ref{ex:ST} with stochastic aspects to illustrate the notion of MSS. Let  be an MSS such that ,  and  are defined as in Example~\ref{ex:ST}, and  \{\textit{takebox}, \textit{takestick}, \textit{takebananaswithbox}, \textit{takebananaswithstick}, \textit{takebananaswithboth}\} where 
\begin{itemize}
\item ,
\item ,
\item ,
\item , and 
\item .
\end{itemize}
In this MSS, the monkey has a strictly positive probability to fail reaching the \textit{bananas}, whatever the items it uses. However, the probability of success increases when it has both the \textit{box} and the \textit{stick}.
\end{example}

In the following, we show that MSSs naturally define monotonic MDPs on which the pseudo-antichain based symblicit algorithm of Section~\ref{sec:paalgo} can be applied.

\paragraph{From MSSs to monotonic MDPs.} Let  be an MSS. We can derive from  an MDP  together with a set of goal states  and a cost function  such that:
\begin{itemize}
\itemsep0.1em
\item ,
\item ,
\item , and for all , ,
\item ,
\item ,  and  are defined for all  and , such that:
\begin{itemize} 
\item for all , ,
\item for all , , and
\item .
\end{itemize}
\end{itemize}
Note that we might have that  is not -non-blocking, if no operator can be applied on some state of . In this case, we get a -non-blocking MDP from  by eliminating states  with   as long as it is necessary.

\begin{lemma} \label{lem:StripsMono}
The MDP  is monotonic,  is closed, and functions  are independent from .
\end{lemma}
\begin{proof}
First,  is equipped with the partial order  and  is a semilattice. Second,  is closed for  by definition. 
Thirdly,  we have that  is compatible with . Indeed, for all  such that , for all  and , . Finally the set  of goal states is closed for , and  are clearly independent from .
\qed\end{proof}

\paragraph{Symblicit algorithm.} In order to apply the pseudo-antichain based symblicit algorithm of Section~\ref{sec:paalgo} on the monotonic MDPs derived from MSSs, Assumptions~\ref{atwo} and~\ref{aone} must hold. Let us show that Assumption~\ref{aone} is satisfied. For all ,  and , we clearly have an algorithm for computing , and . Let us now consider Assumption~\ref{atwo}. An algorithm for computing , for all ,  and , is given by the next proposition. 
\begin{proposition}
Let ,  and . If , then  , otherwise .
\end{proposition}
\begin{proof}
Suppose first that .

We first prove that .  We have to show that  and  . Recall that . We have that , showing that . We have that  since . We thus have that .

We then prove that for all , , i.e. . Let . We have that  and , that is,   and . By classical set properties, it follows that , and then . Finally, since , we have , as required.

Suppose now that , then . Indeed for all , we have  , and by definition of , there is no  such that .
\qed\end{proof}

Finally, notice that for the class of monotonic MDPs derived from MSSs, the symbolic representations described in Section~\ref{subsec:symbrep} are compact, since  is closed and   are independent from  (see Lemma~\ref{lem:StripsMono}).
Therefore we have all the required ingredients for an efficient pseudo-antichain based algorithm to solve the SSP problem for MSSs. The next experiments show its performance.

\paragraph{Experiments.} We have implemented  in Python and C the pseudo-antichain based symblicit algorithm for the SSP problem. The C language is used for all the low level operations while the orchestration is done with Python. The binding between C and Python is realized with the ctypes library of Python. The source code is publicly available at \url{http://lit2.ulb.ac.be/STRIPSSolver/}, together with the two benchmarks presented in this section. We compared our implementation with the purely explicit strategy iteration algorithm implemented in the development release 4.1.dev.r7712 of the tool ~\cite{KNP11}, since to the best of our knowledge, there is no tool implementing an MTBDD based symblicit algorithm for the SSP problem.\footnote{A comparison with an MTBDD based symblicit algorithm is done in the second application for the EMP problem.} Note that this explicit implementation exists primarily to prototype new techniques and is thus not fully optimized~\cite{Parker}. Note that value iteration algorithms are also implemented in . 
While those algorithms are usually efficient, they only compute approximations. As a consequence, for the sake of a fair comparison, we consider here only the performances of strategy iteration algorithms. 

\medskip
The first benchmark () is obtained from Example~\ref{ex:mst}. In this benchmark, the monkey has several items at its disposal to reach the bunch of bananas, one of them being a stick. However, the stick is available as a set of several pieces that the monkey has to assemble. Moreover, the monkey has multiple ways to build the stick as there are several sets of pieces that can be put together. However, the time required to build a stick varies from a set of pieces to another. Additionally, we add useless items in the room: there is always a set of pieces from which the probability of getting a stick is . The operators of getting some items are stochastic, as well as the operator of getting the bananas: the probability of success varies according to the owned items (cf. Example~\ref{ex:mst}). The benchmark is parameterized in the number  of pieces required to build a stick, and in the number  of sticks that can be built. Note that the monkey can only use one stick, and thus has no interest to build a second stick if it already has one. Results are given in Table~\ref{table:STRIPS1}. 

\begin{table}[h!]
	\caption{Stochastic shortest path on the  benchmark. The column  gives the parameters of the problem,  the expected truncated sum of the computed strategy , and  the number of states of the MDP.
For the pseudo-antichain based implementation (), it is the number of iterations of the strategy iteration algorithm,  the maximum size of computed bisimulation quotients, \textit{lump} the total time spent for lumping, \textit{syst} the total time spent for solving the linear systems, and \textit{impr} the total time spent for improving the strategies. For the explicit implementation (), \textit{constr} is the time spent for model construction and \textit{strat} the time spent for the strategy iteration algorithm. For both implementations, \textit{total} is the total execution time and \textit{mem} the total memory consumption. All times are given in seconds and all memory consumptions are given in megabytes.}
	\label{table:STRIPS1}
	\centering
		\scriptsize
 		\begin{tabular}{|r|r|r||r|r|r|r|r|r|r||r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
		\hline
	  	& & & \multicolumn{7}{|c||}{{\small }} & \multicolumn{4}{|c|}{{\small }}\rule[-2pt]{0pt}{10pt}\\
		 &   &   & \ it \  &  & \ lump  \ &  \ syst \  &  \ impr  \ & \  total \  & mem & \ constr \  & \ strat \ & \ total \ & \ mem\ \rule[-3pt]{0pt}{10pt}\\
\hline\hline
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
\hline  &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
\hline  &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
\hline  &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
\hline  &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
\hline
		\end{tabular}
		\normalsize
\end{table}

The second benchmark () is an adaptation of a benchmark of~\cite{DBLP:conf/aips/MajercikL98} as proposed in~\cite{blum2000probabilistic}\footnote{In~\cite{blum2000probabilistic}, the authors study a different problem that is to maximize the probability of reaching the goal within a given number of steps.}. The goal is to build a sand castle on the beach; a moat can be dug before in a way to protect it. We consider up to  discrete depths of moat. The operator of building the castle is stochastic: there is a strictly positive probability for the castle to be demolished by the waves. However, the deeper the moat is, the higher the probability of success is. For example, the first depth of moat offers a probability  of success, while with the second depth of moat, the castle has probability  to resist to the waves. The optimal strategy for this problem is to dig up to a given depth of moat and then repeat the action of building the castle until it succeeds. The optimal depth of moat then depends on the cost of the operators and the respective probability of successfully building the castle for each depth of moat. To increase the difficulty of the problem, we consider building several castles, each one having its own moat. The benchmark is parameterized in the number  of depths of moat that can be dug, and the number  of castles that have to be built. Results are given in Table~\ref{table:STRIPS2}.

\begin{table}[h!]
	\caption{Stochastic shortest path on the  benchmark. The column  gives the parameters of the problem and all other columns have the same meaning as in Table~\ref{table:STRIPS1}.}
	\label{table:STRIPS2}
	\centering
	\scriptsize
 		\begin{tabular}{|r|r|r||r|r|r|r|r|r|r||r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
		\hline
	  	& & & \multicolumn{7}{|c||}{{\small }} & \multicolumn{4}{|c|}{{\small }}\rule[-2pt]{0pt}{10pt}\\
		 &   &   & \ it \  &  & \ lump  \ &  \ syst \  &  \ impr  \ & \  total \  & mem & \ constr \  & \ strat \ & \ total \ & \ mem\ \rule[-3pt]{0pt}{10pt}\\
\hline\hline
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
\hline  &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
\hline  &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &  &   &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
\hline
		\end{tabular}
	\normalsize
\end{table}

On those two benchmarks, we observe that the explicit implementation quickly runs out of memory when the state space of the MDP grows. Indeed, with this method, we were not able to solve MDPs with more than  (resp. ) states in Table~\ref{table:STRIPS1} (resp. Table~\ref{table:STRIPS2}). On the other hand, the symblicit algorithm behaves well on large models: the memory consumption never exceeds Mo and this even for MDPs with hundreds of millions of states. For instance, the example  of the  benchmark is an MDP of more than  billions of states that is solved in less than  hours with only Mo of memory\footnote{On our benchmarks, the value iteration algorithm of ~performs better than the strategy iteration one w.r.t. the run time and memory consumption. However, it still consumes more memory than the pseudo-antichain based algorithm, and runs out of memory on several examples.}.


\subsection{Expected mean-payoff with \LTLMP synthesis}
We consider another application of the pseudo-antichain based symblicit algorithm, but now for the EMP problem. This application is related to the problems of \LTLMP realizability and synthesis~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13}. Let us fix some notations and definitions. Let  be an \LTL formula defined over the set  of signals and let ,  and . Let  be the set of literals over . Let  be a weight function where positive numbers represent rewards.\footnote{Note that in~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13}, the weight function  is more general since it also associates values to . However, for this application, we restrict  to .} This function is extended to  as follows:   for all .

\paragraph{ realizability and synthesis.} The problem of \LTLMP realizability is best seen as a game between two players, Player  and Player . This game is infinite and such that at each turn , Player  gives a subset  and Player  responds by giving a subset . The outcome of the game is the infinite word . A strategy for Player  is a mapping , while a strategy for Player  is a mapping . The outcome of the strategies  and  is the word  such that  and for all  and . A \textit{value}  is associated with each outcome  such that
\begin{center}


\end{center}
i.e.  is the mean-payoff value of  if  satisfies , otherwise, it is . Given an  formula  over , a weight function  and a threshold value , the  \textit{realizability problem} asks to decide whether there exists a strategy  for Player  such that  for all strategies  of Player~. If the answer is ,  is said -realizable. The  \textit{synthesis problem} is then to produce such a strategy  for Player . 

To illustrate the problems of  realizability and synthesis, let us consider the following specification of a server that should grant exclusive access to a resource to two clients.

\begin{example} \label{ex:ltl} 
A client requests access to the resource by setting to true its request signal ( for client~ and  for client~), and the server grants those requests by setting to true the respective grant signal  or . We want to synthesize a server that eventually grants any client request, and that only grants one request at a time. Additionally, we ask client~'s requests to take the priority over client 1's requests.  Moreover, we would like to keep minimal the delay between requests and grants. This can be formalized by the \LTL formula  given below where the signals in  are controlled by the two clients, and the signals in  are controlled by the server. Moreover we add the next weight function :

\begin{minipage}{0.5\linewidth}

\end{minipage}
\begin{minipage}{0.4\linewidth}
\begin{center}


\end{center}
\end{minipage}

\medskip\noindent
A possible strategy for the server is to behave as follows: it grants immediately any request of client~ if the last ungranted request of client~ has been emitted less than  steps in the past, otherwise it grants the request of client~. The mean-payoff value of this solution in the worst-case (when the two clients always emit their respective request) is equal to .
\end{example}

\paragraph{Reduction to safety games.} In~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13}, we propose an antichain based algorithm for solving the  realizability and synthesis problems with a reduction to a two-player turn-based safety game. We here present this game  without explaining the underlying reasoning, see~\cite{DBLP:journals/corr/abs-1210-3539} for more details. The game  is a turn-based safety game such that  (resp. ) is the set of positions of Player  (resp. Player ),  is the set of edges labeled by  (resp. ) when leaving a position in  (resp. ), and  is the set of bad positions (i.e. positions that Player  must avoid to reach). Let  be the set of positions in  from which Player  can force Player  to stay in , that is the set of winning positions for Player . The safety game  restricted to positions  is a representation of a subset of the set of all winning strategies  for Player  that ensure a value  greater than or equal to the given threshold , for all strategies  of Player . Those strategies are called \textit{worst-case winning strategies}. 

Note that the reduction to safety games given in~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13} allows to compute the set of all worst-case winning strategies (instead of a subset of them). 
Indeed the proposed algorithm is incremental on two parameters  and , and works as follows. For each value of  and , a corresponding safety game is constructed, whose number of states depends on  and . Those safety games have the following nice property. If player  has a worst-case winning strategy in the safety game for  and , then he has a worst-case winning strategy in all the safety games for  and . The algorithm thus stops as soon as a worst-case winning strategy is found. There exist theoretical bounds  and  such that the set of all worst-case winning strategies can be represented by the safety game with parameters  and . However,  and  being huge, constructing this game is unfeasible in practice. 


\paragraph{From safety games to MDPs.} We can go beyond  synthesis. Let  be a safety game as above, that represents a subset of worst-case winning strategies. 
For each state , we denote by  the set of actions that are safe to play in  (i.e. actions that force Player  to stay in ). For all , we know that  by construction of .
From this set of worst-case winning strategies, we want to compute the one that behaves \textit{the best against a stochastic opponent}. Let  be a probability distribution on the actions of Player . Note that we require  so that it makes sense with the worst-case. 
By replacing Player  by  in the safety game  restricted to , we derive an MDP  where:
\begin{itemize}
\itemsep0.1em
\item ,
\item , and for all , ,
\item ,
\item ,  and  are defined for all  and , such that:
\begin{itemize}
\item for all ,  such that ,
\item for all , , and
\item .
\end{itemize}
\end{itemize}
Note that since  for all , we have that  is -non-blocking. 

Computing the best strategy against a stochastic opponent among the worst-case winning strategies represented by  reduces to solving the EMP problem for the MDP \footnote{More precisely, it reduces to the EMP problem where the objective is to maximize the expected mean-payoff (see footnotes~\ref{fn:altobj} and~\ref{fn:MPmax}).}.

\begin{lemma}  \label{lem:LTLMono}
The MDP  is monotonic, and functions  are independent from . 
\end{lemma}

\begin{proof}
It is shown in~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13} that the safety game  has properties of monotony. The set  is equipped with a partial order  such that  is a complete lattice, and the sets  and  are closed for . For the MDP  derived from , we thus have that  is a (semi)lattice, and  is closed for . Moreover, by construction of  (see details in~\cite[Sec. 5.1]{DBLP:journals/corr/abs-1210-3539}) and , 
we have that  is compatible with~. By construction,  are independent from .
\qed\end{proof}


\paragraph{Symblicit algorithm.} 
In order to apply the pseudo-antichain based symblicit algorithm of Section~\ref{sec:paalgo}, Assumptions~\ref{atwo} and~\ref{aone} must hold for . This is the case for Assumption~\ref{aone} since  can be computed for all  and  (see~\cite[Sec. 5.1]{DBLP:journals/corr/abs-1210-3539}), and  is given by . Moreover, from \cite[Prop. 24]{DBLP:journals/corr/abs-1210-3539} and , we have an algorithm for computing , for all . So, Assumption~\ref{atwo} holds too. 
Notice also that for the MDP  derived from the safety game , the symbolic representations described in Section~\ref{subsec:symbrep} are compact, since  and  are independent from  (see Lemma~\ref{lem:LTLMono}).

Therefore for this second class of MDPs, we have again an efficient pseudo-antichain based algorithm to solve the EMP problem, as indicated by the next experiments.

\paragraph{Experiments.} We have implemented the pseudo-antichain based symblicit algorithm for the EMP problem and integrated it into ~()~\cite{DBLP:conf/cav/BohyBFJR12}.  is a tool written in Python and C that provides an antichain based version of the algorithm described above for solving the  realizability and synthesis problems. The last version of  is available at \url{http://lit2.ulb.ac.be/acaciaplus/}, together with all the examples considered in this section. It can also be used directly online via a web interface. We compared our implementation with an MTBDD based symblicit algorithm implemented in ~\cite{prismEMP}. To the best of our knowledge, only strategy iteration algorithms are implemented for the EMP problem. In the sequel, for the sake of simplicity, we refer to the MTBDD based implementation as  and to the pseudo-antichain based one as . Notice that for , the given execution times and memory consumptions only correspond to the part of the execution concerning the symblicit algorithm (and not the construction of the safety game  and the subset of worst-case winning strategies that it represents).

\medskip
We compare the two implementations on a benchmark of~\cite{DBLP:conf/tacas/BohyBFR13} obtained from the  specification of Example~\ref{ex:ltl} extended with stochastic aspects (). For the stochastic opponent, we set a probability distribution such that requests of client~ are more likely to happen than requests of client~: at each turn, client~ has probability  to make a request, while client~ has probability . The probability distribution  is then defined as , ,  and . We use the backward algorithm of  for solving the related safety games. The benchmark is parameterized in the threshold value . Results are given in Table~\ref{table:LTL}. Note that the number of states in the MDPs depends on the implementation. Indeed, for , it is the number of reachable states of the MDP, denoted , that is, the states that are really taken into account by the algorithm, while for , it is the total number of states since unlike , our implementation does not prune unreachable states. For this application scenario, we observe that the ratio (number of reachable states)/(total number of states) is in general quite small\footnote{For all the MDPs considered in Tables~\ref{table:STRIPS1} and \ref{table:STRIPS2}, this ratio is .}. 

\begin{table}[h!]
	\caption{Expected mean-payoff on the  benchmark with  clients and decreasing threshold values. The column  gives the threshold,  the number of reachable states in the MDP, and all other columns have the same meaning as in Table~\ref{table:STRIPS1}. The expected mean-payoff  of the optimal strategy  for all the examples is .}	\label{table:LTL}
	\centering
	\scriptsize
 	\begin{tabular}{|r||r|r|r|r|r|r|r|r||r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
		\hline
	  	& \multicolumn{8}{|c||}{{\small }} & \multicolumn{5}{|c|}{{\small }}\rule[-2pt]{0pt}{10pt}\\
		 &   & \ it \  &  & \ lump  \ &  \ LS \  &  \ impr  \ & \  total \  & \ mem \ &  & \ constr \  & \ strat \  & \ total \ & \ mem\ \rule[-3pt]{0pt}{10pt}\\
\hline  &  &  &  &  &  &   &  &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &   &  &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &   &  &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &   &  &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &   &  &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &   &  &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &   &  &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &   &  &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
 &  &  &  &  &  &   &  &  &  &  &   &  & \rule[-3pt]{0pt}{10pt}\\
\hline
	\end{tabular}
	\normalsize
\end{table}

On this benchmark,  is faster that  on large models, but  is more efficient regarding the memory consumption and this in spite of considering the whole state space. For instance, the last MDP of Table~\ref{table:LTL} contains more than  millions of states and is solved by  in around  hours with less than Mo of memory, while for this example,  runs out of memory. Note that the surprisingly large amount of memory consumption of both implementations on small instances is due to Python libraries loaded in memory for , and to the JVM and the CUDD package for ~\cite{DBLP:conf/hvc/JansenKOSZ07}.

To fairly compare the two implementations, let us consider Figure~\ref{fig:time} (resp. Figure~\ref{fig:memory}) that gives a graphical representation of the execution times (resp.  the memory consumption) of  and  as a function of the number of states taken into account, that is, the total number of states for  and the number of reachable states for . For that experiment, we consider the benchmark of examples of Table~\ref{table:LTL} with four different probability distributions on . Moreover, for each instance, we consider the two MDPs obtained with the backward and the forward algorithms of  for solving safety games. The forward algorithm always leads to smaller MDPs. On the whole benchmark,  times out on three instances, while  runs out of memory on four of them. Note that all scales in Figures~\ref{fig:time} and~\ref{fig:memory} are logarithmic.


\begin{figure}[!h]
   \begin{minipage}[t]{.48\linewidth}
	\includegraphics[width=\textwidth]{img/all_time.pdf}
	\caption{Execution time}
	\label{fig:time}   
\end{minipage} \hfill
   \begin{minipage}[t]{.48\linewidth}
	\includegraphics[width=\textwidth]{img/all_mem.pdf}
	\caption{Memory consumption}
	\label{fig:memory}  
 \end{minipage}
\end{figure}

On Figure~\ref{fig:time}, we observe that for most of the executions,  works faster that . We also observe that   does not behave well for a few particular executions, and that these executions all correspond to MDPs obtained from the forward algorithm of .

Figure~\ref{fig:memory} shows that regarding the memory consumption,  is  more efficient than  and it can thus solve larger MDPs (the largest MDP solved by  contains half a million states while  solves MDPs of more than  million states). This points out that monotonic MDPs are better handled by pseudo-antichains, which exploit the partial order on the state space, than by BDDs.

\medskip
Finally, in the majority of experiments we performed for both the EMP and the SSP problems, we observe that most of the execution time of the pseudo-antichain based symblicit algorithms is spent for lumping. It is also the case for the MTBDD based symblicit algorithm~\cite{DBLP:conf/qest/WimmerBBHCHDT10}.
