\section{Experiments} \label{sec:experiments}
In this section, we present two application scenarios of the pseudo-antichain based symblicit algorithm of the previous section, one for the SSP problem and the other for the EMP problem. In both cases, we first show the reduction to monotonic MDPs that satisfy Assumptions~\ref{atwo} and~\ref{aone}, and we then present some experimental results. All our experiments have been done on a Linux platform with a $3.2$GHz CPU (Intel Core i7) and $12$GB of memory. Note that our implementations are single-threaded and thus use only one core. For all those experiments, the timeout is set to $10$ hours and is denoted by \timeout. Finally, we restrict the memory usage to $4$GB\footnote{Restricting the memory usage to $4$GB is enough to have a good picture of the behavior of each implementation with respect to the memory consumption.} and when an execution runs out of memory, we denote it by \memout.

\subsection{Stochastic shortest path on STRIPSs}
We consider the following application of the pseudo-antichain based symblicit algorithm for the SSP problem. In the field of planning, a class of problems called \textit{planning from STRIPSs}~\cite{fikes1972strips} operate with states represented by valuations of propositional variables. Informally, a STRIPS is defined by an \textit{initial} state representing the initial configuration of the system and a set of \textit{operators} that transform a state into another state. The problem of planning from STRIPSs then asks, given a valuation of propositional variables representing a set of \textit{goal} states, to find a sequence of operators that lead from the initial state to a goal one. Let us first formally define the notion of STRIPS and show that each STRIPS can be made monotonic. We will then add stochastic aspects and show how to construct a monotonic MDP from a monotonic stochastic STRIPS.

\paragraph{STRIPSs.} A \textit{STRIPS}~\cite{fikes1972strips} is a tuple $(P, I, (M, N), O)$ where $P$ is a finite set of \textit{conditions} (i.e. propositional variables), $I \subseteq P$ is a subset of conditions that are initially true (all others are assumed to be false), $(M, N)$, with $M, N \subseteq P$ and $M \cap N = \emptyset$, specifies which conditions are true and false, respectively, in order for a state to be considered a goal state, and $O$ is a finite set of \textit{operators}. An operator $o \in O$ is a pair $((\gamma, \theta), (\alpha, \delta))$ such that $(\gamma, \theta)$ is the \textit{guard} of $o$, that is, $\gamma \subseteq P$ (resp. $\theta \subseteq P$) is the set of conditions that must be true (resp. false) for $o$ to be executable, and $(\alpha, \delta)$ is the \textit{effect} of $o$, that is, $\alpha \subseteq P$ (resp. $\delta \subseteq P$) is the set of conditions that are made true (resp. false) by the execution of $o$. For all $((\gamma, \theta), (\alpha, \delta)) \in O$, we have that $\gamma \cap \theta = \emptyset$ and $\alpha \cap \delta = \emptyset$.

\noindent From a STRIPS, we derive a transition system as follows. The set of states is $2^P$, that is, a state is represented by the set of conditions that are true in it. The initial state is $I$. The set of goal states are states $Q$ such that $Q  \supseteq M$ and $Q \cap N = \emptyset$. There is a transition from state $Q$ to state $Q'$ under operator $o = ((\gamma, \theta), (\alpha, \delta))$ if $Q \supseteq \gamma$, $Q \cap \theta = \emptyset$ (the guard is satisfied) and $Q' = (Q \cap \alpha) \setminus \delta$ (the effect is applied). A standard problem is to ask whether or not there exists a path from the initial state to a goal state.

\paragraph{Monotonic STRIPSs.} 
A \textit{monotonic STRIPS (MS)} is a tuple $(P, I, M, O)$ where $P$ and $I$ are defined as for STRIPSs, $M \subseteq P$ specifies which conditions must be true in a goal state, and $O$ is a finite set of operators. In the MS definition, an operator $o \in O$ is a pair $(\gamma, (\alpha, \delta))$ where $\gamma \subseteq P$ is the guard of $o$, that is, the set of conditions that must be true for $o$ to be executable, and $(\alpha, \delta)$ is the effect of $o$ as in the STRIPS definition. 
MSs thus differ from STRIPS in the sense that guards only apply on conditions that are true in states, and goal states are only specified by true conditions. The monotonicity will appear more clearly when we will derive hereafter monotonic MDPs from MSs.

\noindent
Each STRIPS ${\cal S} = (P, I, (M, N), O)$ can be made monotonic by duplicating the set of conditions, in the following way. We denote by $\overline{P}$ the set $\{\overline{p} \mid p \in P\}$ containing a new condition $\overline{p}$ for each $p \in P$ such that $\overline{p}$ represents the negation of the propositional variable $p$. We construct from $\cal S$ an MS ${\cal S}' = (P', I', M', O')$ such that $P' = P \cup \overline{P}$, $I' = I \cup \overline{P\diff I}\subseteq P'$, $M' = M \cup \overline{N} \subseteq P'$ and $O' = \{(\gamma \cup \overline{\theta}, (\alpha \cup \overline{\delta}, \delta \cup \overline{\alpha})) \mid ((\gamma, \theta), (\alpha, \delta)) \in O\}$. It is easy to check that ${\cal S}$ and ${\cal S}'$ are equivalent (a state $Q$ in ${\cal S}$ has its counterpart $Q \cup \overline{P\diff Q}$ in ${\cal S}'$). In the following, we thus only consider MSs. 

\begin{example} \label{ex:ST}
To illustrate the notion of MS, let us consider the following example of the monkey trying to reach a bunch of bananas (cf. Example~\ref{ex:monkey}). Let $(P, I, M, O)$ be an MS such that $P =$ \{\textit{box}, \textit{stick}, \textit{bananas}$\}$, $I = \emptyset$, $M = \{\textit{bananas}\}$, and $O = \{\textit{takebox}, \textit{takestick}, \textit{takebananas}\}$ where $\textit{takebox} = (\emptyset, (\{\textit{box}\}, \emptyset))$,  $\textit{takestick} = (\emptyset, (\{\textit{stick}\}, \emptyset))$ and $\textit{takebananas} = (\{\textit{box},\textit{stick}\}, (\{\textit{bananas}\}, \emptyset))$. In this MS, a condition $p \in P$ is true when the monkey possesses the item corresponding to $p$. At the beginning, the monkey possesses no item, i.e. $I$ is the empty set, and its goal is to get the \textit{bananas}, i.e. to reach a state $s \supseteq \{\textit{bananas}\}$. This can be done by first executing the operators \textit{takebox} and \textit{takestick} to respectively get the \textit{box} and the \textit{stick}, and then executing \textit{takebananas}, whose guard is $\{\textit{box}, \textit{stick}\}$.
\end{example}


\paragraph{Monotonic stochastic STRIPSs.} MSs can be extended with stochastic aspects as follows~\cite{blum2000probabilistic}. Each operator $o = (\gamma, \pi) \in O$ now consists of a guard $\gamma$ as before, and an effect given as a probability distribution $\pi : 2^P\times 2^P \rightarrow [0,1]$ on the set of pairs $(\alpha, \delta)$. An MS extended with such stochastic aspects is called a \textit{monotonic stochastic STRIPS (MSS)}. 

\noindent
Additionally, we associate with an MSS $(P, I, M, O)$ a cost function $C : O \rightarrow \R_{> 0}$ that associates a strictly positive cost with each operator. The problem of planning from MSSs is then to minimize the expected truncated sum up to the set of goal states from the initial state, i.e. this is a version of the SSP problem.

\begin{example}\label{ex:mst}
We extend the MS of Example~\ref{ex:ST} with stochastic aspects to illustrate the notion of MSS. Let $(P, I, M, O)$ be an MSS such that $P$, $I$ and $M$ are defined as in Example~\ref{ex:ST}, and $O =$ \{\textit{takebox}, \textit{takestick}, \textit{takebananaswithbox}, \textit{takebananaswithstick}, \textit{takebananaswithboth}\} where 
\begin{itemize}
\item $\textit{takebox} = (\emptyset, (1 : (\{\textit{box}\}, \emptyset)))$,
\item $\textit{takestick} = (\emptyset, (1 : (\{\textit{stick}\}, \emptyset)))$,
\item $\textit{takebananaswithbox} = (\{\textit{box}\}, (\frac{1}{4}: (\{\textit{bananas}\}, \emptyset), \frac{3}{4}: (\emptyset, \emptyset)))$,
\item $\textit{takebananaswithstick} = (\{\textit{stick}\}, (\frac{1}{5}: (\{\textit{bananas}\}, \emptyset), \frac{4}{5}: (\emptyset, \emptyset)))$, and 
\item $\textit{takebananaswithboth} = (\{\textit{box}, \textit{stick}\}, (\frac{1}{2}: (\{\textit{bananas}\}, \emptyset), \frac{1}{2}: (\emptyset, \emptyset)))$.
\end{itemize}
In this MSS, the monkey has a strictly positive probability to fail reaching the \textit{bananas}, whatever the items it uses. However, the probability of success increases when it has both the \textit{box} and the \textit{stick}.
\end{example}

In the following, we show that MSSs naturally define monotonic MDPs on which the pseudo-antichain based symblicit algorithm of Section~\ref{sec:paalgo} can be applied.

\paragraph{From MSSs to monotonic MDPs.} Let ${\cal S} = (P, I, M, O)$ be an MSS. We can derive from ${\cal S}$ an MDP $M_{\cal S} = (S, \ActionsO, \ActionsI, \edges, \distr)$ together with a set of goal states $G$ and a cost function $\reward$ such that:
\begin{itemize}
\itemsep0.1em
\item $S = 2^P$,
\item $G = \{s \in S \mid s \supseteq M\}$,
\item $\ActionsO = O$, and for all $s \in S$, $\enabledactions = \{(\gamma, \pi) \in \ActionsO \mid s \supseteq \gamma\}$,
\item $\ActionsI = \{(\alpha, \delta) \in 2^P\times2^P \mid \exists (\gamma, \pi) \in O, (\alpha, \delta) \in \support(\pi)\}$,
\item $\edges$, $\distr$ and $\reward$ are defined for all $s \in S$ and $\actionO = (\gamma, \pi) \in \enabledactions$, such that:
\begin{itemize} 
\item for all $\actionI = (\alpha, \delta) \in \ActionsI$, $\edges(s, \actionO)(\actionI) = (s \cup \alpha) \diff \delta$,
\item for all $\actionI \in \ActionsI$, $\distr(s, \actionO)(\actionI) = \pi(\actionI)$, and
\item $\reward(s, \actionO) = C(\actionO)$.
\end{itemize}
\end{itemize}
Note that we might have that $M_{\cal S}$ is not $\Sigma$-non-blocking, if no operator can be applied on some state of $\cal S$. In this case, we get a $\Sigma$-non-blocking MDP from $M_{\cal S}$ by eliminating states $s$ with $\enabledactions = \emptyset$  as long as it is necessary.

\begin{lemma} \label{lem:StripsMono}
The MDP $M_{\cal S}$ is monotonic, $G$ is closed, and functions $\distr, \reward$ are independent from $S$.
\end{lemma}
\begin{proof}
First, $S$ is equipped with the partial order $\supseteq$ and $(S, \supseteq)$ is a semilattice. Second, $S$ is closed for $\supseteq$ by definition. 
Thirdly,  we have that $\supseteq$ is compatible with $\edges$. Indeed, for all $s, s' \in S$ such that $s \supseteq s'$, for all $\actionO \in \ActionsO$ and $\actionI = (\alpha, \delta) \in \ActionsI$, $\edges(s, \actionO)(\actionI) = (s \cup \alpha)\diff \delta \supseteq (s' \cup \alpha) \diff \delta = \edges(s', \actionO)(\actionI)$. Finally the set $G =\ \antclos\{M\}$ of goal states is closed for $\supseteq$, and $\distr, \reward$ are clearly independent from $S$.
\qed\end{proof}

\paragraph{Symblicit algorithm.} In order to apply the pseudo-antichain based symblicit algorithm of Section~\ref{sec:paalgo} on the monotonic MDPs derived from MSSs, Assumptions~\ref{atwo} and~\ref{aone} must hold. Let us show that Assumption~\ref{aone} is satisfied. For all $s \in S$, $\actionO = (\gamma, \pi) \in \enabledactions$ and $\actionI = (\alpha, \delta) \in \ActionsI$, we clearly have an algorithm for computing $\edges(s, \actionO)(\actionI) = (s \cup \alpha)\diff \delta$, and $\distr(s, \actionO)(\actionI) = \pi(\actionI)$. Let us now consider Assumption~\ref{atwo}. An algorithm for computing $\lceil \Pre_{\actionO,\actionI}(\antclos\{x\})\rceil$, for all $x \in S$, $\actionO \in \ActionsO$ and $\actionI \in \ActionsI$, is given by the next proposition. 
\begin{proposition}
Let $x \in S$, $\actionO = (\gamma, \pi) \in \ActionsO$ and $\actionI = (\alpha, \delta) \in \ActionsI$. If $x \cap \delta \neq \emptyset$, then  $\lceil \Pre_{\actionO,\actionI}(\antclos\{x\})\rceil = \emptyset$, otherwise $\lceil \Pre_{\actionO,\actionI}(\antclos\{x\})\rceil = \{\gamma \cup (x \diff \alpha)\}$.
\end{proposition}
\begin{proof}
Suppose first that $x \cap \delta = \emptyset$.

We first prove that $s = \gamma \cup (x \diff \alpha) \in \Pre_{\actionO,\actionI}(\antclos\{x\})$.  We have to show that $\actionO \in \Sigma_{s}$ and  $\edges(s, \actionO)(\actionI) \in \antclos\{x\}$. Recall that $\actionO = (\gamma, \pi)$. We have that $s = \gamma \cup (x\diff \alpha) \supseteq \gamma$, showing that $\actionO \in \Sigma_{s}$. We have that $\edges(s, \actionO)(\actionI)  = (\gamma \cup (x \diff \alpha) \cup \alpha) \diff \delta =  (\gamma \cup x \cup \alpha) \diff \delta \supseteq x$ since $x \cap \delta = \emptyset$. We thus have that $\edges(s, \actionO)(\actionI) \in \antclos\{x\}$.

We then prove that for all $s \in \Pre_{\actionO,\actionI}(\antclos\{x\})$, $s \in \antclos\{\gamma \cup (x \diff \alpha)\}$, i.e. $s \supseteq \gamma \cup (x \diff \alpha)$. Let $s \in \Pre_{\actionO,\actionI}(\antclos\{x\})$. We have that $\actionO \in \Sigma_{s}$ and $\edges(s, \actionO)(\actionI) \in \antclos\{x\}$, that is,  $s \supseteq \gamma$ and $\edges(s, \actionO)(\actionI) = (s \cup \alpha) \diff \delta \supseteq x$. By classical set properties, it follows that $(s \cup \alpha) \supseteq x$, and then $s  \supseteq x\diff\alpha$. Finally, since $s \supseteq \gamma$, we have $s \supseteq \gamma \cup (x \diff \alpha)$, as required.

Suppose now that $x \cap \delta \neq \emptyset$, then $\Pre_{\actionO,\actionI}(\antclos\{x\}) = \emptyset$. Indeed for all $s \in \antclos\{x\}$, we have  $s \cap \delta \neq \emptyset$, and by definition of $\edges$, there is no $s'$ such that $\edges(s', \actionO)(\actionI) = s$.
\qed\end{proof}

Finally, notice that for the class of monotonic MDPs derived from MSSs, the symbolic representations described in Section~\ref{subsec:symbrep} are compact, since $G$ is closed and  $\distr, \reward$ are independent from $S$ (see Lemma~\ref{lem:StripsMono}).
Therefore we have all the required ingredients for an efficient pseudo-antichain based algorithm to solve the SSP problem for MSSs. The next experiments show its performance.

\paragraph{Experiments.} We have implemented  in Python and C the pseudo-antichain based symblicit algorithm for the SSP problem. The C language is used for all the low level operations while the orchestration is done with Python. The binding between C and Python is realized with the ctypes library of Python. The source code is publicly available at \url{http://lit2.ulb.ac.be/STRIPSSolver/}, together with the two benchmarks presented in this section. We compared our implementation with the purely explicit strategy iteration algorithm implemented in the development release 4.1.dev.r7712 of the tool $\sf{PRISM}$~\cite{KNP11}, since to the best of our knowledge, there is no tool implementing an MTBDD based symblicit algorithm for the SSP problem.\footnote{A comparison with an MTBDD based symblicit algorithm is done in the second application for the EMP problem.} Note that this explicit implementation exists primarily to prototype new techniques and is thus not fully optimized~\cite{Parker}. Note that value iteration algorithms are also implemented in $\sf{PRISM}$. 
While those algorithms are usually efficient, they only compute approximations. As a consequence, for the sake of a fair comparison, we consider here only the performances of strategy iteration algorithms. 

\medskip
The first benchmark ($\sf{Monkey}$) is obtained from Example~\ref{ex:mst}. In this benchmark, the monkey has several items at its disposal to reach the bunch of bananas, one of them being a stick. However, the stick is available as a set of several pieces that the monkey has to assemble. Moreover, the monkey has multiple ways to build the stick as there are several sets of pieces that can be put together. However, the time required to build a stick varies from a set of pieces to another. Additionally, we add useless items in the room: there is always a set of pieces from which the probability of getting a stick is $0$. The operators of getting some items are stochastic, as well as the operator of getting the bananas: the probability of success varies according to the owned items (cf. Example~\ref{ex:mst}). The benchmark is parameterized in the number $p$ of pieces required to build a stick, and in the number $s$ of sticks that can be built. Note that the monkey can only use one stick, and thus has no interest to build a second stick if it already has one. Results are given in Table~\ref{table:STRIPS1}. 

\begin{table}[h!]
	\caption{Stochastic shortest path on the $\sf{Monkey}$ benchmark. The column $(s, p)$ gives the parameters of the problem, $\ETP_\lambda$ the expected truncated sum of the computed strategy $\lambda$, and $|M_{\cal S}|$ the number of states of the MDP.
For the pseudo-antichain based implementation ($\sf{PA}$), $\#$it is the number of iterations of the strategy iteration algorithm, $|\partlump|$ the maximum size of computed bisimulation quotients, \textit{lump} the total time spent for lumping, \textit{syst} the total time spent for solving the linear systems, and \textit{impr} the total time spent for improving the strategies. For the explicit implementation ($\sf{Explicit}$), \textit{constr} is the time spent for model construction and \textit{strat} the time spent for the strategy iteration algorithm. For both implementations, \textit{total} is the total execution time and \textit{mem} the total memory consumption. All times are given in seconds and all memory consumptions are given in megabytes.}
	\label{table:STRIPS1}
	\centering
		\scriptsize
 		\begin{tabular}{|r|r|r||r|r|r|r|r|r|r||r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
		\hline
	  	& & & \multicolumn{7}{|c||}{{\small $\sf{PA}$}} & \multicolumn{4}{|c|}{{\small $\sf{Explicit}$}}\rule[-2pt]{0pt}{10pt}\\
		$\ (s, p) \ $ & $\ETP_\lambda$  & $|M_{\cal S}|$  & \ $\#$it \  & $\ |\partlump|\ $ & \ lump  \ &  \ syst \  &  \ impr  \ & \  total \  & mem & \ constr \  & \ strat \ & \ total \ & \ mem\ \rule[-3pt]{0pt}{10pt}\\
\hline\hline
$\ (1,2)\ $ & $\ 35.75\ $ & $\ 256\ $ & $\ 4\ $ & $\ 15\ $ & $\ 0.01\ $ & $\ 0.00\ $ & $\ 0.02\ $  & $\ 0.03\ $ & $\ 15.6\ $ & $\ 0.4\ $ & $\ 0.03\ $  & $\ 0.43\ $ & $\ 178.2\ $\rule[-3pt]{0pt}{10pt}\\
$\ (1,3)\ $ & $\ 35.75\ $ & $\ 1024\ $ & $\ 5\ $ & $\ 19\ $ & $\ 0.04\ $ & $\ 0.00\ $ & $\ 0.03\ $  & $\ 0.07\ $ & $\ 15.8\ $ & $\ 3.42\ $ & $\ 0.09\ $  & $\ 3.51\ $ & $\ 336.7\ $\rule[-3pt]{0pt}{10pt}\\
$\ (1,4)\ $ & $\ 35.75\ $ & $\ 4096\ $ & $\ 6\ $ & $\ 31\ $ & $\ 0.17\ $ & $\ 0.00\ $ & $\ 0.12\ $  & $\ 0.29\ $ & $\ 16.3\ $ & $\ 55.29\ $ & $\ 0.16\ $  & $\ 55.45\ $ & $\ 1735.1\ $\rule[-3pt]{0pt}{10pt}\\
$\ (1,5)\ $ & $\ 36.00\ $ & $\ 16384\ $ & $\ 7\ $ & $\ 39\ $ & $\ 0.75\ $ & $\ 0.00\ $ & $\ 0.62\ $  & $\ 1.37\ $ & $\ 17.1\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
\hline $\ (2,2)\ $ & $\ 34.75\ $ & $\ 1024\ $ & $\ 5\ $ & $\ 19\ $ & $\ 0.05\ $ & $\ 0.00\ $ & $\ 0.03\ $  & $\ 0.09\ $ & $\ 15.8\ $ & $\ 3.2\ $ & $\ 0.12\ $  & $\ 3.32\ $ & $\ 379.9\ $\rule[-3pt]{0pt}{10pt}\\
$\ (2,3)\ $ & $\ 34.75\ $ & $\ 8192\ $ & $\ 5\ $ & $\ 37\ $ & $\ 0.32\ $ & $\ 0.00\ $ & $\ 0.13\ $  & $\ 0.45\ $ & $\ 16.4\ $ & $\ 240.66\ $ & $\ 0.30\ $  & $\ 240.96\ $ & $\ 3463.2\ $\rule[-3pt]{0pt}{10pt}\\
$\ (2,4)\ $ & $\ 34.75\ $ & $\ 65536\ $ & $\ 6\ $ & $\ 45\ $ & $\ 2.39\ $ & $\ 0.01\ $ & $\ 1.04\ $  & $\ 3.44\ $ & $\ 18.0\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
$\ (2,5)\ $ & $\ 35.75\ $ & $\ 524288\ $ & $\ 7\ $ & $\ 65\ $ & $\ 27.56\ $ & $\ 0.02\ $ & $\ 10.13\ $  & $\ 37.71\ $ & $\ 23.4\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
\hline $\ (3,2)\ $ & $\ 35.75\ $ & $\ 4096\ $ & $\ 4\ $ & $\ 23\ $ & $\ 0.09\ $ & $\ 0.00\ $ & $\ 0.07\ $  & $\ 0.16\ $ & $\ 16.0\ $ & $\ 60.43\ $ & $\ 0.16\ $  & $\ 60.59\ $ & $\ 1625.8\ $\rule[-3pt]{0pt}{10pt}\\
$\ (3,3)\ $ & $\ 35.75\ $ & $\ 65536\ $ & $\ 5\ $ & $\ 43\ $ & $\ 1.14\ $ & $\ 0.00\ $ & $\ 0.43\ $  & $\ 1.57\ $ & $\ 17.3\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
$\ (3,4)\ $ & $\ 35.75\ $ & $\ 1048576\ $ & $\ 6\ $ & $\ 57\ $ & $\ 12.89\ $ & $\ 0.01\ $ & $\ 4.92\ $  & $\ 17.83\ $ & $\ 21.7\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
$\ (3,5)\ $ & $\ 36.00\ $ & $\ 16777216\ $ & $\ 7\ $ & $\ 88\ $ & $\ 208.33\ $ & $\ 0.05\ $ & $\ 63.73\ $  & $\ 272.13\ $ & $\ 37.5\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
\hline $\ (4,2)\ $ & $\ 35.75\ $ & $\ 16384\ $ & $\ 4\ $ & $\ 29\ $ & $\ 0.22\ $ & $\ 0.02\ $ & $\ 0.14\ $  & $\ 0.38\ $ & $\ 16.3\ $ & $\ 1114.19\ $ & $\ 0.70\ $  & $\ 1114.89\ $ & $\ 1704.3\ $\rule[-3pt]{0pt}{10pt}\\
$\ (4,3)\ $ & $\ 35.75\ $ & $\ 524288\ $ & $\ 5\ $ & $\ 50\ $ & $\ 2.72\ $ & $\ 0.00\ $ & $\ 1.26\ $  & $\ 4.00\ $ & $\ 18.3\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
$\ (4,4)\ $ & $\ 35.75\ $ & $\ 16777216\ $ & $\ 6\ $ & $\ 87\ $ & $\ 45.68\ $ & $\ 0.04\ $ & $\ 22.41\ $  & $\ 68.14\ $ & $\ 25.0\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
$\ (4,5)\ $ & $\ 36.00\ $ & $\ 536870912\ $ & $\ 7\ $ & $\ 114\ $ & $\ 724.77\ $ & $\ 0.11\ $ & $\ 532.46\ $  & $\ 1257.41\ $ & $\ 60.9\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
\hline $\ (5,2)\ $ & $\ 35.75\ $ & $\ 65536\ $ & $\ 4\ $ & $\ 31\ $ & $\ 0.36\ $ & $\ 0.00\ $ & $\ 0.18\ $  & $\ 0.54\ $ & $\ 16.6\ $ & $\ 20312.67\ $ & $\ 3.50\ $  & $\ 20316.17\ $ & $\ 2342.6\ $\rule[-3pt]{0pt}{10pt}\\
$\ (5,3)\ $ & $\ 35.75\ $ & $\ 4194304\ $ & $\ 5\ $ & $\ 56\ $ & $\ 5.71\ $ & $\ 0.02\ $ & $\ 2.47\ $  & $\ 8.20\ $ & $\ 19.5\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
$\ (5,4)\ $ & $\ 35.75\ $ & $\ 268435456\ $ & $\ 6\ $ & $\ 97\ $ & $\ 95.49\ $ & $\ 0.04\ $ & $\ 101.27\ $  & $\ 196.83\ $ & $\ 31.3\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
$\ (5,5)\ $ & $\ 36.00\ $ & $\ 17179869184\ $ & $\ 7\ $ & $\ 152\ $ & $\ 1813.78\ $ & $\ 0.08\ $ & $\ 5284.31\ $  & $\ 7098.40\ $ & $\ 81.3\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
\hline
		\end{tabular}
		\normalsize
\end{table}

The second benchmark ($\sf{Moats~and~castles}$) is an adaptation of a benchmark of~\cite{DBLP:conf/aips/MajercikL98} as proposed in~\cite{blum2000probabilistic}\footnote{In~\cite{blum2000probabilistic}, the authors study a different problem that is to maximize the probability of reaching the goal within a given number of steps.}. The goal is to build a sand castle on the beach; a moat can be dug before in a way to protect it. We consider up to $7$ discrete depths of moat. The operator of building the castle is stochastic: there is a strictly positive probability for the castle to be demolished by the waves. However, the deeper the moat is, the higher the probability of success is. For example, the first depth of moat offers a probability $\frac{1}{4}$ of success, while with the second depth of moat, the castle has probability $\frac{9}{20}$ to resist to the waves. The optimal strategy for this problem is to dig up to a given depth of moat and then repeat the action of building the castle until it succeeds. The optimal depth of moat then depends on the cost of the operators and the respective probability of successfully building the castle for each depth of moat. To increase the difficulty of the problem, we consider building several castles, each one having its own moat. The benchmark is parameterized in the number $d$ of depths of moat that can be dug, and the number $c$ of castles that have to be built. Results are given in Table~\ref{table:STRIPS2}.

\begin{table}[h!]
	\caption{Stochastic shortest path on the $\sf{Moats~and~castles}$ benchmark. The column $(c, d)$ gives the parameters of the problem and all other columns have the same meaning as in Table~\ref{table:STRIPS1}.}
	\label{table:STRIPS2}
	\centering
	\scriptsize
 		\begin{tabular}{|r|r|r||r|r|r|r|r|r|r||r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
		\hline
	  	& & & \multicolumn{7}{|c||}{{\small $\sf{PA}$}} & \multicolumn{4}{|c|}{{\small $\sf{Explicit}$}}\rule[-2pt]{0pt}{10pt}\\
		$\ (c, d) \ $ & $\ETP_\lambda$  & $|M_{\cal S}|$  & \ $\#$it \  & $\ |\partlump|\ $ & \ lump  \ &  \ syst \  &  \ impr  \ & \  total \  & mem & \ constr \  & \ strat \ & \ total \ & \ mem\ \rule[-3pt]{0pt}{10pt}\\
\hline\hline
$\ (2,3)\ $ & $\ 39.3333\ $ & $\ 256\ $ & $\ 3\ $ & $\ 17\ $ & $\ 0.05\ $ & $\ 0.01\ $ & $\ 0.04\ $  & $\ 0.10\ $ & $\ 15.8\ $ & $\ 0.46\ $ & $\ 0.03\ $  & $\ 0.49\ $ & $\ 206.9\ $\rule[-3pt]{0pt}{10pt}\\
$\ (2,4)\ $ & $\ 34.6667\ $ & $\ 1024\ $ & $\ 3\ $ & $\ 34\ $ & $\ 0.41\ $ & $\ 0.00\ $ & $\ 0.17\ $  & $\ 0.58\ $ & $\ 16.5\ $ & $\ 6.30\ $ & $\ 0.08\ $  & $\ 6.38\ $ & $\ 483.8\ $\rule[-3pt]{0pt}{10pt}\\
$\ (2,5)\ $ & $\ 32.2222\ $ & $\ 4096\ $ & $\ 3\ $ & $\ 49\ $ & $\ 1.36\ $ & $\ 0.00\ $ & $\ 0.45\ $  & $\ 1.82\ $ & $\ 17.3\ $ & $\ 133.46\ $ & $\ 0.20\ $  & $\ 133.66\ $ & $\ 1202.5\ $\rule[-3pt]{0pt}{10pt}\\
$\ (2,6)\ $ & $\ 32.2222\ $ & $\ 16384\ $ & $\ 3\ $ & $\ 66\ $ & $\ 9.71\ $ & $\ 0.01\ $ & $\ 1.95\ $  & $\ 11.68\ $ & $\ 19.3\ $ & $\ 2966.01\ $ & $\ 0.79\ $  & $\ 2966.80\ $ & $\  1706.2\ $\rule[-3pt]{0pt}{10pt}\\
\hline $\ (3,2)\ $ & $\ 72.6667\ $ & $\ 512\ $ & $\ 3\ $ & $\ 45\ $ & $\ 0.52\ $ & $\ 0.00\ $ & $\ 0.24\ $  & $\ 0.77\ $ & $\ 16.6\ $ & $\ 1.77\ $ & $\ 0.06\ $  & $\ 1.83\ $ & $\ 282.9\ $\rule[-3pt]{0pt}{10pt}\\
$\ (3,3)\ $ & $\ 59.0000\ $ & $\ 4096\ $ & $\ 3\ $ & $\ 84\ $ & $\ 12.58\ $ & $\ 0.03\ $ & $\ 2.73\ $  & $\ 15.35\ $ & $\ 20.2\ $ & $\ 149.44\ $ & $\ 0.20\ $  & $\ 149.64\ $ & $\ 1205.5\ $\rule[-3pt]{0pt}{10pt}\\
$\ (3,4)\ $ & $\ 52.0000\ $ & $\ 32768\ $ & $\ 3\ $ & $\ 219\ $ & $\ 129.17\ $ & $\ 0.05\ $ & $\ 21.56\ $  & $\ 150.83\ $ & $\ 30.7\ $ & $\ 14658.22\ $ & $\ 2.47\ $  & $\ 14660.69\ $ & $\ 1610.9\ $\rule[-3pt]{0pt}{10pt}\\
$\ (3,5)\ $ & $\ 48.3333\ $ & $\ 262144\ $ & $\ 3\ $ & $\ 357\ $ & $\ 658.86\ $ & $\ 0.13\ $ & $\ 81.08\ $  & $\ 740.17\ $ & $\ 49.1\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
$\ (3,6)\ $ & $\ 48.3333\ $ & $\ 2097152\ $ & $\ 3\ $ & $\ 595\ $ & $\ 10730.09\ $ & $\ 0.42\ $ & $\ 865.48\ $  & $\ 11596.71\ $ & $\ 145.8\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
\hline $\ (4,2)\ $ & $\ 96.8889\ $ & $\ 4096\ $ & $\ 3\ $ & $\ 132\ $ & $\ 31.61\ $ & $\ 0.03\ $ & $\ 12.06\ $  & $\ 43.72\ $ & $\ 26.5\ $ & $\ 173.40\ $ & $\ 0.22\ $  & $\ 173.62\ $ & $\ 1211.2\ $\rule[-3pt]{0pt}{10pt}\\
$\ (4,3)\ $ & $\ 78.6667\ $ & $\ 65536\ $ & $\ 3\ $ & $\ 464\ $ & $\ 1376.94\ $ & $\ 0.21\ $ & $\ 217.06\ $  & $\ 1594.48\ $ & $\ 82.2\ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
\hline
		\end{tabular}
	\normalsize
\end{table}

On those two benchmarks, we observe that the explicit implementation quickly runs out of memory when the state space of the MDP grows. Indeed, with this method, we were not able to solve MDPs with more than $65536$ (resp. $32768$) states in Table~\ref{table:STRIPS1} (resp. Table~\ref{table:STRIPS2}). On the other hand, the symblicit algorithm behaves well on large models: the memory consumption never exceeds $150$Mo and this even for MDPs with hundreds of millions of states. For instance, the example $(5, 5)$ of the $\sf{Monkey}$ benchmark is an MDP of more than $17$ billions of states that is solved in less than $2$ hours with only $82$Mo of memory\footnote{On our benchmarks, the value iteration algorithm of ${\sf PRISM}$~performs better than the strategy iteration one w.r.t. the run time and memory consumption. However, it still consumes more memory than the pseudo-antichain based algorithm, and runs out of memory on several examples.}.


\subsection{Expected mean-payoff with \LTLMP synthesis}
We consider another application of the pseudo-antichain based symblicit algorithm, but now for the EMP problem. This application is related to the problems of \LTLMP realizability and synthesis~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13}. Let us fix some notations and definitions. Let $\phi$ be an \LTL formula defined over the set $P = I \uplus O$ of signals and let $\Sigma_P = 2^P$, $\Sigma_O = 2^O$ and $\Sigma_I = 2^I$. Let $\Lit(O) = \{o \mid o \in O\} \cup \{\neg o \mid o \in O\}$ be the set of literals over $O$. Let $w : \Lit(O) \mapsto \Z$ be a weight function where positive numbers represent rewards.\footnote{Note that in~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13}, the weight function $w$ is more general since it also associates values to $\Lit(I)$. However, for this application, we restrict $w$ to $\Lit(O)$.} This function is extended to $\Sigma_O$ as follows:  $w(\sigma) = \Sigma_{o \in \sigma} w(o) +  \Sigma_{o \in O \setminus \{\sigma\}} w(\neg o)$ for all $\sigma \in \Sigma_O$.

\paragraph{$\LTLMP$ realizability and synthesis.} The problem of \LTLMP realizability is best seen as a game between two players, Player $O$ and Player $I$. This game is infinite and such that at each turn $k$, Player $O$ gives a subset $o_k \in \Sigma_O$ and Player $I$ responds by giving a subset $i_k \in \Sigma_I$. The outcome of the game is the infinite word $(o_0 \cup i_0)(o_1 \cup i_1)\dots \in \Sigma_P^\omega$. A strategy for Player $O$ is a mapping $\lambda_O : (\Sigma_O\Sigma_I)^* \rightarrow \Sigma_O$, while a strategy for Player $I$ is a mapping $\lambda_I : (\Sigma_O\Sigma_I)^*\Sigma_O \rightarrow \Sigma_I$. The outcome of the strategies $\lambda_O$ and $\lambda_I$ is the word $\Outcome(\lambda_O, \lambda_I) = (o_0 \cup i_0)(o_1 \cup i_1)\dots$ such that $o_0 = \lambda_O(\epsilon), i_0 = \lambda_I(o_0)$ and for all $k \geq 1, o_k = \lambda_O(o_0i_0\dots o_{k-1}i_{k-1})$ and $i_k = \lambda_I(o_0i_0\dots o_{k-1}i_{k-1}o_k)$. A \textit{value} $\mpval(u)$ is associated with each outcome $u \in \Sigma_P^\omega$ such that
\begin{center}
$\mpval(u) =$
$\begin{cases} \liminf_{n \rightarrow \infty} \frac{1}{n} \sum_{k=0}^{n-1} w(o_k) \text{ if } u \models \phi\\  - \infty \text{ otherwise} \end{cases}$
\end{center}
i.e. $\mpval(u)$ is the mean-payoff value of $u$ if $u$ satisfies $\phi$, otherwise, it is $-\infty$. Given an $\LTL$ formula $\phi$ over $P$, a weight function $w$ and a threshold value $\nu \in \Z$, the $\LTLMP$ \textit{realizability problem} asks to decide whether there exists a strategy $\lambda_O$ for Player $O$ such that $\mpval(\Outcome(\lambda_O, \lambda_I)) \geq \nu$ for all strategies $\lambda_I$ of Player~$I$. If the answer is $\mathsf{Yes}$, $\phi$ is said $\MP$-realizable. The $\LTLMP$ \textit{synthesis problem} is then to produce such a strategy $\lambda_O$ for Player $O$. 

To illustrate the problems of $\LTLMP$ realizability and synthesis, let us consider the following specification of a server that should grant exclusive access to a resource to two clients.

\begin{example} \label{ex:ltl} 
A client requests access to the resource by setting to true its request signal ($r_1$ for client~$1$ and $r_2$ for client~$2$), and the server grants those requests by setting to true the respective grant signal $g_1$ or $g_2$. We want to synthesize a server that eventually grants any client request, and that only grants one request at a time. Additionally, we ask client~$2$'s requests to take the priority over client 1's requests.  Moreover, we would like to keep minimal the delay between requests and grants. This can be formalized by the \LTL formula $\phi$ given below where the signals in $I = \{r_1, r_2\}$ are controlled by the two clients, and the signals in $O = \{g_1,w_1, g_2,w_2 \}$ are controlled by the server. Moreover we add the next weight function $w : \Lit(O) \rightarrow \Z$:

\begin{minipage}{0.5\linewidth}
\begin{equation*} 
	\begin{split}
		\phi_1 &= \square(r_1 \rightarrow \nextLTL(w_1 \until g_1))\\
		\phi_2 &= \square(r_2 \rightarrow \nextLTL(w_2 \until g_2))\\
		\phi_3 &= \square(\lnot g_1 \vee \lnot g_2)\\
		\phi &= \phi_1 \wedge \phi_2 \wedge \phi_3
	\end{split} 
\end{equation*}
\end{minipage}
\begin{minipage}{0.4\linewidth}
\begin{center}
$w(l) =$
$\begin{cases} -1 \text{ if } l = w_1\\ -2 \text{ if } l = w_2\\ 0 \text{ otherwise.} \end{cases}$
\end{center}
\end{minipage}

\medskip\noindent
A possible strategy for the server is to behave as follows: it grants immediately any request of client~$2$ if the last ungranted request of client~$1$ has been emitted less than $n$ steps in the past, otherwise it grants the request of client~$1$. The mean-payoff value of this solution in the worst-case (when the two clients always emit their respective request) is equal to $-(1 + \frac{1}{n})$.
\end{example}

\paragraph{Reduction to safety games.} In~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13}, we propose an antichain based algorithm for solving the $\LTLMP$ realizability and synthesis problems with a reduction to a two-player turn-based safety game. We here present this game $G$ without explaining the underlying reasoning, see~\cite{DBLP:journals/corr/abs-1210-3539} for more details. The game $G = (S_O, S_I, E, \alpha)$ is a turn-based safety game such that $S_O$ (resp. $S_I$) is the set of positions of Player $O$ (resp. Player $I$), $E$ is the set of edges labeled by $o \in \Sigma_O$ (resp. $i \in \Sigma_I$) when leaving a position in $S_O$ (resp. $S_I$), and $\alpha \subseteq S_O \cup S_I$ is the set of bad positions (i.e. positions that Player $O$ must avoid to reach). Let $\Win_O$ be the set of positions in $G$ from which Player $O$ can force Player $I$ to stay in $(S_O\cup S_I)\diff\alpha$, that is the set of winning positions for Player $O$. The safety game $G$ restricted to positions $\Win_O$ is a representation of a subset of the set of all winning strategies $\lambda_O$ for Player $O$ that ensure a value $\mpval(\Outcome(\lambda_O,\lambda_I))$ greater than or equal to the given threshold $\nu$, for all strategies $\lambda_I$ of Player $I$. Those strategies are called \textit{worst-case winning strategies}. 

Note that the reduction to safety games given in~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13} allows to compute the set of all worst-case winning strategies (instead of a subset of them). 
Indeed the proposed algorithm is incremental on two parameters $K = 0, 1, \dots$ and $C= 0, 1, \dots$, and works as follows. For each value of $K$ and $C$, a corresponding safety game is constructed, whose number of states depends on $K$ and $C$. Those safety games have the following nice property. If player $O$ has a worst-case winning strategy in the safety game for $K = k$ and $C = c$, then he has a worst-case winning strategy in all the safety games for $K \geq k$ and $C \geq c$. The algorithm thus stops as soon as a worst-case winning strategy is found. There exist theoretical bounds $\mathbb{K}$ and $\mathbb{C}$ such that the set of all worst-case winning strategies can be represented by the safety game with parameters $\mathbb{K}$ and $\mathbb{C}$. However, $\mathbb{K}$ and $\mathbb{C}$ being huge, constructing this game is unfeasible in practice. 


\paragraph{From safety games to MDPs.} We can go beyond $\LTLMP$ synthesis. Let $G$ be a safety game as above, that represents a subset of worst-case winning strategies. 
For each state $s \in \Win_O\cap S_O$, we denote by $\Sigma_{O,s} \subseteq \Sigma_O$ the set of actions that are safe to play in $s$ (i.e. actions that force Player $I$ to stay in $\Win_O$). For all $s \in \Win_O\cap S_O$, we know that $\Sigma_{O,s} \neq \emptyset$ by construction of $\Win_O$.
From this set of worst-case winning strategies, we want to compute the one that behaves \textit{the best against a stochastic opponent}. Let $\probI : \Sigma_I \rightarrow\ ]0,1]$ be a probability distribution on the actions of Player $I$. Note that we require $\support(\probI) = \Sigma_I$ so that it makes sense with the worst-case. 
By replacing Player $I$ by $\probI$ in the safety game $G$ restricted to $\Win_O$, we derive an MDP $M_G = (S, \ActionsO, \ActionsI, \edges, \distr)$ where:
\begin{itemize}
\itemsep0.1em
\item $S = \Win_O\cap S_O$,
\item $\ActionsO = \Sigma_O$, and for all $s \in S$, $\enabledactions = \Sigma_{O,s}$,
\item $\ActionsI = \Sigma_I$,
\item $\edges$, $\distr$ and $\reward$ are defined for all $s \in S$ and $\actionO \in \enabledactions$, such that:
\begin{itemize}
\item for all $\actionI \in \ActionsI$, $\edges(s, \actionO)(\actionI) = s'$ such that $(s, \actionO, s''), (s'', \actionI, s') \in E$,
\item for all $\actionI \in \ActionsI$, $\distr(s, \actionO)(\actionI) = \probI(\actionI)$, and
\item $\reward(s, \actionO) = w(\actionO)$.
\end{itemize}
\end{itemize}
Note that since $\Sigma_{O,s} \neq \emptyset$ for all $s \in S$, we have that $M$ is $\Sigma$-non-blocking. 

Computing the best strategy against a stochastic opponent among the worst-case winning strategies represented by $G$ reduces to solving the EMP problem for the MDP $M_G$\footnote{More precisely, it reduces to the EMP problem where the objective is to maximize the expected mean-payoff (see footnotes~\ref{fn:altobj} and~\ref{fn:MPmax}).}.

\begin{lemma}  \label{lem:LTLMono}
The MDP $M_G$ is monotonic, and functions $\distr, \reward$ are independent from $S$. 
\end{lemma}

\begin{proof}
It is shown in~\cite{DBLP:journals/corr/abs-1210-3539,DBLP:conf/tacas/BohyBFR13} that the safety game $G$ has properties of monotony. The set $S_O\cup S_I$ is equipped with a partial order $\preceq$ such that $(S_O\cup S_I, \preceq)$ is a complete lattice, and the sets $S_O, S_I$ and $\Win_O$ are closed for $\preceq$. For the MDP $M_G$ derived from $G$, we thus have that $(S, \preceq)$ is a (semi)lattice, and $S$ is closed for $\preceq$. Moreover, by construction of $G$ (see details in~\cite[Sec. 5.1]{DBLP:journals/corr/abs-1210-3539}) and $M_G$, 
we have that $\preceq$ is compatible with~$\edges$. By construction, $\distr, \reward$ are independent from $S$.
\qed\end{proof}


\paragraph{Symblicit algorithm.} 
In order to apply the pseudo-antichain based symblicit algorithm of Section~\ref{sec:paalgo}, Assumptions~\ref{atwo} and~\ref{aone} must hold for $M_G$. This is the case for Assumption~\ref{aone} since $\edges(s, \actionO)(\actionI)$ can be computed for all $s\in S, \actionO \in \enabledactions$ and $\actionI \in \ActionsI$ (see~\cite[Sec. 5.1]{DBLP:journals/corr/abs-1210-3539}), and $\distr$ is given by $\probI$. Moreover, from \cite[Prop. 24]{DBLP:journals/corr/abs-1210-3539} and $\support(\probI) = \Sigma_I$, we have an algorithm for computing $\lceil \Pre_{\actionO,\actionI}(\antclos \{x\})\rceil$, for all $x \in S$. So, Assumption~\ref{atwo} holds too. 
Notice also that for the MDP $M_G$ derived from the safety game $G$, the symbolic representations described in Section~\ref{subsec:symbrep} are compact, since $\distr$ and $\reward$ are independent from $S$ (see Lemma~\ref{lem:LTLMono}).

Therefore for this second class of MDPs, we have again an efficient pseudo-antichain based algorithm to solve the EMP problem, as indicated by the next experiments.

\paragraph{Experiments.} We have implemented the pseudo-antichain based symblicit algorithm for the EMP problem and integrated it into $\sf{Acacia+}$~($\sf{v2.2}$)~\cite{DBLP:conf/cav/BohyBFJR12}. $\sf{Acacia+}$ is a tool written in Python and C that provides an antichain based version of the algorithm described above for solving the $\LTLMP$ realizability and synthesis problems. The last version of $\sf{Acacia+}$ is available at \url{http://lit2.ulb.ac.be/acaciaplus/}, together with all the examples considered in this section. It can also be used directly online via a web interface. We compared our implementation with an MTBDD based symblicit algorithm implemented in $\sf{PRISM}$~\cite{prismEMP}. To the best of our knowledge, only strategy iteration algorithms are implemented for the EMP problem. In the sequel, for the sake of simplicity, we refer to the MTBDD based implementation as $\sf{PRISM}$ and to the pseudo-antichain based one as $\sf{Acacia+}$. Notice that for $\sf{Acacia+}$, the given execution times and memory consumptions only correspond to the part of the execution concerning the symblicit algorithm (and not the construction of the safety game $G$ and the subset of worst-case winning strategies that it represents).

\medskip
We compare the two implementations on a benchmark of~\cite{DBLP:conf/tacas/BohyBFR13} obtained from the $\LTLMP$ specification of Example~\ref{ex:ltl} extended with stochastic aspects ($\sf{Stochastic~shared~resource~arbiter}$). For the stochastic opponent, we set a probability distribution such that requests of client~$1$ are more likely to happen than requests of client~$2$: at each turn, client~$1$ has probability $\frac{3}{5}$ to make a request, while client~$2$ has probability $\frac{1}{5}$. The probability distribution $\probI : \Sigma_I \rightarrow\ ]0,1]$ is then defined as $\probI(\{\neg r_1, \neg r_2\}) = \frac{8}{25}$, $\probI(\{r_1, \neg r_2\}) = \frac{12}{25}$, $\probI(\{\neg r_1, r_2\}) = \frac{2}{25}$ and $\probI(\{r_1, r_2\}) = \frac{3}{25}$. We use the backward algorithm of $\sf{Acacia+}$ for solving the related safety games. The benchmark is parameterized in the threshold value $\nu$. Results are given in Table~\ref{table:LTL}. Note that the number of states in the MDPs depends on the implementation. Indeed, for $\sf{PRISM}$, it is the number of reachable states of the MDP, denoted $|M_G^R|$, that is, the states that are really taken into account by the algorithm, while for $\sf{Acacia+}$, it is the total number of states since unlike $\sf{PRISM}$, our implementation does not prune unreachable states. For this application scenario, we observe that the ratio (number of reachable states)/(total number of states) is in general quite small\footnote{For all the MDPs considered in Tables~\ref{table:STRIPS1} and \ref{table:STRIPS2}, this ratio is $1$.}. 

\begin{table}[h!]
	\caption{Expected mean-payoff on the $\sf{Stochastic\ shared\ resource\ arbiter}$ benchmark with $2$ clients and decreasing threshold values. The column $\nu$ gives the threshold, $|M_G^R|$ the number of reachable states in the MDP, and all other columns have the same meaning as in Table~\ref{table:STRIPS1}. The expected mean-payoff $\EMP_\lambda$ of the optimal strategy $\lambda$ for all the examples is $-0.130435$.}	\label{table:LTL}
	\centering
	\scriptsize
 	\begin{tabular}{|r||r|r|r|r|r|r|r|r||r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
		\hline
	  	& \multicolumn{8}{|c||}{{\small $\sf{Acacia+}$}} & \multicolumn{5}{|c|}{{\small $\sf{PRISM}$}}\rule[-2pt]{0pt}{10pt}\\
		$\nu~$ & $|M_G|$  & \ $\#$it \  & $\ |\partlump|\ $ & \ lump  \ &  \ LS \  &  \ impr  \ & \  total \  & \ mem \ & $|M_G^R| $ & \ constr \  & \ strat \  & \ total \ & \ mem\ \rule[-3pt]{0pt}{10pt}\\
\hline $\ -1.1\ $ & $\ 5259\ $ & $\ 2\ $ & $\ 22\ $ & $\ 0.12\ $ & $\ 0.01\ $ & $\ 0.02\ $  & $\ 0.15\ $ & $\ 17.4\ $ & $\ 691\ $ & $\ 0.43\ $ & $\ 0.07\ $  & $\ 0.50\ $ & $\ 168.1\ $\rule[-3pt]{0pt}{10pt}\\
$\ -1.05\ $ & $\ 72159\ $ & $\ 2\ $ & $\ 42\ $ & $\ 0.86\ $ & $\ 0.01\ $ & $\ 0.09\ $  & $\ 0.97\ $ & $\ 17.7\ $ & $\ 6440\ $ & $\ 1.58\ $ & $\ 0.18\ $  & $\ 1.76\ $ & $\ 249.9\ $\rule[-3pt]{0pt}{10pt}\\
$\ -1.04\ $ & $\ 35750\ $ & $\ 2\ $ & $\ 52\ $ & $\ 1.63\ $ & $\ 0.02\ $ & $\ 0.13\ $  & $\ 1.79\ $ & $\ 18.1\ $ & $\ 3325\ $ & $\ 1.78\ $ & $\ 0.28\ $  & $\ 2.06\ $ & $\ 264.1\ $\rule[-3pt]{0pt}{10pt}\\
$\ -1.03\ $ & $\ 501211\ $ & $\ 2\ $ & $\ 70\ $ & $\ 4.41\ $ & $\ 0.04\ $ & $\ 0.26\ $  & $\ 4.71\ $ & $\ 18.8\ $ & $\ 15829\ $ & $\ 4.83\ $ & $\ 0.46\ $  & $\ 5.29\ $ & $\ 277.0\ $\rule[-3pt]{0pt}{10pt}\\
$\ -1.02\ $ & $\ 530299\ $ & $\ 2\ $ & $\ 102\ $ & $\ 16.62\ $ & $\ 0.11\ $ & $\ 0.64\ $  & $\ 17.39\ $ & $\ 20.2\ $ & $\ 11641\ $ & $\ 6.74\ $ & $\ 0.59\ $  & $\ 7.33\ $ & $\ 343.4\ $\rule[-3pt]{0pt}{10pt}\\
$\ -1.01\ $ & $\ 4120599\ $ & $\ 2\ $ & $\ 202\ $ & $\ 237.78\ $ & $\ 0.50\ $ & $\ 3.94\ $  & $\ 242.30\ $ & $\ 26.2\ $ & $\ 43891\ $ & $\ 29.91\ $ & $\ 1.61\ $  & $\ 31.52\ $ & $\ 642.5\ $\rule[-3pt]{0pt}{10pt}\\
$\ -1.005\ $ & $\ 64801599\ $ & $\ 2\ $ & $\ 402\ $ & $\ 3078.88\ $ & $\ 3.07\ $ & $\ 28.19\ $  & $\ 3110.50\ $ & $\ 48.0\ $ & $\ 563585\ $ & $\ 179.23\ $ & $\ 4.72\ $  & $\ 183.95\ $ & $\ 1629.2\ $\rule[-3pt]{0pt}{10pt}\\
$\ -1.004\ $ & $\ 63251499\ $ & $\ 2\ $ & $\ 502\ $ & $\ 7357.72\ $ & $\ 5.68\ $ & $\ 52.81\ $  & $\ 7416.77\ $ & $\ 60.5\ $ & $\ 264391\ $ & $\ 270.30\ $ & $\ 7.71\ $  & $\ 278.01\ $ & $\ 2544.0\ $\rule[-3pt]{0pt}{10pt}\\
$\ -1.003\ $ & $\ 450012211\ $ & $\ 2\ $ & $\ 670\ $ & $\ 23455.44\ $ & $\ 12.72\ $ & $\ 120.25\ $  & $\ 23589.49\ $ & $\ 93.6\ $ & $\ \ $ & $\ \ $ & $\ \ $  & $\ \ $ & $\ \memout\ $\rule[-3pt]{0pt}{10pt}\\
\hline
	\end{tabular}
	\normalsize
\end{table}

On this benchmark, $\sf{PRISM}$ is faster that $\sf{Acacia+}$ on large models, but $\sf{Acacia+}$ is more efficient regarding the memory consumption and this in spite of considering the whole state space. For instance, the last MDP of Table~\ref{table:LTL} contains more than $450$ millions of states and is solved by $\sf{Acacia+}$ in around $6.5$ hours with less than $100$Mo of memory, while for this example, $\sf{PRISM}$ runs out of memory. Note that the surprisingly large amount of memory consumption of both implementations on small instances is due to Python libraries loaded in memory for $\sf{Acacia+}$, and to the JVM and the CUDD package for $\sf{PRISM}$~\cite{DBLP:conf/hvc/JansenKOSZ07}.

To fairly compare the two implementations, let us consider Figure~\ref{fig:time} (resp. Figure~\ref{fig:memory}) that gives a graphical representation of the execution times (resp.  the memory consumption) of $\sf{Acacia+}$ and $\sf{PRISM}$ as a function of the number of states taken into account, that is, the total number of states for $\sf{Acacia+}$ and the number of reachable states for $\sf{PRISM}$. For that experiment, we consider the benchmark of examples of Table~\ref{table:LTL} with four different probability distributions on $\Sigma_I$. Moreover, for each instance, we consider the two MDPs obtained with the backward and the forward algorithms of $\sf{Acacia+}$ for solving safety games. The forward algorithm always leads to smaller MDPs. On the whole benchmark, $\sf{Acacia+}$ times out on three instances, while $\sf{PRISM}$ runs out of memory on four of them. Note that all scales in Figures~\ref{fig:time} and~\ref{fig:memory} are logarithmic.


\begin{figure}[!h]
   \begin{minipage}[t]{.48\linewidth}
	\includegraphics[width=\textwidth]{img/all_time.pdf}
	\caption{Execution time}
	\label{fig:time}   
\end{minipage} \hfill
   \begin{minipage}[t]{.48\linewidth}
	\includegraphics[width=\textwidth]{img/all_mem.pdf}
	\caption{Memory consumption}
	\label{fig:memory}  
 \end{minipage}
\end{figure}

On Figure~\ref{fig:time}, we observe that for most of the executions, $\sf{Acacia+}$ works faster that $\sf{PRISM}$. We also observe that  $\sf{Acacia+}$ does not behave well for a few particular executions, and that these executions all correspond to MDPs obtained from the forward algorithm of $\sf{Acacia+}$.

Figure~\ref{fig:memory} shows that regarding the memory consumption, $\sf{Acacia+}$ is  more efficient than $\sf{PRISM}$ and it can thus solve larger MDPs (the largest MDP solved by $\sf{PRISM}$ contains half a million states while $\sf{Acacia+}$ solves MDPs of more than $450$ million states). This points out that monotonic MDPs are better handled by pseudo-antichains, which exploit the partial order on the state space, than by BDDs.

\medskip
Finally, in the majority of experiments we performed for both the EMP and the SSP problems, we observe that most of the execution time of the pseudo-antichain based symblicit algorithms is spent for lumping. It is also the case for the MTBDD based symblicit algorithm~\cite{DBLP:conf/qest/WimmerBBHCHDT10}.
