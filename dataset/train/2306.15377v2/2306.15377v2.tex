\section{Experiments} \label{sec:experiments}
\vspace{-3mm}
\subsection{Experimental Details}
\vspace{-2mm}
\noindent \textbf{Architecture.} We adopt the STCN architecture \cite{cheng2021rethinking} with key changes; we i) fix the memory size to two frames (first and last) to avoid memory explosion, ii) replace key/value encoders with MobileNetV2 \cite{sandler2018mobilenetv2}, iii) reduce key dimension to 32, iv) remove ASPP and v) use mask as the only input to the value encoder \cite{miles2023mobilevos}. These changes make our architecture, denoted as the \textit{baseline} throughout the paper, less accurate than STCN, but improves its runtime performance (see Figure \ref{fig:overall_diagram}).

\noindent \textbf{Datasets.} DAVIS 2016 \cite{perazzi2016benchmark} is a single-object SVOS benchmark with 30 training and 20 validation videos, where DAVIS 2017 \cite{pont20172017} extends it to multi-object with 60 training videos, 30 test and 30 validation videos. YouTube-VOS 2019 \cite{xu2018youtube} is a large-scale multi-object SVOS benchmark with 3471 training videos formed of 65 categories and 507 videos for validation. The validation set includes 26 unseen object categories, allowing model assessment on unseen object categories. 

\noindent \textbf{Metrics.} We use the metrics region similarity  and contour accuracy , and their average . On YouTube, we also report seen and unseen class . We evaluate our methods on the validation sets of DAVIS and YouTube datasets.

\noindent \textbf{Training Details.}
We follow the training regime of \cite{cheng2021rethinking}, where we first pretrain on static images with synthetic deformations, and then train on YouTube and DAVIS 2017 jointly for 300K iterations with a batch size of 16. We use Adam optimizer with a learning rate of  and a weight decay of . The only difference is that we use a sequence length of 5 instead of 3. Trick 3 PT variant is trained separately, using bounding boxes extracted from the ground-truth masks. In PT training, we further penalize the predicted bounding boxes that are smaller than the ground-truth by increasing their loss weights. We use RTX 3090 GPUs for all experiments.


\vspace{-4mm}
\subsection{Ablation Studies}
\vspace{-2mm}
\noindent We conduct detailed ablation studies to show the effect of each TrickVOS component.
The results are shown in Table \ref{tab:ablation}. We use a single model checkpoint to evaluate on all datasets.

 \begin{table}[t!]
\resizebox{0.48\textwidth}{!}{\begin{tabular}{l|c|c|c}
Method & DAVIS'16 & DAVIS'17 & YouTube-VOS'19 \\ \hline
Baseline & 88.18 & 80.86 & 77.18 	\\
+  SSIM  & 87.81 \red{0.37} & 81.25 \cyan{0.39} & 78.42 \cyan{1.24}	\\
+  IoU  & 89.04 \cyan{0.86} & 81.88 \cyan{1.02} & 79.39 \cyan{2.21}\\
+  Trick 1 & 88.80 \cyan{0.62}& 81.97 \cyan{1.11}  & 79.42 \cyan{2.24}	\\ \hline
+ Trick 2 & 88.47 \cyan{0.25} & 81.06 \cyan{0.20} & 	78.07 \cyan{0.89}\\ \hline
+ Trick 3 (CV) & 88.30 \cyan{0.12} & 81.17 \cyan{0.31} & 78.04 \cyan{0.86}	\\ 
+ Trick 3 (PT) & 88.31 \cyan{0.13} & 81.23 \cyan{0.37} & 78.15 \cyan{0.97}	\\ \hline
+ Trick 1+2 & 89.19 \cyan{0.99} &  82.54 \cyan{1.68} & 79.94 \cyan{2.76}	\\
+ TrickVOS (CV) & 89.28 \cyan{1.10} &  82.56 \cyan{1.70} & 80.38 \cyan{3.20}	\\
+ TrickVOS (PT) & 89.29 \cyan{1.11} & 82.67 \cyan{1.81} & 80.47 \cyan{3.29}	\\

\end{tabular}}
\vspace{-4mm}
    \caption{Ablation study  values. CV and PT indicate constant velocity and parametric variants of our tracker (Trick 3). TrickVOS combines all tricks. \cyan{} and \red{} show  increase and decrease compared to the baseline, respectively. }
\vspace{1mm}
    \label{tab:ablation}
\end{table}


\begin{table*}[t!]
\resizebox{\textwidth}{!}{\begin{tabular}{lc|ccccccccccccc}
\multicolumn{2}{l|}{Method} & \multicolumn{3}{c|}{DAVIS'16} & \multicolumn{3}{c|}{DAVIS'17} & \multicolumn{5}{c|}{YouTube'19} & \multicolumn{2}{c}{FPS} \\ \hline
\multicolumn{1}{c}{} & \multicolumn{1}{c|}{CC} &  &  & \multicolumn{1}{c|}{} & &  & \multicolumn{1}{c|}{} &  &  &  &  & \multicolumn{1}{c|}{} & DAVIS'16 & DAVIS'17 \\ \hline
MiVOS\cite{cheng2021modular} &  & 91.0 & 89.7 & \multicolumn{1}{c|}{92.4} & 84.5 & 81.7 & \multicolumn{1}{c|}{87.4} & 82.4 & 80.6 & 84.7 & 78.2 & \multicolumn{1}{c|}{85.9} & 16.9 & 11.2 \\
STM \cite{oh2019video} &  & 89.3 & 88.7 & \multicolumn{1}{c|}{89.9} & 81.8 & 79.2 & \multicolumn{1}{c|}{84.3} & 79.2 & 79.6 & 83.6 & 73.0 & \multicolumn{1}{c|}{80.6} & 6.30 & 10.2 \\
STCN \cite{cheng2021rethinking} &  & 91.7 & 90.4 & \multicolumn{1}{c|}{93.0} & 85.3 & 82.0 & \multicolumn{1}{c|}{88.6} & 84.2 & 82.6 & 87.0 & 79.4 & \multicolumn{1}{c|}{87.7} & 26.9 & 20.2 \\
XMem \cite{cheng2022xmem} &   & 91.5 & 90.4 & \multicolumn{1}{c|}{92.7} & 86.2 & 82.9 & \multicolumn{1}{c|}{89.5} & 85.5 & 84.3 & 88.6 & 80.3 & \multicolumn{1}{c|}{88.6} & 29.6 & 22.6 \\ \hline
GCNet \cite{li2020fast} &  & 86.6 & 87.6 & \multicolumn{1}{c|}{85.7} & 71.4 & 69.3 & \multicolumn{1}{c|}{73.5} & 73.2 & 72.6 & 75.6 & 68.9 & \multicolumn{1}{c|}{75.7} & 25.0 & \textless{}25.0 \\
SwiftNet \cite{wang2021swiftnet} &  & 90.4 & 90.5 & \multicolumn{1}{c|}{90.3} & 81.1 & 78.3 & \multicolumn{1}{c|}{83.9} & 77.8 & 77.8 & 81.8 & 72.3 & \multicolumn{1}{c|}{79.5} & 25.0 & \textless{}25.0 \\
CFBI  \cite{yang2020collaborative} &  & 89.9 & 88.7 & \multicolumn{1}{c|}{91.1} & 81.9 & 79.3 & \multicolumn{1}{c|}{84.5} & 81.0 & 80.6 & 85.1 & 75.2 & \multicolumn{1}{c|}{83.0} & 5.90 & -- \\
RDE-VOS \cite{li2022recurrent} &  & 91.1 & 89.7 & \multicolumn{1}{c|}{92.5} & 84.2 & 80.8 & \multicolumn{1}{c|}{87.5} & 81.9 & 81.1 & 85.5 & 76.2 & \multicolumn{1}{c|}{84.8} & 35.0 & 27.0 \\ \hline
Baseline &  & 88.2 & 87.5 & \multicolumn{1}{c|}{88.8} & 80.9 & 77.5 & \multicolumn{1}{c|}{84.2} & 77.2 & 76.9 & 81.0 & 71.3 & \multicolumn{1}{c|}{79.4} & {102.1} & {99.5} \\
Baseline + \textbf{TrickVOS (CV)} &  & 89.3 & 88.7 & \multicolumn{1}{c|}{89.8} & 82.6 & 79.3 & \multicolumn{1}{c|}{85.9} & 80.3 & 80.0 & 83.9 & 74.5 & \multicolumn{1}{c|}{82.9} & 91.4  & 79.3 \\
Baseline + \textbf{TrickVOS (PT)} &  & 89.3 & 88.7 & \multicolumn{1}{c|}{89.9} & 82.7 &  79.4 & \multicolumn{1}{c|}{86.0} & 80.5 & 79.5 & 83.3 & 75.2 & \multicolumn{1}{c|}{84.0} & 86.4 & 76.4 \\ \hline
STCN  \cite{cheng2021rethinking} &  & 91.2 & 90.1 & \multicolumn{1}{c|}{92.3} & 84.0 & 80.6 & \multicolumn{1}{c|}{87.4} & 80.7 & 80.2 & 84.5 & 75.6 & \multicolumn{1}{c|}{82.7} & 62.4 & 48.3 \\
STCN  \cite{cheng2021rethinking} + \textbf{Trick 1+2} &  & 91.7 & 90.4 & \multicolumn{1}{c|}{92.9} & 85.4 & 82.1 & \multicolumn{1}{c|}{88.8} & 81.6 & 81.2 & 85.5 & 75.8 & \multicolumn{1}{c|}{83.9} & 62.4 & 48.3 \\
STCN  \cite{cheng2021rethinking} + \textbf{TrickVOS (PT)} &  & 91.8 & 90.5 & \multicolumn{1}{c|}{93.1} & 86.1 & 82.6 & \multicolumn{1}{c|}{89.6} & 82.8 & 82.1 & 86.4 & 77.2 & \multicolumn{1}{c|}{85.5} & 45.4 & 35.1 \\
\end{tabular}}
\vspace{-4mm}
    \caption{Results on the DAVIS 2016/2017 and YouTube-VOS 2019 validation sets. \textbf{CC} indicate constant cost. 
     indicates models trained with additional data (BL30K).  indicates STCN re-training by us (without ASPP module or BL30K pretraining).
    }
    \label{tab:sota_results}
    \vspace{-6mm}
\end{table*}


\noindent \textbf{Trick 1.} We add SSIM and IoU losses over the cross-entropy baseline, and then combine all of them. Table \ref{tab:ablation} third row shows that IoU increases the accuracy significantly. The second row shows that SSIM improves DAVIS'17 but hurts DAVIS'16. Note that we focus on DAVIS'17 as it is a larger dataset with multiple object annotations, which is more reminiscent of an actual SVOS use case. When we combine all (Table \ref{tab:ablation} 4th row), we see solid improvements; 0.62, 1.11  and 2.24  on DAVIS'16, 17 and YouTube, respectively.


\noindent \textbf{Trick 2.} Fifth row of Table \ref{tab:ablation} (Trick 2) shows that we get consistent improvements on all datasets. Note that this result shows that we do not overfit, especially to DAVIS since it is quite smaller compared to YouTube. Improvements on YouTube especially emphasize the benefit of Trick 2.


\noindent \textbf{Trick 3.} We observe improvements even with the constant velocity variant (Table \ref{tab:ablation} row 6, Trick 3 CV), which has little computational overhead. With the PT variant, we observe slightly better results compared to CV, but at the cost of a slightly increased performance overhead. Note that we do not explicitly optimize the tracker, therefore the performance overhead can be reduced with optimized implementations.




\noindent \textbf{All tricks.} An important question remains; can we simply use all tricks at the same time and get further improvement? The results show that Trick 1 and 2 combined perform better on all datasets, compared to using them separately. When all tricks are combined, we get the largest improvement on all datasets. 


\vspace{-5mm}
    
\subsection{Results}
\vspace{-2mm}
\noindent We compare TrickVOS against the state-of-the-art methods. We indicate methods with bounded memory during inference with Constant Cost (CC) \cite{li2022recurrent}. Unlike others \cite{li2022recurrent}, we use a single model checkpoint for all dataset evaluations. The results are shown in Table \ref{tab:sota_results}. Note that methods with unbounded memory are not practical, as they eventually run out of memory and their FPS decrease. We provide their results, but we mainly compare ours against other constant cost methods. 


\noindent \textbf{DAVIS 2016.} Our model performs quite competitively; it is slightly behind RDE-VOS, but it is 2.5 faster and has 32 fewer parameters (our 1.9M vs their 64M). Our methods comfortably perform in real-time, with minimal reductions (10\% and 15\% for CV and PT) in runtime compared to the baseline.



\noindent \textbf{DAVIS 2017.} In multi-object scenarios, the results look even better. We are the second best after RDE-VOS in constant cost methods. Also note that in multi-object scenarios, most of the methods perform significantly slower, but we are affected less than the others; we are now 3 faster than RDE-VOS. Our methods are still comfortably in real-time regime, with acceptable reductions in runtime compared to the baseline.  


\noindent \textbf{YouTube-VOS.} We observe the most significant improvements over our baseline on YouTube (3.3 ), which emphasizes the benefit of our tricks as YouTube is the largest and the most diverse of all datasets we experiment with. 

\noindent \textbf{On-device deployment.} TrickVOS unlocks a practical application; mobile deployment. To test on-device performance, we convert our models to tflite and benchmark them using the tflite benchmarking tool. Our method reaches 30+ FPS on a Samsung Galaxy S22 GPU; to the best of our knowledge, this is the first STM-based SVOS method that achieves real-time on a mobile device. Note that we are already faster than others on high-end GPUs (see Table \ref{tab:sota_results}); assuming a naive linear scaling of speed from desktop to mobile GPUs and based on our 30 FPS, no other method is likely to perform in real-time.


\noindent \textbf{Qualitative results.} Figure \ref{fig:qual} shows example images from DAVIS'17 validation set; the baseline fails to either track objects (gun in leftmost image) or mispredicts masks (erroneous predictions in the wheel and on the mountain). Our tricks, when applied, visibly improve such error cases.

\noindent \textbf{TrickVOS + other methods.} We apply TrickVOS to the original STCN (with no ASPP module or BL30K pretraining) to highlight its plug-and-play nature. For a fair comparison, we re-train it (using authors' code) with and without TrickVOS; the last three rows of Table \ref{tab:sota_results} show TrickVOS consistently improves the results on all datasets. 




\begin{figure}[!t]
  \centering
        \includegraphics[width=0.48\textwidth]{figures/figure_qual.pdf}
  \vspace{-7mm}
  \caption{Rows show (topbottom) the ground-truth, baseline, baseline + trick 1, baseline + trick 2 and baseline + TrickVOS predictions. Note the red boxes/arrows showing the erroneous regions, which are gradually improved with our tricks. } 
  \vspace{1mm}
  \label{fig:qual}
\end{figure}


\vspace{-5mm}



