



\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}



\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}





\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{hyperref}

\usepackage{xspace}
\usepackage{amsfonts,amstext,amsmath,amssymb}
\usepackage{multirow}
\usepackage{array,url}
\usepackage{color}



\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\fb}{{\sc Freebase}\xspace}
\newcommand{\dbp}{{\sc DBPedia}\xspace}
\newcommand{\wk}{{\sc WikiAnswers}\xspace}
\newcommand{\wq}{{\sc WebQuestions}\xspace}
\newcommand{\rv}{{\sc ReVerb}\xspace}
\newcommand{\cw}{{\sc ClueWeb}\xspace}



\newcommand{\us}{Our approach}

\newcommand{\ent}[1]{{\small {\tt #1}}} 


\begin{document}

\mainmatter  

\title{Question Answering with Subgraph Embeddings }

\titlerunning{Question Answering with Subgraph Embeddings}



\author{Antoine Bordes
\and Jason Weston \and Sumit Chopra}


\institute{Facebook, 770 Broadway, New York, NY, USA \\
\path|{{abordes,jase,spchopra}@fb.com}|
}



\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle




\begin{abstract}
This paper presents a system which learns to answer questions on a
broad range of topics from a knowledge base using few hand-crafted features.
Our model learns low-dimensional embeddings of words and
knowledge base constituents; these representations
are used to score natural language questions against candidate answers.
Training our system using pairs of questions and structured
representations of their answers, and pairs of
question paraphrases, yields competitive results on a
recent benchmark of the literature.
\end{abstract}



\section{Introduction}


Teaching machines how to automatically answer questions asked in natural
language on any topic or in any domain has always been a long standing
goal in Artificial Intelligence.
With the rise of large scale structured knowledge bases (KBs), this
problem, known as open-domain question answering (or open QA), 
boils down to being able to
query efficiently such databases with natural language.
These KBs, such as \fb \cite{bollacker2008freebase} 
encompass huge ever growing amounts of information
and
ease open QA by organizing a great variety of answers in a structured
format.
However, the scale and the difficulty for machines to
interpret natural language still makes this task a challenging problem.

The state-of-the-art techniques in open QA can be classified 
into two main classes, namely, information retrieval based and 
semantic parsing based. 
Information retrieval systems first retrieve a broad set
of candidate answers by querying the search API of KBs with a
transformation of the question into a valid query and then use
fine-grained detection heuristics to identify the exact answer
\cite{kolomiyets2011survey,unger2012template,yao2014information}.
On the other hand, semantic parsing methods focus on the correct
interpretation of the meaning of a question by a semantic parsing
system. A correct interpretation converts a question
into the exact database query that returns the correct answer.
Interestingly, recent works
\cite{berant-EtAl:2013:EMNLP,kwiatkowski-EtAl:2013:EMNLP,berant2014semantic,fader2014open}
have shown that such systems can be efficiently trained under
indirect and imperfect supervision and hence scale to large-scale
regimes, while bypassing most of the annotation costs.

Yet, even if both kinds of system have shown the ability to handle
large-scale KBs, they still require experts to hand-craft lexicons,
grammars, and KB schema to be effective. 
This non-negligible human
intervention might not be generic enough to conveniently scale up to
new databases with other schema, broader vocabularies or 
languages other than English. In contrast, \cite{paralex} proposed a framework for open QA requiring
almost no human annotation.
Despite being an interesting approach, this method is outperformed by other
competing methods.
\cite{bordes2014open}  introduced an embedding model, which
learns low-dimensional vector representations of words and 
symbols
(such as KBs constituents) and can be trained
with even less supervision than the system of \cite{paralex} while
being able to achieve better prediction performance.
\if0
Embedding methods are based on learning of low-dimensional vector
representations of words and symbols (such as KBS constituents); they
have now reached near state-of-the-art performance on many standard
tasks while usually requiring less hand-crafted features
\cite{collobert:2011b,socher2013recursive}, and have already been used
to perform a connection between natural language and KBs for
word-sense disambiguation and information extraction
\cite{bordes:12aistats,weston-EtAl:2013:EMNLP}.
The work of \cite{bordes2014open} was a continuation of this trend for
open QA, but 
\fi
However, this approach is only compared with \cite{paralex} which
operates in a simplified setting and has not been applied in 
more realistic conditions nor evaluated against the best performing
methods.

In this paper, we improve the model of \cite{bordes2014open} by
providing the ability to answer more complicated questions. 
sThe main contributions of the paper are: (1) a more sophisticated 
inference procedure that is both efficient and can consider longer paths (\cite{bordes2014open} considered only answers directly connected to the question in the graph); and (2) a richer
representation of the answers which encodes the question-answer path and surrounding
subgraph of the KB. 
Our approach is competitive with the current state-of-the-art 
on the recent benchmark \wq
\cite{berant-EtAl:2013:EMNLP} without using any lexicon, rules or additional 
system for part-of-speech tagging, syntactic or dependency parsing
during training as most other systems do.






\section{Task Definition} \label{data}


Our main motivation is to provide a system for open QA able to be
trained as long as it has access to: (1) a training set of questions
paired with answers and (2) a KB providing a structure among answers.
We suppose that all potential answers are entities in the KB and that
questions are sequences of words that include one identified KB
entity. When this entity is not given, plain string matching is used to
perform entity resolution. Smarter methods could be used but
this is not our focus.


We use \wq \cite{berant-EtAl:2013:EMNLP} as our evaluation
bemchmark.
Since it contains few training samples, it is impossible to learn on
it alone, and this section describes the various data sources that
were used for training. These are similar to those used in
\cite{berant2014semantic}.


\paragraph{WebQuestions}
This dataset is built using \fb as the KB and contains 5,810
question-answer pairs.  It was created by crawling questions through
the Google Suggest API, and then obtaining answers using Amazon
Mechanical Turk.  \if0 It groups questions frequently asked on the
web, such as ``Who is Shakira married to?'' or ``What to see near
Grand Canyon?''.  \fi We used the original split (3,778 examples for
training and 2,032 for testing), and isolated 1k questions from the
training set for validation.
\wq is built on \fb since all answers are defined as \fb entities.
In each question, we identified one \fb entity using string matching between words
of the question and entity names in \fb.
When the same string matches multiple entities, only the entity
appearing in most triples, i.e. the most popular in \fb, was kept.
\if 0 
This process allowed to retrieve an entity in 95\% of the
questions; the remaining 5\% were discarded (for test questions, we
considered them as wrongly answered).  All valid questions were
finally modified by replacing any matched string with the \ent{mid} of
its corresponding entity.  
\fi 
Example questions (answers) in the dataset include 
{\em ``Where did Edgar  Allan Poe died?''} (\ent{baltimore}) or  {\em ``What degrees did
  Barack Obama get?''} (\ent{bachelor\_of\_arts}, \ent{juris\_doctor}).



\paragraph{Freebase}
\fb \cite{bollacker2008freebase} is a huge and freely available
database of general facts; data is organized as triplets
(\ent{subject}, \ent{type1.type2.predicate}, \ent{object}), where two
entities \ent{subject} and \ent{object} (identified by \ent{mids}) are
connected by the relation type \ent{type1.type2.predicate}.
\if0 , such as (\ent{Barack\_Obama},
\ent{people.person.nationality}, \ent{United\_States}): entities like
\ent{Barack\_Obama} or \ent{United\_States} are identified in \fb by
\ent{mids} like \ent{m.02mjmr} but, in this paper, we replace them by
their names for clarity. There are currently around 1.2 billion
triples and more than 80 million entities.  
\fi
We used a subset, created by only keeping
triples where one of the entities was appearing in either the \wq
training/validation set or in  \cw extractions.
We also removed all entities appearing less than 5 times and finally
obtained a \fb set containing 14M triples made of 2.2M entities and 7k
relation types.\footnote{\wq contains 2k entities, hence
  restricting \fb to 2.2M entities does not ease the task for us.}
Since the format of triples does not correspond to any structure one could find in
language, we decided to transform them into automatically generated
questions.  Hence, all triples were converted into questions ``What is
the \ent{predicate} of the \ent{type2} \ent{subject}?'' (using the
\ent{mid} of the subject) with the answer being \ent{object}.  
An example is {\em ``What is the \ent{nationality} of
the \ent{person} \ent{barack\_obama}?''} (\ent{united\_states}). 
\if 0
We used the type of the
entities in \fb to choose a suitable interrogative pronoun. This
process allows to convert all triples to a form whose structure is
much closer from what actual questions could be.  \fi
More examples and details are given in a longer version of this paper  \cite{DBLP:journals/corr/BordesCW14}.





\begin{table}[t!]
\begin{center}
\begin{small}
\begin{tabular}{|ll|r|}
\hline 
\wq & -- Train. ex. & 2,778\\
& -- Valid. ex. & 1,000\\
& -- Test. ex. & 2,032\\
\fb & -- Train. ex. & 14,790,259 \\
\cw & -- Train. ex. & 2,169,033 \\
\wk & -- Train. quest. & 2,423,185 \\
 & -- Parap. clust. & 349,957 \\
\hline 
Dictionary & -- Words & 1,526,768\\
& -- Entities &  2,154,345 \\
& -- Rel. types & 7,210 \\ 
\hline 
\end{tabular}
\end{small}
\caption{\label{tab:stats} Statistics of data sets used in the paper.}
\end{center}
\end{table}



\paragraph{ClueWeb Extractions}

\fb data allows to train our model on 14M questions but these have a
fixed lexicon and vocabulary, which is not realistic.
Following  \cite{berant-EtAl:2013:EMNLP}, we also created questions
using \cw extractions provided by \cite{lin2012entity}.
\if0
Millions of text triples such as (``Obama'', ``was allegedly bear
in'', ``Hawaii'') were extracted from ClueWeb09 using the ReVerb
system \cite{ReVerb2011}. A subset of these triples was released
by \cite{lin2012entity} with the subject linked to a \fb entity.
Using string matching, we heuristically attempted to connect objects
of those triples to \fb. 
\fi
Using string matching, we ended up with 2M extractions structured as
(\ent{subject}, ``text string'', \ent{object}) with both \ent{subject} and
\ent{object} linked to \fb.
We also converted these triples into questions by using simple patterns and
\fb types. 
An example of generated question is {\em ``Where
\ent{barack\_obama} was allegedly bear in?''} (\ent{hawaii}).
\if0  
For supervising our system with the path to the answer and not the
answer alone (see details in next section), we also tried to match the
verb extraction with relation types by searching for paths (up to
length 2) connecting subjects with objects in the \fb graph.
When no match was found, the relation type was identified as
\ent{unknown\_relation}.
\fi 

\paragraph{Paraphrases}

The automatically generated questions that are useful to connect \fb
triples and natural language, do not provide a satisfactory
modeling of natural language because of their semi-automatic wording
and rigid syntax.
To overcome this issue, we follow \cite{paralex} and supplement our training data with an
indirect supervision signal made of pairs of question paraphrases
collected from the \wk website.
On \wk, users can tag pairs of questions as rephrasings of each other:
\cite{paralex} harvested a set of 2M distinct questions from
\wk, which were grouped into 350k paraphrase clusters.
\if0
During training, we used pairs of questions belonging to the same
clusters as examples of valid paraphrases.
\fi





\begin{table*}[t!]
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|}
\hline 
& what is the judicial capital of the in state \ent{sikkim} ? -- \ent{gangtok}\\
& (\ent{sikkim}, \ent{location.in\_state.judicial\_capital},  \ent{gangtok}) \\ 
\cline{2-2}
&   who influenced the influence node \ent{yves\_saint\_laurent} ? -- \ent{helmut\_newton}\\
&  (\ent{yves\_saint\_laurent}, \ent{influence.influence\_node.influenced}, \ent{helmut\_newton})  \\
\cline{2-2}
\fb &   who is born in the location \ent{brighouse} ? -- \ent{edward\_barber} \\
{\it generated questions} & (\ent{brighouse}, \ent{location.location.people\_born\_here}, \ent{edward\_barber})  \\
\cline{2-2}
{\it and associated triples} & who is the producer of the recording
\ent{rhapsody\_in\_b\_minor,\_op.\_79,\_no.\_1} ? -- \ent{glenn\_gould}\\
& (\ent{rhapsody\_in\_b\_minor,\_op.\_79,\_no.\_1},
\ent{music.recording.producer}, \ent{glenn\_gould})  \\
\cline{2-2}
&   what are the symptoms of the disease \ent{sepsis} ? -- \ent{skin\_discoloration}\\
& (\ent{sepsis}, \ent{medicine.disease.symptoms}, \ent{skin\_discoloration}) \\
\hline 
\hline 
& what is \ent{cher}'s son's name ? -- \ent{elijah\_blue\_allman}\\
& (\ent{cher}, \ent{people.person.children},
\ent{elijah\_blue\_allman})\\
\cline{2-2}
& what are dollars called in \ent{spain} ? -- \ent{peseta} \\
& (\ent{spain}, \ent{location.country.currency\_formerly\_used}, \ent{peseta}) \\        
\cline{2-2}
\wq & what is \ent{henry\_clay} known for ? -- \ent{lawyer} \\
{\it training questions} & (\ent{henry\_clay}, \ent{people.person.profession}, \ent{lawyer}) \\
\cline{2-2}
{\it and associated paths} & who is the president of the
\ent{european\_union} 2011 ? -- \ent{jerzy\_buzek}\\
& (\ent{european\_union}, \ent{government.governmental\_jurisdiction.governing\_officials}\\
&\ent{government.government\_position\_held.office\_holder},  \ent{jerzy\_buzek}) \\ 
\cline{2-2}
& what 6 states border \ent{south\_dakota} ? -- \ent{iowa}\\
& (\ent{south\_dakota}, \ent{location.location.contains}
\ent{location.location.partially\_containedby},  \ent{iowa}) \\
\hline
\hline
& what does \ent{acetazolamide} be an inhibitor of ? -- \ent{carbonic\_anhydrase}\\
& (\ent{acetazolamide}, \ent{medicine.drug\_ingredient.active\_moiety\_of\_drug}, \ent{carbonic\_anhydrase})\\
\cline{2-2}
& which place is a district in \ent{andhra\_pradesh} ? -- \ent{adilabad}\\
& (\ent{andhra\_pradesh}, \ent{location.location.contains}, \ent{adilabad}) \\
\cline{2-2}
\cw & what is a regional airline based in \ent{dublin} ? -- \ent{aer\_arann} \\
{\it generated questions} & (\ent{dublin}, \ent{location.location.nearby\_airports} \ent{aviation.airport.focus\_city\_for},  \ent{aer\_arann})\\
\cline{2-2}
{\it and associated paths} & what mean fire in \ent{sanskrit} ? -- \ent{agni}\\
& (\ent{sanskrit}, \ent{unknown\_relation}, \ent{agni}) \\
\cline{2-2}
& where does \ent{american\_legion} proceed to ? -- \ent{san\_francisco}\\  
& (\ent{american\_legion}, \ent{22-rdf-syntax-ns\#type} \ent{type.type.instance}, \ent{san\_francisco})  \\
\hline 
\hline 
& what are two reason to get a 404 ? \\
& what is error 404 ? \\
& you receive a 404 - unable to find error message ?\\
& how do you correct error 404 ?\\
\cline{2-2}
\wk & what is the term for a teacher of islamic law ?\\
{\it clusters of } & what is the islamic religious teacher called ?\\
{\it quest. paraphrases } & what is the name of the religious book islam use ?\\
& who is chief of islamic religious authority ?\\
\cline{2-2}
& what  country is bueno aire in ?\\
& what countrie is buenos aires in ?\\
& what country is buenas aire in ?\\
&  what country is bueno are in ?\\
\hline 
\end{tabular}
}
\caption{\label{tab:ex} Examples of questions, answer paths and paraphrases  used in this
paper.}
\end{center}
\end{table*}


\section{Embedding Questions and Answers} \label{model} 


Inspired
by~\cite{bordes2014open}, our model works by learning low-dimensional
vector embeddings of words appearing in questions and of
entities and relation types of \fb, so that representations of 
questions and of their corresponding answers are close to each other
in the joint embedding space. 
Let  denote a question and  a candidate answer.  
Learning embeddings is achieved by learning a scoring function ,  
so that  generates a high score if  is the correct 
answer to the question , and a low score otherwise. Note that both  and  
are represented as a combination of the embeddings of their individual words and/or 
symbols; hence, learning  essentially involves learning these 
embeddings. In our model, the form of the scoring function is: 

Let  be a matrix of , 
where  is the dimension of the embedding space which is fixed a-priori,
and  is the dictionary of embeddings to be learned.
Let  denote the total number of words
and  the total number of entities and relation types.
With , the
 -th column of  is the embedding of the -th 
element (word, entity or relation type) in the dictionary. 
The function , which maps the questions into the embedding 
space  is defined as , where 
, is a sparse vector indicating the 
number of times each word appears in the question  (usually 0 or 1). 
Likewise the function  which maps the answer 
into the same embedding space  as the questions, 
is given by . 
Here  is a sparse vector representation 
of the answer , which we now detail. 

\begin{figure*}
\begin{center}
\includegraphics[width=12cm]{new-fig.pdf}
\caption{\label{fig:subgraph} Illustration of the subgraph embedding
  model scoring a candidate answer: (i) locate entity in the question;
  (ii) compute path from entity to answer; (iii) represent answer as
  path plus all connected entities to the answer (the subgraph); (iv)
  embed both the question and the answer subgraph separately using the
  learnt embedding vectors, and score the match via their dot
  product.}
\end{center}
\end{figure*}

\subsection{Representing Candidate Answers} \label{ans-features}

We now describe possible feature representations for a single
candidate answer. (When there are multiple correct answers, we average these 
representations, see Section \ref{multiple-ans}.)
We consider three different types of representation,
corresponding to different subgraphs of \fb around
it.
\begin{itemize}
\item[(i)] {\it Single Entity}. The answer is represented 
as a single entity from \fb:
 is a 1-of- coded vector with 1 corresponding to the 
entity of the answer, and 0 elsewhere. 
\item[(ii)] {\it Path Representation}. The answer is represented as a
  path from the entity mentioned in the question to the answer
  entity.  In our experiments, we considered 1- or 2-hops paths
  (i.e. with either 1 or 2 edges to traverse):
(\ent{barack\_obama},
\ent{people.}\ent{person.}\ent{place\_of\_birth},~\ent{honolulu}) is a 1-hop path
and  (\ent{barack\_obama},
\ent{people.person.place\_of\_birth},~\ent{location.} \ent{location.containedby}, \ent{hawaii}) a 2-hops path.
This results in a  which is a 3-of- or 4-of- coded
  vector, expressing the start and end entities of the path and the
  relation types (but not entities) in-between.
\item[(iii)] {\it Subgraph Representation}. We encode both the path
  representation from (ii), and the entire subgraph of entities
  connected to the candidate answer entity. That is, for each entity
  connected to the answer we include both the relation type and the
  entity itself in the representation . In order to represent
  the answer path differently to the surrounding subgraph (so the
  model can differentiate them), we double the dictionary size for
  entities, and use one embedding representation if they are in the
  path and another if they are in the subgraph.  Thus we now learn a
  parameter matrix  where  ( is the total number of entities and relation
  types). If there are  connected entities with  relation types
  to the candidate answer, its representation is a  or
  -of- coded vector, depending on the path length.


\end{itemize}

\label{sec:hypo}

Our hypothesis is that including more information about the answer in
its representation will lead to improved results. While it is possible
that all required information could be encoded in the  dimensional
embedding of the single entity {\it (i)}, it is unclear what dimension
 should be to make this possible. For example the embedding
of a country entity encoding all of its citizens seems unrealistic.
Similarly, only having access to the path ignores all the other
information we have about the answer entity, unless it is encoded in
the embeddings of either the entity of the question, the answer or the
relations linking them, which might be quite complicated
as well.  We thus adopt the subgraph approach.
Figure~\ref{fig:subgraph} illustrates our model. 





\subsection{Training and Loss Function}
As in~\cite{wsabie}, we train our model using a margin-based
ranking loss function.  Let  be the training set of 
questions  paired with their correct answer . The loss
function we minimize is

where  is the margin (fixed to ).  Minimizing
Eq. (\ref{eq:loss}) learns the embedding matrix  so that the score
of a question paired with a correct answer is greater than with any
incorrect answer  by at least . 
 is sampled from a set of incorrect candidates .  
This is achieved by sampling 50\% of the
time from the set of entities connected to the entity of the question
(i.e. other candidate paths), and by replacing the
answer entity by a random one otherwise.
\if0 Our motivation for the latter is that generalization should be
achieved by the model knowing when an entity is completely out of
place in a given (otherwise correct) path.  \fi
Optimization is accomplished using stochastic gradient descent,
multi-threaded with Hogwild!  \cite{recht2011hogwild}, with the 
constraint that the columns  of  remain within the unit-ball, 
i.e., .


\subsection{Multitask Training of Embeddings}
Since a large number of questions in our training datasets are
synthetically generated, they do not adequately
cover the range of syntax used in natural language.
Hence, we also multi-task the training of our model 
with the task of paraphrase prediction. 
We do so by alternating the training of  with that of a scoring
function , which uses the same
embedding matrix  and makes the embeddings of a pair of questions
 similar to each other if they are paraphrases (i.e. if
they belong to the same paraphrase cluster), and make them different
otherwise. 
Training  is similar to that of  except that negative samples are 
obtained by sampling a question from another paraphrase cluster.



We also multitask the training of the embeddings with the
mapping of the \ent{mids} of \fb entities to the actual words of their
names, so that the model learns that the embedding
of the \ent{mid} of an entity should be similar to the embedding of the
word(s) that compose its name(s).




\subsection{Inference} \label{inference} \label{multiple-ans}
Once  is trained, at test time, for a given
question  the model predicts the answer with:

where  is the candidate answer set.
This candidate set could be the whole KB but this has both speed and
potentially precision issues. Instead, we create a candidate set 
 for each question. 

We recall that each question contains
one identified \fb entity.
 is first populated with all triples from \fb
involving this entity. This allows to answer simple factual questions
whose answers are directly connected to them (i.e. 1-hop paths). This
strategy is denoted .
\if0 (in \wq, around 55\% of questions are of such kind). Hence, w \fi


Since a system able to answer only such questions would be limited, we
supplement  with examples situated in the KB graph at
2-hops from the entity of the question. We do not add all such
quadruplets since this would lead to very large candidate sets. Instead,
we consider the following general approach: given that we are
predicting a path, we can predict its elements in turn using a beam
search, and hence avoid scoring all candidates.
Specifically, our model first ranks relation types using
Eq. (\ref{rank-eq}), i.e.  selects which relation types are the most
likely to be expressed in . We keep the top  types ( was
selected on the validation set) and only add 2-hops candidates to
 when these relations appear in their path. Scores of
1-hop triples are weighted by  since they have one less
element than 2-hops quadruplets.  This strategy, denoted , is used by
default.


A prediction  can commonly actually be a set of candidate answers,
not just one answer, for example for questions like {\em ``Who are David
Beckham's children?''}. This is achieved by considering a prediction to
be all the entities that lie on the same 1-hop or 2-hops path from the
entity found in the question. 
Hence, all answers to the above question are connected to
\ent{david\_beckham} via the same path (\ent{david\_beckham},
\ent{people.person.children}, \ent{*}).
The feature representation of the prediction is then the average over
each candidate entity's features (see Section \ref{ans-features}),
i.e. 
where  are the individual entities in the overall prediction
. In the results, we compare to a baseline method that can only
predict single candidates, which understandly performs poorly.



\section{Experiments} 


\begin{table}[t!]
\begin{center}
\begin{tabular}{|@{\,\,}l@{\,}|c|@{}c@{}|c|}
\hline
Method & P@1 & F1 & F1\\
& (\%) & (Berant) & (Yao)\\
\hline
{\bf Baselines} &&&\\
(Berant et al., 2013) \cite{berant-EtAl:2013:EMNLP} & -- & 31.4 & -- \\
(Bordes et al., 2014) \cite{bordes2014open} &  31.3 & 29.7 & 31.8 \\
(Yao and Van Durme, 2014)  \cite{yao2014information} & -- & 33.0 & 42.0\\
(Berant and Liang, 2014) \cite{berant2014semantic} & -- & 39.9 & 43.0\\
\hline
{\bf \us} & & & \\
Subgraph \&   & \bf 40.4 & 39.2 & 43.2\\
Ensemble with (Berant \& Liang, 14) & -- &\bf 41.8 &\bf 45.7\\
\hline
{\bf Variants} & & & \\
Without multiple predictions  &\bf  40.4 & 31.3 & 34.2 \\
Subgraph \&  & 38.0 & 37.1 & 41.4\\
Subgraph \&   & 34.0 & 32.6 & 35.1\\
Path  \&  & 36.2 & 35.3 & 38.5\\
Single Entity  \&  & 25.8 & 16.0 & 17.8\\
\hline
\end{tabular}
\caption{\label{tab:res} Results on the \wq test set.}
\end{center}
\end{table}







We compare our system in terms of F1 score as computed by the official
evaluation script\footnote{Available from
  www-nlp.stanford.edu/software/sempre/} (F1 (Berant)) but also with a
slightly different F1 definition, termed F1 (Yao) which was used in
\cite{yao2014information} (the difference being the way that questions
with no answers are dealt with), and precision @ 1 (p@1) of the first
candidate entity (even when there are a set of correct answers),
comparing to recently published systems.\footnote{Results of baselines
  except \cite{bordes2014open} have been extracted from the original
  papers. For our experiments, all hyperparameters have been selected
  on the \wq validation set:  was chosen among ,
  the learning rate on a log. scale between  and 
  and we used at most  paths in the subgraph representation.}
The upper part of Table~\ref{tab:res} indicates that our approach
outperforms \cite{yao2014information},
\cite{berant-EtAl:2013:EMNLP} and \cite{bordes2014open}, and performs
similarly as \cite{berant2014semantic}.

The lower part of Table~\ref{tab:res} compares various versions of our
model. Our default approach uses the Subgraph representation for
answers and  as the candidate answers set.
Replacing  by  induces a large drop in performance because
many questions do not have answers thatare directly connected to their
inluded entity (not in ). However, using all 2-hops connections
as a candidate set is also detrimental, because the larger number of
candidates confuses (and slows a lot) our ranking based inference.
Our results also verify our hypothesis of Section~\ref{sec:hypo},
that a richer representation for answers (using the local subgraph)
can store more pertinent information.
Finally, we demonstrate that we greatly improve upon the model of
\cite{bordes2014open}, which actually corresponds to a setting with
the Path representation and  as candidate set.

We also considered an ensemble of our approach and that of
\cite{berant2014semantic}.  As we only had access to their test
predictions we used the following combination method. Our approach
gives a score  for the answer it predicts. We chose a
threshold such that our approach predicts 50\% of the time (when
 is above its value), and the other 50\% of the time we
use the prediction of \cite{berant2014semantic} instead. 
We aimed for a 50/50 ratio because both methods perform similarly.
The ensemble improves the state-of-the-art, and indicates that
our models are significantly different in their design.

\section{Conclusion}
This paper presented an embedding model that learns to perform open QA
using training data made of questions paired with their answers and of
a KB to provide a structure among answers, and can achieve
promising performance on the competitive benchmark \wq.


\bibliography{../qawemb.bib}
\bibliographystyle{abbrv}


\end{document}
