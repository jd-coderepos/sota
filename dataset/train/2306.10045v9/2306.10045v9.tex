

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2023}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{amsmath}
\usepackage{amssymb,enumitem}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{wrapfig}
\usepackage{graphics}
\usepackage{dsfont}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}
\urlstyle{same}

\icmltitlerunning{Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction}

\begin{document}

\twocolumn[
\icmltitle{Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yuchao Lin}{tamu}
\icmlauthor{Keqiang Yan}{tamu}
\icmlauthor{Youzhi Luo}{tamu}
\icmlauthor{Yi Liu}{fsu}
\icmlauthor{Xiaoning Qian}{tamu,tamuee}
\icmlauthor{Shuiwang Ji}{tamu}
\end{icmlauthorlist}

\icmlaffiliation{tamu}{Department of Computer Science \& Engineering, Texas A\&M University, College Station, TX, USA}
\icmlaffiliation{tamuee}{Department of Electrical \& Computer Engineering, Texas A\&M University, College Station, TX, USA}

\icmlaffiliation{fsu}{Department of Computer Science, Florida State University, Tallahassee, FL, USA}

\icmlcorrespondingauthor{Shuiwang Ji}{sji@tamu.edu}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations, where we extend the Ewald summation for several potential series approximations with provable error bounds. Finally, we propose to incorporate our computations of complete interatomic potentials into message passing neural networks for representation learning. We perform experiments on the JARVIS and Materials Project benchmarks for evaluation. Results show that the use of interatomic potentials and complete interatomic potentials leads to consistent performance improvements with reasonable computational costs. Our code is publicly available as part of the AIRS library (\url{https://github.com/divelab/AIRS/tree/main/OpenMat/PotNet}).
\end{abstract}

\section{Introduction}

The past decade has witnessed a surge of interests and rapid developments in machine learning for molecular analysis~\citep{duvenaud2015convolutional}. These initial studies mainly focus on the prediction and generation problems of small molecules. To enable computational analyses, molecules need to be featurized in an appropriate mathematical representation form. 
Recently, with the advances of graph neural networks (GNNs)~\citep{gilmer2017neural,battaglia2018relational,gao2021topology,Liu:Protein}, molecules are more commonly represented as graphs in which each node corresponds to an atom, and each edge corresponds to a chemical bond~\citep{stokes2020deep,wang2020advanced}. 
A variety of molecular graph prediction~\citep{stokes2020deep,wang2020advanced} and generation~\citep{shi2019graphaf,jin2018junction,luo2021graphdf} methods have been developed based on 2D molecular graph representations. A key limitation of the 2D graph representations is that the 3D geometries of molecules are not captured, but such information may be critical in many molecular property prediction problems~\citep{hu2021ogb}. To enable the encoding of 3D molecular geometries in GNNs, a series of 3D GNN methods have been developed for prediction~\citep{schutt2017schnet,gasteiger2019directional,liu2022spherical,wang2022comenet} and generation~\citep{liu2022generating,luo2021autoregressive,hoogeboom2022equivariant} problems.
In these 3D graph representations, each node is associated with the corresponding atom's coordinate in 3D space. Geometric information, such as distances between nodes and angles between edges, is used during message passing in GNNs. 
Recently, these methods have been extended to learn representations for proteins~\citep{jing2020learning,wang2023learning}.

Inspired by the success of GNNs on small molecules, 
\cite{xie2018crystal} developed the crystal graph convolutional neural network~(CGCNN) for crystal material property prediction. Different from small molecules and proteins, crystal materials are typically modeled by a minimal unit cell (similar to a small molecule) that is repeated in 3D space with certain directions and step sizes. In theory, the unit cell is repeated infinitely in 3D space, but any real-world material has finite size. However, given that our modeling is at the atomic level, modeling crystal materials as infinite repetitions of unit cells is approximately accurate. Therefore, a key challenge in crystal material modeling is how to accurately capture the infinite-range interatomic interactions resulted from the repetitions of unit cells in 3D space. Current GNN-based crystal property prediction methods construct graphs by creating edges only between atoms within a pre-specified distance threshold~\citep{xie2018crystal,MegNet,GATGNN,CyAtt,ALIGNN}. Thus, they fail to capture interactions between distant atoms explicitly. 


In this work, we propose a new graph deep learning method, \textbf{PotNet}, with several innovations to significantly advance the field of crystal material modeling. First, we propose to model interatomic potentials directly as edge features in PotNet, instead of using distance as in prior methods. These potentials include the Coulomb potential~\citep{west}, London dispersion potential~\citep{london}, and Pauli repulsion potential~\citep{pauli}. Second, a distinguishing feature of PotNet is to model the \textbf{complete} set of potentials among all atoms, instead of only between nearby atoms as in prior methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop efficient algorithms to compute the approximations. Finally, we propose to incorporate our computations of interatomic potentials and complete interatomic potentials into message passing neural networks for representation learning. We performed comprehensive experiments on the JARVIS and Materials Project benchmarks to evaluate our methods. Results show that the use of interatomic potentials and complete interatomic potentials in our methods leads to consistent performance improvements with reasonable computational costs.



\section{Background and Related Work}
\label{background}


\subsection{Crystal Representation and Property Prediction}
\label{crystal}

A crystal structure can be represented as periodic repetitions of unit cells in the three-dimensional~(3D) Euclidean space, where the unit cell contains the smallest repeatable structure of a given crystal. Specifically, let  be the number of atoms in the unit cell, a crystal can be represented as . Here,  describes one of the unit cell structures of , where  and  denote the -dimensional feature vector and the 3D Cartesian coordinates of the -th atom in the unit cell, respectively.  is the lattice matrix describing how a unit cell repeats itself in the 3D space. In the complete crystal structure, every atom in a unit cell repeats itself periodically in the 3D space. Specifically, from an arbitrary integer vector  and the unit cell structure , we can always obtain another repeated unit cell structure , where , . Hence, the complete crystal structure  of  with all unit cells can be described as 

In this work, we study the problem of crystal property prediction. Our objective is to learn a property prediction model  that can predict the property  of the given crystal structure . We will focus on predicting the total energy, or other energy-related properties of crystals.

\subsection{Crystal Property Prediction with Interatomic Potentials}
\label{background:potentail}

Most of the classical crystal energy prediction methods are based on interatomic potentials. According to existing studies in physics~\citep{west,eam,born}, the total energy of a crystal structure can be approximated by the summation of interatomic potentials in the crystal. Particularly, the three following categories of interatomic potentials are widely used in crystals, and they can be considered sufficient for accurate energy approximation.

\begin{itemize}[leftmargin=*]

\item \textbf{Coulomb potential} is caused by the electrostatic interaction of two atoms with charges. Coulomb potentials are closely related to ionic bonding and metallic bonding in crystals~\citep{west}. For any two atoms  and , let  and  denote the number of charges in the atom  and , and let  be the Euclidean distance between the atom  and . The Coulomb potential  is defined as . Here  is the elementary charge constant, and  is the permittivity constant of free space.

\item \textbf{London dispersion potential} describes the Van der Waals interaction between atoms. It is often considered in energy estimation since its contribution is cumulative over the volume of crystals~\citep{london} and can be sometimes very strong in bulk crystals, such as sulfur and phosphorus. The mathematical form of this potential is described as , where  is a hyperparameter.

\item \textbf{Pauli repulsion potential} results from the Pauli exclusion principle that generally exists in all crystal structures. The Pauli exclusion principle forces any two atoms to be sufficiently far away from each other so that their electron orbits do not overlap. Such exclusion interactions lead to Pauli repulsion potential with the form of , where  is a hyperparameter~\citep{buckingham,helium}.

\end{itemize}

\subsection{Crystal Property Prediction with Deep Learning}
\label{related}

Physics-based methodologies have long been employed for crystal energy prediction, albeit with a certain degree of specificity. Typically, these methods are highly specific to a particular type of crystal, implying that a single methodology can only deliver precise approximations for one distinct crystal type. Drawing inspiration from the field of physics, Coulomb matrices, as elucidated by~\citep{rupp2012fast, elton2018applying}, assume a pivotal function in the prediction of crystal energy. Nevertheless, their application is constrained, primarily modeling a specific subset of materials, namely, ionic and metallic materials. Moreover, a significant limitation of these matrices is their lack of permutation invariance. Recently, thanks to the advances of deep learning, many studies have been done to develop a general crystal property predictor for a variety of different crystals with powerful deep neural network models. Some studies~\citep{behler2007generalized, crabnet,jha2018elemnet,jha2019irnet,goodall2020predicting} represent crystals as chemical formulas, and adopt sequence models to predict properties from these string representations. However, more recent studies consider crystals as 3D graphs and employ expressive 3D GNNs~\citep{schutt2017schnet,klicpera2020directional,liu2022spherical}, a family of deep neural networks specifically designed for 3D graph-structured data, to crystal representation learning. CGCNN~\citep{xie2018crystal} is the first method that proposes to represent crystals with radius graphs and adopts a graph convolutional network to predict the property from the graph. Based on the pioneering exploration of CGCNN, subsequent studies~\citep{CyAtt,GATGNN,MegNet,ALIGNN,nequip, OMEE2022100491} propose various 3D GNN architectures to achieve more effective crystal representation learning. Particularly, by enhancing the input features with periodic invariance and periodic patterns, Matformer~\citep{yan2022periodic} develops the currently most powerful 3D GNN architecture for crystals and achieves the best crystal property prediction performance.


\section{Method}

Although existing GNN-based methods have achieved impressive performance in crystal property prediction, they struggle in further boosting the performance due to the approximation of interatomic interactions using functional expansions based on distances and failing in capturing complete interatomic interactions. In this section, we present PotNet, a novel crystal representation model that can overcome these limitations of prior methods. Based on the physical modeling of crystal energy, PotNet explicitly uses interatomic potentials and complete interatomic potentials as input features. The complete interatomic potentials are incorporated into the message passing mechanism of graph neural networks and efficiently approximated by an efficient algorithm.


\subsection{Approximating Crystal Energy with Complete Interatomic Potentials}
\label{importance}


According to the density functional theory (DFT) in physics, for any crystal  with the complete structure  defined in Eqn.~(\ref{eqn:comp_struc}), its total energy 
 can be approximated by the embedded atom method~\citep{daw1984embedded,eam,eamcovalent,eamionic,riffe2018vibrational} in the form of

where  denotes the interatomic potentials between the atoms  and , capturing the magnitude of interactions;  is the local electron density of the atom ,  determined by the coordinate and number of charges of the atom  according to the Hohenberg-Kohn theorem;  is a parametrized function to embed the electron density . Actually, existing studies~\citep{density} show that  can be considered as a function of  mathematically\footnote{Under zeroth-order approximation, the electron density  is represented as the aggregate of functions, analogous in type to those used for potential energy calculations. Due to computational efficiency, the approximation form of  is intentionally excluded from this study. This series type can be computed using the Riemann Theta Function as described in Appendix~\ref{gls}.}. Hence, Eqn.~(\ref{eqn:eam}) can be rewritten in the following form:

where  is a parametrized function. Eqn.~(\ref{eqn:new_eam}) can be considered as a way to compute the energy from the complete interatomic potential summation  of every atom  in the unit cell . However, in practice, the function  is computationally expensive if not infeasible. Hence, more and more studies have turned to the powerful learning capability of modern deep neural network models to approximate it effectively. 




\subsection{Limitations of Existing Deep Learning Methods}
\label{sec:limitation}

Currently, most of the existing graph deep learning methods for crystals~\citep{xie2018crystal, MegNet, GATGNN, ALIGNN} use radius graph representations and distance-based features as inputs to predict the crystal energy in Eqn.~(\ref{eqn:new_eam}). Specifically, for a crystal , the radius graph is constructed by adding edges between any atom  in the unit cell  and any other atom  in the complete crystal structure  whose distances are smaller than a pre-specified distance threshold . In addition, some functional expansions of distances, e.g., radial basis functions~(RBF), are used to model interatomic interactions and form the input edge features to 3D GNN models. Hence, let , the crystal energy prediction  of these methods can be generally described as

where ,  denotes the functional expansions, and  is a non-linear function based on 3D GNN models.

However, we argue that predicting or approximating the energy with Eqn.~(\ref{eqn:old_method}) is a suboptimal solution. Actually, compared with Eqn.~(\ref{eqn:new_eam}), which is physics-principled, there exist non-negligible approximation errors in Eqn.~(\ref{eqn:old_method}). First, Eqn.~(\ref{eqn:old_method}) captures the interatomic interactions based on interatomic distances, while the energy can be more accurately approximated by a function of interatomic potentials as in Eqn.~(\ref{eqn:new_eam}). Though according to Sec.~\ref{background:potentail}, interatomic potentials themselves are also functions of distances, we argue that directly using functional expansions of distances is not the best solution to crystal energy prediction. The commonly used functional expansions in existing methods, such as RBF , have different mathematical forms from potentials defined in Sec.~\ref{background:potentail}. Intuitively, this poses more challenges to 3D GNN models since they need to learn a mapping from  to the energy , while the energy  is not a direct function of . Hence, we argue that directly employing the physics-principled potential functions instead of  as input features is more suitable for crystal energy prediction. 

Second, different from Eqn.~(\ref{eqn:new_eam}), Eqn.~(\ref{eqn:old_method}) does not capture the complete set of interatomic interactions because the summation set  of atoms  is constrained to be the atoms whose distances to the atom  are smaller than . This can lead to an approximation error due to ignoring the accumulation of interatomic potentials. By the first principles in physics, interatomic potentials decay algebraically when pairwise interatomic distances become larger. Hence, for a finite structure like molecules, the potentials from atoms that are far away from a given atom are limited and can be ignored. However, this cannot be ignored for crystals since they accumulate infinitely. As a result, the accumulation of interatomic potentials can have a significant effect on a given atom in the infinite crystal structure. Let  be the distance between atoms  and  and  be a positive real number. Considering interatomic potentials , and assuming a 3D crystal structure containing an atom repeating itself with a Euclidean distance of 1, then the energy contribution by considering all its repetitions to it is simply the summation of all these interatomic potentials. To be concrete, the total potential summation  satisfies . Considering potentials of the pairwise atoms within the distance threshold , i.e., a sphere , we have the smallest possible prediction error  satisfying . Different from the geometry series,  decays at an approximately algebraical rate rather than exponentially. This suggests that a large radius  is needed to accurately approximate . Taking London dispersion potential  as an example and it can be calculated that to approximate  with  absolute error, we need at least , while in common radius crystal graph construction~\citep{xie2018crystal,MegNet,ALIGNN,GATGNN,schutt2017schnet}, the radius covers only a unit cell and its neighbors at average (see Appendix~\ref{sec:dataset}), analogy to . In addition, a larger radius will consume much more time for crystal graph construction since it induces a cubic time complexity in the 3D space.
We can observe from this example that the failure to capture complete interatomic potentials due to the use of radius is a key factor that prevents accurate energy prediction in existing GNN-based methods. In addition, we experimentally show that large cutoffs will produce better results for classic crystal-graph-based GNNs but consume more processing time in Appendix~\ref{sec:additional}. To remedy this problem, our efficient algorithm is presented in Sec.~\ref{algorithm}.



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{graph.pdf}
    \vskip -0.1in
    \caption{
    Schematic illustrations of how complete interatomic interactions are captured in PotNet. Note that PotNet models 3D crystals while we have 2D illustration for simplicity. (a) An example crystal in which each unit cell contains two atoms  and . In PotNet, the potentials between all pairs of atoms are captured. For simplicity, we only show the potentials from all  atoms to a  atom. (b) The complete set of potentials in (a) can be grouped into four categories, including , , , and . (c) We propose to compute an approximate summation for each category of potentials. 
    }
    \label{fig:infinite}
\vspace{-0.4cm}\end{figure*}

\subsection{Message Passing with Complete Interatomic Potentials}
\label{icg}

It follows from the analysis in Sec.~\ref{sec:limitation} that major limitations of existing deep learning methods for crystal representation learning lie in: (a) not making predictions from physics-principled interatomic potentials, and (b) not considering complete interatomic interactions. To overcome these limitations, we propose to explicitly use complete interatomic potential summations in GNN models. Since our proposed method is tightly related to potentials, we name it PotNet.

By reformulating Eqn.~(\ref{eqn:new_eam}), our PotNet incorporates the crystal energy computation with complete interatomic potentials into the message passing scheme of GNN models. For any material structure , we can rewrite the definition of its complete structure  in Eqn.~(\ref{eqn:comp_struc}) as    

where  denotes the set of atoms containing the atom  from the unit cell  and all its periodic repetitions in the complete crystal structure. With Eqn.~(\ref{eqn:new_comp_struc}), we can reformulate Eqn.~(\ref{eqn:new_eam}) as

where the infinite potential summation  denotes the sum of the interatomic potentials from the atom  together with its all periodic repetitions to the atom . Eqn.~(\ref{eqn:graph_eam}) can be integrated into the message passing scheme of GNN models. Specifically, we can create a graph  for , where each atom in the unit cell  corresponds to a node in the graph. For any two nodes  in the graph, there is an edge connecting them, and every node  in the graph is also connected to itself by a self-loop edge. If we consider the infinite potential summation  as the feature of the edge from node  to , we can use the message passing based non-linear neural network model in GNN to fit the function .

\begin{table*}[t]
\caption{Comparison between our method and other baselines in terms of test MAE on the Materials Project dataset. To make the comparison clear and fair, we follow~\citet{yan2022periodic} and use the same dataset settings. The best results are shown in \textbf{bold} and the second best results are shown with \underline{underlines}.}
\label{tb:mp}
\begin{center}
\begin{tabular}{l|cccc}
\toprule
& Formation Energy & Band Gap & Bulk Moduli & Shear Moduli \\
\cmidrule(r){2-5}
Method & eV/atom  &  eV &  log(GPa) & log(GPa)  \\
\midrule
CGCNN & 0.031 & 0.292  & 0.047 &0.077 \\
SchNet & 0.033 & 0.345 & 0.066 & 0.099 \\
MEGNET & 0.030 & 0.307 & 0.051 & 0.099 \\
GATGNN & 0.033 & 0.280 & 0.045 & 0.075 \\
ALIGNN & 0.0221 & 0.218 & 0.051 & 0.078 \\
Matformer & \underline{0.0210} & \underline{0.211} & \underline{0.043} & \underline{0.073} \\
PotNet  & \textbf{0.0188} & \textbf{0.204} & \textbf{0.040} & \textbf{0.065} \\
\bottomrule
\end{tabular}
\end{center}
\vskip -0.25in
\end{table*}

Based on this design of directly using interatomic potentials as edge features, our PotNet employs a GNN model with multiple message passing layers on the graph  to predict the crystal energy of . The computational process of the -th message passing layer for the node  can be described as

where  denotes the embedding vector of node  generated from the -th message passing layer,  is initialized to the atom feature vector of the atom , and  are both neural network models with trainable parameters  and , respectively. Here, the model  plays the role of capturing information from both atomic features and complete interatomic potentials. Detailed information about model architectures of  and  is provided in Appendix~\ref{model}. Note that our PotNet is actually a 3D GNN model even though 3D geometric information is not explicitly involved in Eqn.~(\ref{eqn:mpnn}). This is because the edge feature  is related to potential functions, and by Sec.~\ref{background:potentail} we know that they are computed from interatomic distances. In other words, PotNet can be considered to encode 3D geometric information with potential functions, though our direct motivation of using potential functions comes from the physical modeling of crystal energy.

Intuitively, the message passing process in Eqn.~(\ref{eqn:mpnn}) over the graph  can be considered as a general case of employing a radius graph where the distance threshold  goes to infinity, i.e., . In this case, as shown in Fig.~\ref{fig:infinite}(a), for any atom in the crystal, all the other atoms in the complete crystal structure have been included to interact with it. If we follow the radius graph construction process in the previous methods~\citep{xie2018crystal, MegNet, GATGNN, ALIGNN}, we obtain a graph  in which there exist an infinite number of edges between every pair of nodes. However, PotNet simplifies this complicated graph  to the graph  in which only one edge exists between every node pair. Specifically, PotNet directly models interatomic interactions as potentials and for any two nodes in , PotNet aggregates all edges between them to a single edge by the use of infinite potential summation  (see Fig.~\ref{fig:infinite}(b)). In other words, PotNet provides an effective solution that enables GNN models to capture complete interatomic interactions through the use of infinite potential summations.

\subsection{Efficient Computation of Infinite Potential Summation}
\label{algorithm}

Although the infinite potential summations have been effectively incorporated into the message passing based GNN models, the computation of these infinite potential summations is not trivial. Basically, there are two challenges to achieve accurate and efficient computation of the infinite potential summations. For accuracy, the computation algorithm requires provable error bounds. For efficiency, fast algorithm is needed to achieve scalable GNN training and fast crystal property prediction. To tackle these two challenges, we derive a fast approximation algorithm for infinite potential summations based on the Ewald summation method~\citep{ewald, kosmala2023ewald}. To be concrete, we unify the summations of three infinite potentials between the position of atom  and all repeated positions of atom  into an integral form, so that the Ewald summation method can be efficiently implemented in PotNet~(Fig.~\ref{fig:infinite}(c)). The key idea of the Ewald summation is that a slowly converging summation in the real space is guaranteed to be converted into a quickly converging summation in the Fourier space~\citep{woodward2014probability, kosmala2023ewald}. Based on this, the Ewald summation method divides a summation into two parts. One part has a quicker converging rate in the real space than the original summation. The other slower-to-converge part is then transformed into the Fourier space and becomes quickly convergent. In our method, the Ewald summation method is used with the infinite summations by dividing the integral into two parts, including one part that converges quickly in the Fourier space and another part that converges quickly in the real space, to obtain a fast approximation with provable error bounds.

\begin{table*}[t]
\caption{Comparison between our method and other baselines in terms of test MAE on JARVIS dataset. The best results are shown in \textbf{bold} and the second best results are shown with \underline{underlines}.}
\label{tb:jarvis}
\begin{center}
\begin{tabular}{l|ccccc}
\toprule
& Formation Energy & Bandgap(OPT) & Total energy & Bandgap(MBJ) & Ehull\\ 

\cmidrule(r){2-6}
Method & eV/atom  &  eV & eV/atom & eV & eV   \\
\midrule
CFID & 0.14 &    0.30 & 0.24  &  0.53 & 0.22 \\
CGCNN  & 0.063 &   0.20 & 0.078 &  0.41 & 0.17  \\
SchNet & 0.045 &   0.19 & 0.047 & 0.43 & 0.14  \\
MEGNET  & 0.047 &   0.145 & 0.058 & 0.34 & 0.084  \\
GATGNN  & 0.047 &   0.17 & 0.056 & 0.51 & 0.12  \\
ALIGNN  & 0.0331 & 0.142 & 0.037 & 0.31 & 0.076\\
Matformer  & \underline{0.0325} & \underline{0.137} & \underline{0.035} & \underline{0.30} & \underline{0.064}\\
PotNet   & \textbf{0.0294} & \textbf{0.127} & \textbf{0.032} & \textbf{0.27} & \textbf{0.055}\\
\bottomrule
\end{tabular}
\end{center}
\vskip -0.25in
\end{table*}

Following notations in Sec.~\ref{background} and Sec.~\ref{icg}, we denote the positions of atoms in the set  as . The Euclidean distances between the atom  and all atoms in  can be represented as . We then investigate the three types of potential mentioned in Sec.~\ref{background:potentail}. Since charges and constants in the Coulomb potential function can be extracted outside the summation and modeled as part of atom features, we simplify the Coulomb potential function as , where  is a hyperparameter scaling the Coulomb potential. As a result, we can represent Coulomb potentials from all atoms in  to the atom  as . Similarly, London dispersion potentials from all atoms in  to the atom  can be represented as . It is worth noting that Coulomb and London dispersion potentials can be represented in a unified view as  with a positive real number . In addition, we represent Pauli potentials from all atoms in  to the atom  as  with a hyperparameter . We provide detailed proofs in Appendix~\ref{proof_34_1} that the summations of these three potentials can be unified in an integral form as
\vspace{-0.1cm}

where  are constants derived from the corresponding specific potential forms and  is the generalized delta function such that  if and only if  and , otherwise . We then apply the Ewald summation method~\citep{ewald} to Eqn.~(\ref{equ:sum}) and split it into two parts as

where  denotes the short-range part that converges quickly in real space,  denotes the long-range\footnote{Note that the distinction between the short-range and long-range terms is not determined by a radius cutoff in Euclidean space. Instead, it is delineated by the rate of series convergence, or in more mathematical terms, a constant point in the integral summation. This can be represented as , where  denotes the series with a slower convergence rate in the direct space but a faster rate in the Fourier space. Conversely,  denotes the opposite scenario.} part that converges quickly in Fourier space, and the total summation converges as shown by \citet{ewald}. We demonstrate in Appendix~\ref{integralsummation} that  and  can be represented as sums of incomplete Bessel functions . Rigorous mathematical proofs are supplied in~\ref{besselconverge}, establishing the convergence of these summations of incomplete Bessel functions and their ability to be approximated with an error that remains within the bounds set by the Gaussian Lattice Sum. The practical application of the proposed summation algorithm is outlined in detail in Appendix~\ref{example}. 

Notably, the summations of incomplete Bessel functions pertaining to London dispersion potentials and Pauli potentials can be approximated directly. In contrast, the summations related to Coulomb potentials are computed by leveraging established mathematical work~\citep{terras1973bessel,kirsten1994generalized}, and employing analytic continuation to extend the domain of , as elucidated in Appendix~\ref{analytic} and~\ref{sec:zeta}. Considering the crystal system's inherent tendency towards neutrality and equilibrium, we illustrate the application of Coulomb potential summation in Appendix~\ref{born-lande}. It is of significance to note that PotNet stands as the pioneering methodology to apply the incomplete Bessel function for computing the Pauli potential summation, a feat unattainable by previous methods~\citep{epstein, lee2009ewald, fft}. Additionally, our method enables computation of other types of interatomic potential summations, including the Lennard-Jones potential, Morse potential, and screened Coulomb potential, as detailed in Appendix~\ref{addition}.


\section{Experimental Studies}
\label{experiments} 

\subsection{Experimental Setup} \label{sec:setup}


We conduct experiments on two material benchmark datasets, including The Materials Project and JARVIS. Baseline methods include CFID~\citep{CFID}, SchNet~\citep{schutt2017schnet}, CGCNN~\citep{xie2018crystal}, MEGNET~\citep{MegNet}, GATGNN~\citep{GATGNN}, ALIGNN~\citep{ALIGNN}, and Matformer~\citep{yan2022periodic}. All PotNet models are trained using the Adam~\citep{kingma2014adam} optimizer with weight decay~\citep{loshchilov2017decoupled} and one cycle learning rate scheduler~\citep{smith2018super} with a learning rate of 0.001, training epoch of 500, and batch size of 64. We use Pytorch and Cython to implement our models. For all tasks on two benchmark datasets, we use one NVIDIA RTX A6000 48G GPU as well as Intel Xeon Gold 6258R CPU for computing. Other detailed configurations of PotNet are provided in Appendix~\ref{model}.

In the implementation, PotNet employs both local and infinite crystal graphs, allowing it to encapsulate global, infinite interactions without compromising the fidelity of local interactions. More specifically, for the local crystal graph, we adopt the radius crystal graph as introduced by CGCNN~\citep{xie2018crystal}, albeit with modifications; we substitute Euclidean distances with interatomic potentials to serve as edge features. Given that the impacts of both London dispersion and Pauli potentials are circumscribed within the radius crystal graph, and can be largely disregarded when focusing solely on radius regions, we restrict ourselves to using Coulomb potentials within the radius crystal graph. In contrast, the infinite crystal graph is formulated as detailed in Section~\ref{icg}, where we incorporate all forms of interatomic potentials - Coulomb, London dispersion, and Pauli potentials. This approach enables us to capture both local and global interactions simultaneously, increasing the model's capacity for crystal representation. In order to enhance our model by leveraging infinite potential features, we have also integrated two supplementary techniques, namely the incorporation of periodic table information and the implementation of transformer operations~\citep{graphormer, yan2022periodic}. It should be noted that these methods are not applied in the main body of this paper. They have been exclusively applied and discussed in the supplementary section, as elucidated in Appendix~\ref{sec:improve}.

\begin{table}[t]
\vskip -0.1in
\caption{Training time per epoch, total training time, total testing time, and model complexity compared with ALIGNN and Matformer on JARVIS formation energy prediction.}
\begin{center}
\vskip -0.1in
\label{tb:efficiency}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc}
\toprule
Method   & Time/Epoch & Total Training Time & Total Testing Time & Model Para. \\ 
\midrule
ALIGNN & 327 s &  27.3 h & 156 s & 15.4MB  \\
Matformer & 64 s &  8.9 h & 59 s & 11.0 MB  \\
Ours  & \textbf{42 s} &  \textbf{5.8 h} & \textbf{31 s} & \textbf{6.7 MB} \\
\bottomrule
\end{tabular}
}
\end{center}
\vskip -0.3in
\end{table}

\subsection{Experimental Results}
\label{exp_results}

\textbf{The Materials Project}. We first evaluate PotNet on The Materials Project-2018.6.1, which is a widely used large-scale material benchmark with 69239 crystals. Following previous works~\citep{xie2018crystal,MegNet,ALIGNN,GATGNN,yan2022periodic}, four crystal properties including formation energy, band gap, bulk moduli, and shear moduli, are used for evaluating our model. We notice that previous works~\citep{xie2018crystal,MegNet,ALIGNN,GATGNN,schutt2017schnet} compare with each other using different versions of splitting training, evaluation, and testing datasets with different random seeds. For instance, \emph{the original CGCNN paper only uses 28046 training samples for formation energy prediction, resulting in the original result of 0.039.} To make the comparisons fair, we follow the settings of the previous state-of-the-art~(SOTA) Matformer~\citep{yan2022periodic} for all tasks since they retrain all baselines using the same dataset settings and the same data splits from ALIGNN~\citep{ALIGNN}. Note that most of the retrained results recorded in \citet{yan2022periodic} are better than their counterparts in their original papers. We present our results in Table~\ref{tb:mp}, where PotNet consistently outperforms other SOTA methods on all four tasks. 
Furthermore, the impressive results of PotNet for Bulk Moduli and Shear Moduli tasks with only 4664 training samples demonstrate the robustness and adaptive ability of PotNet to the tasks with small training data.  

\textbf{JARVIS Dataset}. We then evaluate PotNet on JARVIS-DFT-2021.8.18 3D dataset, a newly released benchmark dataset proposed by \citet{choudhary2020joint} with 55722 crystals. We evaluate PotNet on five crystal property prediction tasks, including formation energy, bandgap~(OPT), bandgap~(MBJ), total energy, and Ehull. We follow Matformer~\citep{yan2022periodic} and use the same training, validation, and test splits for all these tasks, and also use their retrained baseline results. As shown in Table~\ref{tb:jarvis}, PotNet achieves the best performances on all five tasks consistently. The superior performances of PotNet show the effectiveness of interaction modeling using potentials and explicit modeling of infinite interactions for crystal structures. 




\textbf{Efficiency of PotNet}. Beyond the superior modeling capacity for crystals, our PotNet is faster and more efficient than ALIGNN and Matformer. To demonstrate the efficiency of PotNet, we compare PotNet with ALIGNN and Matformer in terms of training time per epoch, total training time, inference time and model parameters for the task of JARVIS formation energy prediction. From Table~\ref{tb:efficiency},  PotNet is four times faster in terms of total training time and inference time compared with ALIGNN, and also faster than Matformer by 34\% and 47\% in terms of training time per epoch and inference time, respectively.

We also analyze the time cost of the infinite potential summation algorithm in Table~\ref{table:algorithm_cost}. Since there lack baselines for comparison of our infinite potential summation, we compare the prediction time (involving both data preprocessing and model inference time) with the most recent methods ALIGNN and Matformer. To perform preprocessing, unlike previous methods, we need to compute the infinite potential summations besides constructing graphs. However, as shown in Table~\ref{table:algorithm_cost}, the mean prediction time of PotNet for a single crystal is at the level of milliseconds, which is similar to ALIGNN and Matformer. Particularly, PotNet has a faster prediction speed than ALIGNN as the latter involves the computing of angles. 
PotNet is slightly slower than Matformer even though our PotNet needs to compute infinite summations, yet PotNet is much more effective.
To provide more details, we further present numerical examples and their corresponding time cost of our infinite potential summation algorithm in Table~\ref{table:algorithm} in Appendix~\ref{example}.




\begin{table}[t]
\caption{Prediction time cost of infinite potential summation algorithm on the JARVIS test dataset with 5572 crystals. The prediction time considers both preprocessing and model inference time.}
\begin{center}
\vspace{-0.4cm}

\label{table:algorithm_cost}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc}
\toprule
Method & Total Prediction Time & Prediction Time/Per Crystal\\
\midrule
 ALIGNN & 167s & 30 ms\\
 Matformer & 67s  & 12ms\\
 PotNet & 91s  & 16ms\\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.8cm}
\end{table} 


\subsection{Ablation Studies}

In this section, we demonstrate the importance of two core components of PotNet, including interaction modeling using interatomic potentials and infinite potential summations for crystal prediction. We conduct experiments on the JARVIS-DFT 3D formation energy task, and use test MAE for evaluation. In order to highlight the infinite potential summations, a comprehensive ablation study pertaining to various metrics, performed on the JARVIS-DFT 3D dataset, is detailed in Appendix~\ref{sec:full_ablation}.

\textbf{Interaction Modeling using Potentials}. We demonstrate the importance of interaction modeling using potentials by directly replacing potentials with Euclidean distances used by previous works in our PotNet with exactly the same model architecture. Specifically, we denote PotNet with only local crystal graph as the base model. We use `Base + Euclidean' to represent the base model with Euclidean distances as edge features and `Base + Potential' to represent the base model using Coulomb potentials as edge features. As shown in Table~\ref{ablation}, by replacing Euclidean distances with Coulomb potentials, PotNet without considering infinite potential summation already obtains a significant performance gain from 0.0363 to 0.0301, revealing the importance of interaction modeling using potentials in PotNet.

\textbf{Infinite Potential Summations}. The importance of infinite summation of potentials is demonstrated by comparing the previous base models with `Base + Potential + Infinite', denoting the full PotNet model with infinite summation in infinite crystal graph. It can be seen from Table~\ref{ablation} that by using infinite crystal graphs, the global information of crystal structures is captured, resulting in a performance gain from 0.0301 to 0.0294 for formation energy prediction.

\section{Limitation}

Our research indicates an encouraging performance enhancement using interatomic potentials and complete interatomic potentials. However, we must acknowledge a limitation: the interatomic potential's inherent incapacity to account for interactions extending beyond two atomic participants. Even though the total potential of a material system can be estimated by the embedded atom method~\citep{eam}, which sidesteps the necessity for many-body interactions, explicit consideration of angular and many-body interactions may lead to more precise simulations. This is due to the potential influence of additional atoms on interatomic interactions, which could subsequently modify the total potential outcome. Future studies might gain from integrating many-body interactions into the model, such as modeling the three-body potential and extending this methodology to an infinite summation approach.

\begin{table}[t]
\caption{Ablation studies for the effects of adding Coulomb potentials and infinite summation.}
\begin{center}
\vspace{-0.3cm}
\label{ablation}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c}
\toprule
  & JARVIS Formation Energy \\ 
\cmidrule(r){2-2}
Method  & eV/atom \\ 

\midrule
Base + Euclidean        & 0.0363\\
Base + Potential             & 0.0301 \\
Base + Potential + Infinite\quad   \quad \quad \quad           & \textbf{0.0294} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.9cm}
\end{table}


\section{Conclusion}

We study the problem of how to capture infinite-range interatomic potentials in crystal property prediction directly. As a radical departure from prior methods that only consider distances among nearby atoms, we develop a new GNN, PotNet, with a message passing scheme that considers interatomic potentials as well as efficient approximations to capture the complete set of potentials among all atoms. Experiments show that the use of complete potentials leads to consistent performance improvements. Altogether, our work provides a theoretically principled and practically effective framework for crystal modeling. In the future, we expect our approximations may be further improved to obtain a lower error bound. We also expect our algorithms for computing the summation could be further improved.

\section*{Acknowledgements}

This work was supported in part by National Science Foundation grants IIS-1908220, CCF-1553281, IIS-1812641, DMR-2119103, and IIS-2212419, and National Institutes of Health grant U01AG070112.





































































































































































\bibliography{potnet,dive,potnet_ref}
\bibliographystyle{icml2023}


\newpage
\appendix
\onecolumn

\section{Gaussian Lattice Sum}
\label{gls}
Let  be the full rank lattice matrix and . By extending the definition of \textit{Gaussian Lattice Sum}~\citep{gaussian}, we consider a summation of -dimensional Gaussian functions on a given shifted lattice,

One of the characteristics of the Gaussian Lattice Sum is the term  rapidly decays as  becomes large, leading to a fast converging rate of . By simple derivation, Gaussian Lattice Sum can be written w.r.t \textit{Riemann Theta Function},

where  denotes \textit{Riemann Theta Function}~\citep{riemann}, and it is easy to verify that  is positive definite that guarantees convergence of Gaussian Lattice Sum.~\citet{riemann} further shows several upper bounds. In particular, given ,

where  is the incomplete Gamma function and . Let  and a lattice matrix be . We obtain 

where . In addition, given , we can obtain the upper bound of the Gaussian Lattice Sum by incomplete Gamma function,




\section{Incomplete Bessel Function}

Let , , , . The \textit{incomplete Bessel function}~\citep{bessel} has the below integral form

In existing studies~\citep{bessel, besselcal, jones2007incomplete},
 is analytically continued to . In this work, we follow these studies and also use the analytic continuation for our calculations, i.e., we also consider .


\subsection{Fast Approximation of the Incomplete Bessel Function}
\label{fastbessel}
Computing the incomplete Bessel function is challenging as there does not exist direct closed-form solution. In this work, we investigate a fast approximation of the incomplete Bessel function.
Concretely, we adopt the algorithm in work by \citet{fft, besselcal}, where the incomplete Bessel function  is approximated by the  transformation with the linear time complexity of , and  here is the number of iterations. To begin with, the approximation  to  is given as a solution of  

where it is assumed that , , and  is the largest of  such that  for . And we aim to obtain the unknown coefficients  to approximate  from these  linear equations. By considering  in the Eqn.~(\ref{eq:diff}), we obtain

To eliminate the summation, \citet{besselcal} applied the  operator  times such that

By doing this,

where it sets

and

This leads to a recursive algorithm for  transformation. To compute the incomplete Bessel function , \citet{besselcal} investigated the following property

where the term  can be approximated by  transformation. Therefore, to approximate , we have corresponding approximation 


As a result, the approximation  to  is obtained by recursively solving  and ~\citep{besselcal, gaudreau2012computation}. The detailed expressions of  and  are given in \citet{besselcal} Proposition 2.2. In addition, \citet{fft} further optimizes the approximation of the incomplete Bessel function when  and  are both small. In this case, the remaining part of the Taylor expansion of  is small and we can approximate  by the first 
 terms of the Taylor series such that

The detailed error bound by this expansion is shown in \citet{fft}.

\subsection{Convergence of Incomplete Bessel Function Summation}
\label{besselconverge}

Let  and be constants. Given a -dimensional lattice, we define the \textit{incomplete Bessel function summation} on the lattice as

where , , ,  and  denoting the full rank lattice matrix. We aim to prove this summation of incomplete Bessel functions converges and can be approximated with an error bounded by the Gaussian Lattice Sum as introduced in Appendix~\ref{gls}. And this convergence property can be easily extended to the summation with  coefficients since .
\begin{proof} An upper bound of the incomplete Bessel function can be derived as

where  is the incomplete Gamma function described in Appendix~\ref{gls}. Based on this, we obtain

Given ,  has an upper bound~\citep{gamma} such that

Considering a prefixed value , ,  and , we have

where  is the Gaussian Lattice Sum described in Appendix~\ref{gls}. Therefore, the incomplete Bessel function summation can be divided into two parts

where the first part is a finite part inside an ellipsoid with a size of , and the second part  is bounded by the Gaussian Lattice Sum  and is convergent. Therefore, the incomplete Bessel function summation is convergent. Consequently, to approximate the incomplete Bessel function summation, we can choose to evaluate the summation inside an ellipsoid with the size of  for a prefixed , such that ,  and . Then the error  is bounded by Gaussian Lattice Sum . We can further bound the error by the inequality~(\ref{eq:gaussianbound}) such that 

where . This completes the proof.
\end{proof}


\section{Fast Algorithm of Potential Summation}

\subsection{Integral Transformation}
\label{proof_34_1}
In the main sections of our paper, it is much clear to use node positions to deliver our ideas. However, in the below sections, we are more interested in the vectors between nodes rather than the positions of those nodes. Given a full rank lattice matrix  and a vector  between two atoms inside the unit cell, we denote  as the potential summation and  as the potential function. Here, if a vector  is not inside the unit cell, we can apply a simple transformation~\citep{epstein} to  by converting it into the fractional coordinate, reducing mod 1, and then converting it back, i.e.,  
For a potential summation  of a crystal with a lattice matrix , we have . Based on these notations, we aim to prove that the summation of the three introduced potentials can be transformed into an integral form as 

where  are constants derived from the corresponding specific potential forms and  is the generalized delta function such that  if and only if  and , otherwise .
\begin{proof}
1). For the potentials in the form of  and , we apply the Mellin transform such that

Consequently, we obtain

Next, by deriving the summation,

Apparently, we can obtain , ,  and  for the summation of . The same result is also given in~\citet{epstein}.

2). For the potentials in the form of , we consider the inverse Laplace transform on  as shown by \citet{bateman1954tables}, such that

Therefore, by applying the Laplace transform in Eqn.~(\ref{eq:inverse}) we derive the integral form of : 

Next, by deriving the summation,

Apparently, we can obtain , ,  and  for the summation of .

This completes the proof.
\end{proof}

\subsection{Calculating Integral Summation}
\label{integralsummation}

A short mathematical summary of this section is presented in \url{https://github.com/divelab/AIRS/blob/main/OpenMat/PotNet/summary.pdf}. As shown in Sec.~\ref{algorithm},  can be written as the summations in the direct space and the Fourier space: 

where  denotes the summation in Fourier space, and 
 denotes the summation in direct space. Below, we prove that both  and  can be deduced into the incomplete Bessel function summation.
\begin{proof}
For , by deriving in direct space,

If  and , 

And if  and , 

Otherwise,

Overall,

Here we obtain  as the incomplete Bessel function summation. Inspired by the Ewald summation~\citep{ewald, epstein}, we consider  on the reciprocal lattice using the Poisson summation~\citep{epstein}: 

where  and  in our case, and  is the lattice matrix of the reciprocal lattice. Therefore, we derive

If , we obtain

By applying analytic continuation to domain of  as discussed in Appendix~\ref{analytic}, we obtain

with poles  and ; Otherwise, if ,

Together, we obtain

Apparently,  is deduced into the incomplete Bessel function summation, and this completes the proof.
\end{proof}
Therefore, both  and  can be expressed by the incomplete Bessel function summation. In addition, as shown in Appendix~\ref{besselconverge}, the incomplete Bessel function summation  converges and can be approximated. Therefore,  converges and also can be approximated.

\subsection{Analytic Continuation of Potential Summations}
\label{analytic}

To represent the series that is not well-defined in its original domain, including the Coulomb potential summation , we need to investigate the analytic continuation of the potential summation. Analytic continuation is a technique to extend the domain  of a given analytic function . If there exists a domain  containing , and a function  that is analytic on , and  holding for all  in , consequently,  is an analytic continuation of  to . As shown by \citet{kung2003complex}, the analytic continuation is unique and satisfies the permanence of functional relationships, i.e., the equations holding for  will also hold for . 

In our case, we can expand the domain of  in  to  such that  is well-defined for any . This is enabled by analytic continuation in Eqn.~(\ref{eqn:analyticorigin}). To be concrete, the original domain of  is  in Eqn.~(\ref{eqn:analyticorigin}) and we can extend the domain of  for , , and , respectively. By applying the analytically continued incomplete Bessel function we can extend the domain of  to . On the other hand, the analytic continuation on  and  will result in two poles  and  on . Therefore, the final analytically continued domain of  is . For the potential summation , we have  as shown in Appendix~\ref{proof_34_1}. By analytic continuation, we are able to compute the summation for any . Since  and  for our crystal dataset, we are able to compute the Coulomb potential summation. In next section, a more general analytic continuation result  is given. 


\subsection{Generalized Epstein Zeta Function and Analytic Continuation}
\label{sec:zeta}

In fact, the potential summation  is a special case of \textit{generalized Epstein zeta function}~\citep{crandall1987elementary, terras1973bessel, epstein}, and is a generalization of the Riemann zeta function. Let ,  and  inside the unit cell and reciprocal unit cell respectively. The generalized Epstein zeta function~\citep{epstein} has the below summation form

Apparently, the potential summation  can be expressed in terms of generalized Epstein zeta function as . In addition,  has an analytic continuation to the entire complex plane, except for simple poles at  and ~\citep{crandall1987elementary}, which is corresponding to our result in Appendix~\ref{analytic}. Moreover,  can be written in the form of an integral summation~\citep{epstein} similar to Eqn.~(\ref{eq:epstein}) such that

Based on this, we can also split the integral and apply Poisson summation in Eqn.~(\ref{eq:possion}) to obtain two summations of incomplete Bessel functions to evaluate this series. To be concrete, similar to Eqn.~(\ref{eqn:direct}), the summation of generalized Epstein zeta function on direct space is

By further exploring two cases of , we obtain

The summation of generalized Epstein zeta function on Fourier space is

if ,

Otherwise,

Then

Therefore,

Similar to Eqn.~(\ref{eqn:analyticorigin}), we apply analytic continuation and obtain

Here,~\citet{epstein} gives the same result Eqn.~(\ref{eqn:generalepstein}) with incomplete Gamma function, while we provide detailed derivation on . For more details of the generalized Epstein zeta function, we refer readers to~\citet{crandall1987elementary, terras1973bessel, epstein, kirsten1994generalized, selberg1967epstein}. The generalized Epstein zeta function is used for the computation of Madelung constants~\citep{epstein} as described in Appendix~\ref{born-lande}. This is because one can view the term  as the charge distribution in the crystal system. If the phase  for any , the generalized Epstein zeta function will become an alternating series. This is useful for the ionic crystal systems where the unit cell is generally neutral and the Coulomb potentials cancel each other. A famous example is the Madelung constant of , which is derived by the summation of Coulomb potentials among  and  ions. By using the generalized Epstein zeta function, we have the Madelung constant of  as ~\citep{epstein} and here  denotes a diagonal matrix with all main diagonal values as the scalar . 

One can also calculate the Madelung constant by considering extracting terms in  with the same coefficient  as individual potential summations and computing those individual summations (by analytic continuation). This is due to the fact that  and these individual summations are all calculated by incomplete Bessel functions and share the same analytically continued domain. In other words,  can be calculated by a linear combination of individual potential summations (by analytic continuation). It is useful for the case where  is initially unknown but can be learned. Here we give a simple numerical example to calculate eta function  by analytic continuation such that 

where  is Hurwitz zeta function and  when , and its analytic continuation elsewhere. That is, we can use the analytically continued zeta function  to precisely evaluate a conditionally convergent series (or alternating zeta function). Overall, the above shows that we can use analytically continued potential summations to approximate the total contribution of potentials where individual potential summations cancel each other. We further show the Madelung constant calculation of  by analytic continuation in Appendix~\ref{born-lande}.  

\subsection{Implementation and Numerical Examples of Approximation}
\label{example}

\begin{table}[t]
\caption{Numerical examples of our algorithm. We approximate results by using all grid elements and approximating the error upper bound. The first column denotes the ground truth targets we aim to approximate. By choosing different grid length  in our algorithm, we can obtain different evaluation results and different estimated error upper bound of the approximation as described in the second, the third, and the fourth column. The fifth column gives the truth error between our approximation and ground truth. And the sixth column denotes time cost of our approximation. Here, . Implementation details can be found in Appendix~\ref{example}.}
\label{table:algorithm}
\begin{center}
\begin{tabular}{lccccc}
\toprule
Ground Truth  & Evaluation &  & Estimated Error & True Error & Time \\ 
\midrule
 & 3.28068288 & 1 & 9e-1 & 9e-3 & 2 ms \\
 & 3.28984070 & 2 & 6e-2 & 3e-5 & 3 ms \\
 & 3.28986812 & 3 & 7e-4 & 1e-8 & 3 ms \\
 & 3.28986813 & 4 & 1e-6 &  1e-8 & 3 ms \\
 & 2.40411381 & 4 & 1e-6 &  1e-8 & 3 ms \\
 & 2.16464647 & 4 & 1e-6 &  1e-8 & 3 ms \\
 & 5.99068949 & 1 & 3 & 4e-2 & 3 ms\\
 & 6.02670959 & 2 & 4e-1 & 1e-4 & 3 ms\\
 & 6.02681199 & 3 & 7e-3 & 5e-8 & 3 ms\\
 & 6.02681204 & 4 & 2e-5 &  1e-8  & 3 ms\\
 & 2.16395326 & 1 & 4e-1 & 2e-7 & 2 ms\\
 & 2.16395341 & 2 & 3e-4 &  1e-8 & 4 ms\\
 & 25.39268214 & 1 & 2.5 & 5e-7 & 3 ms\\
 & 25.39268269 & 2 & 1e-2 &  1e-8 & 3 ms \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

In this section, we explicate the methodology applied for the implementation of our algorithm. A predefined -dimensional discrete grid, centered at the origin and having a length defined by , is applied for point selection of summation approximation. Typically,  is delineated in the algorithm. Owing to the complexities associated with computing the inverse of the incomplete Gamma function, our primary objective is to establish the value of , after which we calculate its corresponding error bound. To elaborate, we commence by determining the value of  within a crystalline structure, following which we ascertain the associated error bound, as prescribed by Eqn.~(\ref{eq:error}). Within the -dimensional discrete grid, a set of points inside an ellipsoid predicated on  is selected for the evaluation of the summation of incomplete Bessel functions. It is essential to underscore that determining  for each crystal is a nontrivial procedure. In cases where  is not predetermined, we employ all grid elements to approximate the result and estimate the error upper bound by approximating . It is also worth noting that we can take advantage of operations with vectorization when summing over grid elements and thus boost the final computation speed. As  is inside the lattice, the approximation of  is achieved by considering , which is the length of the minor axis of an ellipsoid. And the ellipsoid is constructed via the expression  or , with  denoting points derived from a sphere centered at the origin with a radius . Formally, given a lattice matrix , a vector  inside a unit cell, and the constants  derived from specific potential functions as described in Appendix~\ref{proof_34_1}, we aim to evaluate these two parts  and .

To evaluate  according to Eqn.~(\ref{eqn:gdirect}), we derive the following steps. 

\emph{\textbf{Step 1:}} Determine the grid residing in the -dimensional integer space  with length denoted as  and a value  such that the conditions  and ,  are satisfied. Subsequently, select points denoted by  from the grid, adhering to the inequality . Once the points are selected, calculate the function represented by Eqn.~(\ref{eqn:gdirect}). In the scenario where  remains undefined, it is recommended to use large  and select all available points on the grid for computation of Eqn.~(\ref{eqn:gdirect}).  is then approximated by .

\emph{\textbf{Step 2:}} 
Compute the error bound denoted as  by the following formula: , where  satisfies the condition .

To evaluate  according to Eqn.~(\ref{eqn:gfourier}), we derive the following steps.


\emph{\textbf{Step 1:}} Determine the grid residing in the -dimensional integer space  with length denoted as  and a value  such that the conditions  and , ,  are satisfied. Subsequently, select points denoted by  from the grid, adhering to the inequality . Once the points are selected, calculate the function represented by Eqn.~(\ref{eqn:gfourier}). In the scenario where  remains undefined, it is recommended to use large  and select all available points on the grid for computation of Eqn.~(\ref{eqn:gfourier}).  is then approximated by .

\emph{\textbf{Step 2:}} Compute the error bound denoted as  by the following formula: , where  satisfies the condition .

Our implementation is based on Cython, GNU Scientific Library~\citep{galassi2002gnu} and ScaFaCoS~\citep{scafacos-web}, in which the native incomplete Gamma function and incomplete Bessel function are used. We show the evaluation examples in Table~\ref{table:algorithm} with the corresponding error bound and evaluation time. The running time is at the scale of milliseconds.


\subsection{Potential Summation Extensions}
\label{addition}

To highlight the versatility of our potential summation method, we incorporate additional potentials that can be computed using our algorithm and Laplace transform. These include the Lennard-Jones potential, Morse potential, and screened Coulomb potential, which typically find application in the analysis of specific categories of materials, thereby demonstrating the broad applicability of our method. 


\textbf{Lennard-Jones Potential}~\citep{lj} is an intermolecular pair potential that is usually used for gas or organic materials. Let  and  be hyperparameters. The commonly used expression for the Lennard-Jones potential is

And the summation of  can be converted to two potential summations of  with  and  respectively, such that

where we already show the calculation of the potential summation of  in Appendix~\ref{proof_34_1}.

\textbf{Morse Potential}~\citep{morse1929diatomic} is an interatomic potential of diatomic molecules and is used for simple molecular materials. Let  and  be hyperparameters. The Morse potential has a mathematical form of

Similarly, the summation of  can be converted to two potential summations of  with  and  respectively, such that

where we already show the calculation of the potential summation of  in Appendix~\ref{proof_34_1}.

\textbf{Screened Coulomb Potential} represents the Coulomb interactions with damping of electric fields. It is an important potential reflecting the behaviors of charge-carrying fluids or particles in semiconductors~\citep{kirichenko2021influence}. Let  be elementary charge constant and  be a scaling hyperparameter. The screened Coulomb potential has an analytic form of , where  is the distance between atom  and , and  are charges of atom  and . Similar to our Coulomb potential case, since  are constants and can be extracted outside the summation, we derive a simplified screened Coulomb potential

Consider the inverse Laplace transform on ~\citep{bateman1954tables}, we obtain

Therefore, we can apply the Laplace transform in Eqn.~(\ref{screened}) such that

Then we obtain , ,  and  in Eqn.~(\ref{general}) to fit screened Coulomb potential into our potential summation method.


\section{Experimental Details}

\subsection{PotNet Implementation}
\label{model}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{model.pdf}
    \caption{The developed network architecture for PotNet. The notations used in this figure are defined as follows:  denotes the sigmoid function;  represents the concatenation operation along the dimension ;  signifies the Hadamard product, or element-wise multiplication of two matrices or tensors of the same dimensions; And  represents the aggregation operation over the index .
}
    \label{fig:model}
\end{figure}

The employed network architecture is shown in Fig.~\ref{fig:model}. Since our major contribution is to consider interatomic potentials and their complete form, we simply design our network architecture following the commonly used settings. Specifically, existing methods for 3D graphs~\citep{xie2018crystal,schutt2017schnet,klicpera2020directional,klicpera2020fast,gasteiger2021gemnet,schutt2021equivariant, wang2022comenet, liu2022spherical, yan2022periodic, wang2023learning} share a similar architecture, which usually contains an input block, an interaction block, and an output block. Without loss of generality, we take the updating process for node  as an example to illustrate the network. 
\begin{itemize}

\item \textit{The Inputs} contain atomic features and potentials.
 is the 92-dimensional atomic feature for any atom  following CGCNN~\citep{xie2018crystal}. Below the term  denotes interatomic distances, which form an integral part of our potential features. As delineated in Sec.~\ref{sec:setup}, our computational model employs both local and global graphs. In the context of the local graph, only the Coulomb potential is considered, as per the details provided in the aforementioned Sec.~\ref{sec:setup}. On the other hand, the global graph encapsulates the summations of infinite potentials as its edge features. We consider all three categories of infinite potential summations in our model, namely Coulomb potentials, London dispersion potentials, and Pauli repulsion potentials. These are discussed in detail in Sec.~\ref{background:potentail}. The hyperparameter  is employed to simplify the mathematical form of Coulomb potentials, expressed as , to . This reduction is permissible since  are all known constants, and  can be derived from atomic features. In the context of the local crystal graph, the Coulomb potentials are denoted as . For the infinite crystal graph, the summations of Coulomb potentials, London dispersion potentials, and Pauli potentials are denoted as , , and , respectively. We aggregate all three infinite features to derive the infinite features for the network, expressed as . In our experimentation, we fixed the values of the various parameters as follows: , , , , and . The selection of these specific constants was the result of manual grid searching.

\item \textit{The Input Block} of our model consists of two primary components: a \textit{\textbf{Linear}} layer and an \textit{\textbf{Embedding}} layer. For every node  in our model, we use the \textit{\textbf{Linear}} layer to generate a 256-dimensional vector. This vector serves as the input node features for the first interaction layer. Simultaneously, we apply an \textit{\textbf{Embedding}} layer for each edge in the model. This layer functions to map the Coulomb potentials and the summations of infinite potentials onto 256-dimensional embeddings. The Coulomb potentials undergo transformation using 256 Radial Basis Function (RBF) kernels, with the centers spanning a range from -4.0 to 4.0. Likewise, we transform the summations of infinite potentials using 64 RBF kernels, with centers also ranging from -4.0 to 4.0. Subsequently, we perform an up-projection on these transformed summations through a Multilayer Perceptron (MLP), resulting in a 256-dimensional output.

\item \textit{The Interaction Block} of our model comprises several \textit{\textbf{Interaction}} layers. Each of these layers dynamically updates the feature vector of a given node , taking into account the features of its neighboring nodes as well as the potential embeddings of the connecting edges. More specifically, for any neighboring node  of node , the corresponding potential embeddings, denoted as  and , are generated by the \textit{\textbf{Embedding}} layer. These embeddings are initially concatenated along the edge dimension. Subsequently, the embeddings are concatenated with the node features  and  along the feature dimension. The principal interaction pattern of a layer parallels the pattern adopted in the CGCNN~\citep{xie2018crystal} as shown in the right side of Fig.~\ref{fig:model}. 

\item \textit{The Readout Block} of our model incorporates an \textit{\textbf{AvgPooling}} layer and a subsequent \textit{\textbf{Linear}} layer. Initially, the \textit{\textbf{AvgPooling}} layer is used to compile and aggregate features from all nodes within a graph. This aggregated feature set is then processed through the \textit{\textbf{Linear}} layer. The function of this layer is to map the hidden dimension, represented as a 256-dimensional vector, to a final scalar output.

\end{itemize}

\subsection{PotNet Improvement}
\label{sec:improve}
\begin{wraptable}[9]{r}{0.25\textwidth}
\vspace{-30 pt}
\begin{center}
\caption{PotNet improving techniques by periodic table information and transformer structure on JARVIS formation energy and training time. }\vspace{-0.1cm}
\label{tb:improvement}
\resizebox{0.25\textwidth}{!}{
\begin{tabular}{l|cc}
\toprule
Model & MAE & Time/Epoch \\ 
\midrule
& eV/atom & s \\
\midrule
PotNet & 0.0294 & 42 \\
PotNet-C  &	0.0293 & 42 \\
PotNet-T &	0.0290 & 50 \\
\bottomrule
\end{tabular}}
\end{center}
\end{wraptable} In order to enhance the efficacy of the model, we incorporated two strategies: the inclusion of periodic table information and the application of a transformer structure for infinite potential features. 

In our model, the charge information is inferred through atomic numbers, which could potentially introduce inaccuracies. To address this, we explicitly encode ten classes that are distinguished based on the categories of elements in the periodic table: alkali metals, alkaline earth metals, transition metals, post-transition metals, metalloids, reactive nonmetals, noble gases, lanthanides, and actinides. This encoding facilitates the model's learning of atomic properties. We have denoted this improved network model as PotNet-C.

Furthermore, we incorporated a transformer operation~\citep{graphormer, yan2022periodic} specifically applied to the infinite potential summation. This operation was particularly apt as the infinite potential summation is based on a fully-connected graph, an ideal fit for the transformer model. The transformer node output was then added to the local graph node output. This enhanced network structure, combined with the prior periodic table information, is referred to as PotNet-T.

As illustrated in Table.~\ref{tb:improvement}, PotNet-C displays performance comparable to that of the original PotNet, while PotNet-T exhibits superior results to both PotNet and PotNet-C. However, it should be noted that the computational efficiency of PotNet-T is lower, with only a modest increase in performance.


\subsection{Dataset Information}
\label{sec:dataset}

\begin{wraptable}[6]{r}{0.3\textwidth}
\vspace{-40 pt}
\begin{center}
\caption{Dataset information on JARVIS and the Materials Project.}\vspace{-0.2cm}
\label{tb:dataset}
\resizebox{0.3\textwidth}{!}{
\begin{tabular}{l|cc}
\toprule
Information & JARVIS & MP \\ 
\midrule
Size&	55722 &	69239\\
Mean Atom Numbers &	10.1 &	29.9\\
Mean Cell Length &	5.95 &	8.02\\
Minimum Cell Length &	0.99&	1.78\\
\bottomrule
\end{tabular}}
\end{center}
\end{wraptable} In Table~\ref{tb:dataset}, we detail the fundamental attributes of two distinct datasets: the Materials Project (MP) and the JARVIS 3D dataset. The considered characteristics comprise the size of each dataset, the average number of atoms per cell, the mean cell lengths, and the minimum cell length. As reflected in Table~\ref{tb:dataset}, the shortest cell length ranges approximately from 1 to 2. Many methods predicting crystal properties~\cite{xie2018crystal, schutt2017schnet, MegNet, ALIGNN} typically opt for a cutoff of either 4 or 8. This range, on average, only encompasses one-hop neighbors of a unit cell, and, at its most extensive, eight-hop neighbors.

\subsection{Full Ablation on JARVIS Dataset}
\label{sec:full_ablation}
\begin{table*}[t]
\caption{Ablation on our method with/without infinite potential summations in terms of test MAE on JARVIS dataset. The best results are shown in \textbf{bold}.}
\label{tb:full}
\begin{center}
\begin{tabular}{l|ccccc}
\toprule
& Formation Energy & Bandgap(OPT) & Total energy & Bandgap(MBJ) & Ehull\\ 

\cmidrule(r){2-6}
Method & eV/atom  &  eV & eV/atom & eV & eV   \\
\midrule
PotNet w/o Infinite & 0.0301 & 0.134 & 0.033 & 0.294 & 0.072\\
PotNet   & \textbf{0.0294} & \textbf{0.127} & \textbf{0.032} & \textbf{0.272} & \textbf{0.055}\\
\bottomrule
\end{tabular}
\end{center}
\vskip -0.25in
\end{table*}

In addition to the results shown in the main body, we conduct a thorough ablation study of the infinite potential summations, employing the JARVIS dataset for this analysis. As highlighted in Table.~\ref{tb:full}, the incorporation of infinite summation features contributes to consistent performance augmentation across all metrics evaluated in this study. Remarkably, a substantial enhancement is observed in the predictive accuracy associated with the BandGap (MBJ) and Ehull properties.

\subsection{Cutoff Experiments}
\label{sec:additional}

\begin{wraptable}[12]{r}{0.3\textwidth}
\vspace{-25 pt}
\begin{center}
\caption{Experiments with varied cutoffs on JARVIS formation energy and dataset preprocessing time.}\vspace{-0.2cm}
\label{tb:additional}
\resizebox{0.3\textwidth}{!}{
\begin{tabular}{l|cccc}
\toprule
Cutoff & SchNet & GATGNN & PotNet & Time \\ 
\midrule
\AA & eV/atom& eV/atom & eV/atom & s\\
\midrule
4&	0.052&	0.048 & 0.036 & 84\\
8&	0.045&	0.047 & 0.034 & 108\\
12&	0.045&	0.047 & 0.033 & 112\\
16&	0.045&	0.046 & 0.033  & 178\\
20&	0.044&	0.046 & 0.033 & 259\\
30&	0.042&	0.045 & 0.031 & 688\\
50&	Unstable&	0.043 & 0.030 & 3239 \\
\bottomrule
\end{tabular}}
\end{center}
\end{wraptable} In this section, we delve deeper into the investigation of two GNN approaches with differing cutoff values, with the aim of directly assessing the importance of complete interatomic potentials. The selection of these two methods is informed by their wide usage and the fact that they do not require additional graph construction techniques apart from radius crystal graph construction.

The first approach involves the application of the conventional GNN methodology, SchNet~\citep{schutt2017schnet}. The second approach, GATGNN~\citep{GATGNN}, incorporates the use of global attention. Additionally, we have incorporated our model without the infinite potential summation and local Coulomb potential into the study as the third approach. It should be noted that as the cutoff value increases, the feasibility of conducting training experiments diminishes due to the increase in time complexity. To facilitate training, we set the maximum number of an atom's neighboring atoms to 16.

We conduct training and testing on these three methodologies using the JARVIS 3D dataset, maintaining the same dataset settings as in Matformer~\citep{yan2022periodic}. We present the results pertaining to the formation energy in Table~\ref{tb:additional}. As indicated in Table~\ref{tb:additional}, it can be inferred that an increase in the cutoff value correlates with an enhancement in the performance of the three methodologies. However, at a cutoff value of 50, the training of SchNet encounters a gradient explosion, rendering its final results unavailable. This issue may potentially stem from the modeling capacity of SchNet.

Further, we demonstrate the preprocessing time required for the entire JARVIS dataset, which comprises 55,722 crystals, in relation to different cutoff values in the fourth column of Table~\ref{tb:additional}. It becomes evident that as the cutoff value increases, the preprocessing time escalates to an unmanageable extent.


\section{Linear Energy Modeling using Infinite Potential Summation}
\label{born-lande}

\begin{wrapfigure}{r}{0.3\textwidth} 
\vspace{-10pt}
  \begin{center}
    \includegraphics[width=0.3\textwidth]{nacl.pdf}
    \vspace{-20pt}

    \caption{Crystal structure of .}
    \label{fig:nacl}
  \end{center}
  \vspace{-20pt}
\end{wrapfigure} In this section, we provide examples of calculations where the total energies of these materials can be directly approximated by linear combinations of infinite potential summations. These are special cases of Eqn.~(\ref{eqn:graph_eam}) with a linear embedded function . Specifically, we evaluate the total energy per atom of  and two other materials (, ) whose crystal structures are similar to . Since they are pure ionic crystals and Coulomb interactions dominate the system, we first consider their total electrostatic energy, i.e., the Coulomb potential summations. 

Inspired by analytic continuation as discussed in Appendix~\ref{sec:zeta}, we can approximate the total energy per atom of  by analytically continued infinite potential summations. The crystal structure of  is shown in Fig.~\ref{fig:nacl}. Due to the symmetry of the  cell, we only involve atoms  in our calculation. Here, atom  represents the body center , atom  represents the face center , atom  represents the edge center , and atom  represents the corner . Given  as Avogadro constant,  as the minimum distance between  and ,  as charges of  and ,  as the elementary charge constant, and  as the permittivity constant of free space, the total energy of  is approximated by the total Coulomb interactions with atom  such that

where  is the distance between atom  and ,  is the normalized distance,  is the infinite potential summation with , approximated by our algorithm in Sec.~\ref{algorithm},  denotes the set of atoms containing atom  and all its repetitions, and the coefficients  denote the fraction of atoms in a unit cell. We finally obtain a constant  and the total electrostatic energy approximation  from our infinite potential summations. In fact, this constant  is exactly the famous Madelung constant ~\citep{borwein1985convergence} of . To obtain a more accurate total energy result by considering an additional repulsion term, we can derive the calculation result in Eqn.~(\ref{cal}) to the famous Born-Land\'e equation~\citep{BornConj}


where  are the charges of cation and anion,  is the Madelung constant computed from Coulomb potential summations, and  is the Born exponent measuring the effect of repulsion. Choosing , we can obtain an approximation for the total energy of  of 7.84 eV. Similarly, we also apply Eqn.~(\ref{lande}) to  and  to approximate the total energy per atom of these crystals. 

\begin{wraptable}[7]{r}{0.4\textwidth}
\vspace{-15 pt}
\begin{center}
\caption{Total energy per atom approximation of ,  and .}\vspace{-0.3cm}
\label{tb:nacl}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
Formula &  &  & Ground Truth  & Eqn.~(\ref{lande})  \\ 
\midrule
 & 282 pm & 9 & 8.15 eV & 7.84 eV\\
 & 210 pm & 6 & 39.33 eV & 39.45 eV \\
 & 201 pm & 7 & 10.67 eV & 10.60 eV\\
\bottomrule
\end{tabular}}
\end{center}
\end{wraptable} We show these approximations in Table~\ref{tb:nacl} and it can be noticed that these approximations already give rough results compared to the ground truth total energy. This implies that our features can serve as a good starting point for machine learning models to learn the ground truth energy. Apparently, previous methods cannot achieve this due to the lack of such informative features. It is worth noting that the Madelung constant is typically unknown because those coefficients for the infinite potential summations depend on the charge distribution in the system, which we do not know at the beginning. Also, we already mention that these crystals are special cases of Eqn.~(\ref{eqn:graph_eam}) with a linear embedded function , while  is typically a nonlinear function~\citep{daw1984embedded}. Therefore, the network serves the purpose of learning those coefficients to learn the Madelung constant and providing nonlinearity.











\end{document}
