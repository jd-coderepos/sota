\documentclass{bmvc2k}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ctable}
\usepackage{capt-of}



\title{Visibility-aware Multi-view Stereo Network}

\addauthor{Jingyang Zhang}{jzhangbs@cse.ust.hk}{1}
\addauthor{Yao Yao}{yyaoag@cse.ust.hk}{1}
\addauthor{Shiwei Li}{sli@altizure.com}{2}
\addauthor{Zixin Luo}{zluoag@cse.ust.hk}{1}
\addauthor{Tian Fang}{fangtian@altizure.com}{2}

\addinstitution{
 The Hong Kong University of Science and Technology\\
 Hong Kong SAR, China
}
\addinstitution{
 Everest Innovation Technology\\
 Hong Kong SAR, China
}

\runninghead{Zhang, Yao, Li, Luo, Fang}{Vis-MVSNet}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\begin{document}

\maketitle

\begin{abstract}
	Learning-based multi-view stereo (MVS) methods have demonstrated promising results. However, very few existing networks explicitly take the pixel-wise visibility into consideration, resulting in erroneous cost aggregation from occluded pixels. In this paper, we explicitly infer and integrate the pixel-wise occlusion information in the MVS network via the matching uncertainty estimation. The pair-wise uncertainty map is jointly inferred with the pair-wise depth map, which is further used as weighting guidance during the multi-view cost volume fusion. As such, the adverse influence of occluded pixels is suppressed in the cost fusion. The proposed framework \textit{Vis-MVSNet} significantly improves depth accuracies in reconstruction scenes with severe occlusion. Extensive experiments are performed on \textit{DTU}, \textit{BlendedMVS}, and \textit{Tanks and Temples} datasets to justify the effectiveness of the proposed framework. 
\end{abstract}

\section{Introduction}
Multi-view Stereo (MVS) is one of the core problems in computer vision, which is essential to a variety of applications including image-based 3D modeling, city-scale survey and autonomous driving. While the problem is mainly solved by classical methods \cite{campbell2008using, furukawa2009accurate, tola2012efficient, galliani2015massively, schonberger2016pixelwise}, recent learning-based methods \cite{yao2018mvsnet,yao2019recurrent,gu2020cascade} have also shown competitive results compared to previous state-of-the-arts. Learning-based methods usually extract deep image features from input images, which implicitly introduces global semantic such as specularity and reflection priors during the reconstruction process. Moreover, MVS networks usually apply 3D convolution neural networks (CNNs) for the cost volume regularization, which is more powerful than engineered cost regularization in classical methods.

One critical factor in MVS is the pixel-wise visibility: whether a 3D point is visible in given images. However, such visibility information is unknown before the 3D model is densely recovered, which implies a chicken-and-egg problem. In traditional MVS algorithms, the visibility issue is well understood: some approaches simply reject patch pairs according to pre-determined criteria, and then update the cost aggregation with only the inlier patch pairs \cite{furukawa2009accurate,tola2012efficient,xu2019multi}. More advanced approaches, such as COLMAP \cite{zheng2014patchmatch,schonberger2016pixelwise}, compute the visibility information and aggregate the pair-wise matching cost based on a probabilistic framework, where visibility and depth are alternatively updated in E-step and M-step. 

However, for current learning-based MVS methods, very few of them have acknowledged this problem and have explicitly handled the visibility issue. For example, MVSNet and its following works \cite{yao2018mvsnet,yao2019recurrent,chen2019point,gu2020cascade,cheng2020deep,yang2020cost} feed multi-view features from all views into a variance-based cost metric regardless of the visibility of the pixel. Other methods apply either averaging \cite{hartmann2017learned} or max pooling \cite{huang2018deepmvs} to aggregate the matching cost. While it is possible that the network could implicitly learn how to discard the invisible views for each pixel, the unsolved visibility problem may inevitably deteriorate the final reconstruction.

In this work, we present an end-to-end network architecture that takes pixel-wise visibility information into account. The depth map is estimated from multi-view images in a two-step manner.
First, matching is performed for each reference-source image pair and a latent volume representing the pair-wise matching quality is obtained. This volume further regresses to an intermediate estimation of a depth map and an uncertainty map, where the uncertainty is transformed from the depth-wise entropy of the probability volume. 
Second, to attenuate unmatchable pixels, we fuse all pair-wise latent volumes to one multi-view cost volume by using pair-wise matching uncertainties as weighting guidance. The fused volume is regularized and regresses to the final depth estimation. 
We also integrate several practical components from recent MVS networks, including group-wise correlation and \cite{guo2019group} coarse-to-fine strategy \cite{gu2020cascade} to further boost the overall reconstruction quality. 
Our network is end-to-end trainable and the uncertainty part is trained in an unsupervised manner. In this case, we can directly utilize existing MVS datasets with only ground truth depth maps to train the visibility-aware MVS network. 


The proposed Vis-MVSNet is evaluated on \textit{DTU} \cite{jensen2014large} and \textit{BlendedMVS} \cite{yao2020blendedmvs} datasets and is benchmarked on \textit{Tanks and Temples} \cite{knapitsch2017tanks} dataset. Our method ranks  among all submissions in the \textit{Tanks and Temples} online benchmark (until May 1, 2020). Comparisons with previous methods and ablation studies in the experiment section demonstrate the significant improvement bought by our approach, especially when the occlusion problem is severe in input images.

\section{Related Work}

\begin{figure}[]
	\centering
	\includegraphics[width=\textwidth]{method2.pdf}
	\vspace{-8mm}
	\caption{Illustration of the visibility-awared fusion. For each reference-source pair, the uncertainty map successfully estimates the visibility of the pixels, and the depths of the occluded pixels are not correct. During the fusion, the occluded pixels are attenuated, resulting in a well reconstructed final depth map. }
	\vspace{-5mm}
	\label{fig:method2}
\end{figure}

\paragraph{Learning-based MVS}
Learning-based methods have shown great potentials to replace each step in traditional MVS reconstructions. The learnable multi-view cost metric \cite{hartmann2017learned} is first proposed to measure the multi-view photo-consistency between image patches. Later, SurfaceNet \cite{ji2017surfacenet} is proposed to learn the cost volume regularization from geometry ground truth. The authors of LSM \cite{kar2017learning} apply the differentiable projection in the network and propose the first end-to-end learnable network for low-resolution MVS reconstruction. DeepMVS \cite{huang2018deepmvs} reprojects images to 3D plane-sweeping volumes, performs intra-volume aggregation, and applies inter-volume aggregation to fuse the volumes and generate the depth map output. RayNet \cite{paschalidou2018raynet} encodes the camera projection to the network, and utilizes the Markov Random Field to predict the surface label. 

Another recent popular network for MVS reconstruction is MVSNet \cite{yao2018mvsnet}. MVSNet first extracts deep image features and then warps these features into the reference camera frustum to build a cost volume via differentiable homographies. To reduce the memory consumption during the network inference, the follow-up R-MVSNet \cite{yao2019recurrent} replaces the 3D CNNs regularization module with a 2D GRU recurrent network. 
Point-MVSNet \cite{chen2019point} proposes a point-based depth map refinement network to improve the output accuracy and MVS-CRF \cite{xue2019mvscrf} introduces the conditional
random field optimization during the depth map estimation. More recently, CasMVSNet \cite{gu2020cascade}, CVP-MVSNet \cite{yang2020cost} and UCSNet \cite{cheng2020deep} integrate the coarse-to-fine strategy to the learning-based MVS reconstruction. These works preserve an image feature pyramid and generate an initial depth estimation with large depth interval at a low resolution. In following stages, cost volumes are constructed with a narrow depth range centering at the depth estimation from previous stages. The coarse-to-fine architecture successfully reduces memory consumption so that they support deeper backbone networks and higher resolution outputs. However, these methods all apply a variance-based cost metric, which is under the assumption that a given pixel is visible in all input images. As a result, an increasing number of input images would lead to even a worse depth map estimation quality.

\vspace{-5mm}\paragraph{Visibility Estimation}
Visibility estimation is a well-acknowledged problem in classic MVS reconstructions. Previous works include heuristic cost thresholding methods \cite{furukawa2009accurate,tola2012efficient,xu2019multi} and more complicated joint depth-visibility estimation methods \cite{zheng2014patchmatch, schonberger2016pixelwise}. For latter approaches, the per-pixel visibility is usually jointly recovered during the depth map estimation process through an EM-based method. However, these methods apply a probabilistic framework which is hard to be directly integrated with deep neural networks. To handle the visibility issue in the learning-based frameworks, we should consider other alternatives for joint depth map and visibility estimation.

Current deep learning methods take visibility into account in an implicit manner. MVSNet \cite{yao2018mvsnet} reduces the feature volumes from different source views by variance metric which considers each view equally and claims that information from invisible pixels can be filtered out in the regularization. Such implicit method heavily relies on the regularization of the neural network. Besides, DeepMVS \cite{huang2018deepmvs} applies max pooling of multiple feature volumes to select the best latent representation, which is expected to be generated from a matchable pair. However, the fused volume is only related to the information from the best view, which loses the advantage of MVS that a more robust prediction can be produced by multiple observation. Instead, we start from pair-wise cost volumes to identify the pair-wise matching quality, and fuse the pair-wise volumes by weighted sum where weights of unmatchable pairs are reduced. 

\vspace{-5mm}\paragraph{Uncertainty Estimation}
In our approach, visibility is indicated by the matching uncertainty of the pair-wise depth map. Uncertainty (or confidence) estimation for two-view depth or disparity estimation has been widely studied for classic methods by Hu and Mordohai \cite{hu2012quantitative}. The majority of such methods examine the properties of the probability distribution over all the depth or disparity hypotheses. End-to-end deep neural networks \cite{poggi2016learning, kim2018unified, tosi2018beyond, kim2019laf} are also applied to estimate the uncertainty map for two-view stereo. Recently, Kendall and Gal \cite{kendall2017uncertainties} propose to jointly estimate the network output and its uncertainty based on the Bayesian neural network. However, this method cannot be directly adopted in our framework because they operate on 2D outputs, while we believe that it is more reasonable to estimate uncertainty from the 3D probability volume. Therefore we follow \cite{zhang2020learning} to use the depth-wise entropy of the probability volume to explicitly measure the pair-wise matching uncertainty. 



\begin{figure}[]
	\centering
	\includegraphics[width=\textwidth]{method.pdf}
	\vspace{-8mm}
	\caption{The proposed framework. For every reference-source pair, we jointly infer the depth map and the uncertainty map. The latent volumes are fused according to the uncertainty. And the fused volume is further regularized for the final depth map regression. *The feature maps. The images here only show the original image of the feature maps. }
	\vspace{-3mm}
	\label{fig:framework}
\end{figure}

\section{Method}


\subsection{Overview}\label{method_overview}
The outline of the framework is illustrated in Fig.\ \ref{fig:framework}. Given a reference image  and a set of neighboring source images , the framework predicts a reference depth map  aligned with . In our network, we apply the coarse-to-fine depth estimation strategy as recent networks \cite{gu2020cascade}. First, all images are fed into a 2D UNet \cite{ronneberger2015u} which extracts the multi-scale image features. The extracted features at the last three scales in the decoder part are preserved and will be used to construct cost volumes at three different resolutions. For the reconstruction at the -th stage, the cost volume will be regularized and produce a depth map  with the same resolution to the input feature map. Intermediate depth maps from previous stages will be used for the cost volume construction at next stages and  will be served as the final output  of the system.

The network details within the -th stage are described as follows. First, pair-wise cost volumes are constructed for each reference-source pairs. 
For the -th pair, by assuming that the reference image has depth , we can obtain a reprojected feature map  from the source view. The groupwise correlation \cite{guo2019group} between the reference and the warped source feature map is calculated as the cost map. Then the cost maps for all the depth hypothesis are stacked together as the cost volume. The resulting cost volume  of the -th image pair in the -th stage is of size , where  is the depth hypothesis number in the -th stage and  is the group number of the group-wise correlation operation.
The set of the hypotheses is predetermined for the first stage, and is dynamically determined for the second and third stages according to the depth map output of the previous stage. The calculation of the dynamic depth range will be explained in Sec.\ \ref{method_cas}. 


The regularization of the cost volume consists of two steps. First, every pair-wise cost volume is regularized to a latent volume  separately. Then, all latent volumes are fused to  which is further regularized to probability volume  and regresses to the final depth map of the current stage  via \textit{soft-argmax} \cite{kendall2017end} operation. The fusion of the latent volumes is visibility-awared. First, we measure the visibility by jointly inferring pair-wise depth and uncertainty. Each latent volume is transformed to a probability volume  through additional 3D CNNs and the \textit{softmax} operation. The depth map  and the uncertainty map  are jointly inferred via \textit{soft-argmax} and \textit{entropy} operation, which will be explained in Sec.\ \ref{method_uncertainty}. Then the uncertainty maps join the volume fusion as the weighting guidance, which is further described in Sec.\ \ref{method_fusion}.

\subsection{Uncertainty Estimation}\label{method_uncertainty}
In current learning-based MVS, the depth map is usually regressed from probability volume via the \textit{soft-argmax} operation. For simplicity, the stage number  is omitted below. We denote the probability distribution over all the depth hypotheses as . The \textit{soft-argmax} operation is equivalent to computing the expectation of this distribution and  is computed as:



To jointly regress the depth estimation and its uncertainty, we assume that the depth estimation follows the Laplacian distribution \cite{kendall2017uncertainties}. In this case, the estimated depth and the uncertainty maximize the likelihood of the observed ground truth: . Notice that the probability distribution  also reflects the matching quality. We thus apply the entropy map  of  to measure the depth estimation quality. And the uncertainty map  is transformed from  by a function , which is presented as a shallow 2D CNN in the network. 

The reason of adopting the entropy is that the randomness of the distribution is negatively related to the uni-modal distribution. And the uni-modality is an indicator of high confidence.

To jointly learn the depth map estimation  and its uncertainty , we minimize the negative log likelihood described above. 

Constants are omitted in the formula. For numerical stability, in practice we infer  instead of  directly. The log uncertainty map  is also transformed from the entropy map  by a shallow 2D CNN. 

The loss can also be interpreted as applying attenuation to the  loss between the estimation and the ground truth with a regularization term. The intuition is that the interference from the erroneous samples should be reduced. 

\subsection{Volume Fusion}\label{method_fusion}
In this section we introduce the visibility-aware volume fusion. For simplicity, the stage number  is omitted. Given the pair-wise latent cost volumes , a single volume  is fused from the volumes by weighted sum, where the weight is negatively related to the estimated pair-wise uncertainty. 

The pixels with large uncertainty are more likely to be located in the occluded regions, and thus the values in the latent volume should be attenuated. The attenuation scale is chosen to be identical with the one in the joint loss (Eq.\ \ref{eq:joint_loss}). 

An alternative to the weighted sum is applying threshold for  and perform a hard visibility selection for each pixel. However, lacking an interpretation of the value , we can only have an empirical threshold that may not be universal. Instead, the volumes are summed with normalized weight, which considers  in a relative manner. 

\subsection{Coarse-to-fine Architecture}\label{method_cas}
Our coarse-to-fine architecture mainly follows the recent Cas-MVSNet \cite{gu2020cascade}. In all the stages, depth hypothesis are uniformly sampled from a depth range. The first stage takes image features at low resolution and constructs cost volume with the predetermined depth range but larger depth interval, while the following stages use high spatial resolution, narrower depth range and smaller depth interval. 

For the first stage, the depth range is  and the depth number is , where ,  and  is predetermined. For the -th stage (), the depth range, sample number and interval are reduced. And the ranges are centered at the depth estimation from the previous stage, which are different for each pixels. The depth range for pixel  is  and the depth number is , where  and  are the predefined scaling factors, and  is the final depth estimation of pixel  from the last stage . 

\subsection{Training Loss}\label{method_loss}
For each stage, the loss is the combination of the pair-wise  loss, the pair-wise joint loss and the  loss of the final depth map. And the total loss is the weighted sum of the loss from three stages. For all the losses derived from the absolute difference between the estimation and the ground truth, the per-pixel differences are divided by the depth interval of the final stage. 

The pair-wise  losses are included because the uncertainty loss tends to over-relax the pair-wise depth and uncertainty estimation. The pair-wise  losses here could guarantee a qualified pair-wise depth map estimation.



\begin{figure}[]
	\centering
	\resizebox{\textwidth}{!}{\begin{tabular}{@{\hskip2pt}c@{\hskip2pt}@{\hskip2pt}c@{\hskip2pt}@{\hskip2pt}c@{\hskip2pt}@{\hskip2pt}c@{\hskip2pt}}
			\includegraphics[width=.15\linewidth]{Family.png} &
			\includegraphics[width=.30\linewidth]{Panther.png} &
			\includegraphics[width=.20\linewidth]{Horse.png} &
			\includegraphics[width=.25\linewidth]{Playground.png} \\
			Family & Panther & Horse & Playground \\
			\includegraphics[width=.1\linewidth]{Francis.png} &
			\includegraphics[width=.27\linewidth]{Train.png} &
			\includegraphics[width=.18\linewidth]{Lighthouse.png} &
			\includegraphics[width=.27\linewidth]{M60.png} \\
			Francis & Train & Lighthouse & M60
		\end{tabular}
	}
	\caption{Qualitative result of the point cloud on the \textit{intermediate set} of \textit{Tanks and Temples}. }
	\vspace{-3mm}
	\label{fig:tanks}
\end{figure}

\section{Experiment}

\subsection{Implementation}\label{sec:implementation}
\paragraph{Training}
Our network is trained on \textit{BlendedMVS} \cite{yao2020blendedmvs} training set for most experiments (Sec. \ref{sec:tnt} and \ref{sec:ablation}) and is trained on DTU training set \cite{jensen2014large} for DTU benchmarking (Sec. \ref{sec:dtu}). For both training sets, we use the input image size of  and output depth map size of . Source images for the given reference are selected as previous methods \cite{yao2018mvsnet,yao2019recurrent} and we set the number of source views to  during training. For depth samples at different stages, we set the depth hypothesis numbers to , and depth range scaling factors  respectively. The loss weights for each stage . The network is trained for 160k iterations with a batch size of 2 by an Adam \cite{kingma2014adam} optimizer. The initial learning rate is 0.001 and is halved at the 100k, 120k and 140k steps. All experiments are performed using one Nvidia GTX 1080Ti card.

\vspace{-5mm}\paragraph{Point cloud generation}
Similar to previous works, we apply depth map filter and fusion approaches to merge all depth maps into a unified point cloud output. Both photometric and geometric consistencies are considered in our depth map filter and fusion step. For the photometric consistency, we follow \cite{yao2018mvsnet} and generate probability maps to filter out unreliable pixels. The summation of probabilities of depth hypothesis within range  are calculated as the probability map of a given depth map output. Moreover, in our coarse-to-fine architecture, we consider all probability maps at different stages, and the filtering criterion is that a pixel in a reference view will be preserved if and only if all probability maps from all three stages are higher than the corresponding thresholds . For geometric consistency, we preserve pixels whose depth estimation is consistent with the reprojected depth from at least  views \cite{yao2018mvsnet}. Finally, the median depth map fusion is applied to refine all depth maps. The 3D point cloud is obtained by projecting all refined depth maps into the 3D space. 

\begin{table}[]
	\centering
	\resizebox{\textwidth}{!}{\begin{tabular}{l|ccccccccc|ccc}
			\specialrule{.2em}{.1em}{.1em}
			& \multicolumn{9}{c|}{\textit{Tanks and Temples}} & \multicolumn{3}{c}{\textit{DTU (\textit{mm})}} \\
			& Mean & Family & Francis & Horse & Lighthouse & M60 & Panther & Playground & Train & Acc. & Comp. & Overall\\ \hline
			COLMAP \cite{schonberger2016pixelwise} & 42.14 & 50.41 & 22.25 & 25.63 & 56.43 & 44.83 & 46.97 & 48.53 & 42.04 & 0.400 & 0.664 & 0.532 \\ 
			MVSNet \cite{yao2018mvsnet} & 43.48 & 55.99 & 28.55 & 25.07 & 50.79 & 53.96 & 50.86 & 47.90 & 34.69 & 0.396 & 0.527 & 0.462 \\ 
			Point-MVSNet \cite{chen2019point} & 48.27 & 61.79 & 41.15 & 34.20 & 50.79 & 51.97 & 50.85 & 52.38 & 43.06 & 0.342 & 0.411 & 0.376 \\
			CVP-MVSNet \cite{yang2020cost} & 54.03 & 76.50 & 47.74 & 36.34 & 55.12 & 57.28 & 54.28 & 57.43 & 47.54 & \textbf{0.296} & 0.406 & 0.351 \\
			UCSNet \cite{cheng2020deep} & 54.83 & 76.09 & 53.16 & 43.03 & 54.00 & 55.60 & 51.49 & 57.38 & 47.89 & 0.338 & \textbf{0.349} & \textbf{0.344} \\
			CasMVSNet \cite{gu2020cascade} & 56.84 & 76.37 & 58.45 & 46.26 & 55.81 & 56.11 & 54.06 & 58.18 & 49.51 & 0.325 & 0.385 & 0.355 \\
			ACMM \cite{xu2019multi} & 57.27 & 69.24 & 51.45 & 46.97 & 63.20 & 55.07 & \textbf{57.64} & 60.08 & \textbf{54.48} & - & - & - \\ \hline
			Vis-MVSNet & \textbf{60.03} & \textbf{77.40} & \textbf{60.23} & \textbf{47.07} & \textbf{63.44} & \textbf{62.21} & 57.28 & \textbf{60.54} & 52.07 & 0.369 & 0.361 & 0.365 \\
			\specialrule{.2em}{.1em}{.1em}
		\end{tabular}
	}
	\caption{Quantitative result of the point cloud on the \textit{intermediate set} of \textit{Tanks and Temples} and the test set of \textit{DTU}. The proposed method achieves the best mean F-score among the listed works on \textit{Tanks and Temples} and comparable overall distance on \textit{DTU}. }
	\vspace{-4mm}
	\label{tab:tanks}
\end{table}

\subsection{Benchmarking on Tanks and Temples Dataset}\label{sec:tnt}

We first evaluate our method on the \textit{intermediate set} of \textit{Tanks and Temples} dataset \cite{knapitsch2017tanks}. As mentioned in Sec. \ref{sec:implementation}, we use the BlendedMVS training set \cite{yao2020blendedmvs} to train the network. BlendedMVS is a recent MVS dataset containing 113 indoor and outdoor scenes with 16904 MVS training samples in total. The dataset is split into 106 training scenes and 7 validation scenes. The trained model is directly applied to the \textit{Tanks and Temples} benchmarking without fine-tuning. 


We use an input image size of  for reconstructions on the \textit{Tanks and Temples} dataset. The source image number is set to  for network inference and we choose ,  for depth map filter and fusion. Quantitative results are shown in Tab.\ \ref{tab:tanks} and corresponding point cloud reconstructions are illustrated in Fig.\ \ref{fig:tanks}. Our Vis-MVSNet achieves a mean F-score of 60.03 and ranks  among all the methods in the benchmark (until May 1, 2020), which outperforms all classical MVS methods \cite{schonberger2016pixelwise,xu2019multi} and recent learning-based approaches \cite{yao2018mvsnet,chen2019point,yang2020cost,cheng2020deep,gu2020cascade}. 


\subsection{Benchmarking on DTU Dataset}\label{sec:dtu}

The proposed method is also benchmarked on the DTU evaluation set \cite{jensen2014large}. \textit{DTU} dataset contains 128 scans under fixed camera trajectories and 7 sets of lighting configuration. Every scan has 49 views with given camera parameters. As suggested by previous methods\cite{ji2017surfacenet,yao2018mvsnet}, DTU dataset is split into training set, validation set and evaluation set. Our model is trained on the DTU training set, which is mentioned in Sec. \ref{sec:implementation}

For the depth map estimation, we use an input image size of  and a fixed depth range of  for all input images. The source image number is set to . We choose  and  for the depth map filter and fusion step. Quantitative results are shown in Tab.\ \ \ref{tab:tanks} and our method achieves a overall score of 0.365, which is comparable with other state-of-the-art methods. 

\begin{table}[]
	\begin{minipage}[b]{.55\linewidth}
		\strut\vspace*{-\baselineskip}\newline
		\centering
		\resizebox{\textwidth}{!}{\begin{tabular}{l|c|ccc}
				\specialrule{.2em}{.1em}{.1em}
				Setting & Fusion Method & Loss & <1 (\%) & <3 (\%) \\ \hline
				base-var & Variance                  & 1.50     & 79.31          & 92.25          \\
				base-ave  & Average                  & 0.999     & 83.03          & 94.95          \\
				base-max  & Max Pooling                & 0.956     & 84.71          & 95.19          \\
				base-vis  & Proposed        & 0.908     & 85.35          & 95.48          \\ \hline
				proposed  & + Coarse-to-fine & 0.759     & 90.86          & 96.05     \\
				\specialrule{.2em}{.1em}{.1em}
		\end{tabular}}
		\vspace{-4mm}\caption{Quantitative result of the depth map on the validation set of \textit{BlendedMVS} with . The settings with proposed fusion method achieve better results than others. }
		\label{tab:ablation}
	\end{minipage}
	\hspace{0.02\linewidth}
	\begin{minipage}[b]{.41\linewidth}
		\strut\vspace*{-\baselineskip}\newline
		\centering
		\includegraphics[width=\textwidth]{depth_curve-eps-converted-to.pdf}
		\vspace{-8mm}\captionof{figure}{Percentage of <1 of the depth maps on \textit{BlendedMVS} w.r.t. . }\vspace{-3mm}
		\label{fig:ablation}
	\end{minipage}
\end{table}

\subsection{Ablation Study}\label{sec:ablation}
In this section, we discuss other alternative volume fusion methods with implicit or explicit visibility awareness. To keep the simplicity of the network and clear demonstrate the effectiveness of the proposed component, we remove the coarse-to-fine architecture and directly use a MVSNet-like network as our baseline. The ablation study is performed on the BlendedMVS validation set and three types of evaluation metrics are considered: 1) the average L1 loss between the inferred depth map and the ground truth depth map; 2) the percentage of pixels with L1 error smaller than 1 depth-wise pixel ( percentage); and 3) the <3 percentage. Quantitative results are shown in Tab.\ \ref{tab:ablation} and Fig. \ref{fig:ablation}

\vspace{-4mm}\paragraph{Baseline}
In this setting (\textit{base-var}), we directly use the variance metric to fuse the feature volumes into one cost volume. The \textit{base-var} setting is widely adopted by MVSNet and its following works \cite{yao2018mvsnet,chen2019point,yang2020cost,cheng2020deep,gu2020cascade}. However, the variance operation is under the assumption that all pixels in the reference should be visible from all views. As a result, the increasing input image number would lead to even worse evaluation metrics (see Fig. \ref{fig:ablation})

\vspace{-4mm}\paragraph{Averaging}
In this setting (\textit{base-ave}), pair-wise cost volumes are fused to one multi-view volume by direct element-wise averaging. To fairly compare this setting with the proposed setting, we also apply the two step regularization as in the proposed framework. As is shown in Fig. \ref{fig:ablation}, the <1 percentage accuracy of the \textit{base-ave} is consistently increasing with the input image number. We believe the visibility information is implicitly encoded in the latent space and is dealt with by the two-step regularization. However, such implicit visibility awareness is apparently inferior to the proposed visibility fusion approach (see \textit{base-vis} in Tab.\ \ref{tab:ablation} and Fig. \ref{fig:ablation}).

\vspace{-4mm}\paragraph{Max Pooling}
In this setting (\textit{base-max}), the fused volume is obtained by finding the element-wise maximum of all the pair-wise volumes. This setting follows the fusion strategy of only considering the best matching pair among all reference-source image pairs. Similarly, all pair-wise losses are not counted toward the final loss. As is shown in Tab.\ \ref{tab:ablation} and Fig. \ref{fig:ablation}, \textit{base-max} outperforms \textit{base-ave} but is still inferior to the proposed \textit{base-vis}.

\vspace{-4mm}\paragraph{Weighted Averaging}
This setting (\textit{base-vis}) is the proposed Vis-MVSNet without the coarse-to-fine architecture. Compared with \textit{base-ave} and \textit{base-max}, this setting utilizes the intermediate uncertainty as the weighting guidance for the pair-wise volume fusion. As the result, the significance of invisible pixels will be explicitly reduced in the volume fusion step. 

The quantitative comparison is shown in Tab.\ \ref{tab:ablation} and Fig.\ \ref{fig:ablation}. A significant improvement can be observed after introducing the two step regularization to the baseline (\textit{base-ave} and \textit{base-max} v.s.\ \textit{base-var}). In addition, the proposed fusion further improves the result (\textit{base-vis} v.s.\ \textit{base-ave} and \textit{base-max}). Finally, the full model with coarse-to-fine architecture outperforms others by a significant margin (\textit{proposed} v.s.\ others).








\section{Conclusion}
We have presented a visibility-aware depth inference framework for multi-view stereo reconstruction. We have proposed the two-step cost volume regularization, the joint inference of the pair-wise depth and the uncertainty, and the weighted average fusion of pair-wise volumes according to the uncertainty maps. The proposed method has been extensively evaluated on several datasets,  demonstrating the effectiveness of the proposed visibility-aware depth inference framework.

\section{Acknowledgments}
This work is supported by Hong Kong RGC GRF 16206819 \& 16203518 and T22-603/15N.

\bibliography{egbib}
\end{document}
