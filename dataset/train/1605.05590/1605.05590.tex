\documentclass{article}

\usepackage[linkcolor=blue,citecolor=blue,colorlinks=true]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,left=25mm,right=25mm]{geometry}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[ruled]{algorithm2e}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subcaption}

\newtheorem*{definition*}{Definition}
\newtheorem{definition}{Definition}
\newtheorem{observation}{Observation}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{claim}{Claim} 
\newtheorem{corol}{Corollary}
\newtheorem{conj}{Conjecture} 
\newtheorem{fact}{Fact}
\newtheorem{corollary}{Corollary}

\newcommand{\dist}{\operatorname{dist}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\diversity}{div}
\DeclareMathOperator{\gendiv}{gen-div}
\DeclareMathOperator{\MST}{MST}
\DeclareMathOperator{\TSP}{TSP}

\newcommand{\prob}[1]{\operatorname{Pr}\left[#1\right]}

\renewcommand{\epsilon}{\varepsilon}
\newcommand{\BO}[1]{O\left( #1 \right)}
\newcommand{\BT}[1]{\Theta\left( #1 \right)}
\newcommand{\BOM}[1]{\Omega\left( #1 \right)}
\newcommand{\Let}[2]{#1  #2} \SetKw{To}{to}

\title{
  MapReduce and Streaming Algorithms for Diversity Maximization in 
  Metric Spaces of Bounded Doubling Dimension\footnote{This work was published in the Proocedings of the VLDB Endowment~\cite{CeccarelloPPU17}.}
}

\date{}

\newcommand{\affaddr}[1]{{\small #1}}  \newcommand{\email}[1]{{\tt\small #1}} 

\author{
  Matteo Ceccarello\and
  Andrea Pietracaprina \and
  Geppino Pucci\and
  Eli Upfal
  \\
  \affaddr{Department of Information Engineering, University of Padova,
    Padova, Italy}\\
  \email{\{ceccarel,capri,geppo\}@dei.unipd.it}
  \\
  \affaddr{Department of Computer Science,
    Brown University, Providence, RI USA}\\
  \email{eli\_upfal@brown.edu}
}


\begin{document}

\maketitle

\begin{abstract}
Given a dataset of points in a metric space and an integer , a
diversity maximization problem requires determining a subset of 
points maximizing some diversity objective measure, e.g., the minimum
or the average distance between two points in the subset.
Diversity maximization is computationally hard, hence only approximate
solutions can be hoped for. Although its applications are mainly in
massive data analysis, most of the past research on diversity
maximization focused on the sequential setting.  In
this work we present space and pass/round-efficient 
diversity maximization algorithms for the
Streaming and MapReduce models and analyze their approximation
guarantees for the relevant class of metric spaces of bounded doubling
dimension. Like other approaches in the literature, our algorithms
rely on the determination of high-quality core-sets, i.e.,
(much) smaller subsets of the input which contain good approximations
to the optimal solution for the whole input. For a variety of
diversity objective functions, our algorithms attain an
-approximation ratio, for any constant
, where  is the best approximation ratio achieved
by a polynomial-time, linear-space sequential algorithm for the same
diversity objective.  This improves substantially  over
the approximation ratios attainable in Streaming and MapReduce
by state-of-the-art algorithms for general metric
spaces. We provide extensive experimental evidence of the
effectiveness of our algorithms on both
real world and synthetic datasets, scaling up to over a
billion points.
\end{abstract}

\section{Introduction}

\emph{Diversity maximization} is a fundamental primitive in massive
data analysis, which provides a succinct summary 
of a dataset while preserving the diversity of the
data~\cite{AbbassiMT13,MasinB08,wu2013,YangMNFCH15}. This summary can be presented visually to the user or can be used as a core for further processing of the dataset. In this paper we present novel efficient algorithms for diversity
maximization in popular computation models for massive data processing,
namely Streaming and MapReduce.

\paragraph{Diversity Measures and their Applications:}
Given a dataset of
points in a metric space and a constant , a solution to the
diversity maximization problem is a subset of  points that
maximizes some diversity objective measure defined in terms of the
distances between the points.

Combinations of relevance ranking and diversity maximization have been
explored in a variety of applications, including web
search~\cite{AngelK11}, e-commerce~\cite{BhattacharyaGM11},
recommendation systems~\cite{YuLA09}, aggregate
websites~\cite{MunsonZR09} and query-result navigation~\cite{ChenL07} (see
\cite{RosenkrantzRT07,AbbassiMT13,IndykMMM14} for further references on the
applications of diversity maximization).  The common problem in all
these applications is that even after filtering and ranking for
relevance, the output set is often too large to be presented to the
user. A practical solution is to present a diverse subset of the
results so the user can evaluate the variety of options and possibly
refine the search.

There are a number of ways to formulate the goal of finding a set of
 points which are as diverse, or as far as possible from each
other. Conceptually, a -diversity maximization problem can be
formulated in terms of a specific graph-theoretic measure defined on
sets of  points, seen as the nodes of a clique where each edge is
weighted with the distance between its endpoints~\cite{ChandraH01}.
Several diversity measures are defined in
Table~\ref{tab:diversity-notions}.  While the most appropriate ones in
the context of web search, e-commerce, aggregator systems and query results
navigation are the remote-edge and the remote-clique measures
\cite{GollapudiS09,AbbassiMT13}, the results in this paper also extend
to the other measures in the table, which have important applications
in analyzing network performance, locating strategic facilities or
noncompeting franchises, or determining initial solutions for
iterative clustering algorithms or heuristics for hard optimization
problems such as
TSP~\cite{HalldorssonIKT99,ChandraH01,RosenkrantzRT07}. We include all
of these measures here to demonstrate the versatility of our approach
to a variety of diversity criteria. We want to stress that different
measures characterize the diversity of a set in a different fashion:
indeed, an optimal solution with respect to one measure is not
necessarily optimal with respect to another measure.

\paragraph{Distance Metric:} All the diversity criteria listed in
Table~\ref{tab:diversity-notions} are known to be NP-hard for general
metric spaces. Following a number of recent
works~\cite{AckermannBS10,ColeL06,KonjevodRX08,GottliebKK14,CeccarelloPPU15,CeccarelloPPU16},
we parameterize our results in terms of the \emph{doubling dimension}
of the metric space. Recall that a metric space has doubling dimension
 if any ball of radius  can be covered by at most  balls of
radius . While our methods yield provably tight bounds in spaces
of bounded doubling dimension (e.g., any bounded dimension Euclidian space) they have the ability of providing good approximations
in more general spaces based on important practical distance functions
such as the cosine distance in web search~\cite{AngelK11} and the dissimilarity
(Jaccard) distance in database queries~\cite{LeskovecRU14}.

\paragraph{Massive Data Computation Models:}
Since the applications of diversity maximization are
mostly in the realm of massive data analysis, it is important to
develop efficient algorithms for computational settings that can
handle very large datasets.  
\newcommand{\definitionTableVerticalSpacing}{\rule{0pt}{10pt}}
\begin{table}[t]
  \centering
  \begin{tabular}{llc}\toprule
    Problem
     & Diversity measure
     & Sequential approximation
    \\
    \midrule
    remote-edge
      & 
      & 2 (2)~\cite{Tamir91} 
    \\
    \definitionTableVerticalSpacing remote-clique
      &  
      & 2 ()~\cite{HassinRT97} 
    \\
    \definitionTableVerticalSpacing remote-star 
      &  
      & 2 ()~\cite{ChandraH01} 
    \\
    \definitionTableVerticalSpacing remote-bipartition
      & 
& 3 ()~\cite{ChandraH01} 
    \\
    \definitionTableVerticalSpacing remote-tree 
      &  
      & 4 (2)~\cite{HalldorssonIKT99} 
    \\
    \definitionTableVerticalSpacing remote-cycle 
      & 
      & 3 (2)~\cite{HalldorssonIKT99} 
    \\
    \bottomrule
  \end{tabular}
  \caption{Diversity measures considered in this paper.   (resp.,
    ) denotes the minimum weight of a spanning tree (resp.,
    Hamiltonian cycle) of the complete graph whose nodes are the points of
     and whose edge weights are the pairwise distances among the
    points.  The last column lists the best known approximation factor,
    the lower bound under the hypothesis  (in parentheses), and
    the related references. 
  }\label{tab:diversity-notions}
\end{table}
The Streaming and MapReduce models are widely recognized as suitable
computational frameworks for big-data processing.  The Streaming
model~\cite{RaghavanH99} copes with large data volumes through an
on-the-fly computation on the streamlined dataset, storing only very
limited information in the process, while the MapReduce
model~\cite{KarloffSV10,PietracaprinaPRSU12} enables the handling of
large datasets through the massive availability of resource-limited
processing elements working in parallel. The major challenge in
both models is devising strategies which work under the constraint
that the number of data items that a single processor can access
simultaneously is substantially limited.

\iffalse More specifically, in the
Streaming model we have a single processor with memory space that is
significantly smaller than the total size of the data stream, while,
in MapReduce, the local memory of each reducer (processor) is
polynomially sublinear in the total data size. An obvious question is
how this limitations affect the quality of the approximation for the
various diversity problems. In this work, we provide an answer to this
question, focusing on the case of metric spaces of bounded doubling
dimension, a class of metric spaces that has attracted significant
interest in the context of data analysis, and that includes the
important family of Euclidean spaces of constant dimension.  
\fi



\paragraph{Related work.}
\sloppy
Diversity maximization has been studied in the literature under
different names (e.g., -Dispersion, Max-Min Facility Dispersion,
etc.).  An extensive account of the existing formulations is provided
in~\cite{ChandraH01}. All of these problems are known to be NP-hard,
and several sequential approximation algorithms have been proposed.
Table~\ref{tab:diversity-notions} summarizes the best known results
for general metric spaces.  There are also some specialized results
for spaces with bounded doubling dimension: for the remote-clique
problem, a polynomial-time -approximation
algorithm on the Euclidean plane, and a polynomial-time
-approximation algorithm on -dimensional spaces with
rectilinear distances, for any positive constants  and
, are presented in~\cite{FeketeM04}.  In~\cite{HalldorssonIKT99} it
is shown that a natural greedy algorithm attains a 
approximation factor on the Euclidean plane for remote-tree. Recently,
the remote-clique problem has been considered under matroid
constraints~\cite{AbbassiMT13,CevallosEZ16}, which generalize the
cardinality constraints considered in previous literature.

In recent years, the notion of (composable) core-set has been
introduced as a key tool for the efficient solution of optimization
problems on large datasets.  A \emph{core-set}~\cite{AgarwalHV05},
with respect to a given computational objective, is a (small) subset
of the entire dataset which contains a good approximation to the
optimal solution for the entire dataset. A \emph{composable
  core-set}~\cite{IndykMMM14} is a collection of core-sets, one for
each subset in an arbitrary partition of the dataset, such that the
union of these core-sets contains a good core-set for the entire
dataset. The approximation factor attained by a (composable) core-set
is defined as the ratio between the value of the global optimal
solution and the value of the optimal solution on the (composable)
core-set. For the problems listed in
Table~\ref{tab:diversity-notions}, composable core-sets with constant
approximation factors have been devised in
\cite{IndykMMM14,AghamolaeiFZ15} (see
Table~\ref{tab:core-set-approximations}).  As observed
in~\cite{IndykMMM14}, (composable) core-sets may become key
ingredients for developing efficient algorithms for the MapReduce and
Streaming frameworks, where the memory available for a processor's
local operations is typically much smaller than the overall input
size.

In recent years, the characterization of data through the doubling
dimension of the space it belongs to has been increasingly used for
algorithm design and analysis in a number of contexts, including
clustering~\cite{AckermannBS10}, nearest neighbour
search~\cite{ColeL06}, routing~\cite{KonjevodRX08}, machine
learning~\cite{GottliebKK14}, and graph
analytics~\cite{CeccarelloPPU15, CeccarelloPPU16}.


\begin{table}
  \centering
  \begin{tabular}{lcc}\toprule
     & Previous~\cite{IndykMMM14,AghamolaeiFZ15}
     & Our results
    \\
     & {\footnotesize General metric spaces}
     & {\footnotesize Bounded doubling dimension}
    \\
    \midrule
    remote-edge
     & 
     & 
    \\
    remote-clique
&  
     & 
    \\
    remote-star
     &  
     & 
    \\
    remote-bipartition
     & 
     & 
    \\
    remote-tree
     &  
     & 
    \\
    remote-cycle
     &  
     & 
    \\
    \bottomrule
  \end{tabular}
  \caption{Approximation factors of the composable core-sets computed by our algorithm,
    compared with previous approaches.}\label{tab:core-set-approximations}
\end{table}

\paragraph{Our contribution.}
In this paper we develop efficient algorithms for diversity
maximization in the Streaming and MapReduce models.  At the heart of
our algorithms are novel constructions of (composable) core-sets.
In contrast to~\cite{IndykMMM14, AghamolaeiFZ15}, where
different constructions are devised for each diversity objective, we
provide a unique construction technique for all of the six objective
functions.  While our approach is applicable to general metric spaces,
on spaces of bounded doubling dimension, our (composable) core-sets
feature a  approximation factor, for any fixed
, for all of the six diversity objectives, with the
core-set size increasing as a function of
. The approximation factor is significantly better than
the ones attained by the known composable core-sets in general metric
spaces, which are reported in Table~\ref{tab:core-set-approximations}
for comparison.

Once a core-set (possibly obtained as the union of composable
core-sets) is extracted from the data, the best known sequential
approximation algorithm can be run on it to derive the final
solution. The resulting approximation ratio attained in this fashion
combines two sources of error: (1) the approximation loss in replacing
the entire dataset with a core-set; and (2) the approximation factor
of the sequential approximation algorithm executed on the core-set.
On metric spaces of bounded doubling dimension the combined
approximation ratio attained by our algorithms for any of the six
diversity objective functions considered in the paper is bounded by
, for any constant , where
 the is best approximation ratio achieved by a
polynomial-time, linear-space sequential algorithm for the same
maximum diversity criterion.

Our algorithms require only one pass over the data, in the streaming
setting, and only two rounds in MapReduce. To the best of our
knowledge, for all six diversity problems, our streaming algorithms
are the first ones that yield approximation ratios close to those of
the best sequential algorithms using space independent of input stream
size. Also, we remark that the parallel strategy at the base of the
MapReduce algorithms can be effectively ported to other models of
parallel computation.

Finally, we provide experimental evidence of the practical relevance
of our algorithms on both synthetic and real-world datasets. In
particular, we show that higher accuracy is achievable by increasing
the size of the core-sets, and that the MapReduce algorithm is
considerably faster (up to three orders of magnitude) than its
state-of-the-art competitors.  Also, we provide evidence that the
proposed approach is highly scalable. We want to remark that our work
provides the first substantial experimental study on the performance
of diversity maximization algorithms on large instances of up to
billions of data points.

The rest of the paper is organized as follows.  In
Section~\ref{sec:preliminaries}, we introduce some fundamental
concepts and useful notations. In Section~\ref{sec:basic-properties},
we identify sufficient conditions for a subset of points to be a
core-set with provable approximation guarantees. These properties are
then crucially exploited by the streaming and MapReduce algorithms
described in Sections~\ref{sec:streaming} and~\ref{sec:mapreduce},
respectively. Section~\ref{sec:generalized} discusses how the higher
memory requirements of four of the six diversity problems can be
reduced, while Section~\ref{sec:experiments} reports on the results of
the experiments.

\section{Preliminaries} \label{sec:preliminaries}

Let  be a metric space. The distance between two points
 is denoted by .  Moreover, we let
 denote the minimum distance between a
point  and an element of a set . Also,
for a point , the \emph{ball of radius  centered at
  } is the set of all points in  at distance at most 
from .
The \emph{doubling dimension} of a space is the smallest  such
that any ball of radius  is covered by  at most  balls of radius
~\cite{GuptaKL03}. As an immediate consequence, for any 
, any ball of radius  can be covered by at most  balls
of radius . 
For ease of presentation, in this paper we concentrate on metric
spaces of constant doubling dimension , although the results can be
immediately extended to nonconstant  by suitably adjusting the
ranges of variability of the parameters involved. Several relevant
metric spaces have constant doubling dimension, a notable case being 
Euclidean space of  constant dimension , which has doubling dimension
~\cite{GuptaKL03}.

Let  be a \emph{diversity
  function} that maps a set  to some nonegative real
number.
In this paper, we will consider the instantiations of function
 listed in Table~\ref{tab:diversity-notions}, which were
introduced and studied in~\cite{ChandraH01,IndykMMM14,AghamolaeiFZ15}.
For a specific diversity function , a set  of size  and a positive integer , 
the goal of the \emph{diversity
  maximization problem} is to find some subset  of size
 that maximizes the value . In the following, we
refer to the \emph{-diversity} of  as


The notion of \emph{core-set}~\cite{AgarwalHV05} captures the idea of
a small set of points that approximates some property of a larger
set.
\begin{definition}\label{def:composable}
Let  be a diversity function,  be a positive
  integer, and . A set , with , is a {\em -core-set} for  if
  
\end{definition}

In~\cite{IndykMMM14, AghamolaeiFZ15}, the concept of core-set is
extended so that, given an arbitrary partition of the input set, the
union of the core-sets of each subset in the partition is a core-set
for the entire input set.
\begin{definition}\label{def:composable-core-set}
  Let  be a diversity function,  be a positive
  integer, and . A function  that maps
   to one of its subsets computes a {\em
    -composable core-set} w.r.t.   if, for any
  collection of disjoint sets  with
  , we have
  
\end{definition}

Consider a set  and a subset . We
define the \emph{range} of  as
, and the \emph{farness} of 
as . Moreover, we
define the \emph{optimal range}  for  w.r.t.  to be the
minimum range of a subset of  points of . Similarly, we define
the \emph{optimal farness}  for  w.r.t.  to be the
maximum farness of a subset of  points of . Observe that
 is also the value of the optimal solution to the
remote-edge problem.

\section{Core-set characterization}\label{sec:basic-properties}

In this section we identify some properties that, when exhibited by a
set of points, guarantee that the set is a -core-set for
the diversity problems listed in Table~\ref{tab:diversity-notions}. In
the subsequent sections we will show how core-sets with these
properties can be obtained in the streaming and MapReduce settings. In
fact, when we discuss the MapReduce setting, we will also show that
these properties also yield composable core-sets featuring tighter
approximation factors than existing ones, for spaces
with bounded doubling dimension.

First, we need to establish a fundamental relation between the optimal
range  and the optimal farness  for a set .  To
this purpose, we observe that the classical greedy approximation
algorithm proposed in \cite{Gonzalez85} for finding a subset of
minimum range (\emph{-center problem}), gives in fact a good
approximation to both measures. We refer to this algorithm as {\sc
  GMM}.  Consider a set of points  and a positive integer . Let  be the subset of  points returned
by the algorithm for this instance. The algorithm initializes  with
an arbitrary point . Then, greedily, it adds to  the point
of  which maximizes the distance from the already
selected points, until  has size .  It is known that the
returned set  is such that ~\cite{Gonzalez85} and
it is easily seen that  (referred to as
\emph{anticover property}). This immediately implies the following
fundamental relation.
\begin{fact}\label{fact:opt-radius-farness}
Given a set  and , we have .
\end{fact}

Let  be a set belonging to a metric space of doubling dimension
. In what follows,  denotes the diversity
function of the problem under consideration, and  denotes an
optimal solution to the problem with respect to instance .
Consider a subset . Intuitively,  is a good core-set
for some diversity measure on , if for each point of the optimal
solution  it contains a point sufficiently close to it.  We
formalize this intuition by suitably adapting the notion of
\emph{proxy function} introduced in \cite{IndykMMM14}. Given a
core-set , we aim at defining a function
 such that the distance between  and  is
bounded, for any . For some problems this function will be
required to be injective, whereas for, some others, injectivity will not
be needed. We begin by studying the remote-edge and the remote-cycle problem.

\begin{lemma}\label{lem:remote-edge}
  For any given , let  be such that
  . A set  is a
  -core-set for the remote-edge and the remote-cycle problems if
   and there is a function  such that, for any
  , .
\end{lemma}
\begin{proof}
  Consider the remote-edge problem first, and observe that 
  . By
  applying the triangle inequality and the stated property of the proxy 
  function  we get
  
  Note that  does not need to be injective: in fact, if two
  points of the optimal solution are mapped into the same proxy, the
  first inequality trivially holds, its right hand side being zero.

  Consider now the remote-cycle problem.
  Note that . Let  and observe that .
Let  be the image of the proxy
  function. Following the argument given in
  \cite{IndykMMM14,AghamolaeiFZ15}, consider ,
  an optimal tour on . We build a weighted graph  whose vertex
  set is  and whose edges are those induced by
   plus two copies of edge , for
  each . The weight of an edge  is .  Clearly,
  the resulting graph  is connected and all its vertices have even
  degree, therefore it admits an Euler tour  of its edges. From
   we obtain a cycle  of  by shortcutting all nodes that
  are not in . By repeated applications of the triangle inequality
  during shortcutting and the fact that
  , we obtain:
  
  Therefore,
  .
  As in the case of the remote-edge problem, the injectivity of  is not necessary.
\end{proof}

Note that the proof of the above lemma does not require  to
be injective. Instead, injectivity is required for the remote-clique,
remote-star, remote-bipartition, and remote-tree problems, which are 
considered next.
\begin{lemma}\label{lem:remote-csbt}
For a given , let  be such that
. A set  is a
-core-set for the remote-clique, remote-star,
remote-bipartition, and remote-tree problems if
 and  there is an injective function  such that,
for any , .
\end{lemma}
\begin{proof}
  Observe that for each of the four problems it holds that
  . Let us consider the
  remote-clique problem first, and define
  
  Clearly, . By combining this observation with the
  triangle inequality we have
  
  The injectivity of  is needed in this case for the first
  inequality above to be true, since  distinct proxies are needed to get a
  feasible solution. The argument for the other problems is virtually
  identical, and we omit it for brevity.
\end{proof}


\section{Applications to data streams}
\label{sec:streaming}

In the Streaming model \cite{RaghavanH99} one processor with a
limited-size main memory is available for the computation. The input
is provided as a continuous stream of items which is typically too
large to fit in main memory, hence it must be processed on the fly
within the limited memory budget.  Streaming algorithms aim at
performing as few passes as possible (ideally just one) over the
input.

In~\cite{IndykMMM14}, the authors propose the following use of
composable core-sets to approximate diversity in the streaming
model. The stream of  input points is partitioned into 
blocks of size  each, and a core-set of size  is
computed from each block and kept in memory. At the end of the pass,
the final solution is computed on the union of the core-sets, whose
total size is . In this section, we show that substantial
savings (a space requirement independent of ) can be obtained by
computing a \emph{single} core-set from the entire stream through two
suitable variants of the 8-approximation \emph{doubling algorithm} for
the -center problem presented in~\cite{CharikarCFM04}, which are
described below.

Let  be two positive integers, with . The first
variant, dubbed {\sc SMM}, works in phases and maintains in memory
a set  of at most  points. Each Phase~ is associated
with a distance threshold , and is divided into a \emph{merge
  step} and an \emph{update step}. Phase~1 starts after an
initialization in which the first  points of the stream are
added to , and  is set equal to
. At the beginning of Phase~,
with , the following invariant holds. Let  be the
prefix of the stream processed so far. Then:
\begin{enumerate}
\item , 
\item , with , we have 
\end{enumerate}

Observe that the invariant holds at the beginning of Phase~1.  The
merge step operates on a graph  where there is an edge
 between two points  if
. In this step, the algorithm seeks a maximal
independent set  of , and sets . The update
step accepts new points from the stream. Let  be one such new
point. If , the algorithm discards , otherwise it
adds  to . The update step terminates when either the stream
ends or the -st point is added to . At the end of the step,
 is set equal to . As shown in~\cite{CharikarCFM04}, at
the end of the update step, the set  and the threshold 
satisfy the above invariants for Phase~.

To be able to use {\sc SMM} for computing a core-set for our diversity
problems, we have to make sure that the set  returned by the
algorithm contains at least  points. However, in the algorithm
described above the last phase could end with . To fix this
situation, we modify the algorithm so to retain in memory, for the
duration of each phase, the set  of points that have been removed
from  during the merge step performed at the beginning of the
phase. Consider the last phase. If at the end of the stream we have
, we can pick  arbitrary nodes from  and add them
to . Note that we can always do so because
, where  is the independent set found
during the last merge step.

Suppose that the input set  belongs to a metric space with doubling
dimension . We have:
\begin{lemma}\label{lem:smm-properties}
  For any , let ,
  and let  be the set of points returned by {\sc SMM}. Then, given an arbitrary set  with ,
  there exist a function  such that, for any
  , .
\end{lemma}
\begin{proof}
  Let  to be the optimal range for  w.r.t. .  Also,
  let  be the range of  and let
   be the optimal farness for  w.r.t. .  Suppose that
  {\sc SMM} performs  phases.  It is immediate to
  see that . As was proved in~\cite{CharikarCFM04},
  , thus . Consider now
  an optimal clustering of  with  centers and range  and,
  for notational convenience, define . From
  the doubling dimension property, we know that there exist at most
   balls in the space (centered at nodes not necessarily in )
  of radius at most  which contain all of the points
  in . By choosing one arbitrary center in  for each such ball,
  we obtain a feasible solution to the -center problem for 
  with range at most . Consequently,
  . Hence, we have that
  . By
  Fact~\ref{fact:opt-radius-farness}, we know that
  . Therefore, we have
  .  Given a
  set  of size , the desired proxy function
   is the one that maps each point  to the closest
  point in . By the discussion above, we have that
  .
\end{proof}

For the diversity problems mentioned in Lemma~\ref{lem:remote-csbt},
we need that for each point of an optimal solution the final core-set
extracted from the data stream contains a \emph{distinct} point very
close to it.  In what follows, we describe a variant of {\sc SMM},
dubbed {\sc SMM-EXT}, which ensures this property. Algorithm {\sc
  SMM-EXT} proceeds as {\sc SMM} but maintains for each  a set
 of at most  delegate points close to , including 
itself. More precisely, at the beginning of the algorithm,  is
initialized with the first  points of the stream, as before, and
 is set equal to , for each . In the merge step
of Phase , with , iteratively for each point  not
included in the independent set , we determine an arbitrary point
 such that  and let  inherit
 points of . Note that one
such point  must exist, otherwise  would not be a maximal
independent set. Also, note that a point  may inherit
points from sets associated with different points not in . Consider
the update step of Phase  and let  be a new point from the
stream. Let  be the point currently in  which is closest
to . If  we add it to . If instead
 and , then we add  to , otherwise
we discard it. Finally, we define  to be the
output of the algorithm, and observe that .

\begin{lemma}\label{lem:smm-ext-properties}
  For any , let ,
  and let  be the set of points returned by {\sc
    SMM-EXT}. Then, given an arbitrary set 
  with , there exist an injective function 
  such that, for any ,
  .
\end{lemma}
\begin{proof}
  Let  be the range of , and
  suppose that {\sc SMM} performs  phases. By
  defining , and by reasoning as in the proof
  of Lemma~\ref{lem:smm-properties} we can show that
  . Consider a point
  . If  then we define . Otherwise, suppose
  that  is discarded during Phase~, for some , because either
  in the merging or in the update step the set  that was supposed
  to host it had already  points. Let  denote the set  at
  the end of Phase~, for any . A simple inductive argument
  shows that at the end of each Phase~, with 
  there is a point  such that  and
  . In particular, there exists a point
   such that  and
  .  Since
  , any point in  is at distance at most
   from , and , we can
  select a proxy  for  from the  points in  such that
   and
   is not a proxy for any other point of .
\end{proof}

It is easy to see that the set  characterized in
Lemma~\ref{lem:smm-properties} satisfies the hypotheses of
Lemma~\ref{lem:remote-edge}. Similarly,
the set  of Lemma~\ref{lem:smm-ext-properties} satisfies the
hypotheses of Lemma~\ref{lem:remote-csbt}. Therefore, as a consequence
of these lemmas, for metric spaces with bounded doubling dimension
, we have that {\sc SMM} and {\sc SMM-EXT} compute
-core-sets for the problems listed in
Table~\ref{tab:diversity-notions}, as stated by the following two
theorems.

\begin{theorem}\label{thm:streaming-remote-edge}
  For any , let  be such that
  , and let
  . Algorithm {\sc SMM}
  computes a -core-set for the remote-edge and
  remote-cycle problems using  memory.
\end{theorem}

\begin{theorem}\label{thm:streaming-remote-clique}
For any , let  be such that
, and let . 
  Algorithm {\sc SMM-EXT} computes a
  -core-set for the remote-clique, remote-star,
  remote-bipartition, and remote-tree problems using
   memory.
\end{theorem}
\paragraph{Streaming Algorithm.}  The core-sets discussed above
can be immediately applied to yield the following streaming algorithm
for diversity maximization.  Let  be the input stream of  points.
One pass on the data is performed using  {\sc SMM}, or
{\sc SMM-EXT}, depending on the problem, to compute a core-set 
in main memory. At the end of the pass, a sequential
approximation algorithm is run on the core-set to compute the final solution.
The following theorem is immediate.
\begin{theorem} \label{thm:streaming-1-pass} 
Let  be a stream of  points of
  a metric space of doubling dimension , and let  be a
  linear-space sequential approximation algorithm for any one of the
  problems of Table~\ref{tab:diversity-notions}, returning a solution
  , with ,
  for some constant . Then, for any ,
  there is a 1-pass streaming algorithm for the same problem yielding an
  approximation factor of , with memory
  \begin{itemize}
  \item  for the remote-edge
    and the remote-cycle problems;
  \item  for the remote-clique,
    the remote-star, the remote-bipartition, and the remote-tree
    problems.
  \end{itemize}
\end{theorem}

\section{Applications to MapReduce}
\label{sec:mapreduce}

Recall that a MapReduce (MR) algorithm \cite{KarloffSV10,
  PietracaprinaPRSU12} executes as a sequence of \emph{rounds} where,
in a round, a multiset  of key-value pairs is transformed into a
new multiset  of pairs by applying a given reducer function (simply
called \emph{reducer}) independently to each subset of pairs of 
having the same key. The model features two parameters  and
, where  is the total memory available to the computation,
and  is the maximum amount of memory locally available to each
reducer. Typically, we seek MR algorithms that, on an input of size
, work in as few rounds as possible while keeping  and
, for some .
 
Consider a set  belonging to a metric space of doubling dimension
, and a partition of  into  disjoints sets
 . In what follows, 
denotes the diversity function of the problem under consideration, and
 denotes an optimal solution to the problem with respect to
instance . Also, we let  be the
optimal farness for  w.r.t. , with , and let
 be the optimal farness for  w.r.t. .  Clearly,
, for every .

The basic idea of our MR algorithms is the following. First, each set
 is mapped to a reducer, which computes a core-set
.  Then, the core-sets are aggregated into one
single core-set  in one reducer, and a
sequential approximation algorithm is run on , yielding the final
output.  We are thus employing the composable core-sets framework
introduced in~\cite{IndykMMM14}.

The following Lemma shows that if we run Algorithm {\sc GMM} 
from Section~\ref{sec:basic-properties} on
each , with , and then take the union of the
outputs, the resulting set satisfies the hypotheses of
Lemma~\ref{lem:remote-edge}.

\begin{lemma}\label{lem:gmm-properties}
  For any , let , and
  let . Then, given
  an arbitrary set  with , there exist a
  function  such that for any ,
  .
\end{lemma}
\begin{proof}
  Fix an arbitrary index , with , and let
  , where  denotes the point
  added to  at the -th iteration of {\sc GMM}. Let
  also  and
  . From the anticover property
  exhibited by GMM, which holds for any prefix of points selected by
  the algorithm, we have
  .  Define
  . Since  can be covered with  balls
  of radius at most , and the space has doubling dimension ,
  then there exist  balls in the space (centered at nodes not
  necessarily in ) of radius at most  that
  contain all the points in . By choosing one arbitrary center in
   in each such ball, we obtain a feasible solution to the
  -center problem for  with range at most ,
  which implies that the cost of the optimal solution to -center
  is at most . As a consequence, {\sc GMM}
  will return a 2-approximate solution  to -center with
  , and we have
  . Let now
   and . We have that , hence, for any
  set , the desired proxy function  is
  obtained by mapping each  to the closest point in . By
  the observations on the range of , we have
  .
\end{proof}

For the diversity problems considered in Lemma~\ref{lem:remote-csbt}
(remote-cycle, remote-star, remote-bipartition, and remote-tree) the
proxy function is required to be injective. Therefore, we develop an
extension of the {\sc GMM} algorithm, dubbed {\sc GMM-EXT} (see
Algorithm~\ref{alg:GMM-EXT} above) which first determines a kernel
 of  points by running {\sc GMM} and then
augments  by first determining the clustering of  whose centers
are the points of  and then picking from each cluster its center
and up to  delegate points.  In this fashion, we ensure that each
point of an optimal solution to the diversity problem under
consideration will have a distinct close ``proxy'' in the returned set
.

\begin{algorithm}[t]
  \caption{{\sc GMM-EXT}()}
  \label{alg:GMM-EXT}
  \DontPrintSemicolon 
  \Let{}{}\;
  Let \;
  \Let{}{}\;
  \For{\Let{}{} \To } {
    \Let{}{h < j} \;
    \Let{}{   arbitrary  points in  }\;
    \Let{}{}\;
  }
  \Return{}\;
\end{algorithm}

As before, let  be disjoint subsets of a metric
space of doubling dimension . We have:
\begin{lemma}\label{lem:gmm-ext-properties}
  For any , let ,
  and let . Then, given an arbitrary set , with ,
  there exist an injective function  such that for
  any , .
\end{lemma}
\begin{proof}
  For any , let
   be the result of the
  invocation of {\sc GMM-EXT} on . By defining
   and by reasoning as in
  Lemma~\ref{lem:gmm-properties}, we have that the range of the set
   computed by the call to {\sc GMM} within {\sc
    GMM-EXT} is . Fix an
  arbitrary index , with , and consider, for
  , the sets  and  as determined by
  Algorithm {\sc GMM-EXT}, and define
  . Since
  , we can associate
  each point in  to a distinct proxy .
  Since both  and  belong to , by the triangle
  inequality we have that
  . Since the input sets
   are disjoint, then we have that all the
   are disjoint. This ensures that we can find a distinct
  proxy for each point of  in , hence,
  the proxy function is injective.
\end{proof}

The two lemmas above guarantee that the set of points obtained by
invoking {\sc GMM} or {\sc GMM-EXT} on the partitioned input complies
with the hypotheses of Lemmas~\ref{lem:remote-edge}
and~\ref{lem:remote-csbt} of
Section~\ref{sec:basic-properties}.  Therefore, for metric spaces with
bounded doubling dimension , we have that {\sc GMM} and {\sc
  GMM-EXT} compute -composable core-sets for the
problems listed in Table~\ref{tab:diversity-notions}, as stated by the
following two theorems.

\begin{theorem}\label{thm:mr-edge-cycle}
  For any , let  be such that
  , and let . The algorithm {\sc GMM} computes a -composable
  core-set for the remote-edge and remote-cycle problems.
\end{theorem}

\begin{theorem}\label{thm:mr-csbt}
  For any , let  be such that
  , and let
  . The algorithm {\sc
    GMM-EXT} computes a -composable core-set
  for the remote-clique, remote-star, remote-bipartition, and
  remote-tree problems.
\end{theorem}

\paragraph{MapReduce Algorithm.}  The composable core-sets discussed above
can be immediately applied to yield the following MR algorithm
for diversity maximization.  Let  be the input set of  points
and consider an arbitrary partition of  into  subsets , each of size . In the first round, each
 is assigned to a distinct reducer, which computes the
corresponding core-set , according to algorithms {\sc GMM}, or
{\sc GMM-EXT}, depending on the problem. In the second round, the
union of the  core-sets  is
concentrated within the same reducer, which runs a sequential
approximation algorithm on  to compute the final solution.
We have:
\begin{theorem} \label{thm-2-rounds} Let  be a set of  points of
  a metric space of doubling dimension , and let  be a
  linear-space sequential approximation algorithm for any one of the
  problems of Table~\ref{tab:diversity-notions}, returning a solution
  , with ,
  for some constant . Then, for any
  , there is a 2-round MR algorithm for the same
  problem yielding an approximation factor of , with
   and
  \begin{itemize}
  \item  for the remote-edge
    and the remote-cycle problems;
  \item  for the remote-tree,
    the remote-clique, the remote-star, and the remote-bipartition
    problems.
  \end{itemize}
\end{theorem}
\begin{proof}
Set  such that , and
recall the the remote-edge and the remote-cycle problems admit
composable core-sets of size , while the problems
remote-tree, remote-clique, remote-star, and remote-bipartition have
core-sets of size , with .  Suppose that
the above MR algorithm is run with  for the former
group of two problems, and  for the latter
group of four problems.  Observe that by the choice of  we have
that both the size of each  and the size of the aggregate set
 are , therefore the stipulated bounds on the local
memory of the reducers are met. The bound on the approximation factor
of the resulting algorithm follows from the fact that the
Theorems~\ref{thm:mr-edge-cycle} and~\ref{thm:mr-csbt} imply that, for
all problems,  and the properties of algorithm
 yield .
\end{proof}

Theorem~\ref{thm-2-rounds} implies that on spaces of constant doubling
dimension, we can get approximations to remote-edge and remote-cycle
in 2 rounds of MR which are almost as good as the best sequential
approximations, with polynomially sublinear local memory
, for values of  up to , while
for the remaining four problems, with polynomially sublinear local
memory  for values of ,
for . In fact, for these four latter problems and the
same range of values for , we can obtain substantial memory savings
either by using randomization (in two rounds), or, deterministically
with an extra round (as will be shown in
Section~\ref{sec:gen-mapreduce}). We have:
\begin{theorem}\label{thm-2-rounds-rand}
  For the problems of remote-clique, remote-star, remote-bipartition,
  and remote-tree, we can obtain a randomized 2-round MR algorithm
  with the same approximation guarantees stated in
  Theorem~\ref{thm-2-rounds} holding with high probability, and with
 
where  is the approximation guarantee given by the current
best sequential algorithms referenced in
Table~\ref{tab:diversity-notions}.
\end{theorem}
\begin{proof}
  We fix  and  as in the proof of
  Theorem~\ref{thm-2-rounds}, and, at the beginning of the first
  round, we use random keys to partition the  points of  among
  
  reducers. Fix
  any of the four problems under consideration and let  be a given
  optimal solution. A simple balls-into-bins argument suffices to show
  that, with high probability, none of the  partitions may
  contain more than  out of the 
  points of .  Therefore, it is sufficient that, within each subset
  of the partition, {\sc GMM-EXT} selects up to those many delegate
  points per cluster (rather than ).  This suffices to establish
  the new space bounds.
\end{proof}

The deterministic strategy underlying the 2-round MR algorithm can be
employed recursively to yield an algorithm with a larger (yet
constant) number of rounds for the case of smaller local memory
budgets. Specifically, let  be as in the
proof of Theorem~\ref{thm:mr-csbt}.  If , we may re-apply the
core-set-based strategy using  as the new input.  The following
theorem shows that this recursive
strategy can still guarantee an approximation comparable to the
sequential one as long as the local memory  is not too small.
\begin{theorem} \label{thm-multi-rounds} Let  be a set of 
  points of a metric space of doubling dimension , let and  be a
  linear-space sequential approximation algorithm for any one of the
  problems of Table~\ref{tab:diversity-notions}, returning a solution
  , with ,
  for some constant .  Then, for any
   and  there is an
  -round MR algorithm for the same problem
  yielding an approximation factor of , with 
  and
  \begin{itemize}
  \item  for the remote-edge
    and the remote-cycle problems;
  \item , for some  for the remote-clique,
    the remote-star, the remote-bipartition, and the remote-tree
    problems.
  \end{itemize}
\end{theorem}
\begin{proof}
  Let  be such that
   and recall that the the remote-edge and the remote-cycle
  problems admit composable core-sets of size ,
  while the problems remote-tree, remote-clique, remote-star, and remote-bipartition,
  have core-sets of size , with
  . We may apply the following recursive
  strategy. We partition the input set  into  sets of size
   and compute the corresponding core-sets. Let  be the union
  of these core-sets. If , then we recursively apply the
  same strategy using  as the new input set, otherwise, we send 
  to a single reducer where algorithm  is applied. By the choice of
  the parameters, it follows that in all cases 
  rounds suffice to shrink the input set to size at most . The
  resulting approximation factor with respect to 
  will then be at most
   
  where the last inequality follows from the known fact
   for every  and , and
  the observation that, by the choice of , we have
  .
\end{proof}


\section{Saving memory: generalized \mbox{core-sets}} \label{sec:generalized}

Consider the problems remote-clique, remote-star, remote-bipartition,
and remote-tree. Our core-sets for these problems are obtained by
exploiting the sufficient conditions stated in
Lemma~\ref{lem:remote-csbt}, which require the existence of an
injective proxy function that maps the points of an optimal solution
into close points of the core-set.  To ensure this property, our
strategy so far has been to add more points to the core-sets.  More
precisely, the core-set is composed by a kernel of  points,
augmented by selecting, for each kernel point, a number of up to 
delegate points laying within a small range.  This augmentation
ensures that for each point  of an optimal solution , there
exists a distinct close proxy among the delegates of the kernel point
closest to , as required by Lemma~\ref{lem:remote-csbt}.

In order to reduce the core-set size, the augmentation can be done
implicitly by keeping track only of the number of delegates that must
be added for each kernel point. A set of pairs  is then
returned, where  is a kernel point and  is the number of
delegates for  (including  itself).
The intuition behind this approach is the following. The set of pairs
described above can be viewed as a compact representation of a
multiset, where each point  of the kernel appears with multiplicity
. If, for a given diversity measure, we solve the natural
generalization of the maximization problem on the multiset, then we
can transform the obtained multiset solution into a feasible solution
for  by selecting, for each multiple occurrence of a kernel point,
a distinct close enough point in .  In what follows we illustrate
this idea in more detail.

Let  be a set of points. A \emph{generalized core-set}  for 
is a set of pairs  with  and  a positive
integer, referred to as the \emph{multiplicity} of , where the
first components of the pairs are all distinct. We define its
\emph{size}  to be the number of pairs it contains, and its
\emph{expanded size} as .
Moreover, we define the \emph{expansion} of a generalized core-set 
as the multiset  formed by including, for each pair
,  replicas of  in .

Given two generalized core-sets  and , we say that  is
a \emph{coherent subset} of , and write , if
for every pair  there exists a pair
 with . For a given diversity
function  and a generalized core-set  for , we
define the \emph{generalized diversity} of , denoted by
, to be the value of  when applied to its
expansion , where  replicas of the same point 
are viewed as  distinct points at distance 0 from one another.
We also define the \emph{generalized -diversity} of  as

Let  be a generalized core-set for a set of points . A set
 with  is referred to as a
\emph{-instantiation} of  if for each pair 
it contains  distinct delegate points (including ), each at
distance at most  from , with the requirement that the sets
of delegates associated with any two pairs in  are disjoint. The
following lemma
ensures that the difference between the generalized
diversity of  and the diversity of any of its
-instantiations is bounded.
 
\begin{lemma}\label{lem-gendiv}
  ~Let  be a generalized core-set for  with , and
  consider the remote-clique, remote-star, remote-bipartition, and
  remote-tree problems.  For any -instantiation  of 
  we have that
   
  where  for remote-clique,  for
  remote-star and remote tree, and
   for
  remote-bipartition.
\end{lemma}
\begin{proof}
 Recall that  is defined over the expansion 
 of  where each pair  is represented by 
 occurrences of .  We create a 1-1 correspondence between
  and  by mapping each occurrence of a point
  into a distinct proxy chosen among the delegates
 for  in . The lemma follows by noting both
  and  are expressed in terms of sums
 of  distances and that, by the triangle inequality, for any
 two points  in the multiset (possibly two occurrences of
 the same point ) the distance of the corresponding proxies is at
 least .
\end{proof}

It is important to observe that the best sequential approximation
algorithms for the remote-clique, remote-star, remote-bipartition, and
remote-tree problems (see Table~\ref{tab:diversity-notions}), which
are essentially based on either finding a maximal matching or running
{\sc GMM} on the input set
\cite{HassinRT97,ChandraH01,HalldorssonIKT99}, can be easily adapted
to work on inputs with multiplicities. We have:
\begin{fact}\label{fact:gen}
  The best existing sequential approximation algorithms for the
  remote-clique, remote-star, remote-bipartition, and remote-tree, can
  be adapted to obtain from a given generalized core-set  a
  coherent subset  with expanded size  and
  , where  is
  the same approximation ratio achieved on the original problems.
  The adaptation works in space .
\end{fact}
\subsection{Streaming}
Using generalized core-sets we can lower the memory requirements for
the  remote-tree, remote-clique, remote-star, and remote-bipartition
problems to match the one of the other two problems, at the expense of
an extra pass on the data. We have:
\begin{theorem}\label{thm-2-pass-gen}
  For the problems of remote-clique, remote star, remote-bipartition,
  and remote-tree, we can obtain a 2-pass streaming algorithm with
  approximation factor  and memory
  , for any , where
   is the approximation guarantee given by the current best
  sequential algorithms referenced in
  Table~\ref{tab:diversity-notions}.
\end{theorem}
\begin{proof}
Let  be such that , and observe that . In the first pass we determine a generalized
core-set  of size  by
suitably adapting the {\sc SMM-EXT} algorithm to maintain counts
rather than delegates for each kernel point. Let  denote the
maximum distance of a point of  from the closest point  such
that  is in .  Using the argument in the proof of
Lemma~\ref{lem:smm-properties}, setting , it is easily shown that . Therefore, we can establish an
injective map  from  to the expansion  of
. Let us focus on the remote-clique problem (the argument for the
other three problems is virtually identical), and define . By reasoning as in the proof of
Lemma~\ref{lem:remote-csbt}, we can show that .

At the end of the pass, the best sequential algorithm for the problem,
adapted as stated in Fact~\ref{fact:gen}, is used to compute in memory
a coherent subset  with  and
such that
.  The second pass
starts with  in memory and computes an -instantiation
 by selecting, for each pair , 
distinct delegates at distance at most
 from .  Note that
a point from the data stream could be a feasible delegate for multiple
pairs. Such a point must be retained as long as the appropriate
delegate count for each such pair has not been met.  By applying
Lemma~\ref{lem-gendiv} with ,
we get .
Since , the space required is
.
\end{proof}

\begin{table}
  \small
  \centering
  \begin{tabular}{l@{\hskip 2pt} | c@{\hskip 1pt} c@{\hskip 1pt} | c@{\hskip -1pt} c@{\hskip 0pt} c}
    \toprule
    Problem
     & \multicolumn{2}{c|}{Streaming}
     & \multicolumn{3}{c}{MapReduce}
    \\
& 1 pass
     & 2 passes
     & 2 rounds det.
     & 2 rounds randomized
     & 3 rounds det.
    \\
    \midrule
    r-edge 
     & \multirow{2}{*}{}
     & \multirow{2}{*}{}
     & \multirow{2}{*}{}
     & \multirow{2}{*}{}
     & \multirow{2}{*}{}
    \\
    r-cycle & & & & & \\
    \midrule
    r-clique 
     & \multirow{4}{*}{
       
       }
     & \multirow{4}{*}{
       
       }
     & \multirow{4}{*}{
       
       }
     & \multirow{4}{*}{
       
       }
     & \multirow{4}{*}{
       
       }
    \\
    r-star & & & & & \\
    r-bipartition & & & & & \\
    r-tree & & & & & \\
    \bottomrule
  \end{tabular}
  \caption{Memory requirements of our streaming and MapReduce approximation
    algorithms. (For MapReduce we report only the size of  since
     is always linear in .)  The approximation factor of each algorithm is ,
    where  is the constant approximation factor of the
    sequential algorithms listed in Table~\ref{tab:diversity-notions}.
    }\label{tab:mapreduce-streaming}
\end{table}

\subsection{MapReduce}
\label{sec:gen-mapreduce}
Let  be a diversity function,  be a positive
integer, and . A function  that maps a set of
points  to a generalized core-set  for  computes a
\emph{-composable generalized core-set} for  if,
for any collection of disjoint sets , we have that

Consider a simple variant of {\sc GMM-EXT}, which we refer to as {\sc
  GMM-GEN}, which on input ,  and  returns a generalized
core-set  of  of size  and extended size
 as follows: for each point  of the kernel set
 {\sc GMM}, algorithm {\sc GMM-GEN} returns a pair
 where  is equal to the size of the set 
computed in the -th iteration of the for loop of {\sc GMM-EXT}. 


\begin{lemma}\label{lem:gmm-gen-core-set}
For any , define .
Algorithm {\sc GMM-GEN} computes a -composable generalized
core-set for the remote-clique, remote-star, remote-bipartition, and
remote-tree problems, with .
\end{lemma}
\begin{proof}
Given a collection of disjoint sets , let 
{\sc GMM-GEN}, and .  Consider
the expansion  of .  Let us focus on the remote-clique
problem (the argument for the other three problems is virtually
identical) and define .  By
reasoning along the lines of the proof of
Theorem~\ref{thm-2-pass-gen}, we can establish an injective map  such that, for any , .  Let  be the generalized
core-set whose expansion into a multiset yields the  points of the
image of . We have:

\end{proof}
We are now able to show that {\sc GMM-GEN}
computes a high-quality -composable generalized core-set, which
can then be employed in a 3-round MR algorithm to approximate the
solution to the four problems under consideration with lower memory
requirements.
This result is summarized in the following theorem.
\begin{theorem}\label{thm:3-rounds-gen}
  For the problems of remote-clique, remote-star, remote-bipartition,
  and remote-tree, we can obtain a 3-round MR algorithm with
  approximation factor  and
  , for any ,
  where  is the approximation guarantee given by the current
  best sequential algorithms referenced in
  Table~\ref{tab:diversity-notions}.
\end{theorem}
\begin{proof}
  Consider the remote-clique problem (the argument for the other three
  problems is virtually identical) and define
  . Let  be such
  that  and observe that
  . Also, set
  .  For 
  consider a arbitrary partition of the input set  into 
  subsets  each of size
   each. In the first round, each reducer
  applies {\sc GMM-GEN} to a distinct subset  to compute
  generalized core-sets of size . In the second round, these
  generalized core-sets are aggregated in a single generalized
  core-set , whose size is  and such
  that the maximum distance of a point of  from the closest point
   with  is
  . Then, one reducer
  applies to  the best sequential algorithm for the problem,
  adapted as stated in Fact~\ref{fact:gen}, to compute a coherent
  subset  with  and such that

where the last inequality follows by Lemma~\ref{lem:gmm-gen-core-set}.
In the third round,  is distributed to  reducers which
are able to compute an instantiation  of  as
follows. For each pair , such that ,
the -th reducer selects  distinct delegates from  at
distance at most  from
. By Lemma~\ref{lem-gendiv}, we have that

As for the memory bound, we have that
.
\end{proof}

A synopsis of the main theoretical results presented in the paper is
given in Table~\ref{tab:mapreduce-streaming}.

\section{Experimental evaluation}
\label{sec:experiments}
We ran extensive experiments on a cluster of 16 machines, each
equipped with 18GB of RAM and an Intel I7 processor.
To the best of our knowledge, ours is the first work on diversity
maximization in the MapReduce and Streaming settings, which
complements theoretical findings with an experimental evaluation.
The MapReduce algorithm has been implemented within the Spark
framework, whereas the streaming algorithm has been implemented in
Scala, simulating a Streaming setting\footnote{The code is available
  as free software at \url{https://github.com/Cecca/diversity-maximization}}.
Since optimal solutions are out of reach for the input sizes that we
considered, for each dataset we computed approximation ratios with
respect to the best solution found by many runs of our MapReduce
algorithm with maximum parallelism and large local memory.
We run our experiments on both synthetic and real-world datasets.
Synthetic datasets are generated randomly from the three-dimensional
Euclidean space in the following way.  For a given ,  points are
randomly picked on the surface of the unit radius sphere centered at
the origin of the space, so to ensure the existence of a set of
far-away points, and the other points are chosen uniformly at random
in the concentric sphere of radius 0.8. Among all the distributions
used to test our algorithms, on which we do not report for brevity, we found that this is the most challenging, hence the more
interesting to demonstrate.
To test our algorithm on real-world workloads we used the
\emph{musiXmatch} dataset~\cite{BertinM11}.  This dataset contains
the lyrics of 237,662 songs, each represented by the vector of word
counts of the most frequent 5,000 words across the entire dataset.  The
dimensionality of the space of these vectors is therefore 5,000.  We
filter out songs represented by less than 10 frequent words, obtaining
a dataset of 234,363 songs.  The reason of this filtering is that one
can build an optimal solution using songs with short, non overlapping
word lists. Thus, removing these songs makes the dataset more challenging
for our algorithm.  On this dataset, as a distance between two vectors
 and , we use the \emph{cosine distance}, defined as
. This distance is closely
related to the \emph{cosine similarity} commonly used in Information
Retrieval \cite{LeskovecRU14}.
For brevity, we will report the results only for the remote-edge
problem.  We observed similar behaviors for the other diversity
measures, which are all implemented in our software.  All results
reported in this section are obtained as averages over at least 10
runs.

\subsection{Streaming algorithm}\label{sec:streaming-experiments}

The first set of experiments investigates the behavior of the
streaming algorithm for various values of , as well as the impact
of the core-set size, as controlled by the parameter , on the
approximation quality. The results of these experiments are reported
in Figure~\ref{fig:streaming-approx-musixmatch},
for the musiXmatch dataset, and Figure~\ref{fig:streaming-approx-synthetic}.
for a synthetic dataset of 100 million points, generated as explained above.

\begin{figure}[t]
  \begin{minipage}{.49\linewidth}
    \centering
    \includegraphics[width=\columnwidth]{lyrics-streaming-approx.pdf}
    \caption{Approximation ratio for the streaming algorithm for different values of  and  on the \emph{musiXmatch} dataset.}\label{fig:streaming-approx-musixmatch}
  \end{minipage}
  \hfill
  \begin{minipage}{.49\linewidth}
    \centering
    \includegraphics[width=\columnwidth]{synthetic-streaming-approx.pdf}
    \caption{Approximation ratios for the streaming algorithm for different values of  and  on a synthetic dataset of 100 million points.}\label{fig:streaming-approx-synthetic}
  \end{minipage}
\end{figure}

First, we observe that as  increases the remote-edge measure becomes harder to approximate: finding a higher number of diverse elements is more difficult.
On the real-world dataset, because of the high dimensionality of its
space, we test the influence of  on the approximation with a
geometric progression of 
(Figure~\ref{fig:streaming-approx-musixmatch}).  On the synthetic
datasets instead (Figure~\ref{fig:streaming-approx-synthetic}), since
 has a smaller doubling dimension, the effect of  is
evident already with small values, therefore we use a linear
progression.
As expected, by increasing  the accuracy of the algorithm increases in both
datasets. Observe that although the theory suggests that good
approximations require rather large values of , in practice our experiments show that
relatively small values of , not much larger than , already
yield very good approximations, even for the real-world dataset
whose doubling dimension is unknown.

In Figure~\ref{fig:throughput}, we consider the performance of the
kernel of streaming algorithm, that is, we concentrate on the time
taken by the algorithm to process each point, ignoring the cost of
streaming data from memory. The rationale is that data may be streamed
from sources with very different throughput: our goal is to show the
maximum rate that can be sustained by our algorithm independently of
the source of the stream.  We report results for the same combination
of parameters shown in Figure~\ref{fig:streaming-approx-musixmatch}.
As expected, the throughput is inversely proportional to both  and
, with values ranging from 3,078 to 544,920 points/s.  The
throughput supported by our algorithm makes it amenable to be used in
streaming pipelines: for instance, in 2013
Twitter\footnote{\url{https://blog.twitter.com/2013/new-tweets-per-second-record-and-how}}
averaged at 5,700 tweets/s and peaked at 143,199 tweets/s.  In this
scenario, it is likely that the bottleneck of the pipeline would be
the data acquisition rather than our core-set construction.

As for the synthetic dataset, the throughput of the algorithm exhibits
a behavior with respect to  and  similar to the one reported in
Figure~\ref{fig:throughput}, but with higher values ranging from
78,260 to 850,615 points/s since the distance function is cheaper to compute.

\begin{figure}[t]
  \begin{minipage}{.49\linewidth}
    \centering
    \includegraphics[width=\columnwidth]{throughput.pdf}
    \caption{Throughput of the kernel of the streaming algorithm on the \emph{musiXmatch} dataset.}
    \label{fig:throughput}
  \end{minipage}
  \hfill
  \begin{minipage}{.49\linewidth}
    \centering
    \includegraphics[width=\columnwidth]{synthetic-mapreduce-approx}
    \caption{Approximation ratios for the MR algorithm for different values of  and  on a synthetic dataset of 100 million points.}\label{fig:mapreduce-approximation}
  \end{minipage}
\end{figure}


\subsection{MapReduce algorithm} \label{subsec:MRexp}

We demonstrate our MapReduce algorithm on the same datasets used in
the previous section. For this set of experiments we fixed  and
we varied two parameters: size of the core-sets, as controlled by
, and parallelism (i.e., the number of reducers).  Because the
solution returned by the MapReduce algorithm for  turns out to
be already very good, we use a geometric progression for  to
highlight the dependency of the approximation factor on .  The
results are reported in Figure~\ref{fig:mapreduce-approximation}.  For
a fixed level of parallelism, we observe that the approximation ratio
decreases as  increases, in accordance to the theory.  Moreover,
we observe that the approximation ratios are in general better than
the ones attained by the streaming algorithm, plausibly because in
MapReduce we use a 2-approximation -center algorithm to build the
core-sets, while in Streaming only a weaker 8-approximation
-center algorithm is available.

Figure~\ref{fig:mapreduce-approximation} also reveals that if we fix
 and increase the level of parallelism, the approximation ratio
tends to decrease.  Indeed, the final core-set obtained by aggregating
the ones produced by the individual reducers grows larger as the
parallelism increases, thus containing more information on the input
set.  Instead, if we fix the product of  and the level of
parallelism, hence the size of the aggregate core-set, we observe that
increasing the parallelism is mildly detrimental to the approximation
quality.  This is to be expected, since with a fixed space budget in
the second round, in the first round each reducer is forced to build a
smaller and less accurate core-set as the parallelism increases.

The experiments for the real-world \emph{musiXmatch} dataset (figures
omitted for brevity) highlight that the {\sc GMM} -center
algorithm returns very good core-sets on this high dimensional
dataset, yielding approximation ratios very close to 1 even for low
values of . As remarked above, the more pronounced dependence on
 in the streaming case may be the result of the weaker approximation
guarantees of its core-set construction.


Since in real scenarios the input might not be distributed randomly
among the reducers, we also experimented with an ``adversarial''
partitioning of the input: each reducer was given points coming from a
region of small volume, so to obfuscate a global view of the
pointset. With such adversarial partitioning, the approximation ratios
worsen by up to . On the other hand, as  increases, the
time required by a random shuffle of the points among the reducers
becomes negligible with respect to the overall running time. Thus,
randomly shuffling the points at the beginning may prove
cost-effective if larger values of  are affordable.

\subsection{Comparison with state of the art}

In Table~\ref{tab:comparison}, we compare our MapReduce algorithm
(dubbed {\tt CPPU}) against its state of the art competitor presented
in~\cite{AghamolaeiFZ15} (dubbed {\tt AFZ}). Since no code was
available for {\tt AFZ}, we implemented it in MapReduce with the same
optimizations used for {\tt CPPU}.  We remark that {\tt AFZ} employs
different core-set constructions for the various diversity measures,
whereas our algorithm uses the same construction for all diversity
measures.  In particular, for remote-edge, {\tt AFZ} is equivalent to
{\tt CPPU} with , hence the comparison 
is less interesting and can be derived from the behavior
of {\tt CPPU} itself. Instead, for remote-clique, the core-set
construction used by {\tt AFZ} is based on local search and may
exhibit highly superlinear complexity.  For remote-clique, we
performed the comparison with various values of , on datasets of 4
million points on the 2-dimensional Euclidean space, using 16 reducers
({\tt AFZ} was prohibitively slow for higher dimensions and bigger
datasets).  The datasets were generated as described in the
introduction to the experimental section. Also, we ran {\tt CPPU} with
 in all cases, so to ensure a good approximation ratio at the
expense of a slight increase of the running time.
As Table~\ref{tab:comparison} shows, {\tt CPPU} is in all cases at least
three orders of magnitude faster than {\tt AFZ}, while achieving a better quality at the same time.


\begin{table}[t]
  \begin{minipage}{.43\linewidth}
    \centering
    \begin{tabular}{lrrrr}
      \toprule
      & \multicolumn{2}{c}{approximation} & \multicolumn{2}{c}{time (s)} \\
      k & {\tt AFZ} & {\tt CPPU} & {\tt AFZ} & {\tt CPPU} \\
      \midrule
      4 &        1.023 &     1.012 &     807.79 &  1.19 \\
      6 &        1.052 &     1.018 &    1,052.39 & 1.29 \\
      8 &        1.029 &     1.028 &    4,625.46 & 1.12 \\
      \bottomrule
    \end{tabular}
    \caption{Approximation ratios and running times
      of our MR algorithm ({\tt CPPU}) and {\tt AFZ}.}
    \label{tab:comparison}
  \end{minipage}
  \hfill
  \begin{minipage}{.54\linewidth}
    \centering
    \includegraphics[width=\columnwidth]{scalability-log}
    \captionof{figure}{Scalability of our algorithms for different number of points and processors. The running time for one processor is obtained with the streaming algorithm.}
    \label{fig:scalability}
  \end{minipage}
\end{table}

\subsection{Scalability}

We report on the scalability of our MR algorithm on datasets drawn
from , ranging from 100 million points (the same dataset
used in subsections~\ref{sec:streaming-experiments}
and~\ref{subsec:MRexp}) up to 1.6 billion points.
We fixed the size  of the memory required by the final reducer and varied the number of processors used.
On a single machine, instead of running MapReduce, which makes little sense, we run the streaming algorithm with , so to have a final coreset of the same size as the ones found in MapReduce runs.
For a given number of processors  and number of points , we run the corresponding experiment only if  points fit into the main memory of a single processor.
As shown in Figure~\ref{fig:scalability}, for a fixed dataset size,
our MapReduce algorithm exhibits super-linear scalability: doubling the number of processors results in a 4-fold gain in running time (at the expense of a mild worsening of the approximation ratio, as pointed out in Subsection~\ref{subsec:MRexp}).
The reason is that each reducer performs  work to
build its core-set, where  is the number of reducers, since the
core-set construction involves  iterations, with each
iteration requiring the scan of  points.

For the dataset with 100 million points, the MR algorithm outperforms
the streaming algorithm in every processor configuration. It must be
remarked that the running time reported in
Figure~\ref{fig:scalability} for the streaming algorithm takes into
account also the time needed to stream data from main memory (unlike
the throughput reported in Figure~\ref{fig:throughput}). This is to
ensure a fair comparison with MapReduce, where we also take into
account the time needed to shuffle data between the first and the
second round, and the setup time of the rounds.  Also, we
note that the streaming algorithm appears to be faster than what the
MR algorithm would be if executed on a single processor, and this is
probably due to the fact that the former is more cache
friendly.

If we fix the number of processors, we observe that our algorithm
exhibits linear scalability in the number of points.
Finally, in a set of experiments, omitted for brevity, we
verified that for a fixed number of processors the
time increases linearly with . Both these behaviors
are in accordance with the theory.


\section{Acknowledgments}

Part of this work was done while the authors were
visiting the Departiment of Computer Science
at Brown University.
This work was supported, in part, by MIUR of Italy under
project AMANDA, and by the University of Padova under project
CPDA152255/15: "Resource-Tradeoffs Based Design
of Hardware and Software for Emerging Computing Platforms".
The work of Eli Upfal was supported in part by NSF grant IIS-1247581
and NIH grant R01-CA180776.


\bibliographystyle{abbrv}
\bibliography{references}
\end{document}
