\documentclass{article}  

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsxtra}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage{bbm}
\usepackage{palatino}
\usepackage{float}
\usepackage{mathtools}
\usepackage{paralist}


\usepackage[pdftex, pagebackref, letterpaper=true, colorlinks=true,
pdfpagemode=none, urlcolor=blue, linkcolor=blue, citecolor=blue,
pdfstartview=FitH] {hyperref}

\usepackage{cleveref}

\def\llb{\llbracket}
\def\rrb{\rrbracket}
\def\llp{\mathbbm{\Big(}}\def\rrp{\mathbbm{\Big)}}\def\bmid{\ \big\vert\ }

\def\isom{\cong}
\def\triangleq{\overset{\mathrm{def}}{=}}
\newcommand{\defn}[1]{\index{#1} \textbf{#1}}



\let\eps=\varepsilon


\newtheorem{theorem}{Theorem}[section] 
\newtheorem{observation}{Observation}[section] 
\newtheorem{definition}{Definition}[section] 
\newtheorem{remark}{Remark}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{notation}{Notation}[section]
\crefname{assumption}{Assumption}{Assumptions}
\crefname{conjecture}{Conjecture}{Conjectures}
\crefname{claim}{Claim}{Claims}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{definition}{Definition}{Definitions}
\crefname{example}{Example}{Examples}
\crefname{exercise}{Exercise}{Exercises}
\crefname{fact}{Fact}{Facts}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{notation}{Notation}{Notations}
\crefname{note}{Notes}{Notes}
\crefname{observation}{Observation}{Observations}
\crefname{proposition}{Proposition}{Propositions}
\crefname{problem}{Problem}{Problems}
\crefname{question}{Question}{Questions}
\crefname{remark}{Remark}{Remarks}
\crefname{theorem}{Theorem}{Theorems}

\DeclareMathOperator{\poly}{poly}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}

\DeclareMathOperator{\vol}{vol}
\newcommand{\elp}{\mathbb{E}}
\newcommand{\sph}{\mathbb{B}}
\DeclareMathOperator{\relint}{relint}
\DeclareMathOperator{\conv}{convex}
\DeclareMathOperator{\aff}{affine}
\DeclareMathOperator{\cl}{cl}

\DeclareMathOperator{\cost}{cost}

\newcommand{\symm}{\mathbb{S}}
\newcommand{\sympm}{\mathbb{S}_{+}}
\newcommand{\symppm}{\mathbb{S}_{++}}

\newcommand{\sos}[2]{\Sigma_{{#1}_{\le #2}}}
\newcommand{\dsos}[2]{{\Sigma_{{#1}_{\le #2}}^\ast}}
\newcommand{\sosi}[1]{\Sigma_{{#1}}}
\newcommand{\dsosi}[1]{\Sigma_{{#1}}^\ast}
\newcommand{\mpoly}{\R[\xvar]/\mathcal{I}}
\newcommand{\mpolyi}[1]{{#1}[\xvar]/\mathcal{I}}
\def\exl{{\langle\!\langle}}
\def\exr{{\rangle\!\rangle}}
\def\bexl{{\bigg\langle\!\!\!\bigg\langle}}
\def\bexr{{\bigg\rangle\!\!\!\bigg\rangle}}
\newcommand{\mxpoly}{\R[\xvar]/\mathcal{I}(x)}
\newcommand{\mxpolyi}[1]{{#1}[\xvar]/\mathcal{I}(x)}
\DeclareMathOperator*{\vrty}{Variety}
\newcommand{\multlin}{\mathbb{ML}[\xvar]}
\newcommand{\multlini}[1]{\mathbb{ML}_{#1}[\xvar]}
\newcommand{\bin}{\mathcal{B}_V}
\newcommand{\repr}[1]{\big[ #1 \big]}
\def\ff{\setfam{F}}
\def\ide{\mathcal{I}}
\newcommand{\dims}{\Upsilon}

\let\proj=\Pi


\DeclareMathOperator{\tr}{Tr}
\renewcommand{\det}[1]{\left|#1\right|}
\newcommand{\unit}[1]{\overline{#1}}
\newcommand{\dunit}[1]{\overline{\overline{#1}}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\spn}{span}
\def\Lnorm{\mathcal{L}}
\def\Anorm{\mathcal{A}}
\def\Gnorm{\mathcal{G}}
\newcommand{\ind}[1]{\mathbbm{1}_{#1}}
\newcommand{\ones}{{1\!\! 1}}
\DeclareMathOperator{\supp}{support}
\DeclareMathOperator{\rank}{rank}

\let\es=\emptyset
\DeclareMathOperator{\mmt}{\mathbf{M}}
\newcommand{\dmmt}[1]{\widehat{#1}}
\DeclareMathOperator{\ex}{ex}
\newcommand{\rss}[2]{{{#1} \choose {\le #2}}}
\newcommand{\bestSi}{{\mathcal{S}^\ast}}
\newcommand{\bestS}{{\mathcal{S}^\ast}}
\newcommand{\lasserrei}[3]{\mathrm{Lasserre}_{#1}(#2,#3)}
\newcommand{\lasserreii}[2]{\mathrm{Lasserre}_{#1}(#2)}
\newcommand{\bestSf}{\|\xvec_{\bestS(f)}\|^2}
\newcommand{\bestSfi}[1]{\|\xvec_{\circ| \bestS(\circ)}\|^2}
\newcommand{\bestSfx}{{\|\xvec_{\circ| \bestS(f)}\|^2}}

\newcommand{\xvar}{\mathbf{X}} \newcommand{\xint}{\mathbf{x}} \newcommand{\xmmt}{x}
\newcommand{\xvec}{\vec{x}} \newcommand{\yvec}{\vec{y}} \newcommand{\xmat}{\vec{X}} \newcommand{\ymat}{\vec{Y}} \newcommand{\zmat}{\vec{Z}} 

\newcommand{\pvec}[1]{{\mathsf{#1}}}
\newcommand{\peval}[2]{\exl {#1}, {#2} \exr}
\newcommand{\setfam}[1]{\EuScript{#1}}
\DeclareMathOperator*{\degr}{degr}
\def\opt{\mathsf{OPT}}
\DeclareMathOperator{\OPT}{FIX ME}

\newcommand{\minbisec}{\textsc{\sf Minimum Bisection}}
\newcommand{\sse}{\textsc{\sf Small Set Expansion}}
\newcommand{\ug}{\textsc{\sf -Unique Games}}
\newcommand{\maxcut}{\textsc{\sf Maximum Cut}}
\newcommand{\maxbisec}{\textsc{\sf Maximum Bisection}}
\newcommand{\usc}{\textsc{\sf Uniform Sparsest Cut}}
\newcommand{\nusc}{\textsc{\sf Non-Uniform Sparsest Cut}}
\newcommand{\iset}{\textsc{\sf Independent Set}}
\newcommand{\lh}{\textsc{\sf Lasserre Hierarchy}}
\newcommand{\sah}{\textsc{\sf Sherali-Adams Hierarchy}}
\newcommand{\lsh}{\textsc{\sf L\'ovasz-Schrijver Hierarchy}}
\newcommand{\sospol}{{SoS}}
\DeclareMathOperator*{\st}{st}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\prob}[2]{\mathrm{Prob}_{#1}\bigg[ #2 \bigg]}
\newcommand{\expct}[2]{\mathbb{E}_{#1}\bigg[ #2 \bigg]}
\newcommand{\var}[2]{\mathrm{Var}_{#1}\bigg[ #2 \bigg]}

\newcommand{\probs}[2]{\mathrm{Prob}_{#1}\big[ #2 \big]}
\newcommand{\expcts}[2]{\mathbb{E}_{#1}\big[ #2 \big]}
\newcommand{\vars}[2]{\mathrm{Var}_{#1}\big[ #2 \big]}

\newcommand{\probv}[2]{\mathrm{Prob}_{#1}\left[ #2 \right]}
\newcommand{\expctv}[2]{\mathbb{E}_{#1}\left[ #2 \right]}
\newcommand{\varv}[2]{\mathrm{Var}_{#1}\left[ #2 \right]}

\newcounter{alg-count}
\floatstyle{ruled}
\newfloat{program}{thp}{lop}\floatname{program}{Algorithm}

\crefname{alg-count}{step}{steps}
\crefname{program}{Algorithm}{Algorithms}

\newenvironment{inp}{
\trivlist \item[\hskip\labelsep\textbf{Input:}]
\begin{list}{\labelitemi}{\leftmargin=0.5em}
}{
\end{list} {\endtrivlist}
}
\newenvironment{outp}{
\trivlist \item[\hskip\labelsep\textbf{Output:}]
\begin{list}{\labelitemi}{\leftmargin=0.5em}
}{
\end{list}{\endtrivlist}
}

\newenvironment{proc}{
\trivlist \item[\hskip\labelsep\textbf{Procedure:}]
\begin{list}{\arabic{alg-count}.}{\leftmargin=0.5em
\usecounter{alg-count}}
}{
\end{list}{\endtrivlist}
}
\makeatother

\def\ngap{}
\def\vec{}
\renewcommand{\nusc}{{\sc Sparsest Cut}}
\renewcommand{\usc}{{\sc Uniform Sparsest Cut}}
\newcommand{\wvec}{\vec{w}}

\newcommand{\vnote}[1]{}
\newcommand{\aknote}[1]{}

\newcommand{\sde}{{\mathcal{S}}}
\newcommand{\sdn}{{\widetilde{\mathcal{S}}}}

\parskip=0.5ex 
\title{
  {\bf 
Approximating Non-Uniform Sparsest Cut via Generalized Spectra}}

\author{ Venkatesan Guruswami\footnote{ Computer Science Department,
    Carnegie Mellon University.  Supported in part by NSF grants
    CCF-0963975 and CCF-1115525.  Email: {\tt guruswami@cmu.edu}} \and
  Ali Kemal Sinop\footnote{ Center for Computational Intractability,
    Department of Computer Science, Princeton University.  Supported
    in part by NSF CCF-1115525, and MSR-CMU Center for Computational
    Thinking.  Email: {\tt asinop@cs.cmu.edu}}} \date{\today}
\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
  We give an approximation algorithm for non-uniform sparsest cut with
  the following guarantee: For any , given cost
  and demand graphs with edge weights 
  respectively, we can find a set  with
   at most
   times the optimal non-uniform sparsest cut
  value, in time  provided . Here  is the 'th smallest
  generalized eigenvalue of the Laplacian matrices of cost and demand
  graphs;  (resp. ) is the
  weight of edges crossing the  cut in cost
  (resp. demand) graph and  is the sparsity of the optimal
  cut. In words, we show that the non-uniform sparsest cut problem is
  easy when the generalized spectrum grows moderately fast. To the
  best of our knowledge, there were no results based on higher order
  spectra for non-uniform sparsest cut prior to this work.

  Even for uniform sparsest cut, the quantitative aspects of our
  result are somewhat stronger than previous methods.  Similar results
  hold for other expansion measures like edge expansion, normalized
  cut, and conductance, with the 'th smallest eigenvalue of the
  {\em normalized} Laplacian playing the role of  in the
  latter two cases.

  Our proof is based on an -embedding of vectors from a
  semi-definite program from the Lasserre hierarchy.  The embedded
  vectors are then rounded to a cut using standard threshold
  rounding. We hope that the ideas connecting -embeddings to
  Lasserre SDPs will find other applications. Another aspect of the
  analysis is the adaptation of the column selection paradigm from our
  earlier work on rounding Lasserre SDPs~\cite{gs11-qip} to pick a set
  of {\em edges} rather than vertices. This feature is important in
  order to extend the algorithms to non-uniform sparsest cut.
\end{abstract}

\section{Introduction}
\ngap The problem of finding sparsest cut on graphs is a fundamental
optimization problem that has been intensively studied. The problem is
inherently interesting, and is important as a building block for
divide-and-conquer algorithms on graphs as well as to many
applications such as image segmentation~\cite{sm00,sg07a}, VLSI
layout~\cite{bl84}, packet routing in distributed
networks~\cite{ap90a}, etc.

Let us define the prototypical sparsest cut problem more
concretely. We are given a set of -vertices, , along with two
functions  representing edge weights of
some cost and demand graphs, respectively.  Then given any subset , we define its {\em sparsity} as the following ratio:

where  is the indicator function of .  Our goal in the
\nusc\ problem is to find a subset  with minimum sparsity,
which we denote by .
The special case of demand graph being a clique, where the denominator
of~\cref{eq:def-sc} becomes , is called the
\textsc{uniform sparsest cut} problem.

The value of the sparsest cut can be understood in terms of the
spectral properties of cost and demand graphs.  Let  be the {\em generalized
  eigenvalues} between the Laplacian matrices of cost and demand
graphs (see~\Cref{sec:prelim} for formal definitions).  In a way
similar to the ``easy" direction of Cheeger's inequality, we can use
Courant-Fischer Theorem to show that . \vnote{Should it be , or is it
   for generalized eigenvalues. Would be good to mention it
  explicitly if latter.} \aknote{Should be , good catch!
  See footnote.}
At some point, the eigenvalue  will exceed
. Our main result is an approximation algorithm for \nusc\ which is
efficient when this happens for small . In particular:
\begin{theorem}\label{thm:intro-main-1}
Given  and , for any positive integer ,
one of the following holds. \begin{itemize}
\itemsep=0ex
\item Either one can find  with 
  in time  where ,
\item Or .
\end{itemize}
\end{theorem}
Our actual approximation guarantee is stronger and offers a trade-off:
for any  we can find a 
approximation to  in  time
provided . The formal result is
stated in~\Cref{cor:final-bnd} (the above follows as a corollary with
suitable choice of parameters).
We can also get similar results for various expansion problems such as
normalized cut, edge expansion and conductance using the same
algorithm.
\ngap
\subsection{Previous approximation algorithms for sparsest cut}
\label{sec:related-work}
As the {\sc (Uniform) Sparsest Cut} problem and closely related
variants (such as edge expansion and conductance) are all NP-hard in
general, theoretically much effort has gone into the design of good
approximation algorithm for the problem.

For \usc\ problem, the hard direction of Cheeger's inequality shows
one can ``round" the eigenvector corresponding to  to a cut
 satisfying  where
 is the maximum degree\footnote{Cheeger's inequality is
  usually stated in terms of the second eigenvalue of graph Laplacian
  matrix, which is equal to the smallest generalized eigenvalue,
  .}.
This gives  approximation which is good for
moderate values of  for the case of \usc. To the best of
our knowledge, no analogue of this result is known for \nusc.

For smaller values of , the best approximation for \nusc\
is based on solving a convex relaxation of the problem, and then
rounding the solution to a cut. Using linear programming (LP), in a
seminal work, Leighton and Rao~\cite{LR} gave a factor 
approximation for \nusc\ (here  denotes the number of
vertices). Beautiful connections of approximating sparsest cut to
embeddings of metric spaces into the -metric were later
discovered in \cite{llr,ar}.  Using a semi-definite programming (SDP)
relaxation, the approximation ratio was improved to 
for \usc \ in the breakthrough work \cite{ARV}. For \nusc, using
 embeddings of negative type metrics, an approximation factor
of  was obtained in \cite{CGR} and a factor
, nearly matching the \usc \ case, was
obtained in \cite{ALN}.

Recently, higher order eigenvalues were used to approximate many graph
partitioning problems. In \cite{gs11-qip}, we gave an algorithm based
on SDPs from the Lasserre hierarchy achieving an approximation factor
of the form  for problems such
as minimum bisection, small set expansion, etc, where
 is the 'th smallest eigenvalue of the
normalized Laplacian.  On a similar front, for the \usc\ problem, if
the  eigenvalue is large relative to expansion, one can
combine the eigenspace enumeration of~\cite{ABS} with a cut
improvement procedure
from~\cite{al08} to obtain a constant factor approximation for \usc\ in time .\footnote{We thank an anonymous reviewer for this
  observation.}  The details of this combination are briefly spelled
out in \Cref{apx:rse}. We will revisit this approach in
\Cref{sec:intro-rse} to show why it does not work for \nusc.

A common theme in this line of work is that one can obtain a constant
factor approximation with running time being a function of how fast
the spectrum grows (both our algorithms in this paper and the ones
in~\cite{gs11-qip} in fact allow approximation schemes).  Put
differently, one can identify a generic condition which highlights
what kind of graphs are easy.

To the best of our knowledge, in the case of \nusc\ with an arbitrary
demand graph, no such results of the above vein are known.  In fact,
we are not aware of the analog of the harder direction of Cheeger's
inequality, let alone spectrum based approximation schemes. In this
paper, we present such an approximation scheme based on the
generalized eigenvalues. \ngap
\subsection{Overview of Our Contributions}
\label{sec:our-cont} \label{sec:intro-rse}
In this section, we briefly describe our main contributions in terms
algorithmic tools and techniques over similar algorithms such
as~\cite{gs11-qip}.

\medskip
\paragraph{Main Contributions.}
Our algorithm is based on solving one of the strongest known SDP
relaxations, -rounds of \lh, similar to~\cite{gs11-qip}.  Any
solution for this SDP yields a vector for each -subset of vertices
and each possible labeling of them.  The rounding algorithm
in~\cite{gs11-qip} is based on choosing a set of -nodes, ``seeds'',
then labeling these using the SDP solution. Finally these labels are
``propagated'' to other vertices {\em independently at random}.  Such
rounding is acceptable for constraint satisfaction type problems such
as maximum cut.

Unfortunately for problems such as \nusc, independent rounding is too
``crude'': It tends to break the graph into many disconnected
components, which is rather disastrous for \nusc.

In this paper, we consider a more ``delicate'' rounding based on
thresholding.  Our main contribution is to show how the performance of
such rounding is related to some strong geometrical quantities of
underlying SDP solution, and we show how to bound it using generalized
spectra.

\medskip
\paragraph{Comparison with Subspace Enumeration.} 
One successful technique for designing approximation algorithms based
on higher order spectrum is subspace enumeration~\cite{kolla10,ABS}.
Suppose we have a target set  corresponding to a \usc.  These
techniques rely on the fact that the indicator vector  should have
a large component on the span of small eigenvectors. Thus by
enumerating over the vectors on this subspace using some -net,
we can find a set whose symmetric difference with  is
small. Combining this with a cut improvement algorithm due
to~\cite{al08}, one can obtain an approximation algorithm for \usc\
problem with slightly worse approximation factors than ours
(see~\Cref{apx:rse}).

Unfortunately the immediate extension of this approach to \nusc\ by
using the generalized eigenvectors does not work as the generalized
eigenvectors are not {\em orthogonal} in the Euclidean space.
\ngap
\section{Preliminaries}
\label{sec:prelim}
We now formally define the notation and terminology that will be
useful to us in the paper.

\paragraph{Sets.} Let . Given set  and positive integer , we use
 (resp. ) to denote the set of all
possible size  (resp. size at most ) subsets of .  We use
 to denote the set of non-negative reals.

\paragraph{Euclidean Space.} Given row set , we use  to
denote the set of real vectors where each row (axis) is associated
with an element of .
For any vector , its coordinate at axis  is
denoted by .  Let  be its  norm with
, and  be its transpose.
Finally for any , let  be their inner product .
 
\paragraph{Matrices.} Given row set  and column set , we use  \footnote{We chose
  this notation over the conventional one () so as to
  prevent ambiguity when the rows, , or columns, , are Cartesian
  products themselves.}  we to denote the set of real matrices whose
rows and columns are associated with elements of  and ,
respectively.  Given matrix , for any , we will use  to denote entry of  at
row  and column . For convenience, we use  to
denote the vector corresponding to the column  of . Likewise
given subset of columns of , , we use  to denote the matrix corresponding to the columns  of
.  Given matrix , we use ,  and
 to denote Frobenius norm of , its trace and transpose.

Finally we use  and  to denote the
projection matrices onto the span of  and its orthogonal
complement.

\paragraph{Positive Semi-Definite (PSD) Ordering.} Given a symmetric
matrix , we say  is a PSD matrix, denoted
by , iff  for all .

\paragraph{Eigenvalues.} Given symmetric matrix ,
for any integer , we define its  smallest and
largest eigenvalues as the following, respectively:
\quad\quad 

\paragraph{Generalized Eigenvalues.} Given two symmetric matrices
 with , for any integer , we define their  smallest generalized
eigenvalue as the following:
 
\paragraph{Graphs.} We assume all graphs are simple, undirected and
edge-weighted with non-negative weights. We associate each graph with
its edge weight function of the form , where
we use  to denote the weight of edge between  and  for
convenience.

\smallskip
\paragraph{Laplacian Matrices.} Given a graph with weights
, the associated graph Laplacian matrix, , is defined as the following symmetric matrix:
 
For any , it is easy to see that
, which also implies .
\ngap
\subsection{Lasserre Hierarchy}
\label{sec:lasserre-defn}
We present the formal definitions of the \lh\ of SDP
relaxations~\cite{Las02}, tailored to the setting of the problems we
are interested in, where the goal is to assign to each vertex/variable
from  a label from .
\def\dim{\Upsilon}
\begin{definition}[Lasserre vector set]
\label{def:las-sdp}
Given a set of variables  and a positive integer , a collection
of vectors  is said to satisfy -rounds of \lh, denoted by
, if it satisfies the following conditions:
\begin{enumerate}
\item For each set , there exists a function
   that associates a vector of some
  finite dimension  with each possible labeling of .  We use
   to denote the vector associated with the labeling .

  For singletons , we will use  and 
  interchangeably.

  For  and , we use  as the label 
  receives from .  Also given sets  with labeling  and  with labeling  such that  and
   agree on , we use  to denote the labeling of
   consistent with  and : If ,  and vice versa.
\item .
\item  if there exists
   such that .
\item \label{def:las-sdp:consistent}  if
   and .
\item For any , .
\item (implied by above constraints) For any ,
   and , .
\end{enumerate} 
\end{definition}
\ngap
\section{Our Algorithm and Its Analysis}
\begin{program}[h]
  \caption{: Seed based rounding in
    time .  Sparsity of its output is bounded
    in~\Cref{thm:rnd-from-s}.\label{alg:rnd-from-s}}
\begin{inp} 
\item ; 
and seed set  with .
\end{inp}
\begin{outp} 
\item A set  representing an approximation for \nusc\ problem.
\end{outp}
\begin{proc}
\itemsep=-0.5ex
\item . 
\item For each ,
\begin{enumerate}[(a)] 
\item Let  be an ordering of  so that
.
\item For each , 

\end{enumerate}
\item .
\end{proc}
\end{program}
\begin{program}[h]
\caption{\textsc{select-seeds}(): 
Seed selection in time . \label{alg:select-s}}
\begin{inp} 
\item  and  as  
the demand graph.
\end{inp}
\begin{outp} 
\item  with  as a set of
  seed edges.
\end{outp}
\begin{proc}
\item Let .
\item Use the column selection algorithm from~\cite{gs11-svd} to
  choose -columns, , of matrix
   and return .
\end{proc}
\end{program}
\begin{program}[t]
\caption{\textsc{Approximate-SC}(): 
Main algorithm for approximating \nusc. Sparsity of the output
is bounded in~\Cref{cor:final-bnd}.
A na\"ive implementation will run in time . However 
this algorithm exactly fits into the local rounding framework 
introduced in~\cite{gs12-fast}, therefore we can use the faster solver
from~\cite{gs12-fast} to decrease the running time to .
\label{alg:vanilla-sc}}
\begin{inp} 
\item  as the cost and demand
graphs, respectively.
\end{inp}
\begin{outp} 
\item A set  representing an approximation for \nusc\
  problem.
\end{outp}
\begin{proc}
\item Compute a (near-)optimal solution, , to the following
  SDP:
 
\item  (\Cref{alg:select-s}).
\item 
  (\Cref{alg:rnd-from-s}). Return .
\end{proc}
\end{program}
\label{sec:alg-and-analysis}
The complete algorithm is presented in~\Cref{alg:vanilla-sc}.  It is
based on rounding a certain -rounds of Lasserre Hierarchy
relaxation for the \nusc\ problem given positive integer :
 
It is easy to see that~\cref{eq:sc-sdp0} is indeed a relaxation of
\nusc\ problem.  Even though it is not an SDP problem (it is
quasi-convex), there is an equivalent SDP formulation.
\begin{lemma} \label{lem:sdp-eq}
The following SDP is equivalent to~\cref{eq:sc-sdp0}:
 
\end{lemma}
\begin{remark}
  The constraint  in~\cref{eq:sc-sdp} is
  redundant, but we included it for the sake of clarity.
\end{remark}
\begin{proof} [Proof of~\Cref{lem:sdp-eq}] Given a feasible solution
   for formulation \eqref{eq:sc-sdp0}, consider the following
  collection of vectors, .
  For each , we define  as .
It is easy to see that  and objective values are equal.
Finally  since .

  For the other direction of equivalence, suppose  is a
  feasible solution of~\cref{eq:sc-sdp}. For each , let . It is easy to see that the
  objective values are equal. The rest of the proof for  being
  a feasible solution of~\cref{eq:sc-sdp0} follows similarly to the
  previous direction.
\end{proof}
\begin{remark}
  The main components of our rounding,
  \Cref{alg:rnd-from-s,alg:select-s}, are scale invariant; thus the
  formulation given in~\cref{eq:sc-sdp} is sufficient for rounding
  purposes. But we chose to first present~\cref{eq:sc-sdp0} as it is
  more intuitive.
\end{remark}
\ngap
\subsection{Intuition Behind Our Rounding}
\label{sec:int}
For an intuition behind our rounding procedure, presented
in~\Cref{alg:rnd-from-s}, we start with a simple randomized rounding
procedure, which is based on the seed based propagation framework
from~\cite{gs11-qip}. In \cite{gs11-qip}, the different vertices were
rounded independently according to their marginal distribution
(conditioned on a partial assignment to the seed set), whereas here we
do correlated threshold rounding. The algorithm can be easily
derandomized as described at the end of this subsection. In the below
description, the details of the seed selection procedure will be
skipped and deferred to Section \ref{sec:seed-selection}. For now we
develop the algorithm and analyze it assuming some fixed choice of
seed edges.  (We will use the analysis as a guide to make a prudent
choice of seed edges.)

\vspace{0.1in}
\paragraph{Randomized rounding algorithm.} 
Consider the following procedure.
On input :
\begin{enumerate}
\item Choose a set of -edges from the demand graph, say  ({\em seed edges}).
\item Let  be the set of their endpoints, v\{u,v\}\in \sde.
\item Observe that , hence the values
   define a probability distribution over
  all labelings of ,  .  So sample a labeling
  for , , with probability
  .
\item Choose a threshold  uniformly at random and
  output the following set:

\end{enumerate}
In order for this procedure to make sense, the range of 
should be similar to 's range. In the following claim, we prove
this.
\begin{claim} \label{clm:tau-ulb}
Provided that , we have:
\begin{inparaenum}[(i)]
\item \label{it:353-712-1}  for any ,
\item \label{it:353-712-2}  for any
  pair ,
\item \label{it:353-712-3}  for any .
\end{inparaenum}
\end{claim}
\begin{proof}[of \eqref{it:353-712-1} and \eqref{it:353-712-2}]
  We will only prove \eqref{it:353-712-1}, from which
  \eqref{it:353-712-2} follows immediately.  The lower bound follows
  from . For the upper bound, we
  have:
 
\end{proof}
\begin{proof}[of \eqref{it:353-712-3}]
  Follows from the fact that .
\end{proof}
Let's calculate the probability of separating two vertices 
by this procedure.
\begin{claim}\label{clm:sep-prob}
  
\end{claim}
\begin{proof}
  For fixed , by~\Cref{clm:tau-ulb} the probability of separating
   and  is equal to .
Taking expectation over :

\end{proof}

\paragraph{Derandomization.} 
For any fixed , there are at most  different
's.  Hence instead of choosing  and
 randomly, we can perform an exhaustive search over all
possible such sets and output the one with minimum sparsity.  Since
there are at most  many unique 's, the
exhaustive search can easily be implemented in time .  The rounding procedure along with this modification is
presented in~\Cref{alg:rnd-from-s}.
\ngap
\subsection{Seed Based -embedding}
Toward analyzing the rounding algorithm of the previous section, we
now define an embedding of the vertices into , based on the
Lasserre solution (and the chosen seed edges).
\begin{definition}[Seed Based Embedding]
  \label{def:our-embedding}
  Given  and  with , let  be the endpoints of
  edges in  so that . Then we
  define the seed based embedding of  as the following
  collection of vectors.
For each ,  is given
  by
  
\end{definition}
Observe that  is equal to the
probability that  and  are separated as shown
in~\Cref{clm:sep-prob}.

It is well known that once we have an -embedding, we can get a
cut with similar sparsity by choosing the best threshold cut along
each coordinate and this is exactly what we do
in~\Cref{alg:rnd-from-s}.
The following lemma is well-known but for the sake of completeness we
provide a proof.
\begin{lemma}[\cite{llr}]
  \label{lem:l1-to-cut}
  Given a set of vertices , a collection of vectors  representing an
  embedding of , the following holds.  For any  being the edge weights of graphs  and ,
  respectively:
   
  Here  represents the threshold cut along coordinate .
\end{lemma}
\begin{proof}For any , let 
  and .
Consider the following randomized process.  Choose 
  with probability proportional to  and then sample a
  threshold .
  Then:

We have
  
  thus proving the lemma.
\end{proof}			 

In the rest of this section, we will upper bound the right hand side
of ~\cref{eq:tcut-l1} for our embedding
from~\Cref{def:our-embedding}. To this end, we now obtain upper and
lower bounds on the -distance  in terms of the SDP vectors.

\begin{claim} \label{clm:l1-ub} .
\end{claim}
\begin{proof}
  Since , we can express 
  and  as  and
   respectively.  Thus
  .
Now the following identity follows easily\footnote{ Intuitively, it
    corresponds to the following.  The ``probability'' of  and 
    are separated is equal to the probability of  and  being
    labeled with  and  or  and .  }:
  
Therefore:
  
\end{proof}
\begin{claim}\label{clm:l1-lb}
   where  is the unit vector for
  .
\end{claim}
\begin{proof}
For any , 
by~\Cref{clm:tau-ulb},  thus
.  
Multiplying both sides with
, we obtain

Summing over all , we obtain the desired lower
bound,

\end{proof}
In its current form, our lower bound is not very useful as it involves
the {\em higher order} vectors ('s) from our
relaxation.  Unfortunately these vectors are very hard to reason
about: We do not have any direct handle on them.  Therefore our goal
is to relate this expression to some other expression that only
involves the vectors for edges ('s). We first
introduce some notation.
\begin{notation}
  Let .
\end{notation}
We can rewrite the lower bound from~\Cref{clm:l1-lb} 
in terms of  as follows:

As observed in~\cite{gs11-qip}, 
 has a special structure --- it is a projection matrix
onto the span of vectors . 
\begin{proposition}\label{lem:pi-is-proj}
  , i.e.  is a projection matrix onto
  the span of vectors in .
\end{proposition}
\begin{proof}
Observe that 
f=g. 
Then we have:
 
\end{proof}
For each seed edge ,
.
This means we can lower bound the matrix  in terms of the
projection matrix onto the span of vectors corresponding to seed
edges!
\begin{notation}
\label{not:proj-mtx}
Let  be the projection matrix onto the span of . Similarly let  be
projection matrix onto the orthogonal complement of , i.e., . Here  is the identity matrix of appropriate dimension.
\end{notation}
The final ingredient in our embedding is to lower bound the 
distance.
\begin{lemma} \label{lem:l1-lb} .
\end{lemma}
\begin{proof}
  From~\Cref{clm:l1-lb} and~\cref{eq:601-7112} we see that  For any ,  hence . In particular, for any pair :
  , which means:
  
  Consequently, . \qedhere
\end{proof}
We wrap up this section with the following theorem which bounds the
approximation factor of our \Cref{alg:rnd-from-s} in terms of the SDP
vectors  and the projection matrix  corresponding
to the seed edges .
\newcommand{\scsdp}{\Phi^{\mathrm{SDP}}}
\begin{theorem}\label{thm:rnd-from-s}
  Given  and a set of seed edges  with projection matrices  as in~\Cref{not:proj-mtx}; let  be the
  set returned by \Cref{alg:rnd-from-s}.
Then the sparsity  of  is bounded by:
   where
  
\end{theorem}
\begin{proof}
   follows from \Cref{lem:l1-to-cut}.
\Cref{clm:l1-ub} and \Cref{lem:l1-lb} together imply:

\end{proof}
\ngap
\subsection{Choosing Seed Edges}
\label{sec:seed-selection}

We now turn to the main missing piece in our algorithm and its
analysis: how to make a good choice of the seed edges , and how
to relate the guarantee of \cref{eq:rnd-bnd} for that choice of 
to the generalized eigenvalues between the Laplacians of the cost and
demand graphs.

\begin{notation} Given  and
  , let  be the following
  matrix whose columns are associated with vertex pairs:
  .
  \label{not:xmat}
\end{notation}
Observe that .
Since , the matrix
, consisting of columns of  indexed by
, is well defined.  Moreover there is a strong connection
between  and , which we
formalize next:
\begin{claim}
  . Furthermore if
   then .
\end{claim}
\begin{proof}
  Recall that  and  represents
  , which contains every
  column of .
\end{proof} 

After substituting~\Cref{not:xmat}, the approximation factor in
\Cref{thm:rnd-from-s} becomes

One way to think about
 is in
terms of column based matrix reconstruction. If we were to express
each column of  as a linear combination of only
-columns of , what is the minimum reconstruction
error (in terms of Frobenius norm) we can achieve?
Without the restriction of choosing only columns, this question
becomes easy to answer: the best error is achieved by the top 
singular vectors of  and equals the sum of all but
largest  eigenvalues of the Gram matrix, . For convenience, we record this in
\Cref{lem:col-res-lb} below.
\begin{notation}
  Let  be the
  eigenvalues of  in descending
  order.
\end{notation}
\begin{claim}
  \label{lem:col-res-lb}
  For any seed set  with ,
  \vnote{Changed  to .}
.
\end{claim}
\aknote{Happily removed the proof :).}
In~\cite{gs11-svd}, it was shown that choosing 
many columns suffice to decrease the error within a -factor
of this lower bound and this is essentially the best possible up to
low order terms.
\begin{theorem}[\cite{gs11-svd}] \label{thm:choose-s} For any positive
  integer  and positive real , there exists  columns of ,
  , such that
	
  Furthermore there exists an algorithm to find such  in
  time  (recall  has  columns).
\end{theorem}
Our seed selection procedure is presented in~\Cref{alg:select-s}.  We
bound  in~\Cref{lem:sigma-bnd}. The main
approximation algorithm combining \Cref{alg:rnd-from-s,alg:select-s}
is presented in~\Cref{alg:vanilla-sc} with its analysis
in~\Cref{cor:final-bnd}.
\begin{theorem} \label{lem:sigma-bnd} Let  be the generalized eigenvalues of
  Laplacian matrices for the cost and demand graphs.  Then for
   being the matrix defined in~\Cref{not:xmat}, the
  following bound holds:

\end{theorem}
Before proving~\cref{lem:sigma-bnd}, we will begin with stating a
simple lower bound on the trace of matrix products in terms of the
spectra.
\begin{lemma}\label{thm:birkhoff} Given a symmetric matrix  and
  positive semidefinite matrix :
 where  and  denote the  largest
eigenvalue of  and the  smallest eigenvalues of ,
respectively.
\end{lemma} 
\begin{proof}
  von Neumann's Trace Inequality~\cite{hj-mat-book} states that  for any pair of symmetric
  matrices,  and . This allows us to lower bound  as
  follows, from which the claim follows immediately:
 
\end{proof}
\begin{proof}[Proof of \Cref{lem:sigma-bnd}]
  Since the claimed bound is scale independent, we may assume  without loss of generality.

  Throughout the proof, we will use the following matrices:
\begin{itemize}
\item ,
\item  is the following edge-node
  incidence matrix of the cost graph whose columns and rows are
  associated with vertices and edges, respectively. Its entry at
  column  and row  with 
  (assuming some consistent ordering of ) is given by:

\item  is defined similarly for the
  demand graph, with its rank being .
\item Singular value decomposition of  is given by  with . Here  is an
  orthonormal -by- matrix,  is an -by-
  positive diagonal matrix and  is an orthonormal -by-

  matrix. \item  are the Laplacian matrices for cost and demand
  graphs, respectively.
\item  is the pseudo-inverse of  so that
  .
\item .
\end{itemize}
\vnote{Was the earlier argument, which argued about spectrum of  to
  capture generalized eigenvalues, wrong?}  \aknote{Nope. But I
  thought this proof is simpler.}
The following identities are trivial:
 
Moreover
 by our assumption that .
	
Our goal is to lower bound  by .  Our approach will be to use
\Cref{thm:birkhoff} to prove this, by identifying a suitable matrix
whose eigenvalues equal the generalized eigenvalues .
		
Since  is a projection matrix and , we
have  .  Substituting the
identity  into
this lower bound, we have:

Since the null space of  contains the null
space of , the row span of  is contained in the
span of . Recall that  is an orthonormal matrix, therefore  is a {\em projection} matrix onto its column span. Consequently
 and~\cref{eq:44610112} is
equal to: \vnote{It might be good to remind reader of relation between
  SVD and null space and justify .}

If we substitute the lower bound from~\Cref{thm:birkhoff}
into~\cref{eq:44610113}, we see that

where we used ,  to denote the 
largest and smallest eigenvalues of matrices  and ,
respectively.  Observing that , we
finally obtain \cref{eq:432}:

To complete the proof, we need to relate  to .
\begin{claim} \label{clm:50910112}
.
\end{claim}
\begin{proof} First observe that . Since  is positive definite, any
  eigenvalue of  is also a
  generalized eigenvalue of matrices  and .  is
  an orthonormal matrix, therefore the generalized spectrum does not
  change when we transform both matrices by , implying that
  . Proof is complete
  by observing that any generalized eigenvector, , lies in the span
  of  so that . Hence .
\end{proof}
Proof is complete by combining \Cref{clm:50910112} and \cref{eq:432}.
\end{proof}
We put everything together in our main result below.
\begin{theorem}
\label{cor:final-bnd}
Given  representing cost and demand
graphs, let 
be their generalized eigenvalues in ascending order.
For any positive integer  and real , on input ,  and
, \Cref{alg:vanilla-sc} outputs a
subset  whose sparsity  (w.r.t cost graph  and
demand graph ) is bounded by:

Furthermore using the SDP solver from~\cite{gs12-fast}, the running
time can be decreased to .  \vnote{Where does the
  ``provided" constraint come from? It is not transparent from the
  proof.}  \aknote{Necessary otherwise the quantity is negative, which
  means the upper bound becomes lower bound when we take the
  reciprocal.}
\end{theorem}
\begin{proof} Whenever ,
  the quantity  is
  positive which means
 by \Cref{thm:choose-s,lem:sigma-bnd}. Substituting the bound
from~\Cref{thm:rnd-from-s} completes the proof.
\end{proof}
There are two interesting regimes in~\Cref{cor:final-bnd}, which we
highlight in~\Cref{cor:final-bnd-regimes}.
\begin{corollary}
\label{cor:final-bnd-regimes}
Given  representing cost and demand
graphs, positive real  and positive integer
, there exists an algorithm which outputs a subset , in
time , such that:
\begin{itemize}
\item (Near Optimal) If  then ;
\item (Constant Factor) Otherwise if  then .
\end{itemize}
\end{corollary}
\section{Using Subspace Enumeration for \usc}
\label{apx:rse}
\vnote{Should we acknowledge the reviewer here again?} \aknote{Done.}
Throughout this section, we will assume that the cost graph with
weights  and  for any
. Since  is regular, definitions of uniform sparsest cut / normalized
cut and edge expansion / conductance coincide. Thus we will focus only
on \usc\ which we denote by .

\def\uscopt{\phi^\ast} The following theorem is adapted
from~\cite{al08} for our setting:
\begin{theorem}[Cut Improvement, see~\cite{al08}]
  For any , given 
  satisfying
 
in polynomial time one can find  whose edge
expansion is within a factor  of 's edge expansion.
\end{theorem}
\vnote{What happened to the proof you wrote for this?}  \aknote{I only
  had proof of corollary, which is commented out. Do you want to put
  it back?}  The following is adapted from~\cite{ABS}:
\begin{theorem}[Eigenspace Enumeration~\cite{ABS}]
  In time , there exists an algorithm which outputs
  a set  that contains some  with
  following property.  There exists  with:

\end{theorem}
Combining these two, one can obtain the following\footnote{We thank an
  anonymous reviewer for pointing out this combination for uniform
  sparsest cut.}:
\begin{corollary} For any positive integer , if  smallest eigenvalue of
  Laplacian matrix for cost graph satisfies 
  where  is the \usc\ value, then in time  one can find  whose uniform sparsity is
  bounded by:
  
\end{corollary}
\iffalse
\begin{proof}
  Let  be a uniform sparsest cut with sparsity  having
  .  Then we can find a vector
   using eigenspace enumeration such that:
   where .

  Similarly, complement of , , satisfies:
  
  Therefore
   We apply cut improvement algorithm to  if , and to  otherwise.  Without loss of generality,
  let's assume . Then cut improvement finds
   such that:
   Substituting the expression for  completes the proof.
\end{proof}
\fi \ngap
\section*{Acknowledgments}
We thank anonymous reviewers for their helpful comments on earlier
drafts of this paper as well as pointing out the combination of
\cite{al08} with \cite{ABS} to approximate \usc.
\bibliographystyle{abbrv}
\bibliography{../hdr-bib/references}
\end{document}
