\documentclass[5pt]{article}

\usepackage[letterpaper]{geometry}
\usepackage{spconf,amsmath,epsfig}
\usepackage{enumitem}
\usepackage{multirow}

\usepackage{fancyhdr}
\thispagestyle{fancy}
\fancyhf{} 
\renewcommand{\headrulewidth}{0pt}

\lfoot{978-1-5090-6067-2/17/\nN=2\times np_j=(x,y,z),j\in Nj_{th}tj,k,m,n\in N\otimes\odotj \neq kj\neq m\neq k(i,k)\neq (m,n)C^2_{50}=12253 \times 1225=36751225 \times 48=58800C^2_{1225}=749700 2*C^2_{25}=600C^2_{24}=276C^2_{22}=231897741 570TNN\times TH(eight)\times W(idth)256\times256h_{th}w_{th}f^w_hh_{th}w_{th}f^w_h = f^{T\times w/W}_{N\times h/H}H*WF_h=\{f^1_h,f^2_h,...,f^T_h\}colorbar()[0,1]f^w_h\in R^3h_{th}w_{th}f,Fv,Vv\circF_{max}(\ldotp)0.1^2^1$ & JS1\&LS1-EM2 &62.00\% &62.00\%&\\
				\hline
			\end{tabular}
			{\flushleft \footnotesize  Note 1: this method encodes the RGB channels based on JJd, JLd and LLa respectively. 
				Note 2: this method is not used for final score fusion. }
		\end{table}	
		
			\begin{table}[!t]
				\centering
				\caption{Experimental results (accuracy) on NTU RGB+D Dataset}
				\label{tab:ntu_result}
				\begin{tabular}{|c|c|}
					\hline Method & Accuracy \\
					\hline Lie Group\cite{Vemulapalli2014} & 52.76\% \\
					Dynamic Skeletons\cite{Ohn-Bar2013} & 65.22\% \\
					HBRNN\cite{Du2015} & 63.97\% \\
					Deep RNN\cite{Shahroudy2016} & 64.09\% \\
					Part-aware LSTM\cite{Shahroudy2016} & 70.27\% \\
					ST-LSTM+Trust Gate\cite{Liu2016} & 77.70\% \\
					JTM\cite{Wang2016} & 75.20\% \\
					Geometric Features\cite{Zhang2017} & 82.39\% \\
					STA-LSTM\cite{Song2017}	& 81.20\%\\
					\hline
					Proposed Method & 82.31\% \\
					\hline
				\end{tabular}
			\end{table}
			
\subsection{Evaluation of spatial features}
The results of individual features and different encoding methods are 
 listed in Table~\ref{tab:result_all}, as well as results of 
score-multiplication fusion. There are five features evaluated, each of which 
was evaluated with different feature (joint) selection methods and different 
encoding methods. The methods are denoted in the form `feature selection method 
- encoding method', which have been described in 
Section~\ref{sec:method}.
	 
As illustrated in Fig.~\ref{fig:samples}, images generated from samples 
of different actions have discriminative textures. In addition, the spatial 
features are encoded into different textures by different methods. 
	
From Table~\ref{tab:result_all}, it can be seen that the JJv feature is the best 
joint-joint feature, based on the comparisons of single results and fused 
results. Moreover, JLd seems to be the best feature among the five types of 
features, which coincidences with the observations reported by~\cite{Zhang2017}. 
Among the three kinds of joint selection methods, JS3 generally works better 
than the other two. This observation suggests that some of the joints are 
noise with regard to this task, which is consistent with the above analysis.
	
	
From Table~\ref{tab:ntu_result} the results indicate that, 
compared with methods based hand-crafted features and those based on deep 
learning (RNNs and CNNs), the proposed method achieved state-of-the-art 
results. 
	
\section{Conclusions}
\label{sec:conclusion}
In this paper, a method for skeleton-based action recognition using CNNs is 
proposed. This method explored encoding different spatial features into texture 
color images and achieved state-of-the-art results on NTU RGB+D Dataset. The 
experimental results indicated the effectiveness of texture images when used as 
 spatio-temporal information representation, and the effectiveness of joint 
selection strategies for robust and cost-efficient computation. 
	
\begin{thebibliography}{10}

\bibitem{li2010action}
Wanqing Li, Zhengyou Zhang, and Zicheng Liu,
\newblock ``Action recognition based on a bag of {3D} points,''
\newblock in {\em Proc. IEEE Conference on CVPR Workshops (CVPRW)}, 2010, pp.
  9--14.

\bibitem{Wang2015}
Pichao Wang, Wanqing Li, Zhimin Gao, Chang Tang, Jing Zhang, and Philip
  Ogunbona,
\newblock ``Convnets-based action recognition from depth maps through virtual
  cameras and pseudocoloring,''
\newblock in {\em Proc. ACM Conference on Multimedia}, 2015, pp. 1119--1122.

\bibitem{pichaoTHMS}
Pichao Wang, Wanqing Li, Zhimin Gao, Jing Zhang, Chang Tang, and Phlip
  Ogunbona,
\newblock ``Action recognition from depth maps using deep convolutional neural
  networks,''
\newblock {\em IEEE Transactions on Human-Machine Systems}, vol. 46, no. 4, pp.
  498--509, 2016.

\bibitem{Pichaocvpr2017}
Pichao Wang, Wanqing Li, Zhimin Gao, Yuyao Zhang, Chang Tang, and Philip
  Ogunbona,
\newblock ``Scene flow to action map: A new representation for {RGB-D} based
  action recognition with convolutional neural networks,''
\newblock in {\em CVPR}, 2017.

\bibitem{Zhang2017}
Songyang Zhang, Xiaoming Liu, and Jun Xiao,
\newblock ``On geometric features for skeleton-based action recognition using
  multilayer {LSTM} networks,''
\newblock in {\em Proc. IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, 2017.

\bibitem{xia2012view}
Lu~Xia, Chia-Chih Chen, and JK~Aggarwal,
\newblock ``View invariant human action recognition using histograms of {3D}
  joints,''
\newblock in {\em Proc. IEEE Conference on CVPR Workshops (CVPRW)}, 2012, pp.
  20--27.

\bibitem{wang2014mining}
Pichao Wang, Wanqing Li, Philip Ogunbona, Zhimin Gao, and Hanling Zhang,
\newblock ``Mining mid-level features for action recognition based on effective
  skeleton representation,''
\newblock in {\em Proc. IEEE Conference on Digital lmage Computing: Techniques
  and Applications (DlCTA)}, 2014, pp. 1--8.

\bibitem{Vemulapalli2014}
Raviteja Vemulapalli, Felipe Arrate, and Rama Chellappa,
\newblock ``Human action recognition by representing 3d skeletons as points in
  a lie group,''
\newblock in {\em CVPR}, 2014, pp. 588--595.

\bibitem{Wang2016}
Pichao Wang, Zhaoyang Li, Yonghong Hou, and Wanqing Li,
\newblock ``Action recognition based on joint trajectory maps using
  convolutional neural networks,''
\newblock in {\em Proc. ACM Conference on Multimedia}, 2016, pp. 102--106.

\bibitem{Hou2016}
Yonghong Hou, Zhaoyang Li, Pichao Wang, and Wanqing Li,
\newblock ``Skeleton optical spectra based action recognition using
  convolutional neural networks,''
\newblock {\em IEEE Transactions on Circuits and Systems for Video Technology},
  2016.

\bibitem{Li2017}
Chuankun Li, Yonghong Hou, Pichao Wang, and Wanqing Li,
\newblock ``Joint distance maps based action recognition with convolutional
  neural network,''
\newblock {\em IEEE Signal Processing Letters}, 2017.

\bibitem{Shahroudy2016}
Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang,
\newblock ``{NTU RGB+D}: A large scale dataset for {3D} human activity
  analysis,''
\newblock in {\em CVPR}, June 2016.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton,
\newblock ``Imagenet classification with deep convolutional neural networks,''
\newblock {\em Advances in Neural Information Processing Systems}, pp.
  1097--1105, 2012.

\bibitem{Ohn-Bar2013}
Eshed Ohn-Bar and Mohan Trivedi,
\newblock ``Joint angles similarities and hog2 for action recognition,''
\newblock in {\em Proc. IEEE Conference on CVPR Workshops (CVPRW)}, 2013, pp.
  465--470.

\bibitem{Du2015}
Yong Du, Wei Wang, and Liang Wang,
\newblock ``Hierarchical recurrent neural network for skeleton based action
  recognition,''
\newblock in {\em CVPR}, 2015, pp. 1110--1118.

\bibitem{Liu2016}
Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang,
\newblock ``Spatio-temporal lstm with trust gates for 3d human action
  recognition,''
\newblock in {\em ECCV}, 2016, pp. 816--833.

\bibitem{Song2017}
Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, and Jiaying Liu,
\newblock ``An end-to-end spatio-temporal attention model for human action
  recognition from skeleton data,''
\newblock in {\em Proc. AAAI Conference on Artificial Intelligence}, 2017, pp.
  4263--4270.

\end{thebibliography}
	
\end{document}
