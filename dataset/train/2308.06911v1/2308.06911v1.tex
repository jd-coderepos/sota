\documentclass{article}

\usepackage{arxiv}
\usepackage{amsmath}  

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{makecell} 
\bibliographystyle{ieeetr}
\title{GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text}


\author{
  Pengfei Liu \\
  PengCheng Laboratory\\
  School of Computer Science and Engineering, \\
  Sun Yat-sen University\\
\And
  Yiming Ren \\
  PengCheng Laboratory\\
\And
  Zhixiang Ren\thanks{Corresponding author} \\
  PengCheng Laboratory\\
  \texttt{renzhx@pcl.ac.cn} \\
}

\begin{document}
\maketitle

\begin{abstract}
Large language models have made significant strides in natural language processing, paving the way for innovative applications including molecular representation and generation. 
However, most existing single-modality approaches cannot capture the abundant and complex information in molecular data. 
Here, we introduce \textbf{GIT-Mol}, a multi-modal large language model that integrates the structure \textbf{G}raph, \textbf{I}mage, and \textbf{T}ext information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions. 
To facilitate the integration of multi-modal molecular data, we propose \textbf{GIT-Former}, a novel architecture capable of mapping all modalities into a unified latent space. 
Our study develops an innovative any-to-language molecular translation strategy and achieves a 10\%-15\% improvement in molecular captioning, a 5\%-10\% accuracy increase in property prediction, and a 20\% boost in molecule generation validity compared to baseline or single-modality models.
\end{abstract}


\keywords{Large Language Model \and Multi-modality \and Molecular Representation \and Molecule Generation}



\section{Introduction}

Molecular science is a broad field that studies the structure, properties, and interactions of molecules. It is a multidisciplinary field that draws on chemistry, physics, biology, and computer science.
Molecular science is pivotal in drug discovery applications, such as target identification and validation, structure-based drug design, and side effect prediction.
However, most existing methods of discovering new molecules or tweaking existing ones can be time-consuming, expensive, and prone to failure\cite{rodrigues2016counting}. 
More recently, computational methods have shown significant advantages in molecule generation and tweaking\cite{bilodeau2022generative}. These techniques enable rapid identification and optimization of potential drug candidates.

Fortunately, artificial intelligence (AI) and deep learning have emerged as powerful tools for molecular science. These technologies can potentially revolutionize the field by improving speed, accuracy, and scalability in molecular discovery and understanding. Large Language Models (LLMs) have made significant progress in Natural Language Processing (NLP) and molecular science. MolT5\cite{edwards2022translation}, based on the T5\cite{raffel2020exploring}, which includes capabilities of molecule captioning (Mol2Cap) and text-based molecule generation (Cap2Mol). LLMs like MolT5 help describe molecules in words and generate structures from the text. However, these text-to-text models can not fully use the advantages of molecular structure data and understand molecular images.
To fuse and understand multi-modal data, the multi-modal large language models (MLLMs) like CLIP\cite{radford2021learning}, ALIGN\cite{jia2021scaling}, and BEIT-3\cite{wang2022image} have laid the groundwork for adaptive learning across image-text modalities. In molecular science, SwinOCSR\cite{xu2022swinocsr} is designed for image recognition, which significantly aids in document comprehension and multi-modal drug discovery database construction\cite{wang2022multi}. In addition, MoleculeSTM\cite{liu2022multi} and MoMu\cite{su2022molecular} can combine SMILES and graph representations for molecular property prediction tasks.

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{fig/figure1_x.png}
\caption{\textbf{The overview of GIT-Mol}. \textbf{a. Molecular internal information}, including sequence and graph structure representations, emphasizes inherent chemical properties and simple topology; \textbf{b. Molecular external information}, e.g., images and text descriptions, provide richer details and help the human understanding; \textbf{c. Study case}, featuring molecule generation (from image, caption, or both to molecule) and molecule caption (from SMILES, graph, or both to caption). In molecule generation, our model accurately captures the organophosphate oxoanion structure as described in the caption. In comparison, MolT5 incorrectly represents the ring structure, and GPT-4 makes a mistake in the placement of the ketone functional group. GIT-Mol's output differs from the ground truth for the molecule caption task but still provides a correct and meaningful description of the SMILES string. }
\label{fig:Conceptual-GIT-MOL}
\end{figure*}

Most existing methods have exceptional performance. However, a significant gap remains in models specifically designed for comprehensive molecular modalities (graph, image, and text) in representation and generation, and requiring more efficient training strategies. 
Furthermore, the models that effectively fuse three or more modalities are relatively limited. 
Integrating multiple modalities can be challenging due to differences in data types, missing or incomplete data, and increased model complexity. 
A unified modality mixer must effectively incorporate three or more modalities while addressing cross-modal data differentiation. 
For efficient training, models like Vicuna\cite{chiang2023vicuna} are developed by efficiently fine-tuning the LLaMA model\cite{touvron2023llama} to address these challenges through effective fine-tuning via instruction learning. Other techniques, such as Adapter\cite{zhang2023llamaadapter}, LoRA\cite{hu2021lora}, and Prompt Tuning\cite{lester2021power}, also make training a unified multi-modal model easier. 

To make the most of vast quantities of unlabeled multi-modal data and improve the molecular representation and generation, we develop a neural network architecture GIT-Former with the cross-attention mechanism. 
Integrating a fused MLLM GIT-Mol based on existing state-of-the-art models can predict properties more accurately and generate more reliable results. 
Our main contributions are as follows:

\begin{itemize}
\item To utilize the available large amount of unlabeled multi-modal data, we develop a specialized multi-modal large language model, \textbf{GIT-Mol} (700M), to cover all three modalities in molecular science (Graph, Image, and Text) for molecule generation, molecule captioning, and molecular property prediction.
\item We introduce \textbf{GIT-Former}, a modality mixer that utilizes a cross-attention mechanism to fuse the three molecular modalities seamlessly. Abundantly validated by our ablation studies, the employment of this multi-modal model provides an impressive improvement of 10\%-15\% over the single-modality models.
\item The quantitative evaluations demonstrate the state-of-the-art performance of GIT-Mol, as it outperforms the baseline 20\% in the cross-modal molecule generation tasks regarding the validity of the generated molecules and by 5\%-10\% in molecular property prediction tasks.
\end{itemize}



\section{Related Works}


\subsection{Multi-modal Large Language Models}
In recent years, Large Language Models (LLMs) like the GPT\cite{floridi2020gpt} family have received more attention due to their performance and potential applications, especially the ChatGPT and GPT-4. Some variants of those models have been used in many scientific domains, such as BioGPT\cite{luo2022biogpt}, DrugGPT\cite{li2023druggpt}, and MolReGPT\cite{li2023empowering}, which have been adapted for molecular science tasks. Additionally, models like LLaMA and T5 have inspired a variety of variants and as the language model in multi-modal models.
 
Multi-modal models primarily focus on image captioning tasks\cite{stefanini2022show}, and these models can be classified into several categories:
\begin{itemize}
\item \textbf{Modality-Adaptive Learning} models like BEIT-3\cite{wang2022image} and KOSMOS-1\cite{huang2023language} use the Mixture of Experts (MoE)\cite{bao2022vlmo} and the MAGNETO\cite{wang2022foundation} Transformer. These technologies enable learning across different modalities through encoders and shared self-attention modules.
\item \textbf{Multi-modal Agents} such as Flamingo\cite{alayrac2022flamingo}, Gato\cite{reed2022generalist}, and PaLM-E\cite{driess2023palm} are designed for real-world scenarios. These models use multi-task and reinforcement learning to identify environmental content and execute decision tasks in complex environments.
\item Models like Visual ChatGPT\cite{wu2023visual} and HuggingGPT\cite{shen2023hugginggpt} utilize the \textbf{Chain-of-Thoughts}(CoT)\cite{wei2022chain} inference approach. They combine visual encoders with ChatGPT to perform ongoing reasoning tasks, offering rich and visually informed responses.
\item \textbf{Cross-Modal Learning} (CML) models, including MiniGPT-4\cite{zhu2023minigpt}, LLaVa\cite{liu2023visual}, and X-LLM\cite{chen2023x}, leverage the architecture of BLIP2. These models use the Q-Former or adapter for modality fusion and alignment, with prompts allowing free modality fusion and enhanced model flexibility.
\end{itemize}

In addition, text-image generation models like DALLE-2\cite{ramesh2022hierarchical} and UniDiffuser\cite{bao2023one} employ diffusion models\cite{yang2022diffusion} to generate images from text. These models have potential applications across art, design, and scientific visualization. Moreover, diffusion models are also used in molecular research for molecule structure graph generation tasks.

In multi-modal tasks, image-captioning and text-image generation stand out as significant directions. These models integrate information across different modalities, highlighting the potential of multi-modal learning in comprehensively understanding the connections between different modalities.

\subsection{Multi-modal Model in Molecular Science}
In molecular science, multi-modal models focus on molecule-caption translation tasks and utilize multi-modal representations for downstream tasks like molecular property and chemical reaction prediction.
 
\textbf{Molecule-Caption Translation}, in this task, our model can learn a shared semantic space from a dataset of molecules paired with their text descriptions. Like Text2Mol\cite{edwards2021text2mol}, uses natural language descriptions to retrieve molecules. Moreover, models including KV-PLM\cite{zeng2022deep} and MolT5\cite{edwards2022translation} significantly contribute to this area. KV-PLM builds a machine reading system, pre-trained on the domain-specific corpus, linking molecules and biomedical text. MolT5 is a self-supervised framework enhancing molecule-caption translation tasks. In addition, MoleculeSTM and MoMu bridge molecular graphs and text data through contrastive learning. MolReGPT applies a retrieval-based paradigm for molecule-caption translation, leveraging LLMs like ChatGPT without fine-tuning.
These models, transforming between SMILES expressions and text descriptions, have the potential for further advancement, with possibilities including better modality alignment, fusion strategies, and fine-tuning methods.

\textbf{Molecule Image Captioning}, rule-based models such as MolVec2\cite{peryea2019molvec} and OSRA\cite{filippov2009optical}, along with machine learning-based ones like DECIMER\cite{rajan2021decimer} and SwinOCSR. These models use image encoders like Vision Transformers (ViT)\cite{dosovitskiy2020image} or ResNet\cite{he2016deep} and process image features using recurrent neural networks (RNNs) or transformers\cite{vaswani2017attention} to decode into SMILES strings.
Unlike the general domain, their results mainly focus on transforming images into SMILES strings. None of them can translate molecular images into detailed textual descriptions.
 
\textbf{Molecular Property Prediction}, GNN-based models such as GraphCL\cite{wang2022molecular} and GraphMAE\cite{hou2022graphmae} are used, leveraging contrastive learning and self-supervised graph autoencoders respectively. Models like Uni-mol\cite{zhou2022uni} and GraphMVP\cite{liu2021pre} process 3D graph data effectively, while MoMu and MoleculeSTM combine SMILES and graph representations for downstream tasks.

The current challenge lies in enhancing modal fusion and merging data from various sources like text and graphs. Though these tasks demonstrate the vast potential of multi-modal learning in molecular science, there is still room for improvement in capturing intricate relationships between data sources, paving the way for further scientific discovery and innovation advancement.

\begin{figure*}[t!]
\centering
\includegraphics[width=0.90\textwidth]{fig/figure2_s.png}
\caption{\textbf{Architecture and Pre-train Strategy of GIT-Former}. Inspired by BLIP2's Q-Former, GIT-Former aligns graph, image, and text with the target text modality (SMILES or captions) using self-attention and cross-attention. The learnable queries interact with each other and the various modalities through these attention layers. Xmodal-Text Matching (XTM) and Xmodal-Text Contrastive Learning (XTC) represent our self-supervised learning strategies tailored for specific modalities (X) and target text modalities. \textbf{Encoder}: The graph encoder utilizes Graph Isomorphism Network (GIN), while the image encoder adopts SwinTransformer (SwinTRM). In the Text Embedding layer, we replace the BERT pre-trained model in Q-Former with SciBERT to better align with the demands of molecular science.}
\label{fig: GIT-Former architecture and Pretrain Strategy}
\end{figure*}




\section{Methodology}

\subsection{Overview}
In this work, we introduce GIT-Mol, a multi-modal large language model for molecular science, which is designed to manage various molecular data types, including graphs, images, and text. In addition, we propose an a novel architecture capable of mapping all modalities into a unified latent space, named GIT-Former. This work categorizes Molecular information into internal and external forms, facilitating seamless interchange and generation. As shown in Figure\ref{fig:Conceptual-GIT-MOL}. GIT-Mol can fuse data from diverse sources and present a comprehensive view of molecules, suitable for intricate tasks in molecular science. During the pre-training phase, the model employs cross-attention and contrastive learning to align different modalities, enriching our understanding of the molecular data. The fine-tuning phase is guided by a novel translation strategy that adapts to various tasks, enhancing the model's performance, especially in molecular property prediction.

In summary, GIT-Mol's ability to handle complex molecular tasks significantly advances multi-modal molecular modeling, setting the stage for innovations in molecular representation and generation.

\subsection{Data and Preprocessing Strategy}


We collected approximately 4.8 million chemical compounds from the PubChem\cite{kim2019pubchem} database, providing a robust training dataset for our model. This dataset contains molecular images for image captioning tasks and serves as a rich resource for self-supervised learning with SMILES and molecular graph representations.
In addition, we use the standard ChEBI-20 dataset\cite{edwards2021text2mol}, consisting of 33,010 molecule-description pairs, for fine-tuning and evaluation. While the molecule captions in some databases are less complex than those in ChEBI-20, we created a dataset from ChEBI\cite{hastings2016chebi} and PubChem, then screened them for accurate captions based on specific guidelines. This dataset contains 90,000 valid molecules for fine-tuning in molecule captioning.

\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\textwidth]{fig/finetune.png}
\caption{\textbf{Fine-tuning Strategy of Modality Translation.}. This strategy feeds data from various modalities (i.e. graph, image, and text) into the GIT-Former model. GIT-Former generates a fixed-length representation through the learned \_. This representation along with a prompt are the input of the MolT5 decoder, translating into the target text.}
\label{fig: finetune Strategy}
\end{figure*}

\subsection{GIT-Former}
GIT-Former is an architecture that can map all modalities into a unified latent space, designed based on the Q-Former architecture in BLIP2. It leverages the strengths of various encoder and decoder models, tailored to suit the specific characteristics of different data modalities such as SMILES strings, captions, images, and structure graphs. This section first introduces the model architecture. Then, it delineates the pre-training and fine-tuning in two stages: (1) the Xmodal-to-language representation learning stage with frozen encoders and (2) the Xmodal-to-language generative learning stage with the decoder. The GIT-Former model architecture and pre-train strategy are shown in Figure \ref{fig: GIT-Former architecture and Pretrain Strategy}. The fine-tuning strategy is shown in Figure \ref{fig: finetune Strategy}.

\textbf{GIT-Former} model serves as a modality mixer to 
 fuse molecular data. Unlike existing Visual Language (VL) models like BLIP2's Q-Former, our approach can address the graph and text modality translation tasks, providing the flexibility needed in bio-molecular multi-modal studies. To better fit our scientific scenario, we replace the BERT\cite{devlin2018bert} model with SciBERT\cite{beltagy2019scibert}, tailored explicitly to scientific text, and enhance it with cross-attention mechanisms for graph and text modalities. As shown in Figure 2, our model can adaptively adjust its training modules to achieve modality alignment effectively. Moreover, alternatives such as MolT5 can also be utilized as embedding and self-attention layers, providing flexibility to adapt to different molecular scenarios.

\textbf{Encoder and Decoder}: MolT5 is an advanced language model to translate molecule and molecular textual descriptions. We adopt it to serve as our text encoder and decoder. For image encoding, we adopt the SwinTransformer\cite{liu2021swin} from SwinOCSR, and for graph encoding, we select the GIN model from the pre-trained MoMu model. This multi-encoder and decoder setup equips our model with the flexibility to adapt to the unique demands of each data modality, enhancing the model's overall performance and efficacy.

\subsection{Strategy}
Our research involves various training strategies. In the pre-training phase, we use frozen image and graph encoders and SciBERT for text encoding. 
Moreover, the GIT-Former aligns each modality with the target text by self-supervised learning, enhancing molecular translation tasks. 
We use a unique cross-attention method in pre-training to achieve inter-modal and contrastive learning. 
During the fine-tuning phase, we incorporate our prompt manager to create an any-to-language training mechanism, allowing for training in different modalities. 
Additionally, we optimize our model's parameters using labeled data from MoleculeNet's\cite{wu2018moleculenet} molecular property prediction tasks during the fine-tuning process.

\textbf{Xmodal-Text Matching (XTM)} aims to align different modal representations with corresponding text. It is a binary classification task that determines if a set of cross-modal texts matches. A bi-directional self-attention mask facilitates interaction between learnable queries and modality embeddings, producing query embeddings with multi-modal information. These are processed through a linear classifier to get a logit, and the average across queries gives the final matching score. For the different modalities (X-modal and text-modal), we convert the inputs into embeddings ( and ) and then pair them with matched and mismatched samples, creating a contrast for the model to recognize the correct combinations. Self-attention and cross-attention mechanisms are employed to understand relations within and across modalities, resulting in a unified embedding that contains multi-modal information. This process can be represented with  as the function for both attention mechanisms.



Lastly, the fused embeddings are processed through a linear layer with a weight of , yielding a predictive score (logit). The cross-entropy loss between the logit and actual labels (1 for match, 0 for mismatch) is computed. This loss is a metric for the model's performance on the binary classification task, and its optimization helps the model effectively match information across different modalities.



\textbf{Xmodal-Text Contrastive Learning (XTC)} aligns different information types with corresponding text representations. This technique contrasts the similarity of matched cross-modal text against mismatches. The representation from the task modality is aligned with the text, and the one with the highest similarity is selected as the cross-modal data pair. In the XTC approach, we first employ the GIT-Former to attain the learned representation of the X-modal input. We then extract the X-modal features  from this representation. Subsequently, we compute the mutual information, , individually for X-modal and text-modal representations . Finally, we use cross-entropy to calculate the loss function . This can be formalized in the following equations:



\textbf{Prompt Tuning for Modality Translation}: As shown in Figure \ref{fig: finetune Strategy}, our model encodes data from each modality into vectors, and then the GIT-Former maps vectors into a unified latent space. This allows seamless interaction and translation among different modalities within a common representation. Moreover, the framework supports translating information across any modality to the language modality, providing a flexible setup for managing tasks such as prompt management, data loading, and model training. This is achieved by designing various task types and corresponding prompts decoded and processed according to the specific modality.

\textbf{Fine-tuning for Property Prediction Tasks}: We use MolT5 with LoRA as embedding and self-attention layers to process SMILES and employ contrastive learning to pre-train SMILES and graph embeddings. Our approach emphasizes utilizing multi-modal data, integrating modality representations through attention mechanisms and dynamic weighting.


\section{Experiments}
\label{sec:eva}
In our experiments, we focus on several essential tasks in molecule-caption translation and molecular property prediction, demonstrating the versatility and effectiveness of our multi-modal model.


\subsection{Molecule Captioning}

\textbf{Experimental Setup}: We adopt several standard metrics commonly used in Natural Language Processing (NLP) tasks to evaluate our model's performance. It includes BLEU, ROUGE, and METEOR, which collectively measure the quality and relevance of the generated text. The MolT5-base model is our baseline, chosen for its comparability with our GIT-Former and its use as the text decoder in our setup.
\begin{table*}[t]
	\centering
    \caption{\textbf{Molecule captioning results}. GIT-Mol demonstrates exceptional performance across various caption generation evaluation metrics. In an ablation study, the employment of this multi-modal model resulted in an impressive 10\%-15\% improvement over single-modality models.}
    \label{tab: Molecule captioning results}  
	\begin{tabular}{c|c|c|c|c|c|c}
	\hline
	\textbf{Model} & \textbf{BLEU-2} & \textbf{BLEU-4} & \textbf{ROUGH-1} & \textbf{ROUGH-2} & \textbf{ROUGH-L} & \textbf{METEOR}\\
		\hline
		MolT5-base & 0.316 & 0.247 & 0.572 & 0.480 & 0.545 & 0.529\\	\hline
		GIT-Mol-graph & 0.290  & 0.210 & 0.540 & 0.445 & 0.512 & 0.491\\
            GIT-Mol-SMILES & 0.264  & 0.176 & 0.477 & 0.374 & 0.451 & 0.430\\
            GIT-Mol-(graph+SMILES) & \textbf{0.352}  & \textbf{0.263} & \textbf{0.575} & \textbf{0.485}& \textbf{0.560} & \textbf{0.533}\\
		\hline
	\end{tabular}
\end{table*}

\textbf{Results}: Our model demonstrates superior performance in generating high-quality, relevant molecule captions. Detailed results are shown in Table \ref{tab: Molecule captioning results}.
Our GIT-Former has been proven effective in accurately representing text and successfully transforming various data types within the same modality. Our ablation study confirms that multi-modal representation outperforms single-modality representation, highlighting the value and strength of our approach.

\subsection{Text-Based de novo Molecule Generation}
\textbf{Experimental Setup}: We measure the chemical similarity between ground-truth molecules and those produced by our model using Fingerprint Tanimoto Similarity (FTS), considering MACCS, RDK, and Morgan fingerprints. Given that the generated molecules are represented in SMILES string format, we assess the results using NLP measures, including BLEU, Exact Match, and Levenshtein distance, which provide insights into the differences between the ground-truth molecules and the generated molecules' SMILES. To determine the validity of the molecules produced by our model, we use RDKIT to verify and report the percentage of valid molecules. In line with the molecule captioning task, we utilize MolT5-base as the decoder and baseline for our model.
 
\textbf{Results}: Our model exhibits impressive results in generating chemically valid molecules that closely match the ground-truth molecules regarding chemical and structural characteristics. The results are shown in Table\ref{tab: Molecule generation results}.
 \begin{table*}[htbp]
	\centering
    \caption{\textbf{Molecule generation results}. Our model performs similarly to MolT5-base regarding metrics for generating SMILES molecules but excels in ensuring molecule validity. With a molecule validity of \textbf{92.8\%}, our model surpasses MolT5-base's validity of 77.2\% by over \textbf{20\%}. It also outperforms MolT5-large's molecule validity of \textbf{90.5\%}.}
    \label{tab: Molecule generation results}
	\begin{tabular}{c|c|c|c|c|c|c|c}
	\hline
	\textbf{Model} & \textbf{BLEU} & \textbf{Exact} & \textbf{Levenshtein} & \textbf{MACCS FTS} & \textbf{RDK FTS} & \textbf{Morgan FTS} & \textbf{Validity}\\
		\hline
		MolT5-base-caption & \textbf{0.769} & \textbf{0.081} & \textbf{24.458} & 0.721 & \textbf{0.588} & \textbf{0.529} & 0.772\\	\hline
		GIT-Mol-caption & 0.756  & 0.051 & 26.315 & \textbf{0.738} & 0.582 & 0.519 & \textbf{0.928}\\
		\hline
		SwinOCSR-image & 0.892 & 0.376 & 9.157 & 0.945 & 0.872 & 0.846 & 0.827\\	\hline
		GIT-Mol-image & \textbf{0.924} & \textbf{0.461} & \textbf{6.575} & \textbf{0.962} & \textbf{0.906} & \textbf{0.894} & \textbf{0.899}\\
		\hline
	\end{tabular}
\end{table*}

\subsection{Molecule Image Captioning}
\textbf{Experimental Setup}: We employ the identical set of metrics as those used in "Text-Based de novo Molecule Generation." For a fair and rigorous assessment, we juxtapose our model with SwinOCSR, an acknowledged efficient model in molecular image captioning.
 
\textbf{Results}: The results for the molecule image captioning task are presented in Table \ref{tab: Molecule generation results}. The empirical outcomes indicate that our model outperforms the baseline model, marking a substantial enhancement in this task.

\subsection{Molecular Property Prediction}

\textbf{Experimental Setup}: In our study, we use six essential classification datasets related to molecular biological activity, including Tox21, ToxCast, Sider, ClinTox, Bace, and BBBP. These datasets contribute to understanding a drug's properties and effects.

We ensure the model's fairness and robustness by using a scaffold-based splitting approach for structuring training, validation, and test sets. This leads to diverse molecule evaluations. The pre-trained model on PubChem processes the datasets into molecule embeddings, and we optimize using the Binary Cross-Entropy (BCE) loss function.
\begin{table*}[t]
	\centering
    \caption{ \textbf{Results for molecular property prediction (classification)}. The combined use of \textbf{SMILES} and \textbf{2D graphs} enhances our multi-modal molecular representation, which outperforms both single-modal models and other multi-modal approaches.}
    \label{tab: MoleculeNet results} 
	\begin{tabular}{c|c|c|c|c|c|c|c}
	\hline
	\textbf{Dataset} & \textbf{Tox21 ↑} & \textbf{ToxCast ↑} & \textbf{Sider ↑} & \textbf{ClinTox ↑} & \textbf{BBBP ↑}& \textbf{Bace ↑}&\textbf{Avg}\\
        \textbf{Molecules} & 7831 & 8575 & 1427 & 1478 & 2039& 1513&--\\
        \textbf{Task} & 12 & 617 & 27 & 2 & 1& 1&--\\
		\hline
        KV-PLM & 72.11.0 & 55.01.7 & 59.80.6 & - & 70.50.5 & 78.52.7 
        & 67.20  \\
        GraphCL & 75.10.7 & 63.00.4 & 59.81.3 & 77.53.8 & 67.82.4 & 74.62.1
        & 69.64 \\
        GraphMVP & 74.90.5 & 63.10.2 & 60.20.13 & 79.12.8 & 70.80.5 & 79.31.5
        & 71.23 \\
        MoMu & 75.60.3 & 63.40.5 & 60.50.9 & 79.94.1 & 70.52.0 & 76.72.1
        & 71.1 \\
        Mole-BERT & \textbf{76.80.5 }& 64.30.2 & 62.81.1 & 78.93.0 
        & 71.91.6 & 80.81.4 &72.58 \\
        \hline
        GIT-Mol-SMILES & 73.90.7 & 62.10.4 & 60.11.1 & 83.53.1 & 71.91.8 & 68.41.7 & 70.0 \\
        GIT-Mol-graph & 75.40.5 & 65.30.7 & 58.20.9 & 78.92.5 & 71.11.5 & 
        65.81.8 & 69.1 \\
        GIT-Mol-(graph+SMILES) & 75.90.5 &\textbf{66.80.5} & \textbf{63.40.8} & 
        \textbf{88.31.2}& 
        \textbf{73.90.6} & 
        \textbf{81.081.5} & 
        \textbf{74.90} \\
		\hline
	\end{tabular}
\end{table*}
\textbf{Results}: Our model performs commendably in predicting molecular properties, achieving high AUC scores across multiple runs. The results of our model and baselines are shown in Table \ref{tab: MoleculeNet results}. 

\section{Conclusion and Discussion}
\label{sec:con}
We introduce GIT-Mol, a domain-specific multi-modal large language model for molecular science, and achieve excellent performance in molecule captioning, generation, image captioning, and property prediction. In addition, a new multi-modal data mixer, GIT-Former, is proposed for fusing multi-modal molecular data. Our model outperforms existing methods and offers an any-to-language modality translation strategy, enhancing flexibility for various applications.

While we employ parameter-efficient training methods, the model's training speed remains slow. In future work, we aim to accelerate this by adopting advanced techniques such as Prompt Tuning and Adapters.
Furthermore, We also intend to expand into new task domains, including compound name recognition and chemical reaction prediction.
Finally, we plan to incorporate explainable AI into our model to enhance the model's trustworthiness and scientific interpretability.

\section{Acknowledgements}
The research was supported by the PengCheng Laboratory and the PengCheng Laboratory Cloud-Brain.





































\bibliography{references.bib}

\section*{Appendix}
\label{app:Appendix}
\subsection*{A. Data}
\subsubsection*{A.1 Data Modalities}
Our dataset's diverse data modalities allow the model to learn and understand complex relationships across different modalities, ultimately enhancing its performance in molecular property prediction and generation tasks. As shown in Figure 1. Molecular information is categorized as internal and external information. Internal data like SMILES and structural graphs are important for predicting molecular properties and features. External data, like text descriptions and molecule images, are user-friendly and easy to interpret. Combining these types of information creates a comprehensive representation of molecules that balances machine efficiency and human interpretability. The data modalities utilized in our model include the following:

\textbf{SMILES}: The Simplified Molecular Input Line Entry System (SMILES) offers a compact, textual representation of molecular structures. It encodes information regarding the connectivity and stereochemistry of molecules in a compact and human-readable format. Our model manipulates these SMILES strings to capture critical molecular details.
 
\textbf{Molecule Structure Graphs} depict molecules in a two-dimensional space, with nodes representing atoms and edges representing bonds. This topological portrayal of molecular structures allows our model to analyze and predict molecular properties.
 
\textbf{Molecule Captions} are textual descriptions detailing the molecules' characteristics and properties. These descriptions offer a natural language context that enables the model to understand and generate human-readable explanations of molecular features.
 
\textbf{Molecule Images} are 2D visual representations of molecules that illustrate atomic structures and bonding patterns. They provide an intuitive means for our model to analyze and process molecular structures.
\subsubsection*{A.2 Data Preprocessing}

Regarding data preprocessing, our process involves several stages to prepare the raw data for model training and evaluation:
 
\textbf{Data Cleaning}: This step involves removing or rectifying any inconsistencies, missing values, or errors in the data. This step ensures that the model is fed high-quality input data without issues that could negatively impact its performance.
 
\textbf{Low-Frequency Atoms}: We analyze the distribution of atom counts in the compounds and remove those containing low-frequency atoms. This step ensures the model concentrates on learning patterns from compounds with common and relevant atomic structures.
 
\textbf{Validation of Graph Structures}: We use the RDKit\footnote{http://www.rdkit.org/} toolkit to validate the effectiveness of the graph structures. We refine the dataset by applying these data-cleaning strategies to include high-quality, representative compounds with pertinent features and properties. Improving the learning efficiency of the model results in better performance on the desired tasks.

\subsubsection*{A.3 Pretraining Dataset}

\textbf{Image, Graph, and SMILES pretrain dataset}: We extract pertinent data related to molecular properties from PubChem's classification webpage\footnote{https://pubchem.ncbi.nlm.nih.gov/classification/}, serving as our base dataset. The specifics of the classifications and quantities are delineated in Table 6. We navigate to the corresponding classification's search page and download the summary data, acquiring the SMILES expressions for each molecule. In addition, we can download the associated image set from the page. Following this, we consolidate all classifications into a union, de-duplicate based on the CID, and verify the validity of the molecules using the RDKit tool.

\begin{table*}[ht]
\centering
\begin{tabular}{|c|p{0.5\textwidth}|c|}
\hline
\textbf{Classification Category} & \textbf{Meaning} & \textbf{Number of Items} \\
\hline
Biological Test Results & Biological test results from the PubChem BioAssay database. & 4,497,660 \\
\hline
Chemical and Physical Properties & Various chemical and physical properties & 267,837 \\
\hline
Associated Disorders and Diseases & Disorders and diseases associated with the compound & 29,990 \\
\hline
ChEMBL Target Tree & Classification of this entity in the context of targets, provided by ChEMBL & 1,040,742 \\
\hline
KEGG: Drug & Classification/ontology tree for therapeutic categories of KEGG. & 1,499 \\
\hline
KEGG: Drug Groups & KEGG: Drug Groups tree & 4,851 \\
\hline
Drug and Medication Information & This section provides drug and medication information for this compound. & 19,108 \\
\hline
Pharmacology and Biochemistry & Pharmacodynamics, pharmacokinetics, metabolism, mechanism of action & 113,370 \\
\hline
Toxicity & Routes of exposure, related symptoms, acute and chronic effects, and numerical measures of toxicity. & 116,629 \\
\hline
Union Total & --- & 4,805,592 \\
\hline
\end{tabular}
\caption{Summary of image, graph, and SMILES pretrain dataset.}
\label{tab:pretrain_image_dataset_summary}
\end{table*}

\textbf{SMILES and caption pretrain dataset}: We scraped the biological activity and medicinal property-related caption data for the 4.8 million molecules from their respective display pages. The stipulation is for captions to exceed 50 characters, ensuring their validity. In total, we collected 220k SMILES and caption pairs. Using a data processing method like MolT5, we replace the compound name in the caption with 'the molecule.'

\subsubsection*{A.4 Fine-tuning Dataset}

\textbf{Molecule Caption Dataset.} Our team procured a substantial dataset of 320,000 molecules accompanied by detailed descriptions from the PubChem Record Description category. We enhanced this dataset by conducting a thorough crawl of the corresponding captions (up to a maximum of 3) from each molecule page. We augmented our dataset by combining it with the ChEBI-20 Dataset, thus forming a highly comprehensive and informative multi-modal caption dataset.

We further carried out data cleaning to remove "invalid" descriptions in the captions, such as those that merely specify the source of the compound without any pertinent information about its properties or characteristics. However, we opted to retain succinct descriptions that provide essential information about the compound. This enriched dataset consists of approximately 90,000 data points, incorporating multi-modal data such as images of the molecules, SMILES, and captions.

\textbf{ChEBI-20 Dataset for Molecule Generation Task.} We use the same ChEBI-20 Dataset as MolT5 for fine-tuning our model on the molecule generation task. It contains various molecular structures represented in SMILES notation, providing a rich resource for training models to generate novel molecules. 

\textbf{MoleculeNet for Molecular Property Prediction.} In our study, we chose a few classification task datasets from MoleculeNet as our fine-tuning datasets. These datasets include Tox21, ToxCast, Sider, ClinTox, BBBP, and Bace, each covering different molecular property prediction tasks.
\begin{itemize}
\item \textbf{Tox21}: A collaborative project to identify potential toxins,  containing toxicity data for approximately 7800 compounds across 12 different pathways.
\item \textbf{ToxCast}: Contains biological activity data for about 8600 environmental compounds from over 600 in vitro experiments, used to assess potential toxicity.
\item \textbf{Sider}: A database of side effect information for around 1400 drugs, used for predicting potential drug side effects.
\item \textbf{ClinTox}: Includes toxicity information for known drugs and clinical toxicity predictions for unapproved compounds, primarily used to predict clinical toxicity.
\item \textbf{BBBP}: Utilized to predict whether compounds can penetrate the blood-brain barrier, aiding in understanding brain delivery capacity.
\item \textbf{Bace}: Contains inhibitory activity information on beta-secretase 1 (BACE1) for 1513 compounds, useful in identifying potential candidate drugs for Alzheimer's disease treatment.
\end{itemize}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.90\textwidth]{fig/embedding_vis.png}
\caption{Embeddings visualization.}
\label{fig: embedding_vis}
\end{figure*}

\subsection*{B.Hardware Environment}
\textbf{CPU}:
\begin{itemize}
\item Architecture: x86\_64
\item Number of CPUs: 96
\item Model: Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz
\end{itemize}

\textbf{GPU}:
\begin{itemize}
\item Type: Tesla V100-SXM2-32GB
\item Count: 8
\item Driver Version: 450.80.02
\item CUDA Version: 11.7
\end{itemize}

\textbf{Software Environment}:
\begin{itemize}
\item Operating System: Ubuntu 18.04.6 LTS
\item Python Version: 3.10.9
\item PyTorch Version: 2.0.0
\end{itemize}

\textbf{Tools and Libraries}:
\begin{itemize}
\item The Accelerate library from Hugging Face.
\item Parallel acceleration with DeepSpeed’s ZeRO stage 2.
\item AdamW optimizer.
\end{itemize}

\subsection*{C.Training Settings}
\subsubsection*{C.1 Pretraining Settings}
\begin{itemize}
\item \textbf{Training and Hyperparameters}: The encoder settings freeze both the image and graph encoders. For modality selection, 1-3 arbitrary modalities and the target text modality are selected for pre-training. The learning rate ranges from 1e-4 to 5e-5, and the training process is constrained to less than 20 epochs. The batch size is determined by the number of input modalities, ranging from 32 to 128 (the more input modalities, the smaller the batch size).
\item \textbf{Optional Settings}: There are options for replacement in the GIT-Former model, such as replacing the Embedding layer with other models like BERT or MolT5's Encoder. Training strategies can also be tailored to the experiment, including Xmodal-Text Matching (XTM), Xmodal-Text Contrastive Learning (XTC), or both.
\end{itemize}
\subsubsection*{C.2 Fine-tuning Settings}
\begin{itemize}
\item \textbf{Molecule Translation Task}: The task type determines the hyperparameters, including input and output modalities and batch size. The model has No parameters frozen, and the learning rate ranges from 1e-4 to 5e-5. The model is trained for less than 20 epochs, with a batch size between 32 and 64.
\item \textbf{Molecule Prediction Task}: The task determines specific hyperparameters, such as batch size. The model's Embedding layer utilizes MolT5-large, and the Hugging Face's PEFT library LoRA is employed for efficient parameter training. The learning rate is again set between 1e-4 and 5e-5, with less than 100 epochs in training. Patience is set to 10, and batch size ranges between 16 and 256.
\end{itemize}
\subsection*{D.Embedding Visualization}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.93\textwidth]{fig/figure5_s.png}
\caption{Study case of Molecule Caption and Generation.}
\label{fig: Study case}
\end{figure*}

\textbf{GIT-Former Processing}:  We experimented on 80 molecules to create vector visualizations, as shown in Figure 4. The first image in Figure 4(a) shows the vector representations created by the encoders for each modality of molecular data. The second image displays the vector representations without the image embeddings. Our analysis of these images reveals that the graph vectors are more concentrated in the original vector representations, while the image representations are more diverse. Additionally, the SMILES and caption representations, both text-based, are not easily distinguishable.
 
\textbf{Pre-training Effects}: In Figure 4(b), we display the vectors processed by an untrained GIT-Former. All vectors appear to move towards a uniform distribution. In contrast, Figure 4(c) shows the vectors processed by a pre-trained GIT-Former. The vector distribution shifts from dispersed to concentrated, with the outermost layer being the graph embeddings, the image embeddings, and the SMILES and captions in the text modality. This occurs because our GIT-Former's pre-training strategy involves aligning other modalities to the text modality. The SMILES and captions in the text modality can be further distinguished from the original vector representations.
 
\textbf{Clustering and Distribution of atoms}: The image in Figure 4(d) displays the distribution of the number of atoms in a molecule. The color changes from dark to light, indicating an increase in the number of atoms. The distribution of the number of atoms is similar when viewed on the same vertical axis. Moving on to Figure 4(e), we conducted K-means clustering on the molecules. We observed the distribution of C, N, and O atoms in Figure 4(f) based on different groups. We can see some differences in the distribution among the different groups. However, by representing different molecules based on these differences, our GIT-Former can differentiate between different modalities, data types within the same modality, and molecular properties within the same data type.

\subsection*{E. Case Studies}

\subsubsection*{E.1 Molecule Caption Generation}
We select a set of molecules, feed them to our model, and let it generate captions based on the learned representations. The results are shown in Figure 5. a. For each molecule, we present the ground truth, the captions generated by our model, the baseline model, and GPT4. This comparative analysis allows us to visually inspect and evaluate the quality and relevance of the generated captions. 

\textbf{GIT-Mol Model Characteristics}:
\begin{itemize}
\item\textbf{Specificity}: GIT-Mol provides detailed information that aligns closely with the ground truth, recognizing chemical structures and their origin, such as isolation from specific plants.
\item\textbf{Chemical Understanding}: The model identifies complex molecular structures, including functional groups and specific chemical attributes like neolignans and dimethoxybenzenes.
\item\textbf{Alignment with Ground Truth}: In all three examples, GIT-Mol's descriptions closely align with the ground truth, accurately depicting the molecules.
\end{itemize}

The GIT-Mol model exhibits precise chemical characterization, aligning closely with ground truth information, unlike MOLT5, which can misidentify compounds. Compared to GPT-4, GIT-Mol offers more comprehensive descriptions, demonstrating a deeper understanding of chemistry by connecting structural elements to specific chemical identities and biological relevance.

The GIT-Mol model is robust in understanding and accurately describing complex organic compounds. It identifies specific functional groups and structural features and correlates them to biological functions and origins. Its superiority over the other models lies in its detailed, specific, and accurate interpretation of chemical structures, making it a valuable tool for tasks requiring deep chemical understanding and precise prediction.

\subsubsection*{E.2 Molecule Generation from Text}
We provide some examples of molecule generation based on text descriptions and detailed analysis. In Figure 5. b, we showcase the outcomes for every molecule instance. Our model, the baseline model, and GPT4 all generate SMILES strings, which are compared to the ground truth. This comparison allows for a visual evaluation of the quality and relevance of the generated molecules, offering a better understanding of our model's effectiveness. Check out Figure 5. b for a comprehensive case study on molecule generation.

\textbf{GIT-Mol Model Characteristics}:
\begin{itemize}
\item\textbf{Effectiveness in Generating Molecules}: GIT-Mol significantly improves the effectiveness of generating molecules compared to the baseline, showcasing its superior ability to translate captions into corresponding SMILES representations.
\item\textbf{Precision in Complex Cases}: Unlike the baseline, which may struggle to generate specific molecules, GIT-Mol can generate precise representations even in challenging scenarios.
\item\textbf{Alignment with Text Descriptions}: GIT-Mol stands out for its ability to create molecular representations that, even if not identical to the ground truth, still faithfully adhere to the features and characteristics described in the textual captions.
\end{itemize}

Overall, GIT-Mol demonstrates remarkable advancements in accuracy, complexity handling, and alignment with textual descriptions, positioning it as a robust and intelligent model for converting captions to molecular structures.
\end{document}