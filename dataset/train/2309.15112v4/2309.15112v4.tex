

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{mmstyle}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage[table]{xcolor}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage[hang,flushmargin]{footmisc} 


\newcommand{\rankone}{add8e6}
\newcommand{\ranktwo}{b5dce9}
\newcommand{\rankthree}{bde0eb}
\newcommand{\rankfour}{c6e4ee}
\newcommand{\rankfive}{cee8f0}
\newcommand{\ranksix}{d6ecf3}
\newcommand{\rankseven}{deeff5}
\newcommand{\rankeight}{e6f3f8}
\newcommand{\ranknine}{eff7fa}
\newcommand{\rankten}{f7fbfd}

\usepackage[accsupp]{axessibility} 

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\def\cvprPaperID{0000} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}



\title{InternLM-XComposer: A Vision-Language Large Model for \\ Advanced Text-image Comprehension and Composition}

\author{Pan Zhang$^{*1}$, Xiaoyi Dong$^{*1}$, Bin Wang$^{1}$, Yuhang Cao$^{1}$, Chao Xu$^{1}$, Linke Ouyang$^{1}$, Zhiyuan Zhao$^{1}$, \\ Shuangrui Ding$^{1}$, Songyang Zhang$^{1}$, Haodong Duan$^{1}$, Wenwei Zhang$^{1}$, Hang Yan$^{1}$, Xinyue Zhang$^{1}$, Wei Li$^{1}$, \\ Jingwen Li$^{1}$,
Kai Chen$^{1}$, Conghui He$^{1}$, Xingcheng Zhang$^{1}$, Yu Qiao$^{1}$, Dahua Lin$^{1}$, Jiaqi Wang$^{1,}${\textsuperscript{\Letter}}\\
$^1$Shanghai Artificial Intelligence Laboratory \\
\tt\small
internlm@pjlab.org.cn
}

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \centering
    \vspace{-25pt}
    \includegraphics[width=1.0\linewidth]{figures/teaser.pdf}
    \setlength{\abovecaptionskip}{0mm}
    \captionof{figure}{\small
        The InternLM-XComposer shows excellent interleaved composition and text-image comprehension ability, leading to strong performance on various multimodal benchmarks.
	}
	\label{fig:teaser}
    \vspace{-1pt}
\end{center}
}]
 \maketitle


{\let\thefootnote\relax\footnotetext{\noindent* indicates equal contribution.}}

\begin{abstract}

We propose InternLM-XComposer, a vision-language large model that enables advanced image-text comprehension and composition. The innovative nature of our model is highlighted by three appealing properties:
1) \textbf{Interleaved Text-Image Composition}: 
InternLM-XComposer can effortlessly generate coherent and contextual articles that seamlessly integrate images, providing a more engaging and immersive reading experience. Simply provide a title, and our system will generate the corresponding manuscript. It can intelligently identify the areas in the text where images would enhance the content and automatically insert the most appropriate visual candidates.
2) \textbf{Comprehension with Rich Multilingual Knowledge}: The text-image comprehension is empowered by training on extensive multi-modal multilingual concepts with carefully crafted strategies, resulting in a deep understanding of visual content.
3) \textbf{State-of-the-art Performance}: Our model consistently achieves state-of-the-art results across various mainstream benchmarks for vision-language foundational models, including MME Benchmark, MMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).
Collectively, InternLM-XComposer seamlessly blends advanced text-image comprehension and composition, revolutionizing vision-language interaction and offering new insights and opportunities.
The InternLM-XComposer model series with 7B parameters are publicly available at \url{https://github.com/InternLM/InternLM-XComposer}.

\end{abstract}
 \section{Introduction}
\label{sec:intro}
Over the past year, impressive progress has been made in developing large language models (LLMs)~\cite{raffel2020exploring,brown2020language,chowdhery2022palm,openai2020chatgpt,openai2023gpt4,touvron2023llama,vicuna2023,qwen7b,touvron2023llama2,baichuan2023baichuan2,2023internlm,du2022glm}. These state-of-the-art models, including ChatGPT~\cite{openai2020chatgpt}, GPT4~\cite{openai2023gpt4}, and PaLM 2~\cite{chowdhery2022palm}, have shown an unprecedented ability to follow human instructions and solve open-ended tasks. 
Inspired by the success of PaLM-E~\cite{driess2023palme} and BLIP2~\cite{Li2023BLIP2BL}, there is a promising approach to extending language models for vision-language tasks by leveraging vision features as extra inputs of LLMs. The community has developed several vision-language large models (VLLMs), such as MiniGPT-4~\cite{zhu2023minigpt}, LLaVA~\cite{liu2023visual}, and InstructBLIP~\cite{dai2023instructblip}, based on open-source LLMs like LLaMA~\cite{touvron2023llama}, GLM~\cite{du2022glm}, and InternLM~\cite{2023internlm}.
However, these VLLMs focus on pure text outputs, missing the opportunity to equip generated text with richer information through auxiliary multimedia content like images.

In this work, we propose InternLM-XComposer, which is a vision-language large model that enables advanced text-image comprehension and composition ability.

1) \textbf{Interleaved Text-Image Composition}.
InternLM-XComposer excels in generating long-form content that is interleaved with contextually relevant images, thereby elevating the experience of vision-language interaction. In its operational flow, the framework first crafts text based on human-provided instructions. Subsequently, it autonomously pinpoints optimal locations within the text for image placement and furnishes corresponding, suitable image descriptions. 
In accordance with the generated descriptions, instead of relying on a text-image generation model for assistance, we opt to source aligned images from a large-scale web-crawled image database for realistic quality and contextual alignment. Moreover, it also provides flexibility by allowing users to customize an image repository.

Compared to a baseline approach that relies solely on CLIP~\cite{radford2021learning,Yang2022ChineseCC} for image retrieval, XComposer offers a more reliable solution for choosing the most appropriate image. Initially, we select potential image candidates from our database using CLIP. Then, InternLM-XComposer leverages its comprehension capabilities to identify the image that optimally complements the content.
   
2) \textbf{Comprehension with Rich Multilingual Knowledge}. 
LLM demonstrates remarkable generalizability in handling open-world tasks, a capability attributed to its extensive training data, \eg, 2T tokens used in LLaMA2~\cite{touvron2023llama2}. This vast dataset inherently encapsulates a broad spectrum of semantic knowledge across diverse domains. In contrast, existing vision-language datasets are relatively constrained in both volume and diversity. To tackle these limitations, we employ two practical solutions: First, an interleaved multilingual vision-language dataset with over 11 million semantic concepts is collected from public websites. Second, we carefully crave the pretraining and finetuning strategies in our training pipeline, where we adopt the mixed training data of pure text and image-text data, primarily in English and Chinese.
Consequently, InternLM-XComposer demonstrates a remarkable proficiency in comprehending a wide array of image content and responding with an extensive repository of multilingual knowledge. 


The proposed InternLM-XComposer exhibits superior capabilities in both text-image comprehension and composition, as evidenced by its strong performance in quantitative benchmarks and compelling qualitative demonstrations. It consistently achieves \textbf{state-of-the-art} performance across various leading benchmarks for vision-language large models, encompassing MME Benchmark~\cite{yin2023survey,fu2023mme}, MMBench~\cite{MMBench}, Seed-Bench~\cite{li2023seedbench} in English, and MMBench-CN~\cite{MMBench}, CCBench (Chinese Cultural Benchmark)~\cite{MMBench} for evaluations in Chinese. Notably, our method significantly outperforms existing frameworks on benchmarks in the Chinese language, \ie, MMBench-CN~\cite{MMBench} and CCBench~\cite{MMBench}, demonstrating unparalleled multilingual knowledgeability.

We release InternLM-XComposer series in two versions:

\begin{itemize}
    \item InternLM-XComposer-VL: The pretrained and multi-task trained VLLM model with InternLM~\cite{2023internlm} as the initial LLM.
    \item InternLM-XComposer: The further instruction tuned VLLM based on InternLM-XComposer-VL for interleaved text-image composition and LLM-based AI assistant.
\end{itemize}


    


    











 \section{Related Works}
\label{sec:related}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\linewidth,]{figures/pipeline.pdf}
	\caption{\textbf{The architecture of the InternLM-XComposer.} The proposed model comprises three essential components: a visual encoder, a perceive sampler, and a large language model. The training regimen is divided into two distinct phases, namely Stage A and Stage B. In Stage A, which serves as the pre-training phase, both the perceive sampler and the large language model are subjected to optimization procedures. Stage B focuses on supervised fine-tuning, during which the perceive sampler and LoRA~\cite{hu2022lora} are specifically trained.}
	\label{fig:architecture}
\end{figure*}

\noindent{\textbf{Large Language Models.}}
In recent years, the development of large language models has accelerated. Initially, encoder-decoder models such as BERT~\cite{devlin2018bert} and T5~\cite{raffel2020exploring}, as well as decoder-only models like GPT~\cite{radford2018improving}, leveraged the Transformer architecture~\cite{vaswani2017attention} to achieve remarkable results across various NLP tasks. GPT3~\cite{brown2020language}, employing prompt and in-context learning strategies along with larger models and data, has shown significant performance in few-shot and zero-shot downstream tasks. As a result, using decoder-only structures based on autoregressive decoding for output prediction has gained popularity among researchers. Google's PaLM~\cite{chowdhery2022palm} further expands the model parameter size and data volume, setting the performance benchmark of the time. To enhance the practical conversational experience, models like InstructGPT~\cite{ouyang2022training} and ChatGPT~\cite{openai2020chatgpt} integrate fine-tuning and reinforcement learning strategies guided by human feedback to guide models toward human-like conversation. The open-sourcing of the LLaMA~\cite{touvron2023llama} model has invigorated research on large language models, leading to the successive open-sourcing of a series of notable large language models such as Vicuna~\cite{vicuna2023}, Qwen~\cite{qwen7b}, LLaMA2~\cite{touvron2023llama2}, Baichuan2~\cite{baichuan2023baichuan2}, and InternLM~\cite{2023internlm}.

\noindent{\textbf{Multimodal Large Language Models.}}
Like large language models, visual language learning has emerged as a research hotspot in computer vision. Initially, these models utilized the Transformer architecture to align image-text pairs in unsupervised samples, enabling strong performance in zero-shot learning tasks like Image Captioning and Image-Text Retrieval. CLIP~\cite{radford2021learning} aligns image and text features through contrastive learning objectives on large-scale image-text pairs, outperforming supervised learning on ImageNet~\cite{deng2009imagenet} and exhibiting strong generalization capabilities in various downstream tasks. BLIP~\cite{li2022blip} devises data selection and generation strategies using cleaner and more diversified data, outperforming CLIP. Information mined from image-text pairs can effectively provide labels for basic visual tasks~\cite{li2021grounded,zhang2022glipv2,liu2023grounding}. However, these models show limited capabilities for tasks requiring higher-level understanding, such as visual question answering. Benefiting from existing large language model~\cite{vicuna2023} and visual encoder~\cite{fang2023eva}, MiniGPT-4~\cite{zhu2023minigpt} trains multimodal large language models (MLLMs) through pre-training feature alignment and instruction fine-tuning, exhibiting excellent image-text dialogue capabilities. For the instruction fine-tuning stage of MLLMs, a series of studies \cite{liu2023visual,wang2023vigc,zhao2023mllm} have explored the impact of the quality, diversity, and specificity of the fine-tuning data on the performance of these models, further enhancing their performance. 
MMICL~\cite{zhao2023mmicl} can manage multiple image inputs, enabling multimodal models to understand more complex Prompts. Otter~\cite{li2023otter}, built on the OpenFlamingo~\cite{awadalla2023openflamingo} structure and multimodal context instruction fine-tuning data, can better follow new instructions. InstructBLIP~\cite{dai2023instructblip}, based on various image-text datasets, fine-tunes MLLMs by constructing prompt templates and introduces Q-Former~\cite{li2022blip} to associate more relevant image features in advance. With a larger dataset, this model shows better generalization capabilities across multiple tasks. mPLUG-Owl~\cite{ye2023mplug} introduces additional text-only instruction data during the second phase of instruction fine-tuning, improving its capabilities. Shikra~\cite{chen2023shikra} and KOSMOS-2~\cite{peng2023kosmos} incorporate Grounding data during the training phase, enabling the model to develop Grounding capabilities, reduce hallucinations, and enhance performance. Qwen-VL~\cite{bai2023qwen} uses more pre-training data and higher-resolution inputs to deepen the model's understanding of image-text details.

\noindent{\textbf{Multimodal Retrieval Models.}}
Image-text retrieval, a pivotal area in multimodal modeling, has seen substantial advancements recently. CLIP~\cite{radford2021learning}, utilizing contrastive learning on a large corpus of unsupervised image-text pairs, excels in image-text matching, enabling efficient retrieval in both image-to-text and text-to-image modalities. Expanding on CLIP's foundation, BLIP~\cite{li2022blip} filters out irrelevant image-text pairs, generating a high-quality subset for retraining, thereby enhancing image-text matching performance. ALIGN~\cite{jia2021scaling} extends beyond singular image-text matching by simultaneously accommodating image-text combinations as inputs, retrieving results that meet the given image-text criteria, thus providing a more comprehensive retrieval system. Despite these models' impressive strides in image-text retrieval, they still need to improve in deep image-text understanding. REVEAL~\cite{hu2023reveal} addresses this by introducing an end-to-end retrieval-enhanced visual language model that leverages various knowledge source modalities and operates in tandem with a generator, excelling in knowledge-intensive visual question-answering tasks. RA-CM3~\cite{yasunaga2023retrieval} further enhances this process by retrieving external relevant knowledge as a supplement, facilitating superior generation of image and text results, and demonstrating remarkable performance in knowledge-intensive image generation and multimodal in-context learning tasks. FROMAGe~\cite{koh2023grounding} integrates the large language model~\cite{zhang2205opt} with the visual encoder CLIP ViT/14~\cite{radford2021learning}. Training with image-text pairs utilizes multimodal information from multi-turn dialogues for image retrieval and conversation, offering a more dynamic and interactive approach to retrieval. However, the capabilities of current models are primarily confined to the matching and generation of image-text pairs, revealing a significant gap in their ability to generate comprehensive interleaved image-text articles, signifying a promising future research direction. 

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{5mm}{
\begin{tabular}{ccccc}
\toprule
Lauguage & Type & Dataset & Selected Images & Selected Text\\
\midrule
 \multirow{5}{*}{English} & Image-text paired & SBU-Caption~\cite{Ordonez_2011_im2text}) & 1M & 18M \\
 & Image-text paired & Conceptual Captions 3M~\cite{sharma2018conceptual} & 3M & 37M \\
 & Image-text paired & Conceptual 12M~\cite{changpinyo2021conceptual} & 9M & 250M \\
& Image-text paired & LAION400M~\cite{schuhmann2021laion} & 509M & 10B \\
& Image-text paired & In-house Data & 2M & 321M \\
 & Interleaved image-text & Multimodal C4~\cite{zhu2023multimodal} & 332M & 40B \\
\midrule
\multirow{5}{*}{Chinese} & Image-text paired & WuKong~\cite{gu2022wukong} & 31M & 545M \\
& Image-text paired & TaiSu~\cite{liu2022taisu} & 44M & 865M \\
& Image-text paired & LAION-CN~\cite{schuhmann2022laion} & 80M & 2B \\
& Image-text paired & In-house Data & 9M & 704M \\
& Interleaved image-text & In-house Data & 85M & 13B \\
\midrule
&  & Total & 1.1B & 67.7B \\
\bottomrule
\end{tabular}}
\caption {Details of InternLM-XComposer pre-training multimodal data. LAION-CN represents the Chinese language subset extracted from the larger LAION-5B corpus. This subset is further cleaned utilizing the Chinese CLIP~\cite{Yang2022ChineseCC}. The volume of text data is counted in terms of the number of tokens. The in-house data are collected from public websites, possessing over 11 million semantic concepts collected from public websites. A subset of the in-house data has been made publicly available by WanJuan~\cite{He2023WanJuanAC}.}
\vspace{5pt}
\label{tab:pre-training data}
\end{table*}

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{3.5mm}{
\begin{tabular}{ll}
\toprule
Task &  Dataset\\
\midrule
\multicolumn{2}{l}{\textit{Multi-task training}} \\
Caption  &  COCO~\cite{chen2015microsoft}, SUB~\cite{chen2015microsoft}, TextCaps~\cite{sidorov2020textcaps} \\
VQA      &  VQAv2~\cite{VQAv2}, GQA~\cite{hudson2018gqa}, OK-VQA~\cite{marino2019ok}, VSR~\cite{Liu2022VisualSR}, IConQA~\cite{lu2021iconqa} \\
& Text-VQA~\cite{singh2019towards}, SQA~\cite{lu2022learn}, OCR-VQA~\cite{mishraICDAR19},  In-house data\\
IQG          & VQAv2~\cite{VQAv2}, OK-VQA~\cite{marino2019ok}, A-OKVQA~\cite{schwenk2022okvqa} \\
Conversation  &  Visual Dialog~\cite{das2017visual}, LLava-150k~\cite{liu2023visual} \\
\midrule
\multicolumn{2}{l}{\textit{Instruction tuning}} \\
Image-Text Composiiton  &  In-house data (Refer to Sec.\ref{sec:compos_data})\\
Conversation  &  LLaVA-150k~\cite{liu2023visual}, Alpaca-en\&zh~\cite{alpaca}, ShareGPT-en\&zh, Oasst-en\&zh, LRV~\cite{liu2023aligning} \\
\bottomrule

\end{tabular}}
\vspace{-5pt}
\caption {Datasets used for Supervised Fine-Tuning. }
\vspace{5pt}
\label{tab:sft data}
\end{table*}

\section{Method}

\subsection{Model Architecture}
In the proposed InternLM-XComposer framework, as depicted in Figure~\ref{fig:architecture}, there are three integral components: a visual encoder, a perceive sampler, and a large language model.

\noindent{\textbf{Visual Encoder.}} The visual encoder in InternLM-XComposer employs EVA-CLIP~\cite{fang2023eva}, an refined variant of the standard CLIP~\cite{Yang2022ChineseCC}, enhanced with mask image modeling capabilities, to proficiently capture the visual nuances of the input image. Within this module, images are resized to a consistent dimension of $224\times 224$ and subsequently dissected into patches with a stride of 14. These patches serve as input tokens and enable the self-attention mechanisms within the transformer block, facilitating the extraction of detailed image embeddings.

\noindent{\textbf{Perceive Sampler.}} The perceive sampler within the InternLM-XComposer operates as an attentive pooling mechanism designed to condense the initial set of 257 image embeddings down to 64 refined embeddings. These optimized embeddings are subsequently aligned to be compatible with the knowledge structures understood by the large language model. Following BLIP2~\cite{Li2023BLIP2BL}, we leverage $\text{BERT}_{base}$~\cite{devlin2018bert} equipped with cross-attention layers, serving as the perceive sampler in our framework.

\noindent{\textbf{Large Language Model.}} The InternLM-XComposer is anchored on InternLM~\cite{2023internlm} as its foundational large language model. Notably, InternLM stands as a potent language model equipped with multilingual capabilities, proficient in both English and Chinese. In our framework, we employ the publicly available InternLM-Chat-7B to serve as the large language model. For comprehensive details about InternLM, we refer readers to its official code repository\footnote{\url{https://github.com/InternLM/InternLM}}.

\subsection{Training}
As shown in Figure~\ref{fig:architecture}, the training process of InternLM-XComposer is split into Stage A and Stage B. Stage A serves as the pre-training phase, utilizing vast amounts of data for foundational model training. 
In contrast, Stage B is the supervised fine-tuning phase, involving a multi-task training step and a following instruction tuning step. 
The model is named InternLM-XComposer-VL after multi-task training and InternLM-XComposer after instruction tuning.

\noindent{\textbf{Pre-training.}} The pre-training phase incorporates large-scale, web-crawled image-text pairs along with interleaved image-text data to pre-train the foundational vision-language model. This data comprises multimodal content in both English and Chinese languages. To preserve the linguistic capabilities of the large language model, the partial textual data utilized for InternLM's pre-training is also employed in the pre-training phase of InternLM-XComposer. As indicated in Table~\ref{tab:pre-training data}, the multimodal pre-training process employs 1.1 billion images alongside 67.7 billion text tokens, including both public datasets and in-house data collected from public websites, possessing over 11 million semantic concepts. This corpus includes 50.6 billion English text tokens and 17.1 billion Chinese text tokens. Furthermore, approximately 10 billion text tokens, sampled from the InternLM pre-training dataset, are incorporated to maintain the model's linguistic proficiencies. Prior to the training process, all pre-training data underwent a thorough cleaning procedure to ensure its quality and reliability.

During the pre-training phase, the visual encoder is held constant, allowing for the optimization to be concentrated on the perceive sampler and the large language model. Initial weight for the perceive sampler and the large language model are sourced from BLIP2~\cite{Li2023BLIP2BL} and InternLM~\cite{2023internlm}, respectively. Given that the large language model lacks native understanding of image embeddings, its optimization within the framework of multimodal pre-training serves to enhance its capability to interpret such embeddings effectively. The training objective for the model centers on next-token prediction, utilizing cross-entropy loss as the loss function. The optimization algorithm employed is AdamW, with hyperparameter settings as follows: $\beta_1$=0.9, $\beta_2$=0.95, $eps$ =1e-8. The maximum learning rates for the perceive sampler and the large language model are configured at 2e-4 and 4e-5, respectively, following a cosine learning rate schedule. The minimum learning rate is set at 1e-5. Additionally, a linear warm-up is applied over the initial 200 steps. The training procedure employs a batch size of approximately 15.7 million tokens and spans 8,000 iterations. Utilizing such a large batch size in conjunction with a limited number of iterations contributes to stable training dynamics while also aiding in the preservation of the inherent capabilities of InternLM.

\noindent{\textbf{Supervised Fine-Tuning.}}
In the pre-training phase, image embeddings are aligned with language representations, equipping the large language model with a rudimentary understanding of image content. However, the model still lacks proficiency in utilizing this image information optimally. To address this limitation, we introduce a variety of vision-language tasks that the model undertakes during the subsequent Supervised Fine-Tuning Stage (SFT), which contains two consecutively steps, \ie, \emph{Multi-task training} and \emph{Instruction tuning}.


\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/interleaved.pdf}
        \put(-440,-1.5){\footnotesize (a) Text Generation}
        \put(-305,-1.5){\footnotesize (b) Image Spotting and Captioning}
        \put(-135,-1.5){\footnotesize (c) Image Retrieval and Selection}
	\caption{\textbf{The pipeline of the interleaved image-Text composition.} (a) Given an input title, the model initially generates a corresponding text-based article. (b) Subsequent to the article generation, the model is trained to identify suitable image locations and generate corresponding captions for the ensuing steps. (c) A text-image retrieval algorithm is initially employed to constrict the pool of candidate images. Following this, our vision-language model is fine-tuned to make the final image selection, ensuring thematic and visual coherence by considering both the preceding textual content and images within the article.}
	\label{fig:interleaved}
 \vspace{5pt}
\end{figure*}


\textbf{\emph{Multi-task training}}. As illustrated in Table~\ref{tab:sft data}, the multi-task training dataset is constructed from multiple sources to endow the model with a diverse range of capabilities, including two splits, \ie, multi-task training and instruction training.
These include scene understanding (\eg, COCO Caption~\cite{chen2015microsoft}, SUB~\cite{Ordonez_2011_im2text}), location understanding (\eg, Visual Spatial Reasoning dataset~\cite{Liu2022VisualSR}), Optical Character Recognition (OCR) (\eg, OCR-VQA~\cite{mishraICDAR19}), and open-ended answering (\eg, VQAv2~\cite{VQAv2}, GQA~\cite{hudson2018gqa}), among others. Each of these tasks is formulated as a conversational interaction, adhering to the following format:
\begin{align*}
& <|User|>: \textit{Instruction} \enspace \texttt{<eou>} \\
& <|Bot|>: \textit{Answer} \enspace \texttt{<eob>}
\end{align*}
where $\texttt{<eou>}$ and $\texttt{<eob>}$ represent the \textit{end-of-user} and \textit{end-of-bot} tokens, respectively. For QVA datasets featuring multiple questions per image, we structure them as multi-round conversations with randomly ordered questions, thereby substantially enhancing the efficiency of the SFT process. During this stage, all questions are introduced through manually crafted prompts to augment task diversity. 


In order to achieve stable and efficient fine-tuning, we retains the weights of the pre-existing large language model in a frozen state. Subsequently, we augment the architecture with Low-Rank Adaption (LoRA)~\cite  {hu2022lora} for the fine-tuning process. The perceive sampler is concurrently trained, albeit with a distinct learning rate. Specifically, LoRA is applied to the \textit{query}, \textit{value} and \textit{key} of the attention layer as well as the feed-forward network. We find that a high LoRA rank is conducive for imbuing the model with new capabilities; consequently, we set the LoRA rank and alpha parameter both to 256. The model is trained using a global batch size of 256 over 10,000 iterations. The learning rates are set to $5e^{-5}$ for the LoRA layer and $2e^{-5}$ for the perceive sampler.

\textbf{\emph{Instruction tuning}}. To further empower aforementioned model's instruction following and interleaved image-text composition capability, as shown in Table~\ref{tab:sft data}, we utilize data from pure-text conversation corpora and LLava-150k for instruction-based tuning, and leverage the LRV dataset to mitigate hallucinations. The interleaved image-text composition dataset is constructed based on the methodology delineated in Section \ref{sec:compos_data}. We maintain a batch size of 256 and execute the tuning over 1000 iterations with a small learning rate $1e^{-5}$.

\begin{table*}[t]
\small
\centering
\setlength{\tabcolsep}{0.6mm}{
\begin{tabular}{lcccccccccccccccc}
\toprule


Model & Overall & Exist. & Count & Pos. & Color & OCR & Poster & Cele. & Scene & Land. & Art. & Comm. & NumCal. & Trans. & Code & Avg. \\
\hline
LLaVA\cite{liu2023visual} & 712.5 & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 & 48.8 & 50.0 & 50.0 & 49.0 & 57.1 & 50.0 & 57.5 & 50.0 & 50.9  \\

MiniGPT-4\cite{zhu2023minigpt} & 694.3 & 68.3 & 55.0 & 43.3 & 43.3 & 57.5 & 41.8 & 54.4 & 71.8 & 54.0 & 60.5 & 59.3 & 45.0 & 0.0 & 40.0  & 49.6 \\

MM-GPT\cite{Gong2023MultiModalGPTAV}  & 871.5& 61.7 & 55.0 & 58.3 & 58.3 & 82.5 & 57.8 & 73.8 & 68.0 & 69.8 & 59.5 & 49.3 & 62.5 & 60.0 & 55.0 &  62.3 \\

VisualGLM\cite{du2022glm} & 880.4 & 85.0 & 50.0 & 48.3 & 48.3 & 42.5 & 66.0 & 53.2 & 146.3 & 83.8 & 75.3 & 39.3 & 45.0 & 50.0 & 47.5 & 62.9  \\

LaVIN\cite{Luo2023CheapAQ} & 1201.6 & 185.0 & 88.3 & 63.3 & 63.3 & 107.5 & 79.6 & 47.4 & 136.8 & 93.5 & 87.3 & 87.1 & 65.0 & 47.5 & 50.0 & 85.8\\

mPLUG-Owl\cite{ye2023mplug} & 1238.4 & 120.0 & 50.0 & 50.0 & 50.0 & 65.0 & 136.1 & 100.3 & 135.5 & 159.3 & 96.3 & 78.6 & 60.0 & 80.0 & 57.5 & 88.5 \\

LLaMA-A.-V2\cite{Gao2023LLaMAAdapterVP} & 1194.9 & 120.0 & 50.0 & 48.3 & 48.3 & \underline{125.0} & 99.7 & 86.2 & 148.5 & 150.3 & 69.8 & 81.4 & 62.5 & 50.0 & 55.0 & 85.4 \\

InstructBLIP\cite{dai2023instructblip} & 1417.9 & 185.0 & 143.3 & 66.7 & 66.7 & 72.5 & 123.8 & 101.2 & 153.0 & 79.8 & 134.3 & 129.3 & 40.0 & 65.0 & 57.5 & 101.3\\

BLIP-2\cite{Li2023BLIP2BL} & 1508.8 & 160.0 & 135.0 & 73.3 & 73.3 & 110.0 & 141.8 & 105.6 & 145.3 & 138.0 & 136.5 & 110.0 & 40.0 & 65.0 & 75.0 & 107.8\\

Lynx\cite{Zeng2023WhatMI} & 1508.9 & \underline{195.0} & \underline{151.7} & 90.0 & 90.0 & 77.5 & 124.8 & 118.2 & \underline{164.5} & \underline{162.0} & 119.5 & 110.7 & 17.5 & 42.5 & 45.0 &  107.8\\

GIT2\cite{Wang2022GITAG} & 1532.2 & \underline{190.0} & 118.3 & \underline{96.7} & 96.7 & 65.0 & 112.6 & 145.9 & 158.5 & 140.5 & \underline{146.3} & 99.3 & 50.0 & 67.5 & 45.0 & 109.4\\

LRV-Instruct\cite{Liu2023MitigatingHI} & 1549.7 & 165.0 & 111.7 & 86.7 & 86.7 & 110.0 & 139.0 & 112.7 & 148.0 & 160.5 & 101.3 &100.7 & 70.0 & 85.0 & 72.5 & 110.7\\

Otter\cite{li2023otter} & 1572.0 & \underline{195.0} & 88.33& 86.7 & 86.7 & 72.5 & 138.8 & \underline{172.7} & \underline{158.8} & 137.3 & 129.0 & 106.4 & \underline{72.5} & 57.5 & 70.0 & 112.3\\

Cheetor\cite{Li2023EmpoweringVM} & 1584.4 & 180.0 & 96.7 & 80.0 & 80.0 & 100.0 & 147.3 & \underline{164.1} & 156.0 & 145.7 & 113.5 &98.6 & \underline{77.5} & 57.5 & \underline{87.5} & 113.2 \\

BLIVA\cite{Hu2023BLIVAAS}  & 1669.2 & 180.0 & 138.3 & 81.7 & \underline{180.0} & 87.5& \underline{155.1} & 140.9 & 151.5 & 89.5 & \underline{133.3}  & \underline{136.4} & 57.5 & 77.5 & 60.0 & 119.2 \\

MMICL\cite{Zhao2023MMICLEV} & 1810.7 &170.0 & \underline{160} & 81.7 & 156.7  & 100 & 146.3 & 141.8 & 153.8 & 136.13 & \underline{135.5} & \underline{136.4} & \underline{82.5}	&\underline{132.5} & \underline{77.5} & 129.3 \\


Qwen-VL-Chat\cite{bai2023qwen} & 1848.3 & 158.3 & 150.0 & \underline{128.3} & \underline{170.0} & \underline{140.0}& \underline{178.6} & 120.6 & 152.3 & \underline{164.0} & 125.5  & 130.7 & 40.0 & \underline{147.5} & 42.5 & 132.0 \\

\midrule

\rowcolor{gray!20} Ours & 1919.5 & \underline{190.0} & \underline{158.3} & \underline{126.7} & \underline{165.0} & \underline{125.0} & \underline{161.9} & \underline{150.3} & \underline{159.8} & \underline{165.3} & 126.3 & \underline{138.6} & 55.0 & \underline{112.5} & \underline{85.0} & \textbf{137.1}  \\

\bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{
\textbf{Evaluation of MME-Benchmark}. Here we report the results on all the sub tasks, including Existence(Exist.), Count, Position(Pos.), Color, OCR, Poster, Celebrity(Cele.), Scene, Landmark(Land.), Artwork(Art.), Commonsense Reasoning(Comm.), Numerical Calculation(NumCal.), Text Translation(Trans.), Code Reasoning(Code) and the task-level average (Avg.). We \textbf{bold} the highest average score and highlight the Top-3 model of each sub task with \underline{underline}. }
\vspace{1.5 pt}
\label{table:mme}
\end{table*}

\subsection{Interleaved Image-Text Composition}
\label{sec:compos_data}
To achieve the objective of crafting interleaved image-text compositions, the initial step involves the generation of a text-centric article. Following this, pertinent images are incorporated at well-suited positions within the textual content, thereby enriching the overall narrative and augmenting reader engagement.

\noindent{\textbf{Text Generation.}} To facilitate the generation of extended text-based articles, we curate a dataset comprising interleaved image-text compositions. It is noteworthy that the acquired dataset contains noise, particularly in the form of marketing and advertising content. To address this, we employ GPT-4 to assess the noise level of each individual paragraph. Any paragraphs that are identified as noisy, along with articles where over 30\% of the content is classified as such, are subsequently filtered out of the dataset.

To enable the model to generate text-based articles with respect to specific titles, we formulate the training data in the following manner:
\begin{align*}
& <|User|>: \textit{Write an illustrated article based on the} \\
& \quad\quad\quad\quad\quad\quad \textit{given title: \{Title\}} \enspace \texttt{<eou>}  \\
& <|Bot|>: [para_1]\ldots[para_N] \enspace \texttt{<eob>}
\end{align*}
Here, $\{Title\}$ serves as a placeholder for the article title, while $[para_1]$ and $[para_N]$ denote the first and last paragraphs, respectively. 

To enhance the visual appeal and engagement level of the generated text-centric articles, the incorporation of contextually appropriate images is essential. In line with this aim, we establish an exhaustive database that functions as a candidate pool for the selection of images. The overall procedure is divided into two main components: image spotting, which identifies opportune locations within the text for image integration, and image selection, aimed at choosing the most contextually suitable images. 

\begin{table*}[!t]
\small
\centering
\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{lcccccccccc}
\toprule
Method & Language Model & Vision Model & Overall & LR & AR & RR & FP-S & FP-C & CP \\
\midrule
MMGPT & LLaMA-7B & CLIP ViT-L/14 & 16.0 & 1.1 & 23.8 & 20.7 & 18.3 & 5.2 & 18.3 \\
MiniGPT-4 & Vincuna-7B & EVA-G & 23.0 & 13.6 & 32.9 & 8.9 & 28.8 & 11.2 & 28.3\\
PandaGPT & Vincuna-13B & ImageBind ViT-H/14 & 30.6 & 15.3 & 41.5 & 22.0 & 20.3 & 20.4 & 47.9 \\
VisualGLM & ChatGLM-6B & EVA-CLIP & 33.5 & 11.4 & 48.8 & 27.7 & 35.8 & 17.6 & 41.5 \\
InstructBLIP & Vincuna-7B & EVA-G & 33.9 & 21.6 & 47.4 & 22.5 & 33.0 & 24.4 & 41.1 \\
LLaVA & LLaMA-7B & CLIP ViT-L/14 & 36.2 & 15.9 & 53.6 & 28.6 & 41.8 & 20.0 & 40.4 \\
LLaMA-Adapter-v2 &  LLaMA-7B & CLIP ViT-L/14 & 38.9 & 7.4 & 45.3 & 19.2 & 45.0 & 32.0 & 54.0 \\
G2PT & LLaMA-7B & ViT-G & 39.8 & 14.8 & 46.7 & 31.5 & 41.8 & 34.4 & 49.8 \\
Otter-I & LLaMA-7B & CLIP ViT-L/14 & 48.3 & 22.2 & 63.3 & 39.4 & 46.8 & 36.4 & 60.6 \\
IDEFICS-80B & LLaMA65B & CLIP ViT-H/14 & 54.6 & 29.0 & 67.8 &46.5 & 56.0 & 48.0 & 61.9 \\
Shikra & Vincuna-7B & CLIP ViT-L/14 & 60.2 & 33.5 & 69.6 & 53.1 & 61.8 & 50.4 & 71.7 \\
Qwen-VL-Chat & Qwen-7B & ViT-G/16 & 61.2 & 38.6 & 70.9 & 46.9 & 67.7 & 47.6 & 71.9 \\
LMEye & Flan-XL & CLIP ViT-L/14 & 61.3 & 36.9 & 73.0 & 55.4 & 60.0 & 58.0 & 68.9 \\
MMICL & FLANT5-XXL & EVA-G & 65.2 & 44.3 & 77.9 & 64.8 & 66.5 & 53.6 & 70.6 \\ 
mPLUG-Owl & LLaMA2-7B & CLIP ViT-L/14 & 68.5 & \textbf{56.8} & 77.9 & 62.0 & 72.0 & 58.4 & 72.6 \\

\midrule 
\rowcolor{gray!20}InternLM-XComposer-VL  & InternLM & EVA-G &  \textbf{74.4} & 50.6 & \textbf{82.0} & \textbf{76.1} & \textbf{79.3} & \textbf{59.2} & \textbf{81.7} \\
\bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{
\textbf{Evaluation of MMBench test set.} Here we report the results on the six L-2 abilities, namely Logical Reasoning (LR), Attribute Reasoning (AR), Relation Reasoning (RR), Fine-grained Perception (Cross Instance) (FP-C), Fine-grained Perception (Single Instance) (FP-S), and Coarse Perception (CP).
}
\vspace{-10pt}
\label{table:mmbench_results}
\end{table*}

A basic strategy for image selection involves summarizing the preceding textual content and retrieving the most closely related image from the available image pool. However, this approach is insufficient for maintaining a coherent thematic flow of images across the article. To remedy this limitation, we suggest the employment of our vision-language foundational model. This model is designed to select a portfolio of images that are not only contextually relevant but also maintain thematic consistency throughout the article.

\begin{table*}[t]
\small
\centering
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{lccccccccccccc}
\toprule
Method & Language Model &  Overall & T-Avg. & Sense.U & Inst.Id & Isnt.At & Inst.Lo & Inst.Co & Spat.R & Inst.It & Vis.R & Text.R \\
\midrule
        OpenFlamingo &  MPT-7B  & 42.7 & 39.4 & 53.2 & 45.3 & 40 & 31.2 & 39.3 & 32.6 & 36.1 & 51.4 & 25.9 \\ 
        Otter &  MPT-7B  & 42.9 & 40.08 & 51.3 & 43.5 & 42.3 & 34.2 & 38.4 & 30.9 & 40.2 & 55.3 & 24.7 \\ 
        IDEFICS-9b-instruct &  LLaMA-7B  & 44.5  & 43.01 & 55.8 & 45.3 & 42.3 & 40.2 & 36.8 & 34.9 & 37.1 & 55.9 & 38.8 \\
        MiniGPT-4 &  Vicuna-7B  & 47.4 & 42.6 & 56.3 & 49.2 & 45.8 & 37.9 & 45.3 & 32.6 & 47.4 & 57.1 & 11.8 \\
        BLIP-2 &  Flan-T5-XL  & 49.7 & 45.7 & 59.1 & 53.9 & 49.2 & 42.3 & 43.2 & 36.7 & 55.7 & 45.6 & 25.9 \\ 
        IDEFICS-80b-instruct &  LLaMA-65B  & 53.2  & 54.4& 64 & 52.6 & 50.8 & 48.3 & 46.1 & 45.5 & 62.9 & 68 & 51.8 \\ 
        Kosmos-2 &  Kosmos 1.3B  & 54.4 & 49.4 & 63.4 & 57.1 & 58.5 & 44 & 41.4 & 37.9 & 55.7 & 60.7 & 25.9 \\ 
        InstructBLIP &  Flan-T5-XL  & 57.8 & 49.3 & 60.3 & 58.5 & 63.4 & 40.6 & \textbf{58.4} & 38.7 & 51.6 & 45.9 & 25.9 \\ 
        InstructBLIP-Vicuna &  Vicuna-7B  & 58.8  & 52.2 & 60.2 & 58.9 & 65.6 & 43.6 & 57.2 & 40.3 & 52.6 & 47.7 & 43.5 \\ 
        Qwen-VL &  Qwen-7B  & 62.3 & 59.6 & 71.2 & 66.4 & 67.7 & 53.5 & 44.8 & 43.8 & 62.9 & 74.9 & 51.2 \\
        Qwen-VL-Chat &  Qwen-7B  & 65.4 & 61.9 & 73.3 & 67.3 & \textbf{69.6} & 57.7 & 52.9 & 48.2 & 59.8 & 74.6 & \textbf{53.5} \\
\rowcolor{gray!20}InternLM-XComposer-VL &  InternLM-7B  & \textbf{66.9} & \textbf{65.2} & \textbf{75.0} & \textbf{71.7} & 67.6 & \textbf{60.8} & 56.2 & \textbf{55.3} & \textbf{74.4} & \textbf{77.0} & 48.5 \\ 
\bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{
\textbf{Evaluation of Seed-Bench test set}. Here we report the results on the image-based sub tasks, including  Scene Understanding(Sense.U), Instance Identity(Inst.Id), Instance Attributes(Inst.At), Instance Localization(Inst.Lo), Instance Counting(Inst.Co), Spatial Relation(Spat.R), Instance Interaction(Inst.It), Visual Reasoning(Vis.R), Text Recognition(Text.R), and both overall accuracy(Overall) and task-level average accuracy(T-Avg.)
}
\vspace{-10pt}
\label{table:seed_results}
\end{table*}

\begin{table*}[!t]
\small
\centering
\setlength{\tabcolsep}{2.8mm}{
\begin{tabular}{lcccccccccc}
\toprule
Method & Language Model & Vision Model & Overall & LR & AR & RR & FP-S & FP-C & CP \\
\midrule
        OpenFlamingo & LLaMA 7B & CLIP ViT-L/14 & 1.7 & 1.7 & 4.5 & 0 & 1.5 & 0.8 & 1.3 \\
        MiniGPT-4 & Vicuna 7B & EVA-G & 11.9 & 11.6 & 19.4 & 5.7 & 14.6 & 6.5 & 10.9 \\ 
        InstructBLIP & Vicuna 7B & EVA-G & 23.9 & 9.2 & 38.5 & 16.6 & 20.9 & 15 & 30.8 \\ 
        mPLUG-Owl & LLaMA2 7B & CLIP ViT-L/14 & 24.9 & 6.9 & 34 & 17.5 & 33.4 & 8.5 & 30.6 \\ 
        VisualGLM & ChatGLM 6B & EVA-CLIP & 25.6 & 5.2 & 42 & 18 & 24.1 & 13 & 34.5 \\ 
        LLaVA & LLaMA 7B & CLIP ViT-L/14 & 36.6 & 15 & 52.4 & 17.1 & 34.4 & 27.5 & 50.3 \\ 
        Qwen-VL-Chat & Qwen-7B & ViT-G/16 & 56.3 & 35.3 & 63.5 & 46 & 63.6 & 43.7 & 64.7 \\ 
\rowcolor{gray!20}InternLM-XComposer-VL & InternLM & EVA-G &72.4 & 44.5 & 79.5 & 83.4 & 71.6 & 56.3 & 82.4 \\

\bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{
\textbf{Evaluation of MMBench-CN test set}. Here we report the results on the six L-2 abilities based on Chinese, namely Logical Reasoning (LR), Attribute Reasoning (AR), Relation Reasoning (RR), Fine-grained Perception (Cross Instance) (FP-C), Fine-grained Perception (Single Instance) (FP-S), and Coarse Perception (CP).
}
\vspace{-15pt}
\label{table:mmbench_cn_results}
\end{table*}

In order to enhance computational efficiency, we initially employ a retrieval mechanism to reduce the size of the candidate image pool. Subsequent to this, our vision-language model is deployed to perform the final image selection from this narrowed set of candidates. Consequently, the overarching task is decomposed into image spotting and captioning, along with image retrieval and selection.

\noindent{\textbf{Image Spotting and Captioning.}} 
Leveraging the acquired interleaved image-text compositions, pinpointing image locations becomes a straightforward task. For subsequent image retrieval, it's imperative to generate an appropriate caption, enabling the application of various text-image retrieval algorithms. A straightforward approach involves employing a large language model to summarize preceding content as a caption. Nonetheless, due to limitations in the model's capacity (e.g., 7B), captions generated by the pre-trained language model often misses the central theme or concept of the article. 


To mitigate this challenge, we suggest a supervised fine-tuning approach, utilizing caption data generated via GPT-4. For the creation of this data, GPT-4 is provided with the text-based article and image location, and is instructed to generate a caption that remains coherence with the article's overarching theme and concept, specifically for image retrieval purposes. Upon data generation, the training data is structured as follows:
\begin{align*}
& <|User|>: [seg_1][para_1]\ldots[seg_N][para_N] \enspace \textit{Based}\\
& \quad\quad\quad\quad\quad\quad \textit{on the above text, select the appropriate} \\
& \quad\quad\quad\quad\quad\quad \textit{locations for inserting images and give} \\
& \quad\quad\quad\quad\quad\quad \textit{the image captions} \enspace \texttt{<eou>}  \\
& <|Bot|>: \textit{I think images can be added under the \{$x_1\},$}\\
& \quad\quad\quad\quad\quad \textit{$\ldots, \{x_k\}$ paragraphs. The image captions are} \\
& \quad\quad\quad\quad\quad \textit{as follows: \{$x_1$\} paragraph: \{$cap_1$\}, $\ldots$}, \\
& \quad\quad\quad\quad\quad \textit{ \{$x_k$\} paragraph: \{$cap_k$\}} \enspace \texttt{<eob>}
\end{align*}
Here, \( [seg_1] \) serves as an index token to pinpoint the specific paragraph index. The placeholders \( \{x_1\} \) and \( \{x_k\} \) represent the positions for the first and last image locations, respectively. Correspondingly, \( \{cap_1\} \) and \( \{cap_k\} \) act as the generated captions associated with those image locations.


\begin{table*}[!t]
\small
\centering
\setlength{\tabcolsep}{2.3mm}{
\begin{tabular}{lcccccccccc}
\toprule
Method & Language Model & Vision Model & Overall & CP & CR & F\&C & HF & S\&B & SR & TS \\
\midrule
        OpenFlamingo & LLaMA 7B & CLIP ViT-L/14 & 0.7 & 1.8 & 0 & 0.8 & 0 & 0 & 2.2 & 1.5 \\
        MiniGPT-4 & Vicuna 7B & EVA-G & 1.7 & 7 & 4 & 0 & 0 & 1 & 0 & 0 \\
        LLaVA & LLaMA 7B & CLIP ViT-L/14 & 8.3 & 10.5 & 8.1 & 7.6 & 1.7 & 8 & 11.1 & 10.6 \\ 
        VisualGLM & ChatGLM 6B & EVA-CLIP & 9.2 & 14 & 11.1 & 8.4 & 0 & 14 & 4.4 & 7.6 \\
        InstructBLIP & Vicuna 7B & EVA-G & 12.1 & 8.8 & 9.1 & 21 & 0 & 12 & 6.7 & 18.2 \\ 
        mPLUG-Owl & LLaMA2 7B & CLIP ViT-L/14 & 12.9 & 22.8 & 17.2 & 6.7 & 0 & 25 & 4.4 & 7.6 \\ 
        Qwen-VL-Chat & Qwen-7B & ViT-G/16 & 39.3 & 40.4 & 33.3 & 31.9 & 3.4 & 67 & 51.1 & 42.4 \\ 
\rowcolor{gray!20}InternLM-XComposer-VL & InternLM-7B & EVA-G & 47.6 & 50.9 & 53.5 & 42 & 10.3 & 55 & 73.3 & 50 \\ 
\bottomrule
\end{tabular}}
\caption{
\textbf{Evaluation of CCBench test set}. Here we report all the sub-tasks, including Calligraphy Painting(CP), Cultural Relic(CR), Food \& Clothes(F\&C), Historical Figures(H\&F), Scenery \& Building(S\&B), Sketch Reasoning(SR), Traditional Show(TS), 
}
\label{table:chinese_bench_results}
\end{table*}

\noindent{\textbf{Image Retrieval and Selection.}}
Having obtained the captions, a variety of text-image retrieval methods become available for use. In this work, we opt for the CLIP model, capitalizing on its proven efficacy in zero-shot classification tasks. We compute the similarity scores between the generated caption and each image in the candidate pool. The top \( m \) images, based on these similarity scores, are then selected to constitute the reduced candidate pool for further processing.

To guarantee thematic or conceptual coherence in the images dispersed throughout the article, we deploy our vision-language model to execute the final image selection. When selecting images to accompany the \( j^{th} \) paragraph, the training data is structured in the following manner:
\begin{align*}
& <|User|>: [para_1]\ldots[para_i][img_i][para_{i+1}]\ldots[para_j] \\
& \quad\quad\quad\quad\quad\quad \textit{Based on the given context and candidate} \\
& \quad\quad\quad\quad\quad\quad \textit{images, select the appropriate image. Candidate} \\
& \quad\quad\quad\quad\quad\quad \textit{images include:} [img_j^1]\ldots[img_j^m]\enspace \texttt{<eou>}  \\
& <|Bot|>: \textit{The \{selected index\} image.} \enspace \texttt{<eob>}
\end{align*}
In this configuration, \( [img_i] \) denotes the image associated with the \( i^{th} \) paragraph (preceding the \( j^{th} \) paragraph). The terms \( [img_j^1], \ldots, [img_j^m] \) represent the images present in the reduced candidate pool. Meanwhile, \( \{\textit{selected index}\} \) acts as a placeholder indicating the index of the final selected image.

The vision-language model selects images by considering both preceding text and prior images within the article. This mechanism enables the model to acquire an understanding of thematic and visual coherence, an expertise derived from the curated dataset of interleaved image-text compositions. \section{Experiments}

\subsection{English-Based Benchmark results.}

In this section, we validate the performance of our InternLM-XComposer-VL on several benchmarks. 

\noindent\textbf{MME Benchmark} measures the perception and cognition capability of multi-modality LLMs with carefully crafted questions within 14 sub-tasks. As shown in Table.\ref{table:mme}, our InternLM-XComposer-VL reached a new state-of-the-art performance $137.11\%$, outperforms the previous method QWen-VL-Chat with more than $5.0\%$. We also highlight the Top-3 models within each subtask with \underline{underline} and we notice that our model reaches the Top-3 performance with 12 of the 14 sub-tasks. This proves the outstanding generalize of our model.

\noindent\textbf{MMBench} is a hand-crafted challenging benchmark, which evaluates the vision-related reasoning and perception capability with multi-choice questions. The MME Bench provides both a dev-set and test-set.
Here we report the test-set performance of our model. As shown in Table.\ref{table:mmbench_results}. Our method gets $74.4\%$ accuracy and outperforms previous methods by a large margin. Further, our InternLM-XComposer-VL reaches the best performance in 5 of the 6 dimensions. This proves that our model understands the image information well and can handle diverse vision-related tasks.



\noindent\textbf{Seed-Bench} is a large-scale multi-modality benchmark, which is built with the help of GPT-4 and contains nearly 19K multi-choice questions for both image and video. Here we report the image-set results in Table.\ref{table:seed_results}. It can be observed that our InternLM-XComposer-VL gets the best overall performance and the highest performance in 6 of the 9 sub-tasks. We also notice that the sub-task data number is in-balance, for example, the `\textit{Instance Attributes}' task have 4649 questions, while the `\textit{Text Understanding}' task only has 84 questions. So the overall metric would be biased toward the tasks that have more questions. To better evaluate the generalized capability of the LLMs along different tasks. We also report the task-level average, similar to the MME benchmark. It can be observed that our model reaches the state-of-the-art average accuracy and outperforms the previous method with $3.3\%$. This further proves the general capability of our model.



\subsection{Chinese-Based Benchmark results.}
As we introduced in Sce.\ref{sec:intro}, our model is pretrained with rich multilingual knowledge. To prove the effectiveness of the pretraining, here we further evaluate its performance with two Chinese-based benchmarks.

\noindent\textbf{MMBench-CN} is the Chinese translated benchmark of the original MMbench, which shows the vision-related Chinese understanding and reasoning capability. 
Here we report the test-set performance in Table.\ref{table:mmbench_cn_results}. It can be observed that our method outperforms previous methods by a large margin. When comparing with the English version performance in Table.\ref{table:mmbench_results}. Qwen and VisualGLM have $4.9\%$ and  $7.9\%$ performance degrading, while the performance gap of our model between different languages is only $2.0\%$. This proves the strong multi-lingo capability of our model.

\noindent\textbf{Chinese-Bench} is a Chinese knowledge-related benchmark, that challenges the model with Chinese traditional cultural questions, including art, food, clothes, landmarks, \etc.  
Here we report the performance in Table.\ref{table:chinese_bench_results}. It can be observed that the benchmark is quite challenging, most LLaMA-based model fails to answer these questions, due to the lack of corresponding knowledge. Compared with LLaMA-based methods, the QWen-based model Qwen-VL-Chat shows a much better performance of $39.3\%$. While it is still worse than our InternLM-based model InternLM-XComposer-VL, which reaches a new state-of-the-art performance of $47.6\%$. This proves the rich Chinese knowledge of IntenrLM and the great alignment between the vision and language knowledge by our large-scale pre-training.

\subsection{Interleaved Image-Text Composition}



\noindent\textbf{Qualitative results}. We direct readers to the supplementary materials for detailed qualitative results of the interleaved image-text compositions generated by the InternLM-XComposer and the demonstration of multimodal conversations.


\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/articles1.pdf}
	\caption{\textbf{Interleaved Image-Text Composition.} }
	\label{fig:article1}
\end{figure*}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/articles2.pdf}
	\caption{\textbf{Interleaved Image-Text Composition.} }
	\label{fig:article2}
\end{figure*}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/articles3.pdf}
	\caption{\textbf{Interleaved Image-Text Composition.} }
	\label{fig:article2}
\end{figure*}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/articles4.pdf}
	\caption{\textbf{Interleaved Image-Text Composition.} }
	\label{fig:article2}
\end{figure*}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/conv2.pdf}
	\caption{\textbf{conversation.} }
	\label{fig:conv2}
\end{figure*}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/conv1.pdf}
	\caption{\textbf{conversation.} }
	\label{fig:conv1}
\end{figure*} \section{Conclusion and Outlook}
In this paper, we present InternLM-XComposer, a vision-language large model with superb multi-modality understanding and composition capability. Benefiting from the rich multi-lingual and multi-modality knowledge from our carefully designed pretraining, on one hand, our model could generate coherent and contextual articles with a simple title input, and integrate images at the proper location and content. On the other hand, our model shows state-of-the-art performance across various mainstream vision-language LLM benchmarks. We hope our InternLM-XComposer could provide new insight for the following exploration of advanced vision-language interaction. 




{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
