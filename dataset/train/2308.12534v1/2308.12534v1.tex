\subsection{Implementation details}
We implement the proposed CSRPNet with the PyTorch toolkit on a computer equipped with the NVIDIA GTX 2080Ti GPU. We use a two-stage learning strategy for training. In the first stage, we only optimize the learnable parameters in the encoder, the main path of the DCFR module, and the semantic classifier. In the second stage, we freeze the parameters optimized in the first stage and optimize the learnable parameters in the auxiliary path of the DCFR module and the boundary classifier.
In particular, we use the Stochastic Gradient Descent (SGD) method with a momentum of 0.9 and a weight decay of 0.0005 as the optimizer in both two training stages. The initial learning rates in both two training stages are set to 0.01 and decrease in an exponential manner with a decay factor of 0.95. The balance weights  and  are tuned to be both 1. In addition, the data augmentation methods including random flipping and random cropping are used to alleviate the problem of insufficient training data.

\begin{table}[t]
\centering
\rowcolors{2}{white}{gray!25}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.5}
\caption{\textbf{Experimental results of the six variants of our method on the MFNet dataset.} Herein dot product is used as the relation function. The best and second-best results are denoted by the bold font and the underline, respectively.}
\resizebox{0.65\linewidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
 \multirow{2}{*}{Variants}   &  \multicolumn{2}{c}{Overall} && \multicolumn{2}{c}{Daytime} && \multicolumn{2}{c}{Nighttime}  \\
    \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} 
 ~  & mAcc & mIoU && mAcc & mIoU && mAcc & mIoU\\
\midrule
(a) Baseline & 59.1 & 48.4 && 55.8 & 43.4 && 56.0 & 48.5 \\
(b) +SRP  & 65.6 & 53.1 && 62.8 & 46.4 && 61.6 & 53.4 \\
(c) +CRP  & 67.6 & 53.3 && 61.2 & 45.6 && \underline{64.8} & 54.7 \\
(d) +CSRP    & \underline{67.7} & 54.1 && \underline{64.2} & 47.3 && 62.9 & 54.0 \\
(e) +CSRP+DCFR (Ours) & \textbf{70.4} & \textbf{55.5} && \textbf{65.5} & \textbf{48.0} && \textbf{66.0} & \textbf{55.6} \\
(f) Ours w/o Boundary & \underline{67.7} & \underline{55.2} && 61.8 & \underline{47.9} && 63.5 & \underline{55.2}\\
\bottomrule
\end{tabular}}
\label{Table: ablation}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{Figures/Experiments/Ablation_visualization.pdf} 
\caption{\textbf{Visual comparisons between different variants.} We use the red boxes to denote the parts where the segmentation results are different between different variants. The comparison results show that our intact model with both the CSRP and DCFR modules achieves the best segmentation performance.}
\label{Fig: ablation_visualization}
\end{figure}

\subsection{Benchmarks and metrics}
We evaluate CSRPNet on two popular benchmarks, including MFNet~\cite{MFNet} and PST900~\cite{PSTNet}. The MFNet dataset contains nine semantic classes, including \emph{Car}, \emph{Person}, \emph{Bike}, \emph{Curve}, \emph{Car stop}, \emph{Guardrail}, \emph{Color cone}, and \emph{Bump}. It consists of 1569 annotated RGB and thermal image pairs in total, in which 820 and 749 image pairs are taken in the daytime and nighttime, respectively. 
All images are resized to . For a fair comparison, we use the same training and evaluation configurations as those in MFNet~\cite{MFNet}. In particular, we follow MFNet~\cite{MFNet} to split the data into the training, validation, and testing sets, which contain 784, 392, and 393 image pairs, respectively. The PST900~\cite{PSTNet} dataset is composed of 894 aligned RGB and thermal image pairs and contains four visible artifacts, including \emph{Fire-Extinguisher}, \emph{Backpack}, \emph{Hand-Drill}, and \emph{Survivor}.
Following MFNet~\cite{MFNet}, we use the widely used mean accuracy (mAcc) and mean intersection over union (mIoU) to quantitatively evaluate segmentation performance. The mean accuracy metric is also known as recall. Formally, these two metrics are defined as:

Herein  is the number of semantic classes.  is the number of RGB-T image pairs.  is the number of pixels that belong to class  and are correctly classified as class  in the -th RGB-T image pair.  is the number of pixels that belong to class  but are incorrectly classified as class  in the -th RGB-T image pair.  is the number of pixels that belong to class  but are incorrectly classified as class  in the -the RGB-T image pair. 

\begin{table}[t]
\centering
\rowcolors{2}{white}{gray!25}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.5}
\caption{\textbf{Experimental results of deploying the proposed CSRP module in different layers on the MFNet dataset.} The notation  represents that we deploy the proposed CSRP module in the corresponding layer.}
\resizebox{0.65\linewidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule
\multirow{2}{*}{Layer2} & \multirow{2}{*}{Layer3} & \multirow{2}{*}{Layer4} & \multicolumn{2}{c}{Overall} && \multicolumn{2}{c}{Daytime} && \multicolumn{2}{c}{Nighttime}\\
\cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-11}
 &&& mAcc & mIoU && mAcc & mIoU && mAcc & mIoU\\
\midrule
~ & ~  & ~ & 59.1 & 48.4 && 55.8 & 43.4 && 56.0 & 48.5 \\
\checkmark & ~  & ~ & 60.1 & 50.5 && 58.1 & 45.0 && 56.7 & 50.2 \\
~ & \checkmark & ~ & 62.9 & 53.1 && 59.5 & \underline{46.9} && 58.5 & 51.9 \\
~ & ~ & \checkmark & 65.2 & 53.1 && 60.9 & \underline{46.9} && 60.8 & 52.5\\
\checkmark & \checkmark & ~ & 63.8 & 51.7 && 60.3 & 45.7 && 59.7 & 51.6\\
\checkmark & ~ & \checkmark & 62.6 & 50.6 && 58.4 & 44.1 && 59.3 & 51.6\\
~ & \checkmark & \checkmark & \underline{65.8} & \textbf{54.3} && \underline{61.5} & \underline{46.9} && \underline{62.3} & \textbf{54.6}\\
\checkmark & \checkmark & \checkmark & \textbf{67.7} & \underline{54.1} && \textbf{64.2} & \textbf{47.3} && \textbf{62.9} & \underline{54.0}\\
\bottomrule
\end{tabular}}
\label{Table: ablation1}
\end{table}
\subsection{Ablation study}
We first conduct experiments to investigate the effect of each proposed component and the effect of using features from different layers. Note that we use the dot product as the relation function in this section.


\begin{table}[t]
\centering
\rowcolors{2}{gray!25}{white}
\setlength{\tabcolsep}{1.9pt}
\renewcommand{\arraystretch}{1.5}
\caption{\textbf{Experimental results of different algorithms on MFNet dataset.} `3c' and `4c' denote that the corresponding model is tested with the three-channel RGB data and the four-channel RGB-T data, respectively. Note that mAcc and mIoU are calculated with the unlabeled classes while the results for the unlabeled classes are not displayed.}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lc ccccccccccccccccccccccccc cc}
\toprule
\multirow{2}{*}{Methods} & \multirow{2}{*}{Backbone} && \multicolumn{2}{c}{Car} && \multicolumn{2}{c}{Person} && \multicolumn{2}{c}{Bike} && \multicolumn{2}{c}{Curve} && \multicolumn{2}{c}{Car stop} && \multicolumn{2}{c}{Guardrail} && \multicolumn{2}{c}{Color cone} && \multicolumn{2}{c}{Bump} &&  \multirow{2}{*}{mAcc} & \multirow{2}{*}{mIoU}\\
\cmidrule{4-5}  \cmidrule{7-8}  \cmidrule{10-11}  \cmidrule{13-14}  \cmidrule{16-17}  \cmidrule{19-20}  \cmidrule{22-23} \cmidrule{25-26}
~ & ~ &&Acc & IoU && Acc & IoU && Acc & IoU && Acc & IoU && Acc & IoU && Acc & IoU && Acc & IoU && Acc & IoU  && ~ & ~  \\
\midrule
FRRN (3c) & ResNet && 80.0 & 71.2 && 53.0 & 46.1 && 65.1 & 53.0 && 34.0 & 27.1 && 21.6 & 19.1 && 0.0 & 0.0 && 34.7 & 32.5 && 36.2 & 30.5 && 47.1 & 41.8\\
FRRN (4c) & ResNet && 81.9 & 74.7 && 66.2 & 60.8 && 62.8 & 50.3 && 41.2 & 35.0
&& 12.5 & 11.5 && 0.0 & 0.0 && 37.2 & 34.0 && 35.2 & 34.6 && 48.5 & 44.2\\
DFN (3c) & ResNet && 90.7 & 81.4 && 67.7 & 52.8 && 71.5 & 57.5 && 49.2 & 34.9
&& 35.1 & 23.8 && 4.1 & 1.4 && 44.2 & 31.0 && 54.6 & 47.5 && 57.3 & 47.5\\
DFN (4c) & ResNet && 90.0 & 84.4 && 73.2 & 65.0 && 75.5 & 60.9 && 54.0 & 40.4
&& 38.9 & 25.7 && 10.2 & 2.7 && 48.3 & 42.5 && 55.8 & 47.4 && 60.5 & 52.0\\
BiSeNet (3c) & U-Net && 90.0 & 84.5 && 65.0 & 54.3 && 75.0 & 61.4 && 32.1 & 25.7 && 32.3 & 26.2 && 3.2 & 0.9 && 49.6 & 43.3 && 48.1 & 40.5 && 54.9 & 48.2\\
BiSeNet (4c) & U-Net && 89.7 & 84.1 && 72.0 & 63.2 && 74.1 & 60.1 && 45.1 & 36.7 && 34.2 & 25.3 && 18.2 & 5.0 && 47.4 & 42.2 && 39.8 & 35.9 && 57.7 & 50.0\\
MFNet & CNN && 77.2 & 65.9 && 67.0 & 58.9 && 53.9 & 42.9 && 36.2 & 29.9
&& 12.5 & 9.9 && 0.1 & 0.0 && 30.3 & 25.2 && 30.0 & 27.7 && 45.1 & 39.7\\
FuseNet & VGG && 81.0 & 75.6 && 75.2 & 66.3 && 64.5 & 51.9 && 51.0 & 37.8
&& 28.7 & 15.0 && 0.0 & 0.0 && 31.1 & 21.4 && 51.9 & 45.0 && 52.4 & 45.6\\
DepthAwareCNN & CNN && 85.2 & 77.0 && 61.7 & 53.4 && 76.0 & 56.5 && 40.2
& 30.9 && 9.9 & 29.3 && 22.8 & 6.4 && 32.9 & 30.1 && 36.5 & 32.3 && 55.1 & 46.1\\
RTFNet-50 & ResNet && 91.3 & 86.3 && 78.2 & 67.8 && 71.5 & 58.2 && \underline{69.8}
& 43.7 && 32.1 & 24.3 && 13.4 & 3.6 && 40.4 & 26.0 && 73.5 & \textbf{57.2} && 62.2
& 51.7\\
RTFNet-152 & ResNet && 93.0 & 87.4 && 79.3 & 70.3 && 76.8 & \underline{62.7} && 60.7
& 45.3 && 38.5 & 29.8 && 0.0 & 0.0 && 45.5 & 29.1 && \underline{74.7} & 55.7 && 63.1
& 53.2\\
MMNet & ResNet && -- & 83.9 && -- & 69.3 && -- & 59.0 && -- & 43.2
&& -- & 24.7 && -- & 4.6 && -- & 42.2 && -- & 50.7 && 62.7 & 52.8\\
FuseSeg-161 & DenseNet && 93.1 & \textbf{87.9} && 81.4 & 71.7 && 78.5 & \textbf{64.6}
&& 68.4 & 44.8 && 29.1 & 22.7 && \underline{63.7} & 6.4 && 55.8 & 46.9 && 66.4 & 47.9
&& 70.6 & 54.5\\
ABMDRNet & ResNet && \underline{94.3} & 84.8 && \textbf{90.0} & 69.6 && 75.7 & 60.3 && 64.0
& 45.1 && 44.1 & 33.1 && 31.0 & 5.1 && 61.7 & 47.4 && 66.2 & 50.0 && 69.5 & 54.8\\
EGFNet & ResNet && \textbf{95.8} & 87.6 && 89.0 & 69.8 && \textbf{80.6} & 58.8 && \textbf{71.5} & 42.8
&& \underline{48.7} &  \underline{33.8} && 33.6 & 7.0 && \underline{65.3} &48.3 && 71.1 & 47.1 && \underline{72.7} & 54.8\\
GCNet & ResNet && 94.2 & 86.0 && \underline{89.6} & 72.0 && 77.5 & 60.0 && 68.9 & 42.8
&& 38.3 & 30.7 && 45.8 & 6.2 && 59.6 & \underline{49.5} && \textbf{82.1} & 52.6 && \underline{72.7} & 55.3\\
FEANet & ResNet && 93.3 & \underline{87.8} && 82.7 & 71.1 && 76.7 & 61.1 && 65.5 & 46.5
&& 26.6 & 22.1 && \textbf{70.8} & 6.6 && \textbf{66.6} & \textbf{55.3} && 77.3 & 48.9 && \textbf{73.2} & 55.3\\
GCGLNet & MobileV2 && 92.4 & 83.8 && 85.7 & \textbf{72.4} && 73.7 &59.6 && 68.1 & 40.9
&& \textbf{54.9} & \textbf{43.0} && 52.3 & \textbf{8.7} && 48.8 & 43.8 && 73.2 & 48.6 && 72.0 & 55.4\\
MMDRNet & ResNet && 93.4 & 85.7 && 89.3 & 70.3 && 74.7 & 61.5 && 65.7 & \underline{46.9}
&& 42.7 & 32.7 && 53.9 & 7.7 && 59.9 & 48.2 && 73.0 & 53.4 && 72.4 & \textbf{56.0}\\
\textbf{CSRPNet} (Dot product) & ResNet && 93.7 & 87.6 && 86.5 & \underline{72.3} && 78.1 & 60.7 && \underline{69.8} & 45.0
&& 40.2 & 29.8 && 36.1 & 6.4 && 58.5 & 46.3 && 71.5 & 53.2 && 70.4 & \underline{55.5}\\
\textbf{CSRPNet} (Gaussian) & ResNet && 93.5 & \underline{87.8} && 87.8 & \underline{72.3} && \underline{78.9} & 61.4 && 68.5 & \textbf{47.6} && 38.9 & 28.1 && 58.9 & \underline{8.0} && 57.3 & 43.7 && 71.2 & \underline{57.1} && \underline{72.7} & \textbf{56.0}\\
\bottomrule
\end{tabular}}
\label{Table: compare}
\end{table}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{table}
\centering
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.5}
\caption{\textbf{Experimental results on the MFNet dataset for the daytime and nighttime images.}}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lc cccccccccccccc cc}
\toprule
\multicolumn{2}{l}{Methods} & \tabincell{c}{FRRN\4c)} & \tabincell{c}{BiSeNet
\4c)} & \tabincell{c}{DFN\4c)} & \tabincell{c}{Fuse\\Net}&\tabincell{c}{ABMDR\\Net}&\tabincell{c}{Fuse\\Seg-161}&\tabincell{c}{Depth\\AwareCNN}& \tabincell{c}{MFNet} & \tabincell{c}{RTFNet\\-50} & \tabincell{c}{RTFNet\\-152} & \tabincell{c}{\textbf{CSRPNet}\Gaussian)} \\
\midrule
\multirow{2}{*}{Daytime} & mAcc & 45.1 & 42.4 & 52.1 & 52.9 & 53.7 & 53.4 & 49.5 &58.4&62.1& 50.6& 42.6 & 57.3 & 60.0 & \textbf{65.5} & \underline{64.0} \\
& mIoU & 40.0 & 38.0 & 44.5 & 44.8 & 42.2 & 43.9 & 41.0&46.7&47.8& 42.4& 36.1 & 44.4 & 45.8 & \underline{48.0} & \textbf{48.7}  \\
\midrule
\multirow{2}{*}{Nighttime} & mAcc & 41.6 & 46.2 & 50.3 & 53.1 & 52.4 & 57.4 & 48.9&\underline{68.3}&67.3& 50.7& 41.4 & 59.4 & 60.7 & 66.0 & \textbf{69.4} \\
& mIoU & 37.3  & 42.3 & 45.0 & 47.7 & 44.6 & 51.8 & 43.9&55.5&54.6& 43.2& 36.8 & 52.0 & 54.8 & \underline{55.6} & \textbf{56.0}  \\
\bottomrule
\end{tabular}}
\label{Table: day_night}
\end{table}

\begin{table}[t]
\centering
\rowcolors{2}{white}{gray!25}
\renewcommand{\arraystretch}{1.5}
\caption{\textbf{Experimental results of different algorithms on PST900.}}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccccccccccccccccccc}
\toprule
\multirow{2}{*}{Methods} &\multirow{2}{*}{Backbone} & \multicolumn{2}{c}{Background} && \multicolumn{2}{c}{Hand-Drill} && \multicolumn{2}{c}{Backpack} && \multicolumn{2}{c}{Fire-extinguisher} && \multicolumn{2}{c}{Survior} &&  \multirow{2}{*}{mAcc} & \multirow{2}{*}{mIoU}\\
\cmidrule{3-4}\cmidrule{6-7}\cmidrule{9-10}\cmidrule{12-13}\cmidrule{15-16}
 & &Acc & IoU && Acc & IoU && Acc & IoU && Acc & IoU && Acc & IoU &&  &   \\
\midrule
Efficient FCN (3c)&VGG-16& \underline{99.8} & 98.6 &&32.1 & 30.2 && 60.1 & 58.2&& 78.9 & 40.0 && 32.8 & 28.0 && 60.7 & 51.0\\
Efficient FCN (4c)&VGG-16& \underline{99.8} & 98.9 &&48.8 & 38.6 && 69.9 & 67.6&& 76.5 & 46.3 && 38.9 & 35.1 && 66.8 & 57.3\\
MFNet&DCNN & \textbf{99.9} & 98.7 &&46.7 & 39.3 && 52.8 & 52.4&& 71.8 & 67.4 && 18.8 & 18.9 && 58.0 & 55.3\\
RTFNet&ResNet-152 & \underline{99.8} & 99.0 &&7.8 & 7.1 && 80.0 & 74.2&& 62.4 & 51.9 && \textbf{78.5} & \textbf{70.1} && 65.7 & 60.5\\
CCNet (3c)&ResNet-50 & \textbf{99.9} & 99.1 &&51.8 & 32.3 && 68.3 & 66.4&& 67.8 & 51.8 && 60.8 & 57.5 && 69.7 & 61.4\\
CCNet (4c)&ResNet-50 & 99.6 & 98.7 &&54.1 & 51.0 && 76.0 & 73.0&& \textbf{88.1} & \textbf{73.8} && 49.5 & 33.5 && 73.4 & 66.0\\
PSTNet&ResNet-18 & -- & 98.9 &&-- &53.6 && -- & 69.2&& -- & \underline{70.1} && -- & 50.0 && -- & 68.4\\
ACNet&ResNet-50 & \underline{99.8} & \textbf{99.3} &&53.6 & 51.7 && 85.6 & \textbf{83.2}&& 84.9 & 60.0 &&\underline{ 69.1} & \underline{65.2} && 78.7 & 71.8\\
\textbf{CSRPNet} (Dot product)&ResNet-50 & \underline{99.8} & \textbf{99.3} && \underline{73.7} & \underline{63.2} && \textbf{87.3} & \underline{82.6} &&\underline{86.3} & 67.7 && 60.0 & 56.6 && \textbf{81.4} & \underline{73.9}\\
\textbf{CSRPNet} (Gaussian)&ResNet-50 & \underline{99.8} & \underline{99.2} && \textbf{86.1} & \textbf{71.6} && \underline{86.8} & 82.0 &&77.7 & 67.9 && 55.2 & 54.1 && \underline{81.1} & \textbf{75.0}\\
\bottomrule
\end{tabular}}
\label{Table: compare}
\end{table}

\subsubsection{Effect of each component}
To analyze the effect of each component, we evaluate six variants of our algorithm: (a) the baseline (BL) model without the proposed CSRP and DCFR modules, which directly fuses the features of the RGB and thermal images from the last layer via element-wise summation and uses the decoder in MFNet~\cite{MFNet} to predict the semantic map; (b) the variant that exploits the SRP block for multi-modality feature fusion based on the baseline model; (c) the variant that uses the CRP block for feature fusion based on the baseline model; (d) the variant that uses the proposed CSRP module for feature fusion based on the baseline model; (e) our intact model including the proposed CSRP and DCFR modules; (f) the variant that removes the boundary prediction branch and the boundary supervision from our intact model. Table~\ref{Table: ablation} reports the overall, daytime, and nighttime performance of these variants on the MFNet datasets.

Compared with the baseline model, the variants using the SRP and CRP blocks achieve performance gains of 6.5\%/8.5\% and 4.7\%/4.9\% in overall mAcc and mIoU, respectively. These performance gains demonstrate the effectiveness of spatial or channel relation-propagation. Compared with the variants using the SRP block or the CRP block alone, exploiting them together (the variant using the CSRP module) further promotes segmentation performance, which manifests that the relation-propagation in space and channel is complementary. Furthermore, the performance gap in overall mAcc between the variant using the CSRP module and our intact model validates the effectiveness of the DCFR module. Besides, removing the boundary prediction branch and the boundary supervision leads to performance drops, demonstrating their effectiveness. The comparisons between these variants in terms of daytime and nighttime performance also demonstrate the effectiveness of each proposed component.

We also visualize the segmentation results of different variants to analyze the effect of the CSRP and DCFR modules, as shown in Figure~\ref{Fig: ablation_visualization}. Compared with the baseline model, the variant using the CSRP module predicts more accurate segmentation masks. The comparison between the last two rows shows that the DCFR module enables our method to predict more refined masks, especially for small objects. Removing the boundary prediction branch and boundary supervision leads to a less precise segmentation.

\subsubsection{Effect of using features from different layers} We also conduct experiments to analyze the effect of deploying the CSRP module in different layers of the encoder based on the above-mentioned variant (d).
Table~\ref{Table: ablation1} reports the detailed experimental results. Considering that calculating the relation matrix of the feature map of the first layer with a high resolution is memory-consuming and the low-level features lack abstract semantic information, we omit the CSRP module in the first layer of the encoder. From the first to the fourth rows in Table~\ref{Table: ablation1}, we can observe that deploying the proposed CSRP module in the deeper layer leads to better performance, which demonstrates that the feature from the deeper layer models more abstract semantics to extract the modality-shared information. In addition, we can observe that deploying the proposed CSRP module in \emph{layer2}, \emph{layer3}, and \emph{layer4} leads to the best segmentation performance. 

\subsection{Comparison with state-of-the-art methods}
In this section, we evaluate our method on the MFNet~\cite{MFNet} and PST900~\cite{PSTNet} datasets. We detail the comparison results per dataset in the following.

\vspace{1mm}
\noindent\textbf{The MFNet dataset.}
We compare our proposed CSRPNet with 15 state-of-the-art methods, including FRRN~\cite{FRRN}, BiSeNet~\cite{bisenet}, DFN~\cite{DFN}, FuseNet~\cite{Fusenet}, DepthAwareCNN~\cite{DepthAwareCNN}, MFNet~\cite{MFNet}, RTFNet~\cite{RTFNet}, FuseSeg-161~\cite{FuseSeg}, ABMDRNet~\cite{ABMDRNet}, EGFNet~\cite{EGFNet}, GCNet~\cite{GCNet}, MMDRNet~\cite{MMDRNet}, GCGLNet~\cite{GCGLNet}, FEANet~\cite{FEANet}, MMNet~\cite{MMNet}, on the MFNet dataset. 
Table~\ref{Table: compare} reports the experimental results. For a fair comparison, we test two versions of several RGB segmentation methods. One is the `3c' version taking as input the 3-channel RGB data for testing, and the other is the `4c' version taking as input the 4-channel RGB-T data for testing. Herein we display the results of our approaches with two different relation functions, including dot product and Gaussian function. The one with the Gaussian function performs marginally better than the one using the dot product. We attribute it to the Gaussian function performing relation modeling in a high-dimension space.
Our methods perform comparably against state-of-the-art methods including MMDRNet, GCGLNet, and FEANet. Besides, compared with the recently proposed EGFNet method, CSRPNet (Gaussian) improves the mIoU score by 1.2\%. We also find that several methods tested with the four-channel RGB-T data do not perform better than the version tested with the three-channel RGB data in some cases. We speculate the reason is that these methods ignore the modality gap between the RGB and thermal images, which will adversely affect the segmentation performance.

Furthermore, we compare CSRPNet with the other state-of-the-art methods on the MFNet dataset for daytime and nighttime images. Table~\ref{Table: day_night} reports the corresponding experimental results. CSRPNet (Dot product) and CSRPNet (Gaussian) achieve promising performance for both the daytime and nighttime testing images. Many methods including FuseNet, MFNet, RTFNet-50, and RTFNet-152 ignore the modality gap and thus only obtain limited performance.


\begin{figure}[t!] 
\centering 
\includegraphics[width=1.0\linewidth]{Figures/Experiments/experiment_compare_all_20230411.pdf} 
\caption{\textbf{Qualitative comparisons between different methods.} Compared with other algorithms, our proposed CSRPNet can predict more precise segmentation results in different lighting conditions.} 
\label{Fig: qualitative_comparison} 
\end{figure}

\vspace{1mm}
\noindent\textbf{The PST900 dataset.}
Seven state-of-the-art methods are involved in the comparison, including Efficient FCN~\cite{EffFCN}, CCNet~\cite{ccnet}, ACNet~\cite{ACNet}, MFNet~\cite{MFNet}, RTFNet~\cite{RTFNet}, and PSTNet~\cite{PSTNet}. The proposed CSRPNet (Dot product) and CSRPNet (Gaussian) obtain mAcc scores of 81.4 and 73.9 and mIoU scores of 73.9 and 75.0, respectively. Our methods perform favorably against the other state-of-the-art algorithms, which demonstrates the effectiveness of our method.

\begin{figure}[t!] 
\centering 
\includegraphics[width=0.9\columnwidth]{Figures/Experiments/Qualitative_study1.pdf} 
\caption{\textbf{Visualization of the modality-shared features calculated via the CSRP module (using dot product) in \emph{layer4}.} The visualized images for the two feature maps are generated by averaging all feature channels. The modality-shared feature of the RGB image  and the modality-shared feature of the thermal image  share similar activation distributions and both focus on the foreground objects, which demonstrates that our proposed CSRP module succeeds in capturing the modality-shared information.}
\label{Fig: qualitative_study} 
\end{figure}
\subsection{Qualitative Study}
To obtain more insights into our CSRPNet, we qualitatively compare it with state-of-the-art methods. Figure~\ref{Fig: qualitative_comparison} shows the segmentation results of different methods. The left four image pairs are captured in the daytime, while the right four image pairs are captured in the nighttime. In the daytime scene, the objects with low temperatures are poorly visible in the thermal images, while their texture and color are clearly visible in the RGB images, such as the bump in the first column and the bike in the fourth column. CSRPNet (Dot product) successes recognizing the bump, while the other methods almost cannot. Besides, CSRPNet (Dot product) and CSRPNet (Gaussian) segment the bike more precisely than the MFNet and EGFNet methods. In the night scene, most objects are poorly visible in the RGB images due to insufficient light, while they are clearly visible in the thermal images. Compared with the other methods, CSRPNet (Dot product) and CSRPNet (Gaussian) predict more precise segmentation masks. For example, in the fifth column, the MFNet method misrecognizes several pixels belonging to the person as the car while CSRPNet (Dot product) and CSRPNet (Gaussian) segment the persons precisely. In the seventh column, the EGFNet method misses the bike on the left while CSRPNet (Dot product) and CSRPNet (Gaussian) recognize all the bikes. Besides, the MFNet, RTFNet-50, and RTFNet-152 methods do not use boundaries as auxiliary supervisions. As a result, they cannot process the details of the object boundary well, especially for small objects.


We also visualize the modality-shared features calculated via the CSRP module in \emph{layer4}. Herein dot product is used as the relation function for visualization. Specifically, we visualize two modality-shared feature maps  and ; they contain the modality-shared information captured from the features of the thermal image and the RGB image, respectively. Figure~\ref{Fig: qualitative_study} illustrates the raw RGB-T image pair, ground truth mask, the modality-shared feature of the RGB image , and the modality-shared feature of the thermal image . We can observe that  and  share similar activation distributions and both focus on the foreground objects. Such an observation validates that the proposed CSRP module effectively captures the modality-shared information, allowing interactive feature fusion across modalities without corrupting the modality-specific information.