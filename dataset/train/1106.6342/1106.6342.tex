\documentclass[12pt]{article}

\usepackage{color}
\usepackage{amsmath}
\usepackage{amsthm}


\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[lemma]{Corollary}

\newcommand{\cmt}[1]{{\color{red}#1}}
\newcommand{\bis}{{\prime\prime}}

\newcommand{\algname}[1]{\textsc{#1}\index{algorytm!\textsc{#1}}}
\newcommand{\algdef}[1]{\textsc{#1}\index{algorytm!\textsc{#1}}\label{algdef:#1}}
\newcounter{lnoc}
\newenvironment{algorithm}[1]{\hrule height 0.8pt \vspace{0.6ex} \small#1\vspace{0.6ex}\hrule height 0.5pt \vspace{-2.5ex}
\setcounter{lnoc}{0}
\small
\begin{tabbing}
00000\=XXI\=XXI\=XXI\=XXI\=XXI\=XXI\=\kill
}{\end{tabbing}
\vspace{-2.5ex}\hrule height 0.8pt\vspace{1ex}}
\newcommand{\lno}[1][0]{{\footnotesize\sffamily 
\ifnum#1=0
\stepcounter{lnoc} 
\ifnum\thelnoc<10
\phantom0\fi
\thelnoc
\else
\thelnoc.#1
\fi
}\>}

\newcommand{\pcfor}{{\bfseries for~}}
\newcommand{\pcforall}{{\bfseries for all~}}
\newcommand{\pcforeach}{{\bfseries for each~}}
\newcommand{\pcto}{{\bfseries to~}}
\newcommand{\pcdownto}{{\bfseries downto~}}
\newcommand{\pcdo}{{\bfseries do~}}
\newcommand{\pcif}{{\bfseries if~}}
\newcommand{\pcthen}{{\bfseries then~}}
\newcommand{\pcelse}{{\bfseries else~}}
\newcommand{\pcelseif}{{\bfseries else if~}}
\newcommand{\pcwhile}{{\bfseries while~}}
\newcommand{\pcbreak}{{\bfseries break}}
\newcommand{\pcand}{{\bfseries and~}}
\newcommand{\pcor}{{\bfseries or~}}
\newcommand{\pcnot}{{\bfseries not~}}
\newcommand{\pcreturn}{{\bfseries return~}}
\newcommand{\pcphreturn}{\phantom{\pcreturn}}
\newcommand{\pcfun}[1]{\mbox{\textrm{#1}}}
\newcommand{\pccomment}[1]{\phantom{000}\{\textit{#1}\}}
\newcommand{\pctrue}{\mbox{\textrm{\bfseries true}}}
\newcommand{\pcfalse}{\mbox{\textrm{\bfseries false}}}
\newcommand{\pcdiv}{\mbox{\bfseries div~}}
\newcommand{\pcmod}{\mbox{\bfseries mod~}}
\newcommand{\pcinput}[1]{Input: \textit{#1}}
\newcommand{\pcinpute}[1]{\phantom{Input:} \textit{#1}}
\newcommand{\pcoutput}[1]{Output: \textit{#1}}
\newcommand{\pcoutpute}[1]{\phantom{Output:} \textit{#1}}
\newcommand{\pcendparam}{\end{tabbing}
\vspace{-1cm}
\hrule height 0.5pt
\vspace{-0.5cm}
\begin{tabbing}
00000\=XXI\=XXI\=XXI\=XXI\=XXI\=XXI\=\kill
}

\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\marginparwidth}{0pt}
\setlength{\marginparsep}{0pt}

\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-2in}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-2in}
\setlength{\topmargin}{-0.5in}

\title{Quadratic-time Algorithm\\for the String Constrained LCS Problem}
\author{Sebastian Deorowicz}

\begin{document}
\maketitle

\begin{abstract}
The problem of finding a longest common subsequence of two main sequences with some constraint that must be a substring of the result (STR-IC-LCS) was formulated recently.
It is a variant of the constrained longest common subsequence problem.
As the known algorithms for the STR-IC-LCS problem are cubic-time, the presented quadratic-time algorithm is significantly faster.
\end{abstract}

{\bfseries Keywords}: sequence similarity, longest common subsequence, constrained longest common subsequence

\section{Introduction}

One of the most popular ways of measuring sequence similarity is computation of their longest common subsequence (LCS)~\cite{Gus1997}, in which we are interested in a subsequence that is common to all sequences and has the maximal possible length.
It is well known that for two sequences of length  and  an LCS can be found in  time, which is a lower bound of time complexity in the comparison-based computing model for this problem~\cite{AHU1976}.
In the more practical, RAM model of computations, the asymptotically fastest algorithm is the one by Masek and Paterson which runs in  time for bounded and  for unbounded alphabet~\cite{MP1980}.

One family of LCS-related problems considers one or more {\em constraining} sequences, such that (in some variants) must be included, or (in other problem variants) are forbidden as part of the resulting sequence~\cite{CC2011,Tsa2003}.
The motivation for these generalizations came from bioinformatics in which some prior knowledge is often available and one can specify some requirements on the result~\cite{Tsa2003,Deo2010a}.

In this work, we consider the problem called STR-IC-LCS, introduced in~\cite{CC2011}, in which a constraining sequence of length~ must be included as a substring of a common subsequence of two main sequences and the length of the result must be maximal.
In~\cite{CC2011} an -time algorithm was given for it.
Farhana et al.~\cite{FFMR2010} proposed finite-automata-based algorithms for the STR-IC-LCS, CLCS, and two other problems defined by Chen and Chao~\cite{CC2011}.
The authors claim that the algorithms work in  time in the worst case.
It seems to be a breakthrough as it means also that the LCS problem could be solved in  time.
Unfortunately, the time complexity analysis are based on the claim from~\cite{BY1991} that a directed acyclic subsequence graph (DASG) for two sequences of lengths  and  contains  states and can be built in  time.
As was shown by Crochemore et al.~\cite{CMT2003} this result was wrong and such a DASG contains  states in the worst case, so its construction time cannot be lower.
Thus, the algorithms by Farhana et al.~\cite{FFMR2010} work in  time for the variants of the CLCS problem and  for the LCS problem.
Moreover, these complexities are under assumption that the alphabet size is constant, otherwise they should be multiplied by its size.




In this paper, we propose the first quadratic-time algorithm for the STR-IC-LCS problem and show also further possible improvements of the time complexity.
We also present how this algorithm can be extended to many main input sequences.

The paper is organized as follows.
In Section~\ref{sec:defs}, some definitions are given and the problem is formally stated.
Section~\ref{sec:alg} describes our algorithm.
Extension to the case of many main sequences and some improvements of the algorithm are given in Section~\ref{sec:impr}.
The last section concludes.


\section{Definitions}
\label{sec:defs}
Let us have two main sequences  and  and one constraining sequence .
W.l.o.g.\ we can assume that .
Each sequence is composed of symbols from \emph{alphabet}  of size .
The \emph{length} (or \emph{size}) of any sequence~ is the number of elements it is composed of and is denoted as .
A sequence~ is a subsequence of  if it can be obtained from  by removing zero or more symbols.
The LCS problem for  and  is to find a subsequence~ of both  and~ of the maximal possible length.
The LCS length for  and~ is denoted by~.
A sequence  is a \emph{substring} of  if  for some, possibly empty, sequences , , .
An \emph{appearance} of sequence  in sequence  starting at position  is a sequence of indexes  such that , and .
A \emph{compact} appearance of a sequence  in  starting at position  is the appearance of the smallest last index, .
A \emph{match} for sequences  and  is a pair  such that .
The total number of matches for  and  is denoted by~.
It is obvious that .

The STR-IC-LCS problem for the main sequences , , and the constraining sequence~ is to find a subsequence  of both  and~ of the maximal possible length containing  as its substring.
(In the CLCS problem,  must be a subsequence of .)


\section{The algorithm}
\label{sec:alg}
The algorithm we propose is based on dynamic programming with some preprocessing.
To show its correctness it is necessary to prove some lemma.

Let  be a longest common subsequence with substring constraint for , , and .
Let also  be a sequence of indexes of  symbols in  and~, i.e.,  and .
From the problem statement, there must exists such  that  and  .

\begin{lemma}\label{lem:i-prime}
Let  and for all ,  be the smallest possible, but larger than , index in  such that 
The sequence of indexes  defines a longest common subsequence of~ and~ with string constraint~ equal~.
\end{lemma}

\begin{proof}
From the definition of indexes  it is obvious that they form an increasing sequence, since , and .
The sequence  is of course a compact appearance of  in~ starting at .
Therefore, both components of  pairs form increasing sequences and for any , , so sequence  defines an STR-IC-LCS  equal~.
\end{proof}

A similar lemma can be formulated for -th component of sequence~.
Thus, it is easy to conclude that when looking for an STR-IC-LCS, instead of checking any common subsequences of  and~ it suffices to check only such common subsequences that contain compact appearances of  both in  and~.
(This is a direct consequence of the fact that  for any sequence~.)

The number of different compact appearances of  in  and  will be denoted by  and~, respectively.
It is easy to notice that , since a pair  defines a compact appearance of  in  starting at -th position and compact appearance of  in  starting at -th position only for some matches.

The algorithm computing an STR-IC-LCS (Fig.~\ref{psc:alg}) consists of three main stages.
In the first stage, both main sequences are preprocessed to determine for each occurrence of the first symbol of , the index of the last symbol of a~compact appearance of~.
In the second stage, two DP matrices are computed: the forward one and the reverse one.
The recurrence is exactly as for the LCS computation.

\begin{figure}[t]
\begin{algorithm}{\algdef{STR-IC-LCS}(, , )}
\pccomment{Preprocessing}\\
\lno	\pcfor  \pcto  \pcdo\\
\lno\>	\pcif  \pcthen
				qp_1\ldots p_ra_i \ldots a_q\\
\lno	\pcfor  \pcto  \pcdo\\
\lno\>	\pcif  \pcthen
				qp_1\ldots p_rb_j \ldots b_q\\
\pccomment{Computation of forward and reverse DP matrices}\\
\lno	\pcfor  \pcto  \pcdo
			; \\
\lno	\pcfor  \pcto  \pcdo
			; \\
\lno	\pcfor  \pcto  \pcdo\\
\lno\>	\pcfor  \pcto  \pcdo\\
\lno\>\>		\pcif  \pcthen
					\\
\lno\>\>		\pcelse
					\\
\lno	\pcfor  \pcdownto  \pcdo\\
\lno\>	\pcfor  \pcdownto  \pcdo\\
\lno\>\>		\pcif  \pcthen
					\\
\lno\>\>		\pcelse
					\\
\pccomment{Determination of the result}\\
\lno	; ; \\
\lno	\pcfor  \pcto  \pcdo\\
\lno\>	\pcfor  \pcto  \pcdo\\
\lno\>\>		\pcif  \pcand
					 \pcthen\\
\lno\>\>\>		\\
\lno\>\>\>		; \\
\lno	Backtrack from  according to  and obtain \\
\lno	Backtrack from  according to  and obtain \\
\lno	\pcreturn  and 
\end{algorithm}
\caption{A pseudocode of the STR-IC-LCS computing algorithm for two main sequences and one constraining sequence}
\label{psc:alg}
\end{figure}

In the last stage, the result is determined.
To this end for each match  for  and  the ends  of compact appearances of  in  starting at -th position and in  starting at -th position are read.
The length of an STR-IC-LCS containing these appearances of  is determined as a sum of the LCS length of prefixes of  and  ending at -th and -th positions, respectively, the LCS length of suffixes of  and  starting at -th and -th positions, respectively, and the constraint length.
Since, the first and last constraint symbol was summed twice, the final result is decreased by~2.
According to the  and  matrices, backtracking can be used to obtain the subsequence, not only its length.

\begin{lemma}
The STR-IC-LCS algorithm (Fig.~\ref{psc:alg}) correctly computes an STR-IC-LCS.
\end{lemma}

\begin{proof}
The algorithm considers all pairs of compact appearances of  in  and .
Each such a pair divides the problem into two independent LCS length-computing subproblems.
According to the precomputed  and  matrices it is easy to solve these subproblems (lines 18--20) in constant time.
The length of an STR-IC-LCS must be a sum of the found lengths of LCSs and the constraint length subtracted by~2.
\end{proof}

\begin{lemma}
The worst-case time complexity of the proposed algorithm is .
\end{lemma}

\begin{proof}
The preprocessing stage can be done in  worst-case time.
The main stage consists of computation of two DP matrices which needs  time.
In the final stage, the DP matrix is traversed and for each match a constant number of operations is performed, so these stages consumes  time.
Summing these up gives  time.
\end{proof}

\begin{lemma}
The space consumption of the algorithm is .
\end{lemma}



\section{Improvements and extensions}
\label{sec:impr}
If one is interested only in the STR-IC-LCS length, it is easy to notice that  and  matrices can be computed row-by-row which means that only  words are necessary for them.
The values of  and~ for matches of symbols equal  in~ and  in~ must, however, be stored explicitly, so the space for them is  (where  is the number of such matches).
This gives the total space .
If also the subsequence is requested, the cells for all matches must be stored to allow backtracking, so the space is  (in the worst case ).


As the only cells that are necessary to be stored explicitly are those for matches, the Hunt--Szymanski method~\cite{Gus1997} can be used to speed up the computation of  and  matrices if the number of matches is small.
Therefore, the second stage can be completed in  time if  and  otherwise.
The time complexity of the final stage is~.
Adding the time for the preprocessing we obtain the worst case time complexities:
 for  and  otherwise.


The generalization of the LCS problem for many sequences is direct, but the time complexity of the exact algorithm computing the multidimensional DP matrix is , where  is the number of sequences of length~ each~\cite{Gus1997}.
It is easy to notice that according to Lemma~\ref{lem:i-prime} the STR-IC-LCS problem generalizes in the same way and the worst-case time complexity is also .


\section{Conclusions}
\label{sec:concs}
We investigated the STR-IC-LCS problem introduced recently.
The fastest algorithms solving this problem known to date needed cubic time in case of two main and one constraining sequences.
Our algorithm is faster, as its time complexity is only quadratic.
Moreover, the algorithm uses an LCS-computation procedure as a component and any progresses in the LCS computation can improve the time complexities of the proposed method.

We also showed an irrecoverable flaw in~\cite{FFMR2010}, in which the algorithm of better than cubic time complexity was recently proposed, i.e., we proved this algorithm is supercubic in the worst case.

\subsection*{Acknowledgments}
The author thanks Szymon Grabowski for reading preliminary versions of the paper and suggesting improvements.

\begin{thebibliography}{1}

\bibitem{AHU1976}
A.V. Aho, D.S. Hirschberg, and J.D. Ullman.
\newblock Bounds on the complexity of the longest common subsequence problem.
\newblock {\em Journal of the ACM}, 23(1):1--12, 1976.

\bibitem{BY1991}
R.A. Baeza-Yates.
\newblock Searching subsequences.
\newblock {\em Theoretical Computer Science}, 78:363--376, 1991.

\bibitem{CC2011}
Y.-C. Chen and K.-M. Chao.
\newblock On the generalized constrained longest common subsequence problems.
\newblock {\em Journal of Combinatorial Optimization}, 21:383--392, 2011.

\bibitem{CMT2003}
M.~Crochemore, B.~Melichar, and Z.~Tron{\'i}{\v{c}}ek.
\newblock Directed acyclic subsequence graph---overview.
\newblock {\em Journal of Discrete Algorithms}, 1:255--280, 2003.

\bibitem{Deo2010a}
S.~Deorowicz.
\newblock Bit-parallel algorithm for the constrained longest common subsequence
  problem.
\newblock {\em Fundamenta Informaticae}, 99(4):409--433, 2010.

\bibitem{FFMR2010}
E.~Farhana, J.~Ferdous, T.~Moosa, and M.S. Rahman.
\newblock Finite automata based algorithm for the generalized constrained
  longest common subsequence problems.
\newblock {\em LNCS}, 6393:243--249, 2010.

\bibitem{Gus1997}
D.~Gusfield.
\newblock {\em Algorithms on Strings, Trees and Sequences: Computer Science and
  Computational Biology}.
\newblock Cambridge University Press, 1997.

\bibitem{MP1980}
W.J. Masek and M.S. Paterson.
\newblock A faster algorithm computing string edit distances.
\newblock {\em Journal of Computer System Science}, 20(1):18--31, 1980.

\bibitem{Tsa2003}
Y.-T. Tsai.
\newblock The constrained common subsequence problem.
\newblock {\em Information Processing Letters}, 88:173--176, 2003.

\end{thebibliography}
 
\end{document}
