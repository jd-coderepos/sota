





\newcommand{\customizedparagraph}{\textbf}



\title{View-Invariant Probabilistic Embedding for Human Pose} \subtitle{Supplementary Materials}



\titlerunning{View-Invariant Probabilistic Embedding for Human Pose}
\author{Jennifer~J.~Sun\index{Sun,Jennifer~J.}\inst{1} \and
Jiaping~Zhao\inst{2} \and
Liang-Chieh~Chen\inst{2} \and
Florian~Schroff\inst{2} \and
Hartwig~Adam\inst{2} \and
Ting~Liu\inst{2}}

\authorrunning{J.J.~Sun et al.}
\institute{California Institute of Technology \\
\email{jjsun@caltech.edu}
\and
Google Research\\
\email{\{jiapingz,lcchen,fschroff,hadam,liuti\}@google.com}}
\maketitle

In this document, we cover the details of the implementation and experiments for our work. We also provide additional ablation studies and analysis. Specifically, we have:
\begin{itemize}
    \item[\textbullet] \hyperref[sec:3d_visual_similarity]{\textbf{\textcolor{MidnightBlue}{Section A}}} describes how we decide the \textbf{NP-MPJPE threshold} based on its effect on visual pose similarity.
    \item[\textbullet] \hyperref[sec:implementation_details]{\textbf{\textcolor{MidnightBlue}{Section B}}} provides additional \textbf{implementation details} on model training, keypoint definition and normalization, downstream task experiment setup, etc.
    \item[\textbullet] \hyperref[sec:ablation]{\textbf{\textcolor{MidnightBlue}{Section C}}} provides additional \textbf{ablation studies}, including the effect of key hyperparameters, ordered embedding variance visualizations, and embedding space visualization.
    \item[\textbullet] \hyperref[sec:additional_comp]{\textbf{\textcolor{MidnightBlue}{Section D}}} provides additional \textbf{quantitative pose retrieval} result comparisons with image-based EpipolarPose model~\cite{kocabas2019self} for view-invariant pose retrieval.
    \item[\textbullet] \hyperref[sec:pose_retrieval_qres]{\textbf{\textcolor{MidnightBlue}{Section E}}} provides additional \textbf{qualitative pose retrieval} results.
    \item[\textbullet] \hyperref[sec:video_alignment_qres]{\textbf{\textcolor{MidnightBlue}{Section F}}} describes the \textbf{qualitative video alignment} experiment. Please refer to {\scriptsize{\url{https://drive.google.com/open?id=1kTc_UT0Eq0H2ZBgfEoh8qEJMFBouC-Wv}}} for the video synchronization results.
\end{itemize}

\section{Visualization of 3D Visual Similarity}\label{sec:3d_visual_similarity}

The 3D pose space is continuous, and we use the NP-MPJPE as a proxy to quantify visual similarity between pose pairs. Fig.~\ref{fig:supp_similarity} shows pairs of 3D pose keypoints with their corresponding NP-MPJPE, where each row depicts a different NP-MPJPE range. This plot demonstrates the effect of choosing different $\kappa$, which controls matching threshold between 3D poses. If we choose $\kappa = 0.05$, then only the first row in Fig.~\ref{fig:supp_similarity} would be considered matching, and the rest of the rows are non-matching. Our current value of $\kappa = 0.10$ corresponds to using the first two rows as matching pairs and the rest of the rows as non-matching ones. By loosening $\kappa$, poses with greater differences will be considered as matching, as shown by different rows in Fig.~\ref{fig:supp_similarity}. We note that pairs in rows 3 and 4 shows significant visual differences compared with the first two rows. We further investigate the effects of different $\kappa$ during training and evaluation in Section~\ref{sec:ablation}.

\def\figsize{0.24}
\def\fighspace{1mm}
\def\fighspacer{-3mm}
\begin{figure*}[!t]
\centering
\begin{tabular}{cccccc}
\centering

\scriptsize{NP-MPJPE: $0.011$}\hspace{\fighspace} & \scriptsize{NP-MPJPE: $0.026$}\hspace{\fighspace} & \scriptsize{NP-MPJPE: $0.033$} & \scriptsize{NP-MPJPE: $0.049$}\hspace{\fighspace} \\

\includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff1_p4}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff2_p1.png}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff2_p3.png}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff2_p4.png}\hspace{\fighspace}  \\

\scriptsize{NP-MPJPE: $0.065$} & \scriptsize{NP-MPJPE: $0.074$}\hspace{\fighspace} & \scriptsize{NP-MPJPE: $0.085$} & \scriptsize{NP-MPJPE: $0.091$} \\

\includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff3_p3.png}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff3_p4.png}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff4_p3.png}\hspace{\fighspacer}  & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff4_p2.png}\hspace{\fighspace} \\


\scriptsize{NP-MPJPE: $0.111$} & \scriptsize{NP-MPJPE: $0.121$}\hspace{\fighspace} & \scriptsize{NP-MPJPE: $0.134$} & \scriptsize{NP-MPJPE: $0.145$}\hspace{\fighspace} \\

\includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff5_p4.png}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff5_p3.png}\hspace{\fighspacer} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff6_p3.png}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff6_p4.png}\hspace{\fighspacer} \\


\scriptsize{NP-MPJPE: $0.159$} & \scriptsize{NP-MPJPE: $0.174$}\hspace{\fighspace} & \scriptsize{NP-MPJPE: $0.185$} & \scriptsize{NP-MPJPE: $0.192$}\hspace{\fighspace} \\

\includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff7_p1.png}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff7_p2.png}\hspace{\fighspacer} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff7_p3.png}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/similarity_vis/diff7_p4.png}\hspace{\fighspacer} \\



\end{tabular}
\caption{3D pose pairs with different NP-MPJPE, where the NP-MPJPE increases with each row. The poses are randomly sampled from the hold-out set of H3.6M. Row 1 shows pairs with $0.00$ to $0.05$ NP-MPJPE, row 2 shows pairs with $0.05$ to $0.10$ NP-MPJPE, row 3 shows pairs with $0.10$ to $0.15$ NP-MPJPE, and row 4 shows pairs with $0.15$ to $0.20$ NP-MPJPE.}
\label{fig:supp_similarity}
\vspace{-0.2cm}
\end{figure*}
 
\section{Additional Implementation Details}\label{sec:implementation_details}

The backbone network architecture for our model is based on~\cite{martinez2017simple}. We use two residual blocks, batch normalization, $0.3$ dropout, and no maximum weight norm constraint~\cite{martinez2017simple}. During training, we use exponential moving average with $0.9999$ decay rate and normalize matching probabilities to within $[0.05,0.95]$ for numerical stability. We use Adagrad optimizer~\cite{duchi2011adaptive} with fixed learning rate $0.02$ and batch size $N=256$.

\customizedparagraph{Keypoint Definition} Fig.~\ref{fig:skeleton} illustrates the keypoints that we use in our experiments. The 3D poses used in our experiments are the 17 keypoints corresponding to the H3.6M~\cite{ionescu2013human3} skeleton used in~\cite{martinez2017simple}, shown in Fig.~\ref{fig:skeleton17}. We use this keypoint definition to compute NP-MPJPE for 3D poses and evaluate retrieval accuracy. The Pr-VIPE training and inference process do not depend on a particular 2D keypoint detector. Here, we use PersonLab (ResNet152 single-scale)~\cite{papandreou2018personlab} in our experiments. Our 2D keypoints are selected from the keypoints in COCO~\cite{lin2014microsoft}, which is the set of keypoints detected by PersonLab~\cite{papandreou2018personlab}. We use the 12 body keypoints from COCO and select the ``Nose" keypoint as the head, shown in Fig.~\ref{fig:skeleton13}.

\begin{figure}[t]
  \centering
  \subfloat[17 keypoints based on H3.6M.\label{fig:skeleton17}]{\includegraphics[width=0.45\textwidth]{supplementary_figures/Skeleton17.pdf}}
  \subfloat[13 keypoints based on COCO.\label{fig:skeleton13}]{\includegraphics[width=0.45\textwidth]{supplementary_figures/Skeleton13.pdf}}
  


\caption{Visualization of pose keypoints used in our experiments.}
\label{fig:skeleton}
\end{figure}

\customizedparagraph{Pose Normalization} We normalize our 2D and 3D poses such that camera parameters are not needed during training and inference. 
For 3D poses, our normalization procedure is similar to that in~\cite{chen2019unsupervised}. We translate a 3D pose so that the hip located at the origin. We then scale the hip to spine to thorax distance to a unit scale. 
For 2D poses, we translate the keypoints so that the center between LHip and RHip is at the origin. Then we normalize the pose such that the maximum distance between shoulder and hip joints is $0.5$. This maximum distance is computed between all pairwise distances among RShoulder, LShoulder, RHip, and LHip.

\customizedparagraph{Downstream Task Experiments} For the action recognition experiment, we follow the standard evaluation protocol~\cite{xia2012view} and remove action ``strum guitar'' and several videos in which less than one third of the target person is visible. We use the official train/test split and report the averaged per-class accuracy. For the view-invariant action recognition experiments in which the index set only contains videos from a single view, we exclude the actions that have zero or only one sample under a particular view. We take the bounding boxes provided with the dataset and use~\cite{papandreou2017towards} (ResNet101) for 2D pose keypoint estimation. For frames of which the bounding box is missing, we copy the bounding box from the nearest frame. Finally, since our embedding is chiral, but certain actions can be done with either body side (pitching a baseball with left or right hand), when we compare two frames, we extract our embeddings from both the original and the mirrored version of each frame, and use the minimum distance between all the pairwise combinations as the frame distance.

For the video alignment experiment, we follow the protocol in~\cite{dwibedi2019temporal}, excluding ``jump rope'' and ``strum guitar'' from our evaluation. For the evaluations between videos under only the same or different views, we exclude actions that have zero videos under a particular view from the average Kendall's Tau computation. Since certain actions can be done with either body side, for a video pair $(v_1,v_2)$, we compute the Kendall's Taus between $(v_1,v_2)$ and $(v_1,\text{mirror}(v_2))$, and use the larger number.

\section{Additional Ablation Studies}\label{sec:ablation}

\customizedparagraph{Effect of Number of Samples $K$ and Margin Parameter $\beta$} Table~\ref{tab:ablation} shows the effect of the number of samples $K$ and the margin parameter $\beta$ (actual triplet margin $\alpha=\log\beta$) on Pr-VIPE. The number of samples control how many points we sample from the embedding distribution to compute matching probability and $\beta$ controls the ratio of matching probability between matching and non-matching pairs. Our model is robust to the choice of $\beta$ in terms of retrieval accuracy as shown by Table~\ref{tab:ablation}. The main effect of $\beta$ is on retrieval confidence, as non-matching pairs are scaled to a smaller matching probability for larger $\beta$. Pr-VIPE performance with 10 samples is competitive with the baselines in the main paper, but we do better with 20 samples. Increasing the number of samples further has similar performance. For our experiments, we use $20$ samples and $\beta = 2$.

\begin{table}
  \centering
  \caption{Additional ablation study results of Pr-VIPE on H3.6M with the number of samples $K$ and margin parameter $\beta$.} \label{tab:ablation}
  \scalebox{1.00}{
   \begin{tabular}{c c | c c c}
   \toprule[0.2em]
    Hyperparameter & Value & Hit@$1$ & Hit@$10$ & Hit@$20$ \\ [0.25ex]
   \toprule[0.2em]
 \multirow{3}{*}{$K$} & $10$ & $0.744$ & $0.948$ & $0.971$\\
  & $20$ & $0.762$ & $0.956$ & $0.977$ \\
  & $30$ & $0.755$ & $0.955$ & $0.975$\\
 \hline
 \multirow{4}{*}{$\beta$} & $1.25$ & $0.758$ & $0.956$ & $0.977$\\
  & $1.5$ & $0.759$ & $0.956$ & $0.977$ \\
  & $2$ & $0.762$ & $0.956$ & $0.977$ \\
  & $3$ & $0.760$ & $0.955$ & $0.976$\\
   \bottomrule[0.1em]
\end{tabular}
}
\end{table}

\customizedparagraph{Effect of Camera Augmentation} We explore the effect of different random rotations during camera augmentation on pose retrieval results in Table~\ref{tab:ablation_aug}. All models are trained on the 4 chest-level cameras on H3.6M but the models with camera augmentation also use projected 2D keypoints from randomly rotated 3D poses. For the random rotation, we always use azimuth range of $\pm180^{\circ}$, and we test performance with different angle limits for elevation and roll. We see that the model with no augmentation does the best on the H3.6M, which has the same 4 camera views as training. With increase in rotation angles during mixing, the performance on chest-level cameras drop while performance on new camera views generally increases. The results demonstrate that mixing detected and projected keypoints reduces model overfitting on camera views used during training. Training using randomly rotated keypoints enables our model to generalize much better to new views.

\begin{table}
  \centering
  \caption{Additional ablation study results of Pr-VIPE on H3.6M and 3DHP using different rotation thresholds for camera augmentation. The angle threshold for azimuth is always $\pm180^{\circ}$ and the angle thresholds in the table are for elevation and roll. The row for w/o aug. corresponds to Pr-VIPE without augmentation.} \label{tab:ablation_aug}
  \scalebox{1.00}{
   \begin{tabular}{c c | c c c c c}
   \toprule[0.2em]
   & & \multicolumn{3}{c}{Hit@$1$ on evaluation dataset}\\
    Hyperparameter & Range & H3.6M & 3DHP (all) & 3DHP (chest) \\ [0.25ex]
   \toprule[0.2em]
  \multirow{4}{*}{Elevation and Roll Angle} & w/o aug. & $0.762$ & $0.199$ & $0.255$\\
  & $\pm15^{\circ}$ & $0.747$ & $0.252$ & $0.289$ \\
  & $\pm30^{\circ}$ & $0.737$ & $0.264$ & $0.283$\\
  & $\pm45^{\circ}$ & $0.737$ & $0.262$ & $0.273$ \\
   \bottomrule[0.1em]
\end{tabular}
}
\end{table}


\begin{table}
  \centering
  \caption{Additional ablation study results of Pr-VIPE on H3.6M with different NP-MPJPE threshold $\kappa$ for training and evaluation.} \label{tab:ablation_kappa}
  \scalebox{1.00}{
  \begin{tabular}{c | c c c c}
  \toprule[0.2em]
  & \multicolumn{4}{c}{Hit@$1$ with evaluation $\kappa$}\\
  Training $\kappa$ & $0.05$ & $0.10$ & $0.15$ & $0.20$ \\[0.25ex]
  \toprule[0.2em]
 $0.05$ & $\bm{0.495}$ & $0.761$ & $0.908$ & $0.962$\\
 $0.10$ & $0.489$ & $\bm{0.762}$ & $0.909$ & $0.963$ \\
 $0.15$ & $0.462$ & $0.753$ & $\bm{0.910}$ & $\bm{0.965}$\\
 $0.20$ & $0.429$ & $0.731$ & $0.906$ & $\bm{0.965}$\\
  \bottomrule[0.1em]
\end{tabular}
}
\end{table}

\customizedparagraph{Effect of NP-MPJPE threshold $\kappa$} We train and evaluate with different values of the NP-MPJPE threshold $\kappa$ in Table~\ref{tab:ablation_kappa}. $\kappa$ controls the NP-MPJPE threshold for a matching pose pair and visualizations of pose pairs with different NP-MPJPE are in Fig.~\ref{fig:supp_similarity}. Table~\ref{tab:ablation_kappa} shows that Pr-VIPE generally achieves the best accuracy for a given NP-MPJPE threshold when the model is trained with the same matching threshold. Additionally, when we train with a tight threshold, e.g., $\kappa = 0.05$, we do comparatively well on accuracy at looser thresholds. In contrast, when we train with a loose threshold, e.g., $\kappa = 0.20$, we do not do as well given a tighter accuracy threshold at evaluation. This is because when we push non-matching poses using the triplet ratio loss, $\kappa = 0.20$ only pushes poses that are more than $0.20$ NP-MPJPE apart, and does not explicitly push poses less than the NP-MPJPE threshold. The closest retrieved pose will then be within $0.20$ NP-MPJPE but it is not guaranteed to be within any threshold $<0.20$ NP-MPJPE. But when we use $\kappa = 0.05$ for training, poses that are more than $0.05$ NP-MPJPE are pushed apart, which also satisfies $\kappa = 0.20$ threshold.

In the main paper, we use $\kappa = 0.1$. For future applications with other matching definitions, the Pr-VIPE framework is flexible and can be trained with different $\kappa$ to satisfy different accuracy requirements.


\customizedparagraph{Additional Plots for Ordered Variances} Similar to the main paper, we retrieve poses using 2D NP-MPJPE for the top-$3$ 2D poses with smallest and largest variances in Fig.~\ref{fig:variance}. Fig.~\ref{fig:smallest_variance} shows that for the poses with the top-$3$ smallest variances, the nearest 2D pose neighbors are visually distinct, which means that these 2D poses are less ambiguous. On the other hand, the nearest 2D pose neighbors of the poses with the largest variances in Fig.~\ref{fig:largest_variance} are visually similar, which means that these 2D poses are more ambiguous.

\begin{figure*}[t!]
  \centering
  \subfloat[Poses with top-$3$ smallest variance and their nearest neighbors in terms of 2D NP-MPJPE.\label{fig:smallest_variance}]{\includegraphics[width=\textwidth]{supplementary_figures/Smallest_variance.pdf}}\\
  \subfloat[Poses with top-$3$ largest variance and their nearest neighbors in terms of 2D NP-MPJPE.\label{fig:largest_variance}]{\includegraphics[width=\textwidth]{supplementary_figures/Largest_variance.pdf}}
  




\caption{Top retrievals by 2D NP-MPJPE from H3.6M hold-out subset for queries with top-$3$ largest and smallest variances. 2D poses are shown in the boxes.}
\label{fig:variance}
\end{figure*}

\customizedparagraph{Embedding Space Visualization} We run Principal Component Analysis (PCA) on the $16$-dimensional embeddings using the Pr-VIPE model. Fig.~\ref{fig:pca_supp} visualizes the first two principal dimensions. To visualize more unique poses, we randomly subsample the H3.6M hold-out set and select 3D poses at least 0.1 NP-MPJPE apart. Fig.~\ref{fig:pca_supp} demonstrates that 2D poses from similar 3D poses are close together, while non-matching poses are further apart. Standing and sitting poses seem well separated from the two principle dimensions. Additionally, there are leaning poses between sitting and standing. Poses near the top of the figure have arms raised, and there is generally a gradual transition to the bottom of the figure, where arms are lowered. These results show that from 2D joint keypoints only, we are able to learn view-invariant properties with compact embeddings.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.6\linewidth]{figures/Fig_6.pdf}
  \caption{Visualization of Pr-VIPE space with 2D poses in the H3.6M hold-out subset using the first two PCA dimensions.}
  \label{fig:pca_supp}
  \vspace{-0.3cm}
\end{figure*}


\section{Additional Quantitative Pose Retrieval Results}\label{sec:additional_comp}

We show an additional view-invariant pose retrieval evaluation comparing Pr-VIPE (with camera augmentation) to EpipolarPose~\cite{kocabas2019self}, a recent multi-view image based model, on cross-view pose retrieval. For Human3.6M, EpipolarPose is trained with the same training split as Pr-VIPE. The evaluation split we use is a frame subset provided by~\cite{kocabas2019self} for which the authors provided cropping boxes based on groundtruth 3D keypoints. The input images are cropped using these bounding boxes, and the trained models provided by the authors are then ran on the cropped images. In this way, we evaluate EpipolarPose using all the information provided by the authors. In comparison, Pr-VIPE uses detected keypoints and no groundtruth information for inference.

We show retrieval results on Human3.6M since~\cite{kocabas2019self} is based on images and requires a different model to be trained for 3DHP. We emphasize that this is a different evaluation split from our main paper, since we use the evaluation subset of Human3.6M for which~\cite{kocabas2019self} provides bounding boxes. On this subset, Pr-VIPE with augmentation achieves $75.2\%$ Hit@$1$, fully supervised EpipolarPose achieves $72.7\%$ Hit@$1$ and self-supervised EpipolarPose achieves $67.8\%$ Hit@$1$.

These results show the effectiveness of Pr-VIPE for pose retrieval. Our model, using detected 2D keypoints and no groundtruth information, can retrieve poses more accurately compared with~\cite{kocabas2019self}. We further note that 3D pose estimation models require rigid alignment between every query-index pairs to achieve their best performance for retrieval, while Pr-VIPE does not require post-processing. 

\section{Additional Qualitative Pose Retrieval Results}\label{sec:pose_retrieval_qres}

We present more view-invariant pose retrieval qualitative results for Pr-VIPE on all the relevant datasets in Fig.~\ref{fig:supp_retrieval}. The first two rows show results on H3.6M, the next three rows are on 3DHP and the last two rows shows results using the hold-out set in H3.6M to retrieve from 2DHP. We are able to retrieve across camera views and subjects on all datasets.

\def\figsize{0.15}
\def\fighspace{-0.5mm}
\def\fighspacer{2.5mm}
\begin{figure*}
\centering
\begin{tabular}{cccccc}
\centering
\scriptsize{$C=0.990$}\hspace{\fighspace} & \scriptsize{$E=0.084$}\hspace{\fighspacer} & \scriptsize{$C=0.983$}\hspace{\fighspace} & \scriptsize{$E=0.000$}\hspace{\fighspacer} & \scriptsize{$C=0.994$}\hspace{\fighspace} & \scriptsize{$E=0.132$}\hspace{\fighspacer} \\
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_q1}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_r1}\hspace{\fighspacer}  & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_q2}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_r2}\hspace{\fighspacer} &
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_q3}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_r3}\hspace{\fighspacer} \\

\scriptsize{$C=0.905$}\hspace{\fighspace} & \scriptsize{$E=0.066$}\hspace{\fighspacer} & \scriptsize{$C=0.905$}\hspace{\fighspace} & \scriptsize{$E=0.094$}\hspace{\fighspacer} & \scriptsize{$C=0.888$}\hspace{\fighspace} & \scriptsize{$E=0.130$}\hspace{\fighspacer} \\
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_q4}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_r4}\hspace{\fighspacer}  & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_q5}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_r5}\hspace{\fighspacer} &
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_q6}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/h36m_r6}\hspace{\fighspacer} \\

\scriptsize{$C=0.806$}\hspace{\fighspace} & \scriptsize{$E=0.085$}\hspace{\fighspacer} & \scriptsize{$C=0.891$}\hspace{\fighspace} & \scriptsize{$E=0.119$}\hspace{\fighspacer} & \scriptsize{$C=0.599$}\hspace{\fighspace} & \scriptsize{$E=0.222$}\hspace{\fighspacer} \\
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_q1}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_r1}\hspace{\fighspacer}  & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_q2}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_r2}\hspace{\fighspacer} &
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_q3}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_r3}\hspace{\fighspacer} \\


\scriptsize{$C=0.999$}\hspace{\fighspace} & \scriptsize{$E=0.048$}\hspace{\fighspacer} & \scriptsize{$C=0.976$}\hspace{\fighspace} & \scriptsize{$E=0.152$}\hspace{\fighspacer} & \scriptsize{$C=0.727$}\hspace{\fighspace} & \scriptsize{$E=0.295$}\hspace{\fighspacer} \\
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_q4}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_r4}\hspace{\fighspacer}  & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_q5}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_r5}\hspace{\fighspacer} &
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_q9}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_r9}\hspace{\fighspacer} \\

\scriptsize{$C=0.780$}\hspace{\fighspace} & \scriptsize{$E=0.085$}\hspace{\fighspacer} & \scriptsize{$C=0.951$}\hspace{\fighspace} & \scriptsize{$E=0.208$}\hspace{\fighspacer} & \scriptsize{$C=0.918$}\hspace{\fighspace} & \scriptsize{$E=0.328$}\hspace{\fighspacer} \\
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_q7}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_r7}\hspace{\fighspacer}  & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_q8}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_r8}\hspace{\fighspacer} &
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_q6}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/3dhp_r6}\hspace{\fighspacer} \\


\scriptsize{$C=0.867$}\hspace{\fighspace} & \hspace{\fighspacer} & \scriptsize{$C=0.321$}\hspace{\fighspace} & \hspace{\fighspacer} & \scriptsize{$C=0.936$}\hspace{\fighspace} & \hspace{\fighspacer} \\
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_q1}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_r1}\hspace{\fighspacer}  & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_q3}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_r3}\hspace{\fighspacer} &
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_q5}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_r5}\hspace{\fighspacer} \\

\scriptsize{$C=0.735$}\hspace{\fighspace} & \hspace{\fighspacer} & \scriptsize{$C=0.976$}\hspace{\fighspace} & \hspace{\fighspacer} & \scriptsize{$C=0.448$}\hspace{\fighspace} & \hspace{\fighspacer} \\
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_q6}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_r6}\hspace{\fighspacer}  & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_q7}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_r7}\hspace{\fighspacer} &
\includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_q9}\hspace{\fighspace} & \includegraphics[width=\figsize\textwidth]{supplementary_figures/retrieval_vis/2dhp_r9}\hspace{\fighspacer} \\

\end{tabular}
\caption{Visualization of pose retrieval results. On each row, we show the query pose on the left for each image pair and the top-$1$ retrieval using the Pr-VIPE model with camera augmentation on the right. We display the retrieval confidences (``C'') and top-$1$ NP-MPJPEs (``E'', if 3D pose groundtruth is available).}
\label{fig:supp_retrieval}
\vspace{-0.2cm}
\end{figure*}
 
On H3.6M, retrieval confidence is generally high and retrievals are visually accurate. NP-MPJPE is in general smaller on H3.6M compared to 3DHP, since 3DHP has more diverse poses and camera views. The model works reasonably well on 3DHP despite additional variations on pose, viewpoints and subjects. For the pairs R4C3 and R5C3, the subjects are occluded by the chair and the pose inferred by the 2D keypoint detector may not be accurate. Our model is dependent on the result of the 2D keypoint detector. Interestingly, R3C2 and R4C3 show retrievals with large rolls, which is unseen during training. The results on 3DHP demonstrate the generalization capability of our model to unseen poses and views. To test on in-the-wild images, we use the hold-out set of H3.6M to retrieve from 2DHP. The retrieval results demonstrate that Pr-VIPE embeddings can retrieve visually accurate poses from detected 2D keypoints. R7C2 is particularly interesting, as the retrieval has a large change in viewpoint. For the low confidence pairs R6C2 and R7C3, we can see that the arms of the subjects seems to be bent slightly differently. In contrast, the higher confidence retrieval pairs looks visually similar. The results suggest that performance of existing 2D keypoint detectors, such as~\cite{papandreou2018personlab}, is sufficient to train pose embedding models to achieve the view-invariant property in diverse images.

\section{Qualitative Video Alignment Results}\label{sec:video_alignment_qres}

We show that Pr-VIPE can be applied to synchronize action videos from different views from the Penn Action dataset (test set). The videos are synchronized to the pace of a target video (placed in the center of each video array). This allows us to play different videos of the same action at the same pace. The results for different aligned actions are located at \url{https://drive.google.com/open?id=1kTc_UT0Eq0H2ZBgfEoh8qEJMFBouC-Wv}. The alignment procedure for Pr-VIPE is described in Section 4.3.2 in the main paper.

