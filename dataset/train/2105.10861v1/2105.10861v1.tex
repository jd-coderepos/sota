
Assuming that a document has already been segmented into EDUs, following the traditional approach, the corresponding discourse tree (DT) can be represented as a set of {labeled constituents}.

where  is the number of internal nodes in the tree and  is the relation label between the discourse unit containing EDUs  through  and the one containing EDUs  through . 

Traditionally, in RST parsing, {discourse segmentation} is performed first to obtain the sequence of EDUs, which is followed by the {parsing} process to assemble the EDUs into a labeled tree. In other words, traditionally discourse segmentation and parsing have been considered as two distinct tasks that are solved by two different models. 

On the contrary, in this work we take a radically different approach that directly starts with parsing the (unsegmented) document in a top-down manner and treats discourse segmentation as a special case of parsing that we get as a by-product. Importantly, this novel formulation of the problem allows us to solve the two problems in a single neural model. Our parsing model is generic and also works in the same way when it is fed with an EDU-segmented text. Before presenting the model architecture, we first formulate the problem as a splitting decision problem at the token level.  

\begin{figure*}[t!]
\begin{tikzpicture}
\node[align=left, above] at (-1,0)
{\includegraphics[width=0.95\textwidth]{imgs/discourse_parsing_doc_boundary_indices_example_v4.png}};
\small
\node[align=left, above] at (-3.7,-0.5) {\textbf{Boundary-based splitting representation when EDUs are provided}};     
\node[align =left, above] at (-2.2,-1) {\textbf{} \{, , ,, \}};
\node[align=left, above] at (-3.8,-1.5) {\textbf{Boundary-based splitting representation for end-to-end parsing}};    
\node[align =left, above] at (-1.6,-2.3) {\textbf{} \{, , , , , , \\
, , , , \}};        
\normalsize
\end{tikzpicture}
\caption{A discourse tree for two sentences in the RST discourse treebank. The internal nodes (\eg\ \emph{Attribution}, \emph{Contrast}) denote the coherence relations and the edge labels reflect the nuclearity of the child span. Below the tree, we show the sequence of splitting decisions  when EDUs are provided and  when EDUs are not provided (end-to-end parsing). The \textbf{bold} splitting decision represents the final split of the span, forming an EDU.
} 
\label{fig:Discourse2SplittingFormat}
\end{figure*}

\subsection{Parsing as a Splitting Decision Problem}



We reformulate the discourse parsing problem from \Cref{eq:dicourse_edu_rep} as a sequence of splitting decisions at \emph{token boundaries} (instead of EDUs). Specifically, the input text is first prepended and appended with the special start (\texttt{{sod}}) and end (\texttt{{eod}}) tokens, respectively. We define the token-boundary as the indexed position between two consecutive tokens. For example, the constituent spanning ``But he added :'' in \Cref{fig:Token-Boundary_Convert_Docs} is defined as . 


\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.45\textwidth]{imgs/Token-Boundary_Doc_v2.png}
\end{center}
\caption{Relation between token-boundary (above) and token (below) representations. A token-boundary position  is located between the tokens at  and .} 
\label{fig:Token-Boundary_Convert_Docs}
\end{figure}




Following the standard practice, we convert the discourse tree by transforming each multi-nuclear constituent into a hierarchical right-branching binary sub-tree. Every internal node in the resulting binary tree will have a left and a right constituent, allowing us to represent it by its split into the left and right children. Based on this, we define the parsing as a set of splitting decisions  at token-boundaries by the following proposition:
\begin{prop}
\label{prop:2} 
Given a binarized discourse tree for a document containing  tokens, the tree can be converted into a set of token-boundary splitting decisions  such that the parent constituent  either gets split into two child constituents  and  for , or forms a terminal EDU unit for , \ie\ the span will not be split further (\ie\ marks segmentation).
\end{prop}


Notice that  is a generalized formulation of RST parsing, which also includes the decoding of EDUs as a special case (). It is quite straight-forward to change this formulation to the parsing scenario, where discourse segmentation (sequence of EDUs) is provided. Formally, in that case, the tree can be converted into a set of splitting decisions  such that the constituent  gets split into two constituents  and  for , \ie\ we simply omit the special case of  as the EDUs are given. In other words, in our generalized formulation, discourse segmentation is just one extra step of parsing, and can be done top-down end-to-end. 







An example of our formalism of the parsing problem is shown in \Cref{fig:Discourse2SplittingFormat} for a discourse tree spanning over two sentences {(44 tokens)}; for simplicity, we do not show the relation labels corresponding to the splitting decisions (marked by~). Since each splitting decision corresponds to one and only one internal node in the tree, it guarantees that the transformation from the tree to  (and ) has a one-to-one mapping. Therefore, predicting the sequence of such splitting decisions is equivalent to predicting the discourse tree (DT).   


\begin{figure*}[t!]
\centering
\includegraphics[width=0.8\textwidth]{imgs/basemodel_architecture_v2.png}
\caption{\small Our discourse parser along with a few decoding steps for a given document. The input to the decoder at each step is the representation of the span to be split. We predict the splitting point using the biaffine function between the corresponding decoder state and the token-boundary encoder representations. The figure is for end-to-end parsing, where each EDU-corresponding span points to its right edge to mark the EDU. The coherence relations between the left and right spans are assigned using a label classifier after the (approximately) optimal tree structure is formed using beam search.}
\label{fig:discourse_parsing_base_architecture}
\end{figure*}


\paragraph{Seq2Seq Parsing Model.}

In this work, we adopt a structure-then-label framework. Specifically, we factorize the probability of a DT into the probability of the tree structure and the probability of the relations (\ie\ the node labels) as follows:

\small 

\normalsize

\noindent where  is the input document, and  and  respectively denote the structure and labels of the DT. This formulation allows us to first infer the best tree structure (\eg\ using beam search), and then find the corresponding labels. 

As discussed, we consider the structure prediction problem as a sequence  of splitting decisions to generate the tree in a top-down manner. We use a seq2seq pointer network \citep{VinyalsNIPS2015} to model the sequence of splitting decisions (\Cref{fig:discourse_parsing_base_architecture}). We adopt a depth-first order of the decision sequence, which showed more consistent performance in our preliminary experiments than other alternatives, such as breath-first order. 


First, we encode the tokens in a document ) with a document encoder and get the token-boundary representations (). Then, at each decoding step , the model takes as input an internal node , and produces an output  (by pointing to the token boundaries) that represents the splitting decision  to split it into two child constituents  and . For example, the initial span  in  \Cref{fig:Discourse2SplittingFormat}  is split at boundary position , yielding two child spans  and . If the span  is given as an EDU (\ie\  segmentation given), the splitting stops at , thus omitted in  (\Cref{fig:Discourse2SplittingFormat}). Otherwise, an extra decision  needs to be made to mark the EDUs for end-to-end parsing. With this,
the probability of  can be expressed as:

\small 

\normalsize

\noindent This end-to-end conditional splitting formulation is the main novelty of our method and is in  contrast to previous approaches which rely on offline-inferred EDUs from a separate discourse segmenter. Our formalism streamlines the overall parsing process, unifies the neural components seamlessly and smoothens the training process. 

\subsection{Model Architecture} 

In the following, we describe the components of our parsing model: the document encoder, the boundary and span representations, the decoding process through the decoder and the label classifier.

\paragraph{Document Encoder.}
Given an input document of  words , we first add \texttt{{sod}} and \texttt{{eod}} markers to the  sequence. After that, each token  in the sequence is mapped into its dense vector representation  as: , where , and  are respectively the character and word embeddings of token .  For word embedding, we experiment with \Ni randomly initialized, \Nii pretrained static embeddings ,\eg\ GloVe \citep{pennington2014glove}). To represent the character embedding of a token, we apply a character bidirectional LSTM \ie\ Bi-LSTM \citep{Hochreiter:1997} or pretrained contextualized embeddings, \eg\ XLNet \citep{NEURIPS2019_dc6a7e65}.  The token representations are then passed to a sequence encoder of a three-layer Bi-LSTM  to obtain their forward  and backward  contextual representations. 

\paragraph{Token-boundary Span Representations.}

To represent each token-boundary position  between token positions  and , we use the fencepost representation \citep{cross-huang-2016-span}:

where  and  are the forward and backward LSTM hidden vectors of positions  and  respectively, and  is the concatenation operation. 

Then, to represent the token-boundary span , we use the linear combination of the two endpoints  and  as: 

where  and  are trainable weights. These span representations will be used as input to the decoder or the label classifier. \Cref{fig:boundary_span_representation} illustrates an example boundary span representation.

\begin{figure}[t!]
\centering
\includegraphics[width=0.4\textwidth]{imgs/fencepost_representation_document_v2.png}
\caption{\small Illustration of token-boundary span encoder. The figure lays out an example representation for the boundary at  and the representation of the token-boundary span , which corresponds to the whole document.}
\label{fig:boundary_span_representation}
\end{figure}

\paragraph{The Decoder.}

Our model uses a unidirectional LSTM as the decoder. At each decoding step , the decoder takes as input the corresponding span  (\ie\ ) and its previous LSTM state  to generate the current state  and then the biaffine function \citep{DozatM17} is applied between  and \emph{all} the encoded token-boundary representations  as follows:

\small 

\normalsize

\noindent where each  operation comprises a linear transformation with LeakyReLU activation \citep{Maas13rectifiernonlinearities} to transform  and  into equal-sized vectors , and  and  are respectively the weight matrix and weight vector for the biaffine function. The resulting biaffine scores  are then fed into a \emph{softmax} layer to acquire the pointing distribution  for the splitting decision.
During inference, when decoding the tree at step , we only examine the ``valid'' splitting points between  and , and we look for  such that . 


\paragraph{Label Classifier.}

We perform label assignment after decoding the entire tree structure. Each assignment takes into account the splitting decision that generated it since the label represents the relation between the child spans.
Specifically, for a constituent  that was split into two child constituents  and , we determine the coherence relation between them as follows:


\small 

\normalsize

\noindent where  is the total number of labels (\ie\ coherence relations with nuclearity attached); each of  and  includes a linear transformation with LeakyReLU activation to transform the left and right spans into equal-sized vectors ;  are the weights and  is a bias vector. 





\paragraph{Training Objective.}
Our parsing model is trained by minimizing the total loss defined as:

where structure  and label  losses are cross-entropy losses computed for the splitting and labeling tasks respectively, and ,  and  denote the encoder, decoder and labeling parameters. 

\subsection{Complete Discourse Parsing Models}\label{sec:complete_models}
Having presented the generic framework, we now describe how it can be easily adapted to the two parsing scenarios: \Ni end-to-end parsing and \Nii parsing with EDUs. We also describe the incorporation of beam search for inference.


\paragraph{End-to-End Parsing.} As mentioned, previous work for end-to-end parsing assumes a separate segmenter that provides EDU-segmented texts to the parser. Our method, however, is an end-to-end framework that produces both the EDUs as well as the parse tree in the same inference process. To guide the search better, we incorporate an inductive bias into our inference based on the finding that most sentences have a well-formed subtree in the document-level tree \citep{soricut-marcu-2003-sentence}, \ie\ discourse structure tends to align with the text structure (sentence boundary in this case); for example,  \citet{fisher-roark-2007-utility,joty-carenini-ng-mehdad-acl-13} found that more than  of the sentences have a well-formed subtree in the RST discourse treebank. 


Our goal is to ensure that each sentence corresponds to an internal node in the tree. This can be achieved by a simple adjustment in our inference. When decoding at time step  with the span  as input, if the span contains  sentence boundaries within it, we pick the one that has the highest pointing score (Eq. \ref{eq:decoder_pointing}) among the  alternatives as the split point . If there is no sentence boundary within the input span (), we find the next split point as usual. In other words, sentence boundaries in a document get the chance to be split before the token boundaries inside a sentence. This constraint is indeed similar to the 1S-1S (1 subtree for 1 sentence) constraint of \citet{joty-carenini-ng-mehdad-acl-13}'s bottom-up parsing, and is also consistent with the property that EDUs are always within the sentence boundary. Algorithm \ref{alg:end-to-end-parsing} illustrate the end-to-end inference algorithm.



\begin{algorithm}[t!]
\scriptsize
\captionsetup{font=scriptsize}
    \caption{Discourse Tree Inference (end-to-end)}
    \label{alg2}
    \begin{algorithmic}
    \REQUIRE Document length ; boundary encoder states: ; sentence boundary set  ;  label scores: , , initial decoder state .
    \ENSURE Parse tree 
\STATE   \algorithmiccomment{stack of spans}
    \STATE 
    \WHILE {}
        \STATE  \algorithmiccomment{Current span to split}
        \STATE  \algorithmiccomment{: split prob. dist.}
         \IF{} 
            \STATE 
        \ELSE
            \STATE 
        \ENDIF
        
                \IF{}    
                    \STATE 
                    \STATE 
                \ELSIF{}    
                    \STATE 
                \ELSIF{}    
                    \STATE 
                \ENDIF
                \IF{}    
                    \STATE 
                \ENDIF
    \ENDWHILE
    \STATE 
  \end{algorithmic}
\label{alg:end-to-end-parsing}
\end{algorithm}

\paragraph{Parsing with EDUs.} When segmentation information is provided, we can have a better encoding of the EDUs to construct the tree. Specifically, rather than simply taking the token-boundary representation corresponding to the EDU boundary as the EDU representation, we adopt a hierarchical approach, where we add another Bi-LSTM layer (called ``Boundary LSTM'') that connects EDU boundaries (a figure of this framework is in the Appendix). In other words, the input sequence to this LSTM layer is , where ,  and  such that  is an EDU boundary. For instance, for the example in \Cref{fig:Discourse2SplittingFormat}, the input to the Boundary LSTM layer is . 

This hierarchical representation facilitates better modeling of relations between EDUs and higher order spans, and can capture long-range dependencies better, especially for long documents.   



\paragraph{Incorporating Beam Search.} 

Previous work \citep{lin-etal-2019-unified, zhang-etal-2020-top} which also uses a seq2seq architecture, computes the pointing scores over the token or span representations only within the input span. For example, for an input span , the pointing scores are computed considering only  as opposed to  in our Eq. \ref{eq:decoder_pointing}. This makes the scales of the scores uneven across different input spans as the lengths of the spans vary. Thus, such scores cannot be objectively compared across sub-trees globally at the full-tree level. In addition, since efficient global search methods like beam search cannot be applied properly with non-uniform scores, these previous methods had to remain greedy at each decoding step. In contrast, our decoder points to all the encoded token-boundary representations in every step (Eq. \ref{eq:decoder_pointing}). This ensures that the pointing scores are evenly scaled, allowing fair comparisons between the scores of all candidate sub-trees. Therefore, our method enables the effective use of beam search through highly probable candidate trees. Algorithm \ref{alg:parsing-with-EDUs} illustrates the beam search inference when EDUs are given. 



\begin{algorithm}[t!]
\scriptsize
\captionsetup{font=scriptsize}
    \caption{Discourse Tree Inference with Beam Search (with gold EDUs)}
    \label{alg1}
    \begin{algorithmic}
    \REQUIRE Number of EDUs in document ; beam width ; EDU boundary-based encoder states: ; label scores: , , initial decoder state .
    \ENSURE Parse tree 
\STATE  \algorithmiccomment{Decoding length}
    \STATE {beam} = array of  items            \algorithmiccomment{List of empty beam items}
    \STATE init\_input\_span \algorithmiccomment{- paddings (0,0)}
    \STATE init\_tree \algorithmiccomment{- elements}
    \STATE beam{[0]} =  \algorithmiccomment{Initialize first item (log-prob,state,input\_span,tree)}
    \FOR {}
        \FOR {} 
            \STATE  \algorithmiccomment{Current span to split}
            \STATE  \algorithmiccomment{: split prob. dist.}
            \FOR {}
                \STATE 
                \STATE 
                \STATE 
                \IF{}    
                    \STATE 
                \ENDIF
                \IF{}    
                    \STATE 
                \ENDIF
                \STATE push ({logp} +  {curr-input-span}, {curr-tree}) to beam[t]
            \ENDFOR
        \ENDFOR
        \STATE prune beam[t] \algorithmiccomment{Keep top- highest score trees}
    \ENDFOR
    \STATE  \algorithmiccomment{: best structure}
    \STATE 
  \end{algorithmic}
\label{alg:parsing-with-EDUs}
\end{algorithm}

