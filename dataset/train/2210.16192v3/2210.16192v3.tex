

\documentclass{article}
\usepackage{waspaa23,amsmath,graphicx,url,times}
\usepackage{color}
\usepackage{multirow}
\usepackage{amssymb}

\usepackage{ifthen}
\lfoot{\ifthenelse{\value{page}=1}{979-8-3503-2372-6/23/\#1#1fgx_{i}\in Xy_{i}\in YAi\in \{1...N\}N{g~\circ~f}fhhg'fi\in I=\{1...2 N\}\boldsymbol{z}_{i}=h(f(A(\boldsymbol{x}_{i})))\in\mathbb{R}^{D_P}{D_P}{P(i)={\{p\in I:{{y}}_p={{y}}_i}\}}ii|P(i)|{N(i)={\{n\in I:{{y}}_n\neq{{y}}_i}\}}i\boldsymbol{\cdot}\tau\in\mathbb{R}^{+*}\mathcal{L}^{M-SCL}Kh_{i}i^{th}\lambda_{i}i^{th}\mathcal{L}_{i}^{SCL}\alphafSpSeSc\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pmTPTNFPFN10^{-4}10^{-3}10^{-4}\tau=0.06\alpha=0.5\lambda_{1}=0.25\lambda_{2}=0.75ScSpSeSc\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pmSeSpScHS\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pmHSScSpSe$) is higher than of CE, especially for M-SCL when leveraging metadata information for learning representations. We did not compare ours results on SPRS to other works other than the baseline, because the few published works report their results by combining both inter and intra sets.

On both datasets, contrastive learning has led to higher score than cross entropy training with high sensitivity. Contrastive learning learns to cluster similar breathings in the latent space while pushing apart dissimilar ones, a desirable property for classification. Metadata provided additional information to be taken into account when learning the representations of breathings. We experimented with sex and age (4 groups in total), and they either boosted sensitivity (on SPRSound) or overall score (on ICBHI). Our approach is distinct from previous work that have used contrastive approaches for respiratory sounds; the work of~\cite{clhls} trained a system on ICBHI to diagnose patients, which is an easier task. \cite{9414385} have used SCL on ICBHI with different sampling of negatives examples, as well as a different cross-validation split, making it difficult to compare. Overall, we have proposed a supervised contrastive approach that exploits metadata in a simple, effective and reproducible way on two datasets. 

\section{Conclusion}
\label{sec:end}

We show in this work the potential of supervised contrastive learning for an imbalanced and noisy setting, outperforming cross-entropy using experiments on respiratory sound classification. We also show that using metadata to combine multiple supervised contrastive tasks for learning useful representations obtain state-of-the-art results.
In future work, we will attempt at building upon the multi-head framework using several pretext tasks such as exploiting spatial or temporal metadata associated with recordings, and investigate data augmentation techniques that better address the variability in the low data regime. Ultimately, such approaches could be adapted to generalize to unseen auditory tasks such as detection or localization, and deal with larger domain shifts.

\section{Acknowledgment}
This work was co-funded by the AI@IMT program of the ANR (French National Research Agency) and the company OSO-AI. We would like to thank our colleagues in the BRAIn Team of the Mathematical and Electrical Engineering Department of Institut Mines-Télécom Atlantique for their insights and feedback.

\bibliographystyle{IEEEtran}
\bibliography{refs23}



\end{sloppy}
\end{document}