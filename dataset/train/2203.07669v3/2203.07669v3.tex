

\documentclass[final]{cvpr}
\usepackage{color}
\usepackage{array}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lipsum}
\usepackage{subfiles}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage[colorlinks, linkcolor=red]{hyperref}
\definecolor{red}{rgb}{1,0,0}
\definecolor{blue}{rgb}{1,0.843,0}
\usepackage[accsupp]{axessibility} \usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}} \renewcommand{\algorithmicensure}{\textbf{Output:}} \newcommand{\xjqi}[1]{\textcolor{red}{{[\textbf{xjqi}: #1]}}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 0.2pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\pagenumbering{gobble}

\def\cvprPaperID{2121} \def\confYear{CVPR 2022}


\definecolor{tp}{HTML}{3BA3EC}
\definecolor{fp}{HTML}{FF6100}

\colorlet{mr}{tp}
\colorlet{ji}{orange!60}
\colorlet{ap}{red!70}

\author{\Large 
Anlin Zheng\textsuperscript{\rm 1,3*},
Yuang Zhang\textsuperscript{\rm 2*},
Xiangyu Zhang\textsuperscript{\rm 1},
Xiaojuan Qi\textsuperscript{\rm 3},
Jian Sun\textsuperscript{\rm 1}\\
\textsuperscript{\rm 1} MEGVII Technology \quad
\textsuperscript{\rm 2}Shanghai Jiao Tong University\quad
\textsuperscript{\rm 3}{University of Hong Kong}
\\
\texttt{\small zyayoung@sjtu.edu.cn, xjqi@eee.hku.hk, \{zhenganlin, zhangxiangyu, sunjian\}@megvii.com}\\
}

\begin{document}

\title{Progressive End-to-End Object Detection in Crowded Scenes}




\maketitle

{\let\thefootnote\relax\footnotetext{* Equal contribution.}}


\begin{abstract}
In this paper, we propose a new query-based detection framework for crowd detection. Previous query-based detectors suffer from two drawbacks: first, multiple predictions will be inferred for a single object, typically in crowded scenes; second, the performance saturates as the depth of the decoding stage increases. Benefiting from the nature of the one-to-one label assignment rule, we propose a progressive predicting method to address the above issues. Specifically, we first select accepted queries prone to generate true positive predictions, then refine the rest noisy queries according to the previously accepted predictions. Experiments show that our method can significantly boost the performance of query-based detectors in crowded scenes. Equipped with our approach, Sparse RCNN achieves 92.0\% , 41.4\%  and 83.2\%  on the challenging CrowdHuman~\cite{shao2018crowdhuman} dataset, outperforming the box-based method MIP~\cite{chu2020detection} that specifies in handling crowded scenarios. Moreover, the proposed method, robust to crowdedness, can still obtain consistent improvements on moderately and slightly crowded datasets like CityPersons~\cite{zhang2017citypersons} and COCO~\cite{lin2014microsoft}. Code will be made publicly available at https://github.com/megvii-model/Iter-E2EDET.







\vspace{-2pt}
\end{abstract}

\section{Introduction}\label{intro}
Crowded object detection is a practical yet challenging research field in computer vision. Many research efforts have been made and achieved impressive progress ~\cite{lu2019semantic, chi2020pedhunter, chi2020relational,zhang2019double,zhang2018occlusion,chu2020detection,iterdet2021,psrcnn,lin2020detr} in the last few decades. However, most of them~\cite{lu2019semantic, chi2020pedhunter, chi2020relational,zhang2019double,zhang2018occlusion,chu2020detection,iterdet2021,psrcnn} require hand-craft components, e.g.\ anchor settings and post-processing, resulted in sub-optimal performance in handling scenes.

\begin{figure}[!t]
\begin{center}
 \includegraphics[width=1.\linewidth]{images/angle_piter.pdf}
\vspace{-7ex}
\end{center}
   \caption{\textcolor{red}{1a}. Sparse RCNN~\cite{sun2020sparse} introduces \textit{false positives} in crowded scenes. \textcolor{red}{1b}. Our approach can remove those \textit{false positives} and ensure each object can be detected only once. Green boxes indicate \emph{true positives} while red ones represent \emph{false positives}.}
\label{fig:visulization}
\vspace{-2ex}
\end{figure}

\begin{figure}
  \footnotesize
  \centering
  \begin{subfigure}{0.49\linewidth}
    \centering
    \definecolor{tp}{HTML}{3BA3EC}
\definecolor{fp}{HTML}{FF6100}

\hspace{-1em}
\begin{tikzpicture}
  \scriptsize
  \begin{axis}[
      xlabel={score},
      ylabel=ratio of ratio,
      ybar, axis on top,
      height=4.5cm, width=1.2\linewidth,
      bar width=0.18cm,
      ybar=0,
      ymajorgrids,
      tick align=inside,
      enlarge y limits={value=.1,upper},
      enlarge x limits=.1,
      ymin=0, ymax=0.5,
      axis x line*=bottom,
      axis y line*=left,
      y axis line style={opacity=0},
      major tick length=2pt,
      ytick style={draw=none},
      legend style={
          draw=none,
          fill=none,
          at={(0.5, 0.675)},
          inner sep=1.2,
          anchor=north,
          legend columns=-1,
          /tikz/every even column/.append style={column sep=0.1cm}
        },
      legend image code/.code={\draw [#1, draw=none] (0cm,-0.05cm) rectangle (0.18cm,0.08cm); },
      x label style={at={(axis description cs:.5,-0.004cm)},anchor=north},
      ylabel near ticks,
xtick={0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0},
      xticklabels={.2, .3, .4, .5, .6, .7, .8, .9, 1},
      ytick={.00, .05, .10, .15, .20, .25, .30},
      yticklabels={.00, .05, .10, .15, .20, .25, .30},
    ]
    \addplot [draw=none, fill=tp] coordinates {
        (0.25, 0.0364746622584506)
        (0.35, 0.0435073818617112)
        (0.45, 0.0463361454711280)
        (0.55, 0.0477042125064128)
        (0.65, 0.0497135609644872)
        (0.75, 0.0531479792509833)
        (0.85, 0.0740395029356438)
        (0.95, 0.2959656273157384) };
    \addplot [draw=none,fill=fp] coordinates {
        (0.25, 0.2390412130194379)
        (0.35, 0.0692370176138630)
        (0.45, 0.0240480533546143)
        (0.55, 0.0103745083509091)
        (0.65, 0.0052300062703072)
        (0.75, 0.0025508749928746)
        (0.85, 0.0013466909878584)
        (0.95, 0.0012825628455794) };

    \legend{\emph{TP}, \emph{FP}}
  \end{axis}
  \begin{axis}[
      ybar, axis on top,
      height=4.5cm, width=1.2\linewidth,
      bar width=0.18cm,
      ybar=0,
      ymajorgrids,
      tick align=inside,
      enlarge y limits={value=.1,upper},
      enlarge x limits=.1,
      ymin=-0.3, ymax=0.03,
      axis x line*=top,
      axis y line*=left,
      xlabel={absolute relative improvement},
      x axis line style={opacity=0},
      y axis line style={opacity=0},
      major tick length=2pt,
      xtick style={draw=none},
      ytick style={draw=none},
      x label style={at={(axis description cs:.5,.9)},anchor=south},
      ylabel near ticks,
      ytick={-.03, .00, .03},
      yticklabels={-.03, .00, .03},
      xtick={},
      xticklabels={},
    ]
    \addplot [draw=none, fill=tp] coordinates {
        (0.25, 0.035896917005-0.0364746622584506)
        (0.35, 0.041142910514-0.0435073818617112)
        (0.45, 0.045275881661-0.0463361454711280)
        (0.55, 0.049214268479-0.0477042125064128)
        (0.65, 0.046256586679-0.0497135609644872)
        (0.75, 0.075872321546-0.0531479792509833)
        (0.85, 0.090396095860-0.0740395029356438)
        (0.95, 0.326738221810-0.2959656273157384)};
    \addplot [draw=none, fill=fp] coordinates {
        (0.25, 0.190474707928-0.2390412130194379)
        (0.35, 0.056849757547-0.0692370176138630)
        (0.45, 0.021427626304-0.0240480533546143)
        (0.55, 0.009923800776-0.0103745083509091)
        (0.65, 0.004164104639-0.0052300062703072)
        (0.75, 0.003448034309-0.0025508749928746)
        (0.85, 0.001875792931-0.0013466909878584)
        (0.95, 0.001042972003-0.0012825628455794)};
  \end{axis}
  \draw[draw=black] (0, 2.414) -- (3.32, 2.414);
\end{tikzpicture}
     \vspace{-4ex}
    \caption{}
    \label{pred_distribution}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\linewidth}
    \centering
    \definecolor{tp}{HTML}{3BA3EC}
\definecolor{fp}{HTML}{FF6100}

\hspace{-1em}
\begin{tikzpicture}
  \scriptsize
  \begin{axis}[
      xlabel={\#false positives},
      ylabel={\#trues positives},
      axis on top,
      height=4.35cm, width=1.2\linewidth,
      tick align=inside,
      enlarge y limits=0,
      enlarge x limits=0,
      axis x line*=bottom,
      axis y line*=left,
      ymin=0, ymax=72000,
      xmin=0, xmax=1000,
      major tick length=2pt,
      legend style={
          draw=none,
          fill=none,
          at={(0.65, 1.06)},
          inner sep=1.2,
          anchor=north,
          legend columns=-1,
          /tikz/every even column/.append style={column sep=0.1cm}
        },
      legend image code/.code={
          \draw[mark repeat=2,mark phase=2]
          plot coordinates {
              (0cm,0cm)
              (0.15cm,0cm)        (0.3cm,0cm)         };},
x label style={at={(axis description cs:.5,-0.004cm)},anchor=north},
      y label style={at={(axis description cs:-0.05,.5)},anchor=south},
    ]
    \addplot [color=tp] plot coordinates { (1, 149) (3, 1513) (5, 2904) (7, 4012) (9, 5385) (11, 6328) (13, 7853) (15, 10120) (17, 10638) (19, 11795) (21, 13238) (23, 14265) (25, 14678) (27, 15152) (29, 15820) (31, 16830) (33, 17597) (35, 18202) (37, 18523) (39, 19668) (41, 20733) (43, 21417) (45, 23526) (47, 23826) (49, 24402) (51, 24976) (53, 25304) (55, 25718) (57, 26360) (59, 26553) (61, 27105) (63, 27276) (65, 27702) (67, 27975) (69, 28155) (71, 28508) (73, 28675) (75, 28885) (77, 29314) (79, 29662) (81, 30033) (83, 30517) (85, 31075) (87, 31612) (89, 32178) (91, 32602) (93, 32711) (95, 32969) (97, 33240) (99, 33491) (101, 33619) (103, 33844) (105, 34168) (107, 34514) (109, 34950) (111, 35082) (113, 35693) (115, 35739) (117, 35945) (119, 36194) (121, 36720) (123, 36858) (125, 36899) (127, 37062) (129, 37177) (131, 37391) (133, 37450) (135, 37589) (137, 37755) (139, 37773) (141, 37892) (143, 38210) (145, 38325) (147, 38678) (149, 38978) (151, 39039) (153, 39216) (155, 39403) (157, 39609) (159, 39762) (161, 39936) (163, 40194) (165, 40280) (167, 40378) (169, 40424) (171, 40603) (173, 41002) (175, 41149) (177, 41206) (179, 41413) (181, 41577) (183, 41775) (185, 41942) (187, 42147) (189, 42202) (191, 42350) (193, 42427) (195, 42559) (197, 42791) (199, 42993) (201, 43274) (203, 43404) (205, 43460) (207, 43593) (209, 43633) (211, 43694) (213, 43901) (215, 44170) (217, 44351) (219, 44387) (221, 44549) (223, 44600) (225, 44936) (227, 44936) (229, 45219) (231, 45521) (233, 45597) (235, 45746) (237, 45851) (239, 45909) (241, 46045) (243, 46107) (245, 46206) (247, 46244) (249, 46325) (251, 46363) (253, 46504) (255, 46565) (257, 46597) (259, 46761) (261, 46966) (263, 47034) (265, 47184) (267, 47309) (269, 47558) (271, 47809) (273, 47860) (275, 47887) (277, 47955) (279, 48094) (281, 48130) (283, 48247) (285, 48306) (287, 48389) (289, 48460) (291, 48622) (293, 48725) (295, 48908) (297, 48913) (299, 48996) (301, 49038) (303, 49129) (305, 49355) (307, 49415) (309, 49521) (311, 49589) (313, 49678) (315, 49734) (317, 49780) (319, 49860) (321, 49895) (323, 49953) (325, 49968) (327, 50044) (329, 50108) (331, 50429) (333, 50551) (335, 50751) (337, 50800) (339, 50839) (341, 50943) (343, 51021) (345, 51043) (347, 51095) (349, 51135) (351, 51248) (353, 51293) (355, 51306) (357, 51381) (359, 51458) (361, 51530) (363, 51569) (365, 51736) (367, 51784) (369, 51927) (371, 51949) (373, 51964) (375, 52013) (377, 52067) (379, 52079) (381, 52111) (383, 52207) (385, 52217) (387, 52252) (389, 52308) (391, 52358) (393, 52395) (395, 52437) (397, 52494) (399, 52567) (401, 52590) (403, 52623) (405, 52697) (407, 52751) (409, 52864) (411, 52987) (413, 53036) (415, 53139) (417, 53166) (419, 53250) (421, 53266) (423, 53344) (425, 53369) (427, 53402) (429, 53472) (431, 53522) (433, 53536) (435, 53612) (437, 53650) (439, 53751) (441, 53763) (443, 53783) (445, 53892) (447, 53967) (449, 54001) (451, 54026) (453, 54035) (455, 54060) (457, 54125) (459, 54214) (461, 54224) (463, 54243) (465, 54395) (467, 54435) (469, 54465) (471, 54570) (473, 54633) (475, 54667) (477, 54687) (479, 54699) (481, 54822) (483, 54879) (485, 54920) (487, 54925) (489, 54982) (491, 54991) (493, 55031) (495, 55083) (497, 55119) (499, 55132) (501, 55189) (503, 55218) (505, 55244) (507, 55279) (509, 55339) (511, 55450) (513, 55507) (515, 55588) (517, 55648) (519, 55673) (521, 55698) (523, 55713) (525, 55744) (527, 55774) (529, 55834) (531, 55896) (533, 55926) (535, 56000) (537, 56041) (539, 56074) (541, 56135) (543, 56152) (545, 56230) (547, 56232) (549, 56286) (551, 56359) (553, 56366) (555, 56385) (557, 56407) (559, 56426) (561, 56459) (563, 56507) (565, 56520) (567, 56573) (569, 56648) (571, 56673) (573, 56748) (575, 56781) (577, 56792) (579, 56847) (581, 56889) (583, 56920) (585, 56974) (587, 57001) (589, 57019) (591, 57094) (593, 57156) (595, 57174) (597, 57189) (599, 57215) (601, 57237) (603, 57373) (605, 57387) (607, 57428) (609, 57456) (611, 57473) (613, 57495) (615, 57530) (617, 57536) (619, 57623) (621, 57651) (623, 57732) (625, 57751) (627, 57789) (629, 57801) (631, 57826) (633, 57835) (635, 57845) (637, 57862) (639, 57883) (641, 57891) (643, 57976) (645, 58054) (647, 58082) (649, 58090) (651, 58103) (653, 58109) (655, 58121) (657, 58158) (659, 58177) (661, 58204) (663, 58247) (665, 58281) (667, 58320) (669, 58370) (671, 58381) (673, 58388) (675, 58483) (677, 58500) (679, 58604) (681, 58620) (683, 58650) (685, 58670) (687, 58696) (689, 58703) (691, 58797) (693, 58818) (695, 58875) (697, 58919) (699, 58961) (701, 58969) (703, 59073) (705, 59086) (707, 59104) (709, 59111) (711, 59138) (713, 59149) (715, 59212) (717, 59238) (719, 59305) (721, 59355) (723, 59370) (725, 59377) (727, 59383) (729, 59402) (731, 59488) (733, 59495) (735, 59527) (737, 59568) (739, 59576) (741, 59619) (743, 59629) (745, 59638) (747, 59773) (749, 59775) (751, 59782) (753, 59812) (755, 59824) (757, 59864) (759, 59890) (761, 59912) (763, 59995) (765, 60026) (767, 60034) (769, 60037) (771, 60048) (773, 60060) (775, 60108) (777, 60164) (779, 60178) (781, 60182) (783, 60192) (785, 60242) (787, 60252) (789, 60267) (791, 60297) (793, 60308) (795, 60319) (797, 60346) (799, 60365) (801, 60379) (803, 60400) (805, 60419) (807, 60451) (809, 60480) (811, 60493) (813, 60501) (815, 60503) (817, 60523) (819, 60558) (821, 60569) (823, 60590) (825, 60596) (827, 60629) (829, 60634) (831, 60653) (833, 60741) (835, 60768) (837, 60784) (839, 60792) (841, 60865) (843, 60908) (845, 60920) (847, 60945) (849, 61022) (851, 61046) (853, 61052) (855, 61059) (857, 61094) (859, 61152) (861, 61168) (863, 61174) (865, 61175) (867, 61221) (869, 61236) (871, 61263) (873, 61266) (875, 61306) (877, 61367) (879, 61380) (881, 61393) (883, 61417) (885, 61452) (887, 61472) (889, 61480) (891, 61497) (893, 61544) (895, 61575) (897, 61587) (899, 61597) (901, 61634) (903, 61646) (905, 61655) (907, 61674) (909, 61712) (911, 61722) (913, 61771) (915, 61776) (917, 61792) (919, 61816) (921, 61825) (923, 61872) (925, 61875) (927, 61890) (929, 61898) (931, 61913) (933, 61927) (935, 61988) (937, 62034) (939, 62052) (941, 62090) (943, 62132) (945, 62146) (947, 62151) (949, 62175) (951, 62224) (953, 62251) (955, 62305) (957, 62310) (959, 62321) (961, 62363) (963, 62366) (965, 62385) (967, 62398) (969, 62444) (971, 62455) (973, 62472) (975, 62483) (977, 62519) (979, 62535) (981, 62559) (983, 62595) (985, 62618) (987, 62624) (989, 62683) (991, 62690) (993, 62739) (995, 62759) (997, 62762) (999, 62781) };
    \addplot [color=fp] plot coordinates { (1, 1278) (3, 2973) (5, 6265) (7, 8218) (9, 9367) (11, 13734) (13, 13804) (15, 16137) (17, 16590) (19, 16907) (21, 18176) (23, 18739) (25, 19800) (27, 20925) (29, 21084) (31, 21590) (33, 21952) (35, 22826) (37, 23095) (39, 23899) (41, 23998) (43, 24859) (45, 25139) (47, 25611) (49, 26110) (51, 26596) (53, 26851) (55, 27391) (57, 28899) (59, 29706) (61, 29790) (63, 30050) (65, 30256) (67, 30573) (69, 31231) (71, 32360) (73, 32452) (75, 32834) (77, 33094) (79, 33699) (81, 33943) (83, 34310) (85, 34513) (87, 34754) (89, 34988) (91, 35627) (93, 35780) (95, 36055) (97, 36170) (99, 36675) (101, 37027) (103, 37698) (105, 37731) (107, 37780) (109, 38190) (111, 38360) (113, 38527) (115, 38715) (117, 39098) (119, 40238) (121, 40426) (123, 40640) (125, 41002) (127, 41264) (129, 41290) (131, 41311) (133, 41763) (135, 42010) (137, 42135) (139, 42195) (141, 42237) (143, 42365) (145, 42395) (147, 42518) (149, 42600) (151, 42895) (153, 42911) (155, 43141) (157, 43192) (159, 43248) (161, 43442) (163, 43789) (165, 43891) (167, 44148) (169, 44275) (171, 44295) (173, 44488) (175, 44520) (177, 44534) (179, 44713) (181, 44743) (183, 45000) (185, 45378) (187, 45614) (189, 45649) (191, 45726) (193, 46022) (195, 46068) (197, 46262) (199, 46352) (201, 46429) (203, 46498) (205, 46682) (207, 46720) (209, 46766) (211, 46782) (213, 46925) (215, 47029) (217, 47279) (219, 47370) (221, 47421) (223, 47518) (225, 47585) (227, 47647) (229, 47729) (231, 47766) (233, 47791) (235, 47895) (237, 47973) (239, 48000) (241, 48131) (243, 48153) (245, 48374) (247, 48406) (249, 48482) (251, 48599) (253, 48629) (255, 48678) (257, 48786) (259, 48807) (261, 48872) (263, 48971) (265, 49061) (267, 49240) (269, 49346) (271, 49436) (273, 49485) (275, 49626) (277, 49645) (279, 49792) (281, 49856) (283, 49914) (285, 50070) (287, 50085) (289, 50182) (291, 50201) (293, 50279) (295, 50301) (297, 50390) (299, 50393) (301, 50395) (303, 50459) (305, 50549) (307, 50626) (309, 50682) (311, 50812) (313, 51058) (315, 51170) (317, 51352) (319, 51426) (321, 51658) (323, 51703) (325, 51832) (327, 51900) (329, 51953) (331, 52102) (333, 52219) (335, 52284) (337, 52442) (339, 52444) (341, 52462) (343, 52487) (345, 52553) (347, 52601) (349, 52646) (351, 52679) (353, 52774) (355, 52834) (357, 52859) (359, 52901) (361, 52909) (363, 52953) (365, 52969) (367, 53313) (369, 53367) (371, 53382) (373, 53455) (375, 53515) (377, 53623) (379, 53695) (381, 53743) (383, 53797) (385, 53855) (387, 53926) (389, 53951) (391, 53999) (393, 54010) (395, 54017) (397, 54064) (399, 54099) (401, 54233) (403, 54251) (405, 54291) (407, 54339) (409, 54369) (411, 54380) (413, 54512) (415, 54571) (417, 54626) (419, 54719) (421, 54828) (423, 54975) (425, 55017) (427, 55073) (429, 55190) (431, 55258) (433, 55297) (435, 55447) (437, 55485) (439, 55534) (441, 55566) (443, 55729) (445, 55773) (447, 55787) (449, 55823) (451, 55846) (453, 55864) (455, 55959) (457, 55987) (459, 55994) (461, 55995) (463, 56075) (465, 56082) (467, 56157) (469, 56204) (471, 56241) (473, 56291) (475, 56456) (477, 56467) (479, 56483) (481, 56498) (483, 56643) (485, 56692) (487, 56759) (489, 56846) (491, 56867) (493, 56876) (495, 56939) (497, 57093) (499, 57146) (501, 57163) (503, 57210) (505, 57232) (507, 57269) (509, 57321) (511, 57353) (513, 57402) (515, 57452) (517, 57525) (519, 57564) (521, 57651) (523, 57683) (525, 57747) (527, 57759) (529, 57769) (531, 57862) (533, 57878) (535, 57898) (537, 57994) (539, 58001) (541, 58047) (543, 58073) (545, 58118) (547, 58142) (549, 58167) (551, 58269) (553, 58285) (555, 58300) (557, 58367) (559, 58372) (561, 58402) (563, 58447) (565, 58470) (567, 58477) (569, 58510) (571, 58542) (573, 58585) (575, 58614) (577, 58645) (579, 58705) (581, 58774) (583, 58829) (585, 58865) (587, 58912) (589, 58989) (591, 59001) (593, 59046) (595, 59142) (597, 59203) (599, 59228) (601, 59330) (603, 59366) (605, 59415) (607, 59442) (609, 59479) (611, 59501) (613, 59637) (615, 59668) (617, 59709) (619, 59728) (621, 59741) (623, 59792) (625, 59837) (627, 59929) (629, 59954) (631, 59981) (633, 59982) (635, 60068) (637, 60111) (639, 60158) (641, 60222) (643, 60250) (645, 60300) (647, 60316) (649, 60366) (651, 60411) (653, 60443) (655, 60484) (657, 60530) (659, 60591) (661, 60611) (663, 60699) (665, 60732) (667, 60754) (669, 60773) (671, 60803) (673, 60836) (675, 60904) (677, 60975) (679, 61014) (681, 61016) (683, 61031) (685, 61047) (687, 61091) (689, 61120) (691, 61265) (693, 61291) (695, 61315) (697, 61340) (699, 61350) (701, 61354) (703, 61359) (705, 61364) (707, 61388) (709, 61460) (711, 61534) (713, 61540) (715, 61585) (717, 61627) (719, 61648) (721, 61666) (723, 61696) (725, 61739) (727, 61746) (729, 61811) (731, 61870) (733, 61899) (735, 61946) (737, 61973) (739, 61999) (741, 62060) (743, 62066) (745, 62081) (747, 62110) (749, 62112) (751, 62121) (753, 62145) (755, 62167) (757, 62202) (759, 62235) (761, 62250) (763, 62262) (765, 62314) (767, 62325) (769, 62418) (771, 62426) (773, 62433) (775, 62442) (777, 62513) (779, 62535) (781, 62616) (783, 62646) (785, 62670) (787, 62678) (789, 62726) (791, 62743) (793, 62776) (795, 62851) (797, 62883) (799, 62902) (801, 62915) (803, 63020) (805, 63036) (807, 63053) (809, 63141) (811, 63147) (813, 63223) (815, 63261) (817, 63305) (819, 63359) (821, 63386) (823, 63401) (825, 63430) (827, 63459) (829, 63490) (831, 63512) (833, 63552) (835, 63573) (837, 63599) (839, 63645) (841, 63668) (843, 63699) (845, 63749) (847, 63769) (849, 63815) (851, 63848) (853, 63863) (855, 63886) (857, 63898) (859, 63927) (861, 63952) (863, 64012) (865, 64064) (867, 64090) (869, 64111) (871, 64161) (873, 64175) (875, 64207) (877, 64219) (879, 64237) (881, 64268) (883, 64298) (885, 64320) (887, 64367) (889, 64389) (891, 64419) (893, 64454) (895, 64464) (897, 64477) (899, 64517) (901, 64559) (903, 64590) (905, 64649) (907, 64693) (909, 64712) (911, 64742) (913, 64751) (915, 64762) (917, 64805) (919, 64832) (921, 64871) (923, 64883) (925, 64924) (927, 64972) (929, 65011) (931, 65038) (933, 65047) (935, 65065) (937, 65076) (939, 65119) (941, 65126) (943, 65134) (945, 65141) (947, 65156) (949, 65175) (951, 65181) (953, 65236) (955, 65260) (957, 65271) (959, 65298) (961, 65302) (963, 65325) (965, 65334) (967, 65345) (969, 65381) (971, 65397) (973, 65431) (975, 65434) (977, 65476) (979, 65549) (981, 65594) (983, 65623) (985, 65654) (987, 65659) (989, 65689) (991, 65699) (993, 65713) (995, 65739) (997, 65805) (999, 65837) };

    \legend{\emph{Sparse R-CNN}, \emph{Ours}}
  \end{axis}
\end{tikzpicture}
     \vspace{-4ex}
    \caption{}
    \label{tp-fp_curve}
  \end{subfigure}
\vspace{-2ex}
\caption{~\ref{pred_distribution}. The bottom histogram describes the prediction distribution of Sparse RCNN~\cite{sun2020sparse} under different confidence scores, while the top one reflects the absolute improvements achieved by our approach compared with Sparse RCNN~\cite{sun2020sparse}. ~\ref{tp-fp_curve}. The  curve when computing Average Precision (AP).}
\vspace{-1pc}
\label{fig:histogram}
\end{figure}


Recently, Carion et al. ~\cite{carion2020end} proposed a  fully end-to-end object detection framework DETR, which introduces learnable queries to represent objects and achieves competitive performance without any post-processing. It can be categorized as a \emph{query-based} approach to differentiate from the \emph{box-based}~\cite{lin2020focal, lin2017feature,2020atss} and \emph{point-based}~\cite{tian2019fcos, wang2020end} methods. Following DETR~\cite{carion2020end}, Sparse RCNN~\cite{sun2020sparse} ensures object queries interact with local feature of Region of Interest (RoI), while deformable DETR~\cite{zhu2021deformable} proposes attention modules that only attend to a small set of key sampling points. They further improve the detection accuracy and mitigate several issues occurred in DETR: slow convergence and high computational overhead.






The above success inspires us to study \emph{query-based} object detection methods in crowded scenes, aiming at designing a more sophisticated end-to-end detection framework. Although these ~\emph{query-based} approaches~\cite{gossipnet, zhu2021deformable} can obtain significant results on the slightly crowded datasets like COCO ~\cite{lin2014microsoft}, our initial studies show they suffer from several unresolved challenges in crowded scenes:(1). the \emph{query-based} detector tends to infer multiple predictions for a single object, with \emph{false positives} introduced. Figure.~\ref{fig:visulization}\textcolor{red}{a} shows a common failure case; (2). The performance of a \emph{query-based} detector becomes saturated or even worse as the depth of decoding stage increases, which is depicted in the Appendix.


\vspace{-0.5cm}
\paragraph{Our motivations.}

Further investigations on the \emph{query-based} method, Sparse RCNN~\cite{sun2020sparse}, yield the following intriguing findings in crowd scenes. 
As described in Figure.~\ref{pred_distribution}, a large percentage of target objects can be accurately predicted by those predictions with high confidence scores (e.g.\ higher than a threshold of 0.7), while containing few false positives. These predictions are more likely to be \emph{true positives} that can be taken as \emph{accepted predictions}. While the rest, where a considerable number of \emph{true positives} and \emph{false positives} exist, can be regarded as \emph{noisy predictions}.  Naturally, if an object is detected by one accepted prediction, there is no need for noisy predictions to detect it again. Hence, \emph{Why not strengthen the discrimination of those noisy predictions given the context of the accepted predictions}? To this end, the noisy queries can `perceive' whether their targets have been detected or not. If so, their confidence scores will be reduced and then filtered out.











\vspace{-0.5cm}
\paragraph{Our contributions}

Motivated by this, we propose a progressive prediction method equipped with a prediction selector, relation information extractor, query updater, and label assignment to improve the performance of query-based object detectors in handling crowded scenes. 

First, we develop a prediction selector to select queries associated with high confidence scores as \emph{accepted queries}, leaving the rest as \emph{noisy queries}. Then, to let the \emph{noisy queries} `perceive' whether their targets have been detected or not, we design a relation extractor for relation modeling between \emph{noisy queries} and their accepted neighbors. Further, a query updater is developed by performing a new local self-attention attending to spatially-related neighbors only. Finally, a new one-to-one label assignment rule is introduced to assign samples among the accepted and refined noisy queries step by step. With the proposed method, the above problems can be well addressed: (1). Each object can be detected only once, which greatly decreases the number of false positives while increasing the number of true positives, as described in Figure.~\ref{fig:visulization}\textcolor{red}{b}; (2). As depicted in Figure.~\ref{tp-fp_curve}, the performance is consistently improved compared with its counterparts~\cite{sun2020sparse,zhu2021deformable} that have the same depth of decoding stage. 






Our method is generic and can be incorporated into multiple architectures~\cite{sun2020sparse, zhu2021deformable}, and delivers significant performance improvements of query-based detectors. Equipped with our approach, Sparse RCNN~\cite{sun2020sparse} with \emph{ResNet-50}~\cite{he2016deep} backbone obtains \textbf{92.0\%} , \textbf{41.4\%}  and \textbf{83.2\%}  on the challenging dataset \emph{CrowdHuman}~\cite{shao2018crowdhuman}, outperforming the box-based method MIP~\cite{chu2020detection}. Besides, deformable DETR~\cite{zhu2021deformable}, equipped with our approach, also achieves \textbf{92.1\%}  and \textbf{84.0\%} . Moreover, our approach works reasonably well for less crowded scenes, e.g.\ the Sparse RCNN with our approach can still obtain \textbf{1.0\%}  and \textbf{1.1\%} AP gains on moderately and slightly crowded datasets \emph{Citypersons}~\cite{zhang2017citypersons} and \emph{COCO}~\cite{lin2014microsoft}, respectively.

\vspace{-0.2cm}
\section{Related works}

\vspace{-0.2cm}
\paragraph{End-to-end object detection.}

RelationNet~\cite{hu2018relation} is one of the pioneering works trying to predict results directly, achieving promising performance compared to their counterparts on several famous benchmarks. DETR~\cite{carion2020end}  introduces learnable queries to represent objects and perform single prediction for each instance directly. Subsequently, deformable-DETR~\cite{zhu2021deformable}  limits the attention field of each query to a local area around the reference points to accelerate the convergence and improve detection performance. Meanwhile, Sparse R-CNN~\cite{sun2020sparse}  utilizes a fixed set of learnable queries to formulate objects instead of a number of proxy representation, e.g.\ anchors. Analogous to deformable DETR, RoIAlign~\cite{he2017mask} is applied to limit the attention field in a local region. Adaptive Clustering Transformer~\cite{act2021} proposes to improve the attention distribution in DETR’s encoder by LSH approximate clustering for convergence acceleration. UP-DETR~\cite{up2021detr} designs a new self-supervised method to improve the convergence speed of DETR, while TSP\cite{sun2020tsp} analyzes the main factors contributing to slow convergence in DETR. SMCA~\cite{fastCdetr} explores a better information interaction mechanism to further accelerate convergence and improve the performance of DETR.

\vspace{-0.5cm}
\paragraph{Object detection in crowded scenes.}

Research community has poured much interest in exploring occlusion problems on pedestrian detection. Specific methods have been proposed to mitigate this problem, including detecting by parts \cite{lu2019semantic, chi2020pedhunter, chi2020relational, zhang2019double,zhang2018occlusion} and improving hand-crafted rules in training target design. Recently, CNN-based methods have dominated the crowded object detection and achieved considerable gains. Several works propose new loss functions to address problems of crowded detection~\cite{wang2017repulsion,zhang2018occlusion}. Besides, the effectiveness of NMS is based on the assumption that multiple instances rarely occur at the same location in an image, which is not true in crowded scenes. But designing duplicate removal for crowded scenes is non-trivial. Soft-NMS \cite{bodla2017soft} and Softer-NMS \cite{he2018softer} replace hard removal of nearby proposals with score decay. Several works propose to use a neural network to simulate the function of NMS for duplicates removal~\cite{gossipnet, qi2018sequential}. Others explore NMS-aware training, including NMS with adaptive threshold \cite{hosang2016a,liu2019adaptive}, feature embedding \cite{salscheider2021featurenms} and multiple prediction with set suppression~\cite{chu2020detection, huang2020nms}, to tackle problem of object detection in crowded scenes.

Recently, PEDR~\cite{lin2020detr} proposes several techniques to improve the performance of \emph{query-based} detectors in coping with crowded detection, which is orthogonal to ours. Their techniques are also applicable to our work.

\vspace{-0.5cm}
\paragraph{Relation modeling for object detection.}
As discussed in ~\cite{hu2018relation}, early works~\cite{divvala, co-occurrent, torralba, auto-context, in_the_wild} use object relations as a post-processing step. The detected objects are re-scored by considering object relationships. For example, co-occurrence, which indicates how likely two object classes can exist in the same image, is used by DPM~\cite{dpm2010} to refine object scores. The subsequent approaches~\cite{a_role_of_context, tree_based} try more complex relation models, by taking additional positions and size into account. These methods achieve moderate success in the pre-deep learning era but do not prove the effectiveness in CNNs. Several recent works perform spatial reasoning ~\cite{acfobjdetection,end2endlstm,sptial_memory, gossipnet} to model object relations. Among them, GossipNet~\cite{gossipnet} and RelationNet~\cite{hu2018relation} are the representative methods. Both share the same spirit of modeling relations among boxes. However, the network of GossipNet~\cite{gossipnet} is complex (depth\textgreater 80) and its computation cost is demanding. Although it allows end-to-end learning in principle, no experimental evidence approves. RelationNet~\cite{hu2018relation} utilized the self-attention for feature interaction and obtained a promising improvement in general object detection. Nevertheless, it doesn't show a promising performance in dealing with crowd scenes~\cite{shao2018crowdhuman}.


Recent works related to ours are PS-RCNN~\cite{psrcnn} and IterDet~\cite{iterdet2021}. They proposed to detect objects according to the previous predicted ones. They need to mask the feature~\cite{psrcnn} or produce a history map~\cite{iterdet2021} to memorize the previous detections, introducing noise while limiting performance improvement~\cite{psrcnn} or incur heavy computation~\cite{iterdet2021}. Even so, both of them need a post-processing method to remove duplicates in every iteration. 

Recent query-based object detectors~\cite{sun2020sparse, zhu2021deformable, act2021, up2021detr, sun2020tsp, fastCdetr} utilized learnable queries to represent objects, and take advantage of the self/cross-attention to model the relations among queries, detecting objects in an end-to-end manner.  Our work inherits the methodology and boosts their performance in heavily, moderately, and slightly crowded scenes.


\section{Methodology}

In this section, we first revisit the query-based object detector, e.g.\ Sparse RCNN~\cite{sun2020sparse} briefly. Next, we illustrate our approach primarily deployed on Sparse RCNN explicitly. Finally, the main differences of detector design will be discussed as follows.

\subsection{Query Based Object Detector}
Our approach can be deployed on most \emph{query-based} object detectors~\cite{carion2020end, sun2020sparse, zhu2021deformable}. To illustrate the proposed method, we choose Sparse RCNN~\cite{sun2020sparse} as our default instantiation. Figure.~\ref{fig:sparse_rcnn_arch} depicts its object detection pipeline, which can also be formulated as:


where  denotes the learnable object query.  and  denote the number and dimension of query , respectively. At stage , an RoIAlign~\cite{he2017mask}  extracts RoI features from FPN features , under the guidance of bounding box  predicted by the previous stage. Meanwhile, a multi-head self-attention module  is applied to the input query  to get the transformed query . Then, a dynamic convolution module  takes both  and  as inputs and performs dynamic convolution to generates  for the next stage. Simultaneously,  is fed into the box prediction branch  for current bounding box prediction , which is the input of the next stage .


\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{images/CSRCNN.v2.pdf}
\caption{The diagram of the proposed progressive end-to-end object detection framework. First, the \emph{prediction selector} select queries associated with high confidence scores as \emph{accepted queries}, leaving the rest as \emph{noisy queries}. Then \emph{Relation information extractor} models the relations between \emph{noisy queries} and their neighbors from accepted predictions. Next, the queries are fed into the \emph{queries updater} to be further refined by performing a new local self-attention.}
\label{fig:framwork}
\vspace{-1.5pc}
\end{figure*}

\subsection{Our Method}
\label{sec:approach_declearation}

As illustrated in Figure.~\ref{fig:framwork}, the proposed progressive predicting method consists of several components: prediction selector, relation information extractor, query updater, and label assignment, which will be introduced in detail next.

\begin{figure}[t]
\centering
\begin{subfigure}{0.49\linewidth}
\centering
\definecolor{brown}{HTML}{843C0C}
\definecolor{darkred}{HTML}{C43C0C}
\definecolor{skyblue}{HTML}{00B0F0}
\begin{tikzpicture}[
    scale=0.9,
    sym/.style={inner sep=1},
    box/.style={rectangle, rounded corners=3, draw=black, thick, minimum size=16, minimum width=48, fill=white},
    roi/.style={rectangle, rounded corners=3, draw=black, thick, minimum size=16, inner sep=0, fill=white},
    rnet/.style={rectangle, minimum size=16, minimum width=18},
    dr/.style={rectangle, rounded corners=3, draw=black, thick, minimum height=16, inner sep=1, fill=white},
    arr/.style={thick, ->, >=stealth, draw=black}
  ]
  \footnotesize
  \setlength{\medmuskip}{1\medmuskip}
  \setlength{\thickmuskip}{1\thickmuskip}

\node[sym]   at (0, 0)       (prev_q)              {};
  \node[sym]   at (0, 1)       (prev_box)            {};
  \node[sym]   at (2.35, 1.75) (box)                 {};
  \node[sym]   at (3.8, 1)     (q)                   {};
  \node[box]   at (2.35, 1)     (dyn_conv)            {DyConv};
  
  \node[sym]   at (0.9, 1.75)      (x)                   {\scriptsize };
  \node[roi]   at (0.9, 1)      (roi)                 {\scriptsize };
  \draw[arr] (x)                -- (roi);

  \node[box]   at (2.35, 0)     (self_attn)           {MSA};

\draw[arr] (prev_box) -- (roi) -- (dyn_conv) -- (box);
  \draw[arr] (prev_q)   -- (self_attn) -- (dyn_conv);
  \draw[arr] (dyn_conv) -- (q);

\end{tikzpicture}
 \vspace{-2ex}
\caption{Sparse R-CNN~\cite{sun2020sparse}.}
\label{fig:sparse_rcnn_arch}
\end{subfigure}
\begin{subfigure}{0.49\linewidth}
\centering
\definecolor{brown}{HTML}{843C0C}
\definecolor{darkred}{HTML}{C43C0C}
\definecolor{skyblue}{HTML}{00B0F0}
\begin{tikzpicture}[
    scale=0.9,
    sym/.style={inner sep=1},
    box/.style={rectangle, rounded corners=3, draw=black, thick, minimum size=16, minimum width=48, fill=white},
    roi/.style={rectangle, rounded corners=3, draw=black, thick, minimum size=16, inner sep=0, fill=white},
    rnet/.style={rectangle, minimum size=16, minimum width=18},
    dr/.style={rectangle, rounded corners=3, draw=black, thick, minimum height=16, inner sep=1, fill=white},
    arr/.style={thick, ->, >=stealth, draw=black}
  ]
  \footnotesize
  \setlength{\medmuskip}{1\medmuskip}
  \setlength{\thickmuskip}{1\thickmuskip}

\node[sym]   at (0, 0)       (prev_q)              {};
  \node[sym]   at (0, 1)       (prev_box)            {};
  \node[sym]   at (2.35, 1.75) (box)                 {};
  \node[sym]   at (3.8, 1)     (q)                   {};
  \node[box]   at (2.35, 1)     (dyn_conv)            {DyConv};
  
  \node[sym]   at (0.9, 1.75)      (x)                   {\scriptsize };
  \node[roi]   at (0.9, 1)      (roi)                 {\scriptsize };
  \draw[arr] (x)                -- (roi);

\node[box]   at (2.35, 0)     (self_attn)           {QU};

  \node[dr, anchor=east] at (.9+.015, 0)  (d)             {};
  \node[dr, anchor=west] at (.9-.015, 0)  (r)             {};
  \node[rnet]            at (.9, 0)  (rnet)              {};

\draw[arr] (prev_box) -- (roi) -- (dyn_conv) -- (box);
  \draw[arr] (prev_q)   -- (rnet) -- (self_attn) -- (dyn_conv);
  \draw[arr] (dyn_conv) -- (q);
  \draw[arr] (prev_box) -- (rnet);

\end{tikzpicture}
 \vspace{-2ex}
\caption{\emph{Our approach}}
\label{fig:sr_sparse_rcnn}
\end{subfigure}
\vspace{-2ex}
\caption{The diagram of decoding stage. --RoIAlignPool~\cite{he2017mask},  -- Dynamic Convolution,  -- Multi-head Self-Attention,  -- Prediction Selector,  -- Relation Information Extractor,  -- Query Updater.}
\vspace{-1pc}
\end{figure}

\vspace{-0.4cm}
\paragraph{Prediction selector.}

For the findings described in Sec.~\ref{intro},  a prediction selector is developed to select those queries prone to generating predictions with high confidence scores as accepted queries, while leaving the rest as noisy ones that need to be further refined. This procedure can be formulated in Equ.\eqref{equ:detection_split}.

where  is the stage number.  denote the whole predictions produced by the whole queries in the previous  stage.  and  indicate the accepted predictions and noisy predictions generated from the accepted and noisy queries, respectively.  and  denote the predicted box and its confidence score, respectively.  is the confidence score threshold.

\vspace{-0.4cm}
\paragraph{Relation information extractor.} 
As mentioned in Sec.~\ref{intro}, a  large percentage of target objects can be accurately predicted by the accepted queries. Therefore, if an object is detected by one accepted prediction, there is no need for noisy predictions to detect it again. In order to equip these noisy queries with the capability of perceiving whether their targets have been detected or not, we develop a relation information extractor to model the relation between the noisy predictions and their accepted neighbors.

The detailed design of the relation information extractor is illustrated in Figure.~\ref{fig:relation_extractor}, with the procedure formulated in Equ.\eqref{equ:connection} as well. For each noisy prediction , we first find their accepted neighbors  in , constructing the spatially-related pairs . Then, the encoded pairs together with the \textit{intersection-over-union (IoU)} between them are fed to a compact network to obtain the geometry relation features . Since the number of accepted neighbors corresponding to each noisy prediction is uncertain. An aggregation function is employed to reduce  to the same feature dimension, while maintaining the permutation-invariance property. In our approach, we use \textit{max} pooling by default. Besides, the pooled geometry features, fused with the transformed query features, are further activated by a non-linear function.

\vspace{-5pt}

where  represents a function that finds neighbors for a box  based on the \textit{IoU}  with a threshold . Here, we use it to find the accepted neighbors in  for the noisy predictions in .  refers to the sine and cosine spatial positional encoding function which is the same as that~\cite{hu2018relation, attnyouneed}. Also,  denotes a function used to generate geometry relation features  from the encoded inputs. The noisy query  corresponds to noisy prediction  in , transformed by the function . The pooled geometry features and transformed query features  are fused through element-wise summation, followed by a function  to produce the desired relation features .

As depicted in Figure.~\ref{fig:relation_extractor},  consists of two consecutive ~\textit{fc} layers with \textit{ReLU}~\cite{maas2013rectifier} activation to increase the  non-linearity. Note that  and  share the same architecture, but are weight-independent. Here, the gradients of  are stopped from back-propagating to the previous stages.


\begin{figure}[t]
\centering
\definecolor{brown}{HTML}{843C0C}
\definecolor{darkred}{HTML}{C43C0C}
\definecolor{skyblue}{HTML}{00B0F0}

\begin{tikzpicture}[
    sym/.style={},
    box/.style={rectangle, rounded corners=3, draw=black, thick, minimum size=16},
    res/.style={circle, draw=black, thick, minimum size=12, inner sep=0, fill=white},
    arr/.style={thick, ->, >=stealth, draw=black}
  ]
  \small
\node[sym, anchor=east]               at (0, 0)       (prev_q)      {};
  \node[sym, anchor=east, align=right]  at (0, 1.5)     (prev_box)    {\\};
  \node[box]                            at (0.6, 1.5)   (eps)         {};
  \node[box]                            at (1.8, 1.5)   (r1)          {\textit{linear+r}};
  \node[box]                            at (3.25, 1.5)  (r2)          {\textit{linear}};
  \node[box]                            at (1.2, 0)     (r3)          {\textit{linear+r}};
  \node[box]                            at (2.85, 0)    (r4)          {\textit{linear}};
  \node[box]                            at (4, 0.75)    (pool)        {\textit{MaxPool}};
  \node[res]                            at (4, 0)       (add1)        {};
\node[box]                            at (5.1, 0)     (r5)          {};


\draw[arr] (prev_box) -- (eps);
  \draw[arr] (eps)      -- (r1);
  \draw[arr] (r1)       -- (r2);
  \draw[arr] (r2)       -- (4, 1.5) -- (pool);
  \draw[arr] (pool)     -- (add1);

  \draw[arr] (prev_q) -- (r3);
  \draw[arr] (r3)     -- (r4);
  \draw[arr] (r4)     -- (add1);
  \draw[arr] (add1)   -- (r5);
\draw[arr] (r5)   -- +(1, 0);

\end{tikzpicture}
 \vspace{0.2cm}
\caption{Relation information extractor .  -- sine and cosine spatial positional encoding function~\cite{hu2018relation,attnyouneed}, \textit{linear} -- \textit{fc} layer, \textit{r} -- ReLU, -- \textit{fc} layer.}
\label{fig:relation_extractor}
\vspace{-0.08cm}
\end{figure}

\vspace{-0.5cm}
\paragraph{Query updater.}
\label{para:query_updater}
To further refine the features of noisy queries, a query updater is developed, which is formulated in Equ.\eqref{equ:update_query}. Since the data distribution of  and  is different from that of , a new set of learnable queries is first introduced to complement the relation features through element-wise summation. Then the set of complemented noisy queries is taken as the input query  to perform a new local self-attention  and the subsequent dynamic convolution given in Equ.\eqref{equ:iterative_decoder}.




Since object detection mainly focuses on the local region in an image. We design a new local self-attention module   to update the noisy query .  It ensures each query only interacts with local neighbors instead of the whole queries over the full image. The local self-attention first finds those neighbors of each query based on the boxes' IoUs whose values are greater than 0. Then it performs the `qkv' mechanism in the same way as MSA. To this end, we perform self-attention locally instead of globally.

Different from the neighbor finding rule in ~\cite{lin2020detr}, we adopt the function  to select neighbors from  that are spatially related to  in terms of \textit{IoU}. Note that, the new local self-attention  is used to replace the  in Equ.\eqref{equ:iterative_decoder} for feature interaction.

\vspace{-0.5cm}
\paragraph{Label assignment}


Since accepted queries tend to generate true positive predictions, while the noisy ones involve a considerable number of true positives and false positives. Towards end-to-end object detection, we introduce a new one-to-one label assignment rule to assign samples step by step. Specifically, we first match the accepted predictions  with the ground truth set of objects. Then remove those targets that have been matched, and mainly consider the bipartite matching between noisy predictions  and the remaining ground truth set of objects. This matching process is described in Algorithm~\ref{algorithm:first}\footnote{The HungarianMatch operation in Algorithm~\ref{algorithm:first} is a combinatorial optimization approach that solves the assignment problem, which is commonly used in \cite{carion2020end, sun2020sparse, zhu2021deformable, wang2020end} for one-to-one label assignment.}, where the matching cost computation is slightly different from the original version \cite{sun2020sparse}. A spatial prior is adopted to compute the matching cost , that is, the center of bounding box  needs to fall in the corresponding target box. Except for it, the formulation of the matching cost function is identical to the original work.







\begin{algorithm}[t] 
\caption{Label Assignment for .} 
\label{alg:yn} 
\begin{algorithmic}[1] 
\Require 
, , ; \\
 : results of  in Equ.\eqref{equ:detection_split} from stage ; \\
 :  results of  in Equ.\eqref{equ:detection_split} from stage ;\\
: target boxes.
\Ensure 
The matched predictions  and corresponding targets  after assignment.
\State Compute matching costs  between  and ;
\State ;

\State ;
\State Compute matching costs  between  and ;
\State ;

\\ 
\Return ; 
\end{algorithmic}\label{algorithm:first}
\end{algorithm}

\vspace{-1pt}
\subsection{Difference of Detector Design}

Generally, our approach can be deployed on most query-based object detectors ~\cite{carion2020end, sun2020sparse, zhu2021deformable}. To illustrate the proposed method, we choose Sparse RCNN~\cite{sun2020sparse} as our default instantiation. It consists  ( by default) decoding stages, each of which performs prediction according to Equ.~\eqref{equ:iterative_decoder}. As described in Figure.~\ref{fig:framwork}, we keep the first  decoding stages unchanged and only equip the last stage with the proposed method. Therefore, main differences lie in the last decoding stage, which will be described in the following.

\vspace{-0.6cm}
\paragraph{Architecture of the last stage.} 

As depicted in Figure.~\ref{fig:sr_sparse_rcnn}, the last decoding stage  first employs a prediction selector  to split queries into accepted queries and noisy queries according to the confidence scores of their associated predictions. Then they are input to the relation information extractor  to extract the relation feature between the noisy predictions and their accepted neighbors. Finally, queries are fed into the query updater  to be further refined for recognition and localization.













\vspace{-0.6cm}
\paragraph{Box prediction}
Like~\cite{sun2020sparse}, a box regression branch is used for box prediction in the first  stages. Differently, for the box prediction in the last stage, we directly use the identity mapping results from the previous  stage both in the training and testing phase. This is because, at the latter layers, the predicted bounding boxes are less likely to fluctuate, which is observed in ~\cite{lin2020detr}. Meanwhile, the recognition branch remains the same as that of previous stages. 





\vspace{-0.6cm} 
\paragraph{Training loss}

We adopt the set prediction
loss adopted in ~\cite{sun2020sparse,zhu2021deformable} for training. For the samples to train stage , we remove those samples from accepted predictions . to mitigate the class-imbalance issue, we follow the negative sample filtering mechanism~\cite{zhang2018refinedet} to early reject those well-classified negative queries whose confidence scores are lower than 0.05.


\vspace{-0.3cm}
\section{Experiments}
\label{sec:exp}
In this section, we evaluate our approach on heavily, moderately, and slightly crowded datasets~\cite{shao2018crowdhuman,zhang2017citypersons,lin2014microsoft} to demonstrate the generality of the proposed method in diverse scenarios.
\vspace{-0.6cm} 
\paragraph{Datasets.}

We adopt three datasets -- \emph{CrowdHuman} \cite{shao2018crowdhuman}, \emph{CityPersons} \cite{zhang2017citypersons} and \emph{COCO} \cite{lin2014microsoft} -- for comprehensive evaluations on heavily, moderately and slightly crowded situations, respectively. Table ~\ref{tbl:datasets} lists the ``instance density'' of each dataset.  Since our proposed approach mainly aims to improve crowded detections, we perform most of the comparisons and ablation experiments on \emph{CrowdHuman}. The evaluation experiments on \emph{Citypersons}~\cite{zhang2017citypersons} and \emph{COCO}~\cite{lin2014microsoft} are also conducted to suggest the proposed method is robust to crowdedness.

\begin{table}[t]
  \centering
  \begin{tabular}{l|c|c}
      \toprule
      Dataset & \# objects/img & \# overlaps/img  \\
      \hline
      CrowdHuman \cite{shao2018crowdhuman} & 22.64  & 2.40 \\
      CityPersons \cite{zhang2017citypersons} & 6.47  & 0.32 \\
      COCO \cite{lin2014microsoft} & 9.34  & 0.015 \\
      \bottomrule
  \end{tabular}
  \vspace{-2pt}
  \caption{\emph{Instance density} of each dataset.  The threshold for overlap statistics is .  *Averaged by the number of classes.}
  \label{tbl:datasets}
\vspace{-1.pc}
\end{table} 

\vspace{-1pc} 
\paragraph{Evaluation metrics.}

Following ~\cite{chu2020detection}, we mainly take three criteria: ,  and  as evaluation metrics. Generally, a larger , larger  and smaller  indicates a better performance.

\vspace{-1pc} 
\paragraph{Implementation details.} 
Unless otherwise specified, we take Sparse RCNN~\cite{sun2020sparse} as our default instantiation, using standard \textit{ResNet-50}~\cite{he2016deep} pre-trained on ImageNet as backbone. We train our model with the Adam optimizer with a momentum of 0.9 and weight decay of 0.0001. Models are trained for 50, 000 iterations. The initial learning rate is 0.00005 and reduced by a factor of 0.1 at iteration 37,500. The last stage joints the optimization after 5,000 iterations of training. . The default number of proposal boxes, proposal features, and stages are set to 500, 500, and 6, respectively. Additionally, The dimension of intermediate features in relationship extractor  is 256. The gradients are detached at proposal boxes from the second stage to stabilize training. Besides, those negative samples, whose \textit{intersection-over-area} (IoA) between any ignore region is higher than a threshold of ,  are not involved in training. Further, the hyper-parameters  and  are  and  by default in different query-based detectors~\cite{sun2020sparse,zhu2021deformable}. 

\subsection{Experiments on CrowdHuman}
CrowdHuman~\cite{shao2018crowdhuman} contains 15,000, 4,370 and 5,000 images for training, validation and test, respectively. For a fair comparison, we re-implement most of the involved models~\cite{lin2017feature,huang2020nms,lin2020focal,liu2019adaptive,2020atss,chu2020detection,tian2021fcos, wang2020end,carion2020end,zhu2021deformable, sun2020sparse,lin2020detr, gossipnet, hu2018relation}. Results are evaluated on the validation set, using the full-body annotations in the dataset.

\vspace{-1pc}
\paragraph{Main results.} We compare with mainstream object detectors, including \textit{box-based}: \textit{one-stage}~\cite{lin2020focal, 2020atss} , \textit{two-stages}~\cite{chu2020detection,lin2017feature,huang2020nms,liu2019adaptive}, and \textit{point-based}~\cite{tian2021fcos, wang2020end} as well as \textit{query-based}~\cite{carion2020end,zhu2021deformable, sun2020sparse,lin2020detr}. 

As shown in Table.~\ref{tbl:sota}, our approach outperforms these well-established detectors, achieving significant performance improvements over the \textit{box-based}, \textit{point-based}, and \textit{query-based} counterparts, illustrating the effectiveness of our approach in handling crowded scenes. Specifically, our method achieves 1.8\%  and 0.9\%  gains over the state-of-the-art \textit{box-based} approach MIP~\cite{chu2020detection}, which specializes in coping with crowded scenes.

The \textit{query-based} method Sparse RCNN~\cite{sun2020sparse}, equipped with the proposed method and 500 queries, can achieve 92.0\% , 41.4\%  and 83.2\%  on the challenging CrowdHuman dataset~\cite{shao2018crowdhuman}, which is 1.3\%, 3.3\% and 1.8\% better than its counterpart -- original Sparse RCNN~\cite{sun2020sparse}. When increasing the number of queries to 750, our approach can still obtain a better performance of 92.5\%  and 83.3\% . This is because more queries can cover more patterns of objects in the image, such as scale, size, position, and other characteristics. Additionally, equipped with our approach, deformable DETR ~\cite{zhu2021deformable}\footnote{The detail implementation of deformable DETR with the proposed schema is illustrated in the Appendix.} can also obtain 2.2\%  improvements over the original deformable DETR~\cite{zhu2021deformable}. Moreover, It also achieves 1.4\%  and 1.6\%  gains over the \textit{box-based} method MIP~\cite{chu2020detection}, demonstrating the effectiveness of our approach.
 
\begin{table}[ht]
	\centering
	\begin{tabular}{llccc}
		\toprule
		Method & \#Queries & AP &   & JI \\
		\hline
		\hline
		\textit{box-based} \\
		RetinaNet~\cite{lin2020focal} & - & 85.3 & 55.1 & 73.7 \\
		ATSS~\cite{2020atss} & - & 87.0 & 51.1 & 75.9 \\
        ATSS~\cite{2020atss}+~\cite{chu2020detection} & - & 88.7 & 51.6 & 77.0 \\
		FPN~\cite{lin2017feature}+NMS & - & 85.8 & 42.9 & 79.8 \\
		FPN~\cite{lin2017feature}+soft NMS & - & 88.2 & 42.9 & 79.8 \\
		FPN+MIP~\cite{chu2020detection} & - & 90.7 & 41.4 & 82.4 \\
		\hline
	     & - & 84.9 & 46.3 & -- \\
	    ~\cite{liu2019adaptive} & - & 84.7 & 47.7 & -- \\
	    ~\cite{huang2020nms} & - & 89.3 & 43.4 & -- \\
		\hline
		\hline
		\textit{point-based} \\
		FCOS~\cite{tian2021fcos} & - & 86.8 & 54.0 & 75.7 \\
		FCOS~\cite{tian2021fcos}+~\cite{chu2020detection} & - & 87.3 & 51.2 & 77.3 \\
		POTO~\cite{wang2020end} & - & 89.1 & 47.8 & 79.3 \\
		\hline
		\hline
		\textit{query-based} \\
		DETR~\cite{carion2020end} & 100 & 75.9 & 73.2 & 74.4 \\
		PEDR \cite{lin2020detr} & 1000 & 91.6 & 43.7 & 83.3 \\
		D-DETR \cite{zhu2021deformable} & 1000 & 91.5 & 43.7 & 83.1 \\
		S-RCNN~\cite{sun2020sparse} & 500 & 90.7 & 44.7 & 81.4 \\
		S-RCNN~\cite{sun2020sparse} & 750 & 91.3 & 44.8 & 81.3 \\
		\hline
		S-RCNN+\emph{Ours} & 500 & 92.0 & \textbf{41.4} & 83.2 \\
		S-RCNN+\emph{Ours} & 750 & \textbf{92.5} & 41.6 & 83.3 \\
		D-DETR+\emph{Ours} & 1000 & 92.1 & 41.5 & \textbf{84.0} \\
		\bottomrule
	\end{tabular}
	\caption{Comparisons of different methods on \emph{CrowdHuman} validation set,  represents multiple instance prediction with set NMS as post-processing.  indicates the approach is implemented by PBM~\cite{huang2020nms}. S-RCNN -- Sparse RCNN~\cite{sun2020sparse}. D-DETR -- deformable DETR~\cite{zhu2021deformable}.}
	\label{tbl:sota}
	\vspace{-1pc}
\end{table}

\vspace{-0.5cm}
\paragraph{Ablation study of different modules.}
To explore the effectiveness of the proposed modules in Sec.~\ref{sec:approach_declearation}, we conduct extensive ablation study of the relation information extractor , local self-attention module  and the newly initialized embedding \emph{E}. All experiments are conducted on Sparse RCNN~\cite{sun2020sparse} with 500 queries, \emph{ResNet-50}~\cite{he2016deep} backbone and evaluated on \emph{CrowdHuman} dataset. Table.~\ref{tbl:module_analysis} shows that the relation information extractor  can obtain an improvement of 0.8\% , 1.7\%  and 1.6\% . It indicates its effectiveness in reducing false positives and recalling false negatives. Moreover, when equipped with the new local self-attention , the performance on three evaluation metrics is further boosted, since the local self-attention can reduce duplicates effectively. Further, the newly initialized embeddings, aiming to approximate the new data distribution of noise predictions, can slightly improve .

\vspace{-0.5cm}
\paragraph{Ablation study of hyper-parameter .}
To analyze the effect of the confidence score threshold , we first formulate the relation between detection boxes and target boxes in an image as a bipartite graph . It consists of a set  and nodes .  represents a set of predicted boxes whose scores are higher than the pre-defined score threshold, while  denotes the target boxes. An edge in  is defined as overlapping when the \emph{IoU} value, between a box in  and the other one from , is higher than 0.5 by default\footnote{Here, we follow the procedure to compute evaluation metric .}. Hence, the matching results can be acquired after applying the Hungarian Algorithm. As shown in Figure.~\ref{fig:histogram}, as the confidence score increases, the number of \textit{true positives} shows a clean upward trend while the number of ~\textit{false positives} decreases rapidly. Also, Figure.~\ref{fig:bar_score} depicts the performance our method can achieve under different values of , where the performance increases slightly as  increases. Thus, if not specific, we set  to 0.7 by default.

\vspace{-0.5cm}
\paragraph{Ablation study of hyper-parameter \textbf{}.}
Here, we analyze the effect of the hyper-parameter \textit{intersection-over-union} threshold . As discussed in Sec.~\ref{intro}, making sure a box candidate can `perceive' its neighbors helps a noisy query decide to decrease its confidence score or not, which is also the prerequisite for our method to work effectively. Different settings of \textit{intersection-over-union} (\emph{IoU}) threshold  may affect the performance of the whole detector. We perform experiments on the \emph{CrowdHuman} dataset~\cite{shao2018crowdhuman} with  frozen as  while changing the value of  linearly. From Figure.~\ref{fig:bar_theta}, we found our approach is robust to the change of \emph{IoU} threshold. This success may attribute to the good approximating feature of the newly designed components.

\vspace{-0.5cm}
\paragraph{Comparison with previous relation modeling works.}
To differentiate the previous works and ours, we evaluate several representative \textit{relation modeling} methods: RelationNet~\cite{hu2018relation}, GossipNet~\cite{gossipnet}, IterDet~\cite{iterdet2021}. RelationNet~\cite{hu2018relation} utilized self-attention modules to model the relations among different predictions. Meanwhile, GossipNet~\cite{gossipnet} uses several hand-designed relation blocks to explore the relationships among the predicted boxes, while IterDet~\cite{iterdet2021} iteratively infers predictions based on a historical map produced from the previous iteration. We re-implement its re-scoring version for RelationNet~\cite{hu2018relation}. For GossipNet\footnote{GossipNet:\textit{https://github.com/hosang/gossipnet}} and IterDet\footnote{IterDet:\textit{https://github.com/saic-vul/iterdet}}, we use their open-source implementations for evaluation. All models use FPN~\cite{lin2017feature} with \emph{ResNet-50}~\cite{he2016deep} as backbone, following the same training setting in ~\cite{gossipnet, hu2018relation,lin2017feature}. 

As shown in Table.~\ref{tbl:relation_modeling}, our approach shows better performance when compared with previous relation modeling works. Surprisingly, both RelationNet~\cite{hu2018relation} and GossipNet~\cite{gossipnet} suffer from a significant drop in  and . It could attribute to the sub-optimal label assignment rule. Since both of them choose the prediction with the highest confidence score around one target as the correct box and take the rest as negatives. The predicted coordinates are not involved in computing loss, which might lead to the performance degradation in crowded scenes.


\begin{table*}[!t]
\vspace{0mm}
\centering
\subfloat[Ablations of different modules.\label{tbl:module_analysis}]{
\begin{minipage}{.3\textwidth}
\centering
    \tablestyle{1pc}{1.05}\setlength{\tabcolsep}{1.mm}
    \begin{tabular}{ccc|ccc}
		\toprule
		   &  & \emph{E} & AP &   & JI \\
		\hline
		 & & & 90.7 & 44.7 & 81.4 \\
		\hline 
		\checkmark & & & 91.5 & 43.0 & 83.0  \\
		\checkmark & \checkmark &  & \textbf{92.0} & 42.0 & \textbf{83.5}  \\
		\checkmark & \checkmark & \checkmark & \textbf{92.0} & \textbf{41.4} &  83.2  \\
		\bottomrule
	\end{tabular}
	\vspace{0.4cm}
\end{minipage}
}
\subfloat[Comparisons of different relation modeling approaches.\label{tbl:relation_modeling}]{
\begin{minipage}{.32\textwidth}
\centering
\tablestyle{5pt}{1.05}\setlength{\tabcolsep}{1.mm}
	\begin{tabular}{l|c|ccc}
		\toprule
		Method & \#Queries & AP &   & JI \\
		\hline
		GossipNet~\cite{gossipnet} & - & 80.4 & 49.4 & 81.6 \\
		RelationNet~\cite{hu2018relation} & - & 81.6 & 48.2 & 74.6 \\
		IterDet~\cite{iterdet2021} & - & 88.0 & 47.5 & 78.0 \\
		\hline
		D-DETR+\emph{Ours} & 500 & 91.2 & 42.6 & \textbf{84.0} \\
		S-RCNN+\emph{Ours} & 500 & \textbf{92.0} & \textbf{41.4} & 83.2 \\
		\bottomrule
	\end{tabular}
	\vspace{0.25cm}
\end{minipage}
}\hspace{0.3cm}
\subfloat[Performance comparisons on \emph{CityPersons}.~\label{tbl:citypersons_eval}]{
\begin{minipage}{0.32\textwidth}
\centering
\tablestyle{5pt}{1.05}\setlength{\tabcolsep}{1.mm}
    \begin{tabular}{l|c|cccc}
  \toprule
  Method & \#Queries &  & AP \\
  \hline
  FPN+NMS & & 9.8 & 94.7 \\
  FPN+Soft-NMS~\cite{bodla2017soft} & - & 9.9 & 94.9  \\
  \text{MIP}~\cite{chu2020detection} &  & 8.8 & 95.8   \\
  D-DETR~\cite{zhu2021deformable} & 500 & 9.4 & 96.6 \\
  ~\cite{sun2020sparse} & 500 & 10.0 & 96.8 \\
  \hline
  D-DETR+\emph{Ours} & 500 & 7.8 & 96.7 \\
  S-RCNN+\emph{Ours} & 500 & \textbf{7.8} & \textbf{97.6} \\
  \bottomrule
  \end{tabular}
  \end{minipage}
  }\vspace{-6pt}
  \caption{~\ref{tbl:module_analysis}. Ablation study of different modules proposed in our approach, taking Sparse RCNN~\cite{sun2020sparse} with 500 queries as our default instantiation. ~\ref{tbl:relation_modeling}. Comparisons of different relation modeling appraoches. All the experiments are conducted on ~\emph{CrowdHuman}~\cite{shao2018crowdhuman} dataset. ~\ref{tbl:citypersons_eval} Performance comparisons of different methods on ~\emph{CityPersons}~\cite{zhang2017citypersons}. Both \textit{box-based} ~\cite{chu2020detection,lin2017feature} and \textit{query-based} approaches~\cite{sun2020sparse,zhu2021deformable} are evaluated.}
  \vspace{-1pc}
\end{table*}

\begin{figure*}
\centering
\scriptsize
\begin{minipage}{0.35\textwidth}
  \centering
  \begin{tikzpicture}[
    box/.style={rectangle, rounded corners=2, draw=gray, fill=gray!5, align=center, minimum height=26},
  ]
  \centering
  \begin{axis}[
      ybar, axis on top,
      height=5cm, width=1.12\linewidth,
      bar width=.225cm,
      ybar=0,
      ymajorgrids, tick align=inside,
      major grid style={draw=gray!50},
      enlarge x limits=.15,
      ymin=0, ymax=3e4,
      axis x line*=bottom,
      y axis line style={opacity=0},
      major tick length=2pt,
      ytick style={draw=none},
      legend style={
          draw=none,
          fill=none,
          at={(1, 1.08)},
          inner sep=1.2,
          anchor=north east,
          legend columns=-1,
          /tikz/every even column/.append style={column sep=0.1cm}
        },
      legend image code/.code={\draw [#1, draw=none] (0cm,-0.05cm) rectangle (0.18cm,0.08cm); },
      ylabel=Number of errors (\#errors),
      ylabel near ticks,
      symbolic x coords={Duplicate,Localization,Background,Missing},
      xtick=data,
      ytick={0, 5e3, 1e4, 1.5e4, 2e4}
    ]
    \addplot [draw=none, fill=ji] coordinates {
      (Duplicate, 17798)
      (Localization, 14666)
      (Background, 3515)
      (Missing, 9948) };
    \addplot [draw=none,fill=ap] coordinates {
      (Duplicate, 9022)
      (Localization, 11396)
      (Background, 2607)
      (Missing, 9948) };

    \legend{Sparse R-CNN, ours}
    \node[box] at (axis cs:Duplicate, 2.48e4)    (_5) {\color{ji}{1.780}\\\color{ap}{0.902}};
    \node[box] at (axis cs:Localization, 2.48e4) (_6) {\color{ji}{1.467}\\\color{ap}{1.140}};
    \node[box] at (axis cs:Background, 2.48e4)   (_7) {\color{ji}{0.352}\\\color{ap}{0.261}};
    \node[box] at (axis cs:Missing, 2.48e4)      (_8) {\color{ji}{0.995}\\\color{ap}{0.995}};
  \end{axis}
\end{tikzpicture}
   \vspace{-6ex}
  \caption{Error analysis on Sparse RCNN~\cite{sun2020sparse} and our approach, with \emph{ResNet-50}~\cite{he2016deep} as backbone. The bar plots show different error types that contribute to the \textit{false positives}.}
  \label{fig:tide_decompositon}
\end{minipage}
\hfill
\begin{minipage}{0.62\textwidth}
\begin{subfigure}{0.43\textwidth}
  \centering
  \begin{tikzpicture}[
    box/.style={rectangle, rounded corners=2, draw=gray, fill=gray!5, align=center, minimum height=26},
  ]
  \centering
  \begin{axis}[
      ybar, axis on top,
      height=5cm, width=1.12\linewidth,
      bar width=0.15cm,
      ybar=0,
      ymajorgrids,
      tick align=inside,
      enlarge x limits=.1,
      axis x line*=bottom,
      y axis line style={opacity=0},
      ymin=0, ymax=1.5,
      major tick length=2pt,
      ytick style={draw=none},
      legend style={
          draw=none,
          fill=none,
          at={(1, 1.08)},
          inner sep=1.2,
          anchor=north east,
          legend columns=-1,
          /tikz/every even column/.append style={column sep=0.1cm}
        },
      legend image code/.code={\draw [#1, draw=none] (0cm,-0.05cm) rectangle (0.18cm,0.08cm); },
      ytick={0.0, 0.2, 0.4, 0.6, 0.8, 1.0},
      yticklabels={0.0, 0.2, 0.4, 0.6, 0.8, 1.0},
      xtick={0.5,0.6,0.7,0.8,0.9},
      xticklabels={0.5,0.6,0.7,0.8,0.9},
      ylabel near ticks,
      xlabel near ticks,
    ]
    \addplot [draw=none, fill=mr] coordinates {
        (0.5, 0.4242)
        (0.6, 0.4274)
        (0.7, 0.4140)
        (0.8, 0.4097)
        (0.9, 0.4150) };
    \addplot [draw=none, fill=ji] coordinates {
        (0.5, 0.8312)
        (0.6, 0.8326)
        (0.7, 0.8320)
        (0.8, 0.8330)
        (0.9, 0.8346) };
    \addplot [draw=none, fill=ap] coordinates {
        (0.5, 0.9195)
        (0.6, 0.9200)
        (0.7, 0.9210)
        (0.8, 0.9199)
        (0.9, 0.9186) };
    \legend{MR, JI, AP}
    \node[box] at (axis cs:0.5, 1.24) (_5) {\color{mr}{.424}\\\color{ji}{.831}\\\color{ap}{.920}};
    \node[box] at (axis cs:0.6, 1.24) (_6) {\color{mr}{.427}\\\color{ji}{.833}\\\color{ap}{.920}};
    \node[box] at (axis cs:0.7, 1.24) (_7) {\color{mr}{.414}\\\color{ji}{.832}\\\color{ap}{.921}};
    \node[box] at (axis cs:0.8, 1.24) (_8) {\color{mr}{.410}\\\color{ji}{.834}\\\color{ap}{.920}};
    \node[box] at (axis cs:0.9, 1.24) (_9) {\color{mr}{.415}\\\color{ji}{.834}\\\color{ap}{.919}};
  \end{axis}
\end{tikzpicture}
   \vspace{-1ex}
  \caption{\textit{confidence score threshold} }
  \label{fig:bar_score}
\end{subfigure}
\hfill
\begin{subfigure}{0.56\textwidth}
  \centering
  \begin{tikzpicture}[
    box/.style={rectangle, rounded corners=2, draw=gray, fill=gray!5, align=center, minimum height=26},
  ]
  \centering
  \begin{axis}[
      ybar, axis on top,
      height=5cm, width=1.12\linewidth,
      bar width=0.15cm,
      ybar=0,
      ymajorgrids,
      tick align=inside,
      enlarge x limits=.1,
      axis x line*=bottom,
      y axis line style={opacity=0},
      ymin=0, ymax=1.5,
      major tick length=2pt,
      ytick style={draw=none},
      legend style={
          draw=none,
          fill=none,
          at={(1, 1.08)},
          inner sep=1.2,
          anchor=north east,
          legend columns=-1,
          /tikz/every even column/.append style={column sep=0.1cm}
        },
      legend image code/.code={\draw [#1, draw=none] (0cm,-0.05cm) rectangle (0.18cm,0.08cm); },
      ytick={0.0, 0.2, 0.4, 0.6, 0.8, 1.0},
      yticklabels={0.0, 0.2, 0.4, 0.6, 0.8, 1.0},
      xtick={0.1,0.2,0.3,0.4,0.5,0.6,0.7},
      xticklabels={0.1,0.2,0.3,0.4,0.5,0.6,0.7},
      ylabel near ticks,
      xlabel near ticks,
    ]
    \addplot [draw=none, fill=mr] coordinates {
        (0.1, 0.4249)
        (0.2, 0.4272)
        (0.3, 0.4306)
        (0.4, 0.414)
        (0.5, 0.4222)
        (0.6, 0.4179)
        (0.7, 0.4231) };
    \addplot [draw=none, fill=ji] coordinates {
        (0.1, 0.8346)
        (0.2, 0.8324)
        (0.3, 0.8336)
        (0.4, 0.832)
        (0.5, 0.8357)
        (0.6, 0.8351)
        (0.7, 0.8354) };
    \addplot [draw=none, fill=ap] coordinates {
        (0.1, 0.9191)
        (0.2, 0.9177)
        (0.3, 0.9188)
        (0.4, 0.921)
        (0.5, 0.9201)
        (0.6, 0.9200)
        (0.7, 0.9199) };
    \legend{MR, JI, AP}
    \node[box] at (axis cs:.1, 1.24) (_1) {\color{mr}{.425}\\\color{ji}{.835}\\\color{ap}{.919}};
    \node[box] at (axis cs:.2, 1.24) (_2) {\color{mr}{.427}\\\color{ji}{.833}\\\color{ap}{.918}};
    \node[box] at (axis cs:.3, 1.24) (_3) {\color{mr}{.431}\\\color{ji}{.834}\\\color{ap}{.919}};
    \node[box] at (axis cs:.4, 1.24) (_4) {\color{mr}{.414}\\\color{ji}{.832}\\\color{ap}{.921}};
    \node[box] at (axis cs:.5, 1.24) (_5) {\color{mr}{.422}\\\color{ji}{.836}\\\color{ap}{.920}};
    \node[box] at (axis cs:.6, 1.24) (_6) {\color{mr}{.418}\\\color{ji}{.835}\\\color{ap}{.920}};
    \node[box] at (axis cs:.7, 1.24) (_7) {\color{mr}{.423}\\\color{ji}{.835}\\\color{ap}{.920}};
  \end{axis}
\end{tikzpicture}
   \vspace{-1ex}
  \caption{\textit{intersection over union threshold} }
  \label{fig:bar_theta}
\end{subfigure}
\caption{Performance of the proposed method with different configurations of hyper-parameter  and \textbf{} on \emph{CrowdHuman}~\cite{shao2018crowdhuman} dataset.}
\end{minipage}
\vspace{-1pc}
\end{figure*}

\vspace{-0.5cm}
\paragraph{Analysis on false positives.}
To understand the factors contributing to the performance improvement, we conduct an error analysis on our method. We adopt the recently proposed TIDE~\cite{tide-eccv2020} to compare our approach with the counterpart Sparse RCNN~\cite{sun2020sparse}. We analyzed the composite error at Recall=0.9 for all methods. As illustrated in Figure.~\ref{fig:tide_decompositon}, our method performs better at removing duplication, providing more accurate localization, and reducing mistaken recognition. Since part of queries can perceive whether their targets are detected or not through the relation information extractor. Also, the local self-attention module ensures queries only interact with their neighbors rather than the whole. To this end, the duplicates could be eliminated efficiently. Besides, with identity mapping plugged in the last regression branch for box prediction, the number of training samples in the previous decoding stage increases, making the optimization much easier. Additionally, benefiting from the new learnable embeddings for data distribution approximation, the representation ability of object queries are further enhanced.

\vspace{-0.2cm}
\subsection{Experiments on Citypersons}

CityPersons~\cite{zhang2017citypersons} is one of the widely used benchmarks for pedestrian detection. It contains 5, 000 images (2, 975 for training, 500 for validation, and 1, 525 for testing, respectively). Each image has a size of 1024  2048. To improve the overall performance, we proposed to pre-train all models on the \emph{CrowdHuman} dataset and fine-tune them on ~\emph{CityPersons} (\emph{reasonable}) training subset, then tested on the (\emph{reasonable}) validation subset. For those ~\emph{box-based} methods, we train and evaluate them with the image resolution enlarged by 1.3 compared to the original one for better accuracy. The \emph{query-based} approaches are trained and evaluated at the original image size with 500 queries. The other settings remain the same as those of Sparse RCNN~\cite{sun2020sparse} and deformable DETR~\cite{zhu2021deformable}.

\vspace{-0.2cm}
\subsection{Experiments on COCO.}
According to Table ~\ref{tbl:datasets}, the crowdness of \emph{COCO}~\cite{lin2014microsoft} is
very low, which is beyond our design purpose. Nevertheless, we still conduct an experiment on this dataset to verify: 1) whether our method generalizes well to multi-class detection; 2) whether our approach can still handle slightly crowded scenarios, especially with \emph{isolated} instances.

Following the common practice of Sparse RCNN ~\cite{sun2020sparse} with 300 queries, we use a subset of 5000 images in the original validation set (named \emph{minival}) for validation while using the remaining images in the training and validation set for training. Except for the proposed modules and label assignment rule in the last stage, other settings remain the same as the original methods~\cite{zhu2021deformable, sun2020sparse}. Table.~\ref{tbl:mscoco_eval} shows the performance comparisons with deformable DETR~\cite{zhu2021deformable} and Sparse RCNN~\cite{sun2020sparse}. Moderate improvements are obtained, e.g. \textbf{}{0.9\%}  higher than the deformable DETR~\cite{zhu2021deformable} and \textbf{1.1\%}  higher than the Sparse RCNN~\cite{sun2020sparse}. The experimental results reflect the effectiveness of our progressive predicting approach in slightly crowded scenarios, proving the proposed method can also solve the performance saturation problem of \emph{query-based} detectors.

\begin{table}[t]
  \centering
  \begin{tabular}{p{23mm}|p{5mm}<{\centering}p{5mm}<{\centering}p{6mm}<{\centering}|p{5mm}<{\centering}p{5mm}<{\centering}p{5mm}<{\centering}}
  \toprule
  Method & AP &  &  &   &   & \\
  \hline
  S-RCNN~\cite{sun2020sparse} & 45.0 & 64.2 & 49.1 & 27.6 & 47.5 & 59.1 \\
  D-DETR~\cite{zhu2021deformable} & 45.8 & 64.5 & 49.4 & 28.2 & 49.0 & 61.7 \\
  \hline
  S-RCNN+\emph{Ours} & 46.1 & 65.3 & 50.6 & 29.2  &  48.7  & 59.9 \\
  D-DETR+\emph{Ours} &  \textbf{46.7} & 65.3 & 50.3 & 28.6  &  49.8  & 61.7 \\
  \bottomrule
  \end{tabular}
  \caption{Performance comparisons of different methods on \emph{COCO} 2017 ~\cite{lin2014microsoft} \emph{minival} set.}
  \label{tbl:mscoco_eval}
 \vspace{-1pt}
\end{table}

\vspace{-0.2cm}
\section{Conclusion}
In this paper, we propose a progressive prediction method to boost the performance of \emph{query-based} object detectors in handling crowded scenes. Equipped with our approach, two representatives \textit{query-based} methods, Sparse RCNN~\cite{sun2020sparse} and deformable DETR~\cite{zhu2021deformable} achieve consistent improvements over the heavily, moderately, as well as slightly crowded datasets~\cite{shao2018crowdhuman, zhang2017citypersons, lin2014microsoft}, which suggests our approach is robust to crowdedness. Since Sparse RCNN~\cite{sun2020sparse} and deformable DETR~\cite{zhu2021deformable} require large computing resources, making it difficult for our method to be deployed on devices with limited computing capacity. How to develop a computation-efficacy end-to-end detector is still under exploration. Besides, we found the decision boundary for the noisy queries is unclear. We believe that the performance can be further improved if a better feature engineering method or loss function is adopted. However, it is beyond the purpose of this work.

\clearpage

  {\small
    \bibliographystyle{ieee_fullname}
    \bibliography{ref21, ref22}
  }
\newpage
\appendix
\vspace{-0.4cm}
\section{Implementation of Deformable DETR with progressive predicting method.}

We also deploy our progressive predicting approach on the deformable DETR~\cite{zhu2021deformable} to demonstrate the generality of our method. Similar to Sparse RCNN ~\cite{sun2020sparse}, the decoder in deformable DETR consists of 6 stages, which is depicted in Figure.~\ref{fig:ddetr_arch}. As described in the paper, we integrate our designed components into the last decoding stage. Figure.~\ref{fig:sr_ddetr} also depicts its detail architecture. For the hyper-parameters setting, i.e\ confidence score threshold , are identical to those adopted in Sparse RCNN~\cite{sun2020sparse}.

We choose the deformable DETR with iterative bounding box refinement. Following deformable DETR~\cite{zhu2021deformable}, we use \emph{ResNet-50}~\cite{he2016deep} as backbone. The whole detector is trained with an Adam optimizer~\cite{kingma2014adam} and a weight decay of 0.0001. The total training duration is 50 epochs on 8 GPUs with 1 image per GPU. The initial learning rate is 0.0002 and dropped by a factor of 0.1 after 40 epochs. The parameters initialization in the newly added components and losses weights are identical to the original work~\cite{zhu2021deformable}. The default number of queries and stages is 500 and 6, respectively. The hyper-parameters  and  are also  and , respectively. The gradients are detached at proposal boxes from the second stage to stabilize training. We stop gradient back-propagation from the last stage to the previous ones. Besides, those negative samples that overlap with ignore region with an \textit{intersection-over-area}(IoA) greater than  are not involved in training.



\vspace{-0.2cm}
\begin{figure}[!t]
  \centering
  \begin{subfigure}{\linewidth}
    \definecolor{brown}{HTML}{843C0C}
\definecolor{darkred}{HTML}{C43C0C}
\definecolor{skyblue}{HTML}{00B0F0}
\definecolor{black}{HTML}{000000}

\begin{tikzpicture}[
    scale=0.9,
    sym/.style={inner sep=1},
    box/.style={rectangle, rounded corners=3, draw=black, thick, minimum size=16, minimum width=54},
    roi/.style={rectangle, rounded corners=3, draw=black, thick, minimum size=16, inner sep=0, fill=white},
    rnet/.style={rectangle, minimum size=16},
    dr/.style={rectangle, rounded corners=3, draw=black, thick, minimum height=16, inner sep=1, fill=white},
    arr/.style={thick, ->, >=stealth, draw=black}
  ]
  \small

\node[sym]   at (0, 0)       (prev_q)              {};
  \node[sym]   at (0, 3)       (prev_box)            {};
  \node[box]   at (2, 1.5)     (dyn_conv)            {MCA};
  \node[box]   at (2, 0)       (self_attn)           {MSA};
  \node[sym]   at (2, 3)       (box)                 {};
  \node[sym]   at (.2+4, 0)    (q)                   {};
  \node[box]   at +(.4+6, 1.5) (_dyn_conv)           {MCA};
  \node[box]   at +(.4+6, 0)   (_self_attn)          {MSA};
  \node[sym]   at +(.4+8, 0)   (_q)                  {};
  \node[sym]   at +(.4+6, 3)   (_box)                {};

\draw[arr] (prev_box)         -- (dyn_conv.west);
  \draw[arr] (prev_q)           -- (self_attn);
  \draw[arr] (self_attn)        -- (dyn_conv);
  \draw[arr] (dyn_conv)         -- (box);
  \draw[arr] (dyn_conv.east)    -- (q);
  \draw[arr] (box) -- (.2+4, 3) -- (_dyn_conv.west);
  \draw[arr] (_self_attn)       -- (_dyn_conv);
  \draw[arr] (_dyn_conv)        -- (_box);
  \draw[arr] (_dyn_conv.east)   -- (_q);
  \draw[arr] (q)                -- (_self_attn);
\end{tikzpicture}
     \caption{Decoder in deformable DETR~\cite{zhu2021deformable}.  -- multi-head cross-attention,  -- multi-head self-attention.}
    \label{fig:ddetr_arch}
  \end{subfigure}
  \vspace{-0.5pc}
  \begin{subfigure}{\linewidth}
    \definecolor{brown}{HTML}{843C0C}
\definecolor{darkred}{HTML}{C43C0C}
\definecolor{skyblue}{HTML}{00B0F0}
\definecolor{black}{HTML}{000000}
\begin{tikzpicture}[
    scale=0.9,
    sym/.style={inner sep=1},
    box/.style={rectangle, rounded corners=3, draw=black, thick, minimum size=16, minimum width=54},
    roi/.style={rectangle, rounded corners=3, draw=black, thick, minimum size=16, inner sep=0, fill=white},
    rnet/.style={rectangle, minimum size=16},
    dr/.style={rectangle, rounded corners=3, draw=black, thick, minimum height=16, inner sep=1, fill=white},
    arr/.style={thick, ->, >=stealth, draw=black}
  ]
  \small
\node[sym]             at (0, 0)       (prev_q)        {};
  \node[sym]             at (0, 3)       (prev_box)      {};
  \node[box]             at (2, 1.5)     (dyn_conv)      {MCA};
  \node[box]             at (2, 0)       (self_attn)     {MSA};
  \node[sym]             at (2, 3)       (box)           {};
  \node[box, draw=none]  at (2, 3)       (hidden_box)    {};
  \node[sym]             at (.2+4, 0)    (q)             {};
  \node[box]             at +(.4+6, 1.5) (_dyn_conv)     {MCA};
  \node[box]             at +(.4+6, 0)   (_self_attn)    {LMSA};
  \node[sym]             at +(.4+8, 0)   (_q)            {};
  \node[sym]             at +(.4+6, 3)   (_box)          {};
  \node[dr, anchor=east] at (.2+4+.01, 1.5)  (d)             {};
  \node[dr, anchor=west] at (.2+4-.01, 1.5)  (r)             {};
  \node[rnet]            at (.2+4, 1.5)  (rnet)          {};

\draw[arr] (prev_box)         -- (dyn_conv.west);
  \draw[arr] (prev_q)           -- (self_attn);
  \draw[arr] (self_attn)        -- (dyn_conv);
  \draw[arr] (dyn_conv)         -- (box);
  \draw[arr] (dyn_conv.east)    -- (q);
  \draw[arr] (box) -- (.2+4, 3) -- (_dyn_conv.west);
  \draw[arr] (hidden_box.east)  -- (rnet);
  \draw[arr] (rnet)             -- (_self_attn.west);
  \draw[arr] (_self_attn)       -- (_dyn_conv);
  \draw[arr] (_dyn_conv)        -- (_box);
  \draw[arr] (_dyn_conv.east)   -- (_q);
  \draw[arr] (q)                -- (.2+4, 1.2);
\end{tikzpicture}
     \caption{Decoder in SR-Deformable DETR (Ours).  -- Prediction Selector,  -- Relation information extractor,  -- local multi-head self-attention.}
    \label{fig:sr_ddetr}
  \end{subfigure}
  \vspace{-0.5pc}
  \caption{~\ref{fig:ddetr_arch} is the architecture of decoding stage in deformable DETR~\cite{zhu2021deformable};
  ~\ref{fig:sr_ddetr} describes the decoding stage structure equipped with our designed components for progressive predicting schema. }
  
\end{figure}







\vspace{-0.2cm}
\section{Performance change of a query-based decoder when handling crowded scenes.}
The performance of a \emph{query-based} detector would not be improved but will degrade as the depth of a decoder increases when handling crowded scenes. Experiments are conducted on \emph{CrowdHuman} dataset, taking Sparse RCNN based on \emph{ResNet-50} as base detector. It equips with 500 queries. We adjust the depth of its decoder while keeping the others unchanged. As is described in Table. ~\ref{tbl:depth_exp}, the performance degrades as the depth of the decoder increases.

\vspace{-0.2cm}
\begin{table}[ht]
	\centering
	\begin{tabular}{l|c|ccc}
		\toprule
		 \#Depth& \#Queries & AP &   & JI \\
		\hline
		 6 & & 90.7 & 44.7 & 81.4 \\
		 7 & & 90.6 & 45.7 & 81.0 \\
		8 & 500& 90.4 & 45.9 & 80.3 \\
		9 & & 90.7 & 44.4 & 80.9 \\
        10 & & 90.2 & 46.6 & 80.0 \\
		\bottomrule
	\end{tabular}
	\caption{Experiment analysis as the depth of a decoder increases, which performs on \emph{CrowdHuman} dataset.}
	\label{tbl:depth_exp}
	\vspace{-1.5pc}
\end{table}

\vspace{-0.1cm}
\section{Performance of query detector with large model in crowded scenes.}

To explore the detection upper bound of a query-based detector in tackling crowded scenes, we replace the \emph{ResNet-50} with a large backbone, Swin-Large~\cite{liu2021swin}. Experiments are conducted on \emph{CrowdHuman}~\cite{shao2018crowdhuman} and \emph{CityPersons}~\cite{zhang2017citypersons} datasets, with the same training strategy described in the paper. As depicted in Table.~\ref{tbl:large_model}, our method can significantly boost the performance of a query-based detector, which achieves a \textit{state-of-the-arts} results on both \emph{CrowdHuman} and \emph{CityPersons}  validating datasets.

\vspace{-0.2cm}
\begin{table}[ht]
	\centering
	\begin{tabular}{p{22mm}|p{12mm}<{\centering}|p{11mm}<{\centering}|p{4mm}<{\centering}p{5mm}<{\centering}p{4mm}<{\centering}}
		\toprule
		 Method& Dataset & \#Queries & AP &   & JI \\
		\hline
		 S-RCNN&  & 500 & 93.1 & 39.9 & 85.1 \\
		 S-RCNN+\emph{Ours} &\emph{CHuman} & 500 &  93.8 & \textbf{37.4} & 86.5 \\
		D-DETR & & 1000& 93.4 & 39.6 & 86.3 \\
		D-DETR+\emph{Ours} & & 1000& \textbf{94.1} & 37.7 & \textbf{87.1} \\
		\hline
		S-RCNN&  & 500 & 98.3 & 5.9 & 93.7 \\
		 D-DETR &\emph{CPersons} & 500 &  96.4 & 8.4 & 92.0 \\
		S-RCNN+\emph{Ours} & & 500& \textbf{98.4} & \textbf{4.9} & \textbf{94.2} \\
		D-DETR+\emph{Ours} & & 500& 97.5 & 5.9 & 93.7 \\
		
		\bottomrule
	\end{tabular}
	\caption{Experiment on \emph{CHuman}(\emph{CrowdHuman}) and \emph{CPersons}(\emph{CityPersons}) with Swin-L~\cite{liu2021swin}. S-RCNN -- Sparse RCNN~\cite{sun2020sparse}, D-DETR -- Deformable DETR~\cite{zhu2021deformable}}
	\label{tbl:large_model}
	\vspace{-2pc}
\end{table}
\clearpage

\newpage
\begin{figure*}
  \small
  \centering
  \setlength{\tabcolsep}{3pt}
\begin{tabular}{cccccc}
  \toprule
  RelationNet~\cite{hu2018relation} & IterDet~\cite{iterdet2021} & Sparse R-CNN~\cite{sun2020sparse} & D-DETR~\cite{carion2020end} & Sparse RCNN+Ours & D-DETR+Ours \\
  \midrule

  \includegraphics[width=0.15\textwidth,trim={0 0 0 2.4cm},clip]{images/figs/273278,12d5a00000b7cb45_relation.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 2.4cm},clip]{images/figs/273278,12d5a00000b7cb45_iterdet.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 2.4cm},clip]{images/figs/273278,12d5a00000b7cb45_sparse-rcnn-new.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 2.4cm},clip]{images/figs/273278,12d5a00000b7cb45_detr.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 2.4cm},clip]{images/figs/273278,12d5a00000b7cb45_iter-sparse-rcnn.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 2.4cm},clip]{images/figs/273278,12d5a00000b7cb45_iter.detr.jpg} \\

  \includegraphics[width=0.15\textwidth]{images/figs/282555,16a5a000e5588af5_relation.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,16a5a000e5588af5_iterdet.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,16a5a000e5588af5_sparse-rcnn-new.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,16a5a000e5588af5_detr.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,16a5a000e5588af5_iter-sparse-rcnn.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,16a5a000e5588af5_iter.detr.jpg} \\
  
  \includegraphics[width=0.15\textwidth,trim={0 1.1cm 0 0},clip]{images/figs/283991,1f5900026d74f0d_relation.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 1.1cm 0 0},clip]{images/figs/283991,1f5900026d74f0d_iterdet.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 1.1cm 0 0},clip]{images/figs/283991,1f5900026d74f0d_sparse-rcnn-new.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 1.1cm 0 0},clip]{images/figs/283991,1f5900026d74f0d_detr.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 1.1cm 0 0},clip]{images/figs/283991,1f5900026d74f0d_iter-sparse-rcnn.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 1.1cm 0 0},clip]{images/figs/283991,1f5900026d74f0d_iter.detr.jpg} \\

  \includegraphics[width=0.15\textwidth]{images/figs/283554,2605000054a40402_relation.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/283554,2605000054a40402_iterdet.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/283554,2605000054a40402_sparse-rcnn-new.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/283554,2605000054a40402_detr.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/283554,2605000054a40402_iter-sparse-rcnn.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/283554,2605000054a40402_iter.detr.jpg} \\

  \includegraphics[width=0.15\textwidth]{images/figs/282555,baa5a00000be1da2_relation.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,baa5a00000be1da2_iterdet.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,baa5a00000be1da2_sparse-rcnn-new.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,baa5a00000be1da2_detr.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,baa5a00000be1da2_iter-sparse-rcnn.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/282555,baa5a00000be1da2_iter.detr.jpg} \\

  \includegraphics[width=0.15\textwidth]{images/figs/273271,2277e000891f2e71_relation.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,2277e000891f2e71_iterdet.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,2277e000891f2e71_sparse-rcnn-new.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,2277e000891f2e71_detr.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,2277e000891f2e71_iter-sparse-rcnn.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,2277e000891f2e71_iter.detr.jpg} \\

  \includegraphics[width=0.15\textwidth]{images/figs/273271,1ddda0001685370d_relation.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,1ddda0001685370d_iterdet.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,1ddda0001685370d_sparse-rcnn-new.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,1ddda0001685370d_detr.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,1ddda0001685370d_iter-sparse-rcnn.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273271,1ddda0001685370d_iter.detr.jpg} \\

  \includegraphics[width=0.15\textwidth,trim={0 0 0 .8cm},clip]{images/figs/273275,10cd9200055f04d4a_relation.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 .8cm},clip]{images/figs/273275,10cd9200055f04d4a_iterdet.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 .8cm},clip]{images/figs/273275,10cd9200055f04d4a_sparse-rcnn-new.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 .8cm},clip]{images/figs/273275,10cd9200055f04d4a_detr.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 .8cm},clip]{images/figs/273275,10cd9200055f04d4a_iter-sparse-rcnn.jpg} &
  \includegraphics[width=0.15\textwidth,trim={0 0 0 .8cm},clip]{images/figs/273275,10cd9200055f04d4a_iter.detr.jpg} \\

  \includegraphics[width=0.15\textwidth]{images/figs/273278,103204000dfd1be77_relation.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273278,103204000dfd1be77_iterdet.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273278,103204000dfd1be77_sparse-rcnn-new.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273278,103204000dfd1be77_detr.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273278,103204000dfd1be77_iter-sparse-rcnn.jpg} &
  \includegraphics[width=0.15\textwidth]{images/figs/273278,103204000dfd1be77_iter.detr.jpg} \\


\bottomrule
\end{tabular}
   \caption{Results visualization of RelationNet~\cite{hu2018relation}, IterDet~\cite{iterdet2021}, Sparse RCNN~\cite{sun2020sparse}, deformable DETR~\cite{zhu2021deformable} and our approach based on them~\cite{sun2020sparse, zhu2021deformable}. Blue boxes are true positive detections, light yellow boxes are missed instances and orange boxes are false positives. Green boxes represent progressively refined detections in our method.}
\end{figure*}
\clearpage
\end{document}
