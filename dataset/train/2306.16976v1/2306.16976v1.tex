\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}



\usepackage[preprint]{neurips_2023}
\bibliographystyle{unsrt} 









\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{algpseudocode}
\usepackage{changepage}
\usepackage{graphicx}
\usepackage{xspace}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{caption}
\usepackage[ruled,vlined]{algorithm2e}
\renewcommand*{\UrlFont}{\ttfamily\smaller\relax}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{floatrow}
\floatsetup[table]{capposition=top}
\usepackage{wrapfig}
\usepackage{lipsum}
\newcommand{\mfr}[1]{\textcolor{red}{#1}}
\newcommand{\ahmed}[1]{\textcolor{brown}{[ahmed: #1]}}
\newcommand{\mlozano}[1]{\textcolor{blue}{[mlozano: #1]}}
\newcommand{\edwin}[1]{\textcolor{magenta}{[edwin: #1]}}
\newcommand{\sco}[1]{\textcolor{purple}{[sco: #1]}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\newcommand{\defeq}{\mathrel{\mathop:}=}

\title{Diffusion-Jump GNNs:\\
Homophiliation via Learnable Metric Filters}





\author{Ahmed Begga Hachlafi \\
University of Alicante, Spain\\
\texttt{ahmedbegga@gmail.com} \\
\And
   Francisco Escolano \\
   University of Alicante, Spain\\
   \texttt{escolano.ua@gmail.com} \\
   \And
   Miguel Angel Lozano \\
   University of Alicante, Spain\\
   \texttt{malozano@ua.es} \\
   \And
   Edwin R. Hancock \\
   University of York, U.K.\\
   \texttt{edwin.hancock@york.ac.uk} \\
}


\begin{document}


\maketitle


\begin{abstract}
  High-order Graph Neural Networks (HO-GNNs) have been developed to infer consistent 
  latent spaces in the heterophilic regime, where the label distribution is not correlated 
  with the graph structure. However, most of the existing HO-GNNs are \emph{hop-based}, i.e., 
  they rely on the powers of the transition matrix. 
  As a result, these architectures are not fully reactive to the classification loss and the 
  achieved structural filters have static supports. In other words, neither the filters' supports 
  nor their coefficients can be learned with these networks. They are confined, instead, to learn combinations 
  of filters. 
To address the above concerns, we propose \textsc{Diffusion-jump GNNs} a method relying on 
  asymptotic diffusion distances that operates on \emph{jumps}. A \emph{diffusion-pump} generates 
  pairwise distances whose projections determine both the support and coefficients of each structural 
  filter. These filters are called jumps because they explore a wide range of scales in order to find  
  bonds between scattered nodes with the same label. Actually, the full process is controlled by the 
  classification loss. Both the jumps and the diffusion distances react to classification errors (i.e. 
  they are learnable). 
\emph{Homophiliation}, i.e., the process of learning piecewise smooth latent spaces in the heterophilic 
  regime, is formulated as a Dirichlet problem: the known labels determine the border nodes and the diffusion-pump 
  ensures a minimal deviation of the semi-supervised grouping from a canonical unsupervised grouping. This triggers the update of both the 
  diffusion distances and, consequently, the jumps in order to minimize the classification error. 
The Dirichlet formulation has several advantages. It leads to the definition of \emph{structural heterophily}, a novel 
  measure beyond edge heterophily. It also allows us to investigate links with (learnable) diffusion distances, absorbing random walks and 
  stochastic diffusion. 
Finally, our experimental results outperform significantly those of the state-of-the-art both in homophilic and heterophilic 
  datasets. We are very competitive for large graphs. 
\end{abstract}


\section{Introduction}\label{sec:1}
The success of Graph Neural Networks (GNNs) relies on their convolutional architecture~\citep{kipf2017semi}\citep{hamilton2017inductive}\citep{velickovic2018gat}. Their  \emph{aggregate and combine} mechanism provides a significant degree of expressiveness. However, in the heterophilic regime, such a mechanism (initially designed for homophilic graphs) results in the over-smoothing issue (shadowing of the internal representations of the nodes, due to a non-selective aggregation). In ~\citep{Beyond2020}, three solutions are explored: (i) ego and neighbor embedding separation, (ii) higher-order neighborhoods, and (iii) a combination of intermediate representations. The purpose of these mechanisms is to enforce the internal representations of the node  features so that the resulting latent space becomes consistent (piecewise smooth), for instance, when the downstream task is node-classification~\citep{Non-Local22}\citep{zhu2021graph}\citep{pmlr-v119-chen20v}. 

In this paper, we explore High-Order GNNs (HO-GNNs). One type of HO-GNNs results from \emph{rewiring the edges} in the graph. For instance, the method in~\citep{bi2022make} explores neighborhoods of several orders (hops) selecting those orders who provide a high correlation between the node features. GATs~\citep{velickovic2018gat} are also a well-known rewiring method: the strength of each edge in the input graph is given by a trainable weight. Such a weight is corrected if the concatenation of the node features associated with the corresponding edge has a negative impact on node-classification. Diffwire~\citep{arnaiz2022diffwire} is another trainable rewiring method. The basic idea of Diffwire is to estimate the commute-times distance between each pair of nodes and use the distance matrix to mask the original adjacency matrix. Other non-differentiable rewiring methods are mainly addressed to alleviate the over-squashing issue (bottlenecks obstruct the message-passing process). A couple of recent examples are \citep{topping2022understanding} and \citep{digiovanni2023oversquashing}. 

A second type of HO-GNNs are \emph{Deep/Sequential hop-based} methods, i.e. those models that address over-smoothing with a deep architecture. GGCNs~\citep{Twosides22} attenuate over-smoothing by performing edge correction (corrected edge weights are learned from node degrees, and signed edges are learned from node features). However, Shortest-Paths-MPNNs~\citep{abboud2022shortest} and Ordered-GNNs~\citep{song2023ordered} are more focused on performing robust aggregations. Shortest-Paths-MPNNs compute the shortest paths between any
pair of nodes. Then, for each node, several separate aggregations are performed (each one for increasing lengths of the shortest paths); then, the resulting embeddings are weighted. Ordered-GNNs rely on a similar principle: for each node, the hierarchy of a tree rooted in that node is aligned with the hops wrt this node in the graph. As neighboring nodes within  hops form a depth subtree, aggregations for shallow sub-trees precede those for deeper ones. Interestingly, Ordered-GNNs introduce a differentiable way of deciding the split point between sub-trees.

Finally, \emph{Shallow/Parallel hop-based} methods explore several hop orders in parallel and then integrate the resulting embedding (e.g. via concatenation). 
MixHop~\citep{mixhop19}, FSGNNs~\citep{FSGNN21} and DualNets~\citep{DualNet22} compute several powers 
of the normalized adjacency matrix (transition matrix) . Each power feeds a different 
GNN. The resulting embeddings are weighed and concatenated for later discrimination. SIGN~\citep{sign_icml_grl2020} is similar to 
MixHop but it precomputes the aggregations  for the sake of scalability. More recently, the Simple Graph Convolution (SGG) method~\citep{chanpuriya2022simplified} improves MixHop by learning polynomials of the transition matrix.  

Finally, Generalized PageRank GNNs (GPR-GNNs)\citep{GPR21}learns jointly the best embedding of each node feature and the best weight of each hop. This is very interesting, since the suitability of , which is encoded by a weight , influences the latent space of the features  through a learnable function . As a result the th embedding is  . This mechanism allows GPR-GNNs to avoid over-smoothing and trade node and topology feature informativeness. However, this strategy produces inconsistent results since GPR-GNNs are better suited for heterophilic graphs, instead of being also useful for homophilic graphs. 



\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth,height=5cm]{images/figura1.pdf}
    \caption{Hop-hierarchy (Top) vs Jump-hierarchy (Bottom). Diffusion distances contract the similarity space due to \emph{structural forces}.}
    \label{fig:jump}
\end{wrapfigure}







\textbf{Main Limitation of HO-GNNs.} Most of the existing HO-GNNs explore different powers 
of the normalized adjacency matrix (transition matrix) . In  other words, they are completely \emph{hop-based}. 
As a result, the HO-GNNs exploit the labels of the semi-supervised learning process either to alleviate the over-smoothing issue (in the sequential case~\citep{Twosides22}\citep{abboud2022shortest}~\citep{song2023ordered}) or to weigh the importance of each hop order (in the parallel case~\citep{mixhop19}\citep{FSGNN21}\citep{DualNet22}\citep{sign_icml_grl2020}). However, as the structure of the input graph is \emph{static}, the hops are static as well. Consequently, \emph{the labels cannot be backpropagated 
to change the structure of the hops, but only the relative importance of each hop or the extent of its  aggregation support}.

\textbf{Implications.} As a result, dealing with heterophilic graphs goes beyond the potential achievements of hop-based approaches (see our Experiments in Section~\ref{sec:5}). Despite high-order hops being able of connecting distant 
nodes with the same label, such connections can be neither attenuated nor amplified for the sake of the classification loss. In other words, the probability that a random walk links two nodes is an \emph{in-place coefficient}, not the realization of a probabilistic event. In this regard, parallel HO-GNNs claim that the powers  can be interpreted as a bank of \emph{structural filters}, i.e. a bunch of aggregators inspired by convolutional filters such as Gabor receptive filds~\citep{mixhop19}. However, an expressive characterization of a structural filter requires that both its \emph{support} (specification of what edges have a non-zero coefficient) and its \emph{coefficients} are learnable.  

\textbf{Our contributions.} In this paper, we address the problem of learning a bank of expressive structural filters as follows:
\begin{enumerate}
    \item[\textbf{a)}] We re-formulate the problem of node-classification under heterophily in terms of a \textbf{Dirichlet problem}, i.e. we have \emph{border nodes} (training set) where the classification is optimal and \emph{interior nodes} (remaining nodes) where the resulting latent space (node embedding) must be as harmonic (piecewise smooth) as possible, even when the labeling is far from being harmonic over the graph. 
\item[\textbf{b)}] We have a \textbf{diffusion pump} which generates \emph{asymptotic diffusion distances}  between the nodes by learning the nontrivial top eigenvectors of  subject to the labeling of the training set. 
\item[\textbf{c)}] Given the diffusion distances  we compute the \emph{jump hierarchy} (see Figure~\ref{fig:jump}). For a node  we have that  () is the closest node wrt itself;  () are nodes so that only  is closer to node  than any of them,  () are nodes so that only the nodes in  are closer to  than any of them, and so on. Each of the sets , where  are the nodes of the graph , is called a \textbf{jump}. 
\item[\textbf{d)}] The edges  define the \textbf{support of the jump} and the \textbf{coefficients}  are given by a function  of the diffusion distances (for example the neg-exponential). Then, the \textbf{structural filter}  is a matrix with non-zero coefficients only at . 
\item[\textbf{e)}] Each structural filter  with  feeds a GNN parameterized by  and the resulting embedding   is weighted by a learnable parameter  subject to  . All the weighted embeddings are concatenated and feed a forward network for classification. 
\end{enumerate}

In addition, we propose a novel metric called \textbf{structural heterophily} and we denote the process of generating homophilic embeddings from heterophilic graphs as \textbf{homophiliation}.

This paper is organized as follows. In Section~\ref{sec:2}, we formulate semi-supervised learning in terms of a Dirichlet problem. This allows us to measure heterophily in a structural way. Section~\ref{sec:3} is devoted to formulating the loss functions and explaining the dynamics of the optimization process. This is done through the analysis of the main modules of our model. Section~\ref{sec:4} provides more technical and formal details of our model and establishes links with related inspiring formulations. Our experiments are presented and discussed in Section~\ref{sec:5}. Finally, our conclusions and future work are summarized in Section~\ref{sec:6}.

\section{Heterophily as the Loss of Harmonicity}\label{sec:2}\textbf{Node-classification under heterophily} can be posed as the following semi-supervised learning problem. Given an input graph,  with adjacency matrix  and node features , there is a node subset  whose labels  are known by the learner (border nodes). Similarly, the labels  of the remaining nodes, those in , are hidden (unknown nodes). 

Given the graph Laplacian , and a regularizer (minimizer of , we have , where  is the smoothest labeling of  after propagating  to  through the edges of the graph. A Dirichlet solver ensures that the labeling  is \emph{Harmonic} (the label of a given unknown node is the average of those of its neighbors) subject to the labeling of the border nodes .

In the heterophilic regime, two neighboring nodes rarely share their labels. As a result, , where  are the vectorized labels obtained by an alternative unsupervised learner. The unsupervised learner typically assumes that the labels  are correlated with the topology of the graph (homophily). In other words, \emph{heterophily can be posed in terms of how much harmonicity is lost wrt the homophilic assumption}.  

The objective of a GNN is to learn a parametric function  returning , a matrix (embedding) of latent representations (one row per node) so that the embeddings of either border nodes or hidden nodes with the same label are grouped together. However,  does not necessarily minimize , where  contains the vectorized classification labels. We need to infer a \emph{hidden graph}  where  is minimized. Actually, the edges  in the hidden graph should link nodes with the same label, even if they are not in the same community. 

\textbf{Structural Heterophily.} Given the above formulation we may characterize heterophily in a structural way, namely \emph{as the departure from a structural unsupervised grouping}. In particular, the ratio 

 is close to the unit if the graph is homophilic (the structure is completely correlated with the labels). For  the graph is  heterophilic. The larger the ratio the larger the heterophily. 

We use the example in Figure~\ref{fig:jump} to illustrate how  works. We have two  communities,  (left and right respectively). The \emph{white node} belongs naturally to the right one , and this is what an unsupervised structural clustering detects: the Fiedler vector  has positive components () in  and negative components () in . 

The vector  is the smallest nontrivial eigenvector of  as well as the largest nontrivial eigenvector of . It has been argued that the top eigenvectors of  may be used to decompose the state space into metastable subspaces~\citep{meta04}. In other words, each of the two graph communities in Figure~\ref{fig:jump} defines a metastable state from which a random walker tries to escape (Section~\ref{sec:4}). 

The average escape time is the inverse of the top nontrivial eigenvalue of , i.e. the inverse of the approximate spectral gap~\citep{siam81}\citep{Kramers90}\citep{sclimitations06}. In our example, the spectral gap is very tiny so we can expect large escape times (see more details in Section~\ref{sec:5}). In particular, the two states defined by the Fiedler vector are very compact (they have low variability). As a result, all the pairs of nodes  inside each community have very similar asymptotic diffusion distances~\citep{difmaps05}  according to the structural forces characterizing each metastable state.
    
Consequently, the jump hierarchy defines a succession of unstable states  resulting from the expansion from : . They are unstable because their Dirichlet energies  are greater than that of the unsupervised clustering (\emph{ground energy}) .

Last, but by no means least, if we label the white node as belonging to  instead of belonging to  (i.e., \emph{we introduce heterophily}), we also increase the Dirichlet energy wrt the ground energy, i.e. . Why? This is because the new Fiedler vector  leading to the labeling  does no longer induce a step function but a hyperbolic tangent with a positive slope. This is consistent with the increase of the spectral gap and the reduction of the escape time. 

Therefore, one useful interpretation of heterophily in structural terms (departure from the ground energy) is the fact that heterophily relaxes Dirichlet energies in such a way that it is possible to escape from a community in a few jumps and then find nodes with the same label in other communities. Therefore, paying attention to several jumps simultaneously increases the chance of aggregating distant nodes with the same label, thus solving the heterophily issue. 


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width = \linewidth,height =7cm]{images/figura3_boosted_3.pdf}}
    \caption{Homophiliation while the jumps and pairwise distances are learned. Top-Left: a heterophilic graph. Center-Left: The current distance matrix  leads to a tridimensional weight distribution . Yellow points denote the support  of the filter . The filter coefficients are given by the weights of the support . Bottom-Left: graph used for aggregation with this filter. In the right panel, we show the weight distribution (top), a couple of filters (middle), and the resulting homophiliation (bottom) for some epochs. In particular, we show ,  and . In each epoch , all the embeddings  contribute to identifying potential links between scattered nodes with similar labels. If any of these links is wrong, the matrix of pairwise distances  is updated.}.
    \label{fig:jump-explain}
\label{figure-pump}
\end{center}
\end{figure*} 
   
\section{Homophiliation: Losses and Modules}\label{sec:3}

\textbf{Homophiliation.} Our computational model for node-classification under heterophily cannot only rely on finding Harmonic labelings but also on transforming the matrix of node features  into a piece-wise smooth embedding . The rows in  associated with nodes with the same label must be clustered together and these labels must be consistent with those of the border nodes . Such a process, i.e. the learning of , results from solving the following optimization problem:

where we have an interplay between the \emph{Dirichlet loss}  and the \emph{classification loss} (Cross-Entrophy)  as follows. 

\textbf{Diffusion Pump.} Minimizing the structural heterophily so that  (Eq.~\ref{structuralh}) in  implies learning Dirichlet energies close to the ground energy. However, in the heterophilic regime, we cannot minimize  before discovering the optimal embedding . In the meanwhile, the Dirichlet formulation allows us to learn the smallest nontrivial eigenvectors of  as we do in the unsupervised setting (e.g. the Fiedler vector). These eigenvectors will be in the columns of , but they do not have a free form. The notation  in the above optimization problem goes beyond emphasizing the learnability of . We also constrain the eigenvectors to be projections/transformations of the adjacency matrix . 

We learn the eigenvectors  because it is key to computing diffusion distances between the nodes. In the following, we will replace  by  when we need to emphasize the matrix nature of the pairwise distances. Each pairwise distance  comes from the norm of  (row-difference). As we will detail in Section~\ref{sec:4} ,  approximates  the asymptotic diffusion distance between two nodes  and . Herein, we focus on the fact that nodes belonging to the same sub-structure (e.g. cluster or community) have similar distances. Back to Figure~\ref{fig:jump}, if  and  belong to the same community, two random walks placed in  and  have similar escape probabilities. Therefore, we build a hierarchy of escape probabilities to characterize the respective reachability of any node wrt a given one. Interestingly, the hierarchy induced by hops is isotropic wrt each node whereas the hierarchy induced by escape probabilities is anisotropic. This latter hierarchy is built by specifying binary projection matrices  which select the pairs of distances that support the creation/update each structural filter . 

\textbf{Exploration by Parallel Jumping.} The diffusion pump triggers the creation of  structural filters  derived from their respective jumps . Each filter  has its support  and its coefficients . We illustrate this process in Figure~\ref{fig:jump-explain} over a heterophilic graph. At any epoch, the optimizer creates a distance matrix  and weighs it: . The result is a weight distribution (middle-left). Each yellow point in the weight distribution belongs to the jump . Consequently, the yellow points denote the edges of the filter support  and they are projected in the adjacency matrix below the distribution. The coefficients  of the filter are the heights of the yellow points. Finally, the edges in the graph depicted below the distribution are exactly those of the filter support. Neighbor aggregation wrt this filter  will be constrained to that graph and these weights. 

The right panel in Figure~\ref{fig:jump-explain} shows the evolution of the weight distribution (top), some filters (center), and the status of the homophiliation process (bottom). The optimization process is initially dominated by the diffusion pump since random weight distributions are explored first. As a result, scattered nodes with similar labels can be potentially aggregated: we start to implicitly build the hidden graph . The probability of aggregating distant nodes is leveraged by the fact that, during the first epochs, most of the  filters  have a random nature independently of  , the filter order. Escape probabilities are relaxed during this \emph{exploration stage}.  

\textbf{Classification Loss.} Each filter,  becomes the aggregator of a naive GNN  which generates an embedding . This embedding is weighted by a learnable parameter  and concatenated with the remaining embeddings to feed a classification layer. Therefore, as soon as the structural filters discover interesting bonds for minimizing , the weights  of all the GNNs, the filters' coefficients, and the distance matrix will become more and more stable. At some point in the optimization process, the Dirichlet loss will be stabilized and the exploration stage ends. Later on, the classification loss will refine the almost-homophilic global embedding .
As a result, the embeddings of either border nodes or hidden nodes with the same label are grouped together in the latent space (for instance, see the column of Epoch  in Figure~\ref{fig:jump-explain}).







                                                              


\section{Methodological Details}\label{sec:4}
\subsection{Network Architecture}
\textsc{Diffusion-Jump GNNs} are neural networks  resulting from the optimization problem stated in Eq.~\ref{eq:loss}.  
The interplay between the Dirichlet loss and the classification loss is described above. In this Section,  we give some technical details about the architecture of the network. 

\textbf{Diffusion pump.} The pump is responsible for generating and updating the matrix of pairwise diffusion distances . For the generation, we solve any of the following equivalent problems: 

both s.t. , where , . Since  is the diagonal degree matrix, we have . 
As a result, the Min problem \emph{approximates} the  smallest nontrivial eigenvectors of the normalized Laplacian , where  is the normalized adjacency. Equivalently, the Max problem \emph{approximates} the  largest nontrivial
eigenvectors of the transition matrix . Note that  and  have the same eigenvectors and also that if  is an eigenvalue of  then  an eigenvalue for . Note also, that we use "\emph{approximates}" instead of "\emph{finds}". This is due to the limitations of Stochastic Gradient Descent (SGD) when solving the Trace-Ratio problems~\citep{TraceRatio07}\citep{TraceRatio09}\citep{TraceRatio12} in Eq.~\ref{eq:maxminlosses}. In this regard, we have the following results with practical implications:

\begin{theorem}[Fiedler Environments]\label{th:1} The SGD solution of the Trace-Ratio Min problem in Eq.~\ref{eq:maxminlosses} can be posed in terms of  under orthonormality constraints. This leads to , i.e. to the orthogonal eigenfunctions of the normalized Laplacian  associated with . However,  is not necessarily an eigenvalue of , but an approximation of the Fiedler value : . As a result, the  columns  of  satisfy: , where  denotes the Fiedler vector. Then, we obtain what we call a \emph{Fiedler environment}.
\end{theorem} 
\begin{corollary}[Asymptotic Diffusion Distances]\label{cor:1} The norm of  (row-difference) is proportional to the approximate commute time between nodes  and , which is , where   . Therefore, the matrix  relies on Euclidean distances.  
\end{corollary}
We proof both results in \emph{Appendix A: Formal Results with Practical Implications}. We also give there practical evidence of the need of solving Trace-Ratio problems in an SGD context, instead of solving the original Trace problem in Eq.~\ref{eq:loss}. We also justify the convenience of conditioning  to , . Actually, this setting is inspired in how the LINKX method~\citep{LINKX21} exploits the graph topology. 

\textbf{Jumps and Filters}. The bank of learnable structural filters  is the core of the high-order exploration. Each filter  has a \emph{support} 
 with the pairs of nodes (edges of the filter) belonging to the jump . In Eq.~\ref{eq:loss}, this is implicitly defined with the expression , where  is a  projection matrix defined, for , as follows: 

with  and , where  for  and . 
Then,  are the sorted positions of the distances wrt the node , i.e. the \emph{distance ranks}. In this way, the product  is derivable wrt  as in~\citep{gao2019graph}. Alternatively, we could also rely on the  network~\citep{Topk20}.

\textbf{Individual GNNs.} Each structural filter  feeds a vanilla GNN which obtains a partial embedding . The GNN also receives the  matrix of node features , and  denotes the transpose of the th row of . Since , then, for a given node , its aggregation is given by  instead of being  as in MixHop~\citep{mixhop19} or  as in Simple Graph Convolution (SGG)~\citep{chanpuriya2022simplified}. As the asymptotic diffusion distances are approximations of commute times (Theorem~\ref{th:1}), our aggregation works as a kernel depending on learnable Euclidean distances. 

\textbf{Combining GNNs.} Each partial embedding  is weighted by a learnable parameter , where all the  form a convex combination. Then, we concatenate all the weighted embeddings to form the global embedding . Since  feeds an MLP in order to minimize the classification loss  as in MixHop, the global embedding tends to retain the best partial embeddings for each node. 

\textbf{Homophilic Branch.} One limitation of our method is that setting a small value for the hyperparameter  is not enough to deal with homophilic graphs. For this reason, we have added an extra GNN (the homophilic branch) that works as follows: . Therefore we concatenate , where . See the optimal learned coefficients in Figure~\ref{fig:alfas}.







\subsection{Inspiring Methods}
We conclude this Section by reviewing some \textbf{links with very inspiring methods} in the literature. For instance, our Dirichlet formulation is inspired by classical graph-based semi-supervised methods. In particular, the work in~\citep{Zhou03} addresses the problem of propagating known labels  to unknown nodes . Let  be a  matrix where  means that node  has label  and  otherwise. Then, we have the following result:
\begin{theorem}[Dichilet label propagation~\citep{Zhou03}]\label{th:2}
    The optimal label of each node  is given by , where , being  the transition matrix and . In addition,  minimizes , where  is a regularization parameter satisfying .
\end{theorem}
Consequently, the diffusion pump in our model is governed by a similar equation: Eq.~\ref{eq:loss}. We sketch the proof of the above theorem and its relationship with absorbing random walks~\citep{doyle2000random} and semi-supervised image segmentation~\citep{Grady06} in \emph{Appendix A}. 

Finally, another important source of inspiration was the design of escape probabilities in terms of diffusion equations. Actually, there is a substantial body of theory linking spectral clustering, random walks, diffusion distances, and metastable states~\citep{pmlr-vR3-meila01a}\citep{difmaps05}\citep{sclimitations06} to be analyzed also in the same appendix. Herein, we only highlight the following result: 
\begin{theorem}[~\citep{sclimitations06}]\label{th:3}
Given a probability function in Boltzmann form  in a given latent space , the random walk with transition matrix  converges to the stochastic differential equation , where  denotes Brownian motion. Also, the potential time scales describing the expected time of passage between clusters rely on the potential function .
\end{theorem}
\section{Experiments and Discussion}\label{sec:5}
\label{discussion}
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{images/alfas_new.png}
    \caption{Optimal attention for each jump.}
    \label{fig:alfas}
\end{wrapfigure}
\textbf{Experimental settings}.Table~\ref{tab:results} presents the results obtained by each model on the standard small-medium datasets~\citep{Cora}~\citep{GEOM-GCN}~\citep{actor}. To ensure consistency, we used the same 10 random splits (48\%/32\%/20\%) provided by~\citep{GEOM-GCN}, along with the best configuration for each model. We place the code at \url{https://anonymous.4open.science/r/Diffusion-Jump-GNNs-8EE2/}. The above configurations were extracted from Table 1~\citep{GRAFF} and Tables 3, 10, and 11 in~\citep{LINKX21}. Overall, our model outperformed all others or achieved a close second place, demonstrating strong competitiveness. We assessed the degree of \emph{structural heterophily} using our metric . Specifically, we examined two heterophilic regimes:  indicating low structural heterophily, and  indicating high structural heterophily. \\
\begin{table*}[ht]

\caption{Node-classification accuracies. Top three models are coloured by \textcolor{red}{ \textbf{First}}, \textcolor{blue}{ \textbf{Second}}, \textcolor{violet}{ \textbf{Third}}.}
\label{tab:results}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{l c c c c c c c c c} 

                \toprule
                                \phantom{} & \textbf{Texas} & \textbf{Wisconsin} & \textbf{Cornell} & \textbf{Actor} & \textbf{Squirrel} & \textbf{Chameleon} & \textbf{Citeseer} & \textbf{Pubmed} & \textbf{Cora} \\
                \midrule
                                 Hom level & 0.11 & 0.21 & 0.30 & 0.22 & 0.22 & 0.23 & 0.74 & 0.80 & 0.81 \\ 
                                  & \textbf{18.37} & \textbf{6.90} & \textbf{6.03} & \textbf{209.58} & \textbf{20.62} & \textbf{8.30} & \textbf{5.78} & \textbf{7.64} & \textbf{7.36}
                                 \\
                                \centering \# Nodes     & 183 & 251 & 183 & 7,600 & 5,201 &  2,277 & 3,327 &  19,717 & 2,708 \\ 
                                \centering \# Edges     & 295 & 466 & 280 & 26,752 & 198,493 & 31,421 & 4,676 & 44,324 & 5,278 \\ 
                                \centering \# Classes   & 5 & 5 & 5 & 5 & 5 & 5 & 7 & 3 & 6 \\ 
\midrule
                GGCN~\citep{GGCN} &  &  &  &  &  &  &  &  & \\
                GPRGNN~\citep{GPRGNN} &  &  &  &  &  &  &  &   & \\
                H2GCN~\citep{Beyond2020} &  &  &  &  &  &  &  &  & \\
                GCNII~\citep{GCNII} &  &  &  &  &  &  &  &  & \\
                Geom-GCN~\citep{GEOM-GCN} &  &  &  &  &  &  &  &  & \\
                PairNorm~\citep{PairNorm} &  &  &  &  &  &  &  &  & \\
                GraphSAGE~\citep{hamilton2017inductive} &  &  &  &  &  &  &  &  & \\
                GCN~\citep{kipf2017semi} &  &  &  &  &  &  &  &  & \\
                GAT\citep{velickovic2018gat} &  &  &  &  &  &  &  &  & \\
                MLP &  &  &  &  &  &  &  &  & \\
CGNN\citep{CGNN} &  &  &  &  &  &  &  &  & \\
                MixHop~\citep{mixhop19} &  &  &  &  &  &  &  &  & \\
                FSGNN(8-hop)~\citep{FSGNN21} &  &  &  &  &  &  &  &  & \\
GRAFF~\citep{GRAFF} &  &  &  &  &  &  &  &  & \\
                LINKX~\citep{LINKX21} &  &  &  &  &  &  &  &  & \\

                ACMII-GCN++~\citep{ACM} &  &  &  &  &  &  &  &  & \\
                Ordered GNN~\citep{song2023ordered} &  &  &  &  &  &  &  &  & 
                \\
                ASGC~\citep{chanpuriya2022simplified} &  &  &  &  &  &  &  &  & 
                \\
\midrule
                \textbf{DJ-GNN} &  &  &  &  &  &  &  &  &  \\
                \bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table*}



\textbf{Low Structural Heterophily}. For these datasets, we list their optimal number of jumps (hyperparameter ): \textsc{Wisconsin} (), \textsc{Cornell} (), \textsc{Citeseer} (), \textsc{Pubmed} () and \textsc{Cora} ()) we find that a few jumps are enough for achieving or improving the SOTA. However, not all jumps are equally important. For instance, \textsc{Wisconsin} and \textsc{Cornell} rely mostly on the first two jumps (see Figure{~\ref{fig:alfas}}), while the remaining datasets rely on the \emph{homophilic branch} (no jump). Actually, \textsc{Citeseer}, \textsc{Pubmed}, and \textsc{Cora} are the datasets with the smallest edge heterophily (\textsc{Hom Level}). In addition, we are the second best method only in \textsc{Cornell} ( (ours) vs  (\textsc{FSGNN(8-hop)}) and we are very competitive in \textsc{Citeseer} ( (ours) vs  (\textsc{Geom-GCN})). In the first case, \textsc{FSGNN(8-hop)} uses a fully supervised split (60\%/20\%/20\%) whereas we use the more severe semi-supervised split. For \textsc{Citeseer}, we note that the \textsc{Geom-GCN} method relies on the geometry of the latent space. In this regard, \textsc{Citeseer} is the dataset with the lowest structural heterophily (), i.e. the geometry of the latent space is a fair representation of the topology of the graph. As a result, adding jumps may complicate that geometry: actually, the most important branches are  and the \emph{homophilic branch} (no jump). Finally, in \textsc{Pubmed}, our method is slightly improved by \textsc{Ordered GNN} ( vs ). \\


\textbf{High Structural Heterophily}. For these datasets~\citep{penn94}~\citep{ogbn}, we also list their optimal number of jumps (hyperparameter ): \textsc{Texas} (), \textsc{Squirrel} (), \textsc{Chameleon} () and \textsc{Actor} (). Our best result is for \textsc{Texas} (), where we significantly improve the SOTA ( (ours) vs  (\textsc{ACMII-GCN++}, which is a multi-channel GCN with adaptive channel mixing)). In \textsc{Squirrel}, we are slightly outperformed by \textsc{FSGNN(8-hop)} ( vs ) due, again to their use of a full supervised split (60\%/20\%/20\%). We are very competitive in this dataset because the \textsc{Squirrel} graph is very dense and we only need  jumps to achieve good results. We are also the best model in \textsc{Chameleon} (whose structural heterophily is the smallest in this set): we obtain  vs the second-best model \textsc{FSGNN(8-hop)}  that achieves  (again with a split of 60\%/20\%/20\%). 


\textbf{Parallel (Shallow) vs Sequential (Deep)}. Our method is Parallel (multi-branch shallow GNN) and its performance is the best or it is very competitive in small-medium datasets. There is one exception. In the \textsc{Pubmed} dataset, where we obtain , we are slightly outperformed by \textsc{Ordered GNN} with only  layers () because this dataset is very homophilic. This also happens with \textsc{GCNII} which explores  to  layers. We can conclude that deep methods have a good performance in homophilic datasets but such a performance decays significantly in heterophilic ones. 
\begin{table*}[ht]
\caption{Node-classification accuracies in large graphs. Top three models are coloured by \textcolor{red}{ \textbf{First}}, \textcolor{blue}{ \textbf{Second}}, \textcolor{violet}{ \textbf{Third}}.}
\label{tab:results_large}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l c c c} 

                \toprule
                                \phantom{} & \textbf{Penn94} & \textbf{arXiv-year} & \textbf{ogbn-arXiv}  \\
                \midrule
                                 Hom level & \textbf{0.47} & \textbf{0.21} & \textbf{0.66} \\ 
                                \centering \# Nodes     & 41,554 & 169,343 & 169,343  \\ 
                                \centering \# Edges     &  1,362,229 & 1,166,243 & 1,166,243 \\ 
                                \centering \# Classes   & 5 & 5 & 40  \\ 
\midrule
                MLP &  &  & 
                \\
                GCN &  &  & 
                \\
                GAT &  &  & 
                \\
                MixHop &  &  & OOM
                \\
                LINKX &  &  & 
                \\
\midrule    
                \textbf{DJ-GNN} &  &  & 
                \\
                \bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table*}
\\
We have also tested our model in \textbf{Very Large Graphs} (see Table~\ref{tab:results_large}). In this regard, we note that the memory requirements of our method  , where  is the number of nodes , force us to decouple the diffusion pump from the jump exploration. We first learn the matrix of pairwise diffusion distances in an unsupervised way. Later, we use it in a static way to minimize the classification loss. Despite that limitation, we obtain a very competitive performance both for \textsc{Penn94} ( vs  with \textsc{LINKX}) and  \textsc{ogbn-arXiv} ( vs  with \textsc{GAT}). However, our performance decreases in \textsc{arXiv-year} which is more heterophilic than the others: memory limitations force us to use only  hops for the three datasets. 

Finally, we extend our experimental results in \emph{Appendix B: SBM Analysis} and \emph{Appendix C: Experimental and Computational Details}.

\section{Conclusions and Future Work}\label{sec:6}
In this paper, we propose \textsc{Diffusion-Jump GNNs}, a multi-branch GNN architecture that addresses the heterophily issue from a structural perspective. Firstly, we define node-classification in terms of a Dirichlet problem. This allows us to define a new measure of heterophily: structural heterophily. Having this measure in mind, we formulate a loss function that governs the interplay between the two main components of our architecture: the \textbf{diffusion pump} (which generates diffusion distances) and the \textbf{parallel jumps} (which drive the exploration of links between nodes with similar labels). The most important contribution of our model is that the diffusion distances, and consequently the jumps and the structural filters derived from them, are fully learnable. Our experiments show that our model outperforms the SOTA or it is very competitive. Finally, our future work includes: a) scalability, in terms of memory and b) automatic jump selection. 

\textbf{Broader Impact.} This research contributes to improving the reliability of graph learners in the heterophilic regime. Besides the well-known social benefits concerning the detection of malicious nodes in social networks, we consider the application of our learnable structural filters to fairness.




\medskip


{
\small
\begin{thebibliography}{10}

\bibitem{kipf2017semi}
Thomas~N. Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{hamilton2017inductive}
Will Hamilton, Zhitao Ying, and Jure Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{velickovic2018gat}
Petar Veli{\v{c}}kovi{\'{c}}, Guillem Cucurull, Arantxa Casanova, Adriana
  Romero, Pietro Li{\`{o}}, and Yoshua Bengio.
\newblock {Graph Attention Networks}.
\newblock {\em International Conference on Learning Representations}, 2018.

\bibitem{Beyond2020}
Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai
  Koutra.
\newblock Beyond homophily in graph neural networks: Current limitations and
  effective designs.
\newblock In {\em Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, 2020.

\bibitem{Non-Local22}
Meng Liu, Zhengyang Wang, and Shuiwang Ji.
\newblock Non-local graph neural networks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  44(12):10270--10276, 2022.

\bibitem{zhu2021graph}
Jiong Zhu, Ryan~A. Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen~K. Ahmed,
  and Danai Koutra.
\newblock Graph neural networks with heterophily, 2021.

\bibitem{pmlr-v119-chen20v}
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li.
\newblock Simple and deep graph convolutional networks.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 1725--1735. PMLR, 13--18 Jul
  2020.

\bibitem{bi2022make}
Wendong Bi, Lun Du, Qiang Fu, Yanlin Wang, Shi Han, and Dongmei Zhang.
\newblock Make heterophily graphs better fit gnn: A graph rewiring approach.
\newblock {\em arXiv preprint arXiv:2209.08264}, 2022.

\bibitem{arnaiz2022diffwire}
Adri{\'a}n Arnaiz-Rodr{\'i}guez, Ahmed Begga, Francisco Escolano, and Nuria
  Oliver.
\newblock {DiffWire: Inductive Graph Rewiring via the Lov{\'a}sz Bound}.
\newblock In {\em The First Learning on Graphs Conference}. PMLR, 2022.

\bibitem{topping2022understanding}
Jake Topping, Francesco~Di Giovanni, Benjamin~Paul Chamberlain, Xiaowen Dong,
  and Michael~M. Bronstein.
\newblock Understanding over-squashing and bottlenecks on graphs via curvature.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{digiovanni2023oversquashing}
Francesco~Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro
  Lio', and Michael Bronstein.
\newblock On over-squashing in message passing neural networks: The impact of
  width, depth, and topology, 2023.

\bibitem{Twosides22}
Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.
\newblock Two sides of the same coin: Heterophily and oversmoothing in graph
  convolutional neural networks.
\newblock In Xingquan Zhu, Sanjay Ranka, My~T. Thai, Takashi Washio, and
  Xindong Wu, editors, {\em {IEEE} International Conference on Data Mining,
  {ICDM} 2022, Orlando, FL, USA, November 28 - Dec. 1, 2022}, pages 1287--1292.
  {IEEE}, 2022.

\bibitem{abboud2022shortest}
Ralph Abboud, Radoslav Dimitrov, and Ismail~Ilkan Ceylan.
\newblock Shortest path networks for graph property prediction.
\newblock In {\em The First Learning on Graphs Conference}, 2022.

\bibitem{song2023ordered}
Yunchong Song, Chenghu Zhou, Xinbing Wang, and Zhouhan Lin.
\newblock Ordered {GNN}: Ordering message passing to deal with heterophily and
  over-smoothing.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{mixhop19}
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina
  Lerman, Hrayr Harutyunyan, Greg Ver~Steeg, and Aram Galstyan.
\newblock Mixhop: Higher-order graph convolutional architectures via sparsified
  neighborhood mixing.
\newblock In {\em international conference on machine learning}, pages 21--29.
  PMLR, 2019.

\bibitem{FSGNN21}
Sunil~Kumar Maurya, Xin Liu, and Tsuyoshi Murata.
\newblock Improving graph neural networks with simple architecture design,
  2021.

\bibitem{DualNet22}
Sunil~Kumar Maurya, Xin Liu, and Tsuyoshi Murata.
\newblock Not all neighbors are friendly: Learning to choose hop features to
  improve node classification.
\newblock In {\em Proceedings of the 31st ACM International Conference on
  Information \& Knowledge Management}, CIKM '22, page 4334–4338, New York,
  NY, USA, 2022. Association for Computing Machinery.

\bibitem{sign_icml_grl2020}
Fabrizio Frasca, Emanuele Rossi, Davide Eynard, Benjamin Chamberlain, Michael
  Bronstein, and Federico Monti.
\newblock Sign: Scalable inception graph neural networks.
\newblock In {\em ICML 2020 Workshop on Graph Representation Learning and
  Beyond}, 2020.

\bibitem{chanpuriya2022simplified}
Sudhanshu Chanpuriya and Cameron~N Musco.
\newblock Simplified graph convolution with heterophily.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{GPR21}
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic.
\newblock Adaptive universal generalized pagerank graph neural network.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{meta04}
Wilhelm Huisinga, Sean Meyn, and Christof Schütte.
\newblock Phase transitions and metastability in markovian and molecular
  systems.
\newblock {\em The Annals of Applied Probability}, 14(1):419--458, 2004.

\bibitem{siam81}
B.~J. Matkowsky and Z.~Schuss.
\newblock Eigenvalues of the fokker–planck operator and the approach to
  equilibrium for diffusions in potential fields.
\newblock {\em SIAM Journal on Applied Mathematics}, 40(2):242--254, 1981.

\bibitem{Kramers90}
Peter H\"anggi, Peter Talkner, and Michal Borkovec.
\newblock Reaction-rate theory: fifty years after kramers.
\newblock {\em Rev. Mod. Phys.}, 62:251--341, Apr 1990.

\bibitem{sclimitations06}
Boaz Nadler and Meirav Galun.
\newblock Fundamental limitations of spectral clustering.
\newblock In {\em Proceedings of the 19th International Conference on Neural
  Information Processing Systems}, NIPS'06, page 1017–1024, Cambridge, MA,
  USA, 2006. MIT Press.

\bibitem{difmaps05}
Boaz Nadler, St\'{e}phane Lafon, Ronald~R. Coifman, and Ioannis~G. Kevrekidis.
\newblock Diffusion maps, spectral clustering and eigenfunctions of
  fokker-planck operators.
\newblock In {\em Proceedings of the 18th International Conference on Neural
  Information Processing Systems}, NIPS'05, page 955–962, Cambridge, MA, USA,
  2005. MIT Press.

\bibitem{TraceRatio07}
Huan Wang, Shuicheng Yan, Dong Xu, Xiaoou Tang, and Thomas Huang.
\newblock Trace ratio vs. ratio trace for dimensionality reduction.
\newblock In {\em 2007 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 1--8, 2007.

\bibitem{TraceRatio09}
Yangqing Jia, Feiping Nie, and Changshui Zhang.
\newblock Trace ratio problem revisited.
\newblock {\em IEEE Transactions on Neural Networks}, 20(4):729--735, 2009.

\bibitem{TraceRatio12}
T.~T. Ngo, M.~Bellalij, and Y.~Saad.
\newblock The trace ratio optimization problem.
\newblock {\em SIAM Review}, 54(3):545--569, 2012.

\bibitem{LINKX21}
Derek Lim, Felix~Matthew Hohne, Xiuyu Li, Sijia~Linda Huang, Vaishnavi Gupta,
  Omkar~Prasad Bhalerao, and Ser-Nam Lim.
\newblock Large scale learning on non-homophilous graphs: New benchmarks and
  strong simple methods.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{gao2019graph}
Hongyang Gao and Shuiwang Ji.
\newblock Graph u-nets.
\newblock In {\em international conference on machine learning}, pages
  2083--2092. PMLR, 2019.

\bibitem{Topk20}
Yujia Xie, Hanjun Dai, Minshuo Chen, Bo~Dai, Tuo Zhao, Hongyuan Zha, Wei Wei,
  and Tomas Pfister.
\newblock Differentiable top-k with optimal transport.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 20520--20531. Curran Associates, Inc., 2020.

\bibitem{Zhou03}
Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard
  Sch\"{o}lkopf.
\newblock Learning with local and global consistency.
\newblock In S.~Thrun, L.~Saul, and B.~Sch\"{o}lkopf, editors, {\em Advances in
  Neural Information Processing Systems}, volume~16. MIT Press, 2003.

\bibitem{doyle2000random}
Peter~G. Doyle and J.~Laurie Snell.
\newblock Random walks and electric networks, 2000.

\bibitem{Grady06}
L.~Grady.
\newblock Random walks for image segmentation.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  28(11):1768--1783, 2006.

\bibitem{pmlr-vR3-meila01a}
Marina Meil\u{a} and Jianbo Shi.
\newblock A random walks view of spectral segmentation.
\newblock In Thomas~S. Richardson and Tommi~S. Jaakkola, editors, {\em
  Proceedings of the Eighth International Workshop on Artificial Intelligence
  and Statistics}, volume~R3 of {\em Proceedings of Machine Learning Research},
  pages 203--208. PMLR, 04--07 Jan 2001.
\newblock Reissued by PMLR on 31 March 2021.

\bibitem{Cora}
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher,
  and Tina Eliassi-Rad.
\newblock Collective classification in network data.
\newblock {\em AI Magazine}, 29(3):93, Sep. 2008.

\bibitem{GEOM-GCN}
Hongbin Pei, Bingzhe Wei, Kevin~Chen{-}Chuan Chang, Yu~Lei, and Bo~Yang.
\newblock Geom-gcn: Geometric graph convolutional networks.
\newblock {\em CoRR}, abs/2002.05287, 2020.

\bibitem{actor}
Benedek Rozemberczki, Carl Allen, and Rik Sarkar.
\newblock Multi-scale attributed node embedding.
\newblock {\em CoRR}, abs/1909.13021, 2019.

\bibitem{GRAFF}
Francesco Di~Giovanni, James Rowbottom, Benjamin~P. Chamberlain, Thomas
  Markovich, and Michael~M. Bronstein.
\newblock Graph neural networks as gradient flows: understanding graph
  convolutions via energy, 2022.

\bibitem{GGCN}
Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.
\newblock Two sides of the same coin: Heterophily and oversmoothing in graph
  convolutional neural networks.
\newblock {\em CoRR}, abs/2102.06462, 2021.

\bibitem{GPRGNN}
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic.
\newblock Adaptive universal generalized pagerank graph neural network.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{GCNII}
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li.
\newblock Simple and deep graph convolutional networks.
\newblock {\em CoRR}, abs/2007.02133, 2020.

\bibitem{PairNorm}
Lingxiao Zhao and Leman Akoglu.
\newblock Pairnorm: Tackling oversmoothing in gnns, 2019.

\bibitem{CGNN}
Takenori Yamamoto.
\newblock Crystal graph neural networks for data mining in materials science.
\newblock Technical report, Research Institute for Mathematical and
  Computational Sciences, LLC, Yokohama, Japan, 2019.
\newblock https://github.com/Tony-Y/cgnn.

\bibitem{ACM}
Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang,
  Xiao-Wen Chang, and Doina Precup.
\newblock Revisiting heterophily for graph neural networks, 2022.

\bibitem{penn94}
Amanda~L. Traud, Peter~J. Mucha, and Mason~A. Porter.
\newblock Social structure of facebook networks.
\newblock {\em CoRR}, abs/1102.2166, 2011.

\bibitem{ogbn}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock {\em CoRR}, abs/2005.00687, 2020.

\bibitem{SGDSIAM99}
Alan Edelman, Tom\'{a}s~A. Arias, and Steven~T. Smith.
\newblock The geometry of algorithms with orthogonality constraints.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  20(2):303--353, 1998.

\bibitem{SGDeigen18}
Zhiqiang Xu, Xin Cao, and Xin Gao.
\newblock Convergence analysis of gradient descent for eigenvector computation.
\newblock In {\em Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, IJCAI'18, page 2933–2939. AAAI Press, 2018.

\bibitem{SGD19}
Jing An, Lexing Ying, and Yuhua Zhu.
\newblock Why resampling outperforms reweighting for correcting sampling bias
  with stochastic gradients.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Qiu07CTembedding}
Huaijun Qiu and Edwin~R. Hancock.
\newblock Clustering and embedding using commute times.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  29(11):1873--1890, 2007.

\bibitem{Chandra89}
A.~K. Chandra, P.~Raghavan, W.~L. Ruzzo, and R.~Smolensky.
\newblock The electrical resistance of a graph captures its commute and cover
  times.
\newblock In {\em Proceedings of the Twenty-First Annual ACM Symposium on
  Theory of Computing}, STOC '89, page 574–586, New York, NY, USA, 1989.
  Association for Computing Machinery.

\bibitem{NADLER2006113}
Boaz Nadler, Stéphane Lafon, Ronald~R. Coifman, and Ioannis~G. Kevrekidis.
\newblock Diffusion maps, spectral clustering and reaction coordinates of
  dynamical systems.
\newblock {\em Applied and Computational Harmonic Analysis}, 21(1):113--127,
  2006.
\newblock Special Issue: Diffusion Maps and Wavelets.

\bibitem{beaini2021directional}
Dominique Beaini, Saro Passaro, Vincent L{\'e}tourneau, Will Hamilton, Gabriele
  Corso, and Pietro Li{\'o}.
\newblock Directional graph networks.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 748--758. PMLR, 18--24 Jul 2021.

\bibitem{ASGC}
Sudhanshu Chanpuriya and Cameron~N Musco.
\newblock Simplified graph convolution with heterophily.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas K{\"{o}}pf, Edward~Z. Yang, Zach DeVito, Martin Raison,
  Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and
  Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em CoRR}, abs/1912.01703, 2019.

\bibitem{pyg}
Matthias Fey and Jan~E. Lenssen.
\newblock Fast graph representation learning with {PyTorch Geometric}.
\newblock In {\em ICLR Workshop on Representation Learning on Graphs and
  Manifolds}, 2019.

\bibitem{pei2020geom}
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu~Lei, and Bo~Yang.
\newblock Geom-gcn: Geometric graph convolutional networks.
\newblock {\em arXiv preprint arXiv:2002.05287}, 2020.

\end{thebibliography}

}
\newpage
\section{Appendix}\label{appendix}
\subsection{Appendix A: Formal Results with Practical Implications}
For the sake of clarity, in this appendix we develop the key concepts of the theorems stated in the paper instead of providing detailed proofs. Our emphasis here is on the practical implications of each result. For more details, we refer the reader to the cited papers. 

\textbf{The Trace Ratio Problem.} 
 is achieved by  whose  columns are given by the eigenvectors of the  smallest eigenvalues  of the Laplacian . Then, , when the graph  with adjacency matrix  is connected. However,  does not necessarily minimize  . As a result, we have that 

where  are the sorted degrees. As a result, we have the following bounds: 

The definition of  plays a key role in the original trace-ratio optimization. Following~\citep{TraceRatio12}, such a problem is formulated in scalar terms, i.e. in terms of finding

Actually, for  we have have that 

Therefore, the trace-ratio problem can be solved by alternating two updating steps:
\begin{itemize}
\item[] : Given , apply the Lanczos method to obtain the  largest eigenvalues of the transition matrix  (the smallest of ) and their associated eigenvectors .
\item[] :  Given the current eigenvectors ,  update \:
\end{itemize}
In the above process, the update of  ensures the orthogonality constraint. 
 
\textbf{The Trace Ratio and SGD.} 
However, solving the trace-ratio problem through gradient descent drives us to a different solution from the eigenvectors of . For instance, consider the Dirichlet loss . Then, its gradient (supposing that the orthogonality is enforced by a complementary loss) is given by: 

Therefore,  implies , where  is the asymptotic value of the trace ratio. As a result, we have that the optimal  satisfy , i.e. the gradient descent converges to the (orthonormal) functions of  the normalized Laplacian  associated with the value . However, as  is not necessarily an eigenvalue of , but it is close to the Fiedler value , we denote  as a \textbf{Fiedler environment}. It is an environment since the  columns  are mutually orthonormal and close to the Fiedler vector  insofar their Dirichlet energies  satisfy  with  (\textbf{Theorem}~\ref{th:1}).

In our experiments, we have chosen the trace-ratio formulation because:
\begin{itemize}
    \item [\textbf{a)}] It leads an \textbf{implicit normalization} of the gradient , namely .  
\item [\textbf{b)}] The \textbf{gradient is more structured} when we apply the constrain , where  is the adjacency matrix. 
\end{itemize}

Regarding \textbf{a)}, our implicit normalization alleviates the problem of landing in local minima due to the orthonormalization constraint (that we also enforce in the global loss). As noted~\citep{SGDSIAM99},  constraints of the form  define a Riemannian manifold and the trace problem s.t. them is not geodesically convex. In~\citep{SGDeigen18} this is addressed by introducing a Riemannian gradient and retraction normalization. 

However, our main gain in performance is achieved when we address \textbf{b)} via the joint effect of normalization and . In our preliminary experiments, we compared the gradient when applying the constraint  vs the one when doing only . Discarding the biases, and the non-linearities in both cases, we have  vs . For simplicity, we consider the gradient wrt a single column, i.e. we analyze  vs 

Given a random initialization of , this vector plays the role of a random projector of the rows in . Following, the Johnson-Lindenstrauss Lemma,   tends to replicate the structure of the adjacency. Actually, if the entries of  then, those of the projection satisfy , where  is the degree of node . As a result, if we have  well-defined communities in the graph  with adjacency matrix , then the projection  is near piecewise constant (actually the norm of the th row is preserved:  ). As a result, the projection  is more structured than   and this is propagated and even amplified during gradient descent. In addition, the normalization of  is stronger than that of . 

Overall, when evaluating the performance in \textsc{Squirrel} and \textsc{Chameleon} using only  (i.e. using ) we only obtain  and . However, using  (gradient 
) leads to  and  respectively. 

Finally, a detailed impact of the two above formulations in the variance of the SGD as in~\citep{SGD19} is beyond the scope of this paper.  

\textbf{Assymptotic Diffusions.}  As explained above, optimizing the Dirichlet loss leads to Fiedler environments, i.e. the rows  contain the  nearly orthogonal eigenvectors with eigenvalues . Following~\citep{difmaps05}, the \emph{diffusion distance}  at time  between two nodes  and  is defined spectrally as: 


where  are the components from the stationary probability distribution , i.e. the eigenvector  corresponding to . In the above equation,  denote the true eigenvectors of the transition matrix , and  is the diagonal matrix with the corresponding eigenvalues. Thus, Eq.~\ref{eq:diffdist} can be explained in the following terms: 
\begin{itemize}
    \item [\textbf{a)}]  compare the probabilities that two random walks (one starting in  and another one in ) reach any other node  in time .
    \item [\textbf{b)}] The spectral interpretation relies on the spectral theorem applied to the transition matrix . As a result, , with . 
\end{itemize}
However, since determining what is the correct diffusion time is very hard (it is usually a hyperparameter in some GNNs), we are interested in the asymptotic diffusion distance . Qiu and Hancock~\citep{Qiu07CTembedding} determined that 

i.e. eigenvalues  of the normalized Laplacian  are used instead of those of . Actually, the right side of the above equation is the well-known \emph{commute times}~\citep{Chandra89} distance . Note that such a distance is dominated by the Fiedler value and vector:  and , respectively. This fact simplifies the interpretation of our approximate diffusion distance as follows:
\begin{itemize}
    \item [\textbf{a)}] Our approximated eigenvectors, contained in the  columns of  have eigenvalues (Dirichlet energies)  close to  (the optimal trace ratio achieved by the Dirichlet loss). 
    \item [\textbf{b)}] Theorethically, we have that the smallest  satisfies . Therefore, if we order  in ascending order, then we obtain  
    
    \item [\textbf{c)}] However, in the heterophilic regime (where the labels break the structure) we usually have . See for instance the Fiedler environments obtained for SBMs in Figure~\ref{fig:sbm} and the discussion below (\emph{Appendix B}). As a result, in practice we have
    
    This proofs \textbf{Corollary}~\ref{cor:1}.    
\end{itemize}

\textbf{Escape Probabilities.} Approximating commute times distances is very convenient for our jump-based analysis, since it is well known that the \emph{escape probability} is related to the commute times distance~\citep{doyle2000random}: . Escape probabilities are actually dependent on the spectral gap (approximated by the Fiedler value ). This is illustrated in the very first Figure of this paper (Figure~\ref{fig:jump}), where a random walker tries to escape from the community . A classic result~\citep{pmlr-vR3-meila01a} shows that the probability that a random walk started in its asymptotic () distribution  is transitioning from  to  in one step is , where  (in the Figure we have that ). 

Therefore, as , with  our jump-hierarchy is closer to that of the escape probability than choosing  as asymptotic diffusion distance. In addition, we are sensitive to the spectral gap since the Fiedler environment contains approximations of the Fiedler vector, and the spectral gap is approximated in turn by the Dirichlet energy of the Fiedler vector. 


\textbf{Clustering and Metastable States.} Minimizing the Dirichlet loss in conjunction with the classification loss (see Eq~\ref{eq:loss}) leads to a trade-off between two \emph{clustering} problems. On the one hand, we infer a piecewise-smooth latent space. On the other hand, we simultaneously try to preserve the structure of the input graph as much as possible. In both cases, we try to find metastable states. A metastable state is a concept borrowed from dynamical systems but basically, it is an equilibrium state in a random process (for instance the one defined by a random walk that tries to escape from a community in Figure~\ref{fig:jump}). Metastable states are also characterized by wells in potential functions , where  is a state and its probability is given by the Boltzmann distribution . 
Then, the characteristic relaxation processes and time scales of a given space are usually described by a Stochastic Diffusion Equation (SDE): 

where  denotes Brownian motion. In the above equation, we have a \emph{drag} term (the gradient) that drives the process to a deep well, and a \emph{random} term (the Brownian motion) that allows us to escape from local minima. During this process, we find different time scales: fast scales while we are moving through a given well, and slow scales when we try to escape from it. For instance, escaping from the right community in Figure~\ref{fig:jump} takes a long time which depends on the difference between the potential at the well  and that of the saddle point ~\citep{sclimitations06}. This time is in turn the inverse of the spectral gap, i.e. there is a spectral interpretation of the SDE. Such interpretation comes from the analysis of the Fokker-Planck equation:

This equation leads to the pdf of the SDE and it has a spectral interpretation. More precisely, the eigenvectors of  converge to the eigenfunctions  of the Fokker-Planck equation as follows~\citep{sclimitations06}\citep{NADLER2006113}:  

where  is the Laplacian and  the eigenstates (eigenvalues). As a result, we may use the Fiedler vector to characterize the separation between two clusters. The steepest the Fiedler vector, the better the separation (\textbf{Theorem}~\ref{th:3}). Interestingly, the third eigenvector  may not work well as a state separator when we have different spatial scales, as we will see in \emph{Appendix B}.

\textbf{Dirichlet Label Propagation.} As we mentioned in the main paper, our Dirichlet formulation is inspired by classical graph-based semi-supervised methods. The work in~\citep{Zhou03} poses the problem of propagating known labels  to unknown nodes . Let  be a  matrix where  means that node  has label  and  otherwise. Then, the optimal label of each node  is given by , where

being  the transition matrix and . The  matrix  works as a basic node representation (not exactly a latent space) since each of is  rows is stochastic. However, its construction exploits the powers of  as follows: 

and

In addition, we also noted that  is also the solution of the Dirichlet problem:

where  is a regularization parameter satisfying . The proof is obtained by setting the gradient to zero:

which leads to Eq.~\ref{eq:DirichletLocal}.

Concerning the relationship of this formulation with absorbing random walks~\citep{doyle2000random}, 
the main idea is to extend the  transition matrix  so that:
\begin{itemize}
    \item [\textbf{a})] We include an upper block with the  identity matrix . This block represents the  absorbing states, where . Then the  block  encodes the prior probabilities of reaching an absorbing state from a non-absorbing one. 
    \item [\textbf{b})] The absorbing probabilities are given by \;.
\end{itemize}
Finally, the random-walker version~\citep{Grady06} is quite similar to the above one, but reorganizes the Laplacian matrix (\textbf{Theorem}~\ref{th:2}).











\subsection{Appendix B: SBM Analysis}
The following experiment aims to illustrate the interplay between our novel measure of structural heterophily  and the extent of the spectral gap. We also show the Fiedler Environments and how they are influenced by the classification loss (labels). For each heterophilic regime, we show both the corresponding pairwise distance matrix (diffusion map) and the resulting homophiliation. 
\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width = \linewidth,height =7cm]{images/appendix1.pdf}}
    \caption{Structural Heterophily in SBMs. Left: The original homophilic graph (First row), its  eigenvectors (Second row), the pairwise distance matrix (Third row), and the resulting embedding. The remaining columns to the right have the same structure for increasing levels of structural heterophily. Note the evolution of the Fiedler Environments and the homophiliations. In all cases, we use  jumps.}
    \label{fig:sbm}
\end{center}
\end{figure*} 
We have depicted in Figure~\ref{fig:sbm} the main ingredients of our approach as a means of illustrating some technical details introduced in \emph{Appendix A}. In particular, when analyzing SBM graphs under structural heterophily we observe several interesting phenomena.  

\textbf{Original vs Learned}. Instead of precalculating the eigenvectors, as in Directional GNNs~\citep{beaini2021directional}, we learn them. Our learned (approximate) eigenvectors are relatively close to the Fiedler vector (in terms of how they discriminate the two classes). This is what we call \emph{Fiedler Environments} but, in a semi-supervised setting, i.e. the learned eigenvectors are reactive both to the Dirichlet loss and to the classification loss. Despite being noisy, the vectors in the Fiedler Environments are able of partitioning a class when needed (especially for high values of ). 
    
\textbf{Diffusion Map.}  Our pairwise distances are also reactive to semi-supervised classification. However, the Dirichlet loss tends to flatten the intra-communities distances as much as possible. Flattening is a mechanism to enforce intra-community diffusion in the homophilic regime. In the heterophilic regime, however, the diffusion map enforces exploration via high-order jumping (see lateral steps in the blue region and the loss of the red peak in the small community). 

\textbf{Embedding.} We can also see how the embedding is affected by structural heterophily. When we have a structural cluster with nodes of two classes, the respective embeddings are correctly separated, but the margin of this separation decreases as  increases. This can be seen in graphs that have high  1, where it is common to find subclusters of nodes that belong to the other classes.
\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width = \linewidth,height =7cm]{images/hetero_metrics.pdf}}
    \caption{Interplay between heterophily and the spectral gap . Left: Results wrt structural heterophily. Right: Results wrt node homophily.}
    \label{fig:sh-explain}
\end{center}
\end{figure*} 

\textbf{Interplay between  and the gap.} Finally, we extend the experiments of~\citep{ASGC} by incorporating a third axis in addition to variate  and . This new axis is the structural heterophily. 
We proceed as follows. We generate four \emph{basic SBMs} attending to increasing spectral gaps: . For each basic SBM we have generated six levels of increasing structural heterophily . In parallel, we also generate six levels of increasing node homophily as a means of complementing structural heterophily.

We show our results in Figure~\ref{fig:sh-explain}:
\begin{itemize}
    \item[\textbf{a)}] \textbf{Small Gaps help.} Our method is based on spectral clustering, which means that keeping the gap low factor is key. This helps our method to choose whether to jump outside the cluster looking for a node with the same label (Heterophilic regime) or to stay and only look around (homophilic regime). This common case is supported by our method without problems.
\item[\textbf{b)}] \textbf{Low/Medium Structural Heterophily.} If the structure is quite correlated with the label and the spectral gap is not too high, our method is able of achieving good results even when the structure is noisy.
\item[\textbf{c)}]  \textbf{Large Gaps lead to oversmoohing.} Our worst performance is achieved when the inter-class message passing is massive.   This leads to oversmothing due to the high connectivity of the graph. This high connectivity cannot be controlled by our pump (see the blue dots).
\end{itemize}
We have also performed the same experiment, but changing the structural heterophily measure to node homophily, in order to display the difference between both. Note that our measure fails when the spectral gap is large. This happens because the Dirichlet energy in a near-complete graph is minimal. This lack of structure leads  to consider that all the nodes are in the same cluster, i.e. is no heterophily).


\subsection{Appendix C: Experimental and Computational Details}
In this section, we provide details about the datasets (see Table~\ref{tab:datasets-table}) and all the parameters and configurations of our experiments (see Table~\ref{tab:datasets-tuning} in order to better clarify the architecture and the results. \textsc{Diffusion-Jump GNNs} is implemented in PyTorch~\citep{pytorch}, using PyTorch Geometric~\citep{pyg} and ogbn datasets~\citep{ogbn}. For reproducibility, code, and instructions are available on our GitHub with all the selected configurations and logs. We have also included the computational (See Figure~\ref{fig:topk-explain}) in order to clarify the derivability of  in PyTorch.
\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width = 0.6\linewidth,height =8cm]{images/computational_graph.pdf}}
    \caption{The Computational Graph for  jumps. All branches depend on the diffusion pump (top-left) except  (the Homophily Branch, top-right).}
    \label{fig:topk-explain}
\end{center}
\end{figure*} 
\begin{table}[ht]
\caption{Statistics of the datasets used in our experiments.}
\label{tab:datasets-table}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l c c c c} 

                \toprule
                                \textbf{Dataset} &  \textbf{Avg D} & \textbf{Density}  & \textbf{Node H} &\textbf{Class H}\\
                \midrule
                Texas & 1.77 & 0.0090 & 0.07 & 0.001  \\ 
                Wisconsin  & 2.05 &  0.0080 & 0.17 & 0.094 \\
                Cornell   &  1.62 & 0.0080 & 0.11 & 0.047\\
                Actor  & 3.94 & 0.0005 & 0.16 & 0.011\\
                
                Squirrel & 41.73 & 0.0080 & 0.09 & 0.025\\
                Chameleon  & 15.85 & 0.0070 & 0.10 & 0.062  \\
                Citeseer  & 2.73 & 0.0008  & 0.71 & 0.627   \\
                Pubmed  & 4.49 & 0.0002 &  0.79 & 0.664  \\
                Cora  & 3.89 & 0.0014 &  0.83 & 0.776 \\
                Penn94  & 3.89 & 0.0014 &  0.83 & 0.776 \\
                Ogbn-arXiv  & 7 & 0.0004 &  0.66 & 0.444 \\
                ArXiv-year & 7 & 0.0004 &  0.22 & 0.272 \\
                \bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table}


In the following Table~\ref{tab:datasets-tuning}, we include the hyperparameters that have yielded the best results during the experimentation phase. It is worth noting that the experiments were conducted using the same 10 random splits as in~\citep{pei2020geom}, training during 700 epochs and utilizing early stopping.

\begin{table}[ht]
\caption{Best hyperparameters for our datasets.}
\label{tab:datasets-tuning}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l c c c c c} 

                \toprule
                                \textbf{Dataset} &  \textbf{hidden channels} & \textbf{dropout} &
                                \textbf{lr} &
                                \textbf{weight decay} & \textbf{k/\#Jumps}\\
                \midrule
                Texas      & 64  & 0.2  & 0.03  & 0.0005 & 20  \\ 
                Wisconsin  & 64  & 0.5  & 0.03  & 0.0005 & 5  \\
                Cornell    & 128 & 0.5  & 0.03  & 0.001  & 5  \\
                Actor      & 16  & 0.2  & 0.03  & 0.0001  & 3\\
                Squirrel   & 128 & 0.5  & 0.003 & 0.0005 & 8\\
                Chameleon  & 128 & 0.35 & 0.003 & 0.0005 & 12  \\
                Citeseer   & 128 & 0.5  & 0.003 & 0.0005 & 5 \\
                Pubmed     & 128 & 0.3  & 0.01  & 0.0005 & 3 \\
                Cora       & 128 & 0.5  & 0.002 & 0.0005 & 5\\
                Penn94     & 16  & 0.5  & 0.001 & 0.0001 & 3\\
                Ogbn-arXiv & 128 & 0.3  & 0.01  & 0.0005 & 3\\
                ArXiv-year & 128 & 0.2  & 0.003 & 0.0005 & 3\\
                \bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table}













\end{document}
