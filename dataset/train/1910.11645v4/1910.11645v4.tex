\section{Experiments}
\label{sec:experiments}
In this section, we first conduct an experimental analysis to gain an insight into the effect of SagNets (Sec.~\ref{sec:exp-bias}), then perform comprehensive evaluation on a wide range of cross-domain tasks including domain generalization (DG) (Sec.~\ref{sec:exp-dg}), unsupervised domain adaptation (UDA) (Sec.~\ref{sec:exp-uda}), and semi-supervised domain adaptation (SSDA) (Sec.~\ref{sec:exp-ssda}) compared with existing methods.
All networks are pretrained on ImageNet classification~\cite{russakovsky2015imagenet}.


\subsection{Biases and Domain Gap}
\label{sec:exp-bias}
We examine the effect of SagNets on CNNs' inductive biases and domain discrepancy using \textbf{16-class-ImageNet}~\cite{geirhos2018generalisation} and the \textbf{texture-shape cue conflict stimuli}~\cite{geirhos2019imagenet}.
16-class-ImageNet is a subset of ImageNet containing 213,555 images from 16 entry-level categories.
The texture-shape cue conflict stimuli were introduced to quantify the intrinsic biases of CNNs, which consist of 1,280 images and share the same 16 categories as 16-class-ImageNet.
They are generated by blending the texture (style) and shape (content) from different images via style transfer~\cite{gatys2016image} (see examples in Fig.~\ref{fig:stimuli}), so that we can observe whether a CNN makes a decision based on the texture or shape. 

\cm{We train SagNets with a ResNet-18 baseline on 16-class-ImageNet only, using} SGD with batch size 256, momentum 0.9, weight decay 0.0001, initial learning rate 0.001, and cosine learning rate scheduling for 30 epochs.
The randomization stage (the stage after which the randomizations are performed. i.e. the number of stages in the feature extractor) is set to 2, and we vary the adversarial coefficient .


\paragraph{\textnormal{\textbf{Texture/Shape Bias.}}}
As proposed in \cite{geirhos2019imagenet}, we quantify the texture and shape biases of networks by evaluating them on the cue conflict stimuli and counting the number of predictions that correctly classify the texture or shape of images.
Specifically, shape bias is defined as the fraction of predictions matching the shape within the predictions matching either the shape or texture, and texture bias is defined \hs{in a similar way}.
Fig.~\ref{fig:shape-texture-acc} shows the texture/shape accuracy on the stimuli of the ResNet-18 baseline and SagNets with varying adversarial weight , which clearly demonstrates that SagNets increase the shape accuracy and decrease the texture accuracy.
This consequently increases the shape bias as shown in Fig.~\ref{fig:shape-texture-bias}, which is also equivalent to decreasing the texture bias.
Furthermore, the shape and texture biases are effectively controlled by the adversarial weight , i.e. the shape bias increases as  increases.
In practice, increasing  does not always improve the final accuracy because it makes the optimization more difficult, thus we need to find a fair trade-off, which we investigate in Sec~\ref{sec:exp-dg}.


\paragraph{\textnormal{\textbf{Domain Gap.}}}
We further investigate the capability of SagNets in reducing domain discrepancy.
We treat 16-class-ImageNet and the cue conflict stimuli dataset as two different domains because they share the same object categories but exhibit different appearances.
We then measure the distance between the two domains using the features from the penultimate layer of the network.
Following \cite{mingsheng2015learning}, we calculate a proxy -distance  where  is a generalization error of an SVM classifier trained to distinguish the examples from the two domains.
As illustrated in Fig.~\ref{fig:adistance}, SagNets effectively reduce the domain discrepancy as the adversarial weight increases.
By plotting the -distance against the shape bias as presented in Fig.~\ref{fig:adistance-bias},
we observe an explicit correlation between the bias and domain gap: shape-biased representation generalizes better across domains, which confirms the common intuition~\cite{Hosseini2018AssessingSB,geirhos2019imagenet,Hermann2019ExploringTO}.


\begin{table}[t]
\caption{Multi-source domain generalization accuracy (\%) on PACS.
Each column title indicates the target domain.
The results of our DeepAll baselines \hs{(standard supervised learning on the mixture of source domains)} and SagNets are averaged over three repetitions.
SagNet and SagNet refer to SagNets without content-biased learning and adversarial style-biased learning, respectively.
\hs{Our approach outperforms all competing methods, where each component of our method contributes to the performance improvements.}}
\begin{center}
\vspace{-4mm}
\scalebox{0.85}{
\begin{tabular}{lcccc|c}
\hline
 & Art paint. & Cartoon &  Sketch & Photo & Avg. \\
\hline
\multicolumn{6}{c}{AlexNet} \\
\hline
D-SAM & 63.87 & 70.70 & 64.66 & 85.55 & 71.20 \\
JiGen & 67.63 & 71.71 & 65.18 & 89.00 & 73.38 \\ 
Epi-FCR & 64.7 & 72.3 & 65.0 & 86.1 & 72.0 \\ 
MASF & {70.35} & {72.46} & {67.33} & \textbf{90.68} & {75.21} \\ 
MMLD & 69.27 & \textbf{72.83} & 66.44 & 88.98 & 74.38 \\ 
\hline
DeepAll & 65.19 & 67.83 & 63.75 & {90.08} & 71.71 \\ 
SagNet & \textbf{71.01} & 70.78 & \textbf{70.26} & 90.04 & \textbf{75.52} \\ 
\hline
\multicolumn{6}{c}{ResNet-18} \\
\hline
D-SAM & 77.33 & 72.43 & \textbf{77.83} & 95.30 & 80.72 \\
JiGen & 79.42 & 75.25 & 71.35 & {96.03} & 80.51 \\
Epi-FCR & 82.1 & 77.0 & 73.0 & 93.9 & 81.5 \\ 
MASF & 80.29 & {77.17} & 71.69 & 94.99 & 81.04 \\
MMLD & 81.28 & 77.16 & 72.29 & \textbf{96.09} & 81.83 \\ 
\hline
DeepAll & 78.12 & 75.10 & 68.43 & 95.37 & 79.26 \\
SagNet & 78.86 & 77.05 & 73.28 & 95.43  & 81.15 \\
SagNet & {82.94} & 76.73 & 74.74 & 95.07  & {82.37} \\
SagNet & \textbf{83.58} & \textbf{77.66} & {76.30} & 95.47  & \textbf{83.25} \\ 
\hline
\end{tabular}
}
\end{center}
\label{table:msdg}
\end{table}


 \begin{table}[t]
\caption{Multi-source domain generalization accuracy (\%) on Office-Home with a ResNet-18 backbone.}
\begin{center}
\vspace{-4mm}
\scalebox{0.85}{
\begin{tabular}{lcccc|c}
\hline
& Art & Clipart &  Product & Real-World & Avg. \\
\hline
D-SAM & 58.03 & 44.37 & 69.22 & 71.45 & 60.77 \\
JiGen & 53.04 & \textbf{47.51} & \textbf{71.47} & 72.79 & 61.20 \\ 
\cline{1-6}
DeepAll & 58.51 & 41.44 & 70.06 & 73.28 & 60.82 \\ 
SagNet & 60.00 & 42.85 & 70.11 & 73.12 & 61.52 \\ 
SagNet & 59.31 & 41.89 & 70.44 & \textbf{73.52} & 61.29 \\ 
SagNet & \textbf{60.20} & 45.38 & 70.42 & 73.38 & \textbf{62.34} \\ 
\hline
\end{tabular}
}
\vspace{-10mm}
\end{center}
\label{table:msdg-officehome}
\end{table}




 
\begin{figure*}[t]
\begin{center}
\
\subfigure[Accuracy \textit{vs} randomization stage]{
\label{fig:ab-sl}
\includegraphics[width=0.235\textwidth]{figs/ab_sloc2.pdf}
}
\
\subfigure[Accuracy \textit{vs} adversarial weight]{
\label{fig:ab-wadv}
\includegraphics[width=0.235\textwidth]{figs/ab_wadv2.pdf}
}
\
\subfigure[Domain discrepancy]{
\label{fig:pacs-adistance}
\includegraphics[width=0.235\textwidth]{figs/pacs_pad_v3.pdf}
}
\
\vspace{-1mm}
\caption{
Accuracy of SagNets on PACS with varying (a) randomization stage and (b) adversarial weight; (c) the domain discrepancy (-distance) between the source domains and the target domain.
Results are averaged over the 4 target domains and 3 repetitions, where error bars denote the standard deviation. 
}
\label{fig:ablation}
\vspace{-2mm}
\end{center}
\end{figure*}

\begin{table*}[t]
\caption{Single-source domain generalization accuracy (\%) on PACS averaged over three repetitions (A: Art Painting, C: Cartoon, S: Sketch, P: Photo).
}
\begin{center}
\vspace{-4mm}
\scalebox{0.85}{
\begin{tabular}{lcccccccccccc|c}
\hline
 & AC & AS & AP & CA & CS & CP & SA & SC & SP & PA & PC & PS & Avg.\\
\hline
ResNet-18 & 
62.3 & 49.0 & 95.2 & 
65.7 & 60.7 & 83.6 & 
28.0 & 54.5 & 35.6 & 
64.1 & 23.6 & 29.1 & 54.3 \\
JiGen & 
57.0 &	50.0 &	\textbf{96.1} &
65.3 &	65.9 &	85.5 &
26.6 &	41.1 &	42.8 &
62.4 &	27.2 &	35.5 & 54.6 \\ 
ADA & 
64.3  &	\textbf{58.5}  &	94.5  &
66.7  &	65.6 &	83.6 &
37.0  &	58.6  &	41.6  &
65.3 &	32.7  &	35.9 & 58.7  \\ 
SagNet & 
\textbf{67.1} & 56.8 & 95.7 & 
\textbf{72.1} & \textbf{69.2} & \textbf{85.7} & 
\textbf{41.1} & \textbf{62.9} & \textbf{46.2} & 
\textbf{69.8} & \textbf{35.1} & \textbf{40.7} & \textbf{61.9} \\ 
\hline
\end{tabular}
}
\end{center}
\label{table:ssdg}
\end{table*}




\iffalse
\begin{table*}[t]
\caption{Single-source domain generalization accuracy (\%) on PACS averaged over three repetitions (A: Art Painting, C: Cartoon, S: Sketch, P: Photo).
ResNet-18 and JiGen results are reproduced with their official code and optimal settings~\cite{carlucci2019domain}, while ResNet-18 and SagNet results are produced with our implementation.}
\begin{center}
\vspace{-4mm}
\scalebox{0.85}{
\begin{tabular}{lcccccccccccc|c}
\hline
& AC & AS & AP & CA & CS & CP & SA & SC & SP & PA & PC & PS & Avg.\\
\hline
ResNet-18& 59.1 &	49.2 &	\textbf{96.1} & 65.1 &	65.6 &	84.3 & 27.2 &	37.2 &	39.7 & 66.3 &	26.8 &	34.7 &	54.3  \\ 
JiGen & 57.0 &	50.0 &	\textbf{96.1} &65.3 &	65.9 &	85.5 &26.6 &	41.1 &	42.8 & 62.4 &	27.2 &	35.5 &	54.6 \\ 
\hline
ResNet-18 & 
62.3 & 49.0 & 95.2 & 
65.7 & 60.7 & 83.6 & 
28.0 & 54.5 & 35.6 & 
64.1 & 23.6 & 29.1 & 54.3 \\
SagNet & 
\textbf{67.1} & \textbf{56.8} & 95.7 & 
\textbf{72.1} & \textbf{69.2} & \textbf{85.7} & 
\textbf{41.1} & \textbf{62.9} & \textbf{46.2} & 
\textbf{69.8} & \textbf{35.1} & \textbf{40.7} & \textbf{61.9} \\ 
\hline
\end{tabular}
}
\end{center}
\label{table:ssdg}
\end{table*}
\fi






\iffalse
\begin{table}
\caption{TODO, averaged 3 three repetitions.}
\begin{center}
\scalebox{0.85}{
\begin{tabular}{cc|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
 \multicolumn{2}{c|}{Source} & \multicolumn{3}{c|}{Art Painting} & \multicolumn{3}{c|}{Cartoon} &\multicolumn{3}{c|}{ Photo} & \multicolumn{3}{c|}{Sketch} & \multirow{2}{*}{Average} \\ \cline{1-14}
 \multicolumn{2}{c|}{Target} & C &S & P& A&S&P&A&C&P&A&C&S& \\
 \hline
\multirow{2}{*}{TODO:cite} & Baseline &  &  &  &  & & & & & & & & & \\ 
& JiGen &  &  &  &  & & & & & & & & &  \\ 
\hline
\multirow{2}{*}{TODO:cite} & Baseline &  &  &  &  & & & & & & & & & \\ 
& SagNet &  &  &  &  & & & & & & & & & 61.4 \\ 
\hline
\end{tabular}
}
\end{center}
\label{table:ssdg}
\end{table}
\fi \begin{table*}[t]
\caption{Unsupervised domain adaptation accuracy (\%) on Office-Home with varying adaptation methods and their SagNet combinations (A: Art, C: Clipart, P: Product, R: Real-World).
SagNets consistently boost the performance when combined with various adaptation methods.
}
\begin{center}
\vspace{-4mm}
\scalebox{0.85}{
\begin{tabular}{l|c|cccccccccccc|c}
\hline
  Method & SagNet & AC & AP & AR & CA & CP & CR & PA & PC & PR & RA & RC & RP & Avg.\\
\hline
\multirow{2}{*}{ResNet-50} & & 41.3	&63.8	&71.4	&49.1	&59.6	&61.4	&46.8	&36.1	&68.8	&63.0	&45.9	&76.5	&57.0\\
 & \checkmark & \textbf{45.7}	&\textbf{64.1}	&\textbf{72.6}	&\textbf{49.6}	&\textbf{60.0}	&\textbf{63.5}	&\textbf{49.9}	&\textbf{40.7}	&\textbf{71.1}	&\textbf{64.8}	&\textbf{50.9}	&\textbf{78.1}	&
\textbf{59.2}\\
\hline
\multirow{2}{*}{DANN} & & 44.7	&62.7	&70.3	&47.1	&60.1	&61.4	&46.1	&41.7	&68.5	&62.3	&50.9	&76.7	&57.7\\
& \checkmark &\textbf{48.8}	&\textbf{65.2}	&\textbf{71.4}	&\textbf{50.3}	&\textbf{61.4}	&\textbf{62.5}	&\textbf{50.7}	&\textbf{45.7}	&\textbf{71.8}	&\textbf{65.4}	&\textbf{55.2}	&\textbf{78.6}	&\textbf{60.6}\\
\hline
\multirow{2}{*}{JAN}  &  & 
45.0    &63.3    &72.6    &
53.3    &\textbf{66.0}    &64.4    &
50.9    &40.8    &72.1    &
64.9    &49.4    &78.8    &60.1 \\
& \checkmark &
\textbf{50.1} 	&\textbf{66.8} 	&\textbf{73.9} 	&
\textbf{56.9}	&64.7	&\textbf{66.1} 	&
\textbf{54.9}	&\textbf{45.6}	&\textbf{75.2}	&
\textbf{70.0}	&\textbf{55.3}	&\textbf{80.1}	&\textbf{63.3}\\
\hline
\multirow{2}{*}{CDAN} & & 50.6	&69.0	&\textbf{74.9}	&54.6	&66.1	&67.9	&57.2	&46.9	&75.6	&69.1	&55.8	&\textbf{80.6}	&64.0\\
& \checkmark & \textbf{53.2}	&\textbf{69.2}	&\textbf{74.9}	&\textbf{55.9}	&\textbf{67.8}	&\textbf{68.6}	&\textbf{58.1}	&\textbf{51.8}	&\textbf{76.4}	&\textbf{69.8}	&\textbf{58.1}	&80.4	&\textbf{65.3}\\
\hline
\multirow{2}{*}{SymNet}  &  & 
47.2     &70.9     &77.4     &
64.5     &\textbf{71.7}     &73.0     &
63.3     &49.0     &\textbf{79.1}     &
74.2     &54.1     &82.7     &67.3  \\
& \checkmark &
\textbf{49.6} 	&\textbf{72.4} 	&\textbf{77.9} 	&
\textbf{63.9}	&71.1  	        &\textbf{72.5} 	&
\textbf{65.2}	&\textbf{51.0}	&\textbf{79.1}	&
\textbf{74.9}	&\textbf{56.1}	&\textbf{83.0}	&\textbf{68.0}\\
\hline
\end{tabular}
}
\end{center}
\vspace{-2mm}
\label{table:uda-officehome}
\end{table*}
 \begin{table*}[t]
\caption{Unsupervised domain adaptation accuracy (\%) on DomainNet (C: Clipart, P: Painting, S: Sketch, R: Real).
SagNets improve the performance over both ResNet-18 and CDAN~\cite{mingsheng2018conditional} baselines.
}
\begin{center}
\vspace{-4mm}
\scalebox{0.85}{
\begin{tabular}{l|c|ccccccc|c}
\hline
Method & SagNet & RC &  RP &  PC &  CS & SP & RS & PR &Avg.\\

\hline
 \multirow{2}{*}{ResNet-18} & & 53.1	&57.7	&52.4	&47.5	&52.0	&43.4	&\textbf{68.5}	&53.5\\
&   \checkmark & \textbf{54.4}	&\textbf{58.0}	&\textbf{53.1}	&\textbf{49.2}	&\textbf{52.2}	&\textbf{46.4}	&67.4	&\textbf{54.4}\\
\hline
 \multirow{2}{*}{CDAN} & & 53.0	&57.4	&52.3	&48.0	&52.3	&43.4	&\textbf{67.2}	&53.4\\
&  \checkmark&\textbf{54.4}	&\textbf{59.4}	&\textbf{52.8}	&\textbf{49.5}	&\textbf{52.4}	&\textbf{45.9}	&67.1	&\textbf{54.5}\\
\hline
\end{tabular}
}
\end{center}
\label{table:uda-domainnet}
\end{table*} \begin{table*}[t]
\caption{Semi-supervised domain adaptation accuracy (\%) on DomainNet (C: Clipart, P: Painting, S: Sketch, R: Real).
SagNets consistently improve the performance over various baselines in terms of backbone architectures and adaptation methods.
}
\begin{center}
\vspace{-4mm}
\scalebox{0.78}{
\begin{tabular}{l | l | c | cccccccccccccc|cc }
\hline
\multirow{2}{*}{Backbone} 
& \multirow{2}{*}{Method} 
& \multirow{2}{*}{SagNet} 
&\multicolumn{2}{c}{R  C}
&\multicolumn{2}{c}{R  P} 
& \multicolumn{2}{c}{P  C}  
& \multicolumn{2}{c}{C  S} 
& \multicolumn{2}{c}{S  P} 
& \multicolumn{2}{c}{R  S} 
& \multicolumn{2}{c|}{P  R}   
&\multicolumn{2}{c}{Avg.} \\ & &  & 1\scriptsize{-shot}&3\scriptsize{-shot} &1\scriptsize{-shot}&3\scriptsize{-shot}&1\scriptsize{-shot}&3\scriptsize{-shot} &1\scriptsize{-shot}&3\scriptsize{-shot}&1\scriptsize{-shot}&3\scriptsize{-shot}&1\scriptsize{-shot}&3\scriptsize{-shot} &1\scriptsize{-shot}&3\scriptsize{-shot} &1\scriptsize{-shot}&3\scriptsize{-shot}  \\ \hline

 \multirow{4}{*}{AlexNet}
&\multirow{2}{*}{S+T} & & 43.3 & 47.1 & 42.4 & 45.0 & 40.1 & 44.9 & 33.6 & 36.4 & 35.7 & 38.4 & 29.1 & 33.3 & \textbf{55.8} & \textbf{58.7} & 40.0 & 43.4 \\
& & \checkmark &\textbf{45.8}	&\textbf{49.1}	&\textbf{45.6}	&\textbf{46.7}	&\textbf{42.7}	&\textbf{46.3}	&\textbf{36.1}	&\textbf{39.4}	&\textbf{37.1}	&\textbf{39.8}	&\textbf{34.2}	&\textbf{37.5}	&54.0	&57.0	&\textbf{42.2}	&\textbf{45.1}\\
\cline{2-19}
& \multirow{2}{*}{MME} & & 48.9  &  55.6 &  48.0 &  49.0 &  46.7 & 51.7 &  36.3 &  39.4 &  39.4 &  43.0 &  33.3 &  37.9 &  56.8 &  \textbf{60.7} &  44.2 &  48.2  \\
& & \checkmark & \textbf{54.1}  & \textbf{58.6}  & \textbf{49.8} & \textbf{52.2} & \textbf{48.7} & \textbf{54.4} & \textbf{39.6} & \textbf{43.4} & \textbf{40.4} & \textbf{43.4 }& \textbf{39.7} & \textbf{42.8} & \textbf{57.0} & 60.3 & \textbf{47.0} & \textbf{50.7}  \\
\hline
\hline
\multirow{4}{*}{VGG-16}
& \multirow{2}{*}{S+T} & & 49.0 & 52.3 & 55.4 & 56.7 & 47.7 & 51.0 & 43.9 & 48.5 & 50.8 & 55.1 & 37.9 & 45.0 & \textbf{69.0} & \textbf{71.7}  & 50.5 & 54.3 \\
& & \checkmark &\textbf{51.8}	&\textbf{54.9}	&\textbf{57.8}	&\textbf{59.4}	&\textbf{50.4}	&\textbf{54.2}	&\textbf{48.9}	&\textbf{52.9}	&\textbf{53.1}	&\textbf{56.3}	&\textbf{45.6}	&\textbf{49.4}	&68.3	&70.9	&\textbf{53.7}	&\textbf{56.9}\\
\cline{2-19}
 & \multirow{2}{*}{MME} & &  60.6 &  64.1 &  63.3 &  63.5 &  57.0 &  60.7 & 50.9 &  55.4 &  \textbf{60.5} &  60.9 &  50.2 &  54.8 &  \textbf{72.2} &  \textbf{75.3}  &  59.2 &  62.1 \\
& & \checkmark & \textbf{64.9} & \textbf{67.8} & \textbf{64.5} & \textbf{66.0}  & \textbf{60.4} & \textbf{65.8} & \textbf{54.7} & \textbf{59.0} & 59.8 & \textbf{62.0} & \textbf{56.6} & \textbf{59.6} & 71.1 & 74.2 & \textbf{61.7} & \textbf{64.9} \\
\hline
\hline 
\multirow{4}{*}{ResNet-34} & 
\multirow{2}{*}{S+T}  & & 55.6 & 60.0 & 60.6 & 62.2 & 56.8 & 59.4 & 50.8 & 55.0 & 56.0 & \textbf{59.5} & 46.3 & 50.1 & 71.8 & \textbf{73.9} & 56.9 & 60.0 \\
& &\checkmark & \textbf{59.4  }& \textbf{62.0}& \textbf{61.9} & \textbf{62.9  }&\textbf{59.1} &\textbf{61.5} & \textbf{54.0} & \textbf{57.1} & \textbf{56.6} &59.0 &  \textbf{49.7}&\textbf{54.4}  &\textbf{72.2}  & 73.4 & \textbf{59.0} & \textbf{61.5}  \\
\cline{2-19}
&\multirow{2}{*}{MME} & & 70.0 &  72.2 &  67.7 &  69.7 &  69.0 &  71.7 &  56.3 &  61.8 &  64.8 &  66.8 &  61.0 &  61.9 &  \textbf{76.1} & \textbf{78.5} &  66.4 &  68.9  \\
& & \checkmark & \textbf{72.3}  & \textbf{74.2}& \textbf{69.0} & \textbf{70.5 }&\textbf{70.8 }& \textbf{73.2} & \textbf{61.7} &\textbf{64.6}  & \textbf{66.9} &\textbf{68.3}  & \textbf{64.3} &  \textbf{66.1}& 75.3 & 78.4 & \textbf{68.6} & \textbf{70.8}  \\
\hline
\end{tabular}
}
\end{center}
\vspace{-2mm}
\label{table:ssda-domainnet}
\end{table*}





\iffalse
\begin{table*}[t]
\caption{SSDA accuracy (\%) on the DomainNet dataset (C: Clipart, P: Painting, S: Sketch, R: Real).
}
\begin{center}
\scalebox{0.6}{
\begin{tabular}{L{1.6cm} | L{2.2cm}  C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} C{0.9cm} |C{0.9cm} C{0.9cm} }
\hline
\multirow{2}{*}{Backbone} 
& \multirow{2}{*}{}  
&\multicolumn{2}{c}{R  C}
&\multicolumn{2}{c}{R  P} 
& \multicolumn{2}{c}{P  C}  
& \multicolumn{2}{c}{C  S} 
& \multicolumn{2}{c}{S  P} 
& \multicolumn{2}{c}{R  S} 
& \multicolumn{2}{c|}{P  R}   
&\multicolumn{2}{c}{Avg.} \\ & & 1\scriptsize{-shot}&3\scriptsize{-shot} &1\scriptsize{-shot}&3\scriptsize{-shot}&1\scriptsize{-shot}&3\scriptsize{-shot} &1\scriptsize{-shot}&3\scriptsize{-shot}&1\scriptsize{-shot}&3\scriptsize{-shot}&1\scriptsize{-shot}&3\scriptsize{-shot} &1\scriptsize{-shot}&3\scriptsize{-shot} &1\scriptsize{-shot}&3\scriptsize{-shot}  \\ \hline
\multirow{4}{*}{AlexNet} 
& S+T & 43.3 & 47.1 & 42.4 & 45.0 & 40.1 & 44.9 & 33.6 & 36.4 & 35.7 & 38.4 & 29.1 & 33.3 & 55.8 & 58.7 & 40.0 & 43.4 \\
& ENT & 37.0 & 45.5 & 35.6 & 42.6 & 26.8 & 40.4 & 18.9 & 31.1 & 15.1 & 29.6 & 18.0 & 29.6 & 52.2 & 60.0 & 29.1 & 39.8 \\
& MME & 48.9  &  55.6 &  48.0 &  49.0 &  46.7 & 51.7 &  36.3 &  39.4 &  39.4 &  43.0 &  33.3 &  37.9 &  56.8 &  \textbf{60.7} &  44.2 &  48.2  \\
& MME+SagNet & \textbf{54.1}  & \textbf{58.6}  & \textbf{49.8} & \textbf{52.2} & \textbf{48.7} & \textbf{54.4} & \textbf{39.6} & \textbf{43.4} & \textbf{40.4} & \textbf{43.4 }& \textbf{39.7} & \textbf{42.8} & \textbf{57.0} & 60.3 & \textbf{47.0} & \textbf{50.7}  \\
\hline



\hline 
\multirow{6}{*}{ResNet-34}
& S+T & 55.6 & 60.0 & 60.6 & 62.2 & 56.8 & 59.4 & 50.8 & 55.0 & 56.0 & \textbf{59.5} & 46.3 & 50.1 & 71.8 & \textbf{73.9} & 56.9 & 60.0 \\
& SagNet  & \textbf{59.4  }& \textbf{62.0}& \textbf{61.9} & \textbf{62.9  }&\textbf{59.1} &\textbf{61.5} & \textbf{54.0} & \textbf{57.1} & \textbf{56.6} &59.0 &  \textbf{49.7}&\textbf{54.4}  &\textbf{72.2}  & 73.4 & \textbf{59.0} & \textbf{61.5}  \\
\cline{2-18}
& ENT & 65.2 & 71.0 & 65.9 & 69.2 & 65.4 & 71.1 & 54.6 & 60.0 & 59.7 & 62.1 & 52.1 & 61.1 & 75.0 &  78.6 & 62.6 & 67.6 \\
& ENT+SagNet & \textbf{69.1}  &\textbf{72.7} & \textbf{67.2} & \textbf{70.0} &\textbf{67.7} & \textbf{73.6}& \textbf{54.7} & \textbf{62.6} & \textbf{60.1}& \textbf{64.8}&  \textbf{55.8}& \textbf{62.7} &  \textbf{75.2}&  \textbf{79.2}& \textbf{64.3} &\textbf{69.4}  \\
\cline{2-18}
& MME & 70.0 &  72.2 &  67.7 &  69.7 &  69.0 &  71.7 &  56.3 &  61.8 &  64.8 &  66.8 &  61.0 &  61.9 &  \textbf{76.1} & \textbf{78.5} &  66.4 &  68.9  \\
& MME+SagNet & \textbf{72.3}  & \textbf{74.2}& \textbf{69.0} & \textbf{70.5 }&\textbf{70.8 }& \textbf{73.2} & \textbf{61.7} &\textbf{64.6}  & \textbf{66.9} &\textbf{68.3}  & \textbf{64.3} &  \textbf{66.1}& 75.3 & 78.4 & \textbf{68.6} & \textbf{70.8}  \\
\hline
\end{tabular}}
\end{center}
\label{table:ssda-domainnet}
\end{table*}
\fi 

\subsection{Domain Generalization}
\label{sec:exp-dg}
DG is a problem to train a model on a single or multiple source domain(s), and test on an unseen target domain.
We evaluate the efficacy of SagNets against \hs{recent} DG methods including D-SAM~\cite{innocente2018domain}, JiGen~\cite{carlucci2019domain}, Epi-FCR~\cite{li2019episodic}, MASF~\cite{dou2019domain}, and MMLD~\cite{matsuura2020domain}. 
We adopt two multi-domain datasets: \textbf{PACS}~\cite{li2017deeper} consists of 9,991 images from 7 categories across 4 domains (Art Painting, Cartoon, Sketch, and Photo) and \textbf{Office-Home}~\cite{venkateswara2017deep} comprises 15,588 images from 65 categories and 4 domains (Art, Clipart, Product and Real-World).
We split the training data of PACS into 70\% training and 30\% validation following the official split~\cite{li2017deeper}, and Office-Home into 90\% training and 10\% validation following \cite{innocente2018domain}.
Our networks are trained by SGD with batch size 96, momentum 0.9, weight decay 0.0001, initial learning rate 0.004 (0.002 with AlexNet) and cosine scheduling for 2K iterations (4K with AlexNet or Office-Home).
The randomization stage and the adversarial weight of SagNets are fixed to 3 and 0.1, respectively, throughout all remaining experiments unless otherwise specified.


\paragraph{\textnormal{\textbf{Multi-Source Domain Generalization.}}}
We first examine multi-source DG where the model needs to generalize from multiple source domains to a novel target domain. 
We train our SagNets and the DeepAll baselines (i.e. na\"ive supervised learning) on the combination of all training data from the source domains regardless of their domain labels.
Table~\ref{table:msdg} and \ref{table:msdg-officehome} demonstrate that SagNets not only significantly improve the accuracy over the DeepAll baselines but also outperform the competing methods.
Furthermore, experiments without content-biased learning (SagNet) and adversarial style-biases learning (SagNet) verify the effectiveness of each component of SagNets.
It is also worth noting that while all the compared methods except JiGen exploit additional layers on top of the baseline CNN at test time, SagNets do not require any extra parameters nor computations over the baseline. 


\paragraph{\textnormal{\textbf{Ablation.}}}
We perform an extensive ablation study for SagNets on multi-source DG using PACS.
We first examine the effect of the randomization stage where the randomizations are applied, while keeping the adversarial weight to 0.1.
Karras et al.~\cite{karras2019style} demonstrated that styles at different layers encode distinct visual attributes: the styles from fine spatial resolutions (lower layers in our network) encode low-level attributes such as color and microtextures, while the styles from coarse spatial resolutions (higher layers in our network) encode high-level attributes such as global structures and macrotextures.
In this regard, the randomization modules of SagNets need to be applied at a proper level, where the style incurs undesired bias.
As shown in Fig.~\ref{fig:ab-sl}, SagNets offer the best improvement when the randomizations are applied after stage 3, 
while randomizing too low-level styles is less helpful in reducing style bias; randomizing too high-level styles may lose important semantic information.


We also conduct ablation on the adversarial coefficient , while the randomization stage is fixed to 3.
Fig.~\ref{fig:ab-wadv} illustrates the accuracy of SagNets with varying values of . 
Although increasing  tends to steadily improve the shape bias as shown in Fig.~\ref{fig:shape-texture-bias}, the actual performance peaks around .
This indicates that while increasing the shape bias leads to reducing domain discrepancy, it may complicate the optimization and damage the performance when exceeding a proper range.



\paragraph{\textnormal{\textbf{Domain Gap.}}}
Reducing domain gap in DG setting is particularly challenging compared with other tasks such as UDA or SSDA since one can not make any use of the target distribution.
To demonstrate the effect of SagNets in such scenario, we measure the -distances between the source domains and the target domain following the same procedure as in Sec.~\ref{sec:exp-bias}, and average them over the 4 generalization tasks of PACS.
As shown in Fig.~\ref{fig:pacs-adistance}, SagNets considerably reduce the domain discrepancy compared to the ResNet-18 baseline, JiGen\footnote{\label{fn:jigen}We provide additional comparison with JiGen because it does not use domain labels as same as ours.
The results are reproduced with their official code (\url{https://github.com/fmcarlucci/JigenDG}) and the optimal hyperparameters provided in their paper.
}, and ablated SagNets.


\paragraph{\textnormal{\textbf{Single-Source Domain Generalization.}}}
Our framework seamlessly extends to single-source DG where only a single training domain is provided, because it does not require domain labels nor multiple source domains (which are necessary in the majority of DG methods~\cite{li2017deeper,innocente2018domain,li2019episodic,dou2019domain}).
We train SagNets on each domain of PACS and evaluate them on the remaining domains.
As reported in Table~\ref{table:ssdg}, SagNets remarkably boost the generalization performance, while JiGen and \cm{ADA\footnote{
We re-implement their method on top of our baseline, which gives better performance than reproduction based on their published code.
}~\cite{volpi2018generalizing} are technically applicable to single-source DG but outperformed by our method.}


\subsection{Unsupervised Domain Adaptation}
\label{sec:exp-uda}
UDA is a task of transferring knowledge from a source to the target domain, where unlabeled target data are available for training.
Besides \textbf{Office-Home} that we used for DG, we also employ \textbf{DomainNet}~\cite{peng2019moment} which is a large-scale dataset containing about 0.6 million images from 6 domains and 345 categories.
Since some domains and classes are very noisy, we follow \cite{saito2019semi} to utilize its subset of 145,145 images from 4 domains (Clipart, Painting, Sketch, and Real) with 126 categories, and consider 7 adaptation scenarios.
We demonstrate that SagNets are not only effective when used alone, but also able to complement popular UDA methods such as DANN~\cite{ganin2016domain}, JAN~\cite{long2017deep}, CDAN~\cite{mingsheng2018conditional}, and SymNet~\cite{zhang2019domain} to make further improvements.
\cm{
We reproduce all compared methods and their SagNet variants on top of the same baseline and training policy for fair comparison.}



As shown in Table~\ref{table:uda-officehome}, SagNets lead to impressive performance improvements when combined with any adaptation method.
Furthermore, while CDAN does not make meaningful improvement on DomainNet due to the complexity of the dataset in terms of scale and diversity, SagNets consistently improve the accuracy in most adaptation scenarios as presented in Table~\ref{table:uda-domainnet}.
These results indicate that the effectiveness of SagNets is orthogonal to existing approaches and generates a synergy by complementing them.


\subsection{Semi-Supervised Domain Adaptation}
\label{sec:exp-ssda} 
We also conduct SSDA on \textbf{DomainNet} where a few target labels are provided.
Saito et al.~\cite{saito2019semi} showed that common UDA methods~\cite{ganin2016domain,mingsheng2018conditional,saito2017adversarial} often fail to improve their performance in such scenario, and proposed minimax entropy (MME) setting the state-of-the-art for SSDA.
Here we show that SagNets are successfully applied to SSDA, bringing further improvements over MME.
For a fair comparison, SagNets are built upon the official implementation of MME\footnote{\scriptsize{\url{https://github.com/VisionLearningGroup/SSDA_MME}}}, trained using the same training policy and the few-shot labels specified by \cite{saito2019semi}.


Table~\ref{table:ssda-domainnet} illustrates the results of SSDA with various architectures, where S+T indicates the baseline few-shot learning algorithm based on cosine similarity learning~\cite{chen2019closerfewshot}. 
Our method consistently improves the performance with considerable margins in most adaptation tasks with respect to all tested architectures and baseline methods, which verifies its scalability to various conditions.



