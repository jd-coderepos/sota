In this Section, we present in detail our proposed approach. We start with an outline of Normalizing Flows~\cite{rezende15flows} and the SMPL body model~\cite{loper2015smpl}.  Then, we describe the model architecture and the training procedure. Finally, we show how our trained model can be used in downstream applications in a simple and straightforward manner.

\subsection{Normalizing Flows}
Let  be a random variable with distribution   and  an invertible mapping. If we transform  with , then the resulting random variable  has probability density function:


Normalizing Flow models are used to model arbitrarily complex distributions as a series of invertible transformations of a simple base distribution. Typically, the base distribution  is chosen to be the standard multivariate Gaussian . If we write  as a composition of invertible transformations  with ,  and , then the log-probability density of  can be computed as:


Winkler~\etal~\cite{winkler2019learning} extended Normalizing Flow models to model conditional distributions  by using transformations  that are bijective in  and .

\subsection{SMPL model}

SMPL~\cite{loper2015smpl} is a parametric human body model. It defines a mapping  that takes as input a set of pose parameters  and shape parameters  and outputs a body mesh , where  is the number of mesh vertices. Additionally, given an output mesh, the body joints  can be expressed as a linear combination of the mesh vertices, , where  is a pretrained linear regressor.

\subsection{Model design}

Without loss of generality, we present our pipeline for the case where the input is an image of a person and the target output is the set of SMPL body model parameters. We call this model \textbf{ProHMR}, with the goal of \emph{Probabilistic Human Mesh Recovery}. At the end of this section we also show how the same method can be applied in alternative scenarios with different input and output representations.

In our setting, we are given an input image  containing a person, and our goal is to learn a distribution of plausible poses for that person conditioned on . Since we do not have access to accurate pairs of images-shape annotations, we choose to only model the uncertainty of the SMPL pose parameters . Our architecture follows closely the HMR paradigm~\cite{kanazawa2018end}. The output of our network is the conditional probability distribution  as well as point estimates for the shape and camera parameters  and  respectively.

The complete pipeline is depicted in Figure~\ref{fig:method}. 
Given an input image , we encode it using a CNN  and obtain a context vector .  We model  using Conditional Normalizing Flows. We learn a mapping  that is bijective in  and , \ie,  and .

We employ Normalizing Flows instead of simpler alternatives such as Mixture Density Networks (MDN)~\cite{li2019cvpr} because of their expressiveness and ability to model more complex distributions, as we show later in the evaluation section. In our setting, Normalizing Flows have also clear advantages over VAEs, since VAEs do not offer an easy way to compute the likelihood of a given output sample, which is crucial when using our model in downstream tasks.

Our Normalizing Flow model is based on the Glow architecture~\cite{kingma2018glow}. Each building block  is comprised of 3 basic transformations:

where  (Instance Normalization),  (Linear transformation) and  (Additive coupling). To make the inversion and the Jacobian computation faster, in the linear transformation we parametrize the  decomposition of . The final flow model is obtained by composing four of these building blocks.

The selected flow model allows us to perform both fast likelihood computation and fast sampling from the distribution. At the same time, a very important property is that the determinant of the Jacobian does not depend on , which in turn means that the mode of the output distribution is:

This result allows us to use our model as a \emph{predictive model} in a straightforward way; in the absence of any additional side-information, we make predictions using the mode of the output distribution.

To regress the camera and the SMPL shape parameters, we use a small MLP  that takes as input the context vector~ and outputs a single point estimate, \ie, . We also experimented with having  and  depend on , but there was no observable improvement.

\subsection{Training objective}
Let us assume that we have a collection of images paired with SMPL pose annotations. Typically, Normalizing Flow models are trained to minimize the negative log-likelihood of the ground truth examples , \ie the loss function is:


However, for the task of 3D pose estimation, 3D annotations are generally not available except for a small number of indoor datasets captured in constrained studio environments~\cite{ionescu2014human3, mehta2017monocular} and methods trained on those datasets fail to generalize in challenging in-the-wild scenes. Consequently, previous methods like~\cite{kanazawa2018end} propose to use examples with only 2D keypoint annotations and minimize the keypoint reprojection loss jointly with an adversarial prior. To make such a mixed training possible within our framework, we propose to minimize the expectation of the above error with respect to the learned distribution, \ie,

To make this loss differentiable we use the \emph{Law of the Unconscious Statistician} and rewrite the expectation as:


Conceptually, even though we do not have ground truth annotations, to maximize the conditional probability of these examples we can still constrain the form of the output distribution by forcing the output samples to have low reprojection error on average and lie on the manifold of valid poses. As in the case of VAEs~\cite{kingma2014auto}, we approximate the expectation by drawing a single sample from the prior.

As mentioned previously, our goal is to use our model not only as a generative model but also as a predictive model. Thus, we propose to exploit the property that for each image , the mode  of the output distribution corresponds to the transformation of . We do this by explicitly supervising  with all the available annotations as in a standard regression framework and minimize:

where  is the loss on the available 3D annotations (3D joints and/or SMPL parameters) whenever they are available. As we show in the experimental section, this explicit supervision of the mode of the output distribution helps boost the performance of our model in predictive tasks.

It is important to mention that  is not redundant in the presence of ; the behavior of the mode is not indicative of the full distribution, whereas  encourages the distribution to have certain desirable properties.

Finally, for modeling rotations we use the 6D representation proposed in \cite{zhou2018continuity}. One issue with this particular representation is that it is not unique. For example, for any 3D vectors  and ,  and  are mapped to the same rotation matrix. Empirically we found that putting no constraints on the 6D representation results in large discrepancy between examples with full 3D SMPL parameter supervision and examples with only 2D keypoint annotations. Among other things, this caused mode collapse for the examples without 3D ground truth. Thus, we introduce another loss function  that forces the 6D representations of the samples drawn from the distribution to be close to the orthonormal 6D representation.

Eventually, the final training objective becomes:


\subsection{Downstream applications}
In this part we show how our learned conditional distribution can be used in a series of downstream applications. We highlight that all these applications refer to \emph{test-time} processing with the same trained model without any special per-task training. Examples of such tasks are shown in \figurename~\ref{fig:downstream}. These applications fall under the more general umbrella of Maximum a Posteriori estimation where we use all available evidence to make more informed predictions.

\paragraph{3D pose regression}
As already discussed in previous sections, we can use our model in conventional tasks such as 3D pose regression from a single image.
In the absence of additional evidence, the most appropriate choice for making predictions is to pick the mode  of the distribution.

\paragraph{Body model fitting}
SMPLify~\cite{bogo2016keep} is a popular method that fits the SMPL body model to a set of 2D keypoints using a traditional optimization approach. The objective is:

where  penalizes the weighted 2D distance between the projected model joints and the detected joints,  is a Mixture of Gaussian 3D pose prior,  is a pose prior penalizing unnatural rotations of elbows and knees and  is a quadratic penalty on the shape coefficients.

Fitting a parametric body model to 2D image landmarks is a very challenging and inherently ambiguous problem. The data term  is purely driven by the 2D keypoints and disregards rich information contained in the input image. SPIN~\cite{kolotouros2019spin} partially addresses this issue by using an image-based regression network that provides a good initialization for the optimization, helping the fitting to converge to a better minimum. However, the image information is only used in the initialization phase, as SMPLify does not incorporate explicit image-specific priors that prevent the pose to deviate arbitrarily far from the set of plausible poses for the given image. The drifting problem is also an important limitation of~\cite{joo2020eft}, forcing the approach to rely on good initialization and carefully chosen stopping criteria.

Motivated by these limitations, we propose to replace the weaker generic 3D priors  and  with an explicit pose prior  that models the likelihood of a given pose conditioned on the image evidence. Thus, the final optimization objective becomes:

As initialization for the fitting we use the mode  of the conditional distribution. In the experimental section we show that by using this learned image-based prior we are able to consistently improve the fitting results, both qualitatively and quantitatively, as reflected in the 3D metrics.

\paragraph{Multiple views fusion}
Although our model has been trained for single-image reconstruction, we can still use the learned conditional distribution to obtain refined pose estimates in the presence of multiple views of a person. Let us assume that we have a set  of uncalibrated views of the same subject. We partition the pose vector of each frame as  where  corresponds to the global rotation of the model and  is the body pose. We propose to refine the pose by minimizing the following objective:

where . The second term of the objective is equivalent to minimizing the squared distance between all pairs of poses.

\subsection{Additional details}
\noindent
\textbf{ProHMR}.
Following previous works~\cite{kanazawa2018end, kolotouros2019spin} we use ResNet-50~\cite{he2016deep} as the encoder. For the Normalizing Flows we use 4 building blocks . For more details about the architecture, datasets and the training hyperparameters we refer the reader to the supplementary material.

\noindent
\textbf{2D pose lifting}.
Complementary to ProHMR, we use our approach to lift 2D poses to 3D skeletons, as in Martinez~\etal~\cite{martinez2017simple}. We use the same Normalizing Flow architecture as in ProHMR. In this case the input is a set of 2D Hourglass detections~\cite{newell2016stacked} and the output is the 3D pose coordinates. For the encoder , instead of a CNN, we use the backbone from~\cite{martinez2017simple}. Since all examples have full 3D supervision, our training objective consists only of  and .

\noindent
\textbf{Downstream tasks}.
For the fitting procedure employed in the downstream tasks, we found it beneficial to perform the optimization in the latent space instead of the pose space directly (similarly to SMPLify-X~\cite{pavlakos2019expressive}). Thus, we leave  as a free variable and decode it into the pose vector . Also, since for our Normalizing Flow model the determinant of the Jacobian does not depend on , the likelihood term becomes .