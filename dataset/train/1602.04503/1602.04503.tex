\newif\ifdraft\draftfalse  \newif\ifanon\anonfalse     \newif\iffull\fullfalse   \newif\ifbackref\backreffalse \newif\ifsooner\soonerfalse
\newif\iflater\laterfalse
\newif\ifieee\ieeefalse
\makeatletter \@input{texdirectives} \makeatother

\documentclass[10pt, conference, compsocconf, letterpaper, times]{IEEEtran}

\def\IEEEbibitemsep{3pt}

\renewcommand{\ttdefault}{cmtt}



\usepackage{etex}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{url}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{stmaryrd}
\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n} \usepackage{xcolor}
\usepackage{listings}
\usepackage{paralist}
\usepackage{fancyvrb}
\usepackage{latexsym}
\usepackage{bm}
\usepackage{morefloats}
\let\labelindent\relax \usepackage[shortlabels,inline]{enumitem}
\usepackage{fancyhdr}
\usepackage[yyyymmdd,hhmmss]{datetime}
\usepackage{graphicx}
\usepackage[noadjust]{cite}
\usepackage{coqed}






\usepackage{tikz}
\usetikzlibrary{matrix,arrows,decorations.pathmorphing}



\usepackage{preamble}
\typicallabel{MkKey}
\renewcommand{\andalso}{\quad\;\;}

\pagestyle{plain}

\begin{document}

\title{{\Huge\bf Beyond Good and Evil}
\1ex] \textsuperscript{1}Inria Paris\qquad
  \textsuperscript{2}Universit\'{e} Paris Diderot (Paris 7)\qquad
  \textsuperscript{3}Universit\'{e} Paris 8\qquad
  \textsuperscript{4}University of Pennsylvania
\fi
}

\maketitle

\begin{abstract}
Compartmentalization is good security-engineering practice. By
breaking a large software system into mutually distrustful components
that run with minimal privileges, restricting their interactions to
conform to well-defined interfaces, we can limit the damage caused by
low-level attacks such as control-flow hijacking.  When used to defend
against such attacks, compartmentalization is often implemented
cooperatively by a compiler and a low-level compartmentalization
mechanism. However, the formal guarantees provided by such {\em
  compartmentalizing compilation} have seen surprisingly little
investigation.

We propose a new security property, \emph{secure compartmentalizing
  compilation (SCC)}, that formally characterizes the guarantees
provided by compartmentalizing compilation and clarifies its attacker
model. We reconstruct our property by starting from the
well-established notion of fully abstract compilation, then
identifying and lifting three important limitations that make standard
full abstraction unsuitable for compartmentalization. The connection
to full abstraction allows us to prove SCC by adapting established
proof techniques; we illustrate this with a compiler from a simple
unsafe imperative language with procedures to a compartmentalized
abstract machine.
\end{abstract}

\ifsooner
\bcp{is it truly
the connection to FA that allows us to do this?}\ch{yes? hard to
believe?}\bcp{hard to understand.  but if it's justified later, then OK.}
\fi







\ifsooner
\ch{We're indeed using established proof techniques like trace
  semantics, but my impression is that they do need some
  non-trivial(?) adapting to our setting. If that's indeed the case
  then we should not sell short that adapting work. Yannis?}
\yj{Yes, there is some non-trivial adapting work. I can try to add
  more about this in the intro when my section is finished.}
\ch{For now I've changed ``using [established proof techniques]'' to
    ``adapting'' throughout, but we can make this more explicit in the
    intro and \autoref{sec:instance}.}
\fi

\section{Introduction}
\label{sec:intro}

Computer systems are distressingly insecure.
Visiting a website, opening an email, or serving a client request is often
all it takes to be subjected to a control-hijacking attack.
These devastating low-level attacks typically exploit memory-safety
vulnerabilities such as buffer overflows, use-after-frees, or double
frees, which are abundant in large software systems.
Various techniques have been
proposed for guaranteeing memory safety~\cite{NagarakatteZMZ09, NagarakatteZMZ10, DeviettiBMZ08,
  Nagarakatte2013, NagarakatteMZ14, NagarakatteMZ15,
  interlocks_ahns2012, micropolicies2015, LowFat2013}, but
the challenges of efficiency~\cite{NagarakatteZMZ09, NagarakatteZMZ10},
precision~\cite{m7}, scalability~\cite{ZitserLL04}, backwards
compatibility~\cite{cheri_asplos2015}, and effective
deployment~\cite{DeviettiBMZ08, Nagarakatte2013, NagarakatteMZ14,
  NagarakatteMZ15, interlocks_ahns2012, micropolicies2015, LowFat2013,
  pump_asplos2015}
have hampered their widespread adoption.

Meanwhile, new mitigation techniques have been proposed to deal with the most
onerous consequences of memory  unsafety---for instance, techniques
aimed at preventing
control-flow hijacking even in unsafe
settings~\cite{Abadi2005, AbadiBEL09, Erlingsson07, TiceRCCELP14, BurowCBPNLF16}.
Unfortunately, these defenses often underestimate the power of the attackers
they may face~\cite{Erlingsson07, SnowMDDLS13, outofcontrol_ieeesp2014,
  DaviSLM14, EvansLOSROS15, EvansFGOTSSRO15}---if, indeed, they have any
clear model at all of what they are protecting against.
Clarifying the precise security properties and
attacker models of practical mitigation techniques is thus an important
research problem---and a challenging one, since a good model has to capture
not only the defense mechanism itself but also the essential features of the
complex world in which low-level attacks occur.



In this paper we focus on the use of
{\em compartmentalization}~\cite{GudkaWACDLMNR15, cheri_oakland2015,
  wedge_nsdi2008} as a strong, practical defense mechanism
against low-level attacks exploiting memory unsafety.
The key idea is to break up a large software system into
mutually distrustful components that run with minimal
privileges and can interact only via well-defined interfaces.
This is not only good software engineering; it also gives strong security
benefits.  In particular, control-hijacking attacks can compromise only
specific components with exploitable vulnerabilities, and thus only give the
attacker direct control over the privileges held by these components.
Also, because compartmentalization can be enforced by more coarse-grained
mechanisms, acceptable efficiency and backwards compatibility are generally
easier to achieve than for techniques enforcing full-blown memory safety.
\ch{They also protect against potentially malicious code. Although
  that's not the main focus here, things like NaCl are made to isolate
  untrusted plugins.}

\iffull
\bcp{Here and in the abstract, do we need to say ``when used this way''?
  I.e., when compartmentalization is not used this way, is it {\em never}
  implemented by compiler/runtime cooperation?}\ch{No clue, I'm just trying
  to focus attention to the use of compartmentalization from this paper.
  All other uses are irrelevant as far as I'm concerned.}\bcp{For me, qualifying the statement with ``When used this way'' actually
  focuses attention on the other ways it might be used and implemented.}\ch{I'm really only referencing to the first sentence in the previous
  paragraph, tried to make this more explicit.}\bcp{I know.  My question stands, but I think this is less important than
  most of the other remaining points, so let's save it for later.}\fi
When used as a defense mechanism against memory unsafety,
compartmentalization is often achieved via cooperation
between a compiler and a low-level compartmentalization
mechanism~\cite{KrollSA14, ZengTE13, JuglaretHAPST15, GudkaWACDLMNR15,
  PatrignaniDP16, cheri_oakland2015, cheri_asplos2015}.
In this paper we use {\em compartmentalizing compilation} to refer to
cooperative implementations of this sort.
The compiler might, for instance, insert dynamic checks and cleanup
code when switching components and provide information about
components and their interfaces to the low-level compartmentalizing
mechanism, which generally provides at least basic
isolation.
Two such low-level compartmentalization technologies are already widely
deployed: process-level privilege separation~\cite{Kilpatrick03,
  GudkaWACDLMNR15, wedge_nsdi2008} (used, \EG by OpenSSH~\cite{ProvosFH03}
and for sandboxing plugins and tabs in modern web browsers~\cite{ReisG09})
and software fault isolation~\cite{sfi_sosp1993} (provided, \EG by
Google Native Client~\cite{YeeSDCMOONF10}); many more
are on the drawing boards~\cite{micropolicies2015, sgx, PatrignaniDP16,
  cheri_oakland2015, cheri_asplos2015}.



So what security guarantees does compartmentalizing compilation provide,
and what, exactly, is its attacker model?
\ch{The usual? No longer think it's good but it's standard.}A good starting point for addressing these questions is the familiar notion
of {\em fully abstract compilation}~\cite{abadi_protection98,
  PatrignaniASJCP15, AgtenSJP12, abadi_aslr12, JagadeesanPRR11,
  FournetSCDSL13, AbadiPP13, AbadiP13, AhmedB11, AhmedB08, NewBA16}.
A fully abstract compiler toolchain (compiler, linker, loader, and
underlying architecture with its security mechanisms) protects the
interactions between a compiled program and its low-level environment,
allowing programmers to reason
soundly about the behavior of their code when it is placed in an arbitrary
target-language context, by considering only its behavior in arbitrary
source-language contexts.
In particular, if we link the code produced by such a compiler against
arbitrary low-level libraries---perhaps compiled from an unsafe language or
even written directly in assembly---the resulting execution will not be any less
secure than if we had restricted ourselves to library code written in the
same high-level language as the calling program.  

(Why is it useful to
restrict attention to attackers written in a high-level language?  First,
because reasoning about what attackers might do---in particular, what
privileges they might exercise---is easier in a high-level language.  And
second, because by phrasing the property in terms of low- and
high-level programs rather than directly in terms of attacker behaviors,
specific notions of privilege, etc., we can re-use the same property for
many different languages.)
\iflater
\ch{The parenthesis is good, but only motivates why we prefer to
  reason about high-level attackers, as opposed to security guarantees
  for low-level compartmentalization, by starting from fully abstract
  compilation first of all we reason about high-level {\em
    programs}. Tempted to turn this into full-fledged paragraph
  on why full abstraction is a good start (Task \#1).}
\ch{We want high-level reasoning despite low-level compromise/attacks,
  and the compiler is what relates the high and the low level, so we
  can't escape talking about it?}
\bcp{I think the parenthesis (which is now a paragraph by itself) is pretty
  good as it stands.  Refinements to do with compartmentalization belong
  in the next bit of the story.}
\fi

Since full abstraction works by partitioning the world into a program and
its context, one might
expect it to apply to compartmentalized programs as well:
some set of components that are assumed to be subject to control-hijacking
attacks could be
grouped into the ``low-level context,'' while some others that are assumed
to be immune to 
such attacks would constitute the ``high-level program.''  Full
abstraction would then allow us to reason about the possible behaviors
of the whole system using the simplifying assumption that the
attacker's injected behavior for the compromised components can be
expressed in the same high-level language as the good components.
\ifsooner
\bcp{I've tried to expand this a bit to spell out what
  full abstraction would mean for compartmentalization---I hope it is
  reasonably clear.\ch{looks reasonably clear, yes}
  But it leads me to wonder why we (I) ever thought that
  full abstraction would be a useful property in this setting: If some
  compartment is compromised by an attacker, why would I care what
  language the attacker wrote their injected behavior in??  Two things seem
  important: (1) The attacker can only exercise privileges that the
  vulnerable compartment possesses, and (2) The compromised compartment can
  only interact with other components according to the rules of the
  mediating interfaces.  But both of these seem independent of the language
  in which the attacker writes the injected behavior.
  \ch{I don't really see how (2) could ever be language independent. The
    whole notion of interface usually depends on things like types
    and general interaction model (say procedures vs methods vs
    closures or whatever).}
  \ch{My impression is that it's easier to reason about an arbitrary
    {\bf fully defined} high-level context, then about the compilation of
    an undefined context (\IE more or less arbitrary ASM).}
  So full abstraction
  seems like pretty clearly not the right thing.  However, we then need to
  be clear about why these objections don't also apply to our proposed
  definitions -- in other words, why is FA even a good starting point?}
\ch{I'm tempted to say let's think about such philosophical questions
  in peace after the deadline. Even if FA wasn't such a good starting
  point, it's not like we're going to change our starting point in the
  next 4 days. I think what we have is still nice and useful.}
\fi
Sadly, this intuition does not withstand closer examination.  Full
abstraction, as previously formulated in the literature, suffers
from three important limitations that make it unsuitable for characterizing
the security guarantees of compartmentalizing compilation.

First, fully abstract compilation assumes that the source language itself is
secure,
so that it makes sense to define target-level security with respect to the
semantics of the source language.
However, compartmentalization is often applied to languages like C and C++,
which do {\em not} have a secure semantics---the C and C++ standards leave
most of the security burden to the programmer by calling out a large number
of {\em undefined behaviors}, including memory-safety violations, that are
assumed never to occur.
Valid compilers for these languages are allowed to generate code that does
literally {\em anything}---in particular, anything a remote
attacker may want---when applied to inputs that lead to undefined behavior.
There is no way to tell, statically, whether or not a program may have
undefined behavior, and compilers do not check for this situation.  (Indeed,
not only do they not check: they aggressively exploit the assumption of no
undefined behaviors to produce the fastest possible code for well-defined
programs, often leading to easily exploitable behaviors when this assumption
is broken.)
The point of compartmentalizing compilation
is to ensure that the potential effects of undefined
behavior are limited to the compromise of the component in which it occurs:
other components can only be influenced by compromised
ones via controlled interactions respecting specified interfaces.

To characterize the security of compartmentalizing
compilation, we therefore need a formal property that can meaningfully
accommodate source languages in which components can be compromised
via undefined behavior.
Full abstraction as conventionally formulated does not fit the bill,
because, in order to preserve equivalences of programs with undefined
behavior, compilers must abandon the aggressive optimizations that are
the reason for allowing undefined behaviors in the first place.
To see this, consider C expressions {\tt buf[42]} and {\tt
  buf[43]} that read at different positions {\em outside} the bounds of a
buffer {\tt buf}.
These two programs are equivalent at the source level: they both
lead to arbitrary behavior.
However, a real C compiler would never compile these expressions to
equivalent code, since this would require runtime checks that many C
programmers would deem too expensive.



Second, fully abstract compilation makes an {\em open world} assumption
about the attacker context. While the context is normally required to
be compatible with the protected program, for instance by respecting
the program's typed interface, the structure and privilege of the context are
unrestricted (the full abstraction definition quantifies over {\em
  arbitrary} low-level contexts).
This comes in direct contradiction with the idea of least
privilege\ifsooner
\bcp{well, it's not in {\em direct} contradiction with the {\em
    idea}, it's in direct contradiction with the definition.}\ch{how
  does one define least privilege?}\bcp{I was just looking for a simpler way
of saying it}\ch{what definition are you referring to?}\bcp{I shouldn't have
said definition---I should have said it's not in direct contradiction with
the idea, but rather with the thing itself.  But my real complaint was just
that it seemed like an overly roundabout way of saying it.}\fi,
which is crucial to compartmentalization, and which relies on the fact
that even if a component is compromised, it does not immediately get
more privilege.
Compromised components cannot change the basic rules of the
compartmentalization game.
For instance, in this paper we consider a static compartmentalization
setting, in which the breakup of the application into components is
fixed in advance, as are the privileges of each component.
A security property suitable for this setting needs to be restricted
to contexts that conform to a fixed breakup into components with
static privileges.\footnote{In a setting where new components can be dynamically created and
  privileges can be exchanged dynamically between components, the
  details of this story will be more complicated; still, we expect any
  secure compartmentalizing compilation property to limit the ability
  of low-level attacker contexts to ``forge'' the privileges of
  existing components.}


\iffull
\bcp{Are we sure that there aren't already refinements of full abstraction
  that deal with this properly (e.g., by Amal, maybe)?  Seems like others
  must have hit this issue before, e.g. for programs with refs.}
\ch{I'm not aware, somebody should double check this. By the way,
  limiting privilege is important, but so is enforcing a fixed
  structure, and I would be very surprised if the reference people
  were doing that. And no, structure and privilege are not the same in
  my opinion. I don't think that being split into 3 chunks of 100KB
  each is a privilege. More details in a comment from \autoref{sec:fa-not-enough}}
\bcp{Let's postpone this for the next draft.}
\fi

Third, because the definition of full abstraction involves applying the
compiler only to a program and not to the untrusted context in which it
runs, a fully abstract compiler may choose to achieve its protection goals
by introducing just a single barrier around the trusted part to protect it
from the untrusted part~\cite{PatrignaniASJCP15, AgtenSJP12, LarmuseauPC15,
  PatrignaniCP13, patrignani_thesis}.
Such compilation schemes force the programmer to commit in advance to a
single compromise scenario, \IE to a single static split of their
application into a ``good'' trusted program and an ``evil'' untrusted
context from which this \iffull trusted\fi program has to be protected.
This is not realistic in the setting of compartmentalizing compilation,
where we generally cannot predict which components may be vulnerable to
compromise by control hijacking attacks, and instead must simultaneously
guard against multiple compromise scenarios.
Compartmentalizing compilers allow us to build more secure applications
that go beyond the blunt trusted/untrusted distinction made by some fully
abstract compilers.
To describe their guarantees accurately, we thus need a new property that
captures the protection obtained by breaking up applications into multiple
mutually distrustful components, each running with least privilege, and that
permits reasoning about multiple scenarios in which different subsets of
these components are compromised.


Our main contribution is the definition of such a property,
which we call {\em secure compartmentalizing compilation (SCC)}
(\autoref{sec:sc}).
While similar in many respects to full abstraction, our property
escapes the three limitations discussed above.
First, it
applies to unsafe source languages with
undefined behaviors by introducing a new notion of {\em fully defined}
sets of components.
While undefined behavior is a property of whole programs, full definedness
is compositional.
Intuitively, a set of components is fully defined if they cannot be
{\em blamed}~\cite{FindlerF02prime} for undefined behavior
in any context satisfying fixed interfaces.
Second, SCC makes a {\em closed-world} assumption about compromised
components, enforcing the basic rules of the compartmentalization game
like the fixed division into components and the fixed privileges of
each component, including for instance with which other components it
is allowed to interact.
Third, SCC ensures protection for
multiple, mutually distrustful components; it does not assume we know in
advance which components are going to be compromised (i.e., in the C
setting, which components may contain exploitable undefined behaviors), but
instead explicitly quantifies over all possible compromise scenarios.

Our second contribution is relating SCC
to standard formulations of full abstraction both intuitively and formally
(\autoref{sec:fa-not-enough}).
We start from full abstraction and show how the three
limitations that make it unsuitable in our setting can be lifted
one by one.
This results in two properties we call {\em structured full abstraction} and
{\em separate compilation}, which can be combined and instantiated to obtain
SCC.
\ifsooner
\bcp{I was surprised that this
  didn't say ``which culminates in secure compartmentalization.''  Maybe we
  could say it this way in the introduction and leave discussion of the need
  for the last step, from MDFA to SC, as a detail for the technical
  part.}\ch{This part is still technically unclear; in particular we
  might only have implication not equivalence. We'll have to return to
  it anyway once we figure it out precisely.}
\fi
While our property directly captures the intuition of our
attacker model, reducing it to structured full abstraction is a useful
technical step, since the latter is easier to establish for specific
examples using a variant of existing proof techniques.
Moreover, arriving at the same property by two different paths
increases our confidence that we found the right property.

\ifsooner
\ch{Generally, the third contribution was the most work so trying to
  say more about it and why it's interesting. Please review.}
\bcp{It's good.  It would be even better if the three insights were
  rewritten to be grammatically parallel.}
\fi

Our third contribution is establishing the SCC property for a simple
unsafe imperative language with components interacting via procedure
calls and returns, compiling to an abstract machine with protected
compartments (\autoref{sec:instance}).
Despite the simplicity of the setting, this result gives useful
insights.  
First, the source language and compilation strategy enable interesting
attacks on components with potential buffer overflows, similar to those
found in C.
Second, we illustrate how SCC can be
achieved by the cooperation of a compiler (cleaning and restoring
registers) and a low-level protection mechanism (totally isolating
compartments and providing a secure interaction mechanism using calls
and returns).
Third, our SCC proof adapts a standard technique called {\em trace
  semantics}~\cite{JeffreyR05, PatrignaniC15}, via the reduction to
structured full abstraction.
The closed-world assumption about the context made by structured full
abstraction requires some nontrivial changes to the trace
semantics proof technique.

The remainder of the paper describes each of our three contributions in
detail (\autoref{sec:sc}--\autoref{sec:instance}) and closes by discussing
related work (\autoref{sec:related}) and future directions
(\autoref{sec:conclusion}).
The supplemental materials \ifanon submitted \else
associated \fi with this paper includes:
(a) a Coq proof for \autoref{thm:sfa-to-sc};
(b)~technical details and proofs for the SCC instance from
    \autoref{sec:instance} (while most of these proofs are done only on
    paper, the main structured full abstraction result,
    \autoref{thm:sfa-instance}, is also proved in Coq); and
(c) a trace mapping algorithm in OCaml using property-based
    testing to support \autoref{assumption:definability}.
These materials can be found at:
\url{https://github.com/secure-compilation/beyond-good-and-evil}

\iffalse
\clearpage
\section*{Discussions}

\ch{I think we should explicitly give a name to the MD instance of SFA
  that we use, say mutual distrust full abstraction (MFA),
  not just to SFA in general. In
  this case though, wouldn't MFA be just an equivalent reformulation of SC?}
\ch{Arthur? do you feel like proving both directions of that
  implication?}
\aaa{I'm not sure I see what equivalence you're talking about
  here---didn't we want to relate MD and SFA?}
\ch{I think if we have this we can claim a 3rd contribution:
  formally investigating the relation between SC to FA. We could
  then structure the whole paper around these 3 contributions:
  (1) SC; (2) from FA to SC; (3) SC instance}
\ch{Went for this already, although it would be stronger if we had the
  equivalence}

\apt{I'm unclear about this. Is SFA really different from FA?
  \ch{Yes, I think so. SFA is 2 steps away from FA and
    one step away from SC. \autoref{sec:fa-not-enough} will explain}
FA is usually defined in terms of possible low-level behaviors.
Doesn't our low-level system prohibit by construction behaviors
that violate interfaces?}
\ch{That part of the text has changed quite a lot since then.
  Maybe we can discuss this again once we have a first version
  of \autoref{sec:fa-not-enough}}

\apt{
``First, secure compartmentalization applies to unsafe source
languages with undefined behaviors, by requiring both the program and
the context to be {\em fully defined}.''
Here is where I get lost. How does it apply to languages with
undefined behaviors if it only applies to components that are fully
defined?   It is non-trivial to ensure that an
unsafe HLL component is fully defined (by defensive programming or wrapping
in dynamic contract checking or whatever).  Do we really think this
is a realistic model of what it means to work with an unsafe HLL?
  \ch{Maybe. We're making a worst-case assumption that if
      a set of components can be tricked by the remaining
      compromised ones into an undefined behavior, then they
      can be taken over and our compiler can provide no
      guarantees for them. I don't see how we could prove
      anything stronger than this for an unsafe language.}
Or, are we saying that, having analyzed what it means to use
secure low-level compartmentalization with unsafe languages,
we conclude that this is the \emph{best} we can hope for?
  \ch{So yes, I think we're in the second situation, although
      I wasn't yet attempting to make this point here. Maybe we should?}
Or have I missed the whole point somehow? (I do understand that
being able to consider only fully defined attacking contexts
makes it \emph{easier} to reason about the combined program---but
only when the attacked component is fully defined itself, right?)
  \ch{The restriction on the high-level context is a positive
     thing, since it enables reasoning. The restriction on
     the program is a necessary evil in our setting, but
     there are ways our new property sweetens the pill:
     both the closed-worldedness and the quantification
     over all compromise scenarios make this restriction
     more palatable. This is now explained in \autoref{sec:lfa}}}

\ch{What do other people think about the secure compartmentalization
  name? I like it more than mutual distrust, which I find overly general.}
\apt{But perhaps ``secure compartmentalization'' is too specific?
How about ``mutual abstraction'' ?}
\ch{Actually, I think specific is good here. I don't expect our
  property to meaningfully apply beyond compartmentalization,
  and it seems confusing to use a name that implies otherwise.}

\aaa{About ``full definedness'':
  To me, at a first glance, this looks weaker than full
  abstraction: isn't one of the points of full abstraction that we
  allow contexts not to be fully defined? (i.e., by allowing them to
  perform behaviors that a priori have no source-level counterpart.)}
\yj{The high-level attacker's purpose in FA and SC is to model the
  low-level one. So requiring more restrictions on the high-level
  attacker yields a stronger model (maybe this should be made more
  explicit in the sentence above). In fact, modeling any low-level
  attacker as a trivially non-defined high-level attacker is enough to
  prove full abstraction in a setting with undefined behaviors (say,
  one that voluntarily triggers buffers overflows on one of his
  buffers upon any procedure call), so full abstraction isn't
  interesting in such setting.\ch{Interesting idea, will have to think
    a bit about it though}
  Undefined attackers are trivially as
  powerful as low-level ones and can't be made less powerful than
  them, so you won't prove anything interesting about the power of
  low-level attackers if you allow to model them as undefined
  attackers (the property would say that low-level attackers are as
  powerful as low-level attackers).}
\ch{To answer Arthur's original question:
  In a sense it's both weaker (because of the full definedness
  conditions on the programs) and stronger (because of the full
  definedness conditions on contexts). This when looking at the
  interesting direction of full abstraction. Anyway, I discuss
  such details in \autoref{sec:lfa}}
\aaa{I think it was just a problem with how the sentence was
  structured: it used to say something like ``by requiring both the
  program and the context to be fully defined'', which could be
  interpreted as saying ``in order to use this framework, you have to
  prove that both program and context are fully defined''}
\ch{This was still a problem; changed to ``by considering {\em fully
    defined} programs and contexts'', please fix better if you wish.}
\fi















\section{Secure Compartmentalizing Compilation}
\label{sec:sc}

We start with an intuitive explanation of compartmentalizing compilation,
its attacker model, and its security benefits, and then introduce {\em
  secure compartmentalizing compilation (SCC)}.

We consider compartmentalization mechanisms provided by the
compiler and runtime system for an unsafe programming language with some
notion of components.\footnote{We use the term ``runtime system'' loosely to include operating
  system mechanisms~\cite{Kilpatrick03, GudkaWACDLMNR15, wedge_nsdi2008,
    ProvosFH03, ReisG09} and/or hardware
  protections~\cite{micropolicies2015,sgx,PatrignaniDP16,cheri_oakland2015}
  that may be used by the compiler.}
In \autoref{sec:instance} we will present a simple example in detail,
but for the present discussion it suffices to think informally of C or
C++ enriched with some compartmentalization mechanism.
This mechanism allows security-conscious developers to break large
applications into mutually distrustful components running with
least privilege and interacting only via well-defined interfaces.
We assume that the interface of each component also gives a precise
description of its privilege.
Our notion of interface here is quite generic: interfaces might include any
information that can be dynamically enforced on components, including module
signatures, lists of allowed system calls, or more detailed access control
specifications describing legal parameters to inter-component calls (\EG
ACLs for files).
We assume that the division of the application into components and the
interfaces of those components are statically determined and fixed
throughout execution.
In \autoref{sec:instance}, we instantiate this picture with a rather simple
and rigid notion of components and interfaces, where components don't
directly share any state and where the only thing one component can do to
another one is to call the procedures allowed by the interfaces of both
components.







We do not fix a specific compartmentalizing compilation mechanism; we
just assume that whatever mechanism is chosen can guarantee that, even
if one component is compromised (e.g., by a control-hijacking attack),
it will still be forced to adhere to its specified interface in its
interactions with other components.
What a compromised component {\em can} do in this model is use its access
to other components, as allowed by its interface, to trick them into
misusing their own privileges (confused deputy attacks) and/or attempt to
mount further control-hijacking attacks on other
components by communicating with them via defined
interfaces.


We do not assume we know in advance which components will be
compromised: the compartmentalizing compilation mechanism has to
protect each component from all the others.
This allows developers to reason informally about various compromise
scenarios and their impact on the security of the whole
application~\cite{GudkaWACDLMNR15}, relying on conditional
reasoning of the form: ``If {\em these} components get taken over and {\em
  these} do not, then {\em this} might happen (while {\em that} cannot),
whereas if these other components get taken over, then this other thing
might happen...''
If the practical consequences of some plausible compromise scenario are too
serious\iffull to ignore\fi,
developers can further reduce or separate privilege by
narrowing interfaces or splitting components, 
or they can make components more defensive by dynamically validating the
inputs they receive from other components.

For instance, developers of a compartmentalized web browser~\cite{ReisG09}
might reason about situations in which some subset of plugins and tabs gets
compromised and how this might impact the browser kernel and the remaining
plugins and tabs.
A possible outcome of this exercise might be noticing that, if the browser
kernel itself is compromised, then all bets are off for all the components
and the application as a whole, so the developers should put extra energy in
defending the kernel against attacks from compromised plugins or tabs.  On
the other hand, if interfaces {\em between} tabs and plugins are
appropriately limited, then compromise of one should not disrupt the rest.

Our goal is to articulate a security property that supports reasoning about
multiple compromise scenarios and clarifies the associated attacker model.
At the same time, our property is intended to serve as a benchmark for
developers of compartmentalizing compilation mechanisms who want to argue
formally that their mechanisms are secure.
In the rest of this section we explain the technical ideas behind the
SCC property and then give its formal definition.

An {\em application} is a set  of {\em components}, with
corresponding {\em interfaces} .
\iffull
\bcp{being pedantic, is this saying
that there are two sets and a bijection between them?}\ch{it's trying to say
  that we have a set of pairs; you can see that as a function from components
  to interfaces, but it will not necessarily be bijective.}\bcp{OK.  Not
  sure if it's worth being more pedantic about it.}
\ch{Actually, it might be a bijection, but whatever}
\fi
These components are separately compiled (individually compiling each
component in the set \ii{Cs} is written ) and linked
together (written ) to form an executable
binary for the application.


SCC quantifies over all {\em compromise
  scenarios}---i.e., over all ways of partitioning the components
into a set of compromised ones and a set of uncompromised ones.
In order to ensure that the set
  of compromised components doesn't expand during evaluation,
we require that the uncompromised components be {\em fully defined}
with respect to the interfaces of the compromised components.
That is, the uncompromised components must not exhibit undefined
behaviors
even if we replace the compromised components with arbitrary code (obeying
the same interfaces).



The full definedness condition is a necessary part of
the {\em static compromise model} considered in this paper.
Intuitively, if an uncompromised component can be tricked into an
undefined behavior by interface-respecting communication with other
components, then we need to conservatively assume that the already
compromised components will succeed in compromising this component
dynamically, so it belongs in the set of compromised components from
the start.
This static model is much simpler to reason about than a model of
dynamic compromise, in which one could perhaps provide guarantees to
not-fully-defined components up to the point at which they exhibit
undefined behavior, but which could, however, invalidate standard
compiler optimizations that involve code motion.
Moreover, it seems highly nontrivial to define our property for such a
more complex model.

\autoref{fig:compromise-scenario} illustrates one way to partition
five components  with interfaces ,
representing the scenario where , , and  are
compromised and  and  are not.
In order for this compromise scenario to be considered by our property,
 and  need to be fully defined with respect to interfaces
, , and , which means  and  cannot cause
undefined behaviors when linked with any components 
satisfying interfaces .

Formally, full definedness is a language-specific parameter to our
definition of SCC, just as the program equivalence relations are
language-specific parameters to both SCC and vanilla full abstraction.
For instance, in the simple imperative language in
\autoref{sec:instance}, we will say that components 
are fully defined with respect to a set of adversary interfaces
 if, for all components  
satisfying , the complete program
 cannot reduce
to a stuck non-final state (corresponding to undefined behavior) where
the currently executing component is one of the ones in
 (\IE no component in  can be
``blamed''~\cite{FindlerF02prime} for undefined behavior).
Full definedness might well be defined differently for another
language; for instance, in a concurrent language undefined behaviors
cannot be as easily modeled by stuckness since normally other threads
can proceed even if one of the threads is stuck.
One last thing to note is that full definedness of a set of components is
generally a much weaker property than the full definedness of each
individual component in the set. Since the interfaces of
the adversary components \ii{BIs} can (and in \autoref{sec:instance} do)
restrict not only the operations they export but also the operations they
import from \ii{Cs}, the components in the set can export dangerous
operations just to other components in the set; the components actually in
the set might then all use these operations properly, whereas arbitrary
components with the same interfaces could abuse them to trigger undefined
behaviors.



\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{img/compromise-scenario-cropped-14.pdf}
\caption{Compromise scenarios}
\label{fig:compromise-scenario}
\end{figure}
\begin{figure}[t!]
\centering
\includegraphics[width=0.8\linewidth]{img/secure-compartmentalization-more-formal-cropped-14.pdf}
\caption{SCC distinguishability game, for one of the
  compromise scenarios\ifsooner\bcp{Nit: maybe put yellow backgrounds on the two rows
  upstairs and the two rows downstairs?}\fi}
\label{fig:secure-compartmentalization-more-formal}
\end{figure}

SCC states that, \emph{in all such
  compromise scenarios}, the compiled compromised components must not be
able to cause more harm to the compiled uncompromised components via
low-level attacks than can be caused by some high-level components written
in the source language.
Basically this means that any low-level attack can
be mapped back to a high-level attack by compromised components satisfying
the given interfaces.
\iffull
\bcp{Rest of the paragraph is pretty subtle.  Spell out more?}\ch{Unsure
  what more there is to say}\fi
The property additionally ensures that the high-level components
produced by this ``mapping back'' are fully defined with respect to
the interfaces of the uncompromised components.
So with SCC,
instead of having to reason about the low-level consequences of
undefined behavior in the compromised components, we can
reason in the source language and simply replace the compromised
components by equivalent ones that are guaranteed to
cause no undefined behavior.

Formally, SCC is stated by quantifying over multiple
distinguishability games, one for each
compromise scenario, where the individual games are reminiscent of full
abstraction.
The goal of the attacker in each game is to distinguish between two
variants of the uncompromised components.
\autoref{fig:secure-compartmentalization-more-formal} illustrates
these two variants as  and , where we use  and
 to indicate that the behaviors of two (high- or
low-level) complete programs are distinguishable, \IE they produce
different observable outcomes when executed.
For this compromise scenario, SCC specifies that,
if compiled compromised components , ,
 can distinguish the  and
 variants at the low 
level, then 
there must exist some (fully defined) components 
that distinguish  and  at
the high level.

With all this in mind, the SCC property is
formally\ifsooner\bcp{CSF people are probably going to flag the word
  ``formally''}\bcp{maybe it's a bit better now, after recent changes...}\fi{}
expressed as follows:

\begin{defn}[SCC]~
\begin{itemize}
\item For any complete compartmentalized program and
for all ways of {\em partitioning} this program into
a set of {\em uncompromised} components 
and their interfaces ,
and a set of {\em compromised} components 
and their interfaces , so that
, and 
\item for all ways of replacing the uncompromised components with
  components  that satisfy the same interfaces 
  and are {fully defined} with respect to , 
\item if ,
\item then there exist components  satisfying interfaces 
  and {fully defined} with respect to  such that\\
         .
\end{itemize}
\end{defn}







As suggested before, our property applies to any {\em fully defined}
sets of components \ii{Cs} and \ii{Ds} (which cannot be dynamically
compromised by some components with interfaces \ii{BIs}\ifsooner\bcp{This way of
  saying it seems too strong: undefined behavior may not be the only way for
  one component to compromise another.}\ch{improvements welcome}\fi).
We conjecture that this full definedness precondition is strictly
required in the static corruption model we are assuming.
It is worth noting that we are not proposing any method for proving that
programs are fully 
defined; this comes with the territory when dealing with
C-like languages.
What we are after is bringing formal foundations
to {\em conditional} reasoning of the form ``if these \ii{Cs} are fully
defined and the remaining components \ii{Bs} get compromised,
then...''

Note that the  in our SCC definition need not be fully defined---\IE the
property allows the compromised components to contain undefined
behaviors (this may well be why they are compromised!) and promises
that, even if they do, we can find some other components 
that are able to distinguish between  and  in
the source language without causing any undefined behaviors.
Indeed, for those compromise scenarios in which  are already
fully defined, our SCC property trivially follows from correct
compilation (\autoref{assumption:compiler-correctness}) since in that case
we can always pick .

This generic property is parameterized over
  a source and a target language with a notion of component for each,
  source- and target-level notions of linking sets of components (),
  source- and target-level notions of distinguishability (),
  a compiler mapping source components to target components (),
  a source-level notion of interface and an interface satisfaction relation
  (lifted
to sets of components and interfaces), and
  a notion of a set of components  being
  fully defined with respect to a set of adversary interfaces .

\section{From Full Abstraction to SCC}
\label{sec:fa-not-enough}

\ifsooner
\bcp{Technical question: Is FA an instance of SC for a safe language?  I
  guess no, because we only consider compiled contexts, but is there any
  other reason?}
\fi

\autoref{sec:sc} presented SCC by directly
characterizing the attacker model against which it defends.
In this section we step back and show how SCC can instead be obtained by
starting from the well-established notion of full abstraction and
removing each of the three limitations that make it unsuitable
in our setting.
This results in two properties, {\em structured full abstraction} and {\em
  separate compilation}, which we then combine and instantiate to obtain SCC.
This reduction is
not only theoretically interesting, but also practically useful,
since structured full abstraction can more easily be proved by adapting
existing proof techniques, as we will see in \autoref{sec:instance}.



\ifsooner
\ch{It might be easier to first remove each of the limitations
  independently, and only then to remove all three at once. The
  problem is that things get rather complex when combining all 3
  fixes, so it might be better to explain the parts separately
  before getting there.}
\ch{Just noticed that the MFA idea only makes sense with undefined
  behaviors, so the only parallel split we can do is for SFA and LFA,
  but MFA needs to build on top of LFA}
\ch{If we go for this we would also need a good name for the
  combination of LFA, SFA, and MFA ... LSMFA? Alternatively, we could
  call the complete combination MFA, and call LFA + separate
  compilation something else; like CFA (for Compositional FA) or
  CLFA (for Compositional Low-Level FA}
\fi

\SUBSECTION{Full abstraction}
\label{sec:fa}

A {\em fully abstract} compiler protects compiled programs from their interaction
with unsafe low-level code and thus allows sound reasoning about security
(and other aspects of program behavior) in terms of the source language.
Fully abstract compilation~\cite{abadi_protection98}
intuitively states that no
low-level attacker can do more harm to a compiled program than some
program in the source language already could.
This strong property requires enforcing all high-level language
abstractions against arbitrary low-level attackers.


Formally, full abstraction is phrased as a distinguishability game
requiring that low-level attackers have no more distinguishing
power than high-level ones.

\begin{defn}\label{defn:fa-simple}
We call a compilation function (written ) {\em fully abstract}
if, for all  and ,

\end{defn}

\noindent Here,  and  are partial programs,  is a high-level
context whose job is to try to distinguish  from , and  is a
low-level ``attacker context'' that tries to distinguish  from
.
The relations  and  are parameters to the definition,
representing behavioral equivalence at the two levels.
To be useful, they should allow the context to produce an observable action
every time it has control, letting it convert its knowledge into
observable behaviors.
For instance, a common choice for behavioral equivalence is based on
termination: two deterministic programs are behaviorally equivalent if
they both terminate or both diverge.

When stated this way (as an implication rather than an equivalence),
full abstraction is largely orthogonal to compiler
correctness~\cite{leroy09:compcert, KumarMNO14}.
While compiler correctness is about preserving behaviors when
compiling from the source to the target, proving full abstraction
requires some way to map each distinguishing context target to a
sourge-level one, which goes in the opposite direction.
This is easiest to see by looking at the contrapositive:






\SUBSECTION{Problem 1: Undefined behavior}
\label{sec:prob1}

\ifsooner\ch{This was greatly simplified, please review}\fi

The first limitation of full abstraction is that it cannot realistically be
applied to compiling from an unsafe language with undefined behaviors.
Undefined behaviors are (arbitrarily!) nondeterministic, and no
realistic compiler can preserve this nondeterminism in the target
as required by full abstraction. (Removing it from the source language
would negate the performance and optimization benefits that are the
reason for allowing undefined behaviors in the first place.)





\iffalse
\ch{Problem statement: full abstraction does not apply to unsafe
  low-level languages or stated as in the intro (\autoref{sec:intro})
  ``fully abstract
  compilation assumes that the source language of our compiler is
  secure and that all low-level security has to be provided with
  respect to the secure semantics of this source programming
  language'' and ``we thus need a property that can meaningfully
  accommodate source languages in which components can be compromised
  via undefined behavior, while full abstraction simply assumes that
  the source language has no such thing.''}

\ch{\bf First, should concretely illustrate these (rather strong)
  claims. What exactly would happen if we tried to apply full
  abstraction to an unsafe language?}

\ch{New related claim: FA is impossible to instantiate in a meaningful
  way for a language with undefined behaviors. Probably too strong
  claim without a proof, but let's see how some natural instances
  trivialize.}

The main question we have to answer for instantiating FA is what
should be the behavior of a program that has undefined behavior?
(Tricky question, right?)
In particular, how should we define behavioral equivalence in the
source language () in the presence of undefined behaviors?
There are a couple of ``natural'' but broken choices that we will
investigate below.

\bcp{Based on today's discussions, I'm confused by the phrase ``undefined
  program''---it's not {\em programs} that are undefined, but program {\em
    executions}.}
\ch{We should make this explicit somewhere, but we are in a fully
  deterministic setting without external inputs or outputs, so each
  complete program has only a single execution. Things only get
  interesting for us when we look at partial programs, since in a way
  one can see the context as the input, and then yes, we call a
  partial program fully defined when it's defined for all ``context inputs''.}
\ch{My previous comment ignores the ``implicit nondeterminism''
  associated with undefined behaviors though.}

(1) We could take an undefined program be equivalent to all other
programs, but then because  is an equivalence this would make
any two programs equivalent to each-other (just by transitivity
via an undefined program). This would make building a fully-abstract
compiler impossible because there would be no high-level distinguishers
to which to map the low-level ones. The boring direction of full
abstraction would now become trivial.

(2) We could take any undefined program be equivalent to all other
undefined programs, but not be equivalent to any defined program.
\ch{Note that the semantics of undefined and defined programs
  will in fact overlap, and that's yet another thing we can use
  to break all definitions below.}\bcp{seems more natural to say that an
  undefined program is equivalent to any program (defined or undefined) that
  exhibits all behaviors, nondeterministically?}
\ch{This is partially true, but undefined behavior is worse than this:
  An undefined program can have not just all high-level behaviors but
  also all low-level behaviors. For instance, even though a defined
  program in your source language can't format your disk, doesn't mean
  that an undefined one can't.}
This would make building a fully abstract compiler at least as hard as building
a compiler that's intuitively safe for undefined behaviors.\apt{What
does this last phrase mean?}\ch{It tries to give high-level intuition
  for what's said below} For
illustrating this for our language from \autoref{sec:instance} we can
construct undefined partial programs  and  that simply expose
the whole memory of the process to the attacker: they take in an index
from the attacker and then read at that index in a buffer. Because
they hit undefined behavior no high-level context will be able to
distinguish these two behaviors. However, a low-level attacker can use
this ``backdoor'' to read the code of the programs and tell if there
is any difference in the syntax of  and , and we
can easily make sure that there is a difference when constructing 
and . So in order to be fully abstract a compiler would need to
stop the bad out of bounds read (but\apt{??}\ch{yes, we're not
  enforcing memory safety here} that's just memory safety), or in
general have additional safety mechanisms that go beyond just
compartmentalization.

(3) We could take any undefined program be equivalent only to itself
(\IE  be syntactic equality for programs with undefined
behavior; since  needs to be an equivalence thus reflexive this is the
strongest relation one can choose). This would make the interesting
direction of full abstraction trivial, because there always exists a
high-level context that causes an undefined behavior and uses the new
magic distinguishing power to distinguish any syntactically different but
semantically equivalent programs. Since no low-level context can
distinguish more than syntactic equality, any compiler would satisfy
the interesting direction of full abstraction. Or looking at the
contrapositive, the following property is not very interesting,
because it holds for all compilers:


Moreover, the usually boring direction of full abstraction
would now become stupidly strong:

This requires that low-level contexts are able to distinguish any
syntactically different programs. This is by no means a desirable
property of a compiler.

\apt{What about options in the middle between (2) and
(3), where  is a non-trivial relation based on a semantic
classification of the undefined behavior.  For one example, one could
label undefined behaviors by the state in which the program got stuck,
and define  as equality of labels. (This models guarding
potentially unsafe operations by checks that can raise an observable
fatal exception.) But I guess this would have the same consequences as
as (2): it would require more than just compartmentalization checking
in the compiled code.}

(4) We could just give up high-level reasoning and say that two
undefined complete programs  and  are only equivalent when
. Giving up high-level reasoning is already a
bad thing, but still let's follow along. Our property becomes:


\ch{Work in progress ... This whole discussion starts reminding me of
  the What If? comic book}

\bcp{I don't have anything technical to say about this yet, but I'm
  struck by the similarity between the issues you've raised about
  program equivalence and full abstraction and the `refinement
  paradox' for noninterference...}

\ch{It's an interesting discussion, but I'm
  tempted to move it all to an appendix and just say that we know
  of no way to do this and no work in the literature does it. It's
  not easy to understand, long, and it detracts from the main story.
  Moreover, with the new structure it would come on pg 4, so still at
  a point where most readers will look.
  It also encourages the reviewer to come up with their own broken
  definition, which we can then rebut in the response period?}
\fi 

To adapt full abstraction to a source language with undefined behaviors, we
need to restrict attention only to {\em defined}
complete programs in the source language.
\iffull
\bcp{why do we need this assumption here?  If the source language has a
  little bit of nondeterminism left, and the compiler is willing to preserve
  this nondeterminism in the target code, then everything is fine, right?
  \ch{Maybe, but we assume our target is deterministic from sentence 1}
  However, this discussion makes me wonder something more fundamental: Our
  main target is C / C++, the fully defined parts of these languages are
  still nondeterministic (argument order, etc.)\ch{Argument order is
    not nondeterministic, it's just a global parameter. Even concurrency
    can be expressed without nondeterminism by just using a scheduler
    oracle (\IE global parameter)}\bcp{OK about argument order.  About
    concurrency, I don't buy it: a reasonable compiler may well reduce the
    number of times that the scheduler is called.}
  and compilers do not (and
  would not want to) preserve this nondeterminism.  So aren't we back in the
  same difficulty?}\bcp{We had a long discussion about this by chat and then
  phone, but wound up not sure how to proceed, so we're going to ignore this
  for now}
\ch{I've added a note.}\bcp{I don't understand how the note addresses my
  question.  (I don't actually understand the note in the first place, so
  perhaps this is not surprising.)}

\ch{Q: Would CompCert satisfy this property? I'm a bit worried that
  because of ``implementation specific behavior'' in C, it would not
  (\IE a C compiler can refine non-determinism that's not undefined
  behavior).  But since we don't have such a thing we should be fine
  though. Anyway, I know very little about CompCert.}
\aaa{CompCert C is deterministic, so I think it does (although the
  semantics of the source language is stated as a deterministic
  refinement of a more general, nondeterministic semantics.)}
\bcp{This comment seems related to the discussion we had (that didn't
  converge) about unspecified behavior in C...}

\fi
And even with this restriction, defining full abstraction still
requires a little care. For instance, the following variant is
wrong (formally, {\em defined} is another parameter to this property):
Any  and  that both trigger undefined behavior as soon
as they get control would be considered equivalent in the high-level language
because there is no context that can make these programs defined
while observing some difference between them.
All such programs would thus need to be equivalent at the low level,
which is clearly not the case (since their nondeterminism can be resolved
in different ways by the compiler).
The problem here is that if  and  trigger undefined behavior
then the context often cannot make up for that and make the program
defined in order be able to cause an observation that distinguishes
 and .\ifsooner\bcp{got lost in that sentence}\ch{please improve}\fi

\SUBSECTION{Solution 1: Full abstraction for unsafe languages}
\label{sec:lfa}



The responsibility of keeping  defined should be thus shared
between  and .
For this we assume a compositional notion of {\em fully defined}
behavior for programs and contexts as two parameters to
\autoref{defn:lfa} below.
We require that these parameters satisfy the following properties: (1)
a program is fully defined if it does not cause undefined behavior in
any fully defined context, and (2) a context is fully defined if it
does not cause undefined behavior when we plug any fully defined
program into it.\ch{Above, should we have ``if'' or ``iff''?}
Note that properties (1) and (2) are circular and therefore cannot be
used as the definition of full definedness.
For specific languages (\EG the one in \autoref{sec:instance}) we can
break this circularity and define full definedness using {\em
  blame}~\cite{FindlerF02prime}: intuitively we call a partial program
{\em fully defined} when it cannot be blamed for undefined behavior in
any context whatsoever.
Similarly, we call a context fully defined when it cannot be blamed
for undefined behavior for any program that we plug into it.
We expect such a blame-based definition to satisfy the properties (1)
and (2) above.
\ifsooner
\ch{We haven't proved it though! We should at least try to sketch the
  proof. Nontrivial; we need to map an arbitrary high-level context
  that causes an undefined behavior in the program into a fully
  defined high-level context with the same behavior. Trace semantics
  for the high-level language?}
\fi
Full definedness allows us to introduce a new variant of
full abstraction that applies to unsafe source languages with
undefined behavior:





\begin{defn}[Full abstraction for unsafe languages]\label{defn:lfa}~\\
  We call a compiler  for an unsafe language {\em fully
    abstract} if for all {\em fully defined} partial programs  and
  
\end{defn}

Requiring that , , and  are fully defined means that we can safely
apply  to  and , because neither the programs nor
the context can cause undefined behavior.
This property is incomparable with the original definition of full
abstraction.
Looking at the contrapositive,

the  pre-condition makes this weaker than
full abstraction, while the  post-condition makes it stronger.
The post-condition greatly simplifies reasoning about programs 
by allowing us to replace reasoning about low-level contexts with
reasoning about high-level contexts {\em that cannot cause undefined
behavior}.

One might wonder whether the
 pre-condition is too restrictive,
since full definedness is a rather strong property, requiring each 
component to be very defensive about validating inputs it receives from
others.
In the static compromise model inherent to full abstraction and
without additional restrictions on the program's context, we must 
be conservative and assume that, if any context can cause
undefined behavior in a program, it can compromise it in a way that the
compiler can provide no guarantees for this program.
The structured full abstraction definition below will in fact restrict
the context and thus use a weaker notion of full definedness.
Moreover, separate compilation will allow us to quantify over all splits of a
program into a fully defined partial program and a compromised
context, which also makes the presence of the full definedness
pre-condition more palatable.

\SUBSECTION{Problem 2: Open-world assumption about contexts}
\label{sec:prob2}









While full abstraction normally requires the contexts
to be compatible with the partial program, for instance by respecting
the partial program's typed interface\iffull (see \autoref{app:fa-detail})\fi,
these restrictions are minimal
and do not restrict the shape\ifsooner\bcp{vague}\ch{for instance the breakup
  of the context into components}\fi, size,
\ifsooner\bcp{SC doesn't
  restrict the size either}\ch{It very well could. The only way we
  avoid this now in our instance is by making low-level components
  have infinite separate address spaces. If we went to a more
  realistic model of one single shared address space then hiding the
  sizes of things becomes very expensive and cumbersome, so one better
  option might be to expose sizes in the shape.}\bcp{Fair enough.  But it's
  a minor point in the present context, and IMO mentioning it distracts from
  the main point.}\fi
exported interface, or privilege
of the contexts in any way.
This {\em open world} assumption about contexts does not fit with our
compartmentalization setting, in which the breakup of the
application into components is fixed in advance, as are the
interfaces (and thus privileges) of all the components.
In our setting, the definition of full abstraction needs to be refined
to track and respect such structural constraints; otherwise a
low-level context with 2 components might be mapped back to a
high-level context with, say, 3 components that have completely different
interfaces, and thus privileges.
In particular, the high-level components' interfaces could give them
more privileges than the low-level components had, 
increasing their distinguishing power.

\ifsooner
\bcp{Because of the focus on shape, this reads like a technical issue rather
  than a fundamental conceptual one.  IMO think the emphasis should be
  on privilege.  Ideally, it should include an example showing why it makes
  a real difference.}
\ch{TODO: will try}
\ch{Removed ``Technically, \autoref{thm:sfa-to-sc} would fail without
  this change.'', since it contributed to your problem}
\fi

\ifsooner
\ch{This might still be relevant above, or not}
\yj{If there is a connection between the notion of low-level
compartment and a high-level notion of component, then we can more
accurately model low-level attackers as high-level attackers having
taken over the precise high-level components that correspond to the
compromised low-level components. This allows more precise reasoning
in the high-level than full abstraction, in which the corresponding
high-level attacker could have no relation whatsoever to the low-level
one, so that we can only give guarantees if we are safe
against \emph{any} high-level attacker. In the SFA setting, we can
give guarantees as long as we are safe against the high-level
attackers \emph{that have the same shape as the low-level attacker},
i.e., in our setting, against the attackers that control exactly the
components that we placed on the ``distrusted'' barrier.}
\fi

\SUBSECTION{Solution 2: Structured full abstraction}
\label{sec:sfa}

We therefore introduce a structured variant of full abstraction, in which
partial programs (indicated by  below) and contexts () are
assigned dual parts of predefined complete program {\em shapes}.
A shape might be anything, from a division into components with their
interfaces (as in \autoref{thm:sfa-to-sc} below), to, \EG the maximum
size of a component's code after compilation (which might expose component
sizes in a setting where it's too costly to hide them by padding to a fixed
maximum size~\cite{PatrignaniDP16}).


\begin{defn}[Structured full abstraction]\label{defn:sfa}~\\
  We say that a compiler  for an unsafe language
  satisfies {\em structured full abstraction} if, for all
  {\em program shapes}  and partial programs
     and  so that
     and  are {\em fully defined} with respect to contexts of shape
    ,
1em]
\Rightarrow(\forall a \hasshape{\circ} s.~ a[\comp{P}] \eql a[\comp{Q}])
.
\end{array}

\begin{array}{r}
\forall B.~\forall P,Q\textit{ fully defined}.\qquad
(\comp{(B[P])} \neql \comp{(B[Q])})\\
\Rightarrow (\exists A.~ A\textit{ fully defined} \wedge A[P] \neqh A[Q])
\end{array}

\begin{array}{l}
\forall B,P,Q,s.~
     P \eqhstat Q \land \ct{B}{P} \land \cl{B[P]} \land \\
     B \hasshape{\circ} s \land P \hasshape{\bullet} s \land Q \hasshape{\bullet} s \land
     P\text{ and }Q\text{ fully defined wrt. }{\circ}{s} \land\\
     (\comp{(B[P])} \neql \comp{(B[Q])})\\
    \Rightarrow
    \left(\begin{array}{rl}
    \exists A.& \ct{A}{P} \land \cl{A[P]} \land A \hasshape{\circ} s\\
      &A\text{ fully defined wrt. }{\bullet}{s}
     \land A[P] \neqh A[Q]
    \end{array}\right)
\end{array}

\begin{array}{l@{}l}
  \mathit{e} \ \ \mathrel{::=}\ \  \;&
    i
    \;|\;
    e_1 \otimes e_2
    \;|\;
    \texttt{if }e\texttt{ then }e_1\texttt{ else }e_2
    \;|\;
    b\texttt{[}e\texttt{]}
    \;|\;
  \\  &
    b\texttt{[}e_1\texttt{] := }e_2
    \;|\;
    C\texttt{.}P\texttt{(}e\texttt{)}
    \;|\;
    \texttt{exit}
\end{array}

\begin{array}{l@{}l@{~~~~~}l@{}l}
  \mathit{cfg} \ \mathrel{::=}\ \;& (C, s, \sigma, K, e)
  &
    \mathit{K} \ \mathrel{::=}\ \;&
    \texttt{[]}
    \;|\;
    E \texttt{::} K
\end{array}

\begin{array}{l@{}l}
  \mathit{E} \ \ \mathrel{::=}\ \  \;&
    \square{} \otimes e_2
    \;|\;
    i_1 \otimes \square{}
    \;|\;
    \texttt{if }\square{}\texttt{ then }e_1\texttt{ else }e_2
    \;|\;
    \\ &
    b\texttt{[}\square{}\texttt{] := }e_2
    \;|\;
    b\texttt{[}i_1\texttt{] := }\square{}
    \;|\;
    C\texttt{.}P\texttt{(}\square{}\texttt{)}
\end{array}

  \begin{array}{l@{}l}
  \mathit{instr} \ \ \mathrel{::=}\ \ \;&
    \con{Nop}
    \;|\;
    \con{Const}~i\rightarrow{}r_d
    \;|\;
    \con{Mov}~r_s\rightarrow{}r_d
  \\ &
    \;|\;
    \con{Load}~^{*}r_p\rightarrow{}r_d
    \;|\;
    \con{Store}~^{*}r_p\leftarrow{}r_s
  \\ &
    \;|\;
    \con{Jump}~r
    \;|\;
    \con{Jal}~r
    \;|\;
    \con{Call}~C~P
    \;|\;
    \con{Return}
  \\ &
    \;|\;
    \con{Binop}~r_1\otimes{}r_2\rightarrow{}r_d
    \;|\;
    \con{Bnz}~r~i
    \;|\;
    \con{Halt}
  \end{array}

\begin{array}{l}
\forall P.~ P\text{ defined} \Rightarrow \\
   \text{~~(1) }\TERM{P} \iff \TERM{\comp{P}}~\wedge \\
   \text{~~(2) }\DIV{P} \iff \DIV{\comp{P}}
\end{array}

\begin{array}{l}
\forall s, A \ash{}s, P \psh{}s. \\
~~ P\text{ fully defined wrt. contexts of shape }{\circ}{s} \Rightarrow \\
~~ A\text{ fully defined wrt. programs of shape
}{\bullet}{s} \Rightarrow \\
\text{~~~~(1) }\TERM{A[P]} \iff \TERM{\comp{A}[\comp{P}]}~\wedge \\
\text{~~~~(2) }\DIV{A[P]} \iff \DIV{\comp{A}[\comp{P}]}
\end{array}

  \begin{array}{r@{}l@{~~~~~~~}r@{}l}
  E\alpha\  \mathrel{::=}\; \ \;&
    \gamma!
    \;|\;
    \gamma?
  &
  \gamma \ \mathrel{::=}\; \ \;&
    \con{Call}_{\ii{reg}}~C~P
    \;|\;
    \con{Return}_{\ii{reg}}
    \;|\;
    \checkmark
  \end{array}

\begin{array}{l}
\forall t,\; s,\; p \psh{}s,\; a \ash{}s. \\
  ~~ ( t \in \ptr{s}{p} \wedge t.\gamma? \in \atr{s}{a} \Rightarrow
       t.\gamma? \in \ptr{s}{p} ) ~ \wedge \\
  ~~ ( t \in \atr{s}{a} \wedge t.\gamma! \in \ptr{s}{p} \Rightarrow
       t.\gamma! \in \atr{s}{a} )
\end{array}

\begin{array}{l}
  \forall s,\; p \psh{}s,\; a \ash{}s.~~
  \TERM{a[p]} \Rightarrow \\
  ~~\exists t.~~ \TTERM{t} \wedge t \in \ptr{s}{p} \cap \atr{s}{a}
\end{array}

\begin{array}{l}
  \forall t,\; s,\; p \psh{}s,~ a \ash{}s.~~
  t \in \ptr{s}{p} \cap \atr{s}{a} \Rightarrow \\
  ~~ ( \forall E\alpha.~(t.E\alpha) \not\in \ptr{s}{p} \cap \atr{s}{a} ) \Rightarrow \\
  ~~~~~(\TERM{a[p]} \iff \TTERM{t})
\end{array}

\begin{array}{l}
\forall t,\; s,\; P \psh{}s. \\
~~
P\text{ fully defined wrt. contexts of shape }{\circ}s \Rightarrow \\
~~~~  t \in \ptr{s}{\comp{P}} \iff \zeta_{\circ}(t) \in \ptr{s}{\comp{P}}
\end{array}

\begin{array}{l}
  \forall t,\, \gamma_1,\, s.~~
  t = \zeta_{\circ}(t) \wedge
  (\exists p \psh{}s.~ (t.\gamma_1!) \in \ptr{s}{p}) \Rightarrow \\
  ~~\exists A \ash{}s.~ A\text{ fully defined wrt. programs of shape }{\bullet}s ~\wedge \\
  \text{~~~~(1) }t \in \atr{s}{\comp{A}} ~\wedge \\
  \text{~~~~(2) }( \gamma_1 \neq \checkmark \Rightarrow
    (t.\gamma_1!.\checkmark?) \in \atr{s}{\comp{A}} ) ~\wedge \\
  \text{~~~~(3) }\forall \gamma.\;\text{if }
  \zeta(\gamma) \not= \zeta(\gamma_1) \text{ then } \forall \gamma'.~
  (t.\gamma!.\gamma'?) \not\in \atr{s}{\comp{A}}
\end{array}

\begin{array}{l}
  \text{(1) }t_c \in \atr{s}{\comp{A}}, \\
  \text{(2) }\gamma_1 \neq \checkmark \Rightarrow
  (t_c.\gamma_1!.\checkmark?) \in \atr{s}{\comp{A}}, \\
  \text{(3) }\forall \gamma,~ \gamma'.~~(t_c.\gamma!.\gamma'?) \in \atr{s}{\comp{A}} \Rightarrow

        \zeta(\gamma) = \zeta(\gamma_1).
\end{array}

\begin{array}{l}
  \text{(a) }t_c = \zeta_{\circ}(t_p) \in \ptr{s}{\comp{Q}}, \\
  \text{(b) }(t_c.\checkmark!) = \zeta_{\circ}(t_p.\checkmark!) \in \ptr{s}{\comp{Q}} \Rightarrow
  (t_p.\checkmark!) \in \ptr{s}{\comp{Q}}, \\
  \text{(c) }(t_c.\gamma_1!) = \zeta_{\circ}(t_p.\gamma_1!) \in \ptr{s}{\comp{Q}} \Rightarrow
  (t_p.\gamma_1!) \in \ptr{s}{\comp{Q}}.
\end{array}

\begin{array}{l}
\left(\begin{array}{rl}
\forall A.& \ct{A}{P} \land \cl{A[P]} \,\land\\
          &A~\textit{fully defined} \qquad\quad \Rightarrow A[P] \eqh A[Q]
\end{array}\right) \iff\\
\;(\forall a.~ \ct{a}{\comp{P}} \land\, \cl{a[\comp{P}]} \Rightarrow
  a[\comp{P}] \eql a[\comp{Q}])\\
\end{array}

\begin{array}{rl}
P \eqhstat Q \land \ct{A}{P} \land \cl{A[P]} & \Rightarrow\\
                  \ct{A}{Q} \land \cl{A[Q]} &
\end{array}

\begin{array}{rl}
p \eqlstat q \land \ct{a}{p} \land \cl{a[p]} & \Rightarrow\\
                  \ct{a}{q} \land \cl{a[q]} &
\end{array}

\begin{array}{l}
\left(\begin{array}{r}
\forall A \hasshape{\circ} s.\qquad \ct{A}{P} \land \cl{A[P]} \land\\
   A~\textit{fully defined}\text{ wrt. programs of shape }{\bullet}s\\
   \qquad\Rightarrow A[P] \eqh A[Q]
\end{array}\right)\iff\
\end{defn}

\yj{Isn't a shape preservation assumption about the compiler missing here?
  }

\yj{Could we simplify the property by putting everything in the shape?
  Or why would it be bad?
  \IE require that
  
  At least it seems to hold in my instance from \texttt{while.org}.
  (It might be that we already had this discussion and I don't
  remember the conclusion.)}
\ch{Don't remember either, but what I wrote down now corresponds
  quite directly to the Coq formalization, right?}

\fi

\end{document}
