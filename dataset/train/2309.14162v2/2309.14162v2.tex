

\documentclass[10pt,twocolumn,letterpaper]{article}

 \usepackage[pagenumbers]{cvpr} 
\usepackage[dvipsnames]{xcolor}
\newcommand{\red}[1]{{\color{black}#1}}
\newcommand{\cyan}[1]{{\color{cyan}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{placeins}	
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm2e}
\usepackage{enumitem}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\usepackage[percent]{overpic}
\usepackage{tcolorbox}
\tcbset{standard jigsaw, on line,
	boxsep=1.5pt, left=0pt,right=0pt,top=0pt,bottom=0pt,
	opacityback=0.8,opacityframe=.8,
	colframe=white,
	colback=white
}


 
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}

\def\paperID{1106} \def\confName{CVPR}
\def\confYear{2024}

\title{Data Upcycling Knowledge Distillation for Image Super-Resolution}



 

\author{\hspace{-0.4cm}Yun Zhang ~ Wei Li  ~ Simiao Li ~ Jie Hu ~ Hanting Chen\\
	\hspace{-0.4cm}Hailing Wang ~ Zhijun Tu ~ Wenjia Wang ~ Bingyi Jing ~ Yunhe Wang\thanks{Corresponding author.}\\
	DSA Thrust, INFO Hub, Hong Kong University of Science and Technology (GZ)\\
	Huawei Noah's Ark Lab\\
	Department of Statistics and Data Science, Southern University of Science and Technology\\
	{\tt\hspace{-0.6cm}yzhangjy@connect.ust.hk}\quad\quad {\tt\{wei.lee, lisimiao\}@huawei.com}
}


\begin{document}
\maketitle

\begin{abstract}
\noindent Knowledge distillation (KD) emerges as a  promising yet challenging technique for compressing deep neural networks, aiming to transfer extensive learning representations from proficient and computationally intensive teacher models to compact student models.
However, current KD methods for super-resolution (SR) models have limited performance and restricted applications, since the characteristics of SR tasks are overlooked.
In this paper, we put forth an approach from the perspective of effective data utilization, namely, the Data Upcycling Knowledge Distillation (DUKD), which facilitates the student model by the prior knowledge the teacher provided through the upcycled in-domain data derived from the input images.
Besides, for the first time, we realize the label consistency regularization in KD for SR models, which is implemented by the paired invertible data augmentations. It constrains the training process of KD and leads to better generalization capability of  the student model.
The DUKD, due to its versatility, can be applied across a broad spectrum of teacher-student architectures (e.g., CNN and Transformer models) and SR tasks, such as single image SR, real-world SR, and SR quantization, and is in parallel with other compression techniques. 
Comprehensive experiments on diverse benchmarks demonstrate that the DUKD method significantly outperforms previous art.
\end{abstract} \section{Introduction}\label{sec: introduction}

Single image super-resolution~(SISR) is a fundamental but challenging task within the field of computer vision~(CV), aiming to reconstruct high-resolution~(HR) images from their low-resolution~(LR) counterparts \cite{lim2017enhanced, zhang2018image, liang2021swinir}. Over the past decade, convolutional neural networks~(CNNs) \cite{dong2015image, kim2016accurate,lim2017enhanced,zhang2018image} and Transformers \cite{liang2021swinir, yang2020learning, wang2022uformer,zamir2022restormer} have demonstrated remarkable success in SISR tasks. Despite the impressive performance of deep learning-based SISR models, their practical deployment is typically hindered by the requirement of high computational resources and memory consumption \cite{zhang2021aligned}. Consequently, there has been a growing interest in developing SISR model compression methods to facilitate real-world applications, particularly on resource-limited devices \cite{Huang_2023_CVPR}. 

Knowledge Distillation~(KD) reduces the computational costs and memory requirements for practical model deployments while also considerably improving the performance of the student models. It's achieved by transferring ``dark knowledge'' from the well-performing but complex teacher model to the light and compact student model \cite{gao2019image, hui2019lightweight, zhang2021data}. In comparison to other model compression methods such as quantization \cite{gupta2015deep, hubara2016binarized,ignatov2021real,wu2016quantized}, pruning \cite{anwar2017structured, wang2021exploring,liu2019metapruning,qiao2022dcs} and the neural architectures search~(NAS) \cite{wu2019fbnet,howard2019searching,guo2020single}, the KD has received intensive attention due to its excellent performance and wide range of applications.

\begin{figure*}[t]  \centering
	\includegraphics[width=0.925\linewidth]{figure/dukd_framework_rounded2}
	\caption[Distillation Framework]{Framework of the DUKD method. DUKD facilitates the student with the prior knowledge provided by the teacher through upcycled in-domain data. The label consistency regularization is first realized in the field of KD for SR, and it enhances the generalization capability of the student. Details of these two modules are demonstrated in Fig.\ref{fig:methods}.}
\label{fig:framework}\vspace{-1em}
\end{figure*}


The efficacy of KD has been well-established in natural language processing (NLP) \cite{gou2021knowledge,sanh2019distilbert} and high-level CV tasks such as classification, detection and segmentation \cite{park2019relational,tung2019similarity,chen2017learning,huang2017like}. However, its application in SR tasks is comparatively less researched \cite{he2020fakd, wang2021towards, zhang2021data,lee2020learning}. The utilization of vanilla response-based KD methods \cite{hinton2015distilling} or those that are effective on high-level CV tasks \cite{romero2014fitnets, yim2017gift, zagoruyko2016paying} yields marginal improvements or may even suffer from detrimental effects when used to compress SR models, as noted by \citet{he2020fakd} and our experiments in Sec.~\ref{sec: experiments}.
Prior KD methods tailored explicitly for SR are primarily feature-based, 
forcing the student model to mimic the teacher model's network intermediate features directly
\cite{he2020fakd} or through a pre-trained perceptual characteristics extracting network like VGG \cite{Yao2022MTKDSRMK, wang2021towards}. However, these approaches have only marginal performance gains and limited applications. Practically, the teacher models' architecture is often invisible due to commercial, privacy, and safety concerns, rendering feature-based methods invalid in some real-world applications.

Rethinking the mechanism of KD in the context of SR tasks, we found that many of the previous explanations for why KD works no longer holds due to the intrinsic differences between CV tasks. Motivated by this observation, in this work, we take a distinctively innovative approach to KD for SR, shifting the paradigm from developing various knowledge types \cite{gou2021knowledge} to more effective data utilization, and present the \textit{Data Upcycling Knowledge Distillation (DUKD)}, a simple yet highly effective KD framework for SR models. 
Specifically, DUKD consists of two major modules: \textit{in-domain data upcycling} and \textit{label consistency regularization}. The in-domain data upcycling module utilizes the training data to generate auxiliary training examples which are then used to train the student model with \textit{only} the teacher model's supervision. The data upcycling process broadens the scope of knowledge transfer and allows a more comprehensive harnessing of the teacher model's capabilities rather than just providing an imitating target for the student model on the raw training data. Moreover, for the first time, we realize the label consistency regularization into the KD for SR by defining several \textit{invertible data augmentation} operations. As illustrated in Fig \ref{fig:framework}, the student model is forced to yield the same output as the teacher model, given the ``augmented'' inputs. The regularization makes the student model exposed to a diverse range of inputs, enhancing the training process and its generalization ability \cite{oliver2018realistic, jeong2019consistency, englesson2021consistency}.
Since the DUKD is independent of network architectures, it shows great universality among a diverse array of SR model families and SR tasks.
In summary, our main contributions are three-fold:
\begin{itemize}
	\item[] We propose DUKD, a versatile KD approach that is capable of compressing and accelerating various SR networks. It shifts the emphasis from knowledge types to optimal data utilization through in-domain data upcycling. \item[] We first realize the label consistency regularization to KD for low-level CV tasks by specifying several invertible data augmentations. It improves the model's robustness to perturbed input images.
	\item[] Our proposed DUKD, as shown through comprehensive experiments on various benchmark datasets, SR networks, and SR tasks in Sec.~\ref{sec: experiments}, applies broadly to multiple teacher-student configurations, promising a cutting-edge KD approach for SR. \end{itemize}

 \section{Related Works}\label{sec: related}
\subsection{Image Super-Resolution}
Deep neural networks~(DNNs) based image super-resolution have shown impressive success. Dong \textit{et al.} \cite{dong2014learning} firstly introduced CNN with only three convolution layers for image SR. Then, residual learning was introduced in VDSR \cite{kim2016accurate}, reaching 20 convolution layers. Lim \textit{et al.} proposed EDSR \cite{lim2017enhanced} with a simplified residual blocks \cite{ResNet}. Zhang \textit{et al.} proposed an even deeper network, RCAN \cite{zhang2018image}. Most of them have achieved state-of-the-art results with deeper and wider networks. 
Recently, Transformer has gained a surge of interest in image restoration. Chen \textit{et al.} proposed the first pre-trained image processing transformer IPT \cite{chen2021pre}. 
SwinIR \cite{liang2021swinir} applies the Swin-Transformer architecture to the image restoration for deep feature extraction.
Restormer \cite{zamir2022restormer} proposed a multi-scale hierarchical design incorporating efficient Transformer blocks by modifying self-attention and MLP. 
While CNNs and Transformers have demonstrated impressive performance in SISR, they suffer from high memory and computational costs.

\subsection{Knowledge Distillation}
\textbf{KD for high-level CV}
Knowledge distillation is widely recognized as an effective model compression method that can significantly reduce the computation overload and improve student's capability by transferring dark knowledge from the cumbersome teacher model to the lightweight student model \cite{hinton2015distilling, yim2017gift, gou2021knowledge}. The response-based KD is simple yet effective since the student is asked to directly imitate the final prediction of the teacher model \cite{hinton2015distilling,zhao2022decoupled,chen2017learning}.
The proposed DUKD method falls into this category since the student is only asked to mimic the teacher model's output.
Besides the output of the last layer, the intermediate features of the teacher model can be used as knowledge to train the student model \cite{romero2014fitnets, zagoruyko2016paying,kim2018paraphrasing,passban2021alp,guo2021distilling}. The relations between layers or samples are also used for KD, such as correlation \cite{yim2017gift,you2017learning}, mutual information \cite{passalis2020heterogeneous}, and pairwise or triple-wise geometric relations \cite{park2019relational}.

\noindent\textbf{KD for low-level CV}
Lately, there has been an increasing number of efforts made on the KD for super-resolution. 
\citet{he2020fakd} proposed FAKD to use the spatial affinity matrix of SR networks' intermediate feature maps as knowledge to train the student model. \citet{lee2020learning} employed an encoder to extract the compact features from HR images to initialize the generator network and performed feature distillation. \citet{wang2021towards} proposed CSD that incorporated channel-sharing self-distillation and perceptual contrastive losses.
The common limitation of these methods is that they are only applicable to CNN-based models and impose requirements on the teacher-student structure. However, since DUKD is a response-based method, it not only offers excellent performance but is also more widely applicable. \section{Methodology}\label{sec: dukd}
\begin{figure*}  \vspace{-5mm}
	\centering
	\includegraphics[width=0.9\linewidth]{figure/dataupcycling-rounded}\vspace{-0.5em}
	\caption{Demonstration of data upcycling and invertible data augmentations. The generated in-domain upcycled data allows the teacher to convey prior knowledge to the student. The invertible data augmentations impose label consistency regularization between the teacher and student model and further improve its generalization capability.}
	\label{fig:methods}\vspace{-1.4em}
\end{figure*}

\subsection{Notations and Preliminaries}
Let  and  be a teacher and a student SR model with parameters  and . Given an input pair of LR and HR image , , the output SR images of the two networks are denoted by  and , where  is the input size and  is the scaling factor.
The L1-norm reconstruction loss is computed as:
\vspace{-0.25em}
And the vanilla response-based KD loss is given by
\vspace{-0.25em}
which is directly derived from the output of teacher and student models. 


\red{
\subsection{Motivation} \label{sec: motivation}
Since the emergence of the knowledge distillation technique \cite{hinton2015distilling}, several analyses and discussions have been carried out on the mechanism of why teacher supervision contributes to improving the performance of student models~\cite{tang2020understanding,stanton2021does,wang2021revisiting,zhang2022quantifying,harutyunyan2023supervision}. It has been widely agreed that for vanilla response-based KD, the secret dark knowledge of teacher model is mainly from the inter-class and inter-examples relation information contained in its logits, which is missed in the ground-truth labels. However, there is barely such benefit in image super-resolution tasks since  and  only differ slightly in image regions of complex patterns. And in general,  has more accurate details and thus actually provides a better reference for training.

Another explanation for KD mechanism is that it could generate more stable optimization directions than training from scratch~\cite{tang2020understanding,zhang2022quantifying}. Compared with ,  is a  lower and more realistic ``performance ceiling'' the student model is able to reach and is closer to the  after training. From this line of hypothesis, we're motivated to increase the volume of the teacher model's supervision over a limited number of training examples.
}

\subsection{Data Upcycling} \label{sec: unlabeled-data}

The overall framework of DUKD is demonstrated in Fig.\ref{fig:framework}. It showcases how the pre-trained teacher model is not solely engaged in conventional knowledge distillation but also leverages its capabilities to supervise the student model on upcycled data.This is a significant departure from feature-based KD methods that force the student model to mimic intermediate feature maps of the teacher model. It emphasizes the effective utilization of data and the activation of the teacher model's full capacity to direct the optimization and learning process through this upcycled data.

The in-domain data upcycling module consists of two image zooming operations as illustrated in Fig.\ref{fig:methods} (a). The zoom-in operation is facilitated by randomly cropping a patch from , which is the same size as the LR image  for the convenience of batch processing. Conversely, the zoom-out operation is carried out by downsampling the LR image in the way exactly the same as how  is generated from . For a pair of training examples (, ), the output of the zoom-out operation is unique, but the zoom-in operation on  could result in various outcomes according to the strategy of patch selection. Beyond random cropping, regions can also be alternatively selected based on their reconstruction difficulty or complexity. However, this approach would incur a higher computational cost during training with marginal performance gains as observed in our experiments.

After these zooming operations, the teacher model is used to generate corresponding SR outputs for the zoom-in and zoom-out images to supervise the student model.
This procedure allows for more comprehensive exploitation of the limited available data, consequently enhancing the teacher's knowledge transfer effectiveness to the student model. The DUKD provides a more refined and data-centric approach to KD, reflecting its benefits in effective data utilization and superior performance in SR tasks.

The overall loss is constructed by adding the extra DUKD loss term that is computed on the upcycled data to the reconstruction loss (Eq. \ref{eq:rec-loss}) and response-based KD loss (Eq. \ref{eq:kd-loss}). For the input  image pair, denote the upcycled data as ,

where ,  and the other terms are computed similarly. If zoom-out is performed, we compute the reconstruction loss between  and  as well.

\red{
\subsection{Label Consistency Regularization}

Consistency regularization is commonly used for semi-supervised and self-supervised learning. It encourages the prediction of the network to be consistent across perturbed training examples, thereby leading to robustness against corrupted data in test time~\cite{oliver2018realistic, englesson2021consistency,jeong2019consistency}. The model is expected to correctly identify the crucial semantic information related to specific tasks from the input, despite the noise and perturbations. This regularization is achieved through various image augmentation techniques, such as rotation, shearing, cutout, and mixup.

Given the input data, knowledge distillation forces the proxy student model to imitate the prediction of the teacher model. Such characteristic should be retained for augmented input since the input perturbations shouldn't significantly distinguish between the teacher and student models' output. To realize label consistency regularization, we propose to impose input augmentations on the student model while leaving the teacher model's input intact. It would ideally result in the student model learning invariant processes from diverse transformations. Meanwhile, the student is secretly supervised by a more powerful teacher model, who provides supervision according to non-perturbed inputs that inherently possess superior quality compared to those derived from augmented ones. It's a way of leveraging the unlabeled upcycled data and the powerful teacher model. Denote the perturbation function as , the consistency regularization can be represented as:
\vspace{-0.25em}

However, as SR is a pixel-level image-to-image CV task that is weakly relevant to semantic information, any tweak on the input will alter the model's output. For KD, the student model's output would consequently be incomparable with the teacher model's. Therefore, we need to perform inverse augmentation  on the output of the student model. The label consistency regularization then becomes:
\vspace{-0.25em}
}
The selected augmentations should be invertible and relevant to the SR task to maintain the crucial pixel-level details after augmentation. It requires that for any input image , .
Hence, a number of popular image augmentations, such as blurring, cutout, brightness adjustment, and cropping, are not applicable as they do not meet this prerequisite.
Instead, we employ two geometric transformations, horizontal and vertical flips and 90{\textdegree} rotations, along with a novel \textit{color inversion} transformation 
that subtracts each pixel intensity value of the input image from 255 (or 1 if normalized).
This color inversion transformation is invertible and maintains the relative magnitude among pixel values. It also prompts the student models to be more sensitive to essential structural features such as lines and edges.
Figure \ref{fig:methods} (b) illustrates the three types of invertible data augmentations employed to realize label consistency regularization.

\subsection{Difference with Data Augmentation}
Our proposed DUKD framework diverges significantly from conventional data augmentation or expansion techniques in the following ways:
Firstly, the data upcycling procedure is intimately intertwined with KD, wherein the teacher model generates the HR label corresponding to the upcycled in-domain LR image. This distinct characteristic contrasts with traditional data augmentation techniques that typically operate on the original input data independent of any teacher model, and is different from the dataset expansion that introduces extra out-domain data. In DUKD, data upcycling is not merely an operation to increase the volume or diversity of the data but instead serves a more significant purpose - it acts as a vehicle for the teacher model to transmit its knowledge to the student model. This unique procedure allows the student model to learn more effectively from the teacher model's responses to the upcycled data, thus enhancing its performance.
Secondly, DUKD introduces the label consistency regularization to KD, a novel approach to improve the student model's robustness against perturbations in test time. 
It allows the student model to learn the teacher's invariant features across transformations and encourages it to generalize better to unseen data. The label consistency regularization is realized by selected invertible data augmentations imposed only on the student model's input and output. The way the perturbations are used is essentially different from conventional data augmentations.

 \section{Experiments}\label{sec: experiments}
\subsection{Experiment Setups}\label{sec: exp-setting}


\begin{table}[]
	\caption{SR model specifications and statistics (\texttimes 4 scale). The FLOPs and frames per second (FPS) are computed with 3\texttimes256\texttimes256 input image and FPS is computed on a single V100 GPU with 64GB memory. The block denotes the number of residual blocks (in each residual group) for EDSR and RCAN models or residual swin transformer blocks for SwinIR models.}\vspace{-0.5em}
	\label{tab:model-config}
	\centering
	\small
    \resizebox{\columnwidth}{!}{
	\renewcommand{\arraystretch}{0.9}
	\begin{tabular}{@{}llcllll@{}}
		\toprule
		\multirow{2}{*}{Model} & \multirow{2}{*}{Role} & \multicolumn{3}{c}{Network}         & \multirow{2}{*}{FLOPs (G)} & \multirow{2}{*}{\#Params}  \\ \cmidrule(lr){3-5}
		&                       & Channel & Block & Group &                            &                                                \\ \midrule
		\multirow{2}{*}{EDSR}  & Teacher               & 256       & 32         & -          & 3293.35                    & 43.09 M                                   \\
		& Student               & 64        & 32         & -          & 207.28                     & 2.70 M                                   \\ \midrule
		\multirow{2}{*}{RCAN}  & Teacher               & 64        & 20         & 10         & 1044.03                    & 15.59 M                                  \\
		& Student               & 64        & 6          & 10         & 366.98                     & 5.17 M                                  \\ \midrule
		\multirow{2}{*}{SwinIR} & Teacher               & 180        &   6       &    -      & 861.27                    & 11.90 M                               \\
		& Student               & 60        &      4     &   -       & 121.48                     & 1.24 M                              \\ \bottomrule
	\end{tabular}
 }
\end{table} 
\textbf{Backbones and Baselines.} We use EDSR \cite{lim2017enhanced}, RCAN \cite{zhang2018image}, and SwinIR \cite{liang2021swinir} as backbone models to verify the effectiveness of DUKD and compare it with some existing KD methods. The specifications of teacher and student networks and some statistics including FLOPs, number of parameters, and model inference speed (FPS) are presented in Table \ref{tab:model-config}. We compare our KD framework with the following baseline training and KD methods: train from scratch, response-based KD~\cite{hinton2015distilling}, FitNet \cite{romero2014fitnets}, AT \cite{zagoruyko2016paying}, RKD \cite{park2019relational}, FAKD \cite{he2020fakd}, and CSD \cite{wang2021towards}. To evaluate the performance of SISR models, we calculate the peak signal-to-noise ratio (PSNR) and the structural similarity index (SSIM) on the Y channel of the YCbCr color space.

\noindent\textbf{Training Details.} The SR models are trained with the 800 training images from DIV2K \cite{timofte2017ntire} and evaluated on four benchmark datasets: Set5 \cite{bevilacqua2012low}, Set14 \cite{zeyde2012single}, BSD100 \cite{martin2001database}, and Urban100 \cite{huang2015single}. The LR images used for training were obtained by down-sampling the HR images with the bicubic degradation. During training, the input LR image is randomly cropped into  patches and augmented with random horizontal and vertical flips and rotations. For FAKD and CSD methods, we follow the hyperparameters setting specified in their paper and train the models by ourselves if checkpoint is not provided, as particularly noted in the results section. The zoom in operation of DUKD is conducted by randomly cropping for simplicity. The zoom out is not used when training SwinIR models since the result image would be too small to be valid input for SwinIR. Models are trained with ADAM optimizer \cite{kingma2014adam} with ,  and , with a batch size of 16 and a total of  updates. The initial learning rate is set to  and is decayed by a factor of 10 at every  iteration. We implemented the proposed kd method with the BasicSR \cite{basicsr} and PyTorch 1.10 framework and trained them using 4 NVIDIA V100 GPUs.

\subsection{Results and Comparison}\label{sec: exp-results}
\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{figure/set14_urban100-edited}\vspace{-0.5em}
	\caption{PSNR improvements v.s number of parameters of different SR models (\texttimes 4 scale) on Set14 and Urban100 test sets. Compression rates in terms of the number of parameters are indicated in the middle of the left panel.}
	\label{fig:psnr-improvement}\vspace{-0.5em}
\end{figure}

\noindent\textbf{Comparison with Baseline Methods. }
The quantitative results (PSNR and SSIM) for EDSR, RCAN, and SwinIR networks are presented in table \ref{tab:exp-edsr}, \ref{tab:exp-rcan}, and \ref{tab:exp-swinir} respectively, for \texttimes 2, \texttimes 3, and \texttimes 4 scales. The following conclusions can be drawn from these results: (\textbf{1}) Existing KD techniques have limited benefits, with some even leading to the deterioration of the student model. For instance, EDSR models trained with FAKD sometimes underperform those trained from scratch. (\textbf{2}) Our proposed DUKD framework consistently outperforms the existing KD baseline methods in all experimental settings. For example, when compared with the response-based KD method, the average PSNR gains of the three types of networks on the Urban100 test set for \texttimes 2, \texttimes 3, and \texttimes 4 scales are 0.43 dB, 0.31 dB, 0.31 dB, respectively. 
In Fig.~\ref{fig:psnr-improvement}, we demonstrate the model's PSNR improvements derived from our KD method under different settings of model compression rates and student model sizes. Benefits for the students distilled from a larger teacher (larger compression rates) are more significant, as our KD method can effectively transfer more capabilities from teachers. 
In Fig.~\ref{fig:fedility-generalization}, we demonstrate the effectiveness of different KD methods by comparing the similarity of students' output towards teacher's on the training and Urban100 testing sets to evaluate if the student learns well to mimic the teacher model. It shows that DUKD makes the student not only effectively fit the teacher model on the training set but also imitate it on the test sets so that the student model's outputs get closer to GT as well.






\begin{table}[]
\caption{Quantitative comparison (average PSNR/SSIM) between DUKD and other distillation methods for \textbf{EDSR} of three SR scales. The best and second-best performances are highlighted in bold and underlined, respectively. The asterisk indicates that the results in a row are from our reproduction experiments.}\vspace{-0.5em}
\label{tab:exp-edsr}
\resizebox{\columnwidth}{!}{\begin{tabular}{@{\hspace{2pt}}l@{\hspace{1.5\tabcolsep}}lcccc@{\hspace{2pt}}}
\toprule
\multirow{2}{*}[-0.24em]{Scale} &
  \multirow{2}{*}[-0.24em]{Method} &
  Set5 &
  Set14 &
  BSD100 &
  Urban100 \\ \cmidrule(l){3-6} 
 &
   &
  PSNR/SSIM &
  PSNR/SSIM &
  PSNR/SSIM &
  PSNR/SSIM \\ \midrule
\multirow{8}{*}{\texttimes2} &
  Scratch &
  38.00/0.9605 &
  33.57/0.9171 &
  32.17/0.8996 &
  31.96/0.9268 \\
 &
  KD &
  38.04/0.9606 &
  33.58/0.9172 &
  32.19/0.8998 &
  31.98/0.9269 \\
 &
  FitNet &
  37.59/0.9589 &
  33.09/0.9136 &
  31.79/0.8953 &
  30.46/0.9111 \\
 &
  AT &
  37.96/0.9603 &
  33.48/0.9167 &
  32.12/0.8990 &
  31.71/0.9241 \\
 &
  RKD &
  38.03/0.9606 &
  33.57/0.9173 &
  32.18/0.8998 &
  31.96/0.9270 \\
 &
  FAKD &
  37.99/0.9606 &
  33.60/0.9173 &
  32.19/0.8998 &
  32.04/0.9275 \\
 &
  CSD &
  \underline{38.06}/\underline{0.9607} &
  \underline{33.65}/\underline{0.9179} &
  \underline{32.22}/\underline{0.9004} &
  \underline{32.26}/\underline{0.9300} \\
 &
  \textbf{DUKD} &
  \textbf{38.15}/\textbf{0.9610} &
  \textbf{33.80}/\textbf{0.9195} &
  \textbf{32.27}/\textbf{0.9007} &
  \textbf{32.53}/\textbf{0.9320} \\ \midrule
\multirow{8}{*}{\texttimes3} &
  Scratch &
  34.39/0.9270 &
  30.32/0.8417 &
  29.08/0.8046 &
  27.99/0.8489 \\
 &
  KD &
  34.43/0.9273 &
  30.34/0.8422 &
  29.10/0.8050 &
  28.00/0.8491 \\
 &
  FitNet &
  33.35/0.9178 &
  29.71/0.8323 &
  28.62/0.7949 &
  26.61/0.8167 \\
 &
  AT &
  34.29/0.9262 &
  30.26/0.8406 &
  29.03/0.8035 &
  27.76/0.8443 \\
 &
  RKD &
  34.43/0.9274 &
  30.33/0.8423 &
  29.09/0.8051 &
  27.96/0.8493 \\
 &
  FAKD &
  34.39/0.9272 &
  30.34/0.8426 &
  29.10/0.8052 &
  28.07/0.8511 \\
 &
  CSD &
  \underline{34.45}/\underline{0.9275} &
  \underline{30.32}/\underline{0.8430} &
  \underline{29.11}/\underline{0.8061} &
  \underline{28.21}/\underline{0.8549} \\
 &
  \textbf{DUKD} &
  \textbf{34.59}/\textbf{0.9287} &
  \textbf{30.47}/\textbf{0.8448} &
  \textbf{29.20}/\textbf{0.8073} &
  \textbf{28.44}/\textbf{0.8578} \\ \midrule
\multirow{8}{*}{\texttimes4} &
  Scratch &
  32.29/0.8965 &
  28.68/0.7840 &
  27.64/0.7380 &
  26.21/0.7893 \\
 &
  KD &
  32.30/0.8965 &
  28.70/0.7842 &
  27.64/0.7382 &
  26.21/0.7897 \\
 &
  FitNet &
  31.65/0.8873 &
  28.33/0.7768 &
  27.38/0.7309 &
  25.40/0.7637 \\
 &
  AT &
  32.22/0.8952 &
  28.63/0.7825 &
  27.59/0.7365 &
  25.97/0.7825 \\
 &
  RKD &
  32.30/0.8965 &
  28.69/0.7842 &
  27.64/0.7383 &
  26.20/0.7899 \\
 &
  FAKD &
  32.27/0.8960 &
  28.65/0.7836 &
  27.62/0.7379 &
  26.18/0.7895 \\
 &
  CSD &
  \underline{32.34}/\underline{0.8974} &
  \underline{28.72}/\underline{0.7856} &
  \underline{27.68}/\underline{0.7396} &
  \underline{26.34}/\underline{0.7948} \\
 &
  \textbf{DUKD} &
  \textbf{32.47}/\textbf{0.8981} &
  \textbf{28.80}/\textbf{0.7866} &
  \textbf{27.71}/\textbf{0.7403} &
  \textbf{26.45}/\textbf{0.7963} \\ \bottomrule
\end{tabular}}
\end{table}



\begin{table}[]
\caption{Quantitative comparison (average PSNR/SSIM) between DUKD and other distillation methods for \textbf{RCAN} of three SR scales. The best and second-best performances are highlighted in bold and underlined, respectively.}\vspace{-0.5em}
\label{tab:exp-rcan}
\resizebox{\columnwidth}{!}{\begin{tabular}{@{\hspace{2pt}}l@{\hspace{1.5\tabcolsep}}lcccc@{\hspace{2pt}}}
\toprule
\multirow{2}{*}[-0.24em]{Scale} &
  \multirow{2}{*}[-0.24em]{Method} &
  Set5 &
  Set14 &
  BSD100 &
  Urban100 \\ \cmidrule(l){3-6} 
 &
   &
  PSNR/SSIM &
  PSNR/SSIM &
  PSNR/SSIM &
  PSNR/SSIM \\ \midrule
\multirow{7}{*}{\texttimes2} &
  Scratch &
  38.13/0.9610 &
  33.78/0.9194 &
  32.26/0.9007 &
  32.63/0.9327 \\
 &
  KD &
  38.18/0.9611 &
  33.83/0.9197 &
  32.29/0.9010 &
  32.67/0.9329 \\
 &
  FitNet &
  37.97/0.9602 &
  33.57/0.9174 &
  32.19/0.8999 &
  32.06/0.9279 \\
 &
  AT &
  38.13/0.9610 &
  33.70/0.9187 &
  32.25/0.9005 &
  32.48/0.9313 \\
 &
  RKD &
  \underline{38.18}/\underline{0.9612} &
  33.78/0.9191 &
  32.29/0.9011 &
  \underline{32.70}/\underline{0.9330} \\
 &
  FAKD &
  38.17/0.9612 &
  \underline{33.83}/\underline{0.9199} &
  \underline{32.29}/\underline{0.9011} &
  32.65/0.9330 \\
 &
  \textbf{DUKD} &
  \textbf{38.23}/\textbf{0.9614} &
  \textbf{33.90}/\textbf{0.9201} &
  \textbf{32.33}/\textbf{0.9016} &
  \textbf{32.87}/\textbf{0.9349} \\ \midrule
\multirow{7}{*}{\texttimes3} &
  Scratch &
  34.61/0.9288 &
  30.45/0.8444 &
  29.18/0.8074 &
  28.59/0.8610 \\
 &
  KD &
  34.61/0.9291 &
  30.47/0.8447 &
  \underline{29.21}/\underline{0.8080} &
  \underline{28.62}/\underline{0.8612} \\
 &
  FitNet &
  34.21/0.9248 &
  30.20/0.8399 &
  29.05/0.8044 &
  27.89/0.8472 \\
 &
  AT &
  34.55/0.9287 &
  30.43/0.8438 &
  29.17/0.8070 &
  28.43/0.8577 \\
 &
  RKD &
  \underline{34.67}/\underline{0.9292} &
  30.48/0.8451 &
  29.21/0.8080 &
  28.60/0.8610 \\
 &
  FAKD &
  34.63/0.9290 &
  \underline{30.51}/\underline{0.8453} &
  29.21/0.8079 &
  28.62/0.8612 \\
 &
  \textbf{DUKD} &
  \textbf{34.74}/\textbf{0.9296} &
  \textbf{30.54}/\textbf{0.8458} &
  \textbf{29.25}/\textbf{0.8088} &
  \textbf{28.79}/\textbf{0.8646} \\ \midrule
\multirow{7}{*}{\texttimes4} &
  Scratch &
  32.31/0.8970 &
  28.69/0.7840 &
  27.64/0.7380 &
  26.37/0.7950 \\
 &
  KD &
  32.45/0.8980 &
  28.76/0.7860 &
  27.67/0.7400 &
  26.49/0.7980 \\
 &
  FitNet &
  31.99/0.8899 &
  28.50/0.7789 &
  27.55/0.7353 &
  25.90/0.7791 \\
 &
  AT &
  32.31/0.8967 &
  28.69/0.7839 &
  27.64/0.7385 &
  26.29/0.7927 \\
 &
  RKD &
  32.39/0.8974 &
  28.74/0.7856 &
  27.67/0.7399 &
  26.47/0.7981 \\
 &
  FAKD &
  \underline{32.46}/\underline{0.8980} &
  \underline{28.77}/\underline{0.7860} &
  \underline{27.68}/\underline{0.7400} &
  \underline{26.50}/\underline{0.7980} \\
 &
  \textbf{DUKD} &
  \textbf{32.56}/\textbf{0.8990} &
  \textbf{28.83}/\textbf{0.7870} &
  \textbf{27.72}/\textbf{0.7410} &
  \textbf{26.62}/\textbf{0.8020} \\ \bottomrule
\end{tabular}}
\end{table}


\begin{table}[]
\caption{Quantitative comparison (average PSNR/SSIM) between DUKD and other distillation methods for \textbf{SwinIR} of three SR scales. Best performance is highlighted in bold.}\vspace{-0.5em}
\label{tab:exp-swinir}
\resizebox{\columnwidth}{!}{\begin{tabular}{@{\hspace{2pt}}l@{\hspace{1.5\tabcolsep}}lcccc@{\hspace{2pt}}}
\toprule
\multirow{2}{*}[-0.24em]{Scale} &
  \multirow{2}{*}[-0.24em]{Method} &
  Set5 &
  Set14 &
  BSD100 &
  Urban100 \\ \cmidrule(l){3-6} 
 &
   &
  PSNR/SSIM &
  PSNR/SSIM &
  PSNR/SSIM &
  PSNR/SSIM \\ \midrule
\multirow{3}{*}{\texttimes2} &
  Scratch &
  38.01/0.9607 &
  33.57/0.9178 &
  32.19/0.9000 &
  32.05/0.9279 \\
 &
  KD &
  38.04/0.9608 &
  33.61/0.9184 &
  32.22/0.9003 &
  32.09/0.9282 \\
 &
  \textbf{DUKD} &
  \textbf{38.13}/\textbf{0.9610} &
  \textbf{33.78}/\textbf{0.9194} &
  \textbf{32.26}/\textbf{0.9007} &
  \textbf{32.63}/\textbf{0.9327} \\ \midrule
\multirow{3}{*}{\texttimes3} &
  Scratch &
  34.41/0.9273 &
  30.43/0.8437 &
  29.12/0.8062 &
  28.20/0.8537 \\
 &
  KD &
  34.44/0.9275 &
  30.45/0.8443 &
  29.14/0.8066 &
  28.23/0.8545 \\
 &
  \textbf{DUKD} &
  \textbf{34.55}/\textbf{0.9285} &
  \textbf{30.53}/\textbf{0.8456} &
  \textbf{29.20}/\textbf{0.8080} &
  \textbf{28.53}/\textbf{0.8604} \\ \midrule
\multirow{3}{*}{\texttimes4} &
  Scratch &
  32.31/0.8955 &
  28.67/0.7833 &
  27.61/0.7379 &
  26.15/0.7884 \\
 &
  KD &
  32.27/0.8954 &
  28.67/0.7833 &
  27.62/0.7380 &
  26.15/0.7887 \\
 &
  \textbf{DUKD} &
  \textbf{32.41}/\textbf{0.8973} &
  \textbf{28.79}/\textbf{0.7860} &
  \textbf{27.69}/\textbf{0.7405} &
  \textbf{26.43}/\textbf{0.7972} \\ \bottomrule
\end{tabular}}
\end{table} 
\begin{figure}
	\centering
	\includegraphics[width=1\columnwidth]{figure/div2k_urban100_v1116}\vspace{-0.5em}
	\caption{Agreements between student and teacher models of various SR KD methods on \texttimes4 scale EDSR models. The student model trained with DUKD can effectively imitate the teacher model on training set and inherit its generalizability on unseen data.
	}
	\label{fig:fedility-generalization}
\end{figure}



\begin{table}[]
	\centering
	\caption{The results of heterogeneous distillation using DUKD on the \texttimes 4 scale RCAN model. The DUKD supports cross-architecture distillation, allowing a more powerful teacher model for KD.}\vspace{-0.5em}
	\label{tab:heter-kd}
    \resizebox{\columnwidth}{!}{
		\begin{tabular}{@{}c@{\hspace{1.5\tabcolsep}}cccc@{}}
			\toprule
			\multirow{2}{*}[-0.24em]{Teacher}  & Set5 & Set14 & BSD100 & Urban100 \\ \cmidrule(lr){2-5} 
			& PSNR/SSIM   & PSNR/SSIM   & PSNR/SSIM   & PSNR/SSIM   \\ \midrule
			(Scratch) & 32.31/0.8966 & 28.69/0.7842 & 27.64/0.7384 & 26.37/0.7949 \\ \midrule
			EDSR    & 32.51/0.8986 & 28.80/0.7868 & 27.71/0.7406 & 26.59/0.8014 \\
			SwinIR  & 32.50/0.8986 & 28.82/0.7872 & 27.72/0.7408 & 26.59/0.8007 \\ \bottomrule
		\end{tabular}}
\end{table}

 
\noindent\textbf{Experiment Results on Heterogeneous Settings. }
We extend the experiments to heterogeneous settings where the teacher and student models have different network architectures, as presented in Tab. \ref{tab:heter-kd}. Such cross-architecture distillation is not applicable to other feature-based KD or self-distillation methods, while DUKD can still effectively improve the student models.
For instance, compared to the RCAN model trained from scratch, utilizing DUKD with an EDSR or SwinIR teacher model yields an increase in PSNR by 0.22dB at \texttimes 4 scale on Urban100 test set.

\noindent\textbf{Visual Comparison. }
As shown in Fig. \ref{fig:vis}, we compare our KD method with others on four images from the Urban100 dataset in terms of output qualitative results of the EDSR student model of \texttimes 4 scale. To underscore the differences in detailed pattern and texture reconstruction, we have compared relatively small cropped images, with PSNR calculated solely on these cropped parts. Metrics in the caption are calculated by considering only the cropped part.  Generally, a higher PSNR aligns with superior subjective image quality.  For the reconstruction of textures (e.g. lines, edges, and complex patterns), the model trained with DUKD yields outputs that are both sharper and more similar to the HR images, owing to the data upcycling procedure, which boosts the model's generalizability.


\red{
\noindent\textbf{Experiment Results on Real-world SR task. } 
To test the performance of DUKD for real-world SR, we continue to train the PSNR-oriented student SwinIR models of \texttimes 4 scale by using the BSRGAN degradation model \cite{zhang2021designing, liang2021swinir} on the DF2K dataset. The models are tested on three testing datasets: RealSR \cite{cai2019toward}, DRealSR \cite{wei2020component}, and OST300 \cite{wang2018recovering}. The non-reference image quality assessment (NIQE) \cite{mittal2012making}
scores are
shown in Tab. \ref{tab:realworld-sr}. The model trained with DUKD produces lower NIQE scores and output images with more pleasing visual performance, as shown in supplementary material.
}



\begin{table}[]
\caption{NIQE scores on several real-world SR testing datasets. The lower, the better. Visual comparisons are provided in the supplementary material.}\vspace{-0.5em}
\label{tab:realworld-sr}
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{@{\hspace{2pt}}llccc@{\hspace{2pt}}}
\toprule
Method & \#Params & RealSR~\cite{cai2019toward}  & DRealSR~\cite{wei2020component}     & OST300~\cite{wang2018recovering}    \\ \midrule
Scratch                 & 11.9M                    & 4.771 	    & 4.847 	 & 2.932 
  \\ \midrule
Scratch                 & \multirow{3}{*}{1.24M}   & 5.810 	    & 5.757 	 & 3.788  \\
KD                      &                          & 5.425 	    & 5.408 	 & 3.652 
      \\
DUKD                    &                          & 5.398 	    & 5.378 	 & 3.494 
       \\ \bottomrule
\end{tabular}}
\end{table} 
\begin{figure*}[t]
		\resizebox{\textwidth}{!}{\hspace{0.1em}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img004\_gt\_v\_cr}
					\put(5,5){\tcbox{img004}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img004\_gtsub}
					\put(5,5){\tcbox{HR(PSNR)}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img004\_FAKD}
					\put(5,5){\tcbox{FAKD (15.39)}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img004\_Scratch}
					\put(5,5){\tcbox{Scratch (17.4)}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img004\_CSD}
					\put(5,5){\tcbox{CSD (17.98)}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img004\_KD}
					\put(5,5){\tcbox{KD (17.07)}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img004\_Ours}
					\put(5,5){\tcbox{DUKD (19.96)}}
				\end{overpic}
			\end{minipage}
			
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img089\_gt\_v\_cr}
					\put(5,5){\tcbox{img089}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img089\_gtsub}
					\put(5,5){\tcbox{HR(PSNR)}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img089\_FAKD}
					\put(5,5){\tcbox{FAKD (25.39)}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img089\_Scratch}
					\put(5,5){\tcbox{Scratch (25.5)}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img089\_CSD}
					\put(5,5){\tcbox{CSD (25.82)}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img089\_KD}
					\put(5,5){\tcbox{KD (25.48)}}
				\end{overpic}	
				\begin{overpic}[width=\textwidth]{figure/visualization/img089\_Ours}
					\put(5,5){\tcbox{DUKD (26.61)}}
				\end{overpic}
			\end{minipage}
		} \resizebox{\textwidth}{!}{\hspace{0.1em}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img019\_gt\_v\_cr}
					\put(5,5){\tcbox{img019}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img019\_gtsub}
					\put(5,5){\tcbox{HR(PSNR)}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img019\_FAKD}
					\put(5,5){\tcbox{FAKD (15.55)}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img019\_Scratch}
					\put(5,5){\tcbox{Scratch (16.7}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img019\_CSD}
					\put(5,5){\tcbox{CSD (18.21)}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img019\_KD}
					\put(5,5){\tcbox{KD (16.83)}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img019\_Ours}
					\put(5,5){\tcbox{DUKD (19.52)}}
				\end{overpic}
			\end{minipage}
			
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img096\_gt\_v\_cr}
					\put(5,5){\tcbox{img096}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img096\_gtsub}
					\put(5,5){\tcbox{HR(PSNR)}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img096\_FAKD}
					\put(5,5){\tcbox{FAKD (17.59)}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img096\_Scratch}
					\put(5,5){\tcbox{Scratch (18.2}}
				\end{overpic}
				\begin{overpic}[width=\textwidth]{figure/visualization/img096\_CSD}
					\put(5,5){\tcbox{CSD (19.24)}}
				\end{overpic}
			\end{minipage}
			\begin{minipage}{0.25\linewidth}
				\begin{overpic}[width=\textwidth]{figure/visualization/img096\_KD}
					\put(5,5){\tcbox{KD (18.39)}}
				\end{overpic}	
				\begin{overpic}[width=\textwidth]{figure/visualization/img096\_Ours}
					\put(5,5){\tcbox{DUKD (20.74)}}
				\end{overpic}
			\end{minipage}
			
		}
	
	\caption{The \texttimes4 super resolution results of EDSR models on img004, img019, img089 and img096 from Urban100. PSNRs of the cropped regions are annotated below each image.}\vspace{-1em}
	
	\label{fig:vis}
\end{figure*} 
\subsection{Ablation Analysis}\label{sec: ablation}




\begin{table}[]
\caption{Ablation study of data upcycling and label consistency regularization. The proposed data upcycling and label consistency regularization modules improve the student model's performance.}\vspace{-0.5em} \label{tab:ablation-du-lcr}
\centering
\resizebox{0.9\columnwidth}{!}{\renewcommand{\arraystretch}{1.0}  \begin{tabular}{@{\hspace{2pt}}c@{\hspace{1\tabcolsep}}cc@{\hspace{2pt}}}
\toprule
\multirow{2}{*}[-0.24em]{Data Upcycling} & \multirow{2}{0.4\columnwidth}[-0.24em]{\centering Label consistency regularization} & Urban100 \\\cmidrule(lr){3-3}
                      &                         & PSNR/SSIM    \\ \midrule
\xmark & \xmark & 24.87 / 0.7431 \\
\cmark & \xmark & 25.20 / 0.7558 \\
\cmark & \cmark & 25.34 / 0.7609 \\ 
\bottomrule
\end{tabular}}
\end{table}



\begin{table}[]
\caption{Ablation study of the zoom in and zoom out operation in data upcycling. The in-domain data upcycling module effectively allows the student model to inherit the teacher model's capability and greatly improves the student models' performance on testing sets.}\vspace{-0.5em}  \label{tab:ablation-zi-zo}
\centering
\resizebox{0.625\columnwidth}{!}{\renewcommand{\arraystretch}{1.0}
\begin{tabular}{@{\hspace{2pt}}ccc@{\hspace{2pt}}}
\toprule
\multirow{2}{*}[-0.24em]{Zoom In} & \multirow{2}{*}[-0.24em]{Zoom Out} & Urban100     \\ \cmidrule(lr){3-3} 
                         &                           & PSNR / SSIM    \\ \midrule
\xmark                        & \xmark                         & 24.87 / 0.7431 \\
\cmark                        & \xmark                         & 25.18 / 0.7551 \\
\xmark                        & \cmark                         & 25.18 / 0.7552 \\
\cmark                        & \cmark                         & 25.20 / 0.7558 \\ 
\bottomrule
\end{tabular}}
\end{table}



 \noindent\textbf{Impact of data upcycling and label consistency regularization.}
Table \ref{tab:ablation-du-lcr} shows the effect of DUKD's two modules, using EDSR baseline model distilled by our student model. Further, Tab. \ref{tab:ablation-zi-zo} ablates the zoom-in and zoom-out operations in the data upcycling.
The result shows that adopting data upcycling and label consistency regularization could lead to significant performance improvement upon prior KD methods, whether used individually or together. 
For example, simply upcycling data by zoom-in draws 0.31dB PSNR increment on Urban100 test set, and adding zoom-out and invertible data augmentations yields an additional 0.16dB improvement. 

\vspace{0.75em}
\red{
\noindent\textbf{Integrate DUKD into other model compression methods.} We integrate DUKD with a SOTA SR quantization method, Distribution-Aware Quantization (DAQ) \cite{hong2022daq}, and use the full-precision model to supervise the quantized ones. Figure \ref{fig:dukd_quantized} shows the PSNR of quantized \texttimes 4 scale EDSR baseline models with and without knowledge distillation during training, and the full results are provided in supplementary. As one can see, DAQ with vanilla response-based KD
has barely obvious effects on the models' performance, while DUKD could improve the quantized model by a large margin. 
We also integrate DUKD with the FAKD method in Tab. \ref{tab:fakd-dukd}. The resulting models outperform the ones trained by FAKD by a large margin.
The results indicate that DUKD, as a data-centric approach, can be effectively aggregated with other model compression techniques.

}

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{figure/quantized-edsr-urban100-v11141135}\vspace{-0.5em}
    \caption{PSNR of quantized baseline EDSR model trained with and without knowledge distillation. During quantization, the full precision model is used as the teacher.}\vspace{-0.75em}
    \label{fig:dukd_quantized}
\end{figure}



\begin{table}[]
\caption{Experiment results of combining DUKD and FAKD. By involving in-domain data upcycling and label consistency regularization, the FAKD could achieve better than vanilla logits KD.}\vspace{-0.5em}
\centering
\label{tab:fakd-dukd}
\resizebox{0.65\columnwidth}{!}{\renewcommand{\arraystretch}{0.95}  \begin{tabular}{@{\hspace{2pt}}llc@{\hspace{2pt}}}
\toprule
\multirow{2}{*}[-0.24em]{Model} & \multirow{2}{*}[-0.24em]{Method} & Urban100     \\ \cmidrule(lr){3-3} 
                       &                         & PSNR / SSIM    \\ \midrule
\multirow{4}{*}{EDSR}  & Logits KD               & 26.21 / 0.7897 \\
                       & FAKD                    & 26.18 / 0.7895 \\
                       & FAKD+DUKD               & 26.30 / 0.7930 \\
                       & DUKD                    & 26.45 / 0.7966 \\ 
\bottomrule
\end{tabular}}
\end{table}\vspace{-0.5em} 
\begin{table}[h]
\caption{Comparison with data expansion. DF2K denotes the composite DIV2K and Flickr2K datasets. The models trained on DF2K are updated for  steps, and the model trained with DUKD is trained for  steps.}\vspace{-0.5em}
\label{tab:cmpr-dataexpansion}
\centering
\resizebox{\columnwidth}{!}{\setlength{\tabcolsep}{0.3em}
\renewcommand{\arraystretch}{1.05}  \begin{tabular}{@{}cccccc@{\hspace{1.5pt}}}
\toprule
\multirow{2}{0.2\columnwidth}[-0.24em]{\centering Training Set} & \multirow{2}{*}[-0.24em]{Method} & Set5 & Set14 & BSD100 & Urban100 \\ \cmidrule(lr){3-6} 
&         & PSNR/SSIM   & PSNR/SSIM   & PSNR/SSIM   & PSNR/SSIM   \\ \midrule
DF2K & Scratch & 32.29/0.8960  & 28.66/0.7816 & 27.62/0.7372 & 26.15/0.7872 \\
DF2K & KD      & 32.37/0.8973 & 28.74/0.7847 & 27.67/0.7390 & 26.31/0.7925 \\
DIV2K & DUKD    & 32.39/0.8973 & 28.75/0.7852 & 27.68/0.7390 & 26.32/0.7927 \\ \bottomrule
\end{tabular}}
\end{table}


 
\vspace{0.75em}
\noindent\textbf{Comparison with data expansion} 
At the end of Section \ref{sec: dukd}, we discussed the difference between DUKD and plain data augmentation and expansion. Table \ref{tab:cmpr-dataexpansion} compares DUKD and training/distillation with data expansion. 
We train the \texttimes 4 scale EDSR models on a larger dataset (DF2K: DIV2K+Flickr2K \cite{timofte2017ntire}, which contains 3450 images). 
The number of iterations is doubled for the larger training set since the previous option () is insufficient for the models to converge.
The other settings of the training recipe are kept unchanged.
The result shows that DUKD is superior to training the model with more input data in terms of both training efficiency and performance.

\begin{figure*}[h]
	\resizebox{\textwidth}{!}{\setlength{\abovecaptionskip}{0.cm}
		\setlength{\belowcaptionskip}{-0.cm}
		\centering
		\begin{minipage}{0.24\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/Nikon\_001}
				\put(5,90){\tcbox{Input}}
			\end{overpic}			
		\end{minipage}
		\begin{minipage}{0.12\linewidth}			
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/teacher_Nikon\_001}
				\put(5,85){\tcbox{\footnotesize Teacher}}
			\end{overpic}		
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/kd_Nikon\_001}
				\put(5,85){\tcbox{\footnotesize KD}}
			\end{overpic}		
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/scratch_Nikon\_001}
				\put(5,85){\tcbox{\footnotesize Scratch}}
			\end{overpic}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/dukd_Nikon\_001}
				\put(5,85){\tcbox{\footnotesize DUKD}}
			\end{overpic}
			
		\end{minipage}
		
		
		\begin{minipage}{0.24\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/Nikon\_016}
				\put(5,90){\tcbox{Input}}
			\end{overpic}
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/teacher_Nikon\_016}
				\put(5,85){\tcbox{\footnotesize Teacher}}
			\end{overpic}		
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/kd_Nikon\_016}
				\put(5,85){\tcbox{\footnotesize KD}}
			\end{overpic}		
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/scratch_Nikon\_016}
				\put(5,85){\tcbox{\footnotesize Scratch}}
			\end{overpic}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/dukd_Nikon\_016}
				\put(5,85){\tcbox{\footnotesize DUKD}}
			\end{overpic}
			
		\end{minipage}
		
		
		
		
	} 

	
	\resizebox{\textwidth}{!}{\setlength{\abovecaptionskip}{0.cm}
		\setlength{\belowcaptionskip}{-0.cm}
		\centering
		\begin{minipage}{0.24\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/Nikon\_021}
				\put(5,90){\tcbox{Input}}
			\end{overpic}
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/teacher_Nikon\_021}
				\put(5,85){\tcbox{\footnotesize Teacher}}
			\end{overpic}		
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/kd_Nikon\_021}
				\put(5,85){\tcbox{\footnotesize KD}}
			\end{overpic}		
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/scratch_Nikon\_021}
				\put(5,85){\tcbox{\footnotesize Scratch}}
			\end{overpic}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/dukd_Nikon\_021}
				\put(5,85){\tcbox{\footnotesize DUKD}}
			\end{overpic}
			
		\end{minipage}
		
		
		\begin{minipage}{0.24\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/Nikon\_025}
				\put(5,90){\tcbox{Input}}
			\end{overpic}
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/teacher_Nikon\_025}
				\put(5,85){\tcbox{\footnotesize Teacher}}
			\end{overpic}		
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/kd_Nikon\_025}
				\put(5,85){\tcbox{\footnotesize KD}}
			\end{overpic}		
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/scratch_Nikon\_025}
				\put(5,85){\tcbox{\footnotesize Scratch}}
			\end{overpic}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/dukd_Nikon\_025}
				\put(5,85){\tcbox{\footnotesize DUKD}}
			\end{overpic}
			
		\end{minipage}
	}
	
	\resizebox{\textwidth}{!}{\setlength{\abovecaptionskip}{0.cm}
		\setlength{\belowcaptionskip}{-0.cm}
		\centering
		\begin{minipage}{0.24\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/Nikon\_041}
				\put(5,90){\tcbox{Input}}
			\end{overpic}
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/teacher_Nikon\_041}
				\put(5,85){\tcbox{\footnotesize Teacher}}
			\end{overpic}		
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/kd_Nikon\_041}
				\put(5,85){\tcbox{\footnotesize KD}}
			\end{overpic}		
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/scratch_Nikon\_041}
				\put(5,85){\tcbox{\footnotesize Scratch}}
			\end{overpic}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/dukd_Nikon\_041}
				\put(5,85){\tcbox{\footnotesize DUKD}}
			\end{overpic}
			
		\end{minipage}
		
		\begin{minipage}{0.24\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/Nikon\_044}
				\put(5,90){\tcbox{Input}}
			\end{overpic}
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/teacher_Nikon\_044}
				\put(5,85){\tcbox{\footnotesize Teacher}}
			\end{overpic}		
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/kd_Nikon\_044}
				\put(5,85){\tcbox{\footnotesize KD}}
			\end{overpic}		
		\end{minipage}
		\begin{minipage}{0.12\linewidth}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/scratch_Nikon\_044}
				\put(5,85){\tcbox{\footnotesize Scratch}}
			\end{overpic}
			\begin{overpic}[width=\textwidth]{figure/visualization/realworld/dukd_Nikon\_044}
				\put(5,85){\tcbox{\footnotesize DUKD}}
			\end{overpic}
			
		\end{minipage}
	}
	
	
	\caption{Qualitative comparisons on several representative real-world samples with upsampling scale factor of 4. DUKD outperforms other approaches in removing artifacts and restoring details, and its outputs are more similar with the teacher model's. Zoom in for best view.}
	
	\label{fig:vis_real}
\end{figure*}  
\section{Conclusion}

In this work, we present DUKD, a simple yet significant KD framework for SR that is universally applicable to a wide array of teacher-student architectures. 
Central to our approach is the novel procedure of data upcycling, which we implement through the application of zoom-in and zoom-out operations to LR and HR images, respectively. This data-centric method enables the teacher model to impart its latent knowledge effectively via this upcycled data. Beyond the data upcycling, we have also pioneered the realization of label consistency regularization in the field of KD for SR. This mechanism further bolsters the student model's generalization capabilities, fortifying its learning process with an emphasis on stability and consistency. 
We have conducted extensive experiments across various SR tasks, testing benchmark datasets and diverse network backbones. The consistent outperformance of DUKD in these experiments endorses its promise as a
robust and effective KD method for SR tasks. 
In conclusion, the DUKD framework serves as a KD strategy with effective data utilization, harnessing the power of data upcycling and label consistency regularization to push the boundaries of SR model performance.
\newpage \clearpage
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}


\end{document}
