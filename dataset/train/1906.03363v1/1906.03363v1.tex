\section{Introduction}
A popular way to structure a video is by making use of a shot composition, where shots are delimited by transitions. Since information about the transitions is not available in the video format, automated shot boundary detection is an important step for video management and retrieval systems. For example, information about shots can be employed for video summarization, advanced browsing and filtering in known-item search tasks \cite{CobarzanSBHBLVB17,LokocBSMA18,Lokoc2019}.
Shot changes can be either immediate (hard cuts) or gradual, the later spanning from basic linear interleaving of two shots over a certain number of video frames to more exotic geometric transformations from one shot to another one.
To make matters worse shot boundary detectors must distinguish between shot transitions and sudden changes in a video caused by partial occlusion of the scene by an object passing closer to the camera. Fast camera motion or motion of an object in the scene also should not be mistaken for a shot transition. This may indicate that some semantic representation of a scene is necessary to correctly segment a video.

In this work, we propose TransNet, a scalable architecture with multiple dilated 3D convolutional operations per layer (instead of only one as is usual) resulting in the greater field of view with less trainable parameters. Even though the architecture is trained on just two common types of transitions (hard cuts and dissolves), it achieves state-of-the-art results on the RAI dataset \cite{Baraldi15}.

\section{Related work}
The goal of the shot boundary detection is to temporally segment a video into shots. To determine the shot boundary, one of the first methods utilized thresholded pixel differences \cite{zhang93} effective for stationary shots with a small number of moving objects. Since then, more robust techniques to compare images were developed based on local color histograms, color coherence vectors \cite{Pass1997} or SIFT features. The work of Shao et al. \cite{shao15} utilizes HSV and gradient histograms for shot boundary detection, Apostolidis et al. \cite{apostolidis14} use not only the histogram but also a set of SURF descriptors to detect the differences between a pair of frames. Other approaches revolve around edge information \cite{Huan2008} or motion vectors \cite{amel10}. 

With the advent of deep learning, new methods for shot detection using convolutional neural networks (CNN) emerged. Baraldi et al. \cite{Baraldi15} utilize spectral clustering given a set of features for every frame extracted by a deep siamese network. Recently, Gygli \cite{Gygli18} used a relatively shallow neural network with 3D convolutions with the third dimension over time. Even though 3D convolutions significantly increase computational complexity and memory requirements over standard 2D convolutions due to the added dimension, Gygli has beaten the previous approach in accuracy and speed as well. Another approach by Hassanien et al. \cite{Hassanien17} also uses 3D CNN however its output is fed through SVM classifier and further postprocessing is done to reduce false alarms of gradual transitions through a histogram-driven temporal differencing.
Our work partially overcomes problem of computationally hungry 3D convolutions when a large field of view is required to cope with long gradual transitions by using dilated convolutions over the time dimension, which had been proven useful in speech generation task \cite{oord16}.

The deep learning approaches revolve around the need for large annotated datasets. Until recently \cite{Tang2018}, the size of publicly available datasets for SBD was the limiting factor. Fortunately, synthetic training data can be easily generated from virtually any video content by interleaving randomly selected sequences from different videos as is done in \cite{Gygli18} and others. The downside of this method is, however, that the real data can contain cuts between shots of the same scene which rarely occur in the synthetic data sets due to the nature how they are generated.

\section{Model architecture}

The proposed TransNet architecture (Figure \ref{fig:nn_architecture}) follows the work of Gygli \cite{Gygli18} and other standard convolutional architectures. As an input, the network takes a sequence of $N$ consecutive video frames and applies series of 3D convolutions returning a prediction for every frame in the input. Each prediction expresses how likely a given frame is a shot boundary.

The main building block of the model (Dilated DCNN cell) is designed as four 3D 3$\times$3$\times$3 convolutional operations. The convolutions employ different dilation rates for the time dimension and their outputs are concatenated in the channel dimension. This approach significantly reduces the number of trainable parameters compared to standard 3D convolutions with the same field of view. Multiple DDCNN cells on top of each other followed by spatial max pooling form a Stacked DDCNN block. The TransNet consists of multiple SDDCNN blocks, every next block operating on smaller spatial resolution but a greater channel dimension, further increasing the expressive power and the receptive field of the network.

Two fully connected layers refine the features extracted by the convolutional layers and predict a possible shot boundary for every frame representation independently (layers' weights are shared). ReLU activation function is used in all layers with the only exception of the last fully connected layer with softmax output. Stride 1 and the `same' padding is employed in all convolutional layers.


\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
    box/.style={
    	draw,
    	minimum width=0.7cm,
    	minimum height=0.4cm,
    	font=\scriptsize,
    	rounded corners=2, inner sep=2pt, align=center, anchor=north,
    	text width=1.7cm
    }, pil/.style={
    	-{Stealth[scale=.5]},
    	rounded corners=5pt,
    	line width=1pt
    }]
    \definecolor{my-green}{RGB}{208, 240, 192}
    \definecolor{my-yellow}{RGB}{247, 231, 206}
    \definecolor{my-blue}{RGB}{195, 236, 255}
    \definecolor{my-gray}{RGB}{251, 251, 251}
    \definecolor{my-gray2}{RGB}{245, 245, 245}
    
    
    \node[box, draw=none, text width=3cm] (input) at (0,3.2) {Input\\$N \times width \times height \times 3$};
    
    \node[box, draw=lightgray, text width=8.2cm, text height=2.6cm, fill=my-gray] at (0,2.6) {};

    \node[box, draw=lightgray, text width=8cm, text height=1.6cm, fill=my-gray2] at (0,2.5) {};
    
\node[box, fill=my-green] (layer1-1) at (-3,2) {Conv 3$\times$3$\times$3\\[-2pt]{\tiny dilation 1}};
    \node[box, fill=my-green] (layer1-2) at (-1,2) {Conv 3$\times$3$\times$3\\[-2pt]{\tiny dilation 2}};
    \node[box, fill=my-green] (layer1-3) at (1,2) {Conv 3$\times$3$\times$3\\[-2pt]{\tiny dilation 4}};
    \node[box, fill=my-green] (layer1-4) at (3,2) {Conv 3$\times$3$\times$3\\[-2pt]{\tiny dilation 8}};
    
    \node[box, text width=1.2cm] (layer1-out) at (0,1.3) {Concat};
    
    \node[anchor=south west] at (-4.1,.7) {\tiny \textit{DDCNN cell, each conv with $2^{i-1}F$ channels}};
    \node[anchor=south east, align=right] at (4.1,.7) {\scriptsize stack S times};


    \node[box, anchor=north, fill=my-yellow] (max-pool) at (0,0.6) {Max pooling\\1$\times$2$\times$2};
    
    \node[anchor=south west] at (-4.2,-0.2) {\tiny \textit{SDDCNN block}};
    \node[anchor=south east, align=right] at (4.2,-0.2) {\scriptsize stack L times};
    
    
    \node[box, anchor=north, fill=my-blue] (dense1) at (0,-.3) {Dense D};
    \node[box, anchor=north, fill=my-blue] (dense2) at (0,-.9) {Dense 2};
    \node[box, anchor=north, text width=1.2cm] (softmax) at (0,-1.5) {Softmax};
    \node[box, draw=none] (x) at (0,-2.1) {$N \times 2$};
    
\draw[rounded corners=5pt, line width=1pt] (input) -- (0,2.3);
    
    \draw[pil] (0,2.3) -| (layer1-1);
    \draw[pil] (0,2.3) -| (layer1-2);
    \draw[pil] (0,2.3) -| (layer1-3);
    \draw[pil] (0,2.3) -| (layer1-4);
    
    \draw[pil] (layer1-1) |- (layer1-out);
    \draw[pil] (layer1-2) |- (layer1-out);
    \draw[pil] (layer1-3) |- (layer1-out);
    \draw[pil] (layer1-4) |- (layer1-out);
    
    \draw[pil] (layer1-out) -- (max-pool);
    \draw[pil] (max-pool) -- (dense1);
    \draw[pil] (dense1) -- (dense2);
    \draw[pil] (dense2) -- (softmax);
    \draw[pil] (softmax) -- (x);


    \end{tikzpicture}
    \caption{TransNet shot boundary detection network architecture for $S=1$ and $L=1$. \textmd{Note that $N$ represents length of video sequence, not batch size. In our case $N=100$.}}
    \label{fig:nn_architecture}
\end{figure}

\section{Training}
This section describes the employed dataset and training settings.

\subsection{Dataset}
The TRECVID IACC.3 dataset \cite{2017trecvidawad} was utilized as it is provided with a set of predefined temporal segments. Hence, pairs of the predefined segments can be randomly selected from the pool for automatic creation of transitions for training purposes. More specifically, we considered segments of 3000 IACC.3 randomly selected videos. Furthermore, segments with less than 5 frames were excluded and from the remaining set only every other segment was picked, resulting in selected 54884 segments.

The training examples were generated on demand during training by randomly sampling two shots and joining them by a random type of a transition. Only hard cuts and dissolves were considered for training. Position of the transition was generated randomly. For dissolves, also its length was generated randomly from the interval $[5, 30]$. The length $N$ of each training sequence was selected to be 100 frames. The size of the input frames was set to $48\times 27$ pixels.

In order to validate the models, additional 100 IACC.3 videos (i.e., different from the training set) were manually labeled, resulting in 3800 shots. For testing, the RAI dataset \cite{Baraldi15} was considered.


\subsection{Training details}
The proposed architecture provides the following meta-parameters that were investigated by a grid search:

\begin{enumerate}
\item $S$, the number of DDCNN cells in a SDDCNN layer,
\item $L$, the number of SDDCNN layers,
\item $F$, the number of filters in the first set of DDCNN layers (doubled in each following SDDCNN layer),
\item $D$, the number of neurons in the dense layer.
\end{enumerate}

For training, batch size of 20 was used for all investigated networks. In order to prevent overfitting, only 30 epochs were  considered, each with 300 batches. Adam optimizer \cite{Adam14} with the default learning rate $0.001$ and cross entropy loss function were used. According to our preliminary evaluations, dropout did not improve results. Nevertheless, we plan to investigate advanced forms of regularization and training data augmentation in the future. Depending on the architecture, the whole training took approximately two to four hours to complete on one Tesla V100 GPU.

Even in the case of dissolves, when the transition is over multiple frames, the network was trained to predict only the middle frame as a shot boundary. This creates a discrepancy between the number of `transition' frames (each sequence contains only one) and frames without a transition (99 in our case). 
Increasing the weight of the transitions in the loss function did not produce better results than lowering the acceptance threshold $\theta$ under commonly used $0.5$; therefore, the latter approach is used.

\section{Evaluation}
During validation and testing, the list of shots is constructed in the following way: The shot starts at the first frame when the prediction drops below a threshold $\theta$ and ends at the first frame when the prediction exceeds $\theta$. The evaluation metric described in Section \ref{sec:eval_met} compares the generated shot list with the ground truth. Note that only predictions for frames 25-75 are used due to incomplete temporal information for the first/last frames. Therefore, when processing a video, the input window is shifted by 50 frames between individual forward passes through the network.

\subsection{Evaluation metric}
\label{sec:eval_met}
The F1 score is used as an evaluation metric which is the same metric as in \cite{Baraldi15}. Reported F1 score is computed as an average of individual F1 scores for each video. Based on our analysis of the evaluation script\footnote{Source code of the evaluation method is available at \url{http://imagelab.ing.unimore.it/imagelab/researchActivity.asp?idActivity=19}}, Figure \ref{fig:metricVis} shows cases when detected shots are considered to be true positive, false positive, or false negative. A true positive is detected only if the detected shot transition overlaps with the ground truth transition (3, 4 in green). A false positive is detected if the predicted transition has no overlap with the ground truth (1, 4 in red) or the transition is detected for the second time (3 in red). A false negative is detected if there is no transition overlapping with the ground truth (1, 2 dotted) -- the ground truth transition is missed.


\begin{figure}[ht]
    \centering
    \includegraphics[width=.45\textwidth]{metrics.pdf}
    \caption{Visualization of the evaluation approach. \textmd{Predicted transitions shown with solid and missed with dotted rectangles.}}
    \label{fig:metricVis}
\end{figure}

\subsection{Results}
\begin{figure}
    \centering
    \includegraphics[width=.46\textwidth]{histogram.pdf}
    \caption{Observed average F1 scores of tested networks for the validation and test datasets.}
    \label{fig:F1scores}
\end{figure}

Figure \ref{fig:F1scores} presents the F1 scores of investigated models for validation and test datasets. Note that the top performing weights for each model configuration were selected based on results on validation dataset after each epoch. The confidence threshold $\theta$ indicating transition was set to $\theta=0.1$ as it performed reasonably well for most of the models. The effect of $\theta$ on precision, recall and F1 score is depicted in Figure \ref{fig:prcurve}.

\begin{figure}
    \centering
    \includegraphics[width=.48\textwidth]{prcurve.pdf}
    \caption{Precision/Recall curve for the best performing model with corresponding thresholds $\theta$ next to the points (in red) and F1 score dependency on threshold (in blue). Measured on RAI dataset.}
    \label{fig:prcurve}
\end{figure}

Based on the evaluations presented in Figure \ref{fig:F1scores}, the best performing model is considered the one with 16 filters in the first layer, two stacked DDCNN cells in every one of the three SDDCNN blocks and with 256 neurons in the dense layer (F=16, L=3 S=2, D=256).
The average F1 score $0.94$ of the top performing model on the RAI dataset (see Table \ref{tab:shotDetectors}) is on par with the score reported by Hassanien et al. \cite{Hassanien17}. The overall F1 score even slightly outperforms the work of Hassanien et al., even though they proposed a network with more than 40 times as many parameters trained for a larger set of transition types. Furthermore, our model has the advantage that no additional post-processing is needed.

\begin{table}[b]
    \centering
    \begin{tabular}{r|c|c|c|c}
        & Baraldi et al. & Gygli & Hassanien et al. & ours \\
        \hline
        average & 0.84 \cite{Baraldi15} & 0.88 \cite{Gygli18} & $\mathbf{0.94}$ \cite{Hassanien17} & $\mathbf{0.94}$ \\
        overall & - & - & 0.934 \cite{Hassanien17} & $\mathbf{0.943}$ \\
    \end{tabular}
    
    \caption{Average and overall F1 scores for the RAI test dataset of the best architectures. \textmd{The overall F1 scores are computed by calculating precesion and recall over the whole dataset, not just single video.
    }}
    \label{tab:shotDetectors}
\end{table}

\begin{table}[b]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{Video} & \textbf{\#T}   & \textbf{TP}    & \textbf{FP}    & \textbf{FN}    & \textbf{P}     & \textbf{R}     & \textbf{F1}   \\
        \hline
        V1       &    80 &    57 &     2 &    23 & 0.966 & 0.713 & 0.820\\
        V2       &   146 &   132 &     5 &    14 & 0.964 & 0.904 & 0.933\\
        V3       &   112 &   111 &     4 &     1 & 0.965 & 0.991 & 0.978\\
        V4       &    60 &    59 &     5 &     1 & 0.922 & 0.983 & 0.952\\
        V5       &   104 &   101 &     8 &     3 & 0.927 & 0.971 & 0.948\\
        V6       &    54 &    53 &     3 &     1 & 0.946 & 0.981 & 0.964\\
        V7       &   109 &   103 &     1 &     6 & 0.990 & 0.945 & 0.967\\
        V8       &   196 &   181 &     4 &    15 & 0.978 & 0.923 & 0.950\\
        V9       &    61 &    55 &     2 &     6 & 0.965 & 0.902 & 0.932\\
        V10      &    63 &    57 &     0 &     6 & 1.000 & 0.905 & 0.950\\
        \hline
        Overall  &   985 &   909 &    34 &    76 & 0.964 & 0.923 & 0.943\\
        \hline
    \end{tabular}
    
    \caption{Per video results on the RAI dataset. \textmd{For each video the total number of transitions (\#T), true positives (TP), false positives (FP), false negatives (FN), precision (P), recall (R) and F1 score (F1) are shown.}}
    \label{tab:resultsRAI}
\end{table}

Since the validation dataset contains various sequences of frames where even annotators are not sure whether there is a shot transition, the reported scores for the validation data are lower. In addition, even the top performing TransNet model faces problems with detection of some transitions, for example, false positives in dynamic shots and false negatives in gradual transitions.

The model detected 1058 false positives and 679 false negatives with respect to the annotation. After closer inspection, for about 20\% of false negatives there was one very close false positive (shifted by one frame). This is in contrast to the RAI dataset results (Table \ref{tab:resultsRAI}) where the network achieves a lower number of false positives than false negatives. Based on manual inspection of the videos we conclude that RAI videos do not contain many highly dynamic shots (i.e. resulting in false positives) compared to the IACC.3 validation set.

\section{Conclusion}

In this paper, we present the TransNet neural network, the first shot detection model based on dilated 3D convolutions. The effectiveness of dilated 3D convolutions has been shown on RAI dataset with the TransNet performing on par with the current state-of-the-art approach without any additional post-processing and with a fraction of learnable parameters. The network also runs more than 100x faster than real-time on a single powerful GPU\footnote{It took just 50s to detect shot boundaries of preprocessed frames from the whole RAI dataset (about 98 minutes of video) using Tesla V100 GPU.}.

In the future, we plan to do further evaluation and improvements to enable deeper and more robust models.
The source code and our trained model will be available at \url{https://github.com/soCzech/TransNet}.


\begin{acks}
 This paper has been supported by Czech Science Foundation (GA\v{C}R) project Nr. 19-22071Y.
\end{acks}
