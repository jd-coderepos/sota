\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}


\usepackage{tikz,xcolor,hyperref} \usepackage{makecell}


\begin{document}

\title{Train a Real-world Local Path Planner in One Hour via Partially Decoupled Reinforcement Learning and Vectorized Diversity}

\author{Jinghao Xin, Jinwoo Kim, Zhi Li, and Ning Li
\thanks{This work was supported by the National Natural Science Foundation of China (62273230 and 62203302). \textit{(Corresponding authors: Ning Li.)} \par Jinghao Xin, Zhi Li, and Ning Li are with the Department of Automation, Shanghai Jiao Tong University, Shanghai 200240, P.R. China, and also with Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai 200240, China, and also with Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai 200240, China (E-mail: xjhzsj2019, lizhibeaman, ning\_li@sjtu.edu.cn). \par Jinwoo Kim is with the School of Civil and Environmental Engineering, Nanyang Technological University, Singapore, S639798 (E-mail: jinwoo.kim@ntu.edu.sg)}}





\maketitle

\begin{abstract}
Deep Reinforcement Learning (DRL) has exhibited efficacy in resolving the Local Path Planning (LPP) problem. However, such application in the real world is immensely limited due to the deficient efficiency and generalization capability of DRL. To alleviate these two issues, a solution named Color is proposed, which consists of an Actor-Sharer-Learner (ASL) training framework and a mobile robot-oriented simulator Sparrow. Specifically, the ASL framework, intending to improve the efficiency of the DRL algorithm, employs a Vectorized Data Collection (VDC) mode to expedite data acquisition, decouples the data collection from model optimization by multithreading, and partially connects the two procedures by harnessing a Time Feedback Mechanism (TFM) to evade data underuse or overuse. Meanwhile, the Sparrow simulator utilizes a 2D grid-based world, simplified kinematics, and conversion-free data flow to achieve a lightweight design. The lightness facilitates vectorized diversity, allowing diversified simulation setups across extensive copies of the vectorized environments, resulting in a notable enhancement in the generalization capability of the DRL algorithm being trained. Comprehensive experiments, comprising 57 benchmark video games, 32 simulated and 36 real-world LPP scenarios, have been conducted to corroborate the superiority of our method in terms of efficiency and generalization. The code and the video of the experiments can be accessed on our website\footnote{Code: https://github.com/XinJingHao/Color}\footnote{Experiment video: https://youtu.be/16IZcqJ1jZY}.
\end{abstract}

\begin{IEEEkeywords}
	Local path planning (LPP), partially decoupled Deep Reinforcement Learning, vectorized diversity
\end{IEEEkeywords}

\section{Introduction}

\IEEEPARstart{L}{ocal} Path Planning portrays a pivotal role in the autonomous navigation of mobile robots, endowing them with the capability to execute unmanned missions such as disaster rescue, military reconnaissance, and material distribution. Given the static path generated by the global path planning algorithm, the LPP yields a collision-free path in accordance with certain metrics to circumvent the newly arisen obstacles while navigating. Traditional LPP approaches, such as Dynamic Window Approach\cite{DWA} and Fuzzy Logic Algorithm\cite{FLA}, suffer from a common flaw of relying on expert knowledge to deliberately engineer the obstacle avoidance parameters and principles, which may incur performance deterioration when confronting complex environment\cite{DWA_flaw,UAV_review}.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{img/Color_img.pdf}
	\caption{Color: the left part represents the diversified training maps and the simulation parameters across the vectorized environments of Sparrow; the right part is the ASL training framework.}
	\label{Color}
\end{figure}

Deep Reinforcement Learning (DRL) is promising to alleviate the aforementioned issue. In DRL, the agent learns by autonomously interacting with the environment and is capable of attaining human-level decision-making and control performance. To date, DRL has made remarkable strides in fields such as video games\cite{DQN}, chess\cite{AlphaGo}, and ChatGPT\cite{ChatGPT}. Nevertheless, despite these momentous breakthroughs, the applications of DRL have been predominantly restricted to the virtual or simulated world. The disappointing efficiency and generalization of DRL have formed a longstanding impediment to its real-world applications. (1) \textit{Sample efficiency}: the training of the DRL agent is exceptionally demanding in terms of training samples, which can be prohibitively costly to collect in the real world. (2) \textit{Training time efficiency}: it could consume days or even weeks to train a desirable DRL agent in complex environments. (3) \textit{Simulation to real world generalization (Sim2Real)}: to sidestep the unacceptable training time in the real world, researchers typically resort to the accelerated physics simulation platform, with the expectation that the model trained in a simulator can be transferred to real-world situations. However, the discrepancy between the simulated and the real world is challenging to diminish, thus significantly impeding the transfer. (4) \textit{Task to Task generalization (Task2Task)}: the DRL agent is prone to overfit the training environment, resulting in performance degeneration as the environment changes. Consequently, ameliorating the efficiency and generalization of DRL is challenging but advantageous, which is the overarching purpose of this paper. In summary, our primary contributions can be drawn as follows.

\begin{enumerate}
\item{A partially decoupled training framework, namely Actor-Sharer-Learner (ASL), is proposed to improve the efficiency of the DRL algorithm. Experiments demonstrate that mainstream off-policy DRL algorithms with experience replay can be readily integrated into our framework to enjoy a remarkable promotion.}
\item{A lightweight mobile robot-oriented simulator named Sparrow has been developed. The lightness of Sparrow boosts its vectorization, which not only expedites data generation but also enables vectorized diversity, allowing diverse simulation setups across different copies of the vectorized environments to enhance the generalization of the trained DRL algorithms.}

\item{Combining the ASL with Sparrow through their interdependent vectorized environments, we formulate our DRL solution to the LPP problem, referred to as Color. Through one hour of simulation training, the Color is capable of yielding a real-world local path planner with laudable resilience to generalize over a wide variety of scenarios.}
\end{enumerate}

The remainder of this paper is organized as follows. Related works concerning the efficiency, generalization, and commonly used simulators of DRL are reviewed in Section \ref{section2}. An efficient training framework of DRL and a lightweight simulator that remarkably enhances the generalization capability of the framework are proposed in Section \ref{section3}. Comprehensive experiments involving 57 benchmark video games, 32 simulated and 36 real-world LPP problems have been conducted in Section \ref{section4} to corroborate the effectiveness of our approaches. The conclusion is derived in Section \ref{section5}.

\section{Related Works}
\label{section2}

This section provides a review of related works aimed at enhancing the efficiency and generalizability of DRL algorithms and the simulation platforms commonly utilized in DRL. Each subsection concludes with an analysis of the limitations of these works.

\subsection{Efficiency of DRL}
The prevalence of DRL has been severely bottlenecked by its unsatisfactory sample efficiency. To mitigate this issue, numerous methods have been proposed. From the perspective of model architecture, Wang \textit{et al.} \cite{duel} integrated the dueling network into the DRL. The dueling network enables information generalizing across actions, thus reducing the samples needed for training. Fortunato \textit{et al.} \cite{noisy} injected the linear layer of the DRL model with learnable random variables such that the derived model could induce state-dependent, consistent exploration strategies so as to generate high-quality data. From the perspective of sample utilization, Schaul \textit{et al.} \cite{PER} devised the Prioritized Experience Replay (PER). In contrast to uniformly sampling from past experiences, PER samples experiences in accordance with their respective TD-error. The TD-error mirrors how “surprising” one experience is, thus prompting the agent to focus on the instrumental samples. However, the PER may lead to data overuse or even model collapse. To tackle this issue, Wei \textit{et al.} \cite{QER} augmented the TD-error with a quantum-inspired representation to ensure the diversity of the replayed experiences. One common drawback of the aforementioned methods is that the training procedure is pended during the agent's interaction with the environment. This alternation between model optimization and data collection results in the low training time efficiency of these methods.

 To elevate the training time efficiency of DRL, Mnih \textit{et al.} \cite{A3C} proposed the Asynchronous Advantage Actor-Critic (A3C) framework. With the aid of distributed data collection and asynchronous model optimization, A3C remarkably shortens the training time versus the standard DRL method. However, the training data in A3C is discarded immediately once the gradients have been computed, giving rise to its low sample efficiency. Contrastingly, the Ape-X\cite{apex} framework employed a shared memory to store the experiences generated by these distributed agents and optimized the model in an off-policy manner so that the experiences could be reused. Analogously to A3C, the Ape-X decoupled the data collection and model optimization procedures into multi-actor and one learner, then ran them concurrently to improve training time efficiency. Nevertheless, there are two notable drawbacks in the Ape-X framework. First, the multi-thread deployed actor mode is inefficient, which could exert three adverse repercussions upon the learning system: a) if the Forward Propagation (FP) of the network is computed by the CPU, it precludes the usage of large-scale networks because the CPU is notorious for its inefficiency in large scale networks computing. b) if the FP is computed by the GPU, the parallel computing feature of the GPU is not fully utilized, for the computation is requested separately and asynchronously by actors. c) the data flow in Ape-X is cumbersome and could be bottlenecked by the transmission bandwidth, where the data is firstly accumulated by the local buffer of each actor and then packed into the shared memory by a background thread. Second, the actors and learner are completely decoupled without coordinating their running speeds. If actors run far slower than the learner, it’s likely that the data is overused, which could adversely impact its stability. Otherwise, the data might be abandoned without full utilization, resulting in low sample efficiency. In the experiments of \cite{apex}, despite being equipped with dueling networks and PER to enhance its sample efficiency, Ape-X requires a hundred times more samples to surpass other DRL baselines. Since the relative speed of the actors and learner can be influenced by the breakdown of actors or fluctuates with the hardware occupation\cite{apex}, these two unfavorable cases are inevitable. Hence, further works need to be done to resolve the shortcomings of existing research.


\subsection{Generalization of DRL}
Methods concerning improving the Sim2Real generalization primarily involve three categories. (1) System identification\cite{SI}: define and tune the parameters to match the real-world environment. (2) Domain Randomization (DR)\cite{DR}: randomize the simulation setups while training such that the real-world environment can be deemed as another variation of these randomized environments. (3) Domain adaptation\cite{DualDA,S2R_survey}: adapt the trained model by learning mapping or invariant features from simulation to the real world, or re-training in the real world. Regarding Task2Task generalization, the methodologies mainly encompass four classifications. (1) Employing apposite task description: RMDP\cite{RMDP} and MOO-MDP\cite{MOO-MDP}. (2) Introducing diversity: randomly generated visual properties\cite{Procgen}, randomly initialized agent and environment\cite{LiZhi}. (3) Data augmentation: image processing techniques\cite{DA_img}, automatic data augmentation\cite{ADA}, and random convolution\cite{RandomConv}. (4) Regularization\cite{CoinRun}: L2-regularization, dropout, and batch normalization. Although a wide range of studies has been carried out on the generalization of DRL, their focuses, however, are limited to the standard DRL framework. The combination of such techniques with high-efficient training frameworks, such as decoupled framework, has not been carefully discussed, which we believe is indispensable and could be a desirable incentive to the wide-ranging application of DRL.

\subsection{Simulation Platform}
Gazebo\cite{gazebo} is a 3D physics-based robotic simulation platform and has been wildly employed in various robotic systems, such as Automated Guided Vehicles (AGVs)\cite{LiZhi}, manipulators\cite{manipulator}, and Unmanned Aerial Vehicles (UAVs)\cite{UAVs}. Nonetheless, the Gazebo is demanding in computational resources due to the complicated physics calculation, which results in low simulation speed and considerably impairs the efficiency of the trained DRL algorithm\cite{Gazebo_slow1,Gazebo_slow2,Gazebo_slow3}. CoinRun\cite{CoinRun} and Procgen\cite{Procgen} are two simulators that intend to ameliorate the generalization dilemma of the DRL agent by providing procedurally-generated visual diversity. Regrettably, it must be noted that CoinRun and Procgen are specifically designed for video games and lack support for physics simulation. As a result, they are unable to contribute to improving the generalization capabilities of agents confronting real-world applications.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=1\textwidth]{img/ASL.pdf}
	\caption{Schematic of the Actor-Sharer-Learner training framework}
	\label{ASL}
\end{figure*}

\section{Methodology}
\label{section3}

In this section, we will first elaborate on our efficient training framework ASL and the mobile robot-oriented simulator Sparrow, and then discuss how to elegantly combine them together, fully taking their respective advantage, to form the DRL solution to LPP problems, namely the Color.

\subsection{Actor-Sharer-Learner Training Framework}

The Related Works concerning the efficiency of DRL reveal a contradiction between sample efficiency and training time efficiency. Despite extensive efforts to tackle these challenges, existing approaches remain separate. We argue that both the sample and training time efficiency are prominent for DRL, and improving them simultaneously could profoundly boost the application of DRL. In pursuit of this goal, we proposed the ASL training framework. 

To equip our framework with high training time efficiency, we inherit the decoupled architecture from Ape-X. As shown in Fig. \ref{ASL}, an actor and a learner are employed to collect data and optimize the model individually. The actor and learner run concurrently in different threads, and the information that needs to be shared is managed by a sharer. As aforementioned, the Ape-X is sorely restricted by its multi-thread deployed actor mode and completely decoupled architecture. We tackle the first issue by resorting to the vectorized environments \cite{envpool} and the second issue by adding a Time Feedback Mechanism (TFM) between the actor and the learner. 

\subsubsection{Vectorized Data Collection (VDC)}
The vectorized environments runs multiple copies of the same environment parallelly and independently, interacting with the actor in a batched fashion. The superiority of vectorized environments is evidently. First, the vectorized environments interacts with the actor in a batched way such that the parallel computation advantage of GPU could be exploited to the fullest extent. Second, when interacting with vectorized environments of  copies, the VDC mode necessitates only a single instantiation of the actor network due to the batched interaction, leading to a substantial reduction in hardware resource requirements in contrast to the multi-thread deployed actor mode that requires  instantiations. Third, the batched data is stored as a whole in the experience buffer, eliminating the need for the thread lock stemming from asynchronous data preservation requests caused by multiple actors. Accordingly, it is reasonable to imagine that the three merits could additionally shorten the time needed for training.



Correspondingly, a Vectorized -greedy Exploration Mechanism (VEM) is devised. In canonical -greedy exploration, the agent executes its own policy at a probability of  and explores the environment at a probability of . The exploration noise  is linearly or exponentially decreasing during training. Although the -greedy exploration is promising to maintain a balance between exploitation and exploration, its limitation is self-evident. The exploration is only guaranteed at the beginning of training and is eventually replaced with exploitation due to the decreasing noise. We argue this setting can be irrational in some contexts, especially when the environment is progressively changing as the ability of the agent improves. For instance, in some video games, the agent takes extensive exploration to pass the current scenario and reaches a new scenario, only to find the exploration noise is decreased to a low level, and sufficient exploration can no longer be guaranteed. Contrastingly, the VEM divides the vectorized environments into expl\textbf{oi}ting interval (OI) and expl\textbf{or}ing interval (OR). As shown in Fig.\ref{VEM}, the exploration noises maintain the minimal value  in exploiting interval and are linearly interpolated from  to the maximal value  in the exploring interval. In addition, the exploring interval is linearly diminished to a certain extent during the training process. Relative to standard -greedy exploration, the VEM inherits its positive attribute: progressively focus on exploitation during training. Meanwhile, the VEM also bypasses its flaw: conserve a sound exploration capability even in the final training stage. Consequently, the VEM reaches a more sensible balance between exploration and exploitation, thus enhancing the sample efficiency accordingly.



\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\textwidth]{img/VEM.pdf}
	\caption{Vectorized -greedy Exploration Mechanism. Here, the horizontal axis is the index of the vectorized environments, and  is the total number of vectorized environments.}
	\label{VEM}
\end{figure}

\subsubsection{Time Feedback Mechanism}

In off-policy DRL with experience replay mechanism, one can train the model multiple times after one interaction with the environment or a single update after multiple interactions. To quantify the data utilization rate, we define the \textit{Transitions Per Step (TPS)}:



\noindent where  is the batch size,  is the total backpropagation steps (the number of times that the model has been optimized), and  is the total interacting steps. The \textit{TPS} indicates how many transitions have been replayed for training per interaction. A large \textit{TPS} implies a high data utilization rate but may also lead to model collapse due to data overuse\cite{overuse}. Conversely, a small \textit{TPS} results in data underuse, resulting in low sample efficiency. Since the Ape-X expedites the training at the expense of completely decoupling the interaction and optimization procedures without controlling the \textit{TPS}, it is inevitable to incur a loss of stability or sample efficiency. In contrast, the ASL employs the TFM to modulate the relative speed between actor and learner, imposing a certain amount of dormant period upon the faster one of actor and learner to coordinate the \textit{TPS} of the learning system, as illustrated by Fig.\ref{TFM}. In doing so, the data overuse or underuse phenomenon could be circumvented. In vectorized environments, based on (\ref{TPS}), we have the following equation:


\noindent where  is the number of copies of vectorized environments and  is the total number of vectorized steps. To connect (\ref{V2B}) with time feedback, we define the period, namely the elapsed time, of one VDC procedure and one model optimization procedure as  and , respectively. We also use  to denote  for brevity. Then, we have

\noindent where  is the elapsed time of ASL. Evidently, the following equation holds:


(\ref{TB2TV}) suggests that by modulating the two elapsed times in a manner consistent with such relative relationship, a fixed \textit{TPS} can be approximately upheld. To achieve this end, the actor and learner would record their individual period  and . The sharer then coordinates their running speed according to the two following principles:

\begin{itemize}
	\item{if , the actor sleeps for a period of  every after one VDC procedure.}
	
	\item{if , the learner sleeps for a period of  every after one model optimization procedure.}
\end{itemize}

Regarding the dormancy in TFM, one might wonder whether the TFM maintains the sample efficiency at the cost of sacrificing the training time efficiency, which deviates from our rudimentary purpose. Indeed, the dormancy could slow down the training framework to some extent but bring about a more sensible data recency, which tremendously facilitates model optimization. That is, reaching the same performance with fewer times of optimization, which conversely reduces the time needed for training.

\begin{figure}
	\centering
	\includegraphics[width=0.49\textwidth]{img/TFM.pdf}
	\caption{Illustration of the coupled framework (left, representative: DQN), completely decoupled framework (middle, representative: Ape-X), partially decoupled framework with TFM (right, representative: ASL).}
	\label{TFM}
\end{figure}

\subsubsection{Workflow of ASL}
The pseudocode of ASL is given in Algorithm 1 and 2, and the corresponding schematic is shown in Fig \ref{ASL}. In ASL, the actor and learner run simultaneously until reaching the maximum interaction steps . Note that, to avoid overfitting, the learner starts to learn only after a certain amount of transitions have been collected.

\begin{algorithm}[!t]
	\caption{Actor}
	\begin{algorithmic}
		\STATE Initialize the maximal interaction steps , vectorized environments (envs) with  copies, total steps .
		\STATE  = envs.reset() \hspace{0.2cm}  \textit{Generate the initial batched state}
		\STATE {\textbf{while}} :
		\STATE \hspace{0.5cm}   \hspace{0.2cm}  \textit{Inquire the Sharer whether there is a new model to download.}
		
		\STATE \hspace{0.5cm}   =  \hspace{0.2cm}  \textit{Map the batched state to batched action with the latest policy .} 
		
		\STATE \hspace{0.48cm}   = VEM  \hspace{0.2cm}  \textit{Inject the batched action with stochastic action by the VEM.}
		
		\STATE \hspace{0.53cm}   = envs.step()
		
		\STATE \hspace{0.41cm}  Sharer.buffer.add() \hspace{0.2cm}  \textit{Add the batched transitions to experience buffer of Sharer.}
		
		\STATE \hspace{0.5cm}   Sharer \hspace{0.2cm}  \textit{Record the mean interacting time from Step 1 to 5, and send it to the Sharer.}
		
		\STATE \hspace{0.5cm}  \textbf{if}  : sleep() \hspace{0.2cm}  \textit{Fetch the relative time from the Sharer, and sleep for  seconds if the Actor runs faster.}
		
		\STATE \hspace{0.5cm}   , , Sharer.buffer.size 
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!t]
	\caption{Learner}
	\begin{algorithmic}
		\STATE Initialize the Learner model parameters , learning start steps , model upload frequency , total backpropagation steps .
		
		\STATE {\textbf{while}} :
		
		\STATE \hspace{0.5cm} {\textbf{if}} Sharer.buffer.size  :
		
		\STATE \hspace{1cm}   = Sharer.buffer.sample() \hspace{0.2cm}  \textit{Sample a mini-batch of transitions from the Sharer. }
		
		\STATE \hspace{0.95cm}   = DRL \hspace{0.2cm}  \textit{Optimize the model with the underlying DRL algorithm. }
		
		\STATE \hspace{1cm}  \textbf{if} \ =\ 0:  \hspace{0.2cm}  \textit{Upload the latest model to the Sharer at a fixed frequency . }
		
		\STATE \hspace{0.95cm}   Sharer \hspace{0.2cm}  \textit{Record the mean optimization time from Step 1 to 3, and send it to the Sharer.}
		
		\STATE \hspace{0.98cm}  \textbf{if}  : sleep() \hspace{0.2cm}  \textit{Fetch the relative time from the Sharer, and sleep for  seconds if the Learner runs faster.}		
		
		\STATE \hspace{0.94cm}  
		
	\end{algorithmic}
\end{algorithm}


\subsection{Sparrow simulator}

\begin{figure}
	\centering
	\subfloat[]{\includegraphics[width=0.2\textwidth]{img/Sparrow.jpg}
		\label{Sparrow}}
	\hfil
	\subfloat[]{\includegraphics[width=0.166\textwidth]{img/robot.png}
		\label{robot}}
	\caption{(a) Sparrow simulator: the green area represents the target area; the black rectangles are the obstacles; the blue rays and purple circle denote the LiDAR and the robot, respectively. (b) Real-world counterpart of the robot in Sparrow: a differential wheeled robot.}
	\label{S&R}
\end{figure}

Sparrow is a mobile robot-oriented simulation platform, as shown in Fig.\ref{S&R}(a). The primary function of Sparrow is LPP, but it can also be readily configured to accommodate other problems, such as mapping, with merely minimal effort. The development of Sparrow is triggered by the niche, as detailed in Section \ref{section2}, between two types of widely adopted simulators, the Gazebo and the Procgen. The Gazebo is inordinately heavy and thus forms an impediment to parallelization as well as fast data generation, while the Procgen is purely video game-oriented, unable to foster the real-world application of DRL. Consequently, we posed a question to ourselves: is it possible to propose a mobile robot-oriented simulator that is lightweight to facilitate data acquisition and diversifiable to augment the generalization capacity of the DRL agent? 

To this end, we employed a 2D grid-based world and a simplified kinematic model to describe the obstacles and robot, circumventing the complicated physics calculation. As a result, the Sparrow occupies merely 30KB of hard disk memories and 150MB (per environment) of the GPU running memories. Even with extensive vectorization, the hardware resources would not be extravagantly consumed, which prominently hastens the data collection by aggregating samples from more environment threads under the same computational resources. More importantly, another noteworthy benefit brought about by lightness is vectorized diversity. That is, diversified simulation setups could be adopted by different copies of the vectorized environments and be simulated simultaneously. In doing so, diverse experiences could be generated by the Sparrow, and the generalization capability of the DRL agent is promising to be bolstered. The name “Sparrow” actually originates from an old saying “A sparrow may be small, but it has all the vital organs.” Due to the limited scope, an exhaustive elucidation of Sparrow can not be carried out in this paper but could be accessed on our website\footnotemark[1]. However, two interpretations of pivotal details are necessary to gain a deeper understanding of Sparrow.


\subsubsection{Simplified kinematic model}
The kinematics of the robot in Sparrow is described as:


\noindent where  denotes the velocity of the robot, the subscripts  and  are the respective abbreviations of linear and angular velocities, the superscripts  and  respectively represent the timestep and target velocity, and  is a hyperparameter between (0,1) that describes the combined effect of inertia, friction, and the underlying velocity control algorithm. We simplify the kinematic model in this way for two reasons. First, such representation evades complicated physics calculation, conducing to the lightness of Sparrow. Second, we argue that it is inefficient or even unwise to separately investigate the coupling effects resulting from inertia, friction, and the underlying velocity control algorithm due to their intricate interplay in the real world. Since these factors jointly affect how the velocity of the robot changes given the target velocity, we employed the parameter  to represent the final effects. Although  cannot be measured accurately in real-world or even varies in different scenarios, we can guarantee the effectiveness of the simplified kinematics by utilizing vectorized diversity to train a generalized agent.

\subsubsection{Conversion-free data flow}
Due to disparate implementation, most simulators to date have different data flow compared with the DRL algorithm. Consequently, it is inevitable to perform certain data format conversions during training. A common case is from the \textit{List} format in \textit{Python}\footnote{See https://www.python.org} (generated by the simulator, stored in CPU) to the \textit{Tensor} format in \textit{Pytorch}\footnote{See https://pytorch.org} (utilized by the Deep Neural Networks, stored in GPU). Such conversion is tedious and time-consuming, which slows down the training and can be even aggravated in a large-scale learning framework that requires high data throughput. To tackle this issue, we implemented the Sparrow with the prevalent machine learning framework \textit{Pytorch} such that the data generated by Sparrow is already in \textit{Pytorch.Tensor} format, thus obviating the conversion procedure. To accommodate the needs of users not using \textit{Pytorch}, we also provide the \textit{Numpy}\footnote{See https://numpy.org} (a popular scientific computing package in Python) data format.

Finally, we would like to introduce three promising applications of Sparrow. First, as well as the most discussed, Sparrow is capable of fostering the efficiency and generalization of the trained DRL algorithm by providing vectorized diersity. Second, the broad trend in the DRL community to date is evaluating the performance of agent on the training environment. We contend such evaluation is irrational to some extent because it is equivalent to evaluate on the training set in supervised learning. Thanks to the diversifiable characteristics of Sparrow, it is feasible to quantify the generalization of the DRL agent more precisely by constructing different simulation parameters or maps during evaluation. Last, the lightness and the conversion-free data flow of Sparrow render it notably user-friendly, thereby promoting the potential for expeditious deployment, assessment, and comparison of DRL algorithms. For these reasons, we hold the belief that Sparrow could be deemed as a holistic simulation platform and would profitably contribute to the community of DRL researchers and practitioners.


\subsection{Color}

Having comprehended the ASL framework and the Sparrow simulator, the only remaining question pertains to their optimal amalgamation. That is, forming a comprehensive solution that enables the efficient training of a DRL agent with adequate resilience to generalize over a series of analogous scenarios as well as the real-world scenario. Our method, named Color, is illustrated in Fig.\ref{Color}, wherein the ASL and the Sparrow are seamlessly integrated by their interdependent vectorized environments, and the training maps and simulation parameters are randomized in different copies of the vectorized environments of Sparrow. Through interacting with the vectorized and diverse environments, the actor of ASL is empowered to gather a collection of “colorful” experiences, which is then preserved in the sharer and prepared for the model optimization of the learner. In this fashion, the efficiency of our method is guaranteed by the ASL framework, and the generalization of the trained agent is promised by learning from the “colorful” experience generated by Sparrow. 

\section{Experiments}
\label{section4}

In this section, three types of experiments are carried out to corroborate the superiority of our method. The first type focus on efficiency of ASL. Comprehensive experiments have been conducted on 57 Atari games\cite{atari}, and the results are compared with other DRL baselines previously published, both qualitatively and quantitatively. The second type concentrates on Task2Task generalization, seeking to investigate the generalization gain induced by the Sparrow simulator. The third type puts efforts upon Sim2Real generalization, where the agent trained by Color is assessed via 36 real-world scenarios. 


\subsection{Efficiency}
\subsubsection{Experiment Setup on Atari}

\begin{table*}[htbp]
\centering
	\caption{Raw Score Comparison Across 57 Atari Games}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{lrrrrrrrrrrr}
			\hline
			\textbf{Game} & \textbf{Random} & \textbf{DQN} & \textbf{DDQN} & \textbf{Prior. DDQN} & \textbf{Duel. DDQN} & \textbf{Noisy DQN} & \textbf{DSQN} & \textbf{Ape-X DDQN} & \textbf{ASL DDQN (Ours)} & \textbf{Ipv1} & \textbf{Ipv2} \\
			\hline
			Alien & 227.8 & 3,069.0 & 2,907.3 & 4,203.8 & 4,461.4 & 2,403.0 & -     & 2,596.1 & \textbf{     6,955.2 } & 251.1\% & 284.1\% \\
			Amidar & 5.8   & 739.5 & 702.1 & 1,838.9 & \textbf{2,354.5} & 1,610.0 & -     & 1,271.2 &      2,232.3  & 319.8\% & 176.0\% \\
			Assault & 222.4 & 3,359.0 & 5,022.9 & 7,672.1 & 4,621.0 & 5,510.0 & -     & \textbf{14,490.1} &     14,372.8  & 294.8\% & 99.2\% \\
			Asterix & 210.0 & 6,012.0 & 15,150.0 & 31,527.0 & 28,188.0 & 14,328.0 & -     & 189,300.0 & \textbf{ 567,640.0 } & 3798.1\% & 300.1\% \\
			Asteroids & 719.1 & 1,629.0 & 930.6 & 2,654.3 & 2,837.7 & \textbf{3,455.0} & -     & 1,107.9 &      1,984.5  & 598.3\% & 325.5\% \\
			Atlantis & 12,850.0 & 85,641.0 & 64,758.0 & 357,324.0 & 382,572.0 & 923,733.0 & 487,366.7 & 831,952.0 & \textbf{ 947,275.0 } & 1800.2\% & 114.1\% \\
			BankHeist & 14.2  & 429.7 & 728.3 & 1,054.6 & \textbf{1,611.9} & 1,068.0 & -     & 1,264.9 &      1,340.9  & 185.8\% & 106.1\% \\
			BattleZone & 2,360.0 & 26,300.0 & 25,730.0 & 31,530.0 & 37,150.0 & 36,786.0 & -     & 38,671.0 & \textbf{   38,986.0 } & 156.7\% & 100.9\% \\
			BeamRider & 363.9 & 6,846.0 & 7,654.0 & 23,384.2 & 12,164.0 & 20,793.0 & 7,226.9 & \textbf{27,033.7} &     26,841.6  & 363.2\% & 99.3\% \\
			Berzerk & 123.7 & -     & -     & 1,305.6 & 1,472.6 & 905.0 & -     & 924.8 & \textbf{     2,597.2 } & -     & 308.8\% \\
			Bowling & 23.1  & 42.4  & 70.5  & 47.9  & 65.5  & \textbf{71.0} & -     & 60.2  &           62.4  & 82.9\% & 105.9\% \\
			Boxing & 0.1   & 71.8  & 81.7  & 95.6  & 99.4  & 89.0  & 95.3  & 99.4  & \textbf{         99.6 } & 121.9\% & 100.2\% \\
			Breakout & 1.7   & 401.2 & 375.0 & 373.9 & 345.3 & 516.0 & 386.5 & 370.7 & \textbf{       621.7 } & 166.1\% & 168.0\% \\
			Centipede & 2,090.9 & \textbf{8,309.0} & 4,139.4 & 4,463.2 & 7,561.4 & 4,269.0 & -     & 3,808.4 &      3,899.8  & 88.3\% & 105.3\% \\
			ChopperCommand & 811.0 & 6,687.0 & 4,653.0 & 8,600.0 & 11,215.0 & 8,893.0 & -     & 6,031.0 & \textbf{   15,071.0 } & 371.2\% & 273.2\% \\
			CrazyClimber & 10,780.5 & 14,103.0 & 101,874.0 & 141,161.0 & 143,570.0 & 118,305.0 & 123,916.7 & 118,020.0 & \textbf{ 166,019.0 } & 170.4\% & 144.8\% \\
			Defender & 2,874.5 & -     & -     & 31,286.5 & \textbf{42,214.0} & 20,525.0 & -     & 29,255.0 &     37,026.5  & -     & 129.5\% \\
			DemonAttack & 152.1 & 9,711.0 & 9,711.9 & 71,846.4 & 60,813.3 & 36,150.0 & -     & 114,874.7 & \textbf{ 119,773.9 } & 1251.3\% & 104.3\% \\
			DoubleDunk & -18.6 & -18.1 & -6.3  & \textbf{18.5} & 0.1   & 1.0   & -     & -0.2  &             0.1  & 152.0\% & 101.6\% \\
			Enduro & 0.0   & 301.8 & 319.5 & 2,093.0 & \textbf{2,258.2} & 1,240.0 & -     & 1,969.1 &      2,103.1  & 658.2\% & 106.8\% \\
			FishingDerby & -91.7 & -0.8  & 20.3  & 39.5  & \textbf{46.4} & 11.0  & -     & 31.2  &           35.1  & 113.2\% & 103.2\% \\
			Freeway & 0.0   & 30.3  & 31.8  & 33.7  & 0.0   & 32.0  & -     & 21.4  & \textbf{         33.9 } & 106.6\% & 158.4\% \\
			Frostbite & 65.2  & 328.3 & 241.5 & 4,380.1 & 4,672.8 & 753.0 & -     & 504.7 & \textbf{     8,616.4 } & 4850.4\% & 1945.7\% \\
			Gopher & 257.6 & 8,520.0 & 8,215.4 & 32,487.2 & 15,718.4 & 14,574.0 & 10,107.3 & 47,845.6 & \textbf{ 103,514.4 } & 1297.6\% & 217.0\% \\
			Gravitar & 173.0 & 306.7 & -     & 548.5 & 588.0 & 447.0 & -     & 242.5 & \textbf{       760.0 } & -     & 844.6\% \\
			Hero  & 1,027.0 & 19,950.0 & 20,357.0 & 23,037.7 & 20,818.2 & 6,246.0 & -     & 14,464.0 & \textbf{   26,578.5 } & 132.2\% & 190.2\% \\
			IceHockey & -11.2 & -1.6  & -2.4  & \textbf{1.3} & 0.5   & -3.0  & -     & -2.5  &           -3.6  & 86.4\% & 87.4\% \\
			Jamesbond & 29.0  & 576.7 & 438.0 & \textbf{5,148.0} & 1,312.5 & 1,235.0 & 1,156.7 & 540.0 &      2,237.0  & 539.9\% & 432.1\% \\
			Kangaroo & 52.0  & 6,740.0 & 13,651.0 & \textbf{16,200.0} & 14,854.0 & 10,944.0 & 8,880.0 & 14,710.0 &     13,027.0  & 95.4\% & 88.5\% \\
			Krull & 1,598.0 & 3,805.0 & 4,396.7 & 9,728.0 & \textbf{11,451.9} & 8,805.0 & 9,940.0 & 10,999.4 &     10,422.5  & 315.3\% & 93.9\% \\
			KungFuMaster & 258.5 & 23,270.0 & 29,486.0 & 39,581.0 & 34,294.0 & 36,310.0 & -     & 54,124.0 & \textbf{   85,182.0 } & 290.6\% & 157.7\% \\
			MontezumaRevenge & 0.0   & 0.0   & 0.0   & 0.0   & 0.0   & \textbf{3.0} & -     & 0.0   & 0.0   & -     & - \\
			MsPacman & 307.3 & 2,311.0 & 3,210.0 & \textbf{6,518.7} & 6,283.5 & 2,722.0 & -     & 4,087.7 &      4,416.0  & 141.5\% & 108.7\% \\
			NameThisGame & 2,292.3 & 7,257.0 & 6,997.1 & 12,270.5 & 11,971.1 & 8,181.0 & 10,877.0 & 16,042.7 & \textbf{   16,535.4 } & 302.7\% & 103.6\% \\
			Phoenix & 761.4 & -     & -     & 18,992.7 & 23,092.2 & 16,028.0 & -     & 28,296.0 & \textbf{   71,752.6 } & -     & 257.8\% \\
			Pitfall & -229.4 & -     & -     & -356.5 & \textbf{0.0}   & \textbf{0.0}   & -     & \textbf{0.0}   & \textbf{0.0}   & -     & 100.0\% \\
			Pong  & -20.7 & 18.9  & \textbf{21.0} & 20.6  & \textbf{21.0} & \textbf{21.0} & 20.3  & \textbf{21.0} & \textbf{         21.0 } & 100.0\% & 100.0\% \\
			PrivateEye & 24.9  & 1,788.0 & 670.1 & 200.0 & 103.0 & \textbf{3,712.0} & -     & 173.0 &         349.7  & 50.3\% & 219.3\% \\
			Qbert & 163.9 & 10,596.0 & 14,875.0 & 16,256.5 & 19,220.3 & 15,545.0 & -     & 15,300.0 & \textbf{   24,548.8 } & 165.8\% & 161.1\% \\
			Riverraid & 1,338.5 & 8,316.0 & 12,015.3 & 14,522.3 & 21,162.6 & 9,425.0 & -     & 22,238.0 & \textbf{   24,445.0 } & 216.4\% & 110.6\% \\
			RoadRunner & 11.5  & 18,257.0 & 48,377.0 & 57,608.0 & \textbf{69,524.0} & 45,993.0 & 48,983.3 & 51,208.0 &     56,520.0  & 116.8\% & 110.4\% \\
			Robotank & 2.2   & 51.6  & 46.7  & 62.6  & 65.3  & 51.0  & -     & 42.6  & \textbf{         65.8 } & 142.9\% & 157.4\% \\
			Seaquest & 68.4  & 5,286.0 & 7,995.0 & 26,357.8 & \textbf{50,254.2} & 2,282.0 & -     & 32,101.8 &     29,278.6  & 368.5\% & 91.2\% \\
			Skiing & -17,098.1 & -     & -     & -9,996.9 & -8,857.4 & -14,763.0 & -     & -15,623.8 &     \textbf{-8,295.4}  & -     & 597.1\% \\
			Solaris & 1,236.3 & -     & -     & 4,309.0 & 2,250.8 & \textbf{6,088.0} & -     & 1,523.2 &      3,506.8  & -     & 791.4\% \\
			SpaceInvaders & 148.0 & 1,976.0 & 3,154.6 & 2,865.8 & 6,427.3 & 2,186.0 & 1,832.2 & 3,943.0 & \textbf{   21,602.0 } & 713.6\% & 565.3\% \\
			StarGunner & 664.0 & 57,997.0 & 65,188.0 & 63,302.0 & 89,238.0 & 47,133.0 & 57,686.7 & 60,835.0 & \textbf{ 129,140.0 } & 199.1\% & 213.5\% \\
			Surround & -10.0 & -     & -     & \textbf{8.9} & 4.4   & -1.0  & -     & 2.9   &             2.5  & -     & 96.9\% \\
			Tennis & -23.8 & -2.5  & 1.7   & 0.0   & 5.1   & 0.0   & -1.0  & -0.9  & \textbf{         22.3 } & 180.8\% & 201.3\% \\
			TimePilot & 3,568.0 & 5,947.0 & 7,964.0 & 9,197.0 & 11,666.0 & 7,035.0 & -     & 7,457.0 & \textbf{   12,071.0 } & 193.4\% & 218.6\% \\
			Tutankham & 11.4  & 186.7 & 190.6 & 204.6 & 211.4 & 232.0 & 194.7 & 226.2 & \textbf{       252.9 } & 134.8\% & 112.4\% \\
			UpNDown & 533.4 & 8,456.0 & 16,769.9 & 16,154.1 & 44,939.6 & 14,255.0 & -     & \textbf{46,208.0} &     25,127.4  & 151.5\% & 53.8\% \\
			Venture & 0.0   & 380.0 & 93.0  & 54.0  & \textbf{497.0} & 97.0  & -     & 62.0  &         291.0  & 312.9\% & 469.4\% \\
			VideoPinball & 16,256.9 & 42,684.0 & 70,009.0 & 282,007.3 & 98,209.5 & 322,507.0 & 275,342.8 & 603,075.0 & \textbf{ 626,794.0 } & 1135.8\% & 104.0\% \\
			WizardOfWor & 563.5 & 3,393.0 & 5,204.0 & 4,802.0 & 7,855.0 & 9,198.0 & -     & 14,780.0 & \textbf{   21,049.0 } & 441.5\% & 144.1\% \\
			YarsRevenge & 3,092.9 & -     & -     & 11,357.0 & \textbf{49,622.1} & 23,915.0 & -     & 13,178.5 &     29,231.9  & -     & 259.2\% \\
			Zaxxon & 32.5  & 4,977.0 & 10,182.0 & 10,469.0 & 12,944.0 & 6,920.0 & -     & -     & \textbf{   16,420.0 } & 161.5\% & - \\
			\hline
			\multicolumn{12}{l}{Scores of Random, Prior. DDQN, and Duel. DDQN are taken from \cite{duel}.  }\\
			\multicolumn{12}{l}{Scores of DQN, Noisy DQN, DDQN, and DSQN are taken from \cite{DQN, DDQN, noisy, DSQN}, respectively. }\\
			\multicolumn{12}{l}{Since Ape-X did not publish its score combining with DDQN, the scores of Ape-X DDQN and ASL DDQN are both obtained by our experiments. }\\
			\multicolumn{12}{l}{\textbf{Ipv1} = (ASL DDQN - Random)/(DDQN - Random): the improvement of ASL DDQN over its underlying DRL algorithm DDQN.}\\
			\multicolumn{12}{l}{\textbf{Ipv2} = (ASL DDQN - Random)/(Ape-X DDQN - Random): the improvement of ASL DDQN over Ape-X DDQN.}\\
	\end{tabular} }
	\label{tab:a57}\end{table*}


The Atari environment, composed of 57 video games, is a standardized platform commonly employed by the DRL communities. The high-dimensional visual input and the sparse rewards of Atari form a collection of challenging tasks, making it a valuable tool for the evaluation and comparison of different algorithms. To validate the efficiency of ASL, the results on 57 Atari games are compared with DQN\cite{DQN}, DDQN\cite{DDQN}, Prior. DDQN\cite{PER}, Duel. DDQN\cite{duel}, Noisy DQN\cite{noisy} and DSQN\cite{DSQN}. Essentially, ASL is a DRL training framework and should be applied in conjunction with a specific DRL algorithm. Although extensive improvement techniques have emerged since the ground-breaking work of DQN, we choose to combine the ASL framework merely with the DDQN, an algorithm that mitigates the notorious overestimation problem of DQN by Double Q-learning, and omit other tricks such as PER, dueling network, noisy network, etc. We do this with the intention of preventing these techniques from submerging the ASL. In this context, we refer to our algorithm as ASL DDQN. In addition, since the ASL is mostly inspired by the Ape-X framework, we also compare our algorithm with the Ape-X DDQN. For a fair comparison, we follow the experimental setup of previous works. We employ the same Atari preprocess procedures and model architecture of DDQN. Furthermore, we also adopt the no-ops scheme, which imposes a random number (upper-bounded by 30) of no-op actions at the beginning of each episode. The no-ops scheme could effectively prevent the agent from overfitting when training while also examine its robustness when evaluating. In addition, the episode of each Atari game is terminated at 12.5K steps and 27K steps for training and evaluation, respectively. 

Regarding the hyperparameters, since the ASL DDQN is constituted of a training framework and a DRL algorithm, the hyperparameters consist of two parts as well. Considering the prohibitively high cost of fulfilling an exhaustive search on the combined hyperparameter space, we determine the algorithm-associated hyperparameters based on the published papers and the framework-relevant hyperparameters mostly by manual coordinate descent. For instance, we did not observe a noticeable impact of the model upload frequency  and therefore made an accommodation between the model recency and transmission cost. In addition, the number of vectorized environments  is assigned a value of 128 to correspond with the maximum capacity of our CPU (AMD 3990X, 64 cores, 128 threads). The  is set to 8, the same as DDQN (a backpropagation with a mini-batch of size 32 every after 4 transitions have been collected). More information about the hyperparameter is listed in Table \ref{tab:hp}. Note that the hyperparameters across all 57 Atari games are identical.

The ASL DDQN and Ape-X DDQN are trained for 50M and 500M , respectively. During the training, the model is evaluated every 5K . Fig.\ref{s_e} from the appendix presents a comparison between the sample efficiency of ASL DDQN and Ape-X DDQN, where the horizontal axis is the total samples consumed during training. Fig.\ref{t_e} from the appendix compares their training time efficiency, by a measure of wall-clock time. After training, the best models of the 57 Atari games are evaluated for 100 episodes, and the averaged raw scores are compared with other DRL baselines previously published, as listed in Table \ref{tab:a57}. 

\subsubsection{Sample Efficiency}
It can be observed from Fig.\ref{s_e} that the ASL framework substantially improved the sample efficiency over the Ape-X framework, reaching the equivalent or remarkably better performance with considerably less training data. We contend that the improvement originates from the TFM and the VEM. The relative speed of the Actor and Learner is reasonably coordinated by the TFM in accordance with the , preventing the Actor from generating redundant data. Meanwhile, the VEM overcomes the limitation of canonical -greedy exploration and reaches a more rational compromise between exploration and exploitation.

\subsubsection{Training time efficiency}
Fig.\ref{t_e} presents that ASL outperforms Ape-X in terms of training time efficiency, capable of achieving superior performance under an identical period of time on most of the Atari games. We posit that the superiority of ASL is attributed to the VDC mode and the TFM. As detailed previously, the VDC mode harnesses the parallel processing capabilities of GPU to the fullest extent and simplifies the data preservation, which elevates the computational efficiency and shortens the training time consequently. Furthermore, the prudent data recency upheld by the TFM significantly fosters the model optimization, resulting in comparable performance with fewer training instances and thus decreasing overall required training time.

\subsubsection{Stability}
The ASL framework exhibits better stability than Ape-X, which is supported by the curves from \textit{Freeway}, \textit{Frostbite}, and \textit{Zaxxon}, where the Ape-X DDQN failed to learn. We conjecture such failures are mostly incurred by the incoordination between the Actors and Learner in Ape-X.

\subsubsection{Final performance}
Table \ref{tab:a57} reveals that the ASL DDQN outstrips other DRL baselines over 32 Atari games. Besides, the final performance gain achieved by the ASL DDQN over its underlying algorithm DDQN is a remarkable 508.2\% (averaged over \textit{Ipv1} across 57 Atari games). Furthermore, compared to the framework counterpart Ape-X, the ASL demonstrates a substantial improvement of 234.9\% (averaged over \textit{Ipv2} across 57 Atari games). These findings provide strong evidence of the superiority of our ASL training framework.


\subsection{Task2Task Generalization}
\subsubsection{Problem formulation}

The experiments of this section are conducted on the Sparrow simulator, wherein the LPP problem is formulated as a Markov Decision Process (MDP) to enable the employment of DRL. At timestep , the agent observes the state  from the environment, takes the action  according to its policy , receives the reward , and subsequently transits to the next state , with the objective of maximizing the expected sum of discounted rewards , where  is the discount factor.

\noindent \textbf{State}: The state of the agent is a vector of length 32, containing the pose of the robot (,  and  as shown in Fig.\ref{s_r}(a)), the linear and angular velocity of the robot ( and ), and 27 scanning results of the LiDAR mounted on the robot. The state variables will be normalized and represented in a relative fashion before being fed to the agent. The normalized relative state representation could simplify the training: we could train the agent in a fixed manner (start from the lower left corner, and end at the upper right corner), and the trained agent is capable of handling any start-end scenarios as long as their distance is within , as illustrated in Fig.\ref{s_r}(b).

\noindent \textbf{Action}: We employ 5 discrete actions to control the target velocity  of the robot, which are:

\begin{itemize}
	\item{\textit{Turn left}: [0.36 cm/s, 1 rad/s]}
	\item{\textit{Go straight and turn left}:[18 cm/s, 1 rad/s]}
	\item{\textit{Go straight}: [18 cm/s, 0 rad/s]}
	\item{\textit{Go straight and turn right}: [18 cm/s, -1 rad/s]}
	\item{\textit{Turn right}: [0.36 cm/s, -1 rad/s]}
\end{itemize}

\noindent \textbf{Reward}: If the robot collides with an obstacle or reaches the end point, the episode is terminated, and the agent is rewarded with -10 or 75, respectively. Otherwise, the agent receives a reward according to:

\noindent where  and   are negatively correlated to  and  (refer to Fig.\ref{s_r}(a)), with a maximum value of ;  if the linear velocity of the robot exceeds half of its maximum linear velocity, otherwise 0;  is negatively correlated to the absolute value of ;  if the closest distance between the robot and an obstacle is smaller than 30 cm, otherwise 0. The reward function is devised with the intention of driving the robot to the end point expeditiously without colliding with obstacles. For more details, please refer to our code\footnotemark[1].

\begin{figure}[t]
	\centering
	\subfloat[]{\includegraphics[width=0.23\textwidth]{img/state_train.pdf}
		\label{s_train}}
	\hfil
	\subfloat[]{\includegraphics[width=0.23\textwidth]{img/state_eval.pdf}
		\label{s_eval}}
	\caption{Normalized relative state representation of Sparrow. Here,  is the training map size, and  is the maximum local planning distance.}
	\label{s_r}
\end{figure}

\subsubsection{Experiment Setup on Sparrow}
In this section, we seek to investigate the extent to which the vectorized diversity of Sparrow can contribute to the Task2Task generalization of ASL. To achieve this goal, two experiment setups have been designed. The first setup, denoted as Grey, employs identical simulation parameters and training maps (Map016) across all vectorized environments. In contrast, the other setup, referred to as Color, randomizes the control interval, control delay, velocity range, kinematic parameter , and the magnitude of sensor noise across the vectorized environments that connect the ASL and the Sparrow. Note that the initial values of these simulation parameters are roughly measured from our real-world robot as shown in Fig.\ref{S&R}(b). The randomization is realized by uniformly sampling from a moderate interval around the initial values. Additionally, a collection of diverse maps are leveraged for training, as the Map0 to Map15 shown in Fig.\ref{maps}. Meanwhile, to ensure the diversified dynamics cover all the training maps, the randomization is repeated at the beginning of each episode. 

It is worth noting that, both for Grey and Color, the initial pose of the robot and the obstacles in Map0 are randomly generated to evade overfitting. We harness 4 fully connected layers of shape [32, 256, 128, 5] to map the state variable to the discrete actions. Other hyperparameters are listed in Table \ref{tab:hp}. The Color and Grey were trained until converged under three different random seeds, approximately equating to one hour of wall-clock time per seed. During training, we evaluated the models of Color and Grey on their respective training maps and the common test maps (Map16 to Map31) every 5K , and recorded the target area arrival rate as shown in Fig.\ref{arrival_rate}.

\subsubsection{Analysis}
Fig.\ref{arrival_rate} demonstrates that the overfitting, analogous to that in Supervised Learning, also occurs in DRL. As mirrored by the Grey, the homogeneous environments could result the DRL agent in overfitting and impair its generalization capability over similar tasks. Thanks to the diverse environments, the Color is able to achieve an arrival rate over 80\% on the zero-shot generalization tests, with a tolerable loss of converge speed and final performance on training maps. These results validly corroborate the beneficial impact of Sparrow's vectorized diversity on Task2Task generalization.

\begin{figure}
	\centering
	\includegraphics[width=0.48\textwidth]{img/maps.jpg}
	\caption{Training and Test maps for Task2Task generalization assessment.}
	\label{maps}
\end{figure}

\begin{figure}
	\centering
	\subfloat{\includegraphics[width=0.23\textwidth]{img/train_rate.pdf}
		\label{train_rate}}
	\hfil
	\subfloat{\includegraphics[width=0.23\textwidth]{img/test_rate.pdf}
		\label{test_rate}}
	\caption{Target area arrival rate on training maps (left) and test maps (right). The translucent curve represents the maximal and minimal arrival rate across the three random seeds, and the solid curve corresponds to the mean value.}
	\label{arrival_rate}
\end{figure}

\subsection{Sim2Real Generalization}

To examine the Sim2Real Generalization capability of the agent trained by Color, 36 real-world scenarios of 6 types, as shown in Fig.\ref{real_all}, have been built: a) Blocky; b) Fence-shaped; c) Simply mixed; d) Intricately mixed; e) Random start point; f) Random end point. Utilizing the best model achieved by the Color, we have employed our robot (see Fig.\ref{S&R}(b)) to undertake the LPP task in the real world. Please refer to our website\footnotemark[2] to watch the real-world experiment. Surprisingly, the Color demonstrates superior proficiency in Sim2Real generalization, succeeding in navigating to the end point in 33 out of 36 total scenarios without any additional tuning or adaption in the real world. The three failed cases have been designated in red within Fig.\ref{real_all}, and it can be observed from the experiment video that these failures are primarily induced by the robot's attempts to forcefully navigate through obstacles. We believe that a more appropriate reward function or judicious selection of the diverse training maps could further mitigate such issues, which will be the focus of our future works.

\begin{figure}
	\centering
	\includegraphics[width=0.48\textwidth]{img/real36.jpg}
	\caption{36 real-world LPP scenarios for Sim2Real generalization assessment.}
	\label{real_all}
\end{figure}

\section{Conclusion}
\label{section5}

To improve the efficiency of the existing DRL algorithm, a partially decoupled training framework ASL is proposed. It is noteworthy that the ASL is specifically tailored to accommodate off-policy DRL algorithms that utilize experience replay. DRL algorithms that satisfy this requirement are well-suited for integration with ASL to enjoy a further promotion. In addition, a Sparrow simulator that supports vectorized diversity is developed to enhance the generalization capability of the ASL framework. The Sparrow and the ASL then are seamlessly integrated by their interdependent vectorized environments, resulting in a DRL solution to the LPP problem, namely Color. Impressively, with one hour of training, the Color agent accomplished 33 out of 36 real-world LPP tasks, which we believe have reached a new milestone in the real-world application of DRL.



{\appendix
	
\begin{figure*}
	\centering
	\includegraphics[height=0.9\textheight]{img/s_e.pdf}
	\caption{Sample efficiency comparison between ASL DDQN (red) and Ape-X DDQN (dark). The translucent curve represents the raw training curve, and the solid curve corresponds to the exponentially smoothed training curve with a smoothing factor of 0.95.}
	\label{s_e}
\end{figure*}
}	

\begin{figure*}
	\centering
	\includegraphics[height=0.9\textheight]{img/t_e.pdf}
	\caption{Training time efficiency comparison between ASL DDQN (red) and Ape-X DDQN (dark). The translucent curve represents the raw training curve, and the solid curve corresponds to the exponentially smoothed training curve with a smoothing factor of 0.95.}
	\label{t_e}
\end{figure*}


\begin{table}[htbp]
	\centering
	\caption{Hyperparameters}
	\begin{tabular}{lll}
		\hline
		\textbf{Framework-associated} & \textbf{Atari} & \textbf{Sparrow} \\
		\hline
		 & 128 & 16 \\
		 & 8 & 256 \\
		Linear Decay Steps of  & 500K  & 500K  \\
		 & 128  4 & 16  3 \\
		 & 0.01 & 0.01 \\
		 & 0.8 & 0.8 \\
		 & 50  & 50  \\
		\hline
		\hline
		\textbf{Algorithm-associated} & \textbf{Atari} & \textbf{Sparrow} \\
		\hline
		Learning Start Steps  & 150K transitions & 30K transitions\\
		Replay Buffer Size & 1M transitions & 1M transitions \\
		 & 0.99 & 0.98 \\
		Learning rate &  &  \\
		Target Net Update Frequency & 2k  & 200  \\
		Mini-batch size & 32 & 256 \\
		Optimizer & Adam  & Adam \\
		\hline
	\end{tabular}\label{tab:hp}\end{table}




\newpage

\bibliographystyle{IEEEtran}
\bibliography{myreference}
	
\end{document}
