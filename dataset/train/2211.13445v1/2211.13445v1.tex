\documentclass{article}

\usepackage[final, nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{multirow}
\usepackage{bbm}
\usepackage{color, colortbl}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{layout}
\usepackage{enumitem}
\usepackage{xcolor}
\def\*#1{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand\SL[1]{\textcolor{blue}{[Sharon: #1]}}
\newcommand{\jx}[1]{\textcolor{purple}{#1}} \newcommand{\yf}[1]{\textcolor{brown}{[Yifei: #1]}}
\newcommand{\jc}[1]{\textcolor{magenta}{[Jack: #1]}}

\usepackage[square,numbers]{natbib}


\setlength\parindent{0pt} \theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conjecture}{Conjecture}[]
\newtheorem{example}{Example}[]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\def\Eg{\emph{E.g}.}
\def\ie{\emph{i.e}., }
\def\eg{\emph{e.g}., }
\def\etal{\emph{et al.}}
\def\cf{\emph{c.f}. }
\def\etc{\emph{etc}}
\title{Delving into Out-of-Distribution Detection with\\ Vision-Language Representations} 



\author{Yifei Ming$^1\quad$ Ziyang Cai$^1\quad$ Jiuxiang Gu$^2\quad$  Yiyou Sun$^1\quad$ Wei Li$^3\quad$ Yixuan Li$^1\quad$  \\
  $^1$Department of Computer Sciences, University of Wisconsin-Madison\\ $\quad^2$Adobe $\quad^3$Google Research\\
  \texttt{\{alvinming,ziyangc,sunyiyou,sharonli\}@cs.wisc.edu}\\
 $\quad$ \texttt{jigu@adobe.com} $\quad$ \texttt{mweili@google.com} 
}


\begin{document}


\maketitle

\begin{abstract}
Recognizing out-of-distribution (OOD) samples is critical for machine learning systems deployed in the open world. The vast majority of OOD detection methods are driven by a single modality (\textit{e.g.}, either vision or language), leaving the rich information in multi-modal representations untapped.
Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of OOD detection from a single-modal to a multi-modal regime. Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective zero-shot OOD detection method based on aligning visual features with textual concepts.  We contribute in-depth analysis and theoretical insights to understand the effectiveness of MCM.
Extensive experiments demonstrate that MCM achieves superior performance on a wide variety of real-world tasks. MCM with vision-language features outperforms a common baseline with pure visual features on a hard OOD task with semantically similar classes by 13.1\% (AUROC). Code is available at \url{https://github.com/deeplearning-wisc/MCM}. 
\end{abstract}


\section{Introduction}

Out-of-distribution (OOD) detection is critical for deploying machine learning models in the wild, where samples from novel classes can naturally emerge and should be flagged for caution. Despite increasing attention, the vast majority of OOD detection methods are driven by single-modal learning~\cite{hendrycks-etal-2020-pretrained,hsu2020generalized,jin2022towards,shen2021enhancing,xu-etal-2021-unsupervised,zhan2021out,zheng2020out,zhou2021contrastive}.
For example, labels are typically encoded as one-hot vectors in image classification, 
leaving the semantic information encapsulated in texts largely unexploited. 
OOD detection relying on pure visual information can inherit the limitations, \emph{e.g.}, when an OOD input may be visually similar to  in-distribution (ID) data yet semantically different from any ID class.  





In this paper, we delve into a new landscape for OOD detection, departing from the classic {single-modal} toward a \emph{multi-modal} regime. While the motivation is appealing, a core challenge remains: \textit{how to effectively utilize joint vision-language features for OOD detection?} In the visual domain, existing methods typically require good feature representations~\cite{2021ssd,tack2020csi}, and a distance metric under which OOD data points are relatively far away from the in-distribution (ID) data~\cite{lee2018simple,sun2022knn}. These approaches, however, do not directly translate into the multi-modal regime. On the representation learning side, recent vision-language pre-training schemes such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling} have emerged as promising alternatives for visual representation learning. The main idea is to align an image with its corresponding textual description in the feature space. While the resulting representations are powerful, OOD detection based on such aligned multi-modal features is still in its infancy. 


We bridge the gap by exploring a distance-based OOD detection approach, leveraging the joint vision-language representations. Our method capitalizes on the compatibility between visual features and textual features. By defining the textual features as the ``\emph{concept prototypes}'' for each ID class, we characterize OOD uncertainty by the distance from the visual feature to the closest ID prototype. That is, images closer to one of the textual embeddings of ID classes are more likely to be ID and vice versa. By a proper scaling of the distance, our proposed Maximum Concept Matching (\textbf{MCM}) score achieves strong ID-OOD separability (see Figure~\ref{fig:teaser}).
MCM
stands in contrast with the previous distance-based approaches, such as Mahalanobis~\cite{lee2018simple}, which defines class prototypes based on pure visual embeddings. Indeed, we show later in Section~\ref{sec:closer} that MCM (with multi-modal vision-language features) is far more competitive than Mahalanobis (with single-modal visual features). Moreover, while prior works of CLIP-based OOD detection~\cite{esmaeilpour2022zero,fort2021exploring} rely on a set of candidate OOD labels, MCM is OOD-agnostic and alleviates the need for any prior information about test inputs.

\begin{figure*}[t]
  \centering
    \includegraphics[width=1.0\linewidth]{figures/Arch_figure_v5.pdf}
    
\caption{
Overview of the proposed zero-shot OOD detection framework.
The ID classification task is defined by a set of class labels $\mathcal{Y}_\text{in}$. The goal of OOD detection is to detect samples that do not belong to $\mathcal{Y}_\text{in}$. We view the textual embeddings of ID classes (wrapped by text templates) as concept prototypes. The OOD uncertainty of an input image can be characterized by the distance from visual features to the closest ID prototype. By properly scaling the distance, the MCM score achieves strong ID-OOD separability. See Section~\ref{sec:method} for details.}
\vspace{-3mm}
\label{fig:teaser}
\end{figure*}

Our work also advances the field by showcasing the promise of zero-shot OOD detection, which offers strong performance and generality without training on the ID samples. In particular, classic OOD detection methods often require training from scratch~\cite{chen2021robustifying,hendrycks2018deep} or fine-tuning~\cite{fort2021exploring,huang2021mos} on a given ID dataset. In this setting, a classifier and its companion OOD detector are good at only one task.
Every new task (ID dataset) requires additional training and brings additional computation and storage costs.
In contrast, we show for the first time that: \textbf{(1)} MCM
achieves
superior performance across a wide variety of real-world tasks---with just \emph{one single pre-trained model}. This is encouraging given that there is no training or any OOD information involved. \textbf{(2)} On the challenging ImageNet-1k benchmark, MCM's zero-shot OOD detection performance favorably matches and even outperforms strong task-specific baselines fine-tuned on BiT~\cite{huang2021mos} and ViT models~\cite{fort2021exploring}. \textbf{(3)} MCM remains robust against hard OOD inputs, including both semantically hard OODs~\cite{winkens2020contrastive} and spurious OODs~\cite{ming2022spurious}.

We summarize our main contributions as follows:
\begin{enumerate}
\item We propose MCM, a simple yet effective OOD detection method based on aligned vision-language features.
MCM offers several compelling advantages over other OOD detection methods:
 generalizable (one model supports many tasks), OOD-agnostic (no information required from OOD data), training-free (no downstream fine-tuning required), and scalable to large real-world tasks.
\item We conduct extensive experiments and show that MCM achieves superior performance on a wide range of real-world tasks. On ImageNet-1k, MCM achieves an average AUROC of 91.49\%, outperforming methods that require training. Moreover, we show that MCM remains competitive under challenging hard OOD evaluation tasks.
\item We provide in-depth empirical and theoretical analysis, providing insights to understand the effectiveness of MCM. We hope that this work will serve as a springboard for
future works on OOD detection with multi-modal features.  
\end{enumerate}

\section{Preliminaries}
\paragraph{Contrastive vision-language pre-training.}
Compared to visual representation learning models such as ViT~\cite{dosovitskiy2021an}, vision-language representation learning demonstrates superior performance on image classification tasks. For instance, CLIP~\cite{radford2021learning} adopts a self-supervised contrastive objective (\ie InfoNCE loss~\cite{van2018representation}) to align an image with its corresponding textual description in the feature space. 
Specifically, CLIP adopts a simple dual-stream architecture with one text encoder $\mathcal{T}: t \rightarrow \mathbb{R}^d$ (\eg Transformer~\cite{vaswani2017attention}) and one image encoder $\mathcal{I}: \*x \rightarrow \mathbb{R}^d$ (\eg ViT~\cite{dosovitskiy2021an}). After pre-training on a dataset of 400 million text-image pairs, the joint vision-language embeddings of CLIP well associate objects in different modalities. Despite the promise, existing CLIP-like models perform zero-shot classification in a \emph{closed-world} setting. That is, it will match an input into a fixed set of categories, even if it is irrelevant (\eg  a tree being predicted as a bird in Figure~\ref{fig:teaser}). This motivates our work to leverage the multi-modal representation for OOD detection, which is largely unexplored.

\paragraph{Zero-shot OOD detection.} 
Given a pre-trained model, a classification task of interest is defined by a set of
class labels/names $\mathcal{Y}_\text{in}$, which we refer to as the known (ID) classes. Here ID classes are defined \emph{w.r.t.} the classification task of interest, instead of the classes used in pre-training.
Accordingly, OOD is defined \emph{w.r.t.} the ID classes, not the data distribution during pre-training. The goal of OOD detection
is to (1) detect samples that do not belong to any of the
known classes; (2) otherwise, assign test samples to one of the known
classes. Therefore, the OOD detector can be viewed as a ``safeguard'' for the classification model. Formally, we denote the OOD detector as a binary function: $G(\*x;\mathcal{Y}_\text{in}, \mathcal{T}, \mathcal{I}): \mathcal{X} \rightarrow \{\text{in}, \text{out}\}$,
where $\*x \in \mathcal{X}$ denotes a test image. 
Our method is based on only the names of the
given classes in $\mathcal{Y}_\text{in}$, and a pre-trained model. Different from standard supervised learning, there is no training on the ID samples involved, hence zero-shot.


\section{OOD Detection via Concept Matching}
\label{sec:method}
We illustrate our approach in Figure~\ref{fig:teaser}, which derives the OOD detector $G(\cdot)$ based on \emph{concept matching}. For a given task with label set $\mathcal{Y}_\text{in}=\{y_1, y_2,...,y_K\}$, we can construct a collection of concept vectors $\mathcal{T}(t_i), i\in \{1,2,...,K\}$, where $t_i$ is the text prompt
``\texttt{this is a photo of a $\langle y_i \rangle$}''
for a label $y_i$.
The concept vectors are represented by the embeddings of the text prompts.

For any test input image $\*x'$, we can calculate the label-wise matching score based on the cosine similarity between the image feature $\mathcal{I}(\*x')$ and the concept vector $\mathcal{T}(t_i)$: $s_i(\*x') = \frac{\mathcal{I}(\*x') \cdot \mathcal{T}(t_i)}{\lVert \mathcal{I}(\*x')\rVert \cdot \lVert \mathcal{T}(t_i) \rVert}$.
Formally,
we define the maximum concept matching (\textbf{MCM}) score as:
\begin{equation}
    S_{\text{MCM}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) = \max_i \frac{e^{s_i(\*x')/\tau}}{\sum_{j=1}^K e^{s_j(\*x')/\tau}},
\end{equation}
where $\tau$ is the temperature. For ID data, it will be matched to one of the concept vectors (textual prototypes) with a high score; and vice versa.
Formally, our OOD detection function can be formulated as:
 \begin{align*}
\label{eq:threshold}
	G(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) =\begin{cases} 
      1 & S_{\text{MCM}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I})\ge \lambda \\
      0 & S_{\text{MCM}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) < \lambda 
  \end{cases},
\end{align*}
where by convention $1$ represents the positive class (ID) and $0$ indicates OOD. $\lambda$ is chosen so that a high fraction of ID data (\eg 95\%) is above the threshold. For samples that are classified as ID, one can obtain the class prediction based on the closest concept:
$\hat y = \argmax_{i\in[K]} s_i$. 

\textbf{Remark}: (1) Our work differs from (and is complementary to) CLIP by focusing on OOD detection rather than (closed-world) zero-shot classification. We show new theoretical insights that softmax scaling plays a unique role in zero-shot OOD detection---improving the separability between ID and OOD data. This role has not been studied rigorously for zero-shot OOD detection. Readers familiar with CLIP may notice that MCM can be used for zero-shot classification in the closed world. This also makes MCM practically convenient for dual goals: detect OOD samples and assign ID data to one of the known classes. (2) Our method in principle is not limited to CLIP; it can be generally applicable for contrastive vision-language pre-training models that promote multi-modal feature alignment. 


\paragraph{New insights on softmax scaling for zero-shot OOD detection.} 
We provide theoretical justifications that softmax scaling improves the separability between ID and OOD data for CLIP-based OOD detection, which is \emph{contrary} to models trained with cross-entropy (CE) loss.
In particular, CLIP-like models are trained with a multi-modal contrastive loss, which maximizes the cosine similarity between an image and its textual description in the feature space. The resulting cosine similarity scores display strong \emph{uniformity}\footnote{\noindent This can be explained both theoretically~\cite{wang2020understanding} and empirically ~\cite{wang2021understanding}. It has been shown that self-supervised contrastive learning with a smaller temperature (\eg initialized as 0.07 for CLIP) promotes uniform distribution for $L_2$-normalized features. Moreover, as CLIP features lie on a high-dimensional space (512 for CLIP-B/16 and 768 for CLIP-L/14), uniformly distributed points in a high-dimensional sphere tend to be equidistant to each other~\cite{vershynin2018high}. Therefore, for OOD inputs, we observe approximately uniform cosine similarity with concept vectors.} across labels, as evidenced in Figure~\ref{fig:sep}~(right). Compared to OOD inputs, the {gap} between the maximum cosine similarity and the average is larger for ID inputs. However, the gap can be small when the number of ID classes increases where ID samples occur with lower highest cosine similarity. As a result,  the highest cosine similarity for ID samples and OOD samples can be highly close (\emph{c.f.} Figure~\ref{fig:sep}~(left)). 

\begin{wrapfigure}[16]{r}{0.52\textwidth}
  \begin{center}
        \vspace{-0.4cm}\includegraphics[width=0.52\textwidth]{figures/cosine_v2.pdf}
  \end{center}
    \vspace{-0.2cm}
  \caption{\small Left: Maximum cosine similarity for ID and OOD inputs. There exists overlapping regions (shown in yellow); Right: Cosine similarities between OOD inputs and ID concept vectors. For OOD inputs, the cosine similarities display uniformity.}
  \label{fig:sep}
\end{wrapfigure}
Motivated by these observations, MCM employs softmax as a post hoc mechanism to \textbf{magnify} the difference. This is \emph{fundamentally different from the softmax score derived from a model trained with cross-entropy loss}, which inherently maximizes the posterior $p(y|\*x)$ for the ground-truth label, and minimizes the probability for other labels.  Unlike CLIP-like models, logit scores displaying uniformity would be heavily penalized by the CE loss. As a result, the logit score corresponding to the ground-truth label can already be significantly higher than other labels. Applying softmax on the logit scores can exacerbate overconfident predictions, and reduce the separability between ID and OOD data~\cite{liang2018enhancing}. Indeed, for a model trained with cross-entropy loss, a logit-based score such as Energy~\cite{liu2020energy} is shown to be much more effective than the softmax score. 

Interestingly, for CLIP-like models, the trend is the opposite---applying softmax helps sharpen the uniform-like inner product scores, and increases the separability between ID and OOD data. To help readers better understand the insights, 
we first formalize our observations that OOD inputs trigger \emph{similar cosine similarities} across ID concepts (Figure~\ref{fig:sep}, right) as the following assumption:

\begin{assumption}
\label{thm:assumption} Let $z := \mathbbm{1}\{y \in \mathcal{Y}_\text{in}\}$. $Q_{\*x}$ denotes the out-of-distribution $\mathbb{P}_{\*x \mid z = 0}$ (marginal distribution of $\*x$ conditioned on $z = 0$). Assume $\exists\,\delta>0$ such that
$$
Q_{\*x}\left({1\over K-1}\sum_{i \neq \hat{y}}\left[s_{\hat{y}_{2}}(\*x)-s_{i}(\*x)\right]< \delta\right)=1,
$$
where $\hat y := \text{argmax}_{i\in[K]} s_i(\*x)$ and $\hat y_2 := \text{argmax}_{i\neq \hat y, i\in[K]} s_i(\*x)$ denote the indices of the largest and second largest cosine similarities for an OOD input $\*x$. 
\end{assumption}

Now we provide formal guarantees that using softmax can provably reduce  the false positive rate (FPR) compared to that without softmax.

\begin{theorem}
\label{thm:main}
Given a task with ID label set $\mathcal{Y}_\text{in}=\{y_1, y_2,...,y_K\}$ and a pre-trained CLIP-like model $(\mathcal{T}, \mathcal{I})$. If $Q_{\*x}$ satisfies Assumption~\ref{thm:assumption}, then there exists a constant $T = { \lambda(K - 1)\left(\lambda^{\text{wo}} + \delta - s_{\hat{y}_{2}}\right) \over K\lambda-1} $ such that for any temperature $\tau>T$,
we have
$$
\operatorname{FPR}(\tau, \lambda) \leq \operatorname{FPR}^{\text{wo}}(\lambda^{\text{wo}} ),
$$
where $\operatorname{FPR}(\tau, \lambda)$ is the false positive rate based on softmax scaling \emph{with} temperature $\tau$ and detection threshold $\lambda$; $ \operatorname{FPR}^{\text{wo}}(\lambda^{\text{wo}} )$ is the false positive rate \emph{without} softmax scaling based on threshold $\lambda^{\text{wo}}$. This suggests that applying softmax scaling with a moderate temperature results in superior OOD detection performance compared to that without softmax scaling. 
The proof is in Appendix~\ref{sec:proof}. Later in Section~\ref{sec:closer}, we empirically verify on a real-world ImageNet dataset that our bound can indeed be satisfied in CLIP where the thresholds are chosen at $95\%$ true positive rate. 
\end{theorem}



\paragraph{What MCM offers:} Beyond theoretical insights, we would like to highlight several compelling advantages of our zero-shot OOD detection approach, owing to the strong pre-trained CLIP model:
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]
    \item \textbf{Generalizable to many tasks}: Traditional OOD detection methods are based on a task-specific model. As a result, the OOD detector is not suitable for a realistic online scenario where the task changes from one to another. In contrast, we will show in Section~\ref{sec:exp} that MCM can perform a wide variety of OOD detection tasks, with just one single model. For a new task, only the names of the task’s visual concepts $\mathcal{Y}_\text{in}$ are required. 
    \item \textbf{OOD-agnostic}:  Our method does not rely on any OOD  information, and thus suits many real-world scenarios where one cannot anticipate what the unknowns would be ahead of time. This also mitigates the shortcoming of a recent  approach~\cite{fort2021exploring}, which assumes that a set of unseen labels are given as some weak information about OOD data.  
    \item \textbf{Training-free}: MCM enables OOD detection in a zero-shot fashion. This stands in contrast to the vast majority of OOD detection literature, which often requires training from scratch or fine-tuning to achieve competitive performance. 
    \item \textbf{Scalable}: The contrastive vision-language pre-training paradigm makes MCM scalable to a large number of class labels and realistic high-resolution images.
\end{itemize}

We now proceed to the experimental results, demonstrating these advantages on real-world tasks. 


\section{A Comprehensive Analysis of MCM}
\label{sec:exp}
\subsection{Datasets and Implementation Details}
\paragraph{Datasets.}
Most previous works on OOD detection only focus on small-scale datasets with blurry images such as CIFAR~\cite{krizhevsky2009learning} and TinyImageNet~\cite{le2015tiny}. With
pre-trained
models such as CLIP,
OOD detection can be  extended to more realistic and complex datasets.
In this work, we scale up evaluations in terms of (1) image resolution, (2) dataset variety, and (3) number of classes.  We consider the following ID datasets: \textsc{Cub-200}~\cite{WahCUB_200_2011}, \textsc{Stanford-Cars}~\cite{KrauseStarkDengFei-Fei_3DRR2013}, \textsc{Food-101}~\cite{bossard14},  \textsc{Oxford-Pet}~\cite{parkhi12a} and variants of \textsc{ImageNet}~\cite{deng2009imagenet}. For OOD test datasets, we use the same ones in~\cite{huang2021mos}, including subsets of iNaturalist~\cite{van2018inaturalist}, \textsc{Sun}~\cite{xiao2010sun}, \textsc{Places}~\cite{zhou2017places}, and \textsc{Texture}~\cite{cimpoi2014describing}. For each OOD dataset, the categories are not overlapping with the ID dataset.
We also use subsets of ImageNet-1k for
fine-grained
analysis. For example, we construct ImageNet-10 that mimics the class distribution of CIFAR-10 but with high-resolution images. For hard OOD evaluation, we curate ImageNet-20, which consists of 20 classes semantically similar  to ImageNet-10 (\eg dog (ID) vs. wolf (OOD)). 

\paragraph{Model.} In our experiments, we adopt CLIP~\cite{radford2021learning} as the target pre-trained model, which is one of the most popular and publicly available vision-language models. Note that our method is not limited to CLIP; it can generally be applicable for contrastive vision-language pre-training models that promote multi-modal feature alignment.
Specifically,
we mainly use CLIP-B/16, which consists of a ViT-B/16 Transformer as the image encoder and  a masked self-attention Transformer~\cite{vaswani2017attention} as the text encoder. To indicate the input patch size in ViT models, we append ``/x'' to model names. We prepend -B, -L to indicate \texttt{Base} and \texttt{Large} versions of the corresponding architecture. For instance, ViT-B/16 implies the \texttt{Base} variant with an input patch resolution of $16 \times 16$. We also use CLIP-L/14 which is based on ViT-L/14 as a representative of large models. Unless specified otherwise, the temperature $\tau$ is 1 for all experiments. Details of the datasets, experimental setup, and hyperparameters are provided in Appendix~\ref{sec:exp_detail}.

\paragraph{Metrics.} For evaluation,
we use the following metrics: (1) the false positive rate (\text{FPR}95) of OOD samples when the true positive rate of in-distribution samples is at 95\%, (2) the area under the receiver operating characteristic curve (AUROC), and (3) ID classification accuracy (ID ACC). 

\subsection{Main Results}

\paragraph{MCM supports a diverse collection of tasks while being zero-shot.} 
\begin{table}[tb]
\caption{\small Zero-shot OOD detection with MCM score based on CLIP-B/16 with various ID datasets.}
\label{tab:beyond}
\centering
\resizebox{1\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{ID Dataset}} & \multicolumn{8}{c}{\textbf{OOD Dataset}}                        & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Average} }} \\
                        & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Texture} & \multicolumn{2}{c}{} \\            
             \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11} 
                        & \textbf{FPR95$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR95$\downarrow$}           & \textbf{AUROC$\uparrow$}         & \textbf{FPR95$\downarrow$}          & \textbf{AUROC$\uparrow$}        & \textbf{FPR95$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR95$\downarrow$}          & \textbf{AUROC$\uparrow$}     \\    
                        \midrule
                     \textbf{CUB-200}~\cite{WahCUB_200_2011} 
                     & 9.83&	98.24&	4.93&	99.10&	6.65&	98.57&	6.97&	98.75&	7.09&	98.66\\
\textbf{Stanford-Cars}~\cite{KrauseStarkDengFei-Fei_3DRR2013} 
&0.05&	99.77&	0.02&	99.95&	0.24&	99.89&	0.02&	99.96&	0.08&	99.89\\
\textbf{Food-101}~\cite{bossard14} 
& 0.64& 	99.78& 	0.90& 	99.75& 	1.86& 	99.58& 	4.04& 	98.62& 	1.86& 	99.43\\
\textbf{Oxford-Pet}~\cite{parkhi12a}
&2.85&	99.38&	1.06&	99.73&	2.11&	99.56&	0.80&	99.81&	1.70&	99.62\\
\textbf{ImageNet-10}
&0.12&	99.80&	0.29&	99.79&	0.88&	99.62&	0.04&	99.90&	0.33&	99.78\\
\textbf{ImageNet-20}
&1.02&	99.66&	2.55&	99.50&	4.40&	99.11&	2.43&	99.03&	2.60&	99.32\\
\textbf{ImageNet-100}&18.13	&96.77&	36.45&	94.54&	34.52&	94.36&	41.22	&92.25&	32.58&	94.48\\
\bottomrule
\end{tabular}
}
\end{table}
We first show that zero-shot OOD detection with MCM is effective across a wide variety of tasks---with just \emph{one single pre-trained model}. To showcase the versatility of MCM, we consider the seven ID datasets here. 
To the best of our knowledge, this is among the first attempts to showcase the efficacy under an expansive and diverse collection of ID datasets. The zero-shot OOD detection performance is summarized in Table~\ref{tab:beyond}. A salient observation is that MCM can achieve superior detection performance on many tasks. For example, using \textsc{Stanford-Cars} as ID, MCM yields an average FPR95 of \textbf{0.08}\%.
Considering that there are no training samples or OOD information involved, these results are very encouraging.


It can be also seen from Table~\ref{tab:beyond} that MCM is promising, especially when the number of samples per ID class is limited in the training set. For example, there are only around 40 samples per class for Stanford-Cars, 100 for Oxford-Pet, and 30 for CUB-200. The sample scarcity makes OOD detection methods that rely on fine-tuning difficult. For example, after fine-tuning on Food-101, while the ID accuracy is increased from $86.3\%$ to $92.5\%$ $\uparrow$, OOD detection based on MSP is on par with MCM ($99.5\%$ vs. $99.4\%$ in AUROC). 



\begin{table}[t]
\caption{\small OOD detection performance for ImageNet-1k~\cite{deng2009imagenet} as ID.} \label{tab:1k}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{8}{c}{\textbf{OOD Dataset}}                        & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Average} }} \\
                        & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Texture} & \multicolumn{2}{c}{}             \\
                        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
                        & \textbf{FPR95$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR95$\downarrow$}           & \textbf{AUROC$\uparrow$}         & \textbf{FPR95$\downarrow$}          & \textbf{AUROC$\uparrow$}        & \textbf{FPR95$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR95$\downarrow$}          & \textbf{AUROC$\uparrow$}        \\  
                        \midrule
 &\multicolumn{9}{c}{\textbf{Requires training (or w. fine-tuning)}}          \\
MOS~\cite{huang2021mos} (BiT) & 9.28 & 98.15 & 40.63 & 92.01 & 49.54 & 89.06 & 60.43 & 81.23 & 39.97 & 90.11\\
 Fort et al.~\cite{fort2021exploring} (ViT-B) & 15.07&	96.64&	54.12&	86.37&	57.99&	85.24&	53.32&	84.77&	45.12&	88.25\\
Fort et al.~\cite{fort2021exploring} (ViT-L)&15.74&	96.51	&52.34&	87.32&	55.14&	86.48&	51.38&	85.54&	43.65&	88.96 \\
Energy~\cite{liu2020energy} (CLIP-B) & 21.59&	95.99& 34.28&	93.15 &36.64&	91.82& 51.18&	88.09& 35.92&	92.26 \\ 
Energy~\cite{liu2020energy} (CLIP-L) & 10.62&	97.52 &30.46&	93.83 &32.25&	93.01& 44.35&	89.64& 29.42&	93.50\\
MSP~\cite{hendrycks2016baseline} (CLIP-B)&40.89&	88.63&	65.81&	81.24&	67.90&	80.14&	64.96&	78.16&	59.89&	82.04\\
MSP~\cite{hendrycks2016baseline} (CLIP-L)&34.54&	92.62&	61.18&	83.68&	59.86&	84.10&	59.27&	82.31&	53.71&	85.68\\
\midrule
 &\multicolumn{9}{c}{\textbf{Zero-shot (no training required)}}   \\
\textbf{MCM} (CLIP-B) 
& 30.91	& 94.61	& 37.59& 	92.57& 	44.69& 	89.77& 	57.77& 	86.11& 	42.74& 	90.77\\
\textbf{MCM} (CLIP-L) 
&28.38&	94.95&	29.00&	94.14&	35.42&	92.00&	59.88&	84.88&38.17&	91.49\\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{MCM scales effectively to large datasets.} To examine the scalability of MCM, we compare it with recent competitive OOD detection methods~\cite{fort2021exploring,huang2021mos} on the ImageNet-1k dataset (ID) in Table~\ref{tab:1k}.
We observe the following trends: 
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]
    \item Larger models lead to superior performance. Compared with CLIP-B, MCM based on CLIP-L reduces FPR95 by $4.57\%$. Zero-shot ID classification accuracy is also improved by $6.27\%$ with the larger model, reaching $73.28\%$ (see Appendix~\ref{sec:acc}).  This suggests that larger models are endowed with a better representation quality, which benefits both ID classification and OOD detection with MCM. Our finding echos with the recent observations~\cite{vaze2021open} that higher ID classification accuracy is correlated with stronger OOD detection performance.
    \item  MOS~\cite{huang2021mos} recently demonstrated competitive performance on ImageNet-1k, which requires model fine-tuning based on BiT~\cite{kolesnikov2020big}. In contrast, we show that MCM (CLIP-L) outperforms MOS by $1.38\%$ in AUROC while being zero-shot (training-free).
    \item MCM shares a softmax scaling function with the classic (visual) confidence-based score MSP~\cite{hendrycks2016baseline}. To implement MSP, we adopt the commonly used linear probe approach by fine-tuning a linear layer on frozen visual features of CLIP. After fine-tuning, ID accuracy significantly improves, reaching $84.12\%$ (CLIP-L). Interestingly, the OOD detection performance of MSP is worse than MCM by $15.54\%$ in FPR95. Under the same model fine-tuned with linear probing, we observe that the Energy score outperforms MSP, corroborating findings in~\cite{liu2020energy}. We investigate more in Section~\ref{sec:closer}. 
    \item Recently,  Fort~\etal~\cite{fort2021exploring} explore small-scale OOD detection by fine-tuning the full ViT model. When extended to large-scale tasks, we find that MCM still yields superior performance under the same image encoder configuration (ViT-B or ViT-L). This further highlights the advantage of utilizing vision-language joint embeddings for large-scale visual OOD detection.
\end{itemize}

\iffalse
\begin{enumerate}
    \item  Larger model leads to superior performance. Compared with CLIP-B, MCM based on CLIP-L improves FPR95 by $6.36\%$. Zero-shot ID classification accuracy is also improved by $7\%$ with the larger model, reaching $75.53\%$ (see Appendix~\ref{sec:acc}).  This suggests that larger models are endowed with a better representation quality, which benefits both ID classification and OOD detection. Our finding echoes with the recent observations~\cite{vaze2021open} that higher ID classification accuracy is correlated with stronger OOD detection performance.
    \item  MOS recently demonstrated competitive performance on ImageNet-1k, which requires model fine-tuning based on BiT~\cite{kolesnikov2020big}. In contrast, we show that MCM (CLIP-L) outperforms MOS by $1.67\%$ in AUROC while being zero-shot training-free.
    \item MCM shares a softmax scaling function as the classic (visual) logit-based score MSP. To implement MSP, we adopt the commonly used linear probe approach by fine-tuning a linear layer on frozen CLIP features. After fine-tuning, ID accuracy significantly improves, reaching $84.12\%$ (CLIP-L). Interestingly, the OOD detection performance of MSP is worse than MCM by $13.98\%$ in FPR95. We investigate more in Section~\ref{sec:closer}. 
    \item Recently,  Fort et al.~\cite{fort2021exploring} explore small-scale OOD detection by fine-tuning the full ViT model. When extended to large-scale tasks, we find that MCM still yields superior performance under the same image encoder configuration (ViT-B or ViT-L). This further highlights the advantage of utilizing vision-language embeddings for large-scale visual OOD detection.

\end{enumerate}
\fi


\paragraph{MCM benefits hard OOD detection.} Going beyond, we investigate whether MCM is still effective for hard OOD inputs. We consider the following two categories of hard OOD:

\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]
    \item \textbf{Semantically hard OOD}: OOD samples that are semantically similar to ID samples are particularly challenging for OOD detection algorithms~\cite{winkens2020contrastive}. To evaluate hard OOD detection tasks in realistic settings, here we consider ImageNet-10 (ID) vs. ImageNet-20 (OOD) and vice versa. The pair consists of high-resolution images with semantically similar categories such as dog versus wolf. As shown in Table~\ref{tab:hard}, MCM outperforms Mahalanobis~\cite{lee2018simple} by $\textbf{73.32\%}$ in FPR95 for ImageNet-10 (ID) vs. ImageNet-20 (OOD) and $\textbf{30.12\%}$ vice versa.  
    \item \textbf{Spurious OOD}: Modern neural networks can exploit spurious correlations for predictions~\cite{beery2018recognition}. For example, in the Waterbirds dataset~\cite{sagawa2019distributionally}, there exist spurious correlations between the habitat (\eg water) and bird types. A recent work~\cite{ming2022spurious} proposes a new type of hard OOD named spurious OOD and shows that most OOD detection methods perform much worse for spurious OOD inputs compared to non-spurious inputs.  The spurious OOD inputs are created to share the same background (\ie water) as ID data but have different object labels (\eg a boat rather than a bird). See Appendix~\ref{sec:spurious} for illustrations. The results are shown in Table~\ref{tab:hard}. It has been shown that CLIP representations are robust to distributional shifts~\cite{radford2021learning}. Therefore, while prior works~\cite{ming2022spurious} show that spurious OOD inputs are challenging for methods based on ResNet~\cite{he2016deep}, MCM and Mahalanobis scores based on pre-trained CLIP perform much better. On the other hand, fine-tuning exposes the model to the training set containing spurious correlations. As a result, MSP performs much worse than MCM ($39.57\%$ vs. $5.87\%$ in FPR95).

\end{itemize}



\begin{table}[t]
\centering
\caption{\small Performance comparison on  \textbf{hard OOD detection} tasks. MCM is competitive on all three hard OOD tasks without training involved. MSP (based on fine-tuned CLIP) does not further improve performance.}
\resizebox{0.95\textwidth}{!}{
\small
\begin{tabular}{llccc}
\toprule
 \multirow{2}{*}{\textbf{Method}} & \textbf{ID} & ImageNet-10 & ImageNet-20 & Waterbirds \\
 & \textbf{OOD}        &  ImageNet-20 &  ImageNet-10 & Spurious OOD  \\
 \midrule
 && FPR95 / AUROC & FPR95 / AUROC & FPR95 / AUROC\\
MSP~\cite{hendrycks2016baseline} (fine-tuning)  & & 9.38 / 98.31 & 12.51 / 97.70 & 39.57 / 90.99 \\
Mahalanobis~\cite{lee2018simple} (visual only) &    &  78.32 / 85.60&  43.03 / 89.94& 2.21 / 99.55\\
MCM (zero-shot)    & & {5.00} / {98.71} & 12.91 / {98.09} & 5.87 / 98.36    \\
\bottomrule
\end{tabular}
}
\vspace{-1mm}
\label{tab:hard}
\end{table}


\vspace{-3mm}
\paragraph{MCM outperforms CLIP-based baselines.} 
\begin{wrapfigure}[15]{r}{0.38\textwidth}
    \small
    \vspace{-4mm}
    \begin{center}
    \includegraphics[width=0.38\textwidth]{figures/clip_candidate_label.pdf}
    \end{center}
    \vspace{-2.5mm}
    \caption{Comparison with a candidate label-based score ZO-CLIP on ImageNet-20, based on our implementation of~\cite{esmaeilpour2022zero}. Implementation details are deferred to Appendix~\ref{sec:candidate_label}.}
    \vspace{-4mm}
    \label{fig:label_based}
\end{wrapfigure}
Two recent works also use CLIP embeddings for OOD detection~\cite{esmaeilpour2022zero,fort2021exploring}. However, fundamental limitations exist for both works. Fort \emph{et al.}~\cite{fort2021exploring} assume that a candidate OOD label set $\mathcal{Y}_{C}$ is known, and used $ \sum_{y\in \mathcal{Y}_C}\hat{p}(y|\*x)$ for OOD detection. Here the predictive probability $\hat p(y|\*x)$ is obtained by normalizing the inner products over $|\mathcal{Y}_\text{in}|+|\mathcal{Y}_C|$ classes. While applying softmax converts any vector to probabilities, as we show in Section~\ref{sec:method}, the converted probabilities do not necessarily correspond to $\mathbb{P}(\text{OOD}|\*x)$. 
Moreover, obtaining such an OOD label set is typically not feasible, which fundamentally limits its applicability.  A recent work~\cite{esmaeilpour2022zero} realizes this idea by training an extra text decoder on top of CLIP’s image encoder to generate candidate labels.
However, \cite{esmaeilpour2022zero} cannot guarantee the generated labels are non-overlapping with the ID labels. 

We enhance the baseline with a stronger decoder and a filter module (see Appendix~\ref{sec:candidate_label}).
As shown in Figure~\ref{fig:label_based}, MCM outperforms the enhanced baseline on all OOD datasets. Moreover, MCM is much simpler to use---alleviating the need for an OOD label set or training {an} additional caption generator.
{In contrast, the caption generator's performance largely affects OOD detection. Poor caption quality degenerates the OOD detection performance of candidate label-based methods. Moreover, obtaining a reliable {caption generator} for \emph{any input image} can significantly increase the computational overhead.}


\section{Discussion: A Closer Look at MCM} 
\label{sec:closer}
\paragraph{Empirical verification on the role of softmax.} In Section~\ref{sec:method}, we prove that softmax scaling on cosine similarity scores with a moderate $\tau$ improves the ID-OOD separability. Here we empirically verify our theoretical results. As shown in Figure~\ref{fig:temp}, compared to directly using the maximum cosine similarity without softmax (leftmost figure), softmax scaling with a temperature $\tau = 1$ significantly improves the performance by $22.6\%$ in FPR95, and further increasing $\tau$ (\eg $\tau = 10$) leads to similar performance. The results are based on ImageNet-100 (ID) versus iNaturalist (OOD).

Now, we verify if our theoretical bound (\emph{c.f.} Theorem~\ref{thm:main}) is satisfied empirically as well in Figure~\ref{fig:temp}. From the leftmost figure, we can estimate $\lambda^{\text{wo}} \approx 0.26$, $\delta \approx 0.03$, and $s_{\hat{y}_2} \approx 0.23$. By checking the third figure ($\tau = 1$ is the temperature value we use for most experiments), we approximate $\lambda \approx 0.011$. As $K = 100$, we plug in the values and obtain the lower bound $T = { \lambda(K - 1)\left(\lambda^{\text{wo}} + \delta - s_{\hat{y}_{2}}\right) \over K\lambda-1} \approx 0.65$. Since $\tau  = 1 > 0.65$, by Theorem~\ref{thm:main}, applying softmax scaling with $\tau=1$ is provably superior to without softmax scaling for OOD detection. 


\begin{figure*}[t]
  \centering
    \includegraphics[width=0.95\linewidth]{figures/temp_inaturalist.pdf}
    \vspace{-0.2cm}
\caption{The influence of softmax scaling and temperature. We use ImgeNet-100 (ID) vs. iNaturalist (OOD). Softmax scaling with a moderate temperature significantly improves FPR95.}
\label{fig:temp}
\vspace{-3mm}
\end{figure*}

\begin{wrapfigure}[15]{r}{0.4\textwidth}
    \vspace{-3mm}
    \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/clip_maha.pdf}
    \end{center}
    \vspace{-2mm}
    \caption{Comparison with Mahalanobis ({Maha}) score on ImageNet-1k.}
    \vspace{-3mm}
    \label{fig:mcm_maha}
\end{wrapfigure}

\paragraph{Are vision-language features better than visual feature alone?}
MCM can be interpreted as a distance-based approach---images that are closer to one of the $K$ class prototypes are more likely to be ID and vice versa. Here the class prototypes are defined based on a textual encoder. Alternatively, one can define the class prototypes based on visual features. For example, Mahalanobis~\cite{lee2018simple} defines a class prototype as the average of visual embeddings for images belonging to the same class. This raises the question whether MCM (with \emph{multi-modal} vision-language features) is better than Mahalanobis (with \emph{single-modal} visual feature). For a fair comparison, we use the same ViT image encoder from CLIP-B. Both MCM and Mahalanobis extract visual features from the penultimate layer. On ImageNet-1k, Mahalanobis displays a limited performance, with $73.14\%$ AUROC averaged across four OOD test datasets ($\textbf{90.77\%}$ for MCM), as shown in Figure~\ref{fig:mcm_maha}. From a practical perspective, Mahalanobis requires computing the inverse covariance matrix, which can be both computationally expensive and inaccurate when the number of samples is scarce and the number of ID classes grows.
{In contrast, MCM is easier to use and more robust.}

\paragraph{MCM without softmax scaling.}
In Section~\ref{sec:method}, we provide theoretical justifications for the necessity of softmax scaling for CLIP-like models. To further verify our observations empirically, we show OOD detection performance based on the maximum cosine similarity score $S_{\text{MCM}}^{\text{wo}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) = \max_{i\in[K]} s_i(\*x')$. The results are shown in Table~\ref{tab:no_softmax}. For easy tasks such as Food-101~\cite{KrauseStarkDengFei-Fei_3DRR2013}, Stanford-Cars~\cite{KrauseStarkDengFei-Fei_3DRR2013}, and Oxford-Pet~\cite{parkhi12a} as ID, the performance of maximum cosine similarity score is similar to MCM (see Table~\ref{tab:beyond} and Table~\ref{tab:1k}). However, for more challenging tasks such as ImageNet-20 and ImageNet-1k, MCM significantly outperforms that without softmax scaling. For example, the average FPR95 is improved by $\textbf{11.33\%}$ on ImageNet-20 and $\textbf{26.34\%}$ on ImageNet-1k, which highlights the necessity of a proper scaling function for CLIP-based OOD detection.

\begin{table}[t]
\caption{\small Zero-shot OOD detection of $S_{\text{MCM}}^{\text{wo}}$ based on CLIP-B/16.}
\label{tab:no_softmax}
\centering
\resizebox{1\textwidth}{!}{{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{ID Dataset}} & \multicolumn{8}{c}{\textbf{OOD Dataset}}                        & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Average} }} \\
                        & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Texture} & \multicolumn{2}{c}{}             \\
                        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
                        & \textbf{FPR95$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR95$\downarrow$}           & \textbf{AUROC$\uparrow$}         & \textbf{FPR95$\downarrow$}          & \textbf{AUROC$\uparrow$}        & \textbf{FPR95$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR95$\downarrow$}          & \textbf{AUROC$\uparrow$}     \\     
                        \midrule
       
\textbf{Stanford-Cars}~\cite{KrauseStarkDengFei-Fei_3DRR2013} 
&0.00	&100&	0.02&	99.99&	0.26&	99.94&	0.00	&100&	0.07&	99.98\\
\textbf{Food-101}~\cite{bossard14}
&0.56&	99.86&	0.09&	99.95&	0.49&	99.88&	8.33&	97.44&	2.37&	99.28\\
\textbf{Oxford-Pet}~\cite{parkhi12a}
&0.02&	99.98&	0.05&	99.97&	0.20&	99.94&	0.27&	99.91&	0.14&	99.95\\
\textbf{ImageNet-10}
&2.40&	99.42&	1.79&	99.55&	2.83&	99.32	&1.86&	99.56&	2.22	&99.46\\
\textbf{ImageNet-20}
&14.96&	97.87&	13.10&	97.97	&14.21&	97.67&	13.46&	97.32&	13.93&	97.71\\
\textbf{ImageNet-1k}
&61.66&	89.31&	64.39&	87.43&	63.67&	85.95&	86.61&	71.68&	69.08&	83.59\\
\bottomrule
\end{tabular}}
}
\end{table}


\paragraph{MCM for ResNet-based CLIP models.}
Our main results are based on the CLIP model with ViT image encoder. We additionally investigate the effectiveness of MCM on ResNet-based CLIP. Specifically, we use RN50x4 (178.3M), which shares a similar number of parameters as CLIP-B/16 (149.6M). The results are shown in Table~\ref{tab:resnet}.
We can see that MCM still shows promising results with ResNet-based CLIP models, and the performance is comparable between RN50x4 and CLIP-B/16  ($89.97$ vs. $90.77$ in AUROC).
\begin{table}[hbt]
\caption{Comparison with ResNet-based CLIP models on ImageNet-1k (ID).}
\label{tab:resnet}
\centering
\resizebox{\textwidth}{!}{{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multicolumn{8}{c}{\textbf{OOD Dataset}}                        & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Average} }} \\
                        & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Texture} & \multicolumn{2}{c}{}             \\
                        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
                        & \textbf{FPR95$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR95$\downarrow$}           & \textbf{AUROC$\uparrow$}         & \textbf{FPR95$\downarrow$}          & \textbf{AUROC$\uparrow$}        & \textbf{FPR95$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR95$\downarrow$}          & \textbf{AUROC$\uparrow$}        \\  
                        \midrule


RN50x4 & 44.51 &91.51  &35.11 &92.84  & 43.74 &89.60  & 57.73 &85.93  & 45.27 &89.97 \\
CLIP-B/16 & 30.91	& 94.61	& 37.59& 	92.57& 	44.69& 	89.77& 	57.77& 	86.11& 	42.74& 	90.77\\
\bottomrule
\end{tabular}}
}
\end{table}



\begin{wraptable}{r}{5cm}
\small
\vspace{-3mm}
\begin{tabular}{l}
\toprule
A photo of a <label>.\\
A blurry photo of a <label>.\\
A photo of many <label>.\\
A photo of the large <label>.\\
A photo of the small <label>.\\
\bottomrule
\end{tabular}
\caption{The five prompt templates.}
\label{tab:prompt}
\vspace{-3mm}
\end{wraptable}
\paragraph{Effect of prompt ensembling.}
We examine MCM's performance with prompt ensembling. For example, Radford~\emph{et al.}~\cite{radford2021learning} create 80 possible prompts according to the image modalities and nuances in ImageNet. We experiment with the two prompt sets, one of size 80 as in~\cite{radford2021learning}, and our own set of 5 prompts. Ensembles are obtained by averaging the textual features. As expected, using ensembles increases the ID classification accuracy on ImageNet-1k (2\% with CLIP-B and 3\% with CLIP-L). 
For OOD detection, the average FPR95 is reduced from 38.17\% with the default prompt to 35.23\%{$\downarrow$} with an ensemble of five prompts shown in Table~\ref{tab:prompt}. In addition, the detection performance with 5 prompts is slightly better than with 80 prompts. Note that prompt ensembling does not increase the inference-time cost, as the textual embeddings (across many prompts) can be pre-calculated and averaged into a single embedding.

\section{Related Works}
\label{sec:related_works}
\noindent\textbf{OOD detection in computer vision.} For open-world multi-class classification, the goal of OOD detection is to derive a binary ID-OOD classifier along with a multi-class classification model for visual inputs. A plethora of methods has been proposed for deep neural networks~\cite{yang2021generalized}, including generative model-based methods~\cite{cai2023frequency,ge2017generative,kirichenko2020normalizing,nalisnick2019deep,neal2018open,oza2019c2ae,ren2019likelihood,serra2019input,xiao2020likelihood}, and discriminative-model based methods. For the latter category, an OOD score can be derived based on the softmax output~\cite{openmax16cvpr,2018onemore,hein2019why,hendrycks2016baseline,hsu2020generalized,huang2021mos,liang2018enhancing,yang2021scood}, energy-based score~\cite{du2022vos,liu2020energy,ming2022posterior,sun2021tone,sun2022dice,wang2021canmulti}, gradient information~\cite{huang2021importance}, or the feature embeddings~\cite{du2022siren,lee2018simple,2020gram,2021ssd,sun2022knn,tack2020csi,winkens2020contrastive} of a model. Morteza~\emph{et al.}~\cite{morteza2022provable}, Fang~\emph{et al.}~\cite{fang2022learnable}, and Bitterwolf~\emph{et al.}~\cite{bitterwolf2022breaking} provided theoretical analysis for OOD detection. Recent works~\cite{roy2022does,wang2022partial} also explored OOD detection for long-tailed distributions. Works insofar have mostly focused on OOD detection for a task-specific model using only visual information. In contrast, we explore a novel paradigm of zero-shot OOD detection that incorporates rich textual information and can perform a wide variety of tasks. 

\noindent\textbf{OOD detection in natural language processing.} Distribution shifts can occur due to the change of topics and domains, unexpected user utterances,~\etc. {Challenging}
benchmarks~\cite{koh2021wilds} and characterization of distributional shifts~\cite{arora-etal-2021-types} {have been} proposed in recent years.  Compared to early language models such as ConvNets and LSTM~\cite{hochreiter1997long}, pre-trained language models are more robust to distribution shifts and more effective at identifying OOD instances~\cite{hendrycks-etal-2020-pretrained,Podolskiy21,xu-etal-2021-unsupervised}. Various algorithmic solutions are proposed to handle OOD detection, including outlier exposure~\cite{hu2021uncertainty}, model ensembling~\cite{lietal2021kfolden}, data augmentation~\cite{chen2021gold, zhan2021out, zheng2020out}, contrastive learning~\cite{jin2022towards,zhou2021contrastive}, and an auxiliary module that incorporates domain labels~\cite{shen2021enhancing}. Tan~\emph{et al.}~\cite{Tan2019OutofDomainDF} also explore zero-shot OOD detection for text classification tasks. However, prior works focus on pure natural language processing (NLP) settings, while we explore utilizing textual embeddings for zero-shot \emph{visual} OOD detection.

\paragraph{Vision-language models.} Utilizing large-scale pre-trained vision-language models for multimodal downstream tasks has become an emerging paradigm with remarkable performance~\cite{gu2020self,uppal2022multimodal}. In general, two types of architectures exist: single-stream models like VisualBERT~\cite{li2019visualbert} and ViLT~\cite{kim2021vilt}
{feed the concatenated text and visual features}
into a single transformer-based encoder; dual-stream models such as CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling}, and FILIP~\cite{yao2021filip} use separate encoders for text and image and optimize with contrastive objectives to align semantically similar features in different modalities. In particular, CLIP enjoys popularity due to its simplicity and strong performance. CLIP-like models inspire numerous follow-up works~\cite{li2021supervision,zhang2021tip,zhou2022cocoop}, which aim to improve data efficiency and better adaptation to downstream tasks. This paper adopts CLIP as the target pre-trained model, but our approach can be generally applicable to contrastive models that promote vision-language alignment.

\paragraph{Multi-modal OOD detection.} 
Exploring textual information for visual OOD detection is a new area with limited existing works. Fort~\emph{et al.}~\cite{fort2021exploring} propose to feed the potential OOD labels to the textual encoder of CLIP~\cite{radford2021learning}. Recently, Esmaeilpour~\emph{et al.}~\cite{esmaeilpour2022zero} propose to train a label generator based on the visual encoder of CLIP and use the generated labels for OOD detection. While both works rely on a set of candidate OOD labels, MCM is OOD-agnostic and alleviates the need for prior information on OOD. Moreover, prior works~\cite{esmaeilpour2022zero,radford2021learning} only focus on small-scale inputs. We largely expand the scope to a wide range of large-scale realistic datasets, and show new theoretical insights.

\section{Conclusion}
In this work, we delve into a new landscape for OOD detection, departing from the classic single-modal toward a multi-modal regime. By viewing the textual features as the ``concept prototypes'', we explore a new OOD detection approach MCM, based on the joint vision-language representations. Unlike the majority of OOD detection methods, MCM offers several compelling advantages: training-free, generalizable to many tasks, scalable to hundreds of classes, and does not require any prior information on OOD inputs. Moreover, we provide theoretical guarantees on how softmax scaling provably improves zero-shot OOD detection. We investigate the effectiveness of MCM on a wide range of large-scale realistic tasks, including several types of hard OOD datasets.  Lastly, we demonstrate the advantage of vision-language features over pure visual features for OOD detection. We hope our work will inspire future research toward multi-modal OOD detection.


\section*{Acknowledgement}
The authors wish to thank Junjie Hu, Ying Fan, Ruisu Zhang, Andrew Geng, and Soumya Suvra Ghosal for the helpful discussions.  The work is supported by a Google-Initiated Research Grant, and gift funding from Adobe Research. 

\bibliography{clip_ood}
\bibliographystyle{plain}

\medskip



\newpage




\appendix
\section{Theoretical Justification: Softmax Scaling for Zero-Shot OOD Detection}
\label{sec:proof}
In this section, we provide the proof for Theorem~\ref{thm:main} in Section~\ref{sec:method}, which states the benefits of applying softmax scaling to inner products for OOD detection. We begin with a review of notations.

\paragraph{Notations.} We denote the text encoder of a pre-trained CLIP-like model as $\mathcal{T}: t \rightarrow \mathbb{R}^d$ and the image encoder $\mathcal{I}: \*x \rightarrow \mathbb{R}^d$. For a given task with label set $\mathcal{Y}_\text{in}=\{y_1, y_2,...,y_K\}$, we construct a collection of concept vectors $\mathcal{T}(t_i)$. For a given input $\*x'$, we denote the cosine similarity \emph{w.r.t.} concept vectors as $s_i(\*x') = \frac{\mathcal{I}(\*x') \cdot \mathcal{T}(t_i)}{\lVert \mathcal{I}(\*x')\rVert \cdot \lVert \mathcal{T}(t_i) \rVert} \, \forall i\in[K]$, where $|s_i(\*x')| \leq B$ for all $\*x'\in \mathcal{X}$.\footnote{In practice, we observe that $s_i\in[0.1, 0.3]$ for CLIP with high probability.}. We define the maximum concept matching (MCM) score as: $S_{\text{MCM}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) = \max_{i\in[K]} \frac{e^{s_i(\*x')/\tau}}{\sum_{j=1}^K e^{s_j(\*x')/\tau}}$. We denote the maximum inner product without applying softmax scaling as $S_{\text{MCM}}^{\text{wo}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) = \max_{i\in[K]} s_i(\*x')$. By convention, the OOD detection functions are given by:
\begin{align*}
	G^{\text{wo}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) =\begin{cases} 
      1 & S_{\text{MCM}}^{\text{wo}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I})\ge \lambda^{\text{wo}} \\
      0 & S_{\text{MCM}}^{\text{wo}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) < \lambda^{\text{wo}} 
  \end{cases},
\end{align*}
\begin{align*}
	G(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) =\begin{cases} 
      1 & S_{\text{MCM}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I})\ge \lambda \\
      0 & S_{\text{MCM}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) < \lambda 
  \end{cases},
\end{align*}

\paragraph{Remarks:} By convention, $1$ represents the positive class (ID) and $0$ indicates OOD; $\lambda$ and $\lambda^{\text{wo}}$ are typically chosen such that the true positive rate is at $95\%$.

For convenience, we paste the assumptions and the theorem in Section~\ref{sec:method} below,

\begin{assumption}
\label{thm:sep} Let $z := \mathbbm{1}\{y \in \mathcal{Y}_\text{in}\}$ and $Q_{\*x}$ denotes the out-of-distribution $\mathbb{P}_{\*x \mid z = 0}$ (marginal distribution of $\*x$ conditioned on $z = 0$). Assume $\exists\,\delta>0$ such that
$$
Q_{\*x}\left({1\over K-1}\sum_{i \neq \hat{y}}\left[s_{\hat{y}_{2}}(\*x)-s_{i}(\*x)\right]< \delta\right)=1,
$$
where $\hat y := \text{argmax}_{i\in[K]} s_i(\*x)$ and $\hat y_2 := \text{argmax}_{i\neq \hat y, i\in[K]} s_i(\*x)$ denote the indices of the largest and second largest cosine similarities for an OOD input $\*x$. 
\end{assumption}

\begin{theorem}
Given a pre-trained CLIP-like model $(\mathcal{T}, \mathcal{I})$ and a task with label set $\mathcal{Y}_\text{in}=\{y_1, y_2,...,y_K\}$. If $Q_{\*x}$ satisfy Assumption~\ref{thm:sep}, Then there exists a constant $T = { \lambda(K - 1)\left(\lambda^{\text{wo}} + \delta - s_{\hat{y}_{2}}\right) \over K\lambda-1} $ such that for any temperature $\tau>T$,
we have:
$$
\operatorname{FPR}(\tau, \lambda) \leq \operatorname{FPR}^{\text{wo}}(\lambda^{\text{wo}} ),
$$
where $\operatorname{FPR}(\tau, \lambda)$ is the false positive rate based on softmax scaling with temperature $\tau$ and threshold $\lambda$; $ \operatorname{FPR}^{\text{wo}}(\lambda^{\text{wo}} )$ is the false positive rate without softmax scaling based on threshold $\lambda^{\text{wo}}$. This suggests that applying softmax scaling with temperature results in superior OOD detection performance compared to without softmax scaling. 
\end{theorem}
\begin{proof}


By definition, we express the false positive rate $\operatorname{FPR}(\tau, \lambda)$ as follows,
\begin{equation*}
\begin{aligned}
\operatorname{FPR}(\tau, \lambda) &=  \mathbb{P}\left(G(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) =1 \mid z=0\right) \\
&= Q_{\*x'}\left(G(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) =1\right)\\
&= Q_{\*x'}\left(p_{\hat{y}}\left(\*x' ; \tau\right)>\lambda\right)\\
&=  Q_{\*x'}\left(\frac{e^{s_{\hat{y}}(\*x')/\tau}}{\sum_{j=1}^K e^{s_j(\*x')/\tau}}>\lambda\right)\\
&= Q_{\*x'}\left(\frac{1}{\lambda}>\sum_{i=1}^{K} \exp \left(\frac{s_i(\*x')-s_{\hat{y}}(\*x')}{\tau}\right)\right)
\end{aligned}
\end{equation*}

By inequality $e^{x} \geq 1+x$, we have,
$$
Q_{\*x'}\left(\frac{1}{\lambda}>\sum_{i=1}^{K} \exp \left(\frac{s_{i}(\*x')-s_{\hat{y}}(\*x')}{\tau}\right)\right) \leq Q_{\*x'}\left(\frac{1}{\lambda}>\sum_{i=1}^{K}\left[1+\frac{s_{i}(\*x')-s_{\hat{y}}(\*x')}{\tau}\right]\right)
$$
This indicates
$$
\begin{aligned}
& Q_{\*x'}\left(\frac{1}{\lambda}>\sum_{i=1}^{K} \exp \left(\frac{s_{i}(\*x')-s_{\hat{y}}(\*x')}{\tau}\right)\right) \leq Q_{\*x'}\left(\frac{1}{\lambda}>\sum_{i=1}^{K}\left[1+\frac{s_{i}(\*x')-s_{\hat{y}}(\*x')}{\tau}\right]\right) \\
&= Q_{\*x'}\left(\sum_{i=1}^{K}\left(s_{\hat{y}}(\*x')-s_{i}(\*x')\right)>\left(K-\frac{1}{\lambda}\right) \tau\right)
\end{aligned}
$$
Since
$$
\begin{aligned}
\sum_{i=1}^{K}\left(s_{\hat{y}}(\*x')-s_{i}(\*x')\right) &=\sum_{i \neq \hat{y}}\left(s_{\hat{y}}(\*x')-s_{\hat{y}_{2}}(\*x')+s_{\hat{y}_{2}}(\*x')-s_{i}(\*x')\right) \\
&=\sum_{i \neq \hat{y}}\left(s_{\hat{y}}(\*x')-s_{\hat{y}_{2}}(\*x')\right)+\sum_{i \neq \hat{y}}\left(s_{\hat{y}_{2}}(\*x')-s_{i}(\*x')\right) \\
&=(K-1)\left(s_{\hat{y}}(\*x')-s_{\hat{y}_{2}}(\*x')\right)+\sum_{i \neq \hat{y}}\left(s_{\hat{y}_{2}}(\*x')-s_{i}(\*x')\right)
\end{aligned}
$$
By Assumption~\ref{thm:assumption}, we have
$$
Q_{\*x'}\left(\sum_{i=1}^{K}\left(s_{\hat{y}}(\*x')-s_{i}(\*x')\right)<(K-1)\left(s_{\hat{y}}(\*x')-s_{\hat{y}_{2}}(\*x')\right)+(K-1) \delta\right)=1 .
$$
Therefore,
$$
\begin{aligned}
Q_{\*x'}\left(\sum_{i=1}^{K}\left(s_{\hat{y}}(\*x')-s_{i}(\*x')\right)>\left(K-\frac{1}{\lambda}\right) \tau\right) &\leq  Q_{\*x'}\left(s_{\hat{y}}(\*x')-s_{\hat{y}_{2}}(\*x')>-\delta_{2}+\left(K-\frac{1}{\lambda}\right) \frac{\tau}{K-1}\right) \\
& = Q_{\*x'}\left(s_{\hat{y}}(\*x')-s_{\hat{y}_{2}}(\*x')>-\delta_{2}+\lambda'\right)\\
&= Q_{\*x'}\left(s_{\hat{y}}(\*x')> s_{\hat{y}_{2}}(\*x')-\delta_{2}+\lambda'\right),
\end{aligned}
$$
where $\lambda' = \left(K-\frac{1}{\lambda}\right) \frac{\tau}{K-1}$ is a monotonic function of $\lambda$ (\ie minimizing false positive rate \emph{w.r.t.} $\lambda$ is equivalent to minimizing \emph{w.r.t.}  $\lambda'$.)

For $\tau > 0$, we can rewrite the MCM score as 
$$
\begin{aligned}
S_{\text{MCM}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) &= \max_{i\in[K]} \frac{e^{s_i(\*x')/\tau}}{\sum_{j=1}^K e^{s_j(\*x')/\tau}} =  \frac{e^{s_{\hat{y}}(\*x')/\tau}}{\sum_{j=1}^K e^{s_j(\*x')/\tau}} \\
&= \frac{1}{ 1 + \sum_{j=1, j \neq \hat{y}}^K e^{\left(s_j(\*x') -s_{\hat{y}}(\*x')\right)/\tau}}
\end{aligned}
$$
As $\hat y := \text{argmax}_{i\in[K]} s_i(\*x)$, $s_j(\*x') -s_{\hat{y}}(\*x') \leq 0$,  $S_{\text{MCM}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) $ is a monotonically decreasing function of $\tau$, we have: 
$$
S_{\text{MCM}}(\*x';\mathcal{Y}_\text{in},\mathcal{T},\mathcal{I}) > \lim_{\tau \rightarrow \infty}  \frac{1}{ 1 + \sum_{j=1, j \neq \hat{y}}^K e^{\left(s_j(\*x') -s_{\hat{y}}(\*x')\right)/\tau}} = {1\over K}
$$
Therefore by the definition of $\lambda$, we have $\lambda > {1\over K}$, $\lambda' = \left(K-\frac{1}{\lambda}\right) \frac{\tau}{K-1} > 0$

For moderately large $\tau > T$ where $T = { \lambda(K - 1)\left(\lambda^{\text{wo}} + \delta - s_{\hat{y}_{2}}\right) \over K\lambda-1} $,  we always have $s_{\hat{y}_{2}}(\*x')-\delta+\lambda' >\lambda^{\text{wo}} $. Therefore, we obtain the following inequality,
$$
\operatorname{FPR}(\tau, \lambda) \leq Q_{\*x'}\left(s_{\hat{y}}(\*x')> s_{\hat{y}_{2}}(\*x')-\delta_{2}+\lambda'\right)
\leq Q_{\*x'}\left(s_{\hat{y}}(\*x') >\lambda^{\text{wo}}\right) :=  \operatorname{FPR}^{\text{wo}}(\lambda^{\text{wo}}),
$$
which means that the FPR without softmax scaling is  larger than that with softmax scaling and a moderately large temperature. We show in Section~\ref{sec:closer} that the bound is indeed satisfied in practice with a large-scale ID dataset.
\end{proof}
\newpage
\section{Experimental Details}
\label{sec:exp_detail}

\subsection{Software and Hardware}
All methods are implemented in Pytorch 1.10. We run all OOD detection experiments on NVIDIA GeForce RTX-2080Ti GPU and use NVIDIA A100 GPU for fine-tuning CLIP and ViT.

\subsection{Hyperparameters} The only hyperparameter in MCM is the (temperature) scaling factor $\tau$. We use $\tau=1$ by default unless otherwise specified. Our experiments suggest that MCM is insensitive to the scaling factor, where $\tau$ in a wide range of $[0.5, 100]$ shares similar performance.

\subsection{Datasets}

\textbf{ImageNet-10} We create ImageNet-10 that mimics the class distribution of CIFAR-10 but with high-resolution images. It contains the following categories (with class ID): warplane (n04552348), sports car (n04285008), brambling bird, (n01530575), Siamese cat (n02123597), antelope (n02422699), Swiss mountain dog (n02107574), bull frog (n01641577), garbage truck (n03417042), horse (n02389026), container ship (n03095699).

\textbf{ImageNet-20} For hard OOD evaluation with realistic datasets, we curate ImageNet-20, which consists of 20 classes semantically similar  to ImageNet-10 (\eg dog (ID) vs. wolf (OOD)). The categories are selected based on the distance in the WordNet synsets~\cite{fellbaum2010wordnet}. Specifically, it contains the following categories: sailboat (n04147183), canoe (n02951358), balloon (n02782093), tank (n04389033), missile (n03773504), bullet train (n02917067), starfish (n02317335), spotted salamander (n01632458), common newt (n01630670), zebra (n01631663), frilled lizard (n02391049), green lizard (n01693334), African crocodile (n01697457), Arctic fox (n02120079), timber wolf (n02114367), brown bear (n02132136), moped (n03785016), steam locomotive (n04310018), space shuttle (n04266014), snowmobile (n04252077).

We hope the above two datasets will help future research on large-scale hard OOD detection. We provide a script for generating the datasets at 
\url{https://github.com/deeplearning-wisc/MCM}.

\textbf{ImageNet-100} We randomly sample 100 classes from ImageNet-1k to curate ImageNet-100. To facilitate reproducibility, the script for generating the dataset and the class list are provided at \url{https://github.com/deeplearning-wisc/MCM}.

\paragraph{Conventional (non-spurious) OOD datasets} Huang~\etal~\cite{huang2021mos} curate a diverse collection of subsets from iNaturalist~\cite{van2018inaturalist}, SUN~\cite{xiao2010sun}, Places~\cite{zhou2017places}, and Texture~\cite{cimpoi2014describing} as large-scale OOD datasets for ImageNet-1k, where the classes of the test sets do not overlap with ImageNet-1k. We provide a brief introduction to each dataset as follows.
 
\textbf{iNaturalist} contains images in the natural world~\cite{van2018inaturalist}. It has 13 super-categories and 5,089 sub-categories covering plants, insects, birds, mammals, and so on. We use the subset that contains 110 plant classes not overlapping with ImageNet-1k.

\textbf{SUN} stands for the Scene UNderstanding Dataset~\cite{xiao2010sun}. SUN contains 899 categories that cover more than indoor, urban, and natural places with or without human beings appearing. We use the subset which contains 50 natural objects not showing in ImageNet-1k.

\textbf{Places} is a large scene photographs dataset~\cite{zhou2017places}. It contains photos that are labeled with scene semantic categories from three macro-classes: Indoor, Nature, and Urban. The subset we use is sampled from 50 categories that are not present in ImageNet-1k.

\textbf{Texture} stands for the Describable Textures Dataset~\cite{cimpoi2014describing}. It contains images of textures and abstracted patterns. As no categories overlap with ImageNet-1k, we use the entire dataset as in~\cite{huang2021mos}.
\subsection{Baselines and sources of model checkpoints}
For the Mahalanobis score~\cite{lee2018simple}, we use the feature embeddings without $l_2$ normalization as Gaussian distributions naturally do not fit hyperspherical features. Alternatively, one can normalize the embeddings first and then apply the Mahalanobis score. 

For Fort~\etal~\cite{fort2021exploring} in Table~\ref{tab:1k}, we fine-tune the whole ViT model on the ID dataset. Specifically, we use the publicly available checkpoints from Hugging Face where the ViT model is pre-trained on ImageNet-21k and fine-tuned on ImageNet-1k. For example, the checkpoint for ViT-B is available at \url{https://huggingface.co/google/vit-base-patch16-224}. 

For CLIP models, our reported results are based on checkpoints provided by Hugging Face for CLIP-B \url{https://huggingface.co/openai/clip-vit-base-patch16} and CLIP-L \url{https://huggingface.co/openai/clip-vit-large-patch14}. Similar results can be obtained with checkpoints in the codebase by OpenAI \url{https://github.com/openai/CLIP}. Note that for CLIP (RN50x4), which is not available in Hugging Face, we use the checkpoint provided by OpenAI. 
\section{Spurious OOD Datasets}
\label{sec:spurious}
In general, spurious attributes refer to statistically informative features that co-exist with the majority of ID samples but do not necessarily capture cues related to the labels such as color, texture, background, etc~\cite{ barbu2019objectnet,beery2018recognition,geirhos2018imagenettrained, xiao2021noise,ijcai2017zhu}.
A recent work~\cite{ming2022spurious} investigated a new type of hard OOD samples (called spurious OOD) that contain spurious or environmental features, but no object features related to the ID classes. A concrete example is shown in Figure~\ref{fig:spurious}, where images of birds co-occur frequently with either the land background or water background. Modern neural networks can spuriously rely on the image background (\eg water or land) for classification instead of learning to recognize the actual object~\cite{ribeiro2016should}. Ming~\emph{et al.}~\cite{ming2022spurious} show that spurious OOD samples remain challenging for most common OOD detection methods based on pure vision models such as ResNet~\cite{he2016deep}. 

For ID dataset, we use Waterbirds~\cite{sagawa2019distributionally}, which combines bird photographs from \textsc{CUB-200}~\cite{WahCUB_200_2011}
with water or land background images from \textsc{Places}~\cite{zhou2017places}. For the spurious OOD dataset, we use the one created in~\cite{ming2022spurious} consisting of land and water background from Places~\cite{zhou2017places}.

\begin{figure*}[thb]
  \centering
    \includegraphics[width=0.85\linewidth]{figures/spurious_v2.pdf}
\caption{\small Illustration of spurious OOD samples for Waterbirds~\cite{sagawa2019distributionally}. Images are taken from~\cite{ming2022spurious}.}
\label{fig:spurious}
\end{figure*}



\section{ID Classification Accuracy}
\label{sec:acc}
Table~\ref{tab:acc} shows the multi-class classification accuracy on ImageNet-1k for methods in Table~\ref{tab:1k}.
\begin{table}[hbt]
\caption{ID classification accuracy on ImageNet-1k (\%)}
\label{tab:acc}
\centering
\small
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{ID ACC}\\
\midrule
\multicolumn{2}{c}{\textbf{zero-shot }}   \\ 
MCM (CLIP-B/16) &67.01 \\
MCM (CLIP-L/14) & 73.28\\
\multicolumn{2}{c}{\textbf{w. fine-tuning }}   \\ 
MSP (CLIP-B/16) & 79.39 \\
MSP (CLIP-L/14) &84.12\\
Energy~\cite{liu2020energy} (CLIP-B/16)&79.39 \\
Energy~\cite{liu2020energy} (CLIP-L/14) & 84.12\\
Fort et al.~\cite{fort2021exploring} (ViT-B/16) &81.25\\ 
Fort et al.~\cite{fort2021exploring} (ViT-L/14) & 84.05\\
MOS~\cite{huang2021mos} (BiT) & 75.16 \\
\bottomrule
\end{tabular}
\end{table}

\section{Implementation of CLIP-Based Baselines}
\subsection{Overview of Baselines}
\begin{figure*}[thb]
  \centering
    \includegraphics[width=0.95\linewidth]{figures/candidate_label.pdf}
\caption{\small Zero-shot OOD detection with candidate OOD labels. The ID classification task is defined by a set of class labels $\mathcal{Y}_\text{in}$. With an additional set of candidate labels  $\mathcal{Y}_\text{C}$ that describes the contents of the input image, the OOD detection scoring function can be calculated by normalizing over the expanded space of cosine similarities.}
\vspace{-0.3cm}
\label{fig:teaser_2}
\end{figure*}


\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{figures/pipelines.pdf}
\caption{Improved pipeline to generate candidate OOD labels. It consists of three main components: a caption generator, a syntactic parser, and a filtering module to remove candidate labels that overlap with the ID label set.}
\label{fig:pipeline}
\end{figure*}



\label{sec:candidate_label}
We review two previous works on CLIP-based OOD detection~\cite{esmaeilpour2022zero,fort2021exploring} in Figure~\ref{fig:teaser_2}, which derive the scoring function based on candidate OOD labels. For a given task with ID label set $\mathcal{Y}_\text{in}=\{y_1, y_2,...,y_K\}$ and candidate labels $\mathcal{Y}_\text{C}=\{y_{K+1}, y_{K+2},...,y_{K+L}\}$, where ideally $\mathcal{Y}_\text{in}\cap \mathcal{Y}_\text{C} = \emptyset$, they construct a collection of text embeddings $\mathcal{T}(t_i), i\in \{1,2,...,K+L\}$. Here, $t_i$ is the text prompt ``\texttt{this is a photo of a $\langle y_i \rangle$}''
for a label $y_i$. For any test input image $\*x$, we can calculate the label-wise matching score based on the cosine similarity between the image and text features: $s_i(\*x) = \frac{\mathcal{I}(\*x) \cdot \mathcal{T}(t_i)}{\lVert \mathcal{I}(\*x')\rVert \cdot \lVert \mathcal{T}(t_i) \rVert}$.
Therefore, a detection score can be derived as:
\begin{equation*}
    S(\*x;\mathcal{Y}_\text{in},\mathcal{Y}_\text{C}, \mathcal{T},\mathcal{I}) = \sum_{i=1}^{K} \frac{ e^{s_i(\*x)/\tau}}{\sum_{j=1}^{K+L} e^{s_j(\*x)/\tau}},
\end{equation*}
where $\tau > 0$ is the temperature scaling hyperparameter. 

\subsection{Obtaining OOD Candidate Labels}
For the baseline methods, obtaining OOD candidate labels is a major challenge and limitation. Recently,~\cite{esmaeilpour2022zero} propose ZO-CLIP, where a transformer (decoder)  based on the image encoder of CLIP is used to generate candidate labels. The transformer is trained from scratch on the  COCO dataset~\cite{lin2014microsoft} with simple teacher forcing algorithms. Although the decoder trained on COCO may work well on CIFAR (ID), it does not scale up to large-scale datasets such as ImageNet~\cite{deng2009imagenet} where categories are not covered in COCO. As a result,~\cite{esmaeilpour2022zero} only test on small-scale datasets with common classes such as CIFAR (ID).

We improve the baseline by using a high-quality caption generator pre-trained on much larger datasets, which not only saves computational overhead but can potentially improve the quality of generated labels.  The pipeline involves three components (see Figure~\ref{fig:pipeline}):
\begin{itemize}
    \item A caption generator. Given an input image, it generates a caption serving as the textual description of the input.  In this work, we consider ClipCap~\cite{mokady2021ClipCap}, which uses GPT-2~\cite{radford2019language} to generate captions based on CLIP's image encoder. ClipCap is pre-trained on a much larger dataset Conceptual Captions~\cite{ng2020understanding} compared to COCO, which can be viewed as an enhanced version of the ZO-CLIP baseline~\cite{esmaeilpour2022zero}. The checkpoints are publicly available\footnote{\url{https://github.com/rmokady/CLIP_prefix_caption}}. 
    \item  A syntactic parser. Given a caption, we extract noun objects using a parsing toolkit released by spaCy \footnote{\url{https://spacy.io/models/en}}. Those nouns can be used as candidate labels $\mathcal{Y}_C$ of the input image.
    \item A filter module. Unlike \cite{esmaeilpour2022zero}, we further enhance the baseline by adopting a filtering technique to remove overlapping categories in $\mathcal{Y}_C$ with ID labels $\mathcal{Y}_{\text{in}}$, which we detail below.
\end{itemize}

\subsection{Label Filtering}
\label{sec:example}
\paragraph{Example.} To illustrate the effects of filtering, we begin with a concrete example where ID labels are [``frog'',``bird''$\ldots$ ``truck''], as shown in Figure~\ref{fig:pipeline}. The generated labels (without filtering) of an ID input of a bird sitting on a tree are [``bird'', ``tree'']. Therefore, $\mathcal{Y}_\text{in}\cup\mathcal{Y}_\text{C}=$[``frog'',``bird''$\ldots$ ``truck'',``bird'', ``tree'']. 
 Ideally, the softmax probability distribution over the concatenated labels would be $[0, 0.5, 0, \ldots, 0.5, 0]$ and by definition $S(\*x) \approx 0.5$. However, if we filter the generated labels to eliminate nouns with similar meanings as ID, our concatenated labels would be [``frog'',``bird''$\ldots$ ``truck'',``tree''] and the probability vector would be $[0,1,0, \ldots, 0]$, which leads to a much higher score $S(\*x) = 1$. In contrast, the generated labels for an OOD input with a caption ``man in the desert'' would be [``man'', ``desert'']. The resulting probability vector would be $[0,0,0, \ldots,1, 0]$  and the score $S(\*x) = 0$. Therefore, filtering makes it easier to separate ID inputs from OOD inputs (\cf Figure~\ref{fig:effects}). 
\paragraph{String-based filtering.} To implement the idea of filtering, we need a measurement of the similarity between the generated labels and ID labels. The simplest way is string-based filtering where a generated label is filtered if it matches any ID labels (in the string format), as in the case above. 


\begin{figure*}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/effects_filter.pdf}
\caption{Score distributions for ImageNet-10 (ID) and iNaturalist (OOD) inputs.  Simple string-based filtering alleviates the overlap between OOD inputs and ID inputs especially with scores around 0.5 (yellow rectangle), resulting in better ID-OOD separability.}
\label{fig:effects}
\end{figure*}









\section{Alternative Scoring Functions}
 
We explore the effectiveness of several alternative scoring functions:
\begin{itemize}
    \item Entropy: the (negative) entropy of softmax scaled cosine similarities denoted as $ S_{\text{entropy}}$;
    \item Var: the variance of the cosine similarities denoted as $ S_{\text{var}}$;
    \item Scaled: the scaled difference between the largest and second-largest cosine similarities $ S_{\text{scaled}}:= e^{s_{\hat y}(\*x) - s_{\hat y_2}(\*x)} $ where $\hat y:= \text{argmax}_{i\in[K]} s_i(\*x)$ and $\hat y_2:= \text{argmax}_{i\neq \hat y, i\in[K]} s_i(\*x)$.
\end{itemize}

As shown in Table~\ref{tab:others},  MCM still gives the most promising results compared to the other three alternative scores across most OOD test sets. 

\begin{table}[tbh]
\caption{Comparison with other scaling functions (applied to inner products) on the large-scale benchmark ImageNet-1k (ID). We use CLIP-B/16 as the backbone.}
\label{tab:others}
\centering
\resizebox{0.95\textwidth}{!}{{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{8}{c}{\textbf{OOD Dataset}}                        & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Average} }} \\
                        & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Texture} & \multicolumn{2}{c}{}             \\
                        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
                        & \textbf{FPR$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR$\downarrow$}           & \textbf{AUROC$\uparrow$}         & \textbf{FPR$\downarrow$}          & \textbf{AUROC$\uparrow$}        & \textbf{FPR$\downarrow$}         & \textbf{AUROC$\uparrow$}      & \textbf{FPR$\downarrow$}          & \textbf{AUROC$\uparrow$}        \\  
                        \midrule

Entropy
&84.44&	63.50&	93.79&	62.54&	94.10&	64.15&	97.16&	58.98&	92.37	&62.29\\
Var 
&87.42&	63.87&	68.71&	81.02&	76.28	&75.38&	80.04&	71.90&	78.11&	73.04\\
Scaled
&89.06&	72.26&	89.06&	70.81&	89.08&	69.66&	89.56&	68.17&	89.19&	70.22\\
MCM 
& 30.91	& 94.61	& 37.59& 	92.57& 	44.69& 	89.77& 	57.77& 	86.11& 	42.74& 	90.77\\


\bottomrule
\end{tabular}}
}
\end{table}

\end{document}