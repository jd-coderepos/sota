\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{multirow}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{1012} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mname}{AgentFormer}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{\mname: Agent-Aware Transformers for\\Socio-Temporal Multi-Agent Forecasting}

\author{
Ye Yuan\textsuperscript{1} \qquad Xinshuo Weng\textsuperscript{1} \qquad Yanglan Ou\textsuperscript{2} \qquad Kris Kitani\textsuperscript{1}\1mm]
{ \url{https://www.ye-yuan.com/agentformer}} \\
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
\vspace{-3mm}
    Predicting accurate future trajectories of multiple agents is essential for autonomous systems but is challenging due to the complex interaction between agents and the uncertainty in each agent's future behavior. Forecasting multi-agent trajectories requires modeling two key dimensions:
    (1)~\textbf{time dimension}, where we model the influence of past agent states over future states; (2)~\textbf{social dimension}, where we model how the state of each agent affects others.
    Most prior methods model these two dimensions separately, e.g., first using a temporal model to summarize features over time for each agent independently and then modeling the interaction of the summarized features with a social model. This approach is suboptimal since independent feature encoding over either the time or social dimension can result in a loss of information. Instead, we would prefer a method that allows an agent's state at one time to \textbf{directly} affect another agent's state at a future time.
    To this end, we propose a new Transformer, termed \mname, that simultaneously models the time and social dimensions. The model leverages a sequence representation of multi-agent trajectories by flattening trajectory features across time and agents. Since standard attention operations disregard the agent identity of each element in the sequence, \mname\ uses a novel agent-aware attention mechanism that preserves agent identities by attending to elements of the same agent differently than elements of other agents. Based on \mname, we propose a stochastic multi-agent trajectory prediction model that can attend to features of any agent at any previous timestep when inferring an agent's future position. The latent intent of all agents is also jointly modeled, allowing the stochasticity in one agent's behavior to affect other agents. Extensive experiments show that our method substantially improves the state of the art on well-established pedestrian and autonomous driving datasets.
\end{abstract}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/teaser.pdf}
    \caption{Different from standard approaches that model multi-agent trajectories in the time and social dimensions separately, our \mname\ allows for joint modeling of the time and social dimensions while preserving time and agent information. }
    \label{fig:teaser}
    \vspace{-4mm}
\end{figure}

\vspace{-5mm}
\section{Introduction}
\vspace{-1mm}
\label{sec:intro}
The safe planning of autonomous systems such as self-driving vehicles requires forecasting accurate future trajectories of surrounding agents (\eg, pedestrians, vehicles). However, multi-agent trajectory forecasting is challenging since the social interaction between agents, \ie, behavioral influence of an agent on others, is a complex process. The problem is further complicated by the uncertainty of each agent's future behavior, \ie, each agent has its latent intent unobserved by the system (\eg, turning left or right) that governs its future trajectory and in turn affects other agents. Therefore, a good multi-agent trajectory forecasting method should effectively model (1) the complex social interaction between agents and (2) the latent intent of each agent's future behavior and its social influence on other agents. 

Multi-agent social interaction modeling involves two key dimensions as illustrated in Fig.~\ref{fig:teaser}\,(Top): (1)~\textbf{\emph{time dimension}}, where we model how past agent states (positions and velocities) influence future agent states; (2)~\textbf{\emph{social dimension}}, where we model how each agent's state affects the state of other agents. Most prior multi-agent trajectory forecasting methods model these two dimensions separately (see Fig.~\ref{fig:teaser}\,(Middle)). Approaches like~\cite{kosaraju2019social,alahi2016social,gupta2018social} first use temporal models (\eg, LSTMs~\cite{hochreiter1997long} or Transformers~\cite{vaswani2017attention}) to summarize trajectory features over time for each agent independently and then input the summarized temporal features to social models (\eg, graph neural networks~\cite{kipf2016semi}) to capture social interaction between agents. Alternatively, methods like~\cite{salzmann2020trajectron++,huang2019stgat} first use social models to produce social features for each agent at each independent timestep and then apply temporal models over the social features. In this work, we argue that modeling the time and social dimensions separately can be suboptimal since the independent feature encoding over either the time or social dimension is not informed by features across the other dimension, and the encoded features may not contain the necessary information for modeling the other dimension.

To tackle this problem, we propose a new Transformer model, termed \mname, that simultaneously learns representations from both the time and social dimensions. \mbox{\mname}\ allows an agent's state at one time to affect another agent's state at a future time \emph{directly} instead of through intermediate features encoded over one dimension. As Transformers require sequences as input, we leverage a sequence representation of multi-agent trajectories by flattening trajectory features across time and agents (see Fig.~\ref{fig:teaser}\,(Bottom)). However, directly applying standard Transformers to these multi-agent sequences will result in a loss of \emph{time} and \emph{agent} information since standard attention operations discard the timestep and agent identity associated with each element in the sequence. We solve the loss of time information using a time encoder that appends a timestamp feature to each element. However, the loss of agent identity is a more complicated problem: unlike time, there is no innate ordering between agents, and assigning an agent index-based encoding will break the required permutation invariance of agents and create artificial dependencies on agent indices in the model. Instead, we propose a novel agent-aware attention mechanism to preserve agent information. Specifically, agent-aware attention generates two sets of keys and queries via different linear transformations; one set of keys and queries is used to compute inter-agent attention  (agent to agent) while the other set is designated for intra-agent attention (agent to itself). This design allows agent-aware attention to attend to elements of the same agent differently than elements of other agents, thus keeping the notion of agent identity. Agent-aware attention can be implemented efficiently via masked operations. Furthermore, \mname\ can also encode rule-based connectivity between agents (\eg, based on distance) by masking out the attention weights between unconnected agents.

Based on \mname, which allows us to model social interaction effectively, we propose a multi-agent trajectory prediction framework that also models the social influence of each agent's future trajectory on other agents. The probabilistic formulation of the model follows the conditional variational autoencoder (CVAE~\cite{kingma2013auto}) where we model the generative future trajectory distribution conditioned on context (\eg, past trajectories, semantic maps). We introduce a latent code for each agent to represent its latent intent. To model the social influence of each agent's future behavior (governed by latent intent) on other agents, the latent codes of all agents are jointly inferred from the future trajectories of all agents during training, and they are also jointly used by a trajectory decoder to output socially-aware multi-agent future trajectories. Thanks to \mname, the trajectory decoder can attend to features of any agent at any previous timestep when inferring an agent's future position. To improve the diversity of sampled trajectories and avoid similar samples caused by random sampling, we further adopt a multi-agent trajectory sampler that can generate diverse and plausible multi-agent trajectories by mapping context to various configurations of all agents' latent codes.

We evaluate our method on well-established pedestrian datasets, ETH~\cite{pellegrini2009you} and UCY~\cite{lerner2007crowds}, and an autonomous driving dataset, nuScenes~\cite{caesar2020nuscenes}. On ETH/UCY and nuScenes, we outperform state-of-the-art multi-agent prediction methods with substantial performance improvement. We further conduct extensive ablation studies to show the superiority of \mname\ over various combinations of social and temporal models. We also demonstrate the efficacy of agent-aware attention against agent encoding.

To summarize, the main contributions of this paper are:
(1) We propose a new Transformer that simultaneously models the time and social dimensions of multi-agent trajectories with a sequence representation. (2) We propose a novel agent-aware attention mechanism that preserves the agent identity of each element in the multi-agent trajectory sequence. (3) We present a multi-agent forecasting framework that models the latent intent of all agents jointly to produce socially-plausible future trajectories. (4) Our approach substantially improves the state of the art on well-established pedestrian and autonomous driving datasets.

\section{Related Work}

\noindent\textbf{Sequence Modeling.} Sequences are an important representation of data such as video, audio, price, \etc Historically, RNNs (\eg, LSTMs~\cite{hochreiter1997long}, GRUs~\cite{chung2014empirical}) have achieved remarkable success in sequence modeling, with applications to speech recognition~\cite{xiong2018microsoft,miao2015eesen}, image captioning~\cite{xu2015show}, machine translation~\cite{luong2015effective}, human pose estimation~\cite{yuan20183d,kocabas2020vibe}, \etc In particular, RNNs have been the preferred temporal models for trajectory and motion forecasting. Many RNN-based methods model the trajectory pattern of pedestrians to predict their 2D future locations~\cite{alahi2016social,ivanovic2019trajectron,zhang2019sr}. Prior work has also used RNNs to model the temporal dynamics of 3D human pose~\cite{fragkiadaki2015recurrent,yuan2019ego,yuan2020residual}. With the invention of Transformers and positional encoding~\cite{vaswani2017attention}, many works start to adopt Transformers for sequence modeling due to their strong ability to capture long-range dependencies. Transformers have first dominated the natural language processing (NLP) domain across various tasks~\cite{devlin2018bert,lan2019albert,yang2019xlnet}. Beyond NLP, numerous visual Transformers have been proposed to tackle vision tasks, such as image classification~\cite{dosovitskiy2020image}, object detection~\cite{carion2020end}, and instance segmentation~\cite{wang2020end}. Recently, Transformers have also been used for trajectory forecasting. Transformer-TF~\cite{giuliari2020transformer} applies the standard Transformer to predict the future trajectories of each agent independently. STAR~\cite{yu2020spatio} uses separate temporal and spatial Transformers to forecast multi-agent trajectories. Interaction Transformer~\cite{li2020end} combines RNNs and Transformers for multi-agent trajectory modeling.
Different from prior work, Our \mname\ leverages a sequence representation of multi-agent trajectories and a novel agent-aware attention mechanism to preserve time and agent information in the sequence.

\vspace{1mm}
\noindent\textbf{Trajectory Prediction.}
Early work on trajectory prediction adopts a deterministic approach using models such as social forces~\cite{helbing1995social}, Gaussian process (GP)~\cite{wang2007gaussian}, and RNNs~\cite{alahi2016social,morton2016analysis,vemula2018social}. A thorough review of these deterministic methods is provided in~\cite{rudenko2020human}. As the future trajectory of an agent is uncertain and often multi-modal, recent trajectory prediction methods start to model the trajectory distribution with deep generative models~\cite{kingma2013auto,goodfellow2014generative,rezende2015variational} such as conditional variational autoencoders (CVAEs)~\cite{lee2017desire,yuan2019diverse,ivanovic2019trajectron,tang2019multiple,weng2020joint,salzmann2020trajectron++}, generative adversarial networks (GANs)~\cite{gupta2018social,sadeghian2019sophie,kosaraju2019social,zhao2019multi}, and normalizing flows (NFs)~\cite{rhinehart2018r2p2,rhinehart2019precog,guan2020generative}. Most of these methods follow a seq2seq structure~\cite{bahdanau2014neural,cho2014learning} and predict future trajectories using intermediate features of past trajectories. In contrast, our \mname-based trajectory prediction framework can directly attend to features of any agent at any previous timestep when inferring an agent's future position. Moreover, our approach models the future trajectories of all agents jointly to predict socially-aware trajectories.


\vspace{1mm}
\noindent\textbf{Social Interaction Modeling.} Methods for social interaction modeling can be categorized based on how they model the time and social dimensions. While RNNs~\cite{hochreiter1997long,chung2014empirical} and Transformers~\cite{vaswani2017attention} are the prefered temporal models~\cite{huang2019stgat,alahi2016social,yu2020spatio}, graph neural networks (GNNs)~\cite{kipf2016semi,li2015gated} are often employed as the social models for interaction modeling~\cite{kipf2018neural,li2020evolvegraph,kosaraju2019social}. One popular type of methods~\cite{kosaraju2019social,alahi2016social,gupta2018social} first uses temporal models to summarize trajectory features over time for each agent independently and then feeds the temporal features to social models to obtain socially-aware agent features. Alternatively, approaches like~\cite{salzmann2020trajectron++,huang2019stgat} first use social models to produce social features of each agent at each independent timestep and then apply temporal models to summarize the social features over time for each agent. One common characteristic of these prior works is that they model the time and social dimensions on separate levels. This can be suboptimal since it prevents an agent's feature at one time from directly interacting with another agent's feature at a different time, thus limiting the model's ability to capture long-range dependencies. Instead, our method models both the time and social dimensions simultaneously, allowing direct feature interaction across time and agents.

\section{Approach}
\label{sec:approach}
We formulate multi-agent trajectory prediction as modeling the generative future trajectory distribution of  (variable) agents conditioned on their past trajectories. For observed timesteps , we represent the joint state of all  agents at time  as , where  is the state of agent  at time , which includes the position, velocity and (optional) heading angle of the agent. We denote the history of all agents as  which includes the joint agent state at all  observed timesteps. Similarly, the joint state of all  agents at future time  () is denoted as , where  is the future position of agent  at time . We denote the future trajectories of all  agents over  future timesteps as . Depending on the data, optional contextual information  may also be given, such as a semantic map around the agents (annotations of sidewalks, road boundaries, \etc). Our goal is to learn a generative model  where  are the model parameters.

In the following, we first introduce the proposed agent-aware Transformer, \mname, for joint modeling of socio-temporal relations. We then present a stochastic multi-agent trajectory prediction framework that jointly models the latent intent of all agents. 


\subsection{\mname: Agent-Aware Transformers}
\label{sec:agent_former}
Our agent-aware Transformer, \mname, is a model that learns representations from multi-agent trajectories over both time and social dimensions simultaneously, in contrast to standard approaches that model the two dimensions in separate stages. \mname\ has two types of modules -- encoders and decoders, which follow the encoder and decoder design of the original Transformer~\cite{vaswani2017attention} but with two major differences: (1) it replaces positional encoding with a time encoder; (2) it uses a novel agent-aware attention mechanism instead of the scaled dot-product attention. As we will discuss below, these two modifications are motivated by a sequence representation of multi-agent trajectories that is suitable for Transformers.

\vspace{2mm}
\noindent\textbf{Multi-Agent Trajectories as a Sequence.}
The past multi-agent trajectories  can be denoted as a sequence  of length . Similarly, the future multi-agent trajectories can also be represented as a sequence  of length . We adopt this sequence representation to be compatible with Transformers. At first glance, it may seem that we can directly apply standard Transformers to these sequences to model temporal and social relations. However, there are \emph{two problems} with this approach:
(1)~\textbf{loss of \emph{time} information}, as Transformers have no notion of time when computing attention for each element (\eg, ) \emph{w.r.t.} other elements in the sequence; for instance,  does not know  is a feature of the same timestep while  is a feature of the next timestep;
(2)~\textbf{loss of \emph{agent} information}, since Transformers do not consider agent identities when applying attention to each element, and elements of the same agent are not distinguished from elements of other agents; for example, when computing attention for , both  and  are treated the same, disregarding the fact that  is from the same agent while  is from a different agent. Below, we present the solutions to these two problems -- (1)~time encoder and (2) agent-aware attention.

\vspace{2mm}
\noindent\textbf{Time Encoder.} To inform \mname\ about the timestep associated with each element in the trajectory sequence, we employ a time encoder similar to the positional encoding in the original Transformer. Instead of encoding the position of each element based on its index in the sequence, we compute a timestamp feature based on the timestep  of the element. The timestamp uses the same sinusoidal design as the positional encoding. Let us take the past trajectory sequence  as an example. For each element , the timestamp feature  is defined as

where  denotes the -th feature of  and  is the feature dimension of the timestamp. The time encoder outputs a timestamped sequence  and each element  in  is computed as  where  and  are weight matrices and  denotes concatenation.

\vspace{2mm}
\noindent\textbf{Agent-Aware Attention.} To preserve agent information in the trajectory sequence, it may be tempting to employ a similar strategy to the time encoder, such as an agent encoder that assigns an agent index-based encoding to each element in the sequence. However, using such agent encoding is not effective as we will show in the experiments. The reason is that, different from time which is naturally ordered, there is no innate ordering between agents, and assigning encodings based on agent indices will break the required permutation invariance of agents and create artificial dependencies on agent indices in the model.

We tackle the loss of agent information from a different angle by proposing a novel agent-aware attention mechanism. The agent-aware attention takes as input keys , queries  and values , each of which uses the sequence representation of multi-agent trajectories. As an example, let the keys  and values  be the past trajectory sequence , and let the queries  be the future trajectory sequence . Recall that  is of length  as  contains the trajectory features of  agents of  past timesteps;  is of length  containing trajectory features of  future timesteps. The output of agent-aware attention is computed as

\vspace{-3mm}
\begin{small}

\vspace{-3mm}
\end{small}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/attention.pdf}
    \caption{\textbf{Illustration of agent-aware attention.} The mask  allows the attention weights in  to be computed differently based on whether the -th query and -th key belong to the same agent. }
    \label{fig:attention}
    \vspace{-2mm}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/overview.pdf}
    \caption{\textbf{Overview of our \mname-based multi-agent trajectory prediction framework.}}
    \label{fig:overview}
    \vspace{-2mm}
\end{figure*}

\noindent where  denotes element-wise product and we use two sets of projections  and  to generate projected keys  and queries  with key (query) dimension . Each element  in the attention weight matrix  represents the attention weight between the -th query  and \mbox{-th} key . As illustrated in Fig.~\ref{fig:attention}, when computing the attention weight matrix , we also use a mask  which is defined as

where  denotes each element inside the mask  and  denotes the indicator function. As  mod  computes the agent index of a query/key,  equals to one if the -th query  and -th key  belongs to the same agent, and  equals to zero otherwise, as shown in Fig.~\ref{fig:attention}. Using the mask , Eq.~\eqref{eq:attn} computes each element  of the attention weight matrix  differently based on the agreement of agent identity: If  and  have the same agent identity,  is computed using the projected queries  and keys  designated for intra-agent attention (agent to itself); If  and  have different agent identities,  is computed using the projected queries  and keys  designated for inter-agent attention (agent to other agents). In this way, the agent-aware attention learns to attend to elements of the same agent in the sequence differently than elements of other agents, thus preserving the notion of agent identity. Note that \mname\ only uses agent-aware attention to replace the scaled dot-product attention in the original Transformer and still allows multi-head attention to learn distributed representations.

\vspace{2mm}
\noindent\textbf{Encoding Agent Connectivity.}
\mname\ can also encode rule-based agent connectivity information by masking out the attention weights between unconnected agents. Specifically, we define that two agents  and  are connected if their distance  at the current time () is smaller than a threshold . If agents  and  are not connected, we set the attention weight  between any query  of agent  and any key  of agent .

\subsection{Multi-Agent Prediction with \mname}
\label{sec:cvae}
Having introduced \mname\ for modeling temporal and social relations, we are now ready to apply it in our multi-agent trajectory prediction framework based on CVAEs. As discussed at the start of Sec.~\ref{sec:approach}, the goal of multi-agent trajectory prediction is to model the future trajectory distribution  conditioned on past trajectories  and contextual information . To account for stochasticity and multi-modality in each agent's future behavior, we introduce latent variables  where  represents the latent intent of agent . We can then rewrite the future trajectory distribution as
\vspace{-1mm}

where  is a conditional Gaussian prior factorized over agents and  is a conditional likelihood model. To tackle the intractable integral in Eq.~\eqref{eq:cvae}, we use the negative evidence lower bound (ELBO)  in the CVAE as our loss function:

where  is an approximate posterior distribution factorized over agents and parametrized by . In our probabilistic formulation, the latent codes  of all agents in the posterior  are jointly inferred from the future trajectories  of all agents; similarly, the future trajectories  in the conditional likelihood  are modeled using the latent codes~ of all agents. This design allows each agent's latent intent represented by  to affect not just its own future trajectory but also the future trajectories of other agents, which enables us to generate socially-aware multi-agent trajectories. Having described the probabilistic formulation, we now introduce the detailed model architecture as outlined in Fig.~\ref{fig:overview}.

\vspace{2mm}
\noindent\textbf{Encoding Context (Semantic Map).}
As aforementioned, our model can optionally take as input contextual information  if provided by the data. Here, we assume  is a semantic map around the agents at the current timestep () with annotated semantic information (\eg, sidewalks, crosswalks, and road boundaries). For each agent , we rotate  to align with the agent's heading angle and crop an image patch  around the agent. We use a hand-designed convolutional neural network (CNN) to extract visual features  from , which will later be used by other modules in the model.

\vspace{2mm}
\noindent\textbf{CVAE Past Encoder.}
The past encoder starts with the multi-agent past trajectory sequence . If the semantic map  is provided, the past encoder concatenates each element  with the corresponding visual feature  of agent . The new sequence is then fed into the time encoder to obtain a timestamped sequence, which is then input to the \mname\ encoder as keys, queries, and values. The output of the encoder is a past feature sequence  that summarizes the past agent trajectories  and context .

\vspace{2mm}
\noindent\textbf{CVAE Prior.}
The prior module first performs an agent-wise pooling that computes a mean agent feature  from the past features across timesteps: . We then use a multilayer perceptron (MLP) to map  to the Gaussian parameters  of the prior distribution .

\vspace{2mm}
\noindent\textbf{CVAE Future Encoder.}
Given the multi-agent future trajectory sequence , similar to the past encoder, the future encoder appends visual features from the semantic map  to  and feeds the resulting sequence to the time encoder to produce a timestamped sequence. The timestamped sequence is then input as queries to the \mname\ decoder along with the past feature sequence  which serves as both keys and values. We use the \mname\ decoder here because it allows the feature extraction of  to condition on  through , thus effectively modeling the -conditioning in the posterior . We then perform an agent-wise mean pooling across timesteps on the output sequence of the \mname\ decoder to extract a feature for each agent. Each agent feature is then input to an MLP to obtain the Gaussian parameters  of the approximate posterior distribution .

\vspace{2mm}
\noindent\textbf{CVAE Future Decoder.}
Unlike the original Transformer decoder, our future trajectory decoder is autoregressive, which means it outputs trajectories one step at a time and feeds the currently generated trajectories back into the model to produce the trajectories of the next timestep. This design mitigates compounding errors during test time at the expense of training speed. Starting from an initial sequence  where  ( is the position feature inside ), the future decoder module maps an input sequence  to an output sequence  and grows the input sequence into . By autoregressively applying the decoder  times, we obtain the output sequence . Inside the future decoder module (Fig.~\ref{fig:overview}\,(Right)), we first form a feature sequence  where , thus concatenating the currently generated trajectories with the corresponding latent codes. The latent codes are sampled from the approximate posterior during training but from the trajectory sampler (as discussed below) at test time. The feature sequence  is then concatenated with the semantic map features and timestamped before being input as queries to the \mname\ decoder alongside the past feature sequence  which serves as keys and values. The \mname\ decoder enables the future trajectories to directly attend to features of any agent at any previous timestep (\eg,  or ), allowing the model to effectively infer future trajectories based on the whole agent history. We use proper masking inside the \mname\ decoder to enforce causality of the decoder output sequence. Each element of the output sequence is then passed through an MLP to generate the decoded future agent position . As we use a Gaussian to model the conditional likelihood , where  is the identity matrix and  is a weighting factor, the first term in Eq.~\eqref{eq:elbo} equals the mean squred error (MSE): .

\vspace{2mm}
\noindent\textbf{Trajectory Sampler.}
We adapt a diversity sampling technique, DLow~\cite{yuan2020dlow}, to our multi-agent trajectory prediction setting and employ a trajectory sampler to produce diverse and plausible trajectories once our CVAE model is trained. The trajectory sampler generates  sets of latent codes  where each set  contains the latent codes of all agents and can be decoded by the CVAE decoder into a multi-agent future trajectory sample . Each latent code  is generated by a linear transformation of a Gaussian noise :

where  is a non-singular matrix and  is a vector. Eq.~\eqref{eq:samp} induces a Gaussian sampling distribution  over . The distribution is conditioned on  and  because its inner parameters  are generated by the trajectory sampler module (Fig.~\ref{fig:overview}) through agent-wise pooling of the past feature sequence  and an MLP. The trajectory sampler loss is defined as

\vspace{-3mm}
\begin{small}

\vspace{-2mm}
\end{small}

\noindent where  is a scaling factor. The first term encourages the future trajectory samples  to cover the ground truth . The second KL term encourages each latent code  to follow the prior and be plausible; the KL can be computed analytically as both distributions inside are Gaussians. The third term encourages diversity among the future trajectory samples  by penalizing small pairwise distance. When training the trajectory sampler with Eq.~\eqref{eq:samp_loss}, we freeze the weights of the CVAE modules. At test time, we sample latent codes  using the trajectory sampler instead of sampling from the CVAE prior and decode the latent codes into trajectory samples .

\section{Experiments}
\label{sec:exp}

\noindent\textbf{Datasets.}
We evaluate our method on well-established public datasets: the ETH~\cite{pellegrini2009you}, UCY~\cite{lerner2007crowds}, and nuScenes~\cite{caesar2020nuscenes} datasets. The ETH/UCY datasets are the major benchmark for pedestrian trajectory prediction. There are five datasets in ETH/UCY, each of which contains pedestrian trajectories captured at 2.5Hz in multi-agent social scenarios with rich interaction. nuScenes is a recent large-scale autonomous driving dataset, which consists of 1000 driving scenes with each scene annotated at 2Hz. nuScenes also provides HD semantic maps with 11 semantic classes.

\vspace{2mm}
\noindent\textbf{Metrics.}
We report the minimum average displacement error  and final displacement error  of  trajectory samples of each agent compared to the ground truth: , where   denotes the future position of agent  at time  in the -th sample and  is the corresponding ground truth.  and  are the standard metrics for trajectory prediction~\cite{gupta2018social,sadeghian2019sophie,salzmann2020trajectron++,phan2020covernet,chai2020multipath}.


\vspace{2mm}
\noindent\textbf{Evaluation Protocol.}
For the ETH/UCY datasets, we adopt a leave-one-out strategy for evaluation, following prior work~\cite{gupta2018social,sadeghian2019sophie,salzmann2020trajectron++,mangalam2020not,yu2020spatio}. We forecast 2D future trajectories of 12 timesteps (4.8s) based on observed trajectories of 8 timesteps (3.2s). Similar to most prior works, we do not use any semantic/visual information for ETH/UCY for fair comparisons. All metrics are computed with  samples. For the nuScenes dataset, following prior work~\cite{phan2020covernet,chai2020multipath,cui2019multimodal,ma2020diverse}, we use the vehicle-only train-val-test split provided by the nuScenes prediction challenge and predict 2D future trajectories of 12 timesteps (6s) based on observed trajectories of 4 timesteps (2s). We report results with metrics computed using  samples.

\vspace{2mm}
\noindent\textbf{Implementation Details.}
For all datasets, we represent trajectories in a scene-centered coordinate where the origin is the mean position of all agents at . The future decoder in Fig.~\ref{fig:overview} outputs the offset to the agent's current position , so  is added to obtain  for each element in the output sequence. Following prior work~\cite{salzmann2020trajectron++,yu2020spatio}, random rotation of the scene is adopted for data augment.
Our multi-agent prediction model (Fig.~\ref{fig:overview}) uses two stacks (defined in \cite{vaswani2017attention}) of identical layers in each \mname\ encoder/decoder with 0.1 dropout rate. The dimensions  of keys, queries, and timestamps in \mname\ are all set to 256, and the hidden dimension of feedforward layers is 512. The number of heads for multi-head agent-aware attention is 8. All MLPs in the model have hidden dimensions (512, 256). For the CVAE, the latent code dimension  is 32, the coefficient  of the MSE loss equals 1, and we clip the maximum value of the KL term in  (Eq.~\eqref{eq:elbo}) down to 2. We also use the variety loss in SGAN~\cite{gupta2018social} in addition to . The agent connectivity threshold  is set to 100. We train the CVAE model using the Adam optimizer~\cite{kingma2014adam} for 100 epochs on ETH/UCY and nuScenes. We use an initial learning rate of  and halve the learning rate every 10 epochs. More details including the CNN for encoding semantic maps and the training procedure of the trajectory sampler can be found in Appendix~\ref{sec:supp_details}.

\setlength{\tabcolsep}{3pt}
\begin{table}[t]
\footnotesize
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{\hskip 1mm}l@{\hskip 1mm}|ccccc|@{\hskip 1mm}c@{\hskip 1mm}}
\toprule
\multicolumn{1}{c|}{\multirow{3}{*}[2pt]{Method}} & \multicolumn{6}{c}{ (m),  Samples} \\
\cmidrule(l{0.8mm}r{0.5mm}){2-7}
 & ETH & Hotel & Univ &  Zara1 & Zara2 & Average\\ \midrule SGAN~\cite{gupta2018social} & 0.81/1.52 & 0.72/1.61 & 0.60/1.26 & 0.34/0.69 & 0.42/0.84 & 0.58/1.18 \\
SoPhie~\cite{sadeghian2019sophie} & 0.70/1.43 & 0.76/1.67 & 0.54/1.24 & 0.30/0.63 & 0.38/0.78 & 0.54/1.15 \\
Transformer-TF~\cite{giuliari2020transformer} & 0.61/1.12 & 0.18/0.30 & 0.35/0.65 & 0.22/0.38 & 0.17/0.32 & 0.31/0.55 \\
STAR~\cite{yu2020spatio} & 0.36/0.65 & 0.17/0.36 & 0.31/0.62 & 0.26/0.55 & 0.22/0.46 & 0.26/0.53 \\
PECNet~\cite{mangalam2020not} & 0.54/0.87 & 0.18/0.24 & 0.35/0.60 & 0.22/0.39 & 0.17/0.30 & 0.29/0.48 \\
Trajectron++~\cite{salzmann2020trajectron++} & \textbf{0.39}/0.83 & \textbf{0.12}/\textbf{0.21} & \textbf{0.20}/\textbf{0.44} & \textbf{0.15}/0.33 & \textbf{0.11}/0.25 & \textbf{0.19}/0.41 \\
Ours (\mname) & 0.45/\textbf{0.75} & 0.14/0.22 & 0.25/0.45 & 0.18/\textbf{0.30} & 0.14/\textbf{0.24} & 0.23/\textbf{0.39} \\
\bottomrule
\end{tabular}
}
\vspace{-1mm}
\caption{\textbf{Baseline comparisons} on the ETH/UCY datasets.}
\label{table:eth}
\vspace{-2.5mm}
\end{table} \setlength{\tabcolsep}{4pt}
\begin{table}[t]
\footnotesize
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{\hskip 1mm}l@{\hskip 1mm}|cccc}
\toprule
\multicolumn{1}{c|}{\multirow{3}{*}[2pt]{Method}} & \multicolumn{2}{c}{ Samples} & \multicolumn{2}{c}{ Samples} \\
\cmidrule(l{0.8mm}r{0.8mm}){2-3}
\cmidrule(l{0.8mm}r{0.8mm}){4-5}
 &  &  &  &  \\ \midrule
MTP~\cite{cui2019multimodal} & 2.93 & - & 2.93 & - \\
MultiPath~\cite{chai2020multipath} & 2.32 & - & 1.96 & - \\
CoverNet~\cite{phan2020covernet} & 1.96 & - & 1.48 & - \\
DSF-AF~\cite{ma2020diverse} & 2.06 & 4.67 & 1.66 & 3.71 \\
DLow-AF~\cite{yuan2020dlow} & 2.11 & 4.70 & 1.78 & 3.58 \\
Trajectron++~\cite{salzmann2020trajectron++} & 1.88 & - & 1.51 & - \\
Ours (\mname) & \textbf{1.86} & \textbf{3.89} & \textbf{1.45} & \textbf{2.86}\\
\bottomrule
\end{tabular}
}
\vspace{-.5mm}
\caption{\textbf{Baseline comparisons} on the nuScenes dataset.}
\label{table:nuscene}
\vspace{-3.5mm}
\end{table} 
\subsection{Results}
\noindent\textbf{Baseline Comparisons.}
On the ETH/UCY datasets, we compare our approach with current state-of-the-art methods -- Trajectron++~\cite{salzmann2020trajectron++}, PECNet~\cite{mangalam2020not}, STAR~\cite{yu2020spatio}, and Transformer-TF~\cite{giuliari2020transformer} -- as well as common baselines -- SGAN~\cite{gupta2018social} and Sophie~\cite{sadeghian2019sophie}. The performance of all methods is summarized in Table~\ref{table:eth}, where we use officially-reported results for the baselines. We can observe that our \mname\ achieves very competitive performance and attains the best FDE. Particularly, our approach significantly outperforms prior Transformer-based methods, Transformer-TF~\cite{giuliari2020transformer} and STAR~\cite{yu2020spatio}. As FDE measures the final displacement error of predicted trajectories, it places more emphasis on a method's ability to predict distant futures than ADE. We believe the strong performance of our method in FDE can be attributed to the design of \mname, which can model long-range trajectory dependencies effectively by directly attending to features of any agent at any previous timestep when inferring an agent's future position.

Compared to ETH/UCY, the trajectories in nuScenes are much longer as we evaluate with a longer time horizon (6s) and vehicles are much faster than pedestrians. Thus, nuScenes presents a different challenge for multi-agent prediction methods. On the nuScenes dataset, we evaluate our approach against state-of-the-art vehicle prediction methods -- Trajectron++~\cite{salzmann2020trajectron++}, MTP~\cite{cui2019multimodal}, MultiPath~\cite{chai2020multipath}, CoverNet~\cite{phan2020covernet}, DSF-AF~\cite{ma2020diverse}, and DLow-AF~\cite{yuan2020dlow}. We report the performance of all methods in Table~\ref{table:nuscene}, where the results of Trajectron++ are taken from the nuScenes prediction challenge leaderboard, the performance of DLow-AF is from ~\cite{ma2020diverse}, and we also use the officially-reported results for the other baselines. The FDE of some baselines is not available since the number has not been reported. We can see that our approach, \mname, outperforms the baselines, especially the strong model Trajectron++~\cite{salzmann2020trajectron++}, consistently in ADE and FDE for both 5 and 10 sample settings.

\setlength{\tabcolsep}{3pt}
\begin{table}[t]
\footnotesize
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{@{\hskip 1mm}cc|ccccc|@{\hskip 1mm}c@{\hskip 1mm}}
\toprule
\multicolumn{2}{c|}{Model} & \multicolumn{6}{c}{ (m),  Samples} \\
\cmidrule(l{0mm}r{0.5mm}){1-2}\cmidrule(l{0.5mm}r{1mm}){3-8}
 \hspace{1.5mm} Social \hspace{1.5mm} & Temporal & ETH & Hotel & Univ &  Zara1 & Zara2 & Average\\ \midrule GCN & LSTM & 0.57/0.90 & 0.20/0.34 & 0.29/0.52 & 0.24/0.44 & 0.23/0.42 & 0.31/0.52 \\
GCN & TF & 0.56/0.93 & 0.15/0.28 & 0.28/0.51 & 0.24/0.45 & 0.19/0.35 & 0.28/0.50 \\
TF & LSTM & 0.55/0.91 & 0.18/0.31 & 0.28/0.50 & 0.24/0.44 & 0.21/0.39 & 0.29/0.51 \\
TF & TF & 0.50/0.82 & 0.15/0.27 & 0.28/0.52 & 0.22/0.42 & 0.16/0.31 & 0.26/0.47 \\
\midrule
\multicolumn{2}{c|}{Joint Socio-Temporal} & ETH & Hotel & Univ &  Zara1 & Zara2 & Average\\ \midrule \multicolumn{2}{l|}{Ours w/o joint latent}  & 0.49/0.77 & 0.15/0.25 & 0.29/0.52 & 0.22/0.41 & 0.18/0.33 & 0.27/0.46 \\
\multicolumn{2}{l|}{Ours w/o AA attention} & 0.49/0.80 & 0.15/0.25 & 0.31/0.54 & 0.23/0.41 & 0.19/0.34 & 0.27/0.47 \\
\multicolumn{2}{l|}{Ours w/ agent encoding} & 0.48/0.78 & \textbf{0.14}/0.23 & 0.32/0.55 & 0.22/0.40 & 0.19/0.34 & 0.27/0.46 \\
\multicolumn{2}{l|}{Ours (\mname)} & \textbf{0.45}/\textbf{0.75} & \textbf{0.14}/\textbf{0.22} & \textbf{0.25}/\textbf{0.45} & \textbf{0.18}/\textbf{0.30} & \textbf{0.14}/\textbf{0.24} & \textbf{0.23}/\textbf{0.39} \\
\bottomrule
\end{tabular}
}
\vspace{-0.5mm}
\caption{\textbf{Ablation studies} on the ETH/UCY datasets. ``TF'' means Transformer and ``AA Attention'' denotes agent-aware attention.}
\label{table:eth_abl}
\vspace{-2mm}
\end{table} \setlength{\tabcolsep}{4pt}
\begin{table}[t]
\footnotesize
\centering
\begin{tabular}{@{\hskip 1mm}cc|cccc}
\toprule
\multicolumn{2}{c|}{Model} & \multicolumn{2}{c}{ Samples} & \multicolumn{2}{c}{ Samples}\\
\cmidrule(l{0mm}r{0.5mm}){1-2}
\cmidrule(l{0.5mm}r{0.8mm}){3-4}
\cmidrule(l{0.8mm}r{0.8mm}){5-6}
 \hspace{1.5mm} Social \hspace{1.5mm} & Temporal &  &  &  & \\ \midrule GCN & LSTM & 2.17 & 4.42 & 1.57 & 3.09 \\
GCN & TF & 2.03 & 4.36 & 1.52 & 2.95 \\
TF & LSTM & 2.12 & 4.48 & 1.69 & 3.31 \\
TF & TF & 1.99 & 4.12 & 1.54 & 3.07 \\
\midrule
\multicolumn{2}{c|}{Joint Socio-Temporal} &   &  &  & \\ \midrule \multicolumn{2}{l|}{Ours w/o semantic map}  & 1.97 & 4.21 & 1.58 & 3.14 \\
\multicolumn{2}{l|}{Ours w/o joint latent}  & 1.95 & 3.98 & 1.50 & 2.92 \\
\multicolumn{2}{l|}{Ours w/o AA attention} & 2.02 & 4.29 & 1.55 & 2.91 \\
\multicolumn{2}{l|}{Ours w/ agent encoding} & 2.01 & 4.28 & 1.63 & 3.11 \\
\multicolumn{2}{l|}{Ours (\mname)} & \textbf{1.86} & \textbf{3.89} & \textbf{1.45} & \textbf{2.86}\\
\bottomrule
\end{tabular}
\vspace{1.5mm}
\caption{\textbf{Ablation studies} on the nuScenes dataset. ``TF'' means Transformer and ``AA Attention'' denotes agent-aware attention.}
\label{table:nuscenes_abl}
\vspace{-3mm}
\end{table} 
\vspace{2mm}
\noindent\textbf{Ablation Studies.}
We further perform extensive ablation studies on ETH/UCY and nuScenes to investigate the contribution of key technical components in our method. The first ablation study explores variants of our method that use separate social and temporal models to replace our joint socio-temporal model, \mname, in our multi-agent prediction framework. We choose GCN~\cite{kipf2016semi} or Transformer (TF) as the social model, and LSTM or Transformer as the temporal model. In total, there are 4 () combinations of social and temporal models. The ablation results are summarized in the first group of Table~\ref{table:eth_abl} and~\ref{table:nuscenes_abl}. It is evident that all combinations of separate social and temporal models lead to inferior performance compared to our method which models the social and temporal dimensions jointly. 

The second ablation study investigates the role of (1) joint latent intent modeling, (2) agent-aware attention, and (3) semantic maps, and we denote the corresponding variants as ``w/o joint latent'', ``w/o AA attention'', and ``w/o semantic map''. We further test a variant ``w/ agent encoding'' where we replace agent-aware attention with agent encoding. The results are reported in the second group of Table~\ref{table:eth_abl} and~\ref{table:nuscenes_abl}. We can see that all variants lead to considerably worse performance compared to our full method. In particular, the variants ``w/o AA attention'' and ``w/ agent encoding'' result in pronounced performance drop, which indicates that agent-aware attention is essential in our method and alternatives like agent encoding are not effective.



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/traj_vis.pdf}
    \caption{\textbf{(a,c,d)} Three samples of forecasted multi-agent futures (green) via our method, which exhibit social behaviors like following (A3 \& A4) and collision avoidance (A1 \& A2 in (a), A2 \& A3 in (c)). \textbf{(b)} Attention visualization for sample 1. When predicting the target (red), the model pays more attention (darker color) to key timesteps (turning point) of adjacent agents and spreads out attention to the target's past timesteps to reason about dynamics.}
    \label{fig:vis}
    \vspace{-3mm}
\end{figure}

\vspace{2mm}
\noindent\textbf{Trajectory Visualization.}
Fig.~\ref{fig:vis}\,(a,c,d) shows three samples of forecasted multi-agent futures of the same scene via our method. We can see that the samples correspond to different modes of socially-aware and non-colliding trajectories, and exhibit behaviors like following (A3 \& A4) and collision avoidance (A1 \& A2 in (a), A2 \& A3 in (c)). Fig.~\ref{fig:vis}\,(b) visualizes the attention of sample 1 and shows that, when predicting the target (red), the model pays more attention to key timesteps (turning point) of adjacent agents and also spreads out attention to the target's past timesteps to reason about the dynamics and curvature of its trajectory. More attention visualization can be found in Appendix~\ref{sec:supp_vis_attn}.

\section{Conclusion}
In this paper, we proposed a new Transformer, \mname, that can simultaneously model the time and social dimensions of multi-agent trajectories using a sequence representation. To preserve agent identities in the sequence, we proposed a novel agent-aware attention mechanism that can attend to features of the same agent differently than features of other agents. Based on \mname, we presented a stochastic multi-agent trajectory prediction framework that jointly models the latent intent of all agents to produce diverse and socially-aware multi-agent future trajectories. Experiments demonstrated that our method substantially improved state-of-the-art performance on challenging pedestrian and autonomous driving datasets.

\vspace{2mm}
\noindent\textbf{Acknowledgments.} This work is supported by the Qualcomm Innovation Fellowship.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}

\onecolumn
\appendix

\section{Handling a Time-Varying Number of Agents}
For clarity and ease of exposition, we assume the number of agents remains the same across timesteps in the main paper. However, this assumption is not necessary, and our method can easily generalize to use cases where the number of agents changes over time due to agents going out of the scene or being missed by detection. We illustrate how to apply our method to such cases in Fig.~\ref{fig:var_agent}. Owning to the flexible sequence representation we employ for multi-agent trajectories, we can simply remove the features of missing agents at each timestep from the sequence. The reason why we do not need to fill the missing features is that our method uses time encoding to preserve time information, unlike RNNs which have to use recurrence to encode timesteps and thus necessitate the features of all timesteps. As the number of agents is no longer  for all timesteps, the computation of the mask  in agent-aware attention needs to be changed accordingly:

where Agent() extracts the agent index of a query/key and  denotes the indicator function. An example of mask  is shown in Fig.~\ref{fig:var_agent}\,(Right).

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/var_agent.pdf}
    \caption{Our method can naturally handle a time-varying number of agents because of the flexible sequence representation of multi-agent trajectories. We can simply remove the trajectory features of missing agents at each timestep from the sequence. The mask  of the example sequence (when applying self-attention) is computed based on the agreement of agent identity between each query and key.}
    \label{fig:var_agent}
    \vspace{-2mm}
\end{figure*}

\section{Additional Implementation Details}
\label{sec:supp_details}
\noindent\textbf{Encoding Semantic Maps.}
The semantic map  for each agent  has spatial dimensions  with 3 meters between adjacent pixels. It has  channels annotating drivable areas, road dividers, and lane dividers obtained using the official nuScenes software development kit. Since the semantic map is relatively easy to parse, we use a simple hand-designed CNN to extract visual features  from it. In particular, the CNN has four convolutional layers with channels (32, 32, 32, 1), kernel size (5, 5, 5, 3), and strides (2, 2, 1, 1). A final linear layer is used to obtain a 32-dimensional feature.


\vspace{2mm}
\noindent\textbf{Training Trajectory Sampler.}
The scaling factor  in the trajectory sampler loss  (Eq.~\eqref{eq:samp_loss} in the main paper) is set to 5 for ETH/UCY and 20 for nuScenes. We clip the maximum value of the KL term in  down to 2. We train the trajectory sampler using the Adam optimizer~\cite{kingma2014adam} for 50 epochs on ETH/UCY and nuScenes. We use an initial learning rate of  and halve the learning rate every 5 epochs.


\vspace{2mm}
\noindent\textbf{Ablation Study Details.}
We first provide details for the ablation study of separate social and temporal models (first group of Table~\ref{table:eth_abl} and \ref{table:nuscenes_abl} in the main paper). We first use a temporal model (LSTM or Transformer) to extract the temporal feature of each agent and then apply a social model (GCN~\cite{kipf2016semi} or Transformer) over the temporal features to obtain social features for each agent; final trajectories are decoded from the social features using either an LSTM or Transformer. For the GCN, we use two graph convolutional layers with channels (256, 256) and residual connections within each layer. The hidden dimensions of the LSTMs are set to 256. The Transformers have two layers with key/query dimensions 256 and 8 heads; the feedforward layer has 512 hidden units, and the dropout ratio is 0.1. We use the positional encoding~\cite{vaswani2017attention} for the temporal Transformer but not for the social Transformer as agents are permutation-invariant.

Next, we provide details for the ablation study of each key technical component (second group of Table~\ref{table:eth_abl} and \ref{table:nuscenes_abl} in the main paper). For the variant without joint latent modeling (``w/o joint latent''), we append the latent codes to the trajectory sequence after the \mname\ decoder instead of before the decoder. In this way, the latent code of one agent will not affect the future trajectory of another agent. For the variant without the agent-aware attention (``w/o AA attention''), we replace our agent-aware attention with standard scaled dot-product attention used in the original transformer~\cite{vaswani2017attention}. For the variant with agent encoding (``w/ agent encoding''), in addition to removing the agent aware attention, we also append an agent encoding to each element in the trajectory sequence. The agent encoding is computed similarly as the positional encoding~\cite{vaswani2017attention} but uses the agent index instead of the position index. For the variant without semantic maps (``w/o semantic map''), we simply do not append any visual features extracted from the semantic maps to the trajectory sequence.


\vspace{2mm}
\noindent\textbf{Other Details.}
Our models are implemented using PyTorch~\cite{paszke2019pytorch} and are trained with a single NVIDIA RTX 2080 Ti and standard CPUs. The training time is approximately one day for each dataset in ETH/UCY and three days for nuScenes.


\section{Additional Attention Visualization}
\label{sec:supp_vis_attn}
As discussed in the main paper, our method can attend to any agent at any previous timestep when predicting the future position of an agent. Here, we provide more visualization of the attention in Fig.~\ref{fig:vis_attn_supp} to understand the behavior of our model. Across all the examples, it is evident that when predicting the target future position of an agent, the model pays more attention to the agent's own trajectories and recent timesteps, and it also attends more to nearby agents than distant agents.

\begin{figure*}[h]
\vspace{-3mm}
    \centering
    \includegraphics[width=\linewidth]{figs/vis_attn_supp.pdf}
    \caption{\textbf{Attention Visualization on ETH/UCY.} We plot the attention to past (blue) and future (green) trajectory features of all agents when inferring a target position (red). Darker color means higher attention. When predicting the target future position of an agent, the model pays more attention to the agent's own trajectories and recent timesteps, and it also attends more to nearby agents than distant agents.}
    \label{fig:vis_attn_supp}
    \vspace{-3mm}
\end{figure*}


\section{Trajectory Sample Visualization}
 To demonstrate the importance of agent-aware attention, we also provide qualitative comparisons of our method against the variant without agent-aware attention (w/o AA attention) on the nuScenes dataset in Fig.~\ref{fig:vis_nus}. We can observe that the future trajectory samples produced by our method using agent-aware attention cover the ground truth (GT) future trajectories significantly better. Our method also produces much fewer implausible trajectories such as those going out of the road.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/vis_nus.pdf}
    \caption{\textbf{Trajectory Sample Visualization on nuScenes.} We compare our method against the variant without agent-aware attention (w/o AA attention). The future trajectory samples produced by our method using agent-aware attention cover the ground truth (GT) future trajectories significantly better. Our method also produces much fewer implausible trajectories such as those going out of the road.}
    \label{fig:vis_nus}
    \vspace{-2mm}
\end{figure*}


\end{document}
