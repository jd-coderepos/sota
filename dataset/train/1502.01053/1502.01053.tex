\documentclass[journal]{IEEEtran}
\usepackage{epsf, psfrag, amssymb, amsfonts, amsmath, cite,enumerate}
\usepackage{graphicx, subfigure, color,bbm}
\usepackage{boxedminipage}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{algorithm, algorithmic}
\usepackage{dblfloatfix}


\newtheorem{remark}{Remark}




\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{problem}{Problem}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newcommand{\matplottc}[1]{               \unitlength .45truein
        \begin{center}
        \begin{picture}(5,4.6)(1.05,.45)
        \special{psfile=#1.ps hscale=50 vscale=50}
        \end{picture}
        \end{center}
}
\newcommand{\matplotfp}[1]{               \unitlength .45truein
        \begin{center}
        \begin{picture}(14,16)(.8,-1)
        \special{psfile=#1.ps hscale=100 vscale=100}
        \end{picture}
        \end{center}
}
\newcommand{\matplothp}[1]{               \unitlength .45truein
        \begin{center}  \begin{picture}(14,7.5)(-1,.55)
        \special{psfile=#1.ps hscale=80 vscale=80}
        \end{picture}
        \end{center}
}
\newcommand{\matplothptop}[1]{               \unitlength .45truein                \begin{center}  \begin{picture}(14,6.3)(-1,1.15)
        \special{psfile=#1.ps hscale=80 vscale=80}
        \end{picture}
        \end{center}
}
\newcommand{\matplotcc}[4]{             \vspace{2.8in}
        \special{psfile=#1.ps voffset=-10 hoffset=-22 vscale=52 hscale=52}
        \special{psfile=/user/xu/isl/matlab/psfiles/#2.ps voffset=-10 hoffset=213 vscale=52 hscale=52}
        \hspace*{\fill}\hspace*{0.35in}#3\hspace*{\fill}\hspace*{\fill}\hspace*{
0.35in}#4\hspace*{\fill}
}
\newcommand{\matplottwo}[4]{             \vspace{2.62in}
        \special{psfile=#1.ps voffset=-10 hoffset=-27 vscale=52 hscale=52}
        \special{psfile=#2.ps voffset=-10 hoffset=218 vscale=52 hscale=52}
        \vspace*{0.18in}
        \hspace*{\fill}\hspace*{-.05in}#3\hspace*{\fill}\hspace*{\fill}\hspace*{
0.65in}#4\hspace*{\fill}
}
\def\psfancypar#1#2{\begingroup\def\par{\endgraf\endgroup\lineskiplimit=0pt}
               \setbox2=\hbox{\large\sc #2}
\newdimen\tmpht \tmpht \ht2 \advance\tmpht by \baselineskip
\font\hhuge=Times-Bold at \tmpht
               \setbox1=\hbox{{\hhuge #1}}
\count7=\tmpht \count8=\ht1
\divide\count8 by 1000 \divide\count7 by \count8
\tmpht=.001\tmpht\multiply\tmpht by \count7
\font\hhuge=Times-Bold at \tmpht
               \setbox1=\hbox{{\hhuge #1}}
               \noindent
                \hangindent1.05\wd1
               \hangafter=-2 {\hskip-\hangindent
               \lower1\ht1\hbox{\raise1.0\ht2\copy1}\kern-0\wd1}\copy2\lineskiplimit=-1000pt}


\newenvironment{ap}{
\renewcommand{\thesection}{\mbox{Appendix }\Alph{section}}
\renewcommand{\thesubsection}{\Alph{section}.\arabic{subsection
}}
\renewcommand{\theequation}{\mbox{\Alph{section}.}\arabic{equation}}
}{
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\theequation}{\arabic{equation}}
}
\def\thetabf{{\mbox{\boldmath\unboldmath}}}
\def\Upsilonbf{\hbox{\boldmath\unboldmath}}

\newcommand{\beq}{}
\newcommand{\bqa}{}
\newcommand{\bqn}{}
\newcommand{\nn}{\nonumber}
\newcommand{\cl}{\centerline}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\bd}{\begin{description}}
\newcommand{\ed}{\end{description}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bde}{\begin{definition}}
\newcommand{\ede}{\end{definition}}
\newcommand{\bex}{\begin{example}}
\newcommand{\eex}{\end{example}}

\newcommand{\Phibf}{\mbox{}}
\newcommand{\Gammabf}{\mbox{}}
\newcommand{\Psibf}{\mbox{}}

\newcommand{\thhat}{\mbox{}}
\newcommand{\thbar}{\mbox{}}
\newcommand{\etahat}{\mbox{}}
\newcommand{\etabf}{\mbox{}}
\newcommand{\e}{\mbox{}}
\newcommand{\ehat}{\mbox{}}
\newcommand{\EWE}{\mbox{}}
\newcommand{\DPD}{\mbox{}}
\newcommand{\EWEhat}{\mbox{}}
\newcommand{\E}{\mbox{{\rm E}}}
\newcommand{\Es}{\mbox{}}
\newcommand{\En}{\mbox{}}
\newcommand{\Eshat}{\mbox{}}
\newcommand{\Lshat}{\mbox{}}
\newcommand{\Ls}{\mbox{}}
\newcommand{\Ln}{\mbox{}}
\newcommand{\Ltilde}{\mbox{}}
\newcommand{\Enhat}{\mbox{}}
\newcommand{\Lnhat}{\mbox{}}
\newcommand{\Ai}{\mbox{}}
\newcommand{\abf}{\mbox{}}
\newcommand{\intinf}{\int_{-\infty}^{\infty}}



\def\boxit#1{\vbox{\hrule\hbox{\vrule\kern3pt
        \vbox{\kern3pt#1\kern3pt}\kern3pt\vrule}\hrule}}
\def\dag{{\cal y}}
\def\reals{ { {\rm  I \kern-0.15em R }  } }
\def\complex{ {\,{{\rm C} \kern-0.50em \raise0.20ex {  |}}\, }}
\def\alphabf{\hbox{\boldmath\unboldmath}}
\def\betabf{\hbox{\boldmath\unboldmath}}
\def\gammabf{\hbox{\boldmath\unboldmath}}
\def\deltabf{\hbox{\boldmath\unboldmath}}
\def\epsilonbf{\hbox{\boldmath\unboldmath}}
\def\zetabf{\hbox{\boldmath\unboldmath}}
\def\etabf{\hbox{\boldmath\unboldmath}}
\def\iotabf{\hbox{\boldmath\unboldmath}}
\def\kappabf{\hbox{\boldmath\unboldmath}}
\def\lambdabf{\hbox{\boldmath\unboldmath}}
\def\mubf{\hbox{\boldmath\unboldmath}}
\def\nubf{\hbox{\boldmath\unboldmath}}
\def\xibf{\hbox{\boldmath\unboldmath}}
\def\pibf{\hbox{\boldmath\unboldmath}}
\def\rhobf{\hbox{\boldmath\unboldmath}}
\def\sigmabf{\hbox{\boldmath\unboldmath}}
\def\taubf{\hbox{\boldmath\unboldmath}}
\def\upsilonbf{\hbox{\boldmath\unboldmath}}
\def\phibf{\hbox{\boldmath\unboldmath}}
\def\chibf{\hbox{\boldmath\unboldmath}}
\def\psibf{\hbox{\boldmath\unboldmath}}
\def\omegabf{\hbox{\boldmath\unboldmath}}
\def\Sigmabf{\hbox{}}
\def\Upsilonbf{\hbox{}}
\def\Omegabf{\hbox{}}
\def\Deltabf{\hbox{}}
\def\Gammabf{\hbox{}}
\def\Thetabf{\hbox{}}
\def\Lambdabf{\mbox{}}
\def\Sigmabf{\mbox{}}
\def\Xibf{\hbox{\bf}}
\def\Pibf{{\bf \Pi}}
\def\0bf{{\bf 0}}
\def\1bf{{\bf 1}}
\def\2bf{{\bf 2}}
\def\3bf{{\bf 3}}
\def\4bf{{\bf 4}}
\def\5bf{{\bf 5}}
\def\6bf{{\bf 6}}
\def\7bf{{\bf 7}}
\def\8bf{{\bf 8}}
\def\9bf{{\bf 9}}
\def\abf{{\bf a}}
\def\bbf{{\bf b}}
\def\cbf{{\bf c}}
\def\dbf{{\bf d}}
\def\ebf{{\bf e}}
\def\fbf{{\bf f}}
\def\gbf{{\bf g}}
\def\hbf{{\bf h}}
\def\ibf{{\bf i}}
\def\jbf{{\bf j}}
\def\kbf{{\bf k}}
\def\lbf{{\bf l}}
\def\mbf{{\bf m}}
\def\nbf{{\bf n}}
\def\obf{{\bf o}}
\def\pbf{{\bf p}}
\def\qbf{{\bf q}}
\def\rbf{{\bf r}}
\def\sbf{{\bf s}}
\def\tbf{{\bf t}}
\def\ubf{{\bf u}}
\def\vbf{{\bf v}}
\def\wbf{{\bf w}}
\def\xbf{{\bf x}}
\def\ybf{{\bf y}}
\def\zbf{{\bf z}}
\def\rbf{{\bf r}}
\def\xbf{{\bf x}}
\def\ybf{{\bf y}}
\def\Abf{{\bf A}}
\def\Bbf{{\bf B}}
\def\Cbf{{\bf C}}
\def\Dbf{{\bf D}}
\def\Ebf{{\bf E}}
\def\Fbf{{\bf F}}
\def\Gbf{{\bf G}}
\def\Hbf{{\bf H}}
\def\Ibf{{\bf I}}
\def\Jbf{{\bf J}}
\def\Kbf{{\bf K}}
\def\Lbf{{\bf L}}
\def\Mbf{{\bf M}}
\def\Nbf{{\bf N}}
\def\Obf{{\bf O}}
\def\Pbf{{\bf P}}
\def\Qbf{{\bf Q}}
\def\Rbf{{\bf R}}
\def\Sbf{{\bf S}}
\def\Tbf{{\bf T}}
\def\Ubf{{\bf U}}
\def\Vbf{{\bf V}}
\def\Wbf{{\bf W}}
\def\Xbf{{\bf X}}
\def\Ybf{{\bf Y}}
\def\Zbf{{\bf Z}}
\def\Ac{{\cal A}}
\def\Bc{{\cal B}}
\def\Cc{{\cal C}}
\def\Dc{{\cal D}}
\def\Ec{{\cal E}}
\def\Fc{{\cal F}}
\def\Gc{{\cal G}}
\def\Hc{{\cal H}}
\def\Ic{{\cal I}}
\def\Jc{{\cal J}}
\def\Kc{{\cal K}}
\def\Lc{{\cal L}}
\def\Mc{{\cal M}}
\def\Nc{{\cal N}}
\def\Oc{{\cal O}}
\def\Pc{{\cal P}}
\def\Qc{{\cal Q}}
\def\Rc{{\cal R}}
\def\Sc{{\cal S}}
\def\Tc{{\cal T}}
\def\Uc{{\cal U}}
\def\Vc{{\cal V}}
\def\Wc{{\cal W}}
\def\Xc{{\cal X}}
\def\Yc{{\cal Y}}
\def\Zc{{\cal Z}}

\def\amat{\mathcal{a}}
\def\bmat{\mathcal{b}}
\def\cmat{\mathcal{c}}
\def\dmat{\mathcal{d}}
\def\emat{\mathcal{e}}
\def\fmat{\mathcal{f}}
\def\gmat{\mathcal{g}}
\def\hmat{\mathcal{h}}
\def\imat{\mathcal{i}}
\def\jmat{\mathcal{j}}
\def\kmat{\mathcal{k}}
\def\lmat{\mathcal{l}}
\def\mmat{\mathcal{m}}
\def\nmat{\mathcal{n}}
\def\omat{\mathcal{o}}
\def\pmat{\mathcal{p}}
\def\qmat{\mathcal{q}}
\def\rmat{\mathcal{r}}
\def\smat{\mathcal{s}}
\def\tmat{\mathcal{t}}
\def\umat{\mathcal{u}}
\def\vmat{\mathcal{v}}
\def\wmat{\mathcal{w}}
\def\xmat{\mathcal{x}}
\def\tmat{\mathcal{y}}
\def\zmat{\mathcal{z}}

\def\Amat{\mathcal{A}}
\def\Bmat{\mathcal{B}}
\def\Cmat{\mathcal{C}}
\def\Dmat{\mathcal{D}}
\def\Emat{\mathcal{E}}
\def\Fmat{\mathcal{F}}
\def\Gmat{\mathcal{G}}
\def\Hmat{\mathcal{H}}
\def\Imat{\mathcal{I}}
\def\Jmat{\mathcal{J}}
\def\Kmat{\mathcal{K}}
\def\Lmat{\mathcal{L}}
\def\Mmat{\mathcal{M}}
\def\Nmat{\mathcal{N}}
\def\Omat{\mathcal{O}}
\def\Pmat{\mathcal{P}}
\def\Qmat{\mathcal{Q}}
\def\Rmat{\mathcal{R}}
\def\Smat{\mathcal{S}}
\def\Tmat{\mathcal{T}}
\def\Umat{\mathcal{U}}
\def\Vmat{\mathcal{V}}
\def\Wmat{\mathcal{W}}
\def\Xmat{\mathcal{X}}
\def\Ymat{\mathcal{Y}}
\def\Zmat{\mathcal{Z}}

\def\defeq{{\stackrel{\Delta}{=}}}
\newcommand{\x}{\mbox{}}
\newcommand{\s}{\mbox{}}
\newcommand{\n}{\mbox{}}
\newcommand{\jj}{\mbox{}}
\newcommand{\R}{\mbox{}}
\newcommand{\Rhat}{\mbox{}}
\newcommand{\A}{\mbox{}}
\newcommand{\D}{\mbox{}}
\newcommand{\M}{\mbox{{\bf M}}}
\newcommand{\Tr}{\mbox{Tr}}
\newcommand{\pbox}{\hfill }
\newcommand{\blankline}{\vskip\baselineskip\noindent}
\def\Rxx{\Rbf_{\ssstyle X\kern-.1em X}}
\def\rxx{\hbox{}}
\def\ryy{\hbox{}}
\def\rxy{\hbox{}}\def\ryx{\hbox{}}
\def\rxyT{\hbox{}}\def\ryxT{\hbox{}}
\def\Rss{\Rbf_{\ssstyle SS}}
\def\rss{\mbox{}}
\def\Rnn{\Rbf_{\ssstyle NN}}
\def\rnn{\hbox{}}
\def\rxxinv{\hbox{}}
\def\Rssinv{\Rbf^{-1}_{\ssstyle SS}}
\def\rssinv{\hbox{}}
\def\rzzinv{\Rbf^{-1}_{\ssstyle ZZ}}\let\rzzi=\rzzinv
\def\hatrxx{\widehat{\Rbf}_{\ssstyle XX}}
\def\hatrzz{\widehat{\Rbf}_{\ssstyle ZZ}}
\def\hatrss{\widehat{\Rbf}_{\ssstyle SS}}
\def\phstar{{\phantom{*}}}
\let\rzzhat=\hatrzz \let\rxxhat=\hatrxx \let\Rxxhat=\rxxhat
\let\ssstyle=\scriptscriptstyle
\let\rsshat=\hatrss
\def\eqorneq{=\!\!\!\! ? \,\,}
\def\ltornlt{? \!\!\!\!\! <}
\def\sN{{\ssstyle N}}
\def\sM{{\ssstyle M}}
\def\sF{{\ssstyle F}}
\def\sT{{\ssstyle T}}
\def\sH{{\ssstyle H}}
\def\sK{{\ssstyle K}}
\def\sL{{\ssstyle L}}
\def\sQ{{\ssstyle Q}}
\def\sV{{\ssstyle V}}
\def\cross{\!  \times  \!}
\def\alphat{\alpha^{(t)}}
\def\alphab{\alpha^{(b)}}
\def\betat{\beta^{(t)}}
\def\betab{\beta^{(b)}}
\def\ATbf{{\bf A_\sT}}
\def\AHbf{{\bf A_\sH}}
\def\sspan{\mbox{\it span}}
\def\eg{{\it e.g.,}}
\def\etal{{\it et al. \/}}
\def\viz{{\it viz.,\ \/}}
\def\ie{{\it i.e.,\ \/}}
\def\Kout{\setbox1=\hbox{\Huge\bf K}\hbox to
1.05\wd1{\hspace{.05\wd1}\special{
"/Times-BoldItalic findfont 25 scalefont setfont
/prKzip
{0 0 moveto (K) show} def
/dozip
{gsave
  /xp 0 def
  .08 -.04 0
  {
   setgray
  prKzip
-.7 .4 translate} for
  1 setgray prKzip
  0 0 moveto
  0 setgray
  .5 setlinewidth
  (K) true charpath  stroke
 grestore
} def
dozip}}}
\def\Sout{\setbox1=\hbox{\Huge\bf S}\hbox to 1.05\wd1{\hspace{.05\wd1}\special{"
/Times-BoldItalic findfont 25 scalefont setfont
/prSzip
{0 0 moveto (S) show} def
/dozip
{gsave
  /xp 0 def
  .08 -.04 0
  {
   setgray
  prSzip
-.7 .4 translate} for
  1 setgray prSzip
  0 0 moveto
  0 setgray
  .5 setlinewidth
  (S) true charpath  stroke
 grestore
} def
dozip}}}







 \usepackage{bm}
\newcommand*{\B}[1]{\ifmmode\bm{#1}\else\textbf{#1}\fi}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\scalefig#1{\epsfxsize #1\textwidth}

\begin{document}
\title{Quantized Consensus by the ADMM: Probabilistic versus Deterministic Quantizers}
\author{Shengyu Zhu,~\IEEEmembership{Student~Member,~IEEE,} and Biao Chen,~\IEEEmembership{Fellow,~IEEE}\thanks{This work was supported by National Science Foundation under Award CCF1218289,  by Army Research Office under Award W911NF-12-1-0383, and Air Force Office of Scientific Research under Award FA9550-10-1-0458. The material of this paper was presented in part at IEEE GlobalSIP 2015 \cite{Zhu2015a}.}\thanks{S. Zhu and B. Chen are with the Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY 13244 USA (e-mail: szhu05@syr.edu; bichen@syr.edu).}}

\maketitle

\begin{abstract}
This paper develops efficient algorithms for distributed average consensus with quantized communication using the alternating direction method of multipliers (ADMM). We first study the effects of probabilistic and deterministic quantizations on a distributed ADMM algorithm. With probabilistic quantization, this algorithm yields linear convergence to the desired average in the mean sense with a bounded variance. When deterministic quantization is employed, the distributed ADMM either converges to a consensus or cycles with a finite period after a finite-time iteration. In the cyclic case, local quantized variables have the same mean over one period and hence each node can also reach a consensus. We then obtain an upper bound on the consensus error which depends only on the quantization resolution and the average degree of the network. Finally, we propose a two-stage algorithm which combines both probabilistic and deterministic quantizations. Simulations show that the two-stage algorithm, without picking small algorithm parameter, has consensus errors that are typically less than {one} quantization resolution for {all} connected networks where agents' data can be of {arbitrary magnitudes}. \end{abstract}
\begin{IEEEkeywords}
Quantized consensus, dither, probabilistic quantization, deterministic quantization, alternating direction method of multipliers, linear convergence.
\end{IEEEkeywords}


\section{Introduction}
\IEEEPARstart{I}{n} recent years there has been considerable interest in distributed average consensus where a group of agents aim to reach a consensus on the average of their measurements \cite{Ren2007,Cao2013,Lynch1996distributed, Ren2005,Xiao2005,Xu1996,Kashyap2007,Xiao2004, Jakovetic2010,Nedic2009,Aysal2009,Boyd2006,Schizas2008,Zhu2009,Erseghe2011,Aysal2008,Kar2010,Chamie2014,Carli2010}. This is largely motivated by numerous applications in control, signal processing, and computer science. For example, the distributed averaging is a fundamental problem in {\it{ad hoc}} network applications, such as distributed agreement and synchronization \cite{Lynch1996distributed}, distributed coordination of mobile autonomous agents \cite{Ren2005}, and distributed data fusion in sensor networks \cite{Xiao2005}. It has also found applications in load balancing for parallel computers \cite{Xu1996}.


We consider in this paper distributed averaging algorithms where nodes only exchange information with their immediate neighbors. These algorithms are extremely attractive for large scale networks characterized by the lack of centralized access to information. They are also energy efficient and enhance the survivability of the networks, compared with fusion center based processing. However, a number of factors such as limited bandwidth, sensor battery power, and computing resources place tight constraints on the rate and form of information exchange amongst neighboring nodes, resulting in {\em quantized consensus} problems \cite{Ren2007,Kashyap2007}. This paper is specifically devoted to developing efficient algorithms for quantized consensus in connected networks with static topologies.
\subsection{Related work}
There are three widely used methods for solving distributed averaging problems. A classical approach is to update the state of each node with a weighted average of values from neighboring nodes \cite{Xiao2004,Jakovetic2010,Nedic2009}. The matrix, consisting of the weights associated with the edges, is chosen to be doubly stochastic to ensure convergence to the average. Another method is a gossip based algorithm, initially introduced in \cite {Tsitsiklis1984} for consensus problems and further studied in \cite{Kashyap2007,Aysal2009, Boyd2006}, among others. The third approach is to employ the ADMM which is an iterative algorithm for solving convex problems and has received much attention recently (see \cite{BoydADMM} and references therein). The idea is to formulate the data average as the solution to a least-squares problem and manipulate the ADMM updates to derive a distributed algorithm  \cite{Schizas2008, Zhu2009, Erseghe2011}.

In the most ideal case where agents are able to send and receive real values with infinite precision, the three methods all lead to the desired consensus at the average. When quantization is imposed, however, these methods do not directly apply. A well studied approach for quantized consensus is to use dithered quantizers which add noises to agents' variables before quantization\cite{Schuchman1964}. By imposing certain conditions, the quantization error sequence becomes independent and identically distributed (i.i.d.) and is also independent of the input sequence. The classical approach and the gossip based algorithm then yield the almost sure consensus at a common but random quantization level with the expectation of the consensus value equal to the desired average  \cite{Aysal2008,Kar2010,Carli2010}. To the best of our knowledge, there have been no existing results on the ADMM based method for quantized consensus. Nevertheless, since the quantization error of dithered quantizer is zero-mean and has a bounded variance, we can immediately extend the results in \cite{Zhu2009, Erseghe2011} to quantized consensus (see Section \ref{sec:PQ}). That is, the ADMM based method using dithered quantization leads to the consensus at the data average in the mean sense whose variance converges to a finite value. 

Meanwhile, studies on distributed average consensus with deterministic quantizers have been scarcely reported. Deterministic quantization makes the problem much harder to deal with as the error terms caused by quantization no longer possess tractable statistical characteristics \cite{Aysal2008,Kar2010}. The authors in \cite{Nedic2009} show that the classical approach, where a quantization rule that rounds the values down is adopted, converges to a consensus with an error from the average depending on the quantization resolution, the number of agents, the agents' data and the updated weights of each agent.  A recent result of \cite{Chamie2014} indicates that this approach, with appropriate choices of the weights, reaches a quantized consensus close to the average in finite time or leads all agents' variables to cycle in a small neighborhood around the average; in the latter case, however, the consensus is not guranteed. The gossip based algorithms in \cite{Carli2010} and \cite{Kashyap2007} have similar results to those of the classical approach. The ADMM based algorithms for deterministically quantized consensus, however, have not yet been explored.

\subsection{Our contributions}
One shall note that the consensus error for deterministically quantized consensus in \cite{Nedic2009,Carli2010} is much undesired when the number of agents or the range of agents' data becomes very large. Unfortunately, this is typically the case in large scale networks or big data settings. The ADMM has been known to be an efficient algorithm for large scale optimizations and used in various applications such as regression and classification \cite{BoydADMM}. Moreover, \cite{He2012, Hong2012, Deng2016} validate the fast convergence of the ADMM and \cite{Zhu2009, Erseghe2011} demonstrate the resilience of the ADMM to noise, link failures, etc. We therefore expect ADMM based methods to work well for quantized consensus problems, with regards to both the consensus error and the convergence time.

We first study the effect of probabilistic quantization \cite{Xiao2005a}, which is equivalent to a dithering method as shown by \cite[Lemma 2]{Aysal2008}, on the ADMM based method. Utilizing the first and second order moments of the probabilistic quantizer output, we establish the convergence to the average in the mean sense based on existing convergence results of the ADMM. Furthermore, recent work of \cite{Shi2014} enables us to immediately characterize the convergence rate of the distributed ADMM with probabilistic quantization.

The main contribution of this paper is to design and analyze an ADMM based approach using deterministic quantization. We establish that a distributed deterministically quantized ADMM algorithm either converges to a consensus or cycles around the average after a finite-time iteration as long as a mild initialization condition is satisfied. We also show that the cyclic period is finite and that the quantized variable at each node has the same mean over one period. Thus, a consensus can be reached within finite iterations for both convergent and cyclic cases. We then derive an upper bound that only depends on the quantization resolution and the average degree of the undirected graph (two times the ratio of the number of edges to the number of nodes). This is much preferred for large scale networks as it does not rely on the number of agents or the agent's data.

While numerical examples show that the deterministically quantized ADMM converges in most cases, we notice that it may reach different consensus values with different initial variable values. It is well known that a good starting point usually helps in such settings. This inspires our approach for quantized consensus which first uses the probabilistic method to obtain a good starting point and then employs the deterministic algorithm. Simulations show that this two-stage approach tends to converge and also performs best among all existing methods using deterministic quantization in terms of the consensus error. 

\subsection{Paper organization}
The rest of this paper is organized as follows. Section \ref{sec:ADMMnoQ} reviews the application of the ADMM to the distributed averaging problem without quantization, which leads to a distributed ADMM algorithm. We then develop several convergence results of this algorithm; they will be used later to establish our main results. Section \ref{sec:problemformulation} defines probabilistic and deterministic quantization schemes. Their effects on the distributed ADMM are studied respectively in Sections \ref{sec:PQ} and \ref{sec:DQ}. Section \ref{sec:algorithm} describes the proposed algorithm for quantized consensus which combines the two quantized ADMM methods, followed by simulation results in Section \ref{sec:simulation}. Section \ref{sec:conclusion} concludes the paper. 
\subsection{Notations}
Denote by  the Euclidean norm of a vector  and  the inner product of two vectors  and . Given a semidefinite matrix  with proper dimensions, the -norm of  is . Also denote  as the largest singular value of a square matrix  and  as the smallest nonzero singular value of . 

We use two definitions of rate of convergence for an iterative algorithm. A sequence , where the superscript  stands for time index, is said to converge \emph{Q-linearly} to a point  if there exists a number  such that  with  being a vector norm. A sequence  is said to converge \emph{R-linearly} to  if for all ,  where  converges Q-linearly to .

\section{Distributed Average Consensus by the ADMM}
\label{sec:ADMMnoQ}
This section introduces the consensus ADMM (CADMM) for average consensus without quantization. This ideal case provides a good understanding of how the ADMM works for distributed average consensus. We start with the setting of the distributed averaging problem.

\subsection{Problem setting}
\label{sec:orgnotation}
Consider a connected network of  agents which are bidirectionally connected by  edges (and thus  arcs). We describe this network as a symmetric directed graph  or an undirected graph , where  is the set of vertices with cardinality ,  is the set of arcs with  and  is the set of edges with . Assume that the topology of the network is fixed throughout this paper. Let  be the local data only available at node , , and  the vector concatenating all . The goal of distributed average consensus is to compute the data average  
 
by local data exchanges among neighboring nodes.

\subsection{Application of the ADMM to distributed average consensus: CADMM}
The ADMM applies in general to the convex optimization problem in the form of 

where  and  are optimization variables,  and  are convex functions, and  is a linear constraint on  and . The ADMM solves a sequence of subproblems involving  and  one at a time and iterate to converge when, e.g.,  and  are proper closed convex functions and the Lagrangian of (\ref{eqn:admm}) has a saddle point \cite{BoydADMM}. 

To apply the ADMM, we first formulate (\ref{eqn:averageconsensus}) as a convex optimization problem
 
that is, the data average is the solution to a least-squares minimization problem. We continue to reformulate (\ref{eqn:squareformulation}) in the form of (\ref{eqn:admm}) as

where  is the local copy of the common optimization variable  at node  and  is an auxiliary variable imposing the consensus constraint on neighboring nodes  and . We emphasize that throughout the entire paper,  represents the local data, i.e., the observation at the th agent, while  is referred to as the local variable. Since the network is connected, this constraint ensures the consensus to be achieved over the entire network, i.e., , which in turn guarantees the solution to (\ref{eqn:admmformulation}) is the data average . Further define  as a vector concatenating all ,  as a vector concatenating all , and 

Then (\ref{eqn:admmformulation}) can be written in a matrix form as 

where , and  is a column vector with proper dimensions and all entries being . Here  with  being a  identity matrix and  with . If  and  is the th entry of , then the th entry of  and the th entry of  are ; otherwise the corresponding entries are . 

We are now ready to apply the ADMM to solve the consensus problem. The augmented Lagrangian of (\ref{eqn:matrixform}) is 

where  with  is the Lagrange multiplier and  is a positive algorithm parameter. At iteration , the ADMM first obtains  by minimizing , then calculates  by minimizing  and finally updates  using  and . The updates are 
 
where  is the gradient of  at . 

A nice property of the ADMM, known as \emph{global convergence}, states that the sequence  generated by (\ref{eqn:admmupdates}) has a single limit point   which is a primal-dual solution to (\ref{eqn:Lagragian}). Proofs can be found in \cite{BoydADMM,Deng2016,He2012}. Noting that our objective function  given in (\ref{eqn:objectivefunction}) is strongly convex in , we obtain  as the unique primal solution where  denotes the -dimensional column vector with all entries being . To summarize, we have
\begin{lemma}[Global convergence of the ADMM \cite{BoydADMM,Deng2016,He2012}]
\label{lem:globalconvergence}
For any initial values ,  and , the updates in (\ref{eqn:admmupdates}) yield that as , 
where  is a primal-dual solution to (\ref{eqn:Lagragian}) and  is unique for the distributed average consensus problem (\ref{eqn:squareformulation}).
\end{lemma}

While (\ref{eqn:admmupdates}) provides an efficient centralized algorithm to solve (\ref{eqn:squareformulation}), it is not clear whether (\ref{eqn:admmupdates}) can be carried out in a distributed manner, i.e., data exchanges only occur within neighboring nodes. Interestingly, Lemma \ref{lem:globalconvergence} states that convergence for the ADMM is guaranteed regardless of initial values  and ; there indeed exist initial values that decentralize (\ref{eqn:admmupdates}). Define  and  which are respectively the unoriented and oriented incidence matrices with respect to the directed graph . Initialize  and . As shown in \cite{Shi2014}, the updates in (\ref{eqn:admmupdates}) lead to 
 
at node , where  denotes the set of neighbors of node  and  is the th entry of . Obviously, (\ref{eqn:distributedversiond_plug}) is fully decentralized as the updates of  and  only rely on local and neighboring information. Therefore (\ref{eqn:distributedversiond_plug}) can be used for distributed average consensus. We refer to (\ref{eqn:distributedversiond_plug}) as the CADMM for distributed average consensus.

If we further initialize  in the column space of  (e.g., ), then  lies in the column space of  and converges to a unique . We will use this result immediately but postpone its proof to Lemma \ref{lem:linearconvergence}. Note that this implies  in (\ref{eqn:distributedversiond_plug}) converges uniquely to . We also notice an interesting relation between  and  even though  is rank deficient.\footnote{As defined in Section \ref{sec:LCofDCADMM},  where  is the signed Laplacian matrix of the connected undirected graph and always has  as its eigenvalue. See \cite{ChungSpectral}.}
 
\begin{lemma}
\label{lem:abrelation}
Given a connected network, if  lies in the column space of , then  and  are one-to-one correspondence, i.e., for  and  where  and  are in the column space of ,  if and only if .
\end{lemma}
\begin{IEEEproof}
That  implying  is straightforward. Consider  and write   for some .  and  are similarly defined. Then we have 

where  is the smallest nonzero singular value of , whose existence is guaranteed for a connected graph \cite{ChungSpectral}. We therefore have  if .
\end{IEEEproof}

It is therefore meaningful to define an \emph{initialization condition} for the CADMM. A similar global convergence property for the CADMM is given in Lemma \ref{lem:linearconvergence_DCADMM}.
\begin{center}
{\fbox{\begin{minipage}{0.9\linewidth}
{\bf Initialization condition for the CADMM}:  can be any vector in  and  lies in the column space of .
\end{minipage}}}
\end{center}

\begin{lemma}[Global convergence of the CADMM]
\label{lem:linearconvergence_DCADMM}
For any  and  satisfying the initialization condition, the CADMM leads to 
where  and  which lies in the column space of  are both unique.
\end{lemma}
\begin{IEEEproof}
Global convergence follows from Lemmas \ref{lem:globalconvergence} and \ref{lem:abrelation} together with the fact that  converges to a unique  which lies in the column space of .

Now taking  in (\ref{eqn:distributedversiond_plug}) and using the fact that  for , we have 
\end{IEEEproof}


Throughout the rest of this paper, we assume that the CADMM, wherever used, is initialized to satisfy the initialization condition.

\subsection{Linear convergence of the CADMM}
\label{sec:LCofDCADMM}
We investigate two properties of the CADMM; the first property is built on global convergence while the second considers the rate of convergence. 

Define  and  which are respectively the signless and signed Laplacian matrices with respect to . Let  be the degree matrix related to the underlying network, i.e., a diagonal matrix with its th entry being the degree of node  and other entries being . Then   and Lemma \ref{lem:abrelation} is an immediate result from the property of  \cite{ChungSpectral}. We rewrite (\ref{eqn:distributedversiond_plug}) in the matrix form as

or equivalently,

with 
and

where  denotes the  matrix with all entries being , , , and hence, . From (\ref{eqn:iterateform}), we have 

It is thus interesting to investigate how  behaves as . From (\ref{eqn:matrix_iterate}), a logical approach is to study  through the structures of  and ; fortunately, the global convergence property of the CADMM provides a simple argument to obtain a rough estimate of , which, nevertheless, is good enough for our purpose in establishing the main results. Note that we also have  and  as our optima due to global convergence of the CADMM. Our result about  is given below.
\begin{theorem}
\label{thm:Dproperty}
Consider  defined in (\ref{eqn:matrix_iterate}). Then

for fixed .
\end{theorem}
\begin{IEEEproof}
By Lemma \ref{lem:linearconvergence_DCADMM}, we have for any  that satisfies the initialization condition, Recall that . If we fix  and , global convergence implies that  regardless of the initial value . Thus , . Similarly, fixing  and , we must have . Since  is initialized in the column space of  where  is the signed Laplacian matrix of a connected undirected graph,  and  must be respectively the products of some vectors  and  in  multiplying , such that . Knowing the form of  and ,~, we see that  and  only depend on . Together with the facts that  has each entry of itself reaching the data average  and that  for any , we validate  and  as given in the theorem. The remaining blocks,  and , follow directly from the matrix multiplication.
\end{IEEEproof}

Given global convergence, we now turn our attention to the rate of convergence of the CADMM. Recent work of \cite{Hong2012, Deng2016}
has established the linear convergence of the ADMM. Unfortunately, their results do not apply to the CADMM as their conditions are not satisfied here. In \cite{Hong2012}, the step size of the dual variable update, i.e.,  in the -update of (\ref{eqn:admmupdates}), need be sufficiently small while our CADMM has a fixed step size  that can be any positive number (see Remark \ref{rmk:rho} for further discussion on the choice of ). The linear convergence in \cite{Deng2016} is established provided that either  is strongly convex or  is full row-rank in (\ref{eqn:admmformulation}). In our formulation, however,  is not strongly convex and  is row-rank deficient. Nevertheless, we first give Lemma \ref{lem:linearconvergence} with regards to the convergence rate of a vector concatenating  and . A more general result can be found in \cite[Theorem 1]{Shi2014}. Our proof is similar to that of \cite{Shi2014} but simpler.

\begin{lemma}[{\cite[Theorem 1]{Shi2014}}]
\label{lem:linearconvergence} 
Consider the ADMM iteration (\ref{eqn:admmupdates}) that solves (\ref{eqn:matrixform}). Define where  is the dual variable. If we initialize ,  where  is the other dual variable and  is in the column space of , then for , ,  lies in the column space of , and  converges uniquely to  with ,  and  being a vector in the column space of . Furthermore,  converges Q-linearly to its optimal  with respect to the -norm

where 
 denotes the spectral norm or the largest singular value of , and  denotes the smallest positive singular value of .
\end{lemma}
\begin{IEEEproof}
See Appendix.
\end{IEEEproof}

With this lemma, we can now establish the linear convergence rate of the CADMM .

\begin{theorem}[Linear convergence of the CADMM]
\label{thm:linearconandbound}
Consider the matrix form of the CADMM in (\ref{eqn:iterateform}). If  satisfy the initialization condition, then 
 where  and  are defined in Lemma \ref{lem:linearconvergence}. Therefore,  is R-linearly convergent to .
\end{theorem}
\begin{IEEEproof}
Notice that the initializations in Lemma \ref{lem:linearconvergence} decentralize the ADMM iteration (\ref{eqn:admmupdates}) into the CADMM. Thus  is the same in the ADMM iteration (\ref{eqn:admmupdates}) and the CADMM iteration (\ref{eqn:distributedversiond_plug}) while . Then (\ref{eqn:x13}) implies 
 
We also have  

where the last two inequalities are from the definitions of  and , and (\ref{eqn:Ulinearconvergence}), respectively.
Thus, 

\end{IEEEproof}
\section{Quantized Consensus}
\label{sec:problemformulation}
To model the effect of quantized communications, we assume that each agent can store and compute real values with infinite precision; however, an agent can only transmit quantized data through the channel which are received by its neighbors without any error. The quantization operation is defined as follows. Let  be a given quantization resolution and define the quantization lattice in  by  A quantizer is a function  that maps a real value to some point in . 
Among all quantizers we consider the following two for distributed average consensus:
\begin{enumerate}
\item Probabilistic quantizer  defined as follows:
for ,
\item Rounding quantizer  which projects  to its nearest point in :
\end{enumerate}
We point out that probabilistic quantization is equivalent to a dithered quantization method (see \cite[Lemma 2]{Aysal2008}) while rounding quantization is one of the deterministic quantization schemes. Throughout the rest of this paper, we use  (or  for ease of presentation) to denote the quantized value of  regardless of its quantization scheme; we use  (or ) and  (or ) when it is necessary to specify the quantization scheme. Quantizing a vector means quantizing each of its entries. Define  as the quantization error. It is clear that





As seen from Section \ref{sec:ADMMnoQ}, the CADMM has the advantage of global and linear convergence for solving the average consensus problem as long as the initialization condition is met.  The authors in \cite{Zhu2009, Erseghe2011} have also shown the good behavior of the ADMM in distributed settings when noise or random link failures are imposed. The rest of this paper is devoted to investigating the effects of the two quantization schemes defined in (\ref{eqn:Qp}) and (\ref{eqn:Qd}) on the performance of the CADMM. We remark that the results of probabilistic and rounding quantizations hold respectively for other dithered and deterministic cases, which will be elaborated in Sections \ref{sec:PQ} and \ref{sec:DQ}. 




\section{Probabilistic Quantization}
\label{sec:PQ}


For ease of presentation, we only study the probabilistic quantization defined in (\ref{eqn:Qp}). The results can be easily extended to any other dithered quantization as the only information used is the first and second order moments of the probabilistic quantizer output which are stated in the following lemma. See \cite{Xiao2005a} for a proof.

\begin{lemma}[{\cite[Lemma 2]{Xiao2005a}}]
\label{lem:QPproperty}
For every , it holds that 

\end{lemma}
The iteration in (\ref{eqn:distributedversiond_plug}) now takes the form of
 
Notice that  is also quantized at its own node for the th update; the reason will be given in Remark~\ref{rmk:Qitself}. As illustrated in \cite{Zhu2009}, iteration (\ref{eqn:distributedversiond_plug_Qp}) can be interpreted as a stochastic gradient update. Viewed from this point, the quantization error causes  to fluctuate around the quantization-free updates  (\ref{eqn:distributedversiond_plug}). Our convergence claims are given in Theorem \ref{thm:Qpresult}.
\begin{theorem}
\label{thm:Qpresult}
Let  and   satisfy the initialization condition. The probabilistically quantized CADMM (PQ-CADMM) iteration (\ref{eqn:distributedversiond_plug_Qp}) generates , which converges linearly to the data average  in the mean sense as . In addition, the variance of  converges to a finite value which depends on  and the network topology.
\end{theorem}
\begin{IEEEproof}
Taking expectation of both sides of  (\ref{eqn:distributedversiond_plug_Qp}), we have 
 
Noting that Lemma \ref{lem:QPproperty} implies  and , we see that (\ref{eqn:distributedversiond_plug_Qp_Expectation}) takes exactly the same iterations in the mean sense as the CADMM. By initializing  in the column space of ,  satisfies the initialization condition. The linear convergence of  to  is thus ensured due to Theorem \ref{thm:linearconandbound}.

Since Lemma \ref{lem:QPproperty} also indicates the bounded variance of quantization error, the second claim follows directly from \cite[Proposition 3]{Zhu2009}.
\end{IEEEproof}

We notice that the convergence of  does not indicate that  reaches a consensus when . Nevertheless, a simple method fixes this problem. The idea is to calculate the running average  at each node . One can use similar steps in the proof of \cite[Proposition 3]{Zhu2009} to show that  has diminishing variance. By Chebyshev's inequality, we then get the following corollary.
\begin{corollary}
Let  for . For each node , we have 
\end{corollary}
 
\section{Deterministic Quantization} 
\label{sec:DQ}
Deterministic quantization is usually much harder to handle as the quantization error is not stochastic. Unlike probabilistic quantization, the accumulated error term is very likely to blow up; there have been a few methods proposed to counter such difficulties (see \cite{Nedic2009,Chamie2014,Carli2010}), yet the resulting algorithms either do not guarantee a consensus or reach a consensus with an error from the desired average that depends on the number of agents, the quantization resolution, and the agents' data. Our approach will establish a finite upper bound on the accumulated error term and then use the property and the initialization condition of local Lagrangian multipliers to deduce the consensus reaching result.

Let the local data  be also quantized for the th update at node . The updates become

Rewrite  with  according to (\ref{eqn:Qd}). Then the -update, , is equivalent to 

or written in the matrix form,

where  denotes the vector concatenating all . Recalling the ideal CADMM update (\ref{eqn:idealdcadmm}), we have the matrix form of (\ref{eqn:Qdistributedversion}) as

where  and . It is important to note that  is deterministic and hence the update (\ref{eqn:iterateform_QD}) is deterministic. Our main results are stated in the following theorem.
\begin{theorem}
\label{thm:mainresults}
Consider the deterministically quantized CADMM (DQ-CADMM) iteration (\ref{eqn:Qdistributedversion}). Let  and  satisfy the initialization condition for the CADMM. Then there exists a finite time iteration  such that for  all the quantized variable values
\begin{itemize}
\item either converge to the same quantization value:



\item or cycle around the average  with a finite period , i.e., , and 

\end{itemize}

For both convergent and cyclic cases, we have the following error bound for :

where the upper bound is tight if the DQ-CADMM converges.



\end{theorem}
\begin{IEEEproof} We prove that the DQ-CADMM either converges or cycles after a finite-time iteration and then use this fact to derive the error bounds.

We see from (\ref{eqn:alphamatrixupdate}) that  must lie in the column space of  if  is initialized in the column space of . Following (\ref{eqn:iterateform_QD}), we have 

The first term is simply the ideal CADMM update which converges to a finite value. We will show that the accumulated error term  is bounded and hence that  is bounded. Notice that  is the th update of the CADMM with the initial value . Let  be the vector that concatenates the primal and dual variables in the ADMM iteration (\ref{eqn:admmupdates}), with initial values  and  corresponding to . With  defined in Lemma \ref{lem:linearconvergence}, we obtain

where the last inequality is from (\ref{eqn:Qerror}).
Since Theorem \ref{thm:Dproperty} indicates the form of , we get , i.e.,  and . Therefore,  from Lemma \ref{lem:abrelation} and the fact that . Noting also that the initialization  and  meet the condition of Lemma \ref{lem:linearconvergence}, we thus have 

where  is from Theorem \ref{thm:linearconandbound} and  is due to Lemma \ref{lem:linearconvergence} together with the fact that . Similarly, we have for ,

and when , 
 
Therefore, 

where  is from (\ref{eqn:sx})-(\ref{eqn:sa0}). Then (\ref{eqn:ddd}) must be finite for  as , and thus  is bounded. An important fact from (\ref{eqn:iterateform_QD}) is that the update of  and hence  is fully determined by  due to the deterministic quantization and the CADMM update. Recalling that  and that  with each entry of  being a multiple of , each entry of  being a multiple of , and  being fixed, we conclude that there are only finite possible states of . Therefore,  is either convergent or cyclic with a finite period  after a finite-time iteration. 

We next consider error bounds for the consensus value. The consensus error may be studied directly by calculating the accumulated error term in (\ref{eqn:expansion}). However, the bound in (\ref{eqn:ddd}) is quite loose in general since it results from the worst case. We alternatively derive the error bounds in the respective case using the fact that the DQ-CADMM either converges or cycles.

\emph{Convergent case:} The convergence of the DQ-CADMM implies that  for , and hence 

Since  is the Laplacian matrix of a connected graph , we must have that  reaches a consensus. Now let  denote the convergent quantized value. Then  for , and . Summing up both sides of (\ref{eqn:Qdistributedversion}) from  to , we have 

which is equivalent to 

Here we use the fact that  lies in the column space of , i.e.,   where . Then .
Recalling that  and , we finally obtain


The following example shows the tightness of this bound in this convergent case. Consider a simple two-node network with  and . Set both  and  to be . In this case, we have ,  and  We start with  and . One can easily check that our initialization condition is met, and  and , in the updates of (\ref{eqn:Qdistributedversion}). Hence  and the consensus error is 

This coincides with the error bound in (\ref{eqn:consensuserror}).

\emph{Cyclic case:} When the DQ-CADMM cycles with a period , we must have . Thus, for , we have that 

and consequently,  reaches a consensus, i.e., (\ref{eqn:cyclic_consensus}) is true. Now denote  We then get

Summing both sides of (\ref{eqn:Qdistributedversion}) over one period and dividing the sum by , we have 

Finally, using (\ref{eqn:cycbd}) and following the same steps as in the convergent case we conclude that 

\end{IEEEproof}
\begin{remark}
The result that deterministic quantization may lead the consensus algorithm to either convergent or cyclic cases is also reported in \cite{Chamie2014}. Similar to theirs, one can use the history of agents' variables, e.g., running average, to achieve asymptotic convergence at each node. Differently, while they can make local variable values close to the true average in cyclic cases without guaranteeing  a consensus, our algorithm can reach a consensus but does not make the error arbitrarily small in general. 
\end{remark}
\begin{remark}
\label{rmk:notglobal}
We shall mention that  or  need not be unique. This is because, unlike the ideal CADMM,  in the DQ-CADMM need not decrease monotonically due to the quantization {\em that occurs on  at each update}. Note also that practical consensus value does not necessarily meet the error bound and we usually have smaller errors than (\ref{eqn:consensuserror}) in practice (see Fig.~\ref{fig:cerr}). We hence expect better consensuses when  are initialized closer to the ideal optima, which leads to a two-stage algorithm for quantized consensus in Section~\ref{sec:algorithm}.
\end{remark}
\begin{remark} 
\label{rmk:rho}
An interesting observation of our main result is the ADMM parameter . While a small  indicates a small consensus error bound, the current paper does not quantify how it affects the convergence rate. Here we do not study the optimal selection of  but simply set . Therefore we do not regard  as a factor affecting our algorithm's performance. We refer readers to \cite{BoydADMM,Shi2014,Ghadimi2015ParaSel} for detailed discussions on how  affects the ADMM's performance.  
\end{remark}
\begin{remark}
Theorem~\ref{thm:mainresults} for rounding quantization extends straightforward to other deterministic quantizations as the only information used in our proof is the bounded quantization error. In contrast with \cite{Kashyap2007,Nedic2009} where the algorithms may fail for some deterministic quantization schemes, e.g., the rounding quantization, our results work for all deterministic quantization schemes as long as a finite quantization error bound is provided. 
\end{remark}
\begin{remark}
\label{rmk:Qitself}
In both the PQ-CADMM and DQ-CADMM iterations,  is quantized for the th update at node  even though nodes can compute and store real values with infinite precision. The reason is to guarantee that  lies in the column space of  and thus the ideal CADMM update in either the PQ-CADMM or the DQ-CADMM [cf. Equation (\ref{eqn:iterateform_QD})] possesses the linear convergence property given in Theorem \ref{thm:linearconandbound}. If we do not quantize  at its own node, Theorem \ref{thm:Qpresult} still holds due to  while Theorem \ref{thm:mainresults} may fail.
\end{remark}
\begin{remark}
In the problem reformulation (\ref{eqn:admmformulation}), each node  has its local objective function being  and  minimizes the global objective function  which is the sum of the local objectives. To analyze the DQ-CADMM, we first identify the CADMM update in the matrix form as  where  is fixed throughout the iterations. We then write the DQ-CADMM update as the sum of the ideal CADMM update plus an accumulated error term and finally utilize the linear convergence rate of the CADMM [cf. Equations (\ref{eqn:sx}) and (\ref{eqn:sa})]. In general, if the local objective functions do not have linear gradients or the linear convergence rate is not guaranteed (e.g., the LASSO is not differentiable and the corresponding CADMM update in this paper's fashion does not converge linearly), then the current proof no longer holds with deterministic quantization.
\end{remark}



\section{ADMM Based Algorithm for Quantized Consensus}
\label{sec:algorithm}
Let us summarize the two quantized versions of the CADMM: the PQ-CADMM converges linearly to the data average in the mean sense, but it does not guarantee a consensus within finite iterations; the DQ-CADMM, on the other hand, either converges to a consensus or cycles with the same mean of quantized variable values over one period at each node after a finite-time iteration, but results in an error from the average. 

As discussed in Remark \ref{rmk:notglobal}, we can first run the PQ-CADMM  times to obtain  which is a reasonable estimate of  at node  according to Theorem \ref{thm:Qpresult}. Here  can be chosen such that  is close enough to  when we have the knowledge of agents' data and the network topology. Otherwise, we can simply set  or as large as permitted. Note also that  is also a good estimate of , and that  satisfies the initialization condition as  lies in the column space of . We can therefore run the DQ-CADMM with this  and  as initial values. The probabilistically quantized CADMM followed by deterministically quantized CADMM (PQDQ-CADMM) is presented in Algorithm \ref{tab:PQDQDCADMM}. 
\begin{algorithm}[htbp]
	\caption{PQDQ-CADMM for quantized consensus}
	\begin{algorithmic}[1]\label{tab:PQDQDCADMM}
	\REQUIRE Initialize~, , and . Set .
\FOR{, every node }
	\STATE 
	\ENDFOR
	\STATE {\bf set} , , and .
	\REPEAT
			\STATE For , 
			
			\STATE {\bf set} .	
	\UNTIL{a predefined stopping criterion (e.g., a maximum iteration number) is satisfied.}
\end{algorithmic}
\end{algorithm}
\section{Simulations}
\label{sec:simulation}
This section investigates the performance of the DQ-CADMM and the PQDQ-CADMM via numerical examples. Since existing methods with dithered quantization do not guarantee convergence to a consensus in finite iterations, we only compare our algorithms with those that uses deterministic quantization to reach a consensus, i.e., the gossip based method in \cite{Carli2010} and the classical method in \cite{Nedic2009}.

\subsection{Performance of the PQDQ-CADMM, the DQ-CADMM, the gossip based method, and the classical method}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{CE_trajectory.eps}\caption{Iterative error versus iterations where each plotted value is the average of  runs.}
	\label{fig:CEtra}
\end{figure}
  
 \begin{figure*}[ht]
	\centering
	\subfigure[]{\includegraphics[width=0.325\linewidth]{CE_N.eps}\label{fig:cerra}}
	\subfigure[]{\includegraphics[width=0.325\linewidth]{CE_E.eps}\label{fig:cerrb}}
	\subfigure[]{\includegraphics[width=0.325\linewidth]{CE_ENfixed.eps}\label{fig:cerrc}}
	\caption{Consensus error of the four algorithms where  and the plotted values are the average of  runs; (a) fixing  and varying , (b) fixing  and varying , (c) fixing  and varying .}
	\label{fig:cerr}
\end{figure*}
\begin{figure*}[ht]
	\centering
	\subfigure[]{\includegraphics[width=.325\linewidth]{CT_N.eps}\label{fig:ctimea}}
		\subfigure[]{\includegraphics[width=0.325\linewidth]{CT_E.eps}\label{fig:ctimeb}}
	\subfigure[]{\includegraphics[width=.325\linewidth]{CE_ENfixedAdj.eps}\label{fig:ctimec}}
	\caption{Convergence time of the four algorithms where  and the plotted values are the average of  runs; (a)  and , (b)  and , (c)  and .}
	\label{fig:ctime}
\end{figure*}
To construct a connected graph with  nodes and  edges, we first generate a complete graph consisting of  nodes, and then randomly remove  edges while ensuring that the network stays connected. Set  and assume that agents' data have very high variances in large networks, e.g., let . Our settings are\begin{itemize}
\item PQDQ-CADMM: Set .
\item DQ-CADMM: Set ,  and .
\item Gossip based method: We randomly pick one edge in  and perform the updating, i.e., if  is chosen, then . 
\item Classical method: Let  denote the weight matrix of the graph . The updating rule is then given by  where the subscript  denotes the rounding down quantization. We utilize the Metropolis weights defined in \cite{Xiao2005}: 

\end{itemize}



We simulate a connected network with  nodes and  edges. Define the iterative error as  which is equal to the consensus error  when consensus is reached. Plotted in Fig.~\ref{fig:CEtra} is the iterative error of the four algorithms at every iteration  with each value being the average of  runs. Note that we start the plot of the PQDQ-CADMM from the th iteration as its first  iterations are used only to reach a neighborhood of ; at the th iteration,  is updated based on the running average of the th iteration to the th iteration. The figure indicates that all the four algorithms converge to a consensus at one of the quantization levels. The average consensus error of the DQ-CADMM is , which is much smaller than the upper bound . One can also see that the PQDQ-CADMM converges almost immediately after the th iteration. In the following we compare the consensus error and the convergence time of the four algorithms via simulations that respectively fix the number of nodes, the number of edges, and the average degree of the graph.

{\it Consensus error:}
In Fig.~\ref{fig:cerra} we fix  and vary  until the graph is complete. The gossip based method and the classical method have decreasing consensus errors as  increases. The consensus error of the DQ-CADMM, however, becomes larger as the average degree and therefore the error bound increase. The PQDQ-CADMM has the smallest consensus error whose average of  runs is less than  for all . We then fix  and let  vary. Fig.~\ref{fig:cerrb} shows that the gossip based method and the classical method have increasing consensus errors as  increases. The consensus error of the DQ-CADMM, on the contrary, decreases when  becomes larger. The PQDQ-CADMM also has the smallest consensus error in this case. In the last setting we fix the average degree  while varying . The classical method and the gossip based method then both have increasing consensus errors when  and thus the range of agents' data increase. The consensus error of the DQ-CADMM is relatively small compared with the upper bound  and decreases when  becomes larger. The proposed PQDQ-CADMM still has the smallest consensus error whose average of  runs is less than  for all .  
 
We conclude that the consensus error of the gossip based method and the classical method depends on the average degree of the graph as well as the range of agents' data. Note that their consensus errors can be extremely large for a sparsely connected graph. The DQ-CADMM has an increasing consensus error when the average degree increases while the PQDQ-CADMM performs almost the same for all network structures in terms of the consensus error. 

{\it Convergence time:} We study the convergence time of the four algorithms via numerical examples in Fig.~\ref{fig:ctime}. Since the gossip based method involves only one edge and the other three methods utilize all the edges at each iteration, we plot also the quotient of the convergence time of the gossip based method divided by the number of edges, namely, Gossip based method adjusted, in the figure. 

In Fig.~\ref{fig:ctimea}, the gossip based method and the classical method converge slower as the graph becomes sparser. When the average degree is fixed, they have longer convergence time as  increases. Therefore, the convergence time of the gossip based method and the classical method is also affected by the average degree of the graph and the range of agents' data. Different from the gossip based and classical methods, we see in Fig.~\ref{fig:ctimea} that the convergence time of the DQ-CADMM increase as the graph becomes denser. In Fig.~\ref{fig:ctimeb} and Fig.~\ref{fig:ctimec}, however, the convergence time also increases while the graph becomes sparser, which is possibly because of the increased distance between starting points and optimal values. Exactly characterizing the convergence time of the DQ-CADMM is beyond the scope of the current paper and will be treated as future work. For the PQDQ-CADMM, we observe that the significant portion of its convergence time is spent on achieving an approximate estimate of , i.e., running the PQ-CADMM with  iterations. With good starting points, the DQ-CADMM converges almost immediately. 

\subsection{Performance of the PQDQ-CADMM with different quantization resolutions}
We next consider the effect of the quantization resolution on the PQDQ-CADMM. Fig.~\ref{fig:CEDelta} plots consensus errors of the PQDQ-CADMM with  and  for . The consensus error tends to increase on the average as the quantization resolution becomes larger, which is not surprising since a coarse quantization indicates a higher loss of information at each update. We then calculate the ratio of the consensus error to the quantization resolution: the plotted values, which are the averages of  runs, all lie in  and the variances are less than . Moreover, the convergence time of each quantization resolution has a mean of  iterations and a variance less than , which coincides with our previous analysis that the PQDQ-CADMM converges immediately after the first  iterations.

\begin{figure}[h]
\vspace{-0.15in}
	\centering
	\includegraphics[width=\linewidth]{CE_delta.eps}\caption{Consensus error of the PQDQ-CADMM with different quantization resolutions, i.e., , for  and ; each plotted value is the average of  runs.}
	\label{fig:CEDelta}
\end{figure}

\subsection{Cyclic Cases}
While we prove that the DQ-CADMM either converges or cycles in Theorem~4, it is noted that the above numerical examples all lead to reach convergence results. Indeed, the proposed deterministic algorithms, the DQ-CADMM and PQDQ-CADMM, converges in most cases as shown by the following simulation. For connected networks with  nodes, we consider star graph which has the smallest average degree, randomly generated graph that has intermediate average degree, and complete graph that has the largest average degree. The result is given in Fig.~\ref{fig:cycnum} where the -axis represents the number of cyclic cases in  trials. Clearly, the DQ-CADMM and PQDQ-CADMM with fixed parameter  converge in most cases, particularly with large networks. 
\vspace{-0.15in}
\begin{figure}[h]
    \centering
    {\includegraphics[width=1.0\linewidth,height=0.85\linewidth]{TSP_cyc.eps}}
      \caption{Number of cyclic cases in  trials.}
      \label{fig:cycnum}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
In this paper we have proposed an efficient algorithm, the PQDQ-CADMM, for quantized consensus problems. We first study the effects of both probabilistic and deterministic quantizations on the CADMM. With probabilistic quantization, the PQ-CADMM converges linearly to the data average in the mean sense. In the deterministic case, we can bound the sum of the absolute value of each error term caused by quantization using the global and linear convergence of the CADMM and thus prove that the DQ-CADMM either converges or cycles. We finally combine the two quantized versions of the CADMM to obtain the PQDQ-CADMM algorithm, where the PQ-CADMM to is used to get an initial estimate of the data average and the DQ-CADMM is used subsequently for consensus reaching purpose. Simulations show that our PQDQ-CADMM provides the best result than all existing methods using deterministic quantization in terms of the consensus error.

Our approach also motivates a number of further research directions:
\begin{enumerate}
\item Data communications between agents were assumed to be perfect in this paper. In practice, channel impairment may lead to imperfect transmissions. Moreover, the links between agents may fail and the network topology may vary randomly, as studied in \cite{Nedic2009,Zhu2009,Kar2010}. It is thus meaningful to investigate how our algorithm performs in these settings.
\item The algorithm parameter  is another interesting topic in the DQ-CADMM. Roughly speaking, a smaller  may result in a small consensus error but a longer time to reach the convergent or cyclic result. Therefore, tts choice should be guided depending on whether a small consensus error or fast consensus speed is desired.
\item We only considered the unbounded quantization scheme in this paper. It is also interesting to consider bounded quantization that is used in many applications as it significantly reduces the amount of data that needs to be exchanged.
\end{enumerate}
\appendices
\section*{Appendix}
\begin{IEEEproof}[Proof of Lemma \ref{lem:linearconvergence}] We first manipulate (\ref{eqn:admmupdates}) to derive equivalent updates 

where (\ref{eqn:admmupdatesequivalent1}) and (\ref{eqn:admmupdatesequivalent2}) are from multiplying the two sides of the -update by  and  and adding them to the -update and -update, respectively. Recalling  with  and , we know that  from (\ref{eqn:admmupdatesequivalent2}). Since we initialize , we have  for . Equation (\ref{eqn:admmupdatesequivalent1}) then reduces to , and (\ref{eqn:admmupdatesequivalent3}) splits into  and . Summing and subtracting these two equations we have  and . With the initialization ,  holds true for . Since  is unique and equal to  according to Lemma \ref{lem:globalconvergence},  is also unique. To summarize, with the initialization  and , (\ref{eqn:admmupdatesequivalent1})-(\ref{eqn:admmupdatesequivalent3}) reduce to 

which further lead to  and  uniquely as .
Taking  in (\ref{eqn:admmupdates21})-(\ref{eqn:admmupdates23}) and using global convergence, we get

We can now use (\ref{eqn:admmupdates31}) to demonstrate the uniqueness of  if we also initialize  in the column space of . Note that if  lies in the column space of  then (\ref{eqn:admmupdates22}) indicates that  also lies in the column space of , . The uniqueness of  then follows from the uniqueness of  and Lemma \ref{lem:abrelation}.

Next we show the linear convergence of . Subtracting (\ref{eqn:admmupdates21})-(\ref{eqn:admmupdates23}) from (\ref{eqn:admmupdates31})-(\ref{eqn:admmupdates33}), respectively, and using , we have

We therefore obtain 

where  is from (\ref{eqn:admmupdatesfinal}),  is from (\ref{eqn:admmupdatesfinal1}) and (\ref{eqn:admmupdatesfinal2}), and  is from the definitions of  and . Due to (\ref{eqn:x13}), to prove (\ref{eqn:Ulinearconvergence}) we only need to show

which is equivalent to 

It then suffices to show

The rest of this proof is to establish that  and  are upper bounded by two non-overlapping parts of the left side of (\ref{eqn:x41}), respectively.  

We first have from (\ref{eqn:admmupdatesfinal2}) that

To upper bound , we first notice that  lies in the column space of . Therefore, 

Now using (\ref{eqn:beatalphal}) and (\ref{eqn:admmupdatesfinal}) we get 

where  is from the Cauchy-Schwarz inequality together with the fact  for any .
Combining (\ref{eqn:x5}) and (\ref{eqn:x62}), we have 

The proof is thus complete by picking 

\end{IEEEproof}



\section*{Acknowledgments}
The authors would like to thank Professor Zhi-Quan Luo, from University of Minnesota, Professor Mingyi Hong, from Iowa State University, and Professor Lixin Shen, from Syracuse University, for helpful discussions.


\bibliographystyle{IEEEtran}
\bibliography{SZhuBib}















\end{document}
