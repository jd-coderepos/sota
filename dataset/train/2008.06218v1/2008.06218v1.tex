\section{Experiments}

\noindent\emph{\textbf{Datasets and label corruption schemes}.}  To validate the effectiveness of our method, we perform an image classification task on three benchmark datasets: CIFAR-10\footnotemark, CIFAR-100\footnotemark[\value{footnote}], \footnotetext{https://www.cs.toronto.edu/\textasciitilde{}kriz/cifar.html} and Tiny ImageNet\footnote{https://www.kaggle.com/c/tiny-imagenet} and a real-world dataset Clothing1M~\cite{xiao2015learning} (see \tablename~\ref{tab:data}). For the benchmark datasets, we use four label corruption schemes: symmetric noise, asymmetric noise, mixed noise, and nearest label transfer. In this work, we are concerned with scenarios of abundant data with very poor but realistic label quality. Because labelers make mistakes within very few and similar classes~\cite{han2018co,ren2018learning, yi2019probabilistic}, asymmetric noise is injected to these datasets with varying noise rates , and symmetric noise is mixed with asymmetric noise. To simulate confusions between visually similar classes, we also employ nearest label transfer~\cite{seo2019combinatorial}, in which labels are swapped according to a confusion matrix of a pretrained network. All the noise types are detailed in Appendix A.


\noindent\emph{\textbf{Experimental settings.}}
We use two different schemes for the learning rate policy and number of epochs depending on the type of noise that is used. For symmetric noise, we follow the experimental settings of Arazo et al.~\cite{arazo2019unsupervised} and train PreAct ResNet-18 using SGD with a momentum of 0.9, weight decay of , and a batch size of 128 for 300 epochs. The initial learning rate is 0.1 and reduced by a factor of 10 at epoch 100 and 250. We set , ,  for CIFAR-10 and  for CIFAR-100. Data preprocessing and augmentation is also applied, including mean subtraction, horizontal random flip, 32x32 random crops after padding with 4 pixels on each side, and mixup augmentation~\cite{zhang2018mixup}. We report the best classification accuracy (\ie, the percentage of correct predictions out of the entire test dataset) across epochs following prior works~\cite{seo2019combinatorial, arazo2019unsupervised}. For other types of noise, we trained DenseNet (L=25, k=12) for 100 epochs using SGD with a momentum of 0.9 in line with experiments conducted by Huang et al.~\cite{huang2017densely}. The initial learning rate is 0.1 and divided by 5 at epoch 50 and 75. We use batch size of 128, , , and . Each image is scaled to have zero mean and unit variance. We measure performance by the mean of last classification accuracies over three runs for it is common to measure the robustness of noisy labels with the test error at the end of training~\cite{malach2017decoupling, song2019selfie}. We also compute the label precision by the fraction of true clean samples among all the samples selected for training or samples that have non-zero weights. All of the experiments were executed using NAVER Smart Machine Learning (NSML) platform~\cite{kim2018nsml,sung2017nsml}. 



\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/5_noise_rate_pair_one_col.pdf} 
\caption{Classification accuracy comparison on three benchmark datasets with varying rates of \textbf{asymmetric noise}. The shaded area represents the standard deviation of three repeated experiments.
\label{fig:asymmetric_noise_rate}}
\vspace{0.2em}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/5_label_precision_3.pdf} 
\caption{Label precision comparison on CIFAR-100 with  noise after the warmup phase (epoch ). Mixed noise comprises  asymmetric noise and  symmetric noise. \textit{Default} and \textit{Active Bias} do not distinguish clean samples, so they are not included for comparison.
}
\label{fig:precision}
\end{figure}



\subsection{Performance Comparison}

\noindent\emph{\textbf{Baselines.}}
We compare \prgname{} with a baseline algorithm (denoted by \textit{Default}), which trains noisy data without any strategies, an uncertainty-based approach \textit{Active Bias}, loss-based approach \textit{Coteaching}, and a hybrid approach \textit{SELFIE}. \textit{Active Bias}~\cite{chang2017active} emphasizes uncertain samples with high prediction variances. \textit{Coteaching}~\cite{han2018co} uses two networks that feed each other low loss samples. \textit{SELFIE}~\cite{song2019selfie} selects low loss samples and relabels samples with high certainty. It is fair to compare algorithms with the same number of epochs, so we did not restart \textit{SELFIE}, which caused different results from the paper. \prgname{} can handle label noise with only a noisy train dataset, so we did not compare methods that require an additional clean dataset~\cite{shu2019meta}.


\noindent\emph{\textbf{Asymmetric noise.}}
\figurename~\ref{fig:asymmetric_noise_rate} displays the test accuracies of \prgname{} along with other baseline methods for varying rates of asymmetric noise. It appears that the performance of \textit{Default} degrades drastically as the noise rate increases. Although other methods achieve higher accuracies than those of \textit{Default}, our method outperforms all other baselines with significant margins for each dataset and noise rate. Moreover, as can be seen from \tablename~\ref{tab:all}, there is a remarkable improvement in performance for CIFAR-100 where the accuracy differs with the second-best algorithm by 7\%. \figurename~\ref{fig:precision} also shows that our method is effective at detecting and filtering out noise even for the difficult scenario of asymmetric noise. 


\setlength{\tabcolsep}{6pt}
\setlength{\floatsep}{4pt}
\setlength{\textfloatsep}{12pt}

\ctable[
pos=t,
    caption = {Classification accuracy (\%) on CIFAR-100 with high-level noise. Asymmetric and symmetric noise are denoted by A and S respectively.},
    label = tab:largenoise,
    doinside = \footnotesize
]{cc|ccccc}{
}{
\toprule

\multicolumn{1}{l}{} &                   & Default      & Active Bias   & Coteaching   & SELFIE       & \prgname{}            \\

\midrule

Mixed noise          & 50\% (A-40, S-10) & 38.01.2 & 41.21.0 & 37.30.9 & 42.12.7 & \textbf{48.82.1} \\
Mixed noise          & 60\% (A-30, S-30) & 35.90.4 & 40.81.5 & 37.00.7 & 43.80.4 & \textbf{48.51.4} \\
Mixed noise          & 70\% (A-20, S-50) & 32.90.7 & 35.80.5 & 32.20.2 & 41.50.9 & \textbf{42.02.2} \\
Nearest noise        & 60\%              & 36.30.9 & 43.20.1 & 44.01.6 & 46.61.1 & \textbf{47.10.9} \\

\bottomrule
}

 \ctable[
pos=t,
    caption = {Classification accuracy (\%) of various architectures on CIFAR-100 with  asymmetric noise.},
    label = tab:modelcomparison,
    doinside = \footnotesize,
]{l|ccccc}{
}{
\toprule
            & Default      & Active Bias   & Coteaching   & SELFIE       & \prgname{}            \\
\midrule
DenseNet    & 45.31.4 & 50.30.6 & 47.31.4 & 52.80.5 & \textbf{59.50.9} \\
VGG-19      & 35.41.9 & 31.40.7 & 35.60.5 & 39.30.3 & \textbf{43.10.6} \\
ResNet50    & 29.80.2 & 28.20.5 & 32.60.2 & 32.80.2 & \textbf{35.90.4} \\
MobileNetV2 & 32.90.6 & 38.10.6 & 31.90.8 & 35.10.2 & \textbf{39.10.2} \\ 
\bottomrule
}



 
\noindent\emph{\textbf{Mixed noise.}}
According to \tablename~\ref{tab:all}, the performance of \prgname{} achieves the best performance for mixed noise. As symmetric noise increases under the same noise level, the accuracy of \textit{Default} increases, presumably resulting from that symmetric noise is easy to distinguish. This result is in good agreement with results from Section \ref{loss_var_analysis}. We can also observe that the difference between \textit{Default} and other baselines reduces with more symmetric noise, indicating that symmetric noise does not require developed algorithms and lacks significance. Furthermore, \prgname{} can identify noise with high precision than other methods as indicated in \figurename~\ref{fig:precision}. These results of mixed noise imply our model's advantages against noisy real-world data, where symmetric and asymmetric noise may coexist.


\noindent\emph{\textbf{Nearest noise.}}
We can observe from \tablename~\ref{tab:all} that \prgname{} yields higher accuracy for nearest noise compared to other methods. The label precision of our method also surpasses other methods and continues to increase as training proceeds, while other methods appear to converge towards the end of training as shown in \figurename~\ref{fig:precision}.


\noindent\emph{\textbf{High level noise.}}
To validate our method for another challenging problem, we conducted experiments on CIFAR-100 with larger noise rates for mixed noise and nearest noise. As shown in \tablename~\ref{tab:largenoise}, \prgname{} outperforms other state-of-the-art methods for larger noise rates. These results confirm that our method effectively downplays noisy samples and emphasizes clean and informative samples for all noise types.


\noindent\emph{\textbf{Symmetric noise.}} When adding symmetric noise, the true label can be included or excluded from the candidates of labels to be swapped, so we evaluated our method for both cases. We present the results of both definitions of symmetric noise in Appendix C due to lack of space and show that \prgname{} achieves comparable or better performance than other state-of-the-art methods for symmetric noise. 

\noindent\emph{\textbf{Model architectures.}}
We evaluated whether \prgname{} is generic by comparing the performance of each method using various model architectures trained on CIFAR-100 with 40\% asymmetric noise. As shown in \tablename~\ref{tab:modelcomparison}, \prgname{} obtains the highest accuracy while producing consistent results despite changes in architectures. DenseNet (L=25, k=12) had the smallest architecture, thereby yielding better performance than those of other models such as ResNet50, which suffered severe overfitting. These results suggest that \prgname{} can be reliably applied to different model architectures.


\begin{figure}[t]
\centering
\begin{minipage}[t]{.48\textwidth}
\centering  
    \includegraphics[width=\textwidth]{figures/5_sample_weights_cifar-10_40_asymmetric.pdf}
    \subcaption{CIFAR-10}
    \label{fig:sample-weights-10}
\end{minipage}\hspace{10pt}
\begin{minipage}[t]{.48\textwidth}
\centering
    \includegraphics[width=\textwidth]{figures/5_sample_weights_cifar-100_40_asymmetric.pdf}
    \subcaption{CIFAR-100}
    \label{fig:sample-weights-100}
\end{minipage}

\caption{Changes of weight distribution on clean and noisy samples with non-zero weights. }
\label{fig:sample-weights}
\end{figure}







\setlength{\tabcolsep}{12pt}
\begin{table}[t]
\hspace{1em}
\begin{minipage}[t]{.45\linewidth}
    \centering
    \caption{ Results of variations in sample weighting on CIFAR-100 with 40\% asymmetric noise.  }
    \label{tab:ablation}
    \medskip
    {\small \begin{tabular}{l|cc}
        \toprule
        Variations & Acc.[\%] \\
        \midrule
        None (all the same)                & 57.11.0 \\ 
        Low loss            & 52.41.0 \\
        High uncertainty       & 55.71.7  \\ \hdashline
        Low loss, high uncertainty               & \textbf{59.50.9} \\
        \bottomrule
\end{tabular}
    }\end{minipage}\hspace{1.5em}
\begin{minipage}[t]{.45\linewidth}
    \centering
    \caption{ Results on Clothing1M. }
    \label{tab:clothing1m}
    {\small \begin{tabular}{l|cc}
        \toprule
        Methods                & Acc.[\%] \\
        \midrule
Forward loss  \cite{patrini2017making}         & 69.84         \\
LCCN  \cite{yao2019safeguarded}                 & 71.63         \\
        Joint Optim.  \cite{tanaka2018joint}        & 72.16         \\
        DMI  \cite{xu2019l_dmi}                  & 72.46         \\
        MLNT  \cite{li2019learning}                 & 73.47         \\
        PENCIL  \cite{yi2019probabilistic}               & 73.49         \\ \textbf{\prgname{}} & \textbf{73.78}         \\ 
        \bottomrule
        \end{tabular}
    }\end{minipage}
\hspace{3.2em}
\end{table}


\subsection{ Empirical Analysis of Algorithm }
To comprehensively understand loss and uncertainty in our method, we conducted experiments on CIFAR-100 with 40\% asymmetric noise. \figurename~\ref{fig:sample-weights} displays how sample weights of \prgname{} change from soon after the warm-up stage (epoch 25) to convergence (epoch 75). 
We can observe that clean samples are allocated with larger weights, while noisy samples are allocated with smaller weights as training progresses. Moreover, the number of clean samples with non-zero weights increases, and the number of noisy samples, on the contrary, decreases (see also \figurename~\ref{fig:precision}). These results demonstrate our approach's effectiveness towards minimizing the impact of label noise and boosting the benefits of informative clean samples.

We also evaluated the weighting module for three approaches: placing larger weights on low loss, on high uncertainty, and treating every sample equally. As shown in \tablename~\ref{tab:ablation}, our weighting method outperforms other cases. Interestingly, giving equal weights achieved the best accuracy out of the three alternatives, while emphasizing samples with low loss led to the lowest accuracy. These results are parallel to \figurename~\ref{fig:asymmetric_noise_rate} in that \textit{Coteaching}, which focuses on low loss samples, performs worse than \textit{Active Bias}, which emphasizes high uncertainty samples. The detailed results of the ablation study is in Appendix D.

\subsection{Experiments on Clothing1M}
To further demonstrate our method’s effectiveness to realistic noise, we test on Clothing1M~\cite{xiao2015learning}, which comprises clothing data crawled from online shopping websites. Clothing1M consists of 1M images with real-world noisy labels and additional 50K, 14K, 10K verified clean data for training, validation and testing respectively. We retrain ResNet50 pretrained on ImageNet for 20 epochs using the 1M noisy dataset without any clean data in the training process. We use SGD with momentum of 0.9, , , , and  because the estimated noise rate is 38\%~\cite{huang2019o2u, yi2019probabilistic}. The initial learning rate is 0.002 and is decreased by 10 every 5 epochs. For preprocessing, we resize images to 256x256 and randomly crop 224x224 from the resized images. This dataset is greatly imbalanced so we randomly select a relatively balanced subset of up to 35,000 samples for each class. 

As shown in Table~\ref{tab:clothing1m}, our method achieves 73.8\% accuracy, which is higher than recent state-of-the-art methods. For fair comparison, we do not include methods using different backbone models or any clean data during training.
