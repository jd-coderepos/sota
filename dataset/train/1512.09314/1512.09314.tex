







\documentclass[11pt]{article}

\usepackage{amsfonts,amssymb,amsmath,latexsym,enumitem,ae,aecompl,color}

\textheight           	9in
\textwidth                	6.5in
\oddsidemargin         0pt
\evensidemargin     	0pt
\topmargin            	0pt
\marginparwidth   	0pt
\marginparsep         	0pt
\headheight          	0pt
\headsep           	0pt


\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newcommand{\F}{\vspace*{\smallskipamount}}
\newcommand{\FF}{\vspace*{\medskipamount}}
\newcommand{\FFF}{\vspace*{\bigskipamount}}
\newcommand{\B}{\vspace*{-\smallskipamount}}
\newcommand{\BB}{\vspace*{-\medskipamount}}
\newcommand{\BBB}{\vspace*{-\bigskipamount}}
\newcommand{\TT}{\hspace*{2em}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\newcommand{\poly}{\mathrm{poly}}

\newcommand{\Paragraph}[1]{\BBB\paragraph{#1}}

\newlength{\pagewidth}
\setlength{\pagewidth}{\textwidth}
\addtolength{\pagewidth}{-6em}

\newlength{\captionwidth}
\setlength{\captionwidth}{\textwidth}
\addtolength{\captionwidth}{-6em}

\newcommand{\qed}{\hfill  \smallbreak}
\newenvironment{proof}{\noindent{\bf Proof:}}{\qed}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

\begin{document}

\baselineskip          	3ex
\parskip                	1ex



\title{				Asynchronous Exclusive Selection \footnotemark[1]\vfill\vfill}


\author{			Bogdan S. Chlebus  	\footnotemark[2]   	
				\and
				Dariusz R. Kowalski 	\footnotemark[3]}
			

\footnotetext[1]{	A preliminary version of this paper appeared as~\cite{ChlebusK08}.}

\footnotetext[2]{		School of Computer and Cyber Sciences,
               			Augusta University,
               			Augusta, Georgia, USA.
Work supported by the National Science Foundation Grant No.~1016847.}


\footnotetext[3]{		School of Computer and Cyber Sciences,
               			Augusta University,
               			Augusta, Georgia, USA.
Work supported by the National Science Foundation Grant No.~2131538 and  the Polish National Science Center NCN grant UMO-2017/25/B/ST6/02553.}

\date{}


\maketitle

\vfill

\begin{abstract}

We consider the task of assigning unique integers to a group of processes in an asynchronous distributed system of a total of  processes prone to crashes that communicate through shared read-write registers.
In the Renaming problem, an arbitrary group of  processes that hold the original names from a range , contend to acquire unique integers in a smaller range~  as new names using some~ auxiliary shared registers.
We develop a wait-free -renaming solution, where both  and  are known, operating in  local steps, for , and with  auxiliary registers.
We give a wait-free  -renaming algorithm, where  is known,  operating in  local steps, with  and with   registers.
We develop a wait-free -renaming algorithm, where  known, operating in  time, for  and with  registers.
We give an adaptive wait-free solution of Renaming, where neither  nor  is known, having   as a bound on the range of new names, which operates in~ local steps and uses   registers.
As a  lower bound, we show that a wait-free solution to Renaming requires  steps in the worst case.
We apply  renaming algorithms to obtain solutions to Store\&Collect problem, which is about a group of  processes with the original names in a range  proposing individual values (operation \texttt{Store}) and returning a view of all proposed values (operation \texttt{Collect}), while using some  auxiliary shared read-write registers.
We show that if a known  is polynomial in~, then storing can be performed in  local steps and collecting in  local steps with  shared read-write registers. 
We consider a problem Mining-Names, in which processes may repeatedly request  positive integers as new names subject to the constraints that no integer can be assigned to different processes and the number of integers never acquired as names is finite in an infinite execution.
We give two solutions to Mining-Names in a distributed system in which there are infinitely many shared read-write registers available.
A non-blocking solution leaves at most  nonnegative integers never assigned as names, and a wait-free algorithm leaves at most  nonnegative integers never assigned as names.

\vfill

\noindent
\textbf{Keywords:} 
asynchrony, 
process crash,
read-write shared register, 
renaming, 
store and collect, 
non-blocking algorithm,
wait-free algorithm,
deterministic algorithm,
lower bound,
graph expansion.
\end{abstract}

\vfill

~

\thispagestyle{empty}

\setcounter{page}{0}
\newpage




\section{Introduction}




We consider asynchronous distributed systems consisting of some  processes that are prone to crashes and use read-write registers for inter-process communication.
The studied problems concern assigning positive integers to the processes in an \emph{exclusive} fashion, which means that no integer is assigned to two distinct processes.
We seek wait-free algorithms, and sometimes consider non-blocking ones.

When an integer  is assigned to a process~ exclusively, then we say that  is 's  \emph{new name}. 
In the Renaming problem, some  processes, each having an original name from a large range , contend to acquire unique integers in a smaller range~  as new names, using some  shared registers.
An algorithm can have some of the parameters  and  as a part of code.
We indicate which parameters are known as a part of code by attaching the relevant parameters to problems' names and solutions.
For example, an algorithm solving -renaming has both~ and~ as a part of code, while  and  and the time complexity are characteristics of the algorithm, given as functions of  and  and possibly also of~.
Similarly, an algorithm solving -renaming works for any range  of the original names and for up to  contending processes.
Finally, an -renaming algorithm handles the original names in the range  while the contention  is arbitrary, except for the restriction~.
An adaptive renaming algorithm works for any contention~ and range  of the original names, which are not parts of code, while the range of new names~ and the time performance are  functions of~, and the number of registers~ is a function of~, as this is the maximum value of  when .

We restrict our attention to \emph{one-time} renaming problems in which processes that will contend to acquire new names are designated at the start of an execution, and no new name is ever released and reused. 
Time performance is measure by the number of \emph{local steps}, which is a maximum number of steps a process takes before halting in a final state.

In the problem Store\&Collect, some  processes perform two operations \texttt{Store} and \texttt{Collect}.
The \texttt{Store} operation by a process~ proposes a value, and \texttt{Collect} results in returning a \emph{view}, which is a collection of pairs  where  is the original name of a process that proposed a value~ before the return of \texttt{Collect}, but not a stale one replaced before the invocation of \texttt{Collect}.
The semantics of this problem under asynchrony is well determined by referring to a collection of read-write registers, one register assigned to each process.
In order to propose a value, a process writes its original name and that value in its register.
In order to collect, a process reads once each register storing a pair consisting of a value proposed by a process and its name, in arbitrary order of registers, and includes each such a read pair in the view.

The problem Mining-Names is about a scenario in which processes repeatedly request new positive integers as names in an infinite execution.
There are two constraints on algorithms for the problem. 
One is that each process keeps an exclusive ownership of each acquired new name, to possibly build an infinite collection of new names in an infinite execution.
The other is to leave only finitely many positive integers never assigned as new names in an infinite execution. 
For a fixed integer~, no wait-free solution to Mining-Names can guarantee that  is eventually assigned as a new name.
It follows that some integers may never be used as new names when an algorithm works to assign as many positive integers as new names as possible.
We want to minimize the number of positive integers never assigned as names in an execution, so this number  is proposed as a measure of quality of a solution for Mining-Names.
The model of a distributed system we use to develop solutions for Mining-Names assumes finitely many processes but infinitely many shared read-write registers.


As a preparation to developing Mining-Names solutions, we show how  to implement a repository of infinitely many values using infinitely many  read-write registers. 
The values are generated in a dynamic fashion.
A value is considered as \emph{deposited} in a register when the value is stored in the  register and will never be overwritten.
In this problem, we strive to minimize the number of available registers  that never become used to store deposited values.
The problem is closely related to mining names, as new names can be used to identify registers to make deposits.





\Paragraph{Contributions of this paper.}


The renaming algorithms that we develop are designed to have processes traverse paths in graphs in which vertices represent names.
During such concurrent traversals, processes compete to acquire the names of the visited vertices.
We consider graphs with suitable expansion properties as means to makes algorithms efficient.
The approaches to renaming known in the literature, that have graphs built into algorithms in a similar manner, use graphs with  simple regular topologies, like constant-degree grids.

We develop a wait-free -renaming algorithm operating in  local steps, with a range of new names  and  auxiliary registers.
This is a first known deterministic algorithm with step complexity polylogarithmic in~ and a range of new names  linear in~, for~ that is polynomial in~. 



We show that   local steps are required in the worst case by any wait-free -renaming algorithm to assign names from a  range~ when using~ registers.
This is a first known lower bound on the local-step time performance of Renaming  that comprehensively involves all the four parameters , , , and~.
In particular, if  is unknown, and hence could be arbitrarily large, while  is suitably bounded as a function of~, then  is a lower bound; this resembles the lower bounds given by Jayanti et al.~\cite{JayantiTT00}.
An  lower bound on time, valid under additional assumptions, was proven by Alistarh et al.~\cite{AlistarhACGG14}   by a different argument.

We develop a wait-free -renaming algorithm with  local step complexity, for unknown contention , with the range of new names 
 and the number of registers . 
If a known~ is poly-logarithmic in~, then this renaming algorithm runs in  local steps and uses  auxiliary registers.

We develop a wait-free -renaming algorithm operating in  local steps, with a bound on new names  and with  auxiliary registers.
The time complexity of this algorithm is asymptotically optimal, which follows from the lower bound we show and the property that the algorithm works for arbitrary~.
This is the first algorithm known that has simultaneously two properties: the local step complexity is  and also the range of new names is~.
Among the previously known algorithms that run in time , the value  was smallest known; it is achieved by an algorithm of Moir and Anderson~\cite{MoirA95}.
The fastest algorithm known before, among those having , operates in  time, it was given by Attiya and Fouren~\cite{AttiyaF01}.
The value  is known to be the best possible size of a range of names for infinitely many values of~, as showed by Herlihy and Shavit~\cite{HerlihyS99}. 
The fastest algorithm known prior to this work with  as a bound on the range of new names runs in time , it was given by Afek and Merritt~\cite{AfekM99}.


We give a fully adaptive  renaming algorithm, with neither  nor  known, having a bound on the range of new names~ as small as .
The algorithm operates in~ local steps and uses  auxiliary registers.
This is an improvement with respect to time performance over the previously known algorithms.
The algorithm of Moir and Anderson~\cite{MoirA95} works in time  with a range of new names  and using  registers.
The algorithm of Attiya and Fouren~\cite{AttiyaF01} operates in  time with a range .
The algorithm of Afek and Merritt~\cite{AfekM99} runs in time  with a range of new names~.

We apply  renaming algorithms to obtain solutions to Store\&Collect of the following performing characteristics.
When both parameters  and  are known, then Store\&Collect can be implemented such that the first storing operation by a process takes   steps and collecting  steps, while using  auxiliary registers.
If  is known but  is not, then Store\&Collect can be implemented such that the first instance of storing takes  local steps and collecting  steps, with  auxiliary registers.
If the number of participating processes~ is known but the range of original names~ is not, then Store\&Collect can be implemented such that the first instance of storing takes  local steps and collecting  steps, with  auxiliary registers.
If  and  are both unknown, which is the adaptive case, then Store\&Collect can be implemented such that storing takes  steps and collecting takes  steps, with  auxiliary registers.
Afek and De Levie~\cite{AfekL07} gave an adaptive solution to Store\&Collect achieving storing in  local steps and collecting in  local steps, for~.


We consider the problem called Mining-Names, which is about processes working to claim nonnegative integers  as names in a mutually exclusive manner.
We show that Mining-Names is solvable in a non-blocking way such that at most  integers are never assigned as names, which is asymptotically best possible, and in a wait-free manner so that at most  values are never assigned as names.
The problem Mining-Names  has not been considered  before in the literature, by the knowledge of the authors of the paper.
Name mining  is related to ``depositing'' infinitely many values in read-write registers, where depositing means storing a value in a register such that it is never overwritten.
We give a non-blocking implementation of depositing in which at most  dedicated read-write registers are never used for depositing, and a wait-free implementation with the property that at most  dedicated deposit registers are never used for depositing.




\Paragraph{Related work.}



Now we describe the context of this work by reviewing research related to renaming.
We restrict our attention to asynchronous systems with shared memory consisting of read-write registers only; for a comprehensive survey of renaming see Alistarh~\cite{Alistarh15}.

The problem of renaming was introduced by Attiya et al.~\cite{AttiyaBDPR90} in the model of asynchronous message-passing. 
They showed that  processes may assign themselves new names from the range , where  is an upper bound on the number of crashes.
This established renaming as a non-trivial algorithmic problem with a wait-free solution for  environments in which Consensus cannot be solved; see~\cite{Attiya-Welch-book2004,HerlihyKozlovRajsbaum-book,Lynch-book96}.
The range of new names , with up to  crashes, was shown to be the smallest possible for renaming to be solvable by Herlihy and Shavit~\cite{HerlihyS99}.
Next we consider a scenario win which  contending processes with original names in a large range~ want to obtain new names in a small range~.
Borowsky and Gafni~\cite{BorowskyG92} gave a wait-free algorithm solving this problem in  time  for~.
Moir and Anderson~\cite{MoirA95} gave a solution with time complexity , for new names of magnitude  and using  registers. 
Attiya and Fouren~\cite{AttiyaF01} gave an algorithm working in time  for , and another of time complexity  for . 
Afek and Merritt~\cite{AfekM99} developed an algorithm working in time  for .

A  renaming solution is \emph{long-lived} when processes may invoke the operations to request a name and to release the current name repeatedly, as long as exclusiveness of each name holds within the interval from acquiring to releasing.
It is assumed that at most  processes contend for names concurrently.
The following is a selection of published long-lived renaming algorithms.
Burns and Peterson~\cite{BurnsP89} gave a solution of time complexity  , for  and .
Moir and Anderson~\cite{MoirA95}  improved the time to , for  and .
Further improvements were due to Buhrman et al.~\cite{BuhrmanGHM95}, who achieved  time, for  and the  number of registers , and to Moir and Garay~\cite{MoirG96} whose algorithm achieved  time complexity, for  and  registers, and who also gave another solution with  time, for  and .	

For other work on renaming, see the papers by Afek et al.~\cite{AfekAFST99,AfekBT00,AfekST02}, Brodsky et al. \cite{BrodskyEW06}, and Eberly et al.~\cite{EberlyHW98}.
Randomized renaming algorithms were considered by Alistarh et al.~\cite{AlistarhACGZ11, AlistarhAGW13, AlistarhAGGG10}.
Lower bounds on renaming were given by Alistarh et al.~\cite{AlistarhACGG14}, Attiya et al.~\cite{AttiyaCHP19,AttiyaP16,AttiyaR02}, Burns and Peterson~\cite{BurnsP89}, Casta\~neda et al.~\cite{CastanedaHR14}, Casta\~neda and Rajsbaum~\cite{CastanedaR10,CastanedaR12}, and Helmi et al.~\cite{HelmiHW14}.

Previous work on the problem Store\&Collect includes papers by Afek et al.~\cite{AfekST99}, Attiya et al.~\cite{AttiyaFK04,AttiyaF03,AttiyaFG02,AttiyaKPWW06}, Chlebus et al.~\cite{ChlebusKS-STOC04} and Saks et al.~\cite{SaksSW91}.
Previous work on models with infinite arrivals of processes and infinitely many shared registers includes~\cite{Aguilera04,AspnesSS02,ChocklerM05,GafniMT01,MerrittT03,MerrittT13}.





\section{Technical Preliminaries}

\label{sec:technical-preliminaries}


Algorithms are executed in an asynchronous system with  processes prone to crashes and a set of read-write registers.
Each process~ is identified by its \emph{original name} \texttt{name}, which is a unique number in some range of names .

If a parameter of a distributed system can be used in a code of algorithm then this parameter is \emph{known}.
In particular, each process~ knows its original name, which is referred to by a specialized variable in codes of algorithms, say \texttt{name}, with process~ substituting \texttt{name} as its private value.
We assume throughout that the number  is known.

The following is a standard terminology regarding delays of enabled operations; 
see~\cite{Attiya-Welch-book2004,HerlihyKozlovRajsbaum-book,Lynch-book96} for expositions of these and related concepts.
When, for any configuration in an execution, some process will eventually complete an invoked operation, then the executed algorithm is \emph{non-blocking.}
When, for any configuration in an execution, each process will eventually complete an invoked operation, even when all the remaining processes have crashed, then the algorithm is \emph{wait-free}.






\Paragraph{Competing for registers.}



We introduce a procedure used by processes to compete for a shared register.
The procedure  has two two properties.
First is that a lack of contention guarantees wining.
This means that if there is exactly one process~ working to win a register~, then  eventually wins~.
The second property is that a win provides exclusivity.
This means that once some contender wins a register~, then no other contender will ever win~.
This specification does not require a register to be won by a process when there are multiple contenders but also does not preclude this.

To implement a competition for a register~, we use an auxiliary dedicated shared register~ initialized to \texttt{null}.
This register~ is used as a placeholder to store a reservation for~.
A pseudocode of a procedure for  is given in Figure~\ref{fig:procedure-competition}.




\begin{figure}[t]
\hrule

\FF

\textsf{procedure} \textsc{Compete-For-Register\,}

\FF

\hrule

\FF

\begin{enumerate}[nosep]
\item
read: 

\item
\texttt{if}  \texttt{then} write  \texttt{else} \texttt{exit}

\item
read: 

\item
\texttt{if}  \texttt{then} write  \texttt{else} \texttt{exit}

\item
read: 

\item
\texttt{if}  \texttt{then return win else exit}

\end{enumerate}

\FF

\hrule

\FF

\caption{\label{fig:procedure-competition}
Pseudocode for a process~ with name \texttt{name} to win a shared register . 
It uses a private variable~ and a shared register  associated with . Both registers  and  are initialized to \texttt{null}.}
\end{figure}




\begin{lemma}
\label{lem:compete-for}

Procedure \textsc{Compete-For-Register} is an implementation of competition for a register.
\end{lemma}


\begin{proof} 
To show correctness, consider two cases corresponding to the specification during a competition for a register~.
If a process~ acts as the only contender to win~, then   eventually writes the value  to both registers  and , and the final  read from  makes process~ a winner.
Now suppose some process~ wins~ in the presence of a contender process~.
If the first read of~ by~ does not return \texttt{null}  then  exits immediately and no longer contends to win~. 
Otherwise, process~ reads \texttt{null} from~ as its first action, which means that  process~ managed to read from register~ before process~ wrote to~.
Still process~ wins , so  managed to write \texttt{name} to~ and then the same value to~ and then check that \texttt{name} is still in register~, as all this is required to win~.
When process~ confirms by the second read of~ that  is still there then  already stored .
So process~ could overwrite the value \texttt{name} at register~ only after process~ read it for the second time, which means after register~ has the value \texttt{name} stored in it already.
When process~ reads register~ then this occurs after process~ wrote  to it, so the read returns , which is different from \texttt{null}. 
This makes process~ exit without invoking a write to~.
\end{proof} 




\Paragraph{Graphs.}



Let  be a simple bipartite graph.
This notation means that the vertices are partitioned into the set of \emph{inputs~}  and the set of \emph{outputs~},  is the set of edges, and each edge has one endpoint in  and the other in~.
We say that graph  has \emph{input-degree } if each vertex in~ is connected to exactly  neighbors in~. 
A graph~ is said to be an \emph{-lossless expander}, for a natural number , if  is the input-degree of  and each subset  of  of size  has at least  neighbors in~.
A vertex  is a \emph{unique neighbor of set } if vertex~ is adjacent to exactly one vertex in~.



\begin{lemma}[\cite{CapalboRVW02}]
\label{lem:from-CapalboRVW02}

Let  be a -lossless expander, for some parameters  and .
Then, for each subset  of size , at least the fraction  of vertices among the neighbors of~ are unique neighbors.
\end{lemma}

\begin{proof}
Fix an ordering of the sets of vertices~.
For , let  denote the th neighbor of~ in~.
There are  pairs of the form , for  and .
By the definition of lossless expanders, all these pairs determine at least  neighbors of .
It follows that at most  such pairs denote vertices repeated at least twice. 
Each such a repetition  eliminates two possible unique neighbors of  per one real neighbor , in the total number  of pairs of the form , for  and .
\end{proof}



\begin{lemma}
\label{lem:partial-matching}

If a bipartite graph  is an -lossless expander, for some numbers~ and , then, for each set  of size , there is a partial matching in~, between some vertices in  and  unique neighbors of~, that has at least  edges.
\end{lemma}

\begin{proof}
By Lemma~\ref{lem:from-CapalboRVW02}, a fraction of at least  vertices among the neighbors of~ are unique neighbors.
We can match these inputs to their unique neighbors.
\end{proof}

Let  denote the logarithm of  to the base~, and  be the base of natural logarithms.
We will use the existence of lossless expanders with the following properties:



\begin{lemma}
\label{lem:expander}

Let   and  be two finite disjoint sets and  a natural number such that .
If  then there exists a bipartite graph  of input-degree  such that  and  is a -lossless expander.
\end{lemma}


\begin{proof} 
We show that a set of edges between the vertices in  and  selected randomly subject to degree constraints meets the requirements with a positive probability.
More precisely, for each vertex , we select   neighbors in~ uniformly at random.
Next, we demonstrate that the resulting graph is  a -lossless-expander with a probability greater than~, where a specific value of  will be determined later.

Let   be  a subset of  elements, and  be a subset of  elements.
The probability that all the neighbors of  are in the set  is at most

where we used the bounds .
The number of different subsets  of size  is at most

The number of different subsets  of size  is at most

Therefore, the probability that there exists a set  with at most  neighbors, for a given size~,  is at most 

We assumed .
Now, define .
Observe that , because . 
The assumption on  and the specification fo  produce cancellations that give:

To estimate~\eqref{eqn:expander-exist-one} further, observe that the quantity raised to the power~ in~\eqref{eqn:expander-exist-one} equals:

Let us introduce the notation  and .
By the assumptions, we have  and .
The right-hand side of~\eqref{eqn:expander-exist-two} can be represented as

This quantity is at most , for  and .
It follows that the right-hand side of~\eqref{eqn:expander-exist-one} is at most .

We demonstrated that the probability that some subset  of size  has at most  neighbors in~ is at most .
The probability that some  subset  of size  has at most  neighbors in~ is at most .
By the probabilistic method, there exists a bipartite graph  with input-degree  in which every subset  of size  has more than  neighbors in~, where the number  satisfies .
\end{proof} 






\section{Bounded Selection}


We consider the problem of assigning new names to a group of participating processes, from among the total of , known as \emph{renaming}.
Each among  processes holds an \emph{original} name from some range , which is the value of its variable \texttt{name}.
The number of participating processes is denoted as , where .
These participating processes contend to acquire unique integers in a range~  as new names using some~ auxiliary shared registers, where . 
The goal is to minimize a number of parameters: the running time and a range  of new names, but also the number of auxiliary registers .
Running time means local steps, which is a maximum among the processes of the number of time-steps counted by each process until halting. 

We assume that  is known, but whether  and  are known depends on a precise specification of the renaming problem.
This leads to four variants of the problem of renaming, where either (1)~both  and  are known, or (2)~only  is known, or (3~ only  is known, and finally (4)~with none of  and  known.
The case of both  and  unknown is most challenging.
Algorithms for renaming that do not refer to either  nor  in their codes, and so work for arbitrary unknown values of  and , are called \emph{adaptive}.
We apply the convention to add known parameters among  and  to names of algorithms, so that if a parameter  is missing in the name then this means the parameter is unknown.


Our first goal is to develop a renaming algorithm with both  and  known.
We begin by introducing an auxiliary problem called Majority-Renaming, which is about assigning new names to at least half of some  contending processes.
An algorithm is \emph{-majority renaming with a bound~ on new names} if at least half of any  contending processes with original names in~ acquire unique names in~, while the numbers  and~ can be part of code of the algorithm.
We find a solution for Majority-Renaming based on lossless expanders with good unique-neighbors properties.
This becomes a stepping stone to develop solutions for Renaming itself.
Employing a renaming algorithm that relies on some known information, we then argue how to obtain an adaptive solution.
Finally, we discuss how to use the obtained renaming algorithms to solve Store\&Collect.

We begin by presenting an algorithm called \textsc{Majority}, where  is a natural number and , which is -majority renaming.
The number  serves as a bound on the range of new names.
We consider a bipartite graph , where , , and each input-degree is~.
The topology of  is such that  satisfies the properties given in Lemma~\ref{lem:expander}.
The graph  is part of code of the algorithm.
The set  of inputs corresponds to all  original names of processes.
Each output vertex in the set~ represents a possible new name. 

The edges of  determine which registers will be competed for by the processes, using procedure \textsc{Compete-For-Register} given in Figure~\ref{fig:procedure-competition}.
To facilitate this, there are two unique shared registers associated with each output vertex.

Competition to win registers in an execution of algorithm \textsc{Majority} proceeds as follows.
A process~ with a name in  uses a vertex  in~ labeled .
It begins by attempting to win a register corresponding to the first neighbor of  in~.
If  fails then it attempts to win the register of its second neighbor in~.
This continues until  either wins a register or exhausts all the neighbors. 
As soon as  wins a register in~ then  adopts the number of the won register as its new name and exits.
If  fails all  competitions, then  halts without acquiring a new name.




\begin{lemma}
\label{lem:majority}

Algorithm \textsc{Majority},  for a natural number  and , is -majority-renaming, where  is a  bound on new names.
The algorithm operates in~ local steps and uses  auxiliary registers.
\end{lemma}


\begin{proof} 
The graph~ is a -lossless-expander, as stated in Lemma~\ref{lem:expander}.
A majority of contending processes have unique neighbors not shared with other active processes, by Lemma~\ref{lem:partial-matching} applied to graph~.
If an active process has a unique neighbor then it eventually wins some register representing its neighbor, by Lemma~\ref{lem:compete-for}. 
Hence a majority of active processes get unique names.
The worst-case running time is proportional to the degree  of graph~, which is . 
The number of used registers is~, as we use two unique shared registers per one vertex in~. 
\end{proof} 




\Paragraph{Renaming when both  and  are known.}


Next, we consider an algorithm \textsc{Plain-Rename}, which is  -renaming with  as a bound on new names.
A process~ proceeds through consecutive stages,  up to  stages maximum,  until it gets a unique name. 
In a stage~, where , process~ executes \textsc{Majority} on the set of pairs of registers~, where .
We assume that the sets  are mutually disjoint.
A union of these sets~ constitutes a collection of new names.



\begin{lemma}
\label{lem:Plain-Rename}

Algorithm \textsc{Plain-Rename} is -renaming with  as a bound on new names.
It  operates in~ local steps and uses  auxiliary registers. 
\end{lemma}

\begin{proof}
Procedure \textsc{Majority} is majority renaming, by Lemma~\ref{lem:majority}.
Each call of this procedure at least halves the number of contending processes that still need names.
Calls of the procedure continue until there remain at most one process without a new name, which then eventually also gets a name. 
This takes  local steps in total,  by Lemma~\ref{lem:majority}.
Number  is a bound on the number of stages.
The size of a pool of new names can be estimated as follows: 

The number of  shared registers needed for this to work correctly is .  
\end{proof}



Next, we consider an -renaming algorithm called \textsc{Compact-Rename}. 
It is an improvement over \textsc{Plain-Raname} in that the range of new names is~.

An execution of \textsc{Compact-Rename} is structured as a sequence of epochs.
A process~ participates in consecutive epochs, in each one getting a new name.
At least one epoch is executed as long as , otherwise the original names serve as new names without any change.
The original names are used in the first epoch, and then the names acquired in epoch  are used as original names in epoch .  
This continues until the upper bound on the range of the new names, determined by the properties of a current epoch, becomes less than~.
A process acquires its ultimate name during this last epoch.

In epoch~, process~ executes \textsc{Plain-Rename}, where  and  are bounds on new names in calls of \textsc{Plain-Rename}, for .
The executions of instantiations of algorithm \textsc{Plain-Rename} use sets of shared registers dedicated for each epoch, such that a shared register is used in only one epoch.
Processes use names from the range  in epoch ,  with .
The names assigned in epoch~ are from the range .

The algorithm \textsc{Compact-Rename} terminates because the ranges of new names shrink with passing epochs: , for .
We evaluate the rate of shrinking next.



\begin{lemma}
\label{lem:shrinking-rate}

If  then , and as long as  then ,  for .
\end{lemma}

\begin{proof}
The case of :

The case of :

for .
\end{proof}

Define a sequence :  for .




\begin{lemma}
\label{lem:sequence-a}

If  then .
\end{lemma}

\begin{proof}
The sequence  satisfies the following recurrence, for :

The inequality  holds for , by inspection.
Substituting  for , we obtain that  for , which holds for .
\end{proof}

We use the iterated-logarithm function , which denotes  iterated  times.
A recursive definition of this function reads  and .
We also refer to , for .



\begin{theorem}
\label{thm:compact-rename}

Algorithm \textsc{Compact-Rename} is -renaming with  as a bound on new names,  assuming .
It operates in  local steps and uses  auxiliary registers. 
\end{theorem}

\begin{proof}
We assume  that , as otherwise no epoch is executed.
The ranges of names used through the epochs can be traced back to  as follows.
If  then 

as long as , by Lemma~\ref{lem:sequence-a}.
Combining this with Lemma~\ref{lem:shrinking-rate}, we obtain the bound

 for .
The final range of new names  satisfies .
The number of epochs is . 


The number of local steps can be estimated as follows:

by Lemma~\ref{lem:Plain-Rename} and the bound  on the number of epochs  .

The first epoch uses  registers.
A number of registers used in subsequent epochs is given by Lema~\ref{lem:Plain-Rename}.
These numbers keep decreasing, with a rate determined Lemmas~\ref{lem:shrinking-rate} and~\ref{lem:sequence-a}.
By combining this together we obtain that the number of needed registers  is

The estimate on the rate of decreasing of  given in Lemma~\ref{lem:sequence-a} applies for all but  epochs.
It follows that the number of needed registers is .
\end{proof}

The knowledge of  and range  that is polynonomial in  allows to obtain a renaming algorithm efficient with respect to the total number of processes~.



\begin{corollary}
\label{cor:known-k-and-N-rename}

If  and  are known and , then algorithms \textsc{Compact-Rename} runs in  local steps and uses  auxiliary registers. 
If  and  are known and  is polynomial in , then algorithm \textsc{Compact-Rename} runs in  local steps and uses  auxiliary registers. 
\end{corollary}

\begin{proof}
Algorithm \textsc{Compact-Rename} needs  local steps, by Theorem~\ref{thm:compact-rename}.
This bound is  if  , and it is   if  is polynomial in~.
Algorithm \textsc{Compact-Rename} uses  auxiliary registers, by Theorem~\ref{thm:compact-rename}.
If a known~ is such that  then use , which holds for , to obtain the bound  on the number of registers.  
If a known  satisfies , for , then the bound on the number of registers becomes .
\end{proof}



\Paragraph{Renaming when  is known while  is not.}


We present an algorithm  \textsc{Range-Rename}, which renames  contending processes assigning names of magnitude .
A bound~ on the magnitude of original names is known but the number of participating processes  is unknown. 

The algorithm is specified as follows.
A process participates in consecutive executions of algorithms \textsc{Compact-Rename}, for consecutive integers , until it obtains a new name. 
After the first execution is over, it starts a new execution only after the previous one has failed to assign a new name.
These consecutive executions use disjoint  sets of registers.
An execution of \textsc{Compact-Rename}, for , uses a next  interval of integers as a range of new names, following the intervals used by \textsc{Compact-Rename}, for . 
This means the size of interval used by \textsc{Compact-Rename} is , as indicated by Theorem~\ref{thm:compact-rename}, but it is of the form , for a suitable positive integer~ that is greater than~, except for .




\begin{theorem}
\label{thm:range-rename}

Algorithm \textsc{Range-Rename} is -renaming.
It assigns new names of magnitude , runs in  local steps, and uses  auxiliary registers.
\end{theorem}

\begin{proof}
At most  processes participate in each execution of \textsc{Compact-Rename}. We have that  , because as soon as  then each process acquires a new name.
The size of the range of new names is estimated by Theorem~\ref{thm:compact-rename} to be at most 

The number of steps through executing  \textsc{Compact-Rename} follows from Theorem~\ref{thm:compact-rename}:

The number of registers follows from Theorem~\ref{thm:compact-rename}:

 since .
\end{proof}


The renaming algorithms for the case when  is known use few auxiliary registers if  is   polynomial in~.




\begin{corollary}
\label{cor:only-known-N-rename}

If a known range of the original names~ is polynomial in , then algorithm   \textsc{Range-Rename} runs in  local steps and uses  auxiliary registers. 
If a known range of the original names  satisfies , then algorithm  \textsc{Range-Rename} uses   auxiliary registers. 
\end{corollary}

\begin{proof}
Algorithm \textsc{Range-Rename} runs in  local steps, by Theorem~\ref{thm:range-rename}.
The time bound becomes  if  is polynomial in~, since .
Algorithm \textsc{Range-Rename} uses  auxiliary registers, by Theorem~\ref{thm:range-rename}.
If  then the number of registers is , and if , for , then the number of registers is .
\end{proof}




\Paragraph{Renaming when  is known while  is not.}


We design an algorithm \textsc{Square-Rename}, where a range  is unspecified.
The algorithm assigns new names with  as a range of new names.

The algorithm refers to three other algorithms, some for the case when both parameters  and  are known.
Let \textsc{MA} be an adaptive algorithm given by Moir and Anderson~\cite{MoirA95}, which is -renaming with  as a  bound on new names.
It operates in  local steps and uses  auxiliary registers.
Let \textsc{AF} be the algorithm of Attiya and Fouren~\cite{AttiyaF01}, which is -renaming with  as a bound on new names.
It operates in  local steps and uses  auxiliary registers.
We use algorithm \textsc{Compact-Rename} together with algorithms \textsc{AF} and \textsc{MA} to obtain a new algorithm called \textsc{Square-Rename}, which is -renaming with  as a bound on new names, for any  and .


Algorithm \textsc{Square-Rename} is structured into three parts.
The sets of registers used in all three parts are disjoint.
First run algorithm \textsc{MA} to rename, with  as a  bound on new names, for some .
Continue by invoking \textsc{Compact-Rename} to rename again, with  as a  range of new names, by Theorem~\ref{thm:compact-rename}.
The processes execute \textsc{Compact-Rename} with the names obtained in the preceding execution of \textsc{MA} treated as original names. 
Finally, execute \textsc{AF} to rename one more time.
The processes execute \textsc{AF} with names obtained in \textsc{Compact-Rename} treated as original names. 
The final new names are as assigned by \textsc{AF}.



\begin{theorem}
\label{thm:square-rename}

Algorithm \textsc{Square-Rename} is -renaming with  as a bound on new names.
It operates  in~ local steps and uses  auxiliary registers.
\end{theorem}

\begin{proof}
We rely on the properties of algorithm~\textsc{MA} from~\cite{MoirA95}, and on the properties of  algorithm~\textsc{AF} given in~\cite{AttiyaF01}.
Correctness follows from the fact that each of the three renaming algorithms properly handles original names, possibly yielded by a preceding execution, assuming they are given correct  ranges of original names, if they rely on  this information.
The range of names is reduced first  to  by algorithm~\textsc{MA}, for a suitable .
Then algorithm~\textsc{Compact-Rename} takes over, with , and reduces the range of names to , by Theorem~\ref{thm:compact-rename}. 
Finally, algorithm~\textsc{AF} reduces the range of names to , while using .


The local step complexity of algorithm \textsc{Square-Rename} is  

by the properties of  algorithm \textsc{MA} in~\cite{MoirA95}, and by these of  algorithm~\textsc{AF} given in~\cite{AttiyaF01}, and by Theorem~\ref{thm:compact-rename}.
The number of needed registers is at most

by the respective properties of algorithms \textsc{MA} and \textsc{AF}, and by Theorem~\ref{thm:compact-rename}.
\end{proof}




\Paragraph{Adaptive renaming with both  and  unknown.}



Now we develop algorithm \textsc{Adaptive-Rename} solving Renaming  in a fully adaptive fashion.
The algorithm uses \textsc{Square-Rename} as a subroutine.
It operates as follows.
A process~ executes instantiations of algorithm \textsc{Square-Rename}, for consecutive integers . 
If  does not obtain a new name in an execution of \textsc{Square-Rename}, then it proceeds to execute \textsc{Square-Rename}.
Once a process~ acquires a new name, then it halts.
Executions of \textsc{Square-Rename} for different values of  use dedicated sets of auxiliary registers assigned to each possible integer value~ and also dedicated ranges of names assigned to .
These ranges are disjoint, and a range for  immediately follows a range for , such that the range for  such that  fill a contiguous segment.
Algorithm \textsc{Adaptive-Rename} does  not have the parameters  and  in its code.



\begin{theorem}
\label{thm:adaptive-rename}

Algorithm \textsc{Adaptive-Rename}  solves Renaming  in an adaptive manner. 
The range of new names is .
The number of local steps is~ and the number of auxiliary registers is~.
\end{theorem}

\begin{proof}
Consider an instantiation of \textsc{Square-Rename} for , which is the latest possible. 
At most  processes participate in this execution.
By Theorem~\ref{thm:square-rename}, the magnitude  of new names is bounded above by 

The number of local steps of each process is bounded from above by 

by Theorem~\ref{thm:square-rename}.
The number of needed auxiliary registers used is  

again by Theorem~\ref{thm:square-rename}. 
\end{proof}


There is an alternative adaptive solution for Renaming, which works as follows.
First execute an adaptive version of  algorithm \textsc{MA}.
It accomplishes renaming in  local steps and  new names using  registers.
Follow by executing algorithm \textsc{Range-Rename}.
By Theorem~\ref{thm:range-rename}, this solves Renaming  in  local steps while using  registers.
A drawback of this algorithm is that the range of new names, although still , has a large constant factor by~.
This is not the case for the algorithm \textsc{Adaptive-Rename}, where the range of names is smaller than .





\Paragraph{Solutions for Store\&Collect.}



We show how to implement the \texttt{Store} and \texttt{Collect} operations, with  processes out of  participating, with original names in the range~.
We want the first \texttt{Store} by a process to begin by executing a suitable renaming algorithm.
A new name identifies a register which the process uses to store by writing into it.
Each of the subsequent calls of \texttt{Store} takes a constant number of local steps, because the register to write to has already been identified 
The algorithms for Store\&Collect are categorized by the levels of knowledge of  and , with  always known.


A number~ is a range of new names in a renaming algorithm.
If  can be computed in advance, based on the available knowledge of the numbers  and , then an algorithm can be structured as follows.
We allocate  shared read-write registers  indexed by natural numbers~ in~.
In order to store a value, a process first seeks a new name~ in~.
This assigns a unique read-write register~.
The process with a new name~ stores its values by writing into~.
Collecting is performed by reading the registers~, for all .
This approach works when both parameters~ and~ are known.



\begin{theorem}
\label{thm:collect-k-N-both-known}

If both  and  are known, then Store\&Collect can be implemented such that a first storing operation by a process takes   steps and collecting  steps, while using  auxiliary registers.
\end{theorem}

\begin{proof}
Since we use algorithm \textsc{Compact-Rename} for renaming, it is Theorem~\ref{thm:compact-rename} that summarizes the performance characteristics of renaming. 
The number of registers to store values corresponding to the new names is~.
The first \texttt{Store} operation of a process is preceded by acquiring a new name.
This takes   local steps.
The \texttt{Collect} operation consists of reading all the  shared registers, which takes time .
The algorithm uses  registers for renaming and  registers for storing, which together make  auxiliary registers.
\end{proof}


The knowledge of  and a range of original names~ that is polynomial in~ allows to obtain a Store\&Collect algorithm efficient with respect to the total number of processes~.



\begin{corollary}
\label{cor:store-collect-both-k-and-N-known}

If both  and  are known and   then Store\&Collect can be implemented such that a first storing operation by a process takes   local steps  and collecting  steps, while using  auxiliary registers.
If both  and  are known and   is polynomial in~ then Store\&Collect can be implemented such that a first storing operation by a process takes   local steps  and collecting  steps, while using  auxiliary registers.
\end{corollary}

\begin{proof}
This is a specialization of the solution of Store\&Collect when both  and  are known with performance summarized in Theorem~\ref{thm:collect-k-N-both-known}.
Since we use algorithm \textsc{Compact-Rename} for renaming, we refer to Corollary~\ref{cor:known-k-and-N-rename} that summarizes it performance characteristics with additional assumptions on the magnitude of~.
\end{proof}



Next, we consider the case when  is known but  is unknown.
These assumptions qualify algorithm \textsc{Range-Rename} for renaming to be applicable.
This algorithm assigns new names that are in the range between~ and~, if  processes contend for new names, by Theorem~\ref{thm:range-rename}.
A solution to Store\&Collect needs to have at least~ registers allocated in advance, since the unknown  could be as large as~.

In order to read only the registers actually used for storing, while performing \texttt{Collect}, we suitably mark an initial range of registers during writing into them not to waist time reading unused registers.
This is implemented as follows.
The number~ of registers needs to be incremented by . 
These many registers are identified by the integers in the range  as their \emph{primary addresses}, in that the th register has primary address~.
All these registers are initialized to be null as usual. 
The registers with primary addresses of the form , for , play an \emph{auxiliary} role.
If a value written in such a register is different from null then the register is \emph{marked}. 
The registers with primary addresses between  up to  have secondary addresses between  and , for .
Once can verify directly that each register is either auxiliary or it has a unique secondary address, and each integer between  and  is a secondary address of a unique register. 

A process with a new name  stores its proposed value along with its original name in a register of the secondary address~.
During its first operation \texttt{Store}, the process marks all the auxiliary registers whose primary addresses are less than the primary address of the register with  as the secondary address.
In order to perform \texttt{Collect}, a process reads the consecutive registers up to the first unmarked auxiliary register or all the registers if all auxiliary registers are marked.
An unmarked auxiliary register indicates that no value has been stored beyond this primary address.

\begin{theorem}
\label{thm:collect-only-N-known}

If  is known but  is not, then Store\&Collect can be implemented such that a first instance of storing takes  local steps and collecting  steps, with  auxiliary registers.
\end{theorem}

\begin{proof}
As a renaming subroutine, we use  \textsc{Range-Rename}, and refer to Theorem~\ref{thm:range-rename}, which summarizes the performance characteristics of the algorithm. 
The number of registers to store values corresponding to the new names is~, because the range of new names is  with .
The first \texttt{Store} operation of a process takes time  to identify a new name and then up to  steps to mark auxiliary registers and store a proposed value.
Each of the subsequent \texttt{Store} operations takes constant time.
The \texttt{Collect} operation consists of reading up to  auxiliary registers and up to the  registers with the smallest secondary addresses.
The algorithm uses  auxiliary registers for renaming and  registers for storing, which together make  registers.
\end{proof}

The  algorithms for Store\&Collect for the case when  is known use few auxiliary registers if  is  polynomial in~.



\begin{corollary}
\label{cor:store-collect-N-known}

If a known range of the original names  is such that , then Store\&Collect can be implemented using  auxiliary registers.
If a known range of the original names  is polynomial in , then Store\&Collect can be implemented using  auxiliary registers. 
In each of these cases, a first operation of storing can be performed in a number of local steps poly-logarithmic in~ and collecting takes  steps.
\end{corollary}

\begin{proof}
This is a specialization of the solution of Store\&Collect when  is known but  is not with performance summarized in Theorem~\ref{thm:collect-only-N-known}.
Since we use algorithm \textsc{Range-Rename} for renaming, we refer to Corollary~\ref{cor:only-known-N-rename} that summarizes it performance characteristics with additional assumptions on the magnitude of~.
\end{proof}

Next we consider the case when  is known but  is not.
We want to use the algorithm \textsc{Square-Rename} for renaming.
By Theorem~\ref{thm:square-rename}, the range of new names can be determined as  and renaming requires  auxiliary registers.
A solution to Store\&Collect needs  shared registers for renaming, for a suitable , and  registers for storing.



\begin{theorem}
\label{thm:collect-only-k-known}

If the number of participating processes~ is known but the range of original names~ is not, then Store\&Collect can be implemented such that a first instance of storing takes  local steps and collecting  steps, with  auxiliary registers.
\end{theorem}

\begin{proof}
The design of the implementations of \texttt{Store} and \texttt{Collect} follows the general approach of having a dedicated block of  registers for storing and each participating process first acquiring a new name to identifies a register.
By Theorem~\ref{thm:square-rename}, this takes  local steps while using  auxiliary registers.
To collect the proposed values to build a view, a process reads all  registers used for storing, which takes  local steps.
\end{proof}

Next we consider the adaptive case with both  and  unknown.
We want to use algorithm \textsc{Adaptive-Rename} for renaming. 
According to its performance characteristics summarized in Theorem~\ref{thm:adaptive-rename}, the range of new names is  and the number of auxiliary registers is~.
Registers used for storing can be handled similarly as in the case of unknown  and known~, by arranging a suitably large segment of registers and using them by referring to primary and secondary addresses.
We need  secondary addresses, so a block of registers with  primary addresses suffices.
Additionally, we need to allocate  registers for renaming, for a suitable .


\begin{theorem}
\label{thm:collect-3}

If  and  are both unknown, then Store\&Collect can be implemented adaptively such that first storing takes  steps and collecting takes  steps, with  auxiliary registers.
\end{theorem}

\begin{proof}
We use  \textsc{Adaptive-Rename} for renaming purposes.
The total number of auxiliary registers is , with  for storing and  for renaming. 
The first store operation of a process takes  local steps, by Theorem~\ref{thm:adaptive-rename}.
The subsequent store operations are of constant time each.
The collecting operation takes  steps, since there is only a prefix of  registers corresponding to the names to be read. 
\end{proof}

An adaptive solution of Store\&Collect with performance as in Theorem~\ref{thm:collect-3} has already been given by Afek and De Levie~\cite{AfekL07}, by using a different approach.





\section{Lower Bounds on Local Steps}





We consider the time complexity of renaming and storing for the first time.
A \emph{configuration} of the system consists of events enabled by each process, which are either reads or writes. 
An \emph{execution} consists of events as they occur in time.
An event that occurs is selected from a current configuration. 
A process participating at a current event immediately enables either a read or a  write to contribute an option for such a read or write to occur in the next configuration.




\Paragraph{A lower bound for renaming.}


For a distributed system of  processes, an instance of Renaming  is determined by some of the following four  parameters: an upper bound on the number of participating processes~, a range of new names~, a range of original names~, and a number of auxiliary shared read-write registers~.
For some configurations of these parameters, the number of steps can be very small; for instance,  if  then the original names could do, so the number of steps is~. 
On the other side of the spectrum, if  is not known and can be arbitrarily large, then the step complexity of any renaming algorithm is .

An intuition why the number of steps is  in some scenarios could be build as follows. 
If the original names affect the actions of processes, then there is such an assignment of the  original names that at any point of an execution, if there is a choice that processes can make which is affected by the original names, then all the processes might choose similarly.
In particular, if processes choose not to write, so that there is no communication among them, then all want to assign the same name.
Therefore, one of the processes writes at some point, and let  be the first such a process.
After a write, some processes learn of the value written, by reading. 
All processes have to learn the value written by~, since otherwise one process among these that never learn could choose the same name as~.
This argument can be extended by induction to imply that the process that chooses the name as the last one had to read at least  times.
A general lower bound of Jayanti et al.~\cite{JayantiTT00} captures related scenarios.

We present a lower bound which gives an estimate on , depending on fixed range of original names~ and the number of auxiliary shared read-write register~, for which  steps during assigning new names are necessary.
The lower bound is flexible enough to be applicable to scenarios when algorithms of poly-logarithmic step complexity exist.




\begin{theorem}
\label{thm:rename-lower-bound}

Let  processes among a total of  in an asynchronous system using  shared read-write registers and with original names in the range  execute a wait-free renaming algorithm that assigns new names in a range . 
Then there exists an execution in which some process performs  steps.
\end{theorem}


\begin{proof} 
Consider a conceptual set consisting of  processes, each identified by a different original name.
The first goal is to identify a subset~ of these processes consisting of at most~ elements so that an execution of the algorithm by these processes results in a necessary number of local steps. 
The construction of the set~ is recursive and proceeds through a sequence of stages.

A stage represents a group of concurrent reads or writes to shared registers.
A stage determines groups of processes we call \emph{pool} and \emph{residue}.
When the stages get completed, the pool and the residue together make a set~ we seek.
In notation, let \emph{stage } result in determining a \emph{pool set~} of processes eligible to be considered for stage , a \emph{residue set~}, by determining a prefix~ of an execution.
The construction starts with the initial pool~ consisting of  conceptual processes, each identified by its  original name, the initial residue  is empty, and  is  an empty prefix of an execution.


Suppose we have a configuration with a determined sets  and  and a prefix  of the execution.
Let  consist of the processes in  that have a read enabled in the configuration and  consist of the processes in  that have a write enabled.
There are two cases we considered next that determine , , and .

Suppose first that .
By the pigeonhole principle, there is a register which a group of at least these many processes want to read:
 
Define the pool  to be this group of processes and the residue  to be equal to~. 
The prefix  is obtained from  by having the processes in  read the register in arbitrary order. 

Suppose next that .
By the pigeonhole principle, there is a register to which  a group of at least these many processes want to write to:
 
Define the pool  to be this group of processes.
The prefix  is obtained from  by having the processes in  write to the register in arbitrary order. 
A process~ that writes last in this group is added to  to obtain .

As the recursive construction progresses, for , the following invariant is maintained:
\begin{enumerate}
\item[1)]
the pool  includes at least  processes, 
\item[2)]
all the processes in~ have exactly the same history in~ of reading from  shared registers,
\item[3)] 
the residue  includes  at most  processes,
\item[4)]
all the processes that wrote a value to a shared register that has ever been read in~  are in~. 
\end{enumerate}

We continue for the maximum number of stages~  such that two constraints are met.
One is that the inequality  holds, where  is the range of new names.
The resulting stage number  satisfies  and  contains at least  elements.
The other constraint is that , so that  has at most  elements. 
These two requirements combined determine 

The definitions of  and  imply that the processes in~  have not written in~ yet  and have read the same values from the same shared registers in the same order of reading.
The set  has at least  elements.

Suppose that a decision on a new name is made by each among the processes in  were possible without any further reads or writes.
The range of new names has  elements.
By the pigeonhole principle, there are two processes  and  in  that would get assigned  the same name.
This means some process among  and  performs at least one additional read.
We set , which has at most  elements.

There is an execution  of the algorithm, with the processes in~ as the only contenders for new names, such that  restricted only to the events involving the processes in~ is a prefix of~.
The processes that wrote values ever read in~ are in~.
The processes  and  have the same history of reads  in~, and each of them read  times without writing even once.
The algorithm is a wait-free solution to Renaming, so both  and  eventually assign themselves  names in~.
It follows that at least one of the processes  and  eventually performs at least one read  after~.
This means that the process makes at least  steps in execution~.
\end{proof} 

Theorem~\ref{thm:rename-lower-bound} implies that some process executing a renaming algorithm has to perform at least  steps in a suitable distributed system, which we show next.



\begin{corollary}
\label{cor:lower-bound-k-steps}

For any -renaming algorithm for processes with new names in some range~ and using some  shared read-write registers, there exists a sufficiently large range of original names~ and an execution in which some process performs at least  steps.
\end{corollary}

\begin{proof}
Let us choose  such that the part  in the lower bound of Theorem~\ref{thm:rename-lower-bound} is at least .
To thus end, it suffices to set  to be at least as large as .
Then the lower bound of Theorem~\ref{thm:rename-lower-bound} becomes .
\end{proof}

Algorithm \textsc{Square-Rename} works for arbitrary range  of  original names.
Corollary~\ref{cor:lower-bound-k-steps} implies that this algorithm is asymptotically optimal with respect to local-step performance, which is .



\begin{corollary}
\label{cor:lower-bound-k-N-renaming}

For any -renaming algorithm that has  as a bound on new names and uses  auxiliary registers, for a range of original names , there exists an execution in which some process performs  steps.
\end{corollary}

\begin{proof}
We reformulate the logarithmic part of the bound in Theorem~\ref{thm:rename-lower-bound} as follows:

By the assumptions, we have , ,  and .
Substitute these into~\eqref{eqn:rename-lower-bound} to obtain a bound

which is .
\end{proof}

Corollary~\ref{cor:lower-bound-k-N-renaming} implies that algorithm \textsc{Compact-Rename} may miss local-step optimality by a factor that is about .
More precisely, assuming additionally that , the lower bound of Theorem~\ref{thm:rename-lower-bound} becomes . Each process executing algorithm \textsc{Compact-Rename}  performs   steps, by Theorem~\ref{thm:compact-rename}. 
We have the following asymptotic  identity , under the additional assumption about  that .




\Paragraph{A lower bound on first storing.}


The lower bound on the first operations of storing applies to implementations of solutions of Store\&Collect in which a process proposes a value by writing it to a dedicated register, one such a register per process, and collecting is performed by reading all such registers.
The first operation \texttt{Store} that a process performs requires identifying a register to write a proposed value. 
We assume there are  shared registers available.
The processes have names from the range~.




\begin{theorem}
\label{thm:store-collect-lower-bound}

Let  processes among a total of  in an asynchronous system using  auxiliary shared read-write registers and with names in the range  execute a wait-free algorithm for Store\&Collect in a system with  shared read-write registers. 
Then there exists an execution in which some process performs  steps in its first operation \texttt{Store}.
\end{theorem}

\begin{proof} 
A proof  is similar to that of Theorem~\ref{thm:rename-lower-bound}.
We structure a prefix of an execution  to reflect stages, with a pool  and residue  sets of processes after stage~.
An execution continues for the maximum number of stages~  such that two constraints are met.
One is that the inequality  holds.
The resulting stage number  satisfies  and  contains at least  elements.
The other constraint is that , so that  has at most  elements. 
These two requirements combined determine 

The definitions of  and  imply that the processes in~  have not written in~ yet  and have read the same values from the same shared registers in the same order of reading.
The set  has at least  elements.

Suppose that selections of registers for storing by each among the processes in  were possible without any further reads or writes.
There are  shared registers.
By the pigeonhole principle, there are two processes  and  in  that would select the same register.
This means some process among  and  performs at least one additional read.
We set , which has at most  elements.

There is an execution  of the algorithm, with the processes in~ as the only competitors to select registers for storing, such that  restricted only to the events involving the processes in~ is a prefix of~.
The processes that wrote values ever read in~ are in~.
The processes  and~ have the same history of reads  in~, and each of them read  times without writing even once.
At least one of the processes  and  eventually performs at least one read  after~.
This means that the process makes at least  steps in execution~.
\end{proof}

Theorem~\ref{thm:store-collect-lower-bound} implies that for each solution of Store\&Collect some process has to perform at least  steps during its first storing, in a suitable distributed system, which we show next.



\begin{corollary}
\label{cor:storing-lower-bound-k-steps}

For any Store\&Collect algorithm for a system with some  shared read-write registers, there exists a sufficiently large range of original names~ and an execution in which some process performs at least  steps.
\end{corollary}

\begin{proof}
Let us choose  such that the part  in the lower bound of Theorem~\ref{thm:store-collect-lower-bound} is at least .
To thus end, it suffices to set  to be at least as large as .
Then the lower bound of Theorem~\ref{thm:store-collect-lower-bound} becomes .
\end{proof}


The algorithm for Store\&Collect with performance characteristics summarized in Theorem~\ref{thm:collect-only-k-known} works for an arbitrary range  of original names.
Corollary~\ref{cor:storing-lower-bound-k-steps} implies that this algorithm is asymptotically optimal with respect to local-step performance of first storing, which is .



\begin{corollary}
\label{cor:lower-bound-k-N-storing}

For any algorithm for Store\&Collect that  uses  auxiliary registers for a range of original names , there exists an execution in which some process performs  steps in its first storing.
\end{corollary}

\begin{proof}
Reformulate the logarithmic part of the bound in Theorem~\ref{thm:store-collect-lower-bound} as follows:

By the assumptions, we have   and .
Substitute these into~\eqref{eqn:rename-lower-bound} to obtain a bound

which is .
\end{proof}

Corollary~\ref{cor:lower-bound-k-N-storing} implies that the algorithm for Store\&Collect for the case when both  and  are known, and whose performance characteristics are given in Theorem~\ref{thm:collect-k-N-both-known}, may miss local-step optimality of first storing by a factor that is about .
More precisely, assuming additionally that , the lower bound of Theorem~\ref{thm:store-collect-lower-bound} becomes . 
Each process storing for the first time  performs   steps, by Theorem~\ref{thm:collect-k-N-both-known}. 
We have the following asymptotic  identity , under the additional assumption about  that .





\section{Unbounded Selection}





We consider problems concerning processes selecting positive integers continuously such that each selection is exclusive.
A selected positive integer may be considered as an abstract name from an unbounded range.
Such names can be used to identify registers from an infinite pool of registers.
Infinitely repeated selections of names are efficient when they minimize the number of positive  integers that  never get selected to be assigned as names.

Next, we review the functionality of a distributed dynamic data structure that we call a ``repository.''
We will refer to two related concepts of ``storing'' and ``depositing'' a value in a register.
A value written to a register gets stored in it as long as it is not overwritten by a different value.
Depositing a value means  storing it indefinitely in some register.
A repository is a concurrent data structure for depositing values in shared read-write registers.
The underlying assumption is that, for each point in time, each process will eventually generate a value to be deposited in a repository.

We assume that there are infinitely many shared read-write registers, denoted , used to store deposited values.
We say that these registers are \emph{dedicated} to depositing and that  in the \emph{index of register~}.
We assume random access to these shared registers, in that an index~ allows to identify the shared register~ and perform a read or write on it.
Every register~ is initialized to \texttt{null}, which is interpreted as the register being empty. 
An algorithm managing repeated deposits may also use additional auxiliary registers.


A formal definition of depositing and a repository refers to an algorithm implementing this operation and supporting registers.
Consider an execution of an algorithm implementing depositing.
A value  is considered \emph{deposited in a register } at an event in the execution, when the following is satisfied in the system's configuration just after the event:
\begin{enumerate}
\item
The value  is stored in register~. 

\item
The value  will not be overwritten in register   by any different value in the following events of the execution.
\end{enumerate}
A repository is a distributed data structure that provides a repertoire of operations for each participating process.
We describe these operations next.

A process~ may invoke an operation  to obtain a new value to be deposited.
An event  returns a value~ for , where  is a value to deposit.
An event    indicates that there is no value to deposit yet.

A process~ invokes an operation \texttt{Deposit} to deposit a value~.
An event \texttt{Ack} acknowledges completing \texttt{Deposit}, where  is the register in which the value has been deposited.
When a process crashes while working to deposit a value, and depositing this value has not been acknowledged before the crash, then the value may either get deposited or not in a dedicated register.

Following the standard understanding of simulating executions, as presented by Attiya and Welch~\cite{Attiya-Welch-book2004}, we prohibit ``pipelining'' on the operations \texttt{Query} and \texttt{Deposit}.
This means that a process may invoke a new operation only after the previous one, if any, has returned an outcome or has been acknowledged as completed.
When a process~ obtains a return of a query for a new value, then the process  eventually invokes \texttt{Query} again.
We assume \emph{fair occurrence  of deposit requests} at processes, which means that each process eventually obtains a new value to deposit, after having deposited the previous value, if any, unless the process crashes.

Let us observe that no algorithm depositing values and resilient to even one crash can guarantee for a specific register that a value gets deposited in the register eventually, since otherwise the value stored there could be used to determine  a decision in a solution of Consensus, which is impossible in asynchronous systems with processes prone to crashes and read-write registers~\cite{Attiya-Welch-book2004,HerlihyKozlovRajsbaum-book,Lynch-book96}.
This means that for any execution of a depositing algorithm, some registers dedicated for depositing may never be used for deposits.
Now the question is how many registers dedicated for depositing will never be used to deposit?
There is a simple solution to the problem of repeated deposits in which a process~ deposits only in consecutive registers with indices congruent to~ modulo~.
The problem with this approach is that if even one process crashes then infinitely many registers dedicated for depositing will never deposit a value.
We want to have a solution to repeated depositing in which the number of dedicated registers not used for deposits is finite in any infinite execution.

A \emph{repository} is a concurrent data structure that allows each process to deposit  values in dedicated registers that satisfied the following two properties, subject to fair occurrence  of deposit requests  in an infinite execution:
\begin{description}
\item[\sf Persistence:]  
Starting from an acknowledgment event  \texttt{Ack}, for a process~ and register  dedicated for depositing, the value stored in~ by  becomes deposited. 
\item[\sf Utilization:] 
There are finitely many registers dedicated for depositing that never store a deposited value.
\end{description}

We want the quality of an implementation of a repository to be be minimally non-blocking, but preferably wait-free.
These qualities have the following standard meaning:
\begin{description}
\item[\sf Non-blocking:] 
If at least one nonfaulty process wants to deposit a value in a configuration, then eventually a value gets deposited.
\item[\sf Wait-free:] 
If some nonfaulty process wants to deposit a value in a configuration, then eventually this process succeeds in depositing its value.
\end{description}
The Repository problem is to implement a repository in a distributed system of  processes prone to crashes and an unbounded supply of read-write registers initialized to \texttt{null}.





\Paragraph{Implementations of a repository.} 




The algorithms we give next assign integers to processes such that an assignment provides exclusivity of a selection of an integer by a process.
We interpret a newly assigned integer~ as an indication that the register~ could be  available for depositing.

We start from the algorithm called \textsc{Selfish-Repository}, which works as follows.
Each process~ maintains a sorted list  of  indices  in its local memory, which are interpreted as identifying registers possibly still available for deposits.
The list is initialized to store the first  positive integers arranged in order in consecutive entries.
Pointer  points to the first item on the list.
For an entry  in the list,  is the next item in the list, and  is the integer stored in the entry.
Process~ also  uses a variable~ interpreted as an index of the next available empty register immediately after the registers whose indices are in~.
The variable~ is initialized to~.
The entry at position  in list~ of process~ is denoted  (or  when  is understood).

Process~ may \emph{update list~}, which is performed as follows.
Process~ scans~, starting from .
For each entry  scanned in the list, if , then  reads~. 
If  is still empty, then the next entry  is considered, otherwise  removes entry~ from  list , by making the predecessor of  point to its successor , and begins scanning registers~ one by one in the order of indices, starting from the index~ stored in~.
The scan of array~ continues until an empty~ is found, if any.
Once process~ reads an empty  then  appends a new entry  with   to~, and sets~ to~.
This completes processing entry~.
Next the entry  in~ is processed in a similar manner.
Updating list~ ends after all the entries of~ have been processed.

We will use an atomic-snapshot object~. 
It includes read-write registers~, for each process~, for , such that  is writable by~ and readable by all processes.
A view returned to a process that invoked taking a snapshot consists of a vector , where  stores the value read from~.
Each register~, for , is initialized to \texttt{null}.
The registers~ are supported by other auxiliary registers in~ to equip~ with the functionality of an atomic snapshot object, see~\cite{AfekADGMS93}.
Process~ writes an integer in the interval  to the register~ in~.
After taking a snapshot using~, process~ assigns itself the \emph{rank} defined as follows: it is the rank of  among the indices~ such that the variable~ stores an entry different from \texttt{null}.
A snapshot determines integers in the interval  that are \emph{available}: these are the numbers in the interval that do not occur in the snapshot.
For each snapshot with at least one repetition of entries there are always at least  integers available: this is because at most  numbers in  occur in the snapshot.

The need for a deposit occurs when a process~ that has been querying for a new value to deposit obtains such a value.
Then process~ begins attempts to acquire an index of a register available for deposits.
Such attempts are performed repeatedly until identifying an index of a register~ exclusively available to~, we call such  a \emph{list name}.
Each attempt begins with  reviewing its list~ and setting the \emph{candidate} index, of an eligible register dedicated to depositing, to~.
The first goal is to go through a sequence of candidates to eventually identify a list name, while verifying each candidate until it passes the verification and becomes a list name.

Identifying a candidate and verification proceed as follows. 
After  sets the candidate to~, it  repeats the following three actions.
First  sets  to the candidate index.
Then  invokes obtaining a view~ from the snapshot object~.
The  third action depends on whether the candidate is unique in the view~.
If this is the case then  treats the candidate index in  as a list name produced by the list.
Otherwise,  sets  to the rank of  in , then sets  to the th integer available in , and finally chooses a next candidate as the entry on  at position~.
Now  resumes verifying the candidate.
When process~ acquires a list name~ then it checks to see if  is empty.
If this is the case then  deposits at~.
Otherwise, when  already stores a deposited value then  resumes attempts to acquire an index of an eligible register. 

A pseudocode of algorithm \textsc{Selfish-Deposits} is presented in Figure~\ref{fig:alg-selfish-deposit}.
In the pseudocode, the names of variables \texttt{candidate} and \texttt{list-name} half self-explanatory meaning.
The pseudocode is structured as a repeat loop~(1.), which terminates by return of acknowledgement in the last line~(\ref{sel-dep:ack}).




\begin{figure}[t]
\hrule

\FF

\textsf{algorithm} \textsc{Selfish-Repository}

\FF

\hrule

\FF

\begin{enumerate}[nosep]

\item
\texttt{repeat}

\begin{enumerate}[nosep]
\item
\label{sel-dep-update}
update list ; 
;
; 
\item
\texttt{repeat}
\begin{enumerate}[nosep]
\item

\item
obtain a view  from snapshot object 
\item
\texttt{if} \texttt{candidate} is unique in the view  \texttt{then}  \texttt{else} 
\begin{enumerate}[nosep]
\item
 the rank of  in  
\item
  the th integer available in 
\item

\end{enumerate}
\end{enumerate}
\texttt{until} 
\label{sel-dep-verify-null}
\item
\texttt{if}   \texttt{then}
\begin{enumerate}[nosep]
\item 
\label{sel-dep-store}

\item 


\item 
\label{sel-dep:ack}
\texttt{return} \texttt{Ack}(\texttt{list-name})
\end{enumerate}
\end{enumerate}
\end{enumerate}
\FF

\hrule

\FF

\caption{\label{fig:alg-selfish-deposit}
Pseudocode of procedure \texttt{Deposit}, for a process~, implementing a selfish repository.}
\end{figure}



\begin{theorem}
\label{thm:selfish-deposit}

Depositing based on algorithm \textsc{Selfish-Deposit} is a non-blocking implementation of a repository such that in each execution at most  dedicated deposit registers are not  used for depositing.
\end{theorem}


\begin{proof} 
When a value  gets stored in a register~ by a process~ in instruction~(\ref{sel-dep-store}) in Figure~\ref{fig:alg-selfish-deposit}, then this occurs after \texttt{list-name} has been verified to be unique in the view returned by the snapshot object. 
This entry written by  in~ stayed there until after  completed writing to .
This means that at most one process could attempt to store a value in this register .
The first such process would succeed, as all subsequent attempts, if any, would be prevented by the verification in line~(\ref{sel-dep-verify-null}) of the pseudocode.
This provides the property of Persistence defining a repository.

Next we argue that there are infinitely many successful deposits, assuming fair occurrence  of deposit requests. 
Suppose there is an event after which no deposits occur.
Eventually every process either has crashed or it has a value to deposit, by the assumed fair occurrence of deposit requests.
Each failure to deposit starts a new iteration of the main repeat loop in Figure~\ref{fig:alg-selfish-deposit}, which begins with updating list~ by instruction~(\ref{sel-dep-update}).
As all the non-faulty processes keep updating the lists, while no deposits occur, then eventually all their lists become equal and store the indices of the smallest empty deposit registers.
The values on this list make a set of  natural numbers. 
Let us take the first event when this occurs.
Consider the first following event when each non-faulty process~ has written to~.
Starting from this point in the execution, the ranks of all the processes become fixed in the snapshot object.
Consider the subsequent writes to  by a process~.
If such a write does not produce a unique number in the view, then each next write of a proposed list name is after choosing by rank.
Once the ranks become fixed, each choosing by rank produces a unique entry in the common list~ shared by all the processes.
It follows that eventually some non-faulty process~ acquires a list name.
Now this list name identifies a unique entry in the list  shared by all the processes.
The register dedicated for deposits with the index in this unique entry of  is empty and no other process attempts to use this register, so  completes a deposit.
This contradicts the assumption that there are no deposits after some event.

If a process crashes in the course of depositing, then the process may have identified  a register~ for depositing by acquiring a list name~, but the crash occurred before the value got stored in the register~.
There may be up to  such indices~ and the corresponding registers~.
A crashed process~ may have set  in the snapshot object to its candidate, which now will store its value~ throughout the execution.
If such registers  make an initial segment , then eventually the first  numbers in lists~ stabilize and each of them is a list name assigned to a crashed process. 
If the next  entries in the lists~ stabilize as well, then these will forever stay as the first  available indices never to be assigned as list names.
Since there are at most  crashes and ,  we have that up to  indices of registers dedicated for deposits may never be used as list names.
This means that up to  registers dedicated for depositing may never store a deposited value.
\end{proof} 





\begin{theorem}
\label{thm:non-blocking-repository}

For each non-blocking algorithm implementing a repository there exists an execution in which  registers dedicated for depositing remain unused. 
\end{theorem}

\begin{proof}
We argue that in each implementation of a repository, at least  dedicated registers may be never used for deposits in some execution.
Namely, when a process~ is to deposit by writing to a register~, and a write event to store the value is enabled, we may ``freeze'' the write.
At this point, no  other process~ will want to deposit to , because otherwise after \texttt{ack} happens, the pending write of  to store at  might occur as well, which results in overwriting~, in contradiction of the definition of a repository.
This means that if  crashed rather than merely ``freezed,'' then the register  is never used for depositing by any other process. 
Up to  crashes can happen, so at least these many registers might never be used for deposits.
\end{proof}

By Theorem~\ref{thm:selfish-deposit}, algorithm \textsc{Selfish-Deposit} leaves   registers dedicated for deposits that remain unused.
This combined with Theorem~\ref{thm:non-blocking-repository} shows that algorithm \textsc{Selfish-Deposit} leaves an asymptotically optimum number of shared registers in the worst case in a perpetual state of not storing a deposited value.

Next, we consider a wait-free implementation of a repository.
We call the algorithm that provides the implementation \textsc{Altruistic-Deposit}.
The algorithm uses some of the mechanisms in  \textsc{Selfish-Deposit} and extends them.
The difference between the two algorithms is what a process~ does with acquired list names.
In algorithm \textsc{Selfish-Deposit}, a process acquiring list names uses it selfishly as address of registers to deposit.
In executions of algorithm \textsc{Altruistic-Deposit}, processes share acquired list names  with other processes to help in their deposits.

Algorithm \textsc{Altruistic-Deposit} consists of two threads. 
One auxiliary thread produces register indices, and the other thread  deposits values.
The two threads are interleaved in a fair manner, in that each process alternates invoking  instructions from the two threads.
Both threads work on an  array \texttt{Help}, for , of shared read-write registers.
The processwa use a snapshot object~ to obtain new list names, similarly as in algorithm \textsc{Selfish-Deposit}.
A process~ writes  a verified list name  into \texttt{Help}  to be used by process~ for its deposits.





\begin{figure}[t]
\hrule

\FF

\textsf{algorithm} \textsc{Altruistic-Repository} : producing thread

\FF

\hrule

\FF

\begin{itemize}[nosep]
\item[]

\item[]
\texttt{repeat}
\begin{enumerate}[nosep]
\item
\texttt{if}  \texttt{then} increment \texttt{column} in a round robin manner \texttt{else}
\begin{enumerate}
\item
update list ; 
;
; 
\item
\texttt{repeat}
\begin{enumerate}[nosep]
\item

\item
obtain a view  from snapshot object 
\item
\texttt{if} \texttt{candidate} is unique in the view  \texttt{then}  \texttt{else} 
\begin{enumerate}[nosep]
\item
 the rank of  in  
\item
  the th integer available in 
\item

\end{enumerate}
\end{enumerate}
\texttt{until} 

\end{enumerate}
\item
\label{altruistic-verify-availability}
\texttt{if}  \texttt{then} 
\begin{enumerate}[nosep]
\item

\item

\end{enumerate}
 \item 

\end{enumerate}
\end{itemize}
\FF

\hrule

\FF

\caption{\label{fig:alg-altruistic-repository-produce}
Pseudocode of a producing thread in the altruistic repository, used by a process~ to obtain an index of a register available for depositing.
Such an index gets stored as entry  of the column  of the array \texttt{Help}.}
\end{figure}


A process~ in the producing thread keeps reading the registers in row \texttt{Help} in a round-robin manner starting from the diagonal entry. 
If  finds some register \texttt{Help} equal to \texttt{null} then  works to obtain a new list name.
After successfully acquiring a list name~, process~ verifies if  is empty, which means it has not been reserved yet.
If this is the case then process~ first marks  as \texttt{reserved} and then writes~ into~\texttt{Help}.
The value \texttt{reserved} is assumed to be different from \texttt{null}.
A pseudocode of the producing thread is in Figure~\ref{fig:alg-altruistic-repository-produce}.



\begin{lemma}
\label{lem:column-nonempty}

For each process~ and an event it is involved, eventually some entry in the column  stores an index of a reserved register available for deposits. 
\end{lemma}

\begin{proof}
Processes execute producing threads similarly as depositing selfishly in that this produces new reserved registers in a non-blocking manner.
As new entries in the array  get \texttt{reserved} written in them, the writers wrap around their rows in a round robin manner.
The perpetual existence of a column of the array  with all entries \texttt{null} would contradict the non-blocking progress achieved in the execution.
If a process~ obtains a new \texttt{list-name} in an execution of producing thread, then this number is unique in the view provided by the snapshot object. 
This means that no write to  by some other process is pending when  reads  and finds it empty in instruction \eqref{altruistic-verify-availability} in Figure~\ref{fig:alg-altruistic-repository-produce}, and so eligible to write \texttt{reserved}.
\end{proof}


A process~ in the depositing thread keeps reading the column \texttt{Help} in a round-robin fashion, starting from the diagonal entry.
Once  finds an index  stored at \texttt{Help}, then  deposits in~ and then writes \texttt{null} to erase value~ in \texttt{Help}.
A pseudocode of the depositing thread is in Figure~\ref{fig:alg-altruistic-repository-deposit}.



\begin{figure}[t]
\hrule

\FF

\textsf{algorithm} \textsc{Altruistic-Repository} : depositing thread

\FF

\hrule

\FF

\begin{enumerate}[nosep]
\item

\item
\texttt{while}  \texttt{do}
increment \texttt{row} in a round robin manner
\item

\item

\item 

\item
\texttt{Ack}
\end{enumerate}
\FF

\hrule

\FF

\caption{\label{fig:alg-altruistic-repository-deposit}
Pseudocode of a depositing thread, used by a process~ to deposit a value~, implementing an altruistic repository.}
\end{figure}




\begin{theorem}
\label{thm:altruistic-deposit}

Depositing based on algorithm \textsc{Altruistic-Deposit} is a wait-free implementation of a repository such that at most  dedicated deposit registers will never be used for depositing.
\end{theorem}


\begin{proof}
Consider an event in which a process~ wants to deposit a value~.
The process invokes the depositing thread and so keeps reading entries in the column  in a round-robin manner.
By Lemma~\ref{lem:column-nonempty}, process~ eventually reads ,  for some row~ and index~. 
The register~ was verified to be empty by the process~ that wrote~ to , which occured when executing line~\eqref{altruistic-verify-availability} in Figure~\ref{fig:alg-altruistic-repository-produce}.
Process~ can safely store the value~ in~, because while the entry  stays equal to index~, register~ stays equal to \texttt{reserved}, which is different from \texttt{null}.
This prevents the index~ to be written at other locations of the array \texttt{Help}, by instruction~\eqref{altruistic-verify-availability} in Figure~\ref{fig:alg-altruistic-repository-produce}, and so prevents multiple values to be possibly stored in succession at~.

Next, we estimate the number of registers dedicated for depositing that may never be used to deposit a value.
This number if maximized when  processes crash while many entries of  \texttt{Help} store indices of  registers reserved for depositing.
Let  be the only process that never crashes.
The worst case scenario occurs when each of the crashed processes~ has a full column of~ indices reserved and it crashes when working to produce an index of a register to be placed in column .
Such a process~ may have written some list name~ to  and verified that  but crashed before setting  to \texttt{reserved} and so also did not reset  to~\texttt{null}.
Suppose all the lists  stabilized to the same sequence of entries, and further that such indices  make the first  entries in the lists. 
Then the first  entries will never be removed from the lists along with the first  entries available in the lists~.
The registers with theses  indices will stay equal to \texttt{null} forever.
We have obtained  reserved registers and   empty registers never to be used for depositing.
The total number of registers dedicated for depositing that never store a deposited value could be .
\end{proof}

It is an open problem if there exists a wait-free implementations of a repository that leaves out  registers not used for depositing.





\Paragraph{Mining names.}



Next, we consider the task to have processes work continuously to accumulate a possibly  unlimited collection of exclusive names.
The distributed system consists of  processes prone to crashes and a number of shared objects.
We call designing an algorithm for this task the \emph{Mining-Names} problem.
The complete specification is as follows.

A positive integer~ is considered to be \emph{assigned to process~ as a name} when  exclusively commits to integer~ by writing~  in a dedicated local write-once memory variable.
Exclusivity means that no two processes ever commit to the same integer, so a name can be interpreted as an exclusive reservation of a natural number.
Committing to a name resembles committing to a decision in solutions of Consensus.
After committing to a name, a process can proceed to commit to some other natural number as a name as well.
A process participating in acquiring new names never stops voluntarily.

An algorithm is a solution to Mining-Names if in each infinite execution the following two properties are satisfied:
\begin{description}
\item[\sf Naming:] 
No two different processes ever commit to the same integer as a shared name.

\item[\sf Utilization:] 
There are finitely many positive integers that never get acquired as names.
\end{description}
A mining names solution in a system with process crashes could be non-blocking or wait-free, which is understood as follows.
\begin{description}
\item[\sf Non-blocking:] 
For each  event in an execution, eventually a new name gets acquired.
\item[\sf Wait-free:] 
For each  process and any event in an execution, eventually this process acquires a new name.
\end{description}

Algorithms implementing a repository can be adapted to mining names. 
Namely, let every process keep invoking an operation to deposit a dummy value~.
Rather than deposit  in a register~, for some index~, the process commits to the name~.
After a new name has been acquired, the process invokes depositing a dummy value again.
This transformation from depositing to mining names requires the same distributed system that supports depositing.
In particular, the implementation of repository with properties summarized in Theorems~\ref{thm:selfish-deposit} and~\ref{thm:altruistic-deposit} assumes that an infinite array of shared read-write registers each initialized to \texttt{null} is available.




\begin{theorem}
\label{thm:name-mining-solutions}

The Mining-Names problem can be solved by  processes in a non-blocking fashion by an algorithm that leaves at most  nonnegative integers not assigned as names, or in a wait-free manner by an algorithm that leaves at most  integers never assigned as names.
\end{theorem}

\begin{proof}
This follows from combining the general transformation of implementations of a repository to mining names and Theorems~\ref{thm:selfish-deposit} and~\ref{thm:altruistic-deposit}.
\end{proof}

Mining names can be used to implement a repository  by a general transformation, which works as follows.
Every process keeps mining names, and as soon as a new name~ is acquired, this reserves the register~ for depositing.



\begin{theorem}
\label{thm:name-mining-non-blocking-optimality}

For each non-blocking algorithm for Mining-Names, there exists an execution in which  positive integers are not assigned as names to any process.
\end{theorem}

\begin{proof}
We apply the general transformation from an algorithm mining names to an implementaiton of a repository.
This transformation creates a non-blocking algorithms mining names from a non-blocking implementation of a repository.
If an algorithm for mining names could guarantee fewer than  positive integers never assigned as names then this could be converted into a non-blocking solution for a repository that leaves out at most  registers never used for deposits.
This would contradict Theorem~\ref{thm:non-blocking-repository}.
\end{proof}

The implementations of repository we developed have processes use newly acquired list names as indices of registers in an unbounded array of read-write registers.
This works only if a distributed system includes an unbounded array of shared read-write registers, each initialized to \texttt{null}.
The registers dedicated for depositing allow to keep track of indices of used registers, and to obtain next registers still available for deposits by the operation of updating lists.
It is an open problem if there exist algorithmic solutions to Mining-Names in an asynchronous  distributed system with finitely many shared read-write registers and processes prone to crashes.




\bibliographystyle{plain}

\bibliography{asynchronous-selection}

\end{document}
