\clearpage

\section*{Appendix}

\section{Datasets}
\label{app:datasets}

\subsection{Pre-training}
For continued pre-training, we pre-process the English Wikipedia \footnote{\url{https://dumps.wikimedia.org/enwiki/20211101/}}, the BookCorpus\footnote{\url{https://huggingface.co/datasets/bookcorpusopen}}, OpenWebText \citep{Gokaslan2019OpenWeb} and the CC-News\footnote{\url{https://commoncrawl.org/2016/10/news-dataset-available/}} datasets. We do not use the STORIES dataset as it is no longer available for research use \footnote{\url{https://github.com/tensorflow/models/tree/archive/research/lm\_commonsense\#1-download-data-files}}. We clean every dataset by removing headers, titles, tables and any HTML content. For every document, we keep paragraphs containing at least 60 characters and documents containing at least 200 characters. After cleaning, we obtain 5GB, 10GB, 34GB and 360GB of raw text from the BookCorpus, Wikipedia, OpenWebText and CC-News respectively. We split paragraph into lists of sentences using the blingfire tokenizer\footnote{\url{https://github.com/microsoft/BlingFire}}. We present the details of our pre-training objectives in Section~\ref{sec:objectives}. We present the details on sampling lengths and number of negatives for each of the objectives below:

\begin{itemize}[wide, labelindent=0pt]
\itemsep-0.25em
    \item \textbf{Spans in Same Paragraph (SSP)}
    We randomly sample the number of sentences in $A$ in the interval $\left[1, 3\right]$ and $B$ in $\left[1, 5\right]$. This is to keep the inputs to the model analogous to those in AS2 (shorter question text, followed by longer answer text). We sample up to 2 hard negatives from the same paragraph as $A$ (if possible), and sample easy negatives from other documents so as to make the total number of negatives to be 4.
\item \textbf{Span in Paragraph (SP)}
    We randomly sample the number of sentences in $A \in P_i$ in the interval $\left[1, 3\right]$. The number of sentences in the right part is given by the length of $P_i \setminus A$ (positive pair) or $P_j \setminus X_j$ (negative pair). Similar to SSP, we sample up to 2 hard negatives from the same document (if possible), and sample easy negatives from other documents so as to make the total number to be 4.
\item \textbf{Paragraphs in Same Document (PSD)}
    We chose a random pair of paragraphs $A$ and $B$ from a single document and then we randomly sample 4 paragraphs from other documents to create the negative pairs with $A$.
\end{itemize}


\subsection{Fine-tuning}
Here we present statistics and links for downloading the AS2 datasets used: ASNQ\footnote{\url{https://github.com/alexa/wqa_tanda}}, WikiQA\footnote{\url{http://aka.ms/WikiQA}}, TREC-QA and WQA; to benchmark our pre-trained models. Table \ref{tab:as2_datasets} shows the number of unique questions and answer candidates for each dataset and for each split.

\iffalse
\mypara{ASNQ} A large-scale AS2 dataset~\cite{garg2019tanda} where the answer candidates for a question are extracted from a single Wikipedia page and the questions come from queries of the Google search engine. ASNQ is a modified version of the Natural Questions (NQ)~\cite{kwiatkowski-etal-2019-natural} dataset, obtained by converting it from machine reading to AS2. This is done by labeling sentences from the long answers which contain the short answer string as positive correct answer candidates and all the other as negatives. We use the dev and test splits released by~\citeauthor{soldaini-moschitti-2020-cascade}\footnote{\url{https://github.com/alexa/wqa-cascade-transformers}}.

\begin{table}[h!]
\centering
\small
\begin{tabular}{lccc}
\hline
    \textbf{Split} & \textbf{\# Questions} & \textbf{\# Candidates} & \textbf{Avg. \# C/Q} \\
    \hline
    Train   & 57,242 & 20,377,568 & 356.0 \\
    Dev     & 1,336  & 463,914    & 347.2 \\
    Test    & 1,336  & 466,148    & 348.9 \\ \hline
\end{tabular}
\caption{Data Statistics for ASNQ dataset}
\vspace{1em}
\label{tab:asnq}
\end{table}


\mypara{WikiQA} An AS2 dataset released by \citeauthor{yang2015wikiqa} where the questions are derived from query logs of the Bing search engine, and the answer candidates for a given question are extracted from a Wikipedia page. This dataset has a subset of questions having no correct answers (\textit{all-}) or having only correct answers (\textit{all+}). We remove both the (\textit{all-}) and (\textit{all+}) questions for our experiments (``clean'' setting).

\begin{table}[h!]
\centering
\small
\begin{tabular}{lccc}
\hline
    \textbf{Split} & \textbf{\# Questions} & \textbf{\# Candidates} & \textbf{Avg. \# C/Q} \\
    \hline
    Train   & 2,118 & 20,360     & 9.6 \\
    Dev     & 122   & 1,126      & 9.2 \\
    Test    & 237   & 2,341      & 9.9 \\
    \hline
\end{tabular}
\caption{Data Statistics for ``clean'' WikiQA dataset}
\vspace{1em}
\label{tab:wikiqa}
\end{table}


\mypara{TREC-QA} A popular AS2 dataset of factoid questions released by \citeauthor{wang-etal-2007-jeopardy}. The data pairs were collected from TREC-8 to TREC-13 Question Answering tracks. For each question, the answer candidates were collected from a pool of many documents, selecting sentences that contain one or more non-stopwords in common with the question. For our experiments, we trained on the \textit{train-all} split, which contains more noise but also more question-answer pairs. For the dev and test sets, we removed the questions without answers, or those having only correct or only incorrect answer sentence candidates. This setting refers to the ``clean'' setting~\cite{shen-etal-2017-inter}, which is a TREC-QA standard.

\begin{table}[h!]
\centering
\small
\begin{tabular}{lccc}
\hline
    \textbf{Split} & \textbf{\# Questions} & \textbf{\# Candidates} & \textbf{Avg. \# C/Q} \\
    \hline
    Train   & 1,226 & 53,417     & 43.6 \\
    Dev     & 69    & 1,343      & 19.5 \\
    Test    & 68    & 1,442      & 21.2 \\
    \hline
\end{tabular}
\caption{Data Statistics for TREC-QA dataset.}
\vspace{1em}
\label{tab:trecqa}
\end{table}


\mypara{WQA} An large scale industrial AS2 dataset containing non-representative de-identified user questions from a commercial virtual assistant. For every question, ${\sim}15$ answer candidates are collected from more than 100M documents using Elasticsearch. Results on WQA are presented relative to the RoBERTa-Base baseline due to the data being internal.

\begin{table}[h!]
\centering
\small
\begin{tabular}{lccc}
\hline
    \textbf{Split} & \textbf{\# Questions} & \textbf{\# Candidates} & \textbf{Avg. \# C/Q} \\
  \hline
    Train   & 9,984     & 149,513     & 15.0 \\
    Dev     & 5,000     & 74,805      & 15.0 \\
    Test    & 5,000     & 74,712      & 14.9 \\
    \hline
\end{tabular}
\caption{Data Statistics for WQA dataset.}
\vspace{1em}
\label{tab:wqa}
\end{table}

\fi

\begin{table}[ht!]
\centering
\small
\begin{tabular}{llccc}
\hline
    \textbf{Dataset} & \textbf{Split} & \textbf{\# Q} & \textbf{\# C} & \textbf{Avg. \# C/Q} \\
    
    \hline
    \multirow{3}{*}{ASNQ}       & Train   & 57,242 & 20,377,568 & 356.0 \\
                                & Dev     & 1,336  & 463,914    & 347.2 \\
                                & Test    & 1,336  & 466,148    & 348.9 \\

    \hline
    \multirow{3}{*}{WikiQA}     & Train & 2,118 & 20,360  & 9.6 \\
                                & Dev     & 122   & 1,126   & 9.2 \\
                                & Test    & 237   & 2,341   & 9.9 \\
    \hline

    \multirow{3}{*}{TREC-QA}    & Train   & 1,226 & 53,417     & 43.6 \\
                                & Dev     & 69    & 1,343      & 19.5 \\
                                & Test    & 68    & 1,442      & 21.2 \\

    \hline
    \multirow{3}{*}{WQA}        & Train   & 9,984     & 149,513     & 15.0 \\
                                & Dev     & 5,000     & 74,805      & 15.0 \\
                                & Test    & 5,000     & 74,712      & 14.9 \\
    \hline

\end{tabular}
\caption{Data Statistics for AS2 dataset. ``Avg. \# C/Q'' is the average number of answer candidates per question.}
\label{tab:as2_datasets}
\end{table}






\section{Experimental Setup}
\label{app:experiments}

We experiment with the \textit{base} architecture, which uses an hidden size of $768$, $12$ transformer layers, $12$ attention heads and feed-forward size of $3072$.

\paragraph{Pre-training} We perform continued pre-training starting from the publicly released checkpoints of RoBERTa-Base \cite{liu2019roberta} and ELECTRA-Base \cite{clark2020electra}. We optimize using Adam, which we instantiate with $\beta_1 = 0.9$, $\beta_2 = 0.999$ and $\epsilon = 10^{-8}$. We use a triangular learning rate with $10$k warmup steps. The peak learning rate is set to $1*10^{-4}$. We apply a weight decay of $0.01$, gradient clipping when values are larger than $1.0$ and dropout ratio is set to $0.1$. We set the batch size to $4096$ examples for every combination of models and objectives. We truncate the input sequences to $128$ tokens for SSP and to $256$ tokens with SP and PSD. Finally, we perform $400$k training steps with models using SSP and $200$k steps with the other objectives: SP and PSD. The total amount of tokens seen in the continued pre-training is the same for all models and equal to ${\sim}210B$. 

We combine the binary classification loss of SSP, SP and PSD with MLM for RoBERTa and with MLM (of the generator) and TD (token detection) for ELECTRA. For RoBERTa, we perform binary classification on the first {\small [CLS]} token in addition to MLM. For ELECTRA, using the generator + discriminator architecture, we perform MLM on the generator; and token-detection along with binary classification on the discriminator using our pre-training objectives. Through experimentation, for RoBERTa, we use equal weights for MLM and our pre-training objectives. For ELECTRA, we combine MLM, TD and our pre-training objectives with the weights 1.0, 50.0 and 1.0 respectively.

\paragraph{Fine-tuning} The evaluation of the models is performed on four different datasets for Answer Sentence Selection. We maintain the same hyper-parameters used in pre-training apart from the learning rate, the number of warmup steps and the batch size. We do early stopping on the development set if the number of non-improving validations (patience) is higher than 5. For ASNQ, we found that using a very large batch size is beneficial, providing a higher accuracy. We use a batch size of $2048$ examples on ASNQ for RoBERTa models and $1024$ for ELECTRA models. The peak learning rate is set to $1*10^{-5}$ for all models, and the number of warmup steps to $1000$. For WikiQA, TREC-QA and WQA, we select the best batch size out of $\{ 16, 32, 64 \}$ and learning rate out of $\{ 2*10^{-6}, 5*10^{-5}, 1*10^{-5}, 2*10^{-5} \}$ using cross-validation. We train the model for 6 epochs on ASNQ, and up to 40 epochs on WikiQA, TREC-QA, and WQA. The performance of practical AS2 systems is typically measured using Precision-at-1 P@1~\cite{garg-moschitti-2021-will}. In addition to P@1, we also use Mean Average Precision (MAP) and Mean Reciprocal Recall (MRR) to evaluate the ranking of the set of candidates produced by the model.

We used metrics from Torchmetrics~\cite{torchmetrics} to compute MAP, MRR, Precision@1 and Accuracy.


\section{Experiments and Results} 

\subsection{Ablation: MLM-only Pre-training}
Table~\ref{tab:results_as2_only_mlm} presents a more detailed comparison between models continuously pre-trained only with MLM and models using also the sentence-level classification loss functions we proposed in this paper.

\begin{table*}[t!]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccccccccccccc}
    \toprule
    \multirow{2}{*}{\textbf{Model+ Data Sampling}} & \multicolumn{3}{c}{\textbf{ASNQ}} & &\multicolumn{3}{c}{\textbf{WikiQA}} & & \multicolumn{3}{c}{\textbf{TREC-QA}} & & \multicolumn{3}{c}{\textbf{WQA}} \\
    \cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12} \cmidrule{14-16}
                           & \textbf{P@1}    & \textbf{MAP}    & \textbf{MRR}   & &\textbf{ P@1}     & \textbf{MAP}     & \textbf{MRR}   & & \textbf{P@1}   & \textbf{MAP}     & \textbf{MRR}  & & \textbf{P@1}    & \textbf{MAP}    & \textbf{MRR}    \\
    \midrule
    
    RoBERTa-Base                                             & 61.8 {\small (0.2)} & 66.9 {\small (0.1)} & 73.1 {\small (0.1)}       & & 78.3 {\small (2.8)} & 85.8 {\small (1.3)} & 87.2 {\small (1.3)}     & & 90.0 {\small (1.9)} & 89.7 {\small (0.7)} & 94.4 {\small (1.1)}      & &  \multicolumn{3}{c}{Baseline}  \\

    \midrule

    + SSP Data (\textbf{MLM-only})   & 63.4 {\small (0.4)} & 67.1 {\small (0.2)} & 73.8 {\small (0.2)}       & & 76.7 {\small (0.9)} & 84.5 {\small (0.7)} & 85.8 {\small (0.7)}     & & 87.4 {\small (1.3)} & 88.8 {\small (0.6)} & 93.1 {\small (1.0)}      & & -0.6\%   & -0.2\%    & -0.3\% \\
    + SSP                            & \bfunder{64.1} {\small (0.3)} & \bfunder{68.1} {\small (0.2)} & \bfunder{74.5} {\small (0.3)}      & & \bfunder{82.9} {\small (0.7)} & \bfunder{88.7} {\small (0.3)} & \bfunder{89.9} {\small (0.4)}          & & \textbf{88.5} {\small (1.2)}  & \textbf{89.3} {\small (0.7)} & \textbf{93.6} {\small (0.6)}                               & & \bfunder{+0.2}\% & \bfunder{+0.6\%} & \bfunder{+0.3}\% \\

    \midrule

    + SP Data (\textbf{MLM-only})    & 62.8 {\small (0.3)} & 67.2 {\small (0.2)} & 73.7 {\small (0.2)}       & & 76.8 {\small (1.6)} & 84.7 {\small (0.8)} & 86.2 {\small (0.7)}     & & 88.8 {\small (1.3)} & 89.8 {\small (0.3)} & 93.7 {\small (0.9)}      & & -1.0\%   & -0.4\%    & -0.6\% \\
    + SP               & \bfunder{64.1} {\small (0.2)} & \bfunder{68.3} {\small (0.1)} & \bfunder{74.5} {\small (0.2)}        & & \bfunder{81.0} {\small (0.8)} & \bfunder{87.7} {\small (0.3)} & \bfunder{88.9} {\small (0.4)}    & & \textbf{90.9} {\small (2.6)} & \textbf{90.1} {\small (0.8)} & \textbf{94.7} {\small (1.3)}                       & & \bfunder{+0.4\%} & \bfunder{+0.7\%}  & \bfunder{+0.5\%} \\

    \midrule

    + PSD Data (\textbf{MLM-only})   & \bfunder{64.1} {\small (0.5)} & 67.3 {\small (0.2)} & 73.7 {\small (0.2)}       & & 79.1 {\small (1.6)} & 85.6 {\small (1.4)} & 87.1 {\small (1.2)}     & & 87.1 {\small (2.8)} & 89.6 {\small (1.0)} & 92.7 {\small (1.3)}      & & -1.3\%   & -0.3\%    & -0.6\% \\
    + PSD                            & 62.6 {\small (0.4)} & \bfunder{67.7} {\small (0.2)} & 73.7 {\small (0.3)}              & & \textbf{80.5} {\small (1.6)} & \textbf{86.4} {\small (1.1)} & \textbf{88.0} {\small (1.0)}                                        & & \bfunder{90.3} {\small (1.3)} & \textbf{90.3} {\small (0.5)} & \bfunder{95.1} {\small (0.7)}              & & \bfunder{+0.4\%}  & \bfunder{+0.7\%} & \bfunder{+0.5\%} \\

    \bottomrule

    \end{tabular}
    }
    \caption{Results (with std. dev. across 5 runs in parentheses) of our pretrained transformer models when fine-tuned on AS2 datasets with MLM-only pre-training. SSP, SP and PSD refer to our pretraining objectives. Results on WQA are relative to RoBERTa baseline. We highlight in bold and underline results like in Table \ref{tab:results_as2}.}
    \label{tab:results_as2_only_mlm}
\end{table*}
 

\section{Qualitative Examples from AS2}
\label{app:qualitative}

We present some qualitative examples from the three public AS2 datasets. We highlight cases in which the baseline RoBERTa-Base model is unable to rank the correct answer in the top position, but where our model pretrained with SP is successful. The examples are provided in Table~\ref{tab:qualitative}.

\begin{table*}[t!]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l}
    \toprule
    \textbf{ASNQ} \\
    \midrule


    \textbf{Q:} \textbf{how many players in football hall of fame}\\
    \textbf{A1:} {\color{red} Two coaches ( Marv Levy , Bud Grant ) , one administrator ( Jim Finks ) , and five players ( Warren Moon , Fred Biletnikoff }\\{ \color{red}, John Henry Johnson , Don Maynard , Arnie Weinmeister ) who spent part of their careers in the Canadian Football League ( CFL )}\\{ \color{red} have been inducted ; two of which have been inducted into the Canadian Football Hall of Fame : Warren Moon and Bud Grant.} \\
    \textbf{A2:} {\color{BlueGreen} As of 2018 , 318 individuals have been elected .} \\
    \textbf{A3:} Six players or coaches who spent part of their careers in the short-lived United States Football League ( USFL ) have been inducted . \\
    \textbf{A4:} Current rules of the committee stipulate that between four and eight individuals are selected each year . \\
    \textbf{A5:} Fifteen inductees spent some of their playing career in the All - America Football Conference during the late 1940s . \\
    \bottomrule
    \toprule

    \textbf{WikiQA} \\
    \midrule
    \textbf{Q:} \textbf{how are antibodies used in} \\
    \textbf{A1:} {\color{red} Antibodies are secreted by a type of white blood cell called a plasma cell .}\\
    \textbf{A2:} {\color{BlueGreen} An antibody (Ab), also known as an immunoglobulin (Ig), is a large Y-shaped protein produced by B-cells that is used by the }\\{\color{BlueGreen} immune system to identify and neutralize foreign objects such as bacteria and viruses .}\\
    \textbf{A3:}  {\color{ForestGreen}Using this binding mechanism, an antibody can tag a microbe or an infected cell for attack by other parts of the immune system, }\\{\color{ForestGreen}or can neutralize its target directly (for example, by blocking a part of a microbe that is essential for its invasion and survival).} \\
    \textbf{A4:} Antibodies can occur in two physical forms, a soluble form that is secreted from the cell, and a membrane -bound form that is \\ attached to the surface of a B cell and is referred to as the B cell receptor (BCR). \\
    \textbf{A5:} The BCR is only found on the surface of B cells and facilitates the activation of these cells and their subsequent differentiation into \\ either antibody factories called plasma cells , or memory B cells that will survive in the body and remember that same antigen so the B \\ cells can respond faster upon future exposure. \\
    \bottomrule
    \toprule

    \textbf{TREC-QA} \\
    \midrule
    \textbf{Q:} \textbf{Where is the group Wiggles from ?} \\
    \textbf{A1:} {\color{red} Let 's now give a welcome to the Wiggles , a goofy new import from Australia .}\\
    \textbf{A2:} {\color{BlueGreen} The Wiggles are four effervescent performers from the Sydney area : Anthony Field , Murray Cook , Jeff Fatt and Greg Page .} \\
    \textbf{A3:} In Australia , the Wiggles is like really huge . \\
    \textbf{A4:} His group had kids howling with joy with routines involving Dorothy the Dinosaur , Henry the Octopus and Wags the Dog . \\
    \textbf{A5:} While relatively new to the American scene , the Wiggles seem to be on to something , judging by kids ' reactions to the group 's 
    \\ belly-slapping shows . \\
    \bottomrule

    \end{tabular}}
    \caption{Qualitative examples from AS2 datasets where the baseline RoBERTa-Base model is unable to rank a correct answer for the question at the top position, but our SP pre-trained model can (top ranked {\color{BlueGreen} correct answer by SP}). Here we present the top ranked answers $\{A1,\dots,A5\}$ in the order given by the RoBERTa-Base model. For all these examples we highlight the top ranked answer by the baseline RoBERTa-Base model in {\color{red} red} since it is incorrect, and any other correct answer in {\color{ForestGreen} green}.}
    \label{tab:qualitative}
\end{table*}


