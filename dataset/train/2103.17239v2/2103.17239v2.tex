\documentclass[10pt]{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}


\usepackage{algorithm}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{booktabs} \usepackage{colortbl}
\usepackage{glossaries}
\usepackage{multirow}
\usepackage[font=small]{caption}
\usepackage{palatino}

\usepackage{pifont}\usepackage{xspace}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\newif\ifarxiv


\newcommand{\rv}[1]{{\color{red}#1}}
\newcommand{\hugo}[1]{{\color{blue!20!red}[#1]}}
\newcommand{\gab}[1]{{\color{blue}[\textbf{Gab}:#1]}}
\newcommand{\alex}[1]{{\color{orange}[\textbf{Alex}:#1]}}
\newcommand{\MC}[1]{{\color{magenta}#1}}
\definecolor{myblue}{RGB}{8,48,107}
\definecolor{myred}{RGB}{127,39,4}

\newcommand{\cmark}{\ding{51}\xspace}\newcommand{\cmarkg}{\textcolor{lightgray}{\ding{51}}\xspace}\newcommand{\xmark}{\ding{55}\xspace}\newcommand{\xmarkg}{\textcolor{lightgray}{\ding{55}}\xspace}\newcommand{\mycolrule}{\arrayrulecolor{black!30} \midrule \arrayrulecolor{black}}

\newcommand{\stdminus}[1]{\scalebox{0.65}{}}
\newcommand{\bigfigno}{1}

\newcommand \myfig[1] {\includegraphics[width=0.325\linewidth,height=0.325\linewidth]{figs/images/#1}}
\newcommand \myfigc[1] {\includegraphics[trim=7 6 7 7,clip,width=0.325\linewidth,height=0.325\linewidth,]{figs/images/#1}}

\def \deit          {DeiT\xspace}
\def \pzo {\phantom{0}} 
\def \dzo {\phantom{00}} 
\def \tzo {\phantom{000}}
\def \qzo {\phantom{0000}} 

\def \SA {\mathrm{SA}\xspace}
\def \FFN {\mathrm{FFN}\xspace}
\def \LN {\eta\xspace}
\def \alambic {\xspace}
\def \alambicb {\xspace}

\def \OURS {CaiT\xspace}

\def \etal {\textit{et al.}\xspace}

\def \bestperf {86.5\%\xspace}

\def \ours {\OURS}

\author{
\begin{minipage}{\linewidth}
\begin{center}
\large Hugo Touvron \hspace{0.25cm} Matthieu Cord \hspace{0.25cm} Alexandre Sablayrolles \0.5cm]
\scalebox{1.}{Facebook AI\hspace{0.6cm} Sorbonne University}\
    x_{l+1} = g_l(x_l) + R_l(x_l),

    x'_{l}  & = x_{l} + \SA(\LN(x_l))  \nonumber \\
    x_{l+1} & = x'_{l} + \FFN(\LN(x'_l))
    \label{equ:transformer}

    x'_{l}  & = x_{l} + \alpha_l \, \SA(x_l)  \nonumber \\
    x_{l+1} & = x'_{l} + \alpha'_l \, \FFN(x'_l). 
    \label{equ:transformer3}

    x'_{l}  & = x_{l} + \mathrm{diag}(\lambda_{l,1},\dots,\lambda_{l,d}) \times  \SA(\LN(x_l))  \nonumber \\
    x_{l+1} & = x'_{l} + \mathrm{diag}(\lambda'_{l,1},\dots,\lambda'_{l,d}) \times  \FFN(\LN(x'_l)), 
    \label{equ:transformer2}

Q & = W_{q}  \, x_\mathrm{class} + b_q, \label{equ:ca1} \\
K & = W_{k} \, z + b_k, \label{equ:ca2}\\
V & = W_{v} \, z + b_v. \label{equ:ca3}

A = \mathrm{Softmax}(Q.K^{T}/ \sqrt{d/h})

\mathrm{\mathrm{out}_\mathrm{CA}} =  W_{o} \, A \, V + b_{o},
-1em]
          
          \myfigc{tiger/original_36951856_0.jpeg} \hfill 
          \myfigc{tiger/matrix_index0_0_0.pdf} \myfigc{tiger/matrix_index0_0_1.pdf} \myfigc{tiger/matrix_index0_0_2.pdf} \myfigc{tiger/matrix_index0_0_3.pdf} \-1em]
          
          \myfigc{tiger/original_36951856_1.jpeg} \hfill
          \myfigc{tiger/matrix_index1_0_0.pdf} 
          \myfigc{tiger/matrix_index1_0_1.pdf} 
          \myfigc{tiger/matrix_index1_0_2.pdf} 
          \myfigc{tiger/matrix_index1_0_3.pdf} \-1em]
          
          \myfigc{tiger/original_36951856_4.jpeg} \hfill
          \myfigc{tiger/matrix_index4_0_0.pdf} 
          \myfigc{tiger/matrix_index4_0_1.pdf} 
          \myfigc{tiger/matrix_index4_0_2.pdf} 
          \myfigc{tiger/matrix_index4_0_3.pdf} \-1em]
\end{minipage}}
    \caption{Visualization of the attention maps in the class-attention stage, obtained with a XXS model. For each image we present two rows: the top row correspond to the four heads of the attention maps associated with the first CA layer. The bottom row correspond to the four heads of the second CA layer. 
    \label{fig:visu_attention_appendix1}}
\end{figure}

\newcommand \inssal[2]  {
\centering \scalebox{0.7}{\scriptsize #1} \-7pt]
}

\begin{figure*}[p]
\vspace{-62pt}
\hspace{-55pt}
\scalebox{1.3}{
\begin{minipage}{0.325\linewidth}
\inssal{fountain (57\%)}{bridge-cascade-environment-fall-358457.jpg}
\inssal{American alligator (77\%)}{damon-on-road-xgMicu8jw64-unsplash.jpg}
\inssal{African chameleon (24\%), leaf beetle (12\%)}{erwan-hesry-AnbW1pMD4kY-unsplash.jpg}
\inssal{lakeside (40\%), alp (19\%),  valley(17\%)}{landscape-photography-of-white-mountain-753325.jpg}
\inssal{ambulance (24\%), traffic light (23\%)}{pexels-artem-saranin-1853537.jpg}
\inssal{barbershop (84\%)}{pexels-caleb-oquendo-3162022.jpg}
\inssal{soap dispenser (40\%), lavabo (39\%)}{pexels-castorly-stock-3761559.jpg}
\inssal{70 \% volcano}{pexels-chris-czermak-2444429.jpg}
\end{minipage}
\hfill
\begin{minipage}{0.325\linewidth}
\inssal{racket (73\%), tennis ball (10\%) }{pexels-cottonbro-5739196.jpg}
\inssal{golf ball (71\%)}{pexels-cottonbro-6256831.jpg}
\inssal{African elephant (57\%), water buffalo (15\%)}{pexels-frans-van-heerden-631317.jpg}
\inssal{viaduct (87\%)}{pexels-gabriela-palai-507410.jpg}
\inssal{baboon (22\%), black bear (17\%), hyena (16\%)}{pexels-gylfi-gylfason-5556828.jpg}
\inssal{\makebox{convertible (27\%), taxi (15\%), sport car (12\%), wagon (11\%)}}{pexels-jacob-morch-457418.jpg}
\inssal{catamaran (61\%)}{pexels-jess-vide-4319846.jpg}
\inssal{barrow (50\%), plow (26\%)}{pexels-laker-6156512.jpg}
\end{minipage}
\hfill
\begin{minipage}{0.325\linewidth}
\inssal{monarch butterfly (80\%)}{pexels-mododeolhar-2619377.jpg}
\inssal{minibus (21\%), recreational vehicle (18\%)}{pexels-nothing-ahead-3584101.jpg}
\inssal{cup (43\%), notebook computer (19\%)}{pexels-pixabay-257894.jpg}
\inssal{airliner (83\%)}{pexels-pixabay-46148.jpg}
\inssal{lakeside (24\%), coral fungus (16\%), coral reef (10\%)}{nature-photography-of-river-near-trees-1559117.jpg}
\inssal{plate (50\%), carbonara (18\%)}{pexels-sam-lion-5710181.jpg}
\inssal{flagpole (14\%), dam (11\%)}{pexels-tom-fisk-1624307.jpg}
\inssal{television (69\%)}{sebastien-le-derout-eMBahQSKmYs-unsplash.jpg}
\end{minipage}}
\caption{Illustration of the regions of focus of a \OURS-XXS model, according to the response of the first class-attention layer. 
\label{fig:saliency}}
\end{figure*}

\subsection{Attention map}

In Figure~\ref{fig:visu_attention_appendix1} we show the attention maps associated with the individual 4 heads of a XXS \OURS model, and for the two layers of class-attention. 
In \OURS and in contrast to ViT, the class-attention stage is the only one where there is some interaction between the class token and the patches, therefore it conveniently concentrates all the spatial-class relationship. 
We make two observations: 
\begin{itemize}
    \item The first class-attention layer clearly focuses on the object of interest, corresponding to the main part of the image on which the classification decision is performed (either correct or incorrect). 
    In this layer, the different heads focus either on the same or on complementary parts of the objects. This is especially visible for the waterfall image;
    \item The second class-attention layer seems to focus more on the context, or at least the image more globally. 
\end{itemize}


\subsection{Illustration of saliency in class-attention}

In figure~\ref{fig:saliency} we provide more vizualisations for a XXS model. They are just illustration of the saliency that one may extract from the first class-attention layer. As discussed previously this layer is the one that, empirically, is the most related to the object of interest. 
To produce these visual representations we simply average the attention maps from the different heads (depicted in Figure~\ref{fig:visu_attention_appendix1}), and upsample the resulting map to the image size. We then modulate the gray-level image with the strength of the attention after normalizing it with a simple rule of the form . 
We display the resulting image with \texttt{cividis} colormap.


For each image we show this saliency map and provides all the class for which the model assigns a probability higher than 10\%. These visualizations illustrate how the model can focus on two distinct regions (like racket and tennis ball on the top row/center). We can also observe some failure cases, like the top of the church classified as a flagpole. 
 \section{Related work }
\label{sec:related} 

Since AlexNet \cite{Krizhevsky2012AlexNet}, convolutional neural networks (CNN) are the standard in image classification~\cite{He2016ResNet,tan2019efficientnet,Touvron2019FixRes}, and more generally in computer vision. 
While a deep CNN can theoretically model long range interaction between pixels across many layers, there has been research in increasing the range of interactions within a single layer. Some approaches adapt the receptive field of convolutions dynamically \cite{dai2017deformable,Li2019SelectiveKN}. 
At another end of the spectrum, attention can be viewed as a general form of non-local means, which was used in filtering (e.g.~denoising \cite{buades2005non}), and more recently in conjunction with convolutions \cite{Wang2018NonlocalNN}. 
Various other attention mechanism have been used successfully to give a global view in conjunction with (local) convolutions \cite{bello2019attention,Ramachandran2019StandAloneSI,chen2020dynamic,Zhang2020ResNeStSN,zhao2020exploring}, most mimic \textit{squeeze-and-excitate} \cite{Hu2017SENet} for leveraging global features. 
Lastly, LambdaNetworks \cite{bello2021lambdanetworks} decomposes attention into an approximated content attention and a batch-amortized positional attention component. 

Hybrid architectures combining CNNs and transformers blocks have also been used on ImageNet~\cite{Srinivas2021BottleneckTF,wu2020visual} and on COCO~\cite{carion2020end}. Originally, transformers without convolutions were applied on pixels directly~\cite{parmar2018image}, even scaling to hundred of layers~\cite{child2019generating}, but did not perform at CNNs levels. More recently, a transformer architecture working directly on small patches has obtained state of the art results on ImageNet~\cite{dosovitskiy2020image}.
Nevertheless, the state of the art has since returned to CNNs~\cite{Brock2021HighPerformanceLI,Pham2020MetaPL}. 
While some small improvements have been applied on the transformer architecture with encouraging results \cite{Yuan2021TokenstoTokenVT}, their performance is below the one of  DeiT~\cite{Touvron2020TrainingDI},  which uses a vanilla ViT architecture.

\paragraph{Encoder/decoder architectures.} Transformers were originally introduced for machine translation \cite{vaswani2017attention} with encoder-decoder models, and gained popularity as masked language model encoders (BERT) \cite{devlin2018bert,liu2019roberta}. They yielded impressive results as scaled up language models, e.g. GPT-2 and 3 \cite{radford2019language,brown2020language}. 
They became a staple in speech recognition too \cite{luscher2019transformers,karita2019comparative}, being it in encoder and sequence criterion or encoder-decoder seq2seq \cite{synnaeve2019end} conformations, and hold the state of the art to this day \cite{xu2020self,zhang2020pushing} with models 36 blocks deep. 
Note, transforming only the class token with frozen trunk embeddings in \OURS is reminiscent of non-autoregressive encoder-decoders \cite{gu2017non,lee2018deterministic}, where a whole sequence (we have only one prediction) is produced at once by iterative refinements.

\paragraph{Deeper architectures}
 usually lead to better performance ~\cite{He2016ResNet,Simonyan2015VGG,Szegedy2015Goingdeeperwithconvolutions}, however this complicates their training process~\cite{Srivastava2015HighwayN,Srivastava2015TrainingVD}. One must adapt the architecture and the optimization procedure to train them correctly.
Some approaches focus on the initialization schemes~\cite{Glorot2010UnderstandingTD,He2016ResNet,Xiao2018DynamicalIA}, others on multiple stages training~\cite{Romero2015FitNetsHF,Simonyan2015VGG}, multiple loss at different depth~\cite{Szegedy2015Goingdeeperwithconvolutions}, adding components in the architecture~\cite{Bachlechner2020ReZeroIA,Zhang2019FixupIR} or regularization~\cite{Huang2016DeepNW}.
As pointed in our paper, in that respect our LayerScale approach is more related to Rezero~\cite{Bachlechner2020ReZeroIA} and  Skipinit~\cite{de2020batch}, Fixup \cite{Zhang2019FixupIR}, and T-Fixup \cite{huang2020improving}.

 \section{Conclusion}
\label{sec:conclusion}

In this paper, we have shown how train deeper transformer-based image classification neural networks when training on Imagenet only. 
We have also introduced the simple yet effective \OURS architecture designed in the spirit of encoder/decoder architectures. 
Our work further demonstrates that transformer models offer a competitive alternative to the best convolutional neural networks when considering trade-offs between accuracy and  complexity.  
\section{Acknowledgments}

Thanks to Jakob Verbeek for his detailled feedback on an earlier version of this paper, to Alaa El-Nouby for fruitful discussions, to Mathilde Caron for suggestions regarding the vizualizations, and to Ross Wightman for the Timm library and the insights that he shares with the community. 


\begingroup
    \small
    \bibliographystyle{ieee_fullname}
    \bibliography{egbib}
\endgroup

\clearpage

\appendix\newpage

\appendix


\clearpage
\counterwithin{figure}{section}
\counterwithin{table}{section}
\counterwithin{equation}{section}

\pagenumbering{Roman}  

\newpage
\vskip .375in
\begin{center}
{\Large \bf \inserttitle \\ \vspace{0.5cm} \large Appendix \par}
  \vspace*{24pt}
  {
  \par
  }
\end{center}
 
In this supplemental material, we provide variations on the architecture presented in our main paper. These experiments have guided some of our choices for the design of class-attention layers and LayerScale. 


\section{Variations on LayerScale init}
\label{sec:variant_layerscale}


For the sake of simplicity and to avoid overfitting per model, we have chosen to do a constant initialization with small values depending on the model depth. 
In order to give additional insight on the importance of this initialization we compare in Table~\ref{tab:init_comp_app} other possible choices. 

\paragraph{LayerScale with 0 init.}
We initialize all coefficients of LayerScale to 0. This resembles Rezero, but in this case we have distinct learnable parameters for each channel. We make two observations. First, this choice, which also starts with residual branches that output 0 the beginning of the training, gives a clear boost compared to the block-wise scaling done by our adapted ReZero. This confirms the advantage of introducing a learnable parameter per channel and not only per residual layer.  
Second, LayerScale is better: it is best to initialize to a small  different from zero. 



\paragraph{Random init.} 
We have tested a version in which we try a different initial weight per channel, but with the same average contribution of each residual block as in LayerScale. For this purpose 
we initialize the channel-scaling values with the Uniform law (). 
This simple choice choice ensures that the expectation of the scaling factor is equal to the value of the classical initialization of LayerScale. This choice is overall comparable to the initialization to 0 of the diagonal, and inferior to LayerScale. 

\newcommand {\compval} [1] {{\small [{\it #1}]}}
\begin{table}
    \caption{Performance when increasing the depth. We compare different strategies and report the top-1 accuracy (\%) on ImageNet-1k for the DeiT training  (Baseline) with and without adapting the stochastic depth rate  (uniform drop-rate), and a modified version of Rezero with LayerNorm and warmup. We compare different initialisation of the diagonal matrix for LayerScale. We also report results with 0 initialization, Uniform initialisation and small constant initialisation. Except for the baseline , we have adapted the stochastic depth rate .  
    \label{tab:init_comp_app}}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{c|cc|c|cll}
    \toprule
         \multirow{2}{3em}{\ depth} & baseline & baseline & ReZero & \multicolumn{3}{c}{LayerScale []}  \\
          &   & [] &    &   &    & \\         
         \midrule
         \pzo12    & 79.9\pzo      & 79.9\  \compval{0.05} & 78.3 & 79.7 & \pzo 80.2 \compval{0.1}     & 80.5 \compval{0.1}     \\ 
         \pzo18    & 80.1\pzo      & 80.7\  \compval{0.10} & 80.1 & 81.5 & \pzo 80.8 \compval{0.1}     & 81.7 \compval{0.1}     \\ 
         \pzo24    & 78.9 & 81.0\  \compval{0.20} & 80.8 & 82.1 & \pzo 82.1 \compval{10} & 82.4 \compval{10} \\ 
         \pzo36    & 78.9 & 81.9\  \compval{0.25} & 81.6 & 82.7 & \pzo 82.6 \compval{10} & 82.9 \compval{10} \\ 
          \bottomrule
    \end{tabular}}
\end{table}


\paragraph{Re-training.} 

LayerScale makes it possible to get increased performance by training deeper models. At the end of training we obtain a specific set of scaling factors for each layer.   
Inspired by the lottery ticket hypothesis~\cite{frankle2018lottery}, one question that arises is whether what matters is to have the right scaling factors, or to include these learnable weights in the optimization procedure. 
In other terms, what happens if we re-train the network with the scaling factors obtained by a previous training? 



In this experiment below, we try to empirically answer that question. We compare the performance (top-1 validation accuracy, \%) on ImageNet-1k with DeiT-S architectures of differents depths. Everything being identical otherwise, in the first experiment we use LayerScale, i.e. we have learnable weights initialized at a small value . In the control experiment we use fixed scaling factors initialised at values obtained by the LayerScale training. 

\begin{center}
   \centering \scalebox{0.9}{
    \begin{tabular}{lcccc}
    \toprule
    ~\hspace{3cm} Depth               
                                  &  12  &  18  &   24 &  36  \\
    \midrule
    LayerScale                    & 80.5 & 81.7 & 82.4 & 82.9 \\
    Re-trained with fixed weights & 80.6 & 81.5 & 81.2 & 81.6 \\
    \bottomrule
    \end{tabular}
    }
\end{center}

We can see that the control training with fixed weights also converges, but it is only slightly better than the baseline with adjusted stochastic depth drop-rate . 
Nevertheless, the results are lower than those obtained with the learnable weighting factors. 
This suggests that the evolution of the parameters during training has a beneficial effect on the deepest models. 



\section{Design of the class-attention stage} 
\label{sec:variant_class_attention}

In this subsection we report some results obtained when considering alternative choices for the class-attention stage. 

\paragraph{Not including class embedding in keys of class-attention.} 
In our approach we chose to insert the class embedding in the class-attention: By defining 

we include  in the keys and therefore the class-attention includes attention on the class embedding itself in Eqn.~\ref{equ:ca2} 
and Eqn.~\ref{equ:ca3}. 
This is not a requirement as we could simply use a pure cross-attention between the class embedding and the set of frozen patches. 

If we do not include the class token in the keys of the class-attention layers, i.e., if we define , 
we reach 83.31\% (top-1 acc. on ImageNet1k-val) with CaiT-S-36, versus 83.44\% for the choice adopted in our main paper. 
This difference of +0.13\% is likely not significant, therefore either choice is reasonable. In order to be more consistent with the self-attention layer SA, in the sense that each query has its key counterpart, we have kept the class embedding in the keys of the CA layers as stated in our paper.

\paragraph{Remove LayerScale in Class-Attention.} 
If we remove LayerScale in the Class-Attention blocks in the CaiT-S-36 model, we obtain a top-1 accuracy of 83.36\% on ImageNet1k-val, versus 83.44\% with LayerScale. 
The difference of +0.08\% is not significant enough to conclude on a clear advantage. For the sake of consistency we have used LayerScale after all residual blocks of the network. 

\begin{table}[t]
    \caption{CaiT models with and without distillation token.
    All these models are trained with the same setting during 400 epochs. \smallskip \label{tab:dist_token}}    
    \centering
    \scalebox{0.9}{
    \begin{tabular}{c|cc}
    \toprule
                & \multicolumn{2}{c}{Distillation token}\\
         Model  & \ding{55} & \checkmark \\
         \midrule
         XXS-24\alambic& 78.4 & 78.5\\
         M-24\alambic & 84.8 & 84.7\\
    \bottomrule
    \end{tabular}}
\end{table}


\paragraph{Distillation with class-attention} 

In the main paper we report results with the hard distillation proposed by Touvron \etal~\cite{Touvron2020TrainingDI}, which in essence replaces the label by the average of the label and the prediction of the teacher output. This is the choice we adopted in our main paper, since it provides better performance than traditional distillation. 


The DeiT authors also show the advantage of considering an additional ``distillation token''. In their case, employed with the ViT/DeiT architecture, this choice improves the performance compared to hard distillation. Noticeably it accelerates convergence.   
 
In Table~\ref{tab:dist_token} we report the results obtained when inserting a distillation token at the same layer as the class token, i.e., on input of the class-attention stage. 
In our case we do not observe an advantage of this choice over hard distillation when using class-attention layers. Therefore in our paper we have only considered the hard distillation also employed by Touvron \etal~\cite{Touvron2020TrainingDI}. 


 

\end{document}
