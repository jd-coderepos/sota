\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{comment}
\usepackage[noend]{algpseudocode}
\usepackage{xcolor}
\newcommand{\etal}{\textit{et al.}}

\begin{document}
\title{Study Group Learning: Improving Retinal Vessel Segmentation Trained with Noisy Labels }
\titlerunning{SGL for Retinal Vessel Segmentation}
\author{Yuqian Zhou\inst{1} \and
Hanchao Yu\inst{1}\and
Humphrey Shi\inst{1,2}}
\authorrunning{Zhou et al.}
\institute{University of Illinois at Urbana-Champaign \and
University of Oregon\\
}



\maketitle              \begin{abstract}
Retinal vessel segmentation from retinal images is an essential task for developing the computer-aided diagnosis system for retinal diseases. Efforts have been made on high-performance deep learning-based approaches to segment the retinal images in an end-to-end manner. However, the acquisition of retinal vessel images and segmentation labels requires onerous work from professional clinicians, which results in smaller training dataset with incomplete labels. As known, data-driven methods suffer from data insufficiency, and the models will easily over-fit the small-scale training data. Such a situation becomes more severe when the training vessel labels are incomplete or incorrect. In this paper, we propose a Study Group Learning (SGL) scheme to improve the robustness of the model trained on noisy labels. Besides, a learned enhancement map provides better visualization than conventional methods as an auxiliary tool for clinicians. Experiments demonstrate that the proposed method further improves the vessel segmentation performance in DRIVE and CHASEDB1 datasets, especially when the training labels are noisy. Our code is available at {\color{magenta} \url{https://github.com/SHI-Labs/SGL-Retinal-Vessel-Segmentation}}.

\keywords{Retinal vessel segmentation  \and Image enhancement.}
\end{abstract}
\section{Introduction}
Retinal inspection is an effective approach for the diagnose of multiple retinal diseases including diabetic retinopathy, epiretinal membrane, retinal detachment, retinal tear \textit{etc}. Among them, retinal vacular disorders which affects retinal blood vessels are usually caused by other medical diseases like artherosclerosis, hypertension, or human blood circulation problems \cite{brand2012management}. Those disorders will severely influence human's vision functions and cause obvious symptoms, but can be effectively diagnosed and analyzed by retinal vessel inspection in the collected fundus images. Advanced medical imaging system makes it possible to obtain high-resolution fundus images. However, in practical medical services, visual inspection may still require the involvement and tedious work of neurologists, cardiologists, ophthalmologists, and other experts in retinal vascular diseases. To release their burden on screening multiple diseased retino from thousands even millions of healthy retinos, an automatic and high-performance Computer-Aided Diagnosis (CAD) system is desirable to conduct pre-screening and other auxiliary works. Specifically for retinal vascular diseases, we expect the system to provide high-quality enhanced images for a better visualization, and reasonable segmentation of the vessel patterns from the complex and noisy images.   

Plenty of previous efforts have been made in automatic retinal vessel segmentation. Conventionally, hand-crafted filters \cite{niemeijer2004comparative,oloumi2007detection,ricci2007retinal,you2011segmentation} like Gabor \cite{oloumi2007detection} and Gaussian-based ones \cite{niemeijer2004comparative} are explored to extract features for pixel selection, vessel clustering and segmentation. Recently, data-driven based methods utilize UNet-based model \cite{ronneberger2015u} or its variants \cite{wang2020rvseg,xu2020boosting,zhang2020befd,zhang2020retinal,lan2020elastic} to achieve significant performance compared with traditional methods. Those deep learning methods focus on the design of UNet structures with better feature representation \cite{zhang2020befd,wang2020rvseg}, or the decouple of structure and textures of retinal images \cite{zhang2020retinal}. However, data-driven methods highly suffer from over-fitting issues when the given training data is insufficient. Previously proposed methods cannot overcome the issues of small-scale training data with noisy labels given by the clinicians. 

Effectively training networks with noisy labels \cite{song2020learning} is a rigid need in industry and an interesting task in academia. Previous research works mostly focus on image classification tasks and develop methodologies like optimizing robust loss functions \cite{ghosh2017robust,zhang2018generalized}, regularizing labels \cite{goodfellow2014explaining,pereyra2017regularizing}, or actively selecting samples \cite{jiang2018mentornet,han2018co} \textit{etc.}. However, noisy pixel-level labels existing in segmentation tasks are not well-studied, especially for medical image processing tasks. Shu \textit{et al.} proposed a LVC-Net \cite{shu2019lvc} to adjust the incorrect pixel-wise labels via a deformable spatial transformation module guided by low-level visual cues. But their method cannot be applied to retinal blood vessel images, because the entire small blood vessels may be mislabeled and cannot be corrected via spatial transformation. Xue \textit{et al.} \cite{xue2020cascaded} studied a multi-stage training framework with sample selection for chest X-ray images. They synthesized noisy annotations with image dilation and erosion. However, due to the thickness of blood vessels and less training data compared to other medical image data, neither label synthesis nor sampling procedures are suitable for blood vessel images. Considering the characteristics of blood vessel images, we introduce a novel noisy label synthesis pipeline for retinal vessel images, and propose a Study-Group Learning (SGL) framework to improve the model robustness on noisy labels. 

In this paper, we mainly study the two main practical problems for retinal vessel segmentation task. First, we explore the deeply unsupervised learned enhancement of the original retinal images compared with traditional contrast adjustment methods like CLAHE \cite{pizer1987adaptive}. Second, suppose the ground truth segmentation labels given by the clinicians are incomplete and noisy, which yields missing annotations of some vessel segments, we study the effective learning scheme to improve the robustness of the model while training on noisy labels. Therefore, the contributions can be summarized in the following aspects. 
\begin{itemize}
  \item First, to better visualize the retinal image and understand the model, we design the network to output the learned enhancement map. Compared with other baselines, the high-contrast learned map are better visually plausible, and provides a better auxiliary tool to aid clinicians for visual inspection or manual segmentation.
  \item Second, we design a pipeline to manipulate vessel segmentation labels. Given a complete annotations, the proposed approach simulates to automatically erase some vessel segments. \item Third, we propose a Study Group Learning (SGL) framework to improve the generalization ability of the learned model, and better address the missing annotation problems in the training set. The model achieves a more robust performance even some of the vessel pixels are mislabeled.
\end{itemize}



\section{Methodology}
\subsection{Model Structure}
\begin{figure}[t]\centering
\includegraphics[width=1\linewidth]{model.pdf}
\caption{Baseline model structure and the proposed Study Group Learning (SGL) scheme. The baseline network is a concatenated UNet consisting of an enhancement module and a segmentation module. The three-channel enhancement map  is obtained from the bottleneck structure. The SGL is inspired by the cross-validation training scheme and knowledge distillation idea. We first split the whole training set  into  subsets  and feed the model  with . The obtained estimation  of  is utilized as the pseudo labels for joint optimization.}
\label{fig:model}
\end{figure}
The proposed baseline model structure is illustrated in Fig. \ref{fig:model}. We utilize the concatenated UNet consisting of the enhancement and segmentation modules to learn both the enhancement and the segmentation map. Different from previous works, we do not pre-process the retinal images, but directly utilize the raw captured images to preserve the entire information. Specifically, given a three-channel raw retinal image  as the input, we aim to process the image by enhancing its contrast and highlighting the vessel structures in , and estimate the segmentation map  of the vessels matched with the ground truth segmentation map  given by professional clinicians. Notice that  preserves the maximum image contents including the vessel structures and retinal textures. It helps the clinicians to inspect the segmentation results  and better explains the learned model. 

During testing, we enhance the raw image and infer the segmentation maps from unseen retinal images. To cope with noisy labels in vessel segmentation datasets, we propose and follow a Study Group Learning (SGL) scheme to train the baseline model, which is explained in the following subsections. 

\subsection{Study Group Learning (SGL) Scheme}
Tasks like retinal vessel segmentation faces the problem of small-scale dataset and incorrect or incomplete vessel annotations. These two practical problems will make the deeply trained model very easily over-fitting the training set. The severe over-fitting problem does harm to the generalization ability and robustness of the model to unseen testing data. In addition to conventional data augmentation approaches like image transformation and random warping, we propose to alternate the training scheme. Inspired by K-fold cross-validation scheme and knowledge distillation \cite{hinton2015distilling} approaches, we propose the K-fold Study Group Learning (SGL) to better cope with noisy labels in small datasets, especially for the retinal vessel segmentation task.

The pipeline is illustrated in Fig. \ref{fig:model}. Specifically, we first randomly and averagely split the whole training set  into  subsets , where . Like the cross-validation scheme, we train totally  models . For each , we train it on  using pixel-wise binary cross entropy loss. After optimizing the model set  as , we infer the estimated segmentation label  of  as the pseudo label, where
 
Finally, we train a model  from scratch by jointly optimizing the ground truth vessel labels  and the obtained pseudo label set  as,
 
where ,  is the cross entropy loss, and .
\begin{figure}[t]\centering
\includegraphics[width=1\linewidth]{pseudo.pdf}
\caption{Inference results on one training example. Clinicians may only label some salient vessels while ignoring the ambiguous ones. The model trained on the entire set will over-fit the ground truth annotations. However, inference results from the model trained on the subset can serve as the pseudo label for model regularization purpose. }
  \label{fig:inf}
\end{figure}

Intuitively, we name the proposed learning scheme as Study Group Learning (SGL) because each model  trained with partial training set can be regarded as a 'Study Group'. The final model  is a process of group discussion by merging and fusing the knowledge from different study groups. Theoretically, the second term in Equ. \ref{eq:loss} can be regarded as a regularization to avoid over-fitting the ground-truth labels, especially when the given labels contain some noises and possibly incorrect annotations. Fig. \ref{fig:inf} shows one example of such cases. In this example, we set , and the inference of the training samples  from  and  are shown in the right two columns, where  is in the training set of  but not in the training partition of .  highly overfits the given labels of  by ignoring multiple thin and ambiguous vessels. However, if not trained on ,  can infer some of the ignored vessels as the pseudo labels. While combining these two labels, the final model  can intuitively learn better representation and become more generalized. 

\subsection{Vessel Label Erasing}
\begin{figure}[t]\centering
\includegraphics[width=1\linewidth]{syn.pdf}
\caption{Vessel label erasing process. Given the complete label map , we compute the skeleton of the vessels, and dilate the vessel skeleton to mask by drawing the polylines with width . We then rank the vessel segments by their approximated thickness, and include the thick vessels according to ratio . The second row shows one instance of label erasing with  from 1 to 0.3.}
\label{fig:synp}
\end{figure}
Annotating the retinal vessels requires the involvement of professional clinicians, and the process of manual labeling is onerous, which reveals one of the reasons why public retinal vessel databases are always small-scale or partially-labeled. It is also common that some labels of thinner vessels are missing due to the annotators' errors. To resemble this practical situation in industry, we propose to synthesize an incomplete map  by erasing some labeled vessel segments  from the ground truth segmentation map , where  is the removal ratio. The process is illustrated in Fig. \ref{fig:synp}.

To generate , we first compute and approximate the skeleton of  using the method proposed by Zhang \etal \cite{zhang1984fast} followed by a skeleton tracing approach \footnote{skeleton-tracing: https://github.com/LingDong-/skeleton-tracing}. This algorithm converts the binary segmentation map into a set of polylines , where  and each  is stored as an array of coordinates \textit{i.e.} . The geometric polylines and their spatial relationship represent the topological skeleton of the annotated vessels. We utilize it to locate the vessel center lines, and roughly compute the thickness of the vessels. 

Second, to compute and rank the thickness of each vessel segment, we redraw the polylines with larger width  on a black canvas to form the complete mask . Notice that  is the least value which makes  cover the entire vessel regions in . For each , the corresponding pixel regions covered by the drawing lines of width  is denoted by , then the thickness  of the vessel segment is measured by . We rank the polylines according to their thickness , and include the thickest vessels in order in the partial set , where  and . In , we include the top- thick vessels. Finally, we form the selected mask  from , and the synthetic segmentation mask ablating some of the thin vessels with ratio  can be computed by . Fig. \ref{fig:synp} shows one example of  where r is from 1 to 0.3. Adding a small portions of thin vessels helps the model maintain the ability of segmenting smaller objects. Therefore, we also randomly select  of thin vessels in the set and add them to . 
\section{Experiments}
\subsection{Dataset and Implementation}
\textbf{DRIVE:} The Digital Retinal Images for Vessel Extraction (DRIVE) \cite{staal2004ridge} dataset consists of 40 images of size . The training and testing set are fixed, and the ground truth manual annotations are given. We do not resize the image to alternate the resolution or change the Aspect-Ratio.\\
\textbf{CHASEDB1:} \cite{fraz2012ensemble} It consists of 28 retinal images of size . For a fair comparison with previous methods, we use the first 20 images for training, and the remaining 8 images for testing.

While training the model, we randomly crop the images into  patches, and apply data augmentation including horizontal and vertical flip, rotation, transpose, and random elastic warping \cite{simard2003best}. While testing, suppose the image size is , we zero-pad the input image to size , where , and crop the estimated map accordingly. Specifically,  is 37 for DRIVE and 63 for CHASEDB1 dataset. It makes the arbitrary-sized inputs adaptive to the UNet structure with four down-sampling layers, and retain the original resolution and aspect-ratio of the image. 

\subsection{Learned Retinal Image Enhancement}
\begin{figure}[t]\centering
\includegraphics[width=1\linewidth]{enh.pdf}
\caption{The learned enhancement map  compared with other baseline methods including Histogram Equalization (HE), Contrast Limited Adaptive Histogram Equalization (CLAHE) 
\cite{pizer1987adaptive}, and Single-scale Retinex \cite{zhao2015retinal}. The learned map demonstrates a better contrast and intensity, enhancing the vessel information for a better identifiable visualization for clinicians. Top row is from DRIVE dataset and the bottom row is from CHASEDB1 dataset. Zooming in for better visualization. }
  \label{fig:enh}
\end{figure}

\begin{figure}[t]\setlength{\abovecaptionskip}{0pt}
\centering
\includegraphics[width=1\linewidth]{sgl.pdf}
\caption{The performance of the model in the simulated training set with label noise ratio . The proposed SGL learning scheme overall improves the robustness in all . Left two columns: DICE and AUC scores on DRIVE. Right two columns: on CHASEDB1 dataset.}
\label{fig:drive}
\end{figure}
We learn the enhanced map of the original input images by supervising the model on the manual segmentation labels. We extract the bottleneck 3-channel output which can be visualized in a better contrast level. We compare the learned enhancement of the retinal images with other baseline methods including Histogram Equalization (HE), Contrast Limited Adaptive Histogram Equalization (CLAHE) \cite{pizer1987adaptive}, and Retinex \cite{zhao2015retinal}. 

Fig. \ref{fig:enh} shows one example of the visual comparison. Traditional methods like HE, CLAHE and Retinex cannot achieve a uniform contrast level both locally and globally. The cropped patches are from either a brighter or darker regions of the image, making the vessel pixels hard to parse accurately by the inspector. However, the learned map in the fifth column demonstrates a better contrast and intensity level, enhancing the vessel information for a better identifiable visualization for clinicians, especially for the dark regions in CHASEDB1 images. It highlights the vessel regions while preserving the textures. The obtained enhancement images can also be utilized for visual inspection or labeling.
\subsection{Study Group Learning}


\begin{table}[t]\setlength{\tabcolsep}{5pt}
\centering
\footnotesize
\caption{Comparison with other baseline methods on DRIVE dataset. }
\begin{tabular}{|r|c|c|c|c|c|c|}
\hline
Method & Year & Sensitivity & Specificity & DICE & Accuracy & AUC \\ \hline
R2U-Net \cite{alom2019recurrent}    & 2018 &0.7792 &0.9813 &0.8171 &0.9556 &0.9784\\
LadderNet \cite{zhuang2018laddernet} & 2018 &0.7856 &0.9810 &0.8202 &0.9561 &0.9793\\
Dual E-UNet \cite{wang2019dual}   &2019 &0.7940 &0.9816 &0.8270 &0.9567 &0.9772\\
IterNet \cite{li2020iternet}     &2020 &0.7791 &0.9831 &0.8218 &0.9574 &0.9813\\
SA-UNet \cite{guo2020sa}     &2020 &0.8212 &0.9840 &0.8263 &0.9698 &0.9864\\
BEFD-UNet \cite{zhang2020befd}& 2020 &0.8215 &\bf{0.9845} &0.8267& 0.9701& 0.9867 \\\hline
Our Baseline &2021 &0.8341 &0.9827 &0.8262 &0.9695 &0.9867 \\ 
Our SGL (K=8) &2021&\bf{0.8380} &0.9834 &\bf{0.8316} &\bf{0.9705} &\bf{0.9886} \\\hline

\end{tabular}\label{exp:drive}
\end{table}

\begin{table}[t]\setlength{\tabcolsep}{5pt}
\centering
\footnotesize
\caption{Comparison with other baseline methods on CHASEDB1 dataset. }
\begin{tabular}{|r|c|c|c|c|c|c|}
\hline
Method & Year & Sensitivity & Specificity & DICE & Accuracy & AUC \\\hline
R2U-Net \cite{alom2019recurrent}    & 2018 &0.7756 &0.9712 &0.7928&0.9634 &0.9815\\
LadderNet \cite{zhuang2018laddernet} & 2018 &0.7978 &0.9818 &0.8031 &0.9656 &0.9839\\

Dual E-UNet \cite{wang2019dual}   &2019 &0.8074 &0.9821 &0.8037 &0.9661 &0.9812\\
IterNet \cite{li2020iternet}     &2020 &0.7969 &0.9881 &0.8072 &0.9760 &0.9899\\
SA-UNet \cite{guo2020sa}     &2020 &0.8573 &0.9835 &0.8153 &0.9755 &0.9905\\

\hline
Our Baseline &2021 &0.8502 &\bf{0.9854} &0.8232 &0.9769 &0.9913 \\ 
 Our SGL (K=8) &2021&\bf{0.8690} &0.9843 &\bf{0.8271} &\bf{0.9771} &\bf{0.9920} \\\hline
\end{tabular}\label{exp:chase}
\vspace{-4mm}
\end{table}

Table \ref{exp:drive} and \ref{exp:chase} illustrate the effectiveness of the proposed SGL scheme. Compared with previous works, the proposed learning scheme can boost the DICE score \cite{sorensen1948method} and other evaluation metrics by a large margin. Fig. \ref{fig:drive} shows the DICE and Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) of the model in the simulated training set with label noise ratio , where  represents the original training set. As shown in the figure, erasing some vessel labels in the training set will drastically degrade the system performance, while the SGL learning scheme overall improves the robustness on both datasets. Among all the metrics, AUC does not relate to the thresholding method, indicating a better ability of the model segmenting vessel pixels. Besides, a better sensitivity indicates the model is able to extract more thin vessels and boundary pixels. More results can be found in the supplementary material.

\section{Conclusions}
In this paper, we studied the learning-based retinal vessel segmentation model trained with noisy labels. Specifically, we designed the pipeline of synthesizing noisy labels, and proposed a Study Group Learning (SGL) scheme boosting the performance of model trained with imperfect labels. Besides, the learned enhanced images as a side product made the model explainable and helped the clinicians for visual inspection. We still discovered the gap between models trained with different levels of noisy labels, leaving for future work to further improve the model sensitivity.

\section{Acknowledgement}
This project has been funded by the Jump ARCHES endowment through the Health Care Engineering Systems Center. This work also utilizes resources supported by the National Science Foundationâ€™s Major Research Instrumentation program, grant number 1725729, as well as the University of Illinois at Urbana-Champaign.

\section{Appendix: Quantitative and Qualitative Result}
\begin{table}[h]\setlength{\tabcolsep}{6pt}
\setlength{\abovecaptionskip}{0pt}
\centering
\footnotesize
\caption{Results of different label missing ratio  and fold number  on DRIVE. }
\begin{tabular}{|r|c|c|c|c|c|c|c|}
\hline
r& K & Accuracy & AUC & Sensitivity & Specificity &   DICE (F1) &Vessel IoU  \\ \hline\hline

1 &1&0.9695&0.9867&0.8341&0.9827&0.8262&0.7041\\ \hline
&2&0.9701&0.9884&0.8432&0.9825&0.8309&0.7110\\ \hline
&4&0.9705&0.9887&0.8358&0.9836&0.8312&0.7115\\ \hline
&8&0.9705&0.9886&0.8380&0.9834&0.8316&0.7120\\ \hline\hline
0.9&1&0.9697&0.9867&0.8262&0.9837&0.8260&0.7040\\ \hline
&2&0.9706&0.9884&0.8231&0.9850&0.8299&0.7095\\ \hline
&4&0.9706&0.9886&0.8323&0.9840&0.8310&0.7111\\ \hline
&8&0.9703&0.9887&0.8418&0.9828&0.8313&0.7116\\ \hline\hline
0.7&1&0.9691&0.9858&0.8045&0.9852&0.8194&0.6945\\ \hline
&2&0.9696&0.9873&0.8102&0.9851&0.8224&0.6988\\ \hline
&4&0.9702&0.9879&0.8081&0.9860&0.8250&0.7024\\ \hline
&8&0.9701&0.9882&0.8104&0.9857&0.8251&0.7027\\ \hline
\end{tabular}\label{exp:drive_full}
\end{table}

\begin{table}[th]\setlength{\tabcolsep}{6pt}
\centering
\footnotesize
\caption{Results of different label missing ratio  and fold number  on CHASEDB1. }
\begin{tabular}{|r|c|c|c|c|c|c|c|}
\hline
r& K & Accuracy & AUC & Sensitivity & Specificity &   DICE (F1) &Vessel IoU  \\ \hline\hline
1&1&0.9769&0.9913&0.8502&0.9854&0.8232&0.6998\\ \hline
&2&0.9774&0.9920&0.8586&0.9855&0.8275&0.7060\\ \hline
&4&0.9769&0.9920&0.8632&0.9846&0.8248&0.7020\\ \hline
&8&0.9771&0.9920&0.8690&0.9843&0.8271&0.7054\\ \hline\hline
0.9&1&0.9765&0.9901&0.8483&0.9851&0.8204&0.6957\\ \hline
&2&0.9761&0.9913&0.8716&0.9831&0.8211&0.6967\\ \hline
&4&0.9771&0.9915&0.8494&0.9856&0.8236&0.7004\\ \hline
&8&0.9768&0.9917&0.8652&0.9843&0.8246&0.7017\\ \hline\hline
0.7&1&0.9759&0.9891&0.8182&0.9864&0.8111&0.6824\\ \hline
&2&0.9761&0.9904&0.8250&0.9863&0.8131&0.6852\\ \hline
&4&0.9761&0.9906&0.8341&0.9856&0.8155&0.6887\\ \hline
&8&0.9757&0.9910&0.8480&0.9843&0.8151&0.6881\\ \hline
\end{tabular}\label{exp:chase_full}
\end{table}
\begin{figure}[th]\centering
\includegraphics[width=0.9\linewidth]{drive1.pdf}
\caption{Enhancement and segmentation results on DRIVE. From left to right: the raw retinal image, the learned enhanced image, the probability map, and the binary segmentation result. Red color indicates false negative, and green color indicates false positive. Thin vessels are still very challenging to be segmented. }
  \label{fig:drive2}
\end{figure}
\clearpage
\begin{figure}[th]\centering
\includegraphics[width=0.9\linewidth]{chase.pdf}
\caption{Enhancement and segmentation results on CHASEDB1. From left to right: the raw retinal image, the learned enhanced image, the probability map, and the binary segmentation result. Red color indicates false negative, and green color indicates false positive. }
  \label{fig:chase}
\end{figure}
\clearpage


\bibliographystyle{splncs04}
\bibliography{mybib}

\end{document}
