

\documentclass[11pt]{svjour3} \usepackage{fullpage} 



\usepackage{xspace}


\usepackage{graphicx}

\usepackage{amssymb}

\renewcommand{\paragraph}[1]{\smallskip\vspace{2pt}\par{\em #1}~}

\newcommand{\primalOf}[1]{{{#1}}}
\newcommand{\dualOf}[1]{{\hat{#1}}}

\newcommand{\MM}{M}
\newcommand{\JJ}{J}
\newcommand{\xp}{\primalOf x}
\newcommand{\yp}{\primalOf y}
\newcommand{\pp}{\primalOf p}
\newcommand{\wwp}{\primalOf u}
\newcommand{\Ip}{\calI}

\newcommand{\xd}{\dualOf x}
\newcommand{\yd}{\dualOf y}
\newcommand{\pd}{\dualOf p}
\newcommand{\wwd}{\dualOf u}
\newcommand{\Jd}{\calJ}

\newcommand{\dd}{\delta}



\newenvironment{Proof}{\begin{proof}}{{} ~\hfill\hfill\qed~\end{proof}}

\newcommand{\algfont}{}
\newcommand{\tab}{\hspace*{0.13in}}

\newcounter{myline}

\newenvironment{alg}{
  \medskip
  \par
  \algfont
  \centering
    \begin{tabular}{|@{}|l|}\hline
      \begin{minipage}{0.96\linewidth}\raggedright
          \smallskip
          \begin{list}{\arabic{myline}.}{
            \usecounter{myline}
            \setlength{\listparindent}{0in}
            \setlength{\topsep}{0in}
            \setlength{\itemsep}{.013in}
            \setlength{\parsep}{.013in}
            \setlength{\rightmargin}{0in}
            \setlength{\itemindent}{0in}
\setlength{\labelsep}{0.065in}
            \setlength{\leftmargin}{0.2in}
}
          }{
        \end{list}
        \smallskip
      \end{minipage}\\\hline
    \end{tabular}
    \par
    \noindent
}
\newenvironment{halfalg}{
\centering
    \begin{tabular}{|@{}|l|}\hline
      \begin{minipage}{0.7\textwidth}\raggedright
          \smallskip
          \begin{list}{\arabic{myline}.}{
            \usecounter{myline}
            \setlength{\listparindent}{0in}
            \setlength{\topsep}{0in}
            \setlength{\itemsep}{.01in}
            \setlength{\parsep}{.01in}
            \setlength{\rightmargin}{0in}
            \setlength{\itemindent}{0in}
\setlength{\labelsep}{0.055in}
            \setlength{\leftmargin}{0.175in}
}
          }{
        \end{list}
        \smallskip
      \end{minipage}\\\hline
    \end{tabular}
    \par
    \noindent
}
\newcommand{\A}{\item}
\newcommand{\Anonum}{\item[]}
\newcommand{\Along}[1]{\item\parbox[t]{0.9\linewidth}{ #1}\smallskip}
\newcommand{\Ahead}[1]{\item[]\hspace*{-\leftmargin}{\textrm{#1}}}
\newcommand{\Ain}[1]{\Ahead{{~\textrm{\small{input:} #1}}}}
\newcommand{\Aout}[1]{\Ahead{{~\textrm{\small{output:} #1}}}}

\newcommand{\algbeg}{\addtolength{\labelsep}{0.13in}
  \addtolength{\itemindent}{0.13in}
  \addtolength{\listparindent}{0.13in}
}
\newcommand{\algend}{\addtolength{\labelsep}{-0.13in}
  \addtolength{\itemindent}{-0.13in}
  \addtolength{\listparindent}{-0.13in}
}

\newcommand{\eps}{\varepsilon}
\newcommand{\tran}{^{\scriptscriptstyle \sf T\hspace*{-0.05em}}}

\makeatletter
\newcommand{\mymathfnnolimits}[1]{\mathop {\operator@font\sf #1}\nolimits }
\newcommand{\mymathfn}[1]{\mathop {\operator@font\sf #1}}
\makeatother


\newcommand{\prob}[1]{{\sc #1}\xspace}
\renewcommand{\prob}[1]{{#1}\xspace}

\newcommand{\E}{{\rm E}}
\newcommand{\opt}{\mbox{{\sc opt}}}

\newcommand{\minimize}{\mymathfnnolimits{{minimize\,}}}
\newcommand{\maximize}{\mymathfnnolimits{{maximize\,}}}

\newcommand{\giv}{\,|\,}

\newcommand{\calA}{{\cal A}}
\newcommand{\calB}{{\cal B}}
\newcommand{\calC}{{\cal C}}
\newcommand{\calD}{{\cal D}}
\newcommand{\calE}{{\cal E}}
\newcommand{\calF}{{\cal F}}
\newcommand{\calG}{{\cal G}}
\newcommand{\calH}{{\cal H}}
\newcommand{\calI}{{\cal I}}
\newcommand{\calJ}{{\cal J}}
\newcommand{\calK}{{\cal K}}
\newcommand{\calL}{{\cal L}}
\newcommand{\calM}{{\cal M}}
\newcommand{\calN}{{\cal N}}
\newcommand{\calO}{{\cal O}}
\newcommand{\calP}{{\cal P}}
\newcommand{\calQ}{{\cal Q}}
\newcommand{\calR}{{\cal R}}
\newcommand{\calS}{{\cal S}}
\newcommand{\calT}{{\cal T}}
\newcommand{\calU}{{\cal U}}
\newcommand{\calV}{{\cal V}}
\newcommand{\calW}{{\cal W}}
\newcommand{\calX}{{\cal X}}
\newcommand{\calY}{{\cal Y}}
\newcommand{\calZ}{{\cal Z}}

\newcommand{\R}{{\sf R\hspace*{-1.5ex}\rule{0.15ex}{1.6ex}\hspace*{1.2ex}}}
\newcommand{\Rp}{\R_{\scriptscriptstyle +}}
\newcommand{\N}{{\sf N\hspace*{-1.5ex}\rule{0.15ex}{1.6ex}\hspace*{1.2ex}}}
\newcommand{\Z}{{\sf Z\hspace*{-1.1ex}Z}}
\newcommand{\Zp}{\Z_{\scriptscriptstyle +}}

\newcommand{\etal}{{et al}\xspace}

\newcommand{\Deltat}[1]{\ensuremath{\raisebox{-0.05em}{}}}

\newcommand{\text}[1]{\mbox{#1}}
\newcommand{\rows}{r}
\newcommand{\columns}{c}
\newcommand{\inputsize}{n}

\begin{document}
\journalname{}
\def\makeheadbox{}

\title{A Nearly Linear-Time PTAS for Explicit \\Fractional Packing and Covering Linear Programs}
\titlerunning{Nearly Linear-Time PTAS for Packing and Covering LPs}
\author
{
	Christos Koufogiannakis \and Neal E.\ Young
}
\institute
        {Department~of Computer Science and Engineering, University~of California, Riverside.
          The first author would like to thank the Greek State Scholarship Foundation (IKY).
          The second author's research was partially supported by
          NSF grants 0626912, 0729071, and 1117954.
}

\date{}








\maketitle



\begin{abstract}
We give an approximation algorithm for fractional
packing and covering linear programs (linear programs with non-negative coefficients).
Given a constraint matrix with  non-zeros,  rows, and  columns,
the algorithm (with high probability) computes feasible primal and dual solutions
whose costs are within a factor of  of   (the optimal cost)
in time .\footnote
{Accepted to Algorithmica, 2013.  The conference version of this paper was ``Beating Simplex for fractional packing and covering linear programs'' \cite{Koufogiannakis2007Beating}.
}
\end{abstract}

\section{Introduction}\label{sec:intro}

A {\em packing} problem is a linear program of the form , where the entries of the constraint matrix  are non-negative and  is a convex polytope admitting some form of optimization oracle.  
A {\em covering} problem is of the form .  

This paper focuses on {\em explicitly given} packing and covering problems, that is,  and , where the polytope  is just the positive orthant.  
Explicitly given packing and covering are important special cases of linear programming,
including, for example, fractional set cover, multicommodity flow problems with given paths, two-player zero-sum matrix games with non-negative payoffs, and variants of these problems.

The paper gives a -approximation algorithm ---
that is, an algorithm that returns feasible primal and dual solutions whose costs are within a given factor  of .
With high probability, it runs in time
, where  -- the input size -- is the number of non-zero entries in the constraint matrix and  is the number of rows plus columns (i.e., constraints plus variables).  

For dense instances,  can be as small as .
For moderately dense instances -- as long as  --
the  factor multiplies a sub-linear term.
Generally, the time is linear in the input size  as long as .

\subsection{Related work}
The algorithm is a Lagrangian-relaxation (a.k.a.~price-directed decomposition, multiplicative weights) algorithm.
Broadly, these algorithms work by replacing a set of hard constraints by a sum of smooth penalties, one per constraint, and then iteratively augmenting a solution while trading off the increase in the objective against the increase in the sum of penalties.
Here the penalties are exponential in the constraint violation,
and, in each iteration, only the first-order (linear) approximation 
is used to estimate the change in the sum of penalties.

Such algorithms, which can provide useful alternatives to interior-point and Simplex methods,  have a long history and a large literature.
Bienstock gives an implementation-oriented, operations-research perspective \cite{Bienstock00Potential}.  
Arora {\em et al.}\ discuss them from a computer-science perspective, highlighting connections to other fields such as learning theory \cite{arora2012multiplicative}.
An overview by Todd places them in the context of general linear programming \cite{todd2002mfl}.

The running times of algorithms of this type increase as the approximation parameter  gets small.   For algorithms that rely on linear approximation of the penalty changes in each iteration, the running times grow at least quadratically in  (times a polynomial in the other parameters).  For explicitly given packing and covering, the fastest previous such algorithm that we know of runs in time , where  is the maximum number of columns in which any variable appears \cite{Young01Sequential}.  
That algorithm applies to {\em mixed} packing and covering --- a more general problem.
Using some of the techniques in this paper, one can improve that algorithm to run in time  (an unpublished result),
which is slower than the algorithm here for dense problems.

Technically, the starting point for the work here is a remarkable algorithm by Grigoriadis and Khachiyan for the following special case of packing and covering \cite{Grigoriadis95Sublinear}.
The input is a two-player zero-sum matrix game with payoffs in .
The output is a pair of mixed strategies that guarantee an expected payoff within an {\em additive}  of optimal.   (Note that achieving additive error  is, however, easier than achieving multiplicative error .)
The algorithm computes the desired output 
in  time.
This is remarkable in that, for dense matrices, it is {\em sub-linear} in the input size .\footnote{
The problem studied here, packing and covering, can be reduced to Grigoriadis and Khachiyan's problem.  This reduction leads to an -time algorithm to find a -approximate packing/covering solution,
where .
A pre-processing step \cite[\S2.1]{Luby93Parallel} can bound ,
leading to a running time bound of .}
(For a machine-learning algorithm closely related to Grigoriadis and Khachiyan's result,
see \cite{clarkson2010sublinear,clarkson2012machine}.)

We also use the idea of {\em non-uniform increments}
from algorithms by Garg and K\"onemann
\cite{Garg98Faster,Konemann98Fast,garg2007faster}.

\paragraph{Dependence on .}
Building on work by Nesterov (e.g.,~\cite{nesterov2005smooth,nesterov2009unconstrained}),
recent algorithms for packing and covering problems
have reduced the dependence on  from quadratic to linear, 
at the expense of increased dependence on other parameters.
Roughly, these algorithms better approximate the change in the penalty function in each iteration, leading to fewer iterations but more time per iteration
(although not to the same extent as interior-point algorithms).
For example, Bienstock and Iyengar give an algorithm for concurrent multicommodity flow that solves  shortest-path problems, where  is the number of commodities and  is the number of vertices \cite{Bienstock04Solving}.  
Chudak and Eleuterio continue this direction --- for example, they give an algorithm for fractional set cover running in worst-case time  \cite{chudak2005ias}.

\paragraph{Comparison to Simplex and Interior-Point methods.}
Currently, the most commonly used algorithms for solving linear programs in practice are Simplex and interior-point methods.  Regarding Simplex algorithms, commercial implementations algorithms use many carefully tuned heuristics (e.g.~pre-solvers and heuristics for maintaining sparsity and numerical stability), enabling them to quickly solve many practical problems with millions of non-zeros to optimality.  But, as is well known, 
their worst-case running times are exponential.
Also, for both Simplex and interior-point methods,
running times can vary widely depending on the structure of the underlying problem.
(A detailed analysis of Simplex and interior-point running times is outside the scope of this paper.)
These issues make rigorous comparison between the various algorithms difficult.

Still, here is a meta-argument that may allow some meaningful comparison. Focus on ``square'' constraint matrices, where .   Note that at a minimum, any Simplex implementation must identify a non-trivial basic feasible solution.  Likewise, interior-point algorithms require (in each iteration) a Cholesky decomposition or other matrix factorization.  Thus, essentially, both methods require implicitly (at least) solving an  system of linear equations.  Solving such a system is a relatively well-understood problem, both in theory and in practice, and (barring special structure) takes  time, or  time using Strassen's algorithm.  Thus, on ``square'' instances, Simplex and interior-point algorithms should have running times growing at least with  (and probably more).
This reasoning applies even if Simplex or interior-point methods are terminated early so as to find {\em approximately} optimal solutions.  

In comparison, on ``square'' matrices, the algorithm in this paper takes time  where  or less.  
If the meta-argument holds, 
then, for applications where -approximate solutions suffice
for some fixed and moderate  (say, ),
for very large instances (say, ),
the algorithm here should be orders of magnitude faster than Simplex
or interior-point algorithms.

This conclusion is consistent with experiments reported here, in which the running times of Simplex and interior-point algorithms on large random instances exceed .  Concretely, with , the algorithm here is faster when  is on the order of , with a super-linear (in ) speed-up for larger .

\subsection{Technical roadmap}
Broadly, the running times of iterative optimization algorithms are determined by (1) the number of iterations and (2) the time per iteration.  Various algorithms trade off these two factors in different ways.  The technical approach taken here is to accept a high number of iterations --- , a typical bound for an algorithm of this class
(see e.g.~\cite{Klein99Number} for further discussion) ---
and to focus on implementing each iteration as quickly as possible
(ideally in constant amortized time).

\paragraph{Coupling.}
Grigoriadis and Khachiyan's sub-linear time algorithm
uses an unusual technique of {\em coupling} primal and dual algorithms
that is critical to the algorithm here.
As a starting point, to explain coupling, consider the following ``slow'' coupling algorithm.
(Throughout, assume without loss of generality by scaling that  for all .)
The algorithm starts with all-zero primal and dual solutions,  and , respectively.
In each iteration, it increases one coordinate  of the primal solution  by 1, 
and increases one coordinate  of the dual solution  by 1.
The index  of the primal variable to increment is chosen randomly from a distribution  that depends on the current {\em dual} solution.
Likewise, the index  of the dual variable to increment is chosen randomly from a distribution  that depends on the current {\em primal} solution.
The distribution  is concentrated on the indices of dual constraints  that are ``most violated'' by .
Likewise, the distribution  is concentrated on the indices of primal constraints  that are ``most violated'' by .
Specifically,  is proportional to ,
while  is proportional to .\footnote{
The algorithm can be interpreted as a form of fictitious play of a two-player zero-sum game, where in each round each player plays from a distribution concentrated around the best response to the aggregate of the opponent's historical plays.  In contrast, in many other fictitious-play algorithms, one or both of the player plays a {\em deterministic} pure best-response to the opponent's historical average.}

Lemma~\ref{lemma:slowalg} in the next section proves that this algorithm
achieves the desired approximation guarantee.
Here, broadly, is why coupling helps reduce the time per iteration
in comparison to the standard approach.
The standard approach is to increment the primal variable corresponding 
to a dual constraint that is ``most violated'' by  ---
that is, to increment  where  (approximately) minimizes 
(for  defined as above).
This requires at a minimum {\em maintaining} the vector .
Recall that  is a function of .
Thus, a change in one primal variable  
changes many entries in the vector ,
but {\em even more} entries in .
(In the  bipartite graph  where ,
the neighbors of  change in ,
while all {\em neighbors of those neighbors} change in .)
Thus, maintaining  is costly.
In comparison, to implement coupling, it is enough to maintain the vectors  and .
The further product  is not needed (nor is ).
This is the basic reason why coupling helps reduce the time per iteration.

\paragraph{Non-uniform increments.}
The next main technique, used to make more progress per iteration,
is Garg and K\"onemann's {\em non-uniform increments} \cite{Garg98Faster,Konemann98Fast,garg2007faster}.
Instead of incrementing the primal and dual variables by a uniform amount each time (as described above), the algorithm increments the chosen primal and dual variables  and  by an amount  chosen small enough so that the left-hand side (LHS) of 
each constraint (each  or ) increases by at most 1 (so that the analysis still holds), but {\em large enough} so that the LHS of {\em at least one} such constraint increases by at least .
This is small enough to allow the same correctness proof to go through, 
but is large enough to guarantee a small number of iterations.
The number of iterations is bounded by (roughly) the following argument:
each iteration increases the LHS of some constraint by ,
but, during the course of the algorithm, no LHS ever exceeds .
(The particular  is chosen with foresight so that the relative error works out to .)
Thus, the number of iterations is .

\paragraph{Using slowly changing estimates of  and .}
In fact, we will achieve this bound not just for the number of iterations, 
but also for the total work done (outside of pre- and post-processing).
The key to this is the third main technique.
Most of the work done by the algorithm as described so far
would be in maintaining the vectors  and 
and the distributions  and  (which are functions of  and ).
This would require lots of time in the worst case, because, even with non-uniform increments, there can still be many {\em small} changes in elements of  and .
To work around this, instead of maintaining  and  exactly,
the algorithm maintains more slowly changing {\em estimates} for them (vectors  and , respectively), using random sampling.
The algorithm maintains  as follows.
When the algorithm increases a primal variable  during an iteration,
this increases some elements in the vector 
(specifically, the elements  where ).
For each such element , if the element increases by, say, ,
then the algorithm increases the corresponding  not by , but by 1, but {\em only with probability }.
This maintains not only , but also, with high probability, .
Further, the algorithm only does work for a  (e.g.~updating )
when  increases (by 1).
The algorithm maintains the estimate vector  similarly,
and defines the sampling distributions  and  as functions 
of  and  instead of  and .
In this way {\em each unit of work}  done by the algorithm
can be charged to an increase in  
(or more precisely, an increase in , which never exceeds ).
(Throughout the paper,  denotes the 1-norm of any vector .)

Section~\ref{sec:slow_alg} gives the formal intuition underlying coupling by describing and formally analyzing the first (simpler, slower) coupling algorithm described above.
Section~\ref{sec:correctness} describes the full (main) algorithm and its correctness proof.
Section~\ref{sec:time} gives remaining implementation details and bounds the run time.  
Section~\ref{sec:experiments} presents basic experimental results, including a comparison with the GLPK Simplex algorithm.

\subsection{Preliminaries}
For the rest of the paper, 
assume the primal and dual problems are of the following restricted forms, respectively:
~,
~.
That is, assume  for each .
This is without loss of generality by the transformation .
Recall that  denotes the 1-norm of any vector .

\section{Slow algorithm (coupling)}\label{sec:slow_alg}\label{sec:alg}
To illustrate the coupling technique,
in this section we analyze the first (simpler but slower) algorithm described in the roadmap
in the introduction,
a variant of Grigoriadis and Khachiyan's algorithm \cite{Grigoriadis95Sublinear}.
We show that it returns a -approximate primal-dual pair with high probability.

We do not analyze the running time, which can be large.
In the following section, we describe how to modify this algorithm
(using non-uniform increments
and the random sampling trick described in the previous roadmap)
to obtain the full algorithm with a good time bound.

For just this section, assume that each .
(Assume as always that  for all ;
recall that  denotes the 1-norm of .)
Here is the algorithm:

\noindent
\begin{alg}
\Ahead{\normalsize{\bf slow-alg} }

\A Vectors ;
 scalar .

\A {Repeat} until :

\A\tab Let  (for all )
and  (for all ).

\A \tab Choose random indices  and  respectively
\\\tab~~ from probability distributions  and .

\A \tab Increase  and  each by 1.

\A Let .  \label{slow:scaling}

\A Return .
\end{alg}

\smallskip
The scaling of  and  in line~\ref{slow:scaling} ensures feasibility of the final primal solution  and the final dual solution .
(Recall the assumption that  for all .)
The final primal solution cost and final dual solution costs are, respectively  and .
Since the algorithm keeps the 1-norms  and  of the intermediate primal and dual solutions equal, the final primal and dual costs will be within a factor of  of each other as long as .
If this event happens, then by weak duality implies that each solution is a -approximation of its respective optimum.

To prove that the event

happens with high probability,
we show that   (the product of the 1-norms of  and ,
as defined in the algorithm) is a Lyapunov function --- that is, the product  is non-increasing in expectation with each iteration.
Thus, its expected final value is at most its initial value ,
and with high probability, its final value is at most, say, .
If that happens, then by careful inspection of  and ,
it must be that ,
which (with the termination condition ) implies the desired event.\footnote
{It may be instructive to compare this algorithm to the more standard algorithm.
In fact there are two standard algorithms related to this one: a primal algorithm and a dual algorithm.
In each iteration, the primal algorithm would choose  to minimize 
and increments .
Separately and simultaneously, the dual algorithm would choose  to maximize ,
then increments .
(Note that the primal algorithm and the dual algorithm are independent,
and in fact either can be run without the other.)
To prove the approximation ratio for the primal algorithm, one would bound the increase in  relative to the increase in the primal objective .
To prove the approximation ratio for the dual algorithm, one would bound the decrease in  relative to the increase in the dual objective .
In this view, the coupled algorithm can be obtained by taking these two independent primal and dual algorithms
and randomly coupling their choices of  and .
The analysis of the coupled algorithm uses as a penalty function ,
the product of the respective penalty functions  of the two underlying algorithms.
}

\begin{lemma}\label{lemma:slowalg}
The slow algorithm returns a -approximate primal-dual pair (feasible primal and dual solutions  and  such that ) with probability at least .
\end{lemma}
\begin{Proof}
In a given iteration, let  and  denote the vectors at the start of the iteration.  
Let  and  denote the vectors at the end of the iteration.  
Let  denote the vector whose th entry is the increase in  during the iteration
(or if  is a scalar,  denotes the increase in ).  
Then, using that each ,

Likewise, for the dual,
.

\smallskip
Multiplying these bounds on  and 
and using that  for  gives


The inequality above is what motivates the ``coupling'' of primal and dual increments.
The algorithm chooses the random increments to  and 
precisely so that 
and .
Taking expectations of both sides of the inequality above,
and plugging these equations into the two terms on the right-hand side,
the two terms exactly cancel,
giving .
Thus, the particular random choice of increments to  and 
makes the quantity  non-increasing in expectation
with each iteration.

This and Wald's equation (Lemma~\ref{lemma:walds}, or equivalently a standard optional stopping theorem for supermartingales) imply that the expectation of  at termination is at most its initial value .
So, by the Markov bound, the probability 
that  is at most .
Thus, with probability at least ,
at termination .

Assume this happens.
Note that  ,
so  implies

Taking logs, and using the inequalities 
and , gives


By the termination condition , so the above inequality implies


This and   (and weak duality) imply the approximation guarantee
for the primal-dual pair  returned by the algorithm.
\end{Proof}


\section{Full algorithm}
This section describes the full algorithm 
and gives a proof of its approximation guarantee.
In addition to the coupling idea explained in the previous section,
for speed
the full algorithm uses non-uniform increments
and estimates of  and  as described in the introduction.
Next we describe some more details of those techniques.
After that we give the algorithm in detail
(although some implementation details that are not crucial to the approximation guarantee
are delayed to the next section).

Recall that WLOG we are assuming  for all .
The only assumption on  is .

\paragraph{Non-uniform increments.}
In each iteration, instead of increasing the randomly chosen  and  by 1, the algorithm increases them both by an increment , chosen just so that the maximum resulting increase in any left-hand side (LHS) of any constraint  (i.e.\  or ) is in .  
The algorithm also deletes covering constraints once they become satisfied
(the set  contains indices of not-yet-satisfied covering constraints,
that is  such that ).

We want the analysis of the approximation ratio to continue to hold
(the analogue of Lemma~\ref{lemma:slowalg} for the slow algorithm),
even with the increments adjusted as above.
That analysis requires that the expected change in each  and each  should be proportional to  and , respectively.
Thus, we adjust the sampling distribution for the random pair  so that,
when we choose  and  from the distribution
and increment  and  by  as defined above,
it is the case that, for any  and ,
 and  for an .
This is done by scaling the probability of choosing each given  pair by a factor proportional to .

To implement the above non-uniform increments 
and the adjusted sampling distribution,
the algorithm maintains the following data structures
as a function of the current primal and dual solutions  and :
a set  of indices of still-active (not yet met) covering constraints (columns);
for each column  its maximum entry ;
and for each row  a close upper bound  on its maximum active entry 
(specifically, the algorithm maintains ).

Then, the algorithm takes the increment  to be .
This seemingly odd choice has two key properties:
(1) It satisfies ,
which ensures that when  and  are increased by ,
the maximum increase in any LHS (any , or  with )
is .
(2) It allows the algorithm to select the random pair  in constant time using the following subroutine, called {\algfont random-pair} (the notation  denotes the vector with th entry ):



\begin{alg}
\Ahead{{\algfont\bf random-pair}}

\A With probability

\\ ~~~ choose random  from distribution  , 
\\ ~~~ and independently choose  from ,

\A or, otherwise,
\\ ~~~ choose random  from distribution , 
\\ ~~~ and independently choose  from .

\A Return .
\end{alg}
\smallskip

The key property of {\bf random-pair} is that it makes
the expected changes in  and  correct:
any given pair  is chosen with probability proportional to
,
which makes the expected change in any  and ,
respectively, is proportional to  and .
(See Lemma~\ref{lemma:facts} below.)

\paragraph{Maintaining estimates ( and ) of  and .}
Instead of maintaining the vectors  and  as direct functions
of the vectors  and , 
to save work,
the algorithm maintains more slowly changing {\em estimates} ( and )
of the vectors  and ,
and maintains  and  as functions of the estimates,
rather than as functions of  and .

Specifically, the algorithm maintains  and  as follows.
When any  increases by some  in an iteration,
the algorithm increases the corresponding estimate  
by 1 with probability .
Likewise, when any  increases by some  in an iteration,
the algorithm increases the corresponding estimate  
by 1 with probability .
Then, each  is maintained as  instead of ,
and each  is maintained as  instead of .
This reduces the frequency of updates to  and  (and so reduces the total work),
yet maintains  and  
with high probability,
which is enough to still allow a (suitably modified) coupling argument to go through.

Each change to a  or a  increases the changed element by 1.
Also, no element of  or  gets larger than  before the algorithm stops 
(or the corresponding covering constraint is deleted).
Thus, in total the elements
of  and  are changed at most  times.
We implement the algorithm to do only {\em constant work} maintaining the remaining
vectors for each such change.
This allows us to bound the total time 
by  (plus  pre- and post-processing time).

As a step towards this goal, 
in each iteration, in order to {\em determine} the elements in  and  that
change, using just  work per changed element, the algorithm uses the following trick.
It chooses a random .
It then increments  by 1 for those  
such that the increase  in  is at least .
Likewise, it increments  by 1 for  such that the increase 
 in  is at least .
To do this efficiently, the algorithm preprocesses ,
so that within each row  or column  of ,
the elements can be accessed in (approximately) decreasing order
in constant time per element accessed.
(This preprocessing is described in Section~\ref{sec:implementation}.)
This method of incrementing the elements of  and  uses constant work per changed element and increments each element with the correct probability.
(The random increments of different elements are not independent,
but this is okay because, in the end, each estimate  and 
will be shown seperately to be correct with high probability.)

\smallskip
The detailed algorithm is shown in Fig.~\ref{fig:alg}, except for
the subroutine {\algfont random-pair} (above)
and some implementation details that are left until Section~\ref{sec:time}.  
\begin{figure*}[t]
\begin{alg}
\Ahead{{\bf solve} 
--- {\em return a -approximate primal-dual pair w/ high prob.}
}

\A {Initialize} vectors , 
and scalar .

\A Precompute  for .
(The max.~entry in column .)

As  and  are incremented,
the alg.~maintains  and  
so , .

It maintains vectors  defined by
 
and, as a function of :

It maintains vectors  and ,
where  is a vector whose th entry is .

\A {Repeat} until  or :

\algbeg
\A Let .
\label{line:sample}

\A {Increase}  and  each 
by the same amount .
\label{line:incx}

\A Update , , and the other vectors as follows:  

\A \tab Choose random  uniformly, and 

\A \tab~ for each  with , 
{increase}  by 
\label{line:incyp}
\A \tab~ \hfill (and multiply  and  by );

\A \tab~ for each\, \, with ,
{increase}  by 
\label{line:incyd}
\A \tab~ \hfill (and multiply  and  by ).

\A For each  leaving , update , , and .

\algend

\A Let .
{Return} .
\end{alg}

\caption{The full algorithm.
 denotes .
Implementation details are in Section~\ref{sec:implementation}.
}
\label{fig:alg}
\end{figure*}


\paragraph{Approximation guarantee.} \label{sec:correctness}
Next we state and prove the approximation guarantee for the full algorithm
in Fig.~\ref{fig:alg}.   We first prove three utility lemmas.
The first utility lemma establishes that (in expectation)
, , , and  change 
as desired in each iteration.
\newpage

\begin{lemma}\label{lemma:facts}
In each iteration, 
\begin{enumerate}
\item 
The largest change in any relevant LHS is at least 1/4:


\item Let .
The expected changes in each , , ,  satisfy

6pt]
\E[\Delta \xd_i] &=& \alpha \pp_i/|\pp|,
& \E[\Delta \yd_j] &=& \E[\Delta \MM_j\tran \xd] &=& \alpha \MM\tran \pp_j/|\pp|.
\end{array}

\delta_{i'j'}\max\big(
  \max_{i} \MM_{ij'}, 
  \max_{j\in \JJ} \MM_{i'j}\big)
&\in& [1/2,1]\,\dd_{i'j'} \max(\wwd_{i'}, \wwp_{j'})\\
&\subseteq& [1/4,1]\, \dd_{i'j'} (\wwd_{i'}+ \wwp_{j'})\\
&=& [1/4,1].

|\pp\times\wwd|\,|\pd| 
\frac{\pp_{i}\wwd_{i}}{|\pp\times\wwd|}
\frac{\pd_{j}}{|\pd|}
+
|\pp|\,|\pd\times\wwp| 
\frac{\pp_{i}}{\pp}
\frac{\pd_{j}\wwp_{j}}{|\pd\times\wwp|}

|\pp'| \,=\,
\sum_{i} \pp_i(1+\eps \Delta\yp_i)
\,=\,
|\pp| + \eps \pp\tran\Delta\yp

|\pd'| \,=\,|\pd| - \eps \pd\tran\Delta\yd

|\pp'|\,|\pd'| ~\le~
|\pp|\,|\pd|
 + \eps |\pd|\pp\tran\Delta\yp
 - \eps |\pp|\pd\tran\Delta\yd.
9pt]
\lefteqn{\parbox{0.95\textwidth}{~~By algebra, using  and , it follows for all  and  that}}
\\sum_t |\Ip_t|+|\Jd_t| \le (\rows+\columns) N 
~=~
O((\rows+\columns)\log(\inputsize)/\eps^2).\Pr\Big[~\textstyle (1-\delta)\sum_{t=1}^T E_t ~\ge~ 3 \sum_{t=1}^T (1-E_t) ~+~ A~\Big]
~\le~\exp(-\delta A).
\sum_t (1+ |\Ip'_t| + |\Jd'_t|)~=~O((r+c)N).
\Pr[ i \in \Ip'_t ] 
~\le~
\Pr[ \beta/2 \le \MM_{ij'}\dd_{i'j'} ]
&~\le~&
2\MM_{ij'}\dd_{i'j'}
\\&=&
2\Pr[ \beta \le \MM_{ij'}\dd_{i'j'} ]
~=~
 2\Pr[i\in\Ip_t].
\textstyle
\Pr\Big[~ (1-\delta) \sum_t [i\in\Ip'_t] ~\ge~ 2\sum_t [i\in\Ip_t] ~+~ A ~\Big]
~\le~\exp(-\delta A).
\textstyle
\sum_t [i\in\Ip'_t]~\le~ 4\sum_t [i\in\Ip_t] \,+\, 8\ln(rc).\label{eqn:predtime}
[12(r+c) ~+~ 480 d^{-1}]\frac{\ln(rc)}{\eps^2}
\label{eqn:predspeedup}
\frac{\textstyle
5\min(r,c)rc
}{\textstyle
[12(r+c) ~+~ 480 d^{-1}]\,\ln(rc)/\eps^2.
}
\textstyle
 \E\big[\,x_{t} - y_{t} \,\giv\, \sum_{s< t} x_s, \sum_{s< t} y_s\,\big]
~\le~
 0.\Pr\big[\,(1-\eps) X \,\ge\, Y + A\, \big] ~\le~ \exp({-\epsoverdelta}A).
\pi_t
~\doteq~
\prod_{s\le t} (1+\eps)^{x_s}(1-\eps)^{y_s}
 ~=~
\pi_{t-1}(1+\eps)^{x_t}(1-\eps)^{y_t}
 ~\le~
\pi_{t-1}(1+\eps x_t -\eps y_t)

\Pr[ \pi_T \ge \exp(\eps A)]
~\le~
\exp({-\eps} A).

X\ln(1+\eps) -Y\ln(1/(1-\eps)) 
 ~=~ 
\ln \pi_T
~<~ 
\eps A.
\Pr[(1-\eps)X\ge Y+A]
~\le~
\Pr[T\ge \lambda \E[T]]
+
\Pr[\pi_T \ge \exp(\eps A)]
~\le~
1/\lambda + \exp(-\eps A).

Since  can be arbitrarily large, the lemma follows.
\end{Proof}



\end{document}
