\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{color} 


\begin{document}

\title{No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models}

\author{Zicheng Zhang, Wei Sun, Xiongkuo Min, \emph{Member, IEEE,} Tao Wang, \\ Wei Lu, and Guangtao Zhai, \emph{Senior Member, IEEE} \IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Zicheng Zhang, Wei Sun, Xiongkuo Min, Tao Wang, Wei Lu, and Guangtao Zhai are with the Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, 200240 Shanghai, China. E-mail:\{zzc1998,sunguwei,minxiongkuo,f1603011.wangtao,SJTU-Luwei,zhaiguangtao\}@sjtu.edu.cn.\protect}
\thanks{
Part of the work was presented at \cite{zhang2021}. 

\textsuperscript{1}The code is available at https://github.com/zzc-1998/NR-3DQA.}\\}





\maketitle
\thispagestyle{plain}
\pagestyle{plain}
\begin{abstract}
To improve the viewer's Quality of Experience (QoE) and optimize computer graphics applications, 3D model quality assessment (3D-QA) has become an important task in the multimedia area. Point cloud and mesh are the two most widely used digital representation formats of 3D models, the visual quality of which is quite sensitive to lossy operations like simplification and compression. Therefore, many related studies such as point cloud quality assessment (PCQA) and mesh quality assessment (MQA) have been carried out to measure the visual quality of distorted 3D models. However, most previous studies utilize full-reference (FR) metrics, which indicates they can not predict the quality level in the absence of the reference 3D model. Furthermore, few 3D-QA metrics consider color information, which significantly restricts their effectiveness and scope of application. In this paper, we propose a no-reference (NR) quality assessment metric for colored 3D models represented by both point cloud and mesh. First, we project the 3D models from 3D space into quality-related geometry and color feature domains. Then, the 3D natural scene statistics (3D-NSS) and entropy are utilized to extract quality-aware features. Finally, a support vector regression (SVR) model is employed to regress the quality-aware features into visual quality scores. Our method is validated on the colored point cloud quality assessment database (SJTU-PCQA), the Waterloo point cloud assessment database (WPC), and the colored mesh quality assessment database (CMDM). The experimental results show that the proposed method outperforms most compared NR 3D-QA metrics with competitive computational resources and greatly reduces the performance gap with the state-of-the-art FR 3D-QA metrics. The code of the proposed model is publicly available now to facilitate further research\textsuperscript{1}.  
\end{abstract}

\begin{IEEEkeywords}
3D model quality assessment, colored point cloud, colored mesh, no-reference quality assessment, natural scene statistics 
\end{IEEEkeywords}







\section{Introduction}\label{sec:introduction}
\IEEEPARstart{N}OWADAYS, with the rapid development of computer graphics, the digital representation of 3D models has been widely studied and used in a wide range of application scenarios such as virtual reality (VR), medical 3D reconstruction, and video post-production \cite{application}, etc. Among the digital representation forms of 3D models, point cloud and mesh are the most widely used formats in practical. A point cloud based object is a set of points in space, in which each point is described with geometry coordinates and sometimes with other attributes such as color and surface normals. Mesh is more complicated because it is a collection of vertices, edges, and faces which together define the shape of a 3D model. Except for geometry information, the 3D mesh may also contain other appearance attributes, such as color and material. Both point cloud and mesh are able to vividly display exquisite models and complex scenes. However, since point cloud and mesh record the omnidirectional details of objects and scenes, lossless 3D models usually need large storage space and very high transmission bandwidth in practical applications. 
Hence, a variety of 3D processing algorithms such as simplification and compression, etc. have been proposed to satisfy the specific needs, which inevitably cause damage to the visual quality of 3D models \cite{li2020occupancy,mekuria2016design}. Additionally, in some 3D scanning model APIs like Apple 3D object capture \cite{apple} and Intel Lidar Camera Realsense \cite{intel}, some slight disturbance such as blur, noise, etc. may be introduced to the constructed 3D models. 


Therefore, to improve users' Quality of Experience (QoE) in 3D fields and optimize 3D compression and reconstruction systems, it is of great significance to develop quality metrics for point cloud quality assessment (PCQA) and mesh quality assessment (MQA). However, different from the format of 2D media like images and videos where pixels are distributed in the fixed grid, the points in 3D models are distributed irregularly in the space, which brings a huge challenge for 3D-QA tasks. For example, the neighborhood of pixels in 2D media can be easily obtained, thus the local features are available by analyzing the relationship between the pixel and its neighborhood. However, the neighborhood for points in 3D models is very ambiguous, which makes it difficult to conduct local feature analysis. Furthermore, the geometrical shape of 2D media is fixed, which enables us to easily scale or crop the 2D media for feature extraction while similar operations are very hard to define for 3D models. Although some 3D-QA databases \cite{sjtu-pcqa} \cite{database} \cite{pcqa_database1} \cite{pcqa_database2} \cite{su2021perceptual} have been proposed to push forward the development of 3D-QA algorithms, the difficulty of collecting 3D models and conducting subjective experiments greatly limit the size of the 3D-QA databases, which may also restrict relevant research, especially for NR 3D-QA metrics. 

\subsection{Previous 3D-QA Works}
Quality assessment can be divided into subjective quality assessment and objective quality assessment according to whether human observers are involved. It is known that, in subjective quality assessment, large numbers of people are required to assess the visual quality of 3D models and give their subjective scores. Although the mean opinion scores (MOSs) collected from human observers are straightforward and precise, they cannot be used in practical applications due to the huge time consumption and high expense  \cite{sun2021blind}. Therefore, objective quality metrics are urgently needed to predict the visual quality of 3D models automatically. {The 3D objective quality assessment can then be divided into full-reference (FR), reduced-reference (RR), and no-reference (NR) 3D-QA methods. FR 3D-QA methods work by comparing the difference between the reference 3D models and distorted 3D models, RR 3D-QA methods employ part of the reference models' information for comparison, and NR 3D-QA methods only analyze the distorted 3D models to give the perceptual quality scores.} Considering the complexity of 3D models, in the literature, a large part of 3D-QA metrics are full-reference and they only take geometry features into consideration \cite{p2point, p2plane, m1,ff2_roughness, p2mesh,angular,pcqa2,dame}. When it comes to 3D models with color information, limited works have been proposed \cite{pcqa3,tian-color,guo-color,pcqm,liu2021reduced}. Clearly, the NR 3D-QA for colored models has fallen behind. In this section, we briefly review the development of 3D-QA and introduce the mainstream methods designed for 3D-QA tasks. 

\subsubsection{The Development of PCQA}
The earliest FR-PCQA metrics usually focus on the geometry aspect at the point level, such as p2point \cite{p2point}, p2plane \cite{p2plane}, and p2mesh \cite{p2mesh}. The p2point estimates the levels of distortion by computing the distance vector between the corresponding points. The p2plane further projects the distance vector on the normal orientation for evaluating the quality loss. The p2mesh first reconstructs the point cloud to mesh and then measures the distance from points to the reconstructed surface to predict the quality level, however, it greatly depends on the reconstruction algorithms and lacks stability. Since the point-level difference is difficult to reflect the complex structural distortions, some studies further consider other structural characteristics for PCQA. For example, Alexiou   \cite{angular} adopt the angular difference of point normals to estimate the degradations. Javaheri   \cite{pcqa2} utilize the generalized Hausdorff distance to reflect the distortions caused by compression operations.

In some situations, color information can not be ignored, which challenges the PCQA methods considering only geometry information. To incorporate the color information into PCQA models, Meynet   \cite{pcqm} propose a metric to use the weighted linear combination of curvature and color inf ormation to evaluate the visual quality of distorted point clouds. Inspired by SSIM \cite{ssim}, Alexiou   \cite{pcqa3} compute the similarity of four types of features, including geometry, normal vectors, curvature, and color information. What's more, some studies  \cite{sjtu-pcqa} \cite{pcqa_database2} try to predict the quality level by evaluating the projected 2D images from 3D models. The advantage is that image quality assessment (IQA) metrics have been well developed while the disadvantage is that there is inevitable information loss during the projection and the projected images are easily affected by the projected angles and viewpoints.

Few NR-PCQA metrics have been developed so far, which include a learning-based approach \cite{pcqa-large-scale} using two modified PointNet \cite{Qi_2017_CVPR} as feature extraction backbone and also a learning-based method \cite{liu2021pqa} using multi-view projection. 

\subsubsection{The Development of MQA}
The MQA metrics can be categorized into two types: model-based metrics \cite{m1} \cite{ff2_roughness} \cite{dame} \cite{nr-svr} \cite{nr-cnn} which operate directly on the 3D models, and IQA-based metrics \cite{tian-color} \cite{guo-color} \cite{nr-cnncmp} \cite{2d1} \cite{2d2} which operate on the rendering snapshots of 3D models.
Supported by the vast amount of previous research on FR-IQA methods \cite{ms-ssim,fsim,vif}, many FR-MQA models have been proposed using similar manners, which usually compute local features at the vertex level and then pool the features into a quality value. For example, MSDM2 \cite{m1} uses the differences of structure (captured via curvature statistics) computed on local neighborhoods to predict the quality level. DAME \cite{dame} measures the differences in dihedral angles between the reference and the distorted meshes to evaluate the quality loss. FMPD \cite{ff2_roughness} estimates the local roughness difference derived from Gaussian curvature to assess the quality of the distorted mesh. However, these metrics only take geometry information into consideration.

Furthermore, to analyze the influence of color information, some color-involved FR-MQA metrics have been carried out. Tian   \cite{tian-color} introduce a global distance over texture image using Mean Squared Error (MSE) to quantify the effect of color information. Guo   \cite{guo-color} exploit SSIM to calculate the texture image distance as the color information features. Nehmé   \cite{database} introduce a metric to incorporate perceptually relevant curvature-based and color-based features to evaluate the visual quality of colored meshes.

Recently, thanks to the effectiveness of machine learning technologies, some learning-based NR-MQA metrics have been proposed. Abouelaziz   \cite{nr-svr} extract features using dihedral angle models and train a support vector machine for feature regression. Later, Abouelaziz   \cite{nr-cnn} scale the curvature and dihedral angle into 2D patches and utilize the convolution neural network (CNN) for training. They further introduce a CNN framework with saliency views rendered from 3D meshes \cite{nr-cnncmp}. However, the NR methods mentioned above only extract geometry features and may fail to accurately predict the scores of colored meshes.



\subsection{Our Approach}
Generally speaking, a large part of the metrics mentioned above are full-reference metrics, which can make full use of the relationship between the reference and distorted 3D models and give relatively accurate results. However, the disadvantage of FR 3D-QA metrics is also obvious. They are not able to work in the absence of reference 3D models. Unfortunately, in many application scenarios like 3D reconstruction, pristine reference is not always available.  {Therefore, inspired by the huge success of natural scene statistics (NSS) in IQA tasks \cite{brisque,nss1,nss2}, some researchers \cite{lin2019blind,abouelaziz2018blind,nr-svr} use similar ideas of extracting handcrafted features and estimating the statistical parameters with certain NSS distributions as quality-aware information. In this paper, we further push forward the application of NSS for both colored point cloud and mesh models, and we summarize such a concept as 3D-NSS.} Specifically, NSS is a discipline within the field of perception, which is dependent on the premise that the perceptual system is designed to interpret natural scenes \cite{nss2}. Through observations of feature distributions, we find that the reference features obey certain NSS distributions and different types of distortions change the appearance of such feature distributions. Therefore, we believe that 3D-NSS is effective to quantify the visual quality of 3D models in the presence of distortion. More details of 3D-NSS are discussed in Section \ref{sec:estimate nss parameters}.  


Specifically, we first project the 3D models into quality-related geometry and color feature domains. Then, 3D-NSS and entropy are utilized to extract these color and geometry characteristics. Finally, the obtained features are integrated into a quality value through the support vector regression (SVR) model. In order to test the effectiveness of different types of features and different kinds of distribution models, we test the performance of various combinations of the statistical parameters to find the optimal combination. Further, we conduct the data-sensitivity experiment, the ablation study, and the computational efficiency experiment to demonstrate the effectiveness of our method. In-depth discussions are given as well.   

\begin{figure*}[t]
    \centering
    \includegraphics[width = 14cm]{framework.pdf}
    \caption{The framework of the proposed method. Geometry-based and color-based features are first extracted from the distorted 3D models. Then various statistical parameters are estimated from the extracted features to form the feature vector. Finally, the quality scores are given through the SVR regression module.}
    \label{fig:framework}
    \vspace{-0.5cm}
\end{figure*}




\subsection{Contributions of This Paper}
We summarize our contributions as follows:
\begin{itemize}
    \item {We push forward the development of NSS in the 3D-QA fields based on previous research \cite{lin2019blind,abouelaziz2018blind,nr-svr} and we systematically summarize the concept as 3D-NSS.} 
    \item We propose a no-reference quality assessment metric for both colored point cloud and mesh. We extract features not only from the geometry aspect, but also from the color aspect. Furthermore, it is the first method that can deal with both NR-PCQA and NR-MQA with color information. The proposed NR 3D-QA model follows a common NR framework, which means it is easy to modify our metric for performance improvement or meeting other specific needs.
    \item We deeply investigate the effectiveness of different types of features and different kinds of NSS models, which can provide useful guidelines for future research.
    \item Compared with the state-of-the-art methods, our method is more efficient in the computation process, which means the proposed method is potentially more capable of handling practical situations. The code of the proposed model is also released for promoting the development of NR 3D-QA.
\end{itemize}

The paper is organized as follows. Section \ref{sec:feature projection} describes the feature projection processes. Section \ref{sec:estimate nss parameters} describes the process of quantifying the distortions to statistical parameters. Section \ref{sec:experiment} presents the experiment setup and the experimental results. Section \ref{sec:conclusion} summarizes of this paper.






\section{Feature Projection}
\label{sec:feature projection}
The framework of the proposed method is illustrated in Fig. \ref{fig:framework}, which includes a feature extraction module, a parameters estimation module, and a regression module. Point cloud and mesh have similar quality-aware geometric properties as well as the attached color attributes. Therefore, we uniformly design the point cloud and mesh processing framework, and determine the specific projection domains based on their characteristics.
Before introducing the proposed model, we first define a distorted 3D object O as:

where  and  mean that the 3D object is represented by point cloud and mesh respectively, and the color information is attached to  in point cloud and  in mesh respectively.


\subsection{Geometry Feature Projection}
Geometry features usually have a strong correlation with human perception, which has been firmly proved in 3D-QA studies \cite{p2point, p2plane, m1,ff2_roughness, p2mesh,angular,pcqa2,dame}. Although the geometry features are computed in different ways for point cloud and mesh, they share similar characteristics for the visual quality of 3D models. In this section, we project the given 3D model into several quality-aware geometry feature domains:

where  indicates the set of geometry feature domains of the 3D model,  denotes the geometry projection function.




\subsubsection{  Point Cloud Geometry Feature Domains}
Considering that the point cloud lacks surfaces, we first need to get the neighborhood set for each point so we can further extract the geometry features. Given the point cloud  , the neighborhood  of each vertex  can be obtained utilizing the k-nearest neighbors (k-NN) algorithm:
  
where  indicates the number of points in the point cloud,  is the set of all neighborhoods,  denotes the k-NN algorithm function, the  level is set as 10, and Euclidean distance  is exploited as the distance function. With the computed neighborhood , the corresponding covariance matrix  for each point  (represented by 3D geometry coordinates) can be denoted as:

{where  and  are vectors of dimension 31,  is a matrix of dimension 33,  stands for the size of neighborhood , and  represents the centroid of neighborhood .} Therefore, the eigenvector problem for the covariance matrix  can be described as:

where (, , ) indicate the eigenvalues, (, ,   represent the corresponding eigenvectors, and we assume that ). Thus, a total of three eigenvalues are obtained for each point  in the point cloud .

In previous studies \cite{pcqa-curvature2} \cite{pc-eigenvalue1}, the eigenvalues mentioned above are used to calculate various geometry features for tasks like classification, simplification, and segmentation. These features have shown great ability to describe the complicated geometry structure information and achieved outstanding performance. Hence, we selected several geometry features that may be helpful for point cloud quality assessment: 
\begin{itemize}
    \item Curvature: Curvature is the amount by which a curve deviates from being a straight line, which is frequently used to describe roughness or smoothness.
    \item Anisotropy: Anisotropy is used to exhibit variations in geometrical properties for different directions.
    \item Linearity: Linearity is the property for estimating the similarity to a straight line.
    \item Planarity: Planarity is utilized to measure the similarity to a planar surface.
    \item Sphericity: Sphericity is the measure of how closely the shape of an object resembles that of a perfect sphere. 
\end{itemize}


Moreover, all the geometry features described above represent the local distribution patterns and are calculated at the point level, which indicates that each  has its own curvature, anisotropy, linearity, planarity, and sphericity.
The formulations \cite{pc-eigenvalue2} for geometry features are denoted as:

where , , and  refer to the corresponding eigenvalues respectively. Through the operations mentioned above, the point cloud is transformed into 5 geometry feature domains. 


\begin{figure}[t]
    \centering
    \includegraphics[height = 3cm]{cur.pdf}
    \caption{Example of the variables corresponding to the curvature calculation, where the red line indicates  and  indicates the curvature contribution of .}
    \label{fig:variables}
\end{figure}








\subsubsection{ Mesh Geometry Feature Domains}
\label{sec:mqa}
Compared with point cloud, the 3D mesh is more complicated because it is a collection of vertices, edges, and faces which together define the shape of a 3D model. Therefore, the feature extraction process is a bit different from that in the point cloud.

2-1) \textit{Curvature:}
Thanks to the edges and faces information contained in mesh, the computation of curvature is more precise than the point cloud. So far, various curvature definitions have been introduced for 3D meshes \cite{gaussian_curvature} \cite{average_curvature}, among which the average curvature is proven to be able to stably describe the local structural information of 3D meshes. Hence, a weighted average curvature method introduced in \cite{average_curvature} is adopted to measure the roughness of embedded surfaces in 3D meshes. Given the mesh , the weighted average curvature is calculated by averaging the curvature contributions over a certain region  around each vertex :

where  represents the weighted average curvature for vertex , region  is a sphere region centered at  and its radius is 1/100 of the bounding box of the 3D mesh,  stands for the surface area of region .  is the set of edges that are linked to the vertex ,  indicates the curvature contribution of  (the signed angle between the normals of the two oriented triangles incident to edge ),  denotes the weight which is defined as the length of  (always between 0 and ). An example of the variables corresponding to the curvature calculation is shown in Fig. \ref{fig:variables}.

2-2) \textit{Dihedral Angle:}
For a 3D mesh, the dihedral angle is the angle between the normals of two adjacent faces, which has been utilized in \cite{dihedral2} as an effective indicator for measuring the loss of the mesh simplification. Furthermore, \cite{dame} exploits oriented dihedral angles instead of the simple dot product of normals to better distinguish the convex and concave angles. Inspired by similar ideas, we adopt the dihedral angles set to describe the visual quality of 3D meshes. Assuming that the vertices for two adjacent faces  and  are \{\} and \{\}, we have:

where  is the oriented dihedral angle between two adjacent faces  and ,  and  are the normals of  and ,  denotes the signum function which is used to decide the orientation of the dihedral angle. 

2-3) \textit{Face Area and Angle:}
Area and angle are two simple attributes of 3D mesh faces, which can be easily computed by making use of the coordinates of the face's vertices. In the mesh smoothing algorithm proposed in \cite{angle1}, attributes including face angle are used to predict the new location of the smoothed nodes. While in the 3D mesh encoding method introduced in \cite{angle2}, face angle is utilized to instruct the compression of 3D meshes, which indicates that face angle is related to the quality of 3D meshes. Therefore, to further measure the visual quality degradation of 3D meshes, the face area and angle are collected as feature sets. 

Finally, the mesh is projected into 4 geometry feature domains:
\begin{itemize}
    \item Curvature: The weighted average curvature is used to describe the geometry characteristics like roughness or smoothness.
    \item Dihedral Angle: Dihedral angle is employed as a useful descriptor for measuring the caused degradations.
    \item Face Area \& Angle: These two attributes are highly correlated with the effectiveness of lossy operations like compression.
\end{itemize}







\subsection{Color Feature Projection}
\label{pc:color}
Color is a significant aspect of visual quality assessment. For a colored point cloud, the color is directly determined by the color information of the point, while for a colored mesh, the color of the surface is generally rendered by the color information of the contained vertices. Besides, the color information in the 3D models is usually stored in the form of RGB channels. However, the RGB color space has been proven to have a poor correlation with human perception.
Therefore, we adopt the LAB color transformation as the color feature projection:

where  indicates the set of color feature domains of the 3D model,  stands for the color projection function, and  and  represent the distorted point cloud and mesh respectively. The detailed color transformation is formulated as:


where  represent the corresponding RGB color channels,  stand for the corresponding XYZ color channels,  denote the corresponding RGB color channels, and  describe the specified white achromatic reference illuminant. The  function is described as:

where  is set as . Finally, the LAB color channels are computed as the color feature domains.


\begin{figure*}[!htp]
  \centering
  \subfigure[]{\includegraphics[width = 3.4cm]{Cameleon_Ref_0_1.png}}
  \subfigure[]{\includegraphics[width = 3.4cm]{Cameleon_QuantLAB_1_1.png}}
  \subfigure[]{\includegraphics[width = 3.4cm]{Cameleon_QuantLAB_2_1.png}}
  \subfigure[]{\includegraphics[width = 3.4cm]{Cameleon_QuantLAB_3_1.png}}
  \subfigure[]{\includegraphics[width = 3.4cm]{Cameleon_QuantLAB_4_1.png}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_Ref_0L.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_1L.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_2L.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_3L.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_3L.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_Ref_0A.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_1A.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_2A.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_3A.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_4A.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_Ref_0b.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_1B.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_2B.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_3B.pdf}}
  \subfigure[]{\includegraphics[width = 3.4cm,height = 2.65cm]{Chameleon_QuantLAB_4B.pdf}}
  \caption{A comparison example for color distortion in the CMDM database \cite{database}. (a) represents the snapshots of the reference 3D mesh while (b)-(e) stand for the snapshots of the meshes with 4 increasing levels of color information quantization. (f), (k), (p) represent the normalized probability distributions of LAB channels for (a) model, (g), (l), (q) represent the normalized probability distributions of LAB channels for (b) model, (h), (m), (r) represent the normalized probability distributions of LAB channels for (c) model,(i), (n), (s) represent the normalized probability distributions of LAB channels for (d) model, and (j), (o), (t) represented the normalized probability distributions of LAB channels for (e) model respectively.  }
\label{fig:color}
\end{figure*}






\begin{figure*}[tbp]
\centering
\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=3cm,height = 4.5cm]{0.png}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=3cm,height = 4.5cm]{1.png}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=3cm,height = 4.5cm]{17.png}
\end{minipage}
}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=3cm,height = 4.5cm]{35.png}
\end{minipage}
}

\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{0Curvature.pdf}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{1Curvature.pdf}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{17Curvature.pdf}
\end{minipage}
}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{35Curvature.pdf}
\end{minipage}
}


\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{0Anisotropy.pdf}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{1Anisotropy.pdf}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{17Anisotropy.pdf}
\end{minipage}
}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{35Anisotropy.pdf}
\end{minipage}
}

\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{0Linearity.pdf}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{1Linearity.pdf}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{17Linearity.pdf}
\end{minipage}
}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{35Linearity.pdf}
\end{minipage}
}

\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{0Planarity.pdf}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{1Planarity.pdf}
\end{minipage}}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{17Planarity.pdf}
\end{minipage}
}\subfigure[]{
\begin{minipage}[t]{0.23\linewidth}
\centering
\includegraphics[width=4.1cm]{35Planarity.pdf}
\end{minipage}
}\centering
\caption{Examples of point cloud samples from SJTU-PCQA database \cite{sjtu-pcqa}. (a) is the reference point cloud sample, while (b), (c), and (d) are 3 distorted point cloud samples with different distortion types (compression, downsampling, geometry Gaussian noise). Some features' () normalized probability distributions are selected as examples. More specifically, (e), (i), (m), and (q) are the corresponding features'  normalized probability distributions of (a) model, (f), (j), (n), and (r) are the corresponding features' normalized probability distributions of (b) model, (g), (k), (o), and (s) are the corresponding features' normalized probability distributions of (c) model, (h), (l), (p), and (t) are the corresponding features' normalized probability distributions of (d) model respectively.  }
\label{fig:sjtu-pcqa}
\end{figure*}



\section{Estimating Statistical Parameters}
\label{sec:estimate nss parameters}
Through prior knowledge and observations of the corresponding feature distributions, we find that the characteristic statistical parameters of NSS models can be changed by the presence of distortion. Therefore, we choose entropy and several NSS models including the generalized Gaussian distribution (GGD), the general asymmetric generalized Gaussian distribution (AGGD), and the shape-rate Gamma distribution for parameters estimation to quantify the perceptual quality of 3D models. 



\subsection{Basic Statistical Parameters}
For each set of features, we exploit the normalization operation as the pre-processing:

where  represents the feature domain,  is the average function,  denotes the standard deviation function, and  is a small constant to avoid instability.
Entropy is believed to be highly correlated with the quantization distortion. Fig. \ref{fig:color} presents the normalized probability distributions of LAB channels for color quantization distortion. It can be observed that with the increasing quantization levels, the corresponding distributions become significantly sparse, meaning the number of distinct colors is reduced.  
Considering that some simplification and compression algorithms usually introduce quantization operations to the 3D models, we decide to use entropy as one of the quality-aware features.
Then the entropy can be derived as:

where  indicates the entropy function,  and  are the normalized feature distributions for geometry and color respectively.  



\subsection{GGD Parameters}
Fig. \ref{fig:sjtu-pcqa} shows one reference model and three distorted point cloud samples with  different distortion types  (compression,  downsampling,  geometry  Gaussian  noise). It can be observed that the reference curvature and anisotropy distributions shown in Fig. \ref{fig:sjtu-pcqa} (e, i) exhibit Gamma-like appearance while the reference linearity and planarity distributions shown in Fig. \ref{fig:sjtu-pcqa} (m, q) tend to be Gaussian-like. Obviously, with different distortion types, the shapes of the distributions are changed. For example, with closer inspections, we can see that compression adds more weight to the tail of curvature and anisotropy distributions shown in Fig. \ref{fig:sjtu-pcqa} (f, j) while downsampling makes curvature and anisotropy distributions shown in Fig. \ref{fig:sjtu-pcqa} (g, k) more centered. What's more, geometry Gaussian noise makes all the geometry distributions more smooth and adds more weight to the tail of curvature and anisotropy distributions shown in Fig. \ref{fig:sjtu-pcqa} (h, l) on both sides. 

The GGD distribution is effective for capturing a broad spectrum of statistics and reflecting the changes in distribution shapes, especially for the tail part \cite{sharifi1995estimation}.
To capture the corresponding characteristics of 3D model distributions, we propose to use the generalized Gaussian distribution (GGD):

where ,  is the gamma function, and the two estimated parameters () indicate the shape and variance of the distribution. What's more, considering that the variance for normalized distribution is fixed as 1, we estimate the GGD parameters from distributions before normalization.  

\subsection{AGGD Parameters}
However, in some situations, the shape of the distribution is not symmetrical. We can see clearly from Fig. \ref{fig:sjtu-pcqa} that the reference planarity distribution represented by Fig. \ref{fig:sjtu-pcqa} (q) has a longer tail on the left while the planarity distribution represented by Fig. \ref{fig:sjtu-pcqa} (t) has a longer tail on the right. What's more, all types of distortions change weight of the tails for curvature and anisotropy distributions represented by Fig. \ref{fig:sjtu-pcqa} (f, g, h, j, k, l). Therefore, we utilize the general asymmetric generalized Gaussian distribution (AGGD) model to extract more detailed parameters from normalized feature distributions to describe the different spread extent in two directions:

where ,  the parameter  decides the shape of the distribution,  and  are scale parameters that refer to the spread extent on the left and right sides of the distribution respectively,  is a parameter that can better fit the AGGD model stated in \cite{brisque}. Additionally, the AGGD can be recognized as the extension of GGD. When , the AGGD turns into GGD. Finally, the four parameters () are estimated to describe the  characteristics of asymmetric distributions.



\subsection{Gamma Parameters}
The reference curvature and anisotropy distributions shown in Fig. \ref{fig:sjtu-pcqa} (e, i) are similar to the shape of Gamma distribution. Compression, downsampling, and Gaussian noise all change the shape and scale of the corresponding distributions and such distributions still exhibit Gamma-like appearance. Therefore, we propose to use Gamma distribution parameters to quantify the distortions. The shape-rate Gamma model is formulated as:

where  and  stands for the shape and rate parameters and . 







\begin{table*}[t]
\renewcommand\arraystretch{1.5}
\renewcommand\tabcolsep{3.0pt}
\setlength{\abovecaptionskip}{-5pt}
  \caption{Summary of features extracted in the proposed method}
  
  \label{tab:features}
  \begin{center}
  \begin{tabular}{cccccc}
    \toprule
    Format & Feature Domains & Feature ID & Feature Description  &  Computation\\
    \hline
    \multirow{4}{*}{Point cloud} 
    & () &       & Mean and standard deviation for each feature domain    &  Eq.(\ref{equ:mean})  \\
    \cline{2-5}
    & () &   & Entropy for each feature domain  &  Eq.(\ref{equ:entropy})    \\
    \cline{2-5}
    & () &   &GGD () for each feature domain  &  Eq.(\ref{equ:ggd})    \\
    \cline{2-5}
     & () &   & AGGD () for each normalized feature domain  &  Eq.(\ref{equ:aggd1})    \\
     \cline{2-5}
     & () &   & Gamma () for each normalized feature domain  &  Eq.(\ref{equ:gamma})    \\
     \hline
     
    \multirow{4}{*}{Mesh} 
    & () &       & Mean and standard deviation for each feature domain    &  Eq.(\ref{equ:mean})  \\
    \cline{2-5}
    & () &   & Entropy for each feature domain  &   Eq.(\ref{equ:entropy})    \\
    \cline{2-5}
    & () &   &GGD () for each feature domain  &  Eq.(\ref{equ:ggd})    \\
    \cline{2-5}
     & () &   & AGGD () for each normalized feature domain  &   Eq.(\ref{equ:aggd1})    \\
     \cline{2-5}
     & () &   & Gamma () for each normalized feature domain  & Eq.(\ref{equ:gamma})    \\
    \bottomrule
  \end{tabular}
  \end{center}
\end{table*}

\subsection{Parameters Summary}
In summary, we collect the average, standard deviation, and entropy values in the 3D model feature domains as the fundamental features. Then we use 3 representative distribution models to estimate 8 statistical parameters, including GGD (), AGGD (), Gamma (), for all feature domains. Finally, considering that the colored point cloud has 8 feature domains () and the colored mesh has 7 feature domains (), 88 () features are computed for a single colored point cloud and 77 () features are computed for a single colored mesh respectively. The summary of features extracted in the proposed method is listed in Table \ref{tab:features}.

  



\section{Experiment Evaluation}
\label{sec:experiment}

\subsection{Regression Model}
After the feature extraction process, a feature vector is obtained to describe the characteristics of the 3D model. In our experiment, we propose to use the support vector machine regressor (SVR) as the regression model, which is a common and effective choice to handle high dimensional data in previous quality assessment research \cite{nr-svr} \cite{brisque}. We employ the standard normalization as the pre-processing to scale the features. Then the feature vector can be integrated into a quality score for evaluation. We employ the Python sklearn package \cite{sklearn} to implement the radial basis function (RBF) kernel SVR model with default settings.








\subsection{Experiment Setup}
\subsubsection{Experiment Setup for PCQA}
 To test the performance of the proposed method, we employ the subjective point cloud assessment database (SJTU-PCQA) \cite{sjtu-pcqa} and the Waterloo point cloud assessment database (WPC) proposed in \cite{su2021perceptual}. 
 
 The SJTU-PCQA database provides 420 point cloud samples distorted from 10 reference point clouds. Each reference point cloud is distorted with seven types of common distortions with six levels. Unfortunately, only 9 reference point clouds and their corresponding distorted point cloud samples are now available to the public, thus we can obtain 378 () point cloud samples for the experiment.
Since the proposed approach requires a training procedure to calibrate the SVR model and to avoid the influence of content overlap, we select 8 of the 9 groups of point clouds as training set and leave the rest 1 group as testing set. In order to ensure the validity of the results, we exhaustively list all the =9 database separations for the experiment and use the average performance as the final experimental results. In addition, the MOSs collected in the SJTU-PCQA database are divided by 10 in the training process to scale the MOSs to [0,1]. The predicted scores are re-scaled for validation in the testing process. 

{The WPC database includes 20 high-quality source point clouds and creates 740 distorted point clouds using downsampling, Gaussian noise, and three types of compression. Specifically, we maintain the same training set and testing set split as stated in \cite{liu2021pqa}. Similarly, the MOSs provided in the WPC database are divided by 100 in the training process and the predicted scores are re-scaled for validation in the testing process.
The comparison PCQA metrics can be categorized into two types:}





\begin{itemize}
    \item  Image-based metrics: These metrics evaluate the quality of 3D models by assessing the quality of the corresponding 2D projections. Please refer to \cite{sjtu-pcqa} for detailed projection process and we use only RGB projections. The FR image-based metrics include PSNR, SSIM \cite{ssim}, and PB-PCQA\cite{sjtu-pcqa}. The NR image-based metrics include NIQE \cite{niqe}, BRISQUE \cite{brisque}, and PQA-net \cite{liu2021pqa}.

    \item Model-based metrics: Full-reference metrics operate directly from the 3D model, which include GraphSIM \cite{yang2020inferring}, PointSSIM \cite{pcqa3}, PCQM \cite{pcqm}, and ResCNN \cite{pcqa-large-scale}. Reduced-reference metric includes PCMRR \cite{viola2020reduced}. Specifically, ResCNN is a model-based deep-learning approach. 
\end{itemize}










\subsubsection{Experiment Setup for MQA}
The MQA method proposed in this paper is validated on the color mesh distortion measure (CMDM) database \cite{database}. The database is generated from 5 source models subjected to geometry and color distortions. Then the source models are corrupted with 4 types of distortions based on color and geometry and each type of distortion is adjusted with 4 different strengths. The , , , and  models are selected for training while the  model is chosen for testing. Specifically, each distorted model is provided with 5 subjective scores according to its viewpoints and animation types. For simplification, we use the average of the 5 subjective scores as the final quality score for the distorted model.

In the literature, few metrics are proposed to deal with the 3D-QA tasks of color meshes. In order to evaluate the performance of the proposed method, some image-based metrics that might be able to predict the visual quality of 3D meshes are utilized as competitors. Unfortunately, few no-reference quality assessment metrics for 3D meshes are open-sourced, thus we try to reproduce some of the metrics. Specifically, the metrics used for comparison can be divided into two types: 
\begin{itemize}
    \item  Image-based metrics: These metrics operate by evaluating the quality of 2D images rendered from the 3D colored meshes. The FR image-based metrics include PSNR and SSIM \cite{ssim}. The NR image-based metrics include NIQE \cite{niqe} and BRISQUE \cite{brisque}.
    \item Model-based metrics: The FR-MQA metric includes CMDM \cite{database}. The NR-MQA metrics designed especially for geometry-only 3D meshes include NR-SVR \cite{nr-svr}, NR-GRNN \cite{nr-grnn}, NR-CNN \cite{nr-cnn}.
\end{itemize}



\subsection{Evaluation Criterion}
Four mainstream consistency evaluation criteria are utilized to compare the correlation between the predicted scores and MOSs, which include Spearman Rank Correlation Coefficient (SRCC), Kendall’s Rank Correlation Coefficient (KRCC), Pearson Linear Correlation Coefficient (PLCC), and Root Mean Squared Error (RMSE).
An excellent model should obtain values of SRCC, KRCC, and PLCC close to 1, and the value of RMSE near 0. 





\begin{table*}[t]
\renewcommand\arraystretch{1.4}
\renewcommand\tabcolsep{8pt}
\setlength{\abovecaptionskip}{-5pt}
  \caption{ Performance comparison with competitors on the SJTU-PCQA and WPC databases.}
  \vspace{-0.05cm}
  \begin{center}
  \begin{tabular}{c|c|c|c|cccc|cccc}
    \toprule
    \multirow{2}{*}{Ref} & \multirow{2}{*}{Type} &\multirow{2}{*}{Index} & \multirow{2}{*}{Metric} & \multicolumn{4}{c|}{SJTU-PCQA} & \multicolumn{4}{c}{WPC} \\ \cline{5-12}
     &&&& PLCC &  SRCC & KRCC & RMSE & PLCC &  SRCC & KRCC & RMSE \\
    \hline
    \multirow{6}{*}{FR} & \multirow{3}{*}{Image-based}
    &A&PSNR  & 0.2317 & 0.2422 & 0.1077 & 2.3124 & 0.4872 & 0.4235 & 0.3080 & 15.8133\\
&&B&SSIM  & 0.3476 & 0.2987 & 0.1919  & 21770 & 0.4944 & 0.3878 &0.3234 & 15.7749 \\
&&C &PB-PCQA  &0.6076 & 0.6020 & - & 1.8635  &-&-&-&- \\
    \cline{2-12}
    & \multirow{3}{*}{Model-based}
    &D&GraphSIM  &0.8449 & 0.8483 & 0.6448 & 1.5721 &0.6163 & 0.5831 & 0.4194 & 17.1939\\
&&E &PointSSIM  &0.7136 & 0.6867 & 0.4964  & 1.7001 &0.4667 & 0.4542 & 0.3278  & 20.2733\\
&&F &PCQM  &\textbf{0.8653} & \textbf{0.8544} & \textbf{0.6586}  & \textbf{1.2162} & \textbf{0.7499} & \textbf{0.7434} & \textbf{0.5601}  & \textbf{15.1639}\\
    \hline
    \multirow{1}{*}{RR} & \multirow{1}{*}{Model-based}
     &G&PCMRR  &0.6101 & 0.4816 & 0.3362  & 1.9342 &0.3433 & 0.3097 & 0.2082  & 21.5302\\
     \hline
    \multirow{5}{*}{NR} & \multirow{2}{*}{Image-based}
    &H&NIQE  &0.3764 & 0.2214 &0.1512  & 2.2671 &0.3957 &0.3887 &0.2551  & 22.5502\\


    & &I&BRISQUE  &0.2241 & 0.2051 & 0.1121 & 2.2428 &0.4176 & 0.3781 & 0.2444 & 22.5414\\
    & &J&PQA-Net  &-&-&-&- & \textbf{0.7000} & \textbf{0.6900} & \textbf{0.5100}  & \textbf{15.1800}\\
    \cline{2-12} & \multirow{2}{*}{Model-based}
    &K&ResCNN   & 0.5975 & 0.6187 & - & - &-&-&-&- \\
    & &L&Proposed  & \textbf{0.7382} & \textbf{0.7144} & \textbf{0.5174} & \textbf{1.7686} & 0.6514 & 0.6479 & 0.4417 & 16.5716\\
    \bottomrule
  \end{tabular}
  \end{center}
  \label{tab:pcqa}
  \vspace{-0.7cm}
\end{table*}



\begin{figure*}[!tbp]
\centering
\subfigure[\textit{}]{
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width = 2.6cm]{sjtu1_crop.jpg}
\end{minipage}}\subfigure[\textit{}]{
\begin{minipage}[t]{0.16\linewidth}
\centering
\includegraphics[width = 2.6cm]{sjtu2_crop.jpg}
\end{minipage}}\subfigure[\textit{}]{
\begin{minipage}[t]{0.16\linewidth}
\centering
\includegraphics[width = 2.6cm]{sjtu3_crop.jpg}
\end{minipage}}\subfigure[\textit{}]{
\begin{minipage}[t]{0.16\linewidth}
\centering
\includegraphics[width =2.6cm]{wpc1_crop.jpg}
\end{minipage}}\subfigure[\textit{}]{
\begin{minipage}[t]{0.16\linewidth}
\centering
\includegraphics[width = 2.6cm]{wpc2_crop.jpg}
\end{minipage}}\subfigure[\textit{}]{
\begin{minipage}[t]{0.16\linewidth}
\centering
\includegraphics[width =2.6cm]{wpc3_crop.jpg}
\end{minipage}}

\caption{Illustration of point clouds with consistently increasing downsampling distortion levels from the SJTU-PCQA \cite{sjtu-pcqa} and the WPC \cite{su2021perceptual} databases. (a), (b), and (c) are the snapshots of \textit{rednandblack\_15}, \textit{rednandblack\_16}, and \textit{rednandblack\_17} from the SJTU-PCQA database. (d), (e), and (f) are the snapshots of \textit{banana\_level\_9}, \textit{banana\_level\_8}, and \textit{banana\_level\_7} from the WPC database respectively. }
\label{redandblack}
\end{figure*}



\subsection{Performance Discussion}
\subsubsection{The performance of PCQA validated on the SJTU-PCQA and WPC Databases}
{ The experimental results for PCQA on the SJTU-PCQA database and the WPC database are shown in Table \ref{tab:pcqa}. The best performance results of the NR-PCQA methods along with the best performance results of all methods are marked in bold. We can see clearly that the FR-PCQA metric PCQM outperforms other PCQA metrics. With the assistance of reference information, FR-PCQA methods tend to be more effective at predicting the quality levels of colored point clouds than NR-PCQA methods. The proposed method achieves first place on the SJTU-PCQA database and second place on the WPC database among the compared NR-PCQA metrics respectively. It's worth mentioning that the PQA-Net extracts features from the rendering views with deep-learning networks. The proposed method depends on handcrafted features and remains competitive considering the computational resource consumption of deep-learning networks. 





\begin{table}[!t]
\setlength{\abovecaptionskip}{-5pt}
\renewcommand\arraystretch{1.5}
\renewcommand\tabcolsep{3.4pt}
  \caption{Performance comparison with competitors on the CMDM database.}
  \vspace{-0.05cm}
  \begin{center}
  \begin{tabular}{c|c|c|c|cccc}
    \toprule
    Ref & Type &Index & Metric & PLCC  &  SRCC & KRCC &RMSE\\
    \hline
    \multirow{3}{*}{FR} & \multirow{2}{*}{Image-based} 
    &A&PSNR & 0.7672 & 0.7735 & 0.7280 & 0.8832\\
&&B&SSIM & 0.7944 & 0.7817 &0.7000 & 0.9656\\
\cline{2-8} & \multirow{1}{*}{Model-based} 
    &C&CMDM & \textbf{0.9130} & \textbf{0.9000} & - & -\\
    \hline
    \multirow{6}{*}{NR}  & \multirow{2}{*}{Image-based} 
    &D&NIQE  &0.4059 & 0.4768 & 0.3000 & 1.3352\\
& &E&BRISQUE  & 0.5786 &0.4882 & 0.3598 & 1.2237\\
    \cline{2-8} & \multirow{4}{*}{Model-based} 
    &F&NR-SVR  &0.6082 & 0.4489 & 0.3420 & 1.3147\\
& &G&NR-GRNN  & 0.6599 & 0.6948 & 0.5130 & 1.1121\\
& &H&NR-CNN  &0.5204 & 0.5022 & 0.3420 & 1.2804\\
&  &I&Proposed  & \textbf{0.8626} & \textbf{0.8754} & \textbf{0.7222} & \textbf{0.6062}\\
    \bottomrule
  \end{tabular}
  \end{center}
  \label{tab:mqa}
  \vspace{-0.7cm}
\end{table}










With closer observations, we can make several more detailed analyses: 1-1) Image-based metrics using simple IQA models (such as PSNR and SSIM) are less effective. This is because the rendering views captured from various viewpoints are quite different in content. To get more quality information, more snapshots have to be sampled, which may confuse such IQA models because they are not capable of dealing with multiple pairs of images with dissimilar contents. Thanks to the strong learning ability of CNN, PQA-Net overcomes the difficulty and achieves high performance by utilizing the correlation of the rendering views. To conclude, we think that increasing the sample number of snapshots and optimizing learning models are the focus of image-based methods in the future. However, such methods inevitably require large amounts of computation resources and highly depend on the scale of the database.  1-2) Most model-based methods achieve lower performance on the WPC database than on the SJTU-PCQA database. We attempt to give the reasons. First, although the two databases manually introduce similar types of distortions, the WPC database employs some point cloud samples that are less sensitive to distortions in perceived visual quality.
For example, as can be seen in Fig. \ref{redandblack}, the \textit{rednandblack} sample from the SJTU-PCQA database appears to be more distinct under the influence of downsampling distortions. However, the \textit{banana} sample from the WPC database is less complex in geometry structure and limited in colorfulness, which makes the distortion difference less obvious. Second, the SJTU-PCQA database includes distorted point clouds with mixed distortions while the WPC database only adds one type of distortion to a single point cloud. It seems that point clouds with mixed distortions are more distinguishable in quality under close distortion levels. What's more, the WPC database has twice as many reference point clouds as the SJTU-PCQA database. The expanded scale and content diversity challenge the generalization ability of such methods as well.}      




\begin{figure}[tbp]
\centering
\subfigure[\textit{}]{
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[width = 4cm]{cmdm1_crop.jpg}
\end{minipage}}\subfigure[\textit{}]{
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[width = 4cm]{cmdm2_crop.jpg}
\end{minipage}}\\
\subfigure[\textit{}]{
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[width = 4cm]{cmdm3_crop.jpg}
\end{minipage}}\subfigure[\textit{}]{
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[width = 4cm]{cmdm4_crop.jpg}
\end{minipage}}


\caption{Examples of meshes with increasing geometry quantization distortion levels from the CMDM database \cite{database}. (a), (b), (c), and (d) are the snapshots of \textit{Ari\_QuantGeom\_1}, \textit{Ari\_QuantGeom\_2}, \textit{Ari\_QuantGeom\_3}, and \textit{Ari\_QuantGeom\_4} respectively. }

\label{fig:ari}
\vspace{-0.5cm}
\end{figure}



\subsubsection{The performance of MQA validated on the CMDM Database}
{Table \ref{tab:mqa} presents the experimental results on the CMDM database, in which the top performance results of the NR-PCQA methods along with the best performance results of all methods are marked in bold. Surprisingly, the FR image-based metrics get remarkable performance results. It is because the CMDM database manually adjusts the employed 4 strengths of distortions to cover most range of visual quality levels (from imperceptible levels to high levels of impairment), which makes the distortion levels more coarse-grained. As illustrated in Fig. \ref{fig:ari}, the degradations caused by the 4 distortion strengths are quite obvious, thus enabling such simple methods to gain competitive performance. 

The NR model-based MQA methods such as NR-SVR, NR-GRNN, and NR-CNN have a stronger ability to predict the visual quality than the NR image-based MQA methods, but they do not take color information into consideration, thus resulting in lower performance than the proposed method. The FR-MQA metric CMDM achieves first place in PLCC and SRCC and the proposed method obtains second place in PLCC and SRCC with a slight gap. The reason is that the coarse-grained levels of distortions make it easier for the proposed method to learn the relationship between the corresponding 3D-NSS parameters and quality scores. Therefore, even with relatively little training data, the proposed method still obtains good performance.}



\begin{table*}[t]
\renewcommand\arraystretch{1.5}
\renewcommand\tabcolsep{5.5pt}
\setlength{\abovecaptionskip}{-5pt}
  \caption{ Performance of the ablation study. The best performance for each database is marked in bold.}
  
  \label{tab:ablation}
  \begin{center}
  \begin{tabular}{c|c|cccc|cccc|cccc}
    \toprule
    \multirow{2}{*}{Type}  & \multirow{2}{*}{Feature}  & \multicolumn{4}{c|}{SJTU-PCQA} & \multicolumn{4}{c|}{WPC} & \multicolumn{4}{c}{CMDM} \\
    \cline{3-14}
    &  &PLCC & SRCC & KRCC & RMSE & PLCC & SRCC & KRCC & RMSE & PLCC & SRCC & KRCC & RMSE \\
    \hline
    \multirow{5}{*}{Geometry} & F1 & 0.6406 & 0.6171 & 0.5283 & 1.8825 & 0.4280 & 0.4219 & 0.2801 & 19.5704 & 0.6214  &0.4162  &0.2852 & 0.9288 \\
    &F2 & 0.6082 & 0.5837 & 0.3963 & 1.8989 & 0.5042 & 0.4653 & 0.2992 & 18.7064 & 0.5872  &0.4355  &0.3022 & 1.0861 \\
    &F3 & 0.6259 & 0.5997 & 0.4166 & 1.9205 & 0.3874 & 0.3714 & 0.2350 & 20.4081 & 0.5656  &0.3333  &0.2481 & 1.0053 \\
    &F4 & 0.5187 & 0.4216 & 0.2713 & 1.9892 & 0.3889 & 0.3838 & 0.2761 & 21.2468 & 0.5358  &0.4163  &0.3096 & 1.0692 \\ 
    & (F1\~{}F4)  & 0.7314  & 0.6937 & 0.5067 & 1.8096 & 0.5889  & 0.5827 & 0.4186 & 17.8330 & 0.6561 & 0.5076 & 0.3954 & 0.9267 \\\hline
    \multirow{5}{*}{Color}
    &F5 & 0.1897 & 0.1250 & 0.0840 & 2.5758 & 0.1436 & 0.1327 & 0.0900 & 26.3404 & 0.1245  &0.1101  &0.0807 & 1.2254 \\
    &F6 & 0.2755 & 0.2170 & 0.1548 & 2.3927 & 0.1548 & 0.1009 & 0.0708 & 25.1412 & 0.0074  & 0.0291  & 0.0171 & 1.2582 \\
    &F7  & 0.0097 & 0.0069 & 0.0042 & 2.6471 & 0.1557 & 0.1836 & 0.1242 & 27.9339 & 0.1090  &0.0793  &0.0574 & 1.2556 \\
    &F8 & 0.0004 & 0.0012 & 0.0001 & 2.5288 & 0.1839 & 0.1488 & 0.0906 & 21.9701 & 0.1281  &0.1860  &0.0064 & 1.2617 \\
    &(F5\~{}F8)  & 0.2351 & 0.2408 & 0.1685 & 2.4511 & 0.2220 & 0.1857 & 0.1599 & 24.3489 & 0.2985 & 0.1820  & 0.2283 & 1.2469\\ \hline
    Basic & (F1+F5)  & \textbf{0.7522} & \textbf{0.7449} & \textbf{0.5454}  & \textbf{1.7004} & 0.5335 & 0.5121 & 0.3647  & 21.4647 & {0.8481} & {0.8530}  & {0.6955} & {0.6079}\\
GGD & (F2+F6)  & 0.7173  & 0.6973 & 0.5035 & 1.7781 & 0.6172  & 0.5968 & 0.4353 & 17.1555 & 0.7819  & 0.7392 & 0.5821 & 0.7646\\
AGGD & (F3+F7)  & 0.6106 & 0.6122 & 0.4337 & 2.0208 & 0.4268 & 0.4243 & 0.2921 & 20.9246 & 0.8342 & 0.7974 & 0.6456 & 0.6989\\
Gamma & (F4+F8)  & 0.5841 & 0.4999 & 0.3504 & 2.0469 & 0.4584 & 0.4516 & 0.3177 & 19.8646 & 0.5982 & 0.5262 & 0.3951 & 0.9956\\ \hline




\multirow{2}{*}{NSS} & 2D & 0.3604 & 0.2123 & 0.1613 &2.3125 & 0.4019 &0.3919 & 0.2337 & 22.5500 & 0.5616 &0.4772 & 0.2988 &1.2347\\
    &2D+3D &0.4122 & 0.3344 & 0.2951 &2.2005 & 0.4658 & 0.4452 & 0.2500 & 21.2028 & 0.6658 & 0.6652 & 0.3700 & 0.9981\\\hline
All & F1\~{}F8 & 0.7382 & 0.7144 & 0.5174 & 1.7686 & \textbf{0.6514} & \textbf{0.6479} & \textbf{0.4417} & \textbf{16.5716} & \textbf{0.8626} & \textbf{0.8754} & \textbf{0.7222} & \textbf{0.6062}\\
    \bottomrule
  \end{tabular}
  \end{center}
  \vspace{-5pt}
\end{table*}



\subsection{Ablation Study}

{
To further test the effectiveness and contributions of different types of features (color and geometry) and different statistical parameters, we split the features into 8 groups: (1) F1: mean, standard deviation, and entropy values of geometry feature domains; (2) F2: GGD parameters of geometry feature domains; (3) F3: AGGD parameters of geometry feature domains; (4) F4: Gamma distribution parameters of geometry feature domains; (5) F5: mean, standard deviation, and entropy values of color feature domains; (6) F6: GGD parameters of color feature domains; (7) F7: AGGD parameters of color feature domains; (8) F8: Gamma distribution parameters of color feature domains. Then we can analyze the features' contributions by obtaining the performance of different combinations of feature groups. For example, the F2+F6 model only contains AGGD parameters of both geometry and color feature domains while the F1\~{}F4 model only consists of statistical parameters of geometry feature domains.


Besides, most 3D models are perceived by humans only through the rendered 2D views. To find out whether the information of 2D views would help improve the performance of 3D-NSS, we test the effectiveness of the combined 2D-NSS and 3D-NSS features as well. The 2D-NSS features are extracted by the 2D views of objects using BRISQUE \cite{brisque} and NIQE \cite{niqe} (The redundant features of BRISQUE and NIQE are removed ). The features extracted from different viewpoints are averaged to get the final 2D-NSS features. The proposed all groups' features in this paper are employed as the 3D-NSS features. We simply concatenate the 2D-NSS and 3D-NSS features as the quality-aware feature vector.

The performance results of the ablation study are shown in Table \ref{tab:ablation}, where the mean, standard deviation, and entropy values of geometry and color feature domains are represented as the basic type of features (F1+F5). It can be obviously observed that the geometry features contribute more to the final quality score on all three databases. It might be because all three databases introduce more types of geometry distortions than color distortions and the geometry information weighs more in the human perception of 3D models.





By analyzing the different performances of different models, we can make several observations. 1) On the SJTU-PCQA database, we can see that the mean, standard deviation, and entropy values obtain the best performance, even higher than the proposed all groups. While on the WPC database, the all groups model is significantly superior to the separate distribution models. We attempt to give our explanation as follows. The ability of simple parameters to describe the shape of distributions is limited. For example, some distributions with very different shapes may have the same mean and variance. When the diversity and number of samples are not saturated, simpler parameters may achieve better results (more complex parameters are troubled by the redundant information), but when the diversity and number of samples increase, these simpler parameters are less effective, which requires the help of more complex parameters to describe the distributions. Therefore, we suggest using the proposed all groups model for practical application since such model is more capable of describing and distinguishing different distributions. 2) It seems that the 2D-NSS is not effective for 3D-QA tasks. We try to give the reason for such phenomenon as well. Single rendering 2D views contain limited information, therefore, we have to use multiple viewpoints to cover more quality information. However, the contents of snapshots from various viewpoints are quite different, such as the top and bottom views of the 3D models. The contents difference may have a much greater influence on the distributions rather than the distortions. }





\begin{figure*}[htbp]
\centering
\subfigure[SJTU-PCQA]{
\begin{minipage}[t]{0.3\linewidth}
\centering
\includegraphics[width = 4.8cm]{pc1_heatmap.pdf}
\end{minipage}}\subfigure[WPC]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width = 4.8cm]{pc2_heatmap.pdf}
\end{minipage}}\subfigure[CMDM]{
\begin{minipage}[t]{0.3\linewidth}
\centering
\includegraphics[width = 4.8cm]{mesh_heatmap.pdf}
\end{minipage}}\caption{Statistical test results of the proposed method and compared metrics on the SJTU-PCQA, WPC, and CMDM databases. A black/white block means the row method is statistically worse/better than the column one. A gray block means the row method and the column method are statistically indistinguishable. The metrics are denoted by the same index as in Table \ref{tab:pcqa} and Table \ref{tab:mqa} respectively.}
\label{heatmap}
\vspace{-0.2cm}
\end{figure*}



\begin{table*}[t]
\renewcommand\arraystretch{1.5}
\setlength{\abovecaptionskip}{-5pt}
  \caption{ Performance of the data-sensitivity experiment. The best performance for each database is marked in bold.}
  
  \label{tab:param}
  \begin{center}
  \begin{tabular}{c|cccc|cccc|cccc}
    \toprule
    \multirow{2}{*}{Criteria}  & \multicolumn{4}{c|}{SJTU-PCQA}  & \multicolumn{4}{c|}{WPC}  &\multicolumn{4}{c}{CMDM} \\
    \cline{2-13}
    & 20\% & 40\% & 60\% & 80\% & 20\% & 40\% & 60\% & 80\% & 20\% & 40\% & 60\% & 80\% \\
    \hline
    PLCC & 0.6009 & 0.6458 & 0.6655 & \textbf{0.7282} & 0.5303 & 0.5575 & 0.5702 & \textbf{0.6514} & 0.6935 & 0.7464 & 0.8178   & \textbf{0.8626} \\
    SRCC & 0.5851 & 0.6260 & 0.6462 & \textbf{0.7144} & 0.5208 & 0.5187 & 0.5542 & \textbf{0.6479} & 0.5807 & 0.6375 & 0.7422   & \textbf{0.8754} \\
    KRCC & 0.4083 & 0.4402 & 0.4574 & \textbf{0.5174} & 0.3611 & 0.3655 & 0.3938 & \textbf{0.4417} & 0.4152 & 0.4682 & 0.5680   & \textbf{0.7222} \\
    RMSE & 2.0708 & 1.9482 & 1.9018 & \textbf{1.7686} & 19.2921 & 18.9968 & 18.1487 & \textbf{16.5176} & 0.9064 & 0.8157 & 0.7208   & \textbf{0.6062} \\
    \bottomrule
  \end{tabular}
  \end{center}
  \vspace{-0.4cm}
\end{table*}











\subsection{Statistical Test}
To further analyze the performance of the proposed method, we conduct the statistical test in this section. We follow the same experiment setup as in \cite{statistic-test} and compare the difference between the predicted quality scores with the subjective ratings. All possible pairs of models are tested and the results are listed in Fig. \ref{heatmap}. 
It can be seen that our method is significantly superior to 7 compared PCQA metrics on the SJTU-PCQA and the WPC databases while our method also significantly outperforms 7 compared MQA metrics on the CMDM database. Specifically, the FR metric PCQM achieves significantly better performance than our method on both SJTU-PCQA and WPC databases, the FR metric CMDM is insignificantly distinguishable from our method on the CMDM database.




\subsection{Data-sensitivity Experiment}
{ To find out the influence of the number of training samples, we conduct the data-sensitivity experiment by changing the proportion of training set as about 20\%, 40\%, 60\%, and 80\% (2, 4, 6, 8 sample groups for training on the SJTU-PCQA database, 4, 8, 12, 16 for training on the WPC database, and 1, 2, 3, 4 samples for training on the CMDM database correspondingly). The experimental results are exhibited in Table \ref{tab:param}, from which we can find that increasing the number of training samples is helpful to improve the performance of the proposed method. The CMDM database employs coarse-grained distortion levels and includes relatively small amounts of mesh samples, thus the increase of training data improves the performance more significantly. For the PCQA database, we think using fewer training samples may exaggerate the impact of noisy labels and result in the over-fit of the model, which causes the performance to drop. In all, increasing the diversity and number of the training samples are beneficial for obtaining more robustness and higher performance. }













 

   




\begin{figure}
    \centering
    \includegraphics[width = 8.8cm]{time_cost.pdf}
\caption{The results of time cost comparison on the SJTU-PCQA and the WPC databases. The time cost refers to the average time consumption per point cloud for each database.}
    \label{fig:time}
    \vspace{-0.3cm}
\end{figure}




\subsection{Computational Efficiency}
Considering that the proposed method operates directly from the 3D model, we also focus on computational efficiency. The image-based metrics usually operate quite fast due to the mature development of IQA metrics. Therefore, to make the comparison meaningful, we select model-based methods GraphSIM, PointSSIM, PCQM, and PCMRR as competitors. We conduct the test on a computer with Intel (R) Core (TM) i5-3470 CPU @ 3.20 GHz and 8 GB RAM on the Windows platform. The corresponding time cost results are shown in Fig. \ref{fig:time}, and we can see clearly that the average time cost of the proposed method is smaller than all the compared model-based methods, which indicates that our method achieves relatively considerable computational efficiency.




\section{Conclusion}
This paper proposes a no-reference colored 3D model quality assessment metric based on entropy and 3D natural scene statistics. The proposed method deals with the quality assessment problems for both colored point cloud and mesh models. We first project the 3D models into corresponding quality-related geometry and color feature domains. Then entropy and various 3D-NSS parameters are estimated to better capture the representative characteristics and quantify the distortions that are more in line with human perception. The proposed method is validated on the colored point cloud quality assessment database (SJTU-PCQA), the Waterloo point cloud assessment database (WPC), and the colored mesh quality assessment database (CMDM). The experimental results show that our method outperforms most compared NR 3D-QA metrics with competitive computational resources and reduces the performance gap with the state-of-the-art FR 3D-QA metrics. The proposed method follows a common NR framework and can be easily modified and expanded to satisfy specific needs, which has great application potential. The code of the proposed method is released for promoting the development of NR 3D-QA.
\label{sec:conclusion}















\bibliographystyle{IEEEtran}
\bibliography{output}
\end{document}
