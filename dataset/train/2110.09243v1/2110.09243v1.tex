\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{3dv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{gensymb}

\usepackage{subcaption}
\usepackage{enumitem}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\threedvfinalcopy 

\def\threedvPaperID{67} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifthreedvfinal\pagestyle{empty}\fi

\usepackage[dvipsnames, table]{xcolor}
\usepackage{xspace}
\usepackage{xcolor}
\usepackage{diagbox}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{multirow}
\usepackage{caption}

\definecolor{Gray}{gray}{0.9}
\definecolor{lightgray}{gray}{0.94}
\newcommand{\fb}[1]{{\color{MidnightBlue}{F: #1}}}
\newcommand{\fbt}[1]{{\color{MidnightBlue}{#1}}}

\newcommand{\tg}[1]{{\color{RubineRed}{T: #1}}}
\newcommand{\tgt}[1]{{\color{RubineRed}{#1}}}

\newcommand{\pw}[1]{{\color{PineGreen}{P: #1}}}
\newcommand{\pwt}[1]{{\color{PineGreen}{#1}}}

\newcommand{\rb}[1]{{\color{RedViolet}{R: #1}}}
\newcommand{\rbt}[1]{{\leavevmode\color{RedViolet}{#1}}}

\newcommand{\gr}[1]{{\color{RoyalPurple}{G: #1}}}
\newcommand{\grt}[1]{{\color{RoyalPurple}{#1}}}

\newcommand{\yk}[1]{{\color{Brown}{Y: #1}}}
\newcommand{\ykt}[1]{{\color{Brown}{#1}}}

\newcommand{\smpltransformer}{PoseBERT\xspace}
\newcommand{\poseformer}{\smpltransformer}
\newcommand{\posebert}{\smpltransformer}
\newcommand{\sspin}{MoCap-SPIN\xspace}
\newcommand{\aspin}{Render-SPIN\xspace}
\newcommand{\mocapspin}{\sspin}

\newcommand{\errorgain}[1]{\color{OliveGreen}{( #1)}}


\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother


\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vl}{\mathbf{l}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vo}{\mathbf{o}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vt}{\mathbf{t}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}



\newcommand{\reg}{\text{reg}}
\newcommand{\vxsynth}{\mathbf{s}}



\renewcommand{\paragraph}[1]{\vspace{0.02cm}\noindent\textbf{#1}}
 
\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\begin{document}

\title{Leveraging MoCap Data for Human Mesh Recovery}

\author{
Fabien Baradel
\and
Thibault Groueix
\and
Philippe Weinzaepfel
\and
Romain Br\'egier
\and
Yannis Kalantidis \hspace{10mm}  Gr\'egory Rogez \vspace{1mm}\\
{NAVER LABS Europe}\\
{\tt\small firstname.lastname@naverlabs.com}
} 

\maketitle
\thispagestyle{empty}

\blfootnote{ indicates equal contribution. Thibault Groueix is now at Adobe.}
\begin{abstract}
\noindent
Training state-of-the-art models for human body pose and shape recovery from images or videos requires datasets with corresponding annotations that are really hard and expensive to obtain.
Our goal in this paper is to study whether poses from 3D Motion Capture (MoCap) data can  be used to improve image-based and video-based human mesh recovery methods.
We find that fine-tune image-based models with synthetic renderings from MoCap data can increase their performance, by providing them with a wider variety of poses, textures and backgrounds. In fact, we show that simply fine-tuning the batch normalization layers of the model is enough to achieve large gains. 
We further study the use of MoCap data for video, and introduce \smpltransformer, a transformer module that directly regresses the pose parameters and is trained via masked modeling.
It is simple, generic and can be plugged on top of any state-of-the-art image-based model in order to transform it in a video-based model leveraging  temporal information.
Our experimental results show that the proposed approaches reach state-of-the-art performance on various datasets including 3DPW, MPI-INF-3DHP, MuPoTS-3D, MCB and AIST. Test code and models will be available soon.
\end{abstract} 
\begin{figure}
\centering
\includegraphics[width=\linewidth]{fig/teaser_figure_spin_vibe.pdf} \-0.4cm]
\vspace{-0.15cm}
\caption{\label{fig:synth_pairs} Pairs of augmented real images from COCO~\cite{coco}, LSPE~\cite{lsp}, MPII~\cite{mpii} and corresponding synthetic renderings depicting the same pose with a different background and body appearance.}
\end{subfigure}
\vspace{-0.3cm}
\caption{\label{fig:synth_examples} \textbf{Examples of synthetic human renderings.}}
\vspace{-0.5cm}
\end{figure*}
 \section{Related work}
\label{sec:related}

We review related work on the estimation of the SMPL parameters from images or videos as well as the use of synthetic data for human pose and shape estimation.

\paragraph{SMPL from images.}
SMPLify~\cite{smplify} first introduced an optimization-based method to find the SMPL parameters that best explain a set of detected 2D keypoints by leveraging various priors, but it remains sensible to the initialization.
Since then, most deep learning methods~\cite{nbf,hmr,spin,graphcmr,pose2mesh} process image crops around a person of interest to directly estimate these SMPL parameters. 
In order to handle real-world images, they are trained on a mix of real images where annotations are limited to 2D keypoints, and synthetic or MoCap images for which 3D poses are available.
Losses, typically applied on the difference between SMPL 2D or 3D keypoint predictions and the corresponding annotations, can also be applied on the vertices~\cite{spin,graphcmr} or on texture correspondences~\cite{dct,texturepose}.
In some works~\cite{hmr,texturepose}, an additional discriminator ensures the realism of the predicted SMPL parameters.
SPIN~\cite{spin} combines deep learning-based method with optimization-based approaches by using the optimization stage to refine the prediction made by the network, which is later used in upcoming epochs.
In I2L-MeshNet~\cite{i2lmeshnet}, vertices' locations are estimated with heatmaps for each mesh vertex coordinate instead of directly regressing the parameters while in Pose2Mesh~\cite{pose2mesh}, a 2D pose is first lifted to 3D to obtain a coarse mesh which is then iteratively refined. Both methods are trained on images where SMPL pseudo-ground-truth was obtained from 2D keypoints using SMPLify~\cite{smplify}, which can result in inaccurate 3D data. Instead, we employ the final estimations in the SPIN training procedure to train with direct supervision and fully exploit synthetic data.


\paragraph{SMPL from videos.}
While Arnab \etal~\cite{videokinetics} proposed an optimization-based strategy to handle human pose estimation in videos, recent methods are mostly based on deep learning~\cite{vibe,thmmr,Jiang_2021_CVPR}.
In HMMR~\cite{hmmr}, features from consecutive frames are fed to a 1D temporal convolution, while VIBE~\cite{vibe} uses recurrent neural network, namely Gated Recurrent Unit (GRU), together with a discriminator at the sequence level.
The network is trained on different in-the-wild videos and losses are similar to the ones employed for images and described above, \ie, mainly applied on keypoints.
A similar architecture with GRU is used in TCMR~\cite{tcmr}, except that 3 independent GRUs are used and concatenated, one in each direction and one bi-directional in order to better leverage temporal information.
MEVA~\cite{meva} estimates motion from videos by  also extracting temporal features using GRUs and then estimates the overall coarse motion inside the video with Variational Motion Estimator (VME).
Recently, Pavlakos \etal~\cite{thmmr} have proposed to use a transformer architecture~\cite{transformer} in a concurrent work.
To obtain training data, \ie, in-the-wild videos annotated with 3D mesh information, they use the smoothness of the SMPL parameters over consecutive frames to obtain pseudo-ground-truth. In terms of architecture, the transformer is used to leverage temporal information by modifying the features.
We also consider a transformer architecture but, in our case, it is directly applied to the sequences of SMPL parameters. It has the great advantage of being directly trainable on MoCap data and pluggable to any image-based method.
 

\paragraph{Learning with synthetic data.}
Employing synthetic training images is a standard strategy to overcome the lack of large scale annotated data in computer vision~\cite{GaidonLP18}. This is particularly the case for human 3D pose estimation as it is not possible to accurately annotate 3D information on a large corpus of in-the-wild images~\cite{ChenWLSWTLCC16,RogezS16,surreal}. Usually, a sim2real domain  gap arises  between  synthetic  and real examples that needs to be carefully handled to ensure generalization.  For example, Chen \etal~\cite{ChenWLSWTLCC16} employed computer generated images of people to train a 3D pose estimation network with a domain mixer adversarially trained to discriminate between real and synthetic images, and therefore handle domain adaptation. 
More recently, SimPose~\cite{simpose} proposed to train a model for 2.5D pose estimation using a mixed of synthetic and real data where 2D losses are applied to real data while 2.5D and 3D losses are applied to synthetic data only, which may affect generalization.
Kundu \etal~\cite{kundu2020unsupervised} proposed to estimate an auto-encoder of 3D pose, together with a network that processes real images and output 2D poses. Losses are also applied on the interleaved network to allow training a model to estimate 3D pose in the wild. However, this does not allow the image feature extractor to see more variability in terms of poses. Closer to us is the work by Varol \etal~\cite{surreal} who apply real textures on renderings of the SMPL parametric model completed with real background images to generate a large and varied dataset. In a subsequent work, they trained a network for volumetric body estimation~\cite{bodynet} by pretraining on this large synthetic data and fine-tuning on more limited real data.
Doesh \etal~\cite{sim2real} show that motion through the optical flow in synthetic videos from~\cite{surreal} allows to bridge the sim2real gap for 3D human pose estimation.
In this paper, we also leverage SMPL renderings but fully exploit the synthetic data, \ie, without limiting its use to pretraining.




 \section{MoCap data for human mesh recovery}
\label{sec:method}





In this section we present two ways of using MoCap data for 3D mesh estimation in images and videos.
We first explore MoCap data for regularizing image-based models via fine-tuning using synthetic renderings (Section~\ref{sec:method_fine-tuning}). 
We then introduce \posebert, a pose estimation-oriented transformer architecture that is trained on MoCap data and enables the model to leverage temporal context (Section~\ref{sec:method_temporal}).


\subsection{Improving image-based pose estimation}
\label{sec:method_fine-tuning}



Existing datasets with ground-truth pose annotations for real-world images are not large enough to capture the variability of images/video sequences that can be encountered at test time; this can limit the generalization capabilities of a pose estimation method. We argue that one way to mitigate this issue is through the use of renderings from MoCap data. We leverage such synthetic data to regularize a strong image-based model like SPIN~\cite{spin}. SPIN is a 3D pose estimation model pretrained on datasets with paired RGB and 2D and/or 3D pose annotations. We propose to fine-tune this model using synthetic renderings and therefore expose it to a more diverse set of poses, viewpoints, textures and background changes from the ones seen during pretraining.  



Following~\cite{hmr,spin}, we use a deep convolutional neural network encoder  for feature extraction from image crops followed by an iterative regressor . Let  be an input crop,  be the extracted feature and 
the regressed prediction for the -dimensional body model parameters  and the -dimensional camera parameters . The regressor  is  initialized with the mean pose parameters  and is run for a number of  iterations. Similar to~\cite{spin}, our network uses the 6D representation proposed in~\cite{rotation} for 3D joints orientation.


\begin{figure}[t]
\centering
\includegraphics[width=.95\linewidth]{fig/finetune_amass.pdf} \-0.3cm]
\caption{\label{fig:transformer}\textbf{The \posebert architecture}. Both input and output are SMPL pose parameters for a temporal sequence of  poses. \smpltransformer basic block is repeated  times. The regressor parameters are shared across the  inputs and the  blocks. We regress the pose starting from the mean pose.
}
\vspace{-0.5cm}
\end{figure}



In this section, we present \textbf{\smpltransformer}, a transformer-based architecture that regresses \emph{directly} the SMPL parameters for every pose in a temporal sequence. Although concurrent works have also utilized temporal models based on GRUs~\cite{vibe,meva,tcmr} or transformers~\cite{thmmr} for human mesh recovery, they utilize context to enhance the visual features, and still require an iterative regressor on top of the temporal model for regressing the SMPL parameters. 

\paragraph{The basic \smpltransformer block.}
Figure~\ref{fig:transformer} illustrates the architecture of \smpltransformer. The input is  vectors, corresponding to a temporal sequence of poses of length  and the output is a sequence of SMPL pose parameters predictions, one for each frame. 
We propose to incorporate the regressor as part of the basic transformer block: we replace the feed-forward network that follows the multi-headed dot-product attention with a regressor , similar to the one used in~\cite{hmr}. This gives us a main advantage: at every layer of \smpltransformer, we dynamically attend to parts of the input sequence and refine the regressed SMPL pose prediction accordingly.

Since we are learning from synthetic data, pose ground-truth is available. 
Despite the fact that the transformer only sees ground-truth poses during training, as we experimentally show in Section~\ref{sec:experiments}, the learned model can generalize to noisy poses during testing, \ie, when the input is instead the pose predictions from any image-based model. 


The basic block of \smpltransformer is repeated  times. 
Inside each block, the input is first fed to a multi-head scaled dot-product attention mechanism and then to a regressor MLP. For the latter we use the architecture from~\cite{hmr} and share the regressor parameters across the  inputs.  Although the regressor can be iterative as in~\cite{hmr}, given that this is a process that happens at every layer, \ie,  times, we find that one iteration is enough (see ablation in Section~\ref{sec:experiments}). Moreover, we experiment with versions of \smpltransformer where the regressor parameters are shared across the  blocks, thus reducing the number of learnable parameters.

Like the original transformer~\cite{transformer}, we use layer normalization before self-attention modules. For the first layer only, and similarly to~\cite{transformer}, we add a 1-D positional encoding to the input which is learned from scratch. We also initialize the first layer regressor with the mean pose . Finally, we first learn a linear projection to  dimensions before we feed the input to the transformer; the choice of this parameter directly influences the size of the model.



\paragraph{Learning \smpltransformer parameters.}
Looking at context can help to correct errors of models based on single images. By inputting a sequence of poses to \poseformer,  we want to be able to learn \emph{temporal} dynamics. To do so, we utilize the masked modeling task, one of the self-supervised tasks that the now ubiquitous BERT~\cite{bert} model uses to learn language models. We also add random Gaussian noise to the input.
Figure~\ref{fig:temporal} shows an overview of the training process. The inputs are first masked and noise is added. This is then given to  \poseformer as an input. \poseformer outputs are then compared to the original clean inputs and similarly to the image-based model; for every timestep there is a loss on the SMPL pose parameters as well as a 3D keypoint loss.

In masked modeling, part of the input is masked before it is fed to the model. The correct output is expected to be recovered using the rest of the sequence.
In practice, and for each input sequence, we create a -dimensional random binary vector, where each timestamp has a probability  to be masked, where  is an hyperparameter that controls the number of frames of the input that will be masked.
For the  timesteps that are randomly chosen to be masked, we replace the input pose with a learnable masked token and prohibit the self-attention module to attend to those timesteps.
We ablate  in the experiments. 




\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/posebert_training_poses.pdf} \-0.4cm]
\vspace{-0.15cm}
\caption{\textbf{Varying the percentage of synthetic data}. We report the MPJPE metric on four datasets when varying the percentage of AMASS rendering at each batch. We further plot the  average across the four datasets (dashed black curve); the best performance overall is achieved when the batch is equally split between real and synthetic data.}
\label{fig:mocap_percentage}
\vspace{-0.6cm}
\end{figure}
 
\paragraph{Ablating the balance between real and synthetic data in each batch.} 
We study the impact of varying the percentage of synthetic renderings inside each batch. Results are shown in Figure~\ref{fig:mocap_percentage}.
On all datasets, we observe a U shape curve as the percentage of synthetic data increases, meaning that the optimal value is not an extreme value (neither only real data nor only synthetic). Using half of synthetic data is a good compromise on all datasets.
 
\paragraph{Analysis of the performance gain.}
To better understand in which case our proposed \sspin improves performance, we plot in Figure~\ref{fig:gain} (left) an histogram of the gain compared to the original SPIN model on 3DPW in terms of MPJPE. We observe that most samples are improved by a few millimeters. The overall shape of the histogram has a shape similar to a Gaussian, meaning that some images have actually a higher error, and a few samples are significantly improved. The histogram on the right in Figure~\ref{fig:gain} shows the impact of each bin on the final MPJPE metric.

\begin{figure}
\centering
\includegraphics[width=0.5\columnwidth]{fig/gains_3dpw_analysis/counts.pdf}~
\includegraphics[width=0.5\columnwidth]{fig/gains_3dpw_analysis/gains.pdf} \
\begin{split}
s_{\text{MSE}}(\phi_\vx, \phi_\vxsynth)  & = - || \phi_\vx - \phi_\vxsynth ||^2_2 \\
s_{\text{cosine}}(\phi_\vx, \phi_\vxsynth)  & =  \dfrac{\phi_\vx \cdot \phi_\vxsynth}{\max(\Vert \phi_\vx \Vert _2 \cdot \Vert \phi_\vxsynth \Vert _2, \epsilon)} \\
s_{\text{contr}}(\phi_\vx, \phi_\vxsynth) & = - \log \frac{\exp(\phi_\vx^T \phi_\vxsynth)}{\sum_{\vy \in \mathcal{N}} \exp(\phi^T_{\vx} \phi_\vy)} ,
\end{split}

where  is the set of all synthetic images in the batch. For the contrastive loss case, we first -2 normalize the features and  symmetrise the loss by  setting . The contrastive loss is essentially a softmax function bringing the correct real-synth features closer to each other and further from all other features from synthetic images in the same batch.

We report results in Table~\ref{tab:single_image_feature_loss}. For each loss type, we conduct an hyperparameter search to select the best weight . Using the L2 or cosine losses to align the feature do not yield an improvement. We view this as a valuable negative result. Aligning features with the contrastive loss yields a clear quantitative improvement on 3DPW, MPI-INF-3D and MCB and outperforms \sspin by 4.2mm of MPJPE on AIST. We explored combining this experiment with \sspin \ie using additional data from amass \textit{and} aligning paired samples with a contrastive loss, but  did not observe performance gains compared to \sspin. 

\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc|cc|cc|cc|c}
\toprule
      & \multicolumn{3}{c|}{3DPW~\cite{3dpw}} & \multicolumn{2}{c|}{ MuPoTS-3D~\cite{mupots} } & \multicolumn{2}{c|}{ AIST~\cite{aist} } & \multicolumn{2}{c|}{ MPI-INF-3D~\cite{mpiinf} } & MCB~\cite{mc3dv} \\
      & MPJPE  &   & MPVPE  & MPJPE  &   & MPJPE  &   & MPJPE  &   & MPJPE  \\
    \midrule
     \textbf{SPIN~\cite{spin}} & 97.2 & 59.6 & 116.8 & 154.6 & 83.0 & 119.4 & 71.8 & 104.3 & 68.0  & 155.4 \\
    \midrule
     \textbf{Using paired renderings of SPIN data} & & & & & \\
    \textit{~~~~~~ + pose losses on rendered images} & 93.5 & 58.6 & 109.7 &{152.2} & {\bf 82.1} & 117.4 & {\bf 71.8} & 98.0 & 67.4 & 150.0 \\
    \textit{~~~~~~~~~~~~  +  L2 loss}  & 93.4  & 58.7  & 109.3 & {\bf 152.1} & {\bf 82.1} & 119.2 & 72.7 & 98.5 & 67.4 & 149.5 \\    
    \textit{~~~~~~~~~~~~  +  cosine loss}  & 93.5 & 58.7  & 109.6 & 152.4 & 82.2 & 119.3 & 72.8 & 98.4 & 67.5 & 149.9 \\    
    \textit{~~~~~~~~~~~~  +  contrastive loss}  & {\bf 92.7}  & {\bf 58.0}  & {\bf 106.1} & 152.5 & {\bf 82.1} & \textbf{116.6} & 72.5 & \textbf{97.3} & {\bf 66.8} & {\bf 146.7}\\    \midrule
     \textbf{Using renderings of MoCap data} & & & & & \\
     \sspin & \textbf{90.8} & \textbf{55.6} & \textbf{105.0} & 152.3 & \textbf{81.0} & 120.8 &  {71.6} & 100.8 & 66.7 & \textbf{145.0} \\
\bottomrule
\end{tabular}
}
\caption{
{
\textbf{Additional experiments with paired real/synthetic data.}  For brevity, we denote the PA-MPJPE metric as  . We experiment with different losses to align features between a real image and its synthetic rendering using a pseudo ground-truth. We conducted a hyperparameter search for each type of feature alignment loss. We weight by   the L2 and cosine loss, and by  the contrastive loss.}
}
\label{tab:single_image_feature_loss}
\end{table*}





\subsection{Variance of \sspin performance.}

We fine-tune \sspin 10 times with different random seeds and report the result in Table~\ref{tab:Mocap_spin_seed}. We observe that our fine-tuning strategy gives consistent and stable improvements over SPIN in each case.

\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc|cc|cc|cc}
\toprule
      & \multicolumn{3}{c|}{3DPW~\cite{3dpw}} & \multicolumn{2}{c|}{ MuPoTS-3D~\cite{mupots} } & \multicolumn{2}{c|}{ AIST~\cite{aist} } & \multicolumn{2}{c|}{ MPI-INF-3D~\cite{mpiinf} }  \\
      \sspin & MPJPE  &   & MPVPE  & MPJPE  &   & MPJPE  &   & MPJPE  &    \\
    \midrule
Mean & 91.1 & 55.4 & 105.2 & 152.2 & 81.0 & 120.9 & 71.5 & 100.5 & 66.5 \\
Min & 90.2 & 55.2 & 104.3 & 151.8 & 80.8 & 120.2 & 71.3 & 100.0 & 66.4 \\
Max & 92.2 & 55.5 & 106.2 & 152.6 & 81.2 & 121.3 & 71.6 & 100.9 & 66.7 \\
Median & 91.1 & 55.4 & 105.1 & 152.2 & 81.0 & 120.9 & 71.4 & 100.5 & 66.5 \\
Std & 0.68 & 0.08 & 0.67 & 0.21 & 0.11 & 0.36 & 0.09 & 0.26 & 0.07 \\
\bottomrule
\end{tabular}
}
\caption{
{
\textbf{Variance of \sspin}. We fine-tune SPIN with 10 difference random seed and report the mean, min , max, median and standard deviation of each metric. We conclude that our experiments are stable.}
}
\label{tab:Mocap_spin_seed}
\end{table}


\subsection{Implementation details for \sspin}
\label{appsub:image_details}

Our codebase is based on the official Pytorch~\cite{pytorch} SPIN release: \url{https://github.com/nkolot/SPIN}. In particular, we use the same set of pose losses with the same weights.
For \sspin, in addition to updating the  running statistics, we train the  parameters of the batch-norm affine layers out of the  SPIN trainable parameters (0.2\%).
We fine-tune for 14k iterations using the Adam optimizer~\cite{adam}, with batches of size 64, with a learning rate of , divided by 10 after 10k and 12k iterations. Training takes about 10 hours on a NVIDIA V100 graphics card. The speed bottleneck is on the CPU side and speed could be improved with more dataloader workers.

\section{Additional results for \posebert}
\label{app:posebert}

In this section, we first study the impact on finetuning \posebert on real-world training data (Section~\ref{appsub:finetune}) as well as the impact of other architectures and training strategies (Section~\ref{appsub:training}).

\subsection{Fine-tuning \posebert on real-world data}
\label{appsub:finetune}

While we propose to train \posebert from scratch using MoCap data only, we also study the impact of fine-tuning on real-world training sets from several datasets, see results in Table~\ref{tab:posebert_finetuning}.
We observe that finetuning leads to a boost on the associated test sets but may decrease performances on other test sets.
For example, finetuning on the MPI-INF-3DPH training set improves the results on MPI-INF-3DPH and MuPoTs-3DPH respectively by 3.3 and 0.5 mm but it decreases the performance of \posebert on 3DPW by 1 mm.
This can be explained by the domain shift between these specific training sets.

\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
 & 3DPW & MPI-INF-3DHP & MuPoTS-3D & AIST \\
\midrule
\sspin & 55.6 & 66.7	& 81.0 & 71.6 \\ 
\midrule
\sspin + \bf{\posebert} & 53.2 & 63.8 & 80.3 & 69.7 \\
~~~~~~~~ \textit{+ finetuning on MPI-INF-3DPH} & 53.9 &  \bf{60.5} & \bf{79.8} & 71.5 \\
~~~~~~~~ \textit{+ finetuning on 3DPW} & \bf{52.9} &  63.8 & 79.9 & 70.9  \\
~~~~~~~~ \textit{+ finetuning on AIST} & 53.9  & 65.6 & 80.9 & \bf{69.5} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Fine-tuning on real data.} We study the impact of fine-tuning \posebert on different training sets. We reported the PA-MPJPE on different datasets.}
\label{tab:posebert_finetuning}
\end{table}




\subsection{\posebert: other training strategies}
\label{appsub:training}

In addition to masking and adding Gaussian noise on the input, we have also investigated other training strategies as reported in Table~\ref{tab:posebert_ablation_bis}.
First we compare against the common practice of having the iterative regressor \cite{thmmr} on top of the temporal module.
\posebert shows a gain ranging from 1.4 mm to 0.4mm compare to the baseline described above.
Then we increase the temporal window of the input sequence by reducing the frames per second while keeping the sequence length fixed.
We observe that increasing the time span does not bring significant improvement and even leads to decrease performances.
We also study the impact of incorporating random poses or joints compared to random Gaussian noise as proposed in the main paper.
We note that both noise types bring a small improvement compared to Gaussian noise but for simplicity we do not include them during the training scheme of our best model.




\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
 & 3DPW & MPI-INF-3DHP & MuPoTS-3D \\
\midrule
\sspin & 55.6 & 66.7	& 81.0 \\ 
\midrule
\sspin + Transformer + Regressor & 54.5 & 65.2 & 80.7 \\
\midrule
\sspin + \bf{\posebert} & \bf{53.2} & 63.8 & \bf{80.3} \\
~~~~fps=7.5 & 54.0 & 64.5 & 80.5 \\
~~~~fps=15 & 53.4 & \bf{63.4} & 80.4 \\
~~~~fps=3.75 & 55.0 & 66.0 & 81.0 \\
\midrule
~~~~Random poses ~~5\% & 53.2 & \bf{63.5} & \bf{80.0} \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 10\%  & \bf{53.2} & 63.7 & \bf{80.0} \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 20\% & \bf{53.2} & 64.3 &  80.2 \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 40\%  & 53.6 & 64.5 &  80.4\\
\midrule 
~~~~Random joints ~~5\% & \bf{53.2} & \bf{62.7} & \bf{80.1} \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 10\%  & 53.4 & 63.6 & 80.2 \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 20\% & 53.3 & 64.9 & 81.0 \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 40\%  & 55.8 & 70.0 & 83.0 \\
\bottomrule
\end{tabular}
} \caption{\textbf{Additional ablation on the \posebert hyperparameters.} We first study the impact of having the regressor incorporated into the transformer. We also study the impact of the frame per second (fps, 30 by default) and the percentage of random poses/joints (0 by default) with the PA-MPJPE metric on 3DPW, MPI-INF-3DHP and MuPoTS-3D when using \posebert on top of \sspin, with masking 12.5\% of the input sequences (2 frames with T=16 frames) and using a model of size  and . 
}
\label{tab:posebert_ablation_bis}
\end{table}

\subsection{Motivation and details on adding Gaussian noise}
We see this as a form of data augmentation for the axis-angle pose representation. We utilized histograms of axis-angle errors in radians shown in Figure \ref{fig:error_gaussian}
to estimate the standard deviation of the noise to be added (we used 0.10 to cover the error distribution) and simply sample a high-dimensional noise vector and add it to the ground truth input.

\setlength{\columnsep}{10pt}\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/hist_spin_errors.pdf}  
    \caption{\textbf{Histogram of SPIN axis-angle errors.} On 3DPW train set, in radians.}
    \label{fig:error_gaussian}
\end{figure}

\begin{figure*}
\centering
\includegraphics{fig/teaser_comparison_supplementary.pdf}
\caption{\label{fig:extended_teaser}
\textbf{Qualitative comparison of image-based (left) and video-based (right) pose estimation methods} (extension of paper's Figure 1). Note the left/right legs flipping produced by SPIN, and how PoseBERT improves predictions by leveraging temporal information and MoCap-based motion priors.
}
\end{figure*}

\subsection{Comparison on the use of 3DPW-train}

In the main paper we follow the standard evaluation protocol of 3DPW \cite{3dpw} which does not allow using 3DPW-train for training.
However we also provide in Table \ref{tab:3dpw_train} a comparison against methods using 3DPW-train for training.
We do not get any improvement by incorporating 3DPW into our training set but we still get results on part with other methods.

\begin{table}[h]
\vspace{-0.1cm}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Method} & Train set for & \multirow{2}{*}{Conf} & \multirow{2}{*}{3DPW} & MPI- \\
 & \textbf{the temporal module} & & &INF-3DHP\\
\midrule 
SPIN     & - & ICCV'19 & 59.2 & 67.5 \\
\bf{\sspin} & - & (ours) & 55.6 & 66.7 \\
\midrule
SPIN + VIBE & 2/3D & CVPR'20 & 56.5 & 63.4 \\
SPIN + MEVA & 2/3D+AMASS\textbf{+3DPW} & ACCV'20 & 54.7 & 65.4 \\
SPIN + TCMR & 2/3D & CVPR'21 & 55.8 & 62.8 \\
SPIN + TCMR & 2/3D\textbf{+3DPW} & CVPR'21 & 52.7 & 63.5 \\
\bf{SPIN + \posebert} & only synthetic (AMASS) & (ours) & 57.6 & 64.3 \\
\bf{\sspin + \posebert} & only synthetic (AMASS) & (ours) & 52.9 & 63.3 \\
\bf{\sspin + \posebert} & AMASS+\textbf{3DPW} & (ours) & 52.9 & 63.3 \\
\bottomrule
\end{tabular}
}
\caption{
\textbf{Comparison with state-of-the-art video models.}
"2D/3D" corresponds to the mix of MPI-INF-3DHP/PennAction/PoseTrack/H36M datasets.
}
\label{tab:3dpw_train}
\end{table}

\subsection{PoseBERT as a plug-and-play module}
Here, we further present results when running PoseBERT on top of the more recent ROMP method \cite{ROMP}.
Adding PoseBERT on top, we get a decrease of MPJPE (resp. PA-MPJPE) from 91.1 (resp. 56.5) to 90.2 (resp. 55.3) for 3DPW (using the cropping strategy of SPIN).
Finally, to test it beyond SMPL inputs, we appended PoseBERT on top of 2D/3D pose predictions from LCRNet++ \cite{lcrnet++} and see significant gains on 3DPW, a decrease of MPJPE (resp. PA-MPJPE) from 125.8 (resp. 68.8) to 108.2 (resp. 58.5).
We believe that these additional results further strengthen our argument that PoseBERT is a robust, plug-and-play 
module
for temporal modeling.

 
\end{document}
