
\section{Experimental Results}

\subsection{Datasets}
\label{sec:datasets}
\noindent We run experiments on four large-scale image quality datasets including three technical quality datasets (PaQ-2-PiQ \cite{ying2020patches}, SPAQ \cite{fang2020perceptual},  KonIQ-10k \cite{hosu2020koniq}) and one aesthetics quality dataset (AVA~\cite{murray2012ava}).

PaQ-2-PiQ is the largest picture technical quality dataset by far which contains 40k real-world images and 120k cropped patches. Each image or patch is associated with a MOS. Since our model does not make a distinction between image and extracted patches, we simply use all the 30k full-size images and the corresponding 90k patches from the training split to train the model. We then run the evaluation on the 7.7k full-size validation and 1.8k test set.

SPAQ dataset consists of 11k pictures taken by 66 smartphones. For a fair comparison, we follow \cite{fang2020perceptual} to resize the raw images such that the shorter side is 512. We only use the image and its corresponding MOS for training, not including the extra tag information in the dataset.

KonIQ-10k contains 10k images selected from a large public multimedia database YFCC100M~\cite{thomee2016yfcc100m}.

AVA is an image aesthetic assessment dataset. It contains 250k images with 10-scale score distribution for each.

For KonIQ-10k, we follow \cite{zhu2020metaiqa, su2020blindly} to randomly sample 80\% images for each run and report the results on the remaining 20\%. For other datasets, we use the same split as the previous literature.

\subsection{Implementation Details}
\noindent For \ours, the multi-scale representation is constructed as the native resolution image and two ARP resized input ($L_1=224$ and $L_2=384$) by default. It therefore uses 3-scale input. Our method also works on 1-scale input using just the full-size image without resized variants. We report the results of this single-scale setting as \fullours.

We use patch size $P = 32$. The dimensions for Transformer input tokens are $D=384$, which is also the dimension for pixel patch embedding, HSE and SCE. The grid size of HSE is set to $G=10$. We use the classic Transformer \cite{NIPS2017_3f5ee243} with lightweight parameters (384 hidden size, 14 layers, 1152 MLP size and 6 heads) to make the model size comparable to ResNet-50. The final model has around 27 million total parameters.

We pre-train our models on ImageNet for 300 epochs, using Adam with $\beta_1=0.9, \beta_2 =0.999$, a batch size of $4096$, $0.1$ weight decay and cosine learning rate decay from $0.001$. We set the maximum number of patches from full-size image $l$ to 512 in training. For fine-tuning, we use SGD with momentum and cosine learning rate decay from $0.0002, 0.0001, 0.0001, 0.12$ for 10, 30, 30, 20 epochs on PaQ-2-PiQ, KonIQ-10k, SPAQ, and AVA, respectively. Batch size is set to 512 for AVA, 96 for KonIQ-10k, and 128 for the rest. For AVA, we use the EMD loss with $r=2$. For other datasets, we use the $L1$ loss.

The models are trained on TPUv3. All the results from our method are averaged across 10 runs. Spearman rank ordered correlation (SRCC), Pearson linear correlation (PLCC), and the standard deviation (std) are reported.

\subsection{Comparing with the State-of-the-art (SOTA)}
\noindent\textbf{Results on PaQ-2-PiQ.} Table \ref{tab:paq2piq-results} shows the results on the PaQ-2-PiQ dataset. Our proposed \ours\ outperforms other methods on both the validation and test sets. Notably, the test set is entirely composed of pictures having at least one dimension exceeding 640 \cite{ying2020patches}. This is very challenging for traditional deep learning approaches where resizing is inevitable. Our method is able to outperform previous methods by a large margin on the full-size test set which verifies its robustness and effectiveness. 

\begin{table}[!tp]
\begin{center}
\footnotesize
\begin{tabular}{lccccc}\toprule
&\multicolumn{2}{c}{Validation Set} &\multicolumn{2}{c}{Test Set} \\\cmidrule{2-5}
method &SRCC &PLCC &SRCC &PLCC \\\midrule
BRISQUE \cite{mittal2012no} &0.303 &0.341 &0.288 &0.373 \\
NIQE \cite{mittal2012making} &0.094 &0.131 &0.211 &0.288 \\
CNNIQA \cite{kang2014convolutional} &0.259 &0.242 &0.266 &0.223 \\
NIMA \cite{talebi2018nima} &0.521 &0.609 &0.583 &0.639 \\
Ying \etal \cite{ying2020patches} &0.562 &0.649 &0.601 &0.685 \\\midrule
\fullours  &\second{0.563} &\second{0.651} &\second{0.640} &\second{0.721} \\
\ours\ (Ours) &\best{0.566} &\best{0.661} &\best{0.646} &\best{0.739} \\
std &$\pm 0.002$ &$\pm 0.003$ &$\pm 0.005$ &$\pm 0.006$ \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{Results on PaQ-2-PiQ full-size validation and test sets. Blue and black numbers in bold represent the best and second best respectively. We take numbers from \cite{ying2020patches} for the results of the reference methods.} \label{tab:paq2piq-results}
\vspace{-2mm}
\end{table}

\begin{table}[!tp]
\begin{center}
\footnotesize
\begin{tabular}{lccc}\toprule
method &SRCC &PLCC \\\midrule
BRISQUE \cite{mittal2012no} &0.665 &0.681 \\
ILNIQE \cite{zhang2015feature} &0.507 &0.523 \\
HOSA \cite{xu2016blind} &0.671 &0.694 \\
BIECON \cite{kim2016fully} &0.618 &0.651 \\
WaDIQaM \cite{bosse2017deep} &0.797 &0.805 \\
PQR \cite{zeng2017probabilistic} &0.880 &0.884 \\
SFA \cite{li2018has} &0.856 &0.872 \\
DBCNN \cite{zhang2018blind} &0.875 &0.884 \\
MetaIQA \cite{zhu2020metaiqa}  &0.850 &0.887 \\
BIQA \cite{su2020blindly} (\textbf{25} crops) &\second{0.906} &0.917 \\\midrule
\fullours &0.905 &\second{0.919} \\
\ours\ (Ours) &\best{0.916} &\best{0.928} \\
std &$\pm 0.002$ &$\pm 0.003$ \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{Results on KonIQ-10k dataset. Blue and black numbers in bold represent the best and second best respectively. We take numbers from \cite{su2020blindly, zhu2020metaiqa} for results of the reference methods.} \label{tab:koniq-results}
\vspace{-3mm}
\end{table}

\noindent\textbf{Results on KonIQ-10k.} Table \ref{tab:koniq-results} shows the results on the KonIQ-10k dataset. Our method outperforms the SOTA methods. In particular, BIQA \cite{su2020blindly} needs to sample 25 crops from each image during training and testing. This kind of multi-crops ensemble is a way to mitigate the fixed shape constraint in the CNN models. But since each crop is only a sub-view of the whole image, the ensemble is still an approximate approach. Moreover, it adds additional inference cost for every crop and sampling can introduce randomness in the result. Since \ours\ takes the full-size image as input, it can directly learn the best aggregation of information across the full image and only one evaluation is involved.


\noindent\textbf{Results on SPAQ.} Table \ref{tab:spaq-results} shows the results on the SPAQ dataset. Overall, our model is able to outperform other methods in terms of both SRCC and PLCC. 

\begin{table}[!tp]
\begin{center}
\footnotesize
\begin{tabular}{lrrr}\toprule
method &SRCC &PLCC \\\midrule
DIIVINE \cite{moorthy2011blind} &0.599 &0.600 \\
BRISQUE \cite{mittal2012no} &0.809 &0.817 \\
CORNIA \cite{ye2012unsupervised} &0.709 &0.725 \\
QAC \cite{xue2013learning} &0.092 &0.497 \\
ILNIQE \cite{zhang2015feature} &0.713 &0.721 \\
FRIQUEE \cite{ghadiyaram2017perceptual} &0.819 &0.830 \\
DBCNN \cite{zhang2018blind} &0.911 &0.915 \\
Fang \etal \cite{fang2020perceptual} (w/o extra info) &0.908 &0.909 \\\midrule
\fullours &\best{0.917} &\second{0.920} \\
\ours\ (Ours) &\best{0.917} &\best{0.921} \\
std &$\pm 0.002$ &$\pm 0.002$ \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{Results on SPAQ dataset. Blue and black numbers in bold  represent the best and second best respectively. We take numbers from \cite{fang2020perceptual} for results of the reference methods.}\label{tab:spaq-results}
\vspace{-2mm}
\end{table}

\begin{table}[!tp]
\footnotesize
\begin{center}
\setlength\tabcolsep{2.5pt}
\begin{tabular}{lcccc}
\toprule
method & cls. acc. & MSE $\downarrow$ & SRCC & PLCC \\\midrule
MNA-CNN-Scene \cite{mai2016composition} & 0.765 & - & - & - \\
Kong \etal \cite{kong2016photo} & 0.773 & - & 0.558 & - \\
AMP \cite{murray2017deep} & 0.803 & 0.279 & 0.709 & - \\
A-Lamp \cite{ma2017lamp} (\textbf{50} crops) & 0.825 & - & - & - \\
NIMA (VGG16) \cite{talebi2018nima} & 0.806 & - & 0.592 & 0.610 \\
NIMA (Inception-v2) \cite{talebi2018nima} & 0.815 & - & 0.612 & 0.636 \\
$MP_{ada}$ \cite{sheng2018attention} ( $\ge$ \textbf{32} crops) & 0.830 & - & - & - \\
Zeng \etal (ResNet101) \cite{zeng2019unified} & 0.808 & 0.275  & 0.719 & 0.720 \\
Hosu \etal \cite{hosu2019effective} (\textbf{20} crops) & 0.817 & - & \best{0.756} & \best{0.757} \\
AFDC + SPP (single warp) \cite{chen2020adaptive} & \second{0.830} & 0.273 & 0.648 & - \\
AFDC + SPP (\textbf{4} warps) \cite{chen2020adaptive} & \best{0.832} &0.271 & 0.649 & 0.671 \\\midrule
\fullours &0.814 &\second{0.247} &0.719 &0.731 \\
\ours\ (Ours) & 0.815 & \best{0.242} & \second{0.726} & \second{0.738} \\
std &$\pm 0.121$ &$\pm 0.001$ &$\pm 0.001$ &$\pm 0.001$ \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{Results on AVA dataset. Blue and black numbers in bold represent the best and second best respectively. cls. acc. stands for classification accuracy. MSE stands for mean square error. We take numbers from \cite{chen2020adaptive} for results of the reference methods.}\label{tab:ava-results}
\vspace{-3mm}
\end{table}

\noindent\textbf{Results on AVA.} Table \ref{tab:ava-results} shows the results on the AVA dataset. Our method achieves the best MSE and has top SRCC and PLCC. As previously discussed, instead of multi-crops sampling, our model can accurately predict image aesthetics by directly looking at the full-size image.


\subsection{Ablation Studies}\label{sec:ablation-study}

\noindent\textbf{Importance of Aspect-Ratio-Preserving (ARP).} CNN-based IQA models usually resize the input image to a square resolution without preserving the original aspect ratio. We argue that such preprocessing can be detrimental to IQA because it alters the image composition. To verify that, we compare the performance of the proposed model with either square or ARP resizing. As shown in Table \ref{tab:arp-resizing}, ARP resizing performs better than square resizing, demonstrating the importance of ARP when assessing image quality.

\begin{table}[!tp]
\footnotesize
\begin{center}
\setlength\tabcolsep{1.0pt}
\begin{tabular}{lcccc}\toprule
method & \# Params &SRCC &PLCC \\\midrule
NIMA(Inception-v2) \cite{talebi2018nima} (224 square input) &56M &0.612 &0.636 \\
NIMA(ResNet50)* (384 square input) &24M &0.624 &0.632 \\\midrule
ViT-Base 32* (384 square input) \cite{dosovitskiy2020} &88M &0.654 &0.664 \\
ViT-Small 32* (384 square input) \cite{dosovitskiy2020} &22M &0.656 &0.665 \\\midrule
\ours\ w/ square resizing (512, 384, 224) &27M &0.706 &0.720 \\
\ours\ w/ ARP resizing (512, 384, 224) &27M &0.712 &0.726 \\
\ours\ w/ ARP resizing (full, 384, 224) &27M &\textbf{0.726} &\textbf{0.738} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-1mm}
\caption{Comparison of ARP resizing and square resizing on AVA dataset. * means our implementation. ViT-Small* is constructed by replacing the Transformer backbone in ViT with our 384-dim lightweight Transformer. The last group of rows show our method with different resizing methods. Numbers in the bracket show the resolution used in the multi-scale representation. }\label{tab:arp-resizing}\vspace{-3mm}
\end{table}

\begin{figure}[!tp]
\centering
\includegraphics[width=8.5cm]{images/butterfly_aspect_ratio.png}
\caption{Model predictions for an image resized to different aspect ratios. The blue curve shows \ours\ with ARP resizing. The green curve shows our model trained and evaluated with square input. Orange and red curves show the ViT and ResNet-50 with square input. \ours\ can detect quality degradation due to unnatural resizing while other methods are not sensitive.  }\vspace{-1mm}
\label{fig:butterfly_aspect_ratio} 
\end{figure} 

To intuitively understand the importance of keeping aspect ratios in IQA, we follow \cite{chen2020adaptive} to artificially resize the same image into different aspect ratios and run models to predict quality scores. Since aggressive resizing will cause image quality degradation, a good IQA model should give lower scores to such unnatural looking images. As shown in Figure~\ref{fig:butterfly_aspect_ratio}, {\ours} (blue curve) is discriminative to the change of aspect ratios while scores from the other ones trained with square resizing are not sensitive to the change. This shows that ARP resizing is important and \ours\ can effectively detect quality degradation due to resizing.


\vspace{+1.5mm}
\noindent\textbf{Effect of Full-size Input and the Multi-scale Input Composition.}
In Table \ref{tab:paq2piq-results} \ref{tab:koniq-results} \ref{tab:spaq-results} \ref{tab:ava-results}, we compare using only the full-size input (\fullours) and the multi-scale input (\ours). \fullours\ achieves promising results, showing the importance of preserving full-size input in IQA. The performance is further improved using multi-scale and the gain is larger on PaQ-2-PiQ and AVA because these two datasets have much more diverse resolutions than KonIQ-10k and SPAQ. This shows that multi-scale is important for effectively capturing quality information on real-world images with varying sizes.

We also vary the multi-scale composition and show in Table~\ref{tab:multi-scale-composition} that multi-scale consistently improves performance on top of single-scale models. The performance gain of multi-scale is more than a simple ensemble of individual scales because an average ensemble of individual scales actually under-performs using only the full-size image. Since \ours\ has full receptive field of the multi-scale input sequences, it can more effectively aggregate quality information across scales. 

\begin{table}[!tp]
\footnotesize
\begin{center}
\begin{tabular}{lrrr}\toprule
Multi-scale Composition &SRCC &PLCC \\\midrule
(224) &0.600 &0.667 \\
(384) &0.618 &0.695 \\
(512) &0.620 &0.691 \\
(384, 224) &0.620 &0.707 \\
(512, 384, 224) &0.629 &0.718 \\\midrule
(full) &0.640 &0.721 \\
(full, 224) &0.643 &0.726 \\
(full, 384) &0.642 &0.730 \\
(full, 384, 224) &\textbf{0.646} &\textbf{0.739} \\\midrule
Average ensemble of (full), (224), (384) &0.640 &0.710 \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{Comparison of multi-scale representation composition on PaQ-2-PiQ full-size test set. The multi-scale representation is composed of the resolutions shown in the brackets. Numbers in brackets indicate the longer side length $L$ for ARP resizing. "full" means full-size input image.  }\label{tab:multi-scale-composition}
\vspace{-3mm}
\end{table}

\begin{figure}[!tp]
\centering
\includegraphics[width=8cm]{images/attention_visualization.png}
\caption{Visualization of attention from the output tokens to the multi-scale representation (original resolution image and two ARP resized variants). Note that images here are resized to fit the grid, the model inputs are 3 different resolutions. The model is focusing on details in higher resolution image and on global area in lower resolution ones.}\vspace{-2mm}
\label{fig:attention_visualization} 
\end{figure}

To further verify that the model captures different information at different scales, we visualize the attention weights on each image in the multi-scale representation as Figure~\ref{fig:attention_visualization}. We observe that the model tends to focus on more detailed areas on full-size high-resolution images and on more global areas on the resized ones. This shows that the model learns to capture image quality at different granularities.


\begin{figure}[!tp]
\centering
\includegraphics[width=7cm]{images/hse_cosine_similarity_grid10.png}\vspace{-0.5mm}
\caption{Visualization of the grid of hash-based 2D spatial embedding with $G=10$. Each subplot ($i$, $j$) is of size $G\times G$, showing the cosine similarity between $T_{i,j}$ and every element in $T$. Visualizations for different $G$ are available in Appendix B.3.  }\vspace{-3mm}
\label{fig:hse_cosine_similarity_grid10} 
\end{figure}

\vspace{+1.5mm}
\noindent\textbf{Effectiveness of Proposed Hash-based Spatial Embedding (HSE) and Scale Embedding (SCE).} We run ablations on different ways to encode spatial information and scale information using positional embeddings. As shown in Table \ref{tab:spatial-emb}, there is a large gap between adding and not adding spatial embeddings. This aligns with the finding in \cite{dosovitskiy2020} that spatial embedding is crucial for injecting 2D image structure. To further verify the effectiveness of HSE, we try to add a fixed length spatial embedding as ViT \cite{dosovitskiy2020}. This is done by treating all input tokens as a fixed length sequence and assigning a learnable embedding for each position. The performance of this method is unsatisfactory compared to HSE because of two reasons: 1) the inputs are of different aspect ratios. So each patch in the sequence can come from a different location from the image. Fixed positional embedding fails to capture this change; 2) since each position is modeled independently, there is no cross-scale information, meaning that the model cannot locate spatially close patches from different scales in the multi-scale representation. Moreover, the method is inflexible because fixed length spatial embedding cannot be easily applied to the large images with more patches. On the contrary, HSE is meaningful under all conditions.


\begin{table}[!tp]
\footnotesize
\begin{center}
\begin{tabular}{cccc}\toprule
Spatial Embedding &SRCC &PLCC \\\midrule
w/o &0.704 &0.716 \\
Fixed-length (no HSE) &0.707 &0.722 \\
HSE &\textbf{0.726} &\textbf{0.738} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{Ablation study results for spatial embeddings on AVA. For "Fixed length (not HSE)", we consider the input as a fixed-length sequence and assign a learnable embedding for each position.}\label{tab:spatial-emb}
\vspace{-1mm}
\end{table}

\begin{table}[!tp]
\footnotesize
\begin{center}
\begin{tabular}{cccc}\toprule
Scale Embedding &SRCC &PLCC \\\midrule
w/o &0.717 &0.729 \\
w/ &\textbf{0.726} &\textbf{0.738} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{Ablation study results for scale embedding on AVA.}\label{tab:scale-emb}
\vspace{-2mm}
\end{table}

A visualization of the learned HSE cosine similarity is provided as Figure \ref{fig:hse_cosine_similarity_grid10}. As depicted, the HSE of spatially close locations are more similar (yellow color) and it corresponds well to the 2D structure. For example, the bottom HSEs are brightest at the bottom. This shows that HSE can effectively capture the 2D structure of the image.

In Table \ref{tab:scale-emb}, we show that adding SCE can further improve performance when compared with not adding SCE. This shows that SCE is helpful for the model to capture scale information independently of the spatial information.


\noindent\textbf{Choice of Patch Encoding Module.} We tried different designs for encoding the patch, including linear projection as \cite{dosovitskiy2020} and small numbers of convolutional layers. As shown in Table \ref{tab:patch-encoding}, using a simple convolution based patch encoding module can boost the performance. Adding more conv layers has diminishing returns and we find a 5-layer ResNet can provide satisfactory representation for the patch.



\begin{table}[!tp]
\footnotesize
\begin{center}
\begin{tabular}{lcccc}\toprule
&\# Params &SRCC &PLCC \\\midrule
Linear projection &22M &0.634 &0.714 \\
Simple Conv &23M &0.639 &0.726 \\
5-layer ResNet &27M &\textbf{0.646} &\textbf{0.739} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{Comparison of different patch encoding modules on PaQ-2-PiQ full-size test set. For simple conv, we use the root of ResNet (a 7x7 conv followed by a 3x3 conv). For 5-layer ResNet, we stack a residual block on top of Simple Conv.}\label{tab:patch-encoding}
\vspace{-3mm}
\end{table}
