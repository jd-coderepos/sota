

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{amsmath}

\usepackage{graphicx}
\usepackage{bm, upgreek}
\usepackage{amssymb}
\usepackage{array, booktabs}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{color}
\usepackage{soul}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{pifont}\usepackage{comment}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}



\crefformat{section}{\S#2#1#3} \crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\definecolor{brickred}{rgb}{0.8, 0.25, 0.33}
\definecolor{blue-green}{rgb}{0.0, 0.87, 0.87}
\definecolor{celestialblue}{rgb}{0.29, 0.59, 0.82}
\definecolor{cerulean}{rgb}{0.0, 0.48, 0.65}
\definecolor{ceruleanblue}{rgb}{0.16, 0.32, 0.75}


\newcommand{\Ours}{DensePhrases}
\newcommand{\ours}{DensePhrases}
\newcommand{\sect}[1]{Section \ref{#1}}
\newcommand{\conc}{\mathbin{\|}}
\newcommand{\squad}{SQuAD}
\newcommand{\retrievalonly}{retriever-only}
\newcommand{\readeronly}{reader-only}

\newcommand{\documentset}{\mathcal{D}}
\newcommand{\phraseset}{\mathcal{S}(\mathcal{D})}
\newcommand{\phraseinp}{\mathcal{S}(p)}
\newcommand{\wordset}{\mathcal{W}(\mathcal{D})}
\newcommand{\phrasedump}{\mf{H}}
\newcommand{\traincorpus}{\mathcal{C}}
\newcommand{\lm}{\mathcal{M}}

\newcommand\sys[1]{\textsc{#1}}
\newcommand\ti[1]{\textit{#1}}
\newcommand\ts[1]{\textsc{#1}}
\newcommand\tf[1]{\textbf{#1}}
\newcommand\ttt[1]{\texttt{#1}}
\newcommand\mf[1]{\mathbf{#1}}

\newcommand{\eu}{\mathrm{e}}
\newcommand{\su}{\mathrm{s}}

\newcommand{\am}{{\bm a}}
\newcommand{\dm}{{\bm d}}
\newcommand{\xm}{{\bm x}}
\newcommand{\sm}{{\bm s}}
\newcommand{\wm}{{\bm w}}
\newcommand{\qm}{{\bm q}}

\newcommand{\Ab}{\mathbf{A}}
\newcommand{\Db}{\mathbf{D}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\ab}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\cb}{\mathbf{c}}
\newcommand{\db}{\mathbf{d}}
\newcommand{\eb}{\mathbf{e}}
\newcommand{\fb}{\mathbf{f}}
\newcommand{\kb}{\mathbf{k}}
\newcommand{\lb}{\mathbf{l}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\ob}{\mathbf{o}}
\newcommand{\ib}{\mathbf{i}}
\newcommand{\pb}{\mathbf{p}}
\newcommand{\qb}{\mathbf{q}}
\newcommand{\rb}{\mathbf{r}}
\newcommand{\sbo}{\mathbf{s}}
\newcommand{\wb}{\mathbf{w}}
\newcommand{\Wb}{\mathbf{W}}
\newcommand{\ub}{\mathbf{u}}
\newcommand{\Ub}{\mathbf{U}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\zb}{\mathbf{z}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Kb}{\mathbf{K}}
\newcommand{\Lb}{\mathbf{L}}
\newcommand{\Sb}{\mathbf{S}}
\newcommand{\Vb}{\mathbf{V}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Yb}{\mathbf{Y}}
\newcommand{\Zb}{\mathbf{Z}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\alphab}{\mathbf{\alpha}}
\newcommand{\betab}{\mathbf{\beta}}
\newcommand{\psib}{\mathbf{\psi}}
\newcommand{\phib}{\mathbf{\phi}}
\newcommand{\gammab}{\mathbf{\gamma}}
\newcommand{\thetab}{\mathbf{\theta}}
\newcommand{\Thetab}{\mathbf{\Theta}}
\newcommand{\kappab}{\mathbf{\kappa}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\II}{\mathbb{I}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\gpu}[1]{\textcolor{brickred}{#1}}
\newcommand{\cpu}[1]{\textcolor{ceruleanblue}{#1}}
\newcommand{\smalltext}[1]{{\scriptstyle \text{TEXT}}(#1)}
\newcommand{\tinytext}[1]{{\scriptscriptstyle \text{TEXT}}(#1)}

\newcommand{\dist}{\mathbf{\phi}}

\newcommand{\draftonly}[1]{#1}


\newcommand{\draftcomment}[3]{\draftonly{\textcolor{#2}{{{[#1: #3]}}}}}
\newcommand{\todo}[1]{\draftcomment{TODO}{red}{#1}}
\newcommand{\jinhyuk}[1]{\draftcomment{Jinhyuk}{blue}{#1}}
\newcommand{\mujeen}[1]{\draftcomment{Mujeen}{brown}{#1}}
\newcommand{\danqi}[1]{\draftcomment{Danqi}{teal}{#1}}

 
\aclfinalcopy \def\aclpaperid{1323} 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Learning Dense Representations of Phrases at Scale}


\author{
  Jinhyuk Lee\Thanks{ Work partly done while visiting Princeton University.}\quad Mujeen Sung \quad Jaewoo Kang \quad Danqi Chen\\
  Korea University\quad Princeton University\\
  \texttt{\{jinhyuk\_lee,mujeensung,kangj\}@korea.ac.kr} \\
  \texttt{danqic@cs.princeton.edu} \\}

\date{}

\begin{document}
\maketitle



\begin{abstract}
Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference~\citep{seo2019real}.
However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches.
In this work, we show for the first time that we can learn \ti{dense representations of phrases} alone that achieve much stronger performance in open-domain QA.
We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods.
We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference.
On five popular open-domain QA datasets, our model \ti{DensePhrases} improves over previous phrase retrieval models by -- absolute accuracy and matches the performance of state-of-the-art retriever-reader models.
Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs.
Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing {DensePhrases} as a dense knowledge base for downstream tasks.\footnote{Our code is available at \url{https://github.com/princeton-nlp/DensePhrases}.}


\end{abstract}
 

\section{Introduction}
\label{sec:intro}

Open-domain question answering (QA) aims to provide answers to natural-language questions using a large text corpus~\citep{voorhees1999trec,ferrucci2010building,chen2020open}.
While a dominating approach is a two-stage retriever-reader approach~\citep{chen2017reading,lee2019latent,guu2020realm,karpukhin2020dense},
we focus on a recent new paradigm solely based on \textit{phrase retrieval}~\citep{seo2019real,lee2020contextualized}.
Phrase retrieval highlights the use of phrase representations and finds answers purely based on the similarity search in the vector space of phrases.\footnote{Following previous work~\citep{seo2018phrase}, `phrase' denotes any contiguous segment of text up to  words (including single words), which is not necessarily a linguistic phrase.}
Without relying on an expensive reader model for processing text passages, it has demonstrated great runtime efficiency at inference time.


Despite great promise, it remains a formidable challenge to build vector representations for every single phrase in a large corpus.
Since phrase representations are decomposed from question representations, they are inherently less expressive than cross-attention models~\cite{devlin2019bert}.
Moreover, the approach requires retrieving answers correctly out of {billions} of phrases (e.g.,  phrases in English Wikipedia), making the scale of the learning problem difficult.
Consequently, existing approaches heavily rely on sparse representations for locating relevant documents and paragraphs while still falling behind retriever-reader models~\citep{seo2019real,lee2020contextualized}.



\begin{table*}[t]
\label{table:open_qa_results}
\begin{center}
\centering
\resizebox{2.0\columnwidth}{!}{\begin{tabular}{llccccc}
\toprule
\multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Sparse?}} & {\textbf{Storage}} & {\textbf{\#Q/sec}} & \textbf{NQ}  & \textbf{SQuAD} \\
& & & \multicolumn{1}{c}{(GB)} & (\gpu{GPU}, \cpu{CPU}) & (Acc) & (Acc) \\
\midrule
\multirow{5}{*}{Retriever-Reader} & DrQA~\citep{chen2017reading} & \cmark & 26 & \gpu{1.8}, \cpu{0.6} & - & 29.8 \\
& BERTSerini~\citep{yang2019end} & \cmark & 21 & \gpu{2.0}, \cpu{0.4} & - & \tf{38.6} \\
& ORQA~\citep{lee2019latent} & \xmark & 18 & \gpu{8.6}, \cpu{1.2} & 33.3 & 20.2 \\
& REALM~\citep{guu2020realm} & \xmark & 18 & \gpu{8.4}, \cpu{1.2} & 40.4 & -\\
& DPR-multi~\citep{karpukhin2020dense}& \xmark & 76 & \gpu{0.9}, \cpu{0.04} & \tf{41.5} & 24.1 \\
\midrule
\multirow{3}{*}{Phrase Retrieval} & DenSPI~\citep{seo2019real} & \cmark & 1,200 & \gpu{2.9}, \cpu{2.4} & 8.1 & 36.2\\
& DenSPI + Sparc~\citep{lee2020contextualized} & \cmark & 1,547 & \gpu{2.1}, \cpu{1.7} & 14.5 & \tf{40.7} \\
& \textbf{\ours~(Ours)} & \xmark & 320 & \gpu{20.6},	\cpu{13.6} & \tf{40.9} & 38.0 \\
\bottomrule
\end{tabular}
}
\end{center}\vspace{-0.1cm}
\caption{Retriever-reader and phrase retrieval approaches for open-domain QA. The \textit{retriever-reader} approach retrieves a small number of relevant documents or passages from which the answers are extracted.
The \textit{phrase retrieval} approach retrieves an answer out of billions of phrase representations pre-indexed from the entire corpus. \Cref{apdx:server} provides detailed benchmark specification. The accuracy is measured on the test sets in the open-domain setting.  NQ: Natural Questions.
}\vspace{-0.4cm}
\label{tab:category}
\end{table*}
 
In this work, we investigate whether we can build fully dense phrase representations at scale for open-domain QA.
First, we aim to learn strong phrase representations from the supervision of reading comprehension tasks.
We propose to use data augmentation and knowledge distillation to learn better phrase representations within a single passage.
We then adopt negative sampling strategies such as in-batch negatives~\citep{henderson2017efficient,karpukhin2020dense}, to better discriminate the phrases at a larger scale.
Here, we present a novel method called \ti{pre-batch negatives}, which leverages preceding mini-batches as negative examples to compensate the need of large-batch training.
Lastly, we present a {query-side fine-tuning strategy} that drastically improves phrase retrieval performance and allows for transfer learning to new domains, without re-building billions of phrase representations.

As a result, all these improvements lead to a much stronger phrase retrieval model, without the use of \ti{any} sparse representations (Table~\ref{tab:category}). We evaluate our model, \ti{DensePhrases}, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models~\citep{seo2019real,lee2020contextualized}, with 15\%--25\% absolute improvement on most datasets.
Our model also matches the performance of state-of-the-art retriever-reader models~\citep{guu2020realm,karpukhin2020dense}.
Due to the removal of sparse representations and careful design choices, we further reduce the storage footprint for the full English Wikipedia from 1.5TB to 320GB, as well as drastically improve the throughput.


Finally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowledge from a large text corpus.
To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without re-building the phrase storage.
With only fine-tuning the question encoder on a small number of subject-relation-object triples, we achieve state-of-the-art performance on two slot filling tasks~\citep{petroni2020kilt}, using less than 5\% of the training data.


 

\section{Background}
\label{sec:background}




We first formulate the task of open-domain question answering for a set of  documents .
We follow the recent work~\cite{chen2017reading,lee2019latent} and treat all of English Wikipedia as , hence .
However, most approaches---including ours---are generic and could be applied to other collections of documents.

The task aims to provide an answer  for the input question  based on .
In this work, we focus on the extractive QA setting, where each answer is a segment of text, or a \ti{phrase}, that can be found in .
Denote the set of phrases in  as  and each phrase  consists of contiguous words  in its document .
In practice, we consider all the phrases up to  words in  and  comprises a large number of  phrases.
An extractive QA system returns a phrase  where  is a scoring function. The system finally maps  to an answer string :  and the evaluation is typically done by comparing the predicted answer  with a gold answer .

Although we focus on the extractive QA setting, recent works propose to use a generative model as the reader~\citep{lewis2020retrieval,izacard2020leveraging}, or learn a closed-book QA model~\citep{roberts2020much}, which directly predicts answers  without using an external knowledge source. The extractive setting provides two advantages: first, the model directly locates the source of the answer, which is more interpretable, and second, phrase-level knowledge retrieval can be uniquely adapted to other NLP tasks as we show in~\S\ref{sec:slot_filling}.

\paragraph{Retriever-reader.}
A dominating paradigm in open-domain QA is the retriever-reader approach~\cite{chen2017reading,lee2019latent,karpukhin2020dense}, which leverages a first-stage document retriever  and only reads top  documents with a reader model . The scoring function  is decomposed as:
\vspace{-0.3em}

where  and if , the score will be 0.
It can easily adapt to passages and sentences~\cite{yang2019end,wang2019multi}.
However, this approach suffers from error propagation when incorrect documents are retrieved and can be slow as it usually requires running an expensive reader model on every retrieved document or passage at inference time.


\begin{comment}
\paragraph{Reader Only}
A second paradigm, the reader only approach~\citep{roberts2020much} (also known as a closed-book models\jinhyuk{add EaE?} merely depends on parameters of language models () to store all the factual knowledge:

This approach requires gigantic language models (e.g., T5 models with 11G parameters) to achieve competitive performance. It is also known that these models perform more question memorization than generalization~\cite{lewis2020question}.
\end{comment}

\paragraph{Phrase retrieval.}
\citet{seo2019real} introduce the phrase retrieval approach that encodes phrase and question representations \textit{independently} and performs similarity search over the phrase representations to find an answer.
Their scoring function  is computed as follows:
\vspace{-0.5em}

where  and  denote the phrase encoder and the question encoder respectively.
As  and  representations are decomposable, it can support maximum inner product search (MIPS) and improve the efficiency of open-domain QA models.
Previous approaches~\cite{seo2019real,lee2020contextualized} leverage both dense and sparse vectors for phrase and question representations by taking their concatenation: \footnote{\newcite{seo2019real} use sparse representations of both paragraphs and documents  and \newcite{lee2020contextualized} use contextualized sparse representations conditioned on the phrase.}
However, since the sparse vectors are difficult to parallelize with dense vectors, their method  essentially conducts sparse and dense vector search separately.
The goal of this work is to only use dense representations, i.e., , which can model  solely with MIPS, as well as close the gap in performance.



\begin{figure*}[t]
\begin{center}
\includegraphics[height=7.2cm]{figures/overview_new}
\end{center}
\caption{An overview of \ours.  (a) We learn dense phrase representations in a single passage (\S\ref{sec:single-passage}) along with in-batch and pre-batch negatives (\S\ref{sec:inbatch}, \S\ref{sec:prebatch}).
(b) With the top- retrieved phrase representations from the entire text corpus (\S\ref{sec:indexing_and_search}), we further perform query-side fine-tuning to optimize the question encoder (\S\ref{sec:qsft}).
During inference, our model simply returns the top-1 prediction.
}\label{fig:overview}
\end{figure*}


 
 


\section{{\ours}}

\subsection{Overview}

We introduce {\ours}, a phrase retrieval model that is built on fully dense representations.
Our goal is to learn a {phrase} encoder as well as a {question} encoder, so we can pre-index all the possible phrases in , and efficiently retrieve phrases for any question through MIPS at testing time. We outline our approach as follows:
\begin{itemize}[noitemsep]
\item
We first learn a high-quality phrase encoder and an (initial) question encoder from the supervision of reading comprehension tasks (\S\ref{sec:single-passage}), as well as incorporating effective negative sampling to better discriminate phrases at scale (\S\ref{sec:inbatch}, \S\ref{sec:prebatch}).
\item
Then, we fix the phrase encoder and encode all the phrases  and store the phrase indexing offline to enable efficient search (\S\ref{sec:indexing_and_search}).
\item
Finally, we introduce an additional strategy called query-side fine-tuning (\S\ref{sec:qsft}) by further updating the question encoder.\footnote{In this paper, we use the term \ti{question} and \ti{query} interchangeably as our question encoder can be naturally extended to ``unnatural'' queries.} We find this step to be very effective, as it can reduce the discrepancy between training (the first step) and inference, as well as support transfer learning to new domains.
\end{itemize}

Before we present the approach in detail, we first describe our base architecture below.



\subsection{Base Architecture}
\label{sec:base_model}
Our base architecture consists of a phrase encoder  and a question encoder . Given a passage , we denote all the phrases up to  tokens as . Each phrase  has start and end indicies  and  and the gold phrase is .
Following previous work on phrase or span representations~\citep{lee2017learning,seo2018phrase}, we first apply a pre-trained language model  to obtain contextualized word representations for each passage token: . Then, we can represent each phrase  as the concatenation of corresponding start and end vectors:
\vspace{-0.5em}

A great advantage of this representation is that we eventually only need to index and store all the word vectors (we use  to denote all the words in ), instead of all the phrases , which is at least one magnitude order smaller.



Similarly, we need to learn a question encoder  that maps a question  to a vector of the same dimension as . Since the start and end representations of phrases are produced by the same language model, we use another two different pre-trained encoders  and  to differentiate the start and end positions. We apply  and  on  separately and obtain representations  and  taken from the \texttt{[CLS]} token representations respectively. Finally,  simply takes their concatenation:
\vspace{-0.5em}

Note that we use pre-trained language models to initialize ,  and  and they are fine-tuned with the objectives that we will define later. In our pilot experiments, we found that SpanBERT~\citep{joshi2020spanbert} leads to superior performance compared to BERT~\cite{devlin2019bert}.
SpanBERT is designed to predict the information in the entire span from its two endpoints, therefore it is well suited for our phrase representations.
In our final model, we use SpanBERT-base-cased as our base LMs for  and , and hence .\footnote{Our base model is largely inspired by DenSPI~\cite{seo2019real}, although we deviate from theirs as follows. (1)
We remove coherency scalars and don't split any vectors.
(2) DenSPI uses a shared encoder for phrases and questions while we use 3 separate language models initialized from the same pre-trained model. (3) We use SpanBERT instead of BERT. } See Table~\ref{tab:piqa-ablation} for an ablation study.






\section{Learning Phrase Representations}
\label{sec:learning_phrases}

In this section, we start by learning dense phrase representations from the supervision of reading comprehension tasks, i.e., a single passage  contains an answer  to a question .
Our goal is to learn strong dense representations of phrases for , which can be retrieved by a dense representation of the question and serve as a direct answer (\S\ref{sec:single-passage}).
Then, we introduce two different negative sampling methods (\S\ref{sec:inbatch}, \S\ref{sec:prebatch}), which encourage the phrase representations to be better discriminated at the full Wikipedia scale.
See Figure~\ref{fig:overview} for an overview of \ours.





\subsection{Single-passage Training}\label{sec:single-passage}
To learn phrase representations in a single passage along with question representations, we first maximize the log-likelihood of the start and end positions of the gold phrase  where .
The training loss for predicting the start position of a phrase given a question is computed as:

We can define  in a similar way and the final loss for the single-passage training is

This essentially learns reading comprehension without any cross-attention between the passage and the question tokens, which fully decomposes phrase and question representations.

\begin{comment}


\paragraph{Differences from DenSPI}
We deviate from DenSPI in the following ways: (1) Previous models split a hidden vector from a pre-trained LM into four vectors (start \& end vectors and two vectors for calculating a coherency score).
We don't do any splitting of vectors and remove the use of coherency scalars.
We find that it is beneficial to keep the output dimension of pre-trained LMs for fully utilizing their representational capacity;
(2) Previous models use a shared encoder for phrases and questions.
However, we use two different language models for representing questions. (3) We use SpanBERT instead of BERT. See Table~\ref{tab:piqa-ablation} for an ablation study.
\end{comment}

\paragraph{Data augmentation}
Since the contextualized word representations  are encoded in a query-agnostic way, they are always inferior to \ti{query-dependent} representations in cross-attention models~\citep{devlin2019bert},  where passages are fed along with the questions concatenated by a special token such as \ttt{[SEP]}.
We hypothesize that one key reason for the performance gap is that reading comprehension datasets only provide a few annotated questions in each passage, compared to the set of possible answer phrases. Learning from this supervision is not easy to differentiate similar phrases in one passage (e.g.,  \textit{Charles, Prince of Wales} and another  \textit{Prince George} for a question  \textit{Who is next in line to be the monarch of England?}).


\begin{comment}
Suppose that we are given a passage with the following question-answer pair in the training set:
\begin{quote}
\small
 \textit{Queen Elizabeth II is the sovereign, and her heir apparent is her eldest son, \textbf{Charles, Prince of Wales}. (...) Third in line is Prince George, the eldest child of the Duke of Cambridge (...)} \\
 \textit{who is next in line to be the monarch of england} \\
 \textit{\textbf{Charles, Prince of Wales}}
\end{quote}
\vspace{-0.5em}
\noindent While cross-attention models only need to represent the passage focusing on ``who is Queen Elizabeth II's heir apparent,'' our phrase encoder should take all the other phrases into account, (e.g.,  \textit{Prince George}), because their representations will be re-used for other questions (e.g.,  \textit{who is the eldest child of the duke of cambridge}).
\end{comment}

Following this intuition, we propose to use a simple model to generate additional questions for data augmentation, based on a T5-large model~\cite{raffel2020exploring}.
To train the question generation model, we feed a passage  with the gold answer  highlighted by inserting surrounding special tags.
Then, the model is trained to maximize the log-likelihood of the question words of .
After training, we extract all the named entities in each training passage as candidate answers and feed the passage  with each candidate answer to generate questions.
We keep the question-answer pairs only when a cross-attention reading comprehension model\footnote{SpanBERT-large, 88.2 EM on SQuAD.} makes a correct prediction on the generated pair.
The remaining generated QA pairs  are directly augmented to the original training set.


\paragraph{Distillation} We also propose improving the phrase representations by distilling knowledge from a cross-attention model~\citep{hinton2015distilling}.
We minimize the Kullbackâ€“Leibler divergence between the probability distribution from our phrase encoder and that from a standard SpanBERT-base QA model. The loss is computed as follows:

where  (and ) is defined in Eq.~\eqref{eqn:pstart} and  and  denote the probability distributions used to predict the start and end positions of answers in the cross-attention model.






\begin{figure}[t]
\begin{center}
\includegraphics[height=4.2cm]{figures/batchneg}
\end{center}\vspace{-0.2cm}
\caption{Two types of negative samples for the first batch item () in a mini-batch of size  and .  Note that the negative samples for the end representations () are obtained in a similar manner. See \S\ref{sec:inbatch} and \S\ref{sec:prebatch} for more details.
}\vspace{-0.3cm}\label{fig:batchneg}
\end{figure}

 
\subsection{In-batch Negatives}\label{sec:inbatch}
Eventually, we need to build phrase representations for billions of phrases. Therefore, a bigger challenge is to incorporate more phrases as negatives so the representations can be better discriminated at a larger scale.
While \citet{seo2019real} simply sample two negative passages based on question similarity, we use in-batch negatives for our dense phrase representations, which has been shown to be effective in learning dense passage representations before~\cite{karpukhin2020dense}.

As shown in Figure~\ref{fig:batchneg} (a), for the -th example in a mini-batch of size , we denote the hidden representations of the gold start and end positions  and  as  and , as well as the question representation as . Let  be the  matrices and each row corresponds to  respectively. Basically, we can treat all the gold phrases from other passages  in the same mini-batch as negative examples.  We compute  and  and the -th row of  and  return  scores each, including a positive score and  negative scores:  and . Similar to Eq.~\eqref{eqn:pstart}, we can compute the loss function for the -th example as:\vspace{-0.5em}

We also attempted using non-gold phrases from other passages as negatives but did not find a meaningful improvement.

\subsection{Pre-batch Negatives}\label{sec:prebatch}
The in-batch negatives usually benefit from a large batch size~\citep{karpukhin2020dense}.
However, it is challenging to further increase batch sizes, as they are bounded by the size of GPU memory.
Next, we propose a novel negative sampling method called \ti{pre-batch negatives}, which can effectively utilize the representations from the preceding  mini-batches (Figure~\ref{fig:batchneg} (b)).
In each iteration, we maintain a FIFO queue of  mini-batches to cache phrase representations  and .  The cached phrase representations are then used as negative samples for the next iteration, providing  additional negative samples in total.\footnote{This approach is inspired by the momentum contrast idea proposed in unsupervised visual representation learning~\cite{he2020momentum}. Contrary to their approach, we have separate encoders for phrases and questions and back-propagate to both during training without a momentum update.}


These pre-batch negatives are used together with in-batch negatives and the training loss is the same as Eq.~\eqref{eqn:inbatch}, except that the gradients are \ti{not} back-propagated to the cached pre-batch negatives.
After warming up the model with in-batch negatives, we simply shift from in-batch negatives ( negatives) to in-batch and pre-batch negatives (hence a total number of  negatives).
For simplicity, we use  to denote the loss for both in-batch negatives and pre-batch negatives. Since we do not retain the computational graph for pre-batch negatives, the memory consumption of pre-batch negatives is much more manageable while allowing an increase in the number of negative samples.


\subsection{Training Objective} Finally, we optimize all the three losses together, on both annotated reading comprehension examples and generated questions from \S\ref{sec:single-passage}:

\noindent where  determine the importance of each loss term.
We found that , , and  works well in practice. See  Table~\ref{tab:piqa-ablation} and Table~\ref{tab:sod-qa} for an ablation study of different components.






\section{Indexing and Search}
\label{sec:indexing_and_search}

\paragraph{Indexing}
After training the phrase encoder , we need to encode all the phrases  in the entire English Wikipedia  and store an index of the phrase dump.
We segment each document  into a set of natural paragraphs, from which we obtain token representations for each paragraph using .
Then, we build a phrase dump  by stacking the token representations from all the paragraphs in .
Note that this process is computationally expensive and takes about hundreds of GPU hours with a large disk footprint.
To reduce the size of phrase dump, we follow and modify several techniques introduced in~\citet{seo2019real} (see \Cref{apdx:storage} for details).
After indexing, we can use two rows  and  of  to represent a dense phrase representation . We use \texttt{faiss}~\citep{johnson2017billion} for building a MIPS index of .\footnote{We use \texttt{IVFSQ4} with 1M clusters and set n-probe to 256.}

\paragraph{Search}
For a given question , we can find the answer  as follows:
\vspace{-0.2em}

\noindent where  denotes a phrase with start and end indices as  and  in the index .
We can compute the  of  and  efficiently by performing MIPS over  with  and .
In practice, we search for the top- start and top- end positions separately and perform a constrained search over their end and start positions respectively such that .






\section{Query-side Fine-tuning}
\label{sec:qsft}
So far, we have created a phrase dump  that supports efficient MIPS search. In this section, we propose a novel method called {query-side fine-tuning} by only updating the question encoder  to correctly retrieve a desired answer  for a question  given .
Formally speaking, we optimize the marginal log-likelihood of the gold answer  for a question , which resembles the weakly-supervised QA setting in previous work~\citep{lee2019latent,min2019discrete}.
For every question , we retrieve top  phrases and minimize the objective:

where  is the score of the phrase  (Eq.~\eqref{eqn:decomp}) and  denotes the top  phrases for  (Eq.~\eqref{eqn:formula}).
In practice, we use  for all the experiments.


There are several advantages for doing this: (1) we find that query-side fine-tuning can reduce the discrepancy between training and inference, and hence improve the final performance substantially (\S\ref{sec:qsft-ablation}). Even with effective negative sampling, the model only sees a small portion of passages compared to the full scale of  and this training objective can effectively fill in the gap. (2) This training strategy allows for transfer learning to unseen domains, without rebuilding the entire phrase index. More specifically, the model is able to quickly adapt to new QA tasks (e.g., WebQuestions) when the phrase dump is built using SQuAD or Natural Questions. We also find that this can transfers to non-QA tasks when the query is written in a different format.
In~\Cref{sec:slot_filling}, we show the possibility of directly using \ours~ for slot filling tasks by using a query such as \ti{(Michael Jackson, is a singer of, x)}. In this regard, we can view our model as a dense knowledge base that can be accessed by many different types of queries and it is able to return phrase-level knowledge efficiently.



 



\section{Experiments}
\label{sec:experiments}

\subsection{Setup}
\paragraph{Datasets.}
We use two {reading comprehension} datasets: SQuAD~\cite{rajpurkar2016squad} and Natural Questions (NQ)~\cite{kwiatkowski2019natural} to learn phrase representations, in which a single gold passage is provided for each question.
For the open-domain QA experiments, we evaluate our approach on five popular {open-domain QA} datasets: Natural Questions, WebQuestions (WQ)~\citep{berant2013semantic}, CuratedTREC (TREC)~\citep{baudivs2015modeling}, TriviaQA (TQA)~\citep{joshi2017triviaqa}, and SQuAD.
Note that we only use SQuAD and/or NQ to build the phrase index and perform query-side fine-tuning (\S\ref{sec:qsft}) for other datasets.


We also evaluate our model on two {slot filling} tasks, to show how to adapt our {\ours} for other knowledge-intensive NLP tasks.
We focus on using two slot filling datasets from the KILT benchmark~\citep{petroni2020kilt}: T-REx~\citep{elsahar2018t} and zero-shot relation extraction~\citep{levy2017zero}.
Each query is provided in the form of ``\{subject entity\} \ttt{[SEP]} \{relation\}" and the answer is the object entity.
\Cref{apdx:prepro} provides the statistics of all the datasets.

\paragraph{Implementation details.}
We denote the training datasets used for reading comprehension (Eq.~\eqref{eqn:aggregate}) as .
For open-domain QA, we train two versions of phrase encoders, each of which are trained on  and , respectively.
We build the phrase dump  for the 2018-12-20 Wikipedia snapshot and perform query-side fine-tuning on each dataset using Eq.~\eqref{eqn:qsft}.
For slot filling, we use the same phrase dump for open-domain QA,    and perform query-side fine-tuning on randomly sampled 5K or 10K training examples to see how rapidly our model adapts to the new query types.
See~\Cref{apdx:hyper} for details on the hyperparameters and \Cref{apdx:complexity} for an analysis of computational cost.



\begin{table}[t]
    \centering
    \resizebox{0.95\columnwidth}{!}{\begin{tabular}{lcccc}
        \toprule
         \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{SQuAD}} & \multicolumn{2}{c}{\textbf{NQ (Long)}} \\\cmidrule{2-3} \cmidrule{4-5}
        & EM & F1 & EM & F1 \\
        \midrule

        \multicolumn{3}{l}{\textit{Query-Dependent}}\\\midrule
BERT-base & 80.8 & 88.5 & 69.9 & 78.2 \\
SpanBERT-base & 85.7 & 92.2 & 73.2 & 81.0 \\
\midrule

        \multicolumn{3}{l}{\textit{Query-Agnostic}} \\\midrule
DilBERT~\citep{siblini2020delaying} & \underline{63.0} & \underline{72.0} & - & - \\
        DeFormer~\citep{cao2020deformer} & - & \underline{72.1} & - & - \\
        DenSPI & 73.6 & 81.7 & 68.2 & 76.1 \\
DenSPI + Sparc & 76.4 & 84.8 & - & - \\
\ours~(ours) & \textbf{78.3} & \textbf{86.3} & \textbf{71.9} & \textbf{79.6} \\




        \bottomrule
    \end{tabular}
    }\vspace{-0.0cm}
    \caption{Reading comprehension results, evaluated on the development sets of SQuAD and Natural Questions. Underlined numbers are estimated from the figures from the original papers. : BERT-large model. }\label{tab:pi-qa}\vspace{-0.5cm}
\end{table}
 


\begin{table*}[t]
    \centering
    \resizebox{1.95\columnwidth}{!}{\begin{tabular}{llccccc}
        \toprule
        \textbf{Model} & &\textbf{NQ} & \textbf{WQ} & \textbf{TREC} & \textbf{TQA} &  \textbf{SQuAD}\\
         \midrule

        \textit{Retriever-reader} & : (Pre-)Training \\ \midrule
        DrQA~\citep{chen2017reading} & - & - & 20.7 & 25.4 & - & 29.8 \\
        BERT + BM25~\citep{lee2019latent}  & - & 26.5 & 17.7 & 21.3 & 47.1 & \tf{33.2} \\
        ORQA~\citep{lee2019latent} & \{Wiki.\} & 33.3 & 36.4 & 30.1 & 45.0 & 20.2 \\
        REALM~\citep{guu2020realm} & \{Wiki., CC-News\}& 40.4 & 40.7 & 42.9 & - & - \\
        DPR-multi~\citep{karpukhin2020dense}  & \{NQ, WQ, TREC, TQA\} & \tf{41.5} & \tf{42.4} & \tf{49.4} & \tf{56.8} & 24.1 \\
\midrule



        \textit{Phrase retrieval} & : Training \\\midrule
        DenSPI~\citep{seo2019real} & \{SQuAD\} & 8.1 & 11.1 & 31.6 & 30.7 & 36.2 \\ DenSPI + Sparc~\citep{lee2020contextualized} & \{SQuAD\} & 14.5 & 17.3 & 35.7 & 34.4 & \textbf{40.7}  \\
        DenSPI + Sparc~\citep{lee2020contextualized} & \{NQ, SQuAD\} & 16.5 & - & - & - & -  \\
\ours~(ours) & \{SQuAD\}& 31.2 & 36.3 & 50.3 & \textbf{53.6} & 39.4 \\
\ours~(ours) & \{NQ, SQuAD\}&  \textbf{40.9} & \textbf{37.5} & \textbf{51.0} & 50.7 & 38.0 \\
        \bottomrule
    \end{tabular}
    }\vspace{-.1cm}
    \caption{Open-domain QA results. We report exact match (EM) on the test sets. We also show the additional training or pre-training datasets for learning the retriever models () and creating the phrase dump (). : no supervision using target training data (zero-shot). : unlabeled data used for extra pre-training.
    }\vspace{-0.3cm}
    \label{tab:od-qa}
\end{table*}
 
\begin{comment}
\paragraph{Baselines}
For reading comprehension, we report scores of query-agnostic models including DenSPI~\citep{seo2019real}, DenSPI + Sparc~\citep{lee2020contextualized} as well as Deformer~\citep{cao2020deformer} and DilBERT~\citep{siblini2020delaying}, where they only allow a late interaction (cross-attention) in the last few layers of BERT.
We report their results based on the interaction in the last layer, which mostly resembles the fully query-agnostic models.

For open-domain QA, we report the scores of extractive open-domain QA models including DrQA~\citep{chen2017reading}, BERT + BM25~\citep{lee2019latent}, ORQA~\citep{lee2019latent}, REALM~\citep{guu2020realm}, and DPR-Multi~\citep{karpukhin2020dense}.
We also show the performance of previous phrase retrieval models: DenSPI and DenSPI + Sparc.
In~\Cref{apdx:complexity}, we provide a thorough analysis on the computational complexity of each open-domain QA model.
\end{comment}



\begin{comment}
\begin{table*}[t]
    \centering
    \resizebox{2.0\columnwidth}{!}{\begin{tabular}{lccccccccccccc}
        \toprule
        \multirow{2}{1.2cm}{\textbf{Model}}& \multicolumn{6}{c}{\textbf{T-REx}} && \multicolumn{6}{c}{\textbf{ZsRE}} \\\cmidrule{2-7} \cmidrule{9-14}
        & R-Prec & Recall@5 & Accuracy & F1 & KILT-AC & KILT-F1 & & R-Prec & Recall@5 & Accuracy & F1 & KILT-AC & KILT-F1  \\
        \midrule

        BART & 0.00 & 0.00 & 45.06 & 49.24 & 0.00& 0.00 & & 0.00 & 0.00 & 9.14 & 12.21 & 0.00& 0.00 \\
        T5 &  0.00 & 0.00 & 43.56 & 50.61& 0.00 & 0.00 & & 0.00 & 0.00 & 9.02 & 13.52 & 0.00& 0.00\\
        DPR + BERT & - & - & - & - & - & - & & 40.11 & 40.11 & 6.93 & 37.28 & 4.47 & 27.09 \\
        BART + DPR & 13.26 & 17.04 & 59.16 & 62.76& 11.12 & 11.41 & & 28.90 & 39.21 & 30.43 & 34.47 & 18.91 & 20.32 \\
        RAG & 28.68 & 33.04 & \textbf{59.20} & \textbf{62.96} & 23.12 & 23.94 & & 53.73 & 59.52 & 44.74 & 49.95 & 36.83 & 39.91\\
        \midrule
        \ours~(+5K) & 34.74 & 37.07 & 52.84 & 59.99 & 25.24 & 29.48 & & \textbf{56.79} & \textbf{59.57} & \textbf{45.75} & \textbf{53.68} & \textbf{40.27} & \textbf{46.31} \\
        \ours~(+10K) & \textbf{35.68} & \textbf{38.08} & 55.16 & 61.44 & \textbf{27.60} & \textbf{30.95} & & 53.20 & 55.78 & 43.80 & 52.75 & 38.40 & 45.15 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Slot filling task results on the T-REx and Zero shot RE (ZsRE) test sets.
    \label{tab:re}}
\end{table*}
\end{comment}





\begin{comment}
\begin{table}[t]
    \centering
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}{lcccc}
        \toprule
        \multirow{2}{1.2cm}{\textbf{Model}}& \multicolumn{2}{c}{\textbf{T-REx}} & \multicolumn{2}{c}{\textbf{ZsRE}} \\\cmidrule{2-3} \cmidrule{4-5}
        & Acc & F1 & Acc & F1 \\
        \midrule

        BART & 45.06 & 49.24 & 9.14 & 12.21 \\
        T5 & 43.56 & 50.61 & 9.02 & 13.52 \\
        DPR + BERT & - & - & 6.93 & 37.28 \\
        BART + DPR & 59.16 & 62.76 & 30.43 & 34.47 \\
        RAG & \textbf{59.20} & \textbf{62.96} & 44.74 & 49.95 \\
        \midrule
\ours~(5K) & 51.82 & 59.38 & 46.13 & 53.65 \\
        \ours~(10K) & 53.90 & 61.74 & \textbf{47.42} & \textbf{54.75} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Slot filling task results on the T-REx and Zero shot RE (ZsRE) test sets.\jinhyuk{KILT-Acc is SOTA for both T-REx and ZsRE. Why don't we report it?}
    \label{tab:slot_filling}}
\end{table}
\end{comment}

\begin{table}[t]
    \centering
    \resizebox{0.95\columnwidth}{!}{\begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}}& \multicolumn{2}{c}{\textbf{T-REx}} & \multicolumn{2}{c}{\textbf{ZsRE}} \\\cmidrule{2-3} \cmidrule{4-5}
        & Acc & F1 & Acc & F1 \\
        \midrule

DPR + BERT & - & - & 4.47 & 27.09 \\
        DPR + BART & 11.12 & 11.41 & 18.91 & 20.32 \\
        RAG & 23.12 & 23.94 & 36.83 & 39.91 \\
        \midrule
\ours & 25.32 & 29.76 & 40.39 & 45.89 \\
        \ours & \textbf{27.84} & \textbf{32.34} & \textbf{41.34} & \textbf{46.79} \\
        \bottomrule
    \end{tabular}
    }\vspace{-0.1cm}
    \caption{Slot filling results on the test sets of T-REx and Zero shot RE (ZsRE) in the KILT benchmark.
    We report KILT-AC and KILT-F1 (denoted as \ti{Acc} and \ti{F1} in the table), which consider both span-level accuracy and correct retrieval of evidence documents.
}\vspace{-0.5cm}\label{tab:slot_filling}
\end{table}
 \subsection{Experiments: Question Answering}\label{sec:openqa_result}
\paragraph{Reading comprehension.}\label{sec:rc_result}
In order to show the effectiveness of our phrase representations, we first evaluate our model in the reading comprehension setting for SQuAD and NQ and report its performance with other query-agnostic models (Eq.~\eqref{eqn:aggregate} without query-side fine-tuning). This problem was originally formulated by \newcite{seo2018phrase} as the phrase-indexed question answering (PIQA) task.

Compared to previous query-agnostic models, our model achieves the best performance of 78.3 EM on SQuAD by improving the previous phrase retrieval model (DenSPI) by  (Table~\ref{tab:pi-qa}). Although it is still behind cross-attention models, the gap has been greatly reduced and serves as a strong starting point for the open-domain QA model.

\paragraph{Open-domain QA.}
Experimental results on open-domain QA are summarized in Table~\ref{tab:od-qa}.
Without any sparse representations, \ours~outperforms previous phrase retrieval models by a large margin and achieves a -- absolute improvement on all datasets except SQuAD.
Training the model of \newcite{lee2020contextualized} on  only increases the result from 14.5\% to 16.5\% on NQ, demonstrating that it does not suffice to simply add more datasets for training phrase representations.
Our performance is also competitive with recent retriever-reader models~\cite{karpukhin2020dense}, while running much faster during inference (Table~\ref{tab:category}).


\subsection{Experiments: Slot Filling}\label{sec:slot_filling}
Table~\ref{tab:slot_filling} summarizes the results on the two slot filling datasets, along with the baseline scores provided by~\citet{petroni2020kilt}.
The only extractive baseline is DPR + BERT, which performs poorly in zero-shot relation extraction.
On the other hand, our model achieves competitive performance on all datasets and achieves state-of-the-art performance on two datasets using only 5K training examples. 

\section{Analysis}
\label{sec:analysis}



\paragraph{Ablation of phrase representations.}
Table~\ref{tab:piqa-ablation} shows the ablation result of our model on SQuAD.
Upon our choice of architecture, augmenting training set with generated questions ({QG} = \cmark) and performing distillation from cross-attention models ({Distill} = \cmark) improve performance up to EM = 78.3.
We attempted adding the generated questions to the training of the SpanBERT-QA model but find a 0.3\% improvement, which validates that data sparsity is a bottleneck for query-agnostic models.



\begin{table}[t]
    \centering
    \resizebox{0.95\columnwidth}{!}{\begin{tabular}{lcccccc}
        \toprule
         \textbf{Model} &  &
         {{Share}} & {{Split}} & {{QG}} &  {{Distill}} &  {{EM}} \\


        \midrule
        DenSPI & Bb. & \cmark & \cmark & \xmark & \xmark & 70.2 \\
        & Sb. & \cmark & \cmark & \xmark & \xmark & 68.5 \\
        & Bl. & \cmark & \cmark & \xmark & \xmark & 73.6 \\
        \midrule
        Dense & Bb. & \cmark & \xmark  & \xmark & \xmark & 70.2 \\
        Phrases & Bb. & \xmark & \xmark  & \xmark & \xmark & 71.9 \\
        & Sb. & \xmark & \xmark  & \xmark & \xmark & 73.2 \\
        & Sb. & \xmark & \xmark  & \cmark & \xmark & 76.3 \\
        & Sb. & \xmark & \xmark  & \cmark & \cmark & \textbf{78.3} \\


        \bottomrule
    \end{tabular}
    }
    \caption{Ablation of \ours~on the development set of SQuAD. Bb: BERT-base, Sb: SpanBERT-base, Bl: BERT-large. Share: whether question and phrase encoders are shared or not. Split: whether the full hidden vectors are kept or split into start and end vectors. QG: question generation (\S\ref{sec:single-passage}). Distill: distillation (Eq.\eqref{eqn:distill}). DenSPI~\cite{seo2019real} also included a coherency scalar and see their paper for more details.}\label{tab:piqa-ablation}\vspace{-0.4cm}
\end{table}
 



\begin{table}[t]
    \centering
    \resizebox{0.90\columnwidth}{!}{\begin{tabular}{lcccc}
        \toprule
        {Type} &  &  &
         &  \\
        \midrule
        None & 48 & - & 70.4 & 35.3 \\
        \midrule
        + In-batch & 48 & - & 70.5 & 52.4 \\ & 84 & - & 70.3 & 54.2 \\ \midrule
        + Pre-batch & 84 & 1 & 71.6 & 59.8 \\ & 84 & 2 & \textbf{71.9} & \textbf{60.4} \\ & 84 & 4 & 71.2 & 59.8 \\ \bottomrule
    \end{tabular}
    }
    \caption{Effect of in-batch negatives and pre-batch negatives on the development set of Natural Questions. : batch size. : number of preceding mini-batches used in pre-batch negatives. : all the gold passages in the development set of NQ. : single passage.
}\label{tab:sod-qa} \vspace{-0.3cm}
\end{table}


\begin{comment}
\begin{table*}[t]
    \centering
    \resizebox{2.0\columnwidth}{!}{\begin{tabular}{clcccccccccc}
        \toprule
        & & & & & & \multicolumn{5}{c}{EM for Corpus Size (\% of Wikipedia)} \\ \cmidrule{7-11}
        & \textbf{Model} & Training Data & Sparse & In-Batch & Momentum & Gold & 0.001\% & 1\% & 10\% & 100\% \\
        \midrule

        \multirow{8}{1.5cm}{SQuAD} & DenSPI (w/o sparse) & SQuAD & \xmark & \xmark & \xmark & 73.6 & - & - & - & 11.2 \\
        & DenSPI & SQuAD & \cmark & \xmark & \xmark & 73.6 & - & - & - & 36.2 \\
        & DenSPI + Sparc & SQuAD & \cmark & \xmark & \xmark & 76.4 & 67.0 & \textbf{62.1} & 54.5 & 40.7 \\
        & \ours & SQuAD & \xmark & \xmark & \xmark & \textbf{78.3} & 67.1 & - & - \\
        & \ours & SQuAD & \xmark &  & \xmark & 77.9 & 68.9 & 48.7 & - & -\\
        & \ours & SQuAD & \xmark &  & \xmark & 76.4 & \textbf{73.6} & 60.5 & - & -\\
        & \ours & SQuAD & \xmark &  &  & - & - & - & - \\
        & \ours & SQuAD + NQ & \xmark &  &  & 74.9 & - & - & - \\
        \midrule

        & & & & & & Gold & 0.05\% & 1\% & 10\% & 100\% \\ \midrule
        \multirow{8}{1.5cm}{Natural Questions} & DenSPI (w/o sparse) & NQ & \xmark & \xmark & \xmark & 68.2 & 35.3 & - & - & - \\
        & DenSPI (w/o sparse) & NQ & \xmark &  & \xmark & 68.2 & 54.3 & 35.3 & 31.6 & - \\
        & DenSPI & NQ & \cmark &  & \xmark & 68.2 & 58.0 & 44.1 & 38.5 & 26.3 \\
        & \ours & NQ & \xmark & \xmark & \xmark & 70.3 & - & - & - \\
        & \ours & NQ & \xmark &  & \xmark & 70.2 & 34.7 & - & - & - \\
        & \ours & NQ & \xmark &  & \xmark & 70.5 & 54.9 & - & - \\
        & \ours & NQ & \xmark &  &  & \textbf{71.7} & \textbf{61.2} &  \textbf{44.2} &  \textbf{38.6} \\
        & \ours & SQuAD + NQ & \xmark &  &  & 69.9 & - & - & - \\
        \bottomrule
    \end{tabular}
    }
    \caption{Semi open-domain setting results on the SQuAD and Natural Questions development set. For a better comparison with previous phrase models, we test on SQuAD as well. \jinhyuk{I have to tune in-batch negative size and momentum batch negative size.}\jinhyuk{Numbers for DenSPI are from very old experiment sheets and are overestimated on (slightly easier) sampled questions.}
    \label{tab:sod-qa}}
\end{table*}

\end{comment}
 
\paragraph{Effect of batch negatives.}\label{sec:semi_od}
We further evaluate the effectiveness of various negative sampling methods introduced in~\S\ref{sec:inbatch} and \S\ref{sec:prebatch}.
Since it is computationally expensive to test each setting at the full Wikipedia scale, we use a smaller text corpus  of all the gold passages in the development sets of Natural Questions, for the ablation study.
Empirically, we find that results are generally well correlated when we gradually increase the size of .
As shown in Table~\ref{tab:sod-qa}, both in-batch and pre-batch negatives bring substantial improvements.
While using a larger batch size () is beneficial for in-batch negatives, the number of preceding batches in pre-batch negatives is optimal when .
Surprisingly, the pre-batch negatives also improve the performance when .



\paragraph{Effect of query-side fine-tuning.}~\label{sec:qsft-ablation}
We summarize the effect of query-side fine-tuning in Table~\ref{tab:qsft-ablation}.
For the datasets that were not used for training the phrase encoders (TQA, WQ, TREC), we observe a 15\% to 20\% improvement after query-side fine-tuning. Even for the datasets that have been used (NQ, SQuAD), it leads to significant improvements (e.g., 32.6\%40.9\% on NQ for  = \{NQ\}) and it clearly demonstrates it can effectively reduce the discrepancy between training and inference.

 

\section{Related Work}
\label{sec:related_work}

Learning effective dense representations of words is a long-standing goal in NLP~\citep{bengio2003neural,collobert2011natural,mikolov2013distributed,peters2018deep,devlin2019bert}.
Beyond words, dense representations of many different granularities of text such as sentences~\citep{le2014distributed,kiros2015skip} or documents~\citep{yih2011learning} have been explored.
While dense phrase representations have been also studied for statistical machine translation~\citep{cho2014learning} or syntactic parsing~\citep{socher2010learning}, our work focuses on learning dense phrase representations for QA and any other knowledge-intensive tasks where phrases can be easily retrieved by performing MIPS.

This type of dense retrieval has been also studied for sentence and passage retrieval~\citep{humeau2019poly,karpukhin2020dense} (see~\citealp{lin2020pretrained} for recent advances in dense retrieval).
While DensePhrases is explicitly designed to retrieve phrases that can be used as an answer to given queries, retrieving phrases also naturally entails retrieving larger units of text, provided the datastore maintains the mapping between each phrase and the sentence and passage in which it occurs.




\begin{table}[t]
    \centering
    \resizebox{0.9\columnwidth}{!}{\begin{tabular}{ccccccc}
        \toprule
         {QS} &
        {NQ} & {WQ} & {TREC} & {TQA} & {SQuAD} \\
        \midrule
        \multicolumn{6}{c}{{} = \{SQuAD\}} \\
        \midrule
        \xmark & 12.3 & 11.8 & 36.9 & 34.6  & 35.5 \\
        \cmark & 31.2 & 36.3 & 50.3 & \textbf{53.6} & \textbf{39.4} \\
        \midrule
        \multicolumn{6}{c}{{} = \{NQ\}} \\
        \midrule
        \xmark & 32.6 & 21.1 & 32.3 & 32.4 & 20.7 \\
        \cmark & \tf{40.9} & 37.1 & 49.7 & 49.2 & 25.7 \\
        \midrule
        \multicolumn{6}{c}{{} = \{NQ, SQuAD\}} \\
        \midrule
         \xmark & 28.9 & 18.9 & 34.9 & 31.9 & 33.2 \\
          \cmark & \textbf{40.9} & \textbf{37.5} & \textbf{51.0} & 50.7 & 38.0 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Effect of query-side fine-tuning in \ours~on each test set. We report EM of each model before ({QS} = \xmark) and after ({QS} = \cmark) the query-side fine-tuning.}\label{tab:qsft-ablation} \vspace{-0.3cm}
\end{table}
  

\section{Conclusion}
\label{sec:conclusion}

In this study, we show that we can learn dense representations of phrases at the Wikipedia scale, which are readily retrievable for open-domain QA and other knowledge-intensive NLP tasks.
We learn both phrase and question encoders from the supervision of reading comprehension tasks and introduce two batch-negative techniques to better discriminate phrases at scale.
We also introduce query-side fine-tuning that adapts our model to different types of queries.
We achieve strong performance on five popular open-domain QA datasets, while reducing the storage footprint and improving latency significantly.
We also achieve strong performance on two slot filling datasets using only a small number of training examples, showing the possibility of utilizing our {\ours} as a knowledge base.
 
\vspace{1em}
\section*{Acknowledgments}
We thank Sewon Min, Hyunjae Kim, Gyuwan Kim, Jungsoo Park, Zexuan Zhong, Dan Friedman, Chris Sciavolino for providing valuable comments and feedback.
This research was supported by a grant of the Korea Health Technology R\&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health \& Welfare, Republic of Korea (grant number: HR20C0021) and National Research Foundation of Korea (NRF-2020R1A2C3010638).
It was also partly supported by the James Mi *91 Research Innovation Fund for Data Science and an Amazon Research Award.
\clearpage

\section*{Ethical Considerations}\label{sec:ethics}
Our work builds on standard reading comprehension datasets such as SQuAD to build phrase representations.
SQuAD, in particular, is created from a small number of Wikipedia articles sampled from top-10,000 most popular articles (measured by PageRanks), hence some of our models trained only on SQuAD could be easily biased towards the small number of topics that SQuAD contains.
We hope that excluding such datasets during training or inventing an alternative pre-training procedure for learning phrase representations could mitigate this problem.
Although most of our efforts have been made to reduce the computational complexity of previous phrase retrieval models (further detailed in~\Cref{apdx:complexity,apdx:storage}), leveraging our phrase retrieval model as a knowledge base will inevitably increase the minimum requirement for the additional experiments. We plan to apply vector quantization techniques to reduce the additional cost of using our model as a KB.
 


\bibliographystyle{acl_natbib}
\bibliography{acl2021}

\clearpage
\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{\Alph{section}.\arabic{table}}

\section{Computational Cost}~\label{apdx:complexity}
We describe the resources and time spent during inference (Table~\ref{tab:category} and \ref{tab:complexity}) and indexing (Table~\ref{tab:complexity}).
With our limited GPU resources (24GB  4), it takes about 20 hours for indexing the entire phrase representations.
We also largely reduced the storage from 1,547GB to 320GB by (1) removing sparse representations and (2) using our sharing and split strategy.
See Appendix~\ref{apdx:storage} for the details on the reduction of storage footprint and Appendix~\ref{apdx:server} for the specification of our server for the benchmark.



\begin{table}[h]
    \centering
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}{llll}
        \toprule



        \textbf{Indexing} & Resources & Storage & Time \\ \midrule
        DPR & 32GB GPU  8 & 76GB & 17h \\
        DenSPI + Sparc & 24GB GPU  4 & 1,547GB & 85h \\\ours & 24GB GPU  4 & 320GB & 20h \\ \midrule


        \textbf{Inference} & \multicolumn{1}{l}{RAM / GPU} & \multicolumn{2}{l}{\#Q/sec (\gpu{GPU}, \cpu{CPU})} \\ \midrule
        DPR & 86GB / 17GB & \multicolumn{2}{c}{\gpu{0.9}, \cpu{0.04}} \\
        DenSPI + Sparc & 27GB / 2GB & \multicolumn{2}{c}{\gpu{2.1}, \cpu{1.7}} \\
        \ours & 12GB / 2GB & \multicolumn{2}{c}{\gpu{20.6}, \cpu{13.6}} \\
\bottomrule
    \end{tabular}
    }
    \caption{Complexity analysis of three open-domain QA models during indexing and inference. For inference, we also report the minimum requirement of RAM and GPU memory for running each model with \gpu{GPU}.
    For computing \#Q/s for \cpu{CPU}, we do not use GPUs but load all models on the RAM.
}\vspace{-0.2cm}
    \label{tab:complexity}
\end{table}
 
\section{Server Specifications for Benchmark}\label{apdx:server}
To compare the complexity of open-domain QA models, we install all models in Table~\ref{tab:category} on the same server using their public open-source code.
Our server has the following specifications:

\begin{table}[h]
    \centering
    \resizebox{0.8\columnwidth}{!}{\begin{tabular}{llll}
        \toprule
        Hardware \\ \midrule
        Intel Xeon CPU E5-2630 v4 @ 2.20GHz \\
        128GB RAM \\
        12GB GPU (TITAN Xp)  2\\
        2TB 970 EVO Plus NVMe M.2 SSD  1\\
        \bottomrule
    \end{tabular}
    }
    \caption{Server specification for the benchmark
    }
    \label{tab:server}
\end{table}
 For DPR, due to its large memory consumption, we use a similar server with a 24GB GPU (TITAN RTX).
For all models, we use 1,000 randomly sampled questions from the Natural Questions development set for the speed benchmark and measure \#Q/sec.
We set the batch size to 64 for all models except BERTSerini, ORQA and REALM, which do not allow a batch size of more than 1 in their open-source implementations.
\#Q/sec for DPR includes retrieving passages and running a reader model and the batch size for the reader model is set to 8 to fit in the 24GB GPU (retriever batch size is still 64).
For other hyperparameters, we use the default settings of each model.
We also exclude the time and the number of questions in the first five iterations for warming up each model.
Note that despite our effort to match the environment of each model, their latency can be affected by various different settings in their implementations such as the choice of library (PyTorch vs. Tensorflow).



\section{Data Statistics and Pre-processing}\label{apdx:prepro}


\begin{table}[t]
\label{table:dataset}
\begin{center}
\centering
\resizebox{0.9\columnwidth}{!}{\begin{tabular}{lrrr}
\toprule
\multicolumn{1}{l}{\bf Dataset}  & {\bf Train} & {\bf Dev} & {\bf Test}\\
\midrule
Natural Questions & 79,168 & 8,757 & 3,610 \\
WebQuestions & 3,417 & 361 & 2,032 \\
CuratedTrec & 1,353 & 133 & 694 \\
TriviaQA & 78,785 & 8,837 & 11,313 \\
SQuAD &  78,713 & 8,886 & 10,570 \\
\midrule

T-REx & 2,284,168 & 5,000 & 5,000 \\
Zero-Shot RE & 147,909 & 3,724 & 4,966 \\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Statistics of five open-domain QA datasets and two slot filling datasets. We follow the same splits in open-domain QA for the two reading comprehension datasets (SQuAD and Natural Questions).}\vspace{-0.3cm}\label{table:openqa-data}
\end{table}
 In Table~\ref{table:openqa-data}, we show the statistics of five open-domain QA datasets and two slot filling datasets.
Pre-processed open-domain QA datasets are provided by~\citet{chen2017reading} except Natural Questions and TriviaQA.
We use a version of Natural Questions and TriviaQA provided by~\citet{min2019discrete,lee2019latent}, which are pre-processed for the open-domain QA setting.
Slot filling datasets are provided by~\citet{petroni2020kilt}.
We use two reading comprehension datasets (SQuAD and Natural Questions) for training our model on Eq.~\eqref{eqn:aggregate}.
For SQuAD, we use the original dataset provided by the authors~\citep{rajpurkar2016squad}.
For Natural Questions~\citep{kwiatkowski2019natural}, we use the pre-processed version provided by~\citet{asai2019learning}.\footnote{\url{https://github.com/AkariAsai/learning\_to\_retrieve\_reasoning\_paths}}
We use the short answer as a ground truth answer  and its long answer as a gold passage .
We also match the gold passages in Natural Questions to the paragraphs in Wikipedia whenever possible.
Since we want to check the performance changes of our model with the growing number of tokens, we follow the same split (train/dev/test) used in Natural Questions-Open for the reading comprehension setting as well.
During the validation of our model and baseline models, we exclude samples whose answers lie in a list or a table from a Wikipedia article.

\section{Hyperparameters}\label{apdx:hyper}
We use the Adam optimizer~\citep{kingma2014adam} in all our experiments.
For training our phrase and question encoders with Eq.~\eqref{eqn:aggregate}, we use a learning rate of 3e-5 and the norm of the gradient is clipped at 1.
We use a batch size of 84 and train each model for 4 epochs for all datasets, where the loss of pre-batch negatives is applied in the last two epochs.
We use SQuAD to train our QG model\footnote{The quality of generated questions from a QG model trained on Natural Questions is worse due to the ambiguity of information-seeking questions.} and use spaCy\footnote{\url{https://spacy.io/}} for extracting named entities in each training passage, which are used to generate questions.
The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.
The number of preceding batches  is set to 2.

For the query-side fine-tuning with Eq.~\eqref{eqn:qsft}, we use a learning rate of 3e-5 and the norm of the gradient is clipped at 1.
We use a batch size of 12 and train each model for 10 epochs for all datasets.
The top  for the Eq.~\eqref{eqn:qsft} is set to 100.
While we use a single 24GB GPU (TITAN RTX) for training the phrase encoders with Eq.~\eqref{eqn:aggregate}, query-side fine-tuning is relatively cheap and uses a single 12GB GPU (TITAN Xp).
Using the development set, we select the best performing model (based on EM) for each dataset, which are then evaluated on each test set.
Since SpanBERT only supports cased models, we also truecase the questions~\citep{lita2003truecasing} that are originally provided in the lowercase (Natural Questions and WebQuestions).









\section{Reducing Storage Footprint}\label{apdx:storage}
As shown in Table~\ref{tab:category}, we have reduced the storage footprint from 1,547GB~\citep{lee2020contextualized} to 320GB.
We detail how we can reduce the storage footprint in addition to the several techniques introduced by~\citet{seo2019real}.

First, following~\citet{seo2019real}, we apply a linear transformation on the passage token representations to obtain a set of filter logits, which can be used to filter many token representations from .
This filter layer is supervised by applying the binary cross entropy with the gold start/end positions (trained together with Eq.~\eqref{eqn:aggregate}).
We tune the threshold for the filter logits on the reading comprehension development set to the point where the performance does not drop significantly while maximally filtering tokens.
In the full Wikipedia setting, we filter about 75\% of tokens and store 770M token representations.

Second, in our architecture, we use a base model (SpanBERT-base) for a smaller dimension of token representations () and does not use any sparse representations including tf-idf or contextualized sparse representations~\citep{lee2020contextualized}.
We also use the scalar quantization for storing \ttt{float32} vectors as \ttt{int4} during indexing.

Lastly, since the inference in Eq.~\eqref{eqn:formula} is purely based on MIPS, we do not have to keep the original start and end vectors which takes about 500GB.
However, when we perform query-side fine-tuning, we need the original start and end vectors for reconstructing them to compute Eq.~\eqref{eqn:qsft} since (the on-disk version of) MIPS index only returns the top- scores and their indices, but not the vectors.
 
\end{document}
