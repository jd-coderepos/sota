

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{multirow}
\usepackage{amsmath}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\newcommand{\shiyu}[1]{{\color{blue}\{{#1}\}}}

\usepackage[accepted]{icml2021}

\icmltitlerunning{TransGAN: Two Transformers Can Make One Strong GAN}

\begin{document}

\twocolumn[
\icmltitle{TransGAN: Two Transformers Can Make One Strong GAN}







\begin{icmlauthorlist}
\icmlauthor{Yifan Jiang}{UT}
\icmlauthor{Shiyu Chang}{IBM}
\icmlauthor{Zhangyang Wang}{UT}
\end{icmlauthorlist}

\icmlaffiliation{UT}{Department of Electronic and Computer Engineering, University of Texas at Austin, Texas, USA}
\icmlaffiliation{IBM}{MIT-IBM Watson AI Lab, Massachusetts, USA}

\icmlcorrespondingauthor{Yifan Jiang}{yifanjiang97@utexas.edu}


\icmlkeywords{}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
The recent explosive interest on transformers has suggested their potential to become powerful ``universal" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go - are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN \textbf{completely free of convolutions}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed \textbf{TransGAN}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets \textbf{new state-of-the-art} IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.63 IS score and 11.89 FID score on CIFAR-10, and 12.23 FID score on CelebA , respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN. The code is available at \url{https://github.com/VITA-Group/TransGAN}.
\vspace{-0.5em}
\end{abstract}


\section{Introduction}
\label{introduction}
Generative adversarial networks (GANs) have gained considerable success on numerous tasks including image synthesis \cite{radford2015unsupervised,arjovsky2017wasserstein,gulrajani2017improved,miyato2018spectral,brock2018large}, image translation \cite{isola2017image,zhu2017unpaired,zhu2017toward,chen2020distilling}, and image editing \cite{yang2019controllable,jiang2021enlightengan}. Unfortunately, GANs suffer from the notorious training instability, and numerous efforts have been devoted to stabilizing GAN training, introducing various regularization terms \cite{kurach2019large,roth2017stabilizing,zhang2019consistency,mescheder2018training}, better losses \cite{gulrajani2017improved,mao2017least,jolicoeur2018relativistic,li2017mmd}, and training recipes \cite{salimans2016improved,karras2017progressive}.

Another parallel route to improving GANs examines their \textit{neural architectures}. \cite{lucic2018gans,kurach2019large} reported a large-scale study of GANs and observed that when serving as (generator) backbones, popular neural architectures perform comparably well across the considered datasets. Their ablation study suggested that most of the variations applied in the ResNet family architectures lead to marginal improvements in the sample quality. However, further research introduced neural architecture search (NAS) to GANs and suggests that enhanced backbone designs are also important for improving GANs further, just like for other computer vision tasks. Those works are consistently able to discover stronger GAN architectures beyond the standard ResNet topology \cite{gong2019autogan,gao2020adversarialnas,tian2020off}. Other efforts include customized modules such as self-attention \cite{zhang2019self},  style-based generator \cite{karras2019style}, and autoregressive transformer-based part composition \cite{esser2020taming}.

However, there is one last ``commonsense" that seems to have seldomly been challenged: using convolutional neural networks (CNNs) as GAN backbones. The original GAN \cite{goodfellow2014generative,denton2015deep} used fully-connected networks and can only generate small images. DCGAN \cite{radford2015unsupervised} was the first to scale up GANs using CNN architectures, which allowed for stable training for higher resolution and deeper generative models. Since then, in the computer vision domain, nearly every successful GAN  relies on CNN-based generators and discriminators. Convolutions, with the strong inductive bias for natural images, crucially contribute to the appealing visual results and rich diversity achieved by modern GANs.

\textit{\textbf{Can we build a strong GAN completely free of convolutions?}} This is a question not only arising from intellectual curiosity, but also of practical relevance.  Fundamentally, a convolution operator has a local receptive field, and hence CNNs cannot process long-range dependencies unless passing through a sufficient number of layers. However, that could cause the loss of feature resolution and fine details, in addition to the difficulty of optimization. Vanilla CNN-based models (including conventional GANs) are therefore inherently not well suited for capturing an input image's ``global" statistics, as demonstrated by the benefits from adopting self-attention \cite{zhang2019self} and non-local \cite{wang2018non} operations in computer vision.

We are inspired by the emerging trend of using Transformer architectures for computer vision tasks \cite{carion2020end,zeng2020learning,dosovitskiy2020image}. Transformers \cite{vaswani2017attention,devlin2018bert} have prevailed in natural language processing (NLP), and lately, start to perform comparably or even better than their CNN competitors in a variety of vision benchmarks. The charm of the transformer to computer vision lies in at least two-fold: (1) it has strong representation capability and is free of human-defined inductive bias.  In comparison, CNNs exhibit a strong bias towards feature locality, as well as spatial invariance due to sharing filter weights across all locations;  (2) the transformer architecture is general, conceptually simple, and has the potential to become a powerful “universal” model across tasks and domains \cite{dosovitskiy2020image}. It can get rid of many ad-hoc building blocks commonly seen in CNN-based pipelines \cite{carion2020end}.







\subsection{Our Contributions}
This paper aims for the first pilot study to build a GAN completely free of convolutions, using only pure transformer-based architecture. Our ambitious goal is clearly distinguished from the previous works that only applied self-attention or transformer encoder block in conjunction with CNN-based generative models \cite{zhang2019self,esser2020taming}. However, as all previous pure transformer-based models in computer vision are focused on discriminative tasks such as classification and detection, our goal faces several daunting gaps ahead. \underline{First and foremost}, although a pure transformer architecture applied directly to sequences of image patches can perform very well on image classification tasks \cite{dosovitskiy2020image}, it is unclear whether the same way remains effective in generating images, which poses a high demand for spatial coherency in structure, color, and texture. The handful of existing transformers that output images have unanimously leveraged CNN-based part encoders \cite{esser2020taming} or convolutional feature extractors \cite{yang2020learning,chen2020pre}. \underline{Moreover}, even given well-designed CNN-based architectures, training GANs is notoriously unstable and prone to mode collapse \cite{salimans2016improved}. Training visual transformers are also known to be tedious, heavy, and data-hungry \cite{dosovitskiy2020image}. Mingling the two will undoubtedly amplify the challenges of training. 


In view of those challenges, this paper presents a coherent set of efforts and innovations towards building the \textbf{pure} transformer-based GAN architectures, dubbed \textbf{TransGAN}. A naive option may directly stack multiple transformer blocks from raw pixel inputs, but that would require prohibitive memory and computation.  Instead, we start with a memory-friendly transformer-based generator by gradually increasing the feature map resolution while decreasing the embedding dimension in each stage. The discriminator, also transformer-based, tokenizes the image patches rather than pixels as its inputs and classifies between real and fake images. This vanilla TransGAN architecture naturally inherits the advantages of the global receptive field by the self-attention, but practically it leads to degraded generation and broken visual smoothness. To close the performance gap between CNN-based GANs, we then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy with a self-supervised auxiliary loss, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Our contributions are outlined below:\vspace{-0.5em}
\begin{itemize}
    \item \textbf{Model Architecture:} We build the first GAN using purely transformers and no convolution. To avoid overwhelming memory overheads, we create a memory-friendly generator and a patch-level discriminator, both transformer-based without bells and whistles. TransGAN can be effectively scaled up to larger models.\vspace{-0.5em}
    \item \textbf{Training Technique:} We study a number of techniques to train TransGAN better, ranging from data augmentation, multi-task co-training for generator with self-supervised auxiliary loss, and localized initialization for self-attention. Extensive ablation studies, discussions, and insights are presented. None of them requires any architecture change.\vspace{-0.5em} 
    \item \textbf{Performance:} TransGAN achieves highly competitive performance compared to current state-of-the-art CNN-based GANs.  Specifically, it sets new state-of-the-art IS score of 10.10 and FID score of 25.32 on STL-10 and also reaches competitive 8.63 IS score and 11.89 FID score on CIFAR-10, and 12.23 FID score on CelebA , respectively. We also summarize the current limitations and future potential of TransGAN.\vspace{-0.5em}
\end{itemize}







\section{Relative Works}
\vspace{-0.2em}
\subsection{Generative Adversarial Networks}
\vspace{-0.2em}









GANs \cite{gui2020review} can be generalized to minimizing a large family of divergences, and are practically formulated as minimax optimization. After its origin, GANs quickly embrace fully convolutional backbones  \cite{radford2015unsupervised}, and inherited most successful designs from CNNs such as batch normalization, pooling, ReLU/Leaky ReLU and more. GANs are widely adopted in  image-to-image translation \cite{isola2017image,zhu2017unpaired}, image enhancement \cite{jiang2021enlightengan,ledig2017photo,kupyn2018deblurgan}, and image editing \cite{ouyang2018pedestrian,yu2018generative}. To alleviate its unstable training, a number of techniques have been studied, including the Wasserstein loss \cite{arjovsky2017wasserstein}, the style-based generator \cite{karras2019style}, progressive training \cite{karras2017progressive}, and Spectual Normalization \cite{miyato2018spectral}.







\subsection{Visual Transformer}
\vspace{-0.2em}
The original transformer was built for NLP \cite{vaswani2017attention}, where the multi-head self-attention and feed-forward MLP layer are stacked to capture the long-term correlation between words. Its popularity among computer vision tasks rises  recently~\cite{parmar2018image,yang2020learning,zeng2020learning,carion2020end,wu2020visual,chen2020pre}. The core of a transformer is the self-attention mechanism,  which characterizes the dependencies between any two distant tokens.  It could be viewed as a special case of non-local operations in the embedded Gaussian~\cite{wang2018non}, that captures long-range dependencies of pixels in image/video. A recent work \cite{dosovitskiy2020image} implements highly competitive ImageNet classification using pure transformers, by treating an image as a sequence of  visual words. However, its success relies on pretraining on large-scale external data. \cite{touvron2020training} improves the data efficiency for its training. Besides image classification task, transformer and its variants are also explored on image processing \cite{chen2020pre}, point cloud \cite{zhao2020point}, object detection \cite{carion2020end,zhu2020deformable} and so on. A comprehensive review is referred to \cite{han2020survey}.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/TransGAN_1.pdf}
\end{center}
\vspace{-1em}
\caption{The pipeline of the pure transform-based generator and discriminator of TransGAN. Here  and . We show 9 patches for discriminator as an example while in practice we use  patches across all datasets.}
\label{fig:TransGAN}
\end{figure*}

\subsection{Transformer Modules for Image Generation}
\vspace{-0.2em}
There exist several related works combining the transformer modules into image generation models, by replacing certain components of CNNs. \cite{parmar2018image} firstly formulated image generation as autoregressive sequence generation, for which they adopted a transformer architecture. \cite{child2019generating} propose sparse factorization of the attention matrix to reduce its complexity. While those two works did not tackle the GANs, one recent (concurrent) work \cite{esser2020taming}  used a convolutional GAN to learn a codebook of context-rich visual parts, whose composition is subsequently modeled with an autoregressive transformer architecture. The authors demonstrated success in synthesizing high-resolution images. However, the overall CNN architecture remains in place (including CNN encoder/decoder for the generators, and a fully CNN-based discriminator), and the customized designs (e.g, codebook and quantization) also limit their model's versatility. To our best knowledge, no existing work has tried to completely remove convolutions from their generative frameworks. 















\section{Technical Approach: A Journey Towards GAN with Pure Transformers}
\label{sec:techniques}
A GAN consists of a generator  and a discriminator . We start by replacing  or  with a transformer to understand the design sensitivity; then we replace both of them and optimize our design for memory efficiency. On top of the vanilla TransGAN with both  and  being transformers, we gradually introduce a series of training techniques to fix its weakness, including data augmentation, an auxiliary task for co-training, and injecting locality to self-attention. With those aids, TransGAN can be scaled up to deeper/wider models, and generate images of high quality.





\subsection{Vanilla Architecture Design for TransGAN }
\subsubsection{Transformer Encoder as Basic Block}


We choose the transformer encoder \cite{vaswani2017attention} as our basic block, and try to make minimum changes. An encoder is a composition of two parts. The first part is constructed by a multi-head self-attention module and the second part is a feed-forward MLP with GELU non-linearity. We apply layer normalization \cite{ba2016layer} before both of the two parts. Both parts employ residual connection. 






\subsubsection{Memory-friendly Generator}

Transformers in NLP taking each word as inputs \cite{devlin2018bert}. However, if we similarly generate an image in a pixel-by-pixel manner through stacking transformer encoders, even a low-resolution image (e.g. ) can result in a long sequence (1024), and then even more explosive cost of self-attention (quadratic w.r.t. the sequence length). To avoid this prohibitive cost, we are inspired by a common design philosophy in CNN-based GANs, to iteratively upscale the resolution at multiple stages \cite{denton2015deep,karras2017progressive}. Our strategy is to gradually increase the input sequence and reduce the embedding dimension. 




As shown in Figure \ref{fig:TransGAN} left, we propose a memory-friendly transformer-based generator that consists of multiple stages (default 3 for CIFAR-10). Each stage stacks several encoder blocks (5, 2, and 2 by default). By stages, we gradually increase the feature map resolution until it meets the target resolution . Specifically, the generator takes the random noise as its input, and passes it through a multiple-layer perceptron (MLP) to a vector of length . The vector is reshaped into a  resolution feature map (by default we use ), each point a -dimensional embedding. This ``feature map" is next treated as a length-64 sequence of -dimensional tokens, combined with the learnable positional encoding. 



Similar to BERT \cite{devlin2018bert}, the transformer encoders take embedding tokens as inputs and calculate the correspondence between each token recursively.  To synthesize higher resolution images, we insert an upsampling module after each stage, consisting of a reshaping and \texttt{pixelshuffle} \cite{shi2016real} module. The upsampling module firstly reshapes the 1D sequence of token embedding back to a 2D feature map  and then adopt the \texttt{pixelshuffle} module to upsample its resolution and downsample the embedding dimension, resulting in the output . After that, the 2D feature map  is again reshaped into the 1D sequence of embedding tokens where the token number becomes  and the embedding dimension is .  Therefore, at each stage the resolution  becomes  times larger, while the embedded dimension  is reduced to a quarter of the input. This trade-off mitigates the memory and computation explosion. We repeat multiple stages until the resolution reaches , and then we will project the embedding dimension to  and obtain the RGB image .



\subsubsection{Tokenized-Input For Discriminator}
Unlike the generator which needs to synthesize each pixel precisely, the discriminator is only expected to distinguish between real/fake images. This allows us to semantically tokenizing the input image into the coarser patch-level \cite{dosovitskiy2020image}. As shown in Figure \ref{fig:TransGAN} right, the discriminator takes the patches of an image as inputs. Following \cite{dosovitskiy2020image}, we split the input images  into  patches where each patch can be regarded as a ``word". The  patches are then converted to the 1D sequence of token embeddings through a linear flatten layer, with token number   and embedding dimension equal to . After that, the learnable positional encoding is added and a \texttt{[cls]} token is appended at the beginning of the 1D sequence.  After passing through the transformer encoders, only \texttt{[cls]} token is taken by the classification head to output the real/fake prediction.



\subsubsection{Evaluation of Transformer-based GAN}
\label{sec:combination}
To put the performance of transformer-based generator  and discriminator  into context, we take a reference to one of the state-of-the-art GANs,  AutoGAN \cite{gong2019autogan}, which has convolutional  and .  We study four combinations: i) AutoGAN  + AutoGAN  (i.e., original AutoGAN);  ii) Transformer  + AutoGAN ; iii) AutoGAN  + Transformer ; and iv) Transformer  + Transformer  (i.e., our vanilla TransGAN). Our transformer  has \{5,2,2\} encoder blocks in each stage and transformer  only has one stage with 7 encoder blocks. For all models, we train them on CIFAR-10 to evaluate Inception Score \cite{salimans2016improved} and FID \cite{heusel2017gans}. We try our best on tuning hyperparameters to reach their best performance, detailed setting is shown in the Appendix \ref{sec:Appendix_A}. Table \ref{table:GD} reveals a few very intriguing findings:
\begin{itemize}
\vspace{-0.5em}
    \item Transformer-based  has a strong capacity: when training with the mature AutoGAN , its performances already get on par with the original AutoGAN. That is also aligned with \cite{esser2020taming} who found putting transformers in the generator is successful.\vspace{-0.3em}  
    \item However, Transformer-based  seems to be an inferior ``competitor" and unable to push AutoGAN  towards good generation results. After replacing AutoGAN  with Transformer  , the results slightly improves, possibly benefiting more symmetric  and  structures. However, the numbers still largely lag behind when using convolutional .
    \vspace{-0.5em}
\end{itemize}
Note that although not yet a pure transformer, the promising result of Transformer  + AutoGAN  already has practical relevance - considering in most GAN applications, the discriminator is discarded after training and only the generator is kept for testing use. If one's goal is to simply obtain a transformer-based , then the goal can be fulfilled by Transformer  + AutoGAN . However, for our much more ambitious goal of making GAN completely free of convolutions, our journey has to continue.



\begin{table}[h]
\vspace{-1em}
\caption{Inception Score (IS) and FID results on CIFAR-10. The first row shows the AutoGAN results \cite{gong2019autogan}; the second and thirds row show the mixed transformer-CNN results; and the last row shows the pure-transformer GAN results.}
\vspace{-0.5em}
\label{table:GD}
\begin{center}
\begin{sc}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccr}
\toprule
Generator & Discriminator & IS& FID \\
\midrule
AutoGAN    & AutoGAN & 8.55 0.12 & \textbf{12.42} \\
\midrule
Transformer    & AutoGAN & \textbf{8.59} \textbf{0.10} & 13.23 \\

AutoGAN    & Transformer & 6.17 0.12 & 49.83 \\

Transformer    & Transformer & 6.95  0.13 & 41.41 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{center}
\vspace{-0.5em}
\end{table}







\subsection{Data Augmentation is Crucial for TransGAN}

The preliminary findings in Table \ref{table:GD} inspire us to reflect on the key barrier - it seems that  is not well trained, no matter with CNN- or transformer-based . Note the transformer-based classifiers were known to be highly data-hungry \cite{dosovitskiy2020image} due to the removal of human-designed bias: they were inferior to CNNs until much larger external data was used for pre-training. To remove this roadblock, data augmentation was revealed as a blessing in \cite{touvron2020training}, which showed that different types of strong data augmentation can lead us to data-efficient training for visual transformers. 

Traditionally, contrary to training image classifiers, training GANs hardly refers to data augmentation. Lately, there is an interest surge in training GANs in the ``few-shot" regime, aiming to match state-of-the-art GAN results with orders of magnitude fewer real images, using well-crafted data augmentation \cite{zhao2020differentiable,karras2020training}. 

We target in a different setting: comparing the influence of data augmentation for CNN- and transformer-based GANs, in the full-data regime. We use the whole training set of CIFAR-10, and compare TransGAN with three state-of-the-art CNN-based GANs: WGAN-GP \cite{gulrajani2017improved}, AutoGAN and StyleGAN v2 \cite{karras2020analyzing}. The data augmentation method is DiffAug \cite{zhao2020differentiable}.










\begin{table}[h]
\vspace{-0.5em}
\caption{The effectiveness of Data Augmentation (DA) on both CNN-based GANs and TransGAN. We used the full CIFAR-10 training set and DiffAug \cite{zhao2020differentiable}. }
\label{table:DA}
\vskip 0.05in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{lcccr}
\toprule
Methods & DA & IS   & FID  \\
\midrule
WGAN-GP &   & \textbf{6.49}  0.09 & 39.68 \\
\cite{gulrajani2017improved}     &  & 6.29  0.10 & \textbf{37.14} \\
 \midrule
AutoGAN   &  & 8.55  0.12 & \textbf{12.42} \\
 \cite{gong2019autogan}   &  &  \textbf{8.60}  \textbf{0.10} & 12.72 \\
  \midrule
 StyleGAN v2   &  & 9.18 & 11.07 \\
 \cite{zhao2020differentiable}   &  &  \textbf{9.40} & \textbf{9.89} \\
 \midrule
 \midrule
\multirow{2}{*}{TransGAN}    & & 6.95  0.13& 41.41 \\
                             &  & \textbf{8.15}  \textbf{0.14}& \textbf{19.85}\\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vspace{-0.5em}
\end{table}

As shown in Table \ref{table:DA}, for three CNN-based GANs, the performance gains of data augmentation seems to diminish in the full-data regime.  Only the largest model StyleGAN-V2 seems to gain visibly in both IS and FID. In sharp contrast, TransGAN, also trained on the same training set, sees a shockingly large margin of improvement: IS improving from  to  and from  to , respectively. That re-confirms the findings \cite{dosovitskiy2020image,touvron2020training} that transformer-based architectures are much more data-hungry than CNNs, and that can be helped by stronger data augmentation to a good extent.








\subsection{Co-Training with Self-Supervised Auxiliary Task}
\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{figures/co_training.pdf}
\end{center}
\vspace{-1em}
\caption{Co-training the transformer  with an auxiliary task of super resolution. ``LR" and ``SR" represent low-resolution input and high-resolution output respectively.}
\vspace{-0.5em}
\label{fig:co_training}
\end{figure}


Transformers in the NLP domain benefit from multiple pre-training tasks \cite{devlin2018bert,song2020mpnet}. Interestingly, adding a self-supervised auxiliary task (e.g., rotation prediction) was previously found to stabilize GAN training too \cite{chen2019self}. That makes it a natural idea to incorporate self-supervised auxiliary co-training into TransGAN, which may help it capture more image priors.





Specifically, we construct an auxiliary task of \textit{super resolution}, in addition to the GAN loss. This task comes ``for free", since we can just treat the available real images as high-resolution, and downsample them to obtain low-resolution counterparts.  The generator loss is added with a auxiliary term , where  is the mean-square-error (MSE) loss and the coefficient  is empirically set as . Figure \ref{fig:co_training} illustrates the idea of multi-task co-training (MT-CT) , and it improves TransGAN from  IS and  FID to 0 IS and  FID, respectively, as in Table \ref{table:ablation}. 

\begin{table}[h]
\vspace{-0.5em}
\caption{Ablation studies for multi-task co-training (MT-CT) and locality-aware self-attention initialization on TransGAN.}
\label{table:ablation}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Model &   IS & FID \\
\midrule
TransGAN + DA (*)     & 8.15 0.14& 19.85 \\

(*)  + MT-CT    & 8.20 0.14& 19.12 \\



(*)  + MT-CT + Local init.     & \textbf{8.22} \textbf{0.12}& \textbf{18.58} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vspace{-1em}
\end{table}


\subsection{Locality-Aware Initialization for Self-Attention}



CNN architectures have the built-in prior of natural image smoothness \cite{Ulyanov_2018_CVPR} which was believed to contribute to natural image generation. That was lacked by the transformer architecture which features full learning flexibility. However, \cite{dosovitskiy2020image} observed that transformers still tend to learn convolutional structures from images. Therefore, a meaningful question arises as, whether we can efficiently encode inductive image biases while still retaining the flexibility of transformers. \cite{esser2020taming} pursued so by keeping a convolutional architecture to encode the low-level image structure. In this paper, we show that a similar effect may be achieved without changing the pure transformer architecture at all, yet instead by warm-starting the self-attention properly.




To inject this particular prior, we introduce the locality-aware initialization for self-attention. Our specific strategy is illustrated in Figure \ref{fig:local_attention}. We introduce a mask by which each query is only allowed to interact with its local neighbors that are not ``masked". Different from previous methods~\cite{daras2020your,parmar2018image,child2019generating,beltagy2020longformer} during training we gradually reduce the mask until diminishing it, and eventually the self-attention is fully global\footnote{Implementation-wise, we control the window size for the allowable interactive neighborhood tokens. The window size is 8 for epochs 0-20, then 10 for epochs 20-30, 12 for epochs 30-40, 14 for epochs 40-50, and then the full image all afterward.}.  That strategy stems from our observation that a localized self-attention \cite{parmar2018image} is most helpful at the early training stage, but can hurt the later training stage and the final achievable performance. We consider this locality-aware initialization as a regularizer that comes for the early training dynamics and then gradually fades away \cite{golatkar2019time}. It will enforce TransGAN to learn image generation, by prioritizing on the local neighborhood first (which provides the ``necessary details"), followed by exploiting non-local interactions more broadly (which may supply more ``finer detail" and also noise). Table \ref{table:ablation} shows that it improves both IS and (more notably) FID.  





\begin{figure}[t]
\begin{center}
\vspace{-0.5em}
\includegraphics[width=\linewidth]{figures/local_attention.pdf}
\end{center}
\vspace{-1em}
\caption{Locality-aware initialization for self-attention. The red block indicates a query location, the transparent blocks are its allowable key locations to interact with, and the gray blocks indicate the masked region. TransGAN gradually increases the allowable region during the training process.}
\vspace{-0.5em}
\label{fig:local_attention}
\end{figure}








\begin{table}[h]
\vspace{-0.5em}
\caption{Scaling-up the model size of TransGAN on CIFAR-10. Here ``Dim" represents the embedded dimension of transformer and ``Depth" is the number of transformer encoder block in each stage.}
\label{table:scaling}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{lcccr}
\toprule
Model & Depth & Dim & IS  & FID  \\
\midrule
TransGAN-S    & \{5,2,2\} & 384 & 8.22  0.14& 18.58 \\

TransGAN-M   & \{5,2,2\} & 512 & 8.36  0.12& 16.27 \\

TransGAN-L    & \{5,2,2\} & 768 & 8.50  0.14& 14.46 \\

TransGAN-XL    & \{5,4,2\} & 1024 & \textbf{8.63}  \textbf{0.16} & \textbf{11.89} \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vspace{-0.5em}
\end{table}


\subsection{Scaling up to Large Models}

All previous training techniques have contributed to a better and more stable TransGAN, consisting of only transformer-based  and . Equipped with them all (DA, MT-CT, and Local Init.), we are now ready to scale TransGAN up and see how much further we could gain from bigger models. 



As shown in Table \ref{table:scaling}, we firstly enlarge the embedded dimension of the transformer-based , from (default) 384 to 512 and then 768\footnote{We find enlarging  to significantly improve performance, while enlarging  seems to have negligible impact. Therefore, we increase  size by default and keep  the same.}, and denote the resultant models as \textit{TransGAN-S}, \textit{TransGAN-M} and \textit{TransGAN-L}, respectively. That comes with a consistent and remarkable gain in IS (up to 0.28), and especially FID (up to 4.08) --  without any extra hyperparameter tuning.

We then increase the depth (number of transformer encoder blocks) on top of \textit{TransGAN-L}. The original transformer  has \{5,2,2\} encoder blocks in each stage. We increase the number of encoder blocks to \{5,4,2\} and the embedded dimension to 1024 as well, obtaining \textit{TransGAN-XL} from \textit{TransGAN-L}. Still, both IS and FID benefit, and FID is reduced by another nice margin of 2.57.

















\section{Comparison with State-of-the-art GANs}
\paragraph{Datasets and Implementation} We adopt CIFAR-10 \cite{krizhevsky2009learning} dataset as the main testbed during the ablation study. The CIFAR-10 dataset consists of 60k  color images in 10 classes, with 50k training and 10k testing images respectively. We follow the standard setting to use the 50k training images without labels for training the TransGAN. We further consider the STL-10 \cite{coates2011analysis} and CelebA \cite{liu2015faceattributes} datasets to scale up TransGAN to higher resolution image generation tasks. For the STL-10 dataset, we use both the 5k training images and 100k unlabeled images for training TransGAN, with each image at  resolution.  For the CelebA dataset, we use 200k unlabeled face images (aligned and cropped version), and all are resized to  resolution.

We follow the training setting of WGAN \cite{arjovsky2017wasserstein}, and use the WGAN-GP loss~\cite{gulrajani2017improved}. We adopt a learning rate of  for both generator and discriminator, an Adam optimizer, and a batch size of 128 for generator and 64 for discriminator. For higher resolution (), we decrease the batch sizes to 32 (generator) and 16 (discriminator) to fit the GPU memory. 
Using 4 V100 GPUs, our training takes 2 days on CIFAR-10, and 3 days on STL-10 and CelebA. We focus on the unconditional image generation setting for simplicity. 

\subsection{Results on CIFAR-10}
\label{sec:cifar10}
We compare TransGAN with recently published results by ConvNet-based GANs on the CIFAR-10 dataset, shown in Table \ref{table:cifar10}. The results are all collected from the original papers with their best hand-tuned training settings. 

As shown in Table \ref{table:cifar10}, TransGAN surpasses the strong model of AutoGAN~\cite{gong2019autogan}, and many other latest competitors such as SN-GAN~\cite{miyato2018spectral}, improving MMD-GAN~\cite{wang2018improving}, and MGAN~\cite{hoang2018mgan}, in terms of inception score (IS).It is only next to the huge and heavily engineered Progressive GAN~\cite{karras2017progressive} and StyleGAN v2~\cite{karras2020analyzing}. Once we look at the FID results, TransGAN is even found to outperform Progressive GAN~\cite{karras2017progressive}, while being slightly inferior to StyleGAN v2~\cite{karras2020analyzing}. 

Examples generated on CIFAR-10 are shown in Figure \ref{fig:visual_results}, from which we observe pleasing visual details and diversity. 




\begin{table}[!t]
\caption{Unconditional image generation results on CIFAR-10.}
\label{table:cifar10}
\centering
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.99\linewidth}{!}{
\begin{tabular} {l|c|cc}
\toprule
Methods & IS & FID \\
\midrule
\midrule
WGAN-GP     &      \multirow{2}{*}{6.49  0.09}      &   \multirow{2}{*}{39.68} \\
\cite{gulrajani2017improved}    &          &     \\
\hline
LRGAN & \multirow{2}{*}{7.17  0.17} & \multirow{2}{*}{-}\\
\cite{yang2017lr} &          &     \\
\hline
DFM & \multirow{2}{*}{7.72  0.13} & \multirow{2}{*}{-}\\
\cite{warde2016improving} &          &     \\
\hline
Splitting GAN & \multirow{2}{*}{7.90  0.09} & \multirow{2}{*}{-}\\
\cite{grinblat2017class} &          &     \\
\hline
Improving MMD-GAN& \multirow{2}{*}{8.29} & \multirow{2}{*}{16.21}\\
 \cite{wang2018improving}&          &     \\
\hline
MGAN& \multirow{2}{*}{8.33  0.10} & \multirow{2}{*}{26.7}\\
 \cite{hoang2018mgan}&          &     \\
\hline
SN-GAN     &      \multirow{2}{*}{8.22  0.05}      &   \multirow{2}{*}{21.7} \\
\cite{miyato2018spectral}    &          &     \\
\hline
Progressive-GAN & \multirow{2}{*}{8.80  0.05} & \multirow{2}{*}{15.52} \\
\cite{karras2017progressive}    &          &     \\
\hline
AutoGAN & \multirow{2}{*}{8.55  0.10} &\multirow{2}{*}{12.42}\\
\cite{gong2019autogan} &          &     \\
\hline
StyleGAN V2 & \multirow{2}{*}{\textbf{9.18}} &\multirow{2}{*}{\textbf{11.07}}\\
 \cite{zhao2020differentiable}    &          &     \\
\midrule
\midrule
\multirow{2}{*}{TransGAN-XL}& \multirow{2}{*}{8.63  0.16} &\multirow{2}{*}{11.89}\\
    &          &     \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vspace{-1em}
\end{table}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/Visual_results.pdf}
\end{center}
\vspace{-1em}
\caption{Visual Results of TransGAN on CIFAR-10, STL-10, and CelebA .}
\vspace{-1em}
\label{fig:visual_results}
\end{figure*}

\subsection{Results on STL-10}
We then apply TransGAN on another popular benchmark STL-10, which is larger in scale and higher in resolution. To this end, we increase the input feature map of the generator's first stage from  to  to fit the target resolution. We use our TransGAN-XL, and compare it with both the automatic searched and hand-crafted ConvNet-based GANs, shown in Table \ref{table:stl10}. 

Different from the results on CIFAR-10, we find that TransGAN outperforms all current ConvNet-based GAN models, and sets \textbf{new state-of-the-art} results in terms of both IS and FID score.
This is thanks to the fact that the STL-10 dataset size is  larger than CIFAR-10, re-confirming our assumption that transformer-based architectures benefit much more notably from larger-scale data.
The visual examples generated on STL-10 are shown in Figure \ref{fig:visual_results}.





\begin{table}[!t]
\caption{Unconditional image generation results on STL-10.}
\label{table:stl10}
\centering
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.99\linewidth}{!}{
\begin{tabular} {l|c|cc}
\toprule
Methods & IS  & FID  \\
\midrule
\midrule
DFM  & \multirow{2}{*}{8.51  0.13}   & \multirow{2}{*}{-} \\
\cite{warde2016improving} &          &     \\
\hline
D2GAN     &      \multirow{2}{*}{7.98}      &   \multirow{2}{*}{-} \\
\cite{nguyen2017dual}    &          &     \\
\hline
ProbGAN  & \multirow{2}{*}{8.87  0.09}   & \multirow{2}{*}{47.74} \\
\cite{he2019probgan} &          &     \\
\hline
Dist-GAN     &      \multirow{2}{*}{-}      &   \multirow{2}{*}{36.19} \\
\cite{tran2018dist}    &          &     \\
\hline
SN-GAN     &      \multirow{2}{*}{9.16  0.12}      &   \multirow{2}{*}{40.1} \\
\cite{miyato2018spectral}    &          &     \\
\hline
Improving MMD-GAN& \multirow{2}{*}{9.23  0.08} & \multirow{2}{*}{37.64}\\
 \cite{wang2018improving}&          &     \\
\hline
AutoGAN & \multirow{2}{*}{9.16  0.12} &\multirow{2}{*}{31.01}\\
\cite{gong2019autogan} &          &     \\
\hline
AdversarialNAS-GAN & \multirow{2}{*}{9.63  0.19} &\multirow{2}{*}{26.98}\\
\cite{gao2020adversarialnas} &          &     \\
\midrule
\midrule
\multirow{2}{*}{TransGAN-XL}& \multirow{2}{*}{\textbf{10.10}  \textbf{0.17}} &\multirow{2}{*}{\textbf{25.32}}\\
    &          &     \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\vspace{-1em}
\end{center}
\end{table}


\subsection{Generation on Higher Resolution}
As TransGAN shows to benefit from more data samples, we now examine another challenge for TransGAN: scaling up to higher-resolution, using the more challenging CelebA dataset ( resolution). To synthesize  output images, we increase the stage number of the generator from  to , to let the new generator contain  encoder blocks in each stage. Detailed network configurations are in the Appendix \ref{sec:Appendix_B}. We do \textbf{not} tune any other hyperparameter.

On this new testbed, TransGAN-XL reaches a FID score of 12.23, and its visual results are shown in Figure \ref{fig:visual_results}, which appear to be natural, visually pleasing and diverse in details. In comparison, we also train another ConvNet-based GAN baseline on the same dataset, DCGAN \cite{radford2015unsupervised}, using the TTUR algorithm \cite{heusel2017gans}. That baseline achieves a slightly inferior 12.50 FID score. 

Although stronger ConvNet-based GANs can achieve better FID scores, we believe that the performance of TransGAN can also be boosted more by tuning its training recipe more specifically for the higher-resolution cases, and we will continue to push forward on this direction. 















\section{Conclusions, Limitations, and Discussions}

We have proposed TransGAN, a new GAN paradigm based on pure transformers. We have carefully crafted the architectures and thoughtfully designed training techniques. As a result, TransGAN has achieved comparable performance to some state-of-the-art CNN-based GAN methods across multiple popular datasets. We show that the traditional reliance on CNN backbones and many specialized modules may not be necessary for GANs, and pure transformers can be sufficiently capable for image generation. 

The pure transformer-based architecture brings versatility to TransGAN. As we turn from specialized to general-purpose architectures, one strong motivation is to simplify and unify various task pipelines, so one general suite of models could be extensively reused by many applications, avoiding the need of re-inventing wheels. There is an appealing potential that many associated techniques developed for improving transformers, originally proposed for NLP applications, could become available to TransGAN as well. 

Building a GAN using only transformers appears to be more challenging than other transformer-based vision models \cite{dosovitskiy2020image}, due to the higher bar for realistic image generation (compared to classification) and the high instability of GAN training itself. Considering that existing transformer-based models \cite{carion2020end,dosovitskiy2020image} are mostly on par or slightly inferior to their strongest CNN competitors (assuming no extra data used), we find TransGAN to provide an encouraging starting point. 

Still, there is a large room for TransGAN to improve further, before it can outperform the best hand-designed GANs with more margins. We point out a few specific items that call for continuing efforts:
\begin{itemize}
\vspace{-1em}
    \item More sophisticated tokenizing for both  and , e.g. using some semantic grouping \cite{wu2020visual}.\vspace{-0.3em}
    \item Pre-training transformers using pretext tasks \cite{dai2020up}, which may improve over our current MT-CT.\vspace{-0.3em}
    \item Stronger attention forms, e.g.,  \cite{zhu2020deformable}.\vspace{-0.3em}
    \item More efficient self-attention forms \cite{wang2020linformer,choromanski2020rethinking}, which not only help improve the model efficiency, but also save memory costs and hence help higher-resolution generation.\vspace{-0.5em}
    \item Conditional image generation \cite{lin2019coco}. 
\end{itemize}




\bibliography{example_paper}
\bibliographystyle{icml2021}

\appendix
\section{Implementation Details}
\subsection{Specific Setting of Transformer/CNN Combinations}
\label{sec:Appendix_A}
In Sec. \ref{sec:combination}, we evaluate the performance of transformer-based G and D by studying four combinations: i) AutoGAN  + AutoGAN  (original AutoGAN);  ii) Transformer  + AutoGAN ; iii) AutoGAN  + Transformer ; and iv) Transformer  + Transformer  (TransGAN-S). Since the best hyperparameter setting for each combination is different, we list the specific setting in Table \ref{table:combination_setting}. The most noticeable difference is that we apply hinge loss for AutoGAN  following the original paper \cite{gong2019autogan}, and switch to WGAN-GP \cite{gulrajani2017improved} loss when applying Transformer  as the discriminator. This is due to that AutoGAN  contains Spectral Normalization (SN) \cite{miyato2018spectral} layer but Transformer  does not. And we apply WGAN-GP loss for Transformer  to achieve the similar goal, the Lipschitz constraint. 
Here we do not make any change of the AutoGAN architectures. We apply Adam optimizer for all four combinations.

\begin{table}[h]
\caption{Detailed setting of four combinations.}
\label{table:combination_setting}
\vskip 0.15in
\centering
\begin{center}
\begin{small}
\resizebox{0.99\linewidth}{!}{
\begin{tabular} {c|c|c|c|c}
\toprule
Combination & Loss &  lr & IS & FID \\
\midrule
i)  & Hinge Loss &  & 8.55 & \textbf{12.42}\\
ii)  & Hinge Loss &   & \textbf{8.59} & 13.23\\
iii)  & WGAN-GP &  & 6.17 & 49.83\\
iv)  & WGAN-GP &   & 6.95 & 41.41\\
\bottomrule
\end{tabular}
}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Experiments on STL-10 and CelebA 64 x 64}
Due to that STL-10 and CelebA datasets contain higher resolution images (e.g.,  and ), the memory cost of TransGAN will also increase. As discussed in Sec. \ref{sec:techniques}, since increasing the model size of TransGAN's generator shows a more significant improvement than the multi-task co-training (MT-CT) strategy, we remove the MT-CT loss in order to save the memory for a larger model.

\section{Network Architecture}
\label{sec:Appendix_B}
We include the specific architecture configurations of TransGAN-XL in Table \ref{table:TransGAN_G} and \ref{table:TransGAN_D}. Here we take the architecture used for CIFAR-10 dataset as an example to describe the detailed configuration. The ``Encoder" represents the basic Transformer Encoder block constructed by self-atention, LayerNormalization, and Feed-forward MLP. PixelShuffle layer is adopted for feature map upsampling. The ``input\_shape" and ``output\_shape" denotes the shape of input feature map and output feature map, respectively.
\begin{table}[h]
\caption{Specific configuration of the generator of TransGAN-XL}
\label{table:TransGAN_G}
\centering
\begin{center}
\begin{small}
\resizebox{\linewidth}{!}{
\begin{tabular}{ c|c|c|c }
\toprule
Stage & Layer Type & input\_shape & output\_shape \\ 
\hline
- & MLP & 1024 &  \\ \hline
\multirow{5}{*}{1} & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ \hline

\multirow{5}{*}{2} & PixelShuffle &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\  \hline
\multirow{3}{*}{3} & PixelShuffle &  &  \\
 & Encoder &  &  \\ 
 & Encoder &  &  \\  \hline
- & Linear Layer &  &  \\ 
\bottomrule
\end{tabular}
}
\end{small}
\end{center}
\end{table}

\begin{table}[h]
\caption{Specific configuration of the discriminator of TransGAN-XL}
\label{table:TransGAN_D}
\centering
\begin{center}
\begin{small}
\resizebox{\linewidth}{!}{
\begin{tabular}{ c|c|c|c }
\toprule
Stage & Layer Type & input\_shape & output\_shape \\ 
\hline
- & Linear Flatten &  &  \\ 
\hline
\multirow{7}{*}{1} & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 & Encoder &  &  \\ 
 \hline
- & - &  &  \\ \hline
- & Classification Head &  & 1 \\ 
\hline
\end{tabular}
}
\end{small}
\end{center}
\end{table}







\section{Data Augmentation Strategy}
\label{sec:Appendix_E}
We mainly follow the way of differentiable augmentation \cite{zhao2020differentiable} to apply the data augmentation on our GAN training framework. Specifically, we conduct \{, , \} augmentation for TransGAN with probability , while  is empirically set to be \{, , \}. However, we find that  augmentation will hurt the performance of ConvNet-based GAN when 100\% data is utilized. Therefore, we remove it and only conduct \{, \} augmentation for AutoGAN \cite{gong2019autogan}.


\section{Efficiency Comparison with ConvNet-based GANs}
\label{sec:Appendix_F}
We further compare the computational cost of TransGAN with current state-of-the-art ConvNet-based GANs, including both the smallest model TransGAN-S and largest model TransGAN-XL. As shown in Table \ref{table:efficiency}, we firstly compare TransGAN-S with SN-GAN \cite{miyato2018spectral}, since they achieve similar performance. Specifically, TransGAN-s reaches a similar IS score and better FID score comparing with SN-GAN, while only costs about half FLOPs of SN-GAN. We then compare our largest model TransGAN-XL with AutoGAN \cite{gong2019autogan} and Progressive-GAN \cite{karras2017progressive}. TransGAN-XL reaches the best FID score with much smaller FLOPs compared to Progressive GAN and slightly larger FLOPs (1.06G) compared to AutoGAN.
\begin{table}[h]
\caption{Efficiency comparison between TransGAN and ConvNet-based GANs. Results are reported on CIFAR-10 dataset with  resolution.}
\label{table:efficiency}
\centering
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.99\linewidth}{!}{
\begin{tabular} {l|c|c|cc}
\toprule
Methods & Flops (G)& IS & FID \\
\midrule
\midrule
SNGAN & \multirow{2}{*}{1.57} & \multirow{2}{*}{8.22  0.05} &\multirow{2}{*}{21.7}\\
\cite{miyato2018spectral} &          &     \\
\hline
\multirow{2}{*}{\textbf{TransGAN-S}} & \multirow{2}{*}{\textbf{0.68}} & \multirow{2}{*}{\textbf{8.22}  \textbf{0.14}} &\multirow{2}{*}{\textbf{18.58}}\\
    &          &     \\
\midrule
\midrule
AutoGAN & \multirow{2}{*}{\textbf{1.77}} & \multirow{2}{*}{8.55  0.10} &\multirow{2}{*}{12.42}\\
\cite{gong2019autogan} &          &     \\
\hline
Progressive-GAN & \multirow{2}{*}{6.39} & \multirow{2}{*}{\textbf{8.80}  \textbf{0.05}} &\multirow{2}{*}{15.52}\\
\cite{karras2017progressive} &          &     \\
\hline
\multirow{2}{*}{\textbf{TransGAN-XL}} & \multirow{2}{*}{2.83}& \multirow{2}{*}{8.63  0.16} &\multirow{2}{*}{\textbf{11.89}}\\
    &          &     \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\end{document}
