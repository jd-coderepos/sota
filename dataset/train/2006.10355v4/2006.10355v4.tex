
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}


\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      


\usepackage{subfig}
\usepackage[english]{babel}
\usepackage{threeparttable}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{natbib}
\setcitestyle{sort,num}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{amssymb}
\usepackage{algpseudocode}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage{wrapfig}
\usepackage{xcolor}
\newtheorem{prop}{Proposition}
\newcommand{\xt}[1]{\textcolor{orange}{\scriptsize Xiaocheng: #1}}
\newcommand{\wrc}[1]{\textcolor{blue}{\scriptsize Ruochen: #1}}

\title{DrNAS: \\
Dirichlet Neural Architecture Search}



\author{
Xiangning Chen\textsuperscript{1}\thanks{Equal Contribution.}
\enskip Ruochen Wang\textsuperscript{1}\textsuperscript{}
\enskip Minhao Cheng\textsuperscript{1}\textsuperscript{}
\enskip Xiaocheng Tang\textsuperscript{2}
\enskip Cho-Jui Hsieh\textsuperscript{1}\\
\textsuperscript{1}Department of Computer Science, UCLA, \enskip \textsuperscript{2}DiDi AI Labs\\
\small{\texttt{\{xiangning, chohsieh\}@cs.ucla.edu}} \quad \small{\texttt{\{ruocwang, mhcheng\}@ucla.edu}}\\
\small{\texttt{xiaochengtang@didiglobal.com}}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
This paper proposes a novel differentiable architecture search method by formulating it into a distribution learning problem. We treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirichlet parameters can be easily optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. Specifically, we obtain a test error of 2.46\% for CIFAR-10, 23.7\% for ImageNet under the mobile setting. On NAS-Bench-201, we also achieve state-of-the-art results on all three datasets and provide insights for the effective design of neural architecture search algorithms.
\end{abstract}



\section{Introduction}
Recently, Neural Architecture Search (NAS) has attracted lots of attentions for its potential to democratize deep learning. For a practical end-to-end deep learning platform, NAS plays a crucial role in discovering task-specific architecture depending on users' configurations (e.g., dataset, evaluation metric, etc.).
Pioneers in this field develop prototypes based on reinforcement learning~\citep{nas}, evolutionary algorithms~\citep{amoebanet} and Bayesian optimization~\citep{pnas}. These works usually incur large computation overheads, which make them impractical to use.
More recent algorithms significantly reduce the search cost including one-shot methods~\citep{enas, oneshot}, a continuous relaxation of the space~\citep{darts} and network morphisms~\citep{morphisms}. 
In particular,~\citet{darts} proposes a differentiable NAS framework - DARTS, converting the categorical operation selection problem into learning a continuous architecture mixing weight. They formulate a bi-level optimization objective, allowing the architecture search to be efficiently performed by a gradient-based optimizer.

While current differentiable NAS methods achieve encouraging results, they still have shortcomings that hinder their real-world applications.
Firstly, 
several works have cast doubt on the stability and generalization of these differentiable NAS methods~\citep{smoothdarts, understanding}. They discover that directly optimizing the architecture mixing weight is prone to overfitting the validation set and often leads to distorted structures, e.g., searched architectures dominated by parameter-free operations.
Secondly, there exist disparities between the search and evaluation phases, where proxy tasks are usually employed during search with smaller datasets or shallower and narrower networks, due to the large memory consumption of differentiable NAS.





In this paper, we propose an effective approach that addresses the aforementioned shortcomings named Dirichlet Neural Architecture Search (DrNAS).
Inspired by the fact that directly optimizing the architecture mixing weight is equivalent to performing point estimation (MLE/MAP) from a probabilistic perspective, we formulate the differentiable NAS as a distribution learning problem instead, which naturally induces stochasticity and encourages exploration.
Making use of the probability simplex property of the Dirichlet samples, DrNAS models the architecture mixing weight as random variables sampled from a parameterized Dirichlet distribution. Optimizing the Dirichlet objective can thus be done efficiently in an end-to-end fashion, by employing the pathwise derivative estimators to compute the gradient of the distribution~\citep{pathwise}.
A straightforward optimization, however, turns out to be problematic due to the uncontrolled variance of the Dirichlet, i.e., too much variance leads to training instability and too little variance suffers from insufficient exploration.
In light of that, we apply an additional distance regularizer directly on the Dirichlet concentration parameter to strike a balance between the exploration and the exploitation.
We further derive a theoretical bound showing that the constrained distributional objective promotes stability and generalization of architecture search by implicitly controlling the Hessian of the validation error.






Furthermore, to enable a direct search on large-scale tasks, we propose a progressive 
learning scheme, eliminating the gap between the search and evaluation phases.
Based on partial channel connection~\citep{pcdarts}, we maintain a task-specific super-network of the same depth and number of channels as the evaluation phase throughout searching.
To prevent loss of information and instability induced by partial connection, we divide the search phase into multiple stages and progressively increase the channel fraction via network transformation~\citep{net2net}. 
Meanwhile, we prune the operation space according to the learnt distribution to maintain the memory efficiency.


We conduct extensive experiments on different datasets and search spaces to demonstrate DrNAS's effectiveness. Based on the DARTS search space~\citep{darts}, we achieve an average error rate of 2.46\% on CIFAR-10, which ranks top amongst NAS methods.
Furthermore, DrNAS achieves superior performance on large-scale tasks such as ImageNet. It obtains a top-1/5 error of 23.7\%/7.1\%, surpassing the previous state-of-the-art (24.0\%/7.3\%) under the mobile setting.
On NAS-Bench-201~\citep{nasbench201}, we also set new state-of-the-art results on all three datasets with low variance.
Our code is available at \url{https://github.com/xiangning-chen/DrNAS}.


\section{The Proposed Approach}
In this section, we first briefly review differentiable NAS setups and generalize the formulation to motivate distribution learning.
We then layout our proposed DrNAS and describe its optimization in section~\ref{subsec:drnas}.
In section~\ref{subsec:hessian}, we provide a generalization result by showing that our method implicitly regularizes the Hessian norm over the architecture parameter.
The progressive architecture learning method that enables direct search is then described in section~\ref{subsec:pal}.

\subsection{Preliminaries: Differentiable Architecture Search}
\paragraph{Cell-Based Search Space}
The cell-based search space is constructed by replications of normal and reduction cells~\citep{nasnet,darts}.
A normal cell keeps the spatial resolution while a reduction cell halves it but doubles the number of channels.
Every cell is represented by a DAG with  nodes and  edges, where every node represents a latent representation  and every edge  is associated with an operations  (e.g., \textit{max pooling} or \textit{convolution}) selected from a predefined candidate space .
The output of a node is a summation of all input flows, i.e., , and a concatenation of intermediate node outputs, i.e., , composes the cell output, where the first two input nodes  and  are fixed to be the outputs of previous two cells.

\paragraph{Gradient-Based Search via Continuous Relaxation}
To enable gradient-based optimization,~\citet{darts}
apply a continuous relaxation to the discrete space.
Concretely, the information passed from node  to node  is computed by a weighted sum of all operations alone the edge, forming a mixed-operation .
The operation mixing weight  is defined over the probability simplex and its magnitude represents the strength of each operation.
Therefore, the architecture search can be cast as selecting the operation associated with the highest mixing weight for each edge.
To prevent abuse of terminology, we refer to  as the architecture/operation mixing weight, and concentration parameter  in DrNAS as the architecture parameter throughout the paper.

\paragraph{Bilevel-Optimization with Simplex Constraints}
With continuous relaxation, the network weight  and operation mixing weight  can be jointly optimized by solving a constraint bi-level optimization problem:

where the simplex constraint  can be either solved explicitly via Lagrangian function~\citep{gaea}, or eliminated by substitution method (e.g., )~\citep{darts}.
In the next section we describe how this generalized formulation motivates our method.

\subsection{Differentiable Architecture Search as Distribution Learning}
\label{subsec:drnas}
\paragraph{Learning a Distribution over Operation Mixing Weight}
Previous differentiable architecture search methods view the operation mixing weight  as learnable parameters that can be directly optimized~\citep{darts, pcdarts, gaea}.
This has been shown to cause  to overfit the validation set and thus induce large generalization error~\citep{understanding,nasbench1shot1,smoothdarts}. 
We recognize that this treatment is equivalent to performing point estimation (e.g., MLE/MAP) of  in probabilistic view, which is inherently prone to overfitting~\citep{prml, bayesian}.
Furthermore, directly optimizing  lacks sufficient exploration in the search space, and thus cause the search algorithm to commit to suboptimal paths in the DAG that converges faster at the beginning but plateaus quickly~\citep{WideShallow}.

Based on these insights, we formulate the differentiable architecture search as a distribution learning problem.
The operation mixing weight  is treated as random variables sampled from a learnable distribution.
Formally, let  denote the distribution of  parameterized by .
The bi-level objective is then given by:


where  is a distance function.
Since  lies on the probability simplex, we select Dirichlet distribution to model its behavior, i.e., , where  represents the Dirichlet concentration parameter.
Dirichlet distribution is a widely used distribution over the probability simplex~\citep{drvae, lda, ndp, bcl}, and it enjoys nice properties that enables gradient-based training~\citep{pathwise}.

The concentration parameter  controls the sampling behavior of Dirichlet distribution and is crucial in balancing exploration and exploitation during the search phase.
Let  denote the concentration parameter assign to operation .
When  for most , Dirichlet tends to produce sparse samples with high variance, reducing the training stability; when  for most , the samples will be dense with low variance, leading to insufficient exploration.
Therefore, we add a penalty term in the objective (\ref{eq:cop}) to regularize the distance between  and the anchor , which corresponds to the symmetric Dirichlet.
In section~\ref{subsec:hessian}, we also derive a theoretical bound showing that our formulation additionally promotes stability and generalization of the architecture search by implicitly regularizing the Hessian of validation loss w.r.t. architecture parameters.

\paragraph{Learning Dirichlet Parameters via Pathwise Derivative Estimator}
Optimizing objective (\ref{eq:cop}) with gradient-based methods requires back-propagation through stochastic nodes of Dirichlet samples.
The commonly used reparameterization trick does not apply to Dirichlet distribution,
therefore we approximate the gradient of Dirichlet samples via pathwise derivative estimators~\citep{pathwise}

where  and  denote the CDF and PDF of beta distribution respectively,  is the indicator function, and  is the sum of concentrations.
 is the iregularised incomplete beta function, for which its gradient can be computed by simple numerical approximation.
We refer to~\citep{pathwise} for the complete derivations.

\paragraph{Joint Optimization of Model Weight and Architecture Parameter}
With pathwise derivative estimator, the model weight  and concentration  can be jointly optimized with gradient descent.
Concretely, we draw a sample   for every forward pass, and the gradients can be obtained easily through backpropagation.
Following DARTS~\citep{darts}, we approximate  in the lower level objective of \eqref{eq:cop} with one step of gradient descent, and run alternative updates between  and .

\paragraph{Selecting the Best Architecture}
At the end of the search phase, a learnt distribution of operation mixing weight is obtained.
We then select the best operation for each edge by the most likely operation in expectation:

In the Dirichlet case, the expectation term is simply the Dirichlet mean .
Note that under the distribution learning framework, we are able to sample a wide range of architectures from the learnt distribution.
This property alone has many potentials.
For example, in practical settings where both accuracy and latency are concerned, the learnt distribution can be used to find architectures under resource restrictions in a post search phase.
We leave these extensions to future work. 

\subsection{The implicit Regularization on Hessian}
\label{subsec:hessian}
It has been observed that the generalization error of differentiable NAS is highly related to the dominant eigenvalue of the Hessian of validation loss w.r.t. architecture parameter.
Several recent works report that the large dominant eigenvalue of  in DARTS results in poor generalization performance~\citep{understanding,smoothdarts}.
Our objective (\ref{eq:cop}) is the Lagrangian function of the following constraint objective:

Here we derive an approximated lower bound based on (\ref{eq:co}), which demonstrates that our method implicitly controls this Hessian matrix.
\begin{prop}
    \label{prop:hessian}
    Let  and  in the bi-level formulation (\ref{eq:co}).
    Let  denote the mean under the Laplacian approximation of Dirichlet.
    If  is Positive Semi-definite, 
    the upper-level objective can be approximated bounded by:
    
    with:
    
\end{prop}
This proposition is driven by the Laplacian approximation to the Dirichlet distribution~\citep{laplace,topic}.
The lower bound (\ref{eq:bound}) indicates that minimizing the expected validation loss controls the trace norm of the Hessian matrix.
Empirically, we observe that DrNAS always maintains the dominant eigenvalue of Hessian at a low level (Appendix~\ref{app:hessian_plot}).
The detailed proof can be found in Appendix~\ref{app:hessian}.


\subsection{Progressive Architecture Learning}
\label{subsec:pal}
The GPU memory consumption of differentiable NAS methods grows linearly with the size of operation candidate space. 
Therefore, they usually use a easier proxy task such as training with a smaller dataset, or searching with fewer layers and number of channels~\citep{proxylessnas}. 
For instance, the architecture search is performed on 8 cells and 16 initial channels in DARTS~\citep{darts}.
But during evaluation, the network has 20 cells and 36 initial channels.
Such gap makes it hard to derive an optimal architecture for the target task~\citep{proxylessnas}.


PC-DARTS~\citep{pcdarts} proposes a partial channel connection to reduce the memory overheads of differentiable NAS, where they only send a random subset of channels to the mixed-operation while directly bypassing the rest channels in a shortcut. However, their method causes loss of information and makes the selection of operation unstable since the sampled subsets may vary widely across iterations. 
This drawback is amplified when combining with the proposed method since we learn the architecture distribution from Dirichlet samples, which already injects certain stochasticity.
As shown in Table~\ref{tab:partial channel}, when directly applying partial channel connection with distribution learning,
the test accuracy of the searched architecture  decreases over 3\% and 18\% on CIFAR-10 and CIFAR-100 respectively if we send only 1/8 channels to the mixed-operation.



To alleviate such information loss and instability problem while being memory-efficient, we propose a progressive learning scheme which gradually increases the fraction of channels that are forwarded to the mixed-operation and meanwhile prunes the operation space based on the learnt distribution.
We split the search process into consecutive stages and construct a task-specific super-network with the same depth and number of channels as the evaluation phase at the initial stage.
Then after each stage, we increase the partial channel fraction, which means that the super-network in the next stage will be wider, i.e., have more convolution channels, and in turn preserve more information.
This is achieved by enlarging every convolution weight with a random mapping function similar to Net2Net~\citep{net2net}. The mapping function  with  is defined as 

To widen layer , we replace its convolution weight  with a new weight .

where  denote the number of output and input channels, filter height and width respectively.
Intuitively, we copy  directly into  and fulfill the rest part by choosing randomly as defined in . 
Unlike Net2Net, we do not divide  by a replication factor here because the information flow on each edge has the same scale no matter the partial fraction is.
After widening the super-network, we reduce the operation space by pruning out less important operations according to the Dirichlet concentration parameter  learnt from the previous stage, maintaining a consistent memory consumption.
As illustrated in Table~\ref{tab:partial channel}, the proposed progressive architecture learning scheme effectively discovers high accuracy architectures and retains a low GPU memory overhead.


\begin{wraptable}{r}{0.4\textwidth}
\vspace{-4mm}
    \centering
\caption{Test accuracy of the derived architectures when searching on NAS-Bench-201 with different partial channel fraction, where  channels are sent to the mixed-operation.}
    \resizebox{.4\textwidth}{!}{
    \begin{tabular}{ccc}
    
    \hline
    \multicolumn{3}{c}{CIFAR-10} \\
     & \textbf{\tabincell{c}{Test Accuracy\MB)}} \\ \hline
    1 &  & 2437 \\
    2 &  & 1583 \\
    4 &  & 1159 \\
    8 &  & 949 \\
    Ours &  & 949 \\ \hline \hline
    
    \multicolumn{3}{c}{CIFAR-100} \\
     & \textbf{\tabincell{c}{Test Accuracy\MB)}} \\ \hline
    1 &  & 2439 \\
    2 &  & 1583 \\
    4 &  & 1161 \\
    8 &  & 949 \\
    Ours &  & 949 \\ \hline
    
    \end{tabular}}
    \label{tab:partial channel}
    \vspace{-3mm}
\end{wraptable}


\section{Discussions and Relationship to Prior Work}
Early methods in NAS usually include a full training and evaluation procedure every iteration as the inner loop to guide the consecutive search~\citep{nas,nasnet,amoebanet}.
Consequently, their computational overheads are beyond acceptance for practical usage, especially on large-scale tasks.

\paragraph{Differentiable NAS} Recently, many works are proposed to improve the efficiency of NAS~\citep{enas,morphisms,darts,oneshot,nasp,sif,Mei2020AtomNAS}.
Amongst them, DARTS~\citep{darts} proposes a differentiable NAS framework, which introduces a continuous architecture parameter that relaxes the discrete search space. Despite being efficient, DARTS only optimizes a single point on the simplex every search epoch, which has no guarantee to generalize well after the discretization during evaluation. So its stability and generalization have been widely challenged~\citep{randomnas,understanding,smoothdarts,wang2021rethinking}.
Following DARTS, SNAS~\citep{snas} and GDAS~\citep{gdas} leverage the gumbel-softmax trick to learn the exact architecture parameter.
However, their reparameterization is motivated from reinforcement learning perspective, which is an approximation with softmax rather than an architecture distribution.
Besides, their methods require tuning of temperature schedule~\citep{hman, mann}. GDAS linearly decreases the temperature from 10 to 1 while SNAS anneals it from 1 to 0.03. In comparison, the proposed method can automatically learn the architecture distribution without the requirement of handcrafted scheduling.
BayesNAS~\citep{BayesNAS} applies Bayesian Learning in NAS. Specifically, they cast NAS as model compression problem and use Bayes Neural Network as the super-network, which is difficult to optimize and requires oversimplified approximation. While our method considers the stochasticity in architecture mixing weight, as it is directly related to the generalization of differentiable NAS algorithms~\citep{understanding, smoothdarts}.

\paragraph{Memory overhead} When dealing with the large memory consumption of differentiable NAS, previous works mainly restrain the number of paths sampled during the search phase.
For instance, ProxylessNAS~\citep{proxylessnas} employs binary gates and samples two paths every search epoch.
PARSEC~\citep{parsec} samples discrete architectures according to a categorical distribution to save memory.  
Similarly, GDAS~\citep{gdas} and DSNAS~\citep{dsnas} both enforce a discrete constraint after the gumbel-softmax reparametrization. However, such discretization manifests premature convergence and cause search instability~\citep{nasbench1shot1, mmf}. Our experiments in section~\ref{sec:201} also empirically demonstrate this phenomenon. 
As an alternative, PC-DARTS~\citep{pcdarts} proposes a partial channel connection, where only a portion of channels is sent to the mixed-operation.
However, partial connection can cause loss of information as shown in section~\ref{subsec:pal} and PC-DARTS searches on a shallower network with less channels, suffering the search and evaluation gap.
Our solution, by progressively pruning the operation space and meanwhile widening the network, searches in a task-specific manner and achieves superior accuracy on challenging datasets like ImageNet (+2.8\% over BayesNAS, +2.3\% over GDAS, +2.3\% over PARSEC, +2.0\% over DSNAS, +1.2\% over ProxylessNAS, and +0.5\% over PC-DARTS).


\section{Experiments}
In this section, we evaluate our proposed DrNAS on two search spaces: the CNN search space in DARTS~\citep{darts} and NAS-Bench-201~\citep{nasbench201}. For DARTS space, we conduct experiments on both CIFAR-10 and ImageNet in section~\ref{sec:cifar10} and~\ref{sec:imagenet} respectively. For NAS-Bench-201, we test all 3 supported datasets (CIFAR-10, CIFAR-100, ImageNet-16-120~\citep{imagenet16}) in section~\ref{sec:201}. Furthermore, we empirically study the dynamics of exploration and exploitation throughout the search process in section~\ref{sec:ee}.
\subsection{Results on CIFAR-10}
\label{sec:cifar10}
\paragraph{Architecture Space}
For both search and evaluation phases, we stack 20 cells to compose the network and set the initial channel number as 36. We place the reduction cells at the 1/3 and 2/3 of the network and each cell consists of  nodes. 


\paragraph{Search Settings}
We equally divide the 50K training images into two parts, one is used for optimizing the network weights by momentum SGD and the other for learning the Dirichlet architecture distribution by an Adam optimizer.
Since Dirichlet concentration  must be positive, we apply the shifted exponential linear mapping  and optimize over  instead.
We use  norm to constrain the distance between  and the anchor .
The  is initialized by standard Gaussian with scale 0.001, and  in (\ref{eq:cop}) is set to 0.001.
The ablation study in Appendix~\ref{app:sensitivity} reveals the effectiveness of our anchor regularizer, and DrNAS is insensitive to a wide range of . 
These settings are consistent for all experiments.
For progressive architecture learning, the whole search process consists of 2 stages, each with 25 iterations. 
In the first stage, we set the partial channel parameter  as 6 to fit the super-network into a single GTX 1080Ti GPU with 11GB memory, i.e., only 1/6 features are sampled on each edge.
For the second stage, we prune half candidates and meanwhile widen the network twice, i.e., the operation space size reduces from 8 to 4 and  becomes 3.

\paragraph{Retrain Settings}
The evaluation phase uses the entire 50K training set to train the network from scratch for 600 epochs.
The network weight is optimized by an SGD optimizer with a cosine annealing learning rate initialized as 0.025, a momentum of 0.9, and a weight decay of .
To allow a fair comparison with previous work, we also employ cutout regularization with length 16, drop-path~\citep{nasnet} with probability 0.3 and an auxiliary tower of weight 0.4. 

\paragraph{Results}
Table~\ref{tab:cifar10} summarizes the performance of DrNAS compared with other popular NAS methods, and we also visualize the searched cells in Appendix~\ref{app:vis}. 
DrNAS achieves an average test error of 2.46\%, ranking top amongst recent NAS results.
ProxylessNAS is the only method that achieves lower test error than us, but it searches on a different space with a much longer search time and has larger model size.
We also perform experiments to assign proper credit to the two parts of our proposed algorithm, i.e., Dirichlet architecture distribution and progressive learning scheme.
When searching on a proxy task with 8 stacked cells and 16 initial channels as the convention~\citep{darts,pcdarts}, we achieve a test error of 2.54\% that surpasses most baselines. 
Our progressive learning algorithm eliminates the gap between the proxy and target tasks, which further reduces the test error.
Consequently, both of the two parts contribute a lot to our performance gains.

\begin{table}[!t]
    \centering
\caption{Comparison with state-of-the-art image classifiers on CIFAR-10.}
    \resizebox{.8\textwidth}{!}{
    \begin{threeparttable}
    \begin{tabular}{lcccc}
    \hline
    \textbf{Architecture} & \textbf{\tabincell{c}{Test Error\M)}} & \textbf{\tabincell{c}{Search Cost\M)}}} &
\multirow{2}*{\textbf{\tabincell{c}{Search Cost\
    p(\theta(\bold{h})|\beta) = \frac{\Gamma(\sum_o \beta_o)}{\prod_o \Gamma(\beta_o)} \prod_o \theta_o^{\beta_o} g(\bold{1}^T\bold{h})

    \label{eq:mapping}
    \mu_o = \log{\beta_o} - \frac{1}{|\mathcal{O}|} \sum_{o^{'}} \log{\beta_{o^{'}}} \quad\quad \Sigma_o = \frac{1}{\beta_o}(1 - \frac{2}{|\mathcal{O}|}) + \frac{1}{|\mathcal{O}|^2}\sum_{o^{'}} \frac{1}{\beta_{o^{'}}}

    \label{eq:hess}
    &E_{\theta \sim Dir(\beta)} \big{[}\mathcal{L}_{val} (w^*, \theta)\big{]}\\
    \approx &E_{\epsilon \sim \mathcal{N}(0, \Sigma)} \big{[}\mathcal{L}_{val} (w^*, Softmax(\mu + \epsilon))\big{]}\\
    \equiv &E_{\epsilon \sim \mathcal{N}(0, \Sigma)} \big{[}\mathcal{\Tilde{L}}_{val} (w^*, \mu + \epsilon)\big{]}\\
    \approx &E_{\epsilon \sim \mathcal{N}(0, \Sigma)} \big{[} \mathcal{\Tilde{L}}_{val} (w^*, \mu) + \epsilon^T \nabla_{\mu} \mathcal{\Tilde{L}}_{val} (w^*, \mu) + \frac{1}{2}\epsilon^T\nabla_{\mu}^2 \mathcal{\Tilde{L}}_{val} (w^*, \mu)\epsilon \big{]}\\
    = &\mathcal{\Tilde{L}}_{val} (w^*, \mu) + \frac{1}{2} tr\big{(}E_{\epsilon \sim \mathcal{N}(0, \Sigma)} \big{[} \epsilon\epsilon^T  \big{]} \nabla_{\mu}^2 \mathcal{\Tilde{L}}_{val} (w^*, \mu) \big{)}\\
    = &\mathcal{\Tilde{L}}_{val} (w^*, \mu) + \frac{1}{2} tr\big{(} \Sigma \nabla_{\mu}^2 \mathcal{\Tilde{L}}_{val} (w^*, \mu) \big{)}

    \Sigma_o &= \frac{1}{\beta_o}(1 - \frac{2}{|\mathcal{O}|}) + \frac{1}{|\mathcal{O}|^2}\sum_{o^{'}} \frac{1}{\beta_{o^{'}}} \\
             &\geq \frac{1}{1 + \delta}(1 - \frac{2}{|\mathcal{O}|}) + \frac{1}{|\mathcal{O}|}\frac{1}{1 + \delta}

    &E_{\theta \sim Dir(\beta)} \big{[}\mathcal{L}_{val} (w^*, \theta)\big{]}\\
    \approx &\mathcal{\Tilde{L}}_{val} (w^*, \mu) + \frac{1}{2} tr\big{(} \Sigma \nabla_{\mu}^2 \mathcal{\Tilde{L}}_{val} (w^*, \mu) \big{)}\\
    \geq &\mathcal{\Tilde{L}}_{val} (w^*, \mu) + \frac{1}{2} (\frac{1}{1 + \delta}(1 - \frac{2}{|\mathcal{O}|}) + \frac{1}{|\mathcal{O}|}\frac{1}{1 + \delta}) tr\big{(} \nabla_{\mu}^2 \mathcal{\Tilde{L}}_{val} (w^*, \mu) \big{)}

    \mathcal{L}(\beta) = E_{q(\theta|\beta)}\big{[} \log{p(D|\theta, w)} \big{]} - KL(q(\theta|\beta)||p(\theta|w))

    \label{eq:vi}
    \min_{\beta} &\  \ E_{q(\theta|\beta)}\big{[} -\log{p(D_{valid}|\theta, w^*)}\big{]} + KL(q(\theta|\beta)||p(\theta))

Note that eq. (\ref {eq:vi}) resembles eq. (\ref{eq:cop}) if we use the negative log likelihood as the loss function and replace  with KL divergence.
In practice, we find that using a simple l2 distance regularization works well across datasets and search spaces.

\end{document}
