\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{pifont}
\usepackage{bbding}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{stfloats}

\usepackage{multirow}
\usepackage{multicol}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}




\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling}



\author{\textbf{Jiaqi Xu \hspace{3mm} Bo Liu\hspace{3mm} Yunkuo Chen \hspace{3mm} Mengli Cheng \hspace{3mm} Xing Shi}\\
Alibaba Group\\
{
    \tt\small \{zhoumo.xjq, xuanyuan.lb, chenyunkuo.cyk, mengli.cml, shubao.sx\}@alibaba-inc.com 
}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}

Video-and-language understanding has a variety of applications in the industry, such as video question answering, text-video retrieval and multi-label classification.
Existing video-and-language understanding methods generally adopt heavy multi-modal encoders and feature fusion modules, which consume large amounts of GPU memory. 
Especially, they have difficulty dealing with dense video frames or long text that are prevalent in industrial applications. 


In this paper, we propose MuLTI, a highly accurate and memory-efficient video-and-language understanding model that achieves efficient and effective feature fusion through feature sampling and attention modules.
Specifically, we design a MultiWay-Sampler based on self-attention modules and attention-based residual mapping to sample long sequence features and fuse multi-modal features, which both reduces the memory cost and improves the performance.
Therefore, MuLTI can handle longer sequences with limited GPU memory.
Then, we introduce an attention-based adapter to the encoders, which finetunes the shallow features to improve the model's performance with low GPU memory consumption.
Finally, to further enhance the model's performance, we propose a new pretraining task named Multiple Choice Modeling. This task is designed to bridge the gap between pretraining and downstream tasks and to improve the model's ability to align video and text.
Benefiting from the efficient feature fusion module, the attention-based adapter and the new pretraining task, MuLTI achieves state-of-the-art performance on multiple datasets.
Implementation and pretrained models will be released.
\end{abstract} \vspace{-4ex}
\section{Introduction}\label{sec:intro}
\vspace{-1ex}
\begin{figure*}
    \centering 
    \includegraphics[width=0.72\textwidth]{imgs/Abstract_crop.pdf}
    \vspace{-1ex}
\caption{
        When dealing with multiple sparse frames and long text, previous models like (a) and (b) will cost large video memory because the concatenate features before feature fusion are too long and all the parameters of the model are finetuned. Previous models such as (c) freeze the video encoder, which limits its adaptability to downstream tasks. We design MuLTI like (d). The MultiWay-Sampler is introduced to condense text features and feature fusion, which achieves effective feature interaction with low memory cost. In addition, we introduce an attention-based adapter to the encoders to finetune shallow feature, which is more effective than the unfrozen encoders.
    }
    \label{fig:intro}
    \vspace{-3.5ex}
\end{figure*} 
Video-and-language understanding has a wide range of applications such as {\em video question answering~(videoQA)}, {\em text-video retrieval} and {\em multi-label classification}\footnote{cloudinary.com/documentation/google\_automatic\_video\_tagging\_addon}.
Existing methods have made significant progress in video-and-language understanding. However, they still suffer from two challenges: 
limited ability to fully exploit existing features and huge video memory consumption while training.

The video-text model generally consists of three modules: text encoder, video encoder and feature fusion module. The latter two usually cause high computational costs.

For feature fusion modules, it is challenging to be both efficient and effective. 
Some previous works~\cite{Fu2021VIOLETE, Huang2022CloverTA} directly concatenate the outputs of video and text encoders, which are then processed by transformer encoders. 
The computation complexity and memory consumption is proportional to the square of the length of the concatenated sequence. 
Some works~\cite{Lei2021Less,Li2021AlignAP,Yang2022ZeroShotVQ,Lei2021UnderstandingCV} compress video features by mean pooling or class token before feature fusion to reduce the heavy computation burden. 
However, compressing all the information into a vector before feature fusion may lose essential details.
In summary, how to achieve the balance between computational effort and accuracy of the model in the feature fusion module is still challenging.


In addition to the feature fusion module, video encoders mainly adopt models with a large number of parameters, such as Vision Transformer~(VIT) \cite{Dosovitskiy2021AnII} and Swin Transformer~\cite{Liu2021SwinTH}, which require a large amount of GPU memory.
Other existing works try to freeze all the parameters of the video encoder using offline extracted video features~\cite{Miech2019HowTo100MLA,Sun2019VideoBERTAJ,Li2020HeroHE,Zhu2020ActBERTLG,Miech2020EndtoEndLO,Luo2020Univl}.
The frozen encoders require less memory than unfrozen encoders, but the extracted features may be suboptimal due to the task gap between image-text tasks and video-text tasks.
Recent methods~\cite{Fu2021VIOLETE, Huang2022CloverTA,Lei2021Less,Li2021AlignAP,Yang2022ZeroShotVQ,Lei2021UnderstandingCV} tend to sample sparse frames for feature extraction to solve the above problem, which performs better on downstream tasks.
However, sampling only a few frames may lead to loss of detailed information, which degrades the performance on fine-grained downstream tasks~\cite{Lei2022RevealingSF}. 
Moreover, the sparse video-text paradigm still suffers from substantial computational costs when dealing with long videos and text descriptions. 


To tackle the above challenges, we propose a new video-and-language framework: MuLTI.
MuLTI first uses video and text encoders to extract features from sparse frames and text. 
Then, an efficient feature fusion module is proposed to facilitate the interaction between video and text features. 
Specifically, MultiWay-Sampler is designed to condense the text features and fuse multi-modal features. 
We first use a learnable query vector to sample the text feature through self-attention modules. 
Since the query vector is randomly initialized, it may lose essential information in the original features.
We design an attention-based lightweight feature aggregation method in MultiWay-Sampler to obtain the condensed features by calculating the importance of each sequence block. 
Then, we add the condensed features to the sampled features, like a residual module.
Finally, we use the short text features and the same sampler to sample the long video features again to obtain the fused features. 
We share the self-attention module and reserve different feed forward networks for different modalities in the sampler to accommodate the features of different modalities. 
Thanks to the MultiWay-Sampler, MuLTI can handle more sparse frames and longer text inputs with limited GPU memory. 

Moreover, the sparse video-text paradigm usually unfreezes all model layers for training~\cite{Fu2021VIOLETE, Huang2022CloverTA,Lei2021Less,Li2021AlignAP,Lei2021UnderstandingCV}, leading to high GPU memory cost and suboptimal performance.
We explore training strategies about freezing some parts of the encoders instead of unfreezing all layers.
Specifically, We introduce an attention-based adapter to the encoders, which finetunes the shallow features to improve the model performance with low GPU memory cost. 
Finally, we introduce a new pretraining task named Multiple Choice Modeling (MCM) to improve the representation ability of the video and text encoders. Pretraining on large-scale video-text datasets could improve the performance of video-text models significantly.
However, there are gaps between the existing pretraining tasks and downstream tasks, specifically in the field of videoQA. The difficulty of introducing videoQA to pretraining tasks is constructing suitable question-answer pairs. We propose MCM to bridge the task gap between pretraining and downstream tasks by constructing multiple-choice question answering tasks on large-scale video-text datasets.
It asks the model to find text descriptions that match the video most from a randomly constructed collection, which enhances the alignment between video and text features.

The contributions can be summarized as follows:~

\textbf{(1)}~
We propose MuLTI, a highly accurate and memory-efficient video-and-language framework, which achieves efficient and effective feature fusion through the feature sampling and attention modules.

\textbf{(2)}~
An efficient feature fusion module based on the MultiWay-Sampler is proposed to sample long sequence features and facilitate the interactions between video and text features, reducing memory cost and improving performance.  

\textbf{(3)}~
We explore training strategies about freezing some parts of the encoders instead of unfreezing all layers. In detail, we introduce an attention-based adapter to the encoders, which finetunes the shallow features to improve the model performance with low memory consumption. 

\textbf{(4)}~
We design a new pretraining task called Multiple Choice Modeling (MCM) to bridge the task gap between pretraining and downstream tasks. Experimental results on seven English tasks and one Chinese multi-label classification task demonstrate the effectiveness of MuLTI.
 \vspace{-1.5ex}
\section{Related Work}\label{sec:relatedwork}
\vspace{-0.5ex}
\begin{figure*}[ht]
\centering
\includegraphics[width=0.90\textwidth]{imgs/framwork_crop}
\vspace{-1ex}
\caption{
    (a) shows the framework of MuLTI. MuLTI contains a video encoder, a text encoder, and a MultiWay-Sampler. MultiWay-Sampler is used to condense the extracted features and feature fusion. 
    (b) shows the framework of the MultiWay-Sampler. We share the self-attention module and reserve different feed forward networks for different modalities in the sampler to accommodate the features of different modalities. The aggregation feature provides simple and fast information by residual mapping.
}
\vspace{-4ex}
\label{fig:arch}
\end{figure*} The structure of the existing video-and-language understanding model is summarized in Figure~\ref{fig:intro}, which usually consists of three modules: 
video encoder, text encoder and feature fusion module. As the latter two usually cause high computational costs, the related work will be introduced in detail next. In addition, we also introduce typical pretraining tasks in the video-text model.


\textbf{Video-Text Fusion.} Clover~\cite{Huang2022CloverTA} and VIOLET~\cite{Fu2021VIOLETE} simply concatenate the flattened video and text features, and perform complex interactions between video and text features through the transformer encoder. 
Their complexity is proportional to the square of the length of the concatenated sequences. 
ALPRO~\cite{Li2021AlignAP} also applies transformer encoders for feature fusion, but it adopts mean pooling over video features before concatenation. Although this strategy reduces the length of the video feature, it may lose essential details which are critical for downstream tasks. 
AllInOne~\cite{Wang2022AllIO} alleviates the huge memory cost by fusing text features with per-frame image features. However, its computational burden is still heavy when long OCR (Optical Character Recognition) or ASR (Automatic Speech Recognition) transcripts are used. 
To tackle the above problems, we design a MultiWay-Sampler based on self-attention modules and attention-based residual mapping to sample long sequence features and fuse multi-modal features, which has low memory cost and high performance.

\textbf{Video feature extraction.} 
Many previous methods apply a pretrained video encoder to extract features~\cite{Miech2019HowTo100MLA,Sun2019VideoBERTAJ,Li2020HeroHE,Zhu2020ActBERTLG,Miech2020EndtoEndLO,Luo2020Univl} offline. Although their methods reduce computational overhead and memory cost while training, the offline extracted features are sub-optimal due to the task gap between the pretraining and downstream tasks. Besides, the computational overhead at the testing stage is not reduced. 
In contrast, recent methods~\cite{Lei2021Less, Fu2021VIOLETE, Li2021AlignAP} demonstrate more effective results by finetuning the video encoder end-to-end with a few sparsely sampled frames. However, finetuning the video encoder costs a large amount of GPU memory. 
To tackle the above problems, we explore training strategies about freezing some parts of the encoders instead of unfreezing all layers. We introduce an attention-based adapter to the encoders, finetuning the shallow features to improve the model's performance with low memory consumption.


\textbf{Video-and-Language Pretraining.} The previous pretraining framework generally applies three typical pretraining tasks: Masked Frame Modeling (MVM) tasks~\cite{Lei2021UnderstandingCV, Ma2021Top1SO, Fu2021VIOLETE,Huang2022CloverTA} for video encoder optimization, Masked Language Modeling (MLM) tasks~\cite{Devlin2018Bert,Sun2019VideoBERTAJ,Zhu2020ActBERTLG,Luo2020Univl,Li2020HeroHE,Lei2021Less,Fu2021VIOLETE} for text encoder optimization, Video Text Matching (VTM) and Video Text Comparison (VTC) tasks~\cite{Li2020HeroHE,Luo2020Univl,Fu2021VIOLETE,Li2021AlignAP} for joint optimization of video and text encoders. 
Although the above methods have proven their effectiveness in learning video and text representations, there are still large task gaps between pretraining and downstream tasks, especially in the field of videoQA.
To tackle the above problems, we introduce MCM to pretrain MuLTI further to bridge the gap between pretraining tasks and downstream tasks and enhance the alignment between multi-modal representations.

 \vspace{-1.5ex}
\section{Methodology}
\vspace{-0.5ex}


In this section, we will introduce the MuLTI model in detail. First, we present the overall architecture of the model in Section~\ref{sec:method-arch}.
Then, we introduce the pretraining tasks used in our model in Section~\ref{sec:method-pre}.

\vspace{-1ex}
\subsection{\textbf{MuLTI's}~Architecture}\label{sec:method-arch}
\vspace{-0.5ex}
Figure~\ref{fig:arch}~(a) gives an overview of MuLTI's architecture.
Specifically, MuLTI consists of three components: a video encoder, a text encoder, and a feature fusion module. Details for each component are as follows.

\textbf{Video and Text Encoders:}~
Unless specified, a 12-layer VIT-B/16~\cite{Radford2021LearningTV} is used as video encoder. 
We sparsely sample  frames from the input video. The VIT-B/16 model divides each frame into  non-overlapping patches. 
The per-video features is , where  is the feature dimension.
The output of the video encoder is a video features sequence: , with . 
In experiments, we found that the class token is excessive because information is well represented by other tokens, so it is dropped to save computation.
Unless specified, a 12-layer bert~\cite{Devlin2018Bert} is used as the text encoder. Assuming the length of input text is , the output of the text encoder is a text features sequence : , with . The  is the output of the text \texttt{[CLS]} token. 


The video and text encoders are chosen because the transformer based models have top performance on multiple visual and text tasks specifically, and they could initialize from pretrained weights on large-scale image-text datasets. However, both models mentioned above are computationally intensive. 
We explore training strategies about freezing some encoders instead of unfreezing all layers and introduce an attention-based adapter for the encoders. 
As shown in the Figure~\ref{fig:adapter} (a), adapters \cite{Houlsby2019ParameterEfficientTL} are inserted into the transformer encoder. The adapters make use of feed forward network (FFN) and residual connections to enhance the hidden state . 
We only add the adapter to the text encoder because the video features are more general and require more computation. 
 is the output of the FFN, with , ,  the hidden dimension of the encoder,  the hidden dimension of the adapter.  is the output of the adapter.
\begin{figure}[ht]
\vspace{-2ex}
\centering
\includegraphics[width=0.43\textwidth]{imgs/Adapter_crop.pdf}
\vspace{-1ex}
\caption{
The Attention-Adapter module in text encoder. 
}
\label{fig:adapter}
\vspace{-1ex}
\end{figure} 
Although the adapter~(Fig.~\ref{fig:adapter}b) works well, the representation ability of FFN is limited. To further improve its performance, we add a lightweight attention module to the adapter~(Fig.~\ref{fig:adapter}c), which allows the model to focus more on the critical tokens. 
The attention-based adapter allows the model to optimize the shallow features using much fewer parameters.  is the output of the lightweight attention module, with , . The output of the Attention-Adapter module is . 


\textbf{Multi-Modal Fusion Module:}~The core of the multi-modal fusion module is the MultiWay-Sampler, which is illustrated in Figure~\ref{fig:arch}~(b). The MultiWay-Sampler is adapted from the transformer decoder. Specifically, the MultiWay-Sampler module is designed to condense the text features and fuse different modal features efficiently. 
We first randomly initialize a learnable query vector to condense the text feature generated by the text encoder through sampling. 
Since the MultiWay-Sampler uses multiple learnable queries and self-attention modules, it is more expressive than pooling and class-token based methods. 
The expression  represents the sampling of feature  using the query vector  through MultiWay-Sampler.

Since the query vector is randomly initialized in the MultiWay-Sampler, it may lose essential information in the original features.
Therefore, we design an attention-based lightweight feature aggregation method named Attention-Aggregation to condense long sequence features. 
The detailed structure of the Attention-Aggregation is illustrated in the left part of Figure~\ref{fig:arch}~(b). 
The formula is shown below,  is the output of the Attention-Aggregation, with ,  the hidden dimension of the transformer,  the length of condensed features,  the transposition of the matrix. 
\vspace{-1ex}

\vspace{-3.5ex}

The  is a matrix with the shape , which means the importance of each sequence block,  the length of input features. We add the condensed features to the sampled features, like a residual module. 
Before feature fusion, the learnable time embeddings are added to each frame of image features for temporal modeling. 
The short text features and the same sampler are used to sample the flattened video features again to fuse multi-modal features. 
We share the self-attention modules and reserve different FFN for different modalities in the MultiWay-Sampler to accommodate the multi-modal features.
The fuse feature is shown as follow, with  the query embedding of text features,  the fused feature:
\vspace{-1ex}

\vspace{-3.5ex}

A work similar to ours is Token Learner, which obtains 8 or 16 vectors representing a picture for feature extraction by adding spatial attention modules to the visual model. 
The difference is that we reduce the length of features based on the self-attention module of sequence features and focus on the fusion of multiway features. In the feature fusion module, the sampler extracts the complex learnable information of multi-modal features by the self-attention module, and the Attention-Aggregation provides simple and fast information by residual mapping.


Our feature fusion module is much more efficient than the flatten-based method and transformer encoder, which could be demonstrated by a simple analysis: we assume VIT-B/16 is used as the per-frame extractor, each frame will be flattened into a sequence of length 196. Let the number of queries used in the sampler be  for text, the length of the text features be , and the length of the text features be . Thus the complexity of the flatten method will be . After applying the MultiWay-Sampler, the complexity of our method is . As  are generally much smaller than  and . Our feature fusion method is much more efficient than the flatten-based methods. 

\textbf{MuLTI for different scenes.}~In this section, we constructed models of different scales, which could be deployed to scenes of limited and sufficient resources. We first replace the video encoder from VIT-B/16 to VIT-L/14 and the text encoder from bert-base\cite{Devlin2018Bert} to bert-large\cite{Devlin2018Bert}. Then, we obtain MuLTI-L. 
In addition, we replace the video encoder from VIT-B/16 to VIT-B/32 and reduce the text encoder from 12 layers to 6 layers. Then, we obtain MuLTI-S. The floating point of operations~(FLOPs), parameters~(Params) and frames per second~(FPS) of different models are shown in Table~\ref{tbl:flops_params}.

\vspace{-1ex}
\begin{table}[htb]
\center
\small
\setlength{\tabcolsep}{1mm}
\resizebox{0.48\textwidth}{!}
{
    \aboverulesep = 0.55mm
    \belowrulesep = 0.55mm
	\centering	
		\begin{tabular}	{l l l | c c c}
			\toprule
			    \textbf{Method} &
    		    \textbf{Video Encoder} & 
    		    \textbf{Text Encoder} & 
    			\textbf{FLOPs} &
    			\textbf{Params} &
    			\textbf{FPS} \\
    			\midrule	
    			\textbf{MuLTI-S} & \textbf{VIT-B/32\cite{Dosovitskiy2021AnII}} & \textbf{bert-base-uncased\cite{Devlin2018Bert}} & 99G & 203M & 20.74\\
    			\textbf{MuLTI-B} & \textbf{VIT-B/16\cite{Dosovitskiy2021AnII}} & \textbf{bert-base-uncased\cite{Devlin2018Bert}} & 346G & 247M & 10.13\\
    			\textbf{VIOLET\cite{Fu2021VIOLETE}} & \textbf{Swin Transformer3D\cite{Liu2021SwinTH}} & \textbf{bert-base-uncased\cite{Devlin2018Bert}} & 249G & 198M & 9.05\\
    			\textbf{ALPRO\cite{Li2021AlignAP}} & \textbf{Timesformer\cite{Bertasius2021IsSA}} & \textbf{bert-base-uncased\cite{Devlin2018Bert}} & 432G & 235M & 9.97\\
                    \midrule
    			\textbf{MuLTI-L} & \textbf{VIT-L/14\cite{Dosovitskiy2021AnII}} & \textbf{bert-large-uncased\cite{Devlin2018Bert}} & 1509G & 746M & 3.12\\ 
    			\textbf{FrozenBiLM\cite{Yang2022ZeroShotVQ}} & \textbf{VIT-L/14\cite{Dosovitskiy2021AnII}} & \textbf{DeBERTa-V2-XLarge\cite{he2021deberta}} & 1733G & 1224M & 2.54\\
			\bottomrule
		\end{tabular}}
\caption
	{Comparison among models with 16 sparse frames. Text length is set to 512. FPS is based on 1 NVIDIA V100 16GB GPU.}
	\label{tbl:flops_params}
\vspace{-4ex}
\end{table}		 
\subsection{Pretraining for MuLTI}\label{sec:method-pre}
\vspace{-0.5ex}
We pretrain MuLTI using four objectives, including three canonical objectives: Masked Language Modeling (MLM), Video Text Matching (VTM), and Video Text Comparison (VTC). In this section, we focus on a new technique: Multiple Choice Modeling (MCM). 

\textbf{Multiple Choice Modeling:}~
Although MLM and VTM have proven their effectiveness in learning video and text representations, there is still a large task gap between pretraining tasks and downstream tasks such as videoQA.
The difficulty of introducing videoQA into the pretraining task is constructing suitable question-answer pairs.
Multiple choice questions are a common form of videoQA. Inspired by the multiple choice questions, we find the text descriptions paired with videos are the correct natural answers.
Therefore, we introduce Multiple Choice Modeling, a new pretraining task that bridges the task gap between pretraining and downstream tasks. Specifically, it is constructed as follows, which is a four-choice question.

\noindent{\small\texttt{"[CLS]<Question> ? [SEP] Option 1: <Answer 1>. [SEP] Option 2: <Answer 2>. [SEP] Option 3: <Answer 3>. [SEP] Option 4: <Answer 4>."}}

We randomly place the correct descriptions in \texttt{<Answer 1>, <Answer 2>, <Answer 3>, <Answer 4>}, and obtain answers other than the correct descriptions through the text corpus. The \texttt{<Question>} also has various choices, such as ``What does this picture describe?'', ``What does this video describe?'', ``What can we learn from the video?'' and so on. 
The MCM does not require additional manual annotation or extensive data pre-processing, which is an efficient and scalable solution for pretrained models to be supervised for the question and answer downstream tasks.  
The motivation of MCM is to bridge the task gap between pretraining and downstream tasks. Because MCM can improve the model's ability to align the video and the text features, the model's performance on the text video retrieval task is also improved.


\textbf{Pretraining Objectives:}~We also employ the MLM, VTM and VTC, considering their effectiveness.  
The MLM randomly masks input tokens with 15\% probability and replaces them with [MASK], after which masked text tokens are predicted based on video and text.
The VTC treats matching video text pairs as positive pairs and other video text pairs in the batch as negative pairs.
The VTM is slightly different from VTC, where the multi-modal features are fused before used for classification. 
The overall pretraining objective of MuLTI is:

\vspace{-2ex}

\vspace{-4ex}

\begin{table*}[!]
\centering
\setlength{\tabcolsep}{1mm}
\resizebox{0.98\textwidth}{!}{
    \aboverulesep = 0.55mm
    \belowrulesep = 0.55mm
	\centering	
            \begin{tabular}{ll|ccccc cccccccc}
                \toprule
                 ~ & ~ & MSRVTT-QA & MSVD-QA & TGIF-Act. & TGIF-Trans. & TGIF-Frame & \multicolumn{4}{c}{\textbf{MSRVTT-Ret}}  & \multicolumn{4}{c}{\textbf{DiDeMo-Ret}} \\
                
                Method & Pretrain datasets & \footnotesize\textbf{Acc.} & \footnotesize\textbf{Acc.} & \footnotesize\textbf{Acc.} & \footnotesize\textbf{Acc.} & \footnotesize\textbf{Acc.} & \footnotesize \textbf{R1} &
                \footnotesize\textbf{R5} & 
                \footnotesize\textbf{R10} & \footnotesize\textbf{G-Mean}  & 
                
                \footnotesize \textbf{R1} & \footnotesize\textbf{R5} & \footnotesize\textbf{R10} & \footnotesize\textbf{G-Mean} \\
        
                \midrule
CLIP4CLIP~\cite{Luo2021CLIP4ClipAE} & WIT(400M) & - & - & -  & -  & -  & 43.1 & 70.4 & 80.8 & 62.6 & 43.4 & 70.2 & 80.6 & 62.6 \\
    		QB-Norm~\cite{Bogolin2021CrossMR} & WIT(400M) & - & - & -  & -  & -  & 47.2* & 73.0* & 83.0* & 65.9* & 43.3* & 71.4* & 80.8* & 63.0* \\
    		CAMoE~\cite{Cheng2021ImprovingVR} & WIT(400M) & - & - & -  & -  & -  & 47.3* & 74.2* & 84.5* & 66.7* & 43.8* & 71.4* & 79.9* & 63.0* \\  
TS2-Net~\cite{Liu2022TS2NetTS} & WIT(400M) & - & - & -  & -  & -  & 54.0* & 79.3* & 87.4* & 72.1* & 47.4* & 74.1* & 82.4* & 66.1* \\
ALPRO~\cite{Li2021AlignAP} & Web2M + CC3M (5.5M) & 42.1 & 45.9 & -  & -  & -  & 33.9 & 60.7 & 73.2 & 53.2 & 35.9 & 67.5 & 78.8 & 57.6 \\
    		VIOLET~\cite{Fu2021VIOLETE} & Web2M + YT-180M + CC3M(185.5M) & 43.9 & 47.9 & 92.5 & 95.7 & 68.9 & 34.5 & 63.0 & 73.4 & 54.2 & 32.6 & 62.8 & 74.7 & 53.5\\
    		AllInOne~\cite{Wang2022AllIO} & Web2M + HT (102.5M) & 44.3 & 47.9 & 92.7 & 94.3 & 64.2 & 37.9 & 68.1 & 77.1  & 58.4  & 32.7 & 61.4 & 73.5 & 52.8  \\
    		Clover ~\cite{Huang2022CloverTA} & Web2M + CC3M (5.5M) & 44.1 & 52.4 & 95.0 & 98.2 & 71.6 & 40.5 & 69.8 & 79.4 & 60.7 & 50.1 & 76.7 & 85.6 & 69.0 \\
    		FrozenBiLM ~\cite{Yang2022ZeroShotVQ} & Web10M (10M) & 47.0 & 54.4 & -  & -  & 68.6  & - & -& - & -  & - & - & - & -  \\ 
    		\midrule 
    		\multirow{2}{*}{\textbf{MuLTI-S}} & \multirow{2}{*}{Web2M + CC3M (5.5M)} & \multirow{2}{*}{45.6} & \multirow{2}{*}{50.0} & \multirow{2}{*}{97.3} & \multirow{2}{*}{98.9}  & \multirow{2}{*}{71.2} & 41.3 & 70.6 & 79.7 & 61.5 & 42.6 & 71.4 & 80.0 & 62.5 \\
    		 & & & & & & & 45.8* & 73.5* & 82.0* & 65.1* & 47.9* & 73.0* & 82.6* & 66.1* \\ 
    		\midrule
    		\multirow{2}{*}{\textbf{MuLTI-B}} & \multirow{2}{*}{Web2M + CC3M(5.5M)} & \multirow{2}{*}{46.6} & \multirow{2}{*}{53.0} &  \multirow{2}{*}{97.3} & \multirow{2}{*}{\textbf{99.1}} & \multirow{2}{*}{73.5} & 45.1 & 72.4  & 81.8  & 64.4 & 45.2 & 74.6 & 82.2 & 65.2 \\
    		 & & & & & & & 49.4* & 75.9* & 84.0* & 68.0* & 48.3* & 75.4* & 83.5* & 67.2* \\ 
    		\midrule
    		\multirow{2}{*}{\textbf{MuLTI-L}} & \multirow{2}{*}{Web2M + CC3M (5.5M)} & \multirow{2}{*}{\textbf{47.8}} & \multirow{2}{*}{\textbf{54.7}} & \multirow{2}{*}{\textbf{97.9}} & \multirow{2}{*}{99.0} & \multirow{2}{*}{\textbf{75.6}} & 48.7 & 75.9 & 83.9 & 67.7  & 50.5 & 78.5 & 86.2 & 69.9 \\
    		 & & & & & & & \textbf{54.7*} & \textbf{77.7*} & \textbf{86.0*} & \textbf{71.5*} & \textbf{56.5*} & \textbf{80.2*} & \textbf{87.0*} & \textbf{73.3*} \\ 
    		\bottomrule
		\end{tabular}}
\caption
    {
         Comparisons with existing methods. Acc. (\%) is used to measure the performance of videoQA. R@k denotes recall (\%) with k retrieval efforts. G-Mean denotes the geometric mean of R@1, R@5, R@10. The datasets we use in Table are 
         WebVid2M(Web2M)\cite{Bain2021FrozenIT}, WebVid10M(Web10M)\cite{Bain2021FrozenIT}, WIT\cite{Radford2021LearningTV}, HowTo100M(HT)\cite{Miech2019HowTo100MLA},  
         YT-Temporal-180M(YT-180M)\cite{Zellers2021MERLOTMN},Conceptual Captions(CC3M)\cite{Sharma2018ConceptualCA}. * indicates that the method uses DSL~\cite{Cheng2021ImprovingVR} or QB-Norm~\cite{Bogolin2021CrossMR} for post-processing.
    }
    \vspace{-3ex}
\label{tbl:main}
\end{table*} %
 \vspace{-1.5ex}
\section{Experiments}
\vspace{-0.5ex}

\begin{table}[!t]\centering
\small
\setlength{\tabcolsep}{2mm}
\resizebox{0.36\textwidth}{!}
{
    \aboverulesep = 0.55mm
    \belowrulesep = 0.55mm
	\centering
		\begin{tabular}	{l c c | c}
			\toprule
			\textbf{Method} & \textbf{Pretrained datasets} & \textbf{OCR} & \textbf{Multi-Label} \\
			\midrule
    	    VIOLET~\cite{Fu2021VIOLETE}~ & - & \ding{56} & 55.22\\
    	    ALPRO~\cite{Li2021AlignAP}~ & - & \ding{56} & 58.53\\
			\midrule
    		\textbf{MuLTI-S} &  - & \ding{56} & 63.97\\
    		\textbf{MuLTI-S} & -  &  \ding{52} & \textbf{66.13}\\
			\midrule
    		\textbf{MuLTI-B} &  - & \ding{56} & 64.60\\
    		\textbf{MuLTI-B} & - &  \ding{52} & \textbf{67.86}\\
    		\bottomrule
		\end{tabular}}
\caption
	{
    	Comparisons with existing methods on multi-label classification in mAP (\%).  means the methods are reproduced in our framework.
	}
    \label{tbl:multi-label}
\vspace{-4ex}
\end{table} In this section, we describe the implementation details of MuLTI and introduce the datasets we use in Section~\ref{sec:method-impl} and Section~\ref{sec:exp-setup}. 
MuLTI is compared against previous methods in section~\ref{sec:exp-proposes-method}. We further analyze the main components of MuLTI: the MultiWay-Sampler, the Attention-Adapter and the MCM in Section~\ref{sec:exp-exclusive}, \ref{sec:exp-adapter} and ~\ref{sec:exp-pfe} respectively.


\vspace{-1ex}
\subsection{Implementation Details}\label{sec:method-impl}
\vspace{-0.5ex}

\textbf{Pretraining datasets:}~We pretrained the model using two large datasets. One is WebVid-2M~\cite{Bain2021FrozenIT}, which contains 2.5M video-text pairs. Previous work reports~\cite{Lei2021Less} that pretraining the video-text model using image-text pairs also improves the model's performance on the video-text task. Thus, the CC-3M\cite{Sharma2018ConceptualCA} is also used as a pretrained dataset containing 3M image-text pairs. In total, our pretraining dataset contains 5.5M video-text pairs.

\textbf{We implement MuLTI in PyTorch~\cite{paszke2019pytorch}.} In detail, the video encoder is initialized with pretrained weights from CLIP~\cite{Radford2021LearningTV}. Text encoder is initialized with a 12-layer of the BERT model~\cite{Devlin2018Bert}. 
Then, a 4-layer MultiWay-Sampler is used to condense the output of the text encoder and fuse multi-modal features.
The length of query embedding is set to 16. 
All the parameters of the sampler are randomly initialized. 
We pretrained MuLTI for 10 epochs on 8 NVIDIA A100 GPUs using a batch size of 256, corresponding to 200k iterations. 
We employ the AdamW optimizer~\cite{Adamw2018} with a learning rate of  and weight decay of 0.05 for optimization. The learning rate is initially warmed up to the base value, then linearly decayed to zero.
We uniformly sample 16 frames for each video and scale them to  as inputs to the video encoder.

\vspace{-1ex}
\subsection{Downstream Tasks and Datasets}\label{sec:exp-setup}
\vspace{-0.5ex}

\textbf{Video Question Answering.} We evaluate MuLTI on five widely used videoQA tasks.
\textbf{(1)}~\textbf{MSRVTT-QA}~\cite{xu2017video} is a dataset that utilizes videos and text descriptions from MSRVTT~\cite{Xu2016Msr}. The dataset comprises of 10k videos and 243k question-answer pairs, with 1.5k answer candidates included.
\textbf{(2)}~\textbf{MSVD-QA}~\cite{xu2017video} is a dataset that utilizes videos and text descriptions from MSVD~\cite{chen2011collecting}. The dataset includes 1970 videos and 50k question-answer pairs, with 1k answer candidates provided.
\textbf{(3)}~\textbf{TGIF-QA\cite{Jang2017TGIFQATS}}~requires models to understand the details of GIF videos to answer questions about them. There are three datasets in TGIF-QA. TGIF-Action and TGIF-Transition are multiple-choice tasks, and TGIF-Frame is an open-ended videoQA task. 


\textbf{Text-Video Retrieval.} \textbf{(1)}~\textbf{MSRVTT} contains 10K videos from YouTube with 200K annotations. We follow \cite{Fu2021VIOLETE} and use 9k videos for training and 1k videos for testing.
\textbf{(2)}~\textbf{DiDeMo} consists of 10K videos annotated from Flickr with 40K annotations. We follow \cite{Lei2021Less} and concatenate all annotations from the same video into a title. 

\textbf{Multi-Label Classification.}
The labels on videos are essential features required by ranking models for online advertisement~\footnote{https://algo.qq.com/index.html}. 
We build a large-scale in-house multi-label short video dataset that contains 486k short videos with 486k text captions and 21696 labels. Each video-text pair has multiple labels. Multiple professional editors cross check the labels from a short-video recommendation platform. We also applied an end-to-end text spotter with top performance on icdar challenges\footnote{https://rrc.cvc.uab.es/?ch=4\&com=evaluation\&task=4} to generate OCR transcripts for each frame. The per-frame OCR transcripts are then assembled and truncated to 512. The examples for multi-label classification can be found in the appendix.
\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth]{imgs/memory_models.pdf}
\vspace{-1ex}
\caption{
Comparisons with existing methods on Memory-Usage with different numbers of sparse frames. Text length is set to 512. 
}
\label{fig:memory_models}
\vspace{-3ex}
\end{figure} 
\vspace{-1ex}
\begin{table*}[htb]
\small
\aboverulesep = 0.55mm
\belowrulesep = 0.55mm
\centering	
\setlength{\tabcolsep}{1mm}
\resizebox{0.97\textwidth}{!}
{\begin{tabular}	{l | c | c c c | c c | c c c c c c}
        \toprule 
        \multirow{2}{*}{\textbf{Method}} &
        \multirow{2}{*}{\textbf{Base}}& 
        \multirow{2}{*}{\textbf{Partially Frozen Encoder}}& 
        \multirow{2}{*}{\textbf{MultiWay-Sampler}} &  
        \multirow{2}{*}{\textbf{Attention-Adapter}} &  
        \multirow{2}{*}{\textbf{Pretraining Baseline}} &  
        \multirow{2}{*}{\textbf{MCM}} &  
        \textbf{MSRVTT-QA} & 
        \textbf{MSVD-QA} &
        \multicolumn{4}{c}{\textbf{MSRVTT-Ret}} \\
        &  &  &  &  &  &  & \footnotesize\textbf{Acc.} & \footnotesize\textbf{Acc.} & \footnotesize \textbf{R1} & \footnotesize\textbf{R5}
        & \footnotesize\textbf{R10} & \footnotesize\textbf{G-Mean}  \\
        \midrule
        \multirow{5}* {MuLTI-B} & \ding{52} & \ding{52} & \ding{56} & \ding{56} & \ding{56} & \ding{56} & 44.84 & 48.35 & 38.90 & 69.50 & 78.50 & 59.64 \\
        & \ding{52} & \ding{52} & \ding{52} & \ding{56} & \ding{56} & \ding{56} & 45.54 & 49.86 & 38.80 & 70.30 & 80.10 & 60.22 \\
        & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{56} & \ding{56} & 45.71 & 50.63 & 40.80 & 69.40 & 80.80 & 61.16  \\
        & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{56} & 46.28 & 51.93 & 44.30 & 72.40 & \textbf{81.90} & 64.04 \\
        & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \textbf{46.61} & \textbf{53.03} & \textbf{45.10} & \textbf{72.40} & 81.80 & \textbf{64.40} \\
        \bottomrule
	\end{tabular}
}

\caption
{Evaluations of the proposed methods on three downstream datasets. Pretraining Baseline: Pretraining model with MLM, VTM and VTC. MCM: Multi Choice Modeling. Acc. (\%) is used to measure the performance of videoQA. R@k denotes recall (\%) with k retrieval efforts. G-Mean denotes the geometric mean of R@1, R@5, R@10.}
\label{tbl:main_proposal}
\vspace{-3.5ex}
\end{table*}		 \vspace{-0.5ex}
\subsection{Performance of Proposed methods}\label{sec:exp-proposes-method}
\vspace{-0.5ex}
Table~\ref{tbl:main} compares MuLTI with existing methods on seven commonly-used public datasets. 

In videoQA tasks, MuLTI surpasses all baseline models on MSRVTT-QA, MSVD-QA, TGIF-Action, TGIF-Transition and TGIF-Frames. 
Since MuTLI does not use speech data as input, it is compared with FrozenBiLM\cite{Yang2022ZeroShotVQ} without using speech data. In general, MuLTI achieves state-of-the-art performance in various QA tasks. 

For text-video retrieval tasks, we finetune MuLTI using the MSRVTT and DiDeMo datasets. Our results demonstrate that MuLTI is highly competitive in both benchmarking tasks, particularly in the DiDeMo dataset, where it outperforms other methods significantly. These findings highlight the effectiveness of MuLTI for text-video retrieval applications.

In the multi-label classification task, we use VIOLET\cite{Fu2021VIOLETE} and ALPRO~\cite{Li2021AlignAP} for comparison. 
Since FrozenBiLM\cite{Yang2022ZeroShotVQ} is a large model with more than 1 billion parameters, and it is difficult to deploy it to industrial scenarios, we do not compare with it on the multi-label task. 
VIOLET and ALPRO do not use OCR transcripts as they would lead to out-of-memory on V100 GPUs. The performance without using the OCR transcript is also presented in Table~\ref{tbl:multi-label} for fair comparison. It shows that MuLTI surpasses VIOLET and ALPRO by a significant margin.
At the same time, we compare video memory cost among them in Figure~\ref{fig:memory_models}. Due to the efficient feature fusion module and Attention-Adapter based encoders, MuLTI's memory cost on feature fusion and video encoder do not increase significantly when the number of sparse frames increases. In total, the video memory consumption of ALPRO and VIOLET is more than twice that of MuLTI. 

\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth]{imgs/memory_sampler.pdf}
\vspace{-1ex}
\caption{
Comparisons of different length of text and different number of sparse frames on Memory-Usage. The F means Flatten, the D means Decoder, the E means Encoder, the S means Sampler.
}
\label{fig:memory_sampler}
\vspace{-2ex}
\end{figure} \begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth]{imgs/example_att_crop.pdf}
\caption{
A visualization of the cross-attention map from the MultiWay sampler, showing its ability to locate the most relevant visual parts for the text.
}
\label{fig:example_att}
\vspace{-3ex}
\end{figure} Finally, we evaluate the impact of our main technical contributions (\textit{i.e.} MultiWay-Sampler, Attention-Adapter and MCM) in Table~\ref{tbl:main_proposal}.
Compared with baseline models, our main technical contributions improve performance on all datasets. 
Feature fusion module based on the MultiWay-Sampler empowers MuLTI with strong multi-modal feature processing ability to extract task-relevant information from the redundant video and text features.
The Attention-Adapter allows the model to finetune shallow features without tuning all parameters, which balances the video memory consumption and the model's performance.
The MCM requires the model to select text that matches the video among multiple text descriptions, which improves the model’s ability to align the video and the text.

\vspace{-1.5ex}
\subsection{The importance of MultiWay-Sampler}\label{sec:exp-exclusive}
\vspace{-1ex}
In this section, we presented a detailed analysis of the critical components of the feature fusion module: the Shared-Sampler and the Attention-Aggregation.

\textbf{The importance of Shared-Sampler.}~
During feature fusion, several existing models~\cite{Fu2021VIOLETE, Huang2022CloverTA} concatenate video and text features into one sequence and apply a transformer encoder to fuse them, which costs lots of video memory. Another stream of works~\cite{Li2021AlignAP} summary the video feature by pooling strategy, which lose crucial detailed information.
\begin{table}[!t]
\small
\centering
\setlength{\tabcolsep}{1mm}
\resizebox{0.44\textwidth}{!}{
\aboverulesep = 0.55mm
\belowrulesep = 0.55mm
\centering	
\setlength{\tabcolsep}{3mm}
    \begin{tabular}	{l | c c c c}
        \toprule
        \textbf{Methods} & \textbf{MSRVTT-QA} & \textbf{MSVD-QA} & \textbf{Memory-Usage(MB)} \\
        \midrule
            \textbf{Class Token} & 44.54 & 47.90 & 7081 \\
            \textbf{Mean Pooling} & 44.40 & 47.07 & 6941 \\
            \textbf{Max Pooling} & 44.41 & 46.93 & 6963 \\
            \textbf{Flatten + Encoder} & 44.84 & 48.35 & 15791 \\
        \midrule
            \textbf{MultiWay-Sampler} & \textbf{45.54} & \textbf{49.86} & 10551 \\
        \bottomrule
    \end{tabular}}
\caption
{
Ablation studies on feature retention methods. The number of sparse frames is set to 6 for Flatten method.
}
\label{tbl:cls2flatten}
\vspace{-2ex}
\end{table} \begin{table}[!t]
\small
\setlength{\tabcolsep}{1mm}
\resizebox{0.48\textwidth}{!}{
\aboverulesep = 0.55mm
\belowrulesep = 0.55mm
\centering	
\begin{tabular}	{l | c c c c |c c}
        \toprule
            \textbf{Method} & \textbf{Cond. Video} & \textbf{Cond. Text} & \textbf{Shared-Sampler} & \textbf{Att-Agg} & \textbf{MSRVTT-QA} & \textbf{MSVD-QA}\\
        \midrule
             & \ding{52} & \ding{56} & \ding{56} & \ding{56} & 44.76 & 48.10  \\
             & \ding{52} & \ding{52} & \ding{56} & \ding{56} & 44.57 & 48.50 \\
             \textbf{Flatten} & \ding{56} & \ding{52} & \ding{56} & \ding{56} & 45.08 & 49.38  \\
             \textbf{Decoder} & \ding{56} & \ding{52} & \ding{52} & \ding{56} & 45.16 & 49.80  \\
             & \ding{56} & \ding{52} & \ding{56} & \ding{52} & 45.48 & 49.54  \\
             & \ding{56} & \ding{52} & \ding{52} & \ding{52} & \textbf{45.54} & \textbf{49.86}  \\
        \bottomrule
    \end{tabular}}
\caption{An ablation study on feature compression methods. \textbf{Cond.} means Condensed, \textbf{Att-Agg} means Attention-Aggregation. }
\label{tbl:sampler_decoder}
\vspace{-4ex}
\end{table} 
We first compare the performance and video memory consumption of different standard feature aggregation methods (\textit{i.e.} Class Token, Mean Pooling, Max Pooling and Flatten). 
As shown in Table \ref{tbl:cls2flatten}, the Flatten method performs best in several feature aggregation methods mentioned above but occupies a large amount of video memory. 
As analyzed in the section \ref{sec:method-arch}, the decoder costs much less video memory than the encoder when processing long sequences. 
Therefore, we introduce the decoder for feature fusion. 
The decoder is sufficient for processing public datasets such as MSRVTT-QA. 
However, the memory cost is still high when processing long text and video like our multi-label datasets. 
The specific memory cost is shown in Figure~\ref{fig:memory_sampler}. 
Therefore, we introduce the sampler based on the decoder to condense the features. Based on the experimental results, we choose to condense the text feature. 
Then, we share the sampler and decoder in self-attention modules and preserve different FFN for different modalities, which reduces model parameters and has the same performance. 
Compared with the Flatten Method, the Shared-Sampler improves accuracies on MSRVTT-QA and MSVD-QA by 0.32\% and 1.45\%, respectively.


The video features contain much redundant information, while the text features are more semantic and have higher information density~\cite{He2022MaskedAA}. 
Language guidance is necessary to extract useful information and filter redundant information from video representations. 
With text representations as queries, the redundant information is filtered. As shown in Figure~\ref{fig:example_att}, the visual part most relevant to the problem is given more weight.


\textbf{The importance of Attention-Aggregation.}~
The query vector of the Shared-Sampler is randomly initialized, which may lose useful original features during sampling. 
The residual mapping can stabilize network training and introduce shallow features, which is an excellent structure to solve the problem of the Shared-Sampler.
Since the length of the feature sequence changes after sampling, it is a challenge to implement residual mapping. 
We design a lightweight attention-based feature aggregation module called Attention-Aggregation, which obtains the output by calculating the importance of each input part. 
As shown in Table~\ref{tbl:sampler_decoder}, the Attention-Aggregation significantly improves accuracy on MSRVTT-QA by 0.38\%.


\vspace{-1ex}
\subsection{The importance of Attention-Adapter}\label{sec:exp-adapter}
\vspace{-1ex}

In this section, we explore training strategies for freezing part of the encoders instead of optimizing all the layers. 

\begin{table}[h]
\centering
\renewcommand\arraystretch{1.1}
\small
\vspace{-2ex}
\setlength{\tabcolsep}{1mm}
\resizebox{0.38\textwidth}{!}
{\aboverulesep = 0.55mm
    \belowrulesep = 0.55mm
    \begin{tabular}	{l | c c | c c c}
    	\toprule 
    	\multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c|}{\textbf{Frozen / Total}} & \textbf{MSRVTT} & \textbf{MSVD} & \textbf{Memory-Usage}  \\ 
        \specialrule{0em}{1pt}{0pt} \cline{2-3} \specialrule{0em}{1pt}{0pt}
    	 & \textbf{VE} & \textbf{TE} & \textbf{QA} & \textbf{QA} & \textbf{(MB)} \\
        \midrule
        \multirow{7}* {MuLTI-B}
         & 12/12 & 12/12 & 44.06 & 46.83 & 6109  \\
         & 12/12 & 0/12 & 44.07 & 47.12 & 7439 \\
         & 6/12 & 0/12 & 45.10 & 47.57 & 18219 \\ 
         & 9/12 & 0/12 & 45.59 & 47.52 & 11541 \\
         & 9/12 & 3/12 & 45.50 & 49.63 & 11131 \\
         & 9/12 & 6/12 & \textbf{45.54} & \textbf{49.86} & 10551 \\
         & 9/12 & 9/12 & 45.04 & 49.14 & 10283 \\
    	\bottomrule
	\end{tabular}
}
\caption
{
Ablation studies on the different frozen layers. VE refers to video encoder and TE refers to text encoder. 
Frozen/Total refers to the number of frozen layers and total layers respectively. 
}
\label{tbl:partially-frozen}
\vspace{-4ex}
\end{table}		 
\begin{table}[h]
\renewcommand\arraystretch{1.1}
\small
\centering
\setlength{\tabcolsep}{1mm}
\resizebox{0.46\textwidth}{!}
{\aboverulesep = 0.55mm
    \belowrulesep = 0.55mm
    \begin{tabular}	{l | c c c | c c c c}
        \toprule 
        \multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{PFE}} & \multirow{2}{*}{\textbf{Adapter}} & \textbf{Attention} & \textbf{MSRVTT} & \textbf{MSVD} & \textbf{MSRVTT}  & \textbf{DiDeMo} \\ 
    	 &  &  & \textbf{Adapter} & \textbf{QA} & \textbf{QA} & \textbf{Ret} & \textbf{Ret} \\
        \midrule
        \multirow{3}* {MuLTI-B}
         & \ding{52} & \ding{56} & \ding{56} & 45.54 & 49.86 & 60.22 & 51.68\\
         & \ding{52} & \ding{52} & \ding{56} & 45.61 & 50.48 & 60.54 & 52.08 \\
         & \ding{52} & \ding{52} & \ding{52} & \textbf{45.71} & \textbf{50.63} & \textbf{61.16} & \textbf{52.42} \\
    	\bottomrule
	\end{tabular}
}
\caption
{
Ablation studies on the Attention-Adapter. Partially Frozen Encoder (PFE) means to freeze some layers of encoders.
}
\label{tbl:adapter}
\vspace{-2ex}
\end{table}		 
\textbf{Analysis of Frozen Layers.} 
In this section, we systematically evaluate the effect of the number of frozen layers. The results on videoQA are demonstrated in Table~\ref{tbl:partially-frozen}. It indicates that unfreezing the top layers of video and text encoders can improve performance on both datasets. Overall, the Partially Frozen Encoder (PFE)  improves accuracies on MSRVTT-QA and MSVD-QA by 1.48\% and 3.03\%, respectively. Besides, PFE significantly reduces GPU memory consumption at the training stage.

\textbf{Analysis of Attention-Adapter.} 
By analyzing the number of frozen layers, we find that unfreezing too many layers causes a decrease in accuracy. The decrease in accuracy is caused by over-fitting the dataset by adjusting too many parameters.
Thus we introduce the adapter to adapt features from the shallow layers, which is more effective and consumes less memory.
As shown in Table~\ref{tbl:adapter}, the adapter significantly enhances the performance of the model, particularly in the case of MSVD-QA. This indicates that the adapter is especially effective in mitigating overfitting and underfitting issues during training on small datasets.

Although the adapter works well, its representation ability is still limited by the simple FFN module. We further add a lightweight attention module to the adapter, which allows the model to pay more attention to  more informative tokens. The results show that Attention-Adapter improves performance on downstream tasks.

\vspace{-1ex}
\subsection{The importance of Multiple Choice Modeling}\label{sec:exp-pfe}
\vspace{-1ex}
Multiple Choice Modeling (MCM) aims to reduce the task gap between pretraining and downstream tasks by introducing videoQA tasks during pretraining. 
Essentially, MCM looks for the description that best matches the video, which improves the ability of the model to attend to the subject of the sentence and the subject of the video during feature fusion. Thus the MCM enables our model to extract more representative multi-modal features.
We utilize the classical MLM, VTM and VTC tasks for pretraining the model as a baseline. Due to video content corruption caused by MVM, the MVM task conflicts with other tasks~\cite{Lei2021UnderstandingCV}. In our attempts to include MVM for pretraining, we observed a degradation in performance. Thus, we have decided not to use MVM for pretraining.
Compared to the model pretrained with baseline, MCM explicitly improves the model's performance on the videoQA task by narrowing the task gap between pretraining and downstream tasks.
The model's performance on the retrieval tasks is also improved because MCM promotes the alignment of the features from multi-modal encoders. 
As shown in Table \ref{tbl:main_proposal}, the models pretrained with MCM outperformed the baseline in both videoQA and retrieval tasks, demonstrating its effectiveness. 
 \vspace{-1.5ex}
\section{Conclusion}
\vspace{-1ex}
\label{sec:conclusion}
We introduce MuLTI, an efficient and effective video-and-language framework. In MuLTI, we proposed a novel multi-modal feature fusion module based on the MultiWay Sampler, which is more efficient and effective than existing multi-modal fusion methods. We further proposed the Attention-Adapter to finetune the shallow features to improve performance with low memory consumption. Finally, we propose a new pretraining task to enhance the model's sensitivity to videoQA tasks. The MuLTI achieves state-of-the-art performance on seven typical video-and-language tasks based on the above improvements. In the future, we will extend our work to process audio to improve the performance of our model further. 




































































































































{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\onecolumn
\section*{A. Appendix}
\subsection*{A.1. Examples for Multi-Label Classification}
\vspace{-1.5ex}
\begin{multicols}{2} 
In the Appendix, we show some typical examples of multi-label classification. Each example consists of four parts: video sparse frames, text, OCR transcripts and predicted labels. In each example, \textbf{Text} refers to the description text of the video, \textbf{OCR} is the combination of the OCR transcripts for each sparse frame in the video, and \textbf{Pred} is the predicted labels of MuLTI. The \textbf{...} indicates that the OCR transcripts are truncated due to limited space. The threshold value of the predicted label is 0.5. Red predicted labels are the wrong predicted labels.
\end{multicols}
\begin{figure*}[ht]
\centering
\includegraphics[width=0.91\textwidth]{imgs/Appendix_crop}
\vspace{1ex}
\caption{
Examples of multi-label classification. \textbf{Text} refers to the description text of the video. \textbf{OCR} refers to the combination of OCR transcripts of each sparse frame in the video. \textbf{Pred} refers to the prediction results of MuLTI. The \textbf{...} indicates that the OCR transcripts are truncated due to limited space. The threshold value of the predicted label is 0.5. Red predicted labels are the wrong predicted labels.
}
\vspace{1ex}
\label{fig:appendix}
\end{figure*}
  \end{document}
