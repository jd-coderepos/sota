\documentclass{article}







\usepackage[final]{neurips_2019}

\usepackage{wrapfig}
\makeatletter
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tkz-euclide}  
\usepackage{amsmath}
\usepackage{thmtools}
\usepackage{enumitem}
\usepackage{thm-restate}
\usepackage{graphicx}

\usepackage{floatrow}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\usepackage[export]{adjustbox}

\declaretheorem[name=Theorem,numberwithin=section]{thm}
\declaretheorem[name=Corollary,numberwithin=section]{corr}
\newcommand{\yell}{\textcolor{red}}
\newcommand{\HGCN}{\name}

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{microtype}      \usepackage{xcolor}
\usepackage{url}
\usepackage{verbatim} \usepackage{graphicx}
\usepackage{caption} 
\usepackage{multirow}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{xspace}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{times}
\usepackage{xr}
\usepackage{bbm}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{hyperref}       \usepackage{cleveref}

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0.5\topsep}
\setlist[description]{noitemsep, topsep=0.5\topsep}
\setlist[itemize]{noitemsep, topsep=0.5\topsep}

\newcommand{\note}[1]{{{\textcolor{blue}{[note: #1]}}}}
\newcommand{\jure}[1]{{{\textcolor{magenta}{[JL: #1]}}}}
\definecolor{c1}{RGB}{255,127,0}
\definecolor{c2}{RGB}{127,0,127}
\newcommand{\ines}[1]{\textcolor{c1}{[ines: #1]}}
\newcommand{\rex}[1]{\textcolor{c2}{[rex: #1]}}

\newcommand{\xhdr}[1]{{\noindent\bfseries #1}.}
\newcommand{\red}[1]{{\textcolor{red}{#1}}}
\newcommand{\CITE}{{\textcolor{red}{[CITE]}}}
\newcommand{\name}{\textsc{HGCN}\xspace}
\newcommand{\gnn}{\textrm{GNN}}
\newcommand{\hide}[1]{}

\newcommand{\V}{\mathcal{V}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\R}{\mathbb{R}}



\newcommand{\nedge}{k}
\newcommand{\mb}{\mathbf}
\newcommand{\cut}[1]{}
\newcommand{\casc}{{\mathbf{t}}}
\newcommand{\alphs}{{\mathbf{A}}}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{\textbf{Observation}}
\newcommand{\xsketch}{\noindent {\it Proof Sketch.}}
\newcommand{\netinf}{{\textsc{Net\-Inf}}\xspace}
\newcommand{\multitree}{{\textsc{Multi\-Tree}}\xspace}
\newcommand{\netrate}{{\textsc{Net\-Rate}}\xspace}
\newcommand{\connie}{{\textsc{Co\-nNIe}}\xspace}
\newcommand{\maxinf}{{\textsc{Max\-Inf}}\xspace}
\newcommand{\expo}{{\textsc{Exp}}\xspace}
\newcommand{\pow}{{\textsc{Pow}}\xspace}
\newcommand{\ray}{{\textsc{Ray}}\xspace}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\unobs}{{\infty}}
\newcommand\T{\rule{0pt}{2.6ex}}
\newcommand\B{\rule[-1.2ex]{0pt}{0pt}}
\DeclareMathOperator*{\argmax}{argmax}


\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\etal}{\textit{et al}.~}


\newcommand{\ba}{Barab\'asi-Albert }
\newcommand{\er}{Erd\H{o}s-R\'enyi}
 \usepackage{cleveref}

\title{Appendix for ``Hyperbolic Graph Convolutional Neural Networks''}



\author{Anonymous Authors
}

\begin{document}

\maketitle

\appendix
\section{Review of Differential Geometry}\label{appendix:diff_geom}
We first recall some definitions of differential and hyperbolic geometry.

\subsection{Differential geometry}
\xhdr{Manifold} An $d-$dimensional \textit{manifold} $\mathcal{M}$ is a topological space that locally resembles the topological space $\mathbb{R}^d$ near each point. 
More concretely, for each point $\mathbf{x}$ on $\mathcal{M}$, we can find a \textit{homeomorphism} (continuous bijection with continuous inverse) between a neighbourhood of $\mathbf{x}$ and $\mathbb{R}^d$.
The notion of manifold is a generalization of surfaces in high dimensions.

\xhdr{Tangent space}
Intuitively, if we think of $\mathcal{M}$ as a $d-$dimensional manifold embedded in $\mathbb{R}^{d+1}$, the \textit{tangent space} $\mathcal{T}_\mathbf{x}\mathcal{M}$ at point $\mathbf{x}$ on $\mathcal{M}$ is a $d-$dimensional hyperplane in $\mathbb{R}^{d+1}$ that best approximates $\mathcal{M}$ around $\mathbf{x}$. Another possible interpretation for $\mathcal{T}_\mathbf{x}\mathcal{M}$ is that it contains all the possible directions of curves on $\mathcal{M}$ passing through $\mathbf{x}$.
The elements of $\mathcal{T}_\mathbf{x}\mathcal{M}$ are called \textit{tangent vectors} and the union of all tangent spaces is called the \textit{tangent bundle} $\mathcal{T}\mathcal{M}=\cup_ {\mathbf{x}\in\mathcal{M}}\mathcal{T}_\mathbf{x}\mathcal{M}$. 

\xhdr{Riemannian manifold} A \textit{Riemannian manifold} is a pair $(\mathcal{M}, \mathbf{g})$, where $\mathcal{M}$ is a smooth manifold and $\mathbf{g}=(g_\mathbf{x})_{\mathbf{x}\in\mathcal{M}}$ is a \textit{Riemannian metric}, that is a family of smoothly varying inner products on tangent spaces, $g_\mathbf{x}:\mathcal{T}_\mathbf{x}\mathcal{M}\times\mathcal{T}_\mathbf{x}\mathcal{M}\rightarrow\mathbb{R}$.
Riemannian metrics can be used to measure distances on manifolds. 

\xhdr{Distances and geodesics} Let $(\mathcal{M}, \mathbf{g})$ be a Riemannian manifold. For $\mathbf{v}\in\mathcal{T}_\mathbf{x}\mathcal{M}$, define the norm of $\mathbf{v}$ by $||\mathbf{v}||_\mathbf{g} \coloneqq \sqrt{g_\mathbf{x}(\mathbf{v}, \mathbf{v})}$.
Suppose $\gamma: [a, b] \rightarrow \mathcal{M}$ is a smooth curve on $\mathcal{M}$. Define the length of $\gamma$ by: 
$$L(\gamma) \coloneqq \int_{a}^{b} ||\gamma'(t)||_\mathbf{g}dt.$$
Now with this definition of length, every connected Riemannian manifold  becomes a metric space and the \textit{distance} $d:\mathcal{M}\times\mathcal{M}\rightarrow[0,\infty)$ is defined as:
$$d(\mathbf{x}, \mathbf{y})\coloneqq\mathrm{inf}_\gamma\{L(\gamma): \gamma \text{ is a continuously differentiable curve joining }\mathbf{x}\text{ and }\mathbf{y}\}.$$
\textit{Geodesic} distances are a generalization of straight lines (or shortest paths) to non-Euclidean geometry. 
A curve $\gamma:[a,b]\rightarrow\mathcal{M}$ is \textit{geodesic} if $d(\gamma(t), \gamma(s))=L(\gamma|_{[t,s]})\forall(t,s)\in[a, b] (t<s)$.

\xhdr{Parallel transport} \textit{Parallel transport} is a generalization of translation to non-Euclidean geometry.
Given a smooth manifold $\mathcal{M}$, parallel transport $P_{\mathbf{x}\rightarrow\mathbf{y}}(\cdot)$ maps a vector $\mathbf{v}\in\mathcal{T}_\mathbf{x}\mathcal{M}$ to $P_{\mathbf{x}\rightarrow\mathbf{y}}(\mathbf{v})\in\mathcal{T}_\mathbf{y}\mathcal{M}$. 
In Riemannian geometry, parallel transport preserves the Riemannian metric tensor (norm, inner products...).

\xhdr{Curvature}
At a high level, curvature measures how much a geometric object such as surfaces deviate from a flat plane. 
For instance, the Euclidean space has zero curvature while spheres have positive curvature. 
We illustrate the concept of curvature in Figure \ref{fig:manifold_curvature}.

\subsection{Hyperbolic geometry}
\xhdr{Hyperbolic space} The hyperbolic space in $d$ dimensions is the unique complete, simply connected $d-$dimensional Riemannian manifold with constant negative sectional curvature.
There exist several models of hyperbolic space such as the Poincar\'e model or the hyperboloid model (also known as the Minkowski model or the Lorentz model).
In what follows, we review the Poincar\'e and the hyperboloid models of hyperbolic space as well as connections between these two models. 

\subsubsection{Poincar\'e ball model}
Let $||.||_2$ be the Euclidean norm. 
The Poincar\'e ball model with unit radius and constant negative curvature $-1$ in $d$ dimensions is the Riemannian manifold $(\mathbb{D}^{d,1},(g_\mathbf{x})_\mathbf{x})$ where
$$\mathbb{D}^{d,1}\coloneqq\{\mathbf{x}\in\mathbb{R}^d:||\mathbf{x}||^2<1\},$$
and
$$g_\mathbf{x}=\lambda_\mathbf{x}^2I_d,$$
where $\lambda_\mathbf{x}\coloneqq\frac{2}{1-||\mathbf{x}||_2^2}$ and $I_d$ is the identity matrix.  
The induced distance between two points $(\mathbf{x}, \mathbf{y})$ in $\mathbb{D}^{d,1}$ can be computed as:
$$d_\mathbb{D}^1(\mathbf{x}, \mathbf{y})=\mathrm{arcosh}\bigg(1 + 2\frac{||\mathbf{x}-\mathbf{y}||_2^2}{(1-||\mathbf{x}||_2^2)(1-||\mathbf{y}||_2^2)}\bigg).$$

\begin{figure}[t]
\begin{center}
    \includegraphics[width=\textwidth]{figs/manifold_curvature.pdf}
  \end{center}
\caption{From left to right: a surface of negative curvature, a surface of zero curvature, and a surface of positive curvature.}
  \label{fig:manifold_curvature}
\end{figure}
\subsubsection{Hyperboloid model}
\xhdr{Hyperboloid model}
Let $\langle.,.\rangle_{\mathcal{L}}:\mathbb{R}^{d+1}\times\mathbb{R}^{d+1}\rightarrow\mathbb{R}$ denote the Minkowski inner product,
$$\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}\coloneqq-x_0y_0+x_1y_1+\ldots+x_dy_d.$$
The hyperboloid model with unit imaginary radius and constant negative curvature $-1$ in $d$ dimensions is defined as the Riemannian manifold $(\mathbb{H}^{d,1},(g_\mathbf{x})_\mathbf{x})$ where
$$\mathbb{H}^{d,1}\coloneqq\{\mathbf{x}\in\mathbb{R}^{d+1}:\langle\mathbf{x},\mathbf{x}\rangle_\mathcal{L}=-1,x_0>0\},$$
and
\begin{small}
$$g_\mathbf{x}\coloneqq\begin{bmatrix}
-1 & & & \\
& 1 & & \\
& & \ddots & \\
& & & 1 
\end{bmatrix}.$$
\end{small}
The induced distance between two points $(\mathbf{x}, \mathbf{y})$ in $\mathbb{H}^{d,1}$ can be computed as:
$$d^1_\mathcal{L}(\mathbf{x},\mathbf{y})=\mathrm{arcosh}(-\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}).$$


\xhdr{Geodesics} 
We recall a result that gives the unit speed geodesics in the hyperboloid model with curvature $-1$ \cite{robbin2011introduction}.
This result can be used to show Propositions \ref{cor:geodesic} and \ref{cor:logexp} for the hyperboloid manifold with negative curvature $-1/K$, and then learn $K$ as a model parameter in \name. 
\begin{restatable}[]{thm}{unitspeed}
\label{thm:unit_speed}
Let $\mathbf{x}\in\mathbb{H}^{d,1}$ and $\mathbf{u}\in\mathcal{T}_\mathbf{x}\mathbb{H}^{d,1}$ unit-speed (i.e. $\langle\mathbf{u},\mathbf{u}\rangle_\mathcal{L}=1$).
The unique unit-speed geodesic $\gamma_{\mathbf{x}\rightarrow\mathbf{u}}:[0,1]\rightarrow\mathbb{H}^{d,1}$ such that $\gamma_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{x}$ and $\dot\gamma_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{u}$ is given by:
$$\gamma_{\mathbf{x}\rightarrow\mathbf{u}}(t)=\mathrm{cosh}(t)\mathbf{x}+\mathrm{sinh}(t)\mathbf{u}.$$
\end{restatable}

\xhdr{Parallel Transport}
If two points $\mathbf{x}$ and $\mathbf{y}$ on the hyperboloid $\mathbb{H}^{d,1}$ are connected by a geodesic, then the parallel transport of a tangent vector $\mathbf{v} \in \mathcal{T}_{\mathbf{x}}\mathbb{H}^{d,1}$ to the tangent space $\mathcal{T}_{\mathbf{y}}\mathbb{H}^{d,1}$ is:
\begin{equation}
P_{\mathbf{x}\rightarrow \mathbf{y}}(\mathbf{v})=\mathbf{v}-\frac{\langle\mathrm{log}_\mathbf{x}(\mathbf{y}),\mathbf{v}\rangle_\mathcal{L}}{d^1_\mathcal{L}(\mathbf{x}, \mathbf{y})^2}(\mathrm{log}_\mathbf{x}(\mathbf{y})+\mathrm{log}_\mathbf{y}(\mathbf{x})).
\end{equation}

\xhdr{Projections}
Finally, we recall projections to the hyperboloid manifold and its corresponding tangent spaces. 
A point $\mathbf{x}=(x_0,\mathbf{x}_{1:d})\in\mathbb{R}^{d+1}$ can be projected on the hyperboloid manifold $\mathbb{H}^{d,1}$ with:
\begin{equation}
    \Pi_{\mathbb{R}^{d+1}\rightarrow\mathbb{H}^{d,1}}(\mathbf{x})\coloneqq(\sqrt{1+||\mathbf{x}_{1:d}||_2^2}, \mathbf{x}_{1:d}).\label{eq:proj_hyp}
\end{equation}
Similarly, a point $\mathbf{v}\in\mathbb{R}^{d+1}$ can be projected on $\mathcal{T}_\mathbf{x}\mathbb{H}^{d,1}$ with:
\begin{equation}
    \Pi_{\mathbb{R}^{d+1}\rightarrow\mathcal{T}_\mathbf{x}\mathbb{H}^{d,1}}(\mathbf{v})\coloneqq\mathbf{v}+\langle\mathbf{x},\mathbf{v}\rangle_\mathcal{L}\mathbf{x}.\label{eq:proj_tan}
\end{equation}
In practice, these projections are very useful for optimization purposes as they constrain embeddings and tangent vectors to remain on the manifold and tangent spaces.

\subsubsection{Connection between the Poincar\'e ball model and the hyperboloid model}
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figs/hyperboloid.png}
  \end{center}
  \caption{Illustration of the hyperboloid model (top) in 3 dimensions and its connection to the Poincar\'e disk (bottom).}
  \label{fig:hyperboloid}
\end{figure}
While the hyperboloid model tends to be more stable for optimization than the Poincar\'e model \cite{nickel2018learning}, the Poincar\'e model is very interpretable and embeddings can be directly visualized on the Poincar\'e disk. 
Fortunately, these two models are isomorphic (\emph{cf.} Figure \ref{fig:hyperboloid}) and there exist a diffeomorphism $\Pi_{\mathbb{H}^{d,1}\rightarrow\mathbb{D}^{d,1}}(\cdot)$ mapping one space onto the other:
\begin{align}
    \Pi_{\mathbb{H}^{d,1}\rightarrow\mathbb{D}^{d,1}}(x_0,\ldots,x_d)&=\frac{(x_1,\ldots,x_d)}{x_0+1}\\
    \text{and }\ \  \Pi_{\mathbb{D}^{d,1}\rightarrow\mathbb{H}^{d,1}}(x_1,\ldots,x_d)&=\frac{(1+||\mathbf{x}||_2^2,2x_1,\ldots,2x_d)}{1-||\mathbf{x}||_2^2}.
\end{align}

\section{Proofs of Results}
\subsection{Hyperboloid model of hyperbolic space}
For completeness, we re-derive results of hyperbolic geometry for any arbitrary curvature. 
Similar derivations can be found in the literature \cite{wilson2014spherical}.
\geodesics*
\begin{proof}
Using theorem \ref{thm:unit_speed}, we know that the unique unit-speed geodesic $\gamma_{\mathbf{y}\rightarrow\mathbf{u}}(.)$ in $\mathbb{H}^{d,1}$ must satisfy
\begin{align*}
    \gamma_{\mathbf{y}\rightarrow\mathbf{u}}(0)=\mathbf{y} \ \text{ and }\ \dot{\gamma}_{\mathbf{y}\rightarrow\mathbf{u}}(0)=\mathbf{u} \ \text{and}\ 
    \frac{d}{dt}\langle\dot{\gamma}_{\mathbf{y}\rightarrow\mathbf{u}}(t),\dot{\gamma}_{\mathbf{y}\rightarrow\mathbf{u}}(t)\rangle_\mathcal{L}=0 \ \forall t,
\end{align*}
and is given by
$$\gamma_{\mathbf{y}\rightarrow\mathbf{u}}(t)=\mathrm{cosh}(t)\mathbf{y}+\mathrm{sinh}(t)\mathbf{u}.$$
Now let $\mathbf{x}\in\mathbb{H}^{d,K}$ and $\mathbf{u}\in\mathcal{T}_\mathbf{x}\mathbb{H}^{d,K}$ be unit-speed
and denote $\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(.)$ the unique unit-speed geodesic in $\mathbb{H}^{d,K}$ such that 
$\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{x} \ \text{ and }\ \dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{u}$. 
Let us define $\mathbf{y}:=\frac{\mathbf{x}}{\sqrt{K}}\in\mathbb{H}^{d,1}$ and $\phi_{\mathbf{y}\rightarrow\mathbf{u}}(t)=\frac{1}{\sqrt{K}}\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(\sqrt{K}t)$. We have,
\begin{align*}
\phi_{\mathbf{y}\rightarrow\mathbf{u}}(0)=\mathbf{y} \ \text{ and }\ \dot{\phi}_{\mathbf{y}\rightarrow\mathbf{u}}(0)=\mathbf{u}, \
\end{align*}
and since $\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(.)$ is the unique unit-speed geodesic in $\mathbb{H}^{d,K}$, we also have
$$\frac{d}{dt}\langle\dot{\phi}_{\mathbf{y}\rightarrow\mathbf{u}}(t),\dot{\phi}_{\mathbf{y}\rightarrow\mathbf{u}}(t)\rangle_\mathcal{L}=0 \ \forall t.$$
Furthermore, we have $\mathbf{y}\in\mathbb{H}^{d,1}$, $\mathbf{u}\in\mathcal{T}_{\mathbf{y}}\mathbb{H}^{d,1}$ as $\langle\mathbf{u},\mathbf{y}\rangle_\mathcal{L}=\frac{1}{\sqrt{K}}\langle\mathbf{u},\mathbf{x}\rangle_\mathcal{L}=0$ and $\langle\phi_{\mathbf{y}\rightarrow\mathbf{u}}(t),\phi_{\mathbf{y}\rightarrow\mathbf{u}}(t)\rangle_\mathcal{L}=-1\forall t$.
Therefore $\phi_{\mathbf{y}\rightarrow\mathbf{u}}(.)$ is a unit-speed geodesic in $\mathbb{H}^{d,1}$ and we get
$$\phi_{\mathbf{y}\rightarrow\mathbf{u}}(t)=\mathrm{cosh}(t)\mathbf{y}+\mathrm{sinh}(t)\mathbf{u}.$$
Finally, this leads to 
$$\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(t)=\mathrm{cosh}(\frac{t}{\sqrt{K}})\mathbf{x}+\sqrt{K}\mathrm{sinh}(\frac{t}{\sqrt{K}})\mathbf{u}.$$

\begin{comment}
We know that the unique unit-speed geodesic $\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(.)$ in $\mathbb{H}^{d,K}$ must satisfy
\begin{equation}
    \gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{x} \ \text{ and }\ \dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{u} \ \text{and}\ 
    \frac{d}{dt}\langle\dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(t),\dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(t)\rangle_\mathcal{L}=0 \ \forall t.\label{eq:geodesic_cond}
\end{equation}
Let $\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(t)=\mathrm{cosh}(\frac{t}{\sqrt{K}})\mathbf{x}+\sqrt{K}\mathrm{sinh}(\frac{t}{\sqrt{K}})\mathbf{u}$. 
We have $\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{x} \ \text{ and }\ \dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{u}$. 
Furthermore, since $\mathbf{u}\in\mathcal{T}_{\mathbf{x}}\mathbb{H}^{d,K}$, we have $\langle\mathbf{u},\mathbf{x}\rangle_\mathcal{L}=0$ and for all $t$:
\begin{align*}
    \langle{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(t),{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(t)\rangle_\mathcal{L}&=\mathrm{cosh}^2(\frac{t}{\sqrt{K}})\langle\mathbf{x}, \mathbf{x}\rangle_\mathcal{L}+K\mathrm{sinh}^2(\frac{t}{\sqrt{K}})\langle\mathbf{u}, \mathbf{u}\rangle_\mathcal{L}\\
    &= -K\mathrm{cosh}^2(\frac{t}{\sqrt{K}}) + K\mathrm{sinh}^2(\frac{t}{\sqrt{K}})\\
    &= -K. 
\end{align*}
Therefore, ${\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(\cdot)$ is a curve on $\mathbb{H}^{d,K}$. 
Furthermore, we have $\dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(t)=\frac{1}{\sqrt{K}}\mathrm{sinh}(\frac{t}{\sqrt{K}})\mathbf{x}+\mathrm{cosh}(\frac{t}{\sqrt{K}})\mathbf{u}$ and therefore
\begin{align*}
    \frac{d}{dt}\langle\dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(t),\dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(t)\rangle_\mathcal{L}&=\frac{d}{dt}(\frac{1}{K}\mathrm{sinh}^2(\frac{t}{\sqrt{K}})\langle\mathbf{x}, \mathbf{x}\rangle_\mathcal{L}+\mathrm{cosh}^2(\frac{t}{\sqrt{K}})\langle\mathbf{u}, \mathbf{u}\rangle_\mathcal{L})\\
    &=\frac{d}{dt}(-\mathrm{sinh}^2(\frac{t}{\sqrt{K}}) + \mathrm{cosh}^2(\frac{t}{\sqrt{K}}))\\
    &=0.
\end{align*}
Finally, $\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(.)$ verifies all the conditions in Equation \ref{eq:geodesic_cond} and is therefore the unique unit-speed geodesic on $\mathbb{H}^{d,K}$ such that $\gamma^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{x}$ and $\dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{u}$. 
\end{comment}
\end{proof}

\logexp*
\begin{proof}
We use a similar reasoning to that in Corollary 1.1 in \cite{ganea2018hyperbolic}.
Let $\gamma^K_{\mathbf{x}\rightarrow\mathbf{v}}(.)$ be the unique geodesic such that $\gamma^K_{\mathbf{x}\rightarrow\mathbf{v}}(0)=\mathbf{x}$ and $\dot{\gamma}^K_{\mathbf{x}\rightarrow\mathbf{v}}(0)=\mathbf{v}$.
Let us define $\mathbf{u}:=\frac{\mathbf{v}}{\ \ ||\mathbf{v}||_\mathcal{L}}$ where $||\mathbf{v}||_\mathcal{L}=\sqrt{\langle\mathbf{v},\mathbf{v}\rangle_\mathcal{L}}$ is the Minkowski norm of $\mathbf{v}$
and $$\phi^K_{\mathbf{x}\rightarrow\mathbf{u}}(t):=\gamma^K_{\mathbf{x}\rightarrow\mathbf{v}}\left(\frac{t}{\ \ ||\mathbf{v}||_\mathcal{L}}\right).$$
$\phi_{\mathbf{x}\rightarrow\mathbf{u}}(t)$ satisfies,
\begin{align*}
\phi^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{x} \ \text{ and }\ \dot{\phi}^K_{\mathbf{x}\rightarrow\mathbf{u}}(0)=\mathbf{u} \ \text{and}\ 
\frac{d}{dt}\langle\dot{\phi}^K_{\mathbf{x}\rightarrow\mathbf{u}}(t),\dot{\phi}^K_{\mathbf{x}\rightarrow\mathbf{u}}(t)\rangle_\mathcal{L}=0 \ \forall t.
\end{align*}
Therefore $\phi^K_{\mathbf{x}\rightarrow\mathbf{u}}(.)$ is a unit-speed geodesic in $\mathbb{H}^{d,K}$ and we get
$$\phi^K_{\mathbf{x}\rightarrow\mathbf{u}}(t)=\mathrm{cosh}(\frac{t}{\sqrt{K}})\mathbf{x}+\sqrt{K}\mathrm{sinh}(\frac{t}{\sqrt{K}})\mathbf{u}.$$
By identification, this leads to 
$$\gamma^K_{\mathbf{x}\rightarrow\mathbf{v}}(t)=\mathrm{cosh}\bigg(\frac{||\mathbf{v}||_\mathcal{L}}{\sqrt{K}}t\bigg)\mathbf{x}+\sqrt{K}\mathrm{sinh}\bigg(\frac{||\mathbf{v}||_\mathcal{L}}{\sqrt{K}}t\bigg)\frac{\mathbf{v}}{\ \ ||\mathbf{v}||_\mathcal{L}}.$$
We can use this result to derive exponential and logarthimic maps on the hyperboloid model.
We know that $\mathrm{exp}^K_\mathbf{x}(\mathbf{v})=\gamma^K_{\mathbf{x}\rightarrow\mathbf{v}}(1)$. 
Therefore we get,
$$\mathrm{exp}^K_\mathbf{x}(\mathbf{v})=\mathrm{cosh}\bigg(\frac{||\mathbf{v}||_\mathcal{L}}{\sqrt{K}}\bigg)\mathbf{x}+\sqrt{K}\mathrm{sinh}\bigg(\frac{||\mathbf{v}||_\mathcal{L}}{\sqrt{K}}\bigg)\frac{\mathbf{v}}{\ \ ||\mathbf{v}||_\mathcal{L}}.$$
Now let $\mathbf{y}=\mathrm{exp}^K_\mathbf{x}(\mathbf{v})$.
We have $\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}=-K\mathrm{cosh}\bigg(\frac{||\mathbf{v}||_\mathcal{L}}{\sqrt{K}}\bigg)$ as $\langle\mathbf{x},\mathbf{x}\rangle_\mathcal{L}=-K$ 
and $\langle\mathbf{x},\mathbf{v}\rangle_\mathcal{L}=0$.
Therefore $\mathbf{y}+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}\mathbf{x}=\sqrt{K}\mathrm{sinh}\bigg(\frac{||\mathbf{v}||_\mathcal{L}}{\sqrt{K}}\bigg)\frac{\mathbf{v}}{\ \ ||\mathbf{v}||_\mathcal{L}}$ and we get
$$\mathbf{v}=\sqrt{K}\mathrm{arsinh}\bigg(\frac{||\mathbf{y}+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}\mathbf{x}||_\mathcal{L}}{\sqrt{K}}\bigg)\frac{\mathbf{y}+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}\mathbf{x}}{||\mathbf{y}+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}\mathbf{x}||_\mathcal{L}},$$
where $||\mathbf{y}+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}||_\mathcal{L}$ is well defined since $\mathbf{y}+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}\mathbf{x}\in\mathcal{T}_{\mathbf{x}}\mathbb{H}^{d,K}$.
Note that,
\begin{align*}
	||\mathbf{y}+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}\mathbf{x}||_\mathcal{L}&=\sqrt{\langle\mathbf{y},\mathbf{y}\rangle_\mathcal{L}+\frac{2}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}^2+\frac{1}{K^2}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}^2\langle\mathbf{x},\mathbf{x}\rangle_\mathcal{L}}\\
	&=\sqrt{-K+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}^2}\\
	&=\sqrt{K}\sqrt{\langle\frac{\mathbf{x}}{\sqrt{K}},\frac{\mathbf{y}}{\sqrt{K}}\rangle_\mathcal{L}^2-1}\\
	&=\sqrt{K}\mathrm{sinh}\ \mathrm{arcosh}\bigg(-\langle\frac{\mathbf{x}}{\sqrt{K}},\frac{\mathbf{y}}{\sqrt{K}}\rangle_\mathcal{L}\bigg)
\end{align*}
as $\langle\frac{\mathbf{x}}{\sqrt{K}},\frac{\mathbf{y}}{\sqrt{K}}\rangle_\mathcal{L}\le-1$.
Therefore, we finally have
$$\mathrm{log}_{\mathbf{x}}^K(\mathbf{y})=\sqrt{K}\mathrm{arcosh}\bigg(-\langle\frac{\mathbf{x}}{\sqrt{K}},\frac{\mathbf{y}}{\sqrt{K}}\rangle_\mathcal{L}\bigg)\frac{\mathbf{y}+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}\mathbf{x}}{||\mathbf{y}+\frac{1}{K}\langle\mathbf{x},\mathbf{y}\rangle_\mathcal{L}\mathbf{x}||_\mathcal{L}}.$$
\end{proof}

\subsection{Curvature}\label{appendix:curvature}


\begin{restatable}[]{lemma}{curv}
\label{thm:curv}
For any hyperbolic spaces with constant curvatures $-1/K, -1/K'>0$, and any pair of hyperbolic points  $(\mathbf{u}, \mathbf{v})$ embedded in $\mathbb{H}^{d,K}$, there exists a mapping $\phi: \mathbb{H}^{d,K} \rightarrow \mathbb{H}^{d,K'}$ to another pair of corresponding hyperbolic points in $\mathbb{H}^{d,K'}$, $(\phi(\mathbf{u}), \phi(\mathbf{v}))$ such that the Minkowski inner product is scaled by a constant factor.
\end{restatable}
\begin{proof}
For any hyperbolic embedding $\mathbf{x} = (x_0, x_1, \ldots, x_d) \in \mathbb{H}^{d,K}$ we have the identity:
$\langle \mathbf{x}, \mathbf{x} \rangle_\mathcal{L} = -x_0^2 + \sum_{i=1}^d x_i^2 = -K$. 
For any hyperbolic curvature $-1/K<0$, consider the mapping $\phi(\mathbf{x}) = \sqrt{\frac{K'}{K}} \mathbf{x}$. Then we have the identity
$\langle \phi(\mathbf{x}), \phi(\mathbf{x}) \rangle_\mathcal{L} = -K'$ and therefore $\phi(\mathbf{x}) \in \mathbb{H}^{d,K'}$.
For any pair $(\mathbf{u}$, $\mathbf{v})$, $\langle \phi(\mathbf{u}), \phi(\mathbf{v}) \rangle_\mathcal{L}
= \frac{K'}{K} \left( -\mathbf{u}_0 \mathbf{v}_0 + \sum_{i=1}^d \mathbf{u}_i \mathbf{v}_i \right)
= \frac{K'}{K}\langle \mathbf{u}, \mathbf{v} \rangle_\mathcal{L}$. 
The factor $\frac{K'}{K}$ only depends on curvature, but not the specific  embeddings.
\end{proof}

Lemma \ref{thm:curv} implies that given a set of embeddings learned in hyperbolic space $\mathbb{H}^{d,K}$, we can find embeddings in another hyperbolic space with different curvature, $\mathbb{H}^{d,K'}$, such that the Minkowski inner products for all pairs of embeddings are scaled by the same factor $\frac{K'}{K}$.

For link prediction tasks, Theorem \ref{thm:loss} shows that with infinite precision, the expressive power of hyperbolic spaces with varying curvatures is the same.

\vspace{5pt}
\loss*
\begin{proof}
The Fermi-Dirac decoder predicts that there exists a link between node $i$ and $j$ \emph{iif}
$\left[ e^{(d_\mathcal{L}^K(\mathbf{h}_i, \mathbf{h}_j)-r)/t }+ 1 \right]^{-1} \ge b$, where $b \in (0,1)$ is the threshold for determining existence of links.
The criterion is equivalent to $d_\mathcal{L}^K(\mathbf{h}_i, \mathbf{h}_j) \le r + t \log (\frac{1-b}{b})$.

Given $H = \{\mathbf{h_1}, \ldots, \mathbf{h_n}\}$, the graph $G_H$ reconstructed with the Fermi-Dirac decoder has the edge set $E_H = \left\{ (i, j) | d_\mathcal{L}^K(\mathbf{h}_i, \mathbf{h}_j) \le r + t \log (\frac{1-b}{b}) \right\}$.
Consider the mapping to $\mathbb{H}^{d,K'}$, $\phi(\mathbf{x}):= \ \sqrt{\frac{K'}{K}} \mathbf{x}$.
Let $H' = \{\phi(\mathbf{h_1}), \ldots, \phi(\mathbf{h_n})\}$.
By Lemma \ref{thm:curv}, 
\begin{equation}
    d_\mathcal{L}^{K'}(\phi(\mathbf{h}_i), \phi(\mathbf{h}_j)) = \sqrt{K'} \mathrm{arcosh} \left( - \frac{K'}{K} \langle\mathbf{h}_i,\mathbf{h}_j\rangle_\mathcal{L} / K' \right)
    = \sqrt{\frac{K'}{K}} d_\mathcal{L}^{K}(\mathbf{h}_i, \mathbf{h}_j).
\end{equation}
Due to linearity, we can find decoder parameter, $r'$ and $t'$ that satisfy 
$ r' + t' \log (\frac{1-b}{b}) = \sqrt{\frac{K'}{K}} ( r + t \log (\frac{1-b}{b}))$.
With such $r'$, $t'$, the criterion $d_\mathcal{L}^K(\mathbf{h}_i, \mathbf{h}_j) \le r + t \log (\frac{1-b}{b})$
is equivalent to $d_\mathcal{L}^{K'}(\phi(\mathbf{h}_i), \phi(\mathbf{h}_j)) \le r' + t' \log (\frac{1-b}{b})$.
Therefore, the reconstructed graph $G_{H'}$ based on the set of embeddings $H'$ is identical to $G_H$.
\end{proof}

\cut{
\subsection{Lorentzian Centroid Appoximation}
Aggregation via the Lorentzian Centroid Approximation can be defined as
\begin{align}
w_{ij} &= \textsc{Att}(\mathbf{h}^{\ell+1,E}_i, \mathbf{h}^{\ell+1,E}_j) & \text{(attention)}\\ 
    \mathbf{x}^{\ell+1,H}_i &= \sqrt{K_{\ell+1}}\frac{\sum_j w_{ij} \mathbf{x}_j}{|{||\sum_j w_{ij} \mathbf{x}_j||_\mathcal{L}|}} & \text{(aggregation)}
\end{align}}



\section{Experimental Details}
\subsection{Dataset statistics}
We detail the dataset statistics in Table \ref{table:dataset}.


\begin{table}
    \centering
	\begin{tabular}{r c c c c} 
	\hline
	Name & Nodes & Edges & Classes & Node features  \\
	\hline
	\textsc{Cora} & 2708 & 5429 & 7 & 1433 \\
	{\textsc{Pubmed}} & 19717 & 88651 & 3 & 500 \\
	{\textsc{Human PPI}} & 17598 & 5429 & 4 & 17 \\
	{\textsc{Airport}} & 3188 & 18631 & 4 & 4 \\
	{\textsc{Disease}} & 1044 & 1043 & 2 & 1000 \\
	{\textsc{Disease-M}} & 43193 & 43102 & 2 & 1000 \\
	\hline
	\end{tabular}
    \caption{Benchmarks' statistics}
    \label{table:dataset}
\end{table}

\subsection{Training details}
Here we present details of \name's training pipeline, with optimization and incorporation of DropConnect \cite{wan2013regularization}. 


\xhdr{Parameter optimization}
Recall that linear transformations and attention are defined on the tangent space of points. 
Therefore the linear layer and attention parameters are Euclidean.
For bias, there are two options: one can either define parameters in hyperbolic space, and use hyperbolic addition operation \cite{ganea2018hyperbolicNN}, or define parameters in Euclidean space, and use Euclidean addition after transforming the points into the tangent space.
Through experiments we find that Euclidean optimization is much more stable, and gives slightly better test performance compared to Riemannian optimization, if we define parameters such as bias in hyperbolic space.
Hence different from shallow hyperbolic embeddings, although our model and embeddings are hyperbolic, the learnable graph convolution parameters can be optimized via Euclidean optimization (Adam Optimizer \cite{kingma2014adam}), thanks to exponential and logarithmic maps.
Note that to train shallow Poincar\'e embeddings, we use Riemannian Stochastic Gradient Descent \cite{bonnabel2013stochastic,zhang2016riemannian}, since its model parameters are hyperbolic.
We use early stopping based on validation set performance with a patience of 100 epochs.

\xhdr{Drop connection}
Since rescaling vectors in hyperbolic space requires exponential and logarithmic maps, and is conceptually not tied to the inverse dropout rate in terms of re-normalizing L1 norm, Dropout cannot be directly applied in \name.
However, as a result of using Euclidean parameters in \name, DropConnect \cite{wan2013regularization}, the generalization of Dropout, can be used as a regularization. DropConnect randomly zeros out the neural network connections, \emph{i.e.} elements of the Euclidean parameters during training time, improving the generalization of \name.



\xhdr{Projections}
Finally, we apply projections similar to Equations \ref{eq:proj_hyp} and \ref{eq:proj_tan} for the hyperboloid model $\mathbb{H}^{d,K}$ after each feature transform and $\mathrm{log}$ or $\mathrm{exp}$ map, to constrain embeddings and tangent vectors to remain on the manifold and tangent spaces.

\cut{Trainable curvature also provides a way to skip otherwise bad local minima in optimization.
In Figure \ref{fig:sweep_c} we observe that the curve of performance against curvature has fluctuations, despite theoretically equivalent local minima, suggesting that suboptimal local minima are reached for some curvatures. 
Training the curvature jointly with \name allows effective escape of local minima during optimization.
\subsection{Visualization}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/HGCN_attention_1.png}
        \label{fig:hgcn_att_1}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/GAT_attention_1.png}
        \label{fig:hgcn_att_2}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/HGCN_attention_2.png}
        \label{fig:gat_att_1}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/GAT_attention_2.png}
    \label{fig:gat_att_2}
    \end{subfigure}
    \caption{Each graph represents a 2-hop neighborhood of the \textbf{\textsc{Disease-M.}} dataset. The red node is the node where we compute attention for. The darkness of the color for other nodes denote their hierarchy. The attention weights for nodes in neighborhood with respect to the red node are visualized by the intensity of edges, and the ligher the edge is, the less attention weight. .}
    \label{fig:att_weights_appendix}
\end{figure}
} 
\bibliographystyle{plain}
\bibliography{ref}



\end{document}
