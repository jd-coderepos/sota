\documentclass{article}







\usepackage[final]{neurips_2019}

\usepackage{wrapfig}
\makeatletter
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tkz-euclide}  
\usepackage{amsmath}
\usepackage{thmtools}
\usepackage{enumitem}
\usepackage{thm-restate}
\usepackage{graphicx}

\usepackage{floatrow}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\usepackage[export]{adjustbox}

\declaretheorem[name=Theorem,numberwithin=section]{thm}
\declaretheorem[name=Corollary,numberwithin=section]{corr}
\newcommand{\yell}{\textcolor{red}}
\newcommand{\HGCN}{\name}

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{microtype}      \usepackage{xcolor}
\usepackage{url}
\usepackage{verbatim} \usepackage{graphicx}
\usepackage{caption} 
\usepackage{multirow}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{xspace}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{times}
\usepackage{xr}
\usepackage{bbm}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{hyperref}       \usepackage{cleveref}

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0.5\topsep}
\setlist[description]{noitemsep, topsep=0.5\topsep}
\setlist[itemize]{noitemsep, topsep=0.5\topsep}

\newcommand{\note}[1]{{{\textcolor{blue}{[note: #1]}}}}
\newcommand{\jure}[1]{{{\textcolor{magenta}{[JL: #1]}}}}
\definecolor{c1}{RGB}{255,127,0}
\definecolor{c2}{RGB}{127,0,127}
\newcommand{\ines}[1]{\textcolor{c1}{[ines: #1]}}
\newcommand{\rex}[1]{\textcolor{c2}{[rex: #1]}}

\newcommand{\xhdr}[1]{{\noindent\bfseries #1}.}
\newcommand{\red}[1]{{\textcolor{red}{#1}}}
\newcommand{\CITE}{{\textcolor{red}{[CITE]}}}
\newcommand{\name}{\textsc{HGCN}\xspace}
\newcommand{\gnn}{\textrm{GNN}}
\newcommand{\hide}[1]{}

\newcommand{\V}{\mathcal{V}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\R}{\mathbb{R}}



\newcommand{\nedge}{k}
\newcommand{\mb}{\mathbf}
\newcommand{\cut}[1]{}
\newcommand{\casc}{{\mathbf{t}}}
\newcommand{\alphs}{{\mathbf{A}}}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{\textbf{Observation}}
\newcommand{\xsketch}{\noindent {\it Proof Sketch.}}
\newcommand{\netinf}{{\textsc{Net\-Inf}}\xspace}
\newcommand{\multitree}{{\textsc{Multi\-Tree}}\xspace}
\newcommand{\netrate}{{\textsc{Net\-Rate}}\xspace}
\newcommand{\connie}{{\textsc{Co\-nNIe}}\xspace}
\newcommand{\maxinf}{{\textsc{Max\-Inf}}\xspace}
\newcommand{\expo}{{\textsc{Exp}}\xspace}
\newcommand{\pow}{{\textsc{Pow}}\xspace}
\newcommand{\ray}{{\textsc{Ray}}\xspace}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\unobs}{{\infty}}
\newcommand\T{\rule{0pt}{2.6ex}}
\newcommand\B{\rule[-1.2ex]{0pt}{0pt}}
\DeclareMathOperator*{\argmax}{argmax}


\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\etal}{\textit{et al}.~}


\newcommand{\ba}{Barab\'asi-Albert }
\newcommand{\er}{Erd\H{o}s-R\'enyi}
 \usepackage{cleveref}

\title{Appendix for ``Hyperbolic Graph Convolutional Neural Networks''}



\author{Anonymous Authors
}

\begin{document}

\maketitle

\appendix
\section{Review of Differential Geometry}\label{appendix:diff_geom}
We first recall some definitions of differential and hyperbolic geometry.

\subsection{Differential geometry}
\xhdr{Manifold} An dimensional \textit{manifold}  is a topological space that locally resembles the topological space  near each point. 
More concretely, for each point  on , we can find a \textit{homeomorphism} (continuous bijection with continuous inverse) between a neighbourhood of  and .
The notion of manifold is a generalization of surfaces in high dimensions.

\xhdr{Tangent space}
Intuitively, if we think of  as a dimensional manifold embedded in , the \textit{tangent space}  at point  on  is a dimensional hyperplane in  that best approximates  around . Another possible interpretation for  is that it contains all the possible directions of curves on  passing through .
The elements of  are called \textit{tangent vectors} and the union of all tangent spaces is called the \textit{tangent bundle} . 

\xhdr{Riemannian manifold} A \textit{Riemannian manifold} is a pair , where  is a smooth manifold and  is a \textit{Riemannian metric}, that is a family of smoothly varying inner products on tangent spaces, .
Riemannian metrics can be used to measure distances on manifolds. 

\xhdr{Distances and geodesics} Let  be a Riemannian manifold. For , define the norm of  by .
Suppose  is a smooth curve on . Define the length of  by: 

Now with this definition of length, every connected Riemannian manifold  becomes a metric space and the \textit{distance}  is defined as:

\textit{Geodesic} distances are a generalization of straight lines (or shortest paths) to non-Euclidean geometry. 
A curve  is \textit{geodesic} if .

\xhdr{Parallel transport} \textit{Parallel transport} is a generalization of translation to non-Euclidean geometry.
Given a smooth manifold , parallel transport  maps a vector  to . 
In Riemannian geometry, parallel transport preserves the Riemannian metric tensor (norm, inner products...).

\xhdr{Curvature}
At a high level, curvature measures how much a geometric object such as surfaces deviate from a flat plane. 
For instance, the Euclidean space has zero curvature while spheres have positive curvature. 
We illustrate the concept of curvature in Figure \ref{fig:manifold_curvature}.

\subsection{Hyperbolic geometry}
\xhdr{Hyperbolic space} The hyperbolic space in  dimensions is the unique complete, simply connected dimensional Riemannian manifold with constant negative sectional curvature.
There exist several models of hyperbolic space such as the Poincar\'e model or the hyperboloid model (also known as the Minkowski model or the Lorentz model).
In what follows, we review the Poincar\'e and the hyperboloid models of hyperbolic space as well as connections between these two models. 

\subsubsection{Poincar\'e ball model}
Let  be the Euclidean norm. 
The Poincar\'e ball model with unit radius and constant negative curvature  in  dimensions is the Riemannian manifold  where

and

where  and  is the identity matrix.  
The induced distance between two points  in  can be computed as:


\begin{figure}[t]
\begin{center}
    \includegraphics[width=\textwidth]{figs/manifold_curvature.pdf}
  \end{center}
\caption{From left to right: a surface of negative curvature, a surface of zero curvature, and a surface of positive curvature.}
  \label{fig:manifold_curvature}
\end{figure}
\subsubsection{Hyperboloid model}
\xhdr{Hyperboloid model}
Let  denote the Minkowski inner product,

The hyperboloid model with unit imaginary radius and constant negative curvature  in  dimensions is defined as the Riemannian manifold  where

and
\begin{small}

\end{small}
The induced distance between two points  in  can be computed as:



\xhdr{Geodesics} 
We recall a result that gives the unit speed geodesics in the hyperboloid model with curvature  \cite{robbin2011introduction}.
This result can be used to show Propositions \ref{cor:geodesic} and \ref{cor:logexp} for the hyperboloid manifold with negative curvature , and then learn  as a model parameter in \name. 
\begin{restatable}[]{thm}{unitspeed}
\label{thm:unit_speed}
Let  and  unit-speed (i.e. ).
The unique unit-speed geodesic  such that  and  is given by:

\end{restatable}

\xhdr{Parallel Transport}
If two points  and  on the hyperboloid  are connected by a geodesic, then the parallel transport of a tangent vector  to the tangent space  is:


\xhdr{Projections}
Finally, we recall projections to the hyperboloid manifold and its corresponding tangent spaces. 
A point  can be projected on the hyperboloid manifold  with:

Similarly, a point  can be projected on  with:

In practice, these projections are very useful for optimization purposes as they constrain embeddings and tangent vectors to remain on the manifold and tangent spaces.

\subsubsection{Connection between the Poincar\'e ball model and the hyperboloid model}
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figs/hyperboloid.png}
  \end{center}
  \caption{Illustration of the hyperboloid model (top) in 3 dimensions and its connection to the Poincar\'e disk (bottom).}
  \label{fig:hyperboloid}
\end{figure}
While the hyperboloid model tends to be more stable for optimization than the Poincar\'e model \cite{nickel2018learning}, the Poincar\'e model is very interpretable and embeddings can be directly visualized on the Poincar\'e disk. 
Fortunately, these two models are isomorphic (\emph{cf.} Figure \ref{fig:hyperboloid}) and there exist a diffeomorphism  mapping one space onto the other:


\section{Proofs of Results}
\subsection{Hyperboloid model of hyperbolic space}
For completeness, we re-derive results of hyperbolic geometry for any arbitrary curvature. 
Similar derivations can be found in the literature \cite{wilson2014spherical}.
\geodesics*
\begin{proof}
Using theorem \ref{thm:unit_speed}, we know that the unique unit-speed geodesic  in  must satisfy

and is given by

Now let  and  be unit-speed
and denote  the unique unit-speed geodesic in  such that 
. 
Let us define  and . We have,

and since  is the unique unit-speed geodesic in , we also have

Furthermore, we have ,  as  and .
Therefore  is a unit-speed geodesic in  and we get

Finally, this leads to 


\begin{comment}
We know that the unique unit-speed geodesic  in  must satisfy

Let . 
We have . 
Furthermore, since , we have  and for all :

Therefore,  is a curve on . 
Furthermore, we have  and therefore

Finally,  verifies all the conditions in Equation \ref{eq:geodesic_cond} and is therefore the unique unit-speed geodesic on  such that  and . 
\end{comment}
\end{proof}

\logexp*
\begin{proof}
We use a similar reasoning to that in Corollary 1.1 in \cite{ganea2018hyperbolic}.
Let  be the unique geodesic such that  and .
Let us define  where  is the Minkowski norm of 
and 
 satisfies,

Therefore  is a unit-speed geodesic in  and we get

By identification, this leads to 

We can use this result to derive exponential and logarthimic maps on the hyperboloid model.
We know that . 
Therefore we get,

Now let .
We have  as  
and .
Therefore  and we get

where  is well defined since .
Note that,

as .
Therefore, we finally have

\end{proof}

\subsection{Curvature}\label{appendix:curvature}


\begin{restatable}[]{lemma}{curv}
\label{thm:curv}
For any hyperbolic spaces with constant curvatures , and any pair of hyperbolic points   embedded in , there exists a mapping  to another pair of corresponding hyperbolic points in ,  such that the Minkowski inner product is scaled by a constant factor.
\end{restatable}
\begin{proof}
For any hyperbolic embedding  we have the identity:
. 
For any hyperbolic curvature , consider the mapping . Then we have the identity
 and therefore .
For any pair , , . 
The factor  only depends on curvature, but not the specific  embeddings.
\end{proof}

Lemma \ref{thm:curv} implies that given a set of embeddings learned in hyperbolic space , we can find embeddings in another hyperbolic space with different curvature, , such that the Minkowski inner products for all pairs of embeddings are scaled by the same factor .

For link prediction tasks, Theorem \ref{thm:loss} shows that with infinite precision, the expressive power of hyperbolic spaces with varying curvatures is the same.

\vspace{5pt}
\loss*
\begin{proof}
The Fermi-Dirac decoder predicts that there exists a link between node  and  \emph{iif}
, where  is the threshold for determining existence of links.
The criterion is equivalent to .

Given , the graph  reconstructed with the Fermi-Dirac decoder has the edge set .
Consider the mapping to , .
Let .
By Lemma \ref{thm:curv}, 

Due to linearity, we can find decoder parameter,  and  that satisfy 
.
With such , , the criterion 
is equivalent to .
Therefore, the reconstructed graph  based on the set of embeddings  is identical to .
\end{proof}

\cut{
\subsection{Lorentzian Centroid Appoximation}
Aggregation via the Lorentzian Centroid Approximation can be defined as
}



\section{Experimental Details}
\subsection{Dataset statistics}
We detail the dataset statistics in Table \ref{table:dataset}.


\begin{table}
    \centering
	\begin{tabular}{r c c c c} 
	\hline
	Name & Nodes & Edges & Classes & Node features  \\
	\hline
	\textsc{Cora} & 2708 & 5429 & 7 & 1433 \\
	{\textsc{Pubmed}} & 19717 & 88651 & 3 & 500 \\
	{\textsc{Human PPI}} & 17598 & 5429 & 4 & 17 \\
	{\textsc{Airport}} & 3188 & 18631 & 4 & 4 \\
	{\textsc{Disease}} & 1044 & 1043 & 2 & 1000 \\
	{\textsc{Disease-M}} & 43193 & 43102 & 2 & 1000 \\
	\hline
	\end{tabular}
    \caption{Benchmarks' statistics}
    \label{table:dataset}
\end{table}

\subsection{Training details}
Here we present details of \name's training pipeline, with optimization and incorporation of DropConnect \cite{wan2013regularization}. 


\xhdr{Parameter optimization}
Recall that linear transformations and attention are defined on the tangent space of points. 
Therefore the linear layer and attention parameters are Euclidean.
For bias, there are two options: one can either define parameters in hyperbolic space, and use hyperbolic addition operation \cite{ganea2018hyperbolicNN}, or define parameters in Euclidean space, and use Euclidean addition after transforming the points into the tangent space.
Through experiments we find that Euclidean optimization is much more stable, and gives slightly better test performance compared to Riemannian optimization, if we define parameters such as bias in hyperbolic space.
Hence different from shallow hyperbolic embeddings, although our model and embeddings are hyperbolic, the learnable graph convolution parameters can be optimized via Euclidean optimization (Adam Optimizer \cite{kingma2014adam}), thanks to exponential and logarithmic maps.
Note that to train shallow Poincar\'e embeddings, we use Riemannian Stochastic Gradient Descent \cite{bonnabel2013stochastic,zhang2016riemannian}, since its model parameters are hyperbolic.
We use early stopping based on validation set performance with a patience of 100 epochs.

\xhdr{Drop connection}
Since rescaling vectors in hyperbolic space requires exponential and logarithmic maps, and is conceptually not tied to the inverse dropout rate in terms of re-normalizing L1 norm, Dropout cannot be directly applied in \name.
However, as a result of using Euclidean parameters in \name, DropConnect \cite{wan2013regularization}, the generalization of Dropout, can be used as a regularization. DropConnect randomly zeros out the neural network connections, \emph{i.e.} elements of the Euclidean parameters during training time, improving the generalization of \name.



\xhdr{Projections}
Finally, we apply projections similar to Equations \ref{eq:proj_hyp} and \ref{eq:proj_tan} for the hyperboloid model  after each feature transform and  or  map, to constrain embeddings and tangent vectors to remain on the manifold and tangent spaces.

\cut{Trainable curvature also provides a way to skip otherwise bad local minima in optimization.
In Figure \ref{fig:sweep_c} we observe that the curve of performance against curvature has fluctuations, despite theoretically equivalent local minima, suggesting that suboptimal local minima are reached for some curvatures. 
Training the curvature jointly with \name allows effective escape of local minima during optimization.
\subsection{Visualization}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/HGCN_attention_1.png}
        \label{fig:hgcn_att_1}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/GAT_attention_1.png}
        \label{fig:hgcn_att_2}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/HGCN_attention_2.png}
        \label{fig:gat_att_1}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/GAT_attention_2.png}
    \label{fig:gat_att_2}
    \end{subfigure}
    \caption{Each graph represents a 2-hop neighborhood of the \textbf{\textsc{Disease-M.}} dataset. The red node is the node where we compute attention for. The darkness of the color for other nodes denote their hierarchy. The attention weights for nodes in neighborhood with respect to the red node are visualized by the intensity of edges, and the ligher the edge is, the less attention weight. .}
    \label{fig:att_weights_appendix}
\end{figure}
} 
\bibliographystyle{plain}
\bibliography{ref}



\end{document}
