\documentclass{article}

\PassOptionsToPackage{numbers,compress}{natbib}

\usepackage[final]{nips_2017}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage[toc,page]{appendix}


\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{makeidx}
\usepackage{sectsty}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}[theorem]{Corollary}\newtheorem{proposition}[theorem]{Proposition}\newtheorem{lemma}[theorem]{Lemma}\newtheorem{conjecture}[theorem]{Conjecture}\newtheorem*{theorem*}{Theorem}

\renewcommand\bibsection{} 

\newcommand\Ba{\bm{a}}
\newcommand\Bb{\bm{b}}
\newcommand\Bc{\bm{c}}
\newcommand\Bd{\bm{d}}
\newcommand\Be{\bm{e}}
\newcommand\Bf{\bm{f}}
\newcommand\Bg{\bm{g}}
\newcommand\Bh{\bm{h}}
\newcommand\Bi{\bm{i}}
\newcommand\Bj{\bm{j}}
\newcommand\Bk{\bm{k}}
\newcommand\Bl{\bm{l}}
\newcommand\Bm{\bm{m}}
\newcommand\Bn{\bm{n}}
\newcommand\Bo{\bm{o}}
\newcommand\Bp{\bm{p}}
\newcommand\Bq{\bm{q}}
\newcommand\Br{\bm{r}}
\newcommand\Bs{\bm{s}}
\newcommand\Bt{\bm{t}}
\newcommand\Bu{\bm{u}}
\newcommand\Bv{\bm{v}}
\newcommand\Bw{\bm{w}}
\newcommand\Bx{\bm{x}}
\newcommand\By{\bm{y}}
\newcommand\Bz{\bm{z}}
\newcommand\BA{\bm{A}}
\newcommand\BB{\bm{B}}
\newcommand\BC{\bm{C}}
\newcommand\BD{\bm{D}}
\newcommand\BE{\bm{E}}
\newcommand\BF{\bm{F}}
\newcommand\BG{\bm{G}}
\newcommand\BH{\bm{H}}
\newcommand\BI{\bm{I}}
\newcommand\BJ{\bm{J}}
\newcommand\BK{\bm{K}}
\newcommand\BL{\bm{L}}
\newcommand\BM{\bm{M}}
\newcommand\BN{\bm{N}}
\newcommand\BO{\bm{O}}
\newcommand\BP{\bm{P}}
\newcommand\BQ{\bm{Q}}
\newcommand\BR{\bm{R}}
\newcommand\BS{\bm{S}}
\newcommand\BT{\bm{T}}
\newcommand\BU{\bm{U}}
\newcommand\BV{\bm{V}}
\newcommand\BW{\bm{W}}
\newcommand\BX{\bm{X}}
\newcommand\BY{\bm{Y}}
\newcommand\BZ{\bm{Z}}
\newcommand\Bal{\bm{\alpha}}
\newcommand\Bbe{\bm{\beta}}
\newcommand\Bla{\bm{\lambda}}
\newcommand\Bep{\bm{\epsilon}}
\newcommand\Bga{\bm{\gamma}}
\newcommand\Bmu{\bm{\mu}}
\newcommand\Bnu{\bm{\nu}}
\newcommand\Brh{\bm{\rho}}
\newcommand\Bth{\bm{\theta}}
\newcommand\Bxi{\bm{\xi}}
\newcommand\Bka{\bm{\kappa}}
\newcommand\Bsi{\bm{\sigma}}
\newcommand\Bta{\bm{\tau}}
\newcommand\BLa{\bm{\Lambda}}
\newcommand\BPh{\bm{\Phi}}
\newcommand\BPs{\bm{\Psi}}
\newcommand\BSi{\bm{\Sigma}}
\newcommand\BUp{\bm{\Upsilon}}
\newcommand\BXi{\bm{\Xi}}
\newcommand\BGa{\bm{\Gamma}}
\newcommand\BTh{\bm{\Theta}}
\newcommand\BOn{\bm{1}}
\newcommand\BZe{\bm{0}}
\newcommand\BLOn{\mathlarger{\mathlarger{\bm{1}}}}
\newcommand\BLZe{\mathlarger{\mathlarger{\bm{0}}}}
\newcommand{\dA}{\mathbb{A}} \newcommand{\dB}{\mathbb{B}} 
\newcommand{\dC}{\mathbb{C}} \newcommand{\dD}{\mathbb{D}} 
\newcommand{\dE}{\mathbb{E}} \newcommand{\dF}{\mathbb{F}}
\newcommand{\dG}{\mathbb{G}} \newcommand{\dH}{\mathbb{H}}
\newcommand{\dI}{\mathbb{I}} \newcommand{\dJ}{\mathbb{J}} 
\newcommand{\dK}{\mathbb{K}} \newcommand{\dL}{\mathbb{L}}
\newcommand{\dM}{\mathbb{M}} \newcommand{\dN}{\mathbb{N}}
\newcommand{\dO}{\mathbb{O}} \newcommand{\dP}{\mathbb{P}} 
\newcommand{\dQ}{\mathbb{Q}} \newcommand{\dR}{\mathbb{R}}
\newcommand{\dS}{\mathbb{S}} \newcommand{\dT}{\mathbb{T}} 
\newcommand{\dU}{\mathbb{U}} \newcommand{\dV}{\mathbb{V}} 
\newcommand{\dW}{\mathbb{W}} \newcommand{\dX}{\mathbb{X}}
\newcommand{\dY}{\mathbb{Y}} \newcommand{\dZ}{\mathbb{Z}}

\newcommand{\rA}{\mathrm{A}} \newcommand{\rB}{\mathrm{B}} 
\newcommand{\rC}{\mathrm{C}} \newcommand{\rD}{\mathrm{D}} 
\newcommand{\rE}{\mathrm{E}} \newcommand{\rF}{\mathrm{F}}
\newcommand{\rG}{\mathrm{G}} \newcommand{\rH}{\mathrm{H}}
\newcommand{\rI}{\mathrm{I}} \newcommand{\rJ}{\mathrm{J}} 
\newcommand{\rK}{\mathrm{K}} \newcommand{\rL}{\mathrm{L}}
\newcommand{\rM}{\mathrm{M}} \newcommand{\rN}{\mathrm{N}}
\newcommand{\rO}{\mathrm{O}} \newcommand{\rP}{\mathrm{P}} 
\newcommand{\rQ}{\mathrm{Q}} \newcommand{\rR}{\mathrm{R}}
\newcommand{\rS}{\mathrm{S}} \newcommand{\rT}{\mathrm{T}}
\newcommand{\rU}{\mathrm{U}} \newcommand{\rV}{\mathrm{V}} 
\newcommand{\rW}{\mathrm{W}} \newcommand{\rX}{\mathrm{X}}
\newcommand{\rY}{\mathrm{Y}} \newcommand{\rZ}{\mathrm{Z}}

\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand{\bA}{\mathbf{A}} \newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}} \newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}} \newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}} \newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}} \newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}} \newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}} \newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}} \newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}} \newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}} \newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}} \newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}} \newcommand{\bZ}{\mathbf{Z}}

\newcommand\xs{x}
\newcommand\zs{z}
\newcommand\xsp{\left(x_{\phi}\right)}
\newcommand\xp{\bm{x}_{\phi}}
\newcommand\zp{\bm{z}_{\omega}}
\newcommand\zps{\left(z_{\omega}\right)}
\newcommand\sign{\mbox{sign}}
\newcommand\EXP{\mathbf{\mathrm{E}}}
\newcommand\PR{\mathbf{\mathrm{Pr}}}
\newcommand\VAR{\mathbf{\mathrm{Var}}}
\newcommand\COV{\mathbf{\mathrm{Cov}}}
\newcommand\TR{\mathbf{\mathrm{Tr}}}
\newcommand\ALs{{\mbox{\boldmath }}}
\newcommand\XIs{{\mbox{\boldmath }}}
\newcommand\sgn{\mathop{\mathrm{sgn}\,}}
\newcommand\oo{\mathrm{old}}
\newcommand\nn{\mathrm{new}}
\newcommand\rank{\mathrm{rank}}

\newcommand\munn{{\tilde \mu}}
\newcommand\nunn{{\tilde \nu}}
\newcommand\xinn{{\tilde \xi}}





\newcommand{\p}[4]{{#3}\!\left#1{#4}\right#2} 

\newcommand{\ABS}[1]{{{\left| #1 \right|}}} \newcommand{\BRA}[1]{{{\left\{#1\right\}}}} \newcommand{\NRM}[1]{{{\left\| #1\right\|}}} \newcommand{\PAR}[1]{{{\left(#1\right)}}} \newcommand{\SBRA}[1]{{{\left[#1\right]}}} 


\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

\newcommand{\R}{\textsf{R~}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\texttt{#1}}}



\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\err}{Err}
\DeclareMathOperator{\selu}{selu}

\allowdisplaybreaks

\makeindex


\title{Self-Normalizing Neural Networks}



\author{
 G\"{u}nter Klambauer
 \And 
 Thomas Unterthiner
 \And
 Andreas Mayr
 \And 
 Sepp Hochreiter \\ 
 LIT AI Lab \& Institute of Bioinformatics, \\
 Johannes Kepler University Linz\\
 A-4040 Linz, Austria\\
\texttt{\{klambauer,unterthiner,mayr,hochreit\}@bioinf.jku.at}
}

\begin{document}
\renewcommand\indexname{Brief index}
\maketitle

\begin{abstract}
Deep Learning has revolutionized vision via convolutional neural networks (CNNs) 
and natural language processing via recurrent neural networks (RNNs).
However, success stories of Deep Learning with 
standard feed-forward neural networks (FNNs) are rare. 
FNNs that perform well are typically shallow 
and, therefore cannot exploit many levels of abstract representations. 
We introduce self-normalizing neural networks (SNNs) to
enable high-level abstract representations.
While batch normalization requires explicit normalization, 
neuron activations of SNNs 
automatically converge towards zero mean and unit variance. 
The activation function of SNNs are ``scaled exponential linear units''
(SELUs), which induce self-normalizing properties.
Using the Banach fixed-point theorem, 
we prove that activations close to zero mean and unit variance 
that are propagated through many network layers will converge 
towards zero mean and unit variance ---
even under the presence of noise and perturbations.
This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. 
Furthermore, for activations not close to unit  
variance, we prove an upper and lower bound 
on the variance, thus, vanishing and exploding gradients are impossible.
We compared SNNs on (a) 121 tasks from the UCI machine learning repository, 
on (b) drug discovery benchmarks, and on (c) astronomy tasks
with standard FNNs, and other machine learning methods such as random forests and support vector machines.
For FNNs we considered  
(i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization,
(v) highway networks, and (vi) residual networks. 
SNNs significantly outperformed all competing FNN methods at
121 UCI tasks, outperformed all competing methods 
at the Tox21 dataset, and set a new record at an astronomy data set. 
The winning SNN architectures are often very deep. Implementations are available at: \href{https://www.github.com/bioinf-jku/SNNs}{github.com/bioinf-jku/SNNs}.
\end{abstract}

Accepted for publication at NIPS 2017; please cite as: \\
{\tt Klambauer, G., Unterthiner, T., Mayr, A., \& Hochreiter, S. (2017). Self-Normalizing Neural Networks. In Advances in Neural Information Processing Systems (NIPS).}

\section*{Introduction}
\label{sec:introduction}
Deep Learning has set new records at different benchmarks and
led to various commercial applications \citep{bib:Lecun2015,bib:Schmidhuber2015}. 
Recurrent neural networks (RNNs) \citep{bib:Hochreiter1997} 
achieved new levels at speech and natural language processing, 
for example at the TIMIT benchmark \citep{bib:Graves2013} or at 
language translation \citep{bib:Sutskever2014}, and 
are already employed in mobile devices \citep{bib:Sak2015}. 
RNNs have won handwriting recognition challenges (Chinese and Arabic
handwriting) \cite{bib:Schmidhuber2015, bib:Graves2009,bib:Cirecsan2015}
and Kaggle challenges,
such as the ``Grasp-and Lift EEG'' competition. 
Their counterparts, convolutional neural networks (CNNs) \citep{bib:Lecun1995} excel
at vision and video tasks. 
CNNs are on par with human dermatologists at the 
visual detection of skin cancer \citep{bib:Esteva2017}. 
The visual processing for self-driving cars is based on CNNs \citep{bib:Huval2015},
as is the visual input to AlphaGo which has beaten
one of the best human GO players \citep{bib:Silver2016}.
At vision challenges, CNNs are constantly winning, for example at 
the large ImageNet competition \citep{bib:Krizhevsky2012, bib:He2015res}, but 
also almost all Kaggle vision challenges, such as  the ``Diabetic Retinopathy'' and 
the ``Right Whale'' challenges \citep{bib:Dugan2016,bib:Gulshan2016}. 

However, looking at Kaggle challenges that are not related to vision or sequential
tasks, gradient boosting, random forests, or support vector machines (SVMs) are winning most of the competitions. 
Deep Learning is notably absent, and for the few cases where FNNs won, 
they are shallow. For example, the HIGGS challenge,
the Merck Molecular Activity challenge, and the 
Tox21 Data challenge were all won by FNNs with at most four hidden layers.
Surprisingly, it is hard to find success stories with FNNs that
have many hidden layers, though they would allow for different levels
of abstract representations 
of the input \citep{bib:Bengio2013b}.


To robustly train very deep CNNs, batch normalization evolved into a standard to normalize
neuron activations to zero mean and unit variance \citep{bib:Ioffe2015}. 
Layer normalization \citep{bib:Ba2016} also ensures zero mean and unit
variance, while weight normalization \citep{bib:Salimans2016} ensures
zero mean and unit variance if in the previous layer the activations have
zero mean and unit variance.
However, training with normalization techniques is perturbed by
stochastic gradient descent (SGD), stochastic
regularization (like dropout), 
and the estimation of the normalization parameters.
Both RNNs and CNNs can stabilize learning via weight sharing, 
therefore they are less prone to these perturbations. 
In contrast, FNNs trained with normalization techniques suffer from
these perturbations and have high variance 
in the training error (see Figure~\ref{fig:perturb}). 
This high variance hinders learning and slows it down. 
Furthermore, strong regularization, such as dropout, 
is not possible as it would further 
increase the variance which in turn would lead to divergence of  the
learning process.
We believe that this sensitivity to perturbations 
is the reason that FNNs are less
successful than RNNs and CNNs.

Self-normalizing neural networks (SNNs) are robust to perturbations
and do not have high variance in their training errors (see Figure~\ref{fig:perturb}).
SNNs push neuron activations to zero mean and unit variance 
thereby leading to the same effect as batch normalization,
which enables to robustly learn many layers. 
SNNs are based on scaled exponential linear units ``SELUs''
which induce self-normalizing properties like variance stabilization
which in turn avoids exploding and vanishing gradients.


\section*{Self-normalizing Neural Networks (SNNs)}
\paragraph{Normalization and SNNs.}
\index{definitions}
For a neural network with activation function , 
we consider two consecutive layers 
that are connected by a weight matrix .
Since the input to a neural
network is a random variable, 
the activations  in the lower
layer, the network inputs , and the 
activations  in the higher layer are
random variables as well. 
We assume that all activations  of the lower layer
have mean 
 and variance . 
An activation  in the
higher layer has mean
 and variance
. 
Here  denotes the expectation and
 the variance of a random variable.
A single activation  has net input 
. 
For  units with activation 
 in the lower layer, we define   
times the mean of the 
weight vector  as  and 
times the second moment as .

We consider the mapping  that maps mean and variance of the activations from one layer
to mean and variance of the activations in the next layer \index{mapping }



Normalization techniques like batch, layer, or weight normalization 
ensure a mapping  that keeps 
 and 
close to predefined values, typically . 
\begin{definition}[Self-normalizing neural net] \index{self-normalizing neural networks}
\label{def:SNN}
A neural network is self-normalizing if it possesses a mapping 
 for each activation  that maps mean and variance from one layer to the next 
and has a stable and attracting fixed point depending on  in .
Furthermore, the mean and the variance remain in the domain , that is , where
.
When iteratively applying the mapping , each point within  converges to this fixed point.
\end{definition}
Therefore, we consider activations of a neural network to be normalized, 
if both their mean and their variance across samples are within predefined intervals. 
If mean and variance of  are already within these intervals, then also
mean and variance of  remain in these intervals, i.e., the
normalization is transitive across layers. Within these intervals, 
the mean and variance both converge to a fixed point if the mapping  is applied
iteratively. 


\setlength{\belowcaptionskip}{0pt}
\begin{figure}
 \includegraphics[width=0.49\columnwidth]{figures/mnist.pdf}
 \includegraphics[width=0.49\columnwidth]{figures/cifar10.pdf}
 \caption[FNN and SNN trainin error curves]{The left panel and the right panel show the training error (y-axis) for feed-forward neural networks (FNNs) with batch
   normalization (BatchNorm) and self-normalizing networks (SNN) across update steps (x-axis)
   on the MNIST dataset the CIFAR10 dataset, respectively.  
   We tested networks with 8, 16, and 32 layers and learning rate 1e-5. FNNs 
   with batch normalization exhibit high variance due to perturbations. 
   In contrast, SNNs do not suffer from high variance as they are
   more robust to perturbations and learn faster.  \label{fig:perturb}}
\end{figure}


Therefore, SNNs
keep normalization of activations when propagating them 
through layers of the network. 
The normalization effect is observed across layers of a network: 
in each layer the activations are getting closer to the fixed point.
The normalization effect can also observed be for two fixed layers across learning steps: perturbations of lower layer activations or
weights are damped in the higher layer by drawing the activations
towards the fixed point.
If for all  in the higher layer, 
and  of the corresponding weight vector are the same, then
the fixed points are also the same. In this case we have a
unique fixed point for all activations . 
Otherwise, in the more general case, 
and  differ for different  but the mean activations are
drawn into  and the variances
are drawn into .


\paragraph{Constructing Self-Normalizing Neural Networks.}
We aim at constructing self-normalizing
neural networks by adjusting the properties of the function .
Only two design choices are available for the
function : 
(1) the activation function and 
(2) the initialization of the weights.

For the activation function,
we propose ``scaled exponential linear units'' (SELUs) to render a FNN
as self-normalizing. The SELU activation function is given by \index{SELU!definition}

SELUs allow to construct a mapping  with properties that lead to SNNs.
SNNs cannot be derived with (scaled)
rectified linear units (ReLUs), sigmoid units,  units, and leaky
ReLUs.
The activation function is required to have 
(1) negative and positive values for controlling the mean,
(2) saturation regions (derivatives approaching zero) to dampen the variance if
it is too large in the lower layer, 
(3) a slope larger than one to increase the variance if
it is too small in the lower layer,
(4) a continuous curve. 
The latter ensures a fixed point, where variance damping is equalized by variance increasing.
We met these properties of the activation function by multiplying the exponential linear
unit (ELU) \citep{bib:Clevert2015} with  to ensure a slope larger than one
for positive net inputs. 

For the weight initialization, we 
propose  and  for all units in the higher layer.
The next paragraphs will show the advantages of this initialization. 
Of course, during learning these assumptions on the weight vector will
be violated. However, we can prove the self-normalizing property
even for weight vectors that are not normalized, therefore, the 
self-normalizing property can be kept during learning and weight changes.



\paragraph{Deriving the Mean and Variance Mapping Function .}
We assume that the  are independent from each other but share
the same mean  and variance . 
Of course, the independence assumptions is 
not fulfilled in general. We will elaborate on the independence
assumption below.  
The network input  in the higher layer 
is  for which we can infer the following moments 

and 
,
where we used the independence of the .
The net input  is a weighted sum of independent, 
but not necessarily identically distributed variables ,
for which the central limit theorem (CLT) states that  approaches a normal distribution:
 
with density . 
According to the CLT, the larger , the closer is  to a normal distribution.
For Deep Learning, broad layers with hundreds of neurons  are common. 
Therefore the assumption that  is normally distributed is met well for most currently used
neural networks (see Figure~\ref{fig:clt}).
The function  maps the mean and variance of activations in the lower layer to the mean
 and variance  of the activations  in the next layer:

These integrals can be analytically computed and lead to following \index{mapping }
mappings of the moments: 



\paragraph{Stable and Attracting Fixed Point  for Normalized Weights.}
\label{sec:perfect}

We assume a normalized weight
vector  with  and . 
Given a fixed point ,
we can solve equations Eq.~\eqref{eq:mappingMean} and Eq.~\eqref{eq:mappingVar} for   and
. 
We chose the fixed point ,
which is typical for activation normalization.
We obtain the fixed point equations  and  that we solve  for   and  and obtain the solutions \index{SELU!parameters}  and ,
where the subscript  indicates that these are the parameters for fixed point .
The analytical expressions for  and  are given in Eq.~\eqref{eq:alphalambda}.  We are interested whether the fixed point  is stable
and attracting. If the Jacobian of  has a norm smaller than 1 at the
fixed point, then  is a contraction mapping and the fixed point is stable.
The (2x2)-Jacobian  of  evaluated at the fixed point  with  and 
 is 


The spectral norm of  (its largest
singular value) is . That means  is a contraction
mapping around the fixed point  (the mapping is depicted in Figure~\ref{fig:arrows}).
Therefore,  is a stable fixed point of
the mapping .

\paragraph{Stable and Attracting Fixed Points for Unnormalized Weights.}
A normalized weight vector  cannot be ensured during learning.
For SELU parameters 
 and ,
we show in the next theorem that 
if  is close to , then  still has an
attracting and stable fixed point
that is close to .
Thus, in the general case there still exists a stable fixed point
which, however, depends on .
If we restrict  to certain intervals, then we 
can show that  is mapped to the respective intervals.
Next we present the central theorem of this paper,
from which follows that SELU
networks are self-normalizing under mild conditions on the weights.
\begin{theorem}[Stable and Attracting Fixed Points]
\label{lem:fixedPoint} \index{Theorem 1}
We assume  and .
We restrict the range of the variables to the following intervals
,
,
, and
, 
that define the functions' domain .
For  and , the mapping  Eq.~\eqref{eq:mappingG}
 has the stable
fixed point , whereas for other  and  the mapping  Eq.~\eqref{eq:mappingG}
 has a stable and
attracting fixed point depending on  in the 
-domain:  and 
.
All points within the -domain converge when
iteratively applying the mapping  Eq.~\eqref{eq:mappingG} to this fixed point.
\end{theorem}


\begin{figure}
 \includegraphics[width=\columnwidth]{figures/Figure2_arrows_new-crop.pdf}
 \caption[Visualization of the mapping ]{For  and , 
  the mapping  of mean  (-axis) and variance  (-axis)
  to the next layer's mean  and variance  
  is depicted.
  Arrows show in which direction  is mapped by .
  The fixed point of the mapping  is .
 \label{fig:arrows}}
\end{figure}

\begin{proof} \index{Theorem 1!proof sketch}
We provide a proof sketch (see detailed proof in Appendix~Section~\ref{sec:proofs}).
With the Banach fixed point theorem we show that there exists
a unique attracting and stable fixed point. 
To this end, we have to prove that  a)  is a contraction mapping and b) that 
the mapping stays in the domain, that is, . 
The spectral norm of the Jacobian of  can be obtained via 
an explicit formula for the largest singular value for a  matrix. 
 is a contraction mapping if its spectral norm is smaller than .
We perform a computer-assisted proof to 
evaluate the largest singular value on a fine
grid and ensure the precision of the computer 
evaluation by an error propagation analysis of the implemented
algorithms on the according hardware.
Singular values between grid points are upper bounded by the 
mean value theorem. To this end, we bound the derivatives
of the formula for the largest singular value with respect to
. 
Then we apply the mean value theorem to pairs of points, where one is on the grid and the
other is off the grid. This shows that for all values of
 in  the domain , the spectral norm of
 is smaller than one. 
Therefore,  is a contraction mapping on the domain .
Finally, we show that the mapping  stays in the domain  by
deriving bounds on  and .
Hence, the Banach fixed-point theorem holds and there exists a unique
fixed point in  that is attained.
\end{proof}

Consequently, feed-forward neural networks with many units in each layer 
and with the SELU activation function are self-normalizing (see definition~\ref{def:SNN}), which 
readily follows from Theorem~\ref{lem:fixedPoint}.
To give an intuition, the main property of SELUs is that they damp the variance for negative
net inputs and increase the variance for positive net inputs. 
The variance damping is stronger if net inputs are further away from zero while
the variance increase is stronger if net inputs are close to zero.
Thus, for large variance of the activations in the lower
layer the damping effect is dominant and the variance decreases in the
higher layer.
Vice versa, for small variance the 
variance increase is dominant and the variance increases in the higher layer.

However, we cannot guarantee that mean and variance remain in the domain .
Therefore, we next treat the case where  are outside .
It is especially crucial to consider  because this variable has much stronger 
influence than . Mapping  across layers to a high value corresponds to an 
exploding gradient, since the  Jacobian of the activation of high layers with respect to activations
in lower layers has large singular values. 
Analogously, mapping  across layers to a low value corresponds to an 
vanishing gradient. Bounding the mapping of  from above and below would avoid 
both exploding and vanishing gradients.
Theorem~\ref{th:varDecrease} states that the variance of neuron activations of SNNs
is bounded from above, and therefore ensures that SNNs learn robustly and do not  
suffer from exploding gradients.

\begin{theorem}[Decreasing ]
\label{th:varDecrease} \index{Theorem 2}
For , 
and the domain :
, 
,
, and 
, 
we have for the mapping of the variance
  given in Eq.~\eqref{eq:mappingVar}:
.
\end{theorem}
The proof can be found in the Appendix~Section~\ref{sec:proofs}.
Thus, when mapped across many layers, the variance in the interval  is mapped to a value below . Consequently, all fixed
points  of the mapping  (Eq.~\eqref{eq:mappingG}) have .
Analogously, Theorem~\ref{th:s2Increase} states that the variance of neuron activations of SNNs
is bounded from below, and therefore ensures that SNNs do not suffer from vanishing gradients.
\begin{theorem}[Increasing ]
\label{th:s2Increase} \index{Theorem 3}
We consider , 
and the domain : 
, and
.
For the domain 

and  as well as for the domain

and ,
the mapping of the variance
  given in Eq.~\eqref{eq:mappingVar} 
increases:
.
\end{theorem}
The proof can be found in the Appendix~Section~\ref{sec:proofs}.
All fixed
points  of the mapping  (Eq.~\eqref{eq:mappingG}) ensure for  that
 
and for  that .
Consequently, the variance mapping Eq.~\eqref{eq:mappingVar} ensures a lower bound on the variance . 
Therefore SELU networks control the variance of the activations and
push it into an interval, whereafter the mean and variance move toward
the fixed point. 
Thus, SELU networks are steadily normalizing the variance and
subsequently normalizing the mean, too. 
In all experiments, we observed that 
self-normalizing neural networks push the mean and variance of activations into the domain  .


\paragraph{Initialization.}
\label{sec:init} \index{initialization}
Since SNNs have a fixed point at zero mean and unit variance 
for normalized weights  and
 (see above), 
we initialize SNNs such that these
constraints are fulfilled in expectation.
We draw the weights from a Gaussian distribution 
with  and variance .
Uniform and truncated Gaussian distributions with these moments 
led to networks with similar behavior. 
The ``MSRA initialization''  is similar since 
it uses zero mean and variance  to initialize the weights \citep{bib:He2015init}.
The additional factor  counters the effect of rectified 
linear units.

\paragraph{New Dropout Technique.}
\label{sec:dropout} \index{dropout}
Standard dropout randomly sets an activation  to zero with probability  for . 
In order to preserve the mean, the activations are scaled by  during training. 
If  has mean  and variance
, and the dropout variable  follows
a binomial distribution , then the mean  is kept.
Dropout fits well to rectified linear units, since
zero is in the low variance region and corresponds
to the default value.
For scaled exponential linear units, the default and low variance
value is . 
Therefore, we propose ``alpha dropout'', 
that randomly sets inputs to . 
The new mean and new variance is
, and 
.
We aim at keeping mean and variance to their original values after ``alpha
dropout'', in order to ensure the self-normalizing property even for ``alpha dropout''.
The affine transformation  allows to
determine parameters  and  such that mean and variance are kept to their values: 

In contrast to dropout,  and  will depend on  and ,
however our SNNs converge to activations with
zero mean and unit variance.  
With  and , we obtain  and .
The parameters  and  only depend on the dropout rate  
and the most negative activation . 
Empirically, we found that dropout rates  or  lead to models with good performance.
``Alpha-dropout'' fits well to scaled exponential linear units by randomly setting 
activations to the negative saturation value.

\paragraph{Applicability of the central limit theorem and independence assumption.}
\label{sec:clt} \index{central limit theorem}
In the derivative of the mapping (Eq.~\eqref{eq:mappingG}), we used the central limit theorem (CLT) 
to approximate the network inputs  with a normal distribution.
We justified normality because network inputs represent a weighted sum of the inputs , where for Deep Learning  is typically large.
The Berry-Esseen theorem states that the convergence rate to normality is  \citep{bib:Korolev2012}. 
In the classical version of the CLT, the random variables have to be independent and identically 
distributed, which typically does not hold for neural networks.
However, the Lyapunov CLT does not require the variable to be identically distributed anymore. Furthermore,
even under weak dependence, sums of random variables converge in distribution to a Gaussian distribution \cite{bib:Bradley1981}.



\section*{Experiments}
\index{experiments}
We compare SNNs to other deep networks at different
benchmarks. 
Hyperparameters such as
number of layers (blocks), neurons per layer, learning rate, and dropout rate,
are adjusted by grid-search for each dataset on a separate validation set
(see Section~\ref{sec:experiments}). 
We compare the following FNN methods: \index{experiments!methods compared}
\begin{itemize}
\item {\bf ``MSRAinit'':} FNNs without normalization and 
with ReLU activations and ``Microsoft weight initialization'' \citep{bib:He2015init}.
\item {\bf ``BatchNorm'':} FNNs with batch normalization \citep{bib:Ioffe2015}. 
\item {\bf ``LayerNorm'':} FNNs with layer normalization \citep{bib:Ba2016}. 
\item {\bf ``WeightNorm'':} FNNs with weight normalization \citep{bib:Salimans2016}. 
\item {\bf ``Highway'':} Highway networks \citep{bib:Srivastava2015}.
\item {\bf ``ResNet'':} Residual networks \citep{bib:He2015res} adapted to FNNs  
using residual blocks with 2 or 3 layers with rectangular or diavolo shape. 
\item {\bf ``SNNs'':} Self normalizing networks with SELUs with  and  and 
the proposed dropout technique and initialization strategy. 
\end{itemize}






\paragraph{121 UCI Machine Learning Repository datasets.} \index{experiments!UCI}
The benchmark comprises 121 classification datasets from the UCI Machine Learning repository \cite{bib:Fernandez2014} 
from diverse application areas, such as physics, geology, or biology.
The size of the datasets ranges between  and  data points and the 
number of features from  to . 
In abovementioned work \citep{bib:Fernandez2014},
there were methodological mistakes \citep{bib:Wainberg2016} which we avoided here. 
Each compared FNN method
was optimized with respect to its architecture and hyperparameters on a validation set that was then 
removed from the subsequent analysis. 
The selected hyperparameters served to evaluate the methods in terms of accuracy on 
the pre-defined test sets (details on the hyperparameter selection are given in Section~\ref{sec:experiments}).
The accuracies are reported in the Table~\ref{tab:UCIfull}. 
We ranked the methods by their accuracy for each
prediction task and compared their average ranks.
SNNs significantly outperform all competing networks in pairwise comparisons (paired
Wilcoxon test across datasets) as reported in Table~\ref{tab:uci} (left panel).



\begin{table}[htp]
\caption[Comparison of seven FNNs on 121 UCI tasks]{{\bf Left:} Comparison of seven FNNs on 121 UCI tasks. 
We consider the average rank difference to rank , which is
the average rank of seven methods with random predictions. 
The first column gives the method, the second 
the average rank difference, and the last the -value 
of a paired Wilcoxon test whether the difference to the best performing 
method is significant.
SNNs significantly outperform all other methods.
{\bf Right:} Comparison of 24 machine learning methods (ML) on the UCI datasets
with more than 1000 data points. 
The first column gives the method, the second 
the average rank difference to rank , and the last the -value 
of a paired Wilcoxon test whether the difference to the best performing 
method is significant. Methods that were significantly worse than
the best method are marked with ``*''.
The full tables can be found in Table~\ref{tab:UCIfull}, Table~\ref{tab:uciS1} and Table~\ref{tab:uciS2}.
SNNs outperform all competing methods. 
\label{tab:uci} \label{tab:uci2}}
\centering
\begin{tabular}{lcclcc}
  \toprule
\multicolumn{3}{c}{FNN method comparison}   &  \multicolumn{3}{c}{ML method comparison} \\
 Method      & avg. rank diff. & -value  & Method  &  avg. rank diff. & -value \\ 
    \midrule
SNN         & -0.756 &  &  SNN &  -6.7  &  \\ 
MSRAinit    & -0.240* &    { 2.7e-02}  & SVM  &  -6.4  &  5.8e-01 \\ 
LayerNorm   & -0.198*  &    { 1.5e-02} &  RandomForest &  -5.9  &  2.1e-01 \\ 
Highway     & 0.021*  &    { 1.9e-03} &  MSRAinit &  -5.4* &  { 4.5e-03} \\ 
ResNet      & 0.273* &    { 5.4e-04} &  LayerNorm &  -5.3  &  7.1e-02 \\ 
WeightNorm  & 0.397* &    { 7.8e-07} &  Highway &  -4.6* &  { 1.7e-03} \\ 
BatchNorm   & 0.504* &    { 3.5e-06} &   &    &   \\ 
\bottomrule
\end{tabular}
\end{table}

We further included 17 machine learning methods representing diverse method groups \citep{bib:Fernandez2014} 
in the comparison and 
the grouped the data sets into ``small'' and ``large'' data sets (for details see Section~\ref{sec:experiments}).
On 75 small datasets with less than 1000 data points, random forests and SVMs outperform SNNs and other FNNs. 
On 46 larger datasets with at least 1000 data points, 
SNNs show the highest performance followed by SVMs and random forests (see right panel of Table~\ref{tab:uci2},
for complete results see Tables~\ref{tab:uciS1}~and~\ref{tab:uciS1}).
Overall, SNNs have outperformed state of the art machine learning methods on UCI datasets
with more than 1,000 data points.




Typically, hyperparameter selection chose SNN architectures that were
much deeper than the selected architectures of other FNNs, with an average depth of 10.8 layers, 
compared to average depths of 6.0 for BatchNorm, 3.8 WeightNorm, 7.0 LayerNorm, 5.9 Highway,  
and 7.1 for MSRAinit networks. For ResNet, the average number of blocks was 6.35. 
SNNs with many more than 4 layers often provide the best predictive accuracies across all neural networks. 
























\paragraph{Drug discovery: The Tox21 challenge dataset.} \index{experiments!Tox21}
The Tox21 challenge dataset comprises about 12,000 chemical compounds
whose twelve toxic effects have to be predicted based on their chemical structure. 
We used the validation sets 
of the challenge winners for hyperparameter selection (see Section~\ref{sec:experiments}) and 
the challenge test set for performance comparison. 
We repeated the whole evaluation procedure 5 times 
to obtain error bars. 
The results in terms of average AUC are given in Table~\ref{tab:tox21}.
In 2015, the challenge organized by the US NIH 
was won by an ensemble of shallow ReLU FNNs which achieved an AUC of 0.846 \citep{bib:Mayr2016}.
Besides FNNs, this ensemble also contained random forests and SVMs.
Single SNNs came close with an AUC of 0.8450.003.
The best performing SNNs have 8 layers, compared to the runner-ups ReLU networks with layer normalization with 2 and 3 layers. 
Also batchnorm and weightnorm networks, typically perform best with shallow
networks of 2 to 4 layers (Table~\ref{tab:tox21}). The deeper the networks, the 
larger the difference in performance between SNNs and other methods (see columns 5--8 of Table~\ref{tab:tox21}).
The best performing method is an SNN with 8 layers.

\index{experiments!Tox21!hyperparameters}
\begin{table}[ht]
\caption[Comparison of FNNs at the Tox21 challenge dataset]{Comparison of FNNs at the Tox21 challenge dataset
in terms of AUC. The rows  represent different methods and the columns 
different network depth and for ResNets  the number of residual blocks 
(``na'': 32 blocks were omitted due to computational constraints).
The deeper the networks, the more prominent is the advantage of SNNs.
The best networks are SNNs with 8 layers. 
 \label{tab:tox21}}
\centering
\begin{tabular}{lccccccc}
  \toprule
  \multicolumn{8}{c}{\#layers / \#blocks} \\
  method & 2 & 3 & 4 & 6 & 8 & 16 & 32 \\ 
  \midrule 
  SNN        & {\em 83.7} \tiny  0.3 & {\bf 84.4} \tiny  0.5 & {\bf 84.2} \tiny  0.4 & {\bf 83.9} \tiny  0.5 & {\bf 84.5} \tiny  0.2 & {\bf 83.5} \tiny  0.5 & {\bf 82.5} \tiny  0.7 \\ 
  Batchnorm  & 80.0 \tiny  0.5       & 79.8 \tiny  1.6       & 77.2 \tiny  1.1       & 77.0 \tiny  1.7       & 75.0 \tiny  0.9       & 73.7 \tiny  2.0       & 76.0 \tiny  1.1 \\ 
  WeightNorm & {\em 83.7} \tiny  0.8 & 82.9 \tiny  0.8       & 82.2 \tiny  0.9       & {\em 82.5} \tiny  0.6 & {\em 81.9} \tiny  1.2 & 78.1 \tiny  1.3       & 56.6 \tiny  2.6 \\ 
  LayerNorm  & {\bf 84.3} \tiny  0.3 & {\em 84.3} \tiny  0.5 & {\em 84.0} \tiny  0.2 & {\em 82.5} \tiny  0.8 & 80.9 \tiny  1.8       & 78.7 \tiny  2.3       & 78.8 \tiny  0.8 \\ 
  Highway    & 83.3 \tiny  0.9       & 83.0 \tiny  0.5       & 82.6 \tiny  0.9       & 82.4 \tiny  0.8       & 80.3 \tiny  1.4       & 80.3 \tiny  2.4       & 79.6 \tiny  0.8 \\ 
  MSRAinit     & 82.7 \tiny  0.4       & 81.6 \tiny  0.9       & 81.1 \tiny  1.7       & 80.6 \tiny  0.6       & 80.9 \tiny  1.1       & 80.2 \tiny  1.1       & 80.4 \tiny  1.9 \\ 
  ResNet    & 82.2 \tiny  1.1       & 80.0 \tiny  2.0       & 80.5 \tiny  1.2       &   81.2 \tiny   0.7                       & 81.8 \tiny  0.6       & {\em 81.2} \tiny  0.6 &   na \\ 
   \bottomrule
\end{tabular}
\end{table}


\paragraph{Astronomy: Prediction of pulsars in the HTRU2 dataset.} \index{experiments!HTRU2} \index{experiments!astronomy}

Since a decade, machine learning methods have been used to identify pulsars in radio wave signals \citep{bib:Lyon2016a}.
Recently, the High Time Resolution Universe Survey (HTRU2) dataset has been released with
1,639 real pulsars and 16,259 spurious signals.
Currently, the highest AUC value of a 10-fold cross-validation is 0.976
which has been achieved by Naive Bayes classifiers followed by decision tree C4.5 with 0.949 and SVMs with 0.929.
We used eight features constructed by the PulsarFeatureLab as used previously \citep{bib:Lyon2016a}.
We assessed the performance of FNNs using 10-fold nested cross-validation,
where the hyperparameters were selected in the inner loop on a validation set (for details on the hyperparameter selection see Section~\ref{sec:experiments}). 
Table~\ref{tab:HTRU2} reports the results
in terms of AUC. SNNs outperform all other methods and have pushed the state-of-the-art 
to an AUC of . 


\begin{table}[ht]
\caption[Comparison of FNNs and reference methods at HTRU2]{Comparison of FNNs and reference methods at HTRU2 
in terms of AUC. 
The first, fourth and seventh column give the method, 
the second, fifth and eight column the AUC averaged over 10 cross-validation folds, 
and the third and sixth column the -value of a paired Wilcoxon test of the AUCs against 
the best performing method across the 10 folds. 
FNNs achieve better results than Naive Bayes (NB), C4.5, and SVM.
SNNs exhibit the best performance and set a new record. \label{tab:HTRU2}}
\centering
\begin{tabular}{lcclccllc}
  \toprule
  \multicolumn{3}{c}{FNN methods}   &  \multicolumn{3}{c}{FNN methods} & \multicolumn{2}{c}{ref. methods}  \\
   method &  AUC & -value & method &  AUC & -value & method &  AUC  \\ 
  \midrule
  SNN           &    0.9803 \tiny   0.010 &    & & & & &                     \\ 
  MSRAinit      & 0.9791  \tiny   0.010 & \tiny3.5e-01       &  LayerNorm  & 0.9762* \tiny   0.011 & \tiny { 1.4e-02}     & NB &0.976      \\ 
  WeightNorm    & 0.9786*         \tiny   0.010 & \tiny{ 2.4e-02} &  BatchNorm  & 0.9760 \tiny   0.013 & \tiny 6.5e-02                & C4.5 & 0.946  \\ 
  Highway       & 0.9766*         \tiny   0.009 & \tiny{ 9.8e-03} & ResNet & 0.9753* \tiny  0.010 & \tiny 6.8e-03       & SVM  & 0.929\\ 
   \bottomrule
\end{tabular}
\end{table}




\section*{Conclusion}
We have introduced self-normalizing neural networks for 
which we have proved that neuron activations are pushed towards zero mean and unit variance
when propagated through the network. 
Additionally, for activations not close to unit
variance, we have proved an upper and lower bound 
on the variance mapping. Consequently, SNNs do not face vanishing and exploding gradient 
problems.  Therefore, SNNs work well for architectures with many layers, allowed us to introduce a 
novel regularization scheme, and learn very robustly.
On 121 UCI benchmark datasets, SNNs have outperformed other FNNs with and without normalization techniques, 
such as batch, layer, and weight normalization, or specialized architectures, such as Highway or 
Residual networks. 
SNNs also yielded the best results on drug discovery and astronomy tasks.
The best performing SNN architectures are typically very deep in contrast to other FNNs. 





\section*{Acknowledgments}
This work was supported by IWT research grant IWT150865 (Exaptation), H2020 project
grant 671555 (ExCAPE), grant IWT135122 (ChemBioBridge), 
Zalando SE with Research Agreement 01/2016, 
Audi.JKU Deep Learning Center, Audi Electronic Venture GmbH, 
and the NVIDIA Corporation.


\section*{References}
The references are provided in Section~\ref{sec:references}.



\section*{Appendix}
\renewcommand{\thesection}{A\arabic{section}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}

\sectionfont{\large}
\subsectionfont{\normalsize}
\subsubsectionfont{\normalsize}
\paragraphfont{\normalsize}







\setcounter{theorem}{0}
\tableofcontents




\vspace{1cm}
This appendix is organized as follows: the first section 
sets the background, definitions, and formulations.
The main theorems are presented in the next section.
The following section is devoted to the proofs of these theorems.
The next section reports additional results and details on the
performed computational experiments, such as hyperparameter selection. 
The last section shows that our theoretical bounds can be
confirmed by numerical methods as a sanity check.

The proof of theorem 1 is based on the Banach's fixed point theorem 
for which we require (1) a contraction mapping, which is proved in Subsection~\ref{sec:S}
and (2) that the mapping stays within its domain, which is proved in Subsection~\ref{sec:maptoregion}
For part (1), the proof relies on the main Lemma~12, which is a computer-assisted proof, and can be found 
in Subsection~\ref{sec:S}. The validity of the computer-assisted proof is shown in Subsection~\ref{sec:error} by 
error analysis and the precision of the functions' implementation. 
The last Subsection~\ref{sec:smallLemmata} compiles various lemmata with intermediate results that support 
the proofs of the main lemmata and theorems.

\section{Background}
\label{sec:fixedpointanalysis}
We consider a neural network with {\bf activation function}  and 
two consecutive layers that are connected by {\bf weight matrix} .
Since samples that serve as input to the neural network are chosen according to a distribution, 
the  {\bf activations  in the lower layer}, 
the {\bf network inputs} , and {\bf activations  in the
higher layer} are all random variables. We assume that all units  in the lower layer
have {\bf mean activation}  and {\bf variance of the
activation}
 and a unit  in the
higher layer has mean activation  and variance
. Here  denotes the expectation and
 the variance of a random variable.
For activation of unit , we have net input   and 
the {\bf scaled exponential linear unit (SELU)}
activation , with 

For  units  in the lower layer and 
the {\bf weight vector} , we define 
{\bf  times the mean} by  
and {\bf  times the second moment} by .

We define a {\bf mapping } from mean  and
variance  of one layer 
to the mean  and  variance   in the next layer:

For neural networks with scaled exponential linear units, 
the mean is of the activations in the next layer computed according to

and the second moment of the activations in the next layer is computed according to


Therefore, the expressions  and  have the following form: \index{mapping !definition}



We solve equations Eq.~\ref{eq:mappingMean} and
Eq.~\ref{eq:mappingVar} for fixed points  and . 
For a normalized weight vector with  and  and the
{\bf fixed point },
 we can solve equations Eq.~\ref{eq:mappingMean} and 
Eq.~\ref{eq:mappingVar} for  and .
We denote the solutions to fixed point 
by   and .

The parameters  and  ensure \index{SELU!parameters}




Since we focus on the fixed point  ,
we assume throughout the analysis that  and .
We consider the functions , 
, 
and  
on the {\bf domain} 
.

Figure~\ref{fig:arrows} visualizes 
the mapping  for  and  and 
 and  at few pre-selected points.
It can be seen that  is an attracting 
fixed point of the mapping .






\section{Theorems}
\subsection{Theorem 1: Stable and Attracting Fixed Points Close to (0,1)}

\index{Theorem 1}
Theorem~\ref{lem:fixedPoint}
shows that the mapping  defined by Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} 
exhibits a stable and attracting fixed point close to zero mean and
unit variance. 
Theorem~\ref{lem:fixedPoint} establishes the self-normalizing property of self-normalizing
neural networks (SNNs). The stable and
attracting fixed point leads to robust learning through many layers.

\begin{theorem}[Stable and Attracting Fixed Points]
We assume  and .
We restrict the range of the variables to the domain \index{domain!Theorem 1}
,
,
, and
.
For  and , the mapping  Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} has the stable
fixed point .
For other  and  the mapping  Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar}  has a stable and
attracting fixed point depending on  in the 
-domain:  and 
.
All points within the -domain converge when
iteratively applying the mapping  Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} to this fixed point.
\end{theorem}

\subsection{Theorem 2: Decreasing Variance from Above}
The next Theorem~\ref{th:s2Decrease} states \index{Theorem 2}
that the variance of unit activations 
does not explode through
consecutive layers of self-normalizing networks.
Even more, a large variance of unit activations decreases when
propagated through the network. 
In particular this ensures that exploding gradients will never be
observed.
In contrast to the domain in previous subsection, 
in which , we now consider a domain
in which the variance of the inputs is higher  and even the 
range of the mean is increased . We denote this new domain with 
the symbol  to indicate that the variance lies above the variance of the original domain .
In , we can show that the variance  in the next layer is always smaller 
then the original variance .
Concretely, this theorem states that:

\begin{theorem}[Decreasing ]
\label{th:s2Decrease}
For ,  
and the domain : \index{domain!Theorem 2}
, 
,
, and 
 we have for 
the mapping of the variance
  given in Eq.~\eqref{eq:mappingVar}

The variance decreases in  and all fixed
points  of mapping Eq.~\eqref{eq:mappingVar} and Eq.~\eqref{eq:mappingMean} have .
\end{theorem}



\subsection{Theorem 3: Increasing Variance from Below}
The next Theorem~\ref{th:s2Increase} states \index{Theorem 3}
that the variance of unit activations 
does not vanish through
consecutive layers of self-normalizing networks.
Even more, a small variance of unit activations increases when
propagated through the network. 
In particular this ensures that vanishing gradients will never be
observed.
In contrast to the first domain, 
in which , we now consider two domains  and
 in which the variance of the inputs is lower  and ,
and even the parameter  is different  to the original . 
We denote this new domain with 
the symbol  to indicate that the variance lies below the variance of the original domain .
In  and , 
we can show that the variance  in the next layer is always larger 
then the original variance , which means that the variance does not vanish through
consecutive layers of self-normalizing networks.
Concretely, this theorem states that: \index{Theorem 3}

\begin{theorem}[Increasing ]
We consider , 
and the two domains 
 
 and
. \index{domain!Theorem 3}

The mapping of the variance
  given in Eq.~\eqref{eq:mappingVar} increases

in both  and .
All fixed
points  of mapping Eq.~\eqref{eq:mappingVar} and
Eq.~\eqref{eq:mappingMean} ensure for  that
 
and for  that .
Consequently, the variance mapping Eq.~\eqref{eq:mappingVar} and
Eq.~\eqref{eq:mappingMean} ensures a lower bound on the variance . 
\end{theorem}



\section{Proofs of the Theorems}
\label{sec:proofs}

\subsection{Proof of Theorem 1} \index{Theorem 1!proof}

We have to show that the mapping  defined by Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} 
has a stable and attracting fixed point close to .
To proof this statement and Theorem~\ref{lem:fixedPoint}, 
we apply the Banach fixed point theorem which 
requires (1) that  is a contraction mapping and (2) 
that  does not map outside the function's 
domain, concretely: 

\begin{theorem}[Banach Fixed Point Theorem]
\label{lem:Banach} \index{Banach Fixed Point Theorem}
Let  be a non-empty complete metric space with a 
contraction mapping . Then  has 
a unique fixed-point  with . 
Every sequence 
with starting element  converges to the fixed point:
.
\end{theorem}

Contraction mappings are functions that map two points such that their distance is decreasing:
\begin{definition}[Contraction mapping]
 A function  on a metric space  with distance  is a contraction mapping, if there
 is a , such that for all points  and  in :
 .
\end{definition}


To show that  is a contraction mapping in  with distance , we use the Mean Value 
Theorem for 


in which  is an upper bound on the spectral norm the Jacobian  of .
The spectral norm is given by the largest singular value of the Jacobian of .
If the largest singular value of the Jacobian is smaller than 1, 
the mapping   of the mean and variance to the mean and variance in the next layer is contracting.
We show that the largest singular value is smaller than 1 by
evaluating the function for the singular value
 on a grid.
Then we use the Mean Value Theorem to bound the deviation of the
function  between grid points. 
To this end, we have to bound the gradient of  with respect to
. If all function values plus
gradient times the deltas (differences between grid points and evaluated
points) is still smaller than 1, then we have proofed that the
function is below 1 (Lemma~\ref{lem:sBound}). To show that the mapping does not map outside the function's domain, we 
derive bounds on the expressions for the mean and the variance (Lemma~\ref{lem:region}).
Section~\ref{sec:S} and Section~\ref{sec:maptoregion} are concerned with the contraction mapping and 
the image of the function domain of , respectively.

With the results that the largest singular value of the Jacobian is smaller than 
one (Lemma~\ref{lem:sBound}) and that the mapping stays in the domain 
(Lemma~\ref{lem:region}), we can prove Theorem~\ref{lem:fixedPoint}.
We first recall Theorem~\ref{lem:fixedPoint}:


\begin{theorem*}[Stable and Attracting Fixed Points]
We assume  and .
We restrict the range of the variables to the domain
,
,
, and
.
For  and , the mapping  Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} has the stable
fixed point .
For other  and  the mapping  Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar}  has a stable and
attracting fixed point depending on  in the 
-domain:  and 
.
All points within the -domain converge when
iteratively applying the mapping  Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} to this fixed point.
\end{theorem*}


\begin{proof} \index{Theorem 1!proof}
According to Lemma~\ref{lem:sBound} the mapping  (Eq.~\eqref{eq:mappingMean} and Eq.~\eqref{eq:mappingVar})
is a contraction mapping in the given
domain, that is, it has a Lipschitz constant smaller than one.
We showed that  is a fixed point of the
mapping for . 

The domain is compact (bounded and closed), therefore it is a  
complete metric space.
We further have to make sure the  mapping  does not map outside its domain .
According to Lemma~\ref{lem:region}, the mapping maps into the domain  and 
. 

Now we can apply the Banach fixed point theorem 
given in Theorem~\ref{lem:Banach} from which the statement of the 
theorem follows.
\end{proof}

\subsection{Proof of Theorem 2} \index{Theorem 2!proof}

First we recall Theorem~\ref{th:s2Decrease}:
\begin{theorem*}[Decreasing ]
For ,  
and the domain :
, 
,
, and 
 we have for 
the mapping of the variance
  given in Eq.~\eqref{eq:mappingVar}

The variance decreases in  and all fixed
points  of mapping Eq.~\eqref{eq:mappingVar} and Eq.~\eqref{eq:mappingMean} have .
\end{theorem*}




\begin{proof}
We start to consider an even larger domain
, 
,
, and 
.
We prove facts for this domain and later restrict to
, i.e. .
We consider the function  of the difference between the second moment  in the next layer
and the variance  in the lower layer:

If we can show that  for 
all , then
we would obtain our desired result . 
The derivative with respect to  is according to Theorem~\ref{th:s2Cont}:

Therefore  is strictly monotonically decreasing in .
Since  is a function in 
(these variables only appear as this product), we
have for 

and 

Therefore we have according to Theorem~\ref{th:s2Cont}:

Therefore

Consequently,  is strictly monotonically increasing in .
Now we consider the derivative with respect to  and . We start with , 
which is


We consider the sub-function

We set  and  and obtain


The derivative to this sub-function with respect to  is


The inequality follows from Lemma~\ref{lem:xeErfc}, which states that 
 is monotonically increasing in .
Therefore the sub-function is increasing in . The derivative to this sub-function with respect to  is


The sub-function is increasing in , since the
derivative is larger than zero:




We explain this chain of inequalities:
\begin{itemize}
\item First inequality: We applied Lemma~\ref{lem:Abramowitz} two times.

\item Equalities factor out  and reformulate.

\item Second inequality part 1: we applied

\item Second inequality part 2: we show that for  following holds:
. 
We have  and
 . 
Therefore the minimum is at border for minimal  and maximal :

Thus

for .

\item Equalities only solve square root and factor out the resulting
  terms  and .

\item We set  and multiplied out. Thereafter we
  also factored out  in the numerator. Finally a quadratic
  equations was solved. 
\end{itemize}

The sub-function has its minimal value for 
minimal
 and  minimal 
. 
We further minimize the function



We compute the minimum of the term in brackets of 
in Eq.~\eqref{eq:J21New}:  

Therefore the term in brackets  of Eq.~\eqref{eq:J21New}
is larger than zero.
Thus, 
has the sign of .
Since  is a function in 
(these variables only appear as this product), we
have for 

and 



Since  has the sign of , 
  has the sign of .
Therefore

has the sign of .


We now divide the -domain into
 and .
Analogously we divide the -domain into
 and .
In this domains  is strictly monotonically.

For all domains
 is strictly monotonically decreasing in 
and strictly monotonically increasing in .
Note that we now consider the range  .
For the maximal value of  we set   (we set it to 3!)
and .

We consider now all combination of these domains:
\begin{itemize}
\item  and :

 is decreasing in   and decreasing in .
We set   and  .


\item  and :

 is increasing in  and decreasing in .
We set   and  .




\item  and :

 is decreasing in  and increasing in .
We set   and  .


\item  and :

 is increasing in  and increasing in .
We set   and  .


Therefore the maximal value of  is  .
\end{itemize}


\end{proof}



\subsection{Proof of Theorem 3}\index{Theorem 3!proof}

First we recall Theorem~\ref{th:s2Increase}:
\begin{theorem*}[Increasing ]
We consider , 
and the two domains 
 
 and
 .

The mapping of the variance
  given in Eq.~\eqref{eq:mappingVar} increases

in both  and .
All fixed
points  of mapping Eq.~\eqref{eq:mappingVar} and
Eq.~\eqref{eq:mappingMean} ensure for  that
 
and for  that .
Consequently, the variance mapping Eq.~\eqref{eq:mappingVar} and
Eq.~\eqref{eq:mappingMean} ensures a lower bound on the variance . 
\end{theorem*}


\begin{proof}
The mean value theorem states that there exists a   for which

Therefore


Therefore we are interested to bound the derivative of the -mapping
Eq.~\eqref{eq:mappingSecondMom} with respect to  :



The sub-term Eq.~\eqref{eq:subx1} enters the derivative
Eq.~\eqref{eq:fo1} with a negative sign!
According to Lemma~\ref{th:s2monotone},
the minimal value of sub-term Eq.~\eqref{eq:subx1}
is obtained by the largest largest  , 
by the smallest , and the largest .
Also the positive term
 is multiplied by , which is minimized
by using the smallest .
Therefore we can use the smallest   in whole formula
Eq.~\eqref{eq:fo1} to lower bound it. 


First we consider the domain 
  and .
The factor consisting of the exponential in front of the brackets has
its smallest value for .
Since  is monotonically decreasing we inserted the
smallest argument via  in order to obtain the maximal negative contribution.
Thus, applying Lemma~\ref{th:s2monotone}, we obtain the lower bound on the derivative:


For applying the mean value theorem, we require the smallest .
We follow the proof of Lemma~\ref{lem:mapDerivatives}, which shows
that at the minimum  must be maximal 
and  must be minimal.
Thus, the smallest 

is 

for  and .



Therefore the mean value theorem and the bound on  (Lemma~\ref{lem:musquared}) provide



Next we consider the domain 
 
and .
The factor consisting of the exponential in front of the brackets has
its smallest value for . 
Since  is monotonically decreasing we inserted the
smallest argument via  in order to obtain the maximal negative contribution.

Thus, applying Lemma~\ref{th:s2monotone}, we obtain the lower bound on the derivative:


For applying the mean value theorem, we require the smallest .
We follow the proof of Lemma~\ref{lem:mapDerivatives}, which shows
that at the minimum  must be maximal 
and  must be minimal.
Thus, the smallest 

is 

for  and .
Therefore the mean value theorem and the bound on  (Lemma~\ref{lem:musquared}) gives


\end{proof}

\subsection{Lemmata and Other Tools Required for the Proofs} \index{lemmata}

\subsubsection{Lemmata for proofing Theorem 1 (part 1): Jacobian norm smaller than one} \index{lemmata!Jacobian bound}
\label{sec:S}
In this section, we show that the largest singular value of the Jacobian of the 
mapping  is smaller than one. Therefore,  is a contraction mapping. 
This is even true in a larger domain than the original . We 
do not need to restrict , but we can extend to
. The range of the other variables is unchanged such that 
we consider the following domain throughout this section: ,
,
, and
. \index{domain!singular value}

\paragraph{Jacobian of the mapping.}\index{Jacobian}
In the following, we denote two Jacobians: \index{Jacobian!definition}
(1) the Jacobian  of the mapping  , and
(2) the Jacobian  of the mapping 
because the influence of  on  is small, 
and many properties of the system can already be seen on . 





The definition of the entries of the Jacobian  is: \index{Jacobian!entries}


\paragraph{Proof sketch: Bounding the largest singular value of the Jacobian.}\index{Jacobian}

If the largest singular value of the Jacobian is smaller than 1, then
the spectral norm of the Jacobian is smaller than 1.
Then the mapping  Eq.~\eqref{eq:mappingMean} 
and Eq.~\eqref{eq:mappingVar} 
of the mean and variance to the mean and variance in the next layer is contracting.



We show that the largest singular value is smaller than 1 by
evaluating the function
 on a grid.
Then we use the Mean Value Theorem to bound the deviation of the
function  between grid points. 
Toward this end we have to bound the gradient of  with respect to
. If all function values plus
gradient times the deltas (differences between grid points and evaluated
points) is still smaller than 1, then we have proofed that the
function is below 1. 

The singular values of the  matrix

are

We used an explicit formula for the singular values \citep{Blinn:96}. We now set 

to obtain a formula for the largest singular value of the Jacobian
depending on .
The formula for the largest singular value for the Jacobian is: \index{Jacobian!singular value}



where  are defined in Eq.~\eqref{eq:JacobianEntries} and we left out the dependencies on 
 in order to keep the notation uncluttered, e.g. we 
wrote  instead of .









\paragraph{Bounds on the derivatives of the Jacobian entries.}\index{bounds!derivatives of Jacobian entries}\index{Jacobian!derivatives}

In order to bound the gradient of the singular value, we have to bound
the derivatives of the Jacobian entries 
,
,
, and

with respect to 
, , , and . The values 
 and  are fixed to  and .
The 16 derivatives of the 4 Jacobian entries with respect to the 4
variables are:



\begin{lemma}[Bounds on the Derivatives]
\label{lem:Bounds}
The following bounds on the absolute values of the 
derivatives of the Jacobian entries ,
,
, and

with respect to 
, , , and  hold:




\end{lemma}

\begin{proof}
 See proof~\ref{proof:Bounds}.
\end{proof}


\paragraph{Bounds on the entries of the Jacobian.}\index{bounds!Jacobian entries}\index{Jacobian!entries}\index{Jacobian!bounds}

\begin{lemma}[Bound on J11]
\label{lem:J11}
The absolute value of the function \\ 
 is bounded by 
 in the domain , , , 
and  for  and .
\end{lemma}

\begin{proof}

where we used that (a)  is strictly monotonically increasing in  and 
and (b) Lemma~\ref{lem:mainsubfunctionJ11J12} that 

\end{proof}


\begin{lemma}[Bound on J12]
\label{lem:J12}
The absolute value of the function \\ 
 is bounded by 
 in the domain , , , 
and  for  and .
\end{lemma}

\begin{proof}



For the first term we have   after 
Lemma~\ref{lem:mainsubfunctionJ11J12} and for the second term , which can easily be seen
by maximizing or minimizing the arguments of the exponential or the square root function. The first term scaled by  is

and the second term scaled by  is
.
Therefore, the absolute difference between these terms is at most 
leading to the derived bound.



\end{proof}




\paragraph{Bounds on mean, variance and second moment.}\index{bounds!mean and variance}
For deriving bounds on , , and , we need 
the following lemma.

\begin{lemma}[Derivatives of the Mapping]
\label{lem:mapDerivatives}
We assume  and .
We restrict the range of the variables to the domain
,
,
, and
.

The derivative 
has the sign of .

The derivative 
is positive.

The derivative 
has the sign of .

The derivative 

is positive.
\end{lemma}

\begin{proof}
 See \ref{proof:mapDerivatives}.
\end{proof}


\begin{lemma}[Bounds on mean, variance and second moment]
\label{lem:boundsmeanvar}
The expressions , , and 
for
 and 
are bounded by
,

and
 
in the domain , 
, ,  .
\end{lemma}


\begin{proof}
We use Lemma~\ref{lem:mapDerivatives} which states that with given
sign the derivatives of the mapping   Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} with respect to  
and  are either positive or have the sign of
.
Therefore with given sign of  the mappings are strict monotonic and
the their maxima and minima are found at the borders.  The minimum of  is obtained at
 and its maximum at  and  and  at minimal or maximal values, respectively.
It follows that


Similarly, the maximum and minimum of  is obtained at the values mentioned above:


Hence we obtain the following bounds on :



\end{proof}





\paragraph{Upper Bounds on the Largest Singular Value of the Jacobian.}\index{bounds!singular value}\index{Jacobian!singular value bound}

\begin{lemma}[Upper Bounds on Absolute Derivatives of Largest Singular Value]
\label{lem:Ds1Bounds}
We set
 and  and
restrict the range of the variables to
,
,
, and
.

The absolute values of derivatives of the largest singular value

given in Eq.~\eqref{eq:S} with respect to
 are bounded as follows:









\end{lemma}
\begin{proof}

The Jacobian of our mapping  Eq.~\eqref{eq:mappingMean} and 
Eq.~\eqref{eq:mappingVar} is defined as


and has the largest singular value

according to the formula of \citet{Blinn:96}.

We obtain

and analogously 

and

and


We have

from which follows using the bounds from Lemma~\ref{lem:Bounds}:

Derivative of the singular value w.r.t. :

where we used the results from the lemmata \ref{lem:Bounds}, \ref{lem:J11}, \ref{lem:J12}, and \ref{lem:boundsmeanvar}.

Derivative of the singular value w.r.t. :

where we used the results from the lemmata \ref{lem:Bounds}, \ref{lem:J11}, \ref{lem:J12}, and \ref{lem:boundsmeanvar} and that
 is symmetric for .

Derivative of the singular value w.r.t. :

where we used the results from the lemmata \ref{lem:Bounds}, \ref{lem:J11}, \ref{lem:J12}, and \ref{lem:boundsmeanvar}.


Derivative of the singular value w.r.t. :

where we used the results from the lemmata \ref{lem:Bounds}, \ref{lem:J11}, \ref{lem:J12}, and \ref{lem:boundsmeanvar} and that
 is symmetric for  .

\end{proof}





\begin{lemma}[Mean Value Theorem Bound on Deviation from Largest Singular Value]\index{bounds!singular value}
\label{lem:meanValue}
We set
 and  and
restrict the range of the variables to
,
,
, and
.

The distance of the singular value at

and that at 

is bounded as follows:

\end{lemma}
\begin{proof}


The mean value theorem states that a  exists for
which

from which immediately follows that

We now apply Lemma~\ref{lem:Ds1Bounds} which gives bounds on the
derivatives, which immediately gives the statement of the lemma.
\end{proof}

\begin{lemma}[Largest Singular Value Smaller Than One]
\label{lem:sBound}
We set
 and  and
restrict the range of the variables to
,
,
, and
.

The the largest singular value of the Jacobian is smaller than 1:

Therefore the mapping  Eq.~\eqref{eq:mappingMean} 
and Eq.~\eqref{eq:mappingVar} is a contraction mapping.
\end{lemma}
\begin{proof}
We set 
,
,
, and
.





According to Lemma~\ref{lem:meanValue} we have



For a grid with grid length
,
,
, and
,
we evaluated the function  Eq.~\eqref{eq:S} 
for the largest singular value
in the domain
,
,
, and
.
We did this using a computer.
According to Subsection~\ref{sec:error}
the precision if regarding error propagation
and precision of the implemented functions is larger than
.
We performed the evaluation on different operating systems and
different hardware architectures including CPUs and GPUs.
In all cases the function  Eq.~\eqref{eq:S} for the largest singular
value of the Jacobian is bounded by .

We obtain from Eq.~\eqref{eq:bb1}:

\end{proof}


\subsubsection{Lemmata for proofing Theorem 1 (part 2): Mapping within domain}\index{mapping in domain}
\label{sec:maptoregion}

We further have to investigate whether the the mapping  Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} maps into a predefined domains.

\begin{lemma}[Mapping into the domain]
\label{lem:region}
The mapping   Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} map for 
  and 
into the domain
 and
 with  and .
\end{lemma}


\begin{proof}
We use Lemma~\ref{lem:mapDerivatives} which states that with given
sign the derivatives of the mapping Eq.~\eqref{eq:mappingMean}
and Eq.~\eqref{eq:mappingVar} with respect to  
and  are either positive or have the sign of
.
Therefore with given sign of  the mappings are strict monotonic and
the their maxima and minima are found at the borders.  The minimum of  is obtained at
 and its maximum at  and  and  at their 
minimal and maximal values, respectively. It follows that:

and that .

Similarly, the maximum and minimum of  is obtained at the values mentioned above:

Since , we can conclude that 
 and the variance remains in .
\end{proof}


\begin{corollary}
 The image  of the mapping  (Eq.~\eqref{eq:mapping})
 and the domain  is 
 a subset of : 
 
 for all  and .
\end{corollary}

\begin{proof}
 Directly follows from Lemma~\ref{lem:region}.
\end{proof}









\subsubsection{Lemmata for proofing Theorem 2: The variance is contracting}\index{contracting variance}




\paragraph{Main Sub-Function.}
We consider the main sub-function of the derivate of second moment,  (Eq.~\eqref{eq:JacobianEntries}):


that depends on  and , therefore we 
set   and . Algebraic reformulations provide the 
formula in the following form:



For  and 
,
we consider the domain
, 
,
, and,
.

For  and  we obtain:  and 
.
In the following we assume to remain within this domain.

\begin{lemma}[Main subfunction]
\label{lem:subfunction}

For  and , 

the function

is smaller than zero, is strictly monotonically increasing in ,
and strictly monotonically decreasing in  for the minimal .
\end{lemma}

\begin{proof}
See proof~\ref{proof:mainsubfunction}.
\end{proof}

The graph of the subfunction in the specified domain is displayed in Figure~\ref{fig:subfunction}.

\begin{figure}
 \centering
 \includegraphics[width=0.48\textwidth]{figures/secondMomentMainSubfunctionPlot.pdf}
 \includegraphics[width=0.48\textwidth]{figures/lemma27.pdf}
 \caption[Graph of the main subfunction of the derivative of the second moment]{{\bf Left panel:} Graphs of the main subfunction 
  treated in Lemma~\ref{lem:subfunction}. The function is negative and monotonically increasing with  independent of . 
   {\bf Right panel:}  Graphs of the main subfunction at minimal . The graph shows that the function  is strictly monotonically decreasing in .
  \label{fig:subfunction}}
\end{figure}







\begin{theorem}[Contraction -mapping]
\label{th:s2Cont}
The mapping of the variance   given in Eq.~\eqref{eq:mappingVar}
is contracting for 
, 
and the domain : 
, 
,
, and 
, that is,

\end{theorem}


\begin{proof}
In this domain  we have the following three properties (see further below):
, ,
and . Therefore, we have




\begin{itemize}
\item We first proof that  in an even larger domain that fully contains .
According to  Eq.~\eqref{eq:JacobianEntries},
the derivative of the mapping  Eq.~\eqref{eq:mappingVar} 
with respect to the variance  is


For 
, , 
, 
 
, and 
, we first show that the derivative is positive
and then upper bound it.

According to  Lemma~\ref{lem:subfunction}, the expression

is negative. This expression multiplied by positive factors is
subtracted in the derivative Eq.~\eqref{eq:s2Ds2}, therefore, the
whole term is positive.
The remaining term

of the derivative Eq.~\eqref{eq:s2Ds2}
is also positive according to Lemma~\ref{lem:basics}.
All factors outside the brackets in  Eq.~\eqref{eq:s2Ds2} are
positive. Hence, the derivative Eq.~\eqref{eq:s2Ds2} is positive.


The upper bound of the derivative is:

We explain the chain of inequalities:
\begin{itemize}
\item First equality brings the expression
into a shape where we can apply  Lemma~\ref{lem:subfunction} for the
the function Eq.~\eqref{eq:subfunction}.
\item First inequality: The overall factor  is bounded by 1.25.
\item Second inequality: We apply Lemma~\ref{lem:subfunction}.
 According to Lemma~\ref{lem:subfunction} the function Eq.~\eqref{eq:subfunction} is negative.
The largest contribution is to subtract the most negative value of 
the function Eq.~\eqref{eq:subfunction}, that is, the minimum of 
function Eq.~\eqref{eq:subfunction}.
According to Lemma~\ref{lem:subfunction} the function
Eq.~\eqref{eq:subfunction} is strictly monotonically increasing in 
and strictly monotonically decreasing in  for .
Therefore the function Eq.~\eqref{eq:subfunction} has its minimum 
at minimal  
and maximal . We insert these values into
the expression.

\item Third inequality: We use for the whole expression 
the maximal factor 
 by setting this
factor to 1.
\item Fourth inequality:  is strictly monotonically
  decreasing. Therefore we maximize its argument to obtain the least
  value which is subtracted. We use the minimal  and the maximal .
\item Sixth inequality: evaluation of the terms.
\end{itemize}

\item We now show that . The expression
 (Eq.~\eqref{eq:mappingMean})
is strictly monotonically increasing im  and . Therefore,
the minimal value in  is obtained at 
. 

\item Last we show that .
The expression
 (Eq.~\eqref{eq:JacobianEntries})
can we reformulated as follows:


is larger than zero when the term 
is larger than zero. This term obtains its minimal value 
at  and , which can easily be shown using the 
Abramowitz bounds (Lemma~\ref{lem:Abramowitz})
and evaluates to , therefore  in .

\end{itemize}
\end{proof}







\subsubsection{Lemmata for proofing Theorem 3: The variance is expanding}\index{expanding variance}




\paragraph{Main Sub-Function From Below.}

We consider functions in
 and , therefore we 
set   and .

For  and 
,
we consider the domain
, 
 
, and 
.

For  and  we obtain:  and 
.
In the following we assume to be within this domain.

In this domain, we consider the main sub-function of the derivate of second moment in the next layer,  (Eq.~\eqref{eq:JacobianEntries}):


that depends on  and , therefore we 
set   and . Algebraic reformulations provide the 
formula in the following form:




\begin{lemma}[Main subfunction Below]
\label{lem:subfunction1}

For  and , 
the function

smaller than zero, is strictly monotonically increasing in 
and strictly monotonically increasing in  for the minimal ,
, , and  (lower
bound of  on ).
\end{lemma}

\begin{proof}
See proof~\ref{proof:mainsubfunctionbelow}.
\end{proof}



\begin{lemma}[Monotone Derivative]
\label{th:s2monotone}
For , 
and the domain 
, 
,
, and 
.
We are interested of the derivative of


The derivative of the equation above with
respect to
\begin{itemize}
\item  is larger than zero;
\item  is smaller than zero for maximal
, , and  (with
);
\item  is larger than zero for , , , and .
\end{itemize}

\end{lemma}

\begin{proof}
See proof~\ref{proof:monotonederivative}.
\end{proof}













\subsubsection{Computer-assisted proof details for main Lemma 12 in Section A3.4.1.}\index{computer-assisted proof}
\label{sec:error}

\paragraph{Error Analysis.} We investigate the error propagation for the 
singular value (Eq.~\eqref{eq:S}) if the function arguments 
suffer from numerical imprecisions up to . To this end, we first 
derive error propagation rules based on the mean value theorem and then 
we apply these rules to the formula for the singular value.

\begin{lemma}[Mean value theorem]
\label{th:mvt}
For a real-valued function  which is differentiable in the closed interval ,
there exists  with 

\end{lemma}
It follows that 
for computation with error , there exists a  with 

Therefore the increase of the norm of the error after applying
function  is bounded by the norm of the gradient
.


We now compute for the functions, that we consider their gradient and
its 2-norm:  

\begin{itemize}
\item addition:

 and , which gives
.

We further know that


Adding  terms gives:



\item subtraction:

 and , which gives
.

We further know that


Subtracting  terms gives:


\item multiplication:

 and , which gives
.

We further know that


Multiplying  terms gives:



\item division:

 and , which gives
.

We further know that


\item square root:

 and , which gives
.

\item exponential function:

 and , which gives
.

\item error function:

 and , which gives
.

\item complementary error function:

 and , which gives
.
\end{itemize}



\begin{lemma}
If the values  have a precision of , 
the singular value (Eq.~\eqref{eq:S}) evaluated with the formulas 
given in Eq.~\eqref{eq:JacobianEntries} and Eq.~\eqref{eq:S} has 
a precision better than .  
\end{lemma}

This means for a machine with a typical precision of , we have the rounding error , the evaluation 
of the singular value (Eq.~\eqref{eq:S}) with the formulas given in Eq.~\eqref{eq:JacobianEntries} and Eq.~\eqref{eq:S} has 
a precision better than .

\begin{proof}
We have the numerical precision  of the parameters , that we denote by
 together with our domain .

With the error propagation rules that we derived in Subsection~\ref{sec:error}, we 
can obtain bounds for the numerical errors on the following simple expressions:


Using these bounds on the simple expressions, we can now calculate bounds on the numerical errors of compound expressions:



Subsequently, we can use the above results to get bounds for the numerical errors on the Jacobian entries (Eq.~\eqref{eq:JacobianEntries}), 
applying the rules from Subsection~\ref{sec:error} again:


and we obtain ,  , 
and . 
We also have bounds on the absolute values on  and  (see Lemma~\ref{lem:J11},
Lemma~\ref{lem:J12}, and Lemma~\ref{lem:boundsmeanvar}), therefore we can 
propagate the error also through the function that calculates the singular value (Eq.~\eqref{eq:S}). 


\end{proof}




\paragraph{Precision of Implementations.}
We will show that our computations are correct up to 3 ulps. For
our implementation in GNU C library and the hardware architectures
that we used, the precision of all mathematical functions that we used
is at least one ulp.
The term ``ulp'' (acronym for ``unit in the last place'') was coined
by W. Kahan in 1960. It is the highest precision (up to some factor
smaller 1), which can be
achieved for the given hardware and floating point representation.

Kahan defined ulp as \citep{Kahan:04}:
\begin{quote}
``Ulp is the gap between the two {\em finite} floating-point numbers
nearest , even if  is one of them. (But ulp(NaN) is NaN.)''
\end{quote}
Harrison defined ulp as \citep{Harrison:99}:
\begin{quote}
``an ulp in  is the distance
between the two closest {\em straddling} floating point numbers  and , i.e.\ those with
 and  assuming an unbounded exponent range.''
\end{quote}
In the literature we find also slightly different definitions
\citep{Muller:05}.



According to \citep{Muller:05} who refers to \citep{Goldberg:91}:
\begin{quote}
``IEEE-754 mandates four standard rounding modes:''

``Round-to-nearest:  is the floating-point value closest to  with the
usual distance; if two floating-point value are equally close to , then 
is the one whose least significant bit is equal to zero.''

``IEEE-754 standardises 5 operations: addition (which we shall note  in order to
distinguish it from the operation over the reals), subtraction (), multiplication
(), division (), and also square root.''

``IEEE-754 specifies {em exact rounding} [Goldberg, 1991, \S1.5]: the result of a
floating-point operation is the same as if the operation were performed on the
real numbers with the given inputs, then rounded according to the rules in the
preceding section. Thus,  is defined as , with  and  taken as
elements of ; the same applies for the other operators.''
\end{quote}
Consequently, the IEEE-754 standard guarantees that addition,
subtraction, multiplication, division, and squared root is precise up
to one ulp.

We have to consider transcendental functions. First the is the
exponential function, and then the complementary error
function ,
which can be computed via the error function .

Intel states \citep{Muller:05}:
\begin{quote}
``With the Intel486 processor and Intel 387 math coprocessor, the worst-
case, transcendental function error is typically  or  ulps, but is some-
times as large as  ulps.''
\end{quote}


According to \url{https://www.mirbsd.org/htman/i386/man3/exp.htm} and 
\url{http://man.openbsd.org/OpenBSD-current/man3/exp.3}:
\begin{quote}
``exp, log, expm1 and log1p are accurate to within an ulp''
\end{quote}
which is the same for freebsd \url{https://www.freebsd.org/cgi/man.cgi?query=exp&sektion=3&apropos=0&manpath=freebsd}:
\begin{quote}
``The values of exp(0), expm1(0), exp2(integer), and pow(integer, integer)
are exact provided that they are representable.  Otherwise the error in
these functions is generally below one ulp.''
\end{quote}
The same holds for ``FDLIBM'' \url{http://www.netlib.org/fdlibm/readme}:
\begin{quote}
``FDLIBM is intended to provide a reasonably portable (see
assumptions below), reference quality (below one ulp for
major functions like sin,cos,exp,log) math library
(libm.a).''
\end{quote}

In
\url{http://www.gnu.org/software/libc/manual/html_node/Errors-in-Math-Functions.html}
we find that both  and
 have an error of 1 ulp while  has an
error up to 3 ulps depending on the architecture.
For the most common architectures as used by us, however, the error of
 is 1 ulp.



We implemented the function in the programming language C.
We rely on the GNU C Library \citep{Loosemore:16}.
According to the GNU C Library manual which can be obtained from
\url{http://www.gnu.org/software/libc/manual/pdf/libc.pdf},
the errors of the math functions , , and

are not larger than 3 ulps for all architectures
\citep[pp. 528]{Loosemore:16}.
For the architectures ix86, i386/i686/fpu, and m68k/fpmu68k/m680x0/fpu
that we used the error are at least one ulp
\citep[pp. 528]{Loosemore:16}.




\subsubsection{Intermediate Lemmata and Proofs}
\label{sec:smallLemmata}

Since we focus on the fixed point 
,
we assume for our whole analysis
that  and .
Furthermore, we restrict the range of the variables
,
,
, and
.

For bounding different partial derivatives we need properties of
different functions. 
We will bound a the absolute value of a function by computing an upper
bound on its maximum and a lower bound on its minimum. These bounds
are computed by upper or lower bounding terms. The bounds get tighter
if we can combine terms to a more complex function and bound this
function. The following lemmata give some properties of functions that
we will use in bounding complex functions. 

Throughout this work, we use the error function  and the complementary
error function . \index{error function!definition} \index{complementary error function!definition}
\index{erf} \index{erfc}

\begin{lemma}[Basic functions]
\label{lem:basics}

 is strictly monotonically increasing from  at  to
 at  and has positive curvature.

According to its definition  is strictly monotonically decreasing from 2 at  to 0 at .
\end{lemma}

Next we introduce a bound on :
\begin{lemma}[Erfc bound from Abramowitz]
\label{lem:Abramowitz}
\index{Abramowitz bounds}


for . 
\end{lemma}
\begin{proof}
The statement follows immediately from 
\citep{Abramowitz:64} (page 298, formula 7.1.13).
\end{proof}

These bounds are displayed in figure~\ref{fig:abramowitz}. \index{Abramowitz bounds} \index{error function!bounds}\index{complementary error function!bounds} \index{erf} \index{erfc}
\begin{figure}
 \centering
 \includegraphics[width=0.49\columnwidth]{figures/abramowitzBounds.pdf}
 \caption[Graph of the Abramowitz bound for the complementary error function.]{Graphs of the upper and lower bounds on .  The lower bound  (red),  
  the upper bound  (green) and the function  (blue) as 
  treated in Lemma~\ref{lem:Abramowitz}. \label{fig:abramowitz}}
\end{figure} 


\begin{lemma}[Function ]
\label{lem:exerfc}

 is strictly monotonically decreasing for  
and has positive curvature 
(positive 2nd order derivative), that is, the decreasing slowes down.
\end{lemma}

\begin{figure}
 \includegraphics[width=0.49\columnwidth]{figures/lemma4.pdf}
 \includegraphics[width=0.49\columnwidth]{figures/lemma5.pdf}
 \caption[Graphs of the functions   and .]{Graphs of the functions   (left) and  (right) treated in Lemma~\ref{lem:exerfc} and Lemma~\ref{lem:xeErfc}, 
 respectively. \label{fig:lemma4}}
\end{figure} 

A graph of the function is displayed in Figure~\ref{fig:lemma4}.

\begin{proof}
The derivative of  is

Using Lemma~\ref{lem:Abramowitz}, we get

Thus 
is strictly monotonically decreasing for .

The second order derivative of  is


Again using Lemma~\ref{lem:Abramowitz} (first inequality), we get

For the last inequality we added 1 in the numerator in the square root
which is subtracted, that is, making a larger negative term in the
numerator. 
\end{proof}



\begin{lemma}[Properties of ]
\label{lem:xeErfc}
\index{error function!properties}

The function  has the sign of  and is
monotonically increasing to .
\end{lemma}
\begin{proof}
The derivative of   is

This derivative is positive since


We apply Lemma~\ref{lem:Abramowitz}
to  and divide the terms of the lemma by ,
which gives

For  both the upper and the lower bound go to 
.
\end{proof}


\begin{lemma}[Function ]
\label{lem:x11}

 is monotonically increasing in .
It has minimal value  and maximal value
.
\end{lemma}
\begin{proof}
Obvious.
\end{proof}

\begin{lemma}[Function ]
\label{lem:x22}

 is
monotonically increasing in  and is positive.
It has minimal value  and maximal value
.
\end{lemma}
\begin{proof}
Obvious. 
\end{proof}


\begin{lemma}[Function ]
\label{lem:xx1}


is larger than zero and increasing in both  and .
It has minimal value  and maximal value
.
\end{lemma}
\begin{proof}
The derivative of the function

with respect to  is

since  and .
\end{proof}

\begin{lemma}[Function ]
\label{lem:xx2}


is larger than zero and increasing in both  and .
It has minimal value  and maximal value
.
\end{lemma}
\begin{proof}
The derivative of the function

with respect to  is

\end{proof}

\begin{lemma}[Function ]
\label{lem:xx3}


monotonically decreasing in  and monotonically increasing in .
It has minimal value  and maximal value
.
\end{lemma}
\begin{proof}
Obvious.
\end{proof}

\begin{lemma}[Function ]
\label{lem:xx4}


has a minimum at 0 for  or  and has a maximum for
the smallest   and largest  and is larger or equal to zero.
It has minimal value  and maximal value
.
\end{lemma}
\begin{proof}
Obvious.
\end{proof}


\begin{lemma}[Function ]
\label{lem:F1}


and decreasing in .
\end{lemma}
\begin{proof}
Statements follow directly from elementary functions square root and
division.
\end{proof}

\begin{lemma}[Function ]
\label{lem:F2}


and decreasing in  and increasing in 
.
\end{lemma}
\begin{proof}
Statements follow directly from Lemma~\ref{lem:basics} and .
\end{proof}

\begin{lemma}[Function ]
\label{lem:F3}

For  and ,
 
and increasing in both  and .
\end{lemma}
\begin{proof}
We consider the function
    , 
which has the derivative with respect to :

This derivative is larger than zero, since 

The last inequality follows from 
 for .

We next consider the function
,
which  has the derivative with respect to :

\end{proof}

\begin{lemma}[Function ]
\label{lem:F4}

The function \\
 
is decreasing in  and increasing in .
\end{lemma}
\begin{proof}
We define the function

which has as derivative with respect to :

The derivative of the term

with respect to  is 
, since
.
Therefore the term is maximized with the smallest value for , which
is .
For  we use for each term the value which gives maximal
contribution. We obtain an upper bound for the term:

Therefore the derivative with respect to  
is smaller than zero and the original function is decreasing in 

We now consider the derivative with respect to .
The derivative with respect to  of the function

is

Since 
, the derivative is larger than zero.
Consequently, the original function is increasing in  .

The maximal value is obtained with the minimal  and the maximal  .
The maximal value is

Therefore the original function is smaller than zero.
\end{proof}



\begin{lemma}[Function ]
\label{lem:F5}

For  and , \\

and increasing in both  and .

\end{lemma}
\begin{proof}
The derivative of the function

with respect to  is

since


The derivative of the function

with respect to  is


The maximal function value is obtained by maximal  and the maximal  . 
The maximal value is
.
Therefore the function is negative.
\end{proof}

\begin{lemma}[Function ]
\label{lem:F6}

The function
 
is decreasing in  and increasing in .
\end{lemma}
\begin{proof}
The derivative of the function

with respect to  is

since 
.

The derivative of the function

with respect to  is


The maximal function value is obtained for 
minimal  and the maximal  . 
The value is
.
Thus, the function is negative.
\end{proof}

\begin{lemma}[Function ]
\label{lem:F7}

The function 
  
is increasing in  and decreasing in .
\end{lemma}


\begin{proof}

The derivative of the function

with respect to  is


This derivative is larger than zero, since


We explain this chain of inequalities:
\begin{itemize}
\item The first inequality follows by applying Lemma~\ref{lem:exerfc}
  which says that  
is strictly monotonically decreasing. The minimal value that is larger
than 0.4349 is taken on at
the maximal values  and . 
\item The second inequality uses 
.
\item The equalities are just algebraic reformulations.
\item The last inequality follows from
.
\end{itemize}
Therefore the function is increasing in  .

Decreasing in  follows from decreasing of 
according to Lemma~\ref{lem:exerfc}.
Positivity follows form the fact that  and the
exponential function are positive and that  . 
\end{proof}

\begin{lemma}[Function ]
\label{lem:F8}

The function 

is increasing in  and decreasing in .
\end{lemma}
\begin{proof}
The derivative of the function

is

We only have to determine the sign of
 
since all other factors are obviously larger than zero.

This derivative is larger than zero, since

We explain this chain of inequalities:
\begin{itemize}
\item The first inequality follows by applying Lemma~\ref{lem:exerfc}
  which says that  
is strictly monotonically decreasing. The minimal value that is larger
than 0.261772 is taken on at
the maximal values  and . 
.
\item The equalities are just algebraic reformulations.
\item The last inequality follows from
.
\end{itemize}
Therefore the function is increasing in  .

Decreasing in  follows from decreasing of 
according to Lemma~\ref{lem:exerfc}.
Positivity follows from the fact that  and the
exponential function are positive and that  . 
\end{proof}




\begin{lemma}[Bounds on the Derivatives]
\label{proof:Bounds}
The following bounds on the absolute values of the 
derivatives of the Jacobian entries ,
,
, and

with respect to 
, , , and  hold:




\end{lemma}



\begin{proof}
For each derivative we compute a lower and an upper bound and take the
maximum of the absolute value. 
A lower bound is determined by minimizing the single terms of the
functions that represents the derivative. An upper bound is determined
by maximizing the single terms of the functions that represent the
derivative. Terms can be combined to larger terms for which
the maximum and the minimum must be known. We apply many previous lemmata
which state properties of functions representing single or combined
terms. The more terms are combined, the tighter the bounds can be
made. 

Next we go through all the derivatives, where we use 
Lemma~\ref{lem:x11}, 
Lemma~\ref{lem:x22}, 
Lemma~\ref{lem:xx1}, 
Lemma~\ref{lem:xx2}, 
Lemma~\ref{lem:xx3},
Lemma~\ref{lem:xx4},
Lemma~\ref{lem:basics}, and
Lemma~\ref{lem:exerfc} without citing. Furthermore, we use the bounds on the simple
expressions ,, ..., and  as defined the aforementioned lemmata:
\begin{itemize}
\item 

We use Lemma~\ref{lem:F1} and
consider the expression 

in brackets.
An upper bound on the maximum of is

A lower bound on the minimum is

Thus, an upper bound on the maximal absolute value is 


\item 

We use Lemma~\ref{lem:F1} and
consider the expression 

in brackets.

An upper bound on the maximum is

A lower bound on the minimum is

This term is subtracted, and , therefore we have
to use the minimum and the maximum for the argument of .

Thus, an upper bound on the maximal absolute value is 


\item 

We consider the term in brackets


We apply Lemma~\ref{lem:F3} for the first sub-term.
An upper bound on the maximum is

A lower bound on the minimum is


Thus, an upper bound on the maximal absolute value is 


\item 

We use the results of  item 
were the brackets are only differently scaled.
Thus, an upper bound on the maximal absolute value is 


\item 

Since ,
an upper bound on the maximal absolute value is 


\item 

We use the results of  item 
were the brackets are only differently scaled.
Thus, an upper bound on the maximal absolute value is 


\item 

For the second term in brackets, we see that
 and .

We now check different values for 

where we maximize or minimize all single terms.


A lower bound on the minimum of this expression is

An upper bound on the maximum of this expression is



An upper bound on the maximum is

A lower bound on the minimum is

Thus, an upper bound on the maximal absolute value is 


\item 

We use Lemma~\ref{lem:F4} to obtain
an upper bound on the maximum of the expression of the lemma:

We use Lemma~\ref{lem:F4} to obtain
an lower bound on the minimum of the expression of the lemma:



Next we apply Lemma~\ref{lem:F7} for the expression .
We use Lemma~\ref{lem:F7} to obtain
an upper bound on the maximum of this expression:

We use Lemma~\ref{lem:F7} to obtain
an lower bound on the minimum of this expression:


Next we apply Lemma~\ref{lem:exerfc} for .
An upper bound on this expression is

A lower bound on this expression is


The sum of the minimal values of the terms is
.

The sum of the maximal values of the terms is
.


Thus, an upper bound on the maximal absolute value is 


\item 

An upper bound on the maximum is

A upper bound on the absolute minimum is

Thus, an upper bound on the maximal absolute value is 


\item 

An upper bound on the maximum is

A lower bound on the minimum is

Thus, an upper bound on the maximal absolute value is 


\item 

An upper bound on the maximum is

A lower bound on the minimum is

Thus, an upper bound on the maximal absolute value is 


\item 

An upper bound on the maximum is

A lower bound on the minimum is

Thus, an upper bound on the maximal absolute value is 


\item 


We use the fact that .
Thus, an upper bound on the maximal absolute value is 


\item 

An upper bound on the maximum is

A lower bound on the minimum is

Thus, an upper bound on the maximal absolute value is 


\item 

We apply Lemma~\ref{lem:F5} to the expression
.
Using  Lemma~\ref{lem:F5}, an upper bound on the maximum is

Using  Lemma~\ref{lem:F5}, a lower bound on the minimum is

Thus, an upper bound on the maximal absolute value is 


\item 

We apply Lemma~\ref{lem:F6} to the expression
. \\
We apply Lemma~\ref{lem:F7} to the expression 
. 
We apply Lemma~\ref{lem:F8} to the expression
. 

We combine the results of these lemmata to obtain 
an upper bound on the maximum:

We combine the results of these lemmata to obtain 
an lower bound on the minimum:

Thus, an upper bound on the maximal absolute value is 

\end{itemize}
\end{proof}



\begin{lemma}[Derivatives of the Mapping]
\label{proof:mapDerivatives}
We assume  and .
We restrict the range of the variables to the domain
,
,
, and
.

The derivative 
has the sign of .

The derivative 
is positive.

The derivative 
has the sign of .

The derivative 

is positive.
\end{lemma}


\begin{proof}
\begin{itemize}
\item 

 according to 
Lemma~\ref{lem:basics} and 
is also larger than zero according to Lemma~\ref{lem:exerfc}.
Consequently, has 
the sign of .

\item 

Lemma~\ref{lem:exerfc} says 
 is decreasing in .
The first term (negative) is increasing in  since it is
proportional to minus
one over the squared root of  .

We obtain a lower bound by
setting  for the  term.
The term in brackets is larger than

Consequently, the function is larger than zero.


\item 

We consider the sub-function

We set  and  and obtain


The derivative of this sub-function with respect to  is


The inequality follows from Lemma~\ref{lem:xeErfc}, which states that 
 is monotonically increasing in .
Therefore the sub-function is increasing in . 

The derivative of this sub-function with respect to  is


The sub-function is increasing in , since the
derivative is larger than zero:

We explain this chain of inequalities:
\begin{itemize}
\item First inequality: We applied Lemma~\ref{lem:Abramowitz} two times.

\item Equalities factor out  and reformulate.

\item Second inequality part 1: we applied

\item Second inequality part 2: we show that for  following holds:
. 
We have  and
 . 
Therefore the minimum is at border for minimal  and maximal :

Thus

for .

\item Equalities only solve square root and factor out the resulting
  terms  and .

\item We set  and multiplied out. Thereafter we
  also factored out  in the numerator. Finally a quadratic
  equations was solved. 
\end{itemize}

The sub-function has its minimal value for 
minimal  and minimal 
 and . 
We further minimize the function



We compute the minimum of the term in brackets of :  

Therefore the term in brackets is larger than zero.

Thus, 
has the sign of .


\item 

We look at the sub-term

We obtain a chain of inequalities:

We explain this chain of inequalities:
\begin{itemize}
\item First inequality: We applied Lemma~\ref{lem:Abramowitz} two times.

\item Equalities factor out  and reformulate.

\item Second inequality part 1: we applied

\item Second inequality part 2: we show that for  following holds:
. 
We have  and
 . 
Therefore the minimum is at border for minimal  and maximal :

Thus

for .

\item Equalities only solve square root and factor out the resulting
  terms  and .
\end{itemize}

We know that  according to 
Lemma~\ref{lem:basics}.
For the sub-term we derived  


Consequently, both terms in the brackets of  
are larger than zero.
Therefore  
is larger than zero.
\end{itemize}

\end{proof}



\begin{lemma}[Mean at low variance]
\label{lem:meanLowVar}
The mapping of the mean  (Eq.~\eqref{eq:mappingMean}) 

in the domain ,  ,
and  is bounded by  

and

\end{lemma}

We can consider  with given  as a function in . We show the graph of this function at the
maximal  in the interval  in Figure~\ref{fig:meanAtLowVar}.

\begin{figure}
  \centering
 \includegraphics[width=0.5\textwidth]{figures/munew_closetozero.pdf}
 \caption[The graph of function  for low variances]{The graph of function  for low variances  for , where , is
 displayed in yellow. 
 Lower and upper bounds based on the Abramowitz bounds (Lemma~\ref{lem:Abramowitz}) are displayed in green and blue, respectively. 
 \label{fig:meanAtLowVar}}
\end{figure}



\begin{proof}
Since  is strictly monotonically increasing with  

where we have used the monotonicity of the terms in . 

Similarly, we can use the monotonicity of the terms in  to show that 


such that  at low variances.

Furthermore, when , the terms with the arguments of the complementary error functions  and the exponential function 
go to infinity, therefore these three terms converge to zero. Hence, the remaining terms are only .
\end{proof}



\begin{lemma}[Bounds on derivatives of  in ]
\label{lem:muBounds}
The derivatives of the function  
(Eq.~\eqref{eq:mappingMean})
with respect to  in the domain 

can be bounded as follows:


\end{lemma}


\begin{proof}
The expression

contains the terms 
and 
which are monotonically decreasing in their arguments (Lemma~\ref{lem:exerfc}). We can therefore obtain their
minima and maximal at the minimal and maximal arguments. Since the first term has a negative sign in the expression, both terms
reach their maximal value at , , and . 


Since,  is symmetric in  and , these bounds also hold for the derivate to .

We use the argumentation that the term with the error function is monotonically decreasing (Lemma~\ref{lem:exerfc})
again for the expression


We have used that the term 
and the term .
Since  is symmetric in  and , we only have to chance outermost
term  to   to 
obtain the estimate .


\end{proof}


\begin{lemma}[Tight bound on  in ]
\label{lem:musquared}
The function  
(Eq.~\eqref{eq:mappingMean})
is bounded by


in the domain 
.
\end{lemma}

We visualize the function  at its maximal  and for  in the form
  in Figure~\ref{fig:meanSqu}.

\begin{figure}
 \centering
 \includegraphics[width=0.5\textwidth]{figures/meanSqu_lowVar.pdf}
 \caption[Graph of the function ]{The graph of the function  is displayed. It has a local 
  maximum at  and  in the domain . \label{fig:meanSqu}}
\end{figure}



\begin{proof}
We use a similar strategy to the one we have used to show the bound on the singular value (Lemmata~\ref{lem:Ds1Bounds}, \ref{lem:meanValue}, and \ref{lem:sBound}), where
we evaluted the function on a grid and used bounds on the derivatives together with the mean value theorem.
Here we have


We use Lemma~\ref{lem:muBounds} and Lemma~\ref{lem:meanLowVar}, to obtain 


We evaluated the function  in a grid  of  with ,
,
, and
 using a computer and obtained the maximal value , therefore 
the maximal value of    is bounded by



Furthermore we used error propagation to estimate the numerical error on the function evaluation. Using the error propagation rules 
derived in Subsection~\ref{sec:error}, we found that the numerical error is smaller than  in the worst case.
\end{proof}







\begin{lemma}[Main subfunction]
\label{proof:mainsubfunction}
For  and , 

the function

is smaller than zero, is strictly monotonically increasing in ,
and strictly monotonically decreasing in  for the minimal .
\end{lemma}

\begin{proof}
We first consider the derivative of sub-function
Eq.~\eqref{eq:subfunction} with respect to .
The derivative of the function 

with respect to  is
 


We consider the numerator


For bounding this value, we use the approximation 

from \citet{Ren:07}.
We start with an error analysis of this approximation.
According to \citet{Ren:07} (Figure~1), the approximation 
error is positive in the range
.  This range contains all possible
arguments of  that we consider.
Numerically we maximized and minimized the approximation error of the
whole expression

We numerically determined  for 
 and . 
We used different numerical optimization techniques like  
gradient based constraint BFGS algorithms and 
non-gradient-based Nelder-Mead methods with different start points.
Therefore our approximation is smaller than the function that we
approximate. 
We subtract an additional safety gap of 0.0131259 from our
approximation to ensure that the inequality via the approximation
holds true. With this safety gap the inequality would hold true even 
for negative , where the approximation error becomes negative and
the safety gap would compensate.
Of course, the safety gap of 0.0131259 is not necessary for our
analysis but may help or future investigations.

We have the sequences of inequalities using the approximation of \citet{Ren:07}:



We explain this sequence of inequalities:
\begin{itemize} 
\item First inequality: The approximation of \citet{Ren:07}
and then subtracting a safety gap (which would not be necessary for the
current analysis).

\item Equalities: The factor  is factored out and
  canceled. 

\item Second inequality: adds a positive term in the first root to
  obtain a binomial form. The term containing the root 
is positive and the root is in the denominator, 
therefore the whole term becomes smaller.
\end{itemize}
\begin{itemize} 

\item Equalities: solve for the term and factor out.

\item Bringing all terms to the denominator
.

\item Equalities: Multiplying out and expanding terms.

\item Last inequality  is proofed in the following sequence of
  inequalities.
\end{itemize}

We look at the numerator of the last expression of 
Eq.~\eqref{eq:ineqX}, which we show to be
positive in order to show  in 
Eq.~\eqref{eq:ineqX}. The numerator is 

The factor in front of the root is positive.
If the term, that does not contain the root, was positive, then the whole expression would be positive and 
we would have proofed that the numerator is positive. 
Therefore we consider the case that the term, that does not contain the root, is negative.
The term that contains the root must be larger than the other term in absolute values. 

Therefore the squares of the root term have to be larger 
than the square of the other term to show  in 
Eq.~\eqref{eq:ineqX}.
Thus, we have the inequality:


This is equivalent to

We obtain the inequalities: 

We used  and .
We have proofed the last inequality  of Eq.~\eqref{eq:ineqX}.

Consequently the derivative is always positive independent of ,
thus 

is strictly monotonically increasing in .


\paragraph{The main subfunction is smaller than zero.} 
Next we show that the 
sub-function Eq.~\eqref{eq:subfunction} is smaller
than zero.
We consider the limit:

The limit follows from Lemma~\ref{lem:Abramowitz}.
Since the function is monotonic increasing in , it has to approach
 from below. Thus,

is smaller than zero.

\paragraph{Behavior of the main subfunction with respect to  at minimal .} 
We now consider the derivative of sub-function
Eq.~\eqref{eq:subfunction} with respect to .
We proofed that sub-function
Eq.~\eqref{eq:subfunction} is  strictly monotonically increasing 
independent of . 
In the proof of Theorem~\ref{th:s2Cont}, we need the minimum
of  sub-function
Eq.~\eqref{eq:subfunction}. Therefore we are only interested in the
derivative of sub-function
Eq.~\eqref{eq:subfunction} with respect to 
for the minimum  

Consequently, we insert the minimum  into the  sub-function
Eq.~\eqref{eq:subfunction}. The main terms become

and

Sub-function
Eq.~\eqref{eq:subfunction} becomes:

The derivative of this function with respect to  is


We again will use the approximation of \citet{Ren:07}

Therefore we first perform an error analysis.
We estimated the maximum and minimum of 

We obtained for the maximal absolute error the value .
We added an approximation 
error of  to the approximation of the derivative.
Since we want to show that the approximation upper bounds the true
expression, the addition of the approximation error is required here.
We get a sequence of inequalities:



We explain this sequence of inequalities.
\begin{itemize}
\item First inequality: The approximation of \citet{Ren:07}
and then adding the error bound to ensure that the approximation
is larger than the true value.

\item First equality: The factor  and 
  are factored out and canceled.

\item Second equality: Bringing all terms to the denominator


\item Last inequality  is proofed in the following sequence of
  inequalities.
\end{itemize}
We look at the numerator of the last term in Eq.~\eqref{eq:ineqY}. We
have to proof that this numerator is smaller than zero in order to
proof the last inequality of  Eq.~\eqref{eq:ineqY}.
The numerator is

We now compute upper bounds for this numerator:


For the first inequality we choose  in the roots, so that 
positive terms maximally increase and negative terms maximally decrease.
The second inequality just removed the  term which is always
negative, therefore increased the expression.
For the last inequality, the term in brackets 
is negative for all settings of . 
Therefore we make the brackets as negative as possible 
and make the whole term positive by multiplying with .

Consequently 

is strictly monotonically decreasing in  for the minimal
. 
\end{proof}


\begin{lemma}[Main subfunction below]
\label{proof:mainsubfunctionbelow} 
For  and , 
the function

smaller than zero, is strictly monotonically increasing in 
and strictly monotonically increasing in  for the minimal ,
, , and  (lower
bound of  on ).
\end{lemma}


\begin{proof}
We first consider the derivative of sub-function
Eq.~\eqref{eq:subfunction1} with respect to .
The derivative of the function 

with respect to  is
 


We consider the numerator


For bounding this value, we use the approximation 

from \citet{Ren:07}.
We start with an error analysis of this approximation.
According to \citet{Ren:07} (Figure~1), the approximation 
error is both positive and negative in the range
.  This range contains all possible
arguments of  that we consider in this subsection.
Numerically we maximized and minimized the approximation error of the
whole expression

We numerically determined  for 
 and . 
We used different numerical optimization techniques like  
gradient based constraint BFGS algorithms and 
non-gradient-based Nelder-Mead methods with different start points.
Therefore our approximation is smaller than the function that we
approximate. 

We use an error gap of  to countermand the error due to the
approximation. We have the sequences of inequalities using the approximation of 
\citet{Ren:07}:

We explain this sequence of inequalities:
\begin{itemize} 
\item First inequality: The approximation of \citet{Ren:07}
and then subtracting an error gap of .

\item Equalities: The factor  is factored out and
  canceled. 

\item Second inequality: adds a positive term in the first root to
  obtain a binomial form. The term containing the root 
is positive and the root is in the denominator, 
therefore the whole term becomes smaller.

\item Equalities: solve for the term and factor out.

\item Bringing all terms to the denominator
.

\item Equalities: Multiplying out and expanding terms.

\item Last inequality  is proofed in the following sequence of
  inequalities.
\end{itemize}

We look at the numerator of the last expression of 
Eq.~\eqref{eq:ineqX1}, which we show to be
positive in order to show  in 
Eq.~\eqref{eq:ineqX1}. The numerator is 

The factor 
in front of the root is positive:

If the term that does not contain the root would be positive, 
then everything is positive and we have proofed the the numerator is
positive. Therefore we consider the case that the term that does
not contain the root is negative.
The term that contains the root must be larger than 
the other term in absolute values. 

Therefore the squares of the root term have to be larger 
than the square of the other term to show  in 
Eq.~\eqref{eq:ineqX1}.
Thus, we have the inequality:


This is equivalent to

We obtain the inequalities: 

We used  and  (reducing the negative -term to a
-term).
We have proofed the last inequality  of Eq.~\eqref{eq:ineqX1}.

Consequently the derivative is always positive independent of ,
thus 

is strictly monotonically increasing in .



Next we show that the 
sub-function Eq.~\eqref{eq:subfunction1} is smaller
than zero.
We consider the limit:

The limit follows from Lemma~\ref{lem:Abramowitz}.
Since the function is monotonic increasing in , it has to approach
 from below. Thus,

is smaller than zero.


We now consider the derivative of sub-function
Eq.~\eqref{eq:subfunction1} with respect to .
We proofed that sub-function
Eq.~\eqref{eq:subfunction1} is  strictly monotonically increasing 
independent of . 
In the proof of Theorem~\ref{th:s2Increase}, we need the minimum
of  sub-function
Eq.~\eqref{eq:subfunction1}. First, we are interested in the
derivative of sub-function
Eq.~\eqref{eq:subfunction1} with respect to 
for the minimum .

Consequently, we insert the minimum  into the sub-function
Eq.~\eqref{eq:subfunction1}:

The derivative of this function with respect to  is

For the first inequality, we use Lemma~\ref{lem:xeErfc}.
Lemma~\ref{lem:xeErfc} says that 
the function  has the sign of  and is
monotonically increasing to .
Consequently, we inserted the maximal  to
make the negative term more negative and the minimal 
to make the positive term less positive.


Consequently 

is strictly monotonically increasing in  for the minimal
. 


Next, we consider , which is the maximal 
and minimal .
We insert the minimum  into the  sub-function
Eq.~\eqref{eq:subfunction1}:

The derivative with respect to  is:

For the first inequality we applied Lemma~\ref{lem:xeErfc}
which states that the function  is
monotonically increasing.
Consequently, we inserted the maximal  to
make the negative term more negative and the minimal 
to make the positive term less positive.

Consequently 

is strictly monotonically increasing in  for . 


Next, we consider , which is the minimal .
We insert the minimum  into the  sub-function
Eq.~\eqref{eq:subfunction1}:

The derivative with respect to  is:

For the first inequality we applied Lemma~\ref{lem:xeErfc}
which states that the function  is
monotonically increasing.
Consequently, we inserted the maximal  to
make the negative term more negative and the minimal 
to make the positive term less positive.

Consequently 

is strictly monotonically increasing in  for . 


Next, we consider , which is the minimal
 (here we consider  as lower bound for ).
We insert the minimum  into the  sub-function
Eq.~\eqref{eq:subfunction1}:

The derivative with respect to  is: 

For the first inequality we applied Lemma~\ref{lem:xeErfc}
which states that the function  is
monotonically increasing.
Consequently, we inserted the maximal  to
make the negative term more negative and the minimal 
to make the positive term less positive.

Consequently 

is strictly monotonically increasing in  for . 
\end{proof}




\begin{lemma}[Monotone Derivative]
\label{proof:monotonederivative}
For , 
and the domain 
, 
,
, and 
.
We are interested of the derivative of


The derivative of the equation above with
respect to
\begin{itemize}
\item  is larger than zero;
\item  is smaller than zero for maximal
, , and  (with
);
\item  is larger than zero for , , , and .
\end{itemize}

\end{lemma}




\begin{proof}
We consider the domain:
, 
,
, and 
.



We use Lemma~\ref{lem:subfunction1} to determine the derivatives.
Consequently, the derivative of 

with respect to   is larger than zero, which follows
directly from  Lemma~\ref{lem:subfunction1} using the chain rule.

{Consequently,} the derivative of 

with respect to   is larger than zero for , , , and ,
which also follows
directly from  Lemma~\ref{lem:subfunction1}.

We now consider the derivative with respect to ,
which is not trivial since  is a factor of the whole expression.
The sub-expression should be maximized as it appears with
negative sign in the mapping for .


First,
we consider the function for  
the largest  and the largest  
for determining the derivative with respect to . 


The expression becomes


The derivative with respect to  is 


We are considering only the numerator and use again the approximation
of \citet{Ren:07}.
The error analysis on the whole numerator gives an approximation error . Therefore
we add 200 to the numerator when we use the approximation \citet{Ren:07}.
We obtain the inequalities:

After applying the approximation
of \citet{Ren:07} and adding 200,
we first factored out .
Then we brought all terms to the same denominator.

We now consider the numerator:


First we expanded the term (multiplied it out).
The we put the terms multiplied by the same square root into brackets.
The next inequality sign stems from inserting the maximal value of  for  for
some positive terms and value of  for negative terms. 
These terms are then expanded at the -sign.
The next equality factors the terms under the squared root.
We decreased the negative term by setting
 under the root.
We increased positive terms by setting  
 and

under the root for positive terms.
The positive terms are increase, since
, thus
.
For the next inequality we decreased negative terms by inserting
 and increased positive terms by inserting
. The next equality expands the terms.
We use upper bound of  and lower bound of  to obtain terms with
corresponding exponents of .

For the last -sign we used the function

The derivative of this function is

and the second order derivative is

The derivative at 0.8 is smaller than zero:

Since the second order derivative is negative, the derivative
decreases with increasing . Therefore the derivative is
negative for all values of   that we consider, that is, the
function Eq.~\eqref{eq:funcA} is strictly monotonically decreasing.
The maximum of the function Eq.~\eqref{eq:funcA} is therefore at .
We inserted  to obtain the maximum.



Consequently, the derivative of

with respect to  is smaller than zero for maximal .

Next,
we consider the function for  
the largest  and the largest  
for determining the derivative with respect to . 


The expression becomes


The derivative with respect to  is 


We are considering only the numerator and use again the approximation
of \citet{Ren:07}.
The error analysis on the whole numerator gives an approximation error . Therefore
we add 20 to the numerator when we use the approximation of \citet{Ren:07}.
We obtain the inequalities:

After applying the approximation
of \citet{Ren:07} and adding 20,
we first factored out .
Then we brought all terms to the same denominator.

We now consider the numerator:


First we expanded the term (multiplied it out).
The we put the terms multiplied by the same square root into brackets.
The next inequality sign stems from inserting the maximal value of  for  for
some positive terms and value of  for negative terms. 
These terms are then expanded at the -sign.
The next equality factors the terms under the squared root.
We decreased the negative term by setting
 under the root.
We increased positive terms by setting  
 and

under the root for positive terms.
The positive terms are increase, since
, thus
.
For the next inequality we decreased negative terms by inserting
 and increased positive terms by inserting
. The next equality expands the terms.
We use upper bound of  and lower bound of  to obtain terms with
corresponding exponents of .

Consequently, the derivative of

with respect to  is smaller than zero for maximal .

Next,
we consider the function for  
the largest  and the largest  
for determining the derivative with respect to . 
However we assume , in order to restrict the
domain of .

The expression becomes


The derivative with respect to  is 


We are considering only the numerator and use again the approximation
of \citet{Ren:07}.
The error analysis on the whole numerator gives an approximation error . Therefore
we add 32 to the numerator when we use the approximation of \citet{Ren:07}.
We obtain the inequalities:

After applying the approximation
of \citet{Ren:07} and adding 200,
we first factored out .
Then we brought all terms to the same denominator.

We now consider the numerator:


First we expanded the term (multiplied it out).
The we put the terms multiplied by the same square root into brackets.
The next inequality sign stems from inserting the maximal value of  for  for
some positive terms and value of  for negative terms. 
These terms are then expanded at the -sign.
The next equality factors the terms under the squared root.
We decreased the negative term by setting
 under the root.
We increased positive terms by setting  
 and

under the root for positive terms.
The positive terms are increase, since
, thus
.
For the next inequality we decreased negative terms by inserting
 and increased positive terms by inserting
. The next equality expands the terms.
We use upper bound of  and lower bound of  to obtain terms with
corresponding exponents of .

{Consequently}, the derivative of

with respect to  is smaller than zero for maximal
 and the domain .
\end{proof}


\begin{lemma}
\label{lem:mainsubfunctionJ11J12}
In the domain  and , 
the function  has a global 
maximum at  and  and a global minimum at  and . 
\end{lemma}

\begin{proof}
 is strictly monotonically decreasing
in , since its derivative with respect to  is negative:

The two last inqualities come from applying Abramowitz bounds~\ref{lem:Abramowitz} and from the fact that the expression

does not change monotonicity in the 
domain and hence the maximum must be found at the border. For  that maximizes the function  is monotonically   in , because 
its derivative w.r.t.  at  is 

Therefore, the values  and  give
a global maximum of the function  in the domain  and  and
the values  and  give the global minimum.
\end{proof}



\section{Additional information on experiments}
\label{sec:experiments} \index{experiments}
In this section, we report the hyperparameters that were considered for each method and
data set and give details on the processing of the data sets.

\subsection{121 UCI Machine Learning Repository data sets: Hyperparameters} \index{experiments!UCI}
For the UCI data sets, the best hyperparameter setting was determined by a grid-search over all
hyperparameter combinations using 15\% of the training data as validation set.
The early stopping parameter was determined on the smoothed learning curves of 100 epochs
of the validation set. Smoothing was done using moving averages of 10 consecutive 
values. We tested ``rectangular'' and ``conic'' layers -- rectangular layers have 
constant number of hidden units in each layer, conic layers start with the given 
number of hidden units in the first layer and then decrease the number of hidden units
to the size of the output layer according to the geometric progession.
If multiple hyperparameters provided identical performance on the validation 
set, we preferred settings with a higher number of layers, lower learning rates and higher dropout rates. 
All methods had the chance to adjust their hyperparameters to the data set at hand. 


\index{experiments!UCI!hyperparameters}
\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for self-normalizing networks in the UCI data sets.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{1024, 512, 256\} \\
  Number of hidden layers & \{2, 3, 4, 8, 16, 32\} \\
  Learning rate & \{0.01, 0.1, 1\} \\
  Dropout rate & \{0.05, 0\}\\
  Layer form & \{rectangular, conic\} \\
\bottomrule
\end{tabular}
\end{center}

\end{table}

\begin{table}[htp]
\begin{center}
\caption[Hyperparameters considered for ReLU networks in the UCI data sets.]{Hyperparameters considered for ReLU networks with MS initialization in the UCI data sets.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
   Number of hidden units & \{1024, 512, 256\} \\
  Number of hidden layers & \{2,3,4,8,16,32\} \\
  Learning rate & \{0.01, 0.1, 1\} \\
  Dropout rate & \{0.5, 0\}\\
  Layer form & \{rectangular, conic\} \\
\bottomrule
\end{tabular}
\end{center}

\end{table}





\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for batch normalized networks in the UCI data sets.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{1024, 512, 256\} \\
  Number of hidden layers & \{2, 3, 4, 8, 16, 32\} \\
  Learning rate & \{0.01, 0.1, 1\} \\
  Normalization & \{Batchnorm\} \\
  Layer form & \{rectangular, conic\} \\
\bottomrule
\end{tabular}
\end{center}

\end{table}



\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for weight normalized networks in the UCI data sets.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{1024, 512, 256\} \\
  Number of hidden layers & \{2, 3, 4, 8, 16, 32\} \\
  Learning rate & \{0.01, 0.1, 1\} \\
  Normalization & \{Weightnorm\} \\
  Layer form & \{rectangular, conic\} \\
\bottomrule
\end{tabular}
\end{center}

\end{table}



\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for layer normalized networks in the UCI data sets.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{1024, 512, 256\} \\
  Number of hidden layers & \{2, 3, 4, 8, 16, 32\} \\
  Learning rate & \{0.01, 0.1, 1\} \\
  Normalization & \{Layernorm\} \\
  Layer form & \{rectangular, conic\} \\

\bottomrule
\end{tabular}
\end{center}

\end{table}



\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for Highway networks in the UCI data sets.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden layers & \{2, 3, 4, 8, 16, 32\} \\
  Learning rate & \{0.01, 0.1, 1\} \\
  Dropout rate & \{0, 0.5\} \\
\bottomrule
\end{tabular}
\end{center}

\end{table}




\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for Residual networks in the UCI data sets.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of blocks & \{2, 3, 4, 8, 16\} \\
  Number of neurons per blocks & \{1024, 512, 256\} \\
  Block form & \{rectangular, diavolo\} \\
  Bottleneck & \{25\%, 50\%\} \\
  Learning rate & \{0.01, 0.1, 1\} \\
\bottomrule
\end{tabular}
\end{center}

\end{table}



\clearpage

\subsection{121 UCI Machine Learning Repository data sets: detailed results}
\index{experiments!UCI!details}

\paragraph{Methods compared.} 
We used data sets and preprocessing scripts by \citet{bib:Fernandez2014} for data preparation and
defining training and test sets. With several flaws in the method comparison\citep{bib:Wainberg2016} that we avoided, 
the authors compared 179 machine learning methods of 17 groups in their experiments. 
The method groups were defined by \citet{bib:Fernandez2014} as follows:
Support Vector Machines, RandomForest, Multivariate adaptive regression splines (MARS), 
Boosting, Rule-based, logistic and multinomial regression,
Discriminant Analysis (DA), Bagging, 
Nearest Neighbour, DecisionTree, other Ensembles, Neural Networks, Bayesian, Other Methods,
generalized linear models (GLM), Partial least squares and principal component regression (PLSR), and Stacking.
However, many of methods assigned to those groups were merely different implementations of the 
same method. Therefore, we selected one representative of each of the 17 groups for method 
comparison. The representative method was chosen as the group's method with the median performance 
across all tasks. Finally, we included 17 other machine learning methods of \citet{bib:Fernandez2014},
and 6 FNNs, BatchNorm, WeightNorm, LayerNorm, 
Highway, Residual and MSRAinit networks, and self-normalizing neural networks (SNNs) giving a total of 24 compared methods. 

\paragraph{Results of FNN methods for all 121 data sets.}
The results of the compared FNN methods can be found in Table~\ref{tab:UCIfull}.

\begin{table}
\caption[Comparison of FNN methods on all 121 UCI data sets.]{Comparison of FNN methods on all 121 UCI data sets.. The table reports the accuracy of FNN methods at each individual 
task of the 121 UCI data sets. The first column gives the name of the data set, the second the number
of training data points , the third the number of features  and the consecutive columns the accuracy values of
self-normalizing networks (SNNs), ReLU networks without normalization and with MSRA initialization (MS),
Highway networks (HW), Residual Networks (ResNet), networks with batch normalization (BN), weight 
normalization (WN), and layer normalization (LN). \label{tab:UCIfull}}
\footnotesize
\begin{tabular}{lrrlllllll}
\toprule
dataset &  &  & SNN & MS & HW & ResNet & BN & WN & LN\tabularnewline
\midrule
abalone & 4177 & 9 & 0.6657 & 0.6284 & 0.6427 & 0.6466 & 0.6303 & 0.6351 & 0.6178\tabularnewline
acute-inflammation & 120 & 7 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.9000\tabularnewline
acute-nephritis & 120 & 7 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000\tabularnewline
adult & 48842 & 15 & 0.8476 & 0.8487 & 0.8453 & 0.8484 & 0.8499 & 0.8453 & 0.8517\tabularnewline
annealing & 898 & 32 & 0.7600 & 0.7300 & 0.3600 & 0.2600 & 0.1200 & 0.6500 & 0.5000\tabularnewline
arrhythmia & 452 & 263 & 0.6549 & 0.6372 & 0.6283 & 0.6460 & 0.5929 & 0.6018 & 0.5752\tabularnewline
audiology-std & 196 & 60 & 0.8000 & 0.6800 & 0.7200 & 0.8000 & 0.6400 & 0.7200 & 0.8000\tabularnewline
balance-scale & 625 & 5 & 0.9231 & 0.9231 & 0.9103 & 0.9167 & 0.9231 & 0.9551 & 0.9872\tabularnewline
balloons & 16 & 5 & 1.0000 & 0.5000 & 0.2500 & 1.0000 & 1.0000 & 0.0000 & 0.7500\tabularnewline
bank & 4521 & 17 & 0.8903 & 0.8876 & 0.8885 & 0.8796 & 0.8823 & 0.8850 & 0.8920\tabularnewline
blood & 748 & 5 & 0.7701 & 0.7754 & 0.7968 & 0.8021 & 0.7647 & 0.7594 & 0.7112\tabularnewline
breast-cancer & 286 & 10 & 0.7183 & 0.6901 & 0.7465 & 0.7465 & 0.7324 & 0.6197 & 0.6620\tabularnewline
breast-cancer-wisc & 699 & 10 & 0.9714 & 0.9714 & 0.9771 & 0.9714 & 0.9829 & 0.9657 & 0.9714\tabularnewline
breast-cancer-wisc-diag & 569 & 31 & 0.9789 & 0.9718 & 0.9789 & 0.9507 & 0.9789 & 0.9718 & 0.9648\tabularnewline
breast-cancer-wisc-prog & 198 & 34 & 0.6735 & 0.7347 & 0.8367 & 0.8163 & 0.7755 & 0.8367 & 0.7959\tabularnewline
breast-tissue & 106 & 10 & 0.7308 & 0.4615 & 0.6154 & 0.4231 & 0.4615 & 0.5385 & 0.5769\tabularnewline
car & 1728 & 7 & 0.9838 & 0.9861 & 0.9560 & 0.9282 & 0.9606 & 0.9769 & 0.9907\tabularnewline
cardiotocography-10clases & 2126 & 22 & 0.8399 & 0.8418 & 0.8456 & 0.8173 & 0.7910 & 0.8606 & 0.8362\tabularnewline
cardiotocography-3clases & 2126 & 22 & 0.9153 & 0.8964 & 0.9171 & 0.9021 & 0.9096 & 0.8945 & 0.9021\tabularnewline
chess-krvk & 28056 & 7 & 0.8805 & 0.8606 & 0.5255 & 0.8543 & 0.8781 & 0.7673 & 0.8938\tabularnewline
chess-krvkp & 3196 & 37 & 0.9912 & 0.9900 & 0.9900 & 0.9912 & 0.9862 & 0.9912 & 0.9875\tabularnewline
congressional-voting & 435 & 17 & 0.6147 & 0.6055 & 0.5872 & 0.5963 & 0.5872 & 0.5872 & 0.5780\tabularnewline
conn-bench-sonar-mines-rocks & 208 & 61 & 0.7885 & 0.8269 & 0.8462 & 0.8077 & 0.7115 & 0.8269 & 0.6731\tabularnewline
conn-bench-vowel-deterding & 990 & 12 & 0.9957 & 0.9935 & 0.9784 & 0.9935 & 0.9610 & 0.9524 & 0.9935\tabularnewline
connect-4 & 67557 & 43 & 0.8807 & 0.8831 & 0.8599 & 0.8716 & 0.8729 & 0.8833 & 0.8856\tabularnewline
contrac & 1473 & 10 & 0.5190 & 0.5136 & 0.5054 & 0.5136 & 0.4538 & 0.4755 & 0.4592\tabularnewline
credit-approval & 690 & 16 & 0.8430 & 0.8430 & 0.8547 & 0.8430 & 0.8721 & 0.9070 & 0.8547\tabularnewline
cylinder-bands & 512 & 36 & 0.7266 & 0.7656 & 0.7969 & 0.7734 & 0.7500 & 0.7578 & 0.7578\tabularnewline
dermatology & 366 & 35 & 0.9231 & 0.9121 & 0.9780 & 0.9231 & 0.9341 & 0.9451 & 0.9451\tabularnewline
echocardiogram & 131 & 11 & 0.8182 & 0.8485 & 0.6061 & 0.8485 & 0.8485 & 0.7879 & 0.8182\tabularnewline
ecoli & 336 & 8 & 0.8929 & 0.8333 & 0.8690 & 0.8214 & 0.8214 & 0.8452 & 0.8571\tabularnewline
energy-y1 & 768 & 9 & 0.9583 & 0.9583 & 0.8802 & 0.8177 & 0.8646 & 0.9010 & 0.9479\tabularnewline
energy-y2 & 768 & 9 & 0.9063 & 0.8958 & 0.9010 & 0.8750 & 0.8750 & 0.8906 & 0.8802\tabularnewline
fertility & 100 & 10 & 0.9200 & 0.8800 & 0.8800 & 0.8400 & 0.6800 & 0.6800 & 0.8800\tabularnewline
flags & 194 & 29 & 0.4583 & 0.4583 & 0.4375 & 0.3750 & 0.4167 & 0.4167 & 0.3542\tabularnewline
glass & 214 & 10 & 0.7358 & 0.6038 & 0.6415 & 0.6415 & 0.5849 & 0.6792 & 0.6981\tabularnewline
haberman-survival & 306 & 4 & 0.7368 & 0.7237 & 0.6447 & 0.6842 & 0.7368 & 0.7500 & 0.6842\tabularnewline
hayes-roth & 160 & 4 & 0.6786 & 0.4643 & 0.7857 & 0.7143 & 0.7500 & 0.5714 & 0.8929\tabularnewline
heart-cleveland & 303 & 14 & 0.6184 & 0.6053 & 0.6316 & 0.5658 & 0.5789 & 0.5658 & 0.5789\tabularnewline
heart-hungarian & 294 & 13 & 0.7945 & 0.8356 & 0.7945 & 0.8082 & 0.8493 & 0.7534 & 0.8493\tabularnewline
heart-switzerland & 123 & 13 & 0.3548 & 0.3871 & 0.5806 & 0.3226 & 0.3871 & 0.2581 & 0.5161\tabularnewline
heart-va & 200 & 13 & 0.3600 & 0.2600 & 0.4000 & 0.2600 & 0.2800 & 0.2200 & 0.2400\tabularnewline
hepatitis & 155 & 20 & 0.7692 & 0.7692 & 0.6667 & 0.7692 & 0.8718 & 0.8462 & 0.7436\tabularnewline
hill-valley & 1212 & 101 & 0.5248 & 0.5116 & 0.5000 & 0.5396 & 0.5050 & 0.4934 & 0.5050\tabularnewline
horse-colic & 368 & 26 & 0.8088 & 0.8529 & 0.7794 & 0.8088 & 0.8529 & 0.7059 & 0.7941\tabularnewline
ilpd-indian-liver & 583 & 10 & 0.6986 & 0.6644 & 0.6781 & 0.6712 & 0.5959 & 0.6918 & 0.6986\tabularnewline
\end{tabular}
\end{table}

\begin{table}
\footnotesize
\begin{tabular}{lrrlllllll}
image-segmentation & 2310 & 19 & 0.9114 & 0.9090 & 0.9024 & 0.8919 & 0.8481 & 0.8938 & 0.8838\tabularnewline
ionosphere & 351 & 34 & 0.8864 & 0.9091 & 0.9432 & 0.9545 & 0.9432 & 0.9318 & 0.9432\tabularnewline
iris & 150 & 5 & 0.9730 & 0.9189 & 0.8378 & 0.9730 & 0.9189 & 1.0000 & 0.9730\tabularnewline
led-display & 1000 & 8 & 0.7640 & 0.7200 & 0.7040 & 0.7160 & 0.6280 & 0.6920 & 0.6480\tabularnewline
lenses & 24 & 5 & 0.6667 & 1.0000 & 1.0000 & 0.6667 & 0.8333 & 0.8333 & 0.6667\tabularnewline
letter & 20000 & 17 & 0.9726 & 0.9712 & 0.8984 & 0.9762 & 0.9796 & 0.9580 & 0.9742\tabularnewline
libras & 360 & 91 & 0.7889 & 0.8667 & 0.8222 & 0.7111 & 0.7444 & 0.8000 & 0.8333\tabularnewline
low-res-spect & 531 & 101 & 0.8571 & 0.8496 & 0.9023 & 0.8647 & 0.8571 & 0.8872 & 0.8947\tabularnewline
lung-cancer & 32 & 57 & 0.6250 & 0.3750 & 0.1250 & 0.2500 & 0.5000 & 0.5000 & 0.2500\tabularnewline
lymphography & 148 & 19 & 0.9189 & 0.7297 & 0.7297 & 0.6757 & 0.7568 & 0.7568 & 0.7838\tabularnewline
magic & 19020 & 11 & 0.8692 & 0.8629 & 0.8673 & 0.8723 & 0.8713 & 0.8690 & 0.8620\tabularnewline
mammographic & 961 & 6 & 0.8250 & 0.8083 & 0.7917 & 0.7833 & 0.8167 & 0.8292 & 0.8208\tabularnewline
miniboone & 130064 & 51 & 0.9307 & 0.9250 & 0.9270 & 0.9254 & 0.9262 & 0.9272 & 0.9313\tabularnewline
molec-biol-promoter & 106 & 58 & 0.8462 & 0.7692 & 0.6923 & 0.7692 & 0.7692 & 0.6923 & 0.4615\tabularnewline
molec-biol-splice & 3190 & 61 & 0.9009 & 0.8482 & 0.8833 & 0.8557 & 0.8519 & 0.8494 & 0.8607\tabularnewline
monks-1 & 556 & 7 & 0.7523 & 0.6551 & 0.5833 & 0.7546 & 0.9074 & 0.5000 & 0.7014\tabularnewline
monks-2 & 601 & 7 & 0.5926 & 0.6343 & 0.6389 & 0.6273 & 0.3287 & 0.6644 & 0.5162\tabularnewline
monks-3 & 554 & 7 & 0.6042 & 0.7454 & 0.5880 & 0.5833 & 0.5278 & 0.5231 & 0.6991\tabularnewline
mushroom & 8124 & 22 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.9990 & 0.9995 & 0.9995\tabularnewline
musk-1 & 476 & 167 & 0.8739 & 0.8655 & 0.8992 & 0.8739 & 0.8235 & 0.8992 & 0.8992\tabularnewline
musk-2 & 6598 & 167 & 0.9891 & 0.9945 & 0.9915 & 0.9964 & 0.9982 & 0.9927 & 0.9951\tabularnewline
nursery & 12960 & 9 & 0.9978 & 0.9988 & 1.0000 & 0.9994 & 0.9994 & 0.9966 & 0.9966\tabularnewline
oocytes\_merluccius\_nucleus\_4d & 1022 & 42 & 0.8235 & 0.8196 & 0.7176 & 0.8000 & 0.8078 & 0.8078 & 0.7686\tabularnewline
oocytes\_merluccius\_states\_2f & 1022 & 26 & 0.9529 & 0.9490 & 0.9490 & 0.9373 & 0.9333 & 0.9020 & 0.9412\tabularnewline
oocytes\_trisopterus\_nucleus\_2f & 912 & 26 & 0.7982 & 0.8728 & 0.8289 & 0.7719 & 0.7456 & 0.7939 & 0.8202\tabularnewline
oocytes\_trisopterus\_states\_5b & 912 & 33 & 0.9342 & 0.9430 & 0.9342 & 0.8947 & 0.8947 & 0.9254 & 0.8991\tabularnewline
optical & 5620 & 63 & 0.9711 & 0.9666 & 0.9644 & 0.9627 & 0.9716 & 0.9638 & 0.9755\tabularnewline
ozone & 2536 & 73 & 0.9700 & 0.9732 & 0.9716 & 0.9669 & 0.9669 & 0.9748 & 0.9716\tabularnewline
page-blocks & 5473 & 11 & 0.9583 & 0.9708 & 0.9656 & 0.9605 & 0.9613 & 0.9730 & 0.9708\tabularnewline
parkinsons & 195 & 23 & 0.8980 & 0.9184 & 0.8367 & 0.9184 & 0.8571 & 0.8163 & 0.8571\tabularnewline
pendigits & 10992 & 17 & 0.9706 & 0.9714 & 0.9671 & 0.9708 & 0.9734 & 0.9620 & 0.9657\tabularnewline
pima & 768 & 9 & 0.7552 & 0.7656 & 0.7188 & 0.7135 & 0.7188 & 0.6979 & 0.6927\tabularnewline
pittsburg-bridges-MATERIAL & 106 & 8 & 0.8846 & 0.8462 & 0.9231 & 0.9231 & 0.8846 & 0.8077 & 0.9231\tabularnewline
pittsburg-bridges-REL-L & 103 & 8 & 0.6923 & 0.7692 & 0.6923 & 0.8462 & 0.7692 & 0.6538 & 0.7308\tabularnewline
pittsburg-bridges-SPAN & 92 & 8 & 0.6957 & 0.5217 & 0.5652 & 0.5652 & 0.5652 & 0.6522 & 0.6087\tabularnewline
pittsburg-bridges-T-OR-D & 102 & 8 & 0.8400 & 0.8800 & 0.8800 & 0.8800 & 0.8800 & 0.8800 & 0.8800\tabularnewline
pittsburg-bridges-TYPE & 105 & 8 & 0.6538 & 0.6538 & 0.5385 & 0.6538 & 0.1154 & 0.4615 & 0.6538\tabularnewline
planning & 182 & 13 & 0.6889 & 0.6667 & 0.6000 & 0.7111 & 0.6222 & 0.6444 & 0.6889\tabularnewline
plant-margin & 1600 & 65 & 0.8125 & 0.8125 & 0.8375 & 0.7975 & 0.7600 & 0.8175 & 0.8425\tabularnewline
plant-shape & 1600 & 65 & 0.7275 & 0.6350 & 0.6325 & 0.5150 & 0.2850 & 0.6575 & 0.6775\tabularnewline
plant-texture & 1599 & 65 & 0.8125 & 0.7900 & 0.7900 & 0.8000 & 0.8200 & 0.8175 & 0.8350\tabularnewline
post-operative & 90 & 9 & 0.7273 & 0.7273 & 0.5909 & 0.7273 & 0.5909 & 0.5455 & 0.7727\tabularnewline
primary-tumor & 330 & 18 & 0.5244 & 0.5000 & 0.4512 & 0.3902 & 0.5122 & 0.5000 & 0.4512\tabularnewline
ringnorm & 7400 & 21 & 0.9751 & 0.9843 & 0.9692 & 0.9811 & 0.9843 & 0.9719 & 0.9827\tabularnewline
seeds & 210 & 8 & 0.8846 & 0.8654 & 0.9423 & 0.8654 & 0.8654 & 0.8846 & 0.8846\tabularnewline
semeion & 1593 & 257 & 0.9196 & 0.9296 & 0.9447 & 0.9146 & 0.9372 & 0.9322 & 0.9447\tabularnewline
soybean & 683 & 36 & 0.8511 & 0.8723 & 0.8617 & 0.8670 & 0.8883 & 0.8537 & 0.8484\tabularnewline
spambase & 4601 & 58 & 0.9409 & 0.9461 & 0.9435 & 0.9461 & 0.9426 & 0.9504 & 0.9513\tabularnewline
spect & 265 & 23 & 0.6398 & 0.6183 & 0.6022 & 0.6667 & 0.6344 & 0.6398 & 0.6720\tabularnewline
spectf & 267 & 45 & 0.4973 & 0.6043 & 0.8930 & 0.7005 & 0.2299 & 0.4545 & 0.5561\tabularnewline
statlog-australian-credit & 690 & 15 & 0.5988 & 0.6802 & 0.6802 & 0.6395 & 0.6802 & 0.6860 & 0.6279\tabularnewline
statlog-german-credit & 1000 & 25 & 0.7560 & 0.7280 & 0.7760 & 0.7720 & 0.7520 & 0.7400 & 0.7400\tabularnewline
\end{tabular}
\end{table}

\begin{table}
\footnotesize
\begin{tabular}{lrrlllllll}
statlog-heart & 270 & 14 & 0.9254 & 0.8358 & 0.7761 & 0.8657 & 0.7910 & 0.8657 & 0.7910\tabularnewline
statlog-image & 2310 & 19 & 0.9549 & 0.9757 & 0.9584 & 0.9584 & 0.9671 & 0.9515 & 0.9757\tabularnewline
statlog-landsat & 6435 & 37 & 0.9100 & 0.9075 & 0.9110 & 0.9055 & 0.9040 & 0.8925 & 0.9040\tabularnewline
statlog-shuttle & 58000 & 10 & 0.9990 & 0.9983 & 0.9977 & 0.9992 & 0.9988 & 0.9988 & 0.9987\tabularnewline
statlog-vehicle & 846 & 19 & 0.8009 & 0.8294 & 0.7962 & 0.7583 & 0.7583 & 0.8009 & 0.7915\tabularnewline
steel-plates & 1941 & 28 & 0.7835 & 0.7567 & 0.7608 & 0.7629 & 0.7031 & 0.7856 & 0.7588\tabularnewline
synthetic-control & 600 & 61 & 0.9867 & 0.9800 & 0.9867 & 0.9600 & 0.9733 & 0.9867 & 0.9733\tabularnewline
teaching & 151 & 6 & 0.5000 & 0.6053 & 0.5263 & 0.5526 & 0.5000 & 0.3158 & 0.6316\tabularnewline
thyroid & 7200 & 22 & 0.9816 & 0.9770 & 0.9708 & 0.9799 & 0.9778 & 0.9807 & 0.9752\tabularnewline
tic-tac-toe & 958 & 10 & 0.9665 & 0.9833 & 0.9749 & 0.9623 & 0.9833 & 0.9707 & 0.9791\tabularnewline
titanic & 2201 & 4 & 0.7836 & 0.7909 & 0.7927 & 0.7727 & 0.7800 & 0.7818 & 0.7891\tabularnewline
trains & 10 & 30 & NA & NA & NA & NA & 0.5000 & 0.5000 & 1.0000\tabularnewline
twonorm & 7400 & 21 & 0.9805 & 0.9778 & 0.9708 & 0.9735 & 0.9757 & 0.9730 & 0.9724\tabularnewline
vertebral-column-2clases & 310 & 7 & 0.8312 & 0.8701 & 0.8571 & 0.8312 & 0.8312 & 0.6623 & 0.8442\tabularnewline
vertebral-column-3clases & 310 & 7 & 0.8312 & 0.8052 & 0.7922 & 0.7532 & 0.7792 & 0.7403 & 0.8312\tabularnewline
wall-following & 5456 & 25 & 0.9098 & 0.9076 & 0.9230 & 0.9223 & 0.9333 & 0.9274 & 0.9128\tabularnewline
waveform & 5000 & 22 & 0.8480 & 0.8312 & 0.8320 & 0.8360 & 0.8360 & 0.8376 & 0.8448\tabularnewline
waveform-noise & 5000 & 41 & 0.8608 & 0.8328 & 0.8696 & 0.8584 & 0.8480 & 0.8640 & 0.8504\tabularnewline
wine & 178 & 14 & 0.9773 & 0.9318 & 0.9091 & 0.9773 & 0.9773 & 0.9773 & 0.9773\tabularnewline
wine-quality-red & 1599 & 12 & 0.6300 & 0.6250 & 0.5625 & 0.6150 & 0.5450 & 0.5575 & 0.6100\tabularnewline
wine-quality-white & 4898 & 12 & 0.6373 & 0.6479 & 0.5564 & 0.6307 & 0.5335 & 0.5482 & 0.6544\tabularnewline
yeast & 1484 & 9 & 0.6307 & 0.6173 & 0.6065 & 0.5499 & 0.4906 & 0.5876 & 0.6092\tabularnewline
zoo & 101 & 17 & 0.9200 & 1.0000 & 0.8800 & 1.0000 & 0.7200 & 0.9600 & 0.9600\tabularnewline
\bottomrule
\end{tabular}
\end{table}


\paragraph{Small and large data sets.} 
We assigned each of the 121 UCI data sets into the group ``large datasets'' or 
``small datasets'' if the had more than 1,000 data points or less, respectively. 
We expected that Deep Learning methods require large data sets to competitive to other machine learning methods.
This resulted in 75 small and 46 large data sets. 

\paragraph{Results.}
The results of the method comparison are given in Tables~\ref{tab:uciS1} and \ref{tab:uciS2} for 
small and large data sets, respectively. On small data sets, SVMs performed best followed 
by RandomForest and SNNs. On large data sets, SNNs are the best method followed by SVMs and 
Random Forest.
\index{experiments!UCI!results}






\begin{table}[ht]
\caption[Method comparison on small UCI data sets]{UCI comparison reporting the average rank
of a method on 75 classification task of the 
UCI machine learning repository with  less than 1000 data points. 
For each dataset, the 24 compared methods, 
were ranked by their
accuracy and the ranks were averaged across the tasks. 
The first column gives the method group, the second the 
method, the third  
the average rank , and the last the -value 
of a paired Wilcoxon test whether the difference to the best performing 
method is significant.
SNNs are ranked third having been outperformed by Random Forests and SVMs.  \label{tab:uciS1}}

\centering
\begin{tabular}{llrr}
  \toprule
 methodGroup & method & avg. rank & -value \\ 
  \midrule
  SVM & LibSVM\_weka &  9.3 &\\ 
  RandomForest & RRFglobal\_caret &  9.6 & 2.5e-01 \\ 
  SNN & SNN &  9.6 & 3.8e-01 \\ 
  LMR & SimpleLogistic\_weka &  9.9 & 1.5e-01 \\ 
  NeuralNetworks & lvq\_caret & 10.1 & 1.0e-01 \\ 
  MARS & gcvEarth\_caret & 10.7 & 3.6e-02 \\ 
  MSRAinit & MSRAinit & 11.0 & 4.0e-02 \\ 
  LayerNorm & LayerNorm & 11.3 & 7.2e-02 \\ 
  Highway & Highway & 11.5 & 8.9e-03 \\ 
  DiscriminantAnalysis & mda\_R & 11.8 & 2.6e-03 \\ 
  Boosting & LogitBoost\_weka & 11.9 & 2.4e-02 \\ 
  Bagging & ctreeBag\_R & 12.1 & 1.8e-03 \\ 
  ResNet & ResNet & 12.3 & 3.5e-03 \\ 
  BatchNorm & BatchNorm & 12.6 & 4.9e-04 \\ 
  Rule-based & JRip\_caret & 12.9 & 1.7e-04 \\ 
  WeightNorm & WeightNorm & 13.0 & 8.3e-05 \\ 
  DecisionTree & rpart2\_caret & 13.6 & 7.0e-04 \\ 
  OtherEnsembles & Dagging\_weka & 13.9 & 3.0e-05 \\ 
  Nearest Neighbour & NNge\_weka & 14.0 & 7.7e-04 \\ 
  OtherMethods & pam\_caret & 14.2 & 1.5e-04 \\ 
  PLSR & simpls\_R & 14.3 & 4.6e-05 \\ 
  Bayesian & NaiveBayes\_weka & 14.6 & 1.2e-04 \\ 
  GLM & bayesglm\_caret & 15.0 & 1.6e-06 \\ 
  Stacking & Stacking\_weka & 20.9 & 2.2e-12 \\ 
   \bottomrule
\end{tabular}
\end{table}


\begin{table}[ht]
\caption[Method comparison on large UCI data sets]{UCI comparison reporting the average rank
of a method on 46 classification task of the 
UCI machine learning repository with more than 1000 data points. 
For each dataset, the 24 compared methods, 
were ranked by their
accuracy and the ranks were averaged across the tasks. 
The first column gives the method group, the second the 
method, the third  
the average rank , and the last the -value 
of a paired Wilcoxon test whether the difference to the best performing 
method is significant.
SNNs are ranked first having outperformed diverse machine learning methods and
other FNNs.  \label{tab:uciS2}}

\centering
\begin{tabular}{llrr}
  \toprule
 methodGroup & method & avg. rank & -value \\ 
  \midrule
  SNN & SNN &  5.8 & \\ 
  SVM & LibSVM\_weka &  6.1 & 5.8e-01 \\ 
  RandomForest & RRFglobal\_caret &  6.6 & 2.1e-01 \\ 
  MSRAinit & MSRAinit &  7.1 & 4.5e-03 \\ 
  LayerNorm & LayerNorm &  7.2 & 7.1e-02 \\ 
  Highway & Highway &  7.9 & 1.7e-03 \\ 
  ResNet & ResNet &  8.4 & 1.7e-04 \\ 
  WeightNorm & WeightNorm &  8.7 & 5.5e-04 \\ 
  BatchNorm & BatchNorm &  9.7 & 1.8e-04 \\ 
  MARS & gcvEarth\_caret &  9.9 & 8.2e-05 \\ 
  Boosting & LogitBoost\_weka & 12.1 & 2.2e-07 \\ 
  LMR & SimpleLogistic\_weka & 12.4 & 3.8e-09 \\ 
  Rule-based & JRip\_caret & 12.4 & 9.0e-08 \\ 
  Bagging & ctreeBag\_R & 13.5 & 1.6e-05 \\ 
  DiscriminantAnalysis & mda\_R & 13.9 & 1.4e-10 \\ 
  Nearest Neighbour & NNge\_weka & 14.1 & 1.6e-10 \\ 
  DecisionTree & rpart2\_caret & 15.5 & 2.3e-08 \\ 
  OtherEnsembles & Dagging\_weka & 16.1 & 4.4e-12 \\ 
  NeuralNetworks & lvq\_caret & 16.3 & 1.6e-12 \\ 
  Bayesian & NaiveBayes\_weka & 17.9 & 1.6e-12 \\ 
  OtherMethods & pam\_caret & 18.3 & 2.8e-14 \\ 
  GLM & bayesglm\_caret & 18.7 & 1.5e-11 \\ 
  PLSR & simpls\_R & 19.0 & 3.4e-11 \\ 
  Stacking & Stacking\_weka & 22.5 & 2.8e-14 \\ 
   \bottomrule
\end{tabular}
\end{table}



\clearpage

\subsection{Tox21 challenge data set: Hyperparameters} \index{experiments!Tox21}
For the Tox21 data set, the best hyperparameter setting was determined by a grid-search over all
hyperparameter combinations using the validation set defined by the challenge winners \citep{bib:Mayr2016}.
The hyperparameter space was chosen to be similar to the hyperparameters that were tested by \citet{bib:Mayr2016}.
The early stopping parameter was determined on the smoothed learning curves of 100 epochs 
of the validation set. Smoothing was done using moving averages of 10 consecutive 
values. We tested ``rectangular'' and ``conic'' layers -- rectangular layers have 
constant number of hidden units in each layer, conic layers start with the given 
number of hidden units in the first layer and then decrease the number of hidden units
to the size of the output layer according to the geometric progession.
All methods had the chance to adjust their hyperparameters to the data set at hand. 

\index{experiments!Tox21!hyperparameters}
\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for self-normalizing networks in the Tox21 data set.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{1024, 2048\} \\
  Number of hidden layers & \{2,3,4,6,8,16,32\} \\
  Learning rate & \{0.01, 0.05, 0.1\} \\
  Dropout rate & \{0.05, 0.10\}\\
  Layer form & \{rectangular, conic\} \\
  L2 regularization parameter &  \{0.001,0.0001,0.00001\} \\
\bottomrule
\end{tabular}
\end{center}

\end{table}


\begin{table}[htp]
\begin{center}
\caption[Hyperparameters considered for ReLU networks in the Tox21 data set.]{Hyperparameters considered for ReLU networks with MS initialization in the Tox21 data set.}
\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
   Number of hidden units & \{1024, 2048\} \\
  Number of hidden layers & \{2,3,4,6,8,16,32\} \\
  Learning rate & \{0.01, 0.05, 0.1\} \\
  Dropout rate & \{0.5, 0\}\\
  Layer form & \{rectangular, conic\} \\
  L2 regularization parameter &  \{0.001,0.0001,0.00001\} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}




\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for batch normalized networks in the Tox21 data set.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{1024, 2048\} \\
  Number of hidden layers & \{2, 3, 4, 6, 8, 16, 32\} \\
  Learning rate & \{0.01, 0.05, 0.1\} \\
  Normalization & \{Batchnorm\} \\
  Layer form & \{rectangular, conic\} \\
  L2 regularization parameter &  \{0.001,0.0001,0.00001\} \\

\bottomrule
\end{tabular}
\end{center}

\end{table}



\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for weight normalized networks in the Tox21 data set.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{1024, 2048\} \\
  Number of hidden layers & \{2, 3, 4, 6, 8, 16, 32\} \\
  Learning rate & \{0.01, 0.05, 0.1\} \\
  Normalization & \{Weightnorm\} \\
  Dropout rate & \{0, 0.5\} \\
  Layer form & \{rectangular, conic\} \\
  L2 regularization parameter &  \{0.001,0.0001,0.00001\} \\
\bottomrule
\end{tabular}
\end{center}

\end{table}



\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for layer normalized networks in the Tox21 data set.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{1024, 2048\} \\
  Number of hidden layers & \{2, 3, 4, 6, 8, 16, 32\} \\
  Learning rate & \{0.01, 0.05, 0.1\} \\
  Normalization & \{Layernorm\} \\
  Dropout rate & \{0, 0.5\} \\
  Layer form & \{rectangular, conic\} \\
  L2 regularization parameter &  \{0.001,0.0001,0.00001\} \\

\bottomrule
\end{tabular}
\end{center}

\end{table}



\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for Highway networks in the Tox21 data set.}

\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden layers & \{2, 3, 4, 6, 8, 16, 32\} \\
  Learning rate & \{0.01, 0.05, 0.1\} \\
  Dropout rate & \{0, 0.5\} \\
  L2 regularization parameter &  \{0.001,0.0001,0.00001\} \\
\bottomrule
\end{tabular}
\end{center}

\end{table}




\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for Residual networks in the Tox21 data set.}
\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of blocks & \{2, 3, 4, 6, 8, 16\} \\
  Number of neurons per blocks & \{1024, 2048\} \\
  Block form & \{rectangular, diavolo\} \\
  Bottleneck & \{25\%, 50\%\} \\
  Learning rate & \{0.01, 0.05, 0.1\} \\
  L2 regularization parameter &  \{0.001,0.0001,0.00001\} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\clearpage

\paragraph{Distribution of network inputs.}
We empirically checked the assumption that the distribution of network inputs can 
well be approximated by a normal distribution. To this end, we investigated the 
density of the network inputs before and during learning and found that 
these density are close to normal distributions (see Figure~\ref{fig:clt}).

\begin{figure}
 \includegraphics[width=0.49\columnwidth]{CentralLimitTheorem_Figures/beforeLearning_Tox21.pdf}
 \includegraphics[width=0.49\columnwidth]{CentralLimitTheorem_Figures/afterLearning_Tox21.pdf}
 \caption[Distribution of network inputs in Tox21 SNNs.]{Distribution of network inputs of an SNN for the Tox21 data set. 
  The plots show the distribution of network inputs  of the second layer of a typical Tox21 network.
  The red curves display a kernel density estimator of the network inputs and the black curve is the 
  density of a standard normal distribution. 
  {\bf Left panel:} At initialization time before learning. The distribution of network inputs is close to a standard 
  normal distribution.
  {\bf Right panel:} After 40 epochs of learning. The distributions of network inputs is close to a normal distribution.
  \label{fig:clt}
 }
\end{figure}




\clearpage

\subsection{HTRU2 data set: Hyperparameters}\index{experiments!HTRU2}
For the HTRU2 data set, the best hyperparameter setting was determined by a grid-search over all
hyperparameter combinations using one of the 9 non-testing folds as validation fold in a nested
cross-validation procedure. Concretely, 
if  was the testing fold, we used  as validation fold, and for  we used fold 
for validation. The early stopping parameter was determined on the smoothed learning curves of 100 epochs 
of the validation set. Smoothing was done using moving averages of 10 consecutive 
values. We tested ``rectangular'' and ``conic'' layers -- rectangular layers have 
constant number of hidden units in each layer, conic layers start with the given 
number of hidden units in the first layer and then decrease the number of hidden units
to the size of the output layer according to the geometric progession.
All methods had the chance to adjust their hyperparameters to the data set at hand. \index{experiments!HTRU2!hyperparameters}

\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for self-normalizing networks on the HTRU2 data set.}
\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{256, 512, 1024\} \\
  Number of hidden layers & \{2, 4, 8, 16, 32\} \\
  Learning rate & \{0.1, 0.01, 1\} \\
  Dropout rate & \{ 0, 0.05\}\\
  Layer form & \{rectangular, conic\} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[htp]
\begin{center}
\caption[Hyperparameters considered for ReLU networks on the HTRU2 data set.]{Hyperparameters considered for ReLU networks with Microsoft initialization on the HTRU2 data set.}
\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{256, 512, 1024\} \\
  Number of hidden layers & \{2, 4, 8, 16, 32\} \\
  Learning rate & \{0.1, 0.01, 1\} \\
  Dropout rate & \{0, 0.5\}\\
  Layer form & \{rectangular, conic\} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for BatchNorm networks on the HTRU2 data set.}
\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{256, 512, 1024\} \\
  Number of hidden layers & \{2, 4, 8, 16, 32\} \\
  Learning rate & \{0.1, 0.01, 1\} \\
  Normalization & \{Batchnorm\} \\
  Layer form & \{rectangular, conic\} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for WeightNorm networks on the HTRU2 data set.}
\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{256, 512, 1024\} \\
  Number of hidden layers & \{2, 4, 8, 16, 32\} \\
  Learning rate & \{0.1, 0.01, 1\} \\
  Normalization & \{Weightnorm\} \\
  Layer form & \{rectangular, conic\} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for LayerNorm networks on the HTRU2 data set.}
\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{256, 512, 1024\} \\
  Number of hidden layers & \{2, 4, 8, 16, 32\} \\
  Learning rate & \{0.1, 0.01, 1\} \\
  Normalization & \{Layernorm\} \\
  Layer form & \{rectangular, conic\} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for Highway networks on the HTRU2 data set.}
\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden layers & \{2, 4, 8, 16, 32\} \\
  Learning rate & \{0.1, 0.01, 1\} \\
  Dropout rate & \{0, 0.5\}\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[htp]
\begin{center}
\caption{Hyperparameters considered for Residual networks on the HTRU2 data set.}
\begin{tabular}{ll}
\toprule
Hyperparameter  & Considered values \\ 
\midrule
  Number of hidden units & \{256, 512, 1024\} \\
  Number of residual blocks & \{2, 3, 4, 8, 16\} \\
  Learning rate & \{0.1, 0.01, 1\} \\
  Block form & \{rectangular, diavolo\} \\
  Bottleneck & \{0.25, 0.5\} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\clearpage

\section{Other fixed points}
A similar analysis with corresponding function domains can be performed for other fixed points, for example for  and , which leads
to a SELU activation function with parameters  and . 


\section{Bounds determined by numerical methods}

In this section we report bounds on previously discussed expressions as determined by numerical methods (min and max have been
computed). 






\section{References}
\label{sec:references}
\bibliographystyle{apalike} 
\bibliography{bibliography_fancy} 

\addcontentsline{toc}{section}{List of figures}
\listoffigures
\addcontentsline{toc}{section}{List of tables}
\listoftables

\newpage
\addcontentsline{toc}{section}{Brief index}
\printindex 



\end{document}