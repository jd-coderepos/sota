\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm} 
\usepackage{algpseudocode}

\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

\algnewcommand{\Initialize}[1]{\State \textbf{Initialize:}
	\Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}

\renewcommand{\mathbf}{\boldsymbol}
\newcommand{\LL}{\mathbf{L}}
\renewcommand{\P}{\mathbf{P}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\U}{\mathcal{U}}
\renewcommand{\l}{\mathbf{l}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\rr}{\mathbf{r}}

\algnewcommand\And{\textbf{and}}
\algnewcommand\Or{\textbf{or}}


\algnewcommand{\IIf}[1]{\State\algorithmicif\ #1\ \algorithmicthen}
\algnewcommand{\EndIIf}{\unskip\ \algorithmicend\ \algorithmicif}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\def\cvprPaperID{1446 \vspace{-2mm}} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\cvprfinalcopy 

\renewcommand{\mathbf}{\boldsymbol}
\renewcommand{\L}{\mathbf{L}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\E}{\mathcal{E}}
\renewcommand{\P}{\mathbf{P}}

\newcommand{\p}{\mathbf{p}}
\newcommand{\q}{\mathbf{q}}
\renewcommand{\l}{\mathbf{l}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\e}{\mathbf{e}}

\newcommand{\todo}{{\textbf{\color{red}{TO-DO: }}}}
\newcommand{\note}{{\textbf{\color{red}{NOTE: }}}}
\newcommand{\nop}[1]{}

\begin{document}

\title{Learning to Parse Wireframes in Images of Man-Made Environments \vspace{-4mm}}

\author{Kun Huang, Yifan Wang, Zihan Zhou, Tianjiao Ding, Shenghua Gao, and Yi Ma\\
ShanghaiTech University {\tt\small \{huangkun, wangyf, dingtj, gaoshh\}@shanghaitech.edu.cn}\\
The Pennsylvania State University {\tt\small zzhou@ist.psu.edu}\\
University of California, Berkeley {\tt\small yima@eecs.berkeley.edu}
}


\maketitle


\begin{abstract}
In this paper, we propose a learning-based approach to the task of automatically extracting a ``wireframe'' representation
for images of cluttered man-made environments. The wireframe (see Fig.~\ref{fig:teaser}) contains all salient straight lines
and their junctions of the scene that encode efficiently and accurately large-scale geometry and object shapes. To this end, we have built
a very large new dataset of over 5,000 images with wireframes thoroughly labelled by humans. We have proposed two convolutional neural networks
that are suitable for extracting junctions and lines with large spatial support, respectively. The networks trained on our dataset
have achieved significantly better performance than state-of-the-art methods for junction detection and line segment
detection, respectively. We have conducted extensive experiments to evaluate quantitatively and qualitatively the wireframes
obtained by our method, and have convincingly shown that effectively and efficiently parsing wireframes for images of
man-made environments is a feasible goal within reach. Such wireframes could benefit many important visual tasks such as
feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation. The data and source code are available at \url{https://github.com/huangkuns/wireframe}.
\end{abstract}



\section{Introduction}
How to infer 3D geometric information of a scene from 2D images has been a fundamental problem in computer vision. Conventional approaches to build a 3D model typically rely on detecting, matching, and triangulating local image features (e.g. corners, edges, SIFT features, and patches). One great advantage of working with local features is that the system can be somewhat oblivious to the scene, as long as it contains sufficient distinguishable features. Meanwhile, modern applications of computer vision systems often require an autonomous agent (e.g., a car, a robot, or a UAV) to efficiently and effectively negotiate with a physical space in cluttered {\em man-made} (indoor or outdoor) environments. Such scenarios present significant challenges to the current local-feature based approaches: Man-made environments typically consist of large textureless surfaces (e.g. white walls or the ground); or they may be full of repetitive patterns hence local features are ambiguous to match; and the visual localization system is required to work robustly and accurately over extended routes and sometimes across very large baseline between views. 

\nop{
\begin{figure}[t]
\centering
\includegraphics[width= 0.4\textwidth]{plot/teaser1.png}\vspace{-2mm}
\caption{ \vspace{-5mm}}
\label{fig:teaser}
\end{figure}
}

\begin{figure}
\centering
\includegraphics[height= 1.03in]{plot/fig1/00037060_gt.png}
\includegraphics[height= 1.03in]{plot/fig1/00075427_gt.png}
\includegraphics[height= 1.03in]{plot/fig1/00035326_gt.png}\\
\vspace{0.5mm}\includegraphics[height= 1.03in]{plot/fig1/00037060_4.png}
\includegraphics[height= 1.03in]{plot/fig1/00075427_4.png}
\includegraphics[height= 1.03in]{plot/fig1/00035326_4.png}\\
\caption{{\bf First row:} Examples of typical indoor or outdoor scenes with geometrically meaningful wireframes labelled by humans; {\bf Second row:} Wireframes automatically extracted by our method.}
\vspace{-3mm}
\label{fig:teaser}
\end{figure}

Nevertheless, the human vision system seems capable of effortlessly localizing or navigating among such environments arguably by exploiting larger-scale (global or semi-global) structural features or regularities of the scene. For instance, many works \cite{DelageLN05, LeeHK09, FurukawaCSS09, FlintMR11, RamalingamPJT13} have demonstrated that prior knowledge about the scene such as a Manhattan world could significantly benefit the 3D reconstruction tasks. The Manhattan assumption can often be violated in cluttered man-made environments, but it is rather safe to assume that man-made environments are dominantly piece-wise planar hence rich of visually salient lines (intersection of planes) and junctions (intersection of lines). Conceptually, such junctions or lines could just be a very small ``subset'' among the local corner features (or SIFTs) and edge features detected by conventional methods, but they already encode most information about larger-scale geometry of the scene. For simplicity, we refer to such a set of lines and their intersected junctions collectively as a ``wireframe''.\footnote{In architecture design, a wireframe is often referred to a line drawing of a building or a scene on paper. Interpretation of such line drawings of 3D objects has a long history in computer vision dated back to the '70s and '80s~\cite{Huffman71, Clowes71, Sugihara82, Malik87}.} 


The goal of this work is to {\em study the feasibility of developing a vision system that could efficiently and effectively extract the wireframe of a man-made scene}. Intuitively, we wish such a system could emulate the level of human perception of the scene geometry, even from a single image. To this end, we have collected a database of over 5,000 images of typical indoor and outdoor environments and asked human subjects to manually label out all line segments and junctions that they believe to be important for understanding shape of regular objects or global geometric layout of the scene.\footnote{For simplicity, this work is limited to wireframes consisting of straight lines. But the idea and method obviously apply to wireframes with curves.} Fig. \ref{fig:teaser} (first row) shows some representative examples of labelled wireframes.

In the literature, several methods have been proposed to detect line segments~\cite{von2012lsd} or junctions~\cite{RamalingamPJT13,XiaDG14} in the image, and to further reason about the 3D scene geometry using the detected features~\cite{LeeHK09, FlintMR11, ElqurshE11, RamalingamB13, Yang16}. These methods typically take a bottom-up approach: First, line segments are detected in the image. Then, two or more segments are grouped to form candidate junctions. However, there are several inherent difficulties with this approach. \emph{First}, by enumerating all pairs of line segments, a large number of intersections are created. But only a very small subset of them are true junctions in the image. To retrieve the true junctions, various heuristics as well as RANSAC-based verification techniques have been previously proposed. As result, such methods are often time consuming and even break down when the scene geometry and texture become complex. \emph{Second}, detecting line segments itself is a difficult problem in computer vision. If one fails to detect certain line segments in the image, then it would be impossible for the method to find the associated junctions. \emph{Third}, since all existing methods rely on low-level cues such as image gradients and edge features to detect line segments and junctions, they are generally unable to distinguish junctions and line segments that are of global geometric importance with those produced by local textures or irregular shapes.



In view of the fundamental difficulties of existing methods, we propose a complementary approach to wireframe (junctions and line segments) detection in this paper. Our method does not rely on grouping low-level features such as image gradients and edges. Instead, we directly learn detectors for junctions and lines of large spatial support from the above large-scale dataset of manually labeled junctions and lines. In particular, inspired by the recent success of convolutional neural networks in object detection, we design novel network architectures for junction and line detection, respectively. We then give a simple but effective method to establish incidence relationships among the detected junctions and lines and produce a complete wireframe for the scene. Fig. \ref{fig:teaser} (second row) shows typical results of the proposed method. As one can see, our method is able to detect junctions formed by long line segments with weak gradient while significantly reducing the number of false detections. In addition, as the labelled junctions and line segments are primarily associated with salient, large-scale geometric structures of the scene, the resulting wireframe is geometrically more meaningful, emulating human perception of the scene's geometry.

\noindent{\bf Contributions of this work} include: (i) the establishment of a large dataset for learning-based wireframe detection of man-made environments, and (ii) the development of effective, end-to-end trainable CNNs for detecting geometrically informative junctions and line segments. Comparing with existing methods on junction and line segment detection, our learning-based method has achieved, both quantitatively and qualitatively, superior performances on both tasks, hence convincingly verified the feasibility of wireframe parsing. Furthermore, both junction and line detection achieves almost real-time performance at the testing time, thus is suitable for a wide range of real-world applications such as feature correspondence, 3D reconstruction, vision-based mapping, localization and navigation.
 \subsection*{Related Work}
\vspace{-2mm}
\noindent{\bf Edge and line segment detection.} Much work has been done to extract line segments from images. Existing methods are typically based on perceptual grouping of low-level cues (i.e., image gradients)~\cite{NietoCSG11, von2012lsd, BrownWG15, LiuCGNZT15, LuYLL15}. A key challenge of these local approaches is the choice of some appropriate threshold to discriminate true line segments from false conjunctions. Another line of work extends Hough transform to line segment detection~\cite{MatasGK00, FurukawaS03, XuSK15}. While Hough transform has the ability to accumulate information over the entire image to determine the presence of a line structure, identifying endpoints of the line segment in the image remains a challenge~\cite{Almazan17}. Recently, machine learning based approaches have been shown to produce the state-of-the-art results in generating pixel-wise edge maps~\cite{DollarZ13, XieT15, ManinisPAG16}. But these methods do not attempt to extract straight line segments from the image.

\noindent{\bf Junction detection.} 
Detecting and analyzing junctions in real-world images remains a challenging problem due to a large number of fragmented, spurious, and missing line segments. In the literature, there are typically two ways to tackle this problem. The first group of methods focuses on operators based on local image cues, such as the Harris corner detector~\cite{HarrisS88}. However, local junction detection is known to be difficult, even for humans~\cite{McDermott04}. More recent methods detect junctions by first locating contours (in natural images)~\cite{MaireAFM08} or straight line segments (in man-made environments)~\cite{LeeHK09, RamalingamPJT13, ElqurshE11, XiaDG14} and then grouping them to form junctions. As we discussed before, such bottom-up methods are (i) sensitive to scene complexity, and (ii) vulnerable to imperfect line segment detection results.

\noindent{\bf Line- and junction-based geometric reasoning.} Knowledge about junctions and the associated line structures is known to benefit many real-world 3D vision tasks. From a single image, a series of recent work use these features to recover the 3D layout of the scene~\cite{LeeHK09, RamalingamPJT13, RamalingamB13, Yang16}. Meanwhile, observing that the junctions impose incident constraints on the adjacent line segments, \cite{JainKTS10} devises a method for 3D reconstruction of lines without explicitly matching them across views, whereas \cite{WangFSZLCTQ16} proposes a surface scaffold structure that consists of sets of connected edges to regularize stereo-based methods for building reconstruction. Furthermore, \cite{ElqurshE11} uses line segments and junctions to develop a robust and efficient method for two-view pose estimation, and \cite{Xu2017} systematically studies how knowledge about junctions can affect the complexity and number of solutions to the Perspective-n-Line (PnL) problem.

\noindent{\bf Machine learning and geometry.} There is a large body of work on machine learning based approach to inferring pixel-level geometric properties of the scene, such as the depth~\cite{SaxenaCN08, EigenPF14}, and the surface normal~\cite{FouheyGH13, FouheyGH14}. But few work has been done on detecting mid/high-level geometric primitives with supervised training data. Recently, \cite{HainesC15} proposes a method to recognize planes in a single image, \cite{GuoZH15} uses SVM to classify indoor planes (e.g., walls and floors), and \cite{MallyaL15, RenCLK16a, DasguptaFCS16} train fully convolutional networks (FCNs) to predict ``informative'' edges formed by the pairwise intersections of room faces. However, none of the work aims to detect highly compressive vectorized junctions or line segments in the image, let alone a complete wireframe.
 \section{A New Dataset for Wireframe Detection}

\begin{figure}[t]
\centering
\includegraphics[height= 0.95in]{plot/data/00063799-edge.jpg}
\includegraphics[height= 0.95in]{plot/data/00071079-edge.jpg}
\includegraphics[height= 0.95in]{plot/data/00340153-edge.jpg}\\
\vspace{0.5mm}\includegraphics[height= 0.95in]{plot/data/00063799-junction.jpg}
\includegraphics[height= 0.95in]{plot/data/00071079-junction.jpg}
\includegraphics[height= 0.95in]{plot/data/00340153-junction.jpg}
\caption{Example images of our wireframe dataset, which covers a wide range of man-made scenes with different viewpoints, lighting conditions, and styles. For each image, we show the manually labelled line segments (first row) and the ground truth junctions derived from the line segments (second row).\vspace{-4mm}}
\label{fig:data}
\end{figure}



As part of our learning-based framework to wireframe detection, we have collected 5,462 images of man-made environments. Some examples are shown in Fig.~\ref{fig:data}. The scenes include both indoor environments such as bedroom, living room, and kitchen, and outdoor scenes, such as house and yard. For each image, we manually labelled all the line segments associated with the scene structures. Here, our focus is on the \emph{structural elements} in the image, that is, elements (i.e., line segments) from which meaningful geometric information of the scene can be extracted. As a result, we do not label line segments that are associated with texture (e.g., curtains, tree leaves), irregular or curved objects (e.g., sofa, humans, plants), shadows etc.

With the labelled line segments, ground truth junction locations and their branches can be easily obtained from the intersection or incidence relationships among two or more line segments in the image (Fig.~\ref{fig:data}, second row). Note that, unlike previous works~\cite{RamalingamPJT13, RamalingamB13}, we do not restrict ourselves to \emph{Manhattan junctions}, which are formed by line segments aligned with one of three principal and mutually orthogonal directions in the scene. In fact, many scenes in our dataset do not satisfy the Manhattan world assumption~\cite{CoughlanY03}. For example, the scene depicted in the last column of Fig.~\ref{fig:data} has more than two horizontal directions.

In summary, our annotation in each image includes a set of {\em junction points}  and a set of {\em line segments} . Each junction  is the intersection of several, say , line segments, called its branches. The coordinates of  are denoted as  and its line branches are recorded by their angles . The number  is known as the order of the junction, and the typical ``'', ``'', and ``''-type junctions have orders , , and , respectively. Each line segment is represented by its two end points: . Hence, the {\em wireframe}, denoted as , records all incidence and intersection relationships between junctions in  and lines in . It can be represented by an  matrix  whose -th entry is 1 if  is on , and 0 otherwise. Notice that two line segments are intersected at some junction if and only if the corresponding entry in   is nonzero; and similarly  for connected junctions. 

\section{Wireframe Detection Method}

Recently, deep convolutional neural networks (CNNs) such as~\cite{SermanetEZMFL13, RenHGS15, RedmonF16} have shown impressive performance in object detection tasks. Utilizing the dataset we have, we here design new, end-to-end trainable CNNs for detecting junctions and lines, respectively, and then merge them into a complete wireframe. Fig.~\ref{fig:system} shows the overall architecture of our proposed networks and method. Note that we choose different network architectures for junctions and lines due to the nature of their geometric properties, which we will elaborate below.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\textwidth]{plot/net.pdf}
	\caption{Architecture of the overall system. {\bf Top:} junction detection network. {\bf Bottom:} line detection network.} \vspace{-5mm}
	\label{fig:system}
\end{figure*}

\subsection{Junction Detection}

\subsubsection{Design Rationale} \vspace{-2mm}

\begin{figure}[t]
\centering
\includegraphics[width=0.35\textwidth]{plot/junction-def.pdf}\vspace{-1mm}
\caption{Representation of a junction with three branches. \vspace{-5mm}}
\label{fig:model}
\end{figure}

Our design of the network architecture is guided by several important observations about junctions.

\noindent{\bf Fully convolutional network for global detection.} As we mentioned before, \emph{local} junction detection is a difficult task, which often leads to spurious detections. Therefore, it is important to enable the network to reason globally when making predictions. This motivates us to choose a \emph{fully convolutional network} (FCN), following its recent success in object detection~\cite{RedmonF16}. Unlike other popular object detection techniques that are based on sliding windows~\cite{SermanetEZMFL13} or region proposals~\cite{RenHGS15}, FCN sees the entire image so it implicitly captures the contextual information about the junctions. 
Similar to~\cite{RedmonDGF16, RedmonF16}, our network divides the input image into an  mesh grid, see Fig.~\ref{fig:model} right. If the center of a junction falls into a grid cell, that cell is responsible for detecting it. Thus, each -th cell predicts a confidence score  reflecting how confident the model thinks there exists a junction in that cell. To further locate the junction, each -th cell also predicts its relative displacement  w.r.t. the center of the cell. Note that the behavior of the grid cells resembles the so-called ``anchors'', which serve as regression references in the latest object detection pipelines~\cite{RenHGS15, RedmonDGF16, Liu2016SSD}.

\noindent{\bf Multi-bin representation for junction branches.} Unlike traditional object detection tasks, each cell in our network needs to make different numbers of predictions due to the varying number of branches in a junction. To address this issue, we borrow the idea of spatial grid and propose a new multi-bin representation for the branches, as shown in Fig.~\ref{fig:model} left. We divide the circle (i.e., from 0 to 360 degrees) into  equal bins, with each bin spanning  degrees. Let the center of the -th bin be , we then represent an angle  as , if  fall into the -th bin, where  is the angle residual from the center  in the clockwise direction. Thus, for each bin we regress to this local orientation .

As a result, our network architecture consists of an encoder and two sets of decoders. The encoder takes the whole image as input and produces an  grid of high-level descriptors via a convolutional network. The decoders then use the feature descriptors to make junction predictions.  
Each junction is represented by , where  is the coordinates of the junction center,  is the confidence score for the presence of a junction in the -th grid cell,  is the angle for the branch in the -th bin, and  is the confidence score for the bin. The two sets of decoders predict the junction center and the branches respectively. Each FCN decoder is simply a convolutional layer followed by a regressor, as shown in Fig.~\ref{fig:system} top.

Unlike local junctions, the junctions we aim to detect each is formed by the intersection of two or more long line segments (the branches). While the junction detection does not explicitly rely on edge/line detection as an intermediate step, the knowledge about the associated edges is indirectly learned by enforcing the network to make correct detection of the branches and their directions.

 



\subsubsection{Loss Function}\vspace{-2mm}

To guide the learning process towards the desired output, our loss function consists of four modules. Given a set of ground truth junctions  in an image, we write the loss function as follows:\vspace{-2mm}

In the following, we explain each term in more detail.



\noindent{\bf Junction center confidence loss .} The junction center confidence decoder predicts a score , indicating the probability of a junction for each grid cell. Let  be the ground truth binary class label, we use the cross-entropy loss:


\noindent{\bf Junction center location loss .} The junction center location decoder predicts the relative position  of a junction for each grid cell. We compare the prediction with each ground truth junction using the  loss:\vspace{-2mm}

where  returns the index of the grid cell that the -th ground truth junction falls into, and  is the relative position of the ground truth junction w.r.t. that cell center.



\noindent{\bf Junction branch confidence loss .} The junction branch confidence decoder predicts a score  for each bin in each grid cell, indicating the probability of a junction branch in that bin. Similar to the junction center confidence loss above, we use the cross-entropy loss to compare the predictions with the ground truth labels. The only difference is that we only consider those grid cells in which a ground truth junction is present:\vspace{-2mm}

   
\noindent{\bf Junction branch location loss .} Similar to the junction center location loss, we first decide, for each ground truth junction, the indices of the bins that its branches fall into, denoted as , where  is the order of . Then, we compare our predictions with the ground truth using the  loss:\vspace{-2mm}
\begin{small}

\end{small}




\noindent{\bf Implementation details.} We construct our model to encode an image into  grid of 256-dimensional features. Each cell in the grid is responsible for predicting if a junction is present in the corresponding image region. Our encoder is based on Google's Inception-v2 model~\cite{Szegedy2016RethinkingTI}, which extracts multi-scale features and is well-suited for our problem. For our problem, we only use the early layers in the Inception network, i.e., the first layer to ``Mixed\_3b''. Each decoder consists of a  convolutional layer, followed by a ReLU layer and a regressor. Note that the regressor is conveniently implemented as  convolutional layer, where  is the dimension of the output.

The default values for the weights in Eq.~\eqref{eq:obj} are set to the following: . We choose the number of bins . Our network is trained from scratch with the Stochastic Gradient Descent (SGD) method. The momentum parameter is set to 0.9, and the batch size is set to 1. We follow the standard practice in training deep neural networks to augment the data with image domain operations including mirroring, flipping upside-down, and cropping. The initial learning rate is set to 0.01. We decrease it by a multiple of 0.1 after every 100,000 iterations. Convergence is reached at 300,000 iterations.







\subsection{Line Detection}
Next we design and train a convolutional neural network (Fig.~\ref{fig:system} bottom) to infer line information from RGB images. The network predicts for each pixel  whether it falls on a (long) line . To suppress local edges, short lines, and curves, the predicted value  (of the heat map) at pixel  is set to be the length of the line it belongs to. Given an image with ground truth lines , the target value for  is defined to be:\vspace{-2mm}

where  is the length of the line . Let  be the estimated heatmap value, then the loss function we try to minimize the  loss:\vspace{-2mm}

where the sum is over all pixels of the image. 

\noindent{\bf Implementation details.}  The network architecture is inspired by the Stacked Hourglass network~\cite{NewellYD16}. It takes a  RGB image as input, extracts a  feature maps via three Pyramid Residual Modules (PRM), see Fig. \ref{fig:system} bottom. The feature maps then go through five stacked hourglass modules, followed by two fully convolutional and ReLU layers ( and ) and a  convolutional layer  to output a  pixel-wise heat map. The detailed pyramid residual module and stacked hourglass module can be found in ~\cite{NewellYD16}.

During training, we adopt the Stochastic Gradient Descent (SGD) method. The momentum parameter is set to 0.9, and the batch size is set to 4. Again, we augment the data with image domain operations including mirroring and flipping upside-down. The initial learning rate is set to 0.001. We decrease it by a multiple of 0.1 after 100 epochs. Convergence is reached at 120 epochs.

Notice that we have used an Inception network for junction detection whereas an hourglass network for line detection. In junction detection, we are not interested in the entire support of the line, hence the receptive field of an Inception network is adequate for such tasks. However, we find that for accurately detecting lines with large spatial support, the Stacked Hourglass network works much better due to its large (effective) receptive field. In addition, our experiment also shows that above length-dependent  loss is more effective than the cross-entropy cost often used in learning-based edge detection.  


\subsection{Combine Junctions and Lines for Wireframe}
The final step of the system is to combine the results from junction detection and line detection to generate a wireframe  for the image, which, as mentioned before, consists of a set of junction points  connected by a set of line segments . 

Specifically, given a set of detected junctions  and a line heat map , we first apply a threshold  to convert  into a binary map . Then, we construct the wireframe based on the following rules and procedure:\vspace{-1mm}
\begin{enumerate}
\item The set  is initialized with the output from the junction detector. A pair of detected junctions  and  are connected by a line segment  if they are on (or close to be on) each other's branches, and we add this segment  to . If there are multiple detected junctions on the same branch of a junction point , we only keep the shortest segment to avoid overlap.\footnote{Hence we are less interested in detecting a straight line with the longest possible support, instead, we are interested in its incidence relationship with other lines and junctions.} \vspace{-1mm}
\item For any branch of a junction  that is not connected to any other junction, we attempt to recover additional line segment using . We first find the farthest line pixel  (pixel  is a line pixel if ) that is also on the ray starting at  along the branch. Then, we find all the intersection points  of line segment  with existing segments in . Let  and , we calculate the line support ratio , for each segment. Here,  is defined as the ratio of the number of line pixels to the total length of the segment. If  is above a threshold, say , we add the segment to  and its endpoints to .
\end{enumerate}
Notice that both the sets  and  may have {\em two} sources of candidates. For the junction set , besides those directly detected by the junction detection, the line segments could also produce new intersections or endpoints that were missed by the junction detection. For the line segment set ,  it could come from branches of the detected junctions and the line detection. 

We leave more detailed description of the algorithm to the supplementary material. Of course, there could be more advanced ways to merge the detected junctions and line heat map which we will explore in future work. Nevertheless, from our experiments (see next section), we find that the results from junction detection and line detection are rather complementary to each other and the above simple procedure already produces rather decent results. 
 \section{Experiments}

In this section, we conduct extensive experiments to evaluate the quality of junctions and final wireframes generated by our method, and compare it to the state-of-the-art. All experiments are conducted on one NVIDIA Titan X GPU device. In testing phase, our method runs at about two frames per second, thus our method is potentially suitable for applications which require real-time processing.

\subsection{Datasets and Evaluation Metrics}

For performance evaluation, we split our wireframe dataset into a training set and a testing set. Among the 5,462 images in the dataset, 5,000 images are randomly selected for training and validation, and the remaining 462 images are used for testing. For junction detection (Section~\ref{sec:exp:junction}), we compare the junctions detected by any method with the ground truth junctions (Fig.~\ref{fig:data}, second row). For wireframe construction (Section~\ref{sec:exp:wireframe}), we compare the line segments detected by any method with the ground truth line segments labeled by human subjects (Fig.~\ref{fig:data}, first row).

For both junction detection and wireframe construction experiments, all methods are the evaluated quantitatively by means of the \emph{recall} and \emph{precision} as described in~\cite{MartinFM04, MaireAFM08, XiaDG14}. In the context of junction detection, recall is the fraction of true junctions that are detected, whereas precision is the fraction of junction detections that are indeed true positives. In the context of wireframe construction, recall is the fraction of line segment pixels that are detected, whereas precision is the fraction of line segment pixels that are indeed true positives.

Specifically, let  denote the set of ground truth junctions (or line segment pixels), and  denote the set of junctions (or line segment pixels) detected by any method, the precision and recall are defined as follows:\vspace{-2mm}

Note that, following the protocols of previous work~\cite{MartinFM04, MaireAFM08, XiaDG14},  the particular measures of recall and precision allow for some small tolerance in the localization of the junctions (or line segment pixels). In this paper, we set the tolerance to be 0.01 of the image diagonal.




\subsection{Junction Detection Comparison}
\label{sec:exp:junction}
We compare our junction detection method with two recent methods, namely Manhattan junction detection (MJ)~\cite{RamalingamPJT13} and \emph{a contrario} junction detection (ACJ)~\cite{XiaDG14}.

\noindent{\bf MJ~\cite{RamalingamPJT13}}: This method detects Manhattan junctions formed by line segments in three principal orthogonal directions using a simple voting-based scheme. As the authors did not release their code, we use our own implementation of the method. Line segments are first detected using LSD~\cite{von2012lsd}, and then clustered using J-Linkage~\cite{Tardif09} to obtain the vanishing points. Note that this method only applies to scenes that satisfy the Manhattan world assumption. For fair comparison, we only keep the images in which three principal vanishing points are detected. An important parameter in our implementation is the maximum distance  between a line segment and a point  for that line segment to vote for . We vary the value  pixels.

\noindent{\bf ACJ~\cite{XiaDG14}}: This method relies on statistical modeling of image gradients and an \emph{a contrario} approach to detect junctions. Specifically, meaningful junctions are detected as those which are very unlikely under a null hypothesis , which is defined based on the distribution of gradients of arbitrary natural images. In the method, each candidate junction is associated with a strength value depending on the image gradients around it. Then, the candidate junction is validated with a threshold, which is derived by controlling the number of false detections, , in an image following . For the experiments, we use the implementation provided by the authors of~\cite{XiaDG14} and vary the value .

\begin{figure}[t]
    \centering
    \includegraphics[width = 1.9in]{plot/pr.pdf}
    \caption{The precision-recall curves of different junction detection methods on our test dataset.}\vspace{-3mm}
    \label{fig:pr}
\end{figure}

\noindent{\bf Performance comparison.} Fig.~\ref{fig:pr} shows the precision-recall curves of all methods on our new dataset. For our method, we vary the junction confidence threshold  from 0.1 to 0.9. As one can see, our method outperforms the other methods by a large margin. Fig.~\ref{fig:results} compares qualitatively the results of all methods on our test data. Compared to the other two methods, MJ tends to miss important junctions due to the imperfect line segment detection results. Moreover, since MJ relies on local image features, it noticeably produces quite a few repetitive detections around some junctions. By directly modeling the image gradients, ACJ is able to find most junctions on the scene structures. However, as a local method, ACJ makes a lot of false predictions on textured regions (e.g., floor of the first, sky of the fourth image). In contrast, our method is able to detect most junctions intersected by salient lines, while minimizing the number of false detections. This is no surprise because our supervised framework implicitly encodes high-level structural and semantic information of the scene as it learns from the labeled data provided by humans.


\subsection{Line Segment Detection Comparison}
\label{sec:exp:wireframe}
In this section, we compare the wireframe results of our method with two state-of-the-art line segment detection methods, namely the Line Segment Detector (LSD) ~\cite{von2012lsd} and the Markov Chain Marginal Line Segment Detector (MCMLSD)~\cite{Almazan17}. We test and compare with these methods on both our new dataset and the York Urban dataset \cite{Denis2008EfficientEM} used in the work of MCMLSD \cite{Almazan17}.

\noindent{\bf LSD~\cite{von2012lsd}}: This method is a linear-time line segment detector that requires no parameter tuning. It also uses an \emph{a contrario} approach to control the number of false detections. In this experiment, we use the code released by the authors\footnote{http://www.ipol.im/pub/art/2012/gjmr-lsd/} and vary the threshold for (NFA) (NFA is the number of false alarms) in .



\noindent{\bf MCMLSD~\cite{Almazan17}}: This method proposes a two-stage algorithm to find line segments. In the first stage, it uses the probabilistic Hough transform~\cite{matas2000robust} to identify globally optimal lines. In the second stage, it searches each of these lines for their supports (segments) in the image, which can be modeled as labeling hidden states in a linear Markov chain. In this experiment, we use the code released by the authors.\footnote{http://www.elderlab.yorku.ca/resources/} Be aware that authors of \cite{Almazan17} have introduced a different metric than ours that tends to penalize over-segmentation. Hence our metric can be unfair to their method. Nevertheless, our metric is more appropriate for wireframe detection as we prefer to interpret a long line as several segments between junctions if it intersects with other lines.

\begin{figure}[t]
    \centering
    \includegraphics[width = 1.62in]{plot/LinePR_ours.pdf}
    \includegraphics[width = 1.62in]{plot/LinePR_york.pdf}
    \caption{The precision-recall curves of different line segment detection methods. {\bf Left:} on our test dataset. {\bf Right:} on the York Urban dataset \cite{Denis2008EfficientEM}.} \vspace{-3mm}
    \label{fig:LinePR-ours} \label{fig:LinePR-york}
\end{figure}

\begin{figure*}[t!]
\centering


\includegraphics[height=0.9in]{plot/results/00066618-cvpr13.jpg}
\includegraphics[height=0.9in]{plot/results/00072768-cvpr13.jpg}
\includegraphics[height=0.9in]{plot/results/00036977-cvpr13.jpg}
\includegraphics[height=0.9in]{plot/results/00076835-cvpr13.jpg}
\includegraphics[height=0.9in]{plot/results/00194897-cvpr13.jpg}
\\
\includegraphics[height=0.9in]{plot/results/00066618-acj.jpg}
\includegraphics[height=0.9in]{plot/results/00072768-acj.jpg}
\includegraphics[height=0.9in]{plot/results/00036977-acj.jpg}
\includegraphics[height=0.9in]{plot/results/00076835-acj.jpg}
\includegraphics[height=0.9in]{plot/results/00194897-acj.jpg}
\\
\includegraphics[height=0.9in]{plot/results/00066618-binNet.jpg}
\includegraphics[height=0.9in]{plot/results/00072768-binNet.jpg}
\includegraphics[height=0.9in]{plot/results/00036977-binNet.jpg}
\includegraphics[height=0.9in]{plot/results/00076835-binNet.jpg}
\includegraphics[height=0.9in]{plot/results/00194897-binNet.jpg}
\\
\caption{Junction detection results. {\bf First row:} MJ (). {\bf Second row:} ACJ (). {\bf Third row:} Our method (). } \vspace{-1mm}
\label{fig:results}
\end{figure*}


\begin{figure*}[t!]
\centering
\includegraphics[height=0.88in]{plot/fig8/00031608_lsd.jpg}
\includegraphics[height=0.88in]{plot/fig8/00036242_lsd.jpg}
\includegraphics[height=0.88in]{plot/fig8/00055416_lsd.jpg}
\includegraphics[height=0.88in]{plot/fig8/00064984_lsd.jpg}
\includegraphics[height=0.88in]{plot/fig8/00075103_lsd.jpg}
\\
\includegraphics[height=0.88in]{plot/fig8/00031608_mcm.jpg}
\includegraphics[height=0.88in]{plot/fig8/00036242_mcm.jpg}
\includegraphics[height=0.88in]{plot/fig8/00055416_mcm.jpg}
\includegraphics[height=0.88in]{plot/fig8/00064984_mcm.jpg}
\includegraphics[height=0.88in]{plot/fig8/00075103_mcm.jpg}
\\
\includegraphics[height=0.88in]{plot/fig8/00031608_4.jpg}
\includegraphics[height=0.88in]{plot/fig8/00036242_4.jpg}
\includegraphics[height=0.88in]{plot/fig8/00055416_4.jpg}
\includegraphics[height=0.88in]{plot/fig8/00064984_4.jpg}
\includegraphics[height=0.88in]{plot/fig8/00075103_4.jpg}
\\
\includegraphics[height=0.88in]{plot/fig8/00031608_gt.jpg}
\includegraphics[height=0.88in]{plot/fig8/00036242_gt.jpg}
\includegraphics[height=0.88in]{plot/fig8/00055416_gt.jpg}
\includegraphics[height=0.88in]{plot/fig8/00064984_gt.jpg}
\includegraphics[height=0.88in]{plot/fig8/00075103_gt.jpg}
\\
\caption{Line/wireframe detection results. {\bf First row:} LSD (-(NFA)  ). {\bf Second row:} MCMLSD (confidence top 100). {\bf Third row:} Our method (line heat map ). {\bf Fourth row:} Ground truth.}\vspace{-4mm}
\label{fig:line-results}
\end{figure*}


\noindent{\bf Performance comparison.} Fig.~\ref{fig:LinePR-ours} shows the precision-recall curves of all methods on our dataset and the York Urban dataset, respectively. As one can see, our method outperforms the other methods by a significant margin on our dataset. The margin on the York Urban dataset is decent but not so large. According to \cite{Almazan17}, the labeling of the York Urban dataset is not as complete for salient line segments, hence it is not entirely suitable for the wireframe detection task here.  Fig.~\ref{fig:line-results} compares qualitatively the results of all methods on our test data. Since the other two methods rely on local measurements, they tend to produce many line segments on textured regions (e.g. curtain of the first image) which do not correspond to scene structures.
 \section{Conclusion}
This paper has demonstrated the feasibility of parsing wireframes in images of man-made environments. The proposed method is based on combining junctions and lines detected from respective neural networks trained on a new large-scale dataset. Both quantitatively and qualitatively, the results of our method approximately emulate those labelled by humans. The junctions and line segments in a wireframe and their incidence relationships encode rich and accurate large-scale geometry of the scene and shape of regular objects therein, in a highly compressive and efficient manner. Hence results of this work can significantly facilitate and benefit visual tasks such as feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation in man-made environments. 

\noindent\textbf{Acknowledgement:} The project is supported by NSFC (NO. 61502304) and Program of Shanghai Subject Chief Scientist (A type) (No. 15XD1502900). 
\newpage
{\small
\bibliographystyle{ieee}
\bibliography{cvpr18-parsing}
}

\clearpage

\section*{Supplementary}
\section*{A. Wireframe Construction Algorithm Detail}

Given an image, our wireframe construction algorithm takes a set of junctions , , and a line heat map  as input. Note that for the junctions and their branches predicted by our network, we only keep those with confidence scores higher than certain thresholds  and , respectively. As a pre-processing step, we further adopt a strategy similar to non-maximum suppression to remove duplicate detections. 

\begin{algorithm}
	\caption{Wireframe Construction}
	\label{alg:construct}
	\begin{algorithmic}[1]
		\Require Junctions , , and a line heat map 		\Ensure Wireframe  consisting of a set of junction points  connected by a set of line segments 
		\State Initialize , 
		\State Binarize  with threshold  into 



		\For{}
		\State , , 
		
		\For{}
		\State 	
		




			\If{  \textbf{and}  on  \textbf{and}  on  }
				\If{}
					\State , 		
				\EndIf
			\EndIf
		\EndFor
		\If{}
			\State 
		\EndIf
		
		\EndFor
		
		\ForAll{}
				\If{ \textbf{and} }
				\State , 
				\State , 
				\EndIf
				\EndFor
		
		\ForAll{ not matched to another ray}
		\State Find the intersection of  and image boundary 
		\If{}
			\State , 
		
		\Else
			\State Find the farthest point  along  on 
			\State Find all intersections  of  with segments in 
				\State , 		
				\For{}
					\If{}
						\State 
						\State 
					\EndIf		
				\EndFor
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

Our wireframe construction algorithm is presented in Alg.~\ref{alg:construct}. In the algorithm, we first apply a threshold  to convert the line heat map  into a binary map  (line 2). Note that this threshold  is varied to obtain the precision-recall curve in our experiments on wireframe construction. The algorithm then proceeds as follows:


{\em First}, we connect all pairs of junctions which are aligned with each other's branch directions (lines 3-22).
Let  represent the ray starting at  along its -th branch. We collect all possible rays as , and use  to map the -th ray in  to its junction index  and branch index . Then, for the rays in , we use , , to record the indices of the corresponding ray/branch of the closest opposite junction. Specifically, , we set  to 1 if and only if (i)  is the on the ray  and  is on the ray , where , , and (ii) the distance between  and  is the shortest among all such aligned pairs (lines 5-15). Then, we consider two rays are matched if  and add the corresponding junctions and line segments to  and , respectively (lines 17-21).

{\em Second}, for any ray  which fails to find a matching ray using the above procedure, we attempt to recover additional line segments using the line support  (lines 23-38). We consider the following cases:
\begin{itemize}
	\item[(a)] If the distance between  and , the intersection of  and the image boundary, is smaller than certain threshold (say  where  is the maximum of image width and height), we add  and the connecting line segment to  and , respectively (lines 24-26).
	\item[(b)] For a ray exceeding the length threshold in (a), we first find the farthest line pixel  along the ray on . Then, we find all the intersection points  of line segment  with existing segments in  (lines 28-29). Let  and , we calculate the line support ratio , for each segment. Here,  is defined as the ratio of line pixels (pixel  is a line pixel if ) to the total length of the segment. If  is above a threshold, say , we add the segment to  and its endpoints to  (lines 30-36).	 
	\end{itemize}

\nop{
	\begin{figure}[t]
		\centering
		\begin{tabular}{cc}
			\hspace{-3mm}    \includegraphics[height = 1.25in]{supplementary/plot/r_max.pdf} &
			\hspace{-3mm}    \includegraphics[height = 1.25in]{supplementary/plot/layer.pdf} \\
			(a)  & (b) Network depth
		\end{tabular}
		\caption{Experiments on junction detection network parameters.}
		\label{fig:para}
	\end{figure}
}

\begin{figure}[t]
	\centering
	\begin{tabular}{c}
		\includegraphics[height = 1.8in]{supplementary/plot/r_max.pdf} \\
		(a) \\
		\includegraphics[height = 1.8in]{supplementary/plot/layer.pdf} \\
		(b) Network depth
	\end{tabular}
	\caption{Experiment on junction detection network parameters.}
	\label{fig:para}
\end{figure}



\section*{B. Additional Experiments}

\subsection*{B.1. Experiment on Junction Detection Network Parameters}

In this section, we examine the choices of two important hyper-parameters in our junction detection network.

\medskip
\noindent{\bf Effect of balancing positive and negative samples.} In this experiment, we vary the value , which controls the maximum ratio between negative and positive samples at each iteration. Note that setting  is equivalent to using all grid cells during training. We can observe in Figure~\ref{fig:para}(a) that the precision-recall curves largely overlap. But as  increases, the curve shifts toward the high-precision-low-recall regime, and vice versa. For example, when , the precision and recall at  are 0.19 and 0.94, respectively. And when , the precision and recall at  are 0.70 and 0.44, respectively. Note that this has an important implication in practice, as human annotators tend to miss true junctions much more often than labelling wrong junctions. Empirically, we have found that  yields more satisfactory results.

\medskip
\noindent{\bf Going deeper.} It is also interesting to investigate how the network depth of the encoder affects the performance. In this experiment, we compared two different choices based on Google Inception-v2, namely the first layer to ``Mixed\_3b'', and the first layer to ``Mixed\_4b''. Note that the latter has a larger depth and receptive field, at the cost of spatial resolution (). As one can see in Figure~\ref{fig:para}(b), increasing the depth (i.e., predicting at the ``coarser'' level) results in higher precision but lower recall. This suggests possibilities to further improve the performance of our method using a ``skip-net'' architecture, that is, combining predictions at multiple levels. We leave this for future work.





\subsection*{B.2. Experiment on Line Segment Detection}

In this experiment, we study the possibility of extracting line segments \emph{directly} from the pixel-wise line heat map predicted by our network (i.e., without using junctions). To this end, we simply perform a probabilistic hough transform~\cite{matas2000robust} on the line heat map to generate line segments. We compare the results with LSD, MCMLSD, and our full wireframe construction method. 

\begin{figure}[t]
	\centering
	\begin{tabular}{c}
		\includegraphics[height = 1.8in]{supplementary/plot/hough_result_ours.pdf} \\
		(a) Our test dataset \\
		\includegraphics[height = 1.8in]{supplementary/plot/hough_result_york.pdf} \\
		(b) York Urban dataset
	\end{tabular}
	\caption{Experiment on line segment detection.}
	\label{fig:hough}
\end{figure}

Figure~\ref{fig:hough} shows the precision-recall curves of all methods. We make the following observations on the results: {\em First}, the performance of our ``Heatmap + Hough'' approach is comparable to that of the state-of-the-art line segment detection method MCMLSD, verifying the effectiveness of the our line detection network. {\em Second}, by combining the predicted junctions with the line heat map, our full wireframe construction method performs significantly better than using the line heat map alone. This further illustrates the importance of junction detection in parsing the wireframe: By detecting the ``endpoints'' of the line segments, we effectively overcome the difficulties faced by traditional line segment detection methods, including the false detection problem and the inaccurate endpoint problem.



 
 
\subsection*{B.3. Additional Results on Junction Detection}
In Figure~\ref{fig:junc-results}, we show additional junction detection results obtained by all methods. One can see that our method is able to detect most junctions and their branches in the image, achieving superior performance over existing methods.

From Figure~\ref{fig:junc-results} we can also observe some limitations of our method. Specifically, there are occasionally repeated detections in our result. This may be caused by junctions located at the boundary of two adjacent grid cells used in our junction detection network. Similarly, the use of grid could also lead to missed detection if two junctions are very close to each other. But we note that such cases are rather uncommon in practice and have very small effect on the overall scene structure estimation.

\begin{figure}[t]
	\centering
\includegraphics[height=0.97in]{supplementary/plot/fail/00374962_ours.jpg}
\includegraphics[height=0.97in]{supplementary/plot/fail/00490396_ours.jpg}
\includegraphics[height=0.97in]{supplementary/plot/fail/00492427_ours.jpg}
\\
\includegraphics[height=0.97in]{supplementary/plot/fail/00374962_gt.jpg}
\includegraphics[height=0.97in]{supplementary/plot/fail/00490396_gt.jpg}
\includegraphics[height=0.97in]{supplementary/plot/fail/00492427_gt.jpg}
	\caption{Failure cases on our test dataset. {\bf First row:} Our method. {\bf Second row:} Ground truth.}
	\label{fig:fail}
\end{figure}

\begin{figure*}[t]
\centering


\includegraphics[height=0.9in]{supplementary/plot/junc-results/00051689-cvpr13.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00302938-cvpr13.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00067223-cvpr13.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00074809-cvpr13.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00104267-cvpr13.jpg}
\\
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00051689-acj.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00302938-acj.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00067223-acj.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00074809-acj.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00104267-acj.jpg}
\\
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00051689-binNet.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00302938-binNet.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00067223-binNet.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00074809-binNet.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00104267-binNet.jpg}
\\

\includegraphics[height=0.9in]{supplementary/plot/junc-results/00279189-cvpr13.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00064993-cvpr13.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00191635-cvpr13.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00192240-cvpr13.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00275379-cvpr13.jpg}
\\
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00279189-acj.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00064993-acj.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00191635-acj.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00192240-acj.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00275379-acj.jpg}
\\
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00279189-binNet.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00064993-binNet.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00191635-binNet.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00192240-binNet.jpg}
\includegraphics[height=0.9in]{supplementary/plot/junc-results/00275379-binNet.jpg}
\\

\includegraphics[height=1.02in]{supplementary/plot/junc-results/00071113-cvpr13.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00076567-cvpr13.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00190843-cvpr13.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00225154-cvpr13.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00287970-cvpr13.jpg}
\\
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00071113-acj.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00076567-acj.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00190843-acj.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00225154-acj.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00287970-acj.jpg}
\\
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00071113-binNet.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00076567-binNet.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00190843-binNet.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00225154-binNet.jpg}
\includegraphics[height=1.02in]{supplementary/plot/junc-results/00287970-binNet.jpg}
\\
\caption{Junction detection results. {\bf First row:} MJ (). {\bf Second row:} ACJ (). {\bf Third row:} Our method (). }
\label{fig:junc-results}
\end{figure*}


\subsection*{B.4. Additional Results on Wireframe Construction}
In Figure~\ref{fig:line-results-suppl}, we show additional wireframe detection results obtained by all methods. Our method outperforms other two in most areas and produces much cleaner results as we focus on long line segments and exploit their relations (junctions). Therefore, the resulted wireframes are potentially more suitable for 3D reconstruction tasks. 

In Figure~\ref{fig:fail}, we further show some failure cases of our method. One challenging case corresponds to structures with relatively small scale and weak image gradients (e.g., the stairs in the first image). Also, our method sometimes has difficulty in image region of repetitive patterns (e.g., the handrails in the second image and the brick wall in the third image), generating fragment, incomplete results. This suggests opportunities for further improvement by explicitly harnessing such geometric structure in our wireframe construction. 

\begin{figure*}[t]
\centering
\includegraphics[height=1.00in]{supplementary/plot/line-results/00037075_lsd.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00070999_lsd.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00075395_lsd.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00194226_lsd.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00328916_lsd.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00491318_lsd.jpg}
\\
\includegraphics[height=1.00in]{supplementary/plot/line-results/00037075_mcm.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00070999_mcm.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00075395_mcm.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00194226_mcm.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00328916_mcm.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00491318_mcm.jpg}
\\
\includegraphics[height=1.00in]{supplementary/plot/line-results/00037075_ours.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00070999_ours.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00075395_ours.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00194226_ours.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00328916_ours.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00491318_ours.jpg}
\\
\includegraphics[height=1.00in]{supplementary/plot/line-results/00037075_gt.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00070999_gt.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00075395_gt.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00194226_gt.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00328916_gt.jpg}
\includegraphics[height=1.00in]{supplementary/plot/line-results/00491318_gt.jpg}
\\

\includegraphics[height=0.9in]{supplementary/plot/line-results/00060699_lsd.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00071764_lsd.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00077933_lsd.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00197089_lsd.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00436622_lsd.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00521081_lsd.jpg}
\\
\includegraphics[height=0.9in]{supplementary/plot/line-results/00060699_mcm.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00071764_mcm.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00077933_mcm.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00197089_mcm.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00436622_mcm.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00521081_mcm.jpg}
\\
\includegraphics[height=0.9in]{supplementary/plot/line-results/00060699_ours.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00071764_ours.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00077933_ours.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00197089_ours.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00436622_ours.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00521081_ours.jpg}
\\
\includegraphics[height=0.9in]{supplementary/plot/line-results/00060699_gt.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00071764_gt.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00077933_gt.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00197089_gt.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00436622_gt.jpg}
\includegraphics[height=0.9in]{supplementary/plot/line-results/00521081_gt.jpg}
\\
\caption{Line/wireframe detection results. {\bf First row:} LSD (-(NFA)  ). {\bf Second row:} MCMLSD (confidence top 100). {\bf Third row:} Our method (line heat map ). {\bf Fourth row:} Ground truth.}
\label{fig:line-results-suppl}
\end{figure*} 
\end{document}
