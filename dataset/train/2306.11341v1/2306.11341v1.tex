\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{fancyhdr}       \usepackage{graphicx}       \graphicspath{{media/}}     \usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\etal}{\emph{et al. }}

\usepackage{subcaption}

\newcommand{\word}[1]{\texttt{#1}}
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

\fancyhead[LO]{W.F. Hendria}




  
\title{MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in Indonesian              





}

\author{
  Willy Fitra Hendria \\
  Independent Researcher \\
  Seoul, South Korea\\
  \texttt{willyfitrahendria@gmail.com} \\
\\
}


\begin{document}
\maketitle

\begin{abstract}
Multimodal learning on video and text data has been receiving growing attention from many researchers in various research tasks, including text-to-video retrieval, video-to-text retrieval, and video captioning. Although many algorithms have been proposed for those challenging tasks, most of them are developed on English language datasets. Despite Indonesian being one of the most spoken languages in the world, the research progress on the multimodal video-text with Indonesian sentences is still under-explored, likely due to the absence of the public benchmark dataset. To address this issue, we construct the first public Indonesian video-text dataset by translating English sentences from the MSVD dataset to Indonesian sentences. Using our dataset, we then train neural network models which were developed for the English video-text dataset on three tasks, i.e., text-to-video retrieval, video-to-text retrieval, and video captioning. The recent neural network-based approaches to video-text tasks often utilized a feature extractor that is primarily pretrained on an English vision-language dataset. Since the availability of the pretraining resources with Indonesian sentences is relatively limited, the applicability of those approaches to our dataset is still questionable. To overcome the lack of pretraining resources, we apply cross-lingual transfer learning by utilizing the feature extractors pretrained on the English dataset, and we then fine-tune the models on our Indonesian dataset. Our experimental results show that this approach can help to improve the performance for the three tasks on all metrics. Finally, we discuss potential future works using our dataset, inspiring further research in the Indonesian multimodal video-text tasks. We believe that our dataset and our experimental results could provide valuable contributions to the community. Our dataset is available on GitHub\footnote{\label{fn:github}\href{https://github.com/willyfh/msvd-indonesian}{https://github.com/willyfh/msvd-indonesian}}.
\end{abstract}


\keywords{video-text dataset \and video captioning \and video-to-text retrieval \and text-to-video retrieval \and neural network}




\section{Introduction}


Multimodal machine learning \cite{10.1109/TPAMI.2018.2798607} aims to construct machine learning models that have the ability to learn from multiple modalities, such as natural language, visual, and audio. The recent attention on multimodal machine learning has led to new research problems connecting video and text modalities, such as text-to-video retrieval \cite{DBLP:journals/corr/TorabiTS16}, video-to-text retrieval \cite{PerezMartin2021ACR}, and video captioning \cite{venugopalan15iccv}. While the text-to-video retrieval task aims to retrieve a relevant video given a text query as an input, the video-to-text retrieval task aims to retrieve a relevant text given a video query as an input. For the video captioning task, the objective is to generate a natural language sentence describing the input video. In order to develop a machine learning model for the multimodal video-text tasks, a video-text dataset consisting pair of videos and texts is needed.

The majority of video-text datasets \cite{chen:acl11, xu2016msr-vtt, krishna2017dense} are constructed with English sentences. Only a few of the datasets are publicly available in a non-English language, e.g., Chinese \cite{Wang_2019_ICCV}, Turkish \cite{8806555}, Italian \cite{IJCOL:scaiella_et_al:2019}, and Hindi \cite{10.1007/s00530-021-00816-3}. Despite the Indonesian language is one of the most widely used languages in the world, to the best of our knowledge, there is no public video-text dataset available for the language at the time of writing this paper. Consequently, the research progress of the multimodal video-text for the Indonesian language is still under-explored. In response, we construct the first public Indonesian video-text dataset, by translating English sentences from Microsoft Video Description (MSVD) dataset \cite{chen:acl11} to Indonesian sentences. Our MSVD-Indonesian dataset consists of 1970 videos with roughly 80k sentences in total, which is the same as the MSVD dataset. Using our novel dataset, we then build neural network models which were developed and evaluated on the English video-text dataset on three video-text tasks, i.e., text-to-video retrieval, video-to-text retrieval, and video captioning. Our dataset and the experimental results may well be groundbreaking for the research of multimodal video-text in the Indonesian language on the three tasks.

Employing neural networks algorithms along with the pretrained feature extractors is one of the popular approaches in video-text tasks \cite{DBLP:conf/ecai/Chen0020a, 10.1145/3474085.3479207, 10.1007/978-3-030-77004-4_1, 10.1145/3503161.3547910}. While it is possible that the same approach on the English dataset may also be effective on the Indonesian dataset, the actual performance of the algorithm when applied to Indonesian languages is still unknown. Especially when the model or the feature extractor was mainly pretrained on an English language dataset, the applicability of the algorithm on the Indonesian language is even more questionable due to the lack of pretraining visual-language resources, i.e., pretrained model and pretraining dataset. In our experiments, we consider a scenario when the pretraining resources of visual-language for the Indonesian language are not available. To overcome the unavailability of the pretraining resources, we apply transfer learning by utilizing the models pretrained on the English vision-language dataset as the feature extractors, and then train the overall models on our MSVD-Indonesian dataset. By working on this scenario, we provide insight on how is the performance of language-specific video-text algorithms which were developed for English, when it is trained on a language which does not have the pretraining resources. Note that incorporating a pretraining process using an Indonesian dataset is beyond the scope of this study, which could be explored in future research. In addition, we discuss some potential future works in utilizing our dataset in section~\ref{sec:future}.

In this paper, to investigate the performance of video-text algorithms on our dataset, we employ X-CLIP \cite{10.1145/3503161.3547910}, which is one of the state-of-the-art models in text-to-video retrieval and video-to-text retrieval tasks on the MSVD dataset. For the video captioning task, we employ VNS-GRU \cite{DBLP:conf/ecai/Chen0020a}, which is one of the state-of-the-art models on the MSVD dataset. Following X-CLIP and VNS-GRU, we utilize the visual-language feature extractors, i.e., contrastive language-image pre-Training (CLIP) \cite{Radford2021LearningTV} and semantic concept detection (SCD) model \cite{SCN_CVPR2017}, which was pretrained on English datasets. We then fine-tune the X-CLIP and the VNS-GRU models and perform several ablation studies to investigate the effect of some configurations of the algorithms on our MSVD-Indonesian dataset.

In summary, our main contributions are listed as follows:

\begin{itemize}
\item We construct and release a video-text dataset in the Indonesian language, by translating an existing English video-text dataset, i.e., MSVD. To the best of our knowledge, this is the first publicly available video-text dataset in the Indonesian language at the time of writing this paper.
\item We establish the baseline performance of our dataset for three video-text tasks, i.e., text-to-video retrieval, video-to-text retrieval, and video captioning, by training neural network-based methods on the MSVD-Indonesian dataset.
\item Our experimental results show that the cross-lingual pretraining and fine-tuning can help to improve the performance in the three video-text tasks for the Indonesian language on all metrics.
\item Finally, we present some possible future works using our dataset to inspire other researchers in leading to a broader understanding of the topic and new discoveries.

\end{itemize}


\section{Related Works}
\label{sec:headings}

\subsection{Video-Text Datasets}
The video-text datasets are often constructed to facilitate research and development in multimodal learning, specifically for tasks like video captioning \cite{venugopalan15iccv} and video-text retrieval \cite{10.1145/3503161.3547910}. The sentences in the video-text datasets are commonly annotated in the English language, with only a few available in the non-English language \cite{Wang_2019_ICCV, 8806555, IJCOL:scaiella_et_al:2019, 10.1007/s00530-021-00816-3}. The MSVD dataset \cite{chen:acl11}, which is one of the most popular video-text datasets, was mainly available in the English language, with few subsequent variants constructed in non-English languages, i.e., MSVD-CN \cite{msvdcn} and MSVD-Turkish \cite{8806555}. The original MSVD dataset initially came in multiple languages, which includes, English, Hindi Romanian, Slovene, Serbian, Tamil, Dutch, German, etc. However, since the dataset was not publicly available anymore, Chen and Dolan \cite{chen:acl11} recollected the dataset, and able to reconstruct only the English sentences. Later on, the Chinese version \cite{msvdcn} and the Turkish version \cite{8806555} of video-text datasets are built based on the reconstructed MSVD dataset. In this paper, we construct and release a video-text dataset with Indonesian sentences based on the English MSVD dataset. Making our dataset publicly available encourages innovation by providing a resource for researchers to develop new ideas in the area of multimodal video-text in the Indonesian language.         

\subsection{Video-Text Retrieval}
Video-text retrieval consists of two main problems, i.e., text-to-video and video-to-text retrieval. Given a text as an input query, a text-to-video retrieval model retrieves the relevant video as the output. On the other hand, a video-to-text retrieval model receives a video as the input query, then retrieves the relevant text as the output. While the goals of the two tasks are different, many algorithms developed are often able to be used for both retrieving videos and retrieving text given the text query and the video query, respectively. Luo \etal \cite{10.1145/3474085.3479207} introduced a method called CLIP4Clip, by utilizing a CLIP model \cite{Radford2021LearningTV} which was pretrained on a large-scale image-language dataset. They showed that the usage of the CLIP model is able to achieve a SOTA performance for both text-to-video and video-to-text retrieval tasks with a significant margin. Ma \etal \cite{10.1145/3503161.3547910} then extends the CLIP4Clip model by proposing a multi-grained contrastive model, namely X-CLIP, which helped to even further improve the performance. Although those works have shown that utilizing the pretrained CLIP model could boost the accuracy performance on the video-text datasets in the English language, its applicability to the Indonesian language is still unknown due to the lack of the pertaining resources for the Indonesian language. In this work, we adopt X-CLIP and apply transfer learning to transfer the knowledge of the pretrained English CLIP model by fine-tuning the model to our Indonesian video-text dataset. Through our experiments, we obtain performance results for both text-to-video retrieval and video-to-text retrieval on our dataset, providing insights into how the pretrained English CLIP model can be useful for these two tasks in the Indonesian language. In addition, our experimental results can be served as a benchmark for future research in Indonesian video-text retrieval tasks.

\subsection{Video Captioning}
Video captioning aims to generate a sentence given a video as input. Many recent methods in video captioning utilized feature extractors pretrained on vision and language datasets, e.g., CLIP \cite{Radford2021LearningTV} and SCD \cite{SCN_CVPR2017}. While the CLIP model is pretrained on pairs of images and sentences, the SCD model is pretrained on pairs of videos and keywords in the sentences. Gan \etal \cite{SCN_CVPR2017} introduced SCD for video captioning to capture semantic features from the videos. They pretrained SCD in multi-label settings, where the video is used as the input and the keywords in the sentences are used as the labels. The pretrained SCD is then used as the semantic feature extractor to extract the probabilities of the keywords given the video as the input. Borrowing the success of SCD, some other works \cite{DBLP:conf/ecai/Chen0020a, 10.3389/frobt.2020.475767, Perez-Martin_2021_WACV, 9412898} adopted the SCD-based approach as the feature extractor in their proposed methods. Chen \etal \cite{10.3389/frobt.2020.475767} proposed a network called semantic detection network (SDN), adopting the idea of SCD, i.e., pretrained on video and keywords in the sentences. In another work, Chen \etal \cite{DBLP:conf/ecai/Chen0020a} incorporate a similar network called tagging network, which is also basically utilizing the concept of SCD. Similarly, Perez Martin \etal \cite{Perez-Martin_2021_WACV, 9412898} incorporated SCD into a customized LSTM-based decoder, which additionally receives the semantic features as the input. In this paper, we adopt the VNS-GRU algorithm \cite{DBLP:conf/ecai/Chen0020a}, which has the best performance among those SCD-based algorithms for video captioning on the MSVD dataset. We first pretrain the SCD-based feature extractor using the pair of video and English keywords, then we train the overall algorithm on the Indonesian MSVD dataset. The performance results of this experiment establish the benchmark performance for future studies of video captioning in the Indonesian language.

\section{MSVD-Indonesian Dataset}
\subsection{Dataset Collection}
The initial version of the MSVD dataset consists of 2089 videos \cite{chen:acl11}. However, due to some videos being removed from YouTube, Chen \etal were able to recollect and archive only 1970 out of 2089 videos. Later on, this reconstructed MSVD dataset, i.e., the one with 1970 videos, is the version that is popularly used for video-text research tasks in the English language. In this work, we collect the 1970 videos along with the 80827 English sentences from the MSVD dataset, and then translate each of the sentences using Google Translate API to have the Indonesian sentences. Thus, for each video in our dataset, we have the same number of sentences as in the MSVD dataset. Because every sentence in our dataset has a one-to-one mapping with the sentence in the English MSVD dataset, our dataset can also be used for the research of multilingual video-text research.

\subsection{Dataset Analysis}
Using a machine translation service to translate sentences may lead to inaccuracies in the translations due to the limitations of the service.  Since we utilize Google Translate API, we found that the Indonesian sentences in our dataset could have inaccurate grammar. Although some of the translations are inaccurate, many of the sentences for each video are still well-translated. As shown in Figure~\ref{fig:sample-dataset} (a), we can see that the three samples of the ground truth sentences in the MSVD-Indonesian dataset, i.e., indicated by ID-\#, are translated well w.r.t. to the English ground truth, i.e., indicated by EN-\#. In Figure~\ref{fig:sample-dataset} (b), however, we can notice that the verb `flours' in EN-2 is inaccurately translated as `tepung' in ID-2. The machine translation is unable to translate the verb `flours` correctly since it does not have a direct equivalent verb in Indonesian. The correct translation for the sentence could be `seorang wanita \underline{menuangkan tepung} ke dalam wajan (a woman \underline{pours flour} into the pan)`. The inaccuracy of the sentence in our dataset is not solely due to the problem of the machine translation service. In a few cases, the English sentences from the MSVD dataset originally consist of inaccurate annotations. From the same Figure~\ref{fig:sample-dataset} (b), we can notice that there is an inaccurate annotation in the English annotation, i.e., EN-3. Despite there being no strawberry in the video, the video is inaccurately annotated with `make \underline{strobery} food'. Interestingly, although there is a typographical error in the English annotation, i.e., `strobery', the machine translation translates the inaccurate English word `strobery' as an accurate Indonesian word `stroberi (Strawberry)'. In our experiments, we retain the inaccurate sentences, including both inaccurate translations and annotations, without making any modifications. We consider the low-quality annotations as noise and outliers of our dataset.

\begin{figure}[t!]
  \centering  
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sample-1.png}
    \caption{}
    \label{fig:sample1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/sample-2.png}
    \caption{}
    \label{fig:sample2}
  \end{subfigure}
  
  \caption{Two samples of videos from the MSVD-Indonesian dataset, along with their corresponding annotated sentences. Each video is accompanied by three sample sentences.}
  \label{fig:sample-dataset}
\end{figure}


As illustrated in Figure~\ref{fig:dataset-analysis}, we further analyze and compare the sentences in the MSVD dataset and the MSVD-Indonesian dataset on several data visualizations. In Figure~\ref{fig:dataset-analysis} (a), we can see similar patterns on the most frequent words in both datasets. For instance, the articles `seorang', `seekor', and `sebuah' are frequently used in the MSVD-Indonesian dataset, which is consistent with the MSVD dataset where similar articles are also frequently used, i.e., 'a' and 'an'. The different frequencies for the articles between the two datasets are due to the usage of the articles being different. While the choice between 'a' and 'an' depends on the sound that follows, i.e., consonant or vowel, the choice among `seorang', `seekor', and `sebuah'  depends on the specific category of the noun that follows them, i.e., human, animal, or general item.  From Figure~\ref{fig:dataset-analysis} (b), we can notice that the MSVD dataset has more vocabulary words compared to the MSVD-Indonesian dataset. We found that the MSVD dataset has 12592 unique words, while the MSVD-Indonesian dataset has 9457 unique words. In terms of the average sentence length, as shown in Figure~\ref{fig:dataset-analysis} (c), the MSVD-Indonesian dataset has about 5.7 words and the MSVD dataset has about 7 words. These statistical differences may indicate that a video-text algorithm that performs the best for the MSVD dataset, is not automatically the best for our MSVD-Indonesian dataset.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/wordcloud.png}
    \caption{}
    \label{fig:wordcloud}
  \end{subfigure}
  
  \vspace{\baselineskip}
  
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/num-vocabulary.png}
    \caption{}
    \label{fig:image2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/average-sent-length.png}
    \caption{}
    \label{fig:image3}
  \end{subfigure}
  
  \caption{Comparison of MSVD and MSVD-Indonesian dataset using three different visualizations: (a) word cloud, (b) bar chart of vocabulary word count, and (c) bar chart of average sentence length.}
  \label{fig:dataset-analysis}
\end{figure}

\section{Methods}

The methods that we adopt in this work are the ones that utilized a feature extractor pretrained on the English vision-language dataset. For the text-to-video and video-to-text retrieval tasks, we utilized X-CLIP \cite{10.1145/3503161.3547910} which incorporates the CLIP \cite{Radford2021LearningTV} model pretrained on an image-text dataset. As for the video captioning task, we employ VNS-GRU \cite{DBLP:conf/ecai/Chen0020a} which utilizes the SCD \cite{SCN_CVPR2017} model pretrained on a video-text dataset. Assuming there are no available pretraining resources for the vision-language in Indonesian, we initially use the feature extractors, i.e., CLIP and SCD, which were primarily pretrained on the English datasets. Subsequently, using the English-based pretrained extractors, we train the X-CLIP and VNS-GRU models on the MSVD-Indonesian dataset for their respective tasks.

\subsection{X-CLIP}
As the extension of CLIP4Clip \cite{10.1145/3474085.3479207}, X-CLIP \cite{10.1145/3503161.3547910} also utilized a pretrained CLIP model which was mainly pretrained on the English language. However, different than CLIP4Clip which only considers video-sentence contrastive learning, X-CLIP employed multi-grained contrastive learning where four different types of contrastive are considered, i.e., video-sentence, video-word, sentence-frame, and frame-word. To train the X-CLIP model, we need to extract frame-level features, video-level features, word-level features, and sentence-level features using the pretrained CLIP model. Following the same procedure as in X-CLIP, we first sample the video frames with a sampling rate of 1 FPS. Given the sampled frames, we then extract the frame-level feature using the pretrained CLIP model by inputting the video frames into the visual encoder of the CLIP model. Different than X-CLIP, we do not capture the interaction between frames using the temporal encoder \cite{10.1145/3503161.3547910} since our experiments show that the temporal encoder hurts the performance of the model in our dataset. The video-level features are then computed by averaging the frame-level features in the video. For extracting word-level and sentence-level features, the text encoder of the CLIP model is used by taking the outputs of the tokens from the final layer of the text encoder, where the [EOS] token output is used as the sentence-level features. All of these extracted features are then used to compute the similarity score of the four different types of contrastive relations in the X-CLIP algorithm. Especially for relations of video-word, sentence-frame, and frame-word, the attention over similarity matrix (AOSM) module \cite{10.1145/3503161.3547910} is applied to fuse the similarity vector or matrix, thereby obtaining the instance-level similarity. Please refer to \cite{10.1145/3503161.3547910} for more details explanations of the X-CLIP model. 

\subsection{VNS-GRU}
VNS-GRU \cite{DBLP:conf/ecai/Chen0020a} is one of the methods which adopts a pretrained SCD model \cite{SCN_CVPR2017}, i.e., tagging network in \cite{DBLP:conf/ecai/Chen0020a}, as a semantic feature extractor. The SCD model is a model which is pretrained under multi-label settings, where the training data are pairs of a video and keywords. The keywords are constructed by considering the top  vocabulary from the English sentences in the video-text dataset. Given a video as the input, the pretrained SCD model is able to extract a semantic feature vector that represents the probability of keywords for the given video input. In the VNS-GRU approach, this semantic feature is then concatenated with another feature, which is extracted from the final layer of a multi-class classification model pretrained on ImageNet \cite{deng2009imagenet}. The concatenation of these features is then used as the final semantic features. Along with these semantic features, VNS-GRU also requires video-level features which are extracted from a 3D CNN model pretrained on an action recognition dataset, i.e., Kinetics-400 \cite{DBLP:journals/corr/KayCSZHVVGBNSZ17}. Using the extracted features, the VNS-GRU model is then trained by adopting three techniques \cite{DBLP:conf/ecai/Chen0020a}, i.e., incorporating variational dropout and layer normalization in RNN unit, comprehensive selection method, and professional learning. For a more comprehensive understanding of the VNS-GRU model, please refer to \cite{DBLP:conf/ecai/Chen0020a}.

\section{Experimental Results}
\label{sec:experiments}
In this section, we discuss our experiment details for the retrieval and captioning tasks, including the evaluation metrics and the implementation details. We then discuss our experimental results on the test set, which include quantitative and qualitative results. For both retrieval and captioning tasks, we follow the standard split of the MSVD dataset, i.e., 1200, 100, and 670 videos for train, validation, and test set.

\subsection{Evaluation Metrics}
\subsubsection{Retrieval}
We evaluate our retrieval experiments by using five commonly used metrics in text-to-video and video-to-text retrieval tasks, i.e., R@1, R@5, R@10, MedianRank, and MeanRank. 1) R@1, R@5, and R@10 (recall at ) measure the proportion of relevant items correctly retrieved among the top  items. 2) Median rank indicates the median position at which the relevant items are found in a ranked list. 3) Mean rank, on the other hand, measure the average position of the relevant items found in the same list.

\subsubsection{Captioning}
For the captioning experiments, we assess the performance of the models by using four popular metrics in video captioning, i.e., BLEU@4 \cite{10.3115/1073083.1073135}. ROUGE-L \cite{lin2004rouge}, METEOR \cite{banarjee2005}, and CIDEr \cite{7299087}. 1) BLEU@4 computes the accuracy of a method by taking the precision of the generated sentences in terms of 4-grams, i.e., sequence of 4 words. 2) ROUGE-L measures the harmonic mean of precision and recall on the longest common subsequence (LCS) between generated sentence and ground-truth sentence.  3) METEOR computes its score by utilizing a weighted F-score based on unigrams, and incorporating a penalty function to penalize the incorrect word order in the generated sentence. 3) CIDER utilizes a voting-based method to have a robust measurement against noise or incorrect annotations.


\subsection{Implementation Details}
\subsubsection{X-CLIP}
The feature extractor for both video features and text features is a pretrained CLIP (VIT-B/16) \cite{Radford2021LearningTV} model, which was pretrained on a large-scale image-text dataset. The learning rate in this experiment is set to 1e-4 after carefully tuning. For the maximum word length, maximum frame length, and the number of training epochs, we set the hyperparameters to 32, 12, and 5, respectively. We set the batch size for the training to 16, and apply the gradient accumulation technique to fit the batch of data into the GPU memory. Our experiments are conducted on a Linux environment computer with 1 NVIDIA GeForce GTX 1080 Ti, which takes about 15 hours for training on our dataset. The X-CLIP model is implemented using the PyTorch library.

\subsubsection{VNS-GRU}
We extract the video features using Efficient Convolutional Network (ECN) \cite{10.1007/978-3-030-01216-8_43} which was pretrained on Kinetics-400 dataset \cite{DBLP:journals/corr/KayCSZHVVGBNSZ17}. The features are extracted from the global pooling layers of the network with dimension 1536. The semantic features are the concatenation of the features extracted from the probabilities output of SCD \cite{SCN_CVPR2017} and ResNeXt-101 \cite{8100117}. For the text features, the Indonesian word vectors are extracted using fastText \cite{bojanowski2016enriching}, in which the model was trained using continuous bag-of-words (CBOW) with position-weights and dimension 300. The learning rate in this experiment is tuned and set to 3e-4. The number of sampled annotations is fixed to 4 for the training. We set the batch size and the training epochs to 128 and 50, respectively. For this captioning task, we conducted the experiments using 1 NVIDIA GeForce GTX 1650, which takes around 43 minutes for the training. The TensorFlow library is used to implement the VNS-GRU model.


\begin{table}
\footnotesize
 \caption{{Impact of temporal encoder module in the X-CLIP algorithm on the MSVD-Indonesian dataset. The symbol  indicates the higher value in the metric is better, while the symbol  indicates the lower value in the metric is better.}}
  \centering
  \begin{tabular}{l | l l l l l | l l l l l}
    \toprule
         & \multicolumn{5}{c |}{Text-to-Video Retrieval} & \multicolumn{5}{c}{Video-to-Text Retrieval} \\
    \midrule
    Method & R@1 & R@5 & R@10 & MdR & MnR & R@1 & R@5 & R@10 & MdR & MnR \\
    \midrule
    w/ Temporal Encoder & 32.2 & 62.9 & 74.3 & \textbf{3.0} & 17.8 & 39.9 & 71.6 & 82.6 & \textbf{2.0} & 11.6     \\
    w/o Temporal Encoder    & \textbf{32.3} & \textbf{63.3} & \textbf{74.9} & \textbf{3.0} & \textbf{17.5} & \textbf{44.9} & \textbf{77.6} & \textbf{88.8} & \textbf{2.0} & \textbf{6.4}      \\
    \bottomrule
  \end{tabular}
  \label{tab:temporalencoder}
\end{table}

\begin{table}
 \small
 \caption{{Impact of pretrained CLIP in the X-CLIP algorithm on the MSVD-Indonesian dataset. The symbol  indicates the higher value in the metric is better, while the symbol  indicates the lower value in the metric is better. Initialization scheme for X-CLIP encoders:  \cmark\ indicates the encoder weights are initialized using the pretrained CLIP model, while \xmark\ indicates random initialization.}}
  \centering
  \begin{tabular}{c c | l l l l l | l l l l l}
    \toprule
         \multicolumn{2}{c |}{Pretrained CLIP} & \multicolumn{5}{c |}{Text-to-Video Retrieval} & \multicolumn{5}{c}{Video-to-Text Retrieval} \\
    \midrule
    Visual & Text (EN) & R@1 & R@5 & R@10 & MdR & MnR & R@1 & R@5 & R@10 & MdR & MnR \\
    \midrule
    \xmark & \xmark & 0.8 & 2.3 & 4.4 & 199.0 & 234.5 & 0.5 & 2.4 & 5.1 & 189.0 & 224.6     \\
    \xmark & \cmark & 1.6 & 5.2 & 8.7 & 149.0 & 196.3 & 1.0 & 3.3 & 5.8 & 151.0 & 199.3     \\
    \cmark & \xmark & 12.7 & 34.7 & 47.7 & 12.0 & 53.2 & 9.2 & 33.5 & 48.1 & 11.0 & 37.0      \\
    \cmark & \cmark & \textbf{32.3} & \textbf{63.3} & \textbf{74.9} & \textbf{3.0} & \textbf{17.5} & \textbf{44.9} & \textbf{77.6} & \textbf{88.8} & \textbf{2.0} & \textbf{6.4}      \\
    \bottomrule
  \end{tabular}
  \label{tab:pretrainedclip}
\end{table}


\begin{table}[ht!]
 \caption{{Impact of different CLIP models in the X-CLIP algorithm on the MSVSD-Indonesian dataset. The symbol  indicates the higher value in the metric is better, while the symbol  indicates the lower value in the metric is better.}}
  \centering
  \begin{tabular}{l | l l l l l | l l l l l}
    \toprule
         & \multicolumn{5}{c |}{Text-to-Video Retrieval} & \multicolumn{5}{c}{Video-to-Text Retrieval} \\
    \midrule
    Model & R@1 & R@5 & R@10 & MdR & MnR & R@1 & R@5 & R@10 & MdR & MnR \\
    \midrule
    ViT-B/32 & 28.1 & 58.6 & 70.9 & 4.0 & 20.6 & 35.4 & 68.0 & 78.4 & 3.0 & 13.6     \\
    ViT-B/16    & \textbf{32.3} & \textbf{63.3} & \textbf{74.9} & \textbf{3.0} & \textbf{17.5} & \textbf{44.9} & \textbf{77.6} & \textbf{88.8} & \textbf{2.0} & \textbf{6.4}      \\
    \bottomrule
  \end{tabular}
  \label{tab:vit}
\end{table}

\subsection{Quantitative Results}
\subsubsection{X-CLIP}
\textbf{Is the temporal encoder module needed on a less complex dataset?}
Temporal encoder module \cite{10.1145/3474085.3479207} is a 3-layer transformer that is proposed in the X-CLIP architecture to capture temporal interaction between different frames. Although this module is expected to improve the accuracy of the model, the additional parameters introduced may result in sub-optimal performance on a small dataset \cite{LUO2022293}. As shown in Table~\ref{tab:temporalencoder}, we found that adding the temporal encoder also does not help to improve the performance of the model on our MSVD-Indonesian dataset. For most of the metrics, we can see that without using the temporal encoder module, the X-CLIP model can even outperform the one using the temporal encoder module with a decent margin. On the video-to-retrieval task, the performance gain is 5, 6, and 6.2 points in R@1, R@5, and R@10 metrics. On the text-to-video retrieval task, the performance gain is 0.1, 0.4, and 0.6 in R@1, R@5, and R@10 metrics. We expect these results due to the different characteristics of our dataset compared to the English MSVD dataset. In our dataset, the length of the sentences tends to be shorter, and the vocabulary size is comparatively smaller.

\textbf{Is CLIP model pretrained on English image-text dataset useful for our Indonesian video-text dataset?}
In Table~\ref{tab:pretrainedclip}, we investigate the impact of the CLIP model of the X-CLIP algorithm which was primarily pretrained on the English image-text dataset. When the petrained visual or text encoder of the CLIP model is not used to initialize the X-CLIP encoders, we initialize the X-CLIP encoders with random values. For the text encoder, if the pretrained weights from the CLIP model are not used, we replace the original CLIP tokenizer with the BERT tokenizer for the Indonesian language \cite{koto2020indolem}. From the table, we can see that incorporating the English pretrained CLIP model, both the visual and text encoder, can significantly help to improve performance. Although the text encoder is not specifically pretrained on the Indonesian language, the general linguistic pattern and semantic relationships learned in the pretrained CLIP model may still provide valuable information when it is applied to our MSVD-Indonesian dataset.


\begin{table}[ht]
 \caption{{Ablation study of the SCD model in the VNS-GRU Algorithm on the MSVD-Indonesian dataset.}}
  \centering
  \begin{tabular}{c| l l l l}
    \toprule
    SCD (En) & B4 & C & M & R \\
    \midrule
    \xmark & 38.30 & 84.48 & 32.02 & 65.07    \\
    \cmark & \textbf{58.68} & \textbf{126.65} & \textbf{40.33} & \textbf{76.84}     \\
    \bottomrule
  \end{tabular}
  \label{tab:scd}
\end{table}

\begin{table}[ht!]
 \caption{{Impact of sampling numbers annotations in the VNS-GRU algorithm on the MSVD-Indonesian dataset. EXP denotes a non-fixed sampling schedule, i.e., exponential schedule, as defined in equation (25) in \cite{DBLP:conf/ecai/Chen0020a}.}}
  \centering
  \begin{tabular}{l | l l l l}
    \toprule
    Method & B4 & C & M & R \\
    \midrule
    2 & 54.96 & 121.43 & 39.22 & 75.85    \\
    4 & \textbf{58.68} & \textbf{126.65} & \textbf{40.33} & \textbf{76.84}    \\
    8 & 58.32 & 125.14 & 40.12 & 76.76    \\
    16 & 56.91 & 125.70 & 40.06 & 76.66    \\
    EXP & 56.89 & 122.87 & 39.79 & 76.64    \\
    \bottomrule
  \end{tabular}
  \label{tab:samplingnumber}
\end{table}


\textbf{How do the different CLIP models affect the results?}
We further investigate different CLIP models, i.e., ViT-B/16 and ViT-B/32, on our dataset as presented in Table~\ref{tab:vit}. The results demonstrate that the X-CLIP model utilizing ViT-B/16 consistently outperforms the model utilizing ViT-B/32 across all evaluation metrics on the MSVD-Indonesian dataset. These findings are consistent with the experiments conducted in \cite{10.1145/3503161.3547910}, where the X-CLIP model utilizing ViT-B/16 exhibited superior performance compared to the one utilizing ViT-B/32 on the MSVD dataset.


\subsubsection{VNS-GRU}

\textbf{Is SCD model pretrained on English video-text dataset useful for our Indonesian video-text dataset?} We investigate the usage of pretrained SCD as a feature extractor for the VNS-GRU model in Table~\ref{tab:scd}. The SCD model was pretrained on the English MSVD dataset to investigate the cross-lingual knowledge transfer from English to our Indonesian dataset. From the table, we can see that the performance gain is 20.38, 42.17, 8.31, and 11.77 points in BLEU@4, CIDEr, METEOR, and ROUGE-L metrics, respectively. These results indicate that the pretrained SCD model on the English video-text dataset can be employed to extract useful semantic information, which can be transferred to the Indonesian language. We expect this because most of the top  vocabulary extracted in the English dataset, which was used to pretrained the SCD model, is still semantically similar to the ones in our MSVD-Indonesian dataset. Although the performance may be improved further by using the SCD model which specifically pretrained on the Indonesian video-text dataset, the investigation for those works is left for future study.



\textbf{How do the different sampling numbers of annotations affect the results?} In the VNS-GRU algorithm, the training phase is divided into two phases. The first phase is all the annotations are equally used during training. In the second phase, a number of annotations are sampled, with the motivation to avoid only focusing on common words and forgetting detailed words. In this experiment, as shown in Table~\ref{tab:samplingnumber}, we conduct an ablation study of different sampling numbers of annotations on the configuration of the VNS-GRU algorithm. Chen \etal showed that a fixed sample size of 16 is the best configuration on the MSVD dataset. In our experiment, we found that using 4 as the sampling number is the best on our MSVD-Indonesian dataset. We expect this behavior because in our dataset, the average length of the sentences is relatively shorter and the vocabulary size is relatively smaller. When the sentences are simpler and have more words in common, a model is able to achieve better performance by focusing on fewer sentences \cite{DBLP:conf/ecai/Chen0020a}.

\begin{figure}[htbp]
  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/qualitative-t2v-1.png}
    \caption{}
    \label{fig:t2v1}
  \end{subfigure}
  \vfill
  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/qualitative-t2v-2.png}
    \caption{}
    \label{fig:t2v2}
  \end{subfigure}
  
  \caption{Qualitative results of the X-CLIP model without vs with the pretrained text encoder of CLIP model on the text-to-video retrieval task. When the pretrained text encoder of the CLIP model is not used, the text encoder weights of the X-CLIP model are randomly initialized. With respect to the ground truth, the green box and the red box indicate the relevant and the irrelevant video, respectively.}
  \label{fig:qual-t2v}
\end{figure}



\begin{figure}[t!]
  \centering  
  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/qualitative-v2t-1.png}
    \caption{}
    \label{fig:v2t1}
  \end{subfigure}
  \vfill
  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/qualitative-v2t-2.png}
    \caption{}
    \label{fig:v2t2}
  \end{subfigure}
  
  \caption{Qualitative results of the X-CLIP model without vs with the pretrained text encoder of CLIP model on the video-to-text retrieval task. When the pretrained text encoder of the CLIP model is not used, the text encoder weights of the X-CLIP model are randomly initialized. With respect to the ground truth, the green box and the red box indicate the relevant and the irrelevant text, respectively.}
  \label{fig:qual-v2t}
\end{figure}

\subsection{Qualitative Results}
We show qualitative results of our experiments on the MSVD-Indonesian dataset for text-to-video retrieval, video-to-text retrieval, and video captioning in Figure~\ref{fig:qual-t2v}, Figure~\ref{fig:qual-v2t}, and Figure~\ref{fig:qual-cap}, respectively. For the X-CLIP model, i.e., the retrieval model, we compare the top-5 retrieval results obtained between 1) the text encoder is randomly initialized, i.e., without using the pretrained text encoder of the CLIP model, and 2) the text encoder is initialized from the pretrained CLIP model. In both cases, the weights of the visual encoder are initialized with the pretrained visual encoder of the CLIP model. For the VNS-GRU model, i.e., captioning model, we compare the results obtained between 1) pretrained SCD model is not used and 2) the pretrained SCD model is used.

\subsubsection{X-CLIP}
In the text-to-video retrieval results, as illustrated in Figure~\ref{fig:qual-t2v}, we observe that the X-CLIP model with random initialization on the text encoder does not retrieve the relevant video as accurately as the model with the text encoder weights initialized from the CLIP model. As shown in Figure~\ref{fig:qual-t2v} (a), X-CLIP with random initialization in the text encoder is still able to retrieve the relevant video (green) at the 3rd rank. However, incorporating the pretrained initialization on both visual and text encoder helps to further improve the retrieval results by having the relevant video placed at the 1st rank and discarding the highly irrelevant video from the top-5 retrievals, i.e., videos containing 'human' in the 4th and 5th rank in the figure. Although the replacement at the 4th and the 5th rank is still not exactly relevant to the given text query, retrieving the videos of animals are arguably more relevant than retrieving videos of humans given the query which includes 'tiger' in the sentence. In Figure~\ref{fig:qual-t2v} (b), we can observe that the X-CLIP model with the pretrained text encoder successfully retrieves the relevant video in the top-5 retrieval results. Conversely, the model without the pretrained text encoder, i.e., random initialization, fails to retrieve the relevant video.

In the video-to-text results as shown in Figure~\ref{fig:qual-v2t}, we can notice that the number of relevant texts, w.r.t. the ground truth, in the top-5 retrievals are different between the two X-CLIP models with different text encoder initialization. In Figure~\ref{fig:qual-v2t} (a), the X-CLIP model with random initialization on the text encoder is still able to retrieve the relevant text at the 5th rank. The X-CLIP model with the pretrained text encoder, however, is able to retrieve more relevant texts in the top-5 results. Interestingly, except for the retrieved text in the 2nd rank of results obtained from the model without the pretrained text encoder, the other irrelevant texts (red) in the figure are still semantically well aligned to the video query. Similarly, as shown in Figure~\ref{fig:qual-v2t} (b), we also observe that the X-CLIP model with the randomly initialized text encoder is still capable of retrieving relevant text. However, the model with the pretrained text encoder performs better, yielding more relevant texts in the top-5 retrieval results.

Furthermore, while our experimental results demonstrate the capabilities of the X-CLIP model in retrieving relevant videos and texts on our MSVD-Indonesian dataset, it is clear that there is still room for improvement. Figure~\ref{fig:qual-t2v} and Figure~\ref{fig:qual-v2t} show instances where irrelevant videos and texts are retrieved in the top-5 retrieval results. Future research efforts can focus on enhancing the relevance of retrieved videos and texts, resulting in more precise and comprehensive results. 



\subsubsection{VNS-GRU}
In Figure~\ref{fig:qual-cap}, we can see that the VNS-GRU model without the SCD model pretrained on the English video-text dataset does not generate sentences better compared to the one which employs the pretrained SCD model. From Figure~\ref{fig:qual-cap} (a), we observe that the utilization of the English SCD model helps to generate a more details sentence with the word `daging (meat)' included in the sentence. From Figure~\ref{fig:qual-cap} (b), we can notice that the absence of the SCD model may generate an inaccurate sentence. Incorporating the pretrained SCD model guides the model to better capture the action and the object in the video, i.e., `mengendarai (rides)' and 'sepeda (bicycle)'. Although the SCD model was pretrained on the English video-text dataset, these results show that the extracted semantic information from the model could still be useful for training the VNS-GRU model on our MSVD-Indonesian dataset. 

Despite our experimental results indicating that the model is able to generate Indonesian sentences with reasonable accuracy, one can see that the generated sentences still lack sufficient details. This suggests that there is potential for further improvement in capturing specific details in the generated text. This can involve exploring techniques to incorporate more contextual information, improving the modeling of fine-grained details, and refining the language generation process to produce more informative and detailed sentences.




\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/qualitative-cap-1.png}
    \caption{}
    \label{fig:cap1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/qualitative-cap-2.png}
    \caption{}
    \label{fig:cap2}
  \end{subfigure}
  
  \caption{Qualitative results of the VNS-GRU model without vs with SCD model on the video captioning task.}
  \label{fig:qual-cap}
\end{figure}



\section{Discussion, Future Works and Conclusion}
\label{sec:future}
In this work, we constructed the MSVD-Indonesian dataset, which is the first public video-text dataset in the Indonesian language at the time of writing this paper. We conducted experiments and discussed the performance of two neural network models on our dataset for three different tasks, i.e., text-to-video retrieval, video-to-text retrieval, and video captioning. Our experimental results showed that the state-of-the-art algorithms which performed well on the English video-text dataset, i.e., the MSVD dataset, could also be applicable to our MSVD-Indonesian dataset with some modifications on the algorithms or on the parameters. Besides, incorporating a feature extractor that was pretrained on the English vision-language dataset could also help to improve the performance of the models on our Indonesian video-text dataset. Through our study, we also found that there are several potential works that can be explored for future research.

\textbf{Pretraining on a large-scale vision-language dataset.} A neural network model pretrained on a large-scale vision-language dataset has been widely adopted in many recent algorithms on many research tasks. The pretrained model serves as a powerful feature extractor that can boost the accuracy significantly. In our experiments, we assumed that there is a lack of pretraining resources for the Indonesian language. To address the issue, we utilized the models which mainly pretrained on the English vision-language dataset as the feature extractors. Future studies could delve into the exploration of pretraining the models on a large-scale Indonesian vision-language dataset, as this has the potential to substantially enhance the accuracy of the final models.

\textbf{Multilingual Output.} We conducted experiments on several research tasks focusing on monolingual output, where only the Indonesian language is outputted by a model. However, it is also interesting to develop an algorithm which able to output sentences in multiple languages given a video as the input. Since each Indonesian sentence in our dataset has a corresponding English sentence in the original MSVD dataset, exploring a multilingual approach becomes an interesting prospect for future work.

\textbf{Noise-Robust Algorithm.} We have shown that our dataset includes some inaccurate sentences due to the limitation of the machine translation service and also the inherited inaccurate annotations from the English dataset. Our experiments showed that the models are still able to produce output reasonably well despite the inaccuracy in the sentences. Yet, the existence of the noises is not explicitly addressed and investigated yet.  Investigating the impact of the noise and developing a noise-robust algorithm may also be interesting research to be explored.

In conclusion, we believe that our MSVD-Indonesian dataset can be used as an important benchmark for multiple video-text tasks, including text-to-video retrieval, video-to-text retrieval, and video captioning. Our benchmark dataset can encourage innovation in building a better algorithm for multimodal video-text research in the Indonesian language.

\begin{thebibliography}{10}

\bibitem{10.1109/TPAMI.2018.2798607}
Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency.
\newblock Multimodal machine learning: A survey and taxonomy.
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}, 41(2):423â€“443, feb
  2019.

\bibitem{DBLP:journals/corr/TorabiTS16}
Atousa Torabi, Niket Tandon, and Leonid Sigal.
\newblock Learning language-visual embedding for movie understanding with
  natural-language.
\newblock {\em CoRR}, abs/1609.08124, 2016.

\bibitem{PerezMartin2021ACR}
Jesus Perez-Martin, Benjam{\'i}n Bustos, Silvio Jamil~Ferzoli Guimar{\~a}es,
  Ivan Sipiran, Jorge~A. P'erez, and Grethel~Coello Said.
\newblock A comprehensive review of the video-to-text problem.
\newblock {\em Artificial Intelligence Review}, 55:4165 -- 4239, 2021.

\bibitem{venugopalan15iccv}
Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor
  Darrell, and Kate Saenko.
\newblock Sequence to sequence -- video to text.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, 2015.

\bibitem{chen:acl11}
David~L. Chen and William~B. Dolan.
\newblock Collecting highly parallel data for paraphrase evaluation.
\newblock In {\em Proceedings of the 49th Annual Meeting of the Association for
  Computational Linguistics (ACL-2011)}, Portland, OR, June 2011.

\bibitem{xu2016msr-vtt}
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
\newblock Msr-vtt: A large video description dataset for bridging video and
  language.
\newblock IEEE International Conference on Computer Vision and Pattern
  Recognition (CVPR), June 2016.

\bibitem{krishna2017dense}
Ranjay Krishna, Kenji Hata, Frederic Ren, Li~Fei-Fei, and Juan~Carlos Niebles.
\newblock Dense-captioning events in videos.
\newblock In {\em International Conference on Computer Vision (ICCV)}, 2017.

\bibitem{Wang_2019_ICCV}
Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William~Yang
  Wang.
\newblock Vatex: A large-scale, high-quality multilingual dataset for
  video-and-language research.
\newblock In {\em The IEEE International Conference on Computer Vision (ICCV)},
  October 2019.

\bibitem{8806555}
BegÃ¼m Ã‡tamak, MenekÅŸe Kuyu, Aykut Erdem, and Erkut Erdem.
\newblock Msvd-turkish: A large-scale dataset for video captioning in turkish.
\newblock In {\em 2019 27th Signal Processing and Communications Applications
  Conference (SIU)}, pages 1--4, 2019.

\bibitem{IJCOL:scaiella_et_al:2019}
Antonio Scaiella, Danilo Croce, and Roberto Basili.
\newblock Large scale datasets for image and video captioning in italian.
\newblock {\em Italian Journal of Computational Linguistics}, 2(5):49--60,
  2019.

\bibitem{10.1007/s00530-021-00816-3}
Alok Singh, Thoudam~Doren Singh, and Sivaji Bandyopadhyay.
\newblock Attention based video captioning framework for hindi.
\newblock {\em Multimedia Syst.}, 28(1):195â€“207, feb 2022.

\bibitem{DBLP:conf/ecai/Chen0020a}
Haoran Chen, Jianmin Li, and Xiaolin Hu.
\newblock Delving deeper into the decoder for video captioning.
\newblock In Giuseppe~De Giacomo, Alejandro Catal{\'{a}}, Bistra Dilkina,
  Michela Milano, Sen{\'{e}}n Barro, Alberto Bugar{\'{\i}}n, and
  J{\'{e}}r{\^{o}}me Lang, editors, {\em {ECAI} 2020 - 24th European Conference
  on Artificial Intelligence, 29 August-8 September 2020, Santiago de
  Compostela, Spain, August 29 - September 8, 2020 - Including 10th Conference
  on Prestigious Applications of Artificial Intelligence {(PAIS} 2020)}, volume
  325 of {\em Frontiers in Artificial Intelligence and Applications}, pages
  1079--1086. {IOS} Press, 2020.

\bibitem{10.1145/3474085.3479207}
Mingkang Tang, Zhanyu Wang, Zhenhua LIU, Fengyun Rao, Dian Li, and Xiu Li.
\newblock Clip4caption: Clip for video caption.
\newblock In {\em Proceedings of the 29th ACM International Conference on
  Multimedia}, MM '21, page 4858â€“4862, New York, NY, USA, 2021. Association
  for Computing Machinery.

\bibitem{10.1007/978-3-030-77004-4_1}
Jes\'{u}s~Andr\'{e}s Portillo-Quintero, Jos\'{e}~Carlos Ortiz-Bayliss, and Hugo
  Terashima-Mar\'{\i}n.
\newblock A straightforward framework for video retrieval using clip.
\newblock In {\em Pattern Recognition: 13th Mexican Conference, MCPR 2021,
  Mexico City, Mexico, June 23â€“26, 2021, Proceedings}, page 3â€“12, Berlin,
  Heidelberg, 2021. Springer-Verlag.

\bibitem{10.1145/3503161.3547910}
Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji~Zhang, and Rongrong Ji.
\newblock X-clip: End-to-end multi-grained contrastive learning for video-text
  retrieval.
\newblock In {\em Proceedings of the 30th ACM International Conference on
  Multimedia}, MM '22, page 638â€“647, New York, NY, USA, 2022. Association for
  Computing Machinery.

\bibitem{Radford2021LearningTV}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{SCN_CVPR2017}
Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao,
  Lawrence Carin, and Li~Deng.
\newblock Semantic compositional networks for visual captioning.
\newblock In {\em CVPR}, 2017.

\bibitem{msvdcn}
{Media Computing and Intelligent Systems Lab - Beijing Institute of
  Technology}.
\newblock {MSVD-CN}.
\newblock \url{https://github.com/mcislab-machine-learning/MSVD-CN}, 2018.

\bibitem{10.3389/frobt.2020.475767}
Haoran Chen, Ke~Lin, Alexander Maye, Jianmin Li, and Xiaolin Hu.
\newblock A semantics-assisted video captioning model trained with scheduled
  sampling.
\newblock {\em Frontiers in Robotics and AI}, 7:129, 2020.

\bibitem{Perez-Martin_2021_WACV}
Jesus Perez-Martin, Benjamin Bustos, and Jorge Perez.
\newblock Improving video captioning with temporal composition of a
  visual-syntactic embedding.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision (WACV)}, pages 3039--3049, January 2021.

\bibitem{9412898}
Jesus Perez-Martin, Benjamin Bustos, and Jorge PÃ©rez.
\newblock Attentive visual semantic specialized network for video captioning.
\newblock In {\em 2020 25th International Conference on Pattern Recognition
  (ICPR)}, pages 5767--5774, 2021.

\bibitem{deng2009imagenet}
Jia Deng, R.~Socher, Li~Fei-Fei, Wei Dong, Kai Li, and Li-Jia Li.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern
  Recognition(CVPR)}, volume~00, pages 248--255, 06 2009.

\bibitem{DBLP:journals/corr/KayCSZHVVGBNSZ17}
Will Kay, Jo{\~{a}}o Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier,
  Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul
  Natsev, Mustafa Suleyman, and Andrew Zisserman.
\newblock The kinetics human action video dataset.
\newblock {\em CoRR}, abs/1705.06950, 2017.

\bibitem{10.3115/1073083.1073135}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: A method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th Annual Meeting on Association for
  Computational Linguistics}, ACL '02, page 311â€“318, USA, 2002. Association
  for Computational Linguistics.

\bibitem{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: a package for automatic evaluation of summaries.
\newblock In {\em Workshop on Text Summarization Branches Out, Post-Conference
  Workshop of ACL 2004, Barcelona, Spain}, pages 74--81, July 2004.

\bibitem{banarjee2005}
Satanjeev Banerjee and Alon Lavie.
\newblock {METEOR}: An automatic metric for {MT} evaluation with improved
  correlation with human judgments.
\newblock In {\em Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic
  Evaluation Measures for Machine Translation and/or Summarization}, pages
  65--72, Ann Arbor, Michigan, June 2005. Association for Computational
  Linguistics.

\bibitem{7299087}
Ramakrishna Vedantam, C.~Lawrence Zitnick, and Devi Parikh.
\newblock Cider: Consensus-based image description evaluation.
\newblock In {\em 2015 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 4566--4575, 2015.

\bibitem{10.1007/978-3-030-01216-8_43}
Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox.
\newblock Eco: Efficient convolutional network for online video understanding.
\newblock In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair
  Weiss, editors, {\em Computer Vision -- ECCV 2018}, pages 713--730, Cham,
  2018. Springer International Publishing.

\bibitem{8100117}
Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em 2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 5987--5995, 2017.

\bibitem{bojanowski2016enriching}
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov.
\newblock Enriching word vectors with subword information.
\newblock {\em arXiv preprint arXiv:1607.04606}, 2016.

\bibitem{LUO2022293}
Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li.
\newblock Clip4clip: An empirical study of clip for end to end video clip
  retrieval and captioning.
\newblock {\em Neurocomputing}, 508:293--304, 2022.

\bibitem{koto2020indolem}
Fajri Koto, Afshin Rahimi, Jey~Han Lau, and Timothy Baldwin.
\newblock Indolem and indobert: A benchmark dataset and pre-trained language
  model for indonesian nlp.
\newblock In {\em Proceedings of the 28th COLING}, 2020.

\end{thebibliography}

\end{document}
