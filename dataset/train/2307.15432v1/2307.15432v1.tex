\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{setspace}
\usepackage{threeparttable}
\usepackage{balance}
\usepackage{tikz,xcolor}



\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}



\usepackage[colorlinks,
linkcolor=blue,
anchorcolor=blue,
citecolor=blue]{hyperref}

\hypersetup{hidelinks,
	colorlinks=true,
	allcolors=blue,
	pdfstartview=Fit,
	breaklinks=true}

\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{
\begin{tikzpicture}
\draw[lime, fill=lime] (0,0)
circle[radius=0.16]
node[white]{{\fontfamily{qag}\selectfont \tiny \.{I}D}};
\end{tikzpicture}
\hspace{-2mm}
}
\foreach \x in {A, ..., Z}{\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}{\noexpand\orcidicon}}
}
\newcommand{\orcidauthorA}{0000-0002-0116-5662}
\newcommand{\orcidauthorB}{0000-0002-4909-8286}
\newcommand{\orcidauthorC}{0000-0003-4587-3588}

\begin{document}

\title{CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for Dialogue Emotion Recognition}
\author{Jiang Li\hspace{-1.5mm}\orcidA{}, Yingjian Liu, Xiaoping Wang\hspace{-1.5mm}\orcidB{},~\IEEEmembership{Senior Member,~IEEE}, and Zhigang~Zeng\hspace{-1.5mm}\orcidC{},~\IEEEmembership{Fellow,~IEEE}


\thanks{This work was supported in part by the National Natural Science Foundation of China under Grant 62236005, 61936004, and U1913602. (\textit{Corresponding authors: Jiang Li; Xiaoping Wang.})}
\thanks{The authors are with the School of Artificial Intelligence and Automation, the Key Laboratory of Image Processing and Intelligent Control of Education Ministry of China, and the Hubei Key Laboratory of Brain-inspired Intelligent Systems, Huazhong University of Science and Technology, Wuhan 430074, China (e-mail: lijfrank@hust.edu.cn; M202072868@hust.edu.cn; wangxiaoping@hust.edu.cn; zgzeng@hust.edu.cn).}}


\markboth{Journal of \LaTeX\ Class Files}{LI \MakeLowercase{\textit{et al.}}: A Cross-Modal Fusion Network with Emotion-Shift Awareness for Dialogue Emotion Recognition}



\maketitle

\begin{abstract}
Multimodal Emotion Recognition in Conversation (ERC) has garnered growing attention from research communities in various fields. In this paper, we propose a cross-modal fusion network with emotion-shift awareness (CFN-ESA) for ERC. Extant approaches employ each modality equally without distinguishing the amount of emotional information, rendering it hard to adequately extract complementary and associative information from multimodal data. To cope with this problem, in CFN-ESA, textual modalities are treated as the primary source of emotional information, while visual and acoustic modalities are taken as the secondary sources. Besides, most multimodal ERC models ignore emotion-shift information and overfocus on contextual information, leading to the failure of emotion recognition under emotion-shift scenario. We elaborate an emotion-shift module to address this challenge. CFN-ESA mainly consists of the unimodal encoder (RUME), cross-modal encoder (ACME), and emotion-shift module (LESM). RUME is applied to extract conversation-level contextual emotional cues while pulling together the data distributions between modalities; ACME is utilized to perform multimodal interaction centered on textual modality; LESM is used to model emotion shift and capture related information, thereby guide the learning of the main task. Experimental results demonstrate that CFN-ESA can effectively promote performance for ERC and remarkably outperform the state-of-the-art models.
\end{abstract}

\begin{IEEEkeywords}
Emotion recognition in conversation, multimodal fusion, cross-modal association, emotion shift.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{R}{ecently}, multimodal learning has attracted the attention of both academia and industry, and has been widely applied in many fields, such as biometrics~\cite{talreja2021deep}, information retrieval~\cite{zhang2022towards}, autonomous driving~\cite{chen2023uamdnet}, and emotion recognition~\cite{he2023multimodal}. With the advancement of technologies, the abundance of multimodal data can be more conveniently available for research purposes. In realistic life, multimodal data mainly contains three contents, i.e., transcribed text, visual image or video, and acoustic speech. Multimodal fusion is one of the prominent branches of multimodal learning, whose main purpose is to utilize the organic combination of information from multiple modalities to collaboratively achieve the final downstream task. Thus, how to adequately extract the inter-modal complementary and associative information becomes a formidable challenge in the domain of multimodal fusion.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=3.4in]{example.pdf}
    \caption{An example of conversational scene from the MELD dataset. If only textual modality is taken into account, the emotion of  may be recognized as \textit{neutral}. From the facial expression of the speaker who utters , it is known that the emotion should be \textit{anger}, which is true emotion of the utterance.}
    \label{fig:example}
\end{figure}
The target of Emotion Recognition in Conversation (ERC) is to understand and analyze each utterance in the conversation and render the corresponding emotion. This task has recently drawn widespread interest from researchers in the areas of natural language processing, computer vision, and multimodal learning due to its promising applications, such as human-machine interface in intelligent robots and opinion mining in social media. Most previous ERC models are based on individual modalities, such as text~\cite{jiao2020real,shen2021dialogxl,nie2022igcn,zhao2022cauain,li2022contrast} and speech~\cite{fan2022isnet,latif2022multitask,lei2022bat,zhou2022multiclassifier}. However, very often, the emotions of human beings are elusive. As shown in Fig.~\ref{fig:example}, textual uni-modality may not be capable of correctly recognizing emotions in some scenarios, e.g., the emotion directly expressed by the text is \textit{neutral}, but the corresponding facial expression is actually another emotion, e.g., \textit{anger}. From this example, it can be argued that the model cannot understand and convey human emotions well with only a single modality. As multi-modality gets closer to real-world application scenarios, multimodal ERC has gained numerous research. The information contained in a single modality may not be sufficient or representative enough, while a multimodal-based model can make up for the shortcoming of the unimodal approach and thus improve the performance and robustness of the existing system. Simultaneously, multimodal ERC is more in line with the multiple ways (e.g., language, voice, and facial expressions) in which people express their emotions. Unlike traditional affective computing missions in unimodal~\cite{jiao2020real,shen2021dialogxl,li2022contrast} and non-conversational~\cite{he2023multimodal,mai2022hybrid,yu2021learning} scenarios, multimodal ERC suffers from harsher challenges due to the complex relationship between multiple modalities and conversational contexts.

Although previous studies have made impressive progress, these approaches either ignore the association between multimodal information or model multi-modality insufficiently. Some methods~\cite{poria2017context,hazarika2018conversational,hazarika2018icon,majumder2019dialoguernn} directly concatenate multimodal data without considering the associative information between multiple modalities. Moreover, there is a certain amount of noise in each modality itself, and together with the heterogeneity gap~\cite{hazarika2020misa} of multimodal data, this direct concatenation manner may cause more noise. While some approaches~\cite{hu2021mmgcn,chen2021learning,hu2022mmdfn,chen2022modeling} perform associative modeling for multimodal data, there are flaws in their modeling styles. For instance, these methods assume that each modality contributes equally to the emotional expression of the utterance, which is not the case. The findings of extant multimodal ERC studies~\cite{mao2021dialoguetrm,yuan2023rbagcn} indicate that textual modalities contain more valuable emotional information in comparison to visual and acoustic modalities. Consequently, exploiting each modality equally may not effectively maximize the performance of the model when engaging in multimodal interaction, making it difficult to adequately extract multimodal complementary and associative information.

Towards the above issues, we propose a novel model for conversational emotion recognition to efficiently model associations with multimodal data. We treat visual and acoustic modalities as sources of auxiliary information that are utilized to complement the representation of textual information; in turn, textual information is employed to augment visual and acoustic representations. In addition, existing approaches fail to consider emotion-shift information and concentrate too much on contextual information, causing the imbalance between context- and self-modeling, i.e., the importance of self-information (the utterance information from current and two other modalities) is prone to be neglected. To alleviate this problem, we devise a label based emotion-shift module as the auxiliary task of ERC, which guides the main task of ERC to optimize the emotional expression of utterances by taking into account emotion-shift factor. To put it in a nutshell, we propose a \textbf{C}ross-modal \textbf{F}usion \textbf{N}etwork with \textbf{E}motion-\textbf{S}hift \textbf{A}wareness (CFN-ESA) for emotion recognition in conversation. The main contributions of this work include:
\begin{enumerate}
	\item A novel multimodal ERC method named CFN-ESA is proposed. Our CFN-ESA can efficiently extract multimodal complementary and associative information, which mainly consists of three components, i.e., Recurrence based Uni-Modality Encoder (RUME), Attention based Cross-Modality Encoder (ACME), and Label based Emotion-Shift Module (LESM).
	\item RUME can capture intra-modal contextual emotional cues while narrowing the heterogeneity gap of multimodal data by sharing parameters. ACME perceives textual modality as the primary source of emotional information and two other modalities as the secondary sources, and employs multi-head attention networks to adequately model multimodal interaction. 
	\item LESM is employed as an auxiliary task of ERC to explicitly model emotion shift and extract relevant shift information, thereby enabling the main task to implicitly reduce intra-modal contextual modeling under emotion-shift scenario.
	\item Two public benchmark datasets, MELD and IEMOCAP, are leveraged to conduct numerous experiments for demonstrating the effectiveness of the proposed CFN-ESA. We also explore the impact under different network settings and test the performance of each component in CFN-ESA.
\end{enumerate}

The remaining sections of this paper are organized as follows. Section~\ref{work} is an introduction to the works related to this paper. Section~\ref{model} is a detailed description of our proposed CFN-ESA. Sections~\ref{setup} and~\ref{result} provide the experiments of this paper, including the experimental setup, experimental results and analysis. Section~\ref{conclusion} is a summary of this paper.

\section{Related Works}\label{work}
\subsection{Emotion Recognition in Conversation}
With the mounting interest in the study of dialogue systems, the identification of emotion in the conversation has become a hot research topic. Most previous ERC methods are based on textual modality, which primarily employ Gated Recurrent Unit (GRU), Long and Short Term Memory (LSTM) network, and Graph Neural Network (GNN) to model contexts. 
AGHMN~\cite{jiao2020real} mainly consists of Hierarchical Memory Network (HMN) and Bidirectional Gated Recurrent Unit (BiGRU), where HMN is used to extract the interactive information between historical utterances, and BiGRU is used for the summarization of short- and long-term memory with the help of attentional weights. 
DialogXL~\cite{shen2021dialogxl} applies the pre-trained language model XLNet~\cite{yang2019xlnet} to the ERC task. To achieve this purpose, DialogXL handles long-term context with enhanced memory and speaker dependencies with dialogue-aware self-attention. 
I-GCN~\cite{nie2022igcn} utilizes graph convolutional network to extract the semantic association information of utterances and the temporal sequence information of dialogues. The method firstly exploits graph structure to represent dialogues at different times and then employs incremental graph structure to simulate the process of dynamic dialogues.
CauAIN~\cite{zhao2022cauain} consists of two main causal-aware interactions, namely causal cue retrieval and causal utterance traceback, which introduces commonsense knowledge as a cue for detecting emotional causes in a dialogue, explicitly modeling intra- and inter-speaker dependencies.
CoG-BART~\cite{li2022contrast} is an ERC approach that employs both contrastive learning and generative modeling, which utilizes BART~\cite{lewis2020bart} as a backbone model, and enhances the emotional expression of utterances through contrastive loss and generative loss.
The approaches based on acoustic modality are often termed as Speech Emotion Recognition (SER). ISNet~\cite{fan2022isnet} is an individual standardization network that adopts automatically generated benchmark for individual standardization to deal with the problem of inter-individual emotion confusion in SER. MTL-AUG~\cite{latif2022multitask} is a semi-supervised multitask learning framework that employs speech-based augmentation types, while treating augmented classification and unsupervised reconstruction as auxiliary tasks to enable multi-task training to achieve the learning of generic representations without the need for meta-labeling. BAT~\cite{lei2022bat} splits the hybrid spectrogram into blocks and computes self-attention by combining these blocks with tokens, meanwhile utilizing the cross-block attention mechanism to facilitate the information interaction between blocks. Although there are also some visual modality-based methods~\cite{xu2022mdan,zhu2017dependency,she2020wscnet} known as facial expression recognition, they are mostly outside the scope of the ERC task.

There have been some multimodal ERC efforts recently. 
MMGCN~\cite{hu2021mmgcn} exploits GNN to capture contextual and modal interactive information, which not only compensates for the shortcomings of previous methods that are unable to leverage multimodal dependencies, but also efficiently incorporates the speaker's information for ERC.
DialogueTRM~\cite{mao2021dialoguetrm} uses hierarchical Transformer to manage differentiated contextual preferences within each modality, and designs multi-grained interactive fusion to learn the different contributions of multiple modalities.
MetaDrop~\cite{chen2021learning} presents a dyadic contain or drop decision-making mechanism to learn adaptive fusion paths while extracting multimodal dependencies and contextual relationships.
HU-Dialogue~\cite{chen2022modeling} introduces hierarchical uncertainty for ERC, containing a regularization based attention module that is perturbed by source-adaptive noise to model context-level uncertainty.
MM-DFN~\cite{hu2022mmdfn} utilizes a graph based dynamic fusion module to track conversational contexts in various semantic spaces and to enhance complementarity between modalities.
COGMEN~\cite{joshi2022cogmen} is a multimodal ERC model that uses a GNN architecture to model local dependencies and global contexts in the conversation, which effectively improves the performance of the model.
UniMSE~\cite{hu2022unimse} integrates acoustic and visual features with textual features by applying T5~\cite{raffel2020exploring}, and performs inter-modal contrastive learning to obtain differentiated multimodal representations. 
Distinct from traditional affective computing tasks in single-modal and non-conversational settings, multimodal ERC is more challenging due to the complex relationship of multiple modalities and dialogue contexts.

\subsection{Multi-Head Attention Network}
Vaswani et al.~\cite{vaswani2017attention} propose the Transformer architecture for machine translation task, which achieves exceptional performance. Since then, the Multi-Head Attention (MHA) network of Transformer has been widely applied in the fields of natural language processing, computer vision, and multimodal learning. In this paper, we adopt MHA networks to extract multimodal complementary and associative information, i.e., they are utilized to construct Attention based Cross-Modality Encoder (ACME). Here, we first define the scaled dot-product attention:

where query , key , and value  are the packed feature representations;  denotes the dimensions of  or ;  denotes the softmax function. MHA is a network structure that can enhance the stability and performance of the scaled dot-product attention. The distinction is that different heads employ different query, key, and value matrices. MHA can be computed as follows:

where  denotes the concatenation operation; , , and  are the learnable parameters, which can project , , and  into different representation subspaces, respectively;  is also the trainable parameter.

\section{Proposed Model}\label{model}
This section is a detailed description of CFN-ESA, a multimodal dialogue emotion network. Our CFN-ESA can sufficiently capture multimodal complementary information by leveraging the proposed modules. As shown in Fig.~\ref{fig:overall_architecture}, CFN-ESA mainly consists of the recurrence based uni-modality encoder (Uni-Modality Encoding), attention based cross-modality encoder (Cross-Modality Encoding), emotion classifier (Classifier), and label based emotion-shift module (Emotion-Shift Optimizing).
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=7.1in]{overall_architecture.pdf}
    \caption{The overall architecture of our CFN-ESA. First, the utterance-level features of visual, textual, and acoustic modalities are extracted by DenseNet, RoBERTa, and OpenSmile, respectively; second, the intra-modal contextual information and inter-modal complementary information are captured by uni-modality encoder and cross-modality encoder, in that order; then, the optimization of the utterance expression is performed by utilizing the emotion-shift module; finally, the emotion classifier is adopted for prediction.}
    \label{fig:overall_architecture}
\end{figure*}

\subsection{Problem Definition}
Given a conversation  containing  utterances , , , i.e., , the goal of ERC is to predict the emotion state  for each utterance  in . In other words, the task of ERC is to learn a function  with learnable parameters that maps the feature representation  of an utterance  to the corresponding emotion , i.e., . Here, a conversation is expressed by  different modalities, i.e., ; and the set of modalities can be represented as . In our work, a conversation involves three modalities, i.e., textual (), visual (), and acoustic () modalities, so each utterance  can be represented as .

\subsection{Recurrence Based Uni-Modality Encoder}
\begin{figure*}[htbp]
    \centering
    \subfloat[RUME]{\includegraphics[height=3.0in]{rume.pdf}\label{fig:rume}}
	\hfil
	\subfloat[Visual part in ACME]{\includegraphics[height=3.1in]{acme_v.pdf}\label{fig:acme_v}}
    \hfil
    \subfloat[Textual part in ACME]{\includegraphics[height=3.1in]{acme_t.pdf}\label{fig:acme_t}}
	\hfil
    \subfloat[Acoustic part in ACME]{\includegraphics[height=3.1in]{acme_a.pdf}\label{fig:acme_a}}
    \caption{The network structures of RUME and ACME. Fig.~(a) shows the structure of RUME, which shares parameters for each modality. Fig.~(b), (c), and (d) show the network structure for visual, textual, and acoustic information updating in ACME, respectively. Note that the information updating network for visual modality is similar to that for acoustic modality.  denotes the residual operation.}
    \label{fig:rume_acme_modal}
\end{figure*}
To extract dialogue-level contextual emotional cues, we employ Recurrence based Uni-Modality Encoder (RUME) to encode the utterance in each of three modalities. Inspired by the structure of Transformer~\cite{vaswani2017attention}, we add fully connected networks and residual operations to RUME to improve the expressiveness and stability of Recurrent Neural Network (RNN). Our uni-modality encoder is shown in Fig.~\ref{fig:rume}. Specifically, the structure of RUME can be formalized as:

where  denotes the feature matrix of the utterance; , , and  denote the RNN, normalization, and feedforward network layers, respectively. In this work, the  and  default to bidirectional GRU and layer normalization; while the feed-forward layer consists of two fully connected networks, which can be represented as,

where  and  denote the fully connected network and dropout operation, respectively, and  denotes the activation function.

Note that in order to make the data distribution for each modal utterance as close as possible (i.e., to alleviate the heterogeneity gap problem for multimodal data), we utilize the uni-modality encoder with shared parameter for all three modalities. That is, , where  and  denotes the uni-modality encoder.

\subsection{Attention Based Cross-Modality Encoder}
Multimodal ERC can compensate for the lack of information in unimodal methods. In this work, we devise Attention based Cross-Modality Encoder (ACME) to extract complementary and associative information from multimodal emotion data. As shown in Fig.~\ref{fig:acme_v}, \ref{fig:acme_t}, and \ref{fig:acme_a}, we take inspiration from the Transformer structure and mainly adopt the attention network layer, feedforward network layer, and residual operation to construct our ACME. Several studies~\cite{hu2021mmgcn,mao2021dialoguetrm} on multimodal ERC have revealed that the amount of emotional information embedded in visual and acoustic modalities is lower than that in textual modalities, and thus the expression of emotion in these models is limited. Based on this assumption, we take both visual and acoustic features as complementary information to complement the emotional expression of textual features. In turn, textual features of utterances are used to enhance the visual and acoustic representations. Furthermore, in RUME, it is laborious for RNN to focus on the global contextual information of the utterance. Therefore, we employ a self-attention layer to capture global contextual emotional cues before performing cross-modal interaction. The designed ACME is composed of the following three stages.

\textit{Firstly, enhancing the global contextual awareness of the utterance.} The feature matrices  from three modalities are taken as the inputs to three MHA networks, , and the direct output  is summed with the input  (i.e., the residual operation) to obtain feature matrix . This process can be expressed by equations as:

where  denotes the MHA network.

\textit{Then, performing cross-modal interaction modeling.} The above results are employed as inputs to four MHA networks in pairwise manner, and the information for each modality is updated. In the following, we describe the information update for each modality separately.

For the information update in textual modality, there are mainly two MHA networks and the feature matrices from three modalities being leveraged. Specifically, the textual feature matrix  is utilized as the query  in one MHA network, and the visual feature matrix  is utilized as the key  and the value , and the output  is a textual feature matrix with visual information; similarly, the query  in another MHA network comes from , the key  and value  are , and we obtain , a textual feature matrix with acoustic information; we further concatenate  and  to get , and at the same time, we apply the residual operation to add , , and  to obtain the new textual feature matrix . The above process can be formalized as:

where  represents the concatenation operation.

For the information update in visual modality, one attention network and the feature matrices from two modalities are mainly used. Specifically, we take the visual feature matrix  as the query  in the MHA network, and the textual feature matrix  as the key  and value , to obtain the visual feature matrix  with textual information enhancement; similar to the textual information updating process, the residual operation is applied to add , , and  to gain the new visual feature matrix . The above process can be formalized as: 


The information updating process in acoustic modality is similar to that in visual modality, which can be expressed by the following equation:


\textit{Last but not least, improving the expressiveness and stability of the model.} We take  as the input to each of three feedforward network layers to obtain ; at the same time, the residual operation is used to sum , ,  to obtain the feature matrix . The above process is expressed by the equation as follows: 


\subsection{Emotion Classifier}
After multiple layers of RUME and ACME encoding, we obtain the final feature matrix , , and then they are concatenated to obtain fused feature matrix . Finally, the feature dimensions of  are converted to  (number of emotions) with an emotion classifier, and thus we obtain the predicted emotion  (). The process can be formulated as follows:

where , and  and  are learnable parameters. We define the loss function as follows:

where  is the number of utterances of the -th dialogue, and  is the number of all dialogues in training set;  denotes the probability distribution of predicted emotion label of the -th utterance in the -th dialogue, and  denotes the ground truth label.

\subsection{Label Based Emotion-Shift Module}
In order to extract emotion-shift information and enhance the emotional expression of the utterance, we introduce the Label based Emotion-Shift Module (LESM) to explicitly model the emotion-shift between utterances. LESM consists of three main steps, i.e., firstly, constructing the probability tensor of emotion-shift, then generating the label matrix of emotion-shift, and finally, performing the training exploiting the loss of emotion-shift. Our LESM is used as an auxiliary task to guide the learning of the main task, thereby empowering the main task to reduce intra-modal conceptual modeling during emotion shift scene and instead focus on cross-modal interactive modeling.
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=7.1in]{emotion_shift.pdf}
    \caption{An example of constructing emotion-shift probability tensor . Here,  can be viewed as a  dimensional matrix composed of feature vectors (emotion-shift probability vectors) that are concatenated from the feature vectors of utterances.}
    \label{fig:emotion_shift}
\end{figure*}

\subsubsection{Emotion-Shift Probability}
Inspired by SimCSE~\cite{gao2021simcse}, we employ two parameter-shared ACMEs to generate two feature matrices with different representations but consistent emotion semantics. In other words, the output  () of RUME is treated as the inputs to two parameter-shared ACMEs, and then the two fused feature matrices  and  are obtained. Here, , ,  is the number of utterances in the conversation, and  is feature dimension of  or . We concatenate the feature vectors from each utterance in  and all utterances in  to construct  dimensional emotion-shift probability tensor . If the feature dimension of  is mapped to 1 through the fully-connected layer, then the emotion-shift probability between two utterances can be obtained.

An example of the above process can be illustrated in Fig.~\ref{fig:emotion_shift}. Specifically, assume that there exist three utterances and the corresponding feature vectors are , , , . These feature vectors are taken as inputs to two parameter-shared ACMEs, and thus the fused feature vectors  and  () are obtained, where  and . Then, concatenating  with each  (i.e., , , and ); and similarly, concatenating  with each ; and for , the same concatenation operation is adopted. Finally, the  dimensional emotion-shift probability tensor  is obtained.

\subsubsection{Emotion-Shift Label}
We annotate emotion-shift status between utterances based on true emotion labels of the dataset. Concretely, if the true emotions of two utterances are the same, then we annotate their shift status as 0, meaning that emotion shift has not occurred; conversely, if their true emotions are different, then we annotate the shift status as 1, meaning that emotion shift has occurred. By the above operation, we obtain the  dimensional emotion-shift label matrix.

\subsubsection{Emotion-Shift Loss} 
After constructing the emotion-shift probabilities and labels, we require to define the corresponding emotion-shift loss for training. LESM is a binary-classified auxiliary task, which aims to correctly distinguish the emotion-shift states between utterances. In this way, the model is prompted to capture emotion-shift information, thereby guiding it to attenuate focus on contextual information. First, in order to obtain the predicted emotion-shift state  (), we convert the feature dimension of the probability tensor  to 2 with the fully-connected layer. The above process is as follows:

where  denotes emotion-shift probability vector between the -th and -th utterances, ;  is the probability distribution of predicted emotion-shift label between the -th and -th utterances;  and  are learnable parameters. The defined emotion-shift loss is:

where  is the number of utterances of the -th dialogue, and  is the number of all dialogues in training set;  denotes the probability distribution of predicted emotion-shift label between the -th and -th utterances in the -th dialogue, and  denotes the ground truth label.

\subsection{Training Objective}
We combine the classification loss  and emotion-shift loss  to get the final training objective,

where  is a trade-off parameter with a value in the range [0,1],  is the L2-regularizer weight, and  is the set of all learnable parameters. Further,  can be set manually or automatically and dynamically adjusted using the method of Kendall et al.~\cite{kendall2018multi}.

\section{Experimental Setup}\label{setup} 
\subsection{Datasets}
We adopt two public dialogue emotion datasets: IEMOCAP~\cite{busso2008iemocap} and MELD~\cite{poria2018meld}. The statistics of them are shown in TABLE~\ref{tab:statistics}. 
\textbf{MELD} is a multimodal and multiparty dataset containing more than 1,400 dialogues and 13,000 utterances from the TV series \textit{Friends}. There are seven emotion labels in the dataset, i.e., \textit{anger}, \textit{disgust}, \textit{sadness}, \textit{joy}, \textit{surprise}, \textit{fear}, and \textit{neutral}. 1,153 dialogues with 11,098 utterances are employed as the training and validation sets, where the 10\% of utterances is selected as the validation set. The remaining 2,610 utterances in the dataset are served as the test set, which contains 280 dialogues. 
\textbf{IEMOCAP} is an acted, multimodal and multi-speaker dataset consisting of dyadic conversations, which contains textual, visual, and acoustic modalities. The dataset consists of 151 dialogues and 7,433 utterances labelled with six emotion categories: \textit{happy}, \textit{sad}, \textit{neutral}, \textit{angry}, \textit{excited}, and \textit{frustrated}. We adopt 120 dialogues with 5,810 utterances for training and validation, and the rest for testing. Here, the validation set is randomly selected from the training set with a ratio of 10\%.
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{4pt}
    \caption{The Statistical Information of the Two Datasets}
    \begin{threeparttable}
    \begin{tabular}{c|cccc}
    \hline
    \multirow{2}{*}{\textbf{Datasets}} &\multicolumn{2}{c}{MELD} &\multicolumn{2}{c}{IEMOCAP} \\
           &\#Dialogue &\#Utterance &\#Dialogue &\#Utterance \\ 
    \hline
    Train &1,098 &9,989 &10 &4,778  \\
    Val &114 &1,109 &20 &980 \\
    Test &280 &2,610 &31 &1,622 \\
    \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item \#Dialogue and \#Utterance denote the number of dialogues and utterances, respectively.
    \end{tablenotes}
    \end{threeparttable}
    \label{tab:statistics}
\end{table}

The utterance-level features are extracted in the following manner. The visual and acoustic features are extracted with the way of MMGCN~\cite{hu2021mmgcn}, i.e., the visual features are extracted using a DenseNet~\cite{huang2017densely} pre-traind on the Facial Expression Recognition Plus corpus\cite{barsoum2016training}, the acoustic features are extracted using the OpenSmile toolkit with IS10 configuration~\cite{schuller2011recognising}. The textual feature is processed adopting the approach of COSMIC~\cite{ghosal2020cosmic}, i.e., the RoBERTa~\cite{liu2020roberta} model is applied for pre-training and fine-tuning to extract textual features.

\subsection{Training Details}
The operating system we used is Ubuntu with version 20.04, and the deep learning framework is Pytorch 2.0.0. All experiments are conducted on a single NVIDIA GeForce RTX 3090. In our experiments, the maximum epoch is set to 80, and the basis network of RUME is GRU by default; AdamW~\cite{loshchilov2018decoupled} is employed as the optimizer with the L2 regularization factor of 1e-4; and the number of heads in all MHA networks is set to 8. For the MELD dataset, the learning rate is set to 1e-5, and the batch size is set to 64; the number of network layers for RUME and ACME are 2 and 3, respectively, with corresponding dropout rates of 0.1 and 0.3, respectively; we manually set the trade-off parameter  to 0.9 by default. For the IEMOCAP dataset, the learning rate is set to 2e-5, and the batch size is set to 32; the number of network layers for RUME and ACME are 2 and 5, respectively, with corresponding dropout rates of 0.2 and 0.4, respectively; the trade-off parameter  is manually set to 1.0. 

\subsection{Comparative Methods}
The baselines we use fall into two categories: text based methods and multi-modality based methods. The text based approaches are included AGHMN~\cite{jiao2020real}, DialogXL~\cite{shen2021dialogxl}, I-GCN~\cite{nie2022igcn}, CauAIN~\cite{zhao2022cauain}, CoG-BART~\cite{li2022contrast}. The multi-modality based approaches include MMGCN~\cite{hu2021mmgcn}, DialogueTRM~\cite{mao2021dialoguetrm}, MetaDrop~\cite{chen2021learning}, HU-Dialogue~\cite{chen2022modeling}, MM-DFN~\cite{hu2022mmdfn}, COGMEN~\cite{joshi2022cogmen}, UniMSE~\cite{hu2022unimse}.

\subsection{Evaluation Metrics}
Following previous works~\cite{hu2021mmgcn,hu2022mmdfn}, we report the accuracy (Acc) and weighted F1 score (W-F1) to measure overall performance on the two public datasets (i.e., MELD and IEMOCAP), and also present F1 score for each emotion class.

\section{Results and Analysis}\label{result}
We conduct numerous comparison and ablation experiments to demonstrate the effectiveness of our CFN-ESA. The proposed CFN-ESA is first compared with baselines; secondly, the effects of different settings on the performance are discussed; then, the ablation experiments are conducted to explore the impacts of different components; and finally, a sentiment classification experiment is conducted. In addition, we conduct the case study and error analysis in the last two parts.

\subsection{Comparison to Baselines on the MELD Dataset}
We report the experimental results of CFN-ESA on the MELD dataset in the left half of TABLE~\ref{tab:overall_results}. As can be seen from the table, the proposed CFN-ESA outperforms the results of all the baseline models in terms of weighted F1 score and accuracy. Among all the textual unimodal models, the weighted F1 score of CauAIN is 65.46\%, which is the highest experimental performance. Our CFN-ESA is 66.70\%, which is an improvement of 1.24\% relative to CauAIN. This result suggests that the acoustic and visual modalities in CFN-ESA can contribute complementary information to effectively improve the performance of the model. Relative to MetaDrop's weighted F1 score of 66.08\%, the proposed CFN-ESA improves by 0.62\%. The accuracy of MetaDrop is 66.42\%, while that of CFN-ESA is 67.85\%, with the former being 1.43\% lower than the latter. Comparing the accuracy of CFN-ESA and DialogueTRM, the accuracy of CFN-ESA improves by 2.15\% relative to that of DialogueTRM, yielding similar results as above. These comparative results indicate that our model can more effectively model multimodal emotion datasets.
\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \footnotesize
\setlength{\tabcolsep}{3pt}
    \caption{Overall Results of All Models on the MELD and IEMOCAP Dataset.}
	\begin{threeparttable}
    \begin{tabular}{c|ccccccc|cc|cccccc|cc}
    \hline
    \multirow{3}{*}{\textbf{Models}} & \multicolumn{9}{c}{MELD} & \multicolumn{8}{c}{IEMOCAP}
	\\ \cline{2-18} 
     & \textit{neutral} & \textit{surprise} &\textit{fear} & \textit{sadness} & \textit{joy} &\textit{disgust} &\textit{anger}  & \multirow{2}{*}{W-F1} & \multirow{2}{*}{Acc} & \textit{happy} & \textit{sad} & \textit{neutral} & \textit{angry} &\textit{excited} &\textit{frustrated}  & \multirow{2}{*}{W-F1} & \multirow{2}{*}{Acc} \\ & F1 & F1  & F1 & F1 & F1 & F1 & F1 &  &  & F1 & F1  & F1 & F1 & F1 & F1 &  &  \\
    \hline
	AGHMN  & 76.40  & 49.70 & 11.50 & 27.00 & 52.40 & 14.00 & 39.40 & 58.10 &63.50 & 52.10  & 73.30 & 58.40 & 61.90 & 69.70 & 62.30 & 63.50 &63.50\\
    DialogXL   & - & - & - & - & - & - & - &62.41 &- & - & - & - & - & - & - &62.41 &- \\
    I-GCN   & 78.00 & 51.60 & 8.00 & 38.50 & 54.70 & 11.80 & 43.50 &60.80 &- & 50.00 & \textbf{83.80} & 59.30 & 64.60 & 74.30 & 59.00 &65.40 &65.50\\
    CauAIN   & - & - & - & - & - & - & - &65.46 &-  & - & - & - & - & - & - &67.61 &- \\
    CoG-BART  & -  & - & - & - & - & - & - &64.81 &- & -  & - & - & - & - & - &66.18 &- \\
	\hline
    MMGCN & 76.33 & 48.15 & - & 26.74 & 53.02 & - & 46.09 &58.31 &60.42 & 45.14 & 77.16 & 64.36 & 68.82 & 74.71 & 61.40 &66.26 &66.36 \\
    DialogueTRM  & - & - & - & - & - & - & - &63.50 &65.70 & - & - & - & - & - & - &69.70 &69.50\\
    MetaDrop   & - & - & - & - & - & - & -  &66.08 &66.42 & - & - & - & - & - & -  &69.04 &69.01 \\
	HU-Dialogue   & - & - & - & - & - & - & -  &58.56 &61.38 & - & - & - & - & - & -  &65.36 &65.72 \\
	MM-DFN   & 77.76 & 50.69 & - & 22.93 & 54.78 & - & 47.82  &59.46 &62.49 & 42.22 & 78.98 & 66.42 & 69.77 & \textbf{75.56} & 66.33  &68.18 &68.21 \\
	COGMEN & - & - & - & - & - & - & - & - & - & 51.90 & 81.70 & 68.60 & 66.00 & 75.30 & 58.20 & 67.60 &68.20 \\
	UniMSE   & - & - & - & - & - & - & -  &65.51 &65.09 & - & - & - & - & - & -  &70.66 &70.56 \\
	\hline
	CFN-ESA   & \textbf{80.05} & \textbf{58.78} & \textbf{21.62}  & \textbf{41.82} & \textbf{66.50} & \textbf{26.92} & \textbf{54.18} & \textbf{66.70} &\textbf{67.85} & \textbf{53.67} & 80.60  & \textbf{71.65} & \textbf{70.32} & 74.82 & \textbf{68.06} & \textbf{71.04} &\textbf{70.78} \\
      \hline
    \end{tabular}\begin{tablenotes}
        \footnotesize
        \item Results for MMGCN are from MM-DFN, other results are from the original paper. W-F1, F1, and Acc denote the accuracy (\%), F1 score (\%), and weighted F1 score (\%), respectively. Best performance in bold.
    \end{tablenotes}
    \end{threeparttable}
    \label{tab:overall_results}\end{table*}

It is obvious from TABLE~\ref{tab:overall_results} that our CFN-ESA outperforms all baselines in the experimental results for all emotion classes. CFN-ESA achieves the best F1 score of 80.05\% for \textit{neutral} in all emotion classes. Of particular concern is that the MELD dataset has an extremely severe class imbalance problem, where the emotions \textit{fear} and \textit{disgust} belong to the minority classes among all the classes. As can be noticed from TABLE~\ref{tab:overall_results}, our CFN-ESA achieves F1 scores of 21.62\% and 26.92\% on these two emotions, which are significantly higher than the results of AGHMN and I-GCN. 

\subsection{Comparison to Baselines on the IEMOCAP Dataset}
The comparison results of CFN-ESA on the IEMOCAP dataset are reported in in the right half TABLE~\ref{tab:overall_results}. We can state that CFN-ESA achieves the best performance with the weighted F1 score and accuracy of 71.04\% and 70.78\%, respectively. Focusing our attention on the comparison with the unimodal approaches. Relative to the weighted F1 score of 66.18\% for CoG-BART, that for CFN-ESA has an improvement of 4.86\%. The accuracy of I-GCN is 65.50\%, which is 5.28\% lower than that of our CFN-ESA. This phenomenon can indicate that CFN-ESA effectively leverages the information from multiple modalities and alleviates the problem of insufficient information expression in unimodal models. In the multimodal methods, our CFN-ESA still shows a strong performance. The weighted F1 score of UniMSE is 70.66\%, which is 0.38\% lower than the result of the proposed CFN-ESA. The performance of CFN-ESA improves by 1.77\% relative to the 69.01\% accuracy of MetaDrop. From these, we can conclude that CFN-ESA can more adequately capture multimodal complementary information in comparison to previous multimodal methods.

Compared to MM-DFN, CFN-ESA achieves superior performance on all emotions except \textit{excited}. Particularly, it is evident that the proposed CFN-ESA achieves an F1 score of 53.67\% for the emotion \textit{happy}, which is significantly higher than the 42.22\% of MM-DFN. The F1 score of CFN-ESA in terms of \textit{neutral} is improved by 5.23\% than that of MM-DFN. In addition, we can observe from TABLE~\ref{tab:overall_results} that \textit{sad} achieves the highest F1 scores among all the emotion classes. Fig.~\ref{fig:iemocap_tsne_emo} shows the T-SNE visualization of the original feature and the feature extracted by CFN-ESA on the IEMOCAP dataset. It can be observed that the feature extracted by CFN-ESA can clearly distinguish each emotion class and outperform the original feature, demonstrating the powerful capability of our model for feature extraction.
\begin{figure*}[htbp]
    \centering
    \subfloat[Features before being extracted]{\includegraphics[height=2.5in]{iemocap_initial_emo_pic-best.pdf}\label{fig:iemocap_initial_emo}}
    \hfil
    \subfloat[Features after being extracted]{\includegraphics[height=2.5in]{iemocap_cfnesa_emo_pic-best.pdf}\label{fig:iemocap_cfnesa_emo}}
    \caption{Comparison of T-SNE visualization before and after feature extraction is performed by employing CFN-ESA on the IEMOCAP dataset.}
    \label{fig:iemocap_tsne_emo}
\end{figure*}

\subsection{Effect of Different Modal Settings}
We examine the effects of different modal settings on the proposed model, as shown in TABLE~\ref{tab:modal_settings}. As we expected, the tri-modal setting achieves the best performance relative to the bi-modal and unimodal settings. Among all the unimodal settings, the textual setting attains 67.09\% accuracy on the MELD dataset and 66.57\% F1 score on the IEMOCAP dataset, which is much higher than two other unimodal settings and reaches the best performance. These results indicate that the textual modality contains more emotional information than two other modalities. Compared to visual unimodal setting, the acoustic unimodal setting yields better experimental results on both datasets. The possible reason is that images have a complex background with more ambient noise.
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{8pt}
    \caption{Performance comparison of Different Modal Settings}
    \begin{threeparttable}
    \begin{tabular}{c|cc|cc}
    \hline
    \multirow{2}{*}{\textbf{Modal Settings}} &\multicolumn{2}{c|}{MELD} &\multicolumn{2}{c}{IEMOCAP}\\
&W-F1 &Acc &W-F1 &Acc\\ 
    \hline 
	Textual &65.81 &67.09 &66.57 &66.56 \\
    Visual &32.05 &48.05 &44.23 &45.01 \\
	Acoustic &41.46 &49.35 &51.08 &53.45 \\
	\hline
	T + V &65.93 &67.13 &67.86 &67.58 \\
    T + A &65.94 &67.16 &68.46 &68.67 \\
	V + A &43.25 &50.34 &59.83 &60.36 \\
	\hline
    T + V + A &\textbf{66.70} &\textbf{67.85} &\textbf{71.04} &\textbf{70.78} \\
	\hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item T, V, and A is textual, visual, and acoustic modalities, respectively.
    \end{tablenotes}
    \end{threeparttable}
    \label{tab:modal_settings}
\end{table}

The performance of the bi-modal settings with text are better compared to the visual-acoustic setting. On the IEMOCAP dataset, the textual-acoustic setting achieves an accuracy of 68.67\%, which is 1.09\% higher than the result of the textual-visual setting. Similar experimental results also appear on the MELD dataset. In addition, Fig.~\ref{fig:different_modal_settings} illustrates the comparison among the textual unimodal, textual-visual bi-modal, textual-acoustic bi-modal, and tri-modal settings. It can be observed that the bi-modal setting with visual or acoustic modality has a higher performance than the textual setting. This indicates that the multimodal settings can effectively improve the performance of the ERC task. Similarly, the experimental results of the tri-modal setting with both visual and acoustic modalities are better compared to the bi-modal setting.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=3.4in]{different_modal_settings.pdf}
    \caption{Comparison of different modal settings with textual modality.}
    \label{fig:different_modal_settings}
\end{figure}

\subsection{Impact of Different Network Depths}
We explore the effect of different network depths (number of layers) on the performance in this subsection. We first fix the network depth of one encoder unchanged, then vary the network depth of the other, and record the experimental results. Fig.~\ref{fig:rume_depths} depicts the effect of RUME with different network depths on the experimental results. As can be seen from the figure, the performance of CFN-ESA increases and then decreases as the network deepens. The optimal network depth is 2. The effect of ACME with different network depths on our model is displayed in Fig.~\ref{fig:acme_depths}. We can notice that the variation trend of the experimental results is similar to Fig.~\ref{fig:rume_depths}, i.e., the performance shows a tendency to increase and then decrease. The optimal network depth is 5.
\begin{figure*}[htbp]
    \centering
    \subfloat[Effect of RUME]{\includegraphics[width=3.0in]{rume_depths.pdf}\label{fig:rume_depths}}
    \hfil
    \subfloat[Effect of ACME]{\includegraphics[width=3.0in]{acme_depths.pdf}\label{fig:acme_depths}}
    \caption{Effect of different network depths on the performance. The subfigure on the left (or right) indicates the effect of network depth for RUME (or ACME).}
    \label{fig:depths}
\end{figure*}

\subsection{Effect of Different Variants of RUME}
Our RUME defaults to modified GRU (Modified-GRU), which has three other variants, namely (1) replacing GRU with LSTM (Modified-LSTM), (2) using vanilla GRU directly (Vanilla-GRU), and (3) employing LSTM directly (Vanilla-LSTM). In this subsection, we explore the effects of different variants on the performance of CFN-ESA. Our experimental results are shown in TABLE~\ref{tab:rume_variants}. Overall, both Modified-LSTM and Modified-GRU outperform the direct usage of the corresponding vanilla RNN. For example, on the MELD dataset, the accuracy is 66.55\% when Vanilla-LSTM is used, while the result is 67.13\% when Modified-LSTM is applied, which is an improvement of 0.58\%; on the IEMOCAP dataset, the weighted F1 score when adopting Modified-GRU is 71.04\%, which is an improvement of 3.14\% relative to the performance when adopting Vanilla-GRU. Furthermore, it can be observed from the table that the experimental results employing Modified-GRU outperform the results employing Modified-LSTM. 
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{8pt}
    \caption{Performance Comparison of Different Variants of RUME}
\begin{tabular}{c|cc|cc}
    \hline
    \multirow{2}{*}{\textbf{Types of RUME}} &\multicolumn{2}{c|}{MELD} &\multicolumn{2}{c}{IEMOCAP}\\
&W-F1 &Acc &W-F1 &Acc\\ 
    \hline 
	Vanilla-LSTM &65.46 &66.55 &68.55 &68.54 \\
    Modified-LSTM &65.92 &67.13 &69.32 &69.12 \\
	\hline
	Vanilla-GRU &65.52 &66.55 &67.90 &68.54 \\
    Modified-GRU &\textbf{66.70} &\textbf{67.85} &\textbf{71.04} &\textbf{70.78} \\
	\hline
    \end{tabular}
\label{tab:rume_variants}
\end{table}

\subsection{Impact of Different Trade-Off Parameters}
In our experiments, the trade-off parameter  can be set in two ways, that is, manual setting and automatic setting using the method of Kendall et al.~\cite{kendall2018multi}. In this subsection, we investigate the effects of different trade-off parameters on the performance. TABLE~\ref{tab:trade-off} demonstrates the effect of  on the results on the MELD and IEMOCAP datasets. It can be seen that: (1) on the MELD dataset, the best experimental results are achieved when  is manually set to 0.9; and (2) on the IEMOCAP dataset, the best weighted F1 score is attained when  is manually set to 1.0, whereas automatically setting  results in the best accuracy.
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{9pt}
    \caption{Performance Comparison of Different Trade-Off Parameters}
\begin{tabular}{cc|cc|cc}
    \hline
    \multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Values of }}} &\multicolumn{2}{c|}{MELD} &\multicolumn{2}{c}{IEMOCAP}\\ 
& &W-F1 &Acc &W-F1 &Acc\\ 
    \hline 
	\multicolumn{1}{c|}{\multirow{10}{*}{Manual}} & 0.1 &66.51 &67.74 &70.77 &70.52 \\
    \multicolumn{1}{c|}{}& 0.2 &66.47 &67.70 &70.64 &70.40 \\
	\multicolumn{1}{c|}{}& 0.3 &66.53 &67.74 &70.77 &70.52 \\
	\multicolumn{1}{c|}{}& 0.4 &66.52 &67.74 &70.78 &70.52 \\
	\multicolumn{1}{c|}{}& 0.5 &66.53 &67.78 &70.64 &70.40 \\
	\multicolumn{1}{c|}{}& 0.6 &66.58 &67.78 &70.65 &70.40 \\
	\multicolumn{1}{c|}{}& 0.7 &66.57 &67.74 &70.75 &70.52 \\
	\multicolumn{1}{c|}{}& 0.8 &66.67 &67.82 &70.90 &70.65 \\
    \multicolumn{1}{c|}{}& 0.9 &\textbf{66.70} &\textbf{67.85} &70.96 &70.72 \\
	\multicolumn{1}{c|}{}& 1.0 &66.68 &67.82 &\textbf{71.04} &70.78 \\
	\hline
	\multicolumn{2}{c|}{Automatic} &66.64 &67.78 &70.72 &\textbf{70.98}\\
	\hline
    \end{tabular}
\label{tab:trade-off}
\end{table}

\subsection{Ablation Studies}
To demonstrate the effectiveness of each module in CFN-ESA, we perform a series of ablation experiments in this subsection. Specifically, we remove the Recurrence based Uni-Modality Encoder (RUME), Attention based Cross-Modality Encoder (ACME), and Label based Emotion-Shift Module (LESM), respectively, then report the experimental results. The results are showed in TABLE~\ref{tab:ablation}.
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{10pt}
    \caption{Performance Comparison After Removing Each Module}
    \begin{threeparttable}
    \begin{tabular}{c|cc|cc}
    \hline
    \multirow{2}{*}{\textbf{Models}} &\multicolumn{2}{c|}{MELD} &\multicolumn{2}{c}{IEMOCAP}\\
&W-F1 &Acc &W-F1 &Acc\\ 
    \hline 
	CFN-ESA &\textbf{66.70} &\textbf{67.85} &\textbf{71.04} &\textbf{70.78} \\
	\hline
	-w/o RUME &66.37 &67.51 &70.25 &70.33 \\
    -w/o ACME &65.97 &67.20 &68.08 &67.97 \\
	w/o LESM &66.40 &67.62 &70.22 &70.01 \\
	\hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item -w/o RUME, -w/o ACME, and -w/o LESM denote removing RUME, ACME, and LESM, respectively.
    \end{tablenotes}
    \end{threeparttable}
    \label{tab:ablation}
\end{table}

Validity of RUME: When our RUME is removed, the weighted F1 score of the proposed model on the MELD dataset decreases from 66.70\% to 66.37\%; while on the IEMOCAP dataset, the accuracy of CFN-ESA decreases by 0.45\% from the original 70.78\%. The primary reason for the declines is that CFN-ESA loses the ability to model local context. Thus, our CFN-ESA relies on RUME to extract dialog-level contextual information.

Validity of ACME: Since the input to LESM depends on two forward propagations of ACME, the input comes from the results of two forward propagations of RUME when ACME is removed. To put it differently, we directly use RUME instead of ACME. As can be seen from TABLE~\ref{tab:ablation}, when we remove ACME, the accuracy of our CFN-ESA on the MELD dataset decreases by 0.65\%, obtaining a result of 67.20\%; while on the IEMOCAP dataset, the model's weighted F1 scores show a significant decrease of 2.96\%. The above results indicate that our ACME plays an essential role in adequately capturing multimodal complementary information and has the capability to cross-modal interaction.

Validity of LESM: In similar fashion to the experimental results discussed previously, both the weighted F1 score and accuracy of the proposed CFN-ESA decline when we remove LESM. On the MELD dataset, the weight F1 score of our model drops to 66.40\%; on the IEMOCAP dataset, the accuracy of CFN-ESA decreases by 0.77\% from 70.78\%. These phenomena suggest that capturing emotion-shift information in conversation and acting as an auxiliary task of ERC can facilitate the optimization and enhancement of emotional expression for the utterance.

Overall, regardless of which module of CFN-ESA is removed, there are degradation in the performance on the two datasets. It can be visualized in Fig.~\ref{fig:ablation} that the performance of CFN-ESA decreases after the removal of different modules. In summary, it can be stated that these modules we designed for the model are valid.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=3.4in]{ablation.pdf}
    \caption{Effect of removing different modules on the performance.}
    \label{fig:ablation}
\end{figure}

\subsection{Sentiment Classification}
We replace emotion with sentiment in this subsection in order to conduct the task of sentiment classification in conversations. In other words, we transform CFN-ESA into a three-classification (i.e., neutral, positive, and negative) model. Note that since the IEMOCAP dataset does not contain sentiment labels, we need to merge the original emotion. The specific merging scheme is shown in TABLE~\ref{tab:merging}.
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{3pt}
    \caption{Merging Scheme for Converting Emotions into Sentiments}
    \begin{threeparttable}
    \begin{tabular}{c|ccc}
    \hline
    \textbf{Datasets} &\textit{negative} &\textit{neutral} &\textit{positive} \\
    \hline
	MELD &\textit{negative} &\textit{neutral} &\textit{positive}  \\
	IEMOCAP &\textit{sad, angry, frustrated} &\textit{neutral} &\textit{happy, excited} \\
    \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item For the MELD dataset, we adopt its original sentiment labels.
    \end{tablenotes}
    \end{threeparttable}
    \label{tab:merging}
\end{table}

The experimental results of our sentiment classification are reported in TABLE~\ref{tab:sentiment}. It can be observed that after the emotions are coarsened into sentiments, the weighted F1 scores and accuracies of CFN-ESA on the two datasets are improved. For instance, the accuracy of CFN-ESA on the MELD dataset is improved from 67.85\% to 73.75\%, with an increase of 5.9\%; on the IEMOCAP dataset, the weighted F1 score of the proposed CFN-ESA improves from 71.04\% to 84.49\%, with an increment of 13.45\%. 
\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{7pt}
    \caption{Experimental Results for Sentiment Classification on the Two Datasets}
    \begin{threeparttable}
    \begin{tabular}{c|ccc|cc||ccc|cc}
    \hline
    \multirow{2}{*}{\textbf{Models}} &\multicolumn{5}{c||}{MELD} &\multicolumn{5}{c}{IEMOCAP}\\
    \cline{2-11}
           &neutral &positive &negative &\multirow{2}{*}{W-F1} &\multirow{2}{*}{Acc} &neutral &positive &negative &\multirow{2}{*}{W-F1} &\multirow{2}{*}{Acc}\\ 
		   &F1 &F1 &F1 & & &F1 &F1 &F1 & & \\
	\hline
	CFN-ESA-emo &- &- &- &66.70 &67.85 &- &- &- &71.04 &70.78 \\
    CFN-ESA-sent &78.71 &67.06 &70.42 &73.74 &73.75 &88.03 &70.06 &90.99 &84.49 &84.78 \\
	\hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item CFN-ESA-emo and CFN-ESA-sent denote the tasks of emotion classification and sentiment classification, respectively.
    \end{tablenotes}
    \end{threeparttable}
    \label{tab:sentiment}
\end{table*}

\subsection{Case Study}
We discuss a case of emotion shift in this subsection. Fig.~\ref{fig:case} shows a conversational scenario in the IEMOCAP dataset. When a speaker utters several consecutive times with the true emotion \textit{neutral}, most models such as MM-DFN tend to predict the emotion of next utterance as \textit{neutral}. This is due to the fact that these models tend to model based on context, which leads to overly focusing on the contextual information and ignoring the inter-modal and inter-modal self-information. On the contrary, since CFN-ESA can capture emotion-shift information exploiting LESM, which enables the model to strike a trade-off between contextual modeling and self-modeling, e.g., capturing more inter-modal self-information (AKA multimodal complementary information), it identifies the next utterance as the correct emotion \textit{anger}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=3.4in]{case.pdf}
    \caption{A conversational case in the IEMOCAP dataset.}
    \label{fig:case}
\end{figure}

\subsection{Error Studies}
Fig.~\ref{fig:cm} shows confusion matrices of our CFN-ESA on the MELD and IEMOCAP datasets. Comparing the two subfigures, it can be concluded that the classification effect of CF-ESA on the IEMOCAP dataset is better than that on the MELD dataset. One primary reason is that MELD is a severely class-imbalanced dataset, where \textit{fear}, \textit{sadness}, and \textit{disgust} belong to the extreme minority classes. As can be witnessed in Fig.~\ref{fig:meld_cm}, the above three classes perform the worst. In most cases, the model tends to recognize them as the majority class (i.e., \textit{neutral}) on the MELD dataset.
\begin{figure*}[htbp]
    \centering
	\subfloat[Confusion matrix on MELD]{\includegraphics[width=3.0in]{meld_cm.pdf}\label{fig:meld_cm}}
    \hfil
    \subfloat[Confusion matrix on IEMOCAP]{\includegraphics[width=3.0in]{iemocap_cm.pdf}\label{fig:iemocap_cm}}
    \caption{Confusion matrices on the MELD and IEMOCAP datasets. Note that in the confusion matrices, we convert predicted quantities into proportions.}
    \label{fig:cm}
\end{figure*}

Another limitation is that, like most ERC models, our CFN-ESA suffers from the similar-emotion problem. In other words, because the characteristics of some emotions is close to or belongs to the same sentiment, it is difficult for CFN-ESA to differentiate them. For example, on the MELD dataset, the true emotion \textit{disgust} is easily classified as \textit{anger}; on the IEMOCAP data, the proposed CFN-ESA recognizes the true emotion \textit{happy} as \textit{excited} in some cases, as well as detects the true \textit{angry} as \textit{frustrated}. In the case of class imbalance, a minority class itself is hard to recognize correctly, and it is recognized as either the majority class or similar emotion. For example, in Fig.~\ref{fig:meld_cm}, \textit{disgust} is easily categorized as either the majority class \textit{neutral} or similar emotion \textit{anger}. Thus, the similar-emotion problem becomes more severe in class-imbalanced case.

\section{Conclusion}\label{conclusion}
Previous multimodal ERC models exist some flaws, such as (1) failure to distinguish the amount of emotional information in each modality, which results in difficulty in adequately modeling multimodal data; and (2) failure to consider emotion-shift information and overfocusing on capturing intra-modal contextual information, which results in the model not being ability to correctly identify emotions under some emotion-shift scenarios. To address the above issues, we propose a multimodal conversational emotion recognition network, CFN-ESA, to efficiently capture multimodal emotional information, providing a new modeling scheme for multimodal ERC task. Our CFN-ESA mainly contains a Recurrence based Uni-Modality Encoder (RUME), an Attention based Cross-Modality Encoder (ACME), and a Label based Emotion-Shift Module (LESM). The function of RUME is to capture intra-modal contextual information at the conversation level and to narrow the differences in the distribution of multimodal data; ACME takes textual modality as the main source of information, which can effectively extract inter-modal complementary information; and LESM is used to extract emotion-shift information, which guides the main task to reduce intra-modal contextual modeling under emotion-shift scenario, thereby optimizing the emotional expression of the utterance. To demonstrate the effectiveness of CFN-ESA, we conduct comparison experiments and ablation studies on two conversational emotion datasets (i.e., MELD and IEMOCAP). The results of the comparison experiments prove that the proposed CFN-ESA outperforms all baselines; the ablation studies verify that each component in CFN-ESA can effectively upgrade the performance of the model.

Theoretically, the visual information plays an instrumental role in providing direct cues for emotion recognition. Since vision often involves a lot of noise from complex environmental scenes, our approach, like most models, has difficulty capturing visual emotional information. In future work, we intend to further explore the method that fully utilizes the visual modality for emotion recognition.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,cfnesa.bib}












\end{document}
