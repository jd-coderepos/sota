    
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage[hidelinks]{hyperref}
\usepackage{url}
\usepackage{booktabs} \setcitestyle{square}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{array}

\title{DiffDock: Diffusion Steps, Twists, \\ and Turns for Molecular Docking}




\author{Gabriele Corso\thanks{Equal contribution. Correspondence to \texttt{\{gcorso, hstark, bjing\}@mit.edu}.} ,  Hannes St\"ark,  Bowen Jing,  Regina Barzilay \& Tommi Jaakkola  \\
CSAIL, Massachusetts Institute of Technology \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\newpar}{\marginpar{NEW}}

\iclrfinalcopy 

\usepackage{amsthm,lipsum,xcolor,todonotes}
\newtheorem{theorem}{Theorem}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\ff}{\mathbf{f}}
\renewcommand{\rr}{\mathbf{r}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\ww}{\mathbf{w}}
\renewcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\writing}[1]{\textcolor{blue}{#1}}
\makeatletter

\newcommand*{\@rowstyle}{}

\newcommand*{\rowstyle}[1]{\gdef\@rowstyle{#1}\@rowstyle\ignorespaces }

\newcolumntype{=}{>{\gdef\@rowstyle{}}}

\newcolumntype{+}{>{\@rowstyle}}

\makeatother
\newcommand{\new}[1]{#1}
\newcommand{\rebut}[1]{#1}
\newenvironment{rebuttal}{}{}
\DeclareMathOperator{\rmsd}{RMSD}
\DeclareMathOperator{\rmsdalign}{RMSDAlign}
\DeclareMathOperator{\uni}{Uni}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\xlig}{\mathbf{x}}
\newcommand{\cc}{\mathbf{c}}
\renewcommand{\ss}{\mathbf{s}}
\newcommand{\dd}{\mathbf{d}}
\newcommand{\pp}{\mathbf{p}}
\newcommand{\bb}{\mathbf{b}}
\renewcommand{\vv}{\mathbf{v}}
\newcommand{\lnorm}{\left|\left|}
\newcommand{\rnorm}{\right|\right|}
\usepackage[ruled,vlined]{algorithm2e}



\begin{document}

\maketitle

\begin{abstract}

Predicting the binding structure of a small molecule ligand to a protein---a task known as \emph{molecular docking}---is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a \emph{generative} modeling problem and develop \textsc{DiffDock}, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, \textsc{DiffDock} obtains a 38\% top-1 success rate (RMSD2\AA) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23\%) and deep learning (20\%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4\%), \textsc{DiffDock} maintains significantly higher precision (21.7\%). Finally, \textsc{DiffDock} has fast inference times and provides confidence estimates with high selective accuracy. 
\end{abstract}





\section{Introduction}

The biological functions of proteins can be modulated by small molecule ligands (such as drugs) binding to them. Thus, a crucial task in computational drug design is \emph{molecular docking}---predicting the position, orientation, and conformation of a ligand when bound to a target protein---from which the effect of the ligand (if any) might be inferred. Traditional approaches for docking \citep{trott2010autodock,halgren2004glide} rely on scoring-functions that estimate the correctness of a proposed structure or pose, and an optimization algorithm that searches for the global maximum of the scoring function. However, since the search space is vast and the landscape of the scoring functions rugged, these methods tend to be too slow and inaccurate, especially for high-throughput workflows.

Recent works \citep{equibind, Lu2022TankBind} have developed deep learning models to predict the binding pose in one shot, treating docking as a regression problem. While these methods are much faster than traditional search-based methods, they have yet to demonstrate significant improvements in accuracy. We argue that this may be because the regression-based paradigm  
corresponds imperfectly with the objectives of molecular docking, which is reflected in the fact that standard accuracy metrics resemble the \emph{likelihood} of the data under the predictive model rather than a regression loss. 
We thus frame molecular docking as a \textit{generative modeling problem}---given a ligand and target protein structure, we learn a distribution over ligand poses. 


To this end, we develop \textsc{DiffDock}, a diffusion generative model (DGM) over the space of ligand poses for molecular docking. We define a diffusion process over the degrees of freedom involved in docking: the position of the ligand relative to the protein (locating the binding pocket), its orientation in the pocket, and the torsion angles describing its conformation. \textsc{DiffDock} samples poses by running the learned (reverse) diffusion process, which iteratively transforms an uninformed, noisy prior distribution over ligand poses into the learned model distribution (Figure~\ref{fig:overview}). Intuitively, this process can be viewed as the progressive refinement of random poses via updates of their translations, rotations, and torsion angles.

While DGMs have been applied to other problems in molecular machine learning \citep{xu2021geodiff,jing2022torsional,hoogeboom2022equivariant}, existing approaches are ill-suited for molecular docking, where the space of ligand poses is an -dimensional submanifold , where  and  are, respectively, the number of atoms and torsion angles. To develop \textsc{DiffDock}, we recognize that the docking degrees of freedom define  as the space of poses accessible via a set of allowed \emph{ligand pose transformations}. We use this idea to map elements in  to the product space of the groups corresponding to those transformations, where a DGM can be developed and trained efficiently.

As applications of docking models often require only a fixed number of predictions and a confidence score over these, we train a \emph{confidence model} to provide confidence estimates for the poses sampled from the DGM and to pick out the most likely sample. This two-step process can be viewed as an intermediate approach between brute-force search and one-shot prediction: we retain the ability to consider and compare multiple poses without incurring the difficulties of high-dimensional search.

Empirically, on the standard blind docking benchmark PDBBind, \textsc{DiffDock} achieves 38\% of top-1 predictions with ligand root mean square distance (RMSD) below 2\AA{}, nearly doubling the performance of the previous state-of-the-art deep learning model (20\%). \textsc{DiffDock} significantly outperforms even state-of-the-art search-based methods (23\%), 
while still being 3 to 12 times faster on GPU. Moreover, it provides an accurate confidence score of its predictions, obtaining 8\new{3}\% RMSD2\AA{} on its most confident third of the previously unseen complexes.

We further evaluate the methods on structures generated by ESMFold \citep{Lin2022ESM2}. Our results confirm previous analyses \citep{wong2022benchmarking} that showed that existing methods are not capable of docking against these approximate apo-structures (RMSD2\AA{} equal or below 10\%). Instead, without further training, \textsc{DiffDock} places 22\% of its top-1 predictions within 2\AA{} opening the way for the revolution brought by accurate protein folding methods in the modeling of protein-ligand interactions.

To summarize, the main contributions of this work are:
\begin{enumerate}
    \item We frame the molecular docking task as a generative problem and highlight the issues with previous deep learning approaches.
    \item We formulate a novel diffusion process over ligand poses corresponding to the degrees of freedom involved in molecular docking.
    \item We achieve a new state-of-the-art 38\% top-1 prediction with RMSD2\AA{} on PDBBind blind docking benchmark, considerably surpassing the previous best search-based (23\%) and deep learning methods (20\%).
    \item Using ESMFold to generate approximate protein apo-structures, we show that our method places its top-1 prediction with RMSD2\AA{} on 28\% of the complexes, nearly tripling the accuracy of the most accurate baseline.
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/overview.pdf}
    \caption{Overview of \textsc{DiffDock}. \emph{Left}: The model takes as input the separate ligand and protein structures. \emph{Center}: Randomly sampled initial poses are denoised via a reverse diffusion over translational, rotational, and torsional degrees of freedom. \emph{Right:}. The sampled poses are ranked by the confidence model to produce a final prediction and confidence score.}
    \label{fig:overview}
\end{figure}


\section{Background and Related Work} \label{sec:background}

\textbf{Molecular docking.} The molecular docking task is usually divided between known-pocket and blind docking. Known-pocket docking algorithms receive as input the position on the protein where the molecule will bind (the \emph{binding pocket}) and only have to find the correct orientation and conformation. Blind docking instead does not assume any prior knowledge about the binding pocket; in this work, we will focus on this general setting. Docking methods typically assume the knowledge of the protein holo-structure (bound), this assumption is in many real-world applications unrealistic, therefore, we evaluate methods both with holo and computationally generated apo-structures (unbound). Methods are normally evaluated by the percentage of hits, or approximately correct predictions, commonly considered to be those where the ligand RMSD error is below 2\AA{} \citep{Alhossary2015QuickVina2, Hassan2017QVinaW, mcnutt2021gnina}. 



\textbf{Search-based docking methods.} Traditional docking methods \citep{trott2010autodock,halgren2004glide,thomsen2006moldock} consist of a parameterized physics-based scoring function and a search algorithm. The scoring-function takes in 3D structures and returns an estimate of the quality/likelihood of the given pose, while the search stochastically modifies the ligand pose (position, orientation, and torsion angles) with the goal of finding the scoring function's global optimum. Recently, machine learning has been applied to parameterize the scoring-function \citep{mcnutt2021gnina, mendez2021deepdock}. However, these search-based methods remain computationally expensive to run, are often inaccurate when faced with the vast search space characterizing blind docking \citep{equibind}, and significantly suffer when presented with apo-structures \citep{wong2022benchmarking}. 

\textbf{Machine learning for blind docking.} Recently, EquiBind \citep{equibind} has tried to tackle the blind docking task by directly predicting pocket keypoints on both ligand and protein and aligning them. TANKBind \citep{Lu2022TankBind} improved over this by independently predicting a docking pose (in the form of an interatomic distance matrix) for each possible pocket and then ranking 
them. Although these one-shot or few-shot regression-based prediction methods are orders of magnitude faster, their performance has not yet reached that of traditional search-based methods. 


\textbf{Diffusion generative models.} Let the data distribution be the initial distribution  of a continuous diffusion process described by , where  is the Wiener process. \emph{Diffusion generative models} (DGMs)\footnote{Also known as diffusion probabilistic models (DPMs), denoising diffusion probabilistic models (DDPMs), diffusion models, score-based models (SBMs), or score-based generative models (SGMs).} model the \emph{score}\footnote{Not to be confused with the \emph{scoring function} of traditional docking methods.}  of the diffusing data distribution in order to generate data via the reverse diffusion  \citep{song2021score}. In this work, we always take . Several DGMs have been developed for molecular ML tasks, including molecule generation \citep{hoogeboom2022equivariant}, conformer generation \citep{xu2021geodiff}, and protein design \citep{trippe2022diffusion}. However, these approaches learn distributions over the full Euclidean space  with 3 coordinates per atom, making them ill-suited for molecular docking where the degrees of freedom are much more restricted \new{(see Appendix \ref{appx:torsional})}. 


\section{Docking as Generative Modeling}\label{sec:generative_modeling}

Although EquiBind and other ML methods have provided strong runtime improvements by avoiding an expensive search process, their performance has not reached that of search-based methods. As our analysis below argues, this may be caused by the models' uncertainty and the optimization of an objective that does not correspond to how molecular docking is used and evaluated in practice. 

\textbf{Molecular docking objective.} Molecular docking plays a critical role in drug discovery because the prediction of the 3D structure of a bound protein-ligand complex enables further computational and human expert analyses on the strength and properties of the binding interaction. Therefore, a docked prediction is only useful if its deviation from the true structure does not significantly affect the output of such analyses. Concretely, a prediction is considered acceptable when the distance between the structures (measured in terms of ligand RMSD) is below some small tolerance on the order of the length scale of atomic interactions (a few \AA{}ngstr\"om). Consequently, the standard evaluation metric used in the field has been the percentage of predictions with a ligand RMSD (to the crystal ligand pose) below some value . 

However, the objective of maximizing the proportion of predictions with RMSD within some tolerance  is not differentiable and cannot be used for training with stochastic gradient descent. Instead, maximizing the expected proportion of predictions with RMSD  corresponds to maximizing the likelihood of the true structure under the model's output distribution, in the limit as  goes to 0. This observation motivates training a generative model to minimize an upper bound on the negative log-likelihood of the observed structures under the model's distribution. Thus, we view molecular docking as the problem of learning a distribution over ligand poses conditioned on the protein structure and develop a diffusion generative model over this space (Section~\ref{sec:diffusion_model}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/averaging.pdf}
    \caption{``\textsc{DiffDock} top-1" refers to the sample with the highest confidence. ``\textsc{DiffDock} samples" to the other diffusion model samples. \textit{Left:} Visual diagram of the advantage of generative models over regression models. Given uncertainty in the correct pose (represented by the orange distribution), regression models tend to predict the mean of the distribution, which may lie in a region of low density. \textit{Center:} when there is a global symmetry in the protein (aleatoric uncertainty), EquiBind places the molecule in the center while \textsc{DiffDock} is able to sample all the true poses. \textit{Right:} even in the absence of strong aleatoric uncertainty, the epistemic uncertainty causes EquiBind's prediction to have steric clashes and TANKBind's to have many self-intersections. }
    \label{fig:averaging}
\end{figure}

\textbf{Confidence model.} With a trained diffusion model, it is possible to sample an arbitrary number of ligand poses from the posterior distribution according to the model. However, researchers are often interested in seeing only one or a small number of predicted poses and an associated confidence measure\footnote{For example, the pLDDT confidence score of AlphaFold2 \citep{jumper2021highly} has had a very significant impact in many applications \citep{necci2021critical,bennett2022improving}.} for downstream analysis. Thus, we train a confidence model over the poses sampled by the diffusion model and rank them based on its confidence that they are within the error tolerance. The top-ranked ligand pose and the associated confidence are then taken as \textsc{DiffDock}'s top-1 prediction and confidence score.

\textbf{Problem with regression-based methods.} The difficulty with the development of deep learning models for molecular docking lies in the \new{aleatoric (which is the data inherent uncertainty, e.g., the ligand might bind with multiple poses to the protein) and epistemic uncertainty (which arises from the complexity of the task compared with the limited model capacity and data available) on the pose}. Therefore, given the available co-variate information (only protein structure and ligand identity), any method will exhibit uncertainty about the correct binding pose among many viable alternatives. Any regression-style method that is forced to select a single configuration that minimizes the expected square error would learn to predict the (weighted) mean of such alternatives. In contrast, a generative model with the same co-variate information would instead aim to capture the distribution over the alternatives, populating all/most of the significant modes even if similarly unable to distinguish the correct target. This behavior, illustrated in Figure~\ref{fig:averaging}, causes the regression-based models to produce significantly more physically implausible poses than our method. In particular, we observe frequent steric clashes (e.g., 26\% of EquiBind's predictions) and self-intersections in EquiBind's and TANKBind's predictions (Figures \ref{fig:self_intersections} and \ref{fig:random_examples}). We found no intersections in \textsc{DiffDock}'s predictions. 
Visualizations and quantitative evidence of these phenomena are in Appendix~\ref{appx:steric_clashes}.


\section{Method} \label{sec:diffusion_model}

\subsection{Overview}

A ligand pose is an assignment of atomic positions in , so in principle, we can regard a pose  as an element in , where  is the number of atoms. However, this encompasses far more degrees of freedom than are relevant in molecular docking. In particular, bond lengths, angles, and small rings in the ligand are essentially rigid, such that the ligand flexibility lies almost entirely in the torsion angles at rotatable bonds \new{(see Appendix \ref{appx:torsional} for further discussion)}. Traditional docking methods, as well as most ML ones, take as input a seed conformation  of the ligand in isolation and change only the relative position and the torsion degrees of freedom in the final bound conformation.\footnote{RDKit ETKDG is a popular method for predicting the seed conformation. Although the structures may not be predicted perfectly, the errors lie largely in the torsion angles, which are resampled anyways.} The space of ligand poses consistent with  is, therefore, an -dimensional submanifold , where  is the number of rotatable bonds, and the six additional degrees of freedom come from rototranslations relative to the fixed protein. We follow this paradigm of taking as input a seed conformation , and formulate molecular docking as learning a probability distribution  over the manifold , conditioned on a protein structure .

DGMs on submanifolds have been formulated by \citet{de2022riemannian} in terms of projecting a diffusion in ambient space onto the submanifold. However, the kernel  of such a diffusion is not available in closed form and must be sampled numerically with a geodesic random walk, making training very inefficient. We instead define a one-to-one mapping to another, ``nicer'' manifold where the diffusion kernel can be sampled directly and develop a DGM in that manifold. To start, we restate the discussion in the last paragraph as follows:
\begin{center}
\textit{Any ligand pose consistent with a seed conformation can be reached by a combination of\\ (1) ligand translations, (2) ligand rotations, and (3) changes to torsion angles.}
\end{center}
This can be viewed as an informal definition of the manifold . Simultaneously, it suggests that given a continuous family of ligand pose transformations corresponding to the  degrees of freedom, a distribution on  can be lifted to a distribution on the product space of the corresponding groups---which is itself a manifold. We will then show how to sample the diffusion kernel on this product space and train a DGM over it.


\subsection{Ligand Pose Transformations}


We associate translations of ligand position with the 3D translation group , rigid rotations of the ligand with the 3D rotation group , and changes in torsion angles at each rotatable bond with a copy of the 2D rotation group . More formally, we define operations of each of these groups on a ligand pose . The translation  is defined straightforwardly as  using the isomorphism  where  is the position of the th atom. Similarly, the rotation  is defined by  where , corresponding to rotations around the (unweighted) center of mass of the ligand.

Many valid definitions of a change in torsion angles are possible, as the torsion angle around any bond  can be updated by rotating the  side, the  side, or both. However, we can specify changes of torsion angles to be \emph{disentangled} from rotations or translations. To this end, we define the operation of elements of  such that it causes a minimal perturbation (in an RMSD sense) to the structure:\footnote{Since we do not define or use the composition of elements of , strictly speaking, it is a product \emph{space} but not a \emph{group} and can be alternatively thought of as the torus  with an origin element.}
\newtheorem*{definition*}{Definition}
\begin{definition*}
Let  be any valid torsion update by  around the th rotatable bond . We define such that

where  and

\end{definition*}
This means that we apply all the  torsion updates in any order and then perform a global RMSD alignment with the unmodified pose. The definition is motivated by ensuring that the infinitesimal effect of a torsion is orthogonal to any rototranslation, i.e., it induces no \emph{linear or angular momentum}. These properties can be stated more formally as follows (proof in Appendix~\ref{app:proofs}):

\newtheorem{proposition}{Proposition}
\begin{proposition}
Let  for some  and where . Then the linear and angular momentum are zero:  and  where .
\end{proposition}

Now consider the product space\footnote{Since we never compose elements of , we do not need to define a group structure.}  and define  as

These definitions collectively provide the sought-after product space corresponding to the docking degrees of freedom. Indeed, for a seed ligand conformation , we can formally define the space of ligand poses . This corresponds precisely to the intuitive notion of the space of ligand poses that can be reached by rigid-body motion plus torsion angle flexibility.

\subsection{Diffusion on the Product Space}

We now proceed to show how the product space can be used to learn a DGM over ligand poses in . First, we need a theoretical result (proof in Appendix~\ref{app:proofs}):
\begin{proposition}
    For a given seed conformation , the map  is a bijection.
\end{proposition}
which means that the inverse  given by  maps ligand poses  to points on the product space . We are now ready to develop a diffusion process on .

\citet{de2022riemannian} established that the DGM framework transfers straightforwardly to Riemannian manifolds with the score and score model as elements of the tangent space and with the geodesic random walk as the reverse SDE solver. Further, the score model can be trained in the standard manner with denoising score matching \citep{song2019generative}. Thus, to implement a diffusion model on , it suffices to develop a method for sampling from and computing the score of the diffusion kernel on . Furthermore, since  is a product manifold, the forward diffusion proceeds independently in each manifold \citep{rodola2019functional}, and the tangent space  is a direct sum:  where . Thus, it suffices to sample from the diffusion kernel and regress against its score in each group independently.

In all three groups, we define the forward SDE as  where , , or  for , , and  respectively and where  is the corresponding Brownian motion. Since , the translational case is trivial and involves sampling and computing the score of a standard Gaussian with variance . The diffusion kernel on  is given by the  distribution \citep{nikolayev1970normal, leach2022denoising}, which can be sampled in the axis-angle parameterization by sampling a unit vector  uniformly\footnote{ is the tangent space of  at the identity and is the space of Euler (or rotation) vectors, which are equivalent to the axis-angle parameterization.} and random angle  according to

Further, the score of the diffusion kernel is , where  is the result of applying Euler vector  to  \citep{yim2023se}. The score computation and sampling can be accomplished efficiently by precomputing the truncated infinite series and interpolating the CDF of , respectively. Finally, the  group is diffeomorphic to the torus , on which the diffusion kernel is a \emph{wrapped normal distribution} with variance . This can be sampled directly, and the score can be precomputed as a truncated infinite series \citep{jing2022torsional}.

\subsection{Training and Inference} \label{sec:train_inf}

\textbf{Diffusion model.} Although we have defined the diffusion kernel and score matching objectives on , we nevertheless develop the training and inference procedures to operate on ligand poses in 3D coordinates directly. Providing the full 3D structure, rather than abstract elements of the product space, to the score model allows it to reason about physical interactions using  equivariant models, not be dependent on arbitrary definitions of torsion angles \citep{jing2022torsional}, and better generalize to unseen complexes. In Appendix~\ref{app:train_inf}, we present the training and inference procedures and discuss how we resolve their dependence on the choice of seed conformation  used to define the mapping between  and the product space.

\textbf{Confidence model.} In order to collect training data for the confidence model , we run the trained diffusion model to obtain a set of candidate poses for every training example and generate labels by testing whether or not each pose has RMSD below 2\AA{}. The confidence model is then trained with cross-entropy loss to correctly predict the binary label for each pose. During inference, the diffusion model is run to generate  poses in parallel, which are passed to the confidence model that ranks them based on its confidence that they have RMSD below 2\AA{}. 


\subsection{Model Architecture} \label{sec:architecture}

We construct the score model  and the confidence model  to take as input the current ligand pose  and protein structure  in 3D space. The output of the confidence model is a single scalar that is -invariant (with respect to joint rototranslations of ) as ligand pose distributions are defined relative to the protein structure, which can have arbitrary location and orientation. On the other hand, the output of the score model must be in the tangent space . The space  corresponds to translation vectors and  to rotation (Euler) vectors, both of which are -equivariant. Finally,  corresponds to scores on -invariant quantities (torsion angles). Thus, the score model must predict two -equivariant vectors for the ligand as a whole and an -invariant scalar at each of the  freely rotatable bonds.

The score model and confidence model have similar architectures based on -equivariant convolutional networks over point clouds \citep{thomas2018tensor, e3nn}. However, the score model operates on a coarse-grained representation of the protein with -carbon atoms, while the confidence model has access to the all-atom structure. This multiscale setup yields improved performance and a significant speed-up w.r.t. doing the whole process at the atomic scale. The architectural components are summarized below and detailed in Appendix~\ref{app:architecture}.

Structures are represented as heterogeneous geometric graphs formed by ligand atoms, protein residues, and (for the confidence model) protein atoms. Residue nodes receive as initial features language model embeddings trained on protein sequences \citep{Lin2022ESM2}. Nodes are sparsely connected based on distance cutoffs that depend on the types of nodes being linked and on the diffusion time. The ligand atom representations after the final interaction layer are then used to produce the different outputs. To produce the two  vectors representing the translational and rotational scores, we convolve the node representations with a tensor product filter placed at the center of mass. For the torsional score, we use a pseudotorque convolution to obtain a scalar at each rotatable bond of the ligand analogously to \citet{jing2022torsional}, with the distinction that, since the score model operates on coarse-grained representations, the output is not a pseudoscalar (its parity is neither odd nor even). For the confidence model, the single scalar output is produced by mean-pooling the ligand atoms' scalar representations followed by a fully connected layer. 






\section{Experiments} \label{sec:experiments}

\begin{table}[t]
    \caption{\textbf{PDBBind blind docking.} All methods receive a small molecule and are tasked to find its binding location, orientation, and conformation. Shown is the percentage of predictions with RMSD  2\AA{} and the median RMSD (see Appendix \ref{appx:hyperparameters}). The top half contains methods that directly find the pose; the bottom half those that use a pocket prediction method. The last two lines show our method's performance. In parenthesis,     we specify the number of poses sampled from the generative model. * indicates that the method runs exclusively on CPU, ``-" means not applicable; some cells are empty due to infrastructure constraints. For TANKBind, the runtimes for the top-1 and top-5 predictions are different. No method received further training or tuning on ESMFold structures. Further evaluation details are in Appendix~\ref{appx:baseline_details} and further metrics are reported in Appendix~\ref{appx:additional_results}.}
    \label{tab:results_main}
     \begin{small}
     \begin{center}

    \begin{tabular}{lcc|cc|cc|cc|c}
    \toprule
      & \multicolumn{4}{c}{Holo crystal proteins} & \multicolumn{4}{c}{Apo ESMFold proteins} & \\ \rule{0pt}{2ex}
      
      & \multicolumn{2}{c}{Top-1 RMSD} & \multicolumn{2}{c}{Top-5 RMSD} & \multicolumn{2}{c}{Top-1 RMSD} & \multicolumn{2}{c}{Top-5 RMSD} & Average\\ \rule{0pt}{2ex}  
    
        Method & \%2 & Med. & \%2 & Med. & \%2 & Med. & \%2 & Med. &  Runtime (s)  \\
    \midrule
    \textsc{GNINA}              & 22.9 & 7.7  & 32.9 & 4.5  & 2.0 & 22.3  & 4.0 & 14.22 &  127  \\
    \textsc{SMINA}              & 18.7 & 7.1  & 29.3 & 4.6  & 3.4 & 15.4  & 6.9 & 10.0 &  126*   \\
    \textsc{GLIDE}              & 21.8 & 9.3  &      &      & & & & & 1405* \\
    \textsc{EquiBind}           & 5.5  & 6.2  &  -   &  -   & 1.7 & 7.1  & - & - &  0.04  \\ \midrule
    \textsc{TANKBind}           & 20.4 & 4.0  & 24.5 & 3.4  & 10.4 & 5.4 & 14.7 & 4.3 & 0.7/2.5  \\
    \textsc{P2Rank+SMINA}       & 20.4 & 6.9  & 33.2 & 4.4  & 4.6 & 10.0  & 10.3 & 7.0 & 126* \\ 
    \textsc{P2Rank+GNINA}       & 28.8 & 5.5  & 38.3 & 3.4  & 8.6 & 11.2  & 12.8 &  7.2 & 127  \\ 
    \textsc{EquiBind+SMINA}     & 23.2 & 6.5  & 38.6 & 3.4  & 4.3 & 8.3  & 11.7 & 5.8 & 126*  \\
    \textsc{EquiBind+GNINA}     & 28.8 & 4.9  & 39.1 & 3.1  & 10.2 & 8.8  & 18.6 & 5.6 & 127   \\ \midrule
\textbf{\textsc{DiffDock} (10)}  & 35.0 & 3.6  & 40.7 & 2.65  & \textbf{21.7} & \textbf{5.0}  & \textbf{31.9} & \textbf{3.3} &  10     \\
    \textbf{\textsc{DiffDock} (40)}  & \textbf{38.2} & \textbf{3.3}  & \textbf{44.7} & \textbf{2.40}  & 20.3 & 5.1  & 31.3 & \textbf{3.3} & 40      \\
    \bottomrule
    \end{tabular}
    \end{center}
    \end{small}
\vspace{-15pt}
\end{table}

\textbf{Experimental setup.} We evaluate our method on the complexes from PDBBind \citep{liu2017PDBBind}, a large collection of protein-ligand structures collected from PDB \citep{berman2003PDB}, which was used with time-based splits to benchmark many previous works  \citep{equibind, Volkov2022PDBBindSplits, Lu2022TankBind}. We compare \textsc{DiffDock} with state-of-the-art search-based methods SMINA \citep{koes2013smina}, QuickVina-W \citep{Hassan2017QVinaW}, GLIDE \citep{halgren2004glide}, and GNINA \citep{mcnutt2021gnina} \new{as well as the older Autodock Vina \citep{trott2010autodock}}, and the recent deep learning methods EquiBind and TANKBind presented above.  Extensive details about the experimental setup, data, baselines, and implementation are in Appendix~\ref{appx:baseline_details} and all code is available at \url{https://github.com/gcorso/DiffDock}. The repository also contains videos of the reverse diffusion process (images of the same are in Figure~\ref{fig:reverse_diffusion2}). 

As we are evaluating blind docking, the methods receive two inputs: the ligand with a predicted seed conformation (e.g., from RDKit) and the crystal structure of the protein. Since search-based methods work best when given a starting binding pocket to restrict the search space, we also test the combination of using an ML-based method, such as P2Rank \citep{krivak2018p2rank} (also used by TANKBind) or EquiBind to find an initial binding pocket, followed by a search-based method to predict the exact pose in the pocket. To evaluate the generated poses, we compute the heavy-atom RMSD (permutation symmetry corrected) between the predicted and the ground-truth ligand when the protein structures are aligned. Since all methods except for EquiBind generate multiple ranked structures, we report the metrics for the highest-ranked prediction as the top-1 prediction and top-5 refers to selecting the most accurate pose out of the 5 highest ranked predictions; a useful metric when multiple predictions are used for downstream tasks.

\textbf{Apo-structure docking setup.} In order to test the performance of the models against computationally generated apo-structures, we run ESMFold \citep{Lin2022ESM2} on the proteins of the test-set of PDBBind and align its predictions with the crystal holo-structure. In order to align structure changing as little as possible to the conformation of the binding site, we use in the alignment of the residues a weight exponential with the distance to the ligand. More details on this alignment are provided in Appendix~\ref{appx:apodocking}. The docking methods are then run against the generated structures and compared with the ligand pose inferred by the alignment.


\textbf{Docking accuracy.}  \textsc{DiffDock} significantly outperforms all previous methods (Table~\ref{tab:results_main}). In particular, \textsc{DiffDock} obtains an impressive 3\new{8.2}\% top-1 success rate (i.e., percentage of predictions with RMSD 2\AA{}\footnote{Most commonly used evaluation metric \citep{Alhossary2015QuickVina2, Hassan2017QVinaW, mcnutt2021gnina}}) when sampling 40 poses and 3\new{5.0}\% when sampling just 10. This performance vastly surpasses that of state-of-the-art commercial software such as \textsc{GLIDE} (21.8\%, \new{}) and the previous state-of-the-art deep learning method TANKBind (20.4\%, \new{}). The use of ML-based pocket prediction in combination with search-based docking methods improves over the baseline performances, but even the best of these (\new{EquiBind+GNINA})  reaches a success rate of only 28.8\% \new{()}. Unlike regression methods like EquiBind, \textsc{DiffDock} is able to provide multiple diverse predictions of different likely poses, as highlighted in the top-5 performances. 

\textbf{Apo-structure docking accuracy.} Previous work \cite{wong2022benchmarking}, that highlighted that existing docking methods are not suited to dock to (computational) apo-structures, are validated by the results in Table \ref{tab:results_main} where previous methods obtain top-1 accuracies of only 10\% or below. This is most likely due to their reliance on trying to find key-lock matches, which makes them inflexible to imperfect protein structures. Even when built-in options allowing side-chain flexibility are activated, the results do not improve (see Appendix~\ref{tab:sidechain_flex_baselines}). Meanwhile, \textsc{DiffDock} is able to retain a larger proportion of its accuracy, placing the top-ranked ligand below 2\AA{} away on 22\% of the complexes. This ability to generalize to imperfect structures, even without retraining, can be attributed to a combination of (1) the robustness of the diffusion model to small perturbations in the backbone atoms, and (2) the fact that \textsc{DiffDock} does not use the exact position of side chains in the score model and is therefore forced to implicitly model their flexibility.




\begin{figure}[t]
\begin{center}
\includegraphics[width=.43\textwidth]{figures/ablation_samples.pdf}
\hspace{15pt}
\includegraphics[width=.43\textwidth]{figures/confidence_calibration.pdf}
\vspace{-6pt}
\caption{\textbf{Left:}  \textsc{DiffDock}'s performance as a function of the number of samples from the generative model. ``Perfect selection" refers to choosing the sample with the lowest RMSD. \textbf{Right:} Selective accuracy: percentage of predictions with RMSD below 2\AA{} when only making predictions for the portion of the dataset where \textsc{DiffDock} is most confident.} 
\label{fig:results_main}
\end{center}
\vspace{-15pt}
\end{figure}





\textbf{Inference runtime.} \textsc{DiffDock} holds its superior accuracy while being (on GPU) 3 to 12 times faster than the best search-based method, GNINA (Table~\ref{tab:results_main}). This high speed is critical for applications such as high throughput virtual screening for drug candidates or reverse screening for protein targets, where one often searches over a vast number of complexes. As a diffusion model, \textsc{DiffDock} is inevitably slower than the one-shot deep learning method \textsc{EquiBind}, but as shown in Figure~\ref{fig:results_main}-\textit{left} and Appendix~\ref{app:ablations}, it can be significantly sped up without significant loss of accuracy. 










\textbf{Selective accuracy of confidence score.} As the top-1 results show, \textsc{DiffDock}'s confidence model is very accurate in ranking the sampled poses for a given complex and picking the best one. We also investigate the \textit{selective accuracy} of the confidence model across \emph{different} complexes by evaluating how \textsc{DiffDock}'s accuracy increases if it only makes predictions when the confidence is above a certain threshold, known as \textit{selective prediction}. In Figure~\ref{fig:results_main}-\textit{right}, we plot the success rate as we decrease the percentage of complexes for which we make predictions, i.e., increase the confidence threshold. When only making predictions for the top one-third of complexes in terms of model confidence, the success rate improves from 38\% to 8\new{3}\%. Additionally, there is a Spearman correlation of 0.\new{68} between \textsc{DiffDock}'s confidence and the negative RMSD. Thus, the confidence score is a good indicator of the quality of \textsc{DiffDock}'s top-ranked sampled pose and provides a highly valuable confidence measure for downstream applications. 



\section{Conclusion} \label{sec:conclusion}

We presented \textsc{DiffDock}, a diffusion generative model tailored to the task of molecular docking. This represents a paradigm shift from previous deep learning approaches, which use regression-based frameworks, to a generative modeling approach that is better aligned with the objective of molecular docking. To produce a fast and accurate generative model, we designed a diffusion process over the manifold describing the main degrees of freedom of the task via ligand pose transformations spanning the manifold. Empirically, \textsc{DiffDock} outperforms the state-of-the-art by large margins on PDBBind, has fast inference times, and provides confidence estimates with high selective accuracy. Moreover, unlike previous methods, it retains a large proportion of its accuracy when run on computationally folded protein structures. Thus, \textsc{DiffDock} can offer great value for many existing real-world pipelines and opens up new avenues of research on how to best integrate downstream tasks, such as affinity prediction, into the framework and apply similar ideas to protein-protein and protein-nucleic acid docking.


\section*{Acknowledgments}

We pay tribute to Octavian-Eugen Ganea (1987-2022), dear colleague, mentor, and friend without whom this work would have never been possible. 

We thank Rachel Wu, Jeremy Wohlwend, Felix Faltings, Jason Yim, Victor Quach, Saro Passaro, Patrick Walters, Michael Heinzinger, Mario Geiger, Michael John Arcidiacono, Noah Getz, and John Yang for valuable feedback and insightful discussions. We thank Wei Lu for his help with running TANKBind. We thank Valentin De Bortoli, Emile Mathieu, Brian Trippe, and Jason Yim for the critical discussions and help to formalize diffusion on . Noah Getz contributed to the implementation of the apo-structures alignment procedure. 

This work was supported by the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, the Abdul Latif Jameel Clinic for Machine Learning in Health, the DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats program, the DARPA Accelerated Molecular Discovery program and the Sanofi Computational Antibody Design grant. Bowen Jing acknowledges the support from the Department of Energy Computational Science Graduate Fellowship.

\bibliography{references}
\bibliographystyle{iclr2023_conference}
\newpage
\appendix

\section{Proofs} \label{app:proofs}

\subsection{Proof of Proposition 1}

\newtheorem{prop}{Proposition}
\begin{prop}
Let  for some  and where . Then the linear and angular momentum are zero:  and  where .
\end{prop}
\begin{proof}
    Let  where  =  and  are the rotation (around ) and translation associated with the optimal RMSD alignment between  and . By definition of , \rebut{for any ,  and  minimize}
    \begin{rebuttal}
    
    For infinitesimal , the RHS becomes
    
    where we have used , , and . Thus, we see that RMSD alignment implies that the derivatives of  minimize the norm of
    \end{rebuttal}
    This expression represents the instantaneous velocity of the points  at . \rebut{We now show that minimizing the velocity results in zero linear and angular momentum.}
    
    We abbreviate  and . Further, let , \rebut{such that the rotational contribution to the velocity can be written in terms of an angular velocity vector . With this, at  we have} 
    \begin{rebuttal}
    
    We thus obtain the squared norm as
    \end{rebuttal}
    
    where we have used the fact that  and where  is the  \emph{inertia tensor}. \rebut{To minimize the squared norm (and thus the norm itself), we set gradients with respect to  to zero. This gives}
    
    Now with  we evaluate the linear momentum
    
    which is zero by direct substitution of . Similarly, we evaluate the angular momentum
    
    which is zero by direct substitution of . Thus, the linear and angular momentum are zero at  for arbitrary . 
\end{proof}

Note that since we did not use the particular form of  in the above proof, we have shown that RMSD alignment can be used to disentangle rotations and translations from the infinitesimal action of any arbitrary function.

\subsection{Proof of Proposition 2}

\begin{prop}
    For a given seed conformation , the map  is a bijection.
\end{prop}
\begin{proof}
    Since we defined ,  is automatically surjective. We now show that it is injective. Assume for the sake of contradiction that  is not injective, so that there exist elements of the product space  with  but with . That is,
    
    which we abbreviate as . Since only  changes the center of mass , we have  and . However, since , this implies . Next, consider the torsion angles  of  corresponding to some choice of dihedral angles at each rotatable bond. Because  and  are rigid-body motions, only  changes the dihedral angles; in particular, by definition we have  and  for all . However, because , this means  for all  and therefore  (as elements of ). Now denote  and apply  to both sides of Equation \ref{eq:injective}. We then have
    
    which further leads to
    
    In general, this does not imply that . However,  is possible only if  is \emph{degenerate}, in the sense that all points are collinear along the shared axis of rotation of . However, in practice, conformers never consist of a collinear set of points, so we can safely assume . We now have , or , contradicting our initial assumption. We thus conclude that  is injective, completing the proof.
\end{proof}


\section{Training and Inference} \label{app:train_inf}

In this section we present the training and inference procedures of the diffusion generative model. First, however, there are a few subtleties of the generative approach to molecular docking that are worth mentioning. Unlike the standard generative modeling setting where the dataset consists of many samples drawn from the data distribution, each training example  of protein structure  and ground-truth ligand pose  is the \emph{only sample} from the corresponding conditional distribution  defined over . Thus, the innermost training loop iterates over distinct conditional distributions , along with a single sample from that distribution, rather than over samples from a common data distribution .



As discussed in Section~\ref{sec:diffusion_model}, during inference,  is the ligand structure generated with a method such as RDKit. However, during training we require  in order to define a bijection between  and . If we take , there will be a distribution shift between the manifolds  considered at training time and those considered at inference time. To circumvent this issue, at training time we predict  with RDKit and replace  with  using the conformer matching procedure described in \citet{jing2022torsional}.

The above paragraph may be rephrased more intuitively as follows: during inference, the generative model docks a ligand structure generated by RDKit, keeping its non-torsional degrees of freedom (e.g., local structures) fixed. At training time, however, if we train the score model with the local structures of the ground truth pose, this will not correspond to the local structures seen at inference time. Thus, at training time, we replace the ground truth pose by generating a ligand structure with RDKit and aligning it to the ground truth pose while keeping the local structures fixed.

With these preliminaries, we now continue to the full procedures (Algorithms~\ref{alg:training} and \ref{alg:inference}). The training and inference procedures of a score-based diffusion generative model on a Riemannian manifold consist of (1) sampling and regressing against the score of the diffusion kernel during training; and (2) sampling a geodesic random walk with the score as a drift term during inference \citep{de2022riemannian}. Because we have developed the diffusion process on  but continue to provide the score model with elements in , the full training and inference procedures involve repeatedly interconverting between the two spaces using the bijection given by the seed conformation .

\begin{algorithm}[h]
\caption{Training procedure (single epoch)}\label{alg:training}
\KwIn{Training pairs , RDKit predictions }
\ForEach{}{
    Let \;
    Compute \;
    Sample \;
    Sample  from diffusion kernels \;
    Set \;
    Set \;
    Set \;
    Compute \;
    Predict scores  \;
    Take optimization step on loss 
}
\end{algorithm}

\begin{algorithm}[h]
\caption{Inference procedure}\label{alg:inference}
\KwIn{RDKit prediction , protein structure  (both centered at origin)}
\KwOut{Sampled ligand pose }
Sample , , \;
Let \;
\For{n  \KwTo }{
    Let  and  and similarly for \;
    Predict scores \;
    Sample  from  respectively\;
    Set \;
    Set \;
    Set \;
    Compute \;
}
Return \;
\end{algorithm}

However, as noted in the main text, the dependence of these procedures on the exact choice of  is potentially problematic, as it suggests that at inference time, the model distribution may be different depending on the orientation and torsion angles of . Simply removing the dependence of the score model on  is not sufficient since the update steps themselves still occur on  and require a choice of  to be mapped to . However, notice that the update steps---in both training and inference---consist of (1) sampling the diffusion kernels at the origin; (2) applying these updates to the point on ; and (3) transferring the point on  to  via . Might it instead be possible to apply the updates to 3D ligand poses  \emph{directly}?

It turns out that the notion of applying these steps to ligand poses ``directly'' corresponds to the formal notion of \emph{group action}. The operations  that we have already defined are formally group actions if they satisfy . While true for , this is not generally true for  if we take  to be the direct product group; however, the approximation is increasingly good as the magnitude of the torsion angle updates decreases. If we then define  to be the direct product group of its constituent groups,  is a group action of  on , as the operations of  commute and are (under the approximation) individually group actions.

The implication of  being a group action can be seen as follows. Let  be the update which brings  to  via left multiplication, and let  be the corresponding ligand poses . Then

which means that the updates  can be applied directly to  using the operation . The training and inference procedures then become Algorithm \ref{alg:approx-training} and \ref{alg:approx-inference} below. The initial conformer  is no longer used, except in the initial steps to define the manifold---to find the closest point to  in training, and to sample  from the prior over  in inference.

Conceptually speaking, this procedure corresponds to ``forgetting'' the location of the origin element on , which is permissible because a change of the origin to some equivalent seed  merely translates---via right multiplication by ---the original and diffused data distributions on , but does not cause any changes on  itself. The training and inference routines involve updates---formally left multiplications---to group elements, but as left multiplication on the group corresponds to group actions on , the updates can act on  directly, without referencing the origin .

We find that the approximation of  as a group action works quite well in practice and use Algorithms \ref{alg:approx-training} and \ref{alg:approx-inference} for all training and experiments discussed in the paper. Of course, disentangling the torsion updates from rotations in a way that makes  exactly a group action would justify the procedure further, and we regard this as a possible direction for future work.

\begin{algorithm}[h]
\caption{Approximate training procedure (single epoch)}\label{alg:approx-training}
\KwIn{Training pairs , RDKit predictions }
\ForEach{}{
    Let \;
    Sample \;
    Sample  from diffusion kernels \;
    Compute \;
    Predict scores  \;
    Take optimization step on loss 
}
\end{algorithm}

\begin{algorithm}[h]
\caption{Approximate inference procedure}\label{alg:approx-inference}
\KwIn{RDKit prediction , protein structure  (both centered at origin)}
\KwOut{Sampled ligand pose }
Sample , , \;
Let \;
\For{n  \KwTo }{
    Let  and  and similarly for \;
    Predict scores \;
    Sample  from  respectively\;
    Set \;
    Set \;
    Set \;
    Compute \;
}
Return \;
\end{algorithm}



\section{Architecture Details} \label{app:architecture}

As summarized in Section~\ref{sec:architecture}, we use convolutional networks based on tensor products of irreducible representations (irreps) of SO(3) \citep{thomas2018tensor} as architecture for both the score and confidence models. In particular, these are implemented using the \verb|e3nn| library \citep{e3nn}. Below,  refers to the spherical tensor product of irreps with path weights , and  refers to normal vector addition (with possibly padded inputs). Features have multiple channels for each irrep.
Both the architectures can be decomposed into three main parts: embedding layer, interaction layers, and output layer. We outline each of them below.

\subsection{Embedding layer}

\textbf{Geometric heterogeneous graph.} Structures are represented as heterogeneous geometric graphs with nodes representing ligand (heavy) atoms, receptor residues (located in the position of the -carbon atom), and receptor (heavy) atoms (only for the confidence model). Because of the high number of nodes involved, it is necessary for the graph to be sparsely connected for runtime and memory constraints. Moreover, sparsity can act as a useful inductive bias for the model, however, it is critical for the model to find the right pose that nodes that might have a strong interaction in the final pose to be connected during the diffusion process. Therefore, to build the radius graph, we connect nodes using cutoffs that are dependent on the types of nodes they are connecting:
\begin{enumerate}
    \item Ligand atoms-ligand atoms, receptor atoms-receptor atoms, and ligand atoms-receptor atoms interactions all use a cutoff of 5\AA{}, standard practice for atomic interactions. For the ligand atoms-ligand atoms interactions we also preserve the covalent bonds as separate edges with some initial embedding representing the bond type (single, double, triple and aromatic). For receptor atoms-receptor atoms interactions, we limit at 8 the maximum number of neighbors of each atom. Note that the ligand atoms-receptor atoms only appear in the confidence model where the final structure is already set.
    \item Receptor residues-receptor residues use a cutoff of 15 \AA{} with 24 as the maximum number of neighbors for each residue.
    \item Receptor residues-ligand atoms use a cutoff of  \AA{} where  represents the current standard deviation of the diffusion translational noise present in each dimension (zero for the confidence model). Intuitively this guarantees that with high probability, any of the ligands and receptors that will be interacting in the final pose the diffusion model converges to are connected in the message passing at every step.
    \item Finally, receptor residues are connected to the receptor atoms that form the corresponding amino-acid.
\end{enumerate}


\textbf{Node and edge featurization.} For the receptor residues, we use the residue type as a feature as well as a language model embedding obtained from ESM2 \citep{Lin2022ESM2}. The ligand atoms have the following features: atomic number; chirality; degree; formal charge; implicit valence; the number of connected hydrogens; the number of radical electrons; hybridization type; whether or not it is in an aromatic ring; in how many rings it is; and finally, 6 features for whether or not it is in a ring of size 3, 4, 5, 6, 7, or 8. These are concatenated with sinusoidal embeddings of the diffusion time \citep{vaswani2017attention} and, in the case of edges, radial basis embeddings of edge length \citep{schutt2017schnet}. These scalar features of each node and edge are then transformed with learnable two-layer MLPs (different for each node and edge type) into a set of scalar features that are used as initial representations by the interaction layers.

\textbf{Notation} Let  represent the heterogeneous graph, with  respectively ligand atoms and receptor residues (receptor atoms , present in the confidence model, are for simplicity not included here), and similarly . Let  be the node embeddings (initially only scalar channels) of node ,  the edge embeddings of , and  radial basis embeddings of the edge length. Let , , and  represent the variance of the diffusion kernel in each of the three components: translational, rotational and torsional.


\subsection{Interaction layers}

At each layer, for every pair of nodes in the graph, we construct messages using tensor products of the current node features with the spherical harmonic representations of the edge vector. The weights of this tensor product are computed based on the edge embeddings and the \emph{scalar} features---denoted ---of the outgoing and incoming nodes. The messages are then aggregated at each node and used to update the current node features. For every node  of type :

Here,  indicates an arbitrary node type,  the neighbors of  of type ,  are the spherical harmonics up to , and  the (equivariant) batch normalisation. The orders of the output are restricted to a maximum of . All learnable weights are contained in , a dictionary of MLPs, which uses different sets of weights for different edge types (as an ordered pair so four types for the score model and nine for the confidence) and different rotational orders. Convolutional layers simultaneously operate with different sets of weights for different connection types (so 9 sets of weights for the confidence model and 4 for the score at every layer) and generate scalar and vector representations for each node. 

\subsection{Output layer} 

The ligand atom representations after the final interaction layer are used in the output layer to produce the required outputs. This is where the score and confidence architecture differ significantly. On one hand, the score model's output is in the tangent space . This corresponds to having two -equivariant output vectors representing the translational and rotational score predictions and  -invariant output scalars representing the torsional score. For each of these, we design final tensor-product convolutions inspired by classical mechanics. On the other hand, the confidence model outputs a single -invariant scalar representing the confidence score. Below we detail how each of these outputs is generated.

\textbf{Translational and rotational scores.} The translational and rotational score intuitively represent, respectively, the linear acceleration of the center of mass of the ligand and the angular acceleration of the rest of the molecule around the center. Considering the ligand as a rigid object and given a set of forces and masses at each ligand, a tensor product convolution between the atoms and the center of mass would be capable of computing the desired quantities. Therefore, for each of the two outputs, we perform a convolution of each of the ligand atoms with the (unweighted) center of mass . 
We restrict the output of  to a single odd and a single even vectors (for each of the two scores). Since we are using coarse-grained representations of the protein, the score will neither be even nor odd; therefore, we sum the even and odd vector representations of . Finally, the magnitude (but not direction) of these vectors is adjusted with an MLP taking as input the current magnitude and the sinusoidal embeddings of the diffusion time. Finally, we (revert the normalization) by multiplying the outputs by  for the translational score and by the expected magnitude of a score in  with diffusion parameter  (precomputed numerically). 

\textbf{Torsional score. } To predict the  -invariant scalar describing the torsional score, we use a pseudotorque layer similar to that of \citet{jing2022torsional}. This predicts a scalar score  for each rotatable bond from the per-node outputs of the atomic convolution layers. For rotatable bond  and , let  and  be the magnitude and direction of the vector connecting the center of bond  and . We construct a convolutional filter  for each bond  from the tensor product of the spherical harmonics with a  representation of the {bond axis} :\footnote{Since the parity of the  spherical harmonic is even, this representation is indifferent to the choice of bond direction.}

 is the full (i.e., unweighted) tensor product as described in \citet{geiger2022e3nn}, and the second term contains the spherical harmonics up to  (as usual). This filter (which contains orders up to ) is then used to convolve with the representations of every neighbor on a radius graph:

Here,  and  and  are MLPs with learnable parameters. Since unlike \citet{jing2022torsional}, we use coarse-grained representations the parity also here is neither even nor odd, the irreps in the output are restricted to arrays both even  and odd  scalars. Finally, we produce a single scalar prediction for each bond:

where  is a two-layer MLP with  nonlinearity and no biases. This is also ``denormalized" by multiplying by the expected magnitude of a score in  with diffusion parameter .

\textbf{Confidence output.} The single -invariant scalar representing the confidence score output is instead obtained by concatenating the even and odd final scalar representation of each ligand atom, averaging these feature vectors among the different atoms, and finally applying a three layers MLP (with batch normalization). 


\section{Experimental Details} \label{app:exp_details}
In general, all our code is available at \url{https://github.com/gcorso/DiffDock}. This includes running the baselines, runtime calculations, training and inference scripts for \textsc{DiffDock}, the PDB files of \textsc{DiffDock}'s predictions for all 363 complexes of the test set, and visualization videos of the reverse diffusion.

\subsection{Experimental Setup} \label{appx:experimental_setup}
\textbf{Data.} We use the molecular complexes in PDBBind \citep{liu2017PDBBind} that were extracted from the Protein Data Bank (PDB) \citep{berman2003PDB}. We employ the time-split of PDBBind proposed by \citet{equibind} with 17k complexes from 2018 or earlier for training/validation and 363 test structures from 2019 with no ligand overlap with the training complexes. This is motivated by the further adoption of the same split \citep{Lu2022TankBind} and the critical assessment of PDBBind splits by \citet{Volkov2022PDBBindSplits} who favor temporal splits over artificial splits based on molecular scaffolds or protein sequence/structure similarity. For completeness, we also report the results on protein sequence similarity splits in Appendix~\ref{appx:additional_results}. \new{We download the PDBBind data as it is provided by EquiBind from \texttt{https://zenodo.org/record/6408497}. These files were preprocessed with Open Babel before adding any potentially missing hydrogens, correcting hydrogens, and correctly flipping histidines with the \texttt{reduce} library available at \texttt{https://github.com/rlabduke/reduce}.}


\textbf{Metrics.} To evaluate the generated complexes, we compute the heavy-atom RMSD between the predicted and the crystal ligand atoms when the protein structures are aligned. To account for permutation symmetries in the ligand, we use the symmetry-corrected RMSD of sPyRMSD \citep{spyrmsd2020}. For these RMSD values, we report the percentage of predictions that have an RMSD that is less than 2\AA{}.  We choose 2\AA{} since much prior work considers poses with an RMSD less that 2\AA{} as ``good" or successful \citep{Alhossary2015QuickVina2, Hassan2017QVinaW, mcnutt2021gnina}. This is a chemically relevant metric, unlike the mean RMSD as detailed in Section~\ref{sec:generative_modeling} since for further downstream analyses such as determining function changes, a prediction is only useful below a certain RMSD error threshold. Less relevant metrics such as the mean RMSD are provided in Appendix~\ref{appx:additional_results}.


\subsection{Apo-structure docking}\label{appx:apodocking}

Although large and comprehensive, the PDBBind benchmark only evaluates the capacity that various docking methods have to bind ligands to their corresponding receptor holo-structure. This is a much simpler and less realistic scenario than what is typically encountered in real applications where docking for new ligands is done against apo or holo-structures bound to a different ligand. In particular, since the development of accurate protein folding methods \citep{jumper2021highly}, docking programs are often run on top of AI-generated protein structures. With this in mind, we develop a new benchmark where we combine the complex prediction of PDBBind with protein structures generated by ESMFold \citep{Lin2022ESM2}. 

The structures were obtained running \texttt{esmfold\_v1} on the sequences extracted from the PDB protein files of PDBBind. Protein complexes were passed to ESMFold by concatenating (with \texttt{':'}) the sequences of the proteins and removing all water molecules and other ligands. Predictions were run on 48GB A6000 GPUs, but for 12/361 complexes the prediction ran out of memory even when reducing the \texttt{chunk\_size} and were, therefore, discarded.   

The main design choice when generating this benchmark relies on how to best align the PDBBind complex with the ESMFold structure to obtain the "ground-truth" docked prediction on the ESMFold structure. An unbiased global alignment of the two protein structures is not desirable because a difference in structure not affecting the pocket where the ligand binds would cause the two pockets to misalign; on the other hand, only aligning residues within a single arbitrary pocket cutoff has many undesirable cases where too many or too few residues are selected or not weighted properly. 

Instead, we align receptors' residues with the Kabsch algorithm using exponential weighting, for every receptor  its weight is  where  is a smoothing factor and  is the minimum distance of  to a ligand atom in the original complex, this way residues closer to the ligand will have a higher weight in the alignment. For each complex, we individually select  so that it preserves distances as best as possible, in particular, we use the L-BFGS-B \citep{byrd1995limited} from \texttt{scipy} \citep{virtanen2020scipy} to minimize:

where  and  correspond to the distances between protein residue  and ligand atom  respectively in the original crystal structure from PDBBind and in the complex structure obtained aligning the ESMFold structure with smoothing parameter . We use inverse distances to give more importance to residues closer to the ligand (in either structure) and avoid steric clashes. We only consider protein backbones because the side-chain predictions are often less reliable and their structure typically changes upon binding.

Thus we obtain protein structures on which we run the docking methods and the associated docked ligand positions that we use to evaluate them. 


\subsection{Implementation details: hyperparameters, training, and runtime measurement} \label{appx:hyperparameters}
\textbf{Training Details.} We use Adam \citep{kingma2014adam} as optimizer for the diffusion and the confidence model. The diffusion model with which we run inference uses the exponential moving average of the weights during training, and we update the moving average after every optimization step with a decay factor of 0.999. The batch size is 16. We run inference with 20 denoising steps on 500 validation complexes every 5 epochs and use the set of weights with the highest percentage of RMSDs less than 2\AA{} as the final diffusion model. We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days). The confidence model is trained on a single 48GB GPU. For inference, only a single GPU is required. Scaling up the model size seems to improve performance and future work could explore whether this trend continues further. For the confidence model uses the validation cross-entropy loss is used for early stopping and training only takes 75 epochs. Code to reproduce all results including running the baselines or to perform docking calculations for new complexes is available at \url{https://github.com/gcorso/DiffDock}.

\textbf{Hyperparameters.} For determining the hyperparameters of \textsc{DiffDock}'s score model, we trained smaller models (3.97 million parameters) that fit into 48GB of GPU RAM before scaling it up to the final model (20.24 million parameters) that was trained on four 48GB GPUs. The smaller models were only trained for 250 or 300 epochs, and we used the fraction of predictions with an RMSD below 2\AA{} on the validation set to choose the hyperparameters. Table~\ref{tab:hyperparameters} shows the main hyperparameters we tested and the final parameters of the large model we use to obtain our results. We only did little tuning for the minimum and maximum noise levels of the three components of the diffusion. For the translation, the maximum standard deviation is 19\AA{}. We also experimented with second-order features for the Tensor \new{F}ield Network but did not find them to help. The complete set of hyperparameters next to the main ones we describe here can be found in our repository. \new{From the start we have divided the inference schedule into 20 time steps, the effect of using more or fewer steps for inference is discussed in Appendix \ref{app:ablations}. As we found that the large-scale diffusion models overfit the training data on low-levels of noise we stop the diffusion early after 18 steps. At the last diffusion step no noise is added.}

The confidence model has 4.77 million parameters and the parameters we tried are in Table~\ref{tab:hyperparameters_confidence_model}. We generate 28 different training poses for the confidence model (for which it predicts whether or not they have an RMSD below 2\AA{}) with a \new{small} score model. The score model used to generate the training samples for the confidence model \new{does not need to be the same one that the model will be applied to at inference time.}


\begin{table}[htpb]
\caption[SearchSpace]{The hyperparameter options we searched through for \textsc{DiffDock}'s score model. This was done with small models before scaling up to a large model. The parameters shown here that impact model size (bottom half of the table) are those of the large model. The final parameters for the large \textsc{DiffDock} model are marked in \textbf{bold}.}
\label{tab:hyperparameters}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lc}
\toprule
Parameter & Search Space  \\    
\midrule
using all atoms for the protein graph & Yes, \textbf{No}\\
using language model embeddings & \textbf{Yes}, No\\
using ligand hydrogens & Yes, \textbf{No}\\
using exponential moving average & \textbf{Yes}, {No}\\
maximum number of neighbors in protein graph & 10, 16, \textbf{24}, 30\\
maximum neighbor distance in protein graph & 5, 10, \textbf{15}, 18, 20, 30\\
distance embedding method & \textbf{sinusoidal}, gaussian \\
dropout & 0, 0.05, \textbf{0.1}, 0.2 \\
learning rates & 0.01, 0.008, 0.003, \textbf{0.001}, 0.0008, 0.0001\\
batch size & 8, \textbf{16}, 24\\
non linearities & \textbf{ReLU} \\
\midrule
convolution layers & 6 \\
number of scalar features &  48 \\
number of vector features &  10 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table}[htpb]
\caption[SearchSpace]{The hyperparameter options we searched through for \textsc{DiffDock}'s confidence model. The final parameters are marked in \textbf{bold}.}
\label{tab:hyperparameters_confidence_model}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lc}
\toprule
Parameter & Search Space  \\    
\midrule
using all atoms for the protein graph & \textbf{Yes}, {No}\\
using language model embeddings & \textbf{Yes}, No\\
using ligand hydrogens & \textbf{No}\\
using exponential moving average & \textbf{No}\\
maximum number of neighbors in protein graph & 10, 16, \textbf{24}, 30\\
maximum neighbor distance in protein graph & 5, 10, \textbf{15}, 18, 20, 30\\
distance embedding method & \textbf{sinusoidal} \\
dropout & 0, 0.05, \textbf{0.1}, 0.2 \\
learning rates & 0.03, 0.003, \textbf{0.0003}, 0.00008\\
batch size & \textbf{16}\\
non linearities & \textbf{ReLU} \\
\midrule
convolution layers & 5 \\
number of scalar features &  24 \\
number of vector features &  6 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

 


\textbf{Runtime.} Similar to all the baselines, the preprocessing times are not included in the reported runtimes. For \textsc{DiffDock} the preprocessing time is negligible compared to the rest of the inference time where multiple reverse diffusion steps are performed. Preprocessing mainly consists of a forward pass of ESM2 to generate the protein language model embeddings, RDKit's conformer generation, and the conversion of the protein into a radius graph. We measured the inference time when running on an RTX A100 40GB GPU when generating 10 samples. The runtimes we report for generating 40 samples and ranking them are extrapolations where we multiply the runtime for 10 samples by 4. In practice, this only gives an upper bound on the runtime with 40 samples, and the actual runtime should be faster. 


\textbf{Statistical significance.} To determine the statistical significance of the superior performance of our method we used the paired two-sample t-test implemented in \texttt{scipy} \citep{2020SciPy-NMeth}. 

\subsection{Baselines: implementation, used scripts, and runtime details} \label{appx:baseline_details}
Our scripts to run the baselines are available at \url{https://github.com/gcorso/DiffDock}. For obtaining the runtimes of the different methods, we always used 16 CPUs except for GLIDE as explained below. The runtimes do not include any preprocessing time for any of the methods. For instance, the time that it takes to run P2Rank is not included for TANKBind, and P2Rank + SMINA/GNINA since this receptor preparation only needs to be run once when docking many ligands to the same protein. In applications where different receptors  are processed (such as reverse screening), the experienced runtimes for TANKBind and P2Rank + SMINA/GNINA will thus be higher.

We note that for all these baselines we have used the default hyperparameters unless specified differently below. Modifying some of these hyperparameters (for example the scoring method's exhaustiveness) will change the runtime and performance tradeoffs (e.g., if the searching routine is left running for longer then better poses are likely to be found), however, we leave these analyses to future work.

\textbf{SMINA} \citep{koes2013smina} improves Autodock Vina with a new scoring-function and user-friendliness. The default parameters were used with the exception of setting \texttt{--num\_modes 10}. To define the search box, we use the automatic box creation option around the receptor with the default buffer of 4\AA{} on all 6 sides.

\textbf{GNINA} \citep{mcnutt2021gnina} builds on SMINA by additionally using a learned 3D CNN for scoring. The default parameters were used with the exception of setting \texttt{--num\_modes 10}. To define the search box, we use the automatic box creation option around the receptor with the default buffer of 4\AA{} on all 6 sides.


\textbf{QuickVina-W} \citep{Hassan2017QVinaW} extends the speed-optimized QuickVina 2 \citep{Alhossary2015QuickVina2} for blind docking. We reuse the numbers from \citet{equibind} which had used the default parameters except for increasing the exhaustiveness to 64. \new{The files were preprocessed with the \texttt{prepare\_ligand4.py} and \texttt{prepare\_receptor4.py} scripts of the MGLTools library as it is recommended by the QuickVina-W authors.}

\new{\textbf{Autodock Vina} \citep{trott2010autodock} is older docking software that does not perform as well as the other more recent search-based baselines, but it is a well-established tool. We reuse the numbers reported in TANKBind \citep{Lu2022TankBind}}


\textbf{GLIDE} \citep{halgren2004glide} is a strong heavily used commercial docking tool. These methods all use biophysics based scoring-functions. We reuse the numbers from \citet{equibind} since we do not have a license. \new{Running GLIDE involves running their command line tools for preprocessing the structures into the files required to run the docking algorithm.} As explained by \citet{equibind}, the very high runtime of GLIDE with 1405 seconds per complex is partially explained by the fact that GLIDE only uses a single thread when processing a complex. This fact and the parallelization options of GLIDE are explained here \url{https://www.schrodinger.com/kb/1165}. With GLIDE, it is possible to start data-parallel processes that compute the docking results for a different complex in parallel. However, each process also requires a separate software license. 


\textbf{EquiBind} \citep{equibind}, we reuse the numbers reported in their paper and generate the predictions that we visualize with their code at \url{https://github.com/HannesStark/EquiBind}.

\textbf{TANKBind} \citep{Lu2022TankBind}, we use the code associated with the paper at \url{https://github.com/luwei0917/TankBind}. The runtimes do not include the runtime of P2Rank or any preprocessing steps. In Table~\ref{tab:results_main} we report two runtimes (0.72/2.5 sec). The first is the runtime when making only the top-1 prediction and the second is for producing the top-5 predictions. Producing only the top-1 predictions is faster since TANKBind produces distance predictions that need to be converted to coordinates with a gradient descent algorithm and this step only needs to be run once for the top-1 prediction, while it needs to be run 5 times for producing 5 outputs. To obtain our runtimes we run the forward pass of TANKBind on GPU (0.28 seconds) with the default batch size of 5 that is used in their GitHub repository. To compute the time the distances-to-coordinates conversion step takes, we run the file \texttt{baseline\_run\_tankbind\_parallel.sh} in our repository, which parallelizes the computation across 16 processes which we also run on an Intel Xeon Gold 6230 CPU. This way, we obtain 0.44 seconds runtime for the conversion step of the top-1 prediction (averaged over the 363 complexes of the testset). 

\textbf{P2Rank} \citep{krivak2018p2rank}, is a tool that predicts multiple binding pockets and ranks them. We use it for running TANKBind and P2Rank + SMINA/GNINA. We download the program from \url{https://github.com/rdk/p2rank} and run it with its default parameters.

\textbf{EquiBind + SMINA/GNINA} \citep{equibind}, the bounding box in which GNINA/SMINA searches for binding poses is constructed around the prediction of EquiBind with the \texttt{--autobox\_ligand} option of GNINA/SMINA. EquiBind is thus used to find the binding pocket and SMINA/GNINA to find the exact final binding pose. We use \texttt{--autobox\_add 10} to add an additional 10\AA{} on all 6 sides of the bounding box following \citep{equibind}.

\textbf{EquiBind + SMINA/GNINA} The same as EquiBind + SMINA/GNINA but with the flexibility in terms of torsion angles activated for the sidechains that have an atom within 3.5\AA{} of the output from EquiBind.

\textbf{P2Rank + SMINA/GNINA.} The bounding box in which GNINA/SMINA searches for binding poses is constructed around the pocket center that P2Rank predicts as the most likely binding pocket. P2Rank is thus used to find the binding pocket and SMINA/GNINA to find the exact final binding pose. The diameter of the search box is the diameter of a ligand conformer generated by RDKit with an additional 10\AA{} on all 6 sides of the bounding box.

\textbf{P2Rank + SMINA/GNINA} The same as P2Rank + SMINA/GNINA but with the flexibility in terms of torsion angles activated for the sidechains that have an atom within 3.5\AA{} + the radius of the ligand away from the pocket center of P2Rank. Additionally we consider at most 10 flexible sidechains with \texttt{--flex\_max 10}.

\textbf{SMINA/GNINA + SMINA/GNINA} The same as EquiBind + SMINA/GNINA but the initial ligand is provided by SMINA/GNINA ran without sidechain flexibility. The flexibility is in terms of torsion angles activated for the sidechains that have an atom within 3.5\AA{} of the output from SMINA/GNINA.

\section{\new{Additional Discussion}} \label{appx:discussion}

\subsection{\new{Access to Bound Protein Structure}}\label{appx:apo_holo}

\new{One limitation of \textsc{DiffDock} is that it assumes access to the bound structure of the protein known as holo-structure. Although most of the literature in molecular docking makes this assumption, in practice, one often only has access to the unbound apo protein structure or the holo structure of the protein bound to a different ligand. Tackling the problem of binding to apo structures is challenging due to the limited amount of data of unbound structures that have an atom-to-atom correspondence to the holo structure with which they could be aligned. In \textsc{DiffDock} the limitation of docking to holo structures may be less pronounced than for the search based docking methods that we compare against since \textsc{DiffDock}'s score model only uses the positions of the alpha carbon atoms (and not the side chain atoms). Thus \textsc{DiffDock} would also work well for binding to apo structures when most of the conformational change during binding lies in the side chains and the backbone stays mostly rigid. However, in order to fully model binding to apo structures, one needs to additionally model protein flexibility, we leave the full treatment of this problem to future work. }

\subsection{\new{Torsional vs Euclidean Flexibility}}\label{appx:torsional}
\begin{rebuttal}
A ligand pose (and more generally speaking, a molecule conformation) consists of a position in 3D-space  for each of  atoms of the molecule and can thus be considered as an element of . Each such pose (or conformation), can alternatively be described in terms of its bond lengths, angles, and torsion angles (see \citet{jing2022torsional}, Appendix A for a more formal discussion). Because of the nature of covalent interactions, bond lengths and angles (local structures) are highly energetically constrained, and any specific bond length or angle in a molecule will take on only a very narrow range of values, whether in isolation or bound to a protein receptor. Thus, ligand poses or conformations in which bond lengths or angles are \emph{strained}---i.e., differ significantly from their standard values---can be easily judged to be energetically unfavorable. On the other hand, the energetic profile of varying torsion angles is significantly smoother, as they depend in large part on weaker, noncovalent interactions. A change of chemical environment, or an interaction with another molecule (such as a protein) can alter these profiles. Thus, we say that there is significant \emph{flexibility} in the torsion angles, in contrast to the more rigid local structures.

Since bond lengths and angles are highly constrained, the set of chemically plausible ligand poses is a very small subset of all possible assignments of atoms to ; that is, a very restricted subset of . The set of ligand poses that satisfy all the bond length and angle constraints with equality appears as a lower-dimensional \emph{manifold}. To a good approximation, all actual ligand poses lie on this manifold. Thus, we are interested in developing a generative model on this manifold, as any sample outside of it is chemically nonsensical.

Futher, the constrained values of standard bond lengths and angles are easily and quickly predicted using standard cheminformatics routines, such as those in RDKit. Indeed, far simpler rules of thumb---a fixed length for each type of bonding pair, and fixed angles determined solely by molecular geometry---are already quite accurate and widely taught. Consequently, the manifold of plausible poses is very easy to find---one merely has to generate \emph{any} plausible conformation or ligand pose with RDKit. In previous work on conformer generation, \citet{jing2022torsional} verified that the manifolds corresponding to local structures generated by RDKit are, on average, less than 0.5 \AA\ RMSD away from the true conformations.

The facts that (1) the space of plausible ligand poses is described by a manifold and (2) this manifold is easy to find, motivate the development of a diffusion generative model on the manifold rather than the full ambient space . This significantly reduces the dimensionality of the state space, and thus of the function that the score network needs to approximate. \end{rebuttal}


\section{Additional Results} \label{appx:additional_results}

\subsection{Physically plausible predictions}\label{appx:steric_clashes}

\begin{table}[htb]
    \caption{\textbf{Steric clashes.} Percentage of test complexes for which the predictions of the different methods exhibit steric clashes. Search-based methods never produced steric clashes.}
    \label{tab:steric_clashes}
     \begin{small}
     \begin{center}

    \begin{tabular}{lcc}
    \toprule
      & Top-1  & Top-5 \\
    
        Method & \% steric clashes  & \% steric clashes\\
    \midrule
    \textsc{EquiBind}           & 26  & -   \\
    \textsc{TANKBind}           & 6.6 & 3.6   \\ \midrule
    \textbf{\textsc{DiffDock} (10)}  & \textbf{2.8} & \textbf{0}  \\
    \textbf{\textsc{DiffDock} (40)}  & \textbf{2.2} & \textbf{2.2}  \\
    \bottomrule
    \end{tabular}
    \end{center}
    \end{small}
\end{table}

Due to the averaging phenomenon of regression-based methods such as TANKBind and EquiBind, they make predictions at the mean of the distribution. If aleatoric uncertainty is present, such as in case of symmetric complexes, this leads to predicting the ligand to be at an un-physical state in the middle of the possible binding pockets as visualized in Figure~\ref{fig:symmetric_complexes}. The Figure also illustrates how \textsc{DiffDock} does not suffer from this issue and is able to accurately sample from the modes.

In the scenario when epistemic uncertainty about the correct ligand conformation is present, this often results in ``squashed-up" predictions of the regression-based methods as visualized in Figure~\ref{fig:self_intersections}. If there is uncertainty about the correct conformer, the square error minimizing option is to put all atoms close to the mean.



These averaging phenomena in the presence of either aleatoric or epistemic uncertainty cause the regression-based methods to often generate steric clashes and self intersections. To investigate this quantitatively, we determine the fraction of test complexes for which the methods exhibit steric clashes. We define a ligand as exhibiting a steric clash if one of its heavy atoms is within 0.4\AA{} of a heavy receptor atom. This cutoff is used by protein quality assessment tools and in previous literature \citep{Ramachandran2011stericClashes}. Table~\ref{tab:steric_clashes} shows that \textsc{DiffDock}, as a generative model, produces fewer steric clashes than the regression-based baselines. We generally observe no unphysical predictions from \textsc{DiffDock} unlike the self intersections that, e.g., TANKBind produces (Figure~\ref{fig:self_intersections}) or its incorrect local structures (Figure~\ref{fig:plausible_local_structures}). This is also visible in the randomly chosen examples of Figure~\ref{fig:random_examples} and can be examined in our repository, where we provide all predictions of \textsc{DiffDock} for the test set.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/ligand_examples.png}
    \caption{\textbf{Ligand self-intersections.} TANKBind (blue), EquiBind (cyan), \textsc{DiffDock} (red), and crystal structure (green). Due to the averaging phenomenon that occurs when epistemic uncertainty is present, the regression-based deep learning models tend to produce ligands with atoms that are close together, leading to self-intersections. \textsc{DiffDock}, as a generative model, does not suffer from this averaging phenomenon, and we never found a self-intersection in any of the investigated results of \textsc{DiffDock}.}
    \label{fig:self_intersections}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/tankbind_equibind_poor_local_strucutres.png}
    \caption{\textbf{Chemically plausible local structures.} TANKBind (blue), EquiBind (cyan), and \textsc{DiffDock} (red) structures for complex 6g2f. EquiBind (without their correction step) produces very unrealistic local structures and TANKBind, e.g., produces non-planar aromatic rings. \textsc{DiffDock}'s local structures are the realistic local structures of RDKit. }
    \label{fig:plausible_local_structures}
\end{figure}

\subsection{Further Results and Metrics}

In this section, we present further evaluation metrics on the results presented in Table~\ref{tab:results_main}. In particular, for both top-1 (Table~\ref{tab:top1complete}) and top-5 (Table~\ref{tab:top5complete}) we report: 25th, 50th and 75th percentiles, the proportion below 2\AA{} and below 5\AA{} of both ligand RMSD and centroid distance. Moreover, while \citet{Volkov2022PDBBindSplits} advocated against artificial protein set splits and for time-based splits, for completeness, in Table~\ref{tab:results_unseen} and Figure~\ref{fig:histogram_unseen_rec}, we report the performances of the different methods when evaluated exclusively on the portion of the test set where the UniProt IDs of the proteins are not contained in the data that is seen by \textsc{DiffDock} in its training and validation. 

To assess the impact of the molecule size on the performance of \textsc{DiffDock} and GNINA, we provide scatter plots in Figure \ref{fig:size_vs_rmsd} showing that the correlation between RMSD and the number of rotatable bonds or the number of atoms in the molecule is similar for both methods. In Figure \ref{fig:tanimoto_similarity} we show that, based on Tanimoto similarity, the performance of \textsc{DiffDock} does not depend on the test ligand's similarity to the ligands that have already been seen during training. The Spearman rank correlation coefficient between the RMSD and the Tanimoto similarity to the closest ligand in the training set is negligible with it being -0.031.

Further, in Table \ref{tab:sidechain_flex_baselines}, we provide the performance of SMINA and GNINA in the apo-structure docking setting when the flexibility of sidechains in the pocket is turned on. These show that simply adding sidechain flexibility does not help these methods. Finally, in Figure \ref{fig:esm_by_rmsd}, we plot the relationship between the quality of the ESMFold structure and the performance of the docking methods on apo and holo-structures. While \textsc{DiffDock} retains a large part of its accuracy when the protein backbone is approximately correct, very small variations in the apo-structure backbone (and sidechains) causes search-based methods like GNINA to almost never find the right pose.

\vspace{-6pt}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=.48\textwidth]{figures/rmsds.pdf}
\hspace{5pt}
\includegraphics[width=.48\textwidth]{figures/rmsds_esm.pdf}
\vspace{-6pt}
\caption{Cumulative density histogram of the methods' RMSD: left on holo crystal structures, right on apo ESMFold structures. } 
\label{fig:cum_densities}
\end{center}
 \vskip -0.1cm
\end{figure}

\begin{table*}[htb]
\caption{\textbf{Top-1 PDBBind docking.}  }
    \label{tab:top1complete}
\begin{small}
     \begin{center}
     \makebox[\textwidth][c]{
    \begin{tabular}{=l+c+c+c+c+c|+c+c+c+c+c}
    
    \toprule
     &\multicolumn{5}{c}{Ligand RMSD} & \multicolumn{5}{c}{Centroid Distance} \\
&\multicolumn{3}{c}{Percentiles } & \multicolumn{2}{c}{\begin{tabular}{@{}c@{}}\% below\\threshold \end{tabular} }  &\multicolumn{3}{c}{Percentiles } & \multicolumn{2}{c}{\begin{tabular}{@{}c@{}}\% below\\thresh. \end{tabular} } \\


    \textbf{Methods} & 25th & 50th & 75th & 5 \AA{}  &  2 \AA{} & 25th & 50th & 75th & 5 \AA{}  &  2 \AA{} \\
    \midrule
    \textsc{Autodock Vina} & 5.7  &  10.7 &  21.4 & 21.2 & 5.5 & 1.9 &  6.2 & 20.1 & 47.1 & 26.5 \\
     \textsc{QVina-W} & 2.5  &  7.7 &  23.7 & 40.2 & 20.9 & 0.9 &  3.7 & 22.9 & 54.6 & 41.0 \\
     \textsc{GNINA} & 2.4 & 7.7   & 17.9 & 40.8  & 22.9 & 0.8 &  3.7 & 23.1 & 53.6 & 40.2 \\
    \textsc{SMINA} & 3.1 & 7.1 & 17.9 & 38.0 & 18.7 & 1.0 &  2.6 & 16.1 & 59.8 & 41.6 \\
    \textsc{GLIDE} (c.) & 2.6 & 9.3   & 28.1 & 33.6 & 21.8 & 0.8 &  5.6 & 26.9 & 48.7 & 36.1 \\
    \textsc{EquiBind} & 3.8 & 6.2 &  10.3 & 39.1 & 5.5 &  1.3 & 2.6 & 7.4 & 67.5&  40.0 \\ \midrule
    \textsc{TANKBind} & 2.5 & 4.0 & 8.5 & 59.0 & 20.4 & 0.9 & 1.8 & 4.4 & 77.1 &  55.1  \\ 
    \textsc{P2Rank+SMINA} & 2.9 & 6.9 & 16.0 & 43.0 & 20.4 & 0.8 & 2.6 & 14.8 & 60.1 & 44.1   \\ 
    \textsc{P2Rank+GNINA} & 1.7 & 5.5 & 15.9 & 47.8 & 28.8 & 0.6 & 2.2 & 14.6 & 60.9 & 48.3   \\ 
    \textsc{EquiBind+SMINA} & 2.4 & 6.5 & 11.2 & 43.6 & 23.2 & 0.7 & 2.1 & 7.3 & 69.3 & 49.2  \\ 
    \textsc{EquiBind+GNINA} & 1.8 & 4.9 & 13 & 50.3 & 28.8 & 0.6 & 1.9 & 9.9   & 66.5 & 50.8  \\ \midrule 
    \textbf{\textsc{DiffDock} (10)} & 1.5 & 3.6 & \textbf{7.1} & 61.7 & 35.0 & \textbf{0.5} & \textbf{1.2} & 3.3 & \textbf{80.7} & 63.1   \\ 
    \textbf{\textsc{DiffDock} (40)} & \textbf{1.4} & \textbf{3.3} & 7.3 & \textbf{63.2} & \textbf{38.2} & \textbf{0.5} & \textbf{1.2} & \textbf{3.2} & 80.5 & \textbf{64.5}   \\ 
     \bottomrule
    \end{tabular}}
    \end{center}
    \end{small}
\end{table*}


\begin{table*}[htb]
\caption{\textbf{Top-5 PDBBind docking.}  }
    \label{tab:top5complete}
\begin{small}
     \begin{center}
     \makebox[\textwidth][c]{
    \begin{tabular}{=l+c+c+c+c+c|+c+c+c+c+c}
    
    \toprule
     &\multicolumn{5}{c}{Ligand RMSD} & \multicolumn{5}{c}{Centroid Distance} \\
&\multicolumn{3}{c}{Percentiles } & \multicolumn{2}{c}{\begin{tabular}{@{}c@{}}\% below\\threshold \end{tabular} }  &\multicolumn{3}{c}{Percentiles } & \multicolumn{2}{c}{\begin{tabular}{@{}c@{}}\% below\\thresh. \end{tabular} } \\


    \textbf{Methods} & 25th & 50th & 75th & 5 \AA{}  &  2 \AA{} & 25th & 50th & 75th & 5 \AA{}  &  2 \AA{} \\
    \midrule
    \textsc{GNINA} & 1.6 & 4.5 & 11.8 & 52.8 & 29.3 & 0.6 & 2.0 & 8.2 & 66.8 & 49.7   \\
    \textsc{SMINA} & 1.7 & 4.6 & 9.7 & 53.1 & 29.3 & 0.6 & 1.85 & 6.2 & 72.9 & 50.8  \\ \midrule
    \textsc{TANKBind} & 2.1 & 3.4 & 6.1 & 67.5 & 24.5 & 0.8 & 1.4 & 2.9 & 86.8 & 62.0  \\ 
    \textsc{P2Rank+SMINA} & 1.5 & 4.4 & 14.1 & 54.8 & 33.2 & 0.6 & 1.8 & 12.3 & 66.2 & 53.4   \\ 
    \textsc{P2Rank+GNINA} & 1.4 & 3.4 & 12.5 & 60.3 & 38.3 & 0.5 & 1.4 & 9.2 & 69.3 & 57.3  \\ 
    \textsc{EquiBind+SMINA} & 1.3 & 3.4 & 8.1 & 60.6 & 38.6 & 0.5 & 1.3 & 5.1 & 74.9 & 58.9   \\ 
    \textsc{EquiBind+GNINA} & 1.4 & 3.1 & 9.1 & 61.7 & 39.1 & 0.5 & 1.1 & 5.3 & 73.7 & 60.1  \\ \midrule 
    \textbf{\textsc{DiffDock} (10)} & \textbf{1.2} & 2.7 & \textbf{4.9} & 75.1 & 40.7 & 0.5 & 1.0 & 2.2 & 87.0 & 72.3   \\ 
    \textbf{\textsc{DiffDock} (40)} & \textbf{1.2} & \textbf{2.4} & 5.0 & \textbf{75.5} & \textbf{44.7} & \textbf{0.4} & \textbf{0.9} & \textbf{1.9} & \textbf{88.0} & \textbf{76.7}    \\ 
     \bottomrule
    \end{tabular}}
    \end{center}
    \end{small}
\end{table*}



\begin{figure}[htb]
\begin{center}
\includegraphics[width=.48\textwidth]{figures/rmsds_nooverlap.pdf}
\hspace{5pt}
\includegraphics[width=.48\textwidth]{figures/unseen_confidence_calibration.pdf}
\caption{\new{\textbf{PDBBind docking on unseen receptors.} \textbf{Left:} cumulative density histogram of the methods' RMSD. \textbf{Right:} Percentage of predictions with RMSD below 2\AA{} when only making predictions for the portion of the dataset where \textsc{DiffDock} is most confident. }} 
\label{fig:histogram_unseen_rec}
\end{center}
 \vskip -0.1cm
\end{figure}

\begin{table}[htb]
    \caption{\textbf{PDBBind docking on unseen receptors.} Percentage of predictions for which the RMSD to the crystal structure is below 2\AA{} and the median RMSD. ``*" indicates the method run exclusively on CPU, ``-" means not applicable; some cells are empty due to infrastructure constraints. }
    \label{tab:results_unseen}
     \begin{small}
     \begin{center}

    \begin{tabular}{l=c+c|+c+c|c}
    \toprule
      & \multicolumn{2}{c}{Top-1 RMSD} & \multicolumn{2}{c}{Top-5 RMSD}  &  Average\\
    
        Method & \,\%2\, & \,Med.\, & \,\%2\, & \,Med.\, & \,Runtime (s)\, \\
    \midrule
     \textsc{Autodock Vina}             &  1.4 & 16.6 &      &      &   205*     \\
    \textsc{QVinaW}             &  15.3 & 10.3 &      &      &  49*     \\
    \textsc{GNINA}              &  14.0 & 13.6 & 23.0 & 7.0  &  127    \\
    \textsc{SMINA}              &  14.0 & 8.5  & 21.7 & 6.7  &  126*    \\
    \textsc{GLIDE}              &  19.6 & 18.0 &      &      &  1405*   \\
    \textsc{EquiBind}           &  0.7  & 9.1  &  -   &  -   &  \textbf{0.04}   \\ 
    \textsc{TANKBind}           &  6.3  & \textbf{5.0}  & 11.1 & 4.4  &  0.7/2.5  \\ \midrule
    \textbf{\textsc{DiffDock} (10)}  &  15.7 & 6.1 & 21.8 & 4.2 & 10  \\   
    \textbf{\textsc{DiffDock} (40)}  &  \textbf{20.8} & 6.2  & \textbf{28.7} & \textbf{3.9}  & 40   \\
    \bottomrule
    \end{tabular}
    \end{center}
    \end{small}
\end{table}


\begin{figure}[htb]
\begin{center}
\includegraphics[width=.48\textwidth]{figures/num_atoms_vs_rmsd.pdf}
\includegraphics[width=.48\textwidth]{figures/num_rotatable_vs_rmsd.pdf}
\caption{\new{\textbf{Relationship between the number of atoms and RMSD.} \textbf{Left:} Scatter plot of the RMSD and the number of atoms. \textbf{Right:} Scatter plot of the RMSD and the number of rotatable bonds in the ligand. }} 
\label{fig:size_vs_rmsd}
\end{center}
 \vskip -0.1cm
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/tanimoto_similarity_ranked.pdf}
    \caption{Scatterplot for the RMSD and the Tanimoto similarity of the test ligand to the closest ligand in the training data. The order by maximum Tanimoto similarity is such that the lowest similarity is at the left and the highest similarity is at the right on the x-axis.}
    \label{fig:tanimoto_similarity}
\end{figure}


\begin{table}[htb]
    \caption{\textbf{PDBBind blind docking on apo proteins with sidechain flexibility in baselines.} All methods receive a small molecule and are tasked to find its binding location, orientation, and conformation. Shown is the percentage of predictions with RMSD  2\AA{} and the median RMSD (see Appendix \ref{appx:hyperparameters}). In parenthesis, we specify the number of poses sampled from the generative model. * indicates that the method runs exclusively on CPU. No method received further training or tuning on ESMFold structures.}
    \label{tab:sidechain_flex_baselines}
     \begin{small}
     \begin{center}

    \begin{tabular}{lcc|cc|c}
    \toprule
      & \multicolumn{4}{c}{Apo ESMFold proteins} & \\ \rule{0pt}{2ex}
      
       & \multicolumn{2}{c}{Top-1 RMSD} & \multicolumn{2}{c}{Top-5 RMSD} & Average\\ \rule{0pt}{2ex}  
    
        Method  & \%2 & Med. &  \%2 & Med.  &  Runtime (s)  \\
    \midrule
    \textsc{P2Rank+SMINA}        & 5.7 & 9.6  & 13.4 & 6.3 & 292* \\ 
    \textsc{P2Rank+GNINA}        & 8.3 & 10.9  & 13.4 &  6.4 & 294 \\ 
    \textsc{EquiBind+SMINA}      & 4.3 & 7.3  & 11.7 & 5.8 & 1145* \\
    \textsc{EquiBind+GNINA}      & 6.6 & 9.8  & 14.6 & 6.1 & 1208   \\ 
    \textsc{SMINA+SMINA}      & 3.4 & 12.6  & 8.3 & 11.6 & 1145*  \\
    \textsc{GNINA+GNINA}      & 1.7 & 22.1  & 5.1 & 20.0 & 1208    \\ \midrule
    \textbf{\textsc{DiffDock} (10)}    & \textbf{21.7} & \textbf{5.0}  & \textbf{31.9} & \textbf{3.3} &  10     \\
    \textbf{\textsc{DiffDock} (40)}    & 20.3 & 5.1  & 31.3 & \textbf{3.3} & 40      \\
    \bottomrule
    \end{tabular}
    \end{center}
    \end{small}
\vspace{-12pt}
\end{table}

\clearpage


\begin{figure}[htb]
\begin{center}
\includegraphics[width=.65\textwidth]{figures/esm_by_rmsd.pdf}
\caption{\textbf{Relationship between the ESMFold and docking accuracy.} The test set complexes on PDBBind are divided into three similar-sized groups based on the RMSD of the pocket residues (defined as < 10 \AA{} to a ligand atom) in the aligned ESMFold generated structure. The performance of \textsc{DiffDock} and GNINA are shown when docking to both the ground truth crystal structure and the ESMFold structure. The baseline methods' performance on crystal structures is also negatively correlated with the ESMFold accuracy because the complexes where the methods do badly tend to be larger and with fewer examples in PDB(Bind).  }
\label{fig:esm_by_rmsd}
\end{center}
 \vskip -0.1cm
\end{figure}


\subsection{Ablation studies} \label{app:ablations}

Below we report the performance of our method over different hyperparameter settings. In particular, we highlight the different ways in which it is possible to control the tradeoff between runtime and accuracy in our method. These mainly are: (1) model size, (2) diffusion time, and (3) diffusion samples.

\textbf{Model size.}  The final \textsc{DiffDock} score model has 20.24 million parameters from its 6 convolution layers with 48 scalar and 10 vector features. In Table~\ref{tab:model_size} we show the results for a smaller score model with 5 convolutions, 24 scalar, and 6 vector features resulting in 3.97 million parameters that can be trained on a single 48GB GPU. The confidence model used is the same for both score models. We find that scaling up the model size helped improve performance which we did as far as possible using four 48GB GPUs for training. Scaling the model size further is a promising avenue for future work.

\new{\textbf{Protein embeddings.}  As described in Section \ref{sec:architecture} and Appendix \ref{app:architecture}, the architecture uses as initial features of protein residues the language model embeddings from ESM2 \citep{Lin2022ESM2} in order for the model to more easily reason about the protein sequence. In Table~\ref{tab:model_size} we show that while these provide some improvements they are not necessary to obtain state-of-the-art performance.}


\begin{table}[htb]
    \caption{\textbf{Model size \new{and protein embeddings} comparison.} All methods receive a small molecule and are tasked to find its binding location, orientation, and conformation. Shown is the percentage of predictions for which the RMSD to the crystal structure is below 2\AA{} and the median RMSD.}
    \label{tab:model_size}
     \begin{small}
     \begin{center}

    \begin{tabular}{l=c+c|+c+c|+c}
    \toprule
      & \multicolumn{2}{c}{Top-1 RMSD (\AA{})} & \multicolumn{2}{c}{Top-5 RMSD (\AA{})} & Average\\ \rule{0pt}{1.5ex}  
    
        Method & \,\%2\, & \,Med.\, & \,\%2\, & \,Med.\, & Runtime (s)  \\
    \midrule
     \textbf{\textsc{DiffDock-small-noESM} (10)}  &  26.2 & 4.7  & 32.0 & 3.2  &  7      \\
     \textbf{\textsc{DiffDock-small-noESM} (40)}  &   28.4 & 3.8  & 37.7 & 2.6  & 28      \\ \midrule
    \textbf{\textsc{DiffDock-small} (10)}  &  26.0 & 4.3  & 33.3 & 3.2  &  7      \\
    \textbf{\textsc{DiffDock-small} (40)}  &  31.1 & 4.0  & 38.0 & 2.7  & 28      \\
    \midrule
     \textbf{\textsc{DiffDock-noESM} (10)}  &  33.9 & 3.8  & 39.4 & 2.8  & 10      \\
     \textbf{\textsc{DiffDock-noESM} (40)}  &  34.2 & 3.5  & 42.7 & 2.4  & 40      \\ \midrule
    \textbf{\textsc{DiffDock} (10)}  &  35.0 & 3.6  & 40.7 & 2.7  &  10     \\
    \textbf{\textsc{DiffDock} (40)}  &  \textbf{38.2} & \textbf{3.3}  & \textbf{44.7} & \textbf{2.4}  & 40      \\
    \bottomrule
    \end{tabular}
    \end{center}
    \end{small}
\vspace{-0.2cm}
\end{table}


\textbf{Diffusion steps. } Another hyperparameter determining the runtime of the method during inference is the number of steps we take during the reverse diffusion. Since these are applied sequentially \textsc{DiffDock}'s runtime scales approximately linearly with the number of diffusion steps. In the rest of the paper, we always use 20 steps, but in Figure~\ref{fig:diffusion_steps} we show how the performance of the model varies with the number of steps. We note that the model reaches nearly the full performance even with just 10 steps, suggesting that the model can be sped up 2x with a small drop in accuracy.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/diffusion_steps.pdf}
    \caption{ Ablation study on the number of reverse diffusion steps. }
    \label{fig:diffusion_steps}
\end{figure}



\textbf{Diffusion samples. } Given a score-based model and a number of steps for the diffusion model, it remains to be determined how many independent samples  to query from the diffusion model and then feed to the confidence model. As expected the more samples the confidence model receives the more likely it is that it will find a pose that it is confident about and, therefore, the higher the performance. The runtime of \textsc{DiffDock} on GPU scales sublinearly until the different samples fit in parallel in the model (depends on the protein size and the GPU memory) and approximately linearly for larger sample sizes (however it can be easily parallelized across different GPUs). In Figure~\ref{fig:results_main} we show how the success rate for the top-1, top-5, and top-10 prediction change as a function of . For example, for the top-1 prediction, the proportion of the prediction with RMSD below 2\AA{} varies between 22\% of a random sample of the diffusion model () to 38\% when the confidence model is allowed to choose between 40 samples.


\clearpage
\subsection{Visualizations}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figures/random_visualizations.png}
    \caption{\textbf{Randomly picked examples.} The predictions of TANKBind (blue), EquiBind (cyan), GNINA (magenta), \textsc{DiffDock} (red), and crystal structure (green). Shown are the predictions once with the protein and without it below. The complexes were chosen with a random number generator from the test set. TANKBind often produces self intersections (examples at the top-right; middle-middle; middle-right; bottom-right). \textsc{DiffDock} and GNINA sometimes almost perfectly predict the bound structure (e.g., top-middle). The complexes in reading order are: 6p8y, 6mo8, 6pya, 6t6a, 6e30, 6hld, 6qzh, 6hhg, 6qln.}
    \label{fig:random_examples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/symmetric_complexes.png}
    \caption{\textbf{Symmetric complexes and multiple modes.} EquiBind (cyan), \textsc{DiffDock} highest confidence sample (red), all other \textsc{DiffDock} samples (orange), and the crystal structure (green). We see that, since it is a generative model, \textsc{DiffDock} is able to produce multiple correct modes and to sample around them. Meanwhile, as a regression-based model, EquiBind is only able to predict a structure at the mean of the modes. The complexes are unseen during training. The PDB IDs in reading order: 6agt, 6gdy, 6ckl, 6dz3.}
    \label{fig:symmetric_complexes}
\end{figure}



\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/reverse_diffusion2.png}
    \caption{\textbf{Reverse Diffusion.} Reverse diffusion of a randomly picked complex from the test set. Shown are \textsc{DiffDock} highest confidence sample (red), all other \textsc{DiffDock} samples (orange), and the crystal structure (green). Shown are the 20 steps of the reverse diffusion process (in reading order) of \textsc{DiffDock} for the complex 6oxx. Videos of the reverse diffusion are available at \url{https://github.com/gcorso/DiffDock/visualizations/README.md}.}
    \label{fig:reverse_diffusion2}
\end{figure}






\end{document}
