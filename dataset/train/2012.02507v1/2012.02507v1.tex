\def\year{2021}\relax
\documentclass[letterpaper]{article} \usepackage{aaai21}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  

\usepackage{amsmath}  
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[switch]{lineno}
\usepackage{appendix}

\newcommand*\patchAmsMathEnvironmentForLineno[1]{\expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
  \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
  \renewenvironment{#1}{\linenomath\csname old#1\endcsname}{\csname oldend#1\endcsname\endlinenomath}}\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{\patchAmsMathEnvironmentForLineno{#1}\patchAmsMathEnvironmentForLineno{#1*}}\AtBeginDocument{\patchBothAmsMathEnvironmentsForLineno{equation}\patchBothAmsMathEnvironmentsForLineno{align}}

\pdfinfo{
/Title (Coarse-to-Fine Entity Representations for Document-level Relation Extraction)
/Author ()
/TemplateVersion (2021.1)
} 



\setcounter{secnumdepth}{2} 







\title{Coarse-to-Fine Entity Representations for Document-level Relation Extraction}

\author{
    Damai Dai\textsuperscript{1}\thanks{Equal Contribution.},
    Jing Ren\textsuperscript{1}\footnotemark[1],
    Shuang Zeng\textsuperscript{1},
    \textbf{Baobao Chang\textsuperscript{1,2},
    Zhifang Sui\textsuperscript{1,2}} \\
}

\affiliations{
    \textsuperscript{1}MOE Key Lab of Computational Linguistics, School of EECS, Peking University \\
    \textsuperscript{2}Peng Cheng Laboratory, China \\
    \{daidamai,rjj,zengs,chbb,szf\}@pku.edu.cn
}








\begin{document}


\maketitle

\begin{abstract}

Document-level Relation Extraction (RE) requires extracting relations expressed within and across sentences. 
Recent works show that graph-based methods, usually constructing a document-level graph that captures document-aware interactions, can obtain useful entity representations thus helping tackle document-level RE. 
These methods either focus more on the entire graph, or pay more attention to a part of the graph, e.g., paths between the target entity pair. 
However, we find that document-level RE may benefit from focusing on both of them simultaneously. 
Therefore, to obtain more comprehensive entity representations, we propose the \textbf{C}oarse-to-\textbf{F}ine \textbf{E}ntity \textbf{R}epresentation model (\textbf{CFER}) that adopts a coarse-to-fine strategy involving two phases. 
First, CFER uses graph neural networks to integrate global information in the entire graph at a coarse level. 
Next, CFER utilizes the global information as a guidance to selectively aggregate path information between the target entity pair at a fine level. 
In classification, we combine the entity representations from both two levels into more comprehensive representations for relation extraction. 
Experimental results on a large-scale document-level RE dataset show that CFER achieves better performance than previous baseline models. 
Further, we verify the effectiveness of our strategy through elaborate model analysis.\footnote{We will release our code once the paper is officially accepted. }

\end{abstract}

\section{Introduction}

Relation Extraction (RE) aims to extract semantic relations between named entities from plain text. 
It is an efficient way to acquire structured knowledge automatically, thus benefiting various natural language processing (NLP) applications~\citep{re4kbqa1,re4kbqa2,re4kbp}, especially knowledge graph construction~\citep{scierc}. 
Most of the previous RE works focus on the sentence level, i.e., they extract the relations within only a single sentence~\citep{cnn_re,pcnn_ds_re,bilstm_re,attention_ds_re}. 
However, in real-world scenarios, sentence-level RE models may omit some inter-sentence relations while a considerable number of relations are expressed beyond a single sentence but across multiple sentences in a long document~\citep{docred}. 
Therefore, document-level RE has attracted much attention in recent years. 

Figure~\ref{fig:task} shows an example document and corresponding relational facts for document-level RE. 
In the example, to extract the relation between \textit{Benjamin Bossi} and \textit{Columbia Records}, two entities separated by several sentences, we need the following inference steps. 
First, we need to know that \textit{Benjamin Bossi} is a member of \textit{Romeo Void}. 
Next, we need to infer that \textit{Never Say Never} is performed by \textit{Romeo Void}, and released by \textit{Columbia Records}. 
Based on these facts above, we can draw a conclusion that \textit{Benjamin Bossi} is signed by \textit{Columbia Records}. 
The example indicates that, to tackle document-level RE, a model needs the ability to capture the interactions between long-distance entities. 
In addition, since an entire document has an extremely long text, a model also needs the ability to integrate more global contextual information for words. 

\begin{figure}[t]
\centering
\includegraphics[width=0.99\linewidth]{task.pdf}
\caption{
An example for document-level RE. 
Word spans with background colors denote the mentions. 
Mentions that refer to the same entity have the same background color.
The solid lines denote intra-sentence relations. 
The dotted line denotes an inter-sentence relation. 
Document-level RE requires extracting both intra- and inter-sentence relations between all the target entity pairs. 
}
\label{fig:task}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.99\linewidth]{graph.pdf}
\caption{
An illustration of a document-level graph corresponding to a two-sentence document. 
Each node in the graph corresponds to a word in the document. 
We design five categories of edges to connect nodes in the graph. 
For the simplicity of the illustration, we omit some self-loop edges and adjacent word edges. 
}
\label{fig:graph}
\end{figure*}
 
Recent works show that graph-based methods can obtain useful entity representations thus helping tackle document-level RE. 
These methods usually construct a document-level graph, which represents words as nodes and uses edges between them to capture document-aware interactions. 
To obtain entity representations for relation extraction, some of them use  graph neural networks (GNN)~\citep{labelled_edge_gcn,dcgcn} to integrate neighborhood information for each node~\citep{gcnn,lsr}. 
Although they consider the entire graph structure, they may fail to model the interactions between long-distance entities due to the inherent over-smoothing problem in GNN~\citep{over_smoothing}. 
Other works attempt to encode path information between the target entity pair in the graph~\citep{quirk_and_poon,idepnn,eog}. 
They have the ability to alleviate the problem of modeling long-distance entity interactions, but they may fail to capture more global contextual information since they usually integrate only local contextual information for nodes in the graph. 
Therefore, to obtain more comprehensive entity representations, it is necessary to find a way to integrate global contextual information and model long-distance entity interactions simultaneously. 

In this paper, we propose the \textbf{C}oarse-to-\textbf{F}ine \textbf{E}ntity \textbf{R}epresentation model (\textbf{CFER}) to obtain comprehensive entity representations for document-level RE. 
More specifically, we first construct a document-level graph that captures rich document-aware interactions, reflected by syntactic dependencies, adjacent word connections, cross-sentence connections, and coreferential mention interactions. 
Based on the constructed graph, we design a coarse-to-fine strategy with two phases. 
First, we use Densely Connected Graph Convolutional Networks (DCGCN)~\citep{dccnn,dcgcn} to integrate global contextual information in the entire graph at a coarse level. 
Next, we adopt an attention-based path encoding mechanism, which takes the global contextual information as a guidance, to selectively aggregate path information between the potentially long-distance target entity pair at a fine level. 
Given the entity representations from both two levels that feature global contextual information and long-distance interactions, respectively, we can obtain more comprehensive entity representations for relation extraction by combining them. 

Our contributions are summarized as follows: 
\begin{itemize}
    \item 
    We propose a novel document-level RE model called \textbf{CFER}. 
    It uses a coarse-to-fine strategy to integrate global contextual information and model long-distance interactions between the target entities simultaneously, thus obtaining comprehensive entity representations. 
    \item 
    Experiments on a large-scale document-level RE dataset show that our model achieves better performance than previous baseline models. 
    \item 
    Analysis of our model reveals the effectiveness of our coarse-to-fine strategy, and further highlights the robustness of our model to the uneven label distribution. 
\end{itemize}

\section{Methodology}

\subsection{Task Formulation}

Let  denote a document consisting of  sentences , 
where  denotes the -th sentence containing  words denoted by . 
Let  denote an entity set containing  entities , where  denotes the coreferential mention set of the -th entity, containing  word spans of corresponding mentions denoted by . 
Given  and ,  document-level RE requires extracting all the relational facts in the form of triplets, i.e., extracting , where  is a pre-defined relation category set. 
Since an entity pair may have multiple semantic relations, we formulate document-level RE as a multi-label classification task. 

\begin{figure*}[t]
\centering
\includegraphics[width=0.99\linewidth]{model.png}
\caption{
An illustration of our proposed model, CFER. 
It is composed of four modules: a text encoding module, a coarse-level representation module, a fine-level representation module, and a classification module. 
}
\label{fig:model}
\end{figure*}

\subsection{Document-level Graph Construction}
Given a document, we first construct a document-level graph that captures rich document-aware interactions, reflected by syntactic dependencies, adjacent word connections, cross-sentence connections, and coreferential mention interactions. 
Figure~\ref{fig:graph} shows an example document-level graph corresponding to a two-sentence document. 
The graph regards words in the document as nodes and captures document-aware interactions by five categories of edges. 
These undirected edges are described as follows. 

\noindent\textbf{Syntactic dependency edge:}
Syntactic dependency information is proved effective for document-level or cross-sentence RE in previous works~\citep{gcnn,idepnn,graph_lstm,gs_lstm}. 
Therefore, we use the dependency parser in spaCy\footnote{https://spacy.io/} to parse the syntactic dependency tree for each sentence. 
After that, we add edges between all node pairs that have dependency relations. 

\noindent\textbf{Adjacent word edge:}
\citet{quirk_and_poon} point out that adding edges between adjacent words can mitigate the dependency parser errors.
Therefore, we add edges between all node pairs that are adjacent in the document. 

\noindent\textbf{Self-loop edge:}
For a node, in addition to its neighborhood information, the historical information of the node itself is also essential in information integration. 
Therefore, we add a self-loop edge for each node. 

\noindent\textbf{Adjacent sentence edge:}
To ensure that information can be integrated across sentences, for each adjacent sentence pair, we add an edge between their dependency tree roots. 

\noindent\textbf{Coreferential mention edge:}
Coreferential mentions may share information captured from their respective contexts with each other. 
This could be regarded as global cross-sentence interactions. 
Therefore, we add edges between the first words of all mention pairs that refer to the same entity. 

\subsection{Coarse-to-Fine Entity Representations}

In this subsection, we describe our proposed \textbf{C}oarse-to-\textbf{F}ine \textbf{E}ntity \textbf{R}epresentation model (\textbf{CFER}) in detail. 
As shown in Figure~\ref{fig:model}, our model is composed of a text encoding module, a coarse-level representation module, a fine-level representation module, and a classification module.  

\noindent\textbf{Text Encoding Module:} 
This module aims to encode each word in the document as a vector with text contextual information. 
The text encoding module consists of an embedding layer and a contextual encoder. 

At the embedding layer, we use pretrained GloVe~\citep{glove} embeddings or the pretrained BERT~\citep{bert} model to represent each word as a dense embedding vector. 
This layer outputs a set of word embeddings , where  denotes the word index in the document,  denotes the embedding dimension. 

After the embedding layer, we use a contextual encoder to further integrate text contextual information for each word. 
Specifically, we adopt a Bi-GRU~\citep{gru,bi_rnn} model as the contextual encoder: 

where  are the forward and backward GRU hidden states of the -th word, respectively, with  denoting the hidden dimension. 
We concatenate the hidden states of two directions to obtain the outputs of the contextual encoder: . 

\noindent\textbf{Coarse-level Representation Module:} 
This module aims to integrate local and global contextual information in the entire document-level graph. 
As indicated by \citet{dcgcn}, Densely Connected Graph Convolutional Networks (DCGCN) have the ability to capture rich local and global contextual information. 
Therefore, we adopt DCGCN layers as the coarse-level representation module. 
DCGCN layers are organized into  blocks and the -th block has  sub-layers. 
At the -th sub-layer in block , the calculation for node  is defined as 

where  is the block input of node . 
 is the output of node  at the -th sub-layer in block . 
 is the neighborhood input, obtained by concatenating  and the outputs of node  from all previous sub-layers in the same block. 
 and  are trainable parameters. 
 denotes the neighbor set of node i in the document-level graph. 
Finally, block  adds up the block input and the concatenation of all sub-layer outputs, and then adopts a fully connected layer to compute the block output : 

Finally, we take the output of the final block  as the output of the coarse-level representation module. 

\noindent\textbf{Fine-level Representation Module:} 
The coarse-level representations can capture rich contextual information, but may fail to model long-distance entity interactions. 
Taking the global contextual information as a guidance, the fine-level representation module aims to utilize path information between the target entity pair to alleviate this problem. 
This module adopts an attention-based path encoding mechanism based on a path encoder and an attention aggregator. 

For a target entity pair , we denote the numbers of their corresponding mentions as  and , respectively. 
We first extract  shortest paths between all mention pairs in a subgraph that contains only syntactic dependency and adjacent sentence edges. 
For the -th path ,  we then adopt a Bi-GRU model as the path encoder to compute the path-aware mention representations: 

where  are the forward and backward GRU hidden states of the -th node in the -th path, respectively. 
 are the path-aware representations of the head and tail mentions in the -th path, respectively. 

Since not all the paths contain useful information, we design an attention aggregator, which takes the global contextual information as a guidance, to selectively aggregate the path-aware mention representations: 

where  are the coarse-level head and tail entity representations, respectively, which are computed by averaging their corresponding coarse-level mention representations. 
 and  are trainable parameters. 
 are the path-aware fine-level representations of the head and tail entities, respectively. 

\noindent\textbf{Classification Module:}
In this module, we combine the entity representations from both two levels to obtain comprehensive representations that capture both global contextual information and long-distance entity interactions. 
Next, we predict the probability of each relation by a bilinear scorer: 

where  and   are trainable parameters with  denoting the number of relation categories. 

\subsection{Optimization Objective}
Considering that an entity pair may have multiple relations, we choose the binary cross entropy loss between the ground truth label  and  as the optimization objective: 


\begin{table*}[t]
\centering
\setlength{\tabcolsep}{10pt}
\begin{tabular}{l | c c | c c}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Dev}} & \multicolumn{2}{c}{\textbf{Test}} \\ 
& \textbf{Ign F1} & \textbf{F1} & \textbf{Ign F1} & \textbf{F1}  \\ 
\midrule
\midrule
CNN~\citep{docred} & 41.58 & 43.45 & 40.33 & 42.26 \\
LSTM~\citep{docred} & 48.44 & 50.68 & 47.71 & 50.07 \\
BiLSTM~\citep{docred} & 48.87 & 50.94 & 48.78 & 51.06 \\
Context-Aware~\citep{docred} & 48.94 & 51.09 & 48.40 & 50.70 \\
\midrule
GAT*~\citep{gat} & 45.17 & 51.44 & 47.36 & 49.51 \\
GCNN*~\citep{gcnn} & 46.22 & 51.52 & 49.59 & 51.62 \\
EoG*~\citep{eog} & 45.94 & 52.15 & 49.48 & 51.82 \\
AGGCN*~\citep{aggcn} & 46.29 & 52.47 & 48.89 & 51.45 \\
HIN-GloVe~\citep{hin} & \underline{51.06} & 52.95 & 51.15 & 53.30 \\
LSR-GloVe~\citep{lsr} & 48.82 & \underline{55.17} & \underline{52.15} & \underline{54.18} \\
CFER-GloVe (Ours) & \textbf{54.29} & \textbf{56.40} & \textbf{53.43} & \textbf{55.75} \\
\midrule
BERT-Two-Step~\citep{bert_two_step} & - & 54.42 & - & 53.92 \\
HIN-BERT~\citep{hin} & 54.29 & 56.31 & 53.70 & 55.60 \\
CorefBERT~\citep{coref_bert} & \underline{55.32} & 57.51 & 54.54 & 56.96 \\
LSR-BERT~\citep{lsr} & 52.43 & \underline{59.00} & \underline{56.97} & \underline{59.05} \\
CFER-BERT (Ours) & \textbf{58.00} & \textbf{60.06} & \textbf{57.89} & \textbf{59.82} \\
\bottomrule
\end{tabular}
\caption{Main evaluation results on DocRED. \textbf{Bold} denotes the best. \underline{Underline} denotes the second-best. Results with * are reproduced by \citet{lsr} since their original papers do not provide evaluation results on DocRED. }
\label{tab:docred_rlt}
\end{table*}

\section{Experiments and Analysis}

\subsection{Dataset}
We evaluate our model on a large-scale human-annotated dataset, DocRED~\citep{docred}. 
It is constructed from Wikipedia and Wikidata~\citep{wikidata}, and is currently the largest human-annotated dataset for general domain document-level RE. 
It contains  documents,  entities and  relational facts divided into  relation categories. 
Among the annotated documents,  are used for training,  are used for development, and  are used for testing. 
Among the relational facts, at least  require reading multiple sentences. 

\subsection{Baselines}
We select 14 existing models on three tracks for comparison. 

\noindent(1) \textbf{Baseline track:}
On this track, we select four baseline models proposed along with DocRED~\citep{docred}, including a \textbf{CNN} model~\citep{baseline_cnn}, an \textbf{LSTM} model~\citep{lstm}, a \textbf{Bi-LSTM} model~\citep{baseline_bilstm}, and the \textbf{Context-Aware} model~\citep{baseline_context_aware}. 
These four models are all text-based without constructing a graph. 

\noindent(2) \textbf{Non-BERT track:}
On this track, we select six models without using BERT~\citep{bert}. 
\textbf{GAT}~\citep{gat}, \textbf{GCNN}~\citep{gcnn}, \textbf{AGGCN}~\citep{aggcn}, and \textbf{LSR-GloVe}~\citep{lsr} are graph-based models that leverage GNN to encode nodes. 
Different from the above four models, \textbf{EoG}~\citep{eog} is an edge-oriented graph-based model. 
\textbf{HIN-GloVe}~\citep{hin} is a text-based model and it has a hierarchical architecture to make full use of information from several levels. 
Our model with GloVe (\textbf{CFER-GloVe}) is also on this track. 

\noindent(3) \textbf{BERT track:}
On this track, we select four models that adopt BERT. 
\textbf{BERT-Two-Step}~\citep{bert_two_step} is a text-based model that first predicts the existence of relations and then predicts the specific relations. 
\textbf{HIN-BERT} and \textbf{LSR-BERT} are the BERT versions of HIN-GloVe and LSR-GloVe, respectively. 
\textbf{CorefBERT}~\citep{coref_bert} is another text-based model designed to capture the relations between coreferential noun phrases. 
Our model with BERT (\textbf{CFER-BERT}) is also on this track. 
For a fair comparison, all of the models on the BERT track adopt BERT instead of other pretrained models. 

\subsection{Experimental Settings}
We tune hyper-parameters on the development set. 
Generally, we use AdamW~\citep{adamw} as the optimizer, set the batch size to 16, set the dropout rate to , and use the DCGCN consisting of two blocks with 3 and 4 sub-layers, respectively. 
For CFER-GloVe, we set the initial learning rate to , the embedding dimension to , and the hidden dimension to . 
For CFER-BERT, we set the initial learning rate for BERT modules to , the initial learning rate for other modules to , the embedding dimension to , and the hidden dimension to .\footnote{More details of hyper-parameters are shown in the Appendix. }

Following previous works, we choose micro \textbf{F1} and micro \textbf{Ign F1} as the evaluation metrics. 
Ign F1 denotes F1 excluding relational facts that appear in both the training set and the development or test set. 
We determine relation-specific thresholds  based on the micro F1 on the development set. 
With these thresholds, we classify a triplet  as positive if  or negative otherwise. 

\subsection{Main Results}

Table~\ref{tab:docred_rlt} shows the main evaluation results on DocRED. 
Considering models on the baseline and non-BERT tracks, we have the following observations from Table~\ref{tab:docred_rlt}: 
(1) 
All the models on the baseline track are text-based, and they achieve the poorest performance. 
Another text-based model HIN-GloVe achieves relatively high performance, but still cannot exceed LSR-GloVe and CFER-GloVe, two graph-based models. 
On the whole, graph-based models have significant advantages over text-based models in the task of document-level RE. 
(2) 
EoG pays more attention to encoding paths. 
GAT, GCNN, and AGGCN focus on integrating information on the entire graph. 
These four models achieve similar performance. 
This suggests that these two kinds of models have their own characteristics, and no one has overwhelming advantages. 
(3)
LSR-GloVe also focuses on integrating information on the entire graph, but it achieves better performance than other graph-based baselines. 
This is because it designs a latent structure that can be learned by iterative refinements, thus enabling the model to better integrate information. 
(4)
CFER-GloVe, which integrates global contextual information and model long-distance interactions simultaneously, outperforms all baselines significantly. 
This verifies the effectiveness of our comprehensive entity representations produced by the coarse-to-fine strategy. 

Considering models on the BERT track, we observe from Table~\ref{tab:docred_rlt} that: 
(1)
Consistent with the conclusion on the baseline and non-BERT tracks, LSR-BERT and CFER-BERT, two graph-based models, have significant advantages over the other three text-based models. 
(2)
CFER-BERT achieves the best performance. 
In addition, considering that our model uses a static graph instead of a dynamic graph learned by iterative refinements, our graph structure is simpler and sparser than LSR-BERT.
Nevertheless, with a simpler and sparser static graph structure, our model still outperforms LSR-BERT. 
This suggests that our coarse-to-fine strategy has the ability to maximize the use of the graph structure thus achieving good performance. 

\subsection{Model Analysis}
In this subsection, to explain why and how our model with the coarse-to-fine strategy works, we comprehensively analyze our model by answering four questions. 

\noindent\textbf{What improvement does CFER make? }
To explore where our improvement is located, we divide all the 96 relations into 4 categories according to their ground-truth positive label numbers in the development set. 
We show micro F1 on each category achieved by three models\footnote{Here we select only those models with available and bug-free source codes for comparison.} in Figure~\ref{fig:relation_f1}. 
As shown in the figure, compared to Bi-LSTM, BERT-Two-Step makes more improvement on relations with more than 20 positive labels, denoted by major relations, but less improvement () on long-tail relations with less than or equal to 20 positive labels. 
By contrast, keeping the improvement on major relations, our model makes a much more significant improvement () on long-tail relations. 
Usually, long-tail relations are harder to extract, while our model can better extract them since we can capture both global information and subtle clues that may include special features of long-tail relations. 
With high performance on long-tail relations, our model narrows the performance gap between major and long-tail relations by  (from  of Bi-LSTM to  of ours). 
This suggests that our model is more robust to the uneven label distribution. 

\noindent\textbf{Does Fine-level Representations Matter? }
We show the ablation experiment results of CFER-GloVe on the development set of DocRED in Table~\ref{tab:ablation_rlt}. 
To explore the effectiveness of fine-level representations, we modify our model in three ways: 
(1) Replace the attention aggregator by a simple mean aggregator (denoted by \textit{- Attention Aggregator}). 
(2) Replace all used shortest paths by a random shortest path (denoted by \textit{- Multiple Paths}). 
(3) Remove the whole fine-level representations from the final classification (denoted by \textit{- Fine-level Repr.}). 
As shown in Table~\ref{tab:ablation_rlt}, F1 scores of these three modified models decrease by 1.48, 2.72, and 4.12, respectively. 
This verifies the effectiveness of the fine-level representations. 
In addition, this also verifies that not all shortest paths contain useful information, and our attention aggregator for multiple paths has the ability to selectively aggregate useful information thus producing better fine-level representations. 

\noindent\textbf{Does Coarse-level Representations Matter? }
To explore the effectiveness of coarse-level representations, we modify our model in two ways: 
(1) Remove the whole coarse-level representations from the final classification (denoted by \textit{-  Coarse-level Repr.}). 
(2) Remove DCGCN blocks (denoted by \textit{- DCGCN Blocks}). 
As shown in Table~\ref{tab:ablation_rlt}, F1 scores of these two modified models decrease by 1.14 and 2.08, respectively. 
This verifies that coarse-level representations can benefit relation extraction. 
In addition, this also verifies the ability of DCGCN to capture rich local and global contextual information, since the quality of coarse-level representations drops significantly without DCGCN. 

\noindent\textbf{What If Remove Both-level Modules? }
We have verified the effectiveness of coarse- and fine-level representations separately. 
To draw a more credible conclusion, we remove both coarse- and fine-level representation modules (denoted by \textit{-  Both-level Modules}) for comparison. 
This operation makes our model degenerate into a simple version similar to Bi-LSTM. 
As shown in Table~\ref{tab:ablation_rlt}, this simple version achieves similar performance to Bi-LSTM as expected. 
This suggests that our text encoding module is not much different from the Bi-LSTM baseline, and the performance improvement is totally introduced by our coarse-to-fine strategy. 

\begin{figure}[t]
\centering
\includegraphics[width=0.99\linewidth]{relation_specific_F1.png}
\caption{
Micro F1 on different categories of relations. 
Our model makes a much more significant improvement on long-tail relations with fewer positive labels. 
}
\label{fig:relation_f1}
\end{figure}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{10pt}
\begin{tabular}{l | c}
\toprule
\textbf{~~Model} & \textbf{F1}\\
\midrule
\midrule
CFER-GloVe (Full) & 56.40 (-0.00) \\
\midrule
- Attention Aggregator & 54.92 (-1.48) \\
- Multiple Paths & 53.68 (-2.72) \\
- Fine-level Repr. & 52.28 (-4.12) \\
\midrule
- Coarse-level Repr. & 55.26 (-1.14) \\
- DCGCN Blocks & 54.32 (-2.08) \\
\midrule
- Both-level Modules & 51.67 (-4.73) \\
\bottomrule
\end{tabular}
\caption{Ablation experiment results of our model on the development set of DocRED. }
\label{tab:ablation_rlt}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=0.93\linewidth]{case.pdf}
\caption{
An extraction case from the development set. 
We show the relational facts extracted by four versions of CFER-GloVe. 
For CFER-GloVe (Full) and CFER-GloVe (- DCGCN Blocks), we additionally show the paths along with their attention weights used for producing fine-level representations of entity 0 (Daniel Ajayi Adeniran) and entity 5 (Redeemed Christian Church). 
}
\label{fig:case_study}
\end{figure*}

\subsection{Case Study}
Figure~\ref{fig:case_study} shows an extraction case selected from the development set. 
In this case, CFER-GloVe (- Both-level Modules), a simple version similar to the Bi-LSTM baseline, extracts only two simple relational facts. 
CFER-GloVe (- Fine-level Repr.) extracts three more relational facts since it adopts DCGCN blocks to integrate richer local and global contextual information. 
CFER-GloVe (- DCGCN Blocks) makes use of path information to model long-distance entity interactions, and also extracts three more relational facts compared to CFER-GloVe (- Both-level Modules). 
CFER-GloVe (Full) combines all the advantages of its modified versions. 
As a result, it extracts the most relational facts. 

To reveal how our coarse-to-fine strategy works, we further analyze the relational fact (0, P140, 5), which is extracted by only CFER (Full). 
In Figure~\ref{fig:case_study}, for CFER (Full) and CFER (- DCGCN Blocks), we additionally show several high-weight paths along with their attention weights used for producing fine-level representations. 
From the shown case, we can find that CFER (Full) gives relatively smooth weights to its high-weight paths. 
This enables it to aggregate richer path information from several useful paths. 
In fact, CFER (Full) successfully aggregates both intra-sentence (within sentence 7) and inter-sentence (across sentences 5, 6, and 7) information. 
By contrast, without the guidance of global contextual information, CFER (- DCGCN Blocks) learns extremely unbalanced weights and pays almost all its attention to a sub-optimal path. 
As a result, it fails to extract the P140 relation. 
As for CFER-GloVe (- Fine-level Repr.) and CFER-GloVe (- Both-level Module), they do not consider any path information. 
Therefore, it is hard for them to achieve as good performance as CFER (Full). 

\section{Related Work}

Most of existing RE methods under document-level or cross-sentence settings are graph-based. 
They usually construct a document-level graph, which represents words as nodes and uses edges between them to capture document-aware interactions. 
Some of them focus more on integrating information for each node in the entire graph. 
\citet{graph_lstm} propose the Graph LSTM to encode nodes. 
\citet{gs_lstm} design a multi-layer graph with information flowing through layers to avoid calculation loops. 
\citet{gcnn} use a labeled edge GCN~\citep{labelled_edge_gcn} to integrate information and adopt the Multi-Instance Learning (MIL) to address the problem introduced by multiple mentions. 
\citet{aggcn} and \citet{lsr} apply graph convolutional networks to a complete graph with weighted edges. 
The edge weights are iteratively refined by the multi-head self-attention~\citep{transformer} or structured attention~\citep{structured_att,structured_text_rep} mechanisms. 
Although these methods consider the entire graph structure, they may fail to model the interactions between long-distance entities due to the inherent over-smoothing problem in GNN~\citep{over_smoothing}. 
Other methods attempt to encode path information between the target entity pair in the graph. 
\citet{quirk_and_poon} design feature templates to extract features from multiple paths for classification. 
\citet{idepnn} use dependency subtree representations to augment word embeddings and adopt LSTM~\citep{lstm} to compute the path representation. 
\citet{eog} propose an edge-oriented model that represents paths between the target entity pair as an edge through a walk-based iterative inference mechanism. 
These methods are able to alleviate the problem of modeling long-distance entity interactions, but they usually integrate only local contextual information for nodes instead of more global contextual information. 
Compared to existing graph-based methods, our model can not only integrate global contextual information on the entire graph, but also model long-distance entity interactions by utilizing path information, simultaneously. 
 
Besides graph-based methods, there are also text-based methods without constructing a graph. 
\citet{barn} use a transformer~\citep{transformer} to encode the document and address the problem of multiple mentions by MIL. 
\citet{sor} attempt to explicitly connect the target entity pair via a context token, and then utilize second-order relations to capture complex and long dependencies. 
\citet{multiscale} design a multiscale architecture that learns representations at different scales and then combines them for classification. 
\citet{bert_two_step} adopt BERT~\citep{bert} to encode the document and predict the existence of relations before predicting the specific relations. 
\citet{hin} design a hierarchical architecture to make full use of information from several levels. 
\citet{coref_bert} attempt to explicitly capture relations between coreferential noun phrases to coherently comprehend the whole document. 
All of these text-based methods ignore the graph structure information. 
However, this information is shown effective and even essential for document-level RE in other existing works. 
Compared to text-based methods, our model can utilize text information and graph structure information simultaneously. 

\section{Conclusion}

In this paper, we propose CFER with a coarse-to-fine strategy to learn comprehensive representations for document-level RE. 
Our model integrates global contextual information and models long-distance interactions between the target entity pair simultaneously, thus addressing the disadvantages that existing graph-based models suffer from. 
Experimental results show that our model achieves better performance than previous baseline models. 
Further, elaborate analysis of our model verifies the effectiveness of our coarse-to-fine strategy and highlights the robustness of our model to the uneven label distribution. 
Note that our coarse-to-fine strategy is not limited to only the task of document-level RE. 
It has the potential to be applied to a variety of other NLP tasks.

\bibliography{aaai21}

\onecolumn
\appendix
\appendixpage

\section{Computing Infrastructure}

We run our experiments on a single NVIDIA GeForce GTX 1080Ti GPU with 11 GB memory. 
Our operating system is Ubuntu 16.04 LTS\footnote{https://releases.ubuntu.com/16.04/}. 
We mainly use Python3\footnote{https://www.python.org/} as our coding language. 
Key libraries used in our code include PyTorch 1.4.0\footnote{https://pytorch.org/}, NumPy 1.19.1\footnote{https://numpy.org/}, spaCy 2.3.2\footnote{https://spacy.io/}, transformers 3.0.2\footnote{https://huggingface.co/transformers/v3.0.2/}, and so on\footnote{We will specify the complete requirement list when we formally release our code. }. 

\section{Details of Hyper-parameters}

We adopt the grid search to find the best hyper-parameters that achieve the best F1 on the development set. 
Generally, for both CFER-GloVe and CFER-BERT, we use AdamW with a weight decay of  as the optimizer, use ReLU as the activation function, and use the DCGCN consisting of two blocks with 3 and 4 sub-layers, respectively. 
For other hyper-parameters, we state the values tried and the finally selected value for two models separately as follows. 

For CFER-GloVe: 
(1) We search the batch size in , and finally select . 
(2) We search the dropout rate in , and finally select . 
(3) We search the initial learning rate for all modules in , and finally choose . 
(4) We set the embedding dimension to , the same as the dimension of used GloVe embeddings. 
(5) We search the hidden size in , and finally select . 
(6) For each hyper-parameter configuration, we train  epochs and select the best F1 achieved during these  epochs to evaluate the performance under this configuration. 

For CFER-BERT: 
(1) We search the batch size in , and finally select . 
(2) We search the dropout rate in , and finally select . 
(3) We search the initial learning rate for BERT modules in , and finally select . 
(4) We search the initial learning rate for the other modules in , and finally select . 
(5) We set the embedding dimension to , the same as the output dimension of BERT. 
(6) We search the hidden size in , and finally select . 
(7) For each hyper-parameter configuration, we train  epochs and select the best F1 achieved during these  epochs to evaluate the performance under this configuration. 

\end{document}
