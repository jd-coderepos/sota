\documentclass[10pt]{article}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{amsthm}
\usepackage{mathpartir}
\usepackage{times}
\usepackage{url}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[definition]{Example}

\newenvironment{defn}{\begin{tabbing}
  \hspace{1.5em} \= \hspace{.26\linewidth - 1.5em} \= \hspace{1.5em} \= \kill
  }{
  \end{tabbing}}

\newcounter{ctr:list1}
\newcounter{ctr:list2}
\newenvironment{list1}
    {\setcounter{ctr:list1}{0}
    \begin{list}{{\bf\Roman{ctr:list1}}.}{\usecounter{ctr:list1}\setlength{\leftmargin}{0.3cm}}}
    {\end{list}}
\newenvironment{list2}
    {\setcounter{ctr:list2}{0}
    \begin{list}{(\roman{ctr:list2})}{\usecounter{ctr:list2}\setlength{\leftmargin}{0.7cm}}}
    {\end{list}}
\newenvironment{list2'}
    {\setcounter{ctr:list2}{0}
    \begin{list}{(\roman{ctr:list2})}{\usecounter{ctr:list2}\setlength{\leftmargin}{0.3cm}}}
    {\end{list}}

\newenvironment{blist}[1]
    {\begin{list}{#1}{\setlength{\leftmargin}{0.3cm}}}
    {\end{list}}



\newenvironment{mylist}
    {\begin{list}{}{\setlength{\leftmargin}{0.5cm}}}
    {\end{list}}


\newcounter{Crules}
\newcounter{Cassums}

\newcommand{\entry}[2]{\>\>\>#2}
\newcommand{\clause}[2]{\>\>#2}
\newcommand{\category}[2]{\clause{#1::=}{#2}}

\newcommand{\In}{\mathtt{in}}
\newcommand{\func}[1]{{\bf #1}}
\newcommand{\clk}{\mathsf{Clk}}
\newcommand{\op}{{\it op}}
\newcommand{\adm}{{\it adm}}
\newcommand{\tup}[1]{\langle #1\rangle}
\newcommand{\pause}{;}
\newcommand{\snd}[3]{\overline{#1}\tup{#2}\pause #3}
\newcommand{\asnd}[2]{\overline{#1}\tup{#2}}
\newcommand{\rcv}[3]{#1(#2)\pause #3}
\newcommand{\parl}[2]{#1\:|\:#2}
\newcommand{\new}[2]{(\nu #1)\:#2}
\newcommand{\nnew}[3]{(\nu_{#1}~#2)~(#3)}
\newcommand{\cond}[4]{\mathsf{if}\:#1=#2\:\mathsf{then}\:#3\:\mathsf{else}\:#4}
\newcommand{\symb}[1]{{\bf #1}}
\newcommand{\key}{\mathrm K}
\newcommand{\capb}{\symb{mac}(\tup{\op,T,b},\key_b)}
\newcommand{\sep}{\:|\:}
\newcommand{\col}{\!:}
\newcommand{\ccol}{\!:\!}
\newcommand{\palpha}{\alpha^\circ}
\newcommand{\Req}{\mathsf{Req}}
\newcommand{\OReq}{\mathsf{OReq}}
\newcommand{\EReq}{\mathsf{EReq}}
\newcommand{\Comm}{\mathsf{EOk}}
\newcommand{\PReq}{\mathsf{AReq}}
\newcommand{\CReq}{\mathsf{TReq}}
\newcommand{\Ret}{\mathsf{Ret}}
\newcommand{\AReq}{\mathsf{CReq}}
\newcommand{\DAReq}{\mathsf{DCReq}}
\newcommand{\DPReq}{\mathsf{DAReq}}
\newcommand{\DReq}{\mathsf{DReq}}
\newcommand{\action}[1]{\stackrel{#1}{\longrightarrow}}
\newcommand{\fn}{\mathtt{fn}}
\newcommand{\fv}{\mathtt{fv}}
\newcommand{\fnv}{\mathtt{fnv}}
\newcommand{\fileop}[4]{\mathsf{fileop}_#1~#3/#2;~#4}
\newcommand{\polmod}[3]{\mathsf{admin}_#1~#2;~#3}
\newcommand{\auth}[4]{\mathsf{auth}_#1~#2~\mathsf{for}~#3;~#4}
\newcommand{\dfileop}[4]{\mathsf{fileopauth}_#1~#3/#2;~#4}
\newcommand{\ord}[1]{#1^{\scriptsize \mbox{th}}}
\newcommand{\pre}[2]{\prec_{#1}^{#2}}
\newcommand{\defq}{\stackrel{\mathtt{def}}=}
\newcommand{\at}{\:@\:}
\newcommand{\no}{{\not \exists}}
\newcommand{\cs}{\item[Case]}
\newcommand{\inferc}[1]{\infer[(#1)]}
\newcommand{\dom}{\mathtt{dom}}

\newcommand{\seq}{\widetilde}
\newcommand{\callsec}[1]{Section~\ref{#1}}
\newcommand{\callprop}[1]{Proposition~\ref{#1}}
\newcommand{\callthm}[1]{Theorem~\ref{#1}}
\newcommand{\calllem}[1]{Lemma~\ref{#1}}
\newcommand{\calldef}[1]{Definition~\ref{#1}}
\newcommand{\df}{\stackrel{\tiny\mbox{def}}=}

\newcommand{\betac}{\beta^\circ}
\newcommand{\alphac}{\alpha^\circ}
\newcommand{\gammac}{\gamma^\circ}
\newcommand{\deltac}{\delta^\circ}

\newcommand{\subs}[2]{\{{}^{#1}\!/\!{}_{#2}\}}
\newcommand{\hwrp}{\mathtt{>}\!\!\mathtt{>}\!\mathtt{=}}

\newcommand{\pisymbol}{\sigma}

\newcommand{\longrightleftarrows}{\overrightarrow\longleftarrow~~}

\makeatletter
\newcommand{\R}{\addtocounter{Crules}{1}R\arabic{Crules}\gdef\@currentlabel{\arabic{Crules}}}
\newcommand{\A}{\addtocounter{Cassums}{1}A\arabic{Cassums}\gdef\@currentlabel{\arabic{Cassums}}}
\makeatother

\title{On Secure Distributed Implementations of Dynamic Access Control}
\author{Avik Chaudhuri \\ {\normalsize University of California at Santa Cruz} \\ {\normalsize \tt avik@cs.ucsc.edu}}
\date{}

\begin{document}
\maketitle

\begin{abstract} 
\noindent
Distributed implementations of access control abound in distributed storage protocols. While such implementations are often accompanied by informal justifications of their correctness, our formal analysis reveals that their correctness can be tricky. In particular, we discover several subtleties in a state-of-the-art implementation based on capabilities, that can undermine correctness under a simple specification of access control. 

We consider both safety and security for correctness; loosely, safety requires that an implementation does not introduce unspecified behaviors, and security requires that an implementation preserves the specified behavioral equivalences. 
We show that a secure implementation of a static access policy already requires some care in order to prevent unspecified leaks of information about the access policy. 
A dynamic access policy causes further problems. For instance, if accesses can be dynamically granted then the implementation does not remain secure---it leaks information about the access policy. If accesses can be dynamically revoked then the implementation does not even remain safe. We show that a safe implementation is possible if a clock is introduced in the implementation. A secure implementation is possible if the specification is accordingly generalized.

Our analysis shows how a distributed implementation can be systematically designed from a specification, guided by precise formal goals. While our results are based on formal criteria, we show how violations of each of those criteria can lead to real attacks. We distill the key ideas behind those attacks and propose corrections in terms of useful design principles. We show that other stateful computations can be distributed just as well using those principles. 

\end{abstract}



\section{Introduction}
\noindent
In most file systems, protection relies on access control. Usually the access checks are local---the file system maintains an access policy that specifies which principals may access which files, and any access to a file is guarded by a local check that enforces the policy for that file. In recent file systems, however, the access checks are distributed, and access control is implemented via cryptographic techniques. 
In this paper, we try to understand the extent to which these distributed implementations of access control preserve the simple character of local access checks. 


We focus on implementations that appear in file systems based on networked storage~\cite{gobioff-security}. In such systems, access control and storage are parallelized to improve performance. Execution requests are served by storage servers; such requests are guided by access requests that are served elsewhere by access-control servers. When a user requests access to a file, an access-control server certifies the access decision for that file by providing the user with an unforgeable \emph{capability}. Any subsequent execution request carries that capability as proof of access; a storage server can efficiently verify that the capability is authentic and serve the execution request.

We formally study the correctness of these implementations vis-\`a-vis a simple specification of local access control. Implementing static access policies already requires some care in this setting; dynamic access policies cause further problems that require considerable analysis. We study these cases separately in Sections \ref{intro-stat} and \ref{intro-dyn}. Based on our analysis, we develop formal models and proofs for an implementation of arbitrary access policies in Section \ref{sec:mpdyn}. 




We consider both safety and security for correctness; loosely, safety requires that an implementation does not introduce unspecified behaviors, and security requires that an implementation preserves the specified behavioral equivalences. 
Our proofs of safety and security are built modularly by showing simulations; we develop the necessary definitions and proof techniques in Section \ref{theory}.

Our analysis shows how a distributed implementation can be systematically designed from a specification, guided by precise formal goals. We justify those goals by showing how their violations can lead to real attacks  (Sections \ref{intro-stat} and~\ref{intro-dyn}). Further, we distill the key ideas behind those attacks and propose corrections in terms of useful design principles. We show that other stateful computations can be distributed just as well using those principles (Section \ref{apply}).



\paragraph{\em Comparison with related work} This paper culminates a line of work that we begin in~\cite{ChaudhuriAbadi-FMSE05} and continue in ~\cite{ForteChaudhuriA06}. In~\cite{ChaudhuriAbadi-FMSE05}, we show how to securely implement static access policies with capabilities; in~\cite{ForteChaudhuriA06}, we present a safe (but not secure) implementation of dynamic access policies in that setting. In this paper, we carefully review those results, and systematically analyze the difficulties that arise for security in the case of dynamic access policies. Our analysis leads us to develop variants of the implementation in~\cite{ForteChaudhuriA06} that we can prove secure with appropriate assumptions. The proofs are built by a new, instructive technique, which may be of independent interest. 

Further, guided by our analysis of access control, we show how to automatically derive secure distributed implementations of other stateful computations. This approach is reminiscent of secure program partitioning \cite{spp}. 



Access control for networked storage has been studied in lesser detail by Gobioff~\cite{gobioff-security} using belief logics, and by Halevi \emph{et al.}~\cite{Halevi05} using universal composability~\cite{ucframework}. The techniques used in this paper are similar to those used by Abadi \emph{et al.} for secure implementation of channel abstractions \cite{abadi98secure} and authentication primitives \cite{authprim}, and by Maffeis to study the equivalence of communication patterns in distributed query systems \cite{Maffeis05}. These techniques rely on  programming languages concepts, including testing equivalence~\cite{nicohenn84} and full abstraction \cite{milnerfullabs,pplt}. 
A huge body of such techniques have been developed for formal specification and verification of systems. 



We do not consider access control for untrusted storage~\cite{plutus} in this paper. In file systems based on untrusted storage, files are cryptographically secured before storage, and their access keys are managed and shared by users. As such, untrusted storage is quite similar to public communication, and standard techniques for secure communication on public networks apply for secure storage in this setting. 
Related work in that area includes formal analysis of protocols for secure file sharing on untrusted storage \cite{secfsbyz,Blanchet-Chaudhuri}, as well as correctness proofs for the cryptographic techniques  involved in such protocols~\cite{lazyrevcfs,keyregress,keyupd-lazyrev}. 



\section{Review: the case of static access policies}\label{intro-stat}
\noindent
To warm up, let us focus on implementing access policies that are \emph{static}. In this case, a secure implementation already appears in~\cite{ChaudhuriAbadi-FMSE05}. Below we systematically reconstruct that implementation, focusing on a detailed analysis of its correctness. This analysis allows us to distill some basic design principles, marked with bold {\bf R}, in preparation for later sections, where we consider the more difficult problem of implementing dynamic access policies.


Consider the following protocol, , for networked storage.\footnote{By convention, we use superscripts  and  to denote ``static" and ``dynamic", and superscripts  and  to denote ``extension" and ``restriction".} Principals include users , an access-control server , and a storage server . We assume that  maintains a (static) access policy  and  maintains a store . Access decisions under  follow the relation  over users  and operations . Execution of an operation  under  follows the relation  over next stores  and results . Let  be a secret key shared by  and , and  be a function over messages and keys that produces unforgeable message authentication codes (MACs) \cite{lecnotes}. We assume that MACs can be decoded to retrieve their messages. (Usually MACs are explicitly paired with their messages, so that the decoding is trivial.)

Here a user  requests  for access to an operation , and  returns a capability for  only if  specifies that  may access . Elsewhere, a user  requests  to execute an operation by sending a capability , and  executes the operation only if  authorizes access to that operation.

What does ``safety" or ``security" mean in this setting? A reasonable specification of correctness is the following trivial protocol, , for ideal storage. Here principals include users  and a server . The access policy  and the store  are both maintained by ; the access and execution relations remain as above. There is no cryptography. 
Here a user  requests  to execute an operation , and  executes  only if  specifies that  may access . This trivial protocol is correct ``by definition"; so if  implements this protocol, it is correct as well. 

What notions of implementation correctness are appropriate here? A basic criterion is that of safety \cite{refmap}. 
\begin{definition}[Safety]
Under any context (adversary), the behaviors of a safe implementation are included in the behaviors of the specification.
\end{definition}
\noindent
In practice, a suitable notion of inclusion may need to be crafted to accommodate specific implementation behaviors by design (such as those due to messages , , and  in ). Typically, those behaviors can be eliminated by a specific context (called a ``wrapper"), and safety may be defined modulo that context as long as other, interesting behaviors are not eliminated. 

Still, safety only implies the preservation of certain trace properties. A more powerful criterion may be derived from the programming languages concept of semantics preservation, otherwise known as \emph{full abstraction} \cite{milnerfullabs,pplt}. 
\begin{definition}[Security]
A secure implementation preserves behavioral equivalences of the specification. 
\end{definition}
\noindent
In this paper, we tie security to an appropriate may testing congruence~\cite{nicohenn84}.  We consider a protocol instance to include the file system and some code run by ``honest" users, and assume that an arbitrary context colludes with the remaining ``dishonest" users. From any  instance, we derive its  instance by an appropriate refinement map~\cite{refmap}. If  securely implements , then for all  instances  and ,   and  are congruent if their  instances are congruent. 


Security implies safety for all practical purposes, so a safety counterexample usually suffices to break security. For instance, we are in trouble if operations that cannot be executed in  can somehow be executed in  by manipulating capabilities. Suppose that  for all dishonest . Then no such  can execute  in . Now suppose that some such  requests execution of  in . We know that  is executed only if  shows a capability  for . Since  cannot be forged, it must be obtained from  by some honest  that satisfies . Therefore:
\begin{description}
\item[\R]\label{Rsecret} Capabilities obtained by honest users must not be shared with dishonest users.
\end{description}
(However  can still share  with honest users, and any execution request with  can then be reproduced in the specification as an execution request by .)

While (R\ref{Rsecret}) prevents \emph{explicit} leaking of capabilities, we in fact require that capabilities do not leak \emph{any} information that is not available to  contexts. Information may also be leaked implicitly (by observable effects). Therefore:
\begin{description}
\item[\R]\label{Rexamine} Capabilities obtained by honest users must not be examined or compared. \end{description}
Both (R\ref{Rsecret}) and (R\ref{Rexamine}) may be enforced by typechecking the code run by honest users. 

Finally, we require that information is not leaked via capabilities obtained by dishonest users. (Recall that such capabilities are already available to the adversary.) Unfortunately, a capability for an operation  is provided \emph{only} to those users who have access to  under ; in other words,  leaks information on  whenever it returns a capability! This leak breaks security.
Why? Consider implementation instances  and  with  as the only operation, whose execution returns  and may be observed only by honest users; suppose that a dishonest user has access to  in  but not in . Then  and  can be distinguished by a context that requests a capability for ---a capability will be returned in  but not in ---but their specification instances cannot be distinguished by any context. 

Why does this leak concern us? After all, we expect that executing an operation \emph{should} eventually leak some information about access to that operation, since otherwise, having access to that operation is useless! However the leak here is premature; it allows a dishonest user to obtain information about its access to  in an undetectable way, \emph{without} having to request execution of .

To prevent this leak, we must modify the protocol:
\begin{description}
\item[\R]\label{Rfake} ``Fake" capabilities for  must be returned to users who do not have access to . 
\end{description}
The point is that it should not be possible to distinguish the fake capabilities from the real ones prematurely. Let  be another secret key shared by  and . As a preliminary fix, let us modify the following message in . 

Unfortunately this modification is not enough, since the adversary can still compare capabilities that are obtained by different users for a particular operation , to know if their accesses to  are the same under . To prevent this leak:\begin{description}
\item[\R]\label{Rdiff} Capabilities for different users must be different.
\end{description}
For instance, a capability can mention the user whose access it authenticates.
Making the meaning of a message explicit in its content is a good design principle for security \cite{Abadi96}, and we use it on several occasions in this paper. Accordingly we modify the following messages in . 
(On receiving a capability  from ,  still does not care whether  is the user to which  is issued, even if that information can now be obtained from .)

The following result can then be proved (\emph{cf.} \cite{ChaudhuriAbadi-FMSE05}).
\begin{theorem}\label{thm:static}
 securely implements .
\end{theorem}



\section{The case of dynamic access policies}\label{intro-dyn}
\noindent
We now consider the more difficult problem of implementing dynamic access  policies. 
Let  be dynamic; the following protocol, , is obtained by adding administration messages to . Execution of an administrative operation  under  follows the relation  over next policies  and results . 

Here  executes  (perhaps modifying ) if  specifies that  controls~. 
The following protocol, , is obtained by adding similar messages to .

Unfortunately  does not remain secure with respect to .
Consider the  pseudo-code below. Informally,  means ``obtain a capability " and  means ``request execution with ";  means ``request access modification "; and  means ``detect successful use of a capability". 
Here  is a capability for an operation  and  modifies access to . 
\begin{description}
\item[t1]  
\item[t2] 
\end{description}
Now (t1) and (t2) map to the same  pseudo-code
---informally,  means ``request execution of ". Indeed, requesting execution with  in  amounts to requesting execution of  in , so the refinement map must erase instances of  and replace instances of  with the appropriate instances of .
However, suppose that initially no user has access to , and  specifies that all users may access . Then (t1) and (t2) can be distinguished by testing the event . In (t1)  does not authorize access to , so  must be false; but in (t2)  may authorize access to , so  may be true. 

Moreover, if revocation is possible,  does not even remain safe with respect to ! Why? Let  specify that access to  is revoked for some user , and  be the event that  is executed (thus modifying the access policy). In ,  cannot execute  after . But in ,  can execute  after  by using a capability that it acquires before .  

\paragraph{\em Safety in a special case}
One way of eliminating the counterexample above is to make the following assumption:
\begin{description}
\item[\A]\label{Arevoke} Accesses cannot be dynamically revoked.
\end{description}
We can then prove the following new result (see Section \ref{sec:mpdyn}).
\begin{theorem}
 safely implements  assuming {\rm (A\ref{Arevoke})}.\footnote{Some implementation details, such as (R\ref{Rfake}), are not required for safety.} 
\end{theorem}
\noindent
The key observation is that with (A\ref{Arevoke}), a user  cannot access  until it can always access , so  gains no advantage by acquiring capabilities early.



\paragraph{\em Safety in the general case}
Safety breaks with revocation. However, we can recover
safety by introducing \emph{time}. Let  and  share a logical clock (or counter) that measures time, and let the same clock appear in . We have that:
\begin{description}
\item[\R]\label{Rexpire} Any capability that is produced at time  expires at time .\item[\R]\label{Rdelay} Any administrative operation requested at time  is executed at the next clock tick (to time ), so that policies in  and  may change only at clock ticks (and not between).  
\end{description}

\noindent
We call this arrangement a ``midnight-shift scheme", since the underlying idea is the same as that of periodically shifting guards at a museum or a bank. Implementing this scheme is straightforward. For (R\ref{Rexpire}), capabilities carry timestamps. For (R\ref{Rdelay}), administrative operations are executed on an ``accumulator"  instead of , and at every clock tick,  is updated to . Accordingly, we modify the following messages in  to obtain the protocol . 

Likewise, we modify the following message in  to obtain the protocol .
 
Now a capability that carries  as its timestamp certifies a particular access decision at the instant : the meaning is made explicit in the content, which is good practice. However, recall that MACs can be decoded to retrieve their messages. In particular, one can tell the time in  by decoding capabilities. Clearly we require that:
\begin{description}
\item[\R]\label{Rtime} If it is possible to tell the time in , it must also be possible to do so in .
\end{description}
So we must make it possible to tell the time in . 
(The alternative is to make it impossible to tell the time in , by encrypting the timestamps carried by capabilities. Recall that the notion of ``time" here is purely logical.) 
Accordingly we add the following messages to .

The following result can then be proved (\emph{cf.} \cite{ForteChaudhuriA06}). \begin{theorem}
 safely implements .
\end{theorem}
\noindent
This result appears in~\cite{ForteChaudhuriA06}. Unfortunately, the definition of safety in~\cite{ForteChaudhuriA06} is rather non-standard. Moreover, beyond this result, security is not considered in \cite{ForteChaudhuriA06}. In the rest of this section, we analyze the difficulties that arise for security, and present new results. 

It turns out that there are several recipes to break security, and expiry of capabilities is a common ingredient. Clearly, using an expired capability has no counterpart in . So:
\begin{description}
\item[\R]\label{Rstale} Any use of an expired capability must block (without any observable effect). 
\end{description}
Indeed, security breaks without (R\ref{Rstale}). Consider the  pseudo-code below. Informally,  means ``detect any use of an expired capability". Here  is a capability for operation .
\begin{description}
\item[t3] 
\end{description}
\noindent
Without (R\ref{Rstale}), (t3) can be distinguished from a  event by testing the event . But consider implementation instances  and  with  as the only operation, whose execution has no observable effect on the store; let  run (t3) and  run . Since  cannot be reproduced in the specification, it must map to . So the specification instances of  and  run  and . These instances cannot be distinguished. 

Moreover, expiry of a capability yields the information that time has elapsed between the acquisition and use of that capability. We may expect that leaking this information is harmless; after all, the elapse of time can be trivially detected by inspecting timestamps. Then why should we care about such a leak? If the adversary knows that the clock has ticked at least once, it also knows that any pending administrative operations have been executed, possibly modifying the access policy. If this information is leaked in a way that cannot be reproduced in the specification, we are in trouble. Any such way allows the adversary to \emph{implicitly} control the expiry of a capability before its use. (Explicit controls, such as comparison of timestamps, are not problematic, since they can be reproduced in the specification.) 

For instance, consider the  pseudo-code below. 
Here  and  are capabilities for operations  and , and  modifies access to . 
\begin{description}
\item[t4] 




\item[t5] 


\end{description}
Both (t4) and (t5) map to the same  pseudo-code  
But suppose that initially no user has access to  and all users have access to , and  specifies that all users may access . The intermediate  event is true only if  is executed; therefore it ``forces" time to elapse for progress. Now (t4) and (t5) can be distinguished by testing the final  event. In (t4)  must be stale when used, so the event must be false; but in (t5)  may be fresh when used, so the event may be true. Therefore, security breaks.



\paragraph{\em Security in a special case}
One way of plugging such leaks is to consider that the elapse of time is altogether unobservable. (This prospect is not as shocking as it sounds, since here ``time" is simply the value of a privately maintained counter.) 

We expect that executing an operation has some observable effect. Now if initially a user does not have access to an operation, but that access can be dynamically granted, then the elapse of time \emph{can} be detected by observing the effect of executing that operation. So we must assume that:
\begin{description}
\item[\A]\label{Agrant} Accesses cannot be dynamically granted.
\end{description}
On the other hand, we must allow accesses to be dynamically revoked, since otherwise the access policy becomes static. Now if initially a user has access to an operation, but that access can be dynamically revoked, then it is possible to detect the elapse of time if the \emph{failure} to execute that operation is observable. So we must assume that:
\begin{description}
\item[\A]\label{Ablock} Any unsuccessful use of a capability blocks (without any observable effect).
\end{description}
Let us now try to adapt the counterexample above with (A\ref{Agrant}) and (A\ref{Ablock}). Suppose that initially all users have access to  and , and  specifies that no user may access . Consider the  pseudo-code below. Informally,  means ``detect unsuccessful use of a capability". 
\begin{description}
\item[t6] 




\item[t7] 


\end{description}
Both (t6) and (t7) map to the same  pseudo-code 
Fortunately, now (t6) and (t7) cannot be distinguished, since the intermediate  event cannot be observed if true. (In contrast, recall that the intermediate  event in (t4) and (t5) forces a distinction between them.)

Indeed, with (A\ref{Agrant}) and (A\ref{Ablock}) there remains no way to detect the elapse of time, except by comparing timestamps. To prevent the latter, we
 assume that:
\begin{description}
\item[\A]\label{Aclock} Timestamps are encrypted.\end{description}
Let  be a secret key shared by  and . The encryption of a term  with  under a random coin  is written as . We remove  message  and modify the following messages in  to obtain the protocol . (Note that randomization takes care of (R\ref{Rdiff}), so capabilities are not \emph{required} to mention users here.) 

Accordingly, we remove the messages , , and  from  to obtain the protocol . We can then prove the following new result (see Section \ref{sec:mpdyn}):
\begin{theorem}  securely implements  assuming {\rm (A\ref{Agrant})}, {\rm (A\ref{Ablock})}, and {\rm (A\ref{Aclock})}.
\end{theorem}
\noindent
The key observation is that with (A\ref{Agrant}), (A\ref{Ablock}), and (A\ref{Aclock}), time can stand still (so that capabilities never expire). 

\paragraph{\em Security in the general case}
More generally, we may consider plugging problematic leaks by static analysis. (Any such analysis must be incomplete because of the undecidability of the problem.) However, several  complications arise in this case. 
\begin{mylist}
\item The adversary can control the elapse of time by interacting with honest users in subtle ways. Such interactions lead to counterexamples of the same flavor as the one with (t4) and (t5) above, but are difficult to prevent statically without severely restricting the code run by honest users. For instance, even if the suspicious-looking pseudo-code  in (t4) and (t5) is replaced by an innocuous pair of inputs on a public channel , the adversary can still run the same code in parallel and serialize it by a pair of outputs on  (which serve as ``begin/end" signals).
\item Even if we restrict the code run by honest users, such that every use of a capability can be serialized immediately after its acquisition, the adversary can still force time to elapse \emph{after} a capability is sent to the file system and \emph{before} it is examined. Unless we have a way to constrain this elapse of time, we are in trouble. 
\end{mylist}
To see how the adversary can break security by interacting with honest users, consider the  pseudo-code below. Here  is a capability for operation , and  modifies access to ; further  and  denote input and output on public channels  and . 
\begin{description}
\item[t8] 
\item[t9] 
\end{description}
Although  immediately follows  in (t8), the delay between  and  can be detected by the adversary to force time to elapse between those events. Suppose that initially no user has access to  or ,  specifies that a honest user  may access , and  specifies that all users may access . Consider the following context. Here  and  are capabilities for .
\begin{description}
\item 


\end{description}
This context forces time to elapse between a pair of outputs on . The context can distinguish (t8) and (t9) by testing output on : in (t8)  does not authorize access to , so  is false and there is no output on ; on the other hand, in (t9) there is. Security breaks as a consequence. Consider implementation instances  and  with  as the only honest user and  and  as the only operations, such that only  can detect execution of  and all users can detect execution of ; let  run (t8) and  run (t9). The specification instances of  and  run  and , which cannot be distinguished: the execution of  can always be delayed until  is executed, so that  is true and there is an output on . Intuitively, an execution request in  commits to a time bound (specified by the timestamp of the capability used for the request) within which that request must be processed for progress; but operation requests in  make no such commitment. 

To solve this problem, we must assume that:
\begin{description}
\item[\A]\label{Abound} In  a time bound is specified for every operation request, so that the request is dropped if it is not processed within that time bound. 
\end{description}
Usual (unrestricted) requests now carry a time bound . Accordingly we modify the following messages in .

With (A\ref{Abound}), using an expired capability now has a counterpart in . Informally, if a capability for an operation  is produced at time  in , then any use of that capability in  maps to an execution request for  in  with time bound . There remains no fundamental difference between  and . 
We can then prove our main new result (see Section \ref{sec:mpdyn}):
\begin{theorem}[Main theorem]  securely implements  assuming {\rm (A\ref{Abound})}.\footnote{This result holds with or without (R\ref{Rstale}).}
\end{theorem}
\noindent 
Fortunately, (A\ref{Abound}) seems to be a reasonable requirement, and we impose that requirement implicitly in the sequel. 

\paragraph{\em Discussion}
Let us now revisit the principles developed in Sections \ref{intro-stat} and \ref{intro-dyn}, and discuss some alternatives.

First recall (R\ref{Rfake}), where we introduce fake capabilities to prevent premature leaks of information about the access policy . It is reasonable to consider that we do not care about such leaks, and wish to keep the original message   in . But then we must allow those leaks in the specification. For instance, we can make  public. 
More practically, we can add messages to  that allow a user to know whether it has access to a particular operation.


Next recall (R\ref{Rexpire}) and (R\ref{Rdelay}), where we introduce the midnight-shift scheme. This scheme can be relaxed to allow different capabilities to expire after different intervals, so long as administrative operations that affect their correctness are not executed before those intervals elapse. Let  be a function over users , operations , and clock values  that produces time intervals. We may have that:
\begin{description}
\item[R\ref{Rexpire}] Any capability for  and  that is produced at time  expires at time .
\item[R\ref{Rdelay}] If an administrative operation affects the access decision for  and  and is requested in the interval , it is executed at the clock tick to time .  
\end{description}
This scheme remains sound, since any capability for  and  that is produced at  and expires at  certifies a correct access decision for  and  between .

Finally, the implementation details in Sections~\ref{intro-stat}~and~\ref{intro-dyn} are far from unique. Guided by the same underlying principles, we can design capabilities in various other ways. For instance, we may have an implementation that does not require : any capability is of the form , where  is a fresh nonce and  is the predicate .
Although this design involves more cryptography than the one in , it reflects better practice: the access decision for  and  under  is explicit in the content of any capability that certifies that decision. What does this design buy us? Consider applications where the access decision is not a boolean, but a label, a decision tree, or some arbitrary data structure. 
The design in  requires a different signing key for each value of the access decision. Since the number of such keys may be infinite, verification of capabilities becomes very inefficient. The design above is appropriate for such applications, and we develop it further in Section~\ref{apply}. 



\section{Definitions and proof techniques}\label{theory}
\noindent
Let us now develop formal definitions and proof techniques for security and safety; these serve as background for Section \ref{sec:mpdyn}, where we present formal models and proofs for security and safety of  with respect to . 

Let  be a precongruence on processes and  be the associated congruence. A process  under a context  is written as . Contexts act as tests for behaviors, and  means that any test that is passed by  is passed by ---in other words, `` has no more behaviors than ". 

We describe an implementation as a binary relation  over processes, which relates specification instances to implementation instances. This relation conveniently generalizes a refinement map \cite{refmap}. 

\begin{definition}[Full abstraction] An implementation  is fully abstract if it satisfies:

\end{definition}
\noindent
(\textsc{Preservation}) and (\textsc{Reflection}) are respectively soundness and completeness of the implementation under . Security only requires soundness.
\begin{definition}[\emph{cf.} Definition 2 [Security] An implementation is secure if it satisfies \textsc{(Preservation)}.
\end{definition}
\noindent
Intuitively, a secure implementation does not \emph{introduce} any interesting behaviors---if  and  are in a secure  and  has no more behaviors than , then  has no more behaviors than . A fully abstract implementation moreover does not \emph{eliminate} any interesting behaviors.

Any subset of a secure implementation is secure. Security implies preservation of . 
Finally, testing itself is trivially secure since  is closed under any context.
\begin{proposition}\label{trivprop} Let  be any context. Then  is secure for any set of processes .
\end{proposition}
\noindent
On the other hand, a context may eliminate some interesting behaviors by acting as a test for those behaviors. A fully abstract context does not; it merely \emph{translates} behaviors. 
\begin{definition}[Fully abstract context] A context  is fully abstract for a set of processes  if  is fully abstract.
\end{definition}
\noindent
A fully abstract context can be used as a wrapper to account for any benign differences between the implementation and the specification. An implementation is safe if it does not introduce any behaviors modulo such a wrapper.
\begin{definition}[\emph{cf.} Definition 1 [Safety]\label{safetydef} An implementation  is safe if there exists a fully abstract context  for the set of specification instances such that  satisfies:
\end{definition}
\noindent
Let us see why  must be fully abstract in the definition. Suppose that it is not. Then for some  and  we have  and . Intuitively,  ``covers up" the behaviors of  that are not included in the behaviors of . 
Unfortunately, those behaviors may be unsafe. For instance, let  be a pi calculus process \cite{polypi} that does not contain public channels, and  be the set of specification instances---we consider any output on a public channel to be unsafe. Let  be a public channel; let  and . Then  and , as required. But clearly  is unsafe by our assumptions; yet , so that by definition  is safe! The definition therefore becomes meaningless. 





We now present some proof techniques. A direct proof of security requires mappings between subsets of . Those mappings may be difficult to define and manipulate. Instead a security proof may be built modularly by showing simulations, as in a safety proof. Such a proof requires simpler mappings between processes. \begin{proposition}[Proof of security]\label{pf-fullabs}
Let  and  be contexts such that for all , 
	,
	, and
	.
Then  is secure. 
\end{proposition}
\begin{proof} Suppose that , , and . Then
. 
\end{proof}
\noindent
Intuitively,  is secure if  and  both satisfy \textsc{(Inclusion)}, and the witnessing contexts ``cancel" each other. A simple technique for proving full abstraction for contexts follows as a corollary. 
\begin{corollary}[Proof of full abstraction for contexts]\label{pf-pres} Let there be a context  such that for all 
,
	. 
Then  is a fully abstract context for .
\end{corollary}
\begin{proof} Take  and  in the proposition above to show that  is secure. The converse follows by Proposition \ref{trivprop}.
\end{proof}


\paragraph{Theory for the applied pi calculus}

Let  range over names,  over names and variables,  over terms, and  over extended processes. Semantic relations include the binary relations , , and  over extended processes (structural equivalence, reduction, and labeled transition); here labels  are of the form  or  (where  and ). Both  and  are closed under  and  is closed under arbitrary evaluation contexts. 

We recall some theory on may testing for applied pi calculus programs.
\begin{definition}[Barb] A barb  is a predicate that tests possible output on ; we write  if  for some , , and . A weak barb  tests possible eventual output on , \emph{i.e.}, .
\end{definition}
\noindent
\begin{definition}[Frame]
Let  be closed. Then we have  for some , , and  such that ; define .
\end{definition}
\begin{definition}[Static equivalence] Let  and  be closed. Then  is statically equivalent to , written , if there exists , , and  such that , , , and for all  and ,

\end{definition}
\begin{proposition}  if and only if .
\end{proposition}
\begin{proof} By induction on the structure of closing evaluation contexts.
\end{proof}

We can prove  by showing a simulation relation that approximates .
\begin{definition}[Simulation preorder]
Let  be the largest relation  such that for all  and ,  implies
\begin{itemize}
\item 
\item 
\item 
\end{itemize}
\end{definition}
\begin{proposition}[Proof of testing precongruence]\label{pf-prec} . 
\end{proposition}

\section{Models and proofs for static access policies}\label{sec:mpstat}
We now present implementation and specification models and security proofs for static access policies. Models and proofs for dynamic access policies follow essentially the same routine, and are presented in the next section.

\subsection{Preliminaries}
We fix an equational theory  with the following properties.
\begin{itemize}
\item  includes a theory of natural numbers with symbols  (zero),  (successor), and  (less than or equal to).
\item  includes a theory of finite tuples with symbols  (indexed concatenate) and  (indexed project).
\item  contains exactly one equation that involves the symbol , which is

\end{itemize}
Clients are identified by natural numbers; we fix a finite subset  of  and consider any user not identified in  to be dishonest. 

File-system code and other processes are conveniently modeled by parameterized process expressions, whose semantics are defined (recursively) by extending the usual semantic relations , , and . 

\begin{figure}
\hspace{-0.55cm}
\fbox{\parbox{13.0cm}{\small


\fbox{\parbox{12.75cm}{


}}
}}
\caption{A traditional file system with local access control}
\label{fig:ts-s}
\end{figure}

\begin{figure}
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small



\fbox{\parbox{12.75cm}{


}}
}}
\caption{A network-attached file system with distributed access control}
   \label{fig:nas-s}
\vspace{-0.3cm}
\end{figure}

\subsection{Models}

Figures \ref{fig:ts-s} and \ref{fig:nas-s} show applied pi calculus models for the file systems under study. We ignore the rules in the inner boxes in these figures (labeled (\textsc{Dummy}...)) in a first reading.

Figure \ref{fig:ts-s} models a traditional file system (with local access control). The file system is parameterized by an access policy , a store , and a renaming  of its default interface. That interface includes a channel  for every ; intuitively, a user identified by  may send operation requests on this channel. 

Processes  and  denote internal states. 
In the equational theory  means that user  may access  under , and  means that the execution of  on store  under decision  returns  and store . Decisions are derived by  as follows.

A traditional storage system may be described as 

Here  is code run by honest users; the file-system exports the default interface (implicitly renamed by ``identity"), and channels associated with honest users are hidden from the context. The context may be arbitrary and is left implicit; in particular, channels associated with dishonest users are available to the context.

Figure \ref{fig:nas-s} models a network-attached file system (with distributed access control). As above, the file system is parameterized by an access policy , a store , and a renaming  of its default interface. That interface includes channels  and  for every ; intuitively, a user identified by  may send authorization requests on  and execution requests on . 

Processes , , and  denote internal states. 
In the equational theory  and  have the same meanings as above. Capabilities and decisions are derived by  and   as follows.


A network-attached storage system may be described as 

As above,  is code run by honest users; the file-system exports the default interface and hides the keys that authenticate capabilities. Channels associated with honest users are hidden from the context. The context may be arbitrary and is left implicit; in particular, channels associated with dishonest users are available to the context.

\subsection{Proofs of security}
We prove that the implementation is secure, safe, and fully abstract with respect to the specification. We begin by outlining the proofs, and then present details.
\subsubsection{Outline}
Let , , and  range over access policies, stores, and code for honest users that are ``wellformed" in the implementation. Let  abstract such , , and  in the specification. We define

We prove that  is secure by showing contexts  and  such that:
\begin{figure}
\hspace{-0.6cm}\fbox{\parbox{13.2cm}{\small





}}
\caption{Abstraction function}
   \label{fig:abs-s}
\end{figure}
\begin{lemma}\label{static-lemma} For any , , and ,
\begin{enumerate}
\item 
\item 
\item 
\par 
\end{enumerate}
\end{lemma}
Proposition \ref{pf-fullabs} then applies. Moreover we show:
\begin{lemma}\label{static-safetylemma} For any , , and , 

\end{lemma}
Now  is secure by Proposition \ref{pf-fullabs}. Thus  is proved fully abstract. Moreover Lemmas \ref{static-lemma}.1--2 already imply the converse of Lemma \ref{static-safetylemma}; so  is a fully abstract context by Corollary \ref{pf-pres} (taking ). Thus  is proved safe. 


We now revisit Figures \ref{fig:ts-s} and \ref{fig:nas-s} and focus on the rules in the inner boxes. Those rules define processes  and . Intuitively, these processes translate public requests from  to  and from  to . Let  and  include the public interfaces of  and . We define 

The abstraction function  is shown in Figure \ref{fig:abs-s}. Here  contains special names whose uses in well-formed code are either disciplined or forbidden. 

The names in  are invented to simplify proofs below.
\begin{figure}
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small







}}
\caption{Simulation relation for Lemma \ref{static-lemma}.1 ()}
\label{fig:simreln-1-s}
\end{figure}
\begin{figure}
\hspace{-0.4cm}\fbox{\parbox{12.8cm}{\small




}}
\caption{Simulation relation for Lemma \ref{static-lemma}.2 ()}
\label{fig:simreln-2-s}
\end{figure}
\begin{figure}
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small












}}
\caption{Simulation relation for Lemma \ref{static-lemma}.3 ()}
\label{fig:simreln-3-s}
\end{figure}

\begin{figure}[h]
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small






}}
\caption{Simulation relation for Lemma \ref{static-safetylemma} ()}
\label{fig:simreln-4-s}
\end{figure}

\subsubsection{Simulation relations}

Figures \ref{fig:simreln-1-s}, \ref{fig:simreln-2-s}, and \ref{fig:simreln-3-s} show simulation relations for Lemma \ref{static-lemma}.1--3. All these relations are closed under . Here  and  rename the public interfaces of  and  and  renames the private authentication keys  and . 

These renamings map to names in  that do not occur in wellformed code (see Figure~\ref{fig:abs-s}). 
In particular, the purpose of  and  is to rename some public channels to fresh ones that can be hidden by restriction in  and . (A similar purpose is served by quantification in logic.) Hiding those names strengthens Lemmas \ref{static-lemma}.1--2 while not affecting their proofs; but more importantly, the restrictions are required to prove Lemma \ref{static-lemma}.3. Further the purpose of  is to abstract terms that may be available to contexts. Such terms must be of type ; intuitively,  and  may appear only as authentication keys in capabilities issued to dishonest users.

We show that term abstraction preserves equivalence in the equational theory. 
\begin{lemma} Suppose that  and . Then  iff .
\end{lemma}
This lemma is required to show static equivalence in proofs of soundness for the relations , , and  in Figures \ref{fig:simreln-1-s}, \ref{fig:simreln-2-s}, and \ref{fig:simreln-3-s}, which in turn lead to Lemma \ref{static-lemma}. We prove that those relations are included in the simulation preorder.
\begin{lemma} , , and .
\end{lemma}
Intuitively, by  a network-attached storage system may be simulated by a traditional storage system by forwarding public requests directed at  to a hidden  interface (via ). Symmetrically, by  a traditional storage system may be simulated by a network-attached storage system by forwarding public requests directed at  to a hidden  interface (via ). 
Finally, by  a network-attached storage system may simulate another network-attached storage system by filtering requests directed at  through a hidden  interface before forwarding them to a hidden  interface (via ). This rather mysterious detour forces a fresh capability to be acquired for every execution request.

By definition of  and alphaconversion to default public interfaces, we have for any , , and :
\begin{enumerate}
\item 
\item 
\item 
\par 
\end{enumerate}
Lemma \ref{static-lemma} follows by Proposition \ref{pf-prec}. Thus  is secure.

Further, Figure \ref{fig:simreln-4-s} shows a simulation relation for Lemma \ref{static-safetylemma}. We prove that the relation  is included in the simulation preorder.
\begin{lemma} .
\end{lemma}
By definition of  and alphaconversion to default public interfaces, we have for any , , and :

Lemma \ref{static-safetylemma} follows by Proposition \ref{pf-prec}. Thus  is safe and fully abstract.  








\section{Models and proofs for dynamic access policies}\label{sec:mpdyn}
Next we present models and proofs for dynamic access policies, following the routine of Section \ref{sec:mpstat}.

\subsection{Models}








\begin{figure}
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small








\fbox{\parbox{12.75cm}{




}}
}}
\caption{A traditional file system with local access control}
\label{fig:ts-sem-d}
\vspace{-0.2cm}
\end{figure}

\begin{figure}
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small








\fbox{\parbox{12.75cm}{



}}
}}
\caption{A network-attached file system with distributed access control}
   \label{fig:nas-sem-d}
\end{figure}

The models extend those in Section \ref{sec:mpstat}, and are shown in Figures \ref{fig:ts-sem-d} and \ref{fig:nas-sem-d}. (As usual, we ignore the rules in the inner boxes in a first reading.) Interfaces are extended with channels  and  for every , on which users identified by  send administration requests in the implementation and the specification. 

In the equational theory  and  have the same meanings as in Section \ref{sec:mpstat}. Capabilities are derived by  as follows. 

Recall that administrative operations scheduled at time  are executed at the next clock tick (to ). In the equational theory  means that an administrative operation  pushed on schedule  under decision  at  returns  and the schedule ; and  means that an access policy  synchronized under schedule  at  returns the access policy .

A traditional storage system may be described as

where  is code run by honest users,  is an access policy and  is a store; initially the schedule is empty and the time is .

Similarly a network-attached storage system may be described as





\begin{figure}
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small





}}
\caption{Abstraction function}
   \label{fig:abs-d}
\end{figure}











As usual, let , , and  range over access policies, stores, and code for honest users that are ``wellformed" in the implementation, and let  abstract such , , and  in the specification. We define

Figure \ref{fig:abs-d} shows the abstraction function . Here


\subsection{Examples of security}
At this point we revisit the ``counterexamples" in Section \ref{intro-dyn}. By modeling them formally in this setting, we show that those counterexamples are eliminated. 

Recall (t1) and (t2).
\begin{description}
\item[t1] 
\item[t2] 
\end{description}
The following fragments of  code formalize these traces.
\begin{description}
\item[I1] 
\item[I2] 
\end{description}
This code is abstracted to the following fragments of  code.
\begin{description}
\item[S1] 
\item[S2] 
\end{description}
Now whenever (I1) and (I2) can be distinguished, so can (S1) and (S2). Indeed the time bound  is the same as the timestamp in ; so (in particular) the operation request in (S1) is dropped whenever the execution request in (T1) is dropped.

A similar argument counters the ``dangerous" example with (t4) and (t5):
\begin{description}
\item[t4] 
\item[t5] 
\end{description}

Finally, recall (t8) and (t9).
\begin{description}
\item[t8] 
\item[t9] 
\end{description}
The following fragment of  code formalizes (t8).
\begin{description}
\item[I3] 
\par 
\end{description}
This code is abstracted to the following fragment of  code.
\begin{description}
\item[S3] 
\par 
\end{description}
A  context distinguishes (I3) and (t9):
\begin{description}
\item 
\par 
\end{description}
But likewise a  context distinguishes (S3) and (t9):
\begin{description}
\item 
\par 
\end{description}


\begin{figure}
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small









}}
\caption{Simulation relation for Lemma \ref{dynamic-lemma}.1 ()}
\label{fig:simreln-1-d}
\end{figure}

\begin{figure}
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small









}}
\caption{Simulation relation for Lemma \ref{dynamic-lemma}.2 ()}
\label{fig:simreln-2-d}
\end{figure}




\begin{figure}
\vspace{-0.5cm}
\hspace{-0.5cm}\fbox{\parbox{13.0cm}{\small















}}
\caption{Simulation relation for Lemma \ref{dynamic-lemma}.3 ()}
\label{fig:simreln-3-d}
\end{figure}

\subsection{Proofs of security}

We show that  is secure, safe, and fully abstract. Recall the contexts  and  defined in Section \ref{sec:mpstat}. The processes  and  are redefined in the inner boxes in Figures \ref{fig:ts-sem-d} and \ref{fig:nas-sem-d}. In particular, the rule \textsc{(Dummy Op Req)} in Figure \ref{fig:nas-sem-d} translates time-bounded operation requests by  contexts. 

Simulation relations for security are shown in Figures \ref{fig:simreln-1-d}, \ref{fig:simreln-2-d}, and \ref{fig:simreln-3-d}, and a simulation relation for safety and full abstraction is shown in Figure \ref{fig:simreln-4-d}. Here

A binary relation  (``leads-to") is defined over the product of access policies and clocks. Access policies may change at clock ticks (but not between).

As usual, any term that may be available to contexts must be of type .

\begin{figure}
\vspace{-1.0cm}
\hspace{-1.8cm}\fbox{\parbox{15.6cm}{\small
















}}
\caption{Simulation relation for Lemma \ref{dynamic-safetylemma} ()}
\label{fig:simreln-4-d}
\end{figure}

We prove that the relations , , and  in Figures \ref{fig:simreln-1-d}, \ref{fig:simreln-2-d}, and \ref{fig:simreln-3-d} are included in the simulation preorder. Some interesting points in those proofs are listed below. 
\begin{itemize}
\item In Section \ref{sec:mpstat}, when an operation request is sent in  we send an appropriate authorization request in , obtain a capability, and send an execution request with that capability (see  in Figure \ref{fig:simreln-2-s}). In contrast, here when an operation request is sent in  we \emph{wait} after sending an appropriate authorization request in  (see  in Figure \ref{fig:simreln-2-d}); we continue only when that operation request in  is processed, when we obtain a capability in , send an execution request with that capability, and process the execution request. 

But why wait? Suppose that the operation request in  carries a time bound ; now if we obtain a capability in  before the operation request in  is processed, we commit to a finite time bound, which breaks the simulation. 
\item As before,  forces a fresh capability to be acquired for every execution request by filtering execution requests in  through  and back. When an execution request is sent in  under  we  send an execution request with the same capability in  (see  in Figure \ref{fig:simreln-3-d}). But under  a fresh capability is obtained and the execution request is sent again with the fresh capability. If the capability in the original request expires before the fresh capability, the simulation breaks. Fortunately operation requests in  carry time bounds, so we can communicate this expiry bound through . In fact there seems to be no way around this problem \emph{unless} time bounds can be specified in operation requests in !
\end{itemize}
By Proposition \ref{pf-prec} we have:
\begin{lemma}\label{dynamic-lemma} For any , , and ,
\begin{enumerate}
\item 
\par 
\item 
\par 
\item 
\par 
\end{enumerate}
\end{lemma}
So by Proposition \ref{pf-fullabs},  is secure. 

Further we prove that the relation  in Figure \ref{fig:simreln-4-d} is also included in the simulation preorder. By Proposition \ref{pf-prec} we have: 
\begin{lemma}\label{dynamic-safetylemma} For any , , and , 
\vspace{0.2cm}
\par 
\par 
\end{lemma}
So by Lemmas \ref{dynamic-lemma}.1--2 and Corollary \ref{pf-pres},  is safe and fully abstract. 







\section{Designing secure distributed protocols}\label{apply}
\noindent
In the preceding sections, we present a thorough analysis of the problem of distributing access control. Let us now apply that analysis to a more general problem.


Suppose that we are required to design a distributed protocol that securely implements a specification. (The specification may be an arbitrary computation.) We can solve this problem by partitioning the specification into smaller computations, running those computations in parallel, and securing the intermediate outputs of those computations so that they may be released and absorbed in any order. 
In particular, we can design  by partitioning  into access control and storage, running them in parallel, and securing the intermediate outputs of access control as capabilities. The same principles should guide any such design. For instance,  by (R\ref{Rfake}) and (R\ref{Rdiff}) intermediate outputs should not leak information prematurely;  by (R\ref{Rexpire}) and (R\ref{Rdelay}) such outputs must be timestamped and the states on which they depend must not change between clock ticks; and  by (A\ref{Abound}) the specification must be generalized with time bounds.

\paragraph{\em Computation as a graph}
\noindent
We describe a computation as a directed graph . The \emph{input nodes}, collected by , are the nodes of indegree . The \emph{output nodes}, collected by , are the nodes of outdegree . Further, we consider a set of  \emph{state nodes}  such that . As a technicality, any node that is in a cycle or has outdegree  must be in . 


Nodes other than the input nodes run some code. Let  contain all terms and  be a strict total order on . We label each  with a function , and each  with a function . Further, each state node carries a shared clock, following the midnight-shift scheme. 

A \emph{configuration}  consists of a partial function  such that , and a total function . Intuitively,  assigns values at the state nodes and some other nodes, and  assigns times at the state nodes. For any , the function  outputs the value at , taking as inputs the values at each incoming , and the value at  if  is a state node; further, if such , the value at  is ``consumed" on input. Formally, the operational semantics is given by a binary relation  over configurations.


As usual, we leave the context implicit; the adversary is an arbitrary context that can write values at , read values at , and read times at .

For example, a graph that describes  is:

Here , , , and . Intuitively,  carries accumulators, and  and  carry inputs and outputs for access modifications;  carries access policies, and  carries access decisions;  carries stores, and  and  carry inputs and outputs for store operations. We define: 


\paragraph{\em Distribution as a graph cut}
\noindent
Once described as a graph, a computation can be distributed along any cut of that graph. For instance,  can be distributed along the cut  to obtain . We present this derivation formally in several steps. 


\paragraph{Step 1} 
For each , let  be the set of state nodes that have paths to , and  be the set of input nodes that have paths to  without passing through nodes in .   Then  can be written in a form where, loosely, the values at  and the times at  are explicit in  for each node .  
Formally, the \emph{explication} of  is the graph  where  and . We define:



This translation is sound and complete. 
\begin{theorem}\label{thm:closure}  is fully abstract with respect to .
\end{theorem}
\noindent
For example, the explication of the graph for  is:
Here  is of the form  rather than ;  the ``input" , the ``time" , and the ``output"  of an access check are all explicit in . A capability can be conveniently constructed from this form (see below). 

\paragraph{Step 2}
Next, let  be any cut. As a technicality, we assume that . The \emph{distribution} of  along  is the graph (\mathcal V^\)\mathcal V^\~|~(v,\_) \in \mathcal E_0\}\mathcal E^\)~|~(v,\_) \in \mathcal E_0\} \cup \{(v^\. 
Let  and  be secret keys shared by  and (v,\_) \in \mathcal E_0_v(t_1,\dots,t_{\In(v)}) = \func{mac}(\tup{t,\{m,t'\}_{E_v}},K_v)}\infer{(v,\_) \in \mathcal E_0 \\ \tau(S(v))\mbox{ is included in }t}
{ \lambda^\}(\tup{t,\func{mac}(\tup{t,\{\_,t'\}_{E_v}},K_v)}) = \tup{t,t'}}\infer{v \in \mathcal V \setminus \mathcal V_i \\ (v,\_) \notin \mathcal E_0}{ \lambda^\(v,\_) \in \mathcal E_0v^\ as  does in ; those values are encoded and released at , absorbed at , and decoded back at {\it IS}^{d+}\{(\bullet_6,\star_7)\} & \hat{\bullet_8}  \\
&\downarrow & & \uparrow && \uparrow \\
& \hat{\bullet_3} & & \hat{\bullet_5} &&\overline{\bullet_6}
\end{array}
\right.
\infer{(v,u) \in \mathcal E_0 \\ \tau(S(v)) \leq T}
{\lambda^\#_v(t_1,\dots,t_{\In(v)},T) = \lambda_v(t_1,\dots,t_{\In(v)})}\infer{v \in \mathcal V \setminus \mathcal V_i \\ (v,\_) \notin \mathcal E_0}{\lambda^\#_v = \lambda_v}
\left.
\begin{array}{lcrcr}
\bullet_1 \longrightarrow & \star_2 & \longrightleftarrows \star_4 \longrightarrow \!\!& \bullet_6 & \longrightarrow \star_7 \longrightarrow \bullet_8\\
& \downarrow & \nearrow\!\! & \uparrow\\
& \bullet_3 & \bullet_6^\#~~~~& \bullet_5
\end{array}
\right.
 is fully abstract with respect to .
\end{theorem}
\noindent
By Theorem \ref{thm:general}, the graph for  is fully abstract with respect to the revised graph for . 

Similarly, we can design  from . The induced subgraph of  without  describes . We define  for some static . Distributing along the cut , we obtain the induced subgraph of  without . 
This graph describes a variant of , with  of the form . (Here capabilities do not carry timestamps.) By Theorem \ref{thm:general}, the graph for  is fully abstract with respect to a trivially revised graph for , where .


\section{Conclusion}
\noindent
We present a comprehensive analysis of the problem of implementing distributed access control with capabilities. In previous work, we show how to implement static access policies securely \cite{ChaudhuriAbadi-FMSE05} and dynamic access policies safely \cite{ForteChaudhuriA06}. In this paper, we explain those results in new light, revealing the several pitfalls that any such design must care about for correctness, while discovering interesting special cases that allow simpler implementations. Further, we present new insights on the difficulty of implementing dynamic access policies securely (a problem that has hitherto remained unsolved). We show that such an implementation is in fact possible if the specification is slightly generalized. 

Moreover, our analysis turns out to be surprisingly general. Guided by the same basic principles, we show how to automatically derive secure distributed implementations of other stateful computations. This approach is reminiscent of secure program partitioning \cite{spp}, and investigating its scope should be interesting future work.

\paragraph{Acknowledgments}
This work owes much to Mart\'in Abadi, who formulated the original problem and co-authored our previous work in this area. Many thanks to him and Sergio Maffeis for helpful discussions on this work, and detailed comments on an earlier draft of this paper. It was Mart\'in who suggested the name ``midnight-shift". Thanks also to him and C\'edric Fournet for clarifying an issue about the applied pi calculus, which led to simpler proofs.



















\bibliography{ref}
\bibliographystyle{abbrv} 

\end{document}
