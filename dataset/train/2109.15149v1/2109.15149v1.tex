\section{Experimental Evaluation}

\subsection{Datasets}

To evaluate the performance and generality of DEKM, we conduct experiments on benchmark datasets and compare with state-of-the-art methods. In order to show that DEKM works well on various datasets, we choose four image datasets that cover domains such as handwritten digits, objects,
and human faces and three text datasets. Table \ref{tab1_ds} provides a brief description for each dataset. And some examples in the image datasets are shown in Figure~\ref{fig_samples}.

MNIST~\cite{lecun1998gradient} is a dataset that consists of 70,000 hand-written grayscale digit images. Each image is of size 28$\times$28 pixels. USPS is a dataset of handwritten grayscale digit images from the USPS postal service, containing 9,298 images of size 16$\times$16 pixels. COIL-20~\cite{nene1996columbia} is a dataset containing 1,440 color images of 20 objects (72 images per object). The objects have a wide variety of complex geometric and reflectance characteristics. The size of each image is resized to 28$\times$28 pixels. FRGC is a human face dataset. Following~\cite{yang2016joint}, we randomly select 20 subjects from the original dataset and collect their 2,462 face images. Similarly, we crop the face regions and resize them into 28$\times$28 pixels. For all the image datasets, each image is normalized by scaling between 0 and 1. REUTERS-10K~\cite{DEC_xie2016unsupervised} contains a random subset of 10,000 samples from REUTERS dataset which has about 810,000 English news stories. REUTERS-10K contains four categories: corporate/industrial, government/social, markets, and economics. The 20 Newsgroups dataset (20NEWS)~\cite{lang1995newsweeder} contains 18,846 documents labeled into 20 different classes, each corresponding to a different topic. Reuters Corpus Volume I (RCV1)~\cite{lewis2004rcv1} contains 804,414 manually categorized newswire stories. Following~\cite{fard2020deep}, we sample from the full RCV1 collection a random subset of 10,000 documents from the largest four categories, denoted by RCV1-10K. For the three text datasets, we represent each document as a tf-idf feature vector on the 2,000 most frequently occurring word stems. And each sample $\mathbf{x}_i$ is normalized so that $\frac{1}{d}\lVert\mathbf{x}_i\rVert_2^2$ is approximately 1, where $d$ is the dimension of the input space.

\begin{table}[htbp]
	\caption{Description of datasets}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			Dataset                        & Sample \# & Cluster \# & Dimensions \\ \hline
			MNIST                          & 70,000      & 10      & 28$\times$28$\times$1    \\ \hline
			USPS                             & 9,298        & 10      & 16$\times$16$\times$1    \\ \hline
			COIL-20                       & 1,440         & 20      & 28$\times$28$\times$1    \\ \hline
			FRGC                             & 2,462        & 20      & 32$\times$32$\times$3    \\ \hline
			REUTERS-10K              &10,000       &4         &2,000 \\ \hline    
			20NEWS                       &18,846       &20         &2,000 \\ \hline 
			RCV1-10K                      &10,000       &4         &2,000 \\ \hline 
		\end{tabular}
		\label{tab1_ds}
	\end{center}
\end{table}
\begin{figure}[htbp]
	\centerline{\includegraphics[width=\columnwidth]{img/ds_sample.png}}
	\caption{Some image samples, from top row to bottom row, come from MNIST, USPS, COIL-20 and FRGC datasets, respectively.}
	\label{fig_samples}
\end{figure}

\subsection{Baseline Methods}
We compare the proposed DEKM with the following methods:
\begin{itemize}
	\item K-means: We perform K-means on the original data.
	\item PCA+K-means: We perform K-means in the space spanned by the first $p$ principal components of data using Principal Component Analysis (PCA), where $p$ is selected to keep  90\% of data variance. 
	\item AE+K-means: We perform K-means in the embedding space of our pretrained convolutional/MLP autoencoder.
	\item DEC~\cite{DEC_xie2016unsupervised}: It uses an MLP autoencoder to find the embedding space and then performs clustering in the embedding space by minimizing the Kullback-Leibler (KL) divergence between the cluster distribution and a target Studentâ€™s t-distribution. 
	\item DCEC~\cite{DCEC_guo2017deep}: It replaces the MLP autoencoder in DEC with a convolutional autoencoder.
	\item IDEC~\cite{IDEC_guo2017improved}: It replaces the MLP autoencoder in DEC with a under-complete autoencoder that preserves the local structure of data.
	\item DCN~\cite{DCN_yang2017towards}: It combines the objective function of the MLP autoencoder with that of K-means and alternately optimizes them.
	\item DKM~\cite{fard2020deep}: It considers the K-means objective as a limit of a differentiable function and adopts stochastic gradient descent to jointly optimize representation learning and clustering.
\end{itemize} 

\subsection{Experimental Settings}

Since convolutional neural network (CNN) is good at capturing the semantic visual features of input images, we exploit convolutional autoencoder to find the embedding space for the image datasets. Specifically, we use three convolutional layers followed by a dense layer (embedding layer) in the encoder-to-decoder pathway. The channel numbers of the three convolutional layers are 32, 64, and 128 respectively. The kernel sizes are set to 5$\times$5, 5$\times$5, and 3$\times$3 respectively. The stride of all the convolutional layers is set to two. The number of neurons in the embedding layer is set to the number of clusters of datasets. The decoder is a mirror of the encoder and the output of each layer of the decoder is appropriately zero-padded to match the input size of the corresponding encoder layer. All the intermediate layers of the convolutional autoencoder are activated by ReLU \cite{krizhevsky2012imagenet}. For the text datasets, we use a fully connected multilayer perceptron (MLP) for the backbone of autoencoder. Following the settings in DEC~\cite{DEC_xie2016unsupervised}, the encoder has dimensions of $d$-500-500-2000-10, where $d$ is the dimension of the input data. The decoder is a mirror of the encoder. All the intermediate layers are activated by ReLU. The weights of all the layers are initialized by Xavier approach \cite{glorot2010understanding}. The Adam \cite{kingma2014adam} optimizer is adopted with the initial learning rate  $l=0.001$, ${\beta_1} = 0.9$, ${\beta_2} = 0.999$. 
We stop the clustering process when there are less than 0.1\% of samples that change their clustering assignments between two consecutive iterations. Our code is available at: \textcolor{blue}{\url{https://github.com/spdj2271/DEKM}}. All the experiments are conducted on a machine with one Intel(R) Xeon(R) CPU (2.00GHz) and one Nvidia Tesla P100 GPU.



\subsection{Evaluation Metric}

To evaluate the clustering methods, we adopt two standard evaluation metrics: normalized mutual information (NMI)~\cite{vinh2010information} and unsupervised clustering accuracy (ACC) \cite{xu2003document}. Both the NMI and ACC values are in the range $\left[ 0, 1\right] $. The higher the values, the better the clustering results. NMI is an information-theoretic measure, which calculates the normalized measure of similarity between the ground-truth labels and the obtained cluster assignments. NMI is defined as follows:

\begin{equation}
	\mbox{NMI} = \frac{2\times I(\mathcal{G};\mathcal{C})}{H(\mathcal{G})+H(\mathcal{C})}
\end{equation}
where $\mathcal{G}$ is the ground-truth, $\mathcal{C}$ is the cluster assignments, $I$ denotes mutual information, and $H$ denotes entropy.

ACC measures the proportion of samples whose cluster assignments can be correctly mapped to the ground-truth labels. ACC is defined as follows:
\begin{equation}
	\mbox{ACC} = \max_m \frac{\sum_{i = 1}^n \mathds{1}\left\{ \mathbf{g}_i = m\left( \mathbf{c}_i \right) \right\} }{n}
\end{equation}
where $\mathbf{g}_i$ is the ground-truth label of the $i$-th data point, $\mathbf{c}_i$ is the cluster assignment of the $i$-th data point, $m$ ranges over all possible one-to-one mappings between ground-truth labels and cluster assignments. The mapping is based on the Hungarian algorithm \cite{kuhn2005hungarian}. 

\begin{table*}[!htb]
	\caption{Clustering results of different algorithms in terms of the unsupervised clustering accuracy (ACC\%) and normalized mutual information (NMI\%). The results marked by $^*$ come from the original papers. $-$ denotes that the result is not available. DEKM\_F means that DEKM uses the full-batch updating strategy.}
	\begin{center}
		\resizebox{\textwidth}{!}{
		\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
			\hline
			\multirow{2}{*}{Method} & \multicolumn{2}{c|}{MNIST}      & \multicolumn{2}{c|}{USPS}       & \multicolumn{2}{c|}{COIL-20}    & \multicolumn{2}{c|}{FRGC}    & \multicolumn{2}{c|}{REUTERS-10K} & \multicolumn{2}{c|}{20NEWS} & \multicolumn{2}{c|}{RCV1-10K}   \\ \cline{2-15} 
			& ACC                       &NMI                 &ACC               &NMI                  &ACC             &NMI            &ACC             &NMI            & ACC          & NMI        & ACC          & NMI        & ACC          & NMI      \\ \hline
			K-means                                                       &53.40                      &50.00             &66.81             &62.62               &25.28          &67.17          &12.63          &18.27          &54.05         &41.91       &21.59         &19.70      &51.96          &31.35\\ \hline
			PCA+K-means                                           &53.13                      &49.91              &54.20             &60.96              &26.53          &65.34          &12.55          &15.19          &53.97         &41.35      &21.75         &20.46      &51.91          &31.39\\ \hline
			AE+K-means                               &85.47                     &80.53             &74.84             &74.16              &68.82           &79.56          &33.31          &42.81          &70.52          &49.02      &40.24        &37.60      &64.11         &41.12\\ \hline
			DEC    &84.30$^*$           &83.72$^*$     &73.68$^*$     &75.29$^*$     &65.42$^*$   &80.49$^*$ &37.10$^*$  &44.60$^*$  &72.17$^*$    &54.87          &35.65            &43.79          &66.81           &45.24        \\ \hline
			DCEC            &87.16                     &85.92              &78.35              &81.34              &71.88          &\textbf{82.24}            &33.40        &41.50    &72.17    &54.87          &35.65            &43.79          &66.81           &45.24        \\ \hline
			IDEC        &88.06$^*$           &86.72$^*$     &76.05$^*$     &78.46$^*$      &$-$             &$-$                &$-$              &$-$           &75.64$^*$         &49.81$^*$      &40.50        &38.20       &59.50      &34.70   \\ \hline
			DCN         &84.00                   &80.00               &67.00             &67.00               &\textbf{74.00} &81.00       &$-$              &$-$           &$-$            &$-$           &44.00$^*$        &\textbf{48.00}$^*$      &56.70       &31.60       \\ \hline
DKM                         &84.00$^*$            &79.60$^*$      &75.70$^*$    &77.60$^*$     &$-$             & $-$             &$-$              &$-$             &$-$              &$-$          &\textbf{51.20}$^*$         &46.70$^*$       &58.30$^*$       &33.10$^*$     \\ \hline
			DEKM                                                             &\textbf{95.75}       & \textbf{91.06} &\textbf{79.75}   &\textbf{82.23}   &69.03   &80.06     &\textbf{38.59}    &\textbf{50.78}     &76.28&59.06&41.08&40.27&\textbf{67.15}&\textbf{46.18}     \\ \hline
			DEKM\_F                                                   &94.23                       &90.54                &78.87          &80.51          &72.62          &81.97          &37.29          &49.32    &\textbf{76.42}&\textbf{59.99}&41.20&43.76&62.12&35.98      \\ \hline
\end{tabular}}
		\label{tab_all}
	\end{center}
\end{table*}

\subsection{Clustering Results}

Due to randomizations of the weights of autoencoder and the centroids of K-means, we run DEKM on each dataset for three times and report the average results. Table~\ref{tab_all} reports the clustering performances of different methods on the benchmark datasets in terms of normalized mutual information (NMI) and unsupervised clustering accuracy (ACC). For the comparison methods, we present the reported results from their original papers if they are available (marked by $^*$ in Table~\ref{tab_all}). For unreported results on specific datasets, we run the released code mentioned in the original papers. When the released code is not available or running it is not practical, we put dash marks ($-$) in Table~\ref{tab_all}. 

We can see from Table~\ref{tab_all} that DEKM outperforms the comparison methods on most of the datasets. DEKM achieves competitive results to DCEC on COIL-20 in terms of NMI. DEKM outperforms all the comparison methods with a large margin on MNIST, 20NEWS and RCV1-10K. DEKM\_F uses the full-batch updating strategy to optimize the representation. Compared with DEKM, DEKM\_F performs slightly worse on four datasets MNIST, USPS, FRGC, and RCV1-10K. As can be seen, all the deep clustering methods perform much better than the traditional shallow clustering methods (i.e., K-means and K-means+PCA). This suggests that the embedding space generated by autoencoder is more advantageous for clustering. The performance gap between DEKM and AE+K-means is large, which means our representation optimization strategy is promising. Both DEKM and DCEC use convolutional autoencoders to find the embedding space for the image datasets. The performance gap between DEKM and DCEC reflects the effect of different representation optimization strategy. The representation optimization strategy of DEKM is superior to that of DCEC. Note that DCEC replaces the MLP autoencoder in DEC with a convolutional autoencoder. For the text datasets, DCEC using the MLP autoencoder is equivalent to DEC. Compared with DCEC on the text datasets, we can see that DEKM also performs better. Thus, the representation optimization strategy of DEKM works in different scenarios, making DEKM a universal clustering framework.

\subsection{Representation Optimization Strategy }

We examine the effects of several representation optimization strategies on the MNIST dataset. Specifically, we compare the following strategies: 1. reducing the entropy of the last dimension of $\mathbf{Y}$, 2. reducing the entropy of a random dimension of $\mathbf{Y}$, and 3. reducing the entropy of all the dimensions of $\mathbf{Y}$. We also compare with another two strategies: 4. reducing the entropy of a random dimension of $\mathbf{H}$, and 5. reducing the entropy of all the dimensions of $\mathbf{H}$. Note that all these strategies use the mini-batch updating strategy. Figure~\ref{fig_ACC} shows the comparison results. We can see that the first strategy of entropy reduction in the last dimension of $\mathbf{Y}$ is the best. It outperforms the other four strategies with a large margin. Strategy 2 performs better than strategy 4. Strategy 3 performs similarly to strategy 5.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.45\textwidth]{img/ACC.pdf}
	\caption{The clustering performances of the different representation optimization strategies on MNIST dataset.}
	\label{fig_ACC}
\end{figure}


\subsection{Embedding Space Comparison}
Figure~\ref{fig_em} illustrates the t-SNE~\cite{van2008visualizing} visualization of the embedding spaces of different algorithms on MNIST dataset. Figure~\ref{fig_em}(a) demonstrates the embedding space of PCA. Figure~\ref{fig_em}(b) illustrates the embedding space of a convolutional autoencoder, which is the initial embedding space for DEKM. Figure~\ref{fig_em}(c) shows the embedding space of DEC. And Figure~\ref{fig_em}(d) shows the embedding space of DEKM. Note that all these embedding spaces are used to get the clustering results in Table~\ref{tab_all}. Compared with the clusters in the initial embedding space (as shown in Figure~\ref{fig_em}(b)) of the convolutional autoencoder, the clusters in the embedding space (as shown in Figure~\ref{fig_em}(d)) of DEKM are more focused and isotropic, which are good for K-means. The two clusters in the embedding space of DEC are mixed, which leads to a lower performance compared with DEKM. The clusters in the embedding space of PCA are not isotropic Gaussian clusters, which is the reason why K-means does not perform well.

\begin{figure}[!htb]
	\hspace*{\fill}
	\centering	
	\subfigure[Raw data + PCA]{\includegraphics[width=0.2\textwidth]{img/PCA.png}}
	\hfill
	\subfigure[AE]{\includegraphics[width=0.2\textwidth]{img/convAE.png}}
	\hspace*{\fill}
	
	\hspace*{\fill}
	\centering	
	\subfigure[DEC]{\includegraphics[width=0.2\textwidth]{img/DEC.png}}
	\hfill
	\subfigure[DEKM]{\includegraphics[width=0.2\textwidth]{img/DK.png}}
	\hspace*{\fill}
	\caption{The t-SNE visualization of the embedding spaces of different algorithms on MNIST dataset.}
	\label{fig_em}
\end{figure}





