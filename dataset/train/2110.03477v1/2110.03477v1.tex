
In InfoSeg, we tackle unsupervised semantic image segmentation.
We take a set  of  unlabeled images and assign a label  to every pixel of each image.
Importantly, for one particular image, we do not specify which nor how many labels should be assigned.
We only provide the total number of labels  in all images.
After training, we follow the standard evaluation protocol and map the learned labels of InfoSeg directly to the semantic classes of an annotated dataset.


InfoSeg is designed to tackle the three limitations of state-of-the-art methods discussed in~\cref{sec:moti}.
\Cref{fig:overview} shows an overview of InfoSeg.
In the following, we first discuss how we leverage recent progress in representation learning for semantic segmentation in~\cref{sec:murep}.
Then we provide further details of our method in~\cref{sec:seg} and~\cref{sec:mumax}.

\subsection{Representation Learning for Segmentation}
\label{sec:murep}

We first review how Local Deep InfoMax \cite{deepinfo} captures high-level information of entire images,
and then how InfoSeg adapts this approach to target image segmentation.

Local Deep InfoMax \cite{deepinfo} learns global high-level features of images by maximizing their average \gls{mi} with local features.
Local features cover image patches, and the global feature covers the entire image.
If the global feature has limited capacity, the network cannot simply copy all local features' content into the global feature to maximize \gls{mi}.
Instead, the network has to encode a compact representation that shares information with as many image patches as possible.
Hjelm \etal \cite{deepinfo} showed that the resulting global features encode high-level image information.
They motivated this by the idea that high-level information is often constant over an entire image, while low-level information such as pixel-level noise varies.
Consequently, the global feature is encouraged to encode the former while disregarding the latter.

To enable pixel-wise segmentation, we compute for each image multiple global features instead of a single one.
Each global feature only encodes image areas that depict a particular class.
This allows us to segment images by assigning pixels to classes based on their attribution to each global feature.
During training, we maximize for each global feature \gls{mi} only with local features covering its respective class.
Therefore, we learn high-level features in a similar way as Local Deep InfoMax \cite{deepinfo}, but target segments instead of entire images.
This requires us to learn high-level features together with segmentations.
To this end, we alternate two steps at each iteration.
In the \textit{Segmentation Step}, we assign local to global features based on their content, \ie, we segment images.
In the \textit{Mutual Information Maximization step}, we maximize the \gls{mi} between all global features and assigned local features, \ie, we learn the features.
We describe both steps in the following.





\begin{figure}[t]
	\begin{center}
	\includegraphics[width=.85\columnwidth]{encoder}
	\caption{Feature computation for  classes. \textbf{(a)} Overview of network architecture.
		\textbf{(b)} Used blocks. \textbf{Legend}: \textit{Conv, }: Convolution with filter size ,  channels and stride .
      Blocks that are used multiple times, each have their own set of parameters.}
	\label{fig:blocks}
	\end{center}
\end{figure}


\subsection{Segmentation Step}
\label{sec:seg}
Given an input image , we compute -dimensional global  and patch-wise local  features.
The -th global feature  encodes a high-level representation for the -th class and covers the entire image.
The local feature  at the spatial position  encodes an image patch.
Furthermore, the spatial resolution of  is downsampled by a rate of  from the input resolution, \ie  and .

\Cref{fig:blocks} shows the architecture of our feature computation network.
First, the input image is processed by Block A, resulting in a grid of patch-wise image features.
To compute the local features , we further process these patch-wise features by Block C.
Adding this additional residual block of pointwise convolutions led to better performance, than using Block A's output directly for the local features.
To compute the global features , we first process the output of Block A to image-level features using Block B.
Then, similarly, as for the local features, we add a residual block of pointwise convolutions using Block C.
Finally, each global feature is computed using a separate linear layer using Block D.


To compute an image segmentation, we use the dot-product of a local and global feature pair  as a class score.
A high score indicates that the -th class is shown at the position .
We elaborate in~\cref{sec:mumax} how \gls{mi} maximization increases the dot-product of a local feature and the global feature of its corresponding class.
After computing the class scores, we apply a pixel-wise scaled softmax to compute a class-probability volume  with elements

where  is a hyper-parameter that controls the smoothness of the resulting distribution.
Using the probability volume, we compute the low-res segmentation  for every pixel  with

by taking the class with the largest probability.
We can then compute a full-res segmentation  by upsampling the low-res segmentation  to the input image resolution. 

\subsection{Mutual Information Maximization Step}
\label{sec:mumax}
We first need to assign each local feature to its corresponding class's global feature.
We could do this using the segmentation .
However, this disregards class probabilities, instead of utilizing their exact values, \eg to account for uncertainty.
Especially at the beginning of training, segmentations are uncertain and depend on random network initialization.
Reinforcing possibly incorrect predictions can lead to degenerate solutions.
To alleviate this problem, we do not make hard class assignments using , but soft assignments using class-probabilities .
Instead of assigning a single global feature, we weight each global feature by its respective class probability.
To this end, we define the function  that computes a soft global feature assignment for the local feature  as follows

where the function  computes the -th global feature  for an image , and  denotes the learnable parameters of our network.

During training, we maximize the \gls{mi} between the output of  and the corresponding local feature  for all spatial positions .
Hence our objective is given as

where  denotes the expectation over all training images  and the function  computes the local features  given an input image .


To evaluate our objective~\cref{eq:trainingobjsoft}, consider that local and global features are high-dimensional continuous random variables.
\Gls{mi} computation of such variables is challenging.
Contrarily to discrete variables as in the objective of \gls{iic}~\cref{eq:ic}, where exact computation is possible.
For continuous variables, Belghazi \etal \cite{mine} proposed \gls{mi} estimation by maximizing lower bounds parametrized by neural networks.
They used a bound based on the \gls{dv} representation of the \gls{kl} divergence.
While several other bounds exist \cite{varbounds}, we use a bound based on the \gls{jsd}.
Mainly because Hjelm \etal \cite{deepinfo} showed favorable properties of the \gls{jsd} bound compared to others in their representation learning setting.
This includes increased training stability and better performance with smaller batch sizes.
Nevertheless, we also perform experiments using the \gls{dv} bound in our ablation studies.
A \gls{jsd} based \gls{mi} estimator  for two random variables  and  can be defined as follows \cite{nowozin2016f}

where  and  is a discriminator mapping sample pairs from  and  to a real valued score.
The first and second expectations are taken over samples from the joint  and marginal  distributions.
Consequently, to tighten the bound, the discriminator  needs to discriminate samples from the joint and marginal distributions by assigning high or low scores, respectively.

To use the \gls{jsd} estimator~\cref{eq:muinf} in our objective~\cref{eq:trainingobjsoft}, we have to define the discriminator  and a sampling strategy.
Following recent work \cite{deepinfo,bachman19}, we create joint and marginal samples by combining feature pairs computed from the same image  and two randomly paired images  and , respectively.
The discriminator  can be implemented using any arbitrary function that maps feature pairs to a discrimination score, \eg, a neural network.
For efficiency, we use the dot-product to compute discrimination scores \ie, .
This requires only a single expensive forward pass through our network to compute the features, while we can then score any arbitrary combination with a cheap dot-product.
Omitting the spatial indices  to avoid notational clutter, this leads to the \gls{mi} estimator

where  is the empirical distribution of our dataset,  is an image sampled from  and  is an image sampled from .
We can now simply insert the estimator of~\cref{eq:estim} into our objective~\cref{eq:trainingobjsoft}.
Maximizing the resulting objective increases the dot-product of local features with the global feature of their assigned class.
Consequently, we use the dot-product as a class score, as described in~\cref{sec:seg}.

