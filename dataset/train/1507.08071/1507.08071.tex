\documentclass[10pt]{extarticle}

\usepackage[english]{babel}
\usepackage[ansinew]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{fancybox}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{array}
\usepackage{lmodern}
\usepackage{mathabx}
\usepackage{fancybox}
\usepackage{caption}
\usepackage{fullpage}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}
\usepackage[margin=1.6in]{geometry}

\newcommand{\changefont}[3]{\fontfamily{#1}\fontseries{#2}\fontshape{#3}
\selectfont}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\E}{\mathbb{E}}

\newenvironment{winz}[1]{\begin{tiny} #1 }{\end{tiny}}

\newtheorem{Thm}{Theorem}
\newtheorem{Lem}[Thm]{Lemma}
\newtheorem{Pro}[Thm]{Proposition}
\newtheorem{Fac}[Thm]{Fact}
\newtheorem{Cor}{Corollary}
\newtheorem{Clm}[Thm]{Claim}

\newtheorem{Def}{Definition}
\newtheorem{Con}{Conjecture}
\newtheorem{Ass}{Assumption}
\newtheorem{Exm}{Example}

\newtheorem{Rem}{Remark}
\newtheorem{Not}{Note}
\newtheorem{Ntn}{Notation}


\title{Private Stream Aggregation Revisited
\thanks{The research was supported by the DFG Research Training Group GRK }
}
\author{
Filipp Valovich, Francesco Ald\`{a}\\ Horst G\"{o}rtz Institute for IT Security\\ Faculty of Mathematics\\ Ruhr-Universit\"{a}t Bochum, Universit\"{a}tsstrasse 150, 44801 Bochum, Germany\\ Email: \{filipp.valovich, francesco.alda\}@rub.de
}
\date{}

\bibliographystyle{plain}


\begin{document}

\maketitle

\pagenumbering{gobble}
\pagestyle{empty}



\begin{abstract} \noindent In this work, we investigate the problem of private statistical analysis in the distributed and semi-honest setting. In particular, we study properties of Private Stream Aggregation schemes, first introduced by Shi et al. \cite{2}. These are computationally secure protocols for the aggregation of data in a network and have a very small communication cost. We show that such schemes can be built upon any key-homomorphic \textit{weak} pseudo-random function. Thus, in contrast to the aforementioned work, our security definition can be achieved in the \textit{standard model}. In addition, we give a computationally efficient instantiation of this protocol based on the Decisional Diffie-Hellman problem. 
Moreover, we show that every mechanism which preserves -differential privacy provides \textit{computational} -differential privacy when it is executed through a Private Stream Aggregation scheme. Finally, we introduce a novel perturbation mechanism based on the \textit{Skellam distribution} that is suited for the distributed setting, and compare its performances with those of previous solutions.
\end{abstract}

\section{Introduction}

The framework of statistical disclosure control aims at providing strong privacy guarantees for the records stored in a database while enabling accurate statistical analyses to be performed. In recent years, \textit{differential privacy} has become one of the most important paradigms for privacy-preserving statistical analyses. Generally, the notion of differential privacy is considered in the centralised setting where we assume the existence of a \textit{trusted curator} \cite{16, 24, 8, 9} who collects data in the clear, perturbs it properly (e.g. by adding Laplace noise) and publishes it. In this way, the output statistics are not significantly influenced by the presence (resp. absence) of a particular record in the database.\\
In this work, we study how to preserve differential privacy when we cannot rely on a trusted curator. In this so-called \textit{distributed setting}, the users have to send their data to an untrusted aggregator. Preserving differential privacy and achieving high accuracy in the distributed setting is of course harder than in the centralised setting, since the users have to execute a perturbation mechanism on their own. In order to achieve the same accuracy as provided by well-known techniques in the centralised setting, the work by Shi et al. \cite{2} introduces the \textit{Private Stream Aggregation} (PSA) scheme, a cryptographic protocol which enables each user to securely send encrypted time-series data to an aggregator. The aggregator is then able to decrypt the aggregate of all data in each time step, but cannot retrieve any further information about the individual data. Using such a protocol, the task of perturbation can be split among the users, such that differential privacy is preserved \textit{and} high accuracy is guaranteed. For a survey of applications of this protocol, we refer to \cite{2}.\\
In \cite{2}, a PSA scheme for sum queries is provided and some strong security guarantees under the Decisional Diffie-Hell\-man assumption are shown. However, this instantiation has some limitations. First, the security only holds in the random oracle model; second, its decryption algorithm requires the solution of the discrete logarithm in a given range, which can be very time-consuming if the number of users and the plaintext space are large. Moreover, since a PSA scheme provides \textit{computational} security, the perturbation mechanism in use can only provide a computational version of differential privacy, a notion first introduced by Mironov et al. \cite{15}. In \cite{2}, however, a connection between the security of a PSA scheme and differential privacy is not explicitly shown. In a subsequent work by Chan et al. \cite{3}, this connection is still not completely established, since the polynomial-time reduction between an attacker against a PSA scheme and a database distinguisher is missing.\\

\noindent\textbf{Objectives.} In order to overcome the limitations of the construction in \cite{2}, in this work we address the following problems.
\begin{itemize}
 \item We want to give \textit{sufficient conditions} for a PSA scheme which has a security guarantee in the \textit{standard model}.
 \item According to these conditions, we want to construct a concrete instantiation consisting of efficient algorithms, even when the number of users and the plaintext space become large.
 \item We aim at showing that an information-theoretical differentially private mechanism preserves \textit{computational differential privacy} when it is executed through a (computationally) secure PSA scheme. \item We want to investigate differentially private mechanisms suitable for an execution through a PSA scheme.
\end{itemize}


\noindent\textbf{Contributions.} We achieve the aforementioned goals in the following manner. In order to derive sufficient conditions for PSA schemes with a certain security guarantee, we lower the requirements of Aggregator Obliviousness from \cite{2} by abrogating the attacker's possibility to \textit{adaptively compromise} users during the execution of a PSA scheme with time-series data. We show that a PSA scheme for achieving this lower security level can be built upon any \textit{key-homomorphic weak pseudo-random function}. Since weak pseudo-randomness can be achieved in the standard model, this condition also enables secure schemes in the standard model. In particular, we can build a key-homomorphic weak pseudo-random function based on the Decisional Diffie-Hell\-man assumption in the group of quadratic residues modulo a \textit{squared safe prime}. This function is used for the construction of a PSA scheme for sum queries. By comparing the running times and practical performances of our PSA scheme and the one given by Shi et al. \cite{2} at the same security level, we find that our solution provides a significant speed-up for decryption when the plaintext space is large while decelerating the encryption only by a constant factor. 

Reduction-based security proofs for cryptographic schemes usually require an attacker in the corresponding security game to send two different plaintexts (or plaintext collections) to a challenger. The adversary receives then back a ciphertext which is the encryption of one of these collections and has to guess which one it is. In any security definition for a PSA scheme, these collections must satisfy a particular requirement (i.e. they must lead to the same aggregate), since the attacker has the capability to decrypt them (different aggregates would make the adversary's task trivial). In general, however, this requirement cannot be satisfied in the context of differential privacy. Introducing a novel kind of security reduction which deploys a \textit{biased coin} flip, we can show that, whenever a randomised perturbation procedure is involved in a PSA scheme, the requirement of having collections with equal aggregate can be abolished. This result can be generalised to any cryptographic scheme with such a requirement. Using this property, we are able to show that if a mechanism preserves differential privacy, then it preserves computational differential privacy when it is used as a randomised perturbation procedure in a PSA scheme.



Finally, we compare three mechanisms: the Geometric mechanism from \cite{2}, the Binomial mechanism from \cite{14} and the \textit{Skellam mechanism} introduced in this work. All three mechanisms preserve differential privacy and make use of discrete probability distributions. Therefore, they are well-suited for an execution through a PSA scheme. For generating the right amount of noise among all users, these mechanisms apply two different approaches. While in the Geometric mechanism, with high probability, only one user generates the noise necessary for differential privacy, the Binomial and Skellam mechanisms allow all users to generate noise of small variance, that sums up to the required value for privacy. We show that for high privacy levels, the theoretical error bound of the Skellam mechanism is slightly better than that of the other two. At the same time, we provide experimental results showing that the Geometric and Skellam mechanisms have a comparable accuracy in practice, while beating the one of the Binomial mechanism.\\

\noindent\textbf{Related Work.} As pointed out above, our contributions are mostly related to the work of Shi et al. \cite{2} and Chan et al. \cite{3}.
Privacy-preserving aggregation of time-series data in the presence of an untrusted aggregator has also been studied in various other works, e.g. \cite{4, 37, 33, 34, 38}. Beimel et al. \cite{4} and Eigner et al. \cite{37} show that secure multi-party computation techniques can be used for data aggregation under differential privacy. These techniques usually have a high communication cost, whereas PSA requires each user to send exactly one message per time-step. 
The protocol given by Rastogi et al. \cite{19} is based on the threshold Paillier cryptosystem. It requires an extra round of interaction between the users and the aggregator in every time-step in order to decrypt the sum queries. In contrast, PSA requires the users to interact with the aggregator only for sending the ciphertexts. \'{A}cs et al. \cite{36} use an additive homomorphic encryption scheme for sending time-series data, but it requires the generation of a pair of encryption/decryption keys for each pair of users. Moreover, reuse of key pairs for different time-steps potentially leads to security breaches. In a PSA scheme each user gets only one encryption key, which can be securely used for \textit{every} time-step. Using additive homomorphic encryption, Rieffel et al. \cite{35} construct a scheme which does not require extra rounds of interaction, but is not fully resistant against collusions and the cost of computation and storage is roughly equal to the number of compromised users that is tolerable by the system. 
Li et al. \cite{33,34} use the homomorphic encryption scheme given by Castelluccia et al. \cite{31,32} in order to construct an efficient protocol for sending data in mobile sensing applications. 
This scheme is resistant against collusions, but each user has to store multiple keys, depending on the number of compromised users in the network. Moreover, for encryption and decryption the scheme requires the computation of as many pseudo-random values as the number of keys in the network, making the computational effort for the analyst rather high. Thus, the costs of this scheme depend on the number of compromised users. 
A PSA scheme is fully resistant against \textit{any} number of collusions and furthermore, we provide a solution, where the computation and storage costs are independent of the number of users. Joye et al. \cite{38} provide a protocol with the same security guarantees as in \cite{2} in the random oracle model. The security of their scheme relies on the DCR assumption (rather than DDH as in \cite{2}) and as a result, in the security reduction they can remove a factor which is cubic in the number of users. However, their scheme involves a trusted party for setting some public parameters. In this work we provide an instantiation of our \textit{generic} PSA construction, which is similar to the one in \cite{38} but relies on the DDH assumption. While in our generic security reduction we cannot avoid the cubic factor in the number of users, our construction \textit{does not} involve any trusted party and has security guarantees in the standard model.

Another series of works deals with a distributed generation of noise for preserving differential privacy. Dwork et al. \cite{14} consider the Gaussian distribution for splitting the task of noise generation among all users. Their proposed scheme requires more interactions between the users than our solution. In \cite{36}, privacy-preserving data aggregation is applied to smart metering and the generation of Laplace noise is performed in a distributed manner, since each meter simply generates the difference of two Gamma distributed random variables as a share of a Laplace distributed random variable. In \cite{19} each user generates a share of Laplace noise by generating a vector of four Gaussian random variables. For a survey of the mechanisms given in \cite{19} and \cite{36}, we refer to \cite{21}. However, the aforementioned mechanisms generate noise drawn according to continuous distributions, but for the use in a PSA scheme, discrete noise is required. Therefore, we consider proper discrete distributions and compare their performances for private statistical analyses.



\section{Preliminaries}

\subsection{Problem statement}

In this work, we consider a distributed and semi-honest setting where  users are asked to participate in some statistical analyses but do not trust the data analyst (or aggregator), who is assumed to be honest but curious. Therefore, the users cannot provide their own data in the clear. Moreover, they communicate solely and independently with the untrusted aggregator, who wants to analyse the users data by means of queries in time-series and aims at obtaining answers as accurate as possible. More specifically, assume that the data items belong to a data universe . For a sequence of time-steps , where  is a discrete time period, the analyst sends queries which are answered by the users in a distributed manner. Each query is modeled as a function  for a finite or countably infinite set of possible outputs (i.e. answers to the query) .\\ 
We also assume that some users may act in order to compromise the privacy of the other participants. More precisely, we assume the existence of a publicly known constant  which is the a priori estimate of the lower bound on the fraction 
of non-compromised users who honestly follow the protocol and want to release useful information about their data (with respect to a particular query ), while preserving -differential privacy. 
The remaining -fraction of users is assumed to be compromised. Compromised users honestly follow the protocol but are aimed at violating the privacy of non-compromised users.
For that purpose, these users form a coalition with the analyst and send her auxiliary information, e.g. their own data in the clear.\\ For computing the answers to the aggregator's queries, a special cryptographic protocol, called Private Stream Aggregation (first introduced in \cite{2}), is used by \textit{all} users. In connection with a perturbation mechanism, this scheme assures that the analyst is only able to learn a noisy aggregate of the users' data (as close as possible to the real answer ) and nothing else. In contrast to common secure multi-party techniques \cite{25, 6, 20}, this protocol requires each user to send to the analyst only one message per query. 

\subsection{Definitions}

We consider a database as an element , where  is the data universe and  is the number of users. Since  may contain sensitive information, the users want to protect their privacy. Therefore, a privacy-preserving mechanism must be applied. Unless stated differently, we always assume that a mechanism is applied in the distributed setting. Differential privacy \cite{8} is a well-established notion for privacy-preserving statistical analyses. We recall that a randomised mechanism preserves differential privacy if its application on two adjacent databases, i.e. databases which differ in one entry only, leads to close distributions of the output.

\begin{Def}[Differential Privacy~\cite{8}]
Let  be a (possibly infinite) set and let . A randomised mechanism  preserves -differential privacy (short: \mbox{\upshape\sffamily DP}), if for all adjacent databases  and all :

The probability space is defined over the randomness of .
\end{Def}

The additional parameter  is necessary for mechanisms which cannot preserve -\mbox{\upshape\sffamily DP} (i.e. -\mbox{\upshape\sffamily DP}) for certain cases. However, if the probability that these cases occur is bounded by , then the mechanism preserves -\mbox{\upshape\sffamily DP}.

In the literature, there are well-established mechanisms for preserving differential privacy, e.g. the \textit{Laplace mechanism} \cite{8} and the \textit{Exponential mechanism} \cite{9}. In order to privately evaluate a query, these mechanisms draw noisy values according to some distribution depending on the query's global sensitivity.


\begin{Def}[Global Sensitivity~\cite{8}]
The global sensitivity  of a query\linebreak  is defined as

\end{Def}

In particular, we will consider sum queries  defined as ,\linebreak for  and .\\
For measuring how well the output of a mechanism estimates the real data with respect to a particular query, we use the notion of -accuracy.

\begin{Def}[Accuracy~\cite{17}]
The output of a mechanism  achieves -accuracy for a query  if for all :

The probability space is defined over the randomness of .
\end{Def}

The use of a cryptographic protocol for transferring data provides a computational security level. If such a protocol is applied for preserving differential privacy, this implies that only a computational level of differential privacy can be provided. Our definition of computational differential privacy follows the notion of Chan et al. \cite{3}.

\begin{Def}[Computational Differential Privacy~\cite{3}] 
\mbox{\,\,} Let  be a security parameter and  with . A randomised mechanism  preserves computational -differential privacy (short: \mbox{\upshape\sffamily CDP}), if for all adjacent databases  and all probabilistic polynomial-time distinguishers :

where  is a negligible function in . The probability space is defined over the randomness of  and .
\end{Def}

The notion of computational differential privacy is a natural computational-\linebreak indistinguishability-extension of the infor\-mation-theoretical definition. The advantage is that preserving differential privacy only against bounded attackers helps to substantially reduce the error of the answer provided by the mechanism. In Section \ref{psa}, we investigate how to obtain a computationally secure protocol which allows the analyst to compute only the aggregate of all users' data and no further information. The scheme for sum queries we are going to construct uses a special mapping into a group, which we define formally.

\begin{Def}[-isomorphic embedding] An \,\,\,injec\-tive mapping , where  is a group, is a -isomorphic embedding if for all  and all finite sequences  of values in  with :

\end{Def}

From this definition it is clear that a -isomorphic embedding is also -isomorphic for every integer . In the analysis of the secure protocol, we furthermore make use of the following definition.

\begin{Def}[Weak PRF~\cite{26}] Let  be a security parameter. Let  be sets. A family of functions 
 
is called a weak pseudo-random function (PRF) family if for all probabilistic polynomial-time algorithms  with oracle access to  (where ) on any polynomial number of uniformly chosen inputs, we have:

where  and  is a random mapping from  to .
\end{Def}



















\subsection{Mechanism overview}\label{mechov}

In this work we prove the following result by showing the connection between a key-homomorphic weak pseudo-random function and a differentially private mechanism for sum queries.




\begin{Thm}\label{mainthm} Let , ,  with . Let\linebreak  and  be a sum query. If there exist groups , a key-homomorphic weak pseudo-random function family mapping into  and an efficiently computable and efficiently invertible - isomorphic embedding  then there exists an efficient mechanism for  that preserves -\mbox{\upshape\sffamily CDP} for any constant  with an error bound of  and requires each user to send exactly one message.
\end{Thm}

As already described, we want the untrusted analyst to be able to learn some aggregated statistics  but no additional information about each user's data. Assume that we can design a cryptographic protocol that achieves the aforementioned goal. If we furthermore aim at preserving -\mbox{\upshape\sffamily DP}, it would be sufficient to add a single copy of (properly distributed) noise  to the value . Since we cannot add such noise once the aggregate has been computed, the users have to generate and add noise to their original data in such a way that the sum of the perturbations has the same distribution as . For this purpose, we see two different approaches. In the first approach, there is a small probability (depending on the fixed parameter ) for each user to add noise sufficient to preserve the privacy of the entire statistics. This probability is calibrated in such a way only one of the  users is expected to add noise at all. Shi et al. \cite{2} investigate this method using the Geometric mechanism. In the second approach, each user generates noise of small variance (again depending on ), such that the sum of all noisy terms has enough variance for preserving differential privacy. For this aim, we need discrete probability distributions which are closed under convolution and are known to provide differential privacy. The Binomial mechanism \cite{14} and the Skellam mechanism introduced in this work serve these purposes. In both approaches, the error which is introduced is reasonably small and similar theoretical bounds can be provided. For details, see Section \ref{dpmech}.



For a particular time-step, let the users' values be of the form , , where  is the original data of the user  and  is her noisy value. In the privacy analysis, it is reasonable to assume that  for the  compromised users, since this can only increase their chances to infer some information about the non-compromised users. In order to send the values to the data analyst, the users perform a PSA scheme. First, each user encrypts her own time-series data and sends the ciphertexts to the data analyst. After a distributed key exchange, the evaluation of a single query (i.e. a query analysed in one time-step) requires each user to send exactly one message. The data analyst appropriately aggregates the ciphertexts of all users for a particular time-step and then decrypts the sum of the users' values . From the ciphertexts, the data analyst is not able to leach any additional information about the values of the users, except for the auxiliary information obtained from the compromised users. In this way, there is no privacy-breach if only one user adds the entirely needed noise (first approach) or if the non-compromised users generate noise of low variance (second approach), since the single values are encrypted and the analyst cannot learn anything about them, except for their aggregate. Due to the use of a cryptographic protocol, the plaintexts have to be discrete. This is the reason why we use discrete distributions for generating the noisy values .\\
The perturbation of data potentially yields larger values  due to the (possibly) infinite domain of the underlying probability distribution. Depending on the variance, we therefore need to choose a sufficiently large interval  as plaintext space, where  such that  for all  with high probability. In the following, we always assume that  is a subinterval of .\\
Since the protocol used for the data transmission is computationally secure, the entire mechanism preserves -\mbox{\upshape\sffamily CDP}.




\section{Private Stream Aggregation}\label{psa}

In this section, we define the Private Stream Aggregation scheme and give a security definition for it. Thereby, we mostly follow the concepts introduced by Shi et al. \cite{2}, though we deviate in a few points. Afterwards, we give a condition for the existence of secure PSA schemes. Moreover, we give a concrete and efficient instantiation of a secure PSA scheme in the standard model.

\subsection{The definition of Private Stream Aggregation and its security}

\noindent\textbf{Private Stream Aggregation.} A PSA scheme is a protocol for safe distributed time-series data transfer which enables the receiver to learn only the aggregate  of a query  over some distributed (and possibly perturbed) database . Such a scheme needs a key exchange protocol for all  users together with the analyst as a precomputation, and requires each user to send exactly one message per query. For the definition of PSA, we follow \cite{2}.

\begin{Def}[Private Stream Aggregation]
Let  be a security parameter and  with . A Private Stream Aggregation scheme  is defined by three probabilistic polynomial-time Algorithms:
\begin{description}
\item \textbf{\mbox{\upshape \sffamily Setup}}: , where  are public parameters of the system,  is a set of time-steps and  are private keys.
\item \textbf{\mbox{\upshape \sffamily PSAEnc}}: For time-step  and all : 

\item \textbf{\mbox{\upshape \sffamily PSADec}}: For time-step , ciphertexts , where  is the range of , and a query  compute

For all  and :

\end{description}
\end{Def}

The Setup-phase has to be carried out just once and for all, and can be performed with a secure multi-party protocol among all users and the analyst. In all other phases, no communication between the users is needed.\\
The system parameters  are public and constant for all time-steps with the implicit understanding that they are used in . Every user encrypts her data  with her own secret key  and sends the ciphertext to the analyst. If the analyst receives the ciphertexts of \textit{all} users for a time-step , it can compute the aggregate, i.e. the evaluation of the query , of the users' data with the decryption key~.\\

\noindent\textbf{Security.} Since our model allows the analyst to compromise users, the aggregator can obtain auxiliary information about the data of the compromised users or their secret keys. Even then a secure PSA scheme should release no more information than the aggregate of the non-compromised users' data in a single time-step.\\
Informally, a PSA scheme  is secure if every probabilistic polynomial-time algorithm, with knowledge of the analyst's and compromised users' keys and with adaptive encryption queries, has only negligible advantage in distinguishing between the encryptions of two databases  of its choice, where . We can assume that an adversary knows the secret keys of the entire compromised coalition. If the protocol is secure against such an attacker, then it is also secure against an attacker without the knowledge of every key from the coalition. Thus, in our security definition we consider the most powerful adversary. In what follows, let  denote the evaluation of a query  with input  over a subset  of users. The security definition is similar to the one in \cite{2}.

\begin{Def}[Security of PSA]\label{securitygame}
Let  be a probabilistic polynomial-time adversary for a PSA scheme  and let  be a statistical query over the set . Let  be the set of time-steps for possible data analyses. We define the following security game between a challenger and the adversary .
\begin{description}
 \item\textbf{Setup.} The challenger runs the \text{\upshape\sffamily Setup} algorithm on input security parameter  and returns public parameters , time-steps  with  and secret keys . It sends  to .
\item\textbf{Queries.} The challenger flips a random bit .  chooses  and sends it to the challenger which returns .
  is allowed to query  with  and the challenger returns 
\item\textbf{Challenge.}  chooses  such that no encryption query at  was made. (If there is no such  then the challenger simply aborts.)  queries two different tuples  with
 
For all  the challenger returns 

\item\textbf{Queries.}  is allowed to make the same type of queries as before with the restriction that no encryption query at  can be made.
\item\textbf{Guess.}  outputs a guess about .
\end{description}
The adversary wins the game if it correctly guesses . A PSA scheme is secure if no probabilistic polynomial-time adversary  has more than negligible advantage (with respect to the parameter ) in winning the above game.
\end{Def}

Encryption queries are made only for , since knowing the secret key for all  the adversary can encrypt a value autonomously. If encryption queries in time-step  were allowed, then no deterministic scheme would be secure. The adversary  can determine the original data of all  for every time-step, since it knows . Then  can compute the aggregate of the non-compromised users' data. For example, when  is a sum query we have 
,
where  and  is the encryption of  for all . On the other hand, if there is a user's ciphertext which  does not receive, then it cannot obtain the aggregate for the correspondent time-step.\\ 
The security definition indicates that  cannot distinguish between the encryptions of two different data collections  with the same aggregate at time-step . For proving that a secure PSA scheme can be used for computing differentially private statistics with small error, we have to slightly modify the security game such that an adversary may choose adjacent (and non-perturbed) databases, as it is required in the definition of differential privacy. For details, see Section \ref{cdp}.\\
Definition \ref{securitygame} differs from the definition of Aggregator Obliviousness \cite{2} since we require the adversary to specify the set  of non-compromised users \textit{before} making any query, i.e. we do not allow the adversary to determine  adaptively.


\subsection{Feasibility of efficient and secure Private Stream Aggregation}

Using a secure PSA scheme, we ensure that the transmitted data of non-compromised users do not disclose sensitive information other than their aggregate. 
We now state the condition for the existence of secure (as in Definition \ref{securitygame}) PSA schemes for sum queries.

\begin{Thm}\label{PSATHEOREM}
Let  be a security parameter, and  with . Let  be finite groups and . For some finite set , let  be a family of functions which are homomorphic over  and  an -isomorphic embedding. If  is a weak PRF family, then the following PSA scheme  is secure:
\begin{description}
\item \textbf{\mbox{\upshape \sffamily Setup}}: , where  are parameters of . The keys are  for all  with  and  such that all  are chosen uniformly at random from .
\item \textbf{\mbox{\upshape \sffamily PSAEnc}}: Compute  in , where .
\item \textbf{\mbox{\upshape \sffamily PSADec}}: Compute  and invert.
\end{description}
\end{Thm}

Thus, we need a key-homomorphic weak PRF and a mapping which homomorphically aggregates all users' data. Since every data value is at most , the scheme correctly retrieves the aggregate, which is at most , by the -isomorphic property of . Importantly, the product of all pseudo-random values  is the neutral element in the group  for all . Since the values in  are uniformly distributed in , it is enough to require that  is a \textit{weak} PRF family. Thus, the statement of this theorem does not require a random oracle.\\
The proof of Theorem \ref{PSATHEOREM} works with a sequence of games and builds on the ideas of \cite{2}. The details of the proof are given in Appendix \ref{ptproof}. 
Here we just give the main ideas. In the first step (Lemma \ref{gameonetwo}) 
we construct a real-or-random version of the PSA security game, where encryption is performed equally likely with a weak PRF or a random function. We show that winning the PSA security game is at least as hard as winning its real-or-random version. The second step (Lemma \ref{gametwothree}) 
shows that the plaintext dependence of the ciphertexts generated in the game can be abolished. Since we are dealing with a non-adaptive security definition there is no need of simulating the random choice of time-steps by programming a random oracle as it is required in the proof of Aggregator Obliviousness by Shi et al. \cite{2}. Therefore, in contrast to \cite{2}, our result does not rely on a random oracle and the full proof works in the standard model. In the last step, (Lemma \ref{gamethreeprf})
using the hybrid argument, we show that winning the game is at least as hard as distinguishing the weak PRF from a random function. Here the adversary's specification of  before making the first query allows the PRF distinguisher to be consistent with real random values or pseudo-random values in its replies to the queries. All in all, we get that winning the first game with our construction is at least as hard as distinguishing the weak PRF from a random function, completing the proof of Theorem \ref{PSATHEOREM}.

\subsection{An efficient Private Stream Aggregation scheme}

We give an instantiation of a secure PSA scheme consisting of efficient algorithms. Its security is based on the Decisional Diffie-Hellman (DDH) problem.

\begin{Exm}\label{DDHEXM} Let  and  be large primes. Let furthermore  and  with . Then  generates the group  of quadratic residues modulo . In this group DDH is assumed to be hard. Then we define 
\begin{itemize}
\item . Choose keys  and  mod . Let , i.e.  is a power of  for every .
\item . This is a weak PRF under the DDH assumption, as can be proven using arguments similar to the ones in . \item  mod , where . (It is easy to see that  is an -isomorphic embedding.)
\end{itemize}

\noindent For aggregation, we compute  with

and decrypt  over the integers.
\end{Exm}

The difference to the scheme introduced in \cite{2} lies on the map . Whereas the PRF in \cite{2} works similarly (the underlying group  is  rather than ), the aggregational function is defined by

which requires to solve the discrete logarithm modulo  for decrypting. In contrast, our efficient construction only requires a subtraction and a division over the integers.

\begin{Rem} In the random oracle model, the construction shown in Example \ref{DDHEXM} achieves the stronger notion of Aggregator Obliviousness, which is the adaptive version of our security definition (for details, see the proof in Appendix A of ). The same proof can be applied to our instantiation by simply replacing the map  involved and using a strong version of the PRF .\footnote{For showing Aggregator Obliviousness we would have to substitute the choice of random values  by a hash function  modeled as a random oracle for some domain . Therefore the PRF would be  which is the strong version of the weak PRF in Example \ref{DDHEXM}. In this case, all  may be chosen in a deterministic way.}
\end{Rem}


\begin{table}[t]
\centering
\begin{tabular}{l||c|c|c} Length of  & -bit & -bit & -bit\\ \hline \cite{2} &  ms &  ms &  ms\\ This work &  ms &  ms &  ms
\end{tabular}
\captionof{table}{Time for encryption}\label{enctab}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{l||c|c|c|c|c} \,\,\,\,\,\,\,\,\, &  &  &  &  & \\ \hline \cite{2} b.-f. &  s &  s &  s &  s &  s\\ This work &  s &  s &  s &  s &  s
\end{tabular}
\captionof{table}{Time for decryption ( bit, )}\label{dectab}
\end{table}

Note that for a given , the running time of the decryption in our scheme does not depend on , so it provides a small running time even if  is exponentially large. At the same time, the decryption of the scheme in \cite{2} can also be performed efficiently, even if  is superpolynomial in the security parameter: discretise the plaintext space into  equidistant values and let each user choose the value nearest to her original one as input for encryption. The aggregated value has the correct expectation, but the decryption algorithm has to search only in a range of  values. However, this method causes a superlinear time-dependence on  for the decryption and induces an additional aggregation error due to discretisation.\\
We compare the practical running times for encryption and decryption of the scheme in \cite{2} with the algorithms of our scheme in Table \ref{enctab} and Table \ref{dectab}, respectively. Here, let  denote the size of the plaintext space. Encryption is compared at different security levels with . For comparing the decryption time, we fix the security level and the number of users and let  be variable.\\
All algorithms were executed on an Intel Core i, -bit CPU at  GHz. We compared the schemes at the same security level, assuming that the DDH problem modulo  is as hard as modulo , i.e. we used the same value for  in both schemes. For different bit-lengths of , we observe that the encryption of our scheme is roughly  times slower than the encryption in \cite{2}. The running time of our decryption algorithm is widely dominated by the aggregation phase. Therefore it is clear, that it linearly depends on . Using a -bit prime and fixing , the running time of the decryption in our scheme is less than  second for varying values of . In contrast, the time for the brute-force decryption in \cite{2} grows roughly linearly in .\\
As observed in \cite{2}, using Pollard's lambda method would reduce the running time for decryption of \cite{2} to about . Nevertheless, our scheme provides a speed-up of  whenever  is larger than , while the encryption is decelerated only by a constant factor.


\section{Achieving Computational Differential Privacy}\label{cdp}

\begin{Ntn} Let  be a security parameter. If an expression  is non-negligible in  (i.e. if ), then we write .
\end{Ntn}

In this section, we describe how to preserve computational differential privacy using a PSA scheme. As described above, in the work by Chan et al. \cite{3} the polynomial-time reduction between an attacker against the security of a PSA scheme and an attacker against differential privacy is missing. In this section we provide an appropriate reduction. The content of this section is independent of Theorem \ref{PSATHEOREM}. Specifically, let  be a mechanism which, given some event , evaluates a statistical query  over a database  preserving -\mbox{\upshape\sffamily DP}. Furthermore, let  be a secure PSA scheme for . We show that  executed through  preserves -\mbox{\upshape\sffamily CDP} given . Let  and assume . In Section \ref{dpmech}, we will give instantiations of such a mechanism  and show that they preserve -\mbox{\upshape\sffamily CDP} \textit{unconditionally} if executed through . For simplicity, in this section we focus on sum queries, but our analysis can be easily extended to more general statistical queries. Our technique involves a reduction-based proof using a biased coin toss and is of independent interest.

\subsection{Redefining the security of Private Stream Aggregation}
Let us first modify the security game in Definition \ref{securitygame} in the following way. Let \textbf{game}  be the original game from Definition \ref{securitygame}. Let  and . The -\textbf{game}  for a probabilistic polynomial-time adversary  is defined as \textbf{game}  with the following changes:
\begin{itemize}
 \item Before the challenge phase,  sends  to the challenger.
 \item In the challenge phase, the challenger chooses  with probability  and  with probability .
\end{itemize}
We call a PSA scheme -secure if the probability of every probabilistic polynomial-time adversary  in winning the above game is . Note that \textbf{game}  is a special case of -\textbf{game} , where . We refer to this case as the unbiased version (rather the biased version if ) of -\textbf{game} . In the unbiased case, we just drop the dependence on  and the adversary is not required to send  to its challenger.


\subsection{Constructing a PSA adversary using a CDP adversary}

\subsubsection{Security game for adjacent databases}
For showing that a -secure PSA scheme is suitable for preserving {\upshape\sffamily CDP}, we have to construct a successful adversary in -\textbf{game}  (with a proper choice of ) using a successful distinguisher for adjacent databases. We define the following \textbf{game}  for a probabilistic polynomial-time adversary  which is identical to \textbf{game}  with a changed challenge-phase:
\begin{description}
\item\textbf{Challenge.}  chooses  such that no encryption query at  was made.  queries two \textit{adjacent} tuples . For all  the challenger returns 

where  is a noisy version of  for all  obtained by some randomised perturbation process.
\end{description}
Now consider the following experiment which we call . Let  be a sum query and  a probability distribution function on . For simplicity we consider only the case where . Let . Then  is performed as follows:
\begin{itemize} 
\item Let  be a Bernoulli variable with .
\item Let  be a random vector with probability distribution function , where  is a realisation of .
\item Let .
\end{itemize}
We now define an experiment  and afterwards show that it is statistically equivalent to .
\begin{itemize} 
\item Let  be a random variable as in .
\item Let . Let  be a Bernoulli variable with .
\item Let  be a random vector with conditional probability function

Here  is a realisation of  and  denotes the characteristic function of , which is  if  and  otherwise. Note that the values  for the computation of the conditional probability of  are known.
\end{itemize}

For showing that  and  are statistically equivalent it suffices to show that the joint distributions  and  are equal.

\begin{Lem}\label{EXPEQUIV} .
\end{Lem}
\begin{proof}
We observe that in , . Therefore we have

which exactly corresponds to the conditional probability of  in . Thus, we have

\end{proof}

\noindent Note that Lemma \ref{EXPEQUIV} also applies to the marginals of the triples  and .

\subsubsection{The Reduction}

With Lemma \ref{EXPEQUIV} in mind, we can show that a successful adversary in \textbf{game}  yields a successful adversary in -\textbf{game}  for a particular . Afterwards we show that a successful adversary in -\textbf{game}  for \textit{any}  yields a successful adversary in \textbf{game} .

\begin{Lem}\label{gamezeroPone} Let  be a security parameter. Let  be an adversary in \textbf{\upshape game}  with advantage . Let  denote the random variable describing the challenge bit  in \textbf{\upshape game}  and let  denote the random variable describing the aggregate of . Let  be the probability of  given the choice of  and let . Then there exists an adversary  in -\textbf{\upshape game}  with advantage .
\end{Lem}
\begin{proof} We construct a successful adversary  in -\textbf{game}  using  as follows:
\noindent\begin{description}
 \item\textbf{Setup.} Receive  from the -\textbf{game} -challenger and send it to .
\item\textbf{Queries.} Receive  from  and send it to the challenger. Forward the obtained response  to . Forward 's queries  with  to the challenger and forward the obtained response  to .
\item\textbf{Challenge.}  chooses  such that no encryption query at  was made and queries two adjacent tuples . Choose a realisation  of  according to . Set  and choose  with probability  for  according to . Send  to the challenger. Obtain the response  and forward it to . 
\item\textbf{Queries.}  can make the same type of queries as before with the restriction that no encryption query at  can be made.
\item\textbf{Guess.}  gives a guess about which database was encrypted. Output the same guess.
\end{description}
The rules of -\textbf{game}  are preserved since  sends two tuples of the same aggregate  to its challenger. On the other hand, since the ciphertexts generated by the challenger are determined by the challenge bit and the collection , the rules of \textbf{game}  are preserved by Lemma~\ref{EXPEQUIV} (the triple  is chosen according to ). Therefore  perfectly simulates \textbf{game}  and has the same advantage as .
\end{proof}

We now show that a secure PSA scheme is also -secure for every , where .

\begin{Lem}\label{gamePoneone} Let  be a security parameter. For any  let  be an adversary in -\textbf{\upshape game } with advantage . Then there exists an adversary  in \textbf{\upshape game } with advantage .
\end{Lem}
\begin{proof} Given a successful adversary  in -\textbf{game} , we construct a successful adversary  in \textbf{game}  as follows:
\noindent\begin{description}
 \item\textbf{Setup.} Receive  from the \textbf{game} -challenger and send it to .
\item\textbf{Queries.} Receive  from  and send it to the challenger. Forward the obtained response  to . Forward 's queries  with  to the challenger and forward the obtained response  to .
\item\textbf{Challenge.}  chooses  such that no encryption query at  was made, sends  and queries two different tuples  with . Choose a bit  with  and query  to the challenger, where the  are chosen uniformly at random from  such that . Obtain the response  and forward it to . 
\item\textbf{Queries.}  can make the same type of queries as before with the restriction that no encryption query at  can be made.
\item\textbf{Guess.}  gives a guess about . If the guess is correct, then output ; if not, output .
\end{description}
If  has output the correct guess about  then  can say with high confidence that the challenge ciphertexts are the encryptions of  and therefore outputs . On the other hand, if 's guess was not correct, then  can say with high confidence that the challenge ciphertexts are the encryptions of the random collection  and it outputs . Formally:\par\medskip

\noindent\textbf{Case .} Let . Then  perfectly simulates -\textbf{game}  for  and the distribution of ciphertexts is the same as in -\textbf{game} :
\par\medskip

\noindent\textbf{Case .} Let . Then the ciphertexts are random with the constraint 
that their product is the same as in the first case. The probability that  wins \textbf{game}  is at most  and

Finally we obtain that the advantage of  in winning \textbf{game}  is

\end{proof}


\subsection{Proof of Computational Differential Privacy}

We have shown that no probabilistic polynomial-time adversary can win \textbf{game}  if the underlying PSA scheme is secure. If the perturbation process in \textbf{game}  is -{\sffamily DP}-preserv\-ing, then the whole construction provides -{\sffamily CDP}, as we show now.

\begin{Thm}\label{cdptheorem} Let  be a mechanism for a query  which preserves -\mbox{\upshape\sffamily DP} and let  be a secure PSA scheme for . Then  preserves -\mbox{\upshape\sffamily CDP} if it is used for the perturbation process in \textbf{\upshape game } instantiated with .
\end{Thm}
\begin{proof} Consider again \textbf{game} , -\textbf{game}  and \textbf{game} . We first bound the probability  for the biased coin in -\textbf{game} . Since the perturbation process was performed by , the random variable  corresponds to the output of  and we have

By the Bayes-formula we get

Now let  be a probabilistic polynomial-time Turing machine. Let  denote this Turing machine as adversary in the -\textbf{game}  for any  with\linebreak  and let  denote the same Turing machine as adversary in \textbf{game} . Let finally  denote the same machine as adversary in \textbf{game} . Then for :

Equations \eqref{eqcdp1} and \eqref{eqcdp2} hold because of Lemma \ref{gamezeroPone} and Equations \eqref{eqone1} and \eqref{eqone2} hold because of Lemma \ref{gamePoneone}. 
It follows that

\end{proof}

As mentioned at the beginning of this section, we are considering a mechanism which preserves -\mbox{\upshape\sffamily DP} given some event . Therefore, also Theorem \ref{cdptheorem} applies to this mechanism given . Accordingly, the mechanism \textit{unconditionally} preserves -\mbox{\upshape\sffamily CDP}, where  is a bound on the probability that  does not occur.




























\section{Mechanisms for Differential Privacy}\label{dpmech}

\begin{figure*}\centering
\includegraphics[scale=0.4]{delta_3_mechanisms.eps}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\includegraphics[scale=0.4]{gamma_3_mechanisms.eps}
\caption{Empirical error of the Geometric, Skellam and Binomial mechanisms. The fixed parameters are . The left graph shows the mean of the error in absolute value for variable  and  over  runs, the right graph is for variable  and .}\label{accuracycomp}
\end{figure*}

In this section we recall the Geometric mechanism from \cite{2} and the Binomial mechanism from \cite{14} and introduce the Skellam mechanism. Since these mechanisms make use of a discrete probability distribution, they are well-suited for an execution through a secure PSA scheme, thereby preserving computational differential privacy as shown in the last section.

\begin{Thm}\label{privthm} Let . For all databases  the randomised mechanism 
preserves -\mbox{\upshape\sffamily DP} with respect to any query  with sensitivity , if  is distributed according to one of the following probability distributions:
\begin{enumerate}
\item  with  (and ) \text{\upshape \cite{2}},
\item  with  \text{\upshape \cite{14}},
\item \footnote{ denotes the symmetric Skellam distribution with mean  and variance . For details, see Appendix \ref{skellamsec}.} with 

\end{enumerate}
\end{Thm}

We provide the proof of the third claim in Appendix \ref{skellamsec}. Executing these mechanisms through a PSA scheme requires the use of the known constant  which denotes the a priori estimate of the lower bound on the fraction of non-compromised users. For this case, we provide the accuracy bounds for the aforementioned mechanisms.

\begin{Thm}\label{errorthm} Let  and let  be the a priori estimate of the lower bound on the fraction of non-compromised users in the network. By distributing the execution of a perturbation mechanism as described above and using the parameters from Theorem \ref{privthm}, we obtain -accuracy with the following parameters:
\begin{enumerate}
\item  for the Geometric mechanism, where  bounds the probability that no user has added noise \text{\upshape \cite{2}},
\item  for the Binomial mechanism,
\item  for the Skellam mechanism.
\end{enumerate}
\end{Thm}

The second claim can be easily shown using a standard tail bound for the Binomial distribution. The proof of the third claim is provided in Appendix \ref{skellamsec}.\\
Theorem \ref{errorthm} shows that for constant  the errors of the three mechanisms are bounded by  and therefore do not exceed known bounds in the centralised model. As pointed out in Section \ref{mechov}, the execution of the Geometric mechanism through a PSA scheme requires each user to generate full noise with a small probability. Complementary, the other two mechanisms allow all users to simply generate noise of small variance. While the accuracy bound of the Geometric is roughly a constant factor smaller than the bound of the Binomial, we obtain a better bound for this second approach using the Skellam mechanism. Specifically, the ratio between the factor  in the accuracy of the Skellam mechanism and the factor  in the accuracy of the Geometric mechanism goes to  when  and  go to . For example, fix . Then the Geometric mechanism preserves -\mbox{\upshape\sffamily CDP} with , while the Skellam mechanism preserves -\mbox{\upshape\sffamily CDP} with . An empirical accuracy comparison between the mechanisms is shown in Figure \ref{accuracycomp}. We observe that the error of the Geometric and the Skellam mechanisms have a similar behaviour for both variables  and , while the error of the Binomial mechanism is roughly three times larger. Finally, we are able to prove our main result, Theorem \ref{mainthm}, which follows from the preceding analyses.

\begin{proof}[Proof of Theorem ] The claim follows from Theorem \ref{cdptheorem} together with Theorem \ref{PSATHEOREM} (instantiated with the efficient construction in Example \ref{DDHEXM}) and from Theorem \ref{privthm} together with Theorem \ref{errorthm}.
\end{proof}


\section{Conclusions}

In this work we continued a line of research opened by the work of Shi et al. \cite{2}. By lowering the security definition of a PSA scheme, we were able to prove that a secure scheme (in this sense) can be built upon key-homomorphic weak PRFs. Based on the DDH assumption, we gave an instantiation of a secure PSA scheme. If the plaintext space is large enough, it has a substantially more efficient decryption algorithm than the scheme in \cite{2} at the cost of a slightly less efficient encryption algorithm, and achieves non-adaptive security in the standard model. Using the notion of computational differential privacy, we provided a connection between a secure PSA scheme and a mechanism preserving differential privacy by showing that a differentially private mechanism preserves computational differential privacy if it is executed through a secure PSA scheme. Moreover, we compared the accuracy of the Geometric, the Binomial and the Skellam mechanisms which preserve differential privacy and are suitable for an execution through a PSA scheme. While the practical performances of the Geometric and the Skellam mechanisms are equally better than the performance of the Binomial mechanism, we were able to provide a slightly better bound for the Skellam mechanism at high privacy levels.


\appendix








\section{Proof of Theorem \ref{PSATHEOREM}}\label{ptproof}

Let \textbf{game}  be the security game from Definition \ref{securitygame} instantiated for the PSA scheme of Theorem \ref{PSATHEOREM}. We need to show that the advantage  of a probabilistic polynomial-time adversary  in winning this game is negligible in the security parameter . We define the following intermediate \textbf{game}  for a probabilistic polynomial-time adversary  and then show that winning \textbf{game}  is at least as hard as winning \textbf{game} .\\

\noindent\begin{description}
 \item\textbf{Setup.} The challenger runs the \text{\sffamily Setup} algorithm on input security parameter  and returns public parameters , time-steps  and secret keys  with . It sends  to .
\item\textbf{Queries.} The challenger flips a random bit .  chooses  and sends it to the challenger which returns .  is allowed to query  with  and the challenger returns the following: if  it sends  to ; if  it chooses 

and sends  to .
\item\textbf{Challenge.}  chooses  such that no encryption query at  was made and queries a tuple . If  the challenger sends  to ; if  it chooses 

and sends  to .
\item\textbf{Queries.}  is allowed to make the same type of queries as before with the restriction that no encryption query at  can be made.
\item\textbf{Guess.}  outputs a guess about .
\end{description}
The adversary wins the game if it correctly guesses .\\

\begin{Lem}\label{gameonetwo} Let  be a security parameter. Let  be an adversary in \textbf{\upshape game } with advantage . Then there exists an adversary  in \textbf{\upshape game } with advantage .
\end{Lem}
\begin{proof} Given a successful adversary  in \textbf{game}  we construct a successful adversary  in \textbf{game}  as follows:
\noindent\begin{description}
 \item\textbf{Setup.} Receive  from the \textbf{game} -challenger and send it to .
\item\textbf{Queries.} Flip a random bit . Receive  from  and send it to the challenger. Forward the obtained response  to . Forward 's queries  with  to the challenger and forward the obtained response  to .
\item\textbf{Challenge.}  chooses  such that no encryption query at  was made and queries two different tuples  with . Query  to the challenger. Obtain the response  and forward it to . 
\item\textbf{Queries.}  can make the same type of queries as before with the restriction that no encryption query at  can be made.
\item\textbf{Guess.}  gives a guess about . If the guess is correct, then output ; if not, output .
\end{description}
If  has output the correct guess about  then  can say with high confidence that the challenge ciphertexts were generated using a weak PRF and therefore outputs . On the other hand, if 's guess was not correct, then  can say with high confidence that the challenge ciphertexts were generated using random values and it outputs . Formally:\par\medskip

\noindent\textbf{Case .} Let . Then  perfectly simulates \textbf{game}  for  and the distribution of the ciphertexts is the same as in \textbf{game} :
\par\medskip

\noindent \textbf{Case .} Let . Then the ciphertexts are random with the constraint 

such that decryption yields the same sum as in case .
Because of the perfect security of the one-time pad the probability that  wins \textbf{game}  is  and

Finally we obtain that the advantage of  in winning \textbf{game}  is

\end{proof}

\noindent For a probabilistic polynomial-time adversary , we define a new intermediate \textbf{game}  out of \textbf{game}  by just cancelling the plaintext dependence in each step of \textbf{game} , i.e. in the encryption queries and in the challenge, instead of  the adversary  now just queries  and the challenger in \textbf{game}  sends

to the adversary . The rest remains the same as in \textbf{game}~.\\
It is easy to see that if there exists a successful adversary in \textbf{game}~ then there is also a successful adversary in \textbf{game}~.

\begin{Lem}\label{gametwothree} Let  be a security parameter. Let  be an adversary in \textbf{\upshape game } with advantage . Then there exists an adversary  in \textbf{\upshape game } with advantage .
\end{Lem}

\begin{Rem} For comparison to the proof of adaptive security by Shi et al. \text{\upshape \cite{2}} we emphasise that in the reduction from Aggregator Obliviousness to an intermediate problem (Proof of Theorem  in \text{\upshape \cite{2}}) an adversary  has to compute the ciphertexts  for all users  and for all (!) time-steps , since  does not know in advance for which  it will have to use the PRF  and for which  it will have to use real random values. Thus,  has to program the random oracle  in order to know for all  the corresponding random number  with  (where  is a generator) for simulating the original Aggregator Obliviousness game. In contrast, in the reduction for our non-adaptive version of Aggregator Obliviousness, it is not necessary to program such an oracle, since the simulating adversary  knows in advance the set of non-compromised users and, for all (!) , it can already decide for which  it will use the PRF (which in our case is  instead of ) and for which  it will use a real random value.
\end{Rem}


\noindent In the next step, the problem of distinguishing the weak PRF family 
 
from a random function family has to be reduced to the problem of winning \textbf{\upshape game }. We use a hybrid argument.

\begin{Lem}\label{gamethreeprf} Let  be a security parameter. Let  be an adversary in \textbf{\upshape game } with advantage . Then  if 
 
is a weak PRF family.
\end{Lem}
\begin{proof} We define the following sequence of hybrid games, \textbf{game}  with , for a probabilistic polynomial-time adversary .
\noindent\begin{description}
 \item\textbf{Setup.} As in \textbf{game} .
\item\textbf{Queries.} The challenger flips a random bit .  chooses  and sends it to the challenger which returns .  is allowed to query  with  and the challenger returns the following: if  it sends  to ; if  it chooses 

and sends  to .
\item\textbf{Challenge.}  chooses  such that no encryption query at  was made. The challenger chooses 

and sends the following sequence to : 

\item\textbf{Queries.}  can make the same type of queries as before with the restriction that no encryption query at  can be made.
\item\textbf{Guess.}  outputs a guess about .
\end{description}
The adversary wins the game if it correctly guesses .\\

\noindent It is easy to see that \textbf{game}  with  corresponds to the case  in \textbf{game}  and \textbf{game}  with  corresponds to the case  in \textbf{game} . Moreover the ciphertexts in \textbf{game}  with  have the same distribution as the ciphertexts in \textbf{game}  with . Therefore

Using a successful adversary  in \textbf{\upshape game } we construct a successful probabilistic polynomial-time distinguisher  which has access to an oracle 
 
 
is a weak PRF and 
 
is a real random function.  gets  as input and proceeds as follows.
\noindent\begin{enumerate}
\item Choose two indices  and guess that  will be the  indices in  specified by the adversary . This guess will be correct with probability .
\item Choose , for all , generate  and  with  for all . Compute  for all .
\item Make oracle queries for  and receive  for all .
\item Send  to .
\item\textbf{Queries.} Receive  from . If  or  then abort. Else send  to .
 If  queries  with  then return the following: if  send  to ; if  send  to ; if  choose 

and send  to .
\item\textbf{Challenge.}  chooses  such that no encryption query at  was made. Choose 

and send the following sequence to : 

\item\textbf{Queries.}  can make the same type of queries as before with the restriction that no encryption query at  can be made.
\item\textbf{Guess.}  outputs a guess about whether the  element is random or pseudo-random. Output the same guess.\footnote{Essentially, here the specification of the set of non-compromised users before making any query allows  to be consistent with pseudo-random values or real random values in its replies to the queries.} 
\end{enumerate}
If  has output the correct guess about whether the  element is random or pseudo-random then  can distinguish between  and . Now we prove this result formally and show that, in this way, \textbf{game}  is perfectly simulated by .\par\medskip

\noindent\textbf{Case .} Let . Define . Since  are groups, there exists an element  with 

and for all :

Then for all  the value  is equal to

Therefore the distribution of the ciphertexts corresponds exactly to the case in \textbf{game}  with .\\

\noindent\textbf{Case .} Let . Define the random elements 
 
for all . Since  are groups, there exists an element  with 
 
Let  and . Then for all :
 
and the value  is equal to

and equivalently 

Therefore the distribution of the ciphertexts corresponds exactly to the case in \textbf{game}  with .\par\medskip

\noindent Without loss of generality, let 

All in all, we obtain

and since  is polynomial in , this expression is negligible by the pseudo-randomness of  on uniformly chosen input. Therefore, the advantage of  in winning \textbf{game}  is negligible.\\ 
Finally, by a hybrid argument we have:

\end{proof}\par\medskip

\noindent We can now complete the proof of Theorem \ref{PSATHEOREM}.

\begin{proof}[Proof of Theorem ]  By Lemma \ref{gameonetwo} - \ref{gamethreeprf}:\\
.
\end{proof}






\section{The Skellam mechanism}\label{skellamsec}

\subsection{Preliminaries}

As observed before, the distributed noise generation is feasible with a probability distribution function closed under convolution. For this purpose, we recall the Skellam distribution.


\begin{Def}[Skellam Distribution \cite{29}]\label{skellam} Let , . A discrete random variable  is drawn according to the Skellam distribution with parameters  (short: ) if it has the following probability distribution function :

where  is the modified Bessel function of the first kind (see pages -- in ).
\end{Def}

A random variable  has variance  and can be generated as the difference of two random variables drawn according to the Poisson distribution of mean  and , respectively \cite{29}. Note that the Skellam distribution is not generally symmetric. However, we mainly consider the particular case  and refer to this symmetric distribution as .


\begin{Lem}[\cite{29}]\label{sksum} Let  and  be independent random variables. Then  is distributed according to .
\end{Lem}

An induction step shows that the sum of  i.i.d. symmetric Skellam random variables with variance  is a symmetric Skellam random variable with variance . Suppose that adding symmetric Skellam noise with variance  preserves -\mbox{\upshape\sffamily DP}. Recall that the network is given an a priori known estimate  of the lower bound on the fraction of non-compromised users. We define  and instruct the users to add symmetric Skellam noise with variance  to their own data. If compromised users will not add noise, the total noise will be still sufficient to preserve -\mbox{\upshape\sffamily DP}.\\ 
For our analysis, we will use the following bound on the ratio of modified Bessel functions of the first kind.


\begin{Lem}[\cite{27}]\label{modbesrat} For real  let  be the modified Bessel function of the first kind and order . Then
 
\end{Lem}

For the privacy analysis of the Skellam mechanism, we need a tail bound on the symmetric Skellam distribution.

\begin{Lem}\label{SKELLAMBOUND} Let  and let . Then, for all , 

\end{Lem}
\begin{proof}
We use standard techniques from probability theory. Applying Markov's inequality, for any ,

As shown in \cite{30}, for , the moment generating function of  is 

where . Hence, we have

Fix . In order to conclude the proof, we observe that .
\end{proof}

\noindent One can easily verify that, for ,


\subsection{Analysis of the Skellam mechanism}

In this section, we provide a bound on the variance  of the symmetric Skellam distribution (as stated in Theorem \ref{privthm}) that is needed in order to preserve -differential privacy and we compute the error that is thus introduced.\par\bigskip\bigskip

\noindent\textbf{Privacy analysis}



\begin{Thm}\label{skmech} Let  and let . For all databases  the randomised mechanism 
preserves -\mbox{\upshape\sffamily DP} with respect to any query  of sensitivity , where  with 

\end{Thm}
\begin{proof} Let  be adjacent databases with . The largest ratio between  and  is reached when , where  is any possible output of .
Then, by Lemma \ref{modbesrat}, for all possible outputs  of :

Inequality \eqref{skmechdp} holds if , since it implies  for all  and
 
Applying Lemma \ref{SKELLAMBOUND} with  and , we get

and this expression is set to be smaller or equal than . This inequality is satisfied if

\end{proof}


\begin{Rem} The bound on  from Theorem \text{\upshape \ref{skmech}} is smaller than , thus the standard deviation of  is linear in  (for constant ).
\end{Rem}\par\bigskip\bigskip

\noindent\textbf{Accuracy analysis}

\begin{Thm}\label{erroranalysis} Let  and . Then for all  the mechanism specified in Theorem  has -accuracy, where

\end{Thm}
\begin{proof} Let  be the bound on the variance for the Skellam mechanism provided in Theorem \ref{skmech}. Now, as in the proof of Lemma \ref{SKELLAMBOUND}, for ,

and this expression is set to be equal to . Solving this equality for  yields

\end{proof}\par\medskip

\noindent For the distributed noise generation, each single user adds symmetric Skellam noise with variance  to her data. The worst case for accuracy is when all  users add noise, thus the total noise  is a symmetric Skellam variable with variance  and the accuracy becomes

proving the third claim of Theorem \ref{errorthm}.


\addcontentsline{toc}{chapter}{Literatur}
\bibliography{Literatur}

\end{document}