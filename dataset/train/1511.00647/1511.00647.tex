


\documentclass[runningheads,a4paper]{llncs}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{wrapfig}

\usepackage{url}
\urldef{\mailsa}\path|wenm@seas.upenn.edu|
\urldef{\mailsb}\path|utopcu@utexas.edu|   
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{amsmath}


\usepackage{color}
\usepackage{algpseudocode}
\usepackage{algorithm}


\usepackage{cite}
\usepackage{kantlipsum}



\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}


\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\algnewcommand\True{\textbf{TRUE}\space}
\algnewcommand\False{\textbf{FALSE}\space}

\begin{document}

\mainmatter  

\title{Strategy Synthesis for Stochastic Rabin Games with Discounted Reward}

\titlerunning{Strategy Synthesis for Stochastic Rabin Games with Discounted Reward}

\author{Min Wen\inst{1}\and Ufuk Topcu\inst{2}}


\institute{Department of Electrical and Systems Engineering, University of Pennsylvania \\
\mailsa\\
Department of Aerospace Engineering and Engineering Mechanics, \\ University of Texas at Austin \\
\mailsb\\
}



\maketitle


\begin{abstract}


Stochastic games are often used to model reactive processes. We consider the problem of synthesizing an optimal almost-sure winning strategy in a two-player (namely a system and its environment) turn-based stochastic game with both a qualitative objective as a Rabin winning condition, and a quantitative objective as a discounted reward. Optimality is considered only over the almost-sure winning strategies, i.e., system strategies that guarantee the satisfaction of the Rabin condition with probability 1 regardless of the environment's strategy. We show that optimal almost-sure winning strategies may need infinite memory, but -optimal almost-sure winning strategies can always be finite-memory or even memoryless. We identify a sufficient and necessary condition of the existence of memoryless -optimal almost-sure winning strategies and propose an algorithm to compute one when this condition is satisfied. 





\end{abstract}

\section{Introduction}






Stochastic games, or -player graph games \cite{chatterjee2012survey}, are finite turn-based two-player games between a controlled \emph{system} and its uncontrolled \emph{environment} with probabilistic transitions. The state space is partitioned into system states and environment states, and each player can only take actions at its own states. Stochastic games are commonly used as models of reactive processes, where transition distributions encode the uncertainties in real executions. In reactive synthesis problems, -regular languages are often considered as qualitative descriptions of the desired behaviors \cite{kress2007s}, which can be represented by, for example, Rabin objectives \cite{thomas1997languages}. Rabin objectives are described by a set of Rabin pairs in which each pair contains two disjoint subsets of states. An infinite path of the game is winning for the system if and only if there exists a Rabin pair such that all states in the first subset are visited only for finitely many times, and some states in the second subset are visited infinitely often. Despite this qualitative criterion, game paths can also be evaluated quantitatively with different reward functions. One classical and elegant reward function is the discounted reward \cite{shapley1953stochastic, littman1994markov, filar1996competitive}, which puts exponentially decaying weights to the rewards gained at different steps and therefore rewards gained in near future are weighed more than those gained in far future. The system aims to satisfy the Rabin objective or maximize the reward, while the environment is assumed to be adversarial and tries to violate the Rabin condition or minimize the reward.

In recent years there is an increasing interest in combining qualitative and quantitative objectives in reactive synthesis problems, as the two types of objectives serve different control purposes \cite{bloem2009better, chen2013synthesis, chatterjee2005mean, chatterjee2014perfect}. Intuitively, qualitative objectives like Rabin objectives act as task rules or functionality descriptions of the control system, while quantitative objectives like discounted rewards give a measure of how well the task is implemented. Combining the two types of objectives allows looking for near-optimal strategies for a given task.

We consider the strategy synthesis problems in stochastic games with both a Rabin objective and a discounted reward. Given a game and the objectives, we would like to synthesize a strategy for the system that is optimal or -optimal with respect to the discounted reward and guarantees the satisfaction of the Rabin objective with probability 1. Although the discounted reward majorly cares about finite-time performance, we consider it as a performance criterion of system strategies in the long run. As Rabin objectives are evaluated in infinite sequences, it makes more sense to evaluate system strategies at \emph{all} system states, especially those that are visited infinitely often, rather than only at the given initial state. The difference between these two cases can be illustrated by the example in Fig.~\ref{fig:discount}, for which we want to compute an -optimal strategy for the system. 
\iffalse
\begin{wrapfigure}{r}{6cm}\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
  \tikzstyle{every state}=[draw=black,text=black]

  \node[initial,state,rectangle] (q0)                  {};
  \node[state]         			(q1) [right of=q0]    {};

  \path (q0)    edge [loop above]   node                {} (q0)
                edge [bend left]    node[shift={(0,0)}] {} (q1)
        (q1)    edge [loop above]   node[shift={(0,0)}] {} (q1)
        		    edge [bend left]    node[shift={(0,0)}] {} (q0);
\end{tikzpicture}
\caption{A stochastic game with Rabin pairs .  is an environment state and  is a system state. When the environment takes  at , the state transits to  with probability 0.001 and does a self loop with probability 0.999. The reward is 0 in both cases. If the system takes  at , it does a self loop with probability 1 and the reward is 1; if it takes  at , the state transits back to  with probability 1 and reward 0. }
\end{wrapfigure}
\fi
\begin{figure}[t!]
\minipage{0.52\textwidth}
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,
                    semithick]
  \tikzstyle{every state}=[draw=black,text=black]

  \node[initial,state,rectangle] (q0)                  {};
  \node[state]         			(q1) [above of=q0]    {};

  \path (q0)    edge [loop right]   node                {} (q0)
                edge [bend left]    node[shift={(0,0)}] {} (q1)
        (q1)    edge [loop right]   node[shift={(0,0)}] {} (q1)
        		    edge [bend left]    node[shift={(0,0)}] {} (q0);
\end{tikzpicture}
\caption{A stochastic game with Rabin pairs .  is an environment state and  is a system state. When the environment takes  at , the state transits to  with probability 0.001 and does a self loop with probability 0.999. The reward is 0 in both cases. If the system takes  at , it does a self loop with probability 1 and the reward is 1; if it takes  at , the state transits back to  with probability 1 and reward 0. }
\label{fig:discount}
\endminipage\hfill
\minipage{0.44\textwidth}
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,
                    semithick]
  \tikzstyle{every state}=[draw=black,text=black]

  \node[initial,state] (q0)                        {};
  \node[state]         (q1) [above of=q0]    {};
  \node[state]         (q2) [right of=q0]    {};

  \path (q0)    edge                node                {} (q1)
                edge                node[shift={(0,0)}] {} (q2)
        (q1)    edge [loop right]   node[shift={(0,0)}] {} (q1)
        (q2)    edge [loop above]   node[shift={(0,0)}] {} (q2);
\end{tikzpicture}
\caption{A Rabin game  with Rabin pairs  in which the optimal value in  is strictly less than the optimal value in . All transitions are deterministic and the only transition with positive reward is the self-loop by taking  at . The optimal value over  at  is 0, while the optimal value over  at  is , where  is the discount factor.}
\label{fig:almostsureoptimal}
\endminipage\hfill
\end{figure}
Let  be the initial state and the discount factor be . With probability no less than 0.9, it takes the environment more than 100 steps to leave  for the first time. This suggests that if we consider only the expected future discounted reward gained from the very first step at the initial state, the discounted reward is not adding any further constraints to the synthesis problem if . However, if we consider the expected future discounted reward at all states, the discounted reward effectively encourages the system to stay in  for at least  steps before it chooses to go back to  to ensure the satisfaction of the Rabin objective. This second system strategy is more desirable as the system can recurrently gain high rewards while satisfying the qualitative requirements. 

Although in general, finite-memory -optimal almost-sure winning system strategies always exist and can be synthesized, we are particularly interested in the analysis and synthesis of memoryless solutions for the following three reasons. First, memoryless strategies suffice for the system for being both optimal and almost-sure winning (Theorem~\ref{thm:optimalsufficiency}). Second, memoryless -optimal strategies guarantee -optimal expected future discounted reward not only from any state and but also from any time in infinite game paths, while finite-memory -optimal strategies may only guarantee -optimality with initial memory at all states. Third, the reduced usage of memory keeps the solution simple and efficient.











\paragraph{Related work.} 
It is well-known that linear temporal logic (LTL) specifications can be transformed to deterministic Rabin automata \cite{babiak2013effective}, and there are existing tools to automate this procedure \cite{klein2005linear, gaiser2012rabinizer,  blahoudek2015ltl3dra, klein2015ltl2dstar}. Studies considering both the satisfaction of LTL specifications and optimization with respect to a quantitative objective use different models such as Markov decision processes \cite{wolff2012optimal, ding2014optimal}, non-deterministic systems \cite{wolff2013optimal} and nonlinear systems \cite{wolff2013nonlinear}. Less work has been dedicated to stochastic games. One piece of such work is by Chen et al. \cite{chen2013synthesis}, in which LTL specifications and the expected total reward, rather than the discounted reward, are considered. 

The work in \cite{de2003discounting, almagor2014discounting} connects LTL specifications with discounting and creates the so-called \emph{discounting LTL}, where the satisfaction of LTL specifications is evaluated quantitatively with discounting operators. Studies on discounting LTL focus on the model checking rather than synthesis. The initiatives of combining discounted rewards to encourage close future benefits are the same, but in our case, the discounted reward functions can be used to encode independent preferences from the Rabin objective, which makes the formulation more flexible. 

Most of the previous work on games with both qualitative and quantitative objectives is on mean-payoff parity games \cite{chatterjee2005mean, bloem2009better, chatterjee2014perfect}, which focus on the analysis of game values, (sure or almost-sure) winning regions and the class of strategies that suffices for the given objectives, rather than the synthesis of a system strategy, which is our major interest. 










\paragraph{Contribution.}
We analyze the synthesis of system strategies that both satisfy the given Rabin objective with probability 1, i.e. \emph{almost-sure winning}, and are optimal or -optimal with respect to the given discounted reward function. The discounted reward function is used as a performance criterion of system strategies, and the expected discounted reward is checked not only at initial states but also all other system states. Our main contributions are as follows. 

\begin{enumerate}
\item We show that memoryless strategies suffice for being both optimal and almost-sure winning for the system (Theorem~\ref{thm:optimalsufficiency}) and propose an algorithm to synthesize such a memoryless strategy if it exists (Algorithm~\ref{alg:p1}).\item We show a sufficient and necessary condition of the existence of memoryless almost-sure winning system strategies that can be arbitrarily near-optimal (Theorem~\ref{thm:main}). Randomized strategies with distribution restrictions are used to reduce memory usage. 
\item If the previous condition is satisfied, we propose an algorithm (Algorithm~\ref{alg:suboptimal}) to synthesize an -optimal memoryless system strategy with any , which utilizes off-the-shelf algorithms for the synthesis of stochastic Rabin games. 
\end{enumerate}
 












\section{Preliminaries}
In this section we introduce the definitions and notations used in this paper. For any countable set , denote its cardinality by ; denote be the sets of finite and infinite sequences composed of elements in  by  and  respectively. Let  be the set of all probability distributions defined on . 

\paragraph{Turn-based Rabin game.}
A \emph{turn-based Rabin game} between the system and the environment is defined as a tuple , where  is a finite state space;  is the set of states at which the system chooses actions, and  is the set of states at which the environment chooses actions;  is the set of initial states;  is a finite set of available actions;  is the transition function;  is the set of Rabin pairs;  and  hold for all .

Let  be a mapping from each state to its available actions in . A turn-based Rabin game is \emph{deterministic} if for all  and , ; otherwise it is \emph{probabilistic}. For a nonempty subset , we define the induction of a subgame from a subset as follows. 


\paragraph{Induced game.}
A nonempty set  \emph{induces a subgame } if it holds for all  and  that , and for all , there exists  such that . We denote the induced subgame as , where (1) , , ,  such that  and  hold for all ; (2) for all , , i.e. by taking actions in , the probability of entering  is always zero; (3) for all , , . 

A \emph{run}  of  is an infinite sequence of state-action pairs such that for all ,  and ,  . Without loss of generality, assume that all states are \emph{reachable} from  in , i.e. for any state , there exists a run  and  such that  and . 

\paragraph{Strategy.}
A \emph{(randomized) strategy for the system} is defined as a tuple , where  is a (possibly countably infinite) set of memory states;  is the initial memory state; , and  is the memory update function. If  is a singleton,  is a \emph{memoryless} strategy; if  is a finite set,  is a \emph{finite-memory strategy}. With a slight abuse of notation, we use  to represent  when  is memoryless. 
Let  be a map from each system state  with memory  to the set of actions allowed by , i.e. . If  is memoryless, we use  to represent .
If  for all  and ,  is a \emph{deterministic} strategy. When the exact transition distribution is not of interest, we can define \emph{non-deterministic strategies} with .
A strategy  for the environment can be defined analogously. Let  and  be the sets of all system strategies and environment strategies respectively. 

A run  is \emph{feasible} for a pair of strategies , where  is a strategy for the system and  is a strategy for the environment, if there exist sequences  and  such that (1) , ; (2) for all , , ; (3) for all  such that ,  and ; (4) for all  such that ,  and . Given a pair of strategies  in the game  and a state , we denote the set of feasible runs starting at  by . For all , the probability that the pair of strategies lead to  is denoted by . 



\paragraph{Winning region and winning strategy.} For a run , let  be the set of states that are visited for infinitely many times in . We say that  is \emph{winning for the system} in a turn-based Rabin game  if and only if there exists  such that  and . The set of winning runs for the system is denoted by . 
Given a turn-based Rabin game  where , a strategy  for the system is \emph{sure winning} for the system if  holds at  for all environment strategy ; a strategy  for the system is \emph{almost-sure winning} for the system if the conditional probability  at all . Let  and  be the set of all sure winning and almost-sure winning system strategies in , respectively. Also, let  be the set of all states from which there exists a sure winning strategy for the system, which is called the \emph{sure winning region} of the system. The \emph{almost-sure winning region}  of the system can be defined analogously.

\paragraph{Discounted reward.} Given a turn-based Rabin game , an \emph{instantaneous reward function} is a mapping from transitions to their corresponding rewards . 
The \emph{discounted reward} for the system in a run  of , denoted by , is a discounted sum of the instantaneous rewards it gains at each step, i.e., , where  is a \emph{discount factor}. 


A \emph{value function} is a map , which is the expected discounted reward gained by the system if the system takes the strategy  and the environment takes the strategy  from a state  in , i.e., . In particular, we define the \emph{value of a system strategy } for the system at  as , which is the worst-case expected discounted reward the system can guarantee by taking the strategy . Therefore, the game is zero-sum for the system. 



\paragraph{Optimal strategy and -optimal strategy.} Given a set  of system strategies, a system strategy  is \emph{optimal over } if  holds for all . The \emph{optimal value over } is a mapping  that maps each state  to the value of optimal system strategies over , i.e., for all , . For any , a system strategy  is \emph{-optimal over } if  holds for all . If  is optimal (-optimal) over , it is further called an \emph{optimal (-optimal) system strategy in } and its value is called the \emph{optimal (-optimal) value for the system in }. 
Optimal and -optimal strategies for the environment can be defined analogously. 



\paragraph{Sufficiency of a strategy class for an objective.} 
A class  of strategies for system \emph{suffices for an objective } if whenever there exists a strategy for the system satisfying the objective , there exists a strategy within the class  that also satisfies . The strategy classes discussed in this paper are deterministic strategies, non-deterministic strategies, randomized strategies, memoryless strategies, finite-memory strategies and their intersections. Objectives considered most in this paper are optimality, -optimality, and almost-sure winning for the system. 







\section{Problem Formulation}
\label{section:problemFormulation}
With the definitions and notations introduced in the previous section, we can now formulate the problems. We focus on optimal almost-sure winning strategies in the first problem. 
 


\begin{problem}
Given a zero-sum turn-based Rabin game  in which the system has an almost-sure winning strategy, an instantaneous reward function  and a discount factor , decide if there exists a finite-memory almost-sure winning strategy  that is optimal over all almost-sure winning strategies for the system. Synthesize one such  if it exists. 
\label{problem:existence}
\end{problem}

Note that Problem~\ref{problem:existence} considers the existence of an optimal system strategy over all almost-sure winning strategies in  rather than over all system strategies in , which is consistent with our motivation of considering discounted reward. The same sense of optimality is discussed in \cite{wen2015correct}. The optimal value over  can be strictly less than that over , which is illustrated in the example in Fig.~\ref{fig:almostsureoptimal}. 



It has been shown in \cite{wen2015correct} that finite-memory strategies do not suffice for the objective of being both optimal and sure winning for the system in deterministic Rabin games with discounted rewards. As Problem~\ref{problem:existence} is even more general than the problem in \cite{wen2015correct}, finite-memory strategies do not suffice for the objectives in Problem~\ref{problem:existence}. If finite-memory strategies in Problem~\ref{problem:existence} do not exist in a specific problem, we consider an approximate solution instead, which is a finite-memory -optimal almost-sure winning strategy for the system with an arbitrary . We formulate this problem as follows.

\begin{problem}
Given the inputs of Problem~\ref{problem:existence} and a constant , synthesize a finite-memory almost-sure winning strategy  for the system that is -optimal over all almost-sure winning system strategies, if  in Problem~\ref{problem:existence} does not exist. 
\label{problem:suboptimal}
\end{problem}



We solve the above two problems in the following steps. In Section~\ref{section:optimal} we show that memoryless strategies suffice for being both almost-sure winning and optimal over all almost-sure winning strategies for the system (Theorem~\ref{thm:optimalsufficiency}). We propose Algorithm~\ref{alg:p1} to solve Problem~\ref{alg:p1}. Then in section~\ref{section:nearoptimal} we show a sufficient and necessary condition of the existence of a memoryless almost-sure winning strategy for the system that is -optimal over all almost-sure winning strategies for all  (Theorem~\ref{thm:main}). If this condition is satisfied, Algorithm~\ref{alg:suboptimal} can compute a (randomized) memoryless solution to Problem~\ref{problem:suboptimal} with any given ; otherwise Algorithm~\ref{alg:suboptimal} can get a finite-memory solution. 



\section{Optimal Almost-Sure Winning Strategies}
\label{section:optimal}

In this section, we concentrate on optimal almost-sure winning strategies for Problem~\ref{problem:existence}. 
As explained before, the optimality considered in Problem~\ref{problem:existence} is with respect to the values of all almost-sure winning strategies for the system. Intuitively, to solve Problem~\ref{problem:existence} we need to search for an optimal strategy over all strategies in , but it is hard to encode all almost-sure winning system strategies compactly. However, Lemma~\ref{lemma:aswinningoptimal} and Lemma~\ref{lemma:optimalstrategy} allow searching for an almost-sure winning strategy over all optimal strategies of a newly constructed game. 
Based on this we prove the sufficiency of memoryless deterministic strategies for being both optimal and almost-sure winning, and propose an algorithm to solve Problem~\ref{problem:existence} if a solution exists. 





\paragraph{Optimal value over  and over .}
The example in Fig.~\ref{fig:almostsureoptimal} shows that the optimal value over the set  of all almost-sure winning system strategies can be strictly less than that over the set  of all system strategies. However, Lemma~\ref{lemma:aswinningoptimal} below shows that this can only happen when the almost-sure winning set  is a proper subset of the state space .

\begin{lemma}
Let  be a zero-sum turn-based Rabin game,  be an instantaneous reward function and  be a discount factor. 
If the almost-sure winning region  for the system coincides with , then the optimal value over  is the same as the optimal value over . 
\label{lemma:aswinningoptimal}
\end{lemma}



To prove Lemma~\ref{lemma:aswinningoptimal}, we borrow the results from \cite{chatterjee2005complexity, filar1996competitive} stated as Lemma \ref{lemma:PMsuffice}. 
By Lemma~\ref{lemma:PMsuffice}, the existence of almost-sure winning (respectively, optimal) strategies for the system guarantees the existence of a \emph{memoryless} almost-sure winning (respectively, optimal) strategy for the system. 


\begin{lemma}
\begin{enumerate}
\item \cite{chatterjee2005complexity} Deterministic memoryless strategies suffice for almost-sure winning with respect to Rabin objectives in turn-based stochastic games. 
\item \cite{filar1996competitive} Deterministic memoryless strategies suffice for optimality in zero-sum turn-based stochastic games with discounted rewards.
\end{enumerate}
\label{lemma:PMsuffice}
\end{lemma}

\begin{algorithm}[!t]
\begin{algorithmic}
\Function{FiniteMemStrategy}{, , }
    \State Build a new strategy , where , , 
     and 
\label{alg:FiniteMemStrategy}
\EndFunction
\end{algorithmic}
\end{algorithm}

\paragraph{Proof sketch of Lemma~\ref{lemma:aswinningoptimal}.} 
As the state space  of  coincides with the almost-sure winning region , Lemma~\ref{lemma:PMsuffice} guarantees the existence of a deterministic memoryless almost-sure winning strategy  for the system in . 
Lemma~\ref{lemma:PMsuffice} also guarantees that the system always has a deterministic memoryless optimal system strategy , as there always exist optimal system strategies with respect to Rabin objectives in turn-based games \cite{filar1996competitive}. 
Then for any nonnegative integer , the system strategy  is almost-sure winning at all states in . As  increases, the value of  approaches that of , which is the optimal value over . As the optimal value over  is no less than the supremum of the values of  for all , we know the optimal value over  coincides with that over .



\paragraph{Sufficient and necessary condition of optimality.}
We then consider a sufficient and necessary condition for a system strategy to be optimal in zero-sum turn-based stochastic games with discounted rewards. 
Such games always have optimal value functions \cite{filar1996competitive}, which equal the unique solution of \eqref{eq:optimal_value_function} for all . 


Also, the set of optimal memoryless strategies for the system is exactly those  satisfying the following conditions for all  \cite{filar1996competitive}: 

We call  the set of \emph{optimal actions} at , and actions in  \emph{suboptimal actions} at . Therefore a memoryless system strategy  is optimal if and only if  holds for all . Furthermore, a finite-memory system strategy  is optimal if and only if  holds for all  and . Assume  is a finite-memory optimal strategy, then for all  and , the expected discounted reward at  must be the same as , the optimal value of . Therefore the optimal actions at  can only be a subset of . This key property of optimal strategies is summarized in Lemma~\ref{lemma:optimalstrategy}.

\begin{lemma}
Given a zero-sum turn-based Rabin game , an instantaneous reward function  and a discount factor , a system strategy  is optimal with respect to the discounted reward if and only if  holds for all  and . 
\label{lemma:optimalstrategy}
\end{lemma}

As a result, if we limit the set of available actions at  to be , we can construct a new game  such that a system strategy  is optimal in  if and only if it is \emph{a strategy} for the system in . 

\paragraph{Synthesis of optimal almost-sure winning strategies.} With the previous analysis we are now ready to propose an Algorithm~\ref{alg:p1} to solve Problem~\ref{problem:existence}. 




As we assume that the system has an almost-sure winning strategy in , the almost-sure winning region  is nonempty. Computation of almost-sure winning regions and strategies in stochastic Rabin games can be performed with off-the-shelf algorithms as those in \cite{chatterjee2005complexity, chatterjee2006strategy}, and we omit the details here. 
We can construct a subgame  of  such that . By definition of almost-sure winning region and induced game, , i.e. all almost-sure winning strategies in  are preserved in ; by Lemma~\ref{lemma:aswinningoptimal}, the optimal value over  is the same as that over . 
Therefore a solution to Problem~\ref{problem:existence} must be both almost-sure winning and optimal over all system strategies in . 


Then we utilize existing methods like value iteration to compute the optimal value function  of . With  we compute the optimal actions  for all . Lemma~\ref{lemma:optimalstrategy} suggests that we can construct a new game  from  by forcing the system to take only optimal actions, such that a system strategy is optimal in  if and only if it is a system strategy in . In other words, exactly the set of all optimal system strategies are preserved in . 

Therefore, an almost-sure winning strategy  is optimal over all almost-sure winning system strategies in  if and only if  is both almost-sure winning for the system and optimal in , which is further equivalent to being almost-sure winning in . Hence any solution  to Problem~\ref{problem:existence} is an almost-sure winning strategy in , and vice versa. 

By Lemma~\ref{lemma:PMsuffice}, deterministic memoryless strategies suffice for almost-sure winning with Rabin objectives on turn-based stochastic games, and we end up with the following theorem. 

\begin{theorem}
Given a zero-sum turn-based Rabin game , an instantaneous reward function  and a discount factor , 
deterministic memoryless strategies suffice for being both almost-sure winning and optimal over all almost-sure winning strategies for the system. 
\label{thm:optimalsufficiency}
\end{theorem}
 




\begin{algorithm}[!t]
\begin{algorithmic}[1]
\Require {A turn-based Rabin game  in which the system has an almost-sure winning strategy, an instantaneous reward function , and a discount factor .}
\Ensure {\True if there exists a finite-memory almost-sure winning strategy  for the system that is optimal over all almost-sure winning strategies for the system; \False otherwise. }
    \State Compute the almost-sure winning region  for the system in . \State Construct a new game . 
    \State Compute the optimal value function  for . 
    \State Compute the optimal actions  for all .
\State Construct a new game  such that  for all .  and  are the reachable subsets of their corresponding component in .
\State Compute the almost-sure winning region  for the system in . \If{} 
    		\State Compute a deterministic memoryless almost-sure winning strategy  for the system in . .
        \State \Return \True and  is a solution to Problem~\ref{problem:existence}.
\Else
    		\State \Return \False.
\EndIf
\end{algorithmic}
\caption{Pseudo algorithm for Problem~\ref{problem:existence}.}
\label{alg:p1}
\end{algorithm}


\section{Near-Optimal Almost-Sure Winning Strategies}
\label{section:nearoptimal}





We showed in Section~\ref{section:problemFormulation} that finite-memory strategies do not suffice for the objectives in Problem~\ref{problem:existence}. For cases in which finite-memory solutions to Problem~\ref{problem:existence} do not exist, we relax the optimality objective and consider near-optimal almost-sure winning strategies for the system as stated in Problem~\ref{problem:suboptimal}, and Proposition~\ref{prop:eoptimalfinitesufficient} shows that finite-memory strategies for the system suffice for these objectives. 

\begin{proposition}
Given a zero-sum turn-based Rabin game , an instantaneous reward function  with upper bound  and a discount factor , if  is a memoryless almost-sure winning strategy and  is a memoryless optimal system strategy, then for any  and ,  is an -optimal almost-sure winning strategy for the system. 
\label{prop:eoptimalfinitesufficient}
\end{proposition}

The finite-memory strategy  is a solution to Problem~\ref{problem:suboptimal}, but it may not be desirable compared with memoryless solutions (if exist), for two main reasons.
First, the number of memory states  in  grows linearly in . 
The execution of  is equivalent to taking a memoryless strategy in a game in which the number of states is  times the number of states in the original game . Second, finite-memory -optimal strategies can only guarantee -optimal discounted reward at the start of runs, while memoryless -optimal strategies can guarantee -optimal future discounted reward from any step during the infinite execution. For all these reasons, we focus on \emph{memoryless} -optimal almost-sure winning strategies in this section. 











\paragraph{-optimal memoryless strategies.}
First we check the existence of -optimal memoryless system strategies for any , without considering the almost-sure winning objective. 
We show that for all , -optimal randomized memoryless system strategies always exist. 



Lemma~\ref{lemma:optimalstrategy} guarantees that a system strategy is optimal if and only if the actions allowed at all system states are always optimal actions. Allowing the system to take suboptimal actions will result in suboptimal strategies, but given the upper bound of the instantaneous reward, we can bound the suboptimality of the memoryless system strategy by restricting the probability that the system takes suboptimal actions at each system state. This key observation is summarized in the following lemma. 


\begin{lemma}
Let  be a turn-based game,  be an instantaneous reward function with upper bound , and  be the discount factor. For any , if the probability that the system chooses an optimal action is at least  at all system states, the strategy for the system is -optimal.
\label{lemma:eoptimal}
\end{lemma}

As optimal actions exist at all system states, Lemma~\ref{lemma:eoptimal} proves the existence of -optimal memoryless system strategies for all . Therefore finite-memory strategies suffice for -optimality for all . 



\paragraph{Independence of almost-sure winning on distributions.}


Now we consider memoryless -optimal system strategies that are almost-sure winning with respect to a Rabin objective. 
The following lemma shows that, whether a memoryless system strategy  is almost-sure winning or not is independent of the exact distribution , if  is given for all system state . 






\begin{lemma}
Let a turn-based Rabin game  be given. Let  and  be two memoryless strategies for the system in . 
Provided that  holds for all ,  is almost-sure winning for the system if and only if  is almost-sure winning for the system.
\label{lemma:aswinning}
\end{lemma}

The proof idea of Lemma~\ref{lemma:aswinning} is similar to that of Theorem 3 in \cite{chatterjee2005complexity}. It has been shown that with probability  the set of states that are visited infinitely often in an infinite run is an \emph{end component} \cite{de1997formal}, which is a strongly connected subset of  from which there are no outgoing transitions for both players. Assume , then a system strategy  is almost-sure winning if and only if, by taking , there exists  for each reachable end component  such that  and , regardless of the environment strategy. If  holds for all , then, with any environment strategy , the sets of reachable end components are the same for the two strategy pairs  and . Therefore the two strategies  and  can only be almost-sure winning simultaneously. 




\paragraph{Connecting the two objectives.}
Lemma~\ref{lemma:eoptimal} and Lemma~\ref{lemma:aswinning} suggest that the two objectives in Problem~\ref{problem:suboptimal} can be partially decoupled in the synthesis of a memoryless system strategy : in order to be almost-sure winning, we only need to set  properly; and in order to be -optimal, we only need to make sure that  and  is bounded properly for all . If these conditions can be satisfied at the same time,  is memoryless, almost-sure winning and -optimal. We summarize this result in the following theorem. 

\begin{theorem}
Let a turn-based game , an instantaneous reward function  and a discount factor  be given. If there exists a memoryless almost-sure winning strategy  for the system that allows taking optimal actions at all system states, 
i.e., for all , , then the class of memoryless strategies for the system suffices for the objective of being both almost-sure winning and -optimal for all .
\label{thm:arbitrarye}
\end{theorem}

It is possible that the conditions in Lemma~\ref{lemma:eoptimal} and Lemma~\ref{lemma:aswinning} cannot be satisfied at the same time. In such cases, there exists some positive  such that memoryless -optimal almost-sure winning strategies for the system do not exist. 

\begin{lemma}
Let a turn-based game , an instantaneous reward function  and a discount factor  be given. If, for any memoryless almost-sure winning strategy  for the system, there exists a state  such that no optimal action can be taken, i.e., , then there exists  such that no memoryless strategies for the system can be both -optimal and almost-sure winning.
\label{lemma:epsilonno}
\end{lemma}

The condition in Lemma~\ref{lemma:epsilonno} can be illustrated by the example in Fig.~\ref{fig:counterexample}, in which the system can only win by visiting  infinitely often and visiting  for finitely many times. The optimal actions are , . 
Both states belong to the almost-sure winning region. If a memoryless strategy for the system is to guarantee almost-sure winning, it cannot allow taking  at , i.e., no optimal actions can be allowed at . 
Then if ,
the system does not have \emph{memoryless} -optimal almost-sure winning strategies for the system. 
\begin{figure}[t!]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
  \tikzstyle{every state}=[draw=black,text=black]

  \node[initial,state] (q0)                  {};
  \node[state]         (q1) [right of=q0]    {};

  \path (q0)    edge [loop above]   node                {} (q0)
                edge [bend left]    node[shift={(0,0)}] {} (q1)
        (q1)    edge [loop above]   node[shift={(0,0)}] {} (q1)
                edge [bend left]    node[shift={(0,0)}] {} (q0);
\end{tikzpicture}
\caption{A turn-based Rabin game  in which the system does not have memoryless almost-sure -optimal strategy for all . The labels on transitions show the underlying actions, the instantaneous rewards and the transition probabilities. The unique Rabin pair is . }
\label{fig:counterexample}
\end{figure}







Theorem~\ref{thm:arbitrarye} and Lemma~\ref{lemma:epsilonno} can be combined into the following main theorem, which is a sufficient and necessary condition of the existence of memoryless solutions to Problem~\ref{problem:suboptimal} for all  simultaneously. 

\begin{theorem}
Given a turn-based game , an instantaneous reward function  and a discount factor , the class of memoryless strategies for the system suffices for the objective of being both almost-sure winning and -optimal for all  simultaneously if and only if there exists a memoryless almost-sure winning system strategy  that allows taking optimal actions at all states in , i.e., for all , .
\label{thm:main}
\end{theorem}

\paragraph{Synthesis of optimal almost-sure winning strategies.} 
We now propose Algorithm~\ref{alg:suboptimal} to solve Problem~\ref{problem:suboptimal}. If the condition in Theorem~\ref{thm:arbitrarye} is satisfied, the strategy synthesized by Algorithm~\ref{alg:suboptimal} is memoryless; otherwise it is finite-memory. In the second case, a finite-memory solution can be computed by the function {\bf FiniteMemStrategy}, as explained in Proposition~\ref{prop:eoptimalfinitesufficient}. Here we focus on the synthesis of a memoryless solution. 


\begin{algorithm}[!t]
\begin{algorithmic}[1]
\Require{A turn-based Rabin game  in which the system has an almost-sure winning strategy, an instantaneous reward function  with upper bound , a discount factor  and a constant .}
\Ensure {An -optimal almost-sure winning strategy  for the system. }
    \State Compute the almost-sure winning region  for the system in . Let  be a deterministic memoryless almost-sure winning strategy for the system in .
    \State \label{step:G}Construct a subgame of  as . 
    \State Compute the optimal value function  for . 
    \State Compute the optimal actions  for all .
    \State \label{step:hatG}Construct a new game , where , ,  ; ;  is defined in \eqref{eq:transitionFunction}. 
\State \label{step:ashatG}Compute the almost-sure winning region  and a memoryless almost-sure winning strategy  for the system in . 
    \If{} 
\State \label{step:memoryless}Construct  as in \eqref{eq:construct1}, .
    \Else
        \State Compute a deterministic memoryless optimal system strategy  in .
        \State , . 
\EndIf
    \State \Return .
\end{algorithmic}
\caption{Pseudo algorithm for Problem~\ref{problem:suboptimal}.}
\label{alg:suboptimal}
\end{algorithm}

Let  be the game constructed in Step~\ref{step:G} of Algorithm~\ref{alg:suboptimal}, and the optimal value function be . As the state space of  coincides with the , Lemma~\ref{lemma:aswinningoptimal} guarantees that  is also the optimal value function over all almost-sure winning strategies in . Therefore an -optimal almost-sure winning system strategy in  is also -optimal and almost-sure winning in , and vice versa. As a result, we can synthesize a solution to Problem~\ref{problem:suboptimal} in  instead of .

Assume that the condition in Theorem~\ref{thm:arbitrarye} is satisfied. In order to compute a memoryless -optimal almost-sure winning strategy , we need to consider both the constraint on  and the probability bound on  for all  at the same time. 
The approach in Algorithm~\ref{alg:suboptimal} is to construct a new turn-based Rabin game  from  (Step~\ref{step:hatG}) that satisfies the following lemma. 






\begin{lemma}
Let  and  be the turn-based Rabin games constructed in Step~\ref{step:G} and Step~\ref{step:hatG} of Algorithm~\ref{alg:suboptimal} respectively.  is an input of Algorithm~\ref{alg:suboptimal}. 
Then each memoryless system strategy  in  can be used to construct a memoryless -optimal system strategy  in  such that
\begin{itemize}
\item  is -optimal in ; and
\item  is almost-sure winning for the system in  if and only if the constructed  is almost-sure winning for the system in . 
\end{itemize}
\label{lemma:prop1}
\end{lemma}








Given the two properties of  in Lemma~\ref{lemma:prop1}, 
it suffices to compute a memoryless almost-sure winning strategy for the system in  (Step~\ref{step:ashatG}) in order to compute a memoryless \emph{-optimal} almost-sure winning strategy for the system in , which can be solved again with off-the-shelf algorithms \cite{chatterjee2005complexity, chatterjee2006strategy}. 




We now show the construction of  from  and verify that it satisfies Lemma~\ref{lemma:prop1}. 
Let , , and .  and  are two sets of new system states that are mutually disjoint. Let  and  be two bijective functions, and we use  and  to denote their inverse functions. For each state  in , we add two states  and  to . 
The set of available actions at each state  is defined as

The transition function  is defined  in \eqref{eq:transitionFunction}, where each transition from a state  in  is separated into two transitions in . From each , there is only one available action , which transits from  to  with probability  and to  with probability . If it transits to , the system is free to choose from all actions in ; otherwise the system can only take an optimal action in . The transition distribution  for the second transition where  is the same as  for all . 


With each memoryless system strategy  in , we can construct a memoryless system strategy  in  such that for all  and , 

This two-step decomposition of system transitions in  ensures that when the system takes , the probability of taking suboptimal actions at each system state is bounded by .  
By Lemma~\ref{lemma:eoptimal},  is -optimal. The fact that  is almost-sure winning if and only  is almost-sure winning can be proved by Lemma~\ref{lemma:aswinning}. As a result,  satisfies Lemma~\ref{lemma:prop1}. As  is almost-sure winning in ,  in Step~\ref{step:memoryless} is a memoryless solution to Problem~\ref{problem:suboptimal}. 














\begin{remark}
Algorithm~\ref{alg:suboptimal} is not guaranteed to output a memoryless solution to Problem~\ref{problem:suboptimal} if one exists for the given . The output of Algorithm~\ref{alg:suboptimal} is memoryless only if the condition in Theorem~\ref{thm:arbitrarye} holds. If there \emph{exists an}  such that no memoryless solutions exist, the condition in Theorem~\ref{thm:arbitrarye} is violated and Algorithm~\ref{alg:suboptimal} outputs a finite-memory solution to Problem~\ref{alg:suboptimal}, even if there exists a memoryless solution for the given .
\end{remark}










\section{Conclusion}

We considered the synthesis of optimal and -optimal almost-sure winning strategies in two-player turn-based stochastic games with Rabin winning conditions and discounted performance criteria. We showed that memoryless strategies suffice for being both optimal and almost-sure winning for the system and provided with an algorithm to solve one if they exist. We also showed a sufficient and necessary condition of the existence of memoryless -optimal almost-sure winning system strategies for all  simultaneously. 
Given a specific , we proposed an algorithm which solves a memoryless -optimal almost-sure winning strategy if this condition is satisfied, and a finite-memory -optimal almost-sure winning strategy if this condition is violated. 






\section*{Acknowledgements}
We appreciate R{\"u}diger Ehlers and Guillermo A. P{\'e}rez for the helpful discussions. 

\bibliographystyle{plain}
\bibliography{discountedRabin}







\end{document}
