








\documentclass[conference, a4paper]{IEEEtran}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{colortbl}
\usepackage{float}
\usepackage{algpseudocode}
\usepackage[ruled]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{enumitem}
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}























\ifCLASSINFOpdf
\else
\fi














































\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{\textit{Do You Really Mean That?} Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization}




\author{\IEEEauthorblockN{Zhixi~Cai}
\IEEEauthorblockA{Monash University\\
Australia\\
zhixi.cai@monash.edu}
\and
\IEEEauthorblockN{Kalin~Stefanov}
\IEEEauthorblockA{Monash University\\
Australia\\
kalin.stefanov@monash.edu}
\and
\IEEEauthorblockN{Abhinav Dhall}
\IEEEauthorblockA{Monash University\\
India\\
abhinav@iitrpr.ac.in}
\and
\IEEEauthorblockN{Munawar Hayat}
\IEEEauthorblockA{Monash University\\
Australia\\
munawar.hayat@monash.edu}
}





\author{\IEEEauthorblockN{Zhixi~Cai\IEEEauthorrefmark{1},
Kalin~Stefanov\IEEEauthorrefmark{1},
Abhinav~Dhall\IEEEauthorrefmark{2} and
Munawar~Hayat\IEEEauthorrefmark{1}
}
\IEEEauthorblockA{
\{zhixi.cai,kalin.stefanov,munawar.hayat\}@monash.edu,abhinav@iitrpr.ac.in}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Monash University, Australia}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Indian Institute of Technology Ropar, India
}
}













\maketitle
\newcommand\dataset{Localized Audio Visual DeepFake}
\newcommand\datasetabbr{LAV-DF}
\newcommand\model{Boundary Aware Temporal Forgery Detection}
\newcommand\modelabbr{BA-TFD}
\newcommand\task{Temporal Forgery Localization}
\newcommand\tasklower{temporal forgery localization}
\newcommand\taskabbr{TFL}
\begin{abstract}
Due to its high societal impact, deepfake detection is getting active attention in the computer vision community. Most deepfake detection methods rely on identity, facial attributes, and adversarial perturbation-based spatio-temporal modifications at the whole video or random locations while keeping the meaning of the content intact. However, a sophisticated deepfake may contain only a small segment of video/audio manipulation, through which the meaning of the content can be, for example, completely inverted from a sentiment perspective. We introduce a content-driven audio-visual deepfake dataset, termed \dataset{} (\datasetabbr{}), explicitly designed for the task of learning temporal forgery localization. Specifically, the content-driven audio-visual manipulations are performed strategically to change the sentiment polarity of the whole video. Our baseline method for benchmarking the proposed dataset is a 3DCNN model, termed as \model{} (\modelabbr{}), which is guided via contrastive, boundary matching, and frame classification loss functions. Our extensive quantitative and qualitative analysis demonstrates the proposed method's strong performance for temporal forgery localization and deepfake detection tasks.
\end{abstract}




\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}
Advances in computer vision and deep learning methods (\eg Autoencoders~\cite{rumelhart_learning_1985} and Generative Adversarial Networks~\cite{goodfellow_generative_2020}) have enabled the creation of very realistic fake videos, known as \textit{deepfakes}\footnote{In the text, \textit{deepfake} and \textit{forgery} are used interchangeably.}. There are various ways of creating deepfakes, including voice cloning~\cite{wang_tacotron_2017, jia_transfer_2018}, face reenactment~\cite{tulyakov_mocogan_2018, prajwal_lip_2020}, and face swapping~\cite{korshunova_fast_2017, nirkin_fsgan_2019}. Highly realistic deepfakes are a potential tool for spreading harmful misinformation, given our increasing online presence. This success in generating high-quality deepfakes has raised serious concerns about their role in shaping people's beliefs, with some scholars suggesting that deepfakes are a ``threat to democracy''~\cite{schwartz_you_2018, brandon_there_2019, sample_what_2020, thomas_deepfakes_2020}. As an example of the potentially harmful effect of deepfakes, consider the recent work~\cite{thies_neural_2020} that uses a video of the former United States President Barack Obama to showcase a novel face reenactment method. In this work, the lip movements of Barack Obama are synchronized with another person's speech, resulting in high quality and realistic video in which the former president appears to say something he never did. Given the recent surge in synthesized fake video content on the Internet, it has become increasingly important to identify deepfakes with more accurate and reliable methods. This has led to the release of several benchmark datasets~\cite{korshunov_deepfakes_2018, rossler_faceforensics_2019, dolhansky_deepfake_2020} and methods~\cite{mirsky_creation_2021} for fake content detection. These fake video detection methods aim to correctly classify any given input video as either \textit{real} or \textit{fake}. This suggests that the major assumption behind those datasets and methods is that fake content is present in the entirety of the video/audio signal; that is, there is some form of manipulation throughout the content. And current state-of-the-art deepfake detection methods~\cite{coccomini_combining_2022, heo_deepfake_2021, wodajo_deepfake_2021} achieve impressive results on this problem using the largest benchmark datasets.


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{image/overview4.pdf}
\caption{\textbf{Content-driven audio-visual manipulation.} On the left is a real video with the subject saying ``Vaccinations are safe''. On the right is an audio-visual deepfake created from the real video based on the change in perceived sentiment where ``safe'' is changed to ``dangerous''. Green-edge and red-edge images are real and fake frames, respectively. \textit{Through subtle audio-visual manipulation, the whole meaning of the video content has changed.}}
\label{fig:overview} 
\end{figure}

\begin{table*}[t]
\centering
\caption{\textbf{Comparison of the proposed dataset with ohter publicly available deepfake datasets.} \textit{Cla: Classification}, \textit{SL: Spatial Localization}, \textit{\taskabbr{}: \task{}}, \textit{FS: Face Swapping}, and \textit{RE: ReEnactment}.}
\label{tab:datasets}
\scalebox{1}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Year} & \textbf{Tasks} & \textbf{Manipulated} & \textbf{Manipulation} & \textbf{\#Subjects} & \textbf{\#Real} & \textbf{\#Fake} & \textbf{\#Total} \\
&  &  & \textbf{Modality} & \textbf{Method} &  &  &  & \\
\hline\hline
DF-TIMIT~\cite{korshunov_deepfakes_2018} & 2018 & Cla & V &  FS & 43 & 320 & 640 & 960 \\
UADFV~\cite{yang_exposing_2019} & 2019 & Cla & V & FS & 49 & 49 & 49 & 98  \\
FaceForensics++~\cite{rossler_faceforensics_2019} & 2019 & Cla & V & FS/RE & - & 1,000 & 4,000 & 5,000 \\
Google DFD~\cite{nick_contributing_2019} & 2019 & Cla & V & FS & - & 363 & 3,068 & 3,431 \\
DFDC~\cite{dolhansky_deepfake_2020} & 2020 & Cla & AV & FS & 960 & 23,654 & 104,500 & 128,154 \\
DeeperForensics~\cite{jiang_deeperforensics-10_2020} & 2020 & Cla & V & FS & 100 & 50,000 & 10,000 & 60,000 \\
Celeb-DF~\cite{li_celeb-df_2020} & 2020 & Cla & V & FS & 59 & 590 & 5,639 & 6,229 \\
WildDeepfake~\cite{zi_wilddeepfake_2020} & 2021 & Cla & - & - & - & 3,805 & 3,509 & 7,314 \\
FakeAVCeleb~\cite{khalid_fakeavceleb_2021} & 2021 & Cla & AV & RE & 600 & 570 & 25,000 & 25,500 \\
ForgeryNet~\cite{he_forgerynet_2021} & 2021 & SL/\taskabbr{}/Cla & V & Random FS/RE & 5400 & 99,630 & 121,617 & 221,247 \\
\hline
\datasetabbr~(Ours) & 2022 & \taskabbr{}/Cla & AV & Content-driven RE & 153 & 36,431 & 99,873 & 136,304 \\
\hline
\end{tabular}}
\end{table*}

However, fake content might constitute only a small part of an otherwise long real video, as was initially suggested in~\cite{chugh_not_2020}. Such short modified segments have the power to alter the meaning and sentiment of the original content completely. For example, consider the manipulation illustrated in Figure~\ref{fig:overview}. The real video might represent a person saying ``Vaccinations are safe'', while the fake includes only a short modified segment; for example, ``safe'' is replaced with ``dangerous''. Hence, the meaning and sentiment of the fake video differ significantly from the real one. If done precisely, this type of coordinated manipulation can sway public opinion (\eg when employed for media of a famous person as the example with Barack Obama) in a particular direction, for example, based on target sentiment polarity. Given the discussed central assumption behind current datasets and methods, the state-of-the-art deepfake detectors might not perform well on this type of manipulation.

This paper tackles the important task of detecting content altering fake segments in videos. The literature review on benchmark datasets for deepfake detection indicates that there is no dataset suitable for this task, that is, a dataset that consists of content-driven manipulations. Therefore, this paper describes the process of creating such a large-scale dataset that will enable further research in this important direction. In addition, we propose a novel multimodal method for precisely predicting the boundaries of fake segments based on visual and audio information. The \textbf{main contributions} of our work are as follows,

\begin{enumerate}
\item{We introduce a new large-scale public audio-visual dataset called \textit{\dataset{}}.}
\item{We propose a new multimodal method called \textit{\model{}}.}
\end{enumerate}

\section{Related Work}
\label{sec:related_work}
\noindent \textbf{Deepfake Datasets.} The body of research in deepfake detection is driven by seminal datasets curated with different manipulation methods. A summary of the relevant datasets is presented in Table~\ref{tab:datasets}. Korshunov and Marcel~\cite{korshunov_deepfakes_2018} curated one of the first deepfake datasets, DF-TIMIT, where face-swapping was performed on VidTimit~\cite{sanderson_vidtimit_2009}. Down the lane, other important datasets such as UADFV~\cite{yang_exploring_2018}, FaceForensics++~\cite{rossler_faceforensics_2019}, and Google DFD~\cite{nick_contributing_2019} were introduced. Due to the complexity of face manipulation and limited availability of open-source face manipulation techniques, these datasets are fairly small in size~\cite{li_celeb-df_2020}. Facebook released a large-scale dataset DFDC~\cite{dolhansky_deepfake_2020} in 2020 for the task of deepfake classification. Multiple face manipulation methods generated 128,154 videos, including real videos of 3000 actors. DFDC has become a mainstream benchmark dataset for the task of deepfake detection. With the progress in both audio and visual deepfake manipulation, post DFDC, several new datasets including Celeb-DF~\cite{li_celeb-df_2020}, DeeperForensics~\cite{jiang_deeperforensics-10_2020}, and WildDeepFake~\cite{zi_wilddeepfake_2020} were introduced. All these datasets are designed for the binary task of deepfake classification and focus primarily on visual manipulation detection~\cite{chugh_not_2020}. In 2021, OpenForensics~\cite{le_openforensics_2021} dataset was introduced for spatial detection, segmentation and classification. Recently, FakeAVCeleb~\cite{khalid_fakeavceleb_2021} was released, focusing on both face-swap and face-reenactment methods with manipulated audio and video. ForgeryNet\cite{he_forgerynet_2021} is the latest contribution to the growing list of deepfake detection datasets. This large-scale dataset is also centered around video-only identity manipulation and is suitable for video/image classification and spatial/temporal forgery localization tasks.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{image/data_generation3.pdf}
\caption{\textbf{Generation pipeline of the proposed dataset}. The green-edge audio and video frames are the real data, and the red-edge audio and video frames are the generated fake data. The real audio-based transcript is used to decide the location and content to be replaced based on the largest change in sentiment. The chosen antonyms are used as input for generating fake audio with voice cloning. The post-processing and normalization are applied to the audio to maintain the consistency of the loudness between the generated audio and real audio in the neighborhood. The generated audio is used as input for facial reenactment. Three categories of data are generated: \textit{\textless Fake Audio and Fake Video\textgreater, \textless Fake Audio and Real Video\textgreater} and \textit{\textless Real Audio and Fake Video\textgreater}. The details on dataset generation are discussed in Section~\ref{sec:proposed_dataset}.}
\label{fig:data_generation}
\end{figure*}

All previous datasets provide face manipulations that occur in most of the frames of the video~\cite{chugh_not_2020}. Only the latest one, ForgeryNet, provides examples of the important problem of \tasklower{} since it includes random face-swapping applied to parts of some videos. However, the manipulations present in that dataset are only identity modifications that do not necessarily alter the meaning of the content. Our content-driven manipulation dataset addresses this important gap.

\noindent \textbf{Deepfake Detection.} Deepfake detection methods draw inspiration from observations of artifacts such as different eye colors and unnatural blink and lip-sync issues in deepfake videos. These binary classification approaches are based on both traditional machine learning methods (\eg EM~\cite{guarnera_deepfake_2020} and SVM~\cite{yang_exposing_2019}) and deep learning methods (\eg 3DCNN\cite{de_lima_deepfake_2020}, GRU\cite{montserrat_deepfakes_2020} and ViT~\cite{wodajo_deepfake_2021, heo_deepfake_2021, coccomini_combining_2022}). Previous methods~\cite{lewis_deepfake_2020, gu_spatiotemporal_2021} also aim to detect temporal inconsistencies in deepfake content and recently, several audio-visual deepfake detection methods such as MDS~\cite{chugh_not_2020} and M2TR~\cite{wang_m2tr_2021} were proposed. The methods above are classification centric and do not focus on temporal localization. The only exception is the MDS, shown to work for localization tasks, however, the method is designed primarily for classification. The proposed dataset and method are specifically designed for temporal localization of manipulations.

\noindent \textbf{Temporal Localization.} Given that the task of temporal forgery localization is similar to the task of temporal action localization, previous work in this area is important. Benchmark datasets in this domain include THUMOS~\cite{idrees_thumos_2017} and ActivityNet~\cite{caba_heilbron_activitynet_2015} and the proposed methods can be grouped into two categories: 2-step approaches which first generate segment proposals and then perform multi-class classification to evaluate the proposals~\cite{zeng_graph_2019, xu_g-tad_2020, liu_multi-shot_2021} and 1-step approaches which directly generate the final segment predictions~\cite{lin_single_2017,buch_end--end_2019,nawhal_activity_2021}. For temporal forgery localization, there are no classification requirements for the foreground segments; the background is always real, and the foreground segments are always fake. Therefore, boundary prediction and 1-step approaches are more relevant for our task. Bagchi et al.~\cite{bagchi_hear_2021} divided the approaches to segment proposal estimation in temporal action localization into two main categories: methods based on anchors and methods based on predicting the boundary probabilities. As for the anchor-based, these methods mainly use sliding windows in the video, such as S-CNN~\cite{shou_temporal_2016}, CDC~\cite{shou_cdc_2017}, TURN-TAP~\cite{gao_turn_2017} and CTAP~\cite{gao_ctap_2018}. As for the methods predicting the boundary probabilities, Lin et al.~\cite{lin_bsn_2018} introduced BSN. The method can utilize the global information to overcome the problem that anchor-based methods cannot generate precise and flexible segment proposals. Based on BSN, BMN~\cite{lin_bmn_2019} and BSN++~\cite{su_bsn_2021} were introduced for improved performance. It is worth noting that all these methods are unimodal, which is not optimal for the task of temporal forgery localization. The importance of multimodality was demonstrated recently by AVFusion~\cite{bagchi_hear_2021}.
    
\noindent \textbf{Proposed Approach.} For the task of temporal forgery localization, both the audio and visual information are important, in addition to the required precise boundary proposals. In this paper, we introduce a multimodal method based on boundary probabilities and compare its performance with BMN~\cite{lin_bmn_2019}, AGT~\cite{nawhal_activity_2021}, MDS~\cite{chugh_not_2020} and AVFusion~\cite{bagchi_hear_2021}.

\section{Proposed Dataset}
\label{sec:proposed_dataset}
The proposed dataset \dataset{} (\datasetabbr{}) is a large audio-visual deepfake dataset. The main steps in creating the dataset are 1) Sourcing the real videos, 2) Processing the real videos to manipulate their transcripts, and 3) Audio and video synthesis. The deepfake generation is based on the hypothesis that changing relevant words in a transcript can lead to a change in its perception, and in particular, this can be accomplished by changing the sentiment of the transcript. Therefore, the manipulation strategy is to replace strategic words with their antonyms, which leads to a significant change in the sentiment of the statement. The data generation pipeline is illustrated in Figure~\ref{fig:data_generation}.

\noindent \textbf{Data Sourcing.} The real videos are sourced from the VoxCeleb2~\cite{chung_voxceleb2_2018} dataset, a facial video dataset with over 1 million utterance videos of over 6000 speakers. The faces in the videos are tracked and cropped with the facial detector in~\cite{king_dlib-ml_2009} at 224 224 resolution. The original dataset contains videos with different duration, spoken language, and voice loudness. Only English-speaking videos are chosen using the confidence score from the Google Speech-to-Text service. The same service generates the transcripts, which are used for manipulation.

\subsection{Data Generation}
\noindent \textbf{Transcript Manipulation.} After sourcing the real videos, the next step is to analyse a video's transcript denoted by , where  denotes word tokens and  is the number of tokens. The aim is to find the tokens to be replaced in  such that the sentiment of the transcript changes the most. In other words, to goal is to create a transcript , composed of most of the tokens of  with the exception of a few tokens being replaced. The replacement token  is selected from a set  of antonyms of . The sentiment analyzer in NLTK~\cite{bird_natural_2009} is used to estimate the sentiment value   of a transcript. For each token  in a transcript , we find the replacement as follows,

We find all the replacements in a transcript  as follows,

where  is the sentiment difference with the replacement  and  is the maximum number of replacements in the transcript. For videos shorter than 10 seconds, there is up to 1 replacement; otherwise, there are up to 2 replacements. Figure~\ref{fig:data_distribution}~(a) illustrates the change in sentiment distribution after the manipulations and Figure~\ref{fig:data_distribution}~(b) presents the histogram of , suggesting that the sentiment of most transcripts was successfully changed.

\noindent \textbf{Audio Generation.} The next step is to generate the corresponding audio in the speaker’s style. Several recent adaptive text-to-speech (TTS)  methods~\cite{jia_transfer_2018, casanova_sc-glowtts_2021, neekhara_expressive_2021} which can generate the speech style of a person who is not in the training dataset were evaluated. Based on the better performance, SV2TTS~\cite{jia_transfer_2018} is chosen as the final method for audio generation. The SV2TTS comprises three modules 1) An encoder for extracting style embedding of the reference speaker, 2) Tacotron 2~\cite{shen_natural_2018} based spectrogram generated using the replacement tokens and the speaker style embedding, and 3) WaveNet~\cite{oord_wavenet_2016} based vocoder for generating realistic audio using the spectrogram. The pre-trained SV2TTS is used for generating the fake audio segments which are later loudness normalized using the corresponding real audio neighbors.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{image/stats4.pdf}
\caption{\textbf{Summary of the proposed dataset.} (a) Distribution of sentiment scores, (b) Distribution of sentiment changes, (c) Distribution of fake segment lengths, (d) Distribution of video lengths, (e) Proportion of fake segments, and (f) Proportion of modifications.}
\label{fig:data_distribution}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{image/model5.pdf}
\caption{\textbf{Structure of the proposed method.} The video encoder uses raw video as input. The audio encoder uses spectrograms extracted from raw audio.  denotes concatenation. During inference, post-processing is applied to generate segments from the output of the fusion module. The details on different components of the method are discussed in Section~\ref{sec:proposed_method}.}
\label{fig:model}
\end{figure*}

\noindent \textbf{Video Generation.} The generated fake audio is used as an input for generating the corresponding fake video frames. Wav2Lip~\cite{prajwal_lip_2020} facial reenactment is used for this task as it has been shown to have better output quality than previous methods~\cite{jamaludin_you_2019, k_r_towards_2019}, and has better generalization and robustness to unseen scenarios. It is worth noting that newer methods that achieve better video synthesis quality are not suitable for our task. For example, AD-NeRF~\cite{guo_ad-nerf_2021} is not designed for zero-shot generation of unseen identities, and ATVGNet~\cite{chen_hierarchical_2019} reenacts the face based on a static reference image, which causes pose inconsistencies on the boundary between fake and real segments. Wav2Lip takes a reference video and target audio as input, generating an output video in which the person in the reference video speaks the target audio content with synced lips. The pre-trained Wav2Lip model is used and the generated fake video segments are up-scaled to a resolution of . In the final step, the generated fake audio and video segments are synchronized and used to replace the original audio and video segments.

Similar to~\cite{khalid_evaluation_2021} the proposed dataset includes three variations of deepfake data,

\begin{enumerate}
\item{\textbf{Fake Audio} and \textbf{Fake Video.} The audio and corresponding video are generated for replacement tokens.}
\item{\textbf{Fake Audio} and \textbf{Real Video.} Only the audio is generated for replacement tokens and the corresponding real video is length-normalized.}
\item{\textbf{Real Audio} and \textbf{Fake Video.} Only the video is generated for replacement tokens and the length of the fake video is normalized to match the real audio.}
\end{enumerate}

\subsection{Dataset Statistics}
The dataset contains 136,304 videos, of which 36,431 are completely real, and 99,873 have fake segments, with 153 unique identities. We split the dataset into 3 identity-independent subsets for training (78703 videos of 91 identities), validation (31501 videos of 31 identities), and testing (26100 videos of 31 identities). The summary of the dataset is shown in Figure~\ref{fig:data_distribution}. The total number of fake segments is 114,253, with duration in the range [0-1.6] seconds and an average length of 0.65 seconds, where 89.26\% of the segments are shorter than 1 second. The maximum video length is 20 seconds, and 69.61\% of the videos are shorter than 10 seconds. As for the modality modification types, the amount of the 4 types (\ie video-modified, audio-modified, both-modified, real) is approximately equal. In most videos (62.72\%), there is 1 fake segment, and in some videos (10.55\%), there are 2. 



\section{Proposed Method}
\label{sec:proposed_method}
The proposed method called \model{} (\modelabbr{}) is illustrated in Figure~\ref{fig:model}. The first step of the method is to extract features from the input data , where  is the video and  is the audio.

\subsection{Feature Encoders}
\noindent \textbf{Video Encoder.} The goal of the video encoder is to learn frame-level spatio-temporal features from the input video  using a 3DCNN. For that purpose, we designed the video encoder  to take the whole video  as input, where  is the number of frames,  is the number of channels, and  and  are the height and width of the frame. The output of the  are the frame-level features , where  is the features dimension.  is composed of 4 blocks, each containing multiple 3D convolutional layers with kernel size  and a final max-pooling layer.

\noindent \textbf{Audio Encoder.} The goal of the audio encoder is to learn features from the input audio  using a 2DCNN. In addition, the learned audio features are temporarily aligned with the learned video frame-level video features. The first step is to generate the spectrogram  of the audio signal in log-space, where  is the temporal dimension, and  is the length of mel-frequency cepstrum features. In the second step, we designed the audio encoder  to take the spectrogram  as input. The output of the  are the audio frame features , where  is the features dimension.  is composed of multiple 2D convolutional layers with kernel size  and a final max-pooling layer to reduce the temporal dimension  to .

\subsection{Loss Functions}  
\noindent \textbf{Contrastive Loss.} We hypothesize that content modification in one or more modalities will result in miss-synchronization between the modalities (\ie video and audio), and contrastive loss has been shown~\cite{chung_out_2017, chugh_not_2020} to be a powerful objective for similar tasks. Our method uses the audio and video features learned from real videos as positive pairs. The audio and video features learned from videos with at least one modified modality are considered negative pairs. For the positive pairs, the contrastive loss minimizes the difference between the modalities, while for negative pairs, the contrastive loss keeps that margin larger than ,



\noindent \textbf{Frame Classification Loss.} Since we have access to the frame-level features  and , we utilize the labels and train the encoders to extract powerful and robust features that capture different deepfake artifacts. For that purpose, we designed two frame-level logistic regression classifiers  and  using  and  as input. The classifiers consist of 1D convolutional layers and predict the label  as real or fake for each frame and each modality. The classifiers are trained with cross-entropy loss,



where  is the number of samples in the dataset,  is the number of frames,  is the modality (\ie audio  or video ),  specifies whether modality  is modified, and  is the label for real videos.

\noindent \textbf{Boundary Matching Loss.} The ground truth boundary maps are generated following the procedure in~\cite{lin_bmn_2019}. Given the fusion boundary map , video boundary map  and audio boundary map  predicted by the model we use mean squared error as boundary matching loss for ,  and . The fusion boundary matching loss is,

where,  is the number of samples in the dataset,  is the number of all possible proposal durations and  is the number of frames. The modality boundary matching loss is similar to the frame classification loss,


where,  is the modality (\ie video  or audio ),  specifies whether modality  is modified, and  is the ground truth boundary map for real videos.

\noindent \textbf{Overall Loss.} The overall loss is defined as follows,

where, , ,  and  are weights for different losses.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{image/fusion4.pdf}
\caption{\textbf{Structure of the fusion module.} The gray block normalizes the video and audio weights predicted from the 1D convolutional layers and applies element-wise weighted average.  denotes element-wise addition and  denotes element-wise multiplication. \textit{BM: boundary map}.}
\label{fig:fusion}
\end{figure}

\subsection{Multimodal Fusion}
The predictions of  and  are concatenated with the features  and , and used by two boundary matching layers  and ~\cite{lin_bmn_2019}. The goal is to predict the boundary maps  and  for the video and audio, where  is the number of frames and  is the maximum duration of the fake segments. The fusion module, illustrated in Figure~\ref{fig:fusion}, uses the , ,  and  as input. For the video modality, the ,  and  are used to calculate the video weights  and for the audio modality, the ,  and  are used to calculate the audio weights . In the final step, we perform element-wise weighted average and calculate the fusion boundary map prediction ,

where all operations are element-wise.

\begin{table*}
\centering
\caption{\textbf{Temporal forgery localization results on the full set (see Section~\ref{sec:experiments} for details) of the proposed dataset.} The visual-only version of the proposed method uses the output from the video boundary matching layer (see Figure~\ref{fig:model} for details), showing the performance when using only the video modality.}
\label{tab:fullset}
\scalebox{1}{
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Method} & \textbf{AP@0.5} & \textbf{AP@0.75} & \textbf{AP@0.95} & \textbf{AR@100} & \textbf{AR@50} & \textbf{AR@20} & \textbf{AR@10} \\ \hline\hline
MDS~\cite{chugh_not_2020} & 12.78 & 01.62 & 00.00 & 37.88 & 36.71 & 34.39 & 32.15 \\
AGT~\cite{nawhal_activity_2021} & 17.85 & 09.42 & 00.11 & 43.15 & 34.23 & 24.59 & 16.71 \\
BMN~\cite{lin_bmn_2019} & 24.01 & 07.61 & 00.07 & 53.26 & 41.24 & 31.60 & 26.93 \\
BMN (I3D) & 10.56 & 01.66 & 00.00 & 48.49 & 44.39 & 37.13 & 31.55 \\
AVFusion~\cite{bagchi_hear_2021} & 65.38 & 23.89 & 00.11 & 62.98 & 59.26 & 54.80 & 52.11 \\
\hline
\modelabbr{}~(visual-only)~(Ours) & 58.55 & 28.60 & 00.16 & 62.49 & 58.77 & 53.86 & 50.29 \\
\modelabbr{}~(multimodal)~(Ours) & \textbf{76.90} & \textbf{38.50} & \textbf{00.25} & \textbf{66.90} & \textbf{64.08} & \textbf{60.77} & \textbf{58.42} \\
\hline
\end{tabular}}
\end{table*}

\begin{table*}
\centering
\caption{\textbf{Temporal forgery localization results on the subset (see Section~\ref{sec:experiments} for details) of the proposed dataset.} The visual-only version of the proposed method uses the output from the video boundary matching layer (see Figure~\ref{fig:model} for details), showing the performance when using only the video modality.}
\label{tab:subset}
\scalebox{1}{
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Method} & \textbf{AP@0.5} & \textbf{AP@0.75} & \textbf{AP@0.95} & \textbf{AR@100} & \textbf{AR@50} & \textbf{AR@20} & \textbf{AR@10} \\ \hline\hline
MDS~\cite{chugh_not_2020} & 23.43 & 03.48 & 00.00 & 58.53 & 56.68 & 53.16 & 49.67 \\
AGT~\cite{nawhal_activity_2021} & 15.69 & 10.69 & 00.15 & 49.11 & 40.31 & 31.70 & 23.13 \\
BMN~\cite{lin_bmn_2019} & 32.32 & 11.38 & 00.14 & 59.69 & 48.17 & 39.01 & 34.17 \\
BMN (I3D) & 28.10 & 05.47 & 00.01 & 55.49 & 54.44 & 52.14 & 47.72 \\
AVFusion~\cite{bagchi_hear_2021} & 62.01 & 22.77 & 00.11 & 61.98 & 58.08 & 53.31 & 50.52 \\
\hline
\modelabbr{}~(visual-only)~(Ours) & 83.55 & 41.88 & 00.24 & 65.79 & 62.30 & 57.95 & 55.34 \\
\modelabbr{}~(multimodal)~(Ours) & \textbf{85.20} & \textbf{47.06} & \textbf{00.29} & \textbf{67.34} & \textbf{64.52} & \textbf{61.19} & \textbf{59.32} \\
\hline
\end{tabular}}
\end{table*}    

\subsection{Inference}
During inference, the model uses the video and audio as input and generates a fusion boundary map . The boundary map represents the confidence for all proposals in the video and is very dense (\ie there are many duplicated proposals). Similar to BSN~\cite{lin_bsn_2018}, we utilize post-processing with Soft Non-Maximum Suppression (S-NMS)~\cite{bodla_soft-nms_2017} to eliminate the duplicated proposals.

\section{Experiments}
\label{sec:experiments}
We have performed extensive benchmarking of the proposed dataset via several state-of-the-art methods including, BMN~\cite{lin_bmn_2019}, AGT~\cite{nawhal_activity_2021}, AVFusion~\cite{bagchi_hear_2021}, and MDS~\cite{chugh_not_2020}. Apart from our proposed dataset, we also validate our method for classification on DFDC~\cite{dolhansky_deepfake_2020} dataset.

\noindent \textbf{Dataset Preparation and Evaluation Protocol.} To compare with visual-only methods, we prepare a subset of the test set where the audio-only modified data is removed which is denoted as \textit{subset}. The original test set is denoted as \textit{full set} in the experiments. Unlike temporal action localization methods~\cite{liu_multi-shot_2021, nawhal_activity_2021} that are using only average precision, we follow the protocol proposed in  ForgeryNet~\cite{he_forgerynet_2021} and use both average precision (AP) and average recall (AR) as the evaluation metrics for the quantitative comparison. For AP, we follow the protocol of ActivityNet~\cite{caba_heilbron_activitynet_2015} to set the IoU thresholds to 0.5, 0.75 and 0.95. For AR, as the number of fake segments is small, we set the number of proposals to 100, 50, 20 and 10 with the IoU thresholds [0.5:0.05:0.95]. Our method can also be used for deepfake detection (\ie classification) task. We use area under the curve (AUC) for evaluation of the deepfake classification.

\noindent \textbf{Implementation Details.} The proposed method is implemented in PyTorch~\cite{paszke_pytorch_2019}. For hyperparameters, we set  = ,  = ,  = ,  =  and  = . For comparison, we trained BMN~\cite{lin_bmn_2019}, AGT~\cite{nawhal_activity_2021}, AVFusion~\cite{bagchi_hear_2021} and MDS~\cite{chugh_not_2020} for temporal forgery localization task. In addition, to evaluate the usefulness of the proposed method, we compare with MDS, EfficientViT~\cite{coccomini_combining_2022} and other methods on classification task. We followed the original settings for BMN, AGT, MDS and EfficientViT, and used encoding concatenation fusion for AVFusion. For the methods that require pre-trained features, we trained them end-to-end with trainable encoder. For comparison, we also trained BMN with I3D features~\cite{carreira_quo_2017} (\ie fixed encoder). For the models which require S-NMS~\cite{bodla_soft-nms_2017} post-processing, we used the validation set to search for optimal parameters for post-processing. Final evaluation and results are based on the test set. For DFDC, we consider the whole fake video as one fake segment. For evaluation, we used 2 methods to generate the classification output for our method 1) Using the highest confidence of the predicted segments as the confidence of the video being fake and 2) Training a MLP classifier using the confidences of predicted segments. We chose evaluation method 1) for our dataset and method 2) for DFDC based on performance.

\begin{table*}
\centering
\caption{\textbf{Temporal forgery localization results on the full set (see Section~\ref{sec:experiments} for details) of the proposed dataset.} The contribution of different loss terms in the proposed method (see Section~\ref{sec:proposed_method} for details).}
\label{tab:losses}
\scalebox{1}{
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Loss Function} & \textbf{AP@0.5} & \textbf{AP@0.75} & \textbf{AP@0.95} & \textbf{AR@100} & \textbf{AR@50} & \textbf{AR@20} & \textbf{AR@10} \\ 
\hline\hline
 & 40.50 & 29.74 & 00.13 & 60.51 & 60.50 & 60.47 & 59.90 \\
 & 40.92 & 31.23 & 00.74 & 64.71 & 64.71 & 64.36 & 62.79 \\
 & 53.16 & 11.91 & 00.02 & 53.99 & 50.94 & 47.74 & 45.55 \\
 & 54.70 & 15.50 & 00.04 & 56.64 & 53.57 & 49.46 & 45.85 \\
 & 76.50 & \textbf{39.92} & 00.18 & 66.69 & 63.71 & 60.07 & 57.76 \\
 & \textbf{76.90} & 38.50 & \textbf{00.25} & \textbf{66.90} & \textbf{64.08} & \textbf{60.77} & \textbf{58.42} \\
\hline
\end{tabular}}
\end{table*}
    
\section{Results}
\label{sec:results}
\noindent \textbf{Temporal Forgery Localization.} We compare our method on the full set of the proposed dataset with the latest methods for temporal action localization and deepfake detection. From Table~\ref{tab:fullset}, our method achieves the best performance, which is 76.9 for AP@0.5 and 66.9 for AR@100. Unlike temporal action localization datasets, in our dataset there is a single label for the fake segments, so it is reasonable that the AP score is relatively high. The multimodal MDS method is not designed for temporal forgery localization tasks and predicts only fixed length segments (\ie cannot predict the precise boundaries), hence the scores for that method are low. As for AGT and BMN, the scores are low because they are visual-only unimodal methods and cannot detect the fake segments in videos where only the audio is modified. We also evaluated the performance of our visual-only unimodal method, which shows worse results than the multimodal version and AVFusion. In addition, the results show that when the video encoder is trained with data from the proposed dataset, BMN performs significantly better than using I3D features. We also evaluated the same methods on the subset of the proposed dataset. From Table~\ref{tab:subset}, the performance of the visual-only methods is improved, and for our method, the visual-only score improves from 58.55~(AP@0.5) to 83.55~(AP@0.5) and the margin between the unimodal and multimodal versions is decreased from 18.35~(AP@0.5) to 1.65~(AP@0.5). Overall, our method still ranks first, which demonstrates it's superior performance for temporal forgery detection.

\noindent \textbf{Deepfake Classification.} We also compare our method with previous deepfake detection methods on the full set of the proposed dataset and a subset of DFDC. On our dataset, our method (\textbf{0.990}) outperforms F3Net~\cite{qian_thinking_2020} (0.520), MDS (0.828) and EfficientViT (0.965). As for the subset of DFDC, the performance of our method (\textbf{0.846}) is better than previous methods such as Meso4~\cite{afchar_mesonet_2018} (0.753), FWA~\cite{li_exposing_2019} (0.727) and \cite{mittal_emotions_2020} (0.844) and is close to MDS (0.916). It is worth noting that, our method is not designed and trained for classification task with classification loss. It is trained for temporal forgery localization and then the segment outputs are summarized as a whole video label prediction. Therefore, the performance of our method on DFDC drops as compared to the state-of-the-art classification method MDS. On the other hand, previous deepfake detection methods assume that fake videos are entirely fake, so their performance (\eg the frame-based approach of F3Net) is reduced on our dataset. In summary, our method still performs well on classification task and has potential to reach the state-of-the-art performance.





\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{image/feature_distributions3.pdf}
\caption{\textbf{Feature distribution in PCA subspace.} Each point is the features of a video frame.}
\label{fig:features}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{image/outputs.pdf}
\caption{\textbf{Visualization of boundary map outputs.} The first column illustrates the modality-wise boundary map outputs for a real video. The rest of the columns illustrate the modality-wise boundary map outputs for the corresponding fake videos. \textit{GT: ground truth} and \textit{mod: modified}.} 
\label{fig:outputs}
\end{figure*}

\noindent \textbf{Impact of Loss Functions.} From Table~\ref{tab:losses}, all loss terms have positive effect on the performance of the proposed model. The results suggest that the frame classification loss contributes the most to the method performance. 

With the frame-level labels supervision, the encoders are trained to have a better capacity to extract features relevant to deepfake artifacts. The results also demonstrate the positive contribution of the boundary matching module. To examine the contribution of proposed boundary matching mechanism we aggregate the frame-level results following the algorithm proposed in~\cite{zhao_temporal_2017}. Figure~\ref{fig:features} provides a visualization of the distributions of video features  for 10 videos in the dataset. From the plots, with frame classification loss and contrastive loss added, the features are more separable, which contributes to the temporal localization and improves the performance.

\noindent \textbf{Complexity Analysis.} The proposed model is trained on a single RTX3090 GPU using the proposed large-scale dataset for 120 hours with batch size 4. Table~\ref{tab:complexity} provides comparison of the proposed method and other related methods in terms of the number of parameters. The results demonstrate that the proposed method has the potential to scale up for better performance.

\begin{table}[t]
\centering
\caption{\textbf{Method comparison in terms of number of parameters.} \textit{\modelabbr{}: the proposed method.}}
\label{tab:complexity}
\scalebox{0.92}{
\begin{tabular}{|c|cccc|}
\hline
\textbf{Type} & \multicolumn{2}{c|}{\textbf{Classification}} & \multicolumn{2}{c|}{\textbf{Localization}} \\
\hline\hline
\textbf{Method} & \multicolumn{1}{c|}{\textbf{MDS~\cite{chugh_not_2020}}} &  \multicolumn{1}{c|}{\textbf{EffiencientViT~\cite{coccomini_combining_2022}}} & \multicolumn{1}{c|}{\textbf{BMN~\cite{lin_bmn_2019}}} & \textbf{\modelabbr{}} \\
\hline
\textbf{\#Params} & \multicolumn{1}{c|}{122.8M} & \multicolumn{1}{c|}{109.4M} & \multicolumn{1}{c|}{50.3M} & \multicolumn{1}{c|}{5.5M} \\
\hline
\end{tabular}}
\end{table}

\noindent \textbf{Qualitative Analysis.} We selected 1 real video and the 3 corresponding fake videos and visualized the boundary map outputs ,  and  in Figure~\ref{fig:outputs}. The results illustrates that 1) The video output captures the fake segments when the video modality is modified, 2) The audio output captures the fake segments when the audio modality is manipulated, and 3) Regardless of whether the audio or video modality is modified, the fusion output captures the correct information from audio and visual outputs demonstrating the effectiveness of the fusion module.

\noindent \textbf{Failure Analysis.} The output of the proposed method can be noisy for cases that contain very short video manipulations ( sec) and the corresponding real audio. For such short video-only manipulations, if the visual transition from real to fake and then back to real is smooth, it may lead to noisy output.

\section{Conclusion} This work introduces and investigates a novel problem related content-driven deepfake generation and detection. To this end, we propose a new dataset in which the audio and video are modified at specific locations based on the change in sentiment of the content. We also propose a new method for temporal forgery localization in such partially modified videos. The conducted experiments show that our method achieves better performance than previous relevant state-of-the-art methods.

\noindent \textbf{Ethical Concerns.} 
The proposed dataset potentially might have a negative social impact. Since the individuals in the dataset are celebrities, the content in the dataset may be used for unethical purposes such as making fake rumours. Also, the dataset generation pipeline can be used to create fake videos. To encounter the potential negative impact of our work, we prepared a license for public usage of the dataset and proposed the method.

\noindent \textbf{Limitations.} This work has some limitations 1) The audio reenactment method used in the dataset does not always generate the reference style, 2) The resolution of the dataset is constrained on the basis of source videos and 3) The high score of classification results indicates the necessity of improving the video reenactment method. 



\noindent \textbf{Future Work.} Major improvement in the future will be increasing the dataset with new token insertion, substitution and deletion of existing tokens and converting statements into questions.




















\bibliographystyle{ieeetr}
\bibliography{paper}


\end{document}
