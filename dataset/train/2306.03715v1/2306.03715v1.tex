

\documentclass{article}


\usepackage{bbding}

\usepackage{hyperref}
\usepackage{url}

\usepackage{framed}
\usepackage{color}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{color, colortbl}
\definecolor{greyC}{RGB}{180,180,180}
\definecolor{greyL}{RGB}{235,235,235}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{makecell}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithmic,algorithm}

\usepackage{soul}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}


\usepackage{threeparttable}
\usepackage{subfigure}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{natbib}
\usepackage{multirow}

\usepackage{wrapfig}  


\usepackage{framed}
\usepackage{color}
\definecolor{shadecolor}{rgb}{0.92,0.92,0.92}


\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand{\T}{{\hspace{-0.25ex}\top\hspace{-0.25ex}}}
\newcommand{\ST}{\mathrm{s.t.}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\bx}{{x}}
\newcommand{\bxprime}{{x}'}
\newcommand{\bxtidle}{\tilde{{x}}}
\newcommand{\epsball}{\mathcal{B}_\epsilon}
\newcommand{\xadv}{\tilde{{x}}}
\newcommand{\yadv}{\tilde{y}}

\usepackage{hyperref}





\usepackage[accepted]{icml2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}



\icmltitlerunning{Unleashing Mask: Explore the Intrinsic Out-of-Distribution Detection Capability}

\begin{document}

\twocolumn[
\icmltitle{Unleashing Mask: Explore the Intrinsic Out-of-Distribution Detection Capability}







\begin{icmlauthorlist}
\icmlauthor{Jianing Zhu}{hkbu}
\icmlauthor{Hengzhuang Li}{hkbu}
\icmlauthor{Jiangchao Yao}{sjtu,lab}
\icmlauthor{Tongliang Liu}{syd,sydc}
\icmlauthor{Jianliang Xu}{hkbu}
\icmlauthor{Bo Han}{hkbu}
\end{icmlauthorlist}

\icmlaffiliation{hkbu}{Department of Computer Science, Hong Kong Baptist University}
\icmlaffiliation{sjtu}{CMIC, Shanghai Jiao Tong University}
\icmlaffiliation{lab}{Shanghai AI Laboratory}
\icmlaffiliation{syd}{Mohamed bin Zayed University of Artificial Intelligence}
\icmlaffiliation{sydc}{Sydney AI Centre, The University of Sydney}

\icmlcorrespondingauthor{Bo Han}{bhanml@comp.hkbu.edu.hk}
\icmlcorrespondingauthor{Jiangchao Yao}{Sunarker@sjtu.edu.cn}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Out-of-distribution (OOD) detection is an indispensable aspect of secure AI when deploying machine learning models in real-world applications. Previous paradigms either explore better scoring functions or utilize the knowledge of outliers to equip the models with the ability of OOD detection. However, few of them pay attention to the intrinsic OOD detection capability of the given model. In this work, we generally discover the existence of an intermediate stage of a model trained on in-distribution (ID) data having higher OOD detection performance than that of its final stage across different settings, and further identify one critical data-level attribution to be learning with the \textit{atypical samples}. Based on such insights, we propose a novel method, \textit{Unleashing Mask}, which aims to restore the OOD discriminative capabilities of the well-trained model with ID data. Our method utilizes a mask to figure out the memorized \textit{atypical samples}, and then finetune the model or prune it with the introduced mask to forget them. Extensive experiments and analysis demonstrate the effectiveness of our method. The code is available at: \url{https://github.com/tmlr-group/Unleashing-Mask}.
\end{abstract}

\section{Introduction}



Out-of-distribution (OOD) detection has drawn increasing attention when deploying machine learning models into the open-world scenarios~\citep{Nguyen_2015_CVPR,LeeLLS18, yang2021generalized}. Since the test samples can naturally arise from a label-different distribution, identifying OOD inputs from in-distribution (ID) data is important, especially for those safety-critical applications like autonomous driving and medical intelligence. Previous studies focus on designing a series of scoring functions~\citep{hendrycks17baseline,LiangLS18,liu2020energy,SunM0L22} for OOD uncertainty estimation or fine-tuning with auxiliary outlier data to better distinguish the OOD inputs~\citep{hendrycks2018deep,MohseniPYW20,SehwagCM21}.



Despite the promising results achieved by previous methods~\citep{hendrycks17baseline,hendrycks2018deep,liu2020energy,ming2022poem}, limited attention is paid to considering whether the given well-trained model is the most appropriate basis for OOD detection. In general, models deployed for various applications have different original targets (e.g., multi-class classification~\citep{goodfellow2016deep}) instead of OOD detection~\citep{Nguyen_2015_CVPR}. However, most representative score functions, e.g., MSP~\citep{hendrycks2018deep}, ODIN~\citep{LiangLS18}, and Energy~\citep{liu2020energy}, uniformly leverage the given models for OOD detection~\citep{yang2021generalized}. The above target-oriented discrepancy naturally motivates the following critical question: \textit{does the given well-trained model have the optimal OOD discriminative capability?} If not, \textit{how can we find a more appropriate counterpart for OOD detection?}



In this work, we start by revealing an interesting empirical observation, i.e., there always exists a historical training stage where the model has a higher OOD detection performance than the final well-trained one (as shown in Figure~\ref{fig: motivation_1}), spanning among different OOD/ID datasets~\citep{netzer2011reading_SVHN,van2018inaturalist} under different learning rate schedules~\citep{LoshchilovH17} and model structures~\citep{huang2017densely,zagoruyko2016wide}. It shows the inconsistency between gaining better OOD discriminative capability~\citep{Nguyen_2015_CVPR} and pursuing better performance on ID data during training.
Through the in-depth analysis from various perspectives (as illustrated in Figure 2), we figure out one possible attribution at the data level is memorizing the \textit{atypical samples} (compared with others at the semantic level) that are hard to generalize for the model. Seeking zero training error on those samples leads the model more confident in the unseen OOD inputs.



\begin{figure*}[t!]
\begin{center}
    \hspace{-0.10in}
    \subfigure[Curves of FPR95 based on Energy score]{
\includegraphics[scale=0.17]{fig1a_fprcurves_20X10.pdf}
    \label{fig1:a}
    }
   \hspace{0.15in}
    \subfigure[Diff. LR Schedules]{
    \includegraphics[scale=0.17]{fig1b_lr_cmp.pdf}
    \label{fig1:b}
    }
\subfigure[Diff. Model Structures]{
    \includegraphics[scale=0.17]{fig1c_arch_cmp.pdf}
    \label{fig1:c}
    }
\end{center}
\vspace{-5mm}
\caption{\textbf{Critical reveal of the intermediate stage with better OOD detection performance across various setups:} (a) the curves of FPR95 (false positive rate of OOD examples when the true positive rate of ID examples is at 95\%) based on Energy score~\citep{liu2020energy} across three different OOD datasets during the training on CIFAR-10 dataset; (b) comparison of best/last checkpoints for OOD detection under different lr schedules on CIFAR-10; (c) comparison of best/last checkpoints for OOD detection under different model structures on CIFAR-10/CIFAR-100. The results are obtained after multiple runs, and we leave other setup details to Section~\ref{sec:exp_part1} and Appendix~\ref{app:additional_exp_setup}.
}
\label{fig: motivation_1}
\vspace{-4mm}
\end{figure*}



The above analysis inspires us to propose a new method, namely, \textit{Unleashing Mask} (UM), to excavate the overlaid detection capability of a well-trained given model by alleviating the memorization of those atypical samples (as illustrated in Figure~\ref{fig:method}) of ID data. In general, we aim to backtrack its previous stage with better OOD discriminative capabilities. To achieve this target, there are two essential issues: (1) \textit{the model that is well-trained on ID data has already memorized some atypical samples}; (2) \textit{how to forget those memorized atypical samples considering the given model?} Accordingly, our proposed UM contains two parts utilizing different insights to address the two problems. First, as atypical samples are more sensitive to the change of model parameters, we initialize a mask with the specific cutting rate to mine these samples with constructed parameter discrepancy. Second, with the loss reference estimated by the mask, we conduct the constrained gradient ascent for model forgetting (i.e., Eq.~(\ref{eq:obj})). It will encourage the model to finally stabilize around the optimal stage. To avoid severe sacrifices of the original task performance on ID data, we further propose \textit{UM Adopts Pruning} (UMAP) which tunes on the introduced mask with the newly designed objective.



We conduct extensive experiments (in Section~\ref{sec:exp} and Appendixes~\ref{app:additional_exp_setup} to~\ref{app:eff_um}) to present the working mechanism of our proposed methods. We have verified the effectiveness with a series of OOD detection benchmarks mainly on two common ID datasets, i.e., CIFAR-10 and CIFAR-100. Under the various evaluations, our UM, as well as UMAP, can indeed excavate the better OOD discriminative capability of the well-trained given models and the averaged FPR95 can be reduced by a significant margin. Finally, a range of ablation studies, verification on the ImageNet pretrained model, and further discussions from both empirical and theoretical views are provided. Our main contributions are as follows,
\begin{itemize}
    \item Conceptually, we explore the OOD detection performance via a new perspective, i.e., backtracking the initial model training phase without regularizing by any auxiliary outliers, different from most previous works that start with the well-trained model on ID data.
    \item Empirically, we reveal the potential OOD discriminative capability of the well-trained model, and figure out one data-level attribution of concealing it during original training is memorizing the atypical samples.
    \item Technically, we propose a novel \textit{Unleashing Mask} (UM) and its practical variant UMAP, which utilizes the newly designed forgetting objective with ID data to excavate the intrinsic OOD detection capability. 
\item Experimentally, we conduct extensive explorations to verify the overall effectiveness of our method in improving OOD detection performance, and perform various ablations to provide a thorough understanding. \end{itemize}






\section{Preliminaries}

We consider multi-class classification as the original training task~\citep{Nguyen_2015_CVPR}, where  denotes the input space and  denotes the label space. In practical, a reliable classifier should be able to figure out the OOD input, which can be considered as a binary classification problem. Given , the distribution over , we consider  as the marginal distribution of  for , namely, the distribution of ID data. At test time, the environment can present a distribution  over  of OOD data. In general, the OOD distribution  is defined as an irrelevant distribution of which the label set has no intersection with ~\cite{yang2021generalized} and thus should not be predicted by the model. A decision can be made with the threshold :

Building upon the model  trained on ID data with the logit outputs, the goal of decision is to utilize the scoring function  to distinguish the inputs of  from that of  by . Typically, if the score value is larger than the threshold , the associated input  is classified as ID and vice versa. We consider several representative scoring functions designed for OOD detection, e.g., MSP~\citep{hendrycks17baseline}, ODIN~\citep{LiangLS18}, and Energy~\citep{liu2020energy}. More detailed definitions and implementation are provided in Appendix~\ref{app:baseline_info}.



To mitigate the issue of over-confident predictions for some OOD data~\citep{hendrycks17baseline,liu2020energy},  recent works~\citep{hendrycks2018deep, Tack20CSI} utilize the auxiliary unlabeled dataset to regularize the model behavior. Among them, one representative baseline is outlier exposure (OE)~\citep{hendrycks2018deep}. OE can further improve the detection performance by making the model  finetuned from a surrogate OOD distribution , and its corresponding learning objective is defined as follows,

where  is the balancing parameter,   is the Cross-Entropy (CE) loss, and  is the Kullback-Leibler divergence to the uniform distribution, which can be written as , where  denotes the -th element of a softmax output. The OE loss  is designed for model regularization, making the model learn from surrogate OOD inputs to return low-confident predictions~\citep{hendrycks2018deep}. 


Although previous works show promising results via designing scoring functions or regularizing models with different auxiliary outlier data, few of them investigated or excavated the original discriminative capability of the well-trained model using ID data. In this work, we introduce the layer-wise mask ~\citep{han2015deep,ramanujan2020s} to mine the atypical samples that are memorized by the model. Accordingly, the decision can be rewritten as , and the output of a masked model is defined as . 
















\begin{figure*}[t!]
\begin{center}
\subfigure[Training/Testing Loss and Accuracy]{
    \includegraphics[scale=0.124]{fig2a_loss_curve.pdf}
    \includegraphics[scale=0.124]{fig2b_acc_curves.pdf}
    \label{fig2:a}
    }
    \subfigure[ID and OOD Distributions at Epoch 60/100]{
    \includegraphics[scale=0.124]{fig2f_epoch60_distribution.pdf}
    \includegraphics[scale=0.124]{fig2g_epoch100_distribution.pdf}
    \label{fig2:b}
    }
    \subfigure[ID Distributions]{
    \includegraphics[scale=0.124]{fig2c_2d_distribution.pdf}
    \label{fig2:c}
    }
\\
    \subfigure[Wrongly/Correctly Classified Data at Epoch 60]{
    \includegraphics[scale=0.07]{fig2c_easy_hard_samples_2X5_ps.pdf}
\label{fig2:d}
    }
    \subfigure[TSNE Visualization at Epoch 60/100]{
    \includegraphics[scale=0.1512]{fig2e_tsne_60_kde_v0.pdf}
    \includegraphics[scale=0.1512]{fig2e_tsne_100_kde_v0.pdf}
    \label{fig2:e}
    }
    \subfigure[Diff. Effects]{
    \includegraphics[scale=0.124]{fig2f_typical_atypical.pdf}
    \label{fig2:f}
    }
\end{center}
\vspace{-4mm}
\caption{\textbf{Delve into the data-level attribution of the phenomenon with the original multi-classification on CIFAR-10:}  
(a) training/testing loss and accuracy on ID data; (b) comparison of ID/OOD distributions based on Energy score at Epoch 60/100 (c) scatter plot of wrongly/correctly classified samples at Epoch 60 using Margin value and Energy score (d) visualization of wrongly/correctly classified samples at Epoch 60; (e) TSNE visualization of the feature embedding on ID/OOD data at Epoch 60/100. (f) Effects on OOD detection of tuning with those identified typical/atypical samples, more detailed setup, and results can be referred to in Appendix~\ref{app:exp_typical_atypical}.
Through comparison from various perspectives, we find that achieving a reasonably small loss value (at round Epoch 60) on ID data is enough for OOD detection. However, continually optimizing on those atypical samples (e.g., wrongly classified in (d)) may impair OOD detection. }
\label{fig: motivation_2}
\vspace{-4mm}
\end{figure*}

\section{Proposed Method: Unleashing Mask}

In this section, we introduce our new method, i.e., \textit{Unleashing Mask} (UM), to reveal the potential OOD discriminative capability of the well-trained model. First, we present and discuss the important observation that inspires our methods (Section~\ref{sec:method_part1}). Second, we provide the insights behind the two critical parts of our UM (Section~\ref{sec:method_part2}). Lastly, we introduce the overall framework and its learning objective, as well as a practical variant of UM, i.e., UMAP (Section~\ref{sec:method_part3}). 

\subsection{Overlaid OOD Detection Capability}
\label{sec:method_part1}

First, we present the phenomenon of the inconsistency between pursuing better OOD discriminative capability and smaller training errors during the original task. 
Empirically, as shown in Figure~\ref{fig: motivation_1}, we trace the OOD detection performance during the model training after multiple runs of the experiments. Across three different OOD datasets in Figure~\ref{fig1:a}, we can observe the existence of a better detection performance using the index of FPR95 metric based on the Energy~\citep{liu2020energy} score. The generality has also been demonstrated under different learning schedules, model structures, and ID datasets in Figures~\ref{fig1:b} and~\ref{fig1:c}. Without any auxiliary outliers, it motivates us to explore the underlying mechanism of the training process with ID data.

We further delve into the learning dynamics from various perspectives in Figure~\ref{fig: motivation_2}, and we reveal the critical data-level attribution for the OOD discriminative capability. In Figure~\ref{fig2:a}, we find that the training loss has reached a reasonably small value\footnote{Note that it is not the conventional overfitting~\citep{goodfellow2016deep} as the testing loss is still decreasing. In Section~\ref{sec:exp_part3} and Appendix~\ref{app:overfitting_comp}, we provide both the empirical comparison of some targeted strategies and the conceptual comparison of them.} at Epoch 60 where its detection performance achieves a satisfactory level. However, if we further minimize the training loss, the trend of the FPR95 curve shows almost the opposite direction with both training and testing loss or accuracy (see Figures~\ref{fig1:a} and~\ref{fig2:a}). The comparison of the ID/OOD distributions is presented in Figure~\ref{fig2:b}. To be specific, the statics of the two distributions indicate that the gap between the ID and OOD data gets narrow as their overlap grows along with the training. After Epoch 60, although the model becomes more confident on ID data which satisfies a part of the calibration target~\citep{hendrycks2019using}, its predictions on the OOD data also become more confident which is unexpected. Using the margin value defined in logit space (see Eq.~(\ref{eq:margin})), we gather the statical with Energy score in Figure~\ref{fig2:c}. The misclassified samples are found to be close to the decision boundary and have a high uncertainty level in model prediction. Accordingly, we extract those samples that were learned by the model at this period. As shown in Figures~\ref{fig2:d},~\ref{fig2:e} and~\ref{fig2:f}, the misclassified samples learned after Epoch 60 present much atypical semantic features, which results in more diverse feature embedding and may impair OOD detection. As deep neural networks tend to first learn the data with typical features~\citep{arpit2017closer}, we attribute the inconsistent trend to memorizing those atypical data at the later stage. 



\subsection{Unleashing the Potential Discriminative Power}
\label{sec:method_part2}

In general, the models that are developed for the original classification tasks are always seeking better performance (e.g., higher testing accuracy and lower training loss) in practice. However, the inconsistent trend revealed before provides us the possibility to unleash the potential detection power only considering the ID data in training. To this end, we have two important issues that need to address: (1) \textit{the well-trained model may have already memorized some atypical samples which cannot be figured out}; (2) \textit{how to forget those atypical samples considering the given model?} 

\paragraph{Atypical mining with constructed discrepancy.}  
As shown Figures~\ref{fig2:a} and~\ref{fig2:b}, the training statics provide limited information to accurately differentiate the stage that learns on typical or atypical data. We thus explore to construct the parameter discrepancy to mine the atypical samples from a well-trained given model in the light of the learning dynamics~\citep{goodfellow2016deep, arpit2017closer} of deep neural networks and the model uncertainty representation~\citep{gal2016dropout}. Specifically, we employ a randomly initialized layer-wise mask which applied to all layers. It is consistent with the mask generation in the conventional pruning pipeline~\citep{han2015deep}. In Figure~\ref{fig:method}, we provide empirical evidence to show that we can figure out atypical samples by a certain mask ratio , through which we can gradually mine the model stage that misclassifies atypical samples. We provide more discussion about the underlying intuition of masking in Appendix~\ref{app:atypical_mining}.

\paragraph{Model forgetting with gradient ascent.}
As the training loss achieves zero at the final stage of the given model, we need extra optimization signals to forget those memorized atypical samples. Considering the previous consistent trend before the potential optimal stage (e.g., before Epoch 60 in Figure~\ref{fig1:a}), the optimization signal also needs to control the model update not to be too greedy to drop the discriminative features that can be utilized for OOD detection. Starting with the well-trained given model, we can employ the gradient ascent~\citep{sorg2010reward, ishida2020we} to forget the targeted samples, while the tuning phase should also prevent further updates if it achieves the expected stage. As for another implementation choice, e.g., retraining the model from scratch for our targets, we discuss it in Appendix~\ref{app:exp_less_epochs}.


\begin{figure*}[t!]
\begin{center}
    \hspace{-0.20in}
\includegraphics[scale=0.15]{UM_method_long_long.pdf}
\end{center}
\vspace{-3mm}
\caption{\textbf{Overview of Unleashing Mask:} Given a well-trained model, we initialize a mask for mining the atypical samples that are sensitive to the changes in model parameters. Then we finetune the original model or adopt pruning with the estimated forgetting threshold, i.e., the loss value estimated by the UM. The final model can serve as the base of those representative score functions to utilize the discriminative features and also as a new initialization of outlier exposure methods. In addition, we also present examples of misclassified samples in ID data after masking the original well-trained model, and loss value using the masked outputs w.r.t. different mask ratios.
}
\label{fig:method}
\vspace{-4mm}
\end{figure*}

\subsection{Method Realization}
\label{sec:method_part3}

Based on previous insights, we present our overall framework and the learning objective of the proposed UM and UMAP for OOD detection. Lastly, we discuss their compatibility with either the fundamental scoring functions or the outlier exposure approaches utilizing auxiliary outliers.

\paragraph{Framework.} As illustrated in Figure~\ref{fig:method}, our framework consists of two critical components for uncovering the intrinsic OOD detection capability: (1) the initialized mask with a specific masking rate for constructing the output discrepancy with the original model; (2) the subsequent adjustment for alleviating the memorization of atypical samples. The overall workflow starts with estimating the loss value of misclassifying those atypical samples and then conducts tuning on the model or the masked output to forget them.
\paragraph{Forgetting via Unleashing Mask (UM).} Based on previous insights, we introduce the forgetting objective as,

where  is the layer-wise mask with the masking rate ,  is the CE loss,  is the averaged CE loss over the ID training data,  indicates the computation for absolute value and  denotes the masked output of the fixed pretrained model that is used to estimate the loss constraint for the learning objective of forgetting. The value of   would be constant during the whole finetuning process. Concretely, the well-trained model will start to optimize itself again if it memorizes the atypical samples and achieves almost zero loss value. We provide a positive gradient signal when the current loss value is lower than the estimated one and vice versa. The model is expected to finally stabilize around the stage that can forget those atypical samples. To be more specific, for a mini-batch of ID samples, they are forwarded to the (pre-trained) model and the loss is automatically computed in Eq.~\eqref{eq:obj}. Based on our introduced layer-wise mask, the atypical samples would be easier to induce large loss values than the rest, and will be forced to be wrongly classified in the end-to-end optimization, in which atypical samples are forgotten without being identified.

\paragraph{Unleashing Mask Adopts Pruning (UMAP).}  Considering the potential negative effect on the original task performance when conducting tuning for forgetting, we further propose a variant of UM Adopts Pruning, i.e., UMAP, to conduct tuning based on the masked output (e.g., replace  to  in Eq~\ref{eq:obj}) using a functionally different mask  with its pruning rate  as follows,
\label{eq:obj_umap}
Different from the objective of UM (i.e., Eq \ref{eq:obj}) that minimizes the loss value over the model parameter, the objective of UMAP minimizes the loss over the mask  to achieve the target of forgetting atypical samples. UMAP provides an extra mask to restore the detection capacity but doesn't affect the model parameter for the inference on original tasks, indicating that UMAP is a more practical choice in real-world applications (as empirically verified in our experiments like Table~\ref{tab:my_label}). We present the algorithms of UM (in Algorithm~\ref{alg:um}) and UMAP (in Algorithm~\ref{alg:umap}) in Appendix~\ref{app:algo_realization}.

\paragraph{Compatible with other methods.} As we explore the original OOD detection capability of the well-trained model, it is orthogonal and compatible with those promising methods that equip the given model with better detection ability. To be specific, through our proposed methods, we reveal the overlaid OOD detection capability by tuning the original model toward its intermediate training stage. 
The discriminative feature learned at that stage can be utilized by different scoring functions~\citep{huang2021importance,liu2020energy,sun2022dice}, like ODIN~\citep{LiangLS18} adopted in Figure~\ref{fig4:c}. For those methods~\citep{hendrycks2019using,liu2020energy, ming2022poem} utilizing the auxiliary outliers to regularize the model, our finetuned model obtained by UM and UMAP can also serve as their starting point or adjustment. As our method does not require any auxiliary outlier data to be involved in training, adjusting the model using ID data during its developing phase is practical.




\section{Experiments}
\label{sec:exp}

In this section, we present the performance comparison of the proposed method in the OOD detection scenario. Specifically, we verify the effectiveness of our UM and UMAP with two mainstreams of OOD detection approaches: (i) fundamental scoring function methods; (ii) outlier exposure methods involving auxiliary samples. To better understand our proposed method, we further conduct various explorations on the ablation study and provide the corresponding discussion on each sub-aspect considered in our work. More details and additional results are presented in Appendix~\ref{app:additional_exp_results}.








\begin{table*}[t!]
    \caption{Main Results (). Comparison with competitive OOD detection baselines. (averaged by multiple trials)}
\centering
    \footnotesize
    \renewcommand\arraystretch{0.99}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|ccccc}
        \toprule[1.5pt]
         &  Method & AUROC & AUPR & FPR95 & ID-ACC & w./w.o  \\
        \midrule[0.6pt]
        \multirow{11}*{\textbf{CIFAR-10}}
         & MSP\citep{hendrycks17baseline} &  &  &  &  & \\
         & ODIN\citep{LiangLS18} &  &  &  &  & \\
         & Mahalanobis\citep{10.5555/3327757.3327819} &  &  &  &  & \\
         & Energy\citep{liu2020energy} &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  & \\
         \cmidrule{2-7}
         & OE\citep{hendrycks2018deep} &  &  &  &  & \\
         & Energy (w. )\citep{liu2020energy} &  &  &  &  & \\
& POEM\citep{ming2022poem}  &  &  &  &  & \\
& \textbf{OE+UM} (ours) &  &  &  &  & \\
         & \textbf{OE+UMAP} (ours) &  &  &  &  & \\
\midrule[0.6pt]
        \multirow{11}*{\textbf{CIFAR-100}}
         & MSP\citep{hendrycks17baseline} &  &  &  &  & \\
         & ODIN\citep{LiangLS18} &  &  &  &  & \\
         & Mahalanobis\citep{10.5555/3327757.3327819} &  &  &  &  & \\
         & Energy\citep{liu2020energy} &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  & \\
         \cmidrule{2-7}
         & OE\citep{hendrycks2018deep} &  &  &  &  & \\
         & Energy (w. )\citep{liu2020energy} &  &  &  &  & \\
& POEM\citep{ming2022poem}  &  &  &  &  & \\
         & \textbf{OE+UM} (ours) &  &  &  &  &  \\
         & \textbf{OE+UMAP} (ours) &  &  &  &  & \\
\bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:my_label}
    \vspace{-4mm}
\end{table*}

\begin{table*}[t!]
    \caption{Fine-grained Results (). Comparison on different OOD benchmark datasets. (averaged by multiple trials) }
\centering
    \footnotesize
    \renewcommand\arraystretch{0.95}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|cccccc}
        \toprule[1.5pt]
        \multirow{3}*{\textbf{ID dataset}} & \multirow{3}*{\textbf{Method}} & \multicolumn{6}{c}{\textbf{OOD dataset}} \\
        ~ & ~ & \multicolumn{2}{c}{\textbf{CIFAR-100}} & \multicolumn{2}{c}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Places365}}  \\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \midrule[0.6pt]
        \multirow{12}*{\textbf{CIFAR-10}}
         & MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         \cmidrule{2-8}
         ~ & \multirow{2}*{\textbf{Method}} &\multicolumn{2}{c}{\textbf{SUN}} &
        \multicolumn{2}{c}{\textbf{LSUN}} & \multicolumn{2}{c}{\textbf{iNaturalist}}\\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \cmidrule{2-8}
         ~& MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:my_label2}
\end{table*}

\subsection{Experimental Setups}
\label{sec:exp_part1}

\paragraph{Datasets.} Following the common benchmarks used in previous work~\citep{liu2020energy,ming2022poem}, we adopt \verb+CIFAR-10+, \verb+CIFAR-100+ \citep{krizhevsky2009learning_cifar10} as our major ID datasets, and we also adopt \texttt{ImageNet}~\citep{deng2009imagenet} for performance exploration. We use a series of different image datasets as the OOD datasets, e.g., \verb+Textures+ \citep{cimpoi2014describing}, \verb+Places365+ \citep{zhou2017places}, \verb+SUN+ \citep{5539970}, \verb+LSUN+ \citep{yu2015lsun}, \verb+iNaturalist+ \citep{van2018inaturalist} and \texttt{SVHN}~\citep{netzer2011reading_SVHN}. We also use the other ID dataset as OOD dataset when training on a specific ID dataset, given that none of them shares the same classes, e.g., we treat \verb+CIFAR-100+ as the OOD dataset when training on \verb+CIFAR-10+ for comparison. We utilize the ImageNet-1k~\citep{deng2009imagenet} training set as the auxiliary dataset for all of our experiments about fine-tuning with auxiliary outliers (e.g., OE/Energy/POEM), which is detailed in Appendix \ref{app:additional_exp_setup}. This choice follows previous literature \citep{hendrycks2018deep,liu2020energy,ming2022poem} that considers the dataset's availability and the absence of any overlap with the ID datasets.



\begin{figure*}[t!]
    \begin{center}
    
    \subfigure[Train v.s. finetune]{
    \includegraphics[scale=0.13]{fig4a_finetune_scratch.pdf}
    \label{fig4:a}
    }
    \subfigure[Compare with Overfitting]{
    \includegraphics[scale=0.13]{fig4b_overfitting.pdf}
    \label{fig4:b}
    }
    \subfigure[Diff. Score Functions]{
    \includegraphics[scale=0.13]{fig4c_odin_energy.pdf}
    \label{fig4:c}
    }
    \subfigure[Diff. Masking Ratios]{
    \includegraphics[scale=0.13]{fig4d_mask_ratio.pdf}
    \label{fig4:d}
    }
    \subfigure[UMAP v.s. Prune]{
    \includegraphics[scale=0.13]{fig4e_umap_prune.pdf}
    \label{fig4:e}
    }
    \end{center}
    \vspace{-4mm}
    \caption{\textbf{Ablation studies:} (a) efficiency of the finetuning adopted in UM and UMAP; (b) comparison of UM and UMAP with other strategies for alleviating the conventional overfitting; (c) the historical model stage using different scoring functions for OOD detection; (d) effects of using different masking ratios for atypical mining in UM; (e) comparison of using vanilla pruning with our proposed UMAP.
}
\end{figure*}









\paragraph{Evaluation metrics.} We employ the following three common metrics to evaluate the performance of OOD detection: (i) Area Under the Receiver Operating Characteristic curve (AUROC) \citep{inproceedings} can be interpreted as the probability for a positive sample to have a higher discriminating score than a negative sample \citep{fawcett2006introduction}; (ii) Area Under the Precision-Recall curve (AUPR) \citep{manning99foundations} is an ideal metric to adjust the extreme difference between positive and negative base rates; (iii) False Positive Rate (FPR) at  True Positive Rate (TPR) \citep{LiangLS18} indicates the probability for a negative sample to be misclassified as positive when the true positive rate is at \%. We also include in-distribution testing accuracy (ID-ACC) to reflect the preservation level of the performance for the original classification task on ID data.

\paragraph{OOD detection baselines.} We compare the proposed method with several competitive baselines in the two directions. Specifically, we adopt Maximum Softmax Probability (MSP) \citep{hendrycks17baseline}, ODIN \citep{LiangLS18}, Mahalanobis score \citep{10.5555/3327757.3327819}, and Energy score \citep{liu2020energy} as scoring function baselines; We adopt OE \citep{hendrycks2018deep}, Energy-bounded learning \citep{liu2020energy}, and POEM \citep{ming2022poem} as baselines with outliers. For all scoring function methods, we assume the accessibility of well-trained models. For all methods involving outliers, we constrain all major experiments to a finetuning scenario, which is more practical in real cases. Different from training a dual-task model at the very beginning, equipping deployed models with OOD detection ability is a much more common circumstance, considering the millions of existing deep learning systems. We leave more implementation details in Appendix~\ref{app:baseline_info}.



\subsection{Performance Comparison}
\label{sec:exp_part2}

In this part, we present the performance comparison with some representative baseline methods to demonstrate the effectiveness of our UM and UMAP. 
In each category of Table~\ref{tab:my_label}, we choose one with the best detection performance to adopt UM or UMAP and check the three evaluation metrics of OOD detection and the ID-ACC. 



In Table~\ref{tab:my_label}, we summarize the results using different methods. For the scoring-based methods, our UM can further improve the overall detection performance by alleviating the memorization of atypical ID data, when the ID-ACC keeps comparable with the baseline. For the complex CIFAR-100 dataset, our UMAP can be adopted as a practical way to empower the detection performance and simultaneously avoid severely affecting the original performance on ID data. As for those methods of the second category (i.e., involving auxiliary outlier  sampled from ImageNet), since we consider a practical workflow, i.e., fine-tuning, on the given model, OE achieves the best performance on the task. Due to the special optimization characteristic, Energy (w. ) and POEM focus more on the energy loss on differentiating OOD data while performing not well on the preservation of ID-ACC. Without sacrificing much performance on ID data, OE with our UM can still achieve better detection performance. In Table~\ref{tab:my_label2}, the fine-grained detection performance on each OOD testing set demonstrates the general effectiveness of UM and UMAP. Note that we may observe Mahalanobis can sometimes achieve the best performance on the specific OOD test set (e.g., Textures). It is probably because Mahalanobis is prone to overfitting on texture features during fine-tuning with Textures. In contrast, according to Table \ref{tab:my_label2}, Mahalanobis achieves the worst results on the other five datasets. We leave more results (e.g., completed comparison in Table~\ref{tab:my_label_complete}; more fine-grained results in Tables~\ref{tab:my_label3} and~\ref{tab:my_label4}; using another model structure in Tables~\ref{tab:label_wrn},~\ref{tab:label_wrn_zoom_in_cifar10} and~\ref{tab:label_wrn_zoom_in_cifar100}) to Appendix, which has verified the significant improvement (up to  reduced on averaged FPR95) across various setups and also on a large-scale ID dataset (i.e., ImageNet~\citep{deng2009imagenet} in Table~\ref{tab:my_imagenet}).

\begin{figure*}[t!]
    \begin{center}
    \vspace{2mm}
    \includegraphics[scale=0.11]{fig17_largescale_4X15_layer_wise_scores_compressed_compressed.pdf}
    \end{center}
    \vspace{-2mm}
    \caption{\textbf{Sample Visualizations:} examples of the misclassified samples after adopting masking on the original well-trained model using the ImageNet dataset. At the left of each line, we indicate the mask ratio that is adopted on the model. We can find that masking with a smaller ratio forces the model to misclassify simple samples (clear contours around subjects, single color background) while masking with a larger ratio guides the model to misclassify complex samples (unclear contours, noisy background). This inspection empirically verifies our intuition using the proper mask ratio to identify those atypical samples and then force the model to forget them.
}
    \label{fig:sample_vis}
\end{figure*}






\subsection{Ablation and Further Analysis}
\label{sec:exp_part3}

In this part, we conduct further explorations and analysis to provide a thorough understanding of our UM and UMAP. Moreover, we also provide additional experimental results about further explorations on OOD detection in Appendix~\ref{app:additional_exp_results}.




\paragraph{Practicality of the considered setting and the implementation choice.} Following the previous work~\citep{liu2020energy,hendrycks2018deep}, we consider the same setting that starts from a given well-trained model in major explorations, which is practical but can be extended to another implementation choice, i.e., retraining the whole model. In Figure~\ref{fig4:a}, we show the effectiveness of UM/UMAP under different choices. It is worth noting that UM adopting finetuning has shown the advantages of being cost-effective on convergence compared with train-from-scratch, which we leave more discussion and comparison in Appendix~\ref{app:exp_less_epochs}. 





\paragraph{Specificity and applicability of excavated OOD discriminative capability.} As mentioned before, the intrinsic OOD discriminative capability is distinguishable from conventional overfitting. We empirically compare UM/UMAP with dropout (DR), weight decay (WD), and early stop in Figure~\ref{fig4:b}. UM gain lower FPR95 from the newly designed objective for forgetting. In Figure~\ref{fig4:c}, we present the applicability of the OOD detection capability using different score functions, which implies the generated model stage better meets the requirement of uncertainty estimation.





\paragraph{Effects of the mask on mining atypical samples.} In Figure~\ref{fig4:d}, we compare UM with different mask ratios for mining the atypical samples, which seeks to find the intermediate model stage that wrongly classified the atypical samples. The results show reasonably small ratios (e.g., from  to ) that we knocked off in the original model can help us to achieve the targets. More detailed analysis of the mask ratio and the discussion about the underlying intuition of atypical mining are provided in Appendixes~\ref{app:eff_um} and~\ref{app:atypical_mining}.







\paragraph{Exploration on UMAP and vanilla model pruning.} Although the large constraint on training loss can help reveal the OOD detection performance, the ID-ACC may be undermined under such circumstances. To mitigate this issue, we further adopt pruning in UMAP to learn a mask instead of tuning the model parameters directly. In Figure~\ref{fig4:e}, we explore various prune rates  and demonstrate their effectiveness. Specifically, our UMAP can achieve a lower FPR95 than vanilla pruning with the original objective.
The prune rate can be selected from a wide range (e.g., ) to guarantee a fast convergence and effectiveness. We also provide additional discussion on UMAP in Appendix~\ref{app:comp_prune_umap}.

\paragraph{Sample visualization of the atypical samples identified by our mask.} 
In Figure~\ref{fig:sample_vis}, we visualize the misclassified samples using the ImageNet~\citep{deng2009imagenet} dataset with the pre-trained model by adopting different mask ratios. We can find that masking the model constructs the parameter discrepancy, which helps us to identify some ID samples with atypical semantic information (e.g., those samples in the bottom line compared with the above in each class). It demonstrates the rationality of our intuition to adopt masking. We leave more visualization results in Appendix~\ref{app:atypical_mining}.


\begin{figure}[t!]
    \begin{center}
    \includegraphics[scale=0.095]{daux_as_anchor1_without.pdf}
    \includegraphics[scale=0.095]{daux_as_anchor2.pdf}
    \includegraphics[scale=0.095]{daux_as_anchor3.pdf}
    \includegraphics[scale=0.101]{fig5a_epoch10_distribution.pdf}
    \includegraphics[scale=0.101]{fig5b_epoch60_distribution.pdf}
    \includegraphics[scale=0.101]{fig5c_epoch100_distribution.pdf}
    \end{center}
    \vspace{-4mm}
    \caption{Illustration about the framework for theoretical anslysis. From left to right: the model under-represent on  with the lower confidence on the atypical samples close to the boundary; the model reaches a near-optimal representation status on ; the model over-represent on . See Figure~\ref{fig:theo} for more explanations.
    }
    \label{fig5:theo_ill}
    \vspace{-3mm}
\end{figure}

\paragraph{Theoretical insights on ID data property.} Similar to prior works~\citep{LeeLLS18, SehwagCM21}, here we present the major results based on the sample complexity analysis adopted in POEM~\citep{ming2022poem}. Due to the limited space,  please refer to Appendix~\ref{app:theo} for the completed analysis and Figure~\ref{fig:theo} for more conceptual understanding.

\begin{theorem}
    Given a simple Gaussian mixture model in binary classification with the hypothesis class . There exists constant ,  and  that,
    
\end{theorem}
Since  is monotonically decreasing, as the lower bound of  will increase with the constraint of  (which corresponds to the illustrated distance from the outlier boundary in right-most of Figure~\ref{fig5:theo_ill}) decrease in our methods, the upper bound of  will decrease. One insight is learning more atypical ID samples needs more high-quality auxiliary outliers (near ID data) to shape the OOD detection capability.

\paragraph{Additional experimental results of explorations.} 
Except for the major performance comparisons and the previous ablations, we also provide further discussion and analysis from different views in Appendix~\ref{app:additional_exp_results}, including the practicality of the considered setting, the effects of the mask on mining atypical samples, discussion of UMAP with vanilla pruning, additional comparisons with more advanced methods and completed results of our proposed UM and UMAP.


\section{Related Work}

\textbf{OOD Detection without auxiliary data.} \citet{hendrycks17baseline} formally shed light on out-of-distribution detection, proposing to use softmax prediction probability as a baseline which is demonstrated to be unsuitable for OOD detection~\citep{hendrycks2018deep}. Subsequent works~\citep{sun2021react} keep focusing on designing post-hoc metrics to distinguish ID samples from OOD samples, among which ODIN \citep{LiangLS18} introduces small perturbations into input images to facilitate the separation of softmax score, Mahalanobis distance-based confidence score \citep{10.5555/3327757.3327819} exploits the feature space by obtaining conditional Gaussian distributions, energy-based score \citep{liu2020energy} aligns better with the probability density. Besides directly designing new score functions, many other works pay attention to various aspects to enhance the OOD detection such that LogitNorm \citep{wei2022logitnorm} produces confidence scores by training with a constant vector norm on the logits, and DICE \citep{sun2022dice} reduces the variance of the output distribution by leveraging the model sparsification.

\iffalse
\begin{table}[t!]
    \caption{OOD Detection Performance on ImageNet Dataset. }
    \vspace{1mm}
    \centering
    \footnotesize
    \renewcommand\arraystretch{0.95}
    \resizebox{0.46\textwidth}{!}{
    \begin{tabular}{c|l|cc}
        \toprule[1.5pt]
         & \textbf{Method} & FPR95 & AUROC \\
\midrule[0.6pt]
        \multirow{9}*{\textbf{ImageNet}}
         & MSP &  & \\
         & ODIN &  & \\
& Energy &  & \\
         \cmidrule{2-4}
         & \textbf{MSP+UM} (ours) &  & \\
         & \textbf{ODIN+UM} (ours) &  & \\
         & \textbf{Energy+UM} (ours) &  & \\
         \cmidrule{2-4}
         & \textbf{MSP+UMAP} (ours) &  & \\
         & \textbf{ODIN+UMAP} (ours) &  & \\
         & \textbf{Energy+UMAP} (ours) &  & \\
         
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:my_imagenet}
    \vspace{-4mm}
\end{table}
\fi


\textbf{OOD Detection with auxiliary data.} Another promising direction toward OOD detection involves the auxiliary outliers for model regularization. On the one hand, some works generate virtual outliers such that \citet{LeeLLS18} uses generative adversarial networks to generate boundary samples, VOS \citep{du2022vos} regularizes the decision boundary by adaptively sampling virtual outliers from the low-likelihood region. On the other hand, other works tend to exploit information from natural outliers, such that outlier exposure is introduced by \citet{hendrycks2018deep}, given that diverse data are available in enormous quantities. \citep{DBLP:conf/iccv/YuA19} train an additional "head" and maximizes the discrepancy of decision boundaries of the two heads to detect OOD samples. Energy-bounded learning \citep{liu2020energy} fine-tunes the neural network to widen the energy gap by adding an energy loss term to the objective. Some other works also highlight the sampling strategy, such that ATOM \citep{chen2021atom} greedily utilizes informative auxiliary data to tighten the decision boundary for OOD detection, and POEM \citep{ming2022poem} adopts Thompson sampling to contour the decision boundary precisely. The performance of training with outliers is usually superior to that without outliers, shown in many other works~\citep{liu2020energy, fort2021exploring, sun2021react, SehwagCM21, chen2021atom,salehi2021unified, wei2022logitnorm}.

\section{Conclusion}

In this work, we explore the intrinsic OOD discriminative capability of a well-trained model from a unique data-level attribution. Without involving any auxiliary outliers in training, we reveal the inconsistent trend between minimizing original training loss and gaining OOD detection capability. We further identify the potential attribution to be the memorization on atypical samples. To excavate the overlaid capability, we propose the novel Unleashing Mask (UM) and its practical variant UMAP. Through this, we construct model-level discrepancy that figures out the memorized atypical samples and utilizes the constrained gradient ascent to encourage forgetting. It better utilizes the well-trained given model via backtracking or sub-structure pruning. We hope our work could provide new insights for revisiting the model development in OOD detection, and draw more attention toward the data-level attribution. Future work can be extended to a more systematical ID/OOD data investigation with other topics like data pruning or few-shot finetuning.




\section*{Acknowledgements}

JNZ and BH were supported by NSFC Young Scientists Fund No. 62006202, Guangdong Basic and Applied Basic Research Foundation No. 2022A1515011652, CAAI-Huawei MindSpore Open Fund, and HKBU CSD Departmental Incentive Grant. JCY was supported by the National Key R\&D Program of China (No. 2022ZD0160703),  STCSM (No. 22511106101, No. 22511105700, No. 21DZ1100100), 111 plan (No. BP0719010). JLX was supported by RGC grants 12202221 and C2004-21GF.

\clearpage

\bibliography{main}
\bibliographystyle{icml2023}


\newpage
\appendix
\onecolumn


\section*{Appendix}


\section*{Reproducibility Statement}

We provide the link of our source codes to ensure the reproducibility of our experimental results: \url{https://github.com/tmlr-group/Unleashing-Mask}. Below we summarize critical aspects to facilitate reproducible results:

\begin{itemize}
    \item \textbf{Datasets.}  The datasets we used are all publicly accessible, which is introduced in Section~\ref{sec:exp_part1}. For methods involving auxiliary outliers, we strictly follow previous works \citep{sun2021react, du2022vos} to avoid overlap between the auxiliary dataset (ImageNet-1k)~\citep{deng2009imagenet} and any other OOD datasets.
    \item \textbf{Assumption.} We set our experiments to a post-hoc scenario~\citep{liu2020energy} where a well-trained model is available, and some parts of training samples are also available for subsequent fine-tuning~\citep{hendrycks2018deep}.
\item \textbf{Environment.} All experiments are conducted with multiple runs on NVIDIA Tesla V100-SXM2-32GB GPUs with Python 3.6 and PyTorch 1.8.
\end{itemize}



\section{Details about Considered Baselines and Metrics}
\label{app:baseline_info}

In this section, we provide the details about the baselines for the scoring functions and fine-tuning with auxiliary outliers, as well as the corresponding hyper-parameters and other related metrics that are considered in our work.
\paragraph{Maximum Softmax Probability (MSP).} \citep{hendrycks17baseline} proposes to use maximum softmax probability to discriminate ID and OOD samples. The score is defined as follows,

where  represents the given well-trained model and  is one of the classes . The larger softmax score indicates the larger probability for a sample to be ID data, reflecting the model's confidence on the sample. 

\paragraph{ODIN.} \citep{LiangLS18} designed the ODIN score, leveraging the temperature scaling and tiny perturbations to widen the gap between the distributions of ID and OOD samples. The ODIN score is defined as follows,

where  represents the perturbed samples (controled by ),  represents the temperature. For fair comparison, we adopt the suggested hyperparameters \citep{LiangLS18}: , .

\paragraph{Mahalanobis.} \citep{10.5555/3327757.3327819} introduces a Mahalanobis distance-based confidence score, exploiting the feature space of the neural networks by inspecting the class conditional Gaussian distributions. The Mahalanobis distance score is defined as follows,

where  represents the estimated mean of multivariate Gaussian distribution of class ,  represents the estimated tied covariance of the  class-conditional Gaussian distributions.

\paragraph{Energy.} \citep{liu2020energy} proposes to use the Energy of the predicted logits to distinguish the ID and OOD samples. The Energy score is defined as follows,

where  represents the temperature parameter. As theoretically illustrated in \citet{liu2020energy}, a lower Energy score indicates a higher probability for a sample to be ID. Following \citep{liu2020energy}, we fix the  to  throughout all experiments.

\paragraph{Outlier Exposure (OE).} \citep{hendrycks2018deep} initiates a promising approach towards OOD detections by involving outliers to force apart the distributions of ID and OOD samples. In the experiments, we use the cross-entropy from to the uniform distribution as the  \citep{LeeLLS18},
  

\paragraph{Energy (w. ).} In addition to using the Energy as a post-hoc score to distinguish ID and OOD samples, \citep{liu2020energy} proposes an Energy-bounded objective to further separate the two distributions. The OE objective is as follows,

We keep the thresholds same to \citep{liu2020energy}: , .

\paragraph{POEM.} \citep{ming2022poem} explores the Thompson sampling strategy \citep{thompson} to make the most use of outliers to learn a tight decision boundary. Though given the POEM's nature to be orthogonal to other OE methods, we use the Energy(w. ) as the backbone, which is the same as Eq.(~\ref{eq:energy_aux}) in \citet{liu2020energy}. The details of Thompson sampling can refer to~\citet{ming2022poem}.

\paragraph{FPR and TPR.} Suppose we have a binary classification task (to predict an image to be an ID or OOD sample in this paper). There are two possible outputs: a positive result (the model predicts an image to be an ID sample); a negative result (the model predicts an image to be an OOD sample). Since we have two possible labels and two possible outputs, we can form a confusion matrix with all possible outputs as follows,

\begin{table}[h!]
    \caption{Confusion Matrix.}
    \vspace{3mm}
    \centering
    \footnotesize
    \begin{tabular}{c|c|c}
         \toprule[0.6pt]
         & Truth: ID & Truth: OOD\\
        \midrule[0.6pt]
         Predict: ID & True Positive (TP) & False Positive (FP)\\
        \midrule[0.6pt]
         Predict: OOD & False Negative (FN) & True Negative (TN)\\
         \bottomrule[0.6pt]
    \end{tabular}
    \label{tab:confusion}
\end{table}


The false positive rate (FPR) is calculated as:

The true positive rate (TPR) is calculated as:


\paragraph{Margin value.} Let  be a model that outputs  logits, following previous works~\citep{koltchinskii2002empirical,cao2019learning}, the margin value of an example (x,y) used in our Figure~\ref{fig2:c} is defined as,


\section{Theoretical Insights on ID Data Property}
\label{app:theo}

In this section, we provide a detailed discussion and theoretical analysis to explain the revealed observation and the benefits of our proposed method on ID data property. Specifically, we present the analysis based on the view of sample complexity adopted in POEM~\citep{ming2022poem}. To better demonstrate the conceptual extension, we also provide an intuitive illustration based on a comparison with POEM's previous focus on auxiliary outlier sampling in Figure~\ref{fig:theo} (extended version of Figure~\ref{fig5:theo_ill}). Briefly, we focus on the ID data property which is not discussed in the previous analytical framework.

\paragraph{Preliminary setup and notations.} As the original training task (e.g., the multi-classification task on CIFAR-10) does not involve any outliers data, it is hard to analyze the related property with OOD detection. Here we introduce an Assumption~\ref{assump:daux} about virtual  to help complete the analytical framework. 
To sum up, we consider a binary classification task here for distinguishing ID and OOD data. Following the prior works~\citep{LeeLLS18,SehwagCM21,ming2022poem}, we assume the extracted feature approximately follows a Gaussian mixture model (GMM) with the equal class priors as . To be specific,   and . Considering the hypothesis class as . The classifier outputs 1 if  and outputs -1 if .



\begin{figure*}[t!]
    \begin{center}
    \subfigure[ as an Anchor]{
    \includegraphics[scale=0.135]{daux_as_anchor.pdf}
    \label{fig6:poem_theo}
    }
    \subfigure[Under-represent on ]{
    \includegraphics[scale=0.135]{daux_as_anchor1.pdf}
    \label{fig6:ours_theo1}
    }
    \subfigure[Optimal-represent on ]{
    \includegraphics[scale=0.135]{daux_as_anchor2.pdf}
    \label{fig6:ours_theo2}
    }
    \subfigure[Over-represent on ]{
    \includegraphics[scale=0.135]{daux_as_anchor3.pdf}
    \label{fig6:ours_theo3}
    }
    \end{center}
\caption{Illustration about the theoretical insights on ID data property considering the binary classification scenario, which is presented as a conceptual comparison based on the underlying intuition in POEM~\citep{ming2022poem}. Different from treating the ID distribution  as an analytical anchor in POEM, we present three conceptual visualizations which correspond to different training phases in model development on ID data. (a) Using  as an anchor, the boundary data (defined in \citet{ming2022poem}) sampled from  is important to mitigate the distribution gap with the true  (as indicated with the red arc).  Without the , we assume a virtual  exists for the analytical target, which is highly related to the  and the model in the original classification task on . (b) the model under-represent on  with the lower confidence on the atypical samples close to the boundary; (c) the model reaches a near-optimal representation status on ; (d) the model over-represent on . The corresponding OOD discriminative capability is affected by the different scenarios.
    }
    \label{fig:theo}
    \vspace{-2mm}
\end{figure*}

First, we introduce the assumption about virtual . Considering the representation power of deep neural networks, the assumption can be valid. It is empirically supported by the evidence in Figure~\ref{fig5:theo_ill}, as the part of real  can be viewed as the virtual . Second, to better link our method for the analysis, we introduce another assumption (i.e., Assumption~\ref{assump:masking}) about the ID training status. It can be verified by the relative degree of distinguishability indicated by a fixed threshold in Figure~\ref{fig5:theo_ill}, that the model is more confident on the  along with the training.

\begin{assumption}[Virtual ]
Given the well-trained model in the original classification task on the ID distribution , and considering the binary classification for OOD detection, we can assume the existence of a virtual , that the OOD discriminative capacity of the current model can result from learning on the virtual  with the outlier exposure manner.
\label{assump:daux}
\end{assumption}

\begin{assumption}[ID Training Status w.r.t. Masking]
Considering the model training phase in the original multi-class classification task on the ID distribution , and tuning with a specific mask ratio serving as the sample selection, we assume that the data points  virtual  satisfy the extended constraint based on the boundary scores  defined in POEM~\cite{ming2022poem}: , where the  is a function parameterized by some unknown ground truth weights and maps the high-dimensional input x into a scalar. Generally, it represents the discrepancy between virtual  and the true , indicated with the constraint  results from the masked ID data.
\label{assump:masking}
\end{assumption}

Given the above, we can naturally get the following extended lemma based on that adopted in POEM~\citep{ming2022poem}.

\begin{lemma}[Constraint of Varied Virtual ]
\label{app: lemma1}
Assume the data points  virtual  satisfy the following constraint for resulting in the following varied boundary margin: . 
\end{lemma}

\begin{proof}[proof of Lemma.~\ref{app: lemma1}]
Given the Gaussian mixture model described in the previous setup, we can obtain the following expression by Bayes' rule of ,

where , and  according to its definition. Then we have:

Therefore, we can get the constraint as: .
\end{proof}


With the previous assumption and lemma that incorporate our masking in the variable , we present the analysis as below.

\paragraph{Complexity analysis anchored on .} With the above lemma and the assumptions of virtual  (as illustrated in Figure~\ref{fig:theo}), we can derive the results to understand the benefits from the revealed observation and our UM and UMAP.

Consider the given classifier defined as , assume each  is drawn \textit{i.i.d.} from  and each  is drawn \textit{i.i.d} from , and assume the signal/noise ratio is , the dimentionality/sample size ratio is , as well as exist some constant . By decomposition, we can rewrite  with the following  and :

Since , we have that  and  to form the standard concentration bounds as:


Anchored on , the distribution of  can be treated as a truncated distribution of  as  drawn \textit{i.i.d.} from the virtual  are under the relative constraint with . Without losing the generality, we replace  with , and have the following inequality with a finite positive constant :

According to Lemma~\ref{app: lemma1}, we can have that . Now we can have , ,  simultaneously hold and derive the following recall the decomposition,

and

With the above inequality derived in Eq.~(\ref{eq:overalltheta}) and Eq.~(\ref{eq:overallmutheta}), we can have the following bound with the probability at least 

Since  is monotonically decreasing, as the lower bound of  will increase as the constraint from the virtual  changed accordingly in our UM and UMAP, the upper bound of  will decrease.
From the above analysis, one insight we can draw is learning more atypical ID data may need more high-quality auxiliary outliers to shape the near-the-boundary behavior of the model, which can further enhance the OOD discriminative capability.

\section{Discussion about the "Conflict" Against Previous Empirical Observation}
\label{app:conflict}

In this section, we address what initially appears to be a contradiction between our observation and previous empirical studies~ \citep{vaze2022openset,fort2021exploring}, but it is not a contradiction. This work demonstrates that during training, there exists a middle stage where the model's OOD detection performance is superior to the final stage, even though the model has not achieved the best performance on ID-ACC. Some previous studies \citep{vaze2022openset,fort2021exploring} suggest that a good close-set classifier tends to have higher OOD detection performance, which may seem to contradict our claim. However, this is not the case, and we provide the following explanations.

First, the previous empirical observation \citep{vaze2022openset,fort2021exploring} of a high correlation between a good close-set classifier (e.g., high ID-ACC in \citep{vaze2022openset}) and OOD detection performance is based on \textbf{inter-model comparisons}, such as comparing different model architectures. This is consistent with our results in Table~\ref{tab:conflict1}. Even the previous model stages backtracked via our UM show similar results confirming that a better classifier (e.g., the DenseNet-101 in Table~\ref{tab:conflict1}) is better to achieve better OOD detection performance.

Second, our observation is based on \textbf{intra-model comparisons}, which compare different training stages of a single model. Our results in Figure~\ref{fig: motivation_1} across various training settings confirm this observation. Additionally, Table~\ref{tab:conflict2} shows that when we backtrack the model through UM, we obtain lower ID-ACC but better OOD detection performance. However, if we compare different models, DenseNet-101 with higher ID-ACC still outperforms Wide-ResNet, as previously mentioned.

To summarize, our observation provides an orthogonal view to exploring the relationship between ID-ACC and OOD detection performance. On the one hand, we attribute this observation to the model's memorization of atypical samples, as further demonstrated by our experiments (e.g., in Figure~\ref{fig: motivation_2}). On the other hand, we believe that this observation reveals other characteristics of a "good classifier" beyond ID-ACC, e.g., higher OOD detection capability.

\begin{table}[h!]
    \caption{\textbf{Inter-model comparison} (different models) of ID-ACC with the OOD detection performance on CIFAR-10 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
\centering
    \footnotesize
    \begin{tabular}{c|l|cccc}
        \toprule[1.5pt]
        Method &  Model & AUROC & AUPR & FPR95 & ID-ACC \\
        \midrule[0.6pt]
        \multirow{2}*{MSP}
         & DenseNet-101 &  &  &  & \\
         & WRN-40-4 &  &  &  & \\
         \midrule[0.6pt]
        \multirow{2}*{ODIN}
         & DenseNet-101 &  &  &  & \\
         & WRN-40-4 &  &  &  & \\
         \midrule[0.6pt]
         \multirow{2}*{Energy}
         & DenseNet-101 &  &  &  & \\
         & WRN-40-4 &  &  &  & \\
         \midrule[0.6pt]
         \multirow{2}*{Energy+\textbf{UM} (ours)}
         & DenseNet-101 &  &  &  & \\
         & WRN-40-4 &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:conflict1}
\end{table}

\begin{table}[h!]
    \caption{\textbf{Intra-model comparison }(regarding the same model) of ID-ACC with the OOD detection performance on CIFAR-10 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
\centering
    \footnotesize
    \begin{tabular}{c|l|cccc}
        \toprule[1.5pt]
        Method &  Model & AUROC & AUPR & FPR95 & ID-ACC \\
        \midrule[0.6pt]
        {Energy} & \multirow{2}*{DenseNet-101} &  &  &  & \\
         \cmidrule[0.6pt]{1-1}\cmidrule[0.6pt]{3-6}
         {Energy+\textbf{UM} (ours)} &  &  &  &  & \\
         \midrule[0.6pt]
         {Energy} & \multirow{2}*{WRN-40-4} &  &  &  & \\
         \cmidrule[0.6pt]{1-1}\cmidrule[0.6pt]{3-6}
         {Energy+\textbf{UM} (ours)} &  &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:conflict2}
\end{table}

\section{Discussion with Conventional Overfitting}
\label{app:overfitting_comp}

In this section, we provide a comprehensive comparison of our observation and conventional overfitting in deep learning.

First, we would refer to the concept of the conventional overfitting \citep{goodfellow2016deep,doi:10.1073/pnas.1903070116}, i.e., the model "overfits" the training data but fails to generalize and perform well on the test data that is unseen during training. The common empirical reflection of overfitting is that the training error is decreasing while the test error is increasing at the same time, which enlarges the generalization gap of the model. It has been empirically confirmed not the case in our observation as observed in Figure~\ref{fig2:a} and~\ref{fig2:b}. To be specific, for the original classification task, there is no conventional overfitting observed as the test performance is still improved at the later training stage, which is a general pursuit of the model development phase on the original tasks~\citep{goodfellow2016deep,zhang2016understanding}.

Then, when we consider the OOD detection performance of the well-trained model, our unique observation is about the inconsistency between gaining better OOD detection capability and pursuing better performance on the original classification task for the in-distribution (ID) data. It is worth noting that here the training task is not the binary classification of OOD detection, but the classification task on ID data. It is out of the rigorous concept of conventional overfitting and has received limited focus and discussion through the data-level perspective in the previous literature about OOD detection~\citep{yang2021generalized,yangopenood2022} to the best of our knowledge. Considering the practical scenario that exists target-level discrepancy, our revealed observation may encourage us to revisit the detection capability of the well-trained model.

Third, we also provide an empirical comparison with some strategies targeted for mitigating overfitting. In our experiments, for all the baseline models including that used in Figure~\ref{fig: motivation_1}, we have adopted those strategies \citep{booktitles_jmlr_dropout, TheElementsHastie2009} (e.g., drop-out, weight decay) to reduce overfitting. 
The results are summarized in the following Tables~\ref{tab:overfitting_odin_densenet}, \ref{tab:overfitting_energy_densenet}, \ref{tab:overfitting_odin_wrn} and~\ref{tab:overfitting_energy_wrn}. According to the experiments, most conventional methods proposed to prevent conventional overfitting show limited benefits in gaining better OOD detection performance, since they have a different underlying target from UM/UMAP. However, most of them suffer from the higher sacrifice on the performance of the original task and may not be compatible and practical in the current general setting, i.e., starting from a well-trained model.
In contrast, our proposed UMAP can be a more practical and flexible way to restore detection performance.





\begin{table}[h!]
    \caption{Comparison among overfitting methods and ODIN with DenseNet-101 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
\centering
    \footnotesize
    \begin{tabular}{c|l|cccc}
        \toprule[1.5pt]
         &  Method & AUROC & AUPR & FPR95 & ID-ACC \\
        \midrule[0.6pt]
        \multirow{11}*{\textbf{CIFAR-10}}
         & Baseline &  &  &  & \\
         & Early Stopping w. ACC &  &  &  & \\
& Weight Decay 0.1 &  &  &  & \\
         & Weight Decay 0.01 &  &  &  & \\
         & Weight Decay 0.001 &  &  &  & \\
         & Drop Rate 0.3 &  &  &  & \\
         & Drop Rate 0.4 &  &  &  & \\
         & Drop Rate 0.5 &  &  &  & \\
         \cmidrule{2-6}
         & \textbf{UM} (ours) &  &  &  & \\
         & \textbf{UMAP} (ours) &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:overfitting_odin_densenet}
\end{table}

\begin{table}[h!]
    \caption{Comparison among overfitting methods and Energy with DenseNet-101 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \begin{tabular}{c|l|cccc}
        \toprule[1.5pt]
         &  Method & AUROC & AUPR & FPR95 & ID-ACC \\
        \midrule[0.6pt]
        \multirow{11}*{\textbf{CIFAR-10}}
         & Baseline &  &  &  & \\
         & Early Stopping w. ACC &  &  &  & \\
& Weight Decay 0.1 &  &  &  & \\
         & Weight Decay 0.01 &  &  &  & \\
         & Weight Decay 0.001 &  &  &  & \\
         & Drop Rate 0.3 &  &  &  & \\
         & Drop Rate 0.4 &  &  &  & \\
         & Drop Rate 0.5 &  &  &  & \\
         \cmidrule{2-6}
         & \textbf{UM} (ours) &  &  &  & \\
         & \textbf{UMAP} (ours) &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:overfitting_energy_densenet}
\end{table}

\begin{table}[h!]
    \caption{Comparison among overfitting methods and ODIN with WRN-40-4 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \begin{tabular}{c|l|cccc}
        \toprule[1.5pt]
         &  Method & AUROC & AUPR & FPR95 & ID-ACC \\
        \midrule[0.6pt]
        \multirow{11}*{\textbf{CIFAR-10}}
         & Baseline &  &  &  & \\
         & Early Stopping w. ACC &  &  &  & \\
& Weight Decay 0.1 &  &  &  & \\
         & Weight Decay 0.01 &  &  &  & \\
         & Weight Decay 0.001 &  &  &  & \\
         & Drop Rate 0.3 &  &  &  & \\
         & Drop Rate 0.4 &  &  &  & \\
         & Drop Rate 0.5 &  &  &  & \\
         \cmidrule{2-6}
         & \textbf{UM} (ours) &  &  &  & \\
         & \textbf{UMAP} (ours) &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:overfitting_odin_wrn}
\end{table}

\begin{table}[h!]
    \caption{Comparison among overfitting methods and Energy with WRN-40-4 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \begin{tabular}{c|l|cccc}
        \toprule[1.5pt]
         &  Method & AUROC & AUPR & FPR95 & ID-ACC \\
        \midrule[0.6pt]
        \multirow{11}*{\textbf{CIFAR-10}}
         & Baseline &  &  &  & \\
         & Early Stopping w. ACC &  &  &  & \\
& Weight Decay 0.1 &  &  &  & \\
         & Weight Decay 0.01 &  &  &  & \\
         & Weight Decay 0.001 &  &  &  & \\
         & Drop Rate 0.3 &  &  &  & \\
         & Drop Rate 0.4 &  &  &  & \\
         & Drop Rate 0.5 &  &  &  & \\
         \cmidrule{2-6}
         & \textbf{UM} (ours) &  &  &  & \\
         & \textbf{UMAP} (ours) &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:overfitting_energy_wrn}
\end{table}

Given the concept discrepancy aforementioned, we can know that "memorization of the atypical samples" are not "memorization in overfitting". Those atypical samples are empirically beneficial in improving the performance on the original classification task as shown in Figure~\ref{fig: motivation_2}. However, this part of knowledge is not very necessary and even harmful to the OOD detection task as the detection performance of the model drops significantly. Based on the training and test curves in our observation, the memorization in overfitting is expected to happen later than the final stage in which the test performance would drop. Since we have already used some strategies to prevent overfitting, it does not exist. Intuitively, the "atypical samples" identified in our work are relative to the OOD detection task. The memorization of "atypical samples" indicates that the model may not be able to draw the general information of the ID distribution through further learning on those atypical samples through the original classification task. Since we mainly provide the understanding of the data-level attribution for OOD discriminative capability, further analysis from theoretical views~\citep{fang2022is} to link the conventional overfitting with OOD detection would be an interesting future direction.


\section{Additional Explanation Towards Mining the Atypical Samples}
\label{app:atypical_mining}

In this section, we provide further discussion and explanation about mining the atypical samples.

First, for identifying those atypical samples using a randomly initialized layer-wise mask~\citep{ramanujan2020s} with the well-pre-trained model, the underlying intuition is constructing the parameter-level discrepancy to mine the atypical samples. It is inspired by and based on the evidence drawn from previous literature about learning behaviors \citep{arpit2017closer,goodfellow2016deep} of deep neural networks (DNNs), sparse representation \citep{DBLP:journals/corr/abs-1803-03635,10.1007/978-3-642-42051-1_16,barham2022pathways}, and also model uncertainty representation (like dropout~\citep{gal2016dropout}). To be specific, the atypical samples tend to be learned by the DNNs later than those typical samples \citep{arpit2017closer}, and are relatively more sensitive to the changes of the model parameter as the model does not generalize well on that~\citep{booktitles_jmlr_dropout,gal2016dropout}. By the layer-wise mask, the constructed discrepancy can make the model misclassify the atypical samples and estimate loss constraint for the forgetting objective, as visualized in Figure~\ref{fig:method}.

Second, introducing the layer-wise mask has several advantages for achieving the staged target of mining atypical samples in our proposed method, while we would also admit that the layer-wise mask may not be an irreplaceable option or may not be optimal. On the one hand, considering that the model has been trained to approach the zero error on training data, utilizing the layer-wise mask is an integrated strategy to 1) figure out the atypical samples; and 2) obtain the loss value computed by the masked output that misclassifies them. The loss constraint is later used in the forgetting objective to fine-tune the model. On the other hand, the layer-wise mask is also compatible with the proposed UMAP to generate a flexible mask for restoring the detection capability of the original model.

\paragraph{More discussion and visualization using CIFAR-10 and ImageNet.} Third, we also adopt the unit/weight mask \citep{han2015deep} and visualize the misclassified samples in Figure~\ref{fig17:mine_atypical} (we also present a similar visualization about the experiments on ImageNet~\citep{deng2009imagenet} in Figure~\ref{fig18:mine_atypica_imagenetl}). The detected samples show that traditionally pruning the network according to weights can't efficiently figure out whether an image is typical or atypical while pruning randomly can do so. Intuitively, we attribute this phenomenon to the uncertain relationship~\citep{gal2016dropout} between the magnitudes and the learned patterns. Randomly masking out weights can have a harsh influence on atypical samples, which creates a discrepancy in mining them. Further investigating the specific effect of different methods that construct the parameter-level discrepancy would be an interesting sub-topic in future work. For the value of CE loss, although the atypical samples tend to have high CE loss value, they are already memorized and correctly classified as indicated by the zero training error. Only using the high CE error can not provide the loss estimation when the model does not correctly classify those samples. 


\clearpage
\section{Algorithmic Realization of UM and UMAP}
\label{app:algo_realization}

In this section, we provide the detailed algorithmic realizations of our proposed Unleashing Mask (UM) (i.e., in Algorithm~\ref{alg:um}) and Unleashing Mask Adopt Pruning (UMAP) (i.e., in Algorithm~\ref{alg:umap}) given the well-trained model. 

In general, we seek to unleash the intrinsic detection power of the well-trained model by adjusting the well-trained given model. For the first part, we need to mine the atypical samples and estimate the loss value to misclassify them using the current model. For the second part, we need to tune or prune with the loss constraint for forgetting.

To estimate the loss constrain for forgetting (i.e.,  in Eq \ref{eq:obj} with the fixed given model ), we randomly knock out parts of weights according to a specific mask ratio . To be specific, we sample a score from a Gaussian distribution for every weight. Then we initialize a unit matrix for every layer of the model concerning the size of the layer. We formulate the mask  according to the sampled scores. We then iterate through every layer (termed as ) to find the threshold for each layer that is smaller than the score of the given mask ratio in that layer (termed as ). Then set all the ones, whose corresponding scores are more significant than the layers' thresholds, to zeros. 

We dot-multiply every layer's weights with the formulated binary matrix as if we delete some parts of the weights. Then, we input a batch of training samples to the masked model and treat the mean value of the outputs' CE loss as the loss constraint. After all of these have been done, we begin to fine-tune the model's weights with the loss constraint applied to the original CE loss. In our algorithms, the fine-tuning epochs  is the epochs we finetune after we get the well-trained model.

For UMAP, the major difference from UM is that, instead of fine-tuning the weights, we generate a popup score for every weight, and force the gradients to pass through the scores. In every iteration, we need to formulate a binary mask according to the given prune rate . This is just what we do when estimating the loss constraint. For more details, it can refer to \citep{ramanujan2020s}. In Table~\ref{tab:my_label_complete}, we summarize the complete comparison of UM and UMAP to show their effectiveness. We also provide the performance comparison by switching ID training data to be the large-scaled ImageNet, and demonstrate the effectiveness of our UM and UMAP in Table~\ref{tab:my_imagenet}. In practice, we use SVHN as a validation OOD set to tune the mask ratio. We adopt  (the corresponding estimated loss constraint is about ) in this large-scale experiment to estimate the loss constraint for forgetting. Surprisingly, we also find that loss values smaller than the estimated one (i.e., ) can also help improve OOD detection performance, distinguishing the general effectiveness of UM/UMAP.




\begin{algorithm}[h!]
\caption{Unleashing Mask (UM)}
   \label{alg:um}
   {\bf Input:} well-trained model : , Gaussian distribution: , mask ratio : , fine-tuning epochs of UM : , training samples : , layer-iterated model : , compute the -th quantile of the data  : ;\\
   {\bf Output:} fine-tuned model ;
\begin{algorithmic}[1]
\STATE \begin{footnotesize}\texttt{// Initialize a popup score for every weight}\end{footnotesize}\vspace{2mm}
    \FOR{}
        \STATE {}
    \ENDFOR\vspace{2mm}
\STATE \begin{footnotesize}\texttt{// Generate mask by the popup scores}\end{footnotesize}\vspace{2mm}
    \FOR{}
        \STATE  \begin{footnotesize}\texttt{// Generated mask for layer }\end{footnotesize}
    \ENDFOR\vspace{2mm}
\STATE \begin{footnotesize}\texttt{// Unleashing Mask: fine-tuning}\end{footnotesize}\vspace{2mm}
    \FOR{}
        \STATE 
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
\caption{Unleashing Mask Adopt Pruning (UMAP)}
  \label{alg:umap}
  {\bf Input:} well-trained model : , Gaussian distribution: , mask ratio: , fine-tuning epochs of UM: , training samples: , prune rate:  , layer-iterated model : ,compute the -th quantile of the data  : ;\\
  {\bf Output:} learnt binary mask ;
\begin{algorithmic}[1]
    \STATE \begin{footnotesize}\texttt{// Initialize a popup score for every weight}\end{footnotesize}\vspace{2mm}
\FOR{}
        \STATE {}
    \ENDFOR\vspace{2mm}
\STATE \begin{footnotesize}\texttt{// Generate mask by the popup scores}\end{footnotesize}\vspace{2mm}
    \FOR{}
        \STATE  \begin{footnotesize}\texttt{// Generated mask for layer }\end{footnotesize}
    \ENDFOR\vspace{2mm}
\STATE \begin{footnotesize}\texttt{// Initialize the ready-to-be-learned scores for UMAP}\end{footnotesize}\vspace{2mm}
    \FOR{}
        \STATE 
    \ENDFOR\vspace{2mm}
    \STATE \begin{footnotesize}\texttt{// Unleashing Mask Adopt Pruning: Pruning}\end{footnotesize}\vspace{2mm}
    \FOR{}
        \STATE \begin{footnotesize}\texttt{// Generate the mask for UMAP according to learned scores }\end{footnotesize}
        \FOR{}
            \STATE  
        \ENDFOR
        \STATE 
    \ENDFOR
    \FOR{}
        \STATE  
    \ENDFOR
\end{algorithmic}
\end{algorithm}



\begin{table}[h!]
    \caption{Completed Results (). Comparison with competitive OOD detection baselines.  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|ccccc}
        \toprule[1.5pt]
         &  Method & AUROC & AUPR & FPR95 & ID-ACC & w./w.o  \\
        \midrule[0.6pt]
        \multirow{13}*{\textbf{CIFAR-10}}
         & MSP\citep{hendrycks17baseline} &  &  &  &  & \\
         & ODIN\citep{LiangLS18} &  &  &  &  & \\
         & Mahalanobis\citep{10.5555/3327757.3327819} &  &  &  &  & \\
         & Energy\citep{liu2020energy} &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  & \\
         \cmidrule{2-7}
         & OE\citep{hendrycks2018deep} &  &  &  &  & \\
         & Energy (w. )\citep{liu2020energy} &  &  &  &  & \\
& POEM\citep{ming2022poem}  &  &  &  &  & \\
         & \textbf{OE+UM} (ours) &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  & \\
         & \textbf{POEM+UM} (ours) &  &  &  &  & \\
         & \textbf{OE+UMAP} (ours) &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  & \\
         & \textbf{POEM+UMAP} (ours) &  &  &  &  & \\
        \midrule[0.6pt]
        \multirow{13}*{\textbf{CIFAR-100}}
         & MSP\citep{hendrycks17baseline} &  &  &  &  & \\
         & ODIN\citep{LiangLS18} &  &  &  &  & \\
         & Mahalanobis\citep{10.5555/3327757.3327819} &  &  &  &  & \\
         & Energy\citep{liu2020energy} &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  & \\
         \cmidrule{2-7}
         & OE\citep{hendrycks2018deep} &  &  &  &  & \\
         & Energy (w. )\citep{liu2020energy} &  &  &  &  & \\
& POEM\citep{ming2022poem}  &  &  &  &  & \\
         & \textbf{OE+UM} (ours) & &  &  &  &  \\
         & \textbf{Energy+UM} (ours) &  &  &  &  & \\
         & \textbf{POEM+UM} (ours) &  &  &  &  & \\
         & \textbf{OE+UMAP} (ours) &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  & \\
         & \textbf{POEM+UMAP} (ours) &  &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:my_label_complete}
\end{table}

\begin{table*}[t!]
    \caption{OOD Detection Performance on ImageNet Dataset.  indicates higher values are better, and  indicates lower values are better. We experiment with the large-scale classification on a pretrained Resnet-50 (i.e., provided by PyTorch). To avoid potential semantic or covariate overlap between ID set (ImageNet) and OOD test sets \citep{sun2021react}, we choose iNaturalist, Textures, Places365, and SUN as OOD evaluation sets following previous literatures~\citep{liu2020energy, huang2021importance}. Here we provide results of MSP, ODIN, and Energy using FPR95 and AUROC.}
    \vspace{2mm}
    \centering
    \footnotesize
    \renewcommand\arraystretch{0.9}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|cccccccc|cc}
        \toprule[1.5pt]
        \multirow{3}*{\textbf{ID dataset}} & \multirow{3}*{\textbf{Method}} & \multicolumn{8}{c|}{\textbf{OOD dataset}} \\
~ & ~ & \multicolumn{2}{c}{\textbf{iNaturalist}} & \multicolumn{2}{c}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Places365}} &  \multicolumn{2}{c|}{\textbf{SUN}} & \multicolumn{2}{c}{\textbf{Average}} \\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC \\
        \midrule[0.6pt]
        \multirow{10}*{\textbf{ImageNet}}
         & MSP &  &  &  &  &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  &  &  &  &  & \\
& Energy &  &  &  &  &  &  &  &  &  & \\
         \cmidrule{2-12}
         & \textbf{MSP+UM} (ours) &  &  &  &  &  &  &  &  &  & \\
         & \textbf{ODIN+UM} (ours) &  &  &  &  &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  &  &  &  &  & \\
         \cmidrule{2-12}
         & \textbf{MSP+UMAP} (ours) &  &  &  &  &  &  &  &  &  & \\
         & \textbf{ODIN+UMAP} (ours) &  &  &  &  &  &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  &  &  &  &  &  & \\
         
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:my_imagenet}
\end{table*}


\clearpage

\section{Additional Experiment Results}
\label{app:additional_exp_results}

In this section, we provide more experiment results from different perspectives to characterize our proposed algorithms. 



\subsection{Additional Setups}
\label{app:additional_exp_setup}


\paragraph{Training details.} We conduct all major experiments on DenseNet-101 \citep{huang2017densely} with training epochs fixed to 100. The models are trained using stochastic gradient descent \citep{1177729392} with Nesterov momentum \citep{JMLR:v12:duchi11a}. We adopt Cosine Annealing \citep{LoshchilovH17} to schedule the learning rate which begins at . We set the momentum and weight decay to be  and  respectively throughout all experiments. The size of the mini-batch is  for both ID samples (during training and testing) and OOD samples (during testing). The choice of mask ratio for our UM and UMAP is detailed and further discussed in Appendix \ref{app:eff_um}.

\paragraph{Model architecture.} For DenseNet-101, we fix the growth rate and reduce the rate to 12 and 0.5 respectively with the bottleneck block included in the backbone~\citep{ming2022poem}. We also explore the proposed UM on WideResNet \citep{zagoruyko2016wide} with 40 depth and 4 widen factor, which is termed as WRN-40-4. The batch size for both ID and OOD testing samples is , and the batch size of auxiliary samples is 2000. The  in Eq.~(\ref{eq:oe_app}) is 0.5 to keep the OE loss comparable to the CE loss. As for the outliers sampling, we randomly retrieve  samples from ImageNet-1k~\citep{deng2009imagenet} for OE and Energy (w. ) and  samples using Thompson sampling~\citep{thompson} for POEM~\citep{ming2022poem}.

\begin{wrapfigure}{r}{0.38\textwidth}
  \begin{center}
    \includegraphics[scale=0.14]{fig5c_lr_curve.pdf}
    \end{center}
    \vspace{-2mm}
    \caption{Learning Rate Scheduler.}
    \label{fig7:lre_scheduler}
\end{wrapfigure}
\paragraph{Learning rate schedules.} We use 4 different learning rate schedules to demonstrate the existence of the overlaid OOD detection capability. For cosine annealing, we follow the common setups in \citet{LoshchilovH17}; for linear schedule, the learning rate remains the same in the first one-third epochs, decreases linearly to the tenth of the initial rate in the middle one-third epochs, and decrease linearly to  of the initial rate in the last one-third epochs; for the multiple decay schedule, the learning rate decreases  of the initial rate () every  epochs ( epochs); for the multiple step schedule, the learning rate decreases to  of the current rate every  epochs. All those learning rate schedules for our experiments are intuitively illustrated in Figure~\ref{fig7:lre_scheduler}.


\subsection{Empirical verification on typical/atypical data.}
\label{app:exp_typical_atypical}

In the following Tables \ref{tab:atypical_integrated}, \ref{tab:atypical_cifar10_densenet}, \ref{tab:atypical_cifar10_wrn}, \ref{tab:atypical_cifar100_densenet}, and \ref{tab:atypical_cifar100_wrn}, we further conduct the experiments to identify the negative effect of learning on those atypical samples by comparing with a counterpart that learning only with the typical samples. The results demonstrate that the degeneration in detection performance is more likely to come from learning atypical samples.

In Table \ref{tab:atypical_integrated}, we provide the main results for the verification using typical/atypical samples. Intuitively, we intend to separate the training dataset into a typical set and an atypical set, and train respectively on these two sets to see whether it is learning atypical samples that induce the degradation in OOD detection performance during the latter  training phase. Specifically, we input the training samples through the model (DenseNet-101) of the 60th epoch and get the CE loss for selection. We provide the ACC of the generated sets on the model of the 60th epoch (ACC in the tables). The extremely low ACCs of the atypical sets show that the model of the 60th epoch can hardly predict the right label, which meets our conceptual definition of atypical samples. We then finetune the model of the 60th epoch with the generated dataset and report the OOD performance. The results show learning from only those atypical data fails to gain better detection performance than its counterpart (i.e., learning from only those typical data), although it is beneficial to improve the performance of the original multi-class classification task. The experiments provide a conceptual verification of our conjecture which links our observation and the proposed method.



\begin{table}[h!]
    \caption{Fine-tuning on typical/atypical samples with different model structures ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
\begin{tabular}{c|c|c|cccc}
        \toprule[1.5pt]
         &  Dataset Size & Structure & Atypical/Typical & AUROC & AUPR & FPR95 \\
        \midrule[0.6pt]
        \multirow{4}*{\textbf{CIFAR-10}}
         & \multirow{4}*{}
         & \multirow{2}*{DenseNet-101}
         & Atypical &  &  & \\
         &  & & Typical &  &  & \\
         \cmidrule{3-7}
         & & \multirow{2}*{WRN-40-4}
         & Atypical &  &  & \\
         & & & Typical &  &  & \\
         \cmidrule{1-7}
         \multirow{4}*{\textbf{CIFAR-100}}
         & \multirow{4}*{}
         & \multirow{2}*{DenseNet-101}
         & Atypical &  &  & \\
         & & & Typical &  &  & \\
         \cmidrule{3-7}
         & & \multirow{2}*{WRN-40-4}
         & Atypical &  &  & \\
         & & & Typical &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}\label{tab:atypical_integrated}
\end{table}

\begin{table}[h!]
    \caption{Fine-tuning on typical/atypical CIFAR-10 samples with DenseNet-101 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
\begin{tabular}{c|c|ccccc}
        \toprule[1.5pt]
         &  Dataset Size & Atypical/Typical & ACC & AUROC & AUPR & FPR95 \\
        \midrule[0.6pt]
        \multirow{7}*{\textbf{CIFAR-10}}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
         \cmidrule{2-7}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &   &  & \\
         \cmidrule{2-7}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}\label{tab:atypical_cifar10_densenet}
\end{table}

\begin{table}[h!]
    \caption{Fine-tuning on typical/atypical CIFAR-10 samples with WRN-40-4 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
\begin{tabular}{c|c|ccccc}
        \toprule[1.5pt]
         &  Dataset Size & Atypical/Typical & ACC & AUROC & AUPR & FPR95 \\
        \midrule[0.6pt]
        \multirow{7}*{\textbf{CIFAR-10}}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
         \cmidrule{2-7}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
         \cmidrule{2-7}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}\label{tab:atypical_cifar10_wrn}
\end{table}

\begin{table}[h!]
    \caption{Fine-tuning on typical/atypical CIFAR-100 samples with DenseNet-101 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
\begin{tabular}{c|c|ccccc}
        \toprule[1.5pt]
         &  Dataset Size & Atypical/Typical & ACC & AUROC & AUPR & FPR95 \\
        \midrule[0.6pt]
        \multirow{7}*{\textbf{CIFAR-100}}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
         \cmidrule{2-7}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
         \cmidrule{2-7}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}\label{tab:atypical_cifar100_densenet}
\end{table}

\begin{table}[h!]
    \caption{Fine-tuning on typical/atypical CIFAR-100 samples with WRN-40-4 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
\begin{tabular}{c|c|ccccc}
        \toprule[1.5pt]
         &  Dataset Size & Atypical/Typical & ACC & AUROC & AUPR & FPR95 \\
        \midrule[0.6pt]
        \multirow{7}*{\textbf{CIFAR-100}}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
         \cmidrule{2-7}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
         \cmidrule{2-7}
         & \multirow{2}*{}
         & Atypical &  &  &  & \\
         & & Typical &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}\label{tab:atypical_cifar100_wrn}
\end{table}

\subsection{Empirical Efficiency of UM and UMAP}
\label{app:exp_less_epochs}



As mentioned before, UM adopts finetuning on the proposed objective for forgetting has shown the advantages of being cost-effective compared with train-from-scratch. For the tuning epochs, we show in Figures \ref{fig12:abla_app_6} and \ref{fig13:abla_app_7} that fine-tuning using UM can converge within about 20 epochs, indicating that we can apply our UM/UMAP for far less than 100 epochs (compared with train-from-scratch) to restore the better detection performance of the original well-trained model. It is intuitively reasonable that finetuning with the newly designed objective would benefit from the well-trained model, allowing a faster convergence since the two phases consider the same task with the same training data. As for the major experiments conducted in our work, finetuning adopts 100 epochs for better exploring and presenting its learning dynamics for research purposes, and this configuration is indicated in the training details of Section \ref{sec:exp_part1}.

Here, we also provide an extra comparison to directly show the relative efficiency of our proposed UM/UMAP in the following Table \ref{tab:20epoch_densenet} and Table \ref{tab:20epoch_wrn}. The results demonstrate that UM and UMAP can efficiently restore detection performance compared with the baseline. Considering the significance of the OOD awareness for those safety-critical areas, it is worthwhile to further excavate the OOD detection capability of the deployed well-trained model using our UM and UMAP. 

However, there may be a concern that while both UM/UMAP and OE-based methods need extra fine-tuning processes, why should we choose UM/UMAP instead of OE-based methods, given that OE-based methods can also achieve good performance on OOD detection task. The intuition of UM/UMAP is to unleash the OOD detection capability of a pre-trained model with ID data, which is orthogonal to those OE-based methods (e.g. DOE~\citep{wang2023outofdistribution}), improving the OOD capability of a pre-train model with both ID data and auxiliary data. On the one hand, OE-based methods need sampling/synthesizing large auxiliary OOD datasets, while UM/UMAP only needs the ID data. On the other hand, although both require additional costs to fine-tune the model, they are orthogonal and can be coupled (as discussed in Section~\ref{sec:method_part3}). To further address the concern, we conduct additional experiments(i.e., OE-based results in Table~\ref{tab:additional_sota}) to validate their mutual benefit in the combination. According to the results, we can find that UM/UMAP with DOE achieves better performance. This is because while OE-based methods can improve the performance of OOD detection by fine-tuning with both ID and auxiliary outliers, UM/UMAP can serve as a method (only using ID data) to encourage optimization to learn a more appropriate model for OOD detection.

\begin{table}[h!]
    \caption{Fine-tuning for 20 epochs with DenseNet-101 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \begin{tabular}{c|c|lcccc}
        \toprule[1.5pt]
         &  Epoch & Method &  AUROC & AUPR & FPR95 & ID-ACC\\
        \midrule[0.6pt]
        \multirow{11}*{\textbf{CIFAR-10}}
         & \multirow{5}*{}
         & MSP &  &  &  & \\
         & & ODIN &  &  &  & \\
         & & Energy &  &  &  & \\
         & & Energy + UM &  &  &  & \\
         & & Energy + UMAP &  &  &  & \\
        \cmidrule{2-7}
         & \multirow{6}*{}
         & MSP + UM &  &  &  & \\
         & & ODIN +UM &  &  &  & \\
         & & Energy + UM &  &  &  & \\
         & & MSP + UMAP &  &  &  & \\
         & & ODIN + UMAP &  &  &  & \\
         & & Energy + UMAP &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:20epoch_densenet}
\end{table}

\begin{table}[h!]
    \caption{Fine-tuning for 20 epochs with WRN-40-4 ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \begin{tabular}{c|c|lcccc}
        \toprule[1.5pt]
         &  Epoch & Method &  AUROC & AUPR & FPR95 & ID-ACC\\
        \midrule[0.6pt]
        \multirow{11}*{\textbf{CIFAR-10}}
         & \multirow{5}*{}
         & MSP &  &  &  & \\
         & & ODIN &  &  &  & \\
         & & Energy &  &  &  & \\
         & & Energy + UM &  &  &  & \\
         & & Energy + UMAP &  &  &  & \\
        \cmidrule{2-7}
         & \multirow{6}*{}
         & MSP + UM &  &  &  & \\
         & & ODIN +UM &  &  &  & \\
         & & Energy + UM &  &  &  & \\
         & & MSP + UMAP &  &  &  & \\
         & & ODIN + UMAP &  &  &  & \\
         & & Energy + UMAP &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:20epoch_wrn}
\end{table}

\subsection{Fine-grained Results on OOD Data}
\label{app:exp_finegrained}

In order to further understand the effectiveness of the proposed UM and UMAP on different OOD datasets, we report the fine-grained results of our experiments on CIFAR-10 and CIFAR-100 with  OOD datasets (CIFAR-10/CIFAR-100, textures, Places365, SUN, LSUN, iNaturalist). 
The results on the  OOD datasets show the general effectiveness of the proposed UM as well as UMAP. 
In Table~\ref{tab:my_label3}, \textbf{OE + UM} can outperform all the OOD baselines, and further improve the OOD performance even though the original detection performance is already well. By equipping with our proposed UM and UMAP, the baselines can outperform their counterparts on most of the OOD datasets. For instance, the FPR95 can decrease from  to . In Table~\ref{tab:my_label4}, we also take a closer check about results on CIFAR-100 with  OOD datasets. Our proposed method can almost improve all competitive baselines (either the scoring functions or the finetuning with auxiliary outliers) on the  OOD datasets. In both w.  and w.o.  scenarios, Unleashing Mask can significantly excavate the intrinsic OOD detection capability of the model. In addition to unleashing the excellent OOD performance, UMAP can also maintain the high ID-ACC by learning a binary mask instead of tuning the well-trained original parameters directly. Due to the space limit, we separate the results of SVHN dataset in Tables~\ref{tab:label_zoom_in_SVHN_densenet} and~\ref{tab:label_zoom_in_SVHN_wrn} to show the relative comparison of our UM and UMAP. The results demonstrate the general effectiveness of UM/UMAP compared with the original Energy score. Besides, we find Mahalanobis performs dramatically well which is an outlier method against other post-hoc baselines when SVHN as OOD set in our experiments. Our conjecture about this phenomenon is that the Mahalanobis score can perform better on those specific OOD data by inspecting the class conditional Gaussian distributions~\citep{LeeLLS18}. Nonetheless, the proposed UM/UMAP can still outstrip all the baselines on most OOD datasets under various settings at the perspective of average, showing their distinguishing effectiveness and practicability.


\begin{table}[t!]
    \caption{Fine-grained Results of DenseNet-101 on CIFAR-10 (). Comparison on different OOD benchmark datasets respectively.  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \renewcommand\arraystretch{0.9}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|cccccc}
        \toprule[1.5pt]
        \multirow{3}*{\textbf{ID dataset}} & \multirow{3}*{\textbf{Method}} & \multicolumn{6}{c}{\textbf{OOD dataset}} \\
        ~ & ~ & \multicolumn{2}{c}{\textbf{CIFAR-100}} & \multicolumn{2}{c}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Places365}}  \\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \midrule[0.6pt]
        \multirow{22}*{\textbf{CIFAR-10}}
         & \textbf{Energy + UMAP} &  &  &  &  &  &  \\
         \cmidrule{2-8}
         & OE &  &  &  &  &  & \\
         & Energy (w. ) &  &  &  &  &  & \\
         & POEM &  &  &  &  &  & \\
         & \textbf{OE + UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. )+ UM} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UM} (ours) &  &  &  &  &  & \\
         & \textbf{OE + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. ) + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UMAP} (ours) &  &  &  &  &  & \\
         \cmidrule{2-8}
         ~ & \multirow{2}*{\textbf{Method}} &\multicolumn{2}{c}{\textbf{SUN}} &
        \multicolumn{2}{c}{\textbf{LSUN}} & \multicolumn{2}{c}{\textbf{iNaturalist}}\\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \cmidrule{2-8}
         & \textbf{Energy + UMAP} &  &  &  &  &  &  \\
         \cmidrule{2-8}
         & OE &  &  &  &  &  & \\
         & Energy (w. ) &  &  &  &  &  & \\
         & POEM &  &  &  &  &  & \\
         & \textbf{OE + UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. )+ UM} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UM} (ours) &  &  &  &  &  & \\
         & \textbf{OE + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. ) + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UMAP} (ours) &  &  &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:my_label3}
\end{table}

\begin{table}[t!]
    \caption{Fine-grained Results of DenseNet-101 on CIFAR-100 (). Comparison on different OOD benchmark datasets respectively.  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \renewcommand\arraystretch{0.9}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|cccccc}
        \toprule[1.5pt]
        \multirow{3}*{\textbf{ID dataset}} & \multirow{3}*{\textbf{Method}} & \multicolumn{6}{c}{\textbf{OOD dataset}} \\
        ~ & ~ & \multicolumn{2}{c}{\textbf{CIFAR-10}} & \multicolumn{2}{c}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Places365}}  \\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \midrule[0.6pt]
        \multirow{30}*{\textbf{CIFAR-100}}
         & MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         \cmidrule{2-8}
         & OE &  &  &  &  &  & \\
         & Energy (w. ) &  &  &  &  &  & \\
         & POEM &  &  &  &  &  & \\
         & \textbf{OE + UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. )+ UM} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UM} (ours) &  &  &  &  &  & \\
         & \textbf{OE + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. ) + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UMAP} (ours) &  &  &  &  &  & \\
         \cmidrule{2-8}
         ~ & \multirow{2}*{\textbf{Method}} &\multicolumn{2}{c}{\textbf{SUN}} &
        \multicolumn{2}{c}{\textbf{LSUN}} & \multicolumn{2}{c}{\textbf{iNaturalist}}\\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \cmidrule{2-8}
         & MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         \cmidrule{2-8}
         & OE &  &  &  &  &  & \\
         & Energy (w. ) &  &  &  &  &  & \\
         & POEM &  &  &  &  &  & \\
         & \textbf{OE + UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. )+ UM} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UM} (ours) &  &  &  &  &  & \\
         & \textbf{OE + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. ) + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UMAP} (ours) &  &  &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:my_label4}
\end{table}


\subsection{Experiments on Different Model Structure}
\label{app:exp_diff_model}


Following \ref{sec:exp_part2}, we additionally conduct critical experiments on the WRN-40-4~\citep{lin2021mood} backbone to demonstrate the effectiveness of the proposed UM and UMAP. In Figure~\ref{fig10:abla_app_4}, we can find during the model training phase on ID data, there also exists the overlaid OOD detection capability can be explored in later development. In Table~\ref{tab:label_wrn}, we show the comparison of multiple OOD detection baselines, evaluating the OOD performance on the different OOD datasets mentioned in Section~\ref{sec:exp_part1}. The results again demonstrate that our proposed method indeed excavates the intrinsic detection capability and improves the performance. 


\begin{table}[t!]
    \caption{Results of WRN-40-4. Comparison with competitive OOD detection baselines (). We respectively train WRN-40-4 on CIFAR-10 and CIFAR-100. For those methods involving outliers, we retrieve  samples from ImageNet-1k.  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
\centering
    \footnotesize
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|cccc}
        \toprule[1.5pt]
         &  Method & AUROC & AUPR & FPR95 & ID-ACC \\
        \midrule[0.6pt]
        \multirow{6}*{\textbf{CIFAR-10}}
         & MSP\citep{hendrycks17baseline} &  &  &  &  \\
         & ODIN\citep{LiangLS18} &  &  &  &  \\
         & Mahalanobis\citep{10.5555/3327757.3327819} &  &  &  &  \\
         & Energy\citep{liu2020energy} &  &  &  &  \\
         & \textbf{Energy+UM} (ours) &  &  &  &   \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  \\
\midrule[0.6pt]
        \multirow{6}*{\textbf{CIFAR-100}}
         & MSP\citep{hendrycks17baseline} &  &  &  &   \\
         & ODIN\citep{LiangLS18} &  &  &  &    \\
         & Mahalanobis\citep{10.5555/3327757.3327819} &  &  &  &    \\
         & Energy\citep{liu2020energy} &  &  &  &    \\
         & \textbf{Energy+UM} (ours) &  &  &  &   \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  \\
\bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:label_wrn}
\end{table}

As for the fine-grained results of WRN-40-4, we report results on  OOD datasets respectively. When trained on CIFAR-10, UM can outstrip all the scoring function baselines on  OOD datasets except Textures on which Mahalanobis performs better while UMAP still has excellent OOD performance ranking only second to UM. When trained on CIFAR-100, UM and UMAP can also outperform the baselines on most OOD datasets. The fine-grained results of WRN-40-4 further demonstrate the effectiveness of the proposed UM/UMAP on other architectures. The future extension can also take other advanced model structures for OOD detection~\citep{ming2022delving} into consideration. 

\begin{table}[t!]
    \caption{Fine-grained Results of WRN-40-4 on CIFAR-10 (). Comparison on different OOD benchmark datasets.  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \renewcommand\arraystretch{0.95}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|cccccc}
        \toprule[1.5pt]
        \multirow{3}*{\textbf{ID dataset}} & \multirow{3}*{\textbf{Method}} & \multicolumn{6}{c}{\textbf{OOD dataset}} \\
        ~ & ~ & \multicolumn{2}{c}{\textbf{CIFAR-100}} & \multicolumn{2}{c}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Places365}}  \\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \midrule[0.6pt]
        \multirow{14}*{\textbf{CIFAR-10}}
         & MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  &  & \\
         \cmidrule{2-8}
         ~ & \multirow{2}*{\textbf{Method}} &\multicolumn{2}{c}{\textbf{SUN}} &
        \multicolumn{2}{c}{\textbf{LSUN}} & \multicolumn{2}{c}{\textbf{iNaturalist}}\\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \cmidrule{2-8}
         & MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  &  & \\
         
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:label_wrn_zoom_in_cifar10}
\end{table}

\begin{table}[t!]
    \caption{Fine-grained Results of WRN-40-4 on CIFAR-100 (). Comparison on different OOD benchmark datasets.  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \renewcommand\arraystretch{0.95}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|cccccc}
        \toprule[1.5pt]
        \multirow{3}*{\textbf{ID dataset}} & \multirow{3}*{\textbf{Method}} & \multicolumn{6}{c}{\textbf{OOD dataset}} \\
        ~ & ~ & \multicolumn{2}{c}{\textbf{CIFAR-10}} & \multicolumn{2}{c}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Places365}}  \\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \midrule[0.6pt]
        \multirow{14}*{\textbf{CIFAR-100}}
         & MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  &  & \\
         \cmidrule{2-8}
         ~ & \multirow{2}*{\textbf{Method}} &\multicolumn{2}{c}{\textbf{SUN}} &
        \multicolumn{2}{c}{\textbf{LSUN}} & \multicolumn{2}{c}{\textbf{iNaturalist}}\\
        ~ & ~ & FPR95 & AUROC & FPR95 & AUROC & FPR95 & AUROC \\
        \cmidrule{2-8}
         & MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  &  & \\
         
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:label_wrn_zoom_in_cifar100}
\end{table}

\begin{table}[t!]
    \caption{Results of DenseNet-101 when SVHN as OOD set (). Comparison on different ID benchmark datasets.  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \renewcommand\arraystretch{0.95}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|ccc|ccc}
        \toprule[1.5pt]
        \multirow{3}*{\textbf{OOD dataset}} & \multirow{3}*{\textbf{Method}} & \multicolumn{6}{c}{\textbf{ID dataset}} \\
        ~ & ~ & \multicolumn{3}{c|}{\textbf{CIFAR-10}} & \multicolumn{3}{c}{\textbf{CIFAR-100}} \\
        ~ & ~ & AUROC & AUPR & FPR95 & AUROC & AUPR & FPR95 \\
        \midrule[0.6pt]
        \multirow{15}*{\textbf{SVHN}}
         & MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  &  & \\
         \cmidrule{2-8}
         & OE &  &  &  &  &  & \\
         & Energy (w. ) &  &  &  &  &  & \\
         & POEM &  &  &  &  &  & \\
         & \textbf{OE + UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. )+ UM} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UM} (ours) &  &  &  &  &  & \\
         & \textbf{OE + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{Energy (w. ) + UMAP} (ours) &  &  &  &  &  & \\
         & \textbf{POEM + UMAP} (ours) &  &  &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:label_zoom_in_SVHN_densenet}
\end{table}

\begin{table}[t!]
    \caption{Results of WRN-40-4 when SVHN as OOD set (). Comparison on different ID benchmark datasets.  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \renewcommand\arraystretch{0.95}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|l|ccc|ccc}
        \toprule[1.5pt]
        \multirow{3}*{\textbf{OOD dataset}} & \multirow{3}*{\textbf{Method}} & \multicolumn{6}{c}{\textbf{ID dataset}} \\
        ~ & ~ & \multicolumn{3}{c|}{\textbf{CIFAR-10}} & \multicolumn{3}{c}{\textbf{CIFAR-100}} \\
        ~ & ~ & AUROC & AUPR & FPR95 & AUROC & AUPR & FPR95 \\
        \midrule[0.6pt]
        \multirow{6}*{\textbf{SVHN}}
         & MSP &  &  &  &  &  & \\
         & ODIN &  &  &  &  &  & \\
         & Mahalanobis &  &  &  &  &  & \\
         & Energy &  &  &  &  &  & \\
         & \textbf{Energy+UM} (ours) &  &  &  &  &  & \\
         & \textbf{Energy+UMAP} (ours) &  &  &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}}
    \label{tab:label_zoom_in_SVHN_wrn}
\end{table}

\subsection{Additional Experiments on More Advanced Post-hoc and OE-based Methods}

Except for some representative methods (like MSP, Energy, OE, POEM) that have been considered in the experiments, in Table \ref{tab:additional_sota}, we add more advanced post-hoc and OE-based methods \cite{sun2021react,sun2022dice,djurisic2023extremely,katz2022training,wang2023outofdistribution} as comparison to further validate the effectiveness of the proposed UM/UMAP.

\begin{table}[h!]
    \caption{Results of additional comparison with advanced methods of post-hoc scoring functions or fine-tuning with auxiliary outliers ().  indicates higher values are better, and  indicates lower values are better.}
    \vspace{2mm}
    \centering
    \footnotesize
    \begin{tabular}{c|l|ccccc}
        \toprule[1.5pt]
         &  Method & AUROC & AUPR & FPR95 & ID-ACC & w./w.o  \\
        \midrule[0.6pt]
        \multirow{9}*{\textbf{CIFAR-10}}
         & ReAct\cite{sun2021react} &  &  &  &  & \\
         & DICE\cite{sun2022dice} &  &  &  &  & \\
         & ASH-S\cite{djurisic2023extremely} &  &  &  &  & \\
         & \textbf{ASH-S+UM}(ours) &  &  &  &  & \\
         & \textbf{ASH-S+UMAP}(ours) &  &  &  &  & \\
         \cmidrule{2-7}
         & WOODS\cite{katz2022training} &  &  &  &  & \\
         & DOE\cite{wang2023outofdistribution} &  &  &  &  & \\
         & \textbf{DOE+UM}(ours) &  &  &  &  & \\
         & \textbf{DOE+UMAP}(ours) &  &  &  &  & \\
        \midrule[0.6pt]
        \multirow{9}*{\textbf{CIFAR-100}}
         & ReAct\cite{sun2021react} &  &  &  &  & \\
         & DICE\cite{sun2022dice} &  &  &  &  & \\
         & ASH-S\cite{djurisic2023extremely} &  &  &  &  & \\
         & \textbf{ASH-S+UM}(ours) &  &  &  &  & \\
         & \textbf{ASH-S+UMAP}(ours) &  &  &  &  & \\
         \cmidrule{2-7}
         & WOODS\cite{katz2022training} &  &  &  &  & \\
         & DOE\cite{wang2023outofdistribution} &  &  &  &  & \\
         & \textbf{DOE+UM}(ours) &  &  &  &  & \\
         & \textbf{DOE+UMAP}(ours) &  &  &  &  & \\
        \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:additional_sota}
\end{table}



\subsection{Additional Verification for Intrinsic OOD Discriminative Capability}
\label{app:verf_intrinsic_power}

In Section~\ref{sec:exp_part3}, we display the overlaid OOD detection capability on CIFAR-10 using SVHN as the OOD dataset. Here, we additionally verify the previously observed trend during training when training DenseNet-101 on CIFAR-100 using iNaturalist as an OOD dataset. In Figure~\ref{fig8:abla_app_2}, we trace the three evaluation metrics during training on CIFAR-100 using  different learning rate schedules. Consistent with the original experiment, we still use iNaturalist as the OOD dataset. It can be seen for all three metrics that exists a middle stage where the model has the better OOD detection capability (For FPR95, it is smaller (better) in the middle stage; for AUROC and AUPR, they are higher (better) in the middle stage). Besides that, we also look into the change of OOD performance on other architecture (e.g., WRN-40-4) in Figure~\ref{fig9:abla_app_3} and Figure~\ref{fig10:abla_app_4}. In Figure~\ref{fig9:abla_app_3}, we display the curves of three metrics of WRN-40-4 when trained on CIFAR-10 with SVHN and Textures as OOD datasets.  The trend that the OOD performance first goes better and then converges to worse OOD performance can be reflected. In Figure~\ref{fig10:abla_app_4}, we continually provide curves of the three metrics of WRN-40-4 during training on CIFAR-100 with iNaturalist, Places365, and SUN as OOD datasets. A clear better middle stage can still be 
excavated in this scenario.

\begin{figure}[t!]
    \begin{center}
    \subfigure[CIFAR10 FPR95 Curves]{
    \includegraphics[scale=0.18]{fig5a_cifar10_fpr95.pdf}
    \label{fig8:abla_app_2_a}
    }
    \subfigure[CIFAR10 AUROC Curves]{
    \includegraphics[scale=0.18]{fig5b_cifar10_auroc.pdf}
    \label{fig8:abla_app_2_b}
    }
    \subfigure[CIFAR10 AUPR Curves]{
    \includegraphics[scale=0.18]{fig5c_cifar10_aupr.pdf}
    \label{fig8:abla_app_2_c}
    }
    \subfigure[CIFAR100 FPR95 Curves]{
    \includegraphics[scale=0.18]{fig5d_cifar100_fpr95.pdf}
    \label{fig8:abla_app_2_d}
    }
    \subfigure[CIFAR100 AUROC Curves]{
    \includegraphics[scale=0.18]{fig5e_cifar100_auroc.pdf}
    \label{fig8:abla_app_2_e}
    }
    \subfigure[CIFAR100 AUPR Curves]{
    \includegraphics[scale=0.18]{fig5f_cifar100_aupr.pdf}
    \label{fig8:abla_app_2_f}
    }

    \end{center}
    \caption{Ablation studies on three metrics with 4 different learning rate schedules. The model is DenseNet-101 trained on CIFAR-100 with iNaturalist as the OOD dataset. (a) change of FPR95 throughout the pruning phase when training on CIFAR-100; (b) change of AUROC throughout the pruning phase when training on CIFAR-100; (c) change of AUPR throughout the pruning phase when training on CIFAR-100. It demonstrates a better middle stage exists according to the three metrics.}
    \label{fig8:abla_app_2}
\end{figure}

\begin{figure}[t!]
    \begin{center}
    \subfigure[FPR95 Curves]{
    \includegraphics[scale=0.18]{fig6a_cifar10_fpr95.pdf}
    \label{fig9:abla_app_3_a}
    }
    \subfigure[AUROC Curves]{
    \includegraphics[scale=0.18]{fig6b_cifar10_auroc.pdf}
    \label{fig9:abla_app_3_b}
    }
    \subfigure[AUPR Curves]{
    \includegraphics[scale=0.18]{fig6c_cifar10_aupr.pdf}
    \label{fig9:abla_app_3_c}
    }
    \end{center}
    \caption{Ablation studies on three metrics of WRN-40-4 with CIFAR-10 as ID dataset, SVHN, and Textures as OOD datasets. (a) change of FPR95 throughout the pruning phase when training on CIFAR-10; (b) change of AUROC throughout the pruning phase when training on CIFAR-10; (c) change of AUPR throughout the pruning phase when training on CIFAR-10. It demonstrates a better middle stage exists according to the three metrics.}
    \label{fig9:abla_app_3}
\end{figure}

\begin{figure}[t!]
    \begin{center}
    \subfigure[FPR95 Curves]{
    \includegraphics[scale=0.18]{fig7a_cifar100_fpr95.pdf}
    \label{fig10:abla_app_4_a}
    }
    \subfigure[AUROC Curves]{
    \includegraphics[scale=0.18]{fig7b_cifar100_auroc.pdf}
    \label{fig10:abla_app_4_b}
    }
    \subfigure[AUPR Curves]{
    \includegraphics[scale=0.18]{fig7c_cifar10_aupr.pdf}
    \label{fig10:abla_app_4_c}
    }
    \end{center}
    \caption{Ablation studies on three metrics of WRN-40-4 with CIFAR-100 as ID dataset, iNaturalist, Places365, and SUN as OOD datasets. (a) change of FPR95 throughout the pruning phase when training on CIFAR-100; (b) change of AUROC throughout the pruning phase when training on CIFAR-100; (c) change of AUPR throughout the pruning phase when training on CIFAR-100. It demonstrates a better middle stage exists according to the three metrics.}
    \label{fig10:abla_app_4}
\end{figure}

\subsection{Ablation on UMAP which Adopting Pruning on UM.} 
\label{app:abl_UMAP}

We conduct various experiments to see whether pruning has an impact on Unleashing Mask itself. To be specific, we expect the pruning to learn a mask on the given model while not impairing the excellent OOD performance that UM brings. In Figure~\ref{fig11:abla_app_5}, it presents that pruning from a wide range (e.g. ) can well maintain the effectiveness of UM while possessing a terrific convergence trend. For simplicity, we use prune to indicate the original pruning approach and UMAP indicate UM with pruning on the mask with our newly designed forgetting objective in Figure~\ref{fig11:abla_app_5}. In Figure~\ref{fig11:abla_app_5_a}, the solid lines represent the proposed UMAP and the dashed lines represent only pruning the well-trained model at prune rates , , and . While the model's OOD performance can't be improved (not better than the baseline) through only pruning, using our proposed forgetting objective for the loss constrain can significantly bring out better OOD performance at a wide range of mask rates (e.g. ). In Figure~\ref{fig11:abla_app_5_b}, we intuitively reflect the effect of the estimated loss constraint by the initialized mask which redirects the gradients when the loss reaches the value, while the loss will just approach  when pruning only. In Figure~\ref{fig11:abla_app_5_c}, we can see that ID-ACC for both UMAP and Prune can converge to approximately the same high level (), though we can simply remove the learned mask to recover the original ID-ACC.

\begin{figure}[t!]
    \begin{center}
    \subfigure[FPR95 Curves]{
    \includegraphics[scale=0.18]{fig8a_fpr95.pdf}
    \label{fig11:abla_app_5_a}
    }
    \subfigure[Train Loss Curves]{
    \includegraphics[scale=0.18]{fig8b_loss_curve.pdf}
    \label{fig11:abla_app_5_b}
    }
    \subfigure[Test Acc Curves]{
    \includegraphics[scale=0.18]{fig8c_acc_curve.pdf}
    \label{fig11:abla_app_5_c}
    }
    \end{center}
    \caption{Ablation studies on Prune Rate of UMAP. (a) change of OOD performance throughout the pruning phase; (b) training loss converges to estimated loss constraint properly; (c) though ID-ACC is not taken into consideration for UMAP, it still rises high after training for 100 epochs.}
    \label{fig11:abla_app_5}
\end{figure}


\begin{figure}[t!]
    \begin{center}
    \subfigure[FPR95 Curves]{
    \includegraphics[scale=0.18]{fig9a_fpr95.pdf}
    \label{fig12:abla_app_6_a}
    }
    \subfigure[AUROC Curves]{
    \includegraphics[scale=0.18]{fig9b_auroc.pdf}
    \label{fig12:abla_app_6_b}
    }
    \subfigure[AUPR Curves]{
    \includegraphics[scale=0.18]{fig9c_aupr.pdf}
    \label{fig12:abla_app_6_c}
    }
    \end{center}
    \caption{Ablation studies to reflect the effectiveness of UM. The mask ratio of UM is . (a) change of FPR95 throughout the training phase on CIFAR-10; (b) change of AUROC throughout the training phase  on CIFAR-10; (c) change of AUPR throughout the training phase on CIFAR-10.}
    \label{fig12:abla_app_6}
\end{figure}

\begin{figure}[t!]
    \begin{center}
    \subfigure[FPR95 Curves]{
    \includegraphics[scale=0.18]{fig10a_fpr95.pdf}
    \label{fig13:abla_app_7_a}
    }
    \subfigure[AUROC Curves]{
    \includegraphics[scale=0.18]{fig10b_auroc.pdf}
    \label{fig13:abla_app_7_b}
    }
    \subfigure[AUPR Curves]{
    \includegraphics[scale=0.18]{fig10c_aupr.pdf}
    \label{fig13:abla_app_7_c}
    }
    \end{center}
    \caption{Ablation studies of WRN-40-4 on various Mask Ratios. The mask rate is from  to . (a) change of FPR95 throughout the training on CIFAR-10; (b) change of AUROC throughout the training on CIFAR-10; (c) change of AUPR throughout the training on CIFAR-10.}
    \label{fig13:abla_app_7}
\end{figure}


\subsection{Fine-grained comparison of model weights.}
\label{app:comp_prune_umap}

We display the weights of the original model, pruned model, and the UMAP model respectively in Figure~\ref{fig14:abla_app_8}. The histograms show that the adopted pruning algorithm tends to choose weights far from  for the first convolution layer, shown in Figure~\ref{fig14:abla_app_8_a}. However, for almost all layers (from the 2nd to the 98th), the pruning chooses weights with no respect to the value of weights, shown in Figure~\ref{fig14:abla_app_8_b}. For the fully connected layer, the pruning algorithm itself still keeps its behavior on the first layer, while UMAP forces the pruning algorithm to choose weights near , shown in Figure~\ref{fig14:abla_app_8_c}, indicating that forgetting learned atypical samples doesn't necessarily correspond to larger weights or smaller weights.

\begin{figure}[h!]
    \begin{center}
    \subfigure[First Layer]{
    \includegraphics[scale=0.18]{fig11_0_layer.pdf}
    \label{fig14:abla_app_8_a}
    }
    \subfigure[50th Layer]{
    \includegraphics[scale=0.18]{fig11_50_layer.pdf}
    \label{fig14:abla_app_8_b}
    }
    \subfigure[Last Layer (FC)]{
    \includegraphics[scale=0.18]{fig11_99_layer.pdf}
    \label{fig14:abla_app_8_c}
    }
    \end{center}
    \caption{Histograms of different layers for the original model, pruned model, and UMAP model. The model is DenseNet-101 with a prune rate of . (a) the histogram of the first convolution layer; (b) the histogram of the 50th convolution layer; (c) the histogram of the last (fully connected) layer.}
    \label{fig14:abla_app_8}
\end{figure}

\subsection{The effectiveness of UM}
\label{app:eff_um}

In Figure~\ref{fig12:abla_app_6}, we present the FPR95, AUROC, and AUPR curves during training to show the comparison of the original training and our proposed UM on ID data. We observe that training using UM can consistently outperform the vanilla model training, either for the final stage or the middle stage with the best OOD detection performance indicated by the FPR95 curve. In Figure~\ref{fig13:abla_app_7}, we also adopt different mask rates for the initialized loss constraint estimation for forgetting the atypical samples. The results show that a wide range of mask ratios (i.e., from 96\% to 99\%) to estimate the loss constraint used in Eq.~\eqref{eq:obj} can gain better OOD detection performance than the baseline. It shows the mask ratio would be robust to hyper-parameter selection under a certain small value. The principle intuition behind this is our revealed important observation as indicated in Figures \ref{fig1:a}, \ref{fig2:a}, and \ref{fig2:b}. With the guidance of the general mechanism, empirically choosing the hyper-parameter using the validation set is supportable and valuable for excavating better OOD detection capability of the model as conducted by previous literature \citep{hendrycks2018deep,liu2020energy,sun2021react}.

In our experiments, we empirically determine the value of our proposed UM and UMAP by examining the training loss on the masked output. For CIFAR-10 as ID datasets, the value of the mask ratio is , and the estimated loss constraint for forgetting is  for our tuning until the convergence; For CIFAR-100, the value of the mask ratio is , and the estimated loss constraint for forgetting is  for our tuning until the convergence. To choose the parameters of the estimated loss constraint, we use the TinyImageNet \citep{DBLP:booktitles/corr/abs-2007-06712} dataset as the validation set, which is not seen during training and is not considered in our evaluation of OOD detection performance. Since the core intuition behind our method is to restore the OOD detection performance starting from the well-trained model stage, forgetting a relatively small portion (empirically found around 97\% mask ratio) of atypical samples can be beneficial for the two common benchmarked datasets. In addition, we also verify the effectiveness of UM and UMAP considering the large-scale ImageNet as ID dataset in Table~\ref{tab:my_imagenet} and Appendix~\ref{app:algo_realization}, the loss constraint for forgetting can be  which is estimated using the mask ratio as . To find the optimal parameter for tuning, more advanced searching techniques like AutoML or validation design based on the important observation in our work may be further employed in the future. For the safety concerns, it is affordable and reasonable to gain significant OOD detection performance improvement by investing extra computing resources.

\section{Summarization of the proposed UM/UMAP's advantages}

Regarding the advantages of the proposed method, we kindly interpret them as follows,
\begin{itemize}
    \item \textbf{Novelty.} The proposed UM/UMAP is the first to emphasize the intrinsic OOD detection capability of a given well-trained model during its training phase, better leveraging what has been learned and drawing new insight into the relationship between the OOD detection and the original classification task. This work also shows that ID data is important for a well-trained model's OOD discriminative capability.
    \item \textbf{Simplicity.} Based on the empirical insights that atypical semantics may impair the OOD detection capability, we introduced the easy-to-adopt forgetting objective to weaken the influence of atypical samples on the OOD detection performance. Besides, to maintain the ID performance, we proposed to learning a mask instead of tuning the model directly. Such a design makes UM/UMAP easy to follow and a good starting point to conduct further adjustments. They build on extensive empirical analysis on the point of how to unleash the optimal OOD detection capacity of one given model. Besides, this work explores an orthogonal perspective to previous methods, which shows the consistent improvement combined with previous methods in a range of experiments.
    \item \textbf{Compatibility \& Effectiveness.} UM/UMAP is orthogonal to other competitive methods and can be flexibly combined with them. Extensive experiments demonstrate that UM/UMAP can consistently improve the baselines on average in both benchmarked datasets and large-scale ImageNet (e.g., Tables \ref{tab:my_label},\ref{tab:my_label2},\ref{tab:my_label_complete},\ref{tab:my_imagenet},\ref{tab:my_label3},\ref{tab:my_label4},\ref{tab:label_wrn},\ref{tab:label_wrn_zoom_in_cifar10},\ref{tab:label_wrn_zoom_in_cifar100},\ref{tab:label_zoom_in_SVHN_densenet},\ref{tab:label_zoom_in_SVHN_wrn}; Figures \ref{fig4:a},\ref{fig4:d},\ref{fig4:e},\ref{fig11:abla_app_5},\ref{fig12:abla_app_6},\ref{fig13:abla_app_7},\ref{fig15:um/umap_tsne},\ref{fig16:tsne_UM},\ref{fig16:tsne_UMAP}.
\end{itemize}



\begin{figure*}[t!]
    \begin{center}
    
    \subfigure[UM]{
    \includegraphics[scale=0.24]{fig2e_tsne_100_UM_kde.pdf}
    \label{fig15:umtsne}
    }
    \subfigure[UMAP]{
    \includegraphics[scale=0.24]{fig2e_tsne_100_UMAP_kde.pdf}
    \label{fig15:umaptsne}
    }
    \end{center}
    \vspace{-4mm}
    \caption{Similar to previous Figure~\ref{fig2:e}, we visualize the learned feature by UM and UMAP: (a) TSNE visualization of UM; (b) TSNE visualization of UMAP. Compared to the visualization in Figure~\ref{fig2:e}, it is apparent that ID distribution and OOD distribution have larger intervals, which indicates that the UM/UMAP-processed model can better distinguish ID and OOD samples. Moreover, the OOD distributions are more united compared to those in Figure~\ref{fig2:e}, indicating better OOD discriminative Capability.
    }
    \label{fig15:um/umap_tsne}
    \vspace{-2mm}
\end{figure*}

\begin{figure*}[t!]
    \begin{center}
    
    \subfigure[Epoch 60]{
    \includegraphics[scale=0.151]{fig2e_tsne_60.pdf}
    \label{fig16:tsne_60}
    }
    \subfigure[Epoch 100]{
    \includegraphics[scale=0.151]{fig2e_tsne_100.pdf}
    \label{fig16:tsne_100}
    }
    \subfigure[UM]{
    \includegraphics[scale=0.151]{fig2e_tsne_UM.pdf}
    \label{fig16:tsne_UM}
    }
    \subfigure[UMAP]{
    \includegraphics[scale=0.151]{fig2e_tsne_UMAP.pdf}
    \label{fig16:tsne_UMAP}
    }
    \end{center}
    \vspace{-4mm}
    \caption{The visualization of the decision boundary: (a) decision boundary of epoch 60; (b) decision boundary of epoch 100; (c) decision boundary of UM; (d) decision boundary of UMAP. Based on Figures~\ref{fig2:e} and ~\ref{fig15:um/umap_tsne}, we further use the TSNE embeddings to simulate the decision boundaries generated with K-NN \citep{fix1989discriminatory}. The visualization shows that UM/UMAP can help exclude the OOD distribution from the ID distribution, making it easier for the model to distinguish ID/OOD data. While the OOD performance at epoch 60 is better than that at epoch 100, the OOD distribution gets mixed up with the ID distribution in both figures. Though the simulation can't act as a concrete reflection of the model's feature space, it still can intuitively explain the mechanism of UM/UMAP.
    }
    \label{fig16:decision_boundary}
    \vspace{-2mm}
\end{figure*}


\begin{figure}[t!]
    \begin{center}
    \subfigure[Layer-wise masking scores]{
    \includegraphics[scale=0.094]{fig3c_4X10_layer_wise_scores_compressed.pdf}
    \label{fig17:mine_atypical_a}
    }
    \subfigure[Layer-wise masking weights]{
    \includegraphics[scale=0.094]{fig3c_4X10_layer_wise_weight_compressed.pdf}
    \label{fig17:mine_atypical_b}
    }
    \subfigure[Model-wise masking scores]{
    \includegraphics[scale=0.094]{fig3c_4X10_model_wise_scores_compressed.pdf}
    \label{fig17:mine_atypical_c}
    }
    \subfigure[Model-wise masking weights]{
    \includegraphics[scale=0.094]{fig3c_4X10_model_wise_weight_compressed.pdf}
    \label{fig17:mine_atypical_d}
    }
    \end{center}
    \caption{Examples of misclassified samples after masking the original well-trained model on CIFAR-10. The scores are estimated according to the uniform distribution so that weights are masked out randomly. (a) layer-wise masking scores \citep{ramanujan2020s}; (b) layer-wise masking weights directly; (c) model-wise masking scores; (d) model-wise masking weights directly. For layer-wise masking, a fixed mask ratio is set for every layer of the model, meaning that the same ratio of weights is masked out for every layer; for model-wise masking, the model is considered as a whole with all weights united first and then masked according to the mask ratio. When masking scores, we first generate a score for every weight according to the uniform distribution and then mask out those weights with smaller scores; when masking weights, we directly mask out weights with smaller magnitudes. The images show that masking weights can't efficiently detect atypical samples while masking scores can. For masking scores, masking with a smaller ratio forces the model to misclassify simple samples (clear contours around subjects, single color background) while masking with a larger ratio guide the model to misclassify complex samples (unclear contours, noisy background). This inspection empirically supports our claims that with proper mask ratio, we can detect atypical samples and therefore can force the model to forget them. Besides that, mask weights don't show any particular difference with different mask ratios, and thus it may be inappropriate for mining atypical samples.}
    \label{fig17:mine_atypical}
\end{figure}

\begin{figure}[t!]
    \begin{center}
    \subfigure[Layer-wise masking scores]{
    \includegraphics[scale=0.11]{fig17_largescale_4X15_layer_wise_scores_compressed_compressed.pdf}
    \label{fig18:mine_atypical_a_imagenet}
    }
    \subfigure[Layer-wise masking weights]{
    \includegraphics[scale=0.11]{fig17_largescale_4X15_layer_wise_weight_compressed_compressed.pdf}
    \label{fig18:mine_atypical_b_imagenet}
    }
    \subfigure[Model-wise masking scores]{
    \includegraphics[scale=0.11]{fig17_largescale_4X15_model_wise_scores_compressed_compressed.pdf}
    \label{fig18:mine_atypical_c_imagenet}
    }
    \subfigure[Model-wise masking weights]{
    \includegraphics[scale=0.11]{fig17_largescale_4X15_model_wise_weight_compressed_compressed.pdf}
    \label{fig18:mine_atypical_d_imagenet}
    }
    \end{center}
    \caption{Examples of misclassified samples after masking the original well-trained model on ImageNet.}
    \label{fig18:mine_atypica_imagenetl}
\end{figure}

\end{document}
