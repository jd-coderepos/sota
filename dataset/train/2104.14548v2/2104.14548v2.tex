\section{Experiments}

In this section we compare \methodname features with other state of the art self-supervised image representations. First, we provide details of our architecture and training process. Next, following commonly used evaluation protocol~\cite{grill2020bootstrap,chen2020simple,chen2020big,he2020momentum},
we compare our approach with other self-supervised features on linear evaluation and semi-supervised learning on the ImageNet ILSVRC-2012 dataset. Finally we present results on transferring self-supervised features to other downstream datasets and tasks. 



\subsection{Implementation details}

\noindent\textbf{Architecture.} We use ResNet-50~\cite{he2016deep} as our encoder to be consistent with the existing literature ~\cite{grill2020bootstrap,chen2020simple}.
We spatially average the output of ResNet-50 which makes the output of the encoder a 2048-d embedding. The architecture of the projection MLP is  fully connected layers of sizes  where  is the embedding size used to apply the loss. We use  in the experiments unless otherwise stated. All fully-connected layers are followed by  batch-normalization~\cite{ioffe2015batch}. All the batch-norm layers except the last layer are followed by ReLU activation. The architecture of the prediction MLP  is  fully-connected layers of size .  The hidden layer of the prediction MLP is followed by batch-norm and ReLU. The last layer has no batch-norm or activation.


\noindent\textbf{Training.} Following other self-supervised methods~\cite{grill2020bootstrap,chen2020simple,chen2020big,he2020momentum}, we train our \methodname representation on the ImageNet2012 dataset which contains  images, without using any annotation or class labels. We train for  epochs with a warm-up of  epochs with cosine annealing schedule using the LARS optimizer~\cite{you2017large}. Weight-decay of  is applied during training. As is common practice~\cite{grill2020bootstrap,chen2020big}, we don't apply weight-decay to the bias terms. We use the data augmentation scheme used in BYOL~\cite{grill2020bootstrap} and we use a temperature  of  when applying the softmax during computation of the contrastive loss in Equation~\ref{eqn:nnclrloss}. The best results of \methodname are achieved with  queue size and base learning rate~\cite{grill2020bootstrap} of .


\subsection{ImageNet evaluations}

\noindent\textbf{ImageNet linear evaluation.} Following the standard linear evaluation procedure~\cite{grill2020bootstrap,chen2020simple} we train a linear classifier for  epochs on the frozen -d embeddings from the ResNet-50 encoder using LARS with cosine annealed learning rate of  with Nesterov momentum of  and batch size of . 

Comparison with state of the art methods is presented in Table~\ref{tab:main_results}. First, \methodname achieves the best performance compared to all the other methods using a ResNet-50 encoder trained with two views. \methodname provides more than  improvement over well known constrastive learning approaches such as MoCo v2~\cite{chen2020improved} and SimCLR v2~\cite{chen2020big}. Even compared to InfoMin Aug.\ \cite{tian2020makes}, which explicitly studies ``good view'' transformations to apply in contrastive learning, \methodname achieves more than  improvement on top-1 classification performance. We outperform BYOL\cite{grill2020bootstrap} (which is the state-of-the-art method among methods that use two views) by more than .
 
We also achieve  improvement compared to the state of the art clustering based method SwAV~\cite{caron2020unsupervised} in the same setting of using two views. 
To compare with SwAV's multi-crop models, we pre-train for  epochs with  views (two  and six  views) using only the larger views to calculate the NNs. In this setting our method outperforms SwAV by  in Top-1 accuracy. Note that while multi-crop is responsible for  performance improvement for SwAV, for our method it provides a boost of only .
However, increasing the number of crops quadratically increases the memory and compute requirements, and is quite costly even when low-resolution crops are used as in~\cite{caron2020unsupervised}.



\noindent\textbf{Semi-supervised learning on ImageNet.} We evaluate the effectiveness of our features in a semi-supervised setting on ImageNet  and  subsets following the standard evaluation protocol~\cite{grill2020bootstrap, chen2020big}. 
We present these results in Table~\ref{tab:semi_supervised_learning}. The first key result of Table~\ref{tab:semi_supervised_learning} is that our method outperforms all the state of the art methods on semi-supervised learning on ImageNet  subset, including SwAV's~\cite{caron2020unsupervised} multi-crop setting.
This is a clear indication of good generalization capacity of \methodname features, particularly in low-shot learning scenarios. Using the ImageNet  subset, \methodname outperforms SimCLR~\cite{chen2020simple} and other methods. However, SwAV's~\cite{caron2020unsupervised} multi-crop setting outperforms our method in ImageNet  subset.



\begin{table}[]
    \centering
    \begin{tabular}{l|cc}
         Method & Top-1 & Top-5\\
         \midrule

         PIRL~\cite{misra2020self} & 63.6 & - \\
         CPC v2~\cite{henaff2020data} & 63.8 & 85.3 \\
         PCL~\cite{li2020prototypical} & 65.9 & - \\         
         CMC~\cite{tian2019contrastive} & 66.2 & 87.0 \\
          
         MoCo v2~\cite{chen2020improved} & 71.1  & - \\
         SimSiam~\cite{chen2020exploring} & 71.3  & - \\
         SimCLR v2~\cite{chen2020big} & 71.7 & - \\
         SwAV~\cite{caron2020unsupervised}  & 71.8 & - \\
         InfoMin Aug.~\cite{tian2020makes} & 73.0 & 91.1\\
         BYOL~\cite{grill2020bootstrap} & 74.3 & 91.6\\ 

         \methodname (ours) & \textbf{75.4} & \textbf{92.3} \\
         \midrule
         SwAV (multi-crop)~\cite{caron2020unsupervised} & 75.3 & - \\  
         \methodname (ours) (multi-crop) & \textbf{75.6} & \textbf{92.4}\\  
        
    \end{tabular}
    \caption{{\bf ImageNet linear evaluation results.} Comparison with other self-supervised learning methods on ResNet-50 encoder. Methods on the top section use two views only.
    }
    \label{tab:main_results}
\end{table}


\begin{table}[]
    \centering
    \begin{tabular}{l|c c|c c}
          & \multicolumn{2}{c|}{ImageNet \bf{1\%}} &  \multicolumn{2}{c}{ImageNet \bf{10\%}} \\
         Method & Top-1 & Top-5 & Top-1 & Top-5 \\
         \midrule
         Supervised & 25.4 & 48.4 & 56.4 & 80.4\\
         \midrule
         InstDisc~\cite{wu2018unsupervised} & - & 39.2 & - & 77.4\\
         PIRL~\cite{misra2020self} & -  & 57.2 & - & 83.8\\
         PCL~\cite{li2020prototypical} & - & 75.6 & - & 86.2\\ 
         SimCLR~\cite{chen2020simple} & 48.3 & 75.5 & 65.6 & 87.8\\
         BYOL~\cite{grill2020bootstrap} & 53.2  & 78.4 & 68.8 & 89.0\\

         \methodname (ours) & \textbf{56.4} & \textbf{80.7} & \textbf{69.8} & \textbf{89.3} \\
         \midrule SwAV (multi-crop)~\cite{caron2020unsupervised}& 53.9  & 78.5 & 70.2 & 89.9
    \end{tabular}
    \caption{{\bf Semi-supervised learning results on ImageNet.} Top-1 and top-5 performances are reported on fine-tuning a pre-trained ResNet-50 with ImageNet  and  datasets. }
    \label{tab:semi_supervised_learning}
\end{table}


\begin{table*}[]
\footnotesize
    \centering
    \begin{tabular}{l|cccccccccccc}
         Method & Food101 & CIFAR10 & CIFAR100 &  Birdsnap & SUN397 &  Cars & Aircraft & VOC2007 &  DTD & Pets & Caltech-101 & Flowers\\
         \midrule
         BYOL~\cite{grill2020bootstrap} &  75.3 & 91.3 &  78.4  & 57.2 & 62.2   & \textbf{67.8} &  60.6& 82.5 & 75.5 & 90.4 & 94.2 & \textbf{96.1} \\
         SimCLR~\cite{grill2020bootstrap} & 72.8 & 90.5 & 74.4 & 42.4 & 60.6 & 49.3  & 49.8 & 81.4 & \textbf{75.7} & 84.6 & 89.3 & 92.6 \\
         Sup.-IN~\cite{chen2020simple} & 72.3 & 93.6 & 78.3 &53.7 & 61.9 & 66.7 & 61.0& 82.8 & 74.9 & 91.5 & \textbf{94.5} & 94.7 \\
         \methodname & \textbf{76.7}  & \textbf{93.7} & \textbf{79.0}  & \textbf{61.4} & \textbf{62.5}  & 67.1 & \textbf{64.1} & \textbf{83.0} & 75.5  & \textbf{91.8} & 91.3 & 95.1\\
         
    \end{tabular}
    \caption{\textbf{Transfer learning performance} using ResNet-50 pretrained with ImageNet. For all datasets we report Top-1 classification accuracy except Aircraft, Caltech-101, Pets and Flowers for which we report mean per-class accuracy and VOC2007 for which we report 11-point MAP.}
    \label{tab:transfer_learning}
\end{table*}


\subsection{Transfer learning evaluations}
\label{sec:transfer_learning}
We show representations learned using \methodname are effective for transfer learning on multiple downstream classification tasks on a wide range of datasets. We follow the linear evaluation setup described in~\cite{grill2020bootstrap}. The datasets used in this benchmark are as follows: Food101~\cite{bossard14}, CIFAR10~\cite{Krizhevsky09learningmultiple}, CIFAR100~\cite{Krizhevsky09learningmultiple}, Birdsnap~\cite{berg-birdsnap-cvpr2014}, Sun397~\cite{Xiao:2010}, Cars~\cite{KrauseStarkDengFei-Fei_3DRR2013}, Aircraft~\cite{maji13fine-grained}, VOC2007~\cite{pascal-voc-2007}, DTD~\cite{cimpoi14describing}, Oxford-IIIT-Pets~\cite{parkhi12a}, Caltech-101~\cite{FeiFei2004LearningGV} and Oxford-Flowers~\cite{Nilsback08}. Following the evaluation protocol outlined in~\cite{grill2020bootstrap}, we first train a linear classifier using the training set labels while choosing the best regularization hyper-parameter on the respective validation set. Then we combine the train and validation set to create the final training set which is used to train the linear classifier that is evaluated on the test set. 

We present transfer learning results in Table~\ref{tab:transfer_learning}. \methodname outperforms supervised features (ResNet-50 trained with ImageNet labels) on 11 out of the 12 datasets. Moreover our method improves over BYOL~\cite{grill2020bootstrap} and SimCLR~\cite{chen2020simple} on 8 out of the 12 datasets. These results further validate the generalization performance of \methodname features.

\subsection{Ablations}
\label{sec:ablations}
In this section we present a thorough analysis of \methodname. 
After discussing the default settings, we start by demonstrating the effect of training with nearest-neighbors in a variety of settings. 
Then, we present several design choices such as support set size, varying k in top-k nearest neighbors, type of nearest neighbors, different training epochs, variations of batch size, and embedding size. 
We also briefly discuss memory and computational overhead of our method.

\noindent \textbf{Default settings.} Unless otherwise stated our support set size during ablation experiments is  and our batch size is .  
We train for 1000 epochs with a warm-up of 10 epochs, base learning rate of  and cosine annealing schedule using the LARS optimizer~\cite{you2017large}. We also use the prediction head by default. All the ablations are performed using the ImageNet linear evaluation setting.

\noindent \textbf{Nearest-neighbors as positives.} Our core contribution in this paper is using nearest-neighbors (NN) as positives
in the context of contrastive self-supervised learning.
Here we investigate how this particular change, using nearest neighbors as positives, affects performance in various settings with and without momentum encoders. This analysis is presented in Table~\ref{tab:nn_ablation}. First we show using the NNs in contrastive learning (row 2) is  better in Top-1 accuracy than using view 1 embeddings (similar to SimCLR) shown in row 1. We also explore using momentum encoder (similar to MoCo~\cite{he2020momentum}) in our contrastive setting. Here using NNs also improves the top-1 performance by . 

\begin{table}[]
    \centering
    \begin{tabular}{c|c|cc}
         Mom. Enc. & Positive & Top-1  & Top-5 \\
         \midrule
    & View 1 & 71.4 & 90.4\\

           & NN of View 1 & \textbf{74.5} & \textbf{91.9} \\
         \midrule
          \checkmark & View 1 &  72.5 & 91.3 \\
          \checkmark & NN of View 1  & \textbf{74.9} & \textbf{92.1}\\
    \end{tabular}
    \caption{\textbf{Effect of adding nearest-neighbors as positives} in various settings. Results are obtained for ImageNet linear evaluation.}
    \label{tab:nn_ablation}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|l|l|l}
         Method & SimCLR~\cite{chen2020simple} & BYOL~\cite{grill2020bootstrap} & \methodname\\
         \midrule
         Full aug. & 67.9 & 72.5  & 72.9\\
         Only crop & 40.3 \footnotesize{( -27.6)}& 59.4 \footnotesize{( -13.1)} & \textbf{68.2} \footnotesize{( -4.7)}\\
    \end{tabular}
    \caption{\textbf{Performance with only crop augmentation.} ImageNet top-1 performance for linear evaluation is reported.}
    \label{tab:data_augmentation}
\end{table}

\noindent \textbf{Data Augmentation.} Both SimCLR~\cite{chen2020simple} and BYOL~\cite{grill2020bootstrap} rely heavily on a well designed data augmentation pipeline to get the best performance. However, \methodname is less dependent on complex augmentations as nearest-neighbors already provide richness in sample variations. In this experiment, we remove all color augmentations and Gaussian blur, and train with random crops as the only method of augmentation for 300 epochs following the setup used in~\cite{grill2020bootstrap}. We present the results in Table~\ref{tab:data_augmentation}. We notice \methodname achieves  top-1 performance on the ImageNet linear evaluation task suffering a performance drop of only . On the other hand, SimCLR and BYOL suffer larger relative drops in performance,  and  respectively. The performance drop reduces further as we train our approach longer. With  pre-training epochs, \methodname with all augmentations achieves  while with only random crops \methodname manages to get , further reducing the gap to just .  While \methodname also benefits from complex data augmentation operations, the reliance on color jitter and blurring operations is much less. This is encouraging for adopting \methodname for pre-training in domains where data transformations used for ImageNet might not be suitable.

\begin{table}[]
    \centering
    \begin{tabular}{l|cccc}
         Method & 100 & 200 & 400 & 800\\
         \midrule
         SimCLR~\cite{chen2020simple} & 66.5 & 68.3 & 69.8 & 70.4 \\
         MoCov2~\cite{chen2020improved} & 67.4 & 69.9 & 71.0 &  72.2\\
         BYOL~\cite{grill2020bootstrap} & 66.5& 70.6& 73.2 & 74.3 \\
         SWAV~\cite{caron2020unsupervised} & 66.5 & 69.1 & 70.7 & 71.8\\
         SimSiam~\cite{chen2020exploring} & 68.1 & 70.0 & 70.8 & 71.3\\
         \midrule
         \methodname & \textbf{69.4} & \textbf{70.7} & \textbf{74.2} & \textbf{74.9}  \\
         
        
    \end{tabular}
    \caption{\textbf{Number of pre-training epochs vs.\ performance.} Results are obtained for ImageNet linear evaluation.}
    \label{tab:pretraining_spochs}
\end{table}
\noindent \textbf{Pre-training epochs.} In Table~\ref{tab:pretraining_spochs}, we show how our method compares to other methods when we have different pre-training epoch budgets. \methodname is better than other self-supervised methods when pre-training budget is kept constant. We find that base learning rate of  works best for  epochs, and  works for ,  and  epochs. 

\noindent \textbf{Support set size.} Increasing the size of the support set increases performance in general. We present results of this experiment in Table~\ref{tab:queuesize}.  
By using a larger support set, we increase the chance of getting a closer nearest-neighbour from the full dataset. As also shown in Table~\ref{tab:topk}, getting the closest (i.e.\ top-1) nearest-neighbour obtains the best performance, even compared against top-2. 

We also find that increasing the support set size beyond  doesn't lead to any significant increase in performance possibly due to an increase in the number of stale embeddings in the support set.




\noindent \textbf{Nearest-neighbor selection strategy.} 
Instead of using the nearest-neighbor, we also experiment with taking one of the top- NNs randomly. These results are presented in Table~\ref{tab:topk}. Here we investigate whether increasing the diversity of nearest-neighbors (i.e.\ increasing ) results in improved performance. 
Although our method is somewhat robust to changing the value of , we find that increasing the top- beyond  always results in slight degradation in performance. 
Inspired by recent work~\cite{dwibedi2019temporal, frosst2019analyzing} we also investigate using a soft-nearest neighbor, a convex combination of embeddings in the support-set where each one is weighted by its similarity to the embedding (see~\cite{dwibedi2019temporal} for details). We present results in Table~\ref{tab:hard_vs_soft}. We find that the soft nearest neighbor can be used for training but results in worse performance than using the hard NN. 

\noindent \textbf{Batch size.} Batch size has shown to be an important factor that affects performance, particularly in the contrastive learning setting. We vary the batch size and present the results in Table~\ref{tab:batch_size}. In general, larger batch sizes improve the performance peaking at .

\noindent \textbf{Embedding size.} Our method is robust to choice of the embedding size as shown in Table~\ref{tab:embedding_size}. We vary the embedding size in powers of 2 from 128 to 2048 and find similar performance over all settings.



\noindent \textbf{Prediction head} As shown in Table~\ref{tab:predhead}, adding a prediction head results in a modest  boost in the top-1 performance. 

\noindent \textbf{Different implementations of support set.}
We also investigate some variants of how we can implement the support set from which we sample the nearest neighbor. We present results of this experiment in Table~\ref{tab:memory_variations}. In the first row, instead of using a queue, we pass a random set of images from the dataset through the current encoder and use the nearest neighbor from that set of embeddings. This works reasonably well but we are limited by how many examples we can fit in accelerator memory. Since we cannot increase the size of this set beyond , which results in sub-optimal performance. 
Also using a random set of size  is about four times slower than using a queue (when training with a batch size of ) as each forward pass requires four times more samples through the encoder. This experiment also shows that \methodname does not need features of past samples (akin to momentum encoders ~\cite{he2020momentum}) to learn representations. We also experiment with updating the elements in the support set randomly as opposed to the default FIFO manner. We find that FIFO results in more than  better ImageNet linear evaluation Top-1 accuracy.


\begin{table*}[h]
\begin{subtable}[h]{0.5\textwidth}
    \centering
    \begin{tabular}{c|ccccc}
         Queue Size & 8192 & 16384 & 32768 & 65536 & 98304 \\
         \midrule
         Top-1 & 73.6 &  74.2 & 74.9 &  75.0 & \textbf{75.4}\\
         Top-5 & 91.2 &  91.7 & 92.1 &  92.2 & \textbf{92.3}\\
 
    \end{tabular}
    \caption{Support set size}
    \label{tab:queuesize}
   \end{subtable}
   \hfill
  \begin{subtable}[h]{0.5\textwidth}
    \centering
    \begin{tabular}{c|cccccc}
       \textit{k} in \textit{k}-NN  & 1& 2 & 4 & 8 & 16 & 32\\
         \midrule
         Top-1 & \textbf{74.9} & 74.1 & 73.8 &  73.8 & 73.8 & 73.2\\
         Top-5 & \textbf{92.1} & 91.6 & 91.5  & 91.4 & 91.3 & 91.2\\
    \end{tabular}
    \caption{Varying k in k-NN}
    \label{tab:topk}
    \end{subtable}
    \hfill
    
	\begin{subtable}[h]{0.5\textwidth}
		\centering
    \centering
    \begin{tabular}{c|cccccc}
         Batch size & 256 & 512 & 1024 & 2048 & 4096 & 8192\\
         \midrule
         Top-1 & 68.7 & 71.7 & 72.9 & 73.5 & \textbf{74.9 }& 74.3 \\
         Top-5 & 88.7 & 90.4 & 91.1 & 91.6 & \textbf{92.1} & 91.9\\
    \end{tabular}
    \caption{Batch size.}
    \label{tab:batch_size}
	\end{subtable}
	\hfill
	\begin{subtable}[h]{0.5\textwidth}
		\centering
		\begin{tabular}{c|ccccc}
          & 128 & 256 & 512 & 1024 & 2048\\
         \midrule
         Top-1 & 74.9 & 74.9 & 74.8 & 74.9 & 74.6\\
         Top-5 & 92.1 & 92.1 & 92.0 & 92.0 & 92.0\\
    \end{tabular}
    \caption{Varying embedding size }
    \label{tab:embedding_size}
	\end{subtable}
	\hfill
	
	\begin{subtable}[h]{0.5\textwidth}
    \centering
    \begin{tabular}{c|cc}
        Type of NN & Top-1 & Top-5\\
        \midrule
        Soft nearest-neighbor  & 71.4 & 90.4\\
        Hard nearest-neighbor  & {\bf 74.9} & {\bf 92.1}\\        
 
         
    \end{tabular}
    \caption{Soft vs. hard nearest neighbors as positives.}
    \label{tab:hard_vs_soft}
\end{subtable}
\hfill
\begin{subtable}[h]{0.5\textwidth}
    \centering
    \begin{tabular}{c|cc}
      Prediction MLP & Top-1 & Top-5\\
        \midrule
          &  74.5 & 92.0\\
        \checkmark & {\bf 74.9} & {\bf 92.1}\\        
    \end{tabular}
    \caption{Effect of prediction head.}
    \label{tab:predhead}
\end{subtable}\\

\caption{\textbf{\methodname Ablation Experiments.} Results are obtained for ImageNet linear evaluation.}
\end{table*}


\begin{table}[]
\small
    \centering
    \begin{tabular}{c|c|cc}
        Support set variant & Size & Top-1 & Top-5\\
         \midrule
         NNs from current encoder & 16384 & 74.0 & 91.8 \\
         NNs from queue (older embeddings) & 16384 & 74.2 & 91.7 \\
        
    \end{tabular}
    \caption{\textbf{Different implementations of the support set.}}
    \label{tab:memory_variations}
\end{table}


\noindent \textbf{Compute overhead.} We find increasing the size of the queue results in improved performance but this improvement comes at a cost of additional memory and compute required during training. In Table~\ref{tab:compute} we show how queue scaling with  affects memory required during training and number of training steps per second. With a support size of about  elements we require a modest 100 MB more in memory.

\begin{table}[]
\small
    \centering
    \begin{tabular}{c|ccccc}
         Queue Size & 8192 & 16384 & 32768 & 65536 & 98304\\
         \midrule
         Memory (MB) &  8.4 & 16.8 & 33.6 & 67.3& 100.8\\
         Steps per sec & 6.41 & 6.33 & 6.14 & 5.95 & 5.68  \\
 
    \end{tabular}
    \caption{\textbf{Queue size computational overheads.}}
    \label{tab:compute}
\end{table}

\subsection{Discussion}

\noindent \textbf{Ground Truth Nearest Neighbor.} We investigate two aspects of the NN: first, how often does the NN have the same ImageNet label as the query; and second, if the NN is always picked to be from the same ImageNet class (with an Oracle algorithm), then what is the effect on training and the final performance? Figure~\ref{fig:queue_accuracy}, shows how the accuracy of the NN picked from the queue varies as training proceeds. We observe that towards the end of training the accuracy of picking the right neighbor (i.e.\ from the same class) is about . The reason that it is not higher is possibly due to random crops being of the background, and thus not containing the object described by the ImageNet label. 

We next investigate if \methodname can achieve better performance if our top-1 NN is always from the same ImageNet class. This is quite close to the supervised learning setup except instead of training to predict classes directly, we train using our self-supervised setup. A similar experiment has also been described as \textit{UberNCE} in \cite{Han20}. This experiment verifies if our training dynamics prevent the model from converging to the performance of a supervised learning baseline even when the true NN is known. To do so, we store the ImageNet labels of each element in the queue and always pick a NN with the same ImageNet label as the query view. We observe that with such a setting we achieve  accuracy in  epochs. With the Top-1 NN from the support set, we manage to get  in 300 epochs. This suggests that there is still a possibility of improving performance with a better NN picking strategy, although it might be hard to design one that works in a purely unsupervised way.


\noindent \textbf{Training curves.} In Figure~\ref{fig:losscurves} we show direct comparison between training using cross-entropy loss with an augmentation of the same view as positive (SimCLR) and training with NN as positive (\methodname). The training loss curves indicate \methodname is a more difficult task as the training needs to learn from hard positives from other samples in the dataset. Linear evaluation on ImageNet classification shows that it takes about  epochs for \methodname to start outperforming SimCLR, and remains higher until the end of pre-traininig at  epochs.



\noindent \textbf{NNs in Support Set.} In Figure~\ref{fig:nn} we show a typical batch of nearest neighbors retrieved from the support set towards the end of training.  Column 1 shows examples of view 1, while the other elements in each row shows the retrieved nearest neighbor from the support set. We hypothesize that the improvement in performance is due to this diversity introduced in the positives, something that is not covered by pre-defined data augmentation schemes. We observe that while many times the retrieved NNs are from the same class, it is not uncommon for the retrieval to be based on other similarities like texture. For example, in row 3 we observe retrieved images are all of underwater images and row 4 retrievals are not from a dog class but are all images with cages in them.  


\begin{figure}[t]
\begin{center}
   \includegraphics[width=.98\linewidth]{images/qacc}
\end{center}
   \caption{\textbf{NN Match Accuracy vs. Performance.}}
\label{fig:queue_accuracy}
\end{figure}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=.98\linewidth]{images/losscurves}
\end{center}
   \caption{\textbf{\methodname  vs SimCLR} Training curves and linear evaluation curves.}
\label{fig:losscurves}
\end{figure}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{images/nn}
\end{center}
\caption{\textbf{Nearest neighbors from support set} show the increased diversity of positives in \methodname.}
\label{fig:nn}
\end{figure}
