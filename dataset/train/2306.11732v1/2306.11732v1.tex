\section{Experiments}

\begin{table*}[t]
    \centering
    \adjustbox{width=1.0 \linewidth}{
\begin{tabular}{@{}l|cc|cc|ccccccc@{}}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Language} & \multicolumn{2}{c|}{Vision} & \multicolumn{6}{c}{Benchmarks}  \\
         & model & \#params & model & \#params & MSRVTT-QA & MSVD-QA & ANet-QA & TGIF-QA & iVQA & NextQA\\ 
        \midrule
        \multicolumn{5}{@{}l}{\it Training based Adaption} \\
        CLIP ViT-L/14 \cite{clip}     & Custom & 123M & ViT-L/14 & 300M & 2.1 & 7.2 & 1.2 & 3.6 & 9.2 & -\\
        Just Ask \cite{justask}       & DistilBERT & 66M & S3D & 12M & 5.6 & 13.5 & 12.3 & - & 13.3 & -\\
        Reserve \cite{merlot_reserve} & Custom & - & ViT-L/16 & 300M & 5.8 & - & - & - & -  & -\\
        Flamingo-3B \cite{flamingo}   & Chinchilla-like & 2.6B  & NFNet-F6 & 629M & 11.0 & 27.5 & - & - & 32.7 & 21.3\\
        Flamingo-9B \cite{flamingo}   & Chinchilla-like & 8.7B  & NFNet-F6 & 629M & 13.7 & 30.2 & - & - & 35.2 & 23.0\\
        Flamingo-80B \cite{flamingo}  & Chinchilla-like & 80B   & NFNet-F6 & 629M & 17.4 & 35.6 & - & - & \textbf{40.7} & 26.7\\
        FrozenBiLM \cite{frozenbilm}  & DeBERTa-v2-XL   & 890M  & ViT-L/14 & 300M & 16.9 & 33.7 & 25.9 & 41.9 & 26.2 & - \\
        \midrule
        \multicolumn{5}{@{}l}{\it Language based Adaption} \\
         VidIL* \cite{vidil} & DeBERTa-v2-XL & 890M  &ViT-L/14 & 300M & 16.6 & 31.7 & - & - & - & - \\
        R2A (Ours)    & DeBERTa-v2-XL   & 890M  & ViT-L/14 & 300M & \textbf{18.3} & {\bf 37.0} & {\bf 26.3} & \textbf{52.2} & 29.3 & \textbf{34.7}  \\
\bottomrule
    \end{tabular}
    } \vspace{-2mm}
    \caption{{\bf Comparison with state-of-the-arts on zero-shot videoQA.} 50 retrieved sentences are used per video and the prompt is ``{\tt Hints:}''. *indicates replacement of LM beyond original definitions by authors for fair comparisons and also due to a lack of access to the original Language Model (GPT-3).
} 
\label{tab:sota}
\end{table*} 
We first describe the VideoQA datasets used for our evaluations (Section~\ref{subsec:data}). \
We then give the implementation details of our model (Section~\ref{subsec:implementation}). Next, we compare our R2A with the state of the art methods
(Section~\ref{subsec:sota}). 
Finally, we ablate the effect of different components, \eg, the choice of pretrained models, the number of retrieval samples and the construction of the text corpus (Section~\ref{subsec:ablation}). 


\subsection{Datasets and Evaluation} 
\label{subsec:data}
In this work, we focus on the challenging open-ended VideoQA datasets where the model has to generate an open-ended answer for each video-question pair. We test multiple zero-shot VideoQA benchmarks with data collected from sources of great diversity  (\ie, YouTube videos, Sports Videos, GIFs), including \textbf{MSRVTT-QA}~\cite{msrvtt}, \textbf{MSVD-QA}~\cite{msrvtt}, \textbf{ActivityNet-QA}~\cite{activitynet-qa}, \textbf{TGIF-QA}~\cite{tgif}, \textbf{iVQA}~\cite{justask} and \textbf{NextQA}~\cite{xiao2021next}.
We report top-1 accuracy based on exact matching between the predicted answer and ground-truth annotation following previous evaluation protocols~\cite{allinone}. 
\begin{figure*}
    \centering
    \includegraphics[page=1,width=0.32\textwidth]{figs/msrvtt_vis.pdf}
    \includegraphics[page=2,width=0.32\textwidth]{figs/msrvtt_vis.pdf}
    \includegraphics[page=3,width=0.32\textwidth]{figs/msrvtt_vis.pdf}
\vspace{-3mm}
    \caption{{\bf Qualitative comparison of model predictions with and without retrieved captions.}  For illustrative purposes, we highlight words in \textcolor{orange}{orange} to indicate  answer cues from captions. Words in \textcolor{YellowGreen}{green} indicate correct answer predictions and in \textcolor{red}{red} for incorrect ones.}
    \vspace{-3mm}
    \label{fig:vis}
\end{figure*} 
For the external corpus, we consider text extracted from various multi-modal datasets, including two datasets scraped from web: (1) \textbf{WebVid}~\cite{frozenintime} that contains approximately 10M video-text pairs,
(2) \textbf{CC3M}~\cite{cc} and \textbf{CC12M}~\cite{cc12m} that consists of 3M and 12M image-text pairs, as well as a human-annotated dataset: (3) \textbf{COCO Caption} dataset~\cite{coco} with 1.5M human-generated captions describing over 330K images. 




\subsection{Implementation details}
\label{subsec:implementation}

For video-to-text retrieval (sec.~\ref{method:vidtxt_ret}), we use the ViT-L/14 variant of CLIP~\cite{clip}. For the LLMs, we adopt DeBERTa-XL~\cite{deberta} as our default language model. 
It is worth mentioning that we utilize the MLM pre-trained model checkpoint, which means it has never been trained on QA-related tasks in any modality. Unless stated otherwise, we set 500 as the maximum input length for our language models. We base our implementation on the officially released code of \cite{frozenbilm} and the unmentioned details follow their implementation. 


\vspace{-3mm}

\paragraph{Visual feature extraction} We extract frame features using the ViT-L/14 variant of CLIP~\cite{clip}. The frame preprocessing follows the official implementation of \cite{clip}. We uniformly sample 10 frames from each video and extract one feature vector for each frame by taking the output of the $\texttt{[CLS]}$ token. 


\vspace{-3mm}

\paragraph{Video-to-Text Retrieval} The feature similarity calculation is identical to the original CLIP implementation. We use the naive algorithm for the nearest neighbor search (\ie, calculating the similarity between all pairs and selecting the top-$k$ for each query).  
More advanced nearest neighbor search algorithms (\eg \cite{guo2020accelerating}) can be used for larger datasets.  Duplicate entries are removed from the retrieved set.


\vspace{-3.5mm}

\paragraph{Multi-modal Fine-Tuning} 
For the multi-modal fine-tuning paradigm, we learn a linear projection for visual features as stated in Section \ref{method:ft}. We use an Adam optimizer with a constant learning rate of 1.5e-5, {\it no} weight decay and $\beta_1, \beta_2 = 0.9, 0.95$. By default, we use a batch size of 64, limit the input sequence length to 64 for training, and train for only one epoch on each dataset. We alter each input token with a probability of 0.5. 


\subsection{Comparison with the State-of-the-arts}
\label{subsec:sota}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.24\textwidth]{figs/num_ret_caps_MSRVTT-QA.pdf}
    \includegraphics[width=0.24\textwidth]{figs/num_ret_caps_MSVD-QA.pdf}
    \includegraphics[width=0.24\textwidth]{figs/num_ret_caps_ActivityNet-QA.pdf}
    \includegraphics[width=0.24\textwidth]{figs/num_ret_caps_TGIF-QA.pdf}
    \vspace{-2mm}
    \caption{{\bf Effects of the number of retrieved captions per video on zero-shot VideoQA.} WebVid-10M is used as the retrieval set and ``{\tt Subtitles:}'' is used as the prompt in all experiments. }
    \vspace{-3mm}
    \label{fig:num_retrieve}
\end{figure*} 
\paragraph{Quantitative comparison} Table~\ref{tab:sota} presents the results of our method in comparison with current state-of-the-art approaches on zero-shot VideoQA. Our method outperforms approaches that were additionally trained on million to billion-scale vision-language data, except on iVQA, where our method underperforms Flamingo~\cite{flamingo}. However, it is worth noting that Flamingo, while trained with billion-scale data, even the smallest version has significantly more parameters than our method.
Furthermore, when using the same language model, our method consistently improves over VidIL~\cite{vidil}, which utilizes a caption model to connect video with the language model. These results confirm the effectiveness of our method, as well as the informativeness of the retrieved captions for VideoQA.

\vspace{-3.5mm}


\paragraph{Qualitative comparison} Figure~\ref{fig:vis} illustrates qualitative results of zero-shot VideoQA for our \modelname~in comparison to FrozenBiLM~\cite{frozenbilm} and the text-only baseline without access to visual information (w/o retrieval). First, we observe that for questions only baseline, the LM can predict answers based on commonsense reasoning, \eg in the second example, it is very likely for a teacher to write problems on paper if we do not consider the visual input.  Second, for FrozenBiLM, predictions can be misled by inaccurate visual information (\eg in the first example, it predicts ``paint'' instead of ``egg''). In contrast, our R2M can predict the correct answer based on high-quality informative context retrieved from the supportive text corpus.

\vspace{-3.5mm}

\paragraph{Efficiency analysis} As shown in Table~\ref{tab:efficiency}, the inference time per video for our R2A is 0.11s, which can be further broken down into video encoding latency, retrieval latency, and LM inference latency. For FrozenBiLM, the inference time is 0.07s, which is composed of video encoding latency and LM inference latency. Given the exact same video input setup, the video encoding time is identical for our method and FrozenBiLM. Our LM inference latency is slightly larger than FrozenBiLM's because our input sequence (including the retrieved contextual text) is longer. 
It is worth noting that, the time consumed by retrieval is only 3.5ms (2,800 queries per second on a single A100 GPU from 10M samples) which is almost negligible compared to the total inference time. In addition to the inference time, we also report the pre-inference preparation time which is cross-modal training time for Flamingo and FrozenBiLM. In the case of R2A, it is the time needed to extract the features of the retrieval set which is fractional compared to the time required for cross-modal training. Overall, our results demonstrate that R2A achieves competitive performance while maintaining efficient inference time and requiring minimal pre-inference preparation.



\begin{table}[t]
    \centering
    \adjustbox{width=0.9\linewidth}{
    \begin{tabular}{@{}lccc@{}}
        \toprule
        Method & \begin{tabular}{@{}c@{}}Pre-inference\\computation\end{tabular} & \begin{tabular}{@{}c@{}} Inference\\ time per video\end{tabular} \\
        \midrule
Flamingo-80B \cite{flamingo} & 500Kh (TPUv4) & - \\
        FrozenBiLM \cite{frozenbilm} & 160h (V100) & 0.07s \\
        \midrule
R2A (Ours) & 1.4h (V100) & 0.11s \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-1mm}
    \caption{{\bf Efficiency comparison.} We compare efficiency with two previous methods. {\it Pre-inference} includes the cross-modal training cost for training based methods and retrieval set feature extraction cost for ours. We report efficiency using 10 retrieved sentences per video, at which {\it R2A} outperforms FrozenBiLM on all tasks.}
    \label{tab:efficiency}
\end{table}

%
 
\subsection{Ablation Studies}
\label{subsec:ablation}

\begin{table}[]
    \centering
    \adjustbox{width=\linewidth}{
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        Ret. modality & MSRVTT-QA & MSVD-QA & ANet-QA & TGIF-QA \\
        \midrule
        Video-Video & 15.7 & 32.6 & 25.0 & 45.2 \\
        Video-Text & {\bf 18.3} & {\bf 37.0} & \bf{26.3} & \bf{52.2} \\
        \bottomrule
    \end{tabular}
    } \vspace{-1mm}
    \caption{{\bf Video-Text vs. Video-Video retrieval.} 50 retrieved sentences are used per video and the prompt is ``{\tt Hints:}''.}
    \label{tab:v2v_vs_v2t}
\end{table} 
\begin{table}[]
    \centering
    \adjustbox{width=\linewidth}{
    \begin{tabular}{@{}ccccc@{}}
        \toprule
        ret.? & MSRVTT-QA & MSVD-QA & ANet-QA & TGIF-QA \\
        \midrule
        \multicolumn{5}{@{}l}{\it Language model: BERT-Base} \\
        \xmark & 1.4 & 2.9 & 16.0 & 10.4 \\
        \cmark & 3.0 (+1.6) & 7.5 (+4.6) & 15.4 (-0.6) & 17.4 (+7.0) \\
        \midrule
        \multicolumn{5}{@{}l}{\it Language model: BERT-Large} \\
        \xmark & 2.5 & 3.9 & 15.6 & 13.9\\
        \cmark & 7.0 (+4.5) & 15.0 (+11.1) & 19.6 (+4.0) & 26.7 (+12.8) \\
        \midrule
         \multicolumn{5}{@{}l}{\it Language model: RoBERTa-Large} \\
        \xmark & 3.0 & 5.5 & 18.5 & 13.2 \\
        \cmark & 13.6 (+10.6) & 24.3 (+18.8) & 18.8 (+0.3) & 32.0 (+18.8) \\
        \midrule
        \multicolumn{5}{@{}l}{\it Language model: DeBERTa-v2-xlarge (default)} \\
        \xmark & 6.4 & 11.3 & 22.5 & 32.3  \\
        \cmark & 16.8 (+10.4) & 36.0 (+24.7) & 26.1 (+3.6) & 49.5 (+17.2) \\
        \bottomrule
    \end{tabular}
    } \vspace{-1mm}
    \caption{{\bf Performance of the training-free setting with alternative language models.} WebVid-10M is used as the retrieval set and 10 sentences are retrieved for each video. The prompt is ``{\tt Subtitles:}''. ``ret.?'' denotes whether to use retrieval or not.}
    \label{tab:lm}
    \vspace{-4mm}
\end{table} 
\begin{table}[h]
    \centering
    \adjustbox{width=\linewidth}{
    \begin{tabular}{l@{}cccc@{}}
        \toprule
        Retrieval database & MSRVTT-QA & MSVD-QA & ANet-QA & TGIF-QA \\
        \midrule
        \multicolumn{5}{@{}l}{\it No retrieval dataset (baseline)} \\
        N/A & 6.4 & 11.3 & 22.5 & 32.3 \\
        \midrule
        \multicolumn{5}{@{}l}{\it Down-sampling the retrieval dataset} \\
        WV-10k & 15.6 & 30.3 & 25.0 & 44.2 \\
        WV-100k & 16.2 & 34.1 & 25.1 & 47.8 \\
        WV-1M & 16.3 & 35.6 & 25.8 & 48.5\\
        WV-10M & 16.8 & {\bf 36.0} & {\bf 26.1} & 49.5 \\
        \midrule
        \multicolumn{5}{@{}l}{\it Using the Conceptual Captions (CC) datasets} \\
        CC-3M & 14.2 & 30.2 & 25.0 & 47.1 \\
        WV-10M + CC-3M & 15.9 & 34.7 & 26.0 & 49.5 \\
        CC-12M & 10.9 & 23.6 & 23.4 & 43.2 \\
        WV-10M + CC-12M & 13.8 & 31.6 & 25.6 & 46.4 \\
        \midrule
        \multicolumn{5}{@{}l}{\it Using human-annotated captioning datasets} \\
        COCO & 16.4 & 31.2 & 23.9 & 45.3 \\
        COCO + WV-10M & {\bf 16.9} & {\bf 36.0} & 26.0 & {\bf 49.7}  \\
        \bottomrule
    \end{tabular}
    } \vspace{-1mm}
    \caption{{\bf Effects of retrieval database construction.} 
WV stands for the WebVid dataset \cite{frozenintime}. We use 10 retrieved sentences per video and the prompt ``{\tt Subtitles:}'' in all experiments. Note that we only use the textual part of each of the dataset. 
    }
\label{tab:base_set}
\end{table} 
\begin{table}[h]
    \centering
    \adjustbox{width=\linewidth}{
    \begin{tabular}{@{}l@{}cccc@{}}
        \toprule
        Retrieval database & MSRVTT-QA & MSVD-QA & ANet-QA & TGIF-QA \\
        \midrule
        N/A & 6.4 & 11.3 & 22.5 & 32.3 \\
        \midrule
        CC-12M (Random) & 7.7 & 10.0 & 19.6 & 30.1 \\
        CC-12M (Ours) & 10.9 & 23.6 & 23.4 & 43.2 \\
        \midrule
        WV-10M (Random) & 9.5 & 18.5 & 22.3 & 31.4 \\
        WV-10M (Ours) & 16.8 & 36.0 & 26.1 & 49.5 \\
        \bottomrule
    \end{tabular}
    } \vspace{-2mm}
    \caption{{\bf Retrieval vs. random sample.} {\tt Subtitles:} is used as prompt and 10 sentences are used per video in all experiments.}
    \label{tab:retrieve_vs_random}
    \vspace{-4mm}
\end{table}


 

\paragraph{Retrieval dataset construction} Table~\ref{tab:base_set} presents the results of our method equipped with different text corpora for context retrieval. To investigate the impact of dataset size, we conduct experiments with various sample sizes. As shown in the second section of Table~\ref{tab:base_set}, the model's performance consistently improves with the increase in dataset size. Even a small dataset of 10k samples can bring a noticeable improvement over models without retrieved captions. We observe that the performance gain tends to be smaller when using CC-3M and CC-12M as the retrieval set, which we attribute to the dataset quality issues mentioned in \cite{mapl}. Specifically, we find some texts in the CC datasets unreadable, which can negatively affect the model's performance. Our experiments on COCO Captions reaffirm this hypothesis, as we observe higher performance gains for high-quality captions annotated by humans, even with much smaller dataset sizes. Unless otherwise stated, we use the full WebVid-10M as the default retrieval dataset.


\vspace{-3.5mm}

\paragraph{Impact of the number of retrieved captions}
We plot the top-1 accuracy against the number of retrieved captions, as shown in Figure~\ref{fig:num_retrieve}. The accuracy on all four datasets consistently increases as more retrieved captions are used, demonstrating the robustness of our method. Notably, we observe a substantial performance boost even when using only one retrieved caption, indicating the importance of the retrieval operation in assisting VideoQA. Furthermore, we find that the accuracy continues to improve with an increasing number of retrieved captions. These findings suggest that our method can effectively leverage external sources of information for VideoQA task.

\vspace{-3.5mm}

\paragraph{Impact of the quality of retrieved captions} To demonstrate the effectiveness of the retrieval in providing relevant information for VideoQA, we compare our results with those obtained by randomly sampling text to feed into the LM. As shown in Table~\ref{tab:retrieve_vs_random}, our R2A consistently outperform random sampling on all datasets and retrieval sources by a large margin. Surprisingly, we observe that randomly sampled text show improved performance over the question-only baseline on MSRVTT and MSVD. We hypothesize that this is due to the prior distributions of some retrieval datasets, which may serve as task-specific prompts \cite{coop} for certain target datasets.



\vspace{-3.5mm}

\paragraph{Video-Text vs. Video-Video Retrieval}
Apart from using only text as the external corpus, we experiment with retrieving video-text pairs: For a video-text dataset, we find the samples with the highest video-video similarity and take the corresponding text as the retrieved captions. As shown in Table~\ref{tab:v2v_vs_v2t}, the video-video retrieval is significantly worse on all four datasets.
We conjecture that video-video similarity is more vulnerable to data noise (\ie, some video-text pairs themselves may not be well aligned, especially for those web-crawled data), which consequently makes the retrieved text not well correlated with the query video.
Moreover, retrieving directly from the text corpus is also advantageous in term of storage (\ie, no need to store the images or videos) and flexibility (\ie, able to use both visual-text and text-only data).


\vspace{-3.5mm}

\paragraph{Impact of pretrained language models} We also experiment with alternative pretrained language models and report our results in Table~\ref{tab:lm}. Except for one case (BERT-base on ActivityNet-QA), we observe that the retrieved contexts significantly improve performance, sometimes even doubling the accuracy on all LMs. Notably, our models benefit more from stronger language models, as we observe larger gains with the increase in language model size. 
\vspace{-3.5mm}



\paragraph{Effects of prompts} To investigate the impact of prompt design on our method, we conduct experiments using a few hand-crafted prompts. We replace the words before the retrieved captions to probe the language model. The results are presented in Table~\ref{tab:prompt}. We find that there is no significant variation among the choices attempted, but some words such as {\tt Hints:}'' and {\tt Contexts:}'' perform slightly better on all datasets than the others, such as {\tt Subtitles:}'' and {\tt Captions:}''. Further optimization of the prompting words may potentially improve the performance.
\begin{table}[t]
    \centering
    \adjustbox{width=\linewidth}{
    \begin{tabular}{@{}ccccc@{}}
        \toprule
        Prompt & MSRVTT-QA & MSVD-QA & ANet-QA & TGIF-QA \\
        \midrule
        {\tt Subtitles:} & 17.3 & 36.6 & 26.2 & 51.1 \\
        {\tt Captions:} & 17.2 & 36.8 & 26.2 & 50.9 \\
        {\tt Hints:} & {\bf 18.0} & 36.8 & 26.3 & {\bf 51.2} \\
        {\tt Contexts:} & 17.6 & {\bf 36.9} & {\bf 26.4} & 51.1 \\
        \bottomrule
    \end{tabular}
    } \vspace{-2mm}
    \caption{{\bf Effects of using different prompts.} We use WebVid-10M as the retrieval set and 20 retrieved sentences in all experiments.}
    \vspace{-6mm}
    \label{tab:prompt}
\end{table} \vspace{-3.5mm}
\paragraph{Learning Visual-Text Projection for \shortmodelname} 
In Table~\ref{tab:ft_dataset_size}, we analyze the impact of multi-modal training on R2A. For efficiency purposes, we train R2A on various subsets of WebVid~\cite{frozenintime} for one epoch. Our findings reveal that while fine-tuning with relatively larger cross-modal data (\eg, 500K), there are some improvements on MSRVTT and TGIF. However, fine-tuning with smaller cross-modal data (\eg, 10k or 50k) can hamper the model's performance in the original setup. 
Future improvements may require dedicating more effort to architecture design and utilizing more training data. While beyond the scope of this study, we find this direction intriguing and encourage further research in this area.



\begin{table}[t]
    \centering
    \adjustbox{width=\linewidth}{
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        Size & MSRVTT-QA & MSVD-QA & ANet-QA & TGIF-QA \\
        \midrule
        0 & 18.3 & {\bf 37.0} & {\bf 26.3} & 52.2 \\
        \midrule
        10k & 12.8 & 26.8 & 22.8 & 32.7 \\
        50k & 17.5 & 34.3 & 25.6 & 42.8 \\
        200k & 16.7 & 33.7 & 26.0 & 43.1 \\
        500k & {\bf 19.7} & 36.8 & 25.8 & {\bf 52.5} \\
        \bottomrule
    \end{tabular}
    } \vspace{-2mm}
    \caption{{\bf Learning Visual-Text Projection for R2A.} In each experiment, we downsample the WebVid-10M dataset to the given size, and fix the number of training {\it epochs} to one.}
    \vspace{-5mm}
    \label{tab:ft_dataset_size}
\end{table}


%
 
\vspace{-2mm}