\section{Experiments: General domain}
\label{sec:experiments}
We experiment with the proposed approach \methodname in a general domain first. We pretrain \methodname using the Book corpus and ConceptNet KG (\S \ref{sec:exp-pretrain-setup}), and evaluate on diverse downstream tasks (\S \ref{sec:exp-eval}). We show that \methodname significantly improves on existing models (\S \ref{sec:exp-result}). We extensively analyze the effect of \methodname's key design choices such as self-supervision and use of KGs (\S \ref{sec:analysis-reasoning}, \ref{sec:analysis-pretrain}, \ref{sec:exp-ablation}).
We also experiment in the biomedical domain in \S \ref{sec:experiments-biomed}. 


\subsection{Pretraining setup}
\label{sec:exp-pretrain-setup}

\heading{Data.}
For the text data, we use BookCorpus \cite{Zhu_2015_ICCV}, a general-domain corpus widely used in LM pretraining (e.g., BERT, RoBERTa). It has 6GB of text from online books.
For the KG data, we use {ConceptNet} \citep{speer2016conceptnet}, a general-domain knowledge graph designed to capture background commonsense knowledge. It has 800K nodes and 2M edges in total.
To create a training instance, we sample a text segment of length up to 512 tokens from the text corpus, then retrieve a relevant KG subgraph of size up to 200 nodes (details in Appendix \ref{app:setup:linking}), by which we obtain an aligned (text, local KG) pair.


\heading{Implementation.}
For our encoder (\S \ref{sec:method-enc}), we use the exact same architecture as GreaseLM \cite{zhang2022greaselm} (19 LM layers followed by 5 text-KG fusion layers; 360M parameters in total). As done by \cite{zhang2022greaselm}, we initialize parameters in the LM component with the RoBERTa-Large release \cite{liu2019roberta} and initialize the KG node embeddings with pre-computed ConceptNet entity embeddings (details in Appendix \ref{app:setup:graph}). 
For the link prediction objective (\S \ref{sec:method-pretrain}, Equation \ref{eq:linkpred-loss}), we use DistMult \cite{yang2015embedding} for KG triplet scoring, with a negative exampling of 128 triplets and a margin of .
To pretrain the model, we perform MLM with a token masking rate of 15\% and link prediction with an edge drop rate of 15\%.
We pretrain for 20,000 steps with a batch size of 8,192 and a learning rate of 2e-5 for parameters in the LM component and 3e-4 for the others. 
Training took 7 days on eight A100 GPUs using FP16.
Additional details on the hyperparameters can be found in Appendix \ref{app:setup:hyperparameters}.





\subsection{Downstream evaluation tasks}
\label{sec:exp-eval}

We finetune and evaluate \methodname on nine diverse commonsense reasoning benchmarks: 
{CommonsenseQA (\textbf{CSQA})} \cite{talmor2018commonsenseqa},
{OpenbookQA (\textbf{OBQA})} \cite{obqa}, 
{RiddleSense (\textbf{Riddle})} \cite{lin2021riddlesense},
{AI2 Reasoning Challenge\! --\! Challenge Set (\textbf{ARC})} \cite{clark2018think},
{\textbf{CosmosQA}} \cite{huang2019cosmos},
{\textbf{HellaSwag}} \cite{zellers2019hellaswag},
{Physical Interaction QA (\textbf{PIQA})} \cite{bisk2020piqa},
{Social Interaction QA (\textbf{SIQA})} \cite{sap2019socialiqa}, and
{Abductive Natural Language Inference (\textbf{aNLI})} \cite{bhagavatula2019abductive}. 
For CSQA, we follow the in-house data splits used by prior works \cite{lin2019kagnet}. For OBQA, we follow the original setting where the models only use the question as input and do not use the extra science facts.
Appendix \ref{app:setup:eval} provides the full details on these tasks and data splits.
Hyperparameters used for finetuning can be found in Appendix \ref{app:setup:hyperparameters}.




\subsection{Baselines}
\label{sec:exp-baseline}


\heading{LM.}
To study the effect of using KGs, we compare \methodname with the vanilla language model, RoBERTa \cite{liu2019roberta}. 
As we initialize \methodname's parameters using the RoBERTa-Large release (\S \ref{sec:exp-pretrain-setup}), for fair comparison, we let the baseline be such that we take the RoBERTa-Large release and continue pretraining it with the vanilla MLM objective on the same text data for the same number of steps as \methodname. Hence, the only difference is that \methodname uses KGs during pretraining while RoBERTa does not.
We then perform standard LM finetuning of RoBERTa on downstream tasks.


\heading{LM finetuned with KG.}
We also compare with existing KG-augmented QA models, QAGNN \cite{yasunaga2021qa} and GreaseLM \cite{zhang2022greaselm}, which \textit{finetune} a vanilla LM (i.e.~RoBERTa-Large) with a KG on downstream tasks, but do not \textit{pretrain} with a KG. GreaseLM is the existing top-performing model in this paradigm.
As we use the same encoder architecture as GreaseLM for \methodname, the only difference from GreaseLM is that \methodname performs self-supervised pretraining while GreaseLM does not.







\subsection{Results}
\label{sec:exp-result}

Table \ref{tab:commonsense_main} shows performance on the 9 downstream commonsense reasoning tasks. Across all tasks, \methodname consistently outperforms the existing LM (RoBERTa) and KG-augmented QA models (QAGNN, GreaseLM), e.g., +7\% absolute accuracy boost over RoBERTa and +5\% over GreaseLM on \textit{OBQA}. 
These accuracy boosts indicate the advantage of \methodname over RoBERTa (KG reasoning) and over GreaseLM (pretraining).
The gain is especially significant on datasets that have small training data such as \textit{ARC}, \textit{Riddle} and \textit{OBQA}, and datasets that require complex reasoning such as \textit{CosmosQA} and \textit{HellaSwag}, which we analyze in more detail in the following sections.



\subsubsection{Analysis: Effect of knowledge graph}
\label{sec:analysis-reasoning}
The first key contribution of \methodname (w.r.t. existing LM pretraining methods) is that we incorporate KGs.
We find that this significantly improves the model's performance for robust and complex reasoning, such as resolving multi-step reasoning and negation, as we discuss below.


\begin{table*}
\vspace{-6mm}
\centering
  \scalebox{0.8}{
  \small
  \begin{tabular}{l ccccccccc}
    \toprule
     & \textbf{CSQA} & \textbf{OBQA} & \textbf{Riddle} & \textbf{ARC} & \!\!\textbf{CosmosQA}\!\!  & \!\!\textbf{HellaSwag}\!\! & \textbf{PIQA} & \textbf{SIQA} & \textbf{aNLI}  \\
    \midrule
    RoBERTa \cite{liu2019roberta} & 68.7 & 64.9 & 60.7 & 43.0 & 80.5 & 82.3 & 79.4 & 75.9 & 82.7 
    \\
    \midrule
    QAGNN \cite{yasunaga2021qa}   & 73.4 & 67.8 & 67.0 & 44.4 & 80.7 & 82.6 & 79.6 & 75.7 & 83.0 
    \\
    GreaseLM \cite{zhang2022greaselm} & 74.2 & 66.9 & 67.2 & 44.7 & 80.6 & 82.8 & 79.6 & 75.5 & 83.3 
    \\
    \midrule
    \methodname (\textbf{Ours})
    & \textbf{76.0} & \textbf{72.0} & \textbf{71.3} & \textbf{48.6} & \textbf{82.3} & \textbf{85.2} & \textbf{81.1} & \textbf{76.8} & \textbf{84.0}  
    \\
    \bottomrule
  \end{tabular}}
  \vspace{-1mm}
  \caption{\footnotesize
  Accuracy on downstream commonsense reasoning tasks. \methodname consistently outperforms the existing LM (RoBERTa) and KG-augmented QA models (QAGNN, GreaseLM) on all tasks. The gain is especially significant on tasks that have small training data (\textit{OBQA}, \textit{Riddle}, \textit{ARC}) and tasks that require complex reasoning (\textit{CosmosQA}, \textit{HellaSwag}).
  }
  \label{tab:commonsense_main}
  \vspace{-1mm}
\end{table*}



 \begin{table}[t]
    \centering
    \scalebox{0.7}{
    \small
    \begin{tabular}{l rrrrrrrr }
        \toprule
        \multirow{2}{*}{\textbf{}} & \textbf{Negation} & \textbf{Conjunction} & \textbf{Hedge}  & \multicolumn{4}{c}{\# \textbf{Prepositional Phrases}}  & \multicolumn{1}{c}{\# \textbf{Entities}}\\ 
        & \textbf{} & \textbf{} & \textbf{} & 0 & 1 & 2 & 3  & >10 \\
        \midrule
        RoBERTa   & 61.7 & 70.9 & 68.6 &  67.6 & 71.0 & 71.1 & 73.1 & 74.5 
        \\
        \midrule
        QAGNN     & 65.1 & 74.5 & 74.2 & 72.1 &	71.6 & 75.6 & 71.3 & 78.6 
        \\
        GreaseLM  & 65.1 & 74.9 & 76.6  & 75.6 & 73.8 & 74.7 & 73.6 & 79.4 
        \\
        \midrule
        \methodname (\textbf{Ours}) & \textbf{75.2} & \textbf{79.6} & \textbf{77.5}  & \textbf{79.1} &	\textbf{78.2} & \textbf{77.8} &\textbf{ 80.9} & \textbf{83.5} 
        \\
        \bottomrule
    \end{tabular}
    }
    \vspace{1mm}
    \caption{\footnotesize
    Accuracy of \methodname on \textit{CSQA} + \textit{OBQA} dev sets for \textbf{questions involving complex reasoning} such as negation terms, conjunction terms, hedge terms, prepositional phrases, and more entity mentions. 
    \methodname consistently outperforms the existing LM (RoBERTa) and KG-augmented QA models (QAGNN, GreaseLM) in these complex reasoning settings.
    }
    \label{tab:analysis}
\vspace{-4mm}
\end{table} \heading{Quantitative analysis.}
In Table \ref{tab:analysis}, we study downstream task performance of \methodname on questions involving complex reasoning. Building on \cite{yasunaga2021qa, zhang2022greaselm}, we consider several proxies to categorize complex questions: (i) presence of negation (e.g.~\textit{no}, \textit{never}), (ii) presence of conjunction (e.g.~\textit{and}, \textit{but}), (iii) presence of hedge (e.g.~\textit{sometimes}, \textit{maybe}), (iv) number of prepositional phrases, and (v) number of entity mentions. Having negation or conjunction indicates logical multi-step reasoning, having more prepositional phrases or entity mentions indicates involving more reasoning steps or constraints, and having hedge terms indicates involving complex textual nuance.
\methodname significantly outperforms the baseline LM (RoBERTa) across all these categories (e.g., +14\% accuracy for negation), which confirms that our joint language-knowledge pretraining boosts reasoning performance.
\methodname also consistently outperforms the existing KG-augmented QA models (QAGNN, GreaseLM). We find that QAGNN and GreaseLM only improve moderately on RoBERTa for some categories like conjunction or many prepositional phrases (=2,\,3), but \methodname provides substantial boosts. This suggests that through self-supervised pretraining with larger and diverse data, \methodname has learned more general-purpose reasoning abilities than the finetuning-only models like GreaseLM.




\begin{figure}[t]
\vspace{-2mm}
    \hspace{-3mm}
    \includegraphics[width=1.03\textwidth]{fig_qualitative_v5.pdf}\vspace{-1mm}
    \caption{\footnotesize
    Analysis of \methodname's graph reasoning, where we visualize how graph attention weights and final predictions change given question variations. Darker and thicker edges indicate higher attention weights. \textbf{\methodname exhibits abilities to extrapolate and perform robust reasoning}. \methodname adjusts the entity attention weights and final predictions accordingly when conjunction or negation is given about entities (A1, A2) or when extra context is added to an original question (B1B2), but existing models, RoBERTa and GreaseLM, struggle to predict the correct answers.
    \textbf{A1:} \methodname's final GNN layer shows strong attention to ``school'' but weak attention to ``trip'', likely because the question states ``\textit{and} store one''---hence, the chair is \textit{not} used for a trip. 
    \textbf{A2:} \methodname shows strong attention to ``trip'' and ``beach'', likely because the question now states ``\textit{but not} store one''---hence, the chair \textit{is} used for a trip.
    \textbf{B1B2:} \methodname's final GNN layer shows strong attention to ``movie'' in the original question (B1), but after adding the extra context ``don't enjoy pre-record'' (B2), \methodname shows strong attention to ``live'' and ``concert'', leading to making the correctly adjusted prediction ``concert hall''.
    One interpretation of these findings is that \methodname leverages the KG's graph structure as a scaffold for performing complex reasoning. 
    This insight is related to recent works that provide LMs with scratch space for intermediate reasoning \cite{yasunaga2021qa, nye2021show, wei2022chain}.
    }
    \label{fig:qualitative}
    \vspace{1mm}
\end{figure}

\heading{Qualitative analysis.}
Using the \textit{CSQA} dataset, we further conducted case studies on the behavior of \methodname's KG reasoning component, where we visualize how graph attention weights change given different question variations (Figure \ref{fig:qualitative}). We find that \methodname exhibits abilities to extrapolate and perform robust reasoning. For instance, \methodname adjusts the entity attention weights and final predictions accordingly when we add conjunction or negation about entities (A1, A2) or when we add extra context to an original question (B1B2), but existing models, RoBERTa and GreaseLM, struggle to predict the correct answers. 
As these questions are more complex than ones typically seen in the \textit{CSQA} training set, our insight is that while vanilla LMs (RoBERTa) and finetuning (GreaseLM) have limitation in learning complex reasoning, KG-augmented pretraining (\methodname) helps acquire generalizable reasoning abilities that extrapolate to harder test examples.




\subsubsection{Analysis: Effect of pretraining}
\label{sec:analysis-pretrain}
\begin{table}[t]
\begin{minipage}{0.458\textwidth}
    \centering
    \scalebox{0.7}{
    \small
    \begin{tabular}{lcc}
    \toprule
    \textbf{Method} & \textbf{CosmosQA} \scalebox{0.8}{\textbf{(10\% train)}} & \textbf{PIQA} \scalebox{0.8}{\textbf{(10\% train)}}   \\
    \midrule
    {RoBERTa}  & 72.2 & 66.4 \\
    {GreaseLM} & 73.0 & 67.0 \\
    \midrule
    \methodname (\textbf{Ours}) &  \textbf{77.9} &  \textbf{72.3} \\
    \bottomrule
    \end{tabular}
    }\vspace{1.5mm}
    \caption{\footnotesize
     Performance in low-resource setting where 10\% of finetuning data is used. \methodname attains large gains, suggesting its benefit for downstream data efficiency.
    }\vspace{1mm}
    \label{tab:low-data}
    
    \centering
    \scalebox{0.7}{
    \small
    \begin{tabular}{lcc}
    \toprule
    \textbf{Method} & \textbf{CSQA} & \textbf{OBQA}   \\
    \midrule
    {GreaseLM}  & 74.2 & 66.9 \\
    {GreaseLM-Ex} & 73.9 & 66.2 \\
    \midrule
    \methodname (\textbf{Ours})  & 76.0 & 72.0  \\
    \methodname-Ex (\textbf{Ours}) & \textbf{76.3} & \textbf{72.8} \\
    \bottomrule
    \end{tabular}
    }\vspace{1.5mm}
    \caption{\footnotesize
    Downstream performance when model capacity---number of text-KG fusion layers---is increased (``-Ex''). Increased capacity does not help for the finetuning-only model (GreaseLM), but helps when pretrained (\methodname), suggesting the promise of \methodname to be further scaled up.
    }
    \label{tab:model-cap}
\end{minipage}\hfill
\begin{minipage}{0.52\textwidth}
    \centering
\scalebox{0.65}{
    \small
    \begin{tabular}{rlcc}
     \toprule
    \textbf{Ablation Type}\!\! & \textbf{Ablation} & \!\!\textbf{CSQA}\!\! & \!\!\textbf{OBQA}\!\! \\
    \midrule
    \!\!\multirow{3}{*}{Pretraining objective}\!\! & MLM\,+\,LinkPred (\textbf{final})\!\! &  \textbf{76.0} & \textbf{72.0} \\
    & MLM only & 74.3 & 67.2 \\
    & LinkPred only & 73.8 & 66.4 \\
    \midrule
    \!\multirow{3}{*}{LinkPred head}\!\! & DistMult (\textbf{final}) & \textbf{76.0} & \textbf{72.0} \\
    &  TransE & 75.7 & 71.4 \\
    &  RotatE & 75.8 & 71.7  \\
    \midrule
    \!\multirow{2}{*}{Cross-modal model}\!\! & Bidirectional interaction (\textbf{final}) & \textbf{76.0} & \textbf{72.0} \\
     & Concatenate at end & 74.5 & 68.0 \\
    \midrule
    \!\multirow{2}{*}{KG structure}\!\! & Use graph (\textbf{final}) & \textbf{76.0} & \textbf{72.0} \\
    &  Convert to sentence & 74.7 & 70.1 \\
    \bottomrule
    \end{tabular}}
    \vspace{2mm}
    \caption{\footnotesize
    {Ablation study} of \methodname. Using joint pretraining objective MLM + LinkPred (\S \ref{sec:method-pretrain}) outperforms using one of them only. All variants of LinkPred scoring models (DistMult, TransE, RotatE) outperform the baseline without LinkPred (``MLM only''), suggesting that \methodname can be combined with various KG representation learning models. Cross-modal model with bidirectional modality interaction (\S \ref{sec:method-enc}) outperforms combining text and KG representations only at the end.
    Finally, using KG as graph outperforms converting KG as sentences, suggesting the benefit of graph structure for reasoning. 
    }
    \label{tab:ablation}
\end{minipage}\vspace{-2mm}
\end{table}
 Another key contribution of \methodname (w.r.t. existing QA models like GreaseLM) is pretraining. Here we discuss when and why our pretraining is useful.
Considering the three core factors in machine learning (data, task complexity, and model capacity), pretraining helps when the available downstream task data is smaller compared to the downstream task complexity or model capacity.
Concretely, we find that \methodname is especially helpful for the following three scenarios.

\heading{Downstream tasks with limited data.}
In Table \ref{tab:commonsense_main}, we find that \methodname provides significant boosts over GreaseLM on downstream tasks with limited finetuning data available, such as \textit{ARC} (3K training instances; +4\% accuracy gain), \textit{Riddle} (3K instances; +4\% accuracy) and \textit{OBQA} (5K instances; +5\% accuracy).
For other tasks, we also experimented with a low-resource setting where 10\% of finetuning data is used (Table \ref{tab:low-data}). Here we also see that \methodname attains significant gains over GreaseLM (+5\% accuracy on \textit{PIQA}), suggesting the improved data-efficiency of \methodname.


\heading{Complex downstream tasks.}
In Table \ref{tab:commonsense_main}, we find that \methodname provides substantial gains over GreaseLM on downstream tasks involving more complex reasoning, such as \textit{CosmosQA} and \textit{HellaSwag}, where the inputs have longer context and more entities (thus bigger local KGs). For these tasks, improvements of GreaesLM over RoBERTa were small (+0.1\% on \textit{CosmosQA}), but \methodname provides substantial boosts (+1.8\%). Our insight is that through self-supervised pretraining with larger and more diverse data, \methodname has learned richer text-KG interactions than GreaseLM, enabling solving more complex downstream tasks.
Similarly, as seen in \S \ref{sec:analysis-reasoning}, \methodname also attains large gains over GreaseLM on complex questions containing negation, conjunction and prepositional phrases (Table \ref{tab:analysis}), and extrapolates to questions more complex than seen in training sets (Figure \ref{fig:qualitative}).


\heading{Increased model capacity.}
In Table \ref{tab:model-cap}, we study downstream performance when the model capacity is increased---the number of text-KG fusion layers is increased from 5 to 7---for both GreaseLM and \methodname. We find that increased capacity does not help for the finetuning-only model (GreaseLM) as was also reported in the original GreaseLM paper, but it helps when pretrained (\methodname). This result reveals that increased model capacity can actually be beneficial when combined with pretraining, and suggests the promise of \methodname to be further scaled up.



\subsubsection{Analysis: Design choices of \methodname}
\label{sec:exp-ablation}


\heading{Pretraining objective {\rm (Table \ref{tab:ablation} top).}} The first important design choice of \methodname is the joint pretraining objective: MLM + LinkPred (\S \ref{sec:method-pretrain}). Using the joint objective outperforms using MLM or LinkPred alone (+5\% accuracy on \textit{OBQA}). This suggests that having the bidirectional self-supervised tasks on text and KG facilitates the model to fuse the two modalities for reasoning.
\1.5mm]
\heading{Cross-modal model {\rm (Table \ref{tab:ablation} middle 2).}} 
Another core component of \methodname is the cross-modal encoder with bidirectional text-KG fusion layers (\S \ref{sec:method-enc}). We find that if we ablate them and simply concatenate text and KG representations at the end, the performance drops substantially. This result suggests that deep bidirectional fusion is crucial to model interactions over text and KG for reasoning.
\1.5mm]
\heading{KG structure {\rm (Table \ref{tab:ablation} bottom).}}
The final key design of \methodname is that we leverage the graph structure of KGs via a sequence-graph encoder and link prediction objective. Here we experimented with an alternative pretraining method that drops the graph structure: we convert triplets in the local KG into sentences using a template \cite{feng2020scalable}, append them to the main text input, and perform vanilla MLM pretraining.
We find that \methodname substantially outperforms this variant (+2\% accuracy on \textit{OBQA}), which suggests that the graph structure of KGs helps the model perform reasoning.