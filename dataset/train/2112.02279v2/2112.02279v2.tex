

In this section, we will elaborate the proposed U-Former framework for image restoration. The overall pipeline of the U-Former is illustrated in Figure~\ref{fig1:pipline}. We first introduce the main architecture of the U-Former, and then describe how the contrastive learning guide U-Former to handle complex noise patterns. Finally, we describe specific supervision items for end-to-end parameter learning.

\begin{figure*}[!htbp]
    \centering
    \begin{minipage}[b]{0.95\linewidth}
    \includegraphics[width=\linewidth]{pic/detail.pdf}\vspace{4pt}
    \end{minipage}
    \vspace{-5pt}
    \caption{Main architecture our proposed U-Former for image restoration. (a) presents the outer U-shaped encoder-decoder structure, where each stage consists of an inner U-shaped Transformer Block (UTB) with different depth L as shown in (b). The detailed structure of the Transformer block in UTB is shown in (c), where we proposed a novel Feature-Filtering Window-based Multi-head Self-Attention (FW-MSA) for reducing the computational cost. The detailed structure of Feature-Filtering is illustrated in (d). Finally we introduce depthwise separable convolution in the Feed-Forward Network (FFN) for further capturing local dependencies as shown in (e).}
    \label{fig2:RUT}
    \vspace{-10pt}
\end{figure*}
\subsection{Network architecture of U-Former}
To overcome the limitations of existing Transformer-based structures in model depth and image reconstruction, in our U-Former, We embed multiple inner U-shaped Transformer blocks (UTB) inside the outer U-shaped encoder-decoder structure.


\subsubsection{Inner U-shaped Transformer block.}
The inner UTB is customized as the core operating unit in the U-Former, as shown in Figure~\ref{fig2:RUT}(b). 
Inspired by the CNN architecture in ~\cite{qin2020u2,szegedy2015going}, we design the UTB-L as a lightweight U-shaped Transformer architecture, where L denotes the depth of UTB. Specifically, the Transformer blocks (as shown in Figure~\ref{fig2:RUT}(c)), followed by downsampling and upsampling are introduced to achieve multi-scale feature maps from the input feature maps . Consequently, the global and local features of the image are further extracted, being beneficial for separating the noise component from the background branch. Besides, the mapped features  from UTB is fused with the input feature maps  through a residual connection, so that more comprehensive information is  is achieved.


\noindent\textbf{Feature Filtering Mechanism.}
To reduce computational cost of self-attention and avoid overfitting, we further propose a novel module named Feature-filtering Window-based Multi-head Self-Attention (FW-MSA) in Transformer block as shown in Figure~\ref{fig2:RUT}(c).
Specifically, it first utilizes the attention mechanism to obtain the attention weight of input features in the current feature dimension:

where  is the feature belonging to the -th feature dimension,  denotes the attention weight of the -th feature dimension,  is the Sigmoid function and  is the ReLU function.
Then, by empirically setting the threshold  to filter the attention weights, our FW-MSA selects those features with high weights and distill more valuable information to compute self-attention:

where  denotes the selected features by our proposed feature-filtering. The detailed structure of the feature-filtering is shown in Figure~\ref{fig2:RUT}(d). To avoid overfitting, the residual connection with the input features  is added to the corresponding outputs:

where the W-MSA denotes the window-based multi-head self-attention~\cite{liu2021swin}, LN is the layer normalization, and FFN is the feed-forward network as shown in Figure~\ref{fig2:RUT}(e). Importantly, the FFN here leverages depthwise separable convolution to capture local dependencies.

\subsubsection{Outer U-shaped Encoder-Decoder framework.} The outer U-shaped encoder-decoder framework is illustrated in Figure~\ref{fig2:RUT}(a). The encoder  consists of 5 stages, and two parallel decoders:  for decoupling background layer features and  for decoupling noise layer features, are both composed of 4 stages. Each stage contains a well-designed UTB-L. In the first four stages in , we employ UTB-5, UTB-4, UTB-3, and UTB-2, respectively. In fifth stage in , the resolution of the feature maps is already relatively low. To preserve the valuable features as much as possible, we remove the downsampling operation and merely stack 6 Transformer blocks. More specifically, given input feature maps , the -th stage of the encoder outputs the feature maps . For the decoders, each stage has a symmetric structure compared with that in the encoder. We take the upsampled feature maps and ones from each symmetric encoder stage as input for the next stage in the decoder. Each decoder stage can generate a side feature maps. We then upsample these feature maps to ensure their size to be the same to the input image size and fuse them through concatenation. Finally we generate the reconstructed clear background image  and noise images by  convolutional layer.
\subsection{Multi-view Contrastive Learning}
Contrastive learning is a discriminant-based approach that groups similar contents close and dissimilar contents away. Although it has demonstrated effectiveness in many high-level vision tasks, it still has great potential in image restoration due to the difficulties of semantic guidance in image synthesis. In the paper, we propose a novel multi-view contrastive learning scheme to guide our U-Former learning to remove complex noise patterns. As illustrated in Figure~\ref{fig_add:multi-view}, we first crop restored background image, noise image, and multiple groundtruth images from the same batch into patches. We then label background patches and groundtruth patches as positive samples, while noise patches as negative samples respectively. Contrastive learning are conducted in three aspects (views) when constructing positive pairs:
\begin{itemize}
    \item \textbf{View-1}: The noise distribution in the input image is not necessarily uniform. To ensure the restoring consistency between different regions in the same image, we take two patches from the same restored background image as positive pairs in view-1 contrastive learning.
    \item \textbf{View-2}: We pair the restored background image and the corresponding groundtruth background (with same image content) in patch level to be positive to guild the model to restore the clean background image.
    \item  \textbf{View-3}: We take the restored background image to a random groundtruth background image (with different image content) in patch level as positive pairs to encourage the model to focus on learning the features that are sensitive to noise rather than the image content.
\end{itemize}
The negative pairs are constructed in a fixed manner for all three views: comparing the restored background image and the restored noise image. In summary, through constructing contrastive pairs from multiple perspectives, we encourage our model to learn image degradation rather than similar image content. More specifically, we feed these patches to the encoder followed by extra two-layer fully connection named MLP to obtain feature embeddings for computing feature similarity:

where  denotes the feature embedding of -th image patch , and  is the encoder of the U-Former. In this way, the query patch, positive patches and negative patches are mapped into N-dimension vectors respectively. Thus, to maximize the mutual information between corresponding patches, we employ the noise contrastive estimation(NCE) framework~\cite{oord2018representation,park2020contrastive}, and then the binary classification is set up:
\begin{small}

\end{small}
where , , and  are the number of patches for query, positive and negative sets respectively;  and  denote the feature embeddings of positive samples and negative samples.
\begin{figure}[!tp]
    \centering
    \begin{minipage}[b]{0.95\linewidth}
    \includegraphics[width=\linewidth]{pic/multi-view.pdf}\vspace{4pt}
    \end{minipage}
\caption{Illustration of our proposed multi-view contrastive learning. We construct three types of positive pairs (corresponding to three views of contrastive learning) including: \textbf{View}-1: patches from the same restored image; \textbf{View}-2: patches from the restored background image and the corresponding groundtruth background (with same image content) and \textbf{View}-3 patches from  the restored background image and a random groundtruth background image (with different image content). Besides, the negative pairs are constructed by comparing the restored background image and the restored noise image in all three cases.}
    \label{fig_add:multi-view}
    \vspace{-10pt}
\end{figure}
\subsection{Jointly Supervised Parameter Learning}
We optimize the whole model of our U-Former in an end-to-end manner. 

\subsubsection{Multi-stage Pixel Reconstruction Loss.} We employ  loss to push the pixel values of generated background image  and noise image  in various stages as close as their groundtruth. The multi-stage pixel reconstruction loss is formulated as follows:
\begin{small}

\end{small}
where  and  denote groundtruth of the background image and noise image respectively,  and  denote the reconstructed background image and noise image in the -th stage,  and  are the weights of each loss term,  is the weight of each  loss term, and  is the number of stages of the decoder. Empirically, ,  and .


\subsubsection{Multi-stage Perceptual Loss.} The perceptual loss~\cite{johnson2016perceptual} is proposed to perform semantic supervision on generated images in the deep feature space. Specifically, the pretrained VGG-19~\cite{simonyan2014very} is exploited as the feature extractor:
\begin{small}

\end{small}
where  denotes perceptual distance between two deep features. Empirically, ,  and .

Therefore, the overall training loss is defined as:

where  and  are the hyper-parameters to balance various losses, and we empirically set them as  respectively.



