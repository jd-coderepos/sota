\def\imw#1#2{\includegraphics[width=#2\linewidth]{#1}}
\def\imh#1#2{\includegraphics[height=#2\textheight]{#1}}
\def\imwh#1#2#3{\includegraphics[width=#2\linewidth,height=#3\textheight]{#1}}


\section{Experiments}
\label{sec:exp}




We demonstrate that AMD model can be transferred to three downstream applications.
First,  the appearance pathway is directly applied on static images for salient object detection in a zero-shot fashion.
Second, both the appearance and the motion pathway are transferred to video object segmentation in novel videos with zero human labels.
Third, we fine-tune the appearance pathway on labeled data for semantic segmentation. 

\subsection{Training and Implementation Details}
\label{sec:imp}

AMD is pretrained on the large object-centric video dataset Youtube-VOS~\cite{xu2018youtube}.
The training split for Youtube-VOS contains about 4,000 videos covering 94 categories of objects. The total duration for the dataset is  minutes.
We train the model on the data with a sampling rate 24 frames per second, without using the original segmentation labels.

We train all model parameters from scratch without external pretraining. For the segmentation network, we use ResNet50~\cite{he2016deep} as our backbone followed by a fully convolutional head containing two convolutional blocks. For the motion network, we adopt PWC-Net~\cite{sun2018pwc} architecture because of its effectiveness in estimating optical flows. 
We resize the short edge of the input image to  pixels, and random crop a square image of size  with random horizontal flipping augmentation.
No other augmentations is engineered.
We adopt the symmetric loss that considers either frame as the target frame and sums the two reconstruction errors.
For training the overall model, we use the Adam optimizer with a learning rate of  and a weight decay of . 
We train AMD on eight V100 GPUs, with each processing two pairs of sampled adjacent frames.
The network is optimized for 400K iterations.


\subsection{Zero-Shot Saliency Detection}
\label{sec:saliency}

\def\prow#1#2#3#4{
\imw{figs/fig_saliency/#1.png}{0.244}&
\hspace{0.01cm}
\imw{figs/fig_saliency/#2.png}{0.244}&
\hspace{0.01cm}
\imw{figs/fig_saliency/#3.png}{0.244}&
\hspace{0.005cm}
\vrule
\hspace{0.005cm}
\imw{figs/fig_saliency/#4.png}{0.244}\\
}
\begin{figure}[t]
\centering
\subfloat{\small
    \tb{@{}cccc@{}}{0.05}{
    \prow{0}{1}{22}{12}
    \prow{3}{18}{21}{13}
    \prow{6}{7}{8}{0003}
    \prow{16}{19}{11}{20}
     & movable objects & &
 stationary objects
    }
}
\caption{Qualitative salient object detection results. We directly transfer our pretrained segmentation network to novel images on the DUTS dataset without any finetuning. Surprisingly, we find that the model pretrained on videos to segment moving objects can generalize to detect stationary unmovable objects in a static image, e.g. the statue, the plate, the bench and the tree in the last column.}
\label{fig:saliency}
\vspace{-7pt}
\end{figure}

Once pretrained, AMD's appearance pathway can be directly transferred to object segmentation in novel stationary images without any downstream fine-tuning. To evaluate the quality of the segmentation, we benchmark the results on salient object detection benchmark.


The salient object detection performance is measured on the \textbf{DUTS~\cite{wang2017learning}} benchmark, which contains 5,019 test images with pixel-level ground truth annotations.
We follow two widely used metrics in this area: the  score and the per-pixel mean squared errors (MAE).  is defined as the weighted harmonic mean of the precision () and recall () scores: , with .
MAE is simply the per-pixel averaged error of the soft prediction scores.


\textbf{Experimental results.} 
We compare our saliency estimation results against several traditional methods based on low-level cues.
Useful low-level cues and priors include background priors~\cite{zhu2014saliency}, objectness~\cite{jiang2013saliency,jiang2013salient}, and color contrast~\cite{cheng2014global}.
As shown in Table~\ref{tab:saliency}, our method achieves an  score  and an MAE score of , outperforming all traditional approaches by a notable margin.
We note that AMD is not designed specifically for this task nor for this particular dataset, and its strong performance demonstrates the generalization ability of the model.

In related work on unsupervised learning of saliency detection~\cite{zhang2018deep,zeng2019multi,nguyen2019deepusps}, the priors of traditional low-level methods are ensembled.
Though they do not use saliency annotations, their models are pretrained on ImageNet classification and even semantic segmentation with pixel-level annotations. These methods are thus not fully unsupervised, so they are omitted from the comparisons.

In Figure~\ref{fig:saliency}, we show some qualitative results on salient object detection.
Surprisingly, we find that our model pretrained on videos to segment moving objects not only detects movable objects in images, but also generalizes to detect stationary unmovable objects, such as statues, benches, trees and plates shown in the last column. This suggests that our model learns a generic objectness prior from the unlabeled videos.
We hypothesize that our model may learn objectness from the camera motion as well. Camera motion may cause the object and the background at various depth to have different observed projective 2D optical flow even though the objects are static.



\subsection{Zero-shot Video Object Segmentation}
\label{sec:vos}



\begin{table}[t]
	\begin{minipage}{0.4\linewidth}
		\centering
		\caption{Salient object detection performance on the DUTS dataset. Our model outperforms traditional low-level methods by notable margins.}
		\vspace{3pt}
		\setlength{\tabcolsep}{12pt}
		\begin{tabular}[t]{c|cc}
		\hline
		Model   &   & MAE  \\
		\hline
		RBD\cite{zhu2014saliency}  & 51.0   & 0.20 \\
		HS\cite{zou2015harf}       & 52.1   & 0.23 \\
		MC\cite{jiang2013saliency} & 52.9   & 0.19 \\
		DSR\cite{li2013saliency} & 55.8 & 0.14 \\
		DRFI\cite{jiang2013salient} & 55.2 & 0.15 \\
		\hline
		\textbf{AMD}                       & \textbf{60.2}      & \textbf{0.13} \\
		\hline
		\end{tabular}
		\label{tab:saliency}
	\end{minipage}
	\quad
	\begin{minipage}{0.55\linewidth}  
		\centering
		\caption{Transfer performance for semantic segmentation on VOC2012. Our method outperforms TimeCycle and compares favorably with contrastive methods.}
		\vspace{3pt}
		\setlength{\tabcolsep}{10pt}
		\begin{tabular}[t]{c|c|c|c}
		\hline
		Model   & Data &  Aug. & mIoU   \\
		\hline
		Scratch & -- & -- &  48.0  \\
		TimeCyle\cite{wang2019learning} & VLOG & light & 52.8  \\
MoCo-v2\cite{he2020momentum}   & YTB  & light & 61.5 \\
\textbf{AMD} & YTB  &  light & \textbf{62.0} \\
		\hline
MoCo-v2\cite{he2020momentum}       &  YTB  &  heavy & \textbf{62.8} \\
		\textbf{AMD} & YTB  &  heavy & 62.1 \\
		\hline
		MoCo-v2\cite{he2020momentum}       &  IMN  &  heavy & \textbf{72.4} \\
		\hline
		\end{tabular}
		\label{tab:seg}
	\end{minipage}
	\vspace{-10pt}
\end{table}

We transfer the pretrained AMD model to object segmentation on novel videos.
Since the segmentation prediction from our model is based on static images, inference on images sequentially in a video essentially estimates objectness.
In order to exploit motion information, we use a test-time adaptation approach.
Concretely, given a novel video, we optimize the training objective in Eq.~\ref{eq:loss} on pairs of frames sampled from the novel testing video. The adaptation takes 100 iterations per video. 




We evaluate zero-shot video object segmentation on three testing datasets. 
{\bf DAVIS 2016~\cite{Perazzi2016}} contains 20 validation videos with 1,376 annotated frames.  
{\bf SegTrackv2~\cite{li2013video}} contains 14 videos with 976 annotated frames. Following prior works, we combine multiple foreground objects in the annotation into a single object for evaluation. 
{\bf FBMS59 ~\cite{ochs2013segmentation}} contains 59 videos with 720 annotated frames. The dataset is relatively challenging because the object may be static for a period of time. We pre-process the ground truth labels following prior work~\cite{yang2019unsupervised}.
For evaluation, we report the Jaccard score, which is equivalent to the intersection over union (IoU) between the prediction and the ground truth segmentation.

\textbf{Experimental results.} 
We consider baseline methods claiming to be unsupervised for the full pipeline, including traditional non-learning-based approaches~\cite{wang2017saliency,faktor2014video,papazoglou2013fast,keuper2015motion,koh2017primary} and recent self-supervised learning methods~\cite{yang2019unsupervised,yang2021self}.
In Table~\ref{tab:FID}, we summarize the results for all the methods on the three datasets.
Among these methods, NLC~\cite{faktor2014video} actually relies on an edge model trained with human-annotated edge boundaries, and ARP~\cite{koh2017primary} depends on a segmentation model trained on a human-annotated saliency dataset. We thus gray their entries in the table.
For all the traditional methods, since their original papers do not report results on most of these benchmarks, we simply provide the performance values reported in the CIS paper~\cite{yang2019unsupervised}.

We evaluate the performance for AMD with and without test-time adaptation. No adaptation boils down to per-image saliency estimation using only the appearance pathway. Adaptation transfers both appearance and motion pathways.
On DAVIS 2016, our method achieves a Jaccard score of , surpassing all traditional unsupervised models. 
For CIS~\cite{yang2019unsupervised}, their best performing model uses a significant amount of post-processing, including model ensembing, multi-crop, temporal smoothing and spatial smoothing.
We thus refer to their performance obtained from a single model without post-processing.
Our model is slightly worse than CIS on DAVIS, by .
However, on SegTrackv2 and FBMS59, our method outperforms CIS by large margins of  and , respectively.
Motion grouping~\cite{yang2021self} is a work concurrent with ours.
It is essentially  a motion segmentation approach which relies on an off-the-shelf pre-computed dense optical flow model.
Motion grouping performs worse than our method on DAVIS2016 and SegTrackv2 when a low-performance unsupervised optical flow model ARFlow is used~\cite{liu2020learning}.
With a state-of-the-art supervised optical flow model~\cite{teed2020raft} that is trained on ground truth flow, their performance improves significantly.
Among all the discussed methods, ours is the first end-to-end self-supervised learning approach which does not require a pretrained optical flow model.


In Figure~\ref{fig:vos}, we show qualitative comparisons to the baseline CIS~\cite{yang2019unsupervised}. We display dense flow from pretrained PWC-Net, the CIS results, our segment flows, and our segmentation results. For most of these examples, our segment flow only coarsely reflects the true pixel-level optical flow. However, our segmentation results are significantly better and less noisy, as our model is relatively insensitive to optical flow quality.
In the first and the third examples, our model produces high-quality object segmentations even though the motion cue for the objects is very subtle.



\def\rowone#1#2#3#4#5#6#7{
\imw{figs/fig_vos/#7/#1.png}{0.152}&
\hspace{0.01cm}
\imw{figs/fig_vos/#7/#2.png}{0.152}&
\hspace{0.01cm}
\imw{figs/fig_vos/#7/#3.png}{0.152}&
\hspace{0.01cm}
\imw{figs/fig_vos/#7/#4.png}{0.152}&
\hspace{0.01cm}
\imw{figs/fig_vos/#7/#5.png}{0.152}&
\hspace{0.01cm}
\imw{figs/fig_vos/#7/#6.png}{0.152}\\
}
\begin{figure}[t]
\centering
\subfloat{\small
    \tb{@{}ccccccc@{}}{0.03}{
    \rotatebox{90}{\makecell[c]{\hspace{0.15cm}dense\\\hspace{0.15cm}flow}} & \rowone{1}{5}{9}{17}{15}{18}{cis}
    \vspace{0.03cm}
    \rotatebox{90}{\hspace{0.0cm}CIS~\cite{yang2019unsupervised}}  & \rowone{0}{4}{8}{16}{14}{19}{cis} 
\rotatebox{90}{\makecell[c]{segment\\flow}} & \rowone{10}{0}{8}{14}{16}{18}{ours}
    \rotatebox{90}{\hspace{0.3cm}AMD} & \rowone{11}{1}{9}{15}{17}{19}{ours}
    }
}
\caption{Qualitative comparisons to motion segmentation based method CIS~\cite{yang2019unsupervised} with its input dense flow and our segmentation results with segment flow representation. CIS is prone to noise, articulated motion, and camera motion in the dense flow estimations. By decomposing appearance from motion, our model AMD suffers less from these vulnerabilities of dense optical flow. This leads to results much better and more robust than the motion segmentation based approach.}
\label{fig:vos}
\vspace{-3pt}
\end{figure}

\begin{table}[t]
\centering
\caption{Performance evaluations for unsupervised video object segmentation on DAVIS 2016, SegTrackv2 and FBMS59 datasets. The numbers are measured in terms of Jaccard score. The table is split into traditional non-learning-based and recent self-supervised learning methods. The model results which rely on other kinds of human supervisions (Sup.) are \textcolor[gray]{0.5}{grayed}. Dependence for pretrained dense flow method is also listed for each model. MG's results on SegTrackv2 and FMBS59 using ARFlow are reproduced by ours and marked with . 
We evaluate AMD with appearance pathway only and with both pathways for test time adaptation.
AMD performs favorably to the baseline CIS on DAVIS 2016, while showing large gains on the other two benchmarks.} \vspace{5pt}
\setlength{\tabcolsep}{6pt}
\begin{tabular}[t]{ccccc|ccc}
\hline
\hline
 & Model & e2e & Sup. & Flow & DAVIS 2016  & SegTrackv2 & FBMS59  \\
\hline
\parbox[t]{0.1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{traditional}}} & SAGE\cite{wang2017saliency}   & \xmark  & \xmark     &  LDOF\cite{brox2010large} & 42.6 & 57.6 & 61.2 \\
& NLC\cite{faktor2014video}     & {\xmark}  & {edge}     &  {SIFTFlow}\cite{liu2009beyond} & \textcolor[gray]{0.5}{55.1}  & \textcolor[gray]{0.5}{67.2} & \textcolor[gray]{0.5}{51.5} \\
& CUT\cite{keuper2015motion}    & \xmark  & \xmark     &  LDOF\cite{brox2010large} & 55.2 & 54.3 & 57.2 \\
& FTS\cite{papazoglou2013fast}  & \xmark  & \xmark     &  LDOF\cite{sundaram2010dense} & 55.8 & 47.8 & 47.7 \\
& {ARP}\cite{koh2017primary}      & {\xmark}  & {saliency} &  {CPMFlow}\cite{hu2016cpm} & \textcolor[gray]{0.5}{76.2} & \textcolor[gray]{0.5}{57.2} & \textcolor[gray]{0.5}{59.8} \\
\hline
\parbox[t]{0.1mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{learning}}} & CIS\cite{yang2019unsupervised}  & \xmark  & \xmark & PWC\cite{sun2018pwc}    & 59.2 & 45.6 & 36.8 \\
& MG\cite{yang2021self}           & \xmark  & \xmark & ARFlow\cite{liu2020learning} & 53.2 & 37.8    &  50.4   \\
& \textbf{AMD} {\footnotesize (per-img)} & \cmark  & \xmark & \xmark  & 45.7  & 28.7 & 42.9 \\
& \textbf{AMD}  {\footnotesize (per-vid)} & \cmark  & \xmark & \xmark  & 57.8   & 57.0 & 47.5 \\
\hline
\hline
\end{tabular}
\label{tab:FID}
\vspace{-3pt}
\end{table}


 








\subsection{Semantic Segmentation}
\label{sec:semantic}

Given that our pretrained segmentation network can produce meaningful generic object segmentations, we further examine its semantic modeling ability on semantic segmentation. 
We conduct this experiment on the \textbf{Pascal VOC 2012~\cite{everingham2010pascal}} dataset.
The dataset contains 20 object categories with 10,582 training images and 1,449 validation images.
Given a pretrained model, we finetune the model on the training set and evaluate the performance on the validation set.
The finetuning takes 40,000 iterations with a batch size of 16 and an initial learning rate of 0.01. 
The learning rate undergoes polynomial decay with a power parameter of 0.9.



\textbf{Experimental results.}
We compare the pretrained model to a image-based contrastive model, MoCo-v2~\cite{he2020momentum} and a self-supervised video pretraining model, TimeCycle~\cite{wang2019learning}.
TimeCycle~\cite{wang2019learning} is pretrained on the VLOG dataset, which is larger than our Youtube-VOS dataset.
For MoCo-v2, we also pretrain the contrastive model on the Youtube-VOS dataset, to ablate the role of pretraining datasets.
Since the base version of our method does not utilize heavy augmentations as in contrastive models, we also study the effects of data augmentations.
The results are reported in Table~\ref{tab:seg}.
Our method outperforms the video pretraining approach TimeCylce significantly by .
Compared with MoCo-v2, when light augmentation (resizing, cropping) is used, our model slightly outperforms MoCo-v2 by . However, when heavy data augmentation (color jitter, grayscale, blurring) is applied, our method underperforms MoCo-v2 by .
This is possibly because our model is non-contrastive in nature, and thus unable to take advantage of information effectively in augmentations.
MoCo-v2 performs much stronger when pretrained on ImageNet, possibly because the semantic distribution of ImageNet is well aligned with VOC2012.
Overall, our model outperforms a prior self-supervised video model TimeCycle and compares favorably with contrastive model MoCo-v2 under the same data.


\def\rowone#1#2#3#4#5#6{
\imw{figs/fig_segnum/#1}{0.16}&
\hspace{0.003cm}
\imw{figs/fig_segnum/#2}{0.16}&
\hspace{0.003cm}
\imw{figs/fig_segnum/#3}{0.16}&
\hspace{0.0015cm}
\vrule
\hspace{0.0015cm}
\imw{figs/fig_segnum/#4}{0.16}&
\hspace{0.003cm}
\imw{figs/fig_segnum/#5}{0.16}&
\hspace{0.003cm}
\imw{figs/fig_segnum/#6}{0.16}\\
}


\begin{figure}[t]
    \begin{minipage}{0.7\linewidth}  
    	\centering
    	\subfloat{\small
    		\tb{@{}cccccc@{}}{0.05}{
    		\rowone{drift-straight_00031_iter0000100_im_msk_l5.png}{drift-straight_00031_iter0000100_im_msk_l6.png}{drift-straight_00031_iter0000300_im_msk_l8.png}{blackswan_00008_iter0000200_im_msk_l5.png}{blackswan_00008_iter0000300_im_msk_l6.png}{blackswan_00008_iter0000300_im_msk_l8.png}
    		\rowone{drift-straight_00031_iter0000100_f2_l5.png}{drift-straight_00031_iter0000100_f2_l6.png}{drift-straight_00031_iter0000300_f2_l8.png}{blackswan_00008_iter0000200_f1_l5.png}{blackswan_00008_iter0000300_f1_l6.png}{blackswan_00008_iter0000300_f1_l8.png} 
    		 &  &  &  &  &  \\
    		}
    	}
    	\label{fig:ab}
    \end{minipage}
    \quad
    \begin{minipage}{0.2\linewidth}
    	\centering
    	\setlength{\tabcolsep}{4pt}
    	\begin{tabular}[t]{c|c}
    	\hline
        \#segments & DAVIS ()   \\
    	\hline
    	     & \textbf{57.8}  \\
    	     & 45.3           \\
    	     & 41.0           \\
    	\hline
    	\end{tabular}
    	\label{tab:ab}
    \end{minipage}
    \captionof{figure}{Ablation study on different number of segments. Two examples with segmentation masks and segment flows are shown. The object region is split over multiple masks when  becomes large. The over-segmentation decreases the performance of video object segmentation on DAVIS2016.}
    \vspace{-5pt}
    \label{fig:ab}
\end{figure}

\subsection{Ablation Study}
\label{sec:ablation}

The variable , the number of segmentation channels, is an important hyper-parameter of our model. 
We vary the value of  () for pretraining the model, and examine its transfer performance for video object segmentation on DAVIS2016.
In Figure~\ref{fig:ab}, we visualize the model predictions under different number of segments. We observe that a large  tends to lead to over-segmentation, and a small  tends to lead to large regions.
The car and the swan is split into multiple regions even if the motion for separated regions are very close.
The model trained with  segments a full object, while the model trained with  separates the object into parts.
When pretraining the model with , the training becomes unstable.
Quantitatively, the video object segmentation performance on DAVIS2016 decreases as we increase the number of segments.




\begin{table}[t]
	
	
\end{table}
