
A user study was conducted in order to assess the viability of game-based
pairing in general and Alice Says in particular. These experiments were
intended to achieve several goals. Primarily, their focus was on gaining an
understanding of the usability of this pairing process. These tests were
intended to determine if the new game software could be used by real users to
consistently and reliably pair devices. In other words, we desired to discover
whether users felt that playing a game while pairing was useful or not. In
addition, a secondary test objective was to collect feedback from users regarding
Alice Says. A final aim was to ascertain the efficiency of Alice Says by
measuring its overall practical execution time.

\subsection{Experimental Framework}
\label{sec:framework}

In order to evaluate Alice Says, an environment was established where users
were able to get hands on experience with our prototype of the game. Review
\ref{sec:implementation} for full details on the implementation of this new
pairing system. Our intention was to create a testing framework that mirrored a
real world usage scenario as closely as possible while remaining uncomplicated.
An example of this is the omission of an in-band wireless link between the two
mobile devices. Since this process does not have a noticeable impact on the
usability of the pairing game, it was safely left out in favor of OOB strings
created using a pseudorandom pattern generated on a desktop machine using the
Mersenne Twister \cite{mersenne_twister} implementation included in the random
module of the Python programming language's standard library. The strings
utilized in these test cases were pseudorandomly generated, but fixed from
subject to subject to prevent some volunteers from receiving strings that were
easier to identify than others. These strings were presented to the test
subjects in a random order. This was done to minimize the effects of learning
and fatigue on the test results. In other words, we wanted to prevent users
from anticipating future test cases based on previous ones or losing motivation
to pay proper attention during the pairing procedure.

An automated feedback interface was used on a desktop computer to facilitate
this usability analysis. This was used to present post-condition questionnaires
to users following their completion of the study proper. In addition to the SUS scale, the exact phrasing of the post-condition survey questions that were presented to volunteers were as follows:

\begin{enumerate}
\item The method was enjoyable.
\item The method took a long time.
\item I would like to pair with another user's devices by making use of this method.
\item The sound effects used in this method were pleasant to listen to.
\end{enumerate}

Beyond this, logging was performed on the mobile devices themselves in order to capture both the timing of the events comprising the pairing procedure as well as any user mistakes that were made along the way.

\subsection{Participant Information}
\label{sec:participants}


20 test subjects were invited to participate in this usability survey. 
Participants were gathered from students, professors, and staff members
studying and working in labs at our institution. Word of the study was spread
using flyers, emails, and in-person sign ups during the weeks prior to its
occurrence. To encourage participation in our study, movie theater gift
certificates were offered to testers upon the completion of their involvement
in the study. The post-condition survey questions themselves are provided in Section \ref{sec:framework}.



Demographic information about these testers was collected as a component of the pre-conditioning phase of the experiment. Our subjects had varying ages, but were predominately young as would be expected from a pool containing university students. Half were between 18 and 24, while 30\% had ages of 25 to 29. There were several older individuals counted in our survey as well, however. 5\% of our sample population, or one user, was older than 29 but younger than 35. An additional user, comprising another 5\% of our user group, was between 35 and 39 years of age. Finally, 10\% of our sample was made up of people whose age exceeded 40. A majority, 65\%, of subjects were male although many females were also represented, comprising 35\% of the total. Another area in which sampling college students had an effect was on the level of education our testers had obtained. One tester (5\%) did not possess a college degree, 65\% had obtained their bachelor's, 25\% had obtained their master's degree, and one user (5\%) had completed his or her doctorate.

Beyond demographics, the post-conditional survey presented users with queries
intended to measure their level of expertise with device pairing and video
games. 70\% of participants had paired a wireless device before. This was not
somewhat surprising result considering the ubiquitous nature of wireless
devices and the need to pair them. This can perhaps be attributed to a
misunderstanding of what constitutes device pairing, as it could be anticipated
that most participants had utilized a wireless connection on their laptop or
paired a wireless remote control with a television or gaming appliance.
Remarkably, every participant responded that they played video games. This
figure demonstrates the widespread popularity of computer games and bodes well
for their acceptance by users as a means of facilitating secure usage habitats. 

\subsection{Experimental Design}


In this section, the design choices necessary to recreate the results of this
study are explored. To initiate the experiment, the test administrator first
selected Alice Says, represented by a Alice icon, from the mobile device's
application list. The single player version of Alice Says was then initiated by
selecting the ``1 Player'' option from the application's menu. Users were then
presented with the mobile device to provide them with an opportunity to
acclimate themselves with the user interface and overall gameplay of Alice
Says. 
Once
they felt that they had gained enough experience, users halted the single
player game section by selecting a back button from the game menu.

Next, the formal testing procedure was started by selecting complimentary game
modes on each of the two devices. This was done via a text field that
numerically specified one for the output device and two for the input device.
Matching test cases were specified on the two devices in an identical manner.
Once the options were put into place, the two player pairing mode was initiated
by selecting a ``2 Player'' choice from the menu on the mobile device. This
brought the primary Alice Says interface, consisting of four colored quadrants
and a central pattern score counter, up on their devices. As a final step to
initiate testing, the ``Start'' menu option was pressed on each device.

For the first two test cases performed with each subject, the participant
handled the input device while the test administrator took care of the output
duties. Thus, to start the game and whenever the volunteer successfully matched
a pattern, the administrator selected the ``Next'' option on the Alice Says
menu. In the event that the participant committed a mistake and did not
successfully match the provided pattern, the test conductor pressed a
``previous'' button to signal to the device that it should create a new pattern
starting with the last bit of the incorrectly matched pattern. Meanwhile, the
tester's job was to observe the audiovisual pattern displayed on the output
device and input it on their mobile appliance. Two test cases were performed in
this configuration to ensure that owe provided our subjects with system
experience that was generalizable and not specific to any individual test case.
Following this, a third and final test case was performed with the role of the
administrator and subject reversed by switching the mode options on the two
mobile phones. This was done to give users hands on experience with both sides
of the two player game; this is important because in a real life scenario this
will often be done to achieve mutual authentication as discussed in Section
\ref{sec:design}.

After the conclusion of the central portion of the experiment, subjects were
presented with a web form containing a set of post-conditioning queries. Beyond
the demographic and background information listed in Section
\ref{sec:participants}, this questionnaire consisted of fifteen five point
Likert items which were selected to gauge how volunteers felt about Alice Says.
The precise questions posed are provided in Section \ref{sec:framework}. The
first ten of these questions were provided to evaluate this technique using the
System Usability Scale (SUS) \cite{sus}, modified slightly to refer to Alice
Says as a method rather than a system.

\subsection{Experimental Results}

The observed results of the usability study are presented in this part of the
paper. Each test subject performed two input sessions and one output session
for a total of three test cases per user and 60 test cases overall. The average
time that users took to pair using Alice says was 173.267 seconds with a
standard deviation of 28.638 seconds. In terms of reliability, an average of
1.517 mistakes were made while performing the pairing process (i.e., an average
of 1.517 mistakes per pairing session).  Note that these are all partial errors
as pairing still completed successfully following their occurrence for all test
cases.  
Some user errors were caused by
an inability to recall the displayed pattern, while others were caused by users
accidentally pressing the incorrect color button, though it is difficult to
separate these two occurrences in practice.

\subsubsection{User Feedback}

\begin{figure}[htbp]
\centering{
  \includegraphics[width=.45\textwidth]{pics/survey_results.jpg}}
\caption{{{Average Responses to Post-Conditional Questionnaire}}}
  \label{fig:survey_results}
\end{figure}

Figure \ref{fig:survey_results} provides the responses users gave on the
post-condition survey. Users awarded Alice Says with an average SUS score of
70.5 and a standard deviation of 12.860. In response to whether or not Alice
Says was enjoyable, users provided an average response of 3.85. When asked if
this pairing process took a long time, users responded with a 3.1. However,
they also suggested that they would like to utilize Alice Says by providing the
highest average score observed, 3.88, to question thirteen. Users tended to
agree that the sounds used in the game were pleasant, providing a 3.55 average
response to this query. Finally, on the last question of whether Alice Says was
considered to be secure, users again replied positively with a 3.65.

Some users also responded to our open-ended question about Alice Says.  They
positively indicated that the paring method is ``pretty good'', ``a fun way to
pair'', ``a very cute game'', and ``a nice game'' and that ``they had a lot of
fun''.  However, some of them complained that method is ``too long''.  Some
suggested, as an improvement, to make the game ``less time consuming and
user-friendly'', use better menu options than ``next and previous'', make use
of ``better touchscreen'', and have the game ``count with me while I am picking
the colors''.



\subsection{Interpretation of Results}

This section outlines the implications of our test results.

\subsubsection{Efficiency}

The efficiency of Alice Says is clearly its least desirable characteristic. It
took slightly under three minutes to complete on average, when alternative
pairing approaches may take approximately twenty to thirty seconds (see
\cite{kstu09}, e.g.) for a similar level of security (i.e., corresponding to 30
bits). While this extended execution time would be problematic for pairing
approaches that burden users with tedious and repetitive tasks, this time frame
is less problematic for a game based pairing method. In a way, if a user is
having a good time while executing the pairing process, he or she may wish to
extend the technique's execution time rather than reduce it. However, contrary
to our intuition, users did indicate the game to be a bit lengthy. This is one
of the most important lessons learned from this study. In our future work, we
will experimentally determine the timing threshold that average users deem
appropriate for a pairing game and modify our game design to adhere to this
threshold (our current Alice Says prototype was not optimized in terms of
timing). 




\subsubsection{Reliability}

On the other hand, the reliability of Alice Says is one of its strengths. While
users committed, on an average, around one and half errors per session, the
pairing game was designed to be resilient in the face of such mistakes, and as
a result pairing concluded successfully in each and every attempt we performed.
We believe that as users become more or more familiar with this approach (and
even with the single player mode of the game), these errors will further be
reduced. This, in turn, would also reduce the overall execution timing of the
game, thus improving usability.

\subsubsection{User Feedback}

Considering that the industry average for SUS scores for computer systems are
somewhere between 60 and 70 \cite{sus-ratings}, our overall SUS score for Alice
Says can be termed quite positive. Looking at the questionnaire as a whole,
test subjects generally agreed with the positive statements regarding Alice
Says.  A majority of users concurred that the method was enjoyable, and seemed
secure. Furthermore, they also replied that they would like to make use of it.
Unfortunately, as discussed above, users also agreed with the one negative
statement that was put forth. This was in relation to the timing of the
technique, which was already known to be longer than users are accustomed to.
This aspect of the feedback prompts the need for further work on designing
games that are reasonably fast for the pairing process. 

The raw feedback provided by the users that they found the pairing process to
be entertaining was heartening and is promising towards the adoption of Alice
Says in practice.  

\subsection{Ecological Validity}

This section documents the ways in which this study conformed and deviated from
the real life situation it was intended to capture. The largest difference
between this study and an actual setting is the absence of a wireless link
between the two phones being paired. This may have had an impact on efficiency
because the latency this connection would introduce was not taken into account
in our study, but this step takes negligible time to execute compared to the
other steps involved in Alice Says. Beyond this, the only other ecological
concerns associated with this experiment are those encountered by all usability
studies in general. For example, by performing the tests in a lab rather than
in their own home, test subjects may have performed with a heightened awareness
of their actions. Testers may have altered their responses to survey questions
due to a desire to please the investigators, though this effect was guarded
against by providing subjects with privacy while answering as well as by
anonymizing the results. Finally, providing motivation for the test helped
users have a better understanding of how their actions related to real life
scenario, but also may have had a conditional effect. A pre-condition
questionnaire was not provided in order to minimize this impact, however.
