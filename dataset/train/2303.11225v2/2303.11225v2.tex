\twocolumn[
\begin{@twocolumnfalse}
{
\begin{center}
    \Large \textbf{\name: High-Fidelity 3D Face Reconstruction \\ by Learning Static and Dynamic Details }\\
    \Large \textbf{------------------------Appendix------------------------}
\end{center}
}
{
\vspace*{24pt}
\large
\lineskip .5em
\centering
\begin{tabular}[t]{c}
\bigskip
 \vspace*{1pt}\\\end{tabular}
\par
\vskip .5em
\vspace*{12pt}
}
\end{@twocolumnfalse}
]



In this supplementary, we provide additional results and discussions to make our paper self-contained. Specifically, we present 1). more details on experiments and implementation in Sec.~\ref{sec.supp_imp}, 2). details of loss functions in Sec.~\ref{sec.supp_loss}, 3). more experimental comparisons and results in Sec.~\ref{sec.supp_exp}, 4). discussions on limitations and future work in Sec.~\ref{sec.supp_limit}, and 5). discussions on social impact in Sec.~\ref{sec.supp_social}.



\section{Experimental \& Implementation Details} \label{sec.supp_imp}


\subsection{Details of Dataset}

We follow the synthetic data pipeline~\citesupp{raman2022meshsupp,wood2021fakesupp} to synthesize  images, consisting of  identities with  frames. For each identity, we assign different expressions, viewpoints, illumination, accessories, and backgrounds to improve the model's robustness for training.

In addition, in the synthetic data pipeline, the ground-truth albedo, neutral displacement maps, stretched displacement maps, and compressed displacement maps are sampled from the  captured scans~\citesupp{raman2022meshsupp,wood2021fakesupp} and recorded. 
In this paper, to ensure the efficiency of our model, we resize the  resolution assets into  as the ground-truth labels for training. 
For the real-world data~\citesupp{CelebAMask-HQsupp,mollahosseini2017affectnetsupp}, we split them into the train () and valid (). During training, common data augmentation techniques ({\ie}, random shift, scale, rotation, and flip) are adopted to improve the robustness of our model.


\subsection{Details of Ablation Studies}


In the ablation on datasets, we only remove the real-world data while keeping other settings the same as our full {\name}. In the ablation on loss functions, we only remove specific loss functions while training the models that follow the same settings as our full {\name}.


For the ablation studies on {\module}, in SD-1, instead of obtaining the dynamic detail by interpolating the compressed and stretched displacement maps as in {\module}, we directly synthesize the dynamic detail by a learnable network end-to-end. The ground-truth labels for the synthetic dataset follow the dynamic composition in Eq.~\ref{Eq.dyn_detail}.
More specifically, we use an MLP layer to map expression-aware  into -dim latent code , and follow~\citesupp{feng2021learningsupp} to use a U-Net decoder~\citesupp{ronneberger2015usupp} to synthesize corresponding dynamic displacement map in  resolution. 

In SD-2, instead of generating compressed and stretched displacement maps using the PCA bases as in {\module}, we employ two learnable networks to synthesize the compressed and stretched displacement maps in parallel, and compound the dynamic details by interpolating the two displacement maps using Eq.~\ref{Eq.dyn_detail}. 
Similar to SD-1, we use two MLP layers to map  into -dim latent codes  and , and use two U-Net decoders to synthesize corresponding polarized displacement maps in  resolution.

In SD-3, instead of generating the static displacement map using the PCA basis as in {\module}, we directly synthesize the static detail by a learnable network end-to-end. More specifically, we use an MLP layer to map age-aware  into -dim latent code , and follow~\citesupp{feng2021learningsupp} to use a U-Net decoder~\citesupp{ronneberger2015usupp} to synthesize corresponding static displacement map in  resolution. 




\section{Details of Loss functions} \label{sec.supp_loss}

In our main paper, we propose several simple yet effective loss functions to train our model end-to-end using both synthetic and real-world data. As we have justified through the detailed ablation studies, each loss function contributes to the coarse shape and details. In this section, we introduce details about self-supervised losses  and knowledge distillation .


\subsection{Details of Self-Supervised Losses}
Following~\citesupp{feng2021learningsupp,deng2019accuratesupp}, we leverage the differentiable renderer~\citesupp{genova2018unsupervisedsupp} to obtain the rendered image , then use photo loss  and identity loss  to compute the error between the input image  and . We also follow~\citesupp{wood2022densesupp} to use dense landmark loss  to calculate the error between the detected landmarks from  and projected landmarks from , as self-supervised loss is still crucial to ensure satisfactory generalization to real-world images.


Photo loss  computes the  error between  and :

where  is the region-of-interest mask~\citesupp{Zheng2022DecoupledMLsupp} of image , which only considers facial skins and removes occlusions.

Identity loss  leverages the pretrained face recognition network ~\citesupp{deng2019arcfacesupp} to estimate the cosine similarity between high-level features from  and :


Dense landmark loss  leverages the landmark detector~\citesupp{wood2022densesupp} to detect  dense landmarks of given 2D images, and estimate the distance between the detected and projected 2D points from the reconstructed shape :

where  and  are the coordinates and uncertainty of the -th detected landmark from , respectively.  denotes the -th projected 2D landmark from the reconstructed shape .




\begin{figure}[t!]
    \centering
    \vspace{10pt}
    \begin{overpic}[trim=10.5cm 10cm 8.5cm 6cm,clip,width=1\linewidth,grid=false]{img_supp/supp_errormap_1_v2.jpg}
    \put(3,55){\bfseries\scriptsize G.T.}    
    \put(14,55){\scriptsize Deep3D}
    \put(28,55){\scriptsize MGCNet}
    \put(44,55){\scriptsize DECA}
    \put(58,55){\scriptsize EMOCA}
    \put(74,55){\scriptsize MICA}
    \put(89,55){\bfseries\scriptsize Ours}
    \end{overpic}
    \begin{overpic}[trim=10.5cm 10cm 8.5cm 4cm,clip,width=1\linewidth,grid=false]{img_supp/supp_errormap_2_v2.jpg}
    \end{overpic}
    \begin{overpic}[trim=10.5cm 10cm 8.5cm 4cm,clip,width=1\linewidth,grid=false]{img_supp/supp_errormap_3_v2.jpg}
    \end{overpic}
    \vspace{-5pt}
    \caption{\textbf{Error map on REALY benchmark.} We visualize and compare the reconstruction error of {\name} to previous methods. From left to right: Input image \& ground-truth,  Deep3D~\protect\citesupp{deng2019accuratesupp}, MGCNet~\protect\citesupp{shang2020selfsupp}, DECA~\protect\citesupp{feng2021learningsupp}, EMOCA~\protect\citesupp{danvevcek2022emocasupp}, MICA~\protect\citesupp{MICAsupp}), and {\name} (Ours), where large (small) errors are colored in red (blue). The proposed method presents the best reconstruction quality. } \label{fig:supp_errormap}
    \vspace{-5pt}
\end{figure}


\begin{figure*}[!t]
    \centering
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_coarse_v2.jpg}
    \end{overpic}
    \put(-470,570){\bfseries\scriptsize Input}
    \put(-40,570){\bfseries\scriptsize Ours}
    \put(-400,570){\scriptsize Deep3D}
    \put(-330,570){\scriptsize 3DDFA-v2}
    \put(-260,570){\scriptsize SynergyNet} 
    \put(-185,570){\scriptsize MICA} 
    \put(-110,570){\scriptsize Dense}
    \vspace{5pt}
    \caption{\textbf{Comparison on coarse shape reconstruction.} From left to right: Input image, Deep3D~\protect\citesupp{deng2019accuratesupp}, 3DDFA-v2~\protect\citesupp{guo2020towardssupp}, SynergyNet~\protect\citesupp{wu2021synergysupp}, MICA~\protect\citesupp{MICAsupp}, Dense~\protect\citesupp{wood2022densesupp}, and {\name} (Ours). Note that MICA focuses on identity reconstruction, lacking the consideration of expression.}
    \label{fig:supp_coarse_cmp}
    \vspace{-5pt}
\end{figure*}


\begin{figure*}[!t]
    \centering
\begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_detail.jpg}
    \end{overpic}
    \put(-470,570){\bfseries\scriptsize Input}
    \put(-40,570){\bfseries\scriptsize Ours}
    \put(-400,570){\scriptsize FaceScape}
    \put(-327,570){\scriptsize Unsup}
    \put(-255,570){\scriptsize DECA} 
    \put(-185,570){\scriptsize EMOCA} 
    \put(-115,570){\scriptsize FaceVerse}
    \vspace{5pt}
    \caption{\textbf{Comparison on detail shape reconstruction.}  From left to right: Input image, FaceScape~\protect\citesupp{yang2020facescapesupp}, Unsup~\protect\citesupp{chen2020selfsupp}, DECA~\protect\citesupp{feng2021learningsupp}, EMOCA~\protect\citesupp{danvevcek2022emocasupp}, FaceVerse~\protect\citesupp{wang2022faceversesupp}, and {\name} (Ours). ``{\XBox}'' indicates this method fails to return any reconstruction.}
    \label{fig:supp_detail_cmp}
    \vspace{-5pt}
\end{figure*}


\begin{figure*}[ht!]
    \centering
\begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_flexble.jpg}
    \end{overpic}
    \put(-480,168){\bfseries\scriptsize Input}
    \put(-430,168){\scriptsize Dense~\citesupp{wood2022densesupp}}  
    \put(-380,168){\bfseries\scriptsize + Our Detail}   
    \put(-310,168){\bfseries\scriptsize Input}
    \put(-260,168){\scriptsize Dense~\citesupp{wood2022densesupp}}  
    \put(-210,168){\bfseries\scriptsize + Our Detail}   
    \put(-145,168){\bfseries\scriptsize Input}
    \put(-95,168){\scriptsize  Dense~\citesupp{wood2022densesupp}}  
    \put(-45,168){\bfseries\scriptsize + Our Detail}  
    \vspace{-5pt}
    \caption{\textbf{Illustration on the flexibility of {\module}.} Given the identity and expression coefficients (, ) from the optimization-based method~\protect\citesupp{wood2022densesupp}, {\module} can generate realistic details based on the coarse shape and further improve the visual quality.}
\label{fig:supp_ft_detail}
\end{figure*}




\subsection{Details of Knowledge Distillation}
The age prediction model ~\citesupp{karkkainenfairfacesupp} predicts the age of given images into  categories: , , , , , , , , and . Therefore, we leverage -layer MLP to transform the static coefficient  into a -dim vector, and use \textit{softmax} to map into a probability distribution .






\section{Additional Experimental Results} \label{sec.supp_exp}

In this section, we provide additional results to strengthen the superiority of {\name} in reconstructing 3D shapes with animatable details. Specifically, we present 1). error maps on REALY~\citesupp{REALYsupp} in Fig.~\ref{fig:supp_errormap}, 2). additional reconstruction comparisons of coarse shape and details in Fig.~\ref{fig:supp_coarse_cmp} and Fig.~\ref{fig:supp_detail_cmp}, respectively, 3). additional experiments on flexibility for both images and video sequences in Fig.~\ref{fig:supp_ft_detail} and Fig.~\ref{fig:supp_ft_detail_video1}, Fig.~\ref{fig:supp_ft_detail_video2},  Fig.~\ref{fig:supp_ft_detail_video3}, respectively, and 4). additional animation comparisons in Fig.~\ref{fig:supp_animation} and Fig.~\ref{fig:supp_animation2}, 5). user studies about the reconstruction and animation quality in Tab.~\ref{tab:userstudy} and Tab.~\ref{tab:supp_user2}, respectively.

\subsection{Error Maps of REALY Benchmark}


In Fig.~\ref{fig:supp_errormap}, we present additional comparisons of the error map on the REALY benchmark~\citesupp{REALYsupp}. The RGB color in ground-truth regions is mapped from the vertex-to-vertex error between the ground-truth and predicted shape according to the evaluation protocol in~\citesupp{REALYsupp}. Compared to previous methods, {\name} reaches the smallest error and the best reconstruction quality.









\subsection{Reconstruction Comparisons}


\myparagraph{Qualitative Comparisons.}
In Fig.~\ref{fig:supp_coarse_cmp} and Fig.~\ref{fig:supp_detail_cmp}, we present additional comparisons of {\name} to previous coarse shape reconstruction methods~\citesupp{deng2019accuratesupp,guo2020towardssupp,wu2021synergysupp,MICAsupp,wood2022densesupp} and detail reconstruction methods~\citesupp{yang2020facescapesupp,chen2020selfsupp,feng2021learningsupp,danvevcek2022emocasupp,wang2022faceversesupp}. The input images show diversity {\wrt} ethnicity, gender, age, BMI, pose, environment, occlusion, and expression.
Compared to previous methods, {\name} is robust to occlusions, extreme poses, and diversity expressions. {\name} reconstructs realistic coarse shapes, better expressions, and realistic details. 




\begin{table}[ht!]
\caption{\textbf{User study results on the reconstructed shape and details.} {\name} achieves the best results in coarse shape and details according to human perception compared to prior art~\protect\citesupp{feng2021learningsupp,wood2022densesupp}.}\label{tab:userstudy}
\vspace{5pt}
\resizebox{1\linewidth}{!}{\begin{tabular}{c|ccc}
\toprule[1pt]
\cellcolor{tabblue!10} Group & \cellcolor{red!10}{Best method}  & \cellcolor{tabblue!10} 2nd best method &\cellcolor{red!10} Other methods \\ \midrule[1pt]
Coarse         & \textbf{54.55\%} (Ours) & 35.06\% (Dense~\citesupp{wood2022densesupp})   & 10.39\% (\citesupp{MICAsupp,deng2019accuratesupp,wu2021synergysupp,guo2020towardssupp})      \\ \midrule[1pt]
Detail         & \textbf{80.52\%} (Ours) & 11.69\% (DECA~\citesupp{feng2021learningsupp})    & 7.79\% (\citesupp{danvevcek2022emocasupp,wang2022faceversesupp,chen2020selfsupp,yang2020facescapesupp})       \\ \bottomrule[1pt]
\end{tabular}
}
\vspace{-5pt}
\end{table} 





\myparagraph{User Study.}
To demonstrate that our reconstructed 3D faces present visually better results and are faithfully aligned with human perception, we present a user study by inviting  volunteers with a computer science background to vote for the best-reconstructed shapes, from sampled faces in CelebA~\citesupp{CelebAMask-HQsupp}, FFHQ~\citesupp{karras2019stylesupp}, and AFLW2000~\citesupp{yin2017towardssupp}. Specifically, we separately compare methods for coarse shape reconstruction~\citesupp{deng2019accuratesupp,guo2020towardssupp,wu2021synergysupp,MICAsupp,wood2022densesupp}, and detailed reconstruction~\citesupp{yang2020facescapesupp,chen2020selfsupp,feng2021learningsupp,danvevcek2022emocasupp,wang2022faceversesupp}. The results are summarized in Tab.~\ref{tab:userstudy}.

\begin{figure*}[t!]
    \centering
\begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_input1.jpg}
    \end{overpic}
    \put(-505,40){\bfseries\scriptsize \rotatebox{90}{Input}}
    \put(-480,71){\bfseries\scriptsize T=0}
    \put(-30,71){\bfseries\scriptsize T=10}
    \put(-75,71){\bfseries\scriptsize T=9}
    \put(-120,71){\bfseries\scriptsize T=8}
    \put(-165,71){\bfseries\scriptsize T=7}
    \put(-210,71){\bfseries\scriptsize T=6}
    \put(-255,71){\bfseries\scriptsize T=5}
    \put(-300,71){\bfseries\scriptsize T=4}
    \put(-345,71){\bfseries\scriptsize T=3}
    \put(-390,71){\bfseries\scriptsize T=2}
    \put(-435,71){\bfseries\scriptsize T=1}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_deca1.jpg}
    \end{overpic}
    \put(-505,35){\bfseries\scriptsize \rotatebox{90}{DECA~\citesupp{feng2021learningsupp}}}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_emoca1.jpg}
    \end{overpic}
    \put(-505,30){\bfseries\scriptsize \rotatebox{90}{EMOCA~\citesupp{danvevcek2022emocasupp}}}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_coarse1_v2.jpg}
    \end{overpic}
    \put(-505,35){\bfseries\scriptsize \rotatebox{90}{Dense~\citesupp{wood2022densesupp}}}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_detail1_v2.jpg}
    \end{overpic}
    \put(-505,40){\bfseries\scriptsize \rotatebox{90}{Ours}}
    \vspace{-5pt}
    \caption{\textbf{Illustration on the flexibility of {\module} on video reconstruction (part 1).} We visualized the reconstruction quality of Dense~\protect\citesupp{wood2022densesupp} with/without our {\module} and compare them with prior art~\protect\citesupp{feng2021learningsupp,danvevcek2022emocasupp}. 
    Videos are taken from YouTube.
    }
    \label{fig:supp_ft_detail_video1}
    \vspace{-8pt}
\end{figure*}

\begin{figure*}[t!]
    \centering
\begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_input2.jpg}
    \end{overpic}
    \put(-505,40){\bfseries\scriptsize \rotatebox{90}{Input}}
    \put(-480,71){\bfseries\scriptsize T=0}
    \put(-30,71){\bfseries\scriptsize T=10}
    \put(-75,71){\bfseries\scriptsize T=9}
    \put(-120,71){\bfseries\scriptsize T=8}
    \put(-165,71){\bfseries\scriptsize T=7}
    \put(-210,71){\bfseries\scriptsize T=6}
    \put(-255,71){\bfseries\scriptsize T=5}
    \put(-300,71){\bfseries\scriptsize T=4}
    \put(-345,71){\bfseries\scriptsize T=3}
    \put(-390,71){\bfseries\scriptsize T=2}
    \put(-435,71){\bfseries\scriptsize T=1}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_deca2.jpg}
    \end{overpic}
    \put(-505,35){\bfseries\scriptsize \rotatebox{90}{DECA~\citesupp{feng2021learningsupp}}}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_emoca2.jpg}
    \end{overpic}
    \put(-505,30){\bfseries\scriptsize \rotatebox{90}{EMOCA~\citesupp{danvevcek2022emocasupp}}}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_coarse2_v2.jpg}
    \end{overpic}
    \put(-505,35){\bfseries\scriptsize \rotatebox{90}{Dense~\citesupp{wood2022densesupp}}}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_detail2_v2.jpg}
    \end{overpic}
    \put(-505,40){\bfseries\scriptsize \rotatebox{90}{Ours}}
    \vspace{-5pt}
    \caption{\textbf{Illustration on the flexibility of {\module} on video reconstruction (part 2).} We visualized the reconstruction quality of Dense~\protect\citesupp{wood2022densesupp} with/without our {\module} and compare them with prior art~\protect\citesupp{feng2021learningsupp,danvevcek2022emocasupp}. 
    Videos are taken from YouTube.
    }
    \label{fig:supp_ft_detail_video2}
\end{figure*}

\begin{figure*}[t!]
    \centering
\begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_input3.jpg}
    \end{overpic}
    \put(-505,40){\bfseries\scriptsize \rotatebox{90}{Input}}
    \put(-480,71){\bfseries\scriptsize T=0}
    \put(-30,71){\bfseries\scriptsize T=10}
    \put(-75,71){\bfseries\scriptsize T=9}
    \put(-120,71){\bfseries\scriptsize T=8}
    \put(-165,71){\bfseries\scriptsize T=7}
    \put(-210,71){\bfseries\scriptsize T=6}
    \put(-255,71){\bfseries\scriptsize T=5}
    \put(-300,71){\bfseries\scriptsize T=4}
    \put(-345,71){\bfseries\scriptsize T=3}
    \put(-390,71){\bfseries\scriptsize T=2}
    \put(-435,71){\bfseries\scriptsize T=1}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_deca3.jpg}
    \end{overpic}
    \put(-505,35){\bfseries\scriptsize \rotatebox{90}{DECA~\citesupp{feng2021learningsupp}}}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_emoca3.jpg}
    \end{overpic}
    \put(-505,30){\bfseries\scriptsize \rotatebox{90}{EMOCA~\citesupp{danvevcek2022emocasupp}}}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_coarse3_v2.jpg}
    \end{overpic}
    \put(-505,35){\bfseries\scriptsize \rotatebox{90}{Dense~\citesupp{wood2022densesupp}}}


    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_video_example_detail3_v2.jpg}
    \end{overpic}
    \put(-505,40){\bfseries\scriptsize \rotatebox{90}{Ours}}
    \vspace{-5pt}
    \caption{\textbf{Illustration on the flexibility of {\module} on video reconstruction (part 3).} We visualized the reconstruction quality of Dense~\protect\citesupp{wood2022densesupp} with/without our {\module} and compare them with prior art~\protect\citesupp{feng2021learningsupp,danvevcek2022emocasupp}.
    Videos are taken from YouTube.
    }
    \label{fig:supp_ft_detail_video3}
\end{figure*}


Tab.~\ref{tab:userstudy} shows that more than half of users perceive that our reconstructed shapes are more similar to the given images, and  users vote that {\name} reconstructs more realistic details compared to others. As a comparison, the second-best detail reconstruction method~\citesupp{feng2021learningsupp} has only  votes. It demonstrates the superiority of our methods in reconstructing coarse shapes and details.

\begin{table*}[t!]
\caption{\textbf{User study results on detail and expression transfer.} We group the driving images into young people (A) and elder people (B), and ask artists to score the animation quality of static details and dynamic expressions of the transferred images, corresponding to the 1st and 2nd row of each source image in Fig.~\ref{fig:supp_animation} and Fig.~\ref{fig:supp_animation2}. We report the average scores, median scores, and standard deviation. ``5'' indicates the best score while ``1'' indicates the worst score.
}
\label{tab:supp_user2}
\vspace{5pt}
\resizebox{1\linewidth}{!}{\begin{tabular}{cc|cccccc|cccccc|cccccc}
\toprule[1pt]
\multicolumn{2}{c|}{\cellcolor{red!10}{Driving Group}}                & \multicolumn{6}{c|}{\cellcolor{tabblue!10}{A (1-5)}}  & \multicolumn{6}{c|}{\cellcolor{red!10}{B (6-10)}}  & \multicolumn{6}{{c}}{\cellcolor{tabblue!10}{All}} \\ \midrule[1pt]
\multicolumn{1}{c|}{\multirow{2}{*}{Source}}     &  \multirow{2}{*}{Method} & \multicolumn{3}{c}{Static}   & \multicolumn{3}{c|}{Dynamic}   & \multicolumn{3}{c}{Static}   & \multicolumn{3}{c|}{Dynamic}   & \multicolumn{3}{c}{Static}   & \multicolumn{3}{c}{Dynamic}   \\ \cline{3-20}
\multicolumn{1}{c|}{}                                &                         & avg.   & med.   & std.   & avg.    & med.   & std.   & avg.   & med.   & std.   & avg.  & med.  & std.  & avg.   & med.   & std.   & avg.   & med.   & std.   \\ \midrule[1pt]
\multicolumn{1}{c|}{\multirow{3}{*}{I}} & DECA~\citesupp{feng2021learningsupp}   &  1.63 & 1.50 & 0.72      &  2.50 & 2.50 & 1.05      & 3.27 & 3.00 & 0.65     &   2.73 & 2.50 & 0.80     &          2.45 & 2.50 & 1.07      &  2.62 & 2.50 & 0.93    \\
\multicolumn{1}{c|}{}                   & EMOCA~\citesupp{danvevcek2022emocasupp}  &  1.93 & 2.00 & 0.82      &    1.97 & 2.00 & 0.92     &   3.20 & 3.00 & 0.62   & 2.40 & 2.50 & 0.74       &   2.57 & 3.00 & 0.96      & 2.18 & 2.00 & 0.85       \\
\multicolumn{1}{c|}{}                   & Ours   &   \textbf{4.40} & \textbf{4.50} & 0.60     &   \textbf{4.63} & \textbf{5.00} & 0.44            &   \textbf{4.13} &  \textbf{4.00} & 0.35   &   \textbf{4.53} & \textbf{4.50} & 0.44     &   \textbf{4.27} & \textbf{4.00} & 0.50      &   \textbf{4.58} & \textbf{4.50} & 0.44   \\ \midrule[1pt]
\multicolumn{1}{c|}{\multirow{3}{*}{II}} & DECA~\citesupp{feng2021learningsupp}   &   1.70 & 2.00 & 0.62     &    2.47 & 2.00 & 1.08     & 3.37 & 3.00 & 0.52     &  3.10 & 3.00 & 0.78      &   2.53 & 2.75 & 1.02      & 2.78 & 3.00 & 0.98       \\
\multicolumn{1}{c|}{}                   & EMOCA~\citesupp{danvevcek2022emocasupp}  &   2.07 & 2.00 & 0.73     &   2.37 & 2.00 & 1.03     &  3.73 & 4.00 & 0.68     & 2.07 & 2.00 & 0.84       &   2.90 & 3.00 & 1.09      &  2.22 & 2.00 & 0.90       \\
\multicolumn{1}{c|}{}                   & Ours   &   \textbf{4.17} & \textbf{4.00} & 0.49     &  \textbf{4.80} & \textbf{5.00} & 0.41       &     \textbf{4.30} & \textbf{4.00} & 0.59     &   \textbf{4.47} & \textbf{4.00} & 0.40     &  \textbf{4.23} & \textbf{4.00} & 0.54       &   \textbf{4.63} & \textbf{5.00} & 0.43   \\ \bottomrule[1pt]
\end{tabular}
}
\end{table*}

 
\begin{figure*}[!t]
    \centering
\begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_animation1.jpg}
    \end{overpic}
    \put(-483,38){\bfseries\scriptsize Ours}    
    \put(-493,178){\bfseries\scriptsize EMOCA~\citesupp{danvevcek2022emocasupp}}   
    \put(-490,318){\bfseries\scriptsize DECA~\citesupp{feng2021learningsupp}} 
    \vspace{-5pt}
    \caption{\textbf{Comparison on face animation (part 1).} Given a source image (yellow box), we use the driving images (green box) to drive its person-specific details and expressions. For each method, we manipulate the static (1st-row), dynamic (2nd-row), or both (3rd-row) factors. However, DECA~\protect\citesupp{feng2021learningsupp} (blue box) and EMOCA~\protect\citesupp{danvevcek2022emocasupp} (orange box) can animate the expression-driven details but lack realistic, and cannot transfer the static details from the driving images well. As a comparison, {\name} (red box) is flexible to animate details from static, dynamic, or both factors, and presents vivid animation quality with realistic shapes.
    }
\label{fig:supp_animation}
\end{figure*}

\begin{figure*}[!t]
    \centering
\begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth,grid=false]{img_supp/supp_animation2.jpg}
    \end{overpic}
    \put(-483,38){\bfseries\scriptsize Ours}    
    \put(-493,178){\bfseries\scriptsize EMOCA~\citesupp{danvevcek2022emocasupp}}   
    \put(-490,318){\bfseries\scriptsize DECA~\citesupp{feng2021learningsupp}}  
    \vspace{-5pt}
    \caption{\textbf{Comparison on face animation (part 2).} Given a source image (yellow box), we use the driving images (green box) to drive its person-specific details and expressions. For each method, we manipulate the static (1st-row), dynamic (2nd-row), or both (3rd-row) factors. However, DECA~\protect\citesupp{feng2021learningsupp} (blue box) and EMOCA~\protect\citesupp{danvevcek2022emocasupp} (orange box) can animate the expression-driven details but lack realistic, and cannot transfer the static details from the driving images well. As a comparison, {\name} (red box) is flexible to animate details from static, dynamic, or both factors, and presents vivid animation quality with realistic shapes.
    }
    \vspace{-1pt}
    \label{fig:supp_animation2}
\end{figure*}




\subsection{Flexibility of {\module}}

We introduce the settings of plugging {\module} into optimization-based methods such as Dense~\citesupp{wood2022densesupp}. Specifically, for a given image, we leverage optimization-based methods to regress the identity coefficient  and expression coefficient , and use our feature extractor in {\name} to regress the static coefficient . Then these three coefficients serve as the input of {\module} to synthesize the realistic displacement maps. Finally, we integrate the coarse shape from the optimization-based methods with the synthesized displacement map to obtain the final detailed shapes. 
In Fig.~\ref{fig:supp_ft_detail}, we present additional results to justify the flexibility of {\module}. {\module} can be easily plugged into previous optimization-based methods and introduces realistic details on the coarse shapes to improve the visualized quality.

In addition, we also compare the reconstruction quality in the video sequences and further demonstrate that, given the coarse shape obtained by existing optimization-based methods, the proposed {\module} has the advantage of achieving realistic detailed results based on the existing coarse shape. As Fig.~\ref{fig:supp_ft_detail_video1}-\ref{fig:supp_ft_detail_video3} show, we perform reconstruction on the video sequences by comparing Dense~\citesupp{wood2022densesupp} with/without our {\module} to prior art~\citesupp{feng2021learningsupp,danvevcek2022emocasupp}. Fig.~\ref{fig:supp_ft_detail_video1}-\ref{fig:supp_ft_detail_video3} demonstrate that our {\module} significantly improve the visualized quality compared to the coarse shape from~\citesupp{wood2022densesupp}. Compared to prior art~\citesupp{feng2021learningsupp,danvevcek2022emocasupp}, our {\module} captures subtle and realistic details and outperforms previous methods by a large margin.


\subsection{Detail Animation Comparisons}

While previous state-of-the-art methods~\citesupp{feng2021learningsupp,danvevcek2022emocasupp} directly concatenate the person-specific identity features with expression-aware features and decode them into a displacement map. In our ablation studies, we have demonstrated that simply predicting the dynamic details is rather challenging to achieve satisfactory results (see Fig.~\ref{fig:abldynamic}). Here we present additional comparisons on detail animation to justify our claims.

\myparagraph{Qualitative Comparisons.} In Fig.~\ref{fig:supp_animation} and Fig.~\ref{fig:supp_animation2}, we manipulate the static and/or dynamic details of the source image by assigning the static and/or dynamic codes from the driving image. Specifically, the static coefficient in {\name} (or called ``detail code'' in DECA~\citesupp{feng2021learningsupp} and EMOCA~\citesupp{danvevcek2022emocasupp}) is encoded from the driving image. The dynamic factor is based on the expression coefficient in our {\name}, while in DECA~\citesupp{feng2021learningsupp} and EMOCA~\citesupp{danvevcek2022emocasupp}, it is based on the ``expression parameter'' and ``jaw pose''.
We present the comparisons of the animation results in Fig.~\ref{fig:supp_animation} and Fig.~\ref{fig:supp_animation2}.


We can see that the details are not well decoupled in previous methods.
For example, when we manipulate the static factor to the source image and the driving image is a young child ({\eg}, second column), the output shape should have presented ``young'' details.
However, results in previous methods still exhibit ``noisy'' details, which correspond to ``old'' wrinkles. It demonstrates that such implicit learning is hard to decouple the static and dynamic factors well.

As a comparison, our novelty and insights lie in the essence of 3DMMs that simplify the 2D-to-3D difficulty by statistical models.
We successfully reconstruct plausible static and dynamic details by simplifying such difficulty into feasible regression and interpolation tasks.
The details generated by {\name} are naturally decoupled into static and dynamic factors for animation. For example, if we animate the static factor, the facial details present variation among different age groups, and when we animate the dynamic factor, the expression-driven details are well transferred (see ``Ours'' in Fig.~\ref{fig:supp_animation} and Fig.~\ref{fig:supp_animation2} from left to right). 


\myparagraph{User Study.} We also present another user study to investigate the objective evaluation from  experienced artists in estimating the expression and detail transfer quality. 
More specifically, given the source images in Fig.~\ref{fig:supp_animation} and Fig.~\ref{fig:supp_animation2} (noted as subject I and subject II in Tab.~\ref{tab:supp_user2}), we ask the artists to mark scores ranging from 1 to 5 (5 indicates the best score) for each driving sample.
The scores are evaluated based on the animation quality {\wrt} the static details and the dynamic expressions from the driving images. The driving images are classified into: A. the young group (1-5 images) and B. the elder group (6-10 images). The quantitative results are summarized in Tab.~\ref{tab:supp_user2}.

According to Tab.~\ref{tab:supp_user2}, we demonstrate that our reconstruction and animation results are better aligned with human perception. For the static factor, we can see the results of previous methods~\citesupp{feng2021learningsupp,danvevcek2022emocasupp} present an imbalanced distribution, {\ie}, when the driving images are young people's, the score is, in general, worse than that of the elder group. As for the expression transfer, previous methods reach worse scores when the driving images contain extreme expressions (corresponding to a larger standard deviation in Tab.~\ref{tab:supp_user2}). As a comparison, our method presents higher scores with smaller variances among different age groups, which demonstrates the power of our model in transferring novel expressions and details.







\section{Limitations \& Future Work}
\label{sec.supp_limit}

This paper proposes a novel approach to reconstructing animatable details from monocular images. While we manage to synthesize realistic details and demonstrate higher accuracy compared to previous state-of-the-art, our work still has limitations. We pinpoint these challenges in the 3D face community and leave them for future work.


\myparagraph{Facial Appearance Model.}
We use the vanilla albedo 3DMM to linearly represent the facial appearance. While we focus more on the geometry shape, such albedo inherently lacks details and indirectly influences the training of {\name}. In the future, we plan to integrate the diffuse model and spectral model to present high-fidelity facial appearance and extend our {\name} with photo-realistic textures.


\myparagraph{Reconstruction Quality.}
While we achieve state-of-the-art reconstruction quality in the REALY~\cite{REALY} benchmark in terms of the overall quality in Tab.~\ref{tab:benchmark}. We notice {\name} does not perform the best in the mouth and cheek, which are highly emotional and structural regions. To address this problem, we leave it for future work to incorporate the emotion-aware perceptual loss and structure-aware constraints in these regions to further improve the reconstruction quality of {\name}.

\myparagraph{Displacement Prior.}
We demonstrate the necessity to leverage the statistical model to constrain the displacement distribution. However, due to the high expense of capturing large-scale and high-quality displacements for training a non-linear model. We choose the common practice of linear PCA model to build displacement bases for it is easy to implement and data amount friendly.
We trade off the learning difficulty and personalized details ({\eg}, nevus) through the statistical model. Therefore, it is still challenging to recover pore-level details.
In addition, we notice that the imbalanced data ({\ie}, a majority of young scans and images with fewer children and elders) also influence the representation of our model. In the future, we plan to capture and synthesize more class-balanced data to train a non-linear model and leverage the versatile generative models to reconstruct high-resolution displacement maps for more realistic 3D faces.




\myparagraph{Evaluation on Facial Animation.}
We present the visualized comparisons in transferring facial expressions and their details. However, we can only refer to quantitative comparisons for missing such a benchmark to estimate the transfer accuracy. We leave it for future work to construct a benchmark to evaluate the quality of expression transfer.

\section{Potential Social Impact} \label{sec.supp_social}


While this paper successfully reconstructs 3D shapes with animatable details from the monocular images, it is not intended to create content that is used to mislead or deceive. Therefore, this paper does not raise disinformation or immediate security concerns.
However, like other related 3D face reconstruction and animation techniques, it could still potentially be misused for impersonating humans. We condemn any behavior to create misleading or harmful content of real persons.
We encourage researchers in the 3D face community to consider the questions about preventing privacy disclosure before applying the model to the real world.
%