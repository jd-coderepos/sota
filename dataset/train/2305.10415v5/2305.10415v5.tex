\documentclass{article}





\usepackage[preprint]{neurips_2023}













\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{float}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{array}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{threeparttable}
\usepackage{longtable}
\usepackage[table]{xcolor}
\usepackage{caption}
\usepackage{subfigure}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{multirow}
\usepackage[toc,page]{appendix}
\usepackage{soul}
\usepackage{color}
\usepackage{xcolor}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\newcommand{\weidi}[1]{{\textcolor{magenta}{[Weidi: #1]}}}
\newcommand{\xiaoman}[1]{{\textcolor{cyan}{[xiaoman: #1]}}}
\newcommand{\chaoyi}[1]{{\textcolor{red}{[chaoyi: #1]}}}

\title{PMC-VQA: Visual Instruction Tuning for \\ Medical Visual Question Answering}





\author{Xiaoman Zhang\textsuperscript{1,2,*},
  Chaoyi Wu\textsuperscript{1,2,*},
  Ziheng Zhao\textsuperscript{1}, 
  Weixiong Lin\textsuperscript{1} \\  [2pt]
  \textbf{Ya Zhang\textsuperscript{1,2}},
  \textbf{Yanfeng Wang\textsuperscript{1,2,\dag}},
  \textbf{Weidi Xie\textsuperscript{1,2,\dag}}  \\ [2pt]
  \tt\small{\{xm99sjtu, wtzxxxwcy02, weidi\}@sjtu.edu.cn}\\ [2pt]
   \textsuperscript{1}Shanghai Jiao Tong University \quad
   \textsuperscript{2}Shanghai AI Laboratory \
    \hat{a}_i = \Phi_{\text{MedVQA}}(\mathcal{I}_i, q_i; \Theta) = \Phi_{\text{dec}}(\Phi_{\text{vis}}(\mathcal{I}_i; \theta_{\text{vis}}), \Phi_{\text{text}}(q_i; \theta_{\text{text}}); \theta_{\text{dec}}) 

\mathcal{L}(\Theta) = -\sum_{t=1}^{T} \log p(a^t|\mathcal{I}, q^{1:T}, a^{1:t-1}; \Theta)

where  is the length of the ground-truth answer, and  is the probability of generating the -th token in the answer sequence given the input image , the question sequence , and the previous tokens in the answer sequence . 
This formulation allows the model to generate diverse and informative answers, which can be useful in a wider range of scenarios than traditional classification-based methods.

\subsection{Architecture}
\label{sec:architecture}
In this section, we introduce our proposed architecture for generative MedVQA~(Fig.~\ref{fig:figure1}).
Specifically, we offer two model variants, which are tailored to encoder-based and decoder-based language models, respectively, denoted as MedVInT-TE~(Sec.~\ref{sec:MedVInT-TE}) and MedVInT-TD~(Sec.~\ref{sec:MedVInT-TD}).


\subsubsection{MedVInT-TE}
\label{sec:MedVInT-TE}
\textbf{Visual Encoder.} 
Given one specific image , 
we can obtain the image embedding, {\em i.e.}, ,
where  denotes the embedding dimension,  denotes the patch number.
The vision encoder is based on a pre-trained ResNet-50 adopted from PMC-CLIP~\cite{lin2023pmcclip}, with a trainable projection module.
To produce a fixed shape of visual output,
we add a trainable projection module on top of the ResNet-50, with the aim of bridging the gap between the pre-trained visual and language embeddings. 
We propose two distinct variants for this projection module. 
The first variant, MLP-based, employs a two-layer Multilayer Perceptron (MLP), while the second variant, transformer-based, employs a 12-layer transformer decoder supplemented with several learnable vectors as query input.


\textbf{Language Encoder.} 
Given one question on the image,
to guide the language model with desirable output, 
we append a fixed prompt with the question, 
{\em i.e.}, ``Question: , the answer is:'', and encode it with the language encoder: ,
where  refers to the text embedding,  represents the sequential length for the question, and  is the prompted question.
 is initialized with the pre-trained language model.
Note that our model can also be applied to multiple-choice tasks, 
by providing options and training it to output the right choice as "A/B/C/D". 
The prompt is then modified as ``Question: , the options are: , the answer is:'', where  refers to the -th option.


\textbf{Multimodal Decoder.}
With encoded visual embeddings~() and question embeddings~(), we concatenate them as the input to the multimodal decoder~().
The multimodal decoder is initialized from scratch with a 4-layer transformer structure. 
Additionally, acknowledging that the encoder-based language models lack casual masking, we reform the generation task as a mask language modeling task, 
{\em i.e.}, the question input is padded with several `[MASK]' token 
and the decoder module learns to generate the prediction for the masked token.

\subsubsection{MedVInT-TD}
\label{sec:MedVInT-TD}

\textbf{Visual Encoder.} 
The visual encoder is the same as MedVInT-TE.


\textbf{Text Encoder.} 
We design  as a simple embedding layer similar to the primary GPT-like LLMs and initialized with their parameters. Same with MedVInT-TE, it also encodes the question input into embedding features  and can perform multi-choice or blank through different prompts.

\textbf{Multimodal Decoder.}
For the Transformer decoder-based language model, with its output format already being free-form text, we directly use its architecture as the multimodal decoder initialized with the pre-train weights. 
Specifically, we concatenate the image and text features as the input.
However, directly using the text decoder as a multimodal decoder, may lead to significant mismatching between the image encoding space and the decoder input space. Therefore, to further fill the gap between the image embedding space, here, we pre-train the whole network using the PMC-OA\cite{lin2023pmcclip} dataset in a caption-based manner, which is similar to BLIP-2~\cite{li2023blip}. 


\begin{comment}
\weidi{may be consider this way, from the begining, say, we are gonna introduce two architectures, based on two different philosophies, then subsubsection-MedVinT-TE, subsubsection-MedVinT-TD, separately describe the two models, ......}
\xiaoman{fixed}

\weidi{we also need to detail the situation our model gives free-form and multi-choice, what would be the prompt, etc.}
\xiaoman{highlight in blue}

\weidi{don't understand what is this trainable projection module, how can it be mapping variable number to fixed number, MLP is not able to do that, it has to be transformer decoder, be clear. }
\xiaoman{the trainable projection module is different for TE and TD, for TE, it is a MLP; for TD, it is a 12-layer transformer. fixed and highlight in blue.}
\end{comment}














































































 
\vspace{-0.2cm}
\section{The PMC-VQA Dataset}
Our study has identified the lack of large-scale, multi-modal MedVQA datasets as a significant obstacle to the development of effective generative MedVQA models. To address this issue, we present a scalable and automatic pipeline for creating a new large  MedVQA dataset. 
In this section, we provide a detailed description of our dataset collection process, starting with the source data and continuing with the question-answer generation and data filtering procedures. 
Finally, we analyze the collected data from various perspectives to gain insights into its properties and potential applications.




\noindent {\bf Source Data. }
We start from PMC-OA~\cite{lin2023pmcclip}, which is a comprehensive biomedical dataset comprising 1.6 million image-text pairs collected from PubMedCentral~(PMC)'s OpenAccess subset~\cite{roberts2001pubmed}, which covers 2.4 million papers. 
In order to maintain the diversity and complexity of PMC-VQA, 
we have used a version of \textbf{381K image-caption pairs} obtained from the first stage of the medical figure collection process without subfigure auto-separation. 
We have opted not to use the final released version of the dataset, 
which only includes subfigure separation, subcaption separation, and alignment, in order to maintain a certain level of complexity and avoid oversimplifying the dataset. 



\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/example2.pdf}
    \caption{
    Several examples of challenging questions and answers along with their respective images. 
    To answer questions related to these images, 
    the network must acquire sufficient medical knowledge, 
    for example, for the first two images, it is essential to recognize the anatomy structure and modalities;
    for the third image, recognizing the X-ray image pattern of pathologies is necessary;
    for the final two images, apart from the basic biomedical knowledge, 
    the model is also required to discern colors, differentiate subfigures, 
    and perform Optical Character Recognition (OCR).
}
    \label{fig:example}
    \vspace{-0.8cm}
\end{figure}


\noindent {\bf Question-Answer Generation. }
To automatically generate high-quality question-answer pairs within the constraints of an academic budget, we leverage the power of ChatGPT by inputting the image captions of PMC-OA as the content to the model. 
We use the following prompt to generate 5 question-answer pairs for each caption.

\colorbox[RGB]{230,230,230}{
    \footnotesize
    \parbox{0.95\textwidth}
    {
    Ask 5 questions about the content and generate four options for each question. The questions should be answerable with the information provided in the caption, and the four options should include one correct and three incorrect options, with the position of the correct option randomized. The output should use the following template: i:`the question index' question:`the generate question' choice: `A:option content B:option content C:option content D:option content'  answer: The correct option(ABCD).  
    }
}

This approach allows us to generate a large volume of diverse and high-quality questions that cover a wide range of medical topics.
After generating the question-answer pairs using ChatGPT, we applied a rigorous filtering process to ensure that the pairs met our formatting requirements. As a result, we obtained 1,497,808 question-answer pairs, and since the original captions are linked with images, the pairs can naturally find corresponding images, resulting in an average of 3.93 pairs per image.



\noindent {\bf Data Filtering. }
As the questions are sourced from image captions,
some questions can be answered correctly using biomedical knowledge alone without the need for a specific image, for example, question: ``which type of MRI sequence shows high signal in the marrow edema?''.
To address this issue, we trained a question-answer model using LLaMA-7B~\cite{touvron2023llama} with text data only and eliminated all questions that could be potentially answerable by the language model. This filtering process resulted in 848,433 high-quality question-answer pairs.


Furthermore, some questions in our data rely on additional information in the caption that cannot be answered using only the corresponding image, such as ``How many patients were classified into the middle stage?" 
To identify these questions, we trained a question classification model to determine whether a question is answerable given the image alone. Specifically, we manually annotated 2192 question-answer pairs and randomly split them into a training set of 1752 pairs and a testing set of 440 pairs. We fine-tuned LLaMA-7B~\cite{touvron2023llama} on this training set, and our model achieved an accuracy of 81.77\% on the test set. 
We then used this model for data cleaning, resulting in a total of 226,946 question-answer pairs corresponding to 149,075 images. From this cleaned dataset, we randomly selected 50,000 image-question pairs to create our test set, namely, PMC-VQA-test. 
Additionally, we also provided a small \textbf{clean} test set of 2,000 samples, which were manually verified for quality, 
termed as PMC-VQA-test-clean. During this manual verification procedure, 
we have estimated that over 80\% of PMC-VQA-test can be retained.

\noindent {\bf Data Analysis. } 
This section provides an analysis on images, questions, and answers in the PMC-VQA dataset. In detail, the dataset comprises 227k image-question pairs, some examples are presented in Fig.\ref{fig:example}, which demonstrates the wide diversity of images within our dataset. 
As indicated in Table\ref{tab:vqa_dataset}, PMC-VQA outperforms existing MedVQA datasets in terms of data size and modality diversity. The questions in our dataset cover a range of difficulties, from simple questions such as identifying image modalities, perspectives, and organs to challenging questions that require specialized knowledge and judgment. Additionally, our dataset includes difficult questions that demand the ability to identify the specific target sub-figure from the compound figure.



\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/question_ana2.pdf}
    \caption{Question distribution of the training set by their first four words. From left to right are all questions, questions started with ``What'' and questions started with ``Which''.
    The ordering of the words starts towards the center and radiates outwards. }
    \vspace{-0.3cm}
    \label{fig:question_distribution}
\end{figure}

Our analysis of the PMC-VQA dataset can be summarized in three aspects: 
(i) \textbf{Images}: We show the top 20 figure types in the PMC-VQA in Fig.~\ref{fig:image_distribution}. The images in the PMC-VQA are extremely diverse, ranging from Radiology to Signals. 
(ii) \textbf{Questions}: We clustered the questions into different types based on the words that start the question, as shown in Fig.~\ref{fig:question_distribution}. We found a surprising variety of question types, including "What is the difference...", "What type of imaging...", and "Which image shows...". Most questions range from 5 to 15 words, and detailed information about the distribution of question lengths is shown in the supplementary materials. (iii) \textbf{Answers}: The words in answers primarily encompass positional descriptions, image modalities, and specific anatomical regions. Detailed information about the top 50 words that appeared in the answers is provided in the supplementary materials. Most answers are around 5 words, which is much shorter than the questions. 
The correct options were distributed as follows: A~(24.07), B~(30.87), C~(29.09), D~(15.97 ).


%
 
\section{Experiments}

In this section, we first introduce two existing primary MedVQA datasets, namely VQA-RAD and SLAKE (Sec.~\ref{sec:downstream}). We then provide a detailed description of our proposed dataset, PMC-VQA, which can be used for both multiple-choice and open-ended answering tasks (Sec.~\ref{sec:benchmark}). Finally, we discuss the primary pre-trained models we use for ablation in Sec.~\ref{sec:Pre-trained Models}. The implementation details is provided in the supplementary materials.

\subsection{Existing MedVQA Datasets}
\label{sec:downstream}

\noindent \textbf{VQA-RAD}~\cite{lau2018dataset} is a VQA dataset specifically designed for radiology, consisting of 315 images and 3,515 questions with 517 possible answers. The questions in VQA-RAD are categorized as either close-ended or open-ended, depending on whether answer choices are limited or not. We follow the official dataset split for our evaluation.



\noindent \textbf{SLAKE}~\cite{liu2021slake} is an English-Chinese bilingual VQA dataset composed of 642 images and 14k questions.
The questions are categorized as close-ended if answer choices are limited, otherwise open-ended.
There are  possible answers in total.
We only use the ``English'' part, and follow the official split.


\noindent \textbf{Baselines and Metrics.} We compare with various baselines on these two MedVQA datasets, namely, MEVF-BAN~\cite{nguyen2019overcoming}, CPRD-BAN~\cite{liu2021contrastive}, M3AE~\cite{chen2022multi}, PMC-CLIP~\cite{lin2023pmcclip}. PMC-CLIP~\cite{lin2023pmcclip} is the existing SOTA method on these two datasets. For evaluation, ACC scores are used. \textbf{Note that}, since our model is generative-based, we calculate ACC by matching the generative output with the options using \texttt{difflib.SequenceMatcher}\footnote{\url{https://docs.python.org/3/library/difflib.html}} and choosing the most similar one as the choice of the model, which is more difficult than the evaluation for retrieval-based methods due to the larger output space.

\subsection{PMC-VQA Dataset}
\label{sec:benchmark}

The PMC-VQA dataset consists of a train set with 177K samples and a test set with 50K samples, which are respectively denoted as PMC-VQA-train and PMC-VQA-test. 
Additionally, the smaller clean test set with 2K samples that have been manually verified, is referred to as PMC-VQA-test-clean. 
The dataset can be used for both open-ended and multiple-choice tasks. 


\noindent \textbf{Multi-choice MedVQA. } 
Four candidate answers are provided for each question as the prompt.
The model is then trained to \textbf{select the correct option} among them. The accuracy (ACC) score can be used to evaluate the performance of the model on this task.

\noindent \textbf{Open-ended MedVQA. } 
The total possible answers for PMC-VQA are over K, 
which makes the traditional retrieval-based approach limited in usefulness for the answer set of such a level. Therefore, we provide another training style, called ``blank'', 
where the network is not provided with options in input and is required to \textbf{directly generate answers} based on the questions.
For evaluation, we adopt two metrics. The first is Bleu scores, 
which are widely used to evaluate the quality of generated text against a set of references. The second is ACC scores, which can be computed by comparing the generated answer with the ground-truth answer using sentence similarity, as introduced in Sec.~\ref{sec:downstream}.

\subsection{Pre-trained Backbones}
\label{sec:Pre-trained Models}

In this section, we introduce the pre-trained models used in our experiments. We separate them into language and vision backbones. Notably, while all the following models can be used in our architecture, by default, 
we use the ``PMC-LLaMA'' (or ``PMC-LLaMA-ENC'') and ``PMC-CLIP'' as backbones, since they are known to be more suitable for medical data according to previous works.


\subsubsection{Language Backbone}
\vspace{3pt}
\noindent \textbf{LLaMA~\cite{touvron2023llama}} is a state-of-the-art large-scale language model, pre-trained on trillions of tokens and widely used in the research community. 
We adopt the 7B version, which consists of 32 transformer layers, as our language backbone.

\noindent \textbf{PMC-LLaMA~\cite{wu2023pmcllama}} is an open-source language model that is acquired by fine-tuning LLaMA-7B on a total of 4.8 million biomedical academic papers with auto-regressive loss. Compared to LLaMA, PMC-LLaMA demonstrates stronger fitting capabilities and better performance on medical tasks. 



\noindent \textbf{PubMedBERT~\cite{gu2021domain}} is an encoder-based BERT-like model that is trained from scratch using abstracts from PubMed and full-text articles from PubMedCentral in the corpus ``The Pile''~\cite{pile}. It has 12 transformer layers and 100 million parameters. Such domain-specific models proved to yield excellent text embedding capability before the era of large language models.

\noindent \textbf{LLaMA-ENC and PMC-LLaMA-ENC.} While LLaMA and PMC-LLaMA are known for their performance in text generation tasks, we also experiment with them as encoder models by passing a full attention mask and sampling the embedding from the last token. This allows for a direct comparison to be made with the aforementioned BERT-like models, which are also encoder-based.

\subsubsection{Vision Backbone}

\noindent \textbf{CLIP~\cite{radford2021learning}} is a model trained from scratch on a dataset of 400 million image-text pairs collected from the internet with contrastive loss. We use its ``ViT-base-patch32'' version as our visual encoder with 12 transformer layers, which has been pre-trained on natural images.

\noindent \textbf{PMC-CLIP~\cite{lin2023pmcclip}} is a medical-specific visual model based on CLIP architecture, which was trained on a dataset of  million biomedical image-text pairs collected from PubMed open-access papers using cross-modality contrastive loss. Compared to the pre-trained visual model on natural images, PMC-CLIP is specifically designed to handle medical images and text.
















 
\section{Results}

In this section, we begin by evaluating our model on two publicly-available datasets, VQA-RAD and SLAKE, and compare it with existing MedVQA models, showing state-of-the-art performance. However, these datasets have limited diversity and scope, which led us to propose a more challenging MedVQA benchmark in Sec.~\ref{sec:pmc-vqa-results}. Our benchmark covers significantly more diverse modalities and diseases, and we demonstrate that even state-of-the-art methods struggle to perform well on it.


\begin{table}[htb]
\centering
\footnotesize
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{6pt}
\caption{Comparison of ACC to SOTA approaches on VQA-RAD and SLAKE. 
We use the blank model for evaluation. 
Pre-training data indicates whether the model is pre-trained on the medical multi-modal dataset before training on the target dataset.
The best result is in red, the second-best result is in blue. ``Overal'' refers to the micro-average ACC of all the Open and Close questions.  
}
\vspace{3pt}
\begin{tabular}{ll|ccc|ccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Pre-training Data\tnote{*}} & \multicolumn{3}{c|}{VQA-RAD} &  \multicolumn{3}{c}{SLAKE}  \\  
& & Open  & Close   & Overall & Open  & Close   & Overall \\ \midrule
 MEVF-BAN~\cite{nguyen2019overcoming} & -- &  49.2 & 77.2 & 66.1 & 77.8 & 79.8 & 78.6 \\
 CPRD-BAN~\cite{liu2021contrastive} & -- & 52.5 & 77.9 & 67.8 & 79.5 & 83.4 &  81.1 \\
 M3AE~\cite{chen2022multi} & ROCO~\cite{pelka2018roco}, MedICaT~\cite{subramanian-2020-medicat} &  67.2 & 83.5 & 77.0 & 80.3 & \textcolor{blue}{\bf 87.8} & 83.3 \\
 PMC-CLIP~\cite{lin2023pmcclip}  & PMC-OA~\cite{lin2023pmcclip} &  67.0 &  84.0 & 77.6 & 81.9 & \textcolor{red}{\bf 88.0} &  84.3 \\
 \midrule
MedVInT-TE-S & -- & 53.6 & 76.5 & 67.4 & 84.0 & 85.1 & 84.4 \\
MedVInT-TD-S & --  & 55.3 & 80.5 & 70.5 & 79.7 & 85.1 & 81.8 \\
MedVInT-TE & PMC-VQA & \textcolor{blue}{\bf 69.3} & \textcolor{blue}{\bf 84.2} & \textcolor{blue}{\bf 78.2} & \textcolor{red}{\bf 88.2} &  87.7 & \textcolor{red}{\bf 88.0} \\
 MedVInT-TD & PMC-VQA & \textcolor{red}{\bf 73.7} & \textcolor{red}{\bf 86.8} & \textcolor{red}{\bf 81.6}  &\textcolor{blue}{\bf 84.5} & 86.3 & \textcolor{blue}{\bf 85.2} \\
\bottomrule
\end{tabular}
\label{Tab:Transfer}
\end{table}

\subsection{Comparison on Existing Datasets}

As shown in Tab.~\ref{Tab:Transfer}, comparing our model to existing ones, 
we can draw the following observations: 

\noindent \textbf{State-of-the-art Performance of Generative MedVQA.}
As shown in Tab.~\ref{Tab:Transfer}, our MedVInT model outperforms the previous state-of-the-art (SOTA) methods on both the VQA-RAD and SLAKE datasets, regardless of whether the ``MedVInT-TE'' or ``MedVInT-TD'' variant is used. We improved the overall accuracy (ACC) scores from 77.6\% to 81.6\% on VQA-RAD and from 84.3\% to 88.0\% on SLAKE. Notably, since our model generates answers rather than retrieving one from a pre-defined answer basis, the evaluation metric is more challenging, further demonstrating our superiority.

\noindent \textbf{Pre-training on PMC-VQA is Essential for Generative MedVQA.} 
Comparing results using the same architecture, with and without PMC-VQA, it is clear that pre-training with PMC-VQA significantly outperforms. Specifically, ``MedVInT-TE'' boosts the final results by approximately  on VQA-RAD and  on SLAKE compared to ``MedVInT-TE-S'' that refers to training the model from scratch without pre-trained on PMC-VQA. Similar improvements are observed with 'MedVInT-TD'. 
These results highlight the critical role that our PMC-VQA plays in addressing the major challenges that hinder the development of a generative MedVQA system.

\noindent \textbf{Both MedVInT-TE and MedVInT-TD Perform Well.} The gap between the two training styles mainly exists in open-ended questions, with ``MedVInT-TD'' performing better on VQA-RAD and ``MedVInT-TE'' being more effective on SLAKE. This difference can be attributed to the fact that the VQA-RAD answers are typically longer than those in SLAKE, making the ``MedVInT-TD'' model more suitable for generating such answers. Conversely, SLAKE questions often require short and concise responses, making the MedVInT-TE'' model more appropriate for such retrieve-like tasks.

\subsection{Benchmark on PMC-VQA}
\label{sec:pmc-vqa-results}


\begin{table}[htb]
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\caption{Comparison of baseline models using different pre-trained models on both open-ended and multiple-choice tasks. We reported the results on PMC-VQA-test / PMC-VQA-test-clean. ``Scratch'' means to train the vision model from scratch with the same architecture as ``PMC-CLIP''.} 
\vspace{3pt}
\begin{tabular}{l|l|l|cc|c}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Language Backbone} & \multirow{2}{*}{Vision Backbone} & \multicolumn{2}{c|}{Blanking} & Choice    \\ & & & ACC  & Bleu-1   & ACC  \\ \midrule
  \multicolumn{6}{l}{\textbf{ Zero-shot}} \\ \midrule
 PMC-CLIP~\cite{lin2023pmcclip} &  PMC-CLIP~\cite{lin2023pmcclip} &  PMC-CLIP~\cite{lin2023pmcclip} & -- & -- & 24.0 / 24.7 \\ 
 BLIP-2~\cite{li2023blip} &  OPT-2.7B~\cite{Zhang2022OPTOP} & CLIP~\cite{radford2021learning}  & 22.5 / 21.8 & 5.2 / 7.6 & 24.6 / 24.3 \\
 Open-Flamingo~\cite{awadalla2023openflamingo} & LLaMA\cite{touvron2023llama} & CLIP~\cite{radford2021learning} & 26.1 / 26.5 & 4.1 / 4.1 & 25.0 / 26.4 \\ \midrule
  \multicolumn{6}{l}{\textbf{Trained on PMC-VQA}}\\ \midrule
LLaMA~\cite{touvron2023llama} & LLaMA~\cite{touvron2023llama} & -- & 26.1 / 27.2 & 14.2 / 14.6 & 30.6 / 30.8  \\ \midrule
\multirow{9}{*}{MedVInT-TE-MLP} & \multirow{3}{*}{PubMedBERT~\cite{gu2021domain}} & Scratch  & 33.7 / 34.2 & {20.4} / 20.9 & 34.4 / 34.9\\
 & & CLIP~\cite{radford2021learning}    & 33.7 / 34.4 & {20.4} / 20.8 & 34.5 / 34.3\\
 & & PMC-CLIP~\cite{lin2023pmcclip}  & {35.2} / {36.4} & \textcolor{red}{\bf 22.0} / \textcolor{red}{\bf 23.2} & 37.1 / 37.6\\ \cmidrule{2-6} 
 & \multirow{3}{*}{LLaMA-ENC~\cite{touvron2023llama}} & Scratch  & 32.5  /  32.5 & 15.3 / 15.9 & 35.2 / 35.1\\
 & & CLIP~\cite{radford2021learning}     & 32.3 / 33.4 & 15.6 / 15.1 & 35.3 / 36.1\\
 & & PMC-CLIP~\cite{lin2023pmcclip}  & {35.4} / \textcolor{blue}{\bf 36.8} & 18.2 / 18.4 & 36.9 / 37.1 \\ \cmidrule{2-6} 
 & \multirow{3}{*}{PMC-LLaMA-ENC~\cite{wu2023pmcllama}}      & Scratch  & 32.6 / 35.0 & 16.2 / 17.0 & 37.0 / 38.0 \\
 & & CLIP~\cite{radford2021learning}     & 33.0 / 34.4  & 16.6 / 16.5 & 37.1 / 38.5 \\
 & & PMC-CLIP~\cite{lin2023pmcclip}  & 34.8  / 35.3 & 18.1 / 18.6 & {38.2} / 39.2 \\ \midrule
 \multirow{9}{*}{MedVInT-TE-Transformer} & \multirow{3}{*}{PubMedBERT~\cite{gu2021domain}} & Scratch  & 34.1 / 36.2 & \textcolor{blue}{\bf 21.0} / \textcolor{blue}{\bf 21.9} &  39.8 / 40.6\\
 & & CLIP~\cite{radford2021learning}    & 33.9 / 34.6  & 20.6 / 21.8 & \textcolor{blue}{\bf 39.9} / 40.9   \\
 & & PMC-CLIP~\cite{lin2023pmcclip}  & 33.7 / 35.4 &  20.3 / 21.2 & \textcolor{red}{\bf 40.2} / 40.9   \\ \cmidrule{2-6} 
 & \multirow{3}{*}{LLaMA-ENC~\cite{touvron2023llama}} & Scratch  &  32.0 / 33.5 & 15.1 / 15.3 & 38.4 / 39.7 \\
 & & CLIP~\cite{radford2021learning}     & 32.3 / 34.3  & 15.5 / 15.7 & 38.4 / 38.7  \\
 & & PMC-CLIP~\cite{lin2023pmcclip}  & \textcolor{red}{\bf 35.9} / \textcolor{red}{\bf 37.1} & 19.0 / 19.3 & 38.9 / 39.4 \\ \cmidrule{2-6} 
 & \multirow{3}{*}{PMC-LLaMA-ENC~\cite{wu2023pmcllama}}   & Scratch & 33.2 / 34.7 & 16.6 / 16.5 & 38.1 /39.8 \\
 & & CLIP~\cite{radford2021learning}     & 33.6 / 35.1 & 16.7 / 17.2 & 38.7 / 38.9 \\
 & & PMC-CLIP~\cite{lin2023pmcclip}  & \textcolor{blue}{\bf 35.5} / 36.0 & 18.4 /18.6 & 38.2 / 37.7 \\ 
 \midrule
\multirow{6}{*}{MedVInT-TD-MLP}     & \multirow{3}{*}{LLaMA\cite{touvron2023llama}} & Scratch  &  28.1 / 30.6 & 16.5 / 16.9 & 35.8 / 37.4 \\
 & & CLIP~\cite{radford2021learning}     &  30.2 / 32.7  & 18.6 / 18.5 & 35.8 / 37.1  \\
 & & PMC-CLIP~\cite{lin2023pmcclip}  &    31.3 / 32.6 &  19.5 / 19.8 &  38.4 / \textcolor{blue}{\bf 41.0}  \\ \cmidrule{2-6}
 & \multirow{3}{*}{PMC-LLaMA~\cite{wu2023pmcllama}}      & Scratch  &   28.3 / 30.6 & 16.4 / 17.3  & 35.8 / 37.0 \\
 & & CLIP~\cite{radford2021learning}     &   31.4 / 31.8 & 19.2 / 19.5 & 36.2 / 37.9\\
 & & PMC-CLIP~\cite{lin2023pmcclip}  &   32.1 / 31.7 & 19.7 / 20.2 &  38.4 / \textcolor{red}{\bf 42.3}\\ \midrule
 \multirow{6}{*}{MedVInT-TD-Transformer}     & \multirow{3}{*}{LLaMA\cite{touvron2023llama}} & Scratch  &29.1 / 30.2 & 17.4 / 18.0 & 31.1 / 37.9\\
 & & CLIP~\cite{radford2021learning}     &  31.3 / 32.2 & 19.5 / 20.0 & 38.2 / 38.3\\
 & & PMC-CLIP~\cite{lin2023pmcclip}  & 31.9 / 33.4 & 20.0 /  21.3 & 37.3 / {39.5}\\ \cmidrule{2-6}
 & \multirow{3}{*}{PMC-LLaMA~\cite{wu2023pmcllama}}      & Scratch  & 28.6 / 29.8 &  16.8 / 17.4 & 36.8 / 36.9\\
 & & CLIP~\cite{radford2021learning}     & 31.4 / 32.6 & 19.5 / 20.4 & 36.8 / 36.9\\
 & & PMC-CLIP~\cite{lin2023pmcclip}  & 32.7 / 33.6 & 20.3  / {21.5} & {39.4 / 40.3}\\
 \bottomrule
\end{tabular}
\label{Tab:Benckmark}
\end{table}



In this section, we introduce our new MedVQA benchmark on PMC-VQA. 
We evaluate different methods for both open-ended and multiple-choice tasks. 
The results are summarized in Tab.~\ref{Tab:Benckmark}~({See supplementary for more qualitative comparisons.}).We can draw the following observations:


 \noindent \textbf{Multimodal Understanding is Essential.}
As shown in Tab.~\ref{Tab:Benckmark}, when using only language, the model struggles to provide accurate answers and produces nearly random outcomes, with accuracies of only 26.1\% in Blanking and 30.6\% in Choice. 
It is worth noting that around 30\% of the questions have ``B'' answers, 
making the 30.6\% score nearly equivalent to the highest possible score attainable through guessing. These observations highlight the crucial role of multimodal understanding in our dataset and emphasize the strong relationship between the images and the questions posed.

\noindent \textbf{General Visual-language Models Struggle on MedVQA.}
We evaluated the zero-shot performance of existing SOTA multimodal models, BLIP-2 and open-source version of Flamingo~\cite{li2023blip,awadalla2023openflamingo}.
As shown, even the best-performing models in natural images struggle to answer our MedVQA questions, demonstrating the challenging nature of our dataset and its strong biomedical relevance. 

\noindent \textbf{PMC-VQA-test Presents a Significantly More Challenging Benchmark.}
Notably, the previous SOTA multimodal model for MedVQA, 
PMC-CLIP~\cite{lin2023pmcclip}, struggles on our dataset. 
Not only does it fail to solve the blanking task, but it also significantly underperforms on multi-choice questions, with accuracy close to random. 
These findings underline the difficulty of our dataset and its potential to serve as a more robust benchmark for evaluating VQA models. 

\noindent \textbf{Comparing Generative Model Backbones on PMC-VQA-test.}
To further assess the effectiveness of our proposed method, we compared it against various baselines that use different generative model backbones. 
Our results show that replacing the general visual backbone with a specialized medical one leads to improved performance, highlighting the importance of visual understanding in MedVQA. Additionally, we observed that replacing the language backbone with a domain-specific model also leads to some improvements, although not as significant as those achieved in the visual domain. 

\noindent \textbf{Different Projection Modules Demonstrate Comparable Performance.}
We provide the comparison of baseline models using different projection modules~(MLP or Transformer) on both open-ended and multiple-choice tasks.
As shown, different projection modules demonstrate comparable performance across various evaluation tasks.
Both architectures can effectively reconcile the diversity in the embedding dimensions arising from different pre-trained visual models, making our architecture adaptable to various visual model designs, regardless of whether they are based on ViT or ResNet.






















 
\section{Related Works}




\noindent \textbf{Instruction Tuning with Large-language Models.} 
Large Language Models (LLMs) have recently achieved tremendous success~\cite{ouyang2022training,openai2023gpt4,chatgpt} in generating high-quality text for various tasks such as language translation, summarization, and question answering. 
Open-source models, \emph{e.g.}, Alpaca~\cite{alpaca}, have proposed instruction tuning to train models using examples generated from ChatGPT\cite{chatgpt}, effectively improving the performance of language models. 
In the visual-language domain, concurrent work to ours, Mini-GPT4~\cite{zhu2023minigpt} generates a high-quality image-text dataset by prompting ChatGPT with well-designed inputs. In this paper, we focus on visual instruction tuning for MedVQA, which poses unique challenges due to the complexity of medical texts and the variability of medical images.


\textbf{Medical Visual Question Answering.}
The field of MedVQA has gained significant interest in recent years, with a growing number of studies~\cite{lin2022medical}. Despite the increasing attention, building a robust and reliable MedVQA system remains challenging due to the complexity and variability of medical images, as well as the lack of large-scale and diverse MedVQA datasets. Existing publicly available MedVQA datasets have limitations on diversity,
or dataset scale, for example, RadVisDial~\cite{kovaleva2020towards} only contains samples on chest x-ray images, VQA-Med~\cite{ben2021overview}, VQA-RAD~\cite{lau2018dataset}, and SLAKE~\cite{liu2021slake} have less than 10K images. 
To address these limitations, we propose the PMC-VQA dataset that includes 227k image-question pairs with various image modalities and question types.

 
\section{Conclusion}
In conclusion, this paper addresses the challenge of MedVQA, where even the strongest VQA models trained on natural images yield results that closely resemble random guesses. 
To overcome this,
we propose MedVInT, a generative model tailored to 
advance this crucial medical task.
MedVInT is trained by aligning visual data from a pre-trained vision encoder with language models. 
Additionally, we present a scalable pipeline for constructing PMC-VQA, a comprehensive MedVQA dataset comprising 227k VQA pairs across 149k images, spanning diverse modalities and diseases. 
Our proposed model delivers state-of-the-art performance on existing MedVQA datasets, providing a new and reliable benchmark for evaluating different methods in this field.



%
 
\bibliographystyle{plainnat}
\bibliography{main}
\clearpage
\renewcommand*\contentsname{Supplementary}
\appendix

\section{PMC-VQA Dataset}

\subsection{Examples}
In order to provide a more comprehensive understanding of the dataset, we offer additional examples illustrated in Fig.~\ref{Sample}. This figure showcases random instances of the original image and corresponding captions, along with multiple-choice questions generated from them. Additionally, we present the predictions of MedVInT-TE and MedVInT-TD models, with PMC-CLIP and PMC-LLAMA as their vision and language backbones.


\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figures/sample.pdf}
    \caption{Examples of image captions, images, the generated question-answer pairs, and model prediction. The wrong predictions are highlighted in red.}
    \label{Sample}
    \vspace{-0.5cm}
\end{figure}


\subsection{Data Analysis}
Fig.~\ref{fig:answer_distribution} presents the top 50 words that appeared in the answers. As shown, words in answers primarily encompass positional descriptions such as left and right, image modality such as CT/MRI, and specific anatomical regions.
Fig.~\ref{fig:question_length} shows the percentage of questions and answers with different word lengths.
Most questions range from 5 to 15 words, and most answers are around 5 words.


\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figures/answer_ana.pdf}
    \caption{Answer distribution of training set.}
    \label{fig:answer_distribution}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/question_length.pdf}
    \caption{Percentage of questions and answers with different word lengths.}
    \label{fig:question_length}
\end{figure}



\section{Implementation Details}
\label{sec:implement}
Our models are trained using the AdamW optimizer~\cite {loshchilov2017decoupled} with a learning rate 2e-5.
The max context length is set as 512, and the batch size is 128.
To improve the training speed of our models,
we adopt the Deepspeed acceleration strategy, together with Automatic Mixed Precision~(AMP) and gradient checkpointing~\cite{feng2021optimal}.
All models are implemented in PyTorch and trained on NVIDIA A100 GPU with 80 GB memory


\section{Social Impact}
In an era where the digitization of healthcare is rapidly advancing, and medical data is proliferating, multimodal tools such as Medical Visual Question Answering (MedVQA) present significant potential to revolutionize patient care, empower clinicians, and bolster research. Our contribution in this field is twofold:
First, we introduce a scalable pipeline for the creation of a MedVQA dataset. This scalability ensures a continuous evolution and expansion of the dataset, maintaining its relevance in the ever-changing landscape of healthcare.
Second, we present the PMC-VQA dataset, crafted to overcome the limitations inherent in existing datasets. By encompassing a larger, more diverse selection of medical images, complemented by sophisticated questions and answers, we aim to significantly enhance the reliability and precision of medical multimodal models. This innovation holds the promise of equipping these models with the necessary tools to effectively navigate real-world scenarios.


\section{Limitation}
The proposed PMC-VQA has several limitations:


\noindent \textbf{Inherent Biases:} Despite efforts to construct a comprehensive MedVQA dataset with PMC-VQA, it is important to acknowledge the potential presence of biases in the dataset. Biases might arise from the data collection process, annotation methodology, or underlying distribution of the medical images and questions. Understanding and addressing these biases is crucial for ensuring fair and unbiased performance evaluation.

\noindent \textbf{Potential Annotation Biases:} Despite efforts to ensure quality and accuracy during the annotation process of PMC-VQA-test-clean, the dataset may still be susceptible to annotation biases. The subjective nature of question-answer pairs and the involvement of human annotators introduces the possibility of inconsistencies or subjective interpretations, which could impact the dataset's reliability.

\noindent \textbf{Lacking Comprehensive Evaluation Metrics:} 
Although both the ACC score and Bleu score are utilized in our benchmark for assessing open-ended blanking results, these two metrics fail to capture the fluency of the generated sentence since they measure string similarity irrespective of word order. As exhibited in the third case of Fig.\ref{Sample}, the encoder-based model significantly underperforms compared to the decoder-based model in this regard, a fact not reflected in the quantitative results. Indeed, finding an objective way to evaluate generative results comprehensively poses a significant challenge in the entire generative model community~\cite{vicuna2023}. To address this issue, we plan to explore more evaluation metrics in our benchmark in future work.

\noindent  \textbf{Need for Continual Dataset Expansion and Updates:} The medical field is dynamic, with ongoing advancements and new findings. To ensure the dataset's relevance and coverage of emerging medical knowledge, continual expansion and updates to the PMC-VQA dataset are necessary.
 
\end{document}